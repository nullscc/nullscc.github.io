
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/74/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.SD_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T15:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.SD_2023_07_23/">cs.SD - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition"><a href="#A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition" class="headerlink" title="A meta learning scheme for fast accent domain expansion in Mandarin speech recognition"></a>A meta learning scheme for fast accent domain expansion in Mandarin speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12262">http://arxiv.org/abs/2307.12262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Zhu, Changhao Shan, Bihong Zhang, Jian Yu</li>
<li>for: 这 paper 是为了解决中文识别技术中的方言域扩展问题，提高方言识别精度。</li>
<li>methods: 这 paper 使用了元学习技术，包括模型冻结和多元学习，实现快速方言域扩展。</li>
<li>results: 该方法在方言域扩展任务上达到了3%的相对提升，相比基线模型，在同样的测试集上提高了37%。此外，该方法也在大量数据上实现了4%的相对提升。<details>
<summary>Abstract</summary>
Spoken languages show significant variation across mandarin and accent. Despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. In this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. Meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. So we select meta-learning in the domain expansion task. This more essential learning will cause improved performance on accent domain extension tasks. We combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. Our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. Compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. In addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.
</details>
<details>
<summary>摘要</summary>
spoken languages show significant variation across mandarin and accent. despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. in this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. so we select meta-learning in the domain expansion task. this more essential learning will cause improved performance on accent domain extension tasks. we combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. in addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.
</details></li>
</ul>
<hr>
<h2 id="MyVoice-Arabic-Speech-Resource-Collaboration-Platform"><a href="#MyVoice-Arabic-Speech-Resource-Collaboration-Platform" class="headerlink" title="MyVoice: Arabic Speech Resource Collaboration Platform"></a>MyVoice: Arabic Speech Resource Collaboration Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02503">http://arxiv.org/abs/2308.02503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yousseif Elshahawy, Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali</li>
<li>for: 增强阿拉伯语言技术的发展，收集和整理阿拉伯语言的语音数据。</li>
<li>methods: 使用互联网平台，征集大量的阿拉伯语言口音录音，并提供城市&#x2F;国家精细的口音选择功能。用户可以 switching roles，从记录者变成评估者，并提供反馈。平台还包括质量检查系统，过滤掉低质量和假 recording。</li>
<li>results: 实现了收集大量的阿拉伯语言口音数据，提供城市&#x2F;国家精细的口音选择功能，并且可以进行多方合作，汇集多种阿拉伯语言数据。<details>
<summary>Abstract</summary>
We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data.
</details>
<details>
<summary>摘要</summary>
我团队介绍MyVoice，一个招待人寄语的平台，用于提高阿拉伯语言口音技术。该平台提供了大量地方口音数据的设计机会，并将其公共地发布。MyVoice让参与者可以选择城市/国家精细口音，并录制显示的语音。用户可以在角色之间切换，包括参与者和注释者。平台包含一个质量保证系统，过滤掉低质量和假语音记录，然后将其发送给验证。在验证阶段，参与者可以评估语音质量，注释和提供反馈，这些反馈会被管理员审核。此外，平台允许管理员添加新的数据或任务，以外语言和词汇收集，这些任务将被显示给参与者。因此，MyVoice平台可以促进多方合作，收集多样化和大量的阿拉伯语言数据。
</details></li>
</ul>
<hr>
<h2 id="Signal-Reconstruction-from-Mel-spectrogram-Based-on-Bi-level-Consistency-of-Full-band-Magnitude-and-Phase"><a href="#Signal-Reconstruction-from-Mel-spectrogram-Based-on-Bi-level-Consistency-of-Full-band-Magnitude-and-Phase" class="headerlink" title="Signal Reconstruction from Mel-spectrogram Based on Bi-level Consistency of Full-band Magnitude and Phase"></a>Signal Reconstruction from Mel-spectrogram Based on Bi-level Consistency of Full-band Magnitude and Phase</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12232">http://arxiv.org/abs/2307.12232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YoshikiMas/signal-reconstruction-from-mel-spectrogram">https://github.com/YoshikiMas/signal-reconstruction-from-mel-spectrogram</a></li>
<li>paper_authors: Yoshiki Masuyama, Natsuki Ueno, Nobutaka Ono</li>
<li>for: 重建时域信号从低维спектроgram中</li>
<li>methods: 利用丰富的听取关系和时域信号之间的双层关系，并使用优化问题的形式来重建全带幅强度和相位信息</li>
<li>results: 对话、音乐和环境信号都有较好的重建效果<details>
<summary>Abstract</summary>
We propose an optimization-based method for reconstructing a time-domain signal from a low-dimensional spectral representation such as a mel-spectrogram. Phase reconstruction has been studied to reconstruct a time-domain signal from the full-band short-time Fourier transform (STFT) magnitude. The Griffin-Lim algorithm (GLA) has been widely used because it relies only on the redundancy of STFT and is applicable to various audio signals. In this paper, we jointly reconstruct the full-band magnitude and phase by considering the bi-level relationships among the time-domain signal, its STFT coefficients, and its mel-spectrogram. The proposed method is formulated as a rigorous optimization problem and estimates the full-band magnitude based on the criterion used in GLA. Our experiments demonstrate the effectiveness of the proposed method on speech, music, and environmental signals.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于优化的方法，用于从低维特征表示（如MEL spectrogram）重建时域信号。 phase reconstruction 已经研究过了从全带快时域傅立叙Transform（STFT）大小取得时域信号的重建方法。格里菲恩-林算法（GLA）在广泛使用，因为它只凭借 STFT 的重复性而工作，适用于各种音频信号。在这篇论文中，我们同时重建全带大小和频谱图中的相对关系，并基于这些关系进行优化问题的形式化表述。我们的实验表明，提议的方法在语音、音乐和环境信号上具有效果。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation"><a href="#Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation" class="headerlink" title="Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation"></a>Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12231">http://arxiv.org/abs/2307.12231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhong-Qiu Wang, Nobutaka Ono, Yanmin Qian, Shinji Watanabe</li>
<li>for: 这个论文是为了研究听话筛选和识别的集成，以提高多个人识别性能。</li>
<li>methods: 本论文使用了多通道分离方法、面积基于的扩展射频映射和复杂spectral mapping，以及最佳的后续模型特征。</li>
<li>results: 研究人员使用了最新的自动学习表示(SSLR)来改进filterbank特征，并通过合理的训练策略来集成语音分离和识别。结果显示，该策略在噪音混响WHAMR!测试集上实现了2.5%单词错误率，与现有的面积基于MVDR扩展射频映射和filterbank集成的28.9%相比，显著提高了多个人识别性能。<details>
<summary>Abstract</summary>
Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).
</details>
<details>
<summary>摘要</summary>
neuronal speech separation 已经取得了很大的进步，其与自动语音识别（ASR）的结合是实现多 speaker ASR 的重要方向。本工作提供了深入的Investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end。具体来说，我们探讨了多通道分离方法，Mask-based beamforming和复杂的 spectral mapping，以及ASR back-end模型中最佳的特征。我们使用了最近的自然语言学习表示（SSLR）作为特征，并从filterbank特征中提高了认知性能。为了进一步提高多 speaker recognition性能，我们提出了一种优化的训练策略，将 speech separation和recognition与SSLR结合使用。我们使用TF-GridNet-based complex spectral mapping和WavLM-based SSLR，在抗噪抗干扰 WHAMR! 测试集上实现了2.5% 词错率，与现有的mask-based MVDR beamforming和filterbank结合（28.9%）相比，显著超越了。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Attacks-against-Voice-Recognition-Systems-A-Survey"><a href="#Backdoor-Attacks-against-Voice-Recognition-Systems-A-Survey" class="headerlink" title="Backdoor Attacks against Voice Recognition Systems: A Survey"></a>Backdoor Attacks against Voice Recognition Systems: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13643">http://arxiv.org/abs/2307.13643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baochen Yan, Jiahe Lan, Zheng Yan</li>
<li>for: This paper aims to provide a comprehensive survey on backdoor attacks against Voice Recognition Systems (VRSs) and to discuss the feasibility of deploying classic backdoor defense methods and generic audio defense techniques on VRSs.</li>
<li>methods: The paper employs a comprehensive taxonomy of backdoor attacks against VRSs from different perspectives, and analyzes the characteristic of different categories. It also reviews existing attack methods and classic backdoor defense methods, and discusses the feasibility of deploying them on VRSs.</li>
<li>results: The paper provides a thorough review of backdoor attacks against VRSs, and discusses the open issues and future research directions in this field. It also provides a comprehensive understanding of the vulnerabilities of VRSs to backdoor attacks and the potential solutions to mitigate these attacks.<details>
<summary>Abstract</summary>
Voice Recognition Systems (VRSs) employ deep learning for speech recognition and speaker recognition. They have been widely deployed in various real-world applications, from intelligent voice assistance to telephony surveillance and biometric authentication. However, prior research has revealed the vulnerability of VRSs to backdoor attacks, which pose a significant threat to the security and privacy of VRSs. Unfortunately, existing literature lacks a thorough review on this topic. This paper fills this research gap by conducting a comprehensive survey on backdoor attacks against VRSs. We first present an overview of VRSs and backdoor attacks, elucidating their basic knowledge. Then we propose a set of evaluation criteria to assess the performance of backdoor attack methods. Next, we present a comprehensive taxonomy of backdoor attacks against VRSs from different perspectives and analyze the characteristic of different categories. After that, we comprehensively review existing attack methods and analyze their pros and cons based on the proposed criteria. Furthermore, we review classic backdoor defense methods and generic audio defense techniques. Then we discuss the feasibility of deploying them on VRSs. Finally, we figure out several open issues and further suggest future research directions to motivate the research of VRSs security.
</details>
<details>
<summary>摘要</summary>
声认系统（VRS）利用深度学习进行语音识别和说话人识别。它们在各种现实应用中广泛应用，从智能语音助手到电信监测和生物认证。然而，先前的研究表明，VRS受到后门攻击的威胁，这对VRS的安全性和隐私具有重要性。然而，现有的文献缺乏对这个话题的全面审查。这篇论文填补了这个研究空白，通过进行VRS对后门攻击的全面评估。我们首先提供VRS和后门攻击的概述，并提出评估后门攻击方法的评价标准。然后，我们提出了VRS对后门攻击的多维分类，并分析不同类别的特点。接着，我们对现有的攻击方法进行了全面的审查，并分析了它们的优缺点。此外，我们还评估了经典的后门防御方法和通用音频防御技术，并评估了它们在VRS上的可行性。最后，我们提出了一些未解决的问题，并建议未来研究VRS的安全性。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding"><a href="#Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding" class="headerlink" title="Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding"></a>Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12134">http://arxiv.org/abs/2307.12134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyoun Kim, Akshat Shrivastava, Duc Le, Ju Lin, Ozlem Kalinli, Michael L. Seltzer</li>
<li>for: 提高 END-to-END（E2E）语音理解（SLU）系统的 Robustness，使其在听写识别（ASR）错误时仍能准确理解语音。</li>
<li>methods: 我们提出了一种新的 E2E SLU 系统，利用 audio 和文本表示，并基于 ASR 假设的模态信息确定精度。我们采用了两种新技术：1）有效地编码 ASR 假设质量，2）有效地将其集成到 E2E SLU 模型中。</li>
<li>results: 我们在 STOP 数据集上进行了实验，并发现我们的方法可以提高准确率。我们还进行了分析，以证明我们的方法的有效性。<details>
<summary>Abstract</summary>
End-to-end (E2E) spoken language understanding (SLU) systems that generate a semantic parse from speech have become more promising recently. This approach uses a single model that utilizes audio and text representations from pre-trained speech recognition models (ASR), and outperforms traditional pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems still show weakness when text representation quality is low due to ASR transcription errors. To overcome this issue, we propose a novel E2E SLU system that enhances robustness to ASR errors by fusing audio and text representations based on the estimated modality confidence of ASR hypotheses. We introduce two novel techniques: 1) an effective method to encode the quality of ASR hypotheses and 2) an effective approach to integrate them into E2E SLU models. We show accuracy improvements on STOP dataset and share the analysis to demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
最近，端到端（E2E）的语音理解（SLU）系统在使用预训练的语音识别（ASR）模型时，变得更加有前途。这种方法使用单个模型，利用语音和文本表示从预训练的ASR模型中获取，并在设备上流动enario下超越传统的管道SLU系统。然而，E2E SLU系统仍然在文本表示质量低下时表现弱，这是因为ASR识别错误。为了解决这个问题，我们提出了一种新的E2E SLU系统，增强了对ASR错误的抗钝性。我们介绍了两种新技术：1）一种有效的ASR假设质量编码方法，2）一种有效的将其集成到E2E SLU模型中的方法。我们在STOP数据集上显示了准确性改进，并提供分析，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Estimating-speaker-direction-on-a-humanoid-robot-with-binaural-acoustic-signals"><a href="#Estimating-speaker-direction-on-a-humanoid-robot-with-binaural-acoustic-signals" class="headerlink" title="Estimating speaker direction on a humanoid robot with binaural acoustic signals"></a>Estimating speaker direction on a humanoid robot with binaural acoustic signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12129">http://arxiv.org/abs/2307.12129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Barot, Katja Mombaur, Ewen MacDonald</li>
<li>for: 这篇论文主要用于讲述一种用于人类对话者的位置估计方法，以实现人类与机器人之间的对话。</li>
<li>methods: 这篇论文使用了一种基于眼睛音源的方法来估计对话者的位置，并考虑了实时应用场景。这种方法在机器人人类头上实现了双耳声音源定位框架。</li>
<li>results: 经过实验和分析，这种方法可以在实时应用场景中提供有效的位置估计结果，并且可以适应不同的对话场景。同时，这种方法也可以减少延迟时间，以便实现实时对话。<details>
<summary>Abstract</summary>
To achieve human-like behaviour during speech interactions, it is necessary for a humanoid robot to estimate the location of a human talker. Here, we present a method to optimize the parameters used for the direction of arrival (DOA) estimation, while also considering real-time applications for human-robot interaction scenarios. This method is applied to binaural sound source localization framework on a humanoid robotic head. Real data is collected and annotated for this work. Optimizations are performed via a brute force method and a Bayesian model based method, results are validated and discussed, and effects on latency for real-time use are also explored.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为实现人类样式的语音互动， robot需要估计人类说话者的位置。我们现在提出一种优化DOA估计参数的方法，同时考虑实时应用场景。这种方法应用于人型机器人头部上的双耳声源定位框架。实际数据收集和标注，并对其进行优化。我们使用枚举方法和 Bayesian 模型基于方法进行优化，并对结果进行验证和讨论。我们还探讨了在实时使用中的延迟影响。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.SD_2023_07_23/" data-id="clorjzlao00tyf188ejuhguwz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.CV_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T13:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.CV_2023_07_23/">cs.CV - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ComPtr-Towards-Diverse-Bi-source-Dense-Prediction-Tasks-via-A-Simple-yet-General-Complementary-Transformer"><a href="#ComPtr-Towards-Diverse-Bi-source-Dense-Prediction-Tasks-via-A-Simple-yet-General-Complementary-Transformer" class="headerlink" title="ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer"></a>ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12349">http://arxiv.org/abs/2307.12349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lartpang/comptr">https://github.com/lartpang/comptr</a></li>
<li>paper_authors: Youwei Pang, Xiaoqi Zhao, Lihe Zhang, Huchuan Lu</li>
<li>for: 这个研究是为了开发一个能够同时处理多种不同任务的复合式对话模型，以提高深度学习（DL）在紧密预测领域的表现。</li>
<li>methods: 本研究使用了一种名为ComPtr的复合式对话模型，它基于信息补充的概念，并具有两个组件：协调强化和差异意识部分。这两个组件可以帮助ComPtr从不同的图像来源中获取重要的视觉 semantic 讯号，并将其转换为多个任务中的有用信息。</li>
<li>results: 在多个代表性的视觉任务中，例如离散检测、RGB-T人数掌握、RGB-D&#x2F;T焦点物探测以及RGB-D semantics 类别分类，ComPtr 都能够获得良好的表现。<details>
<summary>Abstract</summary>
Deep learning (DL) has advanced the field of dense prediction, while gradually dissolving the inherent barriers between different tasks. However, most existing works focus on designing architectures and constructing visual cues only for the specific task, which ignores the potential uniformity introduced by the DL paradigm. In this paper, we attempt to construct a novel \underline{ComP}lementary \underline{tr}ansformer, \textbf{ComPtr}, for diverse bi-source dense prediction tasks. Specifically, unlike existing methods that over-specialize in a single task or a subset of tasks, ComPtr starts from the more general concept of bi-source dense prediction. Based on the basic dependence on information complementarity, we propose consistency enhancement and difference awareness components with which ComPtr can evacuate and collect important visual semantic cues from different image sources for diverse tasks, respectively. ComPtr treats different inputs equally and builds an efficient dense interaction model in the form of sequence-to-sequence on top of the transformer. This task-generic design provides a smooth foundation for constructing the unified model that can simultaneously deal with various bi-source information. In extensive experiments across several representative vision tasks, i.e. remote sensing change detection, RGB-T crowd counting, RGB-D/T salient object detection, and RGB-D semantic segmentation, the proposed method consistently obtains favorable performance. The code will be available at \url{https://github.com/lartpang/ComPtr}.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）在密集预测方面取得了 significiant 进步，逐渐消除了不同任务之间的自然障碍。然而，大多数现有的工作都是为特定任务或子集任务设计特有的建筑和视觉提示，忽视了深度学习 парадиг中的可能性。在这篇论文中，我们尝试构建一种新的 ComPlementary trasnformer，即 ComPtr，用于多种生物源密集预测任务。Specifically， unlike existing methods that over-specialize in a single task or a subset of tasks, ComPtr starts from the more general concept of bi-source dense prediction. Based on the basic dependence on information complementarity, we propose consistency enhancement and difference awareness components with which ComPtr can evacuate and collect important visual semantic cues from different image sources for diverse tasks, respectively. ComPtr treats different inputs equally and builds an efficient dense interaction model in the form of sequence-to-sequence on top of the transformer. This task-generic design provides a smooth foundation for constructing the unified model that can simultaneously deal with various bi-source information. In extensive experiments across several representative vision tasks, i.e. 远程感知变化检测、RGB-T人群计数、RGB-D/T突出物检测和RGB-D semantic segmentation, the proposed method consistently obtains favorable performance. Code will be available at \url{https://github.com/lartpang/ComPtr}.
</details></li>
</ul>
<hr>
<h2 id="ResShift-Efficient-Diffusion-Model-for-Image-Super-resolution-by-Residual-Shifting"><a href="#ResShift-Efficient-Diffusion-Model-for-Image-Super-resolution-by-Residual-Shifting" class="headerlink" title="ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting"></a>ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12348">http://arxiv.org/abs/2307.12348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zsyoaoa/resshift">https://github.com/zsyoaoa/resshift</a></li>
<li>paper_authors: Zongsheng Yue, Jianyi Wang, Chen Change Loy</li>
<li>for: 提高图像超分辨率（SR）方法的执行速度，解决现有方法的执行慢速问题。</li>
<li>methods: 提出一种新的和高效的扩散模型，通过减少扩散步数，消除post加速的需求和相关性能下降。</li>
<li>results: 实验表明，提出的方法在 synthetic 和实际数据集上具有较高或相当于当前状态艺术方法的性能，只需15步扩散。<details>
<summary>Abstract</summary>
Diffusion-based image super-resolution (SR) methods are mainly limited by the low inference speed due to the requirements of hundreds or even thousands of sampling steps. Existing acceleration sampling techniques inevitably sacrifice performance to some extent, leading to over-blurry SR results. To address this issue, we propose a novel and efficient diffusion model for SR that significantly reduces the number of diffusion steps, thereby eliminating the need for post-acceleration during inference and its associated performance deterioration. Our method constructs a Markov chain that transfers between the high-resolution image and the low-resolution image by shifting the residual between them, substantially improving the transition efficiency. Additionally, an elaborate noise schedule is developed to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experiments demonstrate that the proposed method obtains superior or at least comparable performance to current state-of-the-art methods on both synthetic and real-world datasets, even only with 15 sampling steps. Our code and model are available at https://github.com/zsyOAOA/ResShift.
</details>
<details>
<summary>摘要</summary>
Diffusion-based图像超分辨 (SR) 方法主要受限于推断速度较低，因为需要数百或者千个抽象步骤。现有的加速抽象技术无论如何，都会 sacrificing performance一定程度，导致SR结果过度模糊。为解决这个问题，我们提出了一种新的和高效的Diffusion模型，可以大幅减少抽象步骤数量，从而消除推断过程中的后加速和其相关的性能下降。我们的方法构建了一个Markov链，将高分辨图像和低分辨图像之间的差异转移到高分辨图像上，大幅提高了转移效率。此外，我们还开发了一种灵活控制抽象速度和噪声强度的附加noise schedule。广泛的实验表明，我们的方法可以在现有的State-of-the-art方法的基础上实现更好的或者相当的性能，只需要15个抽象步骤。我们的代码和模型可以在https://github.com/zsyOAOA/ResShift上下载。
</details></li>
</ul>
<hr>
<h2 id="Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction"><a href="#Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction" class="headerlink" title="Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction"></a>Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12341">http://arxiv.org/abs/2307.12341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lykourgos Chiniadis, Petros Tamvakis<br>for:* 提高农业生产和土壤属性分析，为生态可持续发展提供关键前提。methods:* 使用FT NIR reflectance спектроскопия和深度学习方法来预测土壤碳酸含量。* 利用多种机器学习算法，如：多层感知网络（MLP）回归器和卷积神经网络（CNN），并与传统的多变量分析（PLSR）、矩阵分析（Cubist）和支持向量机（SVM）进行比较。results:* 使用FT NIR reflectance спектроскопия和深度学习方法可以快速和高效地预测土壤碳酸含量，并且在未看过的土壤样本上达到了良好的预测性能。* 与X射Diffraction量测相比，MLP预测值和实际值之间的相对误差在5%以内，表明深度学习模型可以准确地预测土壤碳酸含量。* 本研究的结果表明，深度学习模型可以作为快速和高效的预测工具，用于预测土壤碳酸含量，特别是在没有量imetric方法可用的情况下。<details>
<summary>Abstract</summary>
Soil NIR spectral absorbance/reflectance libraries are utilized towards improving agricultural production and analysis of soil properties which are key prerequisite for agroecological balance and environmental sustainability. Carbonates in particular, represent a soil property which is mostly affected even by mild, let alone extreme, changes of environmental conditions during climate change. In this study we propose a rapid and efficient way to predict carbonates content in soil by means of FT NIR reflectance spectroscopy and by use of deep learning methods. We exploited multiple machine learning methods, such as: 1) a MLP Regressor and 2) a CNN and compare their performance with other traditional ML algorithms such as PLSR, Cubist and SVM on the combined dataset of two NIR spectral libraries: KSSL (USDA), a dataset of soil samples reflectance spectra collected nationwide, and LUCAS TopSoil (European Soil Library) which contains soil sample absorbance spectra from all over the European Union, and use them to predict carbonate content on never before seen soil samples. Soil samples in KSSL and in TopSoil spectral libraries were acquired in the spectral region of visNIR, however in this study, only the NIR spectral region was utilized. Quantification of carbonates by means of Xray Diffraction is in good agreement with the volumetric method and the MLP prediction. Our work contributes to rapid carbonates content prediction in soil samples in cases where: 1) no volumetric method is available and 2) only NIR spectra absorbance data are available. Up till now and to the best of our knowledge, there exists no other study, that presents a prediction model trained on such an extensive dataset with such promising results on unseen data, undoubtedly supporting the notion that deep learning models present excellent prediction tools for soil carbonates content.
</details>
<details>
<summary>摘要</summary>
soil NIR spectral absorbance/reflectance 图书馆是用于提高农业生产和土壤属性分析的重要途径，这些属性是生态平衡和环境可持续性的关键因素。碳酸盐 particularly 是在气候变化中环境条件轻度到极端变化时受到影响的土壤属性。本研究提出了一种快速和高效的碳酸盐含量预测方法，通过FT NIR 反射спектроскопия和深度学习方法。我们利用了多种机器学习方法，如：1）多层感知网络（MLP）回归器和2）卷积神经网络（CNN），并与传统的机器学习算法如：PLSR、Cubist和SVM进行比较，使用了合并的KSSL（美国农业部）和LUCAS TopSoil（欧盟土壤图书馆）的两个NIR spectral库，用于预测碳酸盐含量。KSSL和TopSoil spectral库中的土壤样本反射спектроскопия数据收集在VisNIRspectral区间内，但在本研究中仅使用NIRspectral区间。X射 diffraction 测量和MLP预测结果表明，我们的方法可以准确预测碳酸盐含量。我们的工作支持在没有体积方法可用时，只有NIR Spectra absorbance数据可用时，可以快速预测碳酸盐含量。到目前为止，我们知道没有其他研究，可以在such an extensive dataset上提出类似的预测模型，并且模型在未见数据上表现了惊人的好准确性，无疑地支持深度学习模型在土壤碳酸盐含量预测中的出色表现。
</details></li>
</ul>
<hr>
<h2 id="Learning-Navigational-Visual-Representations-with-Semantic-Map-Supervision"><a href="#Learning-Navigational-Visual-Representations-with-Semantic-Map-Supervision" class="headerlink" title="Learning Navigational Visual Representations with Semantic Map Supervision"></a>Learning Navigational Visual Representations with Semantic Map Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12335">http://arxiv.org/abs/2307.12335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiconghong/ego2map-navit">https://github.com/yiconghong/ego2map-navit</a></li>
<li>paper_authors: Yicong Hong, Yang Zhou, Ruiyi Zhang, Franck Dernoncourt, Trung Bui, Stephen Gould, Hao Tan</li>
<li>for: 本研究旨在提高家用机器人视觉导航能力，尤其是在室内环境中。</li>
<li>methods: 我们提出了一种基于 egocentric 视图和 semantic 地图（Ego$^2$-Map）的视觉表示学习方法，以增强机器人的导航能力。</li>
<li>results: 我们的实验表明，使用我们学习的表示可以在 object-goal 导航任务中表现出优于最新的视觉预训练方法，并在 continuous 环境中实现新的state-of-the-art 结果。<details>
<summary>Abstract</summary>
Being able to perceive the semantics and the spatial structure of the environment is essential for visual navigation of a household robot. However, most existing works only employ visual backbones pre-trained either with independent images for classification or with self-supervised learning methods to adapt to the indoor navigation domain, neglecting the spatial relationships that are essential to the learning of navigation. Inspired by the behavior that humans naturally build semantically and spatially meaningful cognitive maps in their brains during navigation, in this paper, we propose a novel navigational-specific visual representation learning method by contrasting the agent's egocentric views and semantic maps (Ego$^2$-Map). We apply the visual transformer as the backbone encoder and train the model with data collected from the large-scale Habitat-Matterport3D environments. Ego$^2$-Map learning transfers the compact and rich information from a map, such as objects, structure and transition, to the agent's egocentric representations for navigation. Experiments show that agents using our learned representations on object-goal navigation outperform recent visual pre-training methods. Moreover, our representations significantly improve vision-and-language navigation in continuous environments for both high-level and low-level action spaces, achieving new state-of-the-art results of 47% SR and 41% SPL on the test server.
</details>
<details>
<summary>摘要</summary>
“能够感受到环境的 semantics 和空间结构是家用机器人视觉导航的重要前提。然而，现有的大多数工作仅将视觉背bone 进行独立图像的分类或自我类型学习方法进行适应室内导航领域，忽略了 Navigation 中所需的空间关系。 Drawing inspiration from humans' natural ability to build semantically and spatially meaningful cognitive maps in their brains during navigation, in this paper, we propose a novel navigational-specific visual representation learning method by contrasting the agent's egocentric views and semantic maps (Ego$^2$-Map). We apply the visual transformer as the backbone encoder and train the model with data collected from the large-scale Habitat-Matterport3D environments. Ego$^2$-Map learning transfers the compact and rich information from a map, such as objects, structure, and transition, to the agent's egocentric representations for navigation. Experiments show that agents using our learned representations on object-goal navigation outperform recent visual pre-training methods. Moreover, our representations significantly improve vision-and-language navigation in continuous environments for both high-level and low-level action spaces, achieving new state-of-the-art results of 47% SR and 41% SPL on the test server.”Note that Simplified Chinese is the official standard for Chinese writing in mainland China, and it is used in this translation. Traditional Chinese is used in Taiwan and Hong Kong, and it may have slightly different grammar and character usage.
</details></li>
</ul>
<hr>
<h2 id="ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection"><a href="#ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection" class="headerlink" title="ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection"></a>ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12327">http://arxiv.org/abs/2307.12327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingren Yao, Yuan Zhou, Wei Xiang<br>For:* The paper is written for hyperspectral image change detection (HSI-CD) and aims to identify differences in bitemporal HSIs.Methods:* The paper proposes an end-to-end efficient spectral-spatial change detection network (ES2Net) that includes a learnable band selection module and a cluster-wise spatial attention mechanism.Results:* The paper demonstrates the effectiveness and superiority of the proposed method compared with other state-of-the-art methods through experiments on three widely used HSI-CD datasets.<details>
<summary>Abstract</summary>
Hyperspectral image change detection (HSI-CD) aims to identify the differences in bitemporal HSIs. To mitigate spectral redundancy and improve the discriminativeness of changing features, some methods introduced band selection technology to select bands conducive for CD. However, these methods are limited by the inability to end-to-end training with the deep learning-based feature extractor and lack considering the complex nonlinear relationship among bands. In this paper, we propose an end-to-end efficient spectral-spatial change detection network (ES2Net) to address these issues. Specifically, we devised a learnable band selection module to automatically select bands conducive to CD. It can be jointly optimized with a feature extraction network and capture the complex nonlinear relationships among bands. Moreover, considering the large spatial feature distribution differences among different bands, we design the cluster-wise spatial attention mechanism that assigns a spatial attention factor to each individual band to individually improve the feature discriminativeness for each band. Experiments on three widely used HSI-CD datasets demonstrate the effectiveness and superiority of this method compared with other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
干elijah�image Change Detection (HSI-CD) 的目标是 Identify the differences between bitemporal hyperspectral images (HSIs). To reduce spectral redundancy and improve the discriminativeness of changing features, some methods have introduced band selection technology to select bands that are conducive to CD. However, these methods are limited by their inability to perform end-to-end training with deep learning-based feature extractors and their failure to consider the complex nonlinear relationships among bands.In this paper, we propose an end-to-end efficient spectral-spatial change detection network (ES2Net) to address these issues. Specifically, we have devised a learnable band selection module that automatically selects bands that are conducive to CD. This module can be jointly optimized with a feature extraction network and captures the complex nonlinear relationships among bands. Moreover, considering the large spatial feature distribution differences among different bands, we have designed a cluster-wise spatial attention mechanism that assigns a spatial attention factor to each individual band to individually improve the feature discriminativeness for each band.Experiments on three widely used HSI-CD datasets have demonstrated the effectiveness and superiority of this method compared with other state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models"><a href="#Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models" class="headerlink" title="Development of pericardial fat count images using a combination of three different deep-learning models"></a>Development of pericardial fat count images using a combination of three different deep-learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12316">http://arxiv.org/abs/2307.12316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takaaki Matsunaga, Atsushi Kono, Hidetoshi Matsuo, Kaoru Kitagawa, Mizuho Nishio, Hiromi Hashimura, Yu Izawa, Takayoshi Toba, Kazuki Ishikawa, Akie Katsuki, Kazuyuki Ohmura, Takamichi Murakami</li>
<li>for: The paper aims to generate pericardial fat count images (PFCIs) from chest radiographs (CXRs) using a dedicated deep-learning model, in order to evaluate pericardial fat (PF) and its potential role in the development of coronary artery disease.</li>
<li>methods: The proposed method uses three different deep-learning models, including CycleGAN, to generate PFCIs from CXRs. The method first projects the three-dimensional CT images onto a two-dimensional plane, and then uses the deep-learning models to generate PFCIs from the projected images. The performance of the proposed method is evaluated using structural similarity index measure (SSIM), mean squared error (MSE), and mean absolute error (MAE).</li>
<li>results: The results show that the PFCIs generated using the proposed method have better performance than those generated using a single CycleGAN-based model, as measured by SSIM, MSE, and MAE. The proposed method also shows the potential for evaluating PF without the need for CT scans.<details>
<summary>Abstract</summary>
Rationale and Objectives: Pericardial fat (PF), the thoracic visceral fat surrounding the heart, promotes the development of coronary artery disease by inducing inflammation of the coronary arteries. For evaluating PF, this study aimed to generate pericardial fat count images (PFCIs) from chest radiographs (CXRs) using a dedicated deep-learning model.   Materials and Methods: The data of 269 consecutive patients who underwent coronary computed tomography (CT) were reviewed. Patients with metal implants, pleural effusion, history of thoracic surgery, or that of malignancy were excluded. Thus, the data of 191 patients were used. PFCIs were generated from the projection of three-dimensional CT images, where fat accumulation was represented by a high pixel value. Three different deep-learning models, including CycleGAN, were combined in the proposed method to generate PFCIs from CXRs. A single CycleGAN-based model was used to generate PFCIs from CXRs for comparison with the proposed method. To evaluate the image quality of the generated PFCIs, structural similarity index measure (SSIM), mean squared error (MSE), and mean absolute error (MAE) of (i) the PFCI generated using the proposed method and (ii) the PFCI generated using the single model were compared.   Results: The mean SSIM, MSE, and MAE were as follows: 0.856, 0.0128, and 0.0357, respectively, for the proposed model; and 0.762, 0.0198, and 0.0504, respectively, for the single CycleGAN-based model.   Conclusion: PFCIs generated from CXRs with the proposed model showed better performance than those with the single model. PFCI evaluation without CT may be possible with the proposed method.
</details>
<details>
<summary>摘要</summary>
目的和目标：胸膜脂肪（PF），脏膜内附近心脏的脂肪，可以促进心脏疾病的发展，并导致心脏粥玢病变。为评估PF，本研究想要从胸部X射线图像（CXR）中生成胸膜脂肪计数图像（PFCIs）。材料和方法：本研究审查了269例 consecutively admitted patients的 coronary computed tomography（CT）数据。排除了 метал制品、肿胀、历史上的胸部手术和肿瘤等因素，因此使用了191例的数据。PFCIs由三维CT图像的投影生成，其中脂肪堆积表示高像素值。本研究使用了三种深度学习模型，包括 CycleGAN，来生成PFCIs从CXR。单个 CycleGAN-based 模型用于生成PFCIs从CXR，并与提案方法进行比较。为评估生成的PFCIs的图像质量，使用了结构相似度指数（SSIM）、平均平方误差（MSE）和平均绝对误差（MAE）。结果：* SSIM：0.856* MSE：0.0128* MAE：0.0357比较结果：* SSIM：0.762* MSE：0.0198* MAE：0.0504结论：提案方法生成的PFCIs显示与单个模型生成的PFCIs有更好的性能。PFCI评估可能不需要CT扫描。
</details></li>
</ul>
<hr>
<h2 id="Building-Extraction-from-Remote-Sensing-Images-via-an-Uncertainty-Aware-Network"><a href="#Building-Extraction-from-Remote-Sensing-Images-via-an-Uncertainty-Aware-Network" class="headerlink" title="Building Extraction from Remote Sensing Images via an Uncertainty-Aware Network"></a>Building Extraction from Remote Sensing Images via an Uncertainty-Aware Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12309">http://arxiv.org/abs/2307.12309</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henryjiepanli/uncertainty-aware-network">https://github.com/henryjiepanli/uncertainty-aware-network</a></li>
<li>paper_authors: Wei He, Jiepan Li, Weinan Cao, Liangpei Zhang, Hongyan Zhang</li>
<li>for: 减少建筑物识别错误率</li>
<li>methods: 使用uncertainty-aware网络（UANet）</li>
<li>results: 比其他状态对照方法减少误差率<details>
<summary>Abstract</summary>
Building extraction aims to segment building pixels from remote sensing images and plays an essential role in many applications, such as city planning and urban dynamic monitoring. Over the past few years, deep learning methods with encoder-decoder architectures have achieved remarkable performance due to their powerful feature representation capability. Nevertheless, due to the varying scales and styles of buildings, conventional deep learning models always suffer from uncertain predictions and cannot accurately distinguish the complete footprints of the building from the complex distribution of ground objects, leading to a large degree of omission and commission. In this paper, we realize the importance of uncertain prediction and propose a novel and straightforward Uncertainty-Aware Network (UANet) to alleviate this problem. To verify the performance of our proposed UANet, we conduct extensive experiments on three public building datasets, including the WHU building dataset, the Massachusetts building dataset, and the Inria aerial image dataset. Results demonstrate that the proposed UANet outperforms other state-of-the-art algorithms by a large margin.
</details>
<details>
<summary>摘要</summary>
traditional deep learning models always suffer from uncertain predictions and cannot accurately distinguish the complete footprints of the building from the complex distribution of ground objects, leading to a large degree of omission and commission. To address this problem, we propose a novel and straightforward Uncertainty-Aware Network (UANet) to alleviate this problem. To verify the performance of our proposed UANet, we conduct extensive experiments on three public building datasets, including the WHU building dataset, the Massachusetts building dataset, and the Inria aerial image dataset. Results demonstrate that the proposed UANet outperforms other state-of-the-art algorithms by a large margin.Here's the text with some additional information about the Simplified Chinese translation:The text is translated into Simplified Chinese, which is the standard writing system used in mainland China. The translation is done using a machine translation tool, and the result is a more literal translation of the original text.Some notes about the translation:* "uncertain prediction" is translated as "uncertain predictions" (uncertain predictions is a phrase that is commonly used in machine learning to describe the situation where a model is not sure about its predictions).* "complete footprints" is translated as "complete footprint" (singular form) to match the original text.* "ground objects" is translated as "ground objects" (literal translation), but it could be translated as "ground features" or "ground objects" depending on the context.* "state-of-the-art algorithms" is translated as "state-of-the-art algorithm" (singular form) to match the original text.I hope this helps! Let me know if you have any other questions.
</details></li>
</ul>
<hr>
<h2 id="RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC"><a href="#RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC" class="headerlink" title="RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC"></a>RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12301">http://arxiv.org/abs/2307.12301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mxtsai/ransac-nn">https://github.com/mxtsai/ransac-nn</a></li>
<li>paper_authors: Chen-Han Tsai, Yu-Shao Peng</li>
<li>for:  This paper proposes an unsupervised outlier detection algorithm specifically designed for image data, called RANSAC-NN.</li>
<li>methods: The proposed algorithm uses a RANSAC-based approach to compare images and predict the outlier score without additional training or label information.</li>
<li>results: The proposed algorithm consistently performs favorably against state-of-the-art outlier detection algorithms on 15 diverse datasets without any hyperparameter tuning, and it has potential applications in image mislabeled detection.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文提出了一种特制于图像数据的无监督异常检测算法，名为RANSAC-NN。</li>
<li>methods: 该算法使用RANSAC-based方法比较图像，自动预测每个图像的异常分数，无需额外训练或标签信息。</li>
<li>results: 该算法在15个多样化的数据集中，与现状异常检测算法相比，一般性高，而且无需调整Hyperparameter，并且具有图像推理检测的潜在应用。<details>
<summary>Abstract</summary>
Image outlier detection (OD) is crucial for ensuring the quality and accuracy of image datasets used in computer vision tasks. The majority of OD algorithms, however, have not been targeted toward image data. Consequently, the results of applying such algorithms to images are often suboptimal. In this work, we propose RANSAC-NN, a novel unsupervised OD algorithm specifically designed for images. By comparing images in a RANSAC-based approach, our algorithm automatically predicts the outlier score of each image without additional training or label information. We evaluate RANSAC-NN against state-of-the-art OD algorithms on 15 diverse datasets. Without any hyperparameter tuning, RANSAC-NN consistently performs favorably in contrast to other algorithms in almost every dataset category. Furthermore, we provide a detailed analysis to understand each RANSAC-NN component, and we demonstrate its potential applications in image mislabeled detection. Code for RANSAC-NN is provided at https://github.com/mxtsai/ransac-nn
</details>
<details>
<summary>摘要</summary>
Image outlier detection (OD) 是 Ensure the quality and accuracy of image datasets used in computer vision tasks 的 crucial step. However, most OD algorithms have not been designed specifically for images, leading to suboptimal results when applied to images. In this work, we propose RANSAC-NN, a novel unsupervised OD algorithm tailored for images. By comparing images in a RANSAC-based approach, our algorithm automatically predicts the outlier score of each image without requiring additional training or label information. We evaluate RANSAC-NN against state-of-the-art OD algorithms on 15 diverse datasets and show that it consistently performs well without any hyperparameter tuning. Furthermore, we provide a detailed analysis of each RANSAC-NN component and demonstrate its potential applications in image mislabeled detection. The code for RANSAC-NN is available at https://github.com/mxtsai/ransac-nn.
</details></li>
</ul>
<hr>
<h2 id="Hybrid-CSR-Coupling-Explicit-and-Implicit-Shape-Representation-for-Cortical-Surface-Reconstruction"><a href="#Hybrid-CSR-Coupling-Explicit-and-Implicit-Shape-Representation-for-Cortical-Surface-Reconstruction" class="headerlink" title="Hybrid-CSR: Coupling Explicit and Implicit Shape Representation for Cortical Surface Reconstruction"></a>Hybrid-CSR: Coupling Explicit and Implicit Shape Representation for Cortical Surface Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12299">http://arxiv.org/abs/2307.12299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanlin Sun, Thanh-Tung Le, Chenyu You, Hao Tang, Kun Han, Haoyu Ma, Deying Kong, Xiangyi Yan, Xiaohui Xie</li>
<li>for:  cortical surface reconstruction</li>
<li>methods:  geometric deep-learning model combining explicit and implicit shape representations, mesh-based deformation module, optimization-based diffeomorphic surface registration</li>
<li>results:  surpasses existing implicit and explicit cortical surface reconstruction methods in numeric metrics, including accuracy, regularity, and consistency.<details>
<summary>Abstract</summary>
We present Hybrid-CSR, a geometric deep-learning model that combines explicit and implicit shape representations for cortical surface reconstruction. Specifically, Hybrid-CSR begins with explicit deformations of template meshes to obtain coarsely reconstructed cortical surfaces, based on which the oriented point clouds are estimated for the subsequent differentiable poisson surface reconstruction. By doing so, our method unifies explicit (oriented point clouds) and implicit (indicator function) cortical surface reconstruction. Compared to explicit representation-based methods, our hybrid approach is more friendly to capture detailed structures, and when compared with implicit representation-based methods, our method can be topology aware because of end-to-end training with a mesh-based deformation module. In order to address topology defects, we propose a new topology correction pipeline that relies on optimization-based diffeomorphic surface registration. Experimental results on three brain datasets show that our approach surpasses existing implicit and explicit cortical surface reconstruction methods in numeric metrics in terms of accuracy, regularity, and consistency.
</details>
<details>
<summary>摘要</summary>
我们介绍Hybrid-CSR，一种几何深度学习模型，将显式和隐式形态表示结合用于脑表面重建。具体来说，Hybrid-CSR从template mesh的显式变形开始，以获取粗略重建的脑表面，然后根据这些oriented point clouds进行后续的可 differentiable Poisson surface reconstruction。这样，我们的方法将显式（oriented point clouds）和隐式（指示函数）脑表面重建方法融合在一起。与显式表示基于方法相比，我们的半结合方法更友好地捕捉细节结构，而与隐式表示基于方法相比，我们的方法可以保持topology意识，这是因为我们通过粘性变换模块进行终端训练。为了解决topology问题，我们提出了一种新的topology更正管道，该管道基于数据优化的diffusion surface registration。实验结果表明，我们的方法在三个脑数据集上的数据指标上胜过现有的隐式和显式脑表面重建方法。
</details></li>
</ul>
<hr>
<h2 id="Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames"><a href="#Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames" class="headerlink" title="Simultaneous temperature estimation and nonuniformity correction from multiple frames"></a>Simultaneous temperature estimation and nonuniformity correction from multiple frames</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12297">http://arxiv.org/abs/2307.12297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Omri Berman, Nir Sochen, David Mendelovich, Iftach Klapp</li>
<li>for: 这个论文的目的是提出一种同时进行温度估计和不均匀性修正的方法，以提高低成本的红外摄像机在各种应用中的精度和效率。</li>
<li>methods: 该方法基于深度学习核函数网络（KPN），利用摄像机的物理图像捕获模型，并通过一个新的偏移块来 incorporate  ambient temperature。</li>
<li>results: 对实际数据进行测试，该方法可以 achieve 高精度和高效率的温度估计和不均匀性修正，相比 vanilla KPN 有显著的改善。<details>
<summary>Abstract</summary>
Infrared (IR) cameras are widely used for temperature measurements in various applications, including agriculture, medicine, and security. Low-cost IR camera have an immense potential to replace expansive radiometric cameras in these applications, however low-cost microbolometer-based IR cameras are prone to spatially-variant nonuniformity and to drift in temperature measurements, which limits their usability in practical scenarios.   To address these limitations, we propose a novel approach for simultaneous temperature estimation and nonuniformity correction from multiple frames captured by low-cost microbolometer-based IR cameras. We leverage the physical image acquisition model of the camera and incorporate it into a deep learning architecture called kernel estimation networks (KPN), which enables us to combine multiple frames despite imperfect registration between them. We also propose a novel offset block that incorporates the ambient temperature into the model and enables us to estimate the offset of the camera, which is a key factor in temperature estimation.   Our findings demonstrate that the number of frames has a significant impact on the accuracy of temperature estimation and nonuniformity correction. Moreover, our approach achieves a significant improvement in performance compared to vanilla KPN, thanks to the offset block. The method was tested on real data collected by a low-cost IR camera mounted on a UAV, showing only a small average error of $0.27^\circ C-0.54^\circ C$ relative to costly scientific-grade radiometric cameras.   Our method provides an accurate and efficient solution for simultaneous temperature estimation and nonuniformity correction, which has important implications for a wide range of practical applications.
</details>
<details>
<summary>摘要</summary>
infrared (IR) 摄像机广泛应用于温度测量多个应用场景中，如农业、医学和安全。低成本IR摄像机具有取代昂贵 радиометрические摄像机的潜在优势，但低成本微博拉ometer-based IR摄像机受到空间不均和温度测量中的偏差所限制。为解决这些限制，我们提出了一种新的方法，即同时进行温度估计和不均差修正，使用多帧 captured by low-cost microbolometer-based IR摄像机。我们利用摄像机物理捕获模型，并将其integrated into a deep learning architecture called kernel estimation networks (KPN)，这使得我们可以将多帧组合成一起，即使这些帧之间没有完美对齐。我们还提出了一个新的偏移块，即将室外温度 incorporated into the model，这使得我们可以估计摄像机的偏移，这是温度估计中的关键因素。我们的发现表明，帧数对温度估计和不均差修正的精度有着显著的影响。此外，我们的方法在比 vanilla KPN 的情况下具有显著的改善，这是因为偏移块的存在。我们的方法在实际数据 collected by a low-cost IR摄像机 mounted on a UAV 上进行测试，显示只有小于 $0.27^\circ C-0.54^\circ C$ 的平均误差，相比昂贵的科学级 radiometric 摄像机。我们的方法为温度估计和不均差修正提供了一种准确和高效的解决方案，这有着广泛的实际应用前景。
</details></li>
</ul>
<hr>
<h2 id="TransHuman-A-Transformer-based-Human-Representation-for-Generalizable-Neural-Human-Rendering"><a href="#TransHuman-A-Transformer-based-Human-Representation-for-Generalizable-Neural-Human-Rendering" class="headerlink" title="TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering"></a>TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12291">http://arxiv.org/abs/2307.12291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pansanity666/TransHuman">https://github.com/pansanity666/TransHuman</a></li>
<li>paper_authors: Xiao Pan, Zongxin Yang, Jianxin Ma, Chang Zhou, Yi Yang</li>
<li>for: 本研究强调的任务是通过 conditional Neural Radiance Fields (NeRF) 来实现一致的人类渲染，从多视图视频中训练 conditional NeRF 模型，以便在不同的人物上进行一致的渲染。</li>
<li>methods: 本研究使用了一种brand-new的框架，名为 TransHuman，它通过 Transformer-based Human Encoding (TransHE)、Deformable Partial Radiance Fields (DPaRF) 和 Fine-grained Detail Integration (FDI) 来学习涂抹 SMPL 图像，并捕捉人体部件之间的全局关系。</li>
<li>results: 经验表明，TransHuman 可以在 ZJU-MoCap 和 H36M 数据集上达到新的州态艺之绩，同时具有高效性。项目页面：<a target="_blank" rel="noopener" href="https://pansanity666.github.io/TransHuman/">https://pansanity666.github.io/TransHuman/</a><details>
<summary>Abstract</summary>
In this paper, we focus on the task of generalizable neural human rendering which trains conditional Neural Radiance Fields (NeRF) from multi-view videos of different characters. To handle the dynamic human motion, previous methods have primarily used a SparseConvNet (SPC)-based human representation to process the painted SMPL. However, such SPC-based representation i) optimizes under the volatile observation space which leads to the pose-misalignment between training and inference stages, and ii) lacks the global relationships among human parts that is critical for handling the incomplete painted SMPL. Tackling these issues, we present a brand-new framework named TransHuman, which learns the painted SMPL under the canonical space and captures the global relationships between human parts with transformers. Specifically, TransHuman is mainly composed of Transformer-based Human Encoding (TransHE), Deformable Partial Radiance Fields (DPaRF), and Fine-grained Detail Integration (FDI). TransHE first processes the painted SMPL under the canonical space via transformers for capturing the global relationships between human parts. Then, DPaRF binds each output token with a deformable radiance field for encoding the query point under the observation space. Finally, the FDI is employed to further integrate fine-grained information from reference images. Extensive experiments on ZJU-MoCap and H36M show that our TransHuman achieves a significantly new state-of-the-art performance with high efficiency. Project page: https://pansanity666.github.io/TransHuman/
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注通用神经人类渲染任务，这个任务是通过多视图视频来训练 conditional Neural Radiance Fields（NeRF）来生成不同人物的图像。为了处理人类动态运动，之前的方法主要使用了SparseConvNet（SPC）来表示人类，但这种SPC-based表示方式有两个问题：一是优化在不稳定的观察空间下，导致训练和执行阶段的姿势不一致；二是缺乏人类部分之间的全局关系，这是在处理部分SMPL的时候非常重要。为了解决这些问题，我们提出了一个全新的框架名为TransHuman，它在 canonical space 中学习涂抹 SMPL，并且 capture 人类部分之间的全局关系。TransHuman 的主要组成部分包括 Transformer-based Human Encoding（TransHE）、Deformable Partial Radiance Fields（DPaRF）和 Fine-grained Detail Integration（FDI）。TransHE 首先在 canonical space 中使用 transformers 处理涂抹 SMPL，以 capture 人类部分之间的全局关系。然后，DPaRF 将每个输出 Token 绑定到一个可变的辐射场，以在观察空间中编码查询点。最后，FDI 被使用来进一步 интеGRATE 细节信息。我们对 ZJU-MoCap 和 H36M 进行了广泛的实验，并证明了我们的 TransHuman 可以达到新的 estado del arte 性能，同时具有高效性。项目页面：https://pansanity666.github.io/TransHuman/
</details></li>
</ul>
<hr>
<h2 id="Downstream-agnostic-Adversarial-Examples"><a href="#Downstream-agnostic-Adversarial-Examples" class="headerlink" title="Downstream-agnostic Adversarial Examples"></a>Downstream-agnostic Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12280">http://arxiv.org/abs/2307.12280</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cgcl-codes/advencoder">https://github.com/cgcl-codes/advencoder</a></li>
<li>paper_authors: Ziqi Zhou, Shengshan Hu, Ruizhi Zhao, Qian Wang, Leo Yu Zhang, Junhui Hou, Hai Jin</li>
<li>for: 本研究旨在提出一个基于预训模型的攻击框架，可以对具有预训模型的下游任务进行 Universial Adversarial Examples 攻击。</li>
<li>methods: 本研究使用了高频率成分信息来引导生成攻击例子，然后设计了一个生成攻击框架，以学习攻击类别dataset的分布，以提高攻击成功率和传播性。</li>
<li>results: 研究结果显示，攻击者可以成功攻击下游任务，不需要知道预训dataset或下游dataset的详细信息。此外，研究者还提出了四种防护方法，其结果进一步证明了 AdvEncoder 的攻击能力。<details>
<summary>Abstract</summary>
Self-supervised learning usually uses a large amount of unlabeled data to pre-train an encoder which can be used as a general-purpose feature extractor, such that downstream users only need to perform fine-tuning operations to enjoy the benefit of "large model". Despite this promising prospect, the security of pre-trained encoder has not been thoroughly investigated yet, especially when the pre-trained encoder is publicly available for commercial use.   In this paper, we propose AdvEncoder, the first framework for generating downstream-agnostic universal adversarial examples based on the pre-trained encoder. AdvEncoder aims to construct a universal adversarial perturbation or patch for a set of natural images that can fool all the downstream tasks inheriting the victim pre-trained encoder. Unlike traditional adversarial example works, the pre-trained encoder only outputs feature vectors rather than classification labels. Therefore, we first exploit the high frequency component information of the image to guide the generation of adversarial examples. Then we design a generative attack framework to construct adversarial perturbations/patches by learning the distribution of the attack surrogate dataset to improve their attack success rates and transferability. Our results show that an attacker can successfully attack downstream tasks without knowing either the pre-training dataset or the downstream dataset. We also tailor four defenses for pre-trained encoders, the results of which further prove the attack ability of AdvEncoder.
</details>
<details>
<summary>摘要</summary>
自我监督学习通常使用大量未标注数据来预训练一个编码器，以便下游用户只需进行精细调整来获得“大型模型”的好处。然而，预训练编码器的安全性尚未得到全面的调查，尤其是在公开可用于商业用途的情况下。在这篇论文中，我们提出了 AdvEncoder，第一个基于预训练编码器的下游agnostic通用攻击示例生成框架。AdvEncoder的目标是为一组自然图像构建一个通用攻击杂音或贴图，可以让所有继承于受害者预训练编码器的下游任务受到攻击。与传统攻击示例工作不同，预训练编码器只输出图像特征 вектор而不是分类标签。因此，我们首先利用图像高频成分信息来引导攻击示例生成。然后，我们设计了一个生成攻击框架，通过学习攻击代理数据集的分布来提高攻击成功率和传输性。我们的结果表明，攻击者可以成功攻击下游任务，不需要知道预训练数据集或下游数据集。我们还适应四种防御措施 для预训练编码器，结果证明了 AdvEncoder 的攻击能力。
</details></li>
</ul>
<hr>
<h2 id="FDCT-Fast-Depth-Completion-for-Transparent-Objects"><a href="#FDCT-Fast-Depth-Completion-for-Transparent-Objects" class="headerlink" title="FDCT: Fast Depth Completion for Transparent Objects"></a>FDCT: Fast Depth Completion for Transparent Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12274">http://arxiv.org/abs/2307.12274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nonmy/fdct">https://github.com/nonmy/fdct</a></li>
<li>paper_authors: Tianan Li, Zhehan Chen, Huan Liu, Chen Wang</li>
<li>for: 这篇论文的目的是提出一种快速的深度完成框架，用于处理透明物体的RGB-D图像。</li>
<li>methods: 该方法使用了一种新的拼接分支和短circuit来抓取低级特征，并使用了一种损失函数来抑制过拟合。</li>
<li>results: 对比之前的方法，该方法可以在70帧&#x2F;秒的速度下提供更高精度的深度修复结果，并且可以改善物体抓取任务中的姿态估计。Here’s the full summary in Simplified Chinese:</li>
<li>for: 这篇论文的目的是提出一种快速的深度完成框架，用于处理透明物体的RGB-D图像。</li>
<li>methods: 该方法使用了一种新的拼接分支和短circuit来抓取低级特征，并使用了一种损失函数来抑制过拟合。</li>
<li>results: 对比之前的方法，该方法可以在70帧&#x2F;秒的速度下提供更高精度的深度修复结果，并且可以改善物体抓取任务中的姿态估计。I hope that helps!<details>
<summary>Abstract</summary>
Depth completion is crucial for many robotic tasks such as autonomous driving, 3-D reconstruction, and manipulation. Despite the significant progress, existing methods remain computationally intensive and often fail to meet the real-time requirements of low-power robotic platforms. Additionally, most methods are designed for opaque objects and struggle with transparent objects due to the special properties of reflection and refraction. To address these challenges, we propose a Fast Depth Completion framework for Transparent objects (FDCT), which also benefits downstream tasks like object pose estimation. To leverage local information and avoid overfitting issues when integrating it with global information, we design a new fusion branch and shortcuts to exploit low-level features and a loss function to suppress overfitting. This results in an accurate and user-friendly depth rectification framework which can recover dense depth estimation from RGB-D images alone. Extensive experiments demonstrate that FDCT can run about 70 FPS with a higher accuracy than the state-of-the-art methods. We also demonstrate that FDCT can improve pose estimation in object grasping tasks. The source code is available at https://github.com/Nonmy/FDCT
</details>
<details>
<summary>摘要</summary>
深度完成是许多 робо类任务的关键，如自动驾驶、3D重建和机械 manipulate。尽管已有很大的进步，现有方法仍然具有计算投入性和时间约束，并且大多数方法只适用于不透明物体，对于透明物体呈现特殊的反射和折射特性具有困难。为解决这些挑战，我们提出了高速深度完成框架 для透明物体（FDCT），该框架还有利于下游任务如对象 pose 估计。为了利用本地信息和避免过拟合问题，我们设计了新的融合分支和短cut 来利用低级特征，并设计了一个损失函数来抑制过拟合。这导致了一个准确和用户友好的深度修正框架，可以从RGB-D图像中恢复精密的深度估计。广泛的实验表明，FDCT 可以在70 FPS 下运行，并且与现状态艺术方法相比具有更高的准确率。我们还示出了 FDCT 可以改善对象抓取任务中的姿态估计。源代码可以在 <https://github.com/Nonmy/FDCT> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Context-Perception-Parallel-Decoder-for-Scene-Text-Recognition"><a href="#Context-Perception-Parallel-Decoder-for-Scene-Text-Recognition" class="headerlink" title="Context Perception Parallel Decoder for Scene Text Recognition"></a>Context Perception Parallel Decoder for Scene Text Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12270">http://arxiv.org/abs/2307.12270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Chenxia Li, Yuning Du, Yu-Gang Jiang</li>
<li>for: 这篇论文主要研究Scene Text Recognition（STR）方法的高精度和快速推断问题。</li>
<li>methods: 本文提出了一种新的AR模型，并进行了实验研究，发现AR模型的成功不仅归功于语言模型，还归功于视觉上下文的感知。为此，本文提出了Context Perception Parallel Decoder（CPPD）模型，通过计算字符出现频率和字符顺序来提供上下文信息，并且与字符预测任务结合，以准确地推断字符序列。</li>
<li>results: 实验结果表明，CPPD模型在英文和中文benchmark上达到了非常竞争性的准确率，并且比AR模型快约7倍。此外，CPPD模型也是当前最快的recognizer之一。代码将很快发布。<details>
<summary>Abstract</summary>
Scene text recognition (STR) methods have struggled to attain high accuracy and fast inference speed. Autoregressive (AR)-based STR model uses the previously recognized characters to decode the next character iteratively. It shows superiority in terms of accuracy. However, the inference speed is slow also due to this iteration. Alternatively, parallel decoding (PD)-based STR model infers all the characters in a single decoding pass. It has advantages in terms of inference speed but worse accuracy, as it is difficult to build a robust recognition context in such a pass. In this paper, we first present an empirical study of AR decoding in STR. In addition to constructing a new AR model with the top accuracy, we find out that the success of AR decoder lies also in providing guidance on visual context perception rather than language modeling as claimed in existing studies. As a consequence, we propose Context Perception Parallel Decoder (CPPD) to decode the character sequence in a single PD pass. CPPD devises a character counting module and a character ordering module. Given a text instance, the former infers the occurrence count of each character, while the latter deduces the character reading order and placeholders. Together with the character prediction task, they construct a context that robustly tells what the character sequence is and where the characters appear, well mimicking the context conveyed by AR decoding. Experiments on both English and Chinese benchmarks demonstrate that CPPD models achieve highly competitive accuracy. Moreover, they run approximately 7x faster than their AR counterparts, and are also among the fastest recognizers. The code will be released soon.
</details>
<details>
<summary>摘要</summary>
Scene文本识别（STR）方法一直受到高精度和快速推理速度的限制。基于排序（AR）的STR模型利用已经识别的字符来逐个解码下一个字符，其精度较高。然而，推理速度又相对较慢，主要因为这种迭代过程。相反，并行推理（PD）基于STR模型在单个推理过程中推理所有字符，它在推理速度方面具有优势，但精度相对较差，因为在这种情况下难以建立强大的识别Context。在这篇论文中，我们首先进行了AR推理在STR方面的实验研究。此外，我们还构建了一个新的AR模型，并发现AR推理的成功不仅归结于语言模型化，还需要VisualContext的感知。因此，我们提出了Context Perception并行推理器（CPPD），可以在单个PD过程中推理字符序列。CPPD包括字符出现频次预测模块和字符顺序预测模块。对于每个文本实例，前者预测每个字符的出现频次，而后者预测字符的顺序和占位符。与字符预测任务相结合，它们构建了一个Robust的识别Context，能够准确地表示文本序列和字符的位置，与AR推理准确相似。实验表明，CPPD模型在英文和中文标准套件上达到了非常竞争的精度水平，并且运行速度约为AR模型的7倍，同时也是推理速度最快的一部分。代码即将发布。
</details></li>
</ul>
<hr>
<h2 id="ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder"><a href="#ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder" class="headerlink" title="ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder"></a>ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12255">http://arxiv.org/abs/2307.12255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youzhi Liang, Wen Liang</li>
<li>for: 该文章的目的是提出一种轻量级和可靠的深度学习架构，用于解决小型互联网设备中的指纹图像噪声问题。</li>
<li>methods: 该架构基于差分卷积束的残差整合（Res-WCAE），包括两个Encoder和一个Decoder。image Encoder使用残差连接来保持细腻的空间特征，而wavelet Encoder使用卷积束来提取特征。</li>
<li>results: 对比多种现有方法，RES-WCAE在噪声水平较高的指纹图像权重级别上表现出优异性，特别是对于严重损坏的指纹图像。总的来说，RES-WCAE显示出了解决小型互联网设备中生物认证系统中的噪声问题的潜力。<details>
<summary>Abstract</summary>
The utilization of biometric authentication with pattern images is increasingly popular in compact Internet of Things (IoT) devices. However, the reliability of such systems can be compromised by image quality issues, particularly in the presence of high levels of noise. While state-of-the-art deep learning algorithms designed for generic image denoising have shown promise, their large number of parameters and lack of optimization for unique biometric pattern retrieval make them unsuitable for these devices and scenarios. In response to these challenges, this paper proposes a lightweight and robust deep learning architecture, the Residual Wavelet-Conditioned Convolutional Autoencoder (Res-WCAE) with a Kullback-Leibler divergence (KLD) regularization, designed specifically for fingerprint image denoising. Res-WCAE comprises two encoders - an image encoder and a wavelet encoder - and one decoder. Residual connections between the image encoder and decoder are leveraged to preserve fine-grained spatial features, where the bottleneck layer conditioned on the compressed representation of features obtained from the wavelet encoder using approximation and detail subimages in the wavelet-transform domain. The effectiveness of Res-WCAE is evaluated against several state-of-the-art denoising methods, and the experimental results demonstrate that Res-WCAE outperforms these methods, particularly for heavily degraded fingerprint images in the presence of high levels of noise. Overall, Res-WCAE shows promise as a solution to the challenges faced by biometric authentication systems in compact IoT devices.
</details>
<details>
<summary>摘要</summary>
互联网物联网（IoT）设备中的生物识别系统对于图像生物识别的使用越来越普遍。然而，这些系统的可靠性可能受到图像质量问题的影响，特别是在高水平的噪声存在下。现有的深度学习算法，设计用于普通图像数据的数据条件，即使在生物识别领域中显示了损害，但它们的参数数量过多，并且不适合专门适应生物识别图像的条件下进行数据条件。为了解决这些挑战，本文提出了一个轻量级和可靠的深度学习架构，即对应涡回图像条件的内积条件（KLD）调整的内积条件对称卷积数位对称（Res-WCAE）。Res-WCAE包括两个卷积数位：一个图像卷积数位和一个波лет卷积数位，以及一个解oder。内积条件组件使用对应涡回图像条件的条件对称卷积数位，以维持细节的空间特征。实验结果显示，Res-WCAE在多个州数据条件下与其他竞争方法进行比较，特别是在高水平的噪声存在下，Res-WCAE表现出色。总的来说，Res-WCAE具有优秀的应用潜力，用于生物识别系统中的图像数据条件。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Depression-Detection-via-Head-Motion-Patterns"><a href="#Explainable-Depression-Detection-via-Head-Motion-Patterns" class="headerlink" title="Explainable Depression Detection via Head Motion Patterns"></a>Explainable Depression Detection via Head Motion Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12241">http://arxiv.org/abs/2307.12241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Gahalawat, Raul Fernandez Rojas, Tanaya Guha, Ramanathan Subramanian, Roland Goecke</li>
<li>for: 检测抑郁症状</li>
<li>methods: 基于head motion数据的基本运动单元（kinemes）和机器学习方法</li>
<li>results: 头部运动模式是识别抑郁症状的有效标记，并且可以找到与先前发现的解释性kineme patrernsHere’s a more detailed explanation of each point:</li>
<li>for: The paper is written to detect depression using head motion data and machine learning methods.</li>
<li>methods: The paper uses two approaches to analyze head motion data: (a) discovering kinemes from head motion data of both depressed patients and healthy controls, and (b) learning kineme patterns only from healthy controls and computing statistics derived from reconstruction errors for both the patient and control classes. The paper employs machine learning methods to evaluate depression classification performance on two datasets: BlackDog and AVEC2013.</li>
<li>results: The paper finds that head motion patterns are effective biomarkers for detecting depressive symptoms, and that explanatory kineme patterns consistent with prior findings can be observed for the two classes. The paper achieves peak F1 scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 for binary classification over episodic thin-slices, and a peak F1 of 0.72 over videos for AVEC2013.<details>
<summary>Abstract</summary>
While depression has been studied via multimodal non-verbal behavioural cues, head motion behaviour has not received much attention as a biomarker. This study demonstrates the utility of fundamental head-motion units, termed \emph{kinemes}, for depression detection by adopting two distinct approaches, and employing distinctive features: (a) discovering kinemes from head motion data corresponding to both depressed patients and healthy controls, and (b) learning kineme patterns only from healthy controls, and computing statistics derived from reconstruction errors for both the patient and control classes. Employing machine learning methods, we evaluate depression classification performance on the \emph{BlackDog} and \emph{AVEC2013} datasets. Our findings indicate that: (1) head motion patterns are effective biomarkers for detecting depressive symptoms, and (2) explanatory kineme patterns consistent with prior findings can be observed for the two classes. Overall, we achieve peak F1 scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 for binary classification over episodic \emph{thin-slices}, and a peak F1 of 0.72 over videos for AVEC2013.
</details>
<details>
<summary>摘要</summary>
在识别抑郁症的研究中，脑部运动尚未得到过多的关注，作为生物标志。这项研究表明了基本头部运动单元（kinemes）在抑郁检测中的有用性，通过采用两种不同的方法和特征：（a）从头部运动数据中挖掘出kinemes，并对both depressed patients和健康Control进行比较；（b）从健康Control中学习kineme模式，并计算来自重建错误的统计，用于分类patient和control类。通过机器学习方法，我们评估了在BlackDog和AVEC2013 datasets上的抑郁分类性能。我们发现：（1）头部运动模式是有效的生物标志，可以检测抑郁症状；（2）可以看到与之前发现相符的解释性kineme模式。总的来说，我们在BlackDog和AVEC2013上取得了最高的F1分数为0.79和0.82，并在AVEC2013上取得了最高的F1分数为0.72。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dynamic-Query-Combinations-for-Transformer-based-Object-Detection-and-Segmentation"><a href="#Learning-Dynamic-Query-Combinations-for-Transformer-based-Object-Detection-and-Segmentation" class="headerlink" title="Learning Dynamic Query Combinations for Transformer-based Object Detection and Segmentation"></a>Learning Dynamic Query Combinations for Transformer-based Object Detection and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12239">http://arxiv.org/abs/2307.12239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bytedance/dq-det">https://github.com/bytedance/dq-det</a></li>
<li>paper_authors: Yiming Cui, Linjie Yang, Haichao Yu</li>
<li>for: 这种方法用于提高DETR基本模型的性能，包括对象检测、实例分割、精准分割和视频实例分割等多个任务。</li>
<li>methods: 该方法使用一个列表学习的检测查询来从变换器网络中提取信息，并学习预测图像中对象的位置和类别。然后，通过随机几何融合这些学习的查询来生成动态的融合查询，以更好地捕捉图像中对象的先前知识。</li>
<li>results: 通过使用我们的模ulated查询，DETR基本模型在多个任务上取得了一致和稳定的高性能。这些任务包括对象检测、实例分割、精准分割和视频实例分割等。<details>
<summary>Abstract</summary>
Transformer-based detection and segmentation methods use a list of learned detection queries to retrieve information from the transformer network and learn to predict the location and category of one specific object from each query. We empirically find that random convex combinations of the learned queries are still good for the corresponding models. We then propose to learn a convex combination with dynamic coefficients based on the high-level semantics of the image. The generated dynamic queries, named modulated queries, better capture the prior of object locations and categories in the different images. Equipped with our modulated queries, a wide range of DETR-based models achieve consistent and superior performance across multiple tasks including object detection, instance segmentation, panoptic segmentation, and video instance segmentation.
</details>
<details>
<summary>摘要</summary>
transformer-based 检测和分割方法使用一个学习的检测查询列表从 transformer 网络中获取信息并学习预测图像中的对象位置和类别。我们实际发现，随机几何的检测查询仍然可以为相应的模型提供好的性能。然后，我们提议通过图像的高级 semantics 学习动态的查询组合，称为modulated queries。这些生成的动态查询更好地捕捉图像中对象的先驱知识，使得各种基于 DE TR 的模型在多个任务上具有一致性和superior的表现，包括对象检测、实例分割、泛化分割和视频实例分割。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO"><a href="#Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO" class="headerlink" title="Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO"></a>Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12236">http://arxiv.org/abs/2307.12236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longxiang Zhang, Wenping Wang</li>
<li>for: 这种研究是为了评估在视频流处理中的游戏技能，以便向流服务提供者提供个性化推荐和服务促销。</li>
<li>methods: 该研究使用了最新的终端模型，以学习多Modalities的联合表示。在数据集中，研究人员首先识别数据集中的漏洞，然后手动清理数据。</li>
<li>results: 经过广泛的实验，研究人员证明了他们的提议的有效性。然而，研究人员还发现了他们的模型偏向用户标识而不是学习有意义的表示。<details>
<summary>Abstract</summary>
Online streaming is an emerging market that address much attention. Assessing gaming skills from videos is an important task for streaming service providers to discover talented gamers. Service providers require the information to offer customized recommendation and service promotion to their customers. Meanwhile, this is also an important multi-modal machine learning tasks since online streaming combines vision, audio and text modalities. In this study we begin by identifying flaws in the dataset and proceed to clean it manually. Then we propose several variants of latest end-to-end models to learn joint representation of multiple modalities. Through our extensive experimentation, we demonstrate the efficacy of our proposals. Moreover, we identify that our proposed models is prone to identifying users instead of learning meaningful representations. We purpose future work to address the issue in the end.
</details>
<details>
<summary>摘要</summary>
在线串流是一个崛起的市场，吸引了大量的注意力。从视频中评估玩家技巧是串流服务提供商们需要了解潜在的才华玩家的重要任务。服务提供商需要这些信息以为客户提供个性化推荐和服务促销。同时，这也是一个重要的多modal机器学习任务，因为在线串流结合了视觉、音频和文本模式。在这个研究中，我们开始通过手动清理数据集来发现问题，然后我们提出了多种最新的端到端模型，以学习多Modalities的联合表示。通过我们的广泛的实验，我们证明了我们的提议的效果。此外，我们发现我们的提议模型很容易被用户的身份识别，而不是学习有意义的表示。我们未来的工作是解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms"><a href="#EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms" class="headerlink" title="EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms"></a>EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12229">http://arxiv.org/abs/2307.12229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masoudmo/echoglad">https://github.com/masoudmo/echoglad</a></li>
<li>paper_authors: Masoud Mokhtari, Mobina Mahdavi, Hooman Vaseli, Christina Luong, Purang Abolmaesumi, Teresa S. M. Tsang, Renjie Liao</li>
<li>For: The paper aims to automate the task of detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle in the heart, using machine learning.* Methods: The proposed method uses an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection, which includes a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs, and induced hierarchical supervision at different levels of granularity using a multi-level loss.* Results: The paper achieves the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on two datasets under the in-distribution (ID) setting, and shows better out-of-distribution (OOD) generalization than prior works with a testing MAE of 4.3 mm.<details>
<summary>Abstract</summary>
The functional assessment of the left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle. The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss. We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.
</details>
<details>
<summary>摘要</summary>
左心室功能评估需要检测四个标志点和测量左心室内部维度以及周围肌肉的约重。难点在机器学习自动化这个任务是严重缺乏临床标注，即只有一些标注像素在高维度图像中，导致许多前作重要关注于均勋标注。然而，这种标注策略忽视图像的解剖信息并且带来一定偏见。为解决这个挑战，我们提出了一种用电子心室图像（EchoGLAD）来检测左心室标志点的模型。我们的主要贡献包括：1. 基于层次图像表示学习框架，通过层次graph neural network（GNN）实现多分辨率标志点检测。2. 通过多级监督，在不同的级别上实现层次监督。我们在公共和私人数据集上进行了测试，在内 distribuition（ID）和外部 distribuition（OOD）两种设置下。在ID设置下，我们实现了左心室标志点检测的状态机器学习（SOTA）的mean absolute error（MAE）为1.46mm和1.86mm。我们的模型还在OOD设置下表现更好，测试MAE为4.3mm。
</details></li>
</ul>
<hr>
<h2 id="The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery"><a href="#The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery" class="headerlink" title="The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery"></a>The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02502">http://arxiv.org/abs/2308.02502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Keith Wilkinson</li>
<li>For: The paper aims to investigate the use of artificial intelligence techniques and satellite imagery to identify illegal garbage dumps in rural areas of Cyprus.* Methods: The paper uses a novel dataset of images, data augmentation techniques, and an artificial neural network (specifically, a convolutional neural network) to recognize the presence or absence of garbage in new images.* Results: The resulting deep learning model can correctly identify images containing garbage in approximately 90% of cases, and could form the basis of a future system for systematically analyzing the entire landscape of Cyprus to build a comprehensive “garbage” map of the island.Here are the three points in Simplified Chinese text:* For: 这篇论文目标是使用人工智能技术和卫星影像来识别资产报废在cyprus的农村地区。* Methods: 论文使用了一个新的图像集，数据增强技术和人工神经网络来识别新图像中是否包含垃圾。* Results: 结果显示，使用这种方法可以在约90%的情况下正确地识别垃圾图像，并可能成为未来Cyprus岛上系统性地分析整个地图的基础。<details>
<summary>Abstract</summary>
Garbage disposal is a challenging problem throughout the developed world. In Cyprus, as elsewhere, illegal ``fly-tipping" is a significant issue, especially in rural areas where few legal garbage disposal options exist. However, there is a lack of studies that attempt to measure the scale of this problem, and few resources available to address it. A method of automating the process of identifying garbage dumps would help counter this and provide information to the relevant authorities. The aim of this study was to investigate the degree to which artificial intelligence techniques, together with satellite imagery, can be used to identify illegal garbage dumps in the rural areas of Cyprus. This involved collecting a novel dataset of images that could be categorised as either containing, or not containing, garbage. The collection of such datasets in sufficient raw quantities is time consuming and costly. Therefore a relatively modest baseline set of images was collected, then data augmentation techniques used to increase the size of this dataset to a point where useful machine learning could occur. From this set of images an artificial neural network was trained to recognise the presence or absence of garbage in new images. A type of neural network especially suited to this task known as ``convolutional neural networks" was used. The efficacy of the resulting model was evaluated using an independently collected dataset of test images. The result was a deep learning model that could correctly identify images containing garbage in approximately 90\% of cases. It is envisaged that this model could form the basis of a future system that could systematically analyse the entire landscape of Cyprus to build a comprehensive ``garbage" map of the island.
</details>
<details>
<summary>摘要</summary>
垃圾处理是发达国家的一个挑战，在塞浦路斯也是如此。非法投射（fly-tipping）是一个严重的问题，尤其在农村地区，因为有限的法定垃圾处理选择。然而，有很少的研究 Trying to measure the scale of this problem and few resources available to address it. This study aimed to investigate the extent to which artificial intelligence techniques, combined with satellite imagery, can be used to identify illegal garbage dumps in rural areas of Cyprus.To do this, we collected a novel dataset of images that could be categorized as either containing or not containing garbage. However, collecting such datasets in large quantities is time-consuming and costly, so we used data augmentation techniques to increase the size of our dataset to a point where useful machine learning could occur. We then trained an artificial neural network on this dataset to recognize the presence or absence of garbage in new images. We used a type of neural network well-suited to this task, called convolutional neural networks (CNNs).We evaluated the efficacy of our resulting model using an independently collected dataset of test images. The model was able to correctly identify images containing garbage in approximately 90% of cases. We envision this model as the basis for a future system that could systematically analyze the entire landscape of Cyprus to create a comprehensive "garbage" map of the island.
</details></li>
</ul>
<hr>
<h2 id="ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising"><a href="#ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising" class="headerlink" title="ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising"></a>ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12225">http://arxiv.org/abs/2307.12225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hao1635/ASCON">https://github.com/hao1635/ASCON</a></li>
<li>paper_authors: Zhihao Chen, Qi Gao, Yi Zhang, Hongming Shan</li>
<li>for: 低剂量 computed tomography（CT）静止图像减噪</li>
<li>methods: 提出了一种新的 Anatomy-aware Supervised CONtrastive learning框架（ASCON），可以利用静止图像的生物学信息进行减噪，同时提供生物学解释。</li>
<li>results: 对两个公共的低剂量 CT 静止图像减噪数据集进行了广泛的实验，并证明了 ASCON 的超过状态艺术模型的性能。另外，ASCON 提供了低剂量 CT 静止图像减噪中的生物学解释，这是首次实现的。<details>
<summary>Abstract</summary>
While various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, most of them leverage the normal-dose CT images as the ground-truth to supervise the denoising process. These methods typically ignore the inherent correlation within a single CT image, especially the anatomical semantics of human tissues, and lack the interpretability on the denoising process. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to the high-resolution input, an efficient ESAU-Net is introduced by using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models. Remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github.com/hao1635/ASCON.
</details>
<details>
<summary>摘要</summary>
而んどの深度学习方法已经被提出用于低剂量计算机断层成像（CT）去噪，大多数其中 leverages the normal-dose CT影像作为ground truth来监督去噪过程。这些方法通常忽略单个CT影像内的自然 correlation，特别是人体组织学的 semantics，而且缺乏去噪过程的解释性。在这篇论文中，我们提出了一种新的Anatomy-aware Supervised CONtrastive learning框架，称为ASCON，可以利用人体组织学来低剂量CT去噪，同时提供解释性。我们的ASCON包括两个新的设计：一种高效的自我注意力基于U-Net（ESAU-Net）和一种多尺度的组织学对比网络（MAC-Net）。首先，为了更好地捕捉全局-地方交互和适应高分辨率输入，我们使用了通道级别的自我注意力机制。其次，MAC-Net包括一个patch-wise非对比模块，用于捕捉内在的组织信息，以及一个像素级别的对比模块，用于保持内在的组织一致性。我们对公共的两个低剂量CT去噪数据集进行了广泛的实验，结果显示ASCON在状态机器上表现出了superior性。吸取onders, our ASCON为低剂量CT去噪提供了解释性，这是首次。源代码可以在https://github.com/hao1635/ASCON中获取。
</details></li>
</ul>
<hr>
<h2 id="LoLep-Single-View-View-Synthesis-with-Locally-Learned-Planes-and-Self-Attention-Occlusion-Inference"><a href="#LoLep-Single-View-View-Synthesis-with-Locally-Learned-Planes-and-Self-Attention-Occlusion-Inference" class="headerlink" title="LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference"></a>LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12217">http://arxiv.org/abs/2307.12217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cong Wang, Yu-Ping Wang, Dinesh Manocha</li>
<li>for: 本研究旨在提出一种新的方法，即LoLep，可以从单个RGB图像中推断高精度的场景表示，并生成更好的新视图。</li>
<li>methods: 为解决不具有深度信息的情况下推断合适的平面位置的问题，我们采用了预分 partition disparity 空间为 bins，并设计了一种disparity sampler来推断多个平面在每个bin中的本地偏移。此外，我们还提出了两种优化策略，其中一种是与不同的 disparity distribution 集合结合，另一种是在不同的 dataset 上添加 occlusion-aware reprojection loss 作为简单 yet effective 的 геометрической监视技术。</li>
<li>results: 我们的方法可以生成高精度的场景表示，并在不同的数据集上达到了领先的状态。与MINE相比，我们的方法具有 LPIPS 减少量为4.8%-9.0%和 RV 减少量为73.9%-83.5%。此外，我们还评估了实际图像上的性能，并证明了 LoLep 的优势。<details>
<summary>Abstract</summary>
We propose a novel method, LoLep, which regresses Locally-Learned planes from a single RGB image to represent scenes accurately, thus generating better novel views. Without the depth information, regressing appropriate plane locations is a challenging problem. To solve this issue, we pre-partition the disparity space into bins and design a disparity sampler to regress local offsets for multiple planes in each bin. However, only using such a sampler makes the network not convergent; we further propose two optimizing strategies that combine with different disparity distributions of datasets and propose an occlusion-aware reprojection loss as a simple yet effective geometric supervision technique. We also introduce a self-attention mechanism to improve occlusion inference and present a Block-Sampling Self-Attention (BS-SA) module to address the problem of applying self-attention to large feature maps. We demonstrate the effectiveness of our approach and generate state-of-the-art results on different datasets. Compared to MINE, our approach has an LPIPS reduction of 4.8%-9.0% and an RV reduction of 73.9%-83.5%. We also evaluate the performance on real-world images and demonstrate the benefits.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新方法，LoLep，该方法从单个RGB图像中回归地方精度的计划，以便更加准确地表示场景，并生成更好的新视图。在不知道深度信息的情况下，回归合适的平面位置是一个具有挑战性的问题。为解决这个问题，我们先对差分空间进行预分区，并设计了差分抽样器来回归多个平面在每个分区中的本地偏移。然而，只使用这种抽样器将网络训练不整合;我们还提出了两种优化策略，其中一种是基于不同差分分布的数据集的优化策略，另一种是一种简单 yet有效的干扰抑制损失技术。我们还引入了自注意机制，以改善干扰推断，并提出了块抽样自注意模块（BS-SA），以解决应用自注意到大特征地图时存在的问题。我们证明了我们的方法的有效性，并在不同的数据集上达到了领先的结果。相比于MINE，我们的方法有LPIPS减少4.8%-9.0%和RV减少73.9%-83.5%。我们还评估了实际图像上的性能，并证明了其利好。
</details></li>
</ul>
<hr>
<h2 id="LIST-Learning-Implicitly-from-Spatial-Transformers-for-Single-View-3D-Reconstruction"><a href="#LIST-Learning-Implicitly-from-Spatial-Transformers-for-Single-View-3D-Reconstruction" class="headerlink" title="LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction"></a>LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12194">http://arxiv.org/abs/2307.12194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Samiul Arshad, William J. Beksi</li>
<li>for: 用于重构3D对象的几何和 topological结构 from a single 2D图像</li>
<li>methods: 利用本地和全局图像特征，通过一种新的神经网络架构来准确重构3D对象的几何和 topological结构</li>
<li>results: 对比现有方法，本研究的模型能够更高精度地重构3D对象的几何和 topological结构，不需要摄像头估计或像素对齐。<details>
<summary>Abstract</summary>
Accurate reconstruction of both the geometric and topological details of a 3D object from a single 2D image embodies a fundamental challenge in computer vision. Existing explicit/implicit solutions to this problem struggle to recover self-occluded geometry and/or faithfully reconstruct topological shape structures. To resolve this dilemma, we introduce LIST, a novel neural architecture that leverages local and global image features to accurately reconstruct the geometric and topological structure of a 3D object from a single image. We utilize global 2D features to predict a coarse shape of the target object and then use it as a base for higher-resolution reconstruction. By leveraging both local 2D features from the image and 3D features from the coarse prediction, we can predict the signed distance between an arbitrary point and the target surface via an implicit predictor with great accuracy. Furthermore, our model does not require camera estimation or pixel alignment. It provides an uninfluenced reconstruction from the input-view direction. Through qualitative and quantitative analysis, we show the superiority of our model in reconstructing 3D objects from both synthetic and real-world images against the state of the art.
</details>
<details>
<summary>摘要</summary>
通过单个2D图像掌握3D物体的几何和拓扑细节是计算机视觉领域的基本挑战。现有的解决方案往往无法恢复自我遮盖的几何结构和/或准确地重建物体的拓扑形态。为解决这个问题，我们介绍了LIST，一种新的神经网络架构，它利用本地和全局图像特征来准确地从单个图像中重建3D物体的几何和拓扑结构。我们使用全局2D特征预测目标对象的抽象形状，然后使用它作为高分辨率重建的基础。通过利用图像中的本地特征和3D预测结果，我们可以使用隐式预测器来准确地预测目标表面上任意点的负距离。此外，我们的模型不需要相机估计或像素对齐。它可以从输入视图角度提供不受束缚的重建。通过质量和量化分析，我们证明了我们的模型在对真实图像和 sintetic图像进行3D物体重建时具有优势。
</details></li>
</ul>
<hr>
<h2 id="An-X3D-Neural-Network-Analysis-for-Runner’s-Performance-Assessment-in-a-Wild-Sporting-Environment"><a href="#An-X3D-Neural-Network-Analysis-for-Runner’s-Performance-Assessment-in-a-Wild-Sporting-Environment" class="headerlink" title="An X3D Neural Network Analysis for Runner’s Performance Assessment in a Wild Sporting Environment"></a>An X3D Neural Network Analysis for Runner’s Performance Assessment in a Wild Sporting Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12183">http://arxiv.org/abs/2307.12183</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Freire-Obregón, Javier Lorenzo-Navarro, Oliverio J. Santana, Daniel Hernández-Sosa, Modesto Castrillón-Santana</li>
<li>for: 这个研究是为了应用传输学习技术在运动环境中进行3D神经网络的分析。</li>
<li>methods: 该方法使用一个动作识别网络来估计长跑运动员的总赛时（CRT）。</li>
<li>results: 研究发现，使用X3D神经网络可以提供出色的表现，对于短视频输入， Mean Absolute Error为12分钟半。此外，X3D神经网络需要的内存比前一作少得多，可以达到更高的精度。<details>
<summary>Abstract</summary>
We present a transfer learning analysis on a sporting environment of the expanded 3D (X3D) neural networks. Inspired by action quality assessment methods in the literature, our method uses an action recognition network to estimate athletes' cumulative race time (CRT) during an ultra-distance competition. We evaluate the performance considering the X3D, a family of action recognition networks that expand a small 2D image classification architecture along multiple network axes, including space, time, width, and depth. We demonstrate that the resulting neural network can provide remarkable performance for short input footage, with a mean absolute error of 12 minutes and a half when estimating the CRT for runners who have been active from 8 to 20 hours. Our most significant discovery is that X3D achieves state-of-the-art performance while requiring almost seven times less memory to achieve better precision than previous work.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于转移学习的分析，涉及到扩展三维神经网络（X3D）环境中的运动环境。我们的方法使用动作识别网络来估算Runner在长跑比赛中的累累时间（CRT）。我们评估性能时考虑了X3D家族中的动作识别网络，该网络扩展了小于2D图像分类架构的维度，包括空间、时间、宽度和深度。我们表明，这种神经网络可以提供短输入视频时具有很好的性能，其中平均绝对误差为12分钟半，当估算 runner在8到20个小时内活动时。我们最重要的发现是，X3D可以达到现有最佳性能，而且需要只有七分之一的内存，以提高精度。
</details></li>
</ul>
<hr>
<h2 id="Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation"><a href="#Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation" class="headerlink" title="Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation"></a>Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12180">http://arxiv.org/abs/2307.12180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linzy0227/pdminet">https://github.com/linzy0227/pdminet</a></li>
<li>paper_authors: Yafei Zhang, Zhiyuan Li, Huafeng Li, Dapeng Tao</li>
<li>for: 这个论文的目的是提出一种多模态核磁共振（MR）脑肿瘤图像分割方法，以便更好地确定和地理位置脑肿瘤子区域。</li>
<li>methods: 该方法首先提取输入图像中的特征，然后使用脑肿瘤prototype来引导和融合不同模态特征，以便高亮每个脑肿瘤子区域的特征。</li>
<li>results: 实验结果表明，提出的方法在三个竞赛脑肿瘤分割数据集上具有更高的分割精度和稳定性。<details>
<summary>Abstract</summary>
For multi-modal magnetic resonance (MR) brain tumor image segmentation, current methods usually directly extract the discriminative features from input images for tumor sub-region category determination and localization. However, the impact of information aliasing caused by the mutual inclusion of tumor sub-regions is often ignored. Moreover, existing methods usually do not take tailored efforts to highlight the single tumor sub-region features. To this end, a multi-modal MR brain tumor segmentation method with tumor prototype-driven and multi-expert integration is proposed. It could highlight the features of each tumor sub-region under the guidance of tumor prototypes. Specifically, to obtain the prototypes with complete information, we propose a mutual transmission mechanism to transfer different modal features to each other to address the issues raised by insufficient information on single-modal features. Furthermore, we devise a prototype-driven feature representation and fusion method with the learned prototypes, which implants the prototypes into tumor features and generates corresponding activation maps. With the activation maps, the sub-region features consistent with the prototype category can be highlighted. A key information enhancement and fusion strategy with multi-expert integration is designed to further improve the segmentation performance. The strategy can integrate the features from different layers of the extra feature extraction network and the features highlighted by the prototypes. Experimental results on three competition brain tumor segmentation datasets prove the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
现有的多Modal MR脑肿吸引图像分割方法通常直接从输入图像中提取出特征，以确定和 lokalisieren tumor sub-region。然而，信息抖动所引起的影响通常被忽略。此外，现有的方法通常不会采取特化的努力来强调单个 tumor sub-region 的特征。为此，我们提出了一种基于 tumor 原型的多Modal MR脑肿吸引图像分割方法。它可以在 tumor 原型的指导下强调每个 tumor sub-region 的特征。具体来说，为了获得完整的信息，我们提出了一种互传机制，将不同模态特征传递给每个模态，以解决单modal特征不具备的问题。此外，我们还提出了一种基于原型的特征表示和融合方法，通过learned原型来嵌入 tumor 特征，并生成对应的活化地图。通过活化地图，可以强调与原型类别相符的子区域特征。为了进一步提高分割性能，我们还设计了一种多 экспер特性融合策略。该策略可以将不同层次的特征和由原型引导的特征融合在一起。实验结果表明，我们的提议方法在三个竞赛脑肿吸引图像分割数据集上具有优越性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Knowledge-Graphs-for-Zero-Shot-Object-agnostic-State-Classification"><a href="#Leveraging-Knowledge-Graphs-for-Zero-Shot-Object-agnostic-State-Classification" class="headerlink" title="Leveraging Knowledge Graphs for Zero-Shot Object-agnostic State Classification"></a>Leveraging Knowledge Graphs for Zero-Shot Object-agnostic State Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12179">http://arxiv.org/abs/2307.12179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filipos Gouidis, Theodore Patkos, Antonis Argyros, Dimitris Plexousakis</li>
<li>for: 本研究强调解决对象状态分类（OSC）问题，具体来说是一种零例学习问题，即不需要知道对象的类别来预测对象的状态。</li>
<li>methods: 我们提出了首个对象agnosticState Classification（OaSC）方法，即不需要对象类知识或估计来预测对象的状态。我们利用知识 graphs（KGs）来结构化和组织知识，并与视觉信息结合，以便在对象&#x2F;状态对的训练集中未经遭遇的情况下预测对象的状态。</li>
<li>results: 实验结果表明，对象类知识不是预测对象状态的决定因素。此外，我们的OaSC方法在所有数据集和benchmark中都超越了现有方法，差距很大。<details>
<summary>Abstract</summary>
We investigate the problem of Object State Classification (OSC) as a zero-shot learning problem. Specifically, we propose the first Object-agnostic State Classification (OaSC) method that infers the state of a certain object without relying on the knowledge or the estimation of the object class. In that direction, we capitalize on Knowledge Graphs (KGs) for structuring and organizing knowledge, which, in combination with visual information, enable the inference of the states of objects in object/state pairs that have not been encountered in the method's training set. A series of experiments investigate the performance of the proposed method in various settings, against several hypotheses and in comparison with state of the art approaches for object attribute classification. The experimental results demonstrate that the knowledge of an object class is not decisive for the prediction of its state. Moreover, the proposed OaSC method outperforms existing methods in all datasets and benchmarks by a great margin.
</details>
<details>
<summary>摘要</summary>
我们研究对象状态分类（OSC）问题，并将其视为零例学习问题。具体来说，我们提出了首个对象不依赖类别知识的对象状态分类方法（OaSC）。这种方法可以基于知识图（KGs）结构和组织知识，并结合视觉信息，对未在方法训练集中出现的对象/状态对进行状态推理。我们进行了一系列实验，测试方法在不同的设置、假设和现有的对象属性分类方法的比较中的性能。实验结果表明，对象类知识不是决定对象状态预测的关键因素。此外，我们的OaSC方法在所有数据集和标准准则上都超越了现有方法，差距很大。
</details></li>
</ul>
<hr>
<h2 id="Challenges-for-Monocular-6D-Object-Pose-Estimation-in-Robotics"><a href="#Challenges-for-Monocular-6D-Object-Pose-Estimation-in-Robotics" class="headerlink" title="Challenges for Monocular 6D Object Pose Estimation in Robotics"></a>Challenges for Monocular 6D Object Pose Estimation in Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12172">http://arxiv.org/abs/2307.12172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Thalhammer, Dominik Bauer, Peter Hönig, Jean-Baptiste Weibel, José García-Rodríguez, Markus Vincze</li>
<li>for: 本研究旨在探讨单视模式下的物体 pose 估算问题，即 robotics 应用中的核心识别任务。</li>
<li>methods: 该研究使用了现成的 RGB 感知器和 CNN 进行快速推理，以及提出了一种综合视角和数据集的评估方法。</li>
<li>results: 研究发现，虽然现有的多Modal 和单视方法已经达到了 estado del arte，但是它们在 robotics 应用中仍面临着 occlusion 处理、新的 pose 表示方法、Category-level pose estimation 的形式化和改进等挑战。此外，大量对象集、新型对象、干涉物质、不确定性估计等问题仍未得到解决。<details>
<summary>Abstract</summary>
Object pose estimation is a core perception task that enables, for example, object grasping and scene understanding. The widely available, inexpensive and high-resolution RGB sensors and CNNs that allow for fast inference based on this modality make monocular approaches especially well suited for robotics applications. We observe that previous surveys on object pose estimation establish the state of the art for varying modalities, single- and multi-view settings, and datasets and metrics that consider a multitude of applications. We argue, however, that those works' broad scope hinders the identification of open challenges that are specific to monocular approaches and the derivation of promising future challenges for their application in robotics. By providing a unified view on recent publications from both robotics and computer vision, we find that occlusion handling, novel pose representations, and formalizing and improving category-level pose estimation are still fundamental challenges that are highly relevant for robotics. Moreover, to further improve robotic performance, large object sets, novel objects, refractive materials, and uncertainty estimates are central, largely unsolved open challenges. In order to address them, ontological reasoning, deformability handling, scene-level reasoning, realistic datasets, and the ecological footprint of algorithms need to be improved.
</details>
<details>
<summary>摘要</summary>
We argue that there are still several fundamental challenges that need to be addressed in order to improve the performance of monocular object pose estimation in robotics. These challenges include occlusion handling, the development of novel pose representations, and the formalization and improvement of category-level pose estimation. Additionally, there are several central, largely unsolved open challenges that must be addressed, including the need for large object sets, the ability to handle novel objects and refractive materials, and the inclusion of uncertainty estimates.To address these challenges, we propose several areas of improvement, including ontological reasoning, deformability handling, scene-level reasoning, the creation of realistic datasets, and the reduction of the ecological footprint of algorithms. By focusing on these areas, we believe that the performance of monocular object pose estimation in robotics can be significantly improved, enabling more advanced and capable robots.
</details></li>
</ul>
<hr>
<h2 id="Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification"><a href="#Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification" class="headerlink" title="Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification"></a>Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12159">http://arxiv.org/abs/2307.12159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nícolas Barbosa Gomes, Arissa Yoshida, Mateus Roder, Guilherme Camargo de Oliveira, João Paulo Papa</li>
<li>for: 早期诊断阿LS（amyotrophic lateral sclerosis）有助于确定治疗的开始、改善病人的前途和整体健康状况。</li>
<li>methods: 该论文提出使用计算机方法分析病人的脸部表达来自动识别阿LS。</li>
<li>results: 实验结果表明，该方法在渥太华神经面数据集上表现出色，超过了现有的最佳成绩，为早期诊断阿LS带来了有前途的发展。<details>
<summary>Abstract</summary>
Identifying Amyotrophic Lateral Sclerosis (ALS) in its early stages is essential for establishing the beginning of treatment, enriching the outlook, and enhancing the overall well-being of those affected individuals. However, early diagnosis and detecting the disease's signs is not straightforward. A simpler and cheaper way arises by analyzing the patient's facial expressions through computational methods. When a patient with ALS engages in specific actions, e.g., opening their mouth, the movement of specific facial muscles differs from that observed in a healthy individual. This paper proposes Facial Point Graphs to learn information from the geometry of facial images to identify ALS automatically. The experimental outcomes in the Toronto Neuroface dataset show the proposed approach outperformed state-of-the-art results, fostering promising developments in the area.
</details>
<details>
<summary>摘要</summary>
早期诊断阿底士病（ALS）对患者的治疗结果有着重要的影响，能够提高生活质量和整体健康状况。然而，早期诊断和识别病种的症状不是一件容易的事情。这项研究提出使用计算机方法分析病人的面部表达来自动识别ALS。研究发现，当患者进行特定的动作时，如开口嘴巴，健康人的面部肌肉运动与ALS患者不同。该研究使用面部点Graph学习face图像的几何信息，并在渥太华神经面数据集上进行实验，结果表明该方法在比较当前的结果之上出色，这些结果表明了这种方法在诊断ALS方面的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices"><a href="#Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices" class="headerlink" title="Real-Time Neural Video Recovery and Enhancement on Mobile Devices"></a>Real-Time Neural Video Recovery and Enhancement on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12152">http://arxiv.org/abs/2307.12152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Zhaoyuan He, Yifan Yang, Lili Qiu, Kyoungjun Park</li>
<li>for: 提高移动设备上视频流式传输的流畅体验</li>
<li>methods: 提出一种新的视频帧恢复算法、一种新的超分辨率算法和一种接受器增强视频比特率调整算法</li>
<li>results: 实现了30帧&#x2F;秒的实时增强，在不同的网络环境下测试，实现了视频流经验质量（Quality of Experience，QoE）的显著提高（24%-82%）<details>
<summary>Abstract</summary>
As mobile devices become increasingly popular for video streaming, it's crucial to optimize the streaming experience for these devices. Although deep learning-based video enhancement techniques are gaining attention, most of them cannot support real-time enhancement on mobile devices. Additionally, many of these techniques are focused solely on super-resolution and cannot handle partial or complete loss or corruption of video frames, which is common on the Internet and wireless networks.   To overcome these challenges, we present a novel approach in this paper. Our approach consists of (i) a novel video frame recovery scheme, (ii) a new super-resolution algorithm, and (iii) a receiver enhancement-aware video bit rate adaptation algorithm. We have implemented our approach on an iPhone 12, and it can support 30 frames per second (FPS). We have evaluated our approach in various networks such as WiFi, 3G, 4G, and 5G networks. Our evaluation shows that our approach enables real-time enhancement and results in a significant increase in video QoE (Quality of Experience) of 24\% - 82\% in our video streaming system.
</details>
<details>
<summary>摘要</summary>
“随着移动设备在影像流媒体中的普及，实时优化影像流媒体的体验成为了非常重要的。深度学习基本的影像增强技术在获得注目，但大多数这些技术无法在移动设备上支持实时优化。此外，许多这些技术仅专注于超解析，而无法处理部分或完全的影像帧损失或腐败，这是互联网和无线网络上很常见的问题。”“为了解决这些挑战，我们在这篇论文中提出了一个新的方法。我们的方法包括：（i）一个新的影像帧恢复算法，（ii）一个新的超解析算法，以及（iii）一个受到接收端优化影像比特率改变算法的影像流媒体实时优化系统。我们在iPhone 12上实现了我们的方法，并且可以支持30帧每秒（FPS）。我们在WiFi、3G、4G和5G网络中进行了评估，我们的评估结果表明，我们的方法可以实现实时优化，并且导致影像流媒体系统中的影像质量经验（Quality of Experience，QoE）增加了24%-82%。”
</details></li>
</ul>
<hr>
<h2 id="Does-color-modalities-affect-handwriting-recognition-An-empirical-study-on-Persian-handwritings-using-convolutional-neural-networks"><a href="#Does-color-modalities-affect-handwriting-recognition-An-empirical-study-on-Persian-handwritings-using-convolutional-neural-networks" class="headerlink" title="Does color modalities affect handwriting recognition? An empirical study on Persian handwritings using convolutional neural networks"></a>Does color modalities affect handwriting recognition? An empirical study on Persian handwritings using convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12150">http://arxiv.org/abs/2307.12150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abbas Zohrevand, Zahra Imani, Javad Sadri, Ching Y. Suen</li>
<li>for: 这篇论文是 investigate whether color modalities of handwritten digits and words affect their recognition accuracy or speed.</li>
<li>methods: 使用 Convolutional Neural Networks (CNNs) 作为眼动模拟器，在一个新的波斯语手写数据库中进行测试。</li>
<li>results: 结果表明，使用 CNN 对黑白字体和字符图像进行训练后，对测试集进行识别时，比其他两种颜色模式具有更高的性能。然而，在不同颜色模式下的训练时间比较，发现使用 BW 图像进行训练是最效率的。<details>
<summary>Abstract</summary>
Most of the methods on handwritten recognition in the literature are focused and evaluated on Black and White (BW) image databases. In this paper we try to answer a fundamental question in document recognition. Using Convolutional Neural Networks (CNNs), as eye simulator, we investigate to see whether color modalities of handwritten digits and words affect their recognition accuracy or speed? To the best of our knowledge, so far this question has not been answered due to the lack of handwritten databases that have all three color modalities of handwritings. To answer this question, we selected 13,330 isolated digits and 62,500 words from a novel Persian handwritten database, which have three different color modalities and are unique in term of size and variety. Our selected datasets are divided into training, validation, and testing sets. Afterwards, similar conventional CNN models are trained with the training samples. While the experimental results on the testing set show that CNN on the BW digit and word images has a higher performance compared to the other two color modalities, in general there are no significant differences for network accuracy in different color modalities. Also, comparisons of training times in three color modalities show that recognition of handwritten digits and words in BW images using CNN is much more efficient.
</details>
<details>
<summary>摘要</summary>
大多数现成的手写识别方法都是在黑白图像库中进行研究和评估。在这篇论文中，我们试图回答一个基本的问题：使用卷积神经网络（CNN）作为眼动模拟器，我们调查了手写数字和字符的三种颜色模式是否影响了识别精度或速度？至于这个问题，我们认为现有的手写库缺乏三种颜色模式的手写数据，因此这个问题尚未得到解答。为了回答这个问题，我们选择了13330个隔离的数字和62500个字符从一个新的波斯语手写库中，这些数据库具有三种颜色模式和各种大小和样式。我们选择的数据库被分成了训练集、验证集和测试集。然后，我们使用同样的传统CNN模型在训练样本上进行训练。在测试集上的实验结果表明，使用CNN对黑白数字和字符图像进行识别的精度较高，而在其他两种颜色模式下的识别精度相对较低。此外，在三种颜色模式下的训练时间进行比较，发现使用黑白图像进行识别的训练时间相对较短。
</details></li>
</ul>
<hr>
<h2 id="Learned-Gridification-for-Efficient-Point-Cloud-Processing"><a href="#Learned-Gridification-for-Efficient-Point-Cloud-Processing" class="headerlink" title="Learned Gridification for Efficient Point Cloud Processing"></a>Learned Gridification for Efficient Point Cloud Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14354">http://arxiv.org/abs/2307.14354</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/computri/gridifier">https://github.com/computri/gridifier</a></li>
<li>paper_authors: Putri A. van der Linden, David W. Romero, Erik J. Bekkers</li>
<li>for: 将点云数据转换为可以进行操作的稠密grid数据，以提高操作效率和可扩展性。</li>
<li>methods: 提出了一种名为”learnable gridification”的方法，用于将点云数据转换为稠密grid数据，并在后续层使用常见的grid-based操作，如Conv3D。此外，还提出了一种名为”learnable de-gridification”的方法，用于将稠密grid数据还原回原始点云数据。</li>
<li>results: 经过理论和实验分析，显示了gridified网络在内存和时间方面的扩展性和可扩展性，而且可以达到竞争性的结果。<details>
<summary>Abstract</summary>
Neural operations that rely on neighborhood information are much more expensive when deployed on point clouds than on grid data due to the irregular distances between points in a point cloud. In a grid, on the other hand, we can compute the kernel only once and reuse it for all query positions. As a result, operations that rely on neighborhood information scale much worse for point clouds than for grid data, specially for large inputs and large neighborhoods.   In this work, we address the scalability issue of point cloud methods by tackling its root cause: the irregularity of the data. We propose learnable gridification as the first step in a point cloud processing pipeline to transform the point cloud into a compact, regular grid. Thanks to gridification, subsequent layers can use operations defined on regular grids, e.g., Conv3D, which scale much better than native point cloud methods. We then extend gridification to point cloud to point cloud tasks, e.g., segmentation, by adding a learnable de-gridification step at the end of the point cloud processing pipeline to map the compact, regular grid back to its original point cloud form. Through theoretical and empirical analysis, we show that gridified networks scale better in terms of memory and time than networks directly applied on raw point cloud data, while being able to achieve competitive results. Our code is publicly available at https://github.com/computri/gridifier.
</details>
<details>
<summary>摘要</summary>
神经操作需要邻域信息的时候在点云上比在网格数据上更加昂贵，因为点云中点的距离不规则。在网格上，我们可以一次计算核心，然后重复使用它们 для所有查询位置。因此，基于邻域信息的操作在点云上比网格数据更加慢速，尤其是对于大输入和大邻域。在这项工作中，我们解决了点云方法的扩展性问题，通过learned gridification来将点云转换成一个紧凑的、规则的网格。然后，我们扩展了gridification到点云到点云任务，例如分割，通过添加一个学习的de-gridification步骤来将紧凑的网格还原回原始点云形式。我们通过理论和实验分析表明，gridified网络在内存和时间方面比直接应用于原始点云数据更加高效，同时能够实现竞争性的结果。我们的代码可以在https://github.com/computri/gridifier上下载。
</details></li>
</ul>
<hr>
<h2 id="A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter"><a href="#A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter" class="headerlink" title="A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter"></a>A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12145">http://arxiv.org/abs/2307.12145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/river-lab/hyperspectral_macro_plastic_detection">https://github.com/river-lab/hyperspectral_macro_plastic_detection</a></li>
<li>paper_authors: Nathaniel Hanson, Ahmet Demirkaya, Deniz Erdoğmuş, Aron Stubbins, Taşkın Padır, Tales Imbiriba</li>
<li>for: 这个研究旨在解决水体中废弃 пласти克废弃物的监测问题，以提高当地生态和经济环境的健康性。</li>
<li>methods: 这个研究使用计算机成像技术来检测水体中的巨大废弃 пласти克废弃物。研究人员使用快照可见短波辐射 hyperspectral成像技术，并利用机器学习分类方法来实现高精度检测。</li>
<li>results: 实验结果表明，使用 hyperspectral 数据和非线性分类方法可以在具有挑战性的场景下实现高精度的检测精度，特别是在检测部分潜没的废弃 пласти克废弃物时。<details>
<summary>Abstract</summary>
Plastic waste entering the riverine harms local ecosystems leading to negative ecological and economic impacts. Large parcels of plastic waste are transported from inland to oceans leading to a global scale problem of floating debris fields. In this context, efficient and automatized monitoring of mismanaged plastic waste is paramount. To address this problem, we analyze the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios. We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging. Our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy even in challenging scenarios, especially when leveraging hyperspectral data and nonlinear classifiers. All code, data, and models are available online: https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection.
</details>
<details>
<summary>摘要</summary>
塑料垃圾进入河流环境会对当地生态系统造成负面影响，导致生态和经济 Both positive and negative impacts. Large amounts of plastic waste are transported from inland to oceans, causing a global problem of floating debris fields. In this context, efficient and automated monitoring of mismanaged plastic waste is crucial. To address this problem, we explore the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios. We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging. Our experiments show that imaging strategies combined with machine learning classification approaches can achieve high detection accuracy, especially in challenging scenarios, by leveraging hyperspectral data and nonlinear classifiers. All code, data, and models are available online at <https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection>.Here's the breakdown of the translation:* "塑料垃圾" (plastic waste) is translated as "塑料垃圾" (plastic waste)* "进入河流环境" (entering the riverine environment) is translated as "进入河流环境" (entering the riverine environment)* "会对当地生态系统造成负面影响" (leading to negative ecological and economic impacts) is translated as "会对当地生态系统造成负面影响" (leading to negative ecological and economic impacts)* "Large amounts of plastic waste are transported from inland to oceans" is translated as "大量塑料垃圾从内陆流入海洋" (large amounts of plastic waste are transported from inland to oceans)* "causing a global problem of floating debris fields" is translated as "导致全球漂泊垃圾场的问题" (causing a global problem of floating debris fields)* "In this context, efficient and automated monitoring of mismanaged plastic waste is crucial" is translated as "在这种情况下，有效和自动化的塑料垃圾监测是关键" (in this context, efficient and automated monitoring of mismanaged plastic waste is crucial)* "To address this problem, we explore the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios" is translated as "为解决这个问题，我们在河流类场景中explore了计算成像方法的可能性" (to address this problem, we explore the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios)* "We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging" is translated as "我们通过使用快照可见短波infrared hyperspectral成像来实现近实时跟踪部分浸没在水中的塑料" (we enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging)* "Our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy" is translated as "我们的实验表明，与机器学习分类方法相关的成像策略可以实现高的检测精度" (our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy)* "especially in challenging scenarios, by leveraging hyperspectral data and nonlinear classifiers" is translated as "特别是在挑战性的场景下，通过利用快照数据和非线性分类器来提高检测精度" (especially in challenging scenarios, by leveraging hyperspectral data and nonlinear classifiers)* "All code, data, and models are available online" is translated as "所有代码、数据和模型都可以在线获取" (all code, data, and models are available online)
</details></li>
</ul>
<hr>
<h2 id="SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images"><a href="#SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images" class="headerlink" title="SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images"></a>SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12138">http://arxiv.org/abs/2307.12138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueshen Li, Hongshan Liu, Xiaoyu Song, Brigitta C. Brott, Silvio H. Litovsky, Yu Gan</li>
<li>for: 对于摄影镜像胆管疾病的诊断和治疗提供虚拟 histological 信息</li>
<li>methods: 使用 transformer 生成数学类型的 GAN 模型，将 OCT 图像转换为虚拟显色 H&amp;E 厚比例图像</li>
<li>results: 实现了对摄影镜像胆管疾病的诊断和治疗中的虚拟 histological 信息生成，并且可以对应胆管疾病的病理特征Here’s the simplified Chinese text:</li>
<li>for: 为摄影镜像胆管疾病的诊断和治疗提供虚拟 histological 信息</li>
<li>methods: 使用 transformer 生成数学类型的 GAN 模型，将 OCT 图像转换为虚拟显色 H&amp;E 厚比例图像</li>
<li>results: 实现了对摄影镜像胆管疾病的诊断和治疗中的虚拟 histological 信息生成，并且可以对应胆管疾病的病理特征<details>
<summary>Abstract</summary>
There is a significant need for the generation of virtual histological information from coronary optical coherence tomography (OCT) images to better guide the treatment of coronary artery disease. However, existing methods either require a large pixel-wisely paired training dataset or have limited capability to map pathological regions. To address these issues, we proposed a structural constrained, pathology aware, transformer generative adversarial network, namely SCPAT-GAN, to generate virtual stained H&E histology from OCT images. The proposed SCPAT-GAN advances existing methods via a novel design to impose pathological guidance on structural layers using transformer-based network.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> coronary optical coherence tomography (OCT) 图像中的虚拟 histological 信息的生成很需要用于更好地治疗 coronary artery disease。然而，现有的方法 Either require a large pixel-wisely paired training dataset or have limited capability to map pathological regions。为解决这些问题，我们提出了一种基于 transformer 的 generator adversarial network，即 SCPAT-GAN，用于从 OCT 图像中生成虚拟染色 H&E  histology。我们的提议的 SCPAT-GAN 会使现有的方法得到一种新的设计，通过将 pathological guidance 添加到结构层以使用 transformer-based network。Here's the breakdown of the translation:*  coronary optical coherence tomography (OCT) 图像中的虚拟 histological 信息 (Virtual histological information in OCT images)* Either require a large pixel-wisely paired training dataset (Existing methods either require a large pixel-wisely paired training dataset)* or have limited capability to map pathological regions (or have limited capability to map pathological regions)* 用于更好地治疗 coronary artery disease (To better guide the treatment of coronary artery disease)* 基于 transformer 的 generator adversarial network (Based on transformer, a generator adversarial network)* 即 SCPAT-GAN (i.e., SCPAT-GAN)* 用于从 OCT 图像中生成虚拟染色 H&E  histology (To generate virtual stained H&E histology from OCT images)* 我们的提议的 SCPAT-GAN (Our proposed SCPAT-GAN)* 会使现有的方法得到一种新的设计 (Will make existing methods obtain a new design)* 通过将 pathological guidance 添加到结构层 (By adding pathological guidance to the structural layers)* 以使用 transformer-based network (To use transformer-based network)
</details></li>
</ul>
<hr>
<h2 id="Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks"><a href="#Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks" class="headerlink" title="Improving temperature estimation in low-cost infrared cameras using deep neural networks"></a>Improving temperature estimation in low-cost infrared cameras using deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12130">http://arxiv.org/abs/2307.12130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Nir Sochen, David Mendelovich, Iftach Klapp</li>
<li>for: 提高低成本热相机的温度精度和修正不均匀性</li>
<li>methods: 开发了一个考虑 ambient temperature 的非均匀性模拟器，并提出了一种基于批处理神经网络的方法，通过使用单个图像和摄像头自身测量的 ambient temperature 来估算对象的温度和修正不均匀性</li>
<li>results: 比前一些方法更低 ($1^\circ C$) 的mean temperature error，并且通过Physical constraint 降低了错误率 ($4%$)。 总的来说，mean temperature error 在广泛验证集上为 $0.37^\circ C$，并在实际场景中得到了等效的结果。<details>
<summary>Abstract</summary>
Low-cost thermal cameras are inaccurate (usually $\pm 3^\circ C$) and have space-variant nonuniformity across their detector. Both inaccuracy and nonuniformity are dependent on the ambient temperature of the camera. The main goal of this work was to improve the temperature accuracy of low-cost cameras and rectify the nonuniformity.   A nonuniformity simulator that accounts for the ambient temperature was developed. An end-to-end neural network that incorporates the ambient temperature at image acquisition was introduced. The neural network was trained with the simulated nonuniformity data to estimate the object's temperature and correct the nonuniformity, using only a single image and the ambient temperature measured by the camera itself. Results show that the proposed method lowered the mean temperature error by approximately $1^\circ C$ compared to previous works. In addition, applying a physical constraint on the network lowered the error by an additional $4\%$.   The mean temperature error over an extensive validation dataset was $0.37^\circ C$. The method was verified on real data in the field and produced equivalent results.
</details>
<details>
<summary>摘要</summary>
低成本热相机的精度受到 ambient temperature 的影响（通常在 $\pm 3^\circ C$ 之间），并且具有空间不均的非均匀性，这两个问题都与热相机的环境温度相关。本工作的主要目标是提高低成本热相机的温度精度和修正非均匀性。我们开发了一个考虑 ambient temperature 的非均匀性模拟器，并引入了一个结合 ambient temperature 的末端到终端神经网络。这个神经网络通过使用模拟的非均匀数据进行训练，以估算对象的温度并修正非均匀性，只需要使用单个图像和摄像头自带的温度值。结果显示，我们的方法可以比前一些工作降低mean温度错误约 $1^\circ C$。此外，通过应用物理约束，降低错误约 $4\%$。整体来说，我们的方法在广泛验证数据集上的mean温度错误为 $0.37^\circ C$。此方法还在实际场景中进行验证，并获得相同的结果。
</details></li>
</ul>
<hr>
<h2 id="InFusion-Inject-and-Attention-Fusion-for-Multi-Concept-Zero-Shot-Text-based-Video-Editing"><a href="#InFusion-Inject-and-Attention-Fusion-for-Multi-Concept-Zero-Shot-Text-based-Video-Editing" class="headerlink" title="InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing"></a>InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00135">http://arxiv.org/abs/2308.00135</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/infusion-zero-edit/InFusion">https://github.com/infusion-zero-edit/InFusion</a></li>
<li>paper_authors: Anant Khandelwal</li>
<li>for: 这篇论文的目的是提出一个框架，以便透过文本提示进行类型控制的视频编辑，并且在不需要训练的情况下实现高品质的视频编辑。</li>
<li>methods: 这篇论文使用了大型预训的文本扩散模型，并且提出了一个内部插入（Injection）的方法，允许在视频中编辑多个概念，并且具有像素级的控制。</li>
<li>results: 论文的实验结果显示，这个框架可以实现高品质和时间含意的视频编辑，并且可以与现有的图像扩散技术进行整合。<details>
<summary>Abstract</summary>
Large text-to-image diffusion models have achieved remarkable success in generating diverse, high-quality images. Additionally, these models have been successfully leveraged to edit input images by just changing the text prompt. But when these models are applied to videos, the main challenge is to ensure temporal consistency and coherence across frames. In this paper, we propose InFusion, a framework for zero-shot text-based video editing leveraging large pre-trained image diffusion models. Our framework specifically supports editing of multiple concepts with pixel-level control over diverse concepts mentioned in the editing prompt. Specifically, we inject the difference in features obtained with source and edit prompts from U-Net residual blocks of decoder layers. When these are combined with injected attention features, it becomes feasible to query the source contents and scale edited concepts along with the injection of unedited parts. The editing is further controlled in a fine-grained manner with mask extraction and attention fusion, which cut the edited part from the source and paste it into the denoising pipeline for the editing prompt. Our framework is a low-cost alternative to one-shot tuned models for editing since it does not require training. We demonstrated complex concept editing with a generalised image model (Stable Diffusion v1.5) using LoRA. Adaptation is compatible with all the existing image diffusion techniques. Extensive experimental results demonstrate the effectiveness of existing methods in rendering high-quality and temporally consistent videos.
</details>
<details>
<summary>摘要</summary>
大型文本到图像扩散模型已经实现了惊人的成功，可以生成多样化、高质量的图像。此外，这些模型还可以通过修改输入文本来编辑图像。但是在视频 editing 中，主要挑战是保证时间协调一致和各帧的一致性。在这篇论文中，我们提出了 InFusion 框架，一种基于大型预训练的图像扩散模型来实现零搅 diffusion 的文本基于视频编辑。我们的框架具有多个概念编辑、像素级控制的特点，可以在编辑提示中提出多个概念，并且可以在精细化的方式下控制编辑。具体来说，我们将源和编辑提示中的特征差拟合到 U-Net 径弧层的解码层中，并与注入的注意力特征相结合，使得可以查询源内容并将编辑概念扩大到不变的部分。此外，我们还使用抽取Mask和注意力融合来进一步控制编辑。我们的框架是一种低成本的替代方案，不需要训练。我们使用了 Stable Diffusion v1.5 总体图像模型进行复杂概念编辑，并证明了与现有图像扩散技术相容。我们的实验结果表明，InFusion 可以生成高质量和时间协调一致的视频。
</details></li>
</ul>
<hr>
<h2 id="Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network"><a href="#Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network" class="headerlink" title="Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network"></a>Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12122">http://arxiv.org/abs/2307.12122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/octadion/diffusion-stylegan2-ada-pytorch">https://github.com/octadion/diffusion-stylegan2-ada-pytorch</a></li>
<li>paper_authors: One Octadion, Novanto Yudistira, Diva Kurnianingtyas<br>for:本研究旨在帮助 batik 设计师或手工艺术家创造独特和高品质的 batik 模样，并实现有效率的生产时间和成本。methods:本研究使用了 StyleGAN2-Ada 和扩散技术，实现了生成高品质和真实的 synthetic batik 模样。 StyleGAN2-Ada 是一种分离 Style 和 Content 两个方面的 GAN 模型，而扩散技术则引入了随机噪音到数据中。results:根据质量和量度评估，模型测试过程中生成的 batik 模样具有细节丰富和艺术多样性，并且能够实现有效率的生产时间和成本。<details>
<summary>Abstract</summary>
Batik, a unique blend of art and craftsmanship, is a distinct artistic and technological creation for Indonesian society. Research on batik motifs is primarily focused on classification. However, further studies may extend to the synthesis of batik patterns. Generative Adversarial Networks (GANs) have been an important deep learning model for generating synthetic data, but often face challenges in the stability and consistency of results. This research focuses on the use of StyleGAN2-Ada and Diffusion techniques to produce realistic and high-quality synthetic batik patterns. StyleGAN2-Ada is a variation of the GAN model that separates the style and content aspects in an image, whereas diffusion techniques introduce random noise into the data. In the context of batik, StyleGAN2-Ada and Diffusion are used to produce realistic synthetic batik patterns. This study also made adjustments to the model architecture and used a well-curated batik dataset. The main goal is to assist batik designers or craftsmen in producing unique and quality batik motifs with efficient production time and costs. Based on qualitative and quantitative evaluations, the results show that the model tested is capable of producing authentic and quality batik patterns, with finer details and rich artistic variations. The dataset and code can be accessed here:https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details>
<details>
<summary>摘要</summary>
《独特的抽象艺术——batik的研究》batik是印度尼西亚社会独特的艺术和手工艺术材料，研究主要集中在纹理的分类。然而，进一步的研究可能会扩展到纹理的合成。生成对抗网络（GANs）是深度学习模型，用于生成合成数据，但经常面临稳定性和一致性问题。本研究使用StyleGAN2-Ada和扩散技术生成真实和高质量的合成纹理图案。StyleGAN2-Ada是GAN模型中分离风格和内容的变种，而扩散技术引入随机噪声到数据中。在batik中，StyleGAN2-Ada和扩散被用生成真实的合成纹理图案。本研究还对模型结构进行了调整，使用了优化的batik数据集。主要目标是协助batik设计师或手工艺术家生成独特和高质量的纹理图案，减少生产时间和成本。根据质量和量度评价，结果显示，试用的模型能够生成authentic和高质量的纹理图案，细节更加细腻，艺术变化更加丰富。数据集和代码可以在以下链接获取：https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details></li>
</ul>
<hr>
<h2 id="Pyramid-Semantic-Graph-based-Global-Point-Cloud-Registration-with-Low-Overlap"><a href="#Pyramid-Semantic-Graph-based-Global-Point-Cloud-Registration-with-Low-Overlap" class="headerlink" title="Pyramid Semantic Graph-based Global Point Cloud Registration with Low Overlap"></a>Pyramid Semantic Graph-based Global Point Cloud Registration with Low Overlap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12116">http://arxiv.org/abs/2307.12116</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-aerial-robotics/pagor">https://github.com/hkust-aerial-robotics/pagor</a></li>
<li>paper_authors: Zhijian Qiao, Zehuan Yu, Huan Yin, Shaojie Shen</li>
<li>for: 这篇论文是关于全球点云注册的，用于绕过视点变化和 occlusion 等问题，以实现 loop closing 和 relocalization。</li>
<li>methods: 该论文提出了一种基于图论的全球点云注册方法，使用了 robust 的数据关联和可靠的姿态估计，以及semantic 信息来减少点云数据的精度。</li>
<li>results: 实验结果表明，该方法在自行收集的indoor数据集和公共的 KITTI 数据集上具有最高成功率，即使点云之间的重叠率低、semantic质量低。代码已经开源在 GitHub 上（<a target="_blank" rel="noopener" href="https://github.com/HKUST-Aerial-Robotics/Pagor">https://github.com/HKUST-Aerial-Robotics/Pagor</a>）。<details>
<summary>Abstract</summary>
Global point cloud registration is essential in many robotics tasks like loop closing and relocalization. Unfortunately, the registration often suffers from the low overlap between point clouds, a frequent occurrence in practical applications due to occlusion and viewpoint change. In this paper, we propose a graph-theoretic framework to address the problem of global point cloud registration with low overlap. To this end, we construct a consistency graph to facilitate robust data association and employ graduated non-convexity (GNC) for reliable pose estimation, following the state-of-the-art (SoTA) methods.   Unlike previous approaches, we use semantic cues to scale down the dense point clouds, thus reducing the problem size. Moreover, we address the ambiguity arising from the consistency threshold by constructing a pyramid graph with multi-level consistency thresholds. Then we propose a cascaded gradient ascend method to solve the resulting densest clique problem and obtain multiple pose candidates for every consistency threshold. Finally, fast geometric verification is employed to select the optimal estimation from multiple pose candidates. Our experiments, conducted on a self-collected indoor dataset and the public KITTI dataset, demonstrate that our method achieves the highest success rate despite the low overlap of point clouds and low semantic quality. We have open-sourced our code https://github.com/HKUST-Aerial-Robotics/Pagor for this project.
</details>
<details>
<summary>摘要</summary>
To achieve this, we construct a consistency graph to facilitate robust data association and employ graduated non-convexity (GNC) for reliable pose estimation, following state-of-the-art (SoTA) methods. Unlike previous approaches, we use semantic cues to scale down the dense point clouds, reducing the problem size. Additionally, we address the ambiguity arising from the consistency threshold by constructing a pyramid graph with multi-level consistency thresholds.We then propose a cascaded gradient ascend method to solve the resulting densest clique problem and obtain multiple pose candidates for every consistency threshold. Finally, fast geometric verification is employed to select the optimal estimation from multiple pose candidates. Our experiments, conducted on a self-collected indoor dataset and the public KITTI dataset, show that our method achieves the highest success rate despite the low overlap of point clouds and low semantic quality. We have open-sourced our code at https://github.com/HKUST-Aerial-Robotics/Pagor for this project.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.CV_2023_07_23/" data-id="clorjzl5v00fkf18894g4b8ej" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.AI_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T12:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.AI_2023_07_23/">cs.AI - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations"><a href="#Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations" class="headerlink" title="Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?"></a>Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12344">http://arxiv.org/abs/2307.12344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ss-sun/right-for-the-wrong-reason">https://github.com/ss-sun/right-for-the-wrong-reason</a></li>
<li>paper_authors: Susu Sun, Lisa M. Koch, Christian F. Baumgartner</li>
<li>for: 本研究旨在评估不同半结构化解释技术的检测潜在的假 correlate 能力。</li>
<li>methods: 本研究使用了 five 种post-hoc解释技术和一种自然语言解释技术来检测在胸部X射影诊断任务中 искусificially添加的三种干扰因素。</li>
<li>results: 研究发现，使用 SHAP 技术和自然语言解释技术 Attri-Net 可以准确地检测到胸部X射影诊断中的假 correlate，并且这些技术可以被用来可靠地检测模型的异常行为。<details>
<summary>Abstract</summary>
While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net provide the best performance and can be used to reliably identify faulty model behavior.
</details>
<details>
<summary>摘要</summary>
深度神经网络模型具有无可比的分类性能，但它们容易学习潜在的 correlate 信息。这些依赖于潜在信息的关系可能难以通过性能指标来探测，尤其如果测试数据来自同一个分布。可解释的机器学习方法，如后期解释或内置可解释的分类器，承诺可以识别模型的错误思维。然而，有混合的证据表明许多这些技术是否能够做到这一点。在这篇论文中，我们提出了一种严格的评估策略，用于评估一种解释技术的能力 Correctly 识别人工添加的三种隐藏因素在胸部X射影诊断任务中。我们发现，使用 SHAP 和 Attri-Net 的 Post-hoc 解释技术，以及内置可解释的 Attri-Net 可以准确地识别模型的错误行为。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generic-and-Controllable-Attacks-Against-Object-Detection"><a href="#Towards-Generic-and-Controllable-Attacks-Against-Object-Detection" class="headerlink" title="Towards Generic and Controllable Attacks Against Object Detection"></a>Towards Generic and Controllable Attacks Against Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12342">http://arxiv.org/abs/2307.12342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liguopeng0923/LGP">https://github.com/liguopeng0923/LGP</a></li>
<li>paper_authors: Guopeng Li, Yue Xu, Jian Ding, Gui-Song Xia</li>
<li>for: 这个论文的目的是设计一种可控的攻击方法来攻击主流的物件探测器（Object Detectors，OD），以实现对OD的攻击。</li>
<li>methods: 这个论文使用了一种称为LGP（Local Perturbations with Adaptively Global Attacks）的白盒攻击方法，它可以对OD进行攻击，并且可以控制攻击的方向和大小。LGP使用了高品质的提案和三种不同的损失函数来实现攻击。</li>
<li>results: 实验结果显示，LGP可以成功攻击十六种主流的物件探测器，包括MS-COCO和DOTA datasets。此外，LGP也可以实现了不可见和传递性的攻击。codes可以在<a target="_blank" rel="noopener" href="https://github.com/liguopeng0923/LGP.git%E4%B8%AD%E5%8F%96%E5%BE%97%E3%80%82">https://github.com/liguopeng0923/LGP.git中取得。</a><details>
<summary>Abstract</summary>
Existing adversarial attacks against Object Detectors (ODs) suffer from two inherent limitations. Firstly, ODs have complicated meta-structure designs, hence most advanced attacks for ODs concentrate on attacking specific detector-intrinsic structures, which makes it hard for them to work on other detectors and motivates us to design a generic attack against ODs. Secondly, most works against ODs make Adversarial Examples (AEs) by generalizing image-level attacks from classification to detection, which brings redundant computations and perturbations in semantically meaningless areas (e.g., backgrounds) and leads to an emergency for seeking controllable attacks for ODs. To this end, we propose a generic white-box attack, LGP (local perturbations with adaptively global attacks), to blind mainstream object detectors with controllable perturbations. For a detector-agnostic attack, LGP tracks high-quality proposals and optimizes three heterogeneous losses simultaneously. In this way, we can fool the crucial components of ODs with a part of their outputs without the limitations of specific structures. Regarding controllability, we establish an object-wise constraint that exploits foreground-background separation adaptively to induce the attachment of perturbations to foregrounds. Experimentally, the proposed LGP successfully attacked sixteen state-of-the-art object detectors on MS-COCO and DOTA datasets, with promising imperceptibility and transferability obtained. Codes are publicly released in https://github.com/liguopeng0923/LGP.git
</details>
<details>
<summary>摘要</summary>
现有的对象检测器（OD）的敌对攻击受到两种内在的限制。首先，OD有复杂的元结构设计，因此大多数对OD的高级攻击都是针对特定的检测器结构，这使得它们难以在其他检测器上工作，激励我们设计一种通用的攻击方法。其次，大多数对OD的攻击是将图像级别的攻击扩展到检测，这会带来 redundant computations和在意义不明的区域（如背景）中的扰动，从而导致对OD的攻击控制不足。为此，我们提出了一种通用的白盒攻击方法——本地加工攻击（LGP），可以让主流的对象检测器感受到控制的扰动。为了实现检测器无关的攻击，LGP跟踪高质量的提案并同时优化三种多元损失。这样，我们可以使对OD的核心组件的输出中的一部分被扰动，而不是特定的结构。在控制性方面，我们建立了一种对象强制约束，以便随时 Adaptively 调整扰动的分布。实验结果表明，我们提出的LGP方法成功地击败了MS-COCO和DOTA数据集上十六个状态对象检测器，并且在透明度和传输性方面取得了惊人的成绩。代码在https://github.com/liguopeng0923/LGP.git中公开发布。
</details></li>
</ul>
<hr>
<h2 id="Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks"><a href="#Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks" class="headerlink" title="Tackling the Curse of Dimensionality with Physics-Informed Neural Networks"></a>Tackling the Curse of Dimensionality with Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12306">http://arxiv.org/abs/2307.12306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, Kenji Kawaguchi</li>
<li>for: 解决高维精度数学模型的计算问题，提高计算效率和可扩展性。</li>
<li>methods: 使用Stochastic Dimension Gradient Descent（SDGD）方法，即将梯度分解成不同维度的部分，然后随机选择这些维度的部分进行每次训练。</li>
<li>results: 能够快速解决许多困难高维度精度数学问题，如 Hamilton-Jacobi-Bellman 方程和Schrödinger方程，并且可以在单个GPU上进行计算。例如，在100,000维度中解决了一个非线性HJB方程和一个Black-Scholes方程，并且在6小时内完成了计算。<details>
<summary>Abstract</summary>
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed method. We experimentally demonstrate that the proposed method allows us to solve many notoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman (HJB) and the Schr\"{o}dinger equations in thousands of dimensions very fast on a single GPU using the PINNs mesh-free approach. For instance, we solve nontrivial nonlinear PDEs (one HJB equation and one Black-Scholes equation) in 100,000 dimensions in 6 hours on a single GPU using SDGD with PINNs. Since SDGD is a general training methodology of PINNs, SDGD can be applied to any current and future variants of PINNs to scale them up for arbitrary high-dimensional PDEs.
</details>
<details>
<summary>摘要</summary>
科学研究中的困难（CoD）会使计算资源受到极大的挑战，计算成本随着维度的增加而呈指数增长。这在60多年前，理查德·贝尔曼首次提出的高维度 partial differential equations（PDEs）解决问题中 pose great challenges. Although there has been some recent success in numerically solving PDEs in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved.在这篇论文中，我们开发了一种新的方法，即随机维度加速（SDGD），用于扩展物理学信息神经网络（PINNs）解决任意高维度 PDEs。SDGD方法将 PDE 的梯度分解成不同维度的部分，然后在训练 PINNs 时随机选择这些维度的部分。我们证明了该方法的收敛保证和其他愿望的性质。我们通过实验表明，使用 SDGD 方法可以很快地解决许多知名度很高的高维度 PDEs，例如 Hamilton-Jacobi-Bellman 方程和 Schrödinger 方程在千个维度中。例如，我们在6个小时内使用单个 GPU 解决了一个非线性 HJB 方程和一个黑色-肖勒斯方程在10,000个维度中。由于 SDGD 是一种通用的 PINNs 训练方法，因此可以应用于任何当前和未来的 PINNs 变体，以扩展它们的应用范围。
</details></li>
</ul>
<hr>
<h2 id="Controller-Synthesis-for-Timeline-based-Games"><a href="#Controller-Synthesis-for-Timeline-based-Games" class="headerlink" title="Controller Synthesis for Timeline-based Games"></a>Controller Synthesis for Timeline-based Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12289">http://arxiv.org/abs/2307.12289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renato Acampora, Luca Geatti, Nicola Gigante, Angelo Montanari, Valentino Picotti</li>
<li>for: 这篇论文旨在提供一种有效和计算优化的控制器生成方法来解决时间轴基于游戏中的游戏策略问题。</li>
<li>methods: 该论文使用的方法包括时间轴基于游戏和控制器生成。</li>
<li>results: 该论文提出的控制器生成方法可以有效地解决时间轴基于游戏中的游戏策略问题，并且computational complexity是2EXPTIME-complete。<details>
<summary>Abstract</summary>
In the timeline-based approach to planning, the evolution over time of a set of state variables (the timelines) is governed by a set of temporal constraints. Traditional timeline-based planning systems excel at the integration of planning with execution by handling temporal uncertainty. In order to handle general nondeterminism as well, the concept of timeline-based games has been recently introduced. It has been proved that finding whether a winning strategy exists for such games is 2EXPTIME-complete. However, a concrete approach to synthesize controllers implementing such strategies is missing. This paper fills this gap, by providing an effective and computationally optimal approach to controller synthesis for timeline-based games.
</details>
<details>
<summary>摘要</summary>
在时间轴基本方法中，时间变量集（时间轴）的演化遵循一组时间约束。传统的时间轴基本方法具有融合规划与执行的能力，可以处理时间不确定性。为了处理通用非决定性，时间轴基本方法中的游戏概念被最近引入。已证明找到赢家策略的存在是2EXPTIME-完善。但是，实现这种策略的控制器合成方法缺失。这篇论文填补了这个空白，提供了有效和计算优化的控制器合成方法 для时间轴基本方法中的游戏。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Adaptive-Formation-via-Consensus-Oriented-Multi-Agent-Communication"><a href="#Decentralized-Adaptive-Formation-via-Consensus-Oriented-Multi-Agent-Communication" class="headerlink" title="Decentralized Adaptive Formation via Consensus-Oriented Multi-Agent Communication"></a>Decentralized Adaptive Formation via Consensus-Oriented Multi-Agent Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12287">http://arxiv.org/abs/2307.12287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuming Xiang, Sizhao Li, Rongpeng Li, Zhifeng Zhao, Honggang Zhang</li>
<li>for: 这 paper 的目的是提出一种适应多 аген特式控制方法，以适应多 аген系统中 agent 的数量变化，并在有限通信环境下实现高速稳定的formation control.</li>
<li>methods: 该 paper 使用了一种新的 multi-agent reinforcement learning 方法，即 consensus-oriented multi-agent communication (ConsMAC)，以使 agents 能够 perceive 全局信息并在本地状态下达成 consensus。此外，paper 还使用了 policy distillation 来实现 Adaptive formation adjustment.</li>
<li>results: 实验结果表明，提出的方法在高速稳定性方面具有出色的表现，并且可以快速适应多 аген系统中 agent 的数量变化。<details>
<summary>Abstract</summary>
Adaptive multi-agent formation control, which requires the formation to flexibly adjust along with the quantity variations of agents in a decentralized manner, belongs to one of the most challenging issues in multi-agent systems, especially under communication-limited constraints. In this paper, we propose a novel Consensus-based Decentralized Adaptive Formation (Cons-DecAF) framework. Specifically, we develop a novel multi-agent reinforcement learning method, Consensus-oriented Multi-Agent Communication (ConsMAC), to enable agents to perceive global information and establish the consensus from local states by effectively aggregating neighbor messages. Afterwards, we leverage policy distillation to accomplish the adaptive formation adjustment. Meanwhile, instead of pre-assigning specific positions of agents, we employ a displacement-based formation by Hausdorff distance to significantly improve the formation efficiency. The experimental results through extensive simulations validate that the proposed method has achieved outstanding performance in terms of both speed and stability.
</details>
<details>
<summary>摘要</summary>
《适应多智能体formation控制》是多智能体系统中最为复杂的问题之一，尤其是在有限通信环境下。在这篇论文中，我们提出了一种新的Consensus-based Decentralized Adaptive Formation（Cons-DecAF）框架。具体来说，我们开发了一种新的多智能体学习方法——Consensus-oriented Multi-Agent Communication（ConsMAC），使智能体能够从本地状态中获得全局信息并达成一致。接着，我们通过策略填充来实现形态调整。而不是先行指定智能体的具体位置，我们employs a displacement-based formation by Hausdorff distance，以大幅提高形态效率。实验结果表明，我们提出的方法在速度和稳定性两个方面具有出色的表现。
</details></li>
</ul>
<hr>
<h2 id="Towards-Automatic-Boundary-Detection-for-Human-AI-Collaborative-Hybrid-Essay-in-Education"><a href="#Towards-Automatic-Boundary-Detection-for-Human-AI-Collaborative-Hybrid-Essay-in-Education" class="headerlink" title="Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education"></a>Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12267">http://arxiv.org/abs/2307.12267</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/douglashiwo/BoundaryDetectionFromHybridText">https://github.com/douglashiwo/BoundaryDetectionFromHybridText</a></li>
<li>paper_authors: Zijie Zeng, Lele Sha, Yuheng Li, Kaixun Yang, Dragan Gašević, Guanliang Chen</li>
<li>for: 本研究旨在探讨AI生成文本检测在 hybrid 文本中的应用，即检测人类和生成LLMs共同写作的文本中的AI生成部分。</li>
<li>methods: 本研究提出了一种两步方法，包括在encoder训练过程中分离AI生成内容和人类写作内容，然后计算每两个相邻的聚合函数间的距离，并假设存在两个聚合函数间最远的距离处的边界。</li>
<li>results: 实验结果表明，提出的方法在不同的实验设置下 consistently 超过基eline方法的性能，并且 encoder 训练过程可以明显提高方法的性能。 在检测单边 hybrid 文本中的边界时，可以采用一定的 prototype 大小来进一步提高方法的性能，升师22% 在 Domain 评估中和18% 在 Out-of-Domain 评估中。<details>
<summary>Abstract</summary>
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary detection). Then we proposed a two-step approach where we (1) separated AI-generated content from human-written content during the encoder training process; and (2) calculated the distances between every two adjacent prototypes and assumed that the boundaries exist between the two adjacent prototypes that have the furthest distance from each other. Through extensive experiments, we observed the following main findings: (1) the proposed approach consistently outperformed the baseline methods across different experiment settings; (2) the encoder training process can significantly boost the performance of the proposed approach; (3) when detecting boundaries for single-boundary hybrid essays, the proposed approach could be enhanced by adopting a relatively large prototype size, leading to a 22% improvement in the In-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.
</details>
<details>
<summary>摘要</summary>
Recent large language models (LLMs), such as ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. However, educators have concerns that students may use LLMs to complete their writing assignments and pass them off as their own work. To address this issue, many AI content detection studies have been conducted, but most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated.In this study, we investigated AI content detection in a realistic setting where the text to be detected is collaboratively written by humans and generative LLMs (hybrid text). We formalized the detection task as identifying the transition points between human-written content and AI-generated content in a given hybrid text (boundary detection).To solve this problem, we proposed a two-step approach:1. Separate AI-generated content from human-written content during the encoder training process.2. Calculate the distances between every two adjacent prototypes and assume that the boundaries exist between the two adjacent prototypes with the furthest distance from each other.Through extensive experiments, we found the following main findings:1. Our proposed approach consistently outperformed baseline methods across different experiment settings.2. The encoder training process can significantly boost the performance of our proposed approach.3. When detecting boundaries for single-boundary hybrid essays, our proposed approach can be enhanced by adopting a relatively large prototype size, leading to a 22% improvement in the In-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.
</details></li>
</ul>
<hr>
<h2 id="Building-road-Collaborative-Extraction-from-Remotely-Sensed-Images-via-Cross-Interaction"><a href="#Building-road-Collaborative-Extraction-from-Remotely-Sensed-Images-via-Cross-Interaction" class="headerlink" title="Building-road Collaborative Extraction from Remotely Sensed Images via Cross-Interaction"></a>Building-road Collaborative Extraction from Remotely Sensed Images via Cross-Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12256">http://arxiv.org/abs/2307.12256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haonan Guo, Xin Su, Chen Wu, Bo Du, Liangpei Zhang</li>
<li>for: 本研究旨在提高从高分辨率遥感图像中提取建筑和道路的精度和效率，通过建筑和道路之间的协同抽取方法。</li>
<li>methods: 本研究提出了一种基于多任务和跨比例特征互动的建筑-道路共同抽取方法，通过交互信息 между任务和自适应收发场所来提高每个任务的准确性。</li>
<li>results: 实验表明，提出的方法可以在各种城市和农村场景下实现出色的建筑-道路抽取性能和效率。<details>
<summary>Abstract</summary>
Buildings are the basic carrier of social production and human life; roads are the links that interconnect social networks. Building and road information has important application value in the frontier fields of regional coordinated development, disaster prevention, auto-driving, etc. Mapping buildings and roads from very high-resolution (VHR) remote sensing images have become a hot research topic. However, the existing methods often ignore the strong spatial correlation between roads and buildings and extract them in isolation. To fully utilize the complementary advantages between buildings and roads, we propose a building-road collaborative extraction method based on multi-task and cross-scale feature interaction to improve the accuracy of both tasks in a complementary way. A multi-task interaction module is proposed to interact information across tasks and preserve the unique information of each task, which tackle the seesaw phenomenon in multitask learning. By considering the variation in appearance and structure between buildings and roads, a cross-scale interaction module is designed to automatically learn the optimal reception field for different tasks. Compared with many existing methods that train each task individually, the proposed collaborative extraction method can utilize the complementary advantages between buildings and roads by the proposed inter-task and inter-scale feature interactions, and automatically select the optimal reception field for different tasks. Experiments on a wide range of urban and rural scenarios show that the proposed algorithm can achieve building-road extraction with outstanding performance and efficiency.
</details>
<details>
<summary>摘要</summary>
建筑和路径是社会生产和人类生活的基础载体，路径是社会网络之间的连接。建筑和路径信息在前沿领域的区域协调发展、灾害预防、自动驾驶等领域有重要应用价值。从very high-resolution（VHR）Remote sensing图像中提取建筑和路径信息已成为热点研究话题。然而，现有方法 часто忽略了道路和建筑之间的强相关性，单独提取它们。为了充分利用建筑和道路之间的补做优势，我们提议一种建筑道路共同提取方法，基于多任务和跨比例特征互动来提高两个任务的准确率。我们提出的多任务互动模块可以在不同任务之间交换信息，保持每个任务的独特信息，解决多任务学习中的摇摆现象。通过考虑建筑和道路之间的外观和结构变化，我们设计了一种跨比例互动模块，自动学习不同任务的最佳接收频率。与许多现有方法不同，我们的共同提取方法可以利用建筑和道路之间的补做优势，通过我们提出的交互特征互动和自动选择最佳接收频率，提高两个任务的准确率。在各种都市和农村场景下进行了广泛的实验，我们的算法可以实现出色的建筑道路提取和高效率。
</details></li>
</ul>
<hr>
<h2 id="Nature-and-the-Machines"><a href="#Nature-and-the-Machines" class="headerlink" title="Nature and the Machines"></a>Nature and the Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04440">http://arxiv.org/abs/2308.04440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/face-analysis/emonet">https://github.com/face-analysis/emonet</a></li>
<li>paper_authors: Huw Price, Matthew Connolly</li>
<li>for: 这篇论文是关于人工智能（AI）对人类是否 pose existential risk 的问题，而critics认为这个问题 receiving 太多关注，希望把其推迟到一边，Focus on 当前 AI poses 的风险。</li>
<li>methods: 论文使用的方法是 argument 和 persuasion，指出 Nature 期刊在这个问题上的错误判断，并阐述了 AI 的风险和 Its potential consequences.</li>
<li>results: 论文的结论是，Nature 期刊的 error  judgement 是 serious，因为 AI 的风险不仅是今天的问题，也是未来的问题。论文 argues that we should not ignore the potential risks of AI, but instead, we should consider the consequences of error and take appropriate measures to mitigate them.<details>
<summary>Abstract</summary>
Does artificial intelligence (AI) pose existential risks to humanity? Some critics feel this question is getting too much attention, and want to push it aside in favour of conversations about the immediate risks of AI. These critics now include the journal Nature, where a recent editorial urges us to 'stop talking about tomorrow's AI doomsday when AI poses risks today.' We argue that this is a serious failure of judgement, on Nature's part. In science, as in everyday life, we expect influential actors to consider the consequences of error. As the world's leading scientific journal, Nature is certainly an influential actor, especially so in the absence of robust global regulation of AI. Yet it has manifestly failed to consider the cost of error in this case.
</details>
<details>
<summary>摘要</summary>
人类是否面临人工智能（AI）的极大风险？一些批评者认为这个问题Receiving too much attention，希望把其推迟到一Side和讨论当前AI的风险。这些批评者现在包括《Nature》杂志，其latest editorial呼吁我们“停止讨论明天的AIArmageddon，因为AI今天已经存在风险”。我们认为，《Nature》在这个问题上manifestly failed to consider the cost of error。In science, as in everyday life, we expect influential actors to consider the consequences of error. As the world's leading scientific journal, Nature is certainly an influential actor, especially so in the absence of robust global regulation of AI. Yet it has manifestly failed to consider the cost of error in this case.
</details></li>
</ul>
<hr>
<h2 id="MARS-Exploiting-Multi-Level-Parallelism-for-DNN-Workloads-on-Adaptive-Multi-Accelerator-Systems"><a href="#MARS-Exploiting-Multi-Level-Parallelism-for-DNN-Workloads-on-Adaptive-Multi-Accelerator-Systems" class="headerlink" title="MARS: Exploiting Multi-Level Parallelism for DNN Workloads on Adaptive Multi-Accelerator Systems"></a>MARS: Exploiting Multi-Level Parallelism for DNN Workloads on Adaptive Multi-Accelerator Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12234">http://arxiv.org/abs/2307.12234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guan Shen, Jieru Zhao, Zeke Wang, Zhe Lin, Wenchao Ding, Chentao Wu, Quan Chen, Minyi Guo</li>
<li>for: 这个研究旨在提出一个名为 MARS 的新的映射框架，用于在多个加速器系统中选择适当的加速器设计，并对 DNN 进行有效的分割策略，以最大化并行度。</li>
<li>methods: 这个研究使用了 computation-aware 加速器选择策略和 communication-aware 分割策略，以提高 DNN 的执行效率。</li>
<li>results: 实验结果显示，MARS 可以在常见的 DNN 负载中实现32.2% 的延迟增加，并在多核心模型中实现59.4% 的延迟增加，较基eline方法和相关的现有方法更高。<details>
<summary>Abstract</summary>
Along with the fast evolution of deep neural networks, the hardware system is also developing rapidly. As a promising solution achieving high scalability and low manufacturing cost, multi-accelerator systems widely exist in data centers, cloud platforms, and SoCs. Thus, a challenging problem arises in multi-accelerator systems: selecting a proper combination of accelerators from available designs and searching for efficient DNN mapping strategies. To this end, we propose MARS, a novel mapping framework that can perform computation-aware accelerator selection, and apply communication-aware sharding strategies to maximize parallelism. Experimental results show that MARS can achieve 32.2% latency reduction on average for typical DNN workloads compared to the baseline, and 59.4% latency reduction on heterogeneous models compared to the corresponding state-of-the-art method.
</details>
<details>
<summary>摘要</summary>
“深度神经网络的快速演化，硬件系统也在快速发展。作为数据中心、云平台和SoC中高扩展性低生产成本的解决方案，多加速器系统广泛存在。因此，多加速器系统中的一个挑战是选择合适的加速器设计，并搜索高度平行的DNN映射策略。为此，我们提出了MARS，一种新的映射框架，可以实现计算意识加速器选择，以及通信意识分割策略，以最大化并行性。实验结果显示，MARS可以相对基eline方法平均减少32.2%的延迟，对于常见的DNN任务，并且相对相对国际先进方法，对于多元模型，可以减少59.4%的延迟。”
</details></li>
</ul>
<hr>
<h2 id="Geometry-Aware-Adaptation-for-Pretrained-Models"><a href="#Geometry-Aware-Adaptation-for-Pretrained-Models" class="headerlink" title="Geometry-Aware Adaptation for Pretrained Models"></a>Geometry-Aware Adaptation for Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12226">http://arxiv.org/abs/2307.12226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Roberts, Xintong Li, Dyah Adila, Sonia Cromp, Tzu-Heng Huang, Jitian Zhao, Frederic Sala<br>for:The paper is written to improve the performance of machine learning models, specifically zero-shot models, in predicting new classes without any additional training.methods:The proposed approach, called Loki, uses a simple technique to adapt the trained model to predict new classes by swapping the standard prediction rule with the Fréchet mean. The approach is a drop-in replacement and does not require any additional training data.results:The paper shows that Loki achieves up to 29.7% relative improvement over SimCLR on ImageNet and scales to hundreds of thousands of classes. When no external metric is available, Loki can use self-derived metrics from class embeddings and obtains a 10.5% improvement on pretrained zero-shot models such as CLIP.<details>
<summary>Abstract</summary>
Machine learning models -- including prominent zero-shot models -- are often trained on datasets whose labels are only a small proportion of a larger label space. Such spaces are commonly equipped with a metric that relates the labels via distances between them. We propose a simple approach to exploit this information to adapt the trained model to reliably predict new classes -- or, in the case of zero-shot prediction, to improve its performance -- without any additional training. Our technique is a drop-in replacement of the standard prediction rule, swapping argmax with the Fr\'echet mean. We provide a comprehensive theoretical analysis for this approach, studying (i) learning-theoretic results trading off label space diameter, sample complexity, and model dimension, (ii) characterizations of the full range of scenarios in which it is possible to predict any unobserved class, and (iii) an optimal active learning-like next class selection procedure to obtain optimal training classes for when it is not possible to predict the entire range of unobserved classes. Empirically, using easily-available external metrics, our proposed approach, Loki, gains up to 29.7% relative improvement over SimCLR on ImageNet and scales to hundreds of thousands of classes. When no such metric is available, Loki can use self-derived metrics from class embeddings and obtains a 10.5% improvement on pretrained zero-shot models such as CLIP.
</details>
<details>
<summary>摘要</summary>
机器学习模型 -- 包括知名的零 shot 模型 -- 常常在具有小 proportion 的标签空间上进行训练。这些空间通常具有一个将标签相关的度量。我们提议一种简单的方法，利用这些信息来适应训练后的模型，以可靠地预测新类 -- 或在零 shot 预测情况下，提高其性能 -- 无需进一步训练。我们的技术是将 argmax 替换为 Fréchet 平均。我们提供了完整的理论分析，包括（i）交互学习理论，考虑标签空间径、样本复杂度和模型维度之间的负反关系，（ii）对预测任何未知类的全范围情况的Characterizations，以及（iii）用于获取优化训练类的优化活动学习类选择方法。Empirically, 我们的提议方法，Loki，在 ImageNet 上对 SimCLR 进行了29.7% 相对提升，并可扩展到数万个类。当没有外部度量时，Loki 可以使用自己计算出的类嵌入度量，并在预测零 shot 模型 CLIP 时获得10.5% 的提升。
</details></li>
</ul>
<hr>
<h2 id="FATRER-Full-Attention-Topic-Regularizer-for-Accurate-and-Robust-Conversational-Emotion-Recognition"><a href="#FATRER-Full-Attention-Topic-Regularizer-for-Accurate-and-Robust-Conversational-Emotion-Recognition" class="headerlink" title="FATRER: Full-Attention Topic Regularizer for Accurate and Robust Conversational Emotion Recognition"></a>FATRER: Full-Attention Topic Regularizer for Accurate and Robust Conversational Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12221">http://arxiv.org/abs/2307.12221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ludybupt/FATRER">https://github.com/ludybupt/FATRER</a></li>
<li>paper_authors: Yuzhao Mao, Di Lu, Xiaojie Wang, Yang Zhang</li>
<li>for: 这 paper 的目的是理解对话中的情感引起的情绪。</li>
<li>methods: 这 paper 使用了一种基于全注意力话题规范的情绪识别器，以提高模型的Robustness 和准确性。</li>
<li>results: 实验显示，这 paper 的模型比现有模型更好地抵抗三种 adversarial 攻击，并且在情绪识别 tasks 上得到了更高的效果。<details>
<summary>Abstract</summary>
This paper concentrates on the understanding of interlocutors' emotions evoked in conversational utterances. Previous studies in this literature mainly focus on more accurate emotional predictions, while ignoring model robustness when the local context is corrupted by adversarial attacks. To maintain robustness while ensuring accuracy, we propose an emotion recognizer augmented by a full-attention topic regularizer, which enables an emotion-related global view when modeling the local context in a conversation. A joint topic modeling strategy is introduced to implement regularization from both representation and loss perspectives. To avoid over-regularization, we drop the constraints on prior distributions that exist in traditional topic modeling and perform probabilistic approximations based entirely on attention alignment. Experiments show that our models obtain more favorable results than state-of-the-art models, and gain convincing robustness under three types of adversarial attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Expediting-Building-Footprint-Segmentation-from-High-resolution-Remote-Sensing-Images-via-progressive-lenient-supervision"><a href="#Expediting-Building-Footprint-Segmentation-from-High-resolution-Remote-Sensing-Images-via-progressive-lenient-supervision" class="headerlink" title="Expediting Building Footprint Segmentation from High-resolution Remote Sensing Images via progressive lenient supervision"></a>Expediting Building Footprint Segmentation from High-resolution Remote Sensing Images via progressive lenient supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12220">http://arxiv.org/abs/2307.12220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haonanguo/bfseg-efficient-building-footprint-segmentation-framework">https://github.com/haonanguo/bfseg-efficient-building-footprint-segmentation-framework</a></li>
<li>paper_authors: Haonan Guo, Bo Du, Chen Wu, Xin Su, Liangpei Zhang</li>
<li>for: 本研究旨在提高从遥感图像中提取建筑印迹的效率和准确率。</li>
<li>methods: 本文提出了一种高效的建筑印迹分割框架，包括一种精密的均匀连接粗细度特征融合网络，以及一种宽泛的深度监督和采样策略。</li>
<li>results: 根据实验结果，提出的建筑印迹分割框架可以高效地提取建筑印迹，并且可以在不同的encoder网络上达到出色的性能和效率。<details>
<summary>Abstract</summary>
The efficacy of building footprint segmentation from remotely sensed images has been hindered by model transfer effectiveness. Many existing building segmentation methods were developed upon the encoder-decoder architecture of U-Net, in which the encoder is finetuned from the newly developed backbone networks that are pre-trained on ImageNet. However, the heavy computational burden of the existing decoder designs hampers the successful transfer of these modern encoder networks to remote sensing tasks. Even the widely-adopted deep supervision strategy fails to mitigate these challenges due to its invalid loss in hybrid regions where foreground and background pixels are intermixed. In this paper, we conduct a comprehensive evaluation of existing decoder network designs for building footprint segmentation and propose an efficient framework denoted as BFSeg to enhance learning efficiency and effectiveness. Specifically, a densely-connected coarse-to-fine feature fusion decoder network that facilitates easy and fast feature fusion across scales is proposed. Moreover, considering the invalidity of hybrid regions in the down-sampled ground truth during the deep supervision process, we present a lenient deep supervision and distillation strategy that enables the network to learn proper knowledge from deep supervision. Building upon these advancements, we have developed a new family of building segmentation networks, which consistently surpass prior works with outstanding performance and efficiency across a wide range of newly developed encoder networks. The code will be released on https://github.com/HaonanGuo/BFSeg-Efficient-Building-Footprint-Segmentation-Framework.
</details>
<details>
<summary>摘要</summary>
“ remote sensing 图像中的建筑印迹分 segmentation 的效果受到模型传递效果的限制。现有的建筑分 segmentation 方法大多基于encoder-decoder架构的 U-Net，其中 encoder 是从新开发的背景网络中精度调整的。但是，现有的 decoder 设计带来了大量计算负担，使得现代 encoder 网络不能成功传递到 remote sensing 任务中。即使广泛采用的深度监测策略也无法缓解这些挑战，因为它在混合区域中的无效损失。在这篇论文中，我们对现有 decoder 网络设计进行了全面的评估，并提出了一种高效的框架 denoted as BFSeg，以提高学习效率和效果。具体来说，我们提出了一种紧密连接的粗细尺度混合特征卷积网络，以便轻松地实现特征之间的快速混合。此外，我们认为在下采样的真实图像中的混合区域无效的损失问题，我们提出了一种宽松的深度监测和采样策略，使得网络能够从深度监测中学习正确的知识。基于这些进步，我们开发了一个新的建筑分 segmentation 网络家族，这些网络在各种新开发的 encoder 网络上表现出色，在效率和性能方面都超越了先前的工作。代码将在 https://github.com/HaonanGuo/BFSeg-Efficient-Building-Footprint-Segmentation-Framework 上发布。”
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-and-Systematic-Analysis-of-Artificial-Intelligence-Regulation-Policies"><a href="#A-Comprehensive-Review-and-Systematic-Analysis-of-Artificial-Intelligence-Regulation-Policies" class="headerlink" title="A Comprehensive Review and Systematic Analysis of Artificial Intelligence Regulation Policies"></a>A Comprehensive Review and Systematic Analysis of Artificial Intelligence Regulation Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12218">http://arxiv.org/abs/2307.12218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyue Wu, Shaoshan Liu</li>
<li>for: This paper aims to help governing bodies understand and regulate AI technologies in a chaotic global regulatory space.</li>
<li>methods: The paper presents a comprehensive review of AI regulation proposals from different geographical locations and cultural backgrounds, and develops a framework for analyzing these proposals.</li>
<li>results: The paper performs a systematic analysis of AI regulation proposals to identify potential failures and provide insights for governing bodies to untangle the AI regulatory chaos.In Simplified Chinese text, the three key points would be:</li>
<li>for: 这篇论文目标是帮助管理机构理解和调控全球AI regulatory空间中的混乱。</li>
<li>methods: 论文首先提供了AI regulatory proposal的全面回顾，然后开发了一个分析这些提案的框架。</li>
<li>results: 论文进行了系统性的AI regulatory proposal分析，以便发现可能的失败并为管理机构提供干预措施。<details>
<summary>Abstract</summary>
Due to the cultural and governance differences of countries around the world, there currently exists a wide spectrum of AI regulation policy proposals that have created a chaos in the global AI regulatory space. Properly regulating AI technologies is extremely challenging, as it requires a delicate balance between legal restrictions and technological developments. In this article, we first present a comprehensive review of AI regulation proposals from different geographical locations and cultural backgrounds. Then, drawing from historical lessons, we develop a framework to facilitate a thorough analysis of AI regulation proposals. Finally, we perform a systematic analysis of these AI regulation proposals to understand how each proposal may fail. This study, containing historical lessons and analysis methods, aims to help governing bodies untangling the AI regulatory chaos through a divide-and-conquer manner.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:因为世界各国文化和管理差异的存在，目前在全球AI规制空间中存在广泛的AI规制政策提议，创造了混乱。正确地规制AI技术是极其困难的，因为它需要绝对的法律限制和技术发展之间的细致平衡。在这篇文章中，我们首先提供了AI规制提议的全面回顾，然后，Drawing from historical lessons，我们开发了一个框架，以便对AI规制提议进行全面的分析。最后，我们对这些AI规制提议进行了系统性的分析，以了解每一个提议可能会失败的原因。这篇文章，包含历史评论和分析方法， hopes to help governing bodies untangle the AI regulatory chaos through a divide-and-conquer manner.
</details></li>
</ul>
<hr>
<h2 id="Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models"><a href="#Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models" class="headerlink" title="Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models"></a>Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02409">http://arxiv.org/abs/2308.02409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong-Hai Nguyen, Ngumimi Karen Iyortsuun, Hyung-Jeong Yang, Guee-Sang Lee, Soo-Hyung Kim</li>
<li>for: 这篇论文旨在预测 mental workload 的三个状态和级别，以便提高 mental health 评估的准确性。</li>
<li>methods: 该论文使用 Temporal Convolutional Networks 和 Multi-Dimensional Residual Block 等方法，将多维度空间综合利用以实现最佳的 mental 估计。</li>
<li>results: 该论文通过将 mental workload 分类为三个状态并估计级别，帮助早期发现 mental health 问题，从而预防严重的健康问题并提高生活质量。<details>
<summary>Abstract</summary>
The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
</details>
<details>
<summary>摘要</summary>
人脑在工作和休息时都处于不断活跃状态。心理活动是每日的过程，当脑部过度劳累时，可能有负面影响于人类健康。在最近几年，对于早期发现心理问题的检测已经受到了广泛关注，因为它可以帮助预防严重的健康问题并提高生活质量。几种信号都用于评估心理状态，但是电enzephalogram（EEG）在研究人员中广泛使用，因为它可以提供大量关于脑部的信息。本文目的是将心理劳重分为三个状态，并估算维度水平。我们的方法将多个空间维度组合起来，以获得最佳的心理估算结果。在时域方法中，我们使用Temporal Convolutional Networks，在频域中，我们提出了一种新的架构——多维度征存块，这种架构将征存块结合在一起。
</details></li>
</ul>
<hr>
<h2 id="Traffic-Flow-Simulation-for-Autonomous-Driving"><a href="#Traffic-Flow-Simulation-for-Autonomous-Driving" class="headerlink" title="Traffic Flow Simulation for Autonomous Driving"></a>Traffic Flow Simulation for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16762">http://arxiv.org/abs/2307.16762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junfeng Li, Changqing Yan</li>
<li>for: This paper aims to provide a simulation environment for testing and evaluating the development of autonomous driving technology.</li>
<li>methods: The paper uses micro-traffic flow modeling and cellular automata to build the simulation environment, and it also employs a vehicle motion model based on bicycle intelligence.</li>
<li>results: The paper develops a simulation environment for autonomous vehicle flow, which can accurately control the acceleration, braking, steering, and lighting actions of the vehicle based on the bus instructions issued by the decision-making system.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提供自动驾驶技术的测试和评估技术的实验环境。</li>
<li>methods: 这篇论文使用微车流模型和细胞自动机来构建实验环境，同时还使用基于自行车智能的车辆动态模型。</li>
<li>results: 这篇论文建立了一个可以准确控制车辆加速、减速、转向和灯光动作的自动驾驶流 simulator。<details>
<summary>Abstract</summary>
A traffic system is a random and complex large system, which is difficult to conduct repeated modelling and control research in a real traffic environment. With the development of automatic driving technology, the requirements for testing and evaluating the development of automatic driving technology are getting higher and higher, so the application of computer technology for traffic simulation has become a very effective technical means. Based on the micro-traffic flow modelling, this paper adopts the vehicle motion model based on cellular automata and the theory of bicycle intelligence to build the simulation environment of autonomous vehicle flow. The architecture of autonomous vehicles is generally divided into a perception system, decision system and control system. The perception system is generally divided into many subsystems, responsible for autonomous vehicle positioning, obstacle recognition, traffic signal detection and recognition and other tasks. Decision systems are typically divided into many subsystems that are responsible for tasks such as path planning, path planning, behavior selection, motion planning, and control. The control system is the basis of the selfdriving car, and each control system of the vehicle needs to be connected with the decision-making system through the bus, and can accurately control the acceleration degree, braking degree, steering amplitude, lighting control and other driving actions according to the bus instructions issued by the decision-making system, so as to achieve the autonomous driving of the vehicle.
</details>
<details>
<summary>摘要</summary>
traffic system 是一个随机和复杂的大型系统，在实际交通环境中进行重复模拟和控制研究非常困难。随着自动驾驶技术的发展，测试和评估自动驾驶技术的要求越来越高，因此计算机技术在交通模拟中发挥了非常有效的技术作用。基于微流量模拟，本文采用基于细胞自动机和自行车智能理论建立自动车流 simulation 环境。自动车的架构通常分为感知系统、决策系统和控制系统。感知系统通常分为多个子系统，负责自动车定位、障碍物识别、交通信号检测和识别等任务。决策系统通常分为多个子系统，负责任务such as 路径规划、行为选择、动作规划和控制。控制系统是自动车的基础，每个控制系统需要与决策系统通过公共总线连接，并可以准确控制车辆加速度、缓冲度、转向强度、灯光控制和其他驾驶动作，以实现自动驾驶。
</details></li>
</ul>
<hr>
<h2 id="DeepCL-Deep-Change-Feature-Learning-on-Remote-Sensing-Images-in-the-Metric-Space"><a href="#DeepCL-Deep-Change-Feature-Learning-on-Remote-Sensing-Images-in-the-Metric-Space" class="headerlink" title="DeepCL: Deep Change Feature Learning on Remote Sensing Images in the Metric Space"></a>DeepCL: Deep Change Feature Learning on Remote Sensing Images in the Metric Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12208">http://arxiv.org/abs/2307.12208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haonanguo/deepcl">https://github.com/haonanguo/deepcl</a></li>
<li>paper_authors: Haonan Guo, Bo Du, Chen Wu, Chengxi Han, Liangpei Zhang</li>
<li>for: 本研究旨在提高自动变化检测（CD）的精度和可解释性，以便更好地监测地表动态变化。</li>
<li>methods: 我们结合度量学习的强时间关系模型和分割的优势，提出了深度变化特征学习（DeepCL）框架。我们设计了一种具有强时间相关性的强对比损失函数，以显著提高对比样本的重要性。此外，我们利用模型的时间关系知识来引导分割过程，以更好地检测变化区域。</li>
<li>results: 我们对 DeepCL 框架进行了严格的理论和实验评估，结果显示，DeepCL 在特征识别率、鲁棒性和可解释性等方面具有明显的优势，并且可以与多种 CD 方法进行结合使用。广泛的比较实验证明 DeepCL 在 CD 领域的精度和可解释性都达到了国际先进水平。<details>
<summary>Abstract</summary>
Change detection (CD) is an important yet challenging task in the Earth observation field for monitoring Earth surface dynamics. The advent of deep learning techniques has recently propelled automatic CD into a technological revolution. Nevertheless, deep learning-based CD methods are still plagued by two primary issues: 1) insufficient temporal relationship modeling and 2) pseudo-change misclassification. To address these issues, we complement the strong temporal modeling ability of metric learning with the prominent fitting ability of segmentation and propose a deep change feature learning (DeepCL) framework for robust and explainable CD. Firstly, we designed a hard sample-aware contrastive loss, which reweights the importance of hard and simple samples. This loss allows for explicit modeling of the temporal correlation between bi-temporal remote sensing images. Furthermore, the modeled temporal relations are utilized as knowledge prior to guide the segmentation process for detecting change regions. The DeepCL framework is thoroughly evaluated both theoretically and experimentally, demonstrating its superior feature discriminability, resilience against pseudo changes, and adaptability to a variety of CD algorithms. Extensive comparative experiments substantiate the quantitative and qualitative superiority of DeepCL over state-of-the-art CD approaches.
</details>
<details>
<summary>摘要</summary>
优化检测 (CD) 是地球观测领域中重要且挑战性强的任务，用于监测地球表面动态。深度学习技术的出现已经推动了自动化 CD 技术到了技术革命的水平。然而，深度学习基于 CD 方法仍然受到两个主要问题的困扰：1）不充分的时间关系模型化和2）假变误分类。为了解决这些问题，我们将强度表示学习的准确性能与分割的优劣点相结合，并提出了深度变化特征学习 (DeepCL) 框架。首先，我们设计了一种坚持样本感知的对比损失函数，该函数重新评估硬件和简单样本的重要性。这种损失函数允许明确模型 биitemporal 遥感图像之间的时间相关性。其次，模型的时间关系知识被用作引导分割过程，以便检测变化区域。DeepCL 框架进行了严格的理论和实验测试，并证明其具有更高的特征抽象能力、鲁棒性 against 假变和适应性。广泛的比较实验证明 DeepCL 在 CD 领域的超过状态之前的性能。
</details></li>
</ul>
<hr>
<h2 id="Monadic-Deep-Learning"><a href="#Monadic-Deep-Learning" class="headerlink" title="Monadic Deep Learning"></a>Monadic Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12187">http://arxiv.org/abs/2307.12187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ThoughtWorksInc/monadic-deep-learning">https://github.com/ThoughtWorksInc/monadic-deep-learning</a></li>
<li>paper_authors: Bo Yang, Zhihao Zhang Kirisame Marisa, Kai Shi</li>
<li>for: 本研究旨在提供一个可靠的、类型安全的深度学习框架，用于在 Scala 中实现动态神经网络。</li>
<li>methods: 本研究使用了一种新的准确方法，可以在静态类型函数中自动进行微分，并且可以跨语言交互。此外，研究还使用了一些套件和套件变换，以便用户可以创建具有动态神经网络表示的幂等表达式。</li>
<li>results: 研究得到了一个可靠的、类型安全的深度学习框架，可以在 Scala 中实现复杂的神经网络表示。用户可以使用这个框架来创建具有多变量的神经网络，并且仍然保持类型安全性。<details>
<summary>Abstract</summary>
The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.   Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.   We solved this problem in DeepLearning.scala 2. Our contributions are:   1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.   2. We designed a set of monads and monad transformers, which allow users to create monadic expressions that represent dynamic neural networks.   3. Along with these monads, we provide some applicative functors, to perform multiple calculations in parallel.   With these features, users of DeepLearning.scala were able to create complex neural networks in an intuitive and concise way, and still maintain type safety.
</details>
<details>
<summary>摘要</summary>
Java和Scala社区已经建立了一个非常成功的大数据生态系统。然而，大多数运行在其上的神经网络都是使用动态类型编程语言模型的。这些动态类型深度学习框架将神经网络视为可导的表达，并在训练时自动进行差分。直到2019年，静态类型语言的学习框架中没有提供表达能力相当于传统框架的。其用户无法使用自定义算法，除非创建大量的简单代码来实现硬编码的反向差分。我们解决了这个问题，我们在DeepLearning.scala 2中提供了以下贡献：1. 我们发现了一种新的方法，可以在静态类型函数中自动进行差分，并且可以与金属语言自由交互。2. 我们设计了一组幂kk和幂Transformer，这些幂kk可以让用户创建幂表达式，表示动态神经网络。3. 同时，我们还提供了一些应用函数，可以并发地执行多个计算。通过这些特性，DeepLearning.scala 2 的用户可以创建复杂的神经网络，并且保持类型安全性。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-discovers-invariants-of-braids-and-flat-braids"><a href="#Machine-learning-discovers-invariants-of-braids-and-flat-braids" class="headerlink" title="Machine learning discovers invariants of braids and flat braids"></a>Machine learning discovers invariants of braids and flat braids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12185">http://arxiv.org/abs/2307.12185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexei Lisitsa, Mateo Salles, Alexei Vernitski</li>
<li>for: 这个论文用机器学习分类扑斗织（或平坦扑斗）的例子，以确定它们是否是平凡的。</li>
<li>methods: 这个论文使用了指导学习使用神经网络（多层感知器）进行超visisted学习，以达到好的分类结果。</li>
<li>results: 通过这个论文，我们发现了新的便利的扑斗 invariants，包括完全的平坦扑斗 invariants。<details>
<summary>Abstract</summary>
We use machine learning to classify examples of braids (or flat braids) as trivial or non-trivial. Our ML takes form of supervised learning using neural networks (multilayer perceptrons). When they achieve good results in classification, we are able to interpret their structure as mathematical conjectures and then prove these conjectures as theorems. As a result, we find new convenient invariants of braids, including a complete invariant of flat braids.
</details>
<details>
<summary>摘要</summary>
我们使用机器学习来分类拥有螺旋或平板拥的例子（拥有螺旋或平板拥）。我们的机器学习是指监督学习使用神经网络（多层感知器）。当它们在分类中达到好的结果时，我们可以解释它们的结构为数学假设，然后证明这些假设为定理。因此，我们发现了新的便利的拥 invariants，包括完整的平板拥 invariants。Note: "拥" (wò) in the text refers to "braids" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="On-the-Expressivity-of-Multidimensional-Markov-Reward"><a href="#On-the-Expressivity-of-Multidimensional-Markov-Reward" class="headerlink" title="On the Expressivity of Multidimensional Markov Reward"></a>On the Expressivity of Multidimensional Markov Reward</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12184">http://arxiv.org/abs/2307.12184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuwa Miura</li>
<li>for: 本研究考虑了Markov奖励表示在Sequential Decision Making下的表达性。</li>
<li>methods: 我们使用Markov决策过程（MDPs）中的奖励函数来描述代理人希望的行为。我们研究了一个可能性空间中的奖励函数是否可以使得所有希望的策略更加抽象。我们的主要结果表明，如果存在一个可能性空间中的奖励函数，那么这个奖励函数必须满足一定的必要和 suficient conditions。</li>
<li>results: 我们还证明了，对于任何非杂分 deterministic策略集合，存在一个多维度Markov奖励函数可以描述它。<details>
<summary>Abstract</summary>
We consider the expressivity of Markov rewards in sequential decision making under uncertainty. We view reward functions in Markov Decision Processes (MDPs) as a means to characterize desired behaviors of agents. Assuming desired behaviors are specified as a set of acceptable policies, we investigate if there exists a scalar or multidimensional Markov reward function that makes the policies in the set more desirable than the other policies. Our main result states both necessary and sufficient conditions for the existence of such reward functions. We also show that for every non-degenerate set of deterministic policies, there exists a multidimensional Markov reward function that characterizes it
</details>
<details>
<summary>摘要</summary>
我们考虑了马尔可夫奖励在随机决策中的表达性。我们看作奖励函数在马尔可夫决策过程（MDP）中是代表机器人所愿意行为的方式。假设所希望的行为是一个集合的可接受政策，我们实际寻找是否存在一个数值或多维马尔可夫奖励函数，使得这些政策在集合中更加愿意被选择。我们的主要结果表明了这些奖励函数的必要和充分条件，并且显示了每个非当ode deterministic政策都存在一个多维马尔可夫奖励函数，可以Characterize它。
</details></li>
</ul>
<hr>
<h2 id="Security-and-Privacy-Issues-of-Federated-Learning"><a href="#Security-and-Privacy-Issues-of-Federated-Learning" class="headerlink" title="Security and Privacy Issues of Federated Learning"></a>Security and Privacy Issues of Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12181">http://arxiv.org/abs/2307.12181</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JiangChSo/PFLM">https://github.com/JiangChSo/PFLM</a></li>
<li>paper_authors: Jahid Hasan</li>
<li>for: 本研究旨在提供一个涵盖多种机器学习模型的 Federated Learning（FL）安全性和隐私性挑战的全面分类。</li>
<li>methods: 本研究使用了多种攻击和防御策略，包括投毒攻击、后门攻击、会员推理攻击、生成对抗网络（GAN）基于攻击和均衡隐私攻击。</li>
<li>results: 本研究提出了一些新的研究方向，旨在强化FL系统的安全性和隐私性，以保护分布式学习环境中的敏感数据confidentiality。<details>
<summary>Abstract</summary>
Federated Learning (FL) has emerged as a promising approach to address data privacy and confidentiality concerns by allowing multiple participants to construct a shared model without centralizing sensitive data. However, this decentralized paradigm introduces new security challenges, necessitating a comprehensive identification and classification of potential risks to ensure FL's security guarantees. This paper presents a comprehensive taxonomy of security and privacy challenges in Federated Learning (FL) across various machine learning models, including large language models. We specifically categorize attacks performed by the aggregator and participants, focusing on poisoning attacks, backdoor attacks, membership inference attacks, generative adversarial network (GAN) based attacks, and differential privacy attacks. Additionally, we propose new directions for future research, seeking innovative solutions to fortify FL systems against emerging security risks and uphold sensitive data confidentiality in distributed learning environments.
</details>
<details>
<summary>摘要</summary>
受领 learning（FL）已经出现为一种有前途的方法，以解决数据隐私和安全性问题，使得多个参与者可以构建共享模型，而不需要中央化敏感数据。然而，这种分布式模式带来了新的安全挑战，需要进行全面的风险识别和分类，以确保FL的安全保证。这篇论文提出了FL中安全和隐私挑战的完整分类，包括聚合器和参与者的攻击，以及毒ous attacks、后门攻击、会员推理攻击、基于GAN的攻击和权威隐私攻击。此外，我们还提出了未来研究的新方向，寻找创新的解决方案，以固化FL系统，防止数据泄露和分布式学习环境中的安全风险。
</details></li>
</ul>
<hr>
<h2 id="Named-Entity-Resolution-in-Personal-Knowledge-Graphs"><a href="#Named-Entity-Resolution-in-Personal-Knowledge-Graphs" class="headerlink" title="Named Entity Resolution in Personal Knowledge Graphs"></a>Named Entity Resolution in Personal Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12173">http://arxiv.org/abs/2307.12173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayank Kejriwal</li>
<li>for: 本文主要针对个人知识图（PKG）中的命名实体解析（ER）问题。</li>
<li>methods: 本文首先提供了高质量和高效的ER问题定义和必需组件。然后，总结了现有技术在PKG中的应用可能性，以及预期在Web级数据中出现的挑战。</li>
<li>results: 本文结束后，提供了一些应用和未来研究的可能性。<details>
<summary>Abstract</summary>
Entity Resolution (ER) is the problem of determining when two entities refer to the same underlying entity. The problem has been studied for over 50 years, and most recently, has taken on new importance in an era of large, heterogeneous 'knowledge graphs' published on the Web and used widely in domains as wide ranging as social media, e-commerce and search. This chapter will discuss the specific problem of named ER in the context of personal knowledge graphs (PKGs). We begin with a formal definition of the problem, and the components necessary for doing high-quality and efficient ER. We also discuss some challenges that are expected to arise for Web-scale data. Next, we provide a brief literature review, with a special focus on how existing techniques can potentially apply to PKGs. We conclude the chapter by covering some applications, as well as promising directions for future research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>实体归一化（ER）问题是确定两个实体指向同一个基础实体的问题。该问题已经被研究了50多年，并在最近在大量、多元知识图（knowledge graphs）在社交媒体、电商和搜索等领域广泛使用后，又得到了新的重要性。本章将讨论个人知识图（PKGs）中的命名实体归一化问题。我们开始于形式定义问题和完成高质量和高效的实体归一化所需的组件。我们还讨论了预期出现在网络规模数据上的挑战。接下来，我们提供了一 brief文献综述，强调如何现有技术可能应用于PKGs。我们结束本章，涵盖一些应用以及未来研究的承诺。
</details></li>
</ul>
<hr>
<h2 id="Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters"><a href="#Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters" class="headerlink" title="Optimized Network Architectures for Large Language Model Training with Billions of Parameters"></a>Optimized Network Architectures for Large Language Model Training with Billions of Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12169">http://arxiv.org/abs/2307.12169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyang Wang, Manya Ghobadi, Kayvon Shakeri, Ying Zhang, Naader Hasani</li>
<li>for: 这个论文挑战了训练大语言模型（LLM）的传统建立任意对任意网络的思路。</li>
<li>methods: 我们表明了 LLM 的通信模式，只有小组 GPU 需要高带宽任意对任意通信，以达到训练性能的近似最优点。在这些小组 GPU 之间，通信几乎不存在，稀疏、同质化。我们提议一种新的网络架构，与 LLM 的通信需求相匹配。我们将集群分为 HB Domain 中的 GPU，并使用非堵塞任意对任意高速交换机连接这些 GPU。在 HB Domain 内，网络只连接了需要通信的 GPU。我们称这种网络为 “铁路仅” 连接，并证明我们的提议架构可以将网络成本降低至最多 75%，不对 LLM 训练性能产生影响。</li>
<li>results: 我们的实验结果表明，我们的提议架构可以减少网络成本至最多 75%，而无需增加训练时间或缓存大小。此外，我们的架构还可以在不同的 GPU 分布和通信占比情况下保持较高的性能稳定性。<details>
<summary>Abstract</summary>
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Hallucination-Improves-the-Performance-of-Unsupervised-Visual-Representation-Learning"><a href="#Hallucination-Improves-the-Performance-of-Unsupervised-Visual-Representation-Learning" class="headerlink" title="Hallucination Improves the Performance of Unsupervised Visual Representation Learning"></a>Hallucination Improves the Performance of Unsupervised Visual Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12168">http://arxiv.org/abs/2307.12168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Wu, Jennifer Hobbs, Naira Hovakimyan</li>
<li>for: 提高自主学习中的对比学习性能</li>
<li>methods: 基于Siamese结构的对比学习模型，加入Hallucinator来生成额外正例样本，提高对比学习的semantic对比和鲁棒性</li>
<li>results: 在不同的对比学习模型和数据集上，通过Hallucinator来生成额外正例样本，可以提高对比学习模型的稳定性和对应性，并且在下游任务中也可以看到明显的提升。<details>
<summary>Abstract</summary>
Contrastive learning models based on Siamese structure have demonstrated remarkable performance in self-supervised learning. Such a success of contrastive learning relies on two conditions, a sufficient number of positive pairs and adequate variations between them. If the conditions are not met, these frameworks will lack semantic contrast and be fragile on overfitting. To address these two issues, we propose Hallucinator that could efficiently generate additional positive samples for further contrast. The Hallucinator is differentiable and creates new data in the feature space. Thus, it is optimized directly with the pre-training task and introduces nearly negligible computation. Moreover, we reduce the mutual information of hallucinated pairs and smooth them through non-linear operations. This process helps avoid over-confident contrastive learning models during the training and achieves more transformation-invariant feature embeddings. Remarkably, we empirically prove that the proposed Hallucinator generalizes well to various contrastive learning models, including MoCoV1&V2, SimCLR and SimSiam. Under the linear classification protocol, a stable accuracy gain is achieved, ranging from 0.3% to 3.0% on CIFAR10&100, Tiny ImageNet, STL-10 and ImageNet. The improvement is also observed in transferring pre-train encoders to the downstream tasks, including object detection and segmentation.
</details>
<details>
<summary>摘要</summary>
对比学习模型基于单胞结构的表现备受关注，特别是在自监督学习中。然而，这些框架的成功受到两个条件的限制：一个是足够多的正例对，另一个是正例对之间的差异。如果这两个条件不充分满足，这些框架将缺乏含义的对比，并且容易过滤。为了解决这两个问题，我们提出了“幻觉”（Hallucinator），它可以快速生成更多的正例对，并且让这些对之间的差异更加明显。幻觉是可微的，并且在预训任务中直接优化。此外，我们还将幻觉的对应对采用非线性操作，以避免模型在训练过程中过滤。我们实际上证明，提案的幻觉可以跨多种对比学习模型，包括MoCoV1&V2、SimCLR和SimSiam，在线性分类协议下表现稳定，从CIFAR10&100、Tiny ImageNet、STL-10和ImageNet上获得了0.3%到3.0%的稳定精度提升。此外，幻觉也可以在预训对象检测和分类等下游任务中实现稳定的提升。
</details></li>
</ul>
<hr>
<h2 id="The-Imitation-Game-Detecting-Human-and-AI-Generated-Texts-in-the-Era-of-Large-Language-Models"><a href="#The-Imitation-Game-Detecting-Human-and-AI-Generated-Texts-in-the-Era-of-Large-Language-Models" class="headerlink" title="The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models"></a>The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12166">http://arxiv.org/abs/2307.12166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kadhim Hayawi, Sakib Shahriar, Sujith Samuel Mathew</li>
<li>for: 本研究旨在探讨人工智能（AI）基于大型语言模型（LLM）在教育、研究和实践中的潜在潜力，但分辨人类写作和AI生成文本已成为一项重要任务。</li>
<li>methods: 本研究采用了多种机器学习模型来分类文本，并 introduce了一个新的人类写作和LLM生成文本的数据集，包括了不同类型的文章、短篇小说、诗歌和Python代码。</li>
<li>results: 结果表明这些机器学习模型在分类文本时表现出色，即使数据集的样本数较少，但在分类GPT生成文本时，特别是在故事写作方面，任务变得更加困难。结果还表明这些模型在二分类任务中，如人类写作与特定LLM之间的分类，表现出更高的性能，而在多类任务中，如分辨人类写作和多个LLM之间的分类，任务变得更加复杂。<details>
<summary>Abstract</summary>
The potential of artificial intelligence (AI)-based large language models (LLMs) holds considerable promise in revolutionizing education, research, and practice. However, distinguishing between human-written and AI-generated text has become a significant task. This paper presents a comparative study, introducing a novel dataset of human-written and LLM-generated texts in different genres: essays, stories, poetry, and Python code. We employ several machine learning models to classify the texts. Results demonstrate the efficacy of these models in discerning between human and AI-generated text, despite the dataset's limited sample size. However, the task becomes more challenging when classifying GPT-generated text, particularly in story writing. The results indicate that the models exhibit superior performance in binary classification tasks, such as distinguishing human-generated text from a specific LLM, compared to the more complex multiclass tasks that involve discerning among human-generated and multiple LLMs. Our findings provide insightful implications for AI text detection while our dataset paves the way for future research in this evolving area.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）基于大型语言模型（LLM）的潜力在教育、研究和实践中具有巨大的推动力。然而，分辨人类写作和AI生成文本已成为一项重要的任务。这篇论文介绍了一项比较研究，推出了一个新的人类写作和LLM生成文本的数据集，包括了不同类型的文章、故事、诗歌和Python代码。我们使用了多种机器学习模型来分类文本。结果表明这些模型在分类人类写作和AI生成文本的任务中具有remarkable的表现，即使数据集的样本数较少。然而，当分类GPT生成文本时，特别是在故事创作中，任务变得更加困难。结果表明这些模型在二分类任务中，如人类写作与特定LLM的分类，表现更加出色，与多类任务，如分类人类写作和多个LLM之间的分类，相比较，任务变得更加复杂。我们的发现对AI文本检测具有深刻的意义，而我们的数据集也为未来这一领域的研究开辟了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft"><a href="#DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft" class="headerlink" title="DIP-RL: Demonstration-Inferred Preference Learning in Minecraft"></a>DIP-RL: Demonstration-Inferred Preference Learning in Minecraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12158">http://arxiv.org/abs/2307.12158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ellen Novoseller, Vinicius G. Goecks, David Watkins, Josh Miller, Nicholas Waytowich</li>
<li>For:	+ The paper is written for researchers and practitioners in the field of reinforcement learning and machine learning, particularly those interested in using human demonstrations to guide learning in unstructured and open-ended environments.* Methods:	+ The paper presents Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), a novel algorithm that leverages human demonstrations in three distinct ways: training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL.* Results:	+ The paper evaluates DIP-RL in a tree-chopping task in Minecraft, and finds that the method can guide an RL agent to learn a reward function that reflects human preferences. Additionally, DIP-RL performs competitively relative to baselines. Example trajectory rollouts of DIP-RL and baselines are available online.<details>
<summary>Abstract</summary>
In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspired by our previous work on combining demonstrations and pairwise preferences in Minecraft, which was awarded a research prize at the 2022 NeurIPS MineRL BASALT competition, Learning from Human Feedback in Minecraft. Example trajectory rollouts of DIP-RL and baselines are located at https://sites.google.com/view/dip-rl.
</details>
<details>
<summary>摘要</summary>
机器学习中的决策过程中，一个算法代理人会在环境中互动，并从环境接收回报信号作为反馈。但在许多不结构化的实际场景中，这种奖励信号是未知的，人们无法可靠地制定一个正确捕捉愿望行为的奖励信号。为解决这类无结构和开放的环境中的任务，我们提出了人示示 preference reinforcement learning（DIP-RL）算法，它利用人类示例的三种方式，包括训练自动编码器、使用示例数据种子RL训练批次，以及从示例数据中推断行为偏好，以学习一个奖励函数，以引导RL。我们在Minecraft中进行了树割任务的评估，结果表明，DIP-RL可以引导一个RL代理人学习一个奖励函数，满足人类的偏好，并且DIP-RL与基elines相比，表现竞争力强。DIP-RL的灵感来自我们在Minecraft中结合示例和对比式偏好的研究，赢得2022年NeurIPS MineRL BASALT大赛的研究奖，《从人类反馈学习在Minecraft》。有关DIP-RL和基elines的示例轨迹演示，请参考https://sites.google.com/view/dip-rl。
</details></li>
</ul>
<hr>
<h2 id="Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning"><a href="#Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning" class="headerlink" title="Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning"></a>Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12143">http://arxiv.org/abs/2307.12143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aqeel13932/mn_project">https://github.com/aqeel13932/mn_project</a></li>
<li>paper_authors: Aqeel Labash, Florian Fletzer, Daniel Majoral, Raul Vicente</li>
<li>for: 这个论文的目的是研究深度学习代理人在有可靠 periodic 变化的环境中学习寻食任务时，是否可以emerge circadian-like rhythms。</li>
<li>methods: 作者使用了深度学习代理人，在一个可靠 periodic 变化的环境中解决寻食任务。在学习过程中，作者系统地Characterize了代理人的行为，并证明了代理人内部的rhythm是自适应的和可调整的。</li>
<li>results: 研究发现，代理人在学习过程中emerge一种自适应的rhythm，这种rhythm可以适应环境的signal phase的变化，而无需重新训练。此外，作者通过分析bifurcation和相对应 Curve的方法，表明了人工神经元的动力学特性是支持内部 rhythm 的内化的关键。<details>
<summary>Abstract</summary>
Adapting to regularities of the environment is critical for biological organisms to anticipate events and plan. A prominent example is the circadian rhythm corresponding to the internalization by organisms of the $24$-hour period of the Earth's rotation. In this work, we study the emergence of circadian-like rhythms in deep reinforcement learning agents. In particular, we deployed agents in an environment with a reliable periodic variation while solving a foraging task. We systematically characterize the agent's behavior during learning and demonstrate the emergence of a rhythm that is endogenous and entrainable. Interestingly, the internal rhythm adapts to shifts in the phase of the environmental signal without any re-training. Furthermore, we show via bifurcation and phase response curve analyses how artificial neurons develop dynamics to support the internalization of the environmental rhythm. From a dynamical systems view, we demonstrate that the adaptation proceeds by the emergence of a stable periodic orbit in the neuron dynamics with a phase response that allows an optimal phase synchronisation between the agent's dynamics and the environmental rhythm.
</details>
<details>
<summary>摘要</summary>
适应环境的规律是生物体可能预测事件和规划的关键。一个明显的例子是Circadian rhythm，即生物体内化地球的24小时轮征。在这项工作中，我们研究了深度强化学习代理人在环境中的Circadian-like rhythm的出现。特别是，我们在一个可靠的周期变化环境中部署了代理人，并解决了捕食任务。我们系统地描述了代理人在学习过程中的行为，并证明了代理人内生的频率可以适应环境的阶段变化，而无需重新训练。此外，我们通过分支和相对响应曲线分析，展示了人工神经元发展的动力学支持内化环境的频率。从动态系统的视角来看，我们证明了适应过程中的稳定 periodic orbit 在神经元动力学中出现，并且该频率允许代理人的动力学和环境频率进行优化的相对同步。
</details></li>
</ul>
<hr>
<h2 id="Route-Planning-Using-Nature-Inspired-Algorithms"><a href="#Route-Planning-Using-Nature-Inspired-Algorithms" class="headerlink" title="Route Planning Using Nature-Inspired Algorithms"></a>Route Planning Using Nature-Inspired Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12133">http://arxiv.org/abs/2307.12133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Priyansh Saxena, Raahat Gupta, Akshat Maheshwari</li>
<li>for: 本文主要用于介绍 Nature-Inspired Algorithms（NIAs），以及它们在路径规划问题中的应用。</li>
<li>methods: 本文使用了多种 Nature-Inspired Algorithms，包括遗传算法、社会间气候算法、蜂群算法等。</li>
<li>results: 本文通过对路径规划问题的解决，显示了 NIAs 的优化性和可靠性。<details>
<summary>Abstract</summary>
There are many different heuristic algorithms for solving combinatorial optimization problems that are commonly described as Nature-Inspired Algorithms (NIAs). Generally, they are inspired by some natural phenomenon, and due to their inherent converging and stochastic nature, they are known to give optimal results when compared to classical approaches. There are a large number of applications of NIAs, perhaps the most popular being route planning problems in robotics - problems that require a sequence of translation and rotation steps from the start to the goal in an optimized manner while avoiding obstacles in the environment. In this chapter, we will first give an overview of Nature-Inspired Algorithms, followed by their classification and common examples. We will then discuss how the NIAs have applied to solve the route planning problem.
</details>
<details>
<summary>摘要</summary>
《自然引导算法》是解决 combinatorial optimization 问题的多种不同规则算法。通常它们是根据自然现象 inspirited，因为它们的内置的叠合和随机性，可以给出优化的结果，比 класси方法更好。有很多应用的NIAs，最受欢迎的是机器人路径规划问题——需要从起始点到目标点按一个优化的轨迹和旋转步骤，避免环境中的障碍物。本章首先给出了 Nature-Inspired Algorithms 的概述，然后分类和常见的例子，最后讨论了 NIAs 如何应用于路径规划问题。
</details></li>
</ul>
<hr>
<h2 id="AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities"><a href="#AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities" class="headerlink" title="AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities"></a>AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12128">http://arxiv.org/abs/2307.12128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Adewopo, Nelly Elsayed, Zag Elsayed, Murat Ozer, Victoria Wangia-Anderson, Ahmed Abdelgawad</li>
<li>for: 这篇论文主要是为了提高交通管理和交通事故预防。</li>
<li>methods: 该论文提出了一种基于交通监测摄像头和动作识别系统的交通事故探测和应对方案。</li>
<li>results: 该方案可以减少交通事故的频率和严重程度，提高交通管理的效率和安全性。<details>
<summary>Abstract</summary>
Accident detection and traffic analysis is a critical component of smart city and autonomous transportation systems that can reduce accident frequency, severity and improve overall traffic management. This paper presents a comprehensive analysis of traffic accidents in different regions across the United States using data from the National Highway Traffic Safety Administration (NHTSA) Crash Report Sampling System (CRSS). To address the challenges of accident detection and traffic analysis, this paper proposes a framework that uses traffic surveillance cameras and action recognition systems to detect and respond to traffic accidents spontaneously. Integrating the proposed framework with emergency services will harness the power of traffic cameras and machine learning algorithms to create an efficient solution for responding to traffic accidents and reducing human errors. Advanced intelligence technologies, such as the proposed accident detection systems in smart cities, will improve traffic management and traffic accident severity. Overall, this study provides valuable insights into traffic accidents in the US and presents a practical solution to enhance the safety and efficiency of transportation systems.
</details>
<details>
<summary>摘要</summary>
意外探测和交通分析是智能城市和自动化交通系统的关键组成部分，可以降低意外频率、严重程度并改善总体交通管理。这篇论文对美国各地的交通意外进行了全面分析，使用国家公路安全管理局（NHTSA）的交通事故报告采样系统（CRSS）的数据。为了解决意外探测和交通分析的挑战，这篇论文提出了一个框架，该框架使用交通监测摄像头和动作认知系统来自动探测和应对交通意外。将该框架与急救服务集成，可以利用交通摄像头和机器学习算法来创造一个高效的交通意外应急处理解决方案。智能技术，如提议的交通意外探测系统，将改善交通管理和交通意外严重程度。总之，这篇研究对美国交通意外提供了有价值的意见，并提出了一个实用的解决方案，以提高交通系统的安全性和效率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.AI_2023_07_23/" data-id="clorjzl1d0015f188buxwc1el" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.CL_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T11:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.CL_2023_07_23/">cs.CL - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="X-CapsNet-For-Fake-News-Detection"><a href="#X-CapsNet-For-Fake-News-Detection" class="headerlink" title="X-CapsNet For Fake News Detection"></a>X-CapsNet For Fake News Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12332">http://arxiv.org/abs/2307.12332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Hadi Goldani, Reza Safabakhsh, Saeedeh Momtazi</li>
<li>for: 本研究旨在帮助减少社交媒体和网络论坛上的谣言对用户决策产生的影响，通过自动检测和抵御假新闻。</li>
<li>methods: 该研究提出了一种基于变换器的模型，称为X-CapsNet，该模型包括一个带有动态路由算法的 capsule神经网络（CapsNet），以及一个大小基于的分类器。</li>
<li>results: 研究使用了 Covid-19 和 Liar 数据集进行评估，结果表明，模型在 Covid-19 数据集上的 F1 分数和 Liar 数据集上的准确率都高于现有基线。<details>
<summary>Abstract</summary>
News consumption has significantly increased with the growing popularity and use of web-based forums and social media. This sets the stage for misinforming and confusing people. To help reduce the impact of misinformation on users' potential health-related decisions and other intents, it is desired to have machine learning models to detect and combat fake news automatically. This paper proposes a novel transformer-based model using Capsule neural Networks(CapsNet) called X-CapsNet. This model includes a CapsNet with dynamic routing algorithm paralyzed with a size-based classifier for detecting short and long fake news statements. We use two size-based classifiers, a Deep Convolutional Neural Network (DCNN) for detecting long fake news statements and a Multi-Layer Perceptron (MLP) for detecting short news statements. To resolve the problem of representing short news statements, we use indirect features of news created by concatenating the vector of news speaker profiles and a vector of polarity, sentiment, and counting words of news statements. For evaluating the proposed architecture, we use the Covid-19 and the Liar datasets. The results in terms of the F1-score for the Covid-19 dataset and accuracy for the Liar dataset show that models perform better than the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
新闻消耗量已经明显增加，这与网络讨论平台和社交媒体的普及和使用有着直接关系。这种情况设置了误导和混淆人们的场景，为了帮助用户避免基于误information的决策，自动检测和抗误information的机器学习模型变得越来越重要。本文提出了一种基于变换器的新型模型，称为X-CapsNet，该模型包括一个具有动态路由算法的Capsule神经网络（CapsNet）和一个大小分类器。我们使用了两个大小分类器，一个是深度卷积神经网络（DCNN）用于检测长 fake news 声明，另一个是多层感知神经网络（MLP）用于检测短新闻声明。为解决短新闻声明的表示问题，我们使用了 indirect 特征，即新闻发布人的 Profile 向量和新闻声明中的负面情感和计数词向量。为评估提议的体系，我们使用了 Covid-19 和 Liar 数据集。结果表明，模型在 Covid-19 数据集上的 F1 分数和 Liar 数据集上的准确率都高于当前基eline。
</details></li>
</ul>
<hr>
<h2 id="Milimili-Collecting-Parallel-Data-via-Crowdsourcing"><a href="#Milimili-Collecting-Parallel-Data-via-Crowdsourcing" class="headerlink" title="Milimili. Collecting Parallel Data via Crowdsourcing"></a>Milimili. Collecting Parallel Data via Crowdsourcing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12282">http://arxiv.org/abs/2307.12282</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alantonov/milimili">https://github.com/alantonov/milimili</a></li>
<li>paper_authors: Alexander Antonov</li>
<li>for: 这个研究是为了提出一种通过协同劳动来收集并构建平行 Corpora的方法，比聘用专业翻译人员更加经济。</li>
<li>methods: 这种方法利用了互联网平台，通过吸引志愿者参与翻译来收集数据，并使用机器学习算法来进行自动评分。</li>
<li>results: 研究人员通过实验对Chechen-Russian和Fula-English语种的平行数据进行了收集和分析，并发现这种方法可以提供高质量的平行数据，但需要进一步的优化和纠正。<details>
<summary>Abstract</summary>
We present a methodology for gathering a parallel corpus through crowdsourcing, which is more cost-effective than hiring professional translators, albeit at the expense of quality. Additionally, we have made available experimental parallel data collected for Chechen-Russian and Fula-English language pairs.
</details>
<details>
<summary>摘要</summary>
我们提出了一种使用人工协助收集并行文献的方法，这比聘请专业翻译人员更加经济，然而品质可能受到影响。此外，我们已经为Chechen-Russian和Fula-English语种对 experimental parallel数据进行了收集。
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-Joint-Source-Channel-Coding-for-Textual-Semantic-Communication"><a href="#Transformer-based-Joint-Source-Channel-Coding-for-Textual-Semantic-Communication" class="headerlink" title="Transformer-based Joint Source Channel Coding for Textual Semantic Communication"></a>Transformer-based Joint Source Channel Coding for Textual Semantic Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12266">http://arxiv.org/abs/2307.12266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shicong Liu, Zhen Gao, Gaojie Chen, Yu Su, Lu Peng</li>
<li>for: 本文提出了一种文本semantic传输框架，以提高在雷达环境下的文本传输可靠性和效率。</li>
<li>methods: 本文使用了高级自然语言处理技术，将文本句子分解成单词，并使用Transformer编码器进行semantic提取。编码后的数据被归一化为固定长度二进制序列，并对这些二进制序列进行了模拟隐藏 Markov chain 的扩展。</li>
<li>results:  simulationResults表明，提出的模型在semantic传输中具有较高的可靠性和效率，并且在雷达环境下能够有效地抗抗干扰。<details>
<summary>Abstract</summary>
The Space-Air-Ground-Sea integrated network calls for more robust and secure transmission techniques against jamming. In this paper, we propose a textual semantic transmission framework for robust transmission, which utilizes the advanced natural language processing techniques to model and encode sentences. Specifically, the textual sentences are firstly split into tokens using wordpiece algorithm, and are embedded to token vectors for semantic extraction by Transformer-based encoder. The encoded data are quantized to a fixed length binary sequence for transmission, where binary erasure, symmetric, and deletion channels are considered for transmission. The received binary sequences are further decoded by the transformer decoders into tokens used for sentence reconstruction. Our proposed approach leverages the power of neural networks and attention mechanism to provide reliable and efficient communication of textual data in challenging wireless environments, and simulation results on semantic similarity and bilingual evaluation understudy prove the superiority of the proposed model in semantic transmission.
</details>
<details>
<summary>摘要</summary>
天空地海集成网络呼吁更加robust和安全的传输技术以抗干扰。在这篇论文中，我们提出了文本semantic传输框架，利用高级自然语言处理技术来模型和编码句子。具体来说，文本句子首先使用wordpiece算法拆分成单词，然后将单词embedding到token vector中进行semantic提取。编码后的数据被归一化为固定长度二进制序列进行传输，并考虑了三种渠道（ binary erasure、symmetric 和deletion）的传输。接收到的二进制序列被transformer解码器解码成原始句子中的单词，并用于句子重建。我们提出的方法利用神经网络和注意机制提供了可靠和高效的文本数据在困难无线环境中的传输，并在语义传输方面进行了严格的验证和评估。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition"><a href="#A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition" class="headerlink" title="A meta learning scheme for fast accent domain expansion in Mandarin speech recognition"></a>A meta learning scheme for fast accent domain expansion in Mandarin speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12262">http://arxiv.org/abs/2307.12262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Zhu, Changhao Shan, Bihong Zhang, Jian Yu</li>
<li>for: 这篇论文主要用于探讨普通话自动语音识别（ASR）中腔点域扩展问题。</li>
<li>methods: 该论文使用元学习技术来实现快速普通话腔点扩展，包括冻结模型参数以及元学习。</li>
<li>results: 相比基eline模型，该方法在腔点扩展任务中显示出3%的相对提升，并在大量数据下达到4%的相对提升。<details>
<summary>Abstract</summary>
Spoken languages show significant variation across mandarin and accent. Despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. In this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. Meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. So we select meta-learning in the domain expansion task. This more essential learning will cause improved performance on accent domain extension tasks. We combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. Our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. Compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. In addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.
</details>
<details>
<summary>摘要</summary>
⟨SYS⟩ spoken languages show significant variation across mandarin and accent. Despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. In this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. Meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. So we select meta-learning in the domain expansion task. This more essential learning will cause improved performance on accent domain extension tasks. We combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. Our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. Compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. In addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set. traslated by Google Translate
</details></li>
</ul>
<hr>
<h2 id="MyVoice-Arabic-Speech-Resource-Collaboration-Platform"><a href="#MyVoice-Arabic-Speech-Resource-Collaboration-Platform" class="headerlink" title="MyVoice: Arabic Speech Resource Collaboration Platform"></a>MyVoice: Arabic Speech Resource Collaboration Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02503">http://arxiv.org/abs/2308.02503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yousseif Elshahawy, Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali</li>
<li>for: 增强阿拉伯语言技术的研究</li>
<li>methods: 使用拥有者参与的人群征集平台收集阿拉伯语言录音，并提供公共数据集</li>
<li>results: 成功创建了大量的 диалект语言录音数据集，并提供了可Switch用户角色的功能，以及一个质量筛选和反馈系统，以保证数据的质量。<details>
<summary>Abstract</summary>
We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data.
</details>
<details>
<summary>摘要</summary>
我们介绍MyVoice，一个人工智能平台，旨在收集阿拉伯语言的口语，以提高方言技术。这个平台提供了大规模的方言语音数据的设计机会，并将其公开提供。MyVoice让参与者选择城市/国家精细方言，并录制显示的句子。用户可以在参与者和注释者之间进行Switch角色。平台包含一个质量保证系统，将低质量和假的录音过滤掉，然后将其发送到验证。在验证阶段，参与者可以评估录音质量，注释和提供反馈，这些反馈会被管理员审核。此外，平台还允许管理员添加新的数据或任务，超出方言语音和单词收集，这些任务将被显示给参与者。因此，MyVoice可以促进多方合作，收集多样化和大规模的阿拉伯语言数据。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation"><a href="#Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation" class="headerlink" title="Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation"></a>Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12231">http://arxiv.org/abs/2307.12231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhong-Qiu Wang, Nobutaka Ono, Yanmin Qian, Shinji Watanabe</li>
<li>for: 这个论文的目的是提出一种基于自适应学习表示的多 speaker自然语音识别系统。</li>
<li>methods: 本文使用多通道分离方法、封闭抑制映射和复杂 spectral mapping，以及最佳的ASR后端模型特征。</li>
<li>results: 研究人员通过使用最新的自我超级vised学习表示（SSLR）来提高filterbank特征下的识别性能，并通过合理的训练策略将speech separation和识别 integrate into一个系统，实现了WHAMR! reverberation测试集的2.5%字幕误差率，与现有的mask-based MVDR抽样抑制和filterbank综合integration（28.9%）相比，表现显著提高。<details>
<summary>Abstract</summary>
Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).
</details>
<details>
<summary>摘要</summary>
“神经语音分离技术在过去几年中已经做出了很大的进步，其与自动语音识别（ASR）的结合是实现多个说话人ASR的重要方向。本文提供了详细的语音分离在噪音混响和噪音混响 scenarios中的研究，作为ASR前端。 Specifically, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Identifying-Misinformation-on-YouTube-through-Transcript-Contextual-Analysis-with-Transformer-Models"><a href="#Identifying-Misinformation-on-YouTube-through-Transcript-Contextual-Analysis-with-Transformer-Models" class="headerlink" title="Identifying Misinformation on YouTube through Transcript Contextual Analysis with Transformer Models"></a>Identifying Misinformation on YouTube through Transcript Contextual Analysis with Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12155">http://arxiv.org/abs/2307.12155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/christoschr97/misinf-detection-llms">https://github.com/christoschr97/misinf-detection-llms</a></li>
<li>paper_authors: Christos Christodoulou, Nikos Salamanos, Pantelitsa Leonidou, Michail Papadakis, Michael Sirivianos</li>
<li>for: 本研究旨在提出一种新的视频分类方法，以确定视频内容的真实性。</li>
<li>methods: 本方法利用视频转cript中的文本内容，将传统的视频分类任务转化为文本分类任务。采用高级机器学习技术，如传输学习和少量学习。</li>
<li>results: 在三个dataset上进行评估，包括YouTube疫苗谣言相关视频、YouTube pseudoscience视频和一个新闻假消息集合。 fine-tuned模型的 Matthews Correlation Coefficient&gt;0.81，准确率&gt;0.90和F1 score&gt;0.90。而少量学习模型在YouTube pseudoscience数据集上比 fine-tuned模型高20%的准确率和F1 score。<details>
<summary>Abstract</summary>
Misinformation on YouTube is a significant concern, necessitating robust detection strategies. In this paper, we introduce a novel methodology for video classification, focusing on the veracity of the content. We convert the conventional video classification task into a text classification task by leveraging the textual content derived from the video transcripts. We employ advanced machine learning techniques like transfer learning to solve the classification challenge. Our approach incorporates two forms of transfer learning: (a) fine-tuning base transformer models such as BERT, RoBERTa, and ELECTRA, and (b) few-shot learning using sentence-transformers MPNet and RoBERTa-large. We apply the trained models to three datasets: (a) YouTube Vaccine-misinformation related videos, (b) YouTube Pseudoscience videos, and (c) Fake-News dataset (a collection of articles). Including the Fake-News dataset extended the evaluation of our approach beyond YouTube videos. Using these datasets, we evaluated the models distinguishing valid information from misinformation. The fine-tuned models yielded Matthews Correlation Coefficient>0.81, accuracy>0.90, and F1 score>0.90 in two of three datasets. Interestingly, the few-shot models outperformed the fine-tuned ones by 20% in both Accuracy and F1 score for the YouTube Pseudoscience dataset, highlighting the potential utility of this approach -- especially in the context of limited training data.
</details>
<details>
<summary>摘要</summary>
伪信息在YouTube上是一项重要的问题，需要 robust的检测策略。在这篇论文中，我们介绍了一种新的方法ology for video classification, ocus on 视频内容的真实性。我们将传统的视频分类任务转化为文本分类任务，通过利用视频字幕中的文本内容。我们采用了先进的机器学习技术，如传输学习，解决分类挑战。我们的方法包括两种形式的传输学习：（a）精度调整基于BERT、RoBERTa和ELECTRA的transformer模型，以及（b）几shot学习使用 sentence-transformers MPNet和RoBERTa-large。我们对三个数据集进行了应用：（a）YouTube疫苗谣言相关视频，（b）YouTube pseudo科学视频，以及（c） fake news数据集（一个收集了文章）。通过这些数据集，我们评估了我们的方法可以分辨真实信息和伪信息。精度调整模型的 Matthews Correlation Coefficient>0.81，准确率>0.90，和F1分数>0.90在三个数据集中都达到了。有意思的是，几shot模型在YouTube pseudo科学数据集上比精度调整模型高出20%的准确率和F1分数，这highlights了这种方法在有限的训练数据情况下的潜在实用性。
</details></li>
</ul>
<hr>
<h2 id="Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding"><a href="#Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding" class="headerlink" title="Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding"></a>Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12134">http://arxiv.org/abs/2307.12134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyoun Kim, Akshat Shrivastava, Duc Le, Ju Lin, Ozlem Kalinli, Michael L. Seltzer</li>
<li>for: 提高 END-to-END 语言理解系统的稳定性，增强对听写错误的耐受能力。</li>
<li>methods: 提出一种新的 END-to-END 语言理解系统，通过融合音频和文本表示来增强对听写错误的耐受能力，并采用两种新技术：1）有效地编码听写错误的质量信息，2）有效地将其集成到 END-to-END 语言理解模型中。</li>
<li>results: 在 STOP 数据集上实现了准确率的提高，并进行了分析，证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
End-to-end (E2E) spoken language understanding (SLU) systems that generate a semantic parse from speech have become more promising recently. This approach uses a single model that utilizes audio and text representations from pre-trained speech recognition models (ASR), and outperforms traditional pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems still show weakness when text representation quality is low due to ASR transcription errors. To overcome this issue, we propose a novel E2E SLU system that enhances robustness to ASR errors by fusing audio and text representations based on the estimated modality confidence of ASR hypotheses. We introduce two novel techniques: 1) an effective method to encode the quality of ASR hypotheses and 2) an effective approach to integrate them into E2E SLU models. We show accuracy improvements on STOP dataset and share the analysis to demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
最近，终端到终端（E2E）的语音理解系统（SLU）已经变得更加有前途。这种方法使用单个模型，利用采样和文本表示从预训练的语音识别模型（ASR）中获得，并在设备流动中超越传统的管道式SLU系统。然而，E2E SLU系统仍然在文本表示质量低下表现不佳，即使使用ASR转译错误。为解决这个问题，我们提出了一种新的E2E SLU系统，增强了ASR错误的鲁棒性。我们介绍了两种新技术：1）有效的ASR假设质量编码方法和2）有效的E2E SLU模型集成方法。我们在STOP数据集上显示了准确率改善，并进行分析，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Topic-Enhanced-Argument-Mining-from-Heterogeneous-Sources"><a href="#Explainable-Topic-Enhanced-Argument-Mining-from-Heterogeneous-Sources" class="headerlink" title="Explainable Topic-Enhanced Argument Mining from Heterogeneous Sources"></a>Explainable Topic-Enhanced Argument Mining from Heterogeneous Sources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12131">http://arxiv.org/abs/2307.12131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiasheng Si, Yingjie Zhu, Xingyu Shi, Deyu Zhou, Yulan He</li>
<li>for: 本文提出了一种新的可解释话题增强的论据挖掘方法，以提高论据挖掘的精度和效果。</li>
<li>methods: 本文使用了神经网络话题模型和语言模型，将目标信息补充了可解释话题表示，并通过共同学习来捕捉在论据中的句子水平话题信息。</li>
<li>results: 实验结果表明，提出的方法在benchmark数据集上在各种设置下都有显著优势，与现有基线模型相比。<details>
<summary>Abstract</summary>
Given a controversial target such as ``nuclear energy'', argument mining aims to identify the argumentative text from heterogeneous sources. Current approaches focus on exploring better ways of integrating the target-associated semantic information with the argumentative text. Despite their empirical successes, two issues remain unsolved: (i) a target is represented by a word or a phrase, which is insufficient to cover a diverse set of target-related subtopics; (ii) the sentence-level topic information within an argument, which we believe is crucial for argument mining, is ignored. To tackle the above issues, we propose a novel explainable topic-enhanced argument mining approach. Specifically, with the use of the neural topic model and the language model, the target information is augmented by explainable topic representations. Moreover, the sentence-level topic information within the argument is captured by minimizing the distance between its latent topic distribution and its semantic representation through mutual learning. Experiments have been conducted on the benchmark dataset in both the in-target setting and the cross-target setting. Results demonstrate the superiority of the proposed model against the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
Given a controversial target such as "核能源" (nuclear energy), argument mining aims to identify the argumentative text from heterogeneous sources. Current approaches focus on exploring better ways of integrating the target-associated semantic information with the argumentative text. Despite their empirical successes, two issues remain unsolved: (i) a target is represented by a word or a phrase, which is insufficient to cover a diverse set of target-related subtopics; (ii) the sentence-level topic information within an argument, which we believe is crucial for argument mining, is ignored. To tackle the above issues, we propose a novel explainable topic-enhanced argument mining approach. Specifically, with the use of the neural topic model and the language model, the target information is augmented by explainable topic representations. Moreover, the sentence-level topic information within the argument is captured by minimizing the distance between its latent topic distribution and its semantic representation through mutual learning. Experiments have been conducted on the benchmark dataset in both the in-target setting and the cross-target setting. Results demonstrate the superiority of the proposed model against the state-of-the-art baselines.Here's the word-for-word translation:给一个争议性目标，如“核能源”(nuclear energy), argument mining 目标是从不同来源中提取Argumentative text。现有的方法主要关注在更好地将目标相关的语义信息与 argumentative text 集成。despite their empirical successes, two issues remain unsolved: (i) a target is represented by a word or a phrase, which is insufficient to cover a diverse set of target-related subtopics; (ii) the sentence-level topic information within an argument, which we believe is crucial for argument mining, is ignored. To tackle the above issues, we propose a novel explainable topic-enhanced argument mining approach. Specifically, with the use of the neural topic model and the language model, the target information is augmented by explainable topic representations. Moreover, the sentence-level topic information within the argument is captured by minimizing the distance between its latent topic distribution and its semantic representation through mutual learning. Experiments have been conducted on the benchmark dataset in both the in-target setting and the cross-target setting. Results demonstrate the superiority of the proposed model against the state-of-the-art baselines.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.CL_2023_07_23/" data-id="clorjzl3g0087f1881jcjfv2x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.LG_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T10:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.LG_2023_07_23/">cs.LG - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations"><a href="#Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations" class="headerlink" title="Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?"></a>Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12344">http://arxiv.org/abs/2307.12344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ss-sun/right-for-the-wrong-reason">https://github.com/ss-sun/right-for-the-wrong-reason</a></li>
<li>paper_authors: Susu Sun, Lisa M. Koch, Christian F. Baumgartner</li>
<li>for:  This paper aims to evaluate the ability of various explanation techniques to identify spurious correlations in deep neural network models.</li>
<li>methods:  The paper proposes a rigorous evaluation strategy to assess the effectiveness of post-hoc explanation techniques and inherently interpretable classifiers in detecting artificially added confounders in a chest x-ray diagnosis task.</li>
<li>results:  The paper finds that the post-hoc technique SHAP and the inherently interpretable Attri-Net provide the best performance in identifying faulty model behavior and can be used to reliably detect spurious correlations.<details>
<summary>Abstract</summary>
While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net provide the best performance and can be used to reliably identify faulty model behavior.
</details>
<details>
<summary>摘要</summary>
深度神经网络模型可以提供无可比拟的分类性能，但它们容易学习假设关系。这些关系可能是由干扰信息引起的，这些干扰信息可以难以通过性能指标来探测，尤其如果测试数据来自同一个分布。可解释Machine Learning方法，如后处解释或自然可解释的分类器，承诺可以识别模型的错误思维。然而，现有证据表明，许多这些技术并没有充分能力完成这一任务。在这篇文章中，我们提出了一种严格的评估策略，用于评估解释技术的能力是否可以正确地识别假设关系。使用这种策略，我们评估了五种后处解释技术和一种自然可解释的分类器，以 Detect three types of artificially added confounders in a chest x-ray diagnosis task。我们发现，使用SHAP的后处解释技术以及自然可解释的Attri-Net可以提供最好的性能，可以可靠地识别模型的错误行为。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Audio-Based-Emotion-Recognition"><a href="#Self-Supervised-Learning-for-Audio-Based-Emotion-Recognition" class="headerlink" title="Self-Supervised Learning for Audio-Based Emotion Recognition"></a>Self-Supervised Learning for Audio-Based Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12343">http://arxiv.org/abs/2307.12343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peranut Nimitsurachat, Peter Washington<br>for: 这个研究的目的是发展一个基于音频资料的情绪识别模型，以便在心理健康、市场营销、游戏和社交媒体分析等领域中建立互动系统。methods: 这个研究使用了自我超级vised learning（SSL）方法，通过预测资料本身的特性来学习，不需要大量的指导标签。results: 这个研究发现，使用SSL方法可以在小量 annotated data 上提高模型的性能，特别是当情绪较易分类时。此外，这个研究还证明了SSL方法在嵌入特征表示空间中进行自我超级vised learning可以实现更好的表现。<details>
<summary>Abstract</summary>
Emotion recognition models using audio input data can enable the development of interactive systems with applications in mental healthcare, marketing, gaming, and social media analysis. While the field of affective computing using audio data is rich, a major barrier to achieve consistently high-performance models is the paucity of available training labels. Self-supervised learning (SSL) is a family of methods which can learn despite a scarcity of supervised labels by predicting properties of the data itself. To understand the utility of self-supervised learning for audio-based emotion recognition, we have applied self-supervised learning pre-training to the classification of emotions from the CMU- MOSEI's acoustic modality. Unlike prior papers that have experimented with raw acoustic data, our technique has been applied to encoded acoustic data. Our model is first pretrained to uncover the randomly-masked timestamps of the acoustic data. The pre-trained model is then fine-tuned using a small sample of annotated data. The performance of the final model is then evaluated via several evaluation metrics against a baseline deep learning model with an identical backbone architecture. We find that self-supervised learning consistently improves the performance of the model across all metrics. This work shows the utility of self-supervised learning for affective computing, demonstrating that self-supervised learning is most useful when the number of training examples is small, and that the effect is most pronounced for emotions which are easier to classify such as happy, sad and anger. This work further demonstrates that self-supervised learning works when applied to embedded feature representations rather than the traditional approach of pre-training on the raw input space.
</details>
<details>
<summary>摘要</summary>
这个文章探讨了使用语音资料进行情感识别的模型，并 explore了这些模型在心理健康、市场营销、游戏和社交媒体分析等领域的应用。然而，对于情感识别模型的训练label scarcity是一个主要的阻碍因素。自动学习（SSL）是一家 мето�odo，可以在训练labels的缺乏情况下培养出高性能的模型。为了了解SSL在语音基本情感识别中的使用效果，我们将运用SSL预训练在CMU-MOSEI的语音模式上进行分组。不同于先前的研究，我们的技术是对编码语音资料进行预训练。我们的模型首先预训练以探索随机遮盾的语音资料时间戳。预训练后，模型会被精确地调整使用一小sample的标注数据。最终的模型性能会通过一些评估度量与基准深度学习模型进行比较。我们发现，透过SSL预训练可以对情感识别模型进行改进，并且这个改进效果是随着标注数据的数量增加而加强。此外，我们发现这个效果尤其明显在易于分组的情感方面，例如：快乐、沮丧和愤怒。这个研究显示了自动学习在情感识别中的 utility，并且显示了这种方法在语音嵌入特征表现上进行预训练的效果。
</details></li>
</ul>
<hr>
<h2 id="Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction"><a href="#Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction" class="headerlink" title="Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction"></a>Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12341">http://arxiv.org/abs/2307.12341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lykourgos Chiniadis, Petros Tamvakis<br>for: 这个研究旨在提高农业生产和土壤物理特性分析，以实现农业均衡和环境可持续性。methods: 本研究使用FT NIR reflectanceспектроскопія和深度学习方法来预测土壤碳酸含量。results: 研究获得了优异的预测结果，并且在无法使用量imetric方法的情况下，可以快速和有效地预测土壤碳酸含量。<details>
<summary>Abstract</summary>
Soil NIR spectral absorbance/reflectance libraries are utilized towards improving agricultural production and analysis of soil properties which are key prerequisite for agroecological balance and environmental sustainability. Carbonates in particular, represent a soil property which is mostly affected even by mild, let alone extreme, changes of environmental conditions during climate change. In this study we propose a rapid and efficient way to predict carbonates content in soil by means of FT NIR reflectance spectroscopy and by use of deep learning methods. We exploited multiple machine learning methods, such as: 1) a MLP Regressor and 2) a CNN and compare their performance with other traditional ML algorithms such as PLSR, Cubist and SVM on the combined dataset of two NIR spectral libraries: KSSL (USDA), a dataset of soil samples reflectance spectra collected nationwide, and LUCAS TopSoil (European Soil Library) which contains soil sample absorbance spectra from all over the European Union, and use them to predict carbonate content on never before seen soil samples. Soil samples in KSSL and in TopSoil spectral libraries were acquired in the spectral region of visNIR, however in this study, only the NIR spectral region was utilized. Quantification of carbonates by means of Xray Diffraction is in good agreement with the volumetric method and the MLP prediction. Our work contributes to rapid carbonates content prediction in soil samples in cases where: 1) no volumetric method is available and 2) only NIR spectra absorbance data are available. Up till now and to the best of our knowledge, there exists no other study, that presents a prediction model trained on such an extensive dataset with such promising results on unseen data, undoubtedly supporting the notion that deep learning models present excellent prediction tools for soil carbonates content.
</details>
<details>
<summary>摘要</summary>
soil NIR  spectral absorbance/reflectance 图书馆是用于提高农业生产和土壤属性分析的重要前提，这些属性是生态平衡和环境可持续性的关键因素。碳酸盐是土壤属性中受到气候变化的影响最大的一种，因此在这种情况下，我们提出了一种快速和高效的碳酸盐含量预测方法，使用FT NIR 谐振谱分析和深度学习方法。我们利用了多种机器学习方法，如：1）多层感知网络（MLP）回归器，2）卷积神经网络（CNN），并与传统的机器学习算法如PLSR、Cubist和SVM进行比较，用于预测碳酸盐含量。我们使用了KSSL（USDA）和LUCAS TopSoil（欧盟土壤图书馆）两个 spectral 图书馆的共同数据集，其中KSSL包含了全美国的土壤样本谐振谱pectra，而LUCAS TopSoil包含了欧盟各国的土壤样本吸收谱spectra。我们只使用了NIR spectral 区域。我们通过X射晶 diffraction 测量和MLP 预测相比，发现了含碳酸盐的量与volume 方法具有良好一致性。我们的工作可以帮助在没有volume 方法可用时，只有NIR spectra 吸收数据可用时，快速预测土壤中碳酸盐含量。在我们所知道的范围内，没有其他研究可以在这样的广泛数据集上提出类似的预测模型，并且模型在未seen数据上的表现是非常出色，证明了深度学习模型在土壤碳酸盐含量预测中的优秀表现。
</details></li>
</ul>
<hr>
<h2 id="TabADM-Unsupervised-Tabular-Anomaly-Detection-with-Diffusion-Models"><a href="#TabADM-Unsupervised-Tabular-Anomaly-Detection-with-Diffusion-Models" class="headerlink" title="TabADM: Unsupervised Tabular Anomaly Detection with Diffusion Models"></a>TabADM: Unsupervised Tabular Anomaly Detection with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12336">http://arxiv.org/abs/2307.12336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Zamberg, Moshe Salhov, Ofir Lindenbaum, Amir Averbuch</li>
<li>for: 本研究旨在提出一种基于扩散的 probabilistic 模型，用于不监督的异常检测。</li>
<li>methods: 我们的模型通过使用特殊的拒绝机制，使正常样本的浓度估计免受异常样本的影响。在推断阶段，我们可以通过查找低浓度区域的样本来识别异常样本。</li>
<li>results: 我们使用实际数据进行测试，发现我们的方法可以提高异常检测的能力，并且相比基eline，我们的方法更加稳定和不需要较多的超参数调整。<details>
<summary>Abstract</summary>
Tables are an abundant form of data with use cases across all scientific fields. Real-world datasets often contain anomalous samples that can negatively affect downstream analysis. In this work, we only assume access to contaminated data and present a diffusion-based probabilistic model effective for unsupervised anomaly detection. Our model is trained to learn the density of normal samples by utilizing a unique rejection scheme to attenuate the influence of anomalies on the density estimation. At inference, we identify anomalies as samples in low-density regions. We use real data to demonstrate that our method improves detection capabilities over baselines. Furthermore, our method is relatively stable to the dimension of the data and does not require extensive hyperparameter tuning.
</details>
<details>
<summary>摘要</summary>
tables are an abundant form of data with use cases across all scientific fields. Real-world datasets often contain anomalous samples that can negatively affect downstream analysis. In this work, we only assume access to contaminated data and present a diffusion-based probabilistic model effective for unsupervised anomaly detection. Our model is trained to learn the density of normal samples by utilizing a unique rejection scheme to attenuate the influence of anomalies on the density estimation. At inference, we identify anomalies as samples in low-density regions. We use real data to demonstrate that our method improves detection capabilities over baselines. Furthermore, our method is relatively stable to the dimension of the data and does not require extensive hyperparameter tuning.Here's the translation in Traditional Chinese as well:tables are an abundant form of data with use cases across all scientific fields. Real-world datasets often contain anomalous samples that can negatively affect downstream analysis. In this work, we only assume access to contaminated data and present a diffusion-based probabilistic model effective for unsupervised anomaly detection. Our model is trained to learn the density of normal samples by utilizing a unique rejection scheme to attenuate the influence of anomalies on the density estimation. At inference, we identify anomalies as samples in low-density regions. We use real data to demonstrate that our method improves detection capabilities over baselines. Furthermore, our method is relatively stable to the dimension of the data and does not require extensive hyperparameter tuning.
</details></li>
</ul>
<hr>
<h2 id="An-axiomatized-PDE-model-of-deep-neural-networks"><a href="#An-axiomatized-PDE-model-of-deep-neural-networks" class="headerlink" title="An axiomatized PDE model of deep neural networks"></a>An axiomatized PDE model of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12333">http://arxiv.org/abs/2307.12333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tangjun Wang, Wenqi Tao, Chenglong Bao, Zuoqiang Shi</li>
<li>for: 研究深度神经网络（DNN）与 partial differential equations（PDEs）之间的关系，尤其是 DNN 的普遍形式 PDE 模型。</li>
<li>methods: 将 DNN 视为一个进化Operator，从简单的基模型出发，根据一些合理的假设，证明了演化Operator 实际上是受湍涨-扩散方程的推动。</li>
<li>results: 根据演化Operator 的推动，提出了一种新的训练方法，用于改进 ResNet 的性能。实验 validate 了提出的方法的效果。<details>
<summary>Abstract</summary>
Inspired by the relation between deep neural network (DNN) and partial differential equations (PDEs), we study the general form of the PDE models of deep neural networks. To achieve this goal, we formulate DNN as an evolution operator from a simple base model. Based on several reasonable assumptions, we prove that the evolution operator is actually determined by convection-diffusion equation. This convection-diffusion equation model gives mathematical explanation for several effective networks. Moreover, we show that the convection-diffusion model improves the robustness and reduces the Rademacher complexity. Based on the convection-diffusion equation, we design a new training method for ResNets. Experiments validate the performance of the proposed method.
</details>
<details>
<summary>摘要</summary>
Based on the relation between deep neural networks (DNN) and partial differential equations (PDEs), we investigate the general form of PDE models of DNN. To achieve this goal, we formulate DNN as an evolution operator from a simple base model. Under several reasonable assumptions, we prove that the evolution operator is actually determined by a convection-diffusion equation. This convection-diffusion equation model provides a mathematical explanation for several effective networks. Moreover, we show that the convection-diffusion model improves the robustness and reduces the Rademacher complexity. Based on the convection-diffusion equation, we propose a new training method for ResNets. Experimental results validate the performance of the proposed method.Here's the word-for-word translation of the text into Simplified Chinese: Based on Deep Neural Network (DNN) 和 Partial Differential Equations (PDEs) 之间的关系，我们研究 DNN 的总体形式 PDE 模型。为达到这个目标，我们将 DNN 表示为一个从简单基本模型的演化运算器。根据一些合理的假设，我们证明了演化运算器实际上是受湍振-漫步方程的决定。这个湍振-漫步方程模型为许多有效网络提供了数学解释。此外，我们还证明了湍振-漫步模型可以提高稳定性并降低 Rademacher 复杂度。基于湍振-漫步方程，我们提出了一种新的训练方法 для ResNets。实验结果证明了我们的方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks"><a href="#Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks" class="headerlink" title="Tackling the Curse of Dimensionality with Physics-Informed Neural Networks"></a>Tackling the Curse of Dimensionality with Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12306">http://arxiv.org/abs/2307.12306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, Kenji Kawaguchi</li>
<li>for: 解决高维纬度的物理学定义问题 (solving high-dimensional physical definition problems)</li>
<li>methods: 使用Stochastic Dimension Gradient Descent (SDGD)方法，即将梯度分解成不同维度的部分，并随机选择每个训练轮中的一部分维度进行训练physics-informed neural networks (PINNs)。 (using Stochastic Dimension Gradient Descent (SDGD) method, which decomposes the gradient into parts corresponding to different dimensions and randomly selects a subset of these dimensional parts for training physics-informed neural networks (PINNs))</li>
<li>results: 可以很快地解决很多难以解决的高维度纬度的非线性Partial Differential Equations (PDEs)，例如Hamilton-Jacobi-Bellman (HJB)和Schrödinger方程在千个维度中的解决。 (can solve many notoriously hard high-dimensional PDEs, such as the Hamilton-Jacobi-Bellman (HJB) and the Schrödinger equations in thousands of dimensions very fast)<details>
<summary>Abstract</summary>
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed method. We experimentally demonstrate that the proposed method allows us to solve many notoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman (HJB) and the Schr\"{o}dinger equations in thousands of dimensions very fast on a single GPU using the PINNs mesh-free approach. For instance, we solve nontrivial nonlinear PDEs (one HJB equation and one Black-Scholes equation) in 100,000 dimensions in 6 hours on a single GPU using SDGD with PINNs. Since SDGD is a general training methodology of PINNs, SDGD can be applied to any current and future variants of PINNs to scale them up for arbitrary high-dimensional PDEs.
</details>
<details>
<summary>摘要</summary>
“几何约束”（CoD）会卷用计算资源，计算成本随着维度的增加而呈指数增长。这会对解决高维 partial differential equations（PDEs） pose 大量挑战，Richard Bellman 在60年前就这样注意到。虽然在过去几年，有些研究者通过数值方法解决高维 PDEs，但这些计算却是非常昂贵的，并且true scaling of general nonlinear PDEs to high dimensions 从未实现过。在这篇论文中，我们开发了一种新的方法，即随机维度梯度下降（SDGD），用于扩展 physics-informed neural networks（PINNs）来解决任意高维 PDEs。SDGD 方法将 PDE 的梯度分解成不同维度的部分，并在训练 PINNs 时随机选择这些维度的部分。我们理论上证明了该方法的收敛性和其他愿望的性质。我们实验表明，该方法可以很快地解决许多知名度的高维 PDEs，包括 Hamilton-Jacobi-Bellman 方程和 Schrödinger 方程，并且可以在单个 GPU 上完成。例如，我们在 100,000 维度中解决了一些非线性 PDEs（一个 HJB 方程和一个 Black-Scholes 方程），只用了 6 个 GPU 上的 6 小时。由于 SDGD 是 PINNs 的一种通用训练方法，SDGD 可以应用于任何当前和未来的 PINNs 变体，以扩展它们到任意高维 PDEs。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Machine-Learning-of-Argon-Gas-Driven-Melt-Pool-Dynamics"><a href="#Physics-Informed-Machine-Learning-of-Argon-Gas-Driven-Melt-Pool-Dynamics" class="headerlink" title="Physics-Informed Machine Learning of Argon Gas-Driven Melt Pool Dynamics"></a>Physics-Informed Machine Learning of Argon Gas-Driven Melt Pool Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12304">http://arxiv.org/abs/2307.12304</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Sharma, W. Grace Guo, M. Raissi, Y. B. Guo</li>
<li>for: 这 paper 是关于 metal 添加印制 (AM) 过程中溶融池动态的研究，它们的目的是提高过程的稳定性、微结构形成和印制物的性能。</li>
<li>methods: 这 paper 使用了物理学习 (PIML) 方法，通过将神经网络与物理法律相结合来预测溶融池动态，包括温度、速度和压力等参数。PIML 方法可以避免使用数学模拟方法，从而大幅降低计算成本。</li>
<li>results: 该 paper 通过数据驱动发现了模型常数，并且通过优化 PINN 模型来提高模型训练效率。PIML 方法可以高效地预测溶融池动态，并且可以提供更好的初始条件和边界条件。<details>
<summary>Abstract</summary>
Melt pool dynamics in metal additive manufacturing (AM) is critical to process stability, microstructure formation, and final properties of the printed materials. Physics-based simulation including computational fluid dynamics (CFD) is the dominant approach to predict melt pool dynamics. However, the physics-based simulation approaches suffer from the inherent issue of very high computational cost. This paper provides a physics-informed machine learning (PIML) method by integrating neural networks with the governing physical laws to predict the melt pool dynamics such as temperature, velocity, and pressure without using any training data on velocity. This approach avoids solving the highly non-linear Navier-Stokes equation numerically, which significantly reduces the computational cost. The difficult-to-determine model constants of the governing equations of the melt pool can also be inferred through data-driven discovery. In addition, the physics-informed neural network (PINN) architecture has been optimized for efficient model training. The data-efficient PINN model is attributed to the soft penalty by incorporating governing partial differential equations (PDEs), initial conditions, and boundary conditions in the PINN model.
</details>
<details>
<summary>摘要</summary>
金属加料制造（AM）中的熔 pool 动力学是制造过程稳定性、微结构形成和打印物质的关键因素。基于物理定律的数学模拟（CFD）是预测熔 pool 动力学的主要方法。然而，物理基础的模拟方法受到内置的计算成本高峰问题。这篇论文提出了基于物理学习（PIML）方法，通过将神经网络与管理物理法律相结合来预测熔 pool 动力学的温度、速度和压力，不需要使用任何很速度训练数据。这种方法可以避免数值方法中的高级非线性 Navier-Stokes 方程的解算问题，从而减少计算成本。此外，通过数据驱动发现，可以通过推断模型常数来确定管理方程的困难常数。此外，基于物理学习（PINN）架构已经优化了模型训练效率。通过软约束 penalty，将管理的partial differential equations（PDEs）、初始条件和边界条件 integrate 到 PINN 模型中，使得数据效率的 PINN 模型。
</details></li>
</ul>
<hr>
<h2 id="RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC"><a href="#RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC" class="headerlink" title="RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC"></a>RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12301">http://arxiv.org/abs/2307.12301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mxtsai/ransac-nn">https://github.com/mxtsai/ransac-nn</a></li>
<li>paper_authors: Chen-Han Tsai, Yu-Shao Peng</li>
<li>for: 这个论文旨在提出一种专门为图像数据设计的异常检测算法，以确保计算机视觉任务中使用的图像数据质量和准确性。</li>
<li>methods: 该算法基于RANSAC的方法进行比较图像，自动预测每个图像的异常分数而无需额外训练或标签信息。</li>
<li>results: 对于15种多样化的数据集，RANSAC-NN在与当前状态艺算法进行比较时，无需任何超参数调整，一致地表现出优异性。此外，文章还提供了每个RANSAC-NN组件的详细分析，并展示了其在图像涂抹检测中的潜在应用。<details>
<summary>Abstract</summary>
Image outlier detection (OD) is crucial for ensuring the quality and accuracy of image datasets used in computer vision tasks. The majority of OD algorithms, however, have not been targeted toward image data. Consequently, the results of applying such algorithms to images are often suboptimal. In this work, we propose RANSAC-NN, a novel unsupervised OD algorithm specifically designed for images. By comparing images in a RANSAC-based approach, our algorithm automatically predicts the outlier score of each image without additional training or label information. We evaluate RANSAC-NN against state-of-the-art OD algorithms on 15 diverse datasets. Without any hyperparameter tuning, RANSAC-NN consistently performs favorably in contrast to other algorithms in almost every dataset category. Furthermore, we provide a detailed analysis to understand each RANSAC-NN component, and we demonstrate its potential applications in image mislabeled detection. Code for RANSAC-NN is provided at https://github.com/mxtsai/ransac-nn
</details>
<details>
<summary>摘要</summary>
图像异常检测（OD）是计算机视觉任务中至关重要的质量和准确性因素。大多数OD算法 however，没有特地针对图像数据。因此，将这些算法应用于图像时的结果通常不佳。在这项工作中，我们提出了一种新的无监督OD算法，即RANSAC-NN。我们通过对图像进行RANSAC-based的比较，自动地对每个图像预测异常分数，无需额外的训练或标签信息。我们对RANSAC-NN与现有OD算法进行了15种多样化的数据集评估。无需任何超参数调整，RANSAC-NN在大多数数据集类别中一直表现优于其他算法。此外，我们还提供了每个RANSAC-NN组件的详细分析，并证明其在图像涂抹检测中的潜在应用。RANSAC-NN代码可以在https://github.com/mxtsai/ransac-nn上获取。
</details></li>
</ul>
<hr>
<h2 id="ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder"><a href="#ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder" class="headerlink" title="ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder"></a>ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12255">http://arxiv.org/abs/2307.12255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youzhi Liang, Wen Liang</li>
<li>For: This paper proposes a deep learning architecture for fingerprint image denoising in compact IoT devices, aiming to improve the reliability of biometric authentication systems.* Methods: The proposed method, called Residual Wavelet-Conditioned Convolutional Autoencoder (Res-WCAE), combines image and wavelet encoders with a Kullback-Leibler divergence regularization. It leverages residual connections and wavelet-transform domain features to preserve fine-grained spatial information.* Results: The experimental results show that Res-WCAE outperforms several state-of-the-art denoising methods, particularly for heavily degraded fingerprint images with high levels of noise. The proposed method demonstrates promise for improving the reliability of biometric authentication systems in compact IoT devices.Here’s the simplified Chinese text for the three key points:* For: 这篇论文提出了一种用于 compact IoT 设备中的指纹图像干扰 removing 深度学习架构，以提高生物识别系统的可靠性。* Methods: 提议的方法是 Residual Wavelet-Conditioned Convolutional Autoencoder (Res-WCAE)，它组合了图像编码器和波峰编码器，并使用 Kullback-Leibler 分布regularization。它利用了剩余连接和波峰变换域特征来保留细腻的空间信息。* Results: 实验结果表明，Res-WCAE 比许多状态机器人的干扰方法更高效，特别是对于受到高水平噪声的指纹图像。提议的方法表明，可以提高 compact IoT 设备中生物识别系统的可靠性。<details>
<summary>Abstract</summary>
The utilization of biometric authentication with pattern images is increasingly popular in compact Internet of Things (IoT) devices. However, the reliability of such systems can be compromised by image quality issues, particularly in the presence of high levels of noise. While state-of-the-art deep learning algorithms designed for generic image denoising have shown promise, their large number of parameters and lack of optimization for unique biometric pattern retrieval make them unsuitable for these devices and scenarios. In response to these challenges, this paper proposes a lightweight and robust deep learning architecture, the Residual Wavelet-Conditioned Convolutional Autoencoder (Res-WCAE) with a Kullback-Leibler divergence (KLD) regularization, designed specifically for fingerprint image denoising. Res-WCAE comprises two encoders - an image encoder and a wavelet encoder - and one decoder. Residual connections between the image encoder and decoder are leveraged to preserve fine-grained spatial features, where the bottleneck layer conditioned on the compressed representation of features obtained from the wavelet encoder using approximation and detail subimages in the wavelet-transform domain. The effectiveness of Res-WCAE is evaluated against several state-of-the-art denoising methods, and the experimental results demonstrate that Res-WCAE outperforms these methods, particularly for heavily degraded fingerprint images in the presence of high levels of noise. Overall, Res-WCAE shows promise as a solution to the challenges faced by biometric authentication systems in compact IoT devices.
</details>
<details>
<summary>摘要</summary>
现在互联网物联网设备中越来越普遍使用生物特征认证，特别是使用图像特征进行身份验证。然而，这些系统的可靠性可能受到图像质量问题的影响，特别是在高噪声水平下。当前的深度学习算法，专门为普通图像除噪设计的深度学习模型，虽然已经达到了一定的成绩，但它们的参数数量很大，并且没有特定生物特征检索优化，因此不适合这些设备和场景。为了解决这些挑战，本文提出了一种轻量级和可靠的深度学习架构——差分波let-conditioned Convolutional Autoencoder（Res-WCAE），并在其中添加了Kullback-Leibler异质（KLD）正则化。Res-WCAE包括两个编码器——图像编码器和波лет编码器——以及一个解码器。图像编码器和解码器之间的差分连接，使得图像的细腻特征得到保留，而波лет编码器使用波лет变换获得的特征的压缩表示，并通过差分连接和权重Conditioning来与图像编码器进行交互。对比多种现有的去噪方法，Res-WCAE的效果得到了证明，特别是在高噪声水平下的极大噪声图像认证 task。总的来说，Res-WCAE表现出了在compact IoT设备中的可靠性和灵活性，并且有望成为生物认证系统中的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Depression-Detection-via-Head-Motion-Patterns"><a href="#Explainable-Depression-Detection-via-Head-Motion-Patterns" class="headerlink" title="Explainable Depression Detection via Head Motion Patterns"></a>Explainable Depression Detection via Head Motion Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12241">http://arxiv.org/abs/2307.12241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Gahalawat, Raul Fernandez Rojas, Tanaya Guha, Ramanathan Subramanian, Roland Goecke</li>
<li>for: 检测抑郁症状</li>
<li>methods: 基于head motion数据的基本运动单元（kinemes）和机器学习方法</li>
<li>results:  head motion patterns 可以作为抑郁症状的生物标志，并且可以通过基于征料的方法来分类抑郁和健康控制组Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to detect depression symptoms using head motion data and machine learning methods.</li>
<li>methods: The paper uses two approaches to detect depression: (a) discovering kinemes from head motion data corresponding to both depressed patients and healthy controls, and (b) learning kineme patterns only from healthy controls, and computing statistics derived from reconstruction errors for both the patient and control classes.</li>
<li>results: The paper finds that head motion patterns are effective biomarkers for detecting depressive symptoms, and that explanatory kineme patterns consistent with prior findings can be observed for the two classes. The paper achieves peak F1 scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 datasets for binary classification over episodic thin-slices, and a peak F1 of 0.72 over videos for AVEC2013.<details>
<summary>Abstract</summary>
While depression has been studied via multimodal non-verbal behavioural cues, head motion behaviour has not received much attention as a biomarker. This study demonstrates the utility of fundamental head-motion units, termed \emph{kinemes}, for depression detection by adopting two distinct approaches, and employing distinctive features: (a) discovering kinemes from head motion data corresponding to both depressed patients and healthy controls, and (b) learning kineme patterns only from healthy controls, and computing statistics derived from reconstruction errors for both the patient and control classes. Employing machine learning methods, we evaluate depression classification performance on the \emph{BlackDog} and \emph{AVEC2013} datasets. Our findings indicate that: (1) head motion patterns are effective biomarkers for detecting depressive symptoms, and (2) explanatory kineme patterns consistent with prior findings can be observed for the two classes. Overall, we achieve peak F1 scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 for binary classification over episodic \emph{thin-slices}, and a peak F1 of 0.72 over videos for AVEC2013.
</details>
<details>
<summary>摘要</summary>
研究表示，诊断抑郁症可以通过多modal非语言行为迹象来进行。然而，头部运动行为尚未得到很多关注，作为生物标记。本研究演示了基本头部运动单元（kinemes）在抑郁检测中的有用性，通过采用两种不同的方法和特征：（a）从头部运动数据中提取与抑郁患者和健康控制人群相对应的kinemes，以及（b）通过健康控制人群学习kineme模式，并计算来自重建错误的统计。使用机器学习方法，我们评估了抑郁分类性能在BlackDog和AVEC2013数据集上。我们发现：（1）头部运动模式是抑郁症的有效生物标记，和（2）可以观察到健康控制人群和抑郁患者之间的准确的kineme模式。总的来说，我们在BlackDog和AVEC2013数据集上实现了最高的F1分数为0.79和0.82，分别为 binary 分类EPISODE 薄片和视频。
</details></li>
</ul>
<hr>
<h2 id="Demonstration-of-a-Response-Time-Based-Remaining-Useful-Life-RUL-Prediction-for-Software-Systems"><a href="#Demonstration-of-a-Response-Time-Based-Remaining-Useful-Life-RUL-Prediction-for-Software-Systems" class="headerlink" title="Demonstration of a Response Time Based Remaining Useful Life (RUL) Prediction for Software Systems"></a>Demonstration of a Response Time Based Remaining Useful Life (RUL) Prediction for Software Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12237">http://arxiv.org/abs/2307.12237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ray Islam, Peter Sandborn</li>
<li>for: 这个论文旨在应用PHM概念到软件系统中，以预测问题和计算系统的RUL。</li>
<li>methods: 本论文使用了usage参数（例如发布数量和类别）和性能参数（例如响应时间）来预测RUL。</li>
<li>results: 研究人员通过对实际数据进行比较，发现PHM概念可以应用于软件系统，并且可以计算出RUL来做系统管理决策。<details>
<summary>Abstract</summary>
Prognostic and Health Management (PHM) has been widely applied to hardware systems in the electronics and non-electronics domains but has not been explored for software. While software does not decay over time, it can degrade over release cycles. Software health management is confined to diagnostic assessments that identify problems, whereas prognostic assessment potentially indicates when in the future a problem will become detrimental. Relevant research areas such as software defect prediction, software reliability prediction, predictive maintenance of software, software degradation, and software performance prediction, exist, but all of these represent diagnostic models built upon historical data, none of which can predict an RUL for software. This paper addresses the application of PHM concepts to software systems for fault predictions and RUL estimation. Specifically, this paper addresses how PHM can be used to make decisions for software systems such as version update and upgrade, module changes, system reengineering, rejuvenation, maintenance scheduling, budgeting, and total abandonment. This paper presents a method to prognostically and continuously predict the RUL of a software system based on usage parameters (e.g., the numbers and categories of releases) and performance parameters (e.g., response time). The model developed has been validated by comparing actual data, with the results that were generated by predictive models. Statistical validation (regression validation, and k-fold cross validation) has also been carried out. A case study, based on publicly available data for the Bugzilla application is presented. This case study demonstrates that PHM concepts can be applied to software systems and RUL can be calculated to make system management decisions.
</details>
<details>
<summary>摘要</summary>
预测和健康管理（PHM）已广泛应用于硬件系统中，但尚未探讨软件领域。虽然软件不会逝减，但可能会逐渐下降。软件健康管理仅仅是诊断评估，而预测评估可能会预测未来哪一天问题会变得严重。有关研究领域包括软件缺陷预测、软件可靠性预测、软件维护预测、软件衰老和软件性能预测，但这些都是基于历史数据建立的诊断模型，无法预测软件的寿命。本文探讨将PHM概念应用于软件系统中，以预测问题和计算软件系统的寿命。具体来说，本文探讨了如何使用PHM来做软件系统的决策，如版本更新和升级、模块更改、系统重构、重新生成、维护计划、预算和完全废弃。本文提出了一种基于使用量和性能参数预测软件系统的寿命的方法。该模型已经验证了，并通过与预测模型生成的结果进行比较。此外，还进行了统计验证（回归验证和Kfold跨验证）。一个基于公共数据的 Bugzilla 应用程序的案例研究也被提出，这个案例示出了PHM概念可以应用于软件系统，并且可以计算软件系统的寿命以进行系统管理决策。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO"><a href="#Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO" class="headerlink" title="Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO"></a>Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12236">http://arxiv.org/abs/2307.12236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longxiang Zhang, Wenping Wang</li>
<li>for: 本研究旨在为串流服务提供商评估电竞技巧，以便为客户提供个性化推荐和服务促销。</li>
<li>methods: 本研究使用最新的端到端模型学习joint representation of multiple modalities，并进行了大量的实验证明其效果。</li>
<li>results: 研究发现，提议的模型具有识别用户的弱点，而不是学习有意义的表示。未来工作将解决这个问题。<details>
<summary>Abstract</summary>
Online streaming is an emerging market that address much attention. Assessing gaming skills from videos is an important task for streaming service providers to discover talented gamers. Service providers require the information to offer customized recommendation and service promotion to their customers. Meanwhile, this is also an important multi-modal machine learning tasks since online streaming combines vision, audio and text modalities. In this study we begin by identifying flaws in the dataset and proceed to clean it manually. Then we propose several variants of latest end-to-end models to learn joint representation of multiple modalities. Through our extensive experimentation, we demonstrate the efficacy of our proposals. Moreover, we identify that our proposed models is prone to identifying users instead of learning meaningful representations. We purpose future work to address the issue in the end.
</details>
<details>
<summary>摘要</summary>
在线流媒体是一个崛起的市场，吸引了很多注意。通过视频评估游戏技巧是流媒体服务提供者为发掘才华的玩家提供重要的任务。服务提供者需要这些信息以为客户提供个性化推荐和服务促销。同时，这也是一项重要的多modal机器学习任务，因为在线流媒体结合了视觉、音频和文本模式。在本研究中，我们首先发现数据集中的缺陷，然后手动清理数据。然后，我们提出了多种最新的端到端模型，以学习多Modalities的共同表示。通过我们的广泛实验，我们证明了我们的提议的有效性。另外，我们发现我们的提议模型容易被用户认出，而不是学习有意义的表示。我们在结尾提出了未来的工作，以解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms"><a href="#EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms" class="headerlink" title="EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms"></a>EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12229">http://arxiv.org/abs/2307.12229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masoudmo/echoglad">https://github.com/masoudmo/echoglad</a></li>
<li>paper_authors: Masoud Mokhtari, Mobina Mahdavi, Hooman Vaseli, Christina Luong, Purang Abolmaesumi, Teresa S. M. Tsang, Renjie Liao</li>
<li>for: 这 paper 的目的是自动检测心脏左心室的四个标志点和测量左心室内部的尺寸和周围肌肉的大约质量。</li>
<li>methods: 这 paper 使用了一种基于 echo cardiogram 的层次 graph neural network (GNN)，以实现左心室标志点检测。</li>
<li>results: 这 paper 在一个公共数据集和一个私有数据集上进行了评估，在内分布 (ID) 和外分布 (OOD) 两种设置下， achieved  state-of-the-art 的 Mean Absolute Error (MAE) 值为 1.46 mm 和 1.86 mm，并且在 OOD 设置下表现更好。<details>
<summary>Abstract</summary>
The functional assessment of the left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle. The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss. We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.
</details>
<details>
<summary>摘要</summary>
functional assessment of the left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle. The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss. We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.Here's the translation in Traditional Chinese:函数评估左心室的 left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle. The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss. We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.
</details></li>
</ul>
<hr>
<h2 id="The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery"><a href="#The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery" class="headerlink" title="The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery"></a>The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02502">http://arxiv.org/abs/2308.02502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Keith Wilkinson</li>
<li>for: 这个研究旨在使用人工智能技术和卫星图像来识别Cyprus农村地区的非法垃圾弃置。</li>
<li>methods: 这个研究使用了人工智能技术和卫星图像来识别垃圾，首先收集了一个小型数据集，然后使用数据扩展技术来增加数据量，然后训练了一个 convolutional neural network（CNN）来识别垃圾。</li>
<li>results: 这个研究得到了一个深度学习模型，可以在90%的情况下正确地识别垃圾图像。这个模型可以成为未来Cyprus岛上的垃圾映射系统的基础。<details>
<summary>Abstract</summary>
Garbage disposal is a challenging problem throughout the developed world. In Cyprus, as elsewhere, illegal ``fly-tipping" is a significant issue, especially in rural areas where few legal garbage disposal options exist. However, there is a lack of studies that attempt to measure the scale of this problem, and few resources available to address it. A method of automating the process of identifying garbage dumps would help counter this and provide information to the relevant authorities. The aim of this study was to investigate the degree to which artificial intelligence techniques, together with satellite imagery, can be used to identify illegal garbage dumps in the rural areas of Cyprus. This involved collecting a novel dataset of images that could be categorised as either containing, or not containing, garbage. The collection of such datasets in sufficient raw quantities is time consuming and costly. Therefore a relatively modest baseline set of images was collected, then data augmentation techniques used to increase the size of this dataset to a point where useful machine learning could occur. From this set of images an artificial neural network was trained to recognise the presence or absence of garbage in new images. A type of neural network especially suited to this task known as ``convolutional neural networks" was used. The efficacy of the resulting model was evaluated using an independently collected dataset of test images. The result was a deep learning model that could correctly identify images containing garbage in approximately 90\% of cases. It is envisaged that this model could form the basis of a future system that could systematically analyse the entire landscape of Cyprus to build a comprehensive ``garbage" map of the island.
</details>
<details>
<summary>摘要</summary>
垃圾处理是发达国家的一个挑战。在塞浦路斯，如其他地方一样，非法“飞tipping”是一个严重的问题，特别是在农村地区，其法定垃圾处理选择较少。然而，有很少的研究尝试量化这个问题，而且有限的资源来解决它。这项研究的目的是使用人工智能技术和卫星图像来识别塞浦路斯农村地区非法垃圾排放。这包括收集一个新的图像集，这些图像可以分为含垃圾和不含垃圾两类。收集这些图像集的过程是时间consuming和成本高的。因此，我们只收集了一个相对较小的基线集的图像，然后使用数据扩展技术来增加这个集的大小，以便进行有用的机器学习。从这些图像中，我们用人工神经网络来识别新图像中是否含垃圾。我们使用的是一种适合这种任务的特殊类型的神经网络，即“卷积神经网络”。我们评估了这种模型的效果，使用独立收集的测试集。结果是一个深度学习模型，可以在90%的情况下正确地识别含垃圾的图像。我们可以基于这个模型，建立一个将系统地分析整个塞浦路斯岛的系统，并建立一个“垃圾”地图。
</details></li>
</ul>
<hr>
<h2 id="Geometry-Aware-Adaptation-for-Pretrained-Models"><a href="#Geometry-Aware-Adaptation-for-Pretrained-Models" class="headerlink" title="Geometry-Aware Adaptation for Pretrained Models"></a>Geometry-Aware Adaptation for Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12226">http://arxiv.org/abs/2307.12226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Roberts, Xintong Li, Dyah Adila, Sonia Cromp, Tzu-Heng Huang, Jitian Zhao, Frederic Sala</li>
<li>for: 提高零shot预测性能和适应新类预测</li>
<li>methods: 利用度量学空间信息进行适应和预测改进</li>
<li>results: 在ImageNet上实现了29.7%的相对改进，并且可以扩展到千万类的预测任务。当无外部度量时，可以使用自动生成的度量从类嵌入中获得10.5%的改进。<details>
<summary>Abstract</summary>
Machine learning models -- including prominent zero-shot models -- are often trained on datasets whose labels are only a small proportion of a larger label space. Such spaces are commonly equipped with a metric that relates the labels via distances between them. We propose a simple approach to exploit this information to adapt the trained model to reliably predict new classes -- or, in the case of zero-shot prediction, to improve its performance -- without any additional training. Our technique is a drop-in replacement of the standard prediction rule, swapping argmax with the Fr\'echet mean. We provide a comprehensive theoretical analysis for this approach, studying (i) learning-theoretic results trading off label space diameter, sample complexity, and model dimension, (ii) characterizations of the full range of scenarios in which it is possible to predict any unobserved class, and (iii) an optimal active learning-like next class selection procedure to obtain optimal training classes for when it is not possible to predict the entire range of unobserved classes. Empirically, using easily-available external metrics, our proposed approach, Loki, gains up to 29.7% relative improvement over SimCLR on ImageNet and scales to hundreds of thousands of classes. When no such metric is available, Loki can use self-derived metrics from class embeddings and obtains a 10.5% improvement on pretrained zero-shot models such as CLIP.
</details>
<details>
<summary>摘要</summary>
机器学习模型 -- 包括知名的零批量模型 -- 常常在具有小比例标签空间的数据集上训练。这些空间通常具有一个度量关系标签之间的距离。我们提出了一种简单的方法，利用这些信息来适应训练过的模型预测新类 -- 或者在零批量预测中提高性能 --  без需要额外训练。我们的技术是将欧几何均值换取作为标准预测规则的替换。我们提供了全面的理论分析，研究（i）标签空间径距、样本复杂度和模型维度之间的学习理论结果，（ii）可预测任何未见类的全范围描述，以及（iii）针对不能预测整个未见类范围时的优化的活动学习样本选择方法。实际上，我们的提议方法Loki在ImageNet上实现了Relative improvement为29.7%，并且可以扩展到万个类。当没有外部度量时，Loki可以使用自带的类嵌入度量，实现预测零批量模型CLIP的10.5%提高。
</details></li>
</ul>
<hr>
<h2 id="Improving-Out-of-Distribution-Robustness-of-Classifiers-via-Generative-Interpolation"><a href="#Improving-Out-of-Distribution-Robustness-of-Classifiers-via-Generative-Interpolation" class="headerlink" title="Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation"></a>Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12219">http://arxiv.org/abs/2307.12219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyue Bai, Ceyuan Yang, Yinghao Xu, S. -H. Gary Chan, Bolei Zhou</li>
<li>for: 提高神经网络模型对于不同分布数据的鲁棒性</li>
<li>methods: 使用生成模型作为数据增强源，通过混合多个域的生成模型并在 interpolate 模型参数来生成多元的OoD样本</li>
<li>results: 实验结果显示，提出的方法可以明显提高神经网络模型对于不同分布数据的鲁棒性，并且可以控制增强的方向和强度<details>
<summary>Abstract</summary>
Deep neural networks achieve superior performance for learning from independent and identically distributed (i.i.d.) data. However, their performance deteriorates significantly when handling out-of-distribution (OoD) data, where the training and test are drawn from different distributions. In this paper, we explore utilizing the generative models as a data augmentation source for improving out-of-distribution robustness of neural classifiers. Specifically, we develop a simple yet effective method called Generative Interpolation to fuse generative models trained from multiple domains for synthesizing diverse OoD samples. Training a generative model directly on the source domains tends to suffer from mode collapse and sometimes amplifies the data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators where their model parameters have the same initialization thus are aligned. We then linearly interpolate the model parameters of the generators to spawn new sets of generators. Such interpolated generators are used as an extra data augmentation source to train the classifiers. The interpolation coefficients can flexibly control the augmentation direction and strength. In addition, a style-mixing mechanism is applied to further improve the diversity of the generated OoD samples. Our experiments show that the proposed method explicitly increases the diversity of training domains and achieves consistent improvements over baselines across datasets and multiple different distribution shifts.
</details>
<details>
<summary>摘要</summary>
Training a generative model directly on the source domains can lead to mode collapse and amplify data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators with aligned model parameters. We then linearly interpolate the model parameters of the generators to spawn new sets of generators. Such interpolated generators are used as an extra data augmentation source to train the classifiers. The interpolation coefficients can flexibly control the augmentation direction and strength.In addition, we apply a style-mixing mechanism to further improve the diversity of the generated OoD samples. Our experiments show that the proposed method explicitly increases the diversity of training domains and achieves consistent improvements over baselines across datasets and multiple different distribution shifts.
</details></li>
</ul>
<hr>
<h2 id="Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models"><a href="#Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models" class="headerlink" title="Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models"></a>Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02409">http://arxiv.org/abs/2308.02409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong-Hai Nguyen, Ngumimi Karen Iyortsuun, Hyung-Jeong Yang, Guee-Sang Lee, Soo-Hyung Kim</li>
<li>for: 本研究旨在 классифицироватьMENTAL工作负担为三种状态和估算综合指数。</li>
<li>methods: 该方法 combinestemultiple空间维度来获得最佳的心理估算结果。在时域方法中，我们使用Temporal Convolutional Networks，而在频域方法中，我们提出了一种新的Multi-Dimensional Residual Block架构。</li>
<li>results: 我们的方法可以准确地分类MENTAL工作负担为三种状态，并且可以准确地估算综合指数。<details>
<summary>Abstract</summary>
The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
</details>
<details>
<summary>摘要</summary>
人类大脑在工作和休息时都处于不断活跃的状态。心理活动是每日的过程，当大脑过度劳累时，可能会对人类健康产生负面影响。在最近几年，关注早期识别心理健康问题的注意力凝固了，因为它可以帮助预防严重的健康问题并提高生活质量。多种信号都可以评估心理状态，但是电enzephalogram（EEG）在研究人员中广泛使用，因为它可以提供大量关于大脑的信息。本文的目标是将心理劳动力分为三个状态，并估计连续水平。我们的方法将多维空间综合使用，以达到最佳的心理估计结果。在时域方法中，我们使用Temporal Convolutional Networks，在频域中，我们提出了一种新的建筑方案called Multi-Dimensional Residual Block，这个方案将residual blocks综合使用。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Agents-For-Attacking-Inaudible-Voice-Activated-Devices"><a href="#Adversarial-Agents-For-Attacking-Inaudible-Voice-Activated-Devices" class="headerlink" title="Adversarial Agents For Attacking Inaudible Voice Activated Devices"></a>Adversarial Agents For Attacking Inaudible Voice Activated Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12204">http://arxiv.org/abs/2307.12204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Forrest McKee, David Noever</li>
<li>For: 这个论文探讨了基于互联网物联网的听不到攻击的威胁风险。* Methods: 该论文使用了强化学习来解决这些听不到攻击的问题。* Results: 研究发现，使用深度强化学习的方法可以快速拥有所有节点，并且在 fewer steps 中完成。In English, this means:* For: This paper explores the risk of inaudible attacks on voice-activated devices.* Methods: The paper uses reinforcement learning to solve the problem of inaudible attacks.* Results: The study finds that using deep reinforcement learning can quickly gain control of all nodes, and achieve this in fewer steps.<details>
<summary>Abstract</summary>
The paper applies reinforcement learning to novel Internet of Thing configurations. Our analysis of inaudible attacks on voice-activated devices confirms the alarming risk factor of 7.6 out of 10, underlining significant security vulnerabilities scored independently by NIST National Vulnerability Database (NVD). Our baseline network model showcases a scenario in which an attacker uses inaudible voice commands to gain unauthorized access to confidential information on a secured laptop. We simulated many attack scenarios on this baseline network model, revealing the potential for mass exploitation of interconnected devices to discover and own privileged information through physical access without adding new hardware or amplifying device skills. Using Microsoft's CyberBattleSim framework, we evaluated six reinforcement learning algorithms and found that Deep-Q learning with exploitation proved optimal, leading to rapid ownership of all nodes in fewer steps. Our findings underscore the critical need for understanding non-conventional networks and new cybersecurity measures in an ever-expanding digital landscape, particularly those characterized by mobile devices, voice activation, and non-linear microphones susceptible to malicious actors operating stealth attacks in the near-ultrasound or inaudible ranges. By 2024, this new attack surface might encompass more digital voice assistants than people on the planet yet offer fewer remedies than conventional patching or firmware fixes since the inaudible attacks arise inherently from the microphone design and digital signal processing.
</details>
<details>
<summary>摘要</summary>
文章应用再强化学习解决新互联网设备配置中的攻击问题。我们对无声攻击voice控制设备进行分析，确认了攻击性风险因子为7.6/10，强调了设备安全漏洞的独立评分。我们的基线网络模型显示了一种攻击者通过无声voice命令窃取机密信息的场景，我们在这个基线网络模型上进行了许多攻击场景的模拟，发现了大规模攻击INTERNET OF THINGS设备，以获取特权信息，不需要新硬件或设备技能升级。使用Microsoft的CyberBattleSim框架，我们评估了六种再强化学习算法，发现deep Q学习与利用最佳，可以在 fewer steps 内快速拥有所有节点。我们的发现强调了非常 conventional 网络和新的cybersecurity措施在不断扩大的数字ландшаф具 особен需要，特别是包括移动设备、voice控制和非线性 Microphone 在内的设备，遭受恶势力操作的隐藏攻击。到2024年，这个新的攻击表面可能会包括更多的数字voice助手 than人类 на 地球， yet offer fewer remedies than conventional patching or firmware fixes，因为无声攻击来自 Microphone 设计和数字信号处理。
</details></li>
</ul>
<hr>
<h2 id="NCART-Neural-Classification-and-Regression-Tree-for-Tabular-Data"><a href="#NCART-Neural-Classification-and-Regression-Tree-for-Tabular-Data" class="headerlink" title="NCART: Neural Classification and Regression Tree for Tabular Data"></a>NCART: Neural Classification and Regression Tree for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12198">http://arxiv.org/abs/2307.12198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Luo, Shixin Xu</li>
<li>for: 这 paper 旨在提出一种可解释性强的深度学习模型，以解决深度学习模型在大规模或高维数据集中的计算成本高和可解释性差的问题。</li>
<li>methods: 该 paper 提出了一种名为 Neural Classification and Regression Tree (NCART) 的新型可解释性神经网络，它将多层感知网络替换为多个可导的无知决策树。通过将决策树 integrate 到网络架构中，NCART 保持了可解释性，同时具有神经网络的综合能力。</li>
<li>results: 数值实验表明，NCART 比现有的深度学习模型具有更高的性能，并且在不同的数据集中表现出色，建立了 NCART 作为树状模型的强大竞争对手。<details>
<summary>Abstract</summary>
Deep learning models have become popular in the analysis of tabular data, as they address the limitations of decision trees and enable valuable applications like semi-supervised learning, online learning, and transfer learning. However, these deep-learning approaches often encounter a trade-off. On one hand, they can be computationally expensive when dealing with large-scale or high-dimensional datasets. On the other hand, they may lack interpretability and may not be suitable for small-scale datasets. In this study, we propose a novel interpretable neural network called Neural Classification and Regression Tree (NCART) to overcome these challenges. NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees. By integrating decision trees into the architecture, NCART maintains its interpretability while benefiting from the end-to-end capabilities of neural networks. The simplicity of the NCART architecture makes it well-suited for datasets of varying sizes and reduces computational costs compared to state-of-the-art deep learning models. Extensive numerical experiments demonstrate the superior performance of NCART compared to existing deep learning models, establishing it as a strong competitor to tree-based models.
</details>
<details>
<summary>摘要</summary>
深度学习模型在表格数据分析中变得越来越受欢迎，因为它们可以解决决策树的限制，并实现有价值的应用，如半监督学习、在线学习和传输学习。然而，这些深度学习方法经常面临一种负担。一方面，它们在处理大规模或高维度数据时可能会变得计算昂贵。另一方面，它们可能缺乏可解性，并且不适用于小规模数据。在本研究中，我们提出一种新的可解的神经网络模型，即神经分类和回归树（NCART），以解决这些挑战。NCART是基于差分网络的修改版本，它将完全连接层 replaced with 多个可微分的无知决策树。通过将决策树 integrate into 神经网络架构，NCART可以保持其可解性，同时受益于神经网络的终端能力。NCART 的简单架构使其适用于不同规模的数据集，并 reduc 计算成本与现有的深度学习模型相比。我们的数字实验证明 NCART 的表现比现有的深度学习模型更优，从而成为树状模型的强竞争对手。
</details></li>
</ul>
<hr>
<h2 id="Monadic-Deep-Learning"><a href="#Monadic-Deep-Learning" class="headerlink" title="Monadic Deep Learning"></a>Monadic Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12187">http://arxiv.org/abs/2307.12187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ThoughtWorksInc/monadic-deep-learning">https://github.com/ThoughtWorksInc/monadic-deep-learning</a></li>
<li>paper_authors: Bo Yang, Zhihao Zhang Kirisame Marisa, Kai Shi</li>
<li>for: 这个论文的目的是解决 dynamically typed 编程语言中的神经网络模型问题，使得用户可以使用 statically typed 语言来创建神经网络模型。</li>
<li>methods: 这篇论文使用了一种新的方法，即在静态类型函数中自动对含有多个可训练变量的神经网络模型进行差分。它还使用了一些幺athed 和 monad transformers，以便让用户创建具有 intuition 和简洁性的神经网络模型。</li>
<li>results: 该论文的实验结果表明，使用 DeepLearning.scala 可以帮助用户创建复杂的神经网络模型，并且仍然保持类型安全性。<details>
<summary>Abstract</summary>
The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.   Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.   We solved this problem in DeepLearning.scala 2. Our contributions are:   1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.   2. We designed a set of monads and monad transformers, which allow users to create monadic expressions that represent dynamic neural networks.   3. Along with these monads, we provide some applicative functors, to perform multiple calculations in parallel.   With these features, users of DeepLearning.scala were able to create complex neural networks in an intuitive and concise way, and still maintain type safety.
</details>
<details>
<summary>摘要</summary>
直到2019年，静态类型语言中的学习框架都没有提供传统框架的表达力。其用户无法使用自定义算法，除非创建大量的 boilerplate 代码来实现硬编码的反射传播。我们在 DeepLearning.scala 2 中解决了这个问题。我们的贡献包括：1. 我们发现了一种新的方法，可以在静态类型函数中自动进行反Mode微分，并且可以与金属语言进行自由交互。2. 我们设计了一组幂等和幂等转换，这些幂等可以让用户创建幂等表达式，表示动态神经网络。3. 同时，我们还提供了一些应用程序函数，可以在平行计算中进行多个计算。通过这些特性，DeepLearning.scala 的用户可以创建复杂的神经网络，使用 intuition 和简洁的方式来进行训练，并且仍保持类型安全性。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-discovers-invariants-of-braids-and-flat-braids"><a href="#Machine-learning-discovers-invariants-of-braids-and-flat-braids" class="headerlink" title="Machine learning discovers invariants of braids and flat braids"></a>Machine learning discovers invariants of braids and flat braids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12185">http://arxiv.org/abs/2307.12185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexei Lisitsa, Mateo Salles, Alexei Vernitski</li>
<li>for: 用机器学习分类布里论（或平坦布里论）的例子为轻量级或非轻量级。</li>
<li>methods: 使用超visisted学习神经网络（多层感知器）进行分类。</li>
<li>results: 发现新的便利 invariants of braids, including a complete invariant of flat braids.Here’s the translation in English:</li>
<li>for: Using machine learning to classify examples of braids (or flat braids) as trivial or non-trivial.</li>
<li>methods: Using supervised learning with neural networks (multilayer perceptrons).</li>
<li>results: Discovering new convenient invariants of braids, including a complete invariant of flat braids.<details>
<summary>Abstract</summary>
We use machine learning to classify examples of braids (or flat braids) as trivial or non-trivial. Our ML takes form of supervised learning using neural networks (multilayer perceptrons). When they achieve good results in classification, we are able to interpret their structure as mathematical conjectures and then prove these conjectures as theorems. As a result, we find new convenient invariants of braids, including a complete invariant of flat braids.
</details>
<details>
<summary>摘要</summary>
我们使用机器学习来分类拥有布里亚的示例（或平板布里亚）为致命或非致命。我们的机器学习形式为指导学习使用神经网络（多层感知器）。当它们在分类中获得良好的结果时，我们可以解释它们的结构为数学假设，然后证明这些假设为定理。因此，我们发现新的便利 invariants of braids，包括完整的平板布里亚 invariants。Note: "布里亚" (braids) is a word in Chinese that refers to a type of mathematical object, and "平板布里亚" (flat braids) is a specific type of braid that is flat and has no crossing points.
</details></li>
</ul>
<hr>
<h2 id="Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation"><a href="#Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation" class="headerlink" title="Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation"></a>Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12180">http://arxiv.org/abs/2307.12180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linzy0227/pdminet">https://github.com/linzy0227/pdminet</a></li>
<li>paper_authors: Yafei Zhang, Zhiyuan Li, Huafeng Li, Dapeng Tao</li>
<li>for: 这种多Modal MR brain tumor imaging segmentation方法是为了解决现有方法 Directly extracting discriminative features from input images for tumor sub-region category determination and localization，忽略了信息杂化的影响。</li>
<li>methods: 该方法提议使用肿瘤原型驱动的多专家结合，使得每个肿瘤子区域特征得到高亮显示。具体来说，我们提出了一种互传机制，将不同modal的特征传递给每个modal，以解决单modal特征不充分的问题。此外，我们还提出了一种使用学习的肿瘤原型来驱动特征表示和融合方法，使得肿瘤特征得到了融合。</li>
<li>results: 实验结果表明，该方法在三个竞赛肿瘤分割数据集上具有优秀的性能。<details>
<summary>Abstract</summary>
For multi-modal magnetic resonance (MR) brain tumor image segmentation, current methods usually directly extract the discriminative features from input images for tumor sub-region category determination and localization. However, the impact of information aliasing caused by the mutual inclusion of tumor sub-regions is often ignored. Moreover, existing methods usually do not take tailored efforts to highlight the single tumor sub-region features. To this end, a multi-modal MR brain tumor segmentation method with tumor prototype-driven and multi-expert integration is proposed. It could highlight the features of each tumor sub-region under the guidance of tumor prototypes. Specifically, to obtain the prototypes with complete information, we propose a mutual transmission mechanism to transfer different modal features to each other to address the issues raised by insufficient information on single-modal features. Furthermore, we devise a prototype-driven feature representation and fusion method with the learned prototypes, which implants the prototypes into tumor features and generates corresponding activation maps. With the activation maps, the sub-region features consistent with the prototype category can be highlighted. A key information enhancement and fusion strategy with multi-expert integration is designed to further improve the segmentation performance. The strategy can integrate the features from different layers of the extra feature extraction network and the features highlighted by the prototypes. Experimental results on three competition brain tumor segmentation datasets prove the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
对多Modal MR脑肿刺激图像分割，当前方法通常直接从输入图像中提取特征来确定肿瘤子区划分和定位。然而，现有方法通常忽略了肿瘤子区之间的信息冲突的影响。此外，现有方法通常不会特意强调单个肿瘤子区域的特征。为此，我们提议一种基于肿瘤原型的多Modal MR脑肿刺激分割方法。它可以在肿瘤原型的指导下高亮单个肿瘤子区域的特征。具体来说，为了获取完整信息的肿瘤原型，我们提议一种互传机制来传递不同modal特征之间的信息，以解决单modal特征不具备的问题。此外，我们设计了一种基于肿瘤原型的特征表示和融合方法，其可以在肿瘤特征中嵌入肿瘤原型，并生成相应的激活图。通过激活图，可以高亮与肿瘤原型类别相符的子区域特征。此外，我们设计了一种多 экспер特征融合策略，可以将不同层次的特征和肿瘤原型高亮的特征融合在一起，以提高分割性能。实验结果表明，提议的方法在三个竞赛脑肿刺激分割数据集上显示出了超越性。
</details></li>
</ul>
<hr>
<h2 id="Learn-to-Compress-LtC-Efficient-Learning-based-Streaming-Video-Analytics"><a href="#Learn-to-Compress-LtC-Efficient-Learning-based-Streaming-Video-Analytics" class="headerlink" title="Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics"></a>Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12171">http://arxiv.org/abs/2307.12171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quazi Mishkatul Alam, Israat Haque, Nael Abu-Ghazaleh<br>for: 这个论文的目的是建立一个高效的云端视频分析框架，以减少视频流的带宽和能源消耗。methods: 这个框架使用了一个轻量级神经网络，通过教师模型在服务器端进行学习，以减少视频流中不必要的信息。此外，它还使用了一种基于特征差分的时间滤波算法，以便快速忽略不必要的帧。results: 这个框架可以使用28-35% less bandwidth和45% shorter response delay，与最近发布的相关研究框架相比，而且保持了类似的分析性能。<details>
<summary>Abstract</summary>
Video analytics are often performed as cloud services in edge settings, mainly to offload computation, and also in situations where the results are not directly consumed at the video sensors. Sending high-quality video data from the edge devices can be expensive both in terms of bandwidth and power use. In order to build a streaming video analytics pipeline that makes efficient use of these resources, it is therefore imperative to reduce the size of the video stream. Traditional video compression algorithms are unaware of the semantics of the video, and can be both inefficient and harmful for the analytics performance. In this paper, we introduce LtC, a collaborative framework between the video source and the analytics server, that efficiently learns to reduce the video streams within an analytics pipeline. Specifically, LtC uses the full-fledged analytics algorithm at the server as a teacher to train a lightweight student neural network, which is then deployed at the video source. The student network is trained to comprehend the semantic significance of various regions within the videos, which is used to differentially preserve the crucial regions in high quality while the remaining regions undergo aggressive compression. Furthermore, LtC also incorporates a novel temporal filtering algorithm based on feature-differencing to omit transmitting frames that do not contribute new information. Overall, LtC is able to use 28-35% less bandwidth and has up to 45% shorter response delay compared to recently published state of the art streaming frameworks while achieving similar analytics performance.
</details>
<details>
<summary>摘要</summary>
视频分析通常在云服务中进行，主要是为了减轻计算负担，以及在视频感知不直接在视频传感器上进行处理。往往将高质量视频数据从边缘设备传输到云服务器可能会占用很多带宽和电力资源。为建立高效的流动视频分析管道，因此是非常重要减小视频流。传统的视频压缩算法不了解视频的 semantics，可能会导致不fficient和对分析性能有害。在这篇论文中，我们介绍了 LtC，一个协同框架，其中视频源和分析服务器之间协同减小视频流。特别是，LtC 使用全功能的分析算法作为老师来训练一个轻量级神经网络，并将其部署到视频源上。学生网络被训练以理解视频中各个区域的semantic Significance，并使用这些区域来差分保留高质量视频，而其他区域则进行了激进压缩。此外，LtC 还包括一种基于特征差异的时间滤波算法，以便快速忽略不包含新信息的帧。总之，LtC 可以使用28-35%的带宽和45%的响应延迟，相比之下最新的流动框架，而 achieved similar analytics performance。
</details></li>
</ul>
<hr>
<h2 id="Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters"><a href="#Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters" class="headerlink" title="Optimized Network Architectures for Large Language Model Training with Billions of Parameters"></a>Optimized Network Architectures for Large Language Model Training with Billions of Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12169">http://arxiv.org/abs/2307.12169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyang Wang, Manya Ghobadi, Kayvon Shakeri, Ying Zhang, Naader Hasani</li>
<li>for: 这篇论文挑战了在训练大型自然语言模型（LLM）时建立任意对任意网络的惯例。</li>
<li>methods: 我们表明了 LLM 训练中唯一特点的通信模式，只有小组内 GPU 需要高带宽任意对任意通信，以达到训练性能的近似最优。这些小组内 GPU 之间的通信费量极低、稀疏和均匀。我们提议一种新的网络架构，它与 LLM 的通信需求高度相似。我们将集群分为 HB domain，其中每个 HB domain 由非堵塞的任意对任意高带宽互连器连接。在 HB domain 内，网络只与需要通信的 GPU 连接。我们称这种网络为 “铁路仅” 连接，并证明我们的提议架构可以将网络成本降低至最多 75%，不会影响 LLM 训练的性能。</li>
<li>results: 我们的实验结果表明，我们的提议架构可以减少网络成本，同时保持 LLM 训练的性能。<details>
<summary>Abstract</summary>
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification"><a href="#Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification" class="headerlink" title="Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification"></a>Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12159">http://arxiv.org/abs/2307.12159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nícolas Barbosa Gomes, Arissa Yoshida, Mateus Roder, Guilherme Camargo de Oliveira, João Paulo Papa</li>
<li>for: 这篇论文的目的是找到早期诊断amyotrophic lateral sclerosis (ALS)的方法，以提高病人的预后和生活质量。</li>
<li>methods: 这篇论文使用computational方法来分析病人的脸部表情，以检测ALS的症状。具体来说，这篇论文使用Facial Point Graphs来学习脸部图像的几何特征，以自动识别ALS。</li>
<li>results: 论文的实验结果显示，提案的方法在测试数据集Toronto Neuroface Dataset中，与现有方法相比，有着更高的准确性和效率。这些结果显示出这种方法的潜力，并带来了领域的发展。<details>
<summary>Abstract</summary>
Identifying Amyotrophic Lateral Sclerosis (ALS) in its early stages is essential for establishing the beginning of treatment, enriching the outlook, and enhancing the overall well-being of those affected individuals. However, early diagnosis and detecting the disease's signs is not straightforward. A simpler and cheaper way arises by analyzing the patient's facial expressions through computational methods. When a patient with ALS engages in specific actions, e.g., opening their mouth, the movement of specific facial muscles differs from that observed in a healthy individual. This paper proposes Facial Point Graphs to learn information from the geometry of facial images to identify ALS automatically. The experimental outcomes in the Toronto Neuroface dataset show the proposed approach outperformed state-of-the-art results, fostering promising developments in the area.
</details>
<details>
<summary>摘要</summary>
早期识别amyotrophic lateral sclerosis（ALS）是非常重要，可以提高治疗的开始，改善患者的生活质量和总体情况。然而，早期诊断和识别病状的标准方法并不是很直forward。本文提出了一种使用计算机方法分析患者的面部表情来自动识别ALS的方法。当患者进行特定的动作时，如打开嘴巴，特定的面部肌肉的运动会与健康人的不同。这篇论文使用面部点图学习geometry of facial images来自动识别ALS，实验结果表明该方法在多伦多Neuroface dataset中超越了现有的最佳结果，为您的发展提供了有希望的前景。
</details></li>
</ul>
<hr>
<h2 id="DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft"><a href="#DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft" class="headerlink" title="DIP-RL: Demonstration-Inferred Preference Learning in Minecraft"></a>DIP-RL: Demonstration-Inferred Preference Learning in Minecraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12158">http://arxiv.org/abs/2307.12158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ellen Novoseller, Vinicius G. Goecks, David Watkins, Josh Miller, Nicholas Waytowich</li>
<li>for: 本研究旨在解决在无结构化真实世界中，RL算法学习Sequential Decision-Making时，因缺乏奖励信号而难以学习的问题。</li>
<li>methods: 本研究提出了Demonstration-Inferred Preference Reinforcement Learning（DIP-RL）算法，利用人类示范来推导RL算法学习。DIP-RL在三种不同的方式使用示范数据，包括训练自动编码器、RL训练批处理中使用示范数据，以及推导RL奖励函数。</li>
<li>results: 实验结果表明，DIP-RL可以引导RL算法学习人类偏好，并且与基elines相比，DIP-RL在树割任务中表现竞争力强。例如轨迹满足扩展可以在<a target="_blank" rel="noopener" href="https://sites.google.com/view/dip-rl%E3%80%82">https://sites.google.com/view/dip-rl。</a><details>
<summary>Abstract</summary>
In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspired by our previous work on combining demonstrations and pairwise preferences in Minecraft, which was awarded a research prize at the 2022 NeurIPS MineRL BASALT competition, Learning from Human Feedback in Minecraft. Example trajectory rollouts of DIP-RL and baselines are located at https://sites.google.com/view/dip-rl.
</details>
<details>
<summary>摘要</summary>
机器学习 дляsequential decision-making中的算法式代理可以在环境中学习并接受反馈形式为奖signal。但在许多无结构的实际场景中，这种奖signal是未知的，人们无法可靠地制定一个正确捕捉所愿行为的奖 signal。为解决这些无结构和开放的环境中的任务，我们提出了Demonstration-Inferred Preference Reinforcement Learning（DIP-RL）算法，该算法利用人类示范在三种方式：在训练 autoencoder 中，在RL训练批次中使用示范数据，以及推导RL agent 对行为的偏好来学习一个奖函数来导引RL。我们在 Minecraft 中进行了树割任务的评估，结果表明DIP-RL可以引导RL agent 学习一个符合人类偏好的奖函数，并且DIP-RL与基elines相比表现竞争性。DIP-RL是基于我们在 Minecraft 中结合示范和对比偏好的前一项研究，该研究在2022年 NeurIPS MineRL BASALT 比赛中获得了研究奖，Learning from Human Feedback in Minecraft。DIP-RL的示例轨迹扩展位于 <https://sites.google.com/view/dip-rl>。
</details></li>
</ul>
<hr>
<h2 id="Identifying-contributors-to-supply-chain-outcomes-in-a-multi-echelon-setting-a-decentralised-approach"><a href="#Identifying-contributors-to-supply-chain-outcomes-in-a-multi-echelon-setting-a-decentralised-approach" class="headerlink" title="Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach"></a>Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12157">http://arxiv.org/abs/2307.12157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Schoepf, Jack Foster, Alexandra Brintrup</li>
<li>for: 本研究旨在帮助企业快速准确地确定生产过程中metric变化的原因，尤其是在多层供应链中，只能部分可见。</li>
<li>methods: 本研究提议使用可解释人工智能来实现分布式计算估算变量的贡献，以解决数据隐私问题。</li>
<li>results: 实验结果表明，分布式计算可以更有效地检测质量变化的起源，比中央化方法使用Shapley添加itive解释。<details>
<summary>Abstract</summary>
Organisations often struggle to identify the causes of change in metrics such as product quality and delivery duration. This task becomes increasingly challenging when the cause lies outside of company borders in multi-echelon supply chains that are only partially observable. Although traditional supply chain management has advocated for data sharing to gain better insights, this does not take place in practice due to data privacy concerns. We propose the use of explainable artificial intelligence for decentralised computing of estimated contributions to a metric of interest in a multi-stage production process. This approach mitigates the need to convince supply chain actors to share data, as all computations occur in a decentralised manner. Our method is empirically validated using data collected from a real multi-stage manufacturing process. The results demonstrate the effectiveness of our approach in detecting the source of quality variations compared to a centralised approach using Shapley additive explanations.
</details>
<details>
<summary>摘要</summary>
企业们经常难以确定生产质量和交付时间的变化的原因。这个任务在多层供应链中，只能半 observability 的情况下变得更加困难。传统的供应链管理推荐数据共享，以获得更好的洞察力，但在实践中，由于数据隐私问题，这并没有实现。我们建议使用可解释人工智能 для分布式计算估算的贡献因素，以解决不必要地让供应链 aktör 分享数据的问题。我们的方法在实验 validate 使用实际的多Stage生产过程中的数据，结果显示，我们的方法可以更好地探测质量变化的来源，比中央化使用Shapley加itive解释法。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices"><a href="#Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices" class="headerlink" title="Real-Time Neural Video Recovery and Enhancement on Mobile Devices"></a>Real-Time Neural Video Recovery and Enhancement on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12152">http://arxiv.org/abs/2307.12152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Zhaoyuan He, Yifan Yang, Lili Qiu, Kyoungjun Park</li>
<li>for: 提高移动设备上视频流式的流畅体验</li>
<li>methods: 提出了一种新的视频帧恢复方案、一种新的超分辨率算法和一种接受器增强视频比特率调整算法</li>
<li>results: 实现了30帧&#x2F;秒的实时增强，在不同的网络环境下提高了视频流程的质量经验（QoE），具体的提高率为24%-82%。<details>
<summary>Abstract</summary>
As mobile devices become increasingly popular for video streaming, it's crucial to optimize the streaming experience for these devices. Although deep learning-based video enhancement techniques are gaining attention, most of them cannot support real-time enhancement on mobile devices. Additionally, many of these techniques are focused solely on super-resolution and cannot handle partial or complete loss or corruption of video frames, which is common on the Internet and wireless networks.   To overcome these challenges, we present a novel approach in this paper. Our approach consists of (i) a novel video frame recovery scheme, (ii) a new super-resolution algorithm, and (iii) a receiver enhancement-aware video bit rate adaptation algorithm. We have implemented our approach on an iPhone 12, and it can support 30 frames per second (FPS). We have evaluated our approach in various networks such as WiFi, 3G, 4G, and 5G networks. Our evaluation shows that our approach enables real-time enhancement and results in a significant increase in video QoE (Quality of Experience) of 24\% - 82\% in our video streaming system.
</details>
<details>
<summary>摘要</summary>
为了优化移动设备上的视频流处理，随着移动设备的普及，现在已经非常重要。虽然深度学习基于视频提升技术在获得关注，但大多数这些技术无法在移动设备上实时进行提升。此外，许多这些技术都是专注于超解像，而不是处理部分或完全丢失的视频帧，这是网络和无线网络中的常见问题。为了解决这些挑战，我们在本文中提出了一种新的方法。我们的方法包括以下三个部分：(i) 一种新的视频帧恢复算法，(ii) 一种新的超解像算法，(iii) 一种基于接收器提升的视频比特率自适应算法。我们在iPhone 12上实现了我们的方法，并可以支持30帧/秒。我们在WiFi、3G、4G和5G网络中进行了评估，我们的评估结果表明，我们的方法可以实现实时提升，并导致视频Quality of Experience（QoE）提高24%-82%。
</details></li>
</ul>
<hr>
<h2 id="Learned-Gridification-for-Efficient-Point-Cloud-Processing"><a href="#Learned-Gridification-for-Efficient-Point-Cloud-Processing" class="headerlink" title="Learned Gridification for Efficient Point Cloud Processing"></a>Learned Gridification for Efficient Point Cloud Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14354">http://arxiv.org/abs/2307.14354</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/computri/gridifier">https://github.com/computri/gridifier</a></li>
<li>paper_authors: Putri A. van der Linden, David W. Romero, Erik J. Bekkers</li>
<li>for: 这篇论文主要用于解决点云处理领域中的可插入性问题，提高点云处理的缩放性和可扩展性。</li>
<li>methods: 该论文提出了一种名为”学习gridification”的方法，即将点云转化为一个紧凑、规则的网格，以便在网格上使用已有的深度学习方法。</li>
<li>results: 经过 teorтиче 和实验分析，该论文表明，使用学习gridification方法可以提高点云处理的缩放性和可扩展性，同时保持与原始点云数据相比的竞争性。<details>
<summary>Abstract</summary>
Neural operations that rely on neighborhood information are much more expensive when deployed on point clouds than on grid data due to the irregular distances between points in a point cloud. In a grid, on the other hand, we can compute the kernel only once and reuse it for all query positions. As a result, operations that rely on neighborhood information scale much worse for point clouds than for grid data, specially for large inputs and large neighborhoods.   In this work, we address the scalability issue of point cloud methods by tackling its root cause: the irregularity of the data. We propose learnable gridification as the first step in a point cloud processing pipeline to transform the point cloud into a compact, regular grid. Thanks to gridification, subsequent layers can use operations defined on regular grids, e.g., Conv3D, which scale much better than native point cloud methods. We then extend gridification to point cloud to point cloud tasks, e.g., segmentation, by adding a learnable de-gridification step at the end of the point cloud processing pipeline to map the compact, regular grid back to its original point cloud form. Through theoretical and empirical analysis, we show that gridified networks scale better in terms of memory and time than networks directly applied on raw point cloud data, while being able to achieve competitive results. Our code is publicly available at https://github.com/computri/gridifier.
</details>
<details>
<summary>摘要</summary>
神经操作依赖地域信息在点云上比在格子数据上更加昂贵，因为点云中点的距离不规则。在格子中，我们可以一次计算核心，然后将其重复使用所有查询位置。因此，基于地域信息的操作在点云上缩放比格子数据更差，特别是对于大输入和大地域。在这项工作中，我们解决点云方法的扩展性问题，通过将点云转换为可 compact、规则的格子。感谢gridification，后续层可以使用定义在规则格子上的操作，例如Conv3D，这些操作在点云数据上缩放更好。我们还扩展gridification来点云到点云任务，例如分割，通过在点云处理管道的末端添加学习的de-gridification步骤，将紧凑的规则格子映射回原始点云形式。通过理论和实验分析，我们表明gridified网络在内存和时间上比直接应用于原始点云数据更好的扩展性，同时能够达到竞争性的结果。我们的代码公开在https://github.com/computri/gridifier。
</details></li>
</ul>
<hr>
<h2 id="CorrFL-Correlation-Based-Neural-Network-Architecture-for-Unavailability-Concerns-in-a-Heterogeneous-IoT-Environment"><a href="#CorrFL-Correlation-Based-Neural-Network-Architecture-for-Unavailability-Concerns-in-a-Heterogeneous-IoT-Environment" class="headerlink" title="CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment"></a>CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12149">http://arxiv.org/abs/2307.12149</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Western-OC2-Lab/CorrFL">https://github.com/Western-OC2-Lab/CorrFL</a></li>
<li>paper_authors: Ibrahim Shaer, Abdallah Shami</li>
<li>For: 解决 Federated Learning（FL）中模型策略的差异和物联网（IoT）节点的缺失问题。* Methods: 提出了一种基于相关学习（Correlation-based FL，CorrFL）的方法，通过将不同模型权重映射到共同的准则空间来处理模型策略的差异。* Results: 通过对一个真实的使用场景进行评估，发现CorrFL模型在缺失IoT节点和高活动水平时的预测性能较为出色，并且与不同的使用场景中的标准模型进行比较。<details>
<summary>Abstract</summary>
The Federated Learning (FL) paradigm faces several challenges that limit its application in real-world environments. These challenges include the local models' architecture heterogeneity and the unavailability of distributed Internet of Things (IoT) nodes due to connectivity problems. These factors posit the question of "how can the available models fill the training gap of the unavailable models?". This question is referred to as the "Oblique Federated Learning" problem. This problem is encountered in the studied environment that includes distributed IoT nodes responsible for predicting CO2 concentrations. This paper proposes the Correlation-based FL (CorrFL) approach influenced by the representational learning field to address this problem. CorrFL projects the various model weights to a common latent space to address the model heterogeneity. Its loss function minimizes the reconstruction loss when models are absent and maximizes the correlation between the generated models. The latter factor is critical because of the intersection of the feature spaces of the IoT devices. CorrFL is evaluated on a realistic use case, involving the unavailability of one IoT device and heightened activity levels that reflect occupancy. The generated CorrFL models for the unavailable IoT device from the available ones trained on the new environment are compared against models trained on different use cases, referred to as the benchmark model. The evaluation criteria combine the mean absolute error (MAE) of predictions and the impact of the amount of exchanged data on the prediction performance improvement. Through a comprehensive experimental procedure, the CorrFL model outperformed the benchmark model in every criterion.
</details>
<details>
<summary>摘要</summary>
联邦学习（FL）模式面临许多实际环境中的挑战，这些挑战包括本地模型的架构多样性和分布式互联网络端的网络问题，这们问题使得“如何让可用的模型填充缺失的模型？”这个问题被称为“偏角联邦学习”问题。这个问题在分散式互联网络端负责预测CO2浓度的环境中被研究。这篇文章提出了基于相互关联学习（CorrFL）方法，它将多个模型的weight投射到共同的潜在空间以解决模型多样性问题。CorrFL的损失函数将缺失的模型的重建损失降低至最小，并将可用模型生成的模型之间的相互相关性提高。这个因素是critical，因为分散式互联网络端的特征空间 intersection。通过一个实际的使用情况，这篇文章评估了CorrFL在一个 IoT 设备缺失和活动水平增加的情况下的表现。生成的CorrFL模型与不可用的 IoT 设备进行比较，并与不同的使用情况下的模型进行比较，这些模型被称为底线模型。评估标准包括预测误差的总平均误差（MAE）和预测性能改善的资料交换量影响。通过一个完整的实验程序，CorrFL 模型在每个标准中都表现出优于底线模型。
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Machine-Learning-to-Modelling-and-Analysing-Dynamical-Systems"><a href="#Applications-of-Machine-Learning-to-Modelling-and-Analysing-Dynamical-Systems" class="headerlink" title="Applications of Machine Learning to Modelling and Analysing Dynamical Systems"></a>Applications of Machine Learning to Modelling and Analysing Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03763">http://arxiv.org/abs/2308.03763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vedanta Thapar</li>
<li>for: 本研究使用物理学 Informed Neural Networks 分析非线性哈密顿动力系统，具有一个动力平衡方程的第一 интеграル。</li>
<li>methods: 本文提出了一种结合现有哈密顿神经网络结构的 Adaptable Symplectic Recurrent Neural Networks 架构，能够保持哈密顿方程和相互作用的 symplectic 结构，并在整个参数空间预测动力学行为。此架构在预测哈密顿动力学中，特别是在含有多个参数的潜能中表现出了显著的优势。</li>
<li>results: 本研究表明，使用该架构可以高效地预测哈密顿动力学，尤其是在含有多个参数的潜能中。此外， authors 还证明了该方法在单参数潜能中的稳定性和长期预测性。<details>
<summary>Abstract</summary>
We explore the use of Physics Informed Neural Networks to analyse nonlinear Hamiltonian Dynamical Systems with a first integral of motion. In this work, we propose an architecture which combines existing Hamiltonian Neural Network structures into Adaptable Symplectic Recurrent Neural Networks which preserve Hamilton's equations as well as the symplectic structure of phase space while predicting dynamics for the entire parameter space. This architecture is found to significantly outperform previously proposed neural networks when predicting Hamiltonian dynamics especially in potentials which contain multiple parameters. We demonstrate its robustness using the nonlinear Henon-Heiles potential under chaotic, quasiperiodic and periodic conditions.   The second problem we tackle is whether we can use the high dimensional nonlinear capabilities of neural networks to predict the dynamics of a Hamiltonian system given only partial information of the same. Hence we attempt to take advantage of Long Short Term Memory networks to implement Takens' embedding theorem and construct a delay embedding of the system followed by mapping the topologically invariant attractor to the true form. This architecture is then layered with Adaptable Symplectic nets to allow for predictions which preserve the structure of Hamilton's equations. We show that this method works efficiently for single parameter potentials and provides accurate predictions even over long periods of time.
</details>
<details>
<summary>摘要</summary>
我们探讨使用物理 Informed Neural Networks 分析非线性汉密尔顿动力系统，其具有一个动力的第一Integral of motion。在这项工作中，我们提议一种结合现有汉密尔顿神经网络结构的可适应 симплектиче Recurrent Neural Networks 结构，该结构保留汉密尔顿方程以及相对空间的 симплектиче结构，同时预测动力的整个参数空间。这种结构在预测汉密尔顿动力方面表现出了明显的优异，尤其是在含有多个参数的潜能中。我们通过使用非线性 Henon-Heiles 潜能函数进行了robustness测试，并在各种不同的conditions下进行了验证。第二个问题是可以使用高维非线性神经网络来预测汉密尔顿系统的动力，只要知道一部分系统的信息吗。因此，我们尝试使用 Long Short Term Memory 网络实现 Takens 嵌入定理，并将系统的延迟嵌入映射到真正的形式。然后，我们层加 Adaptable Symplectic nets 以使预测保留汉密尔顿方程的结构。我们发现这种方法可以高效地预测单参数潜能中的动力，并且可以在长时间内提供高度准确的预测。
</details></li>
</ul>
<hr>
<h2 id="A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter"><a href="#A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter" class="headerlink" title="A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter"></a>A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12145">http://arxiv.org/abs/2307.12145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/river-lab/hyperspectral_macro_plastic_detection">https://github.com/river-lab/hyperspectral_macro_plastic_detection</a></li>
<li>paper_authors: Nathaniel Hanson, Ahmet Demirkaya, Deniz Erdoğmuş, Aron Stubbins, Taşkın Padır, Tales Imbiriba</li>
<li>for: 本研究旨在开发一种高效自动化的浮游垃圾监测方法，以解决水体中垃圾杂物的监测问题。</li>
<li>methods: 本研究使用计算成像技术进行浮游垃圾物质的检测，包括可见短波谱成像和可见短波谱识别方法。</li>
<li>results: 实验结果表明，使用Snapshot可见短波谱成像和机器学习分类方法可以在实际场景中实现高检测精度，特别是在具有挑战性的场景下。<details>
<summary>Abstract</summary>
Plastic waste entering the riverine harms local ecosystems leading to negative ecological and economic impacts. Large parcels of plastic waste are transported from inland to oceans leading to a global scale problem of floating debris fields. In this context, efficient and automatized monitoring of mismanaged plastic waste is paramount. To address this problem, we analyze the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios. We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging. Our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy even in challenging scenarios, especially when leveraging hyperspectral data and nonlinear classifiers. All code, data, and models are available online: https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection.
</details>
<details>
<summary>摘要</summary>
塑料废弃物进入河流环境会对当地生态系统造成负面影响，导致生态和经济问题。大量塑料废弃物从陆地传输到海洋，导致全球范围内漂浮垃圾场景。在这种情况下，高效和自动化的废弃塑料监测变得非常重要。为解决这个问题，我们分析了使用计算成像方法检测大型塑料废弃物的可能性。我们使用快照可见短波谱 hyperspectral成像进行近实时检测半潜水塑料。我们的实验表明，通过使用机器学习分类方法和非线性分类器，可以在具有挑战性的情况下实现高检测精度。所有代码、数据和模型都可以在 GitHub 上下载：https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection。
</details></li>
</ul>
<hr>
<h2 id="Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning"><a href="#Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning" class="headerlink" title="Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning"></a>Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12143">http://arxiv.org/abs/2307.12143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aqeel13932/mn_project">https://github.com/aqeel13932/mn_project</a></li>
<li>paper_authors: Aqeel Labash, Florian Fletzer, Daniel Majoral, Raul Vicente</li>
<li>For: The paper explores the emergence of circadian-like rhythms in deep reinforcement learning agents.* Methods: The authors use a foraging task and a reliable periodic variation in the environment to train the agents. They systematically characterize the agents’ behavior during learning and analyze the emergence of an endogenous rhythm using bifurcation and phase response curve analyses.* Results: The paper shows that the internal rhythm adapts to shifts in the phase of the environmental signal without any re-training, and demonstrates how artificial neurons develop dynamics to support the internalization of the environmental rhythm. The adaptation proceeds through the emergence of a stable periodic orbit in the neuron dynamics with a phase response that allows optimal phase synchronization between the agent’s dynamics and the environmental rhythm.Here’s the Chinese translation of the three points:* For: 这篇论文研究了深度奖励学习代理人是如何适应环境的 rhythm。* Methods: 作者使用了一个捕食任务和一个可预测的环境变化来训练代理人。他们系统地描述了代理人在学习过程中的行为，并通过分支和相位响应曲线分析来分析内在的 rhythm 的出现。* Results: 论文显示了内在 rhythm 可以适应环境信号的相位变化，而不需要任何再训练。它还表明了人工神经元的动力学发展了一种支持内在 rhythm 的动力学特性，并且在代理人动力学和环境 rhythm 之间进行了优化的相位同步。从动力学视角来看， adaptive 进程是通过内在 rhythm 的稳定 periodic orbit 的出现来实现的，该 periodic orbit 的相位响应允许代理人动力学和环境 rhythm 之间的优化相位同步。<details>
<summary>Abstract</summary>
Adapting to regularities of the environment is critical for biological organisms to anticipate events and plan. A prominent example is the circadian rhythm corresponding to the internalization by organisms of the $24$-hour period of the Earth's rotation. In this work, we study the emergence of circadian-like rhythms in deep reinforcement learning agents. In particular, we deployed agents in an environment with a reliable periodic variation while solving a foraging task. We systematically characterize the agent's behavior during learning and demonstrate the emergence of a rhythm that is endogenous and entrainable. Interestingly, the internal rhythm adapts to shifts in the phase of the environmental signal without any re-training. Furthermore, we show via bifurcation and phase response curve analyses how artificial neurons develop dynamics to support the internalization of the environmental rhythm. From a dynamical systems view, we demonstrate that the adaptation proceeds by the emergence of a stable periodic orbit in the neuron dynamics with a phase response that allows an optimal phase synchronisation between the agent's dynamics and the environmental rhythm.
</details>
<details>
<summary>摘要</summary>
适应环境的规律是生物体预测事件和规划的关键。一个明显的例子是生物体内部的 circadian 频率，即通过生物体内部内化地球的24小时转动周期。在这项工作中，我们研究了深度学习Agent中的 circadian-like 频率的出现。特别是，我们在一个可靠 periodic 变化的环境中部署了 Agent，并在寻食任务中学习。我们系统地描述了 Agent 的行为 durante 学习，并证明了 Agent 内部的频率可以自动适应环境的阶段偏移。此外，我们通过杂化和相对响应曲线分析表明，人工神经元发展了 dynamics 来支持内部化环境的频率。从动力系统视角来看，适应进程由神经元动力学中的稳定 periodic 轨迹的出现和相应的相位协调导致。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-Carbon-Reduction-Potential-with-Reinforcement-Learning-for-the-Three-Dimensional-Loading-Capacitated-Vehicle-Routing-Problem"><a href="#Unlocking-Carbon-Reduction-Potential-with-Reinforcement-Learning-for-the-Three-Dimensional-Loading-Capacitated-Vehicle-Routing-Problem" class="headerlink" title="Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem"></a>Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12136">http://arxiv.org/abs/2307.12136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Schoepf, Stephen Mak, Julian Senoner, Liming Xu, Netland Torbjörn, Alexandra Brintrup</li>
<li>for: 增加效率，提高运输效率</li>
<li>methods: 使用强化学习模型</li>
<li>results: 与现有方法相比，平均差距在3.83%到8.10%之间<details>
<summary>Abstract</summary>
Heavy goods vehicles are vital backbones of the supply chain delivery system but also contribute significantly to carbon emissions with only 60% loading efficiency in the United Kingdom. Collaborative vehicle routing has been proposed as a solution to increase efficiency, but challenges remain to make this a possibility. One key challenge is the efficient computation of viable solutions for co-loading and routing. Current operations research methods suffer from non-linear scaling with increasing problem size and are therefore bound to limited geographic areas to compute results in time for day-to-day operations. This only allows for local optima in routing and leaves global optimisation potential untouched. We develop a reinforcement learning model to solve the three-dimensional loading capacitated vehicle routing problem in approximately linear time. While this problem has been studied extensively in operations research, no publications on solving it with reinforcement learning exist. We demonstrate the favourable scaling of our reinforcement learning model and benchmark our routing performance against state-of-the-art methods. The model performs within an average gap of 3.83% to 8.10% compared to established methods. Our model not only represents a promising first step towards large-scale logistics optimisation with reinforcement learning but also lays the foundation for this research stream.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we have developed a reinforcement learning model to solve the three-dimensional loading capacitated vehicle routing problem in approximately linear time. This problem has been extensively studied in operations research, but no publications on solving it with reinforcement learning exist. We demonstrate the favourable scaling of our reinforcement learning model and benchmark our routing performance against state-of-the-art methods. Our model performs within an average gap of 3.83% to 8.10% compared to established methods.Our model not only represents a promising first step towards large-scale logistics optimization with reinforcement learning but also lays the foundation for this research stream. With the ability to efficiently compute viable solutions for co-loading and routing, we can significantly reduce carbon emissions from heavy goods vehicles and improve the overall efficiency of the supply chain delivery system.
</details></li>
</ul>
<hr>
<h2 id="The-Sample-Complexity-of-Multi-Distribution-Learning-for-VC-Classes"><a href="#The-Sample-Complexity-of-Multi-Distribution-Learning-for-VC-Classes" class="headerlink" title="The Sample Complexity of Multi-Distribution Learning for VC Classes"></a>The Sample Complexity of Multi-Distribution Learning for VC Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12135">http://arxiv.org/abs/2307.12135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranjal Awasthi, Nika Haghtalab, Eric Zhao</li>
<li>for: 多 Distribution Learning 是一种自然推广 PAC 学习到多个数据分布的设置中。</li>
<li>methods: 使用游戏动力学来解决这个问题，并讨论了一些基本障碍。</li>
<li>results: 研究表明，现有的最佳下界是 $\Omega(\epsilon^{-2}(d + k \ln(k)))$, 而实际 Sample Complexity 为 $O(\epsilon^{-2} \ln(k)(d + k) + \min{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d})$.<details>
<summary>Abstract</summary>
Multi-distribution learning is a natural generalization of PAC learning to settings with multiple data distributions. There remains a significant gap between the known upper and lower bounds for PAC-learnable classes. In particular, though we understand the sample complexity of learning a VC dimension d class on $k$ distributions to be $O(\epsilon^{-2} \ln(k)(d + k) + \min\{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d\})$, the best lower bound is $\Omega(\epsilon^{-2}(d + k \ln(k)))$. We discuss recent progress on this problem and some hurdles that are fundamental to the use of game dynamics in statistical learning.
</details>
<details>
<summary>摘要</summary>
多分布学习是自然推广PAC学习的设置中的多个数据分布的一种自然推广。现存在较大的知识上下文和下界之间的差距。具体来说，虽然我们理解了VC阶数d在k个分布上学习的样本复杂度为O（ε^-2 \* ln(k) (d + k) + MIN（ε^-1 dk, ε^-4 \* ln(k) d）），但最好的下界是Ω（ε^-2 (d + k \* ln(k))）。我们讨论了这个问题的最新进展和使用游戏动力学在统计学习中的核心障碍。
</details></li>
</ul>
<hr>
<h2 id="AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities"><a href="#AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities" class="headerlink" title="AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities"></a>AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12128">http://arxiv.org/abs/2307.12128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Adewopo, Nelly Elsayed, Zag Elsayed, Murat Ozer, Victoria Wangia-Anderson, Ahmed Abdelgawad</li>
<li>for: 本研究旨在提高交通管理和交通事故减少，通过分析不同地区的交通事故数据，提出一个基于交通监控摄像头和动作识别系统的交通事故探测和应答框架。</li>
<li>methods: 本研究使用了国家公路交通安全管理局（NHTSA）的交通事故报告采样系统（CRSS）数据进行交通事故分析，并提出了一种基于机器学习算法和交通监控摄像头的交通事故探测和应答框架。</li>
<li>results: 本研究发现了不同地区的交通事故特征和趋势，并提出了一种基于交通监控摄像头和动作识别系统的交通事故探测和应答框架，可以减少交通事故的频率和严重程度，提高交通管理的效率和安全性。<details>
<summary>Abstract</summary>
Accident detection and traffic analysis is a critical component of smart city and autonomous transportation systems that can reduce accident frequency, severity and improve overall traffic management. This paper presents a comprehensive analysis of traffic accidents in different regions across the United States using data from the National Highway Traffic Safety Administration (NHTSA) Crash Report Sampling System (CRSS). To address the challenges of accident detection and traffic analysis, this paper proposes a framework that uses traffic surveillance cameras and action recognition systems to detect and respond to traffic accidents spontaneously. Integrating the proposed framework with emergency services will harness the power of traffic cameras and machine learning algorithms to create an efficient solution for responding to traffic accidents and reducing human errors. Advanced intelligence technologies, such as the proposed accident detection systems in smart cities, will improve traffic management and traffic accident severity. Overall, this study provides valuable insights into traffic accidents in the US and presents a practical solution to enhance the safety and efficiency of transportation systems.
</details>
<details>
<summary>摘要</summary>
智能城市和自动交通系统中的事故探测和交通分析是一个关键组成部分，可以降低事故频率、严重程度并改善总体交通管理。这篇论文对美国各地的交通事故进行了全面的分析，使用国家公路安全管理局（NHTSA）的事故报告采样系统（CRSS）的数据。为了解决事故探测和交通分析的挑战，该论文提出了一个框架，使用交通监控摄像头和动作认知系统来自动探测和应对交通事故。将该框架与紧急服务集成，可以利用交通摄像头和机器学习算法创造一种高效的交通事故应对解决方案，减少人类错误。高级智能技术，如智能城市中的事故探测系统，将改善交通管理和交通事故严重程度。总的来说，这篇研究提供了美国交通事故的有价值的视角，并提出了实用的解决方案，以提高交通系统的安全和效率。
</details></li>
</ul>
<hr>
<h2 id="Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network"><a href="#Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network" class="headerlink" title="Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network"></a>Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12122">http://arxiv.org/abs/2307.12122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/octadion/diffusion-stylegan2-ada-pytorch">https://github.com/octadion/diffusion-stylegan2-ada-pytorch</a></li>
<li>paper_authors: One Octadion, Novanto Yudistira, Diva Kurnianingtyas</li>
<li>for:  assist batik designers or craftsmen in producing unique and quality batik motifs with efficient production time and costs.</li>
<li>methods:  using StyleGAN2-Ada and Diffusion techniques to produce realistic and high-quality synthetic batik patterns, with adjustments to the model architecture and a well-curated batik dataset.</li>
<li>results:  capable of producing authentic and quality batik patterns, with finer details and rich artistic variations.<details>
<summary>Abstract</summary>
Batik, a unique blend of art and craftsmanship, is a distinct artistic and technological creation for Indonesian society. Research on batik motifs is primarily focused on classification. However, further studies may extend to the synthesis of batik patterns. Generative Adversarial Networks (GANs) have been an important deep learning model for generating synthetic data, but often face challenges in the stability and consistency of results. This research focuses on the use of StyleGAN2-Ada and Diffusion techniques to produce realistic and high-quality synthetic batik patterns. StyleGAN2-Ada is a variation of the GAN model that separates the style and content aspects in an image, whereas diffusion techniques introduce random noise into the data. In the context of batik, StyleGAN2-Ada and Diffusion are used to produce realistic synthetic batik patterns. This study also made adjustments to the model architecture and used a well-curated batik dataset. The main goal is to assist batik designers or craftsmen in producing unique and quality batik motifs with efficient production time and costs. Based on qualitative and quantitative evaluations, the results show that the model tested is capable of producing authentic and quality batik patterns, with finer details and rich artistic variations. The dataset and code can be accessed here:https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details>
<details>
<summary>摘要</summary>
《独特的抽象艺术》——巴迪克的研究巴迪克是印度尼西亚社会独特的艺术和手工艺术品。研究巴迪克图案主要集中在分类方面，但可能会扩展到synthesize batik patterns。生成对抗网络（GANs）是深度学习模型，可以生成 sintetic data，但经常面临稳定性和一致性的挑战。本研究使用StyleGAN2-Ada和扩散技术生成高质量和真实的 sintetic batik patterns。StyleGAN2-Ada分离图像中的风格和内容两个方面，而扩散技术引入随机噪音。在batik中，StyleGAN2-Ada和扩散被用来生成真实的 sintetic batik patterns。本研究还对模型结构进行了调整，使用了高质量的batik dataset。主要目标是帮助batik设计师或手工艺术家生成独特和高质量的batik图案，以及减少生产时间和成本。根据质量和量的评估，研究结果表明模型能够生成authentic和高质量的batik patterns，具有细节和艺术变化。数据集和代码可以在以下链接获取：https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.LG_2023_07_23/" data-id="clorjzl8400mpf188hbbq6qul" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/eess.IV_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T09:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/eess.IV_2023_07_23/">eess.IV - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection"><a href="#ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection" class="headerlink" title="ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection"></a>ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12327">http://arxiv.org/abs/2307.12327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingren Yao, Yuan Zhou, Wei Xiang</li>
<li>for: 本研究旨在提高迁移图像特征检测精度，具体目的是针对高spectralresolution干扰图像（HSIs）进行迁移特征检测。</li>
<li>methods: 本研究提出了一种综合利用深度学习和频谱筛选技术的迁移特征检测网络（ES2Net），其中包括一个学习式频谱筛选模块，可以自动选择适合迁移检测的频谱 bands。此外，为了考虑不同频谱 bands 之间的复杂非线性关系，我们还提出了一种帧度级空间注意力机制。</li>
<li>results: 实验表明，ES2Net 比其他当前领域state-of-the-art方法更高效和精度。<details>
<summary>Abstract</summary>
Hyperspectral image change detection (HSI-CD) aims to identify the differences in bitemporal HSIs. To mitigate spectral redundancy and improve the discriminativeness of changing features, some methods introduced band selection technology to select bands conducive for CD. However, these methods are limited by the inability to end-to-end training with the deep learning-based feature extractor and lack considering the complex nonlinear relationship among bands. In this paper, we propose an end-to-end efficient spectral-spatial change detection network (ES2Net) to address these issues. Specifically, we devised a learnable band selection module to automatically select bands conducive to CD. It can be jointly optimized with a feature extraction network and capture the complex nonlinear relationships among bands. Moreover, considering the large spatial feature distribution differences among different bands, we design the cluster-wise spatial attention mechanism that assigns a spatial attention factor to each individual band to individually improve the feature discriminativeness for each band. Experiments on three widely used HSI-CD datasets demonstrate the effectiveness and superiority of this method compared with other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
干支图像变化检测（HSI-CD）的目标是确定双时间干支图像之间的差异。为了减少 спектраль的重复性和提高变化特征的抑制力，一些方法引入了频率选择技术，以选择适合变化检测的频率。然而，这些方法受到练习深度学习基于特征提取器的端到端训练的限制，以及各频率之间的复杂非线性关系的忽略。在本文中，我们提出了一种练习效率的 spectral-spatial 变化检测网络（ES2Net），以解决这些问题。具体来说，我们设计了一个学习型频率选择模块，可以自动选择适合变化检测的频率。这个模块可以与特征提取网络jointly 优化，并 capture 各频率之间的复杂非线性关系。此外，考虑到不同频率之间的大规模空间特征分布差异，我们设计了各个频率的各自精度注意力机制，以进一步提高每个频率的特征抑制力。在三个广泛使用的HSI-CD数据集上进行了实验，我们发现该方法与其他当前状态的方法相比，具有更高的效iveness和优势。
</details></li>
</ul>
<hr>
<h2 id="Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models"><a href="#Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models" class="headerlink" title="Development of pericardial fat count images using a combination of three different deep-learning models"></a>Development of pericardial fat count images using a combination of three different deep-learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12316">http://arxiv.org/abs/2307.12316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takaaki Matsunaga, Atsushi Kono, Hidetoshi Matsuo, Kaoru Kitagawa, Mizuho Nishio, Hiromi Hashimura, Yu Izawa, Takayoshi Toba, Kazuki Ishikawa, Akie Katsuki, Kazuyuki Ohmura, Takamichi Murakami</li>
<li>for: 这个研究旨在使用深度学习模型从胸部X射线图像中生成胸部脂肪计数图像（PFCIs），以评估胸部脂肪的水平。</li>
<li>methods: 这个研究使用了3种不同的深度学习模型，包括CycleGAN，将胸部CT图像投影到2D图像上，并使用高像素值表示脂肪聚集。</li>
<li>results: 比较了使用提案方法生成的PFCIs和单一CycleGAN-based模型生成的PFCIs，发现提案方法生成的PFCIs具有更高的图像质量指标（SSIM、MSE、MAE）。<details>
<summary>Abstract</summary>
Rationale and Objectives: Pericardial fat (PF), the thoracic visceral fat surrounding the heart, promotes the development of coronary artery disease by inducing inflammation of the coronary arteries. For evaluating PF, this study aimed to generate pericardial fat count images (PFCIs) from chest radiographs (CXRs) using a dedicated deep-learning model.   Materials and Methods: The data of 269 consecutive patients who underwent coronary computed tomography (CT) were reviewed. Patients with metal implants, pleural effusion, history of thoracic surgery, or that of malignancy were excluded. Thus, the data of 191 patients were used. PFCIs were generated from the projection of three-dimensional CT images, where fat accumulation was represented by a high pixel value. Three different deep-learning models, including CycleGAN, were combined in the proposed method to generate PFCIs from CXRs. A single CycleGAN-based model was used to generate PFCIs from CXRs for comparison with the proposed method. To evaluate the image quality of the generated PFCIs, structural similarity index measure (SSIM), mean squared error (MSE), and mean absolute error (MAE) of (i) the PFCI generated using the proposed method and (ii) the PFCI generated using the single model were compared.   Results: The mean SSIM, MSE, and MAE were as follows: 0.856, 0.0128, and 0.0357, respectively, for the proposed model; and 0.762, 0.0198, and 0.0504, respectively, for the single CycleGAN-based model.   Conclusion: PFCIs generated from CXRs with the proposed model showed better performance than those with the single model. PFCI evaluation without CT may be possible with the proposed method.
</details>
<details>
<summary>摘要</summary>
理解和目标：胸膈脂肪（PF），脊梗内脂肪环绕心脏，促进了折射病变的发展。为评估PF，本研究旨在通过专门的深度学习模型生成胸膈脂肪计数图像（PFCIs）从胸部X射线图（CXRs）中。材料和方法：本研究查阅了269例 consecutively患者的折射 computed tomography（CT）数据。患有金属设备、肿胀、历史折射手术或肿瘤患者被排除。因此，该研究使用了191例患者的数据。PFCIs通过三维CT图像的投影，表示脂肪堆积的高像素值来生成。三种不同的深度学习模型，包括CycleGAN，被组合在提议的方法中来生成PFCIs从CXRs。单独使用CycleGAN基于模型生成PFCIs从CXRs作为比较。为评估生成的PFCIs的图像质量，使用了结构相似度指标（SSIM）、平均方差（MSE）和平均绝对错误（MAE）进行比较。结果：生成的PFCIs的SSIM、MSE和MAE分别为：0.856、0.0128和0.0357；而单独使用CycleGAN基于模型生成的PFCIs的SSIM、MSE和MAE分别为：0.762、0.0198和0.0504。结论：提议的方法生成的PFCIs在SSIM、MSE和MAE指标上表现更好于单独使用CycleGAN基于模型生成的PFCIs。PFCI评估可能不需要CT成像。
</details></li>
</ul>
<hr>
<h2 id="Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames"><a href="#Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames" class="headerlink" title="Simultaneous temperature estimation and nonuniformity correction from multiple frames"></a>Simultaneous temperature estimation and nonuniformity correction from multiple frames</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12297">http://arxiv.org/abs/2307.12297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Omri Berman, Nir Sochen, David Mendelovich, Iftach Klapp</li>
<li>for: 用于温度测量的低成本紫外线相机中的非均匀性和温度测量偏差问题的解决方案。</li>
<li>methods: 基于物理捕获模型和深度学习核函数网络（KPN）的方法，通过将多帧图像 fusion 来实现同时的温度估计和非均匀性修正。另外，还提出了一个新的偏移块，可以将 ambient 温度包含在模型中，以便估计相机的偏移。</li>
<li>results: 通过对实际数据进行测试，得到了与高科技质量的放射计相机相比，仅有0.27-0.54℃的小差异。这种方法可以提供高精度和高效的同时温度估计和非均匀性修正解决方案，对各种实际应用场景具有重要意义。<details>
<summary>Abstract</summary>
Infrared (IR) cameras are widely used for temperature measurements in various applications, including agriculture, medicine, and security. Low-cost IR camera have an immense potential to replace expansive radiometric cameras in these applications, however low-cost microbolometer-based IR cameras are prone to spatially-variant nonuniformity and to drift in temperature measurements, which limits their usability in practical scenarios.   To address these limitations, we propose a novel approach for simultaneous temperature estimation and nonuniformity correction from multiple frames captured by low-cost microbolometer-based IR cameras. We leverage the physical image acquisition model of the camera and incorporate it into a deep learning architecture called kernel estimation networks (KPN), which enables us to combine multiple frames despite imperfect registration between them. We also propose a novel offset block that incorporates the ambient temperature into the model and enables us to estimate the offset of the camera, which is a key factor in temperature estimation.   Our findings demonstrate that the number of frames has a significant impact on the accuracy of temperature estimation and nonuniformity correction. Moreover, our approach achieves a significant improvement in performance compared to vanilla KPN, thanks to the offset block. The method was tested on real data collected by a low-cost IR camera mounted on a UAV, showing only a small average error of $0.27^\circ C-0.54^\circ C$ relative to costly scientific-grade radiometric cameras.   Our method provides an accurate and efficient solution for simultaneous temperature estimation and nonuniformity correction, which has important implications for a wide range of practical applications.
</details>
<details>
<summary>摘要</summary>
低成本红外线（IR）镜头在不同应用中广泛使用，包括农业、医学和安全领域。低成本微波温度计IR镜头具有巨大的潜在可能性，以取代昂贵的几何学测量镜头，但是它们受到空间不均匀和测量偏差的限制，这限制了它们在实际应用中的可用性。为了解决这些限制，我们提出了一个新的方法，可以同时进行测量温度和非均匀调正。我们利用镜头的物理摄取模型，并将其 integrate into a deep learning architecture called kernel estimation networks (KPN)，这使得我们可以融合多帧影像，即使它们不具有完美的对齐。我们还提出了一个 novel offset block，它包含了环境温度，并允许我们估计镜头的偏移，这是温度估计中的关键因素。我们的研究表明，影像数量有显著的影响温度估计和非均匀调正的精度。此外，我们的方法在比vanilla KPN更好的性能，感谢偏移层的存在。我们的方法在实际应用中使用低成本IR镜头，与较贵的科学级测量镜头相比， пока有小平均误差为0.27-0.54℃。我们的方法提供了一个精确和高效的温度估计和非均匀调正方法，具有广泛的实际应用。
</details></li>
</ul>
<hr>
<h2 id="ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising"><a href="#ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising" class="headerlink" title="ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising"></a>ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12225">http://arxiv.org/abs/2307.12225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hao1635/ASCON">https://github.com/hao1635/ASCON</a></li>
<li>paper_authors: Zhihao Chen, Qi Gao, Yi Zhang, Hongming Shan</li>
<li>for: 这个论文是设计来进行低剂量 Computed Tomography（CT）扫描图像干扰除掉噪声的方法。</li>
<li>methods: 这个方法使用了两个新的设计：一个高效的自我注意力-based U-Net（ESAU-Net）和一个多尺度解剖对比网络（MAC-Net）。ESAU-Net使用了通道对自我注意力的机制来更好地捕捉全域-地方互动，而MAC-Net则包括一个单元非对比模组来捕捉解剖信息和一个像素对比模组来维持自体解剖一致性。</li>
<li>results: 实验结果显示，ASCON在两个公共的低剂量 CT 扫描干扰 dataset 上表现出色，较之前的模型更好。此外，ASCON还提供了解剖解释，允许在低剂量 CT 扫描中进行解剖可读性检查。<details>
<summary>Abstract</summary>
While various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, most of them leverage the normal-dose CT images as the ground-truth to supervise the denoising process. These methods typically ignore the inherent correlation within a single CT image, especially the anatomical semantics of human tissues, and lack the interpretability on the denoising process. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to the high-resolution input, an efficient ESAU-Net is introduced by using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models. Remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github.com/hao1635/ASCON.
</details>
<details>
<summary>摘要</summary>
“Various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, but most of them rely on normal-dose CT images as ground truth to supervise the denoising process, ignoring the inherent correlation within a single CT image and lacking interpretability. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to high-resolution input, an efficient ESAU-Net is introduced using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models, and remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github.com/hao1635/ASCON.”Note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images"><a href="#SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images" class="headerlink" title="SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images"></a>SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12138">http://arxiv.org/abs/2307.12138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueshen Li, Hongshan Liu, Xiaoyu Song, Brigitta C. Brott, Silvio H. Litovsky, Yu Gan</li>
<li>for: 用于生成基于OCT图像的虚拟病理信息，以更好地指导心血管疾病的治疗。</li>
<li>methods: 使用 transformer生成对抗网络，并在结构层进行病理导向的制约，以生成虚拟染色H&amp;E压痕。</li>
<li>results: 提高了现有方法的病理准确率和结构准确率，并可以在不需要大量 paired 训练数据的情况下生成虚拟病理信息。<details>
<summary>Abstract</summary>
There is a significant need for the generation of virtual histological information from coronary optical coherence tomography (OCT) images to better guide the treatment of coronary artery disease. However, existing methods either require a large pixel-wisely paired training dataset or have limited capability to map pathological regions. To address these issues, we proposed a structural constrained, pathology aware, transformer generative adversarial network, namely SCPAT-GAN, to generate virtual stained H&E histology from OCT images. The proposed SCPAT-GAN advances existing methods via a novel design to impose pathological guidance on structural layers using transformer-based network.
</details>
<details>
<summary>摘要</summary>
“ coronary optical coherence tomography（OCT）图像中的虚拟 histological 信息的生成具有抑制 coronary artery disease 的治疗方法的重要需求。然而，现有的方法 either require a large paired training dataset or have limited capability to map pathological regions。为解决这些问题，我们提出了一种基于 transformer 的权重约束、病理相关的生成对抗网络，即 SCPAT-GAN，用于从 OCT 图像中生成虚拟染色 H&E 病理图像。我们的提议的 SCPAT-GAN 在现有方法中提供了一种新的设计，通过 transformer 基于的网络来强制 paths 层中的病理指导。”Note: Please keep in mind that the translation is done by a machine and may not be perfect. If you have any further questions or need more accurate translations, please feel free to ask!
</details></li>
</ul>
<hr>
<h2 id="Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks"><a href="#Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks" class="headerlink" title="Improving temperature estimation in low-cost infrared cameras using deep neural networks"></a>Improving temperature estimation in low-cost infrared cameras using deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12130">http://arxiv.org/abs/2307.12130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Nir Sochen, David Mendelovich, Iftach Klapp</li>
<li>for: 提高低成本热相机的温度准确性和纠正非均匀性。</li>
<li>methods: 开发了一个考虑 ambient temperature 的非均匀性模拟器，并提出了一种基于全连接神经网络的热体温度估算方法，通过使用单个图像和摄像头自身测量的 ambient temperature 来纠正非均匀性。</li>
<li>results: 比前作下降了约 $1^\circ C$ 的平均温度误差，并且通过应用物理约束降低了误差的 $4%$。 验证数据集上的平均温度误差为 $0.37^\circ C$，并在实际场景中也得到了相当的结果。<details>
<summary>Abstract</summary>
Low-cost thermal cameras are inaccurate (usually $\pm 3^\circ C$) and have space-variant nonuniformity across their detector. Both inaccuracy and nonuniformity are dependent on the ambient temperature of the camera. The main goal of this work was to improve the temperature accuracy of low-cost cameras and rectify the nonuniformity.   A nonuniformity simulator that accounts for the ambient temperature was developed. An end-to-end neural network that incorporates the ambient temperature at image acquisition was introduced. The neural network was trained with the simulated nonuniformity data to estimate the object's temperature and correct the nonuniformity, using only a single image and the ambient temperature measured by the camera itself. Results show that the proposed method lowered the mean temperature error by approximately $1^\circ C$ compared to previous works. In addition, applying a physical constraint on the network lowered the error by an additional $4\%$.   The mean temperature error over an extensive validation dataset was $0.37^\circ C$. The method was verified on real data in the field and produced equivalent results.
</details>
<details>
<summary>摘要</summary>
低成本热相机的精度受到 ambient temperature 的影响（通常在 $\pm 3^\circ C$ 范围内），并且具有空间不均的非统一性，这两个问题都与热相机的 ambient temperature 相关。本研究的主要目标是提高低成本热相机的温度精度和修正非统一性。我们开发了一个考虑 ambient temperature 的非统一性模拟器，并提出了一种基于 neural network 的方法，该方法可以使用单个图像和热相机自己测量的 ambient temperature 来估计物体温度并修正非统一性。实验结果表明，我们的方法可以降低mean温度错误约为 $1^\circ C$  compared to 前一代方法。此外，通过 físical 约束对网络进行限制，可以降低错误约为 $4\%$。整体来说，我们的方法在广泛验证数据集上的 mean 温度错误为 $0.37^\circ C$。此外，我们的方法在实际场景中也得到了Equivalent 的结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/eess.IV_2023_07_23/" data-id="clorjzlf1014af1881c291mf5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/cs.CV_2023_07_22/" class="article-date">
  <time datetime="2023-07-22T13:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/22/cs.CV_2023_07_22/">cs.CV - 2023-07-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Spatial-Self-Distillation-for-Object-Detection-with-Inaccurate-Bounding-Boxes"><a href="#Spatial-Self-Distillation-for-Object-Detection-with-Inaccurate-Bounding-Boxes" class="headerlink" title="Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes"></a>Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12101">http://arxiv.org/abs/2307.12101</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucas-vg/pointtinybenchmark">https://github.com/ucas-vg/pointtinybenchmark</a></li>
<li>paper_authors: Di Wu, Pengfei Chen, Xuehui Yu, Guorong Li, Zhenjun Han, Jianbin Jiao</li>
<li>for: 提高object detection的精度和效率，使用低质量 bounding box 监督。</li>
<li>methods: 使用 Spatial Position Self-Distillation (SPSD) 模块和 Spatial Identity Self-Distillation (SISD) 模块， combinig spatial information 和 category information，以提高提档箱的选择过程。</li>
<li>results: 在 MS-COCO 和 VOC  datasets 上，与无isy box 监督实现 state-of-the-art 性能。<details>
<summary>Abstract</summary>
Object detection via inaccurate bounding boxes supervision has boosted a broad interest due to the expensive high-quality annotation data or the occasional inevitability of low annotation quality (\eg tiny objects). The previous works usually utilize multiple instance learning (MIL), which highly depends on category information, to select and refine a low-quality box. Those methods suffer from object drift, group prediction and part domination problems without exploring spatial information. In this paper, we heuristically propose a \textbf{Spatial Self-Distillation based Object Detector (SSD-Det)} to mine spatial information to refine the inaccurate box in a self-distillation fashion. SSD-Det utilizes a Spatial Position Self-Distillation \textbf{(SPSD)} module to exploit spatial information and an interactive structure to combine spatial information and category information, thus constructing a high-quality proposal bag. To further improve the selection procedure, a Spatial Identity Self-Distillation \textbf{(SISD)} module is introduced in SSD-Det to obtain spatial confidence to help select the best proposals. Experiments on MS-COCO and VOC datasets with noisy box annotation verify our method's effectiveness and achieve state-of-the-art performance. The code is available at https://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det.
</details>
<details>
<summary>摘要</summary>
Object detection via inaccurate bounding boxes supervision has aroused broad interest due to the high cost of high-quality annotation data or the occasional low annotation quality (e.g., tiny objects). Previous works usually rely on multiple instance learning (MIL), which heavily relies on category information, to select and refine a low-quality box. These methods are plagued by object drift, group prediction, and part domination problems without exploring spatial information. In this paper, we propose a Spatial Self-Distillation based Object Detector (SSD-Det) to exploit spatial information to refine the inaccurate box in a self-distillation manner. SSD-Det utilizes a Spatial Position Self-Distillation (SPSD) module to leverage spatial information and an interactive structure to combine spatial information and category information, thus constructing a high-quality proposal bag. To further improve the selection procedure, a Spatial Identity Self-Distillation (SISD) module is introduced in SSD-Det to obtain spatial confidence to help select the best proposals. Experiments on MS-COCO and VOC datasets with noisy box annotation demonstrate the effectiveness of our method and achieve state-of-the-art performance. The code is available at https://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det.
</details></li>
</ul>
<hr>
<h2 id="Edge-Guided-GANs-with-Multi-Scale-Contrastive-Learning-for-Semantic-Image-Synthesis"><a href="#Edge-Guided-GANs-with-Multi-Scale-Contrastive-Learning-for-Semantic-Image-Synthesis" class="headerlink" title="Edge Guided GANs with Multi-Scale Contrastive Learning for Semantic Image Synthesis"></a>Edge Guided GANs with Multi-Scale Contrastive Learning for Semantic Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12084">http://arxiv.org/abs/2307.12084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ha0tang/ecgan">https://github.com/ha0tang/ecgan</a></li>
<li>paper_authors: Hao Tang, Guolei Sun, Nicu Sebe, Luc Van Gool</li>
<li>for: 提出了一种新的ECGAN方法，用于解决 semantic image synthesis 问题。</li>
<li>methods: 使用 edge 作为中间表示，并通过提案的注意力导向Edge传输模块来引导图像生成。同时，设计了一种选择性高亮类征图像的模块，以保持 semantic 信息。</li>
<li>results: 提出了一种新的对比学习方法，用于让同类像素内容更相似，并在多个输入Semantic layout中捕捉更多的semantic关系。<details>
<summary>Abstract</summary>
We propose a novel ECGAN for the challenging semantic image synthesis task. Although considerable improvements have been achieved by the community in the recent period, the quality of synthesized images is far from satisfactory due to three largely unresolved challenges. 1) The semantic labels do not provide detailed structural information, making it challenging to synthesize local details and structures; 2) The widely adopted CNN operations such as convolution, down-sampling, and normalization usually cause spatial resolution loss and thus cannot fully preserve the original semantic information, leading to semantically inconsistent results (e.g., missing small objects); 3) Existing semantic image synthesis methods focus on modeling 'local' semantic information from a single input semantic layout. However, they ignore 'global' semantic information of multiple input semantic layouts, i.e., semantic cross-relations between pixels across different input layouts. To tackle 1), we propose to use the edge as an intermediate representation which is further adopted to guide image generation via a proposed attention guided edge transfer module. To tackle 2), we design an effective module to selectively highlight class-dependent feature maps according to the original semantic layout to preserve the semantic information. To tackle 3), inspired by current methods in contrastive learning, we propose a novel contrastive learning method, which aims to enforce pixel embeddings belonging to the same semantic class to generate more similar image content than those from different classes. We further propose a novel multi-scale contrastive learning method that aims to push same-class features from different scales closer together being able to capture more semantic relations by explicitly exploring the structures of labeled pixels from multiple input semantic layouts from different scales.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的ECGAN，用于解决 semantic image synthesis 任务中的三大挑战。尽管社区在最近一段时间内已经取得了显著的进步，但是synthesized图像的质量仍然远远不够满意，主要因为以下三个因素：1）semantic labels 不含细节信息，因此难以synthesize  мест Details和结构; 2）通用的CNN操作，如 convolution、下采样和normalization，通常会导致空间分辨率损失，从而无法完全保留原始semantic信息，导致结果不一致（例如缺少小对象）; 3）现有的semantic image synthesis方法都是基于单个输入semantic layout的本地semantic信息模型化。然而，它们忽略了多个输入semantic layout的semantic信息之间的关系，即多个输入semantic layout中的semantic cross-relations。为了解决1），我们提议使用边为 intermediate representation，并将其采用提取模块来导引图像生成。为了解决2），我们设计了一种有效的模块，可以根据原始semantic layout选择性地强调类型相关的特征图。为了解决3），我们提出了一种基于contrastive learning的新方法，该方法 aimsto enforce pixel embeddings belonging to the same semantic class to generate more similar image content than those from different classes。我们还提出了一种多尺度contrastive learning方法，该方法 aimsto push same-class features from different scales closer together，以便捕捉多个输入semantic layout中的semantic关系。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Reconstruction-Based-on-Latent-Diffusion-Model-for-Sparse-Data-Reconstruction"><a href="#Iterative-Reconstruction-Based-on-Latent-Diffusion-Model-for-Sparse-Data-Reconstruction" class="headerlink" title="Iterative Reconstruction Based on Latent Diffusion Model for Sparse Data Reconstruction"></a>Iterative Reconstruction Based on Latent Diffusion Model for Sparse Data Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12070">http://arxiv.org/abs/2307.12070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linchao He, Hongyu Yan, Mengting Luo, Kunming Luo, Wang Wang, Wenchao Du, Hu Chen, Hongyu Yang, Yi Zhang</li>
<li>For: The paper is written for reconstructing computed tomography (CT) images from sparse measurements, which is an ill-posed inverse problem. The paper proposes a new method called Latent Diffusion Iterative Reconstruction (LDIR) to solve this problem.* Methods: LDIR uses a pre-trained Latent Diffusion Model (LDM) as a data prior to extend the Iterative Reconstruction (IR) method. The LDM is used to approximate the prior distribution of the CT images, and the gradient from the data-fidelity term is used to guide the sampling process. This allows LDIR to integrate iterative reconstruction and LDM in an unsupervised manner, making the reconstruction of high-resolution images more efficient.* Results: The paper shows that LDIR outperforms other state-of-the-art unsupervised and even exceeds supervised methods in terms of both quantity and quality on extremely sparse CT data reconstruction tasks. Additionally, LDIR achieves competitive performance on nature image tasks and exhibits significantly faster execution times and lower memory consumption compared to methods with similar network settings.<details>
<summary>Abstract</summary>
Reconstructing Computed tomography (CT) images from sparse measurement is a well-known ill-posed inverse problem. The Iterative Reconstruction (IR) algorithm is a solution to inverse problems. However, recent IR methods require paired data and the approximation of the inverse projection matrix. To address those problems, we present Latent Diffusion Iterative Reconstruction (LDIR), a pioneering zero-shot method that extends IR with a pre-trained Latent Diffusion Model (LDM) as a accurate and efficient data prior. By approximating the prior distribution with an unconditional latent diffusion model, LDIR is the first method to successfully integrate iterative reconstruction and LDM in an unsupervised manner. LDIR makes the reconstruction of high-resolution images more efficient. Moreover, LDIR utilizes the gradient from the data-fidelity term to guide the sampling process of the LDM, therefore, LDIR does not need the approximation of the inverse projection matrix and can solve various CT reconstruction tasks with a single model. Additionally, for enhancing the sample consistency of the reconstruction, we introduce a novel approach that uses historical gradient information to guide the gradient. Our experiments on extremely sparse CT data reconstruction tasks show that LDIR outperforms other state-of-the-art unsupervised and even exceeds supervised methods, establishing it as a leading technique in terms of both quantity and quality. Furthermore, LDIR also achieves competitive performance on nature image tasks. It is worth noting that LDIR also exhibits significantly faster execution times and lower memory consumption compared to methods with similar network settings. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将计算Tomography（CT）图像重建问题转化为简单的无监测问题。iterative Reconstruction（IR）算法是解决无监测问题的一种方法。然而，最近的IR方法需要匹配数据和近似投影矩阵的approximation。为了解决这些问题，我们介绍Latent Diffusion Iterative Reconstruction（LDIR），一种革新的零shot方法，通过将iterative Reconstruction和Latent Diffusion Model（LDM）集成在一起，以获得高效和准确的数据先天。LDIR通过近似 latent diffusion模型来表示先天分布，因此不需要近似投影矩阵的approximation，可以解决多种CT重建任务。此外，我们还引入了一种新的方法，使用历史梯度信息来导引抽象过程。我们的实验表明，LDIR在极端稀畴CT数据重建任务中表现出色，超过了其他无监测和监测方法，成为当前领导技术。此外，LDIR还在自然图像任务中表现竞争力强。值得注意的是，LDIR也显示出了相对较快的执行时间和较低的内存占用量，相比于与相同网络设置的方法。我们将代码公开。
</details></li>
</ul>
<hr>
<h2 id="Replay-Multi-modal-Multi-view-Acted-Videos-for-Casual-Holography"><a href="#Replay-Multi-modal-Multi-view-Acted-Videos-for-Casual-Holography" class="headerlink" title="Replay: Multi-modal Multi-view Acted Videos for Casual Holography"></a>Replay: Multi-modal Multi-view Acted Videos for Casual Holography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12067">http://arxiv.org/abs/2307.12067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/replay_dataset">https://github.com/facebookresearch/replay_dataset</a></li>
<li>paper_authors: Roman Shapovalov, Yanir Kleiman, Ignacio Rocco, David Novotny, Andrea Vedaldi, Changan Chen, Filippos Kokkinos, Ben Graham, Natalia Neverova</li>
<li>for: 这个论文主要用于提供一个多视角多Modal视频人类社交交互的收集，可以用于多种应用，如新视角合成、3D重建、人体和脸部分析等。</li>
<li>methods: 这个论文使用了多个高品质摄像头和仪器来捕捉和记录人类社交交互的视频，并对视频进行了高精度时间戳和相机pose的标注。</li>
<li>results: 这个论文提供了一个包含超过4000分钟视频和700万个高分辨率帧的大规模数据集，并提供了一个基准测试集用于训练和评估新视角合成方法。<details>
<summary>Abstract</summary>
We introduce Replay, a collection of multi-view, multi-modal videos of humans interacting socially. Each scene is filmed in high production quality, from different viewpoints with several static cameras, as well as wearable action cameras, and recorded with a large array of microphones at different positions in the room. Overall, the dataset contains over 4000 minutes of footage and over 7 million timestamped high-resolution frames annotated with camera poses and partially with foreground masks. The Replay dataset has many potential applications, such as novel-view synthesis, 3D reconstruction, novel-view acoustic synthesis, human body and face analysis, and training generative models. We provide a benchmark for training and evaluating novel-view synthesis, with two scenarios of different difficulty. Finally, we evaluate several baseline state-of-the-art methods on the new benchmark.
</details>
<details>
<summary>摘要</summary>
我们介绍Replay数据集，这是一个多视点多Modal人类社交互动的集合。每场场景由高品质摄像机拍摄，包括固定摄像机和穿戴式动作摄像机，以及不同位置的听音器。总的来说，数据集包含超过4000分钟的视频和700万个时间戳的高分辨率帧，并有部分帧附有相机姿态和部分人体或脸部掩码。Replay数据集有许多应用 potential，如新视图合成、3D重建、新视图声音合成、人体和脸部分析，以及训练生成模型。我们提供了一个对novel-view synthesis进行训练和评估的标准准。最后，我们评估了一些基eline状态的先进方法在新的标准上。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Spatio-Temporal-Rationales-for-Video-Question-Answering"><a href="#Discovering-Spatio-Temporal-Rationales-for-Video-Question-Answering" class="headerlink" title="Discovering Spatio-Temporal Rationales for Video Question Answering"></a>Discovering Spatio-Temporal Rationales for Video Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12058">http://arxiv.org/abs/2307.12058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yicong Li, Junbin Xiao, Chun Feng, Xiang Wang, Tat-Seng Chua</li>
<li>for: 解决复杂的视频问答（VideoQA）问题，其中视频具有多个物体和事件，发生在不同的时间点。</li>
<li>methods: 提出了一种Spatio-Temporal Rationalization（STR），这是一种可 diferenciable 选择模块，可以适应тив地从视频内容中收集问题关键的时间点和空间对象。此外，还提出了一种基于STR的Transformer-style neural network架构，名为TranSTR，它还强调了一种新的答案交互机制来协调STR。</li>
<li>results: 实验结果表明，TranSTR在四个dataset上达到了新的State-of-the-art（SoTA）水平，特别是在NExT-QA和Causal-VidQA上，它超过了之前的SoTA by 5.8%和6.8%。此外，我们还进行了广泛的研究来证明STR的重要性以及提出的答案交互机制的重要性。<details>
<summary>Abstract</summary>
This paper strives to solve complex video question answering (VideoQA) which features long video containing multiple objects and events at different time. To tackle the challenge, we highlight the importance of identifying question-critical temporal moments and spatial objects from the vast amount of video content. Towards this, we propose a Spatio-Temporal Rationalization (STR), a differentiable selection module that adaptively collects question-critical moments and objects using cross-modal interaction. The discovered video moments and objects are then served as grounded rationales to support answer reasoning. Based on STR, we further propose TranSTR, a Transformer-style neural network architecture that takes STR as the core and additionally underscores a novel answer interaction mechanism to coordinate STR for answer decoding. Experiments on four datasets show that TranSTR achieves new state-of-the-art (SoTA). Especially, on NExT-QA and Causal-VidQA which feature complex VideoQA, it significantly surpasses the previous SoTA by 5.8\% and 6.8\%, respectively. We then conduct extensive studies to verify the importance of STR as well as the proposed answer interaction mechanism. With the success of TranSTR and our comprehensive analysis, we hope this work can spark more future efforts in complex VideoQA. Code will be released at https://github.com/yl3800/TranSTR.
</details>
<details>
<summary>摘要</summary>
这篇论文目标解决复杂的视频问答（VideoQA）问题，该问题具有长视频内容中的多个物体和事件，并且发生在不同的时间点。为了解决这个挑战，我们强调了问题关键的时间刻和空间对象的标识，并提出了一种空间时间合理化（STR）模块，该模块通过交叉模式互动来适应性地收集问题关键的时间刻和空间对象。得到的视频刻和对象将被用作问题理解的基础理据。基于STR，我们还提出了TransSTR，一种基于Transformer的神经网络架构，该架构将STR作为核心，并强调了一种新的答案互动机制以协调STR进行答案解码。实验结果表明，TransSTR在四个数据集上达到了新的状态时刻（SoTA），尤其是在NExT-QA和Causal-VidQA这两个复杂的VideoQA数据集上，与之前的SoTA相比，它提高了5.8%和6.8%。我们还进行了广泛的研究来证明STR的重要性以及我们提议的答案互动机制的重要性。通过TransSTR和我们的全面分析，我们希望这项工作可以激发更多的未来的VideoQA研究。代码将在GitHub上发布。
</details></li>
</ul>
<hr>
<h2 id="Patch-Wise-Point-Cloud-Generation-A-Divide-and-Conquer-Approach"><a href="#Patch-Wise-Point-Cloud-Generation-A-Divide-and-Conquer-Approach" class="headerlink" title="Patch-Wise Point Cloud Generation: A Divide-and-Conquer Approach"></a>Patch-Wise Point Cloud Generation: A Divide-and-Conquer Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12049">http://arxiv.org/abs/2307.12049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenc13/patchgeneration">https://github.com/wenc13/patchgeneration</a></li>
<li>paper_authors: Cheng Wen, Baosheng Yu, Rao Fu, Dacheng Tao</li>
<li>for: 本研究旨在生成高精度点云，用于自动驾驶和机器人等应用。</li>
<li>methods: 提出了一种新的点云生成框架，使用分割和聚合的方法，将整个生成过程分解成多个小区域生成任务。每个小区域生成器都基于学习的先验，用于捕捉点云的几何结构信息。还引入了点和小区域之间的交互transformer，以便在不同尺度上进行交互。</li>
<li>results: 实验结果表明，提出的小区域生成方法可以准确地生成高精度点云，并且在ShapeNet数据集上表现出色，与现有的状态对点云生成方法进行比较。<details>
<summary>Abstract</summary>
A generative model for high-fidelity point clouds is of great importance in synthesizing 3d environments for applications such as autonomous driving and robotics. Despite the recent success of deep generative models for 2d images, it is non-trivial to generate 3d point clouds without a comprehensive understanding of both local and global geometric structures. In this paper, we devise a new 3d point cloud generation framework using a divide-and-conquer approach, where the whole generation process can be divided into a set of patch-wise generation tasks. Specifically, all patch generators are based on learnable priors, which aim to capture the information of geometry primitives. We introduce point- and patch-wise transformers to enable the interactions between points and patches. Therefore, the proposed divide-and-conquer approach contributes to a new understanding of point cloud generation from the geometry constitution of 3d shapes. Experimental results on a variety of object categories from the most popular point cloud dataset, ShapeNet, show the effectiveness of the proposed patch-wise point cloud generation, where it clearly outperforms recent state-of-the-art methods for high-fidelity point cloud generation.
</details>
<details>
<summary>摘要</summary>
一个高级别点云生成模型对于 sintesizing 3D 环境而言是非常重要的，特别是在自动驾驶和机器人应用中。虽然最近的深度生成模型在 2D 图像方面已经取得了成功，但是生成 3D 点云则不是一件容易的事情，需要全面了解点云的本地和全局 геометрической结构。在这篇论文中，我们提出了一种新的点云生成框架，使用分治方法，整个生成过程可以分解为一系列的小区域生成任务。具体来说，所有的小区域生成器都基于学习的先验，旨在捕捉点云中的几何基本元素。我们引入了点云和小区域之间的交互，使得我们的分治方法在点云生成中做出了新的贡献，帮助我们更好地理解点云生成的几何结构。我们的实验结果表明，在ShapeNet 上的多种物体类别上，我们的裂解方法可以高效地生成高级别的点云，并且明显超过了最近的状态 искусственный风格方法。
</details></li>
</ul>
<hr>
<h2 id="FSDiffReg-Feature-wise-and-Score-wise-Diffusion-guided-Unsupervised-Deformable-Image-Registration-for-Cardiac-Images"><a href="#FSDiffReg-Feature-wise-and-Score-wise-Diffusion-guided-Unsupervised-Deformable-Image-Registration-for-Cardiac-Images" class="headerlink" title="FSDiffReg: Feature-wise and Score-wise Diffusion-guided Unsupervised Deformable Image Registration for Cardiac Images"></a>FSDiffReg: Feature-wise and Score-wise Diffusion-guided Unsupervised Deformable Image Registration for Cardiac Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12035">http://arxiv.org/abs/2307.12035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/fsdiffreg">https://github.com/xmed-lab/fsdiffreg</a></li>
<li>paper_authors: Yi Qin, Xiaomeng Li</li>
<li>for: 这篇论文主要针对医疗影像注册 зада задачу，尤其是实现高品质的扭转场，同时保持扭转学径的可靠性。</li>
<li>methods: 本论文提出了两个模组，分别是对于Semantic Diffusion Model的多对多映射和Score-wise Diffusion-Guided Module，以利用扩散模型的Semantic Feature空间来帮助注册任务。</li>
<li>results: 实验结果显示，本论文的模型能够对3D医疗心脏影像注册任务提供精确的扭转场，并具有保持扭转学径的可靠性。<details>
<summary>Abstract</summary>
Unsupervised deformable image registration is one of the challenging tasks in medical imaging. Obtaining a high-quality deformation field while preserving deformation topology remains demanding amid a series of deep-learning-based solutions. Meanwhile, the diffusion model's latent feature space shows potential in modeling the deformation semantics. To fully exploit the diffusion model's ability to guide the registration task, we present two modules: Feature-wise Diffusion-Guided Module (FDG) and Score-wise Diffusion-Guided Module (SDG). Specifically, FDG uses the diffusion model's multi-scale semantic features to guide the generation of the deformation field. SDG uses the diffusion score to guide the optimization process for preserving deformation topology with barely any additional computation. Experiment results on the 3D medical cardiac image registration task validate our model's ability to provide refined deformation fields with preserved topology effectively. Code is available at: https://github.com/xmed-lab/FSDiffReg.git.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>医疗影像注册不监督式扭变是一项具有挑战性的任务。在一系列深度学习基于解决方案中，获得高质量扭变场并保持扭变 topology remains demanding. 同时，扩散模型的隐藏特征空间表现出了模型扭变 semantics 的潜力。为了充分利用扩散模型对注册任务的导航，我们提出了两个模块：特征 wise Diffusion-Guided Module (FDG) 和 Score-wise Diffusion-Guided Module (SDG)。具体来说，FDG 使用扩散模型的多尺度semantic特征来导航生成扭变场。SDG 使用扩散分数来导航优化过程，以保持扭变 topology with barely any additional computation。实验结果表明，我们的模型能够提供高精度的扭变场，并具有保持扭变 topology 的能力。代码可以在：https://github.com/xmed-lab/FSDiffReg.git 中找到。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-and-Semi-Supervised-Polyp-Segmentation-using-Synthetic-Data"><a href="#Self-Supervised-and-Semi-Supervised-Polyp-Segmentation-using-Synthetic-Data" class="headerlink" title="Self-Supervised and Semi-Supervised Polyp Segmentation using Synthetic Data"></a>Self-Supervised and Semi-Supervised Polyp Segmentation using Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12033">http://arxiv.org/abs/2307.12033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enric Moreu, Eric Arazo, Kevin McGuinness, Noel E. O’Connor</li>
<li>for: 早期检测Rectal Polyps是肝肠癌预防中非常重要的一步，手术镜检查是 manually carried out to examine the entirety of the patient’s colon的主要方法。</li>
<li>methods: 我们使用计算机视觉技术来助力 профессионаls在诊断阶段，并利用Synthetic Data和自动生成的图像来增加数据量，以便更好地利用无标注数据。</li>
<li>results: 我们的模型Pl-CUT-Seg在标准的Polyp Segmentation benchmark上达到了自动标注和 semi-supervised setup中的state-of-the-art Results，并且我们还提出了PL-CUT-Seg+，一种通过targeted regularization来Address the domain gap between real and synthetic images的改进版本。<details>
<summary>Abstract</summary>
Early detection of colorectal polyps is of utmost importance for their treatment and for colorectal cancer prevention. Computer vision techniques have the potential to aid professionals in the diagnosis stage, where colonoscopies are manually carried out to examine the entirety of the patient's colon. The main challenge in medical imaging is the lack of data, and a further challenge specific to polyp segmentation approaches is the difficulty of manually labeling the available data: the annotation process for segmentation tasks is very time-consuming. While most recent approaches address the data availability challenge with sophisticated techniques to better exploit the available labeled data, few of them explore the self-supervised or semi-supervised paradigm, where the amount of labeling required is greatly reduced. To address both challenges, we leverage synthetic data and propose an end-to-end model for polyp segmentation that integrates real and synthetic data to artificially increase the size of the datasets and aid the training when unlabeled samples are available. Concretely, our model, Pl-CUT-Seg, transforms synthetic images with an image-to-image translation module and combines the resulting images with real images to train a segmentation model, where we use model predictions as pseudo-labels to better leverage unlabeled samples. Additionally, we propose PL-CUT-Seg+, an improved version of the model that incorporates targeted regularization to address the domain gap between real and synthetic images. The models are evaluated on standard benchmarks for polyp segmentation and reach state-of-the-art results in the self- and semi-supervised setups.
</details>
<details>
<summary>摘要</summary>
早期检测肠RectalPolyp非常重要，以采取治疗和预防肠RectalCancer。计算机视觉技术有可能帮助专业人员在诊断阶段进行手动检查患者的整个肠肠Rectal。主要挑战在医疗影像领域是数据不足，而特定于肠Polyp分割方法的另一个挑战是手动标注可用数据的困难。大多数最新的方法解决数据不足的挑战，使用了复杂的技术来更好地利用可用的标注数据。然而，只有几个方法探讨了不supervised或semi-supervised模式，其中可以大幅减少标注数量。为了解决这两个挑战，我们利用生成的数据和提议一个综合模型，即Pl-CUT-Seg，将生成的图像与实际图像结合以训练一个分割模型。我们使用模型预测结果作为pseudo-标注，以更好地利用无标注样本。此外，我们还提出了PL-CUT-Seg+，一个改进的模型，其中包括targeted regularization，以解决实际和生成图像之间的领域差异。模型在标准的肠Polyp分割测试benchmark上进行评估，并在自supervised和semi-supervised setup中达到了状态的末点结果。
</details></li>
</ul>
<hr>
<h2 id="Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space"><a href="#Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space" class="headerlink" title="Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space"></a>Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12032">http://arxiv.org/abs/2307.12032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junzis/contrail-net">https://github.com/junzis/contrail-net</a></li>
<li>paper_authors: Junzi Sun, Esther Roosenbrand</li>
<li>for: 这篇论文旨在提出一种基于增强传输学习的新模型，用于检测飞机烟囱从卫星图像中。</li>
<li>methods: 该模型使用了增强传输学习技术，以及一种新的损失函数SR损失，以提高烟囱线检测精度。</li>
<li>results: 研究发现，该模型能够准确地检测飞机烟囱，并且只需要 minimal data。这些成果开创了机器学习基于烟囱检测的新途径，并可以解决航空研究中烟囱检测模型的缺乏大量手动标注数据问题。<details>
<summary>Abstract</summary>
Air transport poses significant environmental challenges, particularly the contribution of flight contrails to climate change due to their potential global warming impact. Detecting contrails from satellite images has been a long-standing challenge. Traditional computer vision techniques have limitations under varying image conditions, and machine learning approaches using typical convolutional neural networks are hindered by the scarcity of hand-labeled contrail datasets and contrail-tailored learning processes. In this paper, we introduce an innovative model based on augmented transfer learning that accurately detects contrails with minimal data. We also propose a novel loss function, SR Loss, which improves contrail line detection by transforming the image space into Hough space. Our research opens new avenues for machine learning-based contrail detection in aviation research, offering solutions to the lack of large hand-labeled datasets, and significantly enhancing contrail detection models.
</details>
<details>
<summary>摘要</summary>
飞行交通对环境造成重大挑战，特别是飞机烟尘对气候变化的贡献，由于其可能对全球暖化产生影响。传统的计算机视觉技术在不同的图像条件下有限制，机器学习方法使用典型的卷积神经网络受到手动标注烟尘数据的罕见和烟尘特化学习过程的限制。在这篇论文中，我们介绍了一种创新的模型，基于增强转移学习，可以准确地检测烟尘。我们还提出了一种新的损失函数，SR损失，它在图像空间转换到抽象空间，从而改善烟尘线检测。我们的研究开创了机器学习基于烟尘检测的新途径，解决了航空研究中缺乏大量手动标注数据的问题，并有效地提高烟尘检测模型。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effectiveness-of-Spectral-Discriminators-for-Perceptual-Quality-Improvement"><a href="#On-the-Effectiveness-of-Spectral-Discriminators-for-Perceptual-Quality-Improvement" class="headerlink" title="On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement"></a>On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12027">http://arxiv.org/abs/2307.12027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luciennnnnnn/dualformer">https://github.com/luciennnnnnn/dualformer</a></li>
<li>paper_authors: Xin Luo, Yunan Zhu, Shunxin Xu, Dong Liu</li>
<li>for: 本研究旨在解释spectral discriminator在生成模型中的效果，特别是在图像超解像(GAN-based SR)中。</li>
<li>methods: 作者使用了spectral discriminator和ordinary discriminator进行比较，并提出了使用这两种权值 simultaneously。另外，作者还提出了一种基于Transformer的方法来协调spectral discriminator。</li>
<li>results: 作者发现spectral discriminator在高频范围内的性能更高，而ordinary discriminator在低频范围内的性能更高。这些结果表明，使用spectral discriminator和ordinary discriminator simultaneously可以提高SR图像的质量。此外，作者还发现，使用这种方法可以更好地评估图像的品质。<details>
<summary>Abstract</summary>
Several recent studies advocate the use of spectral discriminators, which evaluate the Fourier spectra of images for generative modeling. However, the effectiveness of the spectral discriminators is not well interpreted yet. We tackle this issue by examining the spectral discriminators in the context of perceptual image super-resolution (i.e., GAN-based SR), as SR image quality is susceptible to spectral changes. Our analyses reveal that the spectral discriminator indeed performs better than the ordinary (a.k.a. spatial) discriminator in identifying the differences in the high-frequency range; however, the spatial discriminator holds an advantage in the low-frequency range. Thus, we suggest that the spectral and spatial discriminators shall be used simultaneously. Moreover, we improve the spectral discriminators by first calculating the patch-wise Fourier spectrum and then aggregating the spectra by Transformer. We verify the effectiveness of the proposed method twofold. On the one hand, thanks to the additional spectral discriminator, our obtained SR images have their spectra better aligned to those of the real images, which leads to a better PD tradeoff. On the other hand, our ensembled discriminator predicts the perceptual quality more accurately, as evidenced in the no-reference image quality assessment task.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:一些最近的研究提出了使用spectral discriminator，它们评估图像的快 Fourier spectrum进行生成模型。然而，spectral discriminator的效果还不够理解。我们在perceptual image super-resolution（i.e., GAN-based SR）中研究spectral discriminator，因为SR图像质量对快 Fourier spectrum的变化敏感。我们的分析发现，spectral discriminator在高频范围内比普通的（即空间）discriminator更好地识别图像的差异，但是空间discriminator在低频范围内有优势。因此，我们建议同时使用spectral和空间discriminator。此外，我们改进了spectral discriminator，首先计算每个 patch的快 Fourier spectrum，然后使用Transformer聚合spectrum。我们验证了我们的提议方法的有效性通过两种方式：一是我们的SR图像的spectrum更加靠近真实图像的spectrum，导致PD质量更好的权衡；二是我们的ensembled discriminator在无参图像质量评估任务中预测了更加准确的Perceptual质量。
</details></li>
</ul>
<hr>
<h2 id="Simple-parameter-free-self-attention-approximation"><a href="#Simple-parameter-free-self-attention-approximation" class="headerlink" title="Simple parameter-free self-attention approximation"></a>Simple parameter-free self-attention approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12018">http://arxiv.org/abs/2307.12018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploita123/charmedforfree">https://github.com/exploita123/charmedforfree</a></li>
<li>paper_authors: Yuwen Zhai, Jing Hao, Liang Gao, Xinyu Li, Yiping Gao, Shumin Han</li>
<li>for: 用于提高ViT的效率，适用于边缘设备。</li>
<li>methods: 使用自注意力和卷积的混合模型，以及一种无需训练参数的自注意力 aproximation方法（SPSA），用于捕捉全局空间特征。</li>
<li>results: 通过对图像分类和对象检测任务进行广泛的实验，证明了SPSA与卷积的组合的效果。<details>
<summary>Abstract</summary>
The hybrid model of self-attention and convolution is one of the methods to lighten ViT. The quadratic computational complexity of self-attention with respect to token length limits the efficiency of ViT on edge devices. We propose a self-attention approximation without training parameters, called SPSA, which captures global spatial features with linear complexity. To verify the effectiveness of SPSA combined with convolution, we conduct extensive experiments on image classification and object detection tasks.
</details>
<details>
<summary>摘要</summary>
“半自动化模型，即自注意和卷积的混合模型，是用于轻量化ViT的一种方法。自注意的二次计算复杂度与字符串长度成正比，限制了ViT在边缘设备上的效率。我们提出了一种不需要训练参数的自注意简化方法，称为SPSA，它可以 capture global spatial features with linear complexity。为验证SPSA与卷积结合的效果，我们进行了广泛的图像分类和对象检测任务的实验。”Note that Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details"><a href="#NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details" class="headerlink" title="NLCUnet: Single-Image Super-Resolution Network with Hairline Details"></a>NLCUnet: Single-Image Super-Resolution Network with Hairline Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12014">http://arxiv.org/abs/2307.12014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancong Feng, Yuan-Gen Wang, Fengchuang Xing</li>
<li>for: 提高单张超解像图像质量</li>
<li>methods: 提出了一种基于非本地注意力机制的单张超解像网络（NLCUnet），包括三种核心设计。具体来说，首先引入了一种基于全图区域学习的非本地注意力机制，以恢复本地碎片。然后，我们发现现有工作中的卷积权重学习是无需的，因此我们创建了一种新的网络架构，通过对每个通道进行深度卷积并添加通道注意力，从而实现性能提升。最后，我们提议在中心2K图像中随机选择64×64区域，以便尽可能包含 semantic信息。</li>
<li>results: 经过多次实验 validate 的DF2K数据集上，我们的NLCUnet表现比state-of-the-art更高，按PSNR和SSIM指标评估。同时，它也可以呈现出更加有利的毛细处理细节。<details>
<summary>Abstract</summary>
Pursuing the precise details of super-resolution images is challenging for single-image super-resolution tasks. This paper presents a single-image super-resolution network with hairline details (termed NLCUnet), including three core designs. Specifically, a non-local attention mechanism is first introduced to restore local pieces by learning from the whole image region. Then, we find that the blur kernel trained by the existing work is unnecessary. Based on this finding, we create a new network architecture by integrating depth-wise convolution with channel attention without the blur kernel estimation, resulting in a performance improvement instead. Finally, to make the cropped region contain as much semantic information as possible, we propose a random 64$\times$64 crop inside the central 512$\times$512 crop instead of a direct random crop inside the whole image of 2K size. Numerous experiments conducted on the benchmark DF2K dataset demonstrate that our NLCUnet performs better than the state-of-the-art in terms of the PSNR and SSIM metrics and yields visually favorable hairline details.
</details>
<details>
<summary>摘要</summary>
追求超高清照片细节精度是单图超解像任务中的挑战。这篇论文提出了一种单图超解像网络（NLCUnet），包括三个核心设计。具体来说，我们首先引入非本地注意力机制，以全图区域学习恢复本地块。然后，我们发现现有工作中的模糊核心训练是不必要的，因此我们创建了一种不包含模糊核心的网络架构，通过混合深度卷积和通道注意力，从而实现性能提升。最后，我们提议在中心256x256区域内随机选择64x64区域，而不是直接随机选择2K图像中的整个区域，以便尽可能地包含中心区域的semantic信息。经过多次实验，我们发现NLCUnet在DF2K数据集上的PSNR和SSIM指标上表现比前者更好，并且视觉效果更佳。
</details></li>
</ul>
<hr>
<h2 id="SCOL-Supervised-Contrastive-Ordinal-Loss-for-Abdominal-Aortic-Calcification-Scoring-on-Vertebral-Fracture-Assessment-Scans"><a href="#SCOL-Supervised-Contrastive-Ordinal-Loss-for-Abdominal-Aortic-Calcification-Scoring-on-Vertebral-Fracture-Assessment-Scans" class="headerlink" title="SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans"></a>SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12006">http://arxiv.org/abs/2307.12006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/afsahs/supervised-contrastive-ordinal-loss">https://github.com/afsahs/supervised-contrastive-ordinal-loss</a></li>
<li>paper_authors: Afsah Saleem, Zaid Ilyas, David Suter, Ghulam Mubashar Hassan, Siobhan Reid, John T. Schousboe, Richard Prince, William D. Leslie, Joshua R. Lewis, Syed Zulqarnain Gilani<br>for: 这个研究的目的是开发一种自动评估胸部动脉calcification的方法，以检测 asymptomatic atherosclerotic cardiovascular diseases (ASCVDs) 的风险。methods: 这个研究使用了一种新的Supervised Contrastive Ordinal Loss (SCOL) 函数，并开发了一种 Dual-encoder Contrastive Ordinal Learning (DCOL) 框架，以利用 AAC  regression 标签中的顺序信息。results: 研究结果表明，该方法可以提高 AAC 的 интер-класс分化度和内部一致性，并且可以准确地预测高风险 AAC 类别。<details>
<summary>Abstract</summary>
Abdominal Aortic Calcification (AAC) is a known marker of asymptomatic Atherosclerotic Cardiovascular Diseases (ASCVDs). AAC can be observed on Vertebral Fracture Assessment (VFA) scans acquired using Dual-Energy X-ray Absorptiometry (DXA) machines. Thus, the automatic quantification of AAC on VFA DXA scans may be used to screen for CVD risks, allowing early interventions. In this research, we formulate the quantification of AAC as an ordinal regression problem. We propose a novel Supervised Contrastive Ordinal Loss (SCOL) by incorporating a label-dependent distance metric with existing supervised contrastive loss to leverage the ordinal information inherent in discrete AAC regression labels. We develop a Dual-encoder Contrastive Ordinal Learning (DCOL) framework that learns the contrastive ordinal representation at global and local levels to improve the feature separability and class diversity in latent space among the AAC-24 genera. We evaluate the performance of the proposed framework using two clinical VFA DXA scan datasets and compare our work with state-of-the-art methods. Furthermore, for predicted AAC scores, we provide a clinical analysis to predict the future risk of a Major Acute Cardiovascular Event (MACE). Our results demonstrate that this learning enhances inter-class separability and strengthens intra-class consistency, which results in predicting the high-risk AAC classes with high sensitivity and high accuracy.
</details>
<details>
<summary>摘要</summary>
《腹部动脉钙化（AAC）是无症状栓塞动脉疾病（ASCVD）的知名标志。AAC可以在骨折评估（VFA）扫描机上观察到，因此自动量化AAC on VFA DXA扫描机可能用来检测心血管风险，允许早期干预。在这个研究中，我们将AAC量化作为一个Ordinal regression问题。我们提出了一种名为Supervised Contrastive Ordinal Loss（SCOL）的新的损失函数，它通过融合现有的Supervised contrastive loss和标签висимый距离度量来利用AAC regression标签中的排序信息。我们开发了一个名为Dual-encoder Contrastive Ordinal Learning（DCOL）的框架，它可以在全球和本地两个水平上学习对照的排序ORDINAL表示，以提高在隐藏空间中的特征分类和类别多样性。我们使用两个来自临床的VFA DXA扫描数据集进行评估，并与现有的方法进行比较。此外，我们还对预测的AAC分数进行临床分析，以预测未来的主要急性心血管事件（MACE）的风险。我们的结果显示，这种学习可以提高间隔分类的标准差和内部一致性，从而预测高风险AAC类别的敏感性和准确性。
</details></li>
</ul>
<hr>
<h2 id="COLosSAL-A-Benchmark-for-Cold-start-Active-Learning-for-3D-Medical-Image-Segmentation"><a href="#COLosSAL-A-Benchmark-for-Cold-start-Active-Learning-for-3D-Medical-Image-Segmentation" class="headerlink" title="COLosSAL: A Benchmark for Cold-start Active Learning for 3D Medical Image Segmentation"></a>COLosSAL: A Benchmark for Cold-start Active Learning for 3D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12004">http://arxiv.org/abs/2307.12004</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/medicl-vu/colossal">https://github.com/medicl-vu/colossal</a></li>
<li>paper_authors: Han Liu, Hao Li, Xing Yao, Yubo Fan, Dewei Hu, Benoit Dawant, Vishwesh Nath, Zhoubing Xu, Ipek Oguz</li>
<li>for: 本研究旨在解决医疗图像分割任务中数据标注瓶颈问题，提出了一个叫做COLosSAL的数据集和评估框架，用于评估不同的启动式活动学习策略。</li>
<li>methods: 本研究使用了六种不同的启动式活动学习策略，并对五个3D医疗图像分割任务进行了评估。</li>
<li>results: 研究发现，启动式活动学习仍然是3D分割任务中未解决的问题，但是一些重要的趋势有被观察到。<details>
<summary>Abstract</summary>
Medical image segmentation is a critical task in medical image analysis. In recent years, deep learning based approaches have shown exceptional performance when trained on a fully-annotated dataset. However, data annotation is often a significant bottleneck, especially for 3D medical images. Active learning (AL) is a promising solution for efficient annotation but requires an initial set of labeled samples to start active selection. When the entire data pool is unlabeled, how do we select the samples to annotate as our initial set? This is also known as the cold-start AL, which permits only one chance to request annotations from experts without access to previously annotated data. Cold-start AL is highly relevant in many practical scenarios but has been under-explored, especially for 3D medical segmentation tasks requiring substantial annotation effort. In this paper, we present a benchmark named COLosSAL by evaluating six cold-start AL strategies on five 3D medical image segmentation tasks from the public Medical Segmentation Decathlon collection. We perform a thorough performance analysis and explore important open questions for cold-start AL, such as the impact of budget on different strategies. Our results show that cold-start AL is still an unsolved problem for 3D segmentation tasks but some important trends have been observed. The code repository, data partitions, and baseline results for the complete benchmark are publicly available at https://github.com/MedICL-VU/COLosSAL.
</details>
<details>
<summary>摘要</summary>
医学像素化是医学图像分析中的关键任务。在过去几年，基于深度学习的方法在完全标注的数据集上训练后表现出色。然而，数据标注却是一个重要的瓶颈，特别是 для 3D 医学图像。活动学习（AL）是一种可能的解决方案，但它需要一个初始化标注的样本集来开始活动选择。当整个数据池都是未标注的时候，如何选择要标注的样本呢？这也被称为冷启动 AL，它允许在专家无法访问前一次已经标注的数据时，仅请求一次标注。冷启动 AL 在许多实际场景中是非常有价值的，特别是 для 3D 医学分割任务，需要很大的标注努力。在这篇论文中，我们提出了一个名为 COLosSAL 的标准套件，通过评估六种冷启动 AL 策略在五个 3D 医学图像分割任务上进行了全面性的性能分析。我们进行了详细的性能分析，并探讨了冷启动 AL 中重要的开放问题，如预算对不同策略的影响。我们的结果表明，冷启动 AL 仍然是未解决的问题，但我们在不同任务上观察到了一些重要的趋势。我们在 GitHub 上公开了代码库、数据分区和基线结果，欢迎您在 <https://github.com/MedICL-VU/COLosSAL> 上查看。
</details></li>
</ul>
<hr>
<h2 id="A-Stronger-Stitching-Algorithm-for-Fisheye-Images-based-on-Deblurring-and-Registration"><a href="#A-Stronger-Stitching-Algorithm-for-Fisheye-Images-based-on-Deblurring-and-Registration" class="headerlink" title="A Stronger Stitching Algorithm for Fisheye Images based on Deblurring and Registration"></a>A Stronger Stitching Algorithm for Fisheye Images based on Deblurring and Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11997">http://arxiv.org/abs/2307.11997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Hao, Jingming Xie, Jinyuan Zhang, Moyun Liu</li>
<li>for: 解决 fisheye 图像中的 геометрической扭曲问题，提高 fisheye 图像的融合质量。</li>
<li>methods:  combines 传统图像处理技术和深度学习，提出了 Attention-based Nonlinear Activation Free Network (ANAFNet) 和 ORB-FREAK-GMS (OFG) 两种算法。</li>
<li>results: 实验结果表明，通过我们的方法可以获得高质量的排 compose 图像。<details>
<summary>Abstract</summary>
Fisheye lens, which is suitable for panoramic imaging, has the prominent advantage of a large field of view and low cost. However, the fisheye image has a severe geometric distortion which may interfere with the stage of image registration and stitching. Aiming to resolve this drawback, we devise a stronger stitching algorithm for fisheye images by combining the traditional image processing method with deep learning. In the stage of fisheye image correction, we propose the Attention-based Nonlinear Activation Free Network (ANAFNet) to deblur fisheye images corrected by Zhang calibration method. Specifically, ANAFNet adopts the classical single-stage U-shaped architecture based on convolutional neural networks with soft-attention technique and it can restore a sharp image from a blurred image effectively. In the part of image registration, we propose the ORB-FREAK-GMS (OFG), a comprehensive image matching algorithm, to improve the accuracy of image registration. Experimental results demonstrate that panoramic images of superior quality stitching by fisheye images can be obtained through our method.
</details>
<details>
<summary>摘要</summary>
鱼眼镜，适用于全景拍摄，具有大视野和低成本的优点。然而，鱼眼图像受到严重的几何扭曲的影响，可能会干扰图像registraton和组合stage。为了解决这个缺点，我们提出了一种基于传统图像处理技术和深度学习的更强大的缝合算法 для鱼眼图像。在鱼眼图像修正阶段，我们提出了Attention-based Nonlinear Activation Free Network（ANAFNet），用于deburring鱼眼图像，并可以有效地还原锐化图像。在图像registraton阶段，我们提出了ORB-FREAK-GMS（OFG），一种全面的图像匹配算法，以提高图像registraton的准确性。实验结果表明，通过我们的方法可以得到高质量的全景图像组合。
</details></li>
</ul>
<hr>
<h2 id="Morphology-inspired-Unsupervised-Gland-Segmentation-via-Selective-Semantic-Grouping"><a href="#Morphology-inspired-Unsupervised-Gland-Segmentation-via-Selective-Semantic-Grouping" class="headerlink" title="Morphology-inspired Unsupervised Gland Segmentation via Selective Semantic Grouping"></a>Morphology-inspired Unsupervised Gland Segmentation via Selective Semantic Grouping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11989">http://arxiv.org/abs/2307.11989</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/mssg">https://github.com/xmed-lab/mssg</a></li>
<li>paper_authors: Qixiang Zhang, Yi Li, Cheng Xue, Xiaomeng Li</li>
<li>for: 这个论文的目的是开发一种不需要人工标注的深度学习方法来自动分类腺体，以推动自动癌症诊断和预后评估。</li>
<li>methods: 我们提出了一种新的 Morphology-inspired 方法，将实验资讯引入为额外知识，以指导分类过程。我们首先利用这个实验资讯选择出腺体子区域的提案，然后使用具有形式知识的 Semantic Grouping 模组集成全局资讯。</li>
<li>results: 我们在 GlaS dataset 和 CRAG dataset 上进行实验，结果显示我们的方法在 mIOU 上超过第二名的对手10.56%。<details>
<summary>Abstract</summary>
Designing deep learning algorithms for gland segmentation is crucial for automatic cancer diagnosis and prognosis, yet the expensive annotation cost hinders the development and application of this technology. In this paper, we make a first attempt to explore a deep learning method for unsupervised gland segmentation, where no manual annotations are required. Existing unsupervised semantic segmentation methods encounter a huge challenge on gland images: They either over-segment a gland into many fractions or under-segment the gland regions by confusing many of them with the background. To overcome this challenge, our key insight is to introduce an empirical cue about gland morphology as extra knowledge to guide the segmentation process. To this end, we propose a novel Morphology-inspired method via Selective Semantic Grouping. We first leverage the empirical cue to selectively mine out proposals for gland sub-regions with variant appearances. Then, a Morphology-aware Semantic Grouping module is employed to summarize the overall information about the gland by explicitly grouping the semantics of its sub-region proposals. In this way, the final segmentation network could learn comprehensive knowledge about glands and produce well-delineated, complete predictions. We conduct experiments on GlaS dataset and CRAG dataset. Our method exceeds the second-best counterpart over 10.56% at mIOU.
</details>
<details>
<summary>摘要</summary>
深度学习算法的设计对腺体分 segmentation是抑肿癌诊断和诊断的关键，但是手动标注的高投入成本限制了这种技术的发展和应用。在这篇论文中，我们首次尝试探索一种不需要手动标注的深度学习方法 для腺体分 segmentation。现有的无监督语义分割方法在腺体图像上遇到一大问题：它们都会过分 segment 腺体，或者将腺体区域与背景相混淆。为了解决这个问题，我们的关键思想是通过引入腺体形态的辅助知识来导向分 segmentation 过程。我们首先利用这个辅助知识来选择出腺体各个子区域的提议，然后使用具有形态意识的语义组合模块来总结腺体的全部信息。这样，最终的分 segmentation 网络可以学习腺体的全面信息，并生成完整、清晰的预测。我们在 GlaS 数据集和 CRAG 数据集上进行实验，我们的方法在 mIOU 上超过第二最佳对手的 10.56%。
</details></li>
</ul>
<hr>
<h2 id="Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering"><a href="#Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering" class="headerlink" title="Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering"></a>Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11986">http://arxiv.org/abs/2307.11986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/holipori/mimic-diff-vqa">https://github.com/holipori/mimic-diff-vqa</a></li>
<li>paper_authors: Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M. Summers, Yingying Zhu</li>
<li>for: 提高医疗机器人识别能力，推动医学视觉语言模型的自动化。</li>
<li>methods: 基于专家知识，构建图像差异视Question Answering（VQA）任务，使用新收集的MIMIC-Diff-VQA数据集。</li>
<li>results: 提出了一种新的图像差异VQA任务，并提供了一种基于专家知识的图像差异GRAPH表示学习模型，以提高医学视觉语言模型的性能。<details>
<summary>Abstract</summary>
To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a multi-relationship graph, representing the image differences between two images for the image difference VQA task. The dataset and code can be found at https://github.com/Holipori/MIMIC-Diff-VQA. We believe this work would further push forward the medical vision language model.
</details>
<details>
<summary>摘要</summary>
为推进医疗视语模型的自动化，我们提出了一个新的胸部X射影异常问答任务（VQA）任务。给定一对主要和参照图像，这个任务的目标是回答几个疾病和图像之间的差异相关的问题。这与辐射医生的诊断实践相一致，即比较当前图像与参照图像进行诊断报告。我们收集了一个新的数据集，即MIMIC-Diff-VQA，包含700703个问答对从164324对主要和参照图像的对。与现有的医疗VQA数据集相比，我们的问题更加适合诊断过程中的评估-诊断- interven-评估策略。同时，我们也提出了一种基于专家知识的图像差异学习模型来解决这个任务。我们的基eline模型利用专家知识，如解剖结构优先知识、语义知识和空间知识，构建多关系图，表示图像差异问题中的图像差异。数据集和代码可以在 GitHub 上找到：https://github.com/Holipori/MIMIC-Diff-VQA。我们认为这项工作将进一步推动医疗视语模型的发展。
</details></li>
</ul>
<hr>
<h2 id="Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model"><a href="#Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model" class="headerlink" title="Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model"></a>Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11980">http://arxiv.org/abs/2307.11980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayang Wang, Srivathsa Pasumarthi, Greg Zaharchuk, Ryan Chamberlain</li>
<li>for: 这个论文是为了提出一种基于深度学习的扬化剂抑制和消除技术，以减少或完全消除荷尔蒙酸盐（GBCAs）的负面影响。</li>
<li>methods: 这种技术使用了一种名为Gformer的变换器，它是一种迭代模型，通过扫描和注意力机制来Synthesize图像，并且可以模拟不同的剑药剂和疾病水平。</li>
<li>results: 根据量化评估结果，提出的Gformer模型在比较其他现状技术时表现更好，而且在下游任务如剂量减少和肿瘤分割等方面也进行了评估，以证明这种技术的临床实用性。<details>
<summary>Abstract</summary>
Deep learning (DL) based contrast dose reduction and elimination in MRI imaging is gaining traction, given the detrimental effects of Gadolinium-based Contrast Agents (GBCAs). These DL algorithms are however limited by the availability of high quality low dose datasets. Additionally, different types of GBCAs and pathologies require different dose levels for the DL algorithms to work reliably. In this work, we formulate a novel transformer (Gformer) based iterative modelling approach for the synthesis of images with arbitrary contrast enhancement that corresponds to different dose levels. The proposed Gformer incorporates a sub-sampling based attention mechanism and a rotational shift module that captures the various contrast related features. Quantitative evaluation indicates that the proposed model performs better than other state-of-the-art methods. We further perform quantitative evaluation on downstream tasks such as dose reduction and tumor segmentation to demonstrate the clinical utility.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）基于对比剂减少和消除在MRI成像中得到了进展，由于质子剂基因链（GBCAs）的负面影响。这些DL算法 however 受到低质量低剂量数据的可用性的限制。另外，不同类型的GBCAs和疾病需要不同的剂量水平以便DL算法可靠地工作。在这种工作中，我们提出了一种基于转换器（Gformer）的迭代模型方法，用于生成具有任意对比强化的图像。我们的Gformer模型包括一种归一化基于抽样的注意力机制和一种旋转变换模块，以捕捉不同的对比相关特征。量化评估表明，我们的模型在其他状态对比较好的方法。我们进一步进行了下游任务的量化评估，如剂量减少和肿瘤分 segmentation，以证明我们的临床实用性。
</details></li>
</ul>
<hr>
<h2 id="Two-stream-Multi-level-Dynamic-Point-Transformer-for-Two-person-Interaction-Recognition"><a href="#Two-stream-Multi-level-Dynamic-Point-Transformer-for-Two-person-Interaction-Recognition" class="headerlink" title="Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition"></a>Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11973">http://arxiv.org/abs/2307.11973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Liu, Gangfeng Cui, Jiahui Luo, Lina Yao, Xiaojun Chang</li>
<li>for: 本研究的目的是提出一种基于点云网络的两人交互识别方法，以满足人工智能应用中的个人隐私要求。</li>
<li>methods: 我们提出了一种名为Interval Frame Sampling（IFS）的框选Method，以及一种两树多级特征聚合模块，以提取全局和部分特征。</li>
<li>results: 我们的网络在NTU RGB+D 60和NTU RGB+D 120的交互子集上进行了广泛的实验，并显示了与状态前方法进行比较的优异性。<details>
<summary>Abstract</summary>
As a fundamental aspect of human life, two-person interactions contain meaningful information about people's activities, relationships, and social settings. Human action recognition serves as the foundation for many smart applications, with a strong focus on personal privacy. However, recognizing two-person interactions poses more challenges due to increased body occlusion and overlap compared to single-person actions. In this paper, we propose a point cloud-based network named Two-stream Multi-level Dynamic Point Transformer for two-person interaction recognition. Our model addresses the challenge of recognizing two-person interactions by incorporating local-region spatial information, appearance information, and motion information. To achieve this, we introduce a designed frame selection method named Interval Frame Sampling (IFS), which efficiently samples frames from videos, capturing more discriminative information in a relatively short processing time. Subsequently, a frame features learning module and a two-stream multi-level feature aggregation module extract global and partial features from the sampled frames, effectively representing the local-region spatial information, appearance information, and motion information related to the interactions. Finally, we apply a transformer to perform self-attention on the learned features for the final classification. Extensive experiments are conducted on two large-scale datasets, the interaction subsets of NTU RGB+D 60 and NTU RGB+D 120. The results show that our network outperforms state-of-the-art approaches across all standard evaluation settings.
</details>
<details>
<summary>摘要</summary>
人类生活中的基本方面之一是两个人之间的交互，这些交互含有人们的活动、关系和社会环境中的有用信息。人工智能应用中强调个人隐私，人体动作识别作为基础技术，但Recognizing two-person interactions poses more challenges due to increased body occlusion and overlap compared to single-person actions。在这篇论文中，我们提出了一种基于点云的网络 named Two-stream Multi-level Dynamic Point Transformer for two-person interaction recognition。我们的模型解决了认izing two-person interactions的挑战，通过 integrate local-region spatial information, appearance information, and motion information。为了实现这一点，我们提出了一种设计的Frame Selection Method named Interval Frame Sampling (IFS)，该方法高效地从视频中选择Frame， capture more discriminative information in a relatively short processing time。然后，我们引入了一个Frame Features Learning Module和Two-stream Multi-level Feature Aggregation Module，这两个模块通过EXTract global and partial features from the sampled frames, effectively representing the local-region spatial information, appearance information, and motion information related to the interactions。最后，我们使用 transformer 进行自注意力，以实现最终的分类。我们在 NTU RGB+D 60 和 NTU RGB+D 120 两个大规模数据集上进行了广泛的实验，结果显示，我们的网络在所有标准评估环境下都超过了现有方法。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Remote-Sensing-Image-Quality-Inspection-System"><a href="#Intelligent-Remote-Sensing-Image-Quality-Inspection-System" class="headerlink" title="Intelligent Remote Sensing Image Quality Inspection System"></a>Intelligent Remote Sensing Image Quality Inspection System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11965">http://arxiv.org/abs/2307.11965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijiong Yu, Tao Wang, Kang Ran, Chang Li, Hao Wu</li>
<li>for: 这篇论文旨在提出一个新的两步智能系统，用于Remote Sensing图像质量检查，以提高检查效率。</li>
<li>methods: 本论文提出的方法首先使用多 modelo 进行图像分类，然后使用最佳的方法来ocalize多种图像质量问题。</li>
<li>results:  результа业表示，提出的方法在Remote Sensing图像质量检查中表现出色，比一些一步方法更高效。此外，本论文初步探讨了多modal模型在Remote Sensing图像质量检查中的可行性和潜力。<details>
<summary>Abstract</summary>
Quality inspection is a necessary task before putting any remote sensing image into practical application. However, traditional manual inspection methods suffer from low efficiency. Hence, we propose a novel two-step intelligent system for remote sensing image quality inspection that combines multiple models, which first performs image classification and then employs the most appropriate methods to localize various forms of quality problems in the image. Results demonstrate that the proposed method exhibits excellent performance and efficiency in remote sensing image quality inspection, surpassing the performance of those one-step methods. Furthermore, we conduct an initial exploration of the feasibility and potential of applying multimodal models to remote sensing image quality inspection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Quality inspection is a necessary task before putting any remote sensing image into practical application. However, traditional manual inspection methods suffer from low efficiency. Hence, we propose a novel two-step intelligent system for remote sensing image quality inspection that combines multiple models, which first performs image classification and then employs the most appropriate methods to localize various forms of quality problems in the image. Results demonstrate that the proposed method exhibits excellent performance and efficiency in remote sensing image quality inspection, surpassing the performance of those one-step methods. Furthermore, we conduct an initial exploration of the feasibility and potential of applying multimodal models to remote sensing image quality inspection." into Simplified Chinese.翻译：在应用 remote sensing 图像之前，质量检测是一项必需的任务。然而，传统的手动检测方法具有低效率。因此，我们提出了一种新的两步智能系统，用于 remote sensing 图像质量检测，这个系统首先使用多个模型进行图像分类，然后使用最有效的方法来地址图像中的质量问题。结果显示，我们的方法在 remote sensing 图像质量检测中表现出色，高于一键方法的性能。此外，我们还进行了初步的多模态模型在 remote sensing 图像质量检测中的可行性和潜力的探索。>>
</details></li>
</ul>
<hr>
<h2 id="MIMONet-Multi-Input-Multi-Output-On-Device-Deep-Learning"><a href="#MIMONet-Multi-Input-Multi-Output-On-Device-Deep-Learning" class="headerlink" title="MIMONet: Multi-Input Multi-Output On-Device Deep Learning"></a>MIMONet: Multi-Input Multi-Output On-Device Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11962">http://arxiv.org/abs/2307.11962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexin Li, Xiaoxi He, Yufei Li, Shahab Nikkhoo, Wei Yang, Lothar Thiele, Cong Liu</li>
<li>for: This paper aims to improve the performance of intelligent robotic systems by proposing a novel on-device multi-input multi-output deep neural network (MIMO DNN) framework called MIMONet.</li>
<li>methods: MIMONet leverages existing single-input single-output (SISO) model compression techniques and develops a new deep-compression method tailored to MIMO models, which explores unique properties of the MIMO model to achieve boosted accuracy and on-device efficiency.</li>
<li>results: Extensive experiments on three embedded platforms and a case study using the TurtleBot3 robot demonstrate that MIMONet achieves higher accuracy and superior on-device efficiency compared to state-of-the-art SISO and MISO models, as well as a baseline MIMO model.<details>
<summary>Abstract</summary>
Future intelligent robots are expected to process multiple inputs simultaneously (such as image and audio data) and generate multiple outputs accordingly (such as gender and emotion), similar to humans. Recent research has shown that multi-input single-output (MISO) deep neural networks (DNN) outperform traditional single-input single-output (SISO) models, representing a significant step towards this goal. In this paper, we propose MIMONet, a novel on-device multi-input multi-output (MIMO) DNN framework that achieves high accuracy and on-device efficiency in terms of critical performance metrics such as latency, energy, and memory usage. Leveraging existing SISO model compression techniques, MIMONet develops a new deep-compression method that is specifically tailored to MIMO models. This new method explores unique yet non-trivial properties of the MIMO model, resulting in boosted accuracy and on-device efficiency. Extensive experiments on three embedded platforms commonly used in robotic systems, as well as a case study using the TurtleBot3 robot, demonstrate that MIMONet achieves higher accuracy and superior on-device efficiency compared to state-of-the-art SISO and MISO models, as well as a baseline MIMO model we constructed. Our evaluation highlights the real-world applicability of MIMONet and its potential to significantly enhance the performance of intelligent robotic systems.
</details>
<details>
<summary>摘要</summary>
将来的智能机器人将能同时处理多个输入（如图像和音频数据），并生成相应的多个输出（如性别和情感），类似于人类。最新的研究表明，多输入单输出（MISO）深度神经网络（DNN）的性能比单输入单输出（SISO）模型更高，这表示了大的一步。在这篇论文中，我们提出了一种名为MIMONet的在设备上运行的多输入多输出（MIMO）DNN框架，实现了高准确率和设备上的高效性。我们利用了现有的SISO模型压缩技术，开发了一种专门适应MIMO模型的深度压缩方法。这种新方法利用了MIMO模型独特 yet non-trivial 的性质，从而提高了准确率和设备上的效率。我们在三种常用于机器人系统的嵌入式平台上进行了广泛的实验，以及使用了TurtleBot3机器人进行了一个案例研究。我们的评估表明，MIMONet在与状态对照模型和基eline MIMO模型进行比较时，在准确率和设备上的性能均高于其他模型。我们的评估还表明，MIMONet在实际应用中具有广泛的应用前景和可能性。
</details></li>
</ul>
<hr>
<h2 id="DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation"><a href="#DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation"></a>DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11960">http://arxiv.org/abs/2307.11960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/dhc">https://github.com/xmed-lab/dhc</a></li>
<li>paper_authors: Haonan Wang, Xiaomeng Li</li>
<li>for: 这个研究旨在提出一个基于semi-supervised learning的3D医疗影像分类框架，以解决对于医疗影像分类的专门技能和时间负担问题。</li>
<li>methods: 这个框架使用了一个名为Dual-debiased Heterogeneous Co-training（DHC）的新方法，包括两种损失衡量策略，即Distribution-aware Debiased Weighting（DistDW）和Difficulty-aware Debiased Weighting（DiffDW），以透过对于 pseudo labels的动态指导，帮助模型解决数据和学习偏见。</li>
<li>results: 实验结果显示，提出的方法可以将 pseudo labels 用于偏见调整和纾解类别偏见问题，并且与现有的SSL方法相比，提高了模型的性能。此外，我们还提出了更加代表的医疗影像分类 semi-supervised 测试标准，以全面显示这些类别偏见的设计的效果。<details>
<summary>Abstract</summary>
The volume-wise labeling of 3D medical images is expertise-demanded and time-consuming; hence semi-supervised learning (SSL) is highly desirable for training with limited labeled data. Imbalanced class distribution is a severe problem that bottlenecks the real-world application of these methods but was not addressed much. Aiming to solve this issue, we present a novel Dual-debiased Heterogeneous Co-training (DHC) framework for semi-supervised 3D medical image segmentation. Specifically, we propose two loss weighting strategies, namely Distribution-aware Debiased Weighting (DistDW) and Difficulty-aware Debiased Weighting (DiffDW), which leverage the pseudo labels dynamically to guide the model to solve data and learning biases. The framework improves significantly by co-training these two diverse and accurate sub-models. We also introduce more representative benchmarks for class-imbalanced semi-supervised medical image segmentation, which can fully demonstrate the efficacy of the class-imbalance designs. Experiments show that our proposed framework brings significant improvements by using pseudo labels for debiasing and alleviating the class imbalance problem. More importantly, our method outperforms the state-of-the-art SSL methods, demonstrating the potential of our framework for the more challenging SSL setting. Code and models are available at: https://github.com/xmed-lab/DHC.
</details>
<details>
<summary>摘要</summary>
“三维医疗影像的量化标签是专业需求又是时间耗尽的，因此半监督学习（SSL）是非常有优点的。然而，实际应用中存在资料分布不均的问题，这对实际应用带来了很大的阻碍。为解决这个问题，我们提出了一个新的双重偏见随时执行（DHC）框架，用于半监督三维医疗影像分类。具体来说，我们提出了两种损失调整策略，namely Distribution-aware Debiased Weighting（DistDW）和 Difficulty-aware Debiased Weighting（DiffDW），这些策略可以在执行时将 pseudo labels 用于导引模型解决数据和学习偏见。这个框架在半监督下执行这两个多标的子模型，从而提高了性能。我们还提出了更多的代表性的标签数据集，用于测试实际中的半监督医疗影像分类。实验结果显示，我们的提案的框架可以将 pseudo labels 用于偏见调整和缓和资料分布不均问题，并且超越了现有的SSL方法。代码和模型可以在：https://github.com/xmed-lab/DHC 中找到。”
</details></li>
</ul>
<hr>
<h2 id="Topology-Preserving-Automatic-Labeling-of-Coronary-Arteries-via-Anatomy-aware-Connection-Classifier"><a href="#Topology-Preserving-Automatic-Labeling-of-Coronary-Arteries-via-Anatomy-aware-Connection-Classifier" class="headerlink" title="Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-aware Connection Classifier"></a>Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-aware Connection Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11959">http://arxiv.org/abs/2307.11959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zutsusemi/miccai2023-topolab-labels">https://github.com/zutsusemi/miccai2023-topolab-labels</a></li>
<li>paper_authors: Zhixing Zhang, Ziwei Zhao, Dong Wang, Shishuang Zhao, Yuhang Liu, Jia Liu, Liwei Wang</li>
<li>for: 这个论文主要是为了提高心血管疾病诊断过程中自动标注 coronary artery 的精度和效率。</li>
<li>methods: 该方法使用了新的 TopoLab 框架，该框架包括明确表征动脉连接的方法，以及层次结构特征提取和动脉间特征互动的策略。</li>
<li>results: 实验结果表明，使用 TopoLab 可以在 orCaScore 数据集和一个内部数据集上达到状态机器人的表现，提高了自动标注 coronary artery 的精度和效率。<details>
<summary>Abstract</summary>
Automatic labeling of coronary arteries is an essential task in the practical diagnosis process of cardiovascular diseases. For experienced radiologists, the anatomically predetermined connections are important for labeling the artery segments accurately, while this prior knowledge is barely explored in previous studies. In this paper, we present a new framework called TopoLab which incorporates the anatomical connections into the network design explicitly. Specifically, the strategies of intra-segment feature aggregation and inter-segment feature interaction are introduced for hierarchical segment feature extraction. Moreover, we propose the anatomy-aware connection classifier to enable classification for each connected segment pair, which effectively exploits the prior topology among the arteries with different categories. To validate the effectiveness of our method, we contribute high-quality annotations of artery labeling to the public orCaScore dataset. The experimental results on both the orCaScore dataset and an in-house dataset show that our TopoLab has achieved state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
自动标注 coronary arteries 是诊断冠状动脉疾病的重要任务。经验丰富的医生们认为，将预先确定的 анатомиче连接 explicitly  incorporated into the network design 是标注artery 段 accurately。在这篇论文中，我们提出了一种新的框架 called TopoLab，它使用了 hierarchical segment feature extraction 和 anatomy-aware connection classifier 来实现这一目标。此外，我们还提供了高质量的 artery 标注数据，以 validate 我们的方法。实验结果表明，我们的 TopoLab 已经达到了领先的性能。Here's the text with some additional explanations and notes:自动标注 coronary arteries 是诊断冠状动脉疾病的重要任务。（1） coronary arteries 是冠状动脉系统中的重要组成部分，它们的血流很重要，一旦发生疾病，会对身体产生严重的影响。（2） 为了诊断 coronary arteries 的疾病，需要进行精准的标注，但这是一项复杂的任务，需要具备很好的知识和技能。经验丰富的医生们认为，将预先确定的 anatomical connections  explicitly  incorporated into the network design 是标注 coronary arteries 段 accurately。（3） anatomical connections 是指冠状动脉系统中各个组成部分之间的连接关系，这些连接关系可以帮助医生更好地理解冠状动脉系统的结构和功能。在这篇论文中，我们提出了一种新的框架 called TopoLab，它使用了 hierarchical segment feature extraction 和 anatomy-aware connection classifier 来实现标注 coronary arteries 的任务。（4） TopoLab 的核心思想是利用 hierarchical segment feature extraction 来提取 coronary arteries 的特征，并通过 anatomy-aware connection classifier 来确定各个连接关系的类别。此外，我们还提供了高质量的 artery 标注数据，以 validate 我们的方法。（5） 我们使用了一个高质量的 dataset，并对其进行了严格的验证和验收，以确保我们的方法的可靠性和精准性。实验结果表明，我们的 TopoLab 已经达到了领先的性能。（6） 我们对比了 TopoLab 与其他方法的实验结果，发现 TopoLab 的性能明显超过了其他方法。这表明，我们的方法可以帮助医生更好地诊断 coronary arteries 的疾病。
</details></li>
</ul>
<hr>
<h2 id="Pick-the-Best-Pre-trained-Model-Towards-Transferability-Estimation-for-Medical-Image-Segmentation"><a href="#Pick-the-Best-Pre-trained-Model-Towards-Transferability-Estimation-for-Medical-Image-Segmentation" class="headerlink" title="Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation"></a>Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11958">http://arxiv.org/abs/2307.11958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/endoluminalsurgicalvision-imr/ccfv">https://github.com/endoluminalsurgicalvision-imr/ccfv</a></li>
<li>paper_authors: Yuncheng Yang, Meng Wei, Junjun He, Jie Yang, Jin Ye, Yun Gu</li>
<li>for: 这种论文主要用于适用于医疗图像分割任务中的深度神经网络训练，以便更好地利用庞大的医疗图像数据。</li>
<li>methods: 该论文提出了一种新的转移性能度估计（TE）方法，该方法考虑了类别一致性和特征多样性，以更好地估计转移性能度。</li>
<li>results: 对比现有的TE算法，该方法在医疗图像分割任务中的转移性能度估计表现出色，并且在实验中胜过所有现有的TE算法。<details>
<summary>Abstract</summary>
Transfer learning is a critical technique in training deep neural networks for the challenging medical image segmentation task that requires enormous resources. With the abundance of medical image data, many research institutions release models trained on various datasets that can form a huge pool of candidate source models to choose from. Hence, it's vital to estimate the source models' transferability (i.e., the ability to generalize across different downstream tasks) for proper and efficient model reuse. To make up for its deficiency when applying transfer learning to medical image segmentation, in this paper, we therefore propose a new Transferability Estimation (TE) method. We first analyze the drawbacks of using the existing TE algorithms for medical image segmentation and then design a source-free TE framework that considers both class consistency and feature variety for better estimation. Extensive experiments show that our method surpasses all current algorithms for transferability estimation in medical image segmentation. Code is available at https://github.com/EndoluminalSurgicalVision-IMR/CCFV
</details>
<details>
<summary>摘要</summary>
启用转移学习是训练深度神经网络的关键技术，用于具有巨大资源的医学图像分割任务。由于医学图像数据的备受，许多研究机构发布了基于不同数据集的模型，这些模型可以形成一个庞大的候选源模型库。因此，对于正确和高效地 reuse 模型，估计源模型的传输性（即在不同下游任务中generalization）是非常重要的。为了解决医学图像分割中转移学习的不足，本文提出了一种新的传输性估计（TE）方法。我们首先分析了现有TE算法在医学图像分割中的缺点，然后设计了一个无源TE框架，考虑了类含义一致和特征多样性，以更好地估计传输性。经验表明，我们的方法在医学图像分割中的传输性估计比现有算法更高。代码可以在https://github.com/EndoluminalSurgicalVision-IMR/CCFV 中找到。
</details></li>
</ul>
<hr>
<h2 id="High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization"><a href="#High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization" class="headerlink" title="High-performance real-world optical computing trained by in situ model-free optimization"></a>High-performance real-world optical computing trained by in situ model-free optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11957">http://arxiv.org/abs/2307.11957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyuan Zhao, Xin Shu, Renjie Zhou</li>
<li>for: 提高光学计算系统的高速和低能耗数据处理能力，但面临计算吃力和现实模拟差距问题。</li>
<li>methods: 使用模型独立的光学计算系统优化方法，基于排名评分算法直接倒敲光学权重的概率分布，不需要计算吃力和偏见的系统模拟。</li>
<li>results: 在MNIST和FMNIST数据集上实现了高精度分类，并在单层折射光学计算系统上实现了图像自由和高速细胞分析的潜力。<details>
<summary>Abstract</summary>
Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
</details>
<details>
<summary>摘要</summary>
光学计算系统可以提供高速和低能耗数据处理，但面临 computationally demanding 训练和实际与模拟之间的差距。我们提议一种模型自由的解决方案，基于分布式权重的推估算法，用于优化光学计算系统。这种方法将系统视为黑盒，将损失反射直接到光学权重的概率分布中，因此不需要 computationally 复杂和偏见的系统模拟。我们通过在单层散射光学计算系统上进行实验，示出了在 MNIST 和 FMNIST 数据集上的高精度分类。此外，我们还展示了它的潜在应用于无图像和高速细胞分析。我们的提议的简单性和计算资源的低需求，将推动光学计算从实验室示范向实际应用的过渡。
</details></li>
</ul>
<hr>
<h2 id="Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System"><a href="#Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System" class="headerlink" title="Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System"></a>Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02420">http://arxiv.org/abs/2308.02420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Sinclair, Kayla Kautai, Seyed Reza Shahamiri</li>
<li>for: 这个研究旨在开发一个基于深度学习的智能手机应用程序，用于实时评估运动重复次数。</li>
<li>methods: 这个系统包括五个 ком成分：投射估计、阈值、Optical flow、状态机器和计数器。</li>
<li>results: 这个系统在实际测试中获得了98.89%的准确率，并且在预先录取的资料集上获得了98.85%的准确率。<details>
<summary>Abstract</summary>
Automated exercise repetition counting has applications across the physical fitness realm, from personal health to rehabilitation. Motivated by the ubiquity of mobile phones and the benefits of tracking physical activity, this study explored the feasibility of counting exercise repetitions in real-time, using only on-device inference, on smartphones. In this work, after providing an extensive overview of the state-of-the-art automatic exercise repetition counting methods, we introduce a deep learning based exercise repetition counting system for smartphones consisting of five components: (1) Pose estimation, (2) Thresholding, (3) Optical flow, (4) State machine, and (5) Counter. The system is then implemented via a cross-platform mobile application named P\=uioio that uses only the smartphone camera to track repetitions in real time for three standard exercises: Squats, Push-ups, and Pull-ups. The proposed system was evaluated via a dataset of pre-recorded videos of individuals exercising as well as testing by subjects exercising in real time. Evaluation results indicated the system was 98.89% accurate in real-world tests and up to 98.85% when evaluated via the pre-recorded dataset. This makes it an effective, low-cost, and convenient alternative to existing solutions since the proposed system has minimal hardware requirements without requiring any wearable or specific sensors or network connectivity.
</details>
<details>
<summary>摘要</summary>
自动化运动重复计数有应用于身体健身领域的广泛应用，从个人健康到rehabilitation。驱动于手机的普遍和跟踪物理活动的好处，本研究探讨了使用手机上的只 inference来实时计数运动重复的可能性。在这种工作中，我们首先提供了现有自动运动重复计数方法的广泛概述，然后介绍了一种基于深度学习的运动重复计数系统，该系统包括以下五个组成部分：（1）姿势估计，（2）阈值设定，（3）光流计算，（4）状态机，（5）计数器。该系统后来通过一款可在多个平台上运行的跨平台移动应用程序 named P\=uioio 实现，该应用程序只使用手机摄像头来实时跟踪运动重复。我们对三种标准运动进行测试：蹲下、推上和抓上。我们对这些测试结果进行评估，结果表明，该系统在实际测试中的准确率为98.89%，并且在预录视频数据集上的评估结果为98.85%。这使得该系统成为一种有效、低成本、便捷的代替方案，因为该系统具有最低硬件要求，不需要任何佩戴式设备或特定的感应器或网络连接。
</details></li>
</ul>
<hr>
<h2 id="LAMP-Leveraging-Language-Prompts-for-Multi-person-Pose-Estimation"><a href="#LAMP-Leveraging-Language-Prompts-for-Multi-person-Pose-Estimation" class="headerlink" title="LAMP: Leveraging Language Prompts for Multi-person Pose Estimation"></a>LAMP: Leveraging Language Prompts for Multi-person Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11934">http://arxiv.org/abs/2307.11934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shengnanh20/lamp">https://github.com/shengnanh20/lamp</a></li>
<li>paper_authors: Shengnan Hu, Ce Zheng, Zixiang Zhou, Chen Chen, Gita Sukthankar</li>
<li>for: 这篇论文目的是提高人机交互的效果，帮助社交机器人在拥挤的公共场所中穿梭。</li>
<li>methods: 该论文提出了一种新的提示基于的多人姿态估计策略，即语言助け多人姿态估计（LAMP）。该策略利用了一个已经训练好的语言模型（CLIP）生成的文本表示，以便更好地理解人体姿态，并通过文本提示来增强视觉表示的不可见部分。</li>
<li>results: 该论文表明，使用语言supervised Training可以提高单Stage多人姿态估计的性能，并且both个体级和关节级的提示都是训练中的有价值贡献。<details>
<summary>Abstract</summary>
Human-centric visual understanding is an important desideratum for effective human-robot interaction. In order to navigate crowded public places, social robots must be able to interpret the activity of the surrounding humans. This paper addresses one key aspect of human-centric visual understanding, multi-person pose estimation. Achieving good performance on multi-person pose estimation in crowded scenes is difficult due to the challenges of occluded joints and instance separation. In order to tackle these challenges and overcome the limitations of image features in representing invisible body parts, we propose a novel prompt-based pose inference strategy called LAMP (Language Assisted Multi-person Pose estimation). By utilizing the text representations generated by a well-trained language model (CLIP), LAMP can facilitate the understanding of poses on the instance and joint levels, and learn more robust visual representations that are less susceptible to occlusion. This paper demonstrates that language-supervised training boosts the performance of single-stage multi-person pose estimation, and both instance-level and joint-level prompts are valuable for training. The code is available at https://github.com/shengnanh20/LAMP.
</details>
<details>
<summary>摘要</summary>
人类中心视觉理解是人机交互中的重要需求。为了在人权拥挤的公共场所中导航，社交机器人需要能够理解周围人类的活动。这篇论文解决了人类中心视觉理解的一个关键方面，即多人pose estimation。在拥挤场景中实现好的多人pose estimation是困难的，因为隐藏的关节和实例分离是主要的挑战。为了解决这些挑战并超越图像特征在表示隐藏身体部分方面的局限性，我们提出了一种新的提示基于的pose推断策略called LAMP（语言协助多人pose estimation）。通过利用训练好的语言模型（CLIP）生成的文本表示，LAMP可以促进实例和关节水平的pose理解，并学习更加Robust的视觉表示，更少受到遮挡的影响。这篇论文示出，语言超vised训练可以提高单 Stage多人pose estimation的性能，并且实例水平和关节水平的提示都是训练的有价值。代码可以在https://github.com/shengnanh20/LAMP中获取。
</details></li>
</ul>
<hr>
<h2 id="RICo-Rotate-Inpaint-Complete-for-Generalizable-Scene-Reconstruction"><a href="#RICo-Rotate-Inpaint-Complete-for-Generalizable-Scene-Reconstruction" class="headerlink" title="RICo: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction"></a>RICo: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11932">http://arxiv.org/abs/2307.11932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isaac Kasahara, Shubham Agrawal, Selim Engin, Nikhil Chavan-Dafle, Shuran Song, Volkan Isler</li>
<li>for:  scene reconstruction from a single view, with the goal of estimating the full 3D geometry and texture of a scene containing previously unseen objects.</li>
<li>methods:  leveraging large language models to inpaint missing areas of scene color images rendered from different views, and then lifting these inpainted images to 3D by predicting normals of the inpainted image and solving for the missing depth values.</li>
<li>results:  outperforms multiple baselines while providing generalization to novel objects and scenes, with robustness to changes in depth distributions and scale.<details>
<summary>Abstract</summary>
General scene reconstruction refers to the task of estimating the full 3D geometry and texture of a scene containing previously unseen objects. In many practical applications such as AR/VR, autonomous navigation, and robotics, only a single view of the scene may be available, making the scene reconstruction a very challenging task. In this paper, we present a method for scene reconstruction by structurally breaking the problem into two steps: rendering novel views via inpainting and 2D to 3D scene lifting. Specifically, we leverage the generalization capability of large language models to inpaint the missing areas of scene color images rendered from different views. Next, we lift these inpainted images to 3D by predicting normals of the inpainted image and solving for the missing depth values. By predicting for normals instead of depth directly, our method allows for robustness to changes in depth distributions and scale. With rigorous quantitative evaluation, we show that our method outperforms multiple baselines while providing generalization to novel objects and scenes.
</details>
<details>
<summary>摘要</summary>
全景重建指的是根据已知的2D图像来估算场景中包含未知对象的3D几何和文本ure。在许多实际应用中，如AR/VR、自动导航和机器人等，只有一个视图可用，因此场景重建变得非常困难。在这篇论文中，我们提出了一种场景重建方法，通过分解问题为两步来解决：首先，通过填充缺失部分来渲染新的视图图像；然后，使用这些渲染的图像来提升3D场景的抽象。具体来说，我们利用大型自然语言模型来填充场景颜色图像中的缺失部分。接下来，我们使用这些填充的图像来预测场景的法向量，并通过解决缺失的深度值来提升3D场景。由于预测法向量而不是直接预测深度，我们的方法具有对深度分布和比例的弹性。经过严谨的量化评估，我们表明了我们的方法在多个基eline上方法性能高，同时具有对新对象和场景的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="PartDiff-Image-Super-resolution-with-Partial-Diffusion-Models"><a href="#PartDiff-Image-Super-resolution-with-Partial-Diffusion-Models" class="headerlink" title="PartDiff: Image Super-resolution with Partial Diffusion Models"></a>PartDiff: Image Super-resolution with Partial Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11926">http://arxiv.org/abs/2307.11926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Zhao, Alex Ling Yu Hung, Kaifeng Pang, Haoxin Zheng, Kyunghyun Sung</li>
<li>for: 这个论文主要针对的是图像超分辨率生成任务中的Diffusion-based生成模型，它们可以很好地生成高质量的图像。</li>
<li>methods: 这个论文提出了一种新的Partial Diffusion Model（PartDiff），它在图像扩散过程中只需要扩散到中间 latent state 而不是扩散到完全随机噪声中，从中间 latent state 开始进行恢复。</li>
<li>results: 对于MRI和自然图像的超分辨率生成任务，Partial Diffusion Models 可以significantly reduce the number of denoising steps 而不是 sacrificing the quality of generation。<details>
<summary>Abstract</summary>
Denoising diffusion probabilistic models (DDPMs) have achieved impressive performance on various image generation tasks, including image super-resolution. By learning to reverse the process of gradually diffusing the data distribution into Gaussian noise, DDPMs generate new data by iteratively denoising from random noise. Despite their impressive performance, diffusion-based generative models suffer from high computational costs due to the large number of denoising steps.In this paper, we first observed that the intermediate latent states gradually converge and become indistinguishable when diffusing a pair of low- and high-resolution images. This observation inspired us to propose the Partial Diffusion Model (PartDiff), which diffuses the image to an intermediate latent state instead of pure random noise, where the intermediate latent state is approximated by the latent of diffusing the low-resolution image. During generation, Partial Diffusion Models start denoising from the intermediate distribution and perform only a part of the denoising steps. Additionally, to mitigate the error caused by the approximation, we introduce "latent alignment", which aligns the latent between low- and high-resolution images during training. Experiments on both magnetic resonance imaging (MRI) and natural images show that, compared to plain diffusion-based super-resolution methods, Partial Diffusion Models significantly reduce the number of denoising steps without sacrificing the quality of generation.
</details>
<details>
<summary>摘要</summary>
diffusion probabilistic models (DDPMs) 拥有在不同的图像生成任务中表现出色，包括图像超解像。通过学习逆转数据分布慢慢散发的过程，DDPMs 生成新的数据，通过多次减噪来实现。尽管它们在表现方面印象深刻，但散发基于的生成模型受到高计算成本的限制，这是因为散发步骤的数量太多。在这篇论文中，我们首先发现了将两个低分辨率和高分辨率图像散发到了中间状态后，中间状态会逐渐减少差异，并变得无法区分。这一观察点我们提出了partial diffusion模型（PartDiff），它将图像散发到中间状态而不是完全随机噪声中，中间状态approximerated by low-resolution image的散发latent。在生成过程中，Partial Diffusion Models从中间分布开始减噪，并只完成一部分的减噪步骤。此外，为了减少由approximation引起的误差，我们引入"latent alignment"，在训练期间对低分辨率和高分辨率图像的latent进行对齐。实验表明，与普通的散发基于超解像方法相比，Partial Diffusion Models可以在MRI和自然图像上减少减噪步骤数量，而不是牺牲生成质量。
</details></li>
</ul>
<hr>
<h2 id="Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data"><a href="#Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data" class="headerlink" title="Poverty rate prediction using multi-modal survey and earth observation data"></a>Poverty rate prediction using multi-modal survey and earth observation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11921">http://arxiv.org/abs/2307.11921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Fobi, Manuel Cardona, Elliott Collins, Caleb Robinson, Anthony Ortiz, Tina Sederholm, Rahul Dodhia, Juan Lavista Ferres</li>
<li>For: The paper aims to predict the poverty rate of a region by combining household demographic and living standards survey questions with features derived from satellite imagery.* Methods: The paper uses a single-step featurization method to extract visual features from freely available 10m&#x2F;px Sentinel-2 surface reflectance satellite imagery, and combines these features with ten survey questions in a proxy means test (PMT) to estimate poverty rates. The paper also proposes an approach for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery.* Results: The inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88%, and using a subset of survey questions selected based on their complementarity to the visual features results in the best performance with errors decreasing from 4.09% to 3.71%. The extracted visual features also encode geographic and urbanization differences between regions.<details>
<summary>Abstract</summary>
This work presents an approach for combining household demographic and living standards survey questions with features derived from satellite imagery to predict the poverty rate of a region. Our approach utilizes visual features obtained from a single-step featurization method applied to freely available 10m/px Sentinel-2 surface reflectance satellite imagery. These visual features are combined with ten survey questions in a proxy means test (PMT) to estimate whether a household is below the poverty line. We show that the inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88% over a nationally representative out-of-sample test set. In addition to including satellite imagery features in proxy means tests, we propose an approach for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery. Specifically, we design a survey variable selection approach guided by the full survey and image features and use the approach to determine the most relevant set of small survey questions to include in a PMT. We validate the choice of small survey questions in a downstream task of predicting the poverty rate using the small set of questions. This approach results in the best performance -- errors in poverty rate decrease from 4.09% to 3.71%. We show that extracted visual features encode geographic and urbanization differences between regions.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这项研究提出了一种方法，将家庭调查问题与卫星成像数据特征结合起来预测地区贫困率。该方法使用单步特征化方法提取自由可用的10m/px Sentinal-2表面反射卫星成像数据中的视觉特征，然后与十个调查问题结合在一起进行代理平均测试（PMT），以估算家庭是否下于贫困线。我们发现，通过包含卫星成像特征，贫困率估算的均误率从4.09%降低到3.88%。此外，我们还提出了一种选择调查问题的方法，该方法根据全面调查和图像特征进行指导，以选择最相关的小调查问题来包含在PMT中。我们验证了这些小调查问题的选择，并发现贫困率估算的误差从4.09%降低到3.71%。我们发现，提取的视觉特征含有地域和城市化差异。
</details></li>
</ul>
<hr>
<h2 id="Building3D-An-Urban-Scale-Dataset-and-Benchmarks-for-Learning-Roof-Structures-from-Point-Clouds"><a href="#Building3D-An-Urban-Scale-Dataset-and-Benchmarks-for-Learning-Roof-Structures-from-Point-Clouds" class="headerlink" title="Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds"></a>Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11914">http://arxiv.org/abs/2307.11914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruisheng Wang, Shangfeng Huang, Hongxin Yang</li>
<li>for: 这个论文主要是为了提供一个大规模的城市建筑模型 benchmark，以便进行未来城市建筑模型的研究。</li>
<li>methods: 这个论文使用了 LiDAR 点云测试获得的数据，并使用了各种手工和深度特征基础的算法进行评估。</li>
<li>results: 论文发现了城市建筑模型存在高内分组变化、数据不对称和大规模噪声等挑战，并提供了首个和最大的城市建筑模型 benchmark，以便进行未来城市建筑模型的研究。<details>
<summary>Abstract</summary>
Urban modeling from LiDAR point clouds is an important topic in computer vision, computer graphics, photogrammetry and remote sensing. 3D city models have found a wide range of applications in smart cities, autonomous navigation, urban planning and mapping etc. However, existing datasets for 3D modeling mainly focus on common objects such as furniture or cars. Lack of building datasets has become a major obstacle for applying deep learning technology to specific domains such as urban modeling. In this paper, we present a urban-scale dataset consisting of more than 160 thousands buildings along with corresponding point clouds, mesh and wire-frame models, covering 16 cities in Estonia about 998 Km2. We extensively evaluate performance of state-of-the-art algorithms including handcrafted and deep feature based methods. Experimental results indicate that Building3D has challenges of high intra-class variance, data imbalance and large-scale noises. The Building3D is the first and largest urban-scale building modeling benchmark, allowing a comparison of supervised and self-supervised learning methods. We believe that our Building3D will facilitate future research on urban modeling, aerial path planning, mesh simplification, and semantic/part segmentation etc.
</details>
<details>
<summary>摘要</summary>
城市模型从LiDAR点云是计算机视觉、计算机图形、光学测量和远程感知等领域的重要话题。3D城市模型在智能城市、自动导航、城市规划和地图等领域发现了广泛的应用。然而，现有的3D模型数据集主要集中在常见的物品 such as 家具或车辆。lack of 建筑数据集成为应用深度学习技术于特定领域 such as 城市模型的主要障碍。在这篇论文中，我们提供了一个城市级别的数据集，包括超过16万个建筑物 along with 相应的点云、网格和细线模型，覆盖了16座城市，总面积约998平方公里。我们进行了广泛的性能评估，包括手工设计和深度特征基于方法。实验结果表明，Building3D存在高内类差异、数据不均衡和大规模噪音等挑战。Building3D是首个和最大的城市级别建筑模型 benchmark，允许对supervised和自主学习方法进行比较。我们认为，我们的Building3D将促进未来对城市模型、空中路径规划、网格简化和semantic/part分割等方面的研究。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks"><a href="#Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks" class="headerlink" title="Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks"></a>Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11906">http://arxiv.org/abs/2307.11906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed</li>
<li>for: 防护受到攻击的深度学习系统，因为它们容易受到恶意攻击的威胁。</li>
<li>methods: 我们提出了一种基于微生物遗传学算法的黑盒攻击方法，不需要对目标模型和解释模型有任何专业知识。</li>
<li>results: 我们的实验结果显示，这种攻击方法可以实现高度的攻击成功率，使用攻击示例和属性地图，与正常样本几乎无法区别。<details>
<summary>Abstract</summary>
Deep learning has been rapidly employed in many applications revolutionizing many industries, but it is known to be vulnerable to adversarial attacks. Such attacks pose a serious threat to deep learning-based systems compromising their integrity, reliability, and trust. Interpretable Deep Learning Systems (IDLSes) are designed to make the system more transparent and explainable, but they are also shown to be susceptible to attacks. In this work, we propose a novel microbial genetic algorithm-based black-box attack against IDLSes that requires no prior knowledge of the target model and its interpretation model. The proposed attack is a query-efficient approach that combines transfer-based and score-based methods, making it a powerful tool to unveil IDLS vulnerabilities. Our experiments of the attack show high attack success rates using adversarial examples with attribution maps that are highly similar to those of benign samples which makes it difficult to detect even by human analysts. Our results highlight the need for improved IDLS security to ensure their practical reliability.
</details>
<details>
<summary>摘要</summary>
深度学习已经广泛应用在多个领域，但它们容易受到敌意攻击。这些攻击会对深度学习基于系统造成严重的威胁，对其稳定性、可靠性和信任性造成潜在的影响。可解释深度学习系统（IDLS）是为了使系统更加透明和可解释的，但它们也被证明容易受到攻击。在这种工作中，我们提出了一种基于微生物遗传算法的黑盒攻击方法，不需要攻击目标模型和其解释模型的先前知识。我们的攻击方法结合了传递基本方法和分数基本方法，使其成为攻击IDLS的强大工具。我们的实验结果显示，使用对抗示例和归属地图可以达到高攻击成功率，这些对抗示例与正常样本具有高度相似的归属地图，使其具有难以探测的特点。我们的结果 highlights the need for improved IDLS security to ensure their practical reliability.
</details></li>
</ul>
<hr>
<h2 id="Model-Compression-Methods-for-YOLOv5-A-Review"><a href="#Model-Compression-Methods-for-YOLOv5-A-Review" class="headerlink" title="Model Compression Methods for YOLOv5: A Review"></a>Model Compression Methods for YOLOv5: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11904">http://arxiv.org/abs/2307.11904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Jani, Jamil Fayyad, Younes Al-Younes, Homayoun Najjaran</li>
<li>for: 本文主要针对于增强YOLO对象检测器的研究，以提高其精度和效率。</li>
<li>methods: 本文主要采用network pruning和quantization两种方法来压缩YOLOv5模型，以适应资源有限的边缘设备。</li>
<li>results: 通过对YOLOv5模型进行压缩，可以降低内存使用量和推理时间，使其在硬件限制的边缘设备上进行部署成为可能。但是，在实施中还存在一些挑战，需要进一步的探索和优化。<details>
<summary>Abstract</summary>
Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resource-limited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we identify gaps in adapting pruning and quantization for compressing YOLOv5, and provide future directions in this area for further exploration. Among several versions of YOLO, we specifically choose YOLOv5 for its excellent trade-off between recency and popularity in literature. This is the first specific review paper that surveys pruning and quantization methods from an implementation point of view on YOLOv5. Our study is also extendable to newer versions of YOLO as implementing them on resource-limited devices poses the same challenges that persist even today. This paper targets those interested in the practical deployment of model compression methods on YOLOv5, and in exploring different compression techniques that can be used for subsequent versions of YOLO.
</details>
<details>
<summary>摘要</summary>
过去几年，对 YOLO 对象检测器进行了广泛的研究，旨在提高其精度和效率。自其出现以来，共有八个主要版本的 YOLO 发布，以提高其性能和效能。虽然 YOLO 在许多领域得到了广泛的应用，但在具有限制的硬件设备上部署它存在挑战。为解决这个问题，各种神经网络压缩方法得到了开发，这些方法可以分为三类：网络剪辑、量化和知识传递。由于这些方法的实际成果，如减少内存使用量和计算时间，因此它们在硬件限制的边缘设备上部署成为了必要的。在本文中，我们主要关注剪辑和量化，因为它们在可控性方面比较高。我们按照不同的方法进行了分类和分析，并通过应用这些方法于 YOLOv5 来评估其实际效果。通过这种方式，我们可以了解剪辑和量化在 YOLOv5 上的应用存在哪些挑战，并提供未来研究的方向。在多个 YOLO 版本中，我们选择了 YOLOv5，因为它在文献中的悠久度和受欢迎程度都非常高。这是对剪辑和量化在 YOLOv5 上的实践评估的首个专题评论文。我们的研究也可以扩展到更新版本的 YOLO，因为在具有限制的硬件设备上部署它们也存在同样的挑战。这篇文章针对那些关注实际部署模型压缩方法的人，以及想要探索不同的压缩技术，以应用于更新版本的 YOLO。
</details></li>
</ul>
<hr>
<h2 id="Selecting-the-motion-ground-truth-for-loose-fitting-wearables-benchmarking-optical-MoCap-methods"><a href="#Selecting-the-motion-ground-truth-for-loose-fitting-wearables-benchmarking-optical-MoCap-methods" class="headerlink" title="Selecting the motion ground truth for loose-fitting wearables: benchmarking optical MoCap methods"></a>Selecting the motion ground truth for loose-fitting wearables: benchmarking optical MoCap methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11881">http://arxiv.org/abs/2307.11881</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lalasray/dmcb">https://github.com/lalasray/dmcb</a></li>
<li>paper_authors: Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Paul Lukowicz</li>
<li>for: 用于评估 оптиче markers 和 marker-less MoCap 的性能，以选择最佳的质量评估方法。</li>
<li>methods: 使用了大量的实际录制 MoCap 数据，并对不同的 drape 水平进行并发的 3D 物理 simulations，以评估 marker-based 和 marker-less MoCap 方法的性能。</li>
<li>results: marker-based MoCap 和 marker-less MoCap 在轻度的穿着衣服下 both exhibit  significan performance loss (&gt;10cm)，但是在日常活动中， marker-less MoCap 略微超过 marker-based MoCap，making it a favorable and cost-effective choice for wearable studies.<details>
<summary>Abstract</summary>
To help smart wearable researchers choose the optimal ground truth methods for motion capturing (MoCap) for all types of loose garments, we present a benchmark, DrapeMoCapBench (DMCB), specifically designed to evaluate the performance of optical marker-based and marker-less MoCap. High-cost marker-based MoCap systems are well-known as precise golden standards. However, a less well-known caveat is that they require skin-tight fitting markers on bony areas to ensure the specified precision, making them questionable for loose garments. On the other hand, marker-less MoCap methods powered by computer vision models have matured over the years, which have meager costs as smartphone cameras would suffice. To this end, DMCB uses large real-world recorded MoCap datasets to perform parallel 3D physics simulations with a wide range of diversities: six levels of drape from skin-tight to extremely draped garments, three levels of motions and six body type - gender combinations to benchmark state-of-the-art optical marker-based and marker-less MoCap methods to identify the best-performing method in different scenarios. In assessing the performance of marker-based and low-cost marker-less MoCap for casual loose garments both approaches exhibit significant performance loss (>10cm), but for everyday activities involving basic and fast motions, marker-less MoCap slightly outperforms marker-based MoCap, making it a favorable and cost-effective choice for wearable studies.
</details>
<details>
<summary>摘要</summary>
为帮助智能佩戴设备研究人员选择最佳的股权实验方法，我们提出了一个标准化的测试平台：DrapeMoCapBench（DMCB），用于评估光学标记基本和无标记MoCap的性能。高成本的光学标记基本MoCap系统已经被广泛认可为精度的金标准，但是它们需要在骨部位上粘贴皮肤紧密的标记，以确保 specify 的精度，这使得它们对松裤服不太可靠。而无标记MoCap方法，通过计算机视觉模型已经成熟了多年，它们的成本很低，只需要使用智能手机相机即可。因此，DMCB使用了大量的真实世界记录的MoCap数据进行并行的3D物理模拟，以评估当今最佳的光学标记基本和无标记MoCap方法的性能。在评估松裤服上的光学标记基本和低成本无标记MoCap方法的性能时，两者都表现出了明显的性能损失（>10cm）。但是在日常活动中涉及到基本和快速动作时，无标记MoCap方法尚微出perform marker-based MoCap，使其成为便宜且可靠的选择 для wearable研究。
</details></li>
</ul>
<hr>
<h2 id="Digital-Modeling-on-Large-Kernel-Metamaterial-Neural-Network"><a href="#Digital-Modeling-on-Large-Kernel-Metamaterial-Neural-Network" class="headerlink" title="Digital Modeling on Large Kernel Metamaterial Neural Network"></a>Digital Modeling on Large Kernel Metamaterial Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11862">http://arxiv.org/abs/2307.11862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quan Liu, Hanyu Zheng, Brandon T. Swartz, Ho hin Lee, Zuhayr Asad, Ivan Kravchenko, Jason G. Valentine, Yuankai Huo<br>for: 这个论文是针对现代深度神经网络（DNNs）的物理部署（例如CPUs和GPUs）所做的研究。这种设计可能会导致重要的计算负担，延迟和耗电量问题，这些问题在物联网（IoT）、边缘 computing 和无人机（drones）应用中是critical的限制。methods: 这个论文使用了最新的光学计算单元（例如元material），实现了无源电力和光速神经网络。但是，光学设计的物理限制（例如精度、噪声和宽度）会导致物理设计的限制。此外，现有的元material神经网络（MNN）的优点（例如光速计算）并未经过标准3x3卷积核心的探索。这个论文提出了一个新的大型卷积元material神经网络（LMNN），它可以最大化现代SOTA MNN的数位容量，并考虑光学限制。results: 这个论文的实验结果显示，使用LMNN可以提高分类精度，同时降低计算延迟。实验结果表明，将 computation cost of convolutional front-end offloaded into fabricated optical hardware可以提高分类精度。这个研究表明，LMNN的开发是可能实现无源电力和光速AI的一个重要步骤。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) utilized recently are physically deployed with computational units (e.g., CPUs and GPUs). Such a design might lead to a heavy computational burden, significant latency, and intensive power consumption, which are critical limitations in applications such as the Internet of Things (IoT), edge computing, and the usage of drones. Recent advances in optical computational units (e.g., metamaterial) have shed light on energy-free and light-speed neural networks. However, the digital design of the metamaterial neural network (MNN) is fundamentally limited by its physical limitations, such as precision, noise, and bandwidth during fabrication. Moreover, the unique advantages of MNN's (e.g., light-speed computation) are not fully explored via standard 3x3 convolution kernels. In this paper, we propose a novel large kernel metamaterial neural network (LMNN) that maximizes the digital capacity of the state-of-the-art (SOTA) MNN with model re-parametrization and network compression, while also considering the optical limitation explicitly. The new digital learning scheme can maximize the learning capacity of MNN while modeling the physical restrictions of meta-optic. With the proposed LMNN, the computation cost of the convolutional front-end can be offloaded into fabricated optical hardware. The experimental results on two publicly available datasets demonstrate that the optimized hybrid design improved classification accuracy while reducing computational latency. The development of the proposed LMNN is a promising step towards the ultimate goal of energy-free and light-speed AI.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）最近几年广泛应用，通常通过计算单元（如CPU和GPU）进行物理部署。这种设计可能会导致重大的计算负担、显著的延迟和大量的电力消耗，这些限制在互联网物联网（IoT）、边缘计算和无人机应用中非常 kritisch。 latest advances in optical computational units（如元material）have shed light on energy-free and light-speed neural networks. However, the digital design of the metamaterial neural network (MNN) is fundamentally limited by its physical limitations, such as precision, noise, and bandwidth during fabrication. Moreover, the unique advantages of MNN's (e.g., light-speed computation) are not fully explored via standard 3x3 convolution kernels. In this paper, we propose a novel large kernel metamaterial neural network (LMNN) that maximizes the digital capacity of the state-of-the-art (SOTA) MNN with model re-parametrization and network compression, while also considering the optical limitation explicitly. The new digital learning scheme can maximize the learning capacity of MNN while modeling the physical restrictions of meta-optic. With the proposed LMNN, the computation cost of the convolutional front-end can be offloaded into fabricated optical hardware. The experimental results on two publicly available datasets demonstrate that the optimized hybrid design improved classification accuracy while reducing computational latency. The development of the proposed LMNN is a promising step towards the ultimate goal of energy-free and light-speed AI.Here's a word-for-word translation of the text into Simplified Chinese:深度神经网络（DNN）最近几年广泛应用，通常通过计算单元（如CPU和GPU）进行物理部署。这种设计可能会导致重大的计算负担、显著的延迟和大量的电力消耗，这些限制在互联网物联网（IoT）、边缘计算和无人机应用中非常 kritisch。 最近的光学计算单元（如元material）的进步抛光了能源免费和光速神经网络。然而，光学神经网络（MNN）的数字设计受到物理限制，如精度、雷达和带宽的制约，这些限制在fabrication中表现出来。此外，MNN的独特优点（如光速计算）通过标准3x3卷积核不充分发挥。在这篇论文中，我们提出了一种新的大 kernel metamaterial神经网络（LMNN），该模型可以最大化SOTA MNN的数字容量，同时考虑光学限制。新的数字学习方案可以在MNN中最大化学习能力，同时模拟meta-optic的物理限制。通过提出LMNN，计算前端的计算成本可以卸载到fabricated的光学硬件上。实验结果表明，使用两个公共可用的数据集，LMNN的优化型材料设计可以提高分类精度，同时减少计算延迟。LMNN的发展是前往能源免费和光速AI的普遍进步的重要步骤。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Your-Trained-DETRs-with-Box-Refinement"><a href="#Enhancing-Your-Trained-DETRs-with-Box-Refinement" class="headerlink" title="Enhancing Your Trained DETRs with Box Refinement"></a>Enhancing Your Trained DETRs with Box Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11828">http://arxiv.org/abs/2307.11828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiqunchen1999/refinebox">https://github.com/yiqunchen1999/refinebox</a></li>
<li>paper_authors: Yiqun Chen, Qiang Chen, Peize Sun, Shoufa Chen, Jingdong Wang, Jian Cheng</li>
<li>for: 提高 DETR 和其 variants 的本地化性能</li>
<li>methods: 使用轻量级增强网络进行推理 Outputs 的精度提高</li>
<li>results: 在 COCO 和 LVIS $1.0 $ 上实验表明 RefineBox 可以提高 DETR 和其 variants 的性能，例如 DETR 的性能提高 2.4 AP，Conditinal-DETR 的性能提高 2.5 AP，DAB-DETR 的性能提高 1.9 AP，DN-DETR 的性能提高 1.6 AP。Here’s the summary in English for reference:</li>
<li>for: Improving the localization performance of DETR and its variants</li>
<li>methods: Using lightweight refinement networks to refine the outputs of DETR-like detectors</li>
<li>results: Experimental results on COCO and LVIS $1.0$ show that RefineBox can improve the performance of DETR and its variants, with performance gains of 2.4 AP, 2.5 AP, 1.9 AP, and 1.6 AP for DETR, Conditinal-DETR, DAB-DETR, and DN-DETR, respectively.<details>
<summary>Abstract</summary>
We present a conceptually simple, efficient, and general framework for localization problems in DETR-like models. We add plugins to well-trained models instead of inefficiently designing new models and training them from scratch. The method, called RefineBox, refines the outputs of DETR-like detectors by lightweight refinement networks. RefineBox is easy to implement and train as it only leverages the features and predicted boxes from the well-trained detection models. Our method is also efficient as we freeze the trained detectors during training. In addition, we can easily generalize RefineBox to various trained detection models without any modification. We conduct experiments on COCO and LVIS $1.0$. Experimental results indicate the effectiveness of our RefineBox for DETR and its representative variants (Figure 1). For example, the performance gains for DETR, Conditinal-DETR, DAB-DETR, and DN-DETR are 2.4 AP, 2.5 AP, 1.9 AP, and 1.6 AP, respectively. We hope our work will bring the attention of the detection community to the localization bottleneck of current DETR-like models and highlight the potential of the RefineBox framework. Code and models will be publicly available at: \href{https://github.com/YiqunChen1999/RefineBox}{https://github.com/YiqunChen1999/RefineBox}.
</details>
<details>
<summary>摘要</summary>
我们提出了一个概念简单、高效和通用的框位问题解决方案，用于DETR-like模型。我们在已经训练过的模型上添加插件而不是从scratch开发新的模型和重新训练。我们称之为RefineBox，它使用轻量级修正网络来精度地修正DETR-like探测器的输出。RefineBox易于实现和训练，只需利用已经训练过的检测器的特征和预测框。我们的方法也具有高效性，因为我们在训练过程中冻结了已经训练过的检测器。此外，我们可以轻松地扩展RefineBox到不同的训练过的检测器上，无需修改。我们在COCO和LVIS $1.0$上进行了实验，实验结果表明RefineBox对DETR和其相关变体（图1）具有效果。例如，对DETR、Conditinal-DETR、DAB-DETR和DN-DETR的性能提升分别为2.4 AP、2.5 AP、1.9 AP和1.6 AP。我们希望我们的工作能吸引检测社区对当前DETR-like模型的本地化瓶颈的注意，并强调RefineBox框架的潜在可能性。代码和模型将在GitHub上公开：<https://github.com/YiqunChen1999/RefineBox>。
</details></li>
</ul>
<hr>
<h2 id="BandRe-Rethinking-Band-Pass-Filters-for-Scale-Wise-Object-Detection-Evaluation"><a href="#BandRe-Rethinking-Band-Pass-Filters-for-Scale-Wise-Object-Detection-Evaluation" class="headerlink" title="BandRe: Rethinking Band-Pass Filters for Scale-Wise Object Detection Evaluation"></a>BandRe: Rethinking Band-Pass Filters for Scale-Wise Object Detection Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11748">http://arxiv.org/abs/2307.11748</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shinya7y/UniverseNet">https://github.com/shinya7y/UniverseNet</a></li>
<li>paper_authors: Yosuke Shinya</li>
<li>for: 本文提出了一种新的精度评估方法，用于评估物体检测器在实际应用中的性能。</li>
<li>methods: 本文使用了一个筛 бан组合，包括三角形和梯形带通过筛，来评估物体检测器的精度。</li>
<li>results: 经过实验，本文显示了新的精度评估方法可以准确地评估物体检测器的性能，并且可以强调不同的方法和数据集之间的差异。<details>
<summary>Abstract</summary>
Scale-wise evaluation of object detectors is important for real-world applications. However, existing metrics are either coarse or not sufficiently reliable. In this paper, we propose novel scale-wise metrics that strike a balance between fineness and reliability, using a filter bank consisting of triangular and trapezoidal band-pass filters. We conduct experiments with two methods on two datasets and show that the proposed metrics can highlight the differences between the methods and between the datasets. Code is available at https://github.com/shinya7y/UniverseNet .
</details>
<details>
<summary>摘要</summary>
精度评估是对实际应用中对象检测器的评估非常重要。然而，现有的指标都是 Either too coarse or not reliable enough。在这篇论文中，我们提出了一种新的精度指标，它们能够在精度和可靠性之间做出平衡，使用了一个由三角形和梯形带滤波器组成的筛子。我们在两种方法和两个数据集上进行了实验，并证明了我们的指标可以更好地反映方法和数据集之间的差异。代码可以在 GitHub 上找到：https://github.com/shinya7y/UniverseNet。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Data-Augmentation-Learning-using-Bilevel-Optimization-for-Histopathological-Images"><a href="#Automatic-Data-Augmentation-Learning-using-Bilevel-Optimization-for-Histopathological-Images" class="headerlink" title="Automatic Data Augmentation Learning using Bilevel Optimization for Histopathological Images"></a>Automatic Data Augmentation Learning using Bilevel Optimization for Histopathological Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11808">http://arxiv.org/abs/2307.11808</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smounsav/bilevel_augment_histo">https://github.com/smounsav/bilevel_augment_histo</a></li>
<li>paper_authors: Saypraseuth Mounsaveng, Issam Laradji, David Vázquez, Marco Perdersoli, Ismail Ben Ayed</li>
<li>for: 用于适应深度学习模型在 histopathological 图像分类中的训练问题，因为细胞和组织的颜色和形状变化，以及有限的数据量，使模型学习这些变化困难。</li>
<li>methods: 使用数据扩大（DA）技术，在训练过程中生成更多的样本，以帮助模型对颜色和形状变化变得抗变异。</li>
<li>results: 通过自动学习 DA 参数，使用 truncated backpropagation 进行快速和高效地学习，并在六个不同的 dataset 上进行验证。实验结果表明，我们的模型可以学习更有用的颜色和旋转变换，而不需要手动选择 DA 变换。此外，我们的模型只需要微调几个方法特有的超参数，但是性能更高。<details>
<summary>Abstract</summary>
Training a deep learning model to classify histopathological images is challenging, because of the color and shape variability of the cells and tissues, and the reduced amount of available data, which does not allow proper learning of those variations. Variations can come from the image acquisition process, for example, due to different cell staining protocols or tissue deformation. To tackle this challenge, Data Augmentation (DA) can be used during training to generate additional samples by applying transformations to existing ones, to help the model become invariant to those color and shape transformations. The problem with DA is that it is not only dataset-specific but it also requires domain knowledge, which is not always available. Without this knowledge, selecting the right transformations can only be done using heuristics or through a computationally demanding search. To address this, we propose an automatic DA learning method. In this method, the DA parameters, i.e. the transformation parameters needed to improve the model training, are considered learnable and are learned automatically using a bilevel optimization approach in a quick and efficient way using truncated backpropagation. We validated the method on six different datasets. Experimental results show that our model can learn color and affine transformations that are more helpful to train an image classifier than predefined DA transformations, which are also more expensive as they need to be selected before the training by grid search on a validation set. We also show that similarly to a model trained with RandAugment, our model has also only a few method-specific hyperparameters to tune but is performing better. This makes our model a good solution for learning the best DA parameters, especially in the context of histopathological images, where defining potentially useful transformation heuristically is not trivial.
</details>
<details>
<summary>摘要</summary>
训练深度学习模型分类 histopathological 图像是具有挑战性，因为细胞和组织的颜色和形状可能具有差异，而且可用数据的量相对较少，不允许模型学习这些差异。这些差异可能来自图像获取过程中，例如不同的细胞染色协议或组织变形。为解决这个挑战，数据扩大（DA）可以在训练过程中应用转换来生成更多的样本，以帮助模型对颜色和形状变化变得抗变异。然而，DA并不是 dataset-specific，而且需要域知识，即不一定可以获得。在缺乏域知识的情况下，选择正确的转换只能通过规则或通过计算昂贵的搜索来实现。为此，我们提出了一种自动 DA 学习方法。在这种方法中，DA 参数，即需要改进模型训练的转换参数，被视为可学习的并通过归档逆传播进行自动学习，以便快速和高效地进行学习。我们在六个不同的数据集上进行验证。实验结果表明，我们的模型可以学习更有用的颜色和Affine 转换，而不需要手动选择DA转换。此外，我们的模型也只需要少量的方法特有超参数来调整，但是性能更高。这使得我们的模型成为了学习最佳 DA 参数的好解决方案，特别是在 histopathological 图像上，因为定义有用的转换方法可能不是易事。
</details></li>
</ul>
<hr>
<h2 id="3D-Skeletonization-of-Complex-Grapevines-for-Robotic-Pruning"><a href="#3D-Skeletonization-of-Complex-Grapevines-for-Robotic-Pruning" class="headerlink" title="3D Skeletonization of Complex Grapevines for Robotic Pruning"></a>3D Skeletonization of Complex Grapevines for Robotic Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11706">http://arxiv.org/abs/2307.11706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Schneider, Sushanth Jayanth, Abhisesh Silwal, George Kantor</li>
<li>for: 提高机器人剪刀技术，以便在实际商业葡萄园中进行葡萄藤梢剪刀</li>
<li>methods: 基于植物skeletonization技术，提高机器人视觉能力，以便在 denser和更复杂的葡萄藤结构中进行剪刀</li>
<li>results: 提高剪刀精度，使用3D和skeletal信息可以更 preciselly predict剪刀重量，超过了先前的工作<details>
<summary>Abstract</summary>
Robotic pruning of dormant grapevines is an area of active research in order to promote vine balance and grape quality, but so far robotic efforts have largely focused on planar, simplified vines not representative of commercial vineyards. This paper aims to advance the robotic perception capabilities necessary for pruning in denser and more complex vine structures by extending plant skeletonization techniques. The proposed pipeline generates skeletal grapevine models that have lower reprojection error and higher connectivity than baseline algorithms. We also show how 3D and skeletal information enables prediction accuracy of pruning weight for dense vines surpassing prior work, where pruning weight is an important vine metric influencing pruning site selection.
</details>
<details>
<summary>摘要</summary>
“机械剪裁棕榈葡萄是一项活跃的研究领域，以促进葡萄平衡和质量提高，但目前机械努力主要集中在平面化、简单的葡萄藤上。本文旨在提高机械识别能力，以便在更复杂和紧凑的葡萄藤结构中进行剪裁。我们提出的管道使得植物skeletonization技术得到扩展，生成的股骨葡萄模型具有较低的重oprojection误差和更高的连接率，并且我们还示出了基于3D和股骨信息的剪裁重量预测精度超过先前的工作，剪裁重量是葡萄metric关键参数，对剪裁点选择产生重要影响。”
</details></li>
</ul>
<hr>
<h2 id="SACReg-Scene-Agnostic-Coordinate-Regression-for-Visual-Localization"><a href="#SACReg-Scene-Agnostic-Coordinate-Regression-for-Visual-Localization" class="headerlink" title="SACReg: Scene-Agnostic Coordinate Regression for Visual Localization"></a>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11702">http://arxiv.org/abs/2307.11702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jerome Revaud, Yohann Cabon, Romain Brégier, JongMin Lee, Philippe Weinzaepfel</li>
<li>for: 这个研究旨在提出一个可以应用于各种景象的全局坐标 regression 模型，以提高现有的Scene Regression（SCR）方法的缩减性和应用范围。</li>
<li>methods: 本研究使用了 transformer 架构，并可以处理变数数量的图像和罕见2D-3D标注。具体来说，模型通过从 off-the-shelf 图像检索技术和 Structure-from-Motion 数据库获取输入，并使用 transformer 架构进行训练。</li>
<li>results: 研究发现，这个模型可以在多个测试 benchmark 上表现出色，特别是在Scene Regression 方法中，并在 Cambridge localization benchmark 上设置了新的州立纪录，甚至超越了 feature-matching-based 方法。<details>
<summary>Abstract</summary>
Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain mostly scene-specific or limited to small scenes and thus hardly scale to realistic datasets. In this paper, we propose a new paradigm where a single generic SCR model is trained once to be then deployed to new test scenes, regardless of their scale and without further finetuning. For a given query image, it collects inputs from off-the-shelf image retrieval techniques and Structure-from-Motion databases: a list of relevant database images with sparse pointwise 2D-3D annotations. The model is based on the transformer architecture and can take a variable number of images and sparse 2D-3D annotations as input. It is trained on a few diverse datasets and significantly outperforms other scene regression approaches on several benchmarks, including scene-specific models, for visual localization. In particular, we set a new state of the art on the Cambridge localization benchmark, even outperforming feature-matching-based approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/cs.CV_2023_07_22/" data-id="clorjzl5u00fif18842dg9tkv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/cs.AI_2023_07_22/" class="article-date">
  <time datetime="2023-07-22T12:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/22/cs.AI_2023_07_22/">cs.AI - 2023-07-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC"><a href="#A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC" class="headerlink" title="A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC"></a>A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12115">http://arxiv.org/abs/2307.12115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Chen, Changyan Yi, Hongyang Du, Dusit Niyato, Jiawen Kang, Jun Cai, Xuemin, Shen</li>
<li>for: 这篇论文是为了探讨移动人工智能生成内容（AIGC）技术在人类数字双（HDT）应用中的可能性和挑战而写的。</li>
<li>methods: 该论文提出了一种基于移动AIGC的HDT系统架构，并确定了相关的设计要求和挑战。同时，文章还介绍了两个使用场景，即手术规划和个性化药物。</li>
<li>results: 文章通过实验研究证明了移动AIGC驱动的HDT解决方案的效果，并在虚拟物理治疗教学平台中应用了这种解决方案。<details>
<summary>Abstract</summary>
Mobile Artificial Intelligence-Generated Content (AIGC) technology refers to the adoption of AI algorithms deployed at mobile edge networks to automate the information creation process while fulfilling the requirements of end users. Mobile AIGC has recently attracted phenomenal attentions and can be a key enabling technology for an emerging application, called human digital twin (HDT). HDT empowered by the mobile AIGC is expected to revolutionize the personalized healthcare by generating rare disease data, modeling high-fidelity digital twin, building versatile testbeds, and providing 24/7 customized medical services. To promote the development of this new breed of paradigm, in this article, we propose a system architecture of mobile AIGC-driven HDT and highlight the corresponding design requirements and challenges. Moreover, we illustrate two use cases, i.e., mobile AIGC-driven HDT in customized surgery planning and personalized medication. In addition, we conduct an experimental study to prove the effectiveness of the proposed mobile AIGC-driven HDT solution, which shows a particular application in a virtual physical therapy teaching platform. Finally, we conclude this article by briefly discussing several open issues and future directions.
</details>
<details>
<summary>摘要</summary>
Mobile artificial intelligence生成内容（AIGC）技术指的是通过移动边缘网络部署人工智能算法自动生成信息的过程，同时满足用户的需求。 Mobile AIGC在最近吸引了非常大的关注，可以是人类数字双胞虫（HDT）的关键技术。 HDT通过移动AIGC得到强化，预计将在个性化医疗方面产生极高精度的数字双胞虫，生成罕见疾病数据，建立多样化测试平台，提供24/7个性化医疗服务。在这篇文章中，我们提议了移动AIGC驱动HDT的系统架构，并 highlight了相关的设计要求和挑战。此外，我们还介绍了两个使用场景：移动AIGC驱动HDT在自定义手术规划中和个性化药物。此外，我们进行了实验研究，证明了我们提议的移动AIGC驱动HDT解决方案的有效性，该解决方案在虚拟物理治疗教学平台中得到应用。最后，我们 briefly discussed several open issues and future directions。
</details></li>
</ul>
<hr>
<h2 id="A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks"><a href="#A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks" class="headerlink" title="A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks"></a>A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12114">http://arxiv.org/abs/2307.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanis Labrak, Mickael Rouvier, Richard Dufour</li>
<li>for: 这些论文旨在评估四种现状最佳大型自然语言处理（NLP）模型（ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca）在13种真实世界医疗和生物医学NLP任务中的性能，包括命名实体识别（NER）、问答（QA）、关系提取（RE）等。</li>
<li>methods: 这些论文使用了四种现状最佳大型NLP模型，并在英文语言上进行了13种真实世界医疗和生物医学NLP任务的测试和评估。</li>
<li>results: 研究结果表明，评估的LLMs在零和几个预测场景中对大多数任务的性能几乎相当于状态之前的模型，特别是在问答任务上表现非常出色，即使它们没有看到这些任务的示例。然而，对于分类和RE任务，模型的性能比专门为医疗领域训练的模型，如PubMedBERT，下降。此外，研究发现没有LLM可以在所有任务中占据领先地位，各模型在某些任务上表现更好。<details>
<summary>Abstract</summary>
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
</details>
<details>
<summary>摘要</summary>
我们评估了四种当前最佳的 instruciton-tuned大语言模型（LLMs）——ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca——在英文的13种实际医疗和生物医学自然语言处理（NLP）任务上，如命名实体识别（NER）、问答（QA）、关系提取（RE）等。我们的总结结果表明，评估的LLMs在零和几个预测场景中的性能接近了当前最佳模型的水平，特别是在QA任务上表现出色，即使它们从来没有看到这些任务的示例。然而，我们发现，分类和RE任务的性能下降到了专门为医疗领域训练的模型，如PubMedBERT，所能达到的水平。最后，我们注意到，没有任何LLM在所有研究任务上表现出优于其他模型，一些模型更适合某些任务。
</details></li>
</ul>
<hr>
<h2 id="CFR-p-Counterfactual-Regret-Minimization-with-Hierarchical-Policy-Abstraction-and-its-Application-to-Two-player-Mahjong"><a href="#CFR-p-Counterfactual-Regret-Minimization-with-Hierarchical-Policy-Abstraction-and-its-Application-to-Two-player-Mahjong" class="headerlink" title="CFR-p: Counterfactual Regret Minimization with Hierarchical Policy Abstraction, and its Application to Two-player Mahjong"></a>CFR-p: Counterfactual Regret Minimization with Hierarchical Policy Abstraction, and its Application to Two-player Mahjong</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12087">http://arxiv.org/abs/2307.12087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiheng Wang</li>
<li>for: 这篇论文是为了应用Counterfactual Regret Minimization(CFR)算法到另一款具有多种变体的 incomplete information 游戏——麻将。</li>
<li>methods: 论文使用了game theoretical analysis和层次抽象来改进CFR算法，以适应麻将游戏的复杂性。</li>
<li>results: 研究发现，这种基于赢家策略的CFR框架可以普适应其他不完整信息游戏。<details>
<summary>Abstract</summary>
Counterfactual Regret Minimization(CFR) has shown its success in Texas Hold'em poker. We apply this algorithm to another popular incomplete information game, Mahjong. Compared to the poker game, Mahjong is much more complex with many variants. We study two-player Mahjong by conducting game theoretical analysis and making a hierarchical abstraction to CFR based on winning policies. This framework can be generalized to other imperfect information games.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>Counterfactual Regret Minimization（CFR）在德州扑克游戏中表现出色。我们将这个算法应用到另一款流行的不完全信息游戏——麻将。与扑克游戏相比，麻将更加复杂，有多种变体。我们通过游戏理论分析和基于赢家策略层次化CFR的框架来研究两人麻将。这个框架可以推广到其他不完全信息游戏。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Temporal-Planning-Domains-by-Sequential-Macro-actions-Extended-Version"><a href="#Enhancing-Temporal-Planning-Domains-by-Sequential-Macro-actions-Extended-Version" class="headerlink" title="Enhancing Temporal Planning Domains by Sequential Macro-actions (Extended Version)"></a>Enhancing Temporal Planning Domains by Sequential Macro-actions (Extended Version)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12081">http://arxiv.org/abs/2307.12081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco De Bortoli, Lukáš Chrpa, Martin Gebser, Gerald Steinbauer-Wagner</li>
<li>for: 提高多代理人和资源共享的具有时间约束的计划效率。</li>
<li>methods: 使用带有恒常性的维度和扩展的约束来实现Sequential Temporal Macro-Actions，保证计划的可行性。</li>
<li>results: 在多个计划器和领域中实现提高了获得优化计划和计划质量。<details>
<summary>Abstract</summary>
Temporal planning is an extension of classical planning involving concurrent execution of actions and alignment with temporal constraints. Durative actions along with invariants allow for modeling domains in which multiple agents operate in parallel on shared resources. Hence, it is often important to avoid resource conflicts, where temporal constraints establish the consistency of concurrent actions and events. Unfortunately, the performance of temporal planning engines tends to sharply deteriorate when the number of agents and objects in a domain gets large. A possible remedy is to use macro-actions that are well-studied in the context of classical planning. In temporal planning settings, however, introducing macro-actions is significantly more challenging when the concurrent execution of actions and shared use of resources, provided the compliance to temporal constraints, should not be suppressed entirely. Our work contributes a general concept of sequential temporal macro-actions that guarantees the applicability of obtained plans, i.e., the sequence of original actions encapsulated by a macro-action is always executable. We apply our approach to several temporal planners and domains, stemming from the International Planning Competition and RoboCup Logistics League. Our experiments yield improvements in terms of obtained satisficing plans as well as plan quality for the majority of tested planners and domains.
</details>
<details>
<summary>摘要</summary>
temporal 规划是 classical 规划的扩展，具有同时执行动作和时间约束的整合。持续动作和 invariants 允许在多个代理人在共享资源上并发操作的Domain 模型。因此，通常需要避免资源冲突，而时间约束可以确定同时执行的动作和事件的一致性。然而， temporal 规划引擎的性能通常随多个代理人和对象的数量增加而逐渐下降。为了解决这个问题，我们可以使用 macro-actions，它们在 классиical 规划中非常成熟。然而，在 temporal 规划设置下，引入 macro-actions 是更加挑战，因为需要保持同时执行动作和共享资源的一致性，而不能完全终止。我们的工作提出了一种通用的顺序 temporal macro-actions 概念，该概念保证原始动作序列被封装在 macro-action 中执行的情况下，得到的计划是可靠的。我们对多个 temporal 规划器和领域进行了应用，其中包括国际规划竞赛和 RoboCup 物流联盟。我们的实验表明，我们的方法可以提高大多数测试的规划器和领域中的得到的满意计划以及计划质量。
</details></li>
</ul>
<hr>
<h2 id="Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations"><a href="#Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations" class="headerlink" title="Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations"></a>Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12062">http://arxiv.org/abs/2307.12062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Xiangyu Liu, Tuomas Sandholm, Furong Huang, Stephen McAleer</li>
<li>for: 本研究旨在训练可以在环境干扰或敌意攻击下表现良好的RL策略。</li>
<li>methods: 我们提出了GRAD方法，它将把时间相关的干扰问题看作是一个部分可见两个玩家零 SUM 游戏，通过找到这个游戏的approximate平衡，确保agent对时间相关的干扰具有强大的Robustness。</li>
<li>results: 我们在一系列连续控制任务上进行了实验，结果表明，相比基eline，我们的提议方法在state和action空间中都具有显著的Robustness优势，特别是在面对时间相关的干扰攻击时。<details>
<summary>Abstract</summary>
Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.
</details>
<details>
<summary>摘要</summary>
STRONG REINFORCEMENT LEARNING (RL) aims to train policies that can perform well under environmental perturbations or adversarial attacks. Existing methods typically assume that the space of possible perturbations remains the same across timesteps. However, in many situations, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a new challenge for existing robust RL methods. To address this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks show that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units"><a href="#Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units" class="headerlink" title="Fast Knowledge Graph Completion using Graphics Processing Units"></a>Fast Knowledge Graph Completion using Graphics Processing Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12059">http://arxiv.org/abs/2307.12059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun-Hee Lee, Dong-oh Kang, Hwa Jeon Song</li>
<li>for: 本研究旨在提供一种高效的知识图谱完成框架，用于在 GPU 上获得新关系。</li>
<li>methods: 本研究使用知识图谱嵌入模型来实现知识图谱完成。首先，我们定义 “可转换为度量空间”，然后将知识图谱完成问题转换成度量空间中的相似Join问题。然后，我们利用度量空间的性质 deriv 出公式，并基于这些公式开发了一个快速的知识图谱完成算法。</li>
<li>results: 我们的研究表明，我们的框架可以高效地处理知识图谱完成问题。<details>
<summary>Abstract</summary>
Knowledge graphs can be used in many areas related to data semantics such as question-answering systems, knowledge based systems. However, the currently constructed knowledge graphs need to be complemented for better knowledge in terms of relations. It is called knowledge graph completion. To add new relations to the existing knowledge graph by using knowledge graph embedding models, we have to evaluate $N\times N \times R$ vector operations, where $N$ is the number of entities and $R$ is the number of relation types. It is very costly.   In this paper, we provide an efficient knowledge graph completion framework on GPUs to get new relations using knowledge graph embedding vectors. In the proposed framework, we first define "transformable to a metric space" and then provide a method to transform the knowledge graph completion problem into the similarity join problem for a model which is "transformable to a metric space". After that, to efficiently process the similarity join problem, we derive formulas using the properties of a metric space. Based on the formulas, we develop a fast knowledge graph completion algorithm. Finally, we experimentally show that our framework can efficiently process the knowledge graph completion problem.
</details>
<details>
<summary>摘要</summary>
知识图可以应用于数据semantics中的多个领域，如问答系统、知识基础系统。然而，现有的知识图需要补充以提高知识的关系。这被称为知识图完成。为了通过使用知识图嵌入模型添加新的关系到现有的知识图，我们需要评估 $N\times N \times R$ 矢量操作，其中 $N$ 是实体的数量，$R$ 是关系类型的数量。这非常昂贵。在这篇论文中，我们提供了一个高效的知识图完成框架在GPU上进行新关系获取。我们首先定义"可转换到一个度量空间"，然后将知识图完成问题转换成一个相似Join问题，该问题可以"可转换到一个度量空间"。然后，我们 derivate formulas使用度量空间的性质，然后我们开发了一个快速的知识图完成算法。最后，我们实验表明，我们的框架可以高效地处理知识图完成问题。
</details></li>
</ul>
<hr>
<h2 id="Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning"><a href="#Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning" class="headerlink" title="Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning"></a>Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12996">http://arxiv.org/abs/2307.12996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Lacombe, Andrew Gaut, Jeff He, David Lüdeke, Kateryna Pistunova</li>
<li>for: 本研究旨在将科学知识从文本中提取到分子图表示，以 Bridge 深度学习在计算生物化学中的图表示和文本描述之间的 gap。</li>
<li>methods: 本研究使用对比学习将神经图表示与文本描述的特征进行对应，并使用神经相关性分数策略提高文本检索。此外，我们还提出了一种基于有机反应的新的分子图数据增强策略。</li>
<li>results: 我们的模型在下游 MoleculeNet 性质分类任务上表现出色，与模型只使用图模式alone (+4.26% AUROC提升) 和 MoMu 模型（Su et al. 2022） (+1.54% 提升) 相比，均显著提高了性能。<details>
<summary>Abstract</summary>
Deep learning in computational biochemistry has traditionally focused on molecular graphs neural representations; however, recent advances in language models highlight how much scientific knowledge is encoded in text. To bridge these two modalities, we investigate how molecular property information can be transferred from natural language to graph representations. We study property prediction performance gains after using contrastive learning to align neural graph representations with representations of textual descriptions of their characteristics. We implement neural relevance scoring strategies to improve text retrieval, introduce a novel chemically-valid molecular graph augmentation strategy inspired by organic reactions, and demonstrate improved performance on downstream MoleculeNet property classification tasks. We achieve a +4.26% AUROC gain versus models pre-trained on the graph modality alone, and a +1.54% gain compared to recently proposed molecular graph/text contrastively trained MoMu model (Su et al. 2022).
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)深度学习在计算生物化学中传统上专注于分子图 neural representation; 然而，最近的语言模型发展提出了如何在文本中储存科学知识的问题。为了联系这两种模式，我们研究如何从自然语言中提取分子性质信息并将其转换为图表示。我们使用对比学习将神经图表示与文本描述中的特征表示进行对应。我们还使用神经相关分数策略来改进文本检索，并提出了一种基于有机反应的化学正确分子图增强策略。我们在下游MoleculeNet属性分类任务上达到了+4.26% AUROC提升和+1.54%提升，相比于只使用图模式预训练的模型。
</details></li>
</ul>
<hr>
<h2 id="How-to-Design-and-Deliver-Courses-for-Higher-Education-in-the-AI-Era-Insights-from-Exam-Data-Analysis"><a href="#How-to-Design-and-Deliver-Courses-for-Higher-Education-in-the-AI-Era-Insights-from-Exam-Data-Analysis" class="headerlink" title="How to Design and Deliver Courses for Higher Education in the AI Era: Insights from Exam Data Analysis"></a>How to Design and Deliver Courses for Higher Education in the AI Era: Insights from Exam Data Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02441">http://arxiv.org/abs/2308.02441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Samer Wazan, Imran Taj, Abdulhadi Shoufan, Romain Laborde, Rémi Venant</li>
<li>For: The paper advocates for the idea that courses and exams in the AI era should be designed based on the strengths and limitations of AI, as well as pedagogical educational objectives.* Methods: The paper explores the strengths and limitations of AI based on current advances in the field, and provides examples of how courses and exams can be designed based on these factors. The paper also describes a pedagogical approach inspired by the Socratic teaching method that was adopted from January 2023 to May 2023.* Results: The paper presents data analysis results of seven ChatGPT-authorized exams conducted between December 2022 and March 2023, which show no correlation between students’ grades and whether or not they use ChatGPT to answer their exam questions. The paper also proposes a new exam system that allows for the application of the pedagogical approach in the AI era.Here is the information in Simplified Chinese text:* For: 这篇论文提出了在人工智能时代，课程和考试应该如何设计，以便符合人工智能的优势和局限性，以及教育目标。* Methods: 论文从现有人工智能技术的发展来探讨人工智能的优势和局限性，并提供了不同领域的示例，如IT、英语和艺术等。论文还描述了一种基于索普朗教学方法的教学方法，从2023年1月至2023年5月进行了应用。* Results: 论文提供了七个使用ChatGPT作为考试工具的考试数据分析结果，显示学生的成绩与使用ChatGPT answering考试问题无关。论文还提出了一种新的考试系统，以便在人工智能时代应用教学方法。<details>
<summary>Abstract</summary>
In this position paper, we advocate for the idea that courses and exams in the AI era have to be designed based on two factors: (1) the strengths and limitations of AI, and (2) the pedagogical educational objectives. Based on insights from the Delors report on education [1], we first address the role of education and recall the main objectives that educational institutes must strive to achieve independently of any technology. We then explore the strengths and limitations of AI, based on current advances in AI. We explain how courses and exams can be designed based on these strengths and limitations of AI, providing different examples in the IT, English, and Art domains. We show how we adopted a pedagogical approach that is inspired from the Socratic teaching method from January 2023 to May 2023. Then, we present the data analysis results of seven ChatGPT-authorized exams conducted between December 2022 and March 2023. Our exam data results show that there is no correlation between students' grades and whether or not they use ChatGPT to answer their exam questions. Finally, we present a new exam system that allows us to apply our pedagogical approach in the AI era.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "AI era" is translated as "人工智能时代" (rénxīng zhìnéng shídài)* "pedagogical educational objectives" is translated as "教育目标" (jiàoyù mùbiāo)* "Delors report" is translated as "德洛尔报告" (déluō'ěr bàogāo)* "Socratic teaching method" is translated as "苏格拉底教学方法" (sūgélādī jíxué fāngfa)* "ChatGPT-authorized exams" is translated as "ChatGPT授权考试" (ChatGPT shèngquán kǎoshì)* "data analysis results" is translated as "数据分析结果" (numbers dàxīn yìjī)Note: The translation is in Simplified Chinese, as requested.
</details></li>
</ul>
<hr>
<h2 id="Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors"><a href="#Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors" class="headerlink" title="Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors"></a>Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12015">http://arxiv.org/abs/2307.12015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Maria Aiello, Mehrad Jaloli, Marzia Cescon</li>
<li>for: 这个论文旨在开发一种基于Linear Time-Varying（LTV）Model Predictive Control（MPC）框架的闭环式胰岛素输液控制算法，用于治疗型1 диабеtes（T1D）。</li>
<li>methods: 这个算法使用了一个数据驱动的多步预测器，并将预测结果用于LTV MPC控制器中。在非线性部分，我们使用了一个Long Short-Term Memory（LSTM）网络，而在线性部分，我们使用了一个线性回归模型。</li>
<li>results: 我们对这两种控制器进行了Simulation比较，并发现我们的LSTM-MPC控制器在三个场景中表现更好，即在常规情况下、随机饭物干扰情况下和降低胰岛素敏感性25%情况下。此外，我们的方法可以更好地预测未来血糖浓度，并且closed-loop性能更好。<details>
<summary>Abstract</summary>
We present the design and \textit{in-silico} evaluation of a closed-loop insulin delivery algorithm to treat type 1 diabetes (T1D) consisting in a data-driven multi-step-ahead blood glucose (BG) predictor integrated into a Linear Time-Varying (LTV) Model Predictive Control (MPC) framework. Instead of identifying an open-loop model of the glucoregulatory system from available data, we propose to directly fit the entire BG prediction over a predefined prediction horizon to be used in the MPC, as a nonlinear function of past input-ouput data and an affine function of future insulin control inputs. For the nonlinear part, a Long Short-Term Memory (LSTM) network is proposed, while for the affine component a linear regression model is chosen. To assess benefits and drawbacks when compared to a traditional linear MPC based on an auto-regressive with exogenous (ARX) input model identified from data, we evaluated the proposed LSTM-MPC controller in three simulation scenarios: a nominal case with 3 meals per day, a random meal disturbances case where meals were generated with a recently published meal generator, and a case with 25$\%$ decrease in the insulin sensitivity. Further, in all the scenarios, no feedforward meal bolus was administered. For the more challenging random meal generation scenario, the mean $\pm$ standard deviation percent time in the range 70-180 [mg/dL] was 74.99 $\pm$ 7.09 vs. 54.15 $\pm$ 14.89, the mean $\pm$ standard deviation percent time in the tighter range 70-140 [mg/dL] was 47.78$\pm$8.55 vs. 34.62 $\pm$9.04, while the mean $\pm$ standard deviation percent time in sever hypoglycemia, i.e., $<$ 54 [mg/dl] was 1.00$\pm$3.18 vs. 9.45$\pm$11.71, for our proposed LSTM-MPC controller and the traditional ARX-MPC, respectively. Our approach provided accurate predictions of future glucose concentrations and good closed-loop performances of the overall MPC controller.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种closed-loop胰岛素输液算法，用于治疗型1 диабеtes（T1D），这种算法包括一个数据驱动的多步预测血糖（BG）预测器，integrated into a Linear Time-Varying（LTV） Model Predictive Control（MPC）框架。而不是从可用数据中直接Identify opens loop模型的静脉糖皮肤系统，我们提议直接预测整个BG预测 horizon，作为一个非线性函数，用于MPC中的预测。 For the nonlinear part, a Long Short-Term Memory（LSTM） network is proposed, while for the affine component a linear regression model is chosen. To evaluate the benefits and drawbacks of the proposed LSTM-MPC controller compared to a traditional linear MPC based on an auto-regressive with exogenous（ARX）input model identified from data, we evaluated the proposed LSTM-MPC controller in three simulation scenarios: a nominal case with 3 meals per day, a random meal disturbances case where meals were generated with a recently published meal generator, and a case with 25% decrease in the insulin sensitivity. Further, in all the scenarios, no feedforward meal bolus was administered. For the more challenging random meal generation scenario, the mean ± standard deviation percent time in the range 70-180 [mg/dL] was 74.99 ± 7.09 vs. 54.15 ± 14.89, the mean ± standard deviation percent time in the tighter range 70-140 [mg/dL] was 47.78 ± 8.55 vs. 34.62 ± 9.04, while the mean ± standard deviation percent time in severe hypoglycemia, i.e., <54 [mg/dL] was 1.00 ± 3.18 vs. 9.45 ± 11.71, for our proposed LSTM-MPC controller and the traditional ARX-MPC, respectively. Our approach provided accurate predictions of future glucose concentrations and good closed-loop performances of the overall MPC controller.
</details></li>
</ul>
<hr>
<h2 id="Psy-LLM-Scaling-up-Global-Mental-Health-Psychological-Services-with-AI-based-Large-Language-Models"><a href="#Psy-LLM-Scaling-up-Global-Mental-Health-Psychological-Services-with-AI-based-Large-Language-Models" class="headerlink" title="Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models"></a>Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11991">http://arxiv.org/abs/2307.11991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tin Lai, Yukun Shi, Zicong Du, Jiajie Wu, Ken Fu, Yichao Dou, Ziqi Wang<br>for:The paper aims to provide a novel AI-based system for online psychological consultation, which can assist healthcare professionals in providing timely and professional mental health support.methods:The proposed framework, called Psy-LLM, leverages Large Language Models (LLMs) for question-answering in online psychological consultation. The framework combines pre-trained LLMs with real-world professional Q&amp;A from psychologists and extensively crawled psychological articles.results:The authors evaluated the framework using intrinsic metrics such as perplexity and extrinsic evaluation metrics including human participant assessments of response helpfulness, fluency, relevance, and logic. The results demonstrate the effectiveness of the Psy-LLM framework in generating coherent and relevant answers to psychological questions.<details>
<summary>Abstract</summary>
The demand for psychological counseling has grown significantly in recent years, particularly with the global outbreak of COVID-19, which has heightened the need for timely and professional mental health support. Online psychological counseling has emerged as the predominant mode of providing services in response to this demand. In this study, we propose the Psy-LLM framework, an AI-based system leveraging Large Language Models (LLMs) for question-answering in online psychological consultation. Our framework combines pre-trained LLMs with real-world professional Q&A from psychologists and extensively crawled psychological articles. The Psy-LLM framework serves as a front-end tool for healthcare professionals, allowing them to provide immediate responses and mindfulness activities to alleviate patient stress. Additionally, it functions as a screening tool to identify urgent cases requiring further assistance. We evaluated the framework using intrinsic metrics, such as perplexity, and extrinsic evaluation metrics, with human participant assessments of response helpfulness, fluency, relevance, and logic. The results demonstrate the effectiveness of the Psy-LLM framework in generating coherent and relevant answers to psychological questions. This article concludes by discussing the potential of large language models to enhance mental health support through AI technologies in online psychological consultation.
</details>
<details>
<summary>摘要</summary>
“对于心理辅导的需求在最近的几年中有了很大的增长，特别是COVID-19全球大流行，这使得心理健康支持的需求增加了。在这篇研究中，我们提出了Psy-LLM框架，这是一个基于大语言模型（LLM）的人工智能系统，用于在线心理咨询中回答问题。我们的框架结合了预训语言模型和专业心理师的问答，以及大量爬虫的心理文章。Psy-LLM框架作为健康专业人员的前端工具，可以提供即时的回答和心理活动，以减轻病人的压力。同时，它还可以作为寻找紧急案例需要进一步帮助的萤幕工具。我们使用了自类度、流畅度、相关度和逻辑性等内部评估指标，以及人类参与者的评价，来评估Psy-LLM框架的效果。结果显示，Psy-LLM框架可以生成 coherent 和相关的回答心理问题。本文结束时，讨论了大语言模型在线心理咨询中如何通过人工智能技术增强心理健康支持。”
</details></li>
</ul>
<hr>
<h2 id="Sparse-then-Prune-Toward-Efficient-Vision-Transformers"><a href="#Sparse-then-Prune-Toward-Efficient-Vision-Transformers" class="headerlink" title="Sparse then Prune: Toward Efficient Vision Transformers"></a>Sparse then Prune: Toward Efficient Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11988">http://arxiv.org/abs/2307.11988</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yogiprsty/sparse-vit">https://github.com/yogiprsty/sparse-vit</a></li>
<li>paper_authors: Yogi Prasetyo, Novanto Yudistira, Agus Wahyu Widodo</li>
<li>for: 这个研究旨在investigate the possibility of applying Sparse Regularization and Pruning methods to the Vision Transformer architecture for image classification tasks, and explore the trade-off between performance and efficiency.</li>
<li>methods: 这个研究使用了Sparse Regularization和Pruning方法，并在CIFAR-10、CIFAR-100和ImageNet-100 datasets上进行了实验。模型的训练过程包括两部分：预训练和精度调整。预训练使用了ImageNet21K数据，followed by 20 epochs of fine-tuning.</li>
<li>results: 研究发现，当使用CIFAR-100和ImageNet-100数据进行测试时，带有Sparse Regularization的模型可以提高准确率by 0.12%。此外，对带有Sparse Regularization的模型进行截割，可以更好地提高平均准确率。特别是在CIFAR-10数据集上，截割后的模型可以提高准确率by 0.568%，在CIFAR-100和ImageNet-100数据集上提高了1.764%和0.256%。<details>
<summary>Abstract</summary>
The Vision Transformer architecture is a deep learning model inspired by the success of the Transformer model in Natural Language Processing. However, the self-attention mechanism, large number of parameters, and the requirement for a substantial amount of training data still make Vision Transformers computationally burdensome. In this research, we investigate the possibility of applying Sparse Regularization to Vision Transformers and the impact of Pruning, either after Sparse Regularization or without it, on the trade-off between performance and efficiency. To accomplish this, we apply Sparse Regularization and Pruning methods to the Vision Transformer architecture for image classification tasks on the CIFAR-10, CIFAR-100, and ImageNet-100 datasets. The training process for the Vision Transformer model consists of two parts: pre-training and fine-tuning. Pre-training utilizes ImageNet21K data, followed by fine-tuning for 20 epochs. The results show that when testing with CIFAR-100 and ImageNet-100 data, models with Sparse Regularization can increase accuracy by 0.12%. Furthermore, applying pruning to models with Sparse Regularization yields even better results. Specifically, it increases the average accuracy by 0.568% on CIFAR-10 data, 1.764% on CIFAR-100, and 0.256% on ImageNet-100 data compared to pruning models without Sparse Regularization. Code can be accesed here: https://github.com/yogiprsty/Sparse-ViT
</details>
<details>
<summary>摘要</summary>
“当前的视觉 трансформер架构是一种深度学习模型，受到自然语言处理中的Transformer模型的成功所 inspirited。然而，自我对项 mechanism，大量的参数，以及需要大量的训练数据仍然使得视觉 трансформер Computationally burdensome。在这个研究中，我们 investigate了将Sparse Regularization应用到视觉 трансформер架构中，以及对其进行Prune的影响，以进行性能和效率之间的交易。为此，我们将Sparse Regularization和Prune方法应用到视觉 трансформер架构，进行图像分类任务。训练过程包括两个部分：预训练和精练。预训练使用ImageNet21K数据，接着进行20次精练。结果显示，在CIFAR-100和ImageNet-100数据上进行训练时，具有Sparse Regularization的模型可以提高精确率0.12%。此外，对Sparse Regularization的模型进行Prune操作，产生了更好的结果。具体来说，它可以在CIFAR-10数据上提高平均精确率0.568%，CIFAR-100数据上提高1.764%，ImageNet-100数据上提高0.256%。软件可以在以下github上取得：https://github.com/yogiprsty/Sparse-ViT”
</details></li>
</ul>
<hr>
<h2 id="Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels"><a href="#Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels" class="headerlink" title="Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?"></a>Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11978">http://arxiv.org/abs/2307.11978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cewu/ptnl">https://github.com/cewu/ptnl</a></li>
<li>paper_authors: Cheng-En Wu, Yu Tian, Haichao Yu, Heng Wang, Pedro Morgado, Yu Hen Hu, Linjie Yang</li>
<li>for: 这个论文主要研究了CLIP视觉语言模型如何在几行示例下适应新的分类任务，以及这种示例调整过程对噪声标签的Robustness。</li>
<li>methods: 该论文使用了CLIP视觉语言模型，通过几行示例进行示例调整，并进行了广泛的实验研究以探索这种示例调整过程中的关键因素。</li>
<li>results: 研究发现， CLIP的示例调整过程具有很高的Robustness，这主要归因于模型中的固定类名token提供了强制的Regularization，以及CLIP学习的强大预训练图像文本嵌入，帮助提高图像分类的预测精度。<details>
<summary>Abstract</summary>
Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CEWu/PTNL.
</details>
<details>
<summary>摘要</summary>
视力语模型如CLIP通过大规模训练学习通用文本图像嵌入。一个视力语模型可以通过几招提示调整来适应新的分类任务。我们发现这种提示调整过程具有很高的鲁棒性，使我们感到惊叹。我们进行了广泛的实验研究这种性能的原因，并发现关键因素有：1）固定的类名token提供了模型优化的强制 régularization，减少了噪声样本引起的梯度; 2）通过多样化和通用的网络数据学习的强大预训练图像文本嵌入，为图像分类提供了强大的先验知识。此外，我们示出了CLIP的噪声零时预测可以用来调整其自己的提示，significantly enhance预测精度在无监督Setting下。代码可以在https://github.com/CEWu/PTNL中找到。
</details></li>
</ul>
<hr>
<h2 id="Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection"><a href="#Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection" class="headerlink" title="Multi-representations Space Separation based Graph-level Anomaly-aware Detection"></a>Multi-representations Space Separation based Graph-level Anomaly-aware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12994">http://arxiv.org/abs/2307.12994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fu Lin, Haonan Gong, Mingkang Li, Zitong Wang, Yue Zhang, Xuexiong Luo</li>
<li>for: 本研究的目标是检测图数据中异常的图形。</li>
<li>methods: 我们提出了一种基于多表示空间分离的图级异常检测框架，以考虑不同类型的异常图形之间的重要性。我们还设计了一个异常检测模块，以learn异常图形之间的特定权重。</li>
<li>results: 我们对基eline方法进行了广泛的评估，并获得了显著的效果。<details>
<summary>Abstract</summary>
Graph structure patterns are widely used to model different area data recently. How to detect anomalous graph information on these graph data has become a popular research problem. The objective of this research is centered on the particular issue that how to detect abnormal graphs within a graph set. The previous works have observed that abnormal graphs mainly show node-level and graph-level anomalies, but these methods equally treat two anomaly forms above in the evaluation of abnormal graphs, which is contrary to the fact that different types of abnormal graph data have different degrees in terms of node-level and graph-level anomalies. Furthermore, abnormal graphs that have subtle differences from normal graphs are easily escaped detection by the existing methods. Thus, we propose a multi-representations space separation based graph-level anomaly-aware detection framework in this paper. To consider the different importance of node-level and graph-level anomalies, we design an anomaly-aware module to learn the specific weight between them in the abnormal graph evaluation process. In addition, we learn strictly separate normal and abnormal graph representation spaces by four types of weighted graph representations against each other including anchor normal graphs, anchor abnormal graphs, training normal graphs, and training abnormal graphs. Based on the distance error between the graph representations of the test graph and both normal and abnormal graph representation spaces, we can accurately determine whether the test graph is anomalous. Our approach has been extensively evaluated against baseline methods using ten public graph datasets, and the results demonstrate its effectiveness.
</details>
<details>
<summary>摘要</summary>
graph结构模式在当今数据中广泛应用。如何检测图数据中的异常信息已成为一个流行的研究问题。本研究的目标在于特定的问题：如何在图集中检测异常图。前一些研究发现，异常图主要表现为节点水平和图水平异常，但这些方法在评估异常图时平等对待这两种异常形态，这与实际情况不符。此外，异常图具有微妙的差异，容易被现有方法检测掉。因此，我们提出了一个基于多个表示空间分离的图级异常检测框架。为了考虑节点水平和图水平异常的不同重要性，我们设计了一个异常检测模块，以学习特定的节点水平和图水平异常权重。此外，我们通过四种不同的权重图表示对彼此进行学习，以学习纯正的常见图和异常图表示空间。通过测试图表示空间与常见图表示空间和异常图表示空间之间的距离错误来准确判断测试图是否异常。我们的方法与基准方法进行比较使用了十个公共图数据集，结果表明其效果批示。
</details></li>
</ul>
<hr>
<h2 id="Pyrus-Base-An-Open-Source-Python-Framework-for-the-RoboCup-2D-Soccer-Simulation"><a href="#Pyrus-Base-An-Open-Source-Python-Framework-for-the-RoboCup-2D-Soccer-Simulation" class="headerlink" title="Pyrus Base: An Open Source Python Framework for the RoboCup 2D Soccer Simulation"></a>Pyrus Base: An Open Source Python Framework for the RoboCup 2D Soccer Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16875">http://arxiv.org/abs/2307.16875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyrus2d/pyrus2d">https://github.com/cyrus2d/pyrus2d</a></li>
<li>paper_authors: Nader Zare, Aref Sayareh, Omid Amini, Mahtab Sarvmaili, Arad Firouzkouhi, Stan Matwin, Amilcar Soares</li>
<li>For: The paper is written to introduce Pyrus, a Python base code for the RoboCup Soccer Simulation 2D (SS2D) league, to provide a more accessible and efficient platform for researchers to develop their ideas and integrate machine learning algorithms into their teams.* Methods: The paper uses C++ base codes as the foundation and develops Pyrus, a Python base code, to overcome the challenges of C++ base codes and provide a more user-friendly platform for researchers.* Results: Pyrus is introduced as a powerful baseline for developing machine learning concepts in SS2D, and it is open-source and publicly available under MIT License on GitHub, encouraging researchers to efficiently develop their ideas and integrate machine learning algorithms into their teams.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了介绍PYRUS，一个基于Python的RoboCup足球模拟2D（SS2D）联赛的基础代码，以便更好地为研究人员提供一个访问ibility和效率的平台，以便他们可以更加快速地开发自己的想法并将机器学习算法integrated into their teams。* Methods: 这篇论文使用C++基础代码作为基础，然后开发了PYRUS，一个基于Python的基础代码，以解决C++基础代码的挑战，并提供一个更加用户友好的平台 для研究人员。* Results: PYRUS被引入为SS2D中的一个强大基线，可以帮助研究人员更加快速地开发自己的想法并integrated machine learning算法into their teams，PYRUS的基础代码公开发布在GitHub上，并且以MIT许可证进行公共可用，以便更多的研究人员可以参与到这个项目中。<details>
<summary>Abstract</summary>
Soccer, also known as football in some parts of the world, involves two teams of eleven players whose objective is to score more goals than the opposing team. To simulate this game and attract scientists from all over the world to conduct research and participate in an annual computer-based soccer world cup, Soccer Simulation 2D (SS2D) was one of the leagues initiated in the RoboCup competition. In every SS2D game, two teams of 11 players and one coach connect to the RoboCup Soccer Simulation Server and compete against each other. Over the past few years, several C++ base codes have been employed to control agents' behavior and their communication with the server. Although C++ base codes have laid the foundation for the SS2D, developing them requires an advanced level of C++ programming. C++ language complexity is a limiting disadvantage of C++ base codes for all users, especially for beginners. To conquer the challenges of C++ base codes and provide a powerful baseline for developing machine learning concepts, we introduce Pyrus, the first Python base code for SS2D. Pyrus is developed to encourage researchers to efficiently develop their ideas and integrate machine learning algorithms into their teams. Pyrus base is open-source code, and it is publicly available under MIT License on GitHub
</details>
<details>
<summary>摘要</summary>
足球（也称为足球在一些地方）是一种需要两支队伍的 eleven 名球员，目标是将更多的入球击败对手队伍。为了模拟这场游戏并吸引全球科学家来参与研究和参加每年的计算机基于足球世界杯赛，Football Simulation 2D（SS2D）是RoboCup竞赛中的一个赛事。在每场 SS2D 比赛中，两支队伍的 11 名球员和一位教练通过RoboCup足球 simulate Server 竞争对对手。过去几年，一些 C++ 基础代码被使用来控制代理的行为和与服务器的通信。虽然 C++ 基础代码已经为 SS2D 提供了基础，但是开发它们需要高级的 C++ 编程技能。 C++ 语言复杂性是 C++ 基础代码的限制性，特别是对所有用户来说，尤其是对初学者来说。为了 conquering C++ 基础代码的挑战和提供一个机器学习概念的强大基础，我们引入了 Pyrus，SS2D 的第一个 Python 基础代码。Pyrus 是为了鼓励研究人员尽可能快速地发展他们的想法，并将机器学习算法 integrate 到他们的队伍中。Pyrus 的基础代码是开源的，公开在 GitHub 上，并以 MIT 许可证进行公共发布。
</details></li>
</ul>
<hr>
<h2 id="On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs"><a href="#On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs" class="headerlink" title="On-Robot Bayesian Reinforcement Learning for POMDPs"></a>On-Robot Bayesian Reinforcement Learning for POMDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11954">http://arxiv.org/abs/2307.11954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Nguyen, Sammie Katt, Yuchen Xiao, Christopher Amato</li>
<li>for: 本研究旨在提高机器人学习的效率，因为收集数据的成本很高。</li>
<li>methods: 本paper使用权重学习（BRL）方法，利用专家知识和有效的算法来解决机器人学习的问题。</li>
<li>results: 本paper在两个人机交互任务中实现了近乎最佳性能，只需要几个实际世界话语。视频证明可以在<a target="_blank" rel="noopener" href="https://youtu.be/H9xp60ngOes%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://youtu.be/H9xp60ngOes中找到。</a><details>
<summary>Abstract</summary>
Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach can, for example, utilize typical low-level robot simulators and handle uncertainty over unknown dynamics of the environment. We empirically demonstrate its efficiency by performing on-robot learning in two human-robot interaction tasks with uncertainty about human behavior, achieving near-optimal performance after only a handful of real-world episodes. A video of learned policies is at https://youtu.be/H9xp60ngOes.
</details>
<details>
<summary>摘要</summary>
机器人学习往往困难，主要是因为获取数据的成本高昂。为了解决这个问题，我们需要使用有效的算法和利用机器人动力学专家的知识。泛bayesian学习（BRL）因其样本效率高和能够利用先验知识的特点，成为一种有优势的解决方案。然而，BRL在应用中受到了知识表示和推理问题的限制。这篇论文提出了一种特有的框架，用于解决机器人物理系统中的问题。我们捕捉了专家知识，并证明 posterior 会分解为类似的形式，最后将模型形式化为 bayesian 框架。我们then introduces 一种基于 Monte-Carlo 搜索和粒子筛选的在线解决方法，特化用于解决 resulting 模型。这种方法可以利用典型的低级机器人模拟器，并处理不确定环境中的动力学不确定性。我们实验表明，这种方法可以在两个人机器人互动任务中达到近似优化性，只需要几十个真实世界 episoden。有关学习的视频可以在 https://youtu.be/H9xp60ngOes 中找到。
</details></li>
</ul>
<hr>
<h2 id="Pathology-and-genomics-Multimodal-Transformer-for-Survival-Outcome-Prediction"><a href="#Pathology-and-genomics-Multimodal-Transformer-for-Survival-Outcome-Prediction" class="headerlink" title="Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction"></a>Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11952">http://arxiv.org/abs/2307.11952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cassie07/pathomics">https://github.com/cassie07/pathomics</a></li>
<li>paper_authors: Kexin Ding, Mu Zhou, Dimitris N. Metaxas, Shaoting Zhang</li>
<li>for: 这个研究旨在提高colon和rectum癌 survival outcome预测，通过结合pathology和genomics信息。</li>
<li>methods: 该研究提出了一种多Modal transformer（PathOmics），通过不监督预训练来捕捉组织微环境的内在相互作用，并将这些信息与许多 genomics数据（例如mRNA-sequence、copy number variant和methylation）融合。</li>
<li>results: 研究表明，提出的方法可以在TCGA colon和RECTUM癌组织中表现出优异，并且超越了现有的研究。此外，该方法还可以使用有限的finetunedamples进行数据效率的分析，从而提高预测结果的准确性。<details>
<summary>Abstract</summary>
Survival outcome assessment is challenging and inherently associated with multiple clinical factors (e.g., imaging and genomics biomarkers) in cancer. Enabling multimodal analytics promises to reveal novel predictive patterns of patient outcomes. In this study, we propose a multimodal transformer (PathOmics) integrating pathology and genomics insights into colon-related cancer survival prediction. We emphasize the unsupervised pretraining to capture the intrinsic interaction between tissue microenvironments in gigapixel whole slide images (WSIs) and a wide range of genomics data (e.g., mRNA-sequence, copy number variant, and methylation). After the multimodal knowledge aggregation in pretraining, our task-specific model finetuning could expand the scope of data utility applicable to both multi- and single-modal data (e.g., image- or genomics-only). We evaluate our approach on both TCGA colon and rectum cancer cohorts, showing that the proposed approach is competitive and outperforms state-of-the-art studies. Finally, our approach is desirable to utilize the limited number of finetuned samples towards data-efficient analytics for survival outcome prediction. The code is available at https://github.com/Cassie07/PathOmics.
</details>
<details>
<summary>摘要</summary>
生存结果评估在癌症中是挑战性的，与多种临床因素（例如成像和基因表达 markers）相关。启用多modal分析承诺可以揭示新的预测性模式。在这项研究中，我们提出了一种多modal transformer（PathOmics），将pathology和基因学信息集成到colon相关癌症生存预测中。我们强调了无监督预训来捕捉材料微环境的内在交互。经过多modal知识聚合的预训后，我们的任务特定模型精度调整可以扩大数据的可用范围，包括多modal数据（例如图像或基因数据）以及单modal数据（例如图像或基因数据）。我们在TCGAcolon和rectum癌症群体上评估了我们的方法，并显示了我们的方法与当前最佳实践相比较竞争。最后，我们的方法可以使用有限的精度调整样本来实现数据效率的分析。代码可以在https://github.com/Cassie07/PathOmics中找到。
</details></li>
</ul>
<hr>
<h2 id="HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions"><a href="#HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions" class="headerlink" title="HIQL: Offline Goal-Conditioned RL with Latent States as Actions"></a>HIQL: Offline Goal-Conditioned RL with Latent States as Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11949">http://arxiv.org/abs/2307.11949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seohongpark/hiql">https://github.com/seohongpark/hiql</a></li>
<li>paper_authors: Seohong Park, Dibya Ghosh, Benjamin Eysenbach, Sergey Levine</li>
<li>for: 本研究旨在开发一种基于不监督学习的目标决策策略，能够从大量未标注数据中学习。</li>
<li>methods: 该方法使用一个action-free值函数，并通过层次分解来学习两个策略：一个高级策略用于处理状态作为行为，预测子目标，以及一个低级策略用于达成这个子目标。</li>
<li>results: 该方法可以解决长期任务，并可以在高维图像观察中进行扩展。 Code可以在<a target="_blank" rel="noopener" href="https://seohong.me/projects/hiql/%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://seohong.me/projects/hiql/上下载。</a><details>
<summary>Abstract</summary>
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at https://seohong.me/projects/hiql/
</details>
<details>
<summary>摘要</summary>
“无监督预训”最近已经成为计算机视觉和自然语言处理领域的基础。在征激学习（RL）中，目标conditioned RL可能提供一种自愿supervised的方法，将大量的无回奖数据给利用。然而，建立有效的对目标conditioned RL算法，从多元的过去数据中学习，是一个挑战。这是因为，过去的目标变得越远，预测其价值函数的专业程度就越高。然而，目标 raggiungere问题具有结构，即到达较远的目标需要先通过更近的子目标。这种结构可以非常有用，因为评估靠近目标的动作较 easier than评估更远的目标。基于这个想法，我们提出了一个层次架构的对目标conditioned RL算法。我们使用一个不含动作的价值函数，学习两个政策：一个高层政策，将状态视为动作，预测（一个隐藏表示）子目标，以及一个低层政策，预测将用来达到子目标的动作。我们通过分析和示例，显示了我们的层次分解对于错误估计价值函数的影响。我们然后将我们的方法应用到过去目标 raggiungere测试 benchmark，展示了我们的方法可以解决长期任务，可以扩展到高维度的影像观察，并可以轻松地使用无动作数据。我们的代码可以在https://seohong.me/projects/hiql/ 获取。”
</details></li>
</ul>
<hr>
<h2 id="Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors"><a href="#Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors" class="headerlink" title="Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors"></a>Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11922">http://arxiv.org/abs/2307.11922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim, JB Lanier, Pierre Baldi, Roy Fox, Sameer Singh</li>
<li>for: 这个论文旨在探讨如何使用自然语言处理技术来帮助语言模型在决策过程中更好地处理环境状态信息。</li>
<li>methods: 这篇论文提出了一种名为“布林德”（BLINDER）的方法，它通过学习任务条件下的状态描述值函数来自动选择简洁的状态描述。</li>
<li>results: 实验结果表明，使用布林德方法可以提高任务成功率，降低输入大小和计算成本，并在不同的语言模型actor之间进行泛化。<details>
<summary>Abstract</summary>
Large language models (LLMs) are being applied as actors for sequential decision making tasks in domains such as robotics and games, utilizing their general world knowledge and planning abilities. However, previous work does little to explore what environment state information is provided to LLM actors via language. Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on hand-engineered, task-specific protocols to determine which features to communicate about a state and which to leave out. In this work, we propose Brief Language INputs for DEcision-making Responses (BLINDER), a method for automatically selecting concise state descriptions by learning a value function for task-conditioned state descriptions. We evaluate BLINDER on the challenging video game NetHack and a robotic manipulation task. Our method improves task success rate, reduces input size and compute costs, and generalizes between LLM actors.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型（LLM）在机器人和游戏等领域被应用为序列决策任务的演员，利用其总体世界知识和规划能力。然而，先前的研究几乎没有探讨 LLM 演员所接受的环境状态信息是如何传递给语言中。描述高维状态的详细信息可能会降低性能和提高 LLM 演员的推理成本。先前的 LLM 演员通常通过靠手工设计、任务特定协议来确定要关注哪些状态特征和哪些可以被忽略。在这项工作中，我们提出了 Brief Language INputs for DEcision-making Responses（BLINDER）方法，通过学习任务条件下的状态描述值函数来自动选择简洁的状态描述。我们在 NetHack 游戏和机器人 manipulate 任务上评估 BLINDER。我们的方法可以提高任务成功率，降低输入大小和计算成本，并在不同的 LLM 演员之间进行泛化。
</details></li>
</ul>
<hr>
<h2 id="Bibliometric-Analysis-of-Publisher-and-Journal-Instructions-to-Authors-on-Generative-AI-in-Academic-and-Scientific-Publishing"><a href="#Bibliometric-Analysis-of-Publisher-and-Journal-Instructions-to-Authors-on-Generative-AI-in-Academic-and-Scientific-Publishing" class="headerlink" title="Bibliometric Analysis of Publisher and Journal Instructions to Authors on Generative-AI in Academic and Scientific Publishing"></a>Bibliometric Analysis of Publisher and Journal Instructions to Authors on Generative-AI in Academic and Scientific Publishing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11918">http://arxiv.org/abs/2307.11918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Conner Ganjavi, Michael B. Eppler, Asli Pekcan, Brett Biedermann, Andre Abreu, Gary S. Collins, Inderbir S. Gill, Giovanni E. Cacciamani</li>
<li>for: The paper aims to determine the extent and content of guidance for authors regarding the use of generative-AI (GAI), Generative Pretrained models (GPTs), and Large Language Models (LLMs) powered tools among the top 100 academic publishers and journals in science.</li>
<li>methods: The study screened the websites of the top 100 publishers and journals from May 19th to May 20th, 2023, to identify guidance on the use of GAI.</li>
<li>results: The study found that 17% of the largest 100 publishers and 70% of the top 100 journals provided guidance on the use of GAI. Most publishers and journals prohibited the inclusion of GAI as an author, but there was variability in how to disclose the use of GAI and in the allowable uses of GAI. Some top publishers and journals lacked guidance on the use of GAI by authors, and there was a need for standardized guidelines to protect the integrity of scientific output.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文目的是检查科学领域前100家出版社和期刊的作者指南中对生成AI（GAI）、生成预训模型（GPTs）和大语言模型（LLMs）Powered工具的使用。</li>
<li>methods: 这个研究从5月19日至5月20日，对前100家出版社和期刊的官方网站进行屏幕，以找到关于GAI的指南。</li>
<li>results: 研究发现，前100家出版社中有17%提供了GAI的指南，而前100家期刊中有70%提供了指南。大多数出版社和期刊禁止了GAI作为作者的包含，但是有一定的变化在披露GAI的方式和允许的GAI使用方式。一些顶尖出版社和期刊缺乏关于GAI的指南，需要有标准化的指南来保护科学输出的正当性。<details>
<summary>Abstract</summary>
We aim to determine the extent and content of guidance for authors regarding the use of generative-AI (GAI), Generative Pretrained models (GPTs) and Large Language Models (LLMs) powered tools among the top 100 academic publishers and journals in science. The websites of these publishers and journals were screened from between 19th and 20th May 2023. Among the largest 100 publishers, 17% provided guidance on the use of GAI, of which 12 (70.6%) were among the top 25 publishers. Among the top 100 journals, 70% have provided guidance on GAI. Of those with guidance, 94.1% of publishers and 95.7% of journals prohibited the inclusion of GAI as an author. Four journals (5.7%) explicitly prohibit the use of GAI in the generation of a manuscript, while 3 (17.6%) publishers and 15 (21.4%) journals indicated their guidance exclusively applies to the writing process. When disclosing the use of GAI, 42.8% of publishers and 44.3% of journals included specific disclosure criteria. There was variability in guidance of where to disclose the use of GAI, including in the methods, acknowledgments, cover letter, or a new section. There was also variability in how to access GAI guidance and the linking of journal and publisher instructions to authors. There is a lack of guidance by some top publishers and journals on the use of GAI by authors. Among those publishers and journals that provide guidance, there is substantial heterogeneity in the allowable uses of GAI and in how it should be disclosed, with this heterogeneity persisting among affiliated publishers and journals in some instances. The lack of standardization burdens authors and threatens to limit the effectiveness of these regulations. There is a need for standardized guidelines in order to protect the integrity of scientific output as GAI continues to grow in popularity.
</details>
<details>
<summary>摘要</summary>
我们目的是确定杂志和出版商在使用生成AI（GAI）、生成预训练模型（GPT）和大语言模型（LLM）激活的指导内容和范围。我们在2023年5月19日至20日检查了前100名学术出版商和杂志的网站。 Among the largest 100 publishers, 17% provided guidance on the use of GAI, of which 12 (70.6%) were among the top 25 publishers. Among the top 100 journals, 70% have provided guidance on GAI. Of those with guidance, 94.1% of publishers and 95.7% of journals prohibited the inclusion of GAI as an author. Four journals (5.7%) explicitly prohibit the use of GAI in the generation of a manuscript, while 3 (17.6%) publishers and 15 (21.4%) journals indicated their guidance exclusively applies to the writing process. When disclosing the use of GAI, 42.8% of publishers and 44.3% of journals included specific disclosure criteria. There was variability in guidance of where to disclose the use of GAI, including in the methods, acknowledgments, cover letter, or a new section. There was also variability in how to access GAI guidance and the linking of journal and publisher instructions to authors. There is a lack of guidance by some top publishers and journals on the use of GAI by authors. Among those publishers and journals that provide guidance, there is substantial heterogeneity in the allowable uses of GAI and in how it should be disclosed, with this heterogeneity persisting among affiliated publishers and journals in some instances. The lack of standardization burdens authors and threatens to limit the effectiveness of these regulations. There is a need for standardized guidelines in order to protect the integrity of scientific output as GAI continues to grow in popularity.
</details></li>
</ul>
<hr>
<h2 id="Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning"><a href="#Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning" class="headerlink" title="Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning"></a>Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11897">http://arxiv.org/abs/2307.11897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skandavaidyanath/credit-assignment">https://github.com/skandavaidyanath/credit-assignment</a></li>
<li>paper_authors: Akash Velu, Skanda Vaidyanath, Dilip Arumugam</li>
<li>for: 增强奖励学习 Agent 在缺乏评价反馈的环境中表现，尤其是在长期行为路径上只有单个终态反馈信号，导致奖励学习Agent 困难地归因到特定的行为步骤。</li>
<li>methods: 我们采用了现有的重要性抽象估计技术来改进基eline方法，以提高稳定性和效率。</li>
<li>results: 我们的方法可以在各种环境中稳定、高效地学习，并且可以缓解奖励学习Agent 在奖励分配问题上的困难。<details>
<summary>Abstract</summary>
Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, leading to a significant temporal delay between the observation of a non-trivial reward and the individual steps of behavior culpable for achieving said reward. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning. While prior work has introduced the concept of hindsight policies to develop a theoretically moxtivated method for reweighting on-policy data by impact on achieving the observed trajectory return, we show that these methods experience instabilities which lead to inefficient learning in complex environments. In this work, we adapt existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the stability and efficiency of these so-called hindsight policy methods. Our hindsight distribution correction facilitates stable, efficient learning across a broad range of environments where credit assignment plagues baseline methods.
</details>
<details>
<summary>摘要</summary>
常常，决策问题的环境很少提供评价反馈来引导强化学习代理人。在极端情况下，长期行为只有单个终端反馈信号，从而导致获得非致命奖励的步骤之间的时间延迟。处理这种奖励分配挑战是强化学习的一个标志特征。而优先作业已经介绍了使用影响实现观察路径返回的奖励重要性权重法，但这些方法会导致不稳定性，从而降低复杂环境中学习的效率。在这种情况下，我们采用现有的不当重要性评估技术来重要性权重法，以改善稳定性和效率。我们的往事分布修正方法可以在各种奖励分配问题中稳定、高效地学习。
</details></li>
</ul>
<hr>
<h2 id="On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise"><a href="#On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise" class="headerlink" title="On the Vulnerability of Fairness Constrained Learning to Malicious Noise"></a>On the Vulnerability of Fairness Constrained Learning to Malicious Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11892">http://arxiv.org/abs/2307.11892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avrim Blum, Princewill Okoroafor, Aadirupa Saha, Kevin Stangl</li>
<li>for: 本文研究了对小量恶意噪声的抗性性别平等学习。</li>
<li>methods: 本文使用了随机分类器来减轻恶意噪声的影响。</li>
<li>results: 研究发现，允许随机分类器时，性别平等学习对小量恶意噪声的抗性较为良好，例如对于人口均衡性，可以具有$\Theta(\alpha)$的准确率损失，与无性别约束的最好情况相当。对于平等机会性，可以具有$O(\sqrt{\alpha})$的准确率损失，并给出了匹配的下界$\Omega(\sqrt{\alpha})$。与 Konstantinov 和 Lampert（2021）的研究相比，这些结果表明性别平等学习对小量恶意噪声的抗性较为优秀。此外，本文还考虑了其他的公平性定义，包括平等机会性和均衡性。对这些公平性定义，残余准确率分布在$O(\alpha)$, $O(\sqrt{\alpha})$和$O(1)$三个自然区间内。这些结果为性别平等学习对 adversarial 噪声的抗性提供了更细致的视角。<details>
<summary>Abstract</summary>
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty of our work is how randomization can bypass simple "tricks" an adversary can use to amplify his power. We also consider additional fairness notions including Equalized Odds and Calibration. For these fairness notions, the excess accuracy clusters into three natural regimes $O(\alpha)$,$O(\sqrt{\alpha})$ and $O(1)$. These results provide a more fine-grained view of the sensitivity of fairness-constrained learning to adversarial noise in training data.
</details>
<details>
<summary>摘要</summary>
我们考虑了公平性条件下的学习的易受攻击性。 Konstantinov 和 Lampert (2021) 开始了这个研究，并发现了一些数据分布下，任何合法的学习者都会受到高度易受攻击性的影响，当集合大小不对称时。 在这里，我们提供了一个更有希望的看法，表明如果允许随机分类器， то alors the landscape 会是非常复杂的。 例如，对于人口均衡，我们显示可以允许仅有 $\Theta(\alpha)$ 的精度损失，其中 $\alpha$ 是邪恶噪音率，与不具有公平性限制的情况相同。 对于平等机会，我们显示可以允许 $O(\sqrt{\alpha})$ 的精度损失，并提供了匹配的 $\Omega(\sqrt{\alpha})$ 下界。 与 Konstantinov 和 Lampert (2021) 的结果相比，我们的结果显示，允许随机分类器后，损失精度会分布在三个自然的 режимах $O(\alpha)$, $O(\sqrt{\alpha})$ 和 $O(1)$。 这些结果提供了一个更细部的看法，对于公平性限制下的学习对于噪音训练数据的敏感性。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Document-Analytics-for-Banking-Process-Automation"><a href="#Multimodal-Document-Analytics-for-Banking-Process-Automation" class="headerlink" title="Multimodal Document Analytics for Banking Process Automation"></a>Multimodal Document Analytics for Banking Process Automation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11845">http://arxiv.org/abs/2307.11845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Gerling, Stefan Lessmann</li>
<li>For: This paper aims to understand the potential of advanced document analytics, specifically using multimodal models, in banking processes to improve operational efficiency and enhance process efficiency.* Methods: The paper uses a comprehensive analysis of the diverse banking document landscape, highlighting opportunities for efficiency gains through automation and advanced analytics techniques in the customer business. The study also employs natural language processing (NLP) techniques, including LayoutXLM, a cross-lingual, multimodal, pre-trained model, to analyze diverse documents in the banking sector.* Results: The study achieves an overall F1 score performance of around 80% on German company register extracts, demonstrating the efficiency of LayoutXLM. Additionally, the study finds that over 75% F1 score can be achieved with only 30% of the training data, highlighting the benefits of integrating image information and the potential for real-world applicability and benefits of multimodal models within banking.<details>
<summary>Abstract</summary>
In response to growing FinTech competition and the need for improved operational efficiency, this research focuses on understanding the potential of advanced document analytics, particularly using multimodal models, in banking processes. We perform a comprehensive analysis of the diverse banking document landscape, highlighting the opportunities for efficiency gains through automation and advanced analytics techniques in the customer business. Building on the rapidly evolving field of natural language processing (NLP), we illustrate the potential of models such as LayoutXLM, a cross-lingual, multimodal, pre-trained model, for analyzing diverse documents in the banking sector. This model performs a text token classification on German company register extracts with an overall F1 score performance of around 80\%. Our empirical evidence confirms the critical role of layout information in improving model performance and further underscores the benefits of integrating image information. Interestingly, our study shows that over 75% F1 score can be achieved with only 30% of the training data, demonstrating the efficiency of LayoutXLM. Through addressing state-of-the-art document analysis frameworks, our study aims to enhance process efficiency and demonstrate the real-world applicability and benefits of multimodal models within banking.
</details>
<details>
<summary>摘要</summary>
响应金融科技竞争的增长和业务效率的需求，这项研究专注于理解进步的文档分析技术在银行业务中的潜在优势。我们进行了银行文档多样化领域的全面分析，并指出了自动化和高级分析技术的可能性，以提高客户业务的效率。基于自然语言处理（NLP）领域的快速发展，我们介绍了 LayoutXLM 模型，这是一种跨语言、多modal、预训练的模型，可以分析银行业务中的多种文档。这个模型在德国公司注册报表EXTRACTS上进行文本符号分类，其总 F1 分数为约 80%。我们的实证证明了文档中的布局信息对模型性能的重要性，并进一步强调了将图像信息integrated的利好。奇妙的是，我们的研究表明，只使用 30% 的训练数据，可以达到超过 75% F1 分数，这表明 LayoutXLM 的效率。通过对现代文档分析框架进行调查，我们的研究旨在提高业务效率，并证明在银行业务中的多Modal模型的实际可用性和优势。
</details></li>
</ul>
<hr>
<h2 id="eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review"><a href="#eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review" class="headerlink" title="eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review"></a>eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13704">http://arxiv.org/abs/2307.13704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alena Kalyakulina, Igor Yusipov</li>
<li>for: 这篇论文旨在介绍Explainable Artificial Intelligence（XAI）在年龄预测任务中的应用。</li>
<li>methods: 论文使用了多种XAI方法，包括深度学习模型和特征选择技术。</li>
<li>results: 论文通过对多个身体系统的研究，发现XAI可以帮助提高年龄预测的准确率和可解释性。<details>
<summary>Abstract</summary>
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>现代人工智能（XAI）已成为机器学习的重要和必需的一部分，允许解释模型的预测结果。XAI特别在高风险应用中需要，如医疗领域，人工智能系统的决策对人生命有着重要的影响。一个医学研究领域是年龄预测和衰老病症的生物标志物的预测。然而，XAI在年龄预测任务中的角色尚未得到直接探讨。在这篇评论中，我们讨论了XAI方法在年龄预测任务中的应用。我们按照身体系统进行了系统性的综述，并讨论了医疗应用中XAI的利点，特别是在年龄预测领域。
</details></li>
</ul>
<hr>
<h2 id="HybridAugment-Unified-Frequency-Spectra-Perturbations-for-Model-Robustness"><a href="#HybridAugment-Unified-Frequency-Spectra-Perturbations-for-Model-Robustness" class="headerlink" title="HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness"></a>HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11823">http://arxiv.org/abs/2307.11823</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkyucel/hybrid_augment">https://github.com/mkyucel/hybrid_augment</a></li>
<li>paper_authors: Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, Pinar Duygulu</li>
<li>for: 提高Convolutional Neural Networks (CNN)对分布shift的泛化性能</li>
<li>methods: 提出了一种简单 yet effective的数据增强方法 HybridAugment，以减少CNN对高频组件的依赖，提高其 robustness，保持清晰率高</li>
<li>results: HybridAugment和HybridAugment++在CIFAR-10&#x2F;100和ImageNet上达到或超过了现状的clean accuracy，在ImageNet-C、CIFAR-10-C和CIFAR-100-C中的损坏测试中达到或超过了现状，在CIFAR-10上的抗击性和多种数据集上的out-of-distribution检测中达到了竞争水平<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNN) are known to exhibit poor generalization performance under distribution shifts. Their generalization have been studied extensively, and one line of work approaches the problem from a frequency-centric perspective. These studies highlight the fact that humans and CNNs might focus on different frequency components of an image. First, inspired by these observations, we propose a simple yet effective data augmentation method HybridAugment that reduces the reliance of CNNs on high-frequency components, and thus improves their robustness while keeping their clean accuracy high. Second, we propose HybridAugment++, which is a hierarchical augmentation method that attempts to unify various frequency-spectrum augmentations. HybridAugment++ builds on HybridAugment, and also reduces the reliance of CNNs on the amplitude component of images, and promotes phase information instead. This unification results in competitive to or better than state-of-the-art results on clean accuracy (CIFAR-10/100 and ImageNet), corruption benchmarks (ImageNet-C, CIFAR-10-C and CIFAR-100-C), adversarial robustness on CIFAR-10 and out-of-distribution detection on various datasets. HybridAugment and HybridAugment++ are implemented in a few lines of code, does not require extra data, ensemble models or additional networks.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在分布Shift下表现不佳。其泛化性已经得到了广泛的研究，其中一种方向是从频率角度出发。这些研究表明人类和CNN可能会关注不同频率成分的图像。基于这些观察，我们提出了一种简单又有效的数据扩充方法 HybridAugment，它降低了CNN对高频成分的依赖，从而提高了其Robustness，保持了清晰度高。其次，我们提出了HybridAugment++，它是一种层次扩充方法，它尝试通过不同频谱扩充来统一各种频谱扩充。HybridAugment++ builds on HybridAugment，并且降低了CNN对图像的振荡 Component 的依赖，而且促进图像的频谱信息。这种统一结果在CIFAR-10/100和ImageNet上得到了与或超过了现状的 Results，同时在ImageNet-C、CIFAR-10-C和CIFAR-100-C上的腐坏检验、鲁棒性检验和Out-of-distribution检验中也表现出色。HybridAugment和HybridAugment++都是几行代码，不需要额外数据、ensemble模型或额外网络。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense"><a href="#Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense" class="headerlink" title="Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense"></a>Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11730">http://arxiv.org/abs/2307.11730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enriquetomasmb/fedstellar">https://github.com/enriquetomasmb/fedstellar</a></li>
<li>paper_authors: Enrique Tomás Martínez Beltrán, Pedro Miguel Sánchez Sánchez, Sergio López Bernal, Gérôme Bovet, Manuel Gil Pérez, Gregorio Martínez Pérez, Alberto Huertas Celdrán</li>
<li>for: 这篇研究旨在探讨对 decentralized federated learning (DFL) 的攻击性问题，并提出一个安全模组来对抗通信基础攻击。</li>
<li>methods: 这篇研究使用了 symmetric and asymmetric encryption 以及 Moving Target Defense (MTD) 技术，包括随机选择邻居和 IP&#x2F;port 变换，并在 Fedstellar 平台上实现了安全模组。</li>
<li>results: 实验结果显示，在对 MNIST 数据集和 eclipse 攻击进行评估时，这个安全模组能够提高 F1 分数的平均值至 95%，并导致 CPU 使用率（最高可达 63.2% +-3.5%）和网络流量（最高可达 230 MB +-15 MB）的moderate 增加。<details>
<summary>Abstract</summary>
The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including random neighbor selection and IP/port switching. The security module is implemented in a DFL platform called Fedstellar, allowing the deployment and monitoring of the federation. A DFL scenario has been deployed, involving eight physical devices implementing three security configurations: (i) a baseline with no security, (ii) an encrypted configuration, and (iii) a configuration integrating both encryption and MTD techniques. The effectiveness of the security module is validated through experiments with the MNIST dataset and eclipse attacks. The results indicated an average F1 score of 95%, with moderate increases in CPU usage (up to 63.2% +-3.5%) and network traffic (230 MB +-15 MB) under the most secure configuration, mitigating the risks posed by eavesdropping or eclipse attacks.
</details>
<details>
<summary>摘要</summary>
DFL（分布式联合学习）的出现使得机器学习模型可以在联合参与者之间训练，从而实现分布式模型聚合和减少依赖于服务器。然而，这种方法引入了一些独特的通信安全挑战，在文献中没有得到充分解决。这些挑战主要来自联合聚合过程的分布式特性、参与者的多样化角色和责任，以及缺乏中央权限来监督和 Mitigate 威胁。为了解决这些挑战，本文首先定义了DFL通信的威胁模型，并提出了一种安全模块，用于DFL平台来防御通信基于攻击。该模块结合了加密技术和移动目标防御（MTD）技术，包括随机邻居选择和IP/端口转换。该安全模块在名为Fedstellar的DFL平台上实现， allowing the deployment and monitoring of the federation。一个DFL场景已经被部署，涉及八个物理设备实现三种安全配置：（i）无安全（基准），（ii）加密配置，（iii）加密和MTD技术的配置。安全模块的效果通过使用MNIST数据集和 eclipse 攻击进行实验验证。结果表明，在最安全配置下，F1 分数平均达到 95%，CPU 使用率（最大值63.2% +-3.5%）和网络流量（230 MB +-15 MB）增加较moderate。这些结果验证了安全模块的有效性，抵消了防止窃听或 eclipse 攻击的风险。
</details></li>
</ul>
<hr>
<h2 id="Benchmark-datasets-for-biomedical-knowledge-graphs-with-negative-statements"><a href="#Benchmark-datasets-for-biomedical-knowledge-graphs-with-negative-statements" class="headerlink" title="Benchmark datasets for biomedical knowledge graphs with negative statements"></a>Benchmark datasets for biomedical knowledge graphs with negative statements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11719">http://arxiv.org/abs/2307.11719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rita T. Sousa, Sara Silva, Catia Pesquita</li>
<li>for:  fills the lack of benchmark datasets for knowledge graphs with negative statements, especially in the biomedical domain.</li>
<li>methods:  two popular path-based methods are used to generate knowledge graph embeddings for each dataset.</li>
<li>results:  negative statements can improve the performance of knowledge graph embeddings in relation prediction tasks, such as protein-protein interaction prediction, gene-disease association prediction, and disease prediction.<details>
<summary>Abstract</summary>
Knowledge graphs represent facts about real-world entities. Most of these facts are defined as positive statements. The negative statements are scarce but highly relevant under the open-world assumption. Furthermore, they have been demonstrated to improve the performance of several applications, namely in the biomedical domain. However, no benchmark dataset supports the evaluation of the methods that consider these negative statements.   We present a collection of datasets for three relation prediction tasks - protein-protein interaction prediction, gene-disease association prediction and disease prediction - that aim at circumventing the difficulties in building benchmarks for knowledge graphs with negative statements. These datasets include data from two successful biomedical ontologies, Gene Ontology and Human Phenotype Ontology, enriched with negative statements.   We also generate knowledge graph embeddings for each dataset with two popular path-based methods and evaluate the performance in each task. The results show that the negative statements can improve the performance of knowledge graph embeddings.
</details>
<details>
<summary>摘要</summary>
知识图表示实际世界实体的事实。大多数这些事实被定义为正面声明。然而，负面声明scarce，但在开放世界假设下，它们对许多应用程序的性能有着高度相关性。例如，在生物医学领域中，它们已经被证明可以提高性能。然而，没有一个benchmark dataset来评估这些方法，这使得建立 benchmarks for knowledge graphs with negative statements 变得困难。为了解决这些问题，我们提供了三个关系预测任务的数据集 - protein-protein交互预测、基因疾病相关性预测和疾病预测 - 这些数据集包括了两个成功的生物医学 ontologies，生物学机制 Ontology 和人类现象 Ontology，这些数据集还包括了负面声明。我们还生成了每个数据集的知识图嵌入，使用两种流行的路径基方法，并评估每个任务的性能。结果表明，负面声明可以提高知识图嵌入的性能。
</details></li>
</ul>
<hr>
<h2 id="Statement-based-Memory-for-Neural-Source-Code-Summarization"><a href="#Statement-based-Memory-for-Neural-Source-Code-Summarization" class="headerlink" title="Statement-based Memory for Neural Source Code Summarization"></a>Statement-based Memory for Neural Source Code Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11709">http://arxiv.org/abs/2307.11709</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aakashba/smncode2022">https://github.com/aakashba/smncode2022</a></li>
<li>paper_authors: Aakash Bansal, Siyuan Jiang, Sakib Haque, Collin McMillan</li>
<li>For: The paper is written for programmers who want to quickly understand the behavior of source code without having to read the code itself. It aims to provide natural language descriptions of code behavior.* Methods: The paper proposes a statement-based memory encoder that learns the important elements of flow during training, allowing for a statement-based subroutine representation without the need for dynamic analysis.* Results: The paper demonstrates a significant improvement over the state-of-the-art in code summarization using the proposed statement-based memory encoder.Here is the information in Simplified Chinese text:</li>
<li>for: 该论文是为程序员们提供快速理解代码行为的自然语言描述。</li>
<li>methods: 论文提出了一种基于语句记忆的编码器，通过在训练中学习流程的重要元素，实现了基于语句的子程序表示，不需要动态分析。</li>
<li>results: 论文通过提出的语句基于编码器，实现了对代码摘要的显著改进。<details>
<summary>Abstract</summary>
Source code summarization is the task of writing natural language descriptions of source code behavior. Code summarization underpins software documentation for programmers. Short descriptions of code help programmers understand the program quickly without having to read the code itself. Lately, neural source code summarization has emerged as the frontier of research into automated code summarization techniques. By far the most popular targets for summarization are program subroutines. The idea, in a nutshell, is to train an encoder-decoder neural architecture using large sets of examples of subroutines extracted from code repositories. The encoder represents the code and the decoder represents the summary. However, most current approaches attempt to treat the subroutine as a single unit. For example, by taking the entire subroutine as input to a Transformer or RNN-based encoder. But code behavior tends to depend on the flow from statement to statement. Normally dynamic analysis may shed light on this flow, but dynamic analysis on hundreds of thousands of examples in large datasets is not practical. In this paper, we present a statement-based memory encoder that learns the important elements of flow during training, leading to a statement-based subroutine representation without the need for dynamic analysis. We implement our encoder for code summarization and demonstrate a significant improvement over the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
源代码概要是指将源代码行为写入自然语言描述。代码概要支持软件文档 для程序员。简短的代码描述可以帮助程序员快速理解程序，而无需阅读代码本身。目前，神经源代码概要已经成为自动代码概要技术的前沿。目标最多是程序子循环。基本思路是使用大量的例子来训练神经网络Encoder-Decoder结构。Encoder表示代码，Decoder表示概要。但现有方法通常会将子循环视为单个单元。例如，将整个子循环作为Transformer或RNN基于Encoder的输入。但代码行为通常是从语句到语句的流动的。正常的动态分析可能会暴露这种流动，但是在大量数据集上进行动态分析是不实际的。在本文中，我们提出一个语句基 Memory Encoder，可以在训练中学习重要的流动元素，从而得到语句基的子循环表示，无需动态分析。我们实现了我们的Encoder для代码概要，并在状态前方示出了显著提升。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/cs.AI_2023_07_22/" data-id="clorjzl1d0013f1880r9hd0o8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/cs.CL_2023_07_22/" class="article-date">
  <time datetime="2023-07-22T11:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/22/cs.CL_2023_07_22/">cs.CL - 2023-07-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Revisiting-Distillation-for-Continual-Learning-on-Visual-Question-Localized-Answering-in-Robotic-Surgery"><a href="#Revisiting-Distillation-for-Continual-Learning-on-Visual-Question-Localized-Answering-in-Robotic-Surgery" class="headerlink" title="Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery"></a>Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12045">http://arxiv.org/abs/2307.12045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longbai1006/cs-vqla">https://github.com/longbai1006/cs-vqla</a></li>
<li>paper_authors: Long Bai, Mobarakol Islam, Hongliang Ren<br>for:这篇论文旨在探讨如何使用非例子同时学习（Continual Learning，CL）方法，以提高手术教育中的知识助手系统（VQLA）的能力。methods:这篇论文提出了一种非例子CL框架，以解决深度神经网络（DNNs）在学习新知识时的忘却问题。具体来说，当DNNs学习新的类或任务时，其对于老任务的性能会下降很快。此外，由于医疗数据隐私和许可问题，通常无法访问老数据来更新CL模型。因此，该论文提出了一种具有刚性和柔性特性的CL框架，以平衡DNNs在顺序学习中的刚性和柔性。results:通过对三个公共的手术数据集进行大量的实验，该论文证明了其提出的方法可以在手术VQLA中超越传统CL方法。具体来说，该方法可以保持老任务的性能，同时学习新任务。此外，该方法还可以调整权重对于老和新任务，以适应不同的学习情况。<details>
<summary>Abstract</summary>
The visual-question localized-answering (VQLA) system can serve as a knowledgeable assistant in surgical education. Except for providing text-based answers, the VQLA system can highlight the interested region for better surgical scene understanding. However, deep neural networks (DNNs) suffer from catastrophic forgetting when learning new knowledge. Specifically, when DNNs learn on incremental classes or tasks, their performance on old tasks drops dramatically. Furthermore, due to medical data privacy and licensing issues, it is often difficult to access old data when updating continual learning (CL) models. Therefore, we develop a non-exemplar continual surgical VQLA framework, to explore and balance the rigidity-plasticity trade-off of DNNs in a sequential learning paradigm. We revisit the distillation loss in CL tasks, and propose rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distillation (SH-Dist) to preserve the old knowledge. The weight aligning (WA) technique is also integrated to adjust the weight bias between old and new tasks. We further establish a CL framework on three public surgical datasets in the context of surgical settings that consist of overlapping classes between old and new surgical VQLA tasks. With extensive experiments, we demonstrate that our proposed method excellently reconciles learning and forgetting on the continual surgical VQLA over conventional CL methods. Our code is publicly accessible.
</details>
<details>
<summary>摘要</summary>
Visual-问题本地回答（VQLA）系统可以作为医学教育中的知识助手。除了提供文本回答外，VQLA系统还可以高亮 interessested 区域，以便更好地理解外科场景。然而，深度神经网络（DNNs）在学习新知识时会出现慢性学习问题。具体来说，当 DNNs 学习增量类或任务时，其对于古老任务的性能会下降很快。此外，由于医学数据隐私和许可问题，通常不可以访问老数据，因此在更新 continual learning（CL）模型时困难。因此，我们开发了一个非例外 continual surgical VQLA框架，以探索和衡量 DNNs 在顺序学习中的僵化-柔软性质的权衡。我们重新评估了 CL 任务中的滥览损失，并提出了固有-柔软性感知（RP-Dist）和自适应多样化滥览（SH-Dist）来保持古老知识。此外，我们还使用了 weight aligning（WA）技术来调整新任务和古老任务之间的权重偏好。我们进一步建立了 CL 框架在三个公共外科数据集上，这些数据集在外科设置中包含了古老和新的外科 VQLA任务之间的重叠类。经过广泛的实验，我们证明了我们提出的方法在 continual surgical VQLA 中优化了学习和忘却。我们的代码公共访问。
</details></li>
</ul>
<hr>
<h2 id="FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models"><a href="#FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models" class="headerlink" title="FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models"></a>FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00065">http://arxiv.org/abs/2308.00065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweiyin/finpt">https://github.com/yuweiyin/finpt</a></li>
<li>paper_authors: Yuwei Yin, Yazheng Yang, Jian Yang, Qi Liu</li>
<li>For: Financial risk prediction in the financial sector, specifically addressing the issues of outdated algorithms and lack of a unified benchmark.* Methods: Propose a novel approach called FinPT that leverages large pretrained foundation models and natural language processing techniques to improve financial risk prediction. FinPT fills financial tabular data into pre-defined instruction templates, obtains natural-language customer profiles by prompting LLMs, and fine-tunes large foundation models with the profile text for predictions.* Results: Demonstrate the effectiveness of FinPT by experimenting with a range of representative strong baselines on FinBench, a set of high-quality datasets on financial risks. Analytical studies also deepen the understanding of LLMs for financial risk prediction.<details>
<summary>Abstract</summary>
Financial risk prediction plays a crucial role in the financial sector. Machine learning methods have been widely applied for automatically detecting potential risks and thus saving the cost of labor. However, the development in this field is lagging behind in recent years by the following two facts: 1) the algorithms used are somewhat outdated, especially in the context of the fast advance of generative AI and large language models (LLMs); 2) the lack of a unified and open-sourced financial benchmark has impeded the related research for years. To tackle these issues, we propose FinPT and FinBench: the former is a novel approach for financial risk prediction that conduct Profile Tuning on large pretrained foundation models, and the latter is a set of high-quality datasets on financial risks such as default, fraud, and churn. In FinPT, we fill the financial tabular data into the pre-defined instruction template, obtain natural-language customer profiles by prompting LLMs, and fine-tune large foundation models with the profile text to make predictions. We demonstrate the effectiveness of the proposed FinPT by experimenting with a range of representative strong baselines on FinBench. The analytical studies further deepen the understanding of LLMs for financial risk prediction.
</details>
<details>
<summary>摘要</summary>
financial风险预测在金融领域扮演着关键的角色。机器学习方法在自动检测 potential的风险方面得到广泛的应用，从而节省劳动成本。然而，在这一领域的发展在最近几年来受到了以下两个因素的延迟：1）使用的算法有些已经过时，尤其是在生成式AI和大型自然语言模型（LLM）的快速进步的背景下；2）缺乏一个统一的、开源的金融标准 benchmark，对相关研究造成了多年的阻碍。为解决这些问题，我们提出了 FinPT 和 FinBench。前者是一种新的金融风险预测方法，通过 Profile Tuning 大型预训模型中的大型预训模型，并在 FinBench 中进行了详细的分析研究。在 FinPT 中，我们将金融表格数据填充到预定的指令模板中，通过 LLMS 提取自然语言客户profile，并使用profile文本微调大型基础模型进行预测。我们通过对 FinBench 中的多种代表强基eline进行实验，证明了我们提出的 FinPT 的有效性。分析研究还深入了解LLMS的金融风险预测能力。
</details></li>
</ul>
<hr>
<h2 id="Learning-Vision-and-Language-Navigation-from-YouTube-Videos"><a href="#Learning-Vision-and-Language-Navigation-from-YouTube-Videos" class="headerlink" title="Learning Vision-and-Language Navigation from YouTube Videos"></a>Learning Vision-and-Language Navigation from YouTube Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11984">http://arxiv.org/abs/2307.11984</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeremylinky/youtube-vln">https://github.com/jeremylinky/youtube-vln</a></li>
<li>paper_authors: Kunyang Lin, Peihao Chen, Diwei Huang, Thomas H. Li, Mingkui Tan, Chuang Gan</li>
<li>for: 使用 YouTube 上的房屋游览视频来培养一个embodied agent，以便在真实的3D环境中使用自然语言指令进行导航。</li>
<li>methods: 创建一个大规模的数据集，其包含有理性的路径指令对从房屋游览视频中提取的，并在这个数据集上预训练 agent。</li>
<li>results: 通过使用 entropy 算法构建路径指令对，以及一个action-aware生成器来从未标注的旁路中提取指令，最终通过训练 trajectory judgment 预text task 来让 agent 挖掘到环境的布局知识，实现了在 R2R 和 REVERIE 两个标准测试benchmark上的状态级表现。<details>
<summary>Abstract</summary>
Vision-and-language navigation (VLN) requires an embodied agent to navigate in realistic 3D environments using natural language instructions. Existing VLN methods suffer from training on small-scale environments or unreasonable path-instruction datasets, limiting the generalization to unseen environments. There are massive house tour videos on YouTube, providing abundant real navigation experiences and layout information. However, these videos have not been explored for VLN before. In this paper, we propose to learn an agent from these videos by creating a large-scale dataset which comprises reasonable path-instruction pairs from house tour videos and pre-training the agent on it. To achieve this, we have to tackle the challenges of automatically constructing path-instruction pairs and exploiting real layout knowledge from raw and unlabeled videos. To address these, we first leverage an entropy-based method to construct the nodes of a path trajectory. Then, we propose an action-aware generator for generating instructions from unlabeled trajectories. Last, we devise a trajectory judgment pretext task to encourage the agent to mine the layout knowledge. Experimental results show that our method achieves state-of-the-art performance on two popular benchmarks (R2R and REVERIE). Code is available at https://github.com/JeremyLinky/YouTube-VLN
</details>
<details>
<summary>摘要</summary>
vision-and-language navigation (VLN) 需要一个具体的智能体在真实的3D环境中使用自然语言指令进行导航。现有的VLN方法受到小规模环境或不合理的路径指令数据的限制，导致对未看过的环境的泛化能力受到限制。 YouTube上有大量的房屋游览视频，这些视频提供了丰富的实际导航经验和房屋布局信息。然而，这些视频没有被前面的VLN研究所用。在这篇论文中，我们提议从这些视频中学习一个智能体，并创建了一个大规模的数据集，该数据集包括合理的路径指令对。为了实现这一点，我们首先利用一种 entropy-based 方法构建路径轨迹的节点。然后，我们提出了一种 action-aware 生成器，用于从无标签的轨迹中生成指令。最后，我们设计了一个轨迹判断预测任务，以便让智能体挖掘布局知识。实验结果表明，我们的方法在两个流行的benchmark（R2R和REVERIE）上达到了状态艺术性的表现。代码可以在 https://github.com/JeremyLinky/YouTube-VLN 上获取。
</details></li>
</ul>
<hr>
<h2 id="CARTIER-Cartographic-lAnguage-Reasoning-Targeted-at-Instruction-Execution-for-Robots"><a href="#CARTIER-Cartographic-lAnguage-Reasoning-Targeted-at-Instruction-Execution-for-Robots" class="headerlink" title="CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots"></a>CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11865">http://arxiv.org/abs/2307.11865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil Kakodkar, Dmitriy Rivkin, Bobak H. Baghi, Francois Hogan, Gregory Dudek</li>
<li>for: 这个论文探讨了大语言模型（LLM）如何解决混合语言计划和自然语言导航界面的问题。</li>
<li>methods: 该论文使用了大语言模型来解释用户在对话中提供的描述性语言查询，并在3D simulator AI2Thor中创建了复杂和可重复的场景。</li>
<li>results: 研究表明，使用大语言模型可以更好地解析用户在对话中提供的描述性语言查询，并且可以更好地理解用户的 Navigation 目标。<details>
<summary>Abstract</summary>
This work explores the capacity of large language models (LLMs) to address problems at the intersection of spatial planning and natural language interfaces for navigation.Our focus is on following relatively complex instructions that are more akin to natural conversation than traditional explicit procedural directives seen in robotics. Unlike most prior work, where navigation directives are provided as imperative commands (e.g., go to the fridge), we examine implicit directives within conversational interactions. We leverage the 3D simulator AI2Thor to create complex and repeatable scenarios at scale, and augment it by adding complex language queries for 40 object types. We demonstrate that a robot can better parse descriptive language queries than existing methods by using an LLM to interpret the user interaction in the context of a list of the objects in the scene.
</details>
<details>
<summary>摘要</summary>
To conduct our research, we utilize the 3D simulator AI2Thor to create complex and repeatable scenarios at scale, and augment it by adding complex language queries for 40 object types. Our results show that a robot can better understand and execute descriptive language queries by using an LLM to interpret the user interaction in the context of a list of objects in the scene.
</details></li>
</ul>
<hr>
<h2 id="The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention"><a href="#The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention" class="headerlink" title="The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention"></a>The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11864">http://arxiv.org/abs/2307.11864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Ayoobi, Sadat Shahriar, Arjun Mukherjee</li>
<li>for: 这个研究目的是为 LinkedIn 线上社交网络中检测伪注册和大语言模型（LLM）生成的账户，以避免伪者获取正常用户的私人资讯和推广未来的骗变活动。</li>
<li>methods: 这个研究使用 LinkedIn  Profil 中提供的文本信息，并引入 Section and Subsection Tag Embedding（SSTE）方法，以增强这些数据的归类特征，以分辨伪注册和 manually 或使用 LLM 生成的账户。</li>
<li>results: 这个研究获得了约 95% 的准确率，可以分辨伪注册和正常账户，并且显示 SSTE 在识别 LLM 生成的账户时的准确率为约 90%，即使在训练阶段没有使用 LLM 生成的账户。<details>
<summary>Abstract</summary>
In this paper, we present a novel method for detecting fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections. Early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. This work uses textual information provided in LinkedIn profiles and introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. Additionally, the dearth of a large publicly available LinkedIn dataset motivated us to collect 3600 LinkedIn profiles for our research. We will release our dataset publicly for research purposes. This is, to the best of our knowledge, the first large publicly available LinkedIn dataset for fake LinkedIn account detection. Within our paradigm, we assess static and contextualized word embeddings, including GloVe, Flair, BERT, and RoBERTa. We show that the suggested method can distinguish between legitimate and fake profiles with an accuracy of about 95% across all word embeddings. In addition, we show that SSTE has a promising accuracy for identifying LLM-generated profiles, despite the fact that no LLM-generated profiles were employed during the training phase, and can achieve an accuracy of approximately 90% when only 20 LLM-generated profiles are added to the training set. It is a significant finding since the proliferation of several LLMs in the near future makes it extremely challenging to design a single system that can identify profiles created with various LLMs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法，用于在 LinkedIn 在线社交网络上立即识别假 profiles 和 Large Language Model（LLM）生成的 profiles，以避免骗子在获取正式用户的隐私信息和敏感信息后，进行后续的骗取和骗财活动。这种工作使用 LinkedIn profile 中提供的文本信息，并引入 Section and Subsection Tag Embedding（SSTE）方法，以增强这些数据的抑制特征，以分辨真实 profiles 和骗劫 manually 或使用 LLM 生成的 profiles。此外，由于 LinkedIn 公共可用的大型数据集缺乏，我们自己收集了 3600 个 LinkedIn profiles  для我们的研究。我们将会在研究用途上公开发布我们的数据集。这是，我们知道的， LinkedIn 上假帐户检测的首个大型公共可用数据集。在我们的 paradigm 中，我们评估了静态和Contextualized Word Embeddings，包括 GloVe、Flair、BERT 和 RoBERTa。我们发现，我们的方法可以在所有 Word Embeddings 上分辨真实 profiles 和假 profiles，准确率约为 95%。此外，我们发现 SSTE 在 LLM 生成 profiles 上具有扩展的准确率，即使在训练阶段没有使用 LLM 生成 profiles，可以达到约 90% 的准确率，只需要将 20 个 LLM 生成 profiles 添加到训练集中。这是一个重要的发现，因为未来几年 LLM 的普及会使得设计一个可以分辨多种 LLM 生成的 profiles 的系统变得极其困难。
</details></li>
</ul>
<hr>
<h2 id="MythQA-Query-Based-Large-Scale-Check-Worthy-Claim-Detection-through-Multi-Answer-Open-Domain-Question-Answering"><a href="#MythQA-Query-Based-Large-Scale-Check-Worthy-Claim-Detection-through-Multi-Answer-Open-Domain-Question-Answering" class="headerlink" title="MythQA: Query-Based Large-Scale Check-Worthy Claim Detection through Multi-Answer Open-Domain Question Answering"></a>MythQA: Query-Based Large-Scale Check-Worthy Claim Detection through Multi-Answer Open-Domain Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11848">http://arxiv.org/abs/2307.11848</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonyby/myth-qa">https://github.com/tonyby/myth-qa</a></li>
<li>paper_authors: Yang Bai, Anthony Colas, Daisy Zhe Wang</li>
<li>for: The paper is written for detecting check-worthy claims directly from a large-scale information source, such as Twitter, to accelerate the fact-checking process.</li>
<li>methods: The paper introduces MythQA, a new multi-answer open-domain question answering task that involves contradictory stance mining for query-based large-scale check-worthy claim detection.</li>
<li>results: The paper presents a baseline system for MythQA and evaluates existing NLP models for each system component using the TweetMythQA dataset. The paper also provides initial benchmarks and identifies key challenges for future models to improve upon.Here’s the Simplified Chinese text for the three information points:</li>
<li>for: 这篇论文是为了从大规模信息源，如推特，直接检测check-worthy claim的目的。</li>
<li>methods: 这篇论文提出了一种新的多答问题回答任务，即mythQA，以推特上的矛盾立场挖掘为基础，以加速事实核查的过程。</li>
<li>results: 这篇论文提供了mythQA的基eline系统，并对现有NLP模型进行了TweetMythQA数据集上的评估。 paper还提供了初步的benchmark和未来模型改进的关键挑战。<details>
<summary>Abstract</summary>
Check-worthy claim detection aims at providing plausible misinformation to downstream fact-checking systems or human experts to check. This is a crucial step toward accelerating the fact-checking process. Many efforts have been put into how to identify check-worthy claims from a small scale of pre-collected claims, but how to efficiently detect check-worthy claims directly from a large-scale information source, such as Twitter, remains underexplored. To fill this gap, we introduce MythQA, a new multi-answer open-domain question answering(QA) task that involves contradictory stance mining for query-based large-scale check-worthy claim detection. The idea behind this is that contradictory claims are a strong indicator of misinformation that merits scrutiny by the appropriate authorities. To study this task, we construct TweetMythQA, an evaluation dataset containing 522 factoid multi-answer questions based on controversial topics. Each question is annotated with multiple answers. Moreover, we collect relevant tweets for each distinct answer, then classify them into three categories: "Supporting", "Refuting", and "Neutral". In total, we annotated 5.3K tweets. Contradictory evidence is collected for all answers in the dataset. Finally, we present a baseline system for MythQA and evaluate existing NLP models for each system component using the TweetMythQA dataset. We provide initial benchmarks and identify key challenges for future models to improve upon. Code and data are available at: https://github.com/TonyBY/Myth-QA
</details>
<details>
<summary>摘要</summary>
<<SYS>> CHECK-worthy 声明检测的目标是提供可信的谣言来供下游真伪检查系统或人类专家进行检查。这是减少真伪检查过程的关键步骤。许多努力已经投入到如何从小规模的预收集声明中Identify CHECK-worthy 声明，但如何高效地从大规模信息源，如推特，中直接检测 CHECK-worthy 声明仍未得到充分研究。为了填补这个空白，我们介绍了 MitQA，一个新的多答题开放Domain问答任务，涉及到矛盾立场挖掘，以便从推特等大规模信息源中检测 CHECK-worthy 声明。我们的想法是，矛盾的声明是谣言的强力指标，值得当局的审查。为了研究这个任务，我们构建了 TweetMythQA 评估数据集，包含 522 个多答问题，基于争议话题。每个问题有多个答案。此外，我们收集了每个问题的相关推特，然后将其分为三类：“支持”、“反对”和“中立”。总的来说，我们标注了 5.3K 条推特。为所有答案在数据集中，我们收集了矛盾证据。最后，我们提供了基线系统 для MitQA，并使用 TweetMythQA 数据集评估现有 NLP 模型的每个系统组件。我们提供了初步的标准和标识未来模型改进的关键挑战。代码和数据可以在 GitHub 上获取：https://github.com/TonyBY/Myth-QA。
</details></li>
</ul>
<hr>
<h2 id="OUTFOX-LLM-generated-Essay-Detection-through-In-context-Learning-with-Adversarially-Generated-Examples"><a href="#OUTFOX-LLM-generated-Essay-Detection-through-In-context-Learning-with-Adversarially-Generated-Examples" class="headerlink" title="OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples"></a>OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11729">http://arxiv.org/abs/2307.11729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki</li>
<li>for: 本研究旨在提高LLM生成文本检测器的Robustness，并在实际场景中评估其效果。</li>
<li>methods: 本研究提出了OUTFOX框架，该框架让检测器和攻击者都可以考虑对方的输出，并在学生作业作文中应用。</li>
<li>results: 实验结果表明，OUR proposed detector可以通过在攻击者的帮助下进行培训，提高检测性能，而OUR proposed attacker可以使检测器性能下降至-57.0点F1分。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, the effectiveness of these detectors in real-life situations, such as when students use LLMs for writing homework assignments (e.g., essays) and quickly learn how to evade these detectors, has not been explored. In this paper, we propose OUTFOX, a novel framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output and apply this to the domain of student essays. In our framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect. While the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. Our experiments show that our proposed detector learned in-context from the attacker improves the detection performance on the attacked dataset by up to +41.3 point F1-score. While our proposed attacker can drastically degrade the performance of the detector by up to -57.0 point F1-score compared to the paraphrasing method.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经 дости成人类水准的文本生成能力，使得区分人类写成和LLM生成的文本变得困难。这导致了LLM的滥用风险的增加，并且需要发展检测LLM生成的文本的技术。然而，现有的检测器对LLM生成的文本进行简单的重写，从而降低了检测器的准确度。此外，现有的检测器在实际情况下，例如学生使用LLM写作作业（例如论文）并快速学习如何避免这些检测器的情况下，尚未被探访。在这篇论文中，我们提出了 OUTFOX 框架，它可以提高 LLM 生成文本检测器的Robustness。在我们的框架中，攻击者使用检测器的预测标签作为内容学习的示例，并通过对检测器进行对抗式学习来生成更难以检测的文本。而检测器则使用对抗式生成的文本作为内容学习的示例，以提高检测器对于攻击者生成的文本的准确度。我们的实验显示，我们的提案的检测器在攻击 dataset 上的准确度提高了 +41.3 点 F1 分数。而我们的提案的攻击者可以对检测器造成极大的影响，比如 -57.0 点 F1 分数，相比之下，对文本进行重写方法的影响相对较小。
</details></li>
</ul>
<hr>
<h2 id="GPT-4-Can’t-Reason"><a href="#GPT-4-Can’t-Reason" class="headerlink" title="GPT-4 Can’t Reason"></a>GPT-4 Can’t Reason</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03762">http://arxiv.org/abs/2308.03762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vohidjon123/google">https://github.com/vohidjon123/google</a></li>
<li>paper_authors: Konstantine Arkoudas</li>
<li>for: 评估 GPT-4 模型的逻辑能力</li>
<li>methods: 使用多种评估方法评估 GPT-4 模型的逻辑能力</li>
<li>results: GPT-4 模型现在不具备逻辑能力，尝试用多种方法进行评估，但它只有偶尔出现一些分析天赋。<details>
<summary>Abstract</summary>
GPT-4 was released in March 2023 to wide acclaim, marking a very substantial improvement across the board over GPT-3.5 (OpenAI's previously best model, which had powered the initial release of ChatGPT). However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason. This position paper discusses the nature of reasoning; criticizes the current formulation of reasoning problems in the NLP community, as well as the way in which LLM reasoning performance is currently evaluated; introduces a small collection of 21 diverse reasoning problems; and performs a detailed qualitative evaluation of GPT-4's performance on those problems. Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/cs.CL_2023_07_22/" data-id="clorjzl3g0085f1882ykfbvd8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/cs.LG_2023_07_22/" class="article-date">
  <time datetime="2023-07-22T10:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/22/cs.LG_2023_07_22/">cs.LG - 2023-07-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC"><a href="#A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC" class="headerlink" title="A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC"></a>A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12115">http://arxiv.org/abs/2307.12115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Chen, Changyan Yi, Hongyang Du, Dusit Niyato, Jiawen Kang, Jun Cai, Xuemin, Shen</li>
<li>for: 本研究旨在探讨 mobil AI 生成内容技术如何推动人类数字孪生（HDT）的发展，以提高个人化医疗服务。</li>
<li>methods: 本文提出了一种基于 mobil AI 生成内容技术的 HDT 系统架构，并讨论了相关的设计要求和挑战。</li>
<li>results: 本文通过两个使用场景的示例和一个实验研究证明了该方案的有效性，并提出了一些未来方向和开放问题。<details>
<summary>Abstract</summary>
Mobile Artificial Intelligence-Generated Content (AIGC) technology refers to the adoption of AI algorithms deployed at mobile edge networks to automate the information creation process while fulfilling the requirements of end users. Mobile AIGC has recently attracted phenomenal attentions and can be a key enabling technology for an emerging application, called human digital twin (HDT). HDT empowered by the mobile AIGC is expected to revolutionize the personalized healthcare by generating rare disease data, modeling high-fidelity digital twin, building versatile testbeds, and providing 24/7 customized medical services. To promote the development of this new breed of paradigm, in this article, we propose a system architecture of mobile AIGC-driven HDT and highlight the corresponding design requirements and challenges. Moreover, we illustrate two use cases, i.e., mobile AIGC-driven HDT in customized surgery planning and personalized medication. In addition, we conduct an experimental study to prove the effectiveness of the proposed mobile AIGC-driven HDT solution, which shows a particular application in a virtual physical therapy teaching platform. Finally, we conclude this article by briefly discussing several open issues and future directions.
</details>
<details>
<summary>摘要</summary>
mobile artificial intelligence生成内容（AIGC）技术指的是在移动边缘网络中部署AI算法，以自动化信息创建过程，同时满足用户的需求。 mobile AIGC 在最近受到了极高的关注，并可以是人类数字双（HDT）的关键启用技术。 HDT 通过 mobile AIGC 的 empowerment，预计将重塑个性化医疗，生成罕见疾病数据，模拟高精度数字双，建立多样化测试床，提供24/7个性化医疗服务。为推动这种新的 Paradigma 的发展，本文提出了移动 AIGC 驱动 HDT 的系统架构，并 highlighted 相应的设计要求和挑战。 此外，本文还 illustrate 了两个用例，即移动 AIGC 驱动 HDT 在定制手术规划和个性化药物。 此外，我们还进行了实验研究，证明了提议的移动 AIGC 驱动 HDT 解决方案的效iveness。 最后，我们 briefly discuss 了一些开放问题和未来方向。
</details></li>
</ul>
<hr>
<h2 id="A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks"><a href="#A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks" class="headerlink" title="A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks"></a>A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12114">http://arxiv.org/abs/2307.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanis Labrak, Mickael Rouvier, Richard Dufour</li>
<li>for: 这些大型自然语言处理（NLP）任务，如名实化识别（NER）、问答（QA）、关系抽取（RE）等，是为了评估四种现状最佳的 instruciton-tuned 大语言模型（LLMs）在英文医学和生物医学领域的表现。</li>
<li>methods: 这些LLMs 是通过对 instruction-tuned 模型进行训练，以适应不同的 NLP 任务。</li>
<li>results: 结果表明，评估的 LLMs 在零到几个采样enario 下，对大多数任务的性能都在逐渐提高，特别是在问答任务上，即使它们从来没有看到这些任务的示例。然而，分类和RE任务的性能下降，与专门为医疗领域训练的模型，如PubMedBERT，相比而言，它们的性能较差。此外，我们发现没有任何 LLM 在所有任务上都能超越其他模型，各个模型在不同任务上的表现不同。<details>
<summary>Abstract</summary>
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
</details>
<details>
<summary>摘要</summary>
我们评估了四种现代 instruction-tuned大型自然语言处理（NLP）模型（ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca），在英语的13种实际医疗和生物医学NLP任务上进行评估，包括名称实体识别（NER）、问答（QA）、关系提取（RE）等。我们的总结结果表明，评估的LLMs在零或几个预测enario中的性能已经接近了现有模型的性能，尤其是在QA任务上表现出色，即使它们从来没有看到这些任务的示例。然而，我们发现，分类和RE任务的性能比特别训练的医疗领域模型，如PubMedBERT，还是有所下降。最后，我们注意到，无LLM可以在所有研究任务上表现出优于其他模型，一些模型更适合某些任务。
</details></li>
</ul>
<hr>
<h2 id="Active-Control-of-Flow-over-Rotating-Cylinder-by-Multiple-Jets-using-Deep-Reinforcement-Learning"><a href="#Active-Control-of-Flow-over-Rotating-Cylinder-by-Multiple-Jets-using-Deep-Reinforcement-Learning" class="headerlink" title="Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning"></a>Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12083">http://arxiv.org/abs/2307.12083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamyar Dobakhti, Jafar Ghazanfarian</li>
<li>for: 这个论文主要目的是提出一种基于深度学习的活动流控方法，以减少碰撞体上的阻力。</li>
<li>methods: 该方法使用多个控制的喷流来达到最大可能的阻力减少。具体来说，文章将介绍DRL算法的控制参数、其限制和优化，以及喷流数量和位置、感测器位置和最大喷流速率的优化。</li>
<li>results: 结果表明，将旋转和DRL相结合可以有效地减少阻力系数，达到49.75%的减少级别。此外，文章还表明，在不同的配置下，感测器的数量和位置需要根据用户的需求进行选择。同时，允许代理人访问更高的喷流速率，通常不会提高性能，除非rotating cylinder。<details>
<summary>Abstract</summary>
The real power of artificial intelligence appears in reinforcement learning, which is computationally and physically more sophisticated due to its dynamic nature. Rotation and injection are some of the proven ways in active flow control for drag reduction on blunt bodies. In this paper, rotation will be added to the cylinder alongside the deep reinforcement learning (DRL) algorithm, which uses multiple controlled jets to reach the maximum possible drag suppression. Characteristics of the DRL code, including controlling parameters, their limitations, and optimization of the DRL network for use with rotation will be presented. This work will focus on optimizing the number and positions of the jets, the sensors location, and the maximum allowed flow rate to jets in the form of the maximum allowed flow rate of each actuation and the total number of them per episode. It is found that combining the rotation and DRL is promising since it suppresses the vortex shedding, stabilizes the Karman vortex street, and reduces the drag coefficient by up to 49.75%. Also, it will be shown that having more sensors at more locations is not always a good choice and the sensor number and location should be determined based on the need of the user and corresponding configuration. Also, allowing the agent to have access to higher flow rates, mostly reduces the performance, except when the cylinder rotates. In all cases, the agent can keep the lift coefficient at a value near zero, or stabilize it at a smaller number.
</details>
<details>
<summary>摘要</summary>
真正的人工智能在强化学习中表现出真正的力量，因为它的动态性使其更加复杂。在活动流控中，旋转和注入是已知的降低拖力的方法。在这篇论文中，我们将在筒体上添加旋转，并与深度强化学习（DRL）算法结合使用多个控制的气流来达到最大可能的拖力降低。我们将展示DRL代码中的控制参数、其限制和优化DRL网络的方法，包括气流管道的数量和位置、感应器的位置和每个episode中的最大气流量。我们发现，将旋转和DRL结合使用是有前途的，因为它可以阻断旋转 shedding，稳定卡曼旋流street，并降低拖力系数至最多49.75%。此外，我们还发现，在某些情况下，添加更多的感应器并不总是有利，需要根据用户的需求和相应的配置来确定感应器的数量和位置。此外，允许机器人访问更高的气流量，通常会降低性能，除非筒体在旋转。在所有情况下，机器人都可以保持着降低的升力系数，或者稳定其在更小的数字上。
</details></li>
</ul>
<hr>
<h2 id="Spectral-Normalized-Cut-Graph-Partitioning-with-Fairness-Constraints"><a href="#Spectral-Normalized-Cut-Graph-Partitioning-with-Fairness-Constraints" class="headerlink" title="Spectral Normalized-Cut Graph Partitioning with Fairness Constraints"></a>Spectral Normalized-Cut Graph Partitioning with Fairness Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12065">http://arxiv.org/abs/2307.12065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiali2000/fnm">https://github.com/jiali2000/fnm</a></li>
<li>paper_authors: Jia Li, Yanhao Wang, Arpit Merchant</li>
<li>for: 本文目的是为了分解一个图的节点集 into $k$ 个彩色独立集，以最小化图中任何两个集之间的正规化连接值，同时保证每个属性分布在每个集中是约等的。</li>
<li>methods: 本文提出了一种两阶段的光谱算法，称为 FNM，用于实现公平分解。在第一阶段，我们添加了一个增强的拉格朗日函数基于我们的公平准则，以生成一个公平的光谱节点嵌入。在第二阶段，我们设计了一种圆拟方案，以生成 $k$ 个集从公平嵌入中生成高质量的分解。</li>
<li>results: 通过对九个标准数据集进行广泛的实验，我们证明了 FNM 比三种基准方法更高效。<details>
<summary>Abstract</summary>
Normalized-cut graph partitioning aims to divide the set of nodes in a graph into $k$ disjoint clusters to minimize the fraction of the total edges between any cluster and all other clusters. In this paper, we consider a fair variant of the partitioning problem wherein nodes are characterized by a categorical sensitive attribute (e.g., gender or race) indicating membership to different demographic groups. Our goal is to ensure that each group is approximately proportionally represented in each cluster while minimizing the normalized cut value. To resolve this problem, we propose a two-phase spectral algorithm called FNM. In the first phase, we add an augmented Lagrangian term based on our fairness criteria to the objective function for obtaining a fairer spectral node embedding. Then, in the second phase, we design a rounding scheme to produce $k$ clusters from the fair embedding that effectively trades off fairness and partition quality. Through comprehensive experiments on nine benchmark datasets, we demonstrate the superior performance of FNM compared with three baseline methods.
</details>
<details>
<summary>摘要</summary>
normalized-cut graph partitioning aimed to divide the set of nodes in a graph into $k$ disjoint clusters to minimize the fraction of the total edges between any cluster and all other clusters. In this paper, we considered a fair variant of the partitioning problem, where nodes were characterized by a categorical sensitive attribute (e.g., gender or race) indicating membership to different demographic groups. Our goal was to ensure that each group was approximately proportionally represented in each cluster while minimizing the normalized cut value. To resolve this problem, we proposed a two-phase spectral algorithm called FNM. In the first phase, we added an augmented Lagrangian term based on our fairness criteria to the objective function for obtaining a fairer spectral node embedding. Then, in the second phase, we designed a rounding scheme to produce $k$ clusters from the fair embedding that effectively trades off fairness and partition quality. Through comprehensive experiments on nine benchmark datasets, we demonstrated the superior performance of FNM compared with three baseline methods.
</details></li>
</ul>
<hr>
<h2 id="Balancing-Exploration-and-Exploitation-in-Hierarchical-Reinforcement-Learning-via-Latent-Landmark-Graphs"><a href="#Balancing-Exploration-and-Exploitation-in-Hierarchical-Reinforcement-Learning-via-Latent-Landmark-Graphs" class="headerlink" title="Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs"></a>Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12063">http://arxiv.org/abs/2307.12063</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/papercode2022/hill">https://github.com/papercode2022/hill</a></li>
<li>paper_authors: Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu</li>
<li>for: 这篇论文目的是提出一种可以解决循环对待问题的弹性问题决策学习方法，即 Hierarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL)。</li>
<li>methods: 这篇论文使用了一种名为 HILL 的方法，它使用了对抗表示学习目标来学习隐藏目标表示，然后使用这些表示来动态建立隐藏标签图和选择策略，以解决循环对待问题的问题。</li>
<li>results: 实验结果显示，HILL 比state-of-the-art基eline在缺乏对象奖励的连续控制任务上具有更高的样本效率和渐进性表现。<details>
<summary>Abstract</summary>
Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into subgoal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on subgoal representation functions and subgoal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent subgoal representations and lack an efficient subgoal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.
</details>
<details>
<summary>摘要</summary>
“对于受益从探索和实施的问题，叫做目标调整层次学习（GCHRL）是一种有前途的思路。它将源任务分解为子任务 conditional subtask，并在子任务空间进行探索和实施。GCHRL的有效性很大程度上取决于子任务表示函数和子任务选择策略。然而，现有的工作往往忽略GCHRL中的时间协调性在学习隐藏子任务表示时。此外，缺乏一个能够均衡探索和实施的子任务选择策略。本文提出了层次学习 via 动态建立隐藏地标 graphs（HILL）来解决这些限制。HILL使用了一个对照式表示学习目标来学习隐藏子任务表示，并在这些表示上动态建立隐藏地标 graphs。HILL还使用了节点上的新鲜度量和边上的实用度量。最后，HILL发展了一个子任务选择策略，考虑了这两个度量，以均衡探索和实施。实验结果显示，HILL在缺少奖励的粒子控制任务上比基于 estado-of-the-art 基eline 高效和长期性。我们的代码可以在 <https://github.com/papercode2022/HILL> 获取。”
</details></li>
</ul>
<hr>
<h2 id="Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations"><a href="#Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations" class="headerlink" title="Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations"></a>Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12062">http://arxiv.org/abs/2307.12062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Xiangyu Liu, Tuomas Sandholm, Furong Huang, Stephen McAleer</li>
<li>for: 本研究旨在训练能够在环境干扰或敌意攻击下表现良好的RL策略。</li>
<li>methods: 我们提出了GRAD方法，它将把 temporally-coupled 干扰视为一个部分可见二人零 SUM 游戏，通过查找这个游戏的approximate equilibria来确保代理人的强度对 temporally-coupled 干扰的Robustness。</li>
<li>results: 我们的提议方法在许多连续控制任务中实验证明了与基elines相比，具有显著的Robustness优势，包括对于标准和 temporally-coupled 干扰的攻击。<details>
<summary>Abstract</summary>
Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.
</details>
<details>
<summary>摘要</summary>
Strong reinforcement learning (RL) aims to train policies that can perform well under environmental perturbations or adversarial attacks. Existing methods typically assume that the space of possible perturbations remains the same across timesteps. However, in many situations, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a new challenge for existing robust RL methods. To address this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks show that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.Note: Simplified Chinese is used here, as it is the most widely used variety of Chinese in mainland China and Taiwan. Traditional Chinese is also commonly used, especially in Hong Kong and Macau.
</details></li>
</ul>
<hr>
<h2 id="Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units"><a href="#Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units" class="headerlink" title="Fast Knowledge Graph Completion using Graphics Processing Units"></a>Fast Knowledge Graph Completion using Graphics Processing Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12059">http://arxiv.org/abs/2307.12059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun-Hee Lee, Dong-oh Kang, Hwa Jeon Song</li>
<li>for: 这个论文的目的是提出一种高效的知识图完成框架，用于在GPU上获得新的关系。</li>
<li>methods: 该论文使用知识图嵌入模型，将知识图完成问题转化为一种相似Join问题，然后使用度量空间的性质来 derive 高速的完成算法。</li>
<li>results:  experiments 表明，该框架可以高效处理知识图完成问题。<details>
<summary>Abstract</summary>
Knowledge graphs can be used in many areas related to data semantics such as question-answering systems, knowledge based systems. However, the currently constructed knowledge graphs need to be complemented for better knowledge in terms of relations. It is called knowledge graph completion. To add new relations to the existing knowledge graph by using knowledge graph embedding models, we have to evaluate $N\times N \times R$ vector operations, where $N$ is the number of entities and $R$ is the number of relation types. It is very costly.   In this paper, we provide an efficient knowledge graph completion framework on GPUs to get new relations using knowledge graph embedding vectors. In the proposed framework, we first define "transformable to a metric space" and then provide a method to transform the knowledge graph completion problem into the similarity join problem for a model which is "transformable to a metric space". After that, to efficiently process the similarity join problem, we derive formulas using the properties of a metric space. Based on the formulas, we develop a fast knowledge graph completion algorithm. Finally, we experimentally show that our framework can efficiently process the knowledge graph completion problem.
</details>
<details>
<summary>摘要</summary>
知识图可以应用于数据 semantics 多个领域，如问答系统、知识基础系统。然而，目前构建的知识图需要补充以获得更好的知识，这被称为知识图完成。为添加新的关系到现有的知识图，我们需要评估 $N\times N \times R$ 矢量操作，其中 $N$ 是实体的数量，$R$ 是关系类型的数量。这很费时。在这篇论文中，我们提供了一个高效的知识图完成框架在 GPU 上来获得新关系使用知识图嵌入向量。我们首先定义 "可转换到一个度量空间"，然后提供一种将知识图完成问题转换成一个度量空间中的相似Join问题的方法。接着，我们使用度量空间的性质 deriv 出 formulas，并根据 formulas 开发了一个快速的知识图完成算法。最后，我们通过实验表示，我们的框架可以高效地处理知识图完成问题。
</details></li>
</ul>
<hr>
<h2 id="Exploring-MLOps-Dynamics-An-Experimental-Analysis-in-a-Real-World-Machine-Learning-Project"><a href="#Exploring-MLOps-Dynamics-An-Experimental-Analysis-in-a-Real-World-Machine-Learning-Project" class="headerlink" title="Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project"></a>Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13473">http://arxiv.org/abs/2307.13473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awadelrahman M. A. Ahmed<br>for:这个研究旨在优化机器学习操作（MLOps）过程，以提高机器学习项目的效率和生产力。methods:该实验使用了一个全面的 MLOps 工作流程，覆盖了问题定义、数据收集、数据准备、模型开发、模型部署、监测、管理、扩展性和合规遵守等重要阶段。实验还采用了一种系统化跟踪方法，以记录 especified 阶段之间的重复访问，以捕捉这些访问的原因。results:研究发现，MLOps 工作流程具有很强的融合和循环特性，并且具有很高的可重复性和可缩放性。通过对实验数据进行分析，提供了一些实践建议和推荐，以便在实际应用中进行进一步的优化和改进。<details>
<summary>Abstract</summary>
This article presents an experiment focused on optimizing the MLOps (Machine Learning Operations) process, a crucial aspect of efficiently implementing machine learning projects. The objective is to identify patterns and insights to enhance the MLOps workflow, considering its iterative and interdependent nature in real-world model development scenarios.   The experiment involves a comprehensive MLOps workflow, covering essential phases like problem definition, data acquisition, data preparation, model development, model deployment, monitoring, management, scalability, and governance and compliance. Practical tips and recommendations are derived from the results, emphasizing proactive planning and continuous improvement for the MLOps workflow.   The experimental investigation was strategically integrated within a real-world ML project which followed essential phases of the MLOps process in a production environment, handling large-scale structured data. A systematic tracking approach was employed to document revisits to specific phases from a main phase under focus, capturing the reasons for such revisits. By constructing a matrix to quantify the degree of overlap between phases, the study unveils the dynamic and iterative nature of the MLOps workflow.   The resulting data provides visual representations of the MLOps process's interdependencies and iterative characteristics within the experimental framework, offering valuable insights for optimizing the workflow and making informed decisions in real-world scenarios. This analysis contributes to enhancing the efficiency and effectiveness of machine learning projects through an improved MLOps process.   Keywords: MLOps, Machine Learning Operations, Optimization, Experimental Analysis, Iterative Process, Pattern Identification.
</details>
<details>
<summary>摘要</summary>
The experiment covers a comprehensive MLOps workflow, including problem definition, data acquisition, data preparation, model development, model deployment, monitoring, management, scalability, and governance and compliance. The results provide practical tips and recommendations for proactive planning and continuous improvement of the MLOps workflow.The experimental investigation was conducted within a real-world ML project, which followed the essential phases of the MLOps process in a production environment, handling large-scale structured data. A systematic tracking approach was employed to document revisits to specific phases, capturing the reasons for such revisits. By constructing a matrix to quantify the degree of overlap between phases, the study reveals the dynamic and iterative nature of the MLOps workflow.The resulting data provides visual representations of the MLOps process's interdependencies and iterative characteristics within the experimental framework, offering valuable insights for optimizing the workflow and making informed decisions in real-world scenarios. This analysis contributes to enhancing the efficiency and effectiveness of machine learning projects through an improved MLOps process.Keywords: MLOps, Machine Learning Operations, Optimization, Experimental Analysis, Iterative Process, Pattern Identification.
</details></li>
</ul>
<hr>
<h2 id="Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning"><a href="#Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning" class="headerlink" title="Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning"></a>Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12996">http://arxiv.org/abs/2307.12996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Lacombe, Andrew Gaut, Jeff He, David Lüdeke, Kateryna Pistunova</li>
<li>for: 本研究旨在将科学知识从文本中转移到分子图表示，以推进计算生物化学中深度学习的发展。</li>
<li>methods: 研究者使用了对比学习将神经图表示与文本描述的特征相对转移，以提高分子性质预测性能。他们还提出了一种基于有机反应的新型分子图数据生成策略。</li>
<li>results: 研究者在下游的分子网络Property Classification任务上实现了+4.26%的AUROC提升，比Graph模式alone模型提升+1.54%。这表明将科学知识从文本中转移到分子图表示可以提高分子性质预测性能。<details>
<summary>Abstract</summary>
Deep learning in computational biochemistry has traditionally focused on molecular graphs neural representations; however, recent advances in language models highlight how much scientific knowledge is encoded in text. To bridge these two modalities, we investigate how molecular property information can be transferred from natural language to graph representations. We study property prediction performance gains after using contrastive learning to align neural graph representations with representations of textual descriptions of their characteristics. We implement neural relevance scoring strategies to improve text retrieval, introduce a novel chemically-valid molecular graph augmentation strategy inspired by organic reactions, and demonstrate improved performance on downstream MoleculeNet property classification tasks. We achieve a +4.26% AUROC gain versus models pre-trained on the graph modality alone, and a +1.54% gain compared to recently proposed molecular graph/text contrastively trained MoMu model (Su et al. 2022).
</details>
<details>
<summary>摘要</summary>
深度学习在计算生物化学中传统上专注于分子图神经表示;然而，最近的语言模型发展显示了科学知识在文本中的含义。为了融合这两种模式，我们研究如何从自然语言中提取分子性质信息并将其传递到图表示中。我们使用对比学习对神经图表示和文本描述中的特征进行对齐，并使用神经相关性分数策略来提高文本检索。我们还介绍了一种基于有机反应的新型化学Graph augmentation策略，并在下游MoleculeNet性质分类任务上达到了+4.26% AUROC提升和+1.54%提升 compared to MoMu模型（Su et al., 2022）。
</details></li>
</ul>
<hr>
<h2 id="Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space"><a href="#Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space" class="headerlink" title="Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space"></a>Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12032">http://arxiv.org/abs/2307.12032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junzis/contrail-net">https://github.com/junzis/contrail-net</a></li>
<li>paper_authors: Junzi Sun, Esther Roosenbrand</li>
<li>for: 检测飞行 contrails 从卫星图像中</li>
<li>methods: 基于增强转移学习的新模型，以及一种新的损失函数 SR Loss</li>
<li>results: 准确地检测 contrails  WITH  minimal data<details>
<summary>Abstract</summary>
Air transport poses significant environmental challenges, particularly the contribution of flight contrails to climate change due to their potential global warming impact. Detecting contrails from satellite images has been a long-standing challenge. Traditional computer vision techniques have limitations under varying image conditions, and machine learning approaches using typical convolutional neural networks are hindered by the scarcity of hand-labeled contrail datasets and contrail-tailored learning processes. In this paper, we introduce an innovative model based on augmented transfer learning that accurately detects contrails with minimal data. We also propose a novel loss function, SR Loss, which improves contrail line detection by transforming the image space into Hough space. Our research opens new avenues for machine learning-based contrail detection in aviation research, offering solutions to the lack of large hand-labeled datasets, and significantly enhancing contrail detection models.
</details>
<details>
<summary>摘要</summary>
空中交通对环境造成重要挑战，特别是飞行烟尘的潜在全球暖化影响。从卫星图像探测飞行烟尘是一项长期挑战。传统的计算机视觉技术在不同的图像条件下有限制，机器学习方法使用 Typical convolutional neural networks 也受到手动标注飞行烟尘数据的罕见性和适应飞行烟尘学习过程的限制。在这篇论文中，我们介绍了一种创新的模型，基于增强传输学习，可以准确地检测飞行烟尘，只需 minimal data。我们还提出了一种新的损失函数，SR Loss，它通过将图像空间转换为截距空间，提高了飞行烟尘线检测。我们的研究打开了新的机器学习基于飞行烟尘检测的可能性，解决了航空研究中缺乏大量手动标注数据的问题，并显著提高了飞行烟尘检测模型。
</details></li>
</ul>
<hr>
<h2 id="FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models"><a href="#FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models" class="headerlink" title="FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models"></a>FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00065">http://arxiv.org/abs/2308.00065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweiyin/finpt">https://github.com/yuweiyin/finpt</a></li>
<li>paper_authors: Yuwei Yin, Yazheng Yang, Jian Yang, Qi Liu</li>
<li>for: 这研究旨在提出一种新的金融风险预测方法，以帮助金融机构更好地识别和预测风险。</li>
<li>methods: 该方法使用Profile Tuning技术，将大型预训模型粘贴到金融表格数据中，并通过提问大语言模型（LLMs）获取自然语言客户profile，进而进行预测。</li>
<li>results: 通过对FinBench数据集进行实验，研究人员发现FinPT方法可以与各种代表性的强基线进行比较，并且通过分析LLMs的性能，深入理解它们在金融风险预测中的应用。<details>
<summary>Abstract</summary>
Financial risk prediction plays a crucial role in the financial sector. Machine learning methods have been widely applied for automatically detecting potential risks and thus saving the cost of labor. However, the development in this field is lagging behind in recent years by the following two facts: 1) the algorithms used are somewhat outdated, especially in the context of the fast advance of generative AI and large language models (LLMs); 2) the lack of a unified and open-sourced financial benchmark has impeded the related research for years. To tackle these issues, we propose FinPT and FinBench: the former is a novel approach for financial risk prediction that conduct Profile Tuning on large pretrained foundation models, and the latter is a set of high-quality datasets on financial risks such as default, fraud, and churn. In FinPT, we fill the financial tabular data into the pre-defined instruction template, obtain natural-language customer profiles by prompting LLMs, and fine-tune large foundation models with the profile text to make predictions. We demonstrate the effectiveness of the proposed FinPT by experimenting with a range of representative strong baselines on FinBench. The analytical studies further deepen the understanding of LLMs for financial risk prediction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Flexible-Framework-for-Incorporating-Patient-Preferences-Into-Q-Learning"><a href="#A-Flexible-Framework-for-Incorporating-Patient-Preferences-Into-Q-Learning" class="headerlink" title="A Flexible Framework for Incorporating Patient Preferences Into Q-Learning"></a>A Flexible Framework for Incorporating Patient Preferences Into Q-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12022">http://arxiv.org/abs/2307.12022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua P. Zitovsky, Leslie Wilson, Michael R. Kosorok</li>
<li>for: 这篇论文是为了解决现实世界医疗问题中的多个竞争结果问题而写的，包括治疗效果和不良反应的严重程度。</li>
<li>methods: 这篇论文提出了一种新的方法，即Latent Utility Q-Learning（LUQ-Learning），以解决现有方法的限制，包括只能处理单个时间点和两个结果、不能 incorporate自报病人偏好等。LUQ-Learning 使用隐藏模型方法，自然地扩展 Q-learning 到复合结果设定下，并采取理想的质量评价来对各个病人进行评价。</li>
<li>results: 在基于低背痛的实验中，我们的方法与多种基线方法进行比较，并在所有实验中达到了非常竞争性的实验性表现。<details>
<summary>Abstract</summary>
In real-world healthcare problems, there are often multiple competing outcomes of interest, such as treatment efficacy and side effect severity. However, statistical methods for estimating dynamic treatment regimes (DTRs) usually assume a single outcome of interest, and the few methods that deal with composite outcomes suffer from important limitations. This includes restrictions to a single time point and two outcomes, the inability to incorporate self-reported patient preferences and limited theoretical guarantees. To this end, we propose a new method to address these limitations, which we dub Latent Utility Q-Learning (LUQ-Learning). LUQ-Learning uses a latent model approach to naturally extend Q-learning to the composite outcome setting and adopt the ideal trade-off between outcomes to each patient. Unlike previous approaches, our framework allows for an arbitrary number of time points and outcomes, incorporates stated preferences and achieves strong asymptotic performance with realistic assumptions on the data. We conduct simulation experiments based on an ongoing trial for low back pain as well as a well-known completed trial for schizophrenia. In all experiments, our method achieves highly competitive empirical performance compared to several alternative baselines.
</details>
<details>
<summary>摘要</summary>
在现实医疗问题中，常常存在多个竞争的目的结果，如治疗效果和副作用严重程度。然而，统计方法 для估计动态治疗方案（DTR）通常假设单一的目的结果，而其中几种方法只能处理单个时间点和两个结果。这些方法还具有限制性，例如不能 incorporate自报病人喜好和有限的理论保证。为此，我们提出了一种新的方法，我们称之为潜在用户价值Q学习（LUQ-Learning）。LUQ-Learning 使用潜在模型方法来自然地扩展Q学习到复合结果设定下，并采取每个患者的理想妥协。不同于前一些方法，我们的框架允许任意数量的时间点和结果，并 incorporate 自报病人喜好，并实现强 asymptotic performance 在现实数据下，只需要有限的假设。我们在一个低肢瘤痛试验和一个已完成的躁闹症试验中进行了 simulations experiments。在所有实验中，我们的方法与多个基准方法相比，表现出了非常竞争的实验性。
</details></li>
</ul>
<hr>
<h2 id="Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors"><a href="#Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors" class="headerlink" title="Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors"></a>Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12015">http://arxiv.org/abs/2307.12015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Maria Aiello, Mehrad Jaloli, Marzia Cescon</li>
<li>for: 这个研究是为了开发一个基于Linear Time-Varying（LTV）Model Predictive Control（MPC）框架的关闭循环胰岛素输送算法，用于治疗类型1 диабе尼（T1D）。</li>
<li>methods: 这个研究使用了一个数据驱动的多步预测血糖（BG）预测器，并将其与LTV MPC框架集成。而不是从数据中直接标定胰岛素逻辑系统的开放循环模型，这里提议直接使用BG预测器来预测未来的血糖水平。为非线性部分，使用了Long Short-Term Memory（LSTM）网络，而为线性部分，使用了线性回归模型。</li>
<li>results: 对于三个模拟场景，包括一个标准情况，一个随机餐食干扰情况，以及一个减少胰岛素敏感度25%的情况，我们证明了我们的LSTM-MPC控制器的优势。在随机餐食干扰情况下，我们的方法提供了更加准确的未来血糖水平预测，以及更好的封闭循环性能。<details>
<summary>Abstract</summary>
We present the design and \textit{in-silico} evaluation of a closed-loop insulin delivery algorithm to treat type 1 diabetes (T1D) consisting in a data-driven multi-step-ahead blood glucose (BG) predictor integrated into a Linear Time-Varying (LTV) Model Predictive Control (MPC) framework. Instead of identifying an open-loop model of the glucoregulatory system from available data, we propose to directly fit the entire BG prediction over a predefined prediction horizon to be used in the MPC, as a nonlinear function of past input-ouput data and an affine function of future insulin control inputs. For the nonlinear part, a Long Short-Term Memory (LSTM) network is proposed, while for the affine component a linear regression model is chosen. To assess benefits and drawbacks when compared to a traditional linear MPC based on an auto-regressive with exogenous (ARX) input model identified from data, we evaluated the proposed LSTM-MPC controller in three simulation scenarios: a nominal case with 3 meals per day, a random meal disturbances case where meals were generated with a recently published meal generator, and a case with 25$\%$ decrease in the insulin sensitivity. Further, in all the scenarios, no feedforward meal bolus was administered. For the more challenging random meal generation scenario, the mean $\pm$ standard deviation percent time in the range 70-180 [mg/dL] was 74.99 $\pm$ 7.09 vs. 54.15 $\pm$ 14.89, the mean $\pm$ standard deviation percent time in the tighter range 70-140 [mg/dL] was 47.78$\pm$8.55 vs. 34.62 $\pm$9.04, while the mean $\pm$ standard deviation percent time in sever hypoglycemia, i.e., $<$ 54 [mg/dl] was 1.00$\pm$3.18 vs. 9.45$\pm$11.71, for our proposed LSTM-MPC controller and the traditional ARX-MPC, respectively. Our approach provided accurate predictions of future glucose concentrations and good closed-loop performances of the overall MPC controller.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种关闭Loop抗糖尿病（T1D）的设计和 simulate evaluate 的数据驱动多步预测血糖（BG）预测算法，包括一个基于线性时变（LTV）模型预测控制（MPC）框架的数据驱动多步预测算法。而不是直接从可用数据中Identify一个开 Loop模型的glucoregulatory系统，我们提议直接将整个BG预测 horizon为用于MPC，作为非线性函数过去输入输出数据和未来药物控制输入的非线性函数。 для非线性部分，我们提议使用一个Long Short-Term Memory（LSTM）网络，而对于线性部分，我们选择了一个线性回归模型。为了评估我们提议的LSTM-MPC控制器与传统的ARX-MPC控制器相比，我们在三个模拟场景中评估了这两个控制器的表现：一个标准的3餐/天场景，一个随机餐品干扰场景，以及一个25%的药物敏感度下降场景。此外，在所有场景中，没有feedforward餐品补偿。在更加复杂的随机餐品生成场景中，LSTM-MPC控制器的mean±标准差%时间在70-180[mg/dL]范围内为74.99±7.09 vs. 54.15±14.89，mean±标准差%时间在70-140[mg/dL]范围内为47.78±8.55 vs. 34.62±9.04，而且mean±标准差%时间在严重低血糖（<54[mg/dL]）下为1.00±3.18 vs. 9.45±11.71。我们的方法提供了精准的未来血糖浓度预测和关闭Loop控制器的全面性能的良好表现。
</details></li>
</ul>
<hr>
<h2 id="NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details"><a href="#NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details" class="headerlink" title="NLCUnet: Single-Image Super-Resolution Network with Hairline Details"></a>NLCUnet: Single-Image Super-Resolution Network with Hairline Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12014">http://arxiv.org/abs/2307.12014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancong Feng, Yuan-Gen Wang, Fengchuang Xing</li>
<li>For: 提高单张超解像图像质量，特别是细节部分的精度。* Methods: 提出了一种基于非本地注意力的单张超解像网络（NLCUnet），包括三个核心设计：非本地注意力机制、深度卷积 convolution 和通道注意力。* Results: 在DF2K dataset上进行了许多实验，发现 NLCUnet 在 PSNR 和 SSIM 指标上比现有方法提高较多，并且可以保持更好的细节部分。<details>
<summary>Abstract</summary>
Pursuing the precise details of super-resolution images is challenging for single-image super-resolution tasks. This paper presents a single-image super-resolution network with hairline details (termed NLCUnet), including three core designs. Specifically, a non-local attention mechanism is first introduced to restore local pieces by learning from the whole image region. Then, we find that the blur kernel trained by the existing work is unnecessary. Based on this finding, we create a new network architecture by integrating depth-wise convolution with channel attention without the blur kernel estimation, resulting in a performance improvement instead. Finally, to make the cropped region contain as much semantic information as possible, we propose a random 64$\times$64 crop inside the central 512$\times$512 crop instead of a direct random crop inside the whole image of 2K size. Numerous experiments conducted on the benchmark DF2K dataset demonstrate that our NLCUnet performs better than the state-of-the-art in terms of the PSNR and SSIM metrics and yields visually favorable hairline details.
</details>
<details>
<summary>摘要</summary>
推进超高清照片的精确细节是单图超解像 зада务中的挑战。本文提出了一个单图超解像网络（NLCUnet），包括三个核心设计。具体来说，我们首先引入非本地注意力机制，以便通过整个图像区域学习地址本地副本。然后，我们发现现有工作中训练的模糊核心不是必需的，因此我们创建了一个新的网络架构，通过depthwise核论和通道注意力来提高性能。最后，我们提议在中心256×256区域中随机选择64×64区域，以便尽可能包含图像中的semantic信息。在DF2K数据集上进行了多次实验，表明我们的NLCUnet在PSNR和SSIM指标上比state-of-the-art更高，并且视觉上具有更好的毛细膨胀细节。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Self-Supervised-Learning-Based-Approach-for-Patient-Similarity-A-Case-Study-on-Atrial-Fibrillation-Detection-from-PPG-Signal"><a href="#Contrastive-Self-Supervised-Learning-Based-Approach-for-Patient-Similarity-A-Case-Study-on-Atrial-Fibrillation-Detection-from-PPG-Signal" class="headerlink" title="Contrastive Self-Supervised Learning Based Approach for Patient Similarity: A Case Study on Atrial Fibrillation Detection from PPG Signal"></a>Contrastive Self-Supervised Learning Based Approach for Patient Similarity: A Case Study on Atrial Fibrillation Detection from PPG Signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02433">http://arxiv.org/abs/2308.02433</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/subangkar/simsig">https://github.com/subangkar/simsig</a></li>
<li>paper_authors: Subangkar Karmaker Shanto, Shoumik Saha, Atif Hasan Rahman, Mohammad Mehedy Masud, Mohammed Eunus Ali</li>
<li>for: 这个论文是为了提出一种基于对比学习的深度学习框架，用于搜索基于生物 физи学信号的病人相似性。</li>
<li>methods: 这个框架使用对比学习方法来学习病人的相似embedding，并引入了一些邻居选择算法来确定生成embedding上的最高相似性。</li>
<li>results: 作者通过对一个涉及到心脏病的案例研究，证明了该框架的有效性。实验结果表明，该框架可以准确地检测心脏病AF，并且与其他基线方法相比，其性能更高。<details>
<summary>Abstract</summary>
In this paper, we propose a novel contrastive learning based deep learning framework for patient similarity search using physiological signals. We use a contrastive learning based approach to learn similar embeddings of patients with similar physiological signal data. We also introduce a number of neighbor selection algorithms to determine the patients with the highest similarity on the generated embeddings. To validate the effectiveness of our framework for measuring patient similarity, we select the detection of Atrial Fibrillation (AF) through photoplethysmography (PPG) signals obtained from smartwatch devices as our case study. We present extensive experimentation of our framework on a dataset of over 170 individuals and compare the performance of our framework with other baseline methods on this dataset.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种基于对比学习的深度学习框架，用于通过生物物理信号来查找病人相似性。我们使用对比学习方法来学习病人的相似 embedding，并引入了一些邻居选择算法来确定生成 embedding 中最相似的病人。为了证明我们的框架的有效性，我们选择了基于 photoplethysmography (PPG) 信号检测 Atrial Fibrillation (AF) 为我们的案例研究。我们对一个包含超过 170 个个体的数据集进行了广泛的实验，并与其他基线方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering"><a href="#Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering" class="headerlink" title="Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering"></a>Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11986">http://arxiv.org/abs/2307.11986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/holipori/mimic-diff-vqa">https://github.com/holipori/mimic-diff-vqa</a></li>
<li>paper_authors: Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M. Summers, Yingying Zhu</li>
<li>for: 这 paper 的目的是提出一个新的胸部X射影差异视觉问答任务 (VQA)，以帮助自动化医疗视觉语言模型。</li>
<li>methods: 这 paper 使用了一种新的专家知识感知图表学习模型，将图像差异视觉问答任务解决。该模型利用了 анатомиче结构优先知识、semantic知识和空间知识等专家知识来构建多关系图，表示图像差异的问答任务。</li>
<li>results: 这 paper 收集了一个新的数据集，名为 MIMIC-Diff-VQA，包含 700,703 个问答对from 164,324 对主要和参考图像。与现有的医疗 VQA 数据集相比，这些问题更加适合临床诊断实践中的诊断- intervene-评估过程。<details>
<summary>Abstract</summary>
To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a multi-relationship graph, representing the image differences between two images for the image difference VQA task. The dataset and code can be found at https://github.com/Holipori/MIMIC-Diff-VQA. We believe this work would further push forward the medical vision language model.
</details>
<details>
<summary>摘要</summary>
为了让医疗视语言模型自动化，我们提出了一个新的胸部X射影异常视问答（VQA）任务。给定一对主要和参考图像，这个任务的目标是回答一些疾病和图像之间的异常问题。这与医生诊断实践相一致，即将当前图像与参考图像进行比较，以确定报告。我们收集了一个新的数据集，即MIMIC-Diff-VQA，包含700703个问答对 from 164324对主要和参考图像。与现有的医学VQA数据集相比，我们的问题更加适合医生在诊断过程中采用的评估-诊断- interven-评估（ADIE）治疗流程。此外，我们还提出了一种基于专家知识的图像异常关系学习模型，以解决这个任务。我们的基eline模型利用专家知识，如生物结构优先知识、semantic知识和空间知识，构建多关系图，表示图像之间的异常关系。数据集和代码可以在https://github.com/Holipori/MIMIC-Diff-VQA中找到。我们认为这项工作将会进一步推动医学视语言模型的发展。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Graph-Neural-Networks-for-Attributed-Network-Embedding"><a href="#Collaborative-Graph-Neural-Networks-for-Attributed-Network-Embedding" class="headerlink" title="Collaborative Graph Neural Networks for Attributed Network Embedding"></a>Collaborative Graph Neural Networks for Attributed Network Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11981">http://arxiv.org/abs/2307.11981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiaoyut/conn">https://github.com/qiaoyut/conn</a></li>
<li>paper_authors: Qiaoyu Tan, Xin Zhang, Xiao Huang, Hao Chen, Jundong Li, Xia Hu<br>for: This paper focuses on developing a new graph neural network (GNN) architecture called COllaborative graph Neural Networks (CONN) to improve attribute network embedding.methods: The proposed CONN architecture uses selective message diffusion and cross-correlation to jointly reconstruct node-to-node and node-to-attribute-category interactions, which enhances the model’s capacity.results: The experimental results on real-world networks show that CONN outperforms state-of-the-art embedding algorithms with a significant margin.<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown prominent performance on attributed network embedding. However, existing efforts mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node features at the initial layer. This simple strategy impedes the potential of node attributes in augmenting node connections, leading to limited receptive field for inactive nodes with few or even no neighbors. Furthermore, the training objectives (i.e., reconstructing network structures) of most GNNs also do not include node attributes, although studies have shown that reconstructing node attributes is beneficial. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including graph convolution operations and training objectives. However, this is a nontrivial task since an appropriate way of integration is required to maintain the merits of GNNs. To bridge the gap, in this paper, we propose COllaborative graph Neural Networks--CONN, a tailored GNN architecture for attribute network embedding. It improves model capacity by 1) selectively diffusing messages from neighboring nodes and involved attribute categories, and 2) jointly reconstructing node-to-node and node-to-attribute-category interactions via cross-correlation. Experiments on real-world networks demonstrate that CONN excels state-of-the-art embedding algorithms with a great margin.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) 有出色表现在嵌入属性网络中。然而，现有努力主要是利用网络结构，而忽视节点特征的利用，只是将节点特征作为初始层节点特征使用。这种简单的策略限制了无活节点的潜在范围，因为它们有少量或甚至没有邻居。此外，大多数 GNN 的训练目标（即重建网络结构）并不包括节点特征，尽管研究表明重建节点特征有利。因此，深入涉及节点特征在 GNN 的关键组件中是一项挑战，需要避免降低 GNN 的优点。为了bridging这个差距，在这篇论文中，我们提出了协同图 neural Networks（CONN），一种针对嵌入属性网络的特化 GNN 架构。它提高了模型容量，通过1) 选择性地往返邻居节点和涉及属性类别中传递消息，2) 并同时重建节点到节点和节点到属性类别的交互。实验表明，CONN 在实际网络上超过了当前领先 embedding 算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model"><a href="#Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model" class="headerlink" title="Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model"></a>Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11980">http://arxiv.org/abs/2307.11980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayang Wang, Srivathsa Pasumarthi, Greg Zaharchuk, Ryan Chamberlain</li>
<li>for: 这个研究旨在提出一种基于卷积神经网络的图像合成方法，以实现不同剂量水平的对照增强图像的生成，以便为MRI成像中的医学应用提供更好的依据。</li>
<li>methods: 该方法基于一种名为Gformer的变换器，其包括一种抽样基于注意力机制和一种旋转 shift模块，以捕捉不同对照增强特征。</li>
<li>results: 对比其他状态艺技术，该方法的评估结果表明其性能更高。此外，该方法还在下游任务中，如剂量减少和肿瘤分割中进行了评估，以证明其在临床应用中的价值。<details>
<summary>Abstract</summary>
Deep learning (DL) based contrast dose reduction and elimination in MRI imaging is gaining traction, given the detrimental effects of Gadolinium-based Contrast Agents (GBCAs). These DL algorithms are however limited by the availability of high quality low dose datasets. Additionally, different types of GBCAs and pathologies require different dose levels for the DL algorithms to work reliably. In this work, we formulate a novel transformer (Gformer) based iterative modelling approach for the synthesis of images with arbitrary contrast enhancement that corresponds to different dose levels. The proposed Gformer incorporates a sub-sampling based attention mechanism and a rotational shift module that captures the various contrast related features. Quantitative evaluation indicates that the proposed model performs better than other state-of-the-art methods. We further perform quantitative evaluation on downstream tasks such as dose reduction and tumor segmentation to demonstrate the clinical utility.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）基于对比剂量减少和消除在MRI成像中得到了进一步的发展，因为Gadolinium-based Contrast Agents（GBCAs）的负面效应。但这些DL算法受到高质量低剂量数据的有效性的限制。此外，不同类型的GBCAs和疾病需要不同的剂量水平以便DL算法可靠地工作。在这种工作中，我们提出了一种基于转换器（Gformer）的迭代模型方法，用于生成具有任意对比强化的图像。我们的Gformer模型包括子抽样基于注意力机制和旋转变换模块，以捕捉不同的对比相关特征。量化评估表明，我们提出的模型在其他状态当前的方法之上表现出了更好的性能。我们进一步进行了下游任务如剂量减少和肿瘤分割，以证明临床实用性。
</details></li>
</ul>
<hr>
<h2 id="Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels"><a href="#Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels" class="headerlink" title="Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?"></a>Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11978">http://arxiv.org/abs/2307.11978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cewu/ptnl">https://github.com/cewu/ptnl</a></li>
<li>paper_authors: Cheng-En Wu, Yu Tian, Haichao Yu, Heng Wang, Pedro Morgado, Yu Hen Hu, Linjie Yang</li>
<li>for: 研究了CLIP模型在干预几个示例下调整为新的分类任务中的稳定性。</li>
<li>methods: 使用了几个示例来调整CLIP模型，并发现这种方法具有很高的抗噪性。</li>
<li>results: 发现了两个关键因素导致这种方法的稳定性：1）固定的类名Token提供了模型优化过程中强制的正则化，减少了噪音样本引起的梯度; 2）从多样化和通用的网络数据中学习的强大预训练图文映射提供了图像分类的强大先验知识。此外，我们还示出了使用CLIP模型自己的噪音零例预测来调整其自己的提示，可以显著提高无监督下的预测精度。代码可以在<a target="_blank" rel="noopener" href="https://github.com/CEWu/PTNL%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CEWu/PTNL中找到。</a><details>
<summary>Abstract</summary>
Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CEWu/PTNL.
</details>
<details>
<summary>摘要</summary>
CLIP类的视觉语言模型通过大规模训练学习一个通用的文本图像嵌入。一个视觉语言模型可以通过几个shot提问调整到新的分类任务。我们发现这种提问调整过程具有高度的鲁棒性，这使我们感到感兴趣，并且想 deeper 地研究这种特性的原因。我们进行了广泛的实验，并发现关键因素有两个：1）固定的类名token提供了模型优化的强制性，减少了噪音样本引起的梯度；2）通过多种和通用的网络数据学习的强大预训练图像文本嵌入，为图像分类提供了强大的先验知识。此外，我们示出了使用CLIP生成的噪音零shot预测来调整其自己的提问，可以大幅提高无监督下的预测精度。代码可以在https://github.com/CEWu/PTNL 中找到。
</details></li>
</ul>
<hr>
<h2 id="Out-of-Distribution-Optimality-of-Invariant-Risk-Minimization"><a href="#Out-of-Distribution-Optimality-of-Invariant-Risk-Minimization" class="headerlink" title="Out-of-Distribution Optimality of Invariant Risk Minimization"></a>Out-of-Distribution Optimality of Invariant Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11972">http://arxiv.org/abs/2307.11972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shoji Toyota, Kenji Fukumizu</li>
<li>for: 提高深度神经网络的泛化能力，即使在未经见过的领域下也能准确预测。</li>
<li>methods: 使用偏向风险最小化（IRM）方法，解决深度神经网络继承训练数据中嵌入的假 correlations 问题，以提高模型的泛化能力。</li>
<li>results: 提供了一种理论保证，表明在满足certain conditions下，bi-level optimization problem的解决方案会最小化异常风险。<details>
<summary>Abstract</summary>
Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem. While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established. Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to the bi-level optimization problem minimizes the o.o.d. risk under certain conditions. The result also provides sufficient conditions on distributions providing training data and on a dimension of feature space for the bi-leveled optimization problem to minimize the o.o.d. risk.
</details>
<details>
<summary>摘要</summary>
深度神经网络经常会继承训练数据中嵌入的假 correlations，从而导致在未看到的领域中失败，这些领域的分布与训练数据的分布不同。M. Arjovsky等人（2019）引入了 OUT-OF-DISTRIBUTION（o.o.d）风险，它是所有领域的最大风险，并将嵌入在训练数据中的假 correlations 问题定义为一个 minimization 问题。不变risk Minimization (IRM) 被视为一种有前景的方法来减少 o.o.d. 风险：IRM 通过解决一个二级优化问题来估算 o.o.d. 风险的最小值。虽然 IRM 在实际中得到了广泛的关注并取得了一些成功，但它具有少量的理论保证。特别是，一个坚实的理论保证，即二级优化问题的解决方案实际上是 o.o.d. 风险的最小值，尚未被成功地建立。本文通过坚实的理论证明，解决二级优化问题可以减少 o.o.d. 风险，并提供了一些有关训练数据的分布和特征空间维度的充分条件。
</details></li>
</ul>
<hr>
<h2 id="DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation"><a href="#DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation"></a>DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11960">http://arxiv.org/abs/2307.11960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/dhc">https://github.com/xmed-lab/dhc</a></li>
<li>paper_authors: Haonan Wang, Xiaomeng Li</li>
<li>for: 这个研究的目的是提出一个基于 semi-supervised learning (SSL) 的三维医疗影像分类框架，以解决对于医疗影像分类的专家需求和时间耗费问题。</li>
<li>methods: 这个框架使用了一个新的 Dual-debiased Heterogeneous Co-training (DHC) 方法，包括两种损失衡量策略：Distribution-aware Debiased Weighting (DistDW) 和 Difficulty-aware Debiased Weighting (DiffDW)，这些策略可以动态地使用 Pseudo 标签来导引模型解决数据和学习偏见。</li>
<li>results: 实验结果显示，提出的方法可以将 pseudo 标签用于偏见调整和纠正阶层分类问题，并且与现有的 SSL 方法比较，显示出我们的方法在更加具体的 SSL 设定下表现更好。代码和模型可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/DHC">https://github.com/xmed-lab/DHC</a>.<details>
<summary>Abstract</summary>
The volume-wise labeling of 3D medical images is expertise-demanded and time-consuming; hence semi-supervised learning (SSL) is highly desirable for training with limited labeled data. Imbalanced class distribution is a severe problem that bottlenecks the real-world application of these methods but was not addressed much. Aiming to solve this issue, we present a novel Dual-debiased Heterogeneous Co-training (DHC) framework for semi-supervised 3D medical image segmentation. Specifically, we propose two loss weighting strategies, namely Distribution-aware Debiased Weighting (DistDW) and Difficulty-aware Debiased Weighting (DiffDW), which leverage the pseudo labels dynamically to guide the model to solve data and learning biases. The framework improves significantly by co-training these two diverse and accurate sub-models. We also introduce more representative benchmarks for class-imbalanced semi-supervised medical image segmentation, which can fully demonstrate the efficacy of the class-imbalance designs. Experiments show that our proposed framework brings significant improvements by using pseudo labels for debiasing and alleviating the class imbalance problem. More importantly, our method outperforms the state-of-the-art SSL methods, demonstrating the potential of our framework for the more challenging SSL setting. Code and models are available at: https://github.com/xmed-lab/DHC.
</details>
<details>
<summary>摘要</summary>
医学三维图像的体积级标注是专业技术和时间consuming的;因此使用限制标注数据的 semi-supervised learning (SSL) 是非常有优点的。然而，实际应用中存在严重的类别分布不均问题，这个问题未得到充分关注。为解决这个问题，我们提出了一种新的双向偏置共训（DHC）框架，用于 semi-supervised 三维医学图像分割。我们提出了两种损失补偿策略，即 Distribution-aware Debiased Weighting（DistDW）和 Difficulty-aware Debiased Weighting（DiffDW），这两种策略可以动态使用 pseudo labels 来引导模型解决数据和学习偏见。我们的框架在合作这两个多样和准确的子模型时得到了显著改进。我们还提出了更加代表性的 semi-supervised 医学图像分割 benchmark，可以全面展示我们的类别偏见设计的效果。实验表明，我们的提议的框架可以通过使用 pseudo labels 进行偏见修正和缓解类别偏见问题，并且超越了当前状态的 SSL 方法，表明了我们的框架在更加挑战的 SSL 设定下的潜在力量。代码和模型可以在 GitHub 上找到：https://github.com/xmed-lab/DHC。
</details></li>
</ul>
<hr>
<h2 id="Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection"><a href="#Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection" class="headerlink" title="Multi-representations Space Separation based Graph-level Anomaly-aware Detection"></a>Multi-representations Space Separation based Graph-level Anomaly-aware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12994">http://arxiv.org/abs/2307.12994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fu Lin, Haonan Gong, Mingkang Li, Zitong Wang, Yue Zhang, Xuexiong Luo</li>
<li>for: 本研究的目标是检测图DataSet中的异常图。</li>
<li>methods: 我们提出了一种基于多个表示空间分离的图级异常检测框架。为了考虑不同类型的异常图数据的重要性，我们设计了一个异常感知模块来学习特定的节点级和图级异常重要性。此外，我们学习了严格地分离正常和异常图表示空间，通过四种不同的权重图表示对比彼此。</li>
<li>results: 我们对基eline方法进行了广泛的评估，并通过十个公共图数据集来评估我们的方法。结果表明，我们的方法具有效果。<details>
<summary>Abstract</summary>
Graph structure patterns are widely used to model different area data recently. How to detect anomalous graph information on these graph data has become a popular research problem. The objective of this research is centered on the particular issue that how to detect abnormal graphs within a graph set. The previous works have observed that abnormal graphs mainly show node-level and graph-level anomalies, but these methods equally treat two anomaly forms above in the evaluation of abnormal graphs, which is contrary to the fact that different types of abnormal graph data have different degrees in terms of node-level and graph-level anomalies. Furthermore, abnormal graphs that have subtle differences from normal graphs are easily escaped detection by the existing methods. Thus, we propose a multi-representations space separation based graph-level anomaly-aware detection framework in this paper. To consider the different importance of node-level and graph-level anomalies, we design an anomaly-aware module to learn the specific weight between them in the abnormal graph evaluation process. In addition, we learn strictly separate normal and abnormal graph representation spaces by four types of weighted graph representations against each other including anchor normal graphs, anchor abnormal graphs, training normal graphs, and training abnormal graphs. Based on the distance error between the graph representations of the test graph and both normal and abnormal graph representation spaces, we can accurately determine whether the test graph is anomalous. Our approach has been extensively evaluated against baseline methods using ten public graph datasets, and the results demonstrate its effectiveness.
</details>
<details>
<summary>摘要</summary>
GRAPH结构模式在近期内广泛应用于不同领域的数据模型中。检测图数据中异常Graph信息已成为一个流行的研究问题。我们的研究 objective 是 centered 在特定的问题上，即如何在图数据中检测异常图。前一些研究发现，异常图主要表现为节点级别和图级别异常，但这些方法很容易对两种异常形态进行等效的评估，这与实际情况不符。此外，一些异常图具有轻微异常特征，容易被现有方法排除。因此，我们提出了一个基于多个 Representation space 的图级别异常检测框架。为了考虑不同的节点级别和图级别异常的重要性，我们设计了一个异常检测模块，以学习特定的节点级别和图级别异常之间的权重。此外，我们通过四种不同类型的权重化图表示对之间的竞争学习，以学习纯正的正常图表示空间和异常图表示空间。通过测试图表示与正常图表示空间和异常图表示空间之间的距离错误来准确判断测试图是否异常。我们的方法在比基线方法进行evaluate 后得到了显著的效果。
</details></li>
</ul>
<hr>
<h2 id="High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization"><a href="#High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization" class="headerlink" title="High-performance real-world optical computing trained by in situ model-free optimization"></a>High-performance real-world optical computing trained by in situ model-free optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11957">http://arxiv.org/abs/2307.11957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyuan Zhao, Xin Shu, Renjie Zhou</li>
<li>for: 提高光学计算系统的高速和低能耗数据处理能力，并解决 simulation-to-reality gap。</li>
<li>methods: 使用 score gradient estimation 算法，对光学系统进行模型独立优化，不需要 computation-heavy 和偏见的系统模拟。</li>
<li>results: 在 MNIST 和 FMNIST 数据集上实现了高精度分类，并在无图像和高速细胞分析中展示了潜在的应用前景。<details>
<summary>Abstract</summary>
Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
</details>
<details>
<summary>摘要</summary>
光学计算系统可以提供高速和低能耗数据处理，但面临 computationally demanding 训练和实际-模拟之间的差距。我们提出了一种模型自由的解决方案，基于分布式权重的排名预测算法，用于优化光学计算系统。这种方法将系统视为黑盒子，直接从损失函数反射到光学权重的概率分布，因此不需要计算负担重和偏见的系统模拟。我们通过对单层散射光学计算系统进行实验，在 MNIST 和 FMNIST 数据集上达到了更高的分类精度。此外，我们还示出了无图像和高速细胞分析的潜在可能性。我们的提议的简单性和计算资源的低需求，使得光学计算从实验室示范转移到实际应用变得更加容易。
</details></li>
</ul>
<hr>
<h2 id="Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System"><a href="#Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System" class="headerlink" title="Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System"></a>Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02420">http://arxiv.org/abs/2308.02420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Sinclair, Kayla Kautai, Seyed Reza Shahamiri</li>
<li>for: 这个研究的目的是为了开发一个可靠且低成本的手机应用程序，可以在实时进行运动重复计数。</li>
<li>methods: 这个研究使用了深度学习技术，搭配手机摄像头进行运动重复计数。系统包括五个组件：（1）姿势估计、（2）阈值分类、（3）流动性、（4）状态机器、（5）计数器。</li>
<li>results: 这个系统在实际测试中精度高达98.89%，并且在预先录影的数据集中也达到98.85%的准确性。这使得这个系统成为一个有效、低成本且便捷的选择，不需要特殊的仪器或网络连接。<details>
<summary>Abstract</summary>
Automated exercise repetition counting has applications across the physical fitness realm, from personal health to rehabilitation. Motivated by the ubiquity of mobile phones and the benefits of tracking physical activity, this study explored the feasibility of counting exercise repetitions in real-time, using only on-device inference, on smartphones. In this work, after providing an extensive overview of the state-of-the-art automatic exercise repetition counting methods, we introduce a deep learning based exercise repetition counting system for smartphones consisting of five components: (1) Pose estimation, (2) Thresholding, (3) Optical flow, (4) State machine, and (5) Counter. The system is then implemented via a cross-platform mobile application named P\=uioio that uses only the smartphone camera to track repetitions in real time for three standard exercises: Squats, Push-ups, and Pull-ups. The proposed system was evaluated via a dataset of pre-recorded videos of individuals exercising as well as testing by subjects exercising in real time. Evaluation results indicated the system was 98.89% accurate in real-world tests and up to 98.85% when evaluated via the pre-recorded dataset. This makes it an effective, low-cost, and convenient alternative to existing solutions since the proposed system has minimal hardware requirements without requiring any wearable or specific sensors or network connectivity.
</details>
<details>
<summary>摘要</summary>
自动化的运动重复计数有各种应用在身体健身和重建领域，从个人健康到rehabilitation。为了利用移动电话的普遍性和跟踪物理活动的利点，这项研究探索了使用移动电话上的只有设备推理来实时计数运动重复的可能性。在这项研究中，我们首先提供了现有自动运动重复计数方法的广泛概述，然后引入了一种基于深度学习的运动重复计数系统，该系统由五个组成部分：（1）姿势估计，（2）阈值分割，（3）Optical flow，（4）状态机和（5）计数器。这个系统然后通过一个跨平台移动应用程序 named P\=uioio 实现，该应用程序使用了移动电话摄像头来实时跟踪运动重复，并对三种标准运动进行测试：蹲squats，推push-ups和抓pull-ups。我们对这个系统进行了一系列测试和评估，测试结果表明该系统在实际测试中的准确率达98.89%，并且在预录视频数据集上的评估结果为98.85%。这使得该系统成为一个有效、低成本、方便的替代方案，因为它没有特殊的硬件需求，也没有需要佩戴式设备或特殊的传感器或网络连接。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Interpretation-of-Importance-Weight-Aware-Updates"><a href="#Implicit-Interpretation-of-Importance-Weight-Aware-Updates" class="headerlink" title="Implicit Interpretation of Importance Weight Aware Updates"></a>Implicit Interpretation of Importance Weight Aware Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11955">http://arxiv.org/abs/2307.11955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keyi Chen, Francesco Orabona</li>
<li>for: 这篇论文主要是为了解释importance weight aware（IWA）更新法的性能优劣。</li>
<li>methods: 论文使用了一种新的框架，即通用隐式跟踪领导者（FTRL），来分析通用隐式更新法。</li>
<li>results: 论文表明，IWA更新法在在线学习设置中具有更好的 regret upper bound，比plain gradient更新法更好。<details>
<summary>Abstract</summary>
Due to its speed and simplicity, subgradient descent is one of the most used optimization algorithms in convex machine learning algorithms. However, tuning its learning rate is probably its most severe bottleneck to achieve consistent good performance. A common way to reduce the dependency on the learning rate is to use implicit/proximal updates. One such variant is the Importance Weight Aware (IWA) updates, which consist of infinitely many infinitesimal updates on each loss function. However, IWA updates' empirical success is not completely explained by their theory. In this paper, we show for the first time that IWA updates have a strictly better regret upper bound than plain gradient updates in the online learning setting. Our analysis is based on the new framework, generalized implicit Follow-the-Regularized-Leader (FTRL) (Chen and Orabona, 2023), to analyze generalized implicit updates using a dual formulation. In particular, our results imply that IWA updates can be considered as approximate implicit/proximal updates.
</details>
<details>
<summary>摘要</summary>
由于其速度和简洁性，剪梯下降是机器学习中最常用的优化算法之一。然而，调整学习率是它最严重的瓶颈，以实现一致的好表现。一种常见的方法是使用隐式/辅助更新。一种such variant是重要性评估（IWA）更新，它们包括无限多个infinitesimal更新。然而， IWA更新的实际成功并不完全由其理论来解释。在这篇论文中，我们展示了IWA更新在在线学习 Setting中具有更好的 regret upper bound，比普通的梯度更新更好。我们的分析基于新的框架，通用隐式 Follow-the-Regularized-Leader（FTRL）（Chen和Orabona，2023），用于分析通用隐式更新。特别是，我们的结果表明，IWA更新可以被视为approximate隐式/辅助更新。
</details></li>
</ul>
<hr>
<h2 id="On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs"><a href="#On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs" class="headerlink" title="On-Robot Bayesian Reinforcement Learning for POMDPs"></a>On-Robot Bayesian Reinforcement Learning for POMDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11954">http://arxiv.org/abs/2307.11954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Nguyen, Sammie Katt, Yuchen Xiao, Christopher Amato</li>
<li>for: 这篇论文的目的是提出一种专门适用于物理系统的 bayesian 强化学习方法，以解决 robot 学习中的数据成本问题。</li>
<li>methods: 该方法使用了一种特殊的 factored 表示方法，以捕捉专家知识，并使用 Monte-Carlo tree search 和 particle filtering 来解决 posterior 的推理问题。</li>
<li>results: 在两个人机交互任务中，该方法可以在几个实际世界回合后达到 near-optimal 性能，并且可以利用 typical low-level robot simulators 和处理未知环境的不确定性。<details>
<summary>Abstract</summary>
Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach can, for example, utilize typical low-level robot simulators and handle uncertainty over unknown dynamics of the environment. We empirically demonstrate its efficiency by performing on-robot learning in two human-robot interaction tasks with uncertainty about human behavior, achieving near-optimal performance after only a handful of real-world episodes. A video of learned policies is at https://youtu.be/H9xp60ngOes.
</details>
<details>
<summary>摘要</summary>
机器人学习通常困难由于数据收集成本高昂。为了解决这问题，我们可以采用有效的算法和利用机器人动力学专家的知识。 bayesian reinforcement learning（BRL）因其样本效率高和可以利用先验知识而成为一种适用的解决方案。然而，BRL在应用中受到了专家知识表示和推理问题的限制。本文提出了一种特殊的框架，用于physical systems。我们通过 capture this knowledge in a factored representation，然后证明 posterior factorizes in a similar shape，并 ultimately formalize the model in a Bayesian framework。然后，我们引入了一种基于Monte-Carlo tree search和particle filtering的在线解决方法，专门用于解决这个模型。这种方法可以利用 typical low-level robot simulators and handle uncertainty over unknown dynamics of the environment。我们通过在两个人机交互任务中进行实验，demonstrate its efficiency，只需要几个真实世界回合就能够 дости得 near-optimal performance。视频 display learned policies at https://youtu.be/H9xp60ngOes.
</details></li>
</ul>
<hr>
<h2 id="HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions"><a href="#HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions" class="headerlink" title="HIQL: Offline Goal-Conditioned RL with Latent States as Actions"></a>HIQL: Offline Goal-Conditioned RL with Latent States as Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11949">http://arxiv.org/abs/2307.11949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seohongpark/hiql">https://github.com/seohongpark/hiql</a></li>
<li>paper_authors: Seohong Park, Dibya Ghosh, Benjamin Eysenbach, Sergey Levine</li>
<li>for: 这个论文旨在提出一种基于非监督学习的目标conditioned reinforcement learning算法，可以从无标签数据中学习。</li>
<li>methods: 该算法使用一个action-free value function，通过层次分解来学习两个策略：一个高级策略使得状态被看作动作，预测子目标，以及一个低级策略预测达到子目标的行动。</li>
<li>results: 通过分析和实践示例， authors表明该层次分解使得其方法具有对噪音估计值函数的 Robustness。然后，通过应用该方法于offline目标 дости达标准别件，authors证明其方法可以解决远程目标任务，可以扩展到高维图像观察数据，并可以充分利用无动作数据。<details>
<summary>Abstract</summary>
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at https://seohong.me/projects/hiql/
</details>
<details>
<summary>摘要</summary>
现代计算机视觉和自然语言处理领域中，无监督预训练已经成为核心。在奖励学习（RL）领域，目标受控RL可能提供一种类似的自我监督方法，使用大量无奖数据进行学习。然而，建立有效的目标受控RL算法，直接从多样化的离线数据中学习，是一项挑战。这是因为，难以准确地估计远距离目标的价值函数。然而，目标达成问题具有结构，即达到远距离目标需要先通过更近的亚目标。这种结构可以很有用，因为评估近距离目标的动作质量通常比远距离目标更容易。基于这个想法，我们提出了一种层次算法 для目标受控RL。使用一个没有动作的价值函数，我们学习了两个政策：一个高级政策，将状态看作动作，预测（一个隐藏表示）亚目标，以及一个低级政策，预测用于达到亚目标的动作。通过分析和示例，我们证明了这种层次 decomposition 使我们的方法具有鲁棒性，可以抵抗估计值函数的噪声。然后，我们将我们的方法应用于离线目标达成标准，并证明了我们的方法可以解决长期任务，可以扩展到高维图像观察，并可以轻松地使用无动作数据。我们的代码可以在 <https://seohong.me/projects/hiql/> 上获取。
</details></li>
</ul>
<hr>
<h2 id="The-instabilities-of-large-learning-rate-training-a-loss-landscape-view"><a href="#The-instabilities-of-large-learning-rate-training-a-loss-landscape-view" class="headerlink" title="The instabilities of large learning rate training: a loss landscape view"></a>The instabilities of large learning rate training: a loss landscape view</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11948">http://arxiv.org/abs/2307.11948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lawrence Wang, Stephen Roberts</li>
<li>for: 研究深度学习网络训练中大学习率的稳定性，特别是在大学习率下的训练过程中存在潜在的不稳定性。</li>
<li>methods: 通过分析梯度下降的矩阵Hessian matrix来研究深度学习网络训练过程中的不稳定性。</li>
<li>results: 发现在大学习率下的训练过程中出现了“景观平整”和“景观转移”这两种fenomenon，这两种现象与训练过程中的不稳定性息息相关。<details>
<summary>Abstract</summary>
Modern neural networks are undeniably successful. Numerous works study how the curvature of loss landscapes can affect the quality of solutions. In this work we study the loss landscape by considering the Hessian matrix during network training with large learning rates - an attractive regime that is (in)famously unstable. We characterise the instabilities of gradient descent, and we observe the striking phenomena of \textit{landscape flattening} and \textit{landscape shift}, both of which are intimately connected to the instabilities of training.
</details>
<details>
<summary>摘要</summary>
现代神经网络确实非常成功。许多研究表明损失函数的凹凸度可以影响解决方案的质量。在这篇文章中，我们研究训练神经网络时的损失函数地形，包括在大学习率下进行训练的情况。我们描述梯度下降的不稳定性，并观察到了各种phenomena，如“地形平整”和“地形转移”，这些现象与训练过程中的不稳定性密切相关。
</details></li>
</ul>
<hr>
<h2 id="Collaboratively-Learning-Linear-Models-with-Structured-Missing-Data"><a href="#Collaboratively-Learning-Linear-Models-with-Structured-Missing-Data" class="headerlink" title="Collaboratively Learning Linear Models with Structured Missing Data"></a>Collaboratively Learning Linear Models with Structured Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11947">http://arxiv.org/abs/2307.11947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Cheng, Gary Cheng, John Duchi</li>
<li>for: 这篇论文目的是解决多个代理（agent）协同学习最小二乘估计问题。每个代理都观察到不同的特征集（e.g., 感知器的分辨率不同）。我们想要协调代理，以生成每个代理最佳估计器。</li>
<li>methods: 我们提出了一种分布式、半监督的算法Collab，包括三步：本地训练、聚合和分布。我们的过程不需要交换标注数据，因此具有通信效率和在标注数据不可 accessible 的场景中使用。</li>
<li>results: 我们的方法在真实数据和 sintetic 数据上进行测试，并达到了 Nearly asymptotically local minimax 优化的水平，即在不交换标注数据的情况下，我们的方法与可以交换标注数据的优化方法相比，具有类似的性能。<details>
<summary>Abstract</summary>
We study the problem of collaboratively learning least squares estimates for $m$ agents. Each agent observes a different subset of the features$\unicode{x2013}$e.g., containing data collected from sensors of varying resolution. Our goal is to determine how to coordinate the agents in order to produce the best estimator for each agent. We propose a distributed, semi-supervised algorithm Collab, consisting of three steps: local training, aggregation, and distribution. Our procedure does not require communicating the labeled data, making it communication efficient and useful in settings where the labeled data is inaccessible. Despite this handicap, our procedure is nearly asymptotically local minimax optimal$\unicode{x2013}$even among estimators allowed to communicate the labeled data such as imputation methods. We test our method on real and synthetic data.
</details>
<details>
<summary>摘要</summary>
我们研究多 Agent 协同学习最小二乘估计问题。每个 Agent 观察不同的特征集合$\unicode{x2013}$例如，各种感知器的分辨率不同。我们的目标是在 Agent 之间协调，以生成每个 Agent 最佳估计器。我们提出了分布式、半监督的算法 Collab，包括三个步骤：本地训练、聚合和分布。我们的过程不需要通信标注数据，因此具有通信效率和在标注数据不可 accessible 的场景中使用。尽管这些限制，我们的过程仍然几乎极限本地最小最优$\unicode{x2013}$甚至与可以通信标注数据的估计器相比。我们在真实数据和 sintetic 数据上测试了我们的方法。
</details></li>
</ul>
<hr>
<h2 id="Batch-Clipping-and-Adaptive-Layerwise-Clipping-for-Differential-Private-Stochastic-Gradient-Descent"><a href="#Batch-Clipping-and-Adaptive-Layerwise-Clipping-for-Differential-Private-Stochastic-Gradient-Descent" class="headerlink" title="Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent"></a>Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11939">http://arxiv.org/abs/2307.11939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Toan N. Nguyen, Phuong Ha Nguyen, Lam M. Nguyen, Marten Van Dijk<br>for: 这 paper 是为了提出一种新的权限保护技术，以保证 differential privacy 的实现。methods: 这 paper 使用了 Individual Clipping (IC) 和 Batch Clipping (BC) 两种方法来实现权限保护，并且引入了 Adaptive Layerwise Clipping (ALC) 方法来适应不同层的敏感度。results:  experiments 表明，使用 BC 和 ALC 可以使 Differential Private Stochastic Gradient Descent (DPSGD)  converge，而使用 IC 和 ALC 不能 converge。<details>
<summary>Abstract</summary>
Each round in Differential Private Stochastic Gradient Descent (DPSGD) transmits a sum of clipped gradients obfuscated with Gaussian noise to a central server which uses this to update a global model which often represents a deep neural network. Since the clipped gradients are computed separately, which we call Individual Clipping (IC), deep neural networks like resnet-18 cannot use Batch Normalization Layers (BNL) which is a crucial component in deep neural networks for achieving a high accuracy. To utilize BNL, we introduce Batch Clipping (BC) where, instead of clipping single gradients as in the orginal DPSGD, we average and clip batches of gradients. Moreover, the model entries of different layers have different sensitivities to the added Gaussian noise. Therefore, Adaptive Layerwise Clipping methods (ALC), where each layer has its own adaptively finetuned clipping constant, have been introduced and studied, but so far without rigorous DP proofs. In this paper, we propose {\em a new ALC and provide rigorous DP proofs for both BC and ALC}. Experiments show that our modified DPSGD with BC and ALC for CIFAR-$10$ with resnet-$18$ converges while DPSGD with IC and ALC does not.
</details>
<details>
<summary>摘要</summary>
每个轮次在差分私人梯度下降（DPSGD）中传输一个混合的梯度，其中包含 Gaussian 噪声，并将其发送到中央服务器，以更新一个全球模型，通常是深度神经网络。由于混合的梯度在不同层中计算，因此无法使用批处理正则化层（BNL），这是深度神经网络实现高精度的一个关键组件。为了使用 BNL，我们引入批量混合（BC），其中，而不是归一化单个梯度，我们平均混合批处理的梯度。此外，不同层的模型元素对添加的 Gaussian 噪声具有不同的感度。因此，我们引入自适应层wise混合方法（ALC），其中每个层有自己的自适应调整的混合常量。在本文中，我们提出了一种新的 ALC，并为 BC 和 ALC 提供了严格的 DP 证明。实验表明，我们修改了 DPSGD 的 BC 和 ALC，可以在 CIFAR-10 上使用 resnet-18 进行训练，而 DPSGD 的 IC 和 ALC 不能。
</details></li>
</ul>
<hr>
<h2 id="Mercer-Large-Scale-Kernel-Machines-from-Ridge-Function-Perspective"><a href="#Mercer-Large-Scale-Kernel-Machines-from-Ridge-Function-Perspective" class="headerlink" title="Mercer Large-Scale Kernel Machines from Ridge Function Perspective"></a>Mercer Large-Scale Kernel Machines from Ridge Function Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11925">http://arxiv.org/abs/2307.11925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karol Dziedziul, Sergey Kryzhevich</li>
<li>for: 本文关注大规模kernel机器学习方面的Mercer kernel machines的推理方法，从ridge函数的角度出发，回顾林和拜访的结果。</li>
<li>methods: 本文使用了近期rachimi和recht（2008）的Random features for large-scale kernel machines，以及相关的Approximation Theory来研究哪些kernel可以被简化为一个恒等式的极值函数。</li>
<li>results: 本文发现了一些障碍使用这种方法的问题，并可能有各种应用在深度学习中，特别是图像处理等问题。<details>
<summary>Abstract</summary>
To present Mercer large-scale kernel machines from a ridge function perspective, we recall the results by Lin and Pinkus from Fundamentality of ridge functions. We consider the main theorem of the recent paper by Rachimi and Recht, 2008, Random features for large-scale kernel machines in terms of the Approximation Theory. We study which kernels can be approximated by a sum of cosine function products with arguments depending on $x$ and $y$ and present the obstacles of such an approach. The results of this article may have various applications in Deep Learning, especially in problems related to Image Processing.
</details>
<details>
<summary>摘要</summary>
要从ridge函数角度介绍Mercer大规模kernel机器，我们回忆了林和拜纳斯在基本性理论中的结果。我们考虑了2008年rachimi和 recht的论文《Random features for large-scale kernel machines in terms of Approximation Theory》中的主要定理。我们研究了可以通过cosine函数产品的叠加来近似kernel机器，其中Arguments取决于x和y坐标，并提出了这种方法的阻碍。这些结果可能在深度学习中有各种应用，特别是在图像处理问题中。
</details></li>
</ul>
<hr>
<h2 id="Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors"><a href="#Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors" class="headerlink" title="Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors"></a>Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11922">http://arxiv.org/abs/2307.11922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim, JB Lanier, Pierre Baldi, Roy Fox, Sameer Singh</li>
<li>for: 这个论文是为了研究如何使用自然语言处理技术来提高机器人和游戏中的决策过程。</li>
<li>methods: 该论文提出了一种自动选择简洁状态描述的方法，称为Brief Language INputs for DEcision-making Responses（BLINDER），它通过学习任务条件下的状态描述价值函数来选择描述。</li>
<li>results: 该论文在NetHack游戏和机器人 manipulate任务中实现了提高任务成功率、减少输入大小和计算成本、并且可以在不同的LLM actors之间进行泛化。<details>
<summary>Abstract</summary>
Large language models (LLMs) are being applied as actors for sequential decision making tasks in domains such as robotics and games, utilizing their general world knowledge and planning abilities. However, previous work does little to explore what environment state information is provided to LLM actors via language. Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on hand-engineered, task-specific protocols to determine which features to communicate about a state and which to leave out. In this work, we propose Brief Language INputs for DEcision-making Responses (BLINDER), a method for automatically selecting concise state descriptions by learning a value function for task-conditioned state descriptions. We evaluate BLINDER on the challenging video game NetHack and a robotic manipulation task. Our method improves task success rate, reduces input size and compute costs, and generalizes between LLM actors.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型（LLM）在机器人和游戏等领域中作为决策演员，利用其通用世界知识和规划能力。然而，前一代工作几乎没有探讨在语言中提供环境状态信息给 LLM 演员的问题。描述高维状态的尝试可能会降低性能和提高 LLM 演员的推理成本。先前的 LLM 演员通常采用手动设计、任务特定的协议来确定要将哪些特征包含在状态描述中，并且哪些可以略去。在这项工作中，我们提出了简短语言输入 для决策响应（BLINDER）方法，通过学习任务条件下的状态描述价值函数来自动选择简洁的状态描述。我们在 NetHack 游戏和机器人搅拌任务上评估了 BLINDER。我们的方法可以提高任务成功率，减少输入大小和计算成本，并且可以在不同的 LLM 演员之间进行泛化。
</details></li>
</ul>
<hr>
<h2 id="Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data"><a href="#Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data" class="headerlink" title="Poverty rate prediction using multi-modal survey and earth observation data"></a>Poverty rate prediction using multi-modal survey and earth observation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11921">http://arxiv.org/abs/2307.11921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Fobi, Manuel Cardona, Elliott Collins, Caleb Robinson, Anthony Ortiz, Tina Sederholm, Rahul Dodhia, Juan Lavista Ferres</li>
<li>for: 预测地区贫困率</li>
<li>methods:  combining household demographic and living standards survey questions with features derived from satellite imagery</li>
<li>results: 1)  inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88% 2)  the best performance – errors in poverty rate decrease from 4.09% to 3.71% 3) extracted visual features encode geographic and urbanization differences between regions.<details>
<summary>Abstract</summary>
This work presents an approach for combining household demographic and living standards survey questions with features derived from satellite imagery to predict the poverty rate of a region. Our approach utilizes visual features obtained from a single-step featurization method applied to freely available 10m/px Sentinel-2 surface reflectance satellite imagery. These visual features are combined with ten survey questions in a proxy means test (PMT) to estimate whether a household is below the poverty line. We show that the inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88% over a nationally representative out-of-sample test set. In addition to including satellite imagery features in proxy means tests, we propose an approach for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery. Specifically, we design a survey variable selection approach guided by the full survey and image features and use the approach to determine the most relevant set of small survey questions to include in a PMT. We validate the choice of small survey questions in a downstream task of predicting the poverty rate using the small set of questions. This approach results in the best performance -- errors in poverty rate decrease from 4.09% to 3.71%. We show that extracted visual features encode geographic and urbanization differences between regions.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这项研究提出了一种方法，利用户户普查和卫星成像特征来预测地区贫困率。该方法使用10m/px Sentinel-2表面反射卫星成像中的视觉特征，与十个问题组成一个代表测试（PMT）来估算户户是否下于贫困线。包括卫星成像特征后，贫困率估计的平均错误率由4.09%降低到3.88%。此外，该方法还提出了一种方法，选择与卫星成像特征相关的小问题集，以便在预测贫困率的下游任务中使用。该方法根据全面调查和成像特征选择最相关的小问题集，并用这些问题集来预测贫困率。这种方法实现了最佳性能，贫困率估计错误率由4.09%降低到3.71%。此外，提取的视觉特征还含有地域和城市化差异。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks"><a href="#Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks" class="headerlink" title="Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks"></a>Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11906">http://arxiv.org/abs/2307.11906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed</li>
<li>for: 保障深度学习系统的可靠性、可靠性和信任性，防止恶意攻击。</li>
<li>methods: 使用微生物遗传算法，基于黑盒测试，不需要目标模型和解释模型的先知知识。</li>
<li>results: 实验结果显示，这种攻击具有高成功率，使用挑战性示例和归因地幔，很难于探测。<details>
<summary>Abstract</summary>
Deep learning has been rapidly employed in many applications revolutionizing many industries, but it is known to be vulnerable to adversarial attacks. Such attacks pose a serious threat to deep learning-based systems compromising their integrity, reliability, and trust. Interpretable Deep Learning Systems (IDLSes) are designed to make the system more transparent and explainable, but they are also shown to be susceptible to attacks. In this work, we propose a novel microbial genetic algorithm-based black-box attack against IDLSes that requires no prior knowledge of the target model and its interpretation model. The proposed attack is a query-efficient approach that combines transfer-based and score-based methods, making it a powerful tool to unveil IDLS vulnerabilities. Our experiments of the attack show high attack success rates using adversarial examples with attribution maps that are highly similar to those of benign samples which makes it difficult to detect even by human analysts. Our results highlight the need for improved IDLS security to ensure their practical reliability.
</details>
<details>
<summary>摘要</summary>
深度学习在许多应用中得到了迅速的应用，但它知道是易受到敌意攻击的。这些攻击会对深度学习基于系统的完整性、可靠性和信任造成严重的威胁。可解释深度学习系统（IDLS）是为了使系统更加透明和可解释的，但它们也被证明是易受到攻击的。在这种工作中，我们提出了一种基于微生物遗传算法的黑盒攻击方法，不需要target模型和其解释模型的先前知识。我们的攻击方法结合了传递基本方法和分数基本方法，使其成为对IDLS的可靠性进行检测的强大工具。我们的实验表明，使用对抗例中的特征图可以达到高度的攻击成功率，并且这些特征图与正常样本的特征图几乎相同，使其具有难以检测的特点。我们的结果表明，为了确保IDLS的实际可靠性，需要进一步加强IDLS的安全性。
</details></li>
</ul>
<hr>
<h2 id="Model-Compression-Methods-for-YOLOv5-A-Review"><a href="#Model-Compression-Methods-for-YOLOv5-A-Review" class="headerlink" title="Model Compression Methods for YOLOv5: A Review"></a>Model Compression Methods for YOLOv5: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11904">http://arxiv.org/abs/2307.11904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Jani, Jamil Fayyad, Younes Al-Younes, Homayoun Najjaran</li>
<li>for: 本文主要针对强化YOLO对象检测器的研究进行了概括，以便在资源有限的设备上部署。</li>
<li>methods: 本文主要考虑了网络剪辑和量化两种压缩方法，以减少模型的内存使用量和计算时间。</li>
<li>results: 通过对YOLOv5进行剪辑和量化处理，可以降低模型的内存使用量和计算时间，但是还存在一些 gap 需要进一步研究。<details>
<summary>Abstract</summary>
Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resource-limited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we identify gaps in adapting pruning and quantization for compressing YOLOv5, and provide future directions in this area for further exploration. Among several versions of YOLO, we specifically choose YOLOv5 for its excellent trade-off between recency and popularity in literature. This is the first specific review paper that surveys pruning and quantization methods from an implementation point of view on YOLOv5. Our study is also extendable to newer versions of YOLO as implementing them on resource-limited devices poses the same challenges that persist even today. This paper targets those interested in the practical deployment of model compression methods on YOLOv5, and in exploring different compression techniques that can be used for subsequent versions of YOLO.
</details>
<details>
<summary>摘要</summary>
在过去几年，对 YOLO 对象检测器进行了广泛的研究，以提高其精度和效率。自其引入以来，共有八个主要版本的 YOLO 发布，以提高其精度和效率。虽然 YOLO 在许多领域得到了广泛的应用，但在资源有限的设备上部署它却存在挑战。为解决这个问题，各种神经网络压缩方法被开发出来，这些方法分为三个主要类别：网络剪辑、量化和知识传递。使用这些方法可以降低内存使用量和执行时间，这使得它们在硬件限制的边缘设备上进行部署变得有利可图。在本文中，我们将关注剪辑和量化，因为它们在可模块化方面比较出色。我们将这些方法进行分类和分析，并通过应用这些方法于 YOLOv5 来评估其实际效果。通过这些研究，我们可以了解剪辑和量化在 YOLOv5 上的应用存在哪些挑战，并提供未来研究的方向。在多个 YOLO 版本中，我们选择 YOLOv5，因为它在文献中的悠久度和受欢迎程度均很高。这是关于剪辑和量化方法在 YOLOv5 上的首个具体评估文章。我们的研究也可以扩展到 newer 版本的 YOLO，因为在资源有限的设备上部署它们也存在同样的挑战。本文适合那些关注实际部署模型压缩方法在 YOLOv5 上的人，以及想要探索不同的压缩技术，以应用于未来的 YOLO 版本。
</details></li>
</ul>
<hr>
<h2 id="Project-Florida-Federated-Learning-Made-Easy"><a href="#Project-Florida-Federated-Learning-Made-Easy" class="headerlink" title="Project Florida: Federated Learning Made Easy"></a>Project Florida: Federated Learning Made Easy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11899">http://arxiv.org/abs/2307.11899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Madrigal Diaz, Andre Manoel, Jialei Chen, Nalin Singal, Robert Sim</li>
<li>for: This paper is written for machine learning engineers and application developers who want to deploy large-scale federated learning (FL) solutions across a heterogeneous device ecosystem.</li>
<li>methods: The paper presents a system architecture and software development kit (SDK) called Project Florida, which enables the deployment of FL solutions across a wide range of operating systems and hardware specifications. The paper also discusses the use of cloud-hosted infrastructure and task management interfaces to support the training process.</li>
<li>results: The paper presents illustrative experiments that demonstrate the system’s capabilities, including the ability to train machine learning models across a wide range of devices and the ability to scale the training process to accommodate a large number of client devices.<details>
<summary>Abstract</summary>
We present Project Florida, a system architecture and software development kit (SDK) enabling deployment of large-scale Federated Learning (FL) solutions across a heterogeneous device ecosystem. Federated learning is an approach to machine learning based on a strong data sovereignty principle, i.e., that privacy and security of data is best enabled by storing it at its origin, whether on end-user devices or in segregated cloud storage silos. Federated learning enables model training across devices and silos while the training data remains within its security boundary, by distributing a model snapshot to a client running inside the boundary, running client code to update the model, and then aggregating updated snapshots across many clients in a central orchestrator. Deploying a FL solution requires implementation of complex privacy and security mechanisms as well as scalable orchestration infrastructure. Scale and performance is a paramount concern, as the model training process benefits from full participation of many client devices, which may have a wide variety of performance characteristics. Project Florida aims to simplify the task of deploying cross-device FL solutions by providing cloud-hosted infrastructure and accompanying task management interfaces, as well as a multi-platform SDK supporting most major programming languages including C++, Java, and Python, enabling FL training across a wide range of operating system (OS) and hardware specifications. The architecture decouples service management from the FL workflow, enabling a cloud service provider to deliver FL-as-a-service (FLaaS) to ML engineers and application developers. We present an overview of Florida, including a description of the architecture, sample code, and illustrative experiments demonstrating system capabilities.
</details>
<details>
<summary>摘要</summary>
我们介绍项目“佛罗里达”，这是一个系统架构和软件开发包（SDK），它使得大规模联合学习（FL）解决方案可以在多种设备生态系统中部署。联合学习是一种基于强大数据主权原则的机器学习方法，即数据privacy和安全最好是在数据的原始位置保持，whether on end-user devices or in segregated cloud storage silos。联合学习可以在设备和存储silos之间进行模型训练，而不需要将数据传输到外部，只需在设备上运行客户端代码来更新模型，然后将更新后的模型集中到中央抽象器中。实现FL解决方案需要实施复杂的隐私和安全机制，以及可扩展的管理基础设施。因为模型训练过程需要全面参与多个客户端设备，这些设备可能有各种性能特点。项目“佛罗里达”目标是使得跨设备FL解决方案的部署变得更加简单，通过提供云主机的基础设施和相关的任务管理界面，以及支持多种主要编程语言，包括C++、Java和Python，以实现FL训练在多种操作系统和硬件特性上。架构解决方案的分离，使得云服务提供商可以提供FLaaS（联合学习 как服务），让机器学习工程师和应用程序开发人员快速搭建FL解决方案。我们将对项目“佛罗里达”进行概述，包括架构描述、示例代码和 ilustrative experiments，以示系统的能力。
</details></li>
</ul>
<hr>
<h2 id="Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning"><a href="#Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning" class="headerlink" title="Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning"></a>Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11897">http://arxiv.org/abs/2307.11897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skandavaidyanath/credit-assignment">https://github.com/skandavaidyanath/credit-assignment</a></li>
<li>paper_authors: Akash Velu, Skanda Vaidyanath, Dilip Arumugam</li>
<li>for: 该文章为了解决奖励学习在缺乏评价反馈的环境中表现不佳问题，提出了一种基于追溯政策的方法。</li>
<li>methods: 该文章使用了现有的重要性抽样比例估计技术来稳定化和改进基于追溯政策的方法。</li>
<li>results: 该文章在各种环境中显示了稳定和高效的学习效果，并且可以在奖励学习中缓解奖励分配问题。<details>
<summary>Abstract</summary>
Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, leading to a significant temporal delay between the observation of a non-trivial reward and the individual steps of behavior culpable for achieving said reward. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning. While prior work has introduced the concept of hindsight policies to develop a theoretically moxtivated method for reweighting on-policy data by impact on achieving the observed trajectory return, we show that these methods experience instabilities which lead to inefficient learning in complex environments. In this work, we adapt existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the stability and efficiency of these so-called hindsight policy methods. Our hindsight distribution correction facilitates stable, efficient learning across a broad range of environments where credit assignment plagues baseline methods.
</details>
<details>
<summary>摘要</summary>
经常情况下，决策问题环境往往缺乏评价反馈，导致强化学习代理人受到很大的评价延迟。在极端情况下，长期行为只会被截止符号性的终端反馈信号刺激，从而导致行为减少的减少很大。强化学习面临着寄付问题的挑战。 Prior work已经引入了叫做前景政策的方法，以 theoretically moxtivated 方式重新权重on-policy数据，以便更好地评价 achieve  trajectory return。但我们发现这些方法会导致不稳定性，从而降低强化学习的效率。在这种情况下，我们采用了现有的重要性折衔估计技术，以改善这些叫做 hindsight 政策方法的稳定性和效率。我们的 hindsight 分布修正方法可以在各种缺乏寄付的环境中，稳定、高效地学习。
</details></li>
</ul>
<hr>
<h2 id="On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise"><a href="#On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise" class="headerlink" title="On the Vulnerability of Fairness Constrained Learning to Malicious Noise"></a>On the Vulnerability of Fairness Constrained Learning to Malicious Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11892">http://arxiv.org/abs/2307.11892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avrim Blum, Princewill Okoroafor, Aadirupa Saha, Kevin Stangl</li>
<li>For:  This paper studies the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data.* Methods: The paper uses randomized classifiers to mitigate the vulnerability of fairness-constrained learning to adversarial noise.* Results: The paper shows that for certain fairness notions, such as Demographic Parity, the loss in accuracy can be as low as $\Theta(\alpha)$ when the noise rate is small. For other fairness notions, such as Equal Opportunity, the loss in accuracy can be as low as $O(\sqrt{\alpha})$. The paper also shows that the loss in accuracy clusters into three natural regimes: $O(\alpha)$, $O(\sqrt{\alpha})$, and $O(1)$.Here’s the Chinese translation of the three points:* For: 这篇论文研究了受到训练数据中小量邪恶噪声影响的公平学习的敏感性。* Methods: 这篇论文使用Randomized classifier来减少公平学习受到邪恶噪声影响的敏感性。* Results: 这篇论文显示，对于某些公平性定义，如人口均衡，当噪声率小时，损失率可以为Theta（α）。对于其他公平性定义，如机会平等，损失率可以为O（√α）。论文还显示，损失率分布在三个自然的 régime中：O（α）、O（√α）和O(1)。<details>
<summary>Abstract</summary>
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty of our work is how randomization can bypass simple "tricks" an adversary can use to amplify his power. We also consider additional fairness notions including Equalized Odds and Calibration. For these fairness notions, the excess accuracy clusters into three natural regimes $O(\alpha)$,$O(\sqrt{\alpha})$ and $O(1)$. These results provide a more fine-grained view of the sensitivity of fairness-constrained learning to adversarial noise in training data.
</details>
<details>
<summary>摘要</summary>
我们考虑了公平性条件下的学习敏感性对小量邪恶训练数据的影响。 Konstantinov 和 Lampert (2021) 开始了这个研究，并发现了一些负的结果，表明在某些公平性条件下，任何合法的学习者都将具有高度敏感性，当群体大小不对称时。 在这里，我们提供了一个更optimistic的见解，表明如果允许随机分类器，则情况会变得更加细分。例如，对于人口均衡公平性，我们显示可以允许只有 $\Theta(\alpha)$ 的损失率，其中 $\alpha$ 是邪恶训练数据的损失率，与不具有公平性条件时相同。对于平等机会公平性，我们显示可以允许 $O(\sqrt{\alpha})$ 的损失率，并提供了对应的 $\Omega(\sqrt{\alpha})$ 下界。与 Konstantinov 和 Lampert (2021) 的结果相比，我们的结果显示，在合法学习者下，两个公平性条件的损失率都是 $\Omega(1)$。我们的技术新动向是如何使用随机性来绕过简单的邪恶攻击者可以使用的“套路”。我们还考虑了其他的公平性条件，包括平等机会和准确性。这些结果提供了训练数据中邪恶训练数据的影响的更细分的见解。
</details></li>
</ul>
<hr>
<h2 id="On-the-Universality-of-Linear-Recurrences-Followed-by-Nonlinear-Projections"><a href="#On-the-Universality-of-Linear-Recurrences-Followed-by-Nonlinear-Projections" class="headerlink" title="On the Universality of Linear Recurrences Followed by Nonlinear Projections"></a>On the Universality of Linear Recurrences Followed by Nonlinear Projections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11888">http://arxiv.org/abs/2307.11888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, Samuel L. Smith</li>
<li>for: 本研究目标是表明一种基于回归线性层的字符串模型（包括S4、S5和LRU），可以正确地模拟任何具有 suficiently  régulier不对称序列-到-序列映射。</li>
<li>methods: 本研究使用了扩展的回归层和位置wise多层感知器（MLPs）来模拟序列-to-序列映射。主要想法是看到回归层为压缩算法，可以准确地存储输入序列的信息到内部状态中，然后由高度表达的 MLP 进行处理。</li>
<li>results: 研究发现，这种模型可以将任何 suficiently  régulier不对称序列-to-序列映射 aproximated 到任何 desired 精度。<details>
<summary>Abstract</summary>
In this note (work in progress towards a full-length paper) we show that a family of sequence models based on recurrent linear layers~(including S4, S5, and the LRU) interleaved with position-wise multi-layer perceptrons~(MLPs) can approximate arbitrarily well any sufficiently regular non-linear sequence-to-sequence map. The main idea behind our result is to see recurrent layers as compression algorithms that can faithfully store information about the input sequence into an inner state, before it is processed by the highly expressive MLP.
</details>
<details>
<summary>摘要</summary>
在这份工作进度中（正在prepare一篇全长论文），我们展示了一家系列模型，该模型基于循环线性层（包括S4、S5和LRU）和位置层 wise多层感知器（MLP）。这种模型可以在任何足够规则的序列到序列映射中进行近似。我们的主要想法是看循环层为压缩算法，可以准确地将输入序列存储在内部状态中，然后由高度表达的 MLP 进行处理。
</details></li>
</ul>
<hr>
<h2 id="MORE-Measurement-and-Correlation-Based-Variational-Quantum-Circuit-for-Multi-classification"><a href="#MORE-Measurement-and-Correlation-Based-Variational-Quantum-Circuit-for-Multi-classification" class="headerlink" title="MORE: Measurement and Correlation Based Variational Quantum Circuit for Multi-classification"></a>MORE: Measurement and Correlation Based Variational Quantum Circuit for Multi-classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11875">http://arxiv.org/abs/2307.11875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jindi0/more">https://github.com/jindi0/more</a></li>
<li>paper_authors: Jindi Wu, Tianjie Hu, Qun Li<br>for:MORE is a quantum multi-classifier that leverages the quantum information of a single readout qubit to perform multi-class classification tasks.methods:MORE uses a variational ansatz and quantum state tomography to reconstruct the readout state, and then employs variational quantum clustering and supervised learning to determine the mapping between input data and quantum labels.results:MORE achieves advanced performance in multi-class classification tasks despite using a simple ansatz and limited quantum resources, and outperforms traditional binary classifiers in certain scenarios.Here’s the Chinese translation of the three points:for:MORE 是一个使用单 readout qubit 进行多类别分类任务的量子多类别推断器。methods:MORE 使用量子状态测量来重建 readout 状态，然后使用量子推断 clustering 和 supervised learning 来决定输入数据和量子标签之间的映射。results:MORE 在多类别分类任务中获得进步的表现，即使使用简单的推断器和有限的量子资源，并在某些情况下超越传统的二进制推断器。<details>
<summary>Abstract</summary>
Quantum computing has shown considerable promise for compute-intensive tasks in recent years. For instance, classification tasks based on quantum neural networks (QNN) have garnered significant interest from researchers and have been evaluated in various scenarios. However, the majority of quantum classifiers are currently limited to binary classification tasks due to either constrained quantum computing resources or the need for intensive classical post-processing. In this paper, we propose an efficient quantum multi-classifier called MORE, which stands for measurement and correlation based variational quantum multi-classifier. MORE adopts the same variational ansatz as binary classifiers while performing multi-classification by fully utilizing the quantum information of a single readout qubit. To extract the complete information from the readout qubit, we select three observables that form the basis of a two-dimensional Hilbert space. We then use the quantum state tomography technique to reconstruct the readout state from the measurement results. Afterward, we explore the correlation between classes to determine the quantum labels for classes using the variational quantum clustering approach. Next, quantum label-based supervised learning is performed to identify the mapping between the input data and their corresponding quantum labels. Finally, the predicted label is determined by its closest quantum label when using the classifier. We implement this approach using the Qiskit Python library and evaluate it through extensive experiments on both noise-free and noisy quantum systems. Our evaluation results demonstrate that MORE, despite using a simple ansatz and limited quantum resources, achieves advanced performance.
</details>
<details>
<summary>摘要</summary>
量子计算在最近几年内已经显示了较大的承诺，尤其是对于计算密集的任务。例如，基于量子神经网络（QNN）的分类任务已经吸引了研究者的广泛关注，并在多个场景中进行了评估。然而，大多数量子分类器目前仅限于二进制分类任务，这可能是因为量子计算资源的限制或需要大量的经典后处理。在这篇论文中，我们提出了一种高效的量子多分类器，即MORE（测量和相关性基于量子多分类器）。MORE采用了同binary分类器一样的变量 ansatz，并在完全利用单个读取量子比特的量子信息上进行多分类。为了从读取量子比特中提取完整的信息，我们选择了三个观察量，它们构成了一个二维希尔伯特空间的基。然后，我们使用量子状态探测技术来重建读取状态。接着，我们研究分类关系来确定类别的量子标签，并使用量子分布式学习方法来确定输入数据与其相应的量子标签之间的映射。最后，我们使用类ifier来预测输入数据的标签。我们使用Qiskit Python库实现这种方法，并对噪声量子系统和噪声自由量子系统进行了广泛的实验评估。我们的评估结果表明，MORE，即使使用简单的 ansatz 和有限的量子资源，仍然可以达到高效的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention"><a href="#The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention" class="headerlink" title="The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention"></a>The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11864">http://arxiv.org/abs/2307.11864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Ayoobi, Sadat Shahriar, Arjun Mukherjee<br>for:This paper is written to detect fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections.methods:The paper introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of textual information provided in LinkedIn profiles for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. The paper also uses static and contextualized word embeddings, including GloVe, Flair, BERT, and RoBERTa.results:The suggested method can distinguish between legitimate and fake profiles with an accuracy of about 95% across all word embeddings. Additionally, the SSTE method has a promising accuracy for identifying LLM-generated profiles, with an accuracy of approximately 90% when only 20 LLM-generated profiles are added to the training set.<details>
<summary>Abstract</summary>
In this paper, we present a novel method for detecting fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections. Early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. This work uses textual information provided in LinkedIn profiles and introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. Additionally, the dearth of a large publicly available LinkedIn dataset motivated us to collect 3600 LinkedIn profiles for our research. We will release our dataset publicly for research purposes. This is, to the best of our knowledge, the first large publicly available LinkedIn dataset for fake LinkedIn account detection. Within our paradigm, we assess static and contextualized word embeddings, including GloVe, Flair, BERT, and RoBERTa. We show that the suggested method can distinguish between legitimate and fake profiles with an accuracy of about 95% across all word embeddings. In addition, we show that SSTE has a promising accuracy for identifying LLM-generated profiles, despite the fact that no LLM-generated profiles were employed during the training phase, and can achieve an accuracy of approximately 90% when only 20 LLM-generated profiles are added to the training set. It is a significant finding since the proliferation of several LLMs in the near future makes it extremely challenging to design a single system that can identify profiles created with various LLMs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的方法，用于在 LinkedIn 在线社交网络上立即识别 fake 和 Large Language Model（LLM）生成的 profiless，并在注册后before establishing connections。早期识别假 profiless是维护平台的完整性的关键，因为它防止了假者从获取真正用户的私人和敏感信息，并从获得未来骗财活动的机会。本工作使用 LinkedIn  profiless 中提供的文本信息，并引入 Section and Subsection Tag Embedding（SSTE）方法，以增强这些数据的权威性，以分辨真实 profiless 和由假者或 LLM 生成的 profiless。此外，由于没有大量公开可用的 LinkedIn 数据集，我们自己收集了 3600 个 LinkedIn profiless 为我们的研究。我们将在研究用途上公开我们的数据集。这是，我们知道的， LinkedIn 上假账户检测的首个大规模公开数据集。在我们的 paradigm 中，我们评估了静止和 contextualized 单词嵌入，包括 GloVe、Flair、BERT 和 RoBERTa。我们显示，我们的方法可以在所有单词嵌入上分辨 true 和 fake profiless，准确率约为 95%。此外，我们还显示了 SSTE 在 LLM 生成 profiless 上的扩展性，即使在训练阶段没有使用 LLM 生成 profiless，可以达到约 90% 的准确率，只需要添加 20 个 LLM 生成 profiless 到训练集中。这是一项重要发现，因为未来几年内，许多 LLM 将在未来逐渐普及，设计一个系统可以识别由不同 LLM 生成的 profiless 将变得极其困难。
</details></li>
</ul>
<hr>
<h2 id="Data-Induced-Interactions-of-Sparse-Sensors"><a href="#Data-Induced-Interactions-of-Sparse-Sensors" class="headerlink" title="Data-Induced Interactions of Sparse Sensors"></a>Data-Induced Interactions of Sparse Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11838">http://arxiv.org/abs/2307.11838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrei A. Klishin, J. Nathan Kutz, Krithika Manohar</li>
<li>for: 该论文旨在描述如何使用少量的感知器来重建复杂系统的状态，并且如何选择感知器的位置以实现最佳重建结果。</li>
<li>methods: 论文使用了基于异谱 interpolate 和 QR 分解的多种算法来优化感知器的位置，并通过统计物理学的狄耳诺模型来计算感知器之间的互动。</li>
<li>results: 论文通过计算数据引导的感知器互动的全景，可以结合外部选择标准和预测感知器更换的影响。<details>
<summary>Abstract</summary>
Large-dimensional empirical data in science and engineering frequently has low-rank structure and can be represented as a combination of just a few eigenmodes. Because of this structure, we can use just a few spatially localized sensor measurements to reconstruct the full state of a complex system. The quality of this reconstruction, especially in the presence of sensor noise, depends significantly on the spatial configuration of the sensors. Multiple algorithms based on gappy interpolation and QR factorization have been proposed to optimize sensor placement. Here, instead of an algorithm that outputs a singular "optimal" sensor configuration, we take a thermodynamic view to compute the full landscape of sensor interactions induced by the training data. The landscape takes the form of the Ising model in statistical physics, and accounts for both the data variance captured at each sensor location and the crosstalk between sensors. Mapping out these data-induced sensor interactions allows combining them with external selection criteria and anticipating sensor replacement impacts.
</details>
<details>
<summary>摘要</summary>
大量实际数据在科学和工程频繁具有低维结构，可以通过一些本地感知器来表示。由于这种结构，我们可以使用一些感知器来重建复杂系统的全部状态，尤其是在感知噪声存在时。多种基于异常 interpolate 和 QR 分解的算法已经被提出来优化感知器布局。而不是输出一个“最优”的感知器配置，我们在这里采用热力学视角计算整个感知器与训练数据之间的互动场景。这个场景采用牛顿模型来描述，考虑了每个感知器位置上采集数据的方差以及感知器之间的干扰。通过映射这些数据引起的感知器互动，我们可以与外部选择标准结合并预测感知器更换的影响。
</details></li>
</ul>
<hr>
<h2 id="eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review"><a href="#eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review" class="headerlink" title="eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review"></a>eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13704">http://arxiv.org/abs/2307.13704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alena Kalyakulina, Igor Yusipov</li>
<li>for: 这篇论文探讨了使用可解释人工智能（XAI）技术进行年龄预测任务的应用。</li>
<li>methods: 论文将XAI技术应用于不同的身体系统，进行系统化的文献综述。</li>
<li>results: 论文指出了XAI在医疗应用中的优点，特别是在年龄预测任务中。<details>
<summary>Abstract</summary>
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
</details>
<details>
<summary>摘要</summary>
<<SYS>>可解释人工智能（XAI）现在是机器学习中非常重要和必需的一部分，允许解释复杂模型的预测。XAI特别在危险应用中需要，特别是在医疗领域，人工智能系统的决策直接关系到人们的生命。一个医学研究领域是年龄预测和衰老病症的生物标志物质的预测。然而，XAI在年龄预测任务中的角色没有直接探讨过。在这篇评论中，我们讨论了XAI方法在年龄预测任务中的应用。我们按照身体系统进行了系统性的综述，并讨论了医疗应用中XAI的优点和年龄预测领域中XAI的特点。>>>Note that Simplified Chinese is used here, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="PINNsFormer-A-Transformer-Based-Framework-For-Physics-Informed-Neural-Networks"><a href="#PINNsFormer-A-Transformer-Based-Framework-For-Physics-Informed-Neural-Networks" class="headerlink" title="PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks"></a>PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11833">http://arxiv.org/abs/2307.11833</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adityalab/pinnsformer">https://github.com/adityalab/pinnsformer</a></li>
<li>paper_authors: Leo Zhiyuan Zhao, Xueying Ding, B. Aditya Prakash</li>
<li>for: 用于数值解 partial differential equations (PDEs) 的深度学习框架。</li>
<li>methods: 使用 Transformer 结构，并采用多头注意机制来捕捉 PDEs 中的时间关系。</li>
<li>results: 能够准确地 approximates PDEs 的解，并在不同场景下超过传统 PINNs 的表现，尽管具有较少的计算和存储成本。<details>
<summary>Abstract</summary>
Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions for partial differential equations (PDEs). While conventional PINNs and most related studies adopt fully-connected multilayer perceptrons (MLP) as the backbone structure, they have neglected the temporal relations in PDEs and failed to approximate the true solution. In this paper, we propose a novel Transformer-based framework, namely PINNsFormer, that accurately approximates PDEs' solutions by capturing the temporal dependencies with multi-head attention mechanisms in Transformer-based models. Instead of approximating point predictions, PINNsFormer adapts input vectors to pseudo sequences and point-wise PINNs loss to a sequential PINNs loss. In addition, PINNsFormer is equipped with a novel activation function, namely Wavelet, which anticipates the Fourier decomposition through deep neural networks. We empirically demonstrate PINNsFormer's ability to capture the PDE solutions for various scenarios, in which conventional PINNs have failed to learn. We also show that PINNsFormer achieves superior approximation accuracy on such problems than conventional PINNs with non-sensitive hyperparameters, in trade of marginal computational and memory costs, with extensive experiments.
</details>
<details>
<summary>摘要</summary>
physics-informed neural networks (PINNs) 已经出现为解决数学Physical laws的深度学习框架，但是传统的PINNs和大多数相关研究都是使用完全连接多层感知器(MLP)作为脊梁结构，这些结构忽略了PDEs中的时间关系，并且无法准确地预测解。在本文中，我们提出了一种新的Transformer-based框架，即PINNsFormer，可以准确地预测PDEs的解决方案，通过在Transformer-based模型中使用多头注意机制来捕捉PDEs中的时间相关性。而不是对点预测进行approximation，PINNsFormer将输入向量转化为pseudo序列，并将点级PINNs损失转化为sequential PINNs损失。此外，PINNsFormer还具有一种新的活动函数，即wavelet，该函数预测了深度神经网络中的Fourier分解。我们通过实验证明PINNsFormer可以在不同的情况下，包括传统PINNs无法学习的情况下，准确地预测PDEs的解决方案。此外，我们还证明PINNsFormer在这些问题上的 aproximation精度高于传统PINNs，但是与非敏感的计算和存储成本相比，PINNsFormer的计算和存储成本几乎是零的。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Heavy-Hitter-Detection-using-Federated-Analytics"><a href="#Differentially-Private-Heavy-Hitter-Detection-using-Federated-Analytics" class="headerlink" title="Differentially Private Heavy Hitter Detection using Federated Analytics"></a>Differentially Private Heavy Hitter Detection using Federated Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11749">http://arxiv.org/abs/2307.11749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Chadha, Junye Chen, John Duchi, Vitaly Feldman, Hanieh Hashemi, Omid Javidbakht, Audra McMillan, Kunal Talwar</li>
<li>for: 增强 prefix-tree 算法 隐私检测 differentially private heavy hitter 性能。</li>
<li>methods: 提出了一种基于 adaptive hyperparameter tuning 算法，以满足计算、通信和隐私约束的多用户数据点检测。</li>
<li>results: 通过对 Reddit 数据集进行大量实验，发现该方法可以提高检测性能，同时满足计算、通信和隐私约束。<details>
<summary>Abstract</summary>
In this work, we study practical heuristics to improve the performance of prefix-tree based algorithms for differentially private heavy hitter detection. Our model assumes each user has multiple data points and the goal is to learn as many of the most frequent data points as possible across all users' data with aggregate and local differential privacy. We propose an adaptive hyperparameter tuning algorithm that improves the performance of the algorithm while satisfying computational, communication and privacy constraints. We explore the impact of different data-selection schemes as well as the impact of introducing deny lists during multiple runs of the algorithm. We test these improvements using extensive experimentation on the Reddit dataset~\cite{caldas2018leaf} on the task of learning the most frequent words.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们研究了使用前缀树基于算法来提高分布式隐私极大热点检测的实用规则。我们的模型假设每个用户有多个数据点，目标是通过聚合和本地隐私来学习所有用户数据中的最多频数据点。我们提议一种适应性hyperparameter调整算法，可以提高算法的性能，同时满足计算、通信和隐私约束。我们还研究了不同的数据选择方案以及在多次运行算法时引入拒绝列表的影响。我们对这些改进进行了广泛的实验，使用了Reddit数据集（Caldas et al., 2018），以学习最常见的单词。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Ad-Auction-Realism-Practical-Insights-Modeling-Implications"><a href="#Advancing-Ad-Auction-Realism-Practical-Insights-Modeling-Implications" class="headerlink" title="Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications"></a>Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11732">http://arxiv.org/abs/2307.11732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Chen, Sareh Nabi, Marciano Siniscalchi</li>
<li>for: 这个论文是为了研究当代在线广告拍卖中的四个实际特征，包括广告插播值和点击率因用户搜索词而异常，竞争者的数量和身份在拍卖过程中是未知的，广告主只能得到部分、汇总的反馈。</li>
<li>methods: 作者使用了对抗人工智能算法来模型广告主的行为，不受拍卖机制细节的影响。</li>
<li>results: 研究发现，在更加复杂的环境中，“软底”可以提高关键性能指标，而且可以在竞争者来自同一个人口群体时实现这一效果。此外，研究还证明了如何从观察拍卖价格中推断广告主价值分布，从而证明了这种方法在更加实际的拍卖Setting中的实际效果。<details>
<summary>Abstract</summary>
This paper proposes a learning model of online ad auctions that allows for the following four key realistic characteristics of contemporary online auctions: (1) ad slots can have different values and click-through rates depending on users' search queries, (2) the number and identity of competing advertisers are unobserved and change with each auction, (3) advertisers only receive partial, aggregated feedback, and (4) payment rules are only partially specified. We model advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. Our objective is to simulate the behavior of advertisers for counterfactual analysis, prediction, and inference purposes. Our findings reveal that, in such richer environments, "soft floors" can enhance key performance metrics even when bidders are drawn from the same population. We further demonstrate how to infer advertiser value distributions from observed bids, thereby affirming the practical efficacy of our approach even in a more realistic auction setting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Ad slots can have different values and click-through rates depending on users’ search queries.2. The number and identity of competing advertisers are unobserved and change with each auction.3. Advertisers only receive partial, aggregated feedback.4. Payment rules are only partially specified.We model advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. Our objective is to simulate the behavior of advertisers for counterfactual analysis, prediction, and inference purposes.Our findings show that “soft floors” can enhance key performance metrics even when bidders are drawn from the same population. Additionally, we demonstrate how to infer advertiser value distributions from observed bids, confirming the practical efficacy of our approach in a more realistic auction setting.</details></li>
</ol>
<hr>
<h2 id="Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense"><a href="#Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense" class="headerlink" title="Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense"></a>Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11730">http://arxiv.org/abs/2307.11730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enriquetomasmb/fedstellar">https://github.com/enriquetomasmb/fedstellar</a></li>
<li>paper_authors: Enrique Tomás Martínez Beltrán, Pedro Miguel Sánchez Sánchez, Sergio López Bernal, Gérôme Bovet, Manuel Gil Pérez, Gregorio Martínez Pérez, Alberto Huertas Celdrán</li>
<li>for: This paper aims to address the communication security challenges in Decentralized Federated Learning (DFL) by introducing a security module that combines encryption and Moving Target Defense (MTD) techniques.</li>
<li>methods: The security module is implemented in a DFL platform called Fedstellar, and the authors evaluate the effectiveness of the module through experiments with the MNIST dataset and eclipse attacks.</li>
<li>results: The results show that the security module can mitigate the risks posed by eavesdropping or eclipse attacks, with an average F1 score of 95% and moderate increases in CPU usage and network traffic under the most secure configuration.Here’s the simplified Chinese text for the three points:</li>
<li>for: 这篇论文目的是解决分布式联合学习（DFL）中的通信安全挑战，通过引入加密和移动目标防御（MTD）技术的安全模块。</li>
<li>methods: 这个安全模块在分布式联合学习平台Fedstellar中实现，通过MNIST数据集和eclipse攻击进行测试。</li>
<li>results: 测试结果表明，安全模块可以降低防御 eclipse 攻击和窃听攻击的风险，实现了95%的平均F1分数，并且在最安全配置下，CPU使用率可以达到63.2% +-3.5%，网络流量可以达到230 MB +-15 MB。<details>
<summary>Abstract</summary>
The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including random neighbor selection and IP/port switching. The security module is implemented in a DFL platform called Fedstellar, allowing the deployment and monitoring of the federation. A DFL scenario has been deployed, involving eight physical devices implementing three security configurations: (i) a baseline with no security, (ii) an encrypted configuration, and (iii) a configuration integrating both encryption and MTD techniques. The effectiveness of the security module is validated through experiments with the MNIST dataset and eclipse attacks. The results indicated an average F1 score of 95%, with moderate increases in CPU usage (up to 63.2% +-3.5%) and network traffic (230 MB +-15 MB) under the most secure configuration, mitigating the risks posed by eavesdropping or eclipse attacks.
</details>
<details>
<summary>摘要</summary>
《协同学习的分布式协同学习（DFL）技术在训练机器学习模型方面带来了巨大的改变，使得多个参与者之间的模型协同学习可以实现，从而减少依赖于服务器。然而，这种方法引入了一些独特的通信安全挑战，这些挑战主要来自于协同学习过程的分布式特性，参与者的多样化角色和责任，以及缺乏中央权限来监管和处理威胁。为了解决这些挑战，本文首先提出了一个全面的威胁模型，描述了DFL通信的潜在风险。为应对这些风险，本工作提出了一个专门为DFL平台设计的安全模块，该模块结合了加密技术和移动目标防御（MTD）技术，包括随机 neighber 选择和IP/端口 switching。该安全模块在一个名为Fedstellar的DFL平台上实现，allowing the deployment and monitoring of the federation。一个DFL场景已经被部署，并在八个物理设备上实现了三种安全配置：（i）基eline with no security，（ii）加密配置，和（iii） integrate both encryption and MTD techniques。安全模块的有效性通过使用MNIST数据集和eclipse攻击进行实验 validate。结果显示，在最安全的配置下，模型的F1分数平均为95%，CPU使用率提高至63.2% ± 3.5%，网络流量增加至230 MB ± 15 MB。这些结果表明，通过加密和MTD技术，可以有效地防止遮断或eclipse攻击。
</details></li>
</ul>
<hr>
<h2 id="Local-Kernel-Renormalization-as-a-mechanism-for-feature-learning-in-overparametrized-Convolutional-Neural-Networks"><a href="#Local-Kernel-Renormalization-as-a-mechanism-for-feature-learning-in-overparametrized-Convolutional-Neural-Networks" class="headerlink" title="Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks"></a>Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11807">http://arxiv.org/abs/2307.11807</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Aiudi, R. Pacelli, A. Vezzani, R. Burioni, P. Rotondo</li>
<li>for: 这篇论文主要研究了深度神经网络中的特征学习方法，以及它们在不同类型的架构中的表现。</li>
<li>methods: 研究者使用了一种简单的理论框架，来解释FC和CNN架构中特征学习的不同表现。他们首先显示了一个有限宽FC网络的泛化性能可以通过无穷宽网络来获得，并且提出了一种有限宽效果行动来描述CNN架构中的特征学习。</li>
<li>results: 研究者发现了一种简单的特征学习机制，它只能在浅层CNN中发生，而不是在浅层FC网络或者无Weight连接神经网络中。这种机制导致CNN架构在有限宽 régime中表现优秀，而FC网络则是在无穷宽 régime中表现优秀。<details>
<summary>Abstract</summary>
Feature learning, or the ability of deep neural networks to automatically learn relevant features from raw data, underlies their exceptional capability to solve complex tasks. However, feature learning seems to be realized in different ways in fully-connected (FC) or convolutional architectures (CNNs). Empirical evidence shows that FC neural networks in the infinite-width limit eventually outperform their finite-width counterparts. Since the kernel that describes infinite-width networks does not evolve during training, whatever form of feature learning occurs in deep FC architectures is not very helpful in improving generalization. On the other hand, state-of-the-art architectures with convolutional layers achieve optimal performances in the finite-width regime, suggesting that an effective form of feature learning emerges in this case. In this work, we present a simple theoretical framework that provides a rationale for these differences, in one hidden layer networks. First, we show that the generalization performance of a finite-width FC network can be obtained by an infinite-width network, with a suitable choice of the Gaussian priors. Second, we derive a finite-width effective action for an architecture with one convolutional hidden layer and compare it with the result available for FC networks. Remarkably, we identify a completely different form of kernel renormalization: whereas the kernel of the FC architecture is just globally renormalized by a single scalar parameter, the CNN kernel undergoes a local renormalization, meaning that the network can select the local components that will contribute to the final prediction in a data-dependent way. This finding highlights a simple mechanism for feature learning that can take place in overparametrized shallow CNNs, but not in shallow FC architectures or in locally connected neural networks without weight sharing.
</details>
<details>
<summary>摘要</summary>
“特征学习”，也就是深度神经网络自动从原始数据中学习到 relevante 特征的能力，是深度神经网络解决复杂任务的关键。然而，在完全连接（FC）或卷积（CNN）架构中，特征学习似乎存在不同的实现方式。实际证明表明，在无穷宽限制下，FC神经网络 eventually 超越其固定宽度 counterparts。由于无穷宽网络的kernel不会在训练过程中进行变化，因此深度FC架构中的特征学习不会对泛化提供帮助。相反，当前领域的状态艺术架构，卷积层 achiev 最佳性能，表明在这种情况下，特征学习会出现有效的形式。在这种情况下，我们提出了一个简单的理论框架，用于解释这些差异。首先，我们证明了一个有限宽FC网络的泛化性能可以通过无穷宽网络来获得，并且需要一个适当的高斯先验。其次，我们 deriv 有限宽效果动作，并与FC网络的结果进行比较。意外地，我们发现了一种完全不同的kernel renormalization：FC架构的kernel仅受到全局抽象，而CNN架构的kernel则会在数据依赖的方式进行本地抽象，这意味着网络可以在数据中选择本地组分，以便在数据依赖的方式进行预测。这种发现高光了一种简单的特征学习机制，可以在过参神经网络中发生，但不可以在FC架构中或者在没有权重共享的本地神经网络中发生。
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-SGD-for-Training-Neural-Networks-with-Sliced-Wasserstein-Losses"><a href="#Convergence-of-SGD-for-Training-Neural-Networks-with-Sliced-Wasserstein-Losses" class="headerlink" title="Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses"></a>Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11714">http://arxiv.org/abs/2307.11714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eloi Tanguy</li>
<li>for: 本研究的目的是提供对 fixes step SGD 在 SW 损失函数上的分布学习模型 parameters 的趋势，并对这种方法的有效性进行 теории保证。</li>
<li>methods: 本研究使用了 Bianchi et al. (2022) 所提出的非平滑非对称函数下 SGD 的渐进结果，并在这种 Setting 中进行了实际的应用。</li>
<li>results: 研究发现，随着步长减小，SGD 轨迹会接近 (sub) 导流方程，并且在更加严格的假设下，SGD 轨迹会在极限下 approaching 泛化极点。<details>
<summary>Abstract</summary>
Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much stronger convergence result for noised and projected SGD schemes, namely that the long-run limits of the trajectories approach a set of generalised critical points of the loss function.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="JoinGym-An-Efficient-Query-Optimization-Environment-for-Reinforcement-Learning"><a href="#JoinGym-An-Efficient-Query-Optimization-Environment-for-Reinforcement-Learning" class="headerlink" title="JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning"></a>JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11704">http://arxiv.org/abs/2307.11704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiwen Wang, Junxiong Wang, Yueying Li, Nathan Kallus, Immanuel Trummer, Wen Sun</li>
<li>for: 本文提出了一个高效和轻量级的查询优化环境，用于应用智能学习（RL）。</li>
<li>methods: 本文使用了Markov决策过程（MDP）将左深和叶子变种的JoinOrder选择（JOS）问题转化为一个实际的数据管理问题，并提供了遵循标准Gymnasium API的实现。</li>
<li>results: 本文对各种RL算法进行了测试，并发现至少一种方法可以在训练集查询中near-优化性表现，但是在测试集查询中表现下降数个量级。这个差距驱动了进一步的研究，以确定RL算法在多任务 combinatorial优化问题中的泛化能力。<details>
<summary>Abstract</summary>
In this paper, we present \textsc{JoinGym}, an efficient and lightweight query optimization environment for reinforcement learning (RL). Join order selection (JOS) is a classic NP-hard combinatorial optimization problem from database query optimization and can serve as a practical testbed for the generalization capabilities of RL algorithms. We describe how to formulate each of the left-deep and bushy variants of the JOS problem as a Markov Decision Process (MDP), and we provide an implementation adhering to the standard Gymnasium API. We highlight that our implementation \textsc{JoinGym} is completely based on offline traces of all possible joins, which enables RL practitioners to easily and quickly test their methods on a realistic data management problem without needing to setup any systems. Moreover, we also provide all possible join traces on $3300$ novel SQL queries generated from the IMDB dataset. Upon benchmarking popular RL algorithms, we find that at least one method can obtain near-optimal performance on train-set queries but their performance degrades by several orders of magnitude on test-set queries. This gap motivates further research for RL algorithms that generalize well in multi-task combinatorial optimization problems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一个高效和轻量级的查询优化环境，称为JoinGym，用于应急学习（RL）。Join order选择（JOS）是一个经典的NP困难的 combinatorial optimization问题，可以作为RL算法的总结能力的实际测试场景。我们描述了如何将左深和荔枝两种JOS问题转化为Markov决策过程（MDP），并提供了符合标准Gymnasium API的实现。我们指出，我们的实现基于全部可能的连接轨迹，使得RL专家可以轻松地和快速地在真实的数据管理问题上测试自己的方法，不需要设置任何系统。此外，我们还提供了3300个新的SQL查询，这些查询来自IMDB数据集。在 benchmarking 各种RL算法时，我们发现至少一种方法可以在训练集查询上获得近似优秀性能，但是它们在测试集查询上的性能却减少了几个数量级。这个差距激励了我们进一步研究RL算法在多任务 combinatorial optimization 问题中的总结能力。
</details></li>
</ul>
<hr>
<h2 id="Using-simulation-to-calibrate-real-data-acquisition-in-veterinary-medicine"><a href="#Using-simulation-to-calibrate-real-data-acquisition-in-veterinary-medicine" class="headerlink" title="Using simulation to calibrate real data acquisition in veterinary medicine"></a>Using simulation to calibrate real data acquisition in veterinary medicine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11695">http://arxiv.org/abs/2307.11695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krystian Strzałka, Szymon Mazurek, Maciej Wielgosz, Paweł Russek, Jakub Caputa, Daria Łukasik, Jan Krupiński, Jakub Grzeszczyk, Michał Karwatowski, Rafał Frączek, Ernest Jamro, Marcin Pietroń, Sebastian Koryciak, Agnieszka Dąbrowska-Boruch, Kazimierz Wiatr</li>
<li>for: 这个研究旨在使用模拟环境提高动物医学数据收集和诊断，特点是通过使用Blender和Blenderproc库生成具有多种生物学、环境和行为条件的 sintetic数据集，并用这些数据集训练机器学习模型以识别正常和异常的步态。</li>
<li>methods: 这个研究使用了Blender和Blenderproc库生成 sintetic数据集，并使用这些数据集训练机器学习模型。两个不同的数据集，具有不同的摄像头角度细节，被创建以进一步研究摄像头角度对模型准确性的影响。</li>
<li>results: 初步结果表明，通过使用模拟环境和真实病人数据集的组合，这种基于模拟的方法可能会提高动物医学诊断的效果和效率。<details>
<summary>Abstract</summary>
This paper explores the innovative use of simulation environments to enhance data acquisition and diagnostics in veterinary medicine, focusing specifically on gait analysis in dogs. The study harnesses the power of Blender and the Blenderproc library to generate synthetic datasets that reflect diverse anatomical, environmental, and behavioral conditions. The generated data, represented in graph form and standardized for optimal analysis, is utilized to train machine learning algorithms for identifying normal and abnormal gaits. Two distinct datasets with varying degrees of camera angle granularity are created to further investigate the influence of camera perspective on model accuracy. Preliminary results suggest that this simulation-based approach holds promise for advancing veterinary diagnostics by enabling more precise data acquisition and more effective machine learning models. By integrating synthetic and real-world patient data, the study lays a robust foundation for improving overall effectiveness and efficiency in veterinary medicine.
</details>
<details>
<summary>摘要</summary>
这个研究paper explores the innovative use of simulation environments to enhance data acquisition and diagnostics in veterinary medicine, focusing specifically on gait analysis in dogs. The study harnesses the power of Blender and the Blenderproc library to generate synthetic datasets that reflect diverse anatomical, environmental, and behavioral conditions. The generated data, represented in graph form and standardized for optimal analysis, is utilized to train machine learning algorithms for identifying normal and abnormal gaits. Two distinct datasets with varying degrees of camera angle granularity are created to further investigate the influence of camera perspective on model accuracy. Preliminary results suggest that this simulation-based approach holds promise for advancing veterinary diagnostics by enabling more precise data acquisition and more effective machine learning models. By integrating synthetic and real-world patient data, the study lays a robust foundation for improving overall effectiveness and efficiency in veterinary medicine.Here's the text with traditional Chinese characters:这个研究paper explores the innovative use of simulation environments to enhance data acquisition and diagnostics in veterinary medicine, focusing specifically on gait analysis in dogs. The study harnesses the power of Blender and the Blenderproc library to generate synthetic datasets that reflect diverse anatomical, environmental, and behavioral conditions. The generated data, represented in graph form and standardized for optimal analysis, is utilized to train machine learning algorithms for identifying normal and abnormal gaits. Two distinct datasets with varying degrees of camera angle granularity are created to further investigate the influence of camera perspective on model accuracy. Preliminary results suggest that this simulation-based approach holds promise for advancing veterinary diagnostics by enabling more precise data acquisition and more effective machine learning models. By integrating synthetic and real-world patient data, the study lays a robust foundation for improving overall effectiveness and efficiency in veterinary medicine.
</details></li>
</ul>
<hr>
<h2 id="Fast-Adaptive-Test-Time-Defense-with-Robust-Features"><a href="#Fast-Adaptive-Test-Time-Defense-with-Robust-Features" class="headerlink" title="Fast Adaptive Test-Time Defense with Robust Features"></a>Fast Adaptive Test-Time Defense with Robust Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11672">http://arxiv.org/abs/2307.11672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurag Singh, Mahalakshmi Sabanayagam, Krikamol Muandet, Debarghya Ghoshdastidar</li>
<li>for: 提高深度神经网络的对抗性性能</li>
<li>methods: 基于特征稳定性的抗击攻击策略</li>
<li>results: 在CIFAR-10和CIFAR-100数据集上，与现有最佳方法相比，提出的方法具有较低的计算成本，且对抗性性能较高。<details>
<summary>Abstract</summary>
Adaptive test-time defenses are used to improve the robustness of deep neural networks to adversarial examples. However, existing methods significantly increase the inference time due to additional optimization on the model parameters or the input at test time. In this work, we propose a novel adaptive test-time defense strategy that is easy to integrate with any existing (robust) training procedure without additional test-time computation. Based on the notion of robustness of features that we present, the key idea is to project the trained models to the most robust feature space, thereby reducing the vulnerability to adversarial attacks in non-robust directions. We theoretically show that the top eigenspace of the feature matrix are more robust for a generalized additive model and support our argument for a large width neural network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive experiments on CIFAR-10 and CIFAR-100 datasets for several robustness benchmarks, including the state-of-the-art methods in RobustBench, and observe that the proposed method outperforms existing adaptive test-time defenses at much lower computation costs.
</details>
<details>
<summary>摘要</summary>
使用适应性测试时防御，提高深度神经网络对攻击示例的Robustness。然而，现有方法会significantly增加测试时间，因为它们需要在测试时进行额外的优化模型参数或输入。在这项工作中，我们提出了一种新的适应测试时防御策略，可以轻松地与任何现有的Robust训练过程集成，无需额外的测试时间计算。我们基于特征空间的Robustness提出了一个新的思路，即将训练模型映射到最Robust的特征空间，以降低非Robust方向的攻击性。我们理论上显示，通过对特征矩阵的top射影空间进行投影，可以提高一般加法模型的Robustness。我们在CIFAR-10和CIFAR-100数据集上进行了广泛的实验，包括RobustBench状态OF-the-art方法，并观察到我们提出的方法在计算成本远低于现有适应测试时防御方法时仍能够获得更高的Robustness性。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Interior-Point-Method-for-Online-Convex-Optimization"><a href="#An-Efficient-Interior-Point-Method-for-Online-Convex-Optimization" class="headerlink" title="An Efficient Interior-Point Method for Online Convex Optimization"></a>An Efficient Interior-Point Method for Online Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11668">http://arxiv.org/abs/2307.11668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elad Hazan, Nimrod Megiddo</li>
<li>for: 这个论文是为了最小化在线凸优化中的遗弃量而写的。</li>
<li>methods: 这个论文使用了一种新的算法来最小化遗弃量，该算法是适应的，meaning its regret bounds hold not only for the time periods 1,…，T but also for every sub-interval s,s+1,…，t。</li>
<li>results: 这个论文的结果表明，该算法的遗弃量为O（√T log T），这是最小化遗弃量的下限，只有一个 logs 项。<details>
<summary>Abstract</summary>
A new algorithm for regret minimization in online convex optimization is described. The regret of the algorithm after $T$ time periods is $O(\sqrt{T \log T})$ - which is the minimum possible up to a logarithmic term. In addition, the new algorithm is adaptive, in the sense that the regret bounds hold not only for the time periods $1,\ldots,T$ but also for every sub-interval $s,s+1,\ldots,t$. The running time of the algorithm matches that of newly introduced interior point algorithms for regret minimization: in $n$-dimensional space, during each iteration the new algorithm essentially solves a system of linear equations of order $n$, rather than solving some constrained convex optimization problem in $n$ dimensions and possibly many constraints.
</details>
<details>
<summary>摘要</summary>
新的算法可以最小化 regret 在在线凸优化中描述。这个算法在 $T$ 时间段后的 regret 是 $O(\sqrt{T \log T})$，这是最低的，只有一个对数性 терMINOLOGY。此外，这个新算法是可适应的，意味着其 regret 约束不仅适用于时间段 $1,\ldots,T$，还适用于每个子时间段 $s,s+1,\ldots,t$。算法的运行时间与新引入的内部点算法一样，在 $n$ 维空间中，每次迭代中，新算法基本上解决了一个线性方程组问题，而不是解决一个凸优化问题并且可能有很多约束。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/cs.LG_2023_07_22/" data-id="clorjzl8100mhf1884uuu4e2e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/73/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/72/">72</a><a class="page-number" href="/page/73/">73</a><span class="page-number current">74</span><a class="page-number" href="/page/75/">75</a><a class="page-number" href="/page/76/">76</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/75/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
