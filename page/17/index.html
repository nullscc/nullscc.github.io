
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/17/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/cs.LG_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T10:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/cs.LG_2023_10_18/">cs.LG - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="No-Regret-Learning-in-Bilateral-Trade-via-Global-Budget-Balance"><a href="#No-Regret-Learning-in-Bilateral-Trade-via-Global-Budget-Balance" class="headerlink" title="No-Regret Learning in Bilateral Trade via Global Budget Balance"></a>No-Regret Learning in Bilateral Trade via Global Budget Balance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12370">http://arxiv.org/abs/2310.12370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco</li>
<li>for: 本文研究在双方均有私人估价的 bilateral trade 问题中，learner 设定价格，无知到代理人的估价。</li>
<li>methods: 本文引入全局预算平衡的概念，要求代理人在整个时间轴上保持预算平衡。通过全局预算平衡，提供了首个不负 regret 算法，在不同反馈模型下对 adversarial 输入进行评估。</li>
<li>results: 在全部反馈模型中，learner 可以保证 $\tilde{O}(\sqrt{T})$ regret，与最佳固定价格相比，是OPTimal的。在partial feedback模型中，提供了一个 $\tilde{O}(T^{3&#x2F;4})$ regret Upper bound的算法，并且 complement with a nearly-matching lower bound。<details>
<summary>Abstract</summary>
Bilateral trade revolves around the challenge of facilitating transactions between two strategic agents -- a seller and a buyer -- both of whom have a private valuations for the item. We study the online version of the problem, in which at each time step a new seller and buyer arrive. The learner's task is to set a price for each agent, without any knowledge about their valuations. The sequence of sellers and buyers is chosen by an oblivious adversary. In this setting, known negative results rule out the possibility of designing algorithms with sublinear regret when the learner has to guarantee budget balance for each iteration. In this paper, we introduce the notion of global budget balance, which requires the agent to be budget balance only over the entire time horizon. By requiring global budget balance, we provide the first no-regret algorithms for bilateral trade with adversarial inputs under various feedback models. First, we show that in the full-feedback model the learner can guarantee $\tilde{O}(\sqrt{T})$ regret against the best fixed prices in hindsight, which is order-wise optimal. Then, in the case of partial feedback models, we provide an algorithm guaranteeing a $\tilde{O}(T^{3/4})$ regret upper bound with one-bit feedback, which we complement with a nearly-matching lower bound. Finally, we investigate how these results vary when measuring regret using an alternative benchmark.
</details>
<details>
<summary>摘要</summary>
bilateral trade 环绕着两个策略代理人（一个买家和一个卖家）之间的挑战，这两个代理人都有私人的评价值。我们研究在网络上进行的这个问题，在每个时间步骤中，新的买家和卖家会出现。学习者的任务是设定价格，但是没有任何关于代理人们的评价知识。选择序列的买家和卖家是由一个无知的敌人选择。在这个设定下，已知的负结果规则排除了设计具有下图 regret 的算法的可能性。在这篇论文中，我们引入全面预算平衡的概念，它需要学习者在整个时间频谱上保持预算平衡。通过需要全面预算平衡，我们提供了首个不负担 regret 的算法，在不同的反馈模型下实现了双方贸易。首先，在完整反馈模型中，我们显示学习者可以在对照后获得 $\tilde{O}(\sqrt{T})$ regret，这是很好的估计。然后，在受限反馈模型中，我们提供了一个 garantia $\tilde{O}(T^{3/4})$ regret 的算法，并补充了一个几乎匹配的下限。最后，我们调查了这些结果如何在使用不同的参考基准时变化。
</details></li>
</ul>
<hr>
<h2 id="MARVEL-Multi-Agent-Reinforcement-Learning-for-Large-Scale-Variable-Speed-Limits"><a href="#MARVEL-Multi-Agent-Reinforcement-Learning-for-Large-Scale-Variable-Speed-Limits" class="headerlink" title="MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits"></a>MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12359">http://arxiv.org/abs/2310.12359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Zhang, Marcos Quinones-Grueiro, Zhiyao Zhang, Yanbing Wang, William Barbour, Gautam Biswas, Daniel Work</li>
<li>for: 这个论文的目的是提出一种基于多代理学习（MARL）的大规模Variable Speed Limit（VSL）控制策略，以提高交通安全性和流体性。</li>
<li>methods: 该论文使用MARL框架，通过使用常见的数据来实现大规模VSL控制。代理学习算法通过考虑交通条件的变化、安全性和流体性的奖励结构进行学习，从而实现代理之间的协调。</li>
<li>results: 对于一段7英里的高速公路，MARL方法提高了交通安全性63.4%，并提高了交通流体性14.6%，相比于现有的实践算法。此外，文章还进行了解释性分析，以了解代理在不同交通条件下的决策过程。最后，文章测试了在实际数据上的策略，以证明该策略的可部署性。<details>
<summary>Abstract</summary>
Variable speed limit (VSL) control is a promising traffic management strategy for enhancing safety and mobility. This work introduces MARVEL, a multi-agent reinforcement learning (MARL) framework for implementing large-scale VSL control on freeway corridors using only commonly available data. The agents learn through a reward structure that incorporates adaptability to traffic conditions, safety, and mobility; enabling coordination among the agents. The proposed framework scales to cover corridors with many gantries thanks to a parameter sharing among all VSL agents. The agents are trained in a microsimulation environment based on a short freeway stretch with 8 gantries spanning 7 miles and tested with 34 gantries spanning 17 miles of I-24 near Nashville, TN. MARVEL improves traffic safety by 63.4% compared to the no control scenario and enhances traffic mobility by 14.6% compared to a state-of-the-practice algorithm that has been deployed on I-24. An explainability analysis is undertaken to explore the learned policy under different traffic conditions and the results provide insights into the decision-making process of agents. Finally, we test the policy learned from the simulation-based experiments on real input data from I-24 to illustrate the potential deployment capability of the learned policy.
</details>
<details>
<summary>摘要</summary>
Variable speed limit (VSL) 控制是一种有前途的交通管理策略，可以提高安全性和流动性。这项工作介绍了 MARVEL，一种多代理学习 (MARL) 框架，用于实现大规模 VSL 控制在高速公路段上，只使用常见的数据。代理学习的奖励结构包括适应交通条件、安全性和流动性，使代理之间协调。提出的框架可以涵盖覆盖许多斜塔，因为所有 VSL 代理的参数共享。代理在基于微观 simulate 环境中学习，该环境基于一段长7英里的高速公路，涵盖8个斜塔。代理在基于实际数据进行测试，并在I-24公路上进行了17英里的测试。 MARVEL 可以提高交通安全性63.4%，并提高交通流动性14.6%，相比之前的实践算法。 Explainability 分析用于探索不同交通条件下代理学习的策略，结果提供了决策过程中代理的启示。最后，我们将在实际数据上测试从 simulate 中学习的策略，以 illustrate 学习的可部署性。
</details></li>
</ul>
<hr>
<h2 id="Networkwide-Traffic-State-Forecasting-Using-Exogenous-Information-A-Multi-Dimensional-Graph-Attention-Based-Approach"><a href="#Networkwide-Traffic-State-Forecasting-Using-Exogenous-Information-A-Multi-Dimensional-Graph-Attention-Based-Approach" class="headerlink" title="Networkwide Traffic State Forecasting Using Exogenous Information: A Multi-Dimensional Graph Attention-Based Approach"></a>Networkwide Traffic State Forecasting Using Exogenous Information: A Multi-Dimensional Graph Attention-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12353">http://arxiv.org/abs/2310.12353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Islam, Monika Filipovska</li>
<li>for: 这篇论文主要针对交通管理和控制策略中的交通状态预测问题，以及用户和系统层次的决策。</li>
<li>methods: 该论文提出了一种基于多维空间时间图注意力网络（M-STGAT）的交通预测方法，该方法使用过去观测到的速度、路况事件、温度和视程，并利用交通网络的结构来学习。</li>
<li>results: 实验结果表明，M-STGAT在使用加利福尼亚交通部门（Caltrans）性能衡量系统（PeMS）提供的交通速度和路况数据，并与国家海洋和大气管理局（NOAA）自动Surface Observing Systems（ASOS）提供的天气数据进行比较，在30、45和60分钟预测时间 horizons 上表现出了较好的预测性能，其中error measures包括 Mean Absolute Error（MAE）、Root Mean Square Error（RMSE）和Mean Absolute Percentage Error（MAPE）。但是，模型的传送性可能需要进一步的调查。<details>
<summary>Abstract</summary>
Traffic state forecasting is crucial for traffic management and control strategies, as well as user- and system-level decision making in the transportation network. While traffic forecasting has been approached with a variety of techniques over the last couple of decades, most approaches simply rely on endogenous traffic variables for state prediction, despite the evidence that exogenous factors can significantly impact traffic conditions. This paper proposes a multi-dimensional spatio-temporal graph attention-based traffic prediction approach (M-STGAT), which predicts traffic based on past observations of speed, along with lane closure events, temperature, and visibility across the transportation network. The approach is based on a graph attention network architecture, which also learns based on the structure of the transportation network on which these variables are observed. Numerical experiments are performed using traffic speed and lane closure data from the California Department of Transportation (Caltrans) Performance Measurement System (PeMS). The corresponding weather data were downloaded from the National Oceanic and Atmospheric Administration (NOOA) Automated Surface Observing Systems (ASOS). For comparison, the numerical experiments implement three alternative models which do not allow for the multi-dimensional input. The M-STGAT is shown to outperform the three alternative models, when performing tests using our primary data set for prediction with a 30-, 45-, and 60-minute prediction horizon, in terms of three error measures: Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE). However, the model's transferability can vary for different transfer data sets and this aspect may require further investigation.
</details>
<details>
<summary>摘要</summary>
交通状况预测是交通管理和控制策略以及用户和系统层次的决策中非常重要的一环。自过去几十年来，交通预测已经使用了多种技术，但大多数方法都仅仅基于内生的交通变量进行预测，尽管外生因素可能对交通条件产生重要影响。这篇论文提出了一种多维度空间时间图注意力基本交通预测方法（M-STGAT），该方法基于过去观测到的速度，以及路段 closure事件、温度和视程等外生因素进行预测。该方法基于图注意力网络架构，同时还学习了交通网络上这些变量的结构。我们使用了加利福尼亚交通部门（Caltrans）性能测量系统（PeMS）中的交通速度和路段 closure数据进行数值实验，并下载了国家海洋和大气管理局（NOAA）自动地面观测系统（ASOS）中的天气数据。为比较，我们实现了三种不允许多维度输入的数学模型。M-STGAT在使用我们的主要数据集进行预测时，在30-, 45-, 和 60-分钟预测距离时表现出了与三个错误度量（ Mean Absolute Error，Root Mean Square Error 和 Mean Absolute Percentage Error）相对较高的性能。然而，模型的传输性可能会随着不同的传输数据集而异。这一点可能需要进一步的调查。
</details></li>
</ul>
<hr>
<h2 id="Equipping-Federated-Graph-Neural-Networks-with-Structure-aware-Group-Fairness"><a href="#Equipping-Federated-Graph-Neural-Networks-with-Structure-aware-Group-Fairness" class="headerlink" title="Equipping Federated Graph Neural Networks with Structure-aware Group Fairness"></a>Equipping Federated Graph Neural Networks with Structure-aware Group Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12350">http://arxiv.org/abs/2310.12350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuening-lab/f2gnn">https://github.com/yuening-lab/f2gnn</a></li>
<li>paper_authors: Nan Cui, Xiuling Wang, Wendy Hui Wang, Violet Chen, Yue Ning</li>
<li>for: 这篇论文旨在提出一种解决联合学习中Graph Neural Networks（GNNs）中的偏见问题的方法，以保证GNNs在分布式学习中保持公平性。</li>
<li>methods: 该方法基于两个关键组成部分：一是客户端上的公平性意识更新方案，二是在集成过程中考虑本地模型的公平性指标和数据偏见指标的全球模型更新方案。</li>
<li>results: 该方法在许多基准方法上表现出色，在公平性和模型准确性两个方面均有显著提升。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have been widely used for various types of graph data processing and analytical tasks in different domains. Training GNNs over centralized graph data can be infeasible due to privacy concerns and regulatory restrictions. Thus, federated learning (FL) becomes a trending solution to address this challenge in a distributed learning paradigm. However, as GNNs may inherit historical bias from training data and lead to discriminatory predictions, the bias of local models can be easily propagated to the global model in distributed settings. This poses a new challenge in mitigating bias in federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair Federated Graph Neural Network, that enhances group fairness of federated GNNs. As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN aims to mitigate both types of bias under federated settings. First, we provide theoretical insights on the connection between data bias in a training graph and statistical fairness metrics of the trained GNN models. Based on the theoretical analysis, we design $\text{F}^2$GNN which contains two key components: a fairness-aware local model update scheme that enhances group fairness of the local models on the client side, and a fairness-weighted global model update scheme that takes both data bias and fairness metrics of local models into consideration in the aggregation process. We evaluate $\text{F}^2$GNN empirically versus a number of baseline methods, and demonstrate that $\text{F}^2$GNN outperforms these baselines in terms of both fairness and model accuracy.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) 在不同领域中对各种图数据进行处理和分析任务广泛使用。在中央化图数据上训练 GNNs 可能因为隐私问题和管制约束而成为不可能的。因此，联邦学习 (FL) 成为一种解决这个挑战的趋势。然而， GNNs 可能从训练数据中继承历史偏见，并导致歧视性预测，因此在分布式设置下，本地模型的偏见可能被轻松传播到全球模型。这种挑战需要解决偏见在联邦 GNN 中的问题。为此，我们提出了 $\text{F}^2$GNN，一种增强分布式 Graph Neural Network 的分组公平性。由于偏见可以来自数据和学习算法，$\text{F}^2$GNN 采用了两个关键组成部分：在客户端上使用公平性意识的本地模型更新方案，以及在聚合过程中考虑本地模型的公平性度量和数据偏见的准确度。我们对 $\text{F}^2$GNN 进行了理论分析，并对其与一些基准方法进行了实验比较，并证明 $\text{F}^2$GNN 在公平性和模型准确性两个方面都高于基准方法。
</details></li>
</ul>
<hr>
<h2 id="Tracking-electricity-losses-and-their-perceived-causes-using-nighttime-light-and-social-media"><a href="#Tracking-electricity-losses-and-their-perceived-causes-using-nighttime-light-and-social-media" class="headerlink" title="Tracking electricity losses and their perceived causes using nighttime light and social media"></a>Tracking electricity losses and their perceived causes using nighttime light and social media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12346">http://arxiv.org/abs/2310.12346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel W Kerber, Nicholas A Duncan, Guillaume F LHer, Morgan Bazilian, Chris Elvidge, Mark R Deinert</li>
<li>for: 本研究旨在使用卫星图像、社交媒体和信息提取技术监测停电和其所报导的原因。</li>
<li>methods: 本研究使用了夜晚照明数据（2019年3月的加拉加斯，委内瑞拉），并通过Twitter数据分析公众对停电的感受和看法，以及使用统计分析和话题模型探讨公众归咎政府的停电原因。</li>
<li>results: 研究发现，夜晚照明强度与停电Region之间存在 inverse 关系。twitter上提到委内瑞拉总统的帖子具有更高的负面性和更多的责任相关词汇，这表明公众归咎政府对停电的责任。<details>
<summary>Abstract</summary>
Urban environments are intricate systems where the breakdown of critical infrastructure can impact both the economic and social well-being of communities. Electricity systems hold particular significance, as they are essential for other infrastructure, and disruptions can trigger widespread consequences. Typically, assessing electricity availability requires ground-level data, a challenge in conflict zones and regions with limited access. This study shows how satellite imagery, social media, and information extraction can monitor blackouts and their perceived causes. Night-time light data (in March 2019 for Caracas, Venezuela) is used to indicate blackout regions. Twitter data is used to determine sentiment and topic trends, while statistical analysis and topic modeling delved into public perceptions regarding blackout causes. The findings show an inverse relationship between nighttime light intensity. Tweets mentioning the Venezuelan President displayed heightened negativity and a greater prevalence of blame-related terms, suggesting a perception of government accountability for the outages.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Multivariate-Time-Series-Anomaly-Detection"><a href="#Open-Set-Multivariate-Time-Series-Anomaly-Detection" class="headerlink" title="Open-Set Multivariate Time-Series Anomaly Detection"></a>Open-Set Multivariate Time-Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12294">http://arxiv.org/abs/2310.12294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Lai, Thi Kieu Khanh Ho, Narges Armanfard</li>
<li>for: 提出了一种新的方法来解决时间序列异常检测（TSAD）问题，即在训练阶段只有有限的异常样本可用，并且需要检测未经见过的异常类型。</li>
<li>methods: 该方法包括三个主要模块：特征提取器、多头网络和异常评分模块。特征提取器抽取有用的时间序列特征，多头网络包括生成-, 偏差-和对比头，用于捕捉已见和未见异常类型。</li>
<li>results: 对三个实际数据集进行了广泛的实验，结果显示，我们的方法在不同的设定下都能够超越现有方法，从而在TSAD领域实现了新的状态级表现。<details>
<summary>Abstract</summary>
Numerous methods for time series anomaly detection (TSAD) methods have emerged in recent years. Most existing methods are unsupervised and assume the availability of normal training samples only, while few supervised methods have shown superior performance by incorporating labeled anomalous samples in the training phase. However, certain anomaly types are inherently challenging for unsupervised methods to differentiate from normal data, while supervised methods are constrained to detecting anomalies resembling those present during training, failing to generalize to unseen anomaly classes. This paper is the first attempt in providing a novel approach for the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are visible in the training phase, with the objective of detecting both seen and unseen anomaly classes in the test phase. The proposed method, called Multivariate Open-Set timeseries Anomaly Detection (MOSAD) consists of three primary modules: a Feature Extractor to extract meaningful time-series features; a Multi-head Network consisting of Generative-, Deviation-, and Contrastive heads for capturing both seen and unseen anomaly classes; and an Anomaly Scoring module leveraging the insights of the three heads to detect anomalies. Extensive experiments on three real-world datasets consistently show that our approach surpasses existing methods under various experimental settings, thus establishing a new state-of-the-art performance in the TSAD field.
</details>
<details>
<summary>摘要</summary>
Recently, many time series anomaly detection (TSAD) methods have been proposed. Most of these methods are unsupervised and assume the availability of normal training samples, while only a few supervised methods have shown better performance by incorporating labeled anomalous samples in the training phase. However, some anomaly types are difficult for unsupervised methods to distinguish from normal data, while supervised methods are limited to detecting anomalies similar to those present during training and cannot handle unseen anomaly classes. This paper is the first attempt to solve the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are available during training, with the goal of detecting both seen and unseen anomaly classes in the test phase.The proposed method, called Multivariate Open-Set Time Series Anomaly Detection (MOSAD), consists of three primary modules: a Feature Extractor to extract meaningful time-series features; a Multi-head Network consisting of Generative-, Deviation-, and Contrastive heads to capture both seen and unseen anomaly classes; and an Anomaly Scoring module that leverages the insights of the three heads to detect anomalies. Extensive experiments on three real-world datasets consistently show that our approach outperforms existing methods under various experimental settings, thereby establishing a new state-of-the-art performance in the TSAD field.
</details></li>
</ul>
<hr>
<h2 id="A-PAC-Learning-Algorithm-for-LTL-and-Omega-regular-Objectives-in-MDPs"><a href="#A-PAC-Learning-Algorithm-for-LTL-and-Omega-regular-Objectives-in-MDPs" class="headerlink" title="A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs"></a>A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12248">http://arxiv.org/abs/2310.12248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateo Perez, Fabio Somenzi, Ashutosh Trivedi</li>
<li>for: 表示非Markov决策学中的非Markov目标，使用线性时间逻辑（LTL）和ω-正则目标。</li>
<li>methods: 使用模型基于可能approx Correct（PAC）学习算法，从系统轨迹样本中学习。不需要先知系统结构。</li>
<li>results: 学习omega-正则目标的Markov决策过程中的可能approx Correct算法。<details>
<summary>Abstract</summary>
Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL -- have seen recent use as a way to express non-Markovian objectives in reinforcement learning. We introduce a model-based probably approximately correct (PAC) learning algorithm for omega-regular objectives in Markov decision processes. Unlike prior approaches, our algorithm learns from sampled trajectories of the system and does not require prior knowledge of the system's topology.
</details>
<details>
<summary>摘要</summary>
线性时间逻辑（LTL）和奥米加常量目标——LTL的超集——在人工智能中被用来表达非马普朗的目标。我们介绍了基于模型的可能相对正确（PAC）学习算法 для奥米加常量目标在Markov决策过程中。与先前的方法不同，我们的算法从系统样本轨迹中学习，而不需要先知系统结构。
</details></li>
</ul>
<hr>
<h2 id="Fast-Parameter-Inference-on-Pulsar-Timing-Arrays-with-Normalizing-Flows"><a href="#Fast-Parameter-Inference-on-Pulsar-Timing-Arrays-with-Normalizing-Flows" class="headerlink" title="Fast Parameter Inference on Pulsar Timing Arrays with Normalizing Flows"></a>Fast Parameter Inference on Pulsar Timing Arrays with Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12209">http://arxiv.org/abs/2310.12209</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Shih, Marat Freytsis, Stephen R. Taylor, Jeff A. Dror, Nolan Smyth</li>
<li>for: 这个论文是为了提高�ulsar时间尺度数组（PTAs）的 Bayesian posterior inference 的效率而写的。</li>
<li>methods: 这篇论文使用了模拟数据生成的 conditional normalizing flows 技术来快速和准确地计算狮子时间尺度数组（SGWB）的 posterior distribution，从原来的数天减少到只需几秒钟。</li>
<li>results: 该论文的实验结果表明，使用 conditional normalizing flows 技术可以在狮子时间尺度数组（SGWB）的 posterior distribution 计算中大幅提高效率，从原来的数天减少到只需几秒钟。<details>
<summary>Abstract</summary>
Pulsar timing arrays (PTAs) perform Bayesian posterior inference with expensive MCMC methods. Given a dataset of ~10-100 pulsars and O(10^3) timing residuals each, producing a posterior distribution for the stochastic gravitational wave background (SGWB) can take days to a week. The computational bottleneck arises because the likelihood evaluation required for MCMC is extremely costly when considering the dimensionality of the search space. Fortunately, generating simulated data is fast, so modern simulation-based inference techniques can be brought to bear on the problem. In this paper, we demonstrate how conditional normalizing flows trained on simulated data can be used for extremely fast and accurate estimation of the SGWB posteriors, reducing the sampling time from weeks to a matter of seconds.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dynamic-financial-processes-identification-using-sparse-regressive-reservoir-computers"><a href="#Dynamic-financial-processes-identification-using-sparse-regressive-reservoir-computers" class="headerlink" title="Dynamic financial processes identification using sparse regressive reservoir computers"></a>Dynamic financial processes identification using sparse regressive reservoir computers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12144">http://arxiv.org/abs/2310.12144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredyvides/dynet-cnbs">https://github.com/fredyvides/dynet-cnbs</a></li>
<li>paper_authors: Fredy Vides, Idelfonso B. R. Nogueira, Lendy Banegas, Evelyn Flores</li>
<li>For: 本文研究结构矩阵近似理论，应用于财经系统动态过程的回归表示。* Methods: 使用非线性时间延迟嵌入、稀疏最小二乘和结构矩阵近似方法来探索财经系统的输出封顶矩阵的近似表示。* Results: 通过应用上述技术，可以实现财经系统动态过程的近似识别和预测，包括可能或可能不具有混沌行为的场景。<details>
<summary>Abstract</summary>
In this document, we present key findings in structured matrix approximation theory, with applications to the regressive representation of dynamic financial processes. Initially, we explore a comprehensive approach involving generic nonlinear time delay embedding for time series data extracted from a financial or economic system under examination. Subsequently, we employ sparse least-squares and structured matrix approximation methods to discern approximate representations of the output coupling matrices. These representations play a pivotal role in establishing the regressive models corresponding to the recursive structures inherent in a given financial system. The document further introduces prototypical algorithms that leverage the aforementioned techniques. These algorithms are demonstrated through applications in approximate identification and predictive simulation of dynamic financial and economic processes, encompassing scenarios that may or may not exhibit chaotic behavior.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了结构化矩阵近似理论的关键发现，并应用于金融或经济系统中的回归表现力学过程的重构表示。我们首先探讨了一种通用非线性时间延迟嵌入方法，用于从金融或经济系统中提取时间序列数据。然后，我们使用稀疏最小二乘和结构矩阵近似方法来推导出输出 coupling 矩阵的近似表示。这些表示在建立金融系统中的回归模型中扮演关键角色。文章还介绍了一些原型算法，这些算法利用上述技术来实现精度的回归预测和模拟。这些算法在不同的金融和经济过程中的应用中得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Automatic-prediction-of-mortality-in-patients-with-mental-illness-using-electronic-health-records"><a href="#Automatic-prediction-of-mortality-in-patients-with-mental-illness-using-electronic-health-records" class="headerlink" title="Automatic prediction of mortality in patients with mental illness using electronic health records"></a>Automatic prediction of mortality in patients with mental illness using electronic health records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12121">http://arxiv.org/abs/2310.12121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Kim, Samuel Kim</li>
<li>for: 预测精神疾病患者30天 mortality rate</li>
<li>methods: 使用predictive machine-learning models with electronic health records (EHR)，包括Logistic Regression、Random Forest、Support Vector Machine和K-Nearest Neighbors四种机器学习算法</li>
<li>results: Random Forest和Support Vector Machine模型表现最佳，AUC分数为0.911，Feature importance分析显示 morphine sulfate等药物具有预测作用。<details>
<summary>Abstract</summary>
Mental disorders impact the lives of millions of people globally, not only impeding their day-to-day lives but also markedly reducing life expectancy. This paper addresses the persistent challenge of predicting mortality in patients with mental diagnoses using predictive machine-learning models with electronic health records (EHR). Data from patients with mental disease diagnoses were extracted from the well-known clinical MIMIC-III data set utilizing demographic, prescription, and procedural information. Four machine learning algorithms (Logistic Regression, Random Forest, Support Vector Machine, and K-Nearest Neighbors) were used, with results indicating that Random Forest and Support Vector Machine models outperformed others, with AUC scores of 0.911. Feature importance analysis revealed that drug prescriptions, particularly Morphine Sulfate, play a pivotal role in prediction. We applied a variety of machine learning algorithms to predict 30-day mortality followed by feature importance analysis. This study can be used to assist hospital workers in identifying at-risk patients to reduce excess mortality.
</details>
<details>
<summary>摘要</summary>
精神疾病影响全球数百万人的生活，不仅妨碍日常生活，而且明显减少生存期。本文使用可预测机器学习模型和电子健康纪录（EHR）预测患有精神诊断的患者 mortality。从 клиничеwell-known MIMIC-III数据集中提取了患有精神疾病诊断的患者数据，并使用LOGISTIC REGRESSION、Random Forest、Support Vector Machine和K-Nearest Neighbors四种机器学习算法。结果表明，Random Forest和Support Vector Machine模型在其他四个模型中表现最佳，AUC分数为0.911。特征重要性分析显示，药物处方，特别是摩革定（Morphine Sulfate），在预测中扮演着关键性角色。我们通过不同的机器学习算法预测30天内死亡，并进行特征重要性分析，以帮助医院工作人员识别高风险患者，从而减少过度死亡。
</details></li>
</ul>
<hr>
<h2 id="MMD-based-Variable-Importance-for-Distributional-Random-Forest"><a href="#MMD-based-Variable-Importance-for-Distributional-Random-Forest" class="headerlink" title="MMD-based Variable Importance for Distributional Random Forest"></a>MMD-based Variable Importance for Distributional Random Forest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12115">http://arxiv.org/abs/2310.12115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clément Bénard, Jeffrey Näf, Julie Josse</li>
<li>for: 这篇论文目的是提出一种基于森林方法的全Conditional分布估计方法，用于 Multivariate output of interest 的输入变量。</li>
<li>methods: 该论文使用了 Drop and relearn 原理和MMD距离来实现变量重要性度量，而传统的重要性度量仅检测输出均值的影响变量。</li>
<li>results: 引入的重要性度量是一致的，在实际数据和模拟数据上具有高效性，并且超越竞争者。特别是，该算法可以通过回归特征减少来选择变量，从而提供高精度的Conditional输出分布估计。<details>
<summary>Abstract</summary>
Distributional Random Forest (DRF) is a flexible forest-based method to estimate the full conditional distribution of a multivariate output of interest given input variables. In this article, we introduce a variable importance algorithm for DRFs, based on the well-established drop and relearn principle and MMD distance. While traditional importance measures only detect variables with an influence on the output mean, our algorithm detects variables impacting the output distribution more generally. We show that the introduced importance measure is consistent, exhibits high empirical performance on both real and simulated data, and outperforms competitors. In particular, our algorithm is highly efficient to select variables through recursive feature elimination, and can therefore provide small sets of variables to build accurate estimates of conditional output distributions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用 Distributional Random Forest（DRF）来估算输入变量的多变量输出分布。在本文中，我们提出了基于drop和重新学习原则以及MMD距离的变量重要性算法，该算法可捕捉输入变量对输出分布的影响，而不仅仅是输出均值。我们证明了该算法的一致性和高效性，并在实际和预测数据上实现了比较高的表现。特别是，我们的算法可以通过 recursively feature elimination来选择变量，从而快速建立高精度的 conditional output distribution 估计。Translation notes:* "Distributional Random Forest" is translated as "多变量随机森林" (mányuànxīn sēn lín)* "full conditional distribution" is translated as "完整的分布" (quèzhè de fēn xiǎng)* "variable importance" is translated as "变量重要性" (biànxīn zhòng yào xìng)* "drop and relearn principle" is translated as "drop和重新学习原则" (drop hé zhòng xīn xué xí yuè)* "MMD distance" is translated as "MMD距离" (MMD jù lù)* "recursive feature elimination" is translated as " recursively feature elimination" (jiē yǐjī zhì xiǎng fāng yì)
</details></li>
</ul>
<hr>
<h2 id="Monarch-Mixer-A-Simple-Sub-Quadratic-GEMM-Based-Architecture"><a href="#Monarch-Mixer-A-Simple-Sub-Quadratic-GEMM-Based-Architecture" class="headerlink" title="Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"></a>Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12109">http://arxiv.org/abs/2310.12109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HazyResearch/m2">https://github.com/HazyResearch/m2</a></li>
<li>paper_authors: Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin Spector, Michael Poli, Atri Rudra, Christopher Ré</li>
<li>for: 这篇论文目的是探讨是否存在可以在序列长度和模型维度上呈现高性能的、不 quadratic 的机器学习模型。</li>
<li>methods: 该论文提出了一种新的 Monarch Mixer（M2）架构，使用同样的不 quadratic  primitives来处理序列长度和模型维度：Monarch 矩阵，一种简单的可表示性structured 矩阵，可以在 GPU 上实现高硬件效率，并且可以呈现高性能。</li>
<li>results: 作为证明，该论文在三个领域中explored M2 的性能：非 causal BERT 样式语言模型、ViT 样式图像分类和 causal GPT 样式语言模型。在非 causal BERT 样式模型中，M2 与 BERT-base 和 BERT-large 相比，在下游 GLUE 质量上具有相同的性能，并且可以达到更高的通过put 性能（最高达 9.1 倍）。在 ImageNet 上，M2 超过 ViT-b 的准确率，仅使用半个参数。在 causal GPT 样式模型中，M2 可以与 Transformer 相比，在 360M 参数的预训练质量上具有相同的性能。<details>
<summary>Abstract</summary>
Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.
</details>
<details>
<summary>摘要</summary>
机器学习模型在序列长度和模型维度上逐渐升级以达到更长的上下文和更高的性能。然而，现有的架构，如Transformers，在这两个轴上 quadratic scaling。我们问：是否存在高性能的架构，可以在序列长度和模型维度上下降幂？我们介绍了一新的架构：宫廷混合器（M2），它使用同样的幂次性 primitive来序列长度和模型维度：宫廷矩阵，一种简单的表达 Structured matrices 的类型，可以在 GPU 上实现高硬件效率，并在序列长度和模型维度上下降幂。作为一个证明，我们探索了 M2 在三个领域的性能：非 causal BERT 风格语言模型、ViT 风格图像分类和 causal GPT 风格语言模型。在非 causal BERT 风格模型中，M2 与 BERT-base 和 BERT-large 相当在下游 GLUE 质量上，并且在序列长度 4K 时间点可以达到 9.1 倍的throughput，而且只需要 27% 的参数。在 ImageNet 上，M2 超过 ViT-b 的准确率，只需要一半的参数。 causal GPT 风格模型引入了一个技术挑战：在 маSKing 中引入的 quadratic bottleneck。为了缓解这个瓶颈，我们开发了一种新的理论视角，基于多Variable 多项式评估和插值，这使得我们可以在 M2 中使用 causal 参数化，而不是 quadratic 参数化。使用这种参数化，M2 与 GPT 风格 Transformers 在 360M 参数的预训练损失上匹配，这是第一次证明可以不使用注意力或 MLP 匹配 Transformer 质量。
</details></li>
</ul>
<hr>
<h2 id="An-Online-Learning-Theory-of-Brokerage"><a href="#An-Online-Learning-Theory-of-Brokerage" class="headerlink" title="An Online Learning Theory of Brokerage"></a>An Online Learning Theory of Brokerage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12107">http://arxiv.org/abs/2310.12107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nataša Bolić, Tommaso Cesari, Roberto Colomboni</li>
<li>For: The paper is written for investigating brokerage between traders from an online learning perspective, with a focus on the case where there are no designated buyer and seller roles.* Methods: The paper uses online learning techniques to achieve a low regret bound in the brokerage problem, specifically providing algorithms achieving regret $M \log T$ and $\sqrt{M T}$ under different assumptions about the agents’ valuations.* Results: The paper shows that the optimal regret rate is $M \log T$ when the agents’ valuations are revealed after each interaction, and $\sqrt{M T}$ when only their willingness to sell or buy at the proposed price is revealed. Additionally, the paper demonstrates that the optimal rate degrades to $\sqrt{T}$ when the bounded density assumption is dropped.<details>
<summary>Abstract</summary>
We investigate brokerage between traders from an online learning perspective. At any round $t$, two traders arrive with their private valuations, and the broker proposes a trading price. Unlike other bilateral trade problems already studied in the online learning literature, we focus on the case where there are no designated buyer and seller roles: each trader will attempt to either buy or sell depending on the current price of the good.   We assume the agents' valuations are drawn i.i.d. from a fixed but unknown distribution. If the distribution admits a density bounded by some constant $M$, then, for any time horizon $T$:   $\bullet$ If the agents' valuations are revealed after each interaction, we provide an algorithm achieving regret $M \log T$ and show this rate is optimal, up to constant factors.   $\bullet$ If only their willingness to sell or buy at the proposed price is revealed after each interaction, we provide an algorithm achieving regret $\sqrt{M T}$ and show this rate is optimal, up to constant factors.   Finally, if we drop the bounded density assumption, we show that the optimal rate degrades to $\sqrt{T}$ in the first case, and the problem becomes unlearnable in the second.
</details>
<details>
<summary>摘要</summary>
我们研究在线学习中的经纪人交易。在任意的回合 $t$ 中，两个经纪人会 arrive  WITH 他们的私人估价，经纪人会提议交易价格。与其他双方贸易问题已经在在线学习文献中研究过的不同，我们专注于情况下没有指定的买方和卖方角色：每个经纪人都会尝试 Either 购买或卖出，根据当前商品价格。  我们假设经纪人的估价是从固定而 unknown 的分布中随机样本。如果该分布具有最大值 $M$，那么，对于任意的时间 horizon $T$：❝ 如果经纪人的估价在每次交互后公布，我们提供了一个算法，其 regret 为 $M \log T$，并证明这个率是最佳的，占常数因子。❞❝ 如果只有经纪人对于提议价格的愿意性被公布在每次交互后，我们提供了一个算法，其 regret 为 $\sqrt{M T}$，并证明这个率是最佳的，占常数因子。❞最后，如果我们取消了均勋度 bound 的假设，我们显示了最佳率下降到 $\sqrt{T}$ 在第一个情况下，并问题变得不可学习在第二个情况下。
</details></li>
</ul>
<hr>
<h2 id="On-the-latent-dimension-of-deep-autoencoders-for-reduced-order-modeling-of-PDEs-parametrized-by-random-fields"><a href="#On-the-latent-dimension-of-deep-autoencoders-for-reduced-order-modeling-of-PDEs-parametrized-by-random-fields" class="headerlink" title="On the latent dimension of deep autoencoders for reduced order modeling of PDEs parametrized by random fields"></a>On the latent dimension of deep autoencoders for reduced order modeling of PDEs parametrized by random fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12095">http://arxiv.org/abs/2310.12095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicola Rares Franco, Daniel Fraulin, Andrea Manzoni, Paolo Zunino</li>
<li>for: 这篇论文的目的是提供对随机场生成的Stochastic Partial Differential Equations (SPDEs)中Deep Learning-based Reduced Order Models (DL-ROMs)的理论分析。</li>
<li>methods: 本文使用了深度学习自动编码器作为ROMs的基础工具，并通过非线性神经网络的能力来减少问题的维度。</li>
<li>results: 本文提供了关于DL-ROMs在随机场中的理论分析，并提供了可导的错误 bound，以帮助域专家在选择深度学习自动编码器的缓存维度时进行优化。 数据示例表明，本文的分析对DL-ROMs的性能产生了显著的影响。<details>
<summary>Abstract</summary>
Deep Learning is having a remarkable impact on the design of Reduced Order Models (ROMs) for Partial Differential Equations (PDEs), where it is exploited as a powerful tool for tackling complex problems for which classical methods might fail. In this respect, deep autoencoders play a fundamental role, as they provide an extremely flexible tool for reducing the dimensionality of a given problem by leveraging on the nonlinear capabilities of neural networks. Indeed, starting from this paradigm, several successful approaches have already been developed, which are here referred to as Deep Learning-based ROMs (DL-ROMs). Nevertheless, when it comes to stochastic problems parameterized by random fields, the current understanding of DL-ROMs is mostly based on empirical evidence: in fact, their theoretical analysis is currently limited to the case of PDEs depending on a finite number of (deterministic) parameters. The purpose of this work is to extend the existing literature by providing some theoretical insights about the use of DL-ROMs in the presence of stochasticity generated by random fields. In particular, we derive explicit error bounds that can guide domain practitioners when choosing the latent dimension of deep autoencoders. We evaluate the practical usefulness of our theory by means of numerical experiments, showing how our analysis can significantly impact the performance of DL-ROMs.
</details>
<details>
<summary>摘要</summary>
深度学习对减少顺序模型（ROMs）的设计产生了深刻的影响，特别是在解决复杂问题上，其中经典方法可能会失败时。在这个情况下，深度自适应神经网络扮演了非常重要的角色，因为它们可以通过神经网络的非线性能力来减少问题的维度。从这个角度出发，已经有许多成功的方法被开发出来，这些方法被称为深度学习基于ROMs（DL-ROMs）。然而，当面临随机场所 parametrized 的问题时，现有的理论分析仅限于具有固定数量的 deterministic 参数的PDEs。本文的目的是扩展现有的文献，提供关于DL-ROMs在随机场所下的理论分析。特别是，我们 derive 了明确的错误 bound，可以帮助域专家在选择深度自适应神经网络的缓存维度时作出决策。我们通过数值实验证明了我们的理论对DL-ROMs的性能产生了显著的影响。
</details></li>
</ul>
<hr>
<h2 id="Contributing-Components-of-Metabolic-Energy-Models-to-Metabolic-Cost-Estimations-in-Gait"><a href="#Contributing-Components-of-Metabolic-Energy-Models-to-Metabolic-Cost-Estimations-in-Gait" class="headerlink" title="Contributing Components of Metabolic Energy Models to Metabolic Cost Estimations in Gait"></a>Contributing Components of Metabolic Energy Models to Metabolic Cost Estimations in Gait</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12083">http://arxiv.org/abs/2310.12083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus Gambietz, Marlies Nitschke, Jörg Miehling, Anne Koelewijn</li>
<li>for: 这个研究旨在深入理解人类行走中的代谢能量消耗模型，以便更好地估计代谢能量消耗。</li>
<li>methods: 我们使用了四种代谢能量消耗模型的参数进行 Monte Carlo 敏感分析，然后分析了这些参数的敏感指数、生理上的Context和生理过程中的代谢率。最终选择了一个 quasi-优化的模型。在第二步，我们 investigate了输入参数和变量的重要性，通过使用不同的输入特征来训练神经网络。</li>
<li>results: 我们发现，力量相关的参数在敏感分析中最为重要，而神经网络基于的输入特征选择也显示了承诺。然而，我们发现，使用神经网络模型的代谢能量消耗估计并没有达到传统模型的准确性。<details>
<summary>Abstract</summary>
Objective: As metabolic cost is a primary factor influencing humans' gait, we want to deepen our understanding of metabolic energy expenditure models. Therefore, this paper identifies the parameters and input variables, such as muscle or joint states, that contribute to accurate metabolic cost estimations. Methods: We explored the parameters of four metabolic energy expenditure models in a Monte Carlo sensitivity analysis. Then, we analysed the model parameters by their calculated sensitivity indices, physiological context, and the resulting metabolic rates during the gait cycle. The parameter combination with the highest accuracy in the Monte Carlo simulations represented a quasi-optimized model. In the second step, we investigated the importance of input parameters and variables by analysing the accuracy of neural networks trained with different input features. Results: Power-related parameters were most influential in the sensitivity analysis and the neural network-based feature selection. We observed that the quasi-optimized models produced negative metabolic rates, contradicting muscle physiology. Neural network-based models showed promising abilities but have been unable to match the accuracy of traditional metabolic energy expenditure models. Conclusion: We showed that power-related metabolic energy expenditure model parameters and inputs are most influential during gait. Furthermore, our results suggest that neural network-based metabolic energy expenditure models are viable. However, bigger datasets are required to achieve better accuracy. Significance: As there is a need for more accurate metabolic energy expenditure models, we explored which musculoskeletal parameters are essential when developing a model to estimate metabolic energy.
</details>
<details>
<summary>摘要</summary>
方法：我们在四种代谢能耗模型中进行了Monte Carlo敏感分析，然后分析了模型参数的计算敏感度指数、生理上的文脉和代谢过程中的代谢率。在Monte Carlo优化中，我们选择了最佳的参数组合，并在第二步中，通过不同输入特征的分析，了解输入参数和变量的重要性。结果：在敏感分析中，力量相关的参数具有最大的影响力，而神经网络基于的特征选择也显示了扩展的能力。然而，我们发现，在许多情况下，神经网络模型的准确性不如传统的代谢能耗模型。结论：我们发现，在步行过程中，力量相关的代谢能耗模型参数和输入变量具有最大的影响力。此外，我们的结果表明，神经网络基于的代谢能耗模型是可行的，但需要更大的数据来达到更高的准确性。重要性：由于代谢成本的估计是一个需要更加准确的问题，我们在这篇论文中探讨了 Musculoskeletal 参数是如何影响代谢能耗模型的。
</details></li>
</ul>
<hr>
<h2 id="Differential-Equation-Scaling-Limits-of-Shaped-and-Unshaped-Neural-Networks"><a href="#Differential-Equation-Scaling-Limits-of-Shaped-and-Unshaped-Neural-Networks" class="headerlink" title="Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks"></a>Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12079">http://arxiv.org/abs/2310.12079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mufan Bill Li, Mihai Nica</li>
<li>for: 这篇论文是关于无形activation函数的神经网络性能的研究，尤其是对于两种不同的无形网络架构（即Fully Connected ResNet和Multilayer Perceptron）的性能分析。</li>
<li>methods: 本文使用了不同的方法来分析无形网络的性能，包括使用差分方程来描述无形网络的架构，以及使用初值问题来分析无形网络的层次相关性。</li>
<li>results: 本文发现了两种无形网络架构在初始化时的相同架构准确性限制，并且对无形MLP网络的层次相关性进行了第一项级准确性修正。这些结果表明了无形网络和形态activation函数之间的连接，并开 up了研究正则化方法和形态activation函数之间的关系的可能性。<details>
<summary>Abstract</summary>
Recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. However, these results do not a priori tell us anything about "ordinary" unshaped networks, where the activation is unchanged as the network size grows. In this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks.   Firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected ResNet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (MLP) with depth $d \ll$ width $n$ and shaped ReLU activation at rate $d^{-1/2}$.   Secondly, for an unshaped MLP at initialization, we derive the first order asymptotic correction to the layerwise correlation. In particular, if $\rho_\ell$ is the correlation at layer $\ell$, then $q_t = \ell^2 (1 - \rho_\ell)$ with $t = \frac{\ell}{n}$ converges to an SDE with a singularity at $t=0$.   These results together provide a connection between shaped and unshaped network architectures, and opens up the possibility of studying the effect of normalization methods and how it connects with shaping activation functions.
</details>
<details>
<summary>摘要</summary>
近期的分析表明，在神经网络中使用扩展 activation function（即网络大小增长时Activation function也随着增长）会导致分析限制，这些结果并不直接告诉我们关于“常规”无形网络（即Activation function不变化与网络大小增长）的 anything。在这篇文章中，我们发现了两种类型的无形网络的极限性特征，即：首先，我们证明了以下两个架构在初始化时 converges to the same infinite-depth-and-width limit：（i）一个具有 $d^{-1/2}$ 因子的完全连接 ResNet，其中 $d$ 是网络深度。（ii）一个具有 $d \ll n$ 的多层感知器（MLP），其中 $d$ 是网络深度， activation 是 $d^{-1/2}$ 的折叠函数。其次，对于无形 MLP 的初始化，我们 derive the first order asymptotic correction to the layerwise correlation。 Specifically, if $\rho_\ell$ is the correlation at layer $\ell$, then $q_t = \ell^2 (1 - \rho_\ell)$ with $t = \frac{\ell}{n}$ converges to an SDE with a singularity at $t=0$.这些结果共同表明了无形和形 activation function 之间的连接，并开放了研究正规化方法和 activation function 的拟合方面的可能性。
</details></li>
</ul>
<hr>
<h2 id="Transformers-for-scientific-data-a-pedagogical-review-for-astronomers"><a href="#Transformers-for-scientific-data-a-pedagogical-review-for-astronomers" class="headerlink" title="Transformers for scientific data: a pedagogical review for astronomers"></a>Transformers for scientific data: a pedagogical review for astronomers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12069">http://arxiv.org/abs/2310.12069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Tanoglidis, Bhuvnesh Jain, Helen Qu</li>
<li>for: 该论文主要用于引入transformers深度学习架构和相关的生成AI产品，并为科学家介绍transformers的应用。</li>
<li>methods: 论文使用自注意机制和原始transformer架构，并介绍了在天文学中使用transformers的应用。</li>
<li>results: 论文介绍了自注意机制的数学基础和transformers的应用在时间序列和图像数据中的成果。Note: The above information is in Simplified Chinese text.<details>
<summary>Abstract</summary>
The deep learning architecture associated with ChatGPT and related generative AI products is known as transformers. Initially applied to Natural Language Processing, transformers and the self-attention mechanism they exploit have gained widespread interest across the natural sciences. The goal of this pedagogical and informal review is to introduce transformers to scientists. The review includes the mathematics underlying the attention mechanism, a description of the original transformer architecture, and a section on applications to time series and imaging data in astronomy. We include a Frequently Asked Questions section for readers who are curious about generative AI or interested in getting started with transformers for their research problem.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>与ChatGPT和相关的生成AI产品相关的深度学习架构被称为transformers。初始应用于自然语言处理，transformers和它们利用的自注意机制已经在自然科学领域引起了广泛的关注。本文的教学和非正式评论的目的是引入transformers给科学家。文中包括自注意机制的数学基础、原始transformer架构的描述和在天文学中对时间序列和图像数据的应用。我们附加了关于生成AI或想要使用transformers解决研究问题的常见问题 section。
</details></li>
</ul>
<hr>
<h2 id="Learning-Gradient-Fields-for-Scalable-and-Generalizable-Irregular-Packing"><a href="#Learning-Gradient-Fields-for-Scalable-and-Generalizable-Irregular-Packing" class="headerlink" title="Learning Gradient Fields for Scalable and Generalizable Irregular Packing"></a>Learning Gradient Fields for Scalable and Generalizable Irregular Packing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19814">http://arxiv.org/abs/2310.19814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyang Xue, Mingdong Wu, Lin Lu, Haoxuan Wang, Hao Dong, Baoquan Chen</li>
<li>for:  solves the packing problem with irregularly shaped pieces, minimizing waste and avoiding overlap, using machine learning and conditional generative modeling.</li>
<li>methods:  employs the score-based diffusion model to learn gradient fields that encode constraint satisfaction and spatial relationships, and uses a coarse-to-fine refinement mechanism to generate packing solutions.</li>
<li>results:  demonstrates spatial utilization rates comparable to or surpassing those achieved by the teacher algorithm, and exhibits some level of generalization to shape variations.<details>
<summary>Abstract</summary>
The packing problem, also known as cutting or nesting, has diverse applications in logistics, manufacturing, layout design, and atlas generation. It involves arranging irregularly shaped pieces to minimize waste while avoiding overlap. Recent advances in machine learning, particularly reinforcement learning, have shown promise in addressing the packing problem. In this work, we delve deeper into a novel machine learning-based approach that formulates the packing problem as conditional generative modeling. To tackle the challenges of irregular packing, including object validity constraints and collision avoidance, our method employs the score-based diffusion model to learn a series of gradient fields. These gradient fields encode the correlations between constraint satisfaction and the spatial relationships of polygons, learned from teacher examples. During the testing phase, packing solutions are generated using a coarse-to-fine refinement mechanism guided by the learned gradient fields. To enhance packing feasibility and optimality, we introduce two key architectural designs: multi-scale feature extraction and coarse-to-fine relation extraction. We conduct experiments on two typical industrial packing domains, considering translations only. Empirically, our approach demonstrates spatial utilization rates comparable to, or even surpassing, those achieved by the teacher algorithm responsible for training data generation. Additionally, it exhibits some level of generalization to shape variations. We are hopeful that this method could pave the way for new possibilities in solving the packing problem.
</details>
<details>
<summary>摘要</summary>
<<SYS>> packing 问题，也称为割辑或嵌入，在物流、制造、布局设计和地图生成中有广泛的应用。它涉及到将不规则形状的物品安排，以最小化剩下物和避免重叠。 recent advances in machine learning，特别是强化学习，对 packing 问题提出了新的思路。在这项工作中，我们将更深入地探讨一种基于机器学习的新方法，将 packing 问题转化为 conditional generative modeling。为了解决不规则嵌入中的挑战，包括物体有效性约束和碰撞避免，我们的方法使用分数据模型来学习一系列的梯度场。这些梯度场表达了对约束满足和物体间的空间关系的学习。在测试阶段，我们使用一种粗细层次匀化机制，以指导学习的梯度场来生成嵌入解。为了提高嵌入可行性和优化，我们引入了两种关键的建筑设计：多尺度特征提取和粗细层次关系提取。我们对两种典型的工业嵌入领域进行实验，只考虑翻译。实验结果表明，我们的方法可以与教师算法负责数据生成的空间利用率相当，甚至超过。此外，它还有一定的泛化能力。我们希望这种方法可以为嵌入问题开拓新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Reward-Ambiguity-Through-Optimal-Transport-Theory-in-Inverse-Reinforcement-Learning"><a href="#Understanding-Reward-Ambiguity-Through-Optimal-Transport-Theory-in-Inverse-Reinforcement-Learning" class="headerlink" title="Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning"></a>Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12055">http://arxiv.org/abs/2310.12055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Baheri</li>
<li>for: 这 paper 的中心目标是寻找在观察到的专家行为中隐藏的奖励函数，以便不仅解释数据，还能够泛化到未经见过的情况。</li>
<li>methods: 这 paper 使用 optimal transport (OT) 理论，提供了一种新的视角来解决高维问题和奖励不确定性的问题。</li>
<li>results: 这 paper 的研究发现，通过 Wasserstein 距离来衡量奖励不确定性，并提供了一种中心表示或中心函数的确定方法，这些发现可以为高维 setting 中的 robust IRL 方法提供一种结构化的途径。<details>
<summary>Abstract</summary>
In inverse reinforcement learning (IRL), the central objective is to infer underlying reward functions from observed expert behaviors in a way that not only explains the given data but also generalizes to unseen scenarios. This ensures robustness against reward ambiguity where multiple reward functions can equally explain the same expert behaviors. While significant efforts have been made in addressing this issue, current methods often face challenges with high-dimensional problems and lack a geometric foundation. This paper harnesses the optimal transport (OT) theory to provide a fresh perspective on these challenges. By utilizing the Wasserstein distance from OT, we establish a geometric framework that allows for quantifying reward ambiguity and identifying a central representation or centroid of reward functions. These insights pave the way for robust IRL methodologies anchored in geometric interpretations, offering a structured approach to tackle reward ambiguity in high-dimensional settings.
</details>
<details>
<summary>摘要</summary>
倒 inverse reinforcement learning（IRL）的中心目标是从专家行为中推理出底层奖励函数，以解释数据并在未看到的情况下推广。这 Ensures  robustness  against 奖励ambiguity， where multiple 奖励函数可以一样 explain 专家行为。 although  significant efforts have been made to address this issue, current methods often face challenges with high-dimensional problems and lack a geometric foundation.this paper  harnesses the optimal transport（OT）theory to provide a fresh perspective on these challenges. By utilizing the Wasserstein distance from OT, we establish a geometric framework that allows for quantifying 奖励ambiguity and identifying a central representation or centroid of reward functions. These insights pave the way for robust IRL methodologies anchored in geometric interpretations, offering a structured approach to tackle 奖励ambiguity in high-dimensional settings.
</details></li>
</ul>
<hr>
<h2 id="Applications-of-ML-Based-Surrogates-in-Bayesian-Approaches-to-Inverse-Problems"><a href="#Applications-of-ML-Based-Surrogates-in-Bayesian-Approaches-to-Inverse-Problems" class="headerlink" title="Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems"></a>Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12046">http://arxiv.org/abs/2310.12046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pelin Ersin, Emma Hayes, Peter Matthews, Paramjyoti Mohapatra, Elisa Negrini, Karl Schulz</li>
<li>for: 寻找波源位置在方正区域的逆问题，给出噪音解的解决方案。</li>
<li>methods: 使用神经网络作为代理模型，提高计算效率，使得Markov Chain Monte Carlo方法可以用于评估 posterior 分布中的源位置。</li>
<li>results: 通过寻找波源位置的方法，可以准确地从噪音数据中提取源位置信息。<details>
<summary>Abstract</summary>
Neural networks have become a powerful tool as surrogate models to provide numerical solutions for scientific problems with increased computational efficiency. This efficiency can be advantageous for numerically challenging problems where time to solution is important or when evaluation of many similar analysis scenarios is required. One particular area of scientific interest is the setting of inverse problems, where one knows the forward dynamics of a system are described by a partial differential equation and the task is to infer properties of the system given (potentially noisy) observations of these dynamics. We consider the inverse problem of inferring the location of a wave source on a square domain, given a noisy solution to the 2-D acoustic wave equation. Under the assumption of Gaussian noise, a likelihood function for source location can be formulated, which requires one forward simulation of the system per evaluation. Using a standard neural network as a surrogate model makes it computationally feasible to evaluate this likelihood several times, and so Markov Chain Monte Carlo methods can be used to evaluate the posterior distribution of the source location. We demonstrate that this method can accurately infer source-locations from noisy data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:神经网络已成为数学问题的强大工具，提供了计算效率的增强。这种效率可以在计算复杂的问题中帮助提高解决时间，或者在评估多个相似的分析场景时提高计算效率。一个科学领域的特别兴趣是反问题，即知道系统的前向动力学方程，并且要从（潜在噪声）观测中推断系统的性质。我们考虑了二维声波方程的反问题，即在平方Domain中推断声源的位置，给出噪声解的情况。在假设 Gaussian 噪声时，可以形式化一个likelihood函数，该函数需要一次前向模拟 per 评估。使用标准神经网络作为模拟模型，可以使计算这个likelihood多次成为可能，然后使用Markov Chain Monte Carlo 方法评估 posterior 分布。我们示示了这种方法可以准确地从噪声数据中推断声源位置。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Drug-Property-Prediction-with-Density-Estimation-under-Covariate-Shift"><a href="#Conformal-Drug-Property-Prediction-with-Density-Estimation-under-Covariate-Shift" class="headerlink" title="Conformal Drug Property Prediction with Density Estimation under Covariate Shift"></a>Conformal Drug Property Prediction with Density Estimation under Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12033">http://arxiv.org/abs/2310.12033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/siddharthal/CoDrug">https://github.com/siddharthal/CoDrug</a></li>
<li>paper_authors: Siddhartha Laghuvarapu, Zhen Lin, Jimeng Sun</li>
<li>for:  This paper aims to address the challenge of obtaining reliable uncertainty estimates in drug discovery tasks using Conformal Prediction (CP) and to provide valid prediction sets for molecular properties with a coverage guarantee.</li>
<li>methods:  The proposed method, CoDrug, employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying for distribution shift.</li>
<li>results:  In extensive experiments involving realistic distribution drifts in various small-molecule drug discovery tasks, CoDrug was shown to provide valid prediction sets and to reduce the coverage gap by over 35% when compared to conformal prediction sets not adjusted for covariate shift.<details>
<summary>Abstract</summary>
In drug discovery, it is vital to confirm the predictions of pharmaceutical properties from computational models using costly wet-lab experiments. Hence, obtaining reliable uncertainty estimates is crucial for prioritizing drug molecules for subsequent experimental validation. Conformal Prediction (CP) is a promising tool for creating such prediction sets for molecular properties with a coverage guarantee. However, the exchangeability assumption of CP is often challenged with covariate shift in drug discovery tasks: Most datasets contain limited labeled data, which may not be representative of the vast chemical space from which molecules are drawn. To address this limitation, we propose a method called CoDrug that employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying for distribution shift. In extensive experiments involving realistic distribution drifts in various small-molecule drug discovery tasks, we demonstrate the ability of CoDrug to provide valid prediction sets and its utility in addressing the distribution shift arising from de novo drug design models. On average, using CoDrug can reduce the coverage gap by over 35% when compared to conformal prediction sets not adjusted for covariate shift.
</details>
<details>
<summary>摘要</summary>
在药物发现中，确认计算模型预测的药品性能需要通过costly的湿lab实验进行验证。因此，获得可靠的不确定性估计是关键的，以便在后续实验验证中PRIORITIZE drug molecules。 Conformal Prediction（CP）是一种可靠的工具，可以创建包含预测性能的prediction sets。然而，CP中的交换性假设在药物发现任务中经常遇到冲击：大多数数据集只包含有限的标签数据，这些数据可能不能代表整个化学空间中的分子。为解决这个限制，我们提出了一种方法called CoDrug，它使用能量基本模型利用训练数据和无标签数据，以及Kernel Density Estimation（KDE）来评估分子集的浓度。然后，使用这些估计的浓度来权重分子样本，以建立预测集和纠正分布shift。在具有实际分布滑动的小分子药物发现任务中，我们通过实验证明CoDrug可以提供有效的预测集，并且在de novo drug design模型中 Addressing the distribution shift。在average上，使用CoDrug可以将覆盖缺口减少超过35%，比不 Rectifying for distribution shift。
</details></li>
</ul>
<hr>
<h2 id="Exact-and-efficient-solutions-of-the-LMC-Multitask-Gaussian-Process-model"><a href="#Exact-and-efficient-solutions-of-the-LMC-Multitask-Gaussian-Process-model" class="headerlink" title="Exact and efficient solutions of the LMC Multitask Gaussian Process model"></a>Exact and efficient solutions of the LMC Multitask Gaussian Process model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12032">http://arxiv.org/abs/2310.12032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qwerty6191/projected-lmc">https://github.com/qwerty6191/projected-lmc</a></li>
<li>paper_authors: Olivier Truffinet, Karim Ammar, Jean-Philippe Argaud, Bertrand Bouriquet</li>
<li>for: 这个论文是关于多任务 Gaussian process  regression 或分类的一种非常通用的模型，它的表达能力和概念简单性很吸引人。但是，直接实现方式的复杂性是 cubic 在数据点和任务数量的平方，这意味着大多数应用中需要使用简化方法。然而，最近的研究表明，在某些条件下，模型的隐藏过程可以分离，从而实现 linear 复杂性。</li>
<li>methods: 我们在这篇论文中扩展了这些结果，并证明了在最通用的假设下，只需要一个轻度的噪声模型假设，就可以实现高效的精确计算。我们还提出了一种完整的参数化方法，并给出了质量函数，以便高效地优化。</li>
<li>results: 我们在synthetic数据上进行了参数研究，并证明了我们的方法的出色表现，相比之下unrestricted exact LMC和其他简化方法。总之， проекed LMC 模型是一种可靠和简单的代用方法，它可以大大简化一些计算，如离散一个数据点的cross-validation和幻想。<details>
<summary>Abstract</summary>
The Linear Model of Co-regionalization (LMC) is a very general model of multitask gaussian process for regression or classification. While its expressivity and conceptual simplicity are appealing, naive implementations have cubic complexity in the number of datapoints and number of tasks, making approximations mandatory for most applications. However, recent work has shown that under some conditions the latent processes of the model can be decoupled, leading to a complexity that is only linear in the number of said processes. We here extend these results, showing from the most general assumptions that the only condition necessary to an efficient exact computation of the LMC is a mild hypothesis on the noise model. We introduce a full parametrization of the resulting \emph{projected LMC} model, and an expression of the marginal likelihood enabling efficient optimization. We perform a parametric study on synthetic data to show the excellent performance of our approach, compared to an unrestricted exact LMC and approximations of the latter. Overall, the projected LMC appears as a credible and simpler alternative to state-of-the art models, which greatly facilitates some computations such as leave-one-out cross-validation and fantasization.
</details>
<details>
<summary>摘要</summary>
linear 模型的协同地域化 (LMC) 是一种非常通用的多任务 Gaussian 过程 regression 或 classification 模型。 虽其表达能力和概念简洁吸引人，但直接实现的方法具有 кубиック complexity 在数据点和任务数量上，使得大多数应用中需要使用 Approximations。然而，最近的研究表明，在某些条件下，latent 过程的模型可以减少，导致只有 linear 复杂度在数据点和任务数量上。我们在这里扩展这些结果，表明只需要对模型噪声模型进行一定的假设，就可以实现高效的精确计算。我们介绍了该模型的完整均衡 parametrization，并提供了计算 marginal 概率的表达，使得可以高效地优化。我们在synthetic数据上进行了参数研究，并证明了我们的方法在比较于未限制的精确 LMC 和其approximations 上表现出色。总之，Projected LMC 模型看起来是一种可靠和简单的代码，它可以大大简化一些计算，如离开一个 cross-validation 和幻想。
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Discrete-Choice-Experiments-with-Machine-Learning-Guided-Adaptive-Design"><a href="#Nonparametric-Discrete-Choice-Experiments-with-Machine-Learning-Guided-Adaptive-Design" class="headerlink" title="Nonparametric Discrete Choice Experiments with Machine Learning Guided Adaptive Design"></a>Nonparametric Discrete Choice Experiments with Machine Learning Guided Adaptive Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12026">http://arxiv.org/abs/2310.12026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhang Yin, Ruijiang Gao, Weiran Lin, Steven M. Shugan</li>
<li>For: 这个论文旨在设计用于满足消费者偏好的产品，以提高企业的成功。* Methods: 论文提出了一种名为 Gradient-based Survey (GBS) 的不 Parametric 的选择实验方法，用于多Attribute 产品设计。GBS 通过问题序列和响应者之前的选择来逐步定义产品特性。* Results: 对于在 simulations 中进行比较的 parametric 和非 Parametric 方法，GBS 具有更高的准确率和样本效率。<details>
<summary>Abstract</summary>
Designing products to meet consumers' preferences is essential for a business's success. We propose the Gradient-based Survey (GBS), a discrete choice experiment for multiattribute product design. The experiment elicits consumer preferences through a sequence of paired comparisons for partial profiles. GBS adaptively constructs paired comparison questions based on the respondents' previous choices. Unlike the traditional random utility maximization paradigm, GBS is robust to model misspecification by not requiring a parametric utility model. Cross-pollinating the machine learning and experiment design, GBS is scalable to products with hundreds of attributes and can design personalized products for heterogeneous consumers. We demonstrate the advantage of GBS in accuracy and sample efficiency compared to the existing parametric and nonparametric methods in simulations.
</details>
<details>
<summary>摘要</summary>
为商业成功，设计产品根据消费者的偏好非常重要。我们提议 Gradient-based Survey（GBS），一种多Attribute产品设计的灵活选择实验。这种实验通过一系列对半个配置进行对比，抽取消费者的偏好。与传统的随机Utility最大化理论不同，GBS不需要 Parametric Utility模型，因此更具鲁棒性。通过融合机器学习和实验设计，GBS可扩展到产品上百个特征，设计个性化产品 для多样化的消费者。我们通过模拟表明，GBS在准确性和样本效率方面比现有的参数化和非参数化方法有优势。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Methods-for-Vecchia-Laplace-Approximations-for-Latent-Gaussian-Process-Models"><a href="#Iterative-Methods-for-Vecchia-Laplace-Approximations-for-Latent-Gaussian-Process-Models" class="headerlink" title="Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models"></a>Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12000">http://arxiv.org/abs/2310.12000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fabsig/GPBoost">https://github.com/fabsig/GPBoost</a></li>
<li>paper_authors: Pascal Kündig, Fabio Sigrist</li>
<li>for: 这篇论文旨在探讨高维函数模型（Gaussian Process，GP）的精确估计方法，以及组合Vecchia-Laplace近似法和迭代法的优化。</li>
<li>methods: 本论文使用Vecchia-Laplace近似法和迭代法来实现高维函数模型的精确估计，并提出了一些iterative方法来提高 computations的速度。</li>
<li>results: 论文的实验结果显示，使用Vecchia-Laplace近似法和迭代法可以大幅提高估计的速度，并且在一个大型卫星数据集上比起现有方法实现三倍的预测精度。<details>
<summary>Abstract</summary>
Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present several iterative methods for inference with Vecchia-Laplace approximations which make computations considerably faster compared to Cholesky-based calculations. We analyze our proposed methods theoretically and in experiments with simulated and real-world data. In particular, we obtain a speed-up of an order of magnitude compared to Cholesky-based inference and a threefold increase in prediction accuracy in terms of the continuous ranked probability score compared to a state-of-the-art method on a large satellite data set. All methods are implemented in a free C++ software library with high-level Python and R packages.
</details>
<details>
<summary>摘要</summary>
潜在 Gaussian 过程（GP）模型是一种灵活的可信度非参数函数模型。Vecchia  aproximations 是一种精准的GP模型 Approximations 可以在大量数据时提高计算效率，而 Laplace  Approximations 是一种快速的方法，它具有 asymptotic convergence guarantees 来近似 marginal likelihoods 和 posterior predictive distributions 的非 Gaussian 类型。然而，Vecchia-Laplace  approximations 的计算复杂度随着样本大小增加，使用 direct solver methods such as Cholesky decomposition 时会变得不可持久。因此，在大数据集时，Vecchia-Laplace  approximations 的计算变得繁琐。在这篇文章中，我们提出了一些迭代法来实现Vecchia-Laplace approximations 的推理，使计算速度比 Cholesky-based 计算更快。我们还进行了理论分析和实验室测试，并在 simulated 和实际数据集上 obtaint 一个级别的速度提升和三倍的预测精度。所有方法都是在一个免费 C++ 软件库中实现的，并提供了高级 Python 和 R 包。
</details></li>
</ul>
<hr>
<h2 id="Removing-Spurious-Concepts-from-Neural-Network-Representations-via-Joint-Subspace-Estimation"><a href="#Removing-Spurious-Concepts-from-Neural-Network-Representations-via-Joint-Subspace-Estimation" class="headerlink" title="Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation"></a>Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11991">http://arxiv.org/abs/2310.11991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Floris Holstege, Bram Wouters, Noud van Giersbergen, Cees Diks</li>
<li>for: 本研究旨在提高神经网络模型对异常数据的泛化性能，通过禁用干扰因素。</li>
<li>methods: 本研究提出了一种迭代算法，通过同时确定两个低维度正交子空间来分离干扰因素和主任务因素。</li>
<li>results: 对 Wasserstein 和 CelebA 图像Dataset以及 MultiNLI 自然语言处理Dataset进行评估，发现该算法可以超过现有的概念除除法。<details>
<summary>Abstract</summary>
Out-of-distribution generalization in neural networks is often hampered by spurious correlations. A common strategy is to mitigate this by removing spurious concepts from the neural network representation of the data. Existing concept-removal methods tend to be overzealous by inadvertently eliminating features associated with the main task of the model, thereby harming model performance. We propose an iterative algorithm that separates spurious from main-task concepts by jointly identifying two low-dimensional orthogonal subspaces in the neural network representation. We evaluate the algorithm on benchmark datasets for computer vision (Waterbirds, CelebA) and natural language processing (MultiNLI), and show that it outperforms existing concept removal methods
</details>
<details>
<summary>摘要</summary>
neural networks 中的 out-of-distribution 泛化受到假 correlate 的干扰。一般的方法是通过 removing spurious concepts 来 mitigate 这种情况。现有的 concept-removal 方法往往过于积极，不小心 eliminating 主要任务相关的特征，从而害到模型性能。我们提出了一种迭代算法，jointly identifying two low-dimensional orthogonal subspaces 在 neural network representation 中，以分离假 correlations 和主要任务相关的特征。我们在 Waterbirds、CelebA 和 MultiNLI 等 benchmark datasets 上评估了该算法，并显示它在 existing concept removal 方法 上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Image-Clustering-with-External-Guidance"><a href="#Image-Clustering-with-External-Guidance" class="headerlink" title="Image Clustering with External Guidance"></a>Image Clustering with External Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11989">http://arxiv.org/abs/2310.11989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunfan Li, Peng Hu, Dezhong Peng, Jiancheng Lv, Jianping Fan, Xi Peng</li>
<li>for: 提高图像归一化的性能，利用外部知识作为指导信号</li>
<li>methods: 利用WordNet词语来增强特征分化，并在图像和文本模式之间进行相互馈散学习</li>
<li>results: 在五个常用的图像归一化 benchmark 上达到了状态机的性能，包括全部 ImageNet-1K 数据集<details>
<summary>Abstract</summary>
The core of clustering is incorporating prior knowledge to construct supervision signals. From classic k-means based on data compactness to recent contrastive clustering guided by self-supervision, the evolution of clustering methods intrinsically corresponds to the progression of supervision signals. At present, substantial efforts have been devoted to mining internal supervision signals from data. Nevertheless, the abundant external knowledge such as semantic descriptions, which naturally conduces to clustering, is regrettably overlooked. In this work, we propose leveraging external knowledge as a new supervision signal to guide clustering, even though it seems irrelevant to the given data. To implement and validate our idea, we design an externally guided clustering method (Text-Aided Clustering, TAC), which leverages the textual semantics of WordNet to facilitate image clustering. Specifically, TAC first selects and retrieves WordNet nouns that best distinguish images to enhance the feature discriminability. Then, to improve image clustering performance, TAC collaborates text and image modalities by mutually distilling cross-modal neighborhood information. Experiments demonstrate that TAC achieves state-of-the-art performance on five widely used and three more challenging image clustering benchmarks, including the full ImageNet-1K dataset.
</details>
<details>
<summary>摘要</summary>
核心是在含有先验知识的情况下构建监督信号。从经典k-means基于数据压缩到最近的对照集成监督，集群方法的演化都与监督信号的进步相对应。到目前为止，大量的内部监督信号从数据中被挖掘出来。然而，外部知识，如semantic description，尚未得到了适当的利用。在这种情况下，我们提议利用外部知识作为新的监督信号，即使它与给定数据看起来不相关。为了实现和验证我们的想法，我们设计了一种受外部知识引导的集群方法（Text-Aided Clustering，TAC）。TAC首先选择和检索WordNet词汇，以增强特征描述性。然后，为了提高图像集群性能，TAC与文本和图像模式之间进行协同整合，通过相互洗礼距离信息。实验表明，TAC在5个广泛使用的和3个更加挑战的图像集群 benchmark上达到了状态机器人的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Finite-Horizon-Approach-to-Active-Level-Set-Estimation"><a href="#A-Finite-Horizon-Approach-to-Active-Level-Set-Estimation" class="headerlink" title="A Finite-Horizon Approach to Active Level Set Estimation"></a>A Finite-Horizon Approach to Active Level Set Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11985">http://arxiv.org/abs/2310.11985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phillip Kearns, Bruno Jedynak, John Lipor</li>
<li>for: 本文目的是提出一种活动学习方法来实现等值集 estimation（LSE），以最小化最终估计误差和旅行距离。</li>
<li>methods: 本文使用一种finite-horizon搜索过程来实现LSE，并通过调整一个参数来让方法兼顾估计准确性和旅行距离。</li>
<li>results: 实验表明，当cost of travel增加时，我们的方法可以更好地使用距离非偏视来提高估计精度，并在真实的空气质量数据上实现约一半的估计误差。<details>
<summary>Abstract</summary>
We consider the problem of active learning in the context of spatial sampling for level set estimation (LSE), where the goal is to localize all regions where a function of interest lies above/below a given threshold as quickly as possible. We present a finite-horizon search procedure to perform LSE in one dimension while optimally balancing both the final estimation error and the distance traveled for a fixed number of samples. A tuning parameter is used to trade off between the estimation accuracy and distance traveled. We show that the resulting optimization problem can be solved in closed form and that the resulting policy generalizes existing approaches to this problem. We then show how this approach can be used to perform level set estimation in higher dimensions under the popular Gaussian process model. Empirical results on synthetic data indicate that as the cost of travel increases, our method's ability to treat distance nonmyopically allows it to significantly improve on the state of the art. On real air quality data, our approach achieves roughly one fifth the estimation error at less than half the cost of competing algorithms.
</details>
<details>
<summary>摘要</summary>
我们在各种空间采样中考虑了活动学习，其目标是尽可能快地找到一个函数关注的区域是否超过了一定的阈值。我们提出了一种有限距离搜索过程，用于在一维中进行最优化的水平集估计，同时尽量减少最终估计误差和旅行距离。我们使用一个调整参数，以让最终估计误差和旅行距离之间进行负面交易。我们表明，这个优化问题可以在关闭式形式下解决，并且得到的策略可以折衔现有方法。然后，我们展示了如何使用这种方法来进行高维空间下的水平集估计，使用泊松过程模型。我们的实验结果表明，当成本增加时，我们的方法可以不偏袋见茫地减少估计误差。在实际空气质量数据上，我们的方法可以实现约一剑五分之一的估计误差，而且花费比竞争算法少得多。
</details></li>
</ul>
<hr>
<h2 id="Can-bin-wise-scaling-improve-consistency-and-adaptivity-of-prediction-uncertainty-for-machine-learning-regression"><a href="#Can-bin-wise-scaling-improve-consistency-and-adaptivity-of-prediction-uncertainty-for-machine-learning-regression" class="headerlink" title="Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?"></a>Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11978">http://arxiv.org/abs/2310.11978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ppernot/2023_bvs">https://github.com/ppernot/2023_bvs</a></li>
<li>paper_authors: Pascal Pernot</li>
<li>for: 这篇论文是为了提出一种基于不同变量的准备误差抑制方法，以提高机器学习回归问题的预测不确定性calibration的效果。</li>
<li>methods: 这篇论文使用了 uncertainty-based binning 方法，通过基于不同变量的分配来改进calibration的条件，即consistency。</li>
<li>results: 作者在一个 benchmark 数据集上测试了 BVS 和其变体，与 isotonic regression 进行比较，发现 BVS 和其变体可以更好地适应不同的输入特征，提高calibration的效果。<details>
<summary>Abstract</summary>
Binwise Variance Scaling (BVS) has recently been proposed as a post hoc recalibration method for prediction uncertainties of machine learning regression problems that is able of more efficient corrections than uniform variance (or temperature) scaling. The original version of BVS uses uncertainty-based binning, which is aimed to improve calibration conditionally on uncertainty, i.e. consistency. I explore here several adaptations of BVS, in particular with alternative loss functions and a binning scheme based on an input-feature (X) in order to improve adaptivity, i.e. calibration conditional on X. The performances of BVS and its proposed variants are tested on a benchmark dataset for the prediction of atomization energies and compared to the results of isotonic regression.
</details>
<details>
<summary>摘要</summary>
Binwise Variance Scaling (BVS) 是一种最近提出的机器学习回归问题预测不确定性的后处修正方法，能够更有效地 corrections than uniform variance (或温度) scaling。原版本的 BVS 使用不确定性基于的分类，以提高预测条件上的准确性，即一致性。我在这里 explore 了 BVS 的一些变体，包括使用不同的损失函数和基于输入特征（X）的分类方案，以提高适应性，即预测条件下的准确性。我们对一个 benchmark 数据集进行了预测 atomization energies 的测试，并与ISOREG 的结果进行了比较。Here's the translation in Traditional Chinese: Binwise Variance Scaling (BVS) 是一种最近提出的机器学习回归问题的预测不确定性的后置修正方法，能够更有效地 corrections than uniform variance (或温度) scaling。原版本的 BVS 使用不确定性基于的分类，以提高预测条件上的准确性，即一致性。我在这里 explore 了 BVS 的一些变体，包括使用不同的损失函数和基于输入特征（X）的分类方案，以提高适应性，即预测条件下的准确性。我们对一个 benchmark 数据集进行了预测 atomization energies 的测试，并与ISOREG 的结果进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Take-the-aTrain-Introducing-an-Interface-for-the-Accessible-Transcription-of-Interviews"><a href="#Take-the-aTrain-Introducing-an-Interface-for-the-Accessible-Transcription-of-Interviews" class="headerlink" title="Take the aTrain. Introducing an Interface for the Accessible Transcription of Interviews"></a>Take the aTrain. Introducing an Interface for the Accessible Transcription of Interviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11967">http://arxiv.org/abs/2310.11967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bandas-center/atrain">https://github.com/bandas-center/atrain</a></li>
<li>paper_authors: Armin Haberl, Jürgen Fleiß, Dominik Kowald, Stefan Thalmann</li>
<li>for: 这个论文是为了帮助研究人员进行多种语言的语音数据转译，不需要编程技能，可以在大多数计算机上运行，不需要互联网连线，且不会上传数据到服务器。</li>
<li>methods: 论文使用OpenAI的Whisper模型和声音识别技术，与MAXQDA和ATLAS.ti等流行的质量数据分析软件集成，提供了易于使用的图形用户界面，可以通过Microsoft Store上的Windows应用程序安装。</li>
<li>results: 根据论文的描述，在现有的移动CPU上，转译时间约为音频档案的2-3倍，如果有入门级的图形卡，则转译速度增加到音频档案的20%。<details>
<summary>Abstract</summary>
aTrain is an open-source and offline tool for transcribing audio data in multiple languages with CPU and NVIDIA GPU support. It is specifically designed for researchers using qualitative data generated from various forms of speech interactions with research participants. aTrain requires no programming skills, runs on most computers, does not require an internet connection, and was verified not to upload data to any server. aTrain combines OpenAI's Whisper model with speaker recognition to provide output that integrates with the popular qualitative data analysis software tools MAXQDA and ATLAS.ti. It has an easy-to-use graphical interface and is provided as a Windows-App through the Microsoft Store allowing for simple installation by researchers. The source code is freely available on GitHub. Having developed aTrain with a focus on speed on local computers, we show that the transcription time on current mobile CPUs is around 2 to 3 times the duration of the audio file using the highest-accuracy transcription models. If an entry-level graphics card is available, the transcription speed increases to 20% of the audio duration.
</details>
<details>
<summary>摘要</summary>
aTrain 是一个开源、离线工具，用于转换多种语言的语音数据。它是特意针对对谈话参与者的质数数据进行研究而设计，并且不需要程式码技能，可以在大多数电脑上运行，不需要网页连线，并且确保没有上传数据到服务器。aTrain 结合 OpenAI 的 Whisper 模型和话者识别系统，以提供与 MAXQDA 和 ATLAS.ti 等受欢迎的质数数据分析软件集成。它具有易用的 графі式界面，通过 Microsoft Store 提供为 Windows 应用程序，让研究人员可以简单地安装。源代码则是免费公开在 GitHub 上。我们透过专注于本地电脑的速度，显示在现有的移动 CPU 上，转换时间约为音频档案的2-3倍，使用最高精度转换模型。如果有入门级的显卡可用，则转换速度将提高到音频档案的20%。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Payload-Configuration-for-Satellites-using-Machine-Learning"><a href="#Flexible-Payload-Configuration-for-Satellites-using-Machine-Learning" class="headerlink" title="Flexible Payload Configuration for Satellites using Machine Learning"></a>Flexible Payload Configuration for Satellites using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11966">http://arxiv.org/abs/2310.11966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcele O. K. Mendonca, Flor G. Ortiz-Gomez, Jorge Querol, Eva Lagunas, Juan A. Vásquez Peralvo, Victor Monzon Baeza, Symeon Chatzinotas, Bjorn Ottersten</li>
<li>for: 提高卫星通信系统的效率和质量，适应不同的吞吐量和延迟要求。</li>
<li>methods: 使用机器学习（ML）技术进行无线资源管理（RRM），将RRM任务定义为一个回归型ML问题，并将RRM目标和约束集成到损失函数中，以便ML算法尽可能地减小。</li>
<li>results: 通过对ML模型的表现进行评估，并考虑模型的资源分配决策对总体通信系统性能的影响，提出了一种Context-aware ML metric。<details>
<summary>Abstract</summary>
Satellite communications, essential for modern connectivity, extend access to maritime, aeronautical, and remote areas where terrestrial networks are unfeasible. Current GEO systems distribute power and bandwidth uniformly across beams using multi-beam footprints with fractional frequency reuse. However, recent research reveals the limitations of this approach in heterogeneous traffic scenarios, leading to inefficiencies. To address this, this paper presents a machine learning (ML)-based approach to Radio Resource Management (RRM).   We treat the RRM task as a regression ML problem, integrating RRM objectives and constraints into the loss function that the ML algorithm aims at minimizing. Moreover, we introduce a context-aware ML metric that evaluates the ML model's performance but also considers the impact of its resource allocation decisions on the overall performance of the communication system.
</details>
<details>
<summary>摘要</summary>
卫星通信，现代连接的关键，扩展至海上、航空和远郊地区， terrestrial 网络无法实现。现有的 GEO 系统在多个扫描面上均匀分配功率和频率，使用多扫描面 fractional frequency reuse。然而， latest research 显示这种方法在多样化流量场景下存在限制，导致不充分利用。为解决这个问题，这篇论文提出一种基于机器学习（ML）的Radio Resource Management（RRM）方法。我们将 RRM 任务视为一个回归 ML 问题，将 RRM 目标和约束 integrate 到 ML 算法目标函数中。此外，我们还引入了一种 context-aware ML 指标，评估 ML 模型的性能，同时考虑它的资源分配决策对通信系统的总性能的影响。
</details></li>
</ul>
<hr>
<h2 id="Recasting-Continual-Learning-as-Sequence-Modeling"><a href="#Recasting-Continual-Learning-as-Sequence-Modeling" class="headerlink" title="Recasting Continual Learning as Sequence Modeling"></a>Recasting Continual Learning as Sequence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11952">http://arxiv.org/abs/2310.11952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soochan-lee/cl-as-seq">https://github.com/soochan-lee/cl-as-seq</a></li>
<li>paper_authors: Soochan Lee, Jaehyeon Son, Gunhee Kim</li>
<li>for: 本研究旨在将重要的机器学习领域——启发学习和序列模型——加强连接起来。即我们提议将启发学习视为序列模型问题，使高级序列模型可以用于启发学习。在这种形式下，启发学习过程变成了序列模型的前进传播。</li>
<li>methods: 我们采用了元 continual learning（MCL）框架，在多个启发学习集合中训练序列模型。作为具体的例子，我们示cases了使用 transformers 和其高效变体作为 MCL 方法。</li>
<li>results: 我们在七个 bencmarks 上进行了七个benchmark，包括分类和回归任务，结果显示了序列模型可以是通用 MCL 的有ffektive解决方案。<details>
<summary>Abstract</summary>
In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们想要建立两个机器学习研究领域之间的强有力连接：不间断学习和序列模型。即我们提议将不间断学习问题设置为序列模型问题，以便使用高级序列模型进行不间断学习。根据这种设置，不间断学习过程变成了序列模型的前向传播。通过采用meta-不间断学习（MCL）框架，我们可以在多个不间断学习集合上训练序列模型。为了示例，我们展示了使用Transformers和其高效变体作为MCL方法的应用。我们在七个标准准确的benchmark上进行了七种不同的实验，包括分类和回归任务，结果表明序列模型可以是通用MCL的有效解决方案。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Spectral-Variational-AutoEncoder-ISVAE-for-time-series-clustering"><a href="#Interpretable-Spectral-Variational-AutoEncoder-ISVAE-for-time-series-clustering" class="headerlink" title="Interpretable Spectral Variational AutoEncoder (ISVAE) for time series clustering"></a>Interpretable Spectral Variational AutoEncoder (ISVAE) for time series clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11940">http://arxiv.org/abs/2310.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Óscar Jiménez Rama, Fernando Moreno-Pino, David Ramírez, Pablo M. Olmos</li>
<li>for: 这篇论文是为了提出一种新的变量自动编码器（VAE）模型，该模型具有可解释性的瓶颈（Filter Bank，FB），以便学习更加可解释的潜在空间。</li>
<li>methods: 该模型使用了VAE的基本结构，并在其前置了FB。FB强制VAE关注输入信号中最重要的部分，从而学习一个新的编码${f_0}$，该编码具有更高的可解释性和分化性。</li>
<li>results: 实验结果表明，ISVAE模型比传统的VAE模型在分类率上表现更高，并且可以更好地处理复杂的数据配置。此外，${f_0}$的演化征imatters表明了群集之间的相似性。<details>
<summary>Abstract</summary>
The best encoding is the one that is interpretable in nature. In this work, we introduce a novel model that incorporates an interpretable bottleneck-termed the Filter Bank (FB)-at the outset of a Variational Autoencoder (VAE). This arrangement compels the VAE to attend on the most informative segments of the input signal, fostering the learning of a novel encoding ${f_0}$ which boasts enhanced interpretability and clusterability over traditional latent spaces. By deliberately constraining the VAE with this FB, we intentionally constrict its capacity to access broad input domain information, promoting the development of an encoding that is discernible, separable, and of reduced dimensionality. The evolutionary learning trajectory of ${f_0}$ further manifests as a dynamic hierarchical tree, offering profound insights into cluster similarities. Additionally, for handling intricate data configurations, we propose a tailored decoder structure that is symmetrically aligned with FB's architecture. Empirical evaluations highlight the superior efficacy of ISVAE, which compares favorably to state-of-the-art results in clustering metrics across real-world datasets.
</details>
<details>
<summary>摘要</summary>
最佳编码是可解释的编码。在这项工作中，我们提出了一种新的模型，其中包含了一个可解释的瓶颈（Filter Bank，FB），这个瓶颈位于Variational Autoencoder（VAE）的开头。这种设计使得VAE需要关注输入信号中最重要的信息，从而促进了学习一个新的编码${f_0}$，该编码具有更高的可解释性和分布性。通过强制VAE通过FB进行制约，我们故意削弱VAE对输入信号范围广的信息访问权限，从而促进了编码的可读性、分割性和维度减少。${f_0}$的演化学习轨迹更显示出了动态层次树的形式，提供了深刻的群集相似性的启示。此外，为处理复杂的数据配置，我们提议一种适应FB的编码结构。实验证明，ISVAE的效果明显高于州际级的结果，在真实世界数据集上达到了高度的分 clustering  metrics。
</details></li>
</ul>
<hr>
<h2 id="Accelerated-Policy-Gradient-On-the-Nesterov-Momentum-for-Reinforcement-Learning"><a href="#Accelerated-Policy-Gradient-On-the-Nesterov-Momentum-for-Reinforcement-Learning" class="headerlink" title="Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning"></a>Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11897">http://arxiv.org/abs/2310.11897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nycu-rl-bandits-lab/apg">https://github.com/nycu-rl-bandits-lab/apg</a></li>
<li>paper_authors: Yen-Ju Chen, Nai-Chieh Huang, Ping-Chun Hsieh</li>
<li>for: 本研究证明了使用推移 momentum 加速度 gradient 方法可以在 reinforcement learning 中提高 converges 率。</li>
<li>methods: 本文使用 Nesterov 加速度 gradient 方法（NAG），并对其进行了适应以适应 reinforcement learning 中的 softmax 政策参数化。</li>
<li>results: 我们表明了 NAG 在 true gradient 下可以在 $\tilde{O}(1&#x2F;t^2)$ 时间复杂度下连续 converges 到优化的政策。此外，我们还通过数值验证表明了 NAG 可以在实际应用中提高 converge 性。<details>
<summary>Abstract</summary>
Policy gradient methods have recently been shown to enjoy global convergence at a $\Theta(1/t)$ rate in the non-regularized tabular softmax setting. Accordingly, one important research question is whether this convergence rate can be further improved, with only first-order updates. In this paper, we answer the above question from the perspective of momentum by adapting the celebrated Nesterov's accelerated gradient (NAG) method to reinforcement learning (RL), termed \textit{Accelerated Policy Gradient} (APG). To demonstrate the potential of APG in achieving faster global convergence, we formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. To the best of our knowledge, this is the first characterization of the global convergence rate of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the initialization, APG could end up reaching a locally nearly-concave regime, where APG could benefit significantly from the momentum, within finite iterations. By means of numerical validation, we confirm that APG exhibits $\tilde{O}(1/t^2)$ rate as well as show that APG could significantly improve the convergence behavior over the standard policy gradient.
</details>
<details>
<summary>摘要</summary>
We formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. This is the first characterization of the global convergence rate of NAG in the context of RL. Our analysis relies on one interesting finding: regardless of the initialization, APG could end up reaching a locally nearly-concave regime, where APG could benefit significantly from the momentum, within finite iterations.Numerical validation confirms that APG exhibits a $\tilde{O}(1/t^2)$ rate and shows that APG could significantly improve the convergence behavior over the standard policy gradient.
</details></li>
</ul>
<hr>
<h2 id="A-Hyperparameter-Study-for-Quantum-Kernel-Methods"><a href="#A-Hyperparameter-Study-for-Quantum-Kernel-Methods" class="headerlink" title="A Hyperparameter Study for Quantum Kernel Methods"></a>A Hyperparameter Study for Quantum Kernel Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11891">http://arxiv.org/abs/2310.11891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Egginger, Alona Sakhnenko, Jeanette Miriam Lorenz</li>
<li>for: 本研究旨在investigating the effects of hyperparameter choice on the model performance and the generalization gap between classical and quantum kernels, and exploring the use of the geometric difference as a tool for evaluating the potential for quantum advantage.</li>
<li>methods: 本研究使用了 quantum kernel methods and hyperparameter optimization techniques to evaluate the performance of quantum and classical machine learning models on 11 datasets. The geometric difference was used as a closeness measure between the two kernel-based machine learning approaches.</li>
<li>results: 研究发现，hyperparameter optimization是critical for achieving good model performance and reducing the generalization gap between classical and quantum kernels. The geometric difference can be a useful tool for evaluating the potential for quantum advantage, and can help identify commodities that can be exploited when examining a new dataset.<details>
<summary>Abstract</summary>
Quantum kernel methods are a promising method in quantum machine learning thanks to the guarantees connected to them. Their accessibility for analytic considerations also opens up the possibility of prescreening datasets based on their potential for a quantum advantage. To do so, earlier works developed the geometric difference, which can be understood as a closeness measure between two kernel-based machine learning approaches, most importantly between a quantum kernel and classical kernel. This metric links the quantum and classical model complexities. Therefore, it raises the question of whether the geometric difference, based on its relation to model complexity, can be a useful tool in evaluations other than for the potential for quantum advantage. In this work, we investigate the effects of hyperparameter choice on the model performance and the generalization gap between classical and quantum kernels. The importance of hyperparameter optimization is well known also for classical machine learning. Especially for the quantum Hamiltonian evolution feature map, the scaling of the input data has been shown to be crucial. However, there are additional parameters left to be optimized, like the best number of qubits to trace out before computing a projected quantum kernel. We investigate the influence of these hyperparameters and compare the classically reliable method of cross validation with the method of choosing based on the geometric difference. Based on the thorough investigation of the hyperparameters across 11 datasets we identified commodities that can be exploited when examining a new dataset. In addition, our findings contribute to better understanding of the applicability of the geometric difference.
</details>
<details>
<summary>摘要</summary>
量子kernels方法是量子机器学习中的一种有前途的方法，这主要归功于它们的 garantías。它们的可见性使得可以对数据进行预选择，以确定它们是否具有量子优势。以前的工作在开发了 геомétríain difference，这可以理解为两种基于kernel的机器学习方法之间的距离度量，主要是quantum kernel和классическийkernel之间的距离。这个指标连接了量子和классиical模型复杂性。因此，它提出了问题，是否可以通过其与模型复杂性的关系来使用 geometric difference 作为评估工具？在这种工作中，我们investigate了hyperparameter的选择对模型性能和量子和классиical kernel之间的泛化差异的影响。特别是 для量子 Hamiltonian 演化特征图，输入数据的涨落Scaling 已经被证明是关键。然而，还有其他参数需要优化，例如最佳的量子bits数量来计算projected quantum kernel。我们 investigate了这些超参数的影响，并将cross validation 方法与基于 geometric difference 的选择方法进行比较。通过对 11 个数据集进行了全面的超参数调整，我们发现了一些可以利用的商品，并对量子和классиical kernel之间的泛化差异进行了更好的理解。
</details></li>
</ul>
<hr>
<h2 id="Building-a-Graph-based-Deep-Learning-network-model-from-captured-traffic-traces"><a href="#Building-a-Graph-based-Deep-Learning-network-model-from-captured-traffic-traces" class="headerlink" title="Building a Graph-based Deep Learning network model from captured traffic traces"></a>Building a Graph-based Deep Learning network model from captured traffic traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11889">http://arxiv.org/abs/2310.11889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Güemes-Palau, Miquel Ferriol Galmés, Albert Cabellos-Aparicio, Pere Barlet-Ros</li>
<li>for: 本研究旨在提出一种基于图神经网络（GNN）的解决方案，用于更好地捕捉实际网络场景中的复杂性。</li>
<li>methods: 本研究使用了一种新的编码方法，用于从捕捉的包序列中提取信息，以及一种改进的消息传递算法，用于更好地表示物理网络中的依赖关系。</li>
<li>results: 我们的实验结果表明，提议的解决方案能够学习和泛化到未看过的捕捉网络场景。<details>
<summary>Abstract</summary>
Currently the state of the art network models are based or depend on Discrete Event Simulation (DES). While DES is highly accurate, it is also computationally costly and cumbersome to parallelize, making it unpractical to simulate high performance networks. Additionally, simulated scenarios fail to capture all of the complexities present in real network scenarios. While there exists network models based on Machine Learning (ML) techniques to minimize these issues, these models are also trained with simulated data and hence vulnerable to the same pitfalls. Consequently, the Graph Neural Networking Challenge 2023 introduces a dataset of captured traffic traces that can be used to build a ML-based network model without these limitations. In this paper we propose a Graph Neural Network (GNN)-based solution specifically designed to better capture the complexities of real network scenarios. This is done through a novel encoding method to capture information from the sequence of captured packets, and an improved message passing algorithm to better represent the dependencies present in physical networks. We show that the proposed solution it is able to learn and generalize to unseen captured network scenarios.
</details>
<details>
<summary>摘要</summary>
现在的状态艺术网络模型都基于不可countdown事件模拟（DES）。虽然DES具有高度准确的优点，但也有计算成本高和并行化困难，使得模拟高性能网络不实际。此外，模拟场景不能捕捉实际网络场景中的所有复杂性。而现有的网络模型基于机器学习（ML）技术来减少这些问题，但这些模型又是通过模拟数据进行训练，因此也受到相同的局限性。因此，2023年的图解网络挑战（GNN）引入了一个包含流量轨迹的数据集，可以用于构建一个基于机器学习（ML）的网络模型，不受上述局限性的影响。在本文中，我们提出了一种基于图解网络（GNN）的解决方案，通过一种新的编码方法来捕捉从流量序列中获得的信息，以及一种改进的消息传递算法来更好地表示物理网络中的依赖关系。我们示出了我们的解决方案能够学习和掌握未看过的捕捉网络场景。
</details></li>
</ul>
<hr>
<h2 id="Online-Convex-Optimization-with-Switching-Cost-and-Delayed-Gradients"><a href="#Online-Convex-Optimization-with-Switching-Cost-and-Delayed-Gradients" class="headerlink" title="Online Convex Optimization with Switching Cost and Delayed Gradients"></a>Online Convex Optimization with Switching Cost and Delayed Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11880">http://arxiv.org/abs/2310.11880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spandan Senapati, Rahul Vaze</li>
<li>for: 这个论文研究了在有限信息设定下的在线半正定优化问题，特别是使用quadratic和linear switching cost。</li>
<li>methods: 该论文提出了一种名为online multiple gradient descent（OMGD）算法，用于解决这个问题。</li>
<li>results: 论文显示了OMGD算法的竞争比例upper bound为$4(L + 5) + \frac{16(L + 5)}{\mu}$，并且证明了这个Upper bound是order-wise tight。此外，论文还证明了任何在线算法的竞争比例至少为$\max{\Omega(L), \Omega(\frac{L}{\sqrt{\mu})}$。<details>
<summary>Abstract</summary>
We consider the online convex optimization (OCO) problem with quadratic and linear switching cost in the limited information setting, where an online algorithm can choose its action using only gradient information about the previous objective function. For $L$-smooth and $\mu$-strongly convex objective functions, we propose an online multiple gradient descent (OMGD) algorithm and show that its competitive ratio for the OCO problem with quadratic switching cost is at most $4(L + 5) + \frac{16(L + 5)}{\mu}$. The competitive ratio upper bound for OMGD is also shown to be order-wise tight in terms of $L,\mu$. In addition, we show that the competitive ratio of any online algorithm is $\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu})\}$ in the limited information setting when the switching cost is quadratic. We also show that the OMGD algorithm achieves the optimal (order-wise) dynamic regret in the limited information setting. For the linear switching cost, the competitive ratio upper bound of the OMGD algorithm is shown to depend on both the path length and the squared path length of the problem instance, in addition to $L, \mu$, and is shown to be order-wise, the best competitive ratio any online algorithm can achieve. Consequently, we conclude that the optimal competitive ratio for the quadratic and linear switching costs are fundamentally different in the limited information setting.
</details>
<details>
<summary>摘要</summary>
我们考虑在有限信息设定下的线上凸优化（OCO）问题，其中一个线上算法可以根据过去的目标函数GradientInformation选择行动。对于$L$-smooth和$\mu$-强制凸目标函数，我们提出了一个线上多重梯度降低（OMGD）算法，并证明其在具有quadratic switching cost的OCO问题中的竞争比率不大于$4(L + 5) + \frac{16(L + 5)}{\mu}$。此外，我们还证明了OMGD算法的竞争比率Upper bound是order-wise tight in terms of $L,\mu$。另外，我们还证明了在有限信息设定下，任何线上算法的竞争比率都是$\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu})\}$。此外，我们还证明了OMGD算法在有限信息设定下具有最佳（order-wise）动态遗憾。在linear switching cost的情况下，我们证明了OMGD算法的竞争比率Upper bound取决于问题实体的路径长度和平方路径长度，而且随着$L, \mu$的变化而变化。此外，我们还证明了OMGD算法在linear switching cost的情况下具有order-wise最佳的竞争比率。因此，我们结论到了quadratic和linear switching cost在有限信息设定下的竞争比率是基本不同的。
</details></li>
</ul>
<hr>
<h2 id="SQ-Lower-Bounds-for-Learning-Mixtures-of-Linear-Classifiers"><a href="#SQ-Lower-Bounds-for-Learning-Mixtures-of-Linear-Classifiers" class="headerlink" title="SQ Lower Bounds for Learning Mixtures of Linear Classifiers"></a>SQ Lower Bounds for Learning Mixtures of Linear Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11876">http://arxiv.org/abs/2310.11876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Diakonikolas, Daniel M. Kane, Yuxin Sun</li>
<li>For: 学习混合线性分类器下 Gaussian covariates 问题。* Methods: 使用 Statistical Query (SQ) 算法来解决问题，并提供了一个新的圆形设计技术。* Results: 得到了一个 Statistical Query (SQ) 下界，表明现有算法的复杂性为 $n^{\mathrm{poly}(1&#x2F;\Delta) \log(r)} $，其中 $\Delta$ 是 $\mathbf{v}_\ell$ 对应的下界Pairwise $\ell_2$-separation。<details>
<summary>Abstract</summary>
We study the problem of learning mixtures of linear classifiers under Gaussian covariates. Given sample access to a mixture of $r$ distributions on $\mathbb{R}^n$ of the form $(\mathbf{x},y_{\ell})$, $\ell\in [r]$, where $\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n)$ and $y_\ell=\mathrm{sign}(\langle\mathbf{v}_\ell,\mathbf{x}\rangle)$ for an unknown unit vector $\mathbf{v}_\ell$, the goal is to learn the underlying distribution in total variation distance. Our main result is a Statistical Query (SQ) lower bound suggesting that known algorithms for this problem are essentially best possible, even for the special case of uniform mixtures. In particular, we show that the complexity of any SQ algorithm for the problem is $n^{\mathrm{poly}(1/\Delta) \log(r)}$, where $\Delta$ is a lower bound on the pairwise $\ell_2$-separation between the $\mathbf{v}_\ell$'s. The key technical ingredient underlying our result is a new construction of spherical designs that may be of independent interest.
</details>
<details>
<summary>摘要</summary>
我们研究混合线性分类器学习问题，假设我们有一个混合的$r$个分布在 $\mathbb{R}^n$ 上，每个分布的形式是 $({\mathbf{x},y_{\ell})$, $\ell\in [r] $，其中 $\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n) $ 是一个标准均值为零的均值为 $\mathbf{I}_n $ 的高维Normal分布，$y_{\ell} = \text{sign}(\langle \mathbf{v}_{\ell}, \mathbf{x} \rangle)$ 是一个未知的单位向量 $\mathbf{v}_{\ell} $ 的某种标记。我们的目标是通过总变化距离来学习这个下面的分布。我们的主要结果是一个统计查询（SQ）下界，表明现有的算法是可能最佳的，即使特殊情况下是均匀混合。具体来说，我们证明任何 SQ 算法的复杂度为 $n^{\mathrm{poly}(1/\Delta) \log(r)}$, 其中 $\Delta$ 是 $\mathbf{v}_{\ell}$ 之间的对角线 $\ell_2$  separation 的下界。我们的技术核心是一种新的圆柱体设计，可能具有独立的利用价值。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Optimization-for-Non-convex-Problem-with-Inexact-Hessian-Matrix-Gradient-and-Function"><a href="#Stochastic-Optimization-for-Non-convex-Problem-with-Inexact-Hessian-Matrix-Gradient-and-Function" class="headerlink" title="Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function"></a>Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11866">http://arxiv.org/abs/2310.11866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Liu, Xuanqing Liu, Cho-Jui Hsieh, Dacheng Tao</li>
<li>for: 这个论文的目的是提出一种基于不确定计算的信任区间（TR）和适应正则化（ARC）方法，以便在非对称优化问题中提高优化效率。</li>
<li>methods: 这种方法使用了不确定计算来计算函数值、梯度和希格曼矩阵，从而选择下一个搜索方向和调整参数。</li>
<li>results: 该论文证明了这种方法可以同时提供不确定计算的函数值、梯度和希格曼矩阵，并且可以在非对称优化问题中实现$\epsilon$-近似二阶优 оптимальность。此外，这种方法的迭代复杂度与前一个研究中的精确计算相同。<details>
<summary>Abstract</summary>
Trust-region (TR) and adaptive regularization using cubics (ARC) have proven to have some very appealing theoretical properties for non-convex optimization by concurrently computing function value, gradient, and Hessian matrix to obtain the next search direction and the adjusted parameters. Although stochastic approximations help largely reduce the computational cost, it is challenging to theoretically guarantee the convergence rate. In this paper, we explore a family of stochastic TR and ARC methods that can simultaneously provide inexact computations of the Hessian matrix, gradient, and function values. Our algorithms require much fewer propagations overhead per iteration than TR and ARC. We prove that the iteration complexity to achieve $\epsilon$-approximate second-order optimality is of the same order as the exact computations demonstrated in previous studies. Additionally, the mild conditions on inexactness can be met by leveraging a random sampling technology in the finite-sum minimization problem. Numerical experiments with a non-convex problem support these findings and demonstrate that, with the same or a similar number of iterations, our algorithms require less computational overhead per iteration than current second-order methods.
</details>
<details>
<summary>摘要</summary>
信任区域（TR）和适应正则化使用立方体（ARC）在非对称优化中有非常吸引人的理论性质。它们同时计算函数值、梯度和偏导数矩阵，以获取下一步搜索方向和调整参数。 although stochastic approximations can significantly reduce computational cost, it is challenging to theoretically guarantee the convergence rate.在这篇论文中，我们探讨了一家Stochastic TR和ARC方法，可同时提供不准确的函数值、梯度和偏导数矩阵计算。我们的算法需要每次迭代 fewer propagations overhead than TR和ARC。我们证明，以 Achieve $\epsilon$-近似第二阶优化的迭代复杂度与前一个研究中的精确计算相同顺序。此外，我们的方法可以通过利用随机抽样技术在finite-sum minimization问题中实现轻度的不准确性条件。numerical experiments with a non-convex problem support these findings and demonstrate that, with the same or a similar number of iterations, our algorithms require less computational overhead per iteration than current second-order methods.
</details></li>
</ul>
<hr>
<h2 id="Effective-and-Efficient-Federated-Tree-Learning-on-Hybrid-Data"><a href="#Effective-and-Efficient-Federated-Tree-Learning-on-Hybrid-Data" class="headerlink" title="Effective and Efficient Federated Tree Learning on Hybrid Data"></a>Effective and Efficient Federated Tree Learning on Hybrid Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11865">http://arxiv.org/abs/2310.11865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinbin Li, Chulin Xie, Xiaojun Xu, Xiaoyuan Liu, Ce Zhang, Bo Li, Bingsheng He, Dawn Song</li>
<li>for: 该论文旨在 Addressing the challenges of federated learning in hybrid data settings, where data from different parties may differ in both features and samples.</li>
<li>methods: 该论文提出了 HybridTree，一种基于分布式学习的新方法，可以在混合数据设置下进行树学习。通过分析了树中具有一致的拆分规则，该方法可以在不需要频繁的通信协议的情况下训练树。</li>
<li>results: 实验表明，HybridTree 可以与中央集成集成环境相比，达到相同的准确率，而且可以减少计算和通信协议的开销，最高可以达到 8 倍的速度提升。<details>
<summary>Abstract</summary>
Federated learning has emerged as a promising distributed learning paradigm that facilitates collaborative learning among multiple parties without transferring raw data. However, most existing federated learning studies focus on either horizontal or vertical data settings, where the data of different parties are assumed to be from the same feature or sample space. In practice, a common scenario is the hybrid data setting, where data from different parties may differ both in the features and samples. To address this, we propose HybridTree, a novel federated learning approach that enables federated tree learning on hybrid data. We observe the existence of consistent split rules in trees. With the help of these split rules, we theoretically show that the knowledge of parties can be incorporated into the lower layers of a tree. Based on our theoretical analysis, we propose a layer-level solution that does not need frequent communication traffic to train a tree. Our experiments demonstrate that HybridTree can achieve comparable accuracy to the centralized setting with low computational and communication overhead. HybridTree can achieve up to 8 times speedup compared with the other baselines.
</details>
<details>
<summary>摘要</summary>
《联合学习》已经成为一种有前途的分布式学习 paradigma，它使得多个党 collaboration 学习，无需传输原始数据。然而，现有大多数联合学习研究都集中在水平或垂直数据设置中，即不同党的数据假设来自同一个特征或样本空间。在实际应用中，常见的情景是混合数据设置，其中党的数据可能具有不同的特征和样本。为 Addressing 此问题，我们提出 HybridTree，一种新的联合学习方法，可以在混合数据上进行联合树学习。我们发现了共同拆分规则在树中的存在，这些规则帮助我们 theoretically 表明党的知识可以在树的下层级中被包含。基于我们的理论分析，我们提出一种层级解决方案，不需要频繁的通信协议来训练树。我们的实验表明，HybridTree 可以与中央集成设置具有相同的准确率，同时具有较低的计算和通信协议负担。HybridTree 可以与其他基准值进行比较，达到 8 倍的速度提升。
</details></li>
</ul>
<hr>
<h2 id="Accelerate-Presolve-in-Large-Scale-Linear-Programming-via-Reinforcement-Learning"><a href="#Accelerate-Presolve-in-Large-Scale-Linear-Programming-via-Reinforcement-Learning" class="headerlink" title="Accelerate Presolve in Large-Scale Linear Programming via Reinforcement Learning"></a>Accelerate Presolve in Large-Scale Linear Programming via Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11845">http://arxiv.org/abs/2310.11845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Kuang, Xijun Li, Jie Wang, Fangzhou Zhu, Meng Lu, Zhihai Wang, Jia Zeng, Houqiang Li, Yongdong Zhang, Feng Wu</li>
<li>for: 这篇论文的目的是提出一种基于机器学习的LP解决方法，以提高现代LP解决器的效率和可靠性。</li>
<li>methods: 该方法使用了动态行为序列学习（RL）框架，将LP解决器的routine设计任务形式化为一个Markov决策过程，并通过适应行动序列来生成高质量的解决方案。</li>
<li>results: 实验结果表明，RL4Presolve可以有效地提高大规模LP的解决效率，特别是在来自业界的benchmark中。此外，通过提取学习政策中的规则，可以将RL4Presolve简单地部署到华为的供应链中。这些结果表明，将机器学习技术应用于现代LP解决器可以实现可负担的经济和学术潜力。<details>
<summary>Abstract</summary>
Large-scale LP problems from industry usually contain much redundancy that severely hurts the efficiency and reliability of solving LPs, making presolve (i.e., the problem simplification module) one of the most critical components in modern LP solvers. However, how to design high-quality presolve routines -- that is, the program determining (P1) which presolvers to select, (P2) in what order to execute, and (P3) when to stop -- remains a highly challenging task due to the extensive requirements on expert knowledge and the large search space. Due to the sequential decision property of the task and the lack of expert demonstrations, we propose a simple and efficient reinforcement learning (RL) framework -- namely, reinforcement learning for presolve (RL4Presolve) -- to tackle (P1)-(P3) simultaneously. Specifically, we formulate the routine design task as a Markov decision process and propose an RL framework with adaptive action sequences to generate high-quality presolve routines efficiently. Note that adaptive action sequences help learn complex behaviors efficiently and adapt to various benchmarks. Experiments on two solvers (open-source and commercial) and eight benchmarks (real-world and synthetic) demonstrate that RL4Presolve significantly and consistently improves the efficiency of solving large-scale LPs, especially on benchmarks from industry. Furthermore, we optimize the hard-coded presolve routines in LP solvers by extracting rules from learned policies for simple and efficient deployment to Huawei's supply chain. The results show encouraging economic and academic potential for incorporating machine learning to modern solvers.
</details>
<details>
<summary>摘要</summary>
大规模LP问题从行业 обычно含有很多重复性，这会严重地降低解决LP的效率和可靠性，因此宏观问题简化模块（i.e., 问题简化模块）成为现代LP解决器中最 kritical 的一部分。然而，如何设计高质量的宏观问题简化程序 --- 即确定（P1）哪些简化器选择，（P2）在哪个顺序执行，以及（P3）何时停止 --- 仍然是一项非常困难的任务，这主要归结于宏观问题简化程序的广泛需求和搜索空间的庞大。由于任务具有顺序决策性和缺乏专家示范，我们提出了一种简单和高效的机器学习（RL）框架 --- 即RL4Presolve --- 以同时解决（P1）-（P3）。具体来说，我们将问题简化任务视为一个Markov决策过程，并提出了一种RL框架，其中包含可适应行为序列来生成高质量的宏观问题简化程序。注意，可适应行为序列可以高效地学习复杂的行为并适应不同的标准。在两种解决器（开源和商业）和八个标准（实际世界和 sintetic）上进行了实验，RL4Presolve显示可以有效地提高大规模LP的解决效率，特别是在行业标准上。此外，我们还使用RL学习到来自学习的策略中的规则，以便简单和高效地在Huawei的供应链中部署。结果表明，通过把机器学习技术应用到现代解决器中，可以获得有优 экономиче和学术潜力。
</details></li>
</ul>
<hr>
<h2 id="On-The-Expressivity-of-Objective-Specification-Formalisms-in-Reinforcement-Learning"><a href="#On-The-Expressivity-of-Objective-Specification-Formalisms-in-Reinforcement-Learning" class="headerlink" title="On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning"></a>On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11840">http://arxiv.org/abs/2310.11840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohan Subramani, Marcus Williams, Max Heitmann, Halfdan Holm, Charlie Griffin, Joar Skalse</li>
<li>for: 这个论文主要针对的是 reinforcement learning（RL）任务中的目标形式化问题。</li>
<li>methods: 这篇论文使用了多种目标规定 formalism，包括Linear Temporal Logic和Multi-Objective Reinforcement Learning，并进行了这些 formalism 之间的比较。</li>
<li>results: 论文发现了不同的目标规定 formalism 之间存在一定的限制，并且没有任何一种 formalism 同时具有优化和表达能力。例如，论文证明了 Regularised RL、Outer Nonlinear Markov Rewards、Reward Machines、Linear Temporal Logic 和 Limit Average Rewards 等 formalism 可以表达其他 formalism 无法表达的目标。这些结论有关于RL中目标规定 formalism的选择和实践中的表达限制。<details>
<summary>Abstract</summary>
To solve a task with reinforcement learning (RL), it is necessary to formally specify the goal of that task. Although most RL algorithms require that the goal is formalised as a Markovian reward function, alternatives have been developed (such as Linear Temporal Logic and Multi-Objective Reinforcement Learning). Moreover, it is well known that some of these formalisms are able to express certain tasks that other formalisms cannot express. However, there has not yet been any thorough analysis of how these formalisms relate to each other in terms of expressivity. In this work, we fill this gap in the existing literature by providing a comprehensive comparison of the expressivities of 17 objective-specification formalisms in RL. We place these formalisms in a preorder based on their expressive power, and present this preorder as a Hasse diagram. We find a variety of limitations for the different formalisms, and that no formalism is both dominantly expressive and straightforward to optimise with current techniques. For example, we prove that each of Regularised RL, Outer Nonlinear Markov Rewards, Reward Machines, Linear Temporal Logic, and Limit Average Rewards can express an objective that the others cannot. Our findings have implications for both policy optimisation and reward learning. Firstly, we identify expressivity limitations which are important to consider when specifying objectives in practice. Secondly, our results highlight the need for future research which adapts reward learning to work with a variety of formalisms, since many existing reward learning methods implicitly assume that desired objectives can be expressed with Markovian rewards. Our work contributes towards a more cohesive understanding of the costs and benefits of different RL objective-specification formalisms.
</details>
<details>
<summary>摘要</summary>
要解决一个任务使用强化学习（RL），需要正式 specify 该任务的目标。大多数 RL 算法需要将目标 formalized 为 Markov 奖励函数，但是有其他形式（如线性时间逻辑和多目标强化学习）也有被开发出来。然而，到目前为止，没有任何 thorougly 分析这些形式之间的关系。在这种情况下，我们填充了现有文献中的这种 gap  by 提供了17种目标规定 formalism 在RL中的比较。我们将这些 formalism 按照其表达力排序，并将其显示为一个 Hasse  диаграм。我们发现了不同 formalism 的一些限制，并证明了每种 formalism 都有一些可以表达的任务，而其他 formalism 不能表达。例如，我们证明了 Regularized RL、Outer Nonlinear Markov Rewards、Reward Machines、线性时间逻辑和 Limit Average Rewards 可以表达出其他 formalism 不能表达的任务。我们的发现对于policy优化和奖励学习都有重要的意义。首先，我们identified 表达力的限制，这些限制在实践中需要考虑。其次，我们的结果表明需要将奖励学习适应到不同 formalism 中，因为现有的奖励学习方法通常假设desired objective 可以用 Markov 奖励函数表达。我们的工作对RL objective-specification formalism 的costs and benefits 提供了更加一致的理解。
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Bootstrapping-for-Uncertainty-Quantification-in-Imaging-Inverse-Problems"><a href="#Equivariant-Bootstrapping-for-Uncertainty-Quantification-in-Imaging-Inverse-Problems" class="headerlink" title="Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems"></a>Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11838">http://arxiv.org/abs/2310.11838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tachella/equivariant_bootstrap">https://github.com/tachella/equivariant_bootstrap</a></li>
<li>paper_authors: Julian Tachella, Marcelo Pereyra</li>
<li>for:  This paper aims to accurately quantify the uncertainty in solutions to severely ill-posed scientific imaging problems, which is critical for interpreting experimental results and using reconstructed images as scientific evidence.</li>
<li>methods: The proposed uncertainty quantification methodology is based on an equivariant formulation of the parametric bootstrap algorithm, which leverages symmetries and invariance properties commonly encountered in imaging problems. The method is general and can be applied with any image reconstruction technique, including unsupervised training strategies.</li>
<li>results: The proposed method delivers remarkably accurate high-dimensional confidence regions and outperforms alternative uncertainty quantification strategies in terms of estimation accuracy, uncertainty quantification accuracy, and computing time. The method is demonstrated through a series of numerical experiments.<details>
<summary>Abstract</summary>
Scientific imaging problems are often severely ill-posed, and hence have significant intrinsic uncertainty. Accurately quantifying the uncertainty in the solutions to such problems is therefore critical for the rigorous interpretation of experimental results as well as for reliably using the reconstructed images as scientific evidence. Unfortunately, existing imaging methods are unable to quantify the uncertainty in the reconstructed images in a manner that is robust to experiment replications. This paper presents a new uncertainty quantification methodology based on an equivariant formulation of the parametric bootstrap algorithm that leverages symmetries and invariance properties commonly encountered in imaging problems. Additionally, the proposed methodology is general and can be easily applied with any image reconstruction technique, including unsupervised training strategies that can be trained from observed data alone, thus enabling uncertainty quantification in situations where there is no ground truth data available. We demonstrate the proposed approach with a series of numerical experiments and through comparisons with alternative uncertainty quantification strategies from the state-of-the-art, such as Bayesian strategies involving score-based diffusion models and Langevin samplers. In all our experiments, the proposed method delivers remarkably accurate high-dimensional confidence regions and outperforms the competing approaches in terms of estimation accuracy, uncertainty quantification accuracy, and computing time.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimising-Distributions-with-Natural-Gradient-Surrogates"><a href="#Optimising-Distributions-with-Natural-Gradient-Surrogates" class="headerlink" title="Optimising Distributions with Natural Gradient Surrogates"></a>Optimising Distributions with Natural Gradient Surrogates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11837">http://arxiv.org/abs/2310.11837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan So, Richard E. Turner</li>
<li>for: 优化probability distribution的参数</li>
<li>methods: 使用自然偏导法优化参数</li>
<li>results: 扩展了可以使用自然偏导法优化的 distribuition 类型，以及fast、易于理解、简单实现和不需要详细模型Derivation。<details>
<summary>Abstract</summary>
Natural gradient methods have been used to optimise the parameters of probability distributions in a variety of settings, often resulting in fast-converging procedures. Unfortunately, for many distributions of interest, computing the natural gradient has a number of challenges. In this work we propose a novel technique for tackling such issues, which involves reframing the optimisation as one with respect to the parameters of a surrogate distribution, for which computing the natural gradient is easy. We give several examples of existing methods that can be interpreted as applying this technique, and propose a new method for applying it to a wide variety of problems. Our method expands the set of distributions that can be efficiently targeted with natural gradients. Furthermore, it is fast, easy to understand, simple to implement using standard autodiff software, and does not require lengthy model-specific derivations. We demonstrate our method on maximum likelihood estimation and variational inference tasks.
</details>
<details>
<summary>摘要</summary>
自然均方法已经广泛应用于估计概率分布参数，经常导致快速收敛的过程。然而，许多感兴趣的分布中，计算自然均方的问题充满挑战。在这种情况下，我们提出了一种新的技巧，即将估计变换为一种对准ocker分布参数的估计问题，其中计算自然均方是容易的。我们给出了一些现有的方法，可以看作是应用这种技巧，并提出了一种新的方法，可以应用于各种问题。我们的方法可以扩展到更多的分布，并且快速、易于理解、使用标准自动极化软件实现，不需要详细的模型特定的 derivations。我们在最大 LIKELIHOOD估计和variational推断任务中进行了示例。
</details></li>
</ul>
<hr>
<h2 id="CLARA-Multilingual-Contrastive-Learning-for-Audio-Representation-Acquisition"><a href="#CLARA-Multilingual-Contrastive-Learning-for-Audio-Representation-Acquisition" class="headerlink" title="CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition"></a>CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11830">http://arxiv.org/abs/2310.11830</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/knoriy/CLARA">https://github.com/knoriy/CLARA</a></li>
<li>paper_authors: Kari A Noriy, Xiaosong Yang, Marcin Budka, Jian Jun Zhang</li>
<li>for: 本研究提出了一种多语言语音和声音表示学习框架，用于解决语音处理研究中数据的问题。</li>
<li>methods: 该框架使用了对比学习技术，通过自动生成的卷积数据来增加数据量，并让模型从无标签数据上学习共享表示。</li>
<li>results: 实验结果表明，该模型在识别情绪、音频分类和检索 bencmarks 中表现出色，具有适用于多语言和各种各样的声音条件的共享表示能力，同时还能够编码潜在的情感维度。<details>
<summary>Abstract</summary>
This paper proposes a novel framework for multilingual speech and sound representation learning using contrastive learning. The lack of sizeable labelled datasets hinders speech-processing research across languages. Recent advances in contrastive learning provide self-supervised techniques to learn from unlabelled data. Motivated by reducing data dependence and improving generalisation across diverse languages and conditions, we develop a multilingual contrastive framework. This framework enables models to acquire shared representations across languages, facilitating cross-lingual transfer with limited target language data.   Additionally, capturing emotional cues within speech is challenging due to subjective perceptual assessments. By learning expressive representations from diverse, multilingual data in a self-supervised manner, our approach aims to develop speech representations that encode emotive dimensions.   Our method trains encoders on a large corpus of multi-lingual audio data. Data augmentation techniques are employed to expand the dataset. The contrastive learning approach trains the model to maximise agreement between positive pairs and minimise agreement between negative pairs. Extensive experiments demonstrate state-of-the-art performance of the proposed model on emotion recognition, audio classification, and retrieval benchmarks under zero-shot and few-shot conditions. This provides an effective approach for acquiring shared and generalised speech representations across languages and acoustic conditions while encoding latent emotional dimensions.
</details>
<details>
<summary>摘要</summary>
We train encoders on a large corpus of multi-lingual audio data, and employ data augmentation techniques to expand the dataset. The contrastive learning approach trains the model to maximize agreement between positive pairs and minimize agreement between negative pairs. Extensive experiments demonstrate state-of-the-art performance of the proposed model on emotion recognition, audio classification, and retrieval benchmarks under zero-shot and few-shot conditions. This provides an effective approach for acquiring shared and generalised speech representations across languages and acoustic conditions while encoding latent emotional dimensions.Here's the Simplified Chinese translation:这篇论文提出了一种新的多语言语音和声音表示学习框架，使用对比学习。由于语言上的大量标注数据缺乏，这阻碍了跨语言语音处理研究的进步。但是，最近的对比学习技术提供了一种无监督的学习方法，可以从无标注数据中学习。我们的方法旨在通过学习多语言数据，以便在不同语言和条件下实现交互转移，并且编码潜在的情感维度。我们将编码器训练在一个大量多语言音频数据集上，并使用数据扩展技术来扩大数据集。对比学习方法将模型训练以最大化正方向对的匹配，并最小化负方向对的匹配。广泛的实验表明，提议的模型在情感识别、音频分类和检索benchmark上实现了顶尖性能，包括零shot和几shot情况下。这提供了一种有效的方法，可以在不同语言和音频条件下获得共享和普适的语音表示，同时编码潜在的情感维度。
</details></li>
</ul>
<hr>
<h2 id="Towards-Graph-Foundation-Models-A-Survey-and-Beyond"><a href="#Towards-Graph-Foundation-Models-A-Survey-and-Beyond" class="headerlink" title="Towards Graph Foundation Models: A Survey and Beyond"></a>Towards Graph Foundation Models: A Survey and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11829">http://arxiv.org/abs/2310.11829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, Chuan Shi</li>
<li>for: 这篇论文旨在探讨基于图像学的基本模型，以及其在不同的人工智能应用中的潜在应用。</li>
<li>methods: 本论文提出了基于图像学的基本模型（GFM）的概念，并对其特点和技术进行了系统的描述。此外，文章还分类了现有的工作，根据它们的依赖于图像神经网络和大语言模型的程度。</li>
<li>results: 文章提供了当前基于图像学的基本模型领域的全面的概述，以及这个领域的未来研究方向。<details>
<summary>Abstract</summary>
Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Parallelly, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. The emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. However, there is currently no clear definition and systematic analysis for this type of work. In this article, we propose the concept of graph foundation models (GFMs), and provide the first comprehensive elucidation on their key characteristics and technologies. Following that, we categorize existing works towards GFMs into three categories based on their reliance on graph neural networks and large language models. Beyond providing a comprehensive overview of the current landscape of graph foundation models, this article also discusses potential research directions for this evolving field.
</details>
<details>
<summary>摘要</summary>
emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. 同时, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. the emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. however, there is currently no clear definition and systematic analysis for this type of work. in this article, we propose the concept of graph foundation models (gfms), and provide the first comprehensive elucidation on their key characteristics and technologies. following that, we categorize existing works towards gfms into three categories based on their reliance on graph neural networks and large language models. beyond providing a comprehensive overview of the current landscape of graph foundation models, this article also discusses potential research directions for this evolving field.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="A-Historical-Context-for-Data-Streams"><a href="#A-Historical-Context-for-Data-Streams" class="headerlink" title="A Historical Context for Data Streams"></a>A Historical Context for Data Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19811">http://arxiv.org/abs/2310.19811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Indre Zliobaite, Jesse Read</li>
<li>for: 这篇论文主要针对的是从数据流中学习的机器学习问题，这是一个活跃和快速发展的研究领域。</li>
<li>methods: 这篇论文使用了一些传统的机器学习算法，但是它们受到了数据流的计算资源限制，例如每个实例只能被检查一次，并且需要在任何时间提供预测结果。</li>
<li>results: 这篇论文提出了一些历史上对数据流机器学习的假设，并将这些假设放在历史上的学术背景中进行了回顾。<details>
<summary>Abstract</summary>
Machine learning from data streams is an active and growing research area. Research on learning from streaming data typically makes strict assumptions linked to computational resource constraints, including requirements for stream mining algorithms to inspect each instance not more than once and be ready to give a prediction at any time. Here we review the historical context of data streams research placing the common assumptions used in machine learning over data streams in their historical context.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将数据流中的机器学习作为活跃和快速发展的研究领域。研究从数据流中学习通常做出严格的计算资源限制，包括流程挖掘算法不能再次检查每个实例，并且要准备任何时间给出预测。我们在这里将数据流研究的历史背景和常见的机器学习假设置在历史上的位置。Translation:机器学习从数据流中是一个活跃和快速发展的研究领域。研究从数据流中学习通常做出严格的计算资源限制，包括流程挖掘算法不能再次检查每个实例，并且要准备任何时间给出预测。我们在这里将数据流研究的历史背景和常见的机器学习假设置在历史上的位置。
</details></li>
</ul>
<hr>
<h2 id="De-novo-protein-design-using-geometric-vector-field-networks"><a href="#De-novo-protein-design-using-geometric-vector-field-networks" class="headerlink" title="De novo protein design using geometric vector field networks"></a>De novo protein design using geometric vector field networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11802">http://arxiv.org/abs/2310.11802</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weian Mao, Muzhi Zhu, Zheng Sun, Shuaike Shen, Lin Yuanbo Wu, Hao Chen, Chunhua Shen</li>
<li>for: 这篇论文主要关注的是对蛋白质构造设计的进步，特别是透过蛋白质diffusion的创新，使得蛋白质设计得以进行更加精确和有效的模型化。</li>
<li>methods: 本论文提出了一种新的数据 Computation Network（VFN），可以在蛋白质diffusion中进行更加精确的框架模型化，并且可以同时模型框架和原子。VFN使用了学习可控的 вектор计算，将蛋白质框架中的各个位置转换为可读的 вектор值，然后使用弹性总和将这些 вектор值与蛋白质框架中的各个原子进行相互关联。</li>
<li>results: 本论文的实验结果显示，VFN在蛋白质diffusion中表现出色，比起先前的IPA模型，VFN在设计性（67.04% vs. 53.58%)和多样性（66.54% vs. 51.98%)等方面均有较好的表现。此外，VFN也在倒拾蛋白质（frame和原子模型）中表现出色，比起先前的PiFold模型（54.7% vs. 51.66%），VFN在序列恢复率上有较好的表现。此外，本论文还提出了一种将VFN与ESM模型结合的方法，这种方法在先前的ESM-based SoTA（62.67% vs. 55.65%）上有着较好的表现。<details>
<summary>Abstract</summary>
Innovations like protein diffusion have enabled significant progress in de novo protein design, which is a vital topic in life science. These methods typically depend on protein structure encoders to model residue backbone frames, where atoms do not exist. Most prior encoders rely on atom-wise features, such as angles and distances between atoms, which are not available in this context. Thus far, only several simple encoders, such as IPA, have been proposed for this scenario, exposing the frame modeling as a bottleneck. In this work, we proffer the Vector Field Network (VFN), which enables network layers to perform learnable vector computations between coordinates of frame-anchored virtual atoms, thus achieving a higher capability for modeling frames. The vector computation operates in a manner similar to a linear layer, with each input channel receiving 3D virtual atom coordinates instead of scalar values. The multiple feature vectors output by the vector computation are then used to update the residue representations and virtual atom coordinates via attention aggregation. Remarkably, VFN also excels in modeling both frames and atoms, as the real atoms can be treated as the virtual atoms for modeling, positioning VFN as a potential universal encoder. In protein diffusion (frame modeling), VFN exhibits an impressive performance advantage over IPA, excelling in terms of both designability (67.04% vs. 53.58%) and diversity (66.54% vs. 51.98%). In inverse folding (frame and atom modeling), VFN outperforms the previous SoTA model, PiFold (54.7% vs. 51.66%), on sequence recovery rate. We also propose a method of equipping VFN with the ESM model, which significantly surpasses the previous ESM-based SoTA (62.67% vs. 55.65%), LM-Design, by a substantial margin.
</details>
<details>
<summary>摘要</summary>
新技术如蛋白diffusion已经使得蛋白结构设计得到了重要的进步，这是生命科学中非常重要的话题。这些方法通常依赖于蛋白结构编码器来模拟蛋白质量框架，其中原子不存在。以前的编码器大多数依赖于原子粒子特征，如原子之间的角度和距离，这些特征在这种情况下不可用。只有一些简单的编码器，如IPA，已经被提出，这暴露了框架模型化为瓶颈。在这种工作中，我们提议使用 Vector Field Network（VFN），它使得网络层可以通过学习vector计算来处理坐标相关的操作。VFN在蛋白diffusion（框架模型）中表现出了非常出色的性能优势，比IPA更高，达到67.04% vs. 53.58%的设计性能和66.54% vs. 51.98%的多样性。在 inverse folding（框架和原子模型）中，VFN也超越了之前的SoTA模型，PiFold（54.7% vs. 51.66%），在序列恢复率方面表现出色。我们还提出了使用VFN和ESM模型的方法，该方法在之前的ESM-based SoTA（62.67% vs. 55.65%）之上显著提高了性能。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Training-for-Physics-Informed-Neural-Networks"><a href="#Adversarial-Training-for-Physics-Informed-Neural-Networks" class="headerlink" title="Adversarial Training for Physics-Informed Neural Networks"></a>Adversarial Training for Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11789">http://arxiv.org/abs/2310.11789</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaoli90/at-pinn">https://github.com/yaoli90/at-pinn</a></li>
<li>paper_authors: Yao Li, Shengzhu Shi, Zhichang Guo, Boying Wu</li>
<li>For: 解决复杂的偏微分方程（PDEs）中的缺乏稳定性问题，提高Physics-informed neural networks（PINNs）的测试精度和可靠性。* Methods: 基于投影gradient descent对抗攻击（PGD-based adversarial attack），提出了一种名为AT-PINNs的对抗训练策略，可以增强PINNs的Robustness和稳定性。AT-PINNs可以通过在训练过程中使用对抗样本来准确地识别模型失败位置，并在训练过程中使模型更加注重这些位置。* Results: 应用AT-PINNs于各种复杂的PDEs，包括多尺度约束的圆柱方程、多峰解的波兰射方程、普朗克方程的锐度解和Allen-Cahn方程。结果表明，AT-PINNs可以有效地定位和减少失败区域，并且适用于解决复杂的PDEs，因为对于失败区域的定位无关于失败区域的大小或分布的复杂性。<details>
<summary>Abstract</summary>
Physics-informed neural networks have shown great promise in solving partial differential equations. However, due to insufficient robustness, vanilla PINNs often face challenges when solving complex PDEs, especially those involving multi-scale behaviors or solutions with sharp or oscillatory characteristics. To address these issues, based on the projected gradient descent adversarial attack, we proposed an adversarial training strategy for PINNs termed by AT-PINNs. AT-PINNs enhance the robustness of PINNs by fine-tuning the model with adversarial samples, which can accurately identify model failure locations and drive the model to focus on those regions during training. AT-PINNs can also perform inference with temporal causality by selecting the initial collocation points around temporal initial values. We implement AT-PINNs to the elliptic equation with multi-scale coefficients, Poisson equation with multi-peak solutions, Burgers equation with sharp solutions and the Allen-Cahn equation. The results demonstrate that AT-PINNs can effectively locate and reduce failure regions. Moreover, AT-PINNs are suitable for solving complex PDEs, since locating failure regions through adversarial attacks is independent of the size of failure regions or the complexity of the distribution.
</details>
<details>
<summary>摘要</summary>
物理学 Informed Neural Networks (PINNs) 已经展示了解决partial differential equations (PDEs) 的巨大承诺. 然而，由于不充分的Robustness，vanilla PINNs 经常在解决复杂的PDEs中遇到挑战，特别是包含多尺度行为或解决具有锐利或振荡特征的PDEs. 为了解决这些问题，我们基于Projected gradient descent adversarial attack (PGD-AA)提出了一种名为AT-PINNs的对抗训练策略。AT-PINNs可以增强PINNs的Robustness，通过在训练过程中使用对抗样本，准确地识别模型失败的位置并使模型在训练过程中专注于这些位置。AT-PINNs还可以通过选择时间初值附近的初始坐标来进行时间 causality 的推理。我们将AT-PINNs应用到了各种PDEs，包括具有多尺度系数的圆柱 equation、Poisson equation with multi-peak solutions、Burgers equation with sharp solutions和Allen-Cahn equation。结果表明，AT-PINNs可以有效地定位和减少失败区域。此外，AT-PINNs适用于解决复杂的PDEs，因为通过对抗攻击定位失败区域是独立于失败区域的大小或分布复杂性的。
</details></li>
</ul>
<hr>
<h2 id="NeuroCUT-A-Neural-Approach-for-Robust-Graph-Partitioning"><a href="#NeuroCUT-A-Neural-Approach-for-Robust-Graph-Partitioning" class="headerlink" title="NeuroCUT: A Neural Approach for Robust Graph Partitioning"></a>NeuroCUT: A Neural Approach for Robust Graph Partitioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11787">http://arxiv.org/abs/2310.11787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, Sayan Ranu</li>
<li>for: 分区图形式的问题，即将图分成k个独立的部分，以优化特定的分区目标。</li>
<li>methods:  neural approach，包括一个新的框架NeuroCut，它在查询时可以对图的结构和分区数进行泛化，并通过基于节点表示学习的强化学习框架来满足任何优化目标，包括不可导函数。</li>
<li>results: NeuroCut在实验中表现出色，能够找到高质量的分区，具有强大的泛化性和对图结构 modificatiopn的抗颤势性。<details>
<summary>Abstract</summary>
Graph partitioning aims to divide a graph into $k$ disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. As a result, conventional approximation algorithms rely on heuristic methods, sometimes with approximation guarantees and sometimes without. Unfortunately, traditional approaches are tailored for specific partitioning objectives and do not generalize well across other known partitioning objectives from the literature. To overcome this limitation, and learn heuristics from the data directly, neural approaches have emerged, demonstrating promising outcomes. In this study, we extend this line of work through a novel framework, NeuroCut. NeuroCut introduces two key innovations over prevailing methodologies. First, it is inductive to both graph topology and the partition count, which is provided at query time. Second, by leveraging a reinforcement learning based framework over node representations derived from a graph neural network, NeuroCut can accommodate any optimization objective, even those encompassing non-differentiable functions. Through empirical evaluation, we demonstrate that NeuroCut excels in identifying high-quality partitions, showcases strong generalization across a wide spectrum of partitioning objectives, and exhibits resilience to topological modifications.
</details>
<details>
<summary>摘要</summary>
graf分割的目标是将 Graf 分成 k 个不交叉的子集，同时最大化特定的分割目标。大多数相关的形式化问题都会显示NP困难，因为它们具有各种 combinatorial 特性。因此，现有的approximation算法通常使用了heuristic方法，有时具有approximation保证，有时没有。 unfortunately，传统的方法通常是为特定的分割目标设计的，不能总是泛化到其他从文献中知道的分割目标。为了解决这个限制，并从数据中直接学习heuristics，神经方法出现了。在这项研究中，我们通过一个新的框架，NeuroCut，进一步推动这一线的发展。NeuroCut 具有两个关键创新：首先，它是对 Graf 结构和分割 count  inductive的，可以在查询时提供。其次，通过利用基于节点表示学习的reinforcement learning框架，NeuroCut 可以处理任何优化目标，包括不可导函数。通过实验评估，我们示出NeuroCut 可以提供高质量的分割，具有强大的泛化能力，并且对 Graf 结构的修改 display 强大的抗衡性。
</details></li>
</ul>
<hr>
<h2 id="A-Quasi-Wasserstein-Loss-for-Learning-Graph-Neural-Networks"><a href="#A-Quasi-Wasserstein-Loss-for-Learning-Graph-Neural-Networks" class="headerlink" title="A Quasi-Wasserstein Loss for Learning Graph Neural Networks"></a>A Quasi-Wasserstein Loss for Learning Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11762">http://arxiv.org/abs/2310.11762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minjie Cheng, Hongteng Xu</li>
<li>for: 提高 Graph Neural Network (GNN) 在节点级预测任务中的性能，因为现有的损失函数通常对每个节点独立进行应用，即使节点嵌入和标签不是独立的。</li>
<li>methods: 提出了一种新的 quasi-Wasserstein (QW) 损失函数，基于图上的最优运输定义，用于修改 GNN 的学习和预测方法。该损失函数定义了图边上的 quasi-Wasserstein 距离，用于优化标签的运输定义。</li>
<li>results: 实验表明，提出的 QW 损失函数可以应用于多种 GNN 模型，并且能够提高其性能在节点级预测和回归任务中。此外，该损失函数还可以提供一种新的拟合学习和预测方法。<details>
<summary>Abstract</summary>
When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transport. When predicting node labels, our model combines the output of the GNN with the residual component provided by the optimal label transport, leading to a new transductive prediction paradigm. Experiments show that the proposed QW loss applies to various GNNs and helps to improve their performance in node-level classification and regression tasks.
</details>
<details>
<summary>摘要</summary>
当学习图 neural network (GNN) 在节点级预测任务时，大多数现有的损失函数都是对每个节点独立应用的，即使节点表示和其标签不是独立的，因为它们的图结构。为了消除这种不一致，在本研究中我们提出了一种新的 quasi-Wasserstein (QW) 损失函数，基于图上的最优运输定义。在特定情况下，我们定义了 observe 多维节点标签和其估计之间的 "quasi-Wasserstein" 距离，并且优化了图边上的标签运输定义。这些估计是通过一个 GNN 来 parameterize，其中优化的标签运输可能会确定图边权重。通过将 строго的标签运输约束转换为 Bregman 分布定义based REG regularizer，我们获得了我们的提议的 QW 损失函数，并且可以使用两种高效的算法来学习 GNN 和标签运输。在预测节点标签时，我们将 GNN 的输出与标签运输的 residual 组件相加，这导致了一种新的混合预测 paradigm。实验表明，我们的提议 QW 损失函数可以应用于多种 GNN 和提高它们在节点级预测和回归任务中的性能。
</details></li>
</ul>
<hr>
<h2 id="Unintended-Memorization-in-Large-ASR-Models-and-How-to-Mitigate-It"><a href="#Unintended-Memorization-in-Large-ASR-Models-and-How-to-Mitigate-It" class="headerlink" title="Unintended Memorization in Large ASR Models, and How to Mitigate It"></a>Unintended Memorization in Large ASR Models, and How to Mitigate It</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11739">http://arxiv.org/abs/2310.11739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lun Wang, Om Thakkar, Rajiv Mathews</li>
<li>for: 检测大型自动声音识别（ASR）模型中的 memorization 问题，以保护隐私。</li>
<li>methods: 提出了一种简单的检测方法，通过快速生成的句子速度来创建声音和文本信息之间的易于学习Mapping。</li>
<li>results: 在state-of-the-art ASR模型中发现了memorization问题，并通过gradient clipping来 Mitigate memorization。在大规模分布式训练中，clip each example’s gradient可以保持中性模型质量和计算成本，同时提供强的隐私保护。<details>
<summary>Abstract</summary>
It is well-known that neural networks can unintentionally memorize their training examples, causing privacy concerns. However, auditing memorization in large non-auto-regressive automatic speech recognition (ASR) models has been challenging due to the high compute cost of existing methods such as hardness calibration. In this work, we design a simple auditing method to measure memorization in large ASR models without the extra compute overhead. Concretely, we speed up randomly-generated utterances to create a mapping between vocal and text information that is difficult to learn from typical training examples. Hence, accurate predictions only for sped-up training examples can serve as clear evidence for memorization, and the corresponding accuracy can be used to measure memorization. Using the proposed method, we showcase memorization in the state-of-the-art ASR models. To mitigate memorization, we tried gradient clipping during training to bound the influence of any individual example on the final model. We empirically show that clipping each example's gradient can mitigate memorization for sped-up training examples with up to 16 repetitions in the training set. Furthermore, we show that in large-scale distributed training, clipping the average gradient on each compute core maintains neutral model quality and compute cost while providing strong privacy protection.
</details>
<details>
<summary>摘要</summary>
很多人知道神经网络可能会无意地记忆训练示例，这引起了隐私问题。然而，对大型非自动回归自动语音识别（ASR）模型的审核记忆存在高计算成本的问题，使得现有方法如困难度调整不太实用。在这种情况下，我们提出了一种简单的审核方法，可以不增加计算成本来测量大型ASR模型的记忆。具体来说，我们将随机生成的语音快速播放，以创建语音和文本信息之间的易于学习的映射。因此，只有对快速播放的训练示例进行准确预测时，可以作为记忆的证据，并且可以用这个精度来测量记忆。使用我们的方法，我们显示了state-of-the-art ASR模型中的记忆。为了解决记忆问题，我们尝试使用梯度截断法在训练时进行 bounding 梯度的影响。我们经验显示，对每个示例的梯度进行截断可以 Mitigate 记忆，并且可以在快速播放示例中进行16次复制。此外，我们还显示了在大规模分布式训练中，对每个计算核心的梯度平均截断可以保持中立的模型质量和计算成本，同时提供强的隐私保护。
</details></li>
</ul>
<hr>
<h2 id="On-the-Evaluation-of-Generative-Models-in-Distributed-Learning-Tasks"><a href="#On-the-Evaluation-of-Generative-Models-in-Distributed-Learning-Tasks" class="headerlink" title="On the Evaluation of Generative Models in Distributed Learning Tasks"></a>On the Evaluation of Generative Models in Distributed Learning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11714">http://arxiv.org/abs/2310.11714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiao Wang, Farzan Farnia, Zhenghao Lin, Yunheng Shen, Bei Yu</li>
<li>for: 这篇论文主要关注在分布式学习中评估深度生成模型，包括生成对抗网络（GANs）和扩散模型。</li>
<li>methods: 这篇论文使用了Fréchet对劲距离（FID）和核心对劲距离（KID）等评估生成模型的方法。</li>
<li>results: 论文发现在分布式学习 задача中，使用FID和KID评估生成模型的结果可能会不同，具体来说是FID-avg和FID-all的评估结果可能会不同，而KID-avg和KID-all的评估结果则相同。<details>
<summary>Abstract</summary>
The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsistent, which can lead to different optimal generative models according to the two aggregate scores. Next, we consider the kernel inception distance (KID) and similarly define the KID-avg and KID-all aggregations. Unlike the FID case, we prove that KID-all and KID-avg result in the same rankings of generative models. We perform several numerical experiments on standard image datasets and training schemes to support our theoretical findings on the evaluation of generative models in distributed learning problems.
</details>
<details>
<summary>摘要</summary>
文章研究了深度生成模型（包括生成对抗网络）在分布式学习任务中的评价方法。现有评价方法主要针对中央式学习问题，即训练数据由单个客户端存储。然而，许多生成模型应用场景是分布式学习场景，例如联邦学习场景，其中训练数据由多个客户端分布存储。本文研究了分布式学习任务中各客户端数据分布不同的生成模型评价方法。首先，我们关注Fréchet吸引距离（FID），并考虑以下FID基于客户端的综合分数：1）FID-avg，即客户端个体FID分数的平均值，2）FID-all，即训练模型与所有客户端数据集的FID距离。我们证明了FID-all和FID-avg的模型排名可能不一致，可能导致不同的优化生成模型。接下来，我们考虑核心吸引距离（KID），并定义KID-avg和KID-all综合分数。与FID不同的是，我们证明了KID-all和KID-avg的模型排名是一致的。我们在标准图像集和训练方案上进行了多个数值实验来支持我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Learning-under-Label-Proportions-for-Text-Classification"><a href="#Learning-under-Label-Proportions-for-Text-Classification" class="headerlink" title="Learning under Label Proportions for Text Classification"></a>Learning under Label Proportions for Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11707">http://arxiv.org/abs/2310.11707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jatin Chauhan, Xiaoxuan Wang, Wei Wang</li>
<li>for: 本研究旨在探讨在�xygen Learning from Label Proportions（LLP）的挑战性设置下进行NLPR Training，其数据提供在汇总形式下，仅提供每个类别的样本比例作为ground truth。</li>
<li>methods: 本研究提出了一种新的形式ulation，以及一种learnability result，以提供一个generalization bound under LLP。此外，该研究还使用了一种自我supervised objective。</li>
<li>results: 根据实验结果，该方法在大规模模型和多个维度上的文本数据上 achieved better results compared to基elines in almost 87% of the experimental configurations, across multiple metrics。<details>
<summary>Abstract</summary>
We present one of the preliminary NLP works under the challenging setup of Learning from Label Proportions (LLP), where the data is provided in an aggregate form called bags and only the proportion of samples in each class as the ground truth. This setup is inline with the desired characteristics of training models under Privacy settings and Weakly supervision. By characterizing some irregularities of the most widely used baseline technique DLLP, we propose a novel formulation that is also robust. This is accompanied with a learnability result that provides a generalization bound under LLP. Combining this formulation with a self-supervised objective, our method achieves better results as compared to the baselines in almost 87% of the experimental configurations which include large scale models for both long and short range texts across multiple metrics.
</details>
<details>
<summary>摘要</summary>
我们介绍了一项初步的自然语言处理（NLP）工作，在“学习从标签含量（LLP）”的挑战性设置下进行训练，其中数据提供在归一化的形式下，即袋（bag），并且只有每个类别的样本占总数的比例作为真实的地面信息。这种设置符合训练模型下的隐私设置和弱监督。我们对最常用的基线技术DLLP的不规则性进行描述，并提出了一种新的形式ulation，这种形式ulation具有 robustness。此外，我们还提供了一个learnability result，它在LLP下提供了一个通用的泛化 bound。将这种形式ulation与一种自我超vised目标函数相结合，我们的方法在大规模的实验配置中（包括长文本和短文本） across multiple metrics  Achieves better results than baselines in nearly 87% of the cases.
</details></li>
</ul>
<hr>
<h2 id="AUC-mixup-Deep-AUC-Maximization-with-Mixup"><a href="#AUC-mixup-Deep-AUC-Maximization-with-Mixup" class="headerlink" title="AUC-mixup: Deep AUC Maximization with Mixup"></a>AUC-mixup: Deep AUC Maximization with Mixup</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11693">http://arxiv.org/abs/2310.11693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzhi Xv, Gang Li, Tianbao Yang</li>
<li>for: 提高异常点识别模型的泛化能力，解决深度AUC最大化（DAM）在小据集上存在严重过拟合问题。</li>
<li>methods: 使用混合数据增强（mixup）数据增强技术，并采用AUC环境损失来有效地从混合数据生成的数据中学习，称为AUC-mixup损失。</li>
<li>results: 在异常点识别和医学影像数据集上，与标准DAM训练方法相比，提出的AUC-mixup方法显示出更高的泛化性能。<details>
<summary>Abstract</summary>
While deep AUC maximization (DAM) has shown remarkable success on imbalanced medical tasks, e.g., chest X-rays classification and skin lesions classification, it could suffer from severe overfitting when applied to small datasets due to its aggressive nature of pushing prediction scores of positive data away from that of negative data. This paper studies how to improve generalization of DAM by mixup data augmentation -- an approach that is widely used for improving generalization of the cross-entropy loss based deep learning methods. %For overfitting issues arising from limited data, the common approach is to employ mixup data augmentation to boost the models' generalization performance by enriching the training data. However, AUC is defined over positive and negative pairs, which makes it challenging to incorporate mixup data augmentation into DAM algorithms. To tackle this challenge, we employ the AUC margin loss and incorporate soft labels into the formulation to effectively learn from data generated by mixup augmentation, which is referred to as the AUC-mixup loss. Our experimental results demonstrate the effectiveness of the proposed AUC-mixup methods on imbalanced benchmark and medical image datasets compared to standard DAM training methods.
</details>
<details>
<summary>摘要</summary>
While deep AUC maximization (DAM) has shown remarkable success on imbalanced medical tasks, such as chest X-rays classification and skin lesions classification, it can suffer from severe overfitting when applied to small datasets due to its aggressive nature of pushing prediction scores of positive data away from that of negative data. This paper studies how to improve the generalization of DAM by using mixup data augmentation -- an approach that is widely used for improving the generalization of cross-entropy loss-based deep learning methods. For overfitting issues arising from limited data, the common approach is to employ mixup data augmentation to boost the models' generalization performance by enriching the training data. However, AUC is defined over positive and negative pairs, which makes it challenging to incorporate mixup data augmentation into DAM algorithms. To tackle this challenge, we employ the AUC margin loss and incorporate soft labels into the formulation to effectively learn from data generated by mixup augmentation, which is referred to as the AUC-mixup loss. Our experimental results demonstrate the effectiveness of the proposed AUC-mixup methods on imbalanced benchmark and medical image datasets compared to standard DAM training methods.
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-on-Transformer-architecture-for-power-system-short-term-voltage-stability-assessment-with-class-imbalance"><a href="#Deep-learning-based-on-Transformer-architecture-for-power-system-short-term-voltage-stability-assessment-with-class-imbalance" class="headerlink" title="Deep learning based on Transformer architecture for power system short-term voltage stability assessment with class imbalance"></a>Deep learning based on Transformer architecture for power system short-term voltage stability assessment with class imbalance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11690">http://arxiv.org/abs/2310.11690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Li, Jiting Cao, Yan Xu, Lipeng Zhu, Zhao Yang Dong</li>
<li>for: 本研究提出了一种解决短时电压稳定评估中数据不均衡问题的方法，以提高实时电压稳定评估的精度和可靠性。</li>
<li>methods: 本研究使用了Transformer架构，开发了一种名为StaaT的稳定评估Transformer，并采用了conditional Wasserstein生成敌对网络（CWGAN-GP）来生成Synthetic数据，以帮助创建一个均衡的、代表性的训练集。此外，本研究还采用了半upervised clustering学习来提高划分质量，因为短时电压稳定无一定的量化标准。</li>
<li>results: 数据测试表明，提出的方法在面临100:1的数据不均衡和噪音环境时仍然保持了稳定的性能，并且在增加可再生能源的情况下也保持了一致的效果。比较结果表明，CWGAN-GP生成的数据更具备均衡性，而StaaT也超过了其他深度学习算法。这种方法可以应用于实际短时电压稳定评估中，frequently face着数据不均衡和噪音挑战。<details>
<summary>Abstract</summary>
Most existing data-driven power system short-term voltage stability assessment (STVSA) approaches presume class-balanced input data. However, in practical applications, the occurrence of short-term voltage instability following a disturbance is minimal, leading to a significant class imbalance problem and a consequent decline in classifier performance. This work proposes a Transformer-based STVSA method to address this challenge. By utilizing the basic Transformer architecture, a stability assessment Transformer (StaaT) is developed {as a classification model to reflect the correlation between the operational states of the system and the resulting stability outcomes}. To combat the negative impact of imbalanced datasets, this work employs a conditional Wasserstein generative adversarial network with gradient penalty (CWGAN-GP) for synthetic data generation, aiding in the creation of a balanced, representative training set for the classifier. Semi-supervised clustering learning is implemented to enhance clustering quality, addressing the lack of a unified quantitative criterion for short-term voltage stability. {Numerical tests on the IEEE 39-bus test system extensively demonstrate that the proposed method exhibits robust performance under class imbalances up to 100:1 and noisy environments, and maintains consistent effectiveness even with an increased penetration of renewable energy}. Comparative results reveal that the CWGAN-GP generates more balanced datasets than traditional oversampling methods and that the StaaT outperforms other deep learning algorithms. This study presents a compelling solution for real-world STVSA applications that often face class imbalance and data noise challenges.
</details>
<details>
<summary>摘要</summary>
现有的数据驱动电力系统短期电压稳定评估（STVSA）方法大多假设输入数据具有均衡的分布。然而，在实际应用中，短期电压不稳定的发生率很低，导致数据分布受到很大的偏好问题，从而导致分类器性能下降。这项工作提出了一种基于Transformer的STVSA方法来解决这个挑战。通过利用基本Transformer架构，我们开发了一种稳定评估Transformer（StaaT），用于反映系统运行状态和导致的稳定结果之间的相关性。为了解决偏好数据的负面影响，这项工作采用了 conditional Wasserstein生成敌方网络（CWGAN-GP） для生成人工数据，以帮助创建一个均衡、代表性的训练集 для分类器。 semi-supervised clustering learning 技术被应用以提高归一化质量，因为没有短期电压稳定的准确量标准。 {numeraire tests on the IEEE 39-bus test system extensively demonstrate that the proposed method exhibits robust performance under class imbalances up to 100:1 and noisy environments, and maintains consistent effectiveness even with an increased penetration of renewable energy}. comparative results reveal that the CWGAN-GP generates more balanced datasets than traditional oversampling methods and that the StaaT outperforms other deep learning algorithms. this study presents a compelling solution for real-world STVSA applications that often face class imbalance and data noise challenges.
</details></li>
</ul>
<hr>
<h2 id="Subject-specific-Deep-Neural-Networks-for-Count-Data-with-High-cardinality-Categorical-Features"><a href="#Subject-specific-Deep-Neural-Networks-for-Count-Data-with-High-cardinality-Categorical-Features" class="headerlink" title="Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features"></a>Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11654">http://arxiv.org/abs/2310.11654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hangbin Lee, Il Do Ha, Changha Hwang, Youngjo Lee</li>
<li>for: 提高预测性能和学习效率，适用于高纬度ategorical特征的分布数据处理。</li>
<li>methods: 基于 hierarchical likelihood 学习框架，引入gamma random effects，同时利用最大 likelihood 估计和best unbiased predictors来捕捉输入变量的非线性效应和subject-specific层效应。</li>
<li>results: 通过实验和实际数据分析，证明了提议方法的优势，包括提高预测性能和学习效率，以及适用于高纬度ategorical特征的分布数据处理。<details>
<summary>Abstract</summary>
There is a growing interest in subject-specific predictions using deep neural networks (DNNs) because real-world data often exhibit correlations, which has been typically overlooked in traditional DNN frameworks. In this paper, we propose a novel hierarchical likelihood learning framework for introducing gamma random effects into the Poisson DNN, so as to improve the prediction performance by capturing both nonlinear effects of input variables and subject-specific cluster effects. The proposed method simultaneously yields maximum likelihood estimators for fixed parameters and best unbiased predictors for random effects by optimizing a single objective function. This approach enables a fast end-to-end algorithm for handling clustered count data, which often involve high-cardinality categorical features. Furthermore, state-of-the-art network architectures can be easily implemented into the proposed h-likelihood framework. As an example, we introduce multi-head attention layer and a sparsemax function, which allows feature selection in high-dimensional settings. To enhance practical performance and learning efficiency, we present an adjustment procedure for prediction of random parameters and a method-of-moments estimator for pretraining of variance component. Various experiential studies and real data analyses confirm the advantages of our proposed methods.
</details>
<details>
<summary>摘要</summary>
有越来越多的研究者对特定领域预测使用深度神经网络（DNN），因为实际数据经常具有相关性，传统的DNN框架中通常会忽略这些相关性。在这篇论文中，我们提出了一种新的层次可能性学习框架，以在Poisson DNN中引入γ随机效应，以提高预测性能，同时捕捉输入变量的非线性效应和特定颗集效应。我们的方法同时实现最大可能性估计器和不偏预测器，通过优化单个目标函数。这种方法使得可以快速处理受集分布的端到端算法，这些分布frequently包含高cardinality的分类特征。此外，我们可以轻松地将当前的网络架构 integrate into our proposed h-likelihood framework。例如，我们引入多头注意层和简洁最大化函数，这些功能允许在高维度设置中进行特征选择。为了提高实际性和学习效率，我们提出了预测随机参数的调整方法和预测变量组件的方法-of-moments估计器。多种实验和实际数据分析证明了我们的提出的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Free-text-Keystroke-Authentication-using-Transformers-A-Comparative-Study-of-Architectures-and-Loss-Functions"><a href="#Free-text-Keystroke-Authentication-using-Transformers-A-Comparative-Study-of-Architectures-and-Loss-Functions" class="headerlink" title="Free-text Keystroke Authentication using Transformers: A Comparative Study of Architectures and Loss Functions"></a>Free-text Keystroke Authentication using Transformers: A Comparative Study of Architectures and Loss Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11640">http://arxiv.org/abs/2310.11640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saleh Momeni, Bagher BabaAli</li>
<li>for: 这个研究旨在提出一个基于Transformer的网络，以提高键盘识别和验证的精度。</li>
<li>methods: 这个模型使用自我注意力来提取键盘序列中的有用特征，并评估了两种不同的架构， namely bi-encoder 和 cross-encoder，以及不同的损失函数和距离度量。</li>
<li>results: 这个研究发现，使用 bi-encoder 架构、批量全 triplet 损失函数和圆形距离度量可以实现最佳性能，具体Equla Error Rate 为0.0186%。此外，还探讨了不同的相似度评估方法，以提高模型的精度。<details>
<summary>Abstract</summary>
Keystroke biometrics is a promising approach for user identification and verification, leveraging the unique patterns in individuals' typing behavior. In this paper, we propose a Transformer-based network that employs self-attention to extract informative features from keystroke sequences, surpassing the performance of traditional Recurrent Neural Networks. We explore two distinct architectures, namely bi-encoder and cross-encoder, and compare their effectiveness in keystroke authentication. Furthermore, we investigate different loss functions, including triplet, batch-all triplet, and WDCL loss, along with various distance metrics such as Euclidean, Manhattan, and cosine distances. These experiments allow us to optimize the training process and enhance the performance of our model. To evaluate our proposed model, we employ the Aalto desktop keystroke dataset. The results demonstrate that the bi-encoder architecture with batch-all triplet loss and cosine distance achieves the best performance, yielding an exceptional Equal Error Rate of 0.0186%. Furthermore, alternative algorithms for calculating similarity scores are explored to enhance accuracy. Notably, the utilization of a one-class Support Vector Machine reduces the Equal Error Rate to an impressive 0.0163%. The outcomes of this study indicate that our model surpasses the previous state-of-the-art in free-text keystroke authentication. These findings contribute to advancing the field of keystroke authentication and offer practical implications for secure user verification systems.
</details>
<details>
<summary>摘要</summary>
“键盘生物метри学是一种有前途的方法 для用户识别和验证，利用个人键盘实习独特的模式。在本研究中，我们提出了基于Transformer的网络，使用自我对项来提取键盘序列中有用的特征，超越传统的Recurrent Neural Networks的表现。我们探索了两种不同的架构，分别是双向encoder和cross-encoder，并比较它们在键盘验证中的效果。此外，我们寻找了不同的损失函数，包括三重、批量三重和WDCL损失函数，以及不同的距离度量，如Euclidean、曼哈顿和内角距离。这些实验允许我们优化训练过程，提高模型的性能。为了评估我们的提案模型，我们使用了阿尔托桌面键盘数据集。结果显示，双向encoder架构加 batch-all triplet损失函数和内角距离可以取得最佳性能，具体Equla Error Rate为0.0186%。此外，我们还探索了不同的相似度计算算法，以提高准确性。例如，使用一个一阶支持向量机可以降低Equla Error Rate至0.0163%。研究结果显示，我们的模型超越了过去的州际前进于自由文本键盘验证。这些发现对于键盘验证领域的进步做出了贡献，并且提供了实际的应用于安全用户验证系统。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/cs.LG_2023_10_18/" data-id="closbrorx00rf0g88daadfdpg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/eess.IV_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T09:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/eess.IV_2023_10_18/">eess.IV - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="System-identification-and-closed-loop-control-of-laser-hot-wire-directed-energy-deposition-using-the-parameter-signature-property-modeling-scheme"><a href="#System-identification-and-closed-loop-control-of-laser-hot-wire-directed-energy-deposition-using-the-parameter-signature-property-modeling-scheme" class="headerlink" title="System identification and closed-loop control of laser hot-wire directed energy deposition using the parameter-signature-property modeling scheme"></a>System identification and closed-loop control of laser hot-wire directed energy deposition using the parameter-signature-property modeling scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12286">http://arxiv.org/abs/2310.12286</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Rahmani Dehaghani, Atieh Sahraeidolatkhaneh, Morgan Nilsen, Fredrik Sikström, Pouyan Sajadi, Yifan Tang, G. Gary Wang<br>for: This paper focuses on developing a parameter-signature-property modeling and control approach to enhance the quality of additively manufactured parts using hot-wire directed energy deposition with a laser beam (DED-LB&#x2F;w).methods: The paper employs a dynamic modeling approach to investigate the relationship between process parameters and melt pool width, as well as a fully connected artificial neural network to predict the final part property (bead width) based on melt pool signatures.results: The proposed parameter-signature-property modeling approach shows clear advantages in controlling the width of the part compared to a control loop with only process signature (melt pool width) information. The approach has the potential to be applied to control other part properties that cannot be directly measured or monitored in situ.<details>
<summary>Abstract</summary>
Hot-wire directed energy deposition using a laser beam (DED-LB/w) is a method of metal additive manufacturing (AM) that has benefits of high material utilization and deposition rate, but parts manufactured by DED-LB/w suffer from a substantial heat input and undesired surface finish. Hence, monitoring and controlling the process parameters and signatures during the deposition is crucial to ensure the quality of final part properties and geometries. This paper explores the dynamic modeling of the DED-LB/w process and introduces a parameter-signature-property modeling and control approach to enhance the quality of modeling and control of part properties that cannot be measured in situ. The study investigates different process parameters that influence the melt pool width (signature) and bead width (property) in single and multi-layer beads. The proposed modeling approach utilizes a parameter-signature model as F_1 and a signature-property model as F_2. Linear and nonlinear modeling approaches are compared to describe a dynamic relationship between process parameters and a process signature, the melt pool width (F_1). A fully connected artificial neural network is employed to model and predict the final part property, i.e., bead width, based on melt pool signatures (F_2). Finally, the effectiveness and usefulness of the proposed parameter-signature-property modeling is tested and verified by integrating the parameter-signature (F_1) and signature-property (F_2) models in the closed-loop control of the width of the part. Compared with the control loop with only F_1, the proposed method shows clear advantages and bears potential to be applied to control other part properties that cannot be directly measured or monitored in situ.
</details>
<details>
<summary>摘要</summary>
热束导电能量沉积使用激光束(DED-LB/w)是一种金属添加生产(AM)的方法，它具有高材料利用率和沉积速率的优点，但是制造出来的部件受到了大量的热输入和不想要的表面镀层。因此，对沉积过程参数和特征的监测和控制是至关重要，以确保最终部件的性能和几何尺寸。本文研究了DED-LB/w процесс的动态模型化，并提出了参数-特征-性能模型控制方法，以提高模型和控制不可直接测量或监测的部件性能的能力。研究表示，不同的处理参数对沉积过程中的溶融池宽度（特征）和束宽度（性能）的影响。对于单层和多层束，提出了参数-特征模型和特征-性能模型两种模型方法。使用全连接人工神经网络模型和预测最终部件性能，基于溶融池特征。最后，通过将参数-特征模型和特征-性能模型在关闭控制 loop中集成，证明了提posed方法的效iveness和实用性。相比只使用参数-特征模型控制 loop，提posed方法显示了明显的优势，并可以应用于控制其他不可直接测量或监测的部件性能。
</details></li>
</ul>
<hr>
<h2 id="Denoising-total-scattering-data-using-Compressed-Sensing"><a href="#Denoising-total-scattering-data-using-Compressed-Sensing" class="headerlink" title="Denoising total scattering data using Compressed Sensing"></a>Denoising total scattering data using Compressed Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11887">http://arxiv.org/abs/2310.11887</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Weng, Niklas B. Thompson, Christopher Folmar, James D. Martin, Christina Hoffman</li>
<li>for: 该论文是为了提高Diffraction图像的信号响应率（SNR）而写的。</li>
<li>methods: 该论文使用了压缩感知技术，该技术已经成功应用于摄影、人脸识别和医疗影像等领域，以提高Diffraction图像的SNR。</li>
<li>results: 该论文表明，通过使用压缩感知技术，可以将单个Diffraction测量转化为一个有效无限多的虚拟测量，从而实现超分辨率成像。<details>
<summary>Abstract</summary>
To obtain the best resolution for any measurement there is an ever-present challenge to achieve maximal differentiation between signal and noise over as fine of sampling dimensions as possible. In diffraction science these issues are particularly pervasive when analyzing small crystals, systems with diffuse scattering, or other systems in which the signal of interest is extremely weak and incident flux and instrument time is limited. We here demonstrate that the tool of compressed sensing, which has successfully been applied to photography, facial recognition, and medical imaging, can be effectively applied to diffraction images to dramatically improve the signal-to-noise ratio (SNR) in a data-driven fashion without the need for additional measurements or modification of existing hardware. We outline a technique that leverages compressive sensing to bootstrap a single diffraction measurement into an effectively arbitrary number of virtual measurements, thereby providing a means of super-resolution imaging.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FixPix-Fixing-Bad-Pixels-using-Deep-Learning"><a href="#FixPix-Fixing-Bad-Pixels-using-Deep-Learning" class="headerlink" title="FixPix: Fixing Bad Pixels using Deep Learning"></a>FixPix: Fixing Bad Pixels using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11637">http://arxiv.org/abs/2310.11637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sreetama Sarkar, Xinan Ye, Gourav Datta, Peter A. Beerel</li>
<li>for: 提高图像感知器的生产率和预计寿命，提出了一种基于深度学习的在线检测修复方法，适用于各种像素损害率。</li>
<li>methods: 提出了一种信任报表调整的分割方法，可以在几个训练样本下实现几乎完美的坏像素检测。还提出了一种 Computationally light-weight 的修复算法，可以在低于传统插值技术的环境下达到更高的准确率。</li>
<li>results: 通过使用开源的 Samsung S7 ISP 和 MIT-Adobe FiveK 数据集，实现了高达 99.6% 的检测精度，False Positives 低于 0.6%，并在 70% 损害的图像中实现了平均像素误差在 1.5% 之间的修复。<details>
<summary>Abstract</summary>
Efficient and effective on-line detection and correction of bad pixels can improve yield and increase the expected lifetime of image sensors. This paper presents a comprehensive Deep Learning (DL) based on-line detection-correction approach, suitable for a wide range of pixel corruption rates. A confidence calibrated segmentation approach is introduced, which achieves nearly perfect bad pixel detection, even with few training samples. A computationally light-weight correction algorithm is proposed for low rates of pixel corruption, that surpasses the accuracy of traditional interpolation-based techniques. We also propose an autoencoder based image reconstruction approach which alleviates the need for prior bad pixel detection and yields promising results for high rates of pixel corruption. Unlike previous methods, which use proprietary images, we demonstrate the efficacy of the proposed methods on the open-source Samsung S7 ISP and MIT-Adobe FiveK datasets. Our approaches yield up to 99.6% detection accuracy with <0.6% false positives and corrected images within 1.5% average pixel error from 70% corrupted images.
</details>
<details>
<summary>摘要</summary>
高效和有效的在线检测和修正坏像素可以提高图像传感器的产量和预期的寿命。这篇论文提出了一种基于深度学习（DL）的全面在线检测修正方法，适用于各种坏像素损害率。我们引入了一种决度规则化的分割方法，可以在少量训练样本下达到几乎完美的坏像素检测效果。我们还提出了一种 Computational 轻量级的修正算法，可以在低坏像素率下超越传统的 interpolate-based 技术。此外，我们还提出了一种基于 autoencoder 的图像重建方法，可以消除先前的坏像素检测，并且在高坏像素率下实现了出色的 результаados。与先前的方法不同，我们使用开源的 Samsung S7 ISP 和 MIT-Adobe FiveK 数据集来证明方法的可行性。我们的方法可以达到 99.6% 的检测精度，False Positives  <0.6%，并且在 70% 损害的图像上修正了 <1.5% 的平均像素误差。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/eess.IV_2023_10_18/" data-id="closbroyz018l0g888oy8a3t6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/eess.SP_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T08:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/eess.SP_2023_10_18/">eess.SP - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Instantaneous-Frequency-Estimation-in-Unbalanced-Systems-Using-Affine-Differential-Geometry"><a href="#Instantaneous-Frequency-Estimation-in-Unbalanced-Systems-Using-Affine-Differential-Geometry" class="headerlink" title="Instantaneous Frequency Estimation in Unbalanced Systems Using Affine Differential Geometry"></a>Instantaneous Frequency Estimation in Unbalanced Systems Using Affine Differential Geometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12299">http://arxiv.org/abs/2310.12299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Alshawabkeh, Georgios Tzounas, Federico Milano</li>
<li>for: 本研究探讨了电压和频率之间的关系，以及均匀差异拓扑学中的弹性弯曲长度和曲率的关系。</li>
<li>methods: 本研究使用了均匀差异拓扑学 invariants来链接频率和电压的时间导数关系。基于这种关系，提出了一种新的快速频率估计方程，适用于不均匀系统。</li>
<li>results: 本研究提出了一种新的快速频率估计方程，可以在单相系统中实现高精度的频率估计。数值示例表明，该方程在不均匀、单相系统中具有高精度和稳定性。<details>
<summary>Abstract</summary>
The paper discusses the relationships between electrical quantities, namely voltages and frequency, and affine differential geometry ones, namely affine arc length and curvature. Moreover, it establishes a link between frequency and time derivatives of voltage, through the utilization of affine differential geometry invariants. Based on this link, a new instantaneous frequency estimation formula is proposed, which is particularly suited for unbalanced systems. An application of the proposed formula to single-phase systems is also provided. Several numerical examples based on balanced, unbalanced, as well as single-phase systems illustrate the findings of the paper.
</details>
<details>
<summary>摘要</summary>
文章讨论了电量和几何量之间的关系，即电压和频率，以及几何均衡量的抽象弹性长度和曲率。文章还将频率与电压的时间导数联系起来，通过使用几何均衡量的 invariants。基于这个联系，文章提出了一种新的快速频率估算公式，特别适用于不均衡系统。文章还提供了应用于单相系统的示例。文章的结论由多个平衡、不平衡和单相系统的数据 illustrate。Note: "几何均衡量" (affine differential geometry) is a bit of a mouthful in Chinese, so I translated it as "几何均衡量" (affine differential geometry) instead of using the more common "几何学" (geometry) or "几何均衡" (affine geometry).
</details></li>
</ul>
<hr>
<h2 id="Channel-Estimation-via-Loss-Field-Accurate-Site-Trained-Modeling-for-Shadowing-Prediction"><a href="#Channel-Estimation-via-Loss-Field-Accurate-Site-Trained-Modeling-for-Shadowing-Prediction" class="headerlink" title="Channel Estimation via Loss Field: Accurate Site-Trained Modeling for Shadowing Prediction"></a>Channel Estimation via Loss Field: Accurate Site-Trained Modeling for Shadowing Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12284">http://arxiv.org/abs/2310.12284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Wang, Meles G. Weldegebriel, Neal Patwari</li>
<li>for: 这个论文旨在为未来的移动广播网络提供一种低计算复杂性的通道模型，以确保频率划分和信号干扰水平的要求。</li>
<li>methods: 该论文提出了一种新的通道模型，即Channel Estimation using Loss Field（CELF），该模型使用了部署在地区的通道损失测量数据和bayesian线性回归方法来估算地区具有特定损失场的loss field。</li>
<li>results: 论文使用了广泛的测量数据显示，CELF可以降低通道估计的方差 by up to 56%，并且在 variance reduction和训练效率方面超过了3种popular机器学习方法。<details>
<summary>Abstract</summary>
Future mobile ad hoc networks will share spectrum between many users. Channels will be assigned on the fly to guarantee signal and interference power requirements for requested links. Channel losses must be re-estimated between many pairs of users as they move and as environmental conditions change. Computational complexity must be low, precluding the use of some accurate but computationally intensive site-specific channel models. Channel model errors must be low, precluding the use of standard statistical channel models. We propose a new channel model, CELF, which uses channel loss measurements from a deployed network in the area and a Bayesian linear regression method to estimate a site-specific loss field for the area. The loss field is explainable as the site's 'shadowing' of the radio propagation across the area of interest, but it requires no site-specific terrain or building information. Then, for any arbitrary pair of transmitter and receiver positions, CELF sums the loss field near the link line to estimate its channel loss. We use extensive measurements to show that CELF lowers the variance of channel estimates by up to 56%. It outperforms 3 popular machine learning methods in variance reduction and training efficiency.
</details>
<details>
<summary>摘要</summary>
未来的移动广播网络将共享多个用户的频率谱，为请求链接确保信号和干扰电磁谱的功率要求。在多个用户之间移动和环境条件发生变化时，通道将在实时基础上分配。由于计算复杂性需要低，因此排除了一些精度高但计算复杂度高的站点特定通道模型。通道模型错误也需要低，因此排除了标准的统计学通道模型。我们提出了一种新的通道模型，即 CEFL，它使用已部署网络中的通道损失测量和 bayesian 线性回归方法来估算区域特定的损失场。这个损失场可以解释为当地的“遮挡”，但无需站点特定的地形或建筑信息。然后，为任意传输器和接收器位置对，CEFL将近邻链接线上的损失场总和来估算链接损失。我们使用了广泛的测量数据表明，CEFL可以降低通道估计的方差，最多降低56%。同时，它在 variance 降低和训练效率上比3种受欢迎的机器学习方法表现更好。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Thermal-Profiles-in-High-Explosives-using-Neural-Networks"><a href="#Measuring-Thermal-Profiles-in-High-Explosives-using-Neural-Networks" class="headerlink" title="Measuring Thermal Profiles in High Explosives using Neural Networks"></a>Measuring Thermal Profiles in High Explosives using Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12260">http://arxiv.org/abs/2310.12260</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Greenhall, David K. Zerkle, Eric S. Davis, Robert Broilo, Cristian Pantea</li>
<li>for: 用于评估高爆物质的安全状况，以及其他各种应用场景，如化学反应、热动力过程、铁或塑料铸造、热能存储壳中的能量强度和异常电池运行等。</li>
<li>methods: 使用Convolutional Neural Network（CNN）计算高爆物质的温度profile，并在实验和 simulations中采集了声学和温度数据。实验中使用了多个声学传感器环绕高爆物质容器的外围来收集声波信号，但是在实验中测量高爆物质的温度profile会需要插入大量的热电阻温度探测器，这会对加热过程产生干扰。因此，本研究使用了两个热电阻温度探测器，一个位于高爆物质的中心，另一个位于容器墙上，并使用了finite element方法来计算高爆物质的温度分布，并对实验中心和墙上的温度进行修正。</li>
<li>results: 通过对实验和 simulations中的数据进行分析，本研究发现了一种可以评估高爆物质的安全状况的方法，并且可以在各种应用场景中提供温度profile的内部测量。研究还发现，使用更多的声学Receiver和更高的温度预测分辨率可以提高算法的准确性。<details>
<summary>Abstract</summary>
We present a new method for calculating the temperature profile in high explosive (HE) material using a Convolutional Neural Network (CNN). To train/test the CNN, we have developed a hybrid experiment/simulation method for collecting acoustic and temperature data. We experimentally heat cylindrical containers of HE material until detonation/deflagration, where we continuously measure the acoustic bursts through the HE using multiple acoustic transducers lined around the exterior container circumference. However, measuring the temperature profile in the HE in experiment would require inserting a high number of thermal probes, which would disrupt the heating process. Thus, we use two thermal probes, one at the HE center and one at the wall. We then use finite element simulation of the heating process to calculate the temperature distribution, and correct the simulated temperatures based on the experimental center and wall temperatures. We calculate temperature errors on the order of 15{\deg}C, which is approximately 12% of the range of temperatures in the experiment. We also investigate how the algorithm accuracy is affected by the number of acoustic receivers used to collect each measurement and the resolution of the temperature prediction. This work provides a means of assessing the safety status of HE material, which cannot be achieved using existing temperature measurement methods. Additionally, it has implications for range of other applications where internal temperature profile measurements would provide critical information. These applications include detecting chemical reactions, observing thermodynamic processes like combustion, monitoring metal or plastic casting, determining the energy density in thermal storage capsules, and identifying abnormal battery operation.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来计算高爆物（HE）材料中的温度分布，使用卷积神经网络（CNN）。为了训练/测试CNN，我们开发了一种混合实验/模拟方法来收集振荡和温度数据。我们通过对HE材料中的圆柱形容器进行热处理，直到发生激发/燃烧，并在HE表面附近安装多个声学传感器来记录振荡。但是，在实验中测量HE材料中的温度分布需要插入大量的热度探针，这会对热处理进行干扰。因此，我们使用了两个热度探针，一个位于HE的中心和一个位于容器壁上。我们然后使用HE材料的热处理的数学模拟来计算温度分布，并根据实验中心和壁温度进行修正。我们计算的温度误差在15℃之间，相当于实验中温度范围的12%。我们还研究了使用声学传感器来收集测量数据的数量和分辨率如何影响算法的准确性。这项工作为HE材料的安全状况评估提供了一种新的方法，同时也对其他应用有着潜在的影响。这些应用包括检测化学反应、观察燃烧过程、监测金属或塑料铸造、测量热存储囊中的能量密度、并识别异常电池运行。
</details></li>
</ul>
<hr>
<h2 id="Ordered-Reliability-Direct-Error-Pattern-Testing-Decoding-Algorithm"><a href="#Ordered-Reliability-Direct-Error-Pattern-Testing-Decoding-Algorithm" class="headerlink" title="Ordered Reliability Direct Error Pattern Testing Decoding Algorithm"></a>Ordered Reliability Direct Error Pattern Testing Decoding Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12039">http://arxiv.org/abs/2310.12039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Hadavian, Xiaoting Huang, Dmitri Truhachev, Kamal El-Sankary, Hamid Ebrahimzad, Hossein Najafi</li>
<li>for: 这篇论文是为了提出一种新的通用软决策解码算法，用于二进制块编码。</li>
<li>methods: 该算法使用ordered reliability direct error pattern testing（ORDEPT）技术，并对各种流行的短高速编码进行了测试，结果显示ORDEPT在与相同复杂性的其他解码算法相比，具有较低的解码错误概率和延迟。</li>
<li>results: 该paper的结果表明，ORDEPT可以高效地查找多个候选码word，并在迭代解码中提高产生软输出的能力。<details>
<summary>Abstract</summary>
We introduce a novel universal soft-decision decoding algorithm for binary block codes called ordered reliability direct error pattern testing (ORDEPT). Our results, obtained for a variety of popular short high-rate codes, demonstrate that ORDEPT outperforms state-of-the-art decoding algorithms of comparable complexity such as ordered reliability bits guessing random additive noise decoding (ORBGRAND) in terms of the decoding error probability and latency. The improvements carry on to the iterative decoding of product codes and convolutional product-like codes, where we present a new adaptive decoding algorithm and demonstrate the ability of ORDEPT to efficiently find multiple candidate codewords to produce soft output.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的通用软决策解码算法，即顺序可靠性直接错误模式测试（ORDEPT），用于二进制块码。我们的结果，在各种受欢迎的短高速码中，示出了ORDEPT比同等复杂度的批量解码算法，如顺序可靠性位元随机加速错误推测解码（ORBGRAND），在解码错误probability和延迟方面表现更好。这些改进继续延伸到产生转换码和几何产生码的迭代解码中，我们提出了一个新的适应解码算法，并证明了ORDEPT可以高效地找到多个候选码word来生成软出力。
</details></li>
</ul>
<hr>
<h2 id="One-Bit-Byzantine-Tolerant-Distributed-Learning-via-Over-the-Air-Computation"><a href="#One-Bit-Byzantine-Tolerant-Distributed-Learning-via-Over-the-Air-Computation" class="headerlink" title="One-Bit Byzantine-Tolerant Distributed Learning via Over-the-Air Computation"></a>One-Bit Byzantine-Tolerant Distributed Learning via Over-the-Air Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11998">http://arxiv.org/abs/2310.11998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Yang, Youlong Wu, Yuning Jiang, Yuanming Shi</li>
<li>for: 这篇论文研究了分布式学习在无线数据中心网络中，以提高智能应用的可扩展性和可靠性。</li>
<li>methods: 该论文提出了基于签名泊松度下降（SignSGD）和无线计算（AirComp）的层次投票框架，以解决分布式学习中的Byzantine攻击和无线环境的影响。</li>
<li>results: 研究人员通过分析和验证了该框架在Byzantine攻击和无线环境下的性能，并证明了其在分布式学习中的稳定性和可靠性。<details>
<summary>Abstract</summary>
Distributed learning has become a promising computational parallelism paradigm that enables a wide scope of intelligent applications from the Internet of Things (IoT) to autonomous driving and the healthcare industry. This paper studies distributed learning in wireless data center networks, which contain a central edge server and multiple edge workers to collaboratively train a shared global model and benefit from parallel computing. However, the distributed nature causes the vulnerability of the learning process to faults and adversarial attacks from Byzantine edge workers, as well as the severe communication and computation overhead induced by the periodical information exchange process. To achieve fast and reliable model aggregation in the presence of Byzantine attacks, we develop a signed stochastic gradient descent (SignSGD)-based Hierarchical Vote framework via over-the-air computation (AirComp), where one voting process is performed locally at the wireless edge by taking advantage of Bernoulli coding while the other is operated over-the-air at the central edge server by utilizing the waveform superposition property of the multiple-access channels. We comprehensively analyze the proposed framework on the impacts including Byzantine attacks and the wireless environment (channel fading and receiver noise), followed by characterizing the convergence behavior under non-convex settings. Simulation results validate our theoretical achievements and demonstrate the robustness of our proposed framework in the presence of Byzantine attacks and receiver noise.
</details>
<details>
<summary>摘要</summary>
分布式学习已成为智能应用领域的扩展 Computational parallelism 方法之一，从互联网东西 (IoT) 到自动驾驶和医疗行业。这篇论文研究了无线数据中心网络中的分布式学习，该网络包括中央边缘服务器和多个边缘工作者，共同训练共享全球模型，并且从并行计算中受益。然而，分布式结构导致学习过程中的容易受到故障和恶意攻击，以及由 periodic 信息交换过程引起的严重通信和计算开销。为了在存在恶意攻击情况下实现快速和可靠的模型聚合，我们提出了基于签名随机梯度下降 (SignSGD) 的层次投票框架，该框架通过 wireless 边缘上进行本地 Bernoulli 编码，而在中央边缘服务器上通过多ступChannel 的波形重叠性特性进行无线计算。我们系统分析了提议的框架，包括恶意攻击和无线环境（通道抑降和接收噪声）的影响，然后对非拟合情况进行分析。实验结果证明我们的理论成果，并在存在恶意攻击和接收噪声情况下展示了我们的提议框架的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Parallel-Log-Spectra-index-PaLOSi-a-quality-metric-in-large-scale-resting-EEG-preprocessing"><a href="#Parallel-Log-Spectra-index-PaLOSi-a-quality-metric-in-large-scale-resting-EEG-preprocessing" class="headerlink" title="Parallel Log Spectra index (PaLOSi): a quality metric in large scale resting EEG preprocessing"></a>Parallel Log Spectra index (PaLOSi): a quality metric in large scale resting EEG preprocessing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11994">http://arxiv.org/abs/2310.11994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiang Hu, Jie Ruan, Nicolas Langer, Jorge Bosch-Bayard, Zhao Lv, Dezhong Yao, Pedro Antonio Valdes-Sosa</li>
<li>for: 这 paper 的目的是提出一种 novel data quality metric，以帮助大规模自动预处理中控制质量。</li>
<li>methods: 这 paper 使用了 Independent Component Analysis (ICA) 作为预处理步骤的基础，并对 PaLOS 现象进行了仔细的观察和分析。</li>
<li>results: 这 paper 的结果表明，PaLOS 存在可能导致不正确的连接分析结果，并且提出了一种基于 common principal component analysis 的 PaLOS index (PaLOSi)，可以检测 PaLOS 的存在。PaLOSi 的性能在 30094 个 EEG 数据集上进行了测试，结果显示 PaLOSi 可以检测不正确的预处理结果，并且具有较好的Robustness。<details>
<summary>Abstract</summary>
Toward large scale electrophysiology data analysis, many preprocessing pipelines are developed to reject artifacts as the prerequisite step before the downstream analysis. A mainstay of these pipelines is based on the data driven approach -- Independent Component Analysis (ICA). Nevertheless, there is little effort put to the preprocessing quality control. In this paper, attentions to this issue were carefully paid by our observation that after running ICA based preprocessing pipeline: some subjects showed approximately Parallel multichannel Log power Spectra (PaLOS), namely, multichannel power spectra are proportional to each other. Firstly, the presence of PaLOS and its implications to connectivity analysis were described by real instance and simulation; secondly, we built its mathematical model and proposed the PaLOS index (PaLOSi) based on the common principal component analysis to detect its presence; thirdly, the performance of PaLOSi was tested on 30094 cases of EEG from 5 databases. The results showed that 1) the PaLOS implies a sole source which is physiologically implausible. 2) PaLOSi can detect the excessive elimination of brain components and is robust in terms of channel number, electrode layout, reference, and the other factors. 3) PaLOSi can output the channel and frequency wise index to help for in-depth check. This paper presented the PaLOS issue in the quality control step after running the preprocessing pipeline and the proposed PaLOSi may serve as a novel data quality metric in the large-scale automatic preprocessing.
</details>
<details>
<summary>摘要</summary>
大规模电physiology数据分析中，许多预处理管道被开发出来拒绝噪声作为下游分析的前提步骤。主流的预处理管道基于数据驱动方法---独立组件分析（ICA）。然而，对预处理质量控制的努力不多。在这篇论文中，我们仔细注意到，在运行基于ICA的预处理管道后，一些主体显示了相似的多通道峰谱特征（PaLOS），即多通道峰谱的强度相对彼此成比例。我们首先描述了PaLOS的存在和其对连接分析的影响，然后构建了其数学模型，并基于共同主成分分析提出了PaLOS指数（PaLOSi）来检测其存在。最后，我们测试了PaLOSi在5个数据库中的30094个EEG样本。结果表明：1）PaLOS存在唯一的源，这是生物学上不可能的。2）PaLOSi可以检测预处理过程中的质量问题，并且在通道数、电极布局、参照、其他因素等方面具有稳定性。3）PaLOSi可以输出通道和频率 wise的指数，帮助进行深入的检查。本文描述了预处理管道后的质量控制步骤中PaLOS问题，并提出了PaLOSi作为大规模自动预处理中的新数据质量指标。
</details></li>
</ul>
<hr>
<h2 id="Supporting-UAVs-with-Edge-Computing-A-Review-of-Opportunities-and-Challenges"><a href="#Supporting-UAVs-with-Edge-Computing-A-Review-of-Opportunities-and-Challenges" class="headerlink" title="Supporting UAVs with Edge Computing: A Review of Opportunities and Challenges"></a>Supporting UAVs with Edge Computing: A Review of Opportunities and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11957">http://arxiv.org/abs/2310.11957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malte Janßen, Tobias Pfandzelter, Minghe Wang, David Bermbach</li>
<li>for: 本研究旨在分析现代无人机技术的发展，以及如何通过边缘计算提高无人机的任务完成速度、能效性和可靠性。</li>
<li>methods: 本研究采用系统性的文献综述方法，收集、选择和提取关键领域的研究成果，以探讨无人机活动如何通过边缘计算得到改进。</li>
<li>results: 研究发现，通过边缘计算可以提高无人机的任务完成速度、能效性和可靠性，并且可以应用于多个领域和行业。<details>
<summary>Abstract</summary>
Over the last years, Unmanned Aerial Vehicles (UAVs) have seen significant advancements in sensor capabilities and computational abilities, allowing for efficient autonomous navigation and visual tracking applications. However, the demand for computationally complex tasks has increased faster than advances in battery technology. This opens up possibilities for improvements using edge computing. In edge computing, edge servers can achieve lower latency responses compared to traditional cloud servers through strategic geographic deployments. Furthermore, these servers can maintain superior computational performance compared to UAVs, as they are not limited by battery constraints. Combining these technologies by aiding UAVs with edge servers, research finds measurable improvements in task completion speed, energy efficiency, and reliability across multiple applications and industries. This systematic literature review aims to analyze the current state of research and collect, select, and extract the key areas where UAV activities can be supported and improved through edge computing.
</details>
<details>
<summary>摘要</summary>
过去几年，无人飞行器（UAV）技术已经减少了很多，包括感知和计算能力等方面。这使得无人飞行器可以更加高效地进行自主导航和视觉跟踪应用。然而，计算复杂任务的需求增长 faster than 电池技术的进步，这开 up了可以通过边缘计算提高无人飞行器性能的可能性。在边缘计算中，边缘服务器可以在地理上投入策略的部署下实现更低的响应时间，相比于传统的云服务器。此外，这些服务器可以在无人飞行器上保持更高的计算性能，因为它们不受电池限制。通过将这些技术相结合，研究发现在多个应用和领域中，任务完成速度、能效性和可靠性都有所提高。这个系统性文献综述的目的是分析当前研究的状况，收集、选择和提取无人飞行器活动中可以通过边缘计算提高的关键领域。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Detection-on-RIS-Assisted-RSM-and-RSSK-Techniques"><a href="#Deep-Learning-Based-Detection-on-RIS-Assisted-RSM-and-RSSK-Techniques" class="headerlink" title="Deep Learning Based Detection on RIS Assisted RSM and RSSK Techniques"></a>Deep Learning Based Detection on RIS Assisted RSM and RSSK Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11924">http://arxiv.org/abs/2310.11924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Onur Salan, Ferhat Bayar, Hacı Ilhan, Erdogan Aydin</li>
<li>for: 本研究旨在探讨使用深度学习技术来对RIS帮助的接收SM&#x2F;RSSK系统进行检测，以实现spectral和能量效率的平衡。</li>
<li>methods: 本研究使用了深度学习的概念，特别是块深度神经网络（B-DNN），以提高RIS帮助的SM系统的检测性能。</li>
<li>results:  Monte Carlo simulate results show that B-DNN可以与最大可能性（ML）相比，并且在比较于匀速检测器（Greedy detector）的情况下，提供了更好的检测性能。<details>
<summary>Abstract</summary>
The reconfigurable intelligent surface (RIS) is considered a crucial technology for the future of wireless communication. Recently, there has been significant interest in combining RIS with spatial modulation (SM) or space shift keying (SSK) to achieve a balance between spectral and energy efficiency. In this paper, we have investigated the use of deep learning techniques for detection in RIS-aided received SM (RSM)/received-SSK (RSSK) systems over Weibull fading channels, specifically by extending the RIS-aided SM/SSK system to a specific case of the conventional SM system. By employing the concept of neural networks, the study focuses on model-driven deep learning detection namely block deep neural networks (B-DNN) for RIS-aided SM systems and compares its performance against maximum likelihood (ML) and greedy detectors. Finally, it has been demonstrated by Monte Carlo simulation that while B-DNN achieved a bit error rate (BER) performance close to that of ML, it gave better results than the Greedy detector.
</details>
<details>
<summary>摘要</summary>
“弹性智能表面”（RIS）被视为未来无线通信技术的重要一环。近期，有许多研究将RIS与空间变化（SM）或空间移动键（SSK）结合以实现频率和能源效率的平衡。本研究使用深度学习技术进行RIS-aided SM/RSSK系统中的检测，具体是将传统SM系统扩展到RIS-aided SM系统。通过使用神经网络的概念，本研究专注于使用堆层神经网络（B-DNN）进行检测，并与最大可能性（ML）和探测器进行比较。最后，通过 Monte Carlo 模拟，发现B-DNN对比于ML的比较好，并且在比较探测器时表现更好。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Resource-Management-in-Integrated-NOMA-Terrestrial-Satellite-Networks-using-Multi-Agent-Reinforcement-Learning"><a href="#Dynamic-Resource-Management-in-Integrated-NOMA-Terrestrial-Satellite-Networks-using-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Dynamic Resource Management in Integrated NOMA Terrestrial-Satellite Networks using Multi-Agent Reinforcement Learning"></a>Dynamic Resource Management in Integrated NOMA Terrestrial-Satellite Networks using Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11814">http://arxiv.org/abs/2310.11814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Nauman, Haya Mesfer Alshahrani, Nadhem Nemri, Kamal M. Othman, Nojood O Aljehane, Mashael Maashi, Ashit Kumar Dutta, Mohammed Assiri, Wali Ullah Khan</li>
<li>for: 这个研究旨在提出一个集成卫星-地面网络资源分配框架，以解决这些挑战。</li>
<li>methods: 该框架利用本地缓存池部署和非对称多Access（NOMA）技术来减少时间延迟和提高能效率。</li>
<li>results: 我们的提议使用了多代理搜索深度决策函数算法（MADDPG）优化用户关联、缓存设计和传输功率控制，从而提高能效率。Here’s the breakdown of each point in English:1. for: The paper is written to address the challenges of integrated satellite-terrestrial networks.2. methods: The paper proposes a resource allocation framework that leverages local cache pool deployments and non-orthogonal multiple access (NOMA) to reduce time delays and improve energy efficiency.3. results: The proposed approach using a multi-agent enabled deep deterministic policy gradient algorithm (MADDPG) achieves significantly higher energy efficiency and reduced time delays compared to existing methods.<details>
<summary>Abstract</summary>
This study introduces a resource allocation framework for integrated satellite-terrestrial networks to address these challenges. The framework leverages local cache pool deployments and non-orthogonal multiple access (NOMA) to reduce time delays and improve energy efficiency. Our proposed approach utilizes a multi-agent enabled deep deterministic policy gradient algorithm (MADDPG) to optimize user association, cache design, and transmission power control, resulting in enhanced energy efficiency. The approach comprises two phases: User Association and Power Control, where users are treated as agents, and Cache Optimization, where the satellite (Bs) is considered the agent. Through extensive simulations, we demonstrate that our approach surpasses conventional single-agent deep reinforcement learning algorithms in addressing cache design and resource allocation challenges in integrated terrestrial-satellite networks. Specifically, our proposed approach achieves significantly higher energy efficiency and reduced time delays compared to existing methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Random-Sampling-of-Bandlimited-Graph-Signals-from-Local-Measurements"><a href="#Random-Sampling-of-Bandlimited-Graph-Signals-from-Local-Measurements" class="headerlink" title="Random Sampling of Bandlimited Graph Signals from Local Measurements"></a>Random Sampling of Bandlimited Graph Signals from Local Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11692">http://arxiv.org/abs/2310.11692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lili Shen, Jun Xian, Cheng Cheng</li>
<li>for: 这篇论文研究了图像信号处理中随机抽样的问题，具体来说是对k-带限信号从本地测量中进行随机抽样，并证明了只需要O(klogk)次测量来精确和稳定地重建任何k-带限图像信号。</li>
<li>methods: 该论文提出了两种随机抽样策略，即最优抽样和估计抽样，其中采用了基于最小测量的抽样probability分布。</li>
<li>results:  numerical experiments表明了该方法的效果。<details>
<summary>Abstract</summary>
The random sampling on graph signals is one of the fundamental topics in graph signal processing. In this letter, we consider the random sampling of k-bandlimited signals from the local measurements and show that no more than O(klogk) measurements with replacement are sufficient for the accurate and stable recovery of any k-bandlimited graph signals. We propose two random sampling strategies based on the minimum measurements, i.e., the optimal sampling and the estimated sampling. The geodesic distance between vertices is introduced to design the sampling probability distribution. Numerical experiments are included to show the effectiveness of the proposed methods.
</details>
<details>
<summary>摘要</summary>
《随机抽取图像信号处理》是图像信号处理领域的基本主题之一。在本信，我们考虑了基于地方测量的随机抽取k-带限信号，并证明了只需要O(klogk)次抽取 measurements with replacement 可以准确地重建任何k-带限图像信号。我们提出了两种基于最小测量的随机抽取策略，即最优抽取和估计抽取。我们通过地odesic distance between vertices来设计抽取概率分布。numerical experiments 表明我们提出的方法的效果。Here's the word-for-word translation:“随机抽取图像信号处理”是图像信号处理领域的基本主题之一。在本信，我们考虑了基于地方测量的随机抽取k-带限信号，并证明了只需要O(klogk)次抽取 measurements with replacement 可以准确地重建任何k-带限图像信号。我们提出了两种基于最小测量的随机抽取策略，即最佳抽取和估计抽取。我们通过地odesic distance between vertices来设计抽取概率分布。numerical experiments 表明我们提出的方法的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/eess.SP_2023_10_18/" data-id="closbrp0g01c80g88em3a52v5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/cs.SD_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T15:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/cs.SD_2023_10_17/">cs.SD - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="High-Fidelity-Noise-Reduction-with-Differentiable-Signal-Processing"><a href="#High-Fidelity-Noise-Reduction-with-Differentiable-Signal-Processing" class="headerlink" title="High-Fidelity Noise Reduction with Differentiable Signal Processing"></a>High-Fidelity Noise Reduction with Differentiable Signal Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11364">http://arxiv.org/abs/2310.11364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian J. Steinmetz, Thomas Walther, Joshua D. Reiss</li>
<li>for: 这个论文主要是为了提高录音中的听音质量，使用深度学习和信号处理技术。</li>
<li>methods: 这个论文使用了深度学习模型和信号处理技术，将其结合起来实现自动化的听音质量提高。</li>
<li>results: 论文的实验结果表明，使用这种方法可以实现高精度的听音质量提高，并且比深度学习模型更高效和更少噪声。Listening examples are available online at <a target="_blank" rel="noopener" href="https://tape.it/research/denoiser%E3%80%82">https://tape.it/research/denoiser。</a><details>
<summary>Abstract</summary>
Noise reduction techniques based on deep learning have demonstrated impressive performance in enhancing the overall quality of recorded speech. While these approaches are highly performant, their application in audio engineering can be limited due to a number of factors. These include operation only on speech without support for music, lack of real-time capability, lack of interpretable control parameters, operation at lower sample rates, and a tendency to introduce artifacts. On the other hand, signal processing-based noise reduction algorithms offer fine-grained control and operation on a broad range of content, however, they often require manual operation to achieve the best results. To address the limitations of both approaches, in this work we introduce a method that leverages a signal processing-based denoiser that when combined with a neural network controller, enables fully automatic and high-fidelity noise reduction on both speech and music signals. We evaluate our proposed method with objective metrics and a perceptual listening test. Our evaluation reveals that speech enhancement models can be extended to music, however training the model to remove only stationary noise is critical. Furthermore, our proposed approach achieves performance on par with the deep learning models, while being significantly more efficient and introducing fewer artifacts in some cases. Listening examples are available online at https://tape.it/research/denoiser .
</details>
<details>
<summary>摘要</summary>
“深度学习减声技术已经在录音质量提高方面表现出色。然而，这些方法在音频工程中的应用可能受到一些限制因素。这些因素包括仅适用于语音，无法支持音乐，缺乏实时功能，缺乏可解释的控制参数，运行在较低的� Sampling rate 下，并且会引入错误。另一方面，信号处理减声算法可以提供精确的控制和适用于广泛的内容，但是它们通常需要手动操作以 дости持最佳结果。为了解决这两种方法的限制，在这个工作中，我们提出了一种结合信号处理减声器和神经网络控制器的方法，允许完全自动和高精度的杂声除去，包括语音和音乐信号。我们使用了一系列的对照测试和听觉测试进行评估。我们发现，语音提高模型可以扩展到音乐，但是培训模型只需要去除静止杂声是critical。此外，我们的提案方法可以和深度学习模型的性能相似，同时更高效和更少的错误。听取示例可以在 https://tape.it/research/denoiser 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Content-based-Features-from-Multiple-Acoustic-Models-for-Singing-Voice-Conversion"><a href="#Leveraging-Content-based-Features-from-Multiple-Acoustic-Models-for-Singing-Voice-Conversion" class="headerlink" title="Leveraging Content-based Features from Multiple Acoustic Models for Singing Voice Conversion"></a>Leveraging Content-based Features from Multiple Acoustic Models for Singing Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11160">http://arxiv.org/abs/2310.11160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueyao Zhang, Yicheng Gu, Haopeng Chen, Zihao Fang, Lexiao Zou, Liumeng Xue, Zhizheng Wu</li>
<li>for: 这项研究旨在 Investigating the complementary roles of multiple content features in singing voice conversion (SVC), and developing a diffusion-based SVC model that integrates these features for superior conversion performance.</li>
<li>methods: 研究使用了三种不同的内容特征（来自WeNet、Whisper和ContentVec），并将其集成到一个扩散基于SVC模型中，以提高SVC的对象和主观评估表现。</li>
<li>results: 研究表明，通过将多种内容特征集成到SVC模型中，可以获得更高的对象和主观评估表现，比单个内容特征更好。Code和demo页面可以在<a target="_blank" rel="noopener" href="https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html中找到。</a><details>
<summary>Abstract</summary>
Singing voice conversion (SVC) is a technique to enable an arbitrary singer to sing an arbitrary song. To achieve that, it is important to obtain speaker-agnostic representations from source audio, which is a challenging task. A common solution is to extract content-based features (e.g., PPGs) from a pretrained acoustic model. However, the choices for acoustic models are vast and varied. It is yet to be explored what characteristics of content features from different acoustic models are, and whether integrating multiple content features can help each other. Motivated by that, this study investigates three distinct content features, sourcing from WeNet, Whisper, and ContentVec, respectively. We explore their complementary roles in intelligibility, prosody, and conversion similarity for SVC. By integrating the multiple content features with a diffusion-based SVC model, our SVC system achieves superior conversion performance on both objective and subjective evaluation in comparison to a single source of content features. Our demo page and code can be available https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html.
</details>
<details>
<summary>摘要</summary>
《歌唱voice转换（SVC）技术可以让任意歌手演唱任意歌曲。实现这一点具有挑战性，因为需要从源音频中提取无关于歌手的特征。常见的解决方案是从预训练的音声模型中提取内容基于特征（如PPGs）。然而，不同的音声模型可以提供不同的内容特征，是否可以将这些特征集成起来帮助彼此？以上问题驱动我们进行这种研究。本研究研究了三种不同的内容特征，分别来自WeNet、Whisper和ContentVec。我们研究了这些特征在elligibility、prosody和转换相似性方面的补充作用，并将这些特征集成到一个扩散型SVC模型中。在对象和主观评价中，我们的SVC系统在比较于单个内容特征时表现出更高的转换性能。我们的demo页和代码可以在<https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html>上查看。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="A-High-Fidelity-and-Low-Complexity-Neural-Audio-Coding"><a href="#A-High-Fidelity-and-Low-Complexity-Neural-Audio-Coding" class="headerlink" title="A High Fidelity and Low Complexity Neural Audio Coding"></a>A High Fidelity and Low Complexity Neural Audio Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10992">http://arxiv.org/abs/2310.10992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhe Liu, Wei Xiao, Meng Wang, Shan Yang, Yupeng Shi, Yuyong Kang, Dan Su, Shidong Shang, Dong Yu</li>
<li>for: 这个论文旨在提出一个整合架构，用于实时通信系统中的音频编码。</li>
<li>methods: 这个架构使用神经网络模型宽频域成分，并使用传统信号处理技术压缩高频域成分，根据人类听觉知识设计了根据听觉理论的损失函数，同时还提出了基于对抗学习的各种数据压缩方法。</li>
<li>results: 该方法与先进的神经音频编码相比，在主观和客观指标上均表现出色，并且可以在桌面和手持设备上进行实时推断。<details>
<summary>Abstract</summary>
Audio coding is an essential module in the real-time communication system. Neural audio codecs can compress audio samples with a low bitrate due to the strong modeling and generative capabilities of deep neural networks. To address the poor high-frequency expression and high computational cost and storage consumption, we proposed an integrated framework that utilizes a neural network to model wide-band components and adopts traditional signal processing to compress high-band components according to psychological hearing knowledge. Inspired by auditory perception theory, a perception-based loss function is designed to improve harmonic modeling. Besides, generative adversarial network (GAN) compression is proposed for the first time for neural audio codecs. Our method is superior to prior advanced neural codecs across subjective and objective metrics and allows real-time inference on desktop and mobile.
</details>
<details>
<summary>摘要</summary>
Audio coding是实时通信系统中的一个重要模块。神经网络音频编码器可以通过深度神经网络的强大模型和生成能力，将音频样本压缩到低比特率。为了解决高频表达质量不佳和计算成本高的问题，我们提出了一个整合框架，利用神经网络模型宽频成分，采用传统的信号处理技术压缩高频成分，根据听觉知识。受听觉理论的启发，我们设计了基于听觉模型的损失函数，以改善和声模型。此外，我们还提出了基于生成敌对网络（GAN）的压缩方法，这是神经音频编码器中的首次应用。我们的方法在主观和客观指标上胜过先前的先进神经编码器，并允许实时推理在桌面和移动设备上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/cs.SD_2023_10_17/" data-id="closbrouz00yg0g88a0gg185u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/eess.AS_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T14:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/eess.AS_2023_10_17/">eess.AS - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Iterative-Shallow-Fusion-of-Backward-Language-Model-for-End-to-End-Speech-Recognition"><a href="#Iterative-Shallow-Fusion-of-Backward-Language-Model-for-End-to-End-Speech-Recognition" class="headerlink" title="Iterative Shallow Fusion of Backward Language Model for End-to-End Speech Recognition"></a>Iterative Shallow Fusion of Backward Language Model for End-to-End Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11010">http://arxiv.org/abs/2310.11010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atsunori Ogawa, Takafumi Moriya, Naoyuki Kamo, Naohiro Tawara, Marc Delcroix</li>
<li>for: 这 paper 的目的是提出一种新的浅融合（SF）方法，以利用外部的反向语言模型（BLM）来实现端到端自动语音识别（ASR）系统。</li>
<li>methods: 这 paper 使用的方法包括：（1）使用 BLM 对 ASR  гипотезы进行迭代处理，以取代上一轮计算的 BLM 分数；（2）使用 PBLM 进行部分句子预测，以提高 ISF 的效果。</li>
<li>results: 实验结果表明，使用 ISF 可以在 ASR 系统中提高性能，并且可以避免在解码过程中提前剔除可能的 гипотезы。此外，将 SF 和 ISF 相互结合可以获得更高的性能提升。<details>
<summary>Abstract</summary>
We propose a new shallow fusion (SF) method to exploit an external backward language model (BLM) for end-to-end automatic speech recognition (ASR). The BLM has complementary characteristics with a forward language model (FLM), and the effectiveness of their combination has been confirmed by rescoring ASR hypotheses as post-processing. In the proposed SF, we iteratively apply the BLM to partial ASR hypotheses in the backward direction (i.e., from the possible next token to the start symbol) during decoding, substituting the newly calculated BLM scores for the scores calculated at the last iteration. To enhance the effectiveness of this iterative SF (ISF), we train a partial sentence-aware BLM (PBLM) using reversed text data including partial sentences, considering the framework of ISF. In experiments using an attention-based encoder-decoder ASR system, we confirmed that ISF using the PBLM shows comparable performance with SF using the FLM. By performing ISF, early pruning of prospective hypotheses can be prevented during decoding, and we can obtain a performance improvement compared to applying the PBLM as post-processing. Finally, we confirmed that, by combining SF and ISF, further performance improvement can be obtained thanks to the complementarity of the FLM and PBLM.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的浅合并（SF）方法，利用外部的反向语言模型（BLM）来实现端到端自动语音识别（ASR）。BLM具有与前向语言模型（FLM）的 complementary 特性，其合作效果已经通过重新评分ASR假设来确认。在我们的SF中，我们在解码过程中逐渐应用BLM于部分ASR假设，在反向方向（即从可能的下一个单词到开始符）进行迭代，并将每轮计算的BLM分数替换为上一轮计算的分数。为了增强ISF的效果，我们使用了倒转文本数据来训练一个具有部分句子意识的BLM（PBLM）。在使用了注意力基于encoder-decoder ASR系统的实验中，我们证明了ISF使用PBLM可以与SF使用FLM相比。通过执行ISF，在解码过程中可以避免早期淘汰可能的假设，从而获得性能提升。最后，我们证明了，通过将SF和ISF结合使用，可以增加性能的提升，这是因为FLM和PBLM之间存在 complementarity。
</details></li>
</ul>
<hr>
<h2 id="Advanced-accent-dialect-identification-and-accentedness-assessment-with-multi-embedding-models-and-automatic-speech-recognition"><a href="#Advanced-accent-dialect-identification-and-accentedness-assessment-with-multi-embedding-models-and-automatic-speech-recognition" class="headerlink" title="Advanced accent&#x2F;dialect identification and accentedness assessment with multi-embedding models and automatic speech recognition"></a>Advanced accent&#x2F;dialect identification and accentedness assessment with multi-embedding models and automatic speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11004">http://arxiv.org/abs/2310.11004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahram Ghorbani, John H. L. Hansen<br>for:* 这个研究旨在提高非Native语言speech中的腔调识别和外国腔评估的准确性。methods:* 利用先进的预训练语言标识(LID)和说话人标识(SID)模型的嵌入，以提高非Native语言speech中腔调识别和外国腔评估的准确性。results:* 结果表明，使用预训练LID和SID模型的嵌入可以有效地编码非Native语言speech中的腔调信息。* 此外，LID和SID编码的腔调信息与从scratch开发的端到端腔调识别模型结合使用，可以提高腔调识别的准确性。<details>
<summary>Abstract</summary>
Accurately classifying accents and assessing accentedness in non-native speakers are both challenging tasks due to the complexity and diversity of accent and dialect variations. In this study, embeddings from advanced pre-trained language identification (LID) and speaker identification (SID) models are leveraged to improve the accuracy of accent classification and non-native accentedness assessment. Findings demonstrate that employing pre-trained LID and SID models effectively encodes accent/dialect information in speech. Furthermore, the LID and SID encoded accent information complement an end-to-end accent identification (AID) model trained from scratch. By incorporating all three embeddings, the proposed multi-embedding AID system achieves superior accuracy in accent identification. Next, we investigate leveraging automatic speech recognition (ASR) and accent identification models to explore accentedness estimation. The ASR model is an end-to-end connectionist temporal classification (CTC) model trained exclusively with en-US utterances. The ASR error rate and en-US output of the AID model are leveraged as objective accentedness scores. Evaluation results demonstrate a strong correlation between the scores estimated by the two models. Additionally, a robust correlation between the objective accentedness scores and subjective scores based on human perception is demonstrated, providing evidence for the reliability and validity of utilizing AID-based and ASR-based systems for accentedness assessment in non-native speech.
</details>
<details>
<summary>摘要</summary>
准确地分类不同的口音和讲话风格是一项非常复杂和多样化的任务，特别是在非Native speaker的语音中。在本研究中，我们利用先进的预训练语言标识（LID）和发音标识（SID）模型的嵌入来提高非Native speaker的口音分类和讲话风格评估的准确性。研究发现，使用预训练LID和SID模型可以有效地嵌入语音中的口音/方言信息。此外，LID和SID嵌入的口音信息与从零开始训练的口音标识（AID）模型相结合，可以提高口音标识的准确性。然后，我们 investigate了利用自然语音识别（ASR）和口音标识模型来评估讲话风格。ASR模型是一个端到端的连接式时间分类（CTC）模型，专门使用英文语音训练。ASR错误率和AID模型的en-US输出被用作对象评估风格的标准差分。研究结果表明，两个模型之间存在强相关性，并且对人类对讲话风格的评估也存在robust相关性，这提供了使用AID和ASR基于的系统进行讲话风格评估的可靠性和有效性的证据。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/eess.AS_2023_10_17/" data-id="closbrowc01220g883kh2g5j7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/cs.CV_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T13:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/cs.CV_2023_10_17/">cs.CV - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Holistic-Parking-Slot-Detection-with-Polygon-Shaped-Representations"><a href="#Holistic-Parking-Slot-Detection-with-Polygon-Shaped-Representations" class="headerlink" title="Holistic Parking Slot Detection with Polygon-Shaped Representations"></a>Holistic Parking Slot Detection with Polygon-Shaped Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11629">http://arxiv.org/abs/2310.11629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lihao Wang, Antonyo Musabini, Christel Leonet, Rachid Benmokhtar, Amaury Breheret, Chaima Yedes, Fabian Burger, Thomas Boulay, Xavier Perrotton<br>for:This paper proposes a one-step Holistic Parking Slot Network (HPS-Net) to detect vacant parking slots using a camera-based approach.methods:The proposed method uses a tailor-made adaptation of the You Only Look Once (YOLO)v4 algorithm, which directly outputs the four vertex coordinates of the parking slot in topview domain. A novel regression loss function named polygon-corner Generalized Intersection over Union (GIoU) is proposed to optimize the polygon vertex position and distinguish the entrance line.results:Experiments show that HPS-Net can detect various vacant parking slots with a F1-score of 0.92 on the internal Valeo Parking Slots Dataset (VPSD) and 0.99 on the public dataset PS2.0. The method achieves a satisfying generalization and robustness in various parking scenarios, such as indoor or paved ground, with a real-time detection speed of 17 FPS on Nvidia Drive AGX Xavier.<details>
<summary>Abstract</summary>
Current parking slot detection in advanced driver-assistance systems (ADAS) primarily relies on ultrasonic sensors. This method has several limitations such as the need to scan the entire parking slot before detecting it, the incapacity of detecting multiple slots in a row, and the difficulty of classifying them. Due to the complex visual environment, vehicles are equipped with surround view camera systems to detect vacant parking slots. Previous research works in this field mostly use image-domain models to solve the problem. These two-stage approaches separate the 2D detection and 3D pose estimation steps using camera calibration. In this paper, we propose one-step Holistic Parking Slot Network (HPS-Net), a tailor-made adaptation of the You Only Look Once (YOLO)v4 algorithm. This camera-based approach directly outputs the four vertex coordinates of the parking slot in topview domain, instead of a bounding box in raw camera images. Several visible points and shapes can be proposed from different angles. A novel regression loss function named polygon-corner Generalized Intersection over Union (GIoU) for polygon vertex position optimization is also proposed to manage the slot orientation and to distinguish the entrance line. Experiments show that HPS-Net can detect various vacant parking slots with a F1-score of 0.92 on our internal Valeo Parking Slots Dataset (VPSD) and 0.99 on the public dataset PS2.0. It provides a satisfying generalization and robustness in various parking scenarios, such as indoor (F1: 0.86) or paved ground (F1: 0.91). Moreover, it achieves a real-time detection speed of 17 FPS on Nvidia Drive AGX Xavier. A demo video can be found at https://streamable.com/75j7sj.
</details>
<details>
<summary>摘要</summary>
当前的停车槽检测在高级驾驶支持系统（ADAS）主要依靠 ultrasonic 探测器。这种方法有多个限制，包括需要扫描整个停车槽才能检测它，无法检测多个槽在一行，以及困难的分类问题。由于视觉环境复杂，车辆通常配备卫星视频摄像头系统以检测空闲停车槽。先前的研究工作主要使用图像领域模型解决这个问题。这些两个阶段方法在摄像头准确性上分解了2D检测和3D姿态估计步骤。在本文中，我们提出了一步骤Holistic Parking Slot Network（HPS-Net），这是基于 You Only Look Once（YOLO）v4算法的修改版。这种摄像头基本方法直接输出了四个顶点坐标，而不是Raw摄像头图像中的 bounding box。通过不同的角度可以提出多个可见的点和形状。我们还提出了一种新的 regression loss function，即 polygon-corner Generalized Intersection over Union（GIoU），用于优化额外坐标和分辨入口线。实验表明，HPS-Net可以准确地检测多种空闲停车槽，F1 分数为 0.92 在我们的内部 valeo 停车槽数据集（VPSD）上，并达到 0.99 在公共数据集 PS2.0 上。它在多种停车场景中具有满意的总体化和稳定性，例如室内（F1: 0.86）或沥青地（F1: 0.91）。此外，它在 Nvidia Drive AGX Xavier 上实现了实时检测速度为 17 FPS。更多的详细信息可以在 <https://streamable.com/75j7sj> 找到。
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Building-and-Road-Detection-from-Sentinel-2"><a href="#High-Resolution-Building-and-Road-Detection-from-Sentinel-2" class="headerlink" title="High-Resolution Building and Road Detection from Sentinel-2"></a>High-Resolution Building and Road Detection from Sentinel-2</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11622">http://arxiv.org/abs/2310.11622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RudraxDave/UrbanizationDetection_RoadnBuilding">https://github.com/RudraxDave/UrbanizationDetection_RoadnBuilding</a></li>
<li>paper_authors: Wojciech Sirko, Emmanuel Asiedu Brempong, Juliana T. C. Marcos, Abigail Annkah, Abel Korme, Mohammed Alewi Hassen, Krishna Sapkota, Tomer Shekel, Abdoulaye Diack, Sella Nevo, Jason Hickey, John Quinn</li>
<li>for: This paper demonstrates how to use multiple 10m resolution Sentinel-2 images to generate 50cm resolution building and road segmentation masks.</li>
<li>methods: The authors use a “student” model to reproduce the predictions of a “teacher” model, which has access to high-resolution imagery.</li>
<li>results: The authors achieve 78.3% mIoU for building segmentation and R^2 &#x3D; 0.91 for counting individual buildings, which are comparable to the performance of the high-resolution teacher model.Here is the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文用多个10米分辨率的卫星图像生成50厘米分辨率的建筑和公路分割mask。</li>
<li>methods: 作者使用一个”学生”模型来重现一个”教师”模型的预测，该模型有访问高分辨率卫星图像的能力。</li>
<li>results: 作者在建筑分割方面 achievement 78.3% mIoU，与高分辨率教师模型的性能相似。同时，对个别建筑物的计数也达到R^2 &#x3D; 0.91。<details>
<summary>Abstract</summary>
Mapping buildings and roads automatically with remote sensing typically requires high-resolution imagery, which is expensive to obtain and often sparsely available. In this work we demonstrate how multiple 10 m resolution Sentinel-2 images can be used to generate 50 cm resolution building and road segmentation masks. This is done by training a `student' model with access to Sentinel-2 images to reproduce the predictions of a `teacher' model which has access to corresponding high-resolution imagery. While the predictions do not have all the fine detail of the teacher model, we find that we are able to retain much of the performance: for building segmentation we achieve 78.3% mIoU, compared to the high-resolution teacher model accuracy of 85.3% mIoU. We also describe a related method for counting individual buildings in a Sentinel-2 patch which achieves R^2 = 0.91 against true counts. This work opens up new possibilities for using freely available Sentinel-2 imagery for a range of tasks that previously could only be done with high-resolution satellite imagery.
</details>
<details>
<summary>摘要</summary>
使用远程感知自动地图建模通常需要高分辨率图像，这些图像昂贵并且 sparse 地available。在这项工作中，我们展示了如何使用多张10米分辨率 Sentine-2 图像生成 50 cm 分辨率的建筑和路径分割mask。这是通过训练一个“学生”模型，该模型有访问 Sentine-2 图像，来复制“教师”模型，该模型有访问相应的高分辨率图像的预测。虽然预测没有 teacher 模型的细节，但我们发现可以保留大量的性能：对建筑分割，我们 achieve 78.3% mIoU，比高分辨率教师模型的准确率 85.3% mIoU。我们还描述了一种与此相关的方法，可以在 Sentinel-2 质patch 中计数个建筑，该方法 achieve R^2 = 0.91 与真实计数的相关性。这项工作开 up 了使用免费available Sentinel-2 图像进行许多任务的新可能性，这些任务在过去只能通过高分辨率卫星图像进行。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Safety-Driver-Attention-During-Autonomous-Vehicle-Operation"><a href="#Classification-of-Safety-Driver-Attention-During-Autonomous-Vehicle-Operation" class="headerlink" title="Classification of Safety Driver Attention During Autonomous Vehicle Operation"></a>Classification of Safety Driver Attention During Autonomous Vehicle Operation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11608">http://arxiv.org/abs/2310.11608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Gerling Konrad, Julie Stephany Berrio, Mao Shan, Favio Masson, Stewart Worrall</li>
<li>For: This paper aims to develop a system to monitor the alertness of vehicle operators in ADAS and AVs to ensure safe operation.* Methods: The proposed system uses an infrared camera to detect the driver’s head and calculate head orientation, and incorporates environmental data from the perception system to determine the driver’s attention to objects in the surroundings.* Results: The system was tested using data collected in Sydney, Australia, and was found to effectively determine the vehicle operator’s attention levels, enabling interventions such as warnings or reducing autonomous functionality as appropriate.Here is the same information in Simplified Chinese text:* For: 这篇论文目标是为ADAS和AV开发一种监测车辆操作员的关注度，以确保安全操作。* Methods: 该系统使用红外摄像头探测司机头部，计算头部方向，并在环境感知系统的数据支持下，判断司机是否注意到周围环境中的物体。* Results: 该系统在澳大利亚悉尼的数据集上进行了测试，并被证明可以有效地确定车辆操作员的关注度，并发出警示或减少自动化功能等措施。<details>
<summary>Abstract</summary>
Despite the continual advances in Advanced Driver Assistance Systems (ADAS) and the development of high-level autonomous vehicles (AV), there is a general consensus that for the short to medium term, there is a requirement for a human supervisor to handle the edge cases that inevitably arise. Given this requirement, it is essential that the state of the vehicle operator is monitored to ensure they are contributing to the vehicle's safe operation. This paper introduces a dual-source approach integrating data from an infrared camera facing the vehicle operator and vehicle perception systems to produce a metric for driver alertness in order to promote and ensure safe operator behaviour. The infrared camera detects the driver's head, enabling the calculation of head orientation, which is relevant as the head typically moves according to the individual's focus of attention. By incorporating environmental data from the perception system, it becomes possible to determine whether the vehicle operator observes objects in the surroundings. Experiments were conducted using data collected in Sydney, Australia, simulating AV operations in an urban environment. Our results demonstrate that the proposed system effectively determines a metric for the attention levels of the vehicle operator, enabling interventions such as warnings or reducing autonomous functionality as appropriate. This comprehensive solution shows promise in contributing to ADAS and AVs' overall safety and efficiency in a real-world setting.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:尽管现代驾驶助手系统（ADAS）和高级无人驾驶车辆（AV）在不断发展，但是有一致的共识，在短到中期，需要有人监督处理边缘情况。由于这一点，监控车辆运行者的状态非常重要。这篇文章介绍了一种将 инфракра力相机和车辆感知系统相结合的双源方法，以计算驾驶员注意力水平，以便促进和确保安全的操作行为。这种方法可以检测驾驶员的头部，并计算头部的方向，因为头部通常会根据个人的注意力方向移动。通过与环境数据集成，可以判断车辆运行者是否观察了周围环境。我们在悉尼、澳大利亚进行了实验，使用了在城市环境中采集的数据，模拟了AV的运行。我们的结果表明，该系统可以有效地计算驾驶员注意力水平，并发出警告或降低自动化功能，以适应实际情况。这种全面的解决方案显示了它在ADAS和AV的安全和效率方面的承诺。
</details></li>
</ul>
<hr>
<h2 id="DIAR-Deep-Image-Alignment-and-Reconstruction-using-Swin-Transformers"><a href="#DIAR-Deep-Image-Alignment-and-Reconstruction-using-Swin-Transformers" class="headerlink" title="DIAR: Deep Image Alignment and Reconstruction using Swin Transformers"></a>DIAR: Deep Image Alignment and Reconstruction using Swin Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11605">http://arxiv.org/abs/2310.11605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Kwiatkowski, Simon Matern, Olaf Hellwich</li>
<li>for: 这个论文旨在建立一个深度学习管道，用于同时对扭曲图像进行Alignment和重建。</li>
<li>methods: 论文使用了Swin transformer模型，并使用了自定义的图像扭曲 dataset，以及图像特征地图来检测图像内容和排除噪声。</li>
<li>results: 论文通过使用图像特征地图和深度学习模型，实现了对扭曲图像的高精度Alignment和重建。<details>
<summary>Abstract</summary>
When taking images of some occluded content, one is often faced with the problem that every individual image frame contains unwanted artifacts, but a collection of images contains all relevant information if properly aligned and aggregated. In this paper, we attempt to build a deep learning pipeline that simultaneously aligns a sequence of distorted images and reconstructs them. We create a dataset that contains images with image distortions, such as lighting, specularities, shadows, and occlusion. We create perspective distortions with corresponding ground-truth homographies as labels. We use our dataset to train Swin transformer models to analyze sequential image data. The attention maps enable the model to detect relevant image content and differentiate it from outliers and artifacts. We further explore using neural feature maps as alternatives to classical key point detectors. The feature maps of trained convolutional layers provide dense image descriptors that can be used to find point correspondences between images. We utilize this to compute coarse image alignments and explore its limitations.
</details>
<details>
<summary>摘要</summary>
当拍摄部分受遮挡内容时，常常会遇到每帧图像中存在不必要的artefacts问题，但是一系列图像中会包含所有相关信息，如果正确地对齐和汇总。在这篇论文中，我们尝试建立一个深度学习管道，同时对扭曲图像序列进行对齐和重建。我们创建了一个包含图像扭曲效果，如光照、 Specularities、阴影和遮挡的 dataset。我们创建了对应的地平线扭曲和真实的homographies作为标签。我们使用这些 dataset 来训练 Swin transformer 模型分析序列图像数据。模型的注意力地图使得模型可以检测图像中相关的内容，并将其与异常值和artefacts分开。我们进一步 explore 使用神经网络特征图作为经典关键点检测器的代替。训练 convolutional 层的神经网络特征图提供了密集图像描述符，可以用于找到图像之间的点对应关系。我们利用这个技术来计算粗略图像对齐，并探讨其局限性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Neural-Implicit-through-Volume-Rendering-with-Attentive-Depth-Fusion-Priors"><a href="#Learning-Neural-Implicit-through-Volume-Rendering-with-Attentive-Depth-Fusion-Priors" class="headerlink" title="Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors"></a>Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11598">http://arxiv.org/abs/2310.11598</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MachinePerceptionLab/Attentive_DFPrior">https://github.com/MachinePerceptionLab/Attentive_DFPrior</a></li>
<li>paper_authors: Pengchong Hu, Zhizhong Han</li>
<li>for: 本研究旨在提高基于多视图图像的3D重建精度，使用神经网络学习隐式表示，并通过粘性深度融合优先来解决 incomplete depth at holes 和 occluded structures 问题。</li>
<li>methods: 我们提出了一种基于多视图RGBD图像的神经网络学习隐式表示方法，通过粘性深度融合优先来解决 incomplete depth at holes 和 occluded structures 问题。我们引入了一种novel attention mechanism，允许神经网络直接使用 depth fusion prior 来学习隐式函数。</li>
<li>results: 我们的方法在 widely used benchmarks 上表现出色，超过了 latest neural implicit methods。我们的实验结果表明，我们的方法可以更好地处理 incomplete depth at holes 和 occluded structures 问题，提高了3D重建精度。<details>
<summary>Abstract</summary>
Learning neural implicit representations has achieved remarkable performance in 3D reconstruction from multi-view images. Current methods use volume rendering to render implicit representations into either RGB or depth images that are supervised by multi-view ground truth. However, rendering a view each time suffers from incomplete depth at holes and unawareness of occluded structures from the depth supervision, which severely affects the accuracy of geometry inference via volume rendering. To resolve this issue, we propose to learn neural implicit representations from multi-view RGBD images through volume rendering with an attentive depth fusion prior. Our prior allows neural networks to perceive coarse 3D structures from the Truncated Signed Distance Function (TSDF) fused from all depth images available for rendering. The TSDF enables accessing the missing depth at holes on one depth image and the occluded parts that are invisible from the current view. By introducing a novel attention mechanism, we allow neural networks to directly use the depth fusion prior with the inferred occupancy as the learned implicit function. Our attention mechanism works with either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene in the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on widely used benchmarks including synthetic and real-world scans show our superiority over the latest neural implicit methods. Project page: https://machineperceptionlab.github.io/Attentive_DF_Prior/
</details>
<details>
<summary>摘要</summary>
学习神经隐式表示法已经在多视图图像中实现了很好的3D重建性能。现有的方法使用volume rendering将隐式表示渲染成RGB或深度图像，并且通过多视图ground truth进行超视图指导。然而，每次渲染一个视图会导致部分深度信息缺失和 occluded 结构的不可见性，这会对geometry inference via volume rendering产生严重的影响。为解决这个问题，我们提议通过多视图RGBD图像学习神经隐式表示法，并通过注意力机制将神经网络让掌握TSDF（Truncated Signed Distance Function）的粗略3D结构。TSDF允许访问多视图图像中缺失的深度信息和 occluded 结构，从而提高geometry inference的准确性。我们的注意力机制可以同时使用一次拼接的TSDF或者逐渐拼接的TSDF，它们在Simultaneous Localization and Mapping（SLAM）上下文中表示整个场景或者部分场景。我们的评估结果表明，我们的方法在广泛使用的标准benchmark上表现出色，超过了最新的神经隐式方法。项目页面：https://machineperceptionlab.github.io/Attentive_DF_Prior/
</details></li>
</ul>
<hr>
<h2 id="Studying-the-Effects-of-Sex-related-Differences-on-Brain-Age-Prediction-using-brain-MR-Imaging"><a href="#Studying-the-Effects-of-Sex-related-Differences-on-Brain-Age-Prediction-using-brain-MR-Imaging" class="headerlink" title="Studying the Effects of Sex-related Differences on Brain Age Prediction using brain MR Imaging"></a>Studying the Effects of Sex-related Differences on Brain Age Prediction using brain MR Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11577">http://arxiv.org/abs/2310.11577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Dibaji, Neha Gianchandani, Akhil Nair, Mansi Singhal, Roberto Souza, Mariana Bento</li>
<li>for: 这个论文的目的是研究机器学习模型在不同性别人群中的偏见和公平问题。</li>
<li>methods: 作者使用了基于大脑磁共振成像数据的机器学习模型，并在不同的实验设计下进行了训练和评估。</li>
<li>results: 研究发现在不同性别人群和数据集上训练模型时，模型的性能存在差异，并且偏见可能导致模型在不同性别人群中的决策不具有公平性。<details>
<summary>Abstract</summary>
While utilizing machine learning models, one of the most crucial aspects is how bias and fairness affect model outcomes for diverse demographics. This becomes especially relevant in the context of machine learning for medical imaging applications as these models are increasingly being used for diagnosis and treatment planning. In this paper, we study biases related to sex when developing a machine learning model based on brain magnetic resonance images (MRI). We investigate the effects of sex by performing brain age prediction considering different experimental designs: model trained using only female subjects, only male subjects and a balanced dataset. We also perform evaluation on multiple MRI datasets (Calgary-Campinas(CC359) and CamCAN) to assess the generalization capability of the proposed models. We found disparities in the performance of brain age prediction models when trained on distinct sex subgroups and datasets, in both final predictions and decision making (assessed using interpretability models). Our results demonstrated variations in model generalizability across sex-specific subgroups, suggesting potential biases in models trained on unbalanced datasets. This underlines the critical role of careful experimental design in generating fair and reliable outcomes.
</details>
<details>
<summary>摘要</summary>
在使用机器学习模型时，最重要的一点是如何处理偏见和公平问题，以确保模型对不同的民族团体都有正确的预测结果。在医疗领域机器学习应用中，这些模型正在被用于诊断和治疗规划。在这篇论文中，我们研究了基于大脑磁共振成像（MRI）的机器学习模型中的性偏见。我们对不同实验设计进行了研究：使用只有女性参与者的模型、只有男性参与者的模型以及平衡 dataset。我们还对多个 MRI 数据集（Calgary-Campinas（CC359）和 CamCAN）进行了评估，以评估提议的模型在不同的数据集上的普适性。我们发现在不同的性 subgroup 和数据集上训练的模型表现出了差异， both final predictions 和决策（使用可解释模型进行评估）。我们的结果表明了不同的性团体中模型的一致性不同，这表明了训练在不均衡数据集上的模型可能存在偏见。这些结果提醒我们在设计实验时应该非常小心，以确保获得公平和可靠的结果。
</details></li>
</ul>
<hr>
<h2 id="Learning-Lens-Blur-Fields"><a href="#Learning-Lens-Blur-Fields" class="headerlink" title="Learning Lens Blur Fields"></a>Learning Lens Blur Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11535">http://arxiv.org/abs/2310.11535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esther Y. H. Lin, Zhecheng Wang, Rebecca Lin, Daniel Miau, Florian Kainz, Jiawen Chen, Xuaner Cecilia Zhang, David B. Lindell, Kiriakos N. Kutulakos</li>
<li>for: The paper is written to address the challenge of modeling optical blur in modern cameras with complex optical elements, and to introduce a high-dimensional neural representation of the blur field.</li>
<li>methods: The paper proposes a practical method for acquiring the lens blur field, which is a multilayer perceptron (MLP) designed to capture variations of the lens 2D point spread function over image plane location, focus setting, and depth. The representation models the combined effects of defocus, diffraction, aberration, and accounts for sensor features such as pixel color filters and pixel-specific micro-lenses.</li>
<li>results: The paper shows that the acquired 5D blur fields are expressive and accurate enough to reveal differences in optical behavior of smartphone devices of the same make and model, and provides a first-of-its-kind dataset of 5D blur fields for smartphone cameras, camera bodies equipped with a variety of lenses, etc.<details>
<summary>Abstract</summary>
Optical blur is an inherent property of any lens system and is challenging to model in modern cameras because of their complex optical elements. To tackle this challenge, we introduce a high-dimensional neural representation of blur$-$$\textit{the lens blur field}$$-$and a practical method for acquiring it. The lens blur field is a multilayer perceptron (MLP) designed to (1) accurately capture variations of the lens 2D point spread function over image plane location, focus setting and, optionally, depth and (2) represent these variations parametrically as a single, sensor-specific function. The representation models the combined effects of defocus, diffraction, aberration, and accounts for sensor features such as pixel color filters and pixel-specific micro-lenses. To learn the real-world blur field of a given device, we formulate a generalized non-blind deconvolution problem that directly optimizes the MLP weights using a small set of focal stacks as the only input. We also provide a first-of-its-kind dataset of 5D blur fields$-$for smartphone cameras, camera bodies equipped with a variety of lenses, etc. Lastly, we show that acquired 5D blur fields are expressive and accurate enough to reveal, for the first time, differences in optical behavior of smartphone devices of the same make and model.
</details>
<details>
<summary>摘要</summary>
“光学朦胧是任何镜系统的自然属性，现代摄像机中模型它具有复杂的光学元件的挑战。为了解决这个挑战，我们介绍了一种高维度神经网络表示朦胧$-$镜片朦胧场$-$以及一种实用的获取方法。镜片朦胧场是一种多层感知网络（MLP），旨在：(1) 精确捕捉镜片2D点扩散函数在图像平面位置、焦距设置和（选择）深度上的变化，以及(2) 表示这些变化为单个、设备特定的函数。该表示模型了杂光、折射、笛卡尔等效应，并考虑了感器特性 such as 像素色滤和像素特定的微镜。为了学习具体的朦胧场，我们提出了一种通用非盲目分解问题，直接优化MLP参量使用一小组焦距栈作为输入。此外，我们还提供了一个具有5D朦胧场的首个数据集$-$包括智能手机摄像机、配备多种镜头的相机机身等。最后，我们证明了获取的5D朦胧场是具有表达力和准确性的，可以折衣出智能手机设备的同类型和型号之间的光学行为差异。”
</details></li>
</ul>
<hr>
<h2 id="GenEval-An-Object-Focused-Framework-for-Evaluating-Text-to-Image-Alignment"><a href="#GenEval-An-Object-Focused-Framework-for-Evaluating-Text-to-Image-Alignment" class="headerlink" title="GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment"></a>GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11513">http://arxiv.org/abs/2310.11513</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djghosh13/geneval">https://github.com/djghosh13/geneval</a></li>
<li>paper_authors: Dhruba Ghosh, Hanna Hajishirzi, Ludwig Schmidt</li>
<li>for: 评估文本到图像生成模型的详细特性，如物体共处、位置、数量和颜色等。</li>
<li>methods: 利用现有的 объек检测模型来评估文本到图像模型的多种生成任务，并通过链接其他探索视觉模型来进一步验证特性。</li>
<li>results: 研究发现，现有的文本到图像模型在这些任务上已经显示出了显著的进步，但还缺乏复杂的能力，如空间关系和属性绑定。<details>
<summary>Abstract</summary>
Recent breakthroughs in diffusion models, multimodal pretraining, and efficient finetuning have led to an explosion of text-to-image generative models. Given human evaluation is expensive and difficult to scale, automated methods are critical for evaluating the increasingly large number of new models. However, most current automated evaluation metrics like FID or CLIPScore only offer a holistic measure of image quality or image-text alignment, and are unsuited for fine-grained or instance-level analysis. In this paper, we introduce GenEval, an object-focused framework to evaluate compositional image properties such as object co-occurrence, position, count, and color. We show that current object detection models can be leveraged to evaluate text-to-image models on a variety of generation tasks with strong human agreement, and that other discriminative vision models can be linked to this pipeline to further verify properties like object color. We then evaluate several open-source text-to-image models and analyze their relative generative capabilities on our benchmark. We find that recent models demonstrate significant improvement on these tasks, though they are still lacking in complex capabilities such as spatial relations and attribute binding. Finally, we demonstrate how GenEval might be used to help discover existing failure modes, in order to inform development of the next generation of text-to-image models. Our code to run the GenEval framework is publicly available at https://github.com/djghosh13/geneval.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DELIFFAS-Deformable-Light-Fields-for-Fast-Avatar-Synthesis"><a href="#DELIFFAS-Deformable-Light-Fields-for-Fast-Avatar-Synthesis" class="headerlink" title="DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis"></a>DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11449">http://arxiv.org/abs/2310.11449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youngjoong Kwon, Lingjie Liu, Henry Fuchs, Marc Habermann, Christian Theobalt</li>
<li>for: 生成高品质的数字人像，解决长期存在的图形和视觉领域中的问题。</li>
<li>methods: 提出了一种新的方法，即DELIFFAS，它将人体的外观 Parameterized as a surface light field that is attached to a controllable and deforming human mesh model。</li>
<li>results: 通过 méticulously designed human representation and supervision strategy，实现了 state-of-the-art synthesis results和快速的推理时间。<details>
<summary>Abstract</summary>
Generating controllable and photorealistic digital human avatars is a long-standing and important problem in Vision and Graphics. Recent methods have shown great progress in terms of either photorealism or inference speed while the combination of the two desired properties still remains unsolved. To this end, we propose a novel method, called DELIFFAS, which parameterizes the appearance of the human as a surface light field that is attached to a controllable and deforming human mesh model. At the core, we represent the light field around the human with a deformable two-surface parameterization, which enables fast and accurate inference of the human appearance. This allows perceptual supervision on the full image compared to previous approaches that could only supervise individual pixels or small patches due to their slow runtime. Our carefully designed human representation and supervision strategy leads to state-of-the-art synthesis results and inference time. The video results and code are available at https://vcai.mpi-inf.mpg.de/projects/DELIFFAS.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将生成控制可靠、渲染实际的数字人类头像作为视图和图形领域的长期问题。最近的方法在一个或两个属性中都已经取得了很大的进步，但是同时拥有这两种属性仍然是一个未解决的问题。为此，我们提出了一种新的方法，称为DELIFFAS，它将人类的外观 parameterized为附着在可控制和变形的人类矩阵模型上的表面光场。在核心上，我们使用可变的两面参数化来表示人类周围的光场，这使得快速和准确地推断人类的外观。这使得我们可以在全图像上进行准确的upervison，而不是之前的方法只能在个像素或小块上进行supervision，因为它们的运行时间过长。我们仔细设计的人类表示和监督策略，导致了最新的合成结果和运行时间。影像结果和代码可以在https://vcai.mpi-inf.mpg.de/projects/DELIFFAS中下载。
</details></li>
</ul>
<hr>
<h2 id="4K4D-Real-Time-4D-View-Synthesis-at-4K-Resolution"><a href="#4K4D-Real-Time-4D-View-Synthesis-at-4K-Resolution" class="headerlink" title="4K4D: Real-Time 4D View Synthesis at 4K Resolution"></a>4K4D: Real-Time 4D View Synthesis at 4K Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11448">http://arxiv.org/abs/2310.11448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zju3dv/4K4D">https://github.com/zju3dv/4K4D</a></li>
<li>paper_authors: Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, Xiaowei Zhou</li>
<li>for: 高精度和实时视觉合成动态3D场景的4K解析</li>
<li>methods: 使用4D点云表示法和硬件排版加速，以及新型混合外观模型提高渲染质量，同时保持高效性</li>
<li>results: 在DNA-Rendering数据集1080p分辨率和ENeRF-Outdoor数据集4K分辨率上实现了30倍 быстре的渲染速度，并达到了当前最佳的渲染质量<details>
<summary>Abstract</summary>
This paper targets high-fidelity and real-time view synthesis of dynamic 3D scenes at 4K resolution. Recently, some methods on dynamic view synthesis have shown impressive rendering quality. However, their speed is still limited when rendering high-resolution images. To overcome this problem, we propose 4K4D, a 4D point cloud representation that supports hardware rasterization and enables unprecedented rendering speed. Our representation is built on a 4D feature grid so that the points are naturally regularized and can be robustly optimized. In addition, we design a novel hybrid appearance model that significantly boosts the rendering quality while preserving efficiency. Moreover, we develop a differentiable depth peeling algorithm to effectively learn the proposed model from RGB videos. Experiments show that our representation can be rendered at over 400 FPS on the DNA-Rendering dataset at 1080p resolution and 80 FPS on the ENeRF-Outdoor dataset at 4K resolution using an RTX 4090 GPU, which is 30x faster than previous methods and achieves the state-of-the-art rendering quality. Our project page is available at https://zju3dv.github.io/4k4d/.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="EvalCrafter-Benchmarking-and-Evaluating-Large-Video-Generation-Models"><a href="#EvalCrafter-Benchmarking-and-Evaluating-Large-Video-Generation-Models" class="headerlink" title="EvalCrafter: Benchmarking and Evaluating Large Video Generation Models"></a>EvalCrafter: Benchmarking and Evaluating Large Video Generation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11440">http://arxiv.org/abs/2310.11440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/evalcrafter/EvalCrafter">https://github.com/evalcrafter/EvalCrafter</a></li>
<li>paper_authors: Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, Ying Shan<br>for: 这个论文主要目的是提出一种新的评估视频生成模型的框架和管道，以提高现有的评估方法的精度和全面性。methods: 作者使用了一种新的提问列表和18个对象指标来评估state-of-the-art的视频生成模型。同时，他们还提出了一种基于大语言模型的意见对应方法，以使用户的意见来调整对象指标的权重。results: 作者的研究表明，使用新的评估方法可以更好地评估视频生成模型的性能，并且与用户的意见更高相关。这表明了新的评估方法的效iveness。<details>
<summary>Abstract</summary>
The vision and language generative models have been overgrown in recent years. For video generation, various open-sourced models and public-available services are released for generating high-visual quality videos. However, these methods often use a few academic metrics, for example, FVD or IS, to evaluate the performance. We argue that it is hard to judge the large conditional generative models from the simple metrics since these models are often trained on very large datasets with multi-aspect abilities. Thus, we propose a new framework and pipeline to exhaustively evaluate the performance of the generated videos. To achieve this, we first conduct a new prompt list for text-to-video generation by analyzing the real-world prompt list with the help of the large language model. Then, we evaluate the state-of-the-art video generative models on our carefully designed benchmarks, in terms of visual qualities, content qualities, motion qualities, and text-caption alignment with around 18 objective metrics. To obtain the final leaderboard of the models, we also fit a series of coefficients to align the objective metrics to the users' opinions. Based on the proposed opinion alignment method, our final score shows a higher correlation than simply averaging the metrics, showing the effectiveness of the proposed evaluation method.
</details>
<details>
<summary>摘要</summary>
“在最近几年中，视觉和语言生成模型得到了广泛的应用和发展。为视频生成，各种开源的模型和公共可用的服务被发布，以生成高质量的视频。然而，这些方法经常使用一些学术指标，例如FVD或IS，来评估模型的表现。我们认为，这些简单的指标不能准确评估大型条件生成模型，因为这些模型通常在大量数据集上进行训练，具有多方面能力。因此，我们提出了一新的评估框架和管道，以完整评估生成视频的性能。我们首先对文本到视频生成的新提问列表进行分析，并使用大型语言模型的帮助来生成一个新的提问列表。然后，我们对当今状态的视频生成模型进行了严格的评估，包括视觉质量、内容质量、动作质量和文本描述对齐等18个对象指标。为了获得最终的模型排名，我们还使用一系列的系数进行对象指标的对齐。根据我们的提议的意见对齐方法，我们的最终分数显示与简单地平均指标的相对耗散更高，表明了我们的评估方法的效iveness。”
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Map-Relations-for-Unsupervised-Non-Rigid-Shape-Matching"><a href="#Revisiting-Map-Relations-for-Unsupervised-Non-Rigid-Shape-Matching" class="headerlink" title="Revisiting Map Relations for Unsupervised Non-Rigid Shape Matching"></a>Revisiting Map Relations for Unsupervised Non-Rigid Shape Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11420">http://arxiv.org/abs/2310.11420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongliang Cao, Paul Roetzer, Florian Bernard</li>
<li>for: 非RIGID 3D shape matching</li>
<li>methods: 提出了一种新的无监督学习方法，包括自适应功能图计算器和点精度损失函数</li>
<li>results: 在不同的挑战性场景下（包括非同温、topological noise和部分性），与前状态艺术方法相比，提高了substantially的性能<details>
<summary>Abstract</summary>
We propose a novel unsupervised learning approach for non-rigid 3D shape matching. Our approach improves upon recent state-of-the art deep functional map methods and can be applied to a broad range of different challenging scenarios. Previous deep functional map methods mainly focus on feature extraction and aim exclusively at obtaining more expressive features for functional map computation. However, the importance of the functional map computation itself is often neglected and the relationship between the functional map and point-wise map is underexplored. In this paper, we systematically investigate the coupling relationship between the functional map from the functional map solver and the point-wise map based on feature similarity. To this end, we propose a self-adaptive functional map solver to adjust the functional map regularisation for different shape matching scenarios, together with a vertex-wise contrastive loss to obtain more discriminative features. Using different challenging datasets (including non-isometry, topological noise and partiality), we demonstrate that our method substantially outperforms previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的无监督学习方法 для非定形3D形状匹配。我们的方法在当前状态艺术深度函数地图方法的基础上进行改进，并可以应用于各种不同的复杂enario。先前的深度函数地图方法主要关注特征提取，偏向于获得更表达力强的特征来计算函数地图。然而，函数地图计算本身的重要性经常被忽略，以及特征和点级地图之间的关系也未得到充分调查。在这篇论文中，我们系统地探讨了函数地图从函数地图解决器中的coupling关系，以及特征相似性基础上的点级地图。为此，我们提出了一种自适应函数地图解决器，以及基于特征相似性的Vertex-wise contraste loss。使用不同的复杂的数据集（包括非同一致、拓扑噪声和缺失），我们示出了我们的方法在前一代方法的基础上具有显著的改进。
</details></li>
</ul>
<hr>
<h2 id="VcT-Visual-change-Transformer-for-Remote-Sensing-Image-Change-Detection"><a href="#VcT-Visual-change-Transformer-for-Remote-Sensing-Image-Change-Detection" class="headerlink" title="VcT: Visual change Transformer for Remote Sensing Image Change Detection"></a>VcT: Visual change Transformer for Remote Sensing Image Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11417">http://arxiv.org/abs/2310.11417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/event-ahu/vct_remote_sensing_change_detection">https://github.com/event-ahu/vct_remote_sensing_change_detection</a></li>
<li>paper_authors: Bo Jiang, Zitian Wang, Xixi Wang, Ziyan Zhang, Lan Chen, Xiao Wang, Bin Luo</li>
<li>for: 本文提出了一种Visual change Transformer（VcT）模型，用于解决视觉变化检测问题。</li>
<li>methods: 该模型首先使用共享背景网络提取图像对的特征图，然后使用图 neural network 模型化图像对的结构信息，并采用 top-K 可靠的 токен mines 和改进自&#x2F;交叉关注机制。</li>
<li>results: 广泛的实验 validate 了我们提出的 VcT 模型的有效性。<details>
<summary>Abstract</summary>
Existing visual change detectors usually adopt CNNs or Transformers for feature representation learning and focus on learning effective representation for the changed regions between images. Although good performance can be obtained by enhancing the features of the change regions, however, these works are still limited mainly due to the ignorance of mining the unchanged background context information. It is known that one main challenge for change detection is how to obtain the consistent representations for two images involving different variations, such as spatial variation, sunlight intensity, etc. In this work, we demonstrate that carefully mining the common background information provides an important cue to learn the consistent representations for the two images which thus obviously facilitates the visual change detection problem. Based on this observation, we propose a novel Visual change Transformer (VcT) model for visual change detection problem. To be specific, a shared backbone network is first used to extract the feature maps for the given image pair. Then, each pixel of feature map is regarded as a graph node and the graph neural network is proposed to model the structured information for coarse change map prediction. Top-K reliable tokens can be mined from the map and refined by using the clustering algorithm. Then, these reliable tokens are enhanced by first utilizing self/cross-attention schemes and then interacting with original features via an anchor-primary attention learning module. Finally, the prediction head is proposed to get a more accurate change map. Extensive experiments on multiple benchmark datasets validated the effectiveness of our proposed VcT model.
</details>
<details>
<summary>摘要</summary>
现有的视觉变化探测器通常采用CNN或Transformers来学习特征表示学习，并主要关注学习改变区域 между图像中的有效表示。although these works can achieve good performance by enhancing the features of the changed regions, they are still limited because they ignore the mining of unchanged background context information. It is known that one of the main challenges of change detection is how to obtain consistent representations for two images with different variations, such as spatial variation and sunlight intensity. In this work, we find that carefully mining the common background information provides an important cue to learn the consistent representations for the two images, which thus facilitates the visual change detection problem. Based on this observation, we propose a novel Visual change Transformer (VcT) model for the visual change detection problem. Specifically, a shared backbone network is first used to extract the feature maps for the given image pair. Then, each pixel of the feature map is regarded as a graph node, and a graph neural network is proposed to model the structured information for coarse change map prediction. Top-K reliable tokens can be mined from the map and refined by using a clustering algorithm. Then, these reliable tokens are enhanced by first utilizing self/cross-attention schemes and then interacting with the original features via an anchor-primary attention learning module. Finally, the prediction head is proposed to get a more accurate change map. Extensive experiments on multiple benchmark datasets validated the effectiveness of our proposed VcT model.
</details></li>
</ul>
<hr>
<h2 id="A-voxel-level-approach-to-brain-age-prediction-A-method-to-assess-regional-brain-aging"><a href="#A-voxel-level-approach-to-brain-age-prediction-A-method-to-assess-regional-brain-aging" class="headerlink" title="A voxel-level approach to brain age prediction: A method to assess regional brain aging"></a>A voxel-level approach to brain age prediction: A method to assess regional brain aging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11385">http://arxiv.org/abs/2310.11385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nehagianchandani/voxel-level-brain-age-prediction">https://github.com/nehagianchandani/voxel-level-brain-age-prediction</a></li>
<li>paper_authors: Neha Gianchandani, Mahsa Dibaji, Johanna Ospel, Fernando Vega, Mariana Bento, M. Ethan MacDonald, Roberto Souza</li>
<li>For: 这个研究的目的是预测脑部年龄从T1束缚成像图像中，以获得精细的地方化脑部年龄估计。这有助于理解健康和疾病群体中脑部年龄轨迹的差异。* Methods: 这个研究使用了深度学习多任务模型来预测脑部年龄。这种模型在现有文献中表现出色，并且在健康和疾病群体中提供了价值的临床洞察。* Results: 研究发现，健康群体和疾病群体之间存在差异的脑部年龄轨迹。具体来说，健康群体的脑部年龄轨迹比疾病群体更年轻，而且存在脑部区域异常的差异。这些结果提供了有价值的临床洞察，可以帮助理解脑部年龄的发展和疾病的扩散。<details>
<summary>Abstract</summary>
Brain aging is a regional phenomenon, a facet that remains relatively under-explored within the realm of brain age prediction research using machine learning methods. Voxel-level predictions can provide localized brain age estimates that can provide granular insights into the regional aging processes. This is essential to understand the differences in aging trajectories in healthy versus diseased subjects. In this work, a deep learning-based multitask model is proposed for voxel-level brain age prediction from T1-weighted magnetic resonance images. The proposed model outperforms the models existing in the literature and yields valuable clinical insights when applied to both healthy and diseased populations. Regional analysis is performed on the voxel-level brain age predictions to understand aging trajectories of known anatomical regions in the brain and show that there exist disparities in regional aging trajectories of healthy subjects compared to ones with underlying neurological disorders such as Dementia and more specifically, Alzheimer's disease. Our code is available at https://github.com/nehagianchandani/Voxel-level-brain-age-prediction.
</details>
<details>
<summary>摘要</summary>
脑衰老是一个地域性的现象，在机器学习方法可预测脑 возраст预测研究中尚未得到充分的探讨。 voxel 级预测可提供本地化的脑 возраст估计，从而为诊断不同 Population 提供细化的诊断信息。这对于理解健康和疾病 Population 的脑衰老趋势具有重要意义。在本研究中，我们提出了一种基于深度学习的多任务模型，用于从 T1 束缚磁共振成像图像中预测 voxel 级脑 возраст。该模型在文献中存在的模型之上升级，并且在应用于健康和疾病 Population 时具有价值的临床应用。通过对 voxel 级脑 возраст预测结果进行区域分析，我们可以理解健康 Population 的脑衰老趋势与患有 деменcia 和特别是阿尔ц海默病的脑衰老趋势之间的差异。我们的代码可以在 GitHub 上找到：https://github.com/nehagianchandani/Voxel-level-brain-age-prediction。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalizable-Multi-Camera-3D-Object-Detection-via-Perspective-Debiasing"><a href="#Towards-Generalizable-Multi-Camera-3D-Object-Detection-via-Perspective-Debiasing" class="headerlink" title="Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing"></a>Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11346">http://arxiv.org/abs/2310.11346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Lu, Yunpeng Zhang, Qing Lian, Dalong Du, Yingcong Chen</li>
<li>for: 本研究旨在提高多摄像头3D物体检测（MC3D-Det）的精度和可靠性，解决由于不 familar 测试环境而导致的问题。</li>
<li>methods: 我们提出了一种新的方法，即将3D检测与2D相机平面结果相对应，以确保稳定和准确的检测。我们的框架基于视角偏移 rectification，帮助学习不受视角变化影响的特征。我们首先生成了多视图地图从BEV特征中，然后将这些地图的视角偏移正确 rectified，以利用隐藏的前景体Volume来连接相机和BEV平面。</li>
<li>results: 我们的方法可以在不同的视角和环境下提高object detection的精度和可靠性。我们在Domain Generalization（DG）和Unsupervised Domain Adaptation（UDA）任务上实现了显著的效果。此外，我们还证明了我们的方法可以在实际数据上达到满意的结果，只需训练于虚拟数据集。<details>
<summary>Abstract</summary>
Detecting objects in 3D space using multiple cameras, known as Multi-Camera 3D Object Detection (MC3D-Det), has gained prominence with the advent of bird's-eye view (BEV) approaches. However, these methods often struggle when faced with unfamiliar testing environments due to the lack of diverse training data encompassing various viewpoints and environments. To address this, we propose a novel method that aligns 3D detection with 2D camera plane results, ensuring consistent and accurate detections. Our framework, anchored in perspective debiasing, helps the learning of features resilient to domain shifts. In our approach, we render diverse view maps from BEV features and rectify the perspective bias of these maps, leveraging implicit foreground volumes to bridge the camera and BEV planes. This two-step process promotes the learning of perspective- and context-independent features, crucial for accurate object detection across varying viewpoints, camera parameters and environment conditions. Notably, our model-agnostic approach preserves the original network structure without incurring additional inference costs, facilitating seamless integration across various models and simplifying deployment. Furthermore, we also show our approach achieves satisfactory results in real data when trained only with virtual datasets, eliminating the need for real scene annotations. Experimental results on both Domain Generalization (DG) and Unsupervised Domain Adaptation (UDA) clearly demonstrate its effectiveness. Our code will be released.
</details>
<details>
<summary>摘要</summary>
使用多个摄像头探测3D空间中的对象，称为多摄像头3D对象探测（MC3D-Det），在现代鸟瞰视图（BEV）方法的普及下得到了更多的关注。然而，这些方法经常在不熟悉的测试环境中遇到困难，因为缺乏包括多个视角和环境的多样化训练数据。为解决这个问题，我们提出了一种新的方法，该方法将3D探测与2D摄像头平面结果进行对应，以确保准确和一致的探测。我们的框架基于视角偏移的约束，帮助学习不受视角和环境的偏移影响的特征。在我们的方法中，我们从BEV特征中生成多种视图图，然后对这些图像进行视角偏移的正确化，通过使用隐藏的前景体来连接摄像头和BEV平面。这两步过程推动学习不受视角和环境的特征，这些特征是精度的对象探测所必需的。各种模型的机制无需更改，我们的方法可以轻松地与不同的模型集成，并且简化部署。此外，我们还证明我们的方法在真实数据上获得了满意的结果，无需使用真实场景的注释。实验结果表明，我们的方法在适应不同视角、摄像头参数和环境条件时具有显著的效果。我们的代码将会发布。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generic-Semi-Supervised-Framework-for-Volumetric-Medical-Image-Segmentation"><a href="#Towards-Generic-Semi-Supervised-Framework-for-Volumetric-Medical-Image-Segmentation" class="headerlink" title="Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation"></a>Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11320">http://arxiv.org/abs/2310.11320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/GenericSSL">https://github.com/xmed-lab/GenericSSL</a></li>
<li>paper_authors: Haonan Wang, Xiaomeng Li</li>
<li>For: 这个研究的目的是开发一个通用的semi-supervised learning（SSL）框架，可以处理多个设定，包括SSL、Unsupervised Domain Adaptation（UDA）和Semi-supervised Domain Generalization（SemiDG）。* Methods: 该框架使用了一种含有混合和解耦的方法，包括一个扩散编码器，用于从多个分布&#x2F;领域中提取共同知识集，以及三个解码器，用于在培训过程中避免过拟合标注数据。* Results: 对四个 benchmark dataset进行了评估，并与现有方法进行了比较，结果表明该框架在所有四个设定中具有显著的改进， indicating the potential of the framework to tackle more challenging SSL scenarios.<details>
<summary>Abstract</summary>
Volume-wise labeling in 3D medical images is a time-consuming task that requires expertise. As a result, there is growing interest in using semi-supervised learning (SSL) techniques to train models with limited labeled data. However, the challenges and practical applications extend beyond SSL to settings such as unsupervised domain adaptation (UDA) and semi-supervised domain generalization (SemiDG). This work aims to develop a generic SSL framework that can handle all three settings. We identify two main obstacles to achieving this goal in the existing SSL framework: 1) the weakness of capturing distribution-invariant features; and 2) the tendency for unlabeled data to be overwhelmed by labeled data, leading to over-fitting to the labeled data during training. To address these issues, we propose an Aggregating & Decoupling framework. The aggregating part consists of a Diffusion encoder that constructs a common knowledge set by extracting distribution-invariant features from aggregated information from multiple distributions/domains. The decoupling part consists of three decoders that decouple the training process with labeled and unlabeled data, thus avoiding over-fitting to labeled data, specific domains and classes. We evaluate our proposed framework on four benchmark datasets for SSL, Class-imbalanced SSL, UDA and SemiDG. The results showcase notable improvements compared to state-of-the-art methods across all four settings, indicating the potential of our framework to tackle more challenging SSL scenarios. Code and models are available at: https://github.com/xmed-lab/GenericSSL.
</details>
<details>
<summary>摘要</summary>
医学三维图像的体积标注是一项时间消耗大的任务，需要专家知识。由于此类任务的挑战和实际应用超出了半编制学习（SSL）技术的范畴，因此这项工作的目标是开发一个通用的SSL框架，可以处理所有三个设置。我们在现有的SSL框架中 Identified two main challenges to achieving this goal: 1）不能够捕捉分布不变的特征; 2）无标注数据被标注数据所抑制，导致在训练过程中适应标注数据，从而导致过拟合。为了解决这些问题，我们提出了一个集成&分离框架。集成部分包括一个扩散编码器，通过从多个分布/领域中提取分布不变的特征来构建共同知识集。分离部分包括三个解码器，通过解耦训练过程中的标注数据和无标注数据，从而避免过拟合标注数据、特定领域和类别。我们在四个 benchmark 数据集上进行了四种SSL、不均衡SSL、UDA 和 SemiDG 的测试，结果显示了与现有方法的明显改善，这表明我们的框架有可能在更加复杂的SSL场景中表现出色。代码和模型可以在 GitHub 上获取：https://github.com/xmed-lab/GenericSSL。
</details></li>
</ul>
<hr>
<h2 id="Multi-Self-supervised-Pre-fine-tuned-Transformer-Fusion-for-Better-Intelligent-Transportation-Detection"><a href="#Multi-Self-supervised-Pre-fine-tuned-Transformer-Fusion-for-Better-Intelligent-Transportation-Detection" class="headerlink" title="Multi Self-supervised Pre-fine-tuned Transformer Fusion for Better Intelligent Transportation Detection"></a>Multi Self-supervised Pre-fine-tuned Transformer Fusion for Better Intelligent Transportation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11307">http://arxiv.org/abs/2310.11307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juwu Zheng, Jiangtao Ren</li>
<li>for: 本研究旨在提高智能交通系统中的检测精度，解决现有检测方法受到两个限制：一是模型知识预处理大规模数据集和目标任务知识之间的差异，二是大多数检测模型采用单源学习方式，限制学习能力。</li>
<li>methods: 我们提出了一种多自助学习前练 transformer 融合网络（MSPTF），包括两个步骤：自助学习预练频率学习和多模型协同学习目标任务。在第一步，我们引入了自助学习方法到 transformer 模型预练中，以减少数据成本并减轻预处理模型和目标任务之间的知识差异。在第二步，我们提出了多模型协同学习方法，通过考虑不同模型架构和预练任务之间的信息差异，将不同 transformer 模型特征信息融合到一起，以获得更完整和正确的检测特征。</li>
<li>results: 我们对 vehicle 识别数据集和路径疾病检测数据集进行实验，比基准方法提高了1.1%、5.5%、4.2%，比最新方法（sota）提高了0.7%、1.8%、1.7%。这些结果证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Intelligent transportation system combines advanced information technology to provide intelligent services such as monitoring, detection, and early warning for modern transportation. Intelligent transportation detection is the cornerstone of many intelligent traffic services by identifying task targets through object detection methods. However existing detection methods in intelligent transportation are limited by two aspects. First, there is a difference between the model knowledge pre-trained on large-scale datasets and the knowledge required for target task. Second, most detection models follow the pattern of single-source learning, which limits the learning ability. To address these problems, we propose a Multi Self-supervised Pre-fine-tuned Transformer Fusion (MSPTF) network, consisting of two steps: unsupervised pre-fine-tune domain knowledge learning and multi-model fusion target task learning. In the first step, we introduced self-supervised learning methods into transformer model pre-fine-tune which could reduce data costs and alleviate the knowledge gap between pre-trained model and target task. In the second step, we take feature information differences between different model architectures and different pre-fine-tune tasks into account and propose Multi-model Semantic Consistency Cross-attention Fusion (MSCCF) network to combine different transformer model features by considering channel semantic consistency and feature vector semantic consistency, which obtain more complete and proper fusion features for detection task. We experimented the proposed method on vehicle recognition dataset and road disease detection dataset and achieved 1.1%, 5.5%, 4.2% improvement compared with baseline and 0.7%, 1.8%, 1.7% compared with sota, which proved the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
智能交通系统结合先进的信息技术，为现代交通提供智能服务，如监测、检测和早期警示。智能交通检测是现代交通服务的核心，通过物体检测方法来确定任务目标。然而，现有的检测方法在智能交通中存在两个限制。一是，模型的先行学习知识与目标任务知识之间存在差异。二是，大多数检测模型采用单源学习模式，限制了学习能力。为了解决这些问题，我们提出了多自动学习预练转换器融合网络（MSPTF），包括两个步骤：无监督预练域知识学习和多模型融合目标任务学习。在第一步，我们将自动学习方法引入转换器模型预练，以减少数据成本并缓解先行学习知识与目标任务知识之间的差异。在第二步，我们利用不同模型架构和预练任务的特征信息差异，提出多模型semantic consistency cross-attention融合网络（MSCCF），将不同转换器模型的特征信息融合，以考虑通道semantic consistency和特征向量semantic consistency，从而获得更加完整和正确的融合特征，进而提高检测任务的准确率。我们对汽车识别 dataset 和路况病变 dataset 进行实验，与基准值和state-of-the-art（sota）进行比较，实验结果显示，我们的方法可以提高检测任务的准确率1.1%、5.5%、4.2%，比基准值更高，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="CorrTalk-Correlation-Between-Hierarchical-Speech-and-Facial-Activity-Variances-for-3D-Animation"><a href="#CorrTalk-Correlation-Between-Hierarchical-Speech-and-Facial-Activity-Variances-for-3D-Animation" class="headerlink" title="CorrTalk: Correlation Between Hierarchical Speech and Facial Activity Variances for 3D Animation"></a>CorrTalk: Correlation Between Hierarchical Speech and Facial Activity Variances for 3D Animation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11295">http://arxiv.org/abs/2310.11295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaojie Chu, Kailing Guo, Xiaofen Xing, Yilin Lan, Bolun Cai, Xiangmin Xu<br>for: 这篇论文主要针对的是如何通过语音驱动来生成3D人脸动画，以解决跨Modal Task的挑战。methods: 该论文提出了一种新的框架，即CorrTalk，该框架可以有效地在不同强度的面部活动之间建立时间相关性，并通过 dual-branch decoding 架构来同时synthesize强度不同的面部活动，以保证更宽泛的表情动画生成。results: 根据论文的实验和用户研究，CorrTalk 表现出excelent的性能，可以准确地生成具有不同强度的表情动画，并且能够保持 lip-sync 和合理的表情表达。<details>
<summary>Abstract</summary>
Speech-driven 3D facial animation is a challenging cross-modal task that has attracted growing research interest. During speaking activities, the mouth displays strong motions, while the other facial regions typically demonstrate comparatively weak activity levels. Existing approaches often simplify the process by directly mapping single-level speech features to the entire facial animation, which overlook the differences in facial activity intensity leading to overly smoothed facial movements. In this study, we propose a novel framework, CorrTalk, which effectively establishes the temporal correlation between hierarchical speech features and facial activities of different intensities across distinct regions. A novel facial activity intensity metric is defined to distinguish between strong and weak facial activity, obtained by computing the short-time Fourier transform of facial vertex displacements. Based on the variances in facial activity, we propose a dual-branch decoding framework to synchronously synthesize strong and weak facial activity, which guarantees wider intensity facial animation synthesis. Furthermore, a weighted hierarchical feature encoder is proposed to establish temporal correlation between hierarchical speech features and facial activity at different intensities, which ensures lip-sync and plausible facial expressions. Extensive qualitatively and quantitatively experiments as well as a user study indicate that our CorrTalk outperforms existing state-of-the-art methods. The source code and supplementary video are publicly available at: https://zjchu.github.io/projects/CorrTalk/
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>语音驱动的3D面部动画是一个吸引了研究者们的挑战性跨Modal任务。在说话活动中，口部会显示强烈的动作，而其他面部区域通常会表现出较弱的活动水平。现有的方法通常会简化过程，直接将单级语音特征映射到整个面部动画上，这会忽略面部活动Intensity的差异，导致面部运动过于平滑。在这种研究中，我们提出了一个新的框架，即CorrTalk，它可以有效地在不同Intensity水平上建立语音特征和面部动画的时间相关性。我们定义了一个新的面部动activity强度度量，通过计算面部顶点位移的短时傅立叶变换来获得。基于面部动activity的差异，我们提出了一种双分支解码机制，以同步生成强度不同的面部动画，从而保证更广泛的Intensity面部动画生成。此外，我们还提出了一种加权层次特征编码器，以建立不同Intensity水平上的语音特征和面部动画之间的时间相关性，从而保证唇ync和合理的面部表达。我们的CorrTalk在质量和kvantalitative的实验以及用户研究中表现出色，超越了现有的状态 искус技术。source code和补充视频可以在以下链接获取：https://zjchu.github.io/projects/CorrTalk/
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-3D-Scene-Flow-Estimation-and-Motion-Prediction-using-Local-Rigidity-Prior"><a href="#Self-Supervised-3D-Scene-Flow-Estimation-and-Motion-Prediction-using-Local-Rigidity-Prior" class="headerlink" title="Self-Supervised 3D Scene Flow Estimation and Motion Prediction using Local Rigidity Prior"></a>Self-Supervised 3D Scene Flow Estimation and Motion Prediction using Local Rigidity Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11284">http://arxiv.org/abs/2310.11284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruibo Li, Chi Zhang, Zhe Wang, Chunhua Shen, Guosheng Lin</li>
<li>for:  investigate self-supervised 3D scene flow estimation and class-agnostic motion prediction on point clouds</li>
<li>methods:  build pseudo scene flow labels through piecewise rigid motion estimation and validate with a validity mask</li>
<li>results:  achieve new state-of-the-art performance in self-supervised scene flow learning and outperform previous state-of-the-art self-supervised methods on nuScenes dataset.<details>
<summary>Abstract</summary>
In this article, we investigate self-supervised 3D scene flow estimation and class-agnostic motion prediction on point clouds. A realistic scene can be well modeled as a collection of rigidly moving parts, therefore its scene flow can be represented as a combination of the rigid motion of these individual parts. Building upon this observation, we propose to generate pseudo scene flow labels for self-supervised learning through piecewise rigid motion estimation, in which the source point cloud is decomposed into local regions and each region is treated as rigid. By rigidly aligning each region with its potential counterpart in the target point cloud, we obtain a region-specific rigid transformation to generate its pseudo flow labels. To mitigate the impact of potential outliers on label generation, when solving the rigid registration for each region, we alternately perform three steps: establishing point correspondences, measuring the confidence for the correspondences, and updating the rigid transformation based on the correspondences and their confidence. As a result, confident correspondences will dominate label generation and a validity mask will be derived for the generated pseudo labels. By using the pseudo labels together with their validity mask for supervision, models can be trained in a self-supervised manner. Extensive experiments on FlyingThings3D and KITTI datasets demonstrate that our method achieves new state-of-the-art performance in self-supervised scene flow learning, without any ground truth scene flow for supervision, even performing better than some supervised counterparts. Additionally, our method is further extended to class-agnostic motion prediction and significantly outperforms previous state-of-the-art self-supervised methods on nuScenes dataset.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们调查了无监督3D场景流估计和无类别运动预测。一个现实场景可以很好地被模型为一个由rigidly运动的部件组成的集合，因此其场景流可以被表示为这些个体部件的rigid运动的组合。基于这一观察，我们提议通过地方rigid运动估计来生成 Pseudo场景流标签，其中源点云被分解成地方区域，每个区域都是rigid。通过将每个区域与其可能的对应点云中的区域进行rigid对齐，我们可以获得每个区域的rigid变换，并生成其pseudo流标签。为了mitigate潜在异常值对标签生成的影响，当解决每个区域的rigid注册问题时，我们采取了三个步骤：确定点对应关系、测量对应关系的信任度，并基于对应关系和其信任度更新rigid变换。因此，信任度高的对应关系将dominates标签生成，并生成一个有效性面纱。通过使用这些pseudo标签和其有效性面纱进行监督，我们可以在无监督情况下训练模型。我们的方法在FlyingThings3D和KITTI数据集上进行了广泛的实验，并达到了无监督场景流学习的新状态对抗性性能，甚至超过了一些监督 counterpart。此外，我们的方法进一步扩展到无类别运动预测，并在nuScenes数据集上显著超越了前一个状态对抗性自监督方法。
</details></li>
</ul>
<hr>
<h2 id="Video-Super-Resolution-Using-a-Grouped-Residual-in-Residual-Network"><a href="#Video-Super-Resolution-Using-a-Grouped-Residual-in-Residual-Network" class="headerlink" title="Video Super-Resolution Using a Grouped Residual in Residual Network"></a>Video Super-Resolution Using a Grouped Residual in Residual Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11276">http://arxiv.org/abs/2310.11276</a></li>
<li>repo_url: None</li>
<li>paper_authors: MohammadHossein Ashoori, Arash Amini</li>
<li>for: 提高图像&#x2F;视频内容的分辨率和质量。</li>
<li>methods: 使用 grouped residual in residual network (GRRN) 方法。</li>
<li>results: 与现有方法相比，GRRN 方法可以提供Acceptable的输出图像质量。<details>
<summary>Abstract</summary>
Super-resolution (SR) is the technique of increasing the nominal resolution of image / video content accompanied with quality improvement. Video super-resolution (VSR) can be considered as the generalization of single image super-resolution (SISR). This generalization should be such that more detail is created in the output using adjacent input frames. In this paper, we propose a grouped residual in residual network (GRRN) for VSR. By adjusting the hyperparameters of the proposed structure, we train three networks with different numbers of parameters and compare their quantitative and qualitative results with the existing methods. Although based on some quantitative criteria, GRRN does not provide better results than the existing methods, in terms of the quality of the output image it has acceptable performance.
</details>
<details>
<summary>摘要</summary>
超分解（SR）是增加图像/视频内容的 номинаinal 分辨率，同时提高图像质量的技术。视频超分解（VSR）可以视为单个图像超分解（SISR）的推广。在这篇论文中，我们提出了分组差分网络（GRRN） для VSR。通过调整结构的 hyperparameter，我们训练了三个网络，每个网络有不同的参数数量，并与现有方法进行比较。虽然根据一些量化标准，GRRN 不比现有方法提供更好的结果，但在输出图像质量方面，它的表现是可接受的。
</details></li>
</ul>
<hr>
<h2 id="Image-Compression-using-only-Attention-based-Neural-Networks"><a href="#Image-Compression-using-only-Attention-based-Neural-Networks" class="headerlink" title="Image Compression using only Attention based Neural Networks"></a>Image Compression using only Attention based Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11265">http://arxiv.org/abs/2310.11265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Natacha Luka, Romain Negrel, David Picard</li>
<li>for: 本研究旨在探讨图像压缩中完全使用注意力层，以提高图像压缩性和计算效率。</li>
<li>methods: 本paper使用的方法是基于注意力机制的Transformer architecture，并引入了学习图像查询来汇聚patch信息。</li>
<li>results: 经过广泛的评估，本paper的方法在popular Kodak、DIV2K和CLIC datasets上达到了与传统手动设计ipeline相同或更高的性能，而无需使用 convolutional layers。<details>
<summary>Abstract</summary>
In recent research, Learned Image Compression has gained prominence for its capacity to outperform traditional handcrafted pipelines, especially at low bit-rates. While existing methods incorporate convolutional priors with occasional attention blocks to address long-range dependencies, recent advances in computer vision advocate for a transformative shift towards fully transformer-based architectures grounded in the attention mechanism. This paper investigates the feasibility of image compression exclusively using attention layers within our novel model, QPressFormer. We introduce the concept of learned image queries to aggregate patch information via cross-attention, followed by quantization and coding techniques. Through extensive evaluations, our work demonstrates competitive performance achieved by convolution-free architectures across the popular Kodak, DIV2K, and CLIC datasets.
</details>
<details>
<summary>摘要</summary>
Recent research has shown that Learned Image Compression has gained popularity for its ability to outperform traditional handcrafted pipelines, especially at low bit-rates. While existing methods use convolutional priors with occasional attention blocks to address long-range dependencies, recent advances in computer vision have advocated for a transformative shift towards fully transformer-based architectures grounded in the attention mechanism. This paper explores the feasibility of image compression using only attention layers in our novel model, QPressFormer. We introduce the concept of learned image queries to aggregate patch information via cross-attention, followed by quantization and coding techniques. Through extensive evaluations, our work demonstrates competitive performance achieved by convolution-free architectures across the popular Kodak, DIV2K, and CLIC datasets.
</details></li>
</ul>
<hr>
<h2 id="An-empirical-study-of-automatic-wildlife-detection-using-drone-thermal-imaging-and-object-detection"><a href="#An-empirical-study-of-automatic-wildlife-detection-using-drone-thermal-imaging-and-object-detection" class="headerlink" title="An empirical study of automatic wildlife detection using drone thermal imaging and object detection"></a>An empirical study of automatic wildlife detection using drone thermal imaging and object detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11257">http://arxiv.org/abs/2310.11257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miao Chang, Tan Vuong, Manas Palaparthi, Lachlan Howell, Alessio Bonti, Mohamed Abdelrazek, Duc Thanh Nguyen</li>
<li>For: The paper is written for wildlife management, specifically to explore the use of drones and thermal imaging technology for collecting and interpreting wildlife data.* Methods: The paper uses a comprehensive review and empirical study of drone-based wildlife detection, including the collection of a realistic dataset of drone-derived wildlife thermal detections and the annotation of these detections via bounding boxes by experts. The paper also benchmarks state-of-the-art object detection algorithms on the collected dataset.* Results: The paper provides experimental results to identify issues and discuss future directions in automatic animal monitoring using drones.Here is the information in Simplified Chinese text:* For: 这篇论文是为了管理野生动物而写的，具体来说是利用无人飞行器和热成像技术收集和解读野生动物数据。* Methods: 这篇论文使用了全面的文献综述和实验研究，包括收集了一个真实的无人飞行器捕捉的野生动物热成像检测数据，并由专家 manually annotate these detections via bounding boxes。 paper还使用了现有的对象检测算法来对 collected dataset 进行比较。* Results: 这篇论文提供了实验结果，用于发现issues和讨论未来无人飞行器自动动物监测的方向。<details>
<summary>Abstract</summary>
Artificial intelligence has the potential to make valuable contributions to wildlife management through cost-effective methods for the collection and interpretation of wildlife data. Recent advances in remotely piloted aircraft systems (RPAS or ``drones'') and thermal imaging technology have created new approaches to collect wildlife data. These emerging technologies could provide promising alternatives to standard labourious field techniques as well as cover much larger areas. In this study, we conduct a comprehensive review and empirical study of drone-based wildlife detection. Specifically, we collect a realistic dataset of drone-derived wildlife thermal detections. Wildlife detections, including arboreal (for instance, koalas, phascolarctos cinereus) and ground dwelling species in our collected data are annotated via bounding boxes by experts. We then benchmark state-of-the-art object detection algorithms on our collected dataset. We use these experimental results to identify issues and discuss future directions in automatic animal monitoring using drones.
</details>
<details>
<summary>摘要</summary>
人工智能有可能为野生动物管理提供有价值的贡献，通过cost-effective的方式收集和解释野生动物数据。最近的无人驾驶飞行器系统（RPAS或“无人机”）和热成像技术的发展已经创造了新的方法收集野生动物数据。这些新技术可能会提供标准化Field技术的有优的替代方案，同时能够覆盖更大的区域。在这项研究中，我们进行了全面的文献综述和实验研究，specifically collecting a realistic dataset of drone-derived wildlife thermal detections。我们的收集数据包括树上的物种（如 Koala，phascolarctos cinereus）和地面生物种，并由专家用 bounding boxes 进行了标注。我们然后对 state-of-the-art 对象检测算法进行了 benchmarking 测试，并使用实验结果来评估自动动物监测使用无人机的问题和未来方向。
</details></li>
</ul>
<hr>
<h2 id="Gromov-Wassertein-like-Distances-in-the-Gaussian-Mixture-Models-Space"><a href="#Gromov-Wassertein-like-Distances-in-the-Gaussian-Mixture-Models-Space" class="headerlink" title="Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space"></a>Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11256">http://arxiv.org/abs/2310.11256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Salmona, Julie Delon, Agnès Desolneux</li>
<li>for: 本文介绍了两种Gromov-Wasserstein-type距离在 Gaussian mixture model 空间上。第一种距离是 между两个离散分布在 Gaussian 测度空间上的 Gromov-Wasserstein 距离，可以作为 Gromov-Wasserstein 的替代方法，用于评估分布之间的距离，而不需要直接计算传输计划。</li>
<li>methods: 本文引入了另一种距离 between measures living in incomparable spaces，这种距离与 Gromov-Wasserstein 密切相关，可以用于定义传输计划。 restricting the set of admissible transportation couplings to be themselves Gaussian mixture models in this latter, this defines another distance between Gaussian mixture models that can be used as another alternative to Gromov-Wasserstein。</li>
<li>results: 本文设计了一种基于第一种距离的传输计划，并通过对 medium-to-large scale problems such as shape matching and hyperspectral image color transfer 进行实验，证明了其实用性。<details>
<summary>Abstract</summary>
In this paper, we introduce two Gromov-Wasserstein-type distances on the set of Gaussian mixture models. The first one takes the form of a Gromov-Wasserstein distance between two discrete distributionson the space of Gaussian measures. This distance can be used as an alternative to Gromov-Wasserstein for applications which only require to evaluate how far the distributions are from each other but does not allow to derive directly an optimal transportation plan between clouds of points. To design a way to define such a transportation plan, we introduce another distance between measures living in incomparable spaces that turns out to be closely related to Gromov-Wasserstein. When restricting the set of admissible transportation couplings to be themselves Gaussian mixture models in this latter, this defines another distance between Gaussian mixture models that can be used as another alternative to Gromov-Wasserstein and which allows to derive an optimal assignment between points. Finally, we design a transportation plan associated with the first distance by analogy with the second, and we illustrate their practical uses on medium-to-large scale problems such as shape matching and hyperspectral image color transfer.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了两种Gromov-Wasserstein-类型的距离在 Gaussian mixture model 上。第一个距离是两个抽象分布在 Gaussian measure 空间上的 Gromov-Wasserstein 距离，可以用作 Gromov-Wasserstein 的替代方法，但不能直接 derivate 最佳运输计划 между云集点。为了设计一种定义这种运输计划的方法，我们引入了另一种在不可比较的空间上的距离，该距离与 Gromov-Wasserstein 密切相关。当限制了可用的运输结合为 Gaussian mixture model 时，这个距离定义了另一种 Gaussian mixture model 之间的距离，可以作为 Gromov-Wasserstein 的另一种替代方法，并且可以 derivate 最佳分配计划。最后，我们设计了一个运输计划相关的方法，并在媒体规模到大型问题上如形态匹配和彩色图像传输中 illustrate 其实用性。
</details></li>
</ul>
<hr>
<h2 id="LiDAR-based-4D-Occupancy-Completion-and-Forecasting"><a href="#LiDAR-based-4D-Occupancy-Completion-and-Forecasting" class="headerlink" title="LiDAR-based 4D Occupancy Completion and Forecasting"></a>LiDAR-based 4D Occupancy Completion and Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11239">http://arxiv.org/abs/2310.11239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4ce/occ4cast">https://github.com/ai4ce/occ4cast</a></li>
<li>paper_authors: Xinhao Liu, Moonjun Gong, Qi Fang, Haoyu Xie, Yiming Li, Hang Zhao, Chen Feng</li>
<li>for: 本研究旨在整合Scene Completion和Forecasting两个问题，提出了一种新的LiDAR感知任务——Occupancy Completion and Forecasting（OCF），用于自动驾驶中的4D感知。</li>
<li>methods: 本研究使用了新的算法来解决三个挑战：（1）稀疏到稠密的重建，（2）部分到完整的幻化，（3）3D到4D预测。</li>
<li>results: 研究人员通过对OCFBench dataset进行超视觉和评估，发现了一些相对较好的基线模型和自己的模型的性能。这些结果预示了OCF任务的重要性和潜在应用前景。<details>
<summary>Abstract</summary>
Scene completion and forecasting are two popular perception problems in research for mobile agents like autonomous vehicles. Existing approaches treat the two problems in isolation, resulting in a separate perception of the two aspects. In this paper, we introduce a novel LiDAR perception task of Occupancy Completion and Forecasting (OCF) in the context of autonomous driving to unify these aspects into a cohesive framework. This task requires new algorithms to address three challenges altogether: (1) sparse-to-dense reconstruction, (2) partial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable supervision and evaluation, we curate a large-scale dataset termed OCFBench from public autonomous driving datasets. We analyze the performance of closely related existing baseline models and our own ones on our dataset. We envision that this research will inspire and call for further investigation in this evolving and crucial area of 4D perception. Our code for data curation and baseline implementation is available at https://github.com/ai4ce/Occ4cast.
</details>
<details>
<summary>摘要</summary>
Scene completion and forecasting are two popular perception problems in research for mobile agents like autonomous vehicles. Existing approaches treat the two problems in isolation, resulting in a separate perception of the two aspects. In this paper, we introduce a novel LiDAR perception task of Occupancy Completion and Forecasting (OCF) in the context of autonomous driving to unify these aspects into a cohesive framework. This task requires new algorithms to address three challenges altogether: (1) sparse-to-dense reconstruction, (2) partial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable supervision and evaluation, we curate a large-scale dataset termed OCFBench from public autonomous driving datasets. We analyze the performance of closely related existing baseline models and our own ones on our dataset. We envision that this research will inspire and call for further investigation in this evolving and crucial area of 4D perception. Our code for data curation and baseline implementation is available at https://github.com/ai4ce/Occ4cast.Translation notes:* "Scene completion" and "forecasting" are both translated as "场景完成" (scenario completion) to refer to the same concept.* "LiDAR perception" is translated as "LiDAR感知" (LiDAR perception) to refer to the specific sensing technology used.* "Occupancy Completion and Forecasting" is translated as "场景完成和预测" (scenario completion and forecasting) to refer to the combined task.* "Sparse-to-dense reconstruction" is translated as "稀疏到密集重建" (sparse-to-dense reconstruction) to refer to the challenge of reconstructing a dense 3D point cloud from a sparse set of measurements.* "Partial-to-complete hallucination" is translated as "部分到完整的幻觉" (partial-to-complete hallucination) to refer to the challenge of generating complete 3D information from partial measurements.* "3D-to-4D prediction" is translated as "3D到4D预测" (3D-to-4D prediction) to refer to the challenge of predicting the future 4D state of the environment based on current 3D information.* "OCFBench" is translated as "OCFBench" (OCFBench) to refer to the specific dataset used for evaluation.* "baseline models" is translated as "基线模型" (baseline models) to refer to the existing models used for comparison.* "our own ones" is translated as "我们自己的" (our own ones) to refer to the models developed by the authors.
</details></li>
</ul>
<hr>
<h2 id="Innovative-Methods-for-Non-Destructive-Inspection-of-Handwritten-Documents"><a href="#Innovative-Methods-for-Non-Destructive-Inspection-of-Handwritten-Documents" class="headerlink" title="Innovative Methods for Non-Destructive Inspection of Handwritten Documents"></a>Innovative Methods for Non-Destructive Inspection of Handwritten Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11217">http://arxiv.org/abs/2310.11217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Breci, Luca Guarnera, Sebastiano Battiato</li>
<li>for: 本研究旨在提高手写文档分析的精度和效率，以便通过评估手写文档的特征来确定作者身份。</li>
<li>methods: 本研究使用图像处理和深度学习技术EXTRACT AND ANALYZE handwritten manuscript documents的内在特征，包括文本行高、单词间距和字体大小。每个文档的特征向量最终包括每种类型的平均值和标准差。通过对比文档的特征向量的euclid distance，可以 объектив地确定作者身份。</li>
<li>results: 我们的实验结果表明，我们的方法可以对不同的写作媒体（包括手写纸和数字设备）进行 объектив的作者身份识别，并且超过了现有方法的性能。<details>
<summary>Abstract</summary>
Handwritten document analysis is an area of forensic science, with the goal of establishing authorship of documents through examination of inherent characteristics. Law enforcement agencies use standard protocols based on manual processing of handwritten documents. This method is time-consuming, is often subjective in its evaluation, and is not replicable. To overcome these limitations, in this paper we present a framework capable of extracting and analyzing intrinsic measures of manuscript documents related to text line heights, space between words, and character sizes using image processing and deep learning techniques. The final feature vector for each document involved consists of the mean and standard deviation for every type of measure collected. By quantifying the Euclidean distance between the feature vectors of the documents to be compared, authorship can be discerned. We also proposed a new and challenging dataset consisting of 362 handwritten manuscripts written on paper and digital devices by 124 different people. Our study pioneered the comparison between traditionally handwritten documents and those produced with digital tools (e.g., tablets). Experimental results demonstrate the ability of our method to objectively determine authorship in different writing media, outperforming the state of the art.
</details>
<details>
<summary>摘要</summary>
手写文档分析是法医科学领域中的一个领域，旨在透过评估手写文档的内在特征，以确定文档的作者。法律机关通常采用标准协议，基于手动处理手写文档。这种方法是时间consuming，容易受主观影响，并且不可重复。为了超越这些限制，在这篇论文中，我们提出了一个框架，可以提取和分析手写文档中相关的字体大小、词间距和字体大小等内在特征，使用图像处理和深度学习技术。每个文档的最终特征向量由每种测量类型的均值和标准差组成。通过计算这些特征向量之间的欧氏距离，可以bjectively Determine authorship。我们还提出了一个新的和挑战性的数据集，包含362份手写文档，由124名不同的人写作。我们的研究对手写文档和数字工具（例如平板电脑）生成的文档进行比较，并实验结果表明，我们的方法可以在不同的写作媒体中对作者进行 объекively 确定。
</details></li>
</ul>
<hr>
<h2 id="Learning-Comprehensive-Representations-with-Richer-Self-for-Text-to-Image-Person-Re-Identification"><a href="#Learning-Comprehensive-Representations-with-Richer-Self-for-Text-to-Image-Person-Re-Identification" class="headerlink" title="Learning Comprehensive Representations with Richer Self for Text-to-Image Person Re-Identification"></a>Learning Comprehensive Representations with Richer Self for Text-to-Image Person Re-Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11210">http://arxiv.org/abs/2310.11210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuanglin Yan, Neng Dong, Jun Liu, Liyan Zhang, Jinhui Tang</li>
<li>For: 本研究旨在提高文本检索到图像的精度，并解决现有方法对多视图匹配的缺乏考虑。* Methods: 我们提出了一种简单 yet effective的框架，即LCR$^2$S，它通过学习多视图modalities的承载表示来建立多对多匹配。我们还设计了一个多头注意力混合模块，以混合图像（文本）和其支持集。* Results: 我们的方法在三个popular TIReID数据集上实现了优秀的效果，并新创造了TIReID tasks的最佳表现。<details>
<summary>Abstract</summary>
Text-to-image person re-identification (TIReID) retrieves pedestrian images of the same identity based on a query text. However, existing methods for TIReID typically treat it as a one-to-one image-text matching problem, only focusing on the relationship between image-text pairs within a view. The many-to-many matching between image-text pairs across views under the same identity is not taken into account, which is one of the main reasons for the poor performance of existing methods. To this end, we propose a simple yet effective framework, called LCR$^2$S, for modeling many-to-many correspondences of the same identity by learning comprehensive representations for both modalities from a novel perspective. We construct a support set for each image (text) by using other images (texts) under the same identity and design a multi-head attentional fusion module to fuse the image (text) and its support set. The resulting enriched image and text features fuse information from multiple views, which are aligned to train a "richer" TIReID model with many-to-many correspondences. Since the support set is unavailable during inference, we propose to distill the knowledge learned by the "richer" model into a lightweight model for inference with a single image/text as input. The lightweight model focuses on semantic association and reasoning of multi-view information, which can generate a comprehensive representation containing multi-view information with only a single-view input to perform accurate text-to-image retrieval during inference. In particular, we use the intra-modal features and inter-modal semantic relations of the "richer" model to supervise the lightweight model to inherit its powerful capability. Extensive experiments demonstrate the effectiveness of LCR$^2$S, and it also achieves new state-of-the-art performance on three popular TIReID datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Whole-brain-radiomics-for-clustered-federated-personalization-in-brain-tumor-segmentation"><a href="#Whole-brain-radiomics-for-clustered-federated-personalization-in-brain-tumor-segmentation" class="headerlink" title="Whole-brain radiomics for clustered federated personalization in brain tumor segmentation"></a>Whole-brain radiomics for clustered federated personalization in brain tumor segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11480">http://arxiv.org/abs/2310.11480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthis Manthe, Stefan Duffner, Carole Lartizien</li>
<li>For: The paper focuses on mitigating the impact of statistical heterogeneity in federated learning for medical image segmentation.* Methods: The proposed method, called federated personalization, involves computing radiomic features and clustering analysis on each institution’s local dataset, followed by fine-tuning a global model using the clustered decentralized dataset.* Results: The proposed method was validated on the Federated Brain Tumor Segmentation 2022 Challenge dataset and showed improved performance compared to classical federated learning.Here are the three points in Simplified Chinese:* For: 本文针对适用于医疗影像分类的联合学习中的统计不确定性进行了研究，以提高联合学习的精度和效率。* Methods: 提议的方法是基于每个机构的本地数据进行调整，首先计算每个机构的对应数据，然后使用聚合分析将所有的特征向量转移到中央服务器，并使用每个中心 compute 的对应数据进行精度调整。* Results: 在适用于脑癌分类的 Federated Brain Tumor Segmentation 2022 Challenge 数据集上验证了提议的方法，并与传统的联合学习相比，获得了改善的性能。<details>
<summary>Abstract</summary>
Federated learning and its application to medical image segmentation have recently become a popular research topic. This training paradigm suffers from statistical heterogeneity between participating institutions' local datasets, incurring convergence slowdown as well as potential accuracy loss compared to classical training. To mitigate this effect, federated personalization emerged as the federated optimization of one model per institution. We propose a novel personalization algorithm tailored to the feature shift induced by the usage of different scanners and acquisition parameters by different institutions. This method is the first to account for both inter and intra-institution feature shift (multiple scanners used in a single institution). It is based on the computation, within each centre, of a series of radiomic features capturing the global texture of each 3D image volume, followed by a clustering analysis pooling all feature vectors transferred from the local institutions to the central server. Each computed clustered decentralized dataset (potentially including data from different institutions) then serves to finetune a global model obtained through classical federated learning. We validate our approach on the Federated Brain Tumor Segmentation 2022 Challenge dataset (FeTS2022). Our code is available at (https://github.com/MatthisManthe/radiomics_CFFL).
</details>
<details>
<summary>摘要</summary>
《联邦学习和它的医学图像分割应用已经在最近引起了广泛的研究兴趣。这种培训模式受到参与机构本地数据的统计差异的影响，会导致减速和溢出精度相比于传统培训。为了缓解这些效应，联邦个性化出现了，即在每个机构上进行联邦优化的一个模型。我们提出了一种新的个性化算法，专门针对不同扫描仪和获取参数导致的特征偏移。这种方法是首次考虑了多个机构的内部和外部特征偏移（多个扫描仪在同一个机构中使用）。它基于在每个中心计算的一系列各种激光特征，用于捕捉每个3D图像卷积的全局 текстура，然后对所有从本地机构传输到中央服务器的特征向量进行归一化分析。每个计算的归一化分析后的各个归一化分析结果（可能包括多个机构的数据）然后用于在经典联邦学习中进行精化。我们验证了我们的方法在2022年联邦大脑肿瘤分割挑战数据集（FeTS2022）上。我们的代码可以在（https://github.com/MatthisManthe/radiomics_CFFL）上获取。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Improving-Video-Deepfake-Detection-A-DCT-Based-Approach-with-Patch-Level-Analysis"><a href="#Improving-Video-Deepfake-Detection-A-DCT-Based-Approach-with-Patch-Level-Analysis" class="headerlink" title="Improving Video Deepfake Detection: A DCT-Based Approach with Patch-Level Analysis"></a>Improving Video Deepfake Detection: A DCT-Based Approach with Patch-Level Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11204">http://arxiv.org/abs/2310.11204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Guarnera, Salvatore Manganello, Sebastiano Battiato</li>
<li>for: 这篇研究旨在开发一个可靠、快速且可解释的检测深伪 Multimedia 内容的算法，以应对现在广泛存在的深伪技术滥用问题。</li>
<li>methods: 这篇研究使用了I-frames来提高计算和分析的速度，并分析了个别帧中的背景、脸部、眼睛、鼻子和口部，以找出最有可能性的特征。使用了Discrete Cosine Transform (DCT)来提取Beta комponents，并将其用作标准分类器（例如k-NN、SVM等）的输入，以决定解决这问题的最有可能性的频率。</li>
<li>results: 实验结果显示，眼和口部区域是检测深伪 Multimedia 内容中最有可能性的特征，能够更加可靠地决定影片的性质。提出的方法具有分析性、快速且不需要大量的计算资源。<details>
<summary>Abstract</summary>
The term deepfake refers to all those multimedia contents that were synthetically altered or created from scratch through the use of generative models. This phenomenon has become widespread due to the use of increasingly accurate and efficient architectures capable of rendering manipulated content indistinguishable from real content. In order to fight the illicit use of this powerful technology, it has become necessary to develop algorithms able to distinguish synthetic content from real ones. In this study, a new algorithm for the detection of deepfakes in digital videos is presented, focusing on the main goal of creating a fast and explainable method from a forensic perspective. To achieve this goal, the I-frames were extracted in order to provide faster computation and analysis than approaches described in literature. In addition, to identify the most discriminating regions within individual video frames, the entire frame, background, face, eyes, nose, mouth, and face frame were analyzed separately. From the Discrete Cosine Transform (DCT), the Beta components were extracted from the AC coefficients and used as input to standard classifiers (e.g., k-NN, SVM, and others) in order to identify those frequencies most discriminative for solving the task in question. Experimental results obtained on the Faceforensics++ and Celeb-DF (v2) datasets show that the eye and mouth regions are those most discriminative and able to determine the nature of the video with greater reliability than the analysis of the whole frame. The method proposed in this study is analytical, fast and does not require much computational power.
</details>
<details>
<summary>摘要</summary>
deepfake 指的是通过生成模型制造或修改 multimedia 内容的所有内容。由于使用的生成模型不断改进，使得修改后的内容与原始内容难以区分，因此需要开发一种能够分辨真实内容和修改后的内容的算法。本研究提出了一种用于检测数字视频中的深伪内容的新算法，强调实现快速和可解释的方法。为了实现这一目标，我们提取了 I-frame，以便更快地计算和分析，而不是按照文献中所描述的方法。此外，我们还分析了每帧视频中最有可能区分的地方，包括背景、脸、眼睛、鼻子和口。从 discrete cosine transform (DCT) 中，我们提取了 AC 约束中的 Beta 成分，并将其作为输入给标准分类器（如 k-NN、SVM 等），以确定解决当前问题中最有可能的频率。实验结果表明，脸部和口部是最有可能的区分点，能够更加可靠地判断视频的性质，而不是分析整个帧。该方法具有分析性、快速和不需要大量计算资源的优点。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Multi-Object-Render-and-Compare"><a href="#Sparse-Multi-Object-Render-and-Compare" class="headerlink" title="Sparse Multi-Object Render-and-Compare"></a>Sparse Multi-Object Render-and-Compare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11184">http://arxiv.org/abs/2310.11184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Langer, Ignas Budvytis, Roberto Cipolla</li>
<li>for: 这paper是为了解决单图像中物体的3D形态和姿态重建问题，这问题在机器人、增强现实和数字内容创建等领域都是非常重要的。</li>
<li>methods: 这paper使用了一种新的网络架构 called Multi-SPARC，它可以同时对多个检测到的物体进行CAD模型的对接。</li>
<li>results: 相比单视图方法，这paper在ScanNet dataset上达到了状态机器人性的表现，即从31.8%提高到40.3%的实例对接精度。<details>
<summary>Abstract</summary>
Reconstructing 3D shape and pose of static objects from a single image is an essential task for various industries, including robotics, augmented reality, and digital content creation. This can be done by directly predicting 3D shape in various representations or by retrieving CAD models from a database and predicting their alignments. Directly predicting 3D shapes often produces unrealistic, overly smoothed or tessellated shapes. Retrieving CAD models ensures realistic shapes but requires robust and accurate alignment. Learning to directly predict CAD model poses from image features is challenging and inaccurate. Works, such as ROCA, compute poses from predicted normalised object coordinates which can be more accurate but are susceptible to systematic failure. SPARC demonstrates that following a ''render-and-compare'' approach where a network iteratively improves upon its own predictions achieves accurate alignments. Nevertheless, it performs individual CAD alignment for every object detected in an image. This approach is slow when applied to many objects as the time complexity increases linearly with the number of objects and can not learn inter-object relations. Introducing a new network architecture Multi-SPARC we learn to perform CAD model alignments for multiple detected objects jointly. Compared to other single-view methods we achieve state-of-the-art performance on the challenging real-world dataset ScanNet. By improving the instance alignment accuracy from 31.8% to 40.3% we perform similar to state-of-the-art multi-view methods.
</details>
<details>
<summary>摘要</summary>
重建静止物体的3D形状和姿势从单个图像中是许多领域的关键任务，包括机器人、增强现实和数字内容创建。这可以通过直接预测3D形状或从数据库中检索CAD模型并预测其对齐来完成。直接预测3D形状常常生成不真实、过度缩短或分割的形状。从数据库中检索CAD模型可以保证真实的形状，但需要稳定和准确的对齐。学习直接从图像特征中预测CAD模型姿势是困难且不准确。ROCA等方法计算姿势从预测的 нормализованobject坐标，可以更准确但容易系统性失败。SPARC示例了一个“render-and-compare”方法，其中网络在自己的预测基础上进行多次改进，可以实现准确的对齐。然而，它每个图像中检测到的对象都进行个别CAD对齐，这会导致运行时间linearly增长与对象数量的线性关系，无法学习对象之间的关系。我们提出了一种新的网络架构 Multi-SPARC，可以同时对多个检测到的对象进行CAD模型对齐。与其他单视图方法相比，我们在真实的世界数据集ScanNet上 achieve state-of-the-art性能。我们从31.8%提高了实例对齐精度到40.3%，与多视图方法相当。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Pre-Training-Using-Masked-Autoencoders-for-ECG-Analysis"><a href="#Unsupervised-Pre-Training-Using-Masked-Autoencoders-for-ECG-Analysis" class="headerlink" title="Unsupervised Pre-Training Using Masked Autoencoders for ECG Analysis"></a>Unsupervised Pre-Training Using Masked Autoencoders for ECG Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11153">http://arxiv.org/abs/2310.11153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoxin Wang, Qingyuan Wang, Ganesh Neelakanta Iyer, Avishek Nag, Deepu John</li>
<li>for: 这篇论文的目的是提出一种基于masked autoencoder（MAE）的无监督预训技术，用于电气征ogram（ECG）信号的分析。</li>
<li>methods: 本论文使用的方法包括masked autoencoder（MAE）和任务特定的精致化。</li>
<li>results: 实验结果显示，使用本提案的方法可以在MITDB dataset上达到ECG预订分类任务的94.39%的精度。此外，该方法也在未见之数据中的分类性能比全监督方法更好。<details>
<summary>Abstract</summary>
Unsupervised learning methods have become increasingly important in deep learning due to their demonstrated large utilization of datasets and higher accuracy in computer vision and natural language processing tasks. There is a growing trend to extend unsupervised learning methods to other domains, which helps to utilize a large amount of unlabelled data. This paper proposes an unsupervised pre-training technique based on masked autoencoder (MAE) for electrocardiogram (ECG) signals. In addition, we propose a task-specific fine-tuning to form a complete framework for ECG analysis. The framework is high-level, universal, and not individually adapted to specific model architectures or tasks. Experiments are conducted using various model architectures and large-scale datasets, resulting in an accuracy of 94.39% on the MITDB dataset for ECG arrhythmia classification task. The result shows a better performance for the classification of previously unseen data for the proposed approach compared to fully supervised methods.
</details>
<details>
<summary>摘要</summary>
《深度学习中的无监督学习方法在最近几年变得越来越重要，因为它们在计算机视觉和自然语言处理任务中的精度高于监督学习方法。随着这些方法的扩展到其他领域，可以利用大量的无标签数据。这篇论文提出了基于屏蔽自动编码器（MAE）的无监督预训练技术，用于电cardiogram（ECG）信号分析。此外，我们还提出了任务特定的细化，以形成一个完整的ECG分析框架。这个框架是高级、通用、不具体适应特定的模型结构或任务。在各种模型结构和大规模数据集上进行了实验，实现了MITDB数据集上ECG动力痕迹分类任务的准确率为94.39%。结果显示，提posed方法对于处理前未见数据的分类表现更好于完全监督方法。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="BayesDiff-Estimating-Pixel-wise-Uncertainty-in-Diffusion-via-Bayesian-Inference"><a href="#BayesDiff-Estimating-Pixel-wise-Uncertainty-in-Diffusion-via-Bayesian-Inference" class="headerlink" title="BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference"></a>BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11142">http://arxiv.org/abs/2310.11142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siqi Kou, Lei Gan, Dequan Wang, Chongxuan Li, Zhijie Deng</li>
<li>for: 提高 diffusion 模型生成图像质量</li>
<li>methods: 基于 bayesian 推断，计算扩散过程中像素层次的不确定性</li>
<li>results: 能够减少低质量图像，并帮助改进成功图像和修正失败图像中的缺陷<details>
<summary>Abstract</summary>
Diffusion models have impressive image generation capability, but low-quality generations still exist, and their identification remains challenging due to the lack of a proper sample-wise metric. To address this, we propose BayesDiff, a pixel-wise uncertainty estimator for generations from diffusion models based on Bayesian inference. In particular, we derive a novel uncertainty iteration principle to characterize the uncertainty dynamics in diffusion, and leverage the last-layer Laplace approximation for efficient Bayesian inference. The estimated pixel-wise uncertainty can not only be aggregated into a sample-wise metric to filter out low-fidelity images but also aids in augmenting successful generations and rectifying artifacts in failed generations in text-to-image tasks. Extensive experiments demonstrate the efficacy of BayesDiff and its promise for practical applications.
</details>
<details>
<summary>摘要</summary>
Diffusion模型具有吸引人的图像生成能力，但低质量生成仍然存在，其标识仍然困难由于缺乏适当的样本级度指标。为解决这个问题，我们提出了 BayesDiff，一种基于泛函推理的像素级uncertainty估计器 дляDiffusion模型。具体来说，我们 derivate了一种新的uncertainty迭代原理来描述Diffusion中的uncertainty动态，并利用最后层拉пла斯批处理来实现高效的泛函推理。测试表明，BayesDiff可以不仅将像素级uncertainty聚合成样本级度指标来滤除低准确图像，还可以帮助改善成功生成的图像和修复失败生成的瑕疵。在文本到图像任务中，BayesDiff展示了其效果和实际应用潜力。
</details></li>
</ul>
<hr>
<h2 id="Super-resolution-of-histopathological-frozen-sections-via-deep-learning-preserving-tissue-structure"><a href="#Super-resolution-of-histopathological-frozen-sections-via-deep-learning-preserving-tissue-structure" class="headerlink" title="Super resolution of histopathological frozen sections via deep learning preserving tissue structure"></a>Super resolution of histopathological frozen sections via deep learning preserving tissue structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11112">http://arxiv.org/abs/2310.11112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elad Yoshai, Gil Goldinger, Miki Haifler, Natan T. Shaked</li>
<li>for:  histopathological frozen sections imaging, with a focus on achieving better distortion measures and reducing the risk of diagnostic misinterpretation.</li>
<li>methods:  deep-learning architecture that leverages loss functions in the frequency domain to generate high-resolution images while preserving critical image details.</li>
<li>results:  significant improvements in terms of Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR), as well as the preservation of details lost in low-resolution frozen-section images, which can affect pathologists’ clinical decisions.<details>
<summary>Abstract</summary>
Histopathology plays a pivotal role in medical diagnostics. In contrast to preparing permanent sections for histopathology, a time-consuming process, preparing frozen sections is significantly faster and can be performed during surgery, where the sample scanning time should be optimized. Super-resolution techniques allow imaging the sample in lower magnification and sparing scanning time. In this paper, we present a new approach to super resolution for histopathological frozen sections, with focus on achieving better distortion measures, rather than pursuing photorealistic images that may compromise critical diagnostic information. Our deep-learning architecture focuses on learning the error between interpolated images and real images, thereby it generates high-resolution images while preserving critical image details, reducing the risk of diagnostic misinterpretation. This is done by leveraging the loss functions in the frequency domain, assigning higher weights to the reconstruction of complex, high-frequency components. In comparison to existing methods, we obtained significant improvements in terms of Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR), as well as indicated details that lost in the low-resolution frozen-section images, affecting the pathologist's clinical decisions. Our approach has a great potential in providing more-rapid frozen-section imaging, with less scanning, while preserving the high resolution in the imaged sample.
</details>
<details>
<summary>摘要</summary>
Our deep-learning architecture is designed to learn the error between interpolated images and real images, generating high-resolution images while preserving critical image details. We leverage loss functions in the frequency domain, assigning higher weights to the reconstruction of complex, high-frequency components. Compared to existing methods, our approach achieves significant improvements in terms of Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR), as well as reveals details that were lost in low-resolution frozen-section images, which can affect the pathologist's clinical decisions.Our approach has great potential in providing rapid frozen-section imaging with less scanning, while preserving the high resolution of the imaged sample. This can improve the accuracy of medical diagnosis and treatment.
</details></li>
</ul>
<hr>
<h2 id="3D-Structure-guided-Network-for-Tooth-Alignment-in-2D-Photograph"><a href="#3D-Structure-guided-Network-for-Tooth-Alignment-in-2D-Photograph" class="headerlink" title="3D Structure-guided Network for Tooth Alignment in 2D Photograph"></a>3D Structure-guided Network for Tooth Alignment in 2D Photograph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11106">http://arxiv.org/abs/2310.11106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/douyl/2DToothAlignment">https://github.com/douyl/2DToothAlignment</a></li>
<li>paper_authors: Yulong Dou, Lanzhuju Mei, Dinggang Shen, Zhiming Cui</li>
<li>for: 这篇论文的目的是提供一个基于2D图像空间的牙齿协调网络，用于 dentist-patient 沟通和鼓励病人接受 ortodontic 治疗。</li>
<li>methods: 这篇论文使用了3D intra-oral scanning models 收集在 clinic 中，并使用了一个对应关系学习模型来将预先和后 ortodontic 治疗的3D 牙齿结构与2D 牙齿 outline 进行映射。然后，使用一个演化模型来将牙齿 outline 调整为具有美观和排列的牙齿图像。</li>
<li>results: 这篇论文的结果显示了该网络在不同的 facial photographs 上的优秀表现和强大应用性，并且可以帮助 dentist 快速地生成一个美观和排列的牙齿图像，以便更好地与病人沟通和鼓励病人接受 ortodontic 治疗。<details>
<summary>Abstract</summary>
Orthodontics focuses on rectifying misaligned teeth (i.e., malocclusions), affecting both masticatory function and aesthetics. However, orthodontic treatment often involves complex, lengthy procedures. As such, generating a 2D photograph depicting aligned teeth prior to orthodontic treatment is crucial for effective dentist-patient communication and, more importantly, for encouraging patients to accept orthodontic intervention. In this paper, we propose a 3D structure-guided tooth alignment network that takes 2D photographs as input (e.g., photos captured by smartphones) and aligns the teeth within the 2D image space to generate an orthodontic comparison photograph featuring aesthetically pleasing, aligned teeth. Notably, while the process operates within a 2D image space, our method employs 3D intra-oral scanning models collected in clinics to learn about orthodontic treatment, i.e., projecting the pre- and post-orthodontic 3D tooth structures onto 2D tooth contours, followed by a diffusion model to learn the mapping relationship. Ultimately, the aligned tooth contours are leveraged to guide the generation of a 2D photograph with aesthetically pleasing, aligned teeth and realistic textures. We evaluate our network on various facial photographs, demonstrating its exceptional performance and strong applicability within the orthodontic industry.
</details>
<details>
<summary>摘要</summary>
Orthodontics 专注于 corrections 不对称牙齿（即 malocclusion），影响咀嚼功能和美观。然而，orthodontic 治疗经常包括复杂、长时间的过程。因此，生成一张显示牙齿调整后的2D照片是关键的，以便dentist和病人之间有效沟通，更重要的是，使病人accept orthodontic intervention。在这篇论文中，我们提议一个基于3D结构的牙齿调整网络，输入2D照片（例如，由智能手机拍摄的照片），并将牙齿在2D图像空间中调整，生成一张 featuring 美观、调整后的牙齿的orthodontic comparison照片。需要注意的是，我们的过程在2D图像空间中进行，但我们使用了3D intra-oral scanning模型，收集在临床中，以学习orthodontic treatment。具体来说，我们将预 orthodontic 和后 orthodontic 3D 牙齿结构投影到2D 牙齿轮廓上，然后使用一种扩散模型来学习 mapping 关系。最后，我们使用了调整后的牙齿轮廓来指导生成一张 featuring 美观、调整后的牙齿和实际 Texture的2D照片。我们对多张人脸照片进行了评估，并证明了我们的网络在orthodontic 行业中表现出色，有强大的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Generalizability-of-CNN-Architectures-for-Face-Morph-Presentation-Attack"><a href="#Generalizability-of-CNN-Architectures-for-Face-Morph-Presentation-Attack" class="headerlink" title="Generalizability of CNN Architectures for Face Morph Presentation Attack"></a>Generalizability of CNN Architectures for Face Morph Presentation Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11105">http://arxiv.org/abs/2310.11105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sherko R. HmaSalah, Aras Asaad</li>
<li>for: 防止犯罪分子使用假识别信息越境</li>
<li>methods: 使用Convolutional Neural Network (CNN)模型进行人脸识别，并 investigate CNN模型在各种数据集上的泛化能力</li>
<li>results: InceptionResNet-v2模型在多个数据集上表现出最好的泛化能力，并在人脸识别 task 中获得了最高的性能<details>
<summary>Abstract</summary>
Automatic border control systems are wide spread in modern airports worldwide. Morphing attacks on face biometrics is a serious threat that undermines the security and reliability of face recognition systems deployed in airports and border controls. Therefore, developing a robust Machine Learning (ML) system is necessary to prevent criminals crossing borders with fake identifications especially since it has been shown that security officers cannot detect morphs better than machines. In this study, we investigate the generalization power of Convolutional Neural Network (CNN) architectures against morphing attacks. The investigation utilizes 5 distinct CNNs namely ShuffleNet, DenseNet201, VGG16, EffecientNet-B0 and InceptionResNet-v2. Each CNN architecture represents a well-known family of CNN models in terms of number of parameters, architectural design and performance across various computer vision applications. To ensure robust evaluation, we employ 4 different datasets (Utrecht, London, Defacto and KurdFace) that contain a diverse range of digital face images which cover variations in ethnicity, gender, age, lighting condition and camera setting. One of the fundamental concepts of ML system design is the ability to generalize effectively to previously unseen data, hence not only we evaluate the performance of CNN models within individual datasets but also explore their performance across combined datasets and investigating each dataset in testing phase only. Experimental results on more than 8 thousand images (genuine and morph) from the 4 datasets show that InceptionResNet-v2 generalizes better to unseen data and outperforms the other 4 CNN models.
</details>
<details>
<summary>摘要</summary>
现代机场中的自动边境控制系统广泛应用。但 morphing 攻击对于面部biometrics 是一种严重的威胁，这会使面 recognition 系统在机场和边境控制中受到影响。为了防止罪犯使用假身份证件越境，需要开发一个可靠的机器学习（ML）系统。在这项研究中，我们研究了 CNN 架构对 morphing 攻击的普适性。我们使用 5 种不同的 CNN 模型，即 ShuffleNet、DenseNet201、VGG16、EfficientNet-B0 和 InceptionResNet-v2。每种 CNN 模型都代表了不同的参数量、架构设计和在不同计算机视觉应用中的性能。为了有效评估，我们使用 4 个不同的数据集（UTrecht、London、Defacto 和 KurdFace），这些数据集包含了不同的民族、性别、年龄、照明条件和摄像头设置。 ML 系统设计的一个基本原则是能够有效地普退到未见数据，因此我们不仅在单个数据集中评估 CNN 模型的性能，还在将数据集组合起来评估它们的总体性能。实验结果表明，InceptionResNet-v2 在未见数据中普退性能最好，并且在4个数据集中的测试阶段也表现出色，超过其他 4 种 CNN 模型。
</details></li>
</ul>
<hr>
<h2 id="SODA-Robust-Training-of-Test-Time-Data-Adaptors"><a href="#SODA-Robust-Training-of-Test-Time-Data-Adaptors" class="headerlink" title="SODA: Robust Training of Test-Time Data Adaptors"></a>SODA: Robust Training of Test-Time Data Adaptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11093">http://arxiv.org/abs/2310.11093</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tmlr-group/soda">https://github.com/tmlr-group/soda</a></li>
<li>paper_authors: Zige Wang, Yonggang Zhang, Zhen Fang, Long Lan, Wenjing Yang, Bo Han</li>
<li>For: The paper aims to mitigate the performance degradation caused by distribution shifts in machine learning models, specifically by adapting models deployed to test distributions.* Methods: The paper proposes a method called pseudo-label-robust data adaptation (SODA) that utilizes zeroth-order optimization (ZOO) to train a data adaptor to adapt the test data to fit the deployed models. SODA addresses the issue of unreliable gradients in ZOO by using high-confidence predicted labels as reliable labels to optimize the data adaptor.* Results: The paper shows that SODA can significantly enhance the performance of deployed models in the presence of distribution shifts without requiring access to model parameters. Empirical results indicate that SODA can improve the performance of data adaptation.Here are the three key points in Simplified Chinese text:* For: 这个论文的目标是解决机器学习模型在分布Shift时的性能下降问题，specifically by adapting deployed models to test distributions.* Methods: 论文提出了一种方法called pseudo-label-robust data adaptation (SODA)，which utilizes zeroth-order optimization (ZOO) to train a data adaptor to adapt the test data to fit the deployed models. SODA Addresses the issue of unreliable gradients in ZOO by using high-confidence predicted labels as reliable labels to optimize the data adaptor.* Results: 论文显示SODA可以Significantly enhance deployed models在分布Shift情况下的性能，without requiring access to model parameters. Empirical results indicate that SODA can improve the performance of data adaptation.<details>
<summary>Abstract</summary>
Adapting models deployed to test distributions can mitigate the performance degradation caused by distribution shifts. However, privacy concerns may render model parameters inaccessible. One promising approach involves utilizing zeroth-order optimization (ZOO) to train a data adaptor to adapt the test data to fit the deployed models. Nevertheless, the data adaptor trained with ZOO typically brings restricted improvements due to the potential corruption of data features caused by the data adaptor. To address this issue, we revisit ZOO in the context of test-time data adaptation. We find that the issue directly stems from the unreliable estimation of the gradients used to optimize the data adaptor, which is inherently due to the unreliable nature of the pseudo-labels assigned to the test data. Based on this observation, we propose pseudo-label-robust data adaptation (SODA) to improve the performance of data adaptation. Specifically, SODA leverages high-confidence predicted labels as reliable labels to optimize the data adaptor with ZOO for label prediction. For data with low-confidence predictions, SODA encourages the adaptor to preserve data information to mitigate data corruption. Empirical results indicate that SODA can significantly enhance the performance of deployed models in the presence of distribution shifts without requiring access to model parameters.
</details>
<details>
<summary>摘要</summary>
适应已部署的模型可以减轻由分布shift引起的性能下降。然而，隐私问题可能使模型参数无法访问。一种有 promise的方法是使用零次优化（ZOO）来训练一个数据适应器，以适应已部署的模型。然而，通常情况下，ZOO 训练的数据适应器 Typically brings restricted improvements due to the potential corruption of data features caused by the data adaptor。为Address this issue, we revisit ZOO in the context of test-time data adaptation. We find that the issue directly stems from the unreliable estimation of the gradients used to optimize the data adaptor, which is inherently due to the unreliable nature of the pseudo-labels assigned to the test data。 Based on this observation, we propose pseudo-label-robust data adaptation (SODA) to improve the performance of data adaptation。 Specifically, SODA leverages high-confidence predicted labels as reliable labels to optimize the data adaptor with ZOO for label prediction。 For data with low-confidence predictions, SODA encourages the adaptor to preserve data information to mitigate data corruption。 Empirical results indicate that SODA can significantly enhance the performance of deployed models in the presence of distribution shifts without requiring access to model parameters。
</details></li>
</ul>
<hr>
<h2 id="DORec-Decomposed-Object-Reconstruction-Utilizing-2D-Self-Supervised-Features"><a href="#DORec-Decomposed-Object-Reconstruction-Utilizing-2D-Self-Supervised-Features" class="headerlink" title="DORec: Decomposed Object Reconstruction Utilizing 2D Self-Supervised Features"></a>DORec: Decomposed Object Reconstruction Utilizing 2D Self-Supervised Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11092">http://arxiv.org/abs/2310.11092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Wu, Sicheng Li, Sihui Ji, Yue Wang, Rong Xiong, Yiyi Liao</li>
<li>for: 提高对复杂背景下对象的分解和重建的精度</li>
<li>methods: 基于神经网络隐式表示的Decomposed Object Reconstruction（DORec）网络，利用2D自动学习的自然特征进行分解</li>
<li>results: 在多个 dataset 上实现对eground object的高精度分解和重建<details>
<summary>Abstract</summary>
Decomposing a target object from a complex background while reconstructing is challenging. Most approaches acquire the perception for object instances through the use of manual labels, but the annotation procedure is costly. The recent advancements in 2D self-supervised learning have brought new prospects to object-aware representation, yet it remains unclear how to leverage such noisy 2D features for clean decomposition. In this paper, we propose a Decomposed Object Reconstruction (DORec) network based on neural implicit representations. Our key idea is to transfer 2D self-supervised features into masks of two levels of granularity to supervise the decomposition, including a binary mask to indicate the foreground regions and a K-cluster mask to indicate the semantically similar regions. These two masks are complementary to each other and lead to robust decomposition. Experimental results show the superiority of DORec in segmenting and reconstructing the foreground object on various datasets.
</details>
<details>
<summary>摘要</summary>
分解一个目标对象从复杂背景中分离，而重建时也是一项挑战。大多数方法通过使用手动标注来获得对象实例的感知，但标注过程很昂贵。现代2D自助学习技术的发展带来了新的可能性，但是如何利用这些噪音2D特征来获得清晰的分解仍然是一个未知。本文提出了基于神经无限表示的分解对象网络（DORec），我们的关键想法是将2D自助学习特征转换成两级划分的mask，包括一个二进制划分用于指示前景区域，以及一个K-集群划分用于指示相似区域。这两个划分是相互补偿的，导致了稳定的分解。实验结果表明DORec在不同的数据集上 segment和重建前景对象表现出色。
</details></li>
</ul>
<hr>
<h2 id="United-We-Stand-Using-Epoch-wise-Agreement-of-Ensembles-to-Combat-Overfit"><a href="#United-We-Stand-Using-Epoch-wise-Agreement-of-Ensembles-to-Combat-Overfit" class="headerlink" title="United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit"></a>United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11077">http://arxiv.org/abs/2310.11077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uri Stern, Daniel Shwartz, Daphna Weinshall</li>
<li>for: 该论文的目的是解决深度神经网络在图像分类任务中遇到的过拟合问题，提出了一种新的深度网络集成预测方法来避免过拟合。</li>
<li>methods: 该论文使用了一种新的集成预测方法，该方法基于论文中提出的一种回归模型的预测结果，其中预测结果表明过拟合时，类别分类器的变异度会增加。</li>
<li>results: 在多个图像和文本分类 dataset 上，该方法可以减少过拟合导致的generalization下降，并且在一些情况下，甚至超越了使用 early stopping 的性能。该方法易于实现，可以与任何训练方案和架构结合使用，无需额外的特殊知识。<details>
<summary>Abstract</summary>
Deep neural networks have become the method of choice for solving many image classification tasks, largely because they can fit very complex functions defined over raw images. The downside of such powerful learners is the danger of overfitting the training set, leading to poor generalization, which is usually avoided by regularization and "early stopping" of the training. In this paper, we propose a new deep network ensemble classifier that is very effective against overfit. We begin with the theoretical analysis of a regression model, whose predictions - that the variance among classifiers increases when overfit occurs - is demonstrated empirically in deep networks in common use. Guided by these results, we construct a new ensemble-based prediction method designed to combat overfit, where the prediction is determined by the most consensual prediction throughout the training. On multiple image and text classification datasets, we show that when regular ensembles suffer from overfit, our method eliminates the harmful reduction in generalization due to overfit, and often even surpasses the performance obtained by early stopping. Our method is easy to implement, and can be integrated with any training scheme and architecture, without additional prior knowledge beyond the training set. Accordingly, it is a practical and useful tool to overcome overfit.
</details>
<details>
<summary>摘要</summary>
深度神经网络已成为许多图像分类任务的方法选择，主要是因为它们可以适应非常复杂的图像函数。但是这些强大的学习者也存在过拟合风险，导致泛化性差，通常通过规范和"早停止"等方法来避免。在这篇论文中，我们提出了一种新的深度网络集成分类器，可以很好地避免过拟合。我们从理论分析中开始，对于过拟合情况下的回归模型，其预测结果表明，过拟合时，类ifier的差异量会增加。基于这些结果，我们构建了一种新的集成预测方法，通过在训练过程中确定最一致的预测来对抗过拟合。在多个图像和文本分类 dataset 上，我们证明了，当常见集成遭到过拟合时，我们的方法可以消除过拟合导致的泛化性下降，并经常超越通过早停止获得的性能。我们的方法易于实现，可以与任何训练方案和架构结合使用，无需额外的优先知识，只需要训练集。因此，它是一种实用和有用的工具，可以解决过拟合问题。
</details></li>
</ul>
<hr>
<h2 id="k-t-CLAIR-Self-Consistency-Guided-Multi-Prior-Learning-for-Dynamic-Parallel-MR-Image-Reconstruction"><a href="#k-t-CLAIR-Self-Consistency-Guided-Multi-Prior-Learning-for-Dynamic-Parallel-MR-Image-Reconstruction" class="headerlink" title="$k$-$t$ CLAIR: Self-Consistency Guided Multi-Prior Learning for Dynamic Parallel MR Image Reconstruction"></a>$k$-$t$ CLAIR: Self-Consistency Guided Multi-Prior Learning for Dynamic Parallel MR Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11050">http://arxiv.org/abs/2310.11050</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lpzhang/ktCLAIR">https://github.com/lpzhang/ktCLAIR</a></li>
<li>paper_authors: Liping Zhang, Weitian Chen</li>
<li>for: 用于快速推断心脏疾病的临床诊断</li>
<li>methods: 使用自适应启发式多优先学习框架$k$-$t$CLAIR，利用高度强化的数据来推导动态平行MRI重建</li>
<li>results: 实验结果表明，$k$-$t$CLAIR可以在心脏动态MRI重建中实现高质量的重建，并且与量化和质量方面的表现均有显著改善<details>
<summary>Abstract</summary>
Cardiac magnetic resonance imaging (CMR) has been widely used in clinical practice for the medical diagnosis of cardiac diseases. However, the long acquisition time hinders its development in real-time applications. Here, we propose a novel self-consistency guided multi-prior learning framework named $k$-$t$ CLAIR to exploit spatiotemporal correlations from highly undersampled data for accelerated dynamic parallel MRI reconstruction. The $k$-$t$ CLAIR progressively reconstructs faithful images by leveraging multiple complementary priors learned in the $x$-$t$, $x$-$f$, and $k$-$t$ domains in an iterative fashion, as dynamic MRI exhibits high spatiotemporal redundancy. Additionally, $k$-$t$ CLAIR incorporates calibration information for prior learning, resulting in a more consistent reconstruction. Experimental results on cardiac cine and T1W/T2W images demonstrate that $k$-$t$ CLAIR achieves high-quality dynamic MR reconstruction in terms of both quantitative and qualitative performance.
</details>
<details>
<summary>摘要</summary>
cardiac magnetic resonance imaging (CMR) 已经广泛应用在临床实践中用于医疗诊断心脏疾病。然而，长期获取时间限制了其在实时应用中的发展。我们提议一种新的自适应性导向多优先学习框架，名为 $k$-$t$ CLAIR，以利用高度减掉样本数据中的空间时间相关性进行加速的动态平行MRI重建。 $k$-$t$ CLAIR 逐渐重建准确的图像，利用在 $x$-$t$, $x$-$f$, 和 $k$-$t$ 领域中学习的多个补做先天知识，因为动态MRI在空间时间上具有高度相似性。此外， $k$-$t$ CLAIR 还包含了准确性信息 для先天学习，从而使得重建更加一致。实验结果表明， $k$-$t$ CLAIR 在心脏笔记和 T1W/T2W 图像上达到了高质量的动态MR重建， Both quantitative and qualitative performance。
</details></li>
</ul>
<hr>
<h2 id="Co-Learning-Semantic-aware-Unsupervised-Segmentation-for-Pathological-Image-Registration"><a href="#Co-Learning-Semantic-aware-Unsupervised-Segmentation-for-Pathological-Image-Registration" class="headerlink" title="Co-Learning Semantic-aware Unsupervised Segmentation for Pathological Image Registration"></a>Co-Learning Semantic-aware Unsupervised Segmentation for Pathological Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11040">http://arxiv.org/abs/2310.11040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Liu, Shi Gu</li>
<li>for: 本研究旨在提出一种不需要标注数据的自动化肿瘤图像注册方法，以解决肿瘤图像注册问题中的焦点缺失和肿瘤图像异常扭曲问题。</li>
<li>methods: 本研究提出了一种基于生成、填充和注册（GIR）原理的自动化肿瘤图像注册方法，包括注册、分割和填充三个模块，通过同时训练这三个模块，以提高肿瘤图像分割和注册的精度。</li>
<li>results: 实验结果表明，提出的方法可以准确地注册肿瘤图像，并且可以在不同的成像模式下检测肿瘤。 code available at <a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/GIRNet%E3%80%82">https://github.com/brain-intelligence-lab/GIRNet。</a><details>
<summary>Abstract</summary>
The registration of pathological images plays an important role in medical applications. Despite its significance, most researchers in this field primarily focus on the registration of normal tissue into normal tissue. The negative impact of focal tissue, such as the loss of spatial correspondence information and the abnormal distortion of tissue, are rarely considered. In this paper, we propose GIRNet, a novel unsupervised approach for pathological image registration by incorporating segmentation and inpainting through the principles of Generation, Inpainting, and Registration (GIR). The registration, segmentation, and inpainting modules are trained simultaneously in a co-learning manner so that the segmentation of the focal area and the registration of inpainted pairs can improve collaboratively. Overall, the registration of pathological images is achieved in a completely unsupervised learning framework. Experimental results on multiple datasets, including Magnetic Resonance Imaging (MRI) of T1 sequences, demonstrate the efficacy of our proposed method. Our results show that our method can accurately achieve the registration of pathological images and identify lesions even in challenging imaging modalities. Our unsupervised approach offers a promising solution for the efficient and cost-effective registration of pathological images. Our code is available at https://github.com/brain-intelligence-lab/GIRNet.
</details>
<details>
<summary>摘要</summary>
注册病理图像在医疗应用中扮演着重要的角色。尽管其重要性，大多数研究人员在这个领域主要关注normal tissue到normal tissue的注册。病理区域的负面影响，如损失的空间匹配信息和病理区域的异常扭曲，几乎不被考虑。在这篇论文中，我们提出了GIRNet，一种新的无监督方法，通过生成、填充和注册（GIR）原理，以帮助病理图像注册。注册、分割和填充模块在一起训练，以便在合作方式下提高病理区域的分割和注册匹配的精度。总之，我们的提出的方法可以在完全无监督学习框架下完成病理图像注册。我们的实验结果表明，我们的方法可以准确地注册病理图像，并在具有挑战性的成像模式下识别病理区域。我们的无监督方法可以提供高效、成本效果的病理图像注册解决方案。我们的代码可以在https://github.com/brain-intelligence-lab/GIRNet上获取。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-Using-Large-Pretrained-Models-with-Mixture-of-Adapters"><a href="#Domain-Generalization-Using-Large-Pretrained-Models-with-Mixture-of-Adapters" class="headerlink" title="Domain Generalization Using Large Pretrained Models with Mixture-of-Adapters"></a>Domain Generalization Using Large Pretrained Models with Mixture-of-Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11031">http://arxiv.org/abs/2310.11031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gyuseong Lee, Wooseok Jang, Jin Hyeon Kim, Jaewoo Jung, Seungryong Kim</li>
<li>for: 这个论文的目的是提出一种可以实现执行环境中模型的稳定性和可靠性的预测模型，尤其是在域别测量（Domain Generalization，DG）任务中。</li>
<li>methods: 这个论文使用了parameter-efficient fine-tuning（PEFT）方法，并且在PEFT中使用了束缚器（adapter）来实现更好的稳定性和可靠性。另外，这个论文还提出了一种mixture-of-expert（MoA）方法，这是一种将多个束缚器（adapter）组合起来，以提高模型的稳定性和可靠性。</li>
<li>results: 这个论文的结果显示，使用PEFT和MoA方法可以实现更好的稳定性和可靠性，并且可以减少模型的训练时间和计算成本。另外，这个论文还证明了，在域别测量任务中，使用PEFT和MoA方法可以提高模型的性能，并且可以减少模型的性能下降。<details>
<summary>Abstract</summary>
Learning a robust vision model despite large distribution shift is essential for model deployment in real-world settings. Especially, domain generalization (DG) algorithm aims to maintain the performance of a trained model on different distributions which were not seen during training. One of the most effective methods has been leveraging the already learned rich knowledge of large pretrained models. However, naively fine-tuning large models to DG tasks is often practically infeasible due to memory limitations, extensive time requirements for training, and the risk of learned knowledge deterioration. Recently, parameter-efficient fine-tuning (PEFT) methods have been proposed to reduce the high computational cost during training and efficiently adapt large models to downstream tasks. In this work, for the first time, we find that the use of adapters in PEFT methods not only reduce high computational cost during training but also serve as an effective regularizer for DG tasks. Surprisingly, a naive adapter implementation for large models achieve superior performance on common datasets. However, in situations of large distribution shifts, additional factors such as optimal amount of regularization due to the strength of distribution shifts should be considered for a sophisticated adapter implementation. To address this, we propose a mixture-of-expert based adapter fine-tuning method, dubbed as mixture-of-adapters (MoA). Specifically, we employ multiple adapters that have varying capacities, and by using learnable routers, we allocate each token to a proper adapter. By using both PEFT and MoA methods, we effectively alleviate the performance deterioration caused by distribution shifts and achieve state-of-the-art performance on diverse DG benchmarks.
</details>
<details>
<summary>摘要</summary>
学习一个强健的视觉模型，尤其是在不同的分布下进行模型部署，是实际应用中非常重要的。域外泛化（DG）算法的目标是保持训练后的模型在不同的分布下保持性能。然而，直接将大型模型精细调整到DG任务是实际上不可行，因为内存限制、训练时间的投入和模型学习知识的削弱。最近，参数效率的调整方法（PEFT）被提出，以减少训练时间的计算成本并有效地适应大型模型下推理任务。在这项工作中，我们发现了使用适应器不仅可以减少训练时间的计算成本，还可以作为DG任务的有效常规化。 surprisingly，一个简单的适应器实现对常用 datasets 表现出色。然而，在大分布差情况下，需要考虑适当的补偿因子，以适应强大的分布差。为此，我们提出了一种mixture-of-expert（MoA）适应器细化方法，其中我们采用多个适应器，每个适应器有不同的容量，并通过learnable routers来分配每个 токен到合适的适应器。通过使用 PEFT 和 MoA 方法，我们有效地减少分布差引起的性能下降，并在多种 DG  bencmarks 上达到了国际首席性表现。
</details></li>
</ul>
<hr>
<h2 id="NICE-Improving-Panoptic-Narrative-Detection-and-Segmentation-with-Cascading-Collaborative-Learning"><a href="#NICE-Improving-Panoptic-Narrative-Detection-and-Segmentation-with-Cascading-Collaborative-Learning" class="headerlink" title="NICE: Improving Panoptic Narrative Detection and Segmentation with Cascading Collaborative Learning"></a>NICE: Improving Panoptic Narrative Detection and Segmentation with Cascading Collaborative Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10975">http://arxiv.org/abs/2310.10975</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mr-neko/nice">https://github.com/mr-neko/nice</a></li>
<li>paper_authors: Haowei Wang, Jiayi Ji, Tianyu Guo, Yilong Yang, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji</li>
<li>for: 本研究旨在提出一个统一的和有效的框架，可以同时进行多个目标的图像识别和位置探索，并且可以将它们与文本描述相互对应。</li>
<li>methods: 我们提出了一个名为NICE的框架，它可以同时进行当地描述的识别和分类，并且可以通过将它们与文本描述相互对应，从而提高表现。在这个框架中，我们引入了两个协调模组，它们是协调调节（CGA）和本体驱动的本地化（BDL），它们负责分类和检测。我们还引入了一个将PNS和PND串接在一起的方法，使得它们可以相互对应，并且允许它们相互补偿以提高表现。</li>
<li>results: 我们的方法NICE可以轻松地超越所有现有的方法，实现4.1%的PND和2.9%的PNS表现。这些结果证明了我们的提出的协调学习策略的有效性。<details>
<summary>Abstract</summary>
Panoptic Narrative Detection (PND) and Segmentation (PNS) are two challenging tasks that involve identifying and locating multiple targets in an image according to a long narrative description. In this paper, we propose a unified and effective framework called NICE that can jointly learn these two panoptic narrative recognition tasks. Existing visual grounding tasks use a two-branch paradigm, but applying this directly to PND and PNS can result in prediction conflict due to their intrinsic many-to-many alignment property. To address this, we introduce two cascading modules based on the barycenter of the mask, which are Coordinate Guided Aggregation (CGA) and Barycenter Driven Localization (BDL), responsible for segmentation and detection, respectively. By linking PNS and PND in series with the barycenter of segmentation as the anchor, our approach naturally aligns the two tasks and allows them to complement each other for improved performance. Specifically, CGA provides the barycenter as a reference for detection, reducing BDL's reliance on a large number of candidate boxes. BDL leverages its excellent properties to distinguish different instances, which improves the performance of CGA for segmentation. Extensive experiments demonstrate that NICE surpasses all existing methods by a large margin, achieving 4.1% for PND and 2.9% for PNS over the state-of-the-art. These results validate the effectiveness of our proposed collaborative learning strategy. The project of this work is made publicly available at https://github.com/Mr-Neko/NICE.
</details>
<details>
<summary>摘要</summary>
通用和有效的框架NICE（Nice In Coordinate Embedding）可以同时学习多个目标的涵义和位置识别 task。在这篇论文中，我们提出了一种解决方案，即在Panoptic Narrative Detection（PND）和Panoptic Segmentation（PNS）任务之间进行协同学习。现有的视觉定位任务使用两支分支方法，但是直接应用这种方法于PND和PNS可能会导致预测冲突，因为它们具有内在的多对多对应性。为解决这个问题，我们引入了两个协同模块，即坐标导航集成（CGA）和坐标驱动本地化（BDL），负责分割和检测。我们将PNS和PND串联在一起，使得两个任务之间的对应关系自然地进行了协同学习。具体来说，CGA提供了分割的参考点，从而降低BDL的候选框数量的依赖性。BDL利用其优秀的性能来分辨不同的实例，从而提高CGA的分割性能。我们的实验结果表明，NICE比所有现有方法大幅超越，实现了PND4.1%和PNS2.9%的状态当前最佳性能。这些结果证明了我们提出的协同学习策略的有效性。NICE项目的代码可以在https://github.com/Mr-Neko/NICE上获取。
</details></li>
</ul>
<hr>
<h2 id="Tracking-and-Mapping-in-Medical-Computer-Vision-A-Review"><a href="#Tracking-and-Mapping-in-Medical-Computer-Vision-A-Review" class="headerlink" title="Tracking and Mapping in Medical Computer Vision: A Review"></a>Tracking and Mapping in Medical Computer Vision: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11475">http://arxiv.org/abs/2310.11475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Schmidt, Omid Mohareri, Simon DiMaio, Michael Yip, Septimiu E. Salcudean<br>for:* 医疗图像分析领域的应用，包括诊断和手术指导。methods:* 使用摄像头进行跟踪和场景映射。results:* 提供了一个审查和概述当前领域的状态，包括最新的发展和趋势。Please note that the above information is in Simplified Chinese text.<details>
<summary>Abstract</summary>
As computer vision algorithms are becoming more capable, their applications in clinical systems will become more pervasive. These applications include diagnostics such as colonoscopy and bronchoscopy, guiding biopsies and minimally invasive interventions and surgery, automating instrument motion and providing image guidance using pre-operative scans. Many of these applications depend on the specific visual nature of medical scenes and require designing and applying algorithms to perform in this environment.   In this review, we provide an update to the field of camera-based tracking and scene mapping in surgery and diagnostics in medical computer vision. We begin with describing our review process, which results in a final list of 515 papers that we cover. We then give a high-level summary of the state of the art and provide relevant background for those who need tracking and mapping for their clinical applications. We then review datasets provided in the field and the clinical needs therein. Then, we delve in depth into the algorithmic side, and summarize recent developments, which should be especially useful for algorithm designers and to those looking to understand the capability of off-the-shelf methods. We focus on algorithms for deformable environments while also reviewing the essential building blocks in rigid tracking and mapping since there is a large amount of crossover in methods. Finally, we discuss the current state of the tracking and mapping methods along with needs for future algorithms, needs for quantification, and the viability of clinical applications in the field. We conclude that new methods need to be designed or combined to support clinical applications in deformable environments, and more focus needs to be put into collecting datasets for training and evaluation.
</details>
<details>
<summary>摘要</summary>
为了满足医疗系统中computer vision算法的应用 becoming more pervasive，这些应用包括诊断如colonoscopy和bronchoscopy、导引生物检查和微创入侵性手术、自动化工具动作并提供预操作扫描图像导航。许多这些应用需要特定的医疗场景的视觉特性，因此需要设计和应用算法以在这种环境中工作。在这篇评论中，我们提供了医疗计算机视觉中摄像机基于跟踪和场景映射的更新。我们开始介绍我们的评审过程，从而获得了515篇论文的最终列表。然后，我们提供了高级概述，并为需要跟踪和映射的价值读者提供了相关的背景信息。然后，我们评审了在领域中提供的数据集，并评估了临床应用中的临床需求。接着，我们深入探讨算法的方面，并总结了最近的进展，这将对算法设计者和需要了解摄像机基于跟踪和映射的方法来说特别有用。我们主要关注可变环境中的算法，并同时评估了基础建立的硬件跟踪和映射方法。最后，我们讨论了当前跟踪和映射方法的状况，以及未来需要的算法、评估量化和临床应用的可行性。我们结论认为，新的方法需要被设计或组合以支持临床应用，并更多的精力需要投入到训练和评估数据集的收集中。
</details></li>
</ul>
<hr>
<h2 id="Context-Aware-Meta-Learning"><a href="#Context-Aware-Meta-Learning" class="headerlink" title="Context-Aware Meta-Learning"></a>Context-Aware Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10971">http://arxiv.org/abs/2310.10971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hallogameboy/MARU">https://github.com/hallogameboy/MARU</a></li>
<li>paper_authors: Christopher Fifty, Dennis Duan, Ronald G. Junkins, Ehsan Amid, Jure Leskovec, Christopher Ré, Sebastian Thrun</li>
<li>for: 这个论文旨在解决视觉模型在推理过程中学习新的概念的问题。</li>
<li>methods: 该论文提出了一种基于冻结预训练特征提取器的 meta-学习算法，可以在推理过程中不需要 fine-tuning 地学习新的视觉概念。</li>
<li>results: 在 8 个 meta-学习Benchmark 中，该算法可以无需 meta-训练或 fine-tuning 达到或超过 state-of-the-art 算法 P&gt;M&gt;F 的性能。<details>
<summary>Abstract</summary>
Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach -- without meta-training or fine-tuning -- exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.
</details>
<details>
<summary>摘要</summary>
大语言模型如ChatGPT表现出了惊人的新概念学习能力，而无需任何精度调整。然而，用于检测新对象的视觉模型在推理时表现不佳，或者需要meta-training和/或精度调整。在这项工作中，我们提出了一种meta-学习算法，可以在推理时学习新的视觉概念，无需精度调整。我们的方法利用冻结的预训练特征提取器，类似于上下文学习，将meta-学习视为序列模型化，对已知标签的数据点和未知标签的测试数据点进行模型化。在11个meta-学习benchmark上，我们的方法，无需meta-training或精度调整，超过或等于状态的算法P>M>F，该算法在这些benchmark上进行meta-training。
</details></li>
</ul>
<hr>
<h2 id="MRI-brain-tumor-segmentation-using-informative-feature-vectors-and-kernel-dictionary-learning"><a href="#MRI-brain-tumor-segmentation-using-informative-feature-vectors-and-kernel-dictionary-learning" class="headerlink" title="MRI brain tumor segmentation using informative feature vectors and kernel dictionary learning"></a>MRI brain tumor segmentation using informative feature vectors and kernel dictionary learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10963">http://arxiv.org/abs/2310.10963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyedeh Mahya Mousavi, Mohammad Mostafavi<br>for: 这个论文是用于分类健康和癌症脑磁共振图像中的脑膜区域的方法。methods: 这个方法使用基于内生kernel字典学习算法来学习健康和癌症脑磁共振图像中的特征向量。results: 实验结果表明，提出的方法比其他已有方法更高的准确率和快速采样速度，可以快速完成分类任务，同时也可以减少训练时间和内存需求。<details>
<summary>Abstract</summary>
This paper presents a method based on a kernel dictionary learning algorithm for segmenting brain tumor regions in magnetic resonance images (MRI). A set of first-order and second-order statistical feature vectors are extracted from patches of size 3 * 3 around pixels in the brain MRI scans. These feature vectors are utilized to train two kernel dictionaries separately for healthy and tumorous tissues. To enhance the efficiency of the dictionaries and reduce training time, a correlation-based sample selection technique is developed to identify the most informative and discriminative subset of feature vectors. This technique aims to improve the performance of the dictionaries by selecting a subset of feature vectors that provide valuable information for the segmentation task. Subsequently, a linear classifier is utilized to distinguish between healthy and unhealthy pixels based on the learned dictionaries. The results demonstrate that the proposed method outperforms other existing methods in terms of segmentation accuracy and significantly reduces both the time and memory required, resulting in a remarkably fast training process.
</details>
<details>
<summary>摘要</summary>
这个论文提出了基于kernel字典学习算法的 brain 肿瘤区域分割方法，使用patches 的first-order和second-order统计特征向量从brain MRI扫描中提取特征向量，然后使用这些特征向量训练两个kernel字典，一个用于健康组，一个用于肿瘤组。为了提高字典的效率和减少训练时间，我们提出了一种基于相关性的样本选择技术，以选择最有价值和分类的特征向量，从而提高分类性能。接着，我们使用学习的字典和线性分类器来分割健康和肿瘤组。结果表明，提出的方法在分割精度和训练时间上具有明显的优势，比其他方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Deep-Neural-Network-Training-Efficiency-and-Performance-through-Linear-Prediction"><a href="#Enhancing-Deep-Neural-Network-Training-Efficiency-and-Performance-through-Linear-Prediction" class="headerlink" title="Enhancing Deep Neural Network Training Efficiency and Performance through Linear Prediction"></a>Enhancing Deep Neural Network Training Efficiency and Performance through Linear Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10958">http://arxiv.org/abs/2310.10958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hejie Ying, Mengmeng Song, Yaohong Tang, Shungen Xiao, Zimin Xiao</li>
<li>for: 提高深度神经网络（DNN）模型训练效率和性能。</li>
<li>methods: 基于DNN参数在训练过程中变化的规律发现可以预测DNN参数，并且采用Parameter Linear Prediction（PLP）方法进行预测。</li>
<li>results: 对于Vgg16、Resnet18和GoogLeNet等 represntative底层，通过比较Normal训练和提posed方法的结果，发现提posed方法能够在相同的训练条件和轮数下提高模型性能，具体的结果为CIFAR-100 dataset上的准确率提高约1%和top-1&#x2F;top-5错误降低约0.01。<details>
<summary>Abstract</summary>
Deep neural networks (DNN) have achieved remarkable success in various fields, including computer vision and natural language processing. However, training an effective DNN model still poses challenges. This paper aims to propose a method to optimize the training effectiveness of DNN, with the goal of improving model performance. Firstly, based on the observation that the DNN parameters change in certain laws during training process, the potential of parameter prediction for improving model training efficiency and performance is discovered. Secondly, considering the magnitude of DNN model parameters, hardware limitations and characteristics of Stochastic Gradient Descent (SGD) for noise tolerance, a Parameter Linear Prediction (PLP) method is exploit to perform DNN parameter prediction. Finally, validations are carried out on some representative backbones. Experiment results show that compare to the normal training ways, under the same training conditions and epochs, by employing proposed PLP method, the optimal model is able to obtain average about 1% accuracy improvement and 0.01 top-1/top-5 error reduction for Vgg16, Resnet18 and GoogLeNet based on CIFAR-100 dataset, which shown the effectiveness of the proposed method on different DNN structures, and validated its capacity in enhancing DNN training efficiency and performance.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在不同领域取得了显著成功，包括计算机视觉和自然语言处理。然而，训练有效的DNN模型仍然存在挑战。本文旨在提出一种方法，以提高DNN训练效果并提高模型性能。首先，基于训练过程中DNN参数变化的规律，探讨可以通过预测参数来提高DNN训练效率和性能的潜在可能性。其次，考虑到DNN模型参数的大小、硬件限制和SGD算法对雷yy的耐受性，提出了一种基于PLP方法的DNN参数预测方法。最后，对一些代表性的背bone进行验证。实验结果显示，相比于常规训练方式，通过提议的PLP方法，在同等训练条件和轮数下，可以获得average约1%的准确率提高和0.01的top-1/top-5错误减少，这demonstrates the effectiveness of the proposed method on different DNN structures and validates its ability to enhance DNN training efficiency and performance.
</details></li>
</ul>
<hr>
<h2 id="Medical-Image-Segmentation-via-Sparse-Coding-Decoder"><a href="#Medical-Image-Segmentation-via-Sparse-Coding-Decoder" class="headerlink" title="Medical Image Segmentation via Sparse Coding Decoder"></a>Medical Image Segmentation via Sparse Coding Decoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10957">http://arxiv.org/abs/2310.10957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Zeng, Kaigui Wu</li>
<li>for: 这篇论文主要探讨了如何将Transformer模型应用于医疗影像分类，以提高其捕捉长距离依赖关系的能力。</li>
<li>methods: 这篇论文提出了一种基于核心 sparse vector coding的decoder，named CASCSCDE，它使用核心 sparse vector coding来表示从encoder模组中获取的特征。</li>
<li>results: 实验结果显示，将CASCSCDE与TransUNet结合，可以实现Synapse benchmark上的表现提升，相比于TransUNet alone，提高了3.15%和1.16%的DICE和mIoU分数。<details>
<summary>Abstract</summary>
Transformers have achieved significant success in medical image segmentation, owing to its capability to capture long-range dependencies. Previous works incorporate convolutional layers into the encoder module of transformers, thereby enhancing their ability to learn local relationships among pixels. However, transformers may suffer from limited generalization capabilities and reduced robustness, attributed to the insufficient spatial recovery ability of their decoders. To address this issue, A convolution sparse vector coding based decoder is proposed , namely CAScaded multi-layer Convolutional Sparse vector Coding DEcoder (CASCSCDE), which represents features extracted by the encoder using sparse vectors. To prove the effectiveness of our CASCSCDE, The widely-used TransUNet model is chosen for the demonstration purpose, and the CASCSCDE is incorporated with TransUNet to establish the TransCASCSCDE architecture. Our experiments demonstrate that TransUNet with CASCSCDE significantly enhances performance on the Synapse benchmark, obtaining up to 3.15\% and 1.16\% improvements in DICE and mIoU scores, respectively. CASCSCDE opens new ways for constructing decoders based on convolutional sparse vector coding.
</details>
<details>
<summary>摘要</summary>
transformers在医学图像分割领域取得了重要成功，归功于它的长距离依赖关系捕捉能力。 précédentes works将 convolutional layers incorporated into the encoder module of transformers，以提高它们对像之间的本地关系学习能力。然而，transformers可能会受到有限的泛化能力和减少的稳定性影响，这是由于它们的解码器的空间恢复能力不够。为 Addressing this issue, a convolution sparse vector coding based decoder is proposed, namely CAScaded multi-layer Convolutional Sparse vector Coding DEcoder (CASCSCDE), which represents features extracted by the encoder using sparse vectors。To prove the effectiveness of our CASCSCDE, the widely-used TransUNet model is chosen for the demonstration purpose, and the CASCSCDE is incorporated with TransUNet to establish the TransCASCSCDE architecture。 our experiments show that TransUNet with CASCSCDE significantly enhances performance on the Synapse benchmark, obtaining up to 3.15% and 1.16% improvements in DICE and mIoU scores, respectively。 CASCSCDE opens new ways for constructing decoders based on convolutional sparse vector coding。
</details></li>
</ul>
<hr>
<h2 id="FusionU-Net-U-Net-with-Enhanced-Skip-Connection-for-Pathology-Image-Segmentation"><a href="#FusionU-Net-U-Net-with-Enhanced-Skip-Connection-for-Pathology-Image-Segmentation" class="headerlink" title="FusionU-Net: U-Net with Enhanced Skip Connection for Pathology Image Segmentation"></a>FusionU-Net: U-Net with Enhanced Skip Connection for Pathology Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10951">http://arxiv.org/abs/2310.10951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zongyi-lee/fusionu-net">https://github.com/zongyi-lee/fusionu-net</a></li>
<li>paper_authors: Zongyi Li, Hongbing Lyu, Jun Wang</li>
<li>For: 本研究旨在提高路ологи影像分类任务中U-Net和其变种的性能，提出一种基于U-Net结构的新网络模型，即FusionU-Net。* Methods: FusionU-Net使用了一种基于U-Net结构的增强模块，该模块通过在不同层的Encoder和Decoder之间进行信息交换，以减少Semantic Gap。此外，我们采用了两轮交换机制，以完全考虑当前层输出之间的本地相关性和多层信息交换的需求。* Results: 我们在多个路ологи影像 dataset上进行了广泛的实验，并发现FusionU-Net在与其他竞争方法相比表现更好。我们认为我们的交换模块比现有网络的设计更有效，并且可以轻松地在其他网络中插入，以进一步提高模型性能。<details>
<summary>Abstract</summary>
In recent years, U-Net and its variants have been widely used in pathology image segmentation tasks. One of the key designs of U-Net is the use of skip connections between the encoder and decoder, which helps to recover detailed information after upsampling. While most variations of U-Net adopt the original skip connection design, there is semantic gap between the encoder and decoder that can negatively impact model performance. Therefore, it is important to reduce this semantic gap before conducting skip connection. To address this issue, we propose a new segmentation network called FusionU-Net, which is based on U-Net structure and incorporates a fusion module to exchange information between different skip connections to reduce semantic gaps. Unlike the other fusion modules in existing networks, ours is based on a two-round fusion design that fully considers the local relevance between adjacent encoder layer outputs and the need for bi-directional information exchange across multiple layers. We conducted extensive experiments on multiple pathology image datasets to evaluate our model and found that FusionU-Net achieves better performance compared to other competing methods. We argue our fusion module is more effective than the designs of existing networks, and it could be easily embedded into other networks to further enhance the model performance.
</details>
<details>
<summary>摘要</summary>
Unlike other fusion modules in existing networks, our fusion module is based on a two-round fusion design that fully considers the local relevance between adjacent encoder layer outputs and the need for bi-directional information exchange across multiple layers. We conducted extensive experiments on multiple pathology image datasets to evaluate our model and found that FusionU-Net achieves better performance compared to other competing methods. We argue that our fusion module is more effective than the designs of existing networks and could be easily embedded into other networks to further enhance model performance.
</details></li>
</ul>
<hr>
<h2 id="UNK-VQA-A-Dataset-and-A-Probe-into-Multi-modal-Large-Models’-Abstention-Ability"><a href="#UNK-VQA-A-Dataset-and-A-Probe-into-Multi-modal-Large-Models’-Abstention-Ability" class="headerlink" title="UNK-VQA: A Dataset and A Probe into Multi-modal Large Models’ Abstention Ability"></a>UNK-VQA: A Dataset and A Probe into Multi-modal Large Models’ Abstention Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10942">http://arxiv.org/abs/2310.10942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guoyang9/unk-vqa">https://github.com/guoyang9/unk-vqa</a></li>
<li>paper_authors: Yanyang Guo, Fangkai Jiao, Zhiqi Shen, Liqiang Nie, Mohan Kankanhalli</li>
<li>for: 本研究的目的是帮助视觉问答模型避免回答无法答案的问题，以建立更可靠的人工智能系统。</li>
<li>methods: 我们首先对现有数据进行了有意义的干扰，以使问题图像的 semantics 保持与原始不干扰分布的相似性。然后，我们对多 modal 大型模型进行了 Zero-和 few-shot 性能评估，并发现它们在我们的数据集上表现出了明显的局限性。最后，我们还提出了一种简单的方法来解决这些无法答案的问题。</li>
<li>results: 我们的数据集（UNK-VQA）可以用于提高视觉问答模型的拒绝能力，从而提高人工智能系统的可靠性。我们的研究可以帮助解决现有的问题，并为建立更可靠的人工智能系统提供一个重要的基础。<details>
<summary>Abstract</summary>
Teaching Visual Question Answering (VQA) models to refrain from answering unanswerable questions is necessary for building a trustworthy AI system. Existing studies, though have explored various aspects of VQA but somewhat ignored this particular attribute. This paper aims to bridge the research gap by contributing a comprehensive dataset, called UNK-VQA. The dataset is specifically designed to address the challenge of questions that models do not know. To this end, we first augment the existing data via deliberate perturbations on either the image or question. In specific, we carefully ensure that the question-image semantics remain close to the original unperturbed distribution. By this means, the identification of unanswerable questions becomes challenging, setting our dataset apart from others that involve mere image replacement. We then extensively evaluate the zero- and few-shot performance of several emerging multi-modal large models and discover their significant limitations when applied to our dataset. Additionally, we also propose a straightforward method to tackle these unanswerable questions. This dataset, we believe, will serve as a valuable benchmark for enhancing the abstention capability of VQA models, thereby leading to increased trustworthiness of AI systems. We have made the \href{https://github.com/guoyang9/UNK-VQA}{dataset} available to facilitate further exploration in this area.
</details>
<details>
<summary>摘要</summary>
教学视觉问答模型避免回答无法回答的问题是建立可信worthy AI系统的必要 условия。现有研究，虽曾经探讨了多种VQA方面的问题，但它们却有些忽略了这一特点。这篇论文的目的是填补这个研究漏洞，通过提供一个全面的UNK-VQA数据集来解决问题。这个数据集专门针对模型不知道的问题，我们通过对问题或图像进行意图的拟合来增强问题-图像的Semantics相似性，从而使模型很难以识别无法回答的问题。我们 THEN 进行了广泛的零、少射性性能评估，发现现有的多模式大型模型在我们的数据集上表现出了显著的局限性。此外，我们还提出了一种简单的方法来解决这些无法回答的问题。我们认为这个数据集会成为VQA模型增强其拒绝能力的重要benchmark，从而提高AI系统的可信worthiness。我们已经在github上分享了这个\href{https://github.com/guoyang9/UNK-VQA}{数据集，以便进一步的探索。}
</details></li>
</ul>
<hr>
<h2 id="Towards-Training-free-Open-world-Segmentation-via-Image-Prompting-Foundation-Models"><a href="#Towards-Training-free-Open-world-Segmentation-via-Image-Prompting-Foundation-Models" class="headerlink" title="Towards Training-free Open-world Segmentation via Image Prompting Foundation Models"></a>Towards Training-free Open-world Segmentation via Image Prompting Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10912">http://arxiv.org/abs/2310.10912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lv Tang, Peng-Tao Jiang, Hao-Ke Xiao, Bo Li<br>for:This paper explores open-world segmentation using a novel approach called Image Prompt Segmentation (IPSeg), which leverages vision foundational models and image prompting techniques to segment target objects in input images without requiring exhaustive training sessions.methods:IPSeg utilizes a single image containing a subjective visual concept as a flexible prompt to query vision foundation models like DINOv2 and Stable Diffusion. The approach extracts robust features for the prompt image and input image, then matches the input representations to the prompt representations via a novel feature interaction module to generate point prompts highlighting target objects in the input image.results:Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg’s efficacy for flexible open-world segmentation using intuitive image prompts. The proposed method offers a more efficient and scalable solution for open-world segmentation compared to traditional training-based methods.<details>
<summary>Abstract</summary>
The realm of computer vision has witnessed a paradigm shift with the advent of foundational models, mirroring the transformative influence of large language models in the domain of natural language processing. This paper delves into the exploration of open-world segmentation, presenting a novel approach called Image Prompt Segmentation (IPSeg) that harnesses the power of vision foundational models. At the heart of IPSeg lies the principle of a training-free paradigm, which capitalizes on image prompting techniques. IPSeg utilizes a single image containing a subjective visual concept as a flexible prompt to query vision foundation models like DINOv2 and Stable Diffusion. Our approach extracts robust features for the prompt image and input image, then matches the input representations to the prompt representations via a novel feature interaction module to generate point prompts highlighting target objects in the input image. The generated point prompts are further utilized to guide the Segment Anything Model to segment the target object in the input image. The proposed method stands out by eliminating the need for exhaustive training sessions, thereby offering a more efficient and scalable solution. Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg's efficacy for flexible open-world segmentation using intuitive image prompts. This work pioneers tapping foundation models for open-world understanding through visual concepts conveyed in images.
</details>
<details>
<summary>摘要</summary>
computer vision 领域受到基础模型的洗礼，这对于自然语言处理领域中基础模型的转变产生了类似的影响。这篇论文探讨开放世界分割问题，提出了一种名为图像提示分 segmentation（IPSeg）的新方法。IPSeg 利用视觉基础模型如 DINOv2 和 Stable Diffusion 的力量，并且采用一种无需训练的方法。我们的方法使用一个包含主观视觉概念的单张图像作为灵活的提示，EXTRACT robust的特征于提示图像和输入图像，然后通过一种新的特征互动模块将输入表示与提示表示匹配，以生成指向目标对象的点提示。这些生成的点提示最后用于导引 Segment Anything Model 对输入图像中的目标对象进行分割。我们的方法不需要耗费大量的训练时间，因此更加高效和可扩展。实验结果表明，IPSeg 在 COCO、PASCAL VOC 和其他数据集上表现出色，用于开放世界中的灵活分割。这项工作开拓了基础模型在视觉领域中对开放世界理解的新途径。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/cs.CV_2023_10_17/" data-id="closbropf00k60g88hbtkhuxp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/cs.AI_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T12:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/cs.AI_2023_10_17/">cs.AI - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Learn-Your-Tokens-Word-Pooled-Tokenization-for-Language-Modeling"><a href="#Learn-Your-Tokens-Word-Pooled-Tokenization-for-Language-Modeling" class="headerlink" title="Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"></a>Learn Your Tokens: Word-Pooled Tokenization for Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11628">http://arxiv.org/abs/2310.11628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/avi-jit/etok">https://github.com/avi-jit/etok</a></li>
<li>paper_authors: Avijit Thawani, Saurabh Ghanekar, Xiaoyuan Zhu, Jay Pujara</li>
<li>for: 该文章是关于语言模型tokenization的研究，旨在提出一种“学习你的token”的方法，以优化语言模型的表示能力和性能。</li>
<li>methods: 该文章使用了一种“learn your tokens”的方法，将字节&#x2F;字符 pooling 到单词表示，然后将单词表示传递给主语言模型进行解码。</li>
<li>results:  compared to字节&#x2F;字符模型和子词模型，该文章的中等表达能力和速度的综合型tokenizer在下一个词预测任务中表现出色，特别是在罕见词术中表现出三十倍的提升。<details>
<summary>Abstract</summary>
Language models typically tokenize text into subwords, using a deterministic, hand-engineered heuristic of combining characters into longer surface-level strings such as 'ing' or whole words. Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers. On the other extreme, byte/character-level language models are much less restricted but suffer from increased sequence description lengths and a subsequent quadratic expansion in self-attention computation. Recent attempts to compress and limit these context lengths with fixed size convolutions is helpful but completely ignores the word boundary. This paper considers an alternative 'learn your tokens' scheme which utilizes the word boundary to pool bytes/characters into word representations, which are fed to the primary language model, before again decoding individual characters/bytes per word in parallel. We find that our moderately expressive and moderately fast end-to-end tokenizer outperform by over 300% both subwords and byte/character models over the intrinsic language modeling metric of next-word prediction across datasets. It particularly outshines on rare words, outperforming by a factor of 30! We extensively study the language modeling setup for all three categories of tokenizers and theoretically analyze how our end-to-end models can also be a strong trade-off in efficiency and robustness.
</details>
<details>
<summary>摘要</summary>
This paper proposes an alternative "learn your tokens" scheme, which utilizes word boundaries to pool bytes/characters into word representations, which are then fed to the primary language model. The model then decodes individual characters/bytes per word in parallel. We find that our moderately expressive and moderately fast end-to-end tokenizer outperforms both subwords and byte/character models by over 300% in next-word prediction accuracy across datasets, particularly in rare words, with a factor of 30.We extensively study the language modeling setup for all three categories of tokenizers and theoretically analyze how our end-to-end models can also be a strong trade-off in efficiency and robustness.
</details></li>
</ul>
<hr>
<h2 id="An-Optimistic-Robust-Approach-for-Dynamic-Positioning-of-Omnichannel-Inventories"><a href="#An-Optimistic-Robust-Approach-for-Dynamic-Positioning-of-Omnichannel-Inventories" class="headerlink" title="An Optimistic-Robust Approach for Dynamic Positioning of Omnichannel Inventories"></a>An Optimistic-Robust Approach for Dynamic Positioning of Omnichannel Inventories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12183">http://arxiv.org/abs/2310.12183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavithra Harsha, Shivaram Subramanian, Ali Koc, Mahesh Ramakrishna, Brian Quanz, Dhruv Shah, Chandra Narayanaswami</li>
<li>for: 本研究提出了一种新的数据驱动、分布 свобо的optimistic-robustBI备货优化策略，用于在零售链中均衡时变不确定的多渠道需求，以提高均值性和鲁棒性。</li>
<li>methods: 该策略基于一种新的分布自由的BI备货模型，通过结合数据驱动和鲁棒优化来超越传统的鲁棒优化方法，并且提供了一种可调的tradeoff между鲁棒性和均值性。</li>
<li>results: 实验结果表明，BI备货策略可以在一个实际的大型美国多渠道零售商中实现至少15%的利润提高，同时保持实际最坏情况性能。<details>
<summary>Abstract</summary>
We introduce a new class of data-driven and distribution-free optimistic-robust bimodal inventory optimization (BIO) strategy to effectively allocate inventory across a retail chain to meet time-varying, uncertain omnichannel demand. While prior Robust optimization (RO) methods emphasize the downside, i.e., worst-case adversarial demand, BIO also considers the upside to remain resilient like RO while also reaping the rewards of improved average-case performance by overcoming the presence of endogenous outliers. This bimodal strategy is particularly valuable for balancing the tradeoff between lost sales at the store and the costs of cross-channel e-commerce fulfillment, which is at the core of our inventory optimization model. These factors are asymmetric due to the heterogenous behavior of the channels, with a bias towards the former in terms of lost-sales cost and a dependence on network effects for the latter. We provide structural insights about the BIO solution and how it can be tuned to achieve a preferred tradeoff between robustness and the average-case. Our experiments show that significant benefits can be achieved by rethinking traditional approaches to inventory management, which are siloed by channel and location. Using a real-world dataset from a large American omnichannel retail chain, a business value assessment during a peak period indicates over a 15% profitability gain for BIO over RO and other baselines while also preserving the (practical) worst case performance.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的数据驱动、分布自由的乐观稳定优化策略（BIO），用于在零售链中有效分配存储，以满足时变、不确定的多渠道需求。在优化模型中，BIO策略通过考虑两个模式（下行和上行）来实现乐观稳定性，同时也能够保持优秀的平均情况性。这种策略对于平衡店铺产生损失和多渠道电商配送成本的负担进行了有利的均衡。我们提供了BIO解决方案的结构性分析，以及如何根据需要调整BIO来实现想要的负载均衡。我们的实验结果表明，通过重新思考传统的存储管理方法，可以实现显著的利益提升，并且保持了实际最坏情况性。使用一个大型美国多渠道零售商的实际数据，我们在峰值期进行了业务价值评估，发现BIO相比RO和其他基eline，可以获得超过15%的利润增加，同时保持实际最坏情况性。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-General-Intelligence-Factor-in-Language-Models-A-Psychometric-Approach"><a href="#Unveiling-the-General-Intelligence-Factor-in-Language-Models-A-Psychometric-Approach" class="headerlink" title="Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach"></a>Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11616">http://arxiv.org/abs/2310.11616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidilic/g-in-llms">https://github.com/davidilic/g-in-llms</a></li>
<li>paper_authors: David Ilić</li>
<li>for: 这项研究探讨语言模型中的通用智能因素（g因素），扩展传统地应用于人类和某些动物种类的心理测量理论。</li>
<li>methods: 通过两个大规模数据集（Open LLM Leaderboard和GLUE Leaderboard）的因素分析，发现了一个强度稳定的单一g因素，占据模型性能变化的85%。同时发现模型大小和g因素之间存在moderate相关性（。48）。</li>
<li>results: 发现了语言模型中的g因素，提供了一个统一的评估指标，开启了更加稳定、g基础能力评估的新途径。这些发现将心理测量理论应用于人工通用智能领域奠定基础，有实践意义 для模型评估和开发。<details>
<summary>Abstract</summary>
This study uncovers the factor of general intelligence, or g, in language models, extending the psychometric theory traditionally applied to humans and certain animal species. Utilizing factor analysis on two extensive datasets - Open LLM Leaderboard with 1,232 models and General Language Understanding Evaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for a unidimensional, highly stable g factor that accounts for 85% of the variance in model performance. The study also finds a moderate correlation of .48 between model size and g. The discovery of g in language models offers a unified metric for model evaluation and opens new avenues for more robust, g-based model ability assessment. These findings lay the foundation for understanding and future research on artificial general intelligence from a psychometric perspective and have practical implications for model evaluation and development.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-a-Hierarchical-Planner-from-Humans-in-Multiple-Generations"><a href="#Learning-a-Hierarchical-Planner-from-Humans-in-Multiple-Generations" class="headerlink" title="Learning a Hierarchical Planner from Humans in Multiple Generations"></a>Learning a Hierarchical Planner from Humans in Multiple Generations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11614">http://arxiv.org/abs/2310.11614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Hernandez Cano, Yewen Pu, Robert D. Hawkins, Josh Tenenbaum, Armando Solar-Lezama</li>
<li>for: 本研究旨在帮助机器学习人类知识，提高机器的适应能力和解决复杂任务的能力。</li>
<li>methods: 本研究使用了自然编程，一种组合了程序学习和层次规划的库学习系统。用户通过课程建设来教育系统，选择一个具有挑战性但不是不可能的目标，并提供语言提示，帮助系统在层次规划中找到正确的计划。</li>
<li>results:  simulations and human experiments (n&#x3D;360) 表明，自然编程可以强大地组合来自不同用户和Contexts的程序，并且在对Contexts进行更改时更快地适应，并解决更复杂的任务。相比于程序编程基线，自然编程在解决复杂任务方面表现出了明显的优势。<details>
<summary>Abstract</summary>
A typical way in which a machine acquires knowledge from humans is by programming. Compared to learning from demonstrations or experiences, programmatic learning allows the machine to acquire a novel skill as soon as the program is written, and, by building a library of programs, a machine can quickly learn how to perform complex tasks. However, as programs often take their execution contexts for granted, they are brittle when the contexts change, making it difficult to adapt complex programs to new contexts. We present natural programming, a library learning system that combines programmatic learning with a hierarchical planner. Natural programming maintains a library of decompositions, consisting of a goal, a linguistic description of how this goal decompose into sub-goals, and a concrete instance of its decomposition into sub-goals. A user teaches the system via curriculum building, by identifying a challenging yet not impossible goal along with linguistic hints on how this goal may be decomposed into sub-goals. The system solves for the goal via hierarchical planning, using the linguistic hints to guide its probability distribution in proposing the right plans. The system learns from this interaction by adding newly found decompositions in the successful search into its library. Simulated studies and a human experiment (n=360) on a controlled environment demonstrate that natural programming can robustly compose programs learned from different users and contexts, adapting faster and solving more complex tasks when compared to programmatic baselines.
</details>
<details>
<summary>摘要</summary>
一般来说，机器学习知识从人类那里是通过编程的方式。与学习示例或经验相比，编程学习可以让机器快速学习新的技能，只需要编写一份程序即可，并且通过建立一个程序库，机器可以快速学习完成复杂任务。然而，由于程序经常假设执行上下文，因此它们在上下文变化时会变得不稳定，使得复杂程序适应新上下文变得困难。我们提出了自然编程，一种基于程序库学习系统，它将程序学习与层次规划结合。自然编程保留一个库中的分解，包括目标、语言描述如何将目标分解成子目标，以及具体实例的分解。用户通过课程建设来教育系统，选择一个具有挑战性但并不是不可能的目标，并提供语言提示如何将目标分解成子目标。系统使用层次规划解决目标，使用语言提示来引导搜索的概率分布。系统从这种互动中学习，将成功搜索的新分解添加到库中。 simulated studies and human experiment (n=360) on a controlled environment show that natural programming can robustly compose programs learned from different users and contexts, adapting faster and solving more complex tasks than programmatic baselines.
</details></li>
</ul>
<hr>
<h2 id="Language-Models-as-Zero-Shot-Trajectory-Generators"><a href="#Language-Models-as-Zero-Shot-Trajectory-Generators" class="headerlink" title="Language Models as Zero-Shot Trajectory Generators"></a>Language Models as Zero-Shot Trajectory Generators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11604">http://arxiv.org/abs/2310.11604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teyun Kwon, Norman Di Palo, Edward Johns</li>
<li>for: 这个论文旨在检验Language Model (LLM)是否能直接预测 robot 的低级运动轨迹，以便在 Robotics 中使用 LLM 进行高级 планинг。</li>
<li>methods: 该论文使用了 GPT-4 Language Model，并通过对象检测和分割视觉模型来提供输入。</li>
<li>results: 研究发现，单个任务无关的提示可以在 26 个实际世界语言任务中表现出色，例如 “打开瓶cap” 和 “wipe plate with sponge”。此外，研究还发现，LLM 实际上具有足够的低级控制知识，可以执行许多常见任务，并且可以检测失败并重新规划轨迹。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding of low-level robot control sufficient for a range of common tasks, and that they can additionally detect failures and then re-plan trajectories accordingly. Videos, code, and prompts are available at: https://www.robot-learning.uk/language-models-trajectory-generators.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Efficacy-of-Transformer-based-Adversarial-Attacks-in-Security-Domains"><a href="#The-Efficacy-of-Transformer-based-Adversarial-Attacks-in-Security-Domains" class="headerlink" title="The Efficacy of Transformer-based Adversarial Attacks in Security Domains"></a>The Efficacy of Transformer-based Adversarial Attacks in Security Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11597">http://arxiv.org/abs/2310.11597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunyang Li, Kyle Domico, Jean-Charles Noirot Ferrand, Patrick McDaniel</li>
<li>for: 本研究旨在探讨transformer架构在安全领域中的可靠性和攻击力。</li>
<li>methods: 本研究使用了精心调整的 pré-trained transformer、Convolutional Neural Network (CNN) 和 hybrid (transformer 和 CNN 的ensemble) 模型来解决不同的下游图像任务。然后，使用了一种攻击算法来生成19,367个 adversarial example对于每个任务和每个模型。</li>
<li>results: 我们发现， adversarial examples 生成于 transformer 上的攻击力最高，可以在其他模型上传输25.7%的攻击力。同时，生成于其他模型上的 adversarial examples 在 transformer 上的传输率只有56.7%。这些结果强调了对 transformer 架构的研究在安全领域中的重要性，并建议在传输攻击 Setting 中使用它们作为主要架构。<details>
<summary>Abstract</summary>
Today, the security of many domains rely on the use of Machine Learning to detect threats, identify vulnerabilities, and safeguard systems from attacks. Recently, transformer architectures have improved the state-of-the-art performance on a wide range of tasks such as malware detection and network intrusion detection. But, before abandoning current approaches to transformers, it is crucial to understand their properties and implications on cybersecurity applications. In this paper, we evaluate the robustness of transformers to adversarial samples for system defenders (i.e., resiliency to adversarial perturbations generated on different types of architectures) and their adversarial strength for system attackers (i.e., transferability of adversarial samples generated by transformers to other target models). To that effect, we first fine-tune a set of pre-trained transformer, Convolutional Neural Network (CNN), and hybrid (an ensemble of transformer and CNN) models to solve different downstream image-based tasks. Then, we use an attack algorithm to craft 19,367 adversarial examples on each model for each task. The transferability of these adversarial examples is measured by evaluating each set on other models to determine which models offer more adversarial strength, and consequently, more robustness against these attacks. We find that the adversarial examples crafted on transformers offer the highest transferability rate (i.e., 25.7% higher than the average) onto other models. Similarly, adversarial examples crafted on other models have the lowest rate of transferability (i.e., 56.7% lower than the average) onto transformers. Our work emphasizes the importance of studying transformer architectures for attacking and defending models in security domains, and suggests using them as the primary architecture in transfer attack settings.
</details>
<details>
<summary>摘要</summary>
今天，许多领域的安全性都依赖于机器学习来探测威胁、 indentify漏洞和保护系统从攻击中保护。现在，transformer架构已经提高了一系列任务的状态之末性，如恶意软件检测和网络侵入检测。但是，在switch到transformer架构之前，必须了解它们的性质和影响在安全应用程序中。在这篇论文中，我们评估了transformer架构对抗攻击样本的Robustness（可以抗拒的性）和攻击者的攻击力（可以转移到其他目标模型）。为此，我们首先精度调整一组预训练过的transformer、Convolutional Neural Network（CNN）和混合（一个ensemble of transformer和CNN）模型，以解决不同的图像基于任务。然后，我们使用一种攻击算法来生成19367个攻击样本 для每个任务。我们使用这些攻击样本来测量每个模型之间的 transferred（可以转移的性）。我们发现，在transformer架构上生成的攻击样本具有最高的转移率（即25.7% higher than the average），而其他模型上生成的攻击样本具有最低的转移率（即56.7% lower than the average）。我们的工作强调了在安全领域使用transformer架构进行攻击和防御模型的研究，并建议在转移攻击设置中使用它们作为主要架构。
</details></li>
</ul>
<hr>
<h2 id="WaveAttack-Asymmetric-Frequency-Obfuscation-based-Backdoor-Attacks-Against-Deep-Neural-Networks"><a href="#WaveAttack-Asymmetric-Frequency-Obfuscation-based-Backdoor-Attacks-Against-Deep-Neural-Networks" class="headerlink" title="WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks"></a>WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11595">http://arxiv.org/abs/2310.11595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Xia, Zhihao Yue, Yingbo Zhou, Zhiwei Ling, Xian Wei, Mingsong Chen</li>
<li>for: 针对深度神经网络预测中的背门攻击（backdoor attack），提出一种基于高频特征的新攻击方法——WaveAttack。</li>
<li>methods: 使用分割波峰变换（DWT）获得图像高频特征，并通过异步频率隐蔽方法在训练和推理阶段添加自适应副本，以提高背门诱导器的影响和攻击效iveness。</li>
<li>results: 对比比较常见的背门攻击方法，WaveAttack 可以 дости得高度隐蔽和效果，同时在图像质量上也可以达到28.27%的提升（PSNR）、1.61%的提升（SSIM）和70.59%的减少（IS）。<details>
<summary>Abstract</summary>
Due to the popularity of Artificial Intelligence (AI) technology, numerous backdoor attacks are designed by adversaries to mislead deep neural network predictions by manipulating training samples and training processes. Although backdoor attacks are effective in various real scenarios, they still suffer from the problems of both low fidelity of poisoned samples and non-negligible transfer in latent space, which make them easily detectable by existing backdoor detection algorithms. To overcome the weakness, this paper proposes a novel frequency-based backdoor attack method named WaveAttack, which obtains image high-frequency features through Discrete Wavelet Transform (DWT) to generate backdoor triggers. Furthermore, we introduce an asymmetric frequency obfuscation method, which can add an adaptive residual in the training and inference stage to improve the impact of triggers and further enhance the effectiveness of WaveAttack. Comprehensive experimental results show that WaveAttack not only achieves higher stealthiness and effectiveness, but also outperforms state-of-the-art (SOTA) backdoor attack methods in the fidelity of images by up to 28.27\% improvement in PSNR, 1.61\% improvement in SSIM, and 70.59\% reduction in IS.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于人工智能技术的普及，许多敌对者设计了多种后门攻击，以诱导深度神经网络预测结果的扰乱。尽管后门攻击在实际场景中有效，但它们仍然受到低精度毒品样本和 latent space 中的非至遇抗减 Transfer 的问题，这使得它们可以轻松被现有的后门检测算法检测出来。为了解决这个弱点，本文提出了一种基于频率的后门攻击方法，名为 WaveAttack，它使用 Discrete Wavelet Transform (DWT) 获取图像高频特征来生成后门触发器。此外，我们还引入了不对称频率隐藏方法，可以在训练和推理阶段添加adaptive residual来提高触发器的影响，进一步提高 WaveAttack 的效果。经过全面的实验结果表明，WaveAttack 不仅实现了更高的隐身度和效果，还比 State-of-the-art (SOTA) 后门攻击方法在图像的精度上提高了28.27％的PSNR，1.61％的SSIM，和70.59％的IS。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Robustness-Unhardening-via-Backdoor-Attacks-in-Federated-Learning"><a href="#Adversarial-Robustness-Unhardening-via-Backdoor-Attacks-in-Federated-Learning" class="headerlink" title="Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning"></a>Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11594">http://arxiv.org/abs/2310.11594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong</li>
<li>for: This paper focuses on addressing security challenges in federated learning, specifically the issues of poisoning and backdoor attacks, by exploring the intersection of adversarial training and backdoor attacks.</li>
<li>methods: The paper introduces a new attack called Adversarial Robustness Unhardening (ARU) and evaluates its impact on adversarial training and existing robust aggregation defenses against poisoning and backdoor attacks through extensive empirical experiments.</li>
<li>results: The paper finds that ARU can intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks, and highlights the limitations of existing defenses against ARU. The findings offer insights into bolstering defenses against ARU and the need for further research in this area.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文关注联邦学习中的安全挑战，具体来说是对毒素和后门攻击的问题，通过对抗训练和后门攻击之间的交叉研究。</li>
<li>methods: 论文引入了一种新的攻击方法called Adversarial Robustness Unhardening (ARU)，通过广泛的实验评估ARU对反对抗训练和现有的稳定聚合防御措施的影响。</li>
<li>results: 论文发现ARU可以在分布式训练中故意削弱模型的鲁棒性，使模型面临更广泛的欺骗攻击，并指出现有防御措施对ARU的限制。发现为防止ARU提供了新的策略和研究方向。<details>
<summary>Abstract</summary>
In today's data-driven landscape, the delicate equilibrium between safeguarding user privacy and unleashing data potential stands as a paramount concern. Federated learning, which enables collaborative model training without necessitating data sharing, has emerged as a privacy-centric solution. This decentralized approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data. Our research, initially spurred by test-time evasion attacks, investigates the intersection of adversarial training and backdoor attacks within federated learning, introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a subset of adversaries to intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks. We present extensive empirical experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning and backdoor attacks. Our findings inform strategies for enhancing ARU to counter current defensive measures and highlight the limitations of existing defenses, offering insights into bolstering defenses against ARU.
</details>
<details>
<summary>摘要</summary>
今天的数据驱动时代，保护用户隐私和解释数据潜力之间的细腻平衡已成为首要问题。联邦学习，它允许无需数据共享进行模型训练的协同方法，已经出现为隐私中心的解决方案。这种分布式方法会产生安全挑战，包括腐化和后门攻击，即恶意实体插入假数据。我们的研究，起始于测试时间攻击，探讨在联邦学习中的对抗训练和后门攻击的交叉点，并提出了对抗训练不稳定性的技术——不稳定性脱困（ARU）。ARU可以在分布式训练中被一部分对手使用，故意下降模型的可靠性，使模型对更广泛的欺骗攻击变得感受。我们对ARU的影响进行了广泛的实验研究，包括对抗训练和现有的毒素攻击防御措施的评估。我们的发现可以帮助改进ARU，抗衡当前的防御措施，并 highlighted了现有防御措施的局限性，为加强防御提供了新的思路。
</details></li>
</ul>
<hr>
<h2 id="Automated-Evaluation-of-Personalized-Text-Generation-using-Large-Language-Models"><a href="#Automated-Evaluation-of-Personalized-Text-Generation-using-Large-Language-Models" class="headerlink" title="Automated Evaluation of Personalized Text Generation using Large Language Models"></a>Automated Evaluation of Personalized Text Generation using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11593">http://arxiv.org/abs/2310.11593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaqing Wang, Jiepu Jiang, Mingyang Zhang, Cheng Li, Yi Liang, Qiaozhu Mei, Michael Bendersky</li>
<li>for: 评估个性化文本生成器的质量</li>
<li>methods: 使用大型自然语言模型（LLMs）进行评估，并分析三个主要Semantic Aspects：个性化、质量和相关性</li>
<li>results: 比较LLMs和人工标注的评估结果，发现LLMs能够更准确地评估个性化文本生成器的质量，并且存在较好的一致性和效率。<details>
<summary>Abstract</summary>
Personalized text generation presents a specialized mechanism for delivering content that is specific to a user's personal context. While the research progress in this area has been rapid, evaluation still presents a challenge. Traditional automated metrics such as BLEU and ROUGE primarily measure lexical similarity to human-written references, and are not able to distinguish personalization from other subtle semantic aspects, thus falling short of capturing the nuances of personalized generated content quality. On the other hand, human judgments are costly to obtain, especially in the realm of personalized evaluation. Inspired by these challenges, we explore the use of large language models (LLMs) for evaluating personalized text generation, and examine their ability to understand nuanced user context. We present AuPEL, a novel evaluation method that distills three major semantic aspects of the generated text: personalization, quality and relevance, and automatically measures these aspects. To validate the effectiveness of AuPEL, we design carefully controlled experiments and compare the accuracy of the evaluation judgments made by LLMs versus that of judgements made by human annotators, and conduct rigorous analyses of the consistency and sensitivity of the proposed metric. We find that, compared to existing evaluation metrics, AuPEL not only distinguishes and ranks models based on their personalization abilities more accurately, but also presents commendable consistency and efficiency for this task. Our work suggests that using LLMs as the evaluators of personalized text generation is superior to traditional text similarity metrics, even though interesting new challenges still remain.
</details>
<details>
<summary>摘要</summary>
个人化文本生成技术提供了特殊的内容交付机制，以便为用户的个人背景提供特定的内容。虽然这一领域的研究进步快速，但评估仍然存在挑战。传统的自动化指标如BLEU和ROUGE主要测量人工写出的参考文本和自动生成内容之间的语言相似性，而不能分辨个人化的特点，因此无法捕捉个人化生成内容质量的细微差别。人类评估更加costly，尤其是在个人化评估领域。我们 inspirited by these challenges，explore the use of large language models (LLMs) for evaluating personalized text generation, and examine their ability to understand nuanced user context.我们提出了一种新的评估方法，称为AuPEL，它可以自动测量生成文本中的三个主要semantic aspect：个人化、质量和相关性。为了验证AuPEL的有效性，我们设计了仔细控制的实验，并将LLMs的评估判断与人类标注者的评估判断进行比较，进行了严格的分析。我们发现，相比现有的评估指标，AuPEL不仅更准确地分辨和排序模型的个人化能力，而且具有了很好的一致性和效率。我们的工作表明，使用LLMs作为个人化文本生成评估的方法比传统的文本相似度指标更为有利，尽管还有一些新的挑战 waits ahead。
</details></li>
</ul>
<hr>
<h2 id="Audio-AdapterFusion-A-Task-ID-free-Approach-for-Efficient-and-Non-Destructive-Multi-task-Speech-Recognition"><a href="#Audio-AdapterFusion-A-Task-ID-free-Approach-for-Efficient-and-Non-Destructive-Multi-task-Speech-Recognition" class="headerlink" title="Audio-AdapterFusion: A Task-ID-free Approach for Efficient and Non-Destructive Multi-task Speech Recognition"></a>Audio-AdapterFusion: A Task-ID-free Approach for Efficient and Non-Destructive Multi-task Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13015">http://arxiv.org/abs/2310.13015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hillary Ngai, Rohan Agrawal, Neeraj Gaur, Ronny Huang, Parisa Haghani, Pedro Moreno Mengibar</li>
<li>for: 这篇论文的目的是提出了一些可以将单任问题（single-task）adapter组合在多任问题（multi-task） automatic speech recognition（ASR）中，并且训练这些方法以提高语音识别精度。</li>
<li>methods: 这篇论文提出了三种新的任问题ID-free方法（task-ID-free methods），可以将单任问题adapter组合在多任问题ASR中，并且训练这些方法以提高语音识别精度。这三种方法分别是：1) 使用任问题ID-free映射函数（task-ID-free mapping function）来将单任问题adapter组合在多任问题ASR中；2) 使用任问题ID-free排序（task-ID-free sorting）来将单任问题adapter组合在多任问题ASR中；3) 使用任问题ID-free混合（task-ID-free mixing）来将单任问题adapter组合在多任问题ASR中。</li>
<li>results: 这篇论文的结果显示，使用这三种方法可以将单任问题adapter组合在多任问题ASR中，并且可以提高语音识别精度。 Specifically, the results show that the proposed methods can improve the speech recognition accuracy by 8% on average, compared to full fine-tuning. Additionally, the proposed methods are non-destructive and parameter-efficient, as they only update 17% of the model parameters.<details>
<summary>Abstract</summary>
Adapters are an efficient, composable alternative to full fine-tuning of pre-trained models and help scale the deployment of large ASR models to many tasks. In practice, a task ID is commonly prepended to the input during inference to route to single-task adapters for the specified task. However, one major limitation of this approach is that the task ID may not be known during inference, rendering it unsuitable for most multi-task settings. To address this, we propose three novel task-ID-free methods to combine single-task adapters in multi-task ASR and investigate two learning algorithms for training. We evaluate our methods on 10 test sets from 4 diverse ASR tasks and show that our methods are non-destructive and parameter-efficient. While only updating 17% of the model parameters, our methods can achieve an 8% mean WER improvement relative to full fine-tuning and are on-par with task-ID adapter routing.
</details>
<details>
<summary>摘要</summary>
adapter 是一种高效、可 compose 的替代方案，帮助扩大预训练模型的部署范围。在实践中，通常会在推理时预先 prepend 任务 ID 到输入，以便将输入 routed 到适应的单任务 adapter。然而，这种方法有一个主要的限制，即在多任务 setting 中，任务 ID 可能不知道，这 Render 其不适用。为了解决这个问题，我们提出了三种新的任务 ID  libre 方法，并 investigate 了两种学习算法 для训练。我们在 10 个测试集上从 4 种不同的 ASR 任务中选择了，并显示了我们的方法是非破坏性的和参数有效的。只需更新 17% 的模型参数，我们的方法可以实现一个 8% 的语音识别错误率下降，与完全 fine-tuning 相当，并且与任务 ID adapter 路由相比。
</details></li>
</ul>
<hr>
<h2 id="Eliciting-Human-Preferences-with-Language-Models"><a href="#Eliciting-Human-Preferences-with-Language-Models" class="headerlink" title="Eliciting Human Preferences with Language Models"></a>Eliciting Human Preferences with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11589">http://arxiv.org/abs/2310.11589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alextamkin/generative-elicitation">https://github.com/alextamkin/generative-elicitation</a></li>
<li>paper_authors: Belinda Z. Li, Alex Tamkin, Noah Goodman, Jacob Andreas</li>
<li>for: 本研究用LM来指导任务规定过程。</li>
<li>methods: 本研究提出了Generative Active Task Elicitation（GATE）学习框架，用LM自己进行自由语言互动，以引出用户的意图和行为。</li>
<li>results: 在邮箱验证、内容推荐和道德决策等三个领域中，通过LM自己生成问题或总结特殊情况，可以更好地引出用户的想法和需求，并且用户报告需要更少的努力和更多的创新。<details>
<summary>Abstract</summary>
Language models (LMs) can be directed to perform target tasks by using labeled examples or natural language prompts. But selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of LM behavior. We propose to use *LMs themselves* to guide the task specification process. In this paper, we introduce **Generative Active Task Elicitation (GATE)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. We study GATE in three domains: email validation, content recommendation, and moral reasoning. In preregistered experiments, we show that LMs prompted to perform GATE (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. Users report that interactive task elicitation requires less effort than prompting or example labeling and surfaces novel considerations not initially anticipated by users. Our findings suggest that LM-driven elicitation can be a powerful tool for aligning models to complex human preferences and values.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）可以通过使用标记的示例或自然语言提示来实现目标任务。但选择示例或编写提示可以是困难的——特别是在涉及到特殊的边界情况、需要精确表达抽象的偏好或需要语言模型行为的准确理解。我们提议使用LM自己来指导任务规定过程。在这篇论文中，我们介绍了**生成活动任务提取（GATE）**：一种学习框架，在用户与LM进行自由形式、语言基于的互动中，LM可以透过生成开放式问题或合成有用的边界情况来引导用户提供任务要求。我们在三个领域中进行了预先注册的实验，并证明了LM通过执行GATE（例如，生成开放式问题或合成有用的边界情况）可以从用户提供更多有用信息的回应。用户报告称，与LM进行交互式任务提取比起用户编写提示或标记，需要更少的努力，并且可以浮现用户没有首先预期的考虑。我们的发现表明，LM驱动的提取可以是一种强大的工具，用于将模型与人类偏好和价值相互Alignment。
</details></li>
</ul>
<hr>
<h2 id="When-Rigidity-Hurts-Soft-Consistency-Regularization-for-Probabilistic-Hierarchical-Time-Series-Forecasting"><a href="#When-Rigidity-Hurts-Soft-Consistency-Regularization-for-Probabilistic-Hierarchical-Time-Series-Forecasting" class="headerlink" title="When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting"></a>When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11569">http://arxiv.org/abs/2310.11569</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adityalab/profhit">https://github.com/adityalab/profhit</a></li>
<li>paper_authors: Harshavardhan Kamarthi, Lingkai Kong, Alexander Rodríguez, Chao Zhang, B. Aditya Prakash</li>
<li>for: 这个研究旨在提出一个可靠且具有准确评估的时间序列预测方法，用于模型和预测多ivariate时间序列，并且考虑到这些时间序列之间的层次关系。</li>
<li>methods: 这个方法使用了一种可靠的数据科学方法，具体来说是一种可靠的 Bayesian 方法，并且引入了一个新的 Distributional Coherency 调整，以从层次关系中学习整个预测分布，这使得预测得到了更好的准确性和均匀性。</li>
<li>results: 在评估这个方法的实验中，发现这个方法可以在各种不同的数据集上提供41-88%的更好的性能，并且在数据集中缺失10%的时间序列Data时，预测的性能仍然保持在良好的水平，而其他方法在这种情况下的性能则会严重下降超过70%。<details>
<summary>Abstract</summary>
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gap and propose PROFHiT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHiT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regularization to learn from hierarchical relations for entire forecast distribution that enables robust and calibrated forecasts as well as adapt to datasets of varying hierarchical consistency. On evaluating PROFHiT over wide range of datasets, we observed 41-88% better performance in accuracy and significantly better calibration. Due to modeling the coherency over full distribution, we observed that PROFHiT can robustly provide reliable forecasts even if up to 10% of input time-series data is missing where other methods' performance severely degrade by over 70%.
</details>
<details>
<summary>摘要</summary>
“probabilistic hierarchical time-series forecasting是一种重要的时间序列预测变体，其目的是模型和预测多变量时间序列的层次关系。大多数方法都是专注于点预测，而不提供准确的probabilistic预测分布。当前的状态艺术probabilistic预测方法也强制实施层次关系于点预测和样本分布，而不考虑预测分布的协调性。过去的工作也假设了所有数据都一定程度上遵循给定的层次关系，而不会适应实际的数据，其中很多数据可能会出现层次不一致的情况。我们 closure这两个 gap，并提出PROFHiT，这是一种完全probabilistic层次预测模型，可以同时模型整个层次预测分布。PROFHiT使用 flexible probabilistic Bayesian方法，并引入一种新的分布协调regularization，以学习层次关系，并生成准确和协调的预测。对于各种数据集进行评估，我们发现PROFHiT在精度和准确性方面表现出41-88%的提升，同时也能够更好地适应数据集的层次不一致情况。由于模型整个分布的协调性，我们发现PROFHiT可以在数据损失情况下提供可靠的预测，甚至在数据损失10%以上时仍能保持良好的预测性能，而其他方法在这种情况下会导致预测性能下降超过70%。”
</details></li>
</ul>
<hr>
<h2 id="Integrating-3D-City-Data-through-Knowledge-Graphs"><a href="#Integrating-3D-City-Data-through-Knowledge-Graphs" class="headerlink" title="Integrating 3D City Data through Knowledge Graphs"></a>Integrating 3D City Data through Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11555">http://arxiv.org/abs/2310.11555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linfang Ding, Guohui Xiao, Albulen Pano, Mattia Fumagalli, Dongsheng Chen, Yu Feng, Diego Calvanese, Hongchao Fan, Liqiu Meng</li>
<li>for: 本研究旨在利用知识图（KG）技术，将城市地理信息模型（CityGML）数据模型化为适当的 ontology，以便对CityGML数据进行查询和推理。</li>
<li>methods: 本研究使用了 declarative 映射将CityGML数据与3DCityDB系统的关系表格相关联，以便通过标准 SQL 查询语言进行查询。此外，本研究还使用了 OpenStreetMap 数据作为示例数据，并与其他（地理）KG（如 Wikidata、DBPedia 和 GeoNames）进行集成。</li>
<li>results: 本研究实现了一个基于 CityGML KG 框架的方法，可以快速地将CityGML数据转换为 KG 格式，并且可以与其他数据源进行集成。这种方法可以帮助解决城市地理信息查询和推理的问题，并且可以提高城市规划和管理的效率。<details>
<summary>Abstract</summary>
CityGML is a widely adopted standard by the Open Geospatial Consortium (OGC) for representing and exchanging 3D city models. The representation of semantic and topological properties in CityGML makes it possible to query such 3D city data to perform analysis in various applications, e.g., security management and emergency response, energy consumption and estimation, and occupancy measurement. However, the potential of querying CityGML data has not been fully exploited. The official GML/XML encoding of CityGML is only intended as an exchange format but is not suitable for query answering. The most common way of dealing with CityGML data is to store them in the 3DCityDB system as relational tables and then query them with the standard SQL query language. Nevertheless, for end users, it remains a challenging task to formulate queries over 3DCityDB directly for their ad-hoc analytical tasks, because there is a gap between the conceptual semantics of CityGML and the relational schema adopted in 3DCityDB. In fact, the semantics of CityGML itself can be modeled as a suitable ontology. The technology of Knowledge Graphs (KGs), where an ontology is at the core, is a good solution to bridge such a gap. Moreover, embracing KGs makes it easier to integrate with other spatial data sources, e.g., OpenStreetMap and existing (Geo)KGs (e.g., Wikidata, DBPedia, and GeoNames), and to perform queries combining information from multiple data sources. In this work, we describe a CityGML KG framework to populate the concepts in the CityGML ontology using declarative mappings to 3DCityDB, thus exposing the CityGML data therein as a KG. To demonstrate the feasibility of our approach, we use CityGML data from the city of Munich as test data and integrate OpenStreeMap data in the same area.
</details>
<details>
<summary>摘要</summary>
CityGML 是一种广泛采用的标准，由开放地理空间协会（OGC）用于表示和交换 3D 城市模型。CityGML 中的 semantics 和 topologic 属性的表示使得可以对这些 3D 城市数据进行查询，并且可以在不同应用中进行分析，如安全管理和紧急应急处理、能源消耗和估算、和占用量测量。然而，CityGML 数据的查询潜力尚未得到完全利用。官方的 GML/XML 编码仅供交换用途，不适用于查询 answered。通常情况下，CityGML 数据会被存储在 3DCityDB 系统中的关系表格中，然后使用标准 SQL 查询语言进行查询。然而，为终端用户来直接对 3DCityDB 进行查询是一项困难的任务，因为 CityGML 的概念 semantics 和 3DCityDB 的关系schema 之间存在一个差距。实际上，CityGML 的 semantics 本身可以被视为一个适当的 ontology。知识图（KGs）技术，其核心是 ontology，是一个好的解决方案，可以bridge 这个差距。此外，采用 KGs 可以轻松地与其他空间数据源集成，如 OpenStreetMap 和现有（Geo）KGs（如 Wikidata、DBPedia 和 GeoNames），并在多个数据源之间进行查询。在这项工作中，我们描述了一个 CityGML KG 框架，通过声明映射来将 CityGML 数据与 3DCityDB 关系表格相关联，从而将 CityGML 数据转换为 KG。为证明我们的方法的可行性，我们使用了 Munich 城市的 CityGML 数据作为测试数据，并将 OpenStreetMap 数据与其同一地区集成。
</details></li>
</ul>
<hr>
<h2 id="Towards-Optimal-Regret-in-Adversarial-Linear-MDPs-with-Bandit-Feedback"><a href="#Towards-Optimal-Regret-in-Adversarial-Linear-MDPs-with-Bandit-Feedback" class="headerlink" title="Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback"></a>Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11550">http://arxiv.org/abs/2310.11550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolin Liu, Chen-Yu Wei, Julian Zimmert</li>
<li>for: 本研究探讨在线强化学习中的线性Markov决策过程中的抗敌性损失和随机反馈，不需要对过程转移或模拟器的先前知识。</li>
<li>methods: 我们介绍了两种算法，它们在抗敌性损失和随机反馈下实现了改进的后悔性表现，并且比现有方法更为有效。第一种算法尽管计算效率低下，但可 garantuee $\widetilde{\mathcal{O}\left(\sqrt{K}\right)$的后悔性，这是在考虑的设定中的首次成果。第二种算算法基于策略优化框架，并且可 garantuee $\widetilde{\mathcal{O}\left(K^{\frac{3}{4} \right)$的后悔性，同时具有计算效率。</li>
<li>results: 我们的结果在比较之下显著超过现状态艺术：一个计算效率较低的算法由Kong et al. [2023]提出，其后悔性为 $\widetilde{\mathcal{O}\left(K^{\frac{4}{5}+poly\left(\frac{1}{\lambda_{\min}\right) \right)$，其中$\lambda_{\min}$是问题依赖的常数，可以是任意的小数。另一个计算效率较高的算法由Sherman et al. [2023b]提出，其后悔性为 $\widetilde{\mathcal{O}\left(K^{\frac{6}{7} \right)$。<details>
<summary>Abstract</summary>
We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, ensures a regret of $\widetilde{\mathcal{O}\left(\sqrt{K}\right)$, where $K$ is the number of episodes. This is the first result with the optimal $K$ dependence in the considered setting. The second algorithm, which is based on the policy optimization framework, guarantees a regret of $\widetilde{\mathcal{O}\left(K^{\frac{3}{4} \right)$ and is computationally efficient. Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023] with $\widetilde{\mathcal{O}\left(K^{\frac{4}{5}+poly\left(\frac{1}{\lambda_{\min}\right) \right)$ regret, for some problem-dependent constant $\lambda_{\min}$ that can be arbitrarily close to zero, and a computationally efficient algorithm by Sherman et al. [2023b] with $\widetilde{\mathcal{O}\left(K^{\frac{6}{7} \right)$ regret.
</details>
<details>
<summary>摘要</summary>
我们研究在线强化学习在线马尔可夫遇到冲突损失和随机反馈下，无需对过程或模拟器的知识。我们介绍了两种算法，它们在比现有方法更好的停损性表现。第一种算法，尽管计算效率低下，但能 garantie $\widetilde{\mathcal{O}\left(\sqrt{K}\right)$ 的停损性，其中 $K$ 是集的集数。这是在考虑的设定中的第一个 $\sqrt{K}$ 依赖性的结果。第二种算法，基于策略优化框架，保证 $\widetilde{\mathcal{O}\left(K^{\frac{3}{4} \right)$ 的停损性，并且计算效率高。我们的结果在现有的state-of-the-art之上进行了显著改进：一个计算不fficient的算法由孔等人（2023）提出的 $\widetilde{\mathcal{O}\left(K^{\frac{4}{5}+poly\left(\frac{1}{\lambda_{\min}\right) \right)$ 的停损性，其中 $\lambda_{\min}$ 是问题依赖的常数，可以是arbitrarily close to zero。以及一个计算效率高的算法由战等人（2023b）提出的 $\widetilde{\mathcal{O}\left(K^{\frac{6}{7} \right)$ 的停损性。
</details></li>
</ul>
<hr>
<h2 id="MUST-P-SRL-Multi-lingual-and-Unified-Syllabification-in-Text-and-Phonetic-Domains-for-Speech-Representation-Learning"><a href="#MUST-P-SRL-Multi-lingual-and-Unified-Syllabification-in-Text-and-Phonetic-Domains-for-Speech-Representation-Learning" class="headerlink" title="MUST&amp;P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning"></a>MUST&amp;P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11541">http://arxiv.org/abs/2310.11541</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/noetits/must_p-srl">https://github.com/noetits/must_p-srl</a></li>
<li>paper_authors: Noé Tits</li>
<li>for: 这个论文旨在提出一种语言特征EXTRACTION方法，特别是自动将多语言词语 syllabified into phonetic transcriptions，并设计与蒙特利尔强制对齐器（MFA）兼容。</li>
<li>methods: 该方法在文本和音频频谱领域均注重提取phonetic transcriptions from text，包括强制对齐器（MFA）中的音频频谱。</li>
<li>results: 通过ablation study，我们证明了我们的方法在多种语言（英语、法语和西班牙语）中自动 syllabify 词语的 efficacy。此外，我们还应用了这种技术到CMU ARCTIC数据集的转录中，生成了在线可用的笔记 Footnote \url{<a target="_blank" rel="noopener" href="https://github.com/noetits/MUST_P-SRL%7D%EF%BC%8C%E8%BF%99%E4%BA%9B%E7%AC%94%E8%AE%B0%E9%9D%9E%E5%B8%B8%E6%9C%89%E7%94%A8%E4%BA%8Espeech">https://github.com/noetits/MUST_P-SRL}，这些笔记非常有用于speech</a> representation learning、speech unit discovery和speech factor的分离在多种speech-related领域。<details>
<summary>Abstract</summary>
In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\footnote{\url{https://github.com/noetits/MUST_P-SRL} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-related fields.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种语言特征提取方法，特别是自动将多语言单词分割成音节，并设计 compatibles 于蒙特利尔强制对齐器（MFA）。在文本和音乐频域中，我们的方法专注于从文本中提取音调标记和统一自动分割（在文本和音乐频域）。系统使用开源组件和资源构建。通过减少研究，我们证明了我们的方法在多种语言（英语、法语和西班牙语）中自动分割单词的效果。此外，我们将该技术应用到CMU ARCTIC数据集的转录中，生成了在线可用的注释\footnotemark[\url{https://github.com/noetits/MUST_P-SRL}]，这些注释非常有价值，适用于语音表示学习、语音单位发现和多种语音相关领域的分离 speech factor。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Online-Learning-with-Offline-Datasets-for-Infinite-Horizon-MDPs-A-Bayesian-Approach"><a href="#Efficient-Online-Learning-with-Offline-Datasets-for-Infinite-Horizon-MDPs-A-Bayesian-Approach" class="headerlink" title="Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach"></a>Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11531">http://arxiv.org/abs/2310.11531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dengwang Tang, Rahul Jain, Botao Hao, Zheng Wen</li>
<li>for: 这个论文研究了在无穷远 Setting中的高效在线学习问题，当有一个离线数据集可以作为开始。</li>
<li>methods: 论文使用了模仿专家的行为策略（通过一个能力参数来 parameterize），并使用了 Bayesian 在线学习算法。</li>
<li>results: 论文表明，如果学习代理人模仿专家的行为策略，就可以在最小化累累 regret 方面做出substantially 更好的表现，并且可以提供$\tilde{O}(\sqrt{T})$的上界 regret bound。<details>
<summary>Abstract</summary>
In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了在无穷远Setting中的高效在线强化学习问题，当有一个离线数据集可以作为开始。我们假设离线数据集由专家生成，但专家的水平不确定，即不是完美的和不一定使用优化策略。我们证明，如果学习代理模仿专家的行为策略（参数化为能力参数），它可以在误差总和方面做出较好的表现，比如果不做这样。我们提出了一个新的先验依赖的误差分析方法，并采用了一种新的准确度估计技术。我们then propose一个近似 Informed RLSVI算法，可以视为在线学习后，进行模仿学习。Here's a breakdown of the translation:* "infinite horizon setting" 是 translated as "无穷远Setting" (wú jì yáo jiāng)* "offline dataset" is translated as "离线数据集" (liú xiàn numérical dataset)* "expert" is translated as "专家" (zhuān shī)* "behavioral policy" is translated as "行为策略" (xíng wù zhèng yì)* "competence parameter" is translated as "能力参数" (néng lì jiāng xiàng)* "online learning" is translated as "在线学习" (zhèng xiàng xué xí)* "imitation learning" is translated as "模仿学习" (mó shì xué xí)Note that the translation is in Simplified Chinese, which is the most commonly used variety of Chinese in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Group-Preference-Optimization-Few-Shot-Alignment-of-Large-Language-Models"><a href="#Group-Preference-Optimization-Few-Shot-Alignment-of-Large-Language-Models" class="headerlink" title="Group Preference Optimization: Few-Shot Alignment of Large Language Models"></a>Group Preference Optimization: Few-Shot Alignment of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11523">http://arxiv.org/abs/2310.11523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyan Zhao, John Dang, Aditya Grover</li>
<li>for: 这篇论文的目的是如何将大型自然语言模型（LLM）调整为不同群体的偏好？</li>
<li>methods: 论文使用了一个名为Group Preference Optimization（GPO）的调整框架，该框架通过将独立的transformer模组与基本LLM模型结合，以便让LLM模型更好地适应不同群体的偏好。</li>
<li>results: 论文通过对不同群体的人类意见项目进行评估，证明了GPO的可行性和高效性，并且需要 fewer group-specific preferences 和 less training and inference computing resources，比较出Perform existing strategies such as in-context steering and fine-tuning methods.<details>
<summary>Abstract</summary>
Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users. Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences, and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods.
</details>
<details>
<summary>摘要</summary>
许多大语言模型（LLM）的应用，从聊天机器人到创作写作，需要具有细化的主观评价，这些评价可能会因不同的群体而异常分布。现有的对Alignment算法可能需要较多的群体特定偏好数据和计算资源，这限制了实际应用场景中的使用。我们介绍了一种名为Group Preference Optimization（GPO）的对Alignment框架，可以在几个尝试中导引语言模型到各个群体的偏好。在GPO中，我们将基础的LLM加上一个独立的转换器模块，用于预测群体的偏好。为了进行几个尝试学习，我们将这个模块参数化为受 Context-Aware Transformer 的扩展，并通过元学习训练在多个群体上。我们通过对LLMs With varied sizes进行实质性验证，证明GPO不仅更准确地对齐模型，还需要 fewer group-specific preferences，并且需要更少的训练和推理计算资源，超越现有的方法，如在Context中引导和精度调整方法。
</details></li>
</ul>
<hr>
<h2 id="Guarantees-for-Self-Play-in-Multiplayer-Games-via-Polymatrix-Decomposability"><a href="#Guarantees-for-Self-Play-in-Multiplayer-Games-via-Polymatrix-Decomposability" class="headerlink" title="Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability"></a>Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11518">http://arxiv.org/abs/2310.11518</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/revanmacqueen/self-play-polymatrix">https://github.com/revanmacqueen/self-play-polymatrix</a></li>
<li>paper_authors: Revan MacQueen, James R. Wright</li>
<li>for: 这个论文关注的问题是如何使用自我玩家来学习多智能体系统中的机器学习算法，并且如何 garantuee这些算法在post-training中的性能。</li>
<li>methods: 论文使用了自我玩家来生成大量数据，并且使用了no-external-regret算法来学习。</li>
<li>results: 论文显示了在多智能体系统中，通过使用自我玩家来学习，可以生成高性能的策略，并且这些策略具有 bounded vulnerability。此外，论文还提供了一种Structural property的定义，可以确保这些策略的性能。<details>
<summary>Abstract</summary>
Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multiplayer games. We show that in games that approximately decompose into a set of two-player constant-sum games (called constant-sum polymatrix games) where global $\epsilon$-Nash equilibria are boundedly far from Nash equilibria in each subgame (called subgame stability), any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify a structural property of multiplayer games that enable performance guarantees for the strategies produced by a broad class of self-play algorithms. We demonstrate our findings through experiments on Leduc poker.
</details>
<details>
<summary>摘要</summary>
自我玩家是一种机器学习技术，用于多代理系统中的学习。在这种情况下，学习算法通过与自己的 копиënteract来学习。自我玩家有利于生成大量数据，但是它们可能会导致学习后面对的代理行为与学习期间预期的行为有很大差异。在特殊的两player常数游戏情况下，如果自我玩家达到尼什平衡，那么生成的策略将在任何后期代理对抗中表现出色。然而，对多代理游戏来说，没有类似的保证。我们表明，在 Approximately decomposable 多代理游戏中，如果每个子游戏中的尼什平衡是 global $\epsilon$-尼什平衡的 boundedly far，那么任何没有外部回报折损的自我玩家学习算法将生成有 bounded vulnerability 的策略。这是我们的结果所示，我们的结论标志着多代理游戏中一种结构性质，允许提供性能保证的策略。我们通过启示 Leduc poker 的实验来证明我们的结论。
</details></li>
</ul>
<hr>
<h2 id="Self-RAG-Learning-to-Retrieve-Generate-and-Critique-through-Self-Reflection"><a href="#Self-RAG-Learning-to-Retrieve-Generate-and-Critique-through-Self-Reflection" class="headerlink" title="Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection"></a>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11511">http://arxiv.org/abs/2310.11511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AkariAsai/self-rag">https://github.com/AkariAsai/self-rag</a></li>
<li>paper_authors: Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi</li>
<li>for: 提高语言模型的准确性和多样性</li>
<li>methods: 引入自适应式检索和反思 tokens，使语言模型在推理阶段可控</li>
<li>results: 比对状元模型和检索补充模型在多种任务上表现出色，特别是在开放领域问答、理解和事实核查任务中表现出较高的准确性和引用精度。<details>
<summary>Abstract</summary>
Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.
</details>
<details>
<summary>摘要</summary>
尽管大型语言模型（LLM）具有remarkable的能力，但它们经常生成包含错误信息的响应，这是因为它们仅仅依靠自己学习的参数知识。 Retrieval-Augmented Generation（RAG）是一种补做approach，它在LLM中添加了Retrieval的功能，从而减少这些问题。然而，不经过考虑地检索并 incorporate一定数量的检索到的段落，可能会减少LLM的灵活性，或者导致生成异常的响应。我们提出了一种新的框架，叫做Self-Reflective Retrieval-Augmented Generation（Self-RAG），它可以提高LLM的质量和准确性。我们的框架通过在检索和生成过程中适时使用特殊的 tokens， called reflection tokens，来让LLM在推理阶段变得可控。通过生成reflection tokens，LLM可以根据不同的任务要求进行自适应的调整。我们的实验表明，Self-RAG（7B和13B参数）在多种任务上比state-of-the-art LLMs和检索增强模型表现出色，具体来说，Self-RAG在开放领域问答、理解和事实核实任务上表现出色，并且在长形生成中提高了事实准确性和引用率。
</details></li>
</ul>
<hr>
<h2 id="CoMPosT-Characterizing-and-Evaluating-Caricature-in-LLM-Simulations"><a href="#CoMPosT-Characterizing-and-Evaluating-Caricature-in-LLM-Simulations" class="headerlink" title="CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations"></a>CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11501">http://arxiv.org/abs/2310.11501</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/myracheng/lm_caricature">https://github.com/myracheng/lm_caricature</a></li>
<li>paper_authors: Myra Cheng, Tiziano Piccardi, Diyi Yang</li>
<li>for: 这个论文旨在捕捉人类行为的细腻之处，使用LLM模型模拟特定人群在社会科学实验和公众意见调查中的反应。</li>
<li>methods: 该论文使用了LLM模型来模拟人类行为，并提出了一个框架来评估这些模拟的质量。</li>
<li>results: 研究发现，在某些情况下（如政治和受难群体）和主题（通用不带争议）中，GPT-4模拟的人物有较高的轮廓化风险。<details>
<summary>Abstract</summary>
Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "LLMs" is translated as "人工智能语言模型" (Rényou Jīngjì Yǔyán Módel), which is a more common term in Simplified Chinese.* "simulations" is translated as "模拟" (Móxīng), which is a more general term that can refer to any kind of imitation or representation.* "caricature" is translated as "轮廓" (Lúnkè), which is a more precise term that specifically refers to an exaggerated or distorted representation of someone or something.* "individuation" is translated as "个体化" (Gètǐ Huà), which is a term that specifically refers to the process of creating unique and distinct individuals.* "exaggeration" is translated as "夸大" (Kuòdà), which is a term that specifically refers to the act of amplifying or enlarging something beyond its normal size or proportion.
</details></li>
</ul>
<hr>
<h2 id="Seeking-Neural-Nuggets-Knowledge-Transfer-in-Large-Language-Models-from-a-Parametric-Perspective"><a href="#Seeking-Neural-Nuggets-Knowledge-Transfer-in-Large-Language-Models-from-a-Parametric-Perspective" class="headerlink" title="Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective"></a>Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11451">http://arxiv.org/abs/2310.11451</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maszhongming/ParaKnowTransfer">https://github.com/maszhongming/ParaKnowTransfer</a></li>
<li>paper_authors: Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han, Pengcheng He</li>
<li>for: 本研究旨在empirically investigate知识传递FROM大型语言模型（LLMs）到小型语言模型（SLMs）的 Parametric perspective。</li>
<li>methods: 我们使用敏感性技术提取和对齐知识特定参数 между不同的LLMs，并使用LoRA模块作为中介机制将提取的知识注入到SLMs中。</li>
<li>results: 我们的evaluaions across four benchmarks validate the efficacy of our proposed method, highlighting the critical factors contributing to the process of parametric knowledge transfer and underscoring the transferability of model parameters across LLMs of different scales.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge (encompassing detection, editing, and merging), there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. We release code and data at \url{https://github.com/maszhongming/ParaKnowTransfer}.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）内置了丰富的知识于其参数中，通过前期训练于广泛的文本Corpus。 although prior research has explored operations on these parameters to manipulate the underlying implicit knowledge (including detection, editing, and merging), there is still an ambiguous understanding of their transferability across models with varying scales. In this paper, we aim to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we use sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. We release code and data at \url{https://github.com/maszhongming/ParaKnowTransfer}.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Explaining-Deep-Neural-Networks-for-Bearing-Fault-Detection-with-Vibration-Concepts"><a href="#Explaining-Deep-Neural-Networks-for-Bearing-Fault-Detection-with-Vibration-Concepts" class="headerlink" title="Explaining Deep Neural Networks for Bearing Fault Detection with Vibration Concepts"></a>Explaining Deep Neural Networks for Bearing Fault Detection with Vibration Concepts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11450">http://arxiv.org/abs/2310.11450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Decker, Michael Lebacher, Volker Tresp</li>
<li>for: 本研究旨在应用概念基于的解释方法，以帮助解释复杂深度神经网络在振荡信号上进行磨损fault探测中的预测结果。</li>
<li>methods: 本研究使用了已有的概念基于的解释技术，包括概念活化矢量，来衡量输入数据中抽象或高级特征如何影响神经网络的预测结果。</li>
<li>results: 本研究的评估结果表明，通过使用概念基于的解释方法，可以提供人类可理解的和直观的探测结果，但是需要首先验证下面的假设。<details>
<summary>Abstract</summary>
Concept-based explanation methods, such as Concept Activation Vectors, are potent means to quantify how abstract or high-level characteristics of input data influence the predictions of complex deep neural networks. However, applying them to industrial prediction problems is challenging as it is not immediately clear how to define and access appropriate concepts for individual use cases and specific data types. In this work, we investigate how to leverage established concept-based explanation techniques in the context of bearing fault detection with deep neural networks trained on vibration signals. Since bearings are prevalent in almost every rotating equipment, ensuring the reliability of intransparent fault detection models is crucial to prevent costly repairs and downtimes of industrial machinery. Our evaluations demonstrate that explaining opaque models in terms of vibration concepts enables human-comprehensible and intuitive insights about their inner workings, but the underlying assumptions need to be carefully validated first.
</details>
<details>
<summary>摘要</summary>
通用概念解释方法，如概念启动向量，是复杂深度神经网络预测结果的强大手段。然而，在实际应用中存在许多挑战，因为不清楚如何定义和访问特定用例和数据类型的适当概念。在这种情况下，我们研究如何在深度神经网络对振荡信号进行报废疾病检测时，使用已有的概念基本解释技术。由于机器设备中的承轮很普遍，因此确保报废模型的可靠性是关键，以避免高昂的维护和机器停机成本。我们的评估表明，通过将深度神经网络解释为振荡概念的方式，可以提供人类可理解和直观的内部启示，但是下面的假设需要首先进行验证。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Prediction-Capabilities-Evidence-from-a-Real-World-Forecasting-Tournament"><a href="#Large-Language-Model-Prediction-Capabilities-Evidence-from-a-Real-World-Forecasting-Tournament" class="headerlink" title="Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament"></a>Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13014">http://arxiv.org/abs/2310.13014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Schoenegger, Peter S. Park</li>
<li>for: 测试大语言模型GPT-4的预测能力</li>
<li>methods: 使用GPT-4参加三个月的预测赛，测试其对未来事件的预测能力</li>
<li>results: GPT-4的预测不如人群预测准确，并且没有显著的偏好 towards any particular answer.<details>
<summary>Abstract</summary>
Accurately predicting the future would be an important milestone in the capabilities of artificial intelligence. However, research on the ability of large language models to provide probabilistic predictions about future events remains nascent. To empirically test this ability, we enrolled OpenAI's state-of-the-art large language model, GPT-4, in a three-month forecasting tournament hosted on the Metaculus platform. The tournament, running from July to October 2023, attracted 843 participants and covered diverse topics including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict. Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts. We find that GPT-4's forecasts did not significantly differ from the no-information forecasting strategy of assigning a 50% probability to every question. We explore a potential explanation, that GPT-4 might be predisposed to predict probabilities close to the midpoint of the scale, but our data do not support this hypothesis. Overall, we find that GPT-4 significantly underperforms in real-world predictive tasks compared to median human-crowd forecasts. A potential explanation for this underperformance is that in real-world forecasting tournaments, the true answers are genuinely unknown at the time of prediction; unlike in other benchmark tasks like professional exams or time series forecasting, where strong performance may at least partly be due to the answers being memorized from the training data. This makes real-world forecasting tournaments an ideal environment for testing the generalized reasoning and prediction capabilities of artificial intelligence going forward.
</details>
<details>
<summary>摘要</summary>
如果可以准确预测未来，那将是人工智能的重要突破口。然而，关于大语言模型是否能够提供关于未来事件的 probabilistic 预测的研究仍然处于早期阶段。为了实证这一能力，我们在2023年7月至10月的三个月时间内参加了OpenAI的state-of-the-art大语言模型GPT-4在Metaculus平台上的三个月预测赛。这场赛事吸引了843名参与者，涵盖了多个话题，包括大科技、美国政治、艾滋疫情和乌克兰冲突。我们对binary forecast进行了分析，发现GPT-4的 probabilistic 预测与人群预测的 median 相比远不准确。我们发现GPT-4的预测与无信息预测策略（每个问题的概率为50%）没有显著差异。我们还探讨了一个可能的解释：GPT-4可能倾向于预测概率较 бли至中点的情况，但我们的数据不支持这一假设。总的来说，我们发现GPT-4在真实世界的预测任务中表现 significatively 差，与人群预测的 median 相比。一个可能的解释是，在真实世界预测赛事中，答案并不是在预测时已知的；与其他benchmark任务like专业考试或时间序列预测不同，在这些任务中，AI的出色表现可能归结于训练数据中Memorization。这些因素使得真实世界预测赛事成为测试人工智能总化逻辑和预测能力的理想环境。
</details></li>
</ul>
<hr>
<h2 id="Functional-Invariants-to-Watermark-Large-Transformers"><a href="#Functional-Invariants-to-Watermark-Large-Transformers" class="headerlink" title="Functional Invariants to Watermark Large Transformers"></a>Functional Invariants to Watermark Large Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11446">http://arxiv.org/abs/2310.11446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernandez Pierre, Couairon Guillaume, Furon Teddy, Douze Matthijs</li>
<li>for: 保护大型模型的完整性和所有权</li>
<li>methods: 利用模型的对称性进行功能等价的替换操作，不需要对 weights 进行优化，可以在非盲目白盒设置下实现 watermarking</li>
<li>results: 实验表明该方法可以保持模型的输出不变，同时具有耐变性和隐蔽性，可以实用地保护大型模型的完整性和所有权<details>
<summary>Abstract</summary>
The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance. Watermarking addresses this issue by embedding a unique identifier into the model, while preserving its performance. However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost. This paper explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks). They generate functionally equivalent copies by leveraging the models' invariance, via operations like dimension permutations or scaling/unscaling. This enables to watermark models without any change in their outputs and remains stealthy. Experiments demonstrate the effectiveness of the approach and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to protect the integrity of large models.
</details>
<details>
<summary>摘要</summary>
“transformer模型的快速增长带来了其完整性和所有权保险的问题。水印可以解决这个问题，通过在模型中嵌入唯一标识符，保持其性能。但现有的方法通常需要优化权重来印制水印信号，这会在大规模执行中带来计算成本问题。这篇论文探讨了免计算成本的水印方法，适用于非盲目白盒设定（即可以访问原始网络和水印网络）。它通过利用模型的变换不变性，通过维度重新排序或缩放/减速等操作，生成功能相同的模型复制。这些复制品可以隐蔽地携带水印信号，无需改变模型的输出。实验表明该方法的有效性和鲁棒性，可以实际地保护大型模型的完整性。”
</details></li>
</ul>
<hr>
<h2 id="Set-of-Mark-Prompting-Unleashes-Extraordinary-Visual-Grounding-in-GPT-4V"><a href="#Set-of-Mark-Prompting-Unleashes-Extraordinary-Visual-Grounding-in-GPT-4V" class="headerlink" title="Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"></a>Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11441">http://arxiv.org/abs/2310.11441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng Gao</li>
<li>for: 提高大型多Modal模型（LMMs）的视觉固定能力</li>
<li>methods: 使用可购买的交互分割模型（如SAM）将图像分成不同粒度的区域，并在这些区域上显示marks（如字母、面罩、盒子）</li>
<li>results: GPT-4V与SoM相比，在RefCOCOg zero-shot Setting中表现出比州前进referring segmentation模型更高的性能<details>
<summary>Abstract</summary>
We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring segmentation model on RefCOCOg in a zero-shot setting.
</details>
<details>
<summary>摘要</summary>
我团队在这篇论文中提出了一种新的视觉提示方法，即Set-of-Mark（SoM），用于解 liberate大型多 modal模型（LMMs）的视觉固定能力。如图1（右）所示，我们使用商业化的交互式分割模型，如SAM，将图像分成不同粒度的区域，并将这些区域 overlay 以不同的标记，如字母、面具、盒子。使用这些标记的图像作为输入，GPT-4V可以回答需要视觉固定的问题。我们进行了广泛的实验研究，以验证SoM在多种细化视觉和多模态任务中的效果。例如，我们的实验表明，GPT-4V与SoM在RefCOCOg中的零基础情况下，与现有的全部finetune Referring Segmentation模型相比，表现出了更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Understanding-deep-neural-networks-through-the-lens-of-their-non-linearity"><a href="#Understanding-deep-neural-networks-through-the-lens-of-their-non-linearity" class="headerlink" title="Understanding deep neural networks through the lens of their non-linearity"></a>Understanding deep neural networks through the lens of their non-linearity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11439">http://arxiv.org/abs/2310.11439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Bouniot, Ievgen Redko, Anton Mallasto, Charlotte Laclau, Karol Arndt, Oliver Struckmeier, Markus Heinonen, Ville Kyrki, Samuel Kaski</li>
<li>for: 本研究旨在提供一种 theoretically sound 的方法来跟踪深度神经网络中非线性的传播。</li>
<li>methods: 本研究使用的方法包括计算激活函数的非线性度，并通过提出一种 affinity score 来评估不同架构和学习策略中的非线性传播。</li>
<li>results: 实验结果表明，提出的 affinity score 能够帮助我们更好地理解深度神经网络中的非线性传播，并且具有广泛的实际应用前景。<details>
<summary>Abstract</summary>
The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）的异常成功常被归结于它们的高表达力和能够近似任何复杂性函数的能力。实际上，DNN是非线性模型，活动函数引入到它们中央对此负有责任。虽然许多研究对DNN的表达力进行了研究，但量化DNN的非线性或各个活动函数的非线性仍然是一个开放的问题。在这篇论文中，我们提出了首个理论上有sound的解决方案，用于跟踪深度神经网络中非线性的传播。我们提出的相互关系分数允许我们深入了解各种不同架构和学习方法的内部工作机制。我们提供了广泛的实验结果， highlighting the practical utility of the proposed affinity score and its potential for long-reaching applications.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-LLMs-for-Privilege-Escalation-Scenarios"><a href="#Evaluating-LLMs-for-Privilege-Escalation-Scenarios" class="headerlink" title="Evaluating LLMs for Privilege-Escalation Scenarios"></a>Evaluating LLMs for Privilege-Escalation Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11409">http://arxiv.org/abs/2310.11409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ipa-lab/hackingBuddyGPT">https://github.com/ipa-lab/hackingBuddyGPT</a></li>
<li>paper_authors: Andreas Happe, Aaron Kaplan, Jürgen Cito</li>
<li>for: 这个论文旨在探讨语言模型（LLM）在静态分析中的应用，以及它们在特权提升方面的能力和挑战。</li>
<li>methods: 作者使用了自动化的Linux特权提升测试 benchmark，并开发了一种基于 LLM 的特权提升工具，用于评估不同的 LLM 和提示策略。</li>
<li>results: 研究发现，LLM 可以在特权提升方面具有很高的能力，但也存在一些挑战，如维持测试中的集中性、处理错误等。<details>
<summary>Abstract</summary>
Penetration testing, an essential component of cybersecurity, allows organizations to proactively identify and remediate vulnerabilities in their systems, thus bolstering their defense mechanisms against potential cyberattacks. One recent advancement in the realm of penetration testing is the utilization of Language Models (LLMs). We explore the intersection of LLMs and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation. We create an automated Linux privilege-escalation benchmark utilizing local virtual machines. We introduce an LLM-guided privilege-escalation tool designed for evaluating different LLMs and prompt strategies against our benchmark. We analyze the impact of different prompt designs, the benefits of in-context learning, and the advantages of offering high-level guidance to LLMs. We discuss challenging areas for LLMs, including maintaining focus during testing, coping with errors, and finally comparing them with both stochastic parrots as well as with human hackers.
</details>
<details>
<summary>摘要</summary>
To evaluate the performance of LLMs in privilege escalation, we created an automated Linux privilege-escalation benchmark using local virtual machines. We also developed an LLM-guided privilege-escalation tool that assesses different LLMs and prompt strategies against our benchmark. Our analysis reveals the impact of various prompt designs, the benefits of in-context learning, and the advantages of providing high-level guidance to LLMs.However, LLMs also face challenges, such as maintaining focus during testing, coping with errors, and comparing with both stochastic parrots and human hackers. By understanding these challenges and the capabilities of LLMs, we can better utilize them in penetration testing to enhance the security of our systems.
</details></li>
</ul>
<hr>
<h2 id="Neural-Attention-Enhancing-QKV-Calculation-in-Self-Attention-Mechanism-with-Neural-Networks"><a href="#Neural-Attention-Enhancing-QKV-Calculation-in-Self-Attention-Mechanism-with-Neural-Networks" class="headerlink" title="Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks"></a>Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11398">http://arxiv.org/abs/2310.11398</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ocislyjrti/neuralattention">https://github.com/ocislyjrti/neuralattention</a></li>
<li>paper_authors: Muhan Zhang</li>
<li>for: 这篇论文是用于探讨一种新的自注意力机制，即通过特殊设计的神经网络结构来计算查询、键和值（QKV），以提高自注意力机制的表现。</li>
<li>methods: 这篇论文使用了一个 modificated Marian model，通过实验证明了其对IWSLT 2017德国英语翻译任务数据集的表现有所提高。此外，这篇论文还证明了它的方法在训练Roberta模型时，对Wikitext-103数据集的表现也有所改善。</li>
<li>results: 实验结果显示，这篇论文的方法可以提高BLEU scores，并且在训练Roberta模型时，对模型的误差率有所降低。这些实验结果不仅证明了这篇论文的方法的有效性，而且还显示了对自注意力机制的优化可以通过神经网络基础的QKV计算，对未来的研究和实际应用具有广泛的应用前景。<details>
<summary>Abstract</summary>
In the realm of deep learning, the self-attention mechanism has substantiated its pivotal role across a myriad of tasks, encompassing natural language processing and computer vision. Despite achieving success across diverse applications, the traditional self-attention mechanism primarily leverages linear transformations for the computation of query, key, and value (QKV), which may not invariably be the optimal choice under specific circumstances. This paper probes into a novel methodology for QKV computation-implementing a specially-designed neural network structure for the calculation. Utilizing a modified Marian model, we conducted experiments on the IWSLT 2017 German-English translation task dataset and juxtaposed our method with the conventional approach. The experimental results unveil a significant enhancement in BLEU scores with our method. Furthermore, our approach also manifested superiority when training the Roberta model with the Wikitext-103 dataset, reflecting a notable reduction in model perplexity compared to its original counterpart. These experimental outcomes not only validate the efficacy of our method but also reveal the immense potential in optimizing the self-attention mechanism through neural network-based QKV computation, paving the way for future research and practical applications. The source code and implementation details for our proposed method can be accessed at https://github.com/ocislyjrti/NeuralAttention.
</details>
<details>
<summary>摘要</summary>
在深度学习领域，自注意机制在多种任务中发挥了重要作用，包括自然语言处理和计算机视觉。尽管在多种应用中取得成功，传统的自注意机制通常使用线性变换来计算查询、关键和值（QKV），这可能不一定是最佳选择在特定情况下。这篇论文探讨了一种新的QKV计算方法-通过特制的神经网络结构来实现。我们使用修改后的马里安模型进行实验，并在IWSLT 2017德语英语翻译任务数据集上对我们的方法与传统方法进行了比较。实验结果显示我们的方法可以显著提高BLEU分数，而且我们的方法也在使用Wikitext-103数据集训练Roberta模型时表现出了明显的降低模型困惑度，相比其原始对手。这些实验结果不仅证明了我们的方法的有效性，还暴露了优化自注意机制的可能性，铺开了未来研究和实践应用的道路。我们的代码和实现细节可以通过https://github.com/ocislyjrti/NeuralAttention访问。
</details></li>
</ul>
<hr>
<h2 id="Towards-Automatic-Satellite-Images-Captions-Generation-Using-Large-Language-Models"><a href="#Towards-Automatic-Satellite-Images-Captions-Generation-Using-Large-Language-Models" class="headerlink" title="Towards Automatic Satellite Images Captions Generation Using Large Language Models"></a>Towards Automatic Satellite Images Captions Generation Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11392">http://arxiv.org/abs/2310.11392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingxu He, Qiqi Sun</li>
<li>for:  automatic remote sensing image captioning</li>
<li>methods:  using large language models (LLMs) to guide the description of object annotations</li>
<li>results:  effective collection of captions for remote sensing images<details>
<summary>Abstract</summary>
Automatic image captioning is a promising technique for conveying visual information using natural language. It can benefit various tasks in satellite remote sensing, such as environmental monitoring, resource management, disaster management, etc. However, one of the main challenges in this domain is the lack of large-scale image-caption datasets, as they require a lot of human expertise and effort to create. Recent research on large language models (LLMs) has demonstrated their impressive performance in natural language understanding and generation tasks. Nonetheless, most of them cannot handle images (GPT-3.5, Falcon, Claude, etc.), while conventional captioning models pre-trained on general ground-view images often fail to produce detailed and accurate captions for aerial images (BLIP, GIT, CM3, CM3Leon, etc.). To address this problem, we propose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) to automatically collect captions for remote sensing images by guiding LLMs to describe their object annotations. We also present a benchmark model that adapts the pre-trained generative image2text model (GIT) to generate high-quality captions for remote-sensing images. Our evaluation demonstrates the effectiveness of our approach for collecting captions for remote sensing images.
</details>
<details>
<summary>摘要</summary>
自动图像描述技术是落实视觉信息使用自然语言的有前途技术。它可以帮助卫星遥感任务中的各种任务，如环境监测、资源管理、灾害管理等。然而，这个领域的主要挑战之一是缺乏大规模的图像描述数据集，因为需要大量的人工专业和努力来创建。latest research on large language models (LLMs) has shown their impressive performance in natural language understanding and generation tasks. However, most of them cannot handle images (GPT-3.5, Falcon, Claude, etc.), while conventional captioning models pre-trained on general ground-view images often fail to produce detailed and accurate captions for aerial images (BLIP, GIT, CM3, CM3Leon, etc.). To address this problem, we propose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) to automatically collect captions for remote sensing images by guiding LLMs to describe their object annotations. We also present a benchmark model that adapts the pre-trained generative image2text model (GIT) to generate high-quality captions for remote-sensing images. Our evaluation demonstrates the effectiveness of our approach for collecting captions for remote sensing images.
</details></li>
</ul>
<hr>
<h2 id="End-to-End-real-time-tracking-of-children’s-reading-with-pointer-network"><a href="#End-to-End-real-time-tracking-of-children’s-reading-with-pointer-network" class="headerlink" title="End-to-End real time tracking of children’s reading with pointer network"></a>End-to-End real time tracking of children’s reading with pointer network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11486">http://arxiv.org/abs/2310.11486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishal Sunder, Beulah Karrolla, Eric Fosler-Lussier</li>
<li>for: 这个论文的目的是如何fficiently构建儿童语音实时读物追踪器。</li>
<li>methods: 这个论文使用了一种全端到端模型，而不是以前提出的ASR-based缓存方法。它使用了一个指针网络，直接学习预测文本中的位置，并通过强制对齐来训练指针网络。</li>
<li>results: 这个论文的结果表明，使用一种基于强制对齐的神经网络模型可以达到至少与Montreal强制对齐器相同的对齐精度，并且 surprisingly 是一个更好的训练信号 для指针网络。结果表明，在一个成人语音数据集（TIMIT）和两个儿童语音数据集（CMU Kids和Reading Races）上，这个最佳模型可以高精度地跟踪成人语音（87.8%）和儿童语音（77.1%）。<details>
<summary>Abstract</summary>
In this work, we explore how a real time reading tracker can be built efficiently for children's voices. While previously proposed reading trackers focused on ASR-based cascaded approaches, we propose a fully end-to-end model making it less prone to lags in voice tracking. We employ a pointer network that directly learns to predict positions in the ground truth text conditioned on the streaming speech. To train this pointer network, we generate ground truth training signals by using forced alignment between the read speech and the text being read on the training set. Exploring different forced alignment models, we find a neural attention based model is at least as close in alignment accuracy to the Montreal Forced Aligner, but surprisingly is a better training signal for the pointer network. Our results are reported on one adult speech data (TIMIT) and two children's speech datasets (CMU Kids and Reading Races). Our best model can accurately track adult speech with 87.8% accuracy and the much harder and disfluent children's speech with 77.1% accuracy on CMU Kids data and a 65.3% accuracy on the Reading Races dataset.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们探讨了如何fficiently构建儿童语音实时读写追踪器。之前的提议的读写追踪器都是基于ASR顺序的搅拌方法，而我们提议一个完全端到端模型，使其更少受到语音追踪延迟。我们使用一个指针网络，直接学习 predict文本中的位置，条件于流动的Speech。为了训练这个指针网络，我们生成了ground truth训练信号，通过强制对READING speech和被读文本之间进行对齐。我们explore了不同的强制对齐模型，发现一个神经网络注意力基于模型，与蒙特利尔强制对齐器准确性相当，但是 surprisingly是一个更好的训练信号 для指针网络。我们的结果在一个成人语音数据（TIMIT）和两个儿童语音数据集（CMU Kids和Reading Races）上被报告。我们的最佳模型可以准确地跟踪成人语音87.8%的准确率，以及更难和不稳定的儿童语音77.1%的准确率在CMU Kids数据集上，以及65.3%的准确率在Reading Races数据集上。
</details></li>
</ul>
<hr>
<h2 id="The-effect-of-stemming-and-lemmatization-on-Portuguese-fake-news-text-classification"><a href="#The-effect-of-stemming-and-lemmatization-on-Portuguese-fake-news-text-classification" class="headerlink" title="The effect of stemming and lemmatization on Portuguese fake news text classification"></a>The effect of stemming and lemmatization on Portuguese fake news text classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11344">http://arxiv.org/abs/2310.11344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucca de Freitas Santos, Murilo Varges da Silva</li>
<li>for: 本研究旨在探讨自动伪新闻检测的问题，尤其是在语言学方面，以提高伪新闻检测的精度。</li>
<li>methods: 本研究使用了lemmatization和stemming等预处理技术，并设计了一些类ifier模型，以测试预处理技术对伪新闻分类的影响。</li>
<li>results: 结果显示，预处理步骤对伪新闻分类有着重要的影响，lemmatization和stemming等技术可以帮助提高伪新闻检测的精度。<details>
<summary>Abstract</summary>
With the popularization of the internet, smartphones and social media, information is being spread quickly and easily way, which implies bigger traffic of information in the world, but there is a problem that is harming society with the dissemination of fake news. With a bigger flow of information, some people are trying to disseminate deceptive information and fake news. The automatic detection of fake news is a challenging task because to obtain a good result is necessary to deal with linguistics problems, especially when we are dealing with languages that not have been comprehensively studied yet, besides that, some techniques can help to reach a good result when we are dealing with text data, although, the motivation of detecting this deceptive information it is in the fact that the people need to know which information is true and trustful and which one is not. In this work, we present the effect the pre-processing methods such as lemmatization and stemming have on fake news classification, for that we designed some classifier models applying different pre-processing techniques. The results show that the pre-processing step is important to obtain betters results, the stemming and lemmatization techniques are interesting methods and need to be more studied to develop techniques focused on the Portuguese language so we can reach better results.
</details>
<details>
<summary>摘要</summary>
In this work, we investigate the effect of pre-processing methods, such as lemmatization and stemming, on fake news classification. We designed several classifier models using different pre-processing techniques and analyzed the results. Our findings show that the pre-processing step is crucial for achieving better results. The stemming and lemmatization techniques are promising methods that deserve further study, particularly for the Portuguese language, to improve the accuracy of fake news detection.
</details></li>
</ul>
<hr>
<h2 id="Dual-Cognitive-Architecture-Incorporating-Biases-and-Multi-Memory-Systems-for-Lifelong-Learning"><a href="#Dual-Cognitive-Architecture-Incorporating-Biases-and-Multi-Memory-Systems-for-Lifelong-Learning" class="headerlink" title="Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems for Lifelong Learning"></a>Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems for Lifelong Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11341">http://arxiv.org/abs/2310.11341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neurai-lab/dn4il-dataset">https://github.com/neurai-lab/dn4il-dataset</a></li>
<li>paper_authors: Shruthi Gowda, Bahram Zonooz, Elahe Arani<br>for: 这篇论文的目的是探讨人工神经网络（ANNs）在静止独立数据上的局限性，并提出一种基于人类认知结构和多记忆系统的新框架，以实现人工智能的持久学习能力。methods: 该框架基于多种人类认知结构和多记忆系统，包括多个子系统、隐式和显式知识表示分离、偏见适应和多记忆系统。它还包括一个概率适应学习器，用于编码形态信息，从而避免ANNs学习本地文本的偏好。results: 在不同的设定和数据集上，DUCA显示出了改进的表现，并且不需要额外信息。此外，DUCA还在一个复杂的域逐渐学习数据集DN4IL上表现出优异，证明了其在面临分布转换时的多样化持久学习能力。<details>
<summary>Abstract</summary>
Artificial neural networks (ANNs) exhibit a narrow scope of expertise on stationary independent data. However, the data in the real world is continuous and dynamic, and ANNs must adapt to novel scenarios while also retaining the learned knowledge to become lifelong learners. The ability of humans to excel at these tasks can be attributed to multiple factors ranging from cognitive computational structures, cognitive biases, and the multi-memory systems in the brain. We incorporate key concepts from each of these to design a novel framework, Dual Cognitive Architecture (DUCA), which includes multiple sub-systems, implicit and explicit knowledge representation dichotomy, inductive bias, and a multi-memory system. The inductive bias learner within DUCA is instrumental in encoding shape information, effectively countering the tendency of ANNs to learn local textures. Simultaneously, the inclusion of a semantic memory submodule facilitates the gradual consolidation of knowledge, replicating the dynamics observed in fast and slow learning systems, reminiscent of the principles underpinning the complementary learning system in human cognition. DUCA shows improvement across different settings and datasets, and it also exhibits reduced task recency bias, without the need for extra information. To further test the versatility of lifelong learning methods on a challenging distribution shift, we introduce a novel domain-incremental dataset DN4IL. In addition to improving performance on existing benchmarks, DUCA also demonstrates superior performance on this complex dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Agent-Specific-Effects"><a href="#Agent-Specific-Effects" class="headerlink" title="Agent-Specific Effects"></a>Agent-Specific Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11334">http://arxiv.org/abs/2310.11334</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stelios30/agent-specific-effects">https://github.com/stelios30/agent-specific-effects</a></li>
<li>paper_authors: Stelios Triantafyllou, Aleksa Sukovic, Debmalya Mandal, Goran Radanovic</li>
<li>for: 本研究的目标是提供一种系统性的方法，以评估多个代理的决策对结果的影响。</li>
<li>methods: 本研究使用多个代理Markov决策过程，引入代理特有的影响（Agent-Specific Effects，ASE），用于评估代理的决策对结果的影响。</li>
<li>results: 研究发现，使用cf-ASE可以准确地评估多个代理的决策对结果的影响，并且可以通过实验 validate 这种方法。<details>
<summary>Abstract</summary>
Establishing causal relationships between actions and outcomes is fundamental for accountable multi-agent decision-making. However, interpreting and quantifying agents' contributions to such relationships pose significant challenges. These challenges are particularly prominent in the context of multi-agent sequential decision-making, where the causal effect of an agent's action on the outcome depends on how the other agents respond to that action. In this paper, our objective is to present a systematic approach for attributing the causal effects of agents' actions to the influence they exert on other agents. Focusing on multi-agent Markov decision processes, we introduce agent-specific effects (ASE), a novel causal quantity that measures the effect of an agent's action on the outcome that propagates through other agents. We then turn to the counterfactual counterpart of ASE (cf-ASE), provide a sufficient set of conditions for identifying cf-ASE, and propose a practical sampling-based algorithm for estimating it. Finally, we experimentally evaluate the utility of cf-ASE through a simulation-based testbed, which includes a sepsis management environment.
</details>
<details>
<summary>摘要</summary>
We focus on multi-agent Markov decision processes and introduce agent-specific effects (ASE), a novel causal quantity that measures the effect of an agent's action on the outcome that propagates through other agents. We then introduce the counterfactual counterpart of ASE (cf-ASE) and provide a set of conditions for identifying it. We propose a practical sampling-based algorithm for estimating cf-ASE.We experimentally evaluate the utility of cf-ASE through a simulation-based testbed, including a sepsis management environment. By attributing the causal effects of agents' actions to their influence on other agents, our approach enables accountable decision-making in multi-agent systems.
</details></li>
</ul>
<hr>
<h2 id="Key-Point-based-Orientation-Estimation-of-Strawberries-for-Robotic-Fruit-Picking"><a href="#Key-Point-based-Orientation-Estimation-of-Strawberries-for-Robotic-Fruit-Picking" class="headerlink" title="Key Point-based Orientation Estimation of Strawberries for Robotic Fruit Picking"></a>Key Point-based Orientation Estimation of Strawberries for Robotic Fruit Picking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11333">http://arxiv.org/abs/2310.11333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Le Louëdec, Grzegorz Cielniak</li>
<li>For:  This paper aims to address the issue of labor shortages in modern agriculture by developing a robotic harvesting system that can accurately and efficiently pick fruit.* Methods: The proposed method uses key-point-based fruit orientation estimation, which allows for the prediction of 3D orientation from 2D images directly. The method does not require full 3D orientation annotations and can exploit such information for improved accuracy.* Results: The proposed method achieves state-of-the-art performance with an average error of $8^\circ$, improving predictions by $\sim30%$ compared to previous work. The method also has fast inference times of $\sim30$ms, making it suitable for real-time robotic applications.Here is the text in Simplified Chinese:* For: 这篇论文目的是解决现代农业中的劳动力短缺问题，通过开发一种可以准确地和高效地采集水果的 роботизирован采集系统。* Methods: 该方法使用关键点基于的水果方向估计方法，可以直接从2D图像中预测3D方向。该方法不需要全部3D方向注释，可以利用这些信息来提高准确性。* Results: 该方法在两个独立的苺果图像集上取得了状态机器人应用中的最佳性能，错误率为8度，提高了前一个作者在~\cite{wagner2021efficient}中提出的前一个作者的30%。此外，该方法的推理时间为30ms，适用于实时机器人应用。<details>
<summary>Abstract</summary>
Selective robotic harvesting is a promising technological solution to address labour shortages which are affecting modern agriculture in many parts of the world. For an accurate and efficient picking process, a robotic harvester requires the precise location and orientation of the fruit to effectively plan the trajectory of the end effector. The current methods for estimating fruit orientation employ either complete 3D information which typically requires registration from multiple views or rely on fully-supervised learning techniques, which require difficult-to-obtain manual annotation of the reference orientation. In this paper, we introduce a novel key-point-based fruit orientation estimation method allowing for the prediction of 3D orientation from 2D images directly. The proposed technique can work without full 3D orientation annotations but can also exploit such information for improved accuracy. We evaluate our work on two separate datasets of strawberry images obtained from real-world data collection scenarios. Our proposed method achieves state-of-the-art performance with an average error as low as $8^{\circ}$, improving predictions by $\sim30\%$ compared to previous work presented in~\cite{wagner2021efficient}. Furthermore, our method is suited for real-time robotic applications with fast inference times of $\sim30$ms.
</details>
<details>
<summary>摘要</summary>
选择性机器人收割是一种有前途的技术解决方案，用于解决现代农业中的劳动力短缺问题。为了实现准确和高效的摘取过程，机器人收割器需要准确地知道果实的位置和方向。现有的果实方向估计方法可以通过多视图注册或完全超级vised学习技术来实现，但这些方法具有困难获得的手动参考方向的缺点。在本文中，我们介绍了一种新的关键点基于果实方向估计方法，可以直接从2D图像中预测3D方向。我们的提议方法不需要全部3D方向注释，但也可以利用这些信息以提高准确性。我们在两个独立的苹果图像集上进行了评估，并达到了现有最佳性能，具体错误为$8^{\circ}$，提高了前一个工作（refer to ~\cite{wagner2021efficient}）的预测值 by $\sim30\%$。此外，我们的方法适用于实时机器人应用程序，推理时间为$\sim30$ms。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Language-Models’-Sensitivity-to-Spurious-Features-in-Prompt-Design-or-How-I-learned-to-start-worrying-about-prompt-formatting"><a href="#Quantifying-Language-Models’-Sensitivity-to-Spurious-Features-in-Prompt-Design-or-How-I-learned-to-start-worrying-about-prompt-formatting" class="headerlink" title="Quantifying Language Models’ Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting"></a>Quantifying Language Models’ Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11324">http://arxiv.org/abs/2310.11324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr</li>
<li>for: 本研究旨在准确评估现代预训练语言模型（LLM）的性能，并确定prompt设计的重要性。</li>
<li>methods: 本研究使用了多种现有的开源LLM，并研究了它们对提示格式的敏感性。我们提出了FormatSpread算法，可以快速评估任务中的多个可能的提示格式，并对性能进行interval的预测。</li>
<li>results: 我们发现，使用不同的提示格式可以导致LLM的性能差异非常大，最大差异可达76个准确率点。此外，我们还发现，不同的模型之间的格式敏感性强相关，这提出了评估LLM性能的方法ологи问题。<details>
<summary>Abstract</summary>
As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在语言技术中扮演重要角色，因此精确测量其性能是非常重要的。因为选择提示的设计可以强烈影响模型的行为，因此这个设计过程是使用任何现代预训练的生成语言模型时非常重要。在这个工作中，我们专注于 LLM 对提示格式的敏感性。我们发现了许多常用的开源 LLM 在少量示例设定下表现出敏感性，对 LLLaMA-2-13B 的表现范围为 Up to 76 个精确度点。这些敏感性在提高模型大小、几何示例数量或进行指令调整时仍然存在。我们的分析表明，使用提示方式进行评估 LLM 的方法不应该只报告单一的表现方式，而是应该报告一个范围的表现。此外，我们还证明了不同模型之间的格式表现相互弱相联，这让我们对比模型使用随机选择的固定提示方式的方法存在问题。为了促进系统性的分析，我们提出 FormatSpread，一个算法可以快速评估任务中的可能提示格式，并报告预期的表现范围，不需要访问模型的材料。此外，我们还进行了一系列的分析，以探讨这种敏感性的性质，包括探讨具体的原子变化和内部表现的特点。
</details></li>
</ul>
<hr>
<h2 id="Utilising-a-Large-Language-Model-to-Annotate-Subject-Metadata-A-Case-Study-in-an-Australian-National-Research-Data-Catalogue"><a href="#Utilising-a-Large-Language-Model-to-Annotate-Subject-Metadata-A-Case-Study-in-an-Australian-National-Research-Data-Catalogue" class="headerlink" title="Utilising a Large Language Model to Annotate Subject Metadata: A Case Study in an Australian National Research Data Catalogue"></a>Utilising a Large Language Model to Annotate Subject Metadata: A Case Study in an Australian National Research Data Catalogue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11318">http://arxiv.org/abs/2310.11318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiwei Zhang, Mingfang Wu, Xiuzhen Zhang</li>
<li>for: 本研究提出了一种利用大语言模型（LLM）进行 kost-effective 的主题元数据注释的方法，以便提高数据挖掘和复用性。</li>
<li>methods: 本方法使用 GPT-3.5，并通过自动生成的提示来注释主题元数据。然而，基于Context learning的模型无法学习专业领域规则，导致一些类别的表现较差。</li>
<li>results: 本研究表明，使用 GPT-3.5 进行主题元数据注释可以达到可观的效果，但是基于Context learning的模型无法学习专业领域规则，导致一些类别的表现较差。<details>
<summary>Abstract</summary>
In support of open and reproducible research, there has been a rapidly increasing number of datasets made available for research. As the availability of datasets increases, it becomes more important to have quality metadata for discovering and reusing them. Yet, it is a common issue that datasets often lack quality metadata due to limited resources for data curation. Meanwhile, technologies such as artificial intelligence and large language models (LLMs) are progressing rapidly. Recently, systems based on these technologies, such as ChatGPT, have demonstrated promising capabilities for certain data curation tasks. This paper proposes to leverage LLMs for cost-effective annotation of subject metadata through the LLM-based in-context learning. Our method employs GPT-3.5 with prompts designed for annotating subject metadata, demonstrating promising performance in automatic metadata annotation. However, models based on in-context learning cannot acquire discipline-specific rules, resulting in lower performance in several categories. This limitation arises from the limited contextual information available for subject inference. To the best of our knowledge, we are introducing, for the first time, an in-context learning method that harnesses large language models for automated subject metadata annotation.
</details>
<details>
<summary>摘要</summary>
为支持开放和可重复的研究，现有的数据集数量在不断增加。随着数据集的可用性增加，高质量的元数据成为发现和重用数据的关键。然而，由于数据整理有限的资源，数据集经常缺乏高质量的元数据。在这种情况下，人工智能和大语言模型（LLM）的技术在不断进步。最近，基于这些技术的系统，如ChatGPT，在某些数据整理任务中表现出了承诺的能力。本文提议利用LLM进行cost-effective的主题元数据注释。我们的方法使用GPT-3.5，并通过特制的提示来注释主题元数据，表现出了可观的自动元数据注释性能。然而，基于内容学习的模型无法学习专业领域的规则，导致在某些类别上表现较差。这种局限性来自于内容学习模型所lack的专业领域知识。根据我们所知，我们是第一次在内容学习中使用大语言模型进行自动主题元数据注释。
</details></li>
</ul>
<hr>
<h2 id="Generative-error-correction-for-code-switching-speech-recognition-using-large-language-models"><a href="#Generative-error-correction-for-code-switching-speech-recognition-using-large-language-models" class="headerlink" title="Generative error correction for code-switching speech recognition using large language models"></a>Generative error correction for code-switching speech recognition using large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13013">http://arxiv.org/abs/2310.13013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Hexin Liu, Sabato Marco Siniscalchi, Eng Siong Chng</li>
<li>for: 提高Code-switching自动语音识别（CS-ASR）精度</li>
<li>methods: 利用大型语言模型（LLM）和自动语音识别（ASR）生成的N最佳 гипотезы，并通过学习 гипотезы至转写（H2T）映射来直接预测准确转写。</li>
<li>results: 实验证明，使用这种方法可以显著提高CS-ASR的精度，降低混合错误率（MER）。同时，LLM在H2T学习中表现出了remarkable的数据效率，可能解决CS-ASR在低资源语言中的数据稀缺问题。<details>
<summary>Abstract</summary>
Code-switching (CS) speech refers to the phenomenon of mixing two or more languages within the same sentence. Despite the recent advances in automatic speech recognition (ASR), CS-ASR is still a challenging task ought to the grammatical structure complexity of the phenomenon and the data scarcity of specific training corpus. In this work, we propose to leverage large language models (LLMs) and lists of hypotheses generated by an ASR to address the CS problem. Specifically, we first employ multiple well-trained ASR models for N-best hypotheses generation, with the aim of increasing the diverse and informative elements in the set of hypotheses. Next, we utilize the LLMs to learn the hypotheses-to-transcription (H2T) mapping by adding a trainable low-rank adapter. Such a generative error correction (GER) method directly predicts the accurate transcription according to its expert linguistic knowledge and N-best hypotheses, resulting in a paradigm shift from the traditional language model rescoring or error correction techniques. Experimental evidence demonstrates that GER significantly enhances CS-ASR accuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show remarkable data efficiency for H2T learning, providing a potential solution to the data scarcity problem of CS-ASR in low-resource languages.
</details>
<details>
<summary>摘要</summary>
��enson switching (CS) ���ô��� speech refers to the phenomenon of mixing two or more languages within the same sentence. Despite the recent advances in automatic speech recognition (ASR), CS-ASR is still a challenging task due to the grammatical structure complexity of the phenomenon and the data scarcity of specific training corpus. In this work, we propose to leverage large language models (LLMs) and lists of hypotheses generated by an ASR to address the CS problem. Specifically, we first employ multiple well-trained ASR models for N-best hypotheses generation, with the aim of increasing the diverse and informative elements in the set of hypotheses. Next, we utilize the LLMs to learn the hypotheses-to-transcription (H2T) mapping by adding a trainable low-rank adapter. Such a generative error correction (GER) method directly predicts the accurate transcription according to its expert linguistic knowledge and N-best hypotheses, resulting in a paradigm shift from the traditional language model rescoring or error correction techniques. Experimental evidence demonstrates that GER significantly enhances CS-ASR accuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show remarkable data efficiency for H2T learning, providing a potential solution to the data scarcity problem of CS-ASR in low-resource languages.
</details></li>
</ul>
<hr>
<h2 id="MonoSKD-General-Distillation-Framework-for-Monocular-3D-Object-Detection-via-Spearman-Correlation-Coefficient"><a href="#MonoSKD-General-Distillation-Framework-for-Monocular-3D-Object-Detection-via-Spearman-Correlation-Coefficient" class="headerlink" title="MonoSKD: General Distillation Framework for Monocular 3D Object Detection via Spearman Correlation Coefficient"></a>MonoSKD: General Distillation Framework for Monocular 3D Object Detection via Spearman Correlation Coefficient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11316">http://arxiv.org/abs/2310.11316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/senwang98/monoskd">https://github.com/senwang98/monoskd</a></li>
<li>paper_authors: Sen Wang, Jin Zheng</li>
<li>for: 本文提出了一种基于Spearman相关系数的知识传授框架，用于解决单视图3D物体检测中的困难问题。</li>
<li>methods: 该方法使用Spearman相关系数来学习交叉模式特征之间的相对相关性，并通过选择合适的传授位置和移除 redundancy 来降低 GPU 资源消耗和训练时间。</li>
<li>results: 在KITTI 3D物体检测 benchmark 上进行了广泛的实验，并证明了我们的方法可以在挑战性的情况下达到最佳性能，而且无需额外的计算成本。<details>
<summary>Abstract</summary>
Monocular 3D object detection is an inherently ill-posed problem, as it is challenging to predict accurate 3D localization from a single image. Existing monocular 3D detection knowledge distillation methods usually project the LiDAR onto the image plane and train the teacher network accordingly. Transferring LiDAR-based model knowledge to RGB-based models is more complex, so a general distillation strategy is needed. To alleviate cross-modal prob-lem, we propose MonoSKD, a novel Knowledge Distillation framework for Monocular 3D detection based on Spearman correlation coefficient, to learn the relative correlation between cross-modal features. Considering the large gap between these features, strict alignment of features may mislead the training, so we propose a looser Spearman loss. Furthermore, by selecting appropriate distillation locations and removing redundant modules, our scheme saves more GPU resources and trains faster than existing methods. Extensive experiments are performed to verify the effectiveness of our framework on the challenging KITTI 3D object detection benchmark. Our method achieves state-of-the-art performance until submission with no additional inference computational cost. Our codes are available at https://github.com/Senwang98/MonoSKD
</details>
<details>
<summary>摘要</summary>
单目3D物体检测是一个自然的问题，因为从单一图像中预测 precisel 3D位置是困难的。现有的单目3D检测知识传承方法通常是将LiDAR投射到图像平面上，然后对教师网络进行训练。将LiDAR基础的模型知识转移到RGB基础的模型上是更加复杂的，因此需要一个通用的传承策略。为了解决两 modal 之间的问题，我们提出了MonoSKD，一个基于Spearman相関系数的知识传承框架 для单目3D检测。由于这两种特征之间的差距很大，严格对特征进行对齐可能会导致训练失败，因此我们提出了一个更宽松的Spearman损失。此外，我们选择了适当的传承位置和移除了额外的模组，使我们的方案可以更好地运用GPU资源，训练速度更快。我们进行了广泛的实验，以验证我们的框架在KITTI 3D物体检测标准 benchmark 上的效果。我们的方法可以在提交前的测试中实现state-of-the-art的性能，并且不需要额外的推论计算成本。我们的代码可以在https://github.com/Senwang98/MonoSKD 上找到。
</details></li>
</ul>
<hr>
<h2 id="MiniZero-Comparative-Analysis-of-AlphaZero-and-MuZero-on-Go-Othello-and-Atari-Games"><a href="#MiniZero-Comparative-Analysis-of-AlphaZero-and-MuZero-on-Go-Othello-and-Atari-Games" class="headerlink" title="MiniZero: Comparative Analysis of AlphaZero and MuZero on Go, Othello, and Atari Games"></a>MiniZero: Comparative Analysis of AlphaZero and MuZero on Go, Othello, and Atari Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11305">http://arxiv.org/abs/2310.11305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rlglab/minizero">https://github.com/rlglab/minizero</a></li>
<li>paper_authors: Ti-Rong Wu, Hung Guei, Po-Wei Huang, Pei-Chiun Peng, Ting Han Wei, Chung-Chin Shih, Yun-Jui Tsai</li>
<li>for: 本研究旨在提供一个零知识学习框架，支持四种现今最佳算法，包括AlphaZero、MuZero、Gumbel AlphaZero和Gumbel MuZero。这些算法在许多游戏中表现出了超人般的表现，但是哪一个算法在具体任务上最适合哪一个还未明确。通过MiniZero，我们系统地评估了每个算法在9x9囲棋和8x8奥特洛两个棋盘游戏以及57个Atari游戏中的表现。</li>
<li>methods: 我们使用MiniZero框架进行零知识学习，系统地评估了每个算法在不同的游戏中的表现。我们还引入了一种进步的训练方法，即进步的 simulation，可以将计算资源分配更有效率。</li>
<li>results: 我们的实验结果显示，在两个棋盘游戏中，使用更多的 simulations 通常会导致更高的表现。但是，在不同的游戏中，AlphaZero和MuZero的选择可能会因游戏特性而异。在Atari游戏中，MuZero和Gumbel MuZero都是值得考虑的。进步的 simulation 可以将计算资源分配更有效率，并在两个棋盘游戏中获得了明显的提升。<details>
<summary>Abstract</summary>
This paper presents MiniZero, a zero-knowledge learning framework that supports four state-of-the-art algorithms, including AlphaZero, MuZero, Gumbel AlphaZero, and Gumbel MuZero. While these algorithms have demonstrated super-human performance in many games, it remains unclear which among them is most suitable or efficient for specific tasks. Through MiniZero, we systematically evaluate the performance of each algorithm in two board games, 9x9 Go and 8x8 Othello, as well as 57 Atari games. Our empirical findings are summarized as follows. For two board games, using more simulations generally results in higher performance. However, the choice of AlphaZero and MuZero may differ based on game properties. For Atari games, both MuZero and Gumbel MuZero are worth considering. Since each game has unique characteristics, different algorithms and simulations yield varying results. In addition, we introduce an approach, called progressive simulation, which progressively increases the simulation budget during training to allocate computation more efficiently. Our empirical results demonstrate that progressive simulation achieves significantly superior performance in two board games. By making our framework and trained models publicly available, this paper contributes a benchmark for future research on zero-knowledge learning algorithms, assisting researchers in algorithm selection and comparison against these zero-knowledge learning baselines.
</details>
<details>
<summary>摘要</summary>
For two board games, using more simulations generally leads to higher performance. However, the choice between AlphaZero and MuZero may depend on the properties of the game. For Atari games, both MuZero and Gumbel MuZero are worth considering. As each game has unique characteristics, different algorithms and simulations produce varying results.We also introduce an approach called progressive simulation, which increases the simulation budget during training to allocate computation more efficiently. Our results show that progressive simulation achieves significantly better performance in two board games. By making our framework and trained models publicly available, this paper provides a benchmark for future research on zero-knowledge learning algorithms, assisting researchers in selecting and comparing against these baselines.
</details></li>
</ul>
<hr>
<h2 id="Emulating-Human-Cognitive-Processes-for-Expert-Level-Medical-Question-Answering-with-Large-Language-Models"><a href="#Emulating-Human-Cognitive-Processes-for-Expert-Level-Medical-Question-Answering-with-Large-Language-Models" class="headerlink" title="Emulating Human Cognitive Processes for Expert-Level Medical Question-Answering with Large Language Models"></a>Emulating Human Cognitive Processes for Expert-Level Medical Question-Answering with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11266">http://arxiv.org/abs/2310.11266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khushboo Verma, Marina Moore, Stephanie Wottrich, Karla Robles López, Nishant Aggarwal, Zeel Bhatt, Aagamjit Singh, Bradford Unroe, Salah Basheer, Nitish Sachdeva, Prinka Arora, Harmanjeet Kaur, Tanupreet Kaur, Tevon Hood, Anahi Marquez, Tushar Varshney, Nanfu Deng, Azaan Ramani, Pawanraj Ishwara, Maimoona Saeed, Tatiana López Velarde Peña, Bryan Barksdale, Sushovan Guha, Satwant Kumar</li>
<li>for:  This paper aims to provide a novel framework for clinical problem-solving tools in healthcare, based on a Large Language Model (LLM) that mimics human cognitive processes.</li>
<li>methods:  The framework, called BooksMed, utilizes the GRADE (Grading of Recommendations, Assessment, Development, and Evaluations) framework to effectively quantify evidence strength, and is evaluated using a multispecialty clinical benchmark called ExpertMedQA, which consists of open-ended, expert-level clinical questions validated by a diverse group of medical professionals.</li>
<li>results:  The paper shows that BooksMed outperforms existing state-of-the-art models Med-PaLM 2, Almanac, and ChatGPT in a variety of medical scenarios, demonstrating the effectiveness of the framework in providing reliable and evidence-based responses to clinical inquiries.<details>
<summary>Abstract</summary>
In response to the pressing need for advanced clinical problem-solving tools in healthcare, we introduce BooksMed, a novel framework based on a Large Language Model (LLM). BooksMed uniquely emulates human cognitive processes to deliver evidence-based and reliable responses, utilizing the GRADE (Grading of Recommendations, Assessment, Development, and Evaluations) framework to effectively quantify evidence strength. For clinical decision-making to be appropriately assessed, an evaluation metric that is clinically aligned and validated is required. As a solution, we present ExpertMedQA, a multispecialty clinical benchmark comprised of open-ended, expert-level clinical questions, and validated by a diverse group of medical professionals. By demanding an in-depth understanding and critical appraisal of up-to-date clinical literature, ExpertMedQA rigorously evaluates LLM performance. BooksMed outperforms existing state-of-the-art models Med-PaLM 2, Almanac, and ChatGPT in a variety of medical scenarios. Therefore, a framework that mimics human cognitive stages could be a useful tool for providing reliable and evidence-based responses to clinical inquiries.
</details>
<details>
<summary>摘要</summary>
响应医疗领域的高级临床问题解决工具的急需，我们介绍BooksMed，一种新的框架，基于大型自然语言模型（LLM）。BooksMed模仿人类认知过程，以提供基于证据的可靠回答，使用GRADE（评估、评价、发展和评估）框架来有效地评估证据强度。为了正确评估临床决策，需要一种与临床相关的评价标准，而我们提出ExpertMedQA，一个多学科临床引用库，由多个医疗专业人员组成，并被证明了。通过要求对当前临床文献进行深入理解和批判性评估，ExpertMedQA严格评估LLM性能。BooksMed在多种医疗场景中表现出色，超越了现有的state-of-the-art模型Med-PaLM 2、Almanac和ChatGPT。因此，一个模仿人类认知阶段的框架可能是一种有用的工具，用于提供可靠和基于证据的回答临床问题。
</details></li>
</ul>
<hr>
<h2 id="Revealing-the-Unwritten-Visual-Investigation-of-Beam-Search-Trees-to-Address-Language-Model-Prompting-Challenges"><a href="#Revealing-the-Unwritten-Visual-Investigation-of-Beam-Search-Trees-to-Address-Language-Model-Prompting-Challenges" class="headerlink" title="Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges"></a>Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11252">http://arxiv.org/abs/2310.11252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thilo Spinner, Rebecca Kehlbeck, Rita Sevastjanova, Tobias Stähle, Daniel A. Keim, Oliver Deussen, Andreas Spitz, Mennatallah El-Assady</li>
<li>for: 本研究旨在探讨大语言模型的生成过程中，如何通过提示方法来引导模型输出。</li>
<li>methods: 本研究使用了一种交互式视觉方法，可以帮助分析大语言模型在生成过程中的决策。</li>
<li>results: 研究发现，通过曝光搜索树可以提供有价值的信息，并提供了五种细化分析场景来解决一些关注点。这些结果 validate 现有结果，并提供了新的视角。<details>
<summary>Abstract</summary>
The growing popularity of generative language models has amplified interest in interactive methods to guide model outputs. Prompt refinement is considered one of the most effective means to influence output among these methods. We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges. A comprehensive examination of model outputs, including runner-up candidates and their corresponding probabilities, is needed to address these issues. The beam search tree, the prevalent algorithm to sample model outputs, can inherently supply this information. Consequently, we introduce an interactive visual method for investigating the beam search tree, facilitating analysis of the decisions made by the model during generation. We quantitatively show the value of exposing the beam search tree and present five detailed analysis scenarios addressing the identified challenges. Our methodology validates existing results and offers additional insights.
</details>
<details>
<summary>摘要</summary>
随着生成语言模型的普及，对生成输出的指导方法已经引起了更多的关注。Prompt refinement被认为是影响输出的最有效的方法之一。我们identified several challenges associated with prompting large language models，分为数据特定和模型特定、语言学的和社会语言学的挑战。为了解决这些问题，我们需要进行全面的模型输出的检查，包括 runner-up candidates和其对应的概率。为此，我们引入了一种互动视觉方法，用于调查生成过程中模型做出的决定。我们量化地表明了曝光 beam search tree的价值，并提出了五种细化分析场景，用于解决我们所 identific的挑战。我们的方法证明了现有结果的正确性，同时还提供了更多的洞察。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Model-for-Automatic-Evolving-of-Industrial-Data-Centric-R-D-Cycle"><a href="#Leveraging-Large-Language-Model-for-Automatic-Evolving-of-Industrial-Data-Centric-R-D-Cycle" class="headerlink" title="Leveraging Large Language Model for Automatic Evolving of Industrial Data-Centric R&amp;D Cycle"></a>Leveraging Large Language Model for Automatic Evolving of Industrial Data-Centric R&amp;D Cycle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11249">http://arxiv.org/abs/2310.11249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Yang, Xiao Yang, Weiqing Liu, Jinhui Li, Peng Yu, Zeqi Ye, Jiang Bian</li>
<li>for: 这篇论文旨在探讨大语言模型（LLM）在数据驱动研发中的潜力，以减少人工、计算和时间资源成本。</li>
<li>methods: 该论文利用大语言模型进行数据驱动研发的各个基础元素的研究，包括异ogeneous任务相关数据、多面领域知识和多种计算功能工具。</li>
<li>results: 研究表明，LLM可以快速理解域pecific需求，生成专业意见，使用域specific工具进行实验、解释结果并吸收过去努力的知识来解决新的挑战。示例来自于量化投资研究领域。<details>
<summary>Abstract</summary>
In the wake of relentless digital transformation, data-driven solutions are emerging as powerful tools to address multifarious industrial tasks such as forecasting, anomaly detection, planning, and even complex decision-making. Although data-centric R&D has been pivotal in harnessing these solutions, it often comes with significant costs in terms of human, computational, and time resources. This paper delves into the potential of large language models (LLMs) to expedite the evolution cycle of data-centric R&D. Assessing the foundational elements of data-centric R&D, including heterogeneous task-related data, multi-facet domain knowledge, and diverse computing-functional tools, we explore how well LLMs can understand domain-specific requirements, generate professional ideas, utilize domain-specific tools to conduct experiments, interpret results, and incorporate knowledge from past endeavors to tackle new challenges. We take quantitative investment research as a typical example of industrial data-centric R&D scenario and verified our proposed framework upon our full-stack open-sourced quantitative research platform Qlib and obtained promising results which shed light on our vision of automatic evolving of industrial data-centric R&D cycle.
</details>
<details>
<summary>摘要</summary>
在数字变革不断的背景下，数据驱动的解决方案正在赋予企业多种任务，如预测、异常检测、规划和复杂决策等。虽然数据研发总是数据驱动的关键，但它常常带来人工、计算和时间资源的成本。这篇论文探讨了大语言模型（LLM）在加速数据驱动研发的演化过程中的潜力。我们评估了数据驱动研发的基础元素，包括异常任务相关数据、多元领域知识和多种计算函数工具，以 explored how well LLMs can understand domain-specific requirements, generate professional ideas, utilize domain-specific tools to conduct experiments, interpret results, and incorporate knowledge from past endeavors to tackle new challenges.我们选择了投资研究为典型的工业数据驱动研发场景，并在我们的全栈开源 quantitative research 平台Qlib上验证了我们的提议方案，获得了鼓舞人的结果，这有助于我们实现自动化工业数据驱动研发ecycle的vision。
</details></li>
</ul>
<hr>
<h2 id="Query2Triple-Unified-Query-Encoding-for-Answering-Diverse-Complex-Queries-over-Knowledge-Graphs"><a href="#Query2Triple-Unified-Query-Encoding-for-Answering-Diverse-Complex-Queries-over-Knowledge-Graphs" class="headerlink" title="Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs"></a>Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11246">http://arxiv.org/abs/2310.11246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaooxu/q2t">https://github.com/yaooxu/q2t</a></li>
<li>paper_authors: Yao Xu, Shizhu He, Cunguang Wang, Li Cai, Kang Liu, Jun Zhao</li>
<li>for: 本研究旨在解决知识图(KG)中复杂问题(CQA)的挑战，提出了一种新的方法Query to Triple(Q2T)，用于解决复杂问题。</li>
<li>methods: Q2T方法分为两个阶段：首先，使用神经链预测器在简单 вопро题上预测tail实体，然后使用复杂问题的查询编码器来编码多样化的复杂问题。</li>
<li>results: 实验表明，无需直接模型神经集算器，Q2T方法仍然可以达到多个公共benchmark上的状态作呈现性能。<details>
<summary>Abstract</summary>
Complex Query Answering (CQA) is a challenge task of Knowledge Graph (KG). Due to the incompleteness of KGs, query embedding (QE) methods have been proposed to encode queries and entities into the same embedding space, and treat logical operators as neural set operators to obtain answers. However, these methods train KG embeddings and neural set operators concurrently on both simple (one-hop) and complex (multi-hop and logical) queries, which causes performance degradation on simple queries and low training efficiency. In this paper, we propose Query to Triple (Q2T), a novel approach that decouples the training for simple and complex queries. Q2T divides the training into two stages: (1) Pre-training a neural link predictor on simple queries to predict tail entities based on the head entity and relation. (2) Training a query encoder on complex queries to encode diverse complex queries into a unified triple form that can be efficiently solved by the pretrained neural link predictor. Our proposed Q2T is not only efficient to train, but also modular, thus easily adaptable to various neural link predictors that have been studied well. Extensive experiments demonstrate that, even without explicit modeling for neural set operators, Q2T still achieves state-of-the-art performance on diverse complex queries over three public benchmarks.
</details>
<details>
<summary>摘要</summary>
困难任务复杂问答 (CQA) 是知识图 (KG) 的挑战。由于知识图的不完整性，问题嵌入 (QE) 方法被提议，以将查询和实体编码到同一个嵌入空间中，并将逻辑运算视为神经集合运算来获取答案。然而，这些方法同时在简单 (一元) 和复杂 (多元和逻辑) 查询上培训 KG 嵌入和神经集合运算，导致简单查询的性能下降和培训效率低。在这篇论文中，我们提出了查询到三元 (Q2T)，一种新的方法，它将培训分解成两个阶段：1. 预训练神经链预测器在简单查询上预测尾实体基于头实体和关系。2. 培训查询编码器在复杂查询上编码多样化的复杂查询，以便由预训神经链预测器有效解决。我们提出的 Q2T 不仅具有高效的培训效率，还具有可组合的特点，因此容易适应各种已研究的神经链预测器。我们的实验证明，无需显式模型神经集合运算，Q2T仍然在多种多样的复杂查询上达到了状态艺术级别的表现。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Class-incremental-Learning-in-the-Era-of-Large-Pre-trained-Models-via-Test-Time-Adaptation"><a href="#Rethinking-Class-incremental-Learning-in-the-Era-of-Large-Pre-trained-Models-via-Test-Time-Adaptation" class="headerlink" title="Rethinking Class-incremental Learning in the Era of Large Pre-trained Models via Test-Time Adaptation"></a>Rethinking Class-incremental Learning in the Era of Large Pre-trained Models via Test-Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11482">http://arxiv.org/abs/2310.11482</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iemprog/ttacil">https://github.com/iemprog/ttacil</a></li>
<li>paper_authors: Imad Eddine Marouf, Subhankar Roy, Enzo Tartaglione, Stéphane Lathuilière</li>
<li>for: 这篇论文的目的是为了解决分类增量学习（CIL）任务中的问题，即在新任务上进行分类时，不要忘记之前学习的信息。</li>
<li>methods: 这篇论文使用了大量预训模型（PTM）的表现，并在每个任务上进行微调 Parameter 来获得最佳性能。然而，重复微调会使PTM的表现变差，导致忘记之前的任务。为了寻找一个平衡点，这篇论文提出了一种新的方法，即在试验阶段进行调整（TTA），而不是在训练阶段进行微调。</li>
<li>results: 这篇论文的结果显示，TTACIL可以在多个 CIL 标准 benchmark 上进行最佳化，并且在不同的数据损害情况下保持稳定性。此外，TTACIL 还可以避免忘记之前的任务，同时对每个任务都获得良好的表现。<details>
<summary>Abstract</summary>
Class-incremental learning (CIL) is a challenging task that involves continually learning to categorize classes into new tasks without forgetting previously learned information. The advent of the large pre-trained models (PTMs) has fast-tracked the progress in CIL due to the highly transferable PTM representations, where tuning a small set of parameters results in state-of-the-art performance when compared with the traditional CIL methods that are trained from scratch. However, repeated fine-tuning on each task destroys the rich representations of the PTMs and further leads to forgetting previous tasks. To strike a balance between the stability and plasticity of PTMs for CIL, we propose a novel perspective of eliminating training on every new task and instead performing test-time adaptation (TTA) directly on the test instances. Concretely, we propose "Test-Time Adaptation for Class-Incremental Learning" (TTACIL) that first fine-tunes Layer Norm parameters of the PTM on each test instance for learning task-specific features, and then resets them back to the base model to preserve stability. As a consequence, TTACIL does not undergo any forgetting, while benefiting each task with the rich PTM features. Additionally, by design, our method is robust to common data corruptions. Our TTACIL outperforms several state-of-the-art CIL methods when evaluated on multiple CIL benchmarks under both clean and corrupted data.
</details>
<details>
<summary>摘要</summary>
通过不断学习新任务的类增量学习（CIL），我们可以让模型在新任务上进行分类。然而，在某些情况下，由于重复地训练每个任务，这会导致模型忘记之前学习的信息。为了找到PTM的稳定和可变性的平衡，我们提出了一种新的思路，即在测试实例上进行测试时适应（TTA）。具体来说，我们提出了一种名为“测试时适应for类增量学习”（TTACIL）的方法，它首先在每个测试实例上使用层 нор的参数进行微调，以学习任务特有的特征，然后将其重置回基本模型，以保持稳定性。因此，TTACIL不会出现忘记现象，同时每个任务都可以benefit于PTM的丰富特征。此外，由设计，我们的方法具有对常见数据损害的Robustness。我们的TTACIL在多个CIL标准测试 benchmark上评估得到了与state-of-the-art CIL方法相比的更好的性能。
</details></li>
</ul>
<hr>
<h2 id="RealBehavior-A-Framework-for-Faithfully-Characterizing-Foundation-Models’-Human-like-Behavior-Mechanisms"><a href="#RealBehavior-A-Framework-for-Faithfully-Characterizing-Foundation-Models’-Human-like-Behavior-Mechanisms" class="headerlink" title="RealBehavior: A Framework for Faithfully Characterizing Foundation Models’ Human-like Behavior Mechanisms"></a>RealBehavior: A Framework for Faithfully Characterizing Foundation Models’ Human-like Behavior Mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11227">http://arxiv.org/abs/2310.11227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enyu Zhou, Rui Zheng, Zhiheng Xi, Songyang Gao, Xiaoran Fan, Zichu Fei, Jingting Ye, Tao Gui, Qi Zhang, Xuanjing Huang</li>
<li>for: 这 paper 是为了研究基于模型的人类行为的方法和效果而写的。</li>
<li>methods: 该 paper 使用了一个名为 RealBehavior 的框架，该框架可以评估模型的人类行为是否准确、可重复、内部一致和普适性。</li>
<li>results: 研究发现，直接使用心理学工具不能准确地描述所有人类行为，而 RealBehavior 框架可以评估模型的行为是否准确、可重复、内部一致和普适性。此外， paper 还讨论了将模型与人类和社会价值相对应的影响，并 argue 为多样化对象的定制来避免创建具有限制特征的模型。<details>
<summary>Abstract</summary>
Reports of human-like behaviors in foundation models are growing, with psychological theories providing enduring tools to investigate these behaviors. However, current research tends to directly apply these human-oriented tools without verifying the faithfulness of their outcomes. In this paper, we introduce a framework, RealBehavior, which is designed to characterize the humanoid behaviors of models faithfully. Beyond simply measuring behaviors, our framework assesses the faithfulness of results based on reproducibility, internal and external consistency, and generalizability. Our findings suggest that a simple application of psychological tools cannot faithfully characterize all human-like behaviors. Moreover, we discuss the impacts of aligning models with human and social values, arguing for the necessity of diversifying alignment objectives to prevent the creation of models with restricted characteristics.
</details>
<details>
<summary>摘要</summary>
研究人类行为模型的报告正在增长，心理理论提供了持续适用的工具来调查这些行为。然而，当前的研究通常直接采用人类 oriented 工具而不是验证其结果的准确性。在本文中，我们介绍了一个框架，即 RealBehavior，用于准确描述模型的人类行为。除了直接测量行为外，我们的框架还评估结果的准确性基于可重复性、内部和外部一致性以及泛化性。我们的发现表明，简单地应用心理工具不能准确描述所有人类行为。此外，我们还讨论了将模型与人类和社会价值Alignment的影响， arguing for the necessity of diversifying alignment objectives to prevent the creation of models with restricted characteristics.
</details></li>
</ul>
<hr>
<h2 id="Contracting-Tsetlin-Machine-with-Absorbing-Automata"><a href="#Contracting-Tsetlin-Machine-with-Absorbing-Automata" class="headerlink" title="Contracting Tsetlin Machine with Absorbing Automata"></a>Contracting Tsetlin Machine with Absorbing Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11481">http://arxiv.org/abs/2310.11481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bimal Bhattarai, Ole-Christoffer Granmo, Lei Jiao, Per-Arne Andersen, Svein Anders Tunheim, Rishad Shafik, Alex Yakovlev</li>
<li>for: 提高逻辑学习的速度和能效性</li>
<li>methods: 使用稀疏TM和吸引TA状态</li>
<li>results: 加速学习，降低能耗<details>
<summary>Abstract</summary>
In this paper, we introduce a sparse Tsetlin Machine (TM) with absorbing Tsetlin Automata (TA) states. In brief, the TA of each clause literal has both an absorbing Exclude- and an absorbing Include state, making the learning scheme absorbing instead of ergodic. When a TA reaches an absorbing state, it will never leave that state again. If the absorbing state is an Exclude state, both the automaton and the literal can be removed from further consideration. The literal will as a result never participates in that clause. If the absorbing state is an Include state, on the other hand, the literal is stored as a permanent part of the clause while the TA is discarded. A novel sparse data structure supports these updates by means of three action lists: Absorbed Include, Include, and Exclude. By updating these lists, the TM gets smaller and smaller as the literals and their TA withdraw. In this manner, the computation accelerates during learning, leading to faster learning and less energy consumption.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种稀疏的Tsetlin机器（TM），它具有吸收型Tsetlin自动机（TA）状态。简而言之，每个clauseLiteral的TA具有两个吸收状态： exclude状态和include状态。当TA达到一个吸收状态时，它将不再离开该状态。如果吸收状态是exclude状态，那么自动机和Literal都可以从进一步考虑中除去。Literal因此从不会参与到该clause中。如果吸收状态是include状态，那么Literal将被保存为 clause中的永久部分，而TA则被抛弃。一种新的稀疏数据结构支持这些更新，通过三个动作列表：吸收包含、包含和排除。通过更新这些列表，TM会随着Literals和其TA减少，从而使计算加速，导致学习更快速、 consume less energy。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Fairness-Surrogate-Functions-in-Algorithmic-Fairness"><a href="#Understanding-Fairness-Surrogate-Functions-in-Algorithmic-Fairness" class="headerlink" title="Understanding Fairness Surrogate Functions in Algorithmic Fairness"></a>Understanding Fairness Surrogate Functions in Algorithmic Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11211">http://arxiv.org/abs/2310.11211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Yao, Zhanke Zhou, Zhicong Li, Bo Han, Yong Liu</li>
<li>for: 本研究旨在 Mitigating 机器学习算法对certain population groups的偏袋预测，而 achieve comparable accuracy。</li>
<li>methods: 以实现 fairness 定义的 surrogate 函数来解决这个问题。但是，previous work 中的 fairness surrogate function 可能会导致不公平的结果。本研究通过对 demographic parity 的 fairness定义进行 theoretically 和 empirical 分析，发现 surrogate-fairness  gap 的存在，这个 gap 直接决定了 surrogate function 是否适合 fairness definition。</li>
<li>results: 我们提出了一个通用的 sigmoid surrogate 函数，具有严格和可靠的 fairness 保证。此外，我们还提出了一个 novel 的算法 Balanced Surrogate，可以逐步减少 surrogate-fairness gap，以改善公平性。最后，我们在三个真实世界数据集上提供了实践证据，显示我们的方法可以更好地保证公平性。<details>
<summary>Abstract</summary>
It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The "gap" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the "gap" motivate us that the unbounded surrogate functions will be affected by the points far from the decision boundary, which is the large margin points issue investigated in this paper. To address it, we propose the general sigmoid surrogate with a rigorous and reliable fairness guarantee. Interestingly, the theory also provides insights into two important issues that deal with the large margin points as well as obtaining a more balanced dataset are beneficial to fairness. Furthermore, we elaborate a novel and general algorithm called Balanced Surrogate, which iteratively reduces the "gap" to improve fairness. Finally, we provide empirical evidence showing that our methods achieve better fairness performance in three real-world datasets.
</details>
<details>
<summary>摘要</summary>
observations have shown that machine learning algorithms can make biased predictions against certain groups of people. to address this issue, one approach is to use surrogate functions that satisfy certain fairness definitions. however, previous work has shown that these surrogate functions may not always lead to fair results. in this paper, we investigate the reason for this problem and show that there is a gap between the fairness definition and the surrogate function. this gap determines whether the surrogate function is an appropriate substitute for the fairness definition. we also show that the large margin points issue, which is the points far from the decision boundary, affects the unbounded surrogate functions. to address this issue, we propose a general sigmoid surrogate with a rigorous and reliable fairness guarantee. furthermore, we develop a novel and general algorithm called balanced surrogate, which iteratively reduces the gap to improve fairness. finally, we provide empirical evidence showing that our methods achieve better fairness performance in three real-world datasets.
</details></li>
</ul>
<hr>
<h2 id="EEG-motor-imagery-decoding-A-framework-for-comparative-analysis-with-channel-attention-mechanisms"><a href="#EEG-motor-imagery-decoding-A-framework-for-comparative-analysis-with-channel-attention-mechanisms" class="headerlink" title="EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms"></a>EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11198">http://arxiv.org/abs/2310.11198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Wimpff, Leonardo Gizzi, Jan Zerfowski, Bin Yang</li>
<li>for: 本研究探究了在大脑-计算机界面（BCI）中应用不同的通道注意力机制以提高电enzephalography（EEG）动作干обраoupe的解码性能。</li>
<li>methods: 本研究使用了不同的通道注意力机制，并将其集成到一个轻量级框架中，以评估它们的影响。我们采用了一个简单、轻量级的基线建模，可以方便地集成不同的注意力机制。</li>
<li>results: 我们的实验表明，使用不同的通道注意力机制可以提高EEG动作干обраoupe的性能，同时保持小的存储容量和低的计算复杂度。我们的框架具有简单性和普适性，可以在多个数据集上进行广泛的实验，以评估不同的注意力机制和基线建模的效果。<details>
<summary>Abstract</summary>
The objective of this study is to investigate the application of various channel attention mechanisms within the domain of brain-computer interface (BCI) for motor imagery decoding. Channel attention mechanisms can be seen as a powerful evolution of spatial filters traditionally used for motor imagery decoding. This study systematically compares such mechanisms by integrating them into a lightweight architecture framework to evaluate their impact. We carefully construct a straightforward and lightweight baseline architecture designed to seamlessly integrate different channel attention mechanisms. This approach is contrary to previous works which only investigate one attention mechanism and usually build a very complex, sometimes nested architecture. Our framework allows us to evaluate and compare the impact of different attention mechanisms under the same circumstances. The easy integration of different channel attention mechanisms as well as the low computational complexity enables us to conduct a wide range of experiments on three datasets to thoroughly assess the effectiveness of the baseline model and the attention mechanisms. Our experiments demonstrate the strength and generalizability of our architecture framework as well as how channel attention mechanisms can improve the performance while maintaining the small memory footprint and low computational complexity of our baseline architecture. Our architecture emphasizes simplicity, offering easy integration of channel attention mechanisms, while maintaining a high degree of generalizability across datasets, making it a versatile and efficient solution for EEG motor imagery decoding within brain-computer interfaces.
</details>
<details>
<summary>摘要</summary>
We construct a simple and lightweight baseline architecture that seamlessly integrates different channel attention mechanisms, unlike previous works that only investigate one mechanism and build complex, sometimes nested architectures. Our framework allows us to evaluate and compare the impact of different attention mechanisms under the same circumstances.The easy integration of different channel attention mechanisms and the low computational complexity enable us to conduct a wide range of experiments on three datasets to thoroughly assess the effectiveness of the baseline model and the attention mechanisms. Our experiments demonstrate the strength and generalizability of our architecture framework and how channel attention mechanisms can improve performance while maintaining a small memory footprint and low computational complexity.Our architecture emphasizes simplicity, allowing for easy integration of channel attention mechanisms while maintaining a high degree of generalizability across datasets, making it a versatile and efficient solution for EEG motor imagery decoding within BCIs.
</details></li>
</ul>
<hr>
<h2 id="Medical-Text-Simplification-Optimizing-for-Readability-with-Unlikelihood-Training-and-Reranked-Beam-Search-Decoding"><a href="#Medical-Text-Simplification-Optimizing-for-Readability-with-Unlikelihood-Training-and-Reranked-Beam-Search-Decoding" class="headerlink" title="Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding"></a>Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11191">http://arxiv.org/abs/2310.11191</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ljyflores/simplification-project">https://github.com/ljyflores/simplification-project</a></li>
<li>paper_authors: Lorenzo Jaime Yu Flores, Heyuan Huang, Kejian Shi, Sophie Chheang, Arman Cohan</li>
<li>for:  bridging the communication gap in the medical field, where technical jargon and complex constructs are commonly used.</li>
<li>methods:  using a new unlikelihood loss and a reranked beam search decoding method to improve the readability of text simplification in the medical domain.</li>
<li>results:  better performance on readability metrics on three datasets, offering promising avenues for improving text simplification in the medical field.<details>
<summary>Abstract</summary>
Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to further improve the readability of text simplification in the medical domain. We propose (1) a new unlikelihood loss that encourages generation of simpler terms and (2) a reranked beam search decoding method that optimizes for simplicity, which achieve better performance on readability metrics on three datasets. This study's findings offer promising avenues for improving text simplification in the medical field.
</details>
<details>
<summary>摘要</summary>
文本简化在医学领域中已经成为人工智能应用的一个日益有用的应用，用于bridging通信差距。然而，医学简化方法有时会导致生成的文本质量和多样性偏低。在这项工作中，我们探索了如何进一步提高医学简化文本的可读性。我们提出了（1）一种新的不可能损失函数，以便生成更简单的词汇，以及（2）一种重新排序搜索解码方法，以便优化简单性，这两种方法在三个数据集上都达到了更好的可读性指标。这项研究的发现提供了改进医学简化文本的可能的道路。
</details></li>
</ul>
<hr>
<h2 id="FocDepthFormer-Transformer-with-LSTM-for-Depth-Estimation-from-Focus"><a href="#FocDepthFormer-Transformer-with-LSTM-for-Depth-Estimation-from-Focus" class="headerlink" title="FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus"></a>FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11178">http://arxiv.org/abs/2310.11178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueyang Kang, Fengze Han, Abdur Fayjie, Dong Gong</li>
<li>for: 这个研究旨在推导从专注扫描库中的深度信息，解决了现有方法受到本地性的限制，并且可以处理任意长的扫描库。</li>
<li>methods: 我们提出了一个基于Transformer的网络，具有自我注意力和LSTM模组，以及一个CNN解oder。自我注意力允许学习更有用的特征，而LSTM模组可以将表现集成到扫描库中。</li>
<li>results: 我们的模型在多个扫描库评量 dataset 上表现出色，较前一代模型出色，并且可以从视觉对应的单眼RGB深度测量数据中获得更好的预备training。<details>
<summary>Abstract</summary>
Depth estimation from focal stacks is a fundamental computer vision problem that aims to infer depth from focus/defocus cues in the image stacks. Most existing methods tackle this problem by applying convolutional neural networks (CNNs) with 2D or 3D convolutions over a set of fixed stack images to learn features across images and stacks. Their performance is restricted due to the local properties of the CNNs, and they are constrained to process a fixed number of stacks consistent in train and inference, limiting the generalization to the arbitrary length of stacks. To handle the above limitations, we develop a novel Transformer-based network, FocDepthFormer, composed mainly of a Transformer with an LSTM module and a CNN decoder. The self-attention in Transformer enables learning more informative features via an implicit non-local cross reference. The LSTM module is learned to integrate the representations across the stack with arbitrary images. To directly capture the low-level features of various degrees of focus/defocus, we propose to use multi-scale convolutional kernels in an early-stage encoder. Benefiting from the design with LSTM, our FocDepthFormer can be pre-trained with abundant monocular RGB depth estimation data for visual pattern capturing, alleviating the demand for the hard-to-collect focal stack data. Extensive experiments on various focal stack benchmark datasets show that our model outperforms the state-of-the-art models on multiple metrics.
</details>
<details>
<summary>摘要</summary>
depth 估计从焦点栈中是计算机视觉的基本问题，它目的是从图像栈中提取焦点信息。大多数现有方法通过应用 convolutional neural networks (CNNs) 的 2D 或 3D 卷积来学习图像栈中的特征。它们的性能受到本地属性的限制，而且只能处理固定长度的栈图像，从而限制了泛化性。为了解决这些限制，我们开发了一种新的 Transformer 基于网络，即 FocDepthFormer，它主要由 Transformer 和 LSTM 模块以及 CNN 解码器组成。自我注意力在 Transformer 中允许学习更有用的特征，而 LSTM 模块可以将栈图像中的表示集成到不同的图像中。为了直接捕捉不同程度的焦点/杂论的低级特征，我们提议使用多尺度的卷积核在早期编码器中。由于 LSTM 的设计，我们的 FocDepthFormer 可以通过大量的monocular RGB 深度估计数据进行预训练，从而减轻硬件难以收集的焦点栈数据的需求。我们在多个焦点栈 benchmark 数据集上进行了广泛的实验，并证明我们的模型在多个指标上超过了当前状态的模型。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Extraction-and-Distillation-from-Large-Scale-Image-Text-Colonoscopy-Records-Leveraging-Large-Language-and-Vision-Models"><a href="#Knowledge-Extraction-and-Distillation-from-Large-Scale-Image-Text-Colonoscopy-Records-Leveraging-Large-Language-and-Vision-Models" class="headerlink" title="Knowledge Extraction and Distillation from Large-Scale Image-Text Colonoscopy Records Leveraging Large Language and Vision Models"></a>Knowledge Extraction and Distillation from Large-Scale Image-Text Colonoscopy Records Leveraging Large Language and Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11173">http://arxiv.org/abs/2310.11173</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuowang26/endoked">https://github.com/shuowang26/endoked</a></li>
<li>paper_authors: Shuo Wang, Yan Zhu, Xiaoyuan Luo, Zhiwei Yang, Yizhe Zhang, Peiyao Fu, Manning Wang, Zhijian Song, Quanlin Li, Pinghong Zhou, Yike Guo</li>
<li>for: 本研究旨在开发一种基于人工智能的检测和分类方法，用于检测肠穿刺图像中的肠癌。</li>
<li>methods: 本研究使用了大语言和视觉模型的最新进展，提出了一种数据挖掘 paradigma，称为EndoKED，用于深度知识提取和蒸馏。EndoKED自动将原始肠穿刺图像记录转换为带有像素级注释的图像集。</li>
<li>results: 在使用多地中的肠穿刺图像记录（约100万张图像）进行验证中，EndoKED显示出了更高的性能，可以更好地训练肠癌检测和分类模型。此外，EndoKED预训练的视觉底层模型可以在数据量少的情况下实现数据效果和泛化，达到专家水平的性能。<details>
<summary>Abstract</summary>
The development of artificial intelligence systems for colonoscopy analysis often necessitates expert-annotated image datasets. However, limitations in dataset size and diversity impede model performance and generalisation. Image-text colonoscopy records from routine clinical practice, comprising millions of images and text reports, serve as a valuable data source, though annotating them is labour-intensive. Here we leverage recent advancements in large language and vision models and propose EndoKED, a data mining paradigm for deep knowledge extraction and distillation. EndoKED automates the transformation of raw colonoscopy records into image datasets with pixel-level annotation. We validate EndoKED using multi-centre datasets of raw colonoscopy records (~1 million images), demonstrating its superior performance in training polyp detection and segmentation models. Furthermore, the EndoKED pre-trained vision backbone enables data-efficient and generalisable learning for optical biopsy, achieving expert-level performance in both retrospective and prospective validation.
</details>
<details>
<summary>摘要</summary>
开发人工智能系统用于护肠镜分析通常需要专家标注的图像集。然而，数据集的大小和多样性的限制会阻碍模型的性能和泛化。医疗实践中的图像报告记录，包括数百万张图像和文本报告，可以作为价值的数据源，但是标注它们是劳动密集的。我们利用最新的自然语言和Computer Vision技术，提出了EndoKED，一种深度知识提取和精炼数据挖掘模式。EndoKED自动将护肠镜记录转换为带有像素级别标注的图像集。我们使用多中心的 raw colonoscopy 记录（约100万张图像）进行验证，并证明EndoKED在训练肿瘤检测和分 segmentation 模型时表现出色。此外，EndoKED 预训练的视觉底层模型可以在数据效率和泛化上达到专家水平的性能，在逆向和前向验证中都达到专家水平。
</details></li>
</ul>
<hr>
<h2 id="MST-GAT-A-Multimodal-Spatial-Temporal-Graph-Attention-Network-for-Time-Series-Anomaly-Detection"><a href="#MST-GAT-A-Multimodal-Spatial-Temporal-Graph-Attention-Network-for-Time-Series-Anomaly-Detection" class="headerlink" title="MST-GAT: A Multimodal Spatial-Temporal Graph Attention Network for Time Series Anomaly Detection"></a>MST-GAT: A Multimodal Spatial-Temporal Graph Attention Network for Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11169">http://arxiv.org/abs/2310.11169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoyue Ding, Shiliang Sun, Jing Zhao</li>
<li>for: 本文旨在提出一种基于多模态时间序列的异常检测方法，以维护工作设备的安全稳定性。</li>
<li>methods: 本方法使用多模态图注意力网络（M-GAT）和时间卷积网络来捕捉多模态时间序列中的空间时间关系。M-GAT使用多头注意力模块和两个关系注意力模块（内模态和间模态注意力）来模型模态关系。</li>
<li>results: 实验结果表明，MST-GAT在四个多模态标准数据集上比州元基elines表现出色，并且可以强化异常检测结果的可读性。<details>
<summary>Abstract</summary>
Multimodal time series (MTS) anomaly detection is crucial for maintaining the safety and stability of working devices (e.g., water treatment system and spacecraft), whose data are characterized by multivariate time series with diverse modalities. Although recent deep learning methods show great potential in anomaly detection, they do not explicitly capture spatial-temporal relationships between univariate time series of different modalities, resulting in more false negatives and false positives. In this paper, we propose a multimodal spatial-temporal graph attention network (MST-GAT) to tackle this problem. MST-GAT first employs a multimodal graph attention network (M-GAT) and a temporal convolution network to capture the spatial-temporal correlation in multimodal time series. Specifically, M-GAT uses a multi-head attention module and two relational attention modules (i.e., intra- and inter-modal attention) to model modal correlations explicitly. Furthermore, MST-GAT optimizes the reconstruction and prediction modules simultaneously. Experimental results on four multimodal benchmarks demonstrate that MST-GAT outperforms the state-of-the-art baselines. Further analysis indicates that MST-GAT strengthens the interpretability of detected anomalies by locating the most anomalous univariate time series.
</details>
<details>
<summary>摘要</summary>
多模态时间序列异常检测（MTS）是维护工作设备（如水处理系统和航天器）的关键，其数据由多个变量时间序列组成，具有多种模式。虽然最新的深度学习方法显示出了异常检测的潜力，但它们并不直接捕捉多modal时间序列之间的空间-时间关系，导致更多的假阳和假阴。在本文中，我们提出了一种多模态空间-时间Graph注意网络（MST-GAT）来解决这个问题。MST-GAT首先使用多modal Graph注意网络（M-GAT）和一个时间卷积网络来捕捉多modal时间序列之间的空间-时间相关性。具体来说，M-GAT使用多头注意模块和两种关系注意模块（即内模态注意和间模态注意）来模型modal相关性。此外，MST-GAT同时优化了重建和预测模块。实验结果表明，MST-GAT比州时的基elines表现出色，并且进一步分析表明，MST-GAT可以强化异常检测结果的解释性，即 locates最异常的单变量时间序列。
</details></li>
</ul>
<hr>
<h2 id="Accurate-prediction-of-international-trade-flows-Leveraging-knowledge-graphs-and-their-embeddings"><a href="#Accurate-prediction-of-international-trade-flows-Leveraging-knowledge-graphs-and-their-embeddings" class="headerlink" title="Accurate prediction of international trade flows: Leveraging knowledge graphs and their embeddings"></a>Accurate prediction of international trade flows: Leveraging knowledge graphs and their embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11161">http://arxiv.org/abs/2310.11161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Rincon-Yanez, Chahinez Ounoughi, Bassem Sellami, Tarmo Kalvet, Marek Tiits, Sabrina Senatore, Sadok Ben Yahia</li>
<li>for: 本研究旨在使用知识图谱（KG）来模型国际贸易，帮助政策制定者、企业和经济学家预测国际贸易趋势。</li>
<li>methods: 本研究使用知识图谱嵌入（KGE）来预测国际贸易链接，并与传统机器学习方法相结合，如决策树和图 neural network。</li>
<li>results: 研究发现，通过使用KGE来预测国际贸易链接可以提高预测精度，同时也可以提供嵌入解释性的知识表示。此外，研究还发现embedding方法对其他智能算法产生了影响。<details>
<summary>Abstract</summary>
Knowledge representation (KR) is vital in designing symbolic notations to represent real-world facts and facilitate automated decision-making tasks. Knowledge graphs (KGs) have emerged so far as a popular form of KR, offering a contextual and human-like representation of knowledge. In international economics, KGs have proven valuable in capturing complex interactions between commodities, companies, and countries. By putting the gravity model, which is a common economic framework, into the process of building KGs, important factors that affect trade relationships can be taken into account, making it possible to predict international trade patterns. This paper proposes an approach that leverages Knowledge Graph embeddings for modeling international trade, focusing on link prediction using embeddings. Thus, valuable insights are offered to policymakers, businesses, and economists, enabling them to anticipate the effects of changes in the international trade system. Moreover, the integration of traditional machine learning methods with KG embeddings, such as decision trees and graph neural networks are also explored. The research findings demonstrate the potential for improving prediction accuracy and provide insights into embedding explainability in knowledge representation. The paper also presents a comprehensive analysis of the influence of embedding methods on other intelligent algorithms.
</details>
<details>
<summary>摘要</summary>
知识表示（KR）是设计符号notation的关键，以便实现自动化决策任务。知识图（KG）已经出现为知识表示的流行形式，提供了人类化和上下文rich的知识表示。在国际经济中，KGs已经证明了捕捉复杂的贸易关系的价值，例如商品、公司和国家之间的互动。通过将 gravitation model，这是经济框架的一种常见，integrated into the process of building KGs，可以考虑到影响贸易关系的重要因素，从而预测国际贸易模式。这篇论文提出了基于知识图嵌入的国际贸易模型，重点是预测链接使用嵌入。因此，可以为政策制定者、企业和经济学家提供有价值的信息，让他们预测贸易系统的变化的影响。此外，本文还探讨了将传统机器学习方法与KG嵌入结合的可能性，例如决策树和图 neural networks。研究发现， combining KG embeddings with traditional machine learning methods can improve prediction accuracy and provide insights into embedding explainability in knowledge representation.  Additionally, the paper presents a comprehensive analysis of the influence of embedding methods on other intelligent algorithms.
</details></li>
</ul>
<hr>
<h2 id="Causal-discovery-using-dynamically-requested-knowledge"><a href="#Causal-discovery-using-dynamically-requested-knowledge" class="headerlink" title="Causal discovery using dynamically requested knowledge"></a>Causal discovery using dynamically requested knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11154">http://arxiv.org/abs/2310.11154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neville K Kitson, Anthony C Constantinou</li>
<li>for: 这 paper 旨在提高 causal Bayesian networks (CBNs) 的结构学习精度，并且研究一种基于机器学习的方法，使得结构学习算法本身可以动态地确定和请求人类知识。</li>
<li>methods: 这 paper 使用了 Tabu 结构学习算法，并将人类知识 integrate 到结构学习中。</li>
<li>results: 研究发现，这种方法可以提高结构学习精度，并且可以更好地使用人类知识。此外，这种方法还可以使结构学习过程更加透明和有效。<details>
<summary>Abstract</summary>
Causal Bayesian Networks (CBNs) are an important tool for reasoning under uncertainty in complex real-world systems. Determining the graphical structure of a CBN remains a key challenge and is undertaken either by eliciting it from humans, using machine learning to learn it from data, or using a combination of these two approaches. In the latter case, human knowledge is generally provided to the algorithm before it starts, but here we investigate a novel approach where the structure learning algorithm itself dynamically identifies and requests knowledge for relationships that the algorithm identifies as uncertain during structure learning. We integrate this approach into the Tabu structure learning algorithm and show that it offers considerable gains in structural accuracy, which are generally larger than those offered by existing approaches for integrating knowledge. We suggest that a variant which requests only arc orientation information may be particularly useful where the practitioner has little preexisting knowledge of the causal relationships. As well as offering improved accuracy, the approach can use human expertise more effectively and contributes to making the structure learning process more transparent.
</details>
<details>
<summary>摘要</summary>
causal Bayesian networks (CBNs) 是实际世界系统中不确定性理解的重要工具。确定CBN的图structural structure是一个关键挑战，通常通过从人类获得、使用机器学习从数据中学习或使用这两种方法进行。在后者情况下，人类知识通常会提供给算法之前，但我们在这里调查了一种新的方法，即结构学习算法本身在学习过程中动态确定和请求关系不确定的知识。我们将这种方法集成到Tabu结构学习算法中，并证明了它可以提供较大的结构准确性，通常比现有的知识集成方法更大。我们建议一种只请求路径方向信息的变体可能特别有用，当实践者具有少量先前知识的 causal 关系时。除了提高准确性外，这种方法可以更有效地使用人类专业知识，使结构学习过程更透明。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-wall-shear-stress-dynamics-from-neural-network-enhanced-fluid-flow-measurements"><a href="#Uncovering-wall-shear-stress-dynamics-from-neural-network-enhanced-fluid-flow-measurements" class="headerlink" title="Uncovering wall-shear stress dynamics from neural-network enhanced fluid flow measurements"></a>Uncovering wall-shear stress dynamics from neural-network enhanced fluid flow measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11147">http://arxiv.org/abs/2310.11147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esther Lagemann, Steven L. Brunton, Christian Lagemann<br>for: 这篇论文是为了提供一种准确预测wall-shear stress的方法，以便在交通、公共设施、能源技术和医疗等领域实现可持续发展、资源保存和碳中和。methods: 本论文使用深度光流估计器，结合物理知识来 derive velocity和wall-shear stress场的空间和时间分辨率。results: 该方法可以准确预测wall-shear stress场，并且在实验数据上表明其物理正确性和有效性。<details>
<summary>Abstract</summary>
Friction drag from a turbulent fluid moving past or inside an object plays a crucial role in domains as diverse as transportation, public utility infrastructure, energy technology, and human health. As a direct measure of the shear-induced friction forces, an accurate prediction of the wall-shear stress can contribute to sustainability, conservation of resources, and carbon neutrality in civil aviation as well as enhanced medical treatment of vascular diseases and cancer. Despite such importance for our modern society, we still lack adequate experimental methods to capture the instantaneous wall-shear stress dynamics. In this contribution, we present a holistic approach that derives velocity and wall-shear stress fields with impressive spatial and temporal resolution from flow measurements using a deep optical flow estimator with physical knowledge. The validity and physical correctness of the derived flow quantities is demonstrated with synthetic and real-world experimental data covering a range of relevant fluid flows.
</details>
<details>
<summary>摘要</summary>
fluid 动力阻力从一个湍流中过或在一个物体表面或内部具有关键作用，在不同领域中发挥着重要作用，包括交通运输、公共基础设施、能源技术和人类健康。 wall-shear stress 是直接测量摩擦力的力量的直接测量方法，可以贡献到可持续发展、资源保存和碳中和性在民用航空领域，以及更好的医疗治疗血管疾病和癌症。  despite  Such importance in modern society, we still lack adequate experimental methods to capture the instantaneous wall-shear stress dynamics. In this contribution, we present a holistic approach that derives velocity and wall-shear stress fields with impressive spatial and temporal resolution from flow measurements using a deep optical flow estimator with physical knowledge. The validity and physical correctness of the derived flow quantities is demonstrated with synthetic and real-world experimental data covering a range of relevant fluid flows.
</details></li>
</ul>
<hr>
<h2 id="Long-form-Simultaneous-Speech-Translation-Thesis-Proposal"><a href="#Long-form-Simultaneous-Speech-Translation-Thesis-Proposal" class="headerlink" title="Long-form Simultaneous Speech Translation: Thesis Proposal"></a>Long-form Simultaneous Speech Translation: Thesis Proposal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11141">http://arxiv.org/abs/2310.11141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Polák</li>
<li>for: 这份论文主要是为了解决同时传输语音翻译问题，特别是在长形设定下（无需先分割语音）。</li>
<li>methods: 这篇论文主要采用了深度学习方法，包括端到端同时语音翻译系统，以及对现有方法的修改和提高。</li>
<li>results: 这篇论文主要描述了现有的同时语音翻译方法的缺点和限制，以及一些可能的解决方案，但没有直接提供实验结果。<details>
<summary>Abstract</summary>
Simultaneous speech translation (SST) aims to provide real-time translation of spoken language, even before the speaker finishes their sentence. Traditionally, SST has been addressed primarily by cascaded systems that decompose the task into subtasks, including speech recognition, segmentation, and machine translation. However, the advent of deep learning has sparked significant interest in end-to-end (E2E) systems. Nevertheless, a major limitation of most approaches to E2E SST reported in the current literature is that they assume that the source speech is pre-segmented into sentences, which is a significant obstacle for practical, real-world applications. This thesis proposal addresses end-to-end simultaneous speech translation, particularly in the long-form setting, i.e., without pre-segmentation. We present a survey of the latest advancements in E2E SST, assess the primary obstacles in SST and its relevance to long-form scenarios, and suggest approaches to tackle these challenges.
</details>
<details>
<summary>摘要</summary>
同时语音翻译（SST）目标是在实时翻译说话人说话，就在说话人完成句子之前。传统上，SST通过顺序系统解决，分解任务为多个子任务，包括语音识别、分 segmentation 和机器翻译。然而，深度学习的出现引发了对 E2E 系统的重要兴趣。然而，大多数文献中的 E2E SST 方法假设源语音已经分 segmentation 成句子，这是实际应用中的重要障碍。本论文提案探讨了无 segmentation 的 E2E SST，特别是长形设置下。我们对最新的 E2E SST 进步进行了检查，评估了 SST 的主要障碍和长形场景的相关性，并建议了解决这些挑战的方法。
</details></li>
</ul>
<hr>
<h2 id="USDC-Unified-Static-and-Dynamic-Compression-for-Visual-Transformer"><a href="#USDC-Unified-Static-and-Dynamic-Compression-for-Visual-Transformer" class="headerlink" title="USDC: Unified Static and Dynamic Compression for Visual Transformer"></a>USDC: Unified Static and Dynamic Compression for Visual Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11117">http://arxiv.org/abs/2310.11117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Yuan, Chao Liao, Jianchao Tan, Peng Yao, Jiyuan Jia, Bin Chen, Chengru Song, Di Zhang</li>
<li>for: 提高Visual Transformer模型的部署效率和可扩展性，解决模型压缩和执行速度之间的矛盾。</li>
<li>methods: 提出了一种结合静态压缩和动态压缩技术的输入适应压缩模型，以及一种子组augmentation技术来解决具有不同批处理大小的训练和推理阶段之间的性能差异。</li>
<li>results: 对多种基eline Visual Transformer模型进行了广泛的实验，证明了我们的方法可以更好地均衡压缩率和模型性能，并且在不同批处理大小下提供了更高的性能稳定性。<details>
<summary>Abstract</summary>
Visual Transformers have achieved great success in almost all vision tasks, such as classification, detection, and so on. However, the model complexity and the inference speed of the visual transformers hinder their deployments in industrial products. Various model compression techniques focus on directly compressing the visual transformers into a smaller one while maintaining the model performance, however, the performance drops dramatically when the compression ratio is large. Furthermore, several dynamic network techniques have also been applied to dynamically compress the visual transformers to obtain input-adaptive efficient sub-structures during the inference stage, which can achieve a better trade-off between the compression ratio and the model performance. The upper bound of memory of dynamic models is not reduced in the practical deployment since the whole original visual transformer model and the additional control gating modules should be loaded onto devices together for inference. To alleviate two disadvantages of two categories of methods, we propose to unify the static compression and dynamic compression techniques jointly to obtain an input-adaptive compressed model, which can further better balance the total compression ratios and the model performances. Moreover, in practical deployment, the batch sizes of the training and inference stage are usually different, which will cause the model inference performance to be worse than the model training performance, which is not touched by all previous dynamic network papers. We propose a sub-group gates augmentation technique to solve this performance drop problem. Extensive experiments demonstrate the superiority of our method on various baseline visual transformers such as DeiT, T2T-ViT, and so on.
</details>
<details>
<summary>摘要</summary>
Visual Transformers 已经在各种视觉任务上获得了很大的成功，如分类、检测等。然而，视觉转换器的模型复杂度和推理速度使得它们在工业产品中的部署受到限制。各种模型压缩技术都是直接压缩视觉转换器到更小的模型，以保持模型性能，但当压缩比率较大时，模型性能会下降很快。此外，一些动态网络技术也已经应用于在推理阶段动态压缩视觉转换器，以获得输入适应型的有效子结构，可以更好地平衡压缩率和模型性能。然而，在实际部署中，动态模型的内存顶部不会减少，因为整个原始视觉转换器模型和额外的控制闭合模块都需要在设备上加载。为了解决这两种方法的缺点，我们提议将静态压缩和动态压缩技术联合使用，以获得输入适应型的压缩模型，可以更好地平衡总压缩率和模型性能。此外，在训练和推理阶段的批处理大小不同，通常会导致模型的推理性能下降，这个问题未经所有动态网络文章讨论。我们提议使用 subgroup gates 技术来解决这个问题。我们的方法在多种基eline visual transformers 上进行了广泛的实验，如 DeiT、T2T-ViT 等。
</details></li>
</ul>
<hr>
<h2 id="H2O-Open-Ecosystem-for-State-of-the-art-Large-Language-Models"><a href="#H2O-Open-Ecosystem-for-State-of-the-art-Large-Language-Models" class="headerlink" title="H2O Open Ecosystem for State-of-the-art Large Language Models"></a>H2O Open Ecosystem for State-of-the-art Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13012">http://arxiv.org/abs/2310.13012</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/h2oai/h2o-llmstudio">https://github.com/h2oai/h2o-llmstudio</a></li>
<li>paper_authors: Arno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Chun Ming Lee, Marcos V. Conde</li>
<li>for: The paper is written to introduce an open-source ecosystem for developing and testing large language models (LLMs) to address the risks posed by closed-source approaches and to make AI development more accessible, efficient, and trustworthy.</li>
<li>methods: The paper presents a complete open-source ecosystem for LLMs, including a family of fine-tuned LLMs of diverse sizes and a framework and no-code GUI called H2O LLM Studio for efficient fine-tuning, evaluation, and deployment of LLMs using state-of-the-art techniques.</li>
<li>results: The paper introduces h2oGPT, a family of fine-tuned LLMs of diverse sizes, and demonstrates the effectiveness of the H2O LLM Studio for efficient fine-tuning, evaluation, and deployment of LLMs. The demo is available at <a target="_blank" rel="noopener" href="https://gpt.h2o.ai/">https://gpt.h2o.ai/</a>.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs of diverse sizes. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are fully open-source. We believe this work helps to boost AI development and make it more accessible, efficient and trustworthy. The demo is available at: https://gpt.h2o.ai/
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）表示了人工智能领域的革命，但也存在许多重要的风险，如偏见、私有、版权和危险的文本存在。为了解决这些问题，我们需要开放、透明和安全的解决方案。我们介绍了一个完整的开源生态系统，用于开发和测试LLMs。该项目的目标是推动开放的代替方案，以opposeclosed-source方法。我们发布了h2oGPT家族，包括多种大小的精度调整LLMs。我们还介绍了H2O LLM Studio框架和无代码GUI，用于高效地调整、评估和部署LLMs，使用最新的状态艺术技术。我们的代码和模型都是完全开源的。我们认为这项工作将帮助提高人工智能的发展，使其更加可 accessible、高效和可靠。示例可以在以下链接中找到：https://gpt.h2o.ai/
</details></li>
</ul>
<hr>
<h2 id="ASP-Automatic-Selection-of-Proxy-dataset-for-efficient-AutoML"><a href="#ASP-Automatic-Selection-of-Proxy-dataset-for-efficient-AutoML" class="headerlink" title="ASP: Automatic Selection of Proxy dataset for efficient AutoML"></a>ASP: Automatic Selection of Proxy dataset for efficient AutoML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11478">http://arxiv.org/abs/2310.11478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Yao, Chao Liao, Jiyuan Jia, Jianchao Tan, Bin Chen, Chengru Song, Di Zhang</li>
<li>for: 这篇论文旨在提出一个自动选择代理数据框架（ASP），以便在每个epoch中 dynamically选择有用的代理数据subset，从而节省训练数据大小和AutoML处理时间。</li>
<li>methods: 这篇论文使用了自动选择代理数据框架（ASP），可以在不同的选择比率下选择有用的代理数据subset，以节省训练数据大小和AutoML处理时间。</li>
<li>results: 实验结果显示，这篇论文使用的ASP方法可以在不同的选择比率下比其他数据选择方法取得更好的结果，并且可以节省2x-20x的AutoML处理时间。<details>
<summary>Abstract</summary>
Deep neural networks have gained great success due to the increasing amounts of data, and diverse effective neural network designs. However, it also brings a heavy computing burden as the amount of training data is proportional to the training time. In addition, a well-behaved model requires repeated trials of different structure designs and hyper-parameters, which may take a large amount of time even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms and neural architecture search (NAS) algorithms. In this paper, we propose an Automatic Selection of Proxy dataset framework (ASP) aimed to dynamically find the informative proxy subsets of training data at each epoch, reducing the training data size as well as saving the AutoML processing time. We verify the effectiveness and generalization of ASP on CIFAR10, CIFAR100, ImageNet16-120, and ImageNet-1k, across various public model benchmarks. The experiment results show that ASP can obtain better results than other data selection methods at all selection ratios. ASP can also enable much more efficient AutoML processing with a speedup of 2x-20x while obtaining better architectures and better hyper-parameters compared to utilizing the entire dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HGCVAE-Integrating-Generative-and-Contrastive-Learning-for-Heterogeneous-Graph-Learning"><a href="#HGCVAE-Integrating-Generative-and-Contrastive-Learning-for-Heterogeneous-Graph-Learning" class="headerlink" title="HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning"></a>HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11102">http://arxiv.org/abs/2310.11102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulan Hu, Zhirui Yang, Sheng Ouyang, Junchen Wan, Fuzheng Zhang, Zhongyuan Wang, Yong Liu</li>
<li>for: 本研究旨在探讨生成自监督学习（SSL）在多类 Graph Learning（HGL）中的应用。</li>
<li>methods: 本文提出了一种新的对比学习变量 Graph Autoencoder（HGCVAE），该模型通过结合对比学习和生成 SSL，解决了传统对比学习方法中复杂的多类性捕捉问题。</li>
<li>results: 对比于多种州rror-of-the-art基elines，HGCVAE达到了Remarkable的结果，证明了其superiority。<details>
<summary>Abstract</summary>
Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard negative samples for contrastive learning, utilizing the power of variational inference. Additionally, we present a dynamic mask strategy to ensure effective and stable learning. Moreover, we propose an enhanced scaled cosine error as the criterion for better attribute reconstruction. As an initial step in combining generative and contrastive SSL, HGCVAE achieves remarkable results compared to various state-of-the-art baselines, confirming its superiority.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译为简化中文。<</SYS>>生成自我超级学习（SSL）在图学习中展示了重要性和吸引了越来越多的关注。在这项研究中，我们想要探讨生成SSL在非同质graph学习（HGL）中的问题。前一些SSL方法 для非同质图把主要依靠于对比学习，因此需要设计复杂的视图来捕捉非同质性。然而，现有的生成SSL方法没有充分利用生成模型来解决HGL中的挑战。在本文中，我们提出了HGCVAE，一种新的对比变量图自动编码器。而不是关注复杂的非同质性，HGCVAE fully harnesses the full potential of generative SSL。HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard negative samples for contrastive learning, utilizing the power of variational inference. Additionally, we present a dynamic mask strategy to ensure effective and stable learning. Moreover, we propose an enhanced scaled cosine error as the criterion for better attribute reconstruction. As an initial step in combining generative and contrastive SSL, HGCVAE achieves remarkable results compared to various state-of-the-art baselines, confirming its superiority.
</details></li>
</ul>
<hr>
<h2 id="MeKB-Rec-Personal-Knowledge-Graph-Learning-for-Cross-Domain-Recommendation"><a href="#MeKB-Rec-Personal-Knowledge-Graph-Learning-for-Cross-Domain-Recommendation" class="headerlink" title="MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain Recommendation"></a>MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11088">http://arxiv.org/abs/2310.11088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Su, Yao Zhou, Zifei Shan, Qian Chen<br>for: 强化推荐 для新用户 (addressing the cold-start problem in modern recommender systems)methods: 使用Personal Knowledge Graph (PKG) 和 Pretrained Language Models (PLMs) 来建立用户会兴趣的域别不受限制的 semantic representationresults: 在多个公共 CDR 数据集上实验，证明了 MeKB-Rec 的新定义比前一代方法更具弹性，实现了 HR@10 和 NDCG@10  метри增加24%–91%，zero-shot 用户在目标领域的行为无需准确数据可以获得 significiant 提升（105%）。在 WeiXin 推荐场景中部署 MeKB-Rec，获得了重要的线上数据提升。MeKB-Rec 现在在实际产品中服务百亿用户。<details>
<summary>Abstract</summary>
It is a long-standing challenge in modern recommender systems to effectively make recommendations for new users, namely the cold-start problem. Cross-Domain Recommendation (CDR) has been proposed to address this challenge, but current ways to represent users' interests across systems are still severely limited. We introduce Personal Knowledge Graph (PKG) as a domain-invariant interest representation, and propose a novel CDR paradigm named MeKB-Rec. We first link users and entities in a knowledge base to construct a PKG of users' interests, named MeKB. Then we learn a semantic representation of MeKB for the cross-domain recommendation. To efficiently utilize limited training data in CDR, MeKB-Rec employs Pretrained Language Models to inject world knowledge into understanding users' interests. Beyond most existing systems, our approach builds a semantic mapping across domains which breaks the requirement for in-domain user behaviors, enabling zero-shot recommendations for new users in a low-resource domain. We experiment MeKB-Rec on well-established public CDR datasets, and demonstrate that the new formulation % is more powerful than previous approaches, achieves a new state-of-the-art that significantly improves HR@10 and NDCG@10 metrics over best previous approaches by 24\%--91\%, with a 105\% improvement for HR@10 of zero-shot users with no behavior in the target domain. We deploy MeKB-Rec in WeiXin recommendation scenarios and achieve significant gains in core online metrics. MeKB-Rec is now serving hundreds of millions of users in real-world products.
</details>
<details>
<summary>摘要</summary>
现代推荐系统中长期面临的挑战是如何有效地为新用户提供推荐，即冷启用户问题。跨领域推荐（CDR）已经被提议以解决这个问题，但现有的用户兴趣表示方式仍然受到严重的限制。我们引入个人知识图（PKG）作为领域不变的兴趣表示方式，并提出了一种基于PKG的CDR paradigma，称之为MeKB-Rec。我们首先将用户和实体在知识库中连接，以构建用户兴趣的PKG，称之为MeKB。然后，我们学习MeKB的semantic表示，以便在跨领域推荐中使用。为了有效利用CDR中的有限训练数据，MeKB-Rec使用预训练语言模型，以尝试在理解用户兴趣时涉及世界知识。与现有系统不同，我们的方法建立了领域之间的semantic映射，使得不需要在目标领域中有具体的用户行为，以实现零容量推荐。我们在well-established的公共CDR数据集上实验MeKB-Rec，并证明了新的表示方式比前一代方法更有力，在HR@10和NDCG@10指标上提高了24%--91%，与最佳前一代方法相比提高105%。我们在微信推荐场景中部署MeKB-Rec，得到了显著的提升。MeKB-Rec现在为百亿用户服务。
</details></li>
</ul>
<hr>
<h2 id="Feature-Pyramid-biLSTM-Using-Smartphone-Sensors-for-Transportation-Mode-Detection"><a href="#Feature-Pyramid-biLSTM-Using-Smartphone-Sensors-for-Transportation-Mode-Detection" class="headerlink" title="Feature Pyramid biLSTM: Using Smartphone Sensors for Transportation Mode Detection"></a>Feature Pyramid biLSTM: Using Smartphone Sensors for Transportation Mode Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11087">http://arxiv.org/abs/2310.11087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinrui Tang, Hao Cheng</li>
<li>for: 本研究的目的是提出一种新的终端方法，以优化减少的感知数据来实现日常旅行中准确的交通方式检测。</li>
<li>methods: 该方法称为Feature Pyramid biLSTM（FPbiLSTM），它利用Feature Pyramid Network（FPN），将浅层充满精度的特征与深层特征强度相互补做，以捕捉不同交通模式的时间运动模式。</li>
<li>results: FPbiLSTM使用仅三个感知器（加速度计、陀螺仪和磁场计）的数据，在2018年的Sussex-Huawei Locomotion（SHL）挑战数据集上达到了95.1%的准确率和94.7%的F1分数，在八种不同的交通模式中进行了可识别。<details>
<summary>Abstract</summary>
The widespread utilization of smartphones has provided extensive availability to Inertial Measurement Units, providing a wide range of sensory data that can be advantageous for the detection of transportation modes. The objective of this study is to propose a novel end-to-end approach to effectively explore a reduced amount of sensory data collected from a smartphone to achieve accurate mode detection in common daily traveling activities. Our approach, called Feature Pyramid biLSTM (FPbiLSTM), is characterized by its ability to reduce the number of sensors required and processing demands, resulting in a more efficient modeling process without sacrificing the quality of the outcomes than the other current models. FPbiLSTM extends an existing CNN biLSTM model with the Feature Pyramid Network, leveraging the advantages of both shallow layer richness and deeper layer feature resilience for capturing temporal moving patterns in various transportation modes. It exhibits an excellent performance by employing the data collected from only three out of seven sensors, i.e. accelerometers, gyroscopes, and magnetometers, in the 2018 Sussex-Huawei Locomotion (SHL) challenge dataset, attaining a noteworthy accuracy of 95.1% and an F1-score of 94.7% in detecting eight different transportation modes.
</details>
<details>
<summary>摘要</summary>
通过智能手机的广泛使用，提供了大量的感知数据，这些数据可以为交通方式检测提供有利条件。本研究的目标是提出一种新的端到端方法，使用少量的感知数据来准确地检测日常旅行中的交通方式。我们提出的方法，即特征峰网络bilstm（FPbiLSTM），通过减少感知器数量和处理需求，实现了更高效的模型化过程，而不 sacrificing 结果质量。FPbiLSTM extend了现有的CNN bilstm模型，利用浅层layer的richness和深层layer的特征鲜柔性，捕捉不同交通方式的时间运动模式。它在使用2018年的 sussex-Huawei Locomotion（SHL）挑战数据集中，达到了95.1%的准确率和94.7%的F1分数，在检测八种不同的交通方式中表现出色。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Few-Shot-Relation-Extraction-via-Pre-Trained-Language-Models"><a href="#In-Context-Few-Shot-Relation-Extraction-via-Pre-Trained-Language-Models" class="headerlink" title="In-Context Few-Shot Relation Extraction via Pre-Trained Language Models"></a>In-Context Few-Shot Relation Extraction via Pre-Trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11085">http://arxiv.org/abs/2310.11085</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oezyurty/replm">https://github.com/oezyurty/replm</a></li>
<li>paper_authors: Yilmazcan Ozyurt, Stefan Feuerriegel, Ce Zhang</li>
<li>for: 实现文本文档中的人类知识结构，扩展现有的语言模型技术。</li>
<li>methods: 提出一个基于预训语言模型的内容几个扩展框架，不需要名称实体输入或文档人类标注。</li>
<li>results: 在 DocRED  dataset 上进行评估，表现与原始标签相似或更好，并可以轻松地更新 для新的关系集。<details>
<summary>Abstract</summary>
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extraction, and demonstrate that our framework achieves state-of-the-art performance. Finally, our framework allows us to identify missing annotations, and we thus show that our framework actually performs much better than the original labels from the development set of DocRED.
</details>
<details>
<summary>摘要</summary>
关系提取目标是从文本文档中提取结构化的人类知识。现有的方法基于自然语言模型很多时候受到两种限制：（1）它们需要输入名称实体，或者自动检测名称实体，这会增加额外的噪音，（2）它们需要文档的人类标注。为了解决这些问题，我们提出了一种新的框架，即在文本上进行受限的少量trainingrelation extractionvia预训练的语言模型。根据我们所知，我们是第一个将关系提取任务重新定义为特定的在文本上进行受限的少量学习 paradigm。这种方法比现有的方法更加灵活，因为它可以轻松地更新 для新的关系集 ohne需要重新训练。我们使用DocRED dataset，这是公共可用的最大文档关系提取 dataset，来评估我们的框架。我们的结果显示，我们的框架可以达到领先的性能。此外，我们的框架可以识别缺失的注释，因此我们实际上可以证明我们的框架实际上比原始的标签更好。
</details></li>
</ul>
<hr>
<h2 id="Multi-omics-Sampling-based-Graph-Transformer-for-Synthetic-Lethality-Prediction"><a href="#Multi-omics-Sampling-based-Graph-Transformer-for-Synthetic-Lethality-Prediction" class="headerlink" title="Multi-omics Sampling-based Graph Transformer for Synthetic Lethality Prediction"></a>Multi-omics Sampling-based Graph Transformer for Synthetic Lethality Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11082">http://arxiv.org/abs/2310.11082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xusheng Zhao, Hao Liu, Qiong Dai, Hao Peng, Xu Bai, Huailiang Peng</li>
<li>for: 这项研究的目的是提出一种新的多型数据预测生物学上的致死性静止（Synthetic Lethality，SL）。</li>
<li>methods: 该研究使用了一种新的多型数据预测SL方法，即使用抽样-基于图 transformer（MSGT-SL）。该方法首先使用了一种 shallow 多视图 GNN 来获取 SL 和多型数据中的本地结构特征。然后，通过输入基因特征来捕捉长距离依赖关系。最后，通过并行随机游走检索多型数据中的基因来模仿结构化地 incorporate 多型数据。</li>
<li>results: 研究发现，使用 MSGT-SL 方法可以在真实世界 SL 任务中获得较高的 empirical 效果，证明了该方法在 SL 预测中的效用性。<details>
<summary>Abstract</summary>
Synthetic lethality (SL) prediction is used to identify if the co-mutation of two genes results in cell death. The prevalent strategy is to abstract SL prediction as an edge classification task on gene nodes within SL data and achieve it through graph neural networks (GNNs). However, GNNs suffer from limitations in their message passing mechanisms, including over-smoothing and over-squashing issues. Moreover, harnessing the information of non-SL gene relationships within large-scale multi-omics data to facilitate SL prediction poses a non-trivial challenge. To tackle these issues, we propose a new multi-omics sampling-based graph transformer for SL prediction (MSGT-SL). Concretely, we introduce a shallow multi-view GNN to acquire local structural patterns from both SL and multi-omics data. Further, we input gene features that encode multi-view information into the standard self-attention to capture long-range dependencies. Notably, starting with batch genes from SL data, we adopt parallel random walk sampling across multiple omics gene graphs encompassing them. Such sampling effectively and modestly incorporates genes from omics in a structure-aware manner before using self-attention. We showcase the effectiveness of MSGT-SL on real-world SL tasks, demonstrating the empirical benefits gained from the graph transformer and multi-omics data.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced text into Simplified Chinese.<</SYS>>人工致死性（SL）预测是用来判断两个基因的共同突变是否导致细胞死亡。现有的策略是将SL预测视为基因节点之间的边分类任务，并使用图神经网络（GNN）来实现。然而，GNN受到消息传递机制的局限性，包括过滤和压缩问题。另外，在大规模多Omics数据中利用非SL基因关系来促进SL预测是一个非常困难的问题。为了解决这些问题，我们提出了一种新的多Omics采样基于图变换器 для SL预测（MSGT-SL）。具体来说，我们引入了一个浅层多视图GNN，以获取SL数据和多Omics数据中的本地结构模式。然后，我们将基因特征，其中包含多视图信息，输入到标准自注意力中，以捕捉长距离依赖关系。另外，我们从SL数据中开始，采样多Omics基因图中包含的批处理基因，以模estamente和多Omics基因图中的结构相关的方式进行采样。这种采样方式可以有效地和modestly地在多Omics基因图中包含基因，并在使用自注意力之前进行结构意识。我们在实际SL任务上展示了MSGT-SL的效果，并证明了图变换器和多Omics数据的实际效果。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Red-Teaming-Gender-Bias-Provocation-and-Mitigation-in-Large-Language-Models"><a href="#Learning-from-Red-Teaming-Gender-Bias-Provocation-and-Mitigation-in-Large-Language-Models" class="headerlink" title="Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models"></a>Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11079">http://arxiv.org/abs/2310.11079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsuan Su, Cheng-Chu Cheng, Hua Farn, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee<br>for: 这研究旨在检测大语言模型（LLM）中的可能性偏见，并提出了一种自动生成测试用例的方法来检测这些偏见。methods: 这种方法使用自动生成的测试用例来检测LLMs中的偏见，并对检测到的偏见进行纠正。results: 实验结果表明，使用该方法可以使LLMs生成更公正的回答。<details>
<summary>Abstract</summary>
Recently, researchers have made considerable improvements in dialogue systems with the progress of large language models (LLMs) such as ChatGPT and GPT-4. These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. The traditional biases investigation methods often rely on human-written test cases. However, these test cases are usually expensive and limited. In this work, we propose a first-of-its-kind method that automatically generates test cases to detect LLMs' potential gender bias. We apply our method to three well-known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a novel method that automatically generates test cases to detect LLMs' potential gender bias. We apply our method to three well-known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.
</details></li>
</ul>
<hr>
<h2 id="Sim-to-Real-Transfer-of-Adaptive-Control-Parameters-for-AUV-Stabilization-under-Current-Disturbance"><a href="#Sim-to-Real-Transfer-of-Adaptive-Control-Parameters-for-AUV-Stabilization-under-Current-Disturbance" class="headerlink" title="Sim-to-Real Transfer of Adaptive Control Parameters for AUV Stabilization under Current Disturbance"></a>Sim-to-Real Transfer of Adaptive Control Parameters for AUV Stabilization under Current Disturbance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11075">http://arxiv.org/abs/2310.11075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Chaffre, Jonathan Wheare, Andrew Lammas, Paulo Santos, Gilles Le Chenadec, Karl Sammut, Benoit Clement</li>
<li>for: 该论文旨在开发一种基于深度学习的自适应控制方法，以帮助自主潜水机器人（AUV）在海洋环境中减少过程变化的影响，并最小化人工干预。</li>
<li>methods: 该方法结合了最大 entropy深度学习框架和经典的模型基于控制架构，以 trains general-purpose神经网络策略。为了应对实际环境中的分布偏移和高样本复杂性问题，该方法还提出了一种Sim-to-Real传输策略，包括生物体引zed经验回放机制、加强版域随机化技术和实际平台上的评估协议。</li>
<li>results: 实验结果显示，该方法可以从不优的模拟模型上学习出高效策略，并在实际 Vehicle上实现了控制性能3倍高于其模型基于非适应控制器的对照试验。<details>
<summary>Abstract</summary>
Learning-based adaptive control methods hold the premise of enabling autonomous agents to reduce the effect of process variations with minimal human intervention. However, its application to autonomous underwater vehicles (AUVs) has so far been restricted due to 1) unknown dynamics under the form of sea current disturbance that we can not model properly nor measure due to limited sensor capability and 2) the nonlinearity of AUVs tasks where the controller response at some operating points must be overly conservative in order to satisfy the specification at other operating points. Deep Reinforcement Learning (DRL) can alleviates these limitations by training general-purpose neural network policies, but applications of DRL algorithms to AUVs have been restricted to simulated environments, due to their inherent high sample complexity and distribution shift problem. This paper presents a novel approach, merging the Maximum Entropy Deep Reinforcement Learning framework with a classic model-based control architecture, to formulate an adaptive controller. Within this framework, we introduce a Sim-to-Real transfer strategy comprising the following components: a bio-inspired experience replay mechanism, an enhanced domain randomisation technique, and an evaluation protocol executed on a physical platform. Our experimental assessments demonstrate that this method effectively learns proficient policies from suboptimal simulated models of the AUV, resulting in control performance 3 times higher when transferred to a real-world vehicle, compared to its model-based nonadaptive but optimal counterpart.
</details>
<details>
<summary>摘要</summary>
学习基于控制方法可以让自主Agent减少过程变化的影响，但是在自主水下潜水器（AUV）上应用尚未得到广泛使用，主要原因是1）不能正确地模型海流干扰的不确定动力学特性，因为感知器的限制不能准确地测量，2）AUV任务的非线性性，控制器在某些操作点上必须采取保守的响应，以满足其他操作点的规范。深度优化学习（DRL）可以解决这些限制，通过训练通用神经网络策略来减少模型不确定性和分布偏移问题。这篇论文提出了一种新的方法，将最大 entropy深度优化学习框架与经典模型基于控制架构结合，形成一个适应控制器。在这个框架中，我们提出了一种Sim-to-Real转移策略，包括以下三个组成部分：生物发现经验回放机制、改进的领域随机化技术和在物理平台上执行的评估协议。我们的实验评估表明，这种方法可以从优化的模拟模型中学习出高效策略，在真实世界潜水器上实现控制性能3倍高于其模型基于非适应控制器的优化对照。
</details></li>
</ul>
<hr>
<h2 id="Robust-MBFD-A-Robust-Deep-Learning-System-for-Motor-Bearing-Faults-Detection-Using-Multiple-Deep-Learning-Training-Strategies-and-A-Novel-Double-Loss-Function"><a href="#Robust-MBFD-A-Robust-Deep-Learning-System-for-Motor-Bearing-Faults-Detection-Using-Multiple-Deep-Learning-Training-Strategies-and-A-Novel-Double-Loss-Function" class="headerlink" title="Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults Detection Using Multiple Deep Learning Training Strategies and A Novel Double Loss Function"></a>Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults Detection Using Multiple Deep Learning Training Strategies and A Novel Double Loss Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11477">http://arxiv.org/abs/2310.11477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khoa Tran, Lam Pham, Hai-Canh Vu</li>
<li>for: 这个论文的目的是对电动机承受器 fault detection (MBFD) 进行全面分析，即基于电动机承受器的振荡来识别faults。</li>
<li>methods: 本论文提出了多种机器学习基于系统 для MBFD 任务，并评估了这些系统的性能。此外，本论文还提出了三种深度学习基于系统，每一种都采用了不同的训练策略：supervised learning、semi-supervised learning和Unsupervised learning。</li>
<li>results: 对多个 benchmark 数据集进行了广泛的实验，包括美国机械失效预防学会 (MFPT)、Case Western Reserve University Bearing Center (CWRU) 和Paderborn University (PU) 的condition monitoring of bearing damage in electromechanical drive systems。实验结果表明，深度学习基于系统比机器学习基于系统更有效果于 MBFD 任务。此外，我们还实现了一种可靠和通用的深度学习基于系统，并在多个 benchmark 数据集上实现了良好的性能，demonstrating its potential for real-life MBFD applications。<details>
<summary>Abstract</summary>
This paper presents a comprehensive analysis of motor bearing fault detection (MBFD), which involves the task of identifying faults in a motor bearing based on its vibration. To this end, we first propose and evaluate various machine learning based systems for the MBFD task. Furthermore, we propose three deep learning based systems for the MBFD task, each of which explores one of the following training strategies: supervised learning, semi-supervised learning, and unsupervised learning. The proposed machine learning based systems and deep learning based systems are evaluated, compared, and then they are used to identify the best model for the MBFD task. We conducted extensive experiments on various benchmark datasets of motor bearing faults, including those from the American Society for Mechanical Failure Prevention Technology (MFPT), Case Western Reserve University Bearing Center (CWRU), and the Condition Monitoring of Bearing Damage in Electromechanical Drive Systems from Paderborn University (PU). The experimental results on different datasets highlight two main contributions of this study. First, we prove that deep learning based systems are more effective than machine learning based systems for the MBFD task. Second, we achieve a robust and general deep learning based system with a novel loss function for the MBFD task on several benchmark datasets, demonstrating its potential for real-life MBFD applications.
</details>
<details>
<summary>摘要</summary>
The authors propose three deep learning-based systems for the MBFD task, each of which explores a different training strategy: supervised learning, semi-supervised learning, and unsupervised learning. The proposed machine learning-based systems and deep learning-based systems are evaluated and compared, and the best model for the MBFD task is identified.The authors conducted extensive experiments on various benchmark datasets of motor bearing faults, including those from the American Society for Mechanical Failure Prevention Technology (MFPT), Case Western Reserve University Bearing Center (CWRU), and the Condition Monitoring of Bearing Damage in Electromechanical Drive Systems from Paderborn University (PU). The experimental results show that deep learning-based systems are more effective than machine learning-based systems for the MBFD task. Additionally, the authors develop a novel loss function for the MBFD task that achieves robust and general performance on several benchmark datasets, demonstrating its potential for real-life MBFD applications.
</details></li>
</ul>
<hr>
<h2 id="Denevil-Towards-Deciphering-and-Navigating-the-Ethical-Values-of-Large-Language-Models-via-Instruction-Learning"><a href="#Denevil-Towards-Deciphering-and-Navigating-the-Ethical-Values-of-Large-Language-Models-via-Instruction-Learning" class="headerlink" title="Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning"></a>Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11053">http://arxiv.org/abs/2310.11053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu</li>
<li>for: This paper aims to explore the ethical values of large language models (LLMs) and develop methods to improve their value compliance.</li>
<li>methods: The paper proposes a novel prompt generation algorithm called DeNEVIL to dynamically elicit the violation of ethics in LLMs, and constructs a high-quality dataset called MoralPrompt to benchmark the intrinsic values of LLMs. The paper also develops an in-context alignment method called VILMO to improve the value compliance of LLM outputs.</li>
<li>results: The paper discovers that most LLMs are essentially misaligned and demonstrates the effectiveness of VILMO in improving the value compliance of LLM outputs. The results provide a promising initial step in studying the ethical values of LLMs and aligning their outputs with human values.Here’s the Simplified Chinese text:</li>
<li>for: 这篇论文旨在探索大语言模型（LLMs）的伦理价值，并开发方法来提高其价值兼容性。</li>
<li>methods: 论文提出了一种新的提示生成算法called DeNEVIL，可以动态激发LLMs中的伦理违反行为，并构建了一个高质量的数据集called MoralPrompt，用于对LLMs的内在价值进行评估。论文还开发了一种名为VIMO的增值环境，用于在LLMs输出中提高价值兼容性。</li>
<li>results: 论文发现大多数LLMs是 Essentially Misaligned，并证明了VIMO的效果性。结果提供了一个有前途的初步探索LLMs的伦理价值，并将其输出与人类价值进行对齐。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In response, we develop VILMO, an in-context alignment method that substantially enhances the value compliance of LLM outputs by learning to generate appropriate value instructions, outperforming existing competitors. Our methods are suitable for black-box and open-source models, offering a promising initial step in studying the ethical values of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经创造出了无 precedent 的突破，但是它们的日益加入到日常生活中可能会提高社会风险，因为它们可能会生成不道德的内容。despite extensive research on specific issues such as bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values using Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In response, we develop VILMO, an in-context alignment method that substantially enhances the value compliance of LLM outputs by learning to generate appropriate value instructions, outperforming existing competitors. Our methods are suitable for black-box and open-source models, offering a promising initial step in studying the ethical values of LLMs.
</details></li>
</ul>
<hr>
<h2 id="Nonet-at-SemEval-2023-Task-6-Methodologies-for-Legal-Evaluation"><a href="#Nonet-at-SemEval-2023-Task-6-Methodologies-for-Legal-Evaluation" class="headerlink" title="Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation"></a>Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11049">http://arxiv.org/abs/2310.11049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shubhamkumarnigam/legaleval23_nonet">https://github.com/shubhamkumarnigam/legaleval23_nonet</a></li>
<li>paper_authors: Shubham Kumar Nigam, Aniket Deroy, Noel Shallum, Ayush Kumar Mishra, Anup Roy, Shubham Kumar Mishra, Arnab Bhattacharya, Saptarshi Ghosh, Kripabandhu Ghosh</li>
<li>for: 这个论文是为了参加SemEval-2023任务6：理解法律文档而写的。</li>
<li>methods: 论文使用了多种实验方法，包括法律命名实体识别（L-NER）、法律预测（LJP）和法律审判说明（CJPE）等三个子任务。</li>
<li>results: 论文在这三个子任务中的result包括数据统计和方法ология，并在领先者排名中获得了竞争性的排名，分别为15$^{th}$, 11$^{th}$, 和1$^{st}$。<details>
<summary>Abstract</summary>
This paper describes our submission to the SemEval-2023 for Task 6 on LegalEval: Understanding Legal Texts. Our submission concentrated on three subtasks: Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment Prediction (LJP) for Task-C1, and Court Judgment Prediction with Explanation (CJPE) for Task-C2. We conducted various experiments on these subtasks and presented the results in detail, including data statistics and methodology. It is worth noting that legal tasks, such as those tackled in this research, have been gaining importance due to the increasing need to automate legal analysis and support. Our team obtained competitive rankings of 15$^{th}$, 11$^{th}$, and 1$^{st}$ in Task-B, Task-C1, and Task-C2, respectively, as reported on the leaderboard.
</details>
<details>
<summary>摘要</summary>
这份论文描述了我们在SemEval-2023中的任务6中对LegalEval：理解法律文本的提交。我们的提交集中了三个子任务：法律名称识别（L-NER）、法律预测（LJP）和法律判决预测与解释（CJPE）。我们进行了各种实验，并在详细的报告中提供了数据统计和方法ология。值得注意的是，如果法律任务，如这些研究所解决的，在过去几年中得到了越来越多的重视，因为自动化法律分析和支持的需求不断增长。我们团队在排名榜上获得了15名、11名和1名的竞争性排名，分别对应于任务B、C1和C2。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Contrastive-Learning-via-Distributionally-Robust-Optimization"><a href="#Understanding-Contrastive-Learning-via-Distributionally-Robust-Optimization" class="headerlink" title="Understanding Contrastive Learning via Distributionally Robust Optimization"></a>Understanding Contrastive Learning via Distributionally Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11048">http://arxiv.org/abs/2310.11048</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junkangwu/ADNCE">https://github.com/junkangwu/ADNCE</a></li>
<li>paper_authors: Junkang Wu, Jiawei Chen, Jiancan Wu, Wentao Shi, Xiang Wang, Xiangnan He</li>
<li>for: 这个研究探讨了对比学习（CL）的内在忍容性，即负样本可能包含相似 semantics（例如标签）。然而，现有的理论无法提供这种现象的解释。本研究填补了这个研究漏洞，通过分析CL通过透视分布 robust optimization（DRO）的角度，获得了以下几个关键发现：</li>
<li>methods: CL实际上对负样本分布进行DRO，因此可以在多种可能的分布下实现Robust性，并且具有对 sampling bias 的忍容性; 温度参数 $\tau$ 不仅是优化器的临时方法，而是 Lagrange 系数，控制负样本分布集的大小;</li>
<li>results: 研究发现CL具有 InfoNCE 作为估计的� strongMutual Information 和一种新的 $\phi$- divergence 基于通信信息的 estimator，并且提出了一种改进的 Adjusted InfoNCE 损失函数（ADNCE），可以 Mitigate CL 的缺点，包括过度保守和敏感到异常值。广泛的实验在图像、句子和图 граhp 等领域 validate 了提议的效果。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/junkangwu/ADNCE%7D">https://github.com/junkangwu/ADNCE}</a> 上获取。<details>
<summary>Abstract</summary>
This study reveals the inherent tolerance of contrastive learning (CL) towards sampling bias, wherein negative samples may encompass similar semantics (\eg labels). However, existing theories fall short in providing explanations for this phenomenon. We bridge this research gap by analyzing CL through the lens of distributionally robust optimization (DRO), yielding several key insights: (1) CL essentially conducts DRO over the negative sampling distribution, thus enabling robust performance across a variety of potential distributions and demonstrating robustness to sampling bias; (2) The design of the temperature $\tau$ is not merely heuristic but acts as a Lagrange Coefficient, regulating the size of the potential distribution set; (3) A theoretical connection is established between DRO and mutual information, thus presenting fresh evidence for ``InfoNCE as an estimate of MI'' and a new estimation approach for $\phi$-divergence-based generalized mutual information. We also identify CL's potential shortcomings, including over-conservatism and sensitivity to outliers, and introduce a novel Adjusted InfoNCE loss (ADNCE) to mitigate these issues. It refines potential distribution, improving performance and accelerating convergence. Extensive experiments on various domains (image, sentence, and graphs) validate the effectiveness of the proposal. The code is available at \url{https://github.com/junkangwu/ADNCE}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>CL essentially performs DRO over the negative sampling distribution, enabling robust performance across various potential distributions and demonstrating resistance to sampling bias.2. The temperature parameter $\tau$ is not just a heuristic, but rather a Lagrange coefficient that regulates the size of the potential distribution set.3. We establish a theoretical connection between DRO and mutual information, providing fresh evidence for the idea that “InfoNCE is an estimate of MI” and presenting a new approach for estimating $\phi$-divergence-based generalized mutual information.However, CL also has some limitations, such as over-conservatism and sensitivity to outliers. To address these issues, we propose a novel Adjusted InfoNCE loss (ADNCE) that refines the potential distribution and improves performance and convergence. Extensive experiments on various domains (images, sentences, and graphs) demonstrate the effectiveness of our proposal. The code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/junkangwu/ADNCE%7D">https://github.com/junkangwu/ADNCE}</a>.</details></li>
</ol>
<hr>
<h2 id="Fast-Graph-Condensation-with-Structure-based-Neural-Tangent-Kernel"><a href="#Fast-Graph-Condensation-with-Structure-based-Neural-Tangent-Kernel" class="headerlink" title="Fast Graph Condensation with Structure-based Neural Tangent Kernel"></a>Fast Graph Condensation with Structure-based Neural Tangent Kernel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11046">http://arxiv.org/abs/2310.11046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Wang, Wenqi Fan, Jiatong Li, Yao Ma, Qing Li</li>
<li>for: 减少大规模图数据的计算成本，以便更好地应用图神经网络（GNNs）。</li>
<li>methods: 将图数据 condensed 为更小的图数据，使用 Kernel Ridge Regression（KRR）任务 instead of GNNs 的迭代训练。</li>
<li>results: 提出了一种基于 Structure-based Neural Tangent Kernel（SNTK）的数据压缩框架（GC-SNTK），可以减少图数据的计算成本，保持高精度预测性能。<details>
<summary>Abstract</summary>
The rapid development of Internet technology has given rise to a vast amount of graph-structured data. Graph Neural Networks (GNNs), as an effective method for various graph mining tasks, incurs substantial computational resource costs when dealing with large-scale graph data. A data-centric manner solution is proposed to condense the large graph dataset into a smaller one without sacrificing the predictive performance of GNNs. However, existing efforts condense graph-structured data through a computational intensive bi-level optimization architecture also suffer from massive computation costs. In this paper, we propose reforming the graph condensation problem as a Kernel Ridge Regression (KRR) task instead of iteratively training GNNs in the inner loop of bi-level optimization. More specifically, We propose a novel dataset condensation framework (GC-SNTK) for graph-structured data, where a Structure-based Neural Tangent Kernel (SNTK) is developed to capture the topology of graph and serves as the kernel function in KRR paradigm. Comprehensive experiments demonstrate the effectiveness of our proposed model in accelerating graph condensation while maintaining high prediction performance.
</details>
<details>
<summary>摘要</summary>
“互联网科技的快速发展导致了大量的树结构数据的生成。树神经网络（GNNs）作为许多树采矿任务的有效方法，对于大规模树数据而言，具有很大的计算资源成本。为了缩小大型树dataset，而不是降低GNNs的预测性能，我们提出了一个数据中心的解决方案。但是，现有的实现方法通过重复地训练GNNs内部的双层优化架构，具有巨大的计算成本。在本文中，我们提出了将树数据缩小问题转化为核心ridge regression（KRR）任务，而不是透过双层优化架构的迭代训练GNNs。更specifically，我们提出了一个新的数据缩小框架（GC-SNTK），其中一个基于结构的神经 tangent kernel（SNTK）被设计来捕捉树的结构，并且作为KRR模式中的kernel函数。实验结果显示，我们的提议的模型能够快速缩小树数据，而且保持高预测性能。”
</details></li>
</ul>
<hr>
<h2 id="Spoofing-Attack-Detection-in-the-Physical-Layer-with-Robustness-to-User-Movement"><a href="#Spoofing-Attack-Detection-in-the-Physical-Layer-with-Robustness-to-User-Movement" class="headerlink" title="Spoofing Attack Detection in the Physical Layer with Robustness to User Movement"></a>Spoofing Attack Detection in the Physical Layer with Robustness to User Movement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11043">http://arxiv.org/abs/2310.11043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Romero, Tien Ngoc Ha, Peter Gerstoft</li>
<li>for: 防止 spoofing 攻击</li>
<li>methods:  combining 深度学习 和 图граhp 检测</li>
<li>results: 可以准确地 отлиenciate spoofing 和 user movement<details>
<summary>Abstract</summary>
In a spoofing attack, an attacker impersonates a legitimate user to access or modify data belonging to the latter. Typical approaches for spoofing detection in the physical layer declare an attack when a change is observed in certain channel features, such as the received signal strength (RSS) measured by spatially distributed receivers. However, since channels change over time, for example due to user movement, such approaches are impractical. To sidestep this limitation, this paper proposes a scheme that combines the decisions of a position-change detector based on a deep neural network to distinguish spoofing from movement. Building upon community detection on graphs, the sequence of received frames is partitioned into subsequences to detect concurrent transmissions from distinct locations. The scheme can be easily deployed in practice since it just involves collecting a small dataset of measurements at a few tens of locations that need not even be computed or recorded. The scheme is evaluated on real data collected for this purpose.
</details>
<details>
<summary>摘要</summary>
在 spoofing 攻击中，攻击者会伪装为合法用户，以访问或修改受影响用户的数据。通常的 spoofing 检测方法在物理层将攻击宣告为当前通道特征发生变化，如接收信号强度（RSS）测量的空间分布式接收器。然而，由于通道随着时间的变化，例如用户移动，这些方法是不实用的。为了绕过这些限制，这篇论文提议一种方案，将 deep neural network 基于位置变化探测器的决策与 Movement 分离开来。基于图 communit 探测，接收的序列被分割成子序列，以检测同时从不同位置发送的同时传输。该方案可以轻松实现，只需要收集一小量的测量数据，并且不需要计算或记录。这篇论文使用实际数据进行评估。
</details></li>
</ul>
<hr>
<h2 id="Radio-Map-Estimation-in-the-Real-World-Empirical-Validation-and-Analysis"><a href="#Radio-Map-Estimation-in-the-Real-World-Empirical-Validation-and-Analysis" class="headerlink" title="Radio Map Estimation in the Real-World: Empirical Validation and Analysis"></a>Radio Map Estimation in the Real-World: Empirical Validation and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11036">http://arxiv.org/abs/2310.11036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raju Shrestha, Tien Ngoc Ha, Pham Q. Viet, Daniel Romero</li>
<li>for: 这篇论文主要针对的是量化广播信号强度或其他广播频率环境中每个点的地理区域。</li>
<li>methods: 这篇论文使用了许多现有的广播地图估计器，并对这些估计器进行了实际验证。</li>
<li>results: 研究发现，使用深度神经网络（DNNs）的复杂估计器可以提供最佳性能，但它们需要大量的训练数据来达到显著优势。一种新的混合估计器可以同时利用这两种估计器的优点，并且可能值得进一步探索。<details>
<summary>Abstract</summary>
Radio maps quantify received signal strength or other magnitudes of the radio frequency environment at every point of a geographical region. These maps play a vital role in a large number of applications such as wireless network planning, spectrum management, and optimization of communication systems. However, empirical validation of the large number of existing radio map estimators is highly limited. To fill this gap, a large data set of measurements has been collected with an autonomous unmanned aerial vehicle (UAV) and a representative subset of these estimators were evaluated on this data. The performance-complexity trade-off and the impact of fast fading are extensively investigated. Although sophisticated estimators based on deep neural networks (DNNs) exhibit the best performance, they are seen to require large volumes of training data to offer a substantial advantage relative to more traditional schemes. A novel algorithm that blends both kinds of estimators is seen to enjoy the benefits of both, thereby suggesting the potential of exploring this research direction further.
</details>
<details>
<summary>摘要</summary>
Radio 地图量化接收信号强度或其他频率环境中每个地理区域点的其他物理量。这些地图在许多应用中发挥重要作用，如无线网络规划、频谱管理和通信系统优化。然而，现有的大量Radio map estimator的实验 validate 是非常有限的。为了填补这一空白，一个大量测量数据集被收集，并对这些 estimator 进行了评估。本研究探讨了性能vs复杂度的贸易和快速抖动的影响。虽然基于深度神经网络（DNNs）的复杂 estimator 表现最佳，但它们需要大量的训练数据来提供substantial 的优势 relative to 传统方案。一种混合 estimator 的新算法被发现，它们享有两种 estimator 的优点，因此更多的研究是可能的。
</details></li>
</ul>
<hr>
<h2 id="Core-Building-Blocks-Next-Gen-Geo-Spatial-GPT-Application"><a href="#Core-Building-Blocks-Next-Gen-Geo-Spatial-GPT-Application" class="headerlink" title="Core Building Blocks: Next Gen Geo Spatial GPT Application"></a>Core Building Blocks: Next Gen Geo Spatial GPT Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11029">http://arxiv.org/abs/2310.11029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashley Fernandez, Swaraj Dube</li>
<li>for: 本研究提出了 MapGPT，一种将自然语言理解和地理数据处理技术相结合的新approach，以增强地理数据理解和生成。</li>
<li>methods: 本研究使用了大型语言模型（LLMs）和地理数据处理技术，并提出了一种将这两者相结合的方法。这种方法利用了地理和文本数据的token化和向量表示，以提高响应位置相关的问题的准确性和Contextual awareness。</li>
<li>results: 研究表明，通过结合LMMs和地理数据处理技术，MapGPT可以提供更加准确和Contextual awareness的响应，并且可以实现地理计算和可视化输出。<details>
<summary>Abstract</summary>
This paper proposes MapGPT which is a novel approach that integrates the capabilities of language models, specifically large language models (LLMs), with spatial data processing techniques. This paper introduces MapGPT, which aims to bridge the gap between natural language understanding and spatial data analysis by highlighting the relevant core building blocks. By combining the strengths of LLMs and geospatial analysis, MapGPT enables more accurate and contextually aware responses to location-based queries. The proposed methodology highlights building LLMs on spatial and textual data, utilizing tokenization and vector representations specific to spatial information. The paper also explores the challenges associated with generating spatial vector representations. Furthermore, the study discusses the potential of computational capabilities within MapGPT, allowing users to perform geospatial computations and obtain visualized outputs. Overall, this research paper presents the building blocks and methodology of MapGPT, highlighting its potential to enhance spatial data understanding and generation in natural language processing applications.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了MapGPT，一种新的方法，它将自然语言理解和空间数据处理技术相结合。这篇论文描述了MapGPT的目标是将自然语言理解和空间数据分析相连接，并通过高亮相关核心组件来强调这个目标。通过结合LLMs和地理空间分析的优势，MapGPT可以提供更加准确和上下文感知的回答。该方法利用了地理空间数据和文本数据的Tokenization和 вектор表示，并解决了生成空间 вектор表示的挑战。此外，该研究还探讨了MapGPT的计算能力，允许用户进行地理计算并获得可视化输出。总之，这篇研究论文介绍了MapGPT的建构和方法，并强调其在自然语言处理应用中增强空间数据理解和生成的潜在能力。
</details></li>
</ul>
<hr>
<h2 id="Compatible-Transformer-for-Irregularly-Sampled-Multivariate-Time-Series"><a href="#Compatible-Transformer-for-Irregularly-Sampled-Multivariate-Time-Series" class="headerlink" title="Compatible Transformer for Irregularly Sampled Multivariate Time Series"></a>Compatible Transformer for Irregularly Sampled Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11022">http://arxiv.org/abs/2310.11022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mediabrain-sjtu/coformer">https://github.com/mediabrain-sjtu/coformer</a></li>
<li>paper_authors: Yuxi Wei, Juntong Peng, Tong He, Chenxin Xu, Jian Zhang, Shirui Pan, Siheng Chen</li>
<li>for: 这篇论文的目的是解决 irregularly sampled multivariate time series 的分析问题，因为现有的方法适用于常规时间序列数据不能直接处理这种不规时间序列数据。</li>
<li>methods: 本文提出了 Compatible Transformer（CoFormer），一个基于 transformer 的Encoder，用于实现每个单元样本的综合时间互动特征学习。CoFormer 视每个样本为唯一的 variate-time 点，并通过 intra-variate&#x2F;inter-variate 专注来学习每个样本的时间&#x2F;互动特征基于 intra-variate&#x2F;inter-variate 邻居。</li>
<li>results: 本文的实验结果显示，对于多个真实世界数据集，CoFormer 对于预测和分类 зада问表现出卓越的成绩，与现有的方法相比，CoFormer 具有优异的表现。<details>
<summary>Abstract</summary>
To analyze multivariate time series, most previous methods assume regular subsampling of time series, where the interval between adjacent measurements and the number of samples remain unchanged. Practically, data collection systems could produce irregularly sampled time series due to sensor failures and interventions. However, existing methods designed for regularly sampled multivariate time series cannot directly handle irregularity owing to misalignment along both temporal and variate dimensions. To fill this gap, we propose Compatible Transformer (CoFormer), a transformer-based encoder to achieve comprehensive temporal-interaction feature learning for each individual sample in irregular multivariate time series. In CoFormer, we view each sample as a unique variate-time point and leverage intra-variate/inter-variate attentions to learn sample-wise temporal/interaction features based on intra-variate/inter-variate neighbors. With CoFormer as the core, we can analyze irregularly sampled multivariate time series for many downstream tasks, including classification and prediction. We conduct extensive experiments on 3 real-world datasets and validate that the proposed CoFormer significantly and consistently outperforms existing methods.
</details>
<details>
<summary>摘要</summary>
多变量时间序列分析方法中，大多数先前方法假设时间序列的尺度保持不变，即间隔时间和样本数均不变。然而，实际上，数据收集系统可能会生成不规则的时间序列，这是因为仪器故障和干预等原因。现有的方法无法直接处理不规则的时间序列，这是因为它们在时间和变量维度上存在偏移。为填补这个空白，我们提出了兼容变换器（CoFormer），一种基于变换器的编码器，用于在不规则的多变量时间序列中实现每个样本独特的时间互动特征学习。在CoFormer中，我们视每个样本为独特的变量-时间点，并通过内变量/外变量注意力来学习每个样本的时间互动特征，基于内变量/外变量的邻居。通过CoFormer作为核心，我们可以分析不规则的多变量时间序列，并进行识别和预测等下游任务。我们在3个真实世界数据集上进行了广泛的实验，并证明了提出的CoFormer在 existed 方法的基础上显著并且一致性地提高了性能。
</details></li>
</ul>
<hr>
<h2 id="From-Identifiable-Causal-Representations-to-Controllable-Counterfactual-Generation-A-Survey-on-Causal-Generative-Modeling"><a href="#From-Identifiable-Causal-Representations-to-Controllable-Counterfactual-Generation-A-Survey-on-Causal-Generative-Modeling" class="headerlink" title="From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling"></a>From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11011">http://arxiv.org/abs/2310.11011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aneesh Komanduri, Xintao Wu, Yongkai Wu, Feng Chen</li>
<li>for: 这篇论文的目的是探讨如何通过结构 causal modeling 改进深度生成模型，以提高其解释性、避免偶极相关性和外部数据描述稳定性。</li>
<li>methods: 论文使用了结构 causal modeling 方法，包括 causal representation learning 和可控Counterfactual生成方法，以帮助改进深度生成模型的性能。</li>
<li>results: 论文的结果表明，通过结构 causal modeling 可以提高深度生成模型的解释性、避免偶极相关性和外部数据描述稳定性，同时提高数据生成的准确性和多样性。<details>
<summary>Abstract</summary>
Deep generative models have shown tremendous success in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, the tendency to induce spurious correlations, and poor out-of-distribution extrapolation. In an effort to remedy such challenges, one can incorporate the theory of causality in deep generative modeling. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interoperability. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual generation methods. We focus on fundamental theory, formulations, drawbacks, datasets, metrics, and applications of causal generative models in fairness, privacy, out-of-distribution generalization, and precision medicine. We also discuss open problems and fruitful research directions for future work in the field.
</details>
<details>
<summary>摘要</summary>
深度生成模型在数据密度估计和数据生成从有限样本中表现出色，但它们存在一些基本缺陷，如无法解释、产生假 correlations 和外部扩展不稳定。为了解决这些挑战，可以在深度生成模型中涵盖 causality 理论。结构 causal model（SCM）描述了数据生成过程，模型了系统中变量之间复杂的 causal 关系和机制。因此，SCM 可以自然地与深度生成模型结合。 causal 模型具有许多有利的性能，如分布shift 稳定性、公平性和可操作性。我们提供了深入检查 causal 生成模型的技术survey，分为 causal representation learning 和可控 counterfactual generation 方法。我们关注基本理论、形式、缺陷、数据集、指标和应用于公平、隐私、外部扩展、精准医学等领域。我们还讨论了未解决的问题和未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Scalable-Graph-Neural-Network-Inference-with-Node-Adaptive-Propagation"><a href="#Accelerating-Scalable-Graph-Neural-Network-Inference-with-Node-Adaptive-Propagation" class="headerlink" title="Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation"></a>Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10998">http://arxiv.org/abs/2310.10998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Gao, Wentao Zhang, Junliang Yu, Yingxia Shao, Quoc Viet Hung Nguyen, Bin Cui, Hongzhi Yin</li>
<li>for: 提高大规模图的图神经网络（GNNs）实时推理性能。</li>
<li>methods: 提出在线卷积框架和两种基于节点特征信息自适应卷积深度的节点适应卷积方法，以避免重复的特征传播。</li>
<li>results: 在公共数据集上实现了比例性和效率的较好的推理加速，特别是在大规模图上，实现了75倍的推理速度提升。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have exhibited exceptional efficacy in a diverse array of applications. However, the sheer size of large-scale graphs presents a significant challenge to real-time inference with GNNs. Although existing Scalable GNNs leverage linear propagation to preprocess the features and accelerate the training and inference procedure, these methods still suffer from scalability issues when making inferences on unseen nodes, as the feature preprocessing requires the graph to be known and fixed. To further accelerate Scalable GNNs inference in this inductive setting, we propose an online propagation framework and two novel node-adaptive propagation methods that can customize the optimal propagation depth for each node based on its topological information and thereby avoid redundant feature propagation. The trade-off between accuracy and latency can be flexibly managed through simple hyper-parameters to accommodate various latency constraints. Moreover, to compensate for the inference accuracy loss caused by the potential early termination of propagation, we further propose Inception Distillation to exploit the multi-scale receptive field information within graphs. The rigorous and comprehensive experimental study on public datasets with varying scales and characteristics demonstrates that the proposed inference acceleration framework outperforms existing state-of-the-art graph inference acceleration methods in terms of accuracy and efficiency. Particularly, the superiority of our approach is notable on datasets with larger scales, yielding a 75x inference speedup on the largest Ogbn-products dataset.
</details>
<details>
<summary>摘要</summary>
GRAPHNeuralNetworks (GNNs) 已经在多种应用中表现出色。然而，大规模图表示一个 significante challenge  для实时推理GNNs。虽然现有的可扩展GNNs使用线性宣传来预处理特征和加速训练和推理过程，但这些方法仍然在对未看过节点的推理中遇到缺乏扩展性的问题，因为特征预处理需要知道和固定的图。为了进一步加速可扩展GNNs的推理在这种推理设定中，我们提议了在线宣传框架和两种新的节点适应性宣传方法。这些方法可以根据每个节点的 topological information 自适应地定制最佳宣传深度，并因此避免了 redundant feature propagation。通过简单的 гиперпараметр来管理准确率和延迟的负担，我们可以适应不同的延迟限制。此外，为了补做因推理早期终止而导致的准确性损失，我们进一步提议了 Inception Distillation，以利用图中的多尺度接收器场信息。我们对公共数据集进行了严格和全面的实验研究，结果显示，我们的推理加速框架在准确率和效率方面都超过了现有的状态码图推理加速方法。特别是在大规模的数据集上，我们的方法可以实现75倍的推理速度增加。
</details></li>
</ul>
<hr>
<h2 id="EXMODD-An-EXplanatory-Multimodal-Open-Domain-Dialogue-dataset"><a href="#EXMODD-An-EXplanatory-Multimodal-Open-Domain-Dialogue-dataset" class="headerlink" title="EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset"></a>EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10967">http://arxiv.org/abs/2310.10967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/poplpr/exmodd">https://github.com/poplpr/exmodd</a></li>
<li>paper_authors: Hang Yin, Pinren Lu, Ziang Li, Bin Sun, Kan Li</li>
<li>for: 提高对对话任务的研究质量，减少数据收集成本</li>
<li>methods: 提出多Modal数据建构框架（MDCF），通过设计合适的提示语来让大规模预训练语言模型生成高质量的内容，同时提供图像和对话的自动解释，提高可读性和可监测性</li>
<li>results: 实验表明，使用MDCF生成的对话数据和图像解释具有正确性和高质量，可以帮助提高对对话任务的研究质量<details>
<summary>Abstract</summary>
The need for high-quality data has been a key issue hindering the research of dialogue tasks. Recent studies try to build datasets through manual, web crawling, and large pre-trained models. However, man-made data is expensive and data collected from the internet often includes generic responses, meaningless statements, and toxic dialogues. Automatic data generation through large models is a cost-effective method, but for open-domain multimodal dialogue tasks, there are still three drawbacks: 1) There is currently no open-source large model that can accept multimodal input; 2) The content generated by the model lacks interpretability; 3) The generated data is usually difficult to quality control and require extensive resource to collect. To alleviate the significant human and resource expenditure in data collection, we propose a Multimodal Data Construction Framework (MDCF). MDCF designs proper prompts to spur the large-scale pre-trained language model to generate well-formed and satisfactory content. Additionally, MDCF also automatically provides explanation for a given image and its corresponding dialogue, which can provide a certain degree of interpretability and facilitate manual follow-up quality inspection. Based on this, we release an Explanatory Multimodal Open-Domain dialogue dataset (EXMODD). Experiments indicate a positive correlation between the model's ability to generate accurate understandings and high-quality responses. Our code and data can be found at https://github.com/poplpr/EXMODD.
</details>
<details>
<summary>摘要</summary>
需求高质量数据一直是对对话任务研究的关键障碍。近期研究通过手动、网络爬虫和大型预训练模型建立数据集。然而，人工生成数据昂贵，网络上收集的数据经常包含无关的回答、意义不明确的声明以及恶意对话。通过大型模型自动生成数据是一种经济的方法，但对开放频道多媒体对话任务还存在三个缺点：1）目前没有开源的大型模型可以接受多媒体输入；2）模型生成的内容缺乏可读性；3）生成的数据困难以质量控制，需要广泛的资源来收集。为了减少数据收集的人工和资源投入，我们提出了多媒体数据建构框架（MDCF）。MDCF设计合适的提示，使大规模预训练语言模型生成高质量和满意的内容。此外，MDCF还自动提供图像和对应对话的解释，可以提供一定的可读性，便于手动跟踪质量检查。根据这，我们发布了解释多媒体开放频道对话数据集（EXMODD）。实验表明，模型能够生成准确理解和高质量回答之间存在正相关关系。我们的代码和数据可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="A-State-Vector-Framework-for-Dataset-Effects"><a href="#A-State-Vector-Framework-for-Dataset-Effects" class="headerlink" title="A State-Vector Framework for Dataset Effects"></a>A State-Vector Framework for Dataset Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10955">http://arxiv.org/abs/2310.10955</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/esmatsahak/emnlp-2023_a-state-vector-framework-for-dataset-effects_repository">https://github.com/esmatsahak/emnlp-2023_a-state-vector-framework-for-dataset-effects_repository</a></li>
<li>paper_authors: Esmat Sahak, Zining Zhu, Frank Rudzicz</li>
<li>for: 这个论文旨在研究深度神经网络（DNN）系统的高质量数据集如何影响其性能，以及这些数据集之间的交互作用如何影响模型的性能。</li>
<li>methods: 该论文提出了一种状态向量框架，用于系统地研究数据集之间的交互作用。该框架使用理想化的探测试结果为基准，将数据集转化为一个维度空间中的状态向量。这种方法可以评估单独和相互作用的数据集效果，并且可以探索模型在不同维度上的表现。</li>
<li>results: 研究发现，一些常用的自然语言理解数据集具有特点性的效果，这些效果集中在一些语言维度上。此外，研究还发现了一些“泄漏”效果：数据集可以影响模型在不同维度上的表现，这些维度可能与计划中的任务无关。该研究为负责任和可靠的模型开发提供了一个系统的理解。<details>
<summary>Abstract</summary>
The impressive success of recent deep neural network (DNN)-based systems is significantly influenced by the high-quality datasets used in training. However, the effects of the datasets, especially how they interact with each other, remain underexplored. We propose a state-vector framework to enable rigorous studies in this direction. This framework uses idealized probing test results as the bases of a vector space. This framework allows us to quantify the effects of both standalone and interacting datasets. We show that the significant effects of some commonly-used language understanding datasets are characteristic and are concentrated on a few linguistic dimensions. Additionally, we observe some ``spill-over'' effects: the datasets could impact the models along dimensions that may seem unrelated to the intended tasks. Our state-vector framework paves the way for a systematic understanding of the dataset effects, a crucial component in responsible and robust model development.
</details>
<details>
<summary>摘要</summary>
“深度神经网络（DNN）系统的卓越成功受到训练 datasets 的高质量影响，但 datasets 之间的交互效果还未得到足够探究。我们提出了一个状态向量框架，以便系统地研究这一方面。这个框架使用理想化的 probing 测试结果作为基准，并允许我们量化单独和交互 datasets 的效果。我们发现一些通用语言理解 datasets 的效果是特征性的，集中在一些语言维度上。此外，我们还发现了一些“倒流”效果： datasets 可以影响模型在不直接相关的任务上的表现。我们的状态向量框架为负责任和可靠模型开发提供了一个系统的理解。”
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Transformer-Architecture-for-Natural-Language-Processing"><a href="#Enhanced-Transformer-Architecture-for-Natural-Language-Processing" class="headerlink" title="Enhanced Transformer Architecture for Natural Language Processing"></a>Enhanced Transformer Architecture for Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10930">http://arxiv.org/abs/2310.10930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Woohyeon Moon, Taeyoung Kim, Bumgeun Park, Dongsoo Har</li>
<li>for: 提高自然语言处理（NLP）领域的模型性能</li>
<li>methods: 提出一种新的 transformer 结构，包括全层正常化、权重连接、位置编码和零干扰自注意力</li>
<li>results: 使用 Multi30k 翻译数据集进行评估，提高了对 original transformer 的202.96% 的 BLEU 分数<details>
<summary>Abstract</summary>
Transformer is a state-of-the-art model in the field of natural language processing (NLP). Current NLP models primarily increase the number of transformers to improve processing performance. However, this technique requires a lot of training resources such as computing capacity. In this paper, a novel structure of Transformer is proposed. It is featured by full layer normalization, weighted residual connection, positional encoding exploiting reinforcement learning, and zero masked self-attention. The proposed Transformer model, which is called Enhanced Transformer, is validated by the bilingual evaluation understudy (BLEU) score obtained with the Multi30k translation dataset. As a result, the Enhanced Transformer achieves 202.96% higher BLEU score as compared to the original transformer with the translation dataset.
</details>
<details>
<summary>摘要</summary>
transformer 是当前自然语言处理（NLP）领域的先进模型。现有的 NLP 模型主要通过增加 transformer 的数量来提高处理性能。然而，这种技术需要大量的训练资源，如计算能力。在这篇论文中，一种新的 transformer 结构被提出，具有全层正常化、权重征值连接、位置编码利用强化学习和零层隐藏自注意力。这种提出的 transformer 模型被称为增强 transformer，在使用 Multi30k 翻译集合时通过对比 Bleu 分数来验证其性能。结果显示，增强 transformer 与原始 transformer 相比，在 Multi30k 翻译集合中的 Bleu 分数提高了202.96%。
</details></li>
</ul>
<hr>
<h2 id="Using-Audio-Data-to-Facilitate-Depression-Risk-Assessment-in-Primary-Health-Care"><a href="#Using-Audio-Data-to-Facilitate-Depression-Risk-Assessment-in-Primary-Health-Care" class="headerlink" title="Using Audio Data to Facilitate Depression Risk Assessment in Primary Health Care"></a>Using Audio Data to Facilitate Depression Risk Assessment in Primary Health Care</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10928">http://arxiv.org/abs/2310.10928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Valen Levinson, Abhay Goyal, Roger Ho Chun Man, Roy Ka-Wei Lee, Koustuv Saha, Nimay Parekh, Frederick L. Altice, Lam Yin Cheung, Munmun De Choudhury, Navin Kumar</li>
<li>for: 该研究旨在使用声音数据预测抑郁风险。</li>
<li>methods: 该研究使用了TPOT自动Machine学习工具选择最佳机器学习算法，并使用K-最近邻准确分类器进行预测。</li>
<li>results: 选择的模型在预测抑郁风险中表现出色（准确率0.98，报告率0.93，F1评分0.96）。这些发现可能导致开发识别抑郁风险的工具，以便通过AI驱动的聊天机器人进行初步检测。<details>
<summary>Abstract</summary>
Telehealth is a valuable tool for primary health care (PHC), where depression is a common condition. PHC is the first point of contact for most people with depression, but about 25% of diagnoses made by PHC physicians are inaccurate. Many other barriers also hinder depression detection and treatment in PHC. Artificial intelligence (AI) may help reduce depression misdiagnosis in PHC and improve overall diagnosis and treatment outcomes. Telehealth consultations often have video issues, such as poor connectivity or dropped calls. Audio-only telehealth is often more practical for lower-income patients who may lack stable internet connections. Thus, our study focused on using audio data to predict depression risk. The objectives were to: 1) Collect audio data from 24 people (12 with depression and 12 without mental health or major health condition diagnoses); 2) Build a machine learning model to predict depression risk. TPOT, an autoML tool, was used to select the best machine learning algorithm, which was the K-nearest neighbors classifier. The selected model had high performance in classifying depression risk (Precision: 0.98, Recall: 0.93, F1-Score: 0.96). These findings may lead to a range of tools to help screen for and treat depression. By developing tools to detect depression risk, patients can be routed to AI-driven chatbots for initial screenings. Partnerships with a range of stakeholders are crucial to implementing these solutions. Moreover, ethical considerations, especially around data privacy and potential biases in AI models, need to be at the forefront of any AI-driven intervention in mental health care.
</details>
<details>
<summary>摘要</summary>
电健康是一种有价值的工具 для基础健康护理（PHC）， где抑郁是一种常见的疾病。PHC是大多数抑郁患者的第一个接触点，但约25%的诊断由PHC医生进行的是不正确的。许多其他的障碍也妨碍了抑郁检测和治疗在PHC中。人工智能（AI）可能可以减少在PHC中的抑郁误诊和提高总诊断和治疗结果。电健康咨询经常会出现视频问题，如互联网络不稳定或掉线问题。对于低收入患者来说，音频电健康更加实用，因为他们可能缺乏稳定的互联网连接。因此，我们的研究专注于使用音频数据预测抑郁风险。研究的目标是：1. 收集24名参与者的音频数据（12名抑郁患者和12名没有心理或重要健康状况诊断的人）。2. 使用自动Machine Learning（autoML）工具TPOT选择最佳Machine Learning算法，选择的是K-最近邻准确分类算法。选择的模型在识别抑郁风险方面表现出色（准确率0.98，感知率0.93，F1分数0.96）。这些发现可能导致一系列用于检测和治疗抑郁的工具的开发。通过开发检测抑郁风险的工具，患者可以被导向由AI驱动的聊天机器人进行初步检查。与多个各类投资者合作是实现这些解决方案的关键。此外，在AI驱动的医疗干预中，优先考虑数据隐私和可能存在的AI模型偏见等伦理考虑。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Software-Tooling-for-Improving-Software-Development"><a href="#Intelligent-Software-Tooling-for-Improving-Software-Development" class="headerlink" title="Intelligent Software Tooling for Improving Software Development"></a>Intelligent Software Tooling for Improving Software Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10921">http://arxiv.org/abs/2310.10921</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Nathan Cooper</li>
<li>for: 本研究旨在利用深度学习技术来提高软件开发过程。</li>
<li>methods: 本研究使用深度学习技术来处理大量的不结构化软件工程文档。</li>
<li>results: 研究发现，通过使用深度学习技术可以提高软件开发过程的效率和质量。<details>
<summary>Abstract</summary>
Software has eaten the world with many of the necessities and quality of life services people use requiring software. Therefore, tools that improve the software development experience can have a significant impact on the world such as generating code and test cases, detecting bugs, question and answering, etc., The success of Deep Learning (DL) over the past decade has shown huge advancements in automation across many domains, including Software Development processes. One of the main reasons behind this success is the availability of large datasets such as open-source code available through GitHub or image datasets of mobile Graphical User Interfaces (GUIs) with RICO and ReDRAW to be trained on. Therefore, the central research question my dissertation explores is: In what ways can the software development process be improved through leveraging DL techniques on the vast amounts of unstructured software engineering artifacts?
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="NuclearQA-A-Human-Made-Benchmark-for-Language-Models-for-the-Nuclear-Domain"><a href="#NuclearQA-A-Human-Made-Benchmark-for-Language-Models-for-the-Nuclear-Domain" class="headerlink" title="NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain"></a>NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10920">http://arxiv.org/abs/2310.10920</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pnnl/expert2">https://github.com/pnnl/expert2</a></li>
<li>paper_authors: Anurag Acharya, Sai Munikoti, Aaron Hellinger, Sara Smith, Sridevi Wagle, Sameera Horawalavithana</li>
<li>for: 本研究的目的是为了评估语言模型在核领域的表现，并提供一个专门为核领域设计的语言模型评价 benchmark。</li>
<li>methods: 本研究使用了一个人工制定的核领域语言模型评价 benchmark，包含100道由专家设计的问题，以测试语言模型的能力。 研究还提出了一种新的评价指标，以取代现有的评价指标，以便更好地评估语言模型的表现。</li>
<li>results: 实验表明，even the best LLMs 在本研究中表现不佳，这表明现有的 LLMs 在核领域具有科学知识的 gap。<details>
<summary>Abstract</summary>
As LLMs have become increasingly popular, they have been used in almost every field. But as the application for LLMs expands from generic fields to narrow, focused science domains, there exists an ever-increasing gap in ways to evaluate their efficacy in those fields. For the benchmarks that do exist, a lot of them focus on questions that don't require proper understanding of the subject in question. In this paper, we present NuclearQA, a human-made benchmark of 100 questions to evaluate language models in the nuclear domain, consisting of a varying collection of questions that have been specifically designed by experts to test the abilities of language models. We detail our approach and show how the mix of several types of questions makes our benchmark uniquely capable of evaluating models in the nuclear domain. We also present our own evaluation metric for assessing LLM's performances due to the limitations of existing ones. Our experiments on state-of-the-art models suggest that even the best LLMs perform less than satisfactorily on our benchmark, demonstrating the scientific knowledge gap of existing LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Emergent-Mixture-of-Experts-Can-Dense-Pre-trained-Transformers-Benefit-from-Emergent-Modular-Structures"><a href="#Emergent-Mixture-of-Experts-Can-Dense-Pre-trained-Transformers-Benefit-from-Emergent-Modular-Structures" class="headerlink" title="Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?"></a>Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10908">http://arxiv.org/abs/2310.10908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiuzh20/emoe">https://github.com/qiuzh20/emoe</a></li>
<li>paper_authors: Zihan Qiu, Zeyu Huang, Jie Fu</li>
<li>for: 这篇论文主要研究如何使用隐式模块化结构来提高神经网络的泛化能力和学习效率。</li>
<li>methods: 作者使用了潜在的模块化结构，即emergent modularity，来改进标准预训练的变换器模型。他们构建了一种名为Emergent Mixture-of-Experts（EMoE）的模块化对手，无需增加参数量可以轻松地在下游调整中进行替换。</li>
<li>results: 广泛的实验（对1785个模型进行了调整）表明，EMoE可以有效地提高预训练模型的在域和离域泛化能力。此外，分析和缺失研究表明，EMoE可以减少负知识传递和对不同配置的稳定性。<details>
<summary>Abstract</summary>
Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \textbf{E}mergent $\textbf{M}$ixture-$\textbf{o}$f-$\textbf{E}$xperts (EMoE). Without introducing additional parameters, EMoE can be seen as the modular counterpart of the original model and can be effortlessly incorporated into downstream tuning. Extensive experiments (we tune 1785 models) on various downstream tasks (vision and language) and models (22M to1.5B) demonstrate that EMoE effectively boosts in-domain and out-of-domain generalization abilities. Further analysis and ablation study suggest that EMoE mitigates negative knowledge transfer and is robust to various configurations. Code is available at \url{https://github.com/qiuzh20/EMoE}
</details>
<details>
<summary>摘要</summary>
使用模块化设计在神经网络中表现出色的特点，包括更好的泛化性、学习效率等。现有的模块化神经网络通常是显式的，即其模块化结构是预先定义的，每个模块需要实现特定的功能。然而，latest works表明，标准预训练变换器中存在隐式的模块结构，称为“ Emergent Modularity”。这些模块结构在初始预训练阶段自然地出现，并且是 Totally Spontaneous。然而，大多数变换器仍然被视为坚实的模块化模型，其中模块性未得到利用。因此，我们提出了以下问题：whether and how dense pre-trained transformers can benefit from emergent modular structures。为了研究这个问题，我们构建了Emergent Mixture-of-Experts (EMoE)。EMoE可以看作原始模型的模块化对手，而且可以无需添加参数进行下游调整。我们对多种下游任务（视觉和语言）和模型（22M到1.5B）进行了广泛的实验，结果表明，EMoE可以有效地提高域内和域外泛化能力。进一步的分析和减少研究表明，EMoE可以 mitigate negative knowledge transfer和是对多种配置的稳定。代码可以在 \url{https://github.com/qiuzh20/EMoE} 上下载。
</details></li>
</ul>
<hr>
<h2 id="Instilling-Inductive-Biases-with-Subnetworks"><a href="#Instilling-Inductive-Biases-with-Subnetworks" class="headerlink" title="Instilling Inductive Biases with Subnetworks"></a>Instilling Inductive Biases with Subnetworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10899">http://arxiv.org/abs/2310.10899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rock-z/instilling-inductiva-bias">https://github.com/rock-z/instilling-inductiva-bias</a></li>
<li>paper_authors: Enyan Zhang, Michael A. Lepori, Ellie Pavlick</li>
<li>for: 这篇论文的目的是探讨一种新的机器学习方法，即子任务抽象（Subtask Induction），可以让模型更好地控制其行为。</li>
<li>methods: 这篇论文使用的方法是基于已经训练过的模型中找出一个功能子网络，并使用这个子网络来塑造模型的启发性。</li>
<li>results: 论文的两个实验表明，使用子任务抽象可以减少模型需要的训练数据量，并且可以成功地塑造模型采用人类化的形态偏好。<details>
<summary>Abstract</summary>
Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to adopt a specific, generalizable solution to a modular arithmetic task. Second, we demonstrate that Subtask Induction successfully induces a human-like shape bias while increasing data efficiency for convolutional and transformer-based image classification models.
</details>
<details>
<summary>摘要</summary>
尽管人工神经网络在各种任务上表现出色，但我们对这些模型实际解决方案的具体知识和控制仍然很少。尝试通过填充模型中的预设偏见（preferences for certain solutions）来理解和控制它们的行为是一个有前途的路径。许多研究已经专注于研究模型的内生预设偏见和通过手动设计architecture或特制的训练程序来实现不同的预设偏见。在这个工作中，我们探索了一种更机制化的方法：子任务抽象。我们的方法可以在已经训练过的模型中找到一个功能子网络，该子网络实现了特定的子任务，然后使用这个子任务来填充模型中的预设偏见。子任务抽象是高效和灵活的，我们通过两个实验证明其效果。首先，我们表明了子任务抽象可以减少模型学习数据量，使模型采用特定、普适的解决方案。其次，我们成功地在基于卷积和变换器的图像分类模型中实现了人类化形态偏见，同时提高了数据效率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/cs.AI_2023_10_17/" data-id="closbroky005p0g88ak6k9sod" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/cs.CL_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T11:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/cs.CL_2023_10_17/">cs.CL - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="BasahaCorpus-An-Expanded-Linguistic-Resource-for-Readability-Assessment-in-Central-Philippine-Languages"><a href="#BasahaCorpus-An-Expanded-Linguistic-Resource-for-Readability-Assessment-in-Central-Philippine-Languages" class="headerlink" title="BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages"></a>BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11584">http://arxiv.org/abs/2310.11584</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imperialite/basahacorpus-hierarchicalcrosslingualara">https://github.com/imperialite/basahacorpus-hierarchicalcrosslingualara</a></li>
<li>paper_authors: Joseph Marvin Imperial, Ekaterina Kochmar</li>
<li>for: This paper is written for the purpose of improving the performance of automatic readability assessment (ARA) models in lower resource languages in the Philippines.</li>
<li>methods: The paper uses a corpus of short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and Rinconada to train ARA models using surface-level, syllable-pattern, and n-gram overlap features. The paper also proposes a new hierarchical cross-lingual modeling approach that takes advantage of a language’s placement in the family tree to increase the amount of available training data.</li>
<li>results: The study yields encouraging results that support previous work showcasing the efficacy of cross-lingual models in low-resource settings, as well as similarities in highly informative linguistic features for mutually intelligible languages.<details>
<summary>Abstract</summary>
Current research on automatic readability assessment (ARA) has focused on improving the performance of models in high-resource languages such as English. In this work, we introduce and release BasahaCorpus as part of an initiative aimed at expanding available corpora and baseline models for readability assessment in lower resource languages in the Philippines. We compiled a corpus of short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and Rinconada -- languages belonging to the Central Philippine family tree subgroup -- to train ARA models using surface-level, syllable-pattern, and n-gram overlap features. We also propose a new hierarchical cross-lingual modeling approach that takes advantage of a language's placement in the family tree to increase the amount of available training data. Our study yields encouraging results that support previous work showcasing the efficacy of cross-lingual models in low-resource settings, as well as similarities in highly informative linguistic features for mutually intelligible languages.
</details>
<details>
<summary>摘要</summary>
当前研究自动可读性评估（ARA）主要集中在高资源语言 such as English 中进行改进模型性能。在这项工作中，我们介绍并发布 BasahaCorpus，是一项旨在扩大可用 corpora 和基线模型 для可读性评估的低资源语言在菲律宾的 iniciativa。我们编译了中央菲律宾语族 subgroup 中的希利加纳、民达、卡拉雅和林康达语言的短篇小说，以用于训练 ARA 模型 surface-level、 syllable-pattern 和 n-gram 重叠特征。我们还提出了一种新的 hierarchical cross-lingual 模型方法，利用语言在语族树中的位置，以增加可用训练数据。我们的研究得到了鼓舞人心的结果，支持先前的研究表明在低资源设置中， crossed-lingual 模型具有可读性评估的效果，以及同属语言之间的高度相似性特征。
</details></li>
</ul>
<hr>
<h2 id="What-is-a-good-question-Task-oriented-asking-with-fact-level-masking"><a href="#What-is-a-good-question-Task-oriented-asking-with-fact-level-masking" class="headerlink" title="What is a good question? Task-oriented asking with fact-level masking"></a>What is a good question? Task-oriented asking with fact-level masking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11571">http://arxiv.org/abs/2310.11571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Toles, Yukun Huang, Zhou Yu, Luis Gravano</li>
<li>for: 这篇论文主要是关于问答推理任务中的协作问题，即如何让机器人能够咨询用户并采集有用的信息。</li>
<li>methods: 该论文提出了一种自然语言任务协作问答（TOA）定义和框架，以及一种基于实际事实遮盖（FLM）的自动生成问答数据集的方法。</li>
<li>results: 实际试验表明，当前的零shot语言模型在完成TOA任务时表现不佳，与人工标注者相比。这些结果表明可以使用FLM数据集和TOA框架来训练和评估更好的TOA模型。<details>
<summary>Abstract</summary>
Asking questions is an important element of real-life collaboration on reasoning tasks like question answering. For example, a legal assistant chatbot may be unable to make accurate recommendations without specific information on the user's circumstances. However, large language models are usually deployed to solve reasoning tasks directly without asking follow-up questions to the user or third parties. We term this problem task-oriented asking (TOA). Zero-shot chat models can perform TOA, but their training is primarily based on next-token prediction rather than whether questions contribute to successful collaboration. To enable the training and evaluation of TOA models, we present a definition and framework for natural language task-oriented asking, the problem of generating questions that result in answers useful for a reasoning task. We also present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particular critical facts. Finally, we generate a TOA dataset from the HotpotQA dataset using FLM and evaluate several zero-shot language models on it. Our experiments show that current zero-shot models struggle to ask questions that retrieve useful information, as compared to human annotators. These results demonstrate an opportunity to use FLM datasets and the TOA framework to train and evaluate better TOA models.
</details>
<details>
<summary>摘要</summary>
实际协作中的问题询问是解决理性任务的重要元素，例如法律助手聊天机器人可能无法提供精确的建议 без specific 用户情况信息。然而，大型语言模型通常会直接解决理性任务，而不是询问使用者或第三方的询问。我们称这问题为任务 Orientated Asking（TOA）。零开始聊天模型可以进行 TOA，但它们的训练主要基于下一个字符预测，而不是 Whether 询问对于成功协作有用。为了实现和评估 TOA 模型的训练，我们提出了自然语言任务 Orientated Asking 的定义和框架，以及 факт阶掩蔽（FLM），将自然语言数据集转换为自主监督的 TOA 数据集，并在这些数据集上评估了多个零开始语言模型。我们的实验结果显示，现有的零开始模型在对于有用信息的询问方面表现不佳，与人工标注师相比。这些结果显示了使用 FLM 数据集和 TOA 框架可以训练和评估更好的 TOA 模型。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Soups-Personalized-Large-Language-Model-Alignment-via-Post-hoc-Parameter-Merging"><a href="#Personalized-Soups-Personalized-Large-Language-Model-Alignment-via-Post-hoc-Parameter-Merging" class="headerlink" title="Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"></a>Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11564">http://arxiv.org/abs/2310.11564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joeljang/rlphf">https://github.com/joeljang/rlphf</a></li>
<li>paper_authors: Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, Prithviraj Ammanabrolu</li>
<li>for: 本研究旨在使用人工反馈来适应大语言模型（LLM）与多个个人偏好的多目标强化学习（MORL）问题。</li>
<li>methods: 本研究使用了分解个人偏好为多个维度的方法，并在分布式环境中独立进行了这些维度的快速训练。在训练后，parameters可以通过合并 Parameter Merging 技术来有效地组合。</li>
<li>results: 相比强大的单个目标基eline，我们的方法可以实现个性化的偏好对齐。我们的实验结果表明，RLPHF可以有效地适应多个个人偏好，并且可以在不同的应用场景中提供个性化的结果。<details>
<summary>Abstract</summary>
While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at https://github.com/joeljang/RLPHF.
</details>
<details>
<summary>摘要</summary>
While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at https://github.com/joeljang/RLPHF.Here's the translation in Simplified Chinese:While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at https://github.com/joeljang/RLPHF.
</details></li>
</ul>
<hr>
<h2 id="Multi-stage-Large-Language-Model-Correction-for-Speech-Recognition"><a href="#Multi-stage-Large-Language-Model-Correction-for-Speech-Recognition" class="headerlink" title="Multi-stage Large Language Model Correction for Speech Recognition"></a>Multi-stage Large Language Model Correction for Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11532">http://arxiv.org/abs/2310.11532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Pu, Thai-Son Nguyen, Sebastian Stüker</li>
<li>for: 提高竞争性语音识别系统的性能</li>
<li>methods: 使用大语言模型（LLM）进行语音识别系统的改进</li>
<li>results: 实验结果显示，提议的方法可以在多个测试频谱上实现10%~20%的相对改进，与一个竞争性ASR系统相比<details>
<summary>Abstract</summary>
In this paper, we investigate the usage of large language models (LLMs) to improve the performance of competitive speech recognition systems. Different from traditional language models that focus on one single data domain, the rise of LLMs brings us the opportunity to push the limit of state-of-the-art ASR performance, and at the same time to achieve higher robustness and generalize effectively across multiple domains. Motivated by this, we propose a novel multi-stage approach to combine traditional language model re-scoring and LLM prompting. Specifically, the proposed method has two stages: the first stage uses a language model to re-score an N-best list of ASR hypotheses and run a confidence check; The second stage uses prompts to a LLM to perform ASR error correction on less confident results from the first stage. Our experimental results demonstrate the effectiveness of the proposed method by showing a 10% ~ 20% relative improvement in WER over a competitive ASR system -- across multiple test domains.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了使用大语言模型（LLM）提高竞争性语音识别系统的性能。与传统语言模型不同，LLMs允许我们在多个数据域之间进行跨领域的学习和泛化，从而提高ASR性能和Robustness。我们提出了一种新的多阶段方法， combining traditional language model re-scoring和LLM prompting。这种方法包括两个阶段：第一阶段使用语言模型对N-best列表的ASR假设进行重新分数和信任检查；第二阶段使用提示来让LLM进行错误纠正。我们的实验结果表明，提案的方法可以提高竞争性ASR系统的WER表现，在多个测试领域中显示10%~20%的相对改善。
</details></li>
</ul>
<hr>
<h2 id="Automatic-News-Summerization"><a href="#Automatic-News-Summerization" class="headerlink" title="Automatic News Summerization"></a>Automatic News Summerization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11520">http://arxiv.org/abs/2310.11520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Kavach Dheer, Arpit Dhankhar</li>
<li>for: 这个研究论文主要是为了比较EXTRACTIVE和ABSTRACTIVE方法在新闻文本摘要方面的表现。</li>
<li>methods: 这个研究使用了CNN-Daily Mail dataset，这个 dataset包含了新闻文章和人工生成的参考摘要。研究使用了ROUGE分数来评估生成的摘要质量。</li>
<li>results: 经过评估后，研究人员选择了最佳性能的模型，并将其集成到了一个web应用程序中，以评估它们在实际应用中的表现和用户体验。<details>
<summary>Abstract</summary>
Natural Language Processing is booming with its applications in the real world, one of which is Text Summarization for large texts including news articles. This research paper provides an extensive comparative evaluation of extractive and abstractive approaches for news text summarization, with an emphasis on the ROUGE score analysis. The study employs the CNN-Daily Mail dataset, which consists of news articles and human-generated reference summaries. The evaluation employs ROUGE scores to assess the efficacy and quality of generated summaries. After Evaluation, we integrate the best-performing models on a web application to assess their real-world capabilities and user experience.
</details>
<details>
<summary>摘要</summary>
自然语言处理技术在现实世界中得到了广泛应用，其中之一是文本概要化，特别是对新闻文章进行概要。本研究论文进行了对抽取和抽象方法的比较评估，强调ROUGE分数分析。研究使用了CNN-Daily Mail dataset，该 dataset包括新闻文章和人工生成的参考概要。评估使用ROUGE分数评估生成的概要质量。经评估后，我们将最佳表现的模型集成到了网站应用程序中，以评估它们在实际应用中的能力和用户体验。
</details></li>
</ul>
<hr>
<h2 id="VeRA-Vector-based-Random-Matrix-Adaptation"><a href="#VeRA-Vector-based-Random-Matrix-Adaptation" class="headerlink" title="VeRA: Vector-based Random Matrix Adaptation"></a>VeRA: Vector-based Random Matrix Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11454">http://arxiv.org/abs/2310.11454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dawid Jan Kopiczko, Tijmen Blankevoort, Yuki Markus Asano</li>
<li>For: 降低大型语言模型训练参数数量，并在多个用户或任务适配模型中进行多个适配。* Methods: 使用单个低级别矩阵和学习小扩展矩阵来减少训练参数数量，同时保持与LoRA相同的性能。* Results: 在GLUE和E2E测试集上实现同LoRA相同的性能，并在 instruciton-following 任务中使用Llama2 7B模型，只需1.4M参数。<details>
<summary>Abstract</summary>
Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which reduces the number of trainable parameters by 10x compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, and show its application in instruction-following with just 1.4M parameters using the Llama2 7B model.
</details>
<details>
<summary>摘要</summary>
低阶 adaptive（LoRA）是一种受欢迎的方法，可以降低训练可变参数数量，但仍然面临着扩大模型或部署多个用户或任务特定 adapted 模型时的严重存储挑战。在这种工作中，我们提出了 вектор基的随机矩阵适应（VeRA），它可以在与 LoRA 相比下减少训练可变参数数量达到 10 倍，同时保持性能不变。它实现这一点通过使用所有层共享的低阶矩阵对和学习小扩张 вектор而做。我们在 GLUE 和 E2E 测试上证明了它的有效性，并在使用 Llama2 7B 模型进行 instrucion-following  tasks 中只需要 1.4M 参数。
</details></li>
</ul>
<hr>
<h2 id="BitNet-Scaling-1-bit-Transformers-for-Large-Language-Models"><a href="#BitNet-Scaling-1-bit-Transformers-for-Large-Language-Models" class="headerlink" title="BitNet: Scaling 1-bit Transformers for Large Language Models"></a>BitNet: Scaling 1-bit Transformers for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11453">http://arxiv.org/abs/2310.11453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/BitNet">https://github.com/kyegomez/BitNet</a></li>
<li>paper_authors: Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei</li>
<li>for: 这个研究是为了开发一个可扩展且稳定的1比特transformer架构，以便在大型语言模型中实现高效性和可持续性。</li>
<li>methods: 这个研究使用了BitLinear层来将1比特量化的 weights 训练出来，并且提出了一个可替换的drop-in替代方案。</li>
<li>results: 实验结果显示，BitNet可以与现有的8比特量化方法和FP16 transformer基准相比，在语言模型化上达到竞争性的表现，同时具有较小的内存库存和能源消耗。此外，BitNet显示了与全精度transformer相似的扩展法则， suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.<details>
<summary>Abstract</summary>
The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.
</details>
<details>
<summary>摘要</summary>
大型语言模型的增加会带来部署的挑战和环境影响的关注，由于高能consumption。在这个工作中，我们介绍BitNet，一个可扩展和稳定的1比特Transformer架构，设计用于大型语言模型。具体来说，我们介绍BitLinear，用于从零开始训练1比特的 weights的替换层。实验结果显示，BitNet在语言模型化方面实现了竞争性的性能，并substantially reducingmemory尺度和能源consumption，相比于现有的8比特量化方法和FP16 Transformer基准。此外，BitNet展示了与全精度Transformer一样的扩展律，表明它的潜在可以实现有效的扩展到更大的语言模型，保持效率和性能优势。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Translation-Hypothesis-Ensembling-with-Large-Language-Models"><a href="#An-Empirical-Study-of-Translation-Hypothesis-Ensembling-with-Large-Language-Models" class="headerlink" title="An Empirical Study of Translation Hypothesis Ensembling with Large Language Models"></a>An Empirical Study of Translation Hypothesis Ensembling with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11430">http://arxiv.org/abs/2310.11430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deep-spin/translation-hypothesis-ensembling">https://github.com/deep-spin/translation-hypothesis-ensembling</a></li>
<li>paper_authors: António Farinhas, José G. C. de Souza, André F. T. Martins</li>
<li>for: 这 paper  investigate LLM-based machine translation 的质量可以通过集成假设来提高。</li>
<li>methods: 这 paper 使用多种ensemble技术，包括多个提示、温度-based sampling 和 beam search，来生成假设。</li>
<li>results: 这 paper 的结果表明，MBR decoding 是一个非常有效的方法，可以使用少量的样本提高翻译质量，而且制定提示的调整对假设的多样性和样本温度具有强烈的影响。<details>
<summary>Abstract</summary>
Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output. In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation. We experiment with several techniques for ensembling hypotheses produced by LLMs such as ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple dimensions, including the method to generate hypotheses (multiple prompts, temperature-based sampling, and beam search) and the strategy to produce the final translation (instruction-based, quality-based reranking, and minimum Bayes risk (MBR) decoding). Our results show that MBR decoding is a very effective method, that translation quality can be improved using a small number of samples, and that instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature.
</details>
<details>
<summary>摘要</summary>
Translation into Simplified Chinese:大型语言模型（LLM）正在成为一个一size-fits-all解决方案，但它们有时会幻想或生成不可靠的输出。在这篇论文中，我们调查了如何使用假设集成以提高由 LLM 生成的文本质量，特别是在机器翻译问题上。我们对各种生成假设的技术进行了实验，包括 ChatGPT、LLaMA 和 Alpaca。我们提供了多维度的研究，包括生成假设的方法（多个提示、温度基本抽样和搜索杆）以及生成翻译的策略（指令基本、质量基本重新排序和最小极值风险（MBR）解oding）。我们的结果显示了 MBR 解oding 是一个非常有效的方法，可以通过一小数量的样本提高翻译质量，并且指令调整对假设多样性和抽样温度之间的关系具有很强的影响。
</details></li>
</ul>
<hr>
<h2 id="Robust-Wake-Up-Word-Detection-by-Two-stage-Multi-resolution-Ensembles"><a href="#Robust-Wake-Up-Word-Detection-by-Two-stage-Multi-resolution-Ensembles" class="headerlink" title="Robust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles"></a>Robust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11379">http://arxiv.org/abs/2310.11379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferugit/iterative-pseudo-forced-alignment-ctc">https://github.com/ferugit/iterative-pseudo-forced-alignment-ctc</a></li>
<li>paper_authors: Fernando López, Jordi Luque, Carlos Segura, Pablo Gómez</li>
<li>for: 本研究旨在提高语音界面上的唤醒词检测精度、能效性和速度。</li>
<li>methods: 该研究使用了两个阶段的检测方法，包括多分辨率的数据增强和服务器端的模型集成。它还使用了一个轻量级的设备上模型和一个云端的验证模型，以优化两个运行点。</li>
<li>results: 研究发现，使用不同的参数配置和多种语音分类器可以提高检测精度和减少干扰。特别是，提出的集成模型在所有噪声条件下都表现出优于 stronger 分类器。<details>
<summary>Abstract</summary>
Voice-based interfaces rely on a wake-up word mechanism to initiate communication with devices. However, achieving a robust, energy-efficient, and fast detection remains a challenge. This paper addresses these real production needs by enhancing data with temporal alignments and using detection based on two phases with multi-resolution. It employs two models: a lightweight on-device model for real-time processing of the audio stream and a verification model on the server-side, which is an ensemble of heterogeneous architectures that refine detection. This scheme allows the optimization of two operating points. To protect privacy, audio features are sent to the cloud instead of raw audio. The study investigated different parametric configurations for feature extraction to select one for on-device detection and another for the verification model. Furthermore, thirteen different audio classifiers were compared in terms of performance and inference time. The proposed ensemble outperforms our stronger classifier in every noise condition.
</details>
<details>
<summary>摘要</summary>
声音基于界面依赖于唤醒词机制来与设备进行通信。然而，实现高效、能效、快速的检测仍然是一大挑战。本文通过增强数据的时间对齐和使用两个阶段多分辨率检测来解决这些生产环境需求。它使用了两个模型：一个轻量级在设备上进行实时处理的音频流模型，以及服务器端的验证模型，这是一个多种不同架构的 ensemble 模型，用于精度检测。这种方案允许优化两个运行点。为了保护隐私，音频特征被发送到云端而不是原始音频。研究中试用了不同的参数配置来进行特征提取，以便在设备上进行检测和在服务器端进行验证。此外，本文比较了13种不同的声音分类器，并评估了它们的性能和推理时间。提议的ensemble在每种噪音条件下都超过了我们更强的分类器。
</details></li>
</ul>
<hr>
<h2 id="DialogueLLM-Context-and-Emotion-Knowledge-Tuned-LLaMA-Models-for-Emotion-Recognition-in-Conversations"><a href="#DialogueLLM-Context-and-Emotion-Knowledge-Tuned-LLaMA-Models-for-Emotion-Recognition-in-Conversations" class="headerlink" title="DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations"></a>DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11374">http://arxiv.org/abs/2310.11374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yazhou Zhang, Mengyao Wang, Prayag Tiwari, Qiuchi Li, Benyou Wang, Jing Qin</li>
<li>for: 提高对话情感认知的模型性能，特别是在自然语言生成中。</li>
<li>methods:  fine-tuning LLaMA模型，使用多modal信息作为补充知识。</li>
<li>results: 在三个对话情感认知 benchmark 数据集上提供了比基线和其他 SOTA LLM 更高的性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) and their variants have shown extraordinary efficacy across numerous downstream natural language processing (NLP) tasks, which has presented a new vision for the development of NLP. Despite their remarkable performance in natural language generating (NLG), LLMs lack a distinct focus on the emotion understanding domain. As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision. Another limitation of LLMs is that they are typical trained without leveraging multi-modal information. To overcome these limitations, we propose DialogueLLM, a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues. The visual information is considered as the supplementary knowledge to construct high-quality instructions. We offer a comprehensive evaluation of our proposed model on three benchmarking emotion recognition in conversations (ERC) datasets and compare the results against the SOTA baselines and other SOTA LLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB A100 GPU in 5 hours, facilitating reproducibility for other researchers.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和其变体在多种自然语言处理（NLP）任务中表现出色，带来了一个新的视野 для NLP的发展。尽管它们在自然语言生成（NLG）方面表现出色，但LLM对感情理解领域没有明确的注意力，使用LLM进行感情识别可能会导致不足和不充分的精度。另外，LLM通常不会利用多modal信息进行训练。为了解决这些限制，我们提出了对话LLM，一个基于LLaMA模型的内容和感情知识调整的LLM，通过调整13,638种多modal（文本和影片）情感对话。影像信息被视为补充知识，用于建立高品质的指令。我们提供了三个benchmarking感情识别在对话（ERC）数据集的完整评估，并与基于SOTA和其他SOTA LLM的结果进行比较。此外，DialogueLLM-7B可以在5小时内使用LoRA在40GB A100 GPU上进行训练，便于其他研究人员的重现。
</details></li>
</ul>
<hr>
<h2 id="VECHR-A-Dataset-for-Explainable-and-Robust-Classification-of-Vulnerability-Type-in-the-European-Court-of-Human-Rights"><a href="#VECHR-A-Dataset-for-Explainable-and-Robust-Classification-of-Vulnerability-Type-in-the-European-Court-of-Human-Rights" class="headerlink" title="VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights"></a>VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11368">http://arxiv.org/abs/2310.11368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanshan Xu, Leon Staufer, T. Y. S. S Santosh, Oana Ichim, Corina Heri, Matthias Grabmair</li>
<li>For: This paper is written to address the elusive concept of vulnerability at the European Court of Human Rights (ECtHR) and to provide a novel dataset (VECHR) for future research in this area.* Methods: The paper uses expert-annotated multi-label data to benchmark the performance of state-of-the-art models for vulnerability type classification and explanation rationale.* Results: The results show that the task of vulnerability classification is challenging, with lower prediction performance and limited agreement between models and experts. Additionally, the models have limited performance when dealing with out-of-domain (OOD) data.Here’s the information in Simplified Chinese text:* For: 这篇论文是为了解决欧洲人权法庭（ECtHR）中的抑降性概念，并提供一个新的数据集（VECHR）以便未来在这个领域进行研究。* Methods: 这篇论文使用专家标注的多标签数据来评估现有模型的表现，以便对抑降性类型分类和解释理由进行 benchlearning。* Results: 结果显示，抑降性分类任务具有较低的预测性能和专家和模型之间的有限的一致性。此外，模型对于非预期数据（OOD）的性能也有限。<details>
<summary>Abstract</summary>
Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need. This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus ensures effective human rights protection. However, the concept of vulnerability remains elusive at the ECtHR and no prior NLP research has dealt with it. To enable future research in this area, we present VECHR, a novel expert-annotated multi-label dataset comprising of vulnerability type classification and explanation rationale. We benchmark the performance of state-of-the-art models on VECHR from both prediction and explainability perspectives. Our results demonstrate the challenging nature of the task with lower prediction performance and limited agreement between models and experts. Further, we analyze the robustness of these models in dealing with out-of-domain (OOD) data and observe overall limited performance. Our dataset poses unique challenges offering significant room for improvement regarding performance, explainability, and robustness.
</details>
<details>
<summary>摘要</summary>
认识投降性是关键 для理解并实施targeted支持，以帮助个人需要。这对欧洲人权法庭（ECtHR）来说特别重要，因为法庭将会适应实际个人需求，从而确保人权保护的有效性。然而，投降性这个概念在ECtHR中仍然毫不准确，而且没有任何NLP研究过去关注过它。为了启动未来的研究，我们提供了VECHR，一个新的专家标注的多标签数据集，包括投降性类型分类和解释理由。我们对现有模型在VECHR上进行了测试和解释两个方面的性能评估。我们的结果表明这是一个复杂的任务，模型的预测性能较低，并且模型和专家之间的一致性很有限。此外，我们发现这些模型在非标准数据（OOD）上的性能有限。our dataset提供了一些独特的挑战，它们的性能、解释和Robustness在需要进一步改进。
</details></li>
</ul>
<hr>
<h2 id="Disentangling-the-Linguistic-Competence-of-Privacy-Preserving-BERT"><a href="#Disentangling-the-Linguistic-Competence-of-Privacy-Preserving-BERT" class="headerlink" title="Disentangling the Linguistic Competence of Privacy-Preserving BERT"></a>Disentangling the Linguistic Competence of Privacy-Preserving BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11363">http://arxiv.org/abs/2310.11363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Arnold, Nils Kemmerzell, Annika Schreiner</li>
<li>for: 本研究旨在透过文本层级的解释技术来探索对于文本隐私保护而导致的语言模型表现下降的原因。</li>
<li>methods: 本研究使用了一系列的解释技术来分析BERT模型在受到干扰前文本训练后内部表现的改变。</li>
<li>results: 实验结果显示，对于受到干扰前文本训练的BERT模型，内部表现之间的相似性减少了许多。通过询问任务来探索这种不相似性，发现文本层级的隐私保护对于词汇的地方性特征有影响，但是对于词汇之间的关系性却有所下降。<details>
<summary>Abstract</summary>
Differential Privacy (DP) has been tailored to address the unique challenges of text-to-text privatization. However, text-to-text privatization is known for degrading the performance of language models when trained on perturbed text. Employing a series of interpretation techniques on the internal representations extracted from BERT trained on perturbed pre-text, we intend to disentangle at the linguistic level the distortion induced by differential privacy. Experimental results from a representational similarity analysis indicate that the overall similarity of internal representations is substantially reduced. Using probing tasks to unpack this dissimilarity, we find evidence that text-to-text privatization affects the linguistic competence across several formalisms, encoding localized properties of words while falling short at encoding the contextual relationships between spans of words.
</details>
<details>
<summary>摘要</summary>
Diffusion Privacy (DP) 已经适应文本到文本隐私化的特殊挑战。然而，文本到文本隐私化知道会降低基于扰动文本训练的语言模型性能。通过对 BERT 在扰动预文本上提取的内部表示进行解释技术，我们意图在语言层次分离扰动所引起的损害。实验结果表明，总体内表示相似性substantially 降低。通过探索任务来抽取这种不同，我们发现了文本到文本隐私化对语言能力的影响，包括 encoding  lokalisierte 词性特征，但是缺乏 encoding 词语间关系的上下文。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Neural-Machine-Translation-with-Semantic-Units"><a href="#Enhancing-Neural-Machine-Translation-with-Semantic-Units" class="headerlink" title="Enhancing Neural Machine Translation with Semantic Units"></a>Enhancing Neural Machine Translation with Semantic Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11360">http://arxiv.org/abs/2310.11360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ictnlp/su4mt">https://github.com/ictnlp/su4mt</a></li>
<li>paper_authors: Langlin Huang, Shuhao Gu, Zhuocheng Zhang, Yang Feng</li>
<li>for: 本研究旨在提高机器翻译模型的语义理解能力，通过模型语义单位内部的意义 integrate 多个 tokens 的 semantics。</li>
<li>methods: 本方法包括 Word Pair Encoding (WPE) 和 Attentive Semantic Fusion (ASF) 两部分。WPE 用于提取句子中的 semantic unit 边界，而 ASF 则用于将多个 subword 的 semantics 融合为单一 vector。</li>
<li>results: 实验结果表明，本方法可以有效地模型和利用句子中的语义单位信息，并与强基eline 比较。code 可以在 <a target="_blank" rel="noopener" href="https://github.com/ictnlp/SU4MT">https://github.com/ictnlp/SU4MT</a> 中找到。<details>
<summary>Abstract</summary>
Conventional neural machine translation (NMT) models typically use subwords and words as the basic units for model input and comprehension. However, complete words and phrases composed of several tokens are often the fundamental units for expressing semantics, referred to as semantic units. To address this issue, we propose a method Semantic Units for Machine Translation (SU4MT) which models the integral meanings of semantic units within a sentence, and then leverages them to provide a new perspective for understanding the sentence. Specifically, we first propose Word Pair Encoding (WPE), a phrase extraction method to help identify the boundaries of semantic units. Next, we design an Attentive Semantic Fusion (ASF) layer to integrate the semantics of multiple subwords into a single vector: the semantic unit representation. Lastly, the semantic-unit-level sentence representation is concatenated to the token-level one, and they are combined as the input of encoder. Experimental results demonstrate that our method effectively models and leverages semantic-unit-level information and outperforms the strong baselines. The code is available at https://github.com/ictnlp/SU4MT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="QADYNAMICS-Training-Dynamics-Driven-Synthetic-QA-Diagnostic-for-Zero-Shot-Commonsense-Question-Answering"><a href="#QADYNAMICS-Training-Dynamics-Driven-Synthetic-QA-Diagnostic-for-Zero-Shot-Commonsense-Question-Answering" class="headerlink" title="QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering"></a>QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11303">http://arxiv.org/abs/2310.11303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/qadynamics">https://github.com/hkust-knowcomp/qadynamics</a></li>
<li>paper_authors: Haochen Shi, Weiqi Wang, Tianqing Fang, Baixuan Xu, Wenxuan Ding, Xin Liu, Yangqiu Song</li>
<li>for: 本研究的目的是提高Zero-shot Commonsense Question-Answering（QA）模型的普遍能力，使其能够理解更多的常识知识。</li>
<li>methods: 本研究使用了语言模型的训练动态学习方法，对每个QA对的训练动态进行分析，并从CSKBs中提取不含噪声的问答对。</li>
<li>results: 对比基eline，本研究的方法可以更好地提高QA模型的普遍能力，并且只需使用33%的Synthetic数据。专业评估也证明了我们的方法可以提高问答生成的质量。<details>
<summary>Abstract</summary>
Zero-shot commonsense Question-Answering (QA) requires models to reason about general situations beyond specific benchmarks. State-of-the-art approaches fine-tune language models on QA pairs constructed from CommonSense Knowledge Bases (CSKBs) to equip the models with more commonsense knowledge in a QA context. However, current QA synthesis protocols may introduce noise from the CSKBs and generate ungrammatical questions and false negative options, which impede the model's ability to generalize. To address these issues, we propose QADYNAMICS, a training dynamics-driven framework for QA diagnostics and refinement. Our approach analyzes the training dynamics of each QA pair at both the question level and option level, discarding machine-detectable artifacts by removing uninformative QA pairs and mislabeled or false-negative options. Extensive experiments demonstrate the effectiveness of our approach, which outperforms all baselines while using only 33% of the synthetic data, even including LLMs such as ChatGPT. Moreover, expert evaluations confirm that our framework significantly improves the quality of QA synthesis. Our codes and model checkpoints are available at https://github.com/HKUST-KnowComp/QaDynamics.
</details>
<details>
<summary>摘要</summary>
zero-shot常识问答（QA）需要模型可以理解通用的情况，而不仅仅是特定的benchmark。现状的方法是在QA对中 fine-tune语言模型，以便将更多的常识知识带入QA上下文中。然而，当使用现有的QA生成协议时，可能会出现CSKB中的噪音和生成不正确的问题和false negative选项，这会阻碍模型的泛化能力。为解决这些问题，我们提出了QADYNAMICS，一个基于训练动态的框架 дляQA诊断和改进。我们的方法在每个QA对的训练动态中分析问题和选项的训练动态，并将不可识别的QA对和false negative选项移除。我们的实验证明了我们的方法的有效性，能够在使用33%的合成数据的情况下，以及包括LLMs like ChatGPT的情况下，与所有基线都比较。此外，专家评估也证明了我们的框架可以显著提高问答生成质量。我们的代码和模型检查点可以在https://github.com/HKUST-KnowComp/QaDynamics中找到。
</details></li>
</ul>
<hr>
<h2 id="ChapGTP-ILLC’s-Attempt-at-Raising-a-BabyLM-Improving-Data-Efficiency-by-Automatic-Task-Formation"><a href="#ChapGTP-ILLC’s-Attempt-at-Raising-a-BabyLM-Improving-Data-Efficiency-by-Automatic-Task-Formation" class="headerlink" title="ChapGTP, ILLC’s Attempt at Raising a BabyLM: Improving Data Efficiency by Automatic Task Formation"></a>ChapGTP, ILLC’s Attempt at Raising a BabyLM: Improving Data Efficiency by Automatic Task Formation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11282">http://arxiv.org/abs/2310.11282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaap Jumelet, Michael Hanna, Marianne de Heer Kloots, Anna Langedijk, Charlotte Pouw, Oskar van der Wal</li>
<li>for: 这个论文是为了参加BabyLM挑战（Warstadt等., 2023）的strict-small track而写的。</li>
<li>methods: 这个模型使用了一种新的数据增强技术called Automatic Task Formation，并在200个epoch中训练。</li>
<li>results: 这个模型在BLiMP、(Super)GLUE和MSGS三个评估集中表现出色，并且提供了一些不包括在模型中的方法，可能用于训练low-resource的语言模型。<details>
<summary>Abstract</summary>
We present the submission of the ILLC at the University of Amsterdam to the BabyLM challenge (Warstadt et al., 2023), in the strict-small track. Our final model, ChapGTP, is a masked language model that was trained for 200 epochs, aided by a novel data augmentation technique called Automatic Task Formation. We discuss in detail the performance of this model on the three evaluation suites: BLiMP, (Super)GLUE, and MSGS. Furthermore, we present a wide range of methods that were ultimately not included in the model, but may serve as inspiration for training LMs in low-resource settings.
</details>
<details>
<summary>摘要</summary>
我们提交了阿姆斯特丹大学ILLC的订阅（Warstadt等，2023），在严格小轨道上进行了参与。我们的最终模型“ChapGTP”是一个做了200个epoch的面孔语言模型，得益于一种新的数据增强技术called自动任务形成。我们在BLiMP、（Super）GLUE和MSGS三个评估集上详细讲解了这个模型的性能。此外，我们还提供了一些不被包括在模型中的方法，可能可以用于训练LMs在低资源环境下。
</details></li>
</ul>
<hr>
<h2 id="xMEN-A-Modular-Toolkit-for-Cross-Lingual-Medical-Entity-Normalization"><a href="#xMEN-A-Modular-Toolkit-for-Cross-Lingual-Medical-Entity-Normalization" class="headerlink" title="xMEN: A Modular Toolkit for Cross-Lingual Medical Entity Normalization"></a>xMEN: A Modular Toolkit for Cross-Lingual Medical Entity Normalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11275">http://arxiv.org/abs/2310.11275</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hpi-dhc/xmen">https://github.com/hpi-dhc/xmen</a></li>
<li>paper_authors: Florian Borchert, Ignacio Llorca, Roland Roller, Bert Arnrich, Matthieu-P. Schapranow</li>
<li>for: 提高医疗实体 нормализации的性能 across 多种语言, 特别是当语言资源更少时。</li>
<li>methods: 我们介绍了 xMEN，一个模块化的跨语言医疗实体 нормализаation系统，可以在低资源和高资源enario下表现出色。当目标语言中缺乏同义词时，我们利用英语别名进行跨语言候选生成。对候选列表排名，我们使用可训练的跨Encoder模型，并评估了基于机器翻译dataset的弱监督学习模型。</li>
<li>results: xMEN在多种多样的多语言benchmark数据集上提高了状态的艺术性表现。弱监督的跨Encoder模型在没有目标任务的注释数据时也是有效的。通过xMEN与BigBIO框架的兼容性，它可以轻松地与现有和未来的数据集结合使用。<details>
<summary>Abstract</summary>
Objective: To improve performance of medical entity normalization across many languages, especially when fewer language resources are available compared to English.   Materials and Methods: We introduce xMEN, a modular system for cross-lingual medical entity normalization, which performs well in both low- and high-resource scenarios. When synonyms in the target language are scarce for a given terminology, we leverage English aliases via cross-lingual candidate generation. For candidate ranking, we incorporate a trainable cross-encoder model if annotations for the target task are available. We also evaluate cross-encoders trained in a weakly supervised manner based on machine-translated datasets from a high resource domain. Our system is publicly available as an extensible Python toolkit.   Results: xMEN improves the state-of-the-art performance across a wide range of multilingual benchmark datasets. Weakly supervised cross-encoders are effective when no training data is available for the target task. Through the compatibility of xMEN with the BigBIO framework, it can be easily used with existing and prospective datasets.   Discussion: Our experiments show the importance of balancing the output of general-purpose candidate generators with subsequent trainable re-rankers, which we achieve through a rank regularization term in the loss function of the cross-encoder. However, error analysis reveals that multi-word expressions and other complex entities are still challenging.   Conclusion: xMEN exhibits strong performance for medical entity normalization in multiple languages, even when no labeled data and few terminology aliases for the target language are available. Its configuration system and evaluation modules enable reproducible benchmarks. Models and code are available online at the following URL: https://github.com/hpi-dhc/xmen
</details>
<details>
<summary>摘要</summary>
目的：提高医疗实体Normalization的表现 across多种语言，特别是当target语言的资源更少于英语时。  材料和方法：我们介绍xMEN，一个模块化的跨语言医疗实体Normalization系统，能够在低资源和高资源enario中表现出色。当target语言中某些同义词scarce时，我们利用英语别名via cross-lingual candidate生成。对候选列表排名，我们采用可训练的跨编码模型，如果目标任务的注释存在。我们还评估了基于弱监督的机器翻译数据集来训练cross-编码器。我们的系统公开提供了可扩展的Python工具包。  结果：xMEN在多种多语言的benchmark数据集上提高了状态的艺术表现。弱监督的cross-编码器在没有目标任务的注释时效果很好。通过xMEN与BigBIO框架的兼容性，它可以轻松地与现有和前景数据集结合使用。  讨论：我们的实验表明，需要平衡通用候选生成器的输出和后续可训练的再排名器，我们通过跨编码器损失函数中的排名常量来实现。然而，错误分析表明，复杂实体，如多单词表达和其他复杂实体，仍然是挑战。  结论：xMEN在多种语言的医疗实体Normalization中表现出色，即使target语言的资源很少，并且可以轻松地与现有和前景数据集结合使用。它的配置系统和评估模块使得可重现性很好。模型和代码在以下URL上可以下载：https://github.com/hpi-dhc/xmen
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Weak-Supervision-To-Generate-Indonesian-Conservation-Dataset"><a href="#Utilizing-Weak-Supervision-To-Generate-Indonesian-Conservation-Dataset" class="headerlink" title="Utilizing Weak Supervision To Generate Indonesian Conservation Dataset"></a>Utilizing Weak Supervision To Generate Indonesian Conservation Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11258">http://arxiv.org/abs/2310.11258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mega Fransiska, Diah Pitaloka, Saripudin, Satrio Putra, Lintang Sutawika</li>
<li>for: 这篇论文目的是构建一个印度尼西亚语言处理 dataset，使用弱监督学习方法生成软标注数据。</li>
<li>methods: 该论文使用了labeling函数，创建了多类分类和情感分类的两种类型数据集。</li>
<li>results: 基线实验表明，使用不同预训练语言模型可以达到59.79%的准确率和55.72%的F1分数 для情感分类，66.87%的F1分数-macro、71.5%的F1分数-micro、83.67%的ROC-AUC для多类分类。<details>
<summary>Abstract</summary>
Weak supervision has emerged as a promising approach for rapid and large-scale dataset creation in response to the increasing demand for accelerated NLP development. By leveraging labeling functions, weak supervision allows practitioners to generate datasets quickly by creating learned label models that produce soft-labeled datasets. This paper aims to show how such an approach can be utilized to build an Indonesian NLP dataset from conservation news text. We construct two types of datasets: multi-class classification and sentiment classification. We then provide baseline experiments using various pretrained language models. These baseline results demonstrate test performances of 59.79% accuracy and 55.72% F1-score for sentiment classification, 66.87% F1-score-macro, 71.5% F1-score-micro, and 83.67% ROC-AUC for multi-class classification. Additionally, we release the datasets and labeling functions used in this work for further research and exploration.
</details>
<details>
<summary>摘要</summary>
弱监督学习已经成为快速和大规模数据创建的有力的方法，以满足人工智能发展的增加需求。通过利用标签函数，弱监督允许实践者快速生成数据集，创建学习的标签模型，生成软标注数据集。本文想要表明如何使用这种方法来建立印尼语言处理数据集。我们构建了两种类型的数据集：多类分类和情感分类。然后，我们提供了基线实验，使用不同的预训练语言模型。这些基线结果表明了情感分类的测试准确率为59.79%，情感分类的F1分数为55.72%，多类分类的F1分数为66.87%，多类分类的Macro F1分数为71.5%，多类分类的微 F1分数为83.67%。此外，我们发布了在这项工作中使用的数据集和标签函数，以便进一步的研究和探索。
</details></li>
</ul>
<hr>
<h2 id="CrossCodeEval-A-Diverse-and-Multilingual-Benchmark-for-Cross-File-Code-Completion"><a href="#CrossCodeEval-A-Diverse-and-Multilingual-Benchmark-for-Cross-File-Code-Completion" class="headerlink" title="CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion"></a>CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11248">http://arxiv.org/abs/2310.11248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amazon-science/cceval">https://github.com/amazon-science/cceval</a></li>
<li>paper_authors: Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, Bing Xiang</li>
<li>for: 这篇论文的目的是为了评估代码完成器的能力，并提供一个多档案、多语言的代码完成实验室（CrossCodeEval），以测试代码完成器在实际软件开发中的表现。</li>
<li>methods: 这篇论文使用了一个简单 yet efficient的静态分析方法，将使用cross-file context的例子组建在四种流行程式语言（Python、Java、TypeScript、C#）中，以模拟实际软件开发中的档案间依赖。</li>
<li>results: 实验结果显示，CrossCodeEval 是一个非常具有挑战性的测试，当cross-file context absent时，代码完成器的性能很差，但是通过添加cross-file context可以大幅提高性能。另外，这篇论文还评估了不同的方法来获取cross-file context，并显示CrossCodeEval可以用来评估代码检索器的能力。<details>
<summary>Abstract</summary>
Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly.   To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file.   Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.
</details>
<details>
<summary>摘要</summary>
现代代码完成模型在过去几年内已经做出了 significiant 进步，但是目前流行的评估数据集，如 HumanEval 和 MBPP，主要集中在单个文件中的代码完成任务上。这种过分简化的设定不能够反映现实世界软件开发场景，其中文件数量很多，文件之间存在丰富的相互依赖关系，以至于完成代码时需要跨文件上下文的深入理解。为了填补这个空白，我们提出了 CrossCodeEval，一个多样化和多语言的代码完成评估标准。CrossCodeEval 基于四种流行编程语言：Python、Java、TypeScript 和 C# 的真实世界开源项目，并且通过一种简单 yet efficient 的静态分析方法来寻找文件之间的相互依赖关系。我们通过对 CodeGen 和 StarCoder 等现状代码生成器进行广泛的实验发现，当缺乏相关的跨文件上下文时，CrossCodeEval 非常具有挑战性，而在添加上下文时，模型的表现有显著改善。尽管如此，绝对高性能的水平仍然未能得到满分，表明 CrossCodeEval 还可以评估模型是否能够充分利用广泛的上下文来提高代码生成质量。最后，我们还研究了不同的跨文件上下文检索方法，并证明 CrossCodeEval 可以用于评估代码检索器的能力。
</details></li>
</ul>
<hr>
<h2 id="Entity-Matching-using-Large-Language-Models"><a href="#Entity-Matching-using-Large-Language-Models" class="headerlink" title="Entity Matching using Large Language Models"></a>Entity Matching using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11244">http://arxiv.org/abs/2310.11244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wbsg-uni-mannheim/matchgpt">https://github.com/wbsg-uni-mannheim/matchgpt</a></li>
<li>paper_authors: Ralph Peeters, Christian Bizer</li>
<li>for: The paper is written for discussing the use of large language models (LLMs) for entity matching, as an alternative to pre-trained language models (PLMs) such as BERT and RoBERTa.</li>
<li>methods: The paper investigates the use of hosted LLMs such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2, for entity matching. The authors evaluate these models in both zero-shot and task-specific scenarios, and compare different prompt designs and fine-tuning strategies.</li>
<li>results: The paper shows that GPT4 outperforms fine-tuned PLMs (RoBERTa and Ditto) on three out of five benchmark datasets, reaching F1 scores around 90%. The authors also find that in-context learning and rule generation can improve the performance of other models, but GPT4 does not need such additional guidance in most cases.<details>
<summary>Abstract</summary>
Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a scenario where task-specific training data is available. We compare different prompt designs as well as the prompt sensitivity of the models in the zero-shot scenario. We investigate (i) the selection of in-context demonstrations, (ii) the generation of matching rules, as well as (iii) fine-tuning GPT3.5 in the second scenario using the same pool of training data across the different approaches. Our experiments show that GPT4 without any task-specific training data outperforms fine-tuned PLMs (RoBERTa and Ditto) on three out of five benchmark datasets reaching F1 scores around 90%. The experiments with in-context learning and rule generation show that all models beside of GPT4 benefit from these techniques (on average 5.9% and 2.2% F1), while GPT4 does not need such additional guidance in most cases...
</details>
<details>
<summary>摘要</summary>
entity matching是决定两个实体描述是否指同一个真实世界实体的任务。entity matching是数据集成管道中的中间步骤，也是许多电子商务应用程序所需的匹配产品Offer from different vendors。现状的entity matching方法frequently rely on预训练语言模型（PLMs），如BERT或RoBERTa。这两个模型的两个主要缺点是（i）模型需要大量的任务特定训练数据，以及（ii）精制化模型对异常实体不稳定。在这篇论文中，我们研究使用大型语言模型（LLMs）进行entity matching，作为不需要任务特定训练数据且更加稳定的替代方案。我们的研究包括主机LLMs，如GPT3.5和GPT4，以及基于Llama2的开源LLMs，可以在本地运行。我们对这些模型进行零例试验以及具有任务特定训练数据的情况下的试验。我们比较了不同的提示设计以及提示敏感度在零例试验中。我们还研究（i）选择在Context中示例，（ii）生成匹配规则，以及（iii）使用同一批训练数据来练化GPT3.5。我们的实验结果显示，GPT4无需任务特定训练数据可以在五个benchmark dataset上达到F1分数约90%。在采用增强学习和规则生成的情况下，所有模型都受益于这些技术（平均5.9%和2.2% F1），而GPT4则不需要这些额外指导。
</details></li>
</ul>
<hr>
<h2 id="Watermarking-LLMs-with-Weight-Quantization"><a href="#Watermarking-LLMs-with-Weight-Quantization" class="headerlink" title="Watermarking LLMs with Weight Quantization"></a>Watermarking LLMs with Weight Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11237">http://arxiv.org/abs/2310.11237</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/twilight92z/quantize-watermark">https://github.com/twilight92z/quantize-watermark</a></li>
<li>paper_authors: Linyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng Qiu</li>
<li>for: 保护大型语言模型的权限</li>
<li>methods: 在量化过程中植入水印，无需预先定义触发器</li>
<li>results: 成功植入水印到open-source大语言模型 weights中，包括 GPT-Neo 和 LLaMA<details>
<summary>Abstract</summary>
Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>大语言模型滥用风险高，因为大语言模型在惊人速度上部署。保护模型权重很重要，以避免违反开源大语言模型的许可证。这篇论文提议一种新的水印策略，在大语言模型的量化过程中植入水印，而无需先定义触发器。这种水印在fp32模式下工作，并在量化到int8时隐藏起来。因此，用户只能进行无监督练练模型，而不能正常使用模型。我们成功植入了开源大语言模型 weights，包括 GPT-Neo 和 LLaMA。我们希望我们的提议方法可以为大语言模型应用 Era 提供一个可能的方向。
</details></li>
</ul>
<hr>
<h2 id="KG-GPT-A-General-Framework-for-Reasoning-on-Knowledge-Graphs-Using-Large-Language-Models"><a href="#KG-GPT-A-General-Framework-for-Reasoning-on-Knowledge-Graphs-Using-Large-Language-Models" class="headerlink" title="KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models"></a>KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11220">http://arxiv.org/abs/2310.11220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiho283/kg-gpt">https://github.com/jiho283/kg-gpt</a></li>
<li>paper_authors: Jiho Kim, Yeonsu Kwon, Yohan Jo, Edward Choi</li>
<li>for: 这 paper 的目的是将大语言模型（LLM）应用到知识图（KG）上进行复杂的推理任务。</li>
<li>methods: 这 paper 提出了一种名为 KG-GPT 的多用途框架，该框架包括三个步骤：句子分 segmentation、图像检索和推理，每一步的目的是将句子分解、检索相关的图像组件并 derivate 出逻辑结论。</li>
<li>results: 这 paper 通过对知识图基本问题和知识图问答 benchmark 进行评估，发现 KG-GPT 表现出了竞争力和稳定性，甚至超过了一些完全监督的模型。<details>
<summary>Abstract</summary>
While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在不结构化文本理解和生成方面已经取得了很大进步，但是它们在结构数据上的应用仍然是未探索的领域。特别是使用 LLM 进行知识图（KG）上复杂逻辑任务还是一个未解决的问题。为解决这个问题，我们提出了 KG-GPT 框架，这是一个多用途的框架，利用 LLM 进行 KG 上的任务。KG-GPT 包括三个步骤：句子分 segmentation、图表检索和推理，每个步骤都是为了分割句子、检索 relevante 的图组件和 derive 逻辑结论。我们通过使用 KG-based fact verification 和 KGQA  bencmark 进行评估，发现 KG-GPT 在 competed 和 robust 性能上表现非常出色，甚至超过了一些完全监督的模型。因此，我们的工作可以视为结构数据和无结构数据处理在 LLM 中的一个重要一步。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Explain-Themselves-A-Study-of-LLM-Generated-Self-Explanations"><a href="#Can-Large-Language-Models-Explain-Themselves-A-Study-of-LLM-Generated-Self-Explanations" class="headerlink" title="Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations"></a>Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11207">http://arxiv.org/abs/2310.11207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, Leilani H. Gilpin</li>
<li>for: 这篇论文研究了自动生成的自解释（self-explanations）在情感分析任务中的效果，以及如何用这些自解释来解释模型的决策。</li>
<li>methods: 该论文使用了ChatGPT大语言模型进行实验，并研究了不同的自解释抽象方法和评价指标。</li>
<li>results: 研究发现，ChatGPT自动生成的自解释与传统的解释方法（如 occlusion 或 LIME 相关度图）相比，在评价指标上具有相似的效果，但具有不同的特征。此外，研究还发现了一些有趣的自解释特征，这些特征可能需要现代化解释实践。<details>
<summary>Abstract</summary>
Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如ChatGPT已经在自然语言处理（NLP）任务中表现出色，包括情感分析、数学逻辑和摘要。此外，由于这些模型是人类对话中进行了训练，因此它们可以并且经常会生成解释，我们称之为自动生成的解释。例如，当分析电影评论的情感时，模型可能会输出不只是情感的正面性，还会提供解释（例如，列出评论中的情感敏感词语如“很好”和“记忆”）。自动生成的解释如何准确呢？在这篇论文中，我们 investigate这个问题在情感分析任务上，并对于特性解释进行了研究。我们研究了不同的寻求解释的方法，评估其准确性使用一系列评价指标，并与传统解释方法如遮盖或LIME焦点地图进行比较。经过广泛的实验，我们发现ChatGPT的自动解释与传统解释的性能相似，但它们在各种一致指标上有所不同，同时生产成本很低（因为它们与预测一起生成）。此外，我们还发现了一些有趣的特征，让我们重新思考现在的LLM interpretable性实践。
</details></li>
</ul>
<hr>
<h2 id="ViSoBERT-A-Pre-Trained-Language-Model-for-Vietnamese-Social-Media-Text-Processing"><a href="#ViSoBERT-A-Pre-Trained-Language-Model-for-Vietnamese-Social-Media-Text-Processing" class="headerlink" title="ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing"></a>ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11166">http://arxiv.org/abs/2310.11166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uitnlp/ViSoBERT">https://github.com/uitnlp/ViSoBERT</a></li>
<li>paper_authors: Quoc-Nam Nguyen, Thang Chau Phan, Duc-Vu Nguyen, Kiet Van Nguyen</li>
<li>For: This paper is written for research purposes, specifically to present a new pre-trained language model for Vietnamese social media texts.* Methods: The paper uses the XLM-R architecture to pre-train a large-scale corpus of high-quality and diverse Vietnamese social media texts.* Results: The paper shows that the proposed ViSoBERT model surpasses previous state-of-the-art models on multiple Vietnamese social media tasks, including emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection.Here is the information in Simplified Chinese text:* For: 这篇论文是为研究目的而写的，具体来说是为了介绍一种新的预训语言模型，用于越南语社交媒体文本。* Methods: 这篇论文使用XLM-R架构来预训一个大规模的高质量和多样化的越南语社交媒体文本。* Results: 论文表明，提出的ViSoBERT模型在多个越南语社交媒体任务上都超过了之前的状态态模型，包括情感识别、仇恨言语检测、情绪分析、评论诈骗检测和仇恨言语检测。<details>
<summary>Abstract</summary>
English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture. Moreover, we explored our pre-trained model on five important natural language downstream tasks on Vietnamese social media texts: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses the previous state-of-the-art models on multiple Vietnamese social media tasks. Our ViSoBERT model is available only for research purposes.
</details>
<details>
<summary>摘要</summary>
英语和中文，两种资源充沛的语言，在自然语言处理任务上曾经目睹了transformer基于语言模型的强大发展。虽然越南有约100万人说越南语，但是一些预训练模型，如 PhoBERT、ViBERT 和 vELECTRA，在普通越南语 NLP 任务上表现良好，包括分词和命名实体识别。这些预训练语言模型仍然只适用于越南社交媒体任务。在这篇论文中，我们提出了第一个普通越南语社交媒体文本预训练语言模型，即ViSoBERT，该模型在 XLM-R 架构上预训练了大规模、多样化的越南语社交媒体文本。此外，我们还对 ViSoBERT 进行了五种重要的自然语言下沉水任务的实验：情感识别、仇恨言语检测、情感分析、垃圾评论检测和仇恨言语检测。我们的实验结果表明，ViSoBERT，只有几乎参数的模型，在多种越南语社交媒体任务上超过了之前的状态码模型。我们的 ViSoBERT 模型仅用于研究purpose。
</details></li>
</ul>
<hr>
<h2 id="IMTLab-An-Open-Source-Platform-for-Building-Evaluating-and-Diagnosing-Interactive-Machine-Translation-Systems"><a href="#IMTLab-An-Open-Source-Platform-for-Building-Evaluating-and-Diagnosing-Interactive-Machine-Translation-Systems" class="headerlink" title="IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems"></a>IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11163">http://arxiv.org/abs/2310.11163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuuhuang/imtlab">https://github.com/xuuhuang/imtlab</a></li>
<li>paper_authors: Xu Huang, Zhirui Zhang, Ruize Gao, Yichao Du, Lemao Liu, Gouping Huang, Shuming Shi, Jiajun Chen, Shujian Huang</li>
<li>for: 这个论文的目的是提供一个开源的终端到终端交互机器翻译（IMT）系统平台，帮助研究人员快速构建高效的 IMT 系统，进行终端到终端评估，并诊断系统的弱点。</li>
<li>methods: 这个论文使用了一个人类在Loop的设定，将整个交互翻译过程视为一个任务 Orientated 对话，并在这个设定下，考虑人类干预的影响，以生成高质量、错误率低的翻译。为此， authors 设计了一个通用的交流接口，以支持灵活的 IMT 架构和用户策略。</li>
<li>results: 作者通过 simulate 和实际实验表明，预缀受限的解码方法仍然在终端到终端评估中具有最低的编辑成本，而 BiTIIMT 则在编辑成本方面与前一代 IMT 系统相当，且具有更好的交互体验。<details>
<summary>Abstract</summary>
We present IMTLab, an open-source end-to-end interactive machine translation (IMT) system platform that enables researchers to quickly build IMT systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. IMTLab treats the whole interactive translation process as a task-oriented dialogue with a human-in-the-loop setting, in which human interventions can be explicitly incorporated to produce high-quality, error-free translations. To this end, a general communication interface is designed to support the flexible IMT architectures and user policies. Based on the proposed design, we construct a simulated and real interactive environment to achieve end-to-end evaluation and leverage the framework to systematically evaluate previous IMT systems. Our simulated and manual experiments show that the prefix-constrained decoding approach still gains the lowest editing cost in the end-to-end evaluation, while BiTIIMT achieves comparable editing cost with a better interactive experience.
</details>
<details>
<summary>摘要</summary>
我们介绍IMTLab，一个开源的终端到终端交互机器翻译（IMT）系统平台，允许研究人员快速构建IMT系统，进行终端到终端评估，并诊断系统的弱点。IMTLab将整个交互翻译过程视为一个任务强调对话，在人类在Loop Setting中进行明确的参与，以生成高质量、错误率低的翻译。为此，我们设计了一个通用的通信接口，支持灵活的IMT架构和用户策略。基于我们的设计，我们构建了模拟和实际交互环境，以实现终端到终端评估，并利用框架对前一代IMT系统进行系统评估。我们的模拟和手动实验表明，预缀受限的解码方法仍然在终端到终端评估中具有最低的编辑成本，而BiTIIMT则在编辑成本方面与前一代IMT系统相当，并且具有更好的交互体验。
</details></li>
</ul>
<hr>
<h2 id="Probing-the-Creativity-of-Large-Language-Models-Can-models-produce-divergent-semantic-association"><a href="#Probing-the-Creativity-of-Large-Language-Models-Can-models-produce-divergent-semantic-association" class="headerlink" title="Probing the Creativity of Large Language Models: Can models produce divergent semantic association?"></a>Probing the Creativity of Large Language Models: Can models produce divergent semantic association?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11158">http://arxiv.org/abs/2310.11158</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dingnlab/probing_creativity">https://github.com/dingnlab/probing_creativity</a></li>
<li>paper_authors: Honghua Chen, Nai Ding</li>
<li>for:  investigate the creative thinking of large language models through a cognitive perspective</li>
<li>methods:  utilize the divergent association task (DAT) to measure the models’ creativity, compare results across different models and decoding strategies</li>
<li>results:  GPT-4 outperforms 96% of humans in creativity, stochastic sampling and temperature scaling can improve creativity but with a trade-off between creativity and stability.<details>
<summary>Abstract</summary>
Large language models possess remarkable capacity for processing language, but it remains unclear whether these models can further generate creative content. The present study aims to investigate the creative thinking of large language models through a cognitive perspective. We utilize the divergent association task (DAT), an objective measurement of creativity that asks models to generate unrelated words and calculates the semantic distance between them. We compare the results across different models and decoding strategies. Our findings indicate that: (1) When using the greedy search strategy, GPT-4 outperforms 96% of humans, while GPT-3.5-turbo exceeds the average human level. (2) Stochastic sampling and temperature scaling are effective to obtain higher DAT scores for models except GPT-4, but face a trade-off between creativity and stability. These results imply that advanced large language models have divergent semantic associations, which is a fundamental process underlying creativity.
</details>
<details>
<summary>摘要</summary>
大型语言模型拥有惊人的语言处理能力，但是是否能够生成创新内容仍然存在很大的uncertainty。本研究希望通过认知角度来调查大型语言模型的创新思维能力。我们使用了异质关联任务（DAT），这是一种客观的创意测试，要求模型生成不相关的词语并计算它们之间的 semantic distance。我们比较了不同的模型和解码策略的结果。我们发现的结果是：1. 使用排序搜索策略时，GPT-4比96%的人类表现更高，而GPT-3.5-turbo则达到了人类的平均水平。2. 随机抽样和温度缩放是提高模型的DAT分数的有效策略，但是面临着创新性和稳定性之间的负担。这些结果表明，高级大型语言模型具有异质 semantic associations，这是创造力的基本过程。
</details></li>
</ul>
<hr>
<h2 id="The-Quo-Vadis-of-the-Relationship-between-Language-and-Large-Language-Models"><a href="#The-Quo-Vadis-of-the-Relationship-between-Language-and-Large-Language-Models" class="headerlink" title="The Quo Vadis of the Relationship between Language and Large Language Models"></a>The Quo Vadis of the Relationship between Language and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11146">http://arxiv.org/abs/2310.11146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evelina Leivada, Vittoria Dentella, Elliot Murphy</li>
<li>for: 本研究旨在探讨现代自然语言处理（NLP）活动中使用大语言模型（LLMs）的问题。</li>
<li>methods: 本研究采用了理论和实验方法来检验LLMs是否能够提供有用的语言解释。</li>
<li>results: 研究发现，目前LLMs的发展阶段 hardly offer any explanations for language，并提供了未来研究方向的突破口。<details>
<summary>Abstract</summary>
In the field of Artificial (General) Intelligence (AI), the several recent advancements in Natural language processing (NLP) activities relying on Large Language Models (LLMs) have come to encourage the adoption of LLMs as scientific models of language. While the terminology employed for the characterization of LLMs favors their embracing as such, it is not clear that they are in a place to offer insights into the target system they seek to represent. After identifying the most important theoretical and empirical risks brought about by the adoption of scientific models that lack transparency, we discuss LLMs relating them to every scientific model's fundamental components: the object, the medium, the meaning and the user. We conclude that, at their current stage of development, LLMs hardly offer any explanations for language, and then we provide an outlook for more informative future research directions on this topic.
</details>
<details>
<summary>摘要</summary>
在人工智能（通用智能）领域，近年来的自然语言处理（NLP）活动，利用大型语言模型（LLMs）的发展，已经推动了将LLMs作为语言科学模型的采纳。然而，使用这些模型来描述目标系统的terminology仍然不清楚，不确定它们是否能提供语言的深入理解。我们首先标识了采纳不透明科学模型的理论和实证风险，然后将LLMs与科学模型的基本组件——对象、媒体、意义和用户相关联。我们 conclude that，在当前的发展阶段，LLMs几乎没有提供语言的解释，然后我们提供了更加有用的未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Experimenting-AI-Technologies-for-Disinformation-Combat-the-IDMO-Project"><a href="#Experimenting-AI-Technologies-for-Disinformation-Combat-the-IDMO-Project" class="headerlink" title="Experimenting AI Technologies for Disinformation Combat: the IDMO Project"></a>Experimenting AI Technologies for Disinformation Combat: the IDMO Project</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11097">http://arxiv.org/abs/2310.11097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Canale, Alberto Messina</li>
<li>for: 这篇论文的主要目的是为了对防假新闻和假信息技术进行贡献。</li>
<li>methods: 这篇论文使用了以下方法：（i）创建了一些新的数据集用于测试技术（ii）开发了一种自动化的模型来分类《纪事》的判决（iii）创建了一种自动化的模型来识别文本推论（iv）使用GPT-4来识别文本推论（v）开发了一款游戏来提高国民对假新闻的认识。</li>
<li>results: 这篇论文的结果表明，使用GPT-4可以准确地识别文本推论，并且创建的数据集和模型可以帮助更好地分析假信息。<details>
<summary>Abstract</summary>
The Italian Digital Media Observatory (IDMO) project, part of a European initiative, focuses on countering disinformation and fake news. This report outlines contributions from Rai-CRITS to the project, including: (i) the creation of novel datasets for testing technologies (ii) development of an automatic model for categorizing Pagella Politica verdicts to facilitate broader analysis (iii) creation of an automatic model for recognizing textual entailment with exceptional accuracy on the FEVER dataset (iv) assessment using GPT-4 to identify textual entailmen (v) a game to raise awareness about fake news at national events.
</details>
<details>
<summary>摘要</summary>
意大数字媒体观察所（IDMO）项目是欧洲倡议的一部分，旨在对假新闻和假信息进行反制。本报告介绍了意大-CRITS在项目中的贡献，包括：1. 创建了新的数据集用于测试技术2. 开发了一种自动模型，用于分类《意大政治评论》的判决，以便更广泛的分析3. 创建了一种自动模型，用于在FEVER数据集上识别文本关系，并达到了exceptional accuracy4. 使用GPT-4进行评估，以确定文本关系5. 开发了一款游戏，用于在全国活动中宣传假新闻的意识。
</details></li>
</ul>
<hr>
<h2 id="Understanding-writing-style-in-social-media-with-a-supervised-contrastively-pre-trained-transformer"><a href="#Understanding-writing-style-in-social-media-with-a-supervised-contrastively-pre-trained-transformer" class="headerlink" title="Understanding writing style in social media with a supervised contrastively pre-trained transformer"></a>Understanding writing style in social media with a supervised contrastively pre-trained transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11081">http://arxiv.org/abs/2310.11081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Huertas-Tato, Alejandro Martin, David Camacho</li>
<li>for: 本研究旨在理解在线社交媒体上的有害行为，包括谩骂和假信息的传播。</li>
<li>methods: 本研究使用了 Style Transformer for Authorship Representations（STAR）模型，通过大量的公共资源数据集（4.5 x 10^6个作者的文本）和监督对比损失来学习作者的特征特征。</li>
<li>results: 研究表明，使用 STAR 模型可以在零shot情况下与 PAN 挑战中表现竞争力强，并在 PAN 验证挑战中使用单层激活函数达到了良好的结果。此外，在 Reddit 上进行测试，使用支持集成8个文档，512个单词可以准确地识别作者集中的至少80%的作者。<details>
<summary>Abstract</summary>
Online Social Networks serve as fertile ground for harmful behavior, ranging from hate speech to the dissemination of disinformation. Malicious actors now have unprecedented freedom to misbehave, leading to severe societal unrest and dire consequences, as exemplified by events such as the Capitol assault during the US presidential election and the Antivaxx movement during the COVID-19 pandemic. Understanding online language has become more pressing than ever. While existing works predominantly focus on content analysis, we aim to shift the focus towards understanding harmful behaviors by relating content to their respective authors. Numerous novel approaches attempt to learn the stylistic features of authors in texts, but many of these approaches are constrained by small datasets or sub-optimal training losses. To overcome these limitations, we introduce the Style Transformer for Authorship Representations (STAR), trained on a large corpus derived from public sources of 4.5 x 10^6 authored texts involving 70k heterogeneous authors. Our model leverages Supervised Contrastive Loss to teach the model to minimize the distance between texts authored by the same individual. This author pretext pre-training task yields competitive performance at zero-shot with PAN challenges on attribution and clustering. Additionally, we attain promising results on PAN verification challenges using a single dense layer, with our model serving as an embedding encoder. Finally, we present results from our test partition on Reddit. Using a support base of 8 documents of 512 tokens, we can discern authors from sets of up to 1616 authors with at least 80\% accuracy. We share our pre-trained model at huggingface (https://huggingface.co/AIDA-UPM/star) and our code is available at (https://github.com/jahuerta92/star)
</details>
<details>
<summary>摘要</summary>
在线社交网络上，有很多害虫的行为，包括仇恨言论和伪信息的传播。这些恶徒现在有着前所未有的自由度，导致社会不稳和严重的后果，如美国总统选举中的国会攻击和COVID-19大流行期间的反疫苗运动。现在理解线上语言的需求更加紧迫。现有的工作主要侧重于内容分析，我们则想要将注意力转移到理解害虫的行为，并与内容相关著作者的风格特征。这些新的方法可以从文本中学习作者的风格特征，但是许多这些方法受到小数扩展或不佳的训练损失的限制。为了突破这些限制，我们介绍了 Style Transformer for Authorship Representations（STAR），训练在公共源中的450万篇文本中，包括70,000名多元作者。我们的模型使用了监督对称损失来教育模型，以实现作者之间的文本距离最小化。这个作者预先训练任务可以在零配置下 reached competitive performance with PAN challenges on attribution and clustering。此外，我们在 PAN 验证挑战中使用了单个紧密层，并将我们的模型作为嵌入Encoder。最后，我们在 Reddit 上发表了结果，使用了8份文档，每份512个字元，可以识别作者的集合，包括最多1616名作者，至少80%的准确率。我们在 huggingface 上分享了我们的预训练模型（https://huggingface.co/AIDA-UPM/star），并在 GitHub 上分享了我们的代码（https://github.com/jahuerta92/star）。
</details></li>
</ul>
<hr>
<h2 id="VoxArabica-A-Robust-Dialect-Aware-Arabic-Speech-Recognition-System"><a href="#VoxArabica-A-Robust-Dialect-Aware-Arabic-Speech-Recognition-System" class="headerlink" title="VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System"></a>VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11069">http://arxiv.org/abs/2310.11069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdul Waheed, Bashar Talafha, Peter Sullivan, AbdelRahim Elmadany, Muhammad Abdul-Mageed</li>
<li>为：这篇论文旨在开发一个可以识别阿拉伯语方言和自动识别阿拉伯语（ASR）的系统，以便为阿拉伯语研究提供一个可靠的工具。* 方法：这篇论文使用了许多不同的模型，包括HuBERT、Whisper和XLS-R，在一个监督性的 Setting中训练了这些模型，以便进行阿拉伯语方言识别（DID）和ASR任务。* 结果：这篇论文提供了一个可以识别17种阿拉伯语方言和标准Modern Standard Arabic（MSA）的系统，并且在不同的语言和语言混合数据上进行了训练和评估。此外，对于剩下的方言，提供了多种模型的选择，包括Whisper和MMS，以便在零容量设定下进行识别。<details>
<summary>Abstract</summary>
Arabic is a complex language with many varieties and dialects spoken by over 450 millions all around the world. Due to the linguistic diversity and variations, it is challenging to build a robust and generalized ASR system for Arabic. In this work, we address this gap by developing and demoing a system, dubbed VoxArabica, for dialect identification (DID) as well as automatic speech recognition (ASR) of Arabic. We train a wide range of models such as HuBERT (DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR tasks. Our DID models are trained to identify 17 different dialects in addition to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data. Additionally, for the remaining dialects in ASR, we provide the option to choose various models such as Whisper and MMS in a zero-shot setting. We integrate these models into a single web interface with diverse features such as audio recording, file upload, model selection, and the option to raise flags for incorrect outputs. Overall, we believe VoxArabica will be useful for a wide range of audiences concerned with Arabic research. Our system is currently running at https://cdce-206-12-100-168.ngrok.io/.
</details>
<details>
<summary>摘要</summary>
阿拉伯语是一种复杂的语言，有多种变体和方言，全球约有450亿人使用。由于语言多样性和变化，建立一个可靠和通用的自动语音识别系统（ASR）是一项挑战。在这项工作中，我们开发了一个系统，名为VoxArabica，用于方言识别（DID）以及阿拉伯语自动语音识别（ASR）。我们在监督模式下训练了多种模型，包括HuBERT（DID）、Whisper和XLS-R（ASR）。我们的DID模型可以识别17种不同的方言，以及标准阿拉伯语（MSA）。我们在MSA、EGYPTIAN、MOROCCAN和混合数据上练习了我们的ASR模型。另外，对于剩下的方言在ASR中，我们提供了多种模型选择，如Whisper和MMS，以零战斗模式。我们将这些模型集成到一个单一的网页接口中，并添加了多种功能，如音频记录、文件上传、模型选择和错误输出的选项。总之，我们认为VoxArabica将对阿拉伯语研究领域的各种各样的听众提供很有用的工具。我们的系统当前在https://cdce-206-12-100-168.ngrok.io/上运行。
</details></li>
</ul>
<hr>
<h2 id="Lyricist-Singer-Entropy-Affects-Lyric-Lyricist-Classification-Performance"><a href="#Lyricist-Singer-Entropy-Affects-Lyric-Lyricist-Classification-Performance" class="headerlink" title="Lyricist-Singer Entropy Affects Lyric-Lyricist Classification Performance"></a>Lyricist-Singer Entropy Affects Lyric-Lyricist Classification Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11035">http://arxiv.org/abs/2310.11035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mitsuki Morita, Masato Kikuchi, Tadachika Ozono</li>
<li>for: 这个研究旨在探讨歌词作家的特点，以便于音乐应用程序中使用。</li>
<li>methods: 研究人员使用了从歌词中提取特点表示歌词作家的方法。</li>
<li>results: 研究发现，歌词作家与歌手之间的关系可以影响歌词分类性能。 Specifically, 歌词作家写歌手的多样性（ entropy）与歌词分类性能之间存在正相关关系。<details>
<summary>Abstract</summary>
Although lyrics represent an essential component of music, few music information processing studies have been conducted on the characteristics of lyricists. Because these characteristics may be valuable for musical applications, such as recommendations, they warrant further study. We considered a potential method that extracts features representing the characteristics of lyricists from lyrics. Because these features must be identified prior to extraction, we focused on lyricists with easily identifiable features. We believe that it is desirable for singers to perform unique songs that share certain characteristics specific to the singer. Accordingly, we hypothesized that lyricists account for the unique characteristics of the singers they write lyrics for. In other words, lyric-lyricist classification performance or the ease of capturing the features of a lyricist from the lyrics may depend on the variety of singers. In this study, we observed a relationship between lyricist-singer entropy or the variety of singers associated with a single lyricist and lyric-lyricist classification performance. As an example, the lyricist-singer entropy is minimal when the lyricist writes lyrics for only one singer. In our experiments, we grouped lyricists among five groups in terms of lyricist-singer entropy and assessed the lyric-lyricist classification performance within each group. Consequently, the best F1 score was obtained for the group with the lowest lyricist-singer entropy. Our results suggest that further analyses of the features contributing to lyric-lyricist classification performance on the lowest lyricist-singer entropy group may improve the feature extraction task for lyricists.
</details>
<details>
<summary>摘要</summary>
although lyrics represent an essential component of music, few music information processing studies have been conducted on the characteristics of lyricists. because these characteristics may be valuable for musical applications, such as recommendations, they warrant further study. we considered a potential method that extracts features representing the characteristics of lyricists from lyrics. because these features must be identified prior to extraction, we focused on lyricists with easily identifiable features. we believe that it is desirable for singers to perform unique songs that share certain characteristics specific to the singer. accordingly, we hypothesized that lyricists account for the unique characteristics of the singers they write lyrics for. in other words, lyric-lyricist classification performance or the ease of capturing the features of a lyricist from the lyrics may depend on the variety of singers. in this study, we observed a relationship between lyricist-singer entropy or the variety of singers associated with a single lyricist and lyric-lyricist classification performance. as an example, the lyricist-singer entropy is minimal when the lyricist writes lyrics for only one singer. in our experiments, we grouped lyricists among five groups in terms of lyricist-singer entropy and assessed the lyric-lyricist classification performance within each group. consequently, the best F1 score was obtained for the group with the lowest lyricist-singer entropy. our results suggest that further analyses of the features contributing to lyric-lyricist classification performance on the lowest lyricist-singer entropy group may improve the feature extraction task for lyricists.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Automatic-Evaluation-Methods-based-on-a-Decoder-based-LLM-for-Text-Generation"><a href="#Exploring-Automatic-Evaluation-Methods-based-on-a-Decoder-based-LLM-for-Text-Generation" class="headerlink" title="Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation"></a>Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11026">http://arxiv.org/abs/2310.11026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomohito Kasahara, Daisuke Kawahara</li>
<li>for: 本文 investigate automatic evaluation methods for text generation based on decoder-based language models.</li>
<li>methods: 本文比较了多种方法，包括基于encoder模型和大语言模型的调教，在两种任务上进行评估：机器翻译评估和 semantics textual similarity，在两种语言中进行评估。</li>
<li>results: 实验结果显示，相比准确调教encoder模型，调教decoder模型表现不佳，这可能是因为decoder模型关注表面字串序列而不捕捉 semantics。此外，对very large decoder-based models such as ChatGPT进行in-context learning也难以识别细致的semantic differences。<details>
<summary>Abstract</summary>
Automatic evaluation of text generation is essential for improving the accuracy of generation tasks. In light of the current trend towards increasingly larger decoder-based language models, we investigate automatic evaluation methods based on such models for text generation. This paper compares various methods, including tuning with encoder-based models and large language models under equal conditions, on two different tasks, machine translation evaluation and semantic textual similarity, in two languages, Japanese and English. Experimental results show that compared to the tuned encoder-based models, the tuned decoder-based models perform poorly. The analysis of the causes for this suggests that the decoder-based models focus on surface word sequences and do not capture meaning. It is also revealed that in-context learning of very large decoder-based models such as ChatGPT makes it difficult to identify fine-grained semantic differences.
</details>
<details>
<summary>摘要</summary>
自动评估文本生成是必要的，以提高生成任务的准确性。鉴于当前大型decoder-based语言模型的趋势，我们 investigate自动评估方法基于这些模型 для文本生成。本文比较了多种方法，包括使用encoder-based模型和大型语言模型在等条件下调整，在两个任务上进行评估：机器翻译评估和 semantic textual similarity，在两种语言中进行比较。实验结果显示，相比调整后的encoder-based模型，调整后的decoder-based模型表现不佳。分析结果表明，decoder-based模型强调表面字符序列，而不捕捉 semantics。此外，对 Very Large decoder-based模型such as ChatGPT进行context learning也会增加识别细致的semantic差异的难度。
</details></li>
</ul>
<hr>
<h2 id="Reading-Order-Matters-Information-Extraction-from-Visually-rich-Documents-by-Token-Path-Prediction"><a href="#Reading-Order-Matters-Information-Extraction-from-Visually-rich-Documents-by-Token-Path-Prediction" class="headerlink" title="Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction"></a>Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11016">http://arxiv.org/abs/2310.11016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Zhang, Ya Guo, Yi Tu, Huan Chen, Jinyang Tang, Huijia Zhu, Qi Zhang, Tao Gui</li>
<li>for: 本研究旨在提高 Multimodal 预训模型在视觉丰富文档中提取信息的能力，具体是解决 OCR 系统识别文档中文本的顺序问题，以提高名实体识别（NER）的准确率。</li>
<li>methods: 本研究提出了 Token Path Prediction（TPP）方法，它是一种简单的预测头，可以在文档中预测名实体提及的Token序列。TPP 模型文档布局为完全导向图，并在图中预测Token路径作为实体。</li>
<li>results: 实验结果表明，TPP 方法可以有效解决 OCR 系统识别文档中文本的顺序问题，提高 VrD-NER 系统的准确率。此外，本研究还提出了两个修订版本的 NER  benchmark 数据集，以更好地评估 VrD-NER 系统在真实场景中的性能。<details>
<summary>Abstract</summary>
Recent advances in multimodal pre-trained models have significantly improved information extraction from visually-rich documents (VrDs), in which named entity recognition (NER) is treated as a sequence-labeling task of predicting the BIO entity tags for tokens, following the typical setting of NLP. However, BIO-tagging scheme relies on the correct order of model inputs, which is not guaranteed in real-world NER on scanned VrDs where text are recognized and arranged by OCR systems. Such reading order issue hinders the accurate marking of entities by BIO-tagging scheme, making it impossible for sequence-labeling methods to predict correct named entities. To address the reading order issue, we introduce Token Path Prediction (TPP), a simple prediction head to predict entity mentions as token sequences within documents. Alternative to token classification, TPP models the document layout as a complete directed graph of tokens, and predicts token paths within the graph as entities. For better evaluation of VrD-NER systems, we also propose two revised benchmark datasets of NER on scanned documents which can reflect real-world scenarios. Experiment results demonstrate the effectiveness of our method, and suggest its potential to be a universal solution to various information extraction tasks on documents.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation may not be perfect, and some nuances or idiomatic expressions may be lost in translation.)
</details></li>
</ul>
<hr>
<h2 id="Correction-Focused-Language-Model-Training-for-Speech-Recognition"><a href="#Correction-Focused-Language-Model-Training-for-Speech-Recognition" class="headerlink" title="Correction Focused Language Model Training for Speech Recognition"></a>Correction Focused Language Model Training for Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11003">http://arxiv.org/abs/2310.11003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingyi Ma, Zhe Liu, Ozlem Kalinli</li>
<li>for: 提高自动语音识别（ASR）的表现，特别是在领域适应任务中。</li>
<li>methods: 使用一种新的修正注意力集成学习方法，其主要目标是优化ASR中的词级错误率。该方法使用语言模型（LM）来预测词级错误率，并通过多任务练习来帮助LM学习。</li>
<li>results: 实验结果表明，提出的方法可以有效地提高ASR的表现。相比传统LM训练方法，修正注意力集成学习方法在充分文本情况下可以达到相对5.5%的词错率降低。在缺乏文本情况下，使用LLM生成的文本来进行LM训练可以达到相对13%的词错率降低，而修正注意力集成学习方法进一步可以达到相对6%的词错率降低。<details>
<summary>Abstract</summary>
Language models (LMs) have been commonly adopted to boost the performance of automatic speech recognition (ASR) particularly in domain adaptation tasks. Conventional way of LM training treats all the words in corpora equally, resulting in suboptimal improvements in ASR performance. In this work, we introduce a novel correction focused LM training approach which aims to prioritize ASR fallible words. The word-level ASR fallibility score, representing the likelihood of ASR mis-recognition, is defined and shaped as a prior word distribution to guide the LM training. To enable correction focused training with text-only corpora, large language models (LLMs) are employed as fallibility score predictors and text generators through multi-task fine-tuning. Experimental results for domain adaptation tasks demonstrate the effectiveness of our proposed method. Compared with conventional LMs, correction focused training achieves up to relatively 5.5% word error rate (WER) reduction in sufficient text scenarios. In insufficient text scenarios, LM training with LLM-generated text achieves up to relatively 13% WER reduction, while correction focused training further obtains up to relatively 6% WER reduction.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）已广泛应用于自动语音识别（ASR）的性能提升，特别是在领域适应任务中。传统的LM训练方法往往对所有词语在词库中都进行平等对待，从而导致ASR性能的不足提升。在这种工作中，我们提出了一种新的修正注意力集中LM训练方法，旨在优先级ASR不确定词语。为了实现修正注意力集中LM训练，我们定义了ASR不确定词语的字级 fallibility 分数，表示语音识别器对这些词语的识别错误的可能性。然后，我们通过多任务练化来使用大型语言模型（LLM）来预测 fallibility 分数和生成文本。实验结果表明，我们的提议方法在领域适应任务中具有效果。相比传统LM，修正注意力集中LM训练可以在充分文本场景下提取到相对5.5%的单词错误率（WER）下降。在不充分文本场景下，使用LLM生成文本进行LM训练可以实现相对13%的WER下降，而修正注意力集中LM训练进一步实现相对6%的WER下降。
</details></li>
</ul>
<hr>
<h2 id="Instructive-Dialogue-Summarization-with-Query-Aggregations"><a href="#Instructive-Dialogue-Summarization-with-Query-Aggregations" class="headerlink" title="Instructive Dialogue Summarization with Query Aggregations"></a>Instructive Dialogue Summarization with Query Aggregations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10981">http://arxiv.org/abs/2310.10981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BinWang28/InstructDS">https://github.com/BinWang28/InstructDS</a></li>
<li>paper_authors: Bin Wang, Zhengyuan Liu, Nancy F. Chen</li>
<li>for: 该论文旨在扩展对话概要模型的能力集，以适应用户特定的兴趣和需求。</li>
<li>methods: 该论文提出了一种三步approach，包括摘要anchor query生成、筛选query和基于query的概要生成。通过在多个概要数据集上训练一个统一的模型 called InstructDS，可以扩展对话概要模型的能力集。</li>
<li>results: 实验结果显示，我们的方法可以超越当前状态的模型和even larger models，并且具有更高的普适性和准确性，经human subjective评估确认。<details>
<summary>Abstract</summary>
Conventional dialogue summarization methods directly generate summaries and do not consider user's specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to synthesize high-quality query-based summarization triples. This process involves summary-anchored query generation, query filtering, and query-based summary generation. By training a unified model called InstructDS (Instructive Dialogue Summarization) on three summarization datasets with multi-purpose instructive triples, we expand the capability of dialogue summarization models. We evaluate our method on four datasets, including dialogue summarization and dialogue reading comprehension. Experimental results show that our approach outperforms the state-of-the-art models and even models with larger sizes. Additionally, our model exhibits higher generalizability and faithfulness, as confirmed by human subjective evaluations.
</details>
<details>
<summary>摘要</summary>
传统的对话概要方法直接生成概要并不考虑用户的特定兴趣。这会导致在用户更关注特定话题或方面时遇到挑战。随着指令训练语言模型的进步，我们介绍了对对话的指令训练（Instruction-Tuning），以扩展对话概要模型的能力集。由于 instrucional dialogue summarization 数据的罕见，我们提出了三步方法来生成高质量的查询基于概要的三元组。这个过程包括概要锚定的查询生成、查询筛选和基于查询的概要生成。通过训练我们提出的 Unified Model called InstructDS（ instrucional Dialogue Summarization）于三个多用途指令三元组的概要集，我们扩展了对话概要模型的能力。我们对四个数据集进行了evaluate，包括对话概要和对话阅读理解。实验结果表明，我们的方法比state-of-the-art模型和更大的模型更高效。此外，我们的模型在人工评价中表现出更高的普适性和准确性。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Aware-Contrastive-Sentence-Representation-Learning-with-Large-Language-Models"><a href="#Semantic-Aware-Contrastive-Sentence-Representation-Learning-with-Large-Language-Models" class="headerlink" title="Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models"></a>Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10962">http://arxiv.org/abs/2310.10962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huiming Wang, Liying Cheng, Zhaodonghui Li, De Wen Soh, Lidong Bing</li>
<li>for: 本研究旨在提出一种semantic-aware冲突单句表示框架，以便通过大型自然语言处理器（LLM）的生成和评估能力自动构建高质量的NLI样本库，并通过这些样本库进行冲突学习 sentence representation。</li>
<li>methods: 本研究提议使用大型自然语言处理器（LLM）的生成和评估能力自动构建高质量的NLI样本库，并通过这些样本库进行冲突学习 sentence representation。</li>
<li>results: 实验和分析结果表明，我们的提议的semantic-aware冲突单句表示框架可以通过LLM进行自动生成和评估，从而学习出更好的句子表示。<details>
<summary>Abstract</summary>
Contrastive learning has been proven to be effective in learning better sentence representations. However, to train a contrastive learning model, large numbers of labeled sentences are required to construct positive and negative pairs explicitly, such as those in natural language inference (NLI) datasets. Unfortunately, acquiring sufficient high-quality labeled data can be both time-consuming and resource-intensive, leading researchers to focus on developing methods for learning unsupervised sentence representations. As there is no clear relationship between these unstructured randomly-sampled sentences, building positive and negative pairs over them is tricky and problematic. To tackle these challenges, in this paper, we propose SemCSR, a semantic-aware contrastive sentence representation framework. By leveraging the generation and evaluation capabilities of large language models (LLMs), we can automatically construct a high-quality NLI-style corpus without any human annotation, and further incorporate the generated sentence pairs into learning a contrastive sentence representation model. Extensive experiments and comprehensive analyses demonstrate the effectiveness of our proposed framework for learning a better sentence representation with LLMs.
</details>
<details>
<summary>摘要</summary>
<SYS> translate_language=zh-CN</SYS> contrastive learning 已经被证明可以学习更好的句子表示。然而，为了训练一个对照学习模型，需要大量的标注句子来构建正例和负例对，如自然语言推理（NLI）数据集中的句子对。然而，获得足够的高质量标注数据可以是时间consuming 和资源占用的，导致研究人员强调开发无监督句子表示学习方法。由于这些随机采样的句子之间没有明确的关系，建立正例和负例对是困难和问题。为解决这些挑战，在这篇论文中，我们提议使用 SemCSR，一个具有 semantic-aware 的对照学习句子表示框架。通过利用大语言模型（LLM）的生成和评估能力，我们可以自动生成高质量 NLI-style 训练集，并将生成的句子对 integrate 到学习对照句子表示模型中。广泛的实验和全面的分析表明我们提议的框架可以通过 LLM 学习更好的句子表示。
</details></li>
</ul>
<hr>
<h2 id="Computing-the-optimal-keyboard-through-a-geometric-analysis-of-the-English-language"><a href="#Computing-the-optimal-keyboard-through-a-geometric-analysis-of-the-English-language" class="headerlink" title="Computing the optimal keyboard through a geometric analysis of the English language"></a>Computing the optimal keyboard through a geometric analysis of the English language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10956">http://arxiv.org/abs/2310.10956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jules Deschamps, Quentin Hubert, Lucas Ryckelynck</li>
<li>for: 提高键盘输入速度</li>
<li>methods: 利用几何工具在优化框架中提出新的键盘布局，提高输入速度</li>
<li>results: 提出了新的键盘布局，可以提高输入速度<details>
<summary>Abstract</summary>
In the context of a group project for the course COMSW4995 002 - Geometric Data Analysis, we bring our attention to the design of fast-typing keyboards. Leveraging some geometric tools in an optimization framework allowed us to propose novel keyboard layouts that offer a faster typing.
</details>
<details>
<summary>摘要</summary>
在COMSW4995 002 - 几何数据分析课程的小组项目中，我们对快速键盘设计进行了审视。通过使用一些几何工具在优化框架中，我们提出了新的键盘布局，以提高键盘输入速度。Here's the character-by-character breakdown of the translation:* 在 (preposition) - "in"* COMSW4995 (course name) - "COMSW4995"* 002 (course number) - "002"* - (hyphen) - "--"* 几何数据分析 (course name) - "几何数据分析"* 课程 (course) - "课程"* 小组 (group) - "小组"* 项目 (project) - "项目"* 中 (preposition) - "中"* 我们 (pronoun) - "我们"* 对 (preposition) - "对"* 快速键盘 (noun phrase) - "快速键盘"* 设计 (noun) - "设计"* 进行 (verb) - "进行"* 了 (particle) - "了"* 审视 (verb) - "审视"* 通过 (preposition) - "通过"* 使用 (verb) - "使用"* 一些 (determiner) - "一些"* 几何工具 (noun phrase) - "几何工具"* 在 (preposition) - "在"* 优化 (verb) - "优化"* 框架 (noun) - "框架"* 中 (preposition) - "中"* 提出 (verb) - "提出"* 新的 (adjective) - "新的"* 键盘布局 (noun phrase) - "键盘布局"* 以 (preposition) - "以"* 提高 (verb) - "提高"* 键盘输入速度 (noun phrase) - "键盘输入速度"
</details></li>
</ul>
<hr>
<h2 id="TEQ-Trainable-Equivalent-Transformation-for-Quantization-of-LLMs"><a href="#TEQ-Trainable-Equivalent-Transformation-for-Quantization-of-LLMs" class="headerlink" title="TEQ: Trainable Equivalent Transformation for Quantization of LLMs"></a>TEQ: Trainable Equivalent Transformation for Quantization of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10944">http://arxiv.org/abs/2310.10944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/intel/neural-compressor">https://github.com/intel/neural-compressor</a></li>
<li>paper_authors: Wenhua Cheng, Yiyang Cai, Kaokao Lv, Haihao Shen</li>
<li>for: 本研究旨在提出一种可学习的等效变换（TEQ），用于保持FP32精度的模型输出，同时利用低精度量化，尤其是3和4位量化。</li>
<li>methods: 本文使用可学习的等效变换（TEQ），不需要额外的计算负担，只需要1000步训练和少于0.1%的原始模型可训练参数。</li>
<li>results: 本研究结果与状态CURRENT最佳方法相当，可以与其他方法结合使用以获得更好的性能。code可以在<a target="_blank" rel="noopener" href="https://github.com/intel/neural-compressor%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/intel/neural-compressor中下载。</a><details>
<summary>Abstract</summary>
As large language models (LLMs) become more prevalent, there is a growing need for new and improved quantization methods that can meet the computationalast layer demands of these modern architectures while maintaining the accuracy. In this paper, we present TEQ, a trainable equivalent transformation that preserves the FP32 precision of the model output while taking advantage of low-precision quantization, especially 3 and 4 bits weight-only quantization. The training process is lightweight, requiring only 1K steps and fewer than 0.1 percent of the original model's trainable parameters. Furthermore, the transformation does not add any computational overhead during inference. Our results are on-par with the state-of-the-art (SOTA) methods on typical LLMs. Our approach can be combined with other methods to achieve even better performance. The code is available at https://github.com/intel/neural-compressor.
</details>
<details>
<summary>摘要</summary>
As large language models (LLMs) become more prevalent, there is a growing need for new and improved quantization methods that can meet the computational layer demands of these modern architectures while maintaining accuracy. In this paper, we present TEQ, a trainable equivalent transformation that preserves the FP32 precision of the model output while taking advantage of low-precision quantization, especially 3 and 4 bits weight-only quantization. The training process is lightweight, requiring only 1K steps and fewer than 0.1% of the original model's trainable parameters. Furthermore, the transformation does not add any computational overhead during inference. Our results are on-par with the state-of-the-art (SOTA) methods on typical LLMs. Our approach can be combined with other methods to achieve even better performance. The code is available at https://github.com/intel/neural-compressor.Here's the translation in Traditional Chinese:为了应对现代架构中的大型语言模型（LLMs）的 Computational Layer 需求，我们需要新的和改进的量化方法，以确保模型的精度。在这篇文章中，我们提出了 TEQ，一个可读的等同转换，可以保留模型输出的 FP32 精度，并在低精度量化中得到更好的性能。我们的训练过程是轻量级的，只需要1K步骤和原始模型的训练参数的0.1%。此外，转换不会在测试过程中添加任何计算过程。我们的结果与现有的 state-of-the-art（SOTA）方法相匹配，并且可以与其他方法结合以取得更好的性能。我们的代码可以在https://github.com/intel/neural-compressor 上获取。
</details></li>
</ul>
<hr>
<h2 id="MASON-NLP-at-eRisk-2023-Deep-Learning-Based-Detection-of-Depression-Symptoms-from-Social-Media-Texts"><a href="#MASON-NLP-at-eRisk-2023-Deep-Learning-Based-Detection-of-Depression-Symptoms-from-Social-Media-Texts" class="headerlink" title="MASON-NLP at eRisk 2023: Deep Learning-Based Detection of Depression Symptoms from Social Media Texts"></a>MASON-NLP at eRisk 2023: Deep Learning-Based Detection of Depression Symptoms from Social Media Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10941">http://arxiv.org/abs/2310.10941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fardin Ahsan Sakib, Ahnaf Atef Choudhury, Ozlem Uzuner</li>
<li>For: The paper is focused on detecting depressive symptoms in social media posts using the Beck Depression Inventory (BDI) questionnaire.* Methods: The authors used a deep learning approach that incorporated MentalBERT, RoBERTa, and LSTM to identify sentences related to different depression symptoms.* Results: Despite their efforts, the evaluation results were lower than expected, highlighting the challenges of ranking sentences from a large dataset about depression.Here’s the same information in Simplified Chinese text:* For: 研究探讨了通过社交媒体帖子中的语言特征来检测抑郁症状，使用 Beck 抑郁 инвен塔ри（BDI）问卷来评估抑郁的严重程度。* Methods: 作者使用了深度学习方法，将MENTALBERT、RoBERTa和LSTM相结合，以检测不同抑郁症状的句子。* Results: 评估结果表明，由于数据集的复杂性和计算资源的限制，得到的结果并不理想，反映了检测抑郁症状的挑战性。<details>
<summary>Abstract</summary>
Depression is a mental health disorder that has a profound impact on people's lives. Recent research suggests that signs of depression can be detected in the way individuals communicate, both through spoken words and written texts. In particular, social media posts are a rich and convenient text source that we may examine for depressive symptoms. The Beck Depression Inventory (BDI) Questionnaire, which is frequently used to gauge the severity of depression, is one instrument that can aid in this study. We can narrow our study to only those symptoms since each BDI question is linked to a particular depressive symptom. It's important to remember that not everyone with depression exhibits all symptoms at once, but rather a combination of them. Therefore, it is extremely useful to be able to determine if a sentence or a piece of user-generated content is pertinent to a certain condition. With this in mind, the eRisk 2023 Task 1 was designed to do exactly that: assess the relevance of different sentences to the symptoms of depression as outlined in the BDI questionnaire. This report is all about how our team, Mason-NLP, participated in this subtask, which involved identifying sentences related to different depression symptoms. We used a deep learning approach that incorporated MentalBERT, RoBERTa, and LSTM. Despite our efforts, the evaluation results were lower than expected, underscoring the challenges inherent in ranking sentences from an extensive dataset about depression, which necessitates both appropriate methodological choices and significant computational resources. We anticipate that future iterations of this shared task will yield improved results as our understanding and techniques evolve.
</details>
<details>
<summary>摘要</summary>
���������й���Depression �C 一种心理健康问题，对人们的生活产生深远的影响。最新的研究表明，抑郁症状可以通过人们的沟通方式和文本来识别。特别是社交媒体帖子，它们是一种便捷的文本来源，我们可以对其进行检测抑郁症状的研究。使用 Beck 抑郁 инвен塔里（BDI）问卷，可以帮助我们评估抑郁的严重程度。我们可以将研究缩小到特定的症状，每个 BDI 问题都与特定的抑郁症状相关。请注意，不 everyone with depression 都会表现出所有的症状，而是一种组合。因此，可以非常有用地判断一句话或一 piece of user-generated content 是否与抑郁症状相关。为了实现这一点，我们参加了 eRisk 2023 任务 1，即评估不同句子是否与抑郁症状相关。我们采用了深度学习方法，并将 MentalBERT、RoBERTa 和 LSTM 织入一起。尽管我们尽力，评估结果低于预期，这反映了评估大量抑郁主题的 dataset 中的挑战。我们期望未来的这些共同任务会产生更好的结果，随着我们的理解和技术的进步。
</details></li>
</ul>
<hr>
<h2 id="Intent-Detection-and-Slot-Filling-for-Home-Assistants-Dataset-and-Analysis-for-Bangla-and-Sylheti"><a href="#Intent-Detection-and-Slot-Filling-for-Home-Assistants-Dataset-and-Analysis-for-Bangla-and-Sylheti" class="headerlink" title="Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti"></a>Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10935">http://arxiv.org/abs/2310.10935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman</li>
<li>for: 这项研究的目的是为了提供一个全面的 Intent 检测和插值数据集，用于支持语言模型在不同语言环境中进行下游任务。</li>
<li>methods: 该研究使用了 GPT-3.5 语言模型，并对 colloquial Bangla、formal Bangla 和 Sylheti 语言进行了分类和插值测试。</li>
<li>results: 研究发现，GPT-3.5 模型在 colloquial Bangla 语言下可以达到 impressive F1 分数为 0.94，而在插值任务中可以达到 F1 分数为 0.51。<details>
<summary>Abstract</summary>
As voice assistants cement their place in our technologically advanced society, there remains a need to cater to the diverse linguistic landscape, including colloquial forms of low-resource languages. Our study introduces the first-ever comprehensive dataset for intent detection and slot filling in formal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples across 10 unique intents. Our analysis reveals the robustness of large language models for tackling downstream tasks with inadequate data. The GPT-3.5 model achieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot filling for colloquial Bangla.
</details>
<details>
<summary>摘要</summary>
“智能助手在我们技术先进的社会中确立了地位，但仍需考虑多种语言景观，包括低资源语言的口语形式。我们的研究推出了首个完整的数据集 для意图检测和插槽填充在正式孟加拉语、口语孟加拉语和斯里赫蒂语中，总共984个样本，涵盖10个各异的意图。我们的分析显示大语言模型在资料不足情况下能够成功地处理下游任务。GPT-3.5模型在口语孟加拉语中获得了非常出色的F1分数0.94，在插槽填充方面获得了0.51的F1分数。”
</details></li>
</ul>
<hr>
<h2 id="Spatial-HuBERT-Self-supervised-Spatial-Speech-Representation-Learning-for-a-Single-Talker-from-Multi-channel-Audio"><a href="#Spatial-HuBERT-Self-supervised-Spatial-Speech-Representation-Learning-for-a-Single-Talker-from-Multi-channel-Audio" class="headerlink" title="Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio"></a>Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10922">http://arxiv.org/abs/2310.10922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoni Dimitriadis, Siqi Pan, Vidhyasaharan Sethu, Beena Ahmed</li>
<li>for: 提高speech系统的准确率和泛化能力，通过利用无标注数据进行自动学习</li>
<li>methods: 使用多通道音频输入，实现听说者环境中的噪声和抗噪声性能</li>
<li>results: 比前一代单道音频表示模型更高效，特别在噪声和雾气环境中表现出色，同时也能够在声音地图 Task 上达到优秀的效果<details>
<summary>Abstract</summary>
Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.
</details>
<details>
<summary>摘要</summary>
自我指导学习已经用不标注数据来提高语音系统的准确性和泛化能力。虽然 latest works 尝试生成适用于多种语音频谱、语言、模态和同时说话人的有效表示，但这些研究都受到单通道音频录音的限制。本文介绍 Spatial HuBERT，一种自我指导的语音表示模型，通过多通道音频输入学习到一个说话人在听到的环境中的both acoustic和空间信息。Spatial HuBERT 的表示超过了当前最佳单通道语音表示的状态，特别是在噪音和干扰环境中。我们还证明 Spatial HuBERT 学习的表示在语音地图任务中具有 Utility。此外，我们在这篇论文中公共发布了100000个 simulated first-order ambisonics room impulse responses 的新数据集。
</details></li>
</ul>
<hr>
<h2 id="Compositional-preference-models-for-aligning-LMs"><a href="#Compositional-preference-models-for-aligning-LMs" class="headerlink" title="Compositional preference models for aligning LMs"></a>Compositional preference models for aligning LMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13011">http://arxiv.org/abs/2310.13011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Marc Dymetman</li>
<li>For: 本研究旨在提高语言模型（LM）与人类偏好的匹配。* Methods: 我们提出了 Compositional Preference Models（CPMs），一种新的偏好模型框架，它将一个全局偏好评估 decomposes 成多个可解释的特征，从提示LM中获取特征的scalar scores，并使用逻辑回归分类器进行聚合。* Results: 我们的实验表明，CPMs 不仅提高了通用性和鲁棒性，而且best-of-n 样本获得到使用 CPMs 比使用标准 PMs 更好。总的来说，我们的方法展示了将 PMs 具备人类偏好的假设，并且通过LM的能力来抽取这些特征的方法的优势。<details>
<summary>Abstract</summary>
As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using CPMs tend to be preferred over samples obtained using conventional PMs. Overall, our approach demonstrates the benefits of endowing PMs with priors about which features determine human preferences while relying on LM capabilities to extract those features in a scalable and robust way.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)随着语言模型（LM）的能力不断提高，对其进行人类喜好的调整变得越来越重要。然而，目前主流的偏好模型（PM）训练方法受到一些基本的限制，如不透明性和可扩展性，同时容易过拟合偏好数据。我们提议compositional Preference Models（CPM），一种新的PM框架，将一个全局的喜好评估 decomposes into 多个可解释的特征，从提问LM中获取这些特征的标量分数，并使用逻辑回归分类器进行聚合。CPMs允许控制偏好数据中哪些特性用于训练偏好模型，并基于人类喜好判断中认为是重要的特征来建立偏好模型。我们的实验表明，CPMs不仅提高了通用性和鲁棒性，而且best-of-n样本获得使用CPMs比使用标准PMs更受欢迎。总的来说，我们的方法表明了将PMs具备人类喜好的假设，并且利用LM的能力来提取这些特征的可行和稳定的方式。
</details></li>
</ul>
<hr>
<h2 id="Emergent-AI-Assisted-Discourse-Case-Study-of-a-Second-Language-Writer-Authoring-with-ChatGPT"><a href="#Emergent-AI-Assisted-Discourse-Case-Study-of-a-Second-Language-Writer-Authoring-with-ChatGPT" class="headerlink" title="Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT"></a>Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10903">http://arxiv.org/abs/2310.10903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharin Jacob, Tamara Tate, Mark Warschauer</li>
<li>for: 本研究探讨了ChatGPT如何促进语言学习者的学术写作，以减轻对人类写作标准的担忧。</li>
<li>methods: 本研究采用了 случа研究方法，探讨了Kailing博士在学术写作过程中使用ChatGPT的经验。研究使用了活动理论来理解使用生成AI工具进行写作，数据分析包括 semi-structured interview, writing samples和GPT logs。</li>
<li>results: 结果表明Kailing能够与ChatGPT在不同写作阶段进行有效协作，同时保持自己独特的作者语言和主动性。这表明AI工具如ChatGPT可以增强语言学习者的学术写作，而不会抹杀个体的独特性。本案例研究提供了使用ChatGPT进行学术写作的批判性探讨，以及保持学生独特语言的实践。<details>
<summary>Abstract</summary>
The rapid proliferation of ChatGPT has incited debates regarding its impact on human writing. Amid concerns about declining writing standards, this study investigates the role of ChatGPT in facilitating academic writing, especially among language learners. Using a case study approach, this study examines the experiences of Kailing, a doctoral student, who integrates ChatGPT throughout their academic writing process. The study employs activity theory as a lens for understanding writing with generative AI tools and data analyzed includes semi-structured interviews, writing samples, and GPT logs. Results indicate that Kailing effectively collaborates with ChatGPT across various writing stages while preserving her distinct authorial voice and agency. This underscores the potential of AI tools such as ChatGPT to enhance academic writing for language learners without overshadowing individual authenticity. This case study offers a critical exploration of how ChatGPT is utilized in the academic writing process and the preservation of a student's authentic voice when engaging with the tool.
</details>
<details>
<summary>摘要</summary>
快速扩散的ChatGPT已经引发了人们对人类写作的影响的讨论。本研究探究了ChatGPT如何促进语言学习者的学术写作，特别是在启用AI生成工具的情况下。通过 caso study的方式，本研究研究了Kailing，一名博士学生，在学术写作过程中如何与ChatGPT进行合作。研究使用活动理论作为写作AI工具的理解镜子，数据分析包括 semi-structured 采访、写作样本和 GPT 日志。结果表明，Kailing在不同的写作阶段与ChatGPT进行有效的合作，同时保持自己独特的作者语言和主张。这种情况 highlights  AI工具如ChatGPT可以增强语言学习者的学术写作，不会覆盖个人的 Authenticity。本案例研究如何在学术写作过程中使用ChatGPT，并保持学生的独特语言和主张。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/cs.CL_2023_10_17/" data-id="closbron700cv0g88h9c9efql" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/cs.LG_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T10:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/cs.LG_2023_10_17/">cs.LG - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Balance-Act-Mitigating-Hubness-in-Cross-Modal-Retrieval-with-Query-and-Gallery-Banks"><a href="#Balance-Act-Mitigating-Hubness-in-Cross-Modal-Retrieval-with-Query-and-Gallery-Banks" class="headerlink" title="Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks"></a>Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11612">http://arxiv.org/abs/2310.11612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval">https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval</a></li>
<li>paper_authors: Yimu Wang, Xiangru Jian, Bo Xue</li>
<li>for: 本研究旨在解决跨Modal Retrieval中的干扰问题，即 gallery数据点频繁出现在检索结果中，导致检索性能下降。</li>
<li>methods: 我们提出了一种新的框架，双银行正常化（DBNorm），以及两种新的方法，对比轮径正常化和动态对比轮径正常化，以正常化相似度基于两个银行。这些方法可以减少极值点和查询样本之间的相似度，提高非极值点和查询样本之间的相似度。</li>
<li>results: 我们在多种语言基础 benchmark上进行了广泛的实验，包括文本-图像、文本-视频和文本-音频 benchmark，并证明了我们的方法可以比前方法更好地解决干扰问题，提高检索性能。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval上下载。</a><details>
<summary>Abstract</summary>
In this work, we present a post-processing solution to address the hubness problem in cross-modal retrieval, a phenomenon where a small number of gallery data points are frequently retrieved, resulting in a decline in retrieval performance. We first theoretically demonstrate the necessity of incorporating both the gallery and query data for addressing hubness as hubs always exhibit high similarity with gallery and query data. Second, building on our theoretical results, we propose a novel framework, Dual Bank Normalization (DBNorm). While previous work has attempted to alleviate hubness by only utilizing the query samples, DBNorm leverages two banks constructed from the query and gallery samples to reduce the occurrence of hubs during inference. Next, to complement DBNorm, we introduce two novel methods, dual inverted softmax and dual dynamic inverted softmax, for normalizing similarity based on the two banks. Specifically, our proposed methods reduce the similarity between hubs and queries while improving the similarity between non-hubs and queries. Finally, we present extensive experimental results on diverse language-grounded benchmarks, including text-image, text-video, and text-audio, demonstrating the superior performance of our approaches compared to previous methods in addressing hubness and boosting retrieval performance. Our code is available at https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种后处理解决方案，用于解决跨Modal重现中的枢轴问题，即一些小量的 галеリー数据点频繁地被重现，导致检索性能下降。我们首先理论上证明了在 incorporating both gallery和query数据时，解决枢轴问题的必要性，因为枢轴总是与 gallery和query数据 exhibit high similarity。然后，基于我们的理论结果，我们提出了一种新的框架，双银行Normalization（DBNorm）。在前一些工作中，人们尝试了通过只使用查询样本来缓解枢轴，但DBNorm利用了基于查询和 галеリー样本构建的两个银行来减少在推理中出现的枢轴。此外，为了补充DBNorm，我们提出了两种新的方法，双 inverted softmax和 dual dynamic inverted softmax，用于在两个银行基础上Normalize similarity。specifically，我们的提出的方法可以降低枢轴和查询之间的相似性，而提高非枢轴和查询之间的相似性。最后，我们在多种语言基础 benchmark上进行了广泛的实验，包括文本-图像、文本-视频和文本-声音，并证明了我们的方法比前一些方法在解决枢轴和提高检索性能方面表现更出色。我们的代码可以在 https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval 上获取。
</details></li>
</ul>
<hr>
<h2 id="In-defense-of-parameter-sharing-for-model-compression"><a href="#In-defense-of-parameter-sharing-for-model-compression" class="headerlink" title="In defense of parameter sharing for model-compression"></a>In defense of parameter sharing for model-compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11611">http://arxiv.org/abs/2310.11611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Desai, Anshumali Shrivastava</li>
<li>for: 这篇论文主要目的是对模型减小内存占用的方法进行全面评估，并推广RPS方法在模型压缩中的应用。</li>
<li>methods: 本论文使用了RPS方法、截割技术和建立更小的模型来减少模型的内存占用。</li>
<li>results: 研究发现，RPS方法在压缩范围内 consistently 击败&#x2F;匹配更小的模型和一些中等知识水平的截割策略，特别在高压缩场景下。此外，RPS方法还有较高的鲁棒性和稳定性。<details>
<summary>Abstract</summary>
When considering a model architecture, there are several ways to reduce its memory footprint. Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning. More recently, randomized parameter-sharing (RPS) methods have gained traction for model compression at start of training. In this paper, we comprehensively assess the trade-off between memory and accuracy across RPS, pruning techniques, and building smaller models. Our findings demonstrate that RPS, which is both data and model-agnostic, consistently outperforms/matches smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range. This advantage becomes particularly pronounced in higher compression scenarios. Notably, even when compared to highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS exhibits superior performance in high compression settings. This points out inherent capacity advantage that RPS enjoys over sparse models. Theoretically, we establish RPS as a superior technique in terms of memory-efficient representation when compared to pruning for linear models. This paper argues in favor of paradigm shift towards RPS based models. During our rigorous evaluation of RPS, we identified issues in the state-of-the-art RPS technique ROAST, specifically regarding stability (ROAST's sensitivity to initialization hyperparameters, often leading to divergence) and Pareto-continuity (ROAST's inability to recover the accuracy of the original model at zero compression). We provably address both of these issues. We refer to the modified RPS, which incorporates our improvements, as STABLE-RPS.
</details>
<details>
<summary>摘要</summary>
当考虑模型建 architecture时，有几种方法可以降低其内存占用量。历史上，流行的方法包括选择更小的 architecture和通过剪裁来减少模型的大小。在这篇论文中，我们系统地评估了减少内存和精度之间的权衡，并对 RPS、剪裁技术和建立更小的模型进行了比较。我们的发现表明，RPS在整个压缩范围内一直表现出优异，特别是在更高的压缩场景下。此外，RPS还比所有中等知识的剪裁策略（如MAG、SNIP、SYNFLOW和GRASP）在整个压缩范围内表现出优异。这种优势在高压缩场景下特别明显。在高压缩场景下，RPS甚至超过了高知识剪裁策略（如Lottery Ticket Rewinding）的性能。这表明RPS在压缩场景下具有内存效率的优势。从理论角度来看，我们证明RPS在线性模型上是一种更佳的压缩技术。这篇论文提倡使用基于RPS的模型。在我们对RPS进行了严格的评估后，我们发现了一些问题，包括ROAST的稳定性和紧张性（ROAST的初始化参数的敏感性，常导致偏转）以及级联稳定性（ROAST无法在零压缩场景下恢复原始模型的精度）。我们解决了这些问题，并提出了一种改进后的RPS，称为稳定RPS（STABLE-RPS）。
</details></li>
</ul>
<hr>
<h2 id="Reflection-Equivariant-Diffusion-for-3D-Structure-Determination-from-Isotopologue-Rotational-Spectra-in-Natural-Abundance"><a href="#Reflection-Equivariant-Diffusion-for-3D-Structure-Determination-from-Isotopologue-Rotational-Spectra-in-Natural-Abundance" class="headerlink" title="Reflection-Equivariant Diffusion for 3D Structure Determination from Isotopologue Rotational Spectra in Natural Abundance"></a>Reflection-Equivariant Diffusion for 3D Structure Determination from Isotopologue Rotational Spectra in Natural Abundance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11609">http://arxiv.org/abs/2310.11609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aspuru-guzik-group/kreed">https://github.com/aspuru-guzik-group/kreed</a></li>
<li>paper_authors: Austin Cheng, Alston Lo, Santiago Miret, Brooks Pate, Alán Aspuru-Guzik<br>for: 这篇论文的目的是用翻译光谱来确定小分子有机物的三维结构。methods: 这篇论文使用了翻译光谱来获得小分子有机物的精准三维信息，并使用 Краичман分析来确定氢原子的取代坐标。results: 这篇论文开发了一种基于生成 diffusion 模型，可以从分子式、翻译光谱和重元素取代坐标来推断小分子有机物的完整三维结构。这种方法的顶尖预测结果可以在 QM9 和 GEOM 数据集上达到 &gt;98% 的准确率。<details>
<summary>Abstract</summary>
Structure determination is necessary to identify unknown organic molecules, such as those in natural products, forensic samples, the interstellar medium, and laboratory syntheses. Rotational spectroscopy enables structure determination by providing accurate 3D information about small organic molecules via their moments of inertia. Using these moments, Kraitchman analysis determines isotopic substitution coordinates, which are the unsigned $|x|,|y|,|z|$ coordinates of all atoms with natural isotopic abundance, including carbon, nitrogen, and oxygen. While unsigned substitution coordinates can verify guesses of structures, the missing $+/-$ signs make it challenging to determine the actual structure from the substitution coordinates alone. To tackle this inverse problem, we develop KREED (Kraitchman REflection-Equivariant Diffusion), a generative diffusion model that infers a molecule's complete 3D structure from its molecular formula, moments of inertia, and unsigned substitution coordinates of heavy atoms. KREED's top-1 predictions identify the correct 3D structure with >98% accuracy on the QM9 and GEOM datasets when provided with substitution coordinates of all heavy atoms with natural isotopic abundance. When substitution coordinates are restricted to only a subset of carbons, accuracy is retained at 91% on QM9 and 32% on GEOM. On a test set of experimentally measured substitution coordinates gathered from the literature, KREED predicts the correct all-atom 3D structure in 25 of 33 cases, demonstrating experimental applicability for context-free 3D structure determination with rotational spectroscopy.
</details>
<details>
<summary>摘要</summary>
STRUCTURE determination 是必需的，以确定未知的有机分子，如自然产物、刑事样本、 междузвездmedium 和实验室合成。扭转 спектроскопия 可以提供有机分子的准确三维信息，通过其惯性矩来确定结构。使用这些矩，卡迪曼分析可以确定原子的替换坐标，即所有原子的自然同位素含量，包括碳、氮和氧。然而，未签名的替换坐标无法决定实际结构，这是一个逆向问题。为解决这个问题，我们开发了 KREED（卡迪曼反射相对均匀扩散），一种生成扩散模型，可以从分子式、惯性矩和未签名重元素替换坐标中推断出分子的完整三维结构。KREED 的顶峰预测可以在 QM9 和 GEOM 数据集上确定分子的正确三维结构，并且在所有重元素替换坐标中具有 >98% 的准确率。当替换坐标只限于一 subset of 碳时，准确率仍保持在 91% 的水平。在Literature中测试的实验ally measured substitution coordinates中，KREED 预测了正确的所有atom 3D结构，证明了实验可行性。
</details></li>
</ul>
<hr>
<h2 id="TK-KNN-A-Balanced-Distance-Based-Pseudo-Labeling-Approach-for-Semi-Supervised-Intent-Classification"><a href="#TK-KNN-A-Balanced-Distance-Based-Pseudo-Labeling-Approach-for-Semi-Supervised-Intent-Classification" class="headerlink" title="TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification"></a>TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11607">http://arxiv.org/abs/2310.11607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/servicenow/tk-knn">https://github.com/servicenow/tk-knn</a></li>
<li>paper_authors: Nicholas Botzer, David Vasquez, Tim Weninger, Issam Laradji</li>
<li>For: The paper is written for improving the ability to detect intent in dialogue systems, specifically by using semi-supervised learning methods to label unlabeled data.* Methods: The paper proposes a new method called Top-K K-Nearest Neighbor (TK-KNN) that uses a more robust pseudo-labeling approach based on distance in the embedding space, while maintaining a balanced set of pseudo-labeled examples across classes through a ranking-based approach.* Results: The experiments on several datasets show that TK-KNN outperforms existing models, particularly when labeled data is scarce, such as on popular datasets like CLINC150 and Banking77.Here are the three key points in Simplified Chinese text:* For: 这篇论文是为了提高对话系统中的意图检测能力，特别是使用半监督学习方法来标注无标签数据。* Methods: 论文提出了一种新的方法called Top-K K-Nearest Neighbor (TK-KNN)，它使用了更加可靠的 pseudo-labeling 方法基于 embedding 空间的距离，同时保持了类别之间的 pseudo-标签例子具有平衡的分布。* Results: 实验结果表明，TK-KNN 在几个 dataset 上表现出色，特别是在标注数据scarce的情况下，如 CLINC150 和 Banking77 等 популяр的 dataset 上。<details>
<summary>Abstract</summary>
The ability to detect intent in dialogue systems has become increasingly important in modern technology. These systems often generate a large amount of unlabeled data, and manually labeling this data requires substantial human effort. Semi-supervised methods attempt to remedy this cost by using a model trained on a few labeled examples and then by assigning pseudo-labels to further a subset of unlabeled examples that has a model prediction confidence higher than a certain threshold. However, one particularly perilous consequence of these methods is the risk of picking an imbalanced set of examples across classes, which could lead to poor labels. In the present work, we describe Top-K K-Nearest Neighbor (TK-KNN), which uses a more robust pseudo-labeling approach based on distance in the embedding space while maintaining a balanced set of pseudo-labeled examples across classes through a ranking-based approach. Experiments on several datasets show that TK-KNN outperforms existing models, particularly when labeled data is scarce on popular datasets such as CLINC150 and Banking77. Code is available at https://github.com/ServiceNow/tk-knn
</details>
<details>
<summary>摘要</summary>
现代技术中探测对话系统中的意图已经变得越来越重要。这些系统经常生成大量未标注数据，并且手动标注这些数据需要很大的人工劳动。半超vised方法试图解决这个问题，使用一些标注的示例来训练模型，然后将 pseudo-标签分配给一部分未标注示例，这些示例的模型预测度高于某个阈值。然而，这些方法存在一个特别危险的后果，即选择类别之间不均衡的示例集，这可能导致 poor labels。在 presente 的工作中，我们描述了 Top-K K-Nearest Neighbor (TK-KNN)，这是一种基于距离 embedding 空间的更加可靠的 pseudo-标签方法，同时保持类别之间的 pseudo-标签示例集均衡。在几个数据集上进行了实验，发现 TK-KNN 超越了现有模型，特别是在 CLINC150 和 Banking77 等Popular数据集上。代码可以在 https://github.com/ServiceNow/tk-knn 上获取。
</details></li>
</ul>
<hr>
<h2 id="Towards-Inferring-Users’-Impressions-of-Robot-Performance-in-Navigation-Scenarios"><a href="#Towards-Inferring-Users’-Impressions-of-Robot-Performance-in-Navigation-Scenarios" class="headerlink" title="Towards Inferring Users’ Impressions of Robot Performance in Navigation Scenarios"></a>Towards Inferring Users’ Impressions of Robot Performance in Navigation Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11590">http://arxiv.org/abs/2310.11590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiping Zhang, Nathan Tsoi, Booyeon Choi, Jie Tan, Hao-Tien Lewis Chiang, Marynel Vázquez</li>
<li>for: 这个论文的目的是研究使用非语言行为特征和机器学习技术来预测人们对机器人表现的印象。</li>
<li>methods: 作者提供了一个名为 SEAN TOGETHER 的数据集，包含人与移动机器人在虚拟现实环境中的互动记录，以及用户对机器人表现的评分。同时，作者还进行了人类和超参量学习模型如何预测人们对机器人表现的印象的分析。</li>
<li>results: 研究发现，人脸表情 alone 可以提供有用的信息关于人们对机器人表现的印象，但在我们测试的导航场景中，空间特征是最critical的信息 для这种推断任务。此外，当评分为二分类而不是多类时，人类预测和机器学习模型的 F1 score 更 чем doublies，表明它们都更好地预测机器人表现的方向性而不是具体评分。<details>
<summary>Abstract</summary>
Human impressions of robot performance are often measured through surveys. As a more scalable and cost-effective alternative, we study the possibility of predicting people's impressions of robot behavior using non-verbal behavioral cues and machine learning techniques. To this end, we first contribute the SEAN TOGETHER Dataset consisting of observations of an interaction between a person and a mobile robot in a Virtual Reality simulation, together with impressions of robot performance provided by users on a 5-point scale. Second, we contribute analyses of how well humans and supervised learning techniques can predict perceived robot performance based on different combinations of observation types (e.g., facial, spatial, and map features). Our results show that facial expressions alone provide useful information about human impressions of robot performance; but in the navigation scenarios we tested, spatial features are the most critical piece of information for this inference task. Also, when evaluating results as binary classification (rather than multiclass classification), the F1-Score of human predictions and machine learning models more than doubles, showing that both are better at telling the directionality of robot performance than predicting exact performance ratings. Based on our findings, we provide guidelines for implementing these predictions models in real-world navigation scenarios.
</details>
<details>
<summary>摘要</summary>
人类对机器人性能的印象通常通过调查来衡量。作为一种可扩展和成本效果更高的替代方案，我们研究使用非语言行为特征和机器学习技术预测人类对机器人行为的印象。为此，我们首先提供了SEAN TOGETHER数据集，包括人与移动机器人在虚拟现实环境中的互动记录，以及用户对机器人性能的评分（在5分比例上）。其次，我们分析了人类和监督学习技术如何预测人类对机器人性能的印象，根据不同的观察类型（例如，表情特征、空间特征和地图特征）。我们的结果显示，表情特征alone提供了人类对机器人性能的有用信息；但在我们测试的导航场景中，空间特征是最重要的信息来源。此外，当评估结果为二分类（而不是多类）时，人类预测和机器学习模型的F1分值超过了两倍，表示它们都更好地预测机器人性能的方向性，而不是精确的评分。根据我们的发现，我们提供了实现这些预测模型的指南，用于实际导航场景。
</details></li>
</ul>
<hr>
<h2 id="Partially-Observable-Stochastic-Games-with-Neural-Perception-Mechanisms"><a href="#Partially-Observable-Stochastic-Games-with-Neural-Perception-Mechanisms" class="headerlink" title="Partially Observable Stochastic Games with Neural Perception Mechanisms"></a>Partially Observable Stochastic Games with Neural Perception Mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11566">http://arxiv.org/abs/2310.11566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Yan, Gabriel Santos, Gethin Norman, David Parker, Marta Kwiatkowska</li>
<li>for:  This paper is written for researchers and practitioners interested in multi-agent decision-making under uncertainty, with a focus on partial observability and data-driven perception.</li>
<li>methods: The paper proposes a new model called neuro-symbolic partially-observable stochastic games (NS-POSGs), which incorporates perception mechanisms and is applicable to one-sided settings with discrete, data-driven observations. The paper also introduces a new point-based method called one-sided NS-HSVI for approximating values of NS-POSGs.</li>
<li>results: The paper presents experimental results demonstrating the practical applicability of the proposed method for neural networks whose preimage is in polyhedral form. The results show that the one-sided NS-HSVI method is effective in approximating values of NS-POSGs and can be used to solve real-world problems involving partial observability and data-driven perception.<details>
<summary>Abstract</summary>
Stochastic games are a well established model for multi-agent sequential decision making under uncertainty. In reality, though, agents have only partial observability of their environment, which makes the problem computationally challenging, even in the single-agent setting of partially observable Markov decision processes. Furthermore, in practice, agents increasingly perceive their environment using data-driven approaches such as neural networks trained on continuous data. To tackle this problem, we propose the model of neuro-symbolic partially-observable stochastic games (NS-POSGs), a variant of continuous-space concurrent stochastic games that explicitly incorporates perception mechanisms. We focus on a one-sided setting, comprising a partially-informed agent with discrete, data-driven observations and a fully-informed agent with continuous observations. We present a new point-based method, called one-sided NS-HSVI, for approximating values of one-sided NS-POSGs and implement it based on the popular particle-based beliefs, showing that it has closed forms for computing values of interest. We provide experimental results to demonstrate the practical applicability of our method for neural networks whose preimage is in polyhedral form.
</details>
<details>
<summary>摘要</summary>
We focus on a one-sided setting where the partially-informed agent has discrete, data-driven observations, while the fully-informed agent has continuous observations. We develop a new point-based method called one-sided NS-HSVI for approximating values of one-sided NS-POSGs, which is based on the popular particle-based beliefs. Our method has closed forms for computing values of interest, and we provide experimental results to demonstrate its practical applicability for neural networks whose preimage is in polyhedral form.
</details></li>
</ul>
<hr>
<h2 id="Online-Algorithms-with-Uncertainty-Quantified-Predictions"><a href="#Online-Algorithms-with-Uncertainty-Quantified-Predictions" class="headerlink" title="Online Algorithms with Uncertainty-Quantified Predictions"></a>Online Algorithms with Uncertainty-Quantified Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11558">http://arxiv.org/abs/2310.11558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Sun, Jerry Huang, Nicolas Christianson, Mohammad Hajiesmaili, Adam Wierman</li>
<li>for: This paper focuses on developing online algorithms that incorporate uncertainty-quantified predictions to achieve high-quality performance guarantees while maintaining bounded worst-case guarantees.</li>
<li>methods: The paper explores the use of uncertainty-quantified predictions in online algorithms, specifically for ski rental and online search problems. The authors propose non-trivial modifications to algorithm design to fully leverage the probabilistic predictions.</li>
<li>results: The paper demonstrates the effectiveness of the proposed methods through theoretical analysis and experimental evaluations. The results show that the algorithms achieve better performance guarantees compared to traditional online algorithms, and the uncertainty-quantified predictions provide valuable information for making optimal decisions in multi-instance settings.<details>
<summary>Abstract</summary>
Online algorithms with predictions have become a trending topic in the field of beyond worst-case analysis of algorithms. These algorithms incorporate predictions about the future to obtain performance guarantees that are of high quality when the predictions are good, while still maintaining bounded worst-case guarantees when predictions are arbitrarily poor. In general, the algorithm is assumed to be unaware of the prediction's quality. However, recent developments in the machine learning literature have studied techniques for providing uncertainty quantification on machine-learned predictions, which describes how certain a model is about its quality. This paper examines the question of how to optimally utilize uncertainty-quantified predictions in the design of online algorithms. In particular, we consider predictions augmented with uncertainty quantification describing the likelihood of the ground truth falling in a certain range, designing online algorithms with these probabilistic predictions for two classic online problems: ski rental and online search. In each case, we demonstrate that non-trivial modifications to algorithm design are needed to fully leverage the probabilistic predictions. Moreover, we consider how to utilize more general forms of uncertainty quantification, proposing a framework based on online learning that learns to exploit uncertainty quantification to make optimal decisions in multi-instance settings.
</details>
<details>
<summary>摘要</summary>
在 beyond worst-case 分析算法领域，在线算法 Predictions 已经成为一个流行的话题。这些算法利用未来的预测来获得高质量的性能保证，当预测准确度很好时，而且仍保持 bounded worst-case 保证，当预测准确度很差时。在总的来说，算法假设不知道预测的质量。然而，现代机器学习文献中的技术已经研究了提供机器学习预测的不确定性评估，这种评估描述了模型对其质量的确定程度。本文考虑了如何优化不确定性评估的预测，并在两个经典的在线问题上进行了实践：滑雪租赁和在线搜索。在每个情况下，我们表明了非常轻量级的修改，以便完全利用预测的不确定性。此外，我们考虑了如何利用更加通用的不确定性评估，提出了基于在线学习的框架，用于在多实例设置中学习利用不确定性评估来做出优化的决策。
</details></li>
</ul>
<hr>
<h2 id="Bias-and-Error-Mitigation-in-Software-Generated-Data-An-Advanced-Search-and-Optimization-Framework-Leveraging-Generative-Code-Models"><a href="#Bias-and-Error-Mitigation-in-Software-Generated-Data-An-Advanced-Search-and-Optimization-Framework-Leveraging-Generative-Code-Models" class="headerlink" title="Bias and Error Mitigation in Software-Generated Data: An Advanced Search and Optimization Framework Leveraging Generative Code Models"></a>Bias and Error Mitigation in Software-Generated Data: An Advanced Search and Optimization Framework Leveraging Generative Code Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11546">http://arxiv.org/abs/2310.11546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernesto Giralt Hernández</li>
<li>for:  corrected errors and biases in software systems specializing in data analysis and generation</li>
<li>methods: Solomonoff Induction, Kolmogorov Conditional Complexity, generative models ( LLMS)</li>
<li>results: incrementally improve the quality of output results<details>
<summary>Abstract</summary>
Data generation and analysis is a fundamental aspect of many industries and disciplines, from strategic decision making in business to research in the physical and social sciences. However, data generated using software and algorithms can be subject to biases and errors. These can be due to problems with the original software, default settings that do not align with the specific needs of the situation, or even deeper problems with the underlying theories and models. This paper proposes an advanced search and optimization framework aimed at generating and choosing optimal source code capable of correcting errors and biases from previous versions to address typical problems in software systems specializing in data analysis and generation, especially those in the corporate and data science world. Applying this framework multiple times on the same software system would incrementally improve the quality of the output results. It uses Solomonoff Induction as a sound theoretical basis, extending it with Kolmogorov Conditional Complexity, a novel adaptation, to evaluate a set of candidate programs. We propose the use of generative models for the creation of this set of programs, with special emphasis on the capabilities of Large Language Models (LLMs) to generate high quality code.
</details>
<details>
<summary>摘要</summary>
“数据生成和分析是许多行业和领域的基础方面，从商业战略决策到物理和社会科学研究。但是，由软件和算法生成的数据可能受到偏见和错误的影响。这些问题可能来自原始软件的问题、不适应特定情况的默认设置或更深层次的理论和模型问题。这篇论文提出了一种高级搜索和优化框架，用于生成和选择修正过去版本中的错误和偏见的最佳源代码。通过多次应用这种框架于同一个软件系统，可以逐步提高输出结果的质量。它基于索löмо夫推理为基础，并将其扩展到科尔莫果ров conditional complexity，一种新的适应，以评估候选程序集。我们建议使用生成模型来创建这些候选程序集，尤其是利用大型自然语言模型（LLMs）生成高质量代码。”
</details></li>
</ul>
<hr>
<h2 id="Thin-and-Deep-Gaussian-Processes"><a href="#Thin-and-Deep-Gaussian-Processes" class="headerlink" title="Thin and Deep Gaussian Processes"></a>Thin and Deep Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11527">http://arxiv.org/abs/2310.11527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spectraldani/thindeepgps">https://github.com/spectraldani/thindeepgps</a></li>
<li>paper_authors: Daniel Augusto de Souza, Alexander Nikitin, ST John, Magnus Ross, Mauricio A. Álvarez, Marc Peter Deisenroth, João P. P. Gomes, Diego Mesquita, César Lincoln C. Mattos</li>
<li>for: 这个论文的目的是提出一种新的深度 Gaussian process（TDGP）模型，以解决深度 Gaussian process（DGP）模型中的一些问题，如敏感性和解释性的缺失。</li>
<li>methods: TDGP 模型使用了successively parameterizing kernels with Gaussian process layers，这种方法可以学习输入数据的低维度表示，同时保持 kernel 的 interpretable 性。</li>
<li>results: 研究发现，TDGP 模型可以在标准的 benchmark 数据集上表现出色，并且可以适应增加层数的情况。此外，TDGP 模型可以学习低维度表示，并且不会出现特定的PATHOLOGIES。<details>
<summary>Abstract</summary>
Gaussian processes (GPs) can provide a principled approach to uncertainty quantification with easy-to-interpret kernel hyperparameters, such as the lengthscale, which controls the correlation distance of function values. However, selecting an appropriate kernel can be challenging. Deep GPs avoid manual kernel engineering by successively parameterizing kernels with GP layers, allowing them to learn low-dimensional embeddings of the inputs that explain the output data. Following the architecture of deep neural networks, the most common deep GPs warp the input space layer-by-layer but lose all the interpretability of shallow GPs. An alternative construction is to successively parameterize the lengthscale of a kernel, improving the interpretability but ultimately giving away the notion of learning lower-dimensional embeddings. Unfortunately, both methods are susceptible to particular pathologies which may hinder fitting and limit their interpretability. This work proposes a novel synthesis of both previous approaches: Thin and Deep GP (TDGP). Each TDGP layer defines locally linear transformations of the original input data maintaining the concept of latent embeddings while also retaining the interpretation of lengthscales of a kernel. Moreover, unlike the prior solutions, TDGP induces non-pathological manifolds that admit learning lower-dimensional representations. We show with theoretical and experimental results that i) TDGP is, unlike previous models, tailored to specifically discover lower-dimensional manifolds in the input data, ii) TDGP behaves well when increasing the number of layers, and iii) TDGP performs well in standard benchmark datasets.
</details>
<details>
<summary>摘要</summary>
traducción al chino simplificado:Gaussian processes (GPs) 可以提供一个原理性的方法来评估uncertainty量化，通过容易理解的kernel参数，如lengthscale，控制函数值之间的相关程度。然而，选择合适的kernel可以是困难的。深度GPs 可以通过 successively parameterizing kernels with GP layers 来避免手动kernel工程，从而学习输入数据的低维表示。然而，这些方法通常会失去 shallow GPs 中的解释性。一种alternative construction是 successively parameterize the lengthscale of a kernel，以提高解释性，但是 ultimately give up the notion of learning lower-dimensional embeddings。fortunately, both methods are susceptible to particular pathologies which may hinder fitting and limit their interpretability. This work proposes a novel synthesis of both previous approaches: Thin and Deep GP (TDGP). Each TDGP layer defines locally linear transformations of the original input data maintaining the concept of latent embeddings while also retaining the interpretation of lengthscales of a kernel. Moreover, unlike the prior solutions, TDGP induces non-pathological manifolds that admit learning lower-dimensional representations. We show with theoretical and experimental results that i) TDGP is, unlike previous models, tailored to specifically discover lower-dimensional manifolds in the input data, ii) TDGP behaves well when increasing the number of layers, and iii) TDGP performs well in standard benchmark datasets.
</details></li>
</ul>
<hr>
<h2 id="Value-Biased-Maximum-Likelihood-Estimation-for-Model-based-Reinforcement-Learning-in-Discounted-Linear-MDPs"><a href="#Value-Biased-Maximum-Likelihood-Estimation-for-Model-based-Reinforcement-Learning-in-Discounted-Linear-MDPs" class="headerlink" title="Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs"></a>Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11515">http://arxiv.org/abs/2310.11515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Heng Hung, Ping-Chun Hsieh, Akshay Mete, P. R. Kumar</li>
<li>for:  linear Markov Decision Processes (MDPs) with infinite horizon and linearly parameterized transition probabilities</li>
<li>methods: Value-Biased Maximum Likelihood Estimation (VBMLE)</li>
<li>results: $\widetilde{O}(d\sqrt{T})$ regret, computationally more efficient than existing regression-based approaches, and a generic convergence result of MLE in linear MDPs through a novel supermartingale construct.Here’s the Chinese translation of the three points:</li>
<li>for:  linear Markov 决策过程 (MDPs)  WITH infinite horizon 和 linearly parameterized transition probabilities</li>
<li>methods: Value-Biased Maximum Likelihood Estimation (VBMLE)</li>
<li>results: $\widetilde{O}(d\sqrt{T})$ regret, computationally more efficient than existing regression-based approaches, AND a generic convergence result of MLE in linear MDPs through a novel supermartingale construct.<details>
<summary>Abstract</summary>
We consider the infinite-horizon linear Markov Decision Processes (MDPs), where the transition probabilities of the dynamic model can be linearly parameterized with the help of a predefined low-dimensional feature mapping. While the existing regression-based approaches have been theoretically shown to achieve nearly-optimal regret, they are computationally rather inefficient due to the need for a large number of optimization runs in each time step, especially when the state and action spaces are large. To address this issue, we propose to solve linear MDPs through the lens of Value-Biased Maximum Likelihood Estimation (VBMLE), which is a classic model-based exploration principle in the adaptive control literature for resolving the well-known closed-loop identification problem of Maximum Likelihood Estimation. We formally show that (i) VBMLE enjoys $\widetilde{O}(d\sqrt{T})$ regret, where $T$ is the time horizon and $d$ is the dimension of the model parameter, and (ii) VBMLE is computationally more efficient as it only requires solving one optimization problem in each time step. In our regret analysis, we offer a generic convergence result of MLE in linear MDPs through a novel supermartingale construct and uncover an interesting connection between linear MDPs and online learning, which could be of independent interest. Finally, the simulation results show that VBMLE significantly outperforms the benchmark method in terms of both empirical regret and computation time.
</details>
<details>
<summary>摘要</summary>
我们考虑无穷远线性Markov决策过程（MDP），其过程概率转移可线性参数化通过一个固定的低维度特征映射。现有的回归方法有理论上可达到近似优劣 regret，但 computationally 较为慢，特别是当状态和动作空间较大时。为解决这个问题，我们提议通过Value-Biased Maximum Likelihood Estimation（VBMLE）解决linear MDPs，VBMLE 是适应控制文献中的一种经典的模型基于探索原理，用于解决Maximum Likelihood Estimation 的关闭loop标定问题。我们正式表明VBMLE 具有 $\widetilde{O}(d\sqrt{T})$ regret，其中 $T$ 是时间悬度，$d$ 是模型参数的维度，并且VBMLE  computationally 更高效，只需在每个时间步骤中解决一个优化问题。在我们的 regret 分析中，我们提供了线性 MDPs 的MLE 的普适减少结果，并发现了线性 MDPs 与在线学习之间的有趣连接，这可能是独立的兴趣。最后，实验结果显示VBMLE 在 empirical regret 和计算时间上明显超过参考方法。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Quantum-Sampling-for-Non-Logconcave-Distributions-and-Estimating-Partition-Functions"><a href="#Stochastic-Quantum-Sampling-for-Non-Logconcave-Distributions-and-Estimating-Partition-Functions" class="headerlink" title="Stochastic Quantum Sampling for Non-Logconcave Distributions and Estimating Partition Functions"></a>Stochastic Quantum Sampling for Non-Logconcave Distributions and Estimating Partition Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11445">http://arxiv.org/abs/2310.11445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guneykan Ozgul, Xiantao Li, Mehrdad Mahdavi, Chunhao Wang</li>
<li>for: 这个论文目标是设计一种量子算法来采样非几何均匀概率分布 $\pi(x) \propto \exp(-\beta f(x))$.</li>
<li>methods: 这个方法基于量子模拟热化算法，使用慢变化的马尔可夫链，并使用小批量的梯度诊断来实现量子步进。</li>
<li>results: 这个量子算法在维度和精度两个方面表现出了幂等速度的优化，比最佳known的类型算法更快。<details>
<summary>Abstract</summary>
We present quantum algorithms for sampling from non-logconcave probability distributions in the form of $\pi(x) \propto \exp(-\beta f(x))$. Here, $f$ can be written as a finite sum $f(x):= \frac{1}{N}\sum_{k=1}^N f_k(x)$. Our approach is based on quantum simulated annealing on slowly varying Markov chains derived from unadjusted Langevin algorithms, removing the necessity for function evaluations which can be computationally expensive for large data sets in mixture modeling and multi-stable systems. We also incorporate a stochastic gradient oracle that implements the quantum walk operators inexactly by only using mini-batch gradients. As a result, our stochastic gradient based algorithm only accesses small subsets of data points in implementing the quantum walk. One challenge of quantizing the resulting Markov chains is that they do not satisfy the detailed balance condition in general. Consequently, the mixing time of the algorithm cannot be expressed in terms of the spectral gap of the transition density, making the quantum algorithms nontrivial to analyze. To overcome these challenges, we first build a hypothetical Markov chain that is reversible, and also converges to the target distribution. Then, we quantified the distance between our algorithm's output and the target distribution by using this hypothetical chain as a bridge to establish the total complexity. Our quantum algorithms exhibit polynomial speedups in terms of both dimension and precision dependencies when compared to the best-known classical algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出了量子算法用于采样非几何均勋分布，其形式为 $\pi(x) \propto \exp(-\beta f(x))$。其中，$f$ 可以写作 finite sum $f(x):= \frac{1}{N}\sum_{k=1}^N f_k(x)$。我们的方法基于量子模拟热化法，使用慢变化的马尔可夫链，从无调整的勒文算法中 derivation，从而消除了计算成本较高的函数评估，特别是在混合模型和多稳定系统中。我们还使用 Stochastic gradient oracle，通过使用小批量评估来实现量子步进 operator。因此，我们的 Stochastic gradient 基本算法只需访问小 subsets of data points，实现量子步进。一个挑战是量化得到的马尔可夫链不满足细化平衡条件，因此我们无法通过spectral gap 来衡量混合时间。为了突破这些挑战，我们首先建立一个假的马尔可夫链，该链是可逆的，并且 converge 到目标分布。然后，我们使用这个假链作为桥，来衡量我们算法的输出和目标分布之间的距离。我们的量子算法在维度和精度上都 exhibit 对比 classical algorithms 的多项式减速。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Interpretable-Visual-Features-in-Artificial-and-Biological-Neural-Systems"><a href="#Identifying-Interpretable-Visual-Features-in-Artificial-and-Biological-Neural-Systems" class="headerlink" title="Identifying Interpretable Visual Features in Artificial and Biological Neural Systems"></a>Identifying Interpretable Visual Features in Artificial and Biological Neural Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11431">http://arxiv.org/abs/2310.11431</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Klindt, Sophia Sanborn, Francisco Acosta, Frédéric Poitevin, Nina Miolane</li>
<li>for: 这个论文的目的是为了探讨深度神经网络中单个神经元的可解释性，以及是否存在多个无关的特征在同一个神经元中的表现。</li>
<li>methods: 这篇论文使用了一种自动化的可解释性量化方法，该方法基于大量的人类 psychophysics 判断神经元可解释性的数据库，并且还提出了一种在网络活动空间找到有意义的方向的方法。</li>
<li>results: 研究发现，使用这种方法可以在卷积神经网络中找到更加直观的方向，这些方向不同于单个神经元的表现。此外，研究还应用了这种方法于三个最近发表的视觉神经响应数据集，并发现结论大致传递到实际神经数据中，这建议superposition可能被脑部实现。这也提出了关于稳定、高效和分解表示的基本问题，并且与分解有关。<details>
<summary>Abstract</summary>
Single neurons in neural networks are often interpretable in that they represent individual, intuitively meaningful features. However, many neurons exhibit $\textit{mixed selectivity}$, i.e., they represent multiple unrelated features. A recent hypothesis proposes that features in deep networks may be represented in $\textit{superposition}$, i.e., on non-orthogonal axes by multiple neurons, since the number of possible interpretable features in natural data is generally larger than the number of neurons in a given network. Accordingly, we should be able to find meaningful directions in activation space that are not aligned with individual neurons. Here, we propose (1) an automated method for quantifying visual interpretability that is validated against a large database of human psychophysics judgments of neuron interpretability, and (2) an approach for finding meaningful directions in network activation space. We leverage these methods to discover directions in convolutional neural networks that are more intuitively meaningful than individual neurons, as we confirm and investigate in a series of analyses. Moreover, we apply the same method to three recent datasets of visual neural responses in the brain and find that our conclusions largely transfer to real neural data, suggesting that superposition might be deployed by the brain. This also provides a link with disentanglement and raises fundamental questions about robust, efficient and factorized representations in both artificial and biological neural systems.
</details>
<details>
<summary>摘要</summary>
单一神经元在神经网络中经常是可解释的，它们表示单一、直觉的特征。然而，许多神经元会表现出混合选择性，即它们表示多个无关的特征。一个最近的假设提出了，内部特征在深度网络中可能会被表示为组合，即在非正交的轴上由多个神经元表示。由于自然数据中的可解释特征的数量通常大于给定网络中的神经元数量，因此我们应该能够在网络启动空间中找到有意义的方向。我们提出了以下两个方法来进行这些研究：1. 一个自动化的方法来评估视觉可解释性，该方法被验证了一个大量的人类心理学评价神经元可解释性的数据库。2. 一种方法来在网络启动空间中找到有意义的方向，这些方法可以在实际的神经网络中发现更直觉的方向。我们运用这些方法发现，对于某些问题，深度网络中的活动空间中的方向可能更直觉、更有意义，并且在实际的神经网络中发现了这些方向。此外，我们将这些方法应用到了三个最近的视觉神经反应数据中，发现结果大多转移到了实际的神经资料中，这表明了组合可能被脑部使用。这还提供了与分离开来的连结，并提出了基本问题，例如如何实现可靠、高效和分离的表示在人工和生物神经系统中。
</details></li>
</ul>
<hr>
<h2 id="Butterfly-Effects-of-SGD-Noise-Error-Amplification-in-Behavior-Cloning-and-Autoregression"><a href="#Butterfly-Effects-of-SGD-Noise-Error-Amplification-in-Behavior-Cloning-and-Autoregression" class="headerlink" title="Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression"></a>Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11428">http://arxiv.org/abs/2310.11428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Block, Dylan J. Foster, Akshay Krishnamurthy, Max Simchowitz, Cyril Zhang</li>
<li>for: This paper studies the issue of training instability in behavior cloning with deep neural networks, specifically the sharp oscillations in long-horizon rewards that can occur during training.</li>
<li>methods: The authors use minibatch SGD updates to the policy network during training, and empirically disentangle the statistical and computational causes of the oscillations. They also test several standard mitigation techniques and find an exponential moving average (EMA) of iterates to be effective in alleviating the issue.</li>
<li>results: The authors show that GVA is a common phenomenon in both continuous control and autoregressive language generation, and that EMA can effectively mitigate it. They also provide theoretical vignettes to explain the benefits of EMA in alleviating GVA and shed light on the extent to which classical convex models can help in understanding the benefits of iterate averaging in deep learning.<details>
<summary>Abstract</summary>
This work studies training instabilities of behavior cloning with deep neural networks. We observe that minibatch SGD updates to the policy network during training result in sharp oscillations in long-horizon rewards, despite negligibly affecting the behavior cloning loss. We empirically disentangle the statistical and computational causes of these oscillations, and find them to stem from the chaotic propagation of minibatch SGD noise through unstable closed-loop dynamics. While SGD noise is benign in the single-step action prediction objective, it results in catastrophic error accumulation over long horizons, an effect we term gradient variance amplification (GVA). We show that many standard mitigation techniques do not alleviate GVA, but find an exponential moving average (EMA) of iterates to be surprisingly effective at doing so. We illustrate the generality of this phenomenon by showing the existence of GVA and its amelioration by EMA in both continuous control and autoregressive language generation. Finally, we provide theoretical vignettes that highlight the benefits of EMA in alleviating GVA and shed light on the extent to which classical convex models can help in understanding the benefits of iterate averaging in deep learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Group-blind-optimal-transport-to-group-parity-and-its-constrained-variants"><a href="#Group-blind-optimal-transport-to-group-parity-and-its-constrained-variants" class="headerlink" title="Group-blind optimal transport to group parity and its constrained variants"></a>Group-blind optimal transport to group parity and its constrained variants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11407">http://arxiv.org/abs/2310.11407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quan Zhou, Jakub Marecek</li>
<li>for: 这个论文的目的是为了实现不受人类特征（如性别、种族）影响的机器学习模型。</li>
<li>methods: 这个论文使用了一种单组盲投影 map，将源数据中两个组的特征分布都 align，实现（人口）组均点，无需个体样本中的敏感属性值。</li>
<li>results: 作者通过使用真实数据和 sintetic数据进行数值实验，证明了这种方法可以实现不受敏感属性影响的机器学习模型。<details>
<summary>Abstract</summary>
Fairness holds a pivotal role in the realm of machine learning, particularly when it comes to addressing groups categorised by sensitive attributes, e.g., gender, race. Prevailing algorithms in fair learning predominantly hinge on accessibility or estimations of these sensitive attributes, at least in the training process. We design a single group-blind projection map that aligns the feature distributions of both groups in the source data, achieving (demographic) group parity, without requiring values of the protected attribute for individual samples in the computation of the map, as well as its use. Instead, our approach utilises the feature distributions of the privileged and unprivileged groups in a boarder population and the essential assumption that the source data are unbiased representation of the population. We present numerical results on synthetic data and real data.
</details>
<details>
<summary>摘要</summary>
“公平在机器学习中扮演着关键角色，特别是在面临敏感属性分类的群体时。现有的 Fair learning 算法主要基于敏感属性的访问或估计，至少在训练过程中。我们设计了一个单一群体盲目投影Map，使源数据中两个群体的特征分布相互对齐，实现群体平均性，无需个别样本中的敏感属性值，也无需在计算投影Map时和其使用过程中使用敏感属性值。而是我们的方法基于优先群体和受难群体在更大的人口中的特征分布，以及假设源数据是人口的不偏 representations。我们在synthetic数据和实际数据上提供数字结果。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Group-Fairness-in-Online-Settings-Using-Oblique-Decision-Forests"><a href="#Enhancing-Group-Fairness-in-Online-Settings-Using-Oblique-Decision-Forests" class="headerlink" title="Enhancing Group Fairness in Online Settings Using Oblique Decision Forests"></a>Enhancing Group Fairness in Online Settings Using Oblique Decision Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11401">http://arxiv.org/abs/2310.11401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Somnath Basu Roy Chowdhury, Nicholas Monath, Ahmad Beirami, Rahul Kidambi, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi</li>
<li>for: 这篇论文的目的是提出一种在线上环境中实现公平的机器学习系统，以确保不同群体之间的公平。</li>
<li>methods: 这篇论文提出了一个名为Aranyani的ensemble of oblique decision trees的方法，可以在线上环境中实现公平的决策。Aranyani使用了树结构，允许在决策时计算公平度量，并且可以快速计算公平度量，不需要额外的储存和前向&#x2F;后向通过。</li>
<li>results: 这篇论文的实验结果显示，Aranyani可以在5个公开的benchmark上 achieves a better accuracy-fairness trade-off compared to baseline approaches。<details>
<summary>Abstract</summary>
Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of oblique decision trees, to make fair decisions in online settings. The hierarchical tree structure of Aranyani enables parameter isolation and allows us to efficiently compute the fairness gradients using aggregate statistics of previous decisions, eliminating the need for additional storage and forward/backward passes. We also present an efficient framework to train Aranyani and theoretically analyze several of its properties. We conduct empirical evaluations on 5 publicly available benchmarks (including vision and language datasets) to show that Aranyani achieves a better accuracy-fairness trade-off compared to baseline approaches.
</details>
<details>
<summary>摘要</summary>
“公平性，特别是群体公平性，在机器学习系统中是一个重要考虑因素。通常运用的群体公平化技术是在训练过程中使用混合物的公平目标（例如人口平衡）和任务特定目标（例如十字项目）。但在线上数据来临时，实现这些公平目标是有挑战的。具体来说，群体公平目标是根据不同群体的预期预测结果定义的。在线上设置中，algorithm只有单独的实例，估计群体公平目标需要额外的存储和更多的计算（例如前向/后向通过）。在这篇论文中，我们提出Aranyani，一个以梯形树为基础的混合决策树，以确保在线上设置中做出公平的决策。Aranyani的树状架构允许参数隔离和通过先前的决策统计资料来计算公平的梯度，无需额外的存储和前向/后向通过。我们还提供了一个有效的训练框架和理论分析多个性能。我们在5个公开可用的benchmark（包括视觉和语言dataset）进行实验评估，发现Aranyani在精度-公平性贡献中比基准方法更好。”
</details></li>
</ul>
<hr>
<h2 id="Last-One-Standing-A-Comparative-Analysis-of-Security-and-Privacy-of-Soft-Prompt-Tuning-LoRA-and-In-Context-Learning"><a href="#Last-One-Standing-A-Comparative-Analysis-of-Security-and-Privacy-of-Soft-Prompt-Tuning-LoRA-and-In-Context-Learning" class="headerlink" title="Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning"></a>Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11397">http://arxiv.org/abs/2310.11397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem</li>
<li>for: 这篇论文旨在探讨大语言模型（LLM）适用私有数据时的隐私和安全问题。</li>
<li>methods: 论文使用了三种已知技术来适应LLM：Low-Rank Adaptation（LoRA）、Soft Prompt Tuning（SPT）和In-Context Learning（ICL）。</li>
<li>results: 研究发现，无一种适合所有隐私和安全需求的适应技术，每种技术都有不同的优劣点。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences. However, to achieve optimal performance, LLMs often require adaptation with private data, which poses privacy and security challenges. Several techniques have been proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA), Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative privacy and security properties have not been systematically investigated. In this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security). Our results show that there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="VaR-and-CVaR-Estimation-in-a-Markov-Cost-Process-Lower-and-Upper-Bounds"><a href="#VaR-and-CVaR-Estimation-in-a-Markov-Cost-Process-Lower-and-Upper-Bounds" class="headerlink" title="VaR\ and CVaR Estimation in a Markov Cost Process: Lower and Upper Bounds"></a>VaR\ and CVaR Estimation in a Markov Cost Process: Lower and Upper Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11389">http://arxiv.org/abs/2310.11389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjay Bhat, Prashanth L. A., Gugan Thoppe</li>
<li>for: 这篇论文旨在研究Markov过程中的 Value-at-Risk（VaR）和Conditional Value-at-Risk（CVaR）估计问题。</li>
<li>methods: 该论文首先 derivates a minimax下界为Ω(1&#x2F;√n)，这个下界在预期和 probabilistic上都成立。然后，使用finite-horizon truncation scheme， derivates an upper bound for CVaR estimation error, which matches the lower bound up to constant factors。</li>
<li>results: 该论文的结果表明，在Markovian设定下，our estimation scheme can provide lower and upper bounds on the estimation error for any risk measure, including spectral risk measures and utility-based shortfall risk. Our lower bounds also extend to the infinite-horizon discounted costs’ mean, and improve upon the existing result Ω(1&#x2F;n) [13].<details>
<summary>Abstract</summary>
We tackle the problem of estimating the Value-at-Risk (VaR) and the Conditional Value-at-Risk (CVaR) of the infinite-horizon discounted cost within a Markov cost process. First, we derive a minimax lower bound of $\Omega(1/\sqrt{n})$ that holds both in an expected and in a probabilistic sense. Then, using a finite-horizon truncation scheme, we derive an upper bound for the error in CVaR estimation, which matches our lower bound up to constant factors. Finally, we discuss an extension of our estimation scheme that covers more general risk measures satisfying a certain continuity criterion, e.g., spectral risk measures, utility-based shortfall risk. To the best of our knowledge, our work is the first to provide lower and upper bounds on the estimation error for any risk measure within Markovian settings. We remark that our lower bounds also extend to the infinite-horizon discounted costs' mean. Even in that case, our result $\Omega(1/\sqrt{n}) $ improves upon the existing result $\Omega(1/n)$[13].
</details>
<details>
<summary>摘要</summary>
我们研究了在马尔可夫过程中估计值风险（VaR）和条件值风险（CVaR）的问题。首先，我们 deriv了一个最小最大下界为 $\Omega(1/\sqrt{n})$，这个下界在预期上和概率上都成立。然后，使用一种 finite-horizon  truncation scheme，我们 deriv了 CVaR 估计错误的Upper bound，与我们的下界几乎相同。最后，我们讨论了我们的估计方案的扩展，覆盖更加一般的风险度量，如спектраль风险度量和utilities-based shortfall风险。据我们所知，我们的工作是在马尔可夫 Setting 中提供了任何风险度量的下界和上界。我们的下界还扩展到了无限期折抵费用的mean。甚至在那种情况下，我们的结果 $\Omega(1/\sqrt{n})$ 超越了现有的结果 $\Omega(1/n)$ [13].
</details></li>
</ul>
<hr>
<h2 id="Faster-Algorithms-for-Generalized-Mean-Densest-Subgraph-Problem"><a href="#Faster-Algorithms-for-Generalized-Mean-Densest-Subgraph-Problem" class="headerlink" title="Faster Algorithms for Generalized Mean Densest Subgraph Problem"></a>Faster Algorithms for Generalized Mean Densest Subgraph Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11377">http://arxiv.org/abs/2310.11377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenglin Fan, Ping Li, Hanyu Peng</li>
<li>for: 本研究的目的是解决 $p$-mean densest subgraph问题，即寻找一个 graphs 中的最密集子图，其中 $p $ 是一个非负整数，表示使用 $p $-th 次幂度来衡量子图的密度。</li>
<li>methods: 本研究使用了一种新的通用抽屉算法（GENPEEL），以及一种更快的通用抽屉算法（GENPEEL++），它们可以在 $p \geq 1 $ 时间复杂度为 $O(mn)$，在 $p \in [1, +\infty)$ 时间复杂度为 $O(m(\log n))$，并且具有 $(p+1)^{1&#x2F;p}$ 和 $(2(p+1))^{1&#x2F;p}$ 的近似比率，分别适用于不同的 $p $ 值。</li>
<li>results: 本研究发现，对于 $0 &lt; p &lt; 1 $，标准抽屉算法可以提供 $2^{1&#x2F;p}$ 的近似比率，而不是预期的 $p $ 次幂度。此外，GENPEEL 和 GENPEEL++ 算法可以在 $p \geq 1 $ 和 $p \in [1, +\infty)$ 中提供 $(p+1)^{1&#x2F;p}$ 和 $(2(p+1))^{1&#x2F;p}$ 的近似比率，分别适用于不同的 $p $ 值。<details>
<summary>Abstract</summary>
The densest subgraph of a large graph usually refers to some subgraph with the highest average degree, which has been extended to the family of $p$-means dense subgraph objectives by~\citet{veldt2021generalized}. The $p$-mean densest subgraph problem seeks a subgraph with the highest average $p$-th-power degree, whereas the standard densest subgraph problem seeks a subgraph with a simple highest average degree. It was shown that the standard peeling algorithm can perform arbitrarily poorly on generalized objective when $p>1$ but uncertain when $0<p<1$. In this paper, we are the first to show that a standard peeling algorithm can still yield $2^{1/p}$-approximation for the case $0<p < 1$. (Veldt 2021) proposed a new generalized peeling algorithm (GENPEEL), which for $p \geq 1$ has an approximation guarantee ratio $(p+1)^{1/p}$, and time complexity $O(mn)$, where $m$ and $n$ denote the number of edges and nodes in graph respectively. In terms of algorithmic contributions, we propose a new and faster generalized peeling algorithm (called GENPEEL++ in this paper), which for $p \in [1, +\infty)$ has an approximation guarantee ratio $(2(p+1))^{1/p}$, and time complexity $O(m(\log n))$, where $m$ and $n$ denote the number of edges and nodes in graph, respectively. This approximation ratio converges to 1 as $p \rightarrow \infty$.
</details>
<details>
<summary>摘要</summary>
通常情况下，最密集子图（dense subgraph）指的是一个具有最高平均度的子图。这个概念在$p$-means dense subgraph目标家族中被推广，其中$p$-mean densest subgraph问题 seek一个具有最高$p$-th-power度的子图。与标准的最密集子图问题不同的是，后者仅寻找一个简单的最高平均度的子图。当$p>1$时，标准的剥离算法可能会表现出现 arbitrarily poor performance，而当$0<p<1$时，则 uncertain。在这篇论文中，我们是第一个证明了标准剥离算法可以在$0<p<1$时 still yield $2^{1/p}$-approximation。在这篇论文中，我们提出了一个新的通用剥离算法（GENPEEL），它在$p\geq 1$时有一个 aproximation guarantee ratio $(p+1)^{1/p}$, 并且时间复杂度为$O(mn)$，其中$m$和$n$是图中边和节点的数量。在算法方面，我们提出了一个新的通用剥离算法（GENPEEL++），它在$p\in [1,+\infty)$时有一个 aproximation guarantee ratio $(2(p+1))^{1/p}$, 并且时间复杂度为$O(m(\log n))$,其中$m$和$n$是图中边和节点的数量。这个approximation ratio随着$p$的增长而 convergence to 1。
</details></li>
</ul>
<hr>
<h2 id="Lie-Group-Decompositions-for-Equivariant-Neural-Networks"><a href="#Lie-Group-Decompositions-for-Equivariant-Neural-Networks" class="headerlink" title="Lie Group Decompositions for Equivariant Neural Networks"></a>Lie Group Decompositions for Equivariant Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11366">http://arxiv.org/abs/2310.11366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mircea Mironenco, Patrick Forré</li>
<li>For: 这个论文的目标是构建具有对称性和同态性的神经网络模型，特别在数据端不充足的情况下。* Methods: 这个论文使用了利群的 Lie 代数和群 exponential 和 logarithm 函数来扩展传统的对称性模型，并使用了 Lie 群的结构和几何来实现对称集合的归一化和全局均衡。* Results: 这个论文通过对比各种已有的对称模型和卷积神经网络模型，证明了其在标准对称检测任务上的性能优越性和对于不同的输入数据的泛化能力。<details>
<summary>Abstract</summary>
Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest $G$, the exponential map may not be surjective. Further limitations are encountered when $G$ is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups $G = \text{GL}^{+}(n, \mathbb{R})$ and $G = \text{SL}(n, \mathbb{R})$, as well as their representation as affine transformations $\mathbb{R}^{n} \rtimes G$. Invariant integration as well as a global parametrization is realized by decomposing the `larger` groups into subgroups and submanifolds which can be handled individually. Under this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations. We evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals.
</details>
<details>
<summary>摘要</summary>
固有和等变征对几何变换有利，特别是在数据缺乏时。许多研究都集中在 компакт或很小的symmetry group上。现在的工作探索了使用Lie group的方法，包括Lie algebra、组 exponential和logarithm maps。然而，这些方法的应用 scope limited by the fact that the exponential map may not be surjective, and further limitations are encountered when the group of interest $G$ is neither compact nor abelian.我们使用 Lie group的结构和几何特性，提出一个框架，可以让我们在 $G = \text{GL}^{+}(n, \mathbb{R})$ 和 $G = \text{SL}(n, \mathbb{R})$ 上工作，以及它们的表示为抽象变换 $\mathbb{R}^{n} \rtimes G$。我们可以通过将这些 '大' 组织分解成子组织和子抽象变换，并将它们处理一个一个来实现不变 интеграл和全局参数化。在这个框架下，我们可以设计卷积核心来构建对抽象变换具有不变性的模型。我们在标准对称变换分类任务上评估了我们的模型的稳定性和 OUT-OF-distribution泛化能力，并超越了所有均衡变换模型以及所有卷积网络提议。
</details></li>
</ul>
<hr>
<h2 id="Contextualized-Machine-Learning"><a href="#Contextualized-Machine-Learning" class="headerlink" title="Contextualized Machine Learning"></a>Contextualized Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11340">http://arxiv.org/abs/2310.11340</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SAP/contextual-ai">https://github.com/SAP/contextual-ai</a></li>
<li>paper_authors: Benjamin Lengerich, Caleb N. Ellington, Andrea Rubbi, Manolis Kellis, Eric P. Xing</li>
<li>for: 这篇论文旨在探讨Contextualized Machine Learning（ML），一种学习不同和上下文相关的效应的方法。</li>
<li>methods: 该方法使用深度学习来模型meta-关系，将上下文信息翻译成模型参数，实现不同参数的变化。</li>
<li>results: 该方法可以汇集不同的框架，包括带环境分析和年龄模型，并且可以实现非Parametric推断和模型识别性条件。最后，作者提供了一个开源的PyTorch包ContextualizedML。<details>
<summary>Abstract</summary>
We examine Contextualized Machine Learning (ML), a paradigm for learning heterogeneous and context-dependent effects. Contextualized ML estimates heterogeneous functions by applying deep learning to the meta-relationship between contextual information and context-specific parametric models. This is a form of varying-coefficient modeling that unifies existing frameworks including cluster analysis and cohort modeling by introducing two reusable concepts: a context encoder which translates sample context into model parameters, and sample-specific model which operates on sample predictors. We review the process of developing contextualized models, nonparametric inference from contextualized models, and identifiability conditions of contextualized models. Finally, we present the open-source PyTorch package ContextualizedML.
</details>
<details>
<summary>摘要</summary>
我们研究Contextualized Machine Learning（ML），一种学习不同和上下文依赖的效果的方法。Contextualized ML使用深度学习来模型meta关系，即样本上下文信息和样本特定的参数模型之间的关系。这是一种 varying-coefficient modeling，可以统一现有的框架，包括集群分析和团队模型，通过引入两个可重用概念：样本上下文编码器，将样本上下文转换为模型参数，以及样本特定的模型，对样本预测变量进行操作。我们详细介绍了Contextualized模型的开发、非参数推断、和Contextualized模型的可识别条件。最后，我们发布了一个开源的PyTorch包，即ContextualizedML。
</details></li>
</ul>
<hr>
<h2 id="Non-ergodicity-in-reinforcement-learning-robustness-via-ergodicity-transformations"><a href="#Non-ergodicity-in-reinforcement-learning-robustness-via-ergodicity-transformations" class="headerlink" title="Non-ergodicity in reinforcement learning: robustness via ergodicity transformations"></a>Non-ergodicity in reinforcement learning: robustness via ergodicity transformations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11335">http://arxiv.org/abs/2310.11335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Baumann, Erfaun Noorani, James Price, Ole Peters, Colm Connaughton, Thomas B. Schön</li>
<li>for: This paper aims to address the issue of non-robustness in reinforcement learning (RL) algorithms, specifically in real-world applications such as autonomous driving, precision agriculture, and finance.</li>
<li>methods: The authors propose a new algorithm for learning ergodicity transformations from data, which enables the optimization of long-term return for individual agents rather than the average across infinitely many trajectories.</li>
<li>results: The proposed algorithm is demonstrated to be effective in an instructive, non-ergodic environment and on standard RL benchmarks.Here’s the simplified Chinese text:</li>
<li>for: 本研究旨在解决反对式学习（RL）算法在实际应用中的不稳定性问题，特别是在自动驾驶、精准农业和金融等领域。</li>
<li>methods: 作者提出了一种基于数据学习Ergodicity变换的算法，以便优化个体代理的长期返回，而不是权衡无穷多个轨迹的平均返回。</li>
<li>results: 提出的算法在一个教育性的非ergodic环境以及标准RL benchmark上得到了证明。<details>
<summary>Abstract</summary>
Envisioned application areas for reinforcement learning (RL) include autonomous driving, precision agriculture, and finance, which all require RL agents to make decisions in the real world. A significant challenge hindering the adoption of RL methods in these domains is the non-robustness of conventional algorithms. In this paper, we argue that a fundamental issue contributing to this lack of robustness lies in the focus on the expected value of the return as the sole "correct" optimization objective. The expected value is the average over the statistical ensemble of infinitely many trajectories. For non-ergodic returns, this average differs from the average over a single but infinitely long trajectory. Consequently, optimizing the expected value can lead to policies that yield exceptionally high returns with probability zero but almost surely result in catastrophic outcomes. This problem can be circumvented by transforming the time series of collected returns into one with ergodic increments. This transformation enables learning robust policies by optimizing the long-term return for individual agents rather than the average across infinitely many trajectories. We propose an algorithm for learning ergodicity transformations from data and demonstrate its effectiveness in an instructive, non-ergodic environment and on standard RL benchmarks.
</details>
<details>
<summary>摘要</summary>
拟合应用领域 для强化学习（RL）包括自动驾驶、精细农业和金融，这些领域都需要RL代理人做出实际世界中的决策。然而，现有的RL方法在这些领域的应用受到一定的阻碍。在这篇论文中，我们认为RL方法的一个基本问题在于强调预期返回值作为唯一的“正确”优化目标。预期返回值是统计ensemble中的平均值，对于非ergodic返回，这个平均值与单个但是无限长的轨迹的平均值不同。因此，优化预期返回可能导致政策产生极高的返回，但是几乎确定会导致灾难性的结果。这个问题可以通过将收集到的返回时间序列转换成一个ergodic增量来解决。这种转换允许学习 robust政策，而不是优化infinitely多轨迹的平均值。我们提出了一种从数据中学习ergodicity转换的算法，并在一个 instructive、非ergodic环境中和标准RLbenchmark上进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Elucidating-The-Design-Space-of-Classifier-Guided-Diffusion-Generation"><a href="#Elucidating-The-Design-Space-of-Classifier-Guided-Diffusion-Generation" class="headerlink" title="Elucidating The Design Space of Classifier-Guided Diffusion Generation"></a>Elucidating The Design Space of Classifier-Guided Diffusion Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11311">http://arxiv.org/abs/2310.11311</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexmaols/elucd">https://github.com/alexmaols/elucd</a></li>
<li>paper_authors: Jiajun Ma, Tianyang Hu, Wenjia Wang, Jiacheng Sun</li>
<li>for: 提高Diffusion模型的样本质量和可控性，通过主流方法和训练自由方法都需要额外的标注数据训练，而训练自由方法的性能尚未得到证明。</li>
<li>methods: 我们通过对设计空间的全面调查，发现可以通过免训练的方式，使用市场上可得的预训练分类器来提高 diffusion 模型的性能，同时兼顾主流方法和训练自由方法的优点。我们提出了几种预处理技术来更好地利用预训练分类器来导向Diffusion生成。</li>
<li>results: 我们通过对 ImageNet 进行了广泛的实验，证明了我们的提posed方法可以提高 state-of-the-art  diffusion 模型（DDPM、EDM、DiT）的性能（最多提高20%），而且几乎不需要额外的计算成本。随着可得到的预训练分类器的普及，我们的提posed方法具有极大的潜力，可以快速扩展到文本到图像生成任务。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/AlexMaOLS/EluCD/tree/main">https://github.com/AlexMaOLS/EluCD/tree/main</a> 获取。<details>
<summary>Abstract</summary>
Guidance in conditional diffusion generation is of great importance for sample quality and controllability. However, existing guidance schemes are to be desired. On one hand, mainstream methods such as classifier guidance and classifier-free guidance both require extra training with labeled data, which is time-consuming and unable to adapt to new conditions. On the other hand, training-free methods such as universal guidance, though more flexible, have yet to demonstrate comparable performance. In this work, through a comprehensive investigation into the design space, we show that it is possible to achieve significant performance improvements over existing guidance schemes by leveraging off-the-shelf classifiers in a training-free fashion, enjoying the best of both worlds. Employing calibration as a general guideline, we propose several pre-conditioning techniques to better exploit pretrained off-the-shelf classifiers for guiding diffusion generation. Extensive experiments on ImageNet validate our proposed method, showing that state-of-the-art diffusion models (DDPM, EDM, DiT) can be further improved (up to 20%) using off-the-shelf classifiers with barely any extra computational cost. With the proliferation of publicly available pretrained classifiers, our proposed approach has great potential and can be readily scaled up to text-to-image generation tasks. The code is available at https://github.com/AlexMaOLS/EluCD/tree/main.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>Diffusion模型的指导是样本质量和可控性方面的关键因素。然而，现有的指导方法尚不够。一方面，主流方法如类型器指导和类型器无指导都需要额外的训练频道标注数据，时间consuming并不适应新条件。另一方面，无需训练的方法如通用指导，虽然更灵活，尚未达到相关性表现。在这种情况下，我们通过对设计空间的全面调查，展示了可以通过利用准备好的类型器来获得显著性能提升，同时兼得到最佳的两个世界。采用准确性为总则，我们提出了一些预处理技巧来更好地利用预训练的类型器来导向扩散生成。广泛的实验 validate我们的提议方法，显示了使用预训练的类型器可以提高状态当前的扩散模型（DDPM、EDM、DiT）的性能（最高提升20%），而且几乎没有额外的计算成本。随着公共预训练类型器的普及，我们的提议方法具有巨大的潜力，可以轻松扩展到文本到图生成任务。代码可以在 GitHub 上获取：https://github.com/AlexMaOLS/EluCD/tree/main。
</details></li>
</ul>
<hr>
<h2 id="An-Automatic-Learning-Rate-Schedule-Algorithm-for-Achieving-Faster-Convergence-and-Steeper-Descent"><a href="#An-Automatic-Learning-Rate-Schedule-Algorithm-for-Achieving-Faster-Convergence-and-Steeper-Descent" class="headerlink" title="An Automatic Learning Rate Schedule Algorithm for Achieving Faster Convergence and Steeper Descent"></a>An Automatic Learning Rate Schedule Algorithm for Achieving Faster Convergence and Steeper Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11291">http://arxiv.org/abs/2310.11291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao Song, Chiwun Yang</li>
<li>for: 这个研究的目的是提高神经网络训练中的优化技术，尤其是解决 mini-batch 优化中的测验问题。</li>
<li>methods: 本研究使用了 delta-bar-delta 算法，并提出了一个新的 RDBD (Regrettable Delta-Bar-Delta) 方法来解决 convergence 问题。</li>
<li>results: 经过广泛的实验评估，RDBD 方法能够增加优化过程的速度和稳定性，并且可以与不同的优化算法整合使用。<details>
<summary>Abstract</summary>
The delta-bar-delta algorithm is recognized as a learning rate adaptation technique that enhances the convergence speed of the training process in optimization by dynamically scheduling the learning rate based on the difference between the current and previous weight updates. While this algorithm has demonstrated strong competitiveness in full data optimization when compared to other state-of-the-art algorithms like Adam and SGD, it may encounter convergence issues in mini-batch optimization scenarios due to the presence of noisy gradients.   In this study, we thoroughly investigate the convergence behavior of the delta-bar-delta algorithm in real-world neural network optimization. To address any potential convergence challenges, we propose a novel approach called RDBD (Regrettable Delta-Bar-Delta). Our approach allows for prompt correction of biased learning rate adjustments and ensures the convergence of the optimization process. Furthermore, we demonstrate that RDBD can be seamlessly integrated with any optimization algorithm and significantly improve the convergence speed.   By conducting extensive experiments and evaluations, we validate the effectiveness and efficiency of our proposed RDBD approach. The results showcase its capability to overcome convergence issues in mini-batch optimization and its potential to enhance the convergence speed of various optimization algorithms. This research contributes to the advancement of optimization techniques in neural network training, providing practitioners with a reliable automatic learning rate scheduler for achieving faster convergence and improved optimization outcomes.
</details>
<details>
<summary>摘要</summary>
delta-bar-delta 算法是一种学习率自适应技术，可以增加训练过程的速度并且在全数据优化中与其他当前标准算法如 Adam 和 SGD 进行比较。然而，在小批量优化场景下，这种算法可能会遇到收敛问题，这是因为梯度具有噪音。在这个研究中，我们对 delta-bar-delta 算法在实际神经网络优化中的收敛行为进行了全面的调查。为了解决任何可能出现的收敛挑战，我们提出了一种新的 Approach，即 RDBD（Regrettable Delta-Bar-Delta）。我们的方法可以快速更正偏导学习率调整，并确保优化过程的收敛。此外，我们证明了 RDBD 可以轻松地与任何优化算法结合使用，并显著提高优化速度。通过进行了广泛的实验和评估，我们证明了 RDBD 的效果和效率。结果表明，RDBD 可以在小批量优化中解决收敛问题，并且有可能在不同的优化算法中提高收敛速度。这项研究对神经网络训练中优化技术的进步做出了贡献，为实践者提供了一个可靠的自动学习率调整器，以实现更快的收敛和优化结果。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Impact-of-Humanitarian-Aid-on-Food-Security"><a href="#Evaluating-the-Impact-of-Humanitarian-Aid-on-Food-Security" class="headerlink" title="Evaluating the Impact of Humanitarian Aid on Food Security"></a>Evaluating the Impact of Humanitarian Aid on Food Security</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11287">http://arxiv.org/abs/2310.11287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordi Cerdà-Bautista, José María Tárraga, Vasileios Sitokonstantinou, Gustau Camps-Valls</li>
<li>for: 评估气候变化引起的干旱地区食品安全问题，需要紧急人道主义帮助。</li>
<li>methods: 该文章提出了一种 causal inference 框架，用于评估投入到食品危机中的金钱支付政策的影响。文章包括识别食品安全系统中的 causal 关系、完善全面的数据库、和估计人道援助政策对于营养不良的 causal 效应。</li>
<li>results: 研究发现，这些投入没有显著影响，可能因为样本规模太小、数据质量不佳、和 causal 图不完善，即我们对多学科系统食品安全的理解还有限。这说明需要进一步提高数据收集和精细化 causal 模型，以便更有效地进行未来的投入和政策，提高人道援助的透明度和责任感。<details>
<summary>Abstract</summary>
In the face of climate change-induced droughts, vulnerable regions encounter severe threats to food security, demanding urgent humanitarian assistance. This paper introduces a causal inference framework for the Horn of Africa, aiming to assess the impact of cash-based interventions on food crises. Our contributions encompass identifying causal relationships within the food security system, harmonizing a comprehensive database, and estimating the causal effect of humanitarian interventions on malnutrition. Our results revealed no significant effects, likely due to limited sample size, suboptimal data quality, and an imperfect causal graph resulting from our limited understanding of multidisciplinary systems like food security. This underscores the need to enhance data collection and refine causal models with domain experts for more effective future interventions and policies, improving transparency and accountability in humanitarian aid.
</details>
<details>
<summary>摘要</summary>
面对气候变化引起的干旱，抵触地区面临严重的食品安全威胁，需要急需人道主义援助。这篇论文介绍了一种 causal inference 框架，用于评估针对东非的食品危机造成的影响。我们的贡献包括确定食品安全系统中的 causal 关系，融合全面的数据库，并估算人道主义干预对营养不良的影响。我们的结果表明，没有显著的影响， probable 因为样本规模过小、数据质量不佳和我们对多学科系统的理解不够，从而导致 causal 图不准确。这反映了需要增强数据收集和改进 causal 模型，以更有效地应用未来的援助和政策，提高透明度和责任感。
</details></li>
</ul>
<hr>
<h2 id="Self-supervision-meets-kernel-graph-neural-models-From-architecture-to-augmentations"><a href="#Self-supervision-meets-kernel-graph-neural-models-From-architecture-to-augmentations" class="headerlink" title="Self-supervision meets kernel graph neural models: From architecture to augmentations"></a>Self-supervision meets kernel graph neural models: From architecture to augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11281">http://arxiv.org/abs/2310.11281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawang Dan, Ruofan Wu, Yunpeng Liu, Baokun Wang, Changhua Meng, Tengfei Liu, Tianyi Zhang, Ningtao Wang, Xing Fu, Qi Li, Weiqiang Wang</li>
<li>for: 这 paper 的目的是提高 kernel graph neural networks (KGNNs) 的设计和学习方法，以提高 graph representation learning 的效果。</li>
<li>methods: 本 paper 使用了一种更加灵活的 graph-level similarity定义，以及一种更加简洁的优化目标函数，以解决 MPNNs 中的一些挑战。此外，paper 还提出了一种新的自我监督学习方法 called latent graph augmentation (LGA)，以提高 KGNNs 的表达能力。</li>
<li>results: 实验结果表明，提出的方法可以在 graph classification 任务中达到竞争性的性能，并且在一些比较难的任务中even outperform 现有的状态态-of-the-art 方法。此外，对比其他已有的 graph data augmentation 方法，LGA augmentation scheme 能够更好地捕捉 graph-level 的 semantics。<details>
<summary>Abstract</summary>
Graph representation learning has now become the de facto standard when handling graph-structured data, with the framework of message-passing graph neural networks (MPNN) being the most prevailing algorithmic tool. Despite its popularity, the family of MPNNs suffers from several drawbacks such as transparency and expressivity. Recently, the idea of designing neural models on graphs using the theory of graph kernels has emerged as a more transparent as well as sometimes more expressive alternative to MPNNs known as kernel graph neural networks (KGNNs). Developments on KGNNs are currently a nascent field of research, leaving several challenges from algorithmic design and adaptation to other learning paradigms such as self-supervised learning. In this paper, we improve the design and learning of KGNNs. Firstly, we extend the algorithmic formulation of KGNNs by allowing a more flexible graph-level similarity definition that encompasses former proposals like random walk graph kernel, as well as providing a smoother optimization objective that alleviates the need of introducing combinatorial learning procedures. Secondly, we enhance KGNNs through the lens of self-supervision via developing a novel structure-preserving graph data augmentation method called latent graph augmentation (LGA). Finally, we perform extensive empirical evaluations to demonstrate the efficacy of our proposed mechanisms. Experimental results over benchmark datasets suggest that our proposed model achieves competitive performance that is comparable to or sometimes outperforming state-of-the-art graph representation learning frameworks with or without self-supervision on graph classification tasks. Comparisons against other previously established graph data augmentation methods verify that the proposed LGA augmentation scheme captures better semantics of graph-level invariance.
</details>
<details>
<summary>摘要</summary>
Graph表示学习现在成为了处理图结构数据的标准方法，MPNN框架是最具有影响力的算法工具。然而，MPNN家族受到一些缺点的限制，如透明度和表达能力。近些年，基于图kernels的图神经网络（KGNN）在MPNN的基础上设计图神经网络，被认为是更透明和有时更表达能力的替代方案。KGNN的发展现在是一个有前途的研究领域，还有许多挑战，如算法设计和适应其他学习模式，如无监督学习。在这篇论文中，我们提高了KGNN的设计和学习。首先，我们扩展了KGNN的算法表述，允许更flexible的图级相似性定义，包括过去的提议，如随机步行图kernels，以及提供更平滑的优化目标，以避免引入 combinatorial学习过程。其次，我们通过对KGNN进行自我监督来增强其性能，发展了一种新的结构保持graph数据增强方法，即latent graph augmentation（LGA）。最后，我们进行了广泛的实验评估，以证明我们提出的机制的有效性。实验结果表明，我们的提出的模型在图分类任务上达到了与或超过了现状标准的表现，并且在不含自我监督的情况下也能够达到类似的表现。与其他之前Established graph data增强方法进行比较，我们的LGA增强方案更好地捕捉到图级 semantics。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Sample-Better"><a href="#Learning-to-Sample-Better" class="headerlink" title="Learning to Sample Better"></a>Learning to Sample Better</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11232">http://arxiv.org/abs/2310.11232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Michael S. Albergo, Eric Vanden-Eijnden</li>
<li>for: 本文介绍了最近的生成模型方法，基于测量传输的动力学，从基本概率分布到目标概率分布的样本映射。</li>
<li>methods: 本文使用了变量学习来学习映射，并使用MC采样技术来提高采样效率。</li>
<li>results: 本文在MCMC和重要采样方面获得了改进的结果。<details>
<summary>Abstract</summary>
These lecture notes provide an introduction to recent advances in generative modeling methods based on the dynamical transportation of measures, by means of which samples from a simple base measure are mapped to samples from a target measure of interest. Special emphasis is put on the applications of these methods to Monte-Carlo (MC) sampling techniques, such as importance sampling and Markov Chain Monte-Carlo (MCMC) schemes. In this context, it is shown how the maps can be learned variationally using data generated by MC sampling, and how they can in turn be used to improve such sampling in a positive feedback loop.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这些讲义 introduce 最近的生成模型方法，基于动态传输度量，从简单的基础探障中映射到目标度量的样本。ocus 在 Monte Carlo (MC) 抽样技术上，如重要抽样和 Markov Chain Monte Carlo (MCMC) 方案。这些讲义显示了如何通过变量学习使得映射，使用 MC 抽样生成的数据，并将其用于改进抽样。
</details></li>
</ul>
<hr>
<h2 id="Zipformer-A-faster-and-better-encoder-for-automatic-speech-recognition"><a href="#Zipformer-A-faster-and-better-encoder-for-automatic-speech-recognition" class="headerlink" title="Zipformer: A faster and better encoder for automatic speech recognition"></a>Zipformer: A faster and better encoder for automatic speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11230">http://arxiv.org/abs/2310.11230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/k2-fsa/icefall">https://github.com/k2-fsa/icefall</a></li>
<li>paper_authors: Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, Yifan Yang, Zengrui Jin, Long Lin, Daniel Povey</li>
<li>for: 这个论文是为了提出一种更快速、更具有内存效率的 transformer 模型，即 Zipformer，用于自动语音识别（ASR）。</li>
<li>methods: 该模型使用了以下方法：1）U-Net-like Encoder结构，中间堆叠在较低帧率下运行; 2）重新排序块结构，增加更多模块，并在每个模块中重用注意力权重以实现更好的效率; 3）修改了 LayerNorm 为 BiasNorm，以保留一些长度信息; 4）新的激活函数 SwooshR 和 SwooshL 比 Swish 更好。</li>
<li>results: 对 LibriSpeech、Aishell-1 和 WenetSpeech 数据集进行了广泛的实验，并证明了 Zipformer 模型在其他状态码模型中表现更好。<details>
<summary>Abstract</summary>
The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer. Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster convergence and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer over other state-of-the-art ASR models. Our code is publicly available at https://github.com/k2-fsa/icefall.
</details>
<details>
<summary>摘要</summary>
《充当者》已成为自动语音识别（ASR）最受欢迎的编码器模型。它将卷积模块添加到转换器中，以学习本地和全局依赖关系。在这项工作中，我们描述了一种更快、更有效和性能更高的转换器，即Zipformer。模型变化包括：1. 中堆结构采用U-Net类型，中间堆叠运行速率较低;2. 块结构重新排序，增加更多模块，并在这些模块中重用注意力权重以实现效率;3. 使用修改后的层Normalization，称为BiasNorm，以保留一些长度信息;4. 新的激活函数SwooshR和SwooshL，比Swish更好地工作;5. 我们还提出了一种新的优化器，称为扫描Adam，它可以根据每个tensor的当前尺度缩放更新，以保持相对变化的相同程度，并且显式地学习参数尺度。它在其他状态的ASR模型比Adam更快地 converges和表现更好。我们对LibriSpeech、Aishell-1和WenetSpeech datasets进行了广泛的实验，并证明了我们提出的Zipformer在其他状态的ASR模型之上表现更好。我们的代码可以在https://github.com/k2-fsa/icefall中找到。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-with-Nonvacuous-Generalisation-Bounds"><a href="#Federated-Learning-with-Nonvacuous-Generalisation-Bounds" class="headerlink" title="Federated Learning with Nonvacuous Generalisation Bounds"></a>Federated Learning with Nonvacuous Generalisation Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11203">http://arxiv.org/abs/2310.11203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Jobic, Maxime Haddouche, Benjamin Guedj</li>
<li>for: 本研究旨在采用隐私保护技术来实现 federated learning，每个节点都保持着自己的训练数据私有，而不会泄露给其他节点。</li>
<li>methods: 本研究使用随机生成器来训练 federated learning 模型，每个节点都生成了一个本地隐私predictor，而不把训练数据分享给其他节点。然后，我们建立了一个全局的随机生成器，该随机生成器继承了本地私有predictor的性质，即PAC-Bayesian泛化 bound。</li>
<li>results: 我们通过一系列的数字实验显示，我们的方法可以与批处理方法（其中所有数据集被共享）匹配的预测性能，而不需要共享数据集。此外，我们的方法可以提供 numerically nonvacuous 泛化 bound，保护每个节点的隐私。我们计算了批处理和 federated learning 之间的增量预测性能和泛化 bound，这将为保护隐私而付出的代价。<details>
<summary>Abstract</summary>
We introduce a novel strategy to train randomised predictors in federated learning, where each node of the network aims at preserving its privacy by releasing a local predictor but keeping secret its training dataset with respect to the other nodes. We then build a global randomised predictor which inherits the properties of the local private predictors in the sense of a PAC-Bayesian generalisation bound. We consider the synchronous case where all nodes share the same training objective (derived from a generalisation bound), and the asynchronous case where each node may have its own personalised training objective. We show through a series of numerical experiments that our approach achieves a comparable predictive performance to that of the batch approach where all datasets are shared across nodes. Moreover the predictors are supported by numerically nonvacuous generalisation bounds while preserving privacy for each node. We explicitly compute the increment on predictive performance and generalisation bounds between batch and federated settings, highlighting the price to pay to preserve privacy.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的策略，用于在联合学习中训练随机预测器，每个网络节点都希望保持自己的隐私，通过发布本地预测器，而不把训练数据集与其他节点分享。然后，我们构建了一个全局随机预测器，该预测器继承了本地私有预测器的性质，即PAC-Bayesian泛化约束。我们考虑了同步和异步两种情况，在同步情况下，所有节点共享同一个训练目标（基于一个泛化约束），在异步情况下，每个节点可能有自己的个性化训练目标。我们通过一系列数值实验表示，我们的方法可以与批处理方法（所有数据集在节点间共享）相比，具有相似的预测性能，同时保持隐私性。我们显式计算了批处理和联合学习之间的增量预测性能和泛化约束，强调保护隐私的代价。
</details></li>
</ul>
<hr>
<h2 id="A-Modified-EXP3-and-Its-Adaptive-Variant-in-Adversarial-Bandits-with-Multi-User-Delayed-Feedback"><a href="#A-Modified-EXP3-and-Its-Adaptive-Variant-in-Adversarial-Bandits-with-Multi-User-Delayed-Feedback" class="headerlink" title="A Modified EXP3 and Its Adaptive Variant in Adversarial Bandits with Multi-User Delayed Feedback"></a>A Modified EXP3 and Its Adaptive Variant in Adversarial Bandits with Multi-User Delayed Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11188">http://arxiv.org/abs/2310.11188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chubbro/mud-exp3">https://github.com/chubbro/mud-exp3</a></li>
<li>paper_authors: Yandi Li, Jianxiong Guo</li>
<li>for: 本研究假设了延迟反馈问题中的多用户情况，即每个用户的反馈可能会在不同的延迟时间内提供，而这些延迟时间都是未知的。</li>
<li>methods: 我们采用了修改后EXP3算法，称之为MUD-EXP3算法，它在每个轮次中基于不同用户的重要性权重来做决策。</li>
<li>results: 我们证明了在知道终点轮次索引$T$的情况下，我们的算法的违和为$\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$。此外，我们还提出了一种适应算法 named AMUD-EXP3，它在不知道$T$的情况下可以实现SUBLINEAR的违和。最后，我们进行了广泛的实验来证明我们的算法的正确性和有效性。<details>
<summary>Abstract</summary>
For the adversarial multi-armed bandit problem with delayed feedback, we consider that the delayed feedback results are from multiple users and are unrestricted on internal distribution. As the player picks an arm, feedback from multiple users may not be received instantly yet after an arbitrary delay of time which is unknown to the player in advance. For different users in a round, the delays in feedback have no latent correlation. Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users. On the premise of known terminal round index $T$, the number of users $M$, the number of arms $N$, and upper bound of delay $d_{max}$, we prove a regret of $\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$. Furthermore, for the more common case of unknown $T$, an adaptive algorithm named AMUD-EXP3 is proposed with a sublinear regret with respect to $T$. Finally, extensive experiments are conducted to indicate the correctness and effectiveness of our algorithms.
</details>
<details>
<summary>摘要</summary>
For the adversarial multi-armed bandit problem with delayed feedback, we consider that the delayed feedback results are from multiple users and are unrestricted on internal distribution. As the player picks an arm, feedback from multiple users may not be received instantly yet after an arbitrary delay of time which is unknown to the player in advance. For different users in a round, the delays in feedback have no latent correlation. Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users. On the premise of known terminal round index $T$, the number of users $M$, the number of arms $N$, and upper bound of delay $d_{max}$, we prove a regret of $\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$. Furthermore, for the more common case of unknown $T$, an adaptive algorithm named AMUD-EXP3 is proposed with a sublinear regret with respect to $T$. Finally, extensive experiments are conducted to indicate the correctness and effectiveness of our algorithms.Here is the translation in Traditional Chinese:For the adversarial multi-armed bandit problem with delayed feedback, we consider that the delayed feedback results are from multiple users and are unrestricted on internal distribution. As the player picks an arm, feedback from multiple users may not be received instantly yet after an arbitrary delay of time which is unknown to the player in advance. For different users in a round, the delays in feedback have no latent correlation. Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users. On the premise of known terminal round index $T$, the number of users $M$, the number of arms $N$, and upper bound of delay $d_{max}$, we prove a regret of $\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$. Furthermore, for the more common case of unknown $T$, an adaptive algorithm named AMUD-EXP3 is proposed with a sublinear regret with respect to $T$. Finally, extensive experiments are conducted to indicate the correctness and effectiveness of our algorithms.
</details></li>
</ul>
<hr>
<h2 id="Efficiently-Visualizing-Large-Graphs"><a href="#Efficiently-Visualizing-Large-Graphs" class="headerlink" title="Efficiently Visualizing Large Graphs"></a>Efficiently Visualizing Large Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11186">http://arxiv.org/abs/2310.11186</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/charlie-xiao/embedding-visualization-test">https://github.com/charlie-xiao/embedding-visualization-test</a></li>
<li>paper_authors: Xinyu Li, Yao Xiao, Yuchen Zhou</li>
<li>for: 这个论文旨在提出一种基于维度减少的图像化方法，用于可读性地显示图的结构。</li>
<li>methods: 该方法基于t-SNE算法，但是它采用了邻域结构来降低时间复杂度，从而支持更大的图。此外，该方法还结合了laplacian eigenmaps和最短路算法，以获得高维度的图嵌入。</li>
<li>results: 通过使用这种方法，可以在5分钟内图像化300K个节点和1M个边的图，并且可以达到约10%的视觉质量提升。代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/Charlie-XIAO/embedding-visualization-test%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Charlie-XIAO/embedding-visualization-test上获取。</a><details>
<summary>Abstract</summary>
Most existing graph visualization methods based on dimension reduction are limited to relatively small graphs due to performance issues. In this work, we propose a novel dimension reduction method for graph visualization, called t-Distributed Stochastic Graph Neighbor Embedding (t-SGNE). t-SGNE is specifically designed to visualize cluster structures in the graph. As a variant of the standard t-SNE method, t-SGNE avoids the time-consuming computations of pairwise similarity. Instead, it uses the neighbor structures of the graph to reduce the time complexity from quadratic to linear, thus supporting larger graphs. In addition, to suit t-SGNE, we combined Laplacian Eigenmaps with the shortest path algorithm in graphs to form the graph embedding algorithm ShortestPath Laplacian Eigenmaps Embedding (SPLEE). Performing SPLEE to obtain a high-dimensional embedding of the large-scale graph and then using t-SGNE to reduce its dimension for visualization, we are able to visualize graphs with up to 300K nodes and 1M edges within 5 minutes and achieve approximately 10% improvement in visualization quality. Codes and data are available at https://github.com/Charlie-XIAO/embedding-visualization-test.
</details>
<details>
<summary>摘要</summary>
现有的图视化方法基于维度减少通常只能处理相对较小的图，由于性能问题。在这个工作中，我们提出了一种新的维度减少方法 для图视化，即t-Distributed Stochastic Graph Neighbor Embedding（t-SGNE）。t-SGNE专门用于描述图中的集群结构。作为标准t-SNE方法的变体，t-SGNE避免了对对应之间的相似性进行时间消耗的计算，而是使用图中的邻居结构来降低时间复杂度从quadratico至线性，因此可以支持更大的图。此外，为了适应t-SGNE，我们将Laplacian Eigenmaps与图中最短路算法组合成为图嵌入算法ShortestPath Laplacian Eigenmaps Embedding（SPLEE）。通过对大规模图进行SPLEE嵌入，并使用t-SGNE减少其维度进行视化，我们可以在5分钟内视化300K个节点和1M个边的图，并达到约10%的视化质量提升。代码和数据可以在https://github.com/Charlie-XIAO/embedding-visualization-test中找到。
</details></li>
</ul>
<hr>
<h2 id="Serenade-A-Model-for-Human-in-the-loop-Automatic-Chord-Estimation"><a href="#Serenade-A-Model-for-Human-in-the-loop-Automatic-Chord-Estimation" class="headerlink" title="Serenade: A Model for Human-in-the-loop Automatic Chord Estimation"></a>Serenade: A Model for Human-in-the-loop Automatic Chord Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11165">http://arxiv.org/abs/2310.11165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hendrik Vincent Koops, Gianluca Micchi, Ilaria Manco, Elio Quinton</li>
<li>for: 这 paper 的目的是提高 Musical Instrument Digital Interface (MIDI) 任务中的自动分 segmentation、 corpus analysis 和自动和声标注 estimation 的精度。</li>
<li>methods: 这 paper 使用了一种新的人类和自动推理模型共同创建和声标注的方法，其中人类在自动生成和声预测时提供精度优化的稀疏标注，而模型则根据人类指导进行修正。</li>
<li>results: 这 paper 在一个流行音乐数据集上进行了评估，并显示了人类和模型共同创建和声标注的方法可以提高和声分析性能，并且人类的贡献被模型的第二次、受限预测所强调。<details>
<summary>Abstract</summary>
Computational harmony analysis is important for MIR tasks such as automatic segmentation, corpus analysis and automatic chord label estimation. However, recent research into the ambiguous nature of musical harmony, causing limited inter-rater agreement, has made apparent that there is a glass ceiling for common metrics such as accuracy. Commonly, these issues are addressed either in the training data itself by creating majority-rule annotations or during the training phase by learning soft targets. We propose a novel alternative approach in which a human and an autoregressive model together co-create a harmonic annotation for an audio track. After automatically generating harmony predictions, a human sparsely annotates parts with low model confidence and the model then adjusts its predictions following human guidance. We evaluate our model on a dataset of popular music and we show that, with this human-in-the-loop approach, harmonic analysis performance improves over a model-only approach. The human contribution is amplified by the second, constrained prediction of the model.
</details>
<details>
<summary>摘要</summary>
计算音乐和谐分析对音乐信息 Retrieval（MIR）任务如自动分割、文献分析和自动和声标注有着重要的作用。然而，最近关于音乐和谐的抽象性的研究，使得限制了通用指标的准确率。通常，这些问题通过创建多数规则约束或在训练阶段学习软目标来解决。我们提出了一种新的人机合作方法，在音频轨道上，人类和自动推理模型共同创建和谐标注。首先，模型自动生成和谐预测，然后人类精选部分低度信任的部分并让模型根据人类指导更新预测。我们对流行音乐数据集进行评估，并显示了人机共同Loop Approach可以提高和谐分析性能，人类贡献被模型第二次预测所增强。
</details></li>
</ul>
<hr>
<h2 id="A-new-high-resolution-indoor-radon-map-for-Germany-using-a-machine-learning-based-probabilistic-exposure-model"><a href="#A-new-high-resolution-indoor-radon-map-for-Germany-using-a-machine-learning-based-probabilistic-exposure-model" class="headerlink" title="A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model"></a>A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11143">http://arxiv.org/abs/2310.11143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Petermann, Peter Bossew, Joachim Kemski, Valeria Gruber, Nils Suhr, Bernd Hoffmann</li>
<li>for: 这个研究是为了更正确地估计室内Radon浓度分布，并且实现高空间分辨率的推估。</li>
<li>methods: 这篇研究使用了一个两阶段的模型方法，包括一个量iles regression forest以环境和建筑资料作为预测器，以及一个可靠的Monte Carlo抽样法来组合和人口权重平均。</li>
<li>results: 研究结果显示，室内Radon浓度的算术平均值为63 Bq&#x2F;m3，几何平均值为41 Bq&#x2F;m3，95 %ile值为180 Bq&#x2F;m3。对100 Bq&#x2F;m3和300 Bq&#x2F;m3的过衡 probabilities是12.5 % (10.5 million people)和2.2 % (1.9 million people)，分别。在大城市地区，个人室内Radon曝露较在乡村地区低，这是因为人口分布不同。<details>
<summary>Abstract</summary>
Radon is a carcinogenic, radioactive gas that can accumulate indoors. Indoor radon exposure at the national scale is usually estimated on the basis of extensive measurement campaigns. However, characteristics of the sample often differ from the characteristics of the population due to the large number of relevant factors such as the availability of geogenic radon or floor level. Furthermore, the sample size usually does not allow exposure estimation with high spatial resolution. We propose a model-based approach that allows a more realistic estimation of indoor radon distribution with a higher spatial resolution than a purely data-based approach. We applied a two-stage modelling approach: 1) a quantile regression forest using environmental and building data as predictors was applied to estimate the probability distribution function of indoor radon for each floor level of each residential building in Germany; (2) a probabilistic Monte Carlo sampling technique enabled the combination and population weighting of floor-level predictions. In this way, the uncertainty of the individual predictions is effectively propagated into the estimate of variability at the aggregated level. The results give an arithmetic mean of 63 Bq/m3, a geometric mean of 41 Bq/m3 and a 95 %ile of 180 Bq/m3. The exceedance probability for 100 Bq/m3 and 300 Bq/m3 are 12.5 % (10.5 million people) and 2.2 % (1.9 million people), respectively. In large cities, individual indoor radon exposure is generally lower than in rural areas, which is a due to the different distribution of the population on floor levels. The advantages of our approach are 1) an accurate exposure estimation even if the survey was not fully representative with respect to the main controlling factors, and 2) an estimate of the exposure distribution with a much higher spatial resolution than basic descriptive statistics.
</details>
<details>
<summary>摘要</summary>
气体氧化物Radon是一种致癌的放射性气体，可以在室内堆积。室内Radon暴露的国家规模通常通过广泛的测量运动来估算。然而，样本特点与人口特点之间存在许多相关因素，如地源Radon的可用性和地板层。此外，样本大小通常无法实现高空间分辨率的暴露估计。我们提出了一种基于模型的方法，可以更加准确地估计室内Radon分布，并提高空间分辨率。我们采用了两个阶段的模型方法：1. 使用缺陷回归森林来估计室内Radon的分布函数，使用环境和建筑数据作为预测器。2. 使用 probabilistic Monte Carlo sampling technique来组合和人口权重 floor-level 预测。这样，个体预测的uncertainty会被有效地传递到聚合水平的估计中。结果表明， arithmetic mean 为 63 Bq/m3， geometric mean 为 41 Bq/m3，和 95%ile 为 180 Bq/m3。100 Bq/m3和300 Bq/m3的超过 probabilities 分别为 12.5% (10.5 million people) 和 2.2% (1.9 million people)。在大城市地区，室内Radon暴露通常比农村地区低，这是因为人口分布不同。我们的方法的优点包括：1. 即使调查不具有完全反映主要控制因素的 representativeness，也可以准确地估计暴露水平。2. 可以提供高空间分辨率的暴露估计，比基本描述统计数据更加精准。
</details></li>
</ul>
<hr>
<h2 id="Keep-Various-Trajectories-Promoting-Exploration-of-Ensemble-Policies-in-Continuous-Control"><a href="#Keep-Various-Trajectories-Promoting-Exploration-of-Ensemble-Policies-in-Continuous-Control" class="headerlink" title="Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control"></a>Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11138">http://arxiv.org/abs/2310.11138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Li, Chen Gong, Qiang He, Xinwen Hou</li>
<li>for: 这个研究的目的是强化深度强化学习（DRL） ensemble 方法的 empirical 成功，并且提高多模型的统计价值估计和政策的稳定性。</li>
<li>methods: 本研究使用了 Trajectories-awarE Ensemble exploratioN (TEEN) 算法，它的目的是增加多条 trajectory 的丰富性，以提高 ensemble 政策的表现。</li>
<li>results: 实验结果显示，TEEN 可以提高 ensemble 政策的表现，比基eline ensemble DRL 算法高出41%。<details>
<summary>Abstract</summary>
The combination of deep reinforcement learning (DRL) with ensemble methods has been proved to be highly effective in addressing complex sequential decision-making problems. This success can be primarily attributed to the utilization of multiple models, which enhances both the robustness of the policy and the accuracy of value function estimation. However, there has been limited analysis of the empirical success of current ensemble RL methods thus far. Our new analysis reveals that the sample efficiency of previous ensemble DRL algorithms may be limited by sub-policies that are not as diverse as they could be. Motivated by these findings, our study introduces a new ensemble RL algorithm, termed \textbf{T}rajectories-awar\textbf{E} \textbf{E}nsemble exploratio\textbf{N} (TEEN). The primary goal of TEEN is to maximize the expected return while promoting more diverse trajectories. Through extensive experiments, we demonstrate that TEEN not only enhances the sample diversity of the ensemble policy compared to using sub-policies alone but also improves the performance over ensemble RL algorithms. On average, TEEN outperforms the baseline ensemble DRL algorithms by 41\% in performance on the tested representative environments.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）和集成方法的组合在复杂的顺序决策问题上表现出非常高效。这种成功可以归功于多个模型的使用，增强政策的稳健性和估值函数的准确性。然而，现有的集成RL算法的实证成功仍然受限。我们的新分析发现，前一些集成DRL算法的样本效率可能受到较差的优化策略的限制。驱动于这些发现，我们的研究提出了一种新的集成RL算法，名为天文道-探索-集成探索（TEEN）。TEEN的主要目标是 Maximize the expected return while promoting more diverse trajectories。我们通过广泛的实验表明，TEEN不仅提高了集成政策的样本多样性，还超过了基eline集成DRL算法的性能。在测试环境中，TEEN平均比基eline算法提高41%的性能。
</details></li>
</ul>
<hr>
<h2 id="Non-parametric-Conditional-Independence-Testing-for-Mixed-Continuous-Categorical-Variables-A-Novel-Method-and-Numerical-Evaluation"><a href="#Non-parametric-Conditional-Independence-Testing-for-Mixed-Continuous-Categorical-Variables-A-Novel-Method-and-Numerical-Evaluation" class="headerlink" title="Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation"></a>Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11132">http://arxiv.org/abs/2310.11132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oana-Iuliana Popescu, Andreas Gerhardus, Jakob Runge</li>
<li>for: 这篇研究是关于 conditional independence testing (CIT) 的 mixed-type dataset 的应用。</li>
<li>methods: 这篇研究使用了 conditional mutual information (CMI) 估计器，与 local permutation scheme，并比较了两种新的 CMI 估计器：一种是基于 k-nearest-neighbors (k-NN) 方法，另一种是基于 entropy 度量。</li>
<li>results: 研究发现，该 variants 可以更好地检测依赖性，并且可以适应不同的数据分布和预processing 类型。<details>
<summary>Abstract</summary>
Conditional independence testing (CIT) is a common task in machine learning, e.g., for variable selection, and a main component of constraint-based causal discovery. While most current CIT approaches assume that all variables are numerical or all variables are categorical, many real-world applications involve mixed-type datasets that include numerical and categorical variables. Non-parametric CIT can be conducted using conditional mutual information (CMI) estimators combined with a local permutation scheme. Recently, two novel CMI estimators for mixed-type datasets based on k-nearest-neighbors (k-NN) have been proposed. As with any k-NN method, these estimators rely on the definition of a distance metric. One approach computes distances by a one-hot encoding of the categorical variables, essentially treating categorical variables as discrete-numerical, while the other expresses CMI by entropy terms where the categorical variables appear as conditions only. In this work, we study these estimators and propose a variation of the former approach that does not treat categorical variables as numeric. Our numerical experiments show that our variant detects dependencies more robustly across different data distributions and preprocessing types.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> Conditional independence testing (CIT) 是机器学习中常见的任务之一，例如变量选择，并是约束基于 causal discovery 的主要组成部分。而现实中的大多数 CIT 方法假设所有变量都是数值型或所有变量都是类别型，但是实际应用中经常会出现混合类型的数据集。非 Parametric CIT 可以通过 conditional mutual information (CMI) 估计器和地方排序方案来进行。最近，两种新的 CMI 估计器 для混合类型数据集基于 k-nearest-neighbors (k-NN) 已经被提出。这些估计器都取决于距离度量的定义。一种方法通过一个一键编码的 categorical 变量来计算距离，实际上将 categorical 变量当作数值型处理，而另一种方法通过 entropy 表达来计算 CMI，在 categorical 变量中只有作为条件出现。在这项工作中，我们研究这些估计器，并提出一种不对 categorical 变量进行数值化的变体。我们的数值实验表明，我们的变体在不同的数据分布和预处理类型下能够更加稳定地检测依赖关系。
</details></li>
</ul>
<hr>
<h2 id="FROST-Towards-Energy-efficient-AI-on-5G-Platforms-–-A-GPU-Power-Capping-Evaluation"><a href="#FROST-Towards-Energy-efficient-AI-on-5G-Platforms-–-A-GPU-Power-Capping-Evaluation" class="headerlink" title="FROST: Towards Energy-efficient AI-on-5G Platforms – A GPU Power Capping Evaluation"></a>FROST: Towards Energy-efficient AI-on-5G Platforms – A GPU Power Capping Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11131">http://arxiv.org/abs/2310.11131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Mavromatis, Stefano De Feo, Pietro Carnelli, Robert J. Piechocki, Aftab Khan</li>
<li>for: 该论文targets the Open Radio Access Network (O-RAN) market, which is expected to experience significant growth in the coming years. The paper aims to optimize the energy consumption of Machine Learning (ML) pipelines in O-RAN ecosystems.</li>
<li>methods: 该论文提出了一种名为FROST的解决方案，即Flexible Reconfiguration method with Online System Tuning。FROST可以 Profiling the energy consumption of an ML pipeline and optimizing the hardware accordingly, thereby limiting the power draw.</li>
<li>results: 根据该论文的发现，FROST可以实现energy savings of up to 26.4% without compromising the model’s accuracy or introducing significant time delays.<details>
<summary>Abstract</summary>
The Open Radio Access Network (O-RAN) is a burgeoning market with projected growth in the upcoming years. RAN has the highest CAPEX impact on the network and, most importantly, consumes 73% of its total energy. That makes it an ideal target for optimisation through the integration of Machine Learning (ML). However, the energy consumption of ML is frequently overlooked in such ecosystems. Our work addresses this critical aspect by presenting FROST - Flexible Reconfiguration method with Online System Tuning - a solution for energy-aware ML pipelines that adhere to O-RAN's specifications and principles. FROST is capable of profiling the energy consumption of an ML pipeline and optimising the hardware accordingly, thereby limiting the power draw. Our findings indicate that FROST can achieve energy savings of up to 26.4% without compromising the model's accuracy or introducing significant time delays.
</details>
<details>
<summary>摘要</summary>
openRadio Access Network (O-RAN) 是一个快速发展的市场，未来几年将出现快速增长。RAN 是网络总体的最高CapEx 成本和73% 的能源消耗，因此它成为了优化的目标。然而，机器学习 (ML) 在这些生态系统中的能源消耗frequently 被忽略。我们的工作解决了这个关键问题，提出了FROST - 可变化的重配置方法与在线系统调整 - 一种遵循 O-RAN 规范和原则的能源意识机器学习管道解决方案。FROST 可以对机器学习管道的能源消耗进行 profiling，并根据硬件进行优化，从而限制能源浪费。我们的发现表明，FROST 可以实现能源节约达到 26.4% 而不会 compromise 模型准确性或引入显著的时间延迟。
</details></li>
</ul>
<hr>
<h2 id="Topological-Expressivity-of-ReLU-Neural-Networks"><a href="#Topological-Expressivity-of-ReLU-Neural-Networks" class="headerlink" title="Topological Expressivity of ReLU Neural Networks"></a>Topological Expressivity of ReLU Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11130">http://arxiv.org/abs/2310.11130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekin Ergen, Moritz Grillo</li>
<li>for: 本研究探讨了ReLU神经网络在二分类问题中的表达能力，从拓扑角度来看。</li>
<li>methods: 本研究使用了Betti数来度量神经网络对数据集的拓扑简化程度。</li>
<li>results: 研究结果表明，深度的ReLU神经网络在二分类问题中的表达能力比浅度的神经网络更强，具体来说是 exponentially more powerful。这提供了一个数学上的正式解释，为什么深度的神经网络能够更好地处理复杂和拓扑 ric 的数据集。<details>
<summary>Abstract</summary>
We study the expressivity of ReLU neural networks in the setting of a binary classification problem from a topological perspective. Recently, empirical studies showed that neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simpler one as it passes through the layers. This topological simplification has been measured by Betti numbers, which are algebraic invariants of a topological space. We use the same measure to establish lower and upper bounds on the topological simplification a ReLU neural network can achieve with a given architecture. We therefore contribute to a better understanding of the expressivity of ReLU neural networks in the context of binary classification problems by shedding light on their ability to capture the underlying topological structure of the data. In particular the results show that deep ReLU neural networks are exponentially more powerful than shallow ones in terms of topological simplification. This provides a mathematically rigorous explanation why deeper networks are better equipped to handle complex and topologically rich datasets.
</details>
<details>
<summary>摘要</summary>
我们研究使用ReLU神经网络进行二分类问题的表达能力，从拓扑角度来看。最近的实验证明，神经网络在传递层次时会改变拓扑结构，将复杂的拓扑数据集转化为简单的拓扑结构。这种拓扑简化的度量使用Betti数，它是一种拓扑空间的代数 invariants。我们使用这个度量来确定ReLU神经网络的拓扑简化能力，并提出了对于给定架构的下限和上限。因此，我们对于二分类问题中ReLU神经网络的表达能力做出了更深入的理解，特别是结果表明深度的ReLU神经网络在拓扑简化方面的表达能力是极大的，这提供了一种数学上的正式解释，为复杂和拓扑 ric的数据集进行处理，为何更深度的网络更好。
</details></li>
</ul>
<hr>
<h2 id="On-the-Temperature-of-Bayesian-Graph-Neural-Networks-for-Conformal-Prediction"><a href="#On-the-Temperature-of-Bayesian-Graph-Neural-Networks-for-Conformal-Prediction" class="headerlink" title="On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction"></a>On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11479">http://arxiv.org/abs/2310.11479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seohyeon Cha, Honggu Kang, Joonhyuk Kang</li>
<li>for: 提高Graph Neural Networks（GNNs）中的不确定性评估，尤其在高风险领域where GNNs frequently employed。</li>
<li>methods: 使用Conformal Prediction（CP）框架，提供有效的预测集， garantizing formal probabilistic guarantees that a prediction set contains a true label with a desired probability。</li>
<li>results: 实验表明，可以通过设置温度参数，使得预测集更加有效率。此外，我们还进行了一种分析，以便了解CP性能和模型准确性之间的关系。<details>
<summary>Abstract</summary>
Accurate uncertainty quantification in graph neural networks (GNNs) is essential, especially in high-stakes domains where GNNs are frequently employed. Conformal prediction (CP) offers a promising framework for quantifying uncertainty by providing $\textit{valid}$ prediction sets for any black-box model. CP ensures formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as $\textit{inefficiency}$, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning also provides a credible region based on the estimated posterior distribution, but this region is $\textit{well-calibrated}$ only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP framework. We empirically demonstrate the existence of temperatures that result in more efficient prediction sets. Furthermore, we conduct an analysis to identify the factors contributing to inefficiency and offer valuable insights into the relationship between CP performance and model calibration.
</details>
<details>
<summary>摘要</summary>
precisions of uncertainty quantification in graph neural networks (GNNs) is crucial, especially in high-stakes domains where GNNs are widely used. Conformal prediction (CP) provides a promising framework for uncertainty quantification by offering valid prediction sets for any black-box model. CP guarantees formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as inefficiency, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning provides a credible region based on the estimated posterior distribution, but this region is well-calibrated only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP framework. We empirically demonstrate the existence of temperatures that result in more efficient prediction sets. Furthermore, we conduct an analysis to identify the factors contributing to inefficiency and offer valuable insights into the relationship between CP performance and model calibration.Here's the translation in Traditional Chinese: precisions of uncertainty quantification in graph neural networks (GNNs) is crucial, especially in high-stakes domains where GNNs are widely used. Conformal prediction (CP) provides a promising framework for uncertainty quantification by offering valid prediction sets for any black-box model. CP guarantees formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as inefficiency, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning provides a credible region based on the estimated posterior distribution, but this region is well-calibrated only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP framework. We empirically demonstrate the existence of temperatures that result in more efficient prediction sets. Furthermore, we conduct an analysis to identify the factors contributing to inefficiency and offer valuable insights into the relationship between CP performance and model calibration.
</details></li>
</ul>
<hr>
<h2 id="Sensitivity-Aware-Amortized-Bayesian-Inference"><a href="#Sensitivity-Aware-Amortized-Bayesian-Inference" class="headerlink" title="Sensitivity-Aware Amortized Bayesian Inference"></a>Sensitivity-Aware Amortized Bayesian Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11122">http://arxiv.org/abs/2310.11122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lasse Elsemüller, Hans Olischläger, Marvin Schmitt, Paul-Christian Bürkner, Ullrich Köthe, Stefan T. Radev</li>
<li>for: 这篇论文的目的是提出一种可以在不同假设下进行 Bayesian 推理的方法，以便更好地了解不同假设下的结果之间的关系。</li>
<li>methods: 这篇论文使用了 neural network 来实现 simulation-based inference，并利用 weight sharing 技术来编码结构相似性。</li>
<li>results: 该方法可以快速地评估不同假设下的结果之间的敏感性，并且可以使用 neural network  ensemble 来评估模型的变化。<details>
<summary>Abstract</summary>
Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly bottleneck of refitting the model(s) for each choice of likelihood, prior, or dataset. Finally, we propose to use neural network ensembles to evaluate variation in results induced by unreliable approximation on unseen data. We demonstrate the effectiveness of our method in applied modeling problems, ranging from the estimation of disease outbreak dynamics and global warming thresholds to the comparison of human decision-making models. Our experiments showcase how our approach enables practitioners to effectively unveil hidden relationships between modeling choices and inferential conclusions.
</details>
<details>
<summary>摘要</summary>
泛bayesian推理是一种强大的推理框架，用于在不纯的情况下做出推理和决策。现代泛bayesian工作流程中的基本选择包括可信度函数和先验分布的规定、 posterior approximator 和数据。每一个选择都会对模型基于推理和后续决策产生重要影响，因此需要敏感分析。在这种工作中，我们提议一种多方面的方法，将敏感分析integrated into amortized Bayesian inference (ABI，即通过神经网络进行 simulations-based inference)。首先，我们利用 weight sharing 将结构相似性编码到替代可信度函数和先验分布中的训练过程中，以 minimize computational overhead。其次，我们利用神经网络的快速推理来评估数据变化或预处理过程对结果的敏感性。与大多数泛bayesian方法不同，这两个步骤都可以避免对模型的重新适应过程中的成本。最后，我们提议使用神经网络集合来评估未知数据上的结果变化。我们在应用模型问题中进行了实验，从疾病爆发动力和全球暖化阈值估计到人类决策模型的比较。我们的实验显示了我们的方法可以帮助实践者更好地揭示模型选择和推理结论之间的隐藏关系。
</details></li>
</ul>
<hr>
<h2 id="Minimally-Informed-Linear-Discriminant-Analysis-training-an-LDA-model-with-unlabelled-data"><a href="#Minimally-Informed-Linear-Discriminant-Analysis-training-an-LDA-model-with-unlabelled-data" class="headerlink" title="Minimally Informed Linear Discriminant Analysis: training an LDA model with unlabelled data"></a>Minimally Informed Linear Discriminant Analysis: training an LDA model with unlabelled data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11110">http://arxiv.org/abs/2310.11110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Heintz, Tom Francart, Alexander Bertrand</li>
<li>for: 这 paper 是用来解释如何使用 Linear Discriminant Analysis (LDA) 算法来解决无标签数据的分类问题。</li>
<li>methods: 这 paper 使用了一种名为 Minimally Informed Linear Discriminant Analysis (MILDA) 模型，该模型可以在没有标签数据的情况下计算出 LDA 投影向量，只需要一些最小的先验信息。</li>
<li>results: 该 paper 的实验结果表明，MILDA 模型可以准确地模型分类问题，并且可以快速适应非站ARY 数据，这使得它成为一个可靠的 adaptive classifier。<details>
<summary>Abstract</summary>
Linear Discriminant Analysis (LDA) is one of the oldest and most popular linear methods for supervised classification problems. In this paper, we demonstrate that it is possible to compute the exact projection vector from LDA models based on unlabelled data, if some minimal prior information is available. More precisely, we show that only one of the following three pieces of information is actually sufficient to compute the LDA projection vector if only unlabelled data are available: (1) the class average of one of the two classes, (2) the difference between both class averages (up to a scaling), or (3) the class covariance matrices (up to a scaling). These theoretical results are validated in numerical experiments, demonstrating that this minimally informed Linear Discriminant Analysis (MILDA) model closely matches the performance of a supervised LDA model. Furthermore, we show that the MILDA projection vector can be computed in a closed form with a computational cost comparable to LDA and is able to quickly adapt to non-stationary data, making it well-suited to use as an adaptive classifier.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>一个类型的类均值（或一个类型的平均值）2. 两个类型之间的差异（几乎可以忽略扩大）3. 两个类型的类 covariance 矩阵（几乎可以忽略扩大）这些理论结果在数学实验中得到了验证，表明了这种只需要最小先验信息的 Linear Discriminant Analysis (MILDA) 模型能够与指导类型的 LDA 模型准确匹配。此外，我们还证明了 MILDA 投影向量可以在关闭形式下计算，计算成本与 LDA 相当，能够快速适应非站点数据，使其成为一种适用于适应类型的批处理器。</details></li>
</ol>
<hr>
<h2 id="Local-Lipschitz-Constant-Computation-of-ReLU-FNNs-Upper-Bound-Computation-with-Exactness-Verification"><a href="#Local-Lipschitz-Constant-Computation-of-ReLU-FNNs-Upper-Bound-Computation-with-Exactness-Verification" class="headerlink" title="Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification"></a>Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11104">http://arxiv.org/abs/2310.11104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshio Ebihara, Xin Dai, Victor Magron, Dimitri Peaucelle, Sophie Tarbouriech</li>
<li>for: 这 paper 关注计算Feedforward Neural Networks (FNNs) 的地方 Lipschitz 常数，其中 activation functions 是 Rectified Linear Units (ReLUs)。地方 Lipschitz 常数 是一个 reasonable measure  для FNNs 的评估量。</li>
<li>methods: 我们首先将 upper bound 计算问题转化为一个 semidefinite programming problem (SDP)，然后引入新的 copositive multipliers 来准确地捕捉 ReLU 的行为。然后，我们通过对 SDP 的 dual 进行分析，提出一种可靠的检验方法来验证 computed upper bound 的准确性。</li>
<li>results: 我们通过数学实验来证明方法的有效性，并通过实验示例来验证方法在实际 FNNs 上的可行性。<details>
<summary>Abstract</summary>
This paper is concerned with the computation of the local Lipschitz constant of feedforward neural networks (FNNs) with activation functions being rectified linear units (ReLUs). The local Lipschitz constant of an FNN for a target input is a reasonable measure for its quantitative evaluation of the reliability. By following a standard procedure using multipliers that capture the behavior of ReLUs,we first reduce the upper bound computation problem of the local Lipschitz constant into a semidefinite programming problem (SDP). Here we newly introduce copositive multipliers to capture the ReLU behavior accurately. Then, by considering the dual of the SDP for the upper bound computation, we second derive a viable test to conclude the exactness of the computed upper bound. However, these SDPs are intractable for practical FNNs with hundreds of ReLUs. To address this issue, we further propose a method to construct a reduced order model whose input-output property is identical to the original FNN over a neighborhood of the target input. We finally illustrate the effectiveness of the model reduction and exactness verification methods with numerical examples of practical FNNs.
</details>
<details>
<summary>摘要</summary>
We first convert the upper bound computation problem of the local Lipschitz constant into a semidefinite programming problem (SDP) using multipliers that capture the ReLU behavior. To improve the accuracy of the computation, we introduce new copositive multipliers.Next, we derive a feasibility test for the computed upper bound by considering the dual of the SDP. However, these SDPs are computationally intractable for practical FNNs with hundreds of ReLUs.To address this issue, we propose a method to construct a reduced order model whose input-output property is identical to the original FNN over a neighborhood of the target input. We demonstrate the effectiveness of the model reduction and exactness verification methods with numerical examples of practical FNNs.
</details></li>
</ul>
<hr>
<h2 id="Sparse-DySta-Sparsity-Aware-Dynamic-and-Static-Scheduling-for-Sparse-Multi-DNN-Workloads"><a href="#Sparse-DySta-Sparsity-Aware-Dynamic-and-Static-Scheduling-for-Sparse-Multi-DNN-Workloads" class="headerlink" title="Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads"></a>Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11096">http://arxiv.org/abs/2310.11096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/samsunglabs/sparse-multi-dnn-scheduling">https://github.com/samsunglabs/sparse-multi-dnn-scheduling</a></li>
<li>paper_authors: Hongxiang Fan, Stylianos I. Venieris, Alexandros Kouris, Nicholas D. Lane</li>
<li>for: 本研究旨在探讨多个稀疏深度神经网络（DNN）在 Edge 设备和数据中心之间的平衡执行。</li>
<li>methods: 本文使用了多种简化 Approaches，包括静态稀疏模型和动态稀疏信息，以提高稀疏多DNN 调度。</li>
<li>results: 对于多种部署场景，本文提出了一种新的双级静态和动态调度策略，并通过实验证明其与状态艺术方法相比，可以降低对响应时间的依赖性，并提高平均 норма化响应时间。<details>
<summary>Abstract</summary>
Running multiple deep neural networks (DNNs) in parallel has become an emerging workload in both edge devices, such as mobile phones where multiple tasks serve a single user for daily activities, and data centers, where various requests are raised from millions of users, as seen with large language models. To reduce the costly computational and memory requirements of these workloads, various efficient sparsification approaches have been introduced, resulting in widespread sparsity across different types of DNN models. In this context, there is an emerging need for scheduling sparse multi-DNN workloads, a problem that is largely unexplored in previous literature. This paper systematically analyses the use-cases of multiple sparse DNNs and investigates the opportunities for optimizations. Based on these findings, we propose Dysta, a novel bi-level dynamic and static scheduler that utilizes both static sparsity patterns and dynamic sparsity information for the sparse multi-DNN scheduling. Both static and dynamic components of Dysta are jointly designed at the software and hardware levels, respectively, to improve and refine the scheduling approach. To facilitate future progress in the study of this class of workloads, we construct a public benchmark that contains sparse multi-DNN workloads across different deployment scenarios, spanning from mobile phones and AR/VR wearables to data centers. A comprehensive evaluation on the sparse multi-DNN benchmark demonstrates that our proposed approach outperforms the state-of-the-art methods with up to 10% decrease in latency constraint violation rate and nearly 4X reduction in average normalized turnaround time. Our artifacts and code are publicly available at: https://github.com/SamsungLabs/Sparse-Multi-DNN-Scheduling.
</details>
<details>
<summary>摘要</summary>
running multiple deep neural networks (DNNs) in parallel has become an emerging workload in both edge devices, such as mobile phones where multiple tasks serve a single user for daily activities, and data centers, where various requests are raised from millions of users, as seen with large language models. To reduce the costly computational and memory requirements of these workloads, various efficient sparsification approaches have been introduced, resulting in widespread sparsity across different types of DNN models. In this context, there is an emerging need for scheduling sparse multi-DNN workloads, a problem that is largely unexplored in previous literature. This paper systematically analyzes the use-cases of multiple sparse DNNs and investigates the opportunities for optimizations. Based on these findings, we propose Dysta, a novel bi-level dynamic and static scheduler that utilizes both static sparsity patterns and dynamic sparsity information for the sparse multi-DNN scheduling. Both static and dynamic components of Dysta are jointly designed at the software and hardware levels, respectively, to improve and refine the scheduling approach. To facilitate future progress in the study of this class of workloads, we construct a public benchmark that contains sparse multi-DNN workloads across different deployment scenarios, spanning from mobile phones and AR/VR wearables to data centers. A comprehensive evaluation on the sparse multi-DNN benchmark demonstrates that our proposed approach outperforms the state-of-the-art methods with up to 10% decrease in latency constraint violation rate and nearly 4X reduction in average normalized turnaround time. Our artifacts and code are publicly available at: https://github.com/SamsungLabs/Sparse-Multi-DNN-Scheduling.
</details></li>
</ul>
<hr>
<h2 id="Relearning-Forgotten-Knowledge-on-Forgetting-Overfit-and-Training-Free-Ensembles-of-DNNs"><a href="#Relearning-Forgotten-Knowledge-on-Forgetting-Overfit-and-Training-Free-Ensembles-of-DNNs" class="headerlink" title="Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs"></a>Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11094">http://arxiv.org/abs/2310.11094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uri Stern, Daphna Weinshall</li>
<li>for: 这个论文的目的是解释深度神经网络中的过拟合现象，并提出一种新的评估过拟合方法。</li>
<li>methods: 这篇论文使用了一种新的评估过拟合方法，它基于 validate 数据集中模型忘记率的评估。</li>
<li>results: 该论文的实验结果表明，过拟合可以在模型训练过程中发生，并且可能更为常见 than 先前认为的。此外，该论文还提出了一种基于单个网络训练历史的新ensemble方法，该方法可以提高性能而无需额外的训练时间成本。<details>
<summary>Abstract</summary>
The infrequent occurrence of overfit in deep neural networks is perplexing. On the one hand, theory predicts that as models get larger they should eventually become too specialized for a specific training set, with ensuing decrease in generalization. In contrast, empirical results in image classification indicate that increasing the training time of deep models or using bigger models almost never hurts generalization. Is it because the way we measure overfit is too limited? Here, we introduce a novel score for quantifying overfit, which monitors the forgetting rate of deep models on validation data. Presumably, this score indicates that even while generalization improves overall, there are certain regions of the data space where it deteriorates. When thus measured, we show that overfit can occur with and without a decrease in validation accuracy, and may be more common than previously appreciated. This observation may help to clarify the aforementioned confusing picture. We use our observations to construct a new ensemble method, based solely on the training history of a single network, which provides significant improvement in performance without any additional cost in training time. An extensive empirical evaluation with modern deep models shows our method's utility on multiple datasets, neural networks architectures and training schemes, both when training from scratch and when using pre-trained networks in transfer learning. Notably, our method outperforms comparable methods while being easier to implement and use, and further improves the performance of competitive networks on Imagenet by 1\%.
</details>
<details>
<summary>摘要</summary>
启发性训练深度神经网络中偶尔出现过拟合现象很困惑。一面理论预测，随着模型的大小增加，它们应该逐渐变得特化于具体的训练集，导致泛化性下降。然而，实际研究发现，深度模型的训练时间增加或使用更大的模型在图像分类任务中并没有明显的泛化性下降。我们是否因为量化过拟合的方法有限而导致这种情况呢？在这里，我们引入一种新的过拟合评价指标，可以监测深度模型在验证数据上忘记率。这个指标表明，即使总的泛化性提高，仍然有一些数据空间中的忘记率下降。当如此量化过拟合时，我们发现过拟合可以发生在Validation accuracy下降和不下降的情况下，并且可能更为常见。这一观察可能有助于解释深度神经网络中的困惑场景。我们利用这些观察，构建了一种基于单个网络训练历史的新集成方法，可以在不添加训练时间成本的情况下提供显著性能提升。我们的方法在现代深度模型、不同的 neural network 架构、训练方案和传输学习中都有广泛的实际评估，并且在Imagenet上提高了1%的性能。另外，我们的方法比同类方法更容易实现和使用，并且可以进一步提高竞争力强的网络的性能。
</details></li>
</ul>
<hr>
<h2 id="Data-Drift-Monitoring-for-Log-Anomaly-Detection-Pipelines"><a href="#Data-Drift-Monitoring-for-Log-Anomaly-Detection-Pipelines" class="headerlink" title="Data Drift Monitoring for Log Anomaly Detection Pipelines"></a>Data Drift Monitoring for Log Anomaly Detection Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14893">http://arxiv.org/abs/2310.14893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipak Wani, Samuel Ackerman, Eitan Farchi, Xiaotong Liu, Hau-wen Chang, Sarasi Lalithsena</li>
<li>for: 本文主要用于提出了一种基于 bayes factor 的偏移检测方法，用于检测日志活动偏移，并且可以帮助系统可靠工程师（SRE）在系统诊断中获得协助。</li>
<li>methods: 本文使用了 Bayes Factor 来检测日志活动偏移，并且提出了一种基于人工干预的方法来更新 LAD 模型。</li>
<li>results: 本文通过使用真实收集的日志数据和 simulate 的活动序列来证明了该方法的可靠性和有效性。<details>
<summary>Abstract</summary>
Logs enable the monitoring of infrastructure status and the performance of associated applications. Logs are also invaluable for diagnosing the root causes of any problems that may arise. Log Anomaly Detection (LAD) pipelines automate the detection of anomalies in logs, providing assistance to site reliability engineers (SREs) in system diagnosis. Log patterns change over time, necessitating updates to the LAD model defining the `normal' log activity profile. In this paper, we introduce a Bayes Factor-based drift detection method that identifies when intervention, retraining, and updating of the LAD model are required with human involvement. We illustrate our method using sequences of log activity, both from unaltered data, and simulated activity with controlled levels of anomaly contamination, based on real collected log data.
</details>
<details>
<summary>摘要</summary>
日志可以监控基础设施状态和相关应用程序的性能。日志也是解决问题的根本原因的诊断的重要工具。日志异常检测（LAD）管道自动检测日志中的异常情况，为站点可靠工程师（SRE）提供帮助。日志模式随着时间的变化，因此LAD模型需要定期更新。在这篇文章中，我们介绍了基于 bayes 因子的漂移检测方法，可以在人工参与下确定是否需要介入、重新训练和更新 LAD 模型。我们使用了日志活动序列，包括未修改数据和 simulated 活动，以及控制了异常污染的水平，基于实际收集的日志数据来示例我们的方法。
</details></li>
</ul>
<hr>
<h2 id="CSG-Curriculum-Representation-Learning-for-Signed-Graph"><a href="#CSG-Curriculum-Representation-Learning-for-Signed-Graph" class="headerlink" title="CSG: Curriculum Representation Learning for Signed Graph"></a>CSG: Curriculum Representation Learning for Signed Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11083">http://arxiv.org/abs/2310.11083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Zhang, Jiamou Liu, Kaiqi Zhao, Yifei Wang, Pengqian Han, Xianda Zheng, Qiqi Wang, Zijian Zhang</li>
<li>for: 本研究旨在提高Signed Graph Neural Networks（SGNNs）的精度和稳定性，通过设计一种基于课程的训练方法，以便更好地处理复杂的签名图。</li>
<li>methods: 本研究提出了一种基于课程的训练方法，其中样本按照难度从易到复杂进行排序，以便SGNN模型在处理不同难度的样本上进行学习。此外，我们还引入了一种轻量级的机制，以量化图的学习难度。</li>
<li>results: 经验 validate 表明，我们的训练方法可以提高 SGNN 模型的准确率，在链接签记预测（AUC）中提高了23.7%，并且可以显著降低标准差的AUC分布。<details>
<summary>Abstract</summary>
Signed graphs are valuable for modeling complex relationships with positive and negative connections, and Signed Graph Neural Networks (SGNNs) have become crucial tools for their analysis. However, prior to our work, no specific training plan existed for SGNNs, and the conventional random sampling approach did not address varying learning difficulties within the graph's structure. We proposed a curriculum-based training approach, where samples progress from easy to complex, inspired by human learning. To measure learning difficulty, we introduced a lightweight mechanism and created the Curriculum representation learning framework for Signed Graphs (CSG). This framework optimizes the order in which samples are presented to the SGNN model. Empirical validation across six real-world datasets showed impressive results, enhancing SGNN model accuracy by up to 23.7% in link sign prediction (AUC) and significantly improving stability with an up to 8.4 reduction in the standard deviation of AUC scores.
</details>
<details>
<summary>摘要</summary>
签名图是用于模型复杂关系的工具，它们可以表示正有负连接。然而，在我们的工作之前，没有专门的培训计划 для签名图神经网络（SGNN），而常见的随机抽样方法也不能处理图结构中的变化学习困难。我们提出了一种学习级别的培训方法，其中样本从简单到复杂进行排序，这是基于人类学习的启发。为了测量学习困难，我们引入了一种轻量级机制，并创建了签名图表示学习框架（CSG）。这个框架优化了SGNN模型被抽样的顺序。经验 validate 在六个实际数据集上表现出色，SGNN 模型的准确率提高了最多 23.7%（AUC），并有显著提高稳定性，AUC 标准差下降了最多 8.4。
</details></li>
</ul>
<hr>
<h2 id="Resampling-Stochastic-Gradient-Descent-Cheaply-for-Efficient-Uncertainty-Quantification"><a href="#Resampling-Stochastic-Gradient-Descent-Cheaply-for-Efficient-Uncertainty-Quantification" class="headerlink" title="Resampling Stochastic Gradient Descent Cheaply for Efficient Uncertainty Quantification"></a>Resampling Stochastic Gradient Descent Cheaply for Efficient Uncertainty Quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11065">http://arxiv.org/abs/2310.11065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henry Lam, Zitong Wang</li>
<li>for: 本研究旨在分析 Stochastic Gradient Descent（SGD）训练模型和 Stochastic Optimization 中的解决方案，并对其进行uncertainty quantification。</li>
<li>methods: 我们提出了两种 computationally cheap resampling-based方法来构建SGD解决方案的信任区间。其中一种使用多个SGD并行运行，通过从数据中采样而不填充替换，另一种在在线模式下进行。我们的方法可以视为现有批处理方法的改进，同时可以减少计算努力的重复样本需求。</li>
<li>results: 我们采用了一种称为”便宜bootstrap”的新想法和Berry-Esseen-type bound for SGD，以实现这些目标。我们的方法可以减少计算努力，同时可以快速地生成高质量的信任区间。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) or stochastic approximation has been widely used in model training and stochastic optimization. While there is a huge literature on analyzing its convergence, inference on the obtained solutions from SGD has only been recently studied, yet is important due to the growing need for uncertainty quantification. We investigate two computationally cheap resampling-based methods to construct confidence intervals for SGD solutions. One uses multiple, but few, SGDs in parallel via resampling with replacement from the data, and another operates this in an online fashion. Our methods can be regarded as enhancements of established bootstrap schemes to substantially reduce the computation effort in terms of resampling requirements, while at the same time bypassing the intricate mixing conditions in existing batching methods. We achieve these via a recent so-called cheap bootstrap idea and Berry-Esseen-type bound for SGD.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Locally-Differentially-Private-Graph-Embedding"><a href="#Locally-Differentially-Private-Graph-Embedding" class="headerlink" title="Locally Differentially Private Graph Embedding"></a>Locally Differentially Private Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11060">http://arxiv.org/abs/2310.11060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zening Li, Rong-Hua Li, Meihao Liao, Fusheng Jin, Guoren Wang</li>
<li>for: 本研究旨在开发一种满足本地散列保护（LDP）的图像抽象算法，以保护图据中敏感信息的隐私。</li>
<li>methods: 本文提出了一种名为LDP-GE的隐私保护图像抽象框架，包括一种LDP机制来隐蔽节点数据，以及使用个性化PageRank来学习节点表示。</li>
<li>results: 对多个真实世界图据集进行了广泛的实验，并证明LDP-GE在节点分类和链接预测任务中达到了有利的隐私-用途质量比。<details>
<summary>Abstract</summary>
Graph embedding has been demonstrated to be a powerful tool for learning latent representations for nodes in a graph. However, despite its superior performance in various graph-based machine learning tasks, learning over graphs can raise significant privacy concerns when graph data involves sensitive information. To address this, in this paper, we investigate the problem of developing graph embedding algorithms that satisfy local differential privacy (LDP). We propose LDP-GE, a novel privacy-preserving graph embedding framework, to protect the privacy of node data. Specifically, we propose an LDP mechanism to obfuscate node data and adopt personalized PageRank as the proximity measure to learn node representations. Then, we theoretically analyze the privacy guarantees and utility of the LDP-GE framework. Extensive experiments conducted over several real-world graph datasets demonstrate that LDP-GE achieves favorable privacy-utility trade-offs and significantly outperforms existing approaches in both node classification and link prediction tasks.
</details>
<details>
<summary>摘要</summary>
“图像插入”已经被证明是一种有力的工具，用于学习图像中节点的隐藏表示。然而，在学习图像时，可能会引起个人隐私问题，特别是当图像数据包含敏感信息时。为了解决这个问题，在这篇论文中，我们调查了在图像上学习隐藏表示的问题，并提出了一种具有地方敏感性（LDP）的图像插入框架。我们提议了一种LDP机制，以隐藏节点数据，并采用个性化PageRank作为距离度量来学习节点表示。然后，我们对LDP-GE框架的隐私保证和实用性进行了理论分析。广泛的实验表明，LDP-GE在节点分类和链接预测任务中具有良好的隐私-实用质量比。
</details></li>
</ul>
<hr>
<h2 id="Causal-Feature-Selection-via-Transfer-Entropy"><a href="#Causal-Feature-Selection-via-Transfer-Entropy" class="headerlink" title="Causal Feature Selection via Transfer Entropy"></a>Causal Feature Selection via Transfer Entropy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11059">http://arxiv.org/abs/2310.11059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paolo Bonetti, Alberto Maria Metelli, Marcello Restelli</li>
<li>for: 本研究旨在提出一种新的方法，旨在Feature Selection和Causal Discovery之间的交叉点上，用于处理时间序列数据。</li>
<li>methods: 我们提出了一种新的 causal feature selection 方法，该方法基于前向和后向 Feature Selection 过程，并利用了传输 entropy 来估计特征之间的 causal 信息流。</li>
<li>results: 我们提供了理论保证，证明在 exact 和 finite-sample 情况下，我们的方法可以实现更好的预测和分类性能。此外，我们还在 synthetic 和实际 regression 问题上进行了数值验证，结果与考虑的基准相当竞争。<details>
<summary>Abstract</summary>
Machine learning algorithms are designed to capture complex relationships between features. In this context, the high dimensionality of data often results in poor model performance, with the risk of overfitting. Feature selection, the process of selecting a subset of relevant and non-redundant features, is, therefore, an essential step to mitigate these issues. However, classical feature selection approaches do not inspect the causal relationship between selected features and target, which can lead to misleading results in real-world applications. Causal discovery, instead, aims to identify causal relationships between features with observational data. In this paper, we propose a novel methodology at the intersection between feature selection and causal discovery, focusing on time series. We introduce a new causal feature selection approach that relies on the forward and backward feature selection procedures and leverages transfer entropy to estimate the causal flow of information from the features to the target in time series. Our approach enables the selection of features not only in terms of mere model performance but also captures the causal information flow. In this context, we provide theoretical guarantees on the regression and classification errors for both the exact and the finite-sample cases. Finally, we present numerical validations on synthetic and real-world regression problems, showing results competitive w.r.t. the considered baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Matrix-Compression-via-Randomized-Low-Rank-and-Low-Precision-Factorization"><a href="#Matrix-Compression-via-Randomized-Low-Rank-and-Low-Precision-Factorization" class="headerlink" title="Matrix Compression via Randomized Low Rank and Low Precision Factorization"></a>Matrix Compression via Randomized Low Rank and Low Precision Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11028">http://arxiv.org/abs/2310.11028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajarshi Saha, Varun Srivastava, Mert Pilanci<br>for:这个论文是为了提出一种基于矩阵的压缩算法，以实现矩阵的压缩和处理。methods:该算法利用矩阵的低级结构，通过随机抽象矩阵的列，并对这些列进行量化，以获得一个低级和低精度的矩阵分解。results:该算法可以实现矩阵的压缩，并且可以达到一比特为一的压缩率，同时保持或超过传统压缩技术的性能。<details>
<summary>Abstract</summary>
Matrices are exceptionally useful in various fields of study as they provide a convenient framework to organize and manipulate data in a structured manner. However, modern matrices can involve billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Although prohibitively large, such matrices are often approximately low rank. We propose an algorithm that exploits this structure to obtain a low rank decomposition of any matrix $\mathbf{A}$ as $\mathbf{A} \approx \mathbf{L}\mathbf{R}$, where $\mathbf{L}$ and $\mathbf{R}$ are the low rank factors. The total number of elements in $\mathbf{L}$ and $\mathbf{R}$ can be significantly less than that in $\mathbf{A}$. Furthermore, the entries of $\mathbf{L}$ and $\mathbf{R}$ are quantized to low precision formats $--$ compressing $\mathbf{A}$ by giving us a low rank and low precision factorization. Our algorithm first computes an approximate basis of the range space of $\mathbf{A}$ by randomly sketching its columns, followed by a quantization of the vectors constituting this basis. It then computes approximate projections of the columns of $\mathbf{A}$ onto this quantized basis. We derive upper bounds on the approximation error of our algorithm, and analyze the impact of target rank and quantization bit-budget. The tradeoff between compression ratio and approximation accuracy allows for flexibility in choosing these parameters based on specific application requirements. We empirically demonstrate the efficacy of our algorithm in image compression, nearest neighbor classification of image and text embeddings, and compressing the layers of LlaMa-$7$b. Our results illustrate that we can achieve compression ratios as aggressive as one bit per matrix coordinate, all while surpassing or maintaining the performance of traditional compression techniques.
</details>
<details>
<summary>摘要</summary>
矩阵在不同的领域中非常有用，因为它们可以有效地组织和处理数据。然而，现代矩阵可能包含数百亿个元素，这会导致存储和处理它们的计算资源和内存使用非常高。虽然这些矩阵可能是非常大的，但它们通常是低级别的。我们提出了一个算法，它利用这种结构来获得矩阵 $\mathbf{A}$ 的低级别分解，即 $\mathbf{A} \approx \mathbf{L}\mathbf{R}$，其中 $\mathbf{L}$ 和 $\mathbf{R}$ 是低级别因素。总的来说， $\mathbf{L}$ 和 $\mathbf{R}$ 中的元素数量可以非常少于 $\mathbf{A}$ 中的元素数量。此外， $\mathbf{L}$ 和 $\mathbf{R}$ 的元素可以使用低精度格式进行压缩，从而压缩 $\mathbf{A}$。我们的算法首先计算矩阵 $\mathbf{A}$ 的估计基准的范围空间，然后使用这个基准来压缩 $\mathbf{A}$ 的列。我们then compute approximate projections of the columns of $\mathbf{A}$ onto this quantized basis. We derive upper bounds on the approximation error of our algorithm, and analyze the impact of target rank and quantization bit-budget. The tradeoff between compression ratio and approximation accuracy allows for flexibility in choosing these parameters based on specific application requirements.我们实际实现了我们的算法，并在图像压缩、图像和文本嵌入图像的最近邻紧类фикации以及LLaMa-$7$b层的压缩中进行了实验。我们的结果表明，我们可以达到一个比特为一个矩阵坐标的压缩比，同时超越或保持传统压缩技术的性能。
</details></li>
</ul>
<hr>
<h2 id="SignGT-Signed-Attention-based-Graph-Transformer-for-Graph-Representation-Learning"><a href="#SignGT-Signed-Attention-based-Graph-Transformer-for-Graph-Representation-Learning" class="headerlink" title="SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning"></a>SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11025">http://arxiv.org/abs/2310.11025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinsong Chen, Gaichao Li, John E. Hopcroft, Kun He</li>
<li>for: 这 paper 的目的是提出一种基于签名自注意力的图Transformers，以适应不同图的复杂关系。</li>
<li>methods: 该 paper 使用了自注意力机制，并提出了一种新的签名自注意力机制（SignSA），以便根据节点对的 semantic relevance 生成签名注意力值。此外， paper 还提出了一种结构意识Feed-Forward Network（SFFN），以保留地方topology信息。</li>
<li>results: EXTENSIVE empirical results 表明，SignGT 在 node-level 和 graph-level 任务上表现出色，超过了当前的图Transformers 和高级 GNNs。<details>
<summary>Abstract</summary>
The emerging graph Transformers have achieved impressive performance for graph representation learning over graph neural networks (GNNs). In this work, we regard the self-attention mechanism, the core module of graph Transformers, as a two-step aggregation operation on a fully connected graph. Due to the property of generating positive attention values, the self-attention mechanism is equal to conducting a smooth operation on all nodes, preserving the low-frequency information. However, only capturing the low-frequency information is inefficient in learning complex relations of nodes on diverse graphs, such as heterophily graphs where the high-frequency information is crucial. To this end, we propose a Signed Attention-based Graph Transformer (SignGT) to adaptively capture various frequency information from the graphs. Specifically, SignGT develops a new signed self-attention mechanism (SignSA) that produces signed attention values according to the semantic relevance of node pairs. Hence, the diverse frequency information between different node pairs could be carefully preserved. Besides, SignGT proposes a structure-aware feed-forward network (SFFN) that introduces the neighborhood bias to preserve the local topology information. In this way, SignGT could learn informative node representations from both long-range dependencies and local topology information. Extensive empirical results on both node-level and graph-level tasks indicate the superiority of SignGT against state-of-the-art graph Transformers as well as advanced GNNs.
</details>
<details>
<summary>摘要</summary>
新出现的图变换器技术已经取得了图表示学习中的出色表现，超过传统的图神经网络（GNNs）。在这项工作中，我们将自注意机制，变换器的核心模块，视为一个完全连接的图上的两步积算操作。由于生成正向注意值的性质，自注意机制等同于对所有节点进行缓和操作，保留低频信息。然而，只capture低频信息可能是学习多样性图上的节点关系不充分的。为此，我们提出了一种签名自注意机制基于图变换器（SignGT），以适应不同图上的多样性频率信息。具体来说，SignGT开发了一种新的签名自注意机制（SignSA），生成签名注意值根据节点对的semantic relevance。因此，不同节点对之间的多样频率信息可以得到细致的保留。此外，SignGT还提出了一种结构意识适应链接网络（SFFN），通过引入邻居偏好来保留本地链接信息。因此，SignGT可以从长距离依赖和本地链接信息中学习有用的节点表示。 empirical研究表明，SignGT在节点级和图级任务上表现出色，超过当前的图变换器和高级GNNs。
</details></li>
</ul>
<hr>
<h2 id="Pure-Exploration-in-Asynchronous-Federated-Bandits"><a href="#Pure-Exploration-in-Asynchronous-Federated-Bandits" class="headerlink" title="Pure Exploration in Asynchronous Federated Bandits"></a>Pure Exploration in Asynchronous Federated Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11015">http://arxiv.org/abs/2310.11015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichen Wang, Chuanhao Li, Chenyu Song, Lianghui Wang, Quanquan Gu, Huazheng Wang</li>
<li>for: 这个论文是为了解决多机器人投掷机制中的联合探索问题，即多个机器人在中央服务器的协作下，找到最佳投掷机制。</li>
<li>methods: 这篇论文提出了首个在异步环境下的联合探索多机器人投掷机制和线性投掷机制算法，以提高实际场景中agent的缺失和延迟的Robustness。</li>
<li>results: 论文的理论分析表明，提议的算法在异步环境下可以 дости到近似优化的样本复杂度和有效的通信成本，并且基于实验数据表明，这些算法在实际场景中具有高效性和通信成本的优势。<details>
<summary>Abstract</summary>
We study the federated pure exploration problem of multi-armed bandits and linear bandits, where $M$ agents cooperatively identify the best arm via communicating with the central server. To enhance the robustness against latency and unavailability of agents that are common in practice, we propose the first federated asynchronous multi-armed bandit and linear bandit algorithms for pure exploration with fixed confidence. Our theoretical analysis shows the proposed algorithms achieve near-optimal sample complexities and efficient communication costs in a fully asynchronous environment. Moreover, experimental results based on synthetic and real-world data empirically elucidate the effectiveness and communication cost-efficiency of the proposed algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究多机构共同探索多臂枪和线性枪问题，其中 $M$ 名代理人共同决定最佳臂via 与中央服务器的通信。为了增强实际中常见的延迟和代理人缺失的响应性，我们提出了首个联邦异步多臂枪和线性枪探索算法，并进行了对这些算法的理论分析。我们的分析结果显示，提案的算法可以实现近乎最佳的样本复杂度和有效的通信成本在完全异步环境中。此外，基于实验数据的实验结果也证明了提案的算法的实际性和通信成本效率。
</details></li>
</ul>
<hr>
<h2 id="Hyperspectral-In-Memory-Computing-with-Optical-Frequency-Combs-and-Programmable-Optical-Memories"><a href="#Hyperspectral-In-Memory-Computing-with-Optical-Frequency-Combs-and-Programmable-Optical-Memories" class="headerlink" title="Hyperspectral In-Memory Computing with Optical Frequency Combs and Programmable Optical Memories"></a>Hyperspectral In-Memory Computing with Optical Frequency Combs and Programmable Optical Memories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11014">http://arxiv.org/abs/2310.11014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Honari Latifpour, Byoung Jun Park, Yoshihisa Yamamoto, Myoung-Gyun Suh</li>
<li>for:  This paper aims to develop a highly parallel, programmable, and scalable optical computing system capable of handling matrix-vector multiplication operations for deep learning and optimization tasks.</li>
<li>methods: The proposed hyperspectral in-memory computing architecture integrates space multiplexing with frequency multiplexing of optical frequency combs and uses spatial light modulators as a programmable optical memory.</li>
<li>results: The authors have experimentally demonstrated multiply-accumulate operations with higher than 4-bit precision in both matrix-vector and matrix-matrix multiplications, which suggests the system’s potential for a wide variety of deep learning and optimization tasks.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是开发一种高并行、可编程、可扩展的光学计算机系统，用于处理深度学习和优化任务中的矩阵-向量乘法操作。</li>
<li>methods: 提议的幽挺色响应计算机架构，结合空间多plexing和频率多plexing的光频率剖面，使用空间光模ulator作为可编程光学记忆。</li>
<li>results: 作者们实验表明，在矩阵-向量和矩阵-矩阵乘法操作中，可以实现高于4比特精度的乘法操作。<details>
<summary>Abstract</summary>
The rapid advancements in machine learning across numerous industries have amplified the demand for extensive matrix-vector multiplication operations, thereby challenging the capacities of traditional von Neumann computing architectures. To address this, researchers are currently exploring alternatives such as in-memory computing systems to develop faster and more energy-efficient hardware. In particular, there is renewed interest in computing systems based on optics, which could potentially handle matrix-vector multiplication in a more energy-efficient way. Despite promising initial results, developing a highly parallel, programmable, and scalable optical computing system capable of rivaling electronic computing hardware still remains elusive. In this context, we propose a hyperspectral in-memory computing architecture that integrates space multiplexing with frequency multiplexing of optical frequency combs and uses spatial light modulators as a programmable optical memory, thereby boosting the computational throughput and the energy efficiency. We have experimentally demonstrated multiply-accumulate operations with higher than 4-bit precision in both matrix-vector and matrix-matrix multiplications, which suggests the system's potential for a wide variety of deep learning and optimization tasks. This system exhibits extraordinary modularity, scalability, and programmability, effectively transcending the traditional limitations of optics-based computing architectures. Our approach demonstrates the potential to scale beyond peta operations per second, marking a significant step towards achieving high-throughput energy-efficient optical computing.
</details>
<details>
<summary>摘要</summary>
快速发展的机器学习技术在各个领域的应用使得大量矩阵-向量乘法操作的需求增加，导致传统的 von Neumann 计算架构的能力受到挑战。为了解决这问题，研究人员正在寻找代替方案，如内存计算系统，以开发更快速、更能效的硬件。特别是，光学计算系统在处理矩阵-向量乘法方面可能存在更高的能效性。虽然初步的结果很有前途，但是开发一个高度并行、可编程、扩展的光学计算系统，能与电子计算硬件竞争仍然很困难。在这种情况下，我们提出了一种快速响应的多光谱内存计算架构，通过空间复用和频率复用光谱镜的技术，使用空间光模拟器作为可编程的光学记忆，从而提高计算通过put和能效性。我们在实验中已经实现了高于4位精度的矩阵-向量和矩阵-矩阵乘法 multiply-accumulate 操作，这表明该系统在深度学习和优化任务中的潜在能力。这种系统具有极高的可组合性、可扩展性和可编程性，实际上跨越了传统光学计算架构的限制。我们的方法可以超过PETA操作每秒，这标志着光学计算技术的高性能、能效的发展。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Pairwise-Encodings-for-Link-Prediction"><a href="#Adaptive-Pairwise-Encodings-for-Link-Prediction" class="headerlink" title="Adaptive Pairwise Encodings for Link Prediction"></a>Adaptive Pairwise Encodings for Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11009">http://arxiv.org/abs/2310.11009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harryshomer/lpformer">https://github.com/harryshomer/lpformer</a></li>
<li>paper_authors: Harry Shomer, Yao Ma, Haitao Mao, Juanhui Li, Bo Wu, Jiliang Tang</li>
<li>for: 链接预测任务是图structured数据中常见的任务，有各种应用。过去通常使用手动设计的规则进行预测。</li>
<li>methods: 这些方法使用message-passing神经网络（MPNN）和规则方法的优点，通过对MPNN的输出和“对称编码”（pairwise encoding）进行组合来进行预测。这些方法在许多数据集上达到了强大的表现。但是，现有的对称编码往往带有强大的推导性偏见，使用同一些下面因素来分类所有链接。</li>
<li>results: LPFormer方法可以在许多数据集上达到SOTA表现，同时保持效率。<details>
<summary>Abstract</summary>
Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a new method, LPFormer, which attempts to adaptively learn the pairwise encodings for each link. LPFormer models the link factors via an attention module that learns the pairwise encoding that exists between nodes by modeling multiple factors integral to link prediction. Extensive experiments demonstrate that LPFormer can achieve SOTA performance on numerous datasets while maintaining efficiency.
</details>
<details>
<summary>摘要</summary>
链接预测是图Structured data上常见的任务，它在多个领域应用。过去，人工设计的规则通常用于这种任务。这些规则选择的目的是使其与下面的链接形成因素相吻合。在最近几年里，一种新的方法 emerge，它结合了 message-passing neural networks（MPNN）和规则方法的优点。这些方法通过 MPNN 的输出和候选链接的 "对称编码" 进行预测，其中对于每个链接， captured the relationship between nodes in the candidate link。它们在许多数据集上实现了强的表现。然而，现有的对称编码通常具有强 inductive bias，使用同一些基因来分类所有的链接。这限制了现有方法的能力，以learn how to properly classify a variety of different links that may form from different factors。为了解决这些限制，我们提出了一种新方法，LPFormer，它尝试通过 adaptively learning the pairwise encodings for each link来模型链接因素。LPFormer 通过注意力模块来学习每个链接的对称编码，该编码捕捉了 nodes 之间的多种因素。广泛的实验表明，LPFormer 可以在多个数据集上实现 SOTA 性能，同时保持效率。
</details></li>
</ul>
<hr>
<h2 id="Spatially-resolved-hyperlocal-weather-prediction-and-anomaly-detection-using-IoT-sensor-networks-and-machine-learning-techniques"><a href="#Spatially-resolved-hyperlocal-weather-prediction-and-anomaly-detection-using-IoT-sensor-networks-and-machine-learning-techniques" class="headerlink" title="Spatially-resolved hyperlocal weather prediction and anomaly detection using IoT sensor networks and machine learning techniques"></a>Spatially-resolved hyperlocal weather prediction and anomaly detection using IoT sensor networks and machine learning techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11001">http://arxiv.org/abs/2310.11001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anita B. Agarwal, Rohit Rajesh, Nitin Arul</li>
<li>for: 本研究旨在提供高精度、快速更新的本地天气预测，以满足各种应用需求，如农业、灾害管理等。</li>
<li>methods: 本研究提议一种新的方法， combining 本地天气预测和异常检测，使用互联网物理传感器网络和高级机器学习技术。该方法利用多个空间分布的、但相对较近的位置和物理传感器数据，创建高分辨率的天气模型，可预测短期、本地化的天气 Conditions。</li>
<li>results: 研究发现，该系统可以增强天气预测的空间分辨率，同时实时检测异常天气情况。此外，该系统还可以通过不监督学习算法，找到异常天气模式，为决策提供时间性信息。<details>
<summary>Abstract</summary>
Accurate and timely hyperlocal weather predictions are essential for various applications, ranging from agriculture to disaster management. In this paper, we propose a novel approach that combines hyperlocal weather prediction and anomaly detection using IoT sensor networks and advanced machine learning techniques. Our approach leverages data from multiple spatially-distributed yet relatively close locations and IoT sensors to create high-resolution weather models capable of predicting short-term, localized weather conditions such as temperature, pressure, and humidity. By monitoring changes in weather parameters across these locations, our system is able to enhance the spatial resolution of predictions and effectively detect anomalies in real-time. Additionally, our system employs unsupervised learning algorithms to identify unusual weather patterns, providing timely alerts. Our findings indicate that this system has the potential to enhance decision-making.
</details>
<details>
<summary>摘要</summary>
准确和及时的本地天气预测非常重要，用于各种应用，从农业到灾害管理。在这篇论文中，我们提出了一种新的方法，它结合了本地天气预测和异常检测，使用互联网器件网络和高级机器学习技术。我们的方法利用多个位于不同地点，但相对较近的位置和互联网器件来创建高分解能力的天气模型，能够预测短期、本地化的天气条件，如温度、压力和湿度。通过监测这些位置之间的天气参数变化，我们的系统可以提高地理分解能力，并实时检测异常。此外，我们的系统使用无监督学习算法来识别异常天气模式，提供实时警报。我们的发现表明，这种系统有可能提高决策。
</details></li>
</ul>
<hr>
<h2 id="Program-Translation-via-Code-Distillation"><a href="#Program-Translation-via-Code-Distillation" class="headerlink" title="Program Translation via Code Distillation"></a>Program Translation via Code Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11476">http://arxiv.org/abs/2310.11476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufan Huang, Mengnan Qi, Yongqiang Yao, Maoquan Wang, Bin Gu, Colin Clement, Neel Sundaresan</li>
<li>for: 这篇论文主要写于如何使用Code Distillation（CoDist）模型进行软件版本迁移和程序翻译。</li>
<li>methods: 这篇论文提出了一种基于Code Distillation（CoDist）模型的方法，该方法可以捕捉代码的semantic和structural等价性，并生成一种语言无关的中间表示。这种中间表示可以作为翻译的准则，从而生成并行的训练数据集，并且可以在任何编程语言上应用。</li>
<li>results: 根据CodeXGLUE和TransCoder GeeksForGeeks翻译测试 benchmark，这种方法可以达到当前最佳性能水平，与TransCoder-ST相比，增加了12.7%的平均绝对提升。<details>
<summary>Abstract</summary>
Software version migration and program translation are an important and costly part of the lifecycle of large codebases. Traditional machine translation relies on parallel corpora for supervised translation, which is not feasible for program translation due to a dearth of aligned data. Recent unsupervised neural machine translation techniques have overcome data limitations by included techniques such as back translation and low level compiler intermediate representations (IR). These methods face significant challenges due to the noise in code snippet alignment and the diversity of IRs respectively. In this paper we propose a novel model called Code Distillation (CoDist) whereby we capture the semantic and structural equivalence of code in a language agnostic intermediate representation. Distilled code serves as a translation pivot for any programming language, leading by construction to parallel corpora which scale to all available source code by simply applying the distillation compiler. We demonstrate that our approach achieves state-of-the-art performance on CodeXGLUE and TransCoder GeeksForGeeks translation benchmarks, with an average absolute increase of 12.7% on the TransCoder GeeksforGeeks translation benchmark compare to TransCoder-ST.
</details>
<details>
<summary>摘要</summary>
软件版本迁移和程序翻译是大型代码库生命周期中的重要和昂贵部分。传统机器翻译依赖平行 corpora 进行监督翻译，但对程序翻译来说不是可行的，因为没有准确的数据对齐。现代无监督神经机器翻译技术已经突破了数据限制，通过包括回翻译和低级编译器中间表示（IR）等技术。然而，这些方法面临着代码片段对齐的噪音和 IR 的多样性的挑战。在这篇论文中，我们提出了一种新的模型 called Code Distillation（CoDist），它可以捕捉代码的 semantics 和结构相似性，并将其转化为语言无关的中间表示。浓缩代码可以作为任何编程语言的翻译轮廓，从而自动生成平行 corpora，并且可以通过 simply 应用浓缩编译器来扩展到所有可用的源代码。我们示示了我们的方法可以在 CodeXGLUE 和 TransCoder GeeksForGeeks 翻译benchmark中达到状态机器翻译的性能水平，与TransCoder-ST 的平均绝对增幅为12.7%。
</details></li>
</ul>
<hr>
<h2 id="Why-Do-Students-Drop-Out-University-Dropout-Prediction-and-Associated-Factor-Analysis-Using-Machine-Learning-Techniques"><a href="#Why-Do-Students-Drop-Out-University-Dropout-Prediction-and-Associated-Factor-Analysis-Using-Machine-Learning-Techniques" class="headerlink" title="Why Do Students Drop Out? University Dropout Prediction and Associated Factor Analysis Using Machine Learning Techniques"></a>Why Do Students Drop Out? University Dropout Prediction and Associated Factor Analysis Using Machine Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10987">http://arxiv.org/abs/2310.10987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Kim, Eliot Yoo, Samuel Kim</li>
<li>for: 本研究旨在预测大学学生的毕业和退学情况，以便帮助教育机构和学生更好地规划教学和学习计划。</li>
<li>methods: 本研究使用学术、民生、社会经济和macro经济数据类型进行大学生毕业和退学预测。同时，我们进行了相关因素分析，以便分析这些数据类型对机器学习模型的表现有多大影响。</li>
<li>results: 我们使用这些特征训练了四个二分类器，以确定学生会毕业或退学。总的来说，这些模型在预测退学状况时的ROC-AUC分数为0.935。对于学术数据类型，模型性能最高，当排除所有学术相关特征时，模型性能下降到0.811。初步结果表明，数据类型和退学状况之间存在相关性。<details>
<summary>Abstract</summary>
Graduation and dropout rates have always been a serious consideration for educational institutions and students. High dropout rates negatively impact both the lives of individual students and institutions. To address this problem, this study examined university dropout prediction using academic, demographic, socioeconomic, and macroeconomic data types. Additionally, we performed associated factor analysis to analyze which type of data would be most influential on the performance of machine learning models in predicting graduation and dropout status. These features were used to train four binary classifiers to determine if students would graduate or drop out. The overall performance of the classifiers in predicting dropout status had an average ROC-AUC score of 0.935. The data type most influential to the model performance was found to be academic data, with the average ROC-AUC score dropping from 0.935 to 0.811 when excluding all academic-related features from the data set. Preliminary results indicate that a correlation does exist between data types and dropout status.
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese:毕业和退学率一直是教育机构和学生的严重考虑之一。高退学率对个人学生和机构都有负面影响。为了解决这个问题，本研究使用学术、人口、社会经济和 macroeconomic 数据类型进行大学退学预测。此外，我们还进行了相关因素分析，以分析这些数据类型对机器学习模型的执行性能有多大影响。这些特征被用来训练四个二分类器，以确定学生会毕业或退学。总的来说，这些分类器在预测退学状况时的 ROC-AUC 分数为 0.935。数据类型对模型性能最有影响的是学术数据，当排除所有学术相关特征时，模型的 ROC-AUC 分数从 0.935 下降至 0.811。初步结果表明，数据类型和退学状况之间存在相关性。
</details></li>
</ul>
<hr>
<h2 id="Exact-nonlinear-state-estimation"><a href="#Exact-nonlinear-state-estimation" class="headerlink" title="Exact nonlinear state estimation"></a>Exact nonlinear state estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10976">http://arxiv.org/abs/2310.10976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hristo G. Chipilski</li>
<li>for: 提高数据融合方法的准确性和稳定性，尤其是在高维模型中。</li>
<li>methods: 基于生成型人工智能技术的新非线性估计理论，拓展了现有的准 Gaussian 分布假设，提供了更高准确性和稳定性的数据融合方法。</li>
<li>results: 对理想化的统计实验进行了验证，结果表明，在观测错误小于预测不确定性和状态变量存在强非线性相互关系的情况下，ECTF 可以提供更高的准确性和稳定性。<details>
<summary>Abstract</summary>
The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While these assumptions facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Non-parametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the field of generative artificial intelligence (AI), this article introduces a new nonlinear estimation theory which attempts to bridge the existing gap in DA methodology. Specifically, a Conjugate Transform Filter (CTF) is derived and shown to generalize the celebrated Kalman filter to arbitrarily non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and convergence to highly accurate observations. An ensemble approximation of the new theory (ECTF) is also presented and validated using idealized statistical experiments that feature bounded quantities with non-Gaussian distributions, a prevalent challenge in Earth system models. Results from these experiments indicate that the greatest benefits from ECTF occur when observation errors are small relative to the forecast uncertainty and when state variables exhibit strong nonlinear dependencies. Ultimately, the new filtering theory offers exciting avenues for improving conventional DA algorithms through their principled integration with AI techniques.
</details>
<details>
<summary>摘要</summary>
大多数数据吸收（DA）方法在地球科学中基于 Gaussian 假设。这些假设使得算法效率高，但会导致分析偏误和预测质量下降。非Parametric, 粒子基本的 DA 算法具有更高的准确度，但在高维模型应用中仍存在操作挑战。本文从最近的生成式人工智能（AI）领域启发，提出一种新的非线性估计理论，以尝试桥接现有 DA 方法ología 的空缺。特别是，一种 conjugate transform filter（CTF）被 derivation 和证明能够泛化 kalman 筛到任意非 Gaussian 分布。新筛有多个愉悦性质，如保持先前状态的统计关系和 converge 到高精度观测。一种 ensemble approximation of the new theory（ECTF）也被提出和验证，使用 идеalized 统计实验，这些实验中的量均具有非 Gaussian 分布，是地球系统模型中的普遍问题。实验结果表明，ECTF 在观测Error 小于预测 uncertainty 以及状态变量具有强非线性关系时，具有最大的优势。最后，新的 filtering 理论提供了改进传统 DA 算法的原则性 интеграción with AI 技术的推动力。
</details></li>
</ul>
<hr>
<h2 id="SD-PINN-Deep-Learning-based-Spatially-Dependent-PDEs-Recovery"><a href="#SD-PINN-Deep-Learning-based-Spatially-Dependent-PDEs-Recovery" class="headerlink" title="SD-PINN: Deep Learning based Spatially Dependent PDEs Recovery"></a>SD-PINN: Deep Learning based Spatially Dependent PDEs Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10970">http://arxiv.org/abs/2310.10970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruixian Liu, Peter Gerstoft</li>
<li>for: 这篇论文是为了描述一种能够直接从物理测量数据中恢复部分偏微分方程（PDE）的含义的物理学习神经网络（PINN）的扩展。</li>
<li>methods: 该方法使用一个具有空间依赖性的物理学习神经网络（SD-PINN），可以通过单个神经网络来恢复空间依赖性的PDE含义。该方法还利用物理约束来降低噪声的影响。</li>
<li>results: 该方法可以充分利用物理约束来恢复PDE含义，并且可以在没有测量数据的情况下，通过含义低级假设来恢复PDE含义。<details>
<summary>Abstract</summary>
The physics-informed neural network (PINN) is capable of recovering partial differential equation (PDE) coefficients that remain constant throughout the spatial domain directly from physical measurements. In this work, we propose a spatially dependent physics-informed neural network (SD-PINN), which enables the recovery of coefficients in spatially-dependent PDEs using a single neural network, eliminating the requirement for domain-specific physical expertise. The proposed method exhibits robustness to noise owing to the incorporation of physical constraints. It can also incorporate the low-rank assumption of the spatial variation for the PDE coefficients to recover the coefficients at locations without available measurements.
</details>
<details>
<summary>摘要</summary>
физи学信息泛化神经网络（PINN）可以直接从物理测量中提取常数partial differential equation（PDE）的征量，这些征量在空间领域中保持常数。在这个工作中，我们提议使用空间依赖的 физи学信息泛化神经网络（SD-PINN），它使得可以使用单个神经网络来恢复空间依赖的PDE征量，消除了域pecific的物理专业知识的需求。该方法具有鲁棒性于噪声，并可以 incorporate 空间变化的low-rank假设来恢复征量在测量位置之外的位置。Note: "PINN" and "SD-PINN" in the text are abbreviations for "physics-informed neural network" and "spatially-dependent physics-informed neural network", respectively.
</details></li>
</ul>
<hr>
<h2 id="The-neural-network-models-with-delays-for-solving-absolute-value-equations"><a href="#The-neural-network-models-with-delays-for-solving-absolute-value-equations" class="headerlink" title="The neural network models with delays for solving absolute value equations"></a>The neural network models with delays for solving absolute value equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10965">http://arxiv.org/abs/2310.10965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongmei Yu, Gehao Zhang, Cairong Chen, Deren Han</li>
<li>for: 解决绝值方程 ($Ax - |x| - b &#x3D; 0$)</li>
<li>methods: 使用倒计时间神经网络模型和离散延迟神经网络模型，以及 Lyapunov-Krasovskii 理论和线性矩阵不等式（LMI）方法</li>
<li>results: 能够解决一类绝值方程，其中 $A^{-1}$ 的范数大于 1Here’s the breakdown of each point:1. For: This point states that the paper is written for solving the absolute value equation (AVE).2. Methods: This point lists the methods used in the paper, including the use of inverse-free neural network models with discrete delays, as well as the Lyapunov-Krasovskii theory and LMI method.3. Results: This point states the main result of the paper, which is that the proposed neural network models are exponentially convergent to the solution of the AVE, and can solve a class of AVE with $|A^{-1}|&gt;1$.<details>
<summary>Abstract</summary>
An inverse-free neural network model with mixed delays is proposed for solving the absolute value equation (AVE) $Ax -|x| - b =0$, which includes an inverse-free neural network model with discrete delay as a special case. By using the Lyapunov-Krasovskii theory and the linear matrix inequality (LMI) method, the developed neural network models are proved to be exponentially convergent to the solution of the AVE. Compared with the existing neural network models for solving the AVE, the proposed models feature the ability of solving a class of AVE with $\|A^{-1}\|>1$. Numerical simulations are given to show the effectiveness of the two delayed neural network models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>一种无反函数神经网络模型，包括杂度延迟，被提出来解决绝值方程（AVE） $Ax - |x| - b = 0$。这个模型包括杂度延迟神经网络模型作为特殊情况。通过利用利阿普涅夫-克拉索夫斯基理论和线性矩阵不等式（LMI）方法，我们证明了这些神经网络模型在AVE的解的抽象 convergent。与现有的神经网络模型相比，我们的模型可以解决一类AVE中，$ \|A^{-1}\|>1$。numerical simulations 给出了这两种延迟神经网络模型的效果。
</details></li>
</ul>
<hr>
<h2 id="A-Local-Graph-Limits-Perspective-on-Sampling-Based-GNNs"><a href="#A-Local-Graph-Limits-Perspective-on-Sampling-Based-GNNs" class="headerlink" title="A Local Graph Limits Perspective on Sampling-Based GNNs"></a>A Local Graph Limits Perspective on Sampling-Based GNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10953">http://arxiv.org/abs/2310.10953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeganeh Alimohammadi, Luana Ruiz, Amin Saberi</li>
<li>for: 这项研究旨在提供一种训练图神经网络（GNNs）的理论框架，用于处理大输入图。</li>
<li>methods: 这项研究使用了采样方法，将大输入图分解成小固定大小的子图进行训练。</li>
<li>results: 研究发现，通过采样训练GNNs，可以在减少训练时间和数据量的情况下，保持模型的性能。在一个节点分类任务中，研究发现，使用小型子图进行采样训练，可以达到与直接训练在原始图上的性能相似的水平。<details>
<summary>Abstract</summary>
We propose a theoretical framework for training Graph Neural Networks (GNNs) on large input graphs via training on small, fixed-size sampled subgraphs. This framework is applicable to a wide range of models, including popular sampling-based GNNs, such as GraphSAGE and FastGCN. Leveraging the theory of graph local limits, we prove that, under mild assumptions, parameters learned from training sampling-based GNNs on small samples of a large input graph are within an $\epsilon$-neighborhood of the outcome of training the same architecture on the whole graph. We derive bounds on the number of samples, the size of the graph, and the training steps required as a function of $\epsilon$. Our results give a novel theoretical understanding for using sampling in training GNNs. They also suggest that by training GNNs on small samples of the input graph, practitioners can identify and select the best models, hyperparameters, and sampling algorithms more efficiently. We empirically illustrate our results on a node classification task on large citation graphs, observing that sampling-based GNNs trained on local subgraphs 12$\times$ smaller than the original graph achieve comparable performance to those trained on the input graph.
</details>
<details>
<summary>摘要</summary>
我们提出一种理论框架，用于在大输入图上训练图神经网络（GNNs） via 训练小型、固定大小的采样子图。这个框架适用于各种采样基于GNNs，包括受欢迎的采样GNNs，如GraphSAGE和FastGCN。我们利用图本地限制理论，证明在某些假设下，从训练采样GNNs на小样本上获得的参数与训练同样模型在整个图上的参数在$\epsilon$-邻域内。我们 derive出参数数量、图的大小和训练步骤的上限，作为函数于$\epsilon$。我们的结果为使用采样在训练GNNs提供了新的理论理解，并表明通过训练GNNs于小样本上可以更加快速地确定最佳模型、 гиперпараметры和采样算法。我们在一个节点分类任务上对大量引用图进行了实验，发现采样基于GNNs训练于local子图12$\times$小于原始图的性能与训练于原始图相当。
</details></li>
</ul>
<hr>
<h2 id="Restricted-Tweedie-Stochastic-Block-Models"><a href="#Restricted-Tweedie-Stochastic-Block-Models" class="headerlink" title="Restricted Tweedie Stochastic Block Models"></a>Restricted Tweedie Stochastic Block Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10952">http://arxiv.org/abs/2310.10952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Jian, Mu Zhu, Peijun Sang</li>
<li>for: 本文旨在提出一种基于非正态 Tweedie 分布的随机块模型（SBM），用于社区检测网络中的非负零Inflated 连接量。</li>
<li>methods: 本文提出了一种新的 SBM 模型，使用restricted Tweedie distribution来模型连接量，并考虑了节点信息，如国家之间的地理距离。</li>
<li>results: 在大量的 simulations 和实际国际贸易数据中，本文显示了该模型的效果。特别是，当 nodes 数足够多时，计算最大 likelihood 参数的过程可以独立地计算节点标签。这使得可以开发一种高效的 two-step 算法，将 covariate 效应与其他参数分离。<details>
<summary>Abstract</summary>
The stochastic block model (SBM) is a widely used framework for community detection in networks, where the network structure is typically represented by an adjacency matrix. However, conventional SBMs are not directly applicable to an adjacency matrix that consists of non-negative zero-inflated continuous edge weights. To model the international trading network, where edge weights represent trading values between countries, we propose an innovative SBM based on a restricted Tweedie distribution. Additionally, we incorporate nodal information, such as the geographical distance between countries, and account for its dynamic effect on edge weights. Notably, we show that given a sufficiently large number of nodes, estimating this covariate effect becomes independent of community labels of each node when computing the maximum likelihood estimator of parameters in our model. This result enables the development of an efficient two-step algorithm that separates the estimation of covariate effects from other parameters. We demonstrate the effectiveness of our proposed method through extensive simulation studies and an application to real-world international trading data.
</details>
<details>
<summary>摘要</summary>
Stochastic block model (SBM) 是一种广泛使用的社区探测模型，用于网络结构的表示，通常是一个相对矩阵。然而，传统的 SBM 不直接适用于具有非负零填充连接权重的邻接矩阵。为了模型国际贸易网络，其中边权重表示国家之间贸易值，我们提出了一种创新的 SBM，基于限制的 Tweedie 分布。此外，我们还考虑了节点信息，如国家之间的地理距离，并考虑其动态影响边权重。我们发现，当节点数足够多时，计算每个节点社区标签的最大 LIKELIHOOD 估计器中计算 covariate 效应的结果，是独立的。这一结果允许我们开发一种高效的 two-step 算法，将 covariate 效应与其他参数分离。我们通过广泛的 simulations 研究和实际国际贸易数据应用，证明了我们提出的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Combat-Urban-Congestion-via-Collaboration-Heterogeneous-GNN-based-MARL-for-Coordinated-Platooning-and-Traffic-Signal-Control"><a href="#Combat-Urban-Congestion-via-Collaboration-Heterogeneous-GNN-based-MARL-for-Coordinated-Platooning-and-Traffic-Signal-Control" class="headerlink" title="Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control"></a>Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10948">http://arxiv.org/abs/2310.10948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianyue Peng, Hang Gao, Hao Wang, H. Michael Zhang</li>
<li>for: 提高交通流量和减少拥堵</li>
<li>methods: 使用多智能体学习和交通理论来解决异质性和协调问题，并设计了各自的观察、操作和奖励函数来优化交通流量</li>
<li>results: 通过 SUMO 模拟，实现了交通流量等多个指标的协调结果，并与独立的信号控制或队列控制相比，表现更佳。<details>
<summary>Abstract</summary>
Over the years, reinforcement learning has emerged as a popular approach to develop signal control and vehicle platooning strategies either independently or in a hierarchical way. However, jointly controlling both in real-time to alleviate traffic congestion presents new challenges, such as the inherent physical and behavioral heterogeneity between signal control and platooning, as well as coordination between them. This paper proposes an innovative solution to tackle these challenges based on heterogeneous graph multi-agent reinforcement learning and traffic theories. Our approach involves: 1) designing platoon and signal control as distinct reinforcement learning agents with their own set of observations, actions, and reward functions to optimize traffic flow; 2) designing coordination by incorporating graph neural networks within multi-agent reinforcement learning to facilitate seamless information exchange among agents on a regional scale. We evaluate our approach through SUMO simulation, which shows a convergent result in terms of various transportation metrics and better performance over sole signal or platooning control.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Designing platoon and signal control as distinct reinforcement learning agents with their own set of observations, actions, and reward functions to optimize traffic flow.2. Designing coordination by incorporating graph neural networks within multi-agent reinforcement learning to facilitate seamless information exchange among agents on a regional scale.We evaluate our approach through SUMO simulation, which shows a convergent result in terms of various transportation metrics and better performance over sole signal or platooning control.Translation notes:* “signal control” and “vehicle platooning” are translated as “信号控制” and “车辆队列”, respectively.* “heterogeneous graph multi-agent reinforcement learning” is translated as “多代理信号控制与车辆队列强化学习”.* “coordination” is translated as “协调”.* “ SUMO simulation” is translated as “SUMO仿真”.</details></li>
</ol>
<hr>
<h2 id="Multi-point-Feedback-of-Bandit-Convex-Optimization-with-Hard-Constraints"><a href="#Multi-point-Feedback-of-Bandit-Convex-Optimization-with-Hard-Constraints" class="headerlink" title="Multi-point Feedback of Bandit Convex Optimization with Hard Constraints"></a>Multi-point Feedback of Bandit Convex Optimization with Hard Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10946">http://arxiv.org/abs/2310.10946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasunari Hikima</li>
<li>for: 本 paper 研究了带约束的带射搜索优化问题，learner 需要在部分损失函数信息的情况下生成一个序列决策，以实现总损失降低和约束违反降低。</li>
<li>methods: 我们采用了累积硬约束违反度作为约束违反度的度量，即 $\sum_{t&#x3D;1}^{T} \max{g_t(\boldsymbol{x}_t), 0}$. 由于最大运算，不可能通过不满足约束的解释取消约束违反的效果，与传统的长期约束违反度不同。我们提出了一种罚款基于的 proximal 梯度下降法，可以在梯度估计中使用两点函数评估。</li>
<li>results: 我们的算法可以实现 $O(d^2T^{\max{c,1-c}})$  regret bounds和 $O(d^2T^{1-\frac{c}{2})$ 约束违反度 bounds，其中 $d$ 是可行区域的维度，$c\in[\frac{1}{2}, 1)$ 是用户决定的参数。我们还扩展了结果到损失函数是强 convex 时的情况，并证明了 regret 和约束违反度 bounds 可以进一步降低。<details>
<summary>Abstract</summary>
This paper studies bandit convex optimization with constraints, where the learner aims to generate a sequence of decisions under partial information of loss functions such that the cumulative loss is reduced as well as the cumulative constraint violation is simultaneously reduced. We adopt the cumulative \textit{hard} constraint violation as the metric of constraint violation, which is defined by $\sum_{t=1}^{T} \max\{g_t(\boldsymbol{x}_t), 0\}$. Owing to the maximum operator, a strictly feasible solution cannot cancel out the effects of violated constraints compared to the conventional metric known as \textit{long-term} constraints violation. We present a penalty-based proximal gradient descent method that attains a sub-linear growth of both regret and cumulative hard constraint violation, in which the gradient is estimated with a two-point function evaluation. Precisely, our algorithm attains $O(d^2T^{\max\{c,1-c\})$ regret bounds and $O(d^2T^{1-\frac{c}{2})$ cumulative hard constraint violation bounds for convex loss functions and time-varying constraints, where $d$ is the dimensionality of the feasible region and $c\in[\frac{1}{2}, 1)$ is a user-determined parameter. We also extend the result for the case where the loss functions are strongly convex and show that both regret and constraint violation bounds can be further reduced.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reaching-the-Limit-in-Autonomous-Racing-Optimal-Control-versus-Reinforcement-Learning"><a href="#Reaching-the-Limit-in-Autonomous-Racing-Optimal-Control-versus-Reinforcement-Learning" class="headerlink" title="Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning"></a>Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10943">http://arxiv.org/abs/2310.10943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunlong Song, Angel Romero, Matthias Mueller, Vladlen Koltun, Davide Scaramuzza</li>
<li>for: This paper aims to design a control system for an agile mobile robot, specifically in the context of autonomous drone racing.</li>
<li>methods: The paper uses reinforcement learning (RL) to train a neural network controller, which outperforms optimal control (OC) methods in this setting.</li>
<li>results: The RL controller achieves superhuman control performance within minutes of training on a standard workstation, achieving a peak acceleration greater than 12 times the gravitational acceleration and a peak velocity of 108 kilometers per hour.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是设计一个适应度高的移动机器人控制系统，具体是在自动驾驶飞机比赛中进行。</li>
<li>methods: 这篇论文使用反馈学习（RL）来训练一个神经网络控制器，RL控制器在这个设定中超过优化控制（OC）方法的性能。</li>
<li>results: RL控制器在标准工作站上训练了几分钟后已经实现了人类控制性能的超越，达到了12倍重力加速度的峰值加速和108公里&#x2F;小时的峰值速度。<details>
<summary>Abstract</summary>
A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning (RL) outperformed optimal control (OC) methods in this setting. We then investigated which fundamental factors have contributed to the success of RL or have limited OC. Our study indicates that the fundamental advantage of RL over OC is not that it optimizes its objective better but that it optimizes a better objective. OC decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, RL can directly optimize a task-level objective and can leverage domain randomization to cope with model uncertainty, allowing the discovery of more robust control responses. Our findings allowed us to push an agile drone to its maximum performance, achieving a peak acceleration greater than 12 times the gravitational acceleration and a peak velocity of 108 kilometers per hour. Our policy achieved superhuman control within minutes of training on a standard workstation. This work presents a milestone in agile robotics and sheds light on the role of RL and OC in robot control.
</details>
<details>
<summary>摘要</summary>
中心问题在 роботике是如何设计一个敏捷移动机器人的控制系统。这篇论文系统地研究这个问题，专注于一个挑战性的设定：自主无人机赛车。我们表明，使用强化学习（RL）训练的神经网络控制器在这个设定中表现得更好于优化控制（OC）方法。然后，我们研究了RL和OC之间的基本因素，发现RL的优势不在于更好地优化目标函数，而是在于优化更好的目标函数。OC将问题分解为规划和控制两个部分，使用显式中间表示（如轨迹）作为控制器的界面，这种分解限制控制器可表达的行为范围，导致面临不Modeled Effects时的控制性下降。相比之下，RL可以直接优化任务级目标函数，并通过随机化预测来快速适应模型不确定性，从而发现更加 Robust control response。我们的发现使我们可以将敏捷无人机 pushed to its maximum performance，达到了12倍重力加速度的峰值加速和108公里/小时的峰值速度。我们的策略在标准工作站上训练仅需几分钟便可以达到超人控制水平。这项工作为敏捷 роботиcs带来了里程碑，也照亮了RL和OC在机器人控制中的角色。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Simple-Spectral-Clustering-in-Theory-and-Practice"><a href="#Fast-and-Simple-Spectral-Clustering-in-Theory-and-Practice" class="headerlink" title="Fast and Simple Spectral Clustering in Theory and Practice"></a>Fast and Simple Spectral Clustering in Theory and Practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10939">http://arxiv.org/abs/2310.10939</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pmacg/fast-spectral-clustering">https://github.com/pmacg/fast-spectral-clustering</a></li>
<li>paper_authors: Peter Macgregor</li>
<li>for: 寻找图像中的k个团体</li>
<li>methods: 使用频谱归一化法，使用力量法计算顺序 embed 图像中的顶点</li>
<li>results: 可以快速地找到图像中的团体，并且准确地回归真实的团体结果<details>
<summary>Abstract</summary>
Spectral clustering is a popular and effective algorithm designed to find $k$ clusters in a graph $G$. In the classical spectral clustering algorithm, the vertices of $G$ are embedded into $\mathbb{R}^k$ using $k$ eigenvectors of the graph Laplacian matrix. However, computing this embedding is computationally expensive and dominates the running time of the algorithm. In this paper, we present a simple spectral clustering algorithm based on a vertex embedding with $O(\log(k))$ vectors computed by the power method. The vertex embedding is computed in nearly-linear time with respect to the size of the graph, and the algorithm provably recovers the ground truth clusters under natural assumptions on the input graph. We evaluate the new algorithm on several synthetic and real-world datasets, finding that it is significantly faster than alternative clustering algorithms, while producing results with approximately the same clustering accuracy.
</details>
<details>
<summary>摘要</summary>
spectral clustering 是一种流行的有效算法，用于在图 G 中找到 $k$ 个群。 classical spectral clustering 算法中，图 vertices 被嵌入到 $\mathbb{R}^k$ 中使用 $k$ 个图 Laplacian 矩阵的特征值。然而，计算这个嵌入是计算成本高昂，对算法的运行时间产生很大影响。在本文中，我们提出了一种简单的 spectral clustering 算法，基于一个 $O(\log(k))$ 维的顶点嵌入，通过力方法计算。顶点嵌入的计算时间与图的大小近似线性，而且算法可以证明地回归真实的群集，以下面的自然假设。我们对这种新算法在一些 sintetic 和实际世界数据集上进行了评估，发现它比其他归一化算法更快速，并且生成的结果与实际结果相似。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-in-the-Quantum-Age-Quantum-vs-Classical-Support-Vector-Machines"><a href="#Machine-Learning-in-the-Quantum-Age-Quantum-vs-Classical-Support-Vector-Machines" class="headerlink" title="Machine Learning in the Quantum Age: Quantum vs. Classical Support Vector Machines"></a>Machine Learning in the Quantum Age: Quantum vs. Classical Support Vector Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10910">http://arxiv.org/abs/2310.10910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davut Emre Tasar, Kutan Koruyan, Ceren Ocal Tasar</li>
<li>For: 本研究探讨机器学习算法在经典和量子计算模式下的效率。具体来说，通过强调支持向量机器（SVM），我们研究了使用量子硬件进行分类的量子支持向量机器（QSVM）的性能。* Methods: 本研究采用了一系列实验，使用Qiskit库进行实现，并进行了参数优化。* Results: 发现在某些情况下，QSVM可以与经典SVM匹敌，但是现在的执行时间比较长。此外，我们发现，随着量子计算能力的提高和平行计算的增加，量子机器学习算法的性能可以得到明显改善。这项研究为未来量子机器学习应用提供了有价值的信息。<details>
<summary>Abstract</summary>
This work endeavors to juxtapose the efficacy of machine learning algorithms within classical and quantum computational paradigms. Particularly, by emphasizing on Support Vector Machines (SVM), we scrutinize the classification prowess of classical SVM and Quantum Support Vector Machines (QSVM) operational on quantum hardware over the Iris dataset. The methodology embraced encapsulates an extensive array of experiments orchestrated through the Qiskit library, alongside hyperparameter optimization. The findings unveil that in particular scenarios, QSVMs extend a level of accuracy that can vie with classical SVMs, albeit the execution times are presently protracted. Moreover, we underscore that augmenting quantum computational capacity and the magnitude of parallelism can markedly ameliorate the performance of quantum machine learning algorithms. This inquiry furnishes invaluable insights regarding the extant scenario and future potentiality of machine learning applications in the quantum epoch. Colab: https://t.ly/QKuz0
</details>
<details>
<summary>摘要</summary>
Note:* " classical" and "quantum" are translated as "古典" and "量子" respectively.* "computational paradigms" is translated as "计算框架"* "Support Vector Machines" is translated as "支持向量机"* "Quantum Support Vector Machines" is translated as "量子支持向量机"* "hyperparameter optimization" is translated as "超参数优化"* " execution times" is translated as "执行时间"* "quantum computational capacity" is translated as "量子计算能力"* "magnitude of parallelism" is translated as "并行性的大小"
</details></li>
</ul>
<hr>
<h2 id="Heterogenous-Memory-Augmented-Neural-Networks"><a href="#Heterogenous-Memory-Augmented-Neural-Networks" class="headerlink" title="Heterogenous Memory Augmented Neural Networks"></a>Heterogenous Memory Augmented Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10909">http://arxiv.org/abs/2310.10909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiuzh20/hma">https://github.com/qiuzh20/hma</a></li>
<li>paper_authors: Zihan Qiu, Zhen Liu, Shuicheng Yan, Shanghang Zhang, Jie Fu</li>
<li>for: 本研究旨在提出一种基于异构记忆扩展的神经网络方法，以提高神经网络在数据稀缺和 OUT-OF-DISTRIBUTION（OOD）场景中的表现。</li>
<li>methods: 该方法通过引入学习记忆标记和注意机制，使得神经网络可以更好地处理大量数据，而无需增加巨大的计算成本。</li>
<li>results: 经过广泛的实验证明，该方法可以与不同的底层模型（MLP、CNN、GNN和Transformer）结合使用，并在图像和图格等任务下显示出竞争性的表现，特别是在数据稀缺和OOD情况下。<details>
<summary>Abstract</summary>
It has been shown that semi-parametric methods, which combine standard neural networks with non-parametric components such as external memory modules and data retrieval, are particularly helpful in data scarcity and out-of-distribution (OOD) scenarios. However, existing semi-parametric methods mostly depend on independent raw data points - this strategy is difficult to scale up due to both high computational costs and the incapacity of current attention mechanisms with a large number of tokens. In this paper, we introduce a novel heterogeneous memory augmentation approach for neural networks which, by introducing learnable memory tokens with attention mechanism, can effectively boost performance without huge computational overhead. Our general-purpose method can be seamlessly combined with various backbones (MLP, CNN, GNN, and Transformer) in a plug-and-play manner. We extensively evaluate our approach on various image and graph-based tasks under both in-distribution (ID) and OOD conditions and show its competitive performance against task-specific state-of-the-art methods. Code is available at \url{https://github.com/qiuzh20/HMA}.
</details>
<details>
<summary>摘要</summary>
研究表明，半 Parametric 方法，将标准神经网络与非 Parametric 组件相结合，如外部记忆模块和数据检索，在数据缺乏和外部分布（OOD）场景中特别有帮助。然而，现有的半 Parametric 方法通常依赖于独立的原始数据点，这种策略难以扩展，因为它们的计算成本高，以及当前的注意机制无法处理大量的 tokens。在这篇文章中，我们介绍了一种新的异 heterogeneous 记忆扩展方法，通过引入学习记忆тоoken和注意机制，可以有效提高性能，而无需巨大的计算负担。我们的通用方法可以与不同的基础结构（MLP、CNN、GNN、Transformer）混合使用，并且可以在插入式方式下运行。我们对各种图像和图Structured 任务进行了广泛的评估，并在ID和OOD条件下显示了与任务特有的状态UNTUK 的竞争性性能。代码可以在 \url{https://github.com/qiuzh20/HMA} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Surrogate-Active-Subspaces-for-Jump-Discontinuous-Functions"><a href="#Surrogate-Active-Subspaces-for-Jump-Discontinuous-Functions" class="headerlink" title="Surrogate Active Subspaces for Jump-Discontinuous Functions"></a>Surrogate Active Subspaces for Jump-Discontinuous Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10907">http://arxiv.org/abs/2310.10907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Wycoff</li>
<li>for: 该研究旨在探讨Surrogate模型和活动子空间在社会科学计算中的应用，特别是对于粒子模型，以及这些技术在处理离散函数时的限制。</li>
<li>methods: 该研究使用了Gaussian процеessed估计active subspace，并对离散函数进行了扩展，以便更好地理解估计中的量。数据进行了比较分析，并在Flee模型中进行了应用，以获得关于8个迁徙危机的新的发现。</li>
<li>results: 研究发现，在离散函数上使用Gaussian процеessed估计active subspace可以提供有用的信息，并且可以帮助理解模型中重要的参数。在Flee模型中，该方法提供了新的发现，对8个迁徙危机进行了分析。<details>
<summary>Abstract</summary>
Surrogate modeling and active subspaces have emerged as powerful paradigms in computational science and engineering. Porting such techniques to computational models in the social sciences brings into sharp relief their limitations in dealing with discontinuous simulators, such as Agent-Based Models, which have discrete outputs. Nevertheless, prior applied work has shown that surrogate estimates of active subspaces for such estimators can yield interesting results. But given that active subspaces are defined by way of gradients, it is not clear what quantity is being estimated when this methodology is applied to a discontinuous simulator. We begin this article by showing some pathologies that can arise when conducting such an analysis. This motivates an extension of active subspaces to discontinuous functions, clarifying what is actually being estimated in such analyses. We also conduct numerical experiments on synthetic test functions to compare Gaussian process estimates of active subspaces on continuous and discontinuous functions. Finally, we deploy our methodology on Flee, an agent-based model of refugee movement, yielding novel insights into which parameters of the simulation are most important across 8 displacement crises in Africa and the Middle East.
</details>
<details>
<summary>摘要</summary>
《代理模型和活跃子空间在计算科学和工程领域已经成为强大的趋势。将这些技术应用到社会科学计算模型上可以使其局限性得到鲜明的表现。然而，先前的应用研究表明，代理估计的活跃子空间可以得到有趣的结果。但是，由于活跃子空间是通过梯度定义的，因此不清楚什么量在这种分析中是被估计的。本文开始通过显示一些在进行这种分析时可能出现的病理来激发扩展。这些病理的出现强化了我们对离散函数的扩展。我们还对 synthetic 测试函数进行了数值实验，比较了 Gaussian 过程的活跃子空间估计在连续和离散函数上。最后，我们将我们的方法应用于 Flee，一个基于代理的难民移动模型，并得到了8个驱逐危机在非洲和中东的新的发现。》Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and widely used in other countries. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Modularity-Maximization-in-Approximation-Heuristic-and-Graph-Neural-Network-Algorithms-for-Community-Detection"><a href="#Analyzing-Modularity-Maximization-in-Approximation-Heuristic-and-Graph-Neural-Network-Algorithms-for-Community-Detection" class="headerlink" title="Analyzing Modularity Maximization in Approximation, Heuristic, and Graph Neural Network Algorithms for Community Detection"></a>Analyzing Modularity Maximization in Approximation, Heuristic, and Graph Neural Network Algorithms for Community Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10898">http://arxiv.org/abs/2310.10898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samin Aref, Mahdi Mostajabdaveh</li>
<li>for: 本研究旨在探讨不同模块化最优化算法在网络Community detection中的性能。</li>
<li>methods: 本研究使用了104个网络，包括了真实世界的例子和Synthetic图的模块结构。研究使用了10种模块最优化算法，包括8种搜索算法、2种变种的图神经网络算法和Bayan Approximation算法的多种变种。</li>
<li>results: 研究发现，大多数常用的模块性最优化算法 rarely produce an optimal partition or a partition resembling an optimal partition，即使网络具有模块结构。如果使用模块性来探讨社群，则应用近似优化算法更加合理。<details>
<summary>Abstract</summary>
Community detection, a fundamental problem in computational sciences, finds applications in various domains. Heuristics are often employed to detect communities through maximizing an objective function, modularity, over partitions of network nodes. Our research delves into the performance of different modularity maximization algorithms in achieving optimal partitions. We use 104 networks, comprising real-world instances from diverse contexts and synthetic graphs with modular structures. We analyze ten inexact modularity-based algorithms against an exact baseline which is an exact integer programming method that globally optimizes modularity. The ten algorithms analyzed include eight heuristics, two variations of a graph neural network algorithm, and several variations of the Bayan approximation algorithm. Our analysis uncovers substantial dissimilarities between the partitions obtained by most commonly used modularity-based methods and any optimal partition of the networks, as indicated by both adjusted and reduced mutual information metrics. Importantly, our results show that near-optimal partitions are often disproportionately dissimilar to any optimal partition. Taken together, our analysis points to a crucial limitation of the commonly used unguaranteed modularity-based methods for discovering communities: they rarely produce an optimal partition or a partition resembling an optimal partition even on networks with modular structures. If modularity is to be used for detecting communities, approximate optimization algorithms are recommendable for a more methodologically sound usage of modularity within its applicability limits.
</details>
<details>
<summary>摘要</summary>
We analyze ten inexact modularity-based algorithms, including eight heuristics, two variations of a graph neural network algorithm, and several variations of the Bayan approximation algorithm, against an exact baseline that globally optimizes modularity using integer programming. Our analysis reveals that the partitions obtained by most commonly used modularity-based methods are often substantially dissimilar to any optimal partition, as indicated by both adjusted and reduced mutual information metrics. Furthermore, we find that near-optimal partitions are often disproportionately dissimilar to any optimal partition.Our results suggest that the commonly used unguaranteed modularity-based methods for discovering communities are rarely able to produce an optimal partition or a partition resembling an optimal partition, even on networks with modular structures. As a result, we recommend using approximate optimization algorithms for a more methodologically sound usage of modularity within its applicability limits.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/cs.LG_2023_10_17/" data-id="closbrorx00rd0g883osd89ni" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/eess.IV_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T09:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/eess.IV_2023_10_17/">eess.IV - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hybrid-quantum-classical-graph-neural-networks-for-tumor-classification-in-digital-pathology"><a href="#Hybrid-quantum-classical-graph-neural-networks-for-tumor-classification-in-digital-pathology" class="headerlink" title="Hybrid quantum-classical graph neural networks for tumor classification in digital pathology"></a>Hybrid quantum-classical graph neural networks for tumor classification in digital pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11353">http://arxiv.org/abs/2310.11353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anupama Ray, Dhiraj Madan, Srushti Patil, Maria Anna Rapsomaniki, Pushpak Pati</li>
<li>for: 研究用于理解疾病细胞和肿瘤微环境之间的互动，以加速治疗发现。</li>
<li>methods: 组合图解树神经网络（GNN）和量子变量分类器（VQC），以解决抗癌疾病分类任务。</li>
<li>results: hybrid量子神经网络（QNN）与状态当前的类 graph神经网络（GNN）相当，以重量精度、准确率和F1分数来衡量。 Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 随着古典机器学习和单元细胞技术的进步，我们可以更好地理解疾病细胞和肿瘤微环境之间的互动，以加速治疗发现。</li>
<li>methods: 我们采用了组合图解树神经网络（GNN）和量子变量分类器（VQC），以解决抗癌疾病分类任务。</li>
<li>results: 我们发现，hybrid量子神经网络（QNN）与状态当前的类 graph神经网络（GNN）相当，以重量精度、准确率和F1分数来衡量。 另外，我们还发现，通过幂数编码，可以压缩信息，并且在逻辑数量的级别上实现更好的性能。最后，我们发现，结合练习可以超越固定 GNN 参数，并且也略微提高了与 vanilla GNN 的性能。<details>
<summary>Abstract</summary>
Advances in classical machine learning and single-cell technologies have paved the way to understand interactions between disease cells and tumor microenvironments to accelerate therapeutic discovery. However, challenges in these machine learning methods and NP-hard problems in spatial Biology create an opportunity for quantum computing algorithms. We create a hybrid quantum-classical graph neural network (GNN) that combines GNN with a Variational Quantum Classifier (VQC) for classifying binary sub-tasks in breast cancer subtyping. We explore two variants of the same, the first with fixed pretrained GNN parameters and the second with end-to-end training of GNN+VQC. The results demonstrate that the hybrid quantum neural network (QNN) is at par with the state-of-the-art classical graph neural networks (GNN) in terms of weighted precision, recall and F1-score. We also show that by means of amplitude encoding, we can compress information in logarithmic number of qubits and attain better performance than using classical compression (which leads to information loss while keeping the number of qubits required constant in both regimes). Finally, we show that end-to-end training enables to improve over fixed GNN parameters and also slightly improves over vanilla GNN with same number of dimensions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Automatic-Coronary-Artery-Plaque-Quantification-and-CAD-RADS-Prediction-using-Mesh-Priors"><a href="#Automatic-Coronary-Artery-Plaque-Quantification-and-CAD-RADS-Prediction-using-Mesh-Priors" class="headerlink" title="Automatic Coronary Artery Plaque Quantification and CAD-RADS Prediction using Mesh Priors"></a>Automatic Coronary Artery Plaque Quantification and CAD-RADS Prediction using Mesh Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11297">http://arxiv.org/abs/2310.11297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rudolf L. M. van Herten, Nils Hampe, Richard A. P. Takx, Klaas Jan Franssen, Yining Wang, Dominika Suchá, José P. Henriques, Tim Leiner, R. Nils Planken, Ivana Išgum</li>
<li>for: 这个论文是为了评估抗塞性肺动脉疾病（CAD）的风险和治疗方案。</li>
<li>methods: 这个论文使用了直接从中心线约束的方法，将抗塞性肺动脉的血管和瘤直接推导出来，并用其进行下游任务的CAD-RADS分类。</li>
<li>results: 这个论文的结果表明，直接推导抗塞性肺动脉的血管和瘤的方法是可行的，并可以自动预测 Routinely performed CAD-RADS categorization。 lesion-wise volume intraclass correlation coefficients were 0.98, 0.79, and 0.85 for calcified, non-calcified, and total plaque volume respectively. patient-level CAD-RADS categorization achieved linearly weighted kappa（κ）of 0.75.<details>
<summary>Abstract</summary>
Coronary artery disease (CAD) remains the leading cause of death worldwide. Patients with suspected CAD undergo coronary CT angiography (CCTA) to evaluate the risk of cardiovascular events and determine the treatment. Clinical analysis of coronary arteries in CCTA comprises the identification of atherosclerotic plaque, as well as the grading of any coronary artery stenosis typically obtained through the CAD-Reporting and Data System (CAD-RADS). This requires analysis of the coronary lumen and plaque. While voxel-wise segmentation is a commonly used approach in various segmentation tasks, it does not guarantee topologically plausible shapes. To address this, in this work, we propose to directly infer surface meshes for coronary artery lumen and plaque based on a centerline prior and use it in the downstream task of CAD-RADS scoring. The method is developed and evaluated using a total of 2407 CCTA scans. Our method achieved lesion-wise volume intraclass correlation coefficients of 0.98, 0.79, and 0.85 for calcified, non-calcified, and total plaque volume respectively. Patient-level CAD-RADS categorization was evaluated on a representative hold-out test set of 300 scans, for which the achieved linearly weighted kappa ($\kappa$) was 0.75. CAD-RADS categorization on the set of 658 scans from another hospital and scanner led to a $\kappa$ of 0.71. The results demonstrate that direct inference of coronary artery meshes for lumen and plaque is feasible, and allows for the automated prediction of routinely performed CAD-RADS categorization.
</details>
<details>
<summary>摘要</summary>
coronary artery disease (CAD) 仍然是全球最主要的死亡原因。患有可能的 CAD 的患者通常会通过 coronary CT angiography (CCTA) 来评估心血管事件的风险和治疗方案。在 CCTA 中的临床分析中，需要分析 coronary arteries 的病理和病变。而在这个过程中，我们提议直接从中心线约束下直接推算 coronary artery 的血管和病变表面。这种方法可以保证血管和病变的准确性和可靠性。我们在 2407 个 CCTA 扫描数据集上开发和评估了这种方法。我们的方法在不同类型的病变中的体积涂抹相互关系系数为 0.98、0.79 和 0.85。在一个代表样本中，我们对 300 个扫描数据进行了分类，其中的 linearly weighted kappa 值为 0.75。在另一个医院和扫描机器上进行了进一步的评估，其中的 linearly weighted kappa 值为 0.71。这些结果表明，直接从中心线约束下推算 coronary artery 的血管和病变表面是可能的，并且可以自动地预测通常进行的 CAD-RADS 分类。
</details></li>
</ul>
<hr>
<h2 id="MorphFlow-Estimating-Motion-in-In-Situ-Tests-of-Concrete"><a href="#MorphFlow-Estimating-Motion-in-In-Situ-Tests-of-Concrete" class="headerlink" title="MorphFlow: Estimating Motion in In Situ Tests of Concrete"></a>MorphFlow: Estimating Motion in In Situ Tests of Concrete</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11109">http://arxiv.org/abs/2310.11109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tessa Nogatz, Claudia Redenbach, Katja Schladitz</li>
<li>for: 该论文是为了估计从时间序列3D图像中的运动而设计的。</li>
<li>methods: 该算法使用了一种基于多尺度波лет的新方法来估计运动。</li>
<li>results: 该算法可以快速和可靠地处理大规模的室内实验数据，并且可以捕捉到异常的几何变化。两个例子 validate 了该算法的性能，包括一个经典的抗热试验和一个三点弯矩试验。<details>
<summary>Abstract</summary>
We present a novel algorithm explicitly tailored to estimate motion from time series of 3D images of concrete. Such volumetric images are usually acquired by Computed Tomography and can contain for example in situ tests, or more complex procedures like self-healing. Our algorithm is specifically designed to tackle the challenge of large scale in situ investigations of concrete. That means it cannot only cope with big images, but also with discontinuous displacement fields that often occur in in situ tests of concrete. We show the superior performance of our algorithm, especially regarding plausibility and time efficient processing. Core of the algorithm is a novel multiscale representation based on morphological wavelets. We use two examples for validation: A classical in situ test on refractory concrete and and a three point bending test on normal concrete. We show that for both applications structural changes like crack initiation can be already found at low scales -- a central achievement of our algorithm.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的算法，专门用于从3D图像序列中估算动态。这些三维图像通常由计算 Tomatoes取得，可以包含例如在 situ测试或更复杂的过程，如自适应修复。我们的算法特地设计用于解决大规模在 situ调查咨 concrete 中的挑战。这意味着它不仅可以处理大图像，还可以处理不连续的变位场的问题，这经常发生在 concrete 中的 in situ 测试中。我们展示了我们的算法在真实性和时间效率两个方面的优秀表现。我们的算法核心是一种新的多尺度表示方法，基于 morphological wavelets。我们使用了两个例子进行验证：一个经典的 in situ 测试和一个三点弯曲测试。我们发现，对于两个应用程序，结构变化，如裂隙开始，可以在低尺度上找到，这是我们算法的中心成就。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Clustering-Material-Decomposition-Aided-by-Empirical-Spectral-Correction-for-High-Resolution-Photon-Counting-Detectors-in-Micro-CT"><a href="#Iterative-Clustering-Material-Decomposition-Aided-by-Empirical-Spectral-Correction-for-High-Resolution-Photon-Counting-Detectors-in-Micro-CT" class="headerlink" title="Iterative Clustering Material Decomposition Aided by Empirical Spectral Correction for High-Resolution Photon-Counting Detectors in Micro-CT"></a>Iterative Clustering Material Decomposition Aided by Empirical Spectral Correction for High-Resolution Photon-Counting Detectors in Micro-CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10913">http://arxiv.org/abs/2310.10913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan C. R. Luna, Mini Das<br>for: 这项研究旨在提高计算tomography（CT）成像的精度，特别是在用 photon counting detectors（PCDs）进行多能量投射的情况下。methods: 该研究使用了实用的 инструменталь和测量策略，包括Iterative Clustering Material Decomposition（ICMD），以实现在spectral micro CT中量化多种材料的分离。results: 实验结果表明， combining spectral correction和高维数据归一化可以提高分离精度和降低噪声，并可以分解多于三种材料，包括混合物和K-edge材料。<details>
<summary>Abstract</summary>
Photon counting detectors (PCDs) offer promising advancements in computed tomography (CT) imaging by enabling the quantification and 3D imaging of contrast agents and tissue types through multi-energy projections. However, the accuracy of these decomposition methods hinges on precise composite spectral attenuation values that one must reconstruct from spectral micro CT. Factors such as surface defects, local temperature, signal amplification, and impurity levels can cause variations in detector efficiency between pixels, leading to significant quantitative errors. In addition, some inaccuracies such as the charge-sharing effects in PCDs are amplified with a high Z sensor material and also with a smaller detector pixels that are preferred for micro CT. In this work, we propose a comprehensive approach that combines practical instrumentation and measurement strategies leading to the quantitation of multiple materials within an object in a spectral micro CT with a photon counting detector. Our Iterative Clustering Material Decomposition (ICMD) includes an empirical method for detector spectral response corrections, cluster analysis and multi-step iterative material decomposition. Utilizing a CdTe-1mm Medipix detector with a 55$\mu$m pitch, we demonstrate the quantitatively accurate decomposition of several materials in a phantom study, where the sample includes mixtures of material, soft material and K-edge materials. We also show an example of biological sample imaging and separating three distinct types of tissue in mouse: muscle, fat and bone. Our experimental results show that the combination of spectral correction and high-dimensional data clustering enhances decomposition accuracy and reduces noise in micro CT. This ICMD allows for quantitative separation of more than three materials including mixtures and also effectively separates multi-contrast agents.
</details>
<details>
<summary>摘要</summary>
吸收计数器（PCD）在计算tomography（CT）成像中提供了有前途的改进，使得可以量化和三维成像各种 контраст物质和组织类型通过多能量投影。然而，这些分解方法的准确性取决于重建的复合spectralattenuation值，这些值可以从spectral micro CT中提取。因为表面缺陷、地方温度、信号增强和杂质水平等因素会导致每个像素的探测效率存在差异，这会导致重要的量化错误。此外，一些不精准的效应，如charge-sharing效应，会在高Z探测器材料和小像素尺寸下被增强。在这种情况下，我们提出了一种涵盖实用仪器和测量策略的全面方法，以实现spectral micro CT中多种材料的量化。我们的迭代归一化材料分解（ICMD）包括了实际方法、集群分析和多步迭代材料分解。使用CdTe-1mm Medipix探测器，我们在一个phantom研究中展示了高精度的材料分解，包括混合物、软物和K-edge材料。此外，我们还展示了一个生物样本的成像和分离三种不同的组织类型，包括肌肉、脂肪和骨。我们的实验结果表明，将spectral correction和高维数据归一化相结合可以提高分解精度和减少微CT的噪声。ICMD可以量化超过三种材料，包括混合物，并有效地分离多种对比剂。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/eess.IV_2023_10_17/" data-id="closbroyy018j0g88h9cthod0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/16/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><a class="page-number" href="/page/16/">16</a><span class="page-number current">17</span><a class="page-number" href="/page/18/">18</a><a class="page-number" href="/page/19/">19</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/18/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
