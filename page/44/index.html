
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/44/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_09_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/23/cs.CL_2023_09_23/" class="article-date">
  <time datetime="2023-09-23T11:00:00.000Z" itemprop="datePublished">2023-09-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/23/cs.CL_2023_09_23/">cs.CL - 2023-09-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hierarchical-attention-interpretation-an-interpretable-speech-level-transformer-for-bi-modal-depression-detection"><a href="#Hierarchical-attention-interpretation-an-interpretable-speech-level-transformer-for-bi-modal-depression-detection" class="headerlink" title="Hierarchical attention interpretation: an interpretable speech-level transformer for bi-modal depression detection"></a>Hierarchical attention interpretation: an interpretable speech-level transformer for bi-modal depression detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13476">http://arxiv.org/abs/2309.13476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingkun Deng, Saturnino Luz, Sofia de la Fuente Garcia</li>
<li>For: The paper aims to improve the accuracy and interpretability of automatic depression detection tools using speech, which can help early screening of depression.* Methods: The proposed bi-modal speech-level transformer model avoids segment-level labelling and provides both speech-level and sentence-level interpretations using gradient-weighted attention maps.* Results: The proposed model outperforms a model that learns at a segment level, with improved accuracy and interpretability. The model can identify the most relevant sentences and text tokens within a given speech that are indicative of depression.<details>
<summary>Abstract</summary>
Depression is a common mental disorder. Automatic depression detection tools using speech, enabled by machine learning, help early screening of depression. This paper addresses two limitations that may hinder the clinical implementations of such tools: noise resulting from segment-level labelling and a lack of model interpretability. We propose a bi-modal speech-level transformer to avoid segment-level labelling and introduce a hierarchical interpretation approach to provide both speech-level and sentence-level interpretations, based on gradient-weighted attention maps derived from all attention layers to track interactions between input features. We show that the proposed model outperforms a model that learns at a segment level ($p$=0.854, $r$=0.947, $F1$=0.897 compared to $p$=0.732, $r$=0.808, $F1$=0.768). For model interpretation, using one true positive sample, we show which sentences within a given speech are most relevant to depression detection; and which text tokens and Mel-spectrogram regions within these sentences are most relevant to depression detection. These interpretations allow clinicians to verify the validity of predictions made by depression detection tools, promoting their clinical implementations.
</details>
<details>
<summary>摘要</summary>
抑郁是一种常见的心理疾病。使用机器学习技术实现的自动抑郁检测工具可以帮助早期检测抑郁。这篇论文解决了两个可能阻碍临床应用的限制： segment-level 标注导致的噪音和模型解释性不足。我们提议使用双模块的speech-level transformer来避免 segment-level 标注，并提出一种层次解释方法，以提供 both speech-level 和 sentence-level 的解释，基于所有注意层的梯度权重注意力地图来跟踪输入特征之间的交互。我们表明，提议的模型在比较 segment level 学习的模型（$p$=0.732， $r$=0.808， $F1$=0.768）的情况下表现出色，其中 $p$=0.854， $r$=0.947， $F1$=0.897。为了解释模型，我们使用一个真正正确的样本，显示某些speech中的哪些句子是抑郁检测中最重要的，以及哪些文本字符和 Mel-spectrogram 区域在这些句子中对抑郁检测最重要。这些解释可以帮助临床专业人员验证抑郁检测工具的预测结果，从而促进其临床应用。
</details></li>
</ul>
<hr>
<h2 id="Grounding-Description-Driven-Dialogue-State-Trackers-with-Knowledge-Seeking-Turns"><a href="#Grounding-Description-Driven-Dialogue-State-Trackers-with-Knowledge-Seeking-Turns" class="headerlink" title="Grounding Description-Driven Dialogue State Trackers with Knowledge-Seeking Turns"></a>Grounding Description-Driven Dialogue State Trackers with Knowledge-Seeking Turns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13448">http://arxiv.org/abs/2309.13448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandru Coca, Bo-Hsiang Tseng, Jinghong Chen, Weizhe Lin, Weixuan Zhang, Tisha Anders, Bill Byrne</li>
<li>for: 提高对话管理模型的稳定性和泛化能力</li>
<li>methods: 使用对话 corpora 和知识图来固定状态跟踪模型，并在推理和训练过程中添加知识图 turns</li>
<li>results: 对比原始模型，新的方法可以大幅提高对话管理模型的平均共同目标准确率和schema敏感度<details>
<summary>Abstract</summary>
Schema-guided dialogue state trackers can generalise to new domains without further training, yet they are sensitive to the writing style of the schemata. Augmenting the training set with human or synthetic schema paraphrases improves the model robustness to these variations but can be either costly or difficult to control. We propose to circumvent these issues by grounding the state tracking model in knowledge-seeking turns collected from the dialogue corpus as well as the schema. Including these turns in prompts during finetuning and inference leads to marked improvements in model robustness, as demonstrated by large average joint goal accuracy and schema sensitivity improvements on SGD and SGD-X.
</details>
<details>
<summary>摘要</summary>
Schema-guided dialogue state trackers可以通过新领域掌握而无需进一步训练，但它们受到文本风格的影响很sensitive。增加训练集中的人工或 sintetic schema paraphrase可以提高模型的可靠性，但这可能会成本高或控制困难。我们提议通过将知识寻求turns集成到对话 corpus和schema中来固定状态跟踪模型。在训练和推理中包含这些turns的提问可以获得显著改进，如joint目标准确率和schema敏感度的提高，如SGD和SGD-X所示。
</details></li>
</ul>
<hr>
<h2 id="My-Science-Tutor-MyST-–-A-Large-Corpus-of-Children’s-Conversational-Speech"><a href="#My-Science-Tutor-MyST-–-A-Large-Corpus-of-Children’s-Conversational-Speech" class="headerlink" title="My Science Tutor (MyST) – A Large Corpus of Children’s Conversational Speech"></a>My Science Tutor (MyST) – A Large Corpus of Children’s Conversational Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13347">http://arxiv.org/abs/2309.13347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sameer S. Pradhan, Ronald A. Cole, Wayne H. Ward</li>
<li>for: This paper describes the development of the MyST corpus, a large collection of children’s conversational speech, which can be used to improve automatic speech recognition algorithms, build and evaluate conversational AI agents for education, and develop multimodal applications to improve children’s learning.</li>
<li>methods: The MyST corpus was developed as part of the My Science Tutor project, which involves 100K utterances transcribed from approximately 10.5K virtual tutor sessions by 1.3K third, fourth, and fifth grade students. The corpus is available for non-commercial and commercial use under a creative commons license.</li>
<li>results: To date, ten organizations have licensed the corpus for commercial use, and approximately 40 university and other not-for-profit research groups have downloaded the corpus. The corpus has the potential to be used to improve children’s learning and excitement about science, and to help them learn remotely.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文描述了MyST corpus的开发，这是一个儿童对话语音集，可以用于改进自动语音识别算法、建立和评估教育机器人、并开发多Modal应用程序，以提高儿童学习科学的兴趣和成就。</li>
<li>methods: MyST corpus是My Science Tutor项目的一部分，涉及100万句话的对话语音，来自约10.5万个虚拟导师会议，由1.3万名第三、四、五年级学生提供。这个 corpus 是可以免费使用（<a target="_blank" rel="noopener" href="https://myst.cemantix.org),也可以用于商业用途(https//boulderlearning.com/resources/myst-corpus/%EF%BC%89%E3%80%82%E5%88%B0%E7%9B%AE%E5%89%8D%E4%B8%BA%E6%AD%A2%EF%BC%8C%E6%9C%89%E5%8D%81%E5%AE%B6%E7%BB%84%E7%BB%87%E5%B7%B2%E7%BB%8F%E8%B4%AD%E4%B9%B0%E4%BA%86%E8%BF%99%E4%B8%AA">https://myst.cemantix.org），也可以用于商业用途（https://boulderlearning.com/resources/myst-corpus/）。到目前为止，有十家组织已经购买了这个</a> corpus 的商业授权，并且有约40个大学和其他非营利研究机构下载了这个 corpus。</li>
<li>results: 这个 corpus 的开发可以用于改进儿童学习科学的方法，并且可以帮助儿童在远程学习中学习更好地。到目前为止，有十家组织已经购买了这个 corpus 的商业授权，并且有约40个大学和其他非营利研究机构下载了这个 corpus。<details>
<summary>Abstract</summary>
This article describes the MyST corpus developed as part of the My Science Tutor project -- one of the largest collections of children's conversational speech comprising approximately 400 hours, spanning some 230K utterances across about 10.5K virtual tutor sessions by around 1.3K third, fourth and fifth grade students. 100K of all utterances have been transcribed thus far. The corpus is freely available (https://myst.cemantix.org) for non-commercial use using a creative commons license. It is also available for commercial use (https://boulderlearning.com/resources/myst-corpus/). To date, ten organizations have licensed the corpus for commercial use, and approximately 40 university and other not-for-profit research groups have downloaded the corpus. It is our hope that the corpus can be used to improve automatic speech recognition algorithms, build and evaluate conversational AI agents for education, and together help accelerate development of multimodal applications to improve children's excitement and learning about science, and help them learn remotely.
</details>
<details>
<summary>摘要</summary>
这篇文章介绍了MyST资料集，这是My Science Tutor项目中的一个大型儿童对话语音资料集，包含约400个小时的对话语音，涵盖约230万个语音词汇，分别来自约1.3万名第三、四、五年级学生的10.5万个虚拟教学会话。目前已经转录100万个语音。MyST资料集是免费使用（https://myst.cemantix.org），通过创意共用许可证，用于非商业用途。同时，也可以用于商业用途（https://boulderlearning.com/resources/myst-corpus/），至今已有10家组织购买了商业许可证。我们希望通过这些资料来提高自动语音识别算法，建立和评估教育对话AI代理人，以及在远程学习方面提高儿童对科学的兴趣和学习。
</details></li>
</ul>
<hr>
<h2 id="BAMBOO-A-Comprehensive-Benchmark-for-Evaluating-Long-Text-Modeling-Capacities-of-Large-Language-Models"><a href="#BAMBOO-A-Comprehensive-Benchmark-for-Evaluating-Long-Text-Modeling-Capacities-of-Large-Language-Models" class="headerlink" title="BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models"></a>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13345">http://arxiv.org/abs/2309.13345</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rucaibox/bamboo">https://github.com/rucaibox/bamboo</a></li>
<li>paper_authors: Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen</li>
<li>for: 本研究旨在评估大型自然语言处理（NLP）模型在长文本理解任务上的能力，并提供多任务长文本测试集（BAMBOO）。</li>
<li>methods: 本研究使用了五种长文本模型，在BAMBOO上进行了实验，并评估了这些模型在不同任务上的性能。</li>
<li>results: 研究发现，现有的长文本模型在某些任务上表现出色，但在其他任务上表现较差。研究还指出了未来可以采取的方法来提高长文本模型的能力。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e. question answering, hallucination detection, text sorting, language modeling, and code completion, to cover core capacities and various domains of LLMs. We conduct experiments with five long context models on BAMBOO and further discuss four key research questions of long text. We also qualitatively analyze current long context models and point out future directions for enhancing long text modeling capacities. We release our data, prompts, and code at https://github.com/RUCAIBox/BAMBOO.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在普通长度的NLPT任务上达到了戏剑性能。在最近的几项研究中，研究者们努力扩展了LLM的上下文长度和长文本模型化能力。为全面评估LLM的长文本能力，我们提出了BAMBOO，一个多任务长文本benchmark。BAMBOO遵循四个原则：全面评估能力、数据污染避免、自动评估精度和不同长度级别。它包括10个来自5个不同长文理解任务的数据集，例如问答、幻觉检测、文本排序、语言模型和代码完成等，以覆盖LLM的核心能力和不同领域。我们在BAMBOO上进行了5个长文本模型的实验，并讨论了长文本模型的四个关键研究问题。我们还进行了现有长文本模型的Qualitative分析，并指出了未来扩展长文本模型能力的方向。我们在GitHub上发布了数据、提示和代码，请参考https://github.com/RUCAIBox/BAMBOO。
</details></li>
</ul>
<hr>
<h2 id="From-Text-to-Source-Results-in-Detecting-Large-Language-Model-Generated-Content"><a href="#From-Text-to-Source-Results-in-Detecting-Large-Language-Model-Generated-Content" class="headerlink" title="From Text to Source: Results in Detecting Large Language Model-Generated Content"></a>From Text to Source: Results in Detecting Large Language Model-Generated Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13322">http://arxiv.org/abs/2309.13322</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Wissam Antoun, Benoît Sagot, Djamé Seddah</li>
<li>for: 本研究旨在 investigate Cross-Model Detection，探讨一个基于源LM的泛型分类器是否可以探测目标LM生成的文本。</li>
<li>methods: 本研究使用了多种LM大小和家族，并评估了对分类器泛化的影响。</li>
<li>results: 研究发现，模型大小与泛型分类器效果之间存在明显的反相关关系，大LM更难于探测，特别是当泛型分类器在小LM上训练时。同时，使用相同大小LM的数据进行训练可以提高大LM的探测性能，但可能会导致小LM的性能下降。模型归因实验也表明，LM生成的文本中含有可识别的签名特征。<details>
<summary>Abstract</summary>
The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates "Cross-Model Detection," evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families, and assesses the impact of conversational fine-tuning techniques on classifier generalization. The research also delves into Model Attribution, encompassing source model identification, model family classification, and model size classification. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution.
</details>
<details>
<summary>摘要</summary>
广泛使用大型语言模型（LLM），被夸大为能生成人类语言文本的能力，已引起关于误导和伦理问题的担忧。为解决这些问题，需要开发robust的检测和归因方法。本文研究“交叉模型检测”，检查一个基于源LLM生成文本和人类写作文本的分类器能否检测目标LLM生成的文本。研究全面探讨了不同的LLM大小和家族，以及对分类器泛化的影响。研究还探讨了模型归因，包括来源模型标识、模型家族分类和模型大小分类。我们的结果显示了一些关键发现：与分类器效果相对关系，大型LLM更难于检测，特别是当分类器被训练使用小型LLM的数据时。使用同样大小的LLM数据进行训练可以提高大型LLM的检测性能，但可能导致对小型LLM的性能下降。此外，模型归因实验显示了LLM生成文本中的可察性特征，这些特征可以用于归因LLM。总的来说，我们的研究为LLM检测和归因提供了有价值的发现。
</details></li>
</ul>
<hr>
<h2 id="GlotScript-A-Resource-and-Tool-for-Low-Resource-Writing-System-Identification"><a href="#GlotScript-A-Resource-and-Tool-for-Low-Resource-Writing-System-Identification" class="headerlink" title="GlotScript: A Resource and Tool for Low Resource Writing System Identification"></a>GlotScript: A Resource and Tool for Low Resource Writing System Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13320">http://arxiv.org/abs/2309.13320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cisnlp/GlotScript">https://github.com/cisnlp/GlotScript</a></li>
<li>paper_authors: Amir Hossein Kargaran, François Yvon, Hinrich Schütze</li>
<li>for: 用于identifying low-resource writing systems</li>
<li>methods: 使用存储的writing system resources和Unicode 15.0 scripts</li>
<li>results: 支持清理多语言 corpus和分析语言模型的TokenizationHere’s the same information in Simplified Chinese:</li>
<li>for: 用于识别低资源文字系统</li>
<li>methods: 使用存储的文字系统资源和Unicode 15.0 字体</li>
<li>results: 支持清理多语言 corpus和分析语言模型的Tokenization<details>
<summary>Abstract</summary>
We present GlotScript, an open resource and tool for low resource writing system identification. GlotScript-R is a resource that provides the attested writing systems for more than 7,000 languages. It is compiled by aggregating information from existing writing system resources. GlotScript-T is a writing system identification tool that covers all 161 Unicode 15.0 scripts. For an input text, it returns its script distribution where scripts are identified by ISO 15924 codes. We also present two use cases for GlotScript. First, we demonstrate that GlotScript supports cleaning multilingual corpora such as mC4 and OSCAR. Second, we analyze the tokenization of a number of language models such as GPT-4 using GlotScript and provide insights on the coverage of low resource scripts and languages by each language model. We hope that GlotScript will become a useful resource for work on low resource languages in the NLP community. GlotScript-R and GlotScript-T are available at https://github.com/cisnlp/GlotScript.
</details>
<details>
<summary>摘要</summary>
我们介绍GlotScript，一个开源资源和工具，用于低资源文字系统识别。GlotScript-R是一个提供了超过7,000种语言的验证文字系统资源。它通过将现有文字系统资源集成起来编译而成。GlotScript-T是一个可以识别所有Unicode 15.0 编码中的161种文字系统的写作系统识别工具。对于输入文本，它返回该文本的文字系统分布，并将文字系统用ISO 15924 编码来标识。我们还介绍了GlotScript的两个使用场景。首先，我们示例了GlotScript可以清洁多语言 corpus，如mC4和OSCAR。其次，我们分析了一些语言模型，如GPT-4，使用GlotScript进行分词，并提供了低资源文字和语言的覆盖率的视图。我们希望GlotScript可以成为NPLTcommunity中工作低资源语言的有用资源。GlotScript-R和GlotScript-T可以在https://github.com/cisnlp/GlotScript 上获取。
</details></li>
</ul>
<hr>
<h2 id="Spanish-Resource-Grammar-version-2023"><a href="#Spanish-Resource-Grammar-version-2023" class="headerlink" title="Spanish Resource Grammar version 2023"></a>Spanish Resource Grammar version 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13318">http://arxiv.org/abs/2309.13318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olga Zamaraeva, Carlos Gómez-Rodríguez</li>
<li>for: 这个论文是为了语言研究和自然语言处理应用开发而写的。</li>
<li>methods: 这个论文使用了最新版本的Freeling morphological analyzer和tagger，并提供了手动验证的treebank和问题列表。</li>
<li>results: 这个论文提供了一个新的研究方向，并在一小部分学习 corpus 上测试了 grammar 的覆盖率和过度生成。<details>
<summary>Abstract</summary>
We present the latest version of the Spanish Resource Grammar (SRG). The new SRG uses the recent version of Freeling morphological analyzer and tagger and is accompanied by a manually verified treebank and a list of documented issues. We also present the grammar's coverage and overgeneration on a small portion of a learner corpus, an entirely new research line with respect to the SRG. The grammar can be used for linguistic research, such as for empirically driven development of syntactic theory, and in natural language processing applications such as computer-assisted language learning. Finally, as the treebanks grow, they can be used for training high-quality semantic parsers and other systems which may benefit from precise and detailed semantics.
</details>
<details>
<summary>摘要</summary>
我们现在发布最新版的西班牙资源语法（SRG）。新的SRG使用了最新版的Freeling morphological analyzer和标注器，并附有手动验证的树链和问题列表。我们还对一小部分学习语料进行了覆盖率和过度生成的测试，这是SRG的全新研究方向。这个语法可以用于语言科研，如逐渐驱动发展 syntax理论，以及自然语言处理应用，如计算机助教语言学习。随着树链的增长，它们可以用于训练高质量semantic parser和其他可能受益于精确和详细 semantics的系统。
</details></li>
</ul>
<hr>
<h2 id="Calibrating-LLM-Based-Evaluator"><a href="#Calibrating-LLM-Based-Evaluator" class="headerlink" title="Calibrating LLM-Based Evaluator"></a>Calibrating LLM-Based Evaluator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13308">http://arxiv.org/abs/2309.13308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang</li>
<li>for: 这 paper 的目的是提出一种自动调整和人类偏好Alignment的方法，以便使用大型自然语言模型（LLM）进行自然语言生成质量评估。</li>
<li>methods: 该方法包括Multi-stage, gradient-free Approach，首先在不同的几个阶段中使用语言模型自己学习不同的几个例子，然后选择最佳表现者进行自我反调。</li>
<li>results: 对多个文本质量评估数据集进行实验，显示该方法可以有效地提高与专家评估的相关性。同时，对于各种有效的评价标准的Qualitative分析提供了深入的直观启示和观察。<details>
<summary>Abstract</summary>
Recent advancements in large language models (LLMs) on language modeling and emergent capabilities make them a promising reference-free evaluator of natural language generation quality, and a competent alternative to human evaluation. However, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf LLM-based evaluator towards better human alignment. In this work, we propose AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator toward human preference. Instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. Then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. To further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. Our experiments on multiple text quality evaluation datasets illustrate a significant improvement in correlation with expert evaluation through calibration. Our comprehensive qualitative analysis conveys insightful intuitions and observations on the essence of effective scoring criteria.
</details>
<details>
<summary>摘要</summary>
近期大语言模型（LLM）的进步在语言生成质量评估方面，使得它们成为了无参考的自然语言评估器和人类评估器的有力竞争对手。然而，由于某些原因，很多LLM-based评估器受到了封闭的源代码或高计算需求的限制，导致它们的评估器没有得到进一步的精心调整。在这项工作中，我们提出了一种多stage、gradient-free的自动调整方法，以使得LLM-based评估器更加准确地对应人类的喜好。而不是直接模型人类喜好，我们将人类标签集成到了一个集合中，然后由语言模型自己提出了初始的评估标准。然后，我们选择了最佳表现者，并通过自我修复来重新绘制这些标准。我们在多个文本质量评估数据集上进行了多项实验，并证明了与专家评估的强相关性。我们还提供了深入的Qualitative分析，帮助理解有效的评估标准的本质。
</details></li>
</ul>
<hr>
<h2 id="OATS-Opinion-Aspect-Target-Sentiment-Quadruple-Extraction-Dataset-for-Aspect-Based-Sentiment-Analysis"><a href="#OATS-Opinion-Aspect-Target-Sentiment-Quadruple-Extraction-Dataset-for-Aspect-Based-Sentiment-Analysis" class="headerlink" title="OATS: Opinion Aspect Target Sentiment Quadruple Extraction Dataset for Aspect-Based Sentiment Analysis"></a>OATS: Opinion Aspect Target Sentiment Quadruple Extraction Dataset for Aspect-Based Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13297">http://arxiv.org/abs/2309.13297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siva Uday Sampreeth Chebolu, Franck Dernoncourt, Nedim Lipka, Thamar Solorio</li>
<li>for: 本研究旨在掌握用户生成的评论中的具体元素之情感分析，以提高对文本内容的情感分析和评估。</li>
<li>methods: 本研究使用了新的OATS dataset，包括三个新领域的评论，以及20,000个句子四重和13,000个评论二重。实验还包括了内部和跨领域的实验，以探索不同的ABSA子 зада业和OATS的潜力。</li>
<li>results: 本研究通过实验获得了OATSdataset的初步基线，并证明了OATS可以解决现有的ABSA领域问题，例如餐厅和笔记型评价等领域的问题。<details>
<summary>Abstract</summary>
Aspect-based sentiment Analysis (ABSA) delves into understanding sentiments specific to distinct elements within textual content. It aims to analyze user-generated reviews to determine a) the target entity being reviewed, b) the high-level aspect to which it belongs, c) the sentiment words used to express the opinion, and d) the sentiment expressed toward the targets and the aspects. While various benchmark datasets have fostered advancements in ABSA, they often come with domain limitations and data granularity challenges. Addressing these, we introduce the OATS dataset, which encompasses three fresh domains and consists of 20,000 sentence-level quadruples and 13,000 review-level tuples. Our initiative seeks to bridge specific observed gaps: the recurrent focus on familiar domains like restaurants and laptops, limited data for intricate quadruple extraction tasks, and an occasional oversight of the synergy between sentence and review-level sentiments. Moreover, to elucidate OATS's potential and shed light on various ABSA subtasks that OATS can solve, we conducted in-domain and cross-domain experiments, establishing initial baselines. We hope the OATS dataset augments current resources, paving the way for an encompassing exploration of ABSA.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Natural-Language-Processing-for-Requirements-Formalization-How-to-Derive-New-Approaches"><a href="#Natural-Language-Processing-for-Requirements-Formalization-How-to-Derive-New-Approaches" class="headerlink" title="Natural Language Processing for Requirements Formalization: How to Derive New Approaches?"></a>Natural Language Processing for Requirements Formalization: How to Derive New Approaches?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13272">http://arxiv.org/abs/2309.13272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ifak-prototypes/nlp_reform">https://github.com/ifak-prototypes/nlp_reform</a></li>
<li>paper_authors: Viju Sudhi, Libin Kutty, Robin Gröpler<br>for: 本研究旨在提供一种 semi-自动化的需求ormalization方法，以帮助industry和研究人员尽可能自动化软件开发和测试过程。methods: 本研究使用自然语言处理（NLP）技术，包括创建规则集和iterative开发rule sets，以自动化需求ormalization过程。results: 研究表明，使用现有的预训练NLP模型可以减少创建规则集的努力，并且可以轻松适应特定用例和领域。两个 industriuse cases from the automotive and railway domains are used to demonstrate the effectiveness of the proposed methods.<details>
<summary>Abstract</summary>
It is a long-standing desire of industry and research to automate the software development and testing process as much as possible. In this process, requirements engineering (RE) plays a fundamental role for all other steps that build on it. Model-based design and testing methods have been developed to handle the growing complexity and variability of software systems. However, major effort is still required to create specification models from a large set of functional requirements provided in natural language. Numerous approaches based on natural language processing (NLP) have been proposed in the literature to generate requirements models using mainly syntactic properties. Recent advances in NLP show that semantic quantities can also be identified and used to provide better assistance in the requirements formalization process. In this work, we present and discuss principal ideas and state-of-the-art methodologies from the field of NLP in order to guide the readers on how to create a set of rules and methods for the semi-automated formalization of requirements according to their specific use case and needs. We discuss two different approaches in detail and highlight the iterative development of rule sets. The requirements models are represented in a human- and machine-readable format in the form of pseudocode. The presented methods are demonstrated on two industrial use cases from the automotive and railway domains. It shows that using current pre-trained NLP models requires less effort to create a set of rules and can be easily adapted to specific use cases and domains. In addition, findings and shortcomings of this research area are highlighted and an outlook on possible future developments is given.
</details>
<details>
<summary>摘要</summary>
industry和研究界长期希望自动化软件开发和测试过程，并且需求工程（RE）在这些步骤上扮演了基本角色。基于模型的设计和测试方法已经为软件系统的增长复杂性和可变性提供了解决方案。然而，从大量函циональ需求提供的自然语言处理（NLP）技术仍然需要大量的努力来生成需求模型。在文献中，许多基于NLP的方法已经被提出，主要基于语法特征来生成需求模型。然而，现在的NLP进步还表明可以利用semantic量来提供更好的帮助在需求正式化过程中。在这个工作中，我们将介绍和讨论一些在NLP领域的主要想法和现状技术，以帮助读者创建一套 semi-自动化需求正式化的规则和方法。我们在详细介绍了两种方法，并强调了迭代发展规则集的重要性。需求模型被表示为人类和机器可读的形式，即pseudocode。我们的方法在两个工业用例中（来自汽车和铁路领域）得到了证明，显示使用当前预训练的NLP模型需要较少的努力来创建规则集，并且可以轻松地适应特定用例和领域。此外，我们还高亮了这个研究领域的发现和缺陷，并提供了未来可能发展的前景。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Document-Level-Information-Extraction"><a href="#A-Survey-of-Document-Level-Information-Extraction" class="headerlink" title="A Survey of Document-Level Information Extraction"></a>A Survey of Document-Level Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13249">http://arxiv.org/abs/2309.13249</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Don-No7/Hack-SQL">https://github.com/Don-No7/Hack-SQL</a></li>
<li>paper_authors: Hanwen Zheng, Sijia Wang, Lifu Huang</li>
<li>for: 本文是一篇文献综述，旨在为NLП领域的研究人员提供更多的启示，以进一步提高文档级别的自然语言处理（NLP）性能。</li>
<li>methods: 本文使用了现有的国际先进算法进行了系统性的错误分析，并识别了当前的限制和NLП领域的留下的挑战。</li>
<li>results: 根据我们的发现，标注噪音、实体核心匹配和无理解能力是文档级别IE性能的主要限制因素。<details>
<summary>Abstract</summary>
Document-level information extraction (IE) is a crucial task in natural language processing (NLP). This paper conducts a systematic review of recent document-level IE literature. In addition, we conduct a thorough error analysis with current state-of-the-art algorithms and identify their limitations as well as the remaining challenges for the task of document-level IE. According to our findings, labeling noises, entity coreference resolution, and lack of reasoning, severely affect the performance of document-level IE. The objective of this survey paper is to provide more insights and help NLP researchers to further enhance document-level IE performance.
</details>
<details>
<summary>摘要</summary>
文档级信息提取（IE）是自然语言处理（NLP）中关键的任务。本文进行了最新文档级IE литературе的系统性评审。此外，我们还进行了当前状态的算法评估，并确定了现有算法的局限性以及文档级IE任务中仍存在的挑战。根据我们的发现，标注噪音、实体核心归并和不足的逻辑，对文档级IE性能产生了严重的影响。本文的目标是为NLP研究人员提供更多的洞察和帮助，以进一步提高文档级IE性能。
</details></li>
</ul>
<hr>
<h2 id="ChEDDAR-Student-ChatGPT-Dialogue-in-EFL-Writing-Education"><a href="#ChEDDAR-Student-ChatGPT-Dialogue-in-EFL-Writing-Education" class="headerlink" title="ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education"></a>ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13243">http://arxiv.org/abs/2309.13243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jieun Han, Haneul Yoo, Junho Myung, Minsun Kim, Tak Yeon Lee, So-Yeon Ahn, Alice Oh</li>
<li>for: 这个研究旨在探讨大规模实际场景下学生和AI系统之间的交互，以推动教育领域中AI生成技术的应用。</li>
<li>methods: 这个研究使用了对212名英语为外语学生进行了一个学期长的实验，他们被要求通过对ChatGPT进行对话来修改他们的作业。研究收集了对话记录、utterance-level作业修改历史、自我评价和学生的意图，以及每个会话的前后调查记录学生的目标和总体经验。</li>
<li>results: 研究发现学生在使用生成AI时的使用模式和满意度与他们的意图有直接关系，并提出了基准结果为两个关键任务在教育上的对话系统中：意图检测和满意度估计。研究建议进一步调整教育中AI生成技术的应用，并提出了可能的使用Scenario使用ChEDDAR。ChEDDAR公共可用于<a target="_blank" rel="noopener" href="https://github.com/zeunie/ChEDDAR%E3%80%82">https://github.com/zeunie/ChEDDAR。</a><details>
<summary>Abstract</summary>
The integration of generative AI in education is expanding, yet empirical analyses of large-scale, real-world interactions between students and AI systems still remain limited. In this study, we present ChEDDAR, ChatGPT & EFL Learner's Dialogue Dataset As Revising an essay, which is collected from a semester-long longitudinal experiment involving 212 college students enrolled in English as Foreign Langauge (EFL) writing courses. The students were asked to revise their essays through dialogues with ChatGPT. ChEDDAR includes a conversation log, utterance-level essay edit history, self-rated satisfaction, and students' intent, in addition to session-level pre-and-post surveys documenting their objectives and overall experiences. We analyze students' usage patterns and perceptions regarding generative AI with respect to their intent and satisfaction. As a foundational step, we establish baseline results for two pivotal tasks in task-oriented dialogue systems within educational contexts: intent detection and satisfaction estimation. We finally suggest further research to refine the integration of generative AI into education settings, outlining potential scenarios utilizing ChEDDAR. ChEDDAR is publicly available at https://github.com/zeunie/ChEDDAR.
</details>
<details>
<summary>摘要</summary>
整合生成AI在教育中的推广正在进行，但实际的大规模实验却仍然受到限制。本研究公布了ChEDDAR，ChatGPT & EFL Learner's Dialogue Dataset As Revising an essay，这是基于一个半年长的实验，其中212名大学生参与了英语作为外语写作课程。这些学生被要求通过对ChatGPT的对话来修改他们的文章。ChEDDAR包括对话记录、文章修改历史记录、自我评价满意度，以及学生的意图，以及每个会话的前后调查，记录了学生的目标和总体体验。我们分析学生对生成AI的使用方式和满意度的关系，并为两个关键任务在教育上的对话系统提供基线结果：检测意图和满意度估计。最后，我们建议进一步推进生成AI的教育集成，并提出了可能的应用场景，使用ChEDDAR可以在https://github.com/zeunie/ChEDDAR获取。
</details></li>
</ul>
<hr>
<h2 id="User-Simulation-with-Large-Language-Models-for-Evaluating-Task-Oriented-Dialogue"><a href="#User-Simulation-with-Large-Language-Models-for-Evaluating-Task-Oriented-Dialogue" class="headerlink" title="User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue"></a>User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13233">http://arxiv.org/abs/2309.13233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Davidson, Salvatore Romeo, Raphael Shu, James Gung, Arshit Gupta, Saab Mansour, Yi Zhang</li>
<li>for: 提高自动评估新任务对话系统（TOD）的发展，避免人工评估的多个阶段和迭代过程中的阻碍。</li>
<li>methods: 使用最近发展的大型预训练语言模型（LLM）建立新的用户模拟器，通过受 Context 学习提高语言多样性，模拟人类对话伙伴的行为。</li>
<li>results: 比前一工作更高的语言多样性和语义多样性，能够与多个 TOD 系统进行有效交流，尤其是单意对话目标，而且生成的语音和语法多样性比前一工作更高。<details>
<summary>Abstract</summary>
One of the major impediments to the development of new task-oriented dialogue (TOD) systems is the need for human evaluation at multiple stages and iterations of the development process. In an effort to move toward automated evaluation of TOD, we propose a novel user simulator built using recently developed large pretrained language models (LLMs). In order to increase the linguistic diversity of our system relative to the related previous work, we do not fine-tune the LLMs used by our system on existing TOD datasets; rather we use in-context learning to prompt the LLMs to generate robust and linguistically diverse output with the goal of simulating the behavior of human interlocutors. Unlike previous work, which sought to maximize goal success rate (GSR) as the primary metric of simulator performance, our goal is a system which achieves a GSR similar to that observed in human interactions with TOD systems. Using this approach, our current simulator is effectively able to interact with several TOD systems, especially on single-intent conversational goals, while generating lexically and syntactically diverse output relative to previous simulators that rely upon fine-tuned models. Finally, we collect a Human2Bot dataset of humans interacting with the same TOD systems with which we experimented in order to better quantify these achievements.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unify-word-level-and-span-level-tasks-NJUNLP’s-Participation-for-the-WMT2023-Quality-Estimation-Shared-Task"><a href="#Unify-word-level-and-span-level-tasks-NJUNLP’s-Participation-for-the-WMT2023-Quality-Estimation-Shared-Task" class="headerlink" title="Unify word-level and span-level tasks: NJUNLP’s Participation for the WMT2023 Quality Estimation Shared Task"></a>Unify word-level and span-level tasks: NJUNLP’s Participation for the WMT2023 Quality Estimation Shared Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13230">http://arxiv.org/abs/2309.13230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/njunlp/njuqe">https://github.com/njunlp/njuqe</a></li>
<li>paper_authors: Xiang Geng, Zhejian Lai, Yu Zhang, Shimin Tao, Hao Yang, Jiajun Chen, Shujian Huang</li>
<li>for: 这个研究是为了提出一种基于NJUQE框架的pseudo数据方法，以提高 Machine Translation 的质量预测（QE）性能。</li>
<li>methods: 该研究使用了Parallel Data从WMT翻译任务中生成pseudo MQM数据，然后使用XLMR大型模型在pseudo QE数据上进行预训练，并在实际QE数据上进行细化调整。同时，该研究jointly学习了句子级分数和单词级标签。</li>
<li>results: 该研究在英文-德文语对的 sentence-level和word-level质量预测两个子任务上达到了最佳性能，在两个子任务上的margin上提高了较大的性能。<details>
<summary>Abstract</summary>
We introduce the submissions of the NJUNLP team to the WMT 2023 Quality Estimation (QE) shared task. Our team submitted predictions for the English-German language pair on all two sub-tasks: (i) sentence- and word-level quality prediction; and (ii) fine-grained error span detection. This year, we further explore pseudo data methods for QE based on NJUQE framework (https://github.com/NJUNLP/njuqe). We generate pseudo MQM data using parallel data from the WMT translation task. We pre-train the XLMR large model on pseudo QE data, then fine-tune it on real QE data. At both stages, we jointly learn sentence-level scores and word-level tags. Empirically, we conduct experiments to find the key hyper-parameters that improve the performance. Technically, we propose a simple method that covert the word-level outputs to fine-grained error span results. Overall, our models achieved the best results in English-German for both word-level and fine-grained error span detection sub-tasks by a considerable margin.
</details>
<details>
<summary>摘要</summary>
我们介绍NJUNLP团队在WMT 2023质量估计（QE）共享任务中的提交。我们对英语-德语语对 submitting 预测，包括两个子任务：（i）句子和单词水平质量预测，以及（ii）细化错误范围检测。本年，我们进一步探索基于NJUQE框架（https://github.com/NJUNLP/njuqe）的pseudo数据方法 для QE。我们使用WMT翻译任务的平行数据生成pseudo MQM数据，然后在这些数据上预训练XLMR大型模型，然后精度调整在真实QE数据上。在两个阶段中，我们同时学习句子级分数和单词级标签。实际上，我们进行了实验来找到提高性能的关键超参数。技术上，我们提出了一种简单的方法，将单词级输出转换为细化错误范围结果。总的来说，我们的模型在英语-德语语对上的 both word-level 和细化错误范围检测子任务上 achieved 最佳成绩，差距非常明显。
</details></li>
</ul>
<hr>
<h2 id="COCO-Counterfactuals-Automatically-Constructed-Counterfactual-Examples-for-Image-Text-Pairs"><a href="#COCO-Counterfactuals-Automatically-Constructed-Counterfactual-Examples-for-Image-Text-Pairs" class="headerlink" title="COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs"></a>COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14356">http://arxiv.org/abs/2309.14356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiep Le, Vasudev Lal, Phillip Howard</li>
<li>for: 这篇论文的目的是提出一种可扩展的框架，用于自动生成多模态的反例，以提高自然语言处理（NLP）领域中模型对数据中的偶极相关性的耐误性。</li>
<li>methods: 这篇论文使用了文本到图像扩散模型来生成反例。</li>
<li>results: 作者通过人工评估 validate了 COCO-Counterfactuals 多模态反例集的质量，并表明了现有的多模态模型在这些反例中表现不佳。此外，作者还示出了通过使用 COCO-Counterfactuals 进行训练数据增强来提高多模态视语言模型的对外域数据的泛化能力。<details>
<summary>Abstract</summary>
Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用卷积神经网络生成对应的图像和文本描述的对称对话实例，以便用于评估和改进自然语言处理（NLP）模型对偶合关系在数据集中的Robustness。 DESPITE THEIR DEMONSTRATED UTILITY FOR NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation.难以创建具有最小对称变化的对应图像和文本描述的对称对话实例，这些实例在自然语言处理（NLP）领域已经证明具有检验和改进模型Robustness的价值。 DESPITE THEIR DEMONSTRATED UTILITY FOR NLP, multimodal counterfactual examples have been relatively unexplored。 To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/23/cs.CL_2023_09_23/" data-id="clpztdne500caes885et99q91" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/23/cs.LG_2023_09_23/" class="article-date">
  <time datetime="2023-09-23T10:00:00.000Z" itemprop="datePublished">2023-09-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/23/cs.LG_2023_09_23/">cs.LG - 2023-09-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Interpretable-and-Flexible-Target-Conditioned-Neural-Planners-For-Autonomous-Vehicles"><a href="#Interpretable-and-Flexible-Target-Conditioned-Neural-Planners-For-Autonomous-Vehicles" class="headerlink" title="Interpretable and Flexible Target-Conditioned Neural Planners For Autonomous Vehicles"></a>Interpretable and Flexible Target-Conditioned Neural Planners For Autonomous Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13485">http://arxiv.org/abs/2309.13485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolan Liu, Jishen Zhao, Liangjun Zhang</li>
<li>for: 这 paper 是为了解决自动驾驶车辆 плаanner 中的多个可接受的计划问题而写的。</li>
<li>methods: 这 paper 使用了一种可解释的神经网络执行器，通过灵活的 Gaussian 几何函数和放松的小时钟损失函数来更好地捕捉规划问题的不确定性。</li>
<li>results: 作者在 Lyft 开放数据集上进行系统性的评估，发现其模型在真实世界驾驶enario 中比 Priors 性能更好，具有更安全和更灵活的驾驶性能。<details>
<summary>Abstract</summary>
Learning-based approaches to autonomous vehicle planners have the potential to scale to many complicated real-world driving scenarios by leveraging huge amounts of driver demonstrations. However, prior work only learns to estimate a single planning trajectory, while there may be multiple acceptable plans in real-world scenarios. To solve the problem, we propose an interpretable neural planner to regress a heatmap, which effectively represents multiple potential goals in the bird's-eye view of an autonomous vehicle. The planner employs an adaptive Gaussian kernel and relaxed hourglass loss to better capture the uncertainty of planning problems. We also use a negative Gaussian kernel to add supervision to the heatmap regression, enabling the model to learn collision avoidance effectively. Our systematic evaluation on the Lyft Open Dataset across a diverse range of real-world driving scenarios shows that our model achieves a safer and more flexible driving performance than prior works.
</details>
<details>
<summary>摘要</summary>
学习基本的自动驾驶车辆规划方法有可能在许多复杂的实际驾驶场景中扩大，通过利用庞大量驾驶员示例来担快学习。然而，先前的工作只 learns to estimate 一个规划路径，而实际场景可能存在多个可接受的规划方案。为解决这个问题，我们提议一种可解释性神经网络规划器，使用折衔函数来回归热图，该热图有效表示自动驾驶车辆的飞行视图中的多个可能目标。我们的规划器使用适应 Gaussian 核函数和松弛小时钟损失函数，以更好地捕捉规划问题的不确定性。此外，我们还使用负 Gaussian 核函数来给热图回归添加监督，使模型能够有效地学习避免碰撞。我们对 Lyft 开放数据集进行系统性评估，并在实际驾驶场景中表明我们的模型可以比先前的工作更安全和更灵活地驾驶。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Scheme-of-ResNet-and-Softmax"><a href="#A-Unified-Scheme-of-ResNet-and-Softmax" class="headerlink" title="A Unified Scheme of ResNet and Softmax"></a>A Unified Scheme of ResNet and Softmax</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13482">http://arxiv.org/abs/2309.13482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao Song, Weixin Wang, Junze Yin</li>
<li>for: 这 paper 旨在提供一种统一的分析方法，用于研究深度学习中的 softmax 回归和 residual neural network（ResNet）两种技术的关系。</li>
<li>methods: 这 paper 使用了 theoretically 分析方法，对 regression 问题 $| \langle \exp(Ax) + A x , {\bf 1}_n \rangle^{-1} ( \exp(Ax) + Ax ) - b |_2^2$ 进行了分析。</li>
<li>results: 这 paper 得到了 loss 函数的梯度、Hessian 和 Lipschitz 性质的分析结果，并证明了梯度和 Hessian 都是正semidefinite matrix，这使得可以使用高效的 approximate Newton 方法优化。这种统一的方法可以连接两个之前认为是无关的领域，并提供了新的视角 для深度学习模型的优化。<details>
<summary>Abstract</summary>
Large language models (LLMs) have brought significant changes to human society. Softmax regression and residual neural networks (ResNet) are two important techniques in deep learning: they not only serve as significant theoretical components supporting the functionality of LLMs but also are related to many other machine learning and theoretical computer science fields, including but not limited to image classification, object detection, semantic segmentation, and tensors.   Previous research works studied these two concepts separately. In this paper, we provide a theoretical analysis of the regression problem: $\| \langle \exp(Ax) + A x , {\bf 1}_n \rangle^{-1} ( \exp(Ax) + Ax ) - b \|_2^2$, where $A$ is a matrix in $\mathbb{R}^{n \times d}$, $b$ is a vector in $\mathbb{R}^n$, and ${\bf 1}_n$ is the $n$-dimensional vector whose entries are all $1$. This regression problem is a unified scheme that combines softmax regression and ResNet, which has never been done before. We derive the gradient, Hessian, and Lipschitz properties of the loss function. The Hessian is shown to be positive semidefinite, and its structure is characterized as the sum of a low-rank matrix and a diagonal matrix. This enables an efficient approximate Newton method.   As a result, this unified scheme helps to connect two previously thought unrelated fields and provides novel insight into loss landscape and optimization for emerging over-parameterized neural networks, which is meaningful for future research in deep learning models.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）对人类社会带来了重要的变革。软极值回归和差异神经网络（ResNet）是深度学习中两种重要的技术：它们不仅支持 LLM 的功能，而且与其他机器学习和理论计算机科学领域有着密切的关系，包括图像分类、物体检测、 semantic 分割和矩阵等。在过去的研究中，这两个概念分别得到了研究。在这篇文章中，我们提供了对 regression 问题的理论分析：$\| \langle \exp(Ax) + A x , \mathbf{1}_n \rangle^{-1} ( \exp(Ax) + Ax ) - b \|_2^2$, 其中 $A$ 是一个 $n \times d$ 维度的矩阵，$b$ 是一个 $n$ 维度的向量，${\bf 1}_n$ 是一个 $n$ 维度的向量，其中每个元素都是 1。这个 regression 问题是融合软极值回归和 ResNet 的统一方案，这是之前从未有过的。我们 derive 了梯度、Hessian 和 Lipschitz 性质。Hessian 显示为正定定义的矩阵，其结构可以分解为低级matrix 和 диагональ矩阵。这使得我们可以使用高效的approximate Newton 方法。因此，这个统一方案可以将两个 formerly 不相关的领域相连接，提供了新的视角，对深度学习模型的未来研究具有深刻的意义。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Bandwidth-Estimation-from-Offline-Expert-Demonstrations"><a href="#Real-time-Bandwidth-Estimation-from-Offline-Expert-Demonstrations" class="headerlink" title="Real-time Bandwidth Estimation from Offline Expert Demonstrations"></a>Real-time Bandwidth Estimation from Offline Expert Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13481">http://arxiv.org/abs/2309.13481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aashish Gottipati, Sami Khairy, Gabriel Mittag, Vishak Gopal, Ross Cutler</li>
<li>for: 该论文targets the problem of bandwidth estimation (BWE) for real-time communication systems, with a focus on integrating data-driven bandwidth estimators into real-time systems.</li>
<li>methods: 该论文提出了一种名为Merlin的完全OFFLINE的数据驱动方法，将先前的追溯方法与深度学习技术相结合，以提高BWE的准确性和可靠性。</li>
<li>results: 实验表明，Merlin在对比WebRTC的视频会议中具有42.85%和12.8%的包丢失和延迟减少，分别。这些结果表明Merlin可以在实时网络控制中提供高质量的带宽估计。<details>
<summary>Abstract</summary>
In this work, we tackle the problem of bandwidth estimation (BWE) for real-time communication systems; however, in contrast to previous works, we leverage the vast efforts of prior heuristic-based BWE methods and synergize these approaches with deep learning-based techniques. Our work addresses challenges in generalizing to unseen network dynamics and extracting rich representations from prior experience, two key challenges in integrating data-driven bandwidth estimators into real-time systems. To that end, we propose Merlin, the first purely offline, data-driven solution to BWE that harnesses prior heuristic-based methods to extract an expert BWE policy. Through a series of experiments, we demonstrate that Merlin surpasses state-of-the-art heuristic-based and deep learning-based bandwidth estimators in terms of objective quality of experience metrics while generalizing beyond the offline world to in-the-wild network deployments where Merlin achieves a 42.85% and 12.8% reduction in packet loss and delay, respectively, when compared against WebRTC in inter-continental videoconferencing calls. We hope that Merlin's offline-oriented design fosters new strategies for real-time network control.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们解决了实时通信系统中的带宽估计（BWE）问题，但是与前一些工作不同，我们利用了过去的规则基本方法的巨大努力和深度学习基本技术的相互作用。我们的工作解决了在总结到未经见过的网络动态和从前经验中提取丰富表示的两个关键挑战，以使得数据驱动的带宽估计器可以成功地集成到实时系统中。为此，我们提出了Merlin，第一个完全OFFLINE、数据驱动的BWE解决方案，利用了过去的规则基本方法提取出专家级带宽估计策略。经过一系列实验，我们证明Merlin在对象质量体验指标方面超过了现有的规则基本方法和深度学习基本方法的带宽估计器，并在实际网络部署中实现了42.85%和12.8%的数据损失和延迟减少，分别与WebRTC在跨洲视频会议中比较。我们希望Merlin的OFFLINE-oriented设计会激发新的实时网络控制策略。
</details></li>
</ul>
<hr>
<h2 id="CA-PCA-Manifold-Dimension-Estimation-Adapted-for-Curvature"><a href="#CA-PCA-Manifold-Dimension-Estimation-Adapted-for-Curvature" class="headerlink" title="CA-PCA: Manifold Dimension Estimation, Adapted for Curvature"></a>CA-PCA: Manifold Dimension Estimation, Adapted for Curvature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13478">http://arxiv.org/abs/2309.13478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anna C. Gilbert, Kevin O’Neill</li>
<li>for: 本文旨在提出一种基于拟合 embedding 的维度估计方法，以改进现有的维度估计方法，以便更好地分析高维数据。</li>
<li>methods: 本文使用了本地 PCA 方法，基于拟合 embedding 来进行维度估计。</li>
<li>results: 经过严格的实验表明，本文提出的 CA-PCA 方法在各种设定下都有所改进，可以更好地估计高维数据的维度。<details>
<summary>Abstract</summary>
The success of algorithms in the analysis of high-dimensional data is often attributed to the manifold hypothesis, which supposes that this data lie on or near a manifold of much lower dimension. It is often useful to determine or estimate the dimension of this manifold before performing dimension reduction, for instance. Existing methods for dimension estimation are calibrated using a flat unit ball. In this paper, we develop CA-PCA, a version of local PCA based instead on a calibration of a quadratic embedding, acknowledging the curvature of the underlying manifold. Numerous careful experiments show that this adaptation improves the estimator in a wide range of settings.
</details>
<details>
<summary>摘要</summary>
高维数据分析中算法的成功常被归结于 manifold 假设，即数据位于或靠近一个低维度的抽象 manifold。在进行维度减少之前，常常需要先确定或估算 manifold 的维度。现有的维度估算方法通常使用平坦单位球进行准备。本文提出了 CA-PCA，基于 quadratic embedding 的本地 PCA 方法，考虑到 manifold 的曲率性。详细的实验表明，这种改进可以在各种场景中提高估计器的性能。Note: "高维数据" (gāo wèi xué) in Chinese refers to data with many features or dimensions, and "抽象 manifold" (chōu xiǎng jiāo) refers to a hypothetical lower-dimensional space that the high-dimensional data is assumed to lie on or near.
</details></li>
</ul>
<hr>
<h2 id="SUDS-Sanitizing-Universal-and-Dependent-Steganography"><a href="#SUDS-Sanitizing-Universal-and-Dependent-Steganography" class="headerlink" title="SUDS: Sanitizing Universal and Dependent Steganography"></a>SUDS: Sanitizing Universal and Dependent Steganography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13467">http://arxiv.org/abs/2309.13467</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pkrobinette/suds-ecai-2023">https://github.com/pkrobinette/suds-ecai-2023</a></li>
<li>paper_authors: Preston K. Robinette, Hanchen D. Wang, Nishan Shehadeh, Daniel Moyer, Taylor T. Johnson</li>
<li>for: This paper focuses on developing a deep learning sanitization technique called SUDS to mitigate the shortcomings of steganalysis in detecting steganography.</li>
<li>methods: The paper uses a deep learning approach called SUDS that is not reliant on prior knowledge of steganographic hiding techniques and can sanitize universal and dependent steganography.</li>
<li>results: The paper demonstrates the capabilities and limitations of SUDS through five research questions, including baseline comparisons and an ablation study, and shows that SUDS can increase the resistance of a poisoned classifier against attacks by 1375%.Here’s the Chinese translation of the three key information points:</li>
<li>for: 这篇论文关注开发一种基于深度学习的清洁技术called SUDS，以mitigate隐藏分析的缺陷。</li>
<li>methods: 这篇论文使用一种基于深度学习的方法called SUDS，该方法不依赖于隐藏技术的先前知识，可以清洁universal和依赖隐藏。</li>
<li>results: 这篇论文通过五个研究问题，包括基线比较和减少研究，展示了SUDS的能力和局限性。此外，SUDS在一个实际场景中能够提高恶意分类器对攻击的抵抗力 by 1375%.<details>
<summary>Abstract</summary>
Steganography, or hiding messages in plain sight, is a form of information hiding that is most commonly used for covert communication. As modern steganographic mediums include images, text, audio, and video, this communication method is being increasingly used by bad actors to propagate malware, exfiltrate data, and discreetly communicate. Current protection mechanisms rely upon steganalysis, or the detection of steganography, but these approaches are dependent upon prior knowledge, such as steganographic signatures from publicly available tools and statistical knowledge about known hiding methods. These dependencies render steganalysis useless against new or unique hiding methods, which are becoming increasingly common with the application of deep learning models. To mitigate the shortcomings of steganalysis, this work focuses on a deep learning sanitization technique called SUDS that is not reliant upon knowledge of steganographic hiding techniques and is able to sanitize universal and dependent steganography. SUDS is tested using least significant bit method (LSB), dependent deep hiding (DDH), and universal deep hiding (UDH). We demonstrate the capabilities and limitations of SUDS by answering five research questions, including baseline comparisons and an ablation study. Additionally, we apply SUDS to a real-world scenario, where it is able to increase the resistance of a poisoned classifier against attacks by 1375%.
</details>
<details>
<summary>摘要</summary>
《隐藏信息在明目张观的形式》，也称为隐藏通信，是一种常用于推广邮件、披露数据和秘密交流的信息隐藏方法。现代隐藏媒体包括图像、文本、音频和视频，这种通信方式在不良行为者中日益普及，以散播蠕虫、披露数据和秘密交流。现有的保护机制主要基于隐藏分析（steganalysis），但这些方法依赖于已知的隐藏技术和统计知识，因此对新或独特的隐藏方法无效。为了解决隐藏分析的缺陷，本研究提出了一种基于深度学习的清洁技术called SUDS，不依赖于隐藏技术的知识，可以清洁universal和依赖隐藏。SUDS在LSB、DDH和UDH方法上进行测试，我们通过 five 个研究问题回答了SUDS的能力和局限性，并进行了减少研究。此外，我们将SUDS应用于实际场景，其能够增加毒性分类器的抵抗力，达到1375%。
</details></li>
</ul>
<hr>
<h2 id="Tight-bounds-on-Pauli-channel-learning-without-entanglement"><a href="#Tight-bounds-on-Pauli-channel-learning-without-entanglement" class="headerlink" title="Tight bounds on Pauli channel learning without entanglement"></a>Tight bounds on Pauli channel learning without entanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13461">http://arxiv.org/abs/2309.13461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Senrui Chen, Changhun Oh, Sisi Zhou, Hsin-Yuan Huang, Liang Jiang</li>
<li>for: 这个论文主要研究了无共振学习算法的优势，具体来说是研究了不使用共振状态、测量和操作来学习Pauli通道的算法。</li>
<li>methods: 这个论文使用了无共振学习算法，具体来说是使用分离状态、测量和操作来学习Pauli通道。这种算法等同于在主系统上执行量子电路，并在电路中进行中间测量和классификация。</li>
<li>results: 论文提出了一个紧binding的下界 bounds for 无共振学习Pauli通道，这个下界是cubic gap 的关键。具体来说， authors 证明了需要 $\Theta(2^n\varepsilon^{-2})$ 轮 measurements来估算Pauli通道的每个特征值到 $\varepsilon$ 错误的高概率。与此相比，一个具有共振的学习算法只需要 $\Theta(\varepsilon^{-2})$ 轮 measurements。这个下界加强了实验准确地示出了共振增强的优势。<details>
<summary>Abstract</summary>
Entanglement is a useful resource for learning, but a precise characterization of its advantage can be challenging. In this work, we consider learning algorithms without entanglement to be those that only utilize separable states, measurements, and operations between the main system of interest and an ancillary system. These algorithms are equivalent to those that apply quantum circuits on the main system interleaved with mid-circuit measurements and classical feedforward. We prove a tight lower bound for learning Pauli channels without entanglement that closes a cubic gap between the best-known upper and lower bound. In particular, we show that $\Theta(2^n\varepsilon^{-2})$ rounds of measurements are required to estimate each eigenvalue of an $n$-qubit Pauli channel to $\varepsilon$ error with high probability when learning without entanglement. In contrast, a learning algorithm with entanglement only needs $\Theta(\varepsilon^{-2})$ rounds of measurements. The tight lower bound strengthens the foundation for an experimental demonstration of entanglement-enhanced advantages for characterizing Pauli noise.
</details>
<details>
<summary>摘要</summary>
Entanglement 是一种有用的资源 для学习，但准确地量ify its advantage 可以具有挑战。在这项工作中，我们认为不使用束缚状态的学习算法Equivalent to those that apply quantum circuits on the main system interleaved with mid-circuit measurements and classical feedforward。我们证明了无束缚状态学习Pauli通道的下界， closing a cubic gap between the best-known upper and lower bound。specifically, we show that $\Theta(2^n\varepsilon^{-2})$ rounds of measurements are required to estimate each eigenvalue of an $n$-qubit Pauli channel to $\varepsilon$ error with high probability when learning without entanglement. In contrast, a learning algorithm with entanglement only needs $\Theta(\varepsilon^{-2})$ rounds of measurements. The tight lower bound strengthens the foundation for an experimental demonstration of entanglement-enhanced advantages for characterizing Pauli noise.
</details></li>
</ul>
<hr>
<h2 id="Monotonic-Neural-Ordinary-Differential-Equation-Time-series-Forecasting-for-Cumulative-Data"><a href="#Monotonic-Neural-Ordinary-Differential-Equation-Time-series-Forecasting-for-Cumulative-Data" class="headerlink" title="Monotonic Neural Ordinary Differential Equation: Time-series Forecasting for Cumulative Data"></a>Monotonic Neural Ordinary Differential Equation: Time-series Forecasting for Cumulative Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13452">http://arxiv.org/abs/2309.13452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhichao Chen, Leilei Ding, Zhixuan Chu, Yucheng Qi, Jianmin Huang, Hao Wang</li>
<li>for: 预测时间序数据（TSFCD）是决策过程中的关键问题，但现有的时间序预测方法通常忽略了积累数据中的幂等性和不规则性，限制其实际应用。</li>
<li>methods: 我们提出了一种原理驱动的方法called Monotonic neural Ordinary Differential Equation (MODE)，该方法基于神经ordinary differential equations框架，能够有效捕捉和表示积累数据中的幂等性和不规则性。</li>
<li>results: 通过对奖金分配场景的广泛实验，我们展示了MODE的优异性，能够处理积累数据中的幂等性和不规则性，并提供了更好的预测性能。<details>
<summary>Abstract</summary>
Time-Series Forecasting based on Cumulative Data (TSFCD) is a crucial problem in decision-making across various industrial scenarios. However, existing time-series forecasting methods often overlook two important characteristics of cumulative data, namely monotonicity and irregularity, which limit their practical applicability. To address this limitation, we propose a principled approach called Monotonic neural Ordinary Differential Equation (MODE) within the framework of neural ordinary differential equations. By leveraging MODE, we are able to effectively capture and represent the monotonicity and irregularity in practical cumulative data. Through extensive experiments conducted in a bonus allocation scenario, we demonstrate that MODE outperforms state-of-the-art methods, showcasing its ability to handle both monotonicity and irregularity in cumulative data and delivering superior forecasting performance.
</details>
<details>
<summary>摘要</summary>
时序预测基于累积数据（TSFCD）是决策中的一项重要问题，存在许多工业场景中。然而，现有的时序预测方法通常忽视累积数据中的两个重要特征： monotonicity 和 irregularity，这限制了它们的实际应用。为解决这一限制，我们提议一种原则正的方法 called Monotonic Neural Ordinary Differential Equation（MODE），基于神经常微方程。通过利用 MODE，我们可以有效地捕捉和表示实际累积数据中的 monotonicity 和 irregularity。经过广泛的奖励分配场景的实验，我们示出 MODE 可以在 monotonicity 和 irregularity 的情况下提供更好的预测性能，超过现有的方法。
</details></li>
</ul>
<hr>
<h2 id="NetDiffus-Network-Traffic-Generation-by-Diffusion-Models-through-Time-Series-Imaging"><a href="#NetDiffus-Network-Traffic-Generation-by-Diffusion-Models-through-Time-Series-Imaging" class="headerlink" title="NetDiffus: Network Traffic Generation by Diffusion Models through Time-Series Imaging"></a>NetDiffus: Network Traffic Generation by Diffusion Models through Time-Series Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04429">http://arxiv.org/abs/2310.04429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nirhoshan Sivaroopan, Dumindu Bandara, Chamara Madarasingha, Guilluame Jourjon, Anura Jayasumana, Kanchana Thilakarathna</li>
<li>for: 这篇论文的目的是如何使用扩散模型生成假设数据，以便解决现代网络数据的有限访问问题。</li>
<li>methods: 这篇论文使用了Diffusion Models（DM），将一维时间序列网络流量转换为二维图像，然后生成代表性图像。</li>
<li>results: 论文表明，使用NetDiffus可以提高66.4%的数据准确性和18.1%的下游机器学习任务。在七种不同的流量轨迹上进行评估， synthetic数据可以显著改善流量识别、异常检测和流量分类。<details>
<summary>Abstract</summary>
Network data analytics are now at the core of almost every networking solution. Nonetheless, limited access to networking data has been an enduring challenge due to many reasons including complexity of modern networks, commercial sensitivity, privacy and regulatory constraints. In this work, we explore how to leverage recent advancements in Diffusion Models (DM) to generate synthetic network traffic data. We develop an end-to-end framework - NetDiffus that first converts one-dimensional time-series network traffic into two-dimensional images, and then synthesizes representative images for the original data. We demonstrate that NetDiffus outperforms the state-of-the-art traffic generation methods based on Generative Adversarial Networks (GANs) by providing 66.4% increase in fidelity of the generated data and 18.1% increase in downstream machine learning tasks. We evaluate NetDiffus on seven diverse traffic traces and show that utilizing synthetic data significantly improves traffic fingerprinting, anomaly detection and traffic classification.
</details>
<details>
<summary>摘要</summary>
网络数据分析现在成为网络解决方案的核心。然而，因为现代网络的复杂性、商业敏感性、隐私和法规限制等多种原因，实际网络数据访问受到了限制。在这种情况下，我们探讨了如何利用最近的扩散模型（DM）来生成合成网络流量数据。我们开发了一个端到端框架——NetDiffus，它首先将一维时间序列网络流量转换为二维图像，然后将原始数据生成 representativeness 的图像。我们证明了 NetDiffus 比基于生成对抗网络（GANs）的现有流量生成方法提供了66.4% 的真实性提升和18.1% 的下游机器学任务提升。我们对七个多样化的流量轨迹进行了评估，并显示了利用合成数据可以显著提高流量识别、异常检测和流量分类。
</details></li>
</ul>
<hr>
<h2 id="Early-Classification-for-Dynamic-Inference-of-Neural-Networks"><a href="#Early-Classification-for-Dynamic-Inference-of-Neural-Networks" class="headerlink" title="Early Classification for Dynamic Inference of Neural Networks"></a>Early Classification for Dynamic Inference of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13443">http://arxiv.org/abs/2309.13443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingcun Wang, Bing Li, Grace Li Zhang</li>
<li>for: 降低edge设备上深度神经网络（DNNs）的计算成本，以便在资源有限的平台上应用。</li>
<li>methods: 使用动态神经网络（Dynamic Neural Networks）实现结构适应，并在不同输入上进行早期退出。</li>
<li>results: 通过各级别分类器来除外不相关的类别，从而使后续层只需要确定剩下的目标类别。实验结果表明，可以有效降低DNNs在推理中的计算成本。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have been successfully applied in various fields. In DNNs, a large number of multiply-accumulate (MAC) operations is required to be performed, posing critical challenges in applying them in resource-constrained platforms, e.g., edge devices. Dynamic neural networks have been introduced to allow a structural adaption, e.g., early-exit, according to different inputs to reduce the computational cost of DNNs. Existing early-exit techniques deploy classifiers at intermediate layers of DNNs to push them to make a classification decision as early as possible. However, the learned features at early layers might not be sufficient to exclude all the irrelevant classes and decide the correct class, leading to suboptimal results. To address this challenge, in this paper, we propose a class-based early-exit for dynamic inference. Instead of pushing DNNs to make a dynamic decision at intermediate layers, we take advantages of the learned features in these layers to exclude as many irrelevant classes as possible, so that later layers only have to determine the target class among the remaining classes. Until at a layer only one class remains, this class is the corresponding classification result. To realize this class-based exclusion, we assign each class with a classifier at intermediate layers and train the networks together with these classifiers. Afterwards, an exclusion strategy is developed to exclude irrelevant classes at early layers. Experimental results demonstrate the computational cost of DNNs in inference can be reduced significantly.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose a class-based early-exit for dynamic inference. Instead of pushing DNNs to make a dynamic decision at intermediate layers, we take advantage of the learned features in these layers to exclude as many irrelevant classes as possible. We assign each class with a classifier at intermediate layers and train the networks together with these classifiers. An exclusion strategy is then developed to exclude irrelevant classes at early layers.Experimental results demonstrate that the computational cost of DNNs in inference can be significantly reduced using our approach.Simplified Chinese translation:深度神经网络（DNN）在各个领域得到了成功应用，但是它们需要大量的 multiply-accumulate（MAC）操作，这对于具有限制的资源的平台，如边缘设备，具有挑战性。动态神经网络被引入以实现结构适应，例如早期离开，以降低 DNN 的计算成本。现有的早期离开技术是通过在 DNN 中的中间层添加分类器，以便在不同的输入上使 DNN 尽早做出分类决策。然而，学习在中间层的特征可能不够用于排除所有无关的类并决定正确的类，从而导致低效果。为了解决这个挑战，我们在这篇论文中提出了类型基于的早期离开。而不是在 DNN 中的中间层强制做出动态决策，我们利用中间层学习的特征来排除最多的无关类。在每个层只剩下一个类时，这个类就是对应的分类结果。为实现这种类型基于的排除，我们将每个类分配了中间层的分类器，并与这些分类器一起训练 DNN。后续，我们开发了一种排除策略，以便在早期层中排除无关的类。实验结果表明，使用我们的方法可以在 DNN 的推理中减少计算成本。
</details></li>
</ul>
<hr>
<h2 id="MiliPoint-A-Point-Cloud-Dataset-for-mmWave-Radar"><a href="#MiliPoint-A-Point-Cloud-Dataset-for-mmWave-Radar" class="headerlink" title="MiliPoint: A Point Cloud Dataset for mmWave Radar"></a>MiliPoint: A Point Cloud Dataset for mmWave Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13425">http://arxiv.org/abs/2309.13425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Cui, Shu Zhong, Jiacheng Wu, Zichao Shen, Naim Dahnoun, Yiren Zhao</li>
<li>for: 这项研究是为了开发更有效的点集基于深度学习方法，以激发mmWave雷达技术的应用在人类活动识别领域。</li>
<li>methods: 该研究使用了大规模的开放数据集，并在这些数据集上进行了多种点基的深度神经网络模型的实现，包括DGCNN、PointNet++和PointTransformer等。</li>
<li>results: 研究发现，使用点基的深度神经网络可以在mmWave雷达数据上实现更高的人类活动识别精度。此外，该研究还提供了一个大规模的开放数据集，可供研究者进一步探索mmWave雷达技术在人类活动识别领域的应用。<details>
<summary>Abstract</summary>
Millimetre-wave (mmWave) radar has emerged as an attractive and cost-effective alternative for human activity sensing compared to traditional camera-based systems. mmWave radars are also non-intrusive, providing better protection for user privacy. However, as a Radio Frequency (RF) based technology, mmWave radars rely on capturing reflected signals from objects, making them more prone to noise compared to cameras. This raises an intriguing question for the deep learning community: Can we develop more effective point set-based deep learning methods for such attractive sensors?   To answer this question, our work, termed MiliPoint, delves into this idea by providing a large-scale, open dataset for the community to explore how mmWave radars can be utilised for human activity recognition. Moreover, MiliPoint stands out as it is larger in size than existing datasets, has more diverse human actions represented, and encompasses all three key tasks in human activity recognition. We have also established a range of point-based deep neural networks such as DGCNN, PointNet++ and PointTransformer, on MiliPoint, which can serve to set the ground baseline for further development.
</details>
<details>
<summary>摘要</summary>
幂米波（mmWave）雷达已成为人类活动感知的吸引人和经济实惠的替代方案，比传统的摄像头系统更加cost-effective。另外，mmWave雷达也是不侵入的，为用户隐私提供更好的保护。然而，作为一种Radio Frequency（RF）基于的技术，mmWave雷达需要捕捉到物体上的反射信号，这使其更容易受到噪声的影响，与摄像头相比。这引发了深度学习社区的一个感人问题：可以开发更有效的点集基于深度学习方法吗？为回答这个问题，我们的工作，称为MiliPoint，探讨了这一想法，提供了一个大规模、开放的数据集，让社区可以探索mmWave雷达如何用于人类活动识别。此外，MiliPoint更大、更多样化的人类行为被表示出来，并包括人类活动识别的三个关键任务。我们还在MiliPoint上建立了一些点基的深度神经网络，如DGCNN、PointNet++和PointTransformer，以设置基准 для后续的发展。
</details></li>
</ul>
<hr>
<h2 id="DenMune-Density-peak-based-clustering-using-mutual-nearest-neighbors"><a href="#DenMune-Density-peak-based-clustering-using-mutual-nearest-neighbors" class="headerlink" title="DenMune: Density peak based clustering using mutual nearest neighbors"></a>DenMune: Density peak based clustering using mutual nearest neighbors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13420">http://arxiv.org/abs/2309.13420</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scikit-learn-contrib/denmune-clustering-algorithm">https://github.com/scikit-learn-contrib/denmune-clustering-algorithm</a></li>
<li>paper_authors: Mohamed Abbas, Adel El-Zoghobi, Amin Shoukry</li>
<li>for: 该论文是为了解决聚类算法在具有不规则形状、不均匀密度和数据类别受到近似影响的情况下失效问题。</li>
<li>methods: 该论文提出了一种新的聚类算法，即 DenMune，该算法基于在K个最近邻域中查找密集区域，K是用户需要提供的唯一参数，并遵循最近邻域一致原理。</li>
<li>results: 该论文表明，DenMune算法能够在低维和高维数据集上 produz 稳定和Robust的聚类结果，并能自动除掉聚类过程中的噪音和找到目标聚类。<details>
<summary>Abstract</summary>
Many clustering algorithms fail when clusters are of arbitrary shapes, of varying densities, or the data classes are unbalanced and close to each other, even in two dimensions. A novel clustering algorithm, DenMune is presented to meet this challenge. It is based on identifying dense regions using mutual nearest neighborhoods of size K, where K is the only parameter required from the user, besides obeying the mutual nearest neighbor consistency principle. The algorithm is stable for a wide range of values of K. Moreover, it is able to automatically detect and remove noise from the clustering process as well as detecting the target clusters. It produces robust results on various low and high-dimensional datasets relative to several known state-of-the-art clustering algorithms.
</details>
<details>
<summary>摘要</summary>
很多聚类算法在聚类形状不规则、密度不同、数据类型近似的情况下失败。一种新的聚类算法，DenMune，以解决这些挑战。该算法基于在尺度K中的积 nearest neighborhoods，K是用户需要提供的唯一参数，同时遵循积 nearest neighbor consistency principle。算法在不同的K值下具有稳定性，并且可以自动除掉聚类过程中的噪声，同时检测目标聚类。它在各种低维和高维数据集上显示出了相对稳定的结果，与许多已知的状态 искусственный智能算法相比。
</details></li>
</ul>
<hr>
<h2 id="Learning-Large-Scale-MTP-2-Gaussian-Graphical-Models-via-Bridge-Block-Decomposition"><a href="#Learning-Large-Scale-MTP-2-Gaussian-Graphical-Models-via-Bridge-Block-Decomposition" class="headerlink" title="Learning Large-Scale MTP$_2$ Gaussian Graphical Models via Bridge-Block Decomposition"></a>Learning Large-Scale MTP$_2$ Gaussian Graphical Models via Bridge-Block Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13405">http://arxiv.org/abs/2309.13405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiwen1997/mtp2-bbd">https://github.com/xiwen1997/mtp2-bbd</a></li>
<li>paper_authors: Xiwen Wang, Jiaxi Ying, Daniel P. Palomar</li>
<li>for: 本研究实际问题是学习大规模的 Gaussian  graficial 模型（MTP}_2），通过引入桥梁的概念，将整个问题分解为较小的scale的子问题，并提出一些可诠释的解决方案，从实际上来说，这个简单可诠释的架构可以将大问题分解为小 tractable 问题，实现了巨大的计算复杂度削减和现有算法的重要提升。</li>
<li>methods: 本研究使用了桥梁分解法，将大规模 Gaussian  graficial 模型（MTP}_2）分解为较小的scale的子问题，并提出了一些可诠释的解决方案。</li>
<li>results: 实验结果显示， compared to state-of-the-art 参考标准，本研究的提案方法具有重要的速度优化。<details>
<summary>Abstract</summary>
This paper studies the problem of learning the large-scale Gaussian graphical models that are multivariate totally positive of order two ($\text{MTP}_2$). By introducing the concept of bridge, which commonly exists in large-scale sparse graphs, we show that the entire problem can be equivalently optimized through (1) several smaller-scaled sub-problems induced by a \emph{bridge-block decomposition} on the thresholded sample covariance graph and (2) a set of explicit solutions on entries corresponding to bridges. From practical aspect, this simple and provable discipline can be applied to break down a large problem into small tractable ones, leading to enormous reduction on the computational complexity and substantial improvements for all existing algorithms. The synthetic and real-world experiments demonstrate that our proposed method presents a significant speed-up compared to the state-of-the-art benchmarks.
</details>
<details>
<summary>摘要</summary>
这篇论文研究大规模的高斯图模型，即多变量完全正的第二阶（MTP2）。我们引入了桥梁概念，这种概念在大规模稀疏图中广泛存在。我们显示，整个问题可以等价地通过以下两个步骤优化：1. 使用桥梁块分解法对阈值矩阵相关的一些更小规模的子问题进行优化。2. 对桥梁相对应的输入进行直观的解决方案。从实践角度来看，这种简单可证的方法可以将大问题分解成小可解决的问题，从而减少计算复杂性和提高所有现有算法的性能。实验表明，我们提出的方法与现有的标准准则相比，具有显著的速度提升。
</details></li>
</ul>
<hr>
<h2 id="ML-Algorithm-Synthesizing-Domain-Knowledge-for-Fungal-Spores-Concentration-Prediction"><a href="#ML-Algorithm-Synthesizing-Domain-Knowledge-for-Fungal-Spores-Concentration-Prediction" class="headerlink" title="ML Algorithm Synthesizing Domain Knowledge for Fungal Spores Concentration Prediction"></a>ML Algorithm Synthesizing Domain Knowledge for Fungal Spores Concentration Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13402">http://arxiv.org/abs/2309.13402</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/azminewasi/qcre23-finalist">https://github.com/azminewasi/qcre23-finalist</a></li>
<li>paper_authors: Md Asif Bin Syed, Azmine Toushik Wasi, Imtiaz Ahmed</li>
<li>for: 提高纸品质量控制的效率和可持续性，实现实时精度测试和纠正控制。</li>
<li>methods: 利用时间序列数据和领域知识，采用机器学习算法进行精度预测。</li>
<li>results: 实现实时精度测试和纠正控制，提高纸品质量和可持续性。<details>
<summary>Abstract</summary>
The pulp and paper manufacturing industry requires precise quality control to ensure pure, contaminant-free end products suitable for various applications. Fungal spore concentration is a crucial metric that affects paper usability, and current testing methods are labor-intensive with delayed results, hindering real-time control strategies. To address this, a machine learning algorithm utilizing time-series data and domain knowledge was proposed. The optimal model employed Ridge Regression achieving an MSE of 2.90 on training and validation data. This approach could lead to significant improvements in efficiency and sustainability by providing real-time predictions for fungal spore concentrations. This paper showcases a promising method for real-time fungal spore concentration prediction, enabling stringent quality control measures in the pulp-and-paper industry.
</details>
<details>
<summary>摘要</summary>
《纸品生产业需要精准质量控制，以 Ensure 纸品具备不同应用的纯度和不损害性。蕈菌苗量是影响纸品使用性的关键指标，现有的测试方法具有劳动 INTENSIVE 和延迟结果，使得实时控制策略受阻。为此，一种机器学习算法使用时序数据和领域知识进行建模，选择最佳模型为ridge regression，实现了训练和验证数据的MSE为2.90。这种方法可能会在效率和可持续性方面提供重要的改进，并为纸品生产业提供实时蕈菌苗量预测，帮助实施严格的质量控制措施。本文介绍了实时蕈菌苗量预测的有效方法，为纸品生产业带来更好的质量控制和可持续发展。》Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-the-Sweet-Spot-of-Contrastive-Views-for-Knowledge-enhanced-Recommendation"><a href="#On-the-Sweet-Spot-of-Contrastive-Views-for-Knowledge-enhanced-Recommendation" class="headerlink" title="On the Sweet Spot of Contrastive Views for Knowledge-enhanced Recommendation"></a>On the Sweet Spot of Contrastive Views for Knowledge-enhanced Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13384">http://arxiv.org/abs/2309.13384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haibo Ye, Xinjie Li, Yuan Yao, Hanghang Tong</li>
<li>for: 这个论文旨在提高推荐系统的效果，通过在知识图（KG）和用户项交互图（IG）之间建立对应关系。</li>
<li>methods: 该论文提出了一种新的对照学习框架，通过在IG和KG之间建立两个不同的对照视图，并将IG中的知识信息与KG进行一个方向的融合，以便更好地利用知识。</li>
<li>results: 对于三个实际 dataset，该方法的实验结果显示，相比之前的状态 искус技术，该方法能够更高效地提高推荐系统的效果。代码可以通过以下隐藏链接获取：<a target="_blank" rel="noopener" href="https://figshare.com/articles/conference_contribution/SimKGCL/22783382">https://figshare.com/articles/conference_contribution/SimKGCL/22783382</a><details>
<summary>Abstract</summary>
In recommender systems, knowledge graph (KG) can offer critical information that is lacking in the original user-item interaction graph (IG). Recent process has explored this direction and shows that contrastive learning is a promising way to integrate both. However, we observe that existing KG-enhanced recommenders struggle in balancing between the two contrastive views of IG and KG, making them sometimes even less effective than simply applying contrastive learning on IG without using KG. In this paper, we propose a new contrastive learning framework for KG-enhanced recommendation. Specifically, to make full use of the knowledge, we construct two separate contrastive views for KG and IG, and maximize their mutual information; to ease the contrastive learning on the two views, we further fuse KG information into IG in a one-direction manner.Extensive experimental results on three real-world datasets demonstrate the effectiveness and efficiency of our method, compared to the state-of-the-art. Our code is available through the anonymous link:https://figshare.com/articles/conference_contribution/SimKGCL/22783382
</details>
<details>
<summary>摘要</summary>
在推荐系统中，知识图（KG）可以提供用户-ITEM互动图（IG）缺失的关键信息。近期的进程探索了这个方向，并显示了对照学习是一种有前途的方法来整合两者。然而，我们发现现有的KG强化推荐器在平衡两个对照视图IG和KG的视图之间很困难，有时甚至比不用对IG进行对照学习更不有效。在本文中，我们提出了一个新的对照学习框架 для KG强化推荐。具体来说，为了充分利用知识，我们构建了两个独立的对照视图 для KG和IG，并尽可能地增加它们之间的相互信息。此外，为了让对照学习在两个视图之间更加容易，我们进一步将KG信息集成到IG中一irectionally。我们的实验结果表明，我们的方法比现有的状态前方法更有效和高效，并且可以在三个实际 dataset上进行证明。我们的代码可以通过以下匿名链接获取：https://figshare.com/articles/conference_contribution/SimKGCL/22783382
</details></li>
</ul>
<hr>
<h2 id="Learning-Invariant-Representations-with-a-Nonparametric-Nadaraya-Watson-Head"><a href="#Learning-Invariant-Representations-with-a-Nonparametric-Nadaraya-Watson-Head" class="headerlink" title="Learning Invariant Representations with a Nonparametric Nadaraya-Watson Head"></a>Learning Invariant Representations with a Nonparametric Nadaraya-Watson Head</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13377">http://arxiv.org/abs/2309.13377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alanqrwang/nwhead">https://github.com/alanqrwang/nwhead</a></li>
<li>paper_authors: Alan Q. Wang, Minh Nguyen, Mert R. Sabuncu</li>
<li>for: 本研究旨在提出一种非 Parametric 的协同学习方法，以实现在不同环境下的数据分布不同时，机器学习模型的可重用性。</li>
<li>methods: 本研究使用 Nadaraya-Watson (NW) 头，该头通过比较学习的表示与支持集中的标注数据进行比较，来预测。通过控制支持集，可以编码不同的 causal 假设。</li>
<li>results: 通过在三个实际世界领域的域泛化任务上进行验证，表明本方法可以学习不受环境影响的抽象特征，并且可以在不同环境下提供好的预测性能。<details>
<summary>Abstract</summary>
Machine learning models will often fail when deployed in an environment with a data distribution that is different than the training distribution. When multiple environments are available during training, many methods exist that learn representations which are invariant across the different distributions, with the hope that these representations will be transportable to unseen domains. In this work, we present a nonparametric strategy for learning invariant representations based on the recently-proposed Nadaraya-Watson (NW) head. The NW head makes a prediction by comparing the learned representations of the query to the elements of a support set that consists of labeled data. We demonstrate that by manipulating the support set, one can encode different causal assumptions. In particular, restricting the support set to a single environment encourages the model to learn invariant features that do not depend on the environment. We present a causally-motivated setup for our modeling and training strategy and validate on three challenging real-world domain generalization tasks in computer vision.
</details>
<details>
<summary>摘要</summary>
机器学习模型经常在培育环境不同于训练环境下部署时失败。当有多个环境可用于训练时，许多方法可以学习不受环境影响的表示，以期这些表示可以在未见领域中传输。在这种工作中，我们提出了一种非 Parametric 策略，基于最近提出的 Nadaraya-Watson（NW）头来学习不受环境影响的表示。NW 头通过比较学习的表示和一个支持集中的标注数据进行比较，来预测。我们表明，通过修改支持集，可以编码不同的 causal 假设。例如，限制支持集为单个环境，使模型学习不受环境的无关特征。我们采用 causally-motivated 的模型和训练策略，并在计算机视觉领域中进行了三个复杂的实际领域泛化任务的验证。
</details></li>
</ul>
<hr>
<h2 id="Asca-less-audio-data-is-more-insightful"><a href="#Asca-less-audio-data-is-more-insightful" class="headerlink" title="Asca: less audio data is more insightful"></a>Asca: less audio data is more insightful</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13373">http://arxiv.org/abs/2309.13373</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leeciang/asca">https://github.com/leeciang/asca</a></li>
<li>paper_authors: Xiang Li, Junhao Chen, Chao Li, Hongwu Lv</li>
<li>for: 本研究旨在提高特殊领域中的专业音频识别，如鸟叫声和潜水声频率的标准化和预测。</li>
<li>methods: 本研究使用了Audio Spectrogram Convolution Attention（ASCA）模型，结合了Transformer和卷积储存架构，同时还具有新的网络设计和注意技术，以及资料增强和调整策略。</li>
<li>results: 在BirdCLEF2023和AudioSet（平衡） datasets上，ASCA模型实现了81.2%和35.1%的准确率，均高于竞争方法。<details>
<summary>Abstract</summary>
Audio recognition in specialized areas such as birdsong and submarine acoustics faces challenges in large-scale pre-training due to the limitations in available samples imposed by sampling environments and specificity requirements. While the Transformer model excels in audio recognition, its dependence on vast amounts of data becomes restrictive in resource-limited settings. Addressing this, we introduce the Audio Spectrogram Convolution Attention (ASCA) based on CoAtNet, integrating a Transformer-convolution hybrid architecture, novel network design, and attention techniques, further augmented with data enhancement and regularization strategies. On the BirdCLEF2023 and AudioSet(Balanced), ASCA achieved accuracies of 81.2% and 35.1%, respectively, significantly outperforming competing methods. The unique structure of our model enriches output, enabling generalization across various audio detection tasks. Our code can be found at https://github.com/LeeCiang/ASCA.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>特殊领域的音频识别，如鸟唱和潜船音频识别，由样本环境和特定要求所限制大规模预训练的样本数量。虽然Transformer模型在音频识别方面表现出色，但它在资源有限的设置下变得有限制。为此，我们介绍Audio Spectrogram Convolution Attention（ASCA），基于CoAtNet的干扰混合架构，加入了Transformer- convolution 混合 Architecture，新型网络设计，以及注意技术，并进一步使用数据增强和常规化策略。在BirdCLEF2023和AudioSet（平衡）上，ASCA实现了81.2%和35.1%的准确率，分别大幅超越竞争方法。ASCA的独特结构使得输出更加丰富，使得泛化到不同的音频检测任务。我们的代码可以在https://github.com/LeeCiang/ASCA中找到。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-with-Chaotic-Strange-Attractors"><a href="#Machine-Learning-with-Chaotic-Strange-Attractors" class="headerlink" title="Machine Learning with Chaotic Strange Attractors"></a>Machine Learning with Chaotic Strange Attractors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13361">http://arxiv.org/abs/2309.13361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Bahadır Utku Kesgin, Uğur Teğin</li>
<li>for: 这个论文是为了解决机器学习中的高功耗问题，通过使用混沌非线性吸引器来实现低功耗的机器学习任务。</li>
<li>methods: 该论文提出了一种基于混沌非线性吸引器的分析计算方法，该方法可以高效地进行机器学习任务，而且具有可编程、通用和泛化的特点。</li>
<li>results: 研究发现，使用该方法可以在批量处理和神经网络训练中实现低功耗，并且在分类和回归学习任务中达到了高准确率和低误差。<details>
<summary>Abstract</summary>
Machine learning studies need colossal power to process massive datasets and train neural networks to reach high accuracies, which have become gradually unsustainable. Limited by the von Neumann bottleneck, current computing architectures and methods fuel this high power consumption. Here, we present an analog computing method that harnesses chaotic nonlinear attractors to perform machine learning tasks with low power consumption. Inspired by neuromorphic computing, our model is a programmable, versatile, and generalized platform for machine learning tasks. Our mode provides exceptional performance in clustering by utilizing chaotic attractors' nonlinear mapping and sensitivity to initial conditions. When deployed as a simple analog device, it only requires milliwatt-scale power levels while being on par with current machine learning techniques. We demonstrate low errors and high accuracies with our model for regression and classification-based learning tasks.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:机器学习研究需要巨大的能源来处理庞大的数据集和训练神经网络以达到高精度，这已经变得不可持续。由于 von Neumann 瓶颈，当前的计算架构和方法都在提高能 consumption。在这里，我们提出了一种 Analog computing 方法，利用混沌非线性吸引器来实现机器学习任务，具有低功耗Characteristics。 draw inspiration from neuromorphic computing， our model is a programmable, versatile, and generalized platform for machine learning tasks. Our model shows excellent performance in clustering by leveraging chaotic attractors' nonlinear mapping and sensitivity to initial conditions. When deployed as a simple analog device, it only requires milliwatt-scale power levels while being on par with current machine learning techniques. We demonstrate low errors and high accuracies with our model for regression and classification-based learning tasks.
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Particle-and-Fluid-Simulations-with-Differentiable-Graph-Networks-for-Solving-Forward-and-Inverse-Problems"><a href="#Accelerating-Particle-and-Fluid-Simulations-with-Differentiable-Graph-Networks-for-Solving-Forward-and-Inverse-Problems" class="headerlink" title="Accelerating Particle and Fluid Simulations with Differentiable Graph Networks for Solving Forward and Inverse Problems"></a>Accelerating Particle and Fluid Simulations with Differentiable Graph Networks for Solving Forward and Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13348">http://arxiv.org/abs/2309.13348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krishna Kumar, Yongjin Choi</li>
<li>for: 加速粒子和液体 simulations，解决前向和 inverse problems。</li>
<li>methods: 使用物理嵌入的分发式图网络模型（GNS），通过学习Edge messages来学习本地互动规则，提高对新环境的泛化能力。</li>
<li>results: 对于 granular flow prediction 比 CPU 平行数值计算 Speedup 达到 165 倍，提议一种 hybrid GNS&#x2F;Material Point Method（MPM），能够在 GNS rollouts 中插入 MPM，以满足保守量和误差最小化，实现了对数值计算的 24 倍加速。 GNS 还可以解决 inverse problems，通过自动导数来计算摩擦角的梯度，并且可以逐步更新摩擦角，以实现最佳匹配目标跑道距离。<details>
<summary>Abstract</summary>
We leverage physics-embedded differentiable graph network simulators (GNS) to accelerate particulate and fluid simulations to solve forward and inverse problems. GNS represents the domain as a graph with particles as nodes and learned interactions as edges. Compared to modeling global dynamics, GNS enables learning local interaction laws through edge messages, improving its generalization to new environments. GNS achieves over 165x speedup for granular flow prediction compared to parallel CPU numerical simulations. We propose a novel hybrid GNS/Material Point Method (MPM) to accelerate forward simulations by minimizing error on a pure surrogate model by interleaving MPM in GNS rollouts to satisfy conservation laws and minimize errors achieving 24x speedup compared to pure numerical simulations. The differentiable GNS enables solving inverse problems through automatic differentiation, identifying material parameters that result in target runout distances. We demonstrate the ability of GNS to solve inverse problems by iteratively updating the friction angle (a material property) by computing the gradient of a loss function based on the final and target runouts, thereby identifying the friction angle that best matches the observed runout. The physics-embedded and differentiable simulators open an exciting new paradigm for AI-accelerated design, control, and optimization.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:我们利用嵌入物理的分解urable图示网络优化器（GNS）加速粒子和流体 simulations以解决前向和反向问题。GNS将Domain表示为一个图，粒子作为节点，学习交互作为边。与模型全局动力学相比，GNS可以通过边上消息学习地方交互规则，提高其适应新环境的能力。GNS在粒子流预测方面实现了165倍的加速，比CPU并行数值 simulations 高得多。我们提出了一种新的混合GNS/物理点方法（MPM），通过混合MPM在GNS扫描中来加速前向 simulations，以满足保守法和降低误差，实现24倍的加速比例。可微的GNS可以通过自动导数来解决反向问题，Material angle （一种材料属性）的迭代更新，以实现最佳匹配目标跑道距离。我们示出了GNS可以解决反向问题，通过计算目标跑道距离和最终跑道距离的损失函数梯度来更新Friction angle。物理嵌入和可微的模拟器开启了一个新的AI加速设计、控制和优化的新时代。
</details></li>
</ul>
<hr>
<h2 id="On-the-Asymptotic-Learning-Curves-of-Kernel-Ridge-Regression-under-Power-law-Decay"><a href="#On-the-Asymptotic-Learning-Curves-of-Kernel-Ridge-Regression-under-Power-law-Decay" class="headerlink" title="On the Asymptotic Learning Curves of Kernel Ridge Regression under Power-law Decay"></a>On the Asymptotic Learning Curves of Kernel Ridge Regression under Power-law Decay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13337">http://arxiv.org/abs/2309.13337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yicheng Li, Haobo Zhang, Qian Lin</li>
<li>for: 本研究探讨了 neural network 中广泛观察到的 ‘benign overfitting’ 现象，这个现象对 statistical learning theory 中的 ‘bias-variance trade-off’ 假设提出了挑战。</li>
<li>methods: 本研究使用了 kernel ridge regression 来描述 neural network 的学习曲线，并提供了一个准确的学习曲线 Characterization，包括 regulatory parameter 的选择、source condition 和 noise 的效应。</li>
<li>results: 研究结果表明，在小量噪声下，very wide neural network 才存在 ‘benign overfitting’ 现象。<details>
<summary>Abstract</summary>
The widely observed 'benign overfitting phenomenon' in the neural network literature raises the challenge to the 'bias-variance trade-off' doctrine in the statistical learning theory. Since the generalization ability of the 'lazy trained' over-parametrized neural network can be well approximated by that of the neural tangent kernel regression, the curve of the excess risk (namely, the learning curve) of kernel ridge regression attracts increasing attention recently. However, most recent arguments on the learning curve are heuristic and are based on the 'Gaussian design' assumption. In this paper, under mild and more realistic assumptions, we rigorously provide a full characterization of the learning curve: elaborating the effect and the interplay of the choice of the regularization parameter, the source condition and the noise. In particular, our results suggest that the 'benign overfitting phenomenon' exists in very wide neural networks only when the noise level is small.
</details>
<details>
<summary>摘要</summary>
widely observed "benign overfitting phenomenon" in the neural network literature raises the challenge to the "bias-variance trade-off" doctrine in the statistical learning theory. Since the generalization ability of the "lazy trained" over-parametrized neural network can be well approximated by that of the neural tangent kernel regression, the curve of the excess risk (namely, the learning curve) of kernel ridge regression attracts increasing attention recently. However, most recent arguments on the learning curve are heuristic and are based on the "Gaussian design" assumption. In this paper, under mild and more realistic assumptions, we rigorously provide a full characterization of the learning curve: elaborating the effect and the interplay of the choice of the regularization parameter, the source condition, and the noise. In particular, our results suggest that the "benign overfitting phenomenon" exists in very wide neural networks only when the noise level is small.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Predicting-Temperature-of-Major-Cities-Using-Machine-Learning-and-Deep-Learning"><a href="#Predicting-Temperature-of-Major-Cities-Using-Machine-Learning-and-Deep-Learning" class="headerlink" title="Predicting Temperature of Major Cities Using Machine Learning and Deep Learning"></a>Predicting Temperature of Major Cities Using Machine Learning and Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13330">http://arxiv.org/abs/2309.13330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wasiou Jaharabi, MD Ibrahim Al Hossain, Rownak Tahmid, Md. Zuhayer Islam, T. M. Saad Rayhan</li>
<li>For: The paper aims to develop an accurate temperature prediction method using machine learning algorithms and time series analysis, specifically focusing on the temperature data of major cities.* Methods: The authors use a dataset provided by the University of Dayton, which includes temperature data from major cities. They apply time series analysis techniques such as ARIMA, SARIMA, and Prophet, and incorporate the concept of RNN and LSTM to filter out abnormalities, preprocess the data, and make predictions of future temperature trends.* Results: The authors achieve accurate predictions of temperature in major cities based on the available data, and demonstrate the effectiveness of their method in combating climate change by providing accurate temperature predictions for future reference.<details>
<summary>Abstract</summary>
Currently, the issue that concerns the world leaders most is climate change for its effect on agriculture, environment and economies of daily life. So, to combat this, temperature prediction with strong accuracy is vital. So far, the most effective widely used measure for such forecasting is Numerical weather prediction (NWP) which is a mathematical model that needs broad data from different applications to make predictions. This expensive, time and labor consuming work can be minimized through making such predictions using Machine learning algorithms. Using the database made by University of Dayton which consists the change of temperature in major cities we used the Time Series Analysis method where we use LSTM for the purpose of turning existing data into a tool for future prediction. LSTM takes the long-term data as well as any short-term exceptions or anomalies that may have occurred and calculates trend, seasonality and the stationarity of a data. By using models such as ARIMA, SARIMA, Prophet with the concept of RNN and LSTM we can, filter out any abnormalities, preprocess the data compare it with previous trends and make a prediction of future trends. Also, seasonality and stationarity help us analyze the reoccurrence or repeat over one year variable and removes the constrain of time in which the data was dependent so see the general changes that are predicted. By doing so we managed to make prediction of the temperature of different cities during any time in future based on available data and built a method of accurate prediction. This document contains our methodology for being able to make such predictions.
</details>
<details>
<summary>摘要</summary>
To develop our methodology, we used a database of temperature changes in major cities, provided by the University of Dayton. We employed Time Series Analysis, specifically Long Short-Term Memory (LSTM) models, to turn existing data into a tool for future prediction. LSTM models can capture long-term trends, seasonality, and stationarity in the data, allowing us to make accurate predictions.We used models such as ARIMA, SARIMA, and Prophet, all of which incorporate the concept of Recurrent Neural Networks (RNN) and LSTM. These models can filter out anomalies, preprocess the data, and compare it with previous trends to make accurate predictions. Additionally, seasonality and stationarity help us analyze the reoccurrence of variables over one year and remove the constraints of time, allowing us to see general changes predicted.By using this methodology, we were able to make accurate predictions of temperature in different cities at any time in the future based on available data. This document outlines our methodology for making such predictions.
</details></li>
</ul>
<hr>
<h2 id="An-Interpretable-Systematic-Review-of-Machine-Learning-Models-for-Predictive-Maintenance-of-Aircraft-Engine"><a href="#An-Interpretable-Systematic-Review-of-Machine-Learning-Models-for-Predictive-Maintenance-of-Aircraft-Engine" class="headerlink" title="An Interpretable Systematic Review of Machine Learning Models for Predictive Maintenance of Aircraft Engine"></a>An Interpretable Systematic Review of Machine Learning Models for Predictive Maintenance of Aircraft Engine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13310">http://arxiv.org/abs/2309.13310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdullah Al Hasib, Ashikur Rahman, Mahpara Khabir, Md. Tanvir Rouf Shawon</li>
<li>For: This paper aims to predict aircraft engine failure using machine learning and deep learning models to avoid any kind of disaster.* Methods: The paper utilizes sensor data and employs various machine learning and deep learning models such as LSTM, Bi-LSTM, RNN, Bi-RNN, GRU, Random Forest, KNN, Naive Bayes, and Gradient Boosting to predict aircraft engine failure within a predetermined number of cycles.* Results: The paper achieves a lucrative accuracy of 97.8%, 97.14%, and 96.42% using GRU, Bi-LSTM, and LSTM respectively, demonstrating the capability of the models to predict maintenance at an early stage.<details>
<summary>Abstract</summary>
This paper presents an interpretable review of various machine learning and deep learning models to predict the maintenance of aircraft engine to avoid any kind of disaster. One of the advantages of the strategy is that it can work with modest datasets. In this study, sensor data is utilized to predict aircraft engine failure within a predetermined number of cycles using LSTM, Bi-LSTM, RNN, Bi-RNN GRU, Random Forest, KNN, Naive Bayes, and Gradient Boosting. We explain how deep learning and machine learning can be used to generate predictions in predictive maintenance using a straightforward scenario with just one data source. We applied lime to the models to help us understand why machine learning models did not perform well than deep learning models. An extensive analysis of the model's behavior is presented for several test data to understand the black box scenario of the models. A lucrative accuracy of 97.8%, 97.14%, and 96.42% are achieved by GRU, Bi-LSTM, and LSTM respectively which denotes the capability of the models to predict maintenance at an early stage.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CORE-Common-Random-Reconstruction-for-Distributed-Optimization-with-Provable-Low-Communication-Complexity"><a href="#CORE-Common-Random-Reconstruction-for-Distributed-Optimization-with-Provable-Low-Communication-Complexity" class="headerlink" title="CORE: Common Random Reconstruction for Distributed Optimization with Provable Low Communication Complexity"></a>CORE: Common Random Reconstruction for Distributed Optimization with Provable Low Communication Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13307">http://arxiv.org/abs/2309.13307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengyun Yue, Hanzhen Zhao, Cong Fang, Di He, Liwei Wang, Zhouchen Lin, Song-chun Zhu</li>
<li>For: 降低分布式机器学习中的通信复杂度，以提高训练速度和扩展机器数。* Methods: 提出了一新技术 named Common randOm REconstruction(CORE), 可以压缩在机器之间传输的信息，以降低通信复杂度，不受其他严格条件限制。 CORE 将 вектор值信息投影到低维度的归一化向量上，并在通信后重建信息，通过共同Random vectors。* Results: 应用 CORE 到两个分布式任务，分别是线性模型的凸优化和通用非凸优化，设计了新的分布式算法，可以证明性地降低通信复杂度。例如，我们示出对线性模型，CORE 基于算法可以编码梯度 вектор到 $\mathcal{O}(1)$-bits（对 $\mathcal{O}(d)$ 比），并保持不变的整体趋势，超过现有结果。<details>
<summary>Abstract</summary>
With distributed machine learning being a prominent technique for large-scale machine learning tasks, communication complexity has become a major bottleneck for speeding up training and scaling up machine numbers. In this paper, we propose a new technique named Common randOm REconstruction(CORE), which can be used to compress the information transmitted between machines in order to reduce communication complexity without other strict conditions. Especially, our technique CORE projects the vector-valued information to a low-dimensional one through common random vectors and reconstructs the information with the same random noises after communication. We apply CORE to two distributed tasks, respectively convex optimization on linear models and generic non-convex optimization, and design new distributed algorithms, which achieve provably lower communication complexities. For example, we show for linear models CORE-based algorithm can encode the gradient vector to $\mathcal{O}(1)$-bits (against $\mathcal{O}(d)$), with the convergence rate not worse, preceding the existing results.
</details>
<details>
<summary>摘要</summary>
With 分布式机器学习技术在大规模机器学习任务中成为主要瓶颈，交流复杂性已成为加速训练和扩大机器数量的关键问题。在这篇论文中，我们提出一种新的技术名为通用随机重建（CORE），可以减少机器之间的信息传输量，从而降低交流复杂性，而无需其他严格条件。尤其是，我们的CORE技术将向量值信息映射到低维度的均匀随机向量上，并在通信后重建信息，同时保留了同样的随机噪声。我们在两个分布式任务中应用CORE技术，分别是线性模型的凸优化和非凸优化，并设计了新的分布式算法，实现了可靠性下降的交流复杂性。例如，我们示出了对线性模型的CORE-based算法可以将梯度向量编码为$\mathcal{O}(1)$-比特（对$\mathcal{O}(d)$比特），并且保持不变的整体性能。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Fairness-Age-Harmless-Parkinson’s-Detection-via-Voice"><a href="#Beyond-Fairness-Age-Harmless-Parkinson’s-Detection-via-Voice" class="headerlink" title="Beyond Fairness: Age-Harmless Parkinson’s Detection via Voice"></a>Beyond Fairness: Age-Harmless Parkinson’s Detection via Voice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13292">http://arxiv.org/abs/2309.13292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yicheng Wang, Xiaotian Han, Leisheng Yu, Na Zou</li>
<li>for: 这个研究旨在提高 Parkinson’s disease（PD）早期识别的准确性，特别是针对年轻人群（age ≤ 55），因为现有的深度学习模型具有年龄问题。</li>
<li>methods: 我们使用 GradCAM-based 特征遮罩和组合模型来解决年龄问题，保持公平性和准确性。特别是，GradCAM-based 特征遮罩 selectively 隐藏年龄相关的特征，以保持关键的PD检测信息。组合模型进一步提高了少数群（年轻人群）的预测精度。</li>
<li>results: 我们的方法可以增强PD早期识别的准确性，不会对老年人群（age &gt; 55）的预测精度造成影响。此外，我们也提出了一个两步检测策略，以便评估年轻人群中可能的PD早期病人。<details>
<summary>Abstract</summary>
Parkinson's disease (PD), a neurodegenerative disorder, often manifests as speech and voice dysfunction. While utilizing voice data for PD detection has great potential in clinical applications, the widely used deep learning models currently have fairness issues regarding different ages of onset. These deep models perform well for the elderly group (age $>$ 55) but are less accurate for the young group (age $\leq$ 55). Through our investigation, the discrepancy between the elderly and the young arises due to 1) an imbalanced dataset and 2) the milder symptoms often seen in early-onset patients. However, traditional debiasing methods are impractical as they typically impair the prediction accuracy for the majority group while minimizing the discrepancy. To address this issue, we present a new debiasing method using GradCAM-based feature masking combined with ensemble models, ensuring that neither fairness nor accuracy is compromised. Specifically, the GradCAM-based feature masking selectively obscures age-related features in the input voice data while preserving essential information for PD detection. The ensemble models further improve the prediction accuracy for the minority (young group). Our approach effectively improves detection accuracy for early-onset patients without sacrificing performance for the elderly group. Additionally, we propose a two-step detection strategy for the young group, offering a practical risk assessment for potential early-onset PD patients.
</details>
<details>
<summary>摘要</summary>
帕金森病（PD），一种神经退化疾病，经常表现为语音和嗓音畸形。使用语音数据进行PD检测具有很大的优势，但目前广泛使用的深度学习模型存在年龄偏见问题。这些深度模型对于年龄大于55岁的老年组（age>55）表现良好，但对年龄小于或等于55岁的青年组（age<=55）表现不准确。经过我们的调查，年龄偏见的原因包括1）数据集偏见和2）早期病人的轻微症状。然而，传统的偏见纠正方法不实用，因为它们通常会降低主要组（老年组）的预测精度。为解决这个问题，我们提出了一种新的偏见纠正方法，利用GradCAM基于特征遮盖和ensemble模型，以确保不会降低公平性和精度。具体来说，GradCAM基于特征遮盖选择性地遮盖语音数据中年龄相关的特征，保留PD检测中必要的信息。而ensemble模型进一步提高了少数群（年龄小于或等于55岁）的预测精度。我们的方法可以提高早期PD检测的准确率，而不会损害老年组的表现。此外，我们还提议了一种两步检测策略，为 potential early-onset PD 患者提供实用的风险评估。
</details></li>
</ul>
<hr>
<h2 id="Distributional-Shift-Aware-Off-Policy-Interval-Estimation-A-Unified-Error-Quantification-Framework"><a href="#Distributional-Shift-Aware-Off-Policy-Interval-Estimation-A-Unified-Error-Quantification-Framework" class="headerlink" title="Distributional Shift-Aware Off-Policy Interval Estimation: A Unified Error Quantification Framework"></a>Distributional Shift-Aware Off-Policy Interval Estimation: A Unified Error Quantification Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13278">http://arxiv.org/abs/2309.13278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhuo Zhou, Yuhan Li, Ruoqing Zhu, Annie Qu</li>
<li>for: 该文章的目的是为了在无法预测行为政策下验证高置信度的策略评估。</li>
<li>methods: 该文章使用了一种新的统一错误分析，它同时量化了两种错误来源：模型缺陷和采样统计变化。</li>
<li>results: 该文章的方法可以在无需假设任何弱依赖关系的情况下，对无限时间执行 Markov 决策过程中的目标策略值进行高置信度评估，并且可以适应不同的分布转换。<details>
<summary>Abstract</summary>
We study high-confidence off-policy evaluation in the context of infinite-horizon Markov decision processes, where the objective is to establish a confidence interval (CI) for the target policy value using only offline data pre-collected from unknown behavior policies. This task faces two primary challenges: providing a comprehensive and rigorous error quantification in CI estimation, and addressing the distributional shift that results from discrepancies between the distribution induced by the target policy and the offline data-generating process. Motivated by an innovative unified error analysis, we jointly quantify the two sources of estimation errors: the misspecification error on modeling marginalized importance weights and the statistical uncertainty due to sampling, within a single interval. This unified framework reveals a previously hidden tradeoff between the errors, which undermines the tightness of the CI. Relying on a carefully designed discriminator function, the proposed estimator achieves a dual purpose: breaking the curse of the tradeoff to attain the tightest possible CI, and adapting the CI to ensure robustness against distributional shifts. Our method is applicable to time-dependent data without assuming any weak dependence conditions via leveraging a local supermartingale/martingale structure. Theoretically, we show that our algorithm is sample-efficient, error-robust, and provably convergent even in non-linear function approximation settings. The numerical performance of the proposed method is examined in synthetic datasets and an OhioT1DM mobile health study.
</details>
<details>
<summary>摘要</summary>
我们研究高自信度偏离策略评估在无穷远 horizon Markov决策过程中，目标是使用偏离策略评估数据来建立一个信息interval（CI）的目标策略价值。这个任务面临两个主要挑战：一是提供全面和准确的误差量化，二是Addressing the distributional shift that results from discrepancies between the distribution induced by the target policy and the offline data-generating process。我们受益于一种创新的统一错误分析，可以同时量化两个来源的误差：模型缺陷导致的重要性加权的误差和采样统计误差。这种统一框架显示了一个隐藏的贸易关系，这种关系使得CI的紧密性受到影响。我们采用一种特制的探测函数，该函数可以实现两个目的：打破贸易关系，以便获得最紧密的CI，并适应CI以确保对分布差异的Robustness。我们的方法可以在没有任何弱依赖条件下应用于时间相关的数据，通过利用本地超martingale/martingale结构。理论上，我们显示了我们的算法是样本效率的，误差稳定的，并在非线性函数近似设定下可靠地收敛。我们的方法的数学性能在 sintetic数据和OhioT1DM移动医学研究中进行了数值验证。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Sequential-Decoder-for-Transient-High-Density-Electromyography-in-Hand-Gesture-Recognition-Using-Subject-Embedded-Transfer-Learning"><a href="#A-Deep-Learning-Sequential-Decoder-for-Transient-High-Density-Electromyography-in-Hand-Gesture-Recognition-Using-Subject-Embedded-Transfer-Learning" class="headerlink" title="A Deep Learning Sequential Decoder for Transient High-Density Electromyography in Hand Gesture Recognition Using Subject-Embedded Transfer Learning"></a>A Deep Learning Sequential Decoder for Transient High-Density Electromyography in Hand Gesture Recognition Using Subject-Embedded Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03752">http://arxiv.org/abs/2310.03752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Golara Ahmadi Azar, Qin Hu, Melika Emami, Alyson Fletcher, Sundeep Rangan, S. Farokh Atashzar</li>
<li>for: 这个研究旨在提高人工智能与人类肢体互动的整合，使用深度学习方法来识别手势。</li>
<li>methods: 这个研究使用了深度学习模型，并将Subject-specific transfer learning和多因素混合结构组合在一起，以提高手势识别精度。</li>
<li>results: 研究获得了73%的平均准确率，在65个手势中预测了73%的手势，并且在有限的训练数据下表现比subject-specific方法更好。<details>
<summary>Abstract</summary>
Hand gesture recognition (HGR) has gained significant attention due to the increasing use of AI-powered human-computer interfaces that can interpret the deep spatiotemporal dynamics of biosignals from the peripheral nervous system, such as surface electromyography (sEMG). These interfaces have a range of applications, including the control of extended reality, agile prosthetics, and exoskeletons. However, the natural variability of sEMG among individuals has led researchers to focus on subject-specific solutions. Deep learning methods, which often have complex structures, are particularly data-hungry and can be time-consuming to train, making them less practical for subject-specific applications. In this paper, we propose and develop a generalizable, sequential decoder of transient high-density sEMG (HD-sEMG) that achieves 73% average accuracy on 65 gestures for partially-observed subjects through subject-embedded transfer learning, leveraging pre-knowledge of HGR acquired during pre-training. The use of transient HD-sEMG before gesture stabilization allows us to predict gestures with the ultimate goal of counterbalancing system control delays. The results show that the proposed generalized models significantly outperform subject-specific approaches, especially when the training data is limited, and there is a significant number of gesture classes. By building on pre-knowledge and incorporating a multiplicative subject-embedded structure, our method comparatively achieves more than 13% average accuracy across partially observed subjects with minimal data availability. This work highlights the potential of HD-sEMG and demonstrates the benefits of modeling common patterns across users to reduce the need for large amounts of data for new users, enhancing practicality.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）激活人机界面（HGR）已经吸引了广泛的关注，因为它可以通过解读 périphérique nervous system的深度空间动态特征，如表面电MYography（sEMG）来控制虚拟现实、迅速 prótesis 和 exoskeletons。然而，人类的自然变化导致研究人员更加注重具体化解决方案。深度学习方法，经常具有复杂结构，需要大量数据和训练时间，使其更难实现具体化应用。在这篇论文中，我们提出和开发了一种通用的、顺序解码器，可以在部分观察者下达73%的平均准确率，对65个手势进行预测，通过在pre-training中获得的HGR知识进行嵌入式传播学习。使用transient HD-sEMG передgesture稳定化可以预测手势，以ultimate goal of counterbalancing system control delays。结果表明，我们的总体模型在限制数据量的情况下，特别是有许多手势类型的情况下，较subject-specific方法表现出优异。通过建立在pre-knowledge基础上，并通过multiplicative subject-embedded结构，我们的方法可以在有限的数据可用性下，实现更高的13%的平均准确率。这种工作展示了HD-sEMG的潜力，并证明了模型Users across common patterns可以降低新用户需要的数据量，提高实用性。
</details></li>
</ul>
<hr>
<h2 id="Zen-Near-Optimal-Sparse-Tensor-Synchronization-for-Distributed-DNN-Training"><a href="#Zen-Near-Optimal-Sparse-Tensor-Synchronization-for-Distributed-DNN-Training" class="headerlink" title="Zen: Near-Optimal Sparse Tensor Synchronization for Distributed DNN Training"></a>Zen: Near-Optimal Sparse Tensor Synchronization for Distributed DNN Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13254">http://arxiv.org/abs/2309.13254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuang Wang, Zhaozhuo Xu, Anshumali Shrivastava, T. S. Eugene Ng</li>
<li>for: 这篇论文的目的是提出一个最佳的通信方案，以缩小在分布式训练深度神经网络（DNNs）中的交通量，并提高总训练效率。</li>
<li>methods: 这篇论文使用了系统性地探索了对缓冲量矩阵的通信方案设计空间，以找出最佳的方案。它还开发了一个称为Zen的Gradient Synchronization系统，可以实现这个最佳方案。</li>
<li>results: 这篇论文的结果显示，使用Zen的Gradient Synchronization系统可以实现至多5.09倍的交通时间减少和训练过程中的对比增加，相比之前的方法。<details>
<summary>Abstract</summary>
Distributed training is the de facto standard to scale up the training of Deep Neural Networks (DNNs) with multiple GPUs. The performance bottleneck of distributed training lies in communications for gradient synchronization. Recently, practitioners have observed sparsity in gradient tensors, suggesting the potential to reduce the traffic volume in communication and improve end-to-end training efficiency. Yet, the optimal communication scheme to fully leverage sparsity is still missing. This paper aims to address this gap. We first analyze the characteristics of sparse tensors in popular DNN models to understand the fundamentals of sparsity. We then systematically explore the design space of communication schemes for sparse tensors and find the optimal one. % We then find the optimal scheme based on the characteristics by systematically exploring the design space. We also develop a gradient synchronization system called Zen that approximately realizes it for sparse tensors. We demonstrate that Zen can achieve up to 5.09x speedup in communication time and up to 2.48x speedup in training throughput compared to the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
分布式训练是深度神经网络（DNN）训练的标准化方法，以多个GPU进行扩大。但是，分布式训练的性能瓶颈在交换梯度同步方面。在实践中，人们发现了梯度矩阵中的稀疏性，这表明可以减少交换的流量并提高端到端训练效率。然而，完全利用稀疏性的最佳通信方案仍然缺失。这篇论文的目的是填补这个差距。我们首先分析了流行的DNN模型中稀疏矩阵的特点，以了解稀疏性的基础。然后，我们系统地探索了稀疏矩阵的通信方案的设计空间，并找到最佳的一种。我们还开发了一个名为“Zen”的梯度同步系统，可以对稀疏矩阵进行约束式实现。我们 demonstarte了Zen可以在交换时间方面实现5.09倍的速度提高和在训练吞吐量方面实现2.48倍的速度提高，比现有方法更高。
</details></li>
</ul>
<hr>
<h2 id="Importance-of-negative-sampling-in-weak-label-learning"><a href="#Importance-of-negative-sampling-in-weak-label-learning" class="headerlink" title="Importance of negative sampling in weak label learning"></a>Importance of negative sampling in weak label learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13227">http://arxiv.org/abs/2309.13227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankit Shah, Fuyu Tang, Zelin Ye, Rita Singh, Bhiksha Raj</li>
<li>for: 这个论文的目的是研究如何在弱标注学习中选择最有用的负例。</li>
<li>methods: 该论文使用了多种采样策略来评估负例的用于弱标注学习中的有用性，并选择其中的最有用的负例。</li>
<li>results: 该论文在CIFAR-10和AudioSet datasets上进行测试，并显示了减少计算成本和提高弱标注分类性能的result。<details>
<summary>Abstract</summary>
Weak-label learning is a challenging task that requires learning from data "bags" containing positive and negative instances, but only the bag labels are known. The pool of negative instances is usually larger than positive instances, thus making selecting the most informative negative instance critical for performance. Such a selection strategy for negative instances from each bag is an open problem that has not been well studied for weak-label learning. In this paper, we study several sampling strategies that can measure the usefulness of negative instances for weak-label learning and select them accordingly. We test our method on CIFAR-10 and AudioSet datasets and show that it improves the weak-label classification performance and reduces the computational cost compared to random sampling methods. Our work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning.
</details>
<details>
<summary>摘要</summary>
弱标记学习是一项具有挑战性的任务，需要从包含正例和负例的数据袋中学习，但只知道包袋标签。负例pool通常比正例pool更大，因此选择每个包袋中最有用的负例是一个开放的问题，尚未得到了充分的研究。在这篇论文中，我们研究了一些采样策略，可以衡量负例对弱标记学习的用于fulfillment，并根据此选择负例。我们在CIFAR-10和AudioSet数据集上测试了我们的方法，并证明它可以提高弱标记分类性能和降低计算成本，相比随机采样方法。我们的工作表明，负例不是一样无关，选择它们谨慎可以帮助弱标记学习。
</details></li>
</ul>
<hr>
<h2 id="Grad-DFT-a-software-library-for-machine-learning-enhanced-density-functional-theory"><a href="#Grad-DFT-a-software-library-for-machine-learning-enhanced-density-functional-theory" class="headerlink" title="Grad DFT: a software library for machine learning enhanced density functional theory"></a>Grad DFT: a software library for machine learning enhanced density functional theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15127">http://arxiv.org/abs/2309.15127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XanaduAI/GradDFT">https://github.com/XanaduAI/GradDFT</a></li>
<li>paper_authors: Pablo A. M. Casares, Jack S. Baker, Matija Medvidovic, Roberto dos Reis, Juan Miguel Arrazola</li>
<li>for: 本研究旨在扩展density functional theory（DFT）的可Accuracy，特别是在强 correlate系统中。</li>
<li>methods: 该研究使用机器学习技术来扩展DFT的能力，并采用了一种新的 exchange-correlation functional parametrization方法，其中使用神经网络确定函数的重要性。</li>
<li>results: 研究人员开发了一个名为Grad DFT的完全可导的JAX基础库，可以快速实现和试验机器学习提高DFT的exchange-correlation能量函数。此外，研究人员还编译了一个精心选择的实验数据集，用于训练和测试模型的准确性。<details>
<summary>Abstract</summary>
Density functional theory (DFT) stands as a cornerstone method in computational quantum chemistry and materials science due to its remarkable versatility and scalability. Yet, it suffers from limitations in accuracy, particularly when dealing with strongly correlated systems. To address these shortcomings, recent work has begun to explore how machine learning can expand the capabilities of DFT; an endeavor with many open questions and technical challenges. In this work, we present Grad DFT: a fully differentiable JAX-based DFT library, enabling quick prototyping and experimentation with machine learning-enhanced exchange-correlation energy functionals. Grad DFT employs a pioneering parametrization of exchange-correlation functionals constructed using a weighted sum of energy densities, where the weights are determined using neural networks. Moreover, Grad DFT encompasses a comprehensive suite of auxiliary functions, notably featuring a just-in-time compilable and fully differentiable self-consistent iterative procedure. To support training and benchmarking efforts, we additionally compile a curated dataset of experimental dissociation energies of dimers, half of which contain transition metal atoms characterized by strong electronic correlations. The software library is tested against experimental results to study the generalization capabilities of a neural functional across potential energy surfaces and atomic species, as well as the effect of training data noise on the resulting model accuracy.
</details>
<details>
<summary>摘要</summary>
density functional theory（DFT）是计算量子化学和材料科学中的一种重要方法，它具有优秀的 universality 和可扩展性。然而，它在强 correlate 系统中的准确性有限制。为了解决这些缺陷，最近的工作开始使用机器学习技术来扩展 DFT 的能力；这是一个充满开放 вопросов和技术挑战的尝试。在这个工作中，我们提出了 Grad DFT：一个完全可导的 JAX 基础库，允许快速的原型和机器学习增强 exchange-correlation 能量函数的 экспериментирование。Grad DFT 使用一种先进的 exchange-correlation 函数的参数化方法，该方法通过使用神经网络确定的权重，将 exchange-correlation 函数转化为一个可导的形式。此外，Grad DFT 还包括一系列辅助函数，其中包括一个可编译的和完全可导的自 consistent 迭代过程。为支持训练和测试努力，我们还编译了一个权威的对应的实验性离解能量数据集，该数据集包括含有过渡金属原子的 dimer 的实验离解能量，这些金属原子具有强电子 correlate 性。软件库在实验结果上进行测试，以研究一个神经函数在 potential energy surface 和原子种之间的泛化能力，以及训练数据噪声对模型准确性的影响。
</details></li>
</ul>
<hr>
<h2 id="Causal-Reasoning-Charting-a-Revolutionary-Course-for-Next-Generation-AI-Native-Wireless-Networks"><a href="#Causal-Reasoning-Charting-a-Revolutionary-Course-for-Next-Generation-AI-Native-Wireless-Networks" class="headerlink" title="Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks"></a>Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13223">http://arxiv.org/abs/2309.13223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christo Kurisummoottil Thomas, Christina Chaccour, Walid Saad, Merouane Debbah, Choong Seon Hong</li>
<li>for: 本文提出了一种全面、前瞻的视野，以响应现有的 wireless 网络挑战，通过 causal reasoning 建立可解释、理解、可持续的无线网络。</li>
<li>methods: 本文提出了一种基于 causal discovery、causal representation learning 和 causal inference 的新框架，用于建立 AI-native 无线网络。</li>
<li>results: 本文指出，通过 incorporating causal discovery，可以解决无线网络中的一些挑战，如 ultra-reliable beamforming、near-accurate physical twin modeling、training data augmentation 和 semantic communication。同时，本文还提出了一些可能的框架，用于通过 causal inference 实现未来无线网络的总体目标，包括意图管理、动态适应性、人类水平的认知和理解。<details>
<summary>Abstract</summary>
Despite the basic premise that next-generation wireless networks (e.g., 6G) will be artificial intelligence (AI)-native, to date, most existing efforts remain either qualitative or incremental extensions to existing ``AI for wireless'' paradigms. Indeed, creating AI-native wireless networks faces significant technical challenges due to the limitations of data-driven, training-intensive AI. These limitations include the black-box nature of the AI models, their curve-fitting nature, which can limit their ability to reason and adapt, their reliance on large amounts of training data, and the energy inefficiency of large neural networks. In response to these limitations, this article presents a comprehensive, forward-looking vision that addresses these shortcomings by introducing a novel framework for building AI-native wireless networks; grounded in the emerging field of causal reasoning. Causal reasoning, founded on causal discovery, causal representation learning, and causal inference, can help build explainable, reasoning-aware, and sustainable wireless networks. Towards fulfilling this vision, we first highlight several wireless networking challenges that can be addressed by causal discovery and representation, including ultra-reliable beamforming for terahertz (THz) systems, near-accurate physical twin modeling for digital twins, training data augmentation, and semantic communication. We showcase how incorporating causal discovery can assist in achieving dynamic adaptability, resilience, and cognition in addressing these challenges. Furthermore, we outline potential frameworks that leverage causal inference to achieve the overarching objectives of future-generation networks, including intent management, dynamic adaptability, human-level cognition, reasoning, and the critical element of time sensitivity.
</details>
<details>
<summary>摘要</summary>
尽管下一代无线网络（如6G）将是人工智能（AI）Native，但到目前为止，大多数现有努力仍然是质量的或增量的对现有“AI for wireless” paradigms的扩展。实际上，创建AI Native的无线网络面临着 significativetchnical挑战，主要是因为AI模型的黑盒性、curve-fitting性、需要大量训练数据、以及大 neural networks的能源浪费。为了解决这些挑战，这篇文章提出了一个全面的、前瞻的视野，通过引入一种新的AI Native无线网络框架来解决这些缺陷。这个框架基于emerging field of causal reasoning，可以帮助建立可解释、 reasoning-aware 和可持续的无线网络。为实现这个视野，我们首先 highlight了一些无线网络挑战可以通过 causal discovery 和 representation learning来解决，包括THz系统中的可靠性 beamforming、数字 twin 模型化、训练数据增强和semantic communication。我们示出了如何通过 causal discovery 来实现动态适应、抗难以适应和认知的能力。此外，我们还 outline了可以利用 causal inference 来实现未来 generation networks 的主要目标，包括意图管理、动态适应、人类水平的认知、reasoning 和时间敏感性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/23/cs.LG_2023_09_23/" data-id="clpztdnl800s4es8840i7e0ld" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/23/eess.IV_2023_09_23/" class="article-date">
  <time datetime="2023-09-23T09:00:00.000Z" itemprop="datePublished">2023-09-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/23/eess.IV_2023_09_23/">eess.IV - 2023-09-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Gaining-Insights-into-Denoising-by-Inpainting"><a href="#Gaining-Insights-into-Denoising-by-Inpainting" class="headerlink" title="Gaining Insights into Denoising by Inpainting"></a>Gaining Insights into Denoising by Inpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13486">http://arxiv.org/abs/2309.13486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Gaa, Vassillen Chizhov, Pascal Peter, Joachim Weickert, Robin Dirk Adam</li>
<li>for: 这个论文的目的是研究一种基于扩散过程的图像分析技术，包括填充-基于压缩和稠密运动计算。</li>
<li>methods: 这个论文使用了多种方法，包括多个不同的扩散子集的填充结果的平均值，以及改变函数值的方法来提高全局逼近质量。</li>
<li>results: 实验表明，使用不同的扩散方法不会提高重建质量，而是数据适应性更重要。此外，这个论文还提出了一些基本的理论和估计结果，包括在1-D情况下的等价关系。<details>
<summary>Abstract</summary>
The filling-in effect of diffusion processes is a powerful tool for various image analysis tasks such as inpainting-based compression and dense optic flow computation. For noisy data, an interesting side effect occurs: The interpolated data have higher confidence, since they average information from many noisy sources. This observation forms the basis of our denoising by inpainting (DbI) framework. It averages multiple inpainting results from different noisy subsets. Our goal is to obtain fundamental insights into key properties of DbI and its connections to existing methods. Like in inpainting-based image compression, we choose homogeneous diffusion as a very simple inpainting operator that performs well for highly optimized data. We propose several strategies to choose the location of the selected pixels. Moreover, to improve the global approximation quality further, we also allow to change the function values of the noisy pixels. In contrast to traditional denoising methods that adapt the operator to the data, our approach adapts the data to the operator. Experimentally we show that replacing homogeneous diffusion inpainting by biharmonic inpainting does not improve the reconstruction quality. This again emphasizes the importance of data adaptivity over operator adaptivity. On the foundational side, we establish deterministic and probabilistic theories with convergence estimates. In the non-adaptive 1-D case, we derive equivalence results between DbI on shifted regular grids and classical homogeneous diffusion filtering via an explicit relation between the density and the diffusion time.
</details>
<details>
<summary>摘要</summary>
Diffusion 过程中的填充效果是许多图像分析任务的有力工具，如填充基于压缩和稠密光流计算。对于噪声污染的数据，有一个 interessante 的侧效： interpolated 数据具有更高的信任度，因为它们平均了许多噪声来源的信息。这个观察成为我们denoising by inpainting（DbI）框架的基础。DbI 平均了不同噪声子集的多个填充结果。我们的目标是获得基本的洞察和现有方法的连接。与填充基于图像压缩类似，我们选择了高度一致的扩散作为非常简单的填充算子，它在高度优化的数据上表现良好。我们还提出了多种选择选择的像素位置策略，以及改进全局approximation质量的方法。与传统的噪声除法方法不同，我们的方法将数据适应到算子而不是适应到数据。实验表明，将Homogeneous替换为Biharmonic不会提高重建质量。这再次强调了数据适应性的重要性，而不是算子适应性。在基础方面，我们建立了deterministic和probabilistic 理论，并提供了收敛估计。在非适应的1-D情况下，我们 derivation 了DbI 在偏移的正规网格上和经典扩散滤波器之间的等价关系，这种关系可以用来描述density 和扩散时间之间的直接关系。
</details></li>
</ul>
<hr>
<h2 id="Design-of-Novel-Loss-Functions-for-Deep-Learning-in-X-ray-CT"><a href="#Design-of-Novel-Loss-Functions-for-Deep-Learning-in-X-ray-CT" class="headerlink" title="Design of Novel Loss Functions for Deep Learning in X-ray CT"></a>Design of Novel Loss Functions for Deep Learning in X-ray CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14367">http://arxiv.org/abs/2309.14367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Obaidullah Rahman, Ken D. Sauer, Madhuri Nagare, Charles A. Bouman, Roman Melnyk, Jie Tang, Brian Nett</li>
<li>for: 提高透射计算机断层（CT）图像质量</li>
<li>methods: 使用深度学习（DL）方法，包括在数据频谱域和重建图像域中进行训练</li>
<li>results: 提出创新的损失函数方法，以更好地衡量图像质量和频谱内容的损失，以提高CT图像重建的精度<details>
<summary>Abstract</summary>
Deep learning (DL) shows promise of advantages over conventional signal processing techniques in a variety of imaging applications. The networks' being trained from examples of data rather than explicitly designed allows them to learn signal and noise characteristics to most effectively construct a mapping from corrupted data to higher quality representations. In inverse problems, one has options of applying DL in the domain of the originally captured data, in the transformed domain of the desired final representation, or both.   X-ray computed tomography (CT), one of the most valuable tools in medical diagnostics, is already being improved by DL methods. Whether for removal of common quantum noise resulting from the Poisson-distributed photon counts, or for reduction of the ill effects of metal implants on image quality, researchers have begun employing DL widely in CT. The selection of training data is driven quite directly by the corruption on which the focus lies. However, the way in which differences between the target signal and measured data is penalized in training generally follows conventional, pointwise loss functions.   This work introduces a creative technique for favoring reconstruction characteristics that are not well described by norms such as mean-squared or mean-absolute error. Particularly in a field such as X-ray CT, where radiologists' subjective preferences in image characteristics are key to acceptance, it may be desirable to penalize differences in DL more creatively. This penalty may be applied in the data domain, here the CT sinogram, or in the reconstructed image. We design loss functions for both shaping and selectively preserving frequency content of the signal.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）在各种成像应用中显示出优势，比如传统的信号处理技术。DL网络从数据示例而不是直接设计，因此可以学习信号和噪声特征，以最有效地构建受损数据到高质量表示的映射。在逆问题中，可以在原始数据频谱中应用DL，在愿望的最终表示频谱中应用DL，或者两者都应用。X射针 Computed Tomography（CT）是医学诊断中最重要的工具，已经由DL方法进行改进。DL可以用于去除常见的量子噪声，或者去除金属implant的影响而导致的图像质量下降。选择训练数据的驱动因素受到受损的影响很直接。然而，在训练中对目标信号和测量数据之间的差别进行惩罚通常采用传统的点均方差或点绝对差惩罚函数。本工作介绍了一种创新的技术，即在DL中不以均方或绝对差惩罚函数来惩罚差别。特别在X射针CT领域， где radiologists的主观偏好在图像特征上对接受性至关重要。在这种情况下，可能需要通过更创新的惩罚方式来惩罚DL。我们设计了在数据频谱中和重建图像中应用的损失函数，以Shape和选择性保留信号的频谱特征。
</details></li>
</ul>
<hr>
<h2 id="Statistically-Adaptive-Filtering-for-Low-Signal-Correction-in-X-ray-Computed-Tomography"><a href="#Statistically-Adaptive-Filtering-for-Low-Signal-Correction-in-X-ray-Computed-Tomography" class="headerlink" title="Statistically Adaptive Filtering for Low Signal Correction in X-ray Computed Tomography"></a>Statistically Adaptive Filtering for Low Signal Correction in X-ray Computed Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13406">http://arxiv.org/abs/2309.13406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Obaidullah Rahman, Ken D. Sauer, Charles A. Bouman, Roman Melnyk, Brian Nett</li>
<li>for: 实现低X射线剂量CT图像成像，并且维护适当的医学效果。</li>
<li>methods: 使用灵活范围滤波器来缓和低信号领域的残留artefacts。</li>
<li>results: 提高低频率偏好、条状artefacts、本地平均值和标准差、模拟转换函数和杂音功率спектrum等指标。<details>
<summary>Abstract</summary>
Low x-ray dose is desirable in x-ray computed tomographic (CT) imaging due to health concerns. But low dose comes with a cost of low signal artifacts such as streaks and low frequency bias in the reconstruction. As a result, low signal correction is needed to help reduce artifacts while retaining relevant anatomical structures.   Low signal can be encountered in cases where sufficient number of photons do not reach the detector to have confidence in the recorded data. % NOTE: SNR is ratio of powers, not std. dev. X-ray photons, assumed to have Poisson distribution, have signal to noise ratio proportional to the dose, with poorer SNR in low signal areas. Electronic noise added by the data acquisition system further reduces the signal quality.   In this paper we will demonstrate a technique to combat low signal artifacts through adaptive filtration. It entails statistics-based filtering on the uncorrected data, correcting the lower signal areas more aggressively than the high signal ones. We look at local averages to decide how aggressive the filtering should be, and local standard deviation to decide how much detail preservation to apply. Implementation consists of a pre-correction step i.e. local linear minimum mean-squared error correction, followed by a variance stabilizing transform, and finally adaptive bilateral filtering. The coefficients of the bilateral filter are computed using local statistics. Results show that improvements were made in terms of low frequency bias, streaks, local average and standard deviation, modulation transfer function and noise power spectrum.
</details>
<details>
<summary>摘要</summary>
低剂量X射线是在X射线计算机断层成像（CT）中所需的，因为它可以降低健康风险。然而，低剂量也会导致低信号artefacts，如斜线和低频偏好。为了减少这些artefacts，而不失去有关生物结构的信息，需要进行低信号修正。低信号可以在具有不足的X射线 фотоン数据 recording 时出现，这会导致信号质量下降。在这种情况下，X射线 photons 的信号噪声比（SNR）会随剂量的增加。electronic noise 由数据获取系统添加到数据中，进一步减少信号质量。本文将介绍一种用于解决低信号artefacts的技术 - 适应 filters。这种技术基于统计分析，通过对未经修正的数据进行统计分析，更加严格地修正低信号区域。我们根据本地平均值和本地标准差来决定修正的程度，以保留生物结构的细节。实现方式包括先进行本地线性最小二乘均值修正，然后应用变量稳定化变换，最后使用适应二值滤波。适应滤波的系数是根据本地统计来计算的。结果表明，该技术可以提高低频偏好、斜线、本地平均值和标准差、模ulation transfer function 和噪声电力谱的性能。
</details></li>
</ul>
<hr>
<h2 id="MBIR-Training-for-a-2-5D-DL-network-in-X-ray-CT"><a href="#MBIR-Training-for-a-2-5D-DL-network-in-X-ray-CT" class="headerlink" title="MBIR Training for a 2.5D DL network in X-ray CT"></a>MBIR Training for a 2.5D DL network in X-ray CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13399">http://arxiv.org/abs/2309.13399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Obaidullah Rahman, Madhuri Nagare, Ken D. Sauer, Charles A. Bouman, Roman Melnyk, Brian Nett, Jie Tang</li>
<li>for: 这个论文目的是使用深度学习模型来快速实现基于模型的迭代重建图像技术（MBIR）的高品质图像。</li>
<li>methods: 这个论文使用了一种基于Unet的modified 2.5D深度学习网络来模仿MBIR图像。</li>
<li>results: 研究发现，使用这种深度学习模型可以快速获得高品质MBIR图像，并且计算成本远低于传统的MBIR方法。图像的文本特征和噪声功率谱都与MBIR图像相似，表明深度学习模型成功模拟了MBIR操作。<details>
<summary>Abstract</summary>
In computed tomographic imaging, model based iterative reconstruction methods have generally shown better image quality than the more traditional, faster filtered backprojection technique. The cost we have to pay is that MBIR is computationally expensive. In this work we train a 2.5D deep learning (DL) network to mimic MBIR quality image. The network is realized by a modified Unet, and trained using clinical FBP and MBIR image pairs. We achieve the quality of MBIR images faster and with a much smaller computation cost. Visually and in terms of noise power spectrum (NPS), DL-MBIR images have texture similar to that of MBIR, with reduced noise power. Image profile plots, NPS plots, standard deviation, etc. suggest that the DL-MBIR images result from a successful emulation of an MBIR operator.
</details>
<details>
<summary>摘要</summary>
在计算tomografic imaging中，基于模型的迭代重建方法通常会提供更好的图像质量，相比较传统的快速滤波后 проекcion技术。然而，MBIR是计算成本高的。在这项工作中，我们使用一个modified U-Net架构来模拟MBIR图像质量。我们使用临床FBP和MBIR图像对的 pairs来训练网络，并在计算成本下降的情况下实现MBIR图像质量。视觉和噪声电磁谱（NPS）等指标表明，DL-MBIR图像具有与MBIR图像相似的 текстура，噪声电磁谱下降。图像profile plot、NPS plot等指标表明，DL-MBIR图像是一个成功地模拟MBIROperator的结果。
</details></li>
</ul>
<hr>
<h2 id="Direct-Iterative-Reconstruction-of-Multiple-Basis-Material-Images-in-Photon-counting-Spectral-CT"><a href="#Direct-Iterative-Reconstruction-of-Multiple-Basis-Material-Images-in-Photon-counting-Spectral-CT" class="headerlink" title="Direct Iterative Reconstruction of Multiple Basis Material Images in Photon-counting Spectral CT"></a>Direct Iterative Reconstruction of Multiple Basis Material Images in Photon-counting Spectral CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13397">http://arxiv.org/abs/2309.13397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Obaidullah Rahman, Ken Sauer, Connor Evans, Ryan Roeder</li>
<li>for: 这项研究旨在利用基于模型的迭代重建(MBIR)方法直接从спектральCT数据中重建材料。</li>
<li>methods: 该方法使用了一种基于模型的迭代重建方法，其中材料含量测量为体积分数，总为最大值 unity。使用了iodine和gadolinium作为常见的contrast agent，并使用了一个包含这两种材料的phantom。</li>
<li>results: 在low-concentration scan中，使用了这种方法可以在ROIs中获得volume fractions的 Close to ground truth值。这项研究旨在为将来包含空间含义和&#x2F;或材料含量regularization的phantoms、动物成像和临床应用铺垫。<details>
<summary>Abstract</summary>
In this work, we perform direct material reconstruction from spectral CT data using a model based iterative reconstruction (MBIR) approach. Material concentrations are measured in volume fractions, whose total is constrained by a maximum of unity. A phantom containing a combination of 4 basis materials (water, iodine, gadolinium, calcium) was scanned using a photon-counting detector. Iodine and gadolinium were chosen because of their common use as contrast agents in CT imaging. Scan data was binned into 5 energy (keV) levels. Each energy bin in a calibration scan was reconstructed, allowing the linear attenuation coefficient of each material for every energy to be estimated by a least-squares fit to ground truth in the image domain. The resulting $5\times 4$ matrix, for $5$ energies and $4$ materials, is incorporated into the forward model in direct reconstruction of the $4$ basis material images with spatial and/or inter-material regularization. In reconstruction from a subsequent low-concentration scan, volume fractions within regions of interest (ROIs) are found to be close to the ground truth. This work is meant to lay the foundation for further work with phantoms including spatially coincident mixtures of contrast materials and/or contrast agents in widely varying concentrations, molecular imaging from animal scans, and eventually clinical applications.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们使用基于模型的迭代重建（MBIR）方法直接重建物质图像从 spectral CT 数据。物质浓度表示为体积分数，总是受限于最大unity。一个包含四种基本材料（水、iodine、gadolinium、 calcium）的phantom在一个photon-counting 探测器上进行了扫描。iodine 和 gadolinium 选择是因为它们在 CT 图像中广泛使用为contrast agent。扫描数据被分割成5个能量（keV）层。每个能量层在一个calibration scan中的每个像素的直线吸收系数可以通过对真实图像中的图像域最小二乘来估算。这个 $5\times 4$ 矩阵，其中有5个能量和4种材料，被 incorporated 到了直接重建的 forward 模型中。在重建从后续的低浓度扫描中，ROIs 中的体积分数几乎与真实值一致。这项工作的目的是为了铺垫将来的灵活材料混合物和高级分子成像、动物扫描和临床应用。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Communications-using-Foundation-Models-Design-Approaches-and-Open-Issues"><a href="#Semantic-Communications-using-Foundation-Models-Design-Approaches-and-Open-Issues" class="headerlink" title="Semantic Communications using Foundation Models: Design Approaches and Open Issues"></a>Semantic Communications using Foundation Models: Design Approaches and Open Issues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13315">http://arxiv.org/abs/2309.13315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiwen Jiang, Chao-Kai Wen, Xinping Yi, Xiao Li, Shi Jin, Jun Zhang</li>
<li>for: This paper aims to investigate the impact of foundation models (FMs) on different system levels, including computation and memory complexity, and to explore the use of compact models to balance performance and complexity.</li>
<li>methods: The paper uses universal knowledge to profoundly transform system design and employs three separate approaches that employ FMs to balance performance and complexity.</li>
<li>results: The study highlights unresolved issues in the field that need addressing, and provides insights into the effectiveness, semantic, and physical levels of system design.<details>
<summary>Abstract</summary>
Foundation models (FMs), including large language models, have become increasingly popular due to their wide-ranging applicability and ability to understand human-like semantics. While previous research has explored the use of FMs in semantic communications to improve semantic extraction and reconstruction, the impact of these models on different system levels, considering computation and memory complexity, requires further analysis. This study focuses on integrating FMs at the effectiveness, semantic, and physical levels, using universal knowledge to profoundly transform system design. Additionally, it examines the use of compact models to balance performance and complexity, comparing three separate approaches that employ FMs. Ultimately, the study highlights unresolved issues in the field that need addressing.
</details>
<details>
<summary>摘要</summary>
基于语言模型（FM）的应用，包括大型语言模型，在各种领域得到广泛应用，这主要归功于它们能够理解人类语言 semantics。 although previous research has explored the use of FMs in semantic communications to improve semantic extraction and reconstruction, the impact of these models on different system levels, considering computation and memory complexity, requires further analysis.本研究强调在效iveness、semantic和physical各级别中集成FMs，使用通用知识进行深度变换系统设计。此外，它还研究了使用压缩模型来平衡性能和复杂性，对三种使用FMs的方法进行比较。最终，研究披露了这个领域中还有待解决的问题。Here's the word-for-word translation:基于语言模型（FM）的应用，包括大型语言模型，在各种领域得到广泛应用，这主要归功于它们能够理解人类语言 semantics。 although previous research has explored the use of FMs in semantic communications to improve semantic extraction and reconstruction, the impact of these models on different system levels, considering computation and memory complexity, requires further analysis.本研究强调在效iveness、semantic和physical各级别中集成FMs，使用通用知识进行深度变换系统设计。此外，它还研究了使用压缩模型来平衡性能和复杂性，对三种使用FMs的方法进行比较。最终，研究披露了这个领域中还有待解决的问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/23/eess.IV_2023_09_23/" data-id="clpztdnsn01axes880rqdd6is" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/23/eess.SP_2023_09_23/" class="article-date">
  <time datetime="2023-09-23T08:00:00.000Z" itemprop="datePublished">2023-09-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/23/eess.SP_2023_09_23/">eess.SP - 2023-09-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sens-BERT-Enabling-Transferability-and-Re-calibration-of-Calibration-Models-for-Low-cost-Sensors-under-Reference-Measurements-Scarcity"><a href="#Sens-BERT-Enabling-Transferability-and-Re-calibration-of-Calibration-Models-for-Low-cost-Sensors-under-Reference-Measurements-Scarcity" class="headerlink" title="Sens-BERT: Enabling Transferability and Re-calibration of Calibration Models for Low-cost Sensors under Reference Measurements Scarcity"></a>Sens-BERT: Enabling Transferability and Re-calibration of Calibration Models for Low-cost Sensors under Reference Measurements Scarcity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13390">http://arxiv.org/abs/2309.13390</a></li>
<li>repo_url: None</li>
<li>paper_authors: M V Narayana, Kranthi Kumar Rachvarapu, Devendra Jalihal, Shiva Nagendra S M<br>for:这个研究旨在提高低成本测器（LCS）的精确性，以便在空气质量监控中大规模地应用。methods:这个研究使用了一种基于BERT的学习方法，称为Sens-BERT，来对LCS进行准确化。这个方法分成两个阶段：自主学习预训和精确训练。在预训阶段，Sens-BERT被训练以使其学习LCS测器的资料分布特征，并生成对应的嵌入。在精确训练阶段，我们使用Sens-BERT嵌入来学习一个准确化模型。results:这个研究的结果显示，Sens-BERT可以对LCS进行高精确性的准确化，而且不需要大量的对照站资料或频繁的重新准确化。此外，Sens-BERT可以跨测器和位置进行转移学习，因此可以在不同的测器和位置上进行准确化。<details>
<summary>Abstract</summary>
Low-cost sensors measurements are noisy, which limits large-scale adaptability in airquality monitoirng. Calibration is generally used to get good estimates of air quality measurements out from LCS. In order to do this, LCS sensors are typically co-located with reference stations for some duration. A calibration model is then developed to transfer the LCS sensor measurements to the reference station measurements. Existing works implement the calibration of LCS as an optimization problem in which a model is trained with the data obtained from real-time deployments; later, the trained model is employed to estimate the air quality measurements of that location. However, this approach is sensor-specific and location-specific and needs frequent re-calibration. The re-calibration also needs massive data like initial calibration, which is a cumbersome process in practical scenarios.   To overcome these limitations, in this work, we propose Sens-BERT, a BERT-inspired learning approach to calibrate LCS, and it achieves the calibration in two phases: self-supervised pre-training and supervised fine-tuning. In the pre-training phase, we train Sens-BERT with only LCS data (without reference station observations) to learn the data distributional features and produce corresponding embeddings. We then use the Sens-BERT embeddings to learn a calibration model in the fine-tuning phase. Our proposed approach has many advantages over the previous works. Since the Sens-BERT learns the behaviour of the LCS, it can be transferable to any sensor of the same sensing principle without explicitly training on that sensor. It requires only LCS measurements in pre-training to learn the characters of LCS, thus enabling calibration even with a tiny amount of paired data in fine-tuning. We have exhaustively tested our approach with the Community Air Sensor Network (CAIRSENSE) data set, an open repository for LCS.
</details>
<details>
<summary>摘要</summary>
低成本感测数据具有噪声，限制了大规模适应性在空气质量监测中。通常情况下，使用均拌法来获得良好的空气质量测量结果。为了实现这一点，低成本感测器通常会与参照站同时进行数据采集。然后，通过开发一个均拌模型，将低成本感测器的测量结果转换为参照站的测量结果。现有的方法通常是通过实时部署来训练一个模型，然后使用这个训练好的模型来估计当地的空气质量测量结果。然而，这种方法具有感测器和地点特定的限制，需要频繁重新均拌，并且重新均拌需要大量的数据，如初始均拌，这在实际应用中是一个繁琐的过程。为了解决这些限制，在这项工作中，我们提出了一种基于BERT的学习方法来均拌低成本感测器。我们的方法分为两个阶段：自主启动阶段和精度调整阶段。在自主启动阶段，我们使用只有低成本感测器数据（没有参照站观测）来帮助Sens-BERT学习数据分布特征，并生成相应的嵌入。然后，在精度调整阶段，我们使用Sens-BERT嵌入来学习一个均拌模型。我们的方法具有许多优势。因为Sens-BERT学习了低成本感测器的行为，因此它可以在任何相同感测原理的感测器上进行传输学习，不需要单独对每个感测器进行均拌。此外，我们只需要在启动阶段使用低成本感测器数据来学习低成本感测器的特征，因此在精度调整阶段只需要小量的配对数据，这在实际应用中是一个方便的。我们在社区空气感测网络（CAIRSENSE）数据集上进行了广泛的测试，并证明了我们的方法的可行性。
</details></li>
</ul>
<hr>
<h2 id="Multi-Static-ISAC-in-Cell-Free-Massive-MIMO-Precoder-Design-and-Privacy-Assessment"><a href="#Multi-Static-ISAC-in-Cell-Free-Massive-MIMO-Precoder-Design-and-Privacy-Assessment" class="headerlink" title="Multi-Static ISAC in Cell-Free Massive MIMO: Precoder Design and Privacy Assessment"></a>Multi-Static ISAC in Cell-Free Massive MIMO: Precoder Design and Privacy Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13368">http://arxiv.org/abs/2309.13368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/isabella-gomes/globecom2023">https://github.com/isabella-gomes/globecom2023</a></li>
<li>paper_authors: Isabella W. G. da Silva, Diana P. M. Osorio, Markku Juntti</li>
<li>for: 本研究旨在提高 Cell-free 大量多输入多输出基础设施上的感知通信网络的多样性和功率消耗。</li>
<li>methods: 本文使用了jointly optimizes 的秘密预编码器设计来满足感知和通信需求，并考虑了内部敌对者的攻击。</li>
<li>results: 结果表明，在多Static 环境中，可以更精准地估算目标位置，比单Static 实现更好。<details>
<summary>Abstract</summary>
A multi-static sensing-centric integrated sensing and communication (ISAC) network can take advantage of the cell-free massive multiple-input multiple-output infrastructure to achieve remarkable diversity gains and reduced power consumption. While the conciliation of sensing and communication requirements is still a challenge, the privacy of the sensing information is a growing concern that should be seriously taken on the design of these systems to prevent other attacks. This paper tackles this issue by assessing the probability of an internal adversary to infer the target location information from the received signal by considering the design of transmit precoders that jointly optimizes the sensing and communication requirements in a multi-static-based cell-free ISAC network. Our results show that the multi-static setting facilitates a more precise estimation of the location of the target than the mono-static implementation.
</details>
<details>
<summary>摘要</summary>
一种多Static感知中心Integrated sensing and communication（ISAC）网络可以利用无细结构巨量多输入多输出基础设施，实现Remarkable的多样性增强和降低功率消耗。虽然感知和通信需求的妥协仍然是一个挑战，但感知信息的隐私问题在这些系统的设计中应该严重考虑，以防止其他攻击。本文通过评估接收信号中target位置信息的泄露概率，来评估 transmit precoder的设计，并jointly optimizes the sensing and communication requirements in a multi-static-based cell-free ISAC network。我们的结果表明，在多Static设计下，可以更准确地估计目标的位置，比单Static实现更高精度。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-Robust-Header-Compression-under-Model-Uncertainty"><a href="#Reinforcement-Learning-for-Robust-Header-Compression-under-Model-Uncertainty" class="headerlink" title="Reinforcement Learning for Robust Header Compression under Model Uncertainty"></a>Reinforcement Learning for Robust Header Compression under Model Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13291">http://arxiv.org/abs/2309.13291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shusen Jing, Songyang Zhang, Zhi Ding</li>
<li>For: This paper investigates the integration of bi-directional header compression (BD-ROHC) with reinforcement learning (RL) to improve data efficiency in modern wireless communication systems.* Methods: The paper formulates a partially observable Markov decision process (POMDP) to model the compression process, and uses a deep Q-network (DQN) to learn the optimal compression policy.* Results: Compared to ideal dynamic programming (DP), the proposed method is more scalable and does not require prior knowledge of the transition dynamics or accurate observation dependency of the model.<details>
<summary>Abstract</summary>
Robust header compression (ROHC), critically positioned between the network and the MAC layers, plays an important role in modern wireless communication systems for improving data efficiency. This work investigates bi-directional ROHC (BD-ROHC) integrated with a novel architecture of reinforcement learning (RL). We formulate a partially observable \emph{Markov} decision process (POMDP), in which agent is the compressor, and the environment consists of the decompressor, channel and header source. Our work adopts the well-known deep Q-network (DQN), which takes the history of actions and observations as inputs, and outputs the Q-values of corresponding actions. Compared with the ideal dynamic programming (DP) proposed in the existing works, our method is scalable to the state, action and observation spaces. In contrast, DP often suffers from formidable computational complexity when the number of states becomes large due to long decompressor feedback delay and complex channel models. In addition, our method does not require prior knowledge of the transition dynamics and accurate observation dependency of the model, which are often not available in many practical applications.
</details>
<details>
<summary>摘要</summary>
Robust header compression（ROHC），位于网络和 MAC 层之间，在现代无线通信系统中扮演着重要的角色，以提高数据效率。这项工作 investigate 双向 ROHC（BD-ROHC）与 reinforcement learning（RL）新的架构相结合。我们将 partially observable 马尔可夫决策过程（POMDP）形式ulated，其中 compressor 是 agent，环境包括 decompressor、通道和 header source。我们采用了著名的深度优化网络（DQN），它接受了历史动作和观察输入，并输出对应动作的 Q-值。相比于现有的理想动态计划（DP），我们的方法可扩展到状态、动作和观察空间。而 DP 则经常由长 decompressor 反馈延迟和复杂的通道模型而受到强大的计算复杂度限制。此外，我们的方法不需要transition dynamics 和 observation dependency 的准确知识，这些知识在许多实际应用中通常不可获得。
</details></li>
</ul>
<hr>
<h2 id="How-to-Differentiate-between-Near-Field-and-Far-Field-Revisiting-the-Rayleigh-Distance"><a href="#How-to-Differentiate-between-Near-Field-and-Far-Field-Revisiting-the-Rayleigh-Distance" class="headerlink" title="How to Differentiate between Near Field and Far Field: Revisiting the Rayleigh Distance"></a>How to Differentiate between Near Field and Far Field: Revisiting the Rayleigh Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13238">http://arxiv.org/abs/2309.13238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shu Sun, Renwang Li, Xingchen Liu, Liuxun Xue, Chong Han, Meixia Tao</li>
<li>for: This paper aims to provide a comprehensive overview of the existing near field (NF) and far field (FF) boundaries in wireless communication systems, and to introduce a novel NF-FF demarcation method based on effective degrees of freedom (EDoF) of the channel.</li>
<li>methods: The proposed method uses EDoF to characterize the channel and demarcate the NF and FF regions. The authors analyze the main features of the EDoF-based NF-FF boundary and provide insights into wireless system design.</li>
<li>results: The authors demonstrate that the EDoF-based border is able to characterize key channel performance more accurately than the classic Rayleigh distance, and provide insights into wireless system design.Here is the result in Simplified Chinese text:</li>
<li>for: 这篇论文旨在提供无线通信系统中现有的近场（NF）和远场（FF）边界的总览，并提出一种基于有效度分度（EDoF）的信道边界分类方法。</li>
<li>methods: 该方法使用EDoF来特征化信道并分类NF和FF区域。作者分析了EDoF基于的NF-FF边界的主要特征，并提供无线系统设计的启示。</li>
<li>results: 作者表明，EDoF基于的边界能够更准确地特征化频率响应的关键性能特征，并提供无线系统设计的启示。<details>
<summary>Abstract</summary>
Future wireless communication systems are likely to adopt extremely large aperture arrays and millimeter-wave/sub-THz frequency bands to achieve higher throughput, lower latency, and higher energy efficiency. Conventional wireless systems predominantly operate in the far field (FF) of the radiation source of signals. As the array size increases and the carrier wavelength shrinks, however, the near field (NF) becomes non-negligible. Since the NF and FF differ in many aspects, it is essential to distinguish their corresponding regions. In this article, we first provide a comprehensive overview of the existing NF-FF boundaries, then introduce a novel NF-FF demarcation method based on effective degrees of freedom (EDoF) of the channel. Since EDoF is intimately related to spectral efficiency, the EDoF-based border is able to characterize key channel performance more accurately, as compared with the classic Rayleigh distance. Furthermore, we analyze the main features of the EDoF-based NF-FF boundary and provide insights into wireless system design.
</details>
<details>
<summary>摘要</summary>
未来无线通信系统可能会采用非常大的天线数组和毫米波/亿赫兹频段来实现更高的传输速率、更低的延迟时间和更高的能效率。传统无线系统主要在辐射源信号的远场（FF）中运行。然而，随着天线数组的增大和辐射波长的减小，近场（NF）变得不可或缺。由于NF和FF在多方面存在差异，因此需要明确NF-FF的分界线。在这篇文章中，我们首先提供了NF-FF分界线的全面回顾，然后介绍了一种基于效果度量（EDoF）的通道分界方法。由于EDoF与spectral efficiency之间存在紧密的关系，EDoF-based分界线能够更加准确地描述通道性能的关键特征，相比于 классический辐射距离。此外，我们还分析了NF-FF分界线的主要特征，并对无线系统设计提供了深入的理解。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/23/eess.SP_2023_09_23/" data-id="clpztdnuf01ewes885y0y8kga" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/22/cs.SD_2023_09_22/" class="article-date">
  <time datetime="2023-09-22T15:00:00.000Z" itemprop="datePublished">2023-09-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/22/cs.SD_2023_09_22/">cs.SD - 2023-09-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Massive-End-to-end-Models-for-Short-Search-Queries"><a href="#Massive-End-to-end-Models-for-Short-Search-Queries" class="headerlink" title="Massive End-to-end Models for Short Search Queries"></a>Massive End-to-end Models for Short Search Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12963">http://arxiv.org/abs/2309.12963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiran Wang, Rohit Prabhavalkar, Dongseong Hwang, Qiujia Li, Khe Chai Sim, Bo Li, James Qin, Xingyu Cai, Adam Stooke, Zhong Meng, CJ Zheng, Yanzhang He, Tara Sainath, Pedro Moreno Mengibar</li>
<li>for:  investigate two popular end-to-end automatic speech recognition (ASR) models, namely Connectionist Temporal Classification (CTC) and RNN-Transducer (RNN-T), for offline recognition of voice search queries.</li>
<li>methods:  use the neural architecture of Google’s universal speech model (USM), with additional funnel pooling layers to significantly reduce the frame rate and speed up training and inference.</li>
<li>results:  despite the speculation that larger CTC models can perform as well as RNN-T models, the authors observe that a 900M RNN-T model outperforms a 1.8B CTC model and is more tolerant to severe time reduction, although the WER gap can be largely removed by LM shallow fusion.<details>
<summary>Abstract</summary>
In this work, we investigate two popular end-to-end automatic speech recognition (ASR) models, namely Connectionist Temporal Classification (CTC) and RNN-Transducer (RNN-T), for offline recognition of voice search queries, with up to 2B model parameters. The encoders of our models use the neural architecture of Google's universal speech model (USM), with additional funnel pooling layers to significantly reduce the frame rate and speed up training and inference. We perform extensive studies on vocabulary size, time reduction strategy, and its generalization performance on long-form test sets. Despite the speculation that, as the model size increases, CTC can be as good as RNN-T which builds label dependency into the prediction, we observe that a 900M RNN-T clearly outperforms a 1.8B CTC and is more tolerant to severe time reduction, although the WER gap can be largely removed by LM shallow fusion.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究了两种流行的端到端自动语音识别（ASR）模型，即Connectionist Temporal Classification（CTC）和RNN-Transducer（RNN-T），用于离线识别voice搜索 queries，模型参数达2B。我们的模型encoder使用Google的通用语音模型（USM）的神经网络结构，并添加了挥发池化层以大幅降低帧率和加速训练和推理。我们进行了广泛的词汇大小、时间减少策略和长形测试集的总体性能研究。虽然有人推测，随着模型参数的增加，CTC可能与RNN-T相当，但我们发现一个900M RNN-T明显超过了1.8B CTC，并且更具耐用性。虽然WER差距可以通过LM浅合并大大减少，但CTC的性能仍然落后RNN-T。
</details></li>
</ul>
<hr>
<h2 id="VIC-KD-Variance-Invariance-Covariance-Knowledge-Distillation-to-Make-Keyword-Spotting-More-Robust-Against-Adversarial-Attacks"><a href="#VIC-KD-Variance-Invariance-Covariance-Knowledge-Distillation-to-Make-Keyword-Spotting-More-Robust-Against-Adversarial-Attacks" class="headerlink" title="VIC-KD: Variance-Invariance-Covariance Knowledge Distillation to Make Keyword Spotting More Robust Against Adversarial Attacks"></a>VIC-KD: Variance-Invariance-Covariance Knowledge Distillation to Make Keyword Spotting More Robust Against Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12914">http://arxiv.org/abs/2309.12914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heitor R. Guimarães, Arthur Pimentel, Anderson Avila, Tiago H. Falk</li>
<li>for: 这个论文的目的是提出一种robust distillation recipe，用于压缩模型并提高对抗攻击性能。</li>
<li>methods: 该论文使用了自动学习的speech representation，并在教师和学生模型中强制实施几何约束，以提高模型的Robustness和鲁棒性。</li>
<li>results: 实验结果显示，提出的方法可以提高对current state-of-the-art robust distillation方法（ARD和RSLAD）的robust准确率，分别提高12%和8%。<details>
<summary>Abstract</summary>
Keyword spotting (KWS) refers to the task of identifying a set of predefined words in audio streams. With the advances seen recently with deep neural networks, it has become a popular technology to activate and control small devices, such as voice assistants. Relying on such models for edge devices, however, can be challenging due to hardware constraints. Moreover, as adversarial attacks have increased against voice-based technologies, developing solutions robust to such attacks has become crucial. In this work, we propose VIC-KD, a robust distillation recipe for model compression and adversarial robustness. Using self-supervised speech representations, we show that imposing geometric priors to the latent representations of both Teacher and Student models leads to more robust target models. Experiments on the Google Speech Commands datasets show that the proposed methodology improves upon current state-of-the-art robust distillation methods, such as ARD and RSLAD, by 12% and 8% in robust accuracy, respectively.
</details>
<details>
<summary>摘要</summary>
键词检索（KWS）是指在语音流中确定一组预定义的词语的任务。随着深度神经网络的发展，KWS已成为许多小设备，如语音助手的激活和控制技术。但是，基于这些模型的边缘设备可能会受到硬件限制。此外，对语音技术的敌意攻击也在增加，因此开发对抗这些攻击的解决方案已经成为一项重要任务。在这种情况下，我们提出了VIC-KD，一种鲁棒的混合整合法。我们使用自我supervised的语音表示，并在教师和学生模型的秘密表示中加入几何约束，以便更加鲁棒的目标模型。在Google Speech Commands数据集上进行了实验，结果显示，我们的方法性比现有的State-of-the-art robust distillation方法，如ARD和RSLAD，提高了12%和8%的鲁棒精度，分别。
</details></li>
</ul>
<hr>
<h2 id="DurIAN-E-Duration-Informed-Attention-Network-For-Expressive-Text-to-Speech-Synthesis"><a href="#DurIAN-E-Duration-Informed-Attention-Network-For-Expressive-Text-to-Speech-Synthesis" class="headerlink" title="DurIAN-E: Duration Informed Attention Network For Expressive Text-to-Speech Synthesis"></a>DurIAN-E: Duration Informed Attention Network For Expressive Text-to-Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12792">http://arxiv.org/abs/2309.12792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Gu, Yianrao Bian, Guangzhi Lei, Chao Weng, Dan Su</li>
<li>for: 本研究提出了一种改进的时间知识注意力神经网络（DurIAN-E），用于实现高质量和高准确度的文本读音合成。</li>
<li>methods: 该模型采用了自适应的核心层结构，并使用多层核心层-based Transformer块作为语言编码器。此外，文本编码器还采用了样式适应的实例normalization（SAIN）层以提高表达能力。</li>
<li>results: 实验结果表明，提出的表达力强的TTS模型在对比前一个状态的方法的测试中表现出色，在主观意见分数（MOS）和偏好测试中都达到了更高的水平。<details>
<summary>Abstract</summary>
This paper introduces an improved duration informed attention neural network (DurIAN-E) for expressive and high-fidelity text-to-speech (TTS) synthesis. Inherited from the original DurIAN model, an auto-regressive model structure in which the alignments between the input linguistic information and the output acoustic features are inferred from a duration model is adopted. Meanwhile the proposed DurIAN-E utilizes multiple stacked SwishRNN-based Transformer blocks as linguistic encoders. Style-Adaptive Instance Normalization (SAIN) layers are exploited into frame-level encoders to improve the modeling ability of expressiveness. A denoiser incorporating both denoising diffusion probabilistic model (DDPM) for mel-spectrograms and SAIN modules is conducted to further improve the synthetic speech quality and expressiveness. Experimental results prove that the proposed expressive TTS model in this paper can achieve better performance than the state-of-the-art approaches in both subjective mean opinion score (MOS) and preference tests.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文介绍了一种改进的文本识别模型（DurIAN-E），它继承了原始DurIAN模型的自适应模型结构，并在语言编码器中使用多层折衔RNN-基于Transformer块。此外，提议中的DurIAN-E还利用了frame级别编码器中的样式适应实例归一化（SAIN）层，以提高模型的表达能力。此外，通过混合DDPM和SAIN模块，提高了生成的语音质量和表达性。实验结果表明，提议的表达力TTS模型在主观意见分数（MOS）和偏好测试中表现更好于当前状态的方法。
</details></li>
</ul>
<hr>
<h2 id="A-Study-on-Incorporating-Whisper-for-Robust-Speech-Assessment"><a href="#A-Study-on-Incorporating-Whisper-for-Robust-Speech-Assessment" class="headerlink" title="A Study on Incorporating Whisper for Robust Speech Assessment"></a>A Study on Incorporating Whisper for Robust Speech Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12766">http://arxiv.org/abs/2309.12766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryandhimas E. Zezario, Yu-Wen Chen, Szu-Wei Fu, Yu Tsao, Hsin-Min Wang, Chiou-Shann Fuh</li>
<li>for: 这个研究旨在提出一个基于多目标批判学习的声音评估模型，即MOSA-Net+，通过利用大规模预训练的弱监督模型Whisper的声学特征来创建嵌入特征。</li>
<li>methods: 这个研究的第一部分investigates the correlation between Whisper的嵌入特征和两种自动学习（SSL）模型的主观质量和语言可理解得分。第二部分评估了Whisper在实施更加稳定的声音评估模型方面的可用性。第三部分分析了将Whisper和SSL模型的表示结合在MOSA-Net+中的可能性。</li>
<li>results: 实验结果表明，Whisper的嵌入特征与主观质量和语言可理解得分更加强相关，从而提高MOSA-Net+的预测性能。此外，将Whisper和SSL模型的表示结合只会导致微妙的改善。相比MOSA-Net和其他基于SSL的声音评估模型，MOSA-Net+在评估主观质量和语言可理解得分上具有显著的改善。此外，MOSA-Net+在VoiceMOS挑战2023的Track 3上获得了总成绩的第一名。<details>
<summary>Abstract</summary>
This research introduces an enhanced version of the multi-objective speech assessment model, called MOSA-Net+, by leveraging the acoustic features from large pre-trained weakly supervised models, namely Whisper, to create embedding features. The first part of this study investigates the correlation between the embedding features of Whisper and two self-supervised learning (SSL) models with subjective quality and intelligibility scores. The second part evaluates the effectiveness of Whisper in deploying a more robust speech assessment model. Third, the possibility of combining representations from Whisper and SSL models while deploying MOSA-Net+ is analyzed. The experimental results reveal that Whisper's embedding features correlate more strongly with subjective quality and intelligibility than other SSL's embedding features, contributing to more accurate prediction performance achieved by MOSA-Net+. Moreover, combining the embedding features from Whisper and SSL models only leads to marginal improvement. As compared to MOSA-Net and other SSL-based speech assessment models, MOSA-Net+ yields notable improvements in estimating subjective quality and intelligibility scores across all evaluation metrics. We further tested MOSA-Net+ on Track 3 of the VoiceMOS Challenge 2023 and obtained the top-ranked performance.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这项研究引入了一个改进版的多目标语音评估模型，称为MOSA-Net+，通过利用大型预训练的弱监督模型Whisper的语音特征来生成嵌入特征。研究的第一部分 investigate了Whisper和两个自动学习（SSL）模型的嵌入特征与主观质量和听解能力分数之间的相关性。研究的第二部分评估了Whisper是否可以提供更加可靠的语音评估模型。第三部分分析了将Whisper和SSL模型的表示结合使用时MOSA-Net+的效果。实验结果表明，Whisper的嵌入特征与主观质量和听解能力分数相关性更高，对MOSA-Net+的预测性能产生了较大的贡献。此外，将Whisper和SSL模型的表示结合使用只导致了微妙的改进。相比MOSA-Net和其他基于SSL的语音评估模型，MOSA-Net+在所有评价指标上具有显著的改善。我们将MOSA-Net+测试在2023年语音评估挑战赛（VoiceMOS Challenge 2023）的Track 3上，并取得了排名第一的表现。
</details></li>
</ul>
<hr>
<h2 id="CrossSinger-A-Cross-Lingual-Multi-Singer-High-Fidelity-Singing-Voice-Synthesizer-Trained-on-Monolingual-Singers"><a href="#CrossSinger-A-Cross-Lingual-Multi-Singer-High-Fidelity-Singing-Voice-Synthesizer-Trained-on-Monolingual-Singers" class="headerlink" title="CrossSinger: A Cross-Lingual Multi-Singer High-Fidelity Singing Voice Synthesizer Trained on Monolingual Singers"></a>CrossSinger: A Cross-Lingual Multi-Singer High-Fidelity Singing Voice Synthesizer Trained on Monolingual Singers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12672">http://arxiv.org/abs/2309.12672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xintong Wang, Chang Zeng, Jun Chen, Chunhui Wang</li>
<li>for: 这个论文的目的是构建一个多人高准确的唱歌声音合成系统，并且能够在不同语言之间进行跨语言合成。</li>
<li>methods: 这个论文使用了Xiaoicesing2作为基础，并使用国际音声字母表来统一所有语言训练数据的表示。此外，文章还利用了condition层正规化来吸收语言信息，使模型在遇到未看过的语言时能够更好地发音。最后，文章还使用了梯度反转层（GRL）来消除歌手偏见，因为所有歌手都是单语言的，这意味着歌手的身份是隐式地与文本相关联的。</li>
<li>results: 实验结果表明，CrossSinger可以高准确地合成不同歌手的歌曲，并且能够在不同语言之间进行跨语言合成，包括code-switch情况。<details>
<summary>Abstract</summary>
It is challenging to build a multi-singer high-fidelity singing voice synthesis system with cross-lingual ability by only using monolingual singers in the training stage. In this paper, we propose CrossSinger, which is a cross-lingual singing voice synthesizer based on Xiaoicesing2. Specifically, we utilize International Phonetic Alphabet to unify the representation for all languages of the training data. Moreover, we leverage conditional layer normalization to incorporate the language information into the model for better pronunciation when singers meet unseen languages. Additionally, gradient reversal layer (GRL) is utilized to remove singer biases included in lyrics since all singers are monolingual, which indicates singer's identity is implicitly associated with the text. The experiment is conducted on a combination of three singing voice datasets containing Japanese Kiritan dataset, English NUS-48E dataset, and one internal Chinese dataset. The result shows CrossSinger can synthesize high-fidelity songs for various singers with cross-lingual ability, including code-switch cases.
</details>
<details>
<summary>摘要</summary>
“建立一个多人高精当唱歌声合成系统以采用单语言训练数据是挑战。本研究提出了 CrossSinger，它是一个基于 Xiaoicesing2 的跨语言唱歌声合成器。我们使用国际音标字母来统一所有语言训练数据的表示。此外，我们运用了条件层normalization来将语言信息 incorporated 到模型中，以更好地处理 singer 遇到未见的语言时的发音。此外，我们还使用了 Gradient Reversal Layer (GRL) 来移除 singer 的偏好，因为所有 singer 都是单语言，这意味着 singer 的识别是隐式地与文本相关。实验使用了三个唱歌声数据集，包括日本 Kiritan 数据集、英国 NUS-48E 数据集和一个内部的中文数据集。结果显示 CrossSinger 可以实现高精当度的唱歌声合成，包括 code-switch 情况。”
</details></li>
</ul>
<hr>
<h2 id="NTT-speaker-diarization-system-for-CHiME-7-multi-domain-multi-microphone-End-to-end-and-vector-clustering-diarization"><a href="#NTT-speaker-diarization-system-for-CHiME-7-multi-domain-multi-microphone-End-to-end-and-vector-clustering-diarization" class="headerlink" title="NTT speaker diarization system for CHiME-7: multi-domain, multi-microphone End-to-end and vector clustering diarization"></a>NTT speaker diarization system for CHiME-7: multi-domain, multi-microphone End-to-end and vector clustering diarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12656">http://arxiv.org/abs/2309.12656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naohiro Tawara, Marc Delcroix, Atsushi Ando, Atsunori Ogawa</li>
<li>for: 这篇论文描述了一种为多个频率域、多个麦克风的便衣对话进行的Speaker diarization系统。</li>
<li>methods: 该排序管道使用了weighted prediction error（WPE）为前端，然后使用end-to-end神经网络分类（EEND-VC）对每个通道进行分类。它将每个通道的排序结果结合在一起使用排序输出投票错误减少（DOVER-LAP）。</li>
<li>results: 该系统在NTT的CHiME-7挑战中的远程自动语音识别任务中被提交，并在开发集和评估集上分别提高了65%和62%的相对提升，相比于提供的VC-基础系统。<details>
<summary>Abstract</summary>
This paper details our speaker diarization system designed for multi-domain, multi-microphone casual conversations. The proposed diarization pipeline uses weighted prediction error (WPE)-based dereverberation as a front end, then applies end-to-end neural diarization with vector clustering (EEND-VC) to each channel separately. It integrates the diarization result obtained from each channel using diarization output voting error reduction plus overlap (DOVER-LAP). To harness the knowledge from the target domain and results integrated across all channels, we apply self-supervised adaptation for each session by retraining the EEND-VC with pseudo-labels derived from DOVER-LAP. The proposed system was incorporated into NTT's submission for the distant automatic speech recognition task in the CHiME-7 challenge. Our system achieved 65 % and 62 % relative improvements on development and eval sets compared to the organizer-provided VC-based baseline diarization system, securing third place in diarization performance.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文介绍了一个针对多个频道、多个麦克风的休闲对话中的 speaker diarization 系统。提posed系统使用了weighted prediction error（WPE）基于的前端，然后应用每个通道的端到端神经网络抽取（EEND-VC）。系统将每个通道的抽取结果集成使用抽取输出误差减少加上重叠（DOVER-LAP）。为了利用目标频道的知识和所有通道的结果的组合，系统使用了无监督适应（self-supervised adaptation），通过在DOVER-LAP中生成pseudo-labels来重新训练EEND-VC。提posed系统在CHiME-7 challenge中提交了，并在发展集和评估集上分别 achieved 65%和62%的相对提升，与组织者提供的基eline diarization系统相比，获得了第三名的表现。
</details></li>
</ul>
<hr>
<h2 id="SPGM-Prioritizing-Local-Features-for-enhanced-speech-separation-performance"><a href="#SPGM-Prioritizing-Local-Features-for-enhanced-speech-separation-performance" class="headerlink" title="SPGM: Prioritizing Local Features for enhanced speech separation performance"></a>SPGM: Prioritizing Local Features for enhanced speech separation performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12608">http://arxiv.org/abs/2309.12608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia Qi Yip, Shengkui Zhao, Yukun Ma, Chongjia Ni, Chong Zhang, Hao Wang, Trung Hieu Nguyen, Kun Zhou, Dianwen Ng, Eng Siong Chng, Bin Ma</li>
<li>for: 提高Speech separation模型（如Sepformer）的性能，减少参数数量。</li>
<li>methods: 使用Single-Path Global Modulation（SPGM）块取代inter-blocks，SPGM块具有无参数全球 Pooling模块和Modulation模块，共计2%的模型参数。</li>
<li>results: SPGM在WSJ0-2Mix和Libri2Mix上达到22.1 dB SI-SDRi和20.4 dB SI-SDRi，超过Sepformer的性能，并与最新的SOTA模型几乎匹配，但具有8倍少的参数数量。<details>
<summary>Abstract</summary>
Dual-path is a popular architecture for speech separation models (e.g. Sepformer) which splits long sequences into overlapping chunks for its intra- and inter-blocks that separately model intra-chunk local features and inter-chunk global relationships. However, it has been found that inter-blocks, which comprise half a dual-path model's parameters, contribute minimally to performance. Thus, we propose the Single-Path Global Modulation (SPGM) block to replace inter-blocks. SPGM is named after its structure consisting of a parameter-free global pooling module followed by a modulation module comprising only 2% of the model's total parameters. The SPGM block allows all transformer layers in the model to be dedicated to local feature modelling, making the overall model single-path. SPGM achieves 22.1 dB SI-SDRi on WSJ0-2Mix and 20.4 dB SI-SDRi on Libri2Mix, exceeding the performance of Sepformer by 0.5 dB and 0.3 dB respectively and matches the performance of recent SOTA models with up to 8 times fewer parameters.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>双路是一种流行的语音分离模型（例如 Sepformer）的架构，将长序列分割成 overlap 的块，以便在块内和块间分别模型 intra-chunk 的本地特征和 inter-chunk 的全局关系。然而，在这种情况下，inter-blocks 占用了模型的一半参数，但是它们却对性能的贡献很小。因此，我们提出了单路全球修饰（SPGM）块来取代 inter-blocks。SPGM 得名于它的结构，包括一个无参数的全球汇集模块和一个修饰模块，该模块仅占用了模型总参数的 2%。SPGM 块使得所有转换层在模型中都专注于本地特征修饰，从而使整个模型变成单路。SPGM 在 WSJ0-2Mix 和 Libri2Mix 上 достиieves 22.1 dB SI-SDRi 和 20.4 dB SI-SDRi，超过 Sepformer 的性能 by 0.5 dB 和 0.3 dB，并与最新的 SOTA 模型几乎相当。
</details></li>
</ul>
<hr>
<h2 id="ICASSP-2023-Acoustic-Echo-Cancellation-Challenge"><a href="#ICASSP-2023-Acoustic-Echo-Cancellation-Challenge" class="headerlink" title="ICASSP 2023 Acoustic Echo Cancellation Challenge"></a>ICASSP 2023 Acoustic Echo Cancellation Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12553">http://arxiv.org/abs/2309.12553</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/AEC-Challenge">https://github.com/microsoft/AEC-Challenge</a></li>
<li>paper_authors: Ross Cutler, Ando Saabas, Tanel Parnamaa, Marju Purin, Evgenii Indenbom, Nicolae-Catalin Ristea, Jegor Gužvin, Hannes Gamper, Sebastian Braun, Robert Aichner</li>
<li>For: 这个挑战的目的是促进静音干扰（AEC）研究，提高语音干扰和音频通信中的声音质量。* Methods: 挑战使用了两个追踪器，包括一个基于人工智能的追踪器和一个基于模型的追踪器，以及一个全带宽AECMOS。* Results: 挑战开源了两个大规模的训练数据集，包括来自更多于10,000个真实的音频设备和人类说话者的实际环境记录，以及一个 sintetic 数据集。winning 的result是基于所有场景的平均意见度（MOS）和单词准确率（WAcc）。<details>
<summary>Abstract</summary>
The ICASSP 2023 Acoustic Echo Cancellation Challenge is intended to stimulate research in acoustic echo cancellation (AEC), which is an important area of speech enhancement and is still a top issue in audio communication. This is the fourth AEC challenge and it is enhanced by adding a second track for personalized acoustic echo cancellation, reducing the algorithmic + buffering latency to 20ms, as well as including a full-band version of AECMOS. We open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 10,000 real audio devices and human speakers in real environments, as well as a synthetic dataset. We open source an online subjective test framework and provide an objective metric for researchers to quickly test their results. The winners of this challenge were selected based on the average mean opinion score (MOS) achieved across all scenarios and the word accuracy (WAcc) rate.
</details>
<details>
<summary>摘要</summary>
ICASSP 2023 听音障碍挑战是要促进听音障碍（AEC）领域的研究，这是一个重要的声音提升领域，仍然是音频通信中的主要问题。这是第四个AEC挑战，它的改进包括添加个性化听音障碍追踪，降低算法+缓冲延迟至20毫秒，以及包括全带AECMOS。我们对AEC模型进行训练提供了两个大型数据集，包括单个说话和双个说话场景。这些数据集包括来自 более чем10,000个真实的音频设备和人类说话者在真实环境中的录音，以及一个 sintetic 数据集。我们提供了在线主观测试框架，并提供了一个对研究人员快速测试结果的 объек metric。挑战赛中的赢家是根据所有场景的平均主观评分（MOS）和单词准确率（WAcc）而选择的。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/22/cs.SD_2023_09_22/" data-id="clpztdno300zues882s515lu2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/22/cs.CV_2023_09_22/" class="article-date">
  <time datetime="2023-09-22T13:00:00.000Z" itemprop="datePublished">2023-09-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/22/cs.CV_2023_09_22/">cs.CV - 2023-09-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ClusterFormer-Clustering-As-A-Universal-Visual-Learner"><a href="#ClusterFormer-Clustering-As-A-Universal-Visual-Learner" class="headerlink" title="ClusterFormer: Clustering As A Universal Visual Learner"></a>ClusterFormer: Clustering As A Universal Visual Learner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13196">http://arxiv.org/abs/2309.13196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clusterformer/clusterformer">https://github.com/clusterformer/clusterformer</a></li>
<li>paper_authors: James C. Liang, Yiming Cui, Qifan Wang, Tong Geng, Wenguan Wang, Dongfang Liu</li>
<li>for: 这个研究旨在提出一个基于CLUSTERing的概念的普遍性视觉模型，即CLUSTERFORMER，并将其应用于多种视觉任务中，包括图像分类、物体检测和图像分割等。</li>
<li>methods: 这个模型使用了两个新的设计：1. 回归十字运算实现了Transformer中的十字运算机制，并允许逐层更新团中心以便强化表示学习; 2. 图像特征重新分配使用更新的团中心，通过相似度基准来重新分配图像特征，实现了透明的管道。</li>
<li>results: 实验结果显示CLUSTERFORMER可以超过多种知名的特化架构，包括图像分类、物体检测和图像分割等任务，并在不同的团粒度（即图像、方巢和像素粒度）下实现高效性。<details>
<summary>Abstract</summary>
This paper presents CLUSTERFORMER, a universal vision model that is based on the CLUSTERing paradigm with TransFORMER. It comprises two novel designs: 1. recurrent cross-attention clustering, which reformulates the cross-attention mechanism in Transformer and enables recursive updates of cluster centers to facilitate strong representation learning; and 2. feature dispatching, which uses the updated cluster centers to redistribute image features through similarity-based metrics, resulting in a transparent pipeline. This elegant design streamlines an explainable and transferable workflow, capable of tackling heterogeneous vision tasks (i.e., image classification, object detection, and image segmentation) with varying levels of clustering granularity (i.e., image-, box-, and pixel-level). Empirical results demonstrate that CLUSTERFORMER outperforms various well-known specialized architectures, achieving 83.41% top-1 acc. over ImageNet-1K for image classification, 54.2% and 47.0% mAP over MS COCO for object detection and instance segmentation, 52.4% mIoU over ADE20K for semantic segmentation, and 55.8% PQ over COCO Panoptic for panoptic segmentation. For its efficacy, we hope our work can catalyze a paradigm shift in universal models in computer vision.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Recurrent cross-attention clustering: This reformulates the cross-attention mechanism in Transformer to enable recursive updates of cluster centers, facilitating strong representation learning.2. Feature dispatching: This uses updated cluster centers to redistribute image features through similarity-based metrics, resulting in a transparent pipeline.This elegant design enables a streamlined, explainable, and transferable workflow for tackling heterogeneous vision tasks (image classification, object detection, and image segmentation) with varying levels of clustering granularity (image-, box-, and pixel-level). Empirical results show that CLUSTERFORMER outperforms various well-known specialized architectures, achieving:* 83.41% top-1 accuracy over ImageNet-1K for image classification* 54.2% and 47.0% mAP over MS COCO for object detection and instance segmentation* 52.4% mIoU over ADE20K for semantic segmentation* 55.8% PQ over COCO Panoptic for panoptic segmentation.We hope that our work will catalyze a paradigm shift in universal models in computer vision, demonstrating the efficacy of the CLUSTERing paradigm in achieving strong representation learning and transferability across diverse vision tasks.</details></li>
</ol>
<hr>
<h2 id="Spatial-frequency-channels-shape-bias-and-adversarial-robustness"><a href="#Spatial-frequency-channels-shape-bias-and-adversarial-robustness" class="headerlink" title="Spatial-frequency channels, shape bias, and adversarial robustness"></a>Spatial-frequency channels, shape bias, and adversarial robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13190">http://arxiv.org/abs/2309.13190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ajaysub110/critical-band-masking">https://github.com/ajaysub110/critical-band-masking</a></li>
<li>paper_authors: Ajay Subramanian, Elena Sizikova, Najib J. Majaj, Denis G. Pelli<br>for:这种研究旨在探索人类和神经网络在认知物体方面使用的频谱信息是什么。methods:研究人员使用了 crítical band masking 技术，该技术可以揭示人类和神经网络在认知物体过程中使用的频谱滤波器（或“渠道”）的宽度。results:研究发现，人类在自然图像中认知物体时使用的频谱滤波器与人类在字体和梯形图像中认知时使用的频谱滤波器一致，宽度都是一个 octave。然而，神经网络渠道在不同的架构和训练策略下表现为 2-4 倍于人类渠道宽度，这意味着神经网络对高频和低频噪声敏感，而人类不是。 adversarial 和扩展图像训练通常用于提高网络的Robustness和形态偏好。这种训练是否将网络和人类的物体认知渠道进行对接？<details>
<summary>Abstract</summary>
What spatial frequency information do humans and neural networks use to recognize objects? In neuroscience, critical band masking is an established tool that can reveal the frequency-selective filters used for object recognition. Critical band masking measures the sensitivity of recognition performance to noise added at each spatial frequency. Existing critical band masking studies show that humans recognize periodic patterns (gratings) and letters by means of a spatial-frequency filter (or "channel'') that has a frequency bandwidth of one octave (doubling of frequency). Here, we introduce critical band masking as a task for network-human comparison and test 14 humans and 76 neural networks on 16-way ImageNet categorization in the presence of narrowband noise. We find that humans recognize objects in natural images using the same one-octave-wide channel that they use for letters and gratings, making it a canonical feature of human object recognition. On the other hand, the neural network channel, across various architectures and training strategies, is 2-4 times as wide as the human channel. In other words, networks are vulnerable to high and low frequency noise that does not affect human performance. Adversarial and augmented-image training are commonly used to increase network robustness and shape bias. Does this training align network and human object recognition channels? Three network channel properties (bandwidth, center frequency, peak noise sensitivity) correlate strongly with shape bias (53% variance explained) and with robustness of adversarially-trained networks (74% variance explained). Adversarial training increases robustness but expands the channel bandwidth even further away from the human bandwidth. Thus, critical band masking reveals that the network channel is more than twice as wide as the human channel, and that adversarial training only increases this difference.
</details>
<details>
<summary>摘要</summary>
人类和神经网络在认知物体时使用哪些空间频率信息？在神经科学中，关键带掩蔽是一种已知的工具，可以揭示人类和神经网络在认知物体时使用的频率选择性滤波器。关键带掩蔽测量人类和神经网络在噪声添加后的认知性能的敏感度。现有的关键带掩蔽研究表明，人类认知 periodic patterns（格拉丁）和字母使用一个频率带宽（ doubles 频率）的空间频率滤波器（或“渠道”）来认知物体。我们在人类和神经网络之间进行关键带掩蔽任务，并测试了14名人类和76个神经网络在16种 ImageNet 分类任务中的表现。我们发现，人类在自然图像中认知物体使用了同样的一个频率带宽的渠道，这是人类物体认知的启示性特征。然而，神经网络渠道，不同架构和训练策略，宽度为2-4倍于人类渠道。即神经网络具有高频和低频噪声不affects human performance的敏感性。常见的图像增强和抗击训练被用来提高网络的Robustness和形态偏好。这种训练是否与人类物体认知渠道相align？神经网络渠道的三个特性（带宽、中心频率、峰噪敏感度）与形态偏好（53% 额外变化）以及对抗训练后网络的Robustness（74% 额外变化）存在强相关性。抗击训练可以提高网络的Robustness，但是同时也使得网络渠道的宽度更加远离人类渠道。因此，关键带掩蔽表明，神经网络渠道比人类渠道更加宽，并且抗击训练只会进一步扩大这个差距。
</details></li>
</ul>
<hr>
<h2 id="Flow-Factorized-Representation-Learning"><a href="#Flow-Factorized-Representation-Learning" class="headerlink" title="Flow Factorized Representation Learning"></a>Flow Factorized Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13167">http://arxiv.org/abs/2309.13167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kingjamessong/latent-flow">https://github.com/kingjamessong/latent-flow</a></li>
<li>paper_authors: Yue Song, T. Anderson Keller, Nicu Sebe, Max Welling</li>
<li>for: 本研究的主要目标是学习表示，以达到对真实因素的分解。</li>
<li>methods: 我们提出了一种新的视角，即流动因素化表示学习（Flow Factorized Representation Learning），并在这种结构下学习更有效和更有用的表示。</li>
<li>results: 我们的模型在标准表示学习 bencmarks 上达到更高的likelihood，同时也更接近于相对平衡模型。此外，我们还证明了我们的变换是可以composite和适用于新数据，这表明我们的表示学习模型具有一定的抗预测和普适性。<details>
<summary>Abstract</summary>
A prominent goal of representation learning research is to achieve representations which are factorized in a useful manner with respect to the ground truth factors of variation. The fields of disentangled and equivariant representation learning have approached this ideal from a range of complimentary perspectives; however, to date, most approaches have proven to either be ill-specified or insufficiently flexible to effectively separate all realistic factors of interest in a learned latent space. In this work, we propose an alternative viewpoint on such structured representation learning which we call Flow Factorized Representation Learning, and demonstrate it to learn both more efficient and more usefully structured representations than existing frameworks. Specifically, we introduce a generative model which specifies a distinct set of latent probability paths that define different input transformations. Each latent flow is generated by the gradient field of a learned potential following dynamic optimal transport. Our novel setup brings new understandings to both \textit{disentanglement} and \textit{equivariance}. We show that our model achieves higher likelihoods on standard representation learning benchmarks while simultaneously being closer to approximately equivariant models. Furthermore, we demonstrate that the transformations learned by our model are flexibly composable and can also extrapolate to new data, implying a degree of robustness and generalizability approaching the ultimate goal of usefully factorized representation learning.
</details>
<details>
<summary>摘要</summary>
prominent goal of representation learning research 是 achiev representations 是 factorized in a useful manner with respect to the ground truth factors of variation. disentangled and equivariant representation learning  approached this ideal from a range of complimentary perspectives; however, to date, most approaches have proven to either be ill-specified or insufficiently flexible to effectively separate all realistic factors of interest in a learned latent space. In this work, we propose an alternative viewpoint on such structured representation learning, which we call Flow Factorized Representation Learning, and demonstrate it to learn both more efficient and more usefully structured representations than existing frameworks. Specifically, we introduce a generative model that specifies a distinct set of latent probability paths that define different input transformations. Each latent flow is generated by the gradient field of a learned potential following dynamic optimal transport. Our novel setup brings new understandings to both disentanglement and equivariance. We show that our model achieves higher likelihoods on standard representation learning benchmarks while simultaneously being closer to approximately equivariant models. Furthermore, we demonstrate that the transformations learned by our model are flexibly composable and can also extrapolate to new data, implying a degree of robustness and generalizability approaching the ultimate goal of usefully factorized representation learning.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries. The translation is based on the standard Chinese characters and grammar, and may be slightly different from the traditional Chinese used in Hong Kong and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Pixel-wise-Smoothing-for-Certified-Robustness-against-Camera-Motion-Perturbations"><a href="#Pixel-wise-Smoothing-for-Certified-Robustness-against-Camera-Motion-Perturbations" class="headerlink" title="Pixel-wise Smoothing for Certified Robustness against Camera Motion Perturbations"></a>Pixel-wise Smoothing for Certified Robustness against Camera Motion Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13150">http://arxiv.org/abs/2309.13150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanjiang Hu, Zuxin Liu, Linyi Li, Jiacheng Zhu, Ding Zhao<br>for:* 这种方法用于证明深度学习视觉模型对摄像头运动干扰的Robustness。methods:* 该方法使用了一种新的、高效的和实用的框架，利用了像素空间的平滑分布，从而消除了贵重的摄像头运动采样成本，提高了证明Robustness的效率。results:* 通过广泛的实验证明，该方法可以很好地平衡证明效果和计算效率。例如，该方法可以在使用只有30%的投影图像框架的情况下实现约80%的证明准确率。<details>
<summary>Abstract</summary>
In recent years, computer vision has made remarkable advancements in autonomous driving and robotics. However, it has been observed that deep learning-based visual perception models lack robustness when faced with camera motion perturbations. The current certification process for assessing robustness is costly and time-consuming due to the extensive number of image projections required for Monte Carlo sampling in the 3D camera motion space. To address these challenges, we present a novel, efficient, and practical framework for certifying the robustness of 3D-2D projective transformations against camera motion perturbations. Our approach leverages a smoothing distribution over the 2D pixel space instead of in the 3D physical space, eliminating the need for costly camera motion sampling and significantly enhancing the efficiency of robustness certifications. With the pixel-wise smoothed classifier, we are able to fully upper bound the projection errors using a technique of uniform partitioning in camera motion space. Additionally, we extend our certification framework to a more general scenario where only a single-frame point cloud is required in the projection oracle. This is achieved by deriving Lipschitz-based approximated partition intervals. Through extensive experimentation, we validate the trade-off between effectiveness and efficiency enabled by our proposed method. Remarkably, our approach achieves approximately 80% certified accuracy while utilizing only 30% of the projected image frames.
</details>
<details>
<summary>摘要</summary>
现在的计算机视觉技术在自动驾驶和机器人控制方面已经取得了非常出色的进步。然而，已经观察到深度学习基于视觉模型对摄像头运动干扰的Robustness有所不足。现有的证明过程对摄像头运动干扰的Robustness进行评估是非常昂贵和时间consuming的，因为需要进行大量的图像投影以实现Monte Carlo抽象在3D摄像头运动空间中。为解决这些挑战，我们提出了一种新的、高效、实用的框架，用于证明3D-2D投影变换对摄像头运动干扰的Robustness。我们的方法利用2D像素空间中的平滑分布而不是3D物理空间中的平滑分布，从而消除了高昂的摄像头运动样本成本和大量的图像投影。通过使用像素空间平滑分布，我们可以完全上界投影错误，使用一种基于均匀分区的技术来实现Camera motion空间中的均匀分区。此外，我们将证明框架扩展到一个更加通用的场景，只需要提供单帧点云作为投影oracle。我们通过 derivation Lipschitz-basedapproximated partition intervals来实现这一点。通过广泛的实验，我们证明了我们的提出的方法的效率和可靠性之间的trade-off。特别是，我们的方法可以在30%的图像投影帧上达到约80%的证明精度。
</details></li>
</ul>
<hr>
<h2 id="Trading-off-Mutual-Information-on-Feature-Aggregation-for-Face-Recognition"><a href="#Trading-off-Mutual-Information-on-Feature-Aggregation-for-Face-Recognition" class="headerlink" title="Trading-off Mutual Information on Feature Aggregation for Face Recognition"></a>Trading-off Mutual Information on Feature Aggregation for Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13137">http://arxiv.org/abs/2309.13137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Akyash, Ali Zafari, Nasser M. Nasrabadi<br>for: 提高人脸识别精度methods:  aggregate ArcFace和AdaFace两个state-of-the-art深度人脸识别模型的输出，通过利用trasnformer注意机制来把扩展两个特征地图之间的关系，从而提高人脸识别系统的总体识别能力。results: 通过对比多个标准 bencmark 结果，我们观察到了我们的方法在人脸识别 tasks 中的一致性提高。<details>
<summary>Abstract</summary>
Despite the advances in the field of Face Recognition (FR), the precision of these methods is not yet sufficient. To improve the FR performance, this paper proposes a technique to aggregate the outputs of two state-of-the-art (SOTA) deep FR models, namely ArcFace and AdaFace. In our approach, we leverage the transformer attention mechanism to exploit the relationship between different parts of two feature maps. By doing so, we aim to enhance the overall discriminative power of the FR system. One of the challenges in feature aggregation is the effective modeling of both local and global dependencies. Conventional transformers are known for their ability to capture long-range dependencies, but they often struggle with modeling local dependencies accurately. To address this limitation, we augment the self-attention mechanism to capture both local and global dependencies effectively. This allows our model to take advantage of the overlapping receptive fields present in corresponding locations of the feature maps. However, fusing two feature maps from different FR models might introduce redundancies to the face embedding. Since these models often share identical backbone architectures, the resulting feature maps may contain overlapping information, which can mislead the training process. To overcome this problem, we leverage the principle of Information Bottleneck to obtain a maximally informative facial representation. This ensures that the aggregated features retain the most relevant and discriminative information while minimizing redundant or misleading details. To evaluate the effectiveness of our proposed method, we conducted experiments on popular benchmarks and compared our results with state-of-the-art algorithms. The consistent improvement we observed in these benchmarks demonstrates the efficacy of our approach in enhancing FR performance.
</details>
<details>
<summary>摘要</summary>
尽管面Recognition（FR）领域已经取得了一些进步，但FR方法的精度仍然不够高。为了提高FR性能，这篇论文提议了一种将两种现有的深度FR模型，即ArcFace和AdaFace，的输出聚合的技术。在我们的方法中，我们利用了变换器注意机制，以利用两个特征图的不同部分之间的关系。这样做的目的是提高总的识别力。一个挑战在特征聚合中是有效地模型本地和全局依赖关系。传统的变换器通常能够很好地捕捉长距离依赖关系，但它们经常在本地依赖关系上做出不准确的预测。为了解决这个限制，我们在自我注意机制中进行了修改，以同时 capture本地和全局依赖关系。这使得我们的模型能够利用特征图中相互重叠的区域的拥有的相互关系。然而，将两个特征图从不同的FR模型融合可能会导致人脸嵌入中的纬度冗余。这是因为这些模型通常具有相同的背部架构，导致生成的特征图可能包含重复的信息。为了解决这个问题，我们利用信息瓶颈原理，从人脸嵌入中提取最大可能的信息，以确保聚合的特征保留了最有用和权威的信息，同时减少不必要或误导的细节。为了评估我们的提议的效果，我们在popular benchmark上进行了实验，并与当前的算法进行比较。我们在这些benchmark中经常观察到了一致性提高，这表明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Calibration-of-Deep-Neural-Networks-for-Medical-Image-Classification"><a href="#Understanding-Calibration-of-Deep-Neural-Networks-for-Medical-Image-Classification" class="headerlink" title="Understanding Calibration of Deep Neural Networks for Medical Image Classification"></a>Understanding Calibration of Deep Neural Networks for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13132">http://arxiv.org/abs/2309.13132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Singh Sambyal, Usma Niyaz, Narayanan C. Krishnan, Deepti R. Bathula</li>
<li>for: 这篇论文旨在探讨医疗影像分析中，使用深度神经网络时，确保模型的准确性和可靠性是非常重要的。</li>
<li>methods: 这篇论文使用了多种训练方法，包括全supervised training和旋转自给supervised learning，以了解不同训练方法对模型准确性和可靠性的影响。</li>
<li>results: 研究发现，使用旋转自给supervised learning的训练方法可以将模型的准确性和可靠性提高，并且可以实现比全supervised training更好的准确性和可靠性。<details>
<summary>Abstract</summary>
In the field of medical image analysis, achieving high accuracy is not enough; ensuring well-calibrated predictions is also crucial. Confidence scores of a deep neural network play a pivotal role in explainability by providing insights into the model's certainty, identifying cases that require attention, and establishing trust in its predictions. Consequently, the significance of a well-calibrated model becomes paramount in the medical imaging domain, where accurate and reliable predictions are of utmost importance. While there has been a significant effort towards training modern deep neural networks to achieve high accuracy on medical imaging tasks, model calibration and factors that affect it remain under-explored. To address this, we conducted a comprehensive empirical study that explores model performance and calibration under different training regimes. We considered fully supervised training, which is the prevailing approach in the community, as well as rotation-based self-supervised method with and without transfer learning, across various datasets and architecture sizes. Multiple calibration metrics were employed to gain a holistic understanding of model calibration. Our study reveals that factors such as weight distributions and the similarity of learned representations correlate with the calibration trends observed in the models. Notably, models trained using rotation-based self-supervised pretrained regime exhibit significantly better calibration while achieving comparable or even superior performance compared to fully supervised models across different medical imaging datasets. These findings shed light on the importance of model calibration in medical image analysis and highlight the benefits of incorporating self-supervised learning approach to improve both performance and calibration.
</details>
<details>
<summary>摘要</summary>
在医疗影像分析领域，即使达到高精度也不够；保证准确的预测也非常重要。深度神经网络的自信分数在解释性方面发挥关键作用，为模型的certainty提供了信息， помо助分析出需要注意的案例，并建立对预测的信任。因此，在医疗影像领域，准确可靠的预测是非常重要的。虽然社区内有很大的努力，以使现代深度神经网络在医疗影像任务上达到高精度，但模型准确性和可靠性的调整仍然受到了少数研究。为了解决这个问题，我们进行了全面的实验研究，探讨了不同的训练方法对模型性能和准确性的影响。我们考虑了完全监督学习，这是社区中最常用的方法，以及旋转基于自动学习的方法，包括无扩展和带扩展的方法，在不同的数据集和模型大小上进行了测试。我们使用多种准确度指标来了解模型准确性的多方面特性。我们的研究发现，模型的weight分布和学习的表示相似性与模型准确性的趋势相关。特别是，通过旋转基于自动学习的预训练方法进行训练的模型在不同的医疗影像数据集上显示出了显著更好的准确性，而且与完全监督学习模型相比，它们在不同的模型大小上实现了相似或更高的性能。这些发现 shed light on the importance of model calibration in medical image analysis, and highlight the benefits of incorporating self-supervised learning approaches to improve both performance and calibration.
</details></li>
</ul>
<hr>
<h2 id="Robotic-Offline-RL-from-Internet-Videos-via-Value-Function-Pre-Training"><a href="#Robotic-Offline-RL-from-Internet-Videos-via-Value-Function-Pre-Training" class="headerlink" title="Robotic Offline RL from Internet Videos via Value-Function Pre-Training"></a>Robotic Offline RL from Internet Videos via Value-Function Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13041">http://arxiv.org/abs/2309.13041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chethan Bhateja, Derek Guo, Dibya Ghosh, Anikait Singh, Manan Tomar, Quan Vuong, Yevgen Chebotar, Sergey Levine, Aviral Kumar</li>
<li>for: 这个论文是为了帮助机器人学习学习掌控技能，尤其是在没有奖励信号的情况下。</li>
<li>methods: 这个论文使用了视频数据来适应机器人的学习，通过时间差学习来学习值函数，并将其应用于机器人掌控任务中。</li>
<li>results: 这个论文在多个机器人掌控任务上取得了良好的结果，其中包括在一个真实的WidowX机器人上进行的多个掌控任务。政策比之前的方法更好，更加稳定，并能够广泛应用。<details>
<summary>Abstract</summary>
Pre-training on Internet data has proven to be a key ingredient for broad generalization in many modern ML systems. What would it take to enable such capabilities in robotic reinforcement learning (RL)? Offline RL methods, which learn from datasets of robot experience, offer one way to leverage prior data into the robotic learning pipeline. However, these methods have a "type mismatch" with video data (such as Ego4D), the largest prior datasets available for robotics, since video offers observation-only experience without the action or reward annotations needed for RL methods. In this paper, we develop a system for leveraging large-scale human video datasets in robotic offline RL, based entirely on learning value functions via temporal-difference learning. We show that value learning on video datasets learns representations that are more conducive to downstream robotic offline RL than other approaches for learning from video data. Our system, called V-PTR, combines the benefits of pre-training on video data with robotic offline RL approaches that train on diverse robot data, resulting in value functions and policies for manipulation tasks that perform better, act robustly, and generalize broadly. On several manipulation tasks on a real WidowX robot, our framework produces policies that greatly improve over prior methods. Our video and additional details can be found at https://dibyaghosh.com/vptr/
</details>
<details>
<summary>摘要</summary>
在现代机器学习系统中，预训练在互联网数据上有证明是一种关键因素，以实现广泛的通用化。在机器人学习上，可以通过在机器人经验数据集上进行预训练来实现这种能力。然而，这些方法与视频数据（如Ego4D）存在类型匹配问题，因为视频只提供了观察经验，而不提供动作或奖励注释，这些注释是机器人学习方法所需的。在这篇论文中，我们开发了一种将大规模人类视频数据集成入机器人预训练的系统，基于完全通过时间差学习学习值函数。我们表明，在视频数据集上学习值函数可以学习更适合下游机器人预训练的表示，比其他视频数据学习方法更好。我们的系统，即V-PTR，将预训练在视频数据集上的好处与多种机器人数据预训练相结合，以生成更好的 manipulate 任务的价值函数和策略。在一个真实的 WidowX 机器人上，我们的框架可以大幅提高先前方法的政策。我们的视频和其他细节可以在 <https://dibyaghosh.com/vptr/> 找到。
</details></li>
</ul>
<hr>
<h2 id="NeRRF-3D-Reconstruction-and-View-Synthesis-for-Transparent-and-Specular-Objects-with-Neural-Refractive-Reflective-Fields"><a href="#NeRRF-3D-Reconstruction-and-View-Synthesis-for-Transparent-and-Specular-Objects-with-Neural-Refractive-Reflective-Fields" class="headerlink" title="NeRRF: 3D Reconstruction and View Synthesis for Transparent and Specular Objects with Neural Refractive-Reflective Fields"></a>NeRRF: 3D Reconstruction and View Synthesis for Transparent and Specular Objects with Neural Refractive-Reflective Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13039">http://arxiv.org/abs/2309.13039</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dawning77/nerrf">https://github.com/dawning77/nerrf</a></li>
<li>paper_authors: Xiaoxue Chen, Junchen Liu, Hao Zhao, Guyue Zhou, Ya-Qin Zhang</li>
<li>for: 这篇论文是关于图像基于视图合成的研究，旨在解决NeRF无法处理复杂的光路变化问题，导致无法成功合成透明或镜面物体的问题。</li>
<li>methods: 作者们提出了吸收射镜场（Refractive-Reflective Field，RRF），通过使用进攻四面体和进攻编码来重建非LAMBERTIAN对象的几何结构，并使用费勒涅尔定律来模型物体的折射和反射效果。同时，为了实现高效和有效的抑杂，提出了虚拟圆锥超抽样技术。</li>
<li>results: 作者们在不同的形状、背景和费勒涅尔定律上进行了多种实验，并对不同的编辑应用进行了质量和量化的比较，包括材质编辑、物体替换&#x2F;插入和环境照明估计。<details>
<summary>Abstract</summary>
Neural radiance fields (NeRF) have revolutionized the field of image-based view synthesis. However, NeRF uses straight rays and fails to deal with complicated light path changes caused by refraction and reflection. This prevents NeRF from successfully synthesizing transparent or specular objects, which are ubiquitous in real-world robotics and A/VR applications. In this paper, we introduce the refractive-reflective field. Taking the object silhouette as input, we first utilize marching tetrahedra with a progressive encoding to reconstruct the geometry of non-Lambertian objects and then model refraction and reflection effects of the object in a unified framework using Fresnel terms. Meanwhile, to achieve efficient and effective anti-aliasing, we propose a virtual cone supersampling technique. We benchmark our method on different shapes, backgrounds and Fresnel terms on both real-world and synthetic datasets. We also qualitatively and quantitatively benchmark the rendering results of various editing applications, including material editing, object replacement/insertion, and environment illumination estimation. Codes and data are publicly available at https://github.com/dawning77/NeRRF.
</details>
<details>
<summary>摘要</summary>
“对象基于图像的视 synthesis 领域受到对应� Neural Radiance Fields（NeRF）的革命性影响。然而，NeRF 使用直线光束，无法处理由折射和反射导致的复杂光束变化，这限制了 NeRF 在透明或 Specular 物体的成功实现。在这篇论文中，我们介绍了 Refractive-Reflective Field（RRF）。我们将物体照片为输入，首先使用进攻四边形（Marching Tetrahedra）进行非 Lambertian 物体的重建，然后在一个统一框架中模型物体的折射和反射效应，使用 Fresnel 表达。此外，为了获得高效和有效的抑挡遮瑕，我们提出了虚拟圆锥超推数技术。我们在不同的形状、背景和 Fresnel 表达下进行了不同的测试，并评估了不同的编辑应用，包括材料编辑、物体取代/插入和环境照明估计。我们的代码和数据公开在 GitHub 上，请参考 https://github.com/dawning77/NeRRF。”
</details></li>
</ul>
<hr>
<h2 id="Privacy-Assessment-on-Reconstructed-Images-Are-Existing-Evaluation-Metrics-Faithful-to-Human-Perception"><a href="#Privacy-Assessment-on-Reconstructed-Images-Are-Existing-Evaluation-Metrics-Faithful-to-Human-Perception" class="headerlink" title="Privacy Assessment on Reconstructed Images: Are Existing Evaluation Metrics Faithful to Human Perception?"></a>Privacy Assessment on Reconstructed Images: Are Existing Evaluation Metrics Faithful to Human Perception?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13038">http://arxiv.org/abs/2309.13038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Sun, Nidham Gazagnadou, Vivek Sharma, Lingjuan Lyu, Hongdong Li, Liang Zheng</li>
<li>for: 这篇论文主要是为了研究现有的手工图像质量指标是否能够准确反映人类对隐私信息的识别度。</li>
<li>methods: 这篇论文使用了4种现有的攻击方法来重建图像，并询问多个人标注者判断重建图像是否可识。</li>
<li>results: 研究发现现有的手工指标与人类对隐私信息的识别度强度不匹配，甚至自身差异也很大。提出了一种学习基于的measure called SemSim来评估重建图像的semantic相似性，并证明SemSim具有更高的人类评价相关性。<details>
<summary>Abstract</summary>
Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly used to evaluate model privacy risk under reconstruction attacks. Under these metrics, reconstructed images that are determined to resemble the original one generally indicate more privacy leakage. Images determined as overall dissimilar, on the other hand, indicate higher robustness against attack. However, there is no guarantee that these metrics well reflect human opinions, which, as a judgement for model privacy leakage, are more trustworthy. In this paper, we comprehensively study the faithfulness of these hand-crafted metrics to human perception of privacy information from the reconstructed images. On 5 datasets ranging from natural images, faces, to fine-grained classes, we use 4 existing attack methods to reconstruct images from many different classification models and, for each reconstructed image, we ask multiple human annotators to assess whether this image is recognizable. Our studies reveal that the hand-crafted metrics only have a weak correlation with the human evaluation of privacy leakage and that even these metrics themselves often contradict each other. These observations suggest risks of current metrics in the community. To address this potential risk, we propose a learning-based measure called SemSim to evaluate the Semantic Similarity between the original and reconstructed images. SemSim is trained with a standard triplet loss, using an original image as an anchor, one of its recognizable reconstructed images as a positive sample, and an unrecognizable one as a negative. By training on human annotations, SemSim exhibits a greater reflection of privacy leakage on the semantic level. We show that SemSim has a significantly higher correlation with human judgment compared with existing metrics. Moreover, this strong correlation generalizes to unseen datasets, models and attack methods.
</details>
<details>
<summary>摘要</summary>
手工制作的图像质量指标，如PSNR和SSIM，通常用于评估模型隐私风险的重建攻击。在这些指标下，可以重建的图像，如果与原始图像相似，则表示更大的隐私泄露。相反，如果图像与原始图像不相似，则表示更高的鲁棒性。但是，这些指标并不能保证与人类意见相符，人类意见是评估模型隐私泄露的更可靠的判断标准。在这篇论文中，我们全面研究了这些手工制作的指标是否能够准确反映人类对重建图像中的隐私信息的评估。在5个不同类型的数据集上，我们使用4种不同的攻击方法来重建图像，并对每个重建图像请多名人工标注者评估这个图像是否可识别。我们的研究发现，这些手工制作的指标与人类对隐私信息的评估存在较弱的相关性，甚至这些指标本身经常相互矛盾。这些观察表明了现有的指标在社区中的风险。为了解决这个潜在的风险，我们提议一种学习基于的度量方法，即SemSim，用于评估重建图像与原始图像之间的semantic相似性。SemSim通过使用标准的 triplet损失函数，使用原始图像作为固定点，一个可识别的重建图像作为正样本，一个不可识别的重建图像作为负样本进行训练。通过人工标注，SemSim能够更好地反映隐私信息的semantic水平上的泄露。我们展示SemSim与现有指标之间存在高度相关性，并且这种相关性可以在未看到的数据集、模型和攻击方法上进行扩展。
</details></li>
</ul>
<hr>
<h2 id="Performance-Analysis-of-UNet-and-Variants-for-Medical-Image-Segmentation"><a href="#Performance-Analysis-of-UNet-and-Variants-for-Medical-Image-Segmentation" class="headerlink" title="Performance Analysis of UNet and Variants for Medical Image Segmentation"></a>Performance Analysis of UNet and Variants for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13013">http://arxiv.org/abs/2309.13013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Walid Ehab, Yongmin Li</li>
<li>for: 本研究旨在探讨深度学习模型在医疗图像分割中的应用，特别是UNet架构和其变种的表现。</li>
<li>methods: 本研究使用了深度学习模型，包括标准UNet、Res-UNet和Attention Res-UNet三种架构，对多种医疗图像分割任务进行评估。</li>
<li>results: 研究发现，扩展UNet架构具有优秀的医疗图像分割能力，而Res-UNet和Attention Res-UNet架构具有更平滑的整合和更高的性能，特别是处理细节图像时。<details>
<summary>Abstract</summary>
Medical imaging plays a crucial role in modern healthcare by providing non-invasive visualisation of internal structures and abnormalities, enabling early disease detection, accurate diagnosis, and treatment planning. This study aims to explore the application of deep learning models, particularly focusing on the UNet architecture and its variants, in medical image segmentation. We seek to evaluate the performance of these models across various challenging medical image segmentation tasks, addressing issues such as image normalization, resizing, architecture choices, loss function design, and hyperparameter tuning. The findings reveal that the standard UNet, when extended with a deep network layer, is a proficient medical image segmentation model, while the Res-UNet and Attention Res-UNet architectures demonstrate smoother convergence and superior performance, particularly when handling fine image details. The study also addresses the challenge of high class imbalance through careful preprocessing and loss function definitions. We anticipate that the results of this study will provide useful insights for researchers seeking to apply these models to new medical imaging problems and offer guidance and best practices for their implementation.
</details>
<details>
<summary>摘要</summary>
医学影像在现代医疗中扮演着重要的角色，通过非侵入性的视觉化内部结构和异常，提高疾病早期检测、精准诊断和治疗规划。本研究旨在探讨深度学习模型，尤其是UNet架构和其变体，在医学图像分割任务中的应用。我们希望通过不同的挑战性医学图像分割任务来评估这些模型的表现，解决问题如图像normalization、resize、架构选择、损失函数设计和Hyperparameter优化。研究发现，标准的UNet架构，当扩展了深度网络层时，是一个高效的医学图像分割模型，而Res-UNet和Attention Res-UNet架构在处理细节时表现更好，特别是在处理细节时。此外，我们还 Addresses the challenge of high class imbalance through careful preprocessing and loss function definitions。我们预计这些结果将为研究人员在新的医学影像问题上应用这些模型提供有用的指导和最佳实践。
</details></li>
</ul>
<hr>
<h2 id="Deep3DSketch-Rapid-3D-Modeling-from-Single-Free-hand-Sketches"><a href="#Deep3DSketch-Rapid-3D-Modeling-from-Single-Free-hand-Sketches" class="headerlink" title="Deep3DSketch+: Rapid 3D Modeling from Single Free-hand Sketches"></a>Deep3DSketch+: Rapid 3D Modeling from Single Free-hand Sketches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13006">http://arxiv.org/abs/2309.13006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianrun Chen, Chenglong Fu, Ying Zang, Lanyun Zhu, Jia Zhang, Papa Mao, Lingyun Sun</li>
<li>for:  This paper aims to provide an end-to-end approach for 3D modeling using only a single free-hand sketch, without requiring multiple sketches or view information.</li>
<li>methods: The proposed approach, called Deep3DSketch+, uses a lightweight generation network for efficient inference in real-time, and a structural-aware adversarial training approach with a Stroke Enhancement Module (SEM) to capture the structural information and facilitate learning of realistic and fine-detailed shape structures.</li>
<li>results: The proposed approach achieved state-of-the-art (SOTA) performance on both synthetic and real datasets, demonstrating its effectiveness in generating high-fidelity 3D models from a single free-hand sketch.<details>
<summary>Abstract</summary>
The rapid development of AR/VR brings tremendous demands for 3D content. While the widely-used Computer-Aided Design (CAD) method requires a time-consuming and labor-intensive modeling process, sketch-based 3D modeling offers a potential solution as a natural form of computer-human interaction. However, the sparsity and ambiguity of sketches make it challenging to generate high-fidelity content reflecting creators' ideas. Precise drawing from multiple views or strategic step-by-step drawings is often required to tackle the challenge but is not friendly to novice users. In this work, we introduce a novel end-to-end approach, Deep3DSketch+, which performs 3D modeling using only a single free-hand sketch without inputting multiple sketches or view information. Specifically, we introduce a lightweight generation network for efficient inference in real-time and a structural-aware adversarial training approach with a Stroke Enhancement Module (SEM) to capture the structural information to facilitate learning of the realistic and fine-detailed shape structures for high-fidelity performance. Extensive experiments demonstrated the effectiveness of our approach with the state-of-the-art (SOTA) performance on both synthetic and real datasets.
</details>
<details>
<summary>摘要</summary>
rapid development of AR/VR 带来巨大的三维内容需求，而传统的计算机支持设计（CAD）方法需要时间consuming 和 labor-intensive modeling process， sketch-based 三维模型化呈现了一个可能的解决方案，但是绘制缺乏和模糊性使得模型化困难以实现创作者的想法。需要精确地从多个视图或步骤性的绘制来解决这个挑战，但是这并不友好于初学者。在这种工作中，我们介绍了一种新的端到端方法，即 Deep3DSketch+，它可以通过单个自由手绘制来完成3D模型化，不需要多个绘制或视图信息输入。我们还引入了轻量级生成网络以实现实时执行，以及一种结构意识的对抗训练方法和笔触提升模块（SEM），以捕捉结构信息，使模型学习真实和细节rich shape结构，以实现高精度性。我们的实验表明，我们的方法可以与当前最佳性（SOTA）在synthetic和实际数据集上达到最高性能。
</details></li>
</ul>
<hr>
<h2 id="Point-Cloud-Network-An-Order-of-Magnitude-Improvement-in-Linear-Layer-Parameter-Count"><a href="#Point-Cloud-Network-An-Order-of-Magnitude-Improvement-in-Linear-Layer-Parameter-Count" class="headerlink" title="Point Cloud Network: An Order of Magnitude Improvement in Linear Layer Parameter Count"></a>Point Cloud Network: An Order of Magnitude Improvement in Linear Layer Parameter Count</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12996">http://arxiv.org/abs/2309.12996</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/chetterich/pcn-paper-and-materials">https://gitlab.com/chetterich/pcn-paper-and-materials</a></li>
<li>paper_authors: Charles Hetterich</li>
<li>for: 本文介绍了Point Cloud Network（PCN）架构，一种新的深度学习网络实现方式，并提供了实验证明PCN的优越性比多层感知器（MLP）。</li>
<li>methods: 本文使用了MLP和PCN两种不同的架构来训练多个模型，包括原始的AlexNet模型，以便对直接比较线性层的性能。</li>
<li>results: 研究发现，使用PCN架构的AlexNet-PCN16模型可以与原始AlexNet模型具有相同的测试准确率（test accuracy），仅占AlexNet模型的99.5%参数量。所有训练都在云端RTX 4090 GPU上进行，使用了pytorch库进行模型构建和训练。<details>
<summary>Abstract</summary>
This paper introduces the Point Cloud Network (PCN) architecture, a novel implementation of linear layers in deep learning networks, and provides empirical evidence to advocate for its preference over the Multilayer Perceptron (MLP) in linear layers. We train several models, including the original AlexNet, using both MLP and PCN architectures for direct comparison of linear layers (Krizhevsky et al., 2012). The key results collected are model parameter count and top-1 test accuracy over the CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009). AlexNet-PCN16, our PCN equivalent to AlexNet, achieves comparable efficacy (test accuracy) to the original architecture with a 99.5% reduction of parameters in its linear layers. All training is done on cloud RTX 4090 GPUs, leveraging pytorch for model construction and training. Code is provided for anyone to reproduce the trials from this paper.
</details>
<details>
<summary>摘要</summary></li>
<li>The PCN architecture has fewer parameters (99.5% fewer in the linear layers) but still achieves the same level of accuracy as the original AlexNet.* The PCN architecture performs well on both the CIFAR-10 and CIFAR-100 datasets.All of the training was done on cloud RTX 4090 GPUs using PyTorch for model construction and training. The code for reproducing the trials is provided.</details></li>
</ul>
<hr>
<h2 id="License-Plate-Recognition-Based-On-Multi-Angle-View-Model"><a href="#License-Plate-Recognition-Based-On-Multi-Angle-View-Model" class="headerlink" title="License Plate Recognition Based On Multi-Angle View Model"></a>License Plate Recognition Based On Multi-Angle View Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12972">http://arxiv.org/abs/2309.12972</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeniSoida/pl1">https://github.com/zeniSoida/pl1</a></li>
<li>paper_authors: Dat Tran-Anh, Khanh Linh Tran, Hoai-Nam Vu</li>
<li>for: 本研究旨在解决图像&#x2F;视频中文检测问题，尤其是识别车牌上的文字。</li>
<li>methods: 本方法 combinates multiple views of license plates to improve text detection accuracy. 具体来说，我们使用三个视角（view-1、view-2、view-3）来识别车牌上的文字组成部分，并使用相似度和距离度量来确定最佳匹配。</li>
<li>results: 实验结果表明，提出的方法在自主收集的PTITPlates dataset和Stanford Cars Dataset上具有较高的识别精度，较exist方法有所提高。<details>
<summary>Abstract</summary>
In the realm of research, the detection/recognition of text within images/videos captured by cameras constitutes a highly challenging problem for researchers. Despite certain advancements achieving high accuracy, current methods still require substantial improvements to be applicable in practical scenarios. Diverging from text detection in images/videos, this paper addresses the issue of text detection within license plates by amalgamating multiple frames of distinct perspectives. For each viewpoint, the proposed method extracts descriptive features characterizing the text components of the license plate, specifically corner points and area. Concretely, we present three viewpoints: view-1, view-2, and view-3, to identify the nearest neighboring components facilitating the restoration of text components from the same license plate line based on estimations of similarity levels and distance metrics. Subsequently, we employ the CnOCR method for text recognition within license plates. Experimental results on the self-collected dataset (PTITPlates), comprising pairs of images in various scenarios, and the publicly available Stanford Cars Dataset, demonstrate the superiority of the proposed method over existing approaches.
</details>
<details>
<summary>摘要</summary>
在研究领域中，图像/视频中的文本检测/识别问题对研究人员来说是非常困难的。尽管有一些进步，但现有方法仍然需要进一步改进才能在实际场景中应用。与图像/视频中的文本检测方法不同，这篇论文强调车牌上的文本检测，通过将多个视角的帧合并来实现。对于每个视角，我们提出的方法可以提取描述文本组件的特征，包括角点和面积。具体来说，我们提出了三个视角：视角1、视角2和视角3，用于标识同一个车牌线上的相邻组件，并且根据相似度和距离度量来重建车牌上的文本组件。接着，我们使用CnOCR方法进行车牌上文本识别。实验结果表明，我们的提议方法在自己收集的数据集（PTITPlates）和公共可用的 stanford cars 数据集上具有显著优势，超过现有方法。
</details></li>
</ul>
<hr>
<h2 id="PI-RADS-v2-Compliant-Automated-Segmentation-of-Prostate-Zones-Using-co-training-Motivated-Multi-task-Dual-Path-CNN"><a href="#PI-RADS-v2-Compliant-Automated-Segmentation-of-Prostate-Zones-Using-co-training-Motivated-Multi-task-Dual-Path-CNN" class="headerlink" title="PI-RADS v2 Compliant Automated Segmentation of Prostate Zones Using co-training Motivated Multi-task Dual-Path CNN"></a>PI-RADS v2 Compliant Automated Segmentation of Prostate Zones Using co-training Motivated Multi-task Dual-Path CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12970">http://arxiv.org/abs/2309.12970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnab Das, Suhita Ghosh, Sebastian Stober</li>
<li>for: 这个论文的目的是提供一种自动化的检测和评估肾脏癌病变的方法，以帮助提高诊断和治疗的精度。</li>
<li>methods: 这个方法使用了一种双树 convolutional neural network (CNN)，每个树分别捕捉不同的区域（PZ、TZ、DPU和AFS）的表示。在第二个训练阶段，不同的树的表示进行了互补性的调整，以提高 segmentation 精度。此外，这个方法还 integrate 了多任务学习来进一步提高 segmentation 精度。</li>
<li>results: 根据这个方法，误差（mean absolute symmetric distance）的提高量为7.56%、11.00%、58.43%和19.67%对PZ、TZ、DPU和AFS区域进行了提高。<details>
<summary>Abstract</summary>
The detailed images produced by Magnetic Resonance Imaging (MRI) provide life-critical information for the diagnosis and treatment of prostate cancer. To provide standardized acquisition, interpretation and usage of the complex MRI images, the PI-RADS v2 guideline was proposed. An automated segmentation following the guideline facilitates consistent and precise lesion detection, staging and treatment. The guideline recommends a division of the prostate into four zones, PZ (peripheral zone), TZ (transition zone), DPU (distal prostatic urethra) and AFS (anterior fibromuscular stroma). Not every zone shares a boundary with the others and is present in every slice. Further, the representations captured by a single model might not suffice for all zones. This motivated us to design a dual-branch convolutional neural network (CNN), where each branch captures the representations of the connected zones separately. Further, the representations from different branches act complementary to each other at the second stage of training, where they are fine-tuned through an unsupervised loss. The loss penalises the difference in predictions from the two branches for the same class. We also incorporate multi-task learning in our framework to further improve the segmentation accuracy. The proposed approach improves the segmentation accuracy of the baseline (mean absolute symmetric distance) by 7.56%, 11.00%, 58.43% and 19.67% for PZ, TZ, DPU and AFS zones respectively.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging (MRI) 提供了生命critical的信息，用于诊断和治疗前列腺癌。为了提供标准化的获取、解释和使用复杂的MRI图像，PI-RADS v2指南被提出。一个自动 segmentation 可以确保consistent和精确的肿坏检测、stage和治疗。指南建议将前列腺分成四个区域：PZ（周边区）、TZ（过渡区）、DPU（后束肠URETHRA）和AFS（前锥形connective tissue）。不是每个区域都与其他区域接壤，而且不同的区域在每个层次中的表现不同。这种情况motivates我们设计了一个双支分布式 convolutional neural network (CNN)，其中每支分布式 CNN 分别捕捉connected zones 的表现。此外，不同支分布式 CNN 在第二个训练阶段 fine-tune 的损失中进行互补作用，这种损失penalizes 两支分布式 CNN 对同一类型的预测差异。我们还在我们的框架中包含多任务学习，以进一步提高 segmentation 精度。提出的方法与基准（mean absolute symmetric distance）的 segmentation 精度相比，提高了7.56%、11.00%、58.43%和19.67%  respectivly 的PZ、TZ、DPU和AFS区域。
</details></li>
</ul>
<hr>
<h2 id="Detect-Every-Thing-with-Few-Examples"><a href="#Detect-Every-Thing-with-Few-Examples" class="headerlink" title="Detect Every Thing with Few Examples"></a>Detect Every Thing with Few Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12969">http://arxiv.org/abs/2309.12969</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlzxy/devit">https://github.com/mlzxy/devit</a></li>
<li>paper_authors: Xinyu Zhang, Yuting Wang, Abdeslam Boularias</li>
<li>for: 这个论文目的是开发一种基于视觉语言的开放类型物体检测器，可以检测到训练时没有看到的类别。</li>
<li>methods: 这个论文使用了视觉只的DINOv2背景，并通过示例图像来学习新的类别。它还提出了一种将多类分类任务转换为 binary 分类任务的技术，以及一种地区卷积技术来优化本地化检测。</li>
<li>results: 在COCO和LVIS测试集上，DE-ViT比开放类型SoTA高6.9个AP50，并在新类中达到50个AP50。在几shot和一shot SoTA上，DE-ViT比较高7.2个mAP和2.8个AP50。在LVIS测试集上，DE-ViT比开放类型SoTA高2.2个mask AP，达到34.3个mask APr。<details>
<summary>Abstract</summary>
Open-set object detection aims at detecting arbitrary categories beyond those seen during training. Most recent advancements have adopted the open-vocabulary paradigm, utilizing vision-language backbones to represent categories with language. In this paper, we introduce DE-ViT, an open-set object detector that employs vision-only DINOv2 backbones and learns new categories through example images instead of language. To improve general detection ability, we transform multi-classification tasks into binary classification tasks while bypassing per-class inference, and propose a novel region propagation technique for localization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shot object detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms the open-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViT surpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabulary SoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available at https://github.com/mlzxy/devit.
</details>
<details>
<summary>摘要</summary>
“开放集Object检测目标在训练时未经看过的类型检测。最新的进展都采用了开放词汇思想，通过视力语言核心来表示类别。本文介绍DE-ViT，一种基于视力只的DINOv2核心实现开放集Object检测，不需要语言。为提高检测能力，我们将多类型分类任务转化为二分类任务，并提出一种新的区域卷积技术。我们在COCO和LVIS上进行了开放集、少量和一个批量Object检测测试，对COCO的开放集SoTA进行了6.9 AP50的超越和50 AP50的新类表现。对于少量和一个批量SoTA，DE-ViT也进行了15 mAP和7.2 mAP的超越。对于LVIS，DE-ViT超越了开放集SoTA2.2个面积AP和34.3个面积APr。代码可以在https://github.com/mlzxy/devit中下载。”
</details></li>
</ul>
<hr>
<h2 id="Deformable-3D-Gaussians-for-High-Fidelity-Monocular-Dynamic-Scene-Reconstruction"><a href="#Deformable-3D-Gaussians-for-High-Fidelity-Monocular-Dynamic-Scene-Reconstruction" class="headerlink" title="Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction"></a>Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13101">http://arxiv.org/abs/2309.13101</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ingra14m/Deformable-3D-Gaussians">https://github.com/ingra14m/Deformable-3D-Gaussians</a></li>
<li>paper_authors: Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, Xiaogang Jin</li>
<li>for: 本研究旨在解决现有的动态场景重建和渲染方法中的缺陷，提供更高质量和实时速度的方法。</li>
<li>methods: 我们提出了一种基于显式3D高斯函数的弹性3D高斯拼接法，通过对可见空间中的高斯函数进行扭曲学习，来模型单目动态场景。我们还提出了一种缓解训练过程中偏差pose的技术，以提高时间 interpolate任务的平滑性。</li>
<li>results: 我们的方法在渲染质量和实时速度两个方面具有显著优势，与现有方法相比显著提高了渲染质量和速度。这使得我们的方法适用于多视图合成、时间合成和实时渲染等任务。<details>
<summary>Abstract</summary>
Implicit neural representation has opened up new avenues for dynamic scene reconstruction and rendering. Nonetheless, state-of-the-art methods of dynamic neural rendering rely heavily on these implicit representations, which frequently struggle with accurately capturing the intricate details of objects in the scene. Furthermore, implicit methods struggle to achieve real-time rendering in general dynamic scenes, limiting their use in a wide range of tasks. To address the issues, we propose a deformable 3D Gaussians Splatting method that reconstructs scenes using explicit 3D Gaussians and learns Gaussians in canonical space with a deformation field to model monocular dynamic scenes. We also introduced a smoothing training mechanism with no extra overhead to mitigate the impact of inaccurate poses in real datasets on the smoothness of time interpolation tasks. Through differential gaussian rasterization, the deformable 3D Gaussians not only achieve higher rendering quality but also real-time rendering speed. Experiments show that our method outperforms existing methods significantly in terms of both rendering quality and speed, making it well-suited for tasks such as novel-view synthesis, time synthesis, and real-time rendering.
</details>
<details>
<summary>摘要</summary>
匿名神经表示法已经开启了新的动态场景重建和渲染领域。然而，现状的动态神经渲染方法通常依赖于这些匿名表示法，它们经常快速捕捉场景中对象的细节。此外，匿名方法在普通的动态场景中实时渲染通常困难，限制了它们在各种任务中的使用。为了解决这些问题，我们提出了使用可变的3DGAUSSIAN分辨率拼接法来重建场景，这种方法使用显式的3DGAUSSIAN和 canonical space中的扭曲场来模型单目动态场景。我们还提出了一种缓和训练机制，可以在真实数据集中减少不准确的姿势的影响，以提高时间插值任务的平滑性。通过差分 Gaussian 渲染，可变的3DGAUSSIAN不仅实现了更高的渲染质量，还具有实时渲染速度。实验表明，我们的方法与现有方法相比，在渲染质量和速度两个方面具有显著的优势，适用于如新视角合成、时间插值和实时渲染等任务。
</details></li>
</ul>
<hr>
<h2 id="On-Data-Fabrication-in-Collaborative-Vehicular-Perception-Attacks-and-Countermeasures"><a href="#On-Data-Fabrication-in-Collaborative-Vehicular-Perception-Attacks-and-Countermeasures" class="headerlink" title="On Data Fabrication in Collaborative Vehicular Perception: Attacks and Countermeasures"></a>On Data Fabrication in Collaborative Vehicular Perception: Attacks and Countermeasures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12955">http://arxiv.org/abs/2309.12955</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zqzqz/advcollaborativeperception">https://github.com/zqzqz/advcollaborativeperception</a></li>
<li>paper_authors: Qingzhao Zhang, Shuowei Jin, Ruiyang Zhu, Jiachen Sun, Xumiao Zhang, Qi Alfred Chen, Z. Morley Mao</li>
<li>for: 这篇论文旨在探讨Connected and Autonomous Vehicles (CAVs) 在协同感知系统中的安全隐患，以及如何通过协同感知系统中的数据来实现安全驱动。</li>
<li>methods: 本论文使用了现场实验和仿真方法来研究协同感知系统中的数据攻击和防御策略。</li>
<li>results: 本论文的实验结果显示，攻击者可以通过提供假数据来让CAVs做出错误的驾驶决策，导致减速或增加碰撞风险。而提出的异常检测方法可以检测91.5%的攻击，并在实际场景中减少了攻击的影响。<details>
<summary>Abstract</summary>
Collaborative perception, which greatly enhances the sensing capability of connected and autonomous vehicles (CAVs) by incorporating data from external resources, also brings forth potential security risks. CAVs' driving decisions rely on remote untrusted data, making them susceptible to attacks carried out by malicious participants in the collaborative perception system. However, security analysis and countermeasures for such threats are absent. To understand the impact of the vulnerability, we break the ground by proposing various real-time data fabrication attacks in which the attacker delivers crafted malicious data to victims in order to perturb their perception results, leading to hard brakes or increased collision risks. Our attacks demonstrate a high success rate of over 86% on high-fidelity simulated scenarios and are realizable in real-world experiments. To mitigate the vulnerability, we present a systematic anomaly detection approach that enables benign vehicles to jointly reveal malicious fabrication. It detects 91.5% of attacks with a false positive rate of 3% in simulated scenarios and significantly mitigates attack impacts in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
将文本翻译成简化中文：协同感知，它使connected and autonomous vehicles (CAVs) 的感知能力得到了大幅提高，但也涉及到了安全隐患。CAVs 的驾驶决策取决于外部不可靠数据，使其易受到来自collaborative perception系统中的恶意参与者的攻击。然而，对于这些威胁的安全分析和对策缺乏。为了了解攻击的影响，我们开辟了一个研究，在协同感知系统中提出了不同的实时数据造假攻击。攻击者通过向受害者传递预制作的假数据来干扰受害者的感知结果，导致停车或增加碰撞风险。我们的攻击得到了高于86%的成功率在高精度的模拟场景中，并在实际场景中也是可行的。为了缓解攻击，我们提出了一种系统化异常检测方法，它能够在benign vehicles之间共同披露恶意fabrication。它在模拟场景中检测到91.5%的攻击， false positive率仅3%。在实际场景中，它能够有效地缓解攻击的影响。
</details></li>
</ul>
<hr>
<h2 id="Inter-vendor-harmonization-of-Computed-Tomography-CT-reconstruction-kernels-using-unpaired-image-translation"><a href="#Inter-vendor-harmonization-of-Computed-Tomography-CT-reconstruction-kernels-using-unpaired-image-translation" class="headerlink" title="Inter-vendor harmonization of Computed Tomography (CT) reconstruction kernels using unpaired image translation"></a>Inter-vendor harmonization of Computed Tomography (CT) reconstruction kernels using unpaired image translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12953">http://arxiv.org/abs/2309.12953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aravind R. Krishnan, Kaiwen Xu, Thomas Li, Chenyu Gao, Lucas W. Remedios, Praitayini Kanakaraj, Ho Hin Lee, Shunxing Bao, Kim L. Sandler, Fabien Maldonado, Ivana Isgum, Bennett A. Landman</li>
<li>for: This paper aims to investigate the harmonization of computed tomography (CT) scans from different manufacturers using an unpaired image translation approach.</li>
<li>methods: The authors use a multipath cycle generative adversarial network (GAN) to harmonize the CT scans and evaluate the effect of harmonization on the reconstruction kernels.</li>
<li>results: The authors find that their approach minimizes differences in emphysema measurement and highlights the impact of age, sex, smoking status, and vendor on emphysema quantification.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是通过不同生产厂商的计算Tomography（CT）扫描图像的不同构成器进行协调。</li>
<li>methods: 作者使用了一种多路径循环生成算法网络（GAN）来协调CT扫描图像，并评估构成器的影响。</li>
<li>results: 作者发现，他们的方法可以减少不同构成器的差异，并且高亮年龄、性别、吸烟状况和生产厂商对emphysema量化的影响。<details>
<summary>Abstract</summary>
The reconstruction kernel in computed tomography (CT) generation determines the texture of the image. Consistency in reconstruction kernels is important as the underlying CT texture can impact measurements during quantitative image analysis. Harmonization (i.e., kernel conversion) minimizes differences in measurements due to inconsistent reconstruction kernels. Existing methods investigate harmonization of CT scans in single or multiple manufacturers. However, these methods require paired scans of hard and soft reconstruction kernels that are spatially and anatomically aligned. Additionally, a large number of models need to be trained across different kernel pairs within manufacturers. In this study, we adopt an unpaired image translation approach to investigate harmonization between and across reconstruction kernels from different manufacturers by constructing a multipath cycle generative adversarial network (GAN). We use hard and soft reconstruction kernels from the Siemens and GE vendors from the National Lung Screening Trial dataset. We use 50 scans from each reconstruction kernel and train a multipath cycle GAN. To evaluate the effect of harmonization on the reconstruction kernels, we harmonize 50 scans each from Siemens hard kernel, GE soft kernel and GE hard kernel to a reference Siemens soft kernel (B30f) and evaluate percent emphysema. We fit a linear model by considering the age, smoking status, sex and vendor and perform an analysis of variance (ANOVA) on the emphysema scores. Our approach minimizes differences in emphysema measurement and highlights the impact of age, sex, smoking status and vendor on emphysema quantification.
</details>
<details>
<summary>摘要</summary>
computed tomography（CT）生成中的重建核心（kernel）会决定图像的文字。保持重建核心的一致性非常重要，因为下面的CT文字可能会影响量化图像分析中的测量结果。为了解决这个问题，我们采用了一种不带对的图像翻译方法，并使用多条路径生成反向传播神经网络（GAN）来调整不同制造商的重建核心。我们使用来自SIEMENS和GE两家公司的硬件和软件重建核心，从国家肺癌检测试验数据集中选择50个扫描。我们使用50个扫描来训练多条路径GAN，并对每个重建核心进行调整。为了评估调整后的重建核心的影响，我们对SIEMENS硬件重建核心、GE软件重建核心和GE硬件重建核心进行调整，并对每个扫描进行50次评估。我们使用年龄、吸烟状况、性别和制造商作为 Linear 模型的可变量，并对抑瘤率进行分析变异（ANOVA）。我们的方法可以减少不同重建核心之间的差异，并高亮制造商、性别、吸烟状况和年龄对抑瘤率的影响。
</details></li>
</ul>
<hr>
<h2 id="Background-Activation-Suppression-for-Weakly-Supervised-Object-Localization-and-Semantic-Segmentation"><a href="#Background-Activation-Suppression-for-Weakly-Supervised-Object-Localization-and-Semantic-Segmentation" class="headerlink" title="Background Activation Suppression for Weakly Supervised Object Localization and Semantic Segmentation"></a>Background Activation Suppression for Weakly Supervised Object Localization and Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12943">http://arxiv.org/abs/2309.12943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wpy1999/bas-extension">https://github.com/wpy1999/bas-extension</a></li>
<li>paper_authors: Wei Zhai, Pingyu Wu, Kai Zhu, Yang Cao, Feng Wu, Zheng-Jun Zha</li>
<li>For: 本研究旨在提高弱度指导对象本地化和 semantic segmentation的性能，通过生成foreground prediction map（FPM）来实现像素级本地化。* Methods: 该研究提出了两个关键的实验观察：1）当已经训练过的网络中的foreground mask扩展时，cross-entropy会 converge to zero，而且activation value会持续增加 until the foreground mask扩展到对象边界。基于这两个观察，该研究提出了一种Background Activation Suppression（BAS）方法，通过Activation Map Constraint（AMC）模块来减少背景活动值，同时通过foreground region guidance和面积约束来学习整个对象区域。* Results: 对CUB-200-2011和ILSVRC dataset进行了广泛的实验，显示BAS可以 achieve significant and consistent improvement over baseline methods。此外，该方法还 achieve state-of-the-art weakly supervised semantic segmentation性能在PASCAL VOC 2012和MS COCO 2014 dataset上。<details>
<summary>Abstract</summary>
Weakly supervised object localization and semantic segmentation aim to localize objects using only image-level labels. Recently, a new paradigm has emerged by generating a foreground prediction map (FPM) to achieve pixel-level localization. While existing FPM-based methods use cross-entropy to evaluate the foreground prediction map and to guide the learning of the generator, this paper presents two astonishing experimental observations on the object localization learning process: For a trained network, as the foreground mask expands, 1) the cross-entropy converges to zero when the foreground mask covers only part of the object region. 2) The activation value continuously increases until the foreground mask expands to the object boundary. Therefore, to achieve a more effective localization performance, we argue for the usage of activation value to learn more object regions. In this paper, we propose a Background Activation Suppression (BAS) method. Specifically, an Activation Map Constraint (AMC) module is designed to facilitate the learning of generator by suppressing the background activation value. Meanwhile, by using foreground region guidance and area constraint, BAS can learn the whole region of the object. In the inference phase, we consider the prediction maps of different categories together to obtain the final localization results. Extensive experiments show that BAS achieves significant and consistent improvement over the baseline methods on the CUB-200-2011 and ILSVRC datasets. In addition, our method also achieves state-of-the-art weakly supervised semantic segmentation performance on the PASCAL VOC 2012 and MS COCO 2014 datasets. Code and models are available at https://github.com/wpy1999/BAS-Extension.
</details>
<details>
<summary>摘要</summary>
弱地监督对象定位和 semantic segmentation 目标是通过只使用图像级别标签来 lokalisieren objects。 最近，一种新的 paradigm 出现，即通过生成 foreground prediction map (FPM) 来实现像素级定位。 而现有的 FPM 基于方法使用 cross-entropy 来评估 foreground prediction map 并帮助生成器学习，而这篇文章则发现了对对象定位学习过程的两个 astonishing experimental observation：1) 当 foreground mask 扩展时，cross-entropy 会 converge to zero 只有部分object region 被 mask 覆盖; 2) 在 foreground mask 扩展到 object boundary 时，activation value 会不断增加。因此，我们认为使用 activation value 可以更好地学习更多的 object regions。在这篇文章中，我们提出了 Background Activation Suppression (BAS) 方法。具体来说，我们设计了 Activation Map Constraint (AMC) 模块，以便通过压制背景 activation value 来促进生成器的学习。同时，通过使用 foreground region guidance 和 area constraint，BAS 可以学习整个 object 的区域。在推理阶段，我们考虑了不同类别的预测图共同来获得最终的定位结果。我们的实验表明，BAS 可以在 CUB-200-2011 和 ILSVRC 数据集上 achieves 显著和稳定的改进，并且我们的方法也可以在 PASCAL VOC 2012 和 MS COCO 2014 数据集上实现 state-of-the-art 的弱监督 semantic segmentation性能。代码和模型可以在 <https://github.com/wpy1999/BAS-Extension> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Object-Counting-with-Language-Vision-Models"><a href="#Zero-Shot-Object-Counting-with-Language-Vision-Models" class="headerlink" title="Zero-Shot Object Counting with Language-Vision Models"></a>Zero-Shot Object Counting with Language-Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13097">http://arxiv.org/abs/2309.13097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyi Xu, Hieu Le, Dimitris Samaras</li>
<li>for: 本研究旨在实现无需人工标注的物体数量计算，即针对任意类型的物体进行测试时的自动化计数。</li>
<li>methods: 我们提出了一种新的设定方法，即零例SHOT对象计数（ZSC），只需要在测试时提供类名即可。这种方法不需要人工标注，可以自动化操作。我们首先从输入图像中检索一些物体裁剪，然后使用这些裁剪作为计数例子。目标是找到包含目标物体的裁剪，同时也是所有图像中所有物体的视觉表示。我们首先使用大型语言视觉模型，包括CLIP和Stable Diffusion，构建类型质量标准，然后选择包含目标物体的裁剪。此外，我们还提出了一种排名模型，以估算每个裁剪的计数错误，从而选择最适合计数的例子。</li>
<li>results: 我们在最新的无类别物体数量 datasets（FSC-147）上进行了实验，结果表明我们的方法效果很高。<details>
<summary>Abstract</summary>
Class-agnostic object counting aims to count object instances of an arbitrary class at test time. It is challenging but also enables many potential applications. Current methods require human-annotated exemplars as inputs which are often unavailable for novel categories, especially for autonomous systems. Thus, we propose zero-shot object counting (ZSC), a new setting where only the class name is available during test time. This obviates the need for human annotators and enables automated operation. To perform ZSC, we propose finding a few object crops from the input image and use them as counting exemplars. The goal is to identify patches containing the objects of interest while also being visually representative for all instances in the image. To do this, we first construct class prototypes using large language-vision models, including CLIP and Stable Diffusion, to select the patches containing the target objects. Furthermore, we propose a ranking model that estimates the counting error of each patch to select the most suitable exemplars for counting. Experimental results on a recent class-agnostic counting dataset, FSC-147, validate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
“类型无关对象计数”targets counting object instances of an arbitrary class at test time, which is challenging but also enables many potential applications. Current methods require human-annotated exemplars as inputs, which are often unavailable for novel categories, especially for autonomous systems. Therefore, we propose zero-shot object counting (ZSC), a new setting where only the class name is available during test time. This eliminates the need for human annotators and enables automated operation.To perform ZSC, we propose finding a few object crops from the input image and using them as counting exemplars. The goal is to identify patches containing the objects of interest while also being visually representative for all instances in the image. To do this, we first construct class prototypes using large language-vision models, such as CLIP and Stable Diffusion, to select the patches containing the target objects. Additionally, we propose a ranking model that estimates the counting error of each patch to select the most suitable exemplars for counting.Experimental results on a recent class-agnostic counting dataset, FSC-147, validate the effectiveness of our method.
</details></li>
</ul>
<hr>
<h2 id="Bridging-Sensor-Gaps-via-Single-Direction-Tuning-for-Hyperspectral-Image-Classification"><a href="#Bridging-Sensor-Gaps-via-Single-Direction-Tuning-for-Hyperspectral-Image-Classification" class="headerlink" title="Bridging Sensor Gaps via Single-Direction Tuning for Hyperspectral Image Classification"></a>Bridging Sensor Gaps via Single-Direction Tuning for Hyperspectral Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12865">http://arxiv.org/abs/2309.12865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cecilia-xue/hyt-nas">https://github.com/cecilia-xue/hyt-nas</a></li>
<li>paper_authors: Xizhe Xue, Haokui Zhang, Ying Li, Liuwei Wan, Zongwen Bai, Mike Zheng Shou</li>
<li>for:  This paper aims to address the challenge of training ViT models on hyperspectral images (HSIs) with limited training samples.</li>
<li>methods: The proposed method is called single-direction tuning (SDT) and it leverages existing labeled HSI datasets and RGB datasets to enhance the performance on new HSI datasets. SDT uses a parallel architecture, asynchronous cold-hot gradient update strategy, and unidirectional interaction.</li>
<li>results: The proposed Triplet-structured transformer (Tri-Former) achieves better performance compared to several state-of-the-art methods on three representative HSI datasets. Homologous, heterologous and cross-modal tuning experiments verified the effectiveness of the proposed SDT.Here’s the Chinese translation of the three key points:</li>
<li>for: 本研究目的是解决训练 ViT 模型在有限样本的高spectral 图像（HSIs）中的挑战。</li>
<li>methods: 提议的方法是单向调整策略（SDT），它利用现有标注的 HSI 数据集和 RGB 数据集来提高新的 HSI 数据集的性能。SDT 使用并行架构、异步冷热梯度更新策略和单向互动。</li>
<li>results: 提议的 Triplet-structured transformer (Tri-Former) 在三个代表性的 HSI 数据集上达到了许多现状方法的更好性能。同源、异源和跨模态调整实验证明了提议的 SDT 的有效性。<details>
<summary>Abstract</summary>
Recently, some researchers started exploring the use of ViTs in tackling HSI classification and achieved remarkable results. However, the training of ViT models requires a considerable number of training samples, while hyperspectral data, due to its high annotation costs, typically has a relatively small number of training samples. This contradiction has not been effectively addressed. In this paper, aiming to solve this problem, we propose the single-direction tuning (SDT) strategy, which serves as a bridge, allowing us to leverage existing labeled HSI datasets even RGB datasets to enhance the performance on new HSI datasets with limited samples. The proposed SDT inherits the idea of prompt tuning, aiming to reuse pre-trained models with minimal modifications for adaptation to new tasks. But unlike prompt tuning, SDT is custom-designed to accommodate the characteristics of HSIs. The proposed SDT utilizes a parallel architecture, an asynchronous cold-hot gradient update strategy, and unidirectional interaction. It aims to fully harness the potent representation learning capabilities derived from training on heterologous, even cross-modal datasets. In addition, we also introduce a novel Triplet-structured transformer (Tri-Former), where spectral attention and spatial attention modules are merged in parallel to construct the token mixing component for reducing computation cost and a 3D convolution-based channel mixer module is integrated to enhance stability and keep structure information. Comparison experiments conducted on three representative HSI datasets captured by different sensors demonstrate the proposed Tri-Former achieves better performance compared to several state-of-the-art methods. Homologous, heterologous and cross-modal tuning experiments verified the effectiveness of the proposed SDT.
</details>
<details>
<summary>摘要</summary>
近些时候，一些研究人员开始使用ViT来解决高spectralInterval（HSI）分类问题，并取得了显著的成果。然而，ViT模型的训练需要一大量的训练样本，而高spectralInterval数据由于注解成本高，通常只有限量的训练样本。这个矛盾尚未得到有效解决。在这篇论文中，我们提议单向调整（SDT）策略，作为一个桥梁，允许我们通过现有的标注HSI数据集和RGB数据集来提高新的HSI数据集的性能。我们的SDT继承了提前调整的想法，即 reuse pre-trained models with minimal modifications for adaptation to new tasks。不同于提前调整，SDT是特地针对HSIs的定制设计的。我们的SDT采用并行架构、异步冷热Gradient更新策略和单向交互。它旨在完全利用训练在不同数据集上的hetrologous和cross-modal数据的强大表示学习能力。此外，我们还介绍了一种新的Triplet-structured transformer（Tri-Former），其中spectral attention和spatial attention模块在并行的构建token混合组件，以减少计算成本，并integrate了3D卷积基本 Channel mixer模块以提高稳定性和保持结构信息。在三个代表性的HSI数据集上进行了比较实验，我们的Tri-Former表现比一些当前的方法更好。同义、异义和cross-modal调整实验证明了SDT的有效性。
</details></li>
</ul>
<hr>
<h2 id="Associative-Transformer-Is-A-Sparse-Representation-Learner"><a href="#Associative-Transformer-Is-A-Sparse-Representation-Learner" class="headerlink" title="Associative Transformer Is A Sparse Representation Learner"></a>Associative Transformer Is A Sparse Representation Learner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12862">http://arxiv.org/abs/2309.12862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwei Sun, Hideya Ochiai, Zhirong Wu, Stephen Lin, Ryota Kanai</li>
<li>for: 这篇论文旨在探讨如何使用弹性交互来更好地模拟生物学原理，并提出了一种基于全球工作空间理论和相关记忆的Associative Transformer（AiT）模型。</li>
<li>methods: AiT模型使用了跨层聚合的核心空间，并通过结合缓存的方式实现瓶颈式注意力。这些瓶颈式注意力会限制注意力的容量，从而模拟生物学中的弹性交互。</li>
<li>results: 对于多种视觉任务，AiT模型表现出了superiority，可以学习不同的特征弹性，并且可以在不同的输入量和维度上保持复杂度的不变性。<details>
<summary>Abstract</summary>
Emerging from the monolithic pairwise attention mechanism in conventional Transformer models, there is a growing interest in leveraging sparse interactions that align more closely with biological principles. Approaches including the Set Transformer and the Perceiver employ cross-attention consolidated with a latent space that forms an attention bottleneck with limited capacity. Building upon recent neuroscience studies of Global Workspace Theory and associative memory, we propose the Associative Transformer (AiT). AiT induces low-rank explicit memory that serves as both priors to guide bottleneck attention in the shared workspace and attractors within associative memory of a Hopfield network. Through joint end-to-end training, these priors naturally develop module specialization, each contributing a distinct inductive bias to form attention bottlenecks. A bottleneck can foster competition among inputs for writing information into the memory. We show that AiT is a sparse representation learner, learning distinct priors through the bottlenecks that are complexity-invariant to input quantities and dimensions. AiT demonstrates its superiority over methods such as the Set Transformer, Vision Transformer, and Coordination in various vision tasks.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)由传统的对称Transformer模型中的单一对对注意机制而出发，有一种增长的兴趣是利用稀疏的交互来更加准确地遵循生物学原理。包括Set Transformer和Perceiver在内的方法都使用了混合注意力，并通过限制容量的瓶颈注意力来实现稀疏的交互。基于最近的 neuroscience研究的全球工作区理论和相关记忆，我们提出了相关转换器（AiT）。AiT通过强制实现低级别的显式记忆，使得瓶颈注意力在共享工作区中服务为导向注意力的先验知识，并在相关记忆中形成吸引器。通过联合的终端训练，这些先验知识自然发展出模块特化，每个模块增加了不同的抽象偏好，以形成注意瓶颈。这个瓶颈可以促进输入竞争对写入记忆。我们显示AiT是一种稀疏表示学习器，通过瓶颈学习出不同的先验知识，这些先验知识是输入量和维度的复杂性不变的。AiT在不同的视觉任务中表现出优势。
</details></li>
</ul>
<hr>
<h2 id="Cross-Modal-Translation-and-Alignment-for-Survival-Analysis"><a href="#Cross-Modal-Translation-and-Alignment-for-Survival-Analysis" class="headerlink" title="Cross-Modal Translation and Alignment for Survival Analysis"></a>Cross-Modal Translation and Alignment for Survival Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12855">http://arxiv.org/abs/2309.12855</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ft-zhou-zzz/cmta">https://github.com/ft-zhou-zzz/cmta</a></li>
<li>paper_authors: Fengtao Zhou, Hao Chen</li>
<li>for: 这篇论文的目的是提出一个 Cross-Modal Translation and Alignment (CMTA) 框架，以探索不同模式之间的自然联系，并将不同模式之间的资讯转换为彼此对应的形式，以提高统计分析的精度和准确性。</li>
<li>methods: 这篇论文使用了两个平行的encoder-decoder结构，将多modal资料融合为单一的数据表现，并通过将生成的跨模式表现与原始模式表现进行对应，以提高模式之间的联系和转换资讯。此外，这篇论文还提出了一个跨模式注意力模组，作为不同模式之间的资讯桥梁，以实现跨模式的互动和资讯转换。</li>
<li>results: 这篇论文的实验结果显示，跨模式转换和对应的CMTA框架能够在五个公共TCGA数据集上实现更高的统计分析精度和准确性，比起现有的方法。<details>
<summary>Abstract</summary>
With the rapid advances in high-throughput sequencing technologies, the focus of survival analysis has shifted from examining clinical indicators to incorporating genomic profiles with pathological images. However, existing methods either directly adopt a straightforward fusion of pathological features and genomic profiles for survival prediction, or take genomic profiles as guidance to integrate the features of pathological images. The former would overlook intrinsic cross-modal correlations. The latter would discard pathological information irrelevant to gene expression. To address these issues, we present a Cross-Modal Translation and Alignment (CMTA) framework to explore the intrinsic cross-modal correlations and transfer potential complementary information. Specifically, we construct two parallel encoder-decoder structures for multi-modal data to integrate intra-modal information and generate cross-modal representation. Taking the generated cross-modal representation to enhance and recalibrate intra-modal representation can significantly improve its discrimination for comprehensive survival analysis. To explore the intrinsic crossmodal correlations, we further design a cross-modal attention module as the information bridge between different modalities to perform cross-modal interactions and transfer complementary information. Our extensive experiments on five public TCGA datasets demonstrate that our proposed framework outperforms the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
随着高通量测序技术的快速发展，生存分析的注意点从临床指标转移到了将 genomic  profil 与生理图像 incorporate 到生存预测中。现有方法可以分为两类：直接将生理特征和 genomic profil 简单地拼接起来进行生存预测，或者将 genomic profil 作为引导，将生理图像的特征 integrate 到生存预测中。前者可能会忽略不同Modal 之间的自然相关性。后者可能会丢弃不相关于蛋白表达的生理信息。为解决这些问题，我们提出了一种 Cross-Modal Translation and Alignment (CMTA) 框架，用于探索不同Modal 之间的自然相关性，并将 complementary 信息传递。 Specifically, we construct two parallel encoder-decoder structures for multi-modal data to integrate intra-modal information and generate cross-modal representation. Taking the generated cross-modal representation to enhance and recalibrate intra-modal representation can significantly improve its discrimination for comprehensive survival analysis. To explore the intrinsic crossmodal correlations, we further design a cross-modal attention module as the information bridge between different modalities to perform cross-modal interactions and transfer complementary information. Our extensive experiments on five public TCGA datasets demonstrate that our proposed framework outperforms the state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="SRFNet-Monocular-Depth-Estimation-with-Fine-grained-Structure-via-Spatial-Reliability-oriented-Fusion-of-Frames-and-Events"><a href="#SRFNet-Monocular-Depth-Estimation-with-Fine-grained-Structure-via-Spatial-Reliability-oriented-Fusion-of-Frames-and-Events" class="headerlink" title="SRFNet: Monocular Depth Estimation with Fine-grained Structure via Spatial Reliability-oriented Fusion of Frames and Events"></a>SRFNet: Monocular Depth Estimation with Fine-grained Structure via Spatial Reliability-oriented Fusion of Frames and Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12842">http://arxiv.org/abs/2309.12842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tianbo-Pan/SRFNet">https://github.com/Tianbo-Pan/SRFNet</a></li>
<li>paper_authors: Tianbo Pan, Zidong Cao, Lin Wang</li>
<li>for: 本研究旨在提高单目视频中的深度估计精度，以便应用于机器人导航和自动驾驶等场景。</li>
<li>methods: 本研究提出了一种名为SRFNet的新网络模型，包括两个关键技术组件：一是基于注意力的互动式融合模块（AIF），二是可靠性 oriented 深度修正模块（RDR）。AIF模块使用事件和帧的空间偏好作为初始 máscara 来引导多模态特征融合，并通过反馈增强帧和事件特征学习。RDR模块使用融合的特征和 máscara 来估计精度高的深度结构。</li>
<li>results: 本研究在 synthetic 和实际世界数据集上评估了SRFNet的效果，结果显示，无需预训练，SRFNet可以在夜景中比 Priors 等方法更高的性能。<details>
<summary>Abstract</summary>
Monocular depth estimation is a crucial task to measure distance relative to a camera, which is important for applications, such as robot navigation and self-driving. Traditional frame-based methods suffer from performance drops due to the limited dynamic range and motion blur. Therefore, recent works leverage novel event cameras to complement or guide the frame modality via frame-event feature fusion. However, event streams exhibit spatial sparsity, leaving some areas unperceived, especially in regions with marginal light changes. Therefore, direct fusion methods, e.g., RAMNet, often ignore the contribution of the most confident regions of each modality. This leads to structural ambiguity in the modality fusion process, thus degrading the depth estimation performance. In this paper, we propose a novel Spatial Reliability-oriented Fusion Network (SRFNet), that can estimate depth with fine-grained structure at both daytime and nighttime. Our method consists of two key technical components. Firstly, we propose an attention-based interactive fusion (AIF) module that applies spatial priors of events and frames as the initial masks and learns the consensus regions to guide the inter-modal feature fusion. The fused feature are then fed back to enhance the frame and event feature learning. Meanwhile, it utilizes an output head to generate a fused mask, which is iteratively updated for learning consensual spatial priors. Secondly, we propose the Reliability-oriented Depth Refinement (RDR) module to estimate dense depth with the fine-grained structure based on the fused features and masks. We evaluate the effectiveness of our method on the synthetic and real-world datasets, which shows that, even without pretraining, our method outperforms the prior methods, e.g., RAMNet, especially in night scenes. Our project homepage: https://vlislab22.github.io/SRFNet.
</details>
<details>
<summary>摘要</summary>
单目深度估计是一个重要的任务，用于测量相机附近的距离，这对于自动驾驶和机器人定位等应用非常重要。传统的帧基本方法受限于对应数范围和运动模糊的问题，因此latest works将event camera整合或导引帧模式的特性。然而，event流拥有空间罕见性，特别是在光度变化较小的区域，导致direct fusion方法，例如RAMNet，忽略了每个模式的最有信心区域的贡献。这会导致多模式融合过程中的结构混乱，进而下降深度估计性能。在这篇论文中，我们提出了一个名为Spatial Reliability-oriented Fusion Network（SRFNet）的新方法，可以在日间和夜间都 estimate fine-grained的深度结构。我们的方法包括两个关键技术部分。首先，我们提出了一个注意力基于的互动式融合（AIF）模组，它根据事件和帧的空间假设作为初始mask，并学习导引多modal feature融合的共识区域。融合后的特征被反馈以提高帧和事件特征学习。同时，它还使用一个output head生成融合mask，并轮询更新以学习共识的空间假设。其次，我们提出了可靠性对适定深度修正（RDR）模组，用于根据融合特征和mask估计精确的深度结构。我们将这个方法评估在实验和真实世界数据上，结果显示，不需要预训，我们的方法在夜间场景中表现更好，比如RAMNet等先前的方法。更多详细信息可以通过我们的项目首页：<https://vlislab22.github.io/SRFNet>。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptive-Few-Shot-Open-Set-Learning"><a href="#Domain-Adaptive-Few-Shot-Open-Set-Learning" class="headerlink" title="Domain Adaptive Few-Shot Open-Set Learning"></a>Domain Adaptive Few-Shot Open-Set Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12814">http://arxiv.org/abs/2309.12814</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/debabratapal7/dafosnet">https://github.com/debabratapal7/dafosnet</a></li>
<li>paper_authors: Debabrata Pal, Deeptej More, Sai Bhargav, Dipesh Tamboli, Vaneet Aggarwal, Biplab Banerjee</li>
<li>for: 解决目标查询集中未知样本和Visual shift问题，同时可以快速适应新场景。</li>
<li>methods: 提出了一种新的方法called Domain Adaptive Few-Shot Open Set Recognition (DA-FSOS)，并使用了一种名为DAFOSNET的meta-learning-based架构。在训练过程中，模型学习了共享和特异 embedding space，并创建了一个pseudo open-space决策边界。</li>
<li>results: 通过使用一对 conditional adversarial networks和domain-specific batch-normalized class prototypes alignment strategy，模型能够快速适应新场景并提高数据密度。<details>
<summary>Abstract</summary>
Few-shot learning has made impressive strides in addressing the crucial challenges of recognizing unknown samples from novel classes in target query sets and managing visual shifts between domains. However, existing techniques fall short when it comes to identifying target outliers under domain shifts by learning to reject pseudo-outliers from the source domain, resulting in an incomplete solution to both problems. To address these challenges comprehensively, we propose a novel approach called Domain Adaptive Few-Shot Open Set Recognition (DA-FSOS) and introduce a meta-learning-based architecture named DAFOSNET. During training, our model learns a shared and discriminative embedding space while creating a pseudo open-space decision boundary, given a fully-supervised source domain and a label-disjoint few-shot target domain. To enhance data density, we use a pair of conditional adversarial networks with tunable noise variances to augment both domains closed and pseudo-open spaces. Furthermore, we propose a domain-specific batch-normalized class prototypes alignment strategy to align both domains globally while ensuring class-discriminativeness through novel metric objectives. Our training approach ensures that DAFOS-NET can generalize well to new scenarios in the target domain. We present three benchmarks for DA-FSOS based on the Office-Home, mini-ImageNet/CUB, and DomainNet datasets and demonstrate the efficacy of DAFOS-NET through extensive experimentation
</details>
<details>
<summary>摘要</summary>
几个步学习已经在面临未知样本从新类目标查询集中识别难题和处理视觉变化 между领域方面做出了很好的进展。然而，现有技术在确定目标异常点下存在缺陷，即通过学习源领域中的 pseudo-outlier 来拒绝，导致解决这两个问题的答案不完整。为了全面解决这些挑战，我们提出了一种新的方法calledDomain Adaptive Few-Shot Open Set Recognition（DA-FSOS），并介绍了一种基于meta-学习的架构 named DAFOSNET。在训练过程中，我们的模型学习了共享和特异的嵌入空间，同时创建了一个 Pseudo open-space 决策边界，使用完全supervised的源领域和一个标签分离的少量目标领域。为了增强数据密度，我们使用了一对 conditional adversarial networks  WITH tunable noise variances 来扩展两个领域的closed和pseudo-open空间。此外，我们提出了一种适应域特化的batch normalized class prototypes alignment策略，用于对两个领域进行全球协调，并确保类别特异性通过新的度量目标。我们的训练方法确保了 DAFOS-NET 在新enario中能够通过��������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������
</details></li>
</ul>
<hr>
<h2 id="Automatic-view-plane-prescription-for-cardiac-magnetic-resonance-imaging-via-supervision-by-spatial-relationship-between-views"><a href="#Automatic-view-plane-prescription-for-cardiac-magnetic-resonance-imaging-via-supervision-by-spatial-relationship-between-views" class="headerlink" title="Automatic view plane prescription for cardiac magnetic resonance imaging via supervision by spatial relationship between views"></a>Automatic view plane prescription for cardiac magnetic resonance imaging via supervision by spatial relationship between views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12805">http://arxiv.org/abs/2309.12805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wd111624/cmr_plan">https://github.com/wd111624/cmr_plan</a></li>
<li>paper_authors: Dong Wei, Yawen Huang, Donghuan Lu, Yuexiang Li, Yefeng Zheng<br>for: 这种系统的目的是自动化卡路里MR图像的规划，以帮助临床实践中的医生和技术人员更加快速和准确地完成图像规划。methods: 该系统使用了深度学习网络，通过挖掘数据中的空间关系来自动地确定目标平面和源视图之间的交叉线，并通过堆栈锥体网络来逐步提高回归。此外，该系统还使用了多视图规划策略，将所有源视图中的预测热图聚合以获得全球最优的规划。results: 实验结果显示，该系统可以准确地预测四个标准的卡路里MR图像平面，并且比现有的方法更加精准，包括传统的Atlas-based和 newer deep-learning-based方法。此外，该系统还可以预测第一个Cardiac-anatomy-oriented平面（或多个平面），从body-oriented扫描中获得。<details>
<summary>Abstract</summary>
Background: View planning for the acquisition of cardiac magnetic resonance (CMR) imaging remains a demanding task in clinical practice. Purpose: Existing approaches to its automation relied either on an additional volumetric image not typically acquired in clinic routine, or on laborious manual annotations of cardiac structural landmarks. This work presents a clinic-compatible, annotation-free system for automatic CMR view planning. Methods: The system mines the spatial relationship, more specifically, locates the intersecting lines, between the target planes and source views, and trains deep networks to regress heatmaps defined by distances from the intersecting lines. The intersection lines are the prescription lines prescribed by the technologists at the time of image acquisition using cardiac landmarks, and retrospectively identified from the spatial relationship. As the spatial relationship is self-contained in properly stored data, the need for additional manual annotation is eliminated. In addition, the interplay of multiple target planes predicted in a source view is utilized in a stacked hourglass architecture to gradually improve the regression. Then, a multi-view planning strategy is proposed to aggregate information from the predicted heatmaps for all the source views of a target plane, for a globally optimal prescription, mimicking the similar strategy practiced by skilled human prescribers. Results: The experiments include 181 CMR exams. Our system yields the mean angular difference and point-to-plane distance of 5.68 degrees and 3.12 mm, respectively. It not only achieves superior accuracy to existing approaches including conventional atlas-based and newer deep-learning-based in prescribing the four standard CMR planes but also demonstrates prescription of the first cardiac-anatomy-oriented plane(s) from the body-oriented scout.
</details>
<details>
<summary>摘要</summary>
背景：卡路里变 imagine（CMR）成像取得的规划仍然是艰辛的任务在临床实践中。目的：现有的自动化方法都是基于不常见的三维图像或劳累的手动标注卡ди亚Structural landmarks。这个工作提出了一个可以在临床实践中使用的无需标注的自动CMR规划系统。方法：系统利用目标平面和源视图之间的空间关系，具体来说是找出目标平面和源视图之间的交叉点，并使用深度网络来回归定距离 définition heatmaps。交叉点是由技术人员在图像取得时使用卡ди亚Structural landmarks预scribed的规则，并在后续从空间关系中回拟。由于空间关系自身含有所需的信息，因此无需额外的手动标注。此外，系统还利用多个目标平面预测的多个源视图之间的互动，在堆栈ourglass架构中进行渐进改进。然后，提议一种多视图规划策略，将所有源视图中的预测热图集成，以实现全局优化的规划，类似于人类资深决策者的做法。结果：实验包括181个CMR试验。我们的系统的平均角度差和点到平面距离为5.68度和3.12mm。不仅达到了现有方法的精度，还可以成功地预scribed四个标准CMR平面，以及首次预scribedBody-oriented scout中的cardiac-anatomy-oriented平面。
</details></li>
</ul>
<hr>
<h2 id="Scalable-Semantic-3D-Mapping-of-Coral-Reefs-with-Deep-Learning"><a href="#Scalable-Semantic-3D-Mapping-of-Coral-Reefs-with-Deep-Learning" class="headerlink" title="Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning"></a>Scalable Semantic 3D Mapping of Coral Reefs with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12804">http://arxiv.org/abs/2309.12804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Sauder, Guilhem Banc-Prandi, Anders Meibom, Devis Tuia</li>
<li>for: This paper aims to develop a new method for mapping underwater environments from ego-motion video, with a focus on coral reef monitoring.</li>
<li>methods: The method uses machine learning to adapt to challenging underwater conditions and combines 3D mapping with semantic segmentation of images.</li>
<li>results: The method achieves high-precision 3D semantic mapping at unprecedented scale with significantly reduced labor costs, making it possible to monitor coral reefs more efficiently and effectively.Here’s the full text in Simplified Chinese:</li>
<li>for: 这篇论文目的是开发一种基于ego-motion视频的海洋环境地图方法，主要关注珊瑚礁监测。</li>
<li>methods: 该方法使用机器学习适应海洋下难以控制的环境，并将3D地图与图像Semantic分割相结合。</li>
<li>results: 该方法实现了高精度3DSemantic地图，并在减少劳动成本方面取得了显著进步，使得珊瑚礁监测更加高效和可靠。<details>
<summary>Abstract</summary>
Coral reefs are among the most diverse ecosystems on our planet, and are depended on by hundreds of millions of people. Unfortunately, most coral reefs are existentially threatened by global climate change and local anthropogenic pressures. To better understand the dynamics underlying deterioration of reefs, monitoring at high spatial and temporal resolution is key. However, conventional monitoring methods for quantifying coral cover and species abundance are limited in scale due to the extensive manual labor required. Although computer vision tools have been employed to aid in this process, in particular SfM photogrammetry for 3D mapping and deep neural networks for image segmentation, analysis of the data products creates a bottleneck, effectively limiting their scalability. This paper presents a new paradigm for mapping underwater environments from ego-motion video, unifying 3D mapping systems that use machine learning to adapt to challenging conditions under water, combined with a modern approach for semantic segmentation of images. The method is exemplified on coral reefs in the northern Gulf of Aqaba, Red Sea, demonstrating high-precision 3D semantic mapping at unprecedented scale with significantly reduced required labor costs: a 100 m video transect acquired within 5 minutes of diving with a cheap consumer-grade camera can be fully automatically analyzed within 5 minutes. Our approach significantly scales up coral reef monitoring by taking a leap towards fully automatic analysis of video transects. The method democratizes coral reef transects by reducing the labor, equipment, logistics, and computing cost. This can help to inform conservation policies more efficiently. The underlying computational method of learning-based Structure-from-Motion has broad implications for fast low-cost mapping of underwater environments other than coral reefs.
</details>
<details>
<summary>摘要</summary>
珊瑚礁是地球上最多样化的生态系统之一，并且有百万人的生存受其影响。然而，大多数珊瑚礁面临全球气候变化和地方人类活动的威胁。为了更好地理解珊瑚礁的衰退机制，高精度空间和时间分辨率的监测是关键。although computer vision工具已经被应用于这一过程，特别是使用SfM摄ogrammetry для3D地图和深度神经网络 для图像分割，但是分析数据产品创造了瓶颈，从而限制了其扩展性。这篇文章介绍了一种新的珊瑚礁监测方法，基于自己的运动来自视频，结合机器学习来适应水下挑战的3D地图系统，并与现代图像分割方法相结合。这种方法在北红海的珊瑚礁中进行了高精度3Dsemantic地图，覆盖100米视频 transect，只需5分钟投入和分析时间。我们的方法可以快速扩大珊瑚礁监测，减少劳动、设备、运输和计算成本，从而更有效地 Inform conservation policies。我们的方法可以把珊瑚礁 transect democratized，减少劳动和设备成本，以便更多的人可以参与监测和保护。这种方法的计算方法，基于学习的Structure-from-Motion，对于快速低成本地图的水下环境的应用有广泛的应用前景。
</details></li>
</ul>
<hr>
<h2 id="NOC-High-Quality-Neural-Object-Cloning-with-3D-Lifting-of-Segment-Anything"><a href="#NOC-High-Quality-Neural-Object-Cloning-with-3D-Lifting-of-Segment-Anything" class="headerlink" title="NOC: High-Quality Neural Object Cloning with 3D Lifting of Segment Anything"></a>NOC: High-Quality Neural Object Cloning with 3D Lifting of Segment Anything</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12790">http://arxiv.org/abs/2309.12790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobao Wei, Renrui Zhang, Jiarui Wu, Jiaming Liu, Ming Lu, Yandong Guo, Shanghang Zhang</li>
<li>for: 本研究旨在提出一种基于神经场的高品质3D对象重建方法，以便在用户指定的实时下重建目标对象。</li>
<li>methods: 本方法基于神经场和Segment Anything Model (SAM)的优点，首先将多视图2D分割Masks lifted到3D变化场中，然后将2D特征 lifted到3D SAM场中以提高重建质量。</li>
<li>results: 在多个 benchmark 数据集上进行了详细的实验，表明本方法能够提供高品质的目标对象重建结果。<details>
<summary>Abstract</summary>
With the development of the neural field, reconstructing the 3D model of a target object from multi-view inputs has recently attracted increasing attention from the community. Existing methods normally learn a neural field for the whole scene, while it is still under-explored how to reconstruct a certain object indicated by users on-the-fly. Considering the Segment Anything Model (SAM) has shown effectiveness in segmenting any 2D images, in this paper, we propose Neural Object Cloning (NOC), a novel high-quality 3D object reconstruction method, which leverages the benefits of both neural field and SAM from two aspects. Firstly, to separate the target object from the scene, we propose a novel strategy to lift the multi-view 2D segmentation masks of SAM into a unified 3D variation field. The 3D variation field is then projected into 2D space and generates the new prompts for SAM. This process is iterative until convergence to separate the target object from the scene. Then, apart from 2D masks, we further lift the 2D features of the SAM encoder into a 3D SAM field in order to improve the reconstruction quality of the target object. NOC lifts the 2D masks and features of SAM into the 3D neural field for high-quality target object reconstruction. We conduct detailed experiments on several benchmark datasets to demonstrate the advantages of our method. The code will be released.
</details>
<details>
<summary>摘要</summary>
随着神经场的发展，从多视图输入中重建目标对象的3D模型已经吸引了社区的越来越多的关注。现有方法通常学习一个整个场景的神经场，而即时根据用户指定的对象进行重建仍然是一个未探索的领域。尝试 Segment Anything Model (SAM) 可以效果地 segment any 2D 图像，在这篇论文中，我们提出了一种新的高质量3D对象重建方法——神经对象复制（NOC），该方法利用了神经场和 SAM 的优点，从两个方面进行了利用。首先，为了将目标对象从场景中分离，我们提出了一种新的策略，即将多视图2D 分割面的 SAM 编码器输出 lift 到一个统一的3D 变化场，然后将这个3D 变化场 проек 到2D 空间，生成新的提示，这个过程是迭代的，直到达到对象分离的 converges。然后，除了2D 面，我们还 lift 了 SAM 编码器的2D 特征到3D SAM 场，以提高目标对象的重建质量。NOC 将 SAM 的2D 面和特征 lift 到神经场中，以实现高质量的目标对象重建。我们对多个标准数据集进行了详细的实验，以示出我们的方法的优势。代码将会发布。
</details></li>
</ul>
<hr>
<h2 id="EMS-3D-Eyebrow-Modeling-from-Single-view-Images"><a href="#EMS-3D-Eyebrow-Modeling-from-Single-view-Images" class="headerlink" title="EMS: 3D Eyebrow Modeling from Single-view Images"></a>EMS: 3D Eyebrow Modeling from Single-view Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12787">http://arxiv.org/abs/2309.12787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghong Li, Leyang Jin, Yujian Zheng, Yizhou Yu, Xiaoguang Han</li>
<li>for: 这个论文的目的是提出一种基于学习的方法来实现单视图3D眉毛重建。</li>
<li>methods: 这个方法使用了三个模块：RootFinder、OriPredictor和FiberEnder。RootFinder用于Localizing fiber root positions，OriPredictor用于预测3D空间中的方向场，FiberEnder用于确定每个纤维的长度。</li>
<li>results: 该方法在不同的眉毛样式和长度上表现了效果，并且可以处理部分受阻的根位置问题。<details>
<summary>Abstract</summary>
Eyebrows play a critical role in facial expression and appearance. Although the 3D digitization of faces is well explored, less attention has been drawn to 3D eyebrow modeling. In this work, we propose EMS, the first learning-based framework for single-view 3D eyebrow reconstruction. Following the methods of scalp hair reconstruction, we also represent the eyebrow as a set of fiber curves and convert the reconstruction to fibers growing problem. Three modules are then carefully designed: RootFinder firstly localizes the fiber root positions which indicates where to grow; OriPredictor predicts an orientation field in the 3D space to guide the growing of fibers; FiberEnder is designed to determine when to stop the growth of each fiber. Our OriPredictor is directly borrowing the method used in hair reconstruction. Considering the differences between hair and eyebrows, both RootFinder and FiberEnder are newly proposed. Specifically, to cope with the challenge that the root location is severely occluded, we formulate root localization as a density map estimation task. Given the predicted density map, a density-based clustering method is further used for finding the roots. For each fiber, the growth starts from the root point and moves step by step until the ending, where each step is defined as an oriented line with a constant length according to the predicted orientation field. To determine when to end, a pixel-aligned RNN architecture is designed to form a binary classifier, which outputs stop or not for each growing step. To support the training of all proposed networks, we build the first 3D synthetic eyebrow dataset that contains 400 high-quality eyebrow models manually created by artists. Extensive experiments have demonstrated the effectiveness of the proposed EMS pipeline on a variety of different eyebrow styles and lengths, ranging from short and sparse to long bushy eyebrows.
</details>
<details>
<summary>摘要</summary>
眉毛在面部表情和外貌中扮演了关键角色，尽管3D人脸数字化已得到了广泛的研究，但3D眉毛模型化却受到了较少的关注。在这项工作中，我们提出了EMS框架，是首个基于学习的单视角3D眉毛重建框架。我们将眉毛表示为一组纤维曲线，并将重建转化为纤维增长问题。为确定眉毛的长度和形状，我们设计了三个模块：RootFinder、OriPredictor和FiberEnder。RootFinder先地本地化眉毛根部位置，以便于纤维增长；OriPredictor预测了3D空间中纤维的方向场，以帮助纤维增长；FiberEnder用于确定每个纤维的增长结束点。我们的OriPredictor直接借鉴了毛发重建中使用的方法。由于眉毛和毛发之间存在差异，因此RootFinder和FiberEnder均需要新的设计。具体来说，为了处理眉毛根部位置严重受遮挡的挑战，我们将根部地图估计任务转化为一个density map估计任务。给出预测的density map，然后使用密度基于的划分方法来找到根部。对于每个纤维，增长从根部开始，每步长度为constanteorientation field的oriented line。直到结束，每个步骤都需要通过一个像素对齐的RNN架构来判断是否需要停止增长。为支持所提出的网络的训练，我们建立了首个3D人工眉毛数据集，该数据集包含400个高质量眉毛模型，由艺术家手工创建。广泛的实验证明了我们提出的EMS管道的效果，可以处理不同的眉毛风格和长度，从短毛到长毛。
</details></li>
</ul>
<hr>
<h2 id="LMC-Large-Model-Collaboration-with-Cross-assessment-for-Training-Free-Open-Set-Object-Recognition"><a href="#LMC-Large-Model-Collaboration-with-Cross-assessment-for-Training-Free-Open-Set-Object-Recognition" class="headerlink" title="LMC: Large Model Collaboration with Cross-assessment for Training-Free Open-Set Object Recognition"></a>LMC: Large Model Collaboration with Cross-assessment for Training-Free Open-Set Object Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12780">http://arxiv.org/abs/2309.12780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harryqu123/lmc">https://github.com/harryqu123/lmc</a></li>
<li>paper_authors: Haoxuan Qu, Xiaofei Hui, Yujun Cai, Jun Liu</li>
<li>for: 这 paper 的目的是如何精确地进行开放集object recognition，以减少依赖伪阳特征。</li>
<li>methods: 本 paper 提出了一个名为 Large Model Collaboration (LMC) 的新框架，通过多个 off-the-shelf 大型模型的协力来解决这个问题。此外，paper 还提出了多个新的设计来有效地从大型模型中提取隐藏知识。</li>
<li>results: 实验结果显示了我们提出的框架的有效性。可以在 <a target="_blank" rel="noopener" href="https://github.com/Harryqu123/LMC">https://github.com/Harryqu123/LMC</a> 获取代码。<details>
<summary>Abstract</summary>
Open-set object recognition aims to identify if an object is from a class that has been encountered during training or not. To perform open-set object recognition accurately, a key challenge is how to reduce the reliance on spurious-discriminative features. In this paper, motivated by that different large models pre-trained through different paradigms can possess very rich while distinct implicit knowledge, we propose a novel framework named Large Model Collaboration (LMC) to tackle the above challenge via collaborating different off-the-shelf large models in a training-free manner. Moreover, we also incorporate the proposed framework with several novel designs to effectively extract implicit knowledge from large models. Extensive experiments demonstrate the efficacy of our proposed framework. Code is available https://github.com/Harryqu123/LMC
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="WiCV-CVPR2023-The-Eleventh-Women-In-Computer-Vision-Workshop-at-the-Annual-CVPR-Conference"><a href="#WiCV-CVPR2023-The-Eleventh-Women-In-Computer-Vision-Workshop-at-the-Annual-CVPR-Conference" class="headerlink" title="WiCV@CVPR2023: The Eleventh Women In Computer Vision Workshop at the Annual CVPR Conference"></a>WiCV@CVPR2023: The Eleventh Women In Computer Vision Workshop at the Annual CVPR Conference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12768">http://arxiv.org/abs/2309.12768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doris Antensteiner, Marah Halawa, Asra Aslam, Ivaxi Sheth, Sachini Herath, Ziqi Huang, Sunnie S. Y. Kim, Aparna Akula, Xin Wang</li>
<li>for: The paper is written to present the details of the Women in Computer Vision Workshop - WiCV 2023, which aims to amplify the voices of underrepresented women in the computer vision community.</li>
<li>methods: The paper uses a comprehensive report on the workshop program, historical trends from past WiCV@CVPR events, and a summary of statistics related to presenters, attendees, and sponsorship for the WiCV 2023 workshop.</li>
<li>results: The paper presents a detailed report on the WiCV 2023 workshop, including the program, historical trends, and statistics related to presenters, attendees, and sponsorship. The paper also highlights the importance of such events in addressing gender imbalances within the field of computer vision.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍女性计算机视觉工作坊（WiCV 2023）的详细信息。WiCV 的目标是促进计算机视觉领域中少数女性的声音，并且推动该领域的多样性和平等。</li>
<li>methods: 这篇论文使用了 WiCV 2023 工作坊的全面报告，以及过去 WiCV@CVPR 事件的历史趋势和统计数据，以描述 WiCV 2023 工作坊的进程和成果。</li>
<li>results: 这篇论文提供了 WiCV 2023 工作坊的详细报告，包括工作坊的程序、历史趋势和统计数据，以及参与者和赞助商的相关信息。论文还强调了计算机视觉领域内的性别不平衡问题，并认为这类活动对于解决这一问题具有重要意义。<details>
<summary>Abstract</summary>
In this paper, we present the details of Women in Computer Vision Workshop - WiCV 2023, organized alongside the hybrid CVPR 2023 in Vancouver, Canada. WiCV aims to amplify the voices of underrepresented women in the computer vision community, fostering increased visibility in both academia and industry. We believe that such events play a vital role in addressing gender imbalances within the field. The annual WiCV@CVPR workshop offers a) opportunity for collaboration between researchers from minority groups, b) mentorship for female junior researchers, c) financial support to presenters to alleviate finanacial burdens and d) a diverse array of role models who can inspire younger researchers at the outset of their careers. In this paper, we present a comprehensive report on the workshop program, historical trends from the past WiCV@CVPR events, and a summary of statistics related to presenters, attendees, and sponsorship for the WiCV 2023 workshop.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了2023年度的女性计算机视觉工作坊（WiCV 2023），该活动与CVPR 2023合办于加拿大温尼伯市。WiCV 的目标是扩大计算机视觉领域下的弱调女性人群的声音，提高学术和产业领域中的女性 visibility。我们认为这些活动对于解决计算机视觉领域中的性别偏见非常重要。每年的 WiCV@CVPR 工作坊提供了以下机会：（a）少数民族研究者之间的合作，（b）为女性新手研究者提供导师，（c）为参会者提供论文发表支持，以及（d）多样化的角色模范，以激励年轻研究者在职业开始时。在本文中，我们提供了2023年 WiCV@CVPR 工作坊的工作计划、过去事件的历史趋势以及2023年工作坊的统计数据。
</details></li>
</ul>
<hr>
<h2 id="S3TC-Spiking-Separated-Spatial-and-Temporal-Convolutions-with-Unsupervised-STDP-based-Learning-for-Action-Recognition"><a href="#S3TC-Spiking-Separated-Spatial-and-Temporal-Convolutions-with-Unsupervised-STDP-based-Learning-for-Action-Recognition" class="headerlink" title="S3TC: Spiking Separated Spatial and Temporal Convolutions with Unsupervised STDP-based Learning for Action Recognition"></a>S3TC: Spiking Separated Spatial and Temporal Convolutions with Unsupervised STDP-based Learning for Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12761">http://arxiv.org/abs/2309.12761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mireille El-Assal, Pierre Tirilly, Ioan Marius Bilasco<br>for:This paper focuses on developing a more efficient video analysis method using Spiking Neural Networks (SNNs) and Spiking Separated Spatial and Temporal Convolutions (S3TCs).methods:The authors use unsupervised learning with the Spike Timing-Dependent Plasticity (STDP) rule and introduce S3TCs to reduce the number of parameters required for video analysis.results:The proposed method successfully extracts spatio-temporal information from videos, increases the output spiking activity, and outperforms spiking 3D convolutions on the KTH, Weizmann, and IXMAS datasets.Here is the answer in Simplified Chinese text:for: 这篇论文关注开发更高效的视频分析方法，使用神经网络和分离空间和时间卷积（S3TCs）。methods: 作者使用无监督学习和脉冲时间依赖性变化（STDP）规则，并引入S3TCs来降低视频分析所需的参数数量。results: 提议的方法在KTH、Weizmann和IXMAS数据集上成功提取空间-时间信息，提高输出脉冲活动，并超越了脉冲3D卷积。<details>
<summary>Abstract</summary>
Video analysis is a major computer vision task that has received a lot of attention in recent years. The current state-of-the-art performance for video analysis is achieved with Deep Neural Networks (DNNs) that have high computational costs and need large amounts of labeled data for training. Spiking Neural Networks (SNNs) have significantly lower computational costs (thousands of times) than regular non-spiking networks when implemented on neuromorphic hardware. They have been used for video analysis with methods like 3D Convolutional Spiking Neural Networks (3D CSNNs). However, these networks have a significantly larger number of parameters compared with spiking 2D CSNN. This, not only increases the computational costs, but also makes these networks more difficult to implement with neuromorphic hardware. In this work, we use CSNNs trained in an unsupervised manner with the Spike Timing-Dependent Plasticity (STDP) rule, and we introduce, for the first time, Spiking Separated Spatial and Temporal Convolutions (S3TCs) for the sake of reducing the number of parameters required for video analysis. This unsupervised learning has the advantage of not needing large amounts of labeled data for training. Factorizing a single spatio-temporal spiking convolution into a spatial and a temporal spiking convolution decreases the number of parameters of the network. We test our network with the KTH, Weizmann, and IXMAS datasets, and we show that S3TCs successfully extract spatio-temporal information from videos, while increasing the output spiking activity, and outperforming spiking 3D convolutions.
</details>
<details>
<summary>摘要</summary>
视频分析是计算机视觉中的一项重要任务，在过去几年内受到了很多关注。当前的状态艺术性表现在视频分析方面是通过深度神经网络（DNNs）实现的，其计算成本较高，需要大量标注数据进行训练。神经元网络（SNNs）在神经模拟硬件上实现时有 thousands 万次更低的计算成本，但它们的参数数量相对较多，使得它们更难于实现。在这项工作中，我们使用 CSNNs 在无监督的方式进行训练，并 introduce 一种新的 Spiking Separated Spatial and Temporal Convolutions（S3TCs），以降低视频分析所需的参数数量。这种无监督学习具有不需要大量标注数据进行训练的优点。将一个综合空间时间射阻挡分解成空间射阻挡和时间射阻挡，可以降低网络的参数数量。我们在 KTH、Weizmann 和 IXMAS 数据集上测试了我们的网络，并显示了 S3TCs 成功地从视频中提取空间时间信息，提高输出脉冲活动，并超过了射阻挡三维 convolution。
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-Image-Compression-with-Variable-Image-Quality-Objectives"><a href="#Transformer-based-Image-Compression-with-Variable-Image-Quality-Objectives" class="headerlink" title="Transformer-based Image Compression with Variable Image Quality Objectives"></a>Transformer-based Image Compression with Variable Image Quality Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12717">http://arxiv.org/abs/2309.12717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chia-Hao Kao, Yi-Hsin Chen, Cheng Chien, Wei-Chen Chiu, Wen-Hsiao Peng</li>
<li>for: 该 paper 是为了提供一种可变图像质量目标的 transformer 基于压缩系统，以满足用户的偏好。</li>
<li>methods: 该方法使用 learned codec 进行优化，以实现不同质量目标下的图像重建。用户可以通过单一的模型来选择一个质量目标的交易off。</li>
<li>results: 该方法可以通过使用 prompt tokens 来condition transformer 基于 autoencoder，并通过学习 prompt generation network 来生成适应用户偏好和输入图像的 prompt tokens。对于常见的质量指标，广泛的实验表明该方法可以适应不同的质量目标，并且与单一质量目标方法相比，其表现相对较好。<details>
<summary>Abstract</summary>
This paper presents a Transformer-based image compression system that allows for a variable image quality objective according to the user's preference. Optimizing a learned codec for different quality objectives leads to reconstructed images with varying visual characteristics. Our method provides the user with the flexibility to choose a trade-off between two image quality objectives using a single, shared model. Motivated by the success of prompt-tuning techniques, we introduce prompt tokens to condition our Transformer-based autoencoder. These prompt tokens are generated adaptively based on the user's preference and input image through learning a prompt generation network. Extensive experiments on commonly used quality metrics demonstrate the effectiveness of our method in adapting the encoding and/or decoding processes to a variable quality objective. While offering the additional flexibility, our proposed method performs comparably to the single-objective methods in terms of rate-distortion performance.
</details>
<details>
<summary>摘要</summary>
Inspired by the success of prompt-tuning techniques, the system introduces prompt tokens to condition the Transformer-based autoencoder. These prompt tokens are generated adaptively based on the user's preference and input image through learning a prompt generation network. Extensive experiments on commonly used quality metrics demonstrate the effectiveness of the method in adapting the encoding and/or decoding processes to a variable quality objective. Notably, the proposed method offers the additional flexibility while performing comparably to single-objective methods in terms of rate-distortion performance.
</details></li>
</ul>
<hr>
<h2 id="mixed-attention-auto-encoder-for-multi-class-industrial-anomaly-detection"><a href="#mixed-attention-auto-encoder-for-multi-class-industrial-anomaly-detection" class="headerlink" title="mixed attention auto encoder for multi-class industrial anomaly detection"></a>mixed attention auto encoder for multi-class industrial anomaly detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12700">http://arxiv.org/abs/2309.12700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiangqi Liu, Feng Wang</li>
<li>for: 本研究旨在提出一种可以实现多类异常检测的单一模型，以解决现有方法的高存储成本和训练效率低下问题。</li>
<li>methods: 该方法使用混合注意力自适应Encoder（MAAE），并采用空间注意力和通道注意力来有效地捕捉多类特征分布的全球category信息，以及模型多个类别特征分布的模型。此外，该方法还提出了适应噪声生成器和多尺度融合模块，以适应实际噪声和保持不同类别物体表面 semantics。</li>
<li>results: MAAE在 benchmark 数据集上达到了比state-of-the-art 方法更高的性能。<details>
<summary>Abstract</summary>
Most existing methods for unsupervised industrial anomaly detection train a separate model for each object category. This kind of approach can easily capture the category-specific feature distributions, but results in high storage cost and low training efficiency. In this paper, we propose a unified mixed-attention auto encoder (MAAE) to implement multi-class anomaly detection with a single model. To alleviate the performance degradation due to the diverse distribution patterns of different categories, we employ spatial attentions and channel attentions to effectively capture the global category information and model the feature distributions of multiple classes. Furthermore, to simulate the realistic noises on features and preserve the surface semantics of objects from different categories which are essential for detecting the subtle anomalies, we propose an adaptive noise generator and a multi-scale fusion module for the pre-trained features. MAAE delivers remarkable performances on the benchmark dataset compared with the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
现有的方法 для无监督工业异常检测通常将每个物件类别 trains一个分开的模型。这种方法可以轻松地捕捉每个类别的特征分布，但会导致存储成本高且训练效率低。在这篇论文中，我们提出了一个整合式混合注意力自动编码器（MAAE），以实现多类别异常检测的单一模型。为了解决不同类别的特征分布多样性导致性能下降的问题，我们使用空间注意力和通道注意力来有效地捕捉全类别信息和多类别特征分布。其次，为了模拟实际上的噪声和保持不同类别物件表面 semantics，我们提出了适应式噪声生成器和多尺度融合模组。MAAE在比较 dataset 上 delivert 了非常出色的性能，与当前方法相比。
</details></li>
</ul>
<hr>
<h2 id="eWand-A-calibration-framework-for-wide-baseline-frame-based-and-event-based-camera-systems"><a href="#eWand-A-calibration-framework-for-wide-baseline-frame-based-and-event-based-camera-systems" class="headerlink" title="eWand: A calibration framework for wide baseline frame-based and event-based camera systems"></a>eWand: A calibration framework for wide baseline frame-based and event-based camera systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12685">http://arxiv.org/abs/2309.12685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Gossard, Andreas Ziegler, Levin Kolmar, Jonas Tebbe, Andreas Zell</li>
<li>for: 用于高精度对象位置三角推算的精准准确calibration</li>
<li>methods: 使用闪烁LED闪烁在透明球体内，代替传统的印刷或显示 Pattern</li>
<li>results: 提供高精度、易于使用的多摄像头外部坐标calibration方法，适用于frame-和事件基camera<details>
<summary>Abstract</summary>
Accurate calibration is crucial for using multiple cameras to triangulate the position of objects precisely. However, it is also a time-consuming process that needs to be repeated for every displacement of the cameras. The standard approach is to use a printed pattern with known geometry to estimate the intrinsic and extrinsic parameters of the cameras. The same idea can be applied to event-based cameras, though it requires extra work. By using frame reconstruction from events, a printed pattern can be detected. A blinking pattern can also be displayed on a screen. Then, the pattern can be directly detected from the events. Such calibration methods can provide accurate intrinsic calibration for both frame- and event-based cameras. However, using 2D patterns has several limitations for multi-camera extrinsic calibration, with cameras possessing highly different points of view and a wide baseline. The 2D pattern can only be detected from one direction and needs to be of significant size to compensate for its distance to the camera. This makes the extrinsic calibration time-consuming and cumbersome. To overcome these limitations, we propose eWand, a new method that uses blinking LEDs inside opaque spheres instead of a printed or displayed pattern. Our method provides a faster, easier-to-use extrinsic calibration approach that maintains high accuracy for both event- and frame-based cameras.
</details>
<details>
<summary>摘要</summary>
准确的均衡是多摄像头三角测量物体位置精准的关键。然而，这也是一项时间消耗的过程，需要每次移动摄像头时重新进行。标准方法是使用印刷的模式来估算摄像头的内参和外参参数。这种方法可以应用于事件基图像，尽管需要额外的工作。通过从事件中重建幻象，可以直接检测印刷的模式。然后，可以使用幻象的检测来提供内参均衡。但是，使用2D模式有多个相机的外参均衡的限制，因为相机具有不同的视角和广泛的基线。2D模式只能从一个方向检测，需要很大的尺寸来补偿其与摄像头的距离。这使得外参均衡变得时间消耗和困难。为了缓解这些限制，我们提出了ewand，一种新的方法，使用LED灯光在透明球体中闪烁。我们的方法提供了一种更快、更容易使用的外参均衡方法，可以保持高精度 для both事件基图像和帧基图像。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Modality-Specific-Features-For-Multi-Modal-Manipulation-Detection-And-Grounding"><a href="#Exploiting-Modality-Specific-Features-For-Multi-Modal-Manipulation-Detection-And-Grounding" class="headerlink" title="Exploiting Modality-Specific Features For Multi-Modal Manipulation Detection And Grounding"></a>Exploiting Modality-Specific Features For Multi-Modal Manipulation Detection And Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12657">http://arxiv.org/abs/2309.12657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhen Wang, Bin Liu, Changtao Miao, Zhiwei Zhao, Wanyi Zhuang, Qi Chu, Nenghai Yu</li>
<li>for: 本研究旨在提出一种简单且有效的 transformer 基本框架，用于检测和稳定多模态束缚 manipulation。</li>
<li>methods: 我们首先构造了视 language 预训练Encoder，并使用 dual-branch cross-attention (DCA) 技术来抽取和融合模态特有的特征。此外，我们还设计了分离精细类ifier (DFC)，以提高模态特有的特征挖掘和避免模态竞争。</li>
<li>results: 我们在 $\rm DGM^4$ 数据集上进行了广泛的实验，并证明了我们提出的模型在对 state-of-the-art 方法的比较中表现出色。<details>
<summary>Abstract</summary>
AI-synthesized text and images have gained significant attention, particularly due to the widespread dissemination of multi-modal manipulations on the internet, which has resulted in numerous negative impacts on society. Existing methods for multi-modal manipulation detection and grounding primarily focus on fusing vision-language features to make predictions, while overlooking the importance of modality-specific features, leading to sub-optimal results. In this paper, we construct a simple and novel transformer-based framework for multi-modal manipulation detection and grounding tasks. Our framework simultaneously explores modality-specific features while preserving the capability for multi-modal alignment. To achieve this, we introduce visual/language pre-trained encoders and dual-branch cross-attention (DCA) to extract and fuse modality-unique features. Furthermore, we design decoupled fine-grained classifiers (DFC) to enhance modality-specific feature mining and mitigate modality competition. Moreover, we propose an implicit manipulation query (IMQ) that adaptively aggregates global contextual cues within each modality using learnable queries, thereby improving the discovery of forged details. Extensive experiments on the $\rm DGM^4$ dataset demonstrate the superior performance of our proposed model compared to state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
人工智能生成的文本和图像在互联网上广泛传播，尤其是多模态杂化的 manipulate 问题，导致社会受到了负面影响。现有的多模态杂化检测和定位方法主要是通过视觉语言特征的融合来进行预测，而忽略了特定模式特征的重要性，从而导致低效的结果。在这篇论文中，我们提出了一种简单而新的 transformer 基于的多模态杂化检测和定位框架。我们的框架同时探索特定模式的特征，保留多模态对齐的能力。为此，我们引入视觉语言预训练encoder和双支分支交叉注意力（DCA），以EXTRACT和融合特定模式的特征。此外，我们设计了独立细致分类器（DFC），以提高特定模式的特征挖掘和避免模式竞争。此外，我们还提出了适应性 manipulate 查询（IMQ），可以在每个模式中适应学习查询，以提高对伪造细节的发现。我们对 $\rm DGM^4$ 数据集进行了广泛的实验，并证明了我们的提出的模型在对state-of-the-art方法的比较中表现出色。
</details></li>
</ul>
<hr>
<h2 id="FP-PET-Large-Model-Multiple-Loss-And-Focused-Practice"><a href="#FP-PET-Large-Model-Multiple-Loss-And-Focused-Practice" class="headerlink" title="FP-PET: Large Model, Multiple Loss And Focused Practice"></a>FP-PET: Large Model, Multiple Loss And Focused Practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12650">http://arxiv.org/abs/2309.12650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixin Chen, Ourui Fu, Wenrui Shao, Zhaoheng Xie</li>
<li>for: 这项研究旨在提出FP-PET方法，用于医学图像分割，尤其是CT和PET图像。</li>
<li>methods: 该研究使用了多种机器学习模型，包括STUNet-large、SwinUNETR和VNet，以实现最新的分割性能。</li>
<li>results: 研究提出了一个综合评价指标，将多个评价指标（如 dice分数、false positive volume 和 false negative volume）加权平均，以提供全面的模型效果评价。<details>
<summary>Abstract</summary>
This study presents FP-PET, a comprehensive approach to medical image segmentation with a focus on CT and PET images. Utilizing a dataset from the AutoPet2023 Challenge, the research employs a variety of machine learning models, including STUNet-large, SwinUNETR, and VNet, to achieve state-of-the-art segmentation performance. The paper introduces an aggregated score that combines multiple evaluation metrics such as Dice score, false positive volume (FPV), and false negative volume (FNV) to provide a holistic measure of model effectiveness. The study also discusses the computational challenges and solutions related to model training, which was conducted on high-performance GPUs. Preprocessing and postprocessing techniques, including gaussian weighting schemes and morphological operations, are explored to further refine the segmentation output. The research offers valuable insights into the challenges and solutions for advanced medical image segmentation.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这项研究提出了FP-PET，一种涵盖CT和PET图像分割的全面方法。利用AutoPet2023 Challenge数据集，研究使用了多种机器学习模型，包括STUNet-large、SwinUNETR和VNet，以实现最新的分割性能。文章引入了一个汇总分数，将多个评估指标，如 dice分数、false positive volume (FPV) 和 false negative volume (FNV) 汇总而成一个整体评价指标，以提供更全面的模型效果评估。研究还讨论了模型训练中的计算挑战和解决方案，并在高性能GPU上进行训练。研究还探讨了预处理和后处理技术，包括高斯权重方案和形态运算，以进一步细化分割输出。研究提供了进一步了解高级医学图像分割的挑战和解决方案。
</details></li>
</ul>
<hr>
<h2 id="RHINO-Regularizing-the-Hash-based-Implicit-Neural-Representation"><a href="#RHINO-Regularizing-the-Hash-based-Implicit-Neural-Representation" class="headerlink" title="RHINO: Regularizing the Hash-based Implicit Neural Representation"></a>RHINO: Regularizing the Hash-based Implicit Neural Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12642">http://arxiv.org/abs/2309.12642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhu, Fengyi Liu, Qi Zhang, Xun Cao, Zhan Ma</li>
<li>for: 提高Hash表示法中的Regularization，以提高 interpolate 的可靠性和稳定性。</li>
<li>methods: 引入一个连续分析函数，以便在Hash表示法中增强Regularization，不需要修改当前的Hash表示法架构。</li>
<li>results: RHINO在多种任务上表现出色，如图像适应、签名距离函数表示、5D静止&#x2F;6D动态神经辐射场优化等，并且在质量和速度两个方面超过当前状态态技术。<details>
<summary>Abstract</summary>
The use of Implicit Neural Representation (INR) through a hash-table has demonstrated impressive effectiveness and efficiency in characterizing intricate signals. However, current state-of-the-art methods exhibit insufficient regularization, often yielding unreliable and noisy results during interpolations. We find that this issue stems from broken gradient flow between input coordinates and indexed hash-keys, where the chain rule attempts to model discrete hash-keys, rather than the continuous coordinates. To tackle this concern, we introduce RHINO, in which a continuous analytical function is incorporated to facilitate regularization by connecting the input coordinate and the network additionally without modifying the architecture of current hash-based INRs. This connection ensures a seamless backpropagation of gradients from the network's output back to the input coordinates, thereby enhancing regularization. Our experimental results not only showcase the broadened regularization capability across different hash-based INRs like DINER and Instant NGP, but also across a variety of tasks such as image fitting, representation of signed distance functions, and optimization of 5D static / 6D dynamic neural radiance fields. Notably, RHINO outperforms current state-of-the-art techniques in both quality and speed, affirming its superiority.
</details>
<details>
<summary>摘要</summary>
使用含义神经表示（INR）通过哈希表实现了非常出色的效果和效率，可是现有的状态 искусственный интеллект技术表现不够稳定，经常产生不可靠和噪音的结果 durante interpolaciones. 我们认为这个问题的根本原因在于哈希键和输入坐标之间的梯度流不畅，链式规则尝试模型离散的哈希键，而不是连续的坐标。为解决这个问题，我们介绍了犀牛（RHINO），它是一种连续的分析函数，可以在不修改现有哈希基于INR的网络架构的情况下，提供更好的规范。这种连接确保了输入坐标和网络的输出之间的精准的梯度传递，从而提高了规范。我们的实验结果不仅表明了不同的哈希基于INR如DINER和快速NP的规范能力的扩展，还在图像适应、签证距离函数表示和5D静态/6D动态神经辐射场的优化中达到了更高的质量和速度，并且超过了当前状态 искусственный интеллект技术的性能。
</details></li>
</ul>
<hr>
<h2 id="Global-Context-Aggregation-Network-for-Lightweight-Saliency-Detection-of-Surface-Defects"><a href="#Global-Context-Aggregation-Network-for-Lightweight-Saliency-Detection-of-Surface-Defects" class="headerlink" title="Global Context Aggregation Network for Lightweight Saliency Detection of Surface Defects"></a>Global Context Aggregation Network for Lightweight Saliency Detection of Surface Defects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12641">http://arxiv.org/abs/2309.12641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Yan, Xiaoheng Jiang, Yang Lu, Lisha Cui, Shupan Li, Jiale Cao, Mingliang Xu, Dacheng Tao</li>
<li>for: 这个论文主要目的是提出一种轻量级的抗余损检测方法，以提高检测效率和精度。</li>
<li>methods: 本文提出了一种基于encoder-decoder结构的Global Context Aggregation Network (GCANet)，包括一种新的transformerEncoder和Channel Reference Attention (CRA)模块，以提高多层特征表示的综合性。</li>
<li>results: 对三个公共的损害数据集进行实验表明，GCANet可以与17种state-of-the-art方法进行比较，并且在精度和运行效率之间做出了一个更好的平衡。具体来说，GCANet在SD-saliency-900上 achieve了91.79% $F_{\beta}^{w}$, 93.55% $S_\alpha$,和97.35% $E_\phi$的精度，并且在单个GPU上运行272fps。<details>
<summary>Abstract</summary>
Surface defect inspection is a very challenging task in which surface defects usually show weak appearances or exist under complex backgrounds. Most high-accuracy defect detection methods require expensive computation and storage overhead, making them less practical in some resource-constrained defect detection applications. Although some lightweight methods have achieved real-time inference speed with fewer parameters, they show poor detection accuracy in complex defect scenarios. To this end, we develop a Global Context Aggregation Network (GCANet) for lightweight saliency detection of surface defects on the encoder-decoder structure. First, we introduce a novel transformer encoder on the top layer of the lightweight backbone, which captures global context information through a novel Depth-wise Self-Attention (DSA) module. The proposed DSA performs element-wise similarity in channel dimension while maintaining linear complexity. In addition, we introduce a novel Channel Reference Attention (CRA) module before each decoder block to strengthen the representation of multi-level features in the bottom-up path. The proposed CRA exploits the channel correlation between features at different layers to adaptively enhance feature representation. The experimental results on three public defect datasets demonstrate that the proposed network achieves a better trade-off between accuracy and running efficiency compared with other 17 state-of-the-art methods. Specifically, GCANet achieves competitive accuracy (91.79% $F_{\beta}^{w}$, 93.55% $S_\alpha$, and 97.35% $E_\phi$) on SD-saliency-900 while running 272fps on a single gpu.
</details>
<details>
<summary>摘要</summary>
surface defect inspection 是一项非常具有挑战性的任务， surface defects 通常会出现弱化的外观或者在复杂的背景下出现。大多数高精度的缺陷检测方法需要昂贵的计算和存储开销，使其在一些资源受限的缺陷检测应用中不实用。 although some lightweight methods have achieved real-time inference speed with fewer parameters, they show poor detection accuracy in complex defect scenarios. 为了解决这个问题，我们开发了一个全球上下文聚合网络（GCANet），用于轻量级的缺陷检测。我们在轻量级的后ION上加入了一个新的 transformer Encoder，以便在全球上下文信息中捕捉全球上下文信息。我们引入了一种新的 Depth-wise Self-Attention（DSA）模块，用于在通道维度进行元素对元素的相似性检测，同时保持线性复杂度。此外，我们在每个解码块前加入了一个 Channel Reference Attention（CRA）模块，以强化底层特征表示。CRA模块利用不同层次特征之间的通道相关性来适应性地增强特征表示。我们在三个公共缺陷数据集上进行了实验，结果显示，我们的网络在缺陷检测精度和运行效率之间做出了更好的平衡，相比于其他 17 种国际前沿方法。具体来说，GCANet 在 SD-saliency-900 上达到了同等精度（91.79% $F_{\beta}^{w}$, 93.55% $S_\alpha$, 和 97.35% $E_\phi$），而且在单个 GPU 上运行速度为 272 fps。
</details></li>
</ul>
<hr>
<h2 id="CINFormer-Transformer-network-with-multi-stage-CNN-feature-injection-for-surface-defect-segmentation"><a href="#CINFormer-Transformer-network-with-multi-stage-CNN-feature-injection-for-surface-defect-segmentation" class="headerlink" title="CINFormer: Transformer network with multi-stage CNN feature injection for surface defect segmentation"></a>CINFormer: Transformer network with multi-stage CNN feature injection for surface defect segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12639">http://arxiv.org/abs/2309.12639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoheng Jiang, Kaiyi Guo, Yang Lu, Feng Yan, Hao Liu, Jiale Cao, Mingliang Xu, Dacheng Tao</li>
<li>for: 这个研究旨在提高工业生产中的表面缺陷检测精度，并解决深度学习方法中的一些挑战，如微弱缺陷和背景中的干扰。</li>
<li>methods: 本研究提出了一个基于 transformer 网络的 CINFormer，具有多Stage CNN 特征插入和 Top-K 自我注意模组。这个架构可以维持 CNN 捕捉细部特征和 transformer 抑制背景干扰的优点，以提高缺陷检测精度。</li>
<li>results: 实验结果显示，提出的 CINFormer 在 DAGM 2007、Magnetic tile 和 NEU 等表面缺陷数据集上实现了顶尖性能，并且在不同的缺陷类型和背景干扰下都能够获得高度的精度。<details>
<summary>Abstract</summary>
Surface defect inspection is of great importance for industrial manufacture and production. Though defect inspection methods based on deep learning have made significant progress, there are still some challenges for these methods, such as indistinguishable weak defects and defect-like interference in the background. To address these issues, we propose a transformer network with multi-stage CNN (Convolutional Neural Network) feature injection for surface defect segmentation, which is a UNet-like structure named CINFormer. CINFormer presents a simple yet effective feature integration mechanism that injects the multi-level CNN features of the input image into different stages of the transformer network in the encoder. This can maintain the merit of CNN capturing detailed features and that of transformer depressing noises in the background, which facilitates accurate defect detection. In addition, CINFormer presents a Top-K self-attention module to focus on tokens with more important information about the defects, so as to further reduce the impact of the redundant background. Extensive experiments conducted on the surface defect datasets DAGM 2007, Magnetic tile, and NEU show that the proposed CINFormer achieves state-of-the-art performance in defect detection.
</details>
<details>
<summary>摘要</summary>
surface defect inspection 是现代工业生产中非常重要的一环。尽管基于深度学习的抗损检测方法已经取得了 significiant progress，但还有一些挑战，如微弱损害难以辨别和背景中的干扰。为解决这些问题，我们提出了一种基于 transformer 网络的多stage CNN 特征注入的表面抗损分割方法，即 CINFormer。 CINFormer 提供了一种简单 yet 有效的特征集成机制，通过在 transformer 网络的编码器中注入不同级别的 CNN 特征，以维持 CNN 捕捉细节特征的优点，同时使用 transformer 网络压缩背景干扰的优点。此外，CINFormer 还提供了 Top-K 自注意模块，以便更好地强调损害的关键信息，从而进一步减少背景干扰的影响。经过对 DAGM 2007、Magnetic 块和 NEU 等表面抗损数据集的广泛实验，我们发现，提议的 CINFormer 可以达到现代表面抗损检测的州标性性能。
</details></li>
</ul>
<hr>
<h2 id="Auto-Lesion-Segmentation-with-a-Novel-Intensity-Dark-Channel-Prior-for-COVID-19-Detection"><a href="#Auto-Lesion-Segmentation-with-a-Novel-Intensity-Dark-Channel-Prior-for-COVID-19-Detection" class="headerlink" title="Auto-Lesion Segmentation with a Novel Intensity Dark Channel Prior for COVID-19 Detection"></a>Auto-Lesion Segmentation with a Novel Intensity Dark Channel Prior for COVID-19 Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12638">http://arxiv.org/abs/2309.12638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Basma Jumaa Saleh, Zaid Omar, Vikrant Bhateja, Lila Iznita Izhar<br>for: 本研究旨在开发一种基于 computed tomography (CT) 图像的 COVID-19 诊断方法，以帮助诊断可疑 COVID-19 患者。methods: 本研究使用了 radiomic 特征，并采用了强化自动分割原理（IDCP）和深度神经网络（ALS-IDCP-DNN），在定义的分析阈值范围内进行图像分类。results: 验证性 dataset 上，提议的模型实现了98.8%的平均准确率、99%的准确率、98%的 recall 和98%的 F1 score。这些结果表明，我们的模型可以准确地分类 COVID-19 图像，可以帮助 radiologists 诊断可疑 COVID-19 患者。此外，我们的模型表现得更好于现有的10多个国际研究。<details>
<summary>Abstract</summary>
During the COVID-19 pandemic, medical imaging techniques like computed tomography (CT) scans have demonstrated effectiveness in combating the rapid spread of the virus. Therefore, it is crucial to conduct research on computerized models for the detection of COVID-19 using CT imaging. A novel processing method has been developed, utilizing radiomic features, to assist in the CT-based diagnosis of COVID-19. Given the lower specificity of traditional features in distinguishing between different causes of pulmonary diseases, the objective of this study is to develop a CT-based radiomics framework for the differentiation of COVID-19 from other lung diseases. The model is designed to focus on outlining COVID-19 lesions, as traditional features often lack specificity in this aspect. The model categorizes images into three classes: COVID-19, non-COVID-19, or normal. It employs enhancement auto-segmentation principles using intensity dark channel prior (IDCP) and deep neural networks (ALS-IDCP-DNN) within a defined range of analysis thresholds. A publicly available dataset comprising COVID-19, normal, and non-COVID-19 classes was utilized to validate the proposed model's effectiveness. The best performing classification model, Residual Neural Network with 50 layers (Resnet-50), attained an average accuracy, precision, recall, and F1-score of 98.8%, 99%, 98%, and 98% respectively. These results demonstrate the capability of our model to accurately classify COVID-19 images, which could aid radiologists in diagnosing suspected COVID-19 patients. Furthermore, our model's performance surpasses that of more than 10 current state-of-the-art studies conducted on the same dataset.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在COVID-19疫情期间，计算机成像技术如计算机tomography（CT）扫描已经表现出效iveness在防止病毒的迅速传播。因此，需要进行计算机模型的研究，以便使用CT成像来诊断COVID-19。我们开发了一种新的处理方法，利用 радиомics特征，以帮助CT成像诊断COVID-19。传统的特征 oft lack specificity in distinguishing between different causes of pulmonary diseases，因此我们的目标是开发一个基于CT成像的radiomics框架，以区分COVID-19和其他肺病。模型设计用于强调COVID-19斑点，以便更好地识别COVID-19。模型将图像分类为三类：COVID-19、非COVID-19或正常。它使用了增强自动分割原理，使用暗色通道优先预测（IDCP）和深度神经网络（ALS-IDCP-DNN），在定义的分析阈值范围内。一个公共可用的数据集，包括COVID-19、正常和非COVID-19类别，用于验证我们的模型效果。使用最佳表现的分类模型，即Residual Neural Network with 50 layers（Resnet-50），在COVID-19、非COVID-19和正常类别之间达到了平均准确率、精度、回归率和F1分数的98.8%、99%、98%和98%。这些结果表明我们的模型可以准确地分类COVID-19图像，这将助力放医生诊断可能的COVID-19患者。此外，我们的模型性能超过了现有的10个以上state-of-the-art研究，同一个数据集。
</details></li>
</ul>
<hr>
<h2 id="Learning-Actions-and-Control-of-Focus-of-Attention-with-a-Log-Polar-like-Sensor"><a href="#Learning-Actions-and-Control-of-Focus-of-Attention-with-a-Log-Polar-like-Sensor" class="headerlink" title="Learning Actions and Control of Focus of Attention with a Log-Polar-like Sensor"></a>Learning Actions and Control of Focus of Attention with a Log-Polar-like Sensor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12634">http://arxiv.org/abs/2309.12634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Göransson, Volker Krueger</li>
<li>for: 提高自动移动机器人图像处理速度</li>
<li>methods: 使用径向尺度图像数据和 gaze 控制</li>
<li>results: 成功降低图像像素数量，不影响游戏性能Here’s a breakdown of each point:</li>
<li>for: The paper aims to improve the image processing speed of an autonomous mobile robot.</li>
<li>methods: The paper explores the use of log-polar like image data with gaze control, and extends an A3C deep RL approach with an LSTM network to learn the policy for playing Atari games and gaze control.</li>
<li>results: The paper successfully reduces the amount of image pixels by a factor of 5 without losing any gaming performance.<details>
<summary>Abstract</summary>
With the long-term goal of reducing the image processing time on an autonomous mobile robot in mind we explore in this paper the use of log-polar like image data with gaze control. The gaze control is not done on the Cartesian image but on the log-polar like image data. For this we start out from the classic deep reinforcement learning approach for Atari games. We extend an A3C deep RL approach with an LSTM network, and we learn the policy for playing three Atari games and a policy for gaze control. While the Atari games already use low-resolution images of 80 by 80 pixels, we are able to further reduce the amount of image pixels by a factor of 5 without losing any gaming performance.
</details>
<details>
<summary>摘要</summary>
这里使用简化中文翻译文本。</SYS>为了实现机器人自动运行中的图像处理时间缩短，这篇论文探讨了使用对应的对应图像数据，并将控制 gaze 在这些图像数据上。我们从 класи的深度学习游戏方法开始，并将 A3C 深度RL 方法与 LSTM 网络扩展。我们学习了三个 Atari 游戏和一个 gaze 控制策略。 Although Atari games already use low-resolution images of 80 by 80 pixels, we are able to further reduce the amount of image pixels by a factor of 5 without losing any gaming performance.
</details></li>
</ul>
<hr>
<h2 id="Decision-Fusion-Network-with-Perception-Fine-tuning-for-Defect-Classification"><a href="#Decision-Fusion-Network-with-Perception-Fine-tuning-for-Defect-Classification" class="headerlink" title="Decision Fusion Network with Perception Fine-tuning for Defect Classification"></a>Decision Fusion Network with Perception Fine-tuning for Defect Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12630">http://arxiv.org/abs/2309.12630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoheng Jiang, Shilong Tian, Zhiwen Zhu, Yang Lu, Hao Liu, Li Chen, Shupan Li, Mingliang Xu</li>
<li>for: 卷积 neural network 用于Surface defect inspection 中的检测和分类任务</li>
<li>methods: 提出了一种决策融合网络（DFNet），通过将semantic decision和feature decision融合来强化网络的决策能力，同时提出了一种感知细化模块（PFM）来优化前景和背景的分割结果</li>
<li>results: 对于公开的数据集KolektorSDD2和Magnetic-tile-defect-datasets进行了实验，实现了96.1% AP和94.6% mAP的效果<details>
<summary>Abstract</summary>
Surface defect inspection is an important task in industrial inspection. Deep learning-based methods have demonstrated promising performance in this domain. Nevertheless, these methods still suffer from misjudgment when encountering challenges such as low-contrast defects and complex backgrounds. To overcome these issues, we present a decision fusion network (DFNet) that incorporates the semantic decision with the feature decision to strengthen the decision ability of the network. In particular, we introduce a decision fusion module (DFM) that extracts a semantic vector from the semantic decision branch and a feature vector for the feature decision branch and fuses them to make the final classification decision. In addition, we propose a perception fine-tuning module (PFM) that fine-tunes the foreground and background during the segmentation stage. PFM generates the semantic and feature outputs that are sent to the classification decision stage. Furthermore, we present an inner-outer separation weight matrix to address the impact of label edge uncertainty during segmentation supervision. Our experimental results on the publicly available datasets including KolektorSDD2 (96.1% AP) and Magnetic-tile-defect-datasets (94.6% mAP) demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
superficie defecto inspección es una tarea importante en la inspección industrial. los métodos basados en aprendizaje profundo han demostrado un rendimiento prometedor en este dominio. sin embargo, estos métodos todavía sufren de mal juzgar cuando se encuentran desafíos como defectos de baja contraste y fondos complejos. para superar estos problemas, presentamos una red de fusión de decisiones (DFNet) que integra la decisión semántica con la decisión de características para fortalecer la capacidad de toma de decisiones del réseau. en particular, introducimos un módulo de fusión de decisiones (DFM) que extrae un vector semántico de la rama de decisión semántica y un vector de características de la rama de decisión de características y los fusiona para tomar la decisión de clasificación final. Además, propusimos un módulo de finuración de percepción (PFM) que realiza la fine-tuning de la percepción durante la etapa de segmentación. PFM genera las salidas semánticas y de características que se envían a la etapa de toma de decisiones de clasificación. Además, presentamos una matriz de pesos de separación interior-exterior para abordar el impacto de la incertidumbre de la etiqueta en la supervisión de segmentación. nuestros resultados experimentales en los conjuntos de datos públicos, incluyendo KolektorSDD2 (96.1% AP) y Magnetic-tile-defect-datasets (94.6% mAP), demuestran la eficacia de la método propuesto.
</details></li>
</ul>
<hr>
<h2 id="DeFormer-Integrating-Transformers-with-Deformable-Models-for-3D-Shape-Abstraction-from-a-Single-Image"><a href="#DeFormer-Integrating-Transformers-with-Deformable-Models-for-3D-Shape-Abstraction-from-a-Single-Image" class="headerlink" title="DeFormer: Integrating Transformers with Deformable Models for 3D Shape Abstraction from a Single Image"></a>DeFormer: Integrating Transformers with Deformable Models for 3D Shape Abstraction from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12594">http://arxiv.org/abs/2309.12594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Liu, Xiang Yu, Meng Ye, Qilong Zhangli, Zhuowei Li, Zhixing Zhang, Dimitris N. Metaxas</li>
<li>for: 提出了一种新的bi-channel Transformer架构，用于同时估计全局和局部变形。</li>
<li>methods: 使用了一种参数化的塑形模型，称为DeFormer，以便更好地抽象复杂的物体形状。</li>
<li>results: 在ShapeNet上进行了广泛的实验，并达到了比之前最佳的重建精度，并且可以Visualize了更加准确的semantic相关性。<details>
<summary>Abstract</summary>
Accurate 3D shape abstraction from a single 2D image is a long-standing problem in computer vision and graphics. By leveraging a set of primitives to represent the target shape, recent methods have achieved promising results. However, these methods either use a relatively large number of primitives or lack geometric flexibility due to the limited expressibility of the primitives. In this paper, we propose a novel bi-channel Transformer architecture, integrated with parameterized deformable models, termed DeFormer, to simultaneously estimate the global and local deformations of primitives. In this way, DeFormer can abstract complex object shapes while using a small number of primitives which offer a broader geometry coverage and finer details. Then, we introduce a force-driven dynamic fitting and a cycle-consistent re-projection loss to optimize the primitive parameters. Extensive experiments on ShapeNet across various settings show that DeFormer achieves better reconstruction accuracy over the state-of-the-art, and visualizes with consistent semantic correspondences for improved interpretability.
</details>
<details>
<summary>摘要</summary>
通过使用一组基本形状来表示目标形状，现代方法已经实现了在计算机视觉和图形领域中准确地抽象三维形状。然而，这些方法可能使用相对较多的基本形状，或者由于基本形状的有限表达能力而缺乏几何可动性。在本文中，我们提出了一种新的双渠道变换器架构，并结合参数化可变模型，称为DeFormer，以同时估计全局和局部几何变换。这样，DeFormer可以抽象复杂的物体形状，使用少量的基本形状，并且具有更广泛的几何覆盖和细节。然后，我们引入了力场驱动的动态适应和цикли性征重 Projekt loss 来优化基本形状参数。通过对ShapeNet进行了多种设置的实验，我们发现DeFormer可以在计算机视觉和图形领域中实现更好的重建精度，并且可以visualize与更高度的 semantic correspondence  для改进释plausibility。
</details></li>
</ul>
<hr>
<h2 id="Improving-Machine-Learning-Robustness-via-Adversarial-Training"><a href="#Improving-Machine-Learning-Robustness-via-Adversarial-Training" class="headerlink" title="Improving Machine Learning Robustness via Adversarial Training"></a>Improving Machine Learning Robustness via Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12593">http://arxiv.org/abs/2309.12593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Dang, Thushari Hapuarachchi, Kaiqi Xiong, Jing Lin</li>
<li>for: 本研究旨在 investigate 机器学习（ML）Robustness 在中央化和分散化环境中，以帮助设计更加Robust 的 ML 算法。</li>
<li>methods: 本研究使用了 adversarial training 方法在中央化和分散化环境中进行 ML 训练和测试，并使用了 Fast Gradient Sign Method 和 DeepFool 生成抗性例子。</li>
<li>results: 在中央化环境中，我们实现了测试准确率为 65.41% 和 83.0%，比现有研究提高了 18.41% 和 47%。在分散化环境中，我们研究了 Federated learning（FL）的Robustness，并使用了 adversarial training 方法与独立同分布（IID）和非IID数据进行比较。在 IID 数据 caso，我们可以实现类似于中央化环境的Robust准确率。在非IID 数据 caso，自然准确率下降了 25% 和 23.4%，对比 IID 数据 caso，分别是。我们还提出了一种 IID 数据共享方法，可以提高自然准确率到 85.04% 和 Robust准确率从 57% 提高到 72% 和 67%，分别是。<details>
<summary>Abstract</summary>
As Machine Learning (ML) is increasingly used in solving various tasks in real-world applications, it is crucial to ensure that ML algorithms are robust to any potential worst-case noises, adversarial attacks, and highly unusual situations when they are designed. Studying ML robustness will significantly help in the design of ML algorithms. In this paper, we investigate ML robustness using adversarial training in centralized and decentralized environments, where ML training and testing are conducted in one or multiple computers. In the centralized environment, we achieve a test accuracy of 65.41% and 83.0% when classifying adversarial examples generated by Fast Gradient Sign Method and DeepFool, respectively. Comparing to existing studies, these results demonstrate an improvement of 18.41% for FGSM and 47% for DeepFool. In the decentralized environment, we study Federated learning (FL) robustness by using adversarial training with independent and identically distributed (IID) and non-IID data, respectively, where CIFAR-10 is used in this research. In the IID data case, our experimental results demonstrate that we can achieve such a robust accuracy that it is comparable to the one obtained in the centralized environment. Moreover, in the non-IID data case, the natural accuracy drops from 66.23% to 57.82%, and the robust accuracy decreases by 25% and 23.4% in C&W and Projected Gradient Descent (PGD) attacks, compared to the IID data case, respectively. We further propose an IID data-sharing approach, which allows for increasing the natural accuracy to 85.04% and the robust accuracy from 57% to 72% in C&W attacks and from 59% to 67% in PGD attacks.
</details>
<details>
<summary>摘要</summary>
随着机器学习（ML）在实际应用中的广泛使用， Ensuring the robustness of ML algorithms against potential worst-case noises, adversarial attacks, and highly unusual situations during their design has become crucial. Studying ML robustness can significantly help in the design of ML algorithms. In this paper, we investigate ML robustness using adversarial training in centralized and decentralized environments, where ML training and testing are conducted in one or multiple computers.在中央化环境中，我们通过对快速梯度方法和深度欺骗的挑战性例子进行反对抗教程，实现了测试精度为65.41%和83.0%。相比已有研究，这些结果表明了18.41%的提高 для快速梯度方法和47%的提高 для深度欺骗。在分布式学习（FL）环境中，我们研究了 Federated learning（FL）的可靠性，使用反对抗教程与独立且相同分布（IID）和非IID数据进行研究，其中CIFAR-10被用于这项研究。在IID数据情况下，我们的实验结果表明，我们可以实现与中央化环境相同的可靠性。此外，在非IID数据情况下，自然精度从66.23%降低到57.82%，而可靠精度下降25%和23.4%在C&W和投影梯度下降（PGD）攻击中，相比IID数据情况下。我们还提出了一种IID数据分享方法，可以提高自然精度到85.04%和可靠精度从57%提高到72%在C&W攻击中，并提高到67%在PGD攻击中。
</details></li>
</ul>
<hr>
<h2 id="BGF-YOLO-Enhanced-YOLOv8-with-Multiscale-Attentional-Feature-Fusion-for-Brain-Tumor-Detection"><a href="#BGF-YOLO-Enhanced-YOLOv8-with-Multiscale-Attentional-Feature-Fusion-for-Brain-Tumor-Detection" class="headerlink" title="BGF-YOLO: Enhanced YOLOv8 with Multiscale Attentional Feature Fusion for Brain Tumor Detection"></a>BGF-YOLO: Enhanced YOLOv8 with Multiscale Attentional Feature Fusion for Brain Tumor Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12585">http://arxiv.org/abs/2309.12585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkang315/bgf-yolo">https://github.com/mkang315/bgf-yolo</a></li>
<li>paper_authors: Ming Kang, Chee-Ming Ting, Fung Fung Ting, Raphaël C. -W. Phan</li>
<li>for: automated brain tumor detection</li>
<li>methods:  integrate Bi-level Routing Attention (BRA), Generalized feature pyramid networks (GFPN), and Fourth detecting head into YOLOv8</li>
<li>results: 4.7% absolute increase of mAP$_{50}$ compared to YOLOv8x, and achieves state-of-the-art on the brain tumor detection dataset Br35H.Here’s the full Chinese text:</li>
<li>for: 本研究旨在 automatized 脑癌检测</li>
<li>methods:  integrate Bi-level Routing Attention (BRA), Generalized feature pyramid networks (GFPN), 和 Fourth detecting head into YOLOv8</li>
<li>results: 与 YOLOv8x 相比，BGF-YOLO 提供 4.7% 的统计提升，并在脑癌检测 dataset Br35H 上 achievement state-of-the-art.I hope this helps!<details>
<summary>Abstract</summary>
You Only Look Once (YOLO)-based object detectors have shown remarkable accuracy for automated brain tumor detection. In this paper, we develop a novel BGF-YOLO architecture by incorporating Bi-level Routing Attention (BRA), Generalized feature pyramid networks (GFPN), and Fourth detecting head into YOLOv8. BGF-YOLO contains an attention mechanism to focus more on important features, and feature pyramid networks to enrich feature representation by merging high-level semantic features with spatial details. Furthermore, we investigate the effect of different attention mechanisms and feature fusions, detection head architectures on brain tumor detection accuracy. Experimental results show that BGF-YOLO gives a 4.7% absolute increase of mAP$_{50}$ compared to YOLOv8x, and achieves state-of-the-art on the brain tumor detection dataset Br35H. The code is available at https://github.com/mkang315/BGF-YOLO.
</details>
<details>
<summary>摘要</summary>
“你只需要看一次”（YOLO）基本的物体探测器已经在自动脑肿检测中表现出色。在这篇论文中，我们开发了一种新的BGF-YOLO架构，并将Bi-level Routing Attention（BRA）、Generalized feature pyramid networks（GFPN）和第四个探测头纳入YOLOv8。BGF-YOLO包含一种注意机制，以增强关键特征的注意力，并通过将高级 semantic features与空间细节合并来增强特征表示。此外，我们还 investigate了不同的注意机制和特征融合、探测头架构对脑肿检测精度的影响。实验结果表明，BGF-YOLO与YOLOv8x相比，提高了4.7%的mAP$_{50}$精度，并在脑肿检测数据集Br35H中达到了状态机。代码可以在https://github.com/mkang315/BGF-YOLO中获取。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Alzheimers-Disease-with-Deep-Learning-on-Eye-tracking-Data"><a href="#Classification-of-Alzheimers-Disease-with-Deep-Learning-on-Eye-tracking-Data" class="headerlink" title="Classification of Alzheimers Disease with Deep Learning on Eye-tracking Data"></a>Classification of Alzheimers Disease with Deep Learning on Eye-tracking Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12574">http://arxiv.org/abs/2309.12574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harshinee Sriram, Cristina Conati, Thalia Field</li>
<li>For: This paper aims to classify Alzheimer’s Disease (AD) from eye-tracking (ET) data using a Deep-Learning classifier trained end-to-end on raw ET data.* Methods: The proposed method, called VTNet, uses a GRU and a CNN in parallel to leverage both visual (V) and temporal (T) representations of ET data.* Results: VTNet outperforms the state-of-the-art approaches in AD classification, providing encouraging evidence on the generality of this model to make predictions from ET data.Here’s the Chinese translation of the three pieces of information:* For: 这篇论文目标是使用 Raw 眼动追踪数据进行扩散性疾病分类 (AD)。* Methods: 提议的方法是 VTNet，它使用 GRU 和 CNN 并行使用，以利用眼动数据中的视觉 (V) 和时间 (T) 表示。* Results: VTNet 在 AD 分类任务上表现出色，超过了现有的方法，提供了对这种模型在眼动数据上的预测性的有力证明。<details>
<summary>Abstract</summary>
Existing research has shown the potential of classifying Alzheimers Disease (AD) from eye-tracking (ET) data with classifiers that rely on task-specific engineered features. In this paper, we investigate whether we can improve on existing results by using a Deep-Learning classifier trained end-to-end on raw ET data. This classifier (VTNet) uses a GRU and a CNN in parallel to leverage both visual (V) and temporal (T) representations of ET data and was previously used to detect user confusion while processing visual displays. A main challenge in applying VTNet to our target AD classification task is that the available ET data sequences are much longer than those used in the previous confusion detection task, pushing the limits of what is manageable by LSTM-based models. We discuss how we address this challenge and show that VTNet outperforms the state-of-the-art approaches in AD classification, providing encouraging evidence on the generality of this model to make predictions from ET data.
</details>
<details>
<summary>摘要</summary>
先前的研究已经证明了通过眼动跟踪（ET）数据进行阿尔茨heimer病（AD）分类的潜在性。在这篇论文中，我们调查了是否可以通过使用深度学习的分类器，对直接使用原始ET数据进行分类，以提高现有结果。我们称之为VTNet，它使用GRU和CNN并行使用视觉（V）和时间（T）表示，曾用于检测视觉显示的混乱。主要挑战在应用VTNet到我们的target AD分类任务中是，ET数据序列的可用性远比先前的混乱检测任务更长，这使得LSTM模型管理的范围受到挑战。我们详细介绍了我们如何解决这个挑战，并示出VTNet在AD分类任务中的表现，超越了当前的状态艺术方法，提供了对ET数据进行预测的鼓励性证据。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-3D-Multi-Modal-Residual-Convolutional-Neural-Network-for-Mild-Traumatic-Brain-Injury-Diagnosis"><a href="#Interpretable-3D-Multi-Modal-Residual-Convolutional-Neural-Network-for-Mild-Traumatic-Brain-Injury-Diagnosis" class="headerlink" title="Interpretable 3D Multi-Modal Residual Convolutional Neural Network for Mild Traumatic Brain Injury Diagnosis"></a>Interpretable 3D Multi-Modal Residual Convolutional Neural Network for Mild Traumatic Brain Injury Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12572">http://arxiv.org/abs/2309.12572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanem Ellethy, Viktor Vegh, Shekhar S. Chandra</li>
<li>for: 这个研究旨在提高轻度头部创伤（mTBI）的诊断精度，并且使用多Modal的残差算法（MRCNN）和Occlusion Sensitivity Maps（OSM）来增强诊断模型的解释力。</li>
<li>methods: 这个研究使用了一个 interpretable 的 3D Multi-Modal Residual Convolutional Neural Network（MRCNN）模型，并且将 Occlusion Sensitivity Maps（OSM）加入了诊断模型中，以增强诊断的精度。</li>
<li>results: 研究结果显示，MRCNN 模型在 mTBI 诊断中表现出色，精度达 82.4%，sensitivity 达 82.6%，特异性达 81.6%，并且比 CT 基于的 Residual Convolutional Neural Network（RCNN）模型提高了 4.4% 的特异性和 9.0% 的精度。<details>
<summary>Abstract</summary>
Mild Traumatic Brain Injury (mTBI) is a significant public health challenge due to its high prevalence and potential for long-term health effects. Despite Computed Tomography (CT) being the standard diagnostic tool for mTBI, it often yields normal results in mTBI patients despite symptomatic evidence. This fact underscores the complexity of accurate diagnosis. In this study, we introduce an interpretable 3D Multi-Modal Residual Convolutional Neural Network (MRCNN) for mTBI diagnostic model enhanced with Occlusion Sensitivity Maps (OSM). Our MRCNN model exhibits promising performance in mTBI diagnosis, demonstrating an average accuracy of 82.4%, sensitivity of 82.6%, and specificity of 81.6%, as validated by a five-fold cross-validation process. Notably, in comparison to the CT-based Residual Convolutional Neural Network (RCNN) model, the MRCNN shows an improvement of 4.4% in specificity and 9.0% in accuracy. We show that the OSM offers superior data-driven insights into CT images compared to the Grad-CAM approach. These results highlight the efficacy of the proposed multi-modal model in enhancing the diagnostic precision of mTBI.
</details>
<details>
<summary>摘要</summary>
轻度头部Trauma (mTBI) 是一个重要的公共卫生挑战，因其高频率和长期健康影响的可能性。 Despite Computed Tomography (CT) 是 mTBI 的标准诊断工具，它经常在 mTBI 患者中显示正常结果，这再次 highlights 诊断的复杂性。 在这项研究中，我们介绍了一种可解释的 3D 多模态异常感知 Convolutional Neural Network (MRCNN) 模型，用于 mTBI 诊断。我们的 MRCNN 模型在 mTBI 诊断中表现出色，其中的平均准确率为 82.4%，敏感性为 82.6%，特异性为 81.6%，这些结果通过五次交叉验证过程得到验证。尤其是，相比 CT-based Residual Convolutional Neural Network (RCNN) 模型，我们的 MRCNN 模型在特异性和准确率方面增加了4.4%和9.0%。我们表明 OSM 在 CT 图像中提供了更高的数据驱动的权重，相比 Grad-CAM 方法。这些结果表明我们的多模态模型在 mTBI 诊断中增强了诊断精度。
</details></li>
</ul>
<hr>
<h2 id="Wave-informed-dictionary-learning-for-high-resolution-imaging-in-complex-media"><a href="#Wave-informed-dictionary-learning-for-high-resolution-imaging-in-complex-media" class="headerlink" title="Wave-informed dictionary learning for high-resolution imaging in complex media"></a>Wave-informed dictionary learning for high-resolution imaging in complex media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12990">http://arxiv.org/abs/2310.12990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel Moscoso, Alexei Novikov, George Papanicolaou, Chrysoula Tsogka</li>
<li>for: 这个论文目的是提出一种用于吸收媒体成像的方法，当有大量和多样化的数据集available时。</li>
<li>methods: 该方法有两个步骤：第一步使用字典学习算法来估计真正的绿函数向量，并将其作为列存储在一个不ordered的感测矩阵中。第二步使用多维度排序法来让列表的排序，并使用连接信息来 derive from cross-correlations of its columns，如时间反转。</li>
<li>results: 通过 simulation experiments，我们示出了该方法能够在复杂媒体中提供高分辨率的成像图像。<details>
<summary>Abstract</summary>
We propose an approach for imaging in scattering media when large and diverse data sets are available. It has two steps. Using a dictionary learning algorithm the first step estimates the true Green's function vectors as columns in an unordered sensing matrix. The array data comes from many sparse sets of sources whose location and strength are not known to us. In the second step, the columns of the estimated sensing matrix are ordered for imaging using Multi-Dimensional Scaling with connectivity information derived from cross-correlations of its columns, as in time reversal. For these two steps to work together we need data from large arrays of receivers so the columns of the sensing matrix are incoherent for the first step, as well as from sub-arrays so that they are coherent enough to obtain the connectivity needed in the second step. Through simulation experiments, we show that the proposed approach is able to provide images in complex media whose resolution is that of a homogeneous medium.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于在散射媒体中进行成像，当有大量多样化的数据集 disponible。该方法包括两步。在第一步中，使用一个词汇学算法，我们估算了真实的绿函数向量作为排序后的感知矩阵的列。数组数据来自多个稀疏的源集，其位置和强度不知道我们。在第二步中，我们使用多维度尺度学（Multi-Dimensional Scaling）将排序后的感知矩阵的列轴进行了排序，并使用这些列的垂直相关性来获得连接信息。为了使这两个步骤可以共同工作，我们需要从大型接收器阵列中获得数据，以确保感知矩阵的列不受相关性的限制。通过实验 simulate, we show that the proposed approach can provide images in complex media with resolution comparable to that of a homogeneous medium.Note: The translation is provided "as is" and may not be perfect. Please let me know if you need any further assistance or clarification.
</details></li>
</ul>
<hr>
<h2 id="Triple-View-Knowledge-Distillation-for-Semi-Supervised-Semantic-Segmentation"><a href="#Triple-View-Knowledge-Distillation-for-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="Triple-View Knowledge Distillation for Semi-Supervised Semantic Segmentation"></a>Triple-View Knowledge Distillation for Semi-Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12557">http://arxiv.org/abs/2309.12557</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Ping Li, Junjie Chen, Li Yuan, Xianghua Xu, Mingli Song</li>
<li>for: 提高 semi-supervised semantic segmentation 的效果，使用少量标注图像和大量非标注图像预测像素级标签图。</li>
<li>methods: 使用 tri-training 和 triple-view encoder 来捕捉多样化特征，并通过知识储存技术学习对应的 semantics。 dual-frequency decoder 选择重要特征，并通过双频道注意机制来评估特征重要性。</li>
<li>results: 在 Pascal VOC 2012 和 Cityscapes 两个标准测试集上进行了广泛的实验，结果表明提出的方法在精度和推理速度之间做出了好的平衡，并且与其他方法相比具有更好的性能。<details>
<summary>Abstract</summary>
To alleviate the expensive human labeling, semi-supervised semantic segmentation employs a few labeled images and an abundant of unlabeled images to predict the pixel-level label map with the same size. Previous methods often adopt co-training using two convolutional networks with the same architecture but different initialization, which fails to capture the sufficiently diverse features. This motivates us to use tri-training and develop the triple-view encoder to utilize the encoders with different architectures to derive diverse features, and exploit the knowledge distillation skill to learn the complementary semantics among these encoders. Moreover, existing methods simply concatenate the features from both encoder and decoder, resulting in redundant features that require large memory cost. This inspires us to devise a dual-frequency decoder that selects those important features by projecting the features from the spatial domain to the frequency domain, where the dual-frequency channel attention mechanism is introduced to model the feature importance. Therefore, we propose a Triple-view Knowledge Distillation framework, termed TriKD, for semi-supervised semantic segmentation, including the triple-view encoder and the dual-frequency decoder. Extensive experiments were conducted on two benchmarks, \ie, Pascal VOC 2012 and Cityscapes, whose results verify the superiority of the proposed method with a good tradeoff between precision and inference speed.
</details>
<details>
<summary>摘要</summary>
要解决高严格的人类标注成本高昂，半supervised semantic segmentation使用一些标注图像和大量无标注图像预测像素级标签地图，同时使用两个 convolutional network 的不同初始化来提高分类精度。以前的方法通常采用两个 convolutional network 的同 architectures 的 co-training，但这会遗漏重要的多样化特征。这种情况驱使我们使用 tri-training 和三个视角编码器，以利用不同的架构来获得多样化的特征，并利用知识填充技术来学习这些编码器之间的补做 semantics。此外，现有的方法通常将编码器和解码器的特征直接 concatenate，从而导致缓存成本过高。这个灵感我们提出了一种 dual-frequency decoder，可以选择重要的特征，并通过将特征从空间频谱中 проек到频谱频率上，实现了 dual-frequency channel attention mechanism。因此，我们提出了一种 Triple-view Knowledge Distillation框架，称之为 TriKD，用于半supervised semantic segmentation，包括三个视角编码器和 dual-frequency decoder。我们在 Pascal VOC 2012 和 Cityscapes 两个标准benchmark上进行了广泛的实验，结果证明了我们提出的方法具有较好的平衡性和推理速度。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/22/cs.CV_2023_09_22/" data-id="clpztdnjf00mqes883fs7byle" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/22/cs.AI_2023_09_22/" class="article-date">
  <time datetime="2023-09-22T12:00:00.000Z" itemprop="datePublished">2023-09-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/22/cs.AI_2023_09_22/">cs.AI - 2023-09-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Poster-Self-Supervised-Quantization-Aware-Knowledge-Distillation"><a href="#Poster-Self-Supervised-Quantization-Aware-Knowledge-Distillation" class="headerlink" title="Poster: Self-Supervised Quantization-Aware Knowledge Distillation"></a>Poster: Self-Supervised Quantization-Aware Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13220">http://arxiv.org/abs/2309.13220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiqi Zhao, Ming Zhao</li>
<li>for: 提高量化敏感模型的性能</li>
<li>methods: 提出了一种新的无监督自适应量化敏感知识传递（SQAKD）框架，通过同时Minimize KL损失和精度损失来协调量化和敏感知识传递。</li>
<li>results: 对多种现有QAT研究进行评估，显示SQAKD可以显著提高量化敏感模型的性能，并不需要大量标注数据。<details>
<summary>Abstract</summary>
Quantization-aware training (QAT) starts with a pre-trained full-precision model and performs quantization during retraining. However, existing QAT works require supervision from the labels and they suffer from accuracy loss due to reduced precision. To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation framework (SQAKD). SQAKD first unifies the forward and backward dynamics of various quantization functions and then reframes QAT as a co-optimization problem that simultaneously minimizes the KL-Loss and the discretization error, in a self-supervised manner. The evaluation shows that SQAKD significantly improves the performance of various state-of-the-art QAT works. SQAKD establishes stronger baselines and does not require extensive labeled training data, potentially making state-of-the-art QAT research more accessible.
</details>
<details>
<summary>摘要</summary>
Quantization-aware training (QAT) 开始于一个预训练的全精度模型，并在重新训练期间进行量化。然而，现有的 QAT 工作受到标签的监督，并且受到精度降低的影响，导致准确性下降。为解决这些限制，本文提出了一种新的 Self-Supervised Quantization-Aware Knowledge Distillation 框架 (SQAKD)。SQAKD 首先将量化函数的前向和反向动力统一，然后将 QAT 重新定义为一个同时减少 KL-损失和精度损失的合理化问题，在自我监督的方式下进行解决。评估结果表明，SQAKD 可以显著提高不同的状态前训练 QAT 工作的性能。SQAKD 设立了更强的基线，并不需要大量的标签训练数据，可能使状态前训练 QAT 研究更加可 accessible。
</details></li>
</ul>
<hr>
<h2 id="AI-Copilot-for-Business-Optimisation-A-Framework-and-A-Case-Study-in-Production-Scheduling"><a href="#AI-Copilot-for-Business-Optimisation-A-Framework-and-A-Case-Study-in-Production-Scheduling" class="headerlink" title="AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling"></a>AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13218">http://arxiv.org/abs/2309.13218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pivithuru Thejan Amarasinghe, Su Nguyen, Yuan Sun, Damminda Alahakoon</li>
<li>for: 这个论文的目的是提出一种基于大语言模型（LLM）的企业优化问题表述 Synthesizer，以减少人工智能的参与度。</li>
<li>methods: 该论文采用了练好LLM的 fine-tuning方法，并提出了一种AI copilot的设计方法以及模块化和提示工程技术来解决问题表述中的卡通问题。</li>
<li>results: 实验结果显示，通过该方法可以synthesize大型和复杂的企业优化问题表述，并且可以在生产规划中应用。<details>
<summary>Abstract</summary>
Business optimisation refers to the process of finding and implementing efficient and cost-effective means of operation to bring a competitive advantage for businesses. Synthesizing problem formulations is an integral part of business optimisation, which relies on human expertise to construct problem formulations using optimisation languages. Interestingly, with advancements in Large Language Models (LLMs), the human expertise needed in problem formulation can be minimized. However, developing an LLM for problem formulation is challenging, due to training data, token limitations, and lack of appropriate performance metrics. For the requirement of training data, recent attention has been directed towards fine-tuning pre-trained LLMs for downstream tasks rather than training an LLM from scratch for a specific task. In this paper, we adopt an LLM fine-tuning approach and propose an AI-Copilot for business optimisation problem formulation. For token limitations, we introduce modularization and prompt engineering techniques to synthesize complex problem formulations as modules that fit into the token limits of LLMs. Additionally, we design performance evaluation metrics that are better suited for assessing the accuracy and quality of problem formulations. The experiment results demonstrate that with this approach we can synthesize complex and large problem formulations for a typical business optimisation problem in production scheduling.
</details>
<details>
<summary>摘要</summary>
Despite the potential benefits of using LLMs for problem formulation, there are several challenges that need to be addressed. One of the main challenges is the lack of training data, which makes it difficult to train an LLM from scratch for a specific task. To address this challenge, recent attention has been directed towards fine-tuning pre-trained LLMs for downstream tasks.In this paper, we adopt an LLM fine-tuning approach and propose an AI-Copilot for business optimization problem formulation. To overcome the token limitations of LLMs, we introduce modularization and prompt engineering techniques to synthesize complex problem formulations as modules that fit into the token limits of LLMs. Additionally, we design performance evaluation metrics that are better suited for assessing the accuracy and quality of problem formulations.The experiment results demonstrate that with this approach, we can synthesize complex and large problem formulations for a typical business optimization problem in production scheduling. This shows that our proposed AI-Copilot can effectively assist businesses in optimizing their operations and gaining a competitive advantage.
</details></li>
</ul>
<hr>
<h2 id="MISFIT-V-Misaligned-Image-Synthesis-and-Fusion-using-Information-from-Thermal-and-Visual"><a href="#MISFIT-V-Misaligned-Image-Synthesis-and-Fusion-using-Information-from-Thermal-and-Visual" class="headerlink" title="MISFIT-V: Misaligned Image Synthesis and Fusion using Information from Thermal and Visual"></a>MISFIT-V: Misaligned Image Synthesis and Fusion using Information from Thermal and Visual</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13216">http://arxiv.org/abs/2309.13216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aadharc/Visual_Thermal_Image_Fusion">https://github.com/Aadharc/Visual_Thermal_Image_Fusion</a></li>
<li>paper_authors: Aadhar Chauhan, Isaac Remy, Danny Broyles, Karen Leung</li>
<li>for: 本研究旨在提高搜救队伍在霍尔风景中从空中视觉和热成像中检测人员，以提高搜救效率和准确率。</li>
<li>methods: 该研究提出了一种基于Generative Adversarial Network（GAN）和带通信机制的两元深度学习方法，名为Misaligned Image Synthesis and Fusion using Information from Thermal and Visual（MISFIT-V），用于把视觉和热成像模态进行图像 fusión。</li>
<li>results: 实验结果表明，MISFIT-V方法在环境因素不利的情况下具有更高的 robustness 性和精度，比如融合图像中的人员检测结果。<details>
<summary>Abstract</summary>
Detecting humans from airborne visual and thermal imagery is a fundamental challenge for Wilderness Search-and-Rescue (WiSAR) teams, who must perform this function accurately in the face of immense pressure. The ability to fuse these two sensor modalities can potentially reduce the cognitive load on human operators and/or improve the effectiveness of computer vision object detection models. However, the fusion task is particularly challenging in the context of WiSAR due to hardware limitations and extreme environmental factors. This work presents Misaligned Image Synthesis and Fusion using Information from Thermal and Visual (MISFIT-V), a novel two-pronged unsupervised deep learning approach that utilizes a Generative Adversarial Network (GAN) and a cross-attention mechanism to capture the most relevant features from each modality. Experimental results show MISFIT-V offers enhanced robustness against misalignment and poor lighting/thermal environmental conditions compared to existing visual-thermal image fusion methods.
</details>
<details>
<summary>摘要</summary>
搜寻人员从空中视觉和热影像中识别是搜寻和救援队（WiSAR）的基本挑战，需要在压力很大的情况下准确完成。将这两种感知模式融合可能可以减轻人工操作员的认知负担和/或提高计算机视觉对象检测模型的效果。然而，在WiSAR中融合任务 particullay challenging due to hardware limitations and extreme environmental factors。这篇文章介绍了一种新的两重无监督深度学习方法，即 Misaligned Image Synthesis and Fusion using Information from Thermal and Visual（MISFIT-V）。该方法使用生成 adversarial network（GAN）和跨注意机制来捕捉每个模式中最相关的特征。实验结果表明，MISFIT-V在不同的拍摄角度和热环境下具有更高的鲁棒性，相比之下现有的视觉热像重构方法。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Impact-of-Personality-on-Affective-States-from-Video-Game-Communication"><a href="#Assessing-the-Impact-of-Personality-on-Affective-States-from-Video-Game-Communication" class="headerlink" title="Assessing the Impact of Personality on Affective States from Video Game Communication"></a>Assessing the Impact of Personality on Affective States from Video Game Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13214">http://arxiv.org/abs/2309.13214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atieh Kashani, Johannes Pfau, Magy Seif El-Nasr</li>
<li>for: This paper explores the impact of personality on the way players express themselves affectively in a team-based collaborative alternate reality game.</li>
<li>methods: The authors collected chat logs from eleven players over two weeks, labeled them according to their affective state, and assessed the connection between them and the five-factor personality domains and facets using multi-linear regression.</li>
<li>results: The study found a series of reasonable correlations between (combinations of) personality variables and expressed affect, including increased confusion predicted by lower self-competence, personal annoyance predicted by vulnerability to stress, and expressing anger more often in players prone to anxiety.<details>
<summary>Abstract</summary>
Individual differences in personality determine our preferences, traits and values, which should similarly hold for the way we express ourselves. With current advancements and transformations of technology and society, text-based communication has become ordinary and often even surpasses natural voice conversations -- with distinct challenges and opportunities. In this exploratory work, we investigate the impact of personality on the tendency how players of a team-based collaborative alternate reality game express themselves affectively. We collected chat logs from eleven players over two weeks, labeled them according to their affective state, and assessed the connection between them and the five-factor personality domains and facets. After applying multi-linear regression, we found a series of reasonable correlations between (combinations of) personality variables and expressed affect -- as increased confusion could be predicted by lower self-competence (C1), personal annoyance by vulnerability to stress (N6) and expressing anger occured more often in players that are prone to anxiety (N1), less humble and modest (A5), think less carefully before they act (C6) and have higher neuroticism (N). Expanding the data set, sample size and input modalities in subsequent work, we aim to confirm these findings and reveal even more interesting connections that could inform affective computing and games user research equally.
</details>
<details>
<summary>摘要</summary>
人类个体差异对我们的偏好、特质和价值产生影响，这一点应该在我们的表达方式上也有影响。随着技术和社会的发展，文本基本上已经成为了日常交流的一种常见方式，有时甚至超过了自然语音交流。在这项探索性研究中，我们 investigate了玩家在团队合作 alternate reality 游戏中表达情感的影响。我们收集了11名玩家的聊天记录，将其分为不同情感状态，并评估了这些状态与五大人格特征域和特征之间的连接。经多线性回归分析，我们发现了一系列合理的相关关系，例如：增加混乱可以预测低自我竞争力（C1）、个人恼怒可以预测脆弱性（N6），表达愤怒更常见于具有 anxiety（N1）、不谦虚和谨慎（A5）、不思议行为（C6）和高度neurótico（N）。在后续工作中，我们计划扩大数据集、样本大小和输入模式，以确认这些发现和揭示更多的 interessante 连接，以便在情感计算和游戏用户研究中具有参考意义。
</details></li>
</ul>
<hr>
<h2 id="Intent-Aware-Autonomous-Driving-A-Case-Study-on-Highway-Merging-Scenarios"><a href="#Intent-Aware-Autonomous-Driving-A-Case-Study-on-Highway-Merging-Scenarios" class="headerlink" title="Intent-Aware Autonomous Driving: A Case Study on Highway Merging Scenarios"></a>Intent-Aware Autonomous Driving: A Case Study on Highway Merging Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13206">http://arxiv.org/abs/2309.13206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishtha Mahajan, Qi Zhang</li>
<li>for: 本研究使用汽车自动控制器之间的意图交换来促进协作。</li>
<li>methods: 我们在高速公路环境 simulator 中实现了意图分享任务，并在两个代理之间进行了 investigate  highway 合并场景中意图分享如何 помо助接收方调整其行为。</li>
<li>results: 我们发现，通过意图分享，接收方可以更好地适应高速公路合并场景，提高了合并效率和安全性。<details>
<summary>Abstract</summary>
In this work, we use the communication of intent as a means to facilitate cooperation between autonomous vehicle agents. Generally speaking, intents can be any reliable information about its future behavior that a vehicle communicates with another vehicle. We implement this as an intent-sharing task atop the merging environment in the simulator of highway-env, which provides a collection of environments for learning decision-making strategies for autonomous vehicles. Under a simple setting between two agents, we carefully investigate how intent-sharing can aid the receiving vehicle in adjusting its behavior in highway merging scenarios.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们使用意图通信作为自动驾驶车辆间合作的方式。一般来说，意图可以是任何可靠地关于未来行为的信息，车辆通过这些信息与另一辆车辆进行交流。我们在高速公路环境 simulator 中实现了意图分享任务，提供了一个收集多种决策策略学习自动驾驶车辆的环境。在简单的两辆车辆之间的设定下，我们仔细研究了意图分享如何帮助接收车辆在高速公路做入道场景中调整其行为。
</details></li>
</ul>
<hr>
<h2 id="A-Practical-Survey-on-Zero-shot-Prompt-Design-for-In-context-Learning"><a href="#A-Practical-Survey-on-Zero-shot-Prompt-Design-for-In-context-Learning" class="headerlink" title="A Practical Survey on Zero-shot Prompt Design for In-context Learning"></a>A Practical Survey on Zero-shot Prompt Design for In-context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13205">http://arxiv.org/abs/2309.13205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinheng Li<br>for: 这篇论文旨在探讨各种提示技术，包括简洁、连续、几何shot和零shot提示，以及它们对大语言模型（LLM）性能的影响。methods: 论文检讨了不同提示设计方法，包括手动设计、优化算法和评估方法，以优化LLM在多种任务上的性能。results: 论文总结了关键的研究成果，包括提示工程的方法学和贡献，以及评估提示性能的挑战。<details>
<summary>Abstract</summary>
The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, few-shot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single "best" prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, optimization techniques, and rigorous evaluation for more effective and efficient use of LLMs in various NLP tasks.
</details>
<details>
<summary>摘要</summary>
LLMs 的卓越发展对自然语言处理(NLP)任务带来了重要的改善。本文提供了对Context Learning技术的完整回顾，包括不同类型的提示，如离散、连续、少量和零 shot，以及它们对 LLM 性能的影响。我们探讨了不同的提示设计方法，如手动设计、优化算法和评估方法，以优化 LLM 在多种任务上的性能。我们的回顾包括关键的研究成果在提示工程学，讨论了他们的方法和贡献。此外，我们还探讨了评估提示性能的挑战，因为没有单一的 "最佳" 提示，以及考虑多种维度的益虑。结束语，本文强调了提示设计的重要性，并提供了手动设计、优化技术和严格评估的组合，以更有效地和高效地使用 LLM 在多种 NLP 任务中。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-and-Control-Mechanisms-Improve-Text-Readability-of-Biomedical-Abstracts"><a href="#Large-Language-Models-and-Control-Mechanisms-Improve-Text-Readability-of-Biomedical-Abstracts" class="headerlink" title="Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts"></a>Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13202">http://arxiv.org/abs/2309.13202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hecta-uom/plaba-mu">https://github.com/hecta-uom/plaba-mu</a></li>
<li>paper_authors: Zihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Matthew Shardlow, Goran Nenadic<br>for: 本研究旨在提高生物医学领域文献的可读性，使用自然语言处理（NLP）模型自动化报告简化任务，提高公共健康文化知识。methods: 本研究使用现代大语言模型（LLMs）对生物医学报告简化任务进行研究，包括领域细化和提示基本学习（PBL），以及控制符 token 机制。results: 研究使用了多种自动评价指标，包括 BLEU、ROUGE、SARI 和 BERTscore，以及人类评价。 BART-Large  WITH Control Token（BART-L-w-CT）机制得到了最高 SARI 分数46.54，T5-base 得到了最高 BERTscore 72.62。在人类评价中，BART-L-w-CTs 获得了更好的简单性分数（2.9 vs. 2.2），而 T5-Base 获得了更好的意义保持分数（3.1 vs. 2.6）。<details>
<summary>Abstract</summary>
Biomedical literature often uses complex language and inaccessible professional terminologies. That is why simplification plays an important role in improving public health literacy. Applying Natural Language Processing (NLP) models to automate such tasks allows for quick and direct accessibility for lay readers. In this work, we investigate the ability of state-of-the-art large language models (LLMs) on the task of biomedical abstract simplification, using the publicly available dataset for plain language adaptation of biomedical abstracts (\textbf{PLABA}). The methods applied include domain fine-tuning and prompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and BART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT, and 3) Control-token mechanisms on BART-based models. We used a range of automatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and also conducted human evaluations. BART-Large with Control Token (BART-L-w-CT) mechanisms reported the highest SARI score of 46.54 and T5-base reported the highest BERTscore 72.62. In human evaluation, BART-L-w-CTs achieved a better simplicity score over T5-Base (2.9 vs. 2.2), while T5-Base achieved a better meaning preservation score over BART-L-w-CTs (3.1 vs. 2.6). We also categorised the system outputs with examples, hoping this will shed some light for future research on this task. Our code, fine-tuned models, and data splits are available at \url{https://github.com/HECTA-UoM/PLABA-MU}
</details>
<details>
<summary>摘要</summary>
生物医学文献经常使用复杂的语言和不可接触的专业术语，这使得公众健康文化知识的提高受到了限制。因此，简化对于改善公众健康文化知识具有重要的作用。在这项工作中，我们研究了现状最佳的大型自然语言处理（NLP）模型在生物医学摘要简化任务上的能力，使用公共可用的PLABA数据集（plain language adaptation of biomedical abstracts）。我们使用的方法包括域特化 fine-tuning 和提示基本学习（PBL），其中包括：1）Encoder-decoder模型（T5、SciFive和BART），2）Decoder-only GPT模型（GPT-3.5和GPT-4）和3）BART基于模型中的控制符机制。我们使用了一系列自动评估指标，包括BLEU、ROUGE、SARI和BERTscore，并也进行了人类评估。BART-Large with Control Token（BART-L-w-CT）机制获得了46.54的SARI分数，T5-base获得了72.62的BERTscore。在人类评估中，BART-L-w-CTs在 simplicity 分数上赢得了2.9 VS T5-Base的2.2，而T5-Base在 meaning preservation 分数上赢得了3.1 VS BART-L-w-CTs的2.6。我们还将系统输出分类并提供了示例，希望这可以为未来这个任务提供一些灯光。我们的代码、精度调整模型和数据分割可以在https://github.com/HECTA-UoM/PLABA-MU 中获取。
</details></li>
</ul>
<hr>
<h2 id="Towards-Green-AI-in-Fine-tuning-Large-Language-Models-via-Adaptive-Backpropagation"><a href="#Towards-Green-AI-in-Fine-tuning-Large-Language-Models-via-Adaptive-Backpropagation" class="headerlink" title="Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation"></a>Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13192">http://arxiv.org/abs/2309.13192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pittisl/greentrainer">https://github.com/pittisl/greentrainer</a></li>
<li>paper_authors: Kai Huang, Hanyun Yin, Heng Huang, Wei Gao</li>
<li>for: 这个研究旨在提高大型自然语言模型（LLM）的精细化过程中的能效性，以减少环境影响。</li>
<li>methods: 这个研究使用了一新的绿色精细化技术（GreenTrainer），可以根据不同的网络层次和精细化目标，选择最适合的网络层次和精细化方法，以最大化精细化效率和降低总计算量（FLOPs）。</li>
<li>results: 实验结果显示，比较于精细化整个LLM模型，GreenTrainer可以降低总计算量（FLOPs）达64%，而且与其他已有的精细化技术相比，GreenTrainer可以实现更高的模型准确度和相似的总计算量降低。<details>
<summary>Abstract</summary>
Fine-tuning is the most effective way of adapting pre-trained large language models (LLMs) to downstream applications. With the fast growth of LLM-enabled AI applications and democratization of open-souced LLMs, fine-tuning has become possible for non-expert individuals, but intensively performed LLM fine-tuning worldwide could result in significantly high energy consumption and carbon footprint, which may bring large environmental impact. Mitigating such environmental impact towards Green AI directly correlates to reducing the FLOPs of fine-tuning, but existing techniques on efficient LLM fine-tuning can only achieve limited reduction of such FLOPs, due to their ignorance of the backpropagation cost in fine-tuning. To address this limitation, in this paper we present GreenTrainer, a new LLM fine-tuning technique that adaptively evaluates different tensors' backpropagation costs and contributions to the fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the most appropriate set of tensors in training. Such selection in GreenTrainer is made based on a given objective of FLOPs reduction, which can flexibly adapt to the carbon footprint in energy supply and the need in Green AI. Experiment results over multiple open-sourced LLM models and abstractive summarization datasets show that, compared to fine-tuning the whole LLM model, GreenTrainer can save up to 64% FLOPs in fine-tuning without any noticeable model accuracy loss. Compared to the existing fine-tuning techniques such as LoRa, GreenTrainer can achieve up to 4% improvement on model accuracy with on-par FLOPs reduction.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的先进修改是下游应用最有效的方法。随着AI应用的快速发展和开源LLM的普及，非专家个人也可以进行修改，但是全球范围内的修改具有极高的能源消耗和碳脚印，可能对环境产生很大的影响。为了 Mitigating这些环境影响，我们在这篇论文中提出了GreenTrainer，一种新的LLM修改技术，可以自动评估不同张量的反射成本和精度贡献，以最小化修改成本。这种选择在GreenTrainer中是基于一个给定的硬件成本目标，可以适应不同的能源供应和绿色AI的需求。实验结果表明，相比于整个LLM模型的修改，GreenTrainer可以在不同的开源LLM模型和概括摘要 datasets 上节省到64%的FLOPs，而无需 sacrifiSing model精度。相比之下，与LoRa等现有的修改技术，GreenTrainer可以达到4%的模型精度提升，同时具有相同的FLOPs减少。
</details></li>
</ul>
<hr>
<h2 id="Masked-Discriminators-for-Content-Consistent-Unpaired-Image-to-Image-Translation"><a href="#Masked-Discriminators-for-Content-Consistent-Unpaired-Image-to-Image-Translation" class="headerlink" title="Masked Discriminators for Content-Consistent Unpaired Image-to-Image Translation"></a>Masked Discriminators for Content-Consistent Unpaired Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13188">http://arxiv.org/abs/2309.13188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bonifazstuhr/feamgan">https://github.com/bonifazstuhr/feamgan</a></li>
<li>paper_authors: Bonifaz Stuhr, Jürgen Brauer, Bernhard Schick, Jordi Gonzàlez</li>
<li>for: 这篇论文的目的是提高零对零图像转换的效能，尤其是在实际应用中遇到的问题，例如内容不一致和模式转换问题。</li>
<li>methods: 这篇论文使用的方法包括对全球检测器的输入进行封页，并使用对照抽样法选取小图像组合，以及对生成器流中的内容统计进行选择性标准化。</li>
<li>results: 这篇论文的实验结果显示，使用这些方法可以大幅提高零对零图像转换的效能，特别是在实际应用中的图像转换和天气转换领域。此外，论文还提出了一个新的评估指标（cKVD），可以更好地评估图像转换的质量。<details>
<summary>Abstract</summary>
A common goal of unpaired image-to-image translation is to preserve content consistency between source images and translated images while mimicking the style of the target domain. Due to biases between the datasets of both domains, many methods suffer from inconsistencies caused by the translation process. Most approaches introduced to mitigate these inconsistencies do not constrain the discriminator, leading to an even more ill-posed training setup. Moreover, none of these approaches is designed for larger crop sizes. In this work, we show that masking the inputs of a global discriminator for both domains with a content-based mask is sufficient to reduce content inconsistencies significantly. However, this strategy leads to artifacts that can be traced back to the masking process. To reduce these artifacts, we introduce a local discriminator that operates on pairs of small crops selected with a similarity sampling strategy. Furthermore, we apply this sampling strategy to sample global input crops from the source and target dataset. In addition, we propose feature-attentive denormalization to selectively incorporate content-based statistics into the generator stream. In our experiments, we show that our method achieves state-of-the-art performance in photorealistic sim-to-real translation and weather translation and also performs well in day-to-night translation. Additionally, we propose the cKVD metric, which builds on the sKVD metric and enables the examination of translation quality at the class or category level.
</details>
<details>
<summary>摘要</summary>
通常的目标对于无配对图像到图像翻译是保持源图像和翻译图像的内容一致性，同时模仿目标领域的样式。由于两个频谱的数据集之间存在偏见，许多方法受到翻译过程中的不一致性的影响。大多数引入的方法不约束探测器，导致训练 setup 更加不确定。此外，这些方法没有考虑更大的融合尺度。在这种情况下，我们表明，对两个频谱的总探测器的输入进行内容基于的蒙版是可以减少内容不一致性的。然而，这种策略会导致蒙版过程中的痕迹。为了减少这些痕迹，我们引入了一个本地探测器，该探测器在两个小尺度的匹配对上运行。此外，我们采用这种匹配策略来选择全局输入尺度上的源和目标数据集的输入。此外，我们提出了内容基于的降ormalization来选择性地包含生成器流中的内容统计。在我们的实验中，我们发现我们的方法可以在实际图像到图像翻译中达到领先的性能，并且在天气翻译和白天到夜晚翻译中也表现良好。此外，我们提出了 cKVD 指标，它基于 sKVD 指标，可以对翻译质量进行分类或类别级别的评估。
</details></li>
</ul>
<hr>
<h2 id="Diagnosing-and-exploiting-the-computational-demands-of-videos-games-for-deep-reinforcement-learning"><a href="#Diagnosing-and-exploiting-the-computational-demands-of-videos-games-for-deep-reinforcement-learning" class="headerlink" title="Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning"></a>Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13181">http://arxiv.org/abs/2309.13181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lakshmi Narasimhan Govindarajan, Rex G Liu, Drew Linsley, Alekh Karkada Ashok, Max Reuter, Michael J Frank, Thomas Serre</li>
<li>for: 这篇论文旨在探讨深度强化学习（dRL）算法是否可以在视频游戏中学习如人类一样，以及这些成功是由视觉表示学习或强化学习算法的发现更好策略带来的。</li>
<li>methods: 作者提出了学习挑战诊断器（LCD）工具，用于分解任务中的视觉学习和强化学习需求。通过LCD，作者在Procgenbenchmark中发现了一种新的挑战分类，并证明这些预测具有高可靠性和可以指导算法开发。</li>
<li>results: 作者通过LCD发现了多种在优化dRL算法上整个视频游戏benchmark时出现的失败案例，并提供了更有效的进程路径。<details>
<summary>Abstract</summary>
Humans learn by interacting with their environments and perceiving the outcomes of their actions. A landmark in artificial intelligence has been the development of deep reinforcement learning (dRL) algorithms capable of doing the same in video games, on par with or better than humans. However, it remains unclear whether the successes of dRL models reflect advances in visual representation learning, the effectiveness of reinforcement learning algorithms at discovering better policies, or both. To address this question, we introduce the Learning Challenge Diagnosticator (LCD), a tool that separately measures the perceptual and reinforcement learning demands of a task. We use LCD to discover a novel taxonomy of challenges in the Procgen benchmark, and demonstrate that these predictions are both highly reliable and can instruct algorithmic development. More broadly, the LCD reveals multiple failure cases that can occur when optimizing dRL algorithms over entire video game benchmarks like Procgen, and provides a pathway towards more efficient progress.
</details>
<details>
<summary>摘要</summary>
人类学习通过与环境互动和行为结果互动。人工智能领域的一个里程碑是开发深度奖励学习（dRL）算法，能够在电子游戏中学习，与人类或更好的性能。然而，未知 Whether the successes of dRL models reflect advances in visual representation learning, the effectiveness of reinforcement learning algorithms at discovering better policies, or both。为解决这个问题，我们引入学习挑战评价器（LCD），一种能够分解任务的视觉和奖励学习需求。我们使用LCD发现了Procgenbenchmark中的一种新分类器，并证明这些预测具有高可靠性和可以指导算法开发。更广泛地说，LCD揭示了优化dRL算法整个电子游戏benchmark like Procgen时可能出现的多种失败情况，并提供了更有效的进程。
</details></li>
</ul>
<hr>
<h2 id="AI-Risk-Profiles-A-Standards-Proposal-for-Pre-Deployment-AI-Risk-Disclosures"><a href="#AI-Risk-Profiles-A-Standards-Proposal-for-Pre-Deployment-AI-Risk-Disclosures" class="headerlink" title="AI Risk Profiles: A Standards Proposal for Pre-Deployment AI Risk Disclosures"></a>AI Risk Profiles: A Standards Proposal for Pre-Deployment AI Risk Disclosures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13176">http://arxiv.org/abs/2309.13176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eli Sherman, Ian W. Eisenberg</li>
<li>for: 本研究旨在提出一种风险评估标准，用于导引下游决策，包括评估风险、购买和部署，以及指导法规制定。</li>
<li>methods: 本研究使用了作者提出的AI风险分类法，将广泛的风险提议分类到高级分类层次。furthermore, the authors propose a template-based methodology for collating risk information into a standard, yet flexible, structure.</li>
<li>results: 作者采用公开可用信息，应用这种方法对许多知名的AI系统进行了风险评估。结果显示，这种方法可以帮助consumers更好地理解AI系统的风险，并且可以导引下游决策。<details>
<summary>Abstract</summary>
As AI systems' sophistication and proliferation have increased, awareness of the risks has grown proportionally (Sorkin et al. 2023). In response, calls have grown for stronger emphasis on disclosure and transparency in the AI industry (NTIA 2023; OpenAI 2023b), with proposals ranging from standardizing use of technical disclosures, like model cards (Mitchell et al. 2019), to yet-unspecified licensing regimes (Sindhu 2023). Since the AI value chain is complicated, with actors representing various expertise, perspectives, and values, it is crucial that consumers of a transparency disclosure be able to understand the risks of the AI system the disclosure concerns. In this paper we propose a risk profiling standard which can guide downstream decision-making, including triaging further risk assessment, informing procurement and deployment, and directing regulatory frameworks. The standard is built on our proposed taxonomy of AI risks, which reflects a high-level categorization of the wide variety of risks proposed in the literature. We outline the myriad data sources needed to construct informative Risk Profiles and propose a template-based methodology for collating risk information into a standard, yet flexible, structure. We apply this methodology to a number of prominent AI systems using publicly available information. To conclude, we discuss design decisions for the profiles and future work.
</details>
<details>
<summary>摘要</summary>
随着人工智能系统的复杂性和普及度的增加，关注这些风险的意识也在不断增长（索金等2023年）。作为回应，各方强调了更加强制的披透和透明度在人工智能业务中（NTIA等2023年；OpenAI等2023年），并提出了从技术披透标准化到未定许可证 regime（ sindhu等2023年）。由于人工智能价值链非常复杂，各个参与者具有不同的专业知识、观点和价值观，因此在下游决策过程中，披透报告的消费者必须能够理解关注的人工智能系统风险。在这篇论文中，我们提议了一个风险评估标准，可以导引下游决策，包括抢救进一步风险评估、指导采购和部署，以及指导法规框架。这个标准基于我们提议的人工智能风险分类体系，该分类体系反映了Literature中提出的广泛的风险。我们描述了构建信息的各种数据来源，并提议一种模板基于的方法来整理风险信息 into a standard， yet flexible 的结构。我们应用这种方法到了一些公开available的人工智能系统上。最后，我们讨论了配置风险profile的设计决策和未来工作。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Efficient-Deep-Learning-Architectures-For-Side-Channel-Attacks-on-AES"><a href="#Investigating-Efficient-Deep-Learning-Architectures-For-Side-Channel-Attacks-on-AES" class="headerlink" title="Investigating Efficient Deep Learning Architectures For Side-Channel Attacks on AES"></a>Investigating Efficient Deep Learning Architectures For Side-Channel Attacks on AES</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13170">http://arxiv.org/abs/2309.13170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yohaï-Eliel Berreby, Laurent Sauvage</li>
<li>for: 这项研究是为了提高深度学习在嵌入式 криптографических应用中的攻击效率，并减少计算资源和数据量的成本。</li>
<li>methods: 这项研究使用了 JAX 框架，并 investigate 了不同的 Transformer 模型，以便复制和提高先前的结果。</li>
<li>results: 研究人员在 ANSSI Side-Channel Attack Database (ASCAD) 上实现了一些先前已知的攻击结果，并在这些结果的基础之上做出了进一步的改进。<details>
<summary>Abstract</summary>
Over the past few years, deep learning has been getting progressively more popular for the exploitation of side-channel vulnerabilities in embedded cryptographic applications, as it offers advantages in terms of the amount of attack traces required for effective key recovery. A number of effective attacks using neural networks have already been published, but reducing their cost in terms of the amount of computing resources and data required is an ever-present goal, which we pursue in this work. We focus on the ANSSI Side-Channel Attack Database (ASCAD), and produce a JAX-based framework for deep-learning-based SCA, with which we reproduce a selection of previous results and build upon them in an attempt to improve their performance. We also investigate the effectiveness of various Transformer-based models.
</details>
<details>
<summary>摘要</summary>
在过去几年，深度学习在嵌入式加密应用中利用侧渠攻击的潜力得到了普遍的推广，因为它在关键恢复方面提供了更多的优势。许多使用神经网络的有效攻击已经发表，但减少计算资源和数据需求的成本仍然是一个持续的目标。我们在这种工作中关注ASCAD侧渠攻击数据库（ANSSI Side-Channel Attack Database），并基于JAX框架开发了深度学习基于SCA的框架，可以重现一些先前的结果并将其扩展以提高性能。我们还研究了不同的Transformer模型的效果。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Are-Also-Good-Prototypical-Commonsense-Reasoners"><a href="#Large-Language-Models-Are-Also-Good-Prototypical-Commonsense-Reasoners" class="headerlink" title="Large Language Models Are Also Good Prototypical Commonsense Reasoners"></a>Large Language Models Are Also Good Prototypical Commonsense Reasoners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13165">http://arxiv.org/abs/2309.13165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenin Li, Qianglong Chen, Yin Zhang, Yifei Zhang, Hongxiang Yao</li>
<li>For: The paper aims to improve the performance of large language models on complex reasoning tasks by developing novel prompts that better support the models’ commonsense reasoning abilities.* Methods: The authors draw inspiration from the outputs of large models for tailored tasks and semi-automatically develop a set of novel prompts from multiple perspectives, including task-relevance, supportive evidence generation, and diverse path decoding.* Results: The experimental results on the ProtoQA dataset demonstrate that the proposed prompts can achieve a new state-of-the-art (SOTA) on the ProtoQA leaderboard, with improvements of 8% and 4% in the Max Answer@1 and Max Incorrect@1 scores, respectively, compared to the previous SOTA model. The generated Chain-of-Thought and knowledge also improve the interpretability of the model.<details>
<summary>Abstract</summary>
Commonsense reasoning is a pivotal skill for large language models, yet it presents persistent challenges in specific tasks requiring this competence. Traditional fine-tuning approaches can be resource-intensive and potentially compromise a model's generalization capacity. Furthermore, state-of-the-art language models like GPT-3.5 and Claude are primarily accessible through API calls, which makes fine-tuning models challenging. To address these challenges, we draw inspiration from the outputs of large models for tailored tasks and semi-automatically developed a set of novel prompts from several perspectives, including task-relevance, supportive evidence generation (e.g. chain-of-thought and knowledge), diverse path decoding to aid the model. Experimental results on ProtoQA dataset demonstrate that with better designed prompts we can achieve the new state-of-art(SOTA) on the ProtoQA leaderboard, improving the Max Answer@1 score by 8%, Max Incorrect@1 score by 4% (breakthrough 50% for the first time) compared to the previous SOTA model and achieved an improvement on StrategyQA and CommonsenseQA2.0 (3% and 1%, respectively). Furthermore, with the generated Chain-of-Thought and knowledge, we can improve the interpretability of the model while also surpassing the previous SOTA models. We hope that our work can provide insight for the NLP community to develop better prompts and explore the potential of large language models for more complex reasoning tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型的通质性理解是一项重要的技能，但是在特定任务中表现出 persistente 挑战。传统的精度调整方法可能是资源占用的和可能妨碍模型的总体化能力。此外，当前的语言模型如 GPT-3.5 和 Claude 都是通过 API 调用来访问，这使得模型的调整变得困难。为了解决这些挑战，我们从大型模型的输出中提取了特定任务的输出，并 semi-自动生成了一组新的提示，包括任务相关性、证据生成（如链条思维和知识）和多种路径解码，以帮助模型。实验结果表明，我们的提示设计可以超越前一个 SOTA 模型在 ProtoQA 数据集上的 Max Answer@1 得分，提高了8%，并且在 Max Incorrect@1 得分上提高了4%（打破了50%的首次纪录）。此外，我们还可以通过生成的链条思维和知识提高模型的解释性，并超越了前一个 SOTA 模型。我们希望，我们的工作可以为 NLP 社区提供灵感，开发更好的提示，探索大型语言模型在更复杂的理解任务中的潜在能力。
</details></li>
</ul>
<hr>
<h2 id="GAMIX-VAE-A-VAE-with-Gaussian-Mixture-Based-Posterior"><a href="#GAMIX-VAE-A-VAE-with-Gaussian-Mixture-Based-Posterior" class="headerlink" title="GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior"></a>GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13160">http://arxiv.org/abs/2309.13160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariano Rivera</li>
<li>for: 这篇论文探讨了变量自动编码器（VAEs）中关键的底下勒比级（KL）差异，它是生成模型和表示学习中机器学习中的一个重要组成部分。</li>
<li>methods: 该论文提出了一种新的ELBO定义，使用混合 Gaussian 来描述 posterior 概率分布，并在权重抑制方面添加了一个正则化项以避免减少抖动。它还使用 PatchGAN 探测器来提高 texture 的真实感。</li>
<li>results: 实验表明该方法可以生成真实的面孔，提供了一种可行的解决方案来增强 VAE 基于的生成模型。<details>
<summary>Abstract</summary>
Variational Autoencoders (VAEs) have become a cornerstone in generative modeling and representation learning within machine learning. This paper explores a nuanced aspect of VAEs, focusing on interpreting the Kullback Leibler (KL) Divergence, a critical component within the Evidence Lower Bound (ELBO) that governs the trade-off between reconstruction accuracy and regularization. While the KL Divergence enforces alignment between latent variable distributions and a prior imposing a structure on the overall latent space but leaves individual variable distributions unconstrained. The proposed method redefines the ELBO with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. Implementation details involve ResNetV2 architectures for both the Encoder and Decoder. The experiments demonstrate the ability to generate realistic faces, offering a promising solution for enhancing VAE based generative models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Contextual-Emotion-Estimation-from-Image-Captions"><a href="#Contextual-Emotion-Estimation-from-Image-Captions" class="headerlink" title="Contextual Emotion Estimation from Image Captions"></a>Contextual Emotion Estimation from Image Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13136">http://arxiv.org/abs/2309.13136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vera Yang, Archita Srivastava, Yasaman Etesam, Chuxuan Zhang, Angelica Lim</li>
<li>for: 这 paper 探索了 Whether Large Language Models (LLMs) 可以支持情绪 estimation 任务，通过首先生成图像描述，然后使用 LLM 进行推理。</li>
<li>methods: 这 paper 使用了 Computer Vision 方法来直接测量人们的情绪，并使用 LLM 进行推理。</li>
<li>results: 研究发现，GPT-3.5 模型可以提供 surprisingly 合理的情绪预测，但是准确度可以随情绪概念而变化。  Overall, the results suggest promise in the image captioning and LLM approach.<details>
<summary>Abstract</summary>
Emotion estimation in images is a challenging task, typically using computer vision methods to directly estimate people's emotions using face, body pose and contextual cues. In this paper, we explore whether Large Language Models (LLMs) can support the contextual emotion estimation task, by first captioning images, then using an LLM for inference. First, we must understand: how well do LLMs perceive human emotions? And which parts of the information enable them to determine emotions? One initial challenge is to construct a caption that describes a person within a scene with information relevant for emotion perception. Towards this goal, we propose a set of natural language descriptors for faces, bodies, interactions, and environments. We use them to manually generate captions and emotion annotations for a subset of 331 images from the EMOTIC dataset. These captions offer an interpretable representation for emotion estimation, towards understanding how elements of a scene affect emotion perception in LLMs and beyond. Secondly, we test the capability of a large language model to infer an emotion from the resulting image captions. We find that GPT-3.5, specifically the text-davinci-003 model, provides surprisingly reasonable emotion predictions consistent with human annotations, but accuracy can depend on the emotion concept. Overall, the results suggest promise in the image captioning and LLM approach.
</details>
<details>
<summary>摘要</summary>
人工智能识别人类情感是一项复杂的任务，通常使用计算机视觉方法直接测量人脸、姿势和上下文信息来确定人们的情感。在这篇论文中，我们考虑了使用大型自然语言模型（LLM）来支持情感识别任务。我们首先需要了解： LLM 如何识别人类情感吗？哪些信息使得它们能够确定情感呢？我们的首要挑战是构建一个描述人在场景中的自然语言描述，以便用 LLM 进行推理。为此，我们提出了一组面部、身体、互动和环境等自然语言描述器。我们使用它们手动生成了331个图像集EMOTIC中的图像caption和情绪标注。这些caption提供了可解释的表示方式，用于了解场景元素对情感识别在 LLM 和其他方法中的影响。其次，我们测试了一个大型自然语言模型（GPT-3.5）是否可以从图像caption中推断出情绪。我们发现，特别是text-davinci-003模型，能够提供相对准确的情绪预测，但是准确程度可能取决于情绪概念。总的来说，结果表明了图像captioning和LLM方法的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Insights-from-an-OTTR-centric-Ontology-Engineering-Methodology"><a href="#Insights-from-an-OTTR-centric-Ontology-Engineering-Methodology" class="headerlink" title="Insights from an OTTR-centric Ontology Engineering Methodology"></a>Insights from an OTTR-centric Ontology Engineering Methodology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13130">http://arxiv.org/abs/2309.13130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Blum, Basil Ell, Philipp Cimiano</li>
<li>for: This paper is written for the purpose of discussing the use of OTTR templates in ontology engineering for the domain of Material Science.</li>
<li>methods: The paper uses a bottom-up and top-down approach to ontology engineering, starting with existing data and using OTTR templates to feed the data into a knowledge graph.</li>
<li>results: The paper finds that OTTR templates are useful for communicating with domain experts and that the engineering process becomes flexible as a result of encapsulating modeling decisions.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍使用OTTR模板在材料科学领域的ontology工程。</li>
<li>methods: 这篇论文使用底层和顶层的方法来实现ontology工程，从现有数据开始，使用OTTR模板将数据feed到知识图。</li>
<li>results: 这篇论文发现OTTR模板在与领域专家交流时非常有用，并且因为模板封装了设计决策，因此工程过程变得更加灵活，可以轻松地修改设计决策。<details>
<summary>Abstract</summary>
OTTR is a language for representing ontology modeling patterns, which enables to build ontologies or knowledge bases by instantiating templates. Thereby, particularities of the ontological representation language are hidden from the domain experts, and it enables ontology engineers to, to some extent, separate the processes of deciding about what information to model from deciding about how to model the information, e.g., which design patterns to use. Certain decisions can thus be postponed for the benefit of focusing on one of these processes. To date, only few works on ontology engineering where ontology templates are applied are described in the literature.   In this paper, we outline our methodology and report findings from our ontology engineering activities in the domain of Material Science. In these activities, OTTR templates play a key role. Our ontology engineering process is bottom-up, as we begin modeling activities from existing data that is then, via templates, fed into a knowledge graph, and it is top-down, as we first focus on which data to model and postpone the decision of how to model the data.   We find, among other things, that OTTR templates are especially useful as a means of communication with domain experts. Furthermore, we find that because OTTR templates encapsulate modeling decisions, the engineering process becomes flexible, meaning that design decisions can be changed at little cost.
</details>
<details>
<summary>摘要</summary>
OTTR 是一种用于表示 ontology 模式的语言，它可以帮助建立 ontology 或知识库 by instantiating 模板。因此，ontological 表示语言中的特定特点被隐藏，使得域专家不必关注这些特点，可以更专注于决定需要模型的信息和使用哪些设计模式。这样可以抽象出一些决策，以便更专注于一个过程中。在现有文献中，只有一些关于 ontology 工程的研究描述了使用 ontology 模板。在本文中，我们介绍了我们的方法和在材料科学领域中的 ontology 工程活动的发现。在这些活动中，OTTR 模板扮演了关键的角色。我们的 ontology 工程过程是底向的，我们从现有数据开始，将其通过模板feed into 知识图，并是顶向的，我们首先决定需要模型的数据，然后决定如何模型数据。我们发现 OTTR 模板非常有用作域专家与之交流的工具。此外，我们发现因为 OTTR 模板封装了模型决策，工程过程变得灵活，可以在低成本下更改设计决策。
</details></li>
</ul>
<hr>
<h2 id="E-2-Equivariant-Graph-Planning-for-Navigation"><a href="#E-2-Equivariant-Graph-Planning-for-Navigation" class="headerlink" title="E(2)-Equivariant Graph Planning for Navigation"></a>E(2)-Equivariant Graph Planning for Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13043">http://arxiv.org/abs/2309.13043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linfeng Zhao, Hongyu Li, Taskin Padir, Huaizu Jiang, Lawson L. S. Wong</li>
<li>for: 提高机器人导航的学习效率和稳定性，满足实际应用中的需求。</li>
<li>methods: 利用欧几何同态性在规划中，实现参数共享和稳定的训练。在不结构化环境中，通过几何图形规划和对称性保持的消息传递网络实现值迭代。还提出了一种可学习的协变层，将特征映射到 DESIRED 空间。</li>
<li>results: 在五种多样化任务中，包括结构化和不结构化环境，以及已知和未知的目标点或Semantic goal，实现了训练效率、稳定性和泛化性的显著改进。<details>
<summary>Abstract</summary>
Learning for robot navigation presents a critical and challenging task. The scarcity and costliness of real-world datasets necessitate efficient learning approaches. In this letter, we exploit Euclidean symmetry in planning for 2D navigation, which originates from Euclidean transformations between reference frames and enables parameter sharing. To address the challenges of unstructured environments, we formulate the navigation problem as planning on a geometric graph and develop an equivariant message passing network to perform value iteration. Furthermore, to handle multi-camera input, we propose a learnable equivariant layer to lift features to a desired space. We conduct comprehensive evaluations across five diverse tasks encompassing structured and unstructured environments, along with maps of known and unknown, given point goals or semantic goals. Our experiments confirm the substantial benefits on training efficiency, stability, and generalization.
</details>
<details>
<summary>摘要</summary>
学习 robot 导航存在一个极其紧迫和挑战性的任务。因为实际世界数据的稀缺和高价，我们需要开发高效的学习方法。在本文中，我们利用二维 Navigation 中的欧几何 симметрия，来实现参数共享。为了处理无结构环境，我们将导航问题定义为在几何图形上进行规划，并开发了一个对称报essage passing网络来实现值迭代。此外，我们还提出了一个可学习的对称层，以提高多摄像头输入的特征提取。我们在五种不同的任务中进行了广泛的评估，包括结构化和无结构化环境，以及已知和未知的点目标或semantic目标。我们的实验表明，我们的方法可以提高训练效率、稳定性和泛化能力。
</details></li>
</ul>
<hr>
<h2 id="MosaicFusion-Diffusion-Models-as-Data-Augmenters-for-Large-Vocabulary-Instance-Segmentation"><a href="#MosaicFusion-Diffusion-Models-as-Data-Augmenters-for-Large-Vocabulary-Instance-Segmentation" class="headerlink" title="MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation"></a>MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13042">http://arxiv.org/abs/2309.13042</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiahao000/mosaicfusion">https://github.com/jiahao000/mosaicfusion</a></li>
<li>paper_authors: Jiahao Xie, Wei Li, Xiangtai Li, Ziwei Liu, Yew Soon Ong, Chen Change Loy</li>
<li>for: 这篇论文是为了提出一种新的扩展数据生成方法，以提高大词汇实例分割器的性能。</li>
<li>methods: 该方法使用了 diffusion-based 数据生成方法，不需要任何标注数据，可以使用存在的文本至图生成器来生成多个实例。</li>
<li>results: 实验结果表明，使用该方法可以生成大量的合理标注数据，特别是对于罕见和新类别。这有助于提高现有的实例分割器的性能，特别是对于罕见和新类别。<details>
<summary>Abstract</summary>
We present MosaicFusion, a simple yet effective diffusion-based data augmentation approach for large vocabulary instance segmentation. Our method is training-free and does not rely on any label supervision. Two key designs enable us to employ an off-the-shelf text-to-image diffusion model as a useful dataset generator for object instances and mask annotations. First, we divide an image canvas into several regions and perform a single round of diffusion process to generate multiple instances simultaneously, conditioning on different text prompts. Second, we obtain corresponding instance masks by aggregating cross-attention maps associated with object prompts across layers and diffusion time steps, followed by simple thresholding and edge-aware refinement processing. Without bells and whistles, our MosaicFusion can produce a significant amount of synthetic labeled data for both rare and novel categories. Experimental results on the challenging LVIS long-tailed and open-vocabulary benchmarks demonstrate that MosaicFusion can significantly improve the performance of existing instance segmentation models, especially for rare and novel categories. Code will be released at https://github.com/Jiahao000/MosaicFusion.
</details>
<details>
<summary>摘要</summary>
我们介绍MosaicFusion，一种简单 yet effective的扩散基于数据增强方法，用于大词汇实例分割。我们的方法不需要任何标注指导。我们使用两个关键设计来使用市场上可用的文本到图像扩散模型来生成对象实例和mask注释。首先，我们将图像canvas分成多个区域，并在不同的文本提示下进行单次扩散过程，以同时生成多个实例。其次，我们通过聚合层和扩散时间步骤之间的交叉注意力图来获得对象提示的集合，然后进行简单的阈值设定和边缘敏感处理来获得实例mask。无论精雕的设计，MosaicFusion可以生成大量的合成标注数据，特别是为罕见和新类别。我们的实验结果表明，MosaicFusion可以大幅提高现有实例分割模型的性能，特别是为罕见和新类别。代码将在https://github.com/Jiahao000/MosaicFusion中发布。
</details></li>
</ul>
<hr>
<h2 id="Memory-augmented-conformer-for-improved-end-to-end-long-form-ASR"><a href="#Memory-augmented-conformer-for-improved-end-to-end-long-form-ASR" class="headerlink" title="Memory-augmented conformer for improved end-to-end long-form ASR"></a>Memory-augmented conformer for improved end-to-end long-form ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13029">http://arxiv.org/abs/2309.13029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miamoto/conformer-ntm">https://github.com/miamoto/conformer-ntm</a></li>
<li>paper_authors: Carlos Carvalho, Alberto Abad</li>
<li>for: 用于自动语音识别（ASR）模型的改进，特别是对长句子的表现。</li>
<li>methods: 使用外部可微分储存网络（NTM）和encoder-decoder结构的协同作用，以扩展对长句子的泛化能力。</li>
<li>results: 在使用Librispeech数据集的train-clean-100和train-960集上，提出的模型比基eline conformer ohne memory更高的表现于长句子。<details>
<summary>Abstract</summary>
Conformers have recently been proposed as a promising modelling approach for automatic speech recognition (ASR), outperforming recurrent neural network-based approaches and transformers. Nevertheless, in general, the performance of these end-to-end models, especially attention-based models, is particularly degraded in the case of long utterances. To address this limitation, we propose adding a fully-differentiable memory-augmented neural network between the encoder and decoder of a conformer. This external memory can enrich the generalization for longer utterances since it allows the system to store and retrieve more information recurrently. Notably, we explore the neural Turing machine (NTM) that results in our proposed Conformer-NTM model architecture for ASR. Experimental results using Librispeech train-clean-100 and train-960 sets show that the proposed system outperforms the baseline conformer without memory for long utterances.
</details>
<details>
<summary>摘要</summary>
具有最新提议的具有竞争力的模型方法（Conformer）在自动语音识别（ASR）中表现出色，超过了基于回归神经网络的方法和变换器。然而，在总体来说，这些端到端模型，特别是带有注意力的模型，在长句子情况下表现较差。为解决这一限制，我们提议在编码器和解码器之间添加一个可微分的内存增强神经网络。这个外部内存可以为长句子提供更多的信息，从而提高系统的总体化能力。我们研究了神经图理 machine（NTM），从而得到我们的提议的 Conformer-NTM 模型体系结构。实验结果表明，提议的系统在 Librispeech train-clean-100 和 train-960 集上比基础 Conformer  ohne 内存表现出色。
</details></li>
</ul>
<hr>
<h2 id="OpportunityFinder-A-Framework-for-Automated-Causal-Inference"><a href="#OpportunityFinder-A-Framework-for-Automated-Causal-Inference" class="headerlink" title="OpportunityFinder: A Framework for Automated Causal Inference"></a>OpportunityFinder: A Framework for Automated Causal Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13103">http://arxiv.org/abs/2309.13103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy Nguyen, Prince Grover, Devashish Khatwani</li>
<li>for: 用于执行对屏幕数据进行多种 causal inference 研究，为非专家用户提供可编程代码的框架。</li>
<li>methods: 使用 raw 观察数据和配置文件，触发管道进行数据检查&#x2F;处理，选择适合的算法执行 causal 研究，并返回对 outcome 的 causal 影响，以及敏感性和稳定性结果。</li>
<li>results: 返回 causal 影响的结果，包括对 outcome 的 causal 影响，以及敏感性和稳定性结果。<details>
<summary>Abstract</summary>
We introduce OpportunityFinder, a code-less framework for performing a variety of causal inference studies with panel data for non-expert users. In its current state, OpportunityFinder only requires users to provide raw observational data and a configuration file. A pipeline is then triggered that inspects/processes data, chooses the suitable algorithm(s) to execute the causal study. It returns the causal impact of the treatment on the configured outcome, together with sensitivity and robustness results. Causal inference is widely studied and used to estimate the downstream impact of individual's interactions with products and features. It is common that these causal studies are performed by scientists and/or economists periodically. Business stakeholders are often bottle-necked on scientist or economist bandwidth to conduct causal studies. We offer OpportunityFinder as a solution for commonly performed causal studies with four key features: (1) easy to use for both Business Analysts and Scientists, (2) abstraction of multiple algorithms under a single I/O interface, (3) support for causal impact analysis under binary treatment with panel data and (4) dynamic selection of algorithm based on scale of data.
</details>
<details>
<summary>摘要</summary>
我们介绍OpportunityFinder，一个无程式码框架，用于实现各种对组合数据进行可能性推论的不专家用户。目前情况下，OpportunityFinder只需用户提供原始观察数据和配置文件，然后触发一个管道，将数据进行检查和处理，选择适当的算法来执行可能性研究。它返回对定结果的影响，以及敏感度和稳定性结果。可能性推论广泛研究和使用，用于估计个人对产品和功能互动所产生的下游影响。这些可能性研究通常由科学家和/或经济学家定期进行。企业决策者往往因为科学家或经济学家的专业压力而受到瓶颈。我们提供OpportunityFinder作为常见的可能性研究解决方案，具有以下四个关键特点：1. 易用，适合商业分析师和科学家使用。2. 多种算法的抽象，通过单一的输入界面进行处理。3. 支持对组合数据进行可能性影响分析，并且仅需进行二进制对待。4. 基于数据的尺度进行动态算法选择。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-Deep-Learning-based-Approach-for-Optimal-Genotype-by-Environment-Selection"><a href="#A-Hybrid-Deep-Learning-based-Approach-for-Optimal-Genotype-by-Environment-Selection" class="headerlink" title="A Hybrid Deep Learning-based Approach for Optimal Genotype by Environment Selection"></a>A Hybrid Deep Learning-based Approach for Optimal Genotype by Environment Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13021">http://arxiv.org/abs/2309.13021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Khalilzadeh, Motahareh Kashanian, Saeed Khaki, Lizhi Wang</li>
<li>For: The paper aims to improve crop yield prediction by integrating weather data across the growing season, especially for different crop varieties, to understand their adaptability in the face of climate change.* Methods: The authors used a dataset of 93,028 training records and 10,337 test records, covering 159 locations across 28 U.S. states and Canadian provinces over 13 years (2003-2015). They developed two novel convolutional neural network (CNN) architectures: the CNN-DNN model and the CNN-LSTM-DNN model. They also used the Generalized Ensemble Method (GEM) to determine optimal model weights.* Results: The GEM model achieved lower RMSE (5.55% to 39.88%), reduced MAE (5.34% to 43.76%), and higher correlation coefficients (1.1% to 10.79%) compared to baseline models. The CNN-DNN model was used to identify top-performing genotypes for various locations and weather conditions, aiding genotype selection based on weather variables.Here are the three points in Simplified Chinese text:* For: 这个论文目的是提高农业实践中的作物产量预测，以便更好地理解气候变化对作物的适应性。* Methods: 作者使用了一个包含93,028个训练记录和10,337个测试记录的数据集，覆盖了28个美国州和加拿大省的159个地点，时间跨度为13年（2003-2015）。他们开发了两种新的卷积神经网络模型：CNN-DNN模型和CNN-LSTM-DNN模型。他们还使用了通用ensemble方法（GEM）来确定优化模型的权重。* Results: GEM模型在测试数据上实现了较低的RMSE（5.55%到39.88%）、reduced MAE（5.34%到43.76%）和高于基eline模型的 correlation coefficient（1.1%到10.79%）。CNN-DNN模型用于在不同的地点和气候条件下预测最高产量的种子，帮助选择基于气候变量的种子。<details>
<summary>Abstract</summary>
Precise crop yield prediction is essential for improving agricultural practices and ensuring crop resilience in varying climates. Integrating weather data across the growing season, especially for different crop varieties, is crucial for understanding their adaptability in the face of climate change. In the MLCAS2021 Crop Yield Prediction Challenge, we utilized a dataset comprising 93,028 training records to forecast yields for 10,337 test records, covering 159 locations across 28 U.S. states and Canadian provinces over 13 years (2003-2015). This dataset included details on 5,838 distinct genotypes and daily weather data for a 214-day growing season, enabling comprehensive analysis. As one of the winning teams, we developed two novel convolutional neural network (CNN) architectures: the CNN-DNN model, combining CNN and fully-connected networks, and the CNN-LSTM-DNN model, with an added LSTM layer for weather variables. Leveraging the Generalized Ensemble Method (GEM), we determined optimal model weights, resulting in superior performance compared to baseline models. The GEM model achieved lower RMSE (5.55% to 39.88%), reduced MAE (5.34% to 43.76%), and higher correlation coefficients (1.1% to 10.79%) when evaluated on test data. We applied the CNN-DNN model to identify top-performing genotypes for various locations and weather conditions, aiding genotype selection based on weather variables. Our data-driven approach is valuable for scenarios with limited testing years. Additionally, a feature importance analysis using RMSE change highlighted the significance of location, MG, year, and genotype, along with the importance of weather variables MDNI and AP.
</details>
<details>
<summary>摘要</summary>
precisión del cultivo de precisión es esencial para mejorar las prácticas agrícolas y asegurar la resistencia de los cultivos en climas variables. Integrar los datos meteorológicos durante la temporada de crecimiento, especialmente para diferentes variedades de cultivos, es crucial para comprender su adaptabilidad enfrentada al cambio climático. En el Desafío de Predicción de Yield de MLCAS2021, utilizamos un conjunto de datos de entrenamiento que comprendía 93,028 registros para predecir los rendimientos para 10,337 registros de prueba, que cubrían 159 ubicaciones en 28 estados y provincias canadiences durante 13 años (2003-2015). Este conjunto de datos incluyó detalles sobre 5,838 genotipos distinctos y datos meteorológicos diarios para una temporada de crecimiento de 214 días, lo que permitió un análisis exhaustivo. Como uno de los equipos ganadores, desarrollamos dos arquitecturas de red neuronal convolutional (CNN) nuevas: el modelo CNN-DNN, que combina redes neuronales convolutional y fully connected, y el modelo CNN-LSTM-DNN, con una capa adicional de LSTM para variables meteorológicas. Al utilizar el Método de Ensemble Generalizado (GEM), determinamos los pesos óptimos del modelo, lo que resultó en una performance superior a los modelos de referencia. El modelo GEM obtuvo una RMSE reducida (del 5,55% al 39,88%), una MAE reducida (del 5,34% al 43,76%) y coeficientes de correlación más altos (del 1,1% al 10,79%) cuando se evaluó en datos de prueba. Aplicamos el modelo CNN-DNN para identificar los genotipos más renderos para diferentes ubicaciones y condiciones meteorológicas, lo que es útil para la selección de genotipos basada en variables meteorológicas. Nuestra aproximación basada en datos es valiosa para escenarios con años de prueba limitados. Además, un análisis de importancia de características utilizando el cambio de RMSE destacó la importancia de la ubicación, MG, año y genotipo, así como la importancia de las variables meteorológicas MDNI y AP.
</details></li>
</ul>
<hr>
<h2 id="Efficient-N-M-Sparse-DNN-Training-Using-Algorithm-Architecture-and-Dataflow-Co-Design"><a href="#Efficient-N-M-Sparse-DNN-Training-Using-Algorithm-Architecture-and-Dataflow-Co-Design" class="headerlink" title="Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design"></a>Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13015">http://arxiv.org/abs/2309.13015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Fang, Wei Sun, Aojun Zhou, Zhongfeng Wang</li>
<li>for: 这个论文主要针对的是如何使用粗糙训练来降低深度神经网络（DNN）的计算成本，同时保持高度准确性。</li>
<li>methods: 本论文提出了一种 computation-efficient 的训练方案，包括算法、架构和数据流程合理设计。在算法层面，提出了一种双向权重剔除方法（BDWP），可以在前向和反向传播中利用 N：M 粗糙性来减少计算成本，同时保持模型准确性。在架构层面，提出了一种专门用于 DNN 训练的粗糙加速器（SAT），可以支持常见的稠密操作以及计算效率高的 N：M 粗糙操作。在数据流程层面，提出了多种优化方法，包括交叉映射、预生成 N：M 粗糙权重和离线调度等，以提高 SAT 的计算效率。</li>
<li>results: 实验结果显示，使用 SAT 加速器和 BDWP 粗糙训练方法，在 Xilinx VCU1525 FPGA 卡上使用不同的 DNN 模型和数据集，可以实现 average 速度提升1.75倍，同时减少了模型精度下降的0.56%。此外，我们的训练方案可以提高训练吞吐量2.97<del>25.22倍和能效率1.36</del>3.58倍 compared to 先前的 FPGA 加速器。<details>
<summary>Abstract</summary>
Sparse training is one of the promising techniques to reduce the computational cost of DNNs while retaining high accuracy. In particular, N:M fine-grained structured sparsity, where only N out of consecutive M elements can be nonzero, has attracted attention due to its hardware-friendly pattern and capability of achieving a high sparse ratio. However, the potential to accelerate N:M sparse DNN training has not been fully exploited, and there is a lack of efficient hardware supporting N:M sparse training. To tackle these challenges, this paper presents a computation-efficient training scheme for N:M sparse DNNs using algorithm, architecture, and dataflow co-design. At the algorithm level, a bidirectional weight pruning method, dubbed BDWP, is proposed to leverage the N:M sparsity of weights during both forward and backward passes of DNN training, which can significantly reduce the computational cost while maintaining model accuracy. At the architecture level, a sparse accelerator for DNN training, namely SAT, is developed to neatly support both the regular dense operations and the computation-efficient N:M sparse operations. At the dataflow level, multiple optimization methods ranging from interleave mapping, pre-generation of N:M sparse weights, and offline scheduling, are proposed to boost the computational efficiency of SAT. Finally, the effectiveness of our training scheme is evaluated on a Xilinx VCU1525 FPGA card using various DNN models and datasets. Experimental results show the SAT accelerator with the BDWP sparse training method under 2:8 sparse ratio achieves an average speedup of 1.75x over that with the dense training, accompanied by a negligible accuracy loss of 0.56% on average. Furthermore, our proposed training scheme significantly improves the training throughput by 2.97~25.22x and the energy efficiency by 1.36~3.58x over prior FPGA-based accelerators.
</details>
<details>
<summary>摘要</summary>
\begin{blockquote}稀疏训练是深度学习模型的一种有前途的技术，可以降低深度学习模型的计算成本，保持高度准确。特别是N：M精细结构稀疏，在N个连续M个元素中只有N个可以非零，这种硬件友好的模式和高度稀疏比例的实现，引起了关注。然而，N：M稀疏深度学习训练的潜在加速仍未得到完全利用，缺少高效的硬件支持。为了解决这些挑战，本文提出了一种计算效率高的训练方案 для N：M稀疏深度学习模型，通过算法、建筑和数据流合理设计。在算法层面，我们提出了一种双向权重减少方法，称为BDWP，可以在深度学习模型的前向和反向传播中利用N：M稀疏的权重，大幅降低计算成本，保持模型准确性。在建筑层面，我们开发了一种适用于深度学习训练的稀疏加速器，称为SAT，可以方便支持常见的密集操作以及计算效率的N：M稀疏操作。在数据流层面，我们提出了多种优化方法，从批量映射、预生成N：M稀疏权重到离线调度，以提高SAT的计算效率。实验结果表明，使用我们的训练方案和SAT加速器，N：M稀疏深度学习模型在Xilinx VCU1525 FPGA卡上实现了1.75倍的速度提升，相对于密集训练方案，平均准确性损失为0.56%。此外，我们的训练方案可以提高训练吞吐量2.97~25.22倍和能效率1.36~3.58倍，至于先前的FPGA加速器。\end{blockquote}Note that the translation is done using the Simplified Chinese language setting, which may not be exactly the same as the Traditional Chinese language setting used in Taiwan.
</details></li>
</ul>
<hr>
<h2 id="ReConcile-Round-Table-Conference-Improves-Reasoning-via-Consensus-among-Diverse-LLMs"><a href="#ReConcile-Round-Table-Conference-Improves-Reasoning-via-Consensus-among-Diverse-LLMs" class="headerlink" title="ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs"></a>ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13007">http://arxiv.org/abs/2309.13007</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dinobby/reconcile">https://github.com/dinobby/reconcile</a></li>
<li>paper_authors: Justin Chih-Yao Chen, Swarnadeep Saha, Mohit Bansal</li>
<li>for: 提高大型自然语言模型（LLM）的复杂理解能力</li>
<li>methods: 提出ReConcile模型，利用多个LLM代理在圆桌会议中互动，提高代理之间的多样化思维和沟通，以提高LLM的复杂理解能力</li>
<li>results: 在多个benchmark上实验表明，ReConcile模型可以大幅提高LLM的复杂理解能力，比对 Singleshot baseline和多代理baseline高出7.7%，并且在一些数据集上甚至超过GPT-4的表现。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcile determines the final answer by leveraging the confidence of each agent in a weighted voting scheme. We implement ReConcile with ChatGPT, Bard, and Claude2 as the three agents. Our experimental results on various benchmarks demonstrate that ReConcile significantly enhances the reasoning performance of the agents (both individually and as a team), surpassing prior single-agent and multi-agent baselines by 7.7% and also outperforming GPT-4 on some of these datasets. We also experiment with GPT-4 itself as one of the agents in ReConcile and demonstrate that its initial performance also improves by absolute 10.0% through discussion and feedback from other agents. Finally, we also analyze the accuracy after every round and observe that ReConcile achieves better and faster consensus between agents, compared to a multi-agent debate baseline. Our code is available at: https://github.com/dinobby/ReConcile
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）仍然面临复杂的理解任务。 motivated by the society of minds（Minsky，1988），我们提议ReConcile，一种多模型多代理框架，设计为多个LLM代理在多种不同的思路和讨论中促进多元思维和讨论，以提高共识。ReConcile通过多轮讨论、学习感SErrors他们的答案，并使用可信度权重投票机制来提高LLMs的理解能力。在每轮讨论中，ReConcile通过'讨论提示'来initiate discussion between agents，该提示包括每个代理在上一轮的答案和解释、uncertainties，以及answer-rectifying human explanations，用于说服其他代理。这些讨论提示使每个代理可以根据其他代理的启示修改其答案。当讨论结束并达成共识时，ReConcile使用每个代理的可信度在权重投票机制中确定最终答案。我们在ChatGPT、Bard和Claude2作为三个代理来实现ReConcile。我们的实验结果表明，ReConcile可以明显提高LLMs的理解能力（both individually and as a team），比对前的单机和多机基线高出7.7%，同时也超过GPT-4在一些数据集上的性能。我们还在GPT-4作为一个代理参与ReConcile，并证明其初始性能也提高了绝对10.0%通过对其他代理的讨论和反馈。最后，我们还分析了每轮的准确率，发现ReConcile在多个代理之间达成共识的速度和准确率比multi-agent debate基线更高。我们的代码可以在https://github.com/dinobby/ReConcile上获取。
</details></li>
</ul>
<hr>
<h2 id="Pursuing-Counterfactual-Fairness-via-Sequential-Autoencoder-Across-Domains"><a href="#Pursuing-Counterfactual-Fairness-via-Sequential-Autoencoder-Across-Domains" class="headerlink" title="Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains"></a>Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13005">http://arxiv.org/abs/2309.13005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujie Lin, Chen Zhao, Minglai Shao, Baoluo Meng, Xujiang Zhao, Haifeng Chen</li>
<li>for: 提高机器学习系统在不同频率的数据上的性能，并在数据分布逐渐发展的过程中保持公平性。</li>
<li>methods: 提出了一种名为Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder（CDSAE）的创新框架，该框架可以分离类别特征中的环境信息和敏感特征，从而提高模型在多样化和不 Familiar 频率上的泛化性能，同时也能够有效地解决不公正分类问题。</li>
<li>results: 通过在 sintetic 和实际世界数据集上的验证，证明了我们的方法可以提高准确率，同时保持公平性在数据分布逐渐发展的过程中。<details>
<summary>Abstract</summary>
Recognizing the prevalence of domain shift as a common challenge in machine learning, various domain generalization (DG) techniques have been developed to enhance the performance of machine learning systems when dealing with out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data distributions can gradually change across a sequence of sequential domains. While current methodologies primarily focus on improving model effectiveness within these new domains, they often overlook fairness issues throughout the learning process. In response, we introduce an innovative framework called Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder (CDSAE). This approach effectively separates environmental information and sensitive attributes from the embedded representation of classification features. This concurrent separation not only greatly improves model generalization across diverse and unfamiliar domains but also effectively addresses challenges related to unfair classification. Our strategy is rooted in the principles of causal inference to tackle these dual issues. To examine the intricate relationship between semantic information, sensitive attributes, and environmental cues, we systematically categorize exogenous uncertainty factors into four latent variables: 1) semantic information influenced by sensitive attributes, 2) semantic information unaffected by sensitive attributes, 3) environmental cues influenced by sensitive attributes, and 4) environmental cues unaffected by sensitive attributes. By incorporating fairness regularization, we exclusively employ semantic information for classification purposes. Empirical validation on synthetic and real-world datasets substantiates the effectiveness of our approach, demonstrating improved accuracy levels while ensuring the preservation of fairness in the evolving landscape of continuous domains.
</details>
<details>
<summary>摘要</summary>
recognizing the prevalence of domain shift as a common challenge in machine learning, various domain generalization (DG) techniques have been developed to enhance the performance of machine learning systems when dealing with out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data distributions can gradually change across a sequence of sequential domains. While current methodologies primarily focus on improving model effectiveness within these new domains, they often overlook fairness issues throughout the learning process. In response, we introduce an innovative framework called Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder (CDSAE). This approach effectively separates environmental information and sensitive attributes from the embedded representation of classification features. This concurrent separation not only greatly improves model generalization across diverse and unfamiliar domains but also effectively addresses challenges related to unfair classification. Our strategy is rooted in the principles of causal inference to tackle these dual issues. To examine the intricate relationship between semantic information, sensitive attributes, and environmental cues, we systematically categorize exogenous uncertainty factors into four latent variables: 1) semantic information influenced by sensitive attributes, 2) semantic information unaffected by sensitive attributes, 3) environmental cues influenced by sensitive attributes, and 4) environmental cues unaffected by sensitive attributes. By incorporating fairness regularization, we exclusively employ semantic information for classification purposes. Empirical validation on synthetic and real-world datasets substantiates the effectiveness of our approach, demonstrating improved accuracy levels while ensuring the preservation of fairness in the evolving landscape of continuous domains.
</details></li>
</ul>
<hr>
<h2 id="Audience-specific-Explanations-for-Machine-Translation"><a href="#Audience-specific-Explanations-for-Machine-Translation" class="headerlink" title="Audience-specific Explanations for Machine Translation"></a>Audience-specific Explanations for Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12998">http://arxiv.org/abs/2309.12998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renhan Lou, Jan Niehues</li>
<li>for: 解决机器翻译中存在的certain words even if translated can cause incomprehension of the target language audience due to different cultural backgrounds.</li>
<li>methods: 我们提出了一种 semi-automatic technique to extract these explanations from a large parallel corpus.</li>
<li>results: 我们的方法能够从英语-&gt;德语、英语-&gt;法语和英语-&gt;中文语对取得较好的结果，其中有超过10%的句子包含解释，而原始句子中只有1.9%包含解释。<details>
<summary>Abstract</summary>
In machine translation, a common problem is that the translation of certain words even if translated can cause incomprehension of the target language audience due to different cultural backgrounds. A solution to solve this problem is to add explanations for these words. In a first step, we therefore need to identify these words or phrases. In this work we explore techniques to extract example explanations from a parallel corpus. However, the sparsity of sentences containing words that need to be explained makes building the training dataset extremely difficult. In this work, we propose a semi-automatic technique to extract these explanations from a large parallel corpus. Experiments on English->German language pair show that our method is able to extract sentence so that more than 10% of the sentences contain explanation, while only 1.9% of the original sentences contain explanations. In addition, experiments on English->French and English->Chinese language pairs also show similar conclusions. This is therefore an essential first automatic step to create a explanation dataset. Furthermore we show that the technique is robust for all three language pairs.
</details>
<details>
<summary>摘要</summary>
在机器翻译中，一个常见的问题是翻译某些词汇，即使翻译成功，也可能导致目标语言群体的不理解，因为不同的文化背景。为解决这问题，一种解决方案是添加解释。在这项工作中，我们 explore 技术来提取示例解释。然而，在建立训练集时，由于翻译后的句子中包含需要解释的词汇的稀缺性，使得建立训练集非常困难。因此，我们提出了一种半自动的提取方法，来从大量的平行词典中提取示例解释。实验表明，我们的方法可以从英语->德语、英语->法语和英语->中文三种语言对的平行词典中提取句子，使得超过10%的句子包含解释，而原始句子中只有1.9%的句子包含解释。此外，我们还展示了该技术在三种语言对上的稳定性。因此，这是一项重要的自动第一步，用于创建解释数据集。
</details></li>
</ul>
<hr>
<h2 id="Higher-order-Graph-Convolutional-Network-with-Flower-Petals-Laplacians-on-Simplicial-Complexes"><a href="#Higher-order-Graph-Convolutional-Network-with-Flower-Petals-Laplacians-on-Simplicial-Complexes" class="headerlink" title="Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes"></a>Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12971">http://arxiv.org/abs/2309.12971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeniSoida/pl1">https://github.com/zeniSoida/pl1</a></li>
<li>paper_authors: Yiming Huang, Yujie Zeng, Qiang Wu, Linyuan Lü</li>
<li>for: 这 paper 的目的是提出一种基于 simplicial complexes (SCs) 的高级别征特征学习方法，以增强 graph neural network (GNN) 的表达能力。</li>
<li>methods: 该方法基于 Flower-Petals (FP) 模型，并使用 learnable graph filters 来识别不同的高级别交互强度。</li>
<li>results: 实验结果表明，提出的模型可以在多种图任务上达到 state-of-the-art (SOTA) 性能，并且提供一个可扩展和灵活的解决方案来探索图中的高级别交互。<details>
<summary>Abstract</summary>
Despite the recent successes of vanilla Graph Neural Networks (GNNs) on many tasks, their foundation on pairwise interaction networks inherently limits their capacity to discern latent higher-order interactions in complex systems. To bridge this capability gap, we propose a novel approach exploiting the rich mathematical theory of simplicial complexes (SCs) - a robust tool for modeling higher-order interactions. Current SC-based GNNs are burdened by high complexity and rigidity, and quantifying higher-order interaction strengths remains challenging. Innovatively, we present a higher-order Flower-Petals (FP) model, incorporating FP Laplacians into SCs. Further, we introduce a Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians, capable of discerning intrinsic features across varying topological scales. By employing learnable graph filters, a parameter group within each FP Laplacian domain, we can identify diverse patterns where the filters' weights serve as a quantifiable measure of higher-order interaction strengths. The theoretical underpinnings of HiGCN's advanced expressiveness are rigorously demonstrated. Additionally, our empirical investigations reveal that the proposed model accomplishes state-of-the-art (SOTA) performance on a range of graph tasks and provides a scalable and flexible solution to explore higher-order interactions in graphs.
</details>
<details>
<summary>摘要</summary>
尽管最近的vanilla图 neural network (GNN)在许多任务上表现出色，但它们的基础是对应的对之间互动网络，因此它们无法自然地捕捉复杂系统中隐藏的高阶互动。为bridge这个能力差距，我们提出了一种新的方法，利用 simplicial complexes (SC) 的丰富数学理论 - 一种可靠的工具 для模型高阶互动。现有的 SC 基于 GNN 受到高复杂性和僵化的限制，同时量化高阶互动强度仍然是挑战。我们创新地提出了一种高阶花 petal (FP) 模型，将 FP  Laplacians 引入 SC 中。此外，我们还介绍了一种基于 FP Laplacians 的高阶图卷积网络 (HiGCN)，可以在不同的 topological scale 上捕捉到系统内部的自适应特征。通过使用可学习的图滤波器，每个 FP Laplacian 域中的参数组，我们可以识别出多种具有不同特征的图pattern，其中滤波器的权重serve as a quantifiable measure of high-order interaction strengths。我们的理论基础的进一步证明和实验研究表明，提出的模型可以在许多图任务上达到领先的性能水平，并提供一个可扩展和灵活的解决方案来探索图中的高阶互动。
</details></li>
</ul>
<hr>
<h2 id="Trusta-Reasoning-about-Assurance-Cases-with-Formal-Methods-and-Large-Language-Models"><a href="#Trusta-Reasoning-about-Assurance-Cases-with-Formal-Methods-and-Large-Language-Models" class="headerlink" title="Trusta: Reasoning about Assurance Cases with Formal Methods and Large Language Models"></a>Trusta: Reasoning about Assurance Cases with Formal Methods and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12941">http://arxiv.org/abs/2309.12941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zezhong Chen, Yuxin Deng, Wenjie Du</li>
<li>for:  This paper focuses on the development of a tool called Trustworthiness Derivation Tree Analyzer (Trusta) that automates the construction and verification of assurance cases for safety-critical systems.</li>
<li>methods:  The tool uses formal methods, such as Prolog and constraint solvers like Z3 and MONA, to automatically reason about assurance cases. It also utilizes large language models like ChatGPT-3.5, ChatGPT-4, and PaLM 2 to generate and evaluate assurance cases, allowing for interactive human examination and modification.</li>
<li>results:  The paper presents several industrial case studies that demonstrate the practical value of Trusta in finding subtle issues that are typically missed in manual inspection, and shows that the tool can quickly and efficiently enhance the assurance case development process.<details>
<summary>Abstract</summary>
Assurance cases can be used to argue for the safety of products in safety engineering. In safety-critical areas, the construction of assurance cases is indispensable. Trustworthiness Derivation Trees (TDTs) enhance assurance cases by incorporating formal methods, rendering it possible for automatic reasoning about assurance cases. We present Trustworthiness Derivation Tree Analyzer (Trusta), a desktop application designed to automatically construct and verify TDTs. The tool has a built-in Prolog interpreter in its backend, and is supported by the constraint solvers Z3 and MONA. Therefore, it can solve constraints about logical formulas involving arithmetic, sets, Horn clauses etc. Trusta also utilizes large language models to make the creation and evaluation of assurance cases more convenient. It allows for interactive human examination and modification. We evaluated top language models like ChatGPT-3.5, ChatGPT-4, and PaLM 2 for generating assurance cases. Our tests showed a 50%-80% similarity between machine-generated and human-created cases. In addition, Trusta can extract formal constraints from text in natural languages, facilitating an easier interpretation and validation process. This extraction is subject to human review and correction, blending the best of automated efficiency with human insight. To our knowledge, this marks the first integration of large language models in automatic creating and reasoning about assurance cases, bringing a novel approach to a traditional challenge. Through several industrial case studies, Trusta has proven to quickly find some subtle issues that are typically missed in manual inspection, demonstrating its practical value in enhancing the assurance case development process.
</details>
<details>
<summary>摘要</summary>
可信度证明（Assurance Case）可以用于安全工程中证明产品的安全性。在安全关键领域，构建可信度证明是必备的。可信度推导树（TDT）可以增强可信度证明，使其可以进行自动的逻辑推理。我们介绍了一款名为“信任worthiness Derivation Tree Analyzer”（Trusta）的桌面应用程序，用于自动构建和验证TDT。Trusta具有内置的Prolog解释器，并支持Z3和MONA等约束解决器。因此，它可以解决包括逻辑形式中的数学、集合、扩展等约束。Trusta还利用大型自然语言模型来使创建和评估可信度证明更加方便。它允许交互式的人工检查和修改。我们对ChatGPT-3.5、ChatGPT-4和PaLM 2等大型自然语言模型进行测试，测试结果表明，机器生成的可信度证明与人类创建的可信度证明之间存在50%-80%的相似性。此外，Trusta可以从自然语言文本中提取形式约束，使评估和验证过程更加容易。这种提取是人类审核和修正的，将机器自动效率与人类智慧相结合。在我们所知道的情况下，Trusta是首次将大型自然语言模型 integrate into automatic creation and reasoning about assurance cases，为传统挑战提供了一种新的方法。通过多个工业案例研究，Trusta已经证明了它能够快速发现一些通常被人工检查掉的细微问题，表明了它在提高可信度证明开发过程的实际价值。
</details></li>
</ul>
<hr>
<h2 id="Self-Explanation-Prompting-Improves-Dialogue-Understanding-in-Large-Language-Models"><a href="#Self-Explanation-Prompting-Improves-Dialogue-Understanding-in-Large-Language-Models" class="headerlink" title="Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models"></a>Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12940">http://arxiv.org/abs/2309.12940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyu Gao, Ting-En Lin, Hangyu Li, Min Yang, Yuchuan Wu, Wentao Ma, Yongbin Li</li>
<li>for: 提高大语言模型在多回话对话中的理解能力</li>
<li>methods: 使用自我解释提示策略，让模型在对话开始前分析每个对话语言，提高对话中任务执行的表现</li>
<li>results: 经过实验证明，该策略可以在六个 benchmark 数据集中 consistently 超越零shot 提示和几个 shot 提示，达到或超过少量提示的效果，表明其可以强大地增强大语言模型在复杂对话任务中的理解能力。<details>
<summary>Abstract</summary>
Task-oriented dialogue (TOD) systems facilitate users in executing various activities via multi-turn dialogues, but Large Language Models (LLMs) often struggle to comprehend these intricate contexts. In this study, we propose a novel "Self-Explanation" prompting strategy to enhance the comprehension abilities of LLMs in multi-turn dialogues. This task-agnostic approach requires the model to analyze each dialogue utterance before task execution, thereby improving performance across various dialogue-centric tasks. Experimental results from six benchmark datasets confirm that our method consistently outperforms other zero-shot prompts and matches or exceeds the efficacy of few-shot prompts, demonstrating its potential as a powerful tool in enhancing LLMs' comprehension in complex dialogue tasks.
</details>
<details>
<summary>摘要</summary>
干预对话（TOD）系统通过多回对话来帮助用户执行各种活动，但大语言模型（LLM）经常在复杂的上下文中困难理解。在这项研究中，我们提出了一种新的“自我解释”提示策略，以提高LLM在多回对话中的理解能力。这种任务无关的方法需要模型在对话过程中分析每个语音，从而提高对各种对话中心任务的表现。六个基准数据集的实验结果表明，我们的方法在其他零批提示和几批提示之间具有优异的表现，并且能够与或超过几批提示的效果，这表明了这种方法在复杂对话任务中提高LLM的理解能力的潜力。
</details></li>
</ul>
<hr>
<h2 id="Frustrated-with-Code-Quality-Issues-LLMs-can-Help"><a href="#Frustrated-with-Code-Quality-Issues-LLMs-can-Help" class="headerlink" title="Frustrated with Code Quality Issues? LLMs can Help!"></a>Frustrated with Code Quality Issues? LLMs can Help!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12938">http://arxiv.org/abs/2309.12938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nalin Wadhwa, Jui Pradhan, Atharv Sonwane, Surya Prakash Sahu, Nagarajan Natarajan, Aditya Kanade, Suresh Parthasarathy, Sriram Rajamani</li>
<li>for: 这种论文主要是为了提高代码质量，提高软件的可靠性、维护性和安全性。</li>
<li>methods: 这个论文使用了大型自然语言模型（LLM）来帮助开发者修复代码质量问题。具体来说，这个工具使用了一对LLM组成的“推荐-评分”结构，其中一个LLM提供修复建议，另一个LLM则根据开发者的acceptance criterion评分这些建议。</li>
<li>results: 这个论文的实验结果显示，使用CORE工具可以提高Python文件的修复率达59.2%，同时减少了 False Positive 的比例。此外，在Java文件中，CORE工具可以达到76.8%的修复率，与专门的程序修复工具相当。<details>
<summary>Abstract</summary>
As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues. We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The \emph{proposer LLM} of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, the LLM may introduce subtle, unintended functionality changes which may go un-detected by the static analysis. The \emph{ranker LLM} evaluates the changes made by the proposer using a rubric that closely follows the acceptance criteria that a developer would enforce. CORE uses the scores assigned by the ranker LLM to rank the candidate revisions before presenting them to the developer. CORE could revise 59.2% Python files (across 52 quality checks) so that they pass scrutiny by both a tool and a human reviewer. The ranker LLM is able to reduce false positives by 25.8% in these cases. CORE produced revisions that passed the static analysis tool in 76.8% Java files (across 10 quality checks) comparable to 78.3% of a specialized program repair tool, with significantly much less engineering efforts.
</details>
<details>
<summary>摘要</summary>
随着软件项目的进行，代码质量的重要性日益增加，因为它直接影响软件的可靠性、维护性和安全性。为此，开发者在开发过程中使用静态分析工具来检测代码质量问题。然而，开发者需要额外努力来修改代码，以便通过工具的检测。在这个工作中，我们研究了使用大型自然语言模型（LLM）来帮助开发者修改代码，以解决代码质量问题。我们提出了一个工具，称为 CORE（简称代码修订），其核心思想是使用一对LLM组成的“提案-评分”机制。提案LLM使用同样的推荐方法，将static分析工具的推荐改进应用于代码修订。提案LLM生成的候选修订检查通过静态质量检查。然而，LLM可能引入微妙的、意外的功能变化，这些变化可能被静态分析工具忽略。评分LLM使用一个仅次于开发者的接受标准来评价修订。CORE使用评分LLM的分数来排序候选修订，然后向开发者展示。CORE可以在52个质量检查中，对59.2%的Python文件进行修订，使其通过静态分析工具和人工审查。评分LLM可以在这些案例中减少false positives的比例为25.8%。CORE生成的修订可以在76.8%的Java文件中通过静态分析工具，与专门的修复工具相当，但需要的工程努力明显更少。
</details></li>
</ul>
<hr>
<h2 id="On-Separate-Normalization-in-Self-supervised-Transformers"><a href="#On-Separate-Normalization-in-Self-supervised-Transformers" class="headerlink" title="On Separate Normalization in Self-supervised Transformers"></a>On Separate Normalization in Self-supervised Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12931">http://arxiv.org/abs/2309.12931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohui Chen, Yinkai Wang, Yuanqi Du, Soha Hassoun, Li-Ping Liu</li>
<li>for: 本文提出了一种简单的修改，即在masked autoencoders（MAE）中使用分开的normalization层来更好地捕捉token和[CLS]符号的不同特征，以提高下游任务性能。</li>
<li>methods: 本文提出的方法是，在MAE模型中，为token和[CLS]符号分别使用分开的normalization层，以便更好地捕捉它们的不同特征。</li>
<li>results: 经验表明，通过使用分开的normalization层，[CLS] embedding可以更好地编码全局Contextual信息，并且在它的不规则空间中分布更加均匀。 replaced conventional normalization layer with two separate layers, we observe an average performance improvement of 2.7% over the image, natural language, and graph domains.<details>
<summary>Abstract</summary>
Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the [CLS] symbol and the tokens. We propose in this paper a simple modification that employs separate normalization layers for the tokens and the [CLS] symbol to better capture their distinct characteristics and enhance downstream task performance. Our method aims to alleviate the potential negative effects of using the same normalization statistics for both token types, which may not be optimally aligned with their individual roles. We empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space. When replacing the conventional normalization layer with the two separate layers, we observe an average 2.7% performance improvement over the image, natural language, and graph domains.
</details>
<details>
<summary>摘要</summary>
自我超vision方法对transformer模型表现非常出色，在不同领域中达到了优秀的result。以前的transformer模型，如masked autoencoders（MAE），通常使用单个normalization层来处理[CLS]符号和token。我们在这篇论文中提出了一个简单的修改，即在token和[CLS]符号之间使用分开的normalization层，以更好地捕捉它们的特点和提高下游任务表现。我们的方法的目标是解决使用同一个normalization统计数据来处理token和[CLS]符号可能存在的负面影响，这可能不是最佳的对齐。我们在实验中发现，通过使用两个分开的normalization层，[CLS]嵌入可以更好地编码全局上下文信息，并且在其不对称空间中分布更加均匀。当将传统的normalization层替换为两个分开的normalization层时，我们在图像、自然语言和图形领域的average表现提高了2.7%。
</details></li>
</ul>
<hr>
<h2 id="Lamarck’s-Revenge-Inheritance-of-Learned-Traits-Can-Make-Robot-Evolution-Better"><a href="#Lamarck’s-Revenge-Inheritance-of-Learned-Traits-Can-Make-Robot-Evolution-Better" class="headerlink" title="Lamarck’s Revenge: Inheritance of Learned Traits Can Make Robot Evolution Better"></a>Lamarck’s Revenge: Inheritance of Learned Traits Can Make Robot Evolution Better</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13099">http://arxiv.org/abs/2309.13099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Luo, Karine Miras, Jakub Tomczak, Agoston E. Eiben</li>
<li>for: 研究“如果18世纪生物学家拉马克不完全错误，个体特征通过遗传继承给后代？”问题。</li>
<li>methods: 使用进化机器人框架进行模拟，其中机器人体型（身体）和控制器（大脑）都可以进化，同时机器人也可以通过学习在生命中提高控制器。</li>
<li>results: 对拉马克主义系统和达尔文主义系统进行比较，发现拉马克主义系统会增强机器人体型智能的出现，并确定了这种成功的原因：新生机器人的遗传大脑与身体更好匹配，因此其遗传率比达尔文主义系统高。<details>
<summary>Abstract</summary>
Evolutionary robot systems offer two principal advantages: an advanced way of developing robots through evolutionary optimization and a special research platform to conduct what-if experiments regarding questions about evolution. Our study sits at the intersection of these. We investigate the question ``What if the 18th-century biologist Lamarck was not completely wrong and individual traits learned during a lifetime could be passed on to offspring through inheritance?'' We research this issue through simulations with an evolutionary robot framework where morphologies (bodies) and controllers (brains) of robots are evolvable and robots also can improve their controllers through learning during their lifetime. Within this framework, we compare a Lamarckian system, where learned bits of the brain are inheritable, with a Darwinian system, where they are not. Analyzing simulations based on these systems, we obtain new insights about Lamarckian evolution dynamics and the interaction between evolution and learning. Specifically, we show that Lamarckism amplifies the emergence of `morphological intelligence', the ability of a given robot body to acquire a good brain by learning, and identify the source of this success: `newborn' robots have a higher fitness because their inherited brains match their bodies better than those in a Darwinian system.
</details>
<details>
<summary>摘要</summary>
生化机器系统提供了两大优势：一是通过进化优化发展机器人的先进方法，二是一种特殊的研究平台来进行关于进化的问题的什么样的实验。我们的研究位于这两个方面之间。我们研究“如果18世纪的生物学家拉马克不完全错误，个体特征在生命周期中学习得来的不能被遗传下来？”这个问题，通过使用进化机器人框架进行模拟，在这个框架中，机器人的形态（身体）和控制器（脑）都可以进化，同时机器人也可以通过学习提高控制器的性能。在这个框架中，我们比较了拉马克主义系统和达尔文主义系统两种不同的进化方式。通过分析这些系统的模拟结果，我们获得了新的理解关于拉马克主义进化动力学和进化和学习之间的互动。具体来说，我们发现拉马克主义会增强机器人身体的智能化，即机器人身体可以通过学习获得一个好的脑。而这种成功的原因是“新生”机器人的遗传因素更好地匹配其身体，因此它们在达尔文主义系统中的遗传因素更高。
</details></li>
</ul>
<hr>
<h2 id="A-matter-of-attitude-Focusing-on-positive-and-active-gradients-to-boost-saliency-maps"><a href="#A-matter-of-attitude-Focusing-on-positive-and-active-gradients-to-boost-saliency-maps" class="headerlink" title="A matter of attitude: Focusing on positive and active gradients to boost saliency maps"></a>A matter of attitude: Focusing on positive and active gradients to boost saliency maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12913">http://arxiv.org/abs/2309.12913</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oscarllorente/positive_active_saliency_maps">https://github.com/oscarllorente/positive_active_saliency_maps</a></li>
<li>paper_authors: Oscar Llorente, Jaime Boal, Eugenio F. Sánchez-Úbeda</li>
<li>for: 这篇论文旨在探讨如何通过修复权重映射的缺陷，提高多类分类问题中神经网络的解释性。</li>
<li>methods: 该论文使用了修复权重映射的方法，通过恢复权重映射中的符号，提高了对多类分类问题中神经网络的解释性。</li>
<li>results: 研究发现，通过考虑正确类和其他类之间的关系，可以更好地了解神经网络对图像中各个像素的关注。此外，隐藏或改变这些像素会对结果产生什么影响也变得更加清晰。<details>
<summary>Abstract</summary>
Saliency maps have become one of the most widely used interpretability techniques for convolutional neural networks (CNN) due to their simplicity and the quality of the insights they provide. However, there are still some doubts about whether these insights are a trustworthy representation of what CNNs use to come up with their predictions. This paper explores how rescuing the sign of the gradients from the saliency map can lead to a deeper understanding of multi-class classification problems. Using both pretrained and trained from scratch CNNs we unveil that considering the sign and the effect not only of the correct class, but also the influence of the other classes, allows to better identify the pixels of the image that the network is really focusing on. Furthermore, how occluding or altering those pixels is expected to affect the outcome also becomes clearer.
</details>
<details>
<summary>摘要</summary>
静观地図（Saliency map）已成为 convolutional neural network（CNN）的解释技术中最广泛使用的一种，这主要是因为它的简单性和解释的质量。然而，有些人仍存在对静观地図是否准确反映CNN的预测过程中的信息的 doubts。这篇文章探讨了如何从静观地図中救出梯度的正负信息，以便更深入地理解多类分类问题。我们使用预训练和从scratch预测的CNN，发现考虑正负信息和其他类的影响，可以更好地定位图像中网络是真正关注的像素。此外，对这些像素进行遮盖或修改也会对结果产生什么影响也变得更加清楚。
</details></li>
</ul>
<hr>
<h2 id="KG-MDL-Mining-Graph-Patterns-in-Knowledge-Graphs-with-the-MDL-Principle"><a href="#KG-MDL-Mining-Graph-Patterns-in-Knowledge-Graphs-with-the-MDL-Principle" class="headerlink" title="KG-MDL: Mining Graph Patterns in Knowledge Graphs with the MDL Principle"></a>KG-MDL: Mining Graph Patterns in Knowledge Graphs with the MDL Principle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12908">http://arxiv.org/abs/2309.12908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Bariatti, Peggy Cellier, Sébastien Ferré</li>
<li>for: 本文targets the problem of extracting meaningful graph patterns from large knowledge graphs (KGs), which are difficult to mine due to their size and complexity.</li>
<li>methods: 本文提出了一种基于最小描述长度（MDL）原理的图Pattern mining approach called KG-MDL, which generates a human-sized and descriptive set of graph patterns for a given KG.</li>
<li>results: 实验表明，KG-MDL可以生成小 enough to be interpreted by humans yet descriptive of the KG的pattern set, highlighting both the schema used to create the data and the concrete facts it contains.<details>
<summary>Abstract</summary>
Nowadays, increasingly more data are available as knowledge graphs (KGs). While this data model supports advanced reasoning and querying, they remain difficult to mine due to their size and complexity. Graph mining approaches can be used to extract patterns from KGs. However this presents two main issues. First, graph mining approaches tend to extract too many patterns for a human analyst to interpret (pattern explosion). Second, real-life KGs tend to differ from the graphs usually treated in graph mining: they are multigraphs, their vertex degrees tend to follow a power-law, and the way in which they model knowledge can produce spurious patterns. Recently, a graph mining approach named GraphMDL+ has been proposed to tackle the problem of pattern explosion, using the Minimum Description Length (MDL) principle. However, GraphMDL+, like other graph mining approaches, is not suited for KGs without adaptations. In this paper we propose KG-MDL, a graph pattern mining approach based on the MDL principle that, given a KG, generates a human-sized and descriptive set of graph patterns, and so in a parameter-less and anytime way. We report on experiments on medium-sized KGs showing that our approach generates sets of patterns that are both small enough to be interpreted by humans and descriptive of the KG. We show that the extracted patterns highlight relevant characteristics of the data: both of the schema used to create the data, and of the concrete facts it contains. We also discuss the issues related to mining graph patterns on knowledge graphs, as opposed to other types of graph data.
</details>
<details>
<summary>摘要</summary>
现在，知识图（KG）中的数据越来越多，这种数据模型支持高级推理和查询，但它们仍然具有困难 mine 的问题。图минаING Approaches可以提取图中的模式，但这有两个主要问题。首先，图MINING Approaches通常会提取太多的模式，让人类分析者难以处理（pattern explosion）。其次，实际的 KG 与通常在图MINING 中处理的图不同：它们是多 graphs，顶点度遵循力� law，以及知识表示方式可能会生成假Pattern。最近，一种名为 GraphMDL+ 的图MINING Approach 被提出，使用最小描述长度（MDL）原理来解决pattern explosion问题。然而，GraphMDL+ 和其他图MINING Approaches不适用于 KG 而不需要更多的参数。在这篇论文中，我们提出了基于 MDL 原理的图模式挖掘方法，可以给 KG 生成一个人类可以理解的、描述性的图模式集，并且在无参数和实时的情况下进行。我们对中等规模的 KG 进行了实验，并证明了我们的方法可以生成小 enough 且描述性的图模式集，并且这些模式集可以高亮 KG 中的schema和具体事实。我们还讨论了对知识图进行图模式挖掘的问题，与其他类型的图数据进行比较。
</details></li>
</ul>
<hr>
<h2 id="ProtoEM-A-Prototype-Enhanced-Matching-Framework-for-Event-Relation-Extraction"><a href="#ProtoEM-A-Prototype-Enhanced-Matching-Framework-for-Event-Relation-Extraction" class="headerlink" title="ProtoEM: A Prototype-Enhanced Matching Framework for Event Relation Extraction"></a>ProtoEM: A Prototype-Enhanced Matching Framework for Event Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12892">http://arxiv.org/abs/2309.12892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhilei Hu, Zixuan Li, Daozhu Xu, Long Bai, Cheng Jin, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng</li>
<li>for: 本研究旨在提取文本中的多种事件关系，并尝试更好地捕捉这些关系的内在 semantics。</li>
<li>methods: 本研究使用 Prototype-Enhanced Matching (ProtoEM) 框架，包括 prototype representing 和 prototype matching 两个步骤。在第一步中，使用例子来表示不同类型的事件关系的词义特征。在第二步中，使用图 neural network (GNN) 模块来模型事件关系之间的依赖关系。</li>
<li>results: 实验结果表明，ProtoEM 框架可以有效地表示事件关系的词义特征，并在 MAVEN-ERE 数据集上具有显著的提高效果 compared to baseline models。<details>
<summary>Abstract</summary>
Event Relation Extraction (ERE) aims to extract multiple kinds of relations among events in texts. However, existing methods singly categorize event relations as different classes, which are inadequately capturing the intrinsic semantics of these relations. To comprehensively understand their intrinsic semantics, in this paper, we obtain prototype representations for each type of event relation and propose a Prototype-Enhanced Matching (ProtoEM) framework for the joint extraction of multiple kinds of event relations. Specifically, ProtoEM extracts event relations in a two-step manner, i.e., prototype representing and prototype matching. In the first step, to capture the connotations of different event relations, ProtoEM utilizes examples to represent the prototypes corresponding to these relations. Subsequently, to capture the interdependence among event relations, it constructs a dependency graph for the prototypes corresponding to these relations and utilized a Graph Neural Network (GNN)-based module for modeling. In the second step, it obtains the representations of new event pairs and calculates their similarity with those prototypes obtained in the first step to evaluate which types of event relations they belong to. Experimental results on the MAVEN-ERE dataset demonstrate that the proposed ProtoEM framework can effectively represent the prototypes of event relations and further obtain a significant improvement over baseline models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>prototype representing：通过使用例子来表示不同的事件关系的核心含义。2. prototype matching：使用一个基于图 neural network (GNN) 的模块来模型事件关系之间的依赖关系，然后将新的事件对比与已有的原型来评估它们属于哪种事件关系。实验结果表明，ProtoEM 框架可以有效地表示事件关系的原型，并在基eline模型上获得了显著改进。</details></li>
</ol>
<hr>
<h2 id="Gravity-Network-for-end-to-end-small-lesion-detection"><a href="#Gravity-Network-for-end-to-end-small-lesion-detection" class="headerlink" title="Gravity Network for end-to-end small lesion detection"></a>Gravity Network for end-to-end small lesion detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12876">http://arxiv.org/abs/2309.12876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cirorusso2910/gravitynet">https://github.com/cirorusso2910/gravitynet</a></li>
<li>paper_authors: Ciro Russo, Alessandro Bria, Claudio Marrocco</li>
<li>for: 检测医疗影像中的小 lesion</li>
<li>methods: 提出了一种新的一stage末端检测器，通过引入新的像素基于气场点，动态追踪目标小 lesion进行检测</li>
<li>results: 在两个常见的医疗影像任务中（数字乳腺影像和数字胆囊影像），方法展现出了有效地检测小 lesion的表现<details>
<summary>Abstract</summary>
This paper introduces a novel one-stage end-to-end detector specifically designed to detect small lesions in medical images. Precise localization of small lesions presents challenges due to their appearance and the diverse contextual backgrounds in which they are found. To address this, our approach introduces a new type of pixel-based anchor that dynamically moves towards the targeted lesion for detection. We refer to this new architecture as GravityNet, and the novel anchors as gravity points since they appear to be "attracted" by the lesions. We conducted experiments on two well-established medical problems involving small lesions to evaluate the performance of the proposed approach: microcalcifications detection in digital mammograms and microaneurysms detection in digital fundus images. Our method demonstrates promising results in effectively detecting small lesions in these medical imaging tasks.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的一stage端到端检测器，用于检测医疗图像中的小肿。由于小肿的外观和背景的多样性，精准地定位小肿呈现了挑战。为解决这个问题，我们的方法引入了一种新的像素基的锚点，这些锚点在检测过程中会动态向着目标小肿移动。我们称这种新架构为重力网络（GravityNet），这些新锚点为重力点（Gravity Point），因为它们看起来像是吸引小肿的。我们在两个常见的医学问题中进行了实验：数字乳腺癌检测和数字血管图像中的微血管检测。我们的方法在这些医学检测任务中表现出色，能够有效地检测小肿。
</details></li>
</ul>
<hr>
<h2 id="AnglE-optimized-Text-Embeddings"><a href="#AnglE-optimized-Text-Embeddings" class="headerlink" title="AnglE-optimized Text Embeddings"></a>AnglE-optimized Text Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12871">http://arxiv.org/abs/2309.12871</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SeanLee97/AnglE">https://github.com/SeanLee97/AnglE</a></li>
<li>paper_authors: Xianming Li, Jing Li</li>
<li>for: 提高Semantic Textual Similarity（STS）任务的质量文本嵌入，帮助提高Large Language Model（LLM）应用程序的性能。</li>
<li>methods: 提出了一种新的角度优化文本嵌入模型AnglE，通过引入角度优化来解决cosine函数中的恶性区域问题，从而提高 gradient 的演化和优化过程。</li>
<li>results: 对于短文本STS任务和新收集的长文本STS任务以及域специфи STS任务，AnglE 比State-of-the-art（SOTA）STS模型表现更好， demonstrating the ability of AnglE to generate high-quality text embeddings and the usefulness of angle optimization in STS.<details>
<summary>Abstract</summary>
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works with LLM-annotated data. Extensive experiments were conducted on various tasks including short-text STS, long-text STS, and domain-specific STS tasks. The results show that AnglE outperforms the state-of-the-art (SOTA) STS models that ignore the cosine saturation zone. These findings demonstrate the ability of AnglE to generate high-quality text embeddings and the usefulness of angle optimization in STS.
</details>
<details>
<summary>摘要</summary>
高品质文本嵌入是 LLM 应用中关键的Semantic Textual Similarity (STS) 任务的提高的关键，但是现有的文本嵌入模型面临着让 Gradient 消失的挑战，主要是因为它们对 cosine 函数在优化目标中的使用，这会导致缺失区域。为解决这个问题，本文提出了一种新的角度优化文本嵌入模型 called AnglE。 AnglE 的核心思想是在复杂空间中引入角度优化。这种新的方法可以有效地减轻 cosine 函数的缺失区域的影响，从而使得梯度和优化过程得到改善。为进行全面的 STS 评估，我们对现有的短文本 STS 数据集和从 GitHub Issues 上收集的新的长文本 STS 数据集进行实验。此外，我们还研究了受限制的数据集和使用 LLM 标注数据的域特性 STS 场景。我们进行了各种任务，包括短文本 STS、长文本 STS 和域特性 STS 任务。结果表明，AnglE 在忽略 cosine 缺失区域的 SOTA STS 模型之上表现出色，这些发现证明了 AnglE 的能力生成高质量文本嵌入和角度优化在 STS 中的有用性。
</details></li>
</ul>
<hr>
<h2 id="Accurate-and-Fast-Compressed-Video-Captioning"><a href="#Accurate-and-Fast-Compressed-Video-Captioning" class="headerlink" title="Accurate and Fast Compressed Video Captioning"></a>Accurate and Fast Compressed Video Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12867">http://arxiv.org/abs/2309.12867</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/acherstyx/CoCap">https://github.com/acherstyx/CoCap</a></li>
<li>paper_authors: Yaojie Shen, Xin Gu, Kai Xu, Heng Fan, Longyin Wen, Libo Zhang</li>
<li>for: 这种论文是为了提出一种新的视频描述方法，以解决现有视频描述方法中的扫描缺陷。</li>
<li>methods: 该方法使用了压缩视频域的特征，包括I帧、运动向量和差异，并设计了一种特殊的变换器来学习描述视频。</li>
<li>results: 该方法可以在不同的benchmark上达到状态 arts的性能，同时运行速度比现有方法快得多。<details>
<summary>Abstract</summary>
Existing video captioning approaches typically require to first sample video frames from a decoded video and then conduct a subsequent process (e.g., feature extraction and/or captioning model learning). In this pipeline, manual frame sampling may ignore key information in videos and thus degrade performance. Additionally, redundant information in the sampled frames may result in low efficiency in the inference of video captioning. Addressing this, we study video captioning from a different perspective in compressed domain, which brings multi-fold advantages over the existing pipeline: 1) Compared to raw images from the decoded video, the compressed video, consisting of I-frames, motion vectors and residuals, is highly distinguishable, which allows us to leverage the entire video for learning without manual sampling through a specialized model design; 2) The captioning model is more efficient in inference as smaller and less redundant information is processed. We propose a simple yet effective end-to-end transformer in the compressed domain for video captioning that enables learning from the compressed video for captioning. We show that even with a simple design, our method can achieve state-of-the-art performance on different benchmarks while running almost 2x faster than existing approaches. Code is available at https://github.com/acherstyx/CoCap.
</details>
<details>
<summary>摘要</summary>
传统的视频描述方法通常需要先从解码后的视频抽取一些帧并进行后续处理（例如特征提取和/或描述模型学习）。在这个管道中，人工抽取帧可能会忽略视频中的关键信息，从而降低性能。此外，抽取的帧中可能包含重复的信息，导致在视频描述中的效率低下。为了解决这些问题，我们从压缩频道的视角研究视频描述，它带来了多重优势：1）相比于解码后的原始图像，压缩视频中的I-帧、运动向量和差异，具有高度可识别性，允许我们通过特殊的模型设计不需要人工抽取来学习整个视频;2）描述模型在推理中更高效，因为处理的信息更小和更少重复。我们提出了一种简单 yet 有效的终端转换器在压缩频道中进行视频描述，允许我们从压缩视频中学习描述。我们示出了我们的方法可以在不同的标准上达到领先的性能，同时运行速度比现有方法快得多于2倍。代码可以在https://github.com/acherstyx/CoCap中获取。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-for-Arabic-Machine-Translation-The-Case-of-Financial-Texts"><a href="#Domain-Adaptation-for-Arabic-Machine-Translation-The-Case-of-Financial-Texts" class="headerlink" title="Domain Adaptation for Arabic Machine Translation: The Case of Financial Texts"></a>Domain Adaptation for Arabic Machine Translation: The Case of Financial Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12863">http://arxiv.org/abs/2309.12863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emad A. Alghamdi, Jezia Zakraoui, Fares A. Abanmy</li>
<li>for: 本研究目的是探讨阿拉伯语机器翻译（AMT）在未曾探讨的领域中的效果，具体来说是在金融新闻文章的翻译中。</li>
<li>methods: 我们开发了一个精心制作的阿拉伯语-英语（AR-EN）翻译并 fine-tuning 多种预训练的神经机器翻译和大语言模型，包括 ChatGPT-3.5 Turbo。</li>
<li>results: 我们发现，只需要一些良好的AR-EN对应的域 adaptation  segments， fine-tuning 就能够成功。 ChatGPT 的翻译质量较高，超过其他模型的自动和人工评估。 这是首次将 ChatGPT  fine-tuning 应用于金融领域域传输学习。<details>
<summary>Abstract</summary>
Neural machine translation (NMT) has shown impressive performance when trained on large-scale corpora. However, generic NMT systems have demonstrated poor performance on out-of-domain translation. To mitigate this issue, several domain adaptation methods have recently been proposed which often lead to better translation quality than genetic NMT systems. While there has been some continuous progress in NMT for English and other European languages, domain adaption in Arabic has received little attention in the literature. The current study, therefore, aims to explore the effectiveness of domain-specific adaptation for Arabic MT (AMT), in yet unexplored domain, financial news articles. To this end, we developed carefully a parallel corpus for Arabic-English (AR- EN) translation in the financial domain for benchmarking different domain adaptation methods. We then fine-tuned several pre-trained NMT and Large Language models including ChatGPT-3.5 Turbo on our dataset. The results showed that the fine-tuning is successful using just a few well-aligned in-domain AR-EN segments. The quality of ChatGPT translation was superior than other models based on automatic and human evaluations. To the best of our knowledge, this is the first work on fine-tuning ChatGPT towards financial domain transfer learning. To contribute to research in domain translation, we made our datasets and fine-tuned models available at https://huggingface.co/asas-ai/.
</details>
<details>
<summary>摘要</summary>
神经机器翻译（NMT）在大规模训练 corpora 上显示了很好的性能。然而，通用 NMT 系统在域外翻译中表现不佳。为了解决这个问题，一些域 adaptation 方法在过去几年得到了广泛的关注，并常常比通用 NMT 系统的翻译质量更高。英语和其他欧洲语言的 NMT 曾经得到了相当的研究，但在阿拉伯语中的域 adaptation 却受到了 littlet 的关注。因此，本研究的目的是探索在金融新闻文章中使用域adaptation 技术来提高阿拉伯语翻译质量。为此，我们开发了一个精心制作的阿拉伯语-英语（AR-EN）翻译 parallel corpus，并在这个dataset上练习了多种域 adaptation 方法。我们还对多种预训练的 NMT 和 Large Language 模型进行了练习，包括 ChatGPT-3.5 Turbo。结果表明，只需要几个Well-aligned in-domain AR-EN段，我们可以成功地进行了练习。ChatGPT 的翻译质量比其他模型更高，根据自动和人类评估。据我们所知，这是首次在金融域转移学习中使用 ChatGPT 进行了 fine-tuning。为了贡献到域翻译研究，我们将我们的dataset和练习模型上传到了https://huggingface.co/asas-ai/。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Augmentation-for-Sequential-Recommendation"><a href="#Diffusion-Augmentation-for-Sequential-Recommendation" class="headerlink" title="Diffusion Augmentation for Sequential Recommendation"></a>Diffusion Augmentation for Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12858">http://arxiv.org/abs/2309.12858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuqidong07/diffuasr">https://github.com/liuqidong07/diffuasr</a></li>
<li>paper_authors: Qidong Liu, Fan Yan, Xiangyu Zhao, Zhaocheng Du, Huifeng Guo, Ruiming Tang, Feng Tian</li>
<li>for: 这篇论文的目的是解决紧缩式推荐（SRS）中的数据罕见问题和长尾用户问题。</li>
<li>methods: 这篇论文提出了一种叫做散射增强（DiffuASR）的方法，它可以生成高质量的增强数据，并且可以直接使用这些增强数据来训练紧缩式推荐模型。</li>
<li>results: 根据实验结果，DiffuASR 能够有效地解决紧缩式推荐中的数据罕见问题和长尾用户问题，并且可以提高紧缩式推荐模型的表现。<details>
<summary>Abstract</summary>
Sequential recommendation (SRS) has become the technical foundation in many applications recently, which aims to recommend the next item based on the user's historical interactions. However, sequential recommendation often faces the problem of data sparsity, which widely exists in recommender systems. Besides, most users only interact with a few items, but existing SRS models often underperform these users. Such a problem, named the long-tail user problem, is still to be resolved. Data augmentation is a distinct way to alleviate these two problems, but they often need fabricated training strategies or are hindered by poor-quality generated interactions. To address these problems, we propose a Diffusion Augmentation for Sequential Recommendation (DiffuASR) for a higher quality generation. The augmented dataset by DiffuASR can be used to train the sequential recommendation models directly, free from complex training procedures. To make the best of the generation ability of the diffusion model, we first propose a diffusion-based pseudo sequence generation framework to fill the gap between image and sequence generation. Then, a sequential U-Net is designed to adapt the diffusion noise prediction model U-Net to the discrete sequence generation task. At last, we develop two guide strategies to assimilate the preference between generated and origin sequences. To validate the proposed DiffuASR, we conduct extensive experiments on three real-world datasets with three sequential recommendation models. The experimental results illustrate the effectiveness of DiffuASR. As far as we know, DiffuASR is one pioneer that introduce the diffusion model to the recommendation.
</details>
<details>
<summary>摘要</summary>
受sequential recommendation（SRS）技术支持，许多应用程序在最近得到了技术基础。SRS的目标是根据用户的历史交互来推荐下一个项目，但是SRS经常面临数据稀缺的问题，这种问题广泛存在于推荐系统中。另外，大多数用户只与几个项目进行交互，而现有的SRS模型往往不能满足这些用户。这种问题被称为长尾用户问题，仍需解决。数据扩展是一种有效的解决方法，但是它们通常需要复杂的训练策略或低质量生成的交互。为解决这些问题，我们提出了一种增强的数据扩展方法，即增强数据扩展（DiffuASR），可以为高质量生成提供基础。DiffuASR生成的扩展数据可以直接用于训练SRS模型，不需训练复杂的策略。为了利用扩展模型的生成能力，我们首先提出了一种基于扩散的pseudo序列生成框架，用于填充图像和序列生成之间的差异。然后，我们设计了一种序列U-Net，用于适应扩散噪声预测模型U-Net。最后，我们开发了两种引导策略，以便在生成和原始序列之间融合喜好。为验证我们提出的DiffuASR，我们在三个真实世界数据集上进行了广泛的实验，使用三种序列推荐模型。实验结果表明，DiffuASR是有效的。到目前为止，DiffuASR是对推荐领域中的扩展模型做出的开创性贡献。
</details></li>
</ul>
<hr>
<h2 id="AxOCS-Scaling-FPGA-based-Approximate-Operators-using-Configuration-Supersampling"><a href="#AxOCS-Scaling-FPGA-based-Approximate-Operators-using-Configuration-Supersampling" class="headerlink" title="AxOCS: Scaling FPGA-based Approximate Operators using Configuration Supersampling"></a>AxOCS: Scaling FPGA-based Approximate Operators using Configuration Supersampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12830">http://arxiv.org/abs/2309.12830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siva Satyendra Sahoo, Salim Ullah, Soumyo Bhattacharjee, Akash Kumar</li>
<li>For: 这个研究旨在提供一个基于机器学习的精简数据类型设计方法，以减少嵌入式系统中的机器学习实现成本。* Methods: 本研究使用了机器学习基于设计空间探索技术，并利用了机器学习方法来预测PPA和BEHAV的影响。* Results: 实验结果显示，提案的AxOCS方法可以对FPGA优化的精简数据类型进行多目标优化，并获得了较高的品质结果范围。<details>
<summary>Abstract</summary>
The rising usage of AI and ML-based processing across application domains has exacerbated the need for low-cost ML implementation, specifically for resource-constrained embedded systems. To this end, approximate computing, an approach that explores the power, performance, area (PPA), and behavioral accuracy (BEHAV) trade-offs, has emerged as a possible solution for implementing embedded machine learning. Due to the predominance of MAC operations in ML, designing platform-specific approximate arithmetic operators forms one of the major research problems in approximate computing. Recently there has been a rising usage of AI/ML-based design space exploration techniques for implementing approximate operators. However, most of these approaches are limited to using ML-based surrogate functions for predicting the PPA and BEHAV impact of a set of related design decisions. While this approach leverages the regression capabilities of ML methods, it does not exploit the more advanced approaches in ML. To this end, we propose AxOCS, a methodology for designing approximate arithmetic operators through ML-based supersampling. Specifically, we present a method to leverage the correlation of PPA and BEHAV metrics across operators of varying bit-widths for generating larger bit-width operators. The proposed approach involves traversing the relatively smaller design space of smaller bit-width operators and employing its associated Design-PPA-BEHAV relationship to generate initial solutions for metaheuristics-based optimization for larger operators. The experimental evaluation of AxOCS for FPGA-optimized approximate operators shows that the proposed approach significantly improves the quality-resulting hypervolume for multi-objective optimization-of 8x8 signed approximate multipliers.
</details>
<details>
<summary>摘要</summary>
随着人工智能和机器学习处理的应用领域扩大，低成本机器学习实现成为了一个紧迫的需求，特别是 для具有限制的嵌入式系统。为此，精确计算，一种探讨功能、性能、面积（PPA）和行为准确率（BEHAV）的负担规划，已经出现为实现嵌入式机器学习的可能解决方案。由于机器学习中的主要操作是 MAC 操作，因此设计特定平台的精确计算操作成为了研究中的主要问题。最近，人工智能/机器学习基于的设计空间探索技术已经广泛应用于实现精确操作。然而，大多数这些方法仅通过使用机器学习基于的代表函数预测 PPA 和 BEHAV 指标的影响来进行设计。这种方法利用机器学习方法的回归能力，但并不利用更高级别的机器学习方法。为此，我们提出了 AxOCS，一种通过机器学习基于supersampling的精确计算操作设计方法。具体来说，我们提出了一种利用不同位数的运算符之间的 correlation 关系来生成更大位数的运算符的方法。我们的方法是在更小的设计空间中查找更小的位数运算符的解决方案，并使用其相关的 Design-PPA-BEHAV 关系来生成初始的优化解决方案。我们的实验表明，AxOCS 可以对 FPGA 优化的精确 multiply 操作进行多目标优化，并显著提高了质量结果的卷积体积。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Boost-Leveraging-Synthetic-Data-for-Enhanced-Vision-Language-Segmentation-in-Echocardiography"><a href="#Synthetic-Boost-Leveraging-Synthetic-Data-for-Enhanced-Vision-Language-Segmentation-in-Echocardiography" class="headerlink" title="Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography"></a>Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12829">http://arxiv.org/abs/2309.12829</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naamiinepal/synthetic-boost">https://github.com/naamiinepal/synthetic-boost</a></li>
<li>paper_authors: Rabin Adhikari, Manish Dhakal, Safal Thapaliya, Kanchan Poudel, Prasiddha Bhandari, Bishesh Khanal</li>
<li>for: 本研究旨在提高echocardiography图像分割的准确率，使用视力语言分割模型（VLSM），并利用Semantic Diffusion Models（SDM）生成的synthetic图像来增强VLSM的表现。</li>
<li>methods: 本研究使用了两种popular VLSMs（CLIPSeg和CRIS），并使用了七种不同的语言提示（derived from several attributes，自动提取自echocardiography图像、分割masks和metadata）。</li>
<li>results: 研究结果表明，在使用SDM生成的synthetic图像进行预训练后，VLSM的表现得到了提高，并且 convergence faster。code、配置和提示可以在<a target="_blank" rel="noopener" href="https://github.com/naamiinepal/synthetic-boost%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/naamiinepal/synthetic-boost上获取。</a><details>
<summary>Abstract</summary>
Accurate segmentation is essential for echocardiography-based assessment of cardiovascular diseases (CVDs). However, the variability among sonographers and the inherent challenges of ultrasound images hinder precise segmentation. By leveraging the joint representation of image and text modalities, Vision-Language Segmentation Models (VLSMs) can incorporate rich contextual information, potentially aiding in accurate and explainable segmentation. However, the lack of readily available data in echocardiography hampers the training of VLSMs. In this study, we explore using synthetic datasets from Semantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography segmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS) using seven different kinds of language prompts derived from several attributes, automatically extracted from echocardiography images, segmentation masks, and their metadata. Our results show improved metrics and faster convergence when pretraining VLSMs on SDM-generated synthetic images before finetuning on real images. The code, configs, and prompts are available at https://github.com/naamiinepal/synthetic-boost.
</details>
<details>
<summary>摘要</summary>
准确的分割是诊断卡地狱疾病（CVD）的关键。然而，医生之间的差异和超声图像本身的挑战使得准确的分割受到阻碍。通过利用图像和文本Modalities的共同表示，视觉语言分割模型（VLSM）可以捕捉更多的上下文信息，可能帮助实现准确和可解释的分割。然而，有限的实际数据使得训练VLSM困难。在这项研究中，我们探讨使用Semantic Diffusion Models（SDM）生成的 sintetic dataset来增强VLSM的echocardiography分割。我们对两种流行的VLSM（CLIPSeg和CRIS）进行评估，使用七种不同的语言提示，自动从echocardiography图像、分割mask和其Metadata中提取的多种属性。我们的结果表明，在先行训练VLSM在SDM生成的 sintetic图像后，再进行 fine-tuning on real images 中，可以提高 metric 并且更快地 converges。代码、配置和提示可以在https://github.com/naamiinepal/synthetic-boost中找到。
</details></li>
</ul>
<hr>
<h2 id="OmniDrones-An-Efficient-and-Flexible-Platform-for-Reinforcement-Learning-in-Drone-Control"><a href="#OmniDrones-An-Efficient-and-Flexible-Platform-for-Reinforcement-Learning-in-Drone-Control" class="headerlink" title="OmniDrones: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control"></a>OmniDrones: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12825">http://arxiv.org/abs/2309.12825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Botian Xu, Feng Gao, Chao Yu, Ruize Zhang, Yi Wu, Yu Wang</li>
<li>for: 本研究提出了一个高效可扩展的平台，用于在凝视掌控中应用强化学习，基于Nvidia的Omniverse Isaac Sim。</li>
<li>methods: 该平台采用底层设计方法，允许用户轻松设计和实验各种应用场景，并在GPU并行的 simulations 上进行加速。</li>
<li>results: 该平台提供了多种标准任务，包括单批静止、多批静止和过 actuated 系统跟踪等，并提供了一些广泛使用的RL基elines。作者还提供了一些先行结果，以示Platform的能力和支持未来研究。<details>
<summary>Abstract</summary>
In this work, we introduce OmniDrones, an efficient and flexible platform tailored for reinforcement learning in drone control, built on Nvidia's Omniverse Isaac Sim. It employs a bottom-up design approach that allows users to easily design and experiment with various application scenarios on top of GPU-parallelized simulations. It also offers a range of benchmark tasks, presenting challenges ranging from single-drone hovering to over-actuated system tracking. In summary, we propose an open-sourced drone simulation platform, equipped with an extensive suite of tools for drone learning. It includes 4 drone models, 5 sensor modalities, 4 control modes, over 10 benchmark tasks, and a selection of widely used RL baselines. To showcase the capabilities of OmniDrones and to support future research, we also provide preliminary results on these benchmark tasks. We hope this platform will encourage further studies on applying RL to practical drone systems.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们介绍OmniDrones，一个高效和灵活的平台，适用于强化学习控制飞行器，基于Nvidia的Omniverse Isaac Sim。它采用底层设计方法，允许用户轻松地设计和实验各种应用场景，并在GPU平行化的 simulations 上进行了加速。它还提供了一系列的 benchmark 任务，展示了从单架飞行器停在位pto 多架飞行器跟踪系统的挑战。总之，我们提出了一个开源的飞行器 simulate平台，具有广泛的工具，用于飞行器学习。它包括4架飞行器模型、5种感知模式、4种控制方式、超过10个 benchmark 任务，以及一些通用的RL基elines。为了证明OmniDrones的能力和支持未来研究，我们也提供了先行结果这些 benchmark 任务。我们希望这个平台能够鼓励更多的研究人员通过应用RL来解决实际飞行器系统的问题。
</details></li>
</ul>
<hr>
<h2 id="A-Spectral-Theory-of-Neural-Prediction-and-Alignment"><a href="#A-Spectral-Theory-of-Neural-Prediction-and-Alignment" class="headerlink" title="A Spectral Theory of Neural Prediction and Alignment"></a>A Spectral Theory of Neural Prediction and Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12821">http://arxiv.org/abs/2309.12821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdulkadir Canatar, Jenelle Feather, Albert Wakhloo, SueYeon Chung</li>
<li>for: 这个论文的目的是尝试理解深度神经网络如何预测神经活动。</li>
<li>methods: 该论文使用了一种新的理论框架，将泛化误差与模型活动的 спектраль偏好以及神经响应的对齐关系联系起来。</li>
<li>results: 研究发现，使用多种深度神经网络可以获得低级别神经预测误差，且每种神经网络都有不同的表达形式。这些结果表明，可以通过分析表达形式来了解神经网络如何捕捉神经活动。<details>
<summary>Abstract</summary>
The representations of neural networks are often compared to those of biological systems by performing regression between the neural network responses and those measured from biological systems. Many different state-of-the-art deep neural networks yield similar neural predictions, but it remains unclear how to differentiate among models that perform equally well at predicting neural responses. To gain insight into this, we use a recent theoretical framework that relates the generalization error from regression to the spectral bias of the model activations and the alignment of the neural responses onto the learnable subspace of the model. We extend this theory to the case of regression between model activations and neural responses, and define geometrical properties describing the error embedding geometry. We test a large number of deep neural networks that predict visual cortical activity and show that there are multiple types of geometries that result in low neural prediction error as measured via regression. The work demonstrates that carefully decomposing representational metrics can provide interpretability of how models are capturing neural activity and points the way towards improved models of neural activity.
</details>
<details>
<summary>摘要</summary>
neural network 的表示方式 часто和生物系统相比，通过 regression 来比较 neural network 的响应和生物系统测量的响应。许多 state-of-the-art deep neural network 具有类似的预测性能，但是还不清楚如何区分这些模型在预测 neural network 响应的时候表现出类似的性能。为了增加这些信息，我们使用了最近的理论框架，将泛化误差从回归相关到模型活动的spectral bias和模型学习的子空间对齐。我们将这些理论扩展到模型活动和 neural network 之间的回归问题，并定义了 geometrical properties 描述错误嵌入几何。我们测试了许多 deep neural network，它们预测的视觉 Cortical activity 和实际测量的响应之间存在多种几何，这些几何都能够实现低级别预测错误。这项工作显示，细分表示度量可以提供如何模型捕捉 neural activity的解释，并指向了改进的 neural activity 模型。
</details></li>
</ul>
<hr>
<h2 id="Computational-Natural-Philosophy-A-Thread-from-Presocratics-through-Turing-to-ChatGPT"><a href="#Computational-Natural-Philosophy-A-Thread-from-Presocratics-through-Turing-to-ChatGPT" class="headerlink" title="Computational Natural Philosophy: A Thread from Presocratics through Turing to ChatGPT"></a>Computational Natural Philosophy: A Thread from Presocratics through Turing to ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13094">http://arxiv.org/abs/2309.13094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gordana Dodig-Crnkovic</li>
<li>for: 这篇论文旨在探讨计算自然哲学如何理解自然世界，以及如何通过计算机科学和人工智能技术来研究认知和智能。</li>
<li>methods: 这篇论文使用了计算机科学和人工智能的方法，包括深度神经网络和强化学习。</li>
<li>results: 这篇论文描述了一种基于深度神经网络的大语言模型（LLM），并通过人类反馈强化学习（RLHF）来训练这种模型。<details>
<summary>Abstract</summary>
Modern computational natural philosophy conceptualizes the universe in terms of information and computation, establishing a framework for the study of cognition and intelligence. Despite some critiques, this computational perspective has significantly influenced our understanding of the natural world, leading to the development of AI systems like ChatGPT based on deep neural networks. Advancements in this domain have been facilitated by interdisciplinary research, integrating knowledge from multiple fields to simulate complex systems. Large Language Models (LLMs), such as ChatGPT, represent this approach's capabilities, utilizing reinforcement learning with human feedback (RLHF). Current research initiatives aim to integrate neural networks with symbolic computing, introducing a new generation of hybrid computational models.
</details>
<details>
<summary>摘要</summary>
现代计算自然哲学将宇宙视为信息和计算的框架，为认知和智能的研究提供了一个框架。虽有一些批评，但这种计算视角已经对自然世界的理解产生了深远的影响，导致了基于深度神经网络的AI系统，如ChatGPT。这个领域的进步得益于多学科研究的整合，将多个领域的知识融合到模拟复杂系统中。大语言模型（LLMs），如ChatGPT，表示这种方法的能力，通过人工增强学习（RLHF）。当前的研究启动旨在将神经网络与符号计算结合，推出一代新的混合计算模型。
</details></li>
</ul>
<hr>
<h2 id="Masking-Improves-Contrastive-Self-Supervised-Learning-for-ConvNets-and-Saliency-Tells-You-Where"><a href="#Masking-Improves-Contrastive-Self-Supervised-Learning-for-ConvNets-and-Saliency-Tells-You-Where" class="headerlink" title="Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where"></a>Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12757">http://arxiv.org/abs/2309.12757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, Pin-Yu Chen, Wei-Chen Chiu</li>
<li>for: 提高 ConvNet 自动学习效果，使其能够更好地利用掩蔽操作进行自我超vision transformer 的学习。</li>
<li>methods: 在 ConvNet 框架中引入掩蔽操作，并考虑图像中重要对象的焦点分布，以避免掩蔽操作对图像的损害。另外，引入硬negative sample，使用更大的掩蔽区域来提高对抗样本的精度。</li>
<li>results: 在多个数据集、对抗学习机制和下游任务中，提出的方法在许多情况下表现出优于多个基eline。<details>
<summary>Abstract</summary>
While image data starts to enjoy the simple-but-effective self-supervised learning scheme built upon masking and self-reconstruction objective thanks to the introduction of tokenization procedure and vision transformer backbone, convolutional neural networks as another important and widely-adopted architecture for image data, though having contrastive-learning techniques to drive the self-supervised learning, still face the difficulty of leveraging such straightforward and general masking operation to benefit their learning process significantly. In this work, we aim to alleviate the burden of including masking operation into the contrastive-learning framework for convolutional neural networks as an extra augmentation method. In addition to the additive but unwanted edges (between masked and unmasked regions) as well as other adverse effects caused by the masking operations for ConvNets, which have been discussed by prior works, we particularly identify the potential problem where for one view in a contrastive sample-pair the randomly-sampled masking regions could be overly concentrated on important/salient objects thus resulting in misleading contrastiveness to the other view. To this end, we propose to explicitly take the saliency constraint into consideration in which the masked regions are more evenly distributed among the foreground and background for realizing the masking-based augmentation. Moreover, we introduce hard negative samples by masking larger regions of salient patches in an input image. Extensive experiments conducted on various datasets, contrastive learning mechanisms, and downstream tasks well verify the efficacy as well as the superior performance of our proposed method with respect to several state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
而图像数据在使用简单 yet effective的自我超vised学习算法基于masking和自我重建目标时，卷积神经网络作为图像数据的另一个重要和广泛采用的建筑物，虽然通过对它们的contrastive学习技术进行驱动，仍然面临masking操作不能够帮助它们的学习过程获得显著改进的问题。在这项工作中，我们想要解除对ConvNet的contrastive学习框架中masking操作的添加，以避免在contrastive样本对中 randomly sampling masking区域导致重要/焦点对象上的排序。此外，我们还发现在一个视图中，随机 sampling masking区域可能会过度集中在重要/焦点对象上，从而导致对另一个视图的错误对比性。为了解决这个问题，我们提出了一种explicitly considering saliency constraint的方法，以确保masked区域在背景和前景中具有更均匀的分布。此外，我们还引入硬negative samples，通过在输入图像中masking更大的salient patches来提高对downstream任务的适应性。我们的提议方法在多种dataset、contrastive学习机制和下游任务上进行了广泛的实验，并证明了与一些state-of-the-art基eline相比，我们的方法具有更高的效果和性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-an-MLOps-Architecture-for-XAI-in-Industrial-Applications"><a href="#Towards-an-MLOps-Architecture-for-XAI-in-Industrial-Applications" class="headerlink" title="Towards an MLOps Architecture for XAI in Industrial Applications"></a>Towards an MLOps Architecture for XAI in Industrial Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12756">http://arxiv.org/abs/2309.12756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonhard Faubel, Thomas Woudsma, Leila Methnani, Amir Ghorbani Ghezeljhemeidan, Fabian Buelow, Klaus Schmid, Willem D. van Driel, Benjamin Kloepper, Andreas Theodorou, Mohsen Nosratinia, Magnus Bång</li>
<li>for: 本研究旨在提高机器学习操作（MLOps）中的模型解释和反馈能力，以提高用户信任和采纳率。</li>
<li>methods: 本研究使用了一种新的MLOps软件架构，包括在实际用例中实现了解释和反馈功能。</li>
<li>results: 该软件架构具有高效地管理ML模型生产环境的能力，同时允许 интеграción of 解释和反馈功能。<details>
<summary>Abstract</summary>
Machine learning (ML) has become a popular tool in the industrial sector as it helps to improve operations, increase efficiency, and reduce costs. However, deploying and managing ML models in production environments can be complex. This is where Machine Learning Operations (MLOps) comes in. MLOps aims to streamline this deployment and management process. One of the remaining MLOps challenges is the need for explanations. These explanations are essential for understanding how ML models reason, which is key to trust and acceptance. Better identification of errors and improved model accuracy are only two resulting advantages. An often neglected fact is that deployed models are bypassed in practice when accuracy and especially explainability do not meet user expectations. We developed a novel MLOps software architecture to address the challenge of integrating explanations and feedback capabilities into the ML development and deployment processes. In the project EXPLAIN, our architecture is implemented in a series of industrial use cases. The proposed MLOps software architecture has several advantages. It provides an efficient way to manage ML models in production environments. Further, it allows for integrating explanations into the development and deployment processes.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we developed a novel MLOps software architecture that integrates explanations and feedback capabilities into the ML development and deployment processes. This architecture was implemented in the project EXPLAIN, using a series of industrial use cases. The proposed MLOps software architecture offers several advantages, including an efficient way to manage ML models in production environments and the ability to integrate explanations into the development and deployment processes.
</details></li>
</ul>
<hr>
<h2 id="OpenAi’s-GPT4-as-coding-assistant"><a href="#OpenAi’s-GPT4-as-coding-assistant" class="headerlink" title="OpenAi’s GPT4 as coding assistant"></a>OpenAi’s GPT4 as coding assistant</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12732">http://arxiv.org/abs/2309.12732</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lmous/openai-gpt4-coding-assistant">https://github.com/lmous/openai-gpt4-coding-assistant</a></li>
<li>paper_authors: Lefteris Moussiades, George Zografos</li>
<li>for: 本研究使用GPT3.5和GPT4作为代码生成助手，以检验它们在代码开发过程中的能力。</li>
<li>methods: 研究人员采用了适当的测试来检验GPT3.5和GPT4的能力，包括回答常见代码开发中的问题、生成可靠的代码和帮助代码调试。</li>
<li>results: 测试结果吸引人，GPT4的表现出色，表明这些新工具将提高程序员的产效和重新定义软件开发流程。<details>
<summary>Abstract</summary>
Lately, Large Language Models have been widely used in code generation. GPT4 is considered the most potent Large Language Model from Openai. In this paper, we examine GPT3.5 and GPT4 as coding assistants. More specifically, we have constructed appropriate tests to check whether the two systems can a) answer typical questions that can arise during the code development, b) produce reliable code, and c) contribute to code debugging. The test results are impressive. The performance of GPT4 is outstanding and signals an increase in the productivity of programmers and the reorganization of software development procedures based on these new tools.
</details>
<details>
<summary>摘要</summary>
近期，大型自然语言模型在代码生成方面广泛应用。GPT4被视为Openai中最强大的大型自然语言模型。本文我们将 examine GPT3.5和GPT4作为代码助手。更specifically，我们构建了适当的测试，检查这两个系统是否可以：a) 回答代码开发中可能出现的常见问题，b) 生成可靠的代码，c) 帮助代码调试。测试结果印象良好，GPT4的表现出色，这 signal了程序员的产出力和基于这些新工具的软件开发流程的重新组织。
</details></li>
</ul>
<hr>
<h2 id="Defeasible-Reasoning-with-Knowledge-Graphs"><a href="#Defeasible-Reasoning-with-Knowledge-Graphs" class="headerlink" title="Defeasible Reasoning with Knowledge Graphs"></a>Defeasible Reasoning with Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12731">http://arxiv.org/abs/2309.12731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dave Raggett</li>
<li>for: 本文旨在解决人类知识中的不确定性、不精确性、不完整性和不一致性问题，以及这些问题对semantic web的影响。</li>
<li>methods: 本文提出了一种直观notation和模型，用于解决不确定性推理，并与前期工作关于理据论相关。PKN与N3相比， defeasible reasoning是deductive logic的一种替换。</li>
<li>results: 本文结束时提出了一些关于使用声明性语言描述推理策略和战术的想法，以及基于AIF ontology的灵感。此外，文章还讨论了大语言模型时代的符号approaches。<details>
<summary>Abstract</summary>
Human knowledge is subject to uncertainties, imprecision, incompleteness and inconsistencies. Moreover, the meaning of many everyday terms is dependent on the context. That poses a huge challenge for the Semantic Web. This paper introduces work on an intuitive notation and model for defeasible reasoning with imperfect knowledge, and relates it to previous work on argumentation theory. PKN is to N3 as defeasible reasoning is to deductive logic. Further work is needed on an intuitive syntax for describing reasoning strategies and tactics in declarative terms, drawing upon the AIF ontology for inspiration. The paper closes with observations on symbolic approaches in the era of large language models.
</details>
<details>
<summary>摘要</summary>
人类知识受到不确定性、不精确性、不完整性和不一致性的影响。此外，许多日常用语的意义取决于上下文。这 pose 巨大挑战 дляsemantic web。本文介绍了一种直观 notation 和模型，用于解决不确定性推理，并与之前的 argumentation theory 相关。PKN 相当于 N3，而 defeasible reasoning 相当于deductive logic。进一步的工作需要对推理策略和 тактики的描述使用声明性语言， drawing upon the AIF ontology for inspiration。文章结束时，讨论了使用 symbolic approaches in the era of large language models。Note: "PKN" stands for "Prefered Knowledge Network", and "AIF" stands for "Approximate Inference Framework".
</details></li>
</ul>
<hr>
<h2 id="In-context-Interference-in-Chat-based-Large-Language-Models"><a href="#In-context-Interference-in-Chat-based-Large-Language-Models" class="headerlink" title="In-context Interference in Chat-based Large Language Models"></a>In-context Interference in Chat-based Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12727">http://arxiv.org/abs/2309.12727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Nuertey Coleman, Julio Hurtado, Vincenzo Lomonaco</li>
<li>for: This paper aims to study the limitations of in-context learning in large language models (LLMs) and its impact on the model’s performance.</li>
<li>methods: The paper uses a black-box scenario to evaluate the in-context learning ability of LLMs, and proposes an evaluation benchmark based on the bAbI dataset.</li>
<li>results: The study shows that in-context learning can lead to interference between information continually flowing in the context, causing the model to forget previously learned knowledge and reducing its performance.<details>
<summary>Abstract</summary>
Large language models (LLMs) have had a huge impact on society due to their impressive capabilities and vast knowledge of the world. Various applications and tools have been created that allow users to interact with these models in a black-box scenario. However, one limitation of this scenario is that users cannot modify the internal knowledge of the model, and the only way to add or modify internal knowledge is by explicitly mentioning it to the model during the current interaction. This learning process is called in-context training, and it refers to training that is confined to the user's current session or context. In-context learning has significant applications, but also has limitations that are seldom studied. In this paper, we present a study that shows how the model can suffer from interference between information that continually flows in the context, causing it to forget previously learned knowledge, which can reduce the model's performance. Along with showing the problem, we propose an evaluation benchmark based on the bAbI dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="H2O-An-Improved-Framework-for-Hybrid-Offline-and-Online-RL-with-Dynamics-Gaps"><a href="#H2O-An-Improved-Framework-for-Hybrid-Offline-and-Online-RL-with-Dynamics-Gaps" class="headerlink" title="H2O+: An Improved Framework for Hybrid Offline-and-Online RL with Dynamics Gaps"></a>H2O+: An Improved Framework for Hybrid Offline-and-Online RL with Dynamics Gaps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12716">http://arxiv.org/abs/2309.12716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyi Niu, Tianying Ji, Bingqi Liu, Haocheng Zhao, Xiangyu Zhu, Jianying Zheng, Pengfei Huang, Guyue Zhou, Jianming Hu, Xianyuan Zhan</li>
<li>for: This paper focuses on solving real-world complex tasks using reinforcement learning (RL) in imperfect simulation environments and with limited data.</li>
<li>methods: The authors propose a new algorithm called H2O+, which combines offline and online learning methods to address the challenges of sim-to-real transfer and dynamics gaps.</li>
<li>results: The proposed algorithm demonstrates superior performance and flexibility in both simulation and real-world robotics experiments compared to advanced cross-domain online and offline RL algorithms.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文关注使用强化学习（RL）解决实际世界中复杂任务，不需要高精度模拟环境或大量的离线数据。</li>
<li>methods: 作者提出了一种新的算法H2O+，它结合了离线和在线学习方法，以弥补模拟环境和实际环境之间的动态差异。</li>
<li>results: 提出的算法在实验和实际 робо术中表现出了superior性能和灵活性，与高级跨领域在线和离线RL算法相比。<details>
<summary>Abstract</summary>
Solving real-world complex tasks using reinforcement learning (RL) without high-fidelity simulation environments or large amounts of offline data can be quite challenging. Online RL agents trained in imperfect simulation environments can suffer from severe sim-to-real issues. Offline RL approaches although bypass the need for simulators, often pose demanding requirements on the size and quality of the offline datasets. The recently emerged hybrid offline-and-online RL provides an attractive framework that enables joint use of limited offline data and imperfect simulator for transferable policy learning. In this paper, we develop a new algorithm, called H2O+, which offers great flexibility to bridge various choices of offline and online learning methods, while also accounting for dynamics gaps between the real and simulation environment. Through extensive simulation and real-world robotics experiments, we demonstrate superior performance and flexibility over advanced cross-domain online and offline RL algorithms.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:解决实际世界中复杂任务使用回归学习（RL）无需高精度模拟环境或大量的离线数据可以很困难。在线RL代理在不完美的模拟环境中训练后可能会受到严重的模拟到实际（sim-to-real）问题。离线RL方法虽然不需要模拟器，但通常有训练离线数据的质量和量的严格要求。最近出现的混合在线-离线RL提供了一个吸引人的框架，允许在有限的离线数据和不完美的模拟器之间进行可转移的政策学习。在这篇论文中，我们开发了一个新的算法 called H2O+，它提供了很大的灵活性，可以将不同的在线和离线学习方法相互连接，同时也考虑到实际和模拟环境之间的动力差异。通过了较为广泛的模拟和实际机器人实验，我们展示了superior的性能和灵活性，与先进的跨Domain在线和离线RL算法相比。
</details></li>
</ul>
<hr>
<h2 id="The-Mathematical-Game"><a href="#The-Mathematical-Game" class="headerlink" title="The Mathematical Game"></a>The Mathematical Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12711">http://arxiv.org/abs/2309.12711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xploitspeeds/Bookmarklet-Hacks-For-School">https://github.com/xploitspeeds/Bookmarklet-Hacks-For-School</a></li>
<li>paper_authors: Marc Pierre, Quentin Cohen-Solal, Tristan Cazenave</li>
<li>for: 这 paper 用于提高 Holophrasm theorem prover 的性能，使用其他游戏搜索算法。</li>
<li>methods: 这 paper 使用 MCTS combined with 神经网络来实现自动证明。</li>
<li>results: 该 paper 提出了一种基于 MCTS 和神经网络的自动证明方法，以提高 Holophrasm theorem prover 的性能。<details>
<summary>Abstract</summary>
Monte Carlo Tree Search can be used for automated theorem proving. Holophrasm is a neural theorem prover using MCTS combined with neural networks for the policy and the evaluation. In this paper we propose to improve the performance of the Holophrasm theorem prover using other game tree search algorithms.
</details>
<details>
<summary>摘要</summary>
“蒙特卡罗树搜寻”可以用于自动证明。“Holophrasm”是一个使用蒙特卡罗树搜寻和神经网络来决策和评估的神经证明器。在这篇论文中，我们提议使用其他游戏树搜寻算法来提高Holophrasm证明器的性能。
</details></li>
</ul>
<hr>
<h2 id="PointSSC-A-Cooperative-Vehicle-Infrastructure-Point-Cloud-Benchmark-for-Semantic-Scene-Completion"><a href="#PointSSC-A-Cooperative-Vehicle-Infrastructure-Point-Cloud-Benchmark-for-Semantic-Scene-Completion" class="headerlink" title="PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion"></a>PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12708">http://arxiv.org/abs/2309.12708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Yan, Boda Liu, Jianfei Ai, Qinbu Li, Ru Wan, Jian Pu</li>
<li>for: 该论文旨在提出一个 Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion，用于驱动semantic point cloud completion的技术进步。</li>
<li>methods: 该论文使用了一种基于LiDAR的模型，包括一个Spatial-Aware Transformer для全球和本地特征提取，以及一个Completion and Segmentation Cooperative Module для联合完成和分类。</li>
<li>results: 该论文提出了一个名为PointSSC的共同自动车辆-基础设施点云benchmark，用于测试和评估semantic point cloud completion技术的进步。<details>
<summary>Abstract</summary>
Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navigation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Label-Noise-Transition-Matrix-Estimation-with-Label-Correlations-Theory-and-Algorithm"><a href="#Multi-Label-Noise-Transition-Matrix-Estimation-with-Label-Correlations-Theory-and-Algorithm" class="headerlink" title="Multi-Label Noise Transition Matrix Estimation with Label Correlations: Theory and Algorithm"></a>Multi-Label Noise Transition Matrix Estimation with Label Correlations: Theory and Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12706">http://arxiv.org/abs/2309.12706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tmllab/Multi-Label-T">https://github.com/tmllab/Multi-Label-T</a></li>
<li>paper_authors: Shikun Li, Xiaobo Xia, Hansong Zhang, Shiming Ge, Tongliang Liu</li>
<li>for: 这篇论文旨在解决多标签学习中的噪声问题，因为收集大规模准确标签变得更加困难。</li>
<li>methods: 这篇论文提出了一种使用过渡矩阵来模型多标签噪声的方法，并利用标签相互关系来估计噪声过渡矩阵。</li>
<li>results: 这篇论文提出了一种新的估计器，可以在不需要 anchor point 或准确适应噪声类 posterior 的情况下估计多标签噪声过渡矩阵。这种估计器基于标签相互关系，并使用样本选择技术来提取净标签相互关系所含信息。<details>
<summary>Abstract</summary>
Noisy multi-label learning has garnered increasing attention due to the challenges posed by collecting large-scale accurate labels, making noisy labels a more practical alternative. Motivated by noisy multi-class learning, the introduction of transition matrices can help model multi-label noise and enable the development of statistically consistent algorithms for noisy multi-label learning. However, estimating multi-label noise transition matrices remains a challenging task, as most existing estimators in noisy multi-class learning rely on anchor points and accurate fitting of noisy class posteriors, which is hard to satisfy in noisy multi-label learning. In this paper, we address this problem by first investigating the identifiability of class-dependent transition matrices in noisy multi-label learning. Building upon the identifiability results, we propose a novel estimator that leverages label correlations without the need for anchor points or precise fitting of noisy class posteriors. Specifically, we first estimate the occurrence probability of two noisy labels to capture noisy label correlations. Subsequently, we employ sample selection techniques to extract information implying clean label correlations, which are then used to estimate the occurrence probability of one noisy label when a certain clean label appears. By exploiting the mismatches in label correlations implied by these occurrence probabilities, we demonstrate that the transition matrix becomes identifiable and can be acquired by solving a bilinear decomposition problem. Theoretically, we establish an estimation error bound for our multi-label transition matrix estimator and derive a generalization error bound for our statistically consistent algorithm. Empirically, we validate the effectiveness of our estimator in estimating multi-label noise transition matrices, leading to excellent classification performance.
</details>
<details>
<summary>摘要</summary>
噪声多标签学习已经吸引了越来越多的关注，因为收集大规模准确标签变得更加困难。为了模型多标签噪声，引入过渡矩阵可以帮助模型多标签噪声。然而，估计多标签噪声过渡矩阵仍然是一个挑战，因为大多数现有的估计器在噪声多类学习中使用锚点和准确地适应噪声类 posterior，这在多标签噪声学习中很难实现。在这篇论文中，我们解决这个问题，首先研究多标签噪声过渡矩阵的可识别性。基于可识别性结果，我们提出了一种新的估计器，它利用标签相互关系来估计噪声标签的过渡矩阵。具体来说，我们首先估计两个噪声标签之间的发生概率，以捕捉噪声标签之间的相互关系。然后，我们使用样本选择技术提取信息，以便从中提取净标签相关信息。最后，我们使用这些发生概率来估计一个噪声标签的过渡矩阵，并解决一个二元分解问题来获取过渡矩阵。理论上，我们建立了估计误差 bound 和一般化误差 bound，以证明我们的多标签过渡矩阵估计器的可靠性。实验证明了我们的估计器在估计多标签噪声过渡矩阵时的效果。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Conservative-Q-Learning-for-Offline-Multi-agent-Reinforcement-Learning"><a href="#Counterfactual-Conservative-Q-Learning-for-Offline-Multi-agent-Reinforcement-Learning" class="headerlink" title="Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning"></a>Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12696">http://arxiv.org/abs/2309.12696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-rllab/CFCQL">https://github.com/thu-rllab/CFCQL</a></li>
<li>paper_authors: Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, Xiangyang Ji</li>
<li>for: 解决多智能机器人在离线多智能学习中的分布偏移和维度高问题，提高行动外部分布(OOD)和价值误差现象的问题。</li>
<li>methods: 提出了一种新的多智能离线Q学习算法CounterFactual Conservative Q-Learning（CFCQL），通过计算每个机器人的保守补偿来实现保守价值估计。</li>
<li>results: 在四个环境中，包括整数和浮点动作的设定，以及现有和自己制作的数据集上，CFCQL比既有方法高效，尤其是当机器人数量很大时。<details>
<summary>Abstract</summary>
Offline multi-agent reinforcement learning is challenging due to the coupling effect of both distribution shift issue common in offline setting and the high dimension issue common in multi-agent setting, making the action out-of-distribution (OOD) and value overestimation phenomenon excessively severe. Tomitigate this problem, we propose a novel multi-agent offline RL algorithm, named CounterFactual Conservative Q-Learning (CFCQL) to conduct conservative value estimation. Rather than regarding all the agents as a high dimensional single one and directly applying single agent methods to it, CFCQL calculates conservative regularization for each agent separately in a counterfactual way and then linearly combines them to realize an overall conservative value estimation. We prove that it still enjoys the underestimation property and the performance guarantee as those single agent conservative methods do, but the induced regularization and safe policy improvement bound are independent of the agent number, which is therefore theoretically superior to the direct treatment referred to above, especially when the agent number is large. We further conduct experiments on four environments including both discrete and continuous action settings on both existing and our man-made datasets, demonstrating that CFCQL outperforms existing methods on most datasets and even with a remarkable margin on some of them.
</details>
<details>
<summary>摘要</summary>
偏向多智能体异线上学习是因为偏向分布shift问题和高维度问题的交互作用，导致行为out-of-distribution (OOD)和价值误估现象过于严重。为了解决这个问题，我们提出了一种新的多智能体异线上RL算法，名为CounterFactual Conservative Q-Learning (CFCQL)，用于进行保守的价值估计。而不是将所有智能体视为一个高维度单一的智能体并直接应用单个智能体方法，CFCQL计算每个智能体的保守补偿 separately在反对方法下，然后将其线性组合以实现全局保守价值估计。我们证明了它仍然具有下预测性和性能保证，但是引入的补偿和安全策略改进 bound 独立于智能体数量，因此是理论上的超越直接处理。我们进一步在四个环境中进行了实验，包括 both 某些精度和连续动作设置，并在我们自己制作的数据集上进行了实验，示出CFCQL在大多数数据集上表现出优于现有方法，甚至在一些数据集上具有很大的差距。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Graph-Representation-of-the-Environment-through-Local-and-Cloud-Computation"><a href="#Enhancing-Graph-Representation-of-the-Environment-through-Local-and-Cloud-Computation" class="headerlink" title="Enhancing Graph Representation of the Environment through Local and Cloud Computation"></a>Enhancing Graph Representation of the Environment through Local and Cloud Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12692">http://arxiv.org/abs/2309.12692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Francesco Argenziano, Vincenzo Suriani, Daniele Nardi</li>
<li>for:  bridging the gap between low-level sensor readings and high-level semantic understanding</li>
<li>methods: combines classical computer vision tools with modern computer vision cloud services, incorporates an ontology hierarchy with over 800 object classes</li>
<li>results: allows for the handling of small objects and integration into the semantic representation of the environment<details>
<summary>Abstract</summary>
Enriching the robot representation of the operational environment is a challenging task that aims at bridging the gap between low-level sensor readings and high-level semantic understanding. Having a rich representation often requires computationally demanding architectures and pure point cloud based detection systems that struggle when dealing with everyday objects that have to be handled by the robot. To overcome these issues, we propose a graph-based representation that addresses this gap by providing a semantic representation of robot environments from multiple sources. In fact, to acquire information from the environment, the framework combines classical computer vision tools with modern computer vision cloud services, ensuring computational feasibility on onboard hardware. By incorporating an ontology hierarchy with over 800 object classes, the framework achieves cross-domain adaptability, eliminating the need for environment-specific tools. The proposed approach allows us to handle also small objects and integrate them into the semantic representation of the environment. The approach is implemented in the Robot Operating System (ROS) using the RViz visualizer for environment representation. This work is a first step towards the development of a general-purpose framework, to facilitate intuitive interaction and navigation across different domains.
</details>
<details>
<summary>摘要</summary>
Our approach combines classical computer vision tools with modern computer vision cloud services to acquire information from the environment. By incorporating an ontology hierarchy with over 800 object classes, our framework achieves cross-domain adaptability, eliminating the need for environment-specific tools. This allows us to handle small objects and integrate them into the semantic representation of the environment.We have implemented our approach in the Robot Operating System (ROS) using the RViz visualizer for environment representation. This is a first step towards the development of a general-purpose framework for intuitive interaction and navigation across different domains.
</details></li>
</ul>
<hr>
<h2 id="QAL-BP-An-Augmented-Lagrangian-Quantum-Approach-for-Bin-Packing-Problem"><a href="#QAL-BP-An-Augmented-Lagrangian-Quantum-Approach-for-Bin-Packing-Problem" class="headerlink" title="QAL-BP: An Augmented Lagrangian Quantum Approach for Bin Packing Problem"></a>QAL-BP: An Augmented Lagrangian Quantum Approach for Bin Packing Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12678">http://arxiv.org/abs/2309.12678</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lorenz92/qal-bp">https://github.com/lorenz92/qal-bp</a></li>
<li>paper_authors: Lorenzo Cellini, Antonio Macaluso, Michele Lombardi</li>
<li>for: 解决 bin packing 问题，一个 NP-hard 问题，寻找高效的解决方案。</li>
<li>methods: 使用 quantum technologies，尤其是 quantum computing，来解决这个问题。提出了一种新的 Quadratic Unconstrained Binary Optimization (QUBO) 模型，可以更好地处理这个问题。</li>
<li>results: 实验结果表明，使用 quantum annealing Device 可以更有效地解决 bin packing 问题，并且比 classical solvers 更快。<details>
<summary>Abstract</summary>
The bin packing is a well-known NP-Hard problem in the domain of artificial intelligence, posing significant challenges in finding efficient solutions. Conversely, recent advancements in quantum technologies have shown promising potential for achieving substantial computational speedup, particularly in certain problem classes, such as combinatorial optimization. In this study, we introduce QAL-BP, a novel Quadratic Unconstrained Binary Optimization (QUBO) formulation designed specifically for bin packing and suitable for quantum computation. QAL-BP utilizes the augmented Lagrangian method to incorporate the bin packing constraints into the objective function while also facilitating an analytical estimation of heuristic, but empirically robust, penalty multipliers. This approach leads to a more versatile and generalizable model that eliminates the need for empirically calculating instance-dependent Lagrangian coefficients, a requirement commonly encountered in alternative QUBO formulations for similar problems. To assess the effectiveness of our proposed approach, we conduct experiments on a set of bin-packing instances using a real Quantum Annealing device. Additionally, we compare the results with those obtained from two different classical solvers, namely simulated annealing and Gurobi. The experimental findings not only confirm the correctness of the proposed formulation but also demonstrate the potential of quantum computation in effectively solving the bin-packing problem, particularly as more reliable quantum technology becomes available.
</details>
<details>
<summary>摘要</summary>
bin packing 是人工智能领域的一个非常知名的 NP-硬Problem，它提出了很大的计算挑战。然而，最近的量子技术发展有显著的潜在能力，特别是在 combinatorial optimization 中，以获得显著的计算速度提升。在这项研究中，我们介绍了 QAL-BP，一种特有的 Quadratic Unconstrained Binary Optimization (QUBO) 形式，适用于 bin packing 问题并且适用于量子计算。QAL-BP 利用了扩展拉格朗日方法，将箱包约束直接添加到目标函数中，同时还可以 analytically 计算 heuristic，但是实际上是可靠的 penalty multipliers。这种方法使得我们的模型更加通用和可重用，消除了对实际计算 instance-dependent Lagrangian coefficients 的需求，这种需求通常在类似问题的alternative QUBO formulations中出现。为评估我们提出的方法的有效性，我们在一组 bin-packing 实例上进行了实验，使用了真实的 Quantum Annealing 设备。此外，我们还与 simulated annealing 和 Gurobi 两种类型的 classical solver进行了比较。实验结果不仅证明了我们的提出的形式的正确性，还表明了量子计算在解决箱包问题上的潜在能力，特别是随着可靠的量子技术的发展。
</details></li>
</ul>
<hr>
<h2 id="TrTr-A-Versatile-Pre-Trained-Large-Traffic-Model-based-on-Transformer-for-Capturing-Trajectory-Diversity-in-Vehicle-Population"><a href="#TrTr-A-Versatile-Pre-Trained-Large-Traffic-Model-based-on-Transformer-for-Capturing-Trajectory-Diversity-in-Vehicle-Population" class="headerlink" title="TrTr: A Versatile Pre-Trained Large Traffic Model based on Transformer for Capturing Trajectory Diversity in Vehicle Population"></a>TrTr: A Versatile Pre-Trained Large Traffic Model based on Transformer for Capturing Trajectory Diversity in Vehicle Population</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12677">http://arxiv.org/abs/2309.12677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruyi Feng, Zhibin Li, Bowen Liu, Yan Ding, Ou Zheng</li>
<li>For: The paper aims to learn the diversity of trajectories within vehicle populations using the Transformer architecture, which can handle large-scale parameters and capture the spatial distribution of vehicles.* Methods: The authors apply the Transformer architecture to traffic tasks and design specific pre-training tasks to improve the model’s performance. They also create a data structure tailored to the attention mechanism and introduce noises that correspond to spatio-temporal demands.* Results: The pre-trained model demonstrates excellent performance in capturing the spatial distribution of the vehicle population, with no instances of vehicle overlap and an RMSE of 0.6059 compared to ground truth values. In the context of time series prediction, approximately 95% of the predicted trajectories’ speeds closely align with the true speeds, within a deviation of 7.5144m&#x2F;s. The model also exhibits robustness in the stability test and provides a good basis for downstream fine-tuning tasks.Here’s the format you requested:* For: 学习车辆群体的路径多样性* Methods: 使用变换器架构，设计特定的预训练任务，创建适合注意机制的数据结构，引入相应的空间时间噪声* Results: 预训练模型在捕捉车辆群体的空间分布方面表现出色，无车辆重叠，RMSE为0.6059；在时间序列预测任务中，预测速度与真实速度相似，偏差为7.5144m&#x2F;s；模型在稳定性测试中表现稳定，可以长期预测时间序列，并且展现出多样化的驾驶行为。<details>
<summary>Abstract</summary>
Understanding trajectory diversity is a fundamental aspect of addressing practical traffic tasks. However, capturing the diversity of trajectories presents challenges, particularly with traditional machine learning and recurrent neural networks due to the requirement of large-scale parameters. The emerging Transformer technology, renowned for its parallel computation capabilities enabling the utilization of models with hundreds of millions of parameters, offers a promising solution. In this study, we apply the Transformer architecture to traffic tasks, aiming to learn the diversity of trajectories within vehicle populations. We analyze the Transformer's attention mechanism and its adaptability to the goals of traffic tasks, and subsequently, design specific pre-training tasks. To achieve this, we create a data structure tailored to the attention mechanism and introduce a set of noises that correspond to spatio-temporal demands, which are incorporated into the structured data during the pre-training process. The designed pre-training model demonstrates excellent performance in capturing the spatial distribution of the vehicle population, with no instances of vehicle overlap and an RMSE of 0.6059 when compared to the ground truth values. In the context of time series prediction, approximately 95% of the predicted trajectories' speeds closely align with the true speeds, within a deviation of 7.5144m/s. Furthermore, in the stability test, the model exhibits robustness by continuously predicting a time series ten times longer than the input sequence, delivering smooth trajectories and showcasing diverse driving behaviors. The pre-trained model also provides a good basis for downstream fine-tuning tasks. The number of parameters of our model is over 50 million.
</details>
<details>
<summary>摘要</summary>
理解轨迹多样性是实际交通任务的基本方面。然而，捕捉轨迹多样性带来挑战，特别是使用传统的机器学习和回归神经网络，因为它们需要庞大的参数量。然而，出现的Transformer技术，因其可平行计算能力，使得使用大量参数的模型变得可能。在这个研究中，我们将Transformer架构应用到交通任务中，以学习车辆人口中的轨迹多样性。我们分析Transformer的注意机制和它的适应性，然后设计特定的预训练任务。为此，我们创建了适应注意机制的数据结构，并引入了相应的噪声，这些噪声在预训练过程中被包含到结构化数据中。预训练模型显示出色的性能，可以准确地捕捉车辆人口的空间分布，没有车辆重叠，RMSE为0.6059，相比真实值。在时间序列预测任务中，大约95%的预测轨迹速度与真实速度高度相似，差异在7.5144米/秒之间。此外，在稳定测试中，模型表现了稳定性，连续预测了10个输入序列的时间序列，输出了平滑的轨迹和多样的驾驶行为。预训练模型还提供了下游细化任务的良好基础。我们的模型参数数量超过5000万。
</details></li>
</ul>
<hr>
<h2 id="Vision-Transformers-for-Computer-Go"><a href="#Vision-Transformers-for-Computer-Go" class="headerlink" title="Vision Transformers for Computer Go"></a>Vision Transformers for Computer Go</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12675">http://arxiv.org/abs/2309.12675</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/assasinator/Swin_Transformers">https://github.com/assasinator/Swin_Transformers</a></li>
<li>paper_authors: Amani Sagri, Tristan Cazenave, Jérôme Arjonilla, Abdallah Saffidine</li>
<li>for:  investigate the application of transformers in the game of Go, specifically in the analysis of the Transformer in Vision.</li>
<li>methods: compare transformers to usual Residual Networks.</li>
<li>results: highlight the substantial role that transformers can play in the game of Go, through a detailed analysis of numerous points such as prediction accuracy, win rates, memory, speed, size, or even learning rate.<details>
<summary>Abstract</summary>
Motivated by the success of transformers in various fields, such as language understanding and image analysis, this investigation explores their application in the context of the game of Go. In particular, our study focuses on the analysis of the Transformer in Vision. Through a detailed analysis of numerous points such as prediction accuracy, win rates, memory, speed, size, or even learning rate, we have been able to highlight the substantial role that transformers can play in the game of Go. This study was carried out by comparing them to the usual Residual Networks.
</details>
<details>
<summary>摘要</summary>
受到 transformer 在不同领域的成功启发，本研究探讨它们在围棋游戏中的应用。特别是我们在视觉领域中使用 transformer 进行分析。通过对各种指标（如预测精度、赢局率、内存、速度、大小、学习率等）的细致分析，我们得出了 transformer 在围棋游戏中的重要作用。这个研究通过与常见的 Residual Networks 进行比较，得出了这种结论。
</details></li>
</ul>
<hr>
<h2 id="On-Sparse-Modern-Hopfield-Model"><a href="#On-Sparse-Modern-Hopfield-Model" class="headerlink" title="On Sparse Modern Hopfield Model"></a>On Sparse Modern Hopfield Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12673">http://arxiv.org/abs/2309.12673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, Han Liu</li>
<li>For: 本文提出了一种含有稀畴结构的现代戴维尔模型（Sparse Modern Hopfield Model），作为现代戴维尔模型的稀畴扩展。* Methods: 本文使用了稀畴注意机制，并提出了一种关于稀畴能函数的关闭式表示，以及基于此的稀畴记忆抽取动力学。* Results: 本文提出了一种基于稀畴结构的记忆抽取动力学，并证明其一步采样等价于稀畴结构的注意机制。此外，本文还提供了稀畴取决因子的记忆抽取误差 bound，并证明其比密集模型更紧。<details>
<summary>Abstract</summary>
We introduce the sparse modern Hopfield model as a sparse extension of the modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to the sparse attention mechanism. Theoretically, our key contribution is a principled derivation of a closed-form sparse Hopfield energy using the convex conjugate of the sparse entropic regularizer. Building upon this, we derive the sparse memory retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention. Importantly, we provide a sparsity-dependent memory retrieval error bound which is provably tighter than its dense analog. The conditions for the benefits of sparsity to arise are therefore identified and discussed. In addition, we show that the sparse modern Hopfield model maintains the robust theoretical properties of its dense counterpart, including rapid fixed point convergence and exponential memory capacity. Empirically, we use both synthetic and real-world datasets to demonstrate that the sparse Hopfield model outperforms its dense counterpart in many situations.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个对� tenir 的现代丰码模型，这是一个对于现代丰码模型的延伸。这个模型具有一个备受归属的记忆回传动态，它的一步近似对应于简短的注意力机制。理论上，我们的主要贡献是通过对简短Entropic regularizer的对偶函数而 derive 一个关闭的简短丰码能量函数。建基于这个能量函数，我们 derive 简短的记忆回传动态，并证明它的一步近似与简短结构的注意力相等。更重要的是，我们提供了一个对简短性的记忆回传错误 bound，这是与对简短性的预设相比较为紧的。因此，我们可以识别出简短性在哪些情况下获得好的情况，并讨论这些情况。此外，我们显示了对简短性的丰码模型保持了现代丰码模型的理论性质，包括快速的稳定点收敛和exponential的记忆容量。实验上，我们使用了 synthetic 和实际世界数据来显示，简短丰码模型在许多情况下比对简短性的丰码模型表现更好。
</details></li>
</ul>
<hr>
<h2 id="How-to-Fine-tune-the-Model-Unified-Model-Shift-and-Model-Bias-Policy-Optimization"><a href="#How-to-Fine-tune-the-Model-Unified-Model-Shift-and-Model-Bias-Policy-Optimization" class="headerlink" title="How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization"></a>How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12671">http://arxiv.org/abs/2309.12671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/betray12138/unified-model-shift-and-model-bias-policy-optimization">https://github.com/betray12138/unified-model-shift-and-model-bias-policy-optimization</a></li>
<li>paper_authors: Hai Zhang, Hang Yu, Junqiao Zhao, Di Zhang, Chang Huang, Hongtu Zhou, Xiao Zhang, Chen Ye</li>
<li>for: 这篇论文主要目标是提出一种能够满足性能保证的模型基于强化学习（MBRL）算法设计方法。</li>
<li>methods: 该方法使用返回差值来导引模型学习，并通过自适应调整模型更新来保证性能提升。</li>
<li>results: 实验结果表明，该算法在多个复杂任务上达到了状态之最的性能。<details>
<summary>Abstract</summary>
Designing and deriving effective model-based reinforcement learning (MBRL) algorithms with a performance improvement guarantee is challenging, mainly attributed to the high coupling between model learning and policy optimization. Many prior methods that rely on return discrepancy to guide model learning ignore the impacts of model shift, which can lead to performance deterioration due to excessive model updates. Other methods use performance difference bound to explicitly consider model shift. However, these methods rely on a fixed threshold to constrain model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. In this paper, we theoretically derive an optimization objective that can unify model shift and model bias and then formulate a fine-tuning process. This process adaptively adjusts the model updates to get a performance improvement guarantee while avoiding model overfitting. Based on these, we develop a straightforward algorithm USB-PO (Unified model Shift and model Bias Policy Optimization). Empirical results show that USB-PO achieves state-of-the-art performance on several challenging benchmark tasks.
</details>
<details>
<summary>摘要</summary>
“设计和推导具有性能改善保证的模型基于学习（MBRL）算法是具有挑战，主要是因为模型学习和政策优化之间存在高度的整合。许多先前的方法将返回差异用来引导模型学习，忽略模型转移的影响，可能导致因为过度更新模型而导致性能下降。其他方法使用性能差异 bound来考虑模型转移，但这些方法靠摄设定的阈值来限制模型转移，导致执行过程中的依赖性和缺乏灵活性。在这篇文章中，我们理论上 derive 一个优化目标，可以将模型转移和模型偏差统一，然后构成一个精致调整过程。这个过程可以适应性地调整模型更新，以确保性能改善保证，同时避免模型过滤。基于这些，我们开发了一个简单的算法 USB-PO（对于模型转移和模型偏差的政策优化）。实验结果显示，USB-PO 在一些具有挑战性的 bencark task 上实现了顶尖性能。”
</details></li>
</ul>
<hr>
<h2 id="Natural-revision-is-contingently-conditionalized-revision"><a href="#Natural-revision-is-contingently-conditionalized-revision" class="headerlink" title="Natural revision is contingently-conditionalized revision"></a>Natural revision is contingently-conditionalized revision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12655">http://arxiv.org/abs/2309.12655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paolo Liberatore</li>
<li>for: 本研究旨在探讨自然修订的扩展，以满足更复杂的条件。</li>
<li>methods: 本研究使用了自然修订的基本原则，包括最小变化、等同和纯洁。</li>
<li>results: 研究发现，自然修订的扩展可以帮助解决一些Counterexample，但也存在一些限制。<details>
<summary>Abstract</summary>
Natural revision seems so natural: it changes beliefs as little as possible to incorporate new information. Yet, some counterexamples show it wrong. It is so conservative that it never fully believes. It only believes in the current conditions. This is right in some cases and wrong in others. Which is which? The answer requires extending natural revision from simple formulae expressing universal truths (something holds) to conditionals expressing conditional truth (something holds in certain conditions). The extension is based on the basic principles natural revision follows, identified as minimal change, indifference and naivety: change beliefs as little as possible; equate the likeliness of scenarios by default; believe all until contradicted. The extension says that natural revision restricts changes to the current conditions. A comparison with an unrestricting revision shows what exactly the current conditions are. It is not what currently considered true if it contradicts the new information. It includes something more and more unlikely until the new information is at least possible.
</details>
<details>
<summary>摘要</summary>
自然修订似乎很自然：它最小化信念更改，以接纳新信息。然而，一些例外证明它错误。它太保守，从未全heartedly 信任。它只信任当前情况。这对于一些情况是正确的，对于另外一些情况是错误的。哪些是哪些？答案需要扩展自然修订从简单的公式表达universal truth（something holds）扩展到 conditionals表达条件真理（something holds in certain conditions）。这种扩展基于自然修订的基本原则：变化信念最少化，默许enario  equality 和简单信任（change beliefs as little as possible; equate the likeliness of scenarios by default; believe all until contradicted）。这种扩展说明自然修订限制更改到当前情况。与不限制的修订进行比较显示了现在所考虑的真实情况是什么。不是现在被认为是真的，如果它与新信息相 contradicted。而是更加不可能的事情，直到新信息至少可能。
</details></li>
</ul>
<hr>
<h2 id="Are-Deep-Learning-Classification-Results-Obtained-on-CT-Scans-Fair-and-Interpretable"><a href="#Are-Deep-Learning-Classification-Results-Obtained-on-CT-Scans-Fair-and-Interpretable" class="headerlink" title="Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?"></a>Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12632">http://arxiv.org/abs/2309.12632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamad M. A. Ashames, Ahmet Demir, Omer N. Gerek, Mehmet Fidan, M. Bilginer Gulmezoglu, Semih Ergin, Mehmet Koc, Atalay Barkana, Cuneyt Calisir</li>
<li>for: 这篇研究的目的是提高医学影像识别的可解释性和准确性。</li>
<li>methods: 这篇研究使用了深度学习方法，并将患者级别分类 strict 地隔离在训练、验证和测试数据集中。</li>
<li>results: 研究发现，使用传统的不平等排序方法训练深度学习模型可能会报告误导性的准确率，并且学习无关的特征。但是，使用严格的患者级别分类方法训练深度学习模型可以保持其准确率，并且在新患者影像上进行测试时仍然表现出高度的可解释性。<details>
<summary>Abstract</summary>
Following the great success of various deep learning methods in image and object classification, the biomedical image processing society is also overwhelmed with their applications to various automatic diagnosis cases. Unfortunately, most of the deep learning-based classification attempts in the literature solely focus on the aim of extreme accuracy scores, without considering interpretability, or patient-wise separation of training and test data. For example, most lung nodule classification papers using deep learning randomly shuffle data and split it into training, validation, and test sets, causing certain images from the CT scan of a person to be in the training set, while other images of the exact same person to be in the validation or testing image sets. This can result in reporting misleading accuracy rates and the learning of irrelevant features, ultimately reducing the real-life usability of these models. When the deep neural networks trained on the traditional, unfair data shuffling method are challenged with new patient images, it is observed that the trained models perform poorly. In contrast, deep neural networks trained with strict patient-level separation maintain their accuracy rates even when new patient images are tested. Heat-map visualizations of the activations of the deep neural networks trained with strict patient-level separation indicate a higher degree of focus on the relevant nodules. We argue that the research question posed in the title has a positive answer only if the deep neural networks are trained with images of patients that are strictly isolated from the validation and testing patient sets.
</details>
<details>
<summary>摘要</summary>
Traditional deep learning methods for image and object classification have achieved great success, and the biomedical image processing community has also applied these methods to various automatic diagnosis cases. However, most deep learning-based classification attempts in the literature focus solely on achieving high accuracy scores without considering interpretability or patient-wise separation of training and test data. For example, most lung nodule classification papers using deep learning randomly shuffle data and split it into training, validation, and test sets, causing certain images from the same person's CT scan to be in the training set, while other images of the exact same person are in the validation or testing image sets. This can lead to misleading accuracy rates and the learning of irrelevant features, ultimately reducing the real-life usability of these models. When the deep neural networks trained on the traditional, unfair data shuffling method are challenged with new patient images, they perform poorly. In contrast, deep neural networks trained with strict patient-level separation maintain their accuracy rates even when new patient images are tested. Heat-map visualizations of the activations of the deep neural networks trained with strict patient-level separation indicate a higher degree of focus on the relevant nodules. We argue that the research question posed in the title has a positive answer only if the deep neural networks are trained with images of patients that are strictly isolated from the validation and testing patient sets.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Quantum-Computing-based-System-for-Portfolio-Optimization-using-Future-Asset-Values-and-Automatic-Reduction-of-the-Investment-Universe"><a href="#A-Quantum-Computing-based-System-for-Portfolio-Optimization-using-Future-Asset-Values-and-Automatic-Reduction-of-the-Investment-Universe" class="headerlink" title="A Quantum Computing-based System for Portfolio Optimization using Future Asset Values and Automatic Reduction of the Investment Universe"></a>A Quantum Computing-based System for Portfolio Optimization using Future Asset Values and Automatic Reduction of the Investment Universe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12627">http://arxiv.org/abs/2309.12627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eneko Osaba, Guillaume Gelabert, Esther Villar-Rodriguez, Antón Asla, Izaskun Oregi</li>
<li>for: 这个研究是为了解决股票 portefolio优化问题，并使用量子计算技术来解决。</li>
<li>methods: 这个系统使用了未来资产预测值来模型，而不是历史值；此外，它还包括自动宇宙减少模块，以降低问题的复杂性。</li>
<li>results: 作者们提出了一个概述性的讨论，描述了不同模块的先进性表现。<details>
<summary>Abstract</summary>
One of the problems in quantitative finance that has received the most attention is the portfolio optimization problem. Regarding its solving, this problem has been approached using different techniques, with those related to quantum computing being especially prolific in recent years. In this study, we present a system called Quantum Computing-based System for Portfolio Optimization with Future Asset Values and Automatic Universe Reduction (Q4FuturePOP), which deals with the Portfolio Optimization Problem considering the following innovations: i) the developed tool is modeled for working with future prediction of assets, instead of historical values; and ii) Q4FuturePOP includes an automatic universe reduction module, which is conceived to intelligently reduce the complexity of the problem. We also introduce a brief discussion about the preliminary performance of the different modules that compose the prototypical version of Q4FuturePOP.
</details>
<details>
<summary>摘要</summary>
一个在量化金融中受到最多关注的问题是资产配置优化问题（Portfolio Optimization Problem）。它的解决方法有很多，其中使用量子计算技术的方法在过去几年中特别繁荣。在这篇研究中，我们介绍了一个名为量子计算基于系统 для资产配置优化与未来资产价值预测（Q4FuturePOP）的系统，它解决了资产配置优化问题，并包括以下两点创新：首先，该工具采用未来预测资产价值，而不是历史数据；其次，Q4FuturePOP包括自动宇宙减少模块，以智能减少问题的复杂性。我们还介绍了批处理版Q4FuturePOP的不同模块的初步性能。
</details></li>
</ul>
<hr>
<h2 id="Construction-contract-risk-identification-based-on-knowledge-augmented-language-model"><a href="#Construction-contract-risk-identification-based-on-knowledge-augmented-language-model" class="headerlink" title="Construction contract risk identification based on knowledge-augmented language model"></a>Construction contract risk identification based on knowledge-augmented language model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12626">http://arxiv.org/abs/2309.12626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saika Wong, Chunmo Zheng, Xing Su, Yinqiu Tang</li>
<li>for: 本研究旨在提高建筑项目合同审核效果，以避免潜在的损失。</li>
<li>methods: 本研究使用大型自然语言处理模型（LLM），并通过适应 contruction 合同知识来增强语言模型的风险识别能力。</li>
<li>results: 我们对实际的建筑合同进行了评估，并实现了良好的性能。此外，我们还研究了LLM如何在这种任务中运用逻辑思维，并提供了未来研究的建议。<details>
<summary>Abstract</summary>
Contract review is an essential step in construction projects to prevent potential losses. However, the current methods for reviewing construction contracts lack effectiveness and reliability, leading to time-consuming and error-prone processes. While large language models (LLMs) have shown promise in revolutionizing natural language processing (NLP) tasks, they struggle with domain-specific knowledge and addressing specialized issues. This paper presents a novel approach that leverages LLMs with construction contract knowledge to emulate the process of contract review by human experts. Our tuning-free approach incorporates construction contract domain knowledge to enhance language models for identifying construction contract risks. The use of a natural language when building the domain knowledge base facilitates practical implementation. We evaluated our method on real construction contracts and achieved solid performance. Additionally, we investigated how large language models employ logical thinking during the task and provide insights and recommendations for future research.
</details>
<details>
<summary>摘要</summary>
CONTRACT REVIEW IS AN ESSENTIAL STEP IN CONSTRUCTION PROJECTS TO PREVENT POTENTIAL LOSSES. HOWEVER, THE CURRENT METHODS FOR REVIEWING CONSTRUCTION CONTRACTS LACK EFFECTIVENESS AND RELIABILITY, LEADING TO TIME-CONSUMING AND ERROR-PRONE PROCESSES. WHILE LARGE LANGUAGE MODELS (LLMs) HAVE SHOWN PROMISE IN REVOLUTIONIZING NATURAL LANGUAGE PROCESSING (NLP) TASKS, THEY STRUGGLE WITH DOMAIN-SPECIFIC KNOWLEDGE AND ADDRESSING SPECIALIZED ISSUES. THIS PAPER PRESENTS A NOVEL APPROACH THAT LEVERAGES LLMs WITH CONSTRUCTION CONTRACT KNOWLEDGE TO EMULATE THE PROCESS OF CONTRACT REVIEW BY HUMAN EXPERTS. OUR TUNING-FREE APPROACH INCORPORATES CONSTRUCTION CONTRACT DOMAIN KNOWLEDGE TO ENHANCE LANGUAGE MODELS FOR IDENTIFYING CONSTRUCTION CONTRACT RISKS. THE USE OF A NATURAL LANGUAGE WHEN BUILDING THE DOMAIN KNOWLEDGE BASE FACILITATES PRACTICAL IMPLEMENTATION. WE EVALUATED OUR METHOD ON REAL CONSTRUCTION CONTRACTS AND ACHIEVED SOLID PERFORMANCE. ADDITIONALLY, WE INVESTIGATED HOW LARGE LANGUAGE MODELS EMPLOY LOGICAL THINKING DURING THE TASK AND PROVIDE INSIGHTS AND RECOMMENDATIONS FOR FUTURE RESEARCH.
</details></li>
</ul>
<hr>
<h2 id="DRG-LLaMA-Tuning-LLaMA-Model-to-Predict-Diagnosis-related-Group-for-Hospitalized-Patients"><a href="#DRG-LLaMA-Tuning-LLaMA-Model-to-Predict-Diagnosis-related-Group-for-Hospitalized-Patients" class="headerlink" title="DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients"></a>DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12625">http://arxiv.org/abs/2309.12625</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hanyin88/drg-llama">https://github.com/hanyin88/drg-llama</a></li>
<li>paper_authors: Hanyin Wang, Chufan Gao, Christopher Dantona, Bryan Hull, Jimeng Sun</li>
<li>For: This paper aims to improve the efficiency of the Diagnosis-Related Group (DRG) assignment process in the U.S. inpatient payment system by using an advanced large language model (LLM) fine-tuned on clinical notes.* Methods: The paper introduces DRG-LLaMA, a LLM fine-tuned on 236,192 MIMIC-IV discharge summaries using Low-Rank Adaptation (LoRA) to enhance DRG assignment. The model uses a maximum input token length of 512 and achieves a noteworthy macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0%, and a macro-averaged AUC of 0.986.* Results: The DRG-LLaMA model surpassed the performance of prior leading models in DRG prediction, showing a relative improvement of 40.3% and 35.7% in macro-averaged F1 score compared to ClinicalBERT and CAML, respectively. The model also achieved a top-1 prediction accuracy of 67.8% and 67.5% for base DRG and CC&#x2F;MCC prediction, respectively.<details>
<summary>Abstract</summary>
In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) is pivotal, but its assignment process is inefficient. The study introduces DRG-LLaMA, an advanced large language model (LLM) fine-tuned on clinical notes to enhance DRGs assignment. Utilizing LLaMA as the foundational model and optimizing it through Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries, our DRG-LLaMA-7B model exhibited a noteworthy macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0%, and a macro-averaged Area Under the Curve (AUC) of 0.986, with a maximum input token length of 512. This model surpassed the performance of prior leading models in DRG prediction, showing a relative improvement of 40.3% and 35.7% in macro-averaged F1 score compared to ClinicalBERT and CAML, respectively. Applied to base DRG and complication or comorbidity (CC)/major complication or comorbidity (MCC) prediction, DRG-LLaMA achieved a top-1 prediction accuracy of 67.8% and 67.5%, respectively. Additionally, our findings indicate that DRG-LLaMA's performance correlates with increased model parameters and input context lengths.
</details>
<details>
<summary>摘要</summary>
在美国医疗机构中，诊断相关组（DRG）是关键，但其分配过程不具有效率。这项研究介绍DRG-LLaMA，一种基于临床笔记的大型自然语言模型（LLM）进行DRG分配的提高。通过将LLaMA作为基础模型，并通过对236,192份MIMIC-IV病卷摘要进行低级适应（LoRA）优化，我们的DRG-LLaMA-7B模型在macro-averaged F1分数上表现出了很好的成绩，即0.327，Top-1预测精度为52.0%，和macro-averaged AUC为0.986，最大输入符号长度为512。这个模型在DRG预测中超越了先前的主导模型，显示了相对提升40.3%和35.7%的macro-averaged F1分数相比于ClinicalBERT和CAML，分别。应用于基本DRG和complication或comorbidity（CC）/主要complication或comorbidity（MCC）预测中，DRG-LLaMA达到了Top-1预测精度为67.8%和67.5%，分别。此外，我们的发现表明DRG-LLaMA的性能与模型参数和输入上下文长度相关。
</details></li>
</ul>
<hr>
<h2 id="From-Text-to-Trends-A-Unique-Garden-Analytics-Perspective-on-the-Future-of-Modern-Agriculture"><a href="#From-Text-to-Trends-A-Unique-Garden-Analytics-Perspective-on-the-Future-of-Modern-Agriculture" class="headerlink" title="From Text to Trends: A Unique Garden Analytics Perspective on the Future of Modern Agriculture"></a>From Text to Trends: A Unique Garden Analytics Perspective on the Future of Modern Agriculture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12579">http://arxiv.org/abs/2309.12579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parag Saxena</li>
<li>for: 这个研究报告旨在提高现代农业中的数据驱动智能，即通过机器学习框架来改善农业教育和沟通。</li>
<li>methods: 该研究使用了机器学习模型和自然语言处理（NLP）技术，将问题分类并预测未来的趋势。</li>
<li>results: 研究结果表明，机器学习可以预测园艺趋势，并且可以预测未来园艺问题的主题和趋势。这些结果表明，大规模农业产业可以通过积累和维护文本数据来预测趋势和实施战略农业规划。<details>
<summary>Abstract</summary>
Data-driven insights are essential for modern agriculture. This research paper introduces a machine learning framework designed to improve how we educate and reach out to people in the field of horticulture. The framework relies on data from the Horticulture Online Help Desk (HOHD), which is like a big collection of questions from people who love gardening and are part of the Extension Master Gardener Program (EMGP). This framework has two main parts. First, it uses special computer programs (machine learning models) to sort questions into categories. This helps us quickly send each question to the right expert, so we can answer it faster. Second, it looks at when questions are asked and uses that information to guess how many questions we might get in the future and what they will be about. This helps us plan on topics that will be really important. It's like knowing what questions will be popular in the coming months. We also take into account where the questions come from by looking at the Zip Code. This helps us make research that fits the challenges faced by gardeners in different places. In this paper, we demonstrate the potential of machine learning techniques to predict trends in horticulture by analyzing textual queries from homeowners. We show that NLP, classification, and time series analysis can be used to identify patterns in homeowners' queries and predict future trends in horticulture. Our results suggest that machine learning could be used to predict trends in other agricultural sectors as well. If large-scale agriculture industries curate and maintain a comparable repository of textual data, the potential for trend prediction and strategic agricultural planning could be revolutionized. This convergence of technology and agriculture offers a promising pathway for the future of sustainable farming and data-informed agricultural practices
</details>
<details>
<summary>摘要</summary>
“数据驱动的洞察力是现代农业的关键。这项研究发表在园艺领域的机器学习框架，旨在提高我们向园艺领域人员的教育和沟通方式。该框架包括两个主要部分。首先，它使用特殊的计算机程序（机器学习模型）将问题分类。这有助于我们快速将每个问题传递给相应的专家，以便更快地回答。其次，它考虑问题的提交时间，并使用这些信息预测将来的问题数量和主题。这有助于我们在未来准备相关的研究和规划。我们还考虑问题来源的ZipCode，以便制定地域化的研究。我们的研究表明，机器学习技术可以通过分析家庭主持人的文本查询来预测园艺领域的趋势。我们的结果表明，机器学习可以预测其他农业领域的趋势。如果大规模农业产业积累和维护类似的文本数据库，那么机器学习的潜力可以为未来的可持续农业和数据驱动农业实践带来革命性的改变。这种技术和农业的融合将为未来的可持续农业和数据驱动农业实践提供了一条优秀的道路。”
</details></li>
</ul>
<hr>
<h2 id="Understanding-Patterns-of-Deep-Learning-ModelEvolution-in-Network-Architecture-Search"><a href="#Understanding-Patterns-of-Deep-Learning-ModelEvolution-in-Network-Architecture-Search" class="headerlink" title="Understanding Patterns of Deep Learning ModelEvolution in Network Architecture Search"></a>Understanding Patterns of Deep Learning ModelEvolution in Network Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12576">http://arxiv.org/abs/2309.12576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Underwood, Meghana Madhastha, Randal Burns, Bogdan Nicolae</li>
<li>for: 这个论文主要探讨了深度学习模型的结构优化方法，具体来说是用于研究模型在时间推移中的Empirical Evolution Patterns，以便为缓存策略、优化搜索算法和其他应用场景设计。</li>
<li>methods: 这个论文使用了Regularized Evolution algorithm来研究模型的进化趋势，并对Candle项目和Nasbench-201搜索空间中的模型进行了算法性分析和量化Characterization。</li>
<li>results: 研究发现，Regularized Evolution algorithm会影响模型结构的进化趋势，而且在分布式环境中，模型的进化 Patterns可以被利用来优化缓存和调度策略。此外，研究还发现了一些影响模型结构的升降温度的 Condition。<details>
<summary>Abstract</summary>
Network Architecture Search and specifically Regularized Evolution is a common way to refine the structure of a deep learning model.However, little is known about how models empirically evolve over time which has design implications for designing caching policies, refining the search algorithm for particular applications, and other important use cases.In this work, we algorithmically analyze and quantitatively characterize the patterns of model evolution for a set of models from the Candle project and the Nasbench-201 search space.We show how the evolution of the model structure is influenced by the regularized evolution algorithm. We describe how evolutionary patterns appear in distributed settings and opportunities for caching and improved scheduling. Lastly, we describe the conditions that affect when particular model architectures rise and fall in popularity based on their frequency of acting as a donor in a sliding window.
</details>
<details>
<summary>摘要</summary>
网络建构搜索和特别是减少演化是深度学习模型结构的常见方法。然而，有很 little is known about如何模型实际在时间演化，这有关缓存策略、改进搜索算法、特定应用场景等重要用例的设计假设。在这项工作中，我们使用算法分析和量化描述了一组 Candle 项目和 Nasbench-201 搜索空间中模型的演化趋势。我们表明了减少演化算法如何影响模型结构的演化。我们还描述了分布式设置中的演化趋势以及缓存和调度的机会。最后，我们描述了模型结构的崛起和衰落的条件，基于它们在滚动窗口中的频率被用作donor。
</details></li>
</ul>
<hr>
<h2 id="Creativity-Support-in-the-Age-of-Large-Language-Models-An-Empirical-Study-Involving-Emerging-Writers"><a href="#Creativity-Support-in-the-Age-of-Large-Language-Models-An-Empirical-Study-Involving-Emerging-Writers" class="headerlink" title="Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers"></a>Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12570">http://arxiv.org/abs/2309.12570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuhin Chakrabarty, Vishakh Padmakumar, Faeze Brahman, Smaranda Muresan</li>
<li>for: 这个论文旨在探讨现代大型自然语言模型（LLM）在职业写作支持工具中的可用性。</li>
<li>methods: 这个研究采用了实证研究方法（n&#x3D;30），检查了现代LLM在助手写作过程中的可用性。</li>
<li>results: 研究发现，写作者在不同的认知活动中寻求LLM的帮助，尤其是在翻译和审阅阶段。<details>
<summary>Abstract</summary>
The development of large language models (LLMs) capable of following instructions and engaging in conversational interactions sparked increased interest in their utilization across various support tools. We investigate the utility of modern LLMs in assisting professional writers via an empirical user study (n=30). The design of our collaborative writing interface is grounded in the cognitive process model of writing that views writing as a goal-oriented thinking process encompassing non-linear cognitive activities: planning, translating, and reviewing. Participants are asked to submit a post-completion survey to provide feedback on the potential and pitfalls of LLMs as writing collaborators. Upon analyzing the writer-LLM interactions, we find that while writers seek LLM's help across all three types of cognitive activities, they find LLMs more helpful in translation and reviewing. Our findings from analyzing both the interactions and the survey responses highlight future research directions in creative writing assistance using LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的发展，能够跟随 instrux 和进行对话交互，对各种支持工具的应用产生了增加的兴趣。我们通过实验用户研究（n=30）进行了 LLM 在职业作家助手方面的可用性检查。我们的协作写作界面设计基于写作认知过程模型，视写作为一个目标导向的思考过程，包括观察、翻译和审核等非线性认知活动。参与者被要求提供完成后的调查，以提供 LLM 作为写作伙伴的潜在和障碍。我们分析了写作者与 LLM 之间的互动，发现写作者对 LLM 的帮助最多是在翻译和审核阶段。我们从分析互动和调查回应中获得了未来创作写作助手领域的研究方向。
</details></li>
</ul>
<hr>
<h2 id="A-Study-on-Learning-Social-Robot-Navigation-with-Multimodal-Perception"><a href="#A-Study-on-Learning-Social-Robot-Navigation-with-Multimodal-Perception" class="headerlink" title="A Study on Learning Social Robot Navigation with Multimodal Perception"></a>A Study on Learning Social Robot Navigation with Multimodal Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12568">http://arxiv.org/abs/2309.12568</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robotixx/multimodal-fusion-network">https://github.com/robotixx/multimodal-fusion-network</a></li>
<li>paper_authors: Bhabaranjan Panigrahi, Amir Hossain Raj, Mohammad Nazeri, Xuesu Xiao</li>
<li>for: 本研究旨在开发一种能够在人类 inhabited 的公共空间中自主移动的 robot，并能够根据围绕着它的人类行为和意图进行相应的导航决策。</li>
<li>methods: 本研究使用机器学习方法来捕捉人类社交互动的复杂和微妙性，从数据驱动的角度来capture这些互动。研究者使用多个可用的感知模式，包括 LiDAR 和 RGB 摄像头，并对不同的社交情境进行了比较。</li>
<li>results: 研究结果显示，使用多模态感知可以在社交导航决策中获得明显的优势，并且在人类研究中也被证明。研究者还分析了学习过程中的训练和普适性性能。开源代码可供社区未来研究多模态感知导航。<details>
<summary>Abstract</summary>
Autonomous mobile robots need to perceive the environments with their onboard sensors (e.g., LiDARs and RGB cameras) and then make appropriate navigation decisions. In order to navigate human-inhabited public spaces, such a navigation task becomes more than only obstacle avoidance, but also requires considering surrounding humans and their intentions to somewhat change the navigation behavior in response to the underlying social norms, i.e., being socially compliant. Machine learning methods are shown to be effective in capturing those complex and subtle social interactions in a data-driven manner, without explicitly hand-crafting simplified models or cost functions. Considering multiple available sensor modalities and the efficiency of learning methods, this paper presents a comprehensive study on learning social robot navigation with multimodal perception using a large-scale real-world dataset. The study investigates social robot navigation decision making on both the global and local planning levels and contrasts unimodal and multimodal learning against a set of classical navigation approaches in different social scenarios, while also analyzing the training and generalizability performance from the learning perspective. We also conduct a human study on how learning with multimodal perception affects the perceived social compliance. The results show that multimodal learning has a clear advantage over unimodal learning in both dataset and human studies. We open-source our code for the community's future use to study multimodal perception for learning social robot navigation.
</details>
<details>
<summary>摘要</summary>
自适应移动 робоッツ需要通过其 бордов的感知器 (例如 LiDAR 和 RGB 摄像头) 识别环境，然后采取相应的导航决策。在人类居住的公共空间中导航，这种导航任务不仅仅是避免障碍物，还需要考虑周围的人和他们的意图，并根据下面社会规范进行相应的导航行为变化。机器学习方法可以有效地捕捉这些复杂和柔和的社会互动，无需显式地手工设计简化模型或成本函数。 Considering multiple available sensor modalities and the efficiency of learning methods, this paper presents a comprehensive study on learning social robot navigation with multimodal perception using a large-scale real-world dataset. The study investigates social robot navigation decision making on both the global and local planning levels and contrasts unimodal and multimodal learning against a set of classical navigation approaches in different social scenarios, while also analyzing the training and generalizability performance from the learning perspective. We also conduct a human study on how learning with multimodal perception affects the perceived social compliance. The results show that multimodal learning has a clear advantage over unimodal learning in both dataset and human studies. We open-source our code for the community's future use to study multimodal perception for learning social robot navigation.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Meets-Advanced-Robotic-Manipulation"><a href="#Machine-Learning-Meets-Advanced-Robotic-Manipulation" class="headerlink" title="Machine Learning Meets Advanced Robotic Manipulation"></a>Machine Learning Meets Advanced Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12560">http://arxiv.org/abs/2309.12560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeid Nahavandi, Roohallah Alizadehsani, Darius Nahavandi, Chee Peng Lim, Kevin Kelly, Fernando Bello</li>
<li>for: 提高自动化生产质量、降低生产成本和更好地利用人员资源</li>
<li>methods: 机器学习方法</li>
<li>results: 提高安全性、可靠性和效率Here is a more detailed explanation of each point:</li>
<li>for: The paper is written to discuss the application of machine learning methods in automation and robotics, specifically in the context of manipulation tasks. The goal is to improve the quality, efficiency, and safety of automation systems.</li>
<li>methods: The paper reviews cutting-edge technologies and recent trends in machine learning methods applied to real-world manipulation tasks. It covers a wide range of applications in different domains, including industry, healthcare, agriculture, space, military, and search and rescue.</li>
<li>results: The paper highlights the potential of machine learning methods to improve the safety, reliability, and efficiency of automation systems. It provides an overview of the current state of the field and identifies important research directions for future works.<details>
<summary>Abstract</summary>
Automated industries lead to high quality production, lower manufacturing cost and better utilization of human resources. Robotic manipulator arms have major role in the automation process. However, for complex manipulation tasks, hard coding efficient and safe trajectories is challenging and time consuming. Machine learning methods have the potential to learn such controllers based on expert demonstrations. Despite promising advances, better approaches must be developed to improve safety, reliability, and efficiency of ML methods in both training and deployment phases. This survey aims to review cutting edge technologies and recent trends on ML methods applied to real-world manipulation tasks. After reviewing the related background on ML, the rest of the paper is devoted to ML applications in different domains such as industry, healthcare, agriculture, space, military, and search and rescue. The paper is closed with important research directions for future works.
</details>
<details>
<summary>摘要</summary>
自动化业务会导致高质量生产、低成本生产和更好的人员资源利用。 robotic manipulator arms 在自动化过程中扮演着重要的角色。然而，对于复杂的抓拍任务，使用硬编程方法设计有效和安全的轨迹是挑战和时间consuming。机器学习方法有 potential 可以学习出专家示范的控制器。 despite promising advances, 以下是未来研究的重要方向：1. 提高安全性、可靠性和效率的机器学习方法，包括在训练和部署阶段。2. 应用机器学习方法到不同领域，如工业、医疗、农业、航天、军事和搜救等。3. 开发出可靠的机器学习模型，以满足实际应用需求。本文首先介绍了机器学习的相关背景，然后分别介绍了机器学习在不同领域的应用，包括工业、医疗、农业、航天、军事和搜救等。 finally, 本文结束于未来研究的重要方向。
</details></li>
</ul>
<hr>
<h2 id="Invariant-Learning-via-Probability-of-Sufficient-and-Necessary-Causes"><a href="#Invariant-Learning-via-Probability-of-Sufficient-and-Necessary-Causes" class="headerlink" title="Invariant Learning via Probability of Sufficient and Necessary Causes"></a>Invariant Learning via Probability of Sufficient and Necessary Causes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12559">http://arxiv.org/abs/2309.12559</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ymy4323460/casn">https://github.com/ymy4323460/casn</a></li>
<li>paper_authors: Mengyue Yang, Zhen Fang, Yonggang Zhang, Yali Du, Furui Liu, Jean-Francois Ton, Jun Wang</li>
<li>for: 提高模型在未知训练分布下的泛化能力（OOD generalization）</li>
<li>methods: 利用 causality 的方法，具体是计算可能性极值（PNS），来捕捉可能性的必要和充分条件，并利用 PNS 风险来学习表示</li>
<li>results: 对synthetic和实际数据进行了实验，证明提出的方法有效，并进行了理论分析和证明，证明方法的泛化性。更多细节可以查看 GitHub 仓库：<a target="_blank" rel="noopener" href="https://github.com/ymy4323460/CaSN%E3%80%82">https://github.com/ymy4323460/CaSN。</a><details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose PNS risk and formulate an algorithm to learn representation with a high PNS value. We theoretically analyze and prove the generalizability of the PNS risk. Experiments on both synthetic and real-world benchmarks demonstrate the effectiveness of the proposed method. The details of the implementation can be found at the GitHub repository: https://github.com/ymy4323460/CaSN.
</details>
<details>
<summary>摘要</summary>
OUT-OF-DISTRIBUTION（OOD）通用性是学习模型的必要条件，因为测试分布通常不同于训练分布。 recent methods based on causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Specifically, a necessary but insufficient cause (feature) is invariant to distribution shift, but it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose PNS risk and formulate an algorithm to learn representation with a high PNS value. We theoretically analyze and prove the generalizability of the PNS risk. Experiments on both synthetic and real-world benchmarks demonstrate the effectiveness of the proposed method. For more details, please refer to the GitHub repository: <https://github.com/ymy4323460/CaSN>.
</details></li>
</ul>
<hr>
<h2 id="PlanFitting-Tailoring-Personalized-Exercise-Plans-with-Large-Language-Models"><a href="#PlanFitting-Tailoring-Personalized-Exercise-Plans-with-Large-Language-Models" class="headerlink" title="PlanFitting: Tailoring Personalized Exercise Plans with Large Language Models"></a>PlanFitting: Tailoring Personalized Exercise Plans with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12555">http://arxiv.org/abs/2309.12555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donghoon Shin, Gary Hsieh, Young-Ho Kim</li>
<li>for: 本研究旨在帮助用户创建个性化的锻炼计划，以满足用户的具体需求和基本原则。</li>
<li>methods: 本研究使用了大语言模型的生成能力，让用户通过自然语言描述约束和查询，以生成和优化用户每周的锻炼计划。</li>
<li>results: 经过用户研究（N&#x3D;18）和专家评估（N&#x3D;3），研究发现PlanFitting可以生成个性化、可行、基于证据的锻炼计划。未来，研究人员可以通过AI助手创建计划，更好地遵循锻炼原则，并更好地适应用户的个性约束。<details>
<summary>Abstract</summary>
A personally tailored exercise regimen is crucial to ensuring sufficient physical activities, yet challenging to create as people have complex schedules and considerations and the creation of plans often requires iterations with experts. We present PlanFitting, a conversational AI that assists in personalized exercise planning. Leveraging generative capabilities of large language models, PlanFitting enables users to describe various constraints and queries in natural language, thereby facilitating the creation and refinement of their weekly exercise plan to suit their specific circumstances while staying grounded in foundational principles. Through a user study where participants (N=18) generated a personalized exercise plan using PlanFitting and expert planners (N=3) evaluated these plans, we identified the potential of PlanFitting in generating personalized, actionable, and evidence-based exercise plans. We discuss future design opportunities for AI assistants in creating plans that better comply with exercise principles and accommodate personal constraints.
</details>
<details>
<summary>摘要</summary>
一个专门设计的运动计划是不可或缺的，以确保人们有足够的身体活动，但创建计划可以是具有复杂的时间表和考虑的挑战。我们介绍PlanFitting，一个以语言模型为基础的对话式人工智能，可以帮助用户创建个性化的运动计划。通过让用户使用自然语言描述各种限制和查询，PlanFitting可以帮助用户创建和调整每周的运动计划，以满足他们的具体情况，同时尊重基本的运动原则。经过一次用户研究（N=18）和专家规划师（N=3）评估这些计划，我们发现PlanFitting具有创建个性化、可行、基于证据的运动计划的潜力。我们讨论未来的设计机会，以更好地让人工智能助手遵循运动原则，并考虑个人的限制。
</details></li>
</ul>
<hr>
<h2 id="Provably-Robust-and-Plausible-Counterfactual-Explanations-for-Neural-Networks-via-Robust-Optimisation"><a href="#Provably-Robust-and-Plausible-Counterfactual-Explanations-for-Neural-Networks-via-Robust-Optimisation" class="headerlink" title="Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation"></a>Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12545">http://arxiv.org/abs/2309.12545</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junqi-jiang/proplace">https://github.com/junqi-jiang/proplace</a></li>
<li>paper_authors: Junqi Jiang, Jianglin Lan, Francesco Leofante, Antonio Rago, Francesca Toni</li>
<li>for: 这篇论文的目的是解释神经网络分类器的counterfactual explanations（CEs）。</li>
<li>methods: 这篇论文提出了一种名为PROPLACE的方法，利用Robust optimization技术来解释神经网络分类器的CEs。</li>
<li>results: 对比六种基eline，PROPLACE在三个评价方面的表现最佳，其中五种基eline都是targeting robustness。<details>
<summary>Abstract</summary>
Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for proximity and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limitations in the literature. We formulate an iterative algorithm to compute provably robust CEs and prove its convergence, soundness and completeness. Through a comparative experiment involving six baselines, five of which target robustness, we show that PROPLACE achieves state-of-the-art performances against metrics on three evaluation aspects.
</details>
<details>
<summary>摘要</summary>
counterfactual explanations (CEs) 已经收到了增加的关注，作为神经网络分类器的主要方法ологи。通常，对输入输出对的 CE 是指最近输入的数据点，被分类为不同的标签。为解决已经存在的问题， CE 在模型参数更新（例如 retrained）后会被无效化，研究者们已经提出了保证 CE 在模型参数变化下的稳定性的方法。然而，现有的方法不具备完善性和准确性，可能生成不合理的 CE，即训练数据集中的异常值。事实上，现有的方法没有同时优化 proximity 和 plausibility，保持robustness guarantees。在这种情况下，我们提出了可证实 Robust and PLAusible Counterfactual Explanations (PROPLACE)，一种基于robust optimization技术来解决现有文献中的限制。我们设计了一种迭代算法来计算可证实的 CE，并证明其 converges，完整性和准确性。通过对六个基elines进行比较实验，我们显示了 PROPLACE 在三个评价方面的状态前 performances。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/22/cs.AI_2023_09_22/" data-id="clpztdnbq004fes88d6bf4d8f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/22/cs.CL_2023_09_22/" class="article-date">
  <time datetime="2023-09-22T11:00:00.000Z" itemprop="datePublished">2023-09-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/22/cs.CL_2023_09_22/">cs.CL - 2023-09-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Document-Understanding-for-Healthcare-Referrals"><a href="#Document-Understanding-for-Healthcare-Referrals" class="headerlink" title="Document Understanding for Healthcare Referrals"></a>Document Understanding for Healthcare Referrals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13184">http://arxiv.org/abs/2309.13184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Jimit Mistry, Natalia M. Arzeno</li>
<li>for: 提高医疗referral管理效率，减少管理成本和错误</li>
<li>methods: 提出了一种гибрид模型，结合LayoutLMv3和域pecific规则，用于在传输referral文档中自动识别关键病人、医生和检查相关信息</li>
<li>results: 结果表明，通过添加域pecific规则，使用变换器模型的精度和F1分数得到了大幅提高，表明混合模型在实际应用中可以提高referral管理效率。<details>
<summary>Abstract</summary>
Reliance on scanned documents and fax communication for healthcare referrals leads to high administrative costs and errors that may affect patient care. In this work we propose a hybrid model leveraging LayoutLMv3 along with domain-specific rules to identify key patient, physician, and exam-related entities in faxed referral documents. We explore some of the challenges in applying a document understanding model to referrals, which have formats varying by medical practice, and evaluate model performance using MUC-5 metrics to obtain appropriate metrics for the practical use case. Our analysis shows the addition of domain-specific rules to the transformer model yields greatly increased precision and F1 scores, suggesting a hybrid model trained on a curated dataset can increase efficiency in referral management.
</details>
<details>
<summary>摘要</summary>
靠扫描文档和传真communication для医疗referral导致高行政成本和错误，这些错误可能影响病人护理。在这个工作中，我们提出了一种hybrid模型，利用LayoutLMv3 alongside domain-specific规则来标识患者、医生和检查相关实体在传真referral文档中。我们探讨了应用文档理解模型到referral的挑战，因为referral的格式可能因医疗实践而异常，并评估模型性能使用MUC-5指标，以获得实用的指标。我们的分析显示，将域专门规则添加到变换器模型可以提高准确率和F1分数，表明一种基于 cura dataset的hybrid模型可以提高referral管理的效率。
</details></li>
</ul>
<hr>
<h2 id="Effective-Distillation-of-Table-based-Reasoning-Ability-from-LLMs"><a href="#Effective-Distillation-of-Table-based-Reasoning-Ability-from-LLMs" class="headerlink" title="Effective Distillation of Table-based Reasoning Ability from LLMs"></a>Effective Distillation of Table-based Reasoning Ability from LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13182">http://arxiv.org/abs/2309.13182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao, Chenghua Lin<br>for:This paper aims to specialize table reasoning skills in smaller models for table-to-text generation tasks.methods:The proposed method uses distillation to transfer specific capabilities of large language models (LLMs) to smaller models, specifically tailored for table-based reasoning.results:The fine-tuned model (Flan-T5-base) achieved significant improvement compared to traditional baselines and outperformed specific LLMs like gpt-3.5-turbo on the scientific table-to-text generation dataset (SciGen).<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their remarkable parameter size and their impressive high requirement of computing resources pose challenges for their practical deployment. Recent research has revealed that specific capabilities of LLMs, such as numerical reasoning, can be transferred to smaller models through distillation. Some studies explore the potential of leveraging LLMs to perform table-based reasoning. Nevertheless, prior to our work, there has been no investigation into the prospect of specialising table reasoning skills in smaller models specifically tailored for table-to-text generation tasks. In this paper, we propose a novel table-based reasoning distillation, with the aim of distilling distilling LLMs into tailored, smaller models specifically designed for table-based reasoning task. Experimental results have shown that a 0.22 billion parameter model (Flan-T5-base) fine-tuned using distilled data, not only achieves a significant improvement compared to traditionally fine-tuned baselines but also surpasses specific LLMs like gpt-3.5-turbo on the scientific table-to-text generation dataset (SciGen). The code and data are released in https://github.com/Bernard-Yang/TableDistill.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BenLLMEval-A-Comprehensive-Evaluation-into-the-Potentials-and-Pitfalls-of-Large-Language-Models-on-Bengali-NLP"><a href="#BenLLMEval-A-Comprehensive-Evaluation-into-the-Potentials-and-Pitfalls-of-Large-Language-Models-on-Bengali-NLP" class="headerlink" title="BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP"></a>BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13173">http://arxiv.org/abs/2309.13173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsinul Kabir, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, M Saiful Bari, Enamul Hoque</li>
<li>for: 本研究评估了大型自然语言处理（NLP）模型（LLMs）在低资源语言如孟加拉语（Bangla）中的表现。</li>
<li>methods: 本研究使用了多种重要和多样化的孟加拉语NLP任务，如抽象摘要、问答、重叠、自然语言推理、文本分类和情感分析，对ChatGPT、LLaMA-2和Claude-2等LLMs进行零搅evaluation，并比较其表现与现有的精度调整模型。</li>
<li>results: 实验结果显示了不同孟加拉语NLP任务中LLMs的表现较差，这表明需要进一步的研究以提高LLMs在低资源语言如孟加拉语的理解。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have emerged as one of the most important breakthroughs in natural language processing (NLP) for their impressive skills in language generation and other language-specific tasks. Though LLMs have been evaluated in various tasks, mostly in English, they have not yet undergone thorough evaluation in under-resourced languages such as Bengali (Bangla). In this paper, we evaluate the performance of LLMs for the low-resourced Bangla language. We select various important and diverse Bangla NLP tasks, such as abstractive summarization, question answering, paraphrasing, natural language inference, text classification, and sentiment analysis for zero-shot evaluation with ChatGPT, LLaMA-2, and Claude-2 and compare the performance with state-of-the-art fine-tuned models. Our experimental results demonstrate an inferior performance of LLMs for different Bangla NLP tasks, calling for further effort to develop better understanding of LLMs in low-resource languages like Bangla.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经被认为是自然语言处理（NLP）领域的一个重要突破，它们在语言生成和其他语言特定任务中表现出了卓越的能力。虽然 LLM 已经在英语等语言上进行了评估，但它们尚未在低资源语言 such as 孟加拉语（Bangla）进行了系统性的评估。在这篇论文中，我们对低资源 Bangla 语言进行了 LLM 的评估。我们选择了一些重要和多样的 Bangla NLP 任务，如抽象摘要、问答、重叠、自然语言推理、文本分类和情感分析，并对 ChatGPT、LLaMA-2 和 Claude-2 进行零 shot 评估，并与当前的精度模型进行比较。我们的实验结果表明 LLMs 在不同的 Bangla NLP 任务中表现出了较差的性能，这表明需要进一步的研究，以更好地理解 LLMs 在低资源语言如 Bangla 的性能。
</details></li>
</ul>
<hr>
<h2 id="Cardiovascular-Disease-Risk-Prediction-via-Social-Media"><a href="#Cardiovascular-Disease-Risk-Prediction-via-Social-Media" class="headerlink" title="Cardiovascular Disease Risk Prediction via Social Media"></a>Cardiovascular Disease Risk Prediction via Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13147">http://arxiv.org/abs/2309.13147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Al Zadid Sultan Bin Habib, Md Asif Bin Syed, Md Tanvirul Islam, Donald A. Adjeroh</li>
<li>for: 预测心血管疾病（CVD）风险</li>
<li>methods: 使用推特和情感分析预测CVD风险，开发了新的CVD相关关键词词典，并使用VADER模型进行情感分析，将用户分类为可能存在CVD风险</li>
<li>results: 结果表明通过分析推特中的情感，可以超过基于人口数据alone的预测力，并能够识别可能发展CVD的个体，这些结果表明了自然语言处理和机器学习技术在使用推特来识别CVD风险的潜力。<details>
<summary>Abstract</summary>
Researchers use Twitter and sentiment analysis to predict Cardiovascular Disease (CVD) risk. We developed a new dictionary of CVD-related keywords by analyzing emotions expressed in tweets. Tweets from eighteen US states, including the Appalachian region, were collected. Using the VADER model for sentiment analysis, users were classified as potentially at CVD risk. Machine Learning (ML) models were employed to classify individuals' CVD risk and applied to a CDC dataset with demographic information to make the comparison. Performance evaluation metrics such as Test Accuracy, Precision, Recall, F1 score, Mathew's Correlation Coefficient (MCC), and Cohen's Kappa (CK) score were considered. Results demonstrated that analyzing tweets' emotions surpassed the predictive power of demographic data alone, enabling the identification of individuals at potential risk of developing CVD. This research highlights the potential of Natural Language Processing (NLP) and ML techniques in using tweets to identify individuals with CVD risks, providing an alternative approach to traditional demographic information for public health monitoring.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dynamic-ASR-Pathways-An-Adaptive-Masking-Approach-Towards-Efficient-Pruning-of-A-Multilingual-ASR-Model"><a href="#Dynamic-ASR-Pathways-An-Adaptive-Masking-Approach-Towards-Efficient-Pruning-of-A-Multilingual-ASR-Model" class="headerlink" title="Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model"></a>Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13018">http://arxiv.org/abs/2309.13018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiamin Xie, Ke Li, Jinxi Guo, Andros Tjandra, Yuan Shangguan, Leda Sari, Chunyang Wu, Junteng Jia, Jay Mahadeokar, Ozlem Kalinli</li>
<li>for: 这个研究的目的是实现对多语言自动话语识别（ASR）模型的压缩，并且将其转换为单语言模型或多语言模型。</li>
<li>methods: 这个研究使用了适应性遮盾方法，包括两个情况：一是生成简单的单语言模型，二是将多语言模型转换为简单的多语言模型。这个方法可以避免固定的子网络结构，并且在不同的初始化情况下进行适应。</li>
<li>results: 这个研究发现，使用适应性遮盾方法可以在对多语言模型进行压缩时，比较有效率，并且可以实现更好的表现。此外，这个方法可以将多语言模型转换为简单的多语言模型，并且可以实现更好的表现。<details>
<summary>Abstract</summary>
Neural network pruning offers an effective method for compressing a multilingual automatic speech recognition (ASR) model with minimal performance loss. However, it entails several rounds of pruning and re-training needed to be run for each language. In this work, we propose the use of an adaptive masking approach in two scenarios for pruning a multilingual ASR model efficiently, each resulting in sparse monolingual models or a sparse multilingual model (named as Dynamic ASR Pathways). Our approach dynamically adapts the sub-network, avoiding premature decisions about a fixed sub-network structure. We show that our approach outperforms existing pruning methods when targeting sparse monolingual models. Further, we illustrate that Dynamic ASR Pathways jointly discovers and trains better sub-networks (pathways) of a single multilingual model by adapting from different sub-network initializations, thereby reducing the need for language-specific pruning.
</details>
<details>
<summary>摘要</summary>
中文简体版：神经网络剪枝提供了一种有效的压缩方法，以最小化多语言自动语音识别（ASR）模型的性能损失。然而，它需要每种语言进行多轮剪枝和重新训练。在这个工作中，我们提议使用适应maskingapproach来有效地剪枝多语言ASR模型，分别得到简洁的单语言模型或简洁的多语言模型（名为动态ASR PATHways）。我们的方法可以动态适应子网络，避免提前决定固定子网络结构。我们显示，我们的方法在targeting简洁的单语言模型时比既有的剪枝方法高效。此外，我们还示出了Dynamic ASR PATHways可以将多语言模型中的不同子网络初始化相互转换，从而降低语言特定的剪枝需求。
</details></li>
</ul>
<hr>
<h2 id="Nested-Event-Extraction-upon-Pivot-Element-Recogniton"><a href="#Nested-Event-Extraction-upon-Pivot-Element-Recogniton" class="headerlink" title="Nested Event Extraction upon Pivot Element Recogniton"></a>Nested Event Extraction upon Pivot Element Recogniton</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12960">http://arxiv.org/abs/2309.12960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weicheng Ren, Zixuan Li, Xiaolong Jin, Long Bai, Miao Su, Yantao Liu, Saiping Guan, Jiafeng Guo, Xueqi Cheng</li>
<li>for: 提高复杂事件结构抽取精度，解决现有方法不能很好地处理嵌入式事件结构中的 pivot element 问题。</li>
<li>methods: 基于识别触发器对 triggers 和 arguments 的类型和关系进行分类，并通过提示学习获得更好的触发器和 argue 的表示，以提高 NEE 性能。</li>
<li>results: PerNee 在 ACE2005-Nest、Genia11 和 Genia13 上实现了状态之冠性表现，提高了 NEE 精度。<details>
<summary>Abstract</summary>
Nested Event Extraction (NEE) aims to extract complex event structures where an event contains other events as its arguments recursively. Nested events involve a kind of Pivot Elements (PEs) that simultaneously act as arguments of outer events and as triggers of inner events, and thus connect them into nested structures. This special characteristic of PEs brings challenges to existing NEE methods, as they cannot well cope with the dual identities of PEs. Therefore, this paper proposes a new model, called PerNee, which extracts nested events mainly based on recognizing PEs. Specifically, PerNee first recognizes the triggers of both inner and outer events and further recognizes the PEs via classifying the relation type between trigger pairs. In order to obtain better representations of triggers and arguments to further improve NEE performance, it incorporates the information of both event types and argument roles into PerNee through prompt learning. Since existing NEE datasets (e.g., Genia11) are limited to specific domains and contain a narrow range of event types with nested structures, we systematically categorize nested events in generic domain and construct a new NEE dataset, namely ACE2005-Nest. Experimental results demonstrate that PerNee consistently achieves state-of-the-art performance on ACE2005-Nest, Genia11 and Genia13.
</details>
<details>
<summary>摘要</summary>
嵌入式事件提取（NEE）目标是提取嵌入式事件结构，其中事件包含其他事件作为自身参数的嵌入式结构。嵌入事件中的 pivot 元素（PE）同时作为外部事件的参数和内部事件的触发器，因此将其连接到嵌入结构中。这种特殊的 PE 特点带来了现有 NEE 方法的挑战，因为它们无法好地处理 PE 的双重身份。因此，本文提出了一种新模型，即 PerNee，它基于认可 PE 来提取嵌入事件。具体来说，PerNee 先认可外部和内部事件的触发器，然后通过类型化 trigger 对的关系来认定 PE。为了从trigger和参数角度获得更好的表示，PerNee 通过推训来 incorporate 事件类型和参数角色信息。由于现有 NEE 数据集（如 Genia11）限制在特定领域，并且只包含一些嵌入式事件结构，我们系统地分类嵌入事件在通用领域，并构建了一个新的 NEE 数据集，即 ACE2005-Nest。实验结果表明，PerNee 在 ACE2005-Nest、Genia11 和 Genia13 上具有状态的表现。
</details></li>
</ul>
<hr>
<h2 id="TopRoBERTa-Topology-Aware-Authorship-Attribution-of-Deepfake-Texts"><a href="#TopRoBERTa-Topology-Aware-Authorship-Attribution-of-Deepfake-Texts" class="headerlink" title="TopRoBERTa: Topology-Aware Authorship Attribution of Deepfake Texts"></a>TopRoBERTa: Topology-Aware Authorship Attribution of Deepfake Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12934">http://arxiv.org/abs/2309.12934</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adaku Uchendu, Thai Le, Dongwon Lee</li>
<li>for: 本研究旨在开发一种可以判断文本是否为深度伪造文本（deepfake text）的计算方法，以mitigate大量深度伪造文本的散布。</li>
<li>methods: 本研究使用了Topological Data Analysis（TDA）层和RoBERTa模型，以capture更多的语言特征和结构特征，提高作者识别率。</li>
<li>results: 对于3个 datasets，TopRoBERTa模型比vanilla RoBERTa模型提高了2&#x2F;3的Macro F1分数，最高提高7%。<details>
<summary>Abstract</summary>
Recent advances in Large Language Models (LLMs) have enabled the generation of open-ended high-quality texts, that are non-trivial to distinguish from human-written texts. We refer to such LLM-generated texts as \emph{deepfake texts}. There are currently over 11K text generation models in the huggingface model repo. As such, users with malicious intent can easily use these open-sourced LLMs to generate harmful texts and misinformation at scale. To mitigate this problem, a computational method to determine if a given text is a deepfake text or not is desired--i.e., Turing Test (TT). In particular, in this work, we investigate the more general version of the problem, known as \emph{Authorship Attribution (AA)}, in a multi-class setting--i.e., not only determining if a given text is a deepfake text or not but also being able to pinpoint which LLM is the author. We propose \textbf{TopRoBERTa} to improve existing AA solutions by capturing more linguistic patterns in deepfake texts by including a Topological Data Analysis (TDA) layer in the RoBERTa model. We show the benefits of having a TDA layer when dealing with noisy, imbalanced, and heterogeneous datasets, by extracting TDA features from the reshaped $pooled\_output$ of RoBERTa as input. We use RoBERTa to capture contextual representations (i.e., semantic and syntactic linguistic features), while using TDA to capture the shape and structure of data (i.e., linguistic structures). Finally, \textbf{TopRoBERTa}, outperforms the vanilla RoBERTa in 2/3 datasets, achieving up to 7\% increase in Macro F1 score.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）技术的进步，使得可以生成高质量、不易于 distinguishing 的文本，我们称之为“深伪文本”。目前已经有超过 11K 的文本生成模型在 huggingface 模型库中。因此，有恶意用户可以使用这些开源的 LLM 生成大量的危险文本和谣言。为了解决这问题，一种计算方法是需要的——namely，Turing Test（TT）。在这种情况下，我们研究了一个更一般的问题——作者归属问题（AA），在多类别Setting下进行研究——即不仅是判断给定文本是否是深伪文本，还可以确定这个文本的作者是哪个 LLM。我们提出了 TopRoBERTa，用于改进现有 AA 解决方案，通过包含 Topological Data Analysis（TDA）层在 RoBERTa 模型中，从而更好地捕捉深伪文本中的语言特征。我们通过对不规则、不均衡和不一致的数据进行处理，提取 TDA 特征从 RoBERTa 模型中的 pooling 输出中。我们使用 RoBERTa 模型来捕捉语义和语法特征，而使用 TDA 来捕捉数据的形态和结构特征。最后，TopRoBERTa 在 2/3 个数据集上表现出色，与原始 RoBERTa 相比，提高了 macro F1 得分的最高7%。
</details></li>
</ul>
<hr>
<h2 id="PopBERT-Detecting-populism-and-its-host-ideologies-in-the-German-Bundestag"><a href="#PopBERT-Detecting-populism-and-its-host-ideologies-in-the-German-Bundestag" class="headerlink" title="PopBERT. Detecting populism and its host ideologies in the German Bundestag"></a>PopBERT. Detecting populism and its host ideologies in the German Bundestag</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14355">http://arxiv.org/abs/2309.14355</a></li>
<li>repo_url: None</li>
<li>paper_authors: L. Erhard, S. Hanke, U. Remer, A. Falenska, R. Heiberger</li>
<li>for: 本研究旨在提供一种可靠、有效、可扩展的方法来评估民粹主义的语言表达。</li>
<li>methods: 我们创建了基于德国bundestag（2013-2021年）的 parliamentary speeches 的标注 dataset，并采用 transformer-based 模型（PopBERT）作为多类分类器来检测和评估民粹主义语言的多个维度。</li>
<li>results: 验证检查表明，PopBERT 具有强的预测准确率、高质量的面效VALIDITY、与专家调查中党派排名相符、并能正确地检测新的文本片断。PopBERT 可以为德语政治家和党派的语言使用提供动态分析，以及可以在跨领域应用或开发相关的分类器。<details>
<summary>Abstract</summary>
The rise of populism concerns many political scientists and practitioners, yet the detection of its underlying language remains fragmentary. This paper aims to provide a reliable, valid, and scalable approach to measure populist stances. For that purpose, we created an annotated dataset based on parliamentary speeches of the German Bundestag (2013 to 2021). Following the ideational definition of populism, we label moralizing references to the virtuous people or the corrupt elite as core dimensions of populist language. To identify, in addition, how the thin ideology of populism is thickened, we annotate how populist statements are attached to left-wing or right-wing host ideologies. We then train a transformer-based model (PopBERT) as a multilabel classifier to detect and quantify each dimension. A battery of validation checks reveals that the model has a strong predictive accuracy, provides high qualitative face validity, matches party rankings of expert surveys, and detects out-of-sample text snippets correctly. PopBERT enables dynamic analyses of how German-speaking politicians and parties use populist language as a strategic device. Furthermore, the annotator-level data may also be applied in cross-domain applications or to develop related classifiers.
</details>
<details>
<summary>摘要</summary>
populism 的崛起引起了许多政治科学家和实践者的关注，但检测其下面的语言 ainda是 fragmentary。这篇论文目的是提供一种可靠、有效、可扩展的方法来评估 populist 的立场。为此，我们创建了基于德国bundestag parliamentary speeches（2013-2021）的注释数据集。根据意识形态的定义，我们将 moralizing 引用为贤良人或腐败的エリー特定为 populist 语言的核心维度。此外，为了了解 populist 语言如何被膨胀，我们还注释了 populist 声明与左翼或右翼的主义相关的hosts。然后，我们使用 transformer 基本模型（PopBERT）作为多类归一类ifier来检测和评估每一维度。一系列的验证检查表明，模型具有强大预测精度，提供高质量的面 validate，匹配党派评估专家调查的排名，并正确地检测出 sample 文本片段。PopBERT 允许我们动态地分析德语政治人物和党派如何使用 populist 语言作为策略工具。此外，注释数据还可以在跨领域应用或开发相关的分类器。
</details></li>
</ul>
<hr>
<h2 id="Affect-Recognition-in-Conversations-Using-Large-Language-Models"><a href="#Affect-Recognition-in-Conversations-Using-Large-Language-Models" class="headerlink" title="Affect Recognition in Conversations Using Large Language Models"></a>Affect Recognition in Conversations Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12881">http://arxiv.org/abs/2309.12881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shutong Feng, Guangzhi Sun, Nurul Lubis, Chao Zhang, Milica Gašić</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）在对话中识别人类情感的能力，包括开放领域对话和任务导向对话。</li>
<li>methods: 研究使用了三个不同的数据集：IEMOCAP、EmoWOZ和DAIC-WOZ，这些数据集涵盖了从伙伴对话到医疗采访的对话。研究使用了零shot和几shot学习，以及任务特定的精度调整，来评估和比较LLMs的表现。</li>
<li>results: 研究发现LLMs在情感识别方面具有一定的能力，但是其表现受到自动语音识别（ASR）错误的影响。通过这项研究，我们希望探讨LLMs在对话中是否可以模拟人类的情感识别能力。<details>
<summary>Abstract</summary>
Affect recognition, encompassing emotions, moods, and feelings, plays a pivotal role in human communication. In the realm of conversational artificial intelligence (AI), the ability to discern and respond to human affective cues is a critical factor for creating engaging and empathetic interactions. This study delves into the capacity of large language models (LLMs) to recognise human affect in conversations, with a focus on both open-domain chit-chat dialogues and task-oriented dialogues. Leveraging three diverse datasets, namely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues from casual conversations to clinical interviews, we evaluated and compared LLMs' performance in affect recognition. Our investigation explores the zero-shot and few-shot capabilities of LLMs through in-context learning (ICL) as well as their model capacities through task-specific fine-tuning. Additionally, this study takes into account the potential impact of automatic speech recognition (ASR) errors on LLM predictions. With this work, we aim to shed light on the extent to which LLMs can replicate human-like affect recognition capabilities in conversations.
</details>
<details>
<summary>摘要</summary>
人类communication中，情感认知（affect recognition）发挥关键作用。在人工智能对话中，能够识别和回应人类情感cue的能力是创造有趣和同情的交互的关键因素。本研究探讨了大型自然语言模型（LLMs）在对话中识别人类情感的能力，包括开放领域对话和任务导向对话。通过使用三个多样化的数据集，namely IEMOCAP、EmoWOZ和DAIC-WOZ，覆盖了对话的广泛spectrum，从互斥对话到临床采访，我们评估和比较了LLMs的表现。我们的调查探讨了LLMs在零shot和几shot情况下的能力，以及通过任务特定的精度调整来提高模型 capacities。此外，本研究还考虑了自动语音识别（ASR）错误对LLM预测的影响。通过这项工作，我们希望探讨LLMs在对话中是否能够模拟人类情感认知能力。
</details></li>
</ul>
<hr>
<h2 id="StyloMetrix-An-Open-Source-Multilingual-Tool-for-Representing-Stylometric-Vectors"><a href="#StyloMetrix-An-Open-Source-Multilingual-Tool-for-Representing-Stylometric-Vectors" class="headerlink" title="StyloMetrix: An Open-Source Multilingual Tool for Representing Stylometric Vectors"></a>StyloMetrix: An Open-Source Multilingual Tool for Representing Stylometric Vectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12810">http://arxiv.org/abs/2309.12810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inez Okulska, Daria Stetsenko, Anna Kołos, Agnieszka Karlińska, Kinga Głąbińska, Adam Nowakowski</li>
<li>for: 这个论文的目的是为开源多语言工具StyloMetrix提供一个概述。这个工具提供了不同语言的语法、 синтакси和词汇方面的语料，覆盖了波兰语、英语、乌克兰语和俄语四种语言。</li>
<li>methods: 这个论文使用了StyloMetrix工具来生成各种语言的语料，并对这些语料进行了normalization处理。然后，使用了不同的机器学习算法进行超参数的评估。</li>
<li>results: 实验结果表明，StyloMetrix vectors可以在不同的语言上进行有效的内容分类，并且可以帮助提高深度学习算法的表现。在Random Forest Classifier、Voting Classifier、Logistic Regression等简单机器学习算法上进行了超参数的评估，并且在Transformer架构上进行了深度学习的评估。<details>
<summary>Abstract</summary>
This work aims to provide an overview on the open-source multilanguage tool called StyloMetrix. It offers stylometric text representations that cover various aspects of grammar, syntax and lexicon. StyloMetrix covers four languages: Polish as the primary language, English, Ukrainian and Russian. The normalized output of each feature can become a fruitful course for machine learning models and a valuable addition to the embeddings layer for any deep learning algorithm. We strive to provide a concise, but exhaustive overview on the application of the StyloMetrix vectors as well as explain the sets of the developed linguistic features. The experiments have shown promising results in supervised content classification with simple algorithms as Random Forest Classifier, Voting Classifier, Logistic Regression and others. The deep learning assessments have unveiled the usefulness of the StyloMetrix vectors at enhancing an embedding layer extracted from Transformer architectures. The StyloMetrix has proven itself to be a formidable source for the machine learning and deep learning algorithms to execute different classification tasks.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "stylometric" is translated as "式文学的" (shìwén xué de), which is a compound word consisting of "式" (shì) meaning "style" and "文学" (wénxué) meaning "literature" or "linguistics".* "multilanguage" is translated as "多语言" (duō yǔyán), which is a compound word consisting of "多" (duō) meaning "many" and "语言" (yǔyán) meaning "language".* "tool" is translated as "工具" (gōngjù), which is a generic term for any device or software used to perform a specific task.* "cover" is translated as "覆盖" (fùkài), which means "to cover" or "to encompass".* "aspects" is translated as "方面" (fāngmiàn), which means "aspects" or "facets".* "grammar" is translated as "语法" (yǔfǎ), which is the study of the rules and structures of a language.* "syntax" is translated as "语法结构" (yǔfǎ jiégòu), which is the study of the arrangement of words and phrases to form sentences.* "lexicon" is translated as "词汇" (cíhuì), which is a collection of words and their meanings.* "normalized" is translated as "标准化" (biǎozhǔn huà), which means "to make something conform to a standard or norm".* "output" is translated as "输出" (shūchū), which means "output" or "result".* "feature" is translated as "特征" (tèzhèng), which means "feature" or "characteristic".* "developed" is translated as "开发" (kāifā), which means "to develop" or "to create".* "linguistic" is translated as "语言学的" (yǔyán xué de), which is a compound word consisting of "语言" (yǔyán) meaning "language" and "学的" (xué de) meaning "academic" or "scholarly".* "fruitful" is translated as "有益" (yǒu yì), which means "beneficial" or "useful".* "course" is translated as "课程" (kèchéng), which means "course" or "program".* "machine learning" is translated as "机器学习" (jīqì xuéxí), which is a compound word consisting of "机器" (jīqì) meaning "machine" and "学习" (xuéxí) meaning "learning" or "study".* "deep learning" is translated as "深度学习" (shēngrù xuéxí), which is a compound word consisting of "深度" (shēngrù) meaning "depth" and "学习" (xuéxí) meaning "learning" or "study".* "supervised" is translated as "监督学习" (jiāndū xuéxí), which is a compound word consisting of "监督" (jiāndū) meaning "supervise" and "学习" (xuéxí) meaning "learning" or "study".* "content classification" is translated as "内容分类" (nèiróng fēnlèi), which is a compound word consisting of "内容" (nèiróng) meaning "content" and "分类" (fēnlèi) meaning "classification" or "categorization".* "simple algorithms" is translated as "简单的算法" (jiǎnduō de suānfǎ), which is a compound word consisting of "简单" (jiǎnduō) meaning "simple" and "算法" (suānfǎ) meaning "algorithm".* "random forest classifier" is translated as "随机森林分类器" (suījì sēnjīn fēnlèi zhīngjī), which is a compound word consisting of "随机" (suījì) meaning "random" and "森林" (sēnjīn) meaning "forest" and "分类器" (fēnlèi zhīngjī) meaning "classifier".* "voting classifier" is translated as "投票分类器" (tóuchòu fēnlèi zhīngjī), which is a compound word consisting of "投票" (tóuchòu) meaning "vote" and "分类器" (fēnlèi zhīngjī) meaning "classifier".* "logistic regression" is translated as "逻辑回归" (suǒyì huíqiù), which is a compound word consisting of "逻辑" (suǒyì) meaning "logic" and "回归" (huíqiù) meaning "regression".* "deep learning assessments" is translated as "深度学习评估" (shēngrù xuéxí píngjì), which is a compound word consisting of "深度" (shēngrù) meaning "depth" and "学习" (xuéxí) meaning "learning" or "study" and "评估" (píngjì) meaning "assessment" or "evaluation".* "Transformer architectures" is translated as "变换器架构" (biànhuà zhìgòu), which is a compound word consisting of "变换" (biànhuà) meaning "transformation" and "器架构" (zhìgòu) meaning "architecture".
</details></li>
</ul>
<hr>
<h2 id="ChatPRCS-A-Personalized-Support-System-for-English-Reading-Comprehension-based-on-ChatGPT"><a href="#ChatPRCS-A-Personalized-Support-System-for-English-Reading-Comprehension-based-on-ChatGPT" class="headerlink" title="ChatPRCS: A Personalized Support System for English Reading Comprehension based on ChatGPT"></a>ChatPRCS: A Personalized Support System for English Reading Comprehension based on ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12808">http://arxiv.org/abs/2309.12808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xizhe Wang, Yihua Zhong, Changqin Huang, Xiaodi Huang</li>
<li>for: 提高学生的阅读理解能力</li>
<li>methods: 使用大语言模型技术，包括预测学生阅读理解水平、生成问题和自动评估等方法</li>
<li>results: 实验结果显示，ChatPRCS可以为学生提供高质量的阅读理解问题，与专家制定的问题相似程度有统计显著的相似性<details>
<summary>Abstract</summary>
As a common approach to learning English, reading comprehension primarily entails reading articles and answering related questions. However, the complexity of designing effective exercises results in students encountering standardized questions, making it challenging to align with individualized learners' reading comprehension ability. By leveraging the advanced capabilities offered by large language models, exemplified by ChatGPT, this paper presents a novel personalized support system for reading comprehension, referred to as ChatPRCS, based on the Zone of Proximal Development theory. ChatPRCS employs methods including reading comprehension proficiency prediction, question generation, and automatic evaluation, among others, to enhance reading comprehension instruction. First, we develop a new algorithm that can predict learners' reading comprehension abilities using their historical data as the foundation for generating questions at an appropriate level of difficulty. Second, a series of new ChatGPT prompt patterns is proposed to address two key aspects of reading comprehension objectives: question generation, and automated evaluation. These patterns further improve the quality of generated questions. Finally, by integrating personalized ability and reading comprehension prompt patterns, ChatPRCS is systematically validated through experiments. Empirical results demonstrate that it provides learners with high-quality reading comprehension questions that are broadly aligned with expert-crafted questions at a statistical level.
</details>
<details>
<summary>摘要</summary>
通常来说，学习英语的读写涉及到阅读文章并回答相关问题。然而，设计有效的训练活动具有复杂性，导致学生遇到标准化的问题，困难与个性化学生的读写理解水平进行对应。本文基于大语言模型的高级功能，例如ChatGPT，提出了一种新的个性化支持系统，称为ChatPRCS，基于读写理解能力的发展Zone of Proximal Development理论。ChatPRCS使用包括读写理解能力预测、问题生成和自动评估等方法，以提高读写理解教学。首先，我们开发了一种新的算法，可以根据学生的历史数据预测他们的读写理解能力，并使用这些数据来生成适合的题目。其次，我们提出了一系列新的ChatGPT提示模式，用于解决读写理解目标的两个关键方面：问题生成和自动评估。这些模式进一步提高生成的题目质量。最后，通过结合个性化能力和读写理解提示模式，我们系统化验证了ChatPRCS。实验结果表明，它可以为学生提供高质量的读写理解题目，与专家制作的问题在统计上保持一致。
</details></li>
</ul>
<hr>
<h2 id="Furthest-Reasoning-with-Plan-Assessment-Stable-Reasoning-Path-with-Retrieval-Augmented-Large-Language-Models"><a href="#Furthest-Reasoning-with-Plan-Assessment-Stable-Reasoning-Path-with-Retrieval-Augmented-Large-Language-Models" class="headerlink" title="Furthest Reasoning with Plan Assessment: Stable Reasoning Path with Retrieval-Augmented Large Language Models"></a>Furthest Reasoning with Plan Assessment: Stable Reasoning Path with Retrieval-Augmented Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12767">http://arxiv.org/abs/2309.12767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yin Zhu, Zhiling Luo, Gong Cheng</li>
<li>for: 本研究旨在解决现有多步问答（MHQA）方法中的两个主要缺陷：一是信息检索器（IR）因为生成过程中的低质量问题而受到限制，二是语言模型（LLM）因为与 irrelevant knowledge 的交互而导致偏差。</li>
<li>methods: 本研究提出了一种新的管道方法，即 Furthest-Reasoning-with-Plan-Assessment（FuRePA），其包括一种改进的框架（Furthest Reasoning）和一个附加的模块（Plan Assessor）。 Furthest Reasoning operates by masking previous reasoning path and generated queries for LLM, encouraging LLM generating chain of thought from scratch in each iteration。 Plan Assessor 是一个训练过的评价器，可以选择 LLM 提出的合适的计划。</li>
<li>results: 本研究在三个公开的多步问答数据集上进行了评估，并与现有最佳方法进行比较。结果显示， FuRePA 在大多数指标上表现出色，相比之下， achieved a 10%-12% 的答案准确率。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), acting as a powerful reasoner and generator, exhibit extraordinary performance across various natural language tasks, such as question answering (QA). Among these tasks, Multi-Hop Question Answering (MHQA) stands as a widely discussed category, necessitating seamless integration between LLMs and the retrieval of external knowledge. Existing methods employ LLM to generate reasoning paths and plans, and utilize IR to iteratively retrieve related knowledge, but these approaches have inherent flaws. On one hand, Information Retriever (IR) is hindered by the low quality of generated queries by LLM. On the other hand, LLM is easily misguided by the irrelevant knowledge by IR. These inaccuracies, accumulated by the iterative interaction between IR and LLM, lead to a disaster in effectiveness at the end. To overcome above barriers, in this paper, we propose a novel pipeline for MHQA called Furthest-Reasoning-with-Plan-Assessment (FuRePA), including an improved framework (Furthest Reasoning) and an attached module (Plan Assessor). 1) Furthest reasoning operates by masking previous reasoning path and generated queries for LLM, encouraging LLM generating chain of thought from scratch in each iteration. This approach enables LLM to break the shackle built by previous misleading thoughts and queries (if any). 2) The Plan Assessor is a trained evaluator that selects an appropriate plan from a group of candidate plans proposed by LLM. Our methods are evaluated on three highly recognized public multi-hop question answering datasets and outperform state-of-the-art on most metrics (achieving a 10%-12% in answer accuracy).
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）作为强大的理解和生成工具，在不同的自然语言任务中表现出色，其中包括多步 вопро答（MHQA）类型。在这些任务中，我们需要让 LLM 和外部知识的搜寻紧密相连。现有的方法使用 LLM 生成推理路径和计划，并使用 IR 逐步获取相关知识，但这些方法存在问题。一方面，资讯搜寻器（IR）受到 LLM 产生的问题质量低下的限制。另一方面， LLM 受到 IR 提供的无关知识的影响，导致错误的推理。这些错误，在 LLM 和 IR 之间的回归交互中累累积累，最终导致效率下降。为解决以上问题，在这篇论文中，我们提出了一个新的多步 вопро答（MHQA）管道，称为 Furthest-Reasoning-with-Plan-Assessment（FuRePA），包括改进的架构（ Furthest Reasoning）和附加的模组（ Plan Assessor）。1. Furthest Reasoning 运作方式是将前一次的推理路径和生成的问题遮盖，让 LLM 在每次回归中从头开始生成推理链。这种方法允许 LLM 破坏前一次的错误思维和问题（如果有），并将注意力集中在更加重要的问题上。2. Plan Assessor 是一个训练好的评估器，可以从 LLM 提供的候选计划中选择最佳的计划。我们的方法在三个公开的多步 вопро答 datasets 上进行评估，并在大多数指标上超越了现有的state-of-the-art（实现了10%-12%的答案精度提升）。
</details></li>
</ul>
<hr>
<h2 id="Reduce-Reuse-Recycle-Is-Perturbed-Data-better-than-Other-Language-augmentation-for-Low-Resource-Self-Supervised-Speech-Models"><a href="#Reduce-Reuse-Recycle-Is-Perturbed-Data-better-than-Other-Language-augmentation-for-Low-Resource-Self-Supervised-Speech-Models" class="headerlink" title="Reduce, Reuse, Recycle: Is Perturbed Data better than Other Language augmentation for Low Resource Self-Supervised Speech Models"></a>Reduce, Reuse, Recycle: Is Perturbed Data better than Other Language augmentation for Low Resource Self-Supervised Speech Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12763">http://arxiv.org/abs/2309.12763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asad Ullah, Alessandro Ragano, Andrew Hines</li>
<li>for: 本研究旨在提高low resource语言下的自主学习表示学习（SSRL）模型的表现，并评估其在下游phoneme认识任务中的性能。</li>
<li>methods: 本研究使用了音频扩展来预训SSRL模型，并评估其在phoneme认识任务中的表现。我们系统地比较了不同的扩展技术，包括拟音变化、噪音添加、重音目标语言speech和其他语言speech。我们发现，将扩展技术与拟音变化相结合（噪音&#x2F;拟音）是最佳扩展策略，超过了重音和语言知识传递。</li>
<li>results: 我们发现，使用具有不同量和类型的预训数据，SSRL模型在phoneme认识任务中的表现都有所提高。此外，我们还评估了扩展数据的缩放因子，以达到与target domain speech预训数据相等的性能。我们的发现表明，在resource受限的语言下，使用本地生成的扩展数据可以超过语言知识传递和其他语言speech的表现。<details>
<summary>Abstract</summary>
Self-supervised representation learning (SSRL) has improved the performance on downstream phoneme recognition versus supervised models. Training SSRL models requires a large amount of pre-training data and this poses a challenge for low resource languages. A common approach is transferring knowledge from other languages. Instead, we propose to use audio augmentation to pre-train SSRL models in a low resource condition and evaluate phoneme recognition as downstream task. We performed a systematic comparison of augmentation techniques, namely: pitch variation, noise addition, accented target-language speech and other language speech. We found combined augmentations (noise/pitch) was the best augmentation strategy outperforming accent and language knowledge transfer. We compared the performance with various quantities and types of pre-training data. We examined the scaling factor of augmented data to achieve equivalent performance to models pre-trained with target domain speech. Our findings suggest that for resource constrained languages, in-domain synthetic augmentation can outperform knowledge transfer from accented or other language speech.
</details>
<details>
<summary>摘要</summary>
自我指导学习（SSRL）已经提高了下游音频识别的性能，而不需要大量的标注数据。然而，对于低资源语言，具有大量预训练数据的困难。而不是通过语言知识传输，我们提议使用音频加工来预训练SSRL模型，并评估音频识别作为下游任务。我们进行了系统性的比较，包括噪音添加、抖音变化、外语言材料和对应语言材料。我们发现将噪音和抖音相结合是最佳的加工策略，超过了对应语言和外语言知识传输。我们对不同量和类型的预训练数据进行了比较，并评估了增强数据的扩展因子以实现与目标频谱 speech 的相同性。我们的发现表明，在资源受限的语言中，可以通过本地生成的增强数据来超越对应语言和外语言的知识传输。
</details></li>
</ul>
<hr>
<h2 id="Semantic-similarity-prediction-is-better-than-other-semantic-similarity-measures"><a href="#Semantic-similarity-prediction-is-better-than-other-semantic-similarity-measures" class="headerlink" title="Semantic similarity prediction is better than other semantic similarity measures"></a>Semantic similarity prediction is better than other semantic similarity measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12697">http://arxiv.org/abs/2309.12697</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aieng-lab/stsscore">https://github.com/aieng-lab/stsscore</a></li>
<li>paper_authors: Steffen Herbold</li>
<li>for:  mesure la similarité sémantique entre des textes naturels</li>
<li>methods: utilise une modèle fine-tuné pour prédire la similarité</li>
<li>results: obtenu une mesure de similarité plus robuste et alignée avec les attentes que les autres approches<details>
<summary>Abstract</summary>
Semantic similarity between natural language texts is typically measured either by looking at the overlap between subsequences (e.g., BLEU) or by using embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we are only interested in measuring the semantic similarity, it is better to directly predict the similarity using a fine-tuned model for such a task. Using a fine-tuned model for the STS-B from the GLUE benchmark, we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches.
</details>
<details>
<summary>摘要</summary>
<<SYS>>按照以下文本翻译成简化中文：<</SYS>>自然语言文本之间的 semantics 相似性通常通过子序列重叠（例如 BLEU）或使用嵌入（例如 BERTScore、S-BERT）来衡量。在这篇论文中，我们认为只需要量化 semantics 相似性时，直接使用特定任务的 fine-tuned 模型来预测相似性是更好的方法。使用 GLUE benchmark 中的 STS-B 任务中的 fine-tuned 模型，我们定义了 STSScore 方法，并证明其生成的相似性更加符合我们对坚实 semantics 相似性的预期，与其他方法相比。
</details></li>
</ul>
<hr>
<h2 id="AMPLIFY-Attention-based-Mixup-for-Performance-Improvement-and-Label-Smoothing-in-Transformer"><a href="#AMPLIFY-Attention-based-Mixup-for-Performance-Improvement-and-Label-Smoothing-in-Transformer" class="headerlink" title="AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer"></a>AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12689">http://arxiv.org/abs/2309.12689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kiwi-lilo/amplify">https://github.com/kiwi-lilo/amplify</a></li>
<li>paper_authors: Leixin Yang, Yaping Zhang, Haoyu Xiong, Yu Xiang</li>
<li>for: 提高文本分类 task 的性能，降低模型对噪音和异常值的敏感性。</li>
<li>methods: 提出了一种新的 Mixup 方法 called AMPLIFY，通过 Transformer 自带的注意力机制来减少原始样本中噪音和异常值的影响，不增加额外可训练参数，计算成本很低。</li>
<li>results: 在 7 个 benchmark dataset 上，AMPLIFY 在文本分类任务中比其他 Mixup 方法具有更高的性能，而且在较小的计算成本下。<details>
<summary>Abstract</summary>
Mixup is an effective data augmentation method that generates new augmented samples by aggregating linear combinations of different original samples. However, if there are noises or aberrant features in the original samples, Mixup may propagate them to the augmented samples, leading to over-sensitivity of the model to these outliers . To solve this problem, this paper proposes a new Mixup method called AMPLIFY. This method uses the Attention mechanism of Transformer itself to reduce the influence of noises and aberrant values in the original samples on the prediction results, without increasing additional trainable parameters, and the computational cost is very low, thereby avoiding the problem of high resource consumption in common Mixup methods such as Sentence Mixup . The experimental results show that, under a smaller computational resource cost, AMPLIFY outperforms other Mixup methods in text classification tasks on 7 benchmark datasets, providing new ideas and new ways to further improve the performance of pre-trained models based on the Attention mechanism, such as BERT, ALBERT, RoBERTa, and GPT. Our code can be obtained at https://github.com/kiwi-lilo/AMPLIFY.
</details>
<details>
<summary>摘要</summary>
混合是一种有效的数据增强方法，可以生成新的增强样本通过原始样本的线性组合。但是，如果原始样本中存在噪声或异常特征，那么混合可能会传递这些噪声或异常特征到增强样本中，导致模型对这些噪声或异常特征过敏。为解决这个问题，本文提出了一种新的混合方法called AMPLIFY。这种方法使用Transformer自带的注意力机制来减少原始样本中噪声或异常值对预测结果的影响，无需增加额外可训练参数，计算成本非常低，因此可以避免常见的混合方法如 Sentence Mixup 中的高资源消耗问题。实验结果表明，在相对较小的计算资源成本下，AMPLIFY在文本分类任务中比其他混合方法表现更好，提供了新的想法和新的方法来进一步提高基于注意力机制的预测模型，如BERT、ALBERT、RoBERTa和GPT。我们的代码可以在https://github.com/kiwi-lilo/AMPLIFY获取。
</details></li>
</ul>
<hr>
<h2 id="JCoLA-Japanese-Corpus-of-Linguistic-Acceptability"><a href="#JCoLA-Japanese-Corpus-of-Linguistic-Acceptability" class="headerlink" title="JCoLA: Japanese Corpus of Linguistic Acceptability"></a>JCoLA: Japanese Corpus of Linguistic Acceptability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12676">http://arxiv.org/abs/2309.12676</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osekilab/jcola">https://github.com/osekilab/jcola</a></li>
<li>paper_authors: Taiga Someya, Yushi Sugimoto, Yohei Oseki<br>for:这个论文的目的是为了评估不同类型的日语语言模型在语法可接受性领域的性能。methods:这篇论文使用了10,020个句子的手动标注的双值可接受性判断，其中86%是来自语言学教科书和手册的简单可接受性判断，剩下的14%是根据语言学期刊文章中的12种语言现象分类的。然后， authors使用这些数据来评估9种日语语言模型的语法知识。results:论文的结果表明，一些模型在域内数据上可以超越人类性能，而在域外数据上则无法超越人类性能。此外，对具体的语言现象进行分析也表明，虽然神经语言模型在地方语法依赖关系上很强，但在长距离语法依赖关系上表现不佳，如宾格结构和词汇协调等。<details>
<summary>Abstract</summary>
Neural language models have exhibited outstanding performance in a range of downstream tasks. However, there is limited understanding regarding the extent to which these models internalize syntactic knowledge, so that various datasets have recently been constructed to facilitate syntactic evaluation of language models across languages. In this paper, we introduce JCoLA (Japanese Corpus of Linguistic Acceptability), which consists of 10,020 sentences annotated with binary acceptability judgments. Specifically, those sentences are manually extracted from linguistics textbooks, handbooks and journal articles, and split into in-domain data (86 %; relatively simple acceptability judgments extracted from textbooks and handbooks) and out-of-domain data (14 %; theoretically significant acceptability judgments extracted from journal articles), the latter of which is categorized by 12 linguistic phenomena. We then evaluate the syntactic knowledge of 9 different types of Japanese language models on JCoLA. The results demonstrated that several models could surpass human performance for the in-domain data, while no models were able to exceed human performance for the out-of-domain data. Error analyses by linguistic phenomena further revealed that although neural language models are adept at handling local syntactic dependencies like argument structure, their performance wanes when confronted with long-distance syntactic dependencies like verbal agreement and NPI licensing.
</details>
<details>
<summary>摘要</summary>
neural language models 在多种下游任务中表现出色，但是对这些模型内化语法知识的理解还很有限，因此在不同语言之间建立了一些数据集，以便对语言模型的语法评估。本文介绍了日语Corpus of Linguistic Acceptability（JCoLA），包含10,020个句子，每个句子都有 binary acceptability 判断。具体来说，这些句子来自语言学书籍、手册和学术期刊，并分为预测数据（86%；相对简单的acceptability judgments从书籍和手册中提取）和 OUT-OF-DOMAIN 数据（14%；从期刊中提取，并分为12种语言现象）。然后，我们对9种日语语言模型在 JCoLA 上进行了语法知识的评估。结果显示，一些模型在预测数据上能够超越人类性能，而在 OUT-OF-DOMAIN 数据上则没有任何模型能够达到人类性能。进一步的错误分析按语言现象分类，表明了 neural language models 在处理本地语法依赖关系（如语素结构）方面表现出色，但是在面对远程语法依赖关系（如 Nominalization 和 NPI 许可）时，其性能却衰退。
</details></li>
</ul>
<hr>
<h2 id="HRoT-Hybrid-prompt-strategy-and-Retrieval-of-Thought-for-Table-Text-Hybrid-Question-Answering"><a href="#HRoT-Hybrid-prompt-strategy-and-Retrieval-of-Thought-for-Table-Text-Hybrid-Question-Answering" class="headerlink" title="HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering"></a>HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12669">http://arxiv.org/abs/2309.12669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongxu Luo, Fangyu Lei, Jiahe Lei, Weihao Liu, Shihu He, Jun Zhao, Kang Liu</li>
<li>for: 这篇论文是为了解决Answering numerical questions over hybrid contents from the given tables and text(TextTableQA)问题。</li>
<li>methods: 这篇论文使用了Large Language Models (LLMs)和In-Context Learning技术，以及Chain-of-Thought prompting。</li>
<li>results: 这篇论文的方法在 MultiHiertt 数据集中的少量学习情况下达到了State-of-the-Art (SOTA) 性能。<details>
<summary>Abstract</summary>
Answering numerical questions over hybrid contents from the given tables and text(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs) have gained significant attention in the NLP community. With the emergence of large language models, In-Context Learning and Chain-of-Thought prompting have become two particularly popular research topics in this field. In this paper, we introduce a new prompting strategy called Hybrid prompt strategy and Retrieval of Thought for TextTableQA. Through In-Context Learning, we prompt the model to develop the ability of retrieval thinking when dealing with hybrid data. Our method achieves superior performance compared to the fully-supervised SOTA on the MultiHiertt dataset in the few-shot setting.
</details>
<details>
<summary>摘要</summary>
Answering numerical questions over hybrid contents from the given tables and text(文本表格问答) is a challenging task. Recently, Large Language Models (LLMs) have gained significant attention in the NLP community. With the emergence of large language models, In-Context Learning and Chain-of-Thought prompting have become two particularly popular research topics in this field. In this paper, we introduce a new prompting strategy called Hybrid prompt strategy and Retrieval of Thought for TextTableQA. Through In-Context Learning, we prompt the model to develop the ability of retrieval thinking when dealing with hybrid data. Our method achieves superior performance compared to the fully-supervised SOTA on the MultiHiertt dataset in the few-shot setting.Here's the translation breakdown:* Answering numerical questions over hybrid contents (文本表格问答)	+ Answering (答案)	+ Numerical questions (数字问题)	+ Hybrid contents (混合内容)	+ Text and tables (文本和表格)* Recently, Large Language Models (LLMs) have gained significant attention (最近，大型语言模型已经吸引了重要的注意)	+ Recently (最近)	+ Large Language Models (大型语言模型)	+ Gained significant attention (吸引了重要的注意)* With the emergence of large language models (LLMs), In-Context Learning and Chain-of-Thought prompting have become two particularly popular research topics (LLMs的出现使得受到了关注的研究话题)	+ With the emergence of (出现)	+ Large language models (LLMs)	+ In-Context Learning (在Context学习)	+ Chain-of-Thought prompting (Chain-of-Thought提问)	+ Two particularly popular research topics (两个非常流行的研究话题)* In this paper, we introduce a new prompting strategy called Hybrid prompt strategy and Retrieval of Thought (在这篇论文中，我们介绍了一种新的提问策略)	+ In this paper (在这篇论文中)	+ We introduce (介绍)	+ A new prompting strategy (一种新的提问策略)	+ Called Hybrid prompt strategy (被称为Hybrid提问策略)	+ And Retrieval of Thought (以及 Retrieval of Thought)* Through In-Context Learning, we prompt the model to develop the ability of retrieval thinking (通过In-Context学习，我们透过提问模型发展 Retrieval thinking的能力)	+ Through (通过)	+ In-Context Learning (在Context学习)	+ We prompt (我们透过提问)	+ The model (模型)	+ To develop (发展)	+ The ability of retrieval thinking ( Retrieval thinking的能力)* Our method achieves superior performance compared to the fully-supervised SOTA on the MultiHiertt dataset in the few-shot setting (我们的方法在 MultiHiertt 数据集上在少量学习设置下表现出了superior的性能)	+ Our method (我们的方法)	+ Achieves (表现出)	+ Superior performance (superior的性能)	+ Compared to (与)	+ The fully-supervised SOTA (完全指导的SOTA)	+ On the MultiHiertt dataset (在 MultiHiertt 数据集上)	+ In the few-shot setting (在少量学习设置下)
</details></li>
</ul>
<hr>
<h2 id="Decoding-Affect-in-Dyadic-Conversations-Leveraging-Semantic-Similarity-through-Sentence-Embedding"><a href="#Decoding-Affect-in-Dyadic-Conversations-Leveraging-Semantic-Similarity-through-Sentence-Embedding" class="headerlink" title="Decoding Affect in Dyadic Conversations: Leveraging Semantic Similarity through Sentence Embedding"></a>Decoding Affect in Dyadic Conversations: Leveraging Semantic Similarity through Sentence Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12646">http://arxiv.org/abs/2309.12646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen-Wei Yu, Yun-Shiuan Chuang, Alexandros N. Lotsos, Claudia M. Haase</li>
<li>For: The paper aims to explore the use of sentence embeddings in analyzing real-world dyadic interactions and predicting the affect of conversational participants.* Methods: The study employs a Transformer-based model to obtain the embeddings of utterances from each speaker in 50 married couples’ conversations about conflicts and pleasant activities.* Results: The study finds that semantic similarity has a positive association with wives’ affect during conflict conversations, but not with husbands’ affect or during pleasant conversations.Here’s the information in Simplified Chinese text:</li>
<li>for: 这研究旨在利用句子嵌入来分析现实生活中的对话和预测对话参与者的情感。</li>
<li>methods: 这些研究使用Transformer模型来获取每个说话者的句子嵌入。</li>
<li>results: 研究发现，在对话中的 semantic similarity 与妻子在对抗对话中的情感有正相关关系，但不与丈夫在对抗对话中的情感或在愉悦对话中的情感有关系。<details>
<summary>Abstract</summary>
Recent advancements in Natural Language Processing (NLP) have highlighted the potential of sentence embeddings in measuring semantic similarity. Yet, its application in analyzing real-world dyadic interactions and predicting the affect of conversational participants remains largely uncharted. To bridge this gap, the present study utilizes verbal conversations within 50 married couples talking about conflicts and pleasant activities. Transformer-based model all-MiniLM-L6-v2 was employed to obtain the embeddings of the utterances from each speaker. The overall similarity of the conversation was then quantified by the average cosine similarity between the embeddings of adjacent utterances. Results showed that semantic similarity had a positive association with wives' affect during conflict (but not pleasant) conversations. Moreover, this association was not observed with husbands' affect regardless of conversation types. Two validation checks further provided support for the validity of the similarity measure and showed that the observed patterns were not mere artifacts of data. The present study underscores the potency of sentence embeddings in understanding the association between interpersonal dynamics and individual affect, paving the way for innovative applications in affective and relationship sciences.
</details>
<details>
<summary>摘要</summary>
现代自然语言处理（NLP）技术的发展，推祟了句子嵌入的潜在意义。然而，它在实际对话中分析双方对话和预测对话参与者的情感影响仍然是未知之地。为了bridging这一 gab，本研究使用了50对夫妻互动的对话，其中一方为冲突对话，另一方为愉悦对话。使用Transformer模型all-MiniLM-L6-v2，从每个发言人的utterance中获得了嵌入。然后，通过计算 adjacentutterance的cosine相似性的平均值来衡量对话的总相似性。结果表明，在冲突对话中，夫人的情感相关性与句子嵌入的semantic相似性呈正相关关系。此外，这种相关性不存在于愉悦对话中。两项验证检查还为研究的有效性提供了支持，并证明了所见到的模式不是数据的假象。本研究表明，句子嵌入可以帮助我们理解对话中的人际动力和个体情感之间的关系，并为情感科学和关系科学开辟了新的应用领域。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Diversify-Neural-Text-Generation-via-Degenerative-Model"><a href="#Learning-to-Diversify-Neural-Text-Generation-via-Degenerative-Model" class="headerlink" title="Learning to Diversify Neural Text Generation via Degenerative Model"></a>Learning to Diversify Neural Text Generation via Degenerative Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12619">http://arxiv.org/abs/2309.12619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jimin Hong, ChaeHun Park, Jaegul Choo</li>
<li>for: 提高Neural语言模型的多样性和有用性，以扩展其应用范围。</li>
<li>methods: 提出一种新的方法，基于模型学习属性的观察：模型主要学习引起堕落问题的特征。该方法包括两个模型的训练：首先训练一个用于增强不良模式的模型，然后通过关注这个模型无法学习的模式来提高第二个模型的多样性。</li>
<li>results: 通过两个任务， namely语言模型和对话生成，进行了广泛的实验，证明了该方法的有效性。<details>
<summary>Abstract</summary>
Neural language models often fail to generate diverse and informative texts, limiting their applicability in real-world problems. While previous approaches have proposed to address these issues by identifying and penalizing undesirable behaviors (e.g., repetition, overuse of frequent words) from language models, we propose an alternative approach based on an observation: models primarily learn attributes within examples that are likely to cause degeneration problems. Based on this observation, we propose a new approach to prevent degeneration problems by training two models. Specifically, we first train a model that is designed to amplify undesirable patterns. We then enhance the diversity of the second model by focusing on patterns that the first model fails to learn. Extensive experiments on two tasks, namely language modeling and dialogue generation, demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
neural network语言模型经常无法生成多样化和有用的文本，限制它们在实际问题中的应用。而以前的方法已经提议通过识别和处罚不良行为（例如重复、频繁使用常见词）来解决这些问题。我们则基于一个观察：模型主要学习文本中可能导致异常问题的特征。基于这个观察，我们提出了一种新的方法，通过训练两个模型来预防异常问题。具体来说，我们首先训练一个用于强化不良模式的模型。然后，我们通过关注这个模型无法学习的模式来提高第二个模型的多样性。我们在语言模型和对话生成两个任务上进行了广泛的实验，并证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-Model-Insights-A-Dataset-for-Automated-Model-Card-Generation"><a href="#Unlocking-Model-Insights-A-Dataset-for-Automated-Model-Card-Generation" class="headerlink" title="Unlocking Model Insights: A Dataset for Automated Model Card Generation"></a>Unlocking Model Insights: A Dataset for Automated Model Card Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12616">http://arxiv.org/abs/2309.12616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shruti Singh, Hitesh Lodwal, Husain Malwat, Rakesh Thakur, Mayank Singh</li>
<li>for: 这paper是为了提高机器学习模型的训练和应用而写的。</li>
<li>methods: 这paper使用了500个问题对25种机器学习模型进行了询问，以描述这些模型的训练配置、数据集、偏见、结构细节和训练资源。</li>
<li>results: 这paper发现现有的语言模型（如ChatGPT-3.5、LLaMa和Galactica）在理解研讨纸和生成文字回答中存在差距，并且可以使用这些模型来自动生成模型卡。<details>
<summary>Abstract</summary>
Language models (LMs) are no longer restricted to ML community, and instruction-tuned LMs have led to a rise in autonomous AI agents. As the accessibility of LMs grows, it is imperative that an understanding of their capabilities, intended usage, and development cycle also improves. Model cards are a popular practice for documenting detailed information about an ML model. To automate model card generation, we introduce a dataset of 500 question-answer pairs for 25 ML models that cover crucial aspects of the model, such as its training configurations, datasets, biases, architecture details, and training resources. We employ annotators to extract the answers from the original paper. Further, we explore the capabilities of LMs in generating model cards by answering questions. Our initial experiments with ChatGPT-3.5, LLaMa, and Galactica showcase a significant gap in the understanding of research papers by these aforementioned LMs as well as generating factual textual responses. We posit that our dataset can be used to train models to automate the generation of model cards from paper text and reduce human effort in the model card curation process. The complete dataset is available on https://osf.io/hqt7p/?view_only=3b9114e3904c4443bcd9f5c270158d37
</details>
<details>
<summary>摘要</summary>
机器学习模型（LM）不再受限于机器学习社区， instrucion-tuned LM 的出现导致自主AI代理人数量的增加。随着LM的访问权增加，理解其能力、适用范围和开发周期也变得非常重要。模型卡是一种很流行的实践，用于记录ML模型的详细信息。为了自动生成模型卡，我们提出了一个包含25种ML模型的500个问题答案对集。我们采用了人工批注人员，从原始论文中提取答案。此外，我们还explore了LM的可能性，以及它们在生成模型卡时的表现。我们的初步实验表明，ChatGPT-3.5、LLaMa和Galactica等LM在理解研讨文献和生成事实性文本响应方面存在较大的差距。我们认为，我们的数据集可以用于训练模型，以自动生成模型卡从文献中，并减少人类努力在模型卡筹编过程中。完整的数据集可以在https://osf.io/hqt7p/?view_only=3b9114e3904c4443bcd9f5c270158d37中找到。
</details></li>
</ul>
<hr>
<h2 id="Is-it-Possible-to-Modify-Text-to-a-Target-Readability-Level-An-Initial-Investigation-Using-Zero-Shot-Large-Language-Models"><a href="#Is-it-Possible-to-Modify-Text-to-a-Target-Readability-Level-An-Initial-Investigation-Using-Zero-Shot-Large-Language-Models" class="headerlink" title="Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models"></a>Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12551">http://arxiv.org/abs/2309.12551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asma Farajidizaji, Vatsal Raina, Mark Gales</li>
<li>for: 本研究旨在提出一种新的文本修改任务，即可以独立地控制文本的阅读difficulty水平。</li>
<li>methods: 本研究使用了ChatGPT和Llama-2两种基础模型，以及一种扩展方法，即通过语言模型两次生成 парафраз。</li>
<li>results: 研究发现，零配置方法可以将文本的阅读difficulty水平Push in the desired direction，但最终阅读difficulty仍然与原始文本的阅读difficulty相关。此外，研究还发现，阅读difficulty的变化会导致文本 semantic和 lexical similarity降低。<details>
<summary>Abstract</summary>
Text simplification is a common task where the text is adapted to make it easier to understand. Similarly, text elaboration can make a passage more sophisticated, offering a method to control the complexity of reading comprehension tests. However, text simplification and elaboration tasks are limited to only relatively alter the readability of texts. It is useful to directly modify the readability of any text to an absolute target readability level to cater to a diverse audience. Ideally, the readability of readability-controlled generated text should be independent of the source text. Therefore, we propose a novel readability-controlled text modification task. The task requires the generation of 8 versions at various target readability levels for each input text. We introduce novel readability-controlled text modification metrics. The baselines for this task use ChatGPT and Llama-2, with an extension approach introducing a two-step process (generating paraphrases by passing through the language model twice). The zero-shot approaches are able to push the readability of the paraphrases in the desired direction but the final readability remains correlated with the original text's readability. We also find greater drops in semantic and lexical similarity between the source and target texts with greater shifts in the readability.
</details>
<details>
<summary>摘要</summary>
文本简化和文本膨化是常见的任务，它们可以使文本更易于理解。然而，文本简化和膨化任务只能有限地改变文本的可读性。为了直接修改文本的可读性水平，我们提出了一个新的可读性控制文本修改任务。这个任务需要对每个输入文本生成8个版本，每个版本都达到不同的目标可读性水平。我们介绍了一些新的可读性控制文本修改指标。基elines для这个任务使用ChatGPT和Llama-2，我们还提出了一种扩展方法，即通过语言模型两次进行两步过程（生成重叠的重叠）来生成重叠。我们发现零配置方法可以推动文本的可读性水平，但最终的可读性仍然与原始文本的可读性相关。此外，我们还发现，随着可读性的增加，文本之间的semantic和lexical相似度会降低。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Answerability-Evaluation-for-Question-Generation"><a href="#Automatic-Answerability-Evaluation-for-Question-Generation" class="headerlink" title="Automatic Answerability Evaluation for Question Generation"></a>Automatic Answerability Evaluation for Question Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12546">http://arxiv.org/abs/2309.12546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zifan Wang, Kotaro Funakoshi, Manabu Okumura</li>
<li>for: 这 paper 的目的是提出一种新的自动评价指标，以评估生成的问题是否能够由参考答案回答。</li>
<li>methods: 这 paper 使用了一种基于提示的评价指标， named PMAN，通过对生成的问题和参考答案进行对比，来评估问题的可answerability。</li>
<li>results: 经过广泛的实验，这 paper 的评价结果被证明可靠，与人工评价结果相align。此外，这 paper 还应用了其metric来评估生成问题模型的性能，发现其metric 与传统的评价指标 complementary。最后， authors 使用 ChatGPT 实现了一个 SOTA 的问题生成模型。<details>
<summary>Abstract</summary>
Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed for natural language generation (NLG) tasks, are based on measuring the n-gram overlap between the generated and reference text. These simple metrics may be insufficient for more complex tasks, such as question generation (QG), which requires generating questions that are answerable by the reference answers. Developing a more sophisticated automatic evaluation metric, thus, remains as an urgent problem in QG research. This work proposes a Prompting-based Metric on ANswerability (PMAN), a novel automatic evaluation metric to assess whether the generated questions are answerable by the reference answers for the QG tasks. Extensive experiments demonstrate that its evaluation results are reliable and align with human evaluations. We further apply our metric to evaluate the performance of QG models, which shows our metric complements conventional metrics. Our implementation of a ChatGPT-based QG model achieves state-of-the-art (SOTA) performance in generating answerable questions.
</details>
<details>
<summary>摘要</summary>
传统的自动评价指标，如BLEU和ROUGE，是基于生成和参考文本中的n-gram重叠而定义的。这些简单的指标可能不够用于更复杂的任务，如问题生成（QG），因为QG需要生成可回答的问题。开发一种更加复杂的自动评价指标，因此是QG研究中的紧迫问题。本工作提出了Answerability-based Metric on ANswerability（PMAN），一种新的自动评价指标，用于评估生成的问题是否可以由参考答案回答。我们进行了广泛的实验，并证明了其评价结果的可靠性和与人工评价结果的一致性。此外，我们还应用了我们的指标来评估QG模型的性能，并发现了它与传统指标的协同作用。我们实现了基于ChatGPT的QG模型，实现了状态的杰出表现（SOTA）在生成可回答的问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/22/cs.CL_2023_09_22/" data-id="clpztdne300c4es88dyjt3nsm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/22/cs.LG_2023_09_22/" class="article-date">
  <time datetime="2023-09-22T10:00:00.000Z" itemprop="datePublished">2023-09-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/22/cs.LG_2023_09_22/">cs.LG - 2023-09-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-LHCb-ultra-fast-simulation-option-Lamarr-design-and-validation"><a href="#The-LHCb-ultra-fast-simulation-option-Lamarr-design-and-validation" class="headerlink" title="The LHCb ultra-fast simulation option, Lamarr: design and validation"></a>The LHCb ultra-fast simulation option, Lamarr: design and validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13213">http://arxiv.org/abs/2309.13213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucio Anderlini, Matteo Barbetti, Simone Capelli, Gloria Corti, Adam Davis, Denis Derkach, Nikita Kazeev, Artem Maevskiy, Maurizio Martinelli, Sergei Mokonenko, Benedetto Gianluca Siddi, Zehua Xu</li>
<li>for: 用于提高LHCb实验中的详细探测器模拟，以满足Run 3中的数据收集需求。</li>
<li>methods: 使用Gaudi框架，并利用深度生成模型和梯度提升决策树来 parameterize探测器响应和重建算法。</li>
<li>results: 比较详细模拟和Lamarr模拟的结果，发现Lamarr可以提供两个数量级的速度提升，同时保持与详细模拟的一致性。<details>
<summary>Abstract</summary>
Detailed detector simulation is the major consumer of CPU resources at LHCb, having used more than 90% of the total computing budget during Run 2 of the Large Hadron Collider at CERN. As data is collected by the upgraded LHCb detector during Run 3 of the LHC, larger requests for simulated data samples are necessary, and will far exceed the pledged resources of the experiment, even with existing fast simulation options. An evolution of technologies and techniques to produce simulated samples is mandatory to meet the upcoming needs of analysis to interpret signal versus background and measure efficiencies. In this context, we propose Lamarr, a Gaudi-based framework designed to offer the fastest solution for the simulation of the LHCb detector. Lamarr consists of a pipeline of modules parameterizing both the detector response and the reconstruction algorithms of the LHCb experiment. Most of the parameterizations are made of Deep Generative Models and Gradient Boosted Decision Trees trained on simulated samples or alternatively, where possible, on real data. Embedding Lamarr in the general LHCb Gauss Simulation framework allows combining its execution with any of the available generators in a seamless way. Lamarr has been validated by comparing key reconstructed quantities with Detailed Simulation. Good agreement of the simulated distributions is obtained with two-order-of-magnitude speed-up of the simulation phase.
</details>
<details>
<summary>摘要</summary>
具有详细探测器模拟功能的 Lamarr 框架，基于 Gaudi 框架，可以提供最快的 LHCb 探测器模拟解决方案。Lamarr 包含一系列模块，用于 parameterizing LHCb 实验中的探测器响应和重建算法。大多数参数化都是使用深度生成模型和梯度提升决策树，并在训练过程中使用 simulate 样本或实际数据。嵌入 Lamarr 到 LHCb Gauss Simulation 框架中，可以将其与任何可用的生成器结合使用，实现无缝的执行。Lamarr 已经得到了对 Key 重建量的验证，并与详细模拟相比，实现了两个级别的速度提升。
</details></li>
</ul>
<hr>
<h2 id="Evidential-Deep-Learning-Enhancing-Predictive-Uncertainty-Estimation-for-Earth-System-Science-Applications"><a href="#Evidential-Deep-Learning-Enhancing-Predictive-Uncertainty-Estimation-for-Earth-System-Science-Applications" class="headerlink" title="Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications"></a>Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13207">http://arxiv.org/abs/2309.13207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AI2ES/miles-guess">https://github.com/AI2ES/miles-guess</a></li>
<li>paper_authors: John S. Schreck, David John Gagne II, Charlie Becker, William E. Chapman, Kim Elmore, Gabrielle Gantos, Eliot Kim, Dhamma Kimpara, Thomas Martin, Maria J. Molina, Vanessa M. Pryzbylo, Jacob Radford, Belen Saavedra, Justin Willson, Christopher Wirz</li>
<li>for: 这个研究旨在提供一个可靠且实用的深度学习方法来量化气候和天气预测结果的不确定性。</li>
<li>methods: 这个研究使用的方法是 Parametric deep learning 和 Evidential deep learning，这两种方法可以 estimate 预测结果的不确定性，并且可以account for  both aleatoric 和 epistemic uncertainty。</li>
<li>results: 这个研究发现，使用 evidential neural networks 可以实现预测精度与 ensemble 方法相当，同时可以严谨地量化预测结果的不确定性。<details>
<summary>Abstract</summary>
Robust quantification of predictive uncertainty is critical for understanding factors that drive weather and climate outcomes. Ensembles provide predictive uncertainty estimates and can be decomposed physically, but both physics and machine learning ensembles are computationally expensive. Parametric deep learning can estimate uncertainty with one model by predicting the parameters of a probability distribution but do not account for epistemic uncertainty.. Evidential deep learning, a technique that extends parametric deep learning to higher-order distributions, can account for both aleatoric and epistemic uncertainty with one model. This study compares the uncertainty derived from evidential neural networks to those obtained from ensembles. Through applications of classification of winter precipitation type and regression of surface layer fluxes, we show evidential deep learning models attaining predictive accuracy rivaling standard methods, while robustly quantifying both sources of uncertainty. We evaluate the uncertainty in terms of how well the predictions are calibrated and how well the uncertainty correlates with prediction error. Analyses of uncertainty in the context of the inputs reveal sensitivities to underlying meteorological processes, facilitating interpretation of the models. The conceptual simplicity, interpretability, and computational efficiency of evidential neural networks make them highly extensible, offering a promising approach for reliable and practical uncertainty quantification in Earth system science modeling. In order to encourage broader adoption of evidential deep learning in Earth System Science, we have developed a new Python package, MILES-GUESS (https://github.com/ai2es/miles-guess), that enables users to train and evaluate both evidential and ensemble deep learning.
</details>
<details>
<summary>摘要</summary>
Robust量化预测uncertainty是气候和天气结果的关键因素。集合可以提供预测uncertainty估计，但物理和机器学习集合都是计算成本高的。 parametric deep learning可以通过预测概率分布的参数来估计uncertainty，但不能考虑到epistemic uncertainty。 evidential deep learning，一种扩展 parametric deep learning 到更高阶分布的技术，可以同时考虑到aleatoric和epistemic uncertainty。本研究比较了来自集合和 evidential neural network 的uncertainty。通过对冬季降水类型分类和表面层流量预测的应用，我们显示 evidential deep learning 模型可以与标准方法匹配的预测精度，同时坚定地量化两种uncertainty。我们评估预测的uncertainty，包括预测是否准确折叠和预测错误与uncertainty之间的相关性。对输入uncertainty进行分析，可以了解模型对下游气象过程的敏感性，从而更好地理解模型。 evidential neural network 的概念简单、可解释性和计算效率，使其成为可靠和实用的uncertainty量化方法。为促进 Earth System Science 中 evidential deep learning 的广泛应用，我们已经开发了一个新的 Python 包，MILES-GUESS（https://github.com/ai2es/miles-guess），它允许用户训练和评估 evidential 和集合 deep learning。
</details></li>
</ul>
<hr>
<h2 id="Federated-Short-Term-Load-Forecasting-with-Personalization-Layers-for-Heterogeneous-Clients"><a href="#Federated-Short-Term-Load-Forecasting-with-Personalization-Layers-for-Heterogeneous-Clients" class="headerlink" title="Federated Short-Term Load Forecasting with Personalization Layers for Heterogeneous Clients"></a>Federated Short-Term Load Forecasting with Personalization Layers for Heterogeneous Clients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13194">http://arxiv.org/abs/2309.13194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shourya Bose, Kibaek Kim</li>
<li>for: 这篇论文是为了提高 Federated Learning（FL）的精度和减少资料隐私问题。</li>
<li>methods: 本论文使用了Argonne Privacy-Preserving Federated Learning套件，并提出了一个专门 для处理对私页面层的Personalized Federated Learning（PL-FL）算法，以提高模型的精度。</li>
<li>results: 根据NREL ComStock资料集的实验结果显示，PL-FL算法可以提高模型的预测性能，并且可以处理各个客户的对私页面层。<details>
<summary>Abstract</summary>
The advent of smart meters has enabled pervasive collection of energy consumption data for training short-term load forecasting (STLF) models. In response to privacy concerns, federated learning (FL) has been proposed as a privacy-preserving approach for training, but the quality of trained models degrades as client data becomes heterogeneous. In this paper we alleviate this drawback using personalization layers, wherein certain layers of an STLF model in an FL framework are trained exclusively on the clients' own data. To that end, we propose a personalized FL algorithm (PL-FL) enabling FL to handle personalization layers. The PL-FL algorithm is implemented by using the Argonne Privacy-Preserving Federated Learning package. We test the forecast performance of models trained on the NREL ComStock dataset, which contains heterogeneous energy consumption data of multiple commercial buildings. Superior performance of models trained with PL-FL demonstrates that personalization layers enable classical FL algorithms to handle clients with heterogeneous data.
</details>
<details>
<summary>摘要</summary>
智能仪器的出现使得能源消耗数据进行普遍收集，用于训练短期负荷预测（STLF）模型。为了保护隐私，联邦学习（FL）被提议作为隐私保护的方法，但训练模型的质量受到客户数据的不同性的影响。在本文中，我们通过个性化层来缓解这个缺点，其中某些层在联邦学习框架中仅使用客户自己的数据进行训练。为此，我们提出了个性化联邦学习算法（PL-FL），允许联邦学习算法处理个性化层。PL-FL算法使用Argonne隐私保护联邦学习包进行实现。我们在NREL ComStock数据集上测试了由PL-FL训练的预测模型的forecast性能，该数据集包含多个商业建筑物的各种能源消耗数据。我们发现模型通过PL-FL训练显示出了superior的预测性能，这说明个性化层使得传统的联邦学习算法能够处理客户具有不同数据的情况。
</details></li>
</ul>
<hr>
<h2 id="Visualizing-Topological-Importance-A-Class-Driven-Approach"><a href="#Visualizing-Topological-Importance-A-Class-Driven-Approach" class="headerlink" title="Visualizing Topological Importance: A Class-Driven Approach"></a>Visualizing Topological Importance: A Class-Driven Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13185">http://arxiv.org/abs/2309.13185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Qin, Brittany Terese Fasy, Carola Wenk, Brian Summa</li>
<li>for: 本研究首次用图像化方法来显示数据中重要的拓扑特征，以便更好地分析和理解数据的结构。</li>
<li>methods: 本研究使用了已经证明的可解释深度学习方法，并将其应用于拓扑分类任务。这种方法可以在每个数据集中找出重要的拓扑结构，并为每个类别分配不同的权重。</li>
<li>results: 本研究通过创建 persistente point density 的重要性场来显示数据中重要的拓扑特征。这种方法可以在图像、3D 形状和医疗图像等数据上进行实际应用，并提供了真实世界中这种方法的应用示例。<details>
<summary>Abstract</summary>
This paper presents the first approach to visualize the importance of topological features that define classes of data. Topological features, with their ability to abstract the fundamental structure of complex data, are an integral component of visualization and analysis pipelines. Although not all topological features present in data are of equal importance. To date, the default definition of feature importance is often assumed and fixed. This work shows how proven explainable deep learning approaches can be adapted for use in topological classification. In doing so, it provides the first technique that illuminates what topological structures are important in each dataset in regards to their class label. In particular, the approach uses a learned metric classifier with a density estimator of the points of a persistence diagram as input. This metric learns how to reweigh this density such that classification accuracy is high. By extracting this weight, an importance field on persistent point density can be created. This provides an intuitive representation of persistence point importance that can be used to drive new visualizations. This work provides two examples: Visualization on each diagram directly and, in the case of sublevel set filtrations on images, directly on the images themselves. This work highlights real-world examples of this approach visualizing the important topological features in graph, 3D shape, and medical image data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了首先使用 topological features 来定义数据类别的方法。 topological features 拥有抽象复杂数据的基本结构的能力，因此是数据可视化和分析管道中的一个重要组成部分。although not all topological features in data are of equal importance. Until now, the default definition of feature importance has been often assumed and fixed. This work shows how proven explainable deep learning approaches can be adapted for use in topological classification. In doing so, it provides the first technique that illuminates what topological structures are important in each dataset in regards to their class label. In particular, the approach uses a learned metric classifier with a density estimator of the points of a persistence diagram as input. This metric learns how to reweigh this density such that classification accuracy is high. By extracting this weight, an importance field on persistent point density can be created. This provides an intuitive representation of persistence point importance that can be used to drive new visualizations. This work provides two examples: Visualization on each diagram directly and, in the case of sublevel set filtrations on images, directly on the images themselves. This work highlights real-world examples of this approach visualizing the important topological features in graph, 3D shape, and medical image data.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Multi-Objective-Optimization-through-Machine-Learning-Supported-Multiphysics-Simulation"><a href="#Enhancing-Multi-Objective-Optimization-through-Machine-Learning-Supported-Multiphysics-Simulation" class="headerlink" title="Enhancing Multi-Objective Optimization through Machine Learning-Supported Multiphysics Simulation"></a>Enhancing Multi-Objective Optimization through Machine Learning-Supported Multiphysics Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13179">http://arxiv.org/abs/2309.13179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Botache, Jens Decke, Winfried Ripken, Abhinay Dornipati, Franz Götz-Hahn, Mohamed Ayeb, Bernhard Sick</li>
<li>for: 这篇论文旨在提出一个方法ological framework для快速化多物理 simulations，以满足多个目标的优化。</li>
<li>methods: 这篇论文使用了两种机器学习和深度学习算法，以及两种优化算法，并将其组合成一个完整的训练和优化管线。</li>
<li>results: 经过实验和评估，这篇论文发现可以使用相对少量的数据来训练高精度的代理模型，并且可以快速地获得多个目标的Pareto优化结果。<details>
<summary>Abstract</summary>
Multiphysics simulations that involve multiple coupled physical phenomena quickly become computationally expensive. This imposes challenges for practitioners aiming to find optimal configurations for these problems satisfying multiple objectives, as optimization algorithms often require querying the simulation many times. This paper presents a methodological framework for training, self-optimizing, and self-organizing surrogate models to approximate and speed up Multiphysics simulations. We generate two real-world tabular datasets, which we make publicly available, and show that surrogate models can be trained on relatively small amounts of data to approximate the underlying simulations accurately. We conduct extensive experiments combining four machine learning and deep learning algorithms with two optimization algorithms and a comprehensive evaluation strategy. Finally, we evaluate the performance of our combined training and optimization pipeline by verifying the generated Pareto-optimal results using the ground truth simulations. We also employ explainable AI techniques to analyse our surrogates and conduct a preselection strategy to determine the most relevant features in our real-world examples. This approach lets us understand the underlying problem and identify critical partial dependencies.
</details>
<details>
<summary>摘要</summary>
多物理 simulate 快速增加计算成本，这会对实践者们的优化问题提出挑战，因为优化算法通常需要对 simulate 进行多次查询。这篇论文提出了一种方法ológical framework для训练、自动优化和自动组织替身模型，以加速多物理 simulate。我们生成了两个实际世界的表格数据集，并证明了替身模型可以通过相对小量数据来准确地表示下面 simulate。我们在多种机器学习和深度学习算法和两种优化算法的基础上进行了广泛的实验。最后，我们使用了可解释 AI 技术来分析我们的替身和采用预选策略来确定实际世界中最重要的特征。这种方法让我们理解下面的问题，并识别 kritical partial dependencies。
</details></li>
</ul>
<hr>
<h2 id="Invisible-Watermarking-for-Audio-Generation-Diffusion-Models"><a href="#Invisible-Watermarking-for-Audio-Generation-Diffusion-Models" class="headerlink" title="Invisible Watermarking for Audio Generation Diffusion Models"></a>Invisible Watermarking for Audio Generation Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13166">http://arxiv.org/abs/2309.13166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mikiyaxi/watermark-audio-diffusion">https://github.com/mikiyaxi/watermark-audio-diffusion</a></li>
<li>paper_authors: Xirong Cao, Xiang Li, Divyesh Jadav, Yanzhao Wu, Zhehui Chen, Chen Zeng, Wenqi Wei</li>
<li>for: 保护音频扩散模型的 интеграITY和数据权益</li>
<li>methods: 基于mel-spectrogram的音频扩散模型 watermarking技术</li>
<li>results: 实现了不可见水印触发机制，保护模型的有Integrity和数据权益，同时仍能够保持高效的净音频生成能力。<details>
<summary>Abstract</summary>
Diffusion models have gained prominence in the image domain for their capabilities in data generation and transformation, achieving state-of-the-art performance in various tasks in both image and audio domains. In the rapidly evolving field of audio-based machine learning, safeguarding model integrity and establishing data copyright are of paramount importance. This paper presents the first watermarking technique applied to audio diffusion models trained on mel-spectrograms. This offers a novel approach to the aforementioned challenges. Our model excels not only in benign audio generation, but also incorporates an invisible watermarking trigger mechanism for model verification. This watermark trigger serves as a protective layer, enabling the identification of model ownership and ensuring its integrity. Through extensive experiments, we demonstrate that invisible watermark triggers can effectively protect against unauthorized modifications while maintaining high utility in benign audio generation tasks.
</details>
<details>
<summary>摘要</summary>
各种扩散模型在图像领域中得到了广泛应用，以其数据生成和转换能力为特点，在图像和音频领域中实现了状态 искусственный机器学习的最佳性能。在快速发展的音频基于机器学习领域中，保护模型完整性和确立数据版权是核心问题。本文提出了首个应用于音频扩散模型训练的mel-spectrogram watermarking技术。这提供了一种新的方法来解决以上问题。我们的模型不仅在正常的音频生成任务中表现出色，还包含了隐藏的 watermarking 触发器机制，以确保模型的完整性和版权。通过广泛的实验，我们证明了隐藏的 watermark 触发器可以有效地保护 against 未授权修改，同时保持高的用于正常音频生成任务的实用性。
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Response-to-Treatment-with-Global-Deep-Learning-and-Patient-Specific-Pharmacokinetic-Priors"><a href="#Forecasting-Response-to-Treatment-with-Global-Deep-Learning-and-Patient-Specific-Pharmacokinetic-Priors" class="headerlink" title="Forecasting Response to Treatment with Global Deep Learning and Patient-Specific Pharmacokinetic Priors"></a>Forecasting Response to Treatment with Global Deep Learning and Patient-Specific Pharmacokinetic Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13135">http://arxiv.org/abs/2309.13135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Willa Potosnak, Cristian Challu, Kin G. Olivares, Artur Dubrawski</li>
<li>for: 预测医疗时序数据，以早发现不良结果和监测病人状况。</li>
<li>methods: 提议一种新的混合全局-本地架构和药理学编码器，用于深入了解患者特定的治疗效应。</li>
<li>results: 对比patient-specific模型，全局-本地架构提高了9.2-14.6%的准确率；对比alternative编码技术，药理学编码器在模拟数据上提高了4.4%，在实际数据上提高了2.1%。<details>
<summary>Abstract</summary>
Forecasting healthcare time series is crucial for early detection of adverse outcomes and for patient monitoring. Forecasting, however, can be difficult in practice due to noisy and intermittent data. The challenges are often exacerbated by change points induced via extrinsic factors, such as the administration of medication. To address these challenges, we propose a novel hybrid global-local architecture and a pharmacokinetic encoder that informs deep learning models of patient-specific treatment effects. We showcase the efficacy of our approach in achieving significant accuracy gains for a blood glucose forecasting task using both realistically simulated and real-world data. Our global-local architecture improves over patient-specific models by 9.2-14.6%. Additionally, our pharmacokinetic encoder improves over alternative encoding techniques by 4.4% on simulated data and 2.1% on real-world data. The proposed approach can have multiple beneficial applications in clinical practice, such as issuing early warnings about unexpected treatment responses, or helping to characterize patient-specific treatment effects in terms of drug absorption and elimination characteristics.
</details>
<details>
<summary>摘要</summary>
预测医疗时序数据是重要的，可以早期检测不良结果并跟踪病人。然而，在实践中预测可能会困难，因为数据充满噪音和中断。这些挑战通常由外部因素引起的变换点加剧，如药物的给药。为了解决这些挑战，我们提议一种新的全球-本地架构和一种用于深度学习模型的药物生物学编码器。我们在血糖预测任务中使用这种方法，并使用真实的 simulated 数据和实际数据进行比较。我们的全球-本地架构在patient-specific模型的基础上提高了9.2-14.6%的准确率。此外，我们的药物生物学编码器在 simulated 数据上比替代编码技术提高4.4%，并在实际数据上提高2.1%。我们的方法可以在临床实践中有多个有利应用，如发现不ждан的治疗反应，或者帮助characterize patient-specific treatment effects in terms of drug absorption and elimination characteristics。
</details></li>
</ul>
<hr>
<h2 id="AntiBARTy-Diffusion-for-Property-Guided-Antibody-Design"><a href="#AntiBARTy-Diffusion-for-Property-Guided-Antibody-Design" class="headerlink" title="AntiBARTy Diffusion for Property Guided Antibody Design"></a>AntiBARTy Diffusion for Property Guided Antibody Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13129">http://arxiv.org/abs/2309.13129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Venderley</li>
<li>for: 这 paper 是为了探讨用 machine learning 技术来设计和工程抗体的可能性。</li>
<li>methods: 这 paper 使用了一种基于 BART 的语言模型，以及一种基于这种语言模型的扩散模型来导向 IgG 抗体的 de novo 设计。</li>
<li>results: 这 paper 的实验结果表明，可以使用这种方法来生成具有改进的在silico 稳定性的新抗体，同时保持抗体的有效性和序列多样性。<details>
<summary>Abstract</summary>
Over the past decade, antibodies have steadily grown in therapeutic importance thanks to their high specificity and low risk of adverse effects compared to other drug modalities. While traditional antibody discovery is primarily wet lab driven, the rapid improvement of ML-based generative modeling has made in-silico approaches an increasingly viable route for discovery and engineering. To this end, we train an antibody-specific language model, AntiBARTy, based on BART (Bidirectional and Auto-Regressive Transformer) and use its latent space to train a property-conditional diffusion model for guided IgG de novo design. As a test case, we show that we can effectively generate novel antibodies with improved in-silico solubility while maintaining antibody validity and controlling sequence diversity.
</details>
<details>
<summary>摘要</summary>
过去十年，抗体在治疗方面的重要性逐渐增长，主要归功于它们的高特异性和其他药物modalities相比的低风险。而传统抗体发现主要是在湿lab中进行，但随着机器学习（ML）基于生成模型的快速进步，在硬件上进行的方法在抗体发现和工程方面变得越来越有前途。为此，我们训练了一个抗体特有的语言模型 AntiBARTy，基于BART（双向自适应变换器），并使用其潜在空间来训练一个基于属性的扩散模型，用于导引IgG de novo设计。作为一个测试案例，我们显示了我们可以效果地生成改进了室内溶解性的新抗体，同时保持抗体有效性和控制序列多样性。
</details></li>
</ul>
<hr>
<h2 id="Data-is-often-loadable-in-short-depth-Quantum-circuits-from-tensor-networks-for-finance-images-fluids-and-proteins"><a href="#Data-is-often-loadable-in-short-depth-Quantum-circuits-from-tensor-networks-for-finance-images-fluids-and-proteins" class="headerlink" title="Data is often loadable in short depth: Quantum circuits from tensor networks for finance, images, fluids, and proteins"></a>Data is often loadable in short depth: Quantum circuits from tensor networks for finance, images, fluids, and proteins</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13108">http://arxiv.org/abs/2309.13108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raghav Jumade, Nicolas PD Sawaya</li>
<li>For: This paper addresses the “input problem” of loading classical data into a quantum computer, which has been an obstacle to achieving quantum advantage.* Methods: The paper introduces a circuit compilation method based on tensor network (TN) theory, called AMLET (Automatic Multi-layer Loader Exploiting TNs), which can be tailored to arbitrary circuit depths.* Results: The paper performs numerical experiments on real-world classical data from four distinct areas and shows that the required circuit depths are often several orders of magnitude lower than the exponentially-scaling general loading algorithm would require. This demonstrates that many classical datasets can be loaded into a quantum computer in much shorter depth than previously expected, which has positive implications for speeding up classical workloads on quantum computers.<details>
<summary>Abstract</summary>
Though there has been substantial progress in developing quantum algorithms to study classical datasets, the cost of simply loading classical data is an obstacle to quantum advantage. When the amplitude encoding is used, loading an arbitrary classical vector requires up to exponential circuit depths with respect to the number of qubits. Here, we address this ``input problem'' with two contributions. First, we introduce a circuit compilation method based on tensor network (TN) theory. Our method -- AMLET (Automatic Multi-layer Loader Exploiting TNs) -- proceeds via careful construction of a specific TN topology and can be tailored to arbitrary circuit depths. Second, we perform numerical experiments on real-world classical data from four distinct areas: finance, images, fluid mechanics, and proteins. To the best of our knowledge, this is the broadest numerical analysis to date of loading classical data into a quantum computer. Consistent with other recent work in this area, the required circuit depths are often several orders of magnitude lower than the exponentially-scaling general loading algorithm would require. Besides introducing a more efficient loading algorithm, this work demonstrates that many classical datasets are loadable in depths that are much shorter than previously expected, which has positive implications for speeding up classical workloads on quantum computers.
</details>
<details>
<summary>摘要</summary>
尽管在开发量子算法研究类别数据上已经取得了重要进展，但是将类别数据加载到量子计算机上的成本仍然是一个障碍物，以致于实现量子优势。当使用振荡编码时，将任意类别数据加载到多个量子比特（qubit）上可能需要对数量积累的循环深度。在这里，我们提出了两项贡献以解决这个“输入问题”。首先，我们基于张量网络（TN）理论开发了一种简单的练习方法，称之为自动多层加载器（AMLET）。我们的方法通过精心构建特定的TN结构，可以适应任意循环深度。其次，我们在实际的类别数据上进行了数值实验，来评估加载类别数据到量子计算机上的可能性。我们的实验结果表明，可以在循环深度上下文中加载类别数据，而不需要遵循普通的循环深度级数。此外，这项工作还证明了许多类别数据可以在循环深度上下文中加载，这意味着可以通过加速类别工作来减轻量子计算机上的工作负担。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Network-for-Stress-Predictions-in-Stiffened-Panels-Under-Uniform-Loading"><a href="#Graph-Neural-Network-for-Stress-Predictions-in-Stiffened-Panels-Under-Uniform-Loading" class="headerlink" title="Graph Neural Network for Stress Predictions in Stiffened Panels Under Uniform Loading"></a>Graph Neural Network for Stress Predictions in Stiffened Panels Under Uniform Loading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13022">http://arxiv.org/abs/2309.13022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuecheng Cai, Jasmin Jelovica</li>
<li>for: 本研究旨在提出一种novel的图形嵌入技术，用于高效地表示3D厚度板的强度分布。</li>
<li>methods: 本研究使用了Graph Sampling and Aggregation（GraphSAGE）技术，并 comparing withfinite-element-vertex图表示方法。</li>
<li>results: 研究结果表明，使用提议的图形嵌入方法可以更加准确地预测3D厚度板的强度分布，并且可以快速地对不同结构 Parametric study。<details>
<summary>Abstract</summary>
Machine learning (ML) and deep learning (DL) techniques have gained significant attention as reduced order models (ROMs) to computationally expensive structural analysis methods, such as finite element analysis (FEA). Graph neural network (GNN) is a particular type of neural network which processes data that can be represented as graphs. This allows for efficient representation of complex geometries that can change during conceptual design of a structure or a product. In this study, we propose a novel graph embedding technique for efficient representation of 3D stiffened panels by considering separate plate domains as vertices. This approach is considered using Graph Sampling and Aggregation (GraphSAGE) to predict stress distributions in stiffened panels with varying geometries. A comparison between a finite-element-vertex graph representation is conducted to demonstrate the effectiveness of the proposed approach. A comprehensive parametric study is performed to examine the effect of structural geometry on the prediction performance. Our results demonstrate the immense potential of graph neural networks with the proposed graph embedding method as robust reduced-order models for 3D structures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Brain-Age-Revisited-Investigating-the-State-vs-Trait-Hypotheses-of-EEG-derived-Brain-Age-Dynamics-with-Deep-Learning"><a href="#Brain-Age-Revisited-Investigating-the-State-vs-Trait-Hypotheses-of-EEG-derived-Brain-Age-Dynamics-with-Deep-Learning" class="headerlink" title="Brain Age Revisited: Investigating the State vs. Trait Hypotheses of EEG-derived Brain-Age Dynamics with Deep Learning"></a>Brain Age Revisited: Investigating the State vs. Trait Hypotheses of EEG-derived Brain-Age Dynamics with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07029">http://arxiv.org/abs/2310.07029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gemeinl/eeg-brain-age">https://github.com/gemeinl/eeg-brain-age</a></li>
<li>paper_authors: Lukas AW Gemein, Robin T Schirrmeister, Joschka Boedecker, Tonio Ball<br>for:* The paper aims to investigate the relationship between brain age and brain pathology using clinical EEG recordings.methods:* The authors use a state-of-the-art Temporal Convolutional Network (TCN) for age regression, and train the model on recordings from the Temple University Hospital EEG Corpus (TUEG) with explicit labels for non-pathological and pathological recordings.results:* The TCN achieves state-of-the-art performance in age decoding with a mean absolute error of 6.6 years.* The authors find that the brain age gap biomarker is not indicative of pathological EEG, and that the model significantly underestimates the age of non-pathological and pathological subjects.<details>
<summary>Abstract</summary>
The brain's biological age has been considered as a promising candidate for a neurologically significant biomarker. However, recent results based on longitudinal magnetic resonance imaging data have raised questions on its interpretation. A central question is whether an increased biological age of the brain is indicative of brain pathology and if changes in brain age correlate with diagnosed pathology (state hypothesis). Alternatively, could the discrepancy in brain age be a stable characteristic unique to each individual (trait hypothesis)? To address this question, we present a comprehensive study on brain aging based on clinical EEG, which is complementary to previous MRI-based investigations. We apply a state-of-the-art Temporal Convolutional Network (TCN) to the task of age regression. We train on recordings of the Temple University Hospital EEG Corpus (TUEG) explicitly labeled as non-pathological and evaluate on recordings of subjects with non-pathological as well as pathological recordings, both with examinations at a single point in time and repeated examinations over time. Therefore, we created four novel subsets of TUEG that include subjects with multiple recordings: I) all labeled non-pathological; II) all labeled pathological; III) at least one recording labeled non-pathological followed by at least one recording labeled pathological; IV) similar to III) but with opposing transition (first pathological then non-pathological). The results show that our TCN reaches state-of-the-art performance in age decoding with a mean absolute error of 6.6 years. Our extensive analyses demonstrate that the model significantly underestimates the age of non-pathological and pathological subjects (-1 and -5 years, paired t-test, p <= 0.18 and p <= 0.0066). Furthermore, the brain age gap biomarker is not indicative of pathological EEG.
</details>
<details>
<summary>摘要</summary>
研究人员认为大脑的生物龄可能是脑科学中的一个有价值的生物标志物。然而，最近的长期磁共振成像数据显示了解释问题。我们的中心问题是大脑生物龄是脑病学的指标吗，而且改变大脑生物龄与诊断病理相关吗（状态假设）？或者这些差异是每个人的稳定特征吗（性 trait假设）？为了回答这个问题，我们提供了一项全面的大脑老化研究，基于临床EEG。我们使用了当今最佳的时间卷积神经网络（TCN）进行年龄预测任务。我们在记录了普通大学医院EEG资料库（TUEG）的非病理记录上进行训练，并对记录了非病理和病理记录的评估。因此，我们创建了四个新的TUEG子集：I) 所有非病理记录; II) 所有病理记录; III) 至少有一个非病理记录，后跟至少一个病理记录; IV) 与III相似，但具有反向转变（先病理然后非病理）。结果显示，我们的TCN达到了当今最佳性能水平，年龄预测的绝对误差为6.6年。我们进行了广泛的分析，发现TCN对非病理和病理subject下都有显著下降（-1和-5年，paired t-test，p<=0.18和p<=0.0066）。此外，大脑生物龄差异标志物并不是诊断EEG的病理指标。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Deep-Gradient-Leakage-via-Inversion-Influence-Functions"><a href="#Understanding-Deep-Gradient-Leakage-via-Inversion-Influence-Functions" class="headerlink" title="Understanding Deep Gradient Leakage via Inversion Influence Functions"></a>Understanding Deep Gradient Leakage via Inversion Influence Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13016">http://arxiv.org/abs/2309.13016</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/illidanlab/inversion-influence-function">https://github.com/illidanlab/inversion-influence-function</a></li>
<li>paper_authors: Haobo Zhang, Junyuan Hong, Yuyang Deng, Mehrdad Mahdavi, Jiayu Zhou</li>
<li>for: 防止分布式学习中的隐私泄露，尤其是在客户端存储敏感数据时。</li>
<li>methods: 提出了一种新的倒影影响函数(I$^2$F)，通过减少隐私泄露，为分布式学习提供了一种可扩展的解决方案。</li>
<li>results: 在不同的网络架构、数据集、攻击实现和干扰防御方法下，I$^2$F有效地预测了潜在的隐私泄露。 codes are provided in <a target="_blank" rel="noopener" href="https://github.com/illidanlab/inversion-influence-function">https://github.com/illidanlab/inversion-influence-function</a>.<details>
<summary>Abstract</summary>
Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors. This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients. Defending against such attacks requires but lacks an understanding of when and how privacy leakage happens, mostly because of the black-box nature of deep networks. In this paper, we propose a novel Inversion Influence Function (I$^2$F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products. We empirically demonstrate that I$^2$F effectively approximated the DGL generally on different model architectures, datasets, attack implementations, and noise-based defenses. With this novel tool, we provide insights into effective gradient perturbation directions, the unfairness of privacy protection, and privacy-preferred model initialization. Our codes are provided in https://github.com/illidanlab/inversion-influence-function.
</details>
<details>
<summary>摘要</summary>
深度梯度泄露（DGL）是一种非常有效的攻击，可以从梯度向量中提取私人训练图像。这种攻击对于分布式学习从客户端进行训练的数据进行了重大隐私挑战，因为客户端需要共享梯度。防止这种攻击需要一个深入了解梯度泄露发生的时间和方式，但是由于深度网络的黑盒特性，这种理解很困难。在这篇论文中，我们提出了一种新的反向影响函数（I$^2$F），它可以通过解决DGL问题来建立私人梯度和 recovered图像之间的关系。与直接解决DGL相比，I$^2$F是可扩展的，只需要对梯度和Jacobian-vector产品进行 oracle 访问即可。我们通过实验表明，I$^2$F可以有效地适应不同的网络架构、数据集、攻击实现和噪声防御。通过这个新工具，我们提供了关于有效梯度扰动方向、隐私保护不公平性和隐私首选模型初始化的新视角。我们的代码可以在https://github.com/illidanlab/inversion-influence-function中找到。
</details></li>
</ul>
<hr>
<h2 id="Importance-of-Smoothness-Induced-by-Optimizers-in-FL4ASR-Towards-Understanding-Federated-Learning-for-End-to-End-ASR"><a href="#Importance-of-Smoothness-Induced-by-Optimizers-in-FL4ASR-Towards-Understanding-Federated-Learning-for-End-to-End-ASR" class="headerlink" title="Importance of Smoothness Induced by Optimizers in FL4ASR: Towards Understanding Federated Learning for End-to-End ASR"></a>Importance of Smoothness Induced by Optimizers in FL4ASR: Towards Understanding Federated Learning for End-to-End ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13102">http://arxiv.org/abs/2309.13102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheikh Shams Azam, Tatiana Likhomanenko, Martin Pelikan, Jan “Honza” Silovsky</li>
<li>For: 本研究使用 Federated Learning (FL) 技术来训练 End-to-End 语音识别 (ASR) 模型，并研究如何减少 word error rate  между FL 模型和中央化训练模型之间的性能差距。* Methods: 本研究考虑了多个因素，包括适应优化器、 Connectionist Temporal Classification (CTC) 权重的变化、模型初始化方法、将中央化训练经验应用到 FL 中、FL 特有的hyperparameter 等，以探讨如何在 ASR 下面 heterogeneous data distribution 中实现更好的性能。* Results: 研究发现一些优化器可以更好地适应 FL 环境，并且在不同的Client sample size 和学习率调度器下进行了详细的分析。此外，本研究还总结了以前的相关研究中的算法、趋势和最佳实践，以便在 FL 中实现更好的 ASR 性能。<details>
<summary>Abstract</summary>
In this paper, we start by training End-to-End Automatic Speech Recognition (ASR) models using Federated Learning (FL) and examining the fundamental considerations that can be pivotal in minimizing the performance gap in terms of word error rate between models trained using FL versus their centralized counterpart. Specifically, we study the effect of (i) adaptive optimizers, (ii) loss characteristics via altering Connectionist Temporal Classification (CTC) weight, (iii) model initialization through seed start, (iv) carrying over modeling setup from experiences in centralized training to FL, e.g., pre-layer or post-layer normalization, and (v) FL-specific hyperparameters, such as number of local epochs, client sampling size, and learning rate scheduler, specifically for ASR under heterogeneous data distribution. We shed light on how some optimizers work better than others via inducing smoothness. We also summarize the applicability of algorithms, trends, and propose best practices from prior works in FL (in general) toward End-to-End ASR models.
</details>
<details>
<summary>摘要</summary>
在本文中，我们开始由使用联合学习（Federated Learning，FL）训练端到端自动语音识别（ASR）模型，并探讨在减少中心化训练模型和FL模型之间性能差距方面的基本考虑因素。我们专注于以下五个方面：（i）适应性优化器，（ii）修改连接主义时间分类（CTC）重量，（iii）模型初始化通过种子开始，（iv）从中心化训练经验中提取模型设置，例如前层或后层正则化，（v）FL特有的超参数，如本地环节数、客户端抽样大小和学习率调度器。我们解释了一些优化器如何通过减少缓动性来工作更好。我们还总结了先前的FL研究中对端到端ASR模型的算法、趋势和最佳实践。
</details></li>
</ul>
<hr>
<h2 id="Expressive-variational-quantum-circuits-provide-inherent-privacy-in-federated-learning"><a href="#Expressive-variational-quantum-circuits-provide-inherent-privacy-in-federated-learning" class="headerlink" title="Expressive variational quantum circuits provide inherent privacy in federated learning"></a>Expressive variational quantum circuits provide inherent privacy in federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13002">http://arxiv.org/abs/2309.13002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niraj Kumar, Jamie Heredge, Changhao Li, Shaltiel Eloul, Shree Hari Sureshbabu, Marco Pistoia</li>
<li>for: 这个论文目的是提出一种基于量子机器学习模型的联合学习方法，以保护数据隐私。</li>
<li>methods: 这个论文使用了变量量子环境模型，并利用表达性编码映射和过参数化 Ansatz 来保护数据隐私。</li>
<li>results: 论文表明，使用变量量子环境模型可以避免数据泄露，并且在各种攻击模型下保持模型训练可能性。<details>
<summary>Abstract</summary>
Federated learning has emerged as a viable distributed solution to train machine learning models without the actual need to share data with the central aggregator. However, standard neural network-based federated learning models have been shown to be susceptible to data leakage from the gradients shared with the server. In this work, we introduce federated learning with variational quantum circuit model built using expressive encoding maps coupled with overparameterized ans\"atze. We show that expressive maps lead to inherent privacy against gradient inversion attacks, while overparameterization ensures model trainability. Our privacy framework centers on the complexity of solving the system of high-degree multivariate Chebyshev polynomials generated by the gradients of quantum circuit. We present compelling arguments highlighting the inherent difficulty in solving these equations, both in exact and approximate scenarios. Additionally, we delve into machine learning-based attack strategies and establish a direct connection between overparameterization in the original federated learning model and underparameterization in the attack model. Furthermore, we provide numerical scaling arguments showcasing that underparameterization of the expressive map in the attack model leads to the loss landscape being swamped with exponentially many spurious local minima points, thus making it extremely hard to realize a successful attack. This provides a strong claim, for the first time, that the nature of quantum machine learning models inherently helps prevent data leakage in federated learning.
</details>
<details>
<summary>摘要</summary>
Federated learning 已经出现为一种可行的分布式解决方案，用于在没有实际分享数据的情况下训练机器学习模型。然而，标准的神经网络基本的 federated learning 模型已经被证明容易受到数据泄露的威胁，即通过分享梯度来泄露数据。在这种情况下，我们介绍了使用表达式编码映射和过参数 Ansatz 构建的 federated learning 模型。我们表明了表达式编码映射会带来自然的隐私保护，而过参数 Ansatz 可以保证模型可训练。我们的隐私框架基于解决由梯度生成的高阶多变量Chebychev多项式系统的复杂性。我们提供了吸引人的论述，证明在正确和近似情况下解决这些方程是非常困难的。此外，我们还探讨了机器学习基于攻击策略，并证明了过参数化在原始 federated learning 模型中的下降会导致攻击模型下降。最后，我们提供了数学Scaling 理论，表明在攻击模型中下降过参数化会导致搜索空间拥有infiniti多个假的本地最优点，因此非常难实现成功攻击。这提供了一个强有力的证明，即 quantum machine learning 模型的本质带来了防止数据泄露的隐私保护。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-probability-flows-and-entropy-production-rates-in-active-matter"><a href="#Deep-learning-probability-flows-and-entropy-production-rates-in-active-matter" class="headerlink" title="Deep learning probability flows and entropy production rates in active matter"></a>Deep learning probability flows and entropy production rates in active matter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12991">http://arxiv.org/abs/2309.12991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas M. Boffi, Eric Vanden-Eijnden</li>
<li>for: 这 paper 是为了理解 nonequilibrium 状态下 active matter 系统的性质。</li>
<li>methods: 这 paper 使用了 deep learning 方法来计算 entropy production rate 和 probability current。</li>
<li>results: 这 paper 得到了一种可以 direct access 到 entropy production rate 和 probability current 的方法，并且可以分解成各个个体、空间区域和自由度的地方贡献。<details>
<summary>Abstract</summary>
Active matter systems, from self-propelled colloids to motile bacteria, are characterized by the conversion of free energy into useful work at the microscopic scale. These systems generically involve physics beyond the reach of equilibrium statistical mechanics, and a persistent challenge has been to understand the nature of their nonequilibrium states. The entropy production rate and the magnitude of the steady-state probability current provide quantitative ways to do so by measuring the breakdown of time-reversal symmetry and the strength of nonequilibrium transport of measure. Yet, their efficient computation has remained elusive, as they depend on the system's unknown and high-dimensional probability density. Here, building upon recent advances in generative modeling, we develop a deep learning framework that estimates the score of this density. We show that the score, together with the microscopic equations of motion, gives direct access to the entropy production rate, the probability current, and their decomposition into local contributions from individual particles, spatial regions, and degrees of freedom. To represent the score, we introduce a novel, spatially-local transformer-based network architecture that learns high-order interactions between particles while respecting their underlying permutation symmetry. We demonstrate the broad utility and scalability of the method by applying it to several high-dimensional systems of interacting active particles undergoing motility-induced phase separation (MIPS). We show that a single instance of our network trained on a system of 4096 particles at one packing fraction can generalize to other regions of the phase diagram, including systems with as many as 32768 particles. We use this observation to quantify the spatial structure of the departure from equilibrium in MIPS as a function of the number of particles and the packing fraction.
</details>
<details>
<summary>摘要</summary>
活的物质系统，从自驱动溶液到运动细菌，通常表现为在微观尺度上将自由能转化为有用的劳动。这些系统通常包括物理现象超出平衡统计力学的范畴，因此理解其非平衡状态的性质是一个挑战。生成热量率和稳态概率流的大小都是量化Nonequilibrium状态的指标，它们取决于系统的未知和高维度概率密度。在这里，我们基于最近的生成模型技术，开发了一种深度学习框架，可以估算概率密度的分数。我们证明，这个分数，与微观运动方程相结合，可以直接访问生成热量率、稳态概率流和它们的分解为个体粒子、空间区域和自由度的本地贡献。为表示分数，我们引入了一种新的、空间地本符论基于网络架构，可以学习高阶相互作用 между粒子，同时尊重它们的基本卷积共轭性。我们在应用这种方法于多种高维度相互作用的活跃粒子系统时，发现这种方法可以泛化到其他频谱 диаграм中，包括系统中的4096个粒子。我们用这个观察来量化离散于MIPS中的空间结构，并与粒子数和压力 fraction有关。
</details></li>
</ul>
<hr>
<h2 id="BayesDLL-Bayesian-Deep-Learning-Library"><a href="#BayesDLL-Bayesian-Deep-Learning-Library" class="headerlink" title="BayesDLL: Bayesian Deep Learning Library"></a>BayesDLL: Bayesian Deep Learning Library</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12928">http://arxiv.org/abs/2309.12928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/samsunglabs/bayesdll">https://github.com/samsunglabs/bayesdll</a></li>
<li>paper_authors: Minyoung Kim, Timothy Hospedales</li>
<li>for: 这份论文是为了描述一个基于PyTorch的泛型概率神经网络库，用于处理大规模深度网络。</li>
<li>methods: 这个库实现了主流的approximate Bayesian推理算法，包括变分推理、MC-dropout、渐进MCMC和拉пла斯方法。</li>
<li>results: 与其他现有的Bayesian神经网络库相比，这个库可以处理非常大的深度网络，包括视transformer（ViTs）。此外，用户无需编写任何代码修改，可以直接使用现有的backbone网络定义代码。最后，这个库还允许使用预训练模型的权重作为先验均值，这非常有用于使用大规模基础模型如ViTs进行Bayesian推理，这些模型难以从scratch使用下游数据进行优化。<details>
<summary>Abstract</summary>
We release a new Bayesian neural network library for PyTorch for large-scale deep networks. Our library implements mainstream approximate Bayesian inference algorithms: variational inference, MC-dropout, stochastic-gradient MCMC, and Laplace approximation. The main differences from other existing Bayesian neural network libraries are as follows: 1) Our library can deal with very large-scale deep networks including Vision Transformers (ViTs). 2) We need virtually zero code modifications for users (e.g., the backbone network definition codes do not neet to be modified at all). 3) Our library also allows the pre-trained model weights to serve as a prior mean, which is very useful for performing Bayesian inference with the large-scale foundation models like ViTs that are hard to optimise from scratch with the downstream data alone. Our code is publicly available at: \url{https://github.com/SamsungLabs/BayesDLL}\footnote{A mirror repository is also available at: \url{https://github.com/minyoungkim21/BayesDLL}.}.
</details>
<details>
<summary>摘要</summary>
我们发布了一个基于PyTorch的抽象概率神经网络库，用于大规模深度网络。我们的库实现了主流的抽象概率推理算法：变量推理、MC-dropout、随机梯度MCMC和拉пла斯投影。与其他现有的概率神经网络库相比，我们的库具有以下主要优势：1. 我们的库可以处理非常大的深度网络，包括视Transformer（ViTs）。2. 用户没需要修改代码（例如，后ION网络定义代码不需要修改）。3. 我们的库还允许预训练模型的权重服为先验均值，这对于使用大规模基础模型如ViTs进行概率推理非常有用，这些模型难以从头开始使用下游数据进行优化。我们的代码公共可用于：<https://github.com/SamsungLabs/BayesDLL>（备用存储库：<https://github.com/minyoungkim21/BayesDLL>）。
</details></li>
</ul>
<hr>
<h2 id="Topological-Data-Mapping-of-Online-Hate-Speech-Misinformation-and-General-Mental-Health-A-Large-Language-Model-Based-Study"><a href="#Topological-Data-Mapping-of-Online-Hate-Speech-Misinformation-and-General-Mental-Health-A-Large-Language-Model-Based-Study" class="headerlink" title="Topological Data Mapping of Online Hate Speech, Misinformation, and General Mental Health: A Large Language Model Based Study"></a>Topological Data Mapping of Online Hate Speech, Misinformation, and General Mental Health: A Large Language Model Based Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13098">http://arxiv.org/abs/2309.13098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Alexander, Hongbin Wang</li>
<li>for: 这项研究旨在了解社交媒体上的仇恨言论和谣言对poster的心理健康造成的影响。</li>
<li>methods: 研究使用OpenAI的GPT3 derivateposts的嵌入，并通过机器学习分类来理解仇恨言论&#x2F;谣言在不同社区中的角色。</li>
<li>results: 研究发现仇恨言论&#x2F;谣言与心理疾病之间存在紧密的关系，并通过图形分析获得了在线仇恨言论&#x2F;谣言与心理健康之间的视觉地图。<details>
<summary>Abstract</summary>
The advent of social media has led to an increased concern over its potential to propagate hate speech and misinformation, which, in addition to contributing to prejudice and discrimination, has been suspected of playing a role in increasing social violence and crimes in the United States. While literature has shown the existence of an association between posting hate speech and misinformation online and certain personality traits of posters, the general relationship and relevance of online hate speech/misinformation in the context of overall psychological wellbeing of posters remain elusive. One difficulty lies in the lack of adequate data analytics tools capable of adequately analyzing the massive amount of social media posts to uncover the underlying hidden links. Recent progresses in machine learning and large language models such as ChatGPT have made such an analysis possible. In this study, we collected thousands of posts from carefully selected communities on the social media site Reddit. We then utilized OpenAI's GPT3 to derive embeddings of these posts, which are high-dimensional real-numbered vectors that presumably represent the hidden semantics of posts. We then performed various machine-learning classifications based on these embeddings in order to understand the role of hate speech/misinformation in various communities. Finally, a topological data analysis (TDA) was applied to the embeddings to obtain a visual map connecting online hate speech, misinformation, various psychiatric disorders, and general mental health.
</details>
<details>
<summary>摘要</summary>
“社交媒体的出现引发了对其可能传播仇恨言论和谎言的担忧，这些言论可能导致人们偏见和歧视，并被怀疑与社会暴力和犯罪之间存在关系。虽然文献表明在线仇恨言论和谎言与发帖者的个人特征有关，但全面的心理健康和发帖者的关系还未得到了解。一个问题在于分析大量社交媒体帖子的数据分析工具不够完善。Recent progresses in machine learning and large language models such as ChatGPT have made such an analysis possible。在这项研究中，我们收集了Reddit社交媒体平台上的 тысячи篇帖子，然后使用OpenAI的GPT3来 derive embeddings的这些帖子，这些帖子的坐标是高维实数Vecctors，它们可能表示帖子的隐藏 semantics。然后我们通过这些坐标进行了不同的机器学习分类，以了解在不同社区中仇恨言论和谎言的角色。最后，我们对坐标进行了拓扑数据分析（TDA），以获得在线仇恨言论、谎言、心理疾病和总的心理健康之间的视觉地图。”
</details></li>
</ul>
<hr>
<h2 id="FairComp-Workshop-on-Fairness-and-Robustness-in-Machine-Learning-for-Ubiquitous-Computing"><a href="#FairComp-Workshop-on-Fairness-and-Robustness-in-Machine-Learning-for-Ubiquitous-Computing" class="headerlink" title="FairComp: Workshop on Fairness and Robustness in Machine Learning for Ubiquitous Computing"></a>FairComp: Workshop on Fairness and Robustness in Machine Learning for Ubiquitous Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12877">http://arxiv.org/abs/2309.12877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sofia Yfantidou, Dimitris Spathis, Marios Constantinides, Tong Xia, Niels van Berkel</li>
<li>for: 本研讨会旨在讨论 ubicomp 研究中的公平性，以及其社会、技术和法律含义。</li>
<li>methods: 本研讨会将从社会角度探讨公平性和 ubicomp 研究之间的关系，并确定了不会 causing harm 或违反个人权利的技术实践。</li>
<li>results: 本研讨会希望能够培养一个关注公平性的 ubicomp 研究社区，同时也为未来的研究提供明确的指导方针。<details>
<summary>Abstract</summary>
How can we ensure that Ubiquitous Computing (UbiComp) research outcomes are both ethical and fair? While fairness in machine learning (ML) has gained traction in recent years, fairness in UbiComp remains unexplored. This workshop aims to discuss fairness in UbiComp research and its social, technical, and legal implications. From a social perspective, we will examine the relationship between fairness and UbiComp research and identify pathways to ensure that ubiquitous technologies do not cause harm or infringe on individual rights. From a technical perspective, we will initiate a discussion on data practices to develop bias mitigation approaches tailored to UbiComp research. From a legal perspective, we will examine how new policies shape our community's work and future research. We aim to foster a vibrant community centered around the topic of responsible UbiComp, while also charting a clear path for future research endeavours in this field.
</details>
<details>
<summary>摘要</summary>
如何确保宇宙计算（UbiComp）研究成果是公正和公平的？尽管机器学习（ML）中的公正在最近几年得到了更多的关注，但UbiComp中的公正仍然未得到探讨。这个研讨会旨在讨论UbiComp研究中的公正性和其社会、技术和法律因素的影响。从社会角度来看，我们将探讨UBicomp技术不会对个人 права或者造成伤害的关系。从技术角度来看，我们将开始讨论针对UbiComp研究的数据实践，以开发减少偏见的技术策略。从法律角度来看，我们将检查新的政策如何影响我们的社区和未来的研究。我们想建立一个热烈的社区，以讨论负责任的UbiComp研究，同时也映射出未来这一领域的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Robotic-Handling-of-Compliant-Food-Objects-by-Robust-Learning-from-Demonstration"><a href="#Robotic-Handling-of-Compliant-Food-Objects-by-Robust-Learning-from-Demonstration" class="headerlink" title="Robotic Handling of Compliant Food Objects by Robust Learning from Demonstration"></a>Robotic Handling of Compliant Food Objects by Robust Learning from Demonstration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12856">http://arxiv.org/abs/2309.12856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekrem Misimi, Alexander Olofsson, Aleksander Eilertsen, Elling Ruud Øye, John Reidar Mathiassen</li>
<li>for:  robotic grasping of food compliant objects, to improve the consistency of robot learning and reduce the variability of human operators.</li>
<li>methods:  Learning from Demonstration (LfD) approach that combines RGB-D images and tactile data to estimate the necessary gripper pose, finger configuration, and forces for effective robot handling.</li>
<li>results:  the proposed approach can automatically remove inconsistent demonstrations and estimate the teacher’s intended policy, with validated performance for fragile and compliant food objects with complex 3D shapes.<details>
<summary>Abstract</summary>
The robotic handling of compliant and deformable food raw materials, characterized by high biological variation, complex geometrical 3D shapes, and mechanical structures and texture, is currently in huge demand in the ocean space, agricultural, and food industries. Many tasks in these industries are performed manually by human operators who, due to the laborious and tedious nature of their tasks, exhibit high variability in execution, with variable outcomes. The introduction of robotic automation for most complex processing tasks has been challenging due to current robot learning policies. A more consistent learning policy involving skilled operators is desired. In this paper, we address the problem of robot learning when presented with inconsistent demonstrations. To this end, we propose a robust learning policy based on Learning from Demonstration (LfD) for robotic grasping of food compliant objects. The approach uses a merging of RGB-D images and tactile data in order to estimate the necessary pose of the gripper, gripper finger configuration and forces exerted on the object in order to achieve effective robot handling. During LfD training, the gripper pose, finger configurations and tactile values for the fingers, as well as RGB-D images are saved. We present an LfD learning policy that automatically removes inconsistent demonstrations, and estimates the teacher's intended policy. The performance of our approach is validated and demonstrated for fragile and compliant food objects with complex 3D shapes. The proposed approach has a vast range of potential applications in the aforementioned industry sectors.
</details>
<details>
<summary>摘要</summary>
“ robotic food raw material 的自适应和弹性处理，具有高度生物变化、复杂的三维几何形状、机械结构和 текстусту，目前在海洋、农业和食品行业中受到巨大的需求。这些行业中的许多任务现在由人类操作员执行，由于任务的劳动 INTENSIVE 和 monotony，操作员的执行效果存在很大的变化， resulting in variable outcomes。 introducing robotic automation for most complex processing tasks has been challenging due to current robot learning policies. therefore, a more consistent learning policy involving skilled operators is desired. in this paper, we address the problem of robot learning when presented with inconsistent demonstrations. to this end, we propose a robust learning policy based on Learning from Demonstration (LfD) for robotic grasping of food compliant objects. the approach uses a merging of RGB-D images and tactile data in order to estimate the necessary pose of the gripper, gripper finger configuration and forces exerted on the object in order to achieve effective robot handling. during LfD training, the gripper pose, finger configurations and tactile values for the fingers, as well as RGB-D images are saved. we present an LfD learning policy that automatically removes inconsistent demonstrations, and estimates the teacher's intended policy. the performance of our approach is validated and demonstrated for fragile and compliant food objects with complex 3D shapes. the proposed approach has a vast range of potential applications in the aforementioned industry sectors.”Note: The translation is in Simplified Chinese, which is the standard version of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DeepOPF-U-A-Unified-Deep-Neural-Network-to-Solve-AC-Optimal-Power-Flow-in-Multiple-Networks"><a href="#DeepOPF-U-A-Unified-Deep-Neural-Network-to-Solve-AC-Optimal-Power-Flow-in-Multiple-Networks" class="headerlink" title="DeepOPF-U: A Unified Deep Neural Network to Solve AC Optimal Power Flow in Multiple Networks"></a>DeepOPF-U: A Unified Deep Neural Network to Solve AC Optimal Power Flow in Multiple Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12849">http://arxiv.org/abs/2309.12849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Heng Liang, Changhong Zhao</li>
<li>for: 解决不同电力网络中的最优电力流问题</li>
<li>methods: 使用单一深度神经网络（DNN）解决交流电力流问题，并采用滑动输入和输出层对各种电力网络中的负荷和OPT问题进行适应</li>
<li>results: 对IEEE 57&#x2F;118&#x2F;300-bus测试系统和一个逐渐增长的网络进行了优化的性能表现，并且可以处理不同数量的节点、线径和可再生能源资源。<details>
<summary>Abstract</summary>
The traditional machine learning models to solve optimal power flow (OPF) are mostly trained for a given power network and lack generalizability to today's power networks with varying topologies and growing plug-and-play distributed energy resources (DERs). In this paper, we propose DeepOPF-U, which uses one unified deep neural network (DNN) to solve alternating-current (AC) OPF problems in different power networks, including a set of power networks that is successively expanding. Specifically, we design elastic input and output layers for the vectors of given loads and OPF solutions with varying lengths in different networks. The proposed method, using a single unified DNN, can deal with different and growing numbers of buses, lines, loads, and DERs. Simulations of IEEE 57/118/300-bus test systems and a network growing from 73 to 118 buses verify the improved performance of DeepOPF-U compared to existing DNN-based solution methods.
</details>
<details>
<summary>摘要</summary>
传统的机器学习模型用于优化电力流（OPF）大多是为某个特定的电力网络训练，而缺乏对今天的电力网络结构和增加插入式分布式能源资源（DERs）的普适性。在这篇论文中，我们提议了DeepOPF-U，它使用一个通用的深度神经网络（DNN）来解决不同电力网络中的交流电力流优化问题。Specifically，我们设计了弹性输入和输出层，以处理具有不同长度的输入和解决方案Vector在不同的网络中。我们的方法使用单个通用DNN来解决不同的和增加的电力网络中的问题，包括不同数量的总站、线径和负荷。我们的实验结果表明，相比之前的DNN基本方法，DeepOPF-U可以更好地处理不同的电力网络和增加的负荷。Here's a word-for-word translation of the text into Simplified Chinese:传统的机器学习模型用于优化电力流（OPF）大多是为某个特定的电力网络训练，而缺乏对今天的电力网络结构和增加插入式分布式能源资源（DERs）的普适性。在这篇论文中，我们提议了DeepOPF-U，它使用一个通用的深度神经网络（DNN）来解决不同电力网络中的交流电力流优化问题。Specifically，我们设计了弹性输入和输出层，以处理具有不同长度的输入和解决方案Vector在不同的网络中。我们的方法使用单个通用DNN来解决不同的和增加的电力网络中的问题，包括不同数量的总站、线径和负荷。我们的实验结果表明，相比之前的DNN基本方法，DeepOPF-U可以更好地处理不同的电力网络和增加的负荷。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Independent-DE-Optimizations-to-Tackle-Uncertainty-and-Variability-in-Demand-in-Inventory-Management"><a href="#Multiple-Independent-DE-Optimizations-to-Tackle-Uncertainty-and-Variability-in-Demand-in-Inventory-Management" class="headerlink" title="Multiple Independent DE Optimizations to Tackle Uncertainty and Variability in Demand in Inventory Management"></a>Multiple Independent DE Optimizations to Tackle Uncertainty and Variability in Demand in Inventory Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13095">http://arxiv.org/abs/2309.13095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarit Maitra, Sukanya Kundu, Vivek Mishra</li>
<li>for: 本研究旨在找出适用于不确定需求 patrerns 的 Metaheuristic Differeential Evolution 优化策略，以最小化存储成本。</li>
<li>methods: 本研究结合综合 IM 策略的 continuous review 和 Monte Carlo Simulation (MCS)，并对多种算法进行比较，以找到最佳解决方案。</li>
<li>results: 研究发现，Differeential Evolution (DE) 算法在优化 IM 中表现最佳，并通过 Latin Hypercube Sampling (LHS) 统计方法进行参数调整。本研究还提出了一种 combining 多个独立 DE 优化实例的方法，以提高性能和成本效益，特别是在不确定需求 patrerns 下。<details>
<summary>Abstract</summary>
To determine the effectiveness of metaheuristic Differential Evolution optimization strategy for inventory management (IM) in the context of stochastic demand, this empirical study undertakes a thorough investigation. The primary objective is to discern the most effective strategy for minimizing inventory costs within the context of uncertain demand patterns. Inventory costs refer to the expenses associated with holding and managing inventory within a business. The approach combines a continuous review of IM policies with a Monte Carlo Simulation (MCS). To find the optimal solution, the study focuses on meta-heuristic approaches and compares multiple algorithms. The outcomes reveal that the Differential Evolution (DE) algorithm outperforms its counterparts in optimizing IM. To fine-tune the parameters, the study employs the Latin Hypercube Sampling (LHS) statistical method. To determine the final solution, a method is employed in this study which combines the outcomes of multiple independent DE optimizations, each initiated with different random initial conditions. This approach introduces a novel and promising dimension to the field of inventory management, offering potential enhancements in performance and cost efficiency, especially in the presence of stochastic demand patterns.
</details>
<details>
<summary>摘要</summary>
为了判断metaheuristic diferencial evolution优化策略对供应链管理（IM）在不确定的需求 Patterns 上的效果，这个实验室进行了一项严格的调查。主要目标是找到最有效的策略来最小化存储成本在企业中。存储成本包括保持和管理存储的成本。该方法结合了连续性 IM 策略的审查和Monte Carlo Simulation（MCS）。为了找到优化策略，这个研究对meta-heuristic Approaches进行了比较多个算法。研究发现，diferencial Evolution（DE）算法在优化IM方面表现出色。为了调整参数，这个研究使用了Latin Hypercube Sampling（LHS）统计方法。为了确定最终解决方案，这个研究employs a方法，将多个独立的DE优化结果组合起来，每个初始条件都是random。这种方法在供应链管理领域引入了一个新的维度，提供了可能的性能和成本效益，特别是在不确定的需求Patterns 上。
</details></li>
</ul>
<hr>
<h2 id="Reward-Function-Design-for-Crowd-Simulation-via-Reinforcement-Learning"><a href="#Reward-Function-Design-for-Crowd-Simulation-via-Reinforcement-Learning" class="headerlink" title="Reward Function Design for Crowd Simulation via Reinforcement Learning"></a>Reward Function Design for Crowd Simulation via Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12841">http://arxiv.org/abs/2309.12841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ariel Kwiatkowski, Vicky Kalogeiton, Julien Pettré, Marie-Paule Cani</li>
<li>for: 这个论文的目的是探讨基于奖励学习的人群 simulate 的方法，以及设计奖励函数的正确方法。</li>
<li>methods: 这个论文使用了奖励学习方法，并通过 theoretically 和 empirically 分析奖励函数的效果。</li>
<li>results: 该论文的实验结果表明，直接减少能量消耗是一种有效的策略， provided that it is paired with an appropriately scaled guiding potential。这些结果可以帮助开发新的人群 simulate 技术，并对人类 Navigation 的研究产生影响。<details>
<summary>Abstract</summary>
Crowd simulation is important for video-games design, since it enables to populate virtual worlds with autonomous avatars that navigate in a human-like manner. Reinforcement learning has shown great potential in simulating virtual crowds, but the design of the reward function is critical to achieving effective and efficient results. In this work, we explore the design of reward functions for reinforcement learning-based crowd simulation. We provide theoretical insights on the validity of certain reward functions according to their analytical properties, and evaluate them empirically using a range of scenarios, using the energy efficiency as the metric. Our experiments show that directly minimizing the energy usage is a viable strategy as long as it is paired with an appropriately scaled guiding potential, and enable us to study the impact of the different reward components on the behavior of the simulated crowd. Our findings can inform the development of new crowd simulation techniques, and contribute to the wider study of human-like navigation.
</details>
<details>
<summary>摘要</summary>
伪人群模拟在游戏设计中具有重要意义，因为它使得虚拟世界中的自主人物能够在人类化的方式下自主 Navigation。基于奖励学习的人群模拟显示了巨大的潜力，但是奖励函数的设计是获得有效和高效的结果的关键。在这项工作中，我们探讨了基于奖励学习的人群模拟中奖励函数的设计。我们提供了理论上的思路，并通过一系列场景的实验来评估奖励函数的有效性。我们发现，直接减少能量使用是一个有效的策略，只要与适当的拟合潜在能量相关的奖励函数相结合。这些实验结果可以导向新的人群模拟技术的开发，并对人类化导航的更广泛研究产生贡献。
</details></li>
</ul>
<hr>
<h2 id="Doubly-Robust-Proximal-Causal-Learning-for-Continuous-Treatments"><a href="#Doubly-Robust-Proximal-Causal-Learning-for-Continuous-Treatments" class="headerlink" title="Doubly Robust Proximal Causal Learning for Continuous Treatments"></a>Doubly Robust Proximal Causal Learning for Continuous Treatments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12819">http://arxiv.org/abs/2309.12819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Wu, Yanwei Fu, Shouyan Wang, Xinwei Sun</li>
<li>for: 本研究旨在提出一种可以处理连续干扰因素的 proximal causal learning 框架，以便在实际应用中更好地估计 causal effect。</li>
<li>methods: 我们提出了一种基于 kernel 函数的 DR 估计器，可以有效地处理连续干扰因素。我们还提出了一种新的方法来效率地解决干扰函数的问题。</li>
<li>results: 我们对 synthetic 数据和实际应用进行了评估，并证明了我们的估计器具有良好的准确性和稳定性。<details>
<summary>Abstract</summary>
Proximal causal learning is a promising framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatment can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We then provide a comprehensive convergence analysis in terms of the mean square error. We demonstrate the utility of our estimator on synthetic datasets and real-world applications.
</details>
<details>
<summary>摘要</summary>
近似 causal learning 是一个有前途的框架，用于在存在未测量的干扰因素时确定 causal effect。在这个框架下， doubly robust（DR）估计器被 derivation 出来，并在不符合模型假设的情况下表现出优异的效果。然而，现有的 DR 估计器只适用于 binary 治疗，而在实际应用中，治疗可能是连续的。主要的障碍是 delta 函数存在在原始 DR 估计器中，使其无法在 causal effect 估计中使用，并且在 auxiliary function 估计中增加了巨大的计算负担。为了解决这些挑战，我们提出了基于 kernel 的 DR 估计器，可以好好地处理连续治疗。利用其平滑性，我们表明其oracle形式是一个可靠的 influence function 的近似。此外，我们提出了一种新的方法来有效地解决 auxiliary function。然后，我们进行了完整的mean square error（MSE）的收敛分析。我们在 sintetic 数据和实际应用中展示了我们的估计器的实用性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Generalization-in-Game-Agents-with-Data-Augmentation-in-Imitation-Learning"><a href="#Improving-Generalization-in-Game-Agents-with-Data-Augmentation-in-Imitation-Learning" class="headerlink" title="Improving Generalization in Game Agents with Data Augmentation in Imitation Learning"></a>Improving Generalization in Game Agents with Data Augmentation in Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12815">http://arxiv.org/abs/2309.12815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Derek Yadgaroff, Alessandro Sestini, Konrad Tollmar, Linus Gisslén</li>
<li>for: 提高游戏AI的通用能力</li>
<li>methods: 使用数据扩展法提高imitative learning agents的通用能力</li>
<li>results: 数据扩展法可以有效提高imitative learning agents的通用能力，并提供了多种3D环境中的性能指标。<details>
<summary>Abstract</summary>
Imitation learning is an effective approach for training game-playing agents and, consequently, for efficient game production. However, generalization - the ability to perform well in related but unseen scenarios - is an essential requirement that remains an unsolved challenge for game AI. Generalization is difficult for imitation learning agents because it requires the algorithm to take meaningful actions outside of the training distribution. In this paper we propose a solution to this challenge. Inspired by the success of data augmentation in supervised learning, we augment the training data so the distribution of states and actions in the dataset better represents the real state-action distribution. This study evaluates methods for combining and applying data augmentations to observations, to improve generalization of imitation learning agents. It also provides a performance benchmark of these augmentations across several 3D environments. These results demonstrate that data augmentation is a promising framework for improving generalization in imitation learning agents.
</details>
<details>
<summary>摘要</summary>
仿制学习是一种有效的方法用于训练游戏AI代理人，并且可以提高游戏生产效率。然而，通用化（能够在相关 yet unseen 的情况下表现良好）是一个必备的要求，它是一个未解决的挑战。通用化对仿制学习代理人来说是一个困难的任务，因为它需要算法在训练分布之外行为。在这篇论文中，我们提出了一种解决这个挑战的方法。受到超参数学习中的数据扩展成功的启发，我们将训练数据进行扩展，以使得状态和动作的分布更好地表示真实的状态-动作分布。本研究评估了对观察数据的合并和应用的方法，以提高仿制学习代理人的通用化。此外，本研究还提供了不同3D环境下这些扩展的性能比较。这些结果表明，数据扩展是一种有前途的框架，可以提高仿制学习代理人的通用化。
</details></li>
</ul>
<hr>
<h2 id="Deepfake-audio-as-a-data-augmentation-technique-for-training-automatic-speech-to-text-transcription-models"><a href="#Deepfake-audio-as-a-data-augmentation-technique-for-training-automatic-speech-to-text-transcription-models" class="headerlink" title="Deepfake audio as a data augmentation technique for training automatic speech to text transcription models"></a>Deepfake audio as a data augmentation technique for training automatic speech to text transcription models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12802">http://arxiv.org/abs/2309.12802</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandre R. Ferreira, Cláudio E. C. Campelo</li>
<li>for: 本研究旨在提高语音识别模型的Robustness，需要一个大型多样化的标注数据集。</li>
<li>methods: 本文提出了一种基于深度假音的数据增强策略，使用了voice cloner和英文语音 Dataset produced by Indians。</li>
<li>results: 经过实验 validate，使用增强数据可以提高语音识别模型的性能，并且在不同的场景下都有良好的效果。<details>
<summary>Abstract</summary>
To train transcriptor models that produce robust results, a large and diverse labeled dataset is required. Finding such data with the necessary characteristics is a challenging task, especially for languages less popular than English. Moreover, producing such data requires significant effort and often money. Therefore, a strategy to mitigate this problem is the use of data augmentation techniques. In this work, we propose a framework that approaches data augmentation based on deepfake audio. To validate the produced framework, experiments were conducted using existing deepfake and transcription models. A voice cloner and a dataset produced by Indians (in English) were selected, ensuring the presence of a single accent in the dataset. Subsequently, the augmented data was used to train speech to text models in various scenarios.
</details>
<details>
<summary>摘要</summary>
In this work, we conducted experiments using existing deepfake and transcription models. We selected a voice cloner and a dataset produced by Indians (in English) to ensure the presence of a single accent in the dataset. We then augmented the data and used it to train speech-to-text models in various scenarios.
</details></li>
</ul>
<hr>
<h2 id="An-Intelligent-Approach-to-Detecting-Novel-Fault-Classes-for-Centrifugal-Pumps-Based-on-Deep-CNNs-and-Unsupervised-Methods"><a href="#An-Intelligent-Approach-to-Detecting-Novel-Fault-Classes-for-Centrifugal-Pumps-Based-on-Deep-CNNs-and-Unsupervised-Methods" class="headerlink" title="An Intelligent Approach to Detecting Novel Fault Classes for Centrifugal Pumps Based on Deep CNNs and Unsupervised Methods"></a>An Intelligent Approach to Detecting Novel Fault Classes for Centrifugal Pumps Based on Deep CNNs and Unsupervised Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12765">http://arxiv.org/abs/2309.12765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahdi Abdollah Chalaki, Daniyal Maroufi, Mahdi Robati, Mohammad Javad Karimi, Ali Sadighi</li>
<li>for: 本研究旨在Addressing the challenges of data-driven fault diagnosis of rotating machines, particularly the lack of information about various faults in the field.</li>
<li>methods: 本 paper 使用了 convolutional neural network (CNN) 和 t-SNE 方法，以检测 novel faults. 首先，使用受限的系统故障信息进行训练，然后使用 clustering 技术进行检测。 如果检测到新的故障，则使用新数据进行网络的扩展。</li>
<li>results: 实验结果表明，这种 two-stage 方法在一台 centrifugal pump 上得到了高精度的 novel fault 检测结果。<details>
<summary>Abstract</summary>
Despite the recent success in data-driven fault diagnosis of rotating machines, there are still remaining challenges in this field. Among the issues to be addressed, is the lack of information about variety of faults the system may encounter in the field. In this paper, we assume a partial knowledge of the system faults and use the corresponding data to train a convolutional neural network. A combination of t-SNE method and clustering techniques is then employed to detect novel faults. Upon detection, the network is augmented using the new data. Finally, a test setup is used to validate this two-stage methodology on a centrifugal pump and experimental results show high accuracy in detecting novel faults.
</details>
<details>
<summary>摘要</summary>
尽管在数据驱动机器故障诊断方面已经取得了一定的成功，但这个领域仍然存在一些挑战。其中一个问题是系统可能在场景中遇到多种故障的信息不够。在这篇论文中，我们假设系统具有部分故障知识，并使用相应的数据来训练卷积神经网络。然后，我们使用t-SNE方法和聚类技术检测新的故障。检测到故障后，网络被扩展使用新的数据。最后，我们使用测试setup验证这种两个阶段方法在中心泵上的效果，实验结果显示高精度地检测到新的故障。Note: "t-SNE" stands for "t-distributed Stochastic Neighbor Embedding", which is a technique used to reduce the dimensionality of data.
</details></li>
</ul>
<hr>
<h2 id="Prototype-Enhanced-Hypergraph-Learning-for-Heterogeneous-Information-Networks"><a href="#Prototype-Enhanced-Hypergraph-Learning-for-Heterogeneous-Information-Networks" class="headerlink" title="Prototype-Enhanced Hypergraph Learning for Heterogeneous Information Networks"></a>Prototype-Enhanced Hypergraph Learning for Heterogeneous Information Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13092">http://arxiv.org/abs/2309.13092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Wang, Jiayi Shen, Athanasios Efthymiou, Stevan Rudinac, Monika Kackovic, Nachoem Wijnberg, Marcel Worring</li>
<li>for: 本研究旨在提出一种基于超граpher学习的节点分类方法，用于处理具有多样化和复杂关系的 multimedia数据中的异构信息网络（HINs）。</li>
<li>methods: 本方法使用超граpher instead of graph，以捕捉高阶关系 между节点，而无需靠谱定过程。它还利用示例来改善超граpher学习过程的稳定性，从而提供可读性的人类可读性。</li>
<li>results: 对于三个真实的 HINs 实验，本方法显示了效果。<details>
<summary>Abstract</summary>
The variety and complexity of relations in multimedia data lead to Heterogeneous Information Networks (HINs). Capturing the semantics from such networks requires approaches capable of utilizing the full richness of the HINs. Existing methods for modeling HINs employ techniques originally designed for graph neural networks, and HINs decomposition analysis, like using manually predefined metapaths. In this paper, we introduce a novel prototype-enhanced hypergraph learning approach for node classification in HINs. Using hypergraphs instead of graphs, our method captures higher-order relationships among nodes and extracts semantic information without relying on metapaths. Our method leverages the power of prototypes to improve the robustness of the hypergraph learning process and creates the potential to provide human-interpretable insights into the underlying network structure. Extensive experiments on three real-world HINs demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
multimedia数据中的多样性和复杂性导致非同様网络（HINs）的出现。捕捉HINs中的semantics需要能够利用非同様网络的全部 ricinus。现有的HINs模型使用原本设计 для图神经网络的技术，以及手动划定的ме타路径进行分析。本文提出了一种基于prototype强化的超граraph学习方法 дляHINs节点分类。使用超граraph instead of graphs，我们的方法可以捕捉节点之间的高阶关系，并提取无需依赖于ме타路径的semantic信息。我们的方法利用 prototype的力量来提高超гра�学习过程的稳定性，并创造了可以提供人类可读的网络结构下的内部层次结构的可能性。我们在三个真实的HINs上进行了广泛的实验，并证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Make-the-U-in-UDA-Matter-Invariant-Consistency-Learning-for-Unsupervised-Domain-Adaptation"><a href="#Make-the-U-in-UDA-Matter-Invariant-Consistency-Learning-for-Unsupervised-Domain-Adaptation" class="headerlink" title="Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation"></a>Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12742">http://arxiv.org/abs/2309.12742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yue-zhongqi/icon">https://github.com/yue-zhongqi/icon</a></li>
<li>paper_authors: Zhongqi Yue, Hanwang Zhang, Qianru Sun</li>
<li>for: 本研究旨在解决Unsupervised Domain Adaptation (UDA)中存在的假 correlate问题，即源领域中的特征与目标领域的特征之间的假 correlate，这导致了目标领域的模型难以泛化。</li>
<li>methods: 我们提出了一种名为“做 Consistency learning”（ICON）的方法，它通过同时使源领域和目标领域的分类器预测结果相一致，从而消除了目标领域中的假 correlate。</li>
<li>results: 我们在经验证上表明，ICON可以在经典的UDA benchmark上达到最佳性能，并在挑战性的WILDS 2.0 benchmark上超越所有传统方法。<details>
<summary>Abstract</summary>
Domain Adaptation (DA) is always challenged by the spurious correlation between domain-invariant features (e.g., class identity) and domain-specific features (e.g., environment) that does not generalize to the target domain. Unfortunately, even enriched with additional unsupervised target domains, existing Unsupervised DA (UDA) methods still suffer from it. This is because the source domain supervision only considers the target domain samples as auxiliary data (e.g., by pseudo-labeling), yet the inherent distribution in the target domain -- where the valuable de-correlation clues hide -- is disregarded. We propose to make the U in UDA matter by giving equal status to the two domains. Specifically, we learn an invariant classifier whose prediction is simultaneously consistent with the labels in the source domain and clusters in the target domain, hence the spurious correlation inconsistent in the target domain is removed. We dub our approach "Invariant CONsistency learning" (ICON). Extensive experiments show that ICON achieves the state-of-the-art performance on the classic UDA benchmarks: Office-Home and VisDA-2017, and outperforms all the conventional methods on the challenging WILDS 2.0 benchmark. Codes are in https://github.com/yue-zhongqi/ICON.
</details>
<details>
<summary>摘要</summary>
域 adaptation (DA) 总是面临着域特异特征（例如类标识）和域特定特征（例如环境）之间的假设相关性，这种相关性不能泛化到目标域。尽管使用额外的无监督目标域数据，现有的无监督DA（UDA）方法仍然受到这种挑战。这是因为源域监督只考虑目标域样本为辅助数据（例如 pseudo-labeling），忽略了目标域的自然分布，其中包含了价值的分解准则。我们提议使得U在UDA中变得重要，即在源域和目标域之间学习一个不变的分类器，其预测结果同时与源域中的标签和目标域中的团集一致，因此在目标域中排除了假设相关性。我们称之为“不变CONsistency学习”（ICON）。我们进行了广泛的实验，ICON在经典的UDABenchmark上取得了state-of-the-art性能，并在挑战性的WILDS 2.0 Benchmark上超过了所有传统方法。代码在https://github.com/yue-zhongqi/ICON。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Dynamic-Fees-for-Blockchain-Resources"><a href="#Optimal-Dynamic-Fees-for-Blockchain-Resources" class="headerlink" title="Optimal Dynamic Fees for Blockchain Resources"></a>Optimal Dynamic Fees for Blockchain Resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12735">http://arxiv.org/abs/2309.12735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Crapis, Ciamac C. Moallemi, Shouqiao Wang</li>
<li>For: 本研究は多个区块链资源的优化奖励机制的设计问题进行通用和实用的框架开发。* Methods: 我们的框架可以计算优化奖励策略，以让奖励策略与持续的需求变化进行融合，同时保证奖励策略对当地噪声的Robustness。在多个资源的总情况下，我们的优化策略能正确处理资源需求之间的交叉效应（补做和替代）。* Results: 我们的框架可以用来修订或指导使用各种各样的奖励更新规则，如EIP-1559或EIP-4844。我们通过两个案例研究证明了这一点。我们还使用实际市场数据来对一个一维版本的我们模型进行估算，并对EIP-1559的性能与我们的优化策略进行比较。<details>
<summary>Abstract</summary>
We develop a general and practical framework to address the problem of the optimal design of dynamic fee mechanisms for multiple blockchain resources. Our framework allows to compute policies that optimally trade-off between adjusting resource prices to handle persistent demand shifts versus being robust to local noise in the observed block demand. In the general case with more than one resource, our optimal policies correctly handle cross-effects (complementarity and substitutability) in resource demands. We also show how these cross-effects can be used to inform resource design, i.e. combining resources into bundles that have low demand-side cross-effects can yield simpler and more efficient price-update rules. Our framework is also practical, we demonstrate how it can be used to refine or inform the design of heuristic fee update rules such as EIP-1559 or EIP-4844 with two case studies. We then estimate a uni-dimensional version of our model using real market data from the Ethereum blockchain and empirically compare the performance of our optimal policies to EIP-1559.
</details>
<details>
<summary>摘要</summary>
我们开发了一个通用且实用的框架，以解决多个区块链资源的优化设计动态费用机制问题。我们的框架可以计算优化费用更新策略，以优考虑到适应持续强制变化的需求轨迹，同时保持对地方噪声观测到的区块需求的稳定性。在多than one resource的普通情况下，我们的优化策略可以正确处理资源需求之间的交叉效应（补做和替代）。我们还示出了如何使用这些交叉效应来指导资源设计，例如将资源合并成具有低需求层次交叉效应的套件可以得到简单而高效的价格更新规则。我们的框架也是实用的，我们示出了如何使用它来优化或指导基于EIP-1559或EIP-4844的费用更新规则的设计。然后，我们使用实际市场数据从Ethereum区块链进行了一个一维版本的模型估算，并对EIP-1559的性能与我们的优化策略进行了实际比较。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Representations-Improve-Supervised-Learning-in-Speech-Emotion-Recognition"><a href="#Unsupervised-Representations-Improve-Supervised-Learning-in-Speech-Emotion-Recognition" class="headerlink" title="Unsupervised Representations Improve Supervised Learning in Speech Emotion Recognition"></a>Unsupervised Representations Improve Supervised Learning in Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12714">http://arxiv.org/abs/2309.12714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirali Soltani Tehrani, Niloufar Faridani, Ramin Toosi</li>
<li>for: 这个研究旨在提高人机交互的深度理解，通过感知人类情感状态，提高人机交互的效果和同情性。</li>
<li>methods: 该研究提出了一种新的方法，即结合自我指导特征提取和指导分类来实现情绪识别。在预处理步骤中，我们使用基于Wav2Vec模型的自我指导特征提取器，从audio数据中捕捉音频特征。然后，输出特征图的前一步结果被传递给自定义的卷积神经网络模型进行情绪分类。</li>
<li>results: 在使用ShEMO数据集进行测试时，该方法超过了两个基准方法，即支持向量机分类器和转移学习预训练的CNN模型。与状态的艺术方法相比，该方法表现更出色，提供了更高的情感认知水平，为人机交互领域带来更多的同情性和效果。<details>
<summary>Abstract</summary>
Speech Emotion Recognition (SER) plays a pivotal role in enhancing human-computer interaction by enabling a deeper understanding of emotional states across a wide range of applications, contributing to more empathetic and effective communication. This study proposes an innovative approach that integrates self-supervised feature extraction with supervised classification for emotion recognition from small audio segments. In the preprocessing step, to eliminate the need of crafting audio features, we employed a self-supervised feature extractor, based on the Wav2Vec model, to capture acoustic features from audio data. Then, the output featuremaps of the preprocessing step are fed to a custom designed Convolutional Neural Network (CNN)-based model to perform emotion classification. Utilizing the ShEMO dataset as our testing ground, the proposed method surpasses two baseline methods, i.e. support vector machine classifier and transfer learning of a pretrained CNN. comparing the propose method to the state-of-the-art methods in SER task indicates the superiority of the proposed method. Our findings underscore the pivotal role of deep unsupervised feature learning in elevating the landscape of SER, offering enhanced emotional comprehension in the realm of human-computer interactions.
</details>
<details>
<summary>摘要</summary>
人机交互中情感认识（SER）发挥关键作用，帮助更深入理解情感状态，涵盖广泛应用领域，从而提供更 Empathetic 和有效的沟通。本研究提出了一种创新的方法，将自主学习特征提取与经过监督分类结合用于情感识别。在预处理步骤中，我们采用基于 Wav2Vec 模型的自主学习特征提取器，从音频数据中提取了音频特征。然后，预处理步骤的输出特征地图被传递给自定义设计的卷积神经网络（CNN）模型进行情感分类。使用 ShEMO 数据集进行测试，我们的提议方法超过了两个基准方法，即支持向量机学习分类器和转移学习已经训练的 CNN。与状态 искусственный地进行SER任务的方法进行比较，我们的发现表明了深入的无监督特征学习对 SER 任务的提升具有重要作用。我们的发现强调了深入的特征学习在人机交互中的情感理解方面的重要性。
</details></li>
</ul>
<hr>
<h2 id="Big-model-only-for-hard-audios-Sample-dependent-Whisper-model-selection-for-efficient-inferences"><a href="#Big-model-only-for-hard-audios-Sample-dependent-Whisper-model-selection-for-efficient-inferences" class="headerlink" title="Big model only for hard audios: Sample dependent Whisper model selection for efficient inferences"></a>Big model only for hard audios: Sample dependent Whisper model selection for efficient inferences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12712">http://arxiv.org/abs/2309.12712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hugomalard/big-model-only-for-hard-audios">https://github.com/hugomalard/big-model-only-for-hard-audios</a></li>
<li>paper_authors: Hugo Malard, Salah Zaiem, Robin Algayres</li>
<li>for: 这个研究的目的是提出一个可以在不同的模型大小下选择最佳的决策模组，以便在不同的内存和硬件环境下进行自动语音识别（ASR）。</li>
<li>methods: 作者使用了两个不同大小的 Whisper 模型，并将它们联合使用以构建一个决策模组。他们还使用了一些计算效率的技巧来降低决策模组的计算成本。</li>
<li>results: 作者的实验结果显示，使用这个决策模组可以实现substantial的计算成本减少，同时保持transcription的性能水平。具体来说，在两个 Whisper 模型中，使用决策模组可以降低了模型的计算成本，并且对于大多数的测试数据进行了好的调整。<details>
<summary>Abstract</summary>
Recent progress in Automatic Speech Recognition (ASR) has been coupled with a substantial increase in the model sizes, which may now contain billions of parameters, leading to slow inferences even with adapted hardware. In this context, several ASR models exist in various sizes, with different inference costs leading to different performance levels. Based on the observation that smaller models perform optimally on large parts of testing corpora, we propose to train a decision module, that would allow, given an audio sample, to use the smallest sufficient model leading to a good transcription. We apply our approach to two Whisper models with different sizes. By keeping the decision process computationally efficient, we build a decision module that allows substantial computational savings with reduced performance drops.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Discovering-the-Interpretability-Performance-Pareto-Front-of-Decision-Trees-with-Dynamic-Programming"><a href="#Discovering-the-Interpretability-Performance-Pareto-Front-of-Decision-Trees-with-Dynamic-Programming" class="headerlink" title="Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming"></a>Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12701">http://arxiv.org/abs/2309.12701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hector Kohler, Riad Akrour, Philippe Preux</li>
<li>for: 本文提出了一种新的Markov Decision Problem（MDP）形式，用于找到最佳决策树。</li>
<li>methods: 该方法使用了一个单一的动态计划来计算多种解释性-性能质量Front的优化决策树。</li>
<li>results: 实验表明，该方法与当前状态的算法相当于的精度和运行时间，同时返回了一组决策树，用于用户选择最适合其需求的解释性-性能质量Front。<details>
<summary>Abstract</summary>
Decision trees are known to be intrinsically interpretable as they can be inspected and interpreted by humans. Furthermore, recent hardware advances have rekindled an interest for optimal decision tree algorithms, that produce more accurate trees than the usual greedy approaches. However, these optimal algorithms return a single tree optimizing a hand defined interpretability-performance trade-off, obtained by specifying a maximum number of decision nodes, giving no further insights about the quality of this trade-off. In this paper, we propose a new Markov Decision Problem (MDP) formulation for finding optimal decision trees. The main interest of this formulation is that we can compute the optimal decision trees for several interpretability-performance trade-offs by solving a single dynamic program, letting the user choose a posteriori the tree that best suits their needs. Empirically, we show that our method is competitive with state-of-the-art algorithms in terms of accuracy and runtime while returning a whole set of trees on the interpretability-performance Pareto front.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a new Markov Decision Problem (MDP) formulation for finding optimal decision trees. Our approach allows us to compute the optimal decision trees for multiple interpretability-performance trade-offs by solving a single dynamic program. This enables the user to choose the tree that best suits their needs after the fact. Empirical results show that our method is competitive with state-of-the-art algorithms in terms of accuracy and runtime, while providing a set of trees on the interpretability-performance Pareto front.
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Temporal-Revision-Graph-Networks"><a href="#Recurrent-Temporal-Revision-Graph-Networks" class="headerlink" title="Recurrent Temporal Revision Graph Networks"></a>Recurrent Temporal Revision Graph Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12694">http://arxiv.org/abs/2309.12694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhou Chen, Anxiang Zeng, Guangda Huzhang, Qingtao Yu, Kerui Zhang, Cao Yuanpeng, Kangle Wu, Han Yu, Zhiming Zhou</li>
<li>for: 本研究旨在提供一种更准确地模型 temporal graph 的方法，具体来说是一种基于 recurrent neural network (RNN) 的 temporal neighbor aggregation 方法，以便更好地捕捉 temporal graph 中 node 之间的关系。</li>
<li>methods: 本研究使用 RNN  WITH node-wise hidden states 来集成所有历史邻居信息，从而提供更完整的邻居信息。这种方法可以在实际应用中提高 averaged precision 约 9.6%  compared to existing methods。</li>
<li>results: 本研究的实际应用result 显示，使用本研究提出的方法可以在 Ecommerce  dataset 中提高 averaged precision 约 9.6%  compared to existing methods。这表明本研究的方法可以更好地捕捉 temporal graph 中 node 之间的关系，从而提高模型的准确性。<details>
<summary>Abstract</summary>
Temporal graphs offer more accurate modeling of many real-world scenarios than static graphs. However, neighbor aggregation, a critical building block of graph networks, for temporal graphs, is currently straightforwardly extended from that of static graphs. It can be computationally expensive when involving all historical neighbors during such aggregation. In practice, typically only a subset of the most recent neighbors are involved. However, such subsampling leads to incomplete and biased neighbor information. To address this limitation, we propose a novel framework for temporal neighbor aggregation that uses the recurrent neural network with node-wise hidden states to integrate information from all historical neighbors for each node to acquire the complete neighbor information. We demonstrate the superior theoretical expressiveness of the proposed framework as well as its state-of-the-art performance in real-world applications. Notably, it achieves a significant +9.6% improvement on averaged precision in a real-world Ecommerce dataset over existing methods on 2-layer models.
</details>
<details>
<summary>摘要</summary>
时间图表提供更加准确地模型多种实际场景 than static graphs. However, temporal graph neighbor aggregation, a critical component of graph networks, is currently extended straightforwardly from static graphs. This can be computationally expensive when considering all historical neighbors during aggregation. In practice, only a subset of the most recent neighbors are typically involved, but such subsampling leads to incomplete and biased neighbor information. To address this limitation, we propose a novel framework for temporal neighbor aggregation that uses recurrent neural networks with node-wise hidden states to integrate information from all historical neighbors for each node to obtain complete neighbor information. We demonstrate the superior theoretical expressiveness of the proposed framework as well as its state-of-the-art performance in real-world applications. Notably, it achieves a significant +9.6% improvement in averaged precision over existing methods on 2-layer models in a real-world Ecommerce dataset.
</details></li>
</ul>
<hr>
<h2 id="OneNet-Enhancing-Time-Series-Forecasting-Models-under-Concept-Drift-by-Online-Ensembling"><a href="#OneNet-Enhancing-Time-Series-Forecasting-Models-under-Concept-Drift-by-Online-Ensembling" class="headerlink" title="OneNet: Enhancing Time Series Forecasting Models under Concept Drift by Online Ensembling"></a>OneNet: Enhancing Time Series Forecasting Models under Concept Drift by Online Ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12659">http://arxiv.org/abs/2309.12659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yfzhang114/onenet">https://github.com/yfzhang114/onenet</a></li>
<li>paper_authors: Yi-Fan Zhang, Qingsong Wen, Xue Wang, Weiqi Chen, Liang Sun, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan</li>
<li>For: 本研究旨在提出一种能够高效地更新时间序列预测模型，以Addressing the concept drifting problem。* Methods: 本文提出了一种基于online convex programming框架的强化学习方法，可以动态地更新和组合两个模型，其中一个模型专注于时间维度上的关系，另一个模型则是跨变量关系。* Results: 实验结果显示，OneNet可以在线预测错误下降超过50%，至比State-Of-The-Art方法更高。<details>
<summary>Abstract</summary>
Online updating of time series forecasting models aims to address the concept drifting problem by efficiently updating forecasting models based on streaming data. Many algorithms are designed for online time series forecasting, with some exploiting cross-variable dependency while others assume independence among variables. Given every data assumption has its own pros and cons in online time series modeling, we propose \textbf{On}line \textbf{e}nsembling \textbf{Net}work (OneNet). It dynamically updates and combines two models, with one focusing on modeling the dependency across the time dimension and the other on cross-variate dependency. Our method incorporates a reinforcement learning-based approach into the traditional online convex programming framework, allowing for the linear combination of the two models with dynamically adjusted weights. OneNet addresses the main shortcoming of classical online learning methods that tend to be slow in adapting to the concept drift. Empirical results show that OneNet reduces online forecasting error by more than $\mathbf{50\%}$ compared to the State-Of-The-Art (SOTA) method. The code is available at \url{https://github.com/yfzhang114/OneNet}.
</details>
<details>
<summary>摘要</summary>
在线更新时间序列预测模型目的是解决概念漂移问题，通过基于流入数据的高效更新预测模型。许多算法已经为在线时间序列预测设计，其中一些利用时间维度之间的依赖关系，而其他们假设变量之间是独立的。每个数据假设都有其自己的优缺点，在在线时间序列预测中。我们提出了《Online Ensembling Network（OneNet）》，它在实时更新和组合两个模型，其中一个专门关注时间维度之间的依赖关系，另一个则关注变量之间的交叉依赖关系。我们的方法将经验学学习基于的逻辑添加到传统的在线凸programming框架中，允许在线动态调整模型之间的线性组合。OneNet可以快速适应概念漂移，并且实际结果表明，与现状技术（SOTA）方法相比，OneNet可以降低在线预测错误率高于50%。代码可以在 \url{https://github.com/yfzhang114/OneNet} 中找到。
</details></li>
</ul>
<hr>
<h2 id="Neural-Operator-Variational-Inference-based-on-Regularized-Stein-Discrepancy-for-Deep-Gaussian-Processes"><a href="#Neural-Operator-Variational-Inference-based-on-Regularized-Stein-Discrepancy-for-Deep-Gaussian-Processes" class="headerlink" title="Neural Operator Variational Inference based on Regularized Stein Discrepancy for Deep Gaussian Processes"></a>Neural Operator Variational Inference based on Regularized Stein Discrepancy for Deep Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12658">http://arxiv.org/abs/2309.12658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Xu, Shian Du, Junmei Yang, Qianli Ma, Delu Zeng</li>
<li>for: 这个研究旨在提出一种基于神经网络的 Variational Inference 方法，用于深度 Gaussian Process (DGP) 模型中的 Bayesian 推断。</li>
<li>methods: 方法使用神经网络生成器，实现了对真 posterior 的过程独立推断，并使用 Monte Carlo 估计和抽样数据点估计技术来解决问题。</li>
<li>results: 实验结果显示，提出的方法可以实现高精度和高速度的推断，并在许多数据集上实现了比 SOTA Gaussian process 方法更高的分类精度。此外，方法可以 theoretically 控制预测误差，并在各种数据集上展示了优异的表现。<details>
<summary>Abstract</summary>
Deep Gaussian Process (DGP) models offer a powerful nonparametric approach for Bayesian inference, but exact inference is typically intractable, motivating the use of various approximations. However, existing approaches, such as mean-field Gaussian assumptions, limit the expressiveness and efficacy of DGP models, while stochastic approximation can be computationally expensive. To tackle these challenges, we introduce Neural Operator Variational Inference (NOVI) for Deep Gaussian Processes. NOVI uses a neural generator to obtain a sampler and minimizes the Regularized Stein Discrepancy in L2 space between the generated distribution and true posterior. We solve the minimax problem using Monte Carlo estimation and subsampling stochastic optimization techniques. We demonstrate that the bias introduced by our method can be controlled by multiplying the Fisher divergence with a constant, which leads to robust error control and ensures the stability and precision of the algorithm. Our experiments on datasets ranging from hundreds to tens of thousands demonstrate the effectiveness and the faster convergence rate of the proposed method. We achieve a classification accuracy of 93.56 on the CIFAR10 dataset, outperforming SOTA Gaussian process methods. Furthermore, our method guarantees theoretically controlled prediction error for DGP models and demonstrates remarkable performance on various datasets. We are optimistic that NOVI has the potential to enhance the performance of deep Bayesian nonparametric models and could have significant implications for various practical applications
</details>
<details>
<summary>摘要</summary>
深度泊松过程（DGP）模型提供了一种强大的非参数方法 для bayesian推理，但确切的推理通常是不可能的，这导致了不同的 aproximation 被使用。然而，现有的方法，如 Gaussian 假设，限制了 DGP 模型的表达能力和有效性，而随机approximation 可能会是 computationally  expensive。为了解决这些挑战，我们引入了 Neural Operator Variational Inference（NOVI） для Deep Gaussian Processes。NOVI 使用神经网络生成器来获取一个采样器，并将 Regularized Stein Discrepancy 在 L2 空间中减少到真 posterior 和生成的分布之间的差异。我们使用 Monte Carlo 估计和抽样化优化技术来解决最小最大问题。我们发现，我们的方法中引入的偏差可以通过多余的 Fisher 异同平方控制，从而保证算法的稳定性和精度。我们的实验结果表明，我们的方法可以在数据集规模从百万到万个数据点之间进行效果地训练，并且在 CIFAR10 数据集上达到了 93.56% 的分类精度，超过了现有的 Gaussian process 方法。此外，我们的方法可以 theoretically 控制 DGP 模型的预测错误，并在不同的数据集上显示出惊人的性能。我们认为 NOVI 有可能提高深度 Bayesian 非 Parametric 模型的性能，并可能在各种实际应用中具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Sequential-Action-Induced-Invariant-Representation-for-Reinforcement-Learning"><a href="#Sequential-Action-Induced-Invariant-Representation-for-Reinforcement-Learning" class="headerlink" title="Sequential Action-Induced Invariant Representation for Reinforcement Learning"></a>Sequential Action-Induced Invariant Representation for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12628">http://arxiv.org/abs/2309.12628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmu-xmu/sar">https://github.com/dmu-xmu/sar</a></li>
<li>paper_authors: Dayang Liang, Qihang Chen, Yunlong Liu</li>
<li>for: 提高visual reinforcement learning中 task-relevant state representation的学习精度，并在受到视觉干扰的环境中实现更好的性能。</li>
<li>methods: 基于bisimulation metric、prediction、contrast和重建等方法，提出Sequential Action–induced invariant Representation（SAR）方法，通过控制信号驱动encoder的优化，使代理人能够学习对干扰免疫的表示。</li>
<li>results: 在DeepMind Control suite任务上实现了最佳baseline的性能，并在实际的CARLA自动驾驶中证明了方法的有效性。 Code和示例视频可以在<a target="_blank" rel="noopener" href="https://github.com/DMU-XMU/SAR.git%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DMU-XMU/SAR.git中找到。</a><details>
<summary>Abstract</summary>
How to accurately learn task-relevant state representations from high-dimensional observations with visual distractions is a realistic and challenging problem in visual reinforcement learning. Recently, unsupervised representation learning methods based on bisimulation metrics, contrast, prediction, and reconstruction have shown the ability for task-relevant information extraction. However, due to the lack of appropriate mechanisms for the extraction of task information in the prediction, contrast, and reconstruction-related approaches and the limitations of bisimulation-related methods in domains with sparse rewards, it is still difficult for these methods to be effectively extended to environments with distractions. To alleviate these problems, in the paper, the action sequences, which contain task-intensive signals, are incorporated into representation learning. Specifically, we propose a Sequential Action--induced invariant Representation (SAR) method, in which the encoder is optimized by an auxiliary learner to only preserve the components that follow the control signals of sequential actions, so the agent can be induced to learn the robust representation against distractions. We conduct extensive experiments on the DeepMind Control suite tasks with distractions while achieving the best performance over strong baselines. We also demonstrate the effectiveness of our method at disregarding task-irrelevant information by deploying SAR to real-world CARLA-based autonomous driving with natural distractions. Finally, we provide the analysis results of generalization drawn from the generalization decay and t-SNE visualization. Code and demo videos are available at https://github.com/DMU-XMU/SAR.git.
</details>
<details>
<summary>摘要</summary>
如何准确地从高维观察数据中提取任务相关的状态表示是现实和挑战性的问题在视觉回归学中。在最近的无监督表示学方法基于 bisimulation 度量、对比、预测和重构方面，已经显示出提取任务相关信息的能力。但由于预测、对比和重构相关的方法中缺乏任务信息抽取的适当机制，以及 bisimulation 度量相关的方法在射频奖励下的局限性，使得这些方法在环境噪音中仍然具有困难。为解决这些问题，在本文中，我们提出了一种Sequential Action--induced invariant Representation（SAR）方法，其中扩展器是通过辅助学习器优化，以便只保留遵循控制信号的序列动作中的组件，以使代理人能够学习免斥噪音的Robust表示。我们在 DeepMind Control suite任务上进行了广泛的实验，并实现了强基eline的最高表现。我们还证明了我们的方法可以忽略任务无关的信息，通过将 SAR 应用于实际的 CARLA 基于自动驾驶中的自然噪音。最后，我们提供了一些总结和分析结果，包括通过总结衰减和 t-SNE 视觉化来证明代理人学习的一致性。代码和示例视频可以在 <https://github.com/DMU-XMU/SAR.git> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-Preference-Learning-Methods-for-Multiple-Criteria-Sorting-with-Temporal-Criteria"><a href="#Data-driven-Preference-Learning-Methods-for-Multiple-Criteria-Sorting-with-Temporal-Criteria" class="headerlink" title="Data-driven Preference Learning Methods for Multiple Criteria Sorting with Temporal Criteria"></a>Data-driven Preference Learning Methods for Multiple Criteria Sorting with Temporal Criteria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12620">http://arxiv.org/abs/2309.12620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Yijun, Guo Mengzhuo, Zhang Qingpeng</li>
<li>for: 本研究旨在提出新的偏好学习方法，用于多 criterion 排序问题中的时间序列数据处理。</li>
<li>methods: 本研究使用了一种固定时间折扣因子的几何 quadratic programming 模型，以及一种 ensemble learning 算法，可以将多个可能较弱的优化器的输出集成起来，并通过并行计算进行高效地执行。此外，本研究还提出了一种新的幂等 Recurrent Neural Network (mRNN)，可以捕捉时间序列中的偏好动态，并保持多重排序问题中的关键性质，如偏好幂等性、偏好独立性和自然排序。</li>
<li>results: 对于 synthetic 数据和一个实际案例（关于分类用户在 mobil 游戏中的历史行为序列），实验结果表明，提出的模型在与基准方法（包括机器学习、深度学习和传统多 criterion 排序方法）进行比较时，表现出了显著的性能改进。<details>
<summary>Abstract</summary>
The advent of predictive methodologies has catalyzed the emergence of data-driven decision support across various domains. However, developing models capable of effectively handling input time series data presents an enduring challenge. This study presents novel preference learning approaches to multiple criteria sorting problems in the presence of temporal criteria. We first formulate a convex quadratic programming model characterized by fixed time discount factors, operating within a regularization framework. Additionally, we propose an ensemble learning algorithm designed to consolidate the outputs of multiple, potentially weaker, optimizers, a process executed efficiently through parallel computation. To enhance scalability and accommodate learnable time discount factors, we introduce a novel monotonic Recurrent Neural Network (mRNN). It is designed to capture the evolving dynamics of preferences over time while upholding critical properties inherent to MCS problems, including criteria monotonicity, preference independence, and the natural ordering of classes. The proposed mRNN can describe the preference dynamics by depicting marginal value functions and personalized time discount factors along with time, effectively amalgamating the interpretability of traditional MCS methods with the predictive potential offered by deep preference learning models. Comprehensive assessments of the proposed models are conducted, encompassing synthetic data scenarios and a real-case study centered on classifying valuable users within a mobile gaming app based on their historical in-app behavioral sequences. Empirical findings underscore the notable performance improvements achieved by the proposed models when compared to a spectrum of baseline methods, spanning machine learning, deep learning, and conventional multiple criteria sorting approaches.
</details>
<details>
<summary>摘要</summary>
“预测方法的出现刺激了不同领域的数据驱动决策。然而，处理时间序列资料的模型建立仍然是一个持续的挑战。本研究提出了一些新的偏好学习方法，用于多个条件中的排序问题，包括时间条件。我们首先建立了一个固定时间折冲因子的对称quadratic programming模型，并在一个调整框架下进行运算。此外，我们提出了一个ensemble学习算法，用于结合多个、可能的弱来调整器的output，这个过程通过平行计算进行高效执行。为了增强可扩展性和可学习时间折冲因子，我们引入了一个新的对称复环神经网络（mRNN）。这个mRNN可以捕捉时间的演进 Dynamics 的偏好，同时维持多个条件问题的核心性质，包括条件单调性、偏好独立性和时间条件下的自然顺序。提出的mRNN可以描述偏好动态，包括时间条件下的贡献值函数和对个人时间折冲因子的描述，实现了传统多个条件排序方法的解释性和深度偏好学习模型的预测能力。实验结果显示，提出的模型在 synthetic 数据enario 和一个实际的移动游戏APP用户评分案例中均表现出色，与一系列基准方法相比，包括机器学习、深度学习和传统多个条件排序方法。”
</details></li>
</ul>
<hr>
<h2 id="Zero-Regret-Performative-Prediction-Under-Inequality-Constraints"><a href="#Zero-Regret-Performative-Prediction-Under-Inequality-Constraints" class="headerlink" title="Zero-Regret Performative Prediction Under Inequality Constraints"></a>Zero-Regret Performative Prediction Under Inequality Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12618">http://arxiv.org/abs/2309.12618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjing Yan, Xuanyu Cao</li>
<li>for: 本文研究了受约束的performative预测问题，即预测结果会影响未来数据分布的问题。</li>
<li>methods: 本文提出了一种robust预测框架，可以在约束条件下实现高效的预测。此外，本文还提出了一种适应预测算法，可以在各种场景下实现优化的预测。</li>
<li>results: 本文的分析表明，提出的适应预测算法可以在约束条件下实现$\ca{O}(\sqrt{T})$的违规和约束违宪，使用只有$\sqrt{T} + 2T$个样本。这是首次对performative预测问题的优化问题进行分析和研究。<details>
<summary>Abstract</summary>
Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained scenarios, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradients is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\ca{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations.
</details>
<details>
<summary>摘要</summary>
Performative 预测是一种最近提出的框架，在预测导向决策的过程中，预测结果会影响未来数据分布。这种 performative 现象在交通、金融、公共政策和推荐系统等领域都是非常普遍的。然而，现有的工作都是在不受限制的情况下进行预测，忽略了现实世界学习问题往往受到限制。这篇论文尝试填补这个空白，通过研究 performative 预测下 inequality 约束来解决这个问题。不同于大多数现有的工作，我们不仅提供 performative 稳定点，而是寻找最佳解决方案。预测 performative Gradient 是一项非常困难的任务，因为 performative 对数据分布的影响是agnostic的。为 Addressing this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\ca{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations.
</details></li>
</ul>
<hr>
<h2 id="ARRQP-Anomaly-Resilient-Real-time-QoS-Prediction-Framework-with-Graph-Convolution"><a href="#ARRQP-Anomaly-Resilient-Real-time-QoS-Prediction-Framework-with-Graph-Convolution" class="headerlink" title="ARRQP: Anomaly Resilient Real-time QoS Prediction Framework with Graph Convolution"></a>ARRQP: Anomaly Resilient Real-time QoS Prediction Framework with Graph Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02269">http://arxiv.org/abs/2310.02269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suraj Kumar, Soumi Chattopadhyay<br>for: 这种研究旨在提高现代服务套件架构中的质量服务（QoS）预测精度，以便用户可以根据预测结果做出了 Informed 决策。methods: 这种预测框架（名为 ARRQP）利用图 convolution 技术捕捉用户和服务之间的复杂关系和依赖关系，即使数据是有限或缺失的。 ARRQP 集成了上下文信息和协同信息，以获得用户-服务交互的全面理解。 results: 对 WS-DREAM 测试集的实验表明，这种预测框架可以准确地预测 QoS，并且在各种异常情况下保持高度的稳定性。<details>
<summary>Abstract</summary>
In the realm of modern service-oriented architecture, ensuring Quality of Service (QoS) is of paramount importance. The ability to predict QoS values in advance empowers users to make informed decisions. However, achieving accurate QoS predictions in the presence of various issues and anomalies, including outliers, data sparsity, grey-sheep instances, and cold-start scenarios, remains a challenge. Current state-of-the-art methods often fall short when addressing these issues simultaneously, resulting in performance degradation. In this paper, we introduce a real-time QoS prediction framework (called ARRQP) with a specific emphasis on improving resilience to anomalies in the data. ARRQP utilizes the power of graph convolution techniques to capture intricate relationships and dependencies among users and services, even when the data is limited or sparse. ARRQP integrates both contextual information and collaborative insights, enabling a comprehensive understanding of user-service interactions. By utilizing robust loss functions, ARRQP effectively reduces the impact of outliers during the model training. Additionally, we introduce a sparsity-resilient grey-sheep detection method, which is subsequently treated separately for QoS prediction. Furthermore, we address the cold-start problem by emphasizing contextual features over collaborative features. Experimental results on the benchmark WS-DREAM dataset demonstrate the framework's effectiveness in achieving accurate and timely QoS predictions.
</details>
<details>
<summary>摘要</summary>
在现代服务套件架构中，保证服务质量（QoS）的重要性不言而喻。预测QoS值的能力使用户做出了 Informed 决策。然而，在面临各种问题和异常情况，包括异常值、数据稀缺、灰羊实例和冷启动场景时，实现准确的QoS预测仍然是一大挑战。当前的状态艺术方法经常在同时处理这些问题时表现不佳，导致性能下降。在这篇论文中，我们提出了一个实时QoS预测框架（叫做ARRQP），强调改进数据中异常现象的抗逆性。ARRQP利用图 convolution 技术捕捉用户和服务之间的复杂关系和依赖关系，即使数据稀缺或异常。ARRQP结合了上下文信息和协同知识，使得用户-服务交互的全面理解。通过使用robust 损失函数，ARRQP减少了模型训练中异常值的影响。此外，我们还提出了稀缺灰羊检测方法，并将其与QoS预测分开处理。此外，我们解决冷启动问题，强调上下文特征而不是协同特征。实验结果表明，ARRQP在WS-DREAM 数据集上实现了准确和时间性的QoS预测。
</details></li>
</ul>
<hr>
<h2 id="Multiply-Robust-Federated-Estimation-of-Targeted-Average-Treatment-Effects"><a href="#Multiply-Robust-Federated-Estimation-of-Targeted-Average-Treatment-Effects" class="headerlink" title="Multiply Robust Federated Estimation of Targeted Average Treatment Effects"></a>Multiply Robust Federated Estimation of Targeted Average Treatment Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12600">http://arxiv.org/abs/2309.12600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Larry Han, Zhu Shen, Jose Zubizarreta</li>
<li>for: 这 paper 是为了 Derive valid causal inferences for a target population using multi-site data.</li>
<li>methods: 这 paper 使用了一种 novel federated approach, 包括 covariate shift and covariate mismatch between sites 的 adjustment, 以及 transfer learning 来 estimate ensemble weights to combine information from source sites.</li>
<li>results: 这 paper 的研究结果表明，这种方法在不同的 scenario 下具有高效和可靠的特点，并且在 finite sample 上有效性和稳定性比 existed approach 更高.<details>
<summary>Abstract</summary>
Federated or multi-site studies have distinct advantages over single-site studies, including increased generalizability, the ability to study underrepresented populations, and the opportunity to study rare exposures and outcomes. However, these studies are challenging due to the need to preserve the privacy of each individual's data and the heterogeneity in their covariate distributions. We propose a novel federated approach to derive valid causal inferences for a target population using multi-site data. We adjust for covariate shift and covariate mismatch between sites by developing multiply-robust and privacy-preserving nuisance function estimation. Our methodology incorporates transfer learning to estimate ensemble weights to combine information from source sites. We show that these learned weights are efficient and optimal under different scenarios. We showcase the finite sample advantages of our approach in terms of efficiency and robustness compared to existing approaches.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:多站或联合研究有许多优点，包括增加了一般化性、研究少数群体和罕见的暴露和结果。然而，这些研究具有保护每个个体数据隐私和不同站点的 covariate 分布异常性的挑战。我们提出了一种新的联邦方法，以 derivation 适用于目标人口的有效 causal inference。我们对 covariate shift 和 covariate mismatch 进行了修正，并通过开发多重可靠和隐私保护的干扰函数估计。我们的方法包括使用转移学习来估计ensemble weights，将多个源站的信息组合。我们显示了这些学习到的权重是有效的和优化的在不同的场景下。我们还展示了我们的方法在规模和稳定性方面的较好的 finite sample 优势，与现有方法相比。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The Traditional Chinese version is also available upon request.
</details></li>
</ul>
<hr>
<h2 id="Learning-algorithms-for-identification-of-whisky-using-portable-Raman-spectroscopy"><a href="#Learning-algorithms-for-identification-of-whisky-using-portable-Raman-spectroscopy" class="headerlink" title="Learning algorithms for identification of whisky using portable Raman spectroscopy"></a>Learning algorithms for identification of whisky using portable Raman spectroscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13087">http://arxiv.org/abs/2309.13087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kwang Jun Lee, Alexander C. Trowbridge, Graham D. Bruce, George O. Dwapanyin, Kylie R. Dunning, Kishan Dholakia, Erik P. Schartner</li>
<li>for: 鉴定高值饮料的可靠性是一个日益重要的领域，因为问题如品牌替换（即虚假产品）和质量控制对行业是关键的。</li>
<li>methods: 我们检查了一系列机器学习算法，并将其直接与可携带式拉曼谱仪device进行了交互，以 both identify和 characterize commercial whisky samples的 ethanol&#x2F;methanol浓度。</li>
<li>results: 我们示出了机器学习模型可以在二十八个商业样本中实现超过99%的品牌认定率。此外，我们还使用了同样的样本和算法来量化 ethanol浓度，以及在杂入 whisky 样本中测量 methanol 水平。我们的机器学习技术然后与通过瓶装置进行spectral analysis和标识，不需要样本从原始容器中抽取，这表明了这种方法在检测假冒或杂入饮料和其他高值液体样本中的实际潜力。<details>
<summary>Abstract</summary>
Reliable identification of high-value products such as whisky is an increasingly important area, as issues such as brand substitution (i.e. fraudulent products) and quality control are critical to the industry. We have examined a range of machine learning algorithms and interfaced them directly with a portable Raman spectroscopy device to both identify and characterize the ethanol/methanol concentrations of commercial whisky samples. We demonstrate that machine learning models can achieve over 99% accuracy in brand identification across twenty-eight commercial samples. To demonstrate the flexibility of this approach we utilised the same samples and algorithms to quantify ethanol concentrations, as well as measuring methanol levels in spiked whisky samples. Our machine learning techniques are then combined with a through-the-bottle method to perform spectral analysis and identification without requiring the sample to be decanted from the original container, showing the practical potential of this approach to the detection of counterfeit or adulterated spirits and other high value liquid samples.
</details>
<details>
<summary>摘要</summary>
stable 识别高值产品，如威士忌，在当今越来越重要，因为问题如品牌替换（即假冒产品）和质量控制是行业关键。我们已经审查了一系列机器学习算法，并直接与可携带式拉曼谱仪器集成以识别和Characterize商业威士忌样本中的丙醇/甲醇浓度。我们示出了机器学习模型可以在28个商业样本中达到99%以上的品牌识别率。为了 demonstarte 这种方法的灵活性，我们使用了相同的样本和算法来量化丙醇浓度，以及测量杂入威士忌样本中的甲醇含量。我们的机器学习技术然后与通过瓶子方法进行spectral analysis和识别，无需将样本从原始容器中抽取，显示了这种方法在检测假冒或杂入饮料和其他高值液体样本中的实际潜力。
</details></li>
</ul>
<hr>
<h2 id="Sampling-Frequency-Independent-Universal-Sound-Separation"><a href="#Sampling-Frequency-Independent-Universal-Sound-Separation" class="headerlink" title="Sampling-Frequency-Independent Universal Sound Separation"></a>Sampling-Frequency-Independent Universal Sound Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12581">http://arxiv.org/abs/2309.12581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomohiko Nakamura, Kohei Yatabe</li>
<li>for: 这个论文提出了一种能够处理未经训练的采样频率（SF）的通用声音分离（USS）方法，用于分离不同类型的源 signal。</li>
<li>methods: 该方法使用了我们之前提出的SF独立（SFI）扩展，使用SFI convolutional layers来处理不同SF。</li>
<li>results: 实验表明，信号重采样可能会降低USS性能，而我们提出的方法在不同SF下表现更一致。<details>
<summary>Abstract</summary>
This paper proposes a universal sound separation (USS) method capable of handling untrained sampling frequencies (SFs). The USS aims at separating arbitrary sources of different types and can be the key technique to realize a source separator that can be universally used as a preprocessor for any downstream tasks. To realize a universal source separator, there are two essential properties: universalities with respect to source types and recording conditions. The former property has been studied in the USS literature, which has greatly increased the number of source types that can be handled by a single neural network. However, the latter property (e.g., SF) has received less attention despite its necessity. Since the SF varies widely depending on the downstream tasks, the universal source separator must handle a wide variety of SFs. In this paper, to encompass the two properties, we propose an SF-independent (SFI) extension of a computationally efficient USS network, SuDoRM-RF. The proposed network uses our previously proposed SFI convolutional layers, which can handle various SFs by generating convolutional kernels in accordance with an input SF. Experiments show that signal resampling can degrade the USS performance and the proposed method works more consistently than signal-resampling-based methods for various SFs.
</details>
<details>
<summary>摘要</summary>
To address this challenge, the proposed method extends a computationally efficient USS network, SuDoRM-RF, with an SF-independent (SFI) extension. The proposed network uses SFI convolutional layers that can handle various SFs by generating convolutional kernels in accordance with the input SF. This allows the network to maintain its performance across different SFs. Experimental results show that signal resampling can degrade the USS performance, and the proposed method outperforms signal-resampling-based methods for various SFs.In simplified Chinese, the text would be:这篇论文提出了一种能处理不受训练 sampling frequency (SF) 的通用声音分离 (USS) 方法。USS 目标是分离不同类型的原始源，并且可以是下游任务的键技术。为实现这一目标，需要两个关键属性：对于源类型和记录条件的通用性。前者已经在 USS 文献中得到了大量的研究，但是后者（即 SF）尚未得到了 suficient 的关注，尽管它的重要性。由于 SF 在下游任务中变化广泛，通用的源分离器必须能处理多种 SF。为此，我们提议一种 SF-独立 (SFI) 的扩展，使用我们之前提出的 SFI 卷积层，可以根据输入 SF 生成卷积 kernel。实验显示，signal resampling 可能会降低 USS 性能，而我们提议的方法在不同 SF 下表现更稳定。
</details></li>
</ul>
<hr>
<h2 id="SPION-Layer-Wise-Sparse-Training-of-Transformer-via-Convolutional-Flood-Filling"><a href="#SPION-Layer-Wise-Sparse-Training-of-Transformer-via-Convolutional-Flood-Filling" class="headerlink" title="SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling"></a>SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12578">http://arxiv.org/abs/2309.12578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bokyeong Yoon, Yoonsang Han, Gordon Euhyun Moon</li>
<li>for: 这篇论文旨在提高Transformer模型的训练效率和内存压缩，以提高模型的运算效率和评估质量。</li>
<li>methods: 本论文提出了一种新的对Transformer模型进行实体化的方法，使用了对于层次的滤波器和淹水填充方法，以提高对于注意力操作的实体化效率。</li>
<li>results: 本论文的实验结果显示，使用了本方法可以实现Transformer模型的训练时间和内存压缩，并且可以维持评估质量。具体来说，本论文可以在GPU上实现快速的实体化执行，并且可以比起现有的紧缩Transformer模型实现3.08倍的速度提升，同时保持评估质量。<details>
<summary>Abstract</summary>
Sparsifying the Transformer has garnered considerable interest, as training the Transformer is very computationally demanding. Prior efforts to sparsify the Transformer have either used a fixed pattern or data-driven approach to reduce the number of operations involving the computation of multi-head attention, which is the main bottleneck of the Transformer. However, existing methods suffer from inevitable problems, such as the potential loss of essential sequence features due to the uniform fixed pattern applied across all layers, and an increase in the model size resulting from the use of additional parameters to learn sparsity patterns in attention operations. In this paper, we propose a novel sparsification scheme for the Transformer that integrates convolution filters and the flood filling method to efficiently capture the layer-wise sparse pattern in attention operations. Our sparsification approach reduces the computational complexity and memory footprint of the Transformer during training. Efficient implementations of the layer-wise sparsified attention algorithm on GPUs are developed, demonstrating a new SPION that achieves up to 3.08X speedup over existing state-of-the-art sparse Transformer models, with better evaluation quality.
</details>
<details>
<summary>摘要</summary>
减少Transformer的计算复杂性得到了广泛关注，因为训练Transformer很计算昂贵。先前的减少方法包括使用固定模式或数据驱动方法来减少多头注意力计算的数量，但现有方法受到不可避免的问题，如所有层都应用 uniform 固定模式，导致可能丢失重要的序列特征，并且使用更多参数来学习注意力操作的缺省模式。在这篇论文中，我们提出了一种新的减少方案，将 convolution 筛选器和淹水填充方法结合使用，以高效地捕捉层 wise  sparse 模式在注意力操作中。我们的减少方法可以在训练过程中降低Transformer的计算复杂性和内存占用。我们实现了层 wise 减少的注意力算法在GPU上，并达到了3.08倍的速度提升，与评价质量相对较好的现有 sparse Transformer 模型相比。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Network-Resilience-through-Machine-Learning-powered-Graph-Combinatorial-Optimization-Applications-in-Cyber-Defense-and-Information-Diffusion"><a href="#Enhancing-Network-Resilience-through-Machine-Learning-powered-Graph-Combinatorial-Optimization-Applications-in-Cyber-Defense-and-Information-Diffusion" class="headerlink" title="Enhancing Network Resilience through Machine Learning-powered Graph Combinatorial Optimization: Applications in Cyber Defense and Information Diffusion"></a>Enhancing Network Resilience through Machine Learning-powered Graph Combinatorial Optimization: Applications in Cyber Defense and Information Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10667">http://arxiv.org/abs/2310.10667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diksha Goel</li>
<li>For: This paper focuses on developing effective approaches for enhancing network resilience in cyber defense and information diffusion application domains.* Methods: The paper transforms the problems of discovering bottleneck edges and structural hole spanner nodes into graph-combinatorial optimization problems and designs machine learning-based approaches to discover bottleneck points vital for network resilience.* Results: The paper aims to provide effective, efficient, and scalable techniques for enhancing network resilience in specific application domains.Here is the simplified Chinese version of the three key information points:</li>
<li>for: 这篇论文关注于在网络防御和信息传播应用领域中提高网络可恢复性。</li>
<li>methods: 论文将瓶颈边缘和结构孔挫节点的问题转化为图谱-组合优化问题，并采用机器学习方法来找出网络中瓶颈点。</li>
<li>results: 论文目标是为特定应用领域提供有效、高效和可扩展的网络可恢复性提高方法。<details>
<summary>Abstract</summary>
With the burgeoning advancements of computing and network communication technologies, network infrastructures and their application environments have become increasingly complex. Due to the increased complexity, networks are more prone to hardware faults and highly susceptible to cyber-attacks. Therefore, for rapidly growing network-centric applications, network resilience is essential to minimize the impact of attacks and to ensure that the network provides an acceptable level of services during attacks, faults or disruptions. In this regard, this thesis focuses on developing effective approaches for enhancing network resilience. Existing approaches for enhancing network resilience emphasize on determining bottleneck nodes and edges in the network and designing proactive responses to safeguard the network against attacks. However, existing solutions generally consider broader application domains and possess limited applicability when applied to specific application areas such as cyber defense and information diffusion, which are highly popular application domains among cyber attackers.   This thesis aims to design effective, efficient and scalable techniques for discovering bottleneck nodes and edges in the network to enhance network resilience in cyber defense and information diffusion application domains. We first investigate a cyber defense graph optimization problem, i.e., hardening active directory systems by discovering bottleneck edges in the network. We then study the problem of identifying bottleneck structural hole spanner nodes, which are crucial for information diffusion in the network. We transform both problems into graph-combinatorial optimization problems and design machine learning based approaches for discovering bottleneck points vital for enhancing network resilience.
</details>
<details>
<summary>摘要</summary>
随着计算和网络通信技术的不断发展，网络基础设施和其应用环境变得越来越复杂，因此网络更容易受到硬件故障和攻击。为了应对这些攻击和故障，网络可靠性变得非常重要，以确保网络在攻击或故障时仍能提供可接受的服务。在这个视角下，这个论文将关注开发有效的网络可靠性提升方法。现有的网络可靠性提升方法通常是通过确定网络中瓶须节点和边来预防攻击。然而，现有的解决方案通常只适用于更广泛的应用领域，而不是特定的应用领域，如网络防御和信息传播，这些应用领域在网络攻击者中非常受欢迎。这个论文的目标是为网络防御和信息传播应用领域提供有效、高效和可扩展的瓶须节点和边发现方法，以提升网络可靠性。我们首先研究了网络防御图优化问题，即通过发现网络中瓶须边来强化网络防御。然后，我们研究了网络中瓶须结构孔隙节点的问题，这些节点对于信息传播非常重要。我们将这两个问题转化为图-数学优化问题，并使用机器学习方法来发现瓶须点，以提升网络可靠性。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Illustration-of-Interleaved-Learning-using-Kalman-Filter-for-Linear-Least-Squares"><a href="#A-Simple-Illustration-of-Interleaved-Learning-using-Kalman-Filter-for-Linear-Least-Squares" class="headerlink" title="A Simple Illustration of Interleaved Learning using Kalman Filter for Linear Least Squares"></a>A Simple Illustration of Interleaved Learning using Kalman Filter for Linear Least Squares</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03751">http://arxiv.org/abs/2310.03751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Majnu John, Yihren Wu</li>
<li>for: 提出了一种基于Kalman Filter的线性最小二乘算法的机器学习算法协调学习机制。</li>
<li>methods: 使用了Kalman Filter来实现线性最小二乘算法中的协调学习机制。</li>
<li>results: 通过实验证明了该算法的效果。<details>
<summary>Abstract</summary>
Interleaved learning in machine learning algorithms is a biologically inspired training method with promising results. In this short note, we illustrate the interleaving mechanism via a simple statistical and optimization framework based on Kalman Filter for Linear Least Squares.
</details>
<details>
<summary>摘要</summary>
生物学中的混合学习（Interleaved learning）是一种机器学习算法中的训练方法，具有承诺的成果。本短记将通过简单的统计和优化框架，基于加尔曼缓冲器进行线性最小二乘问题的示例阐释interleaving机制。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/22/cs.LG_2023_09_22/" data-id="clpztdnl900s8es880u5rdiq1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/22/eess.SP_2023_09_22/" class="article-date">
  <time datetime="2023-09-22T08:00:00.000Z" itemprop="datePublished">2023-09-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/22/eess.SP_2023_09_22/">eess.SP - 2023-09-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Survey-of-Brain-Computer-Interface-Using-Non-Invasive-Methods"><a href="#A-Survey-of-Brain-Computer-Interface-Using-Non-Invasive-Methods" class="headerlink" title="A Survey of Brain Computer Interface Using Non-Invasive Methods"></a>A Survey of Brain Computer Interface Using Non-Invasive Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13151">http://arxiv.org/abs/2309.13151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritam Ghosh</li>
<li>for: 这篇论文主要用于探讨脑机器接口（BCI）技术的优缺点，以及一些非侵入式技术的应用场景。</li>
<li>methods: 本文使用了脑电图（EEG）、功能磁共振成像（fMRI）、近红外 спектроскопия（NIRs）和混合系统等非侵入式技术。</li>
<li>results: 本文总结了这些非侵入式技术的优点和缺点，并展示了它们在各种应用场景中的应用。<details>
<summary>Abstract</summary>
Research on Brain-Computer Interface (BCI) began in the 1970s and has increased in volume and diversified significantly since then. Today BCI is widely used for applications like assistive devices for physically challenged users, mental state monitoring, input devices for hands-free applications, marketing, education, security, games and entertainment. This article explores the advantages and disadvantages of invasive and non-invasive BCI technologies and focuses on use cases of several non-invasive technologies, namely electroencephalogram (EEG), functional Magnetic Resonance Imaging (fMRI), Near Infrared Spectroscopy (NIRs) and hybrid systems.
</details>
<details>
<summary>摘要</summary>
研究Brain-Computer Interface（BCI）始于1970年代，自 then onwards 已经增加了量和多样化了很多。今天，BCI 广泛应用于帮助 físically challenged 用户、监测 mental state、手sfree 应用程序的输入设备、marketing、教育、安全、游戏和娱乐等领域。本文介绍了 BCIs 的优势和缺点，并关注了多种非侵入式技术的应用情况，namely 电энцефаogram（EEG）、功能磁共振成像（fMRI）、近红外 спектроскопи（NIRs）和混合系统。
</details></li>
</ul>
<hr>
<h2 id="Performance-Evaluation-for-Subarray-based-Reconfigurable-Intelligent-Surface-Aided-Wireless-Communication-Systems"><a href="#Performance-Evaluation-for-Subarray-based-Reconfigurable-Intelligent-Surface-Aided-Wireless-Communication-Systems" class="headerlink" title="Performance Evaluation for Subarray-based Reconfigurable Intelligent Surface-Aided Wireless Communication Systems"></a>Performance Evaluation for Subarray-based Reconfigurable Intelligent Surface-Aided Wireless Communication Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12977">http://arxiv.org/abs/2309.12977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Yang, Weicong Chen, Xiao Li, Shi Jin</li>
<li>for: 提高无线通信系统性能，研究基于减数阵的智能表面协助系统</li>
<li>methods: 使用基于减数阵的协助技术，分析和优化协助系统中的反射率和谱系数</li>
<li>results: 提出基于减数阵的协助系统可以提高系统的均衡 spectral efficiency 和能效率，并且可以同时降低功率消耗和反射率Note: The above results are in Simplified Chinese text.<details>
<summary>Abstract</summary>
Reconfigurable intelligent surfaces (RISs) have received extensive concern to improve the performance of wireless communication systems. In this paper, a subarray-based scheme is investigated in terms of its effects on ergodic spectral efficiency (SE) and energy efficiency (EE) in RIS-assisted systems. In this scheme, the adjacent elements divided into a subarray are controlled by one signal and share the same reflection coefficient. An upper bound of ergodic SE is derived and an optimal phase shift design is proposed for the subarray-based RIS. Based on the upper bound and optimal design, we obtain the maximum of the upper bound. In particular, we analytically evaluate the effect of the subarray-based RIS on EE since it reduces SE and power consumption simultaneously. Numerical results verify the tightness of the upper bound, demonstrate the effectiveness of the optimal phase shift design for the subarray-based RIS, and reveal the effects of the subarray-based scheme on SE and EE.
</details>
<details>
<summary>摘要</summary>
改进无线通信系统性能的重 Configurable intelligent surfaces (RISs) 已经引起了广泛关注。本文 investigate 一种基于subarray的方案，包括其对ergodic spectral efficiency (SE) 和能效率 (EE) 的影响。在这种方案中，邻近元素被分成一个subarray，并由一个信号控制，共享相同的反射系数。我们 derive 一个上限 bound 的ergodic SE，并提出了一种优化的相位偏移设计。基于上限 bound 和优化设计，我们获得了最大的上限。具体来说，我们分析了subarray-based RIS 对EE的影响，因为它同时降低了SE和功率消耗。 numerically 结果证明了上限 bound 的紧张性，证明了优化相位偏移设计的效果，并揭示了subarray-based scheme 对SE和EE的影响。
</details></li>
</ul>
<hr>
<h2 id="Guaranteed-Private-Communication-with-Secret-Block-Structure"><a href="#Guaranteed-Private-Communication-with-Secret-Block-Structure" class="headerlink" title="Guaranteed Private Communication with Secret Block Structure"></a>Guaranteed Private Communication with Secret Block Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12949">http://arxiv.org/abs/2309.12949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Ferreira Da Costa, Jianxiu Li, Urbashi Mitra</li>
<li>for: 这篇论文提出了一种新的私人通信框架，通过在通道实例上传输线性逆问题，使得隐私被引入。这个框架的安全性基于发送方和合法接收方之间的秘密知识。</li>
<li>methods: 这篇论文使用了一种基于秘密块结构的协议，使得接收方可以从不充足的线性测量中解码块稀热消息。这种协议可以应用于实际的多Access无线通信系统中。</li>
<li>results: 研究表明，在某些特定的频率和传输参数下，伪装者可以尝试从通道输出的四次趋势中提取秘密块结构。然而，计算一个统计下界，表明该提出的四次趋势秘密块估计策略几乎是优化的。此外，研究表明，通过spectral clustering算法，可以定义扩大秘密键的时间长度，以确保通信的安全性。<details>
<summary>Abstract</summary>
A novel private communication framework is proposed where privacy is induced by transmitting over channel instances of linear inverse problems that are identifiable to the legitimate receiver, but unidentifiable to an eavesdropper. The gap in identifiability is created in the framework by leveraging secret knowledge between the transmitter and the legitimate receiver. Specifically, the case where the legitimate receiver harnesses a secret block structure to decode a transmitted block-sparse message from underdetermined linear measurements in conditions where classical compressed sensing would provably fail is examined. The applicability of the proposed scheme to practical multiple access wireless communication systems is discussed. The protocol's privacy is studied under a single transmission, and under multiple transmissions without refreshing the secret block structure. It is shown that, under a specific scaling of the channel dimensions and transmission parameters, the eavesdropper can attempt to overhear the block structure from the fourth-order moments of the channel output. Computation of a statistical lower bound, suggests that the proposed fourth-order moment secret block estimation strategy is near optimal. The performance of a spectral clustering algorithm is studied to that end, defining scaling laws on the lifespan of the secret key before the communication is compromised. Finally, numerical experiments corroborating the theoretical findings are conducted.
</details>
<details>
<summary>摘要</summary>
一种新的私人通信框架被提议，其中隐私是通过在通道上传输线性逆问题的实例，这些问题只能被合法接收者识别出来，但不能被侦测者识别出来。在这个框架中，通过 transmitter 和合法接收者之间的秘密知识来创造不可识别的差异。例如，在 transmitter 将块稀疏消息从不充分的线性测量中解码的情况下，合法接收者可以利用秘密块结构来解码消息。本文探讨了这种方案在实际多接入无线通信系统中的可行性，并研究了协议的隐私性。在单次传输和多次传输无需刷新秘密块结构的情况下，分析表明，在某些频率缩放和传输参数的情况下，侦测者可以尝试从通道输出的四次 moments 中找到秘密块结构。计算统计下界，表明该提议的四次 moment 秘密块估计策略是近似优美的。此外，对spectral clustering算法的研究表明，在某些频率缩放和传输参数的情况下，秘密钥的寿命会随着通信的增加而减少。最后，通过实验证明了理论发现的结论。
</details></li>
</ul>
<hr>
<h2 id="A-Proof-of-Concept-for-OTFS-Resilience-in-Doubly-Selective-Channels-by-GPU-Enabled-Real-Time-SDR"><a href="#A-Proof-of-Concept-for-OTFS-Resilience-in-Doubly-Selective-Channels-by-GPU-Enabled-Real-Time-SDR" class="headerlink" title="A Proof of Concept for OTFS Resilience in Doubly-Selective Channels by GPU-Enabled Real-Time SDR"></a>A Proof of Concept for OTFS Resilience in Doubly-Selective Channels by GPU-Enabled Real-Time SDR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12861">http://arxiv.org/abs/2309.12861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Xien Yap, Neil Bhushan, Onur Dizdar, Ata Sattarzadeh, David Redgate, Venkateswara Battula, Stephen Wang</li>
<li>for: 该论文旨在研究Orthogonal Time Frequency Space (OTFS) 模ulation技术，并在实际的实时Software Defined Radio (SDR) 设置中进行实验研究。</li>
<li>methods: 该论文使用了一个基于 Graphical Processing Unit (GPU) 的信号处理程序，以及 Universal Software Radio Peripheral (USRP) 设备来实现一个低延迟的接收结构，并在不同的Doppler值下调查其性能。</li>
<li>results: 研究结果表明，OTFS 比 OFDM 更高度具有对双选择通道的鲁棒性，并在实际实验中表现出色。<details>
<summary>Abstract</summary>
Orthogonal time frequency space (OTFS) is a modulation technique which is robust against the disruptive effects of doubly-selective channels. In this paper, we perform an experimental study of OTFS by a real-time software defined radio (SDR) setup. Our SDR consists of a Graphical Processing Unit (GPU) for signal processing programmed using Sionna and TensorFlow, and Universal Software Radio Peripheral (USRP) devices for air interface. We implement a low-latency transceiver structure for OTFS and investigate its performance under various Doppler values. By comparing the performance of OTFS with Orthogonal Frequency Division Multiplexing (OFDM), we demonstrate that OTFS is highly robust against the disruptive effects of doubly-selective channels in a real-time experimental setup.
</details>
<details>
<summary>摘要</summary>
水平时频空间（OTFS）是一种干扰强度较弱的模调技术，可以在双 selektiv通道中具有高Robustness。在这篇论文中，我们通过实验研究了OTFS，使用了一个真实时间定制的Software Defined Radio（SDR）设置。我们的SDR包括一个图形处理器（GPU）用于信号处理，并使用Sionna和TensorFlow编程，以及Universal Software Radio Peripheral（USRP）设备用于空中接口。我们实现了一种低延迟的接收结构，并在不同的Doppler值下调查其性能。通过对OTFS和Orthogonal Frequency Division Multiplexing（OFDM）的比较，我们表明了OTFS在真实时间实验设置中对双 selektiv通道的破坏性影响具有高Robustness。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Satellites-Collaboration-for-Joint-Code-aided-CFOs-and-CPOs-Estimation"><a href="#Multiple-Satellites-Collaboration-for-Joint-Code-aided-CFOs-and-CPOs-Estimation" class="headerlink" title="Multiple Satellites Collaboration for Joint Code-aided CFOs and CPOs Estimation"></a>Multiple Satellites Collaboration for Joint Code-aided CFOs and CPOs Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12828">http://arxiv.org/abs/2309.12828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pingyue Yue, Yixuan Li, Yue Li, Rui Zhang, Shuai Wang, Jianping An</li>
<li>For: 提高低Earth轨道卫星网络的安全性和可靠性* Methods: 使用合作卫星技术，并提出了一种基于编码的迭代参数估计算法来解决低信号噪比下的参数估计挑战* Results:  simulation results show that the proposed algorithm can approach Bit Error Rate (BER) performance bound within 0.4 dB with regards to four-satellite collaboration.<details>
<summary>Abstract</summary>
Low Earth Orbit (LEO) satellites are being extensively researched in the development of secure Internet of Remote Things (IoRT). In scenarios with miniaturized terminals, the limited transmission power and long transmission distance often lead to low Signal-to-Noise Ratio (SNR) at the satellite receiver, which degrades communication performance. A solution to address this issue is the utilization of cooperative satellites, which can combine signals received from multiple satellites, thereby significantly improve SNR. However, in order to maximize the combination gain, the signal coherent combining is necessary, which requires the carrier frequency and phase of each receiving signal to be aligned.   Under low SNR circumstances, carrier parameter estimation can be a significant challenge, especially for short burst transmission with no training sequence. In order to tackle it, we propose an iterative code-aided estimation algorithm for joint Carrier Frequency Offset (CFO) and Carrier Phase Offset (CPO). The Cram\'er-Rao Lower Bound (CRLB) is suggested as the limit on the parameter estimation performance. Simulation results demonstrate that the proposed algorithm can approach Bit Error Rate (BER) performance bound within 0.4 dB with regards to four-satellite collaboration.
</details>
<details>
<summary>摘要</summary>
低地球轨道（LEO）卫星在网络 remote Things（IoRT）的开发中被广泛研究。在具有小型终端的场景下，由于传输功率和传输距离都很小，因此在卫星接收器上常常出现低信噪比（SNR），这会降低通信性能。为解决这个问题，可以利用合作卫星，即将多个卫星接收器的信号合并，从而显著提高SNR。然而，为了最大化合并增益，需要进行信号干涉合并，这需要每个接收信号的干涉频率和相位相同。在低SNR情况下，干涉参数估计可能是一个 significiant挑战，特别是在短暂的传输中没有训练序列。为解决这个问题，我们提出了一种迭代码帮助估计算法，用于同时估计干涉频率偏移（CFO）和干涉相位偏移（CPO）。对于四个卫星的合作，我们提出的算法可以在0.4dB之内 approaching Bit Error Rate（BER）性能 bound。
</details></li>
</ul>
<hr>
<h2 id="Alteration-of-skeletal-muscle-energy-metabolism-assessed-by-31P-MRS-in-clinical-routine-part-1-Advanced-Quality-Control-pipeline"><a href="#Alteration-of-skeletal-muscle-energy-metabolism-assessed-by-31P-MRS-in-clinical-routine-part-1-Advanced-Quality-Control-pipeline" class="headerlink" title="Alteration of skeletal muscle energy metabolism assessed by 31P MRS in clinical routine, part 1: Advanced Quality Control pipeline"></a>Alteration of skeletal muscle energy metabolism assessed by 31P MRS in clinical routine, part 1: Advanced Quality Control pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12796">http://arxiv.org/abs/2309.12796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Naëgel, Hélène Ratiney, Jabrane Karkouri, Djahid Kennouche, Nicolas Royer, Jill M Slade, Jérôme Morel, Pierre Croisille, Magalie Viallon</li>
<li>for: 本研究目的是提供一种基于Literature current recommendations和临床经验所提出的数据质量控制方法，以帮助在 dynamic 31P-MRS 数据处理中获得可靠的结果。</li>
<li>methods: 本研究使用了一种三元组合的数据质量控制方法，包括对数据进行自适应标准化、对数据进行质量控制分数（QCS）的计算、并对数据进行手动审核。</li>
<li>results: 使用QCS可以快速标识具有数据异常的 subjects，并提供对数据进行修正的指导。总的来说，QCS使得可以自动分类45%的 subjects，其中58名参与者的数据没有规则违反，21名参与者的数据需要拒绝。此外，手动审核还可以Acceptance of full datasets from an additional 80 participants and recovery phase data from an additional 16 subjects。总之，patient数据中出现了更多的异常（35%的dataset），比healthy controls（15%的dataset）更高。<details>
<summary>Abstract</summary>
Background: Implementing a standardized 31P-MRS dynamic acquisition protocol to evaluate skeletal muscle energy metabolism and monitor muscle fatigability1,2, while being compatible with various longitudinal clinical studies on diversified patient cohorts, requires a high level of technicality and expertise. Furthermore, processing data to obtain reliable results also demands a great degree of expertise from the operator. In this two-part article, we present an advanced quality control approach for data acquired using a dynamic 31P-MRS protocol. The aim is to provide decision support to the operator in order to assist in data processing and obtain reliable results based on objective criteria. We present first in part one, an advanced data quality control (QC) approach of a dynamic 31P-MRS protocol. Part two is an impact study demonstrating the added value of the QC approach to explore clinical results derived from two patient populations with significant fatigue: COVID19 and multiple sclerosis (MS). Experimental: 31P-MRS was performed on a 3T clinical MRI in 175 subjects from clinical and healthy control populations conducted in a University Hospital. An advanced data QC Score (QCS) was developed using multiple objective criteria. The criteria were based on current recommendations from the literature enriched by new proposals based on clinical experience. The QCS was designed to indicate valid and corrupt data and guide necessary objective data editing to extract as much valid physiological data as possible. Dynamic acquisitions using an MR-compatible ergometer ran over a rest(40s), exercise(2min), and a recovery phase(6min). Results: Using QCS enabled rapid identification of subjects with data anomalies allowing the user to correct the data series or reject them partially or entirely as well as identify fully valid datasets. Overall, the use of the QCS resulted in the automatic classification of 45% of the subjects including 58 participants that had data with no criterion violation and 21 participants with violations that resulted in the rejection of all dynamic data. The remaining datasets were inspected manually with guidance allowing acceptance of full datasets from an additional 80 participants and recovery phase data from an additional 16 subjects. Overall, more anomalies occurred with patient data (35% of datasets) compared to healthy controls (15% of datasets). Conclusion: This paper describes typical difficulties encountered during the dynamic acquisition of 31P-MRS. Based on these observations, a standardized data quality control pipeline was created and implemented in both healthy and patient populations. The QC scoring ensures a standardized data rejection procedure and rigorous objective analysis of dynamic 31P-MRS data obtained from patients. The contribution of this methodology contributes to efforts made to standardize the practices of the 31P-MRS that has been underway for a decade, with the ultimate goal of making it an empowered tool for clinical research.
</details>
<details>
<summary>摘要</summary>
Background: 实施标准化31P-MRS动态获取协议，以评估骨骼肌能量代谢和监测肌肉疲劳性，需要高水平的技术性和专业知识。此外，从操作员处理数据以获得可靠结果也需要很高的专业度。在这两篇文章中，我们提出了一种高级数据质控方法，以帮助操作员在数据处理中做出客观的决策。在第一篇文章中，我们介绍了一种高级数据质控方法，以帮助操作员在数据处理中做出客观的决策。第二篇文章是一项影响研究，探讨了这种质控方法在 COVID-19 和多发性骨骼炎（MS）两种疲劳性疾病中的价值。Experimental: 31P-MRS在3T临床MRI上进行了175名临床和健康控制群体的测试。我们开发了一种多重目的 criterion 基于当前文献的建议，以及我们的临床经验所提出的新建议。这种 QCS 是用来指示有效和假数据，并帮助操作员对数据进行客观编辑，以提取最多可靠生物学数据。动态获取使用 MR 兼容的自行车在休息（40s）、运动（2分）和恢复阶段（6分）。结果：通过 QCS，可以快速标识具有数据异常的主体，并让用户对数据系列进行修正或者部分或全部拒绝。总的来说，使用 QCS 导致了自动将45%的主体分类为有效数据，其中有58名参与者没有任何 criterion 违反，而有21名参与者因违反 criterion 而拒绝了所有动态数据。剩下的数据被手动检查，以确定是否acceptable。总的来说， patient 数据中出现了更多的异常（35%的数据），compared to healthy controls（15%的数据）。结论：这篇文章描述了在动态获取31P-MRS数据时常见的困难。基于这些观察，我们创建了一个标准化的数据质控管道，并在健康和疾病人群中实施。 QCS  scoring 确保了一个标准化的数据拒绝程序，并且对动态31P-MRS数据从病人中得到的结果进行了严格的客观分析。本质控方法的贡献是为标准化31P-MRS实践做出了贡献，这一实践已经在过去十年中进行了不断的标准化努力，以使其成为严格的研究工具。
</details></li>
</ul>
<hr>
<h2 id="Multi-objective-Optimization-of-Space-Air-Ground-Integrated-Network-Slicing-Relying-on-a-Pair-of-Central-and-Distributed-Learning-Algorithms"><a href="#Multi-objective-Optimization-of-Space-Air-Ground-Integrated-Network-Slicing-Relying-on-a-Pair-of-Central-and-Distributed-Learning-Algorithms" class="headerlink" title="Multi-objective Optimization of Space-Air-Ground Integrated Network Slicing Relying on a Pair of Central and Distributed Learning Algorithms"></a>Multi-objective Optimization of Space-Air-Ground Integrated Network Slicing Relying on a Pair of Central and Distributed Learning Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12783">http://arxiv.org/abs/2309.12783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guorong Zhou, Liqiang Zhao, Gan Zheng, Shenghui Song, Jiankang Zhang, Lajos Hanzo</li>
<li>for: 本文旨在研究如何在全球空天地网络（SAGIN）中 dynamically 考虑三种常见的Radio Access Network（RAN）slice，以提高多种定制化服务的可用性和效率。</li>
<li>methods: 本文提出了一种基于多智能agent的 deep deterministic policy gradient（CDMADDPG）算法，用于同时优化三种类型的RAN slice的吞吐量、延迟和覆盖面积。</li>
<li>results:  simulation 结果表明，提出的方法可以尝试到Pareto优化多个RAN slice，并超越参考模型。<details>
<summary>Abstract</summary>
As an attractive enabling technology for next-generation wireless communications, network slicing supports diverse customized services in the global space-air-ground integrated network (SAGIN) with diverse resource constraints. In this paper, we dynamically consider three typical classes of radio access network (RAN) slices, namely high-throughput slices, low-delay slices and wide-coverage slices, under the same underlying physical SAGIN. The throughput, the service delay and the coverage area of these three classes of RAN slices are jointly optimized in a non-scalar form by considering the distinct channel features and service advantages of the terrestrial, aerial and satellite components of SAGINs. A joint central and distributed multi-agent deep deterministic policy gradient (CDMADDPG) algorithm is proposed for solving the above problem to obtain the Pareto optimal solutions. The algorithm first determines the optimal virtual unmanned aerial vehicle (vUAV) positions and the inter-slice sub-channel and power sharing by relying on a centralized unit. Then it optimizes the intra-slice sub-channel and power allocation, and the virtual base station (vBS)/vUAV/virtual low earth orbit (vLEO) satellite deployment in support of three classes of slices by three separate distributed units. Simulation results verify that the proposed method approaches the Pareto-optimal exploitation of multiple RAN slices, and outperforms the benchmarkers.
</details>
<details>
<summary>摘要</summary>
作为下一代无线通信技术的吸引人之一，网络剖析支持多种个性化服务在全球空天地 Integrated Network (SAGIN) 中，拥有多种资源限制。在这篇论文中，我们动态考虑了三种常见的无线接入网络 (RAN) slice，namely 高速吞吐 slice, 低延迟 slice 和广泛覆盖 slice，在同一层次的物理 SAGIN 中。这三种 RAN slice 的吞吐率、服务延迟和覆盖区域都是jointly 优化的，而且考虑了不同的通信频率和服务优势，以实现 Pareto 优化解决方案。我们提出了一种基于多代理 deep deterministic policy gradient (CDMADDPG) 算法的 JOINT 中央和分布式算法来解决这个问题。该算法首先确定了最佳虚拟无人机 (vUAV) 位置和间 slice Sub-channel 和功率分配，然后对每种 slice 进行内 slice Sub-channel 和功率分配，以及虚拟基站 (vBS)/vUAV/虚拟低地球 (vLEO) 卫星部署。测试结果表明，提议的方法可以实现 Pareto 优化多个 RAN slice，并超过参考值。
</details></li>
</ul>
<hr>
<h2 id="Green-Holographic-MIMO-Communications-With-A-Few-Transmit-Radio-Frequency-Chains"><a href="#Green-Holographic-MIMO-Communications-With-A-Few-Transmit-Radio-Frequency-Chains" class="headerlink" title="Green Holographic MIMO Communications With A Few Transmit Radio Frequency Chains"></a>Green Holographic MIMO Communications With A Few Transmit Radio Frequency Chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12688">http://arxiv.org/abs/2309.12688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuaishuai Guo, Jia Ye, Kaiqian Qu, Shuping Dang</li>
<li>for: 本文旨在探讨绿色束缚多输入多输出通信技术，以减少电磁谱上的束缚数量，同时保持高速度信息传输。</li>
<li>methods: 本文提出了一种名为非均匀束缚模式变换（NUHPM）的有效传输方式，通过利用额外的空间度量来实现高SNR范围内的容量限制。</li>
<li>results: 分析结果表明，通过增大天线覆盖面积而不增加发射RF束缚数量，可以实现绿色评估多input多output通信系统的高性能。数值结果也验证了我们的分析结论。<details>
<summary>Abstract</summary>
Holographic multiple-input multiple-output (MIMO) communications are widely recognized as a promising candidate for the next-generation air interface. With holographic MIMO surface, the number of the spatial degrees-of-freedom (DoFs) considerably increases and also significantly varies as the user moves. To fully employ the large and varying number of spatial DoFs, the number of equipped RF chains has to be larger than or equal to the largest number of spatial DoFs. However, this causes much waste as radio frequency (RF) chains (especially the transmit RF chains) are costly and power-hungry. To avoid the heavy burden, this paper investigates green holographic MIMO communications with a few transmit RF chains under an electromagnetic-based communication model. We not only look at the fundamental capacity limits but also propose an effective transmission, namely non-uniform holographic pattern modulation (NUHPM), to achieve the capacity limit in the high signal-to-noise (SNR) regime. The analytical result sheds light on the green evaluation of MIMO communications, which can be realized by increasing the size of the antenna aperture without increasing the number of transmit RF chains. Numerical results are provided to verify our analysis and to show the great performance gain by employing the additional spatial DoFs as modulation resources.
</details>
<details>
<summary>摘要</summary>
干扰多输入多输出（MIMO）通信被广泛认为是下一代无线接口的优选候选人。干扰MIMO表面上，空间度量自由（DoF）的数量增加很多，同时也因用户移动而异常变化。要完全利用这些很多和变化很大的空间DoF，需要更多的RF扩展（ especial transmit RF），但这会带来很大的浪费。为了避免这种重荷，本文研究了绿色干扰MIMO通信，使用只有一些发射RF扩展。我们不仅研究基本容量的限制，还提议非均匀干扰模式变换（NUHPM），以实现高信号噪响比（SNR）下的容量限制。分析结果抛光绿色评估MIMO通信，可以通过增加天线覆盖面积而不增加发射RF扩展。数值结果证明我们的分析结果，并显示了采用additional spatial DoF作为模ulation资源时的很大性能提升。
</details></li>
</ul>
<hr>
<h2 id="ViT-MDHGR-Cross-day-Reliability-and-Agility-in-Dynamic-Hand-Gesture-Prediction-via-HD-sEMG-Signal-Decoding"><a href="#ViT-MDHGR-Cross-day-Reliability-and-Agility-in-Dynamic-Hand-Gesture-Prediction-via-HD-sEMG-Signal-Decoding" class="headerlink" title="ViT-MDHGR: Cross-day Reliability and Agility in Dynamic Hand Gesture Prediction via HD-sEMG Signal Decoding"></a>ViT-MDHGR: Cross-day Reliability and Agility in Dynamic Hand Gesture Prediction via HD-sEMG Signal Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12602">http://arxiv.org/abs/2309.12602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qin Hu, Golara Ahmadi Azar, Alyson Fletcher, Sundeep Rangan, S. Farokh Atashzar</li>
<li>for: 这个研究是为了提高多天手势识别的精度和可靠性，并且解决现有的问题，例如对于训练和测试日的数据分配不均匀，导致模型的一致性受损。</li>
<li>methods: 本研究使用了一个封闭的ViT-based网络，并且运用了非常短的HD-sEMG信号窗口（仅50ms），从而提高了模型的迅速性和反应性。</li>
<li>results: 研究发现，使用了提案的模型，可以预测11种动作，并且在20名对象中平均精度高于71%，并且在重新训练少于10%的parameters下，可以达到92%的精度。<details>
<summary>Abstract</summary>
Surface electromyography (sEMG) and high-density sEMG (HD-sEMG) biosignals have been extensively investigated for myoelectric control of prosthetic devices, neurorobotics, and more recently human-computer interfaces because of their capability for hand gesture recognition/prediction in a wearable and non-invasive manner. High intraday (same-day) performance has been reported. However, the interday performance (separating training and testing days) is substantially degraded due to the poor generalizability of conventional approaches over time, hindering the application of such techniques in real-life practices. There are limited recent studies on the feasibility of multi-day hand gesture recognition. The existing studies face a major challenge: the need for long sEMG epochs makes the corresponding neural interfaces impractical due to the induced delay in myoelectric control. This paper proposes a compact ViT-based network for multi-day dynamic hand gesture prediction. We tackle the main challenge as the proposed model only relies on very short HD-sEMG signal windows (i.e., 50 ms, accounting for only one-sixth of the convention for real-time myoelectric implementation), boosting agility and responsiveness. Our proposed model can predict 11 dynamic gestures for 20 subjects with an average accuracy of over 71% on the testing day, 3-25 days after training. Moreover, when calibrated on just a small portion of data from the testing day, the proposed model can achieve over 92% accuracy by retraining less than 10% of the parameters for computational efficiency.
</details>
<details>
<summary>摘要</summary>
superficiale electromiografia (sEMG) 和高密度 sEMG (HD-sEMG) 生物信号已经广泛研究用于 prosthetic device 控制、neurorobotics 和最近的人机交互，因为它们可以在穿着和非侵入性的方式下识别/预测手势。 高于同一天的性能已经被报道。然而， между天性能（分开训练和测试日）却很差，这限制了这些技术的应用在实际场景中。有限的最近研究表明了多天手势识别的可能性。现有的研究面临主要挑战：需要长时间的 sEMG 时间窗口，使得相关的神经接口不实用，因为引入的myoelectric控制延迟。本文提议了一个快速的 ViT 基于网络，用于多天动手势预测。我们解决了主要的挑战，因为我们的提议模型只需要非常短的 HD-sEMG 信号窗口（即 50 ms，相当于一半的实时 myoelectric 实现），提高了机敏性和响应性。我们的提议模型可以预测 11 种动手势，对 20 名参与者的测试日有效率超过 71%，并且在只使用测试日少量数据进行升级时，可以达到超过 92% 的精度。
</details></li>
</ul>
<hr>
<h2 id="Movable-Antenna-Empowered-AirComp"><a href="#Movable-Antenna-Empowered-AirComp" class="headerlink" title="Movable Antenna-Empowered AirComp"></a>Movable Antenna-Empowered AirComp</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12596">http://arxiv.org/abs/2309.12596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenqiao Cheng, Nanxi Li, Jianchi Zhu, Xiaoming She, Chongjun Ouyang, Peng Chen</li>
<li>for: 提高计算准确性</li>
<li>methods: joint优化传输功率控制、天线位置调整和接收组合</li>
<li>results: 提供了一种有效的方法来最小化计算均方差误差，并且数据显示了该方法的明显优势 compared to 基于固定天线的参考系统。<details>
<summary>Abstract</summary>
A novel over-the-air computation (AirComp) framework, empowered by the incorporation of movable antennas (MAs), is proposed to significantly enhance computation accuracy. Within this framework, the joint optimization of transmit power control, antenna positioning, and receive combining is investigated. An efficient method is proposed to tackle the problem of computation mean-squared error (MSE) minimization, capitalizing on the approach of alternating optimization. Numerical results are provided to substantiate the superior MSE performance of the proposed framework, which establish its clear advantage over benchmark systems employing conventional fixed-position antennas (FPAs).
</details>
<details>
<summary>摘要</summary>
“一个基于无线电处理（AirComp）框架的新方案，利用可动天线（MA）的增强，以提高计算精度。在这个框架中，联合服务器传输电力控制、天线位置调整和接收结合优化。一种高效的方法是提出来解决计算平均方差误差（MSE）的最小化问题，基于交替优化的方法。实验结果显示了提案的框架具有明显的MSE表现优势，与传统固定天线（FPAs）的系统相比。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Passive-Reflection-Codebook-Design-for-IRS-Integrated-Access-Point"><a href="#Passive-Reflection-Codebook-Design-for-IRS-Integrated-Access-Point" class="headerlink" title="Passive Reflection Codebook Design for IRS-Integrated Access Point"></a>Passive Reflection Codebook Design for IRS-Integrated Access Point</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12563">http://arxiv.org/abs/2309.12563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwei Huang, Lipeng Zhu, Rui Zhang</li>
<li>for: 该研究旨在提高无线信号覆盖范围和通信性能，采用智能反射表面技术（IRS）和接收机天线阵列。</li>
<li>methods: 该研究提出了一种新的codebook-based IRS反射设计，通过在同一个天线覆盖中集成IRS和接收机天线阵列，减少了信道损失。</li>
<li>results: 实验结果表明，该设计可以提高总频谱能量平均值，并在单用户和多用户传输中实现显著的性能提升。<details>
<summary>Abstract</summary>
Intelligent reflecting surface (IRS) has emerged as a promising technique to extend the wireless signal coverage of access point (AP) and improve the communication performance cost-effectively. In order to reduce the path-loss of the cascaded user-IRS-AP channels, the IRS-integrated AP architecture has been proposed to deploy the IRSs and the antenna array of the AP within the same antenna radome. To reduce the pilot overhead for estimating all IRS-involved channels, in this paper, we propose a novel codebook-based IRS reflection design for the IRS-integrated AP to enhance the coverage performance in a given area. In particular, the codebook consisting of a small number of codewords is designed offline by employing an efficient sector division strategy based on the azimuth angle. To ensure the performance of each sector, we optimize its corresponding codeword for IRS reflection pattern to maximize the sector-min-average-effective-channel-power (SMAECP) by applying the alternating optimization (AO) and semidefinite relaxation (SDR) methods. With the designed codebook, the AP performs the IRS reflection training by sequentially applying all codewords and selects the one achieving the best communication performance for data transmission. Numerical results show that our proposed codebook design can enhance the average channel power of the whole coverage area, as compared to the system without IRS. Moreover, our proposed codebook-based IRS reflection design is shown to achieve significant performance gain over other benchmark schemes in both single-user and multi-user transmissions.
</details>
<details>
<summary>摘要</summary>
智能反射表面（IRS）已经成为一种有前途的技术，以提高无线信号覆盖范围和通信性能，而不需要大量的成本投入。为了减少用户-IRS-AP通道的偏移损耗，我们提议在同一个天线覆盖中部署IRS和AP天线阵列。为了减少估算所需的射频资源，在本文中，我们提出了一种新的codebook-based IRS反射设计，以提高在给定区域的覆盖性能。具体来说，我们采用了一个小型的codeword集合来设计codebook，通过使用高效的扇区策略来基于Azimuth角来设计。为了保证每个扇区的性能，我们对每个扇区的相应codeword进行了最优化，以最大化扇区最小平均有效通道功率（SMAECP）。通过将codebook传递给AP，AP可以通过顺序应用所有codeword来进行IRS反射训练，并选择最佳的通信性能来进行数据传输。numerical results表明，我们的提议的codebook设计可以提高整个覆盖区域的平均通道功率，相比于没有IRS的系统。此外，我们的codebook-based IRS反射设计还被证明可以在单用户和多用户传输中具有显著的性能提升。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/22/eess.SP_2023_09_22/" data-id="clpztdnuk01f6es88a5xv3plk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/43/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/42/">42</a><a class="page-number" href="/page/43/">43</a><span class="page-number current">44</span><a class="page-number" href="/page/45/">45</a><a class="page-number" href="/page/46/">46</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/45/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
