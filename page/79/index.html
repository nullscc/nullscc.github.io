
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/79/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_07_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/29/cs.CV_2023_07_29/" class="article-date">
  <time datetime="2023-07-29T13:00:00.000Z" itemprop="datePublished">2023-07-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/29/cs.CV_2023_07_29/">cs.CV - 2023-07-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-Object-Detection-in-Ancient-Documents-with-Synthetic-Data-Generation-and-Transformer-Based-Models"><a href="#Enhancing-Object-Detection-in-Ancient-Documents-with-Synthetic-Data-Generation-and-Transformer-Based-Models" class="headerlink" title="Enhancing Object Detection in Ancient Documents with Synthetic Data Generation and Transformer-Based Models"></a>Enhancing Object Detection in Ancient Documents with Synthetic Data Generation and Transformer-Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16005">http://arxiv.org/abs/2307.16005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Ziran, Francesco Leotta, Massimo Mecella</li>
<li>for: 提高古文献中对象检测精度，减少假阳性结果。</li>
<li>methods: 通过计算媒体创建synthetic数据集，并将视觉特征提取integrated到对象检测过程中。</li>
<li>results: 通过实验，我们表明可以提高对象检测精度，有助于 Paleography 领域进行深入分析和理解历史文献。<details>
<summary>Abstract</summary>
The study of ancient documents provides a glimpse into our past. However, the low image quality and intricate details commonly found in these documents present significant challenges for accurate object detection. The objective of this research is to enhance object detection in ancient documents by reducing false positives and improving precision. To achieve this, we propose a method that involves the creation of synthetic datasets through computational mediation, along with the integration of visual feature extraction into the object detection process. Our approach includes associating objects with their component parts and introducing a visual feature map to enable the model to discern between different symbols and document elements. Through our experiments, we demonstrate that improved object detection has a profound impact on the field of Paleography, enabling in-depth analysis and fostering a greater understanding of these valuable historical artifacts.
</details>
<details>
<summary>摘要</summary>
古文献研究可以带我们回到过去，但是这些文献中的图像质量低下和细节复杂度往往会对准确的对象探测带来挑战。我们的研究目标是提高古文献中对象探测的精度，减少假阳性和提高准确率。我们提出了一种方法，即通过计算媒介创造 synthetic 数据集，并将视觉特征提取 integrate 到对象探测过程中。我们的方法包括对象与其组成部分关联，并通过视觉特征地图来使模型能够辨别不同的符号和文档元素。经过我们的实验，我们发现，改进对象探测对 Paleography 领域有着深远的影响，帮助我们进行深入的分析和更好地理解这些历史遗产。
</details></li>
</ul>
<hr>
<h2 id="Automated-Hit-frame-Detection-for-Badminton-Match-Analysis"><a href="#Automated-Hit-frame-Detection-for-Badminton-Match-Analysis" class="headerlink" title="Automated Hit-frame Detection for Badminton Match Analysis"></a>Automated Hit-frame Detection for Badminton Match Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16000">http://arxiv.org/abs/2307.16000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arthur900530/Automated-Hit-frame-Detection-for-Badminton-Match-Analysis">https://github.com/arthur900530/Automated-Hit-frame-Detection-for-Badminton-Match-Analysis</a></li>
<li>paper_authors: Yu-Hang Chien, Fang Yu</li>
<li>for: 这项研究旨在为羽毛球运动员提供更高水平的表现分析，帮助教练和运动员通过自动化工具来系统地评估自己的表现。</li>
<li>methods: 本研究使用现代深度学习技术来自动检测羽毛球比赛视频中的击框帧。检测过程包括赛事裁剪、运动员和球场关键点检测、拍篮轨迹预测和击框检测等自动化步骤。</li>
<li>results: 在本研究中，我们实现了视频裁剪精度99%，在应用运动员关键点序列中预测拍篮轨迹方向的准确率高于92%，并对赛事裁剪和击框检测进行评估。<details>
<summary>Abstract</summary>
Sports professionals constantly under pressure to perform at the highest level can benefit from sports analysis, which allows coaches and players to reduce manual efforts and systematically evaluate their performance using automated tools. This research aims to advance sports analysis in badminton, systematically detecting hit-frames automatically from match videos using modern deep learning techniques. The data included in hit-frames can subsequently be utilized to synthesize players' strokes and on-court movement, as well as for other downstream applications such as analyzing training tasks and competition strategy. The proposed approach in this study comprises several automated procedures like rally-wise video trimming, player and court keypoints detection, shuttlecock flying direction prediction, and hit-frame detection. In the study, we achieved 99% accuracy on shot angle recognition for video trimming, over 92% accuracy for applying player keypoints sequences on shuttlecock flying direction prediction, and reported the evaluation results of rally-wise video trimming and hit-frame detection.
</details>
<details>
<summary>摘要</summary>
运动专业人员需要一直处于最高水平的压力，可以从运动分析中受益，使得教练和运动员可以通过自动化工具来系统地评估自己的表现。这项研究的目的是为了提高羽毛球运动分析，通过现代深度学习技术自动检测比赛视频中的击框帧。这些数据可以用于synthesize运动员的拍打和场上运动，以及其他下游应用程序，如分析训练任务和竞赛策略。本研究的方法包括多个自动化步骤，如赛事截割、运动员和场地关键点检测、拍球飞行方向预测和击框检测。在研究中，我们实现了视频截割时的射击角度识别率99%，以及在拍球飞行方向预测中的运动员关键点序列应用率超过92%。我们还发布了赛事截割和击框检测的评估结果。
</details></li>
</ul>
<hr>
<h2 id="Separate-Scene-Text-Detector-for-Unseen-Scripts-is-Not-All-You-Need"><a href="#Separate-Scene-Text-Detector-for-Unseen-Scripts-is-Not-All-You-Need" class="headerlink" title="Separate Scene Text Detector for Unseen Scripts is Not All You Need"></a>Separate Scene Text Detector for Unseen Scripts is Not All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15991">http://arxiv.org/abs/2307.15991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Keserwani, Taveena Lotey, Rohit Keshari, Partha Pratim Roy</li>
<li>for: 解决多种文字识别问题在野外环境中</li>
<li>methods: 利用vector embedding将文字的roke信息映射到文字类别中</li>
<li>results: 在零 shot Setting下，提出的方法可以准确地检测未看过的文字类别<details>
<summary>Abstract</summary>
Text detection in the wild is a well-known problem that becomes more challenging while handling multiple scripts. In the last decade, some scripts have gained the attention of the research community and achieved good detection performance. However, many scripts are low-resourced for training deep learning-based scene text detectors. It raises a critical question: Is there a need for separate training for new scripts? It is an unexplored query in the field of scene text detection. This paper acknowledges this problem and proposes a solution to detect scripts not present during training. In this work, the analysis has been performed to understand cross-script text detection, i.e., trained on one and tested on another. We found that the identical nature of text annotation (word-level/line-level) is crucial for better cross-script text detection. The different nature of text annotation between scripts degrades cross-script text detection performance. Additionally, for unseen script detection, the proposed solution utilizes vector embedding to map the stroke information of text corresponding to the script category. The proposed method is validated with a well-known multi-lingual scene text dataset under a zero-shot setting. The results show the potential of the proposed method for unseen script detection in natural images.
</details>
<details>
<summary>摘要</summary>
文本检测在野外是一个非常有挑战性的问题，其中多种文本的检测增加了挑战。过去的一个 десятилетие，一些文本种类在研究者们中引起了关注，并实现了良好的检测性能。然而，许多文本种类具有训练深度学习基于Scene文本检测器的资源不充分的问题。这个问题提出了一个关键问题：是否需要分开训练新的文本种类？这是Scene文本检测领域未曾研究的问题。本文承认这个问题，并提出了一种用于检测训练之外的文本种类的解决方案。在这种情况下，我们进行了跨脚本文本检测的分析，即将训练的一种文本种类测试在另一种文本种类上。我们发现，文本注释的标准化（word-level/line-level）是跨脚本文本检测的关键因素。不同的文本注释 между脚本会导致跨脚本文本检测性能下降。此外，为了检测未经训练的脚本，我们提出了使用vector embedding将文本的行为映射到脚本类别。我们的方法在一个知名的多语言Scene文本 dataset上进行零shot设定下进行验证。结果表明，我们的方法具有检测未经训练的脚本的潜在能力。
</details></li>
</ul>
<hr>
<h2 id="RGB-D-Fusion-Image-Conditioned-Depth-Diffusion-of-Humanoid-Subjects"><a href="#RGB-D-Fusion-Image-Conditioned-Depth-Diffusion-of-Humanoid-Subjects" class="headerlink" title="RGB-D-Fusion: Image Conditioned Depth Diffusion of Humanoid Subjects"></a>RGB-D-Fusion: Image Conditioned Depth Diffusion of Humanoid Subjects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15988">http://arxiv.org/abs/2307.15988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sascha Kirch, Valeria Olyunina, Jan Ondřej, Rafael Pagés, Sergio Martin, Clara Pérez-Molina</li>
<li>for: 生成高分辨率深度图从低分辨率灰度图像中</li>
<li>methods: 使用多模态条件杂音扩散概率模型，首先生成低分辨率深度图，然后使用第二个杂音扩散概率模型来upsample深度图，并 introduce了一种新的增强技术，深度噪声增强</li>
<li>results: 实现高效地生成高分辨率深度图，并提高模型的RobustnessIn English, this translates to:</li>
<li>for: Generating high-resolution depth maps from low-resolution monocular RGB images</li>
<li>methods: Using a multi-modal conditional denoising diffusion probabilistic model, first generating a low-resolution depth map, and then upsampling the depth map using a second denoising diffusion probabilistic model conditioned on a low-resolution RGB-D image. Additionally, introducing a novel augmentation technique, depth noise augmentation, to increase the robustness of the super-resolution model.</li>
<li>results: Achieving high-quality super-resolution of depth maps and improving the robustness of the model.<details>
<summary>Abstract</summary>
We present RGB-D-Fusion, a multi-modal conditional denoising diffusion probabilistic model to generate high resolution depth maps from low-resolution monocular RGB images of humanoid subjects. RGB-D-Fusion first generates a low-resolution depth map using an image conditioned denoising diffusion probabilistic model and then upsamples the depth map using a second denoising diffusion probabilistic model conditioned on a low-resolution RGB-D image. We further introduce a novel augmentation technique, depth noise augmentation, to increase the robustness of our super-resolution model.
</details>
<details>
<summary>摘要</summary>
我们介绍RGB-D-Fusion，一种多模态条件噪声扩散概率模型，用于生成高分辨率深度图像从低分辨率单颜色RGB图像中。RGB-D-Fusion首先使用一种图像条件噪声扩散概率模型生成低分辨率深度图像，然后使用第二种噪声扩散概率模型，条件于低分辨率RGB-D图像，进行� upsampling。我们还介绍了一种新的扩展技术，深度噪声扩散，以增强我们的超分辨率模型的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Class-Specific-Distribution-Alignment-for-Semi-Supervised-Medical-Image-Classification"><a href="#Class-Specific-Distribution-Alignment-for-Semi-Supervised-Medical-Image-Classification" class="headerlink" title="Class-Specific Distribution Alignment for Semi-Supervised Medical Image Classification"></a>Class-Specific Distribution Alignment for Semi-Supervised Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15987">http://arxiv.org/abs/2307.15987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongzheng Huang, Jiawei Wu, Tao Wang, Zuoyong Li, Anastasia Ioannou</li>
<li>for: 这篇论文是为了解决医疗影像分类问题，因为数据标注是时间耗费很大，且疾病的分布是不均匀的。</li>
<li>methods: 本文提出了一个叫做分类对象分配（CSDA）的半有supervised学习框架，这个框架适用于从高度不均匀的数据集学习。特别是，我们从距离基础架的变数空间中考虑分配过程，并将这个过程转换为capture class-dependent marginal predictions的方法，以避免偏向多数组别的问题。此外，我们提出了一个变数条件队列（VCQ）模组，以确保每个类别的不断数据数量具有相同的比例。</li>
<li>results: 在三个公开数据集HAM10000、CheXpert和Kvasir上进行验证，我们发现our方法可以在半有supervised的情况下提供竞争性的表现，并且在医疗影像分类任务上获得了比较好的结果。<details>
<summary>Abstract</summary>
Despite the success of deep neural networks in medical image classification, the problem remains challenging as data annotation is time-consuming, and the class distribution is imbalanced due to the relative scarcity of diseases. To address this problem, we propose Class-Specific Distribution Alignment (CSDA), a semi-supervised learning framework based on self-training that is suitable to learn from highly imbalanced datasets. Specifically, we first provide a new perspective to distribution alignment by considering the process as a change of basis in the vector space spanned by marginal predictions, and then derive CSDA to capture class-dependent marginal predictions on both labeled and unlabeled data, in order to avoid the bias towards majority classes. Furthermore, we propose a Variable Condition Queue (VCQ) module to maintain a proportionately balanced number of unlabeled samples for each class. Experiments on three public datasets HAM10000, CheXpert and Kvasir show that our method provides competitive performance on semi-supervised skin disease, thoracic disease, and endoscopic image classification tasks.
</details>
<details>
<summary>摘要</summary>
尽管深度神经网络在医疗图像分类中取得了成功，但问题仍然具有挑战性，因为数据标注是时间consuming的，而且疾病的分布是不均衡的，因为疾病的相对罕见性。为了解决这个问题，我们提出了类别特定分布对齐（CSDA），一种基于自动训练的 semi-supervised 学习框架，适用于学习高度不均衡的数据集。 Specifically, we first provide a new perspective to distribution alignment by considering the process as a change of basis in the vector space spanned by marginal predictions, and then derive CSDA to capture class-dependent marginal predictions on both labeled and unlabeled data, in order to avoid the bias towards majority classes. Furthermore, we propose a Variable Condition Queue (VCQ) module to maintain a proportionately balanced number of unlabeled samples for each class. Experiments on three public datasets HAM10000, CheXpert and Kvasir show that our method provides competitive performance on semi-supervised skin disease, thoracic disease, and endoscopic image classification tasks.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="GaitASMS-Gait-Recognition-by-Adaptive-Structured-Spatial-Representation-and-Multi-Scale-Temporal-Aggregation"><a href="#GaitASMS-Gait-Recognition-by-Adaptive-Structured-Spatial-Representation-and-Multi-Scale-Temporal-Aggregation" class="headerlink" title="GaitASMS: Gait Recognition by Adaptive Structured Spatial Representation and Multi-Scale Temporal Aggregation"></a>GaitASMS: Gait Recognition by Adaptive Structured Spatial Representation and Multi-Scale Temporal Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15981">http://arxiv.org/abs/2307.15981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Sun, Hu Long, Xueling Feng, Mark Nixon</li>
<li>for: 本研究旨在提出一种新的步态识别方法，以提高步态识别精度和稳定性。</li>
<li>methods: 该方法使用适应结构 repreentation 抽取模块 (ASRE) 和多Scale temporal 聚合模块 (MSTA)，分别提取步态的适应结构和多Scale temporal 信息。此外，提出了一种新的数据增强技术，即随机mask，以增加样本空间的多样性。</li>
<li>results: 对于两个数据集，该方法能够达到竞争力的性能，特别是在复杂场景下（BG和CL）。在CASIA-B数据集上，GaitASMS方法的均值准确率为93.5%，与基准方法相比，在rank-1准确率上提高3.4%和6.3%。<details>
<summary>Abstract</summary>
Gait recognition is one of the most promising video-based biometric technologies. The edge of silhouettes and motion are the most informative feature and previous studies have explored them separately and achieved notable results. However, due to occlusions and variations in viewing angles, their gait recognition performance is often affected by the predefined spatial segmentation strategy. Moreover, traditional temporal pooling usually neglects distinctive temporal information in gait. To address the aforementioned issues, we propose a novel gait recognition framework, denoted as GaitASMS, which can effectively extract the adaptive structured spatial representations and naturally aggregate the multi-scale temporal information. The Adaptive Structured Representation Extraction Module (ASRE) separates the edge of silhouettes by using the adaptive edge mask and maximizes the representation in semantic latent space. Moreover, the Multi-Scale Temporal Aggregation Module (MSTA) achieves effective modeling of long-short-range temporal information by temporally aggregated structure. Furthermore, we propose a new data augmentation, denoted random mask, to enrich the sample space of long-term occlusion and enhance the generalization of the model. Extensive experiments conducted on two datasets demonstrate the competitive advantage of proposed method, especially in complex scenes, i.e. BG and CL. On the CASIA-B dataset, GaitASMS achieves the average accuracy of 93.5\% and outperforms the baseline on rank-1 accuracies by 3.4\% and 6.3\%, respectively, in BG and CL. The ablation experiments demonstrate the effectiveness of ASRE and MSTA.
</details>
<details>
<summary>摘要</summary>
《跟踪识别是视频基metric技术中最有前途的一种。 Edge of silhouettes and motion是最有信息的特征，先前的研究已经分别研究这两个特征，并取得了可观的成果。然而，由于 occlusion 和视角变化， их跟踪识别性能受到预先定义的空间分割策略的影响。此外，传统的时间汇集通常忽略了跑步动作中的独特时间信息。为了解决上述问题，我们提出了一种新的跟踪识别框架， denoted as GaitASMS， which can effectively extract adaptive structured spatial representations and naturally aggregate multi-scale temporal information. Adaptive Structured Representation Extraction Module (ASRE) 使用 adaptive edge mask 来分离 Edge of silhouettes，并在 semantic latent space 中最大化表示。此外， Multi-Scale Temporal Aggregation Module (MSTA) 可以有效地模型长短距离的时间信息。此外，我们还提出了一种新的数据增强技术， denoted random mask，以增加跑步动作的随机 occlusion 样本空间，并提高模型的通用性。Extensive experiments conducted on two datasets demonstrate the competitive advantage of proposed method, especially in complex scenes, i.e. BG and CL. On the CASIA-B dataset, GaitASMS achieves the average accuracy of 93.5% and outperforms the baseline on rank-1 accuracies by 3.4% and 6.3%, respectively, in BG and CL. The ablation experiments demonstrate the effectiveness of ASRE and MSTA.
</details></li>
</ul>
<hr>
<h2 id="Fingerprints-of-Generative-Models-in-the-Frequency-Domain"><a href="#Fingerprints-of-Generative-Models-in-the-Frequency-Domain" class="headerlink" title="Fingerprints of Generative Models in the Frequency Domain"></a>Fingerprints of Generative Models in the Frequency Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15977">http://arxiv.org/abs/2307.15977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyun Yang, Juan Cao, Danding Wang, Chang Xu</li>
<li>for: 这篇论文旨在分析CNN基于生成模型中的唯一指纹，以及这些指纹如何影响生成图像质量。</li>
<li>methods: 这篇论文使用频谱分析方法来解释CNN生成模型中的网络组件，并从这些频谱分析中提取出频谱分布和格子异常的源头。</li>
<li>results: 研究发现，通过使用低成本的生成模型，可以生成图像，这些图像具有与实际CNN生成模型中的频谱分布和格子异常相同的特征。这些特征可以用于验证、识别和分析CNN基于生成模型的唯一指纹。<details>
<summary>Abstract</summary>
It is verified in existing works that CNN-based generative models leave unique fingerprints on generated images. There is a lack of analysis about how they are formed in generative models. Interpreting network components in the frequency domain, we derive sources for frequency distribution and grid-like pattern discrepancies exhibited on the spectrum. These insights are leveraged to develop low-cost synthetic models, which generate images emulating the frequency patterns observed in real generative models. The resulting fingerprint extractor pre-trained on synthetic data shows superior transferability in verifying, identifying, and analyzing the relationship of real CNN-based generative models such as GAN, VAE, Flow, and diffusion.
</details>
<details>
<summary>摘要</summary>
已经在现有的研究中证明，基于Convolutional Neural Network（CNN）的生成模型会留下唯一的指纹在生成图像中。然而，关于如何形成这些指纹的分析却缺乏。我们通过解释网络组件的频谱域分析，得出了频谱分布和格子状差的来源。这些发现被利用来开发低成本的生成模型，这些模型可以生成图像，具有真实生成模型中观察到的频谱特征。这种指纹提取器在嵌入数据上进行预训练后，能够superior的传播性，用于验证、识别和分析真实的CNN基于生成模型，如GAN、VAE、Flow和diffusion。
</details></li>
</ul>
<hr>
<h2 id="XMem-Production-level-Video-Segmentation-From-Few-Annotated-Frames"><a href="#XMem-Production-level-Video-Segmentation-From-Few-Annotated-Frames" class="headerlink" title="XMem++: Production-level Video Segmentation From Few Annotated Frames"></a>XMem++: Production-level Video Segmentation From Few Annotated Frames</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15958">http://arxiv.org/abs/2307.15958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/max810/XMem2">https://github.com/max810/XMem2</a></li>
<li>paper_authors: Maksym Bekuzarov, Ariana Bermudez, Joon-Young Lee, Hao Li</li>
<li>for: 该论文旨在提高现有的内存基于模型，以提高用户指导视频分割的精度和效率。</li>
<li>methods: 该方法使用了一种新的带有常见Memory模块的半监督视频物体分割模型，可以有效地处理多帧用户选择的图像。</li>
<li>results: 该方法可以在不需要重新训练的情况下，在具有多类和多帧的分割任务中提供了最佳性能，并且需要较少的帧注释量。<details>
<summary>Abstract</summary>
Despite advancements in user-guided video segmentation, extracting complex objects consistently for highly complex scenes is still a labor-intensive task, especially for production. It is not uncommon that a majority of frames need to be annotated. We introduce a novel semi-supervised video object segmentation (SSVOS) model, XMem++, that improves existing memory-based models, with a permanent memory module. Most existing methods focus on single frame annotations, while our approach can effectively handle multiple user-selected frames with varying appearances of the same object or region. Our method can extract highly consistent results while keeping the required number of frame annotations low. We further introduce an iterative and attention-based frame suggestion mechanism, which computes the next best frame for annotation. Our method is real-time and does not require retraining after each user input. We also introduce a new dataset, PUMaVOS, which covers new challenging use cases not found in previous benchmarks. We demonstrate SOTA performance on challenging (partial and multi-class) segmentation scenarios as well as long videos, while ensuring significantly fewer frame annotations than any existing method. Project page: https://max810.github.io/xmem2-project-page/
</details>
<details>
<summary>摘要</summary>
尽管用户导导视频分割技术已经取得了进步，但是在高度复杂的场景下，提取复杂对象的工作仍然是一项劳动密集的任务，特别是在生产环境中。一般来说，大多数帧需要被标注。我们介绍了一种新的半监督视频对象分割（SSVOS）模型，XMem++，该模型在已有的记忆型模型基础上进行改进，具有永久记忆模块。大多数现有方法都是单帧标注的，而我们的方法可以有效地处理多个用户选择的帧，这些帧可能有不同的对象或区域的变化。我们的方法可以提取高度一致的结果，同时保持需要的帧标注数量低。我们还引入了一种迭代和注意力基于的帧建议机制，该机制可以计算下一帧的标注。我们的方法是实时的，不需要重新训练 после每个用户输入。我们还介绍了一个新的数据集，PUMaVOS，该数据集包括一些不同于过去的 benchmark 中的新吸引人用 caso。我们在这些挑战性（部分和多类）分割场景以及长视频中表现出了最高级的表现，同时保持了与任何现有方法相比的帧标注数量减少。项目页面：https://max810.github.io/xmem2-project-page/
</details></li>
</ul>
<hr>
<h2 id="CMDA-Cross-Modality-Domain-Adaptation-for-Nighttime-Semantic-Segmentation"><a href="#CMDA-Cross-Modality-Domain-Adaptation-for-Nighttime-Semantic-Segmentation" class="headerlink" title="CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation"></a>CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15942">http://arxiv.org/abs/2307.15942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiarho/cmda">https://github.com/xiarho/cmda</a></li>
<li>paper_authors: Ruihao Xia, Chaoqiang Zhao, Meng Zheng, Ziyan Wu, Qiyu Sun, Yang Tang</li>
<li>for: 提高夜间semantic segmentation的精度和效果，使用多Modalities的信息（图像和事件）进行培育。</li>
<li>methods: 提出了一种基于无监督的cross-modality domain adaptation（CMDA）框架，通过图像动态特征Extractor和图像内容特征Extractor来桥接不同的Modalities和频率域。</li>
<li>results: 在公共图像集和提出的图像-事件集上进行了广泛的实验，并得到了效果的结果，同时还开源了代码、模型和数据集。<details>
<summary>Abstract</summary>
Most nighttime semantic segmentation studies are based on domain adaptation approaches and image input. However, limited by the low dynamic range of conventional cameras, images fail to capture structural details and boundary information in low-light conditions. Event cameras, as a new form of vision sensors, are complementary to conventional cameras with their high dynamic range. To this end, we propose a novel unsupervised Cross-Modality Domain Adaptation (CMDA) framework to leverage multi-modality (Images and Events) information for nighttime semantic segmentation, with only labels on daytime images. In CMDA, we design the Image Motion-Extractor to extract motion information and the Image Content-Extractor to extract content information from images, in order to bridge the gap between different modalities (Images to Events) and domains (Day to Night). Besides, we introduce the first image-event nighttime semantic segmentation dataset. Extensive experiments on both the public image dataset and the proposed image-event dataset demonstrate the effectiveness of our proposed approach. We open-source our code, models, and dataset at https://github.com/XiaRho/CMDA.
</details>
<details>
<summary>摘要</summary>
大多数夜间semantic segmentation研究基于领域适应方法和图像输入。然而，由于普通相机的低动态范围，图像在低照度条件下无法捕捉结构信息和边界信息。事件摄像机作为视觉感知器的新形态，它们与普通相机相比具有高动态范围。为此，我们提出了一种新的无监督 Cross-Modality Domain Adaptation（CMDA）框架，以利用多模态信息（图像和事件）进行夜间semantic segmentation，只需要在白天图像上提供标签。在CMDA中，我们设计了图像运动提取器和图像内容提取器，以EXTRACTING Motion information和图像内容信息，以桥接不同模态（图像到事件）和领域（白天到夜）。此外，我们提出了首个图像-事件夜间semantic segmentation数据集。我们在公共图像集和我们提议的图像-事件集上进行了广泛的实验，并证明了我们的提议方法的有效性。我们在https://github.com/XiaRho/CMDA上分享了我们的代码、模型和数据集。
</details></li>
</ul>
<hr>
<h2 id="Sat2Cap-Mapping-Fine-Grained-Textual-Descriptions-from-Satellite-Images"><a href="#Sat2Cap-Mapping-Fine-Grained-Textual-Descriptions-from-Satellite-Images" class="headerlink" title="Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images"></a>Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15904">http://arxiv.org/abs/2307.15904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aayush Dhakal, Adeel Ahmad, Subash Khanal, Srikumar Sastry, Nathan Jacobs</li>
<li>for: 这个论文旨在开发一种基于自由文本描述（或标题）的弱监督地图创建方法。</li>
<li>methods: 该论文使用了一种叫做Sat2Cap的对比学习框架，在一个大规模的对比图像和地面图像 dataset 上训练。</li>
<li>results: 该模型能够successfully capture细腻概念和有效地适应时间变化。 codes, datasets, 和模型将被公开发布。<details>
<summary>Abstract</summary>
We propose a novel weakly supervised approach for creating maps using free-form textual descriptions (or captions). We refer to this new line of work of creating textual maps as zero-shot mapping. Prior works have approached mapping tasks by developing models that predict over a fixed set of attributes using overhead imagery. However, these models are very restrictive as they can only solve highly specific tasks for which they were trained. Mapping text, on the other hand, allows us to solve a large variety of mapping problems with minimal restrictions. To achieve this, we train a contrastive learning framework called Sat2Cap on a new large-scale dataset of paired overhead and ground-level images. For a given location, our model predicts the expected CLIP embedding of the ground-level scenery. Sat2Cap is also conditioned on temporal information, enabling it to learn dynamic concepts that vary over time. Our experimental results demonstrate that our models successfully capture fine-grained concepts and effectively adapt to temporal variations. Our approach does not require any text-labeled data making the training easily scalable. The code, dataset, and models will be made publicly available.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的无监督方法，通过自由文本描述（或标题）来创建地图。我们称这种新的地图创建方法为零shot mapping。前一些工作都是通过开发模型，以预测Fixed集合属性使用过头像来解决地图任务。然而，这些模型具有很多限制，只能解决高度特定的任务。在text中映射，我们可以解决各种地图问题，几乎没有限制。为 достичь这一点，我们训练了一个异构学习框架called Sat2Cap，使用一个新的大规模的对应过头像和地面照片的数据集来训练。对于给定的位置，我们的模型预测地面照片中预期的CLIP嵌入。Sat2Cap还受到时间信息的限制，使其学习时间变化的动态概念。我们的实验结果表明，我们的模型成功捕捉细腻概念，并有效地适应时间变化。我们的方法不需要任何文本标注数据，因此训练非常可扩展。代码、数据集和模型将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Effective-Whole-body-Pose-Estimation-with-Two-stages-Distillation"><a href="#Effective-Whole-body-Pose-Estimation-with-Two-stages-Distillation" class="headerlink" title="Effective Whole-body Pose Estimation with Two-stages Distillation"></a>Effective Whole-body Pose Estimation with Two-stages Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15880">http://arxiv.org/abs/2307.15880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idea-research/dwpose">https://github.com/idea-research/dwpose</a></li>
<li>paper_authors: Zhendong Yang, Ailing Zeng, Chun Yuan, Yu Li</li>
<li>for: 本研究旨在提高全身姿势估计器的效率和精度。</li>
<li>methods: 我们提出了一个两阶段的姿势炼制方法，名为DWPose，以提高姿势估计器的效果和效率。</li>
<li>results: 我们的方法可以在COCO-WholeBody测试集上实现新的顶尖性能，从RTMPose-l的64.8% Whole Body AP提高到66.5%，甚至超过RTMPose-x教师的65.3% AP。<details>
<summary>Abstract</summary>
Whole-body pose estimation localizes the human body, hand, face, and foot keypoints in an image. This task is challenging due to multi-scale body parts, fine-grained localization for low-resolution regions, and data scarcity. Meanwhile, applying a highly efficient and accurate pose estimator to widely human-centric understanding and generation tasks is urgent. In this work, we present a two-stage pose \textbf{D}istillation for \textbf{W}hole-body \textbf{P}ose estimators, named \textbf{DWPose}, to improve their effectiveness and efficiency. The first-stage distillation designs a weight-decay strategy while utilizing a teacher's intermediate feature and final logits with both visible and invisible keypoints to supervise the student from scratch. The second stage distills the student model itself to further improve performance. Different from the previous self-knowledge distillation, this stage finetunes the student's head with only 20% training time as a plug-and-play training strategy. For data limitations, we explore the UBody dataset that contains diverse facial expressions and hand gestures for real-life applications. Comprehensive experiments show the superiority of our proposed simple yet effective methods. We achieve new state-of-the-art performance on COCO-WholeBody, significantly boosting the whole-body AP of RTMPose-l from 64.8% to 66.5%, even surpassing RTMPose-x teacher with 65.3% AP. We release a series of models with different sizes, from tiny to large, for satisfying various downstream tasks. Our codes and models are available at https://github.com/IDEA-Research/DWPose.
</details>
<details>
<summary>摘要</summary>
全身姿势估计本地化人体、手、面、脚关键点在图像中。这项任务因多级体部、细化本地化低分辨率区域以及数据缺乏而具有挑战性。然而，在人类中心的理解和生成任务中应用高效精度的姿势估计器是急需的。在这种情况下，我们提出了一种两阶段的姿势估计精炼方法，称为DWPose。该方法可以提高姿势估计器的效iveness和精度。首先，我们设计了一种权重衰减策略，并利用教师的中间特征和最终归一化值来监督学生从零开始学习。其次，我们将学生模型自身精炼，以进一步提高其性能。与之前的自知ledge精炼不同，这一阶段只需要训练学生的头部，并且只需20%的训练时间。为了Addressing数据缺乏问题，我们探索了UBody数据集，该数据集包含多种表情和手势，适用于实际应用。我们进行了全面的实验，并证明了我们提出的简单 yet effective的方法的优越性。我们在COCO-WholeBody上 achieve新的状态码性能，从64.8%提高到66.5%，甚至超过RTMPose-x教师的65.3%AP。我们释放了不同大小的模型，从tiny到大，以满足不同的下游任务。我们的代码和模型可以在https://github.com/IDEA-Research/DWPose中获取。
</details></li>
</ul>
<hr>
<h2 id="Cross-dimensional-transfer-learning-in-medical-image-segmentation-with-deep-learning"><a href="#Cross-dimensional-transfer-learning-in-medical-image-segmentation-with-deep-learning" class="headerlink" title="Cross-dimensional transfer learning in medical image segmentation with deep learning"></a>Cross-dimensional transfer learning in medical image segmentation with deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15872">http://arxiv.org/abs/2307.15872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hic-messaoudi/cross-dimensional-transfer-learning-in-medical-image-segmentation-with-deep-learning">https://github.com/hic-messaoudi/cross-dimensional-transfer-learning-in-medical-image-segmentation-with-deep-learning</a></li>
<li>paper_authors: Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem, Pierre-Henri Conze</li>
<li>for: This paper is focused on improving the efficiency of medical image segmentation using convolutional neural networks (CNNs) and transfer learning.</li>
<li>methods: The authors propose two novel architectures based on weight transfer and dimensional transfer to adapt a pre-trained 2D CNN to 2D, 3D uni- and multi-modal medical image segmentation tasks.</li>
<li>results: The proposed methods were tested on several benchmarks and achieved promising results, ranking first on the CAMUS challenge and outperforming other 2D-based methods on the CHAOS challenge. The 3D network also achieved good results on the BraTS 2022 competition, with an average Dice score of 91.69% for the whole tumor.<details>
<summary>Abstract</summary>
Over the last decade, convolutional neural networks have emerged and advanced the state-of-the-art in various image analysis and computer vision applications. The performance of 2D image classification networks is constantly improving and being trained on databases made of millions of natural images. However, progress in medical image analysis has been hindered by limited annotated data and acquisition constraints. These limitations are even more pronounced given the volumetry of medical imaging data. In this paper, we introduce an efficient way to transfer the efficiency of a 2D classification network trained on natural images to 2D, 3D uni- and multi-modal medical image segmentation applications. In this direction, we designed novel architectures based on two key principles: weight transfer by embedding a 2D pre-trained encoder into a higher dimensional U-Net, and dimensional transfer by expanding a 2D segmentation network into a higher dimension one. The proposed networks were tested on benchmarks comprising different modalities: MR, CT, and ultrasound images. Our 2D network ranked first on the CAMUS challenge dedicated to echo-cardiographic data segmentation and surpassed the state-of-the-art. Regarding 2D/3D MR and CT abdominal images from the CHAOS challenge, our approach largely outperformed the other 2D-based methods described in the challenge paper on Dice, RAVD, ASSD, and MSSD scores and ranked third on the online evaluation platform. Our 3D network applied to the BraTS 2022 competition also achieved promising results, reaching an average Dice score of 91.69% (91.22%) for the whole tumor, 83.23% (84.77%) for the tumor core, and 81.75% (83.88%) for enhanced tumor using the approach based on weight (dimensional) transfer. Experimental and qualitative results illustrate the effectiveness of our methods for multi-dimensional medical image segmentation.
</details>
<details>
<summary>摘要</summary>
Our approach is based on two key principles: weight transfer and dimensional transfer. We embed a pre-trained 2D encoder into a higher dimensional U-Net to transfer the weights of the 2D network to the 3D network. Additionally, we expand the 2D segmentation network into a higher dimensional one to transfer the dimensionality of the 2D network to the 3D network.We tested our proposed networks on several benchmarks, including MR, CT, and ultrasound images. Our 2D network ranked first on the CAMUS challenge for echo-cardiographic data segmentation and surpassed the state-of-the-art. On the CHAOS challenge, our approach outperformed other 2D-based methods on Dice, RAVD, ASSD, and MSSD scores and ranked third on the online evaluation platform. Our 3D network achieved promising results on the BraTS 2022 competition, with an average Dice score of 91.69% (91.22%) for the whole tumor, 83.23% (84.77%) for the tumor core, and 81.75% (83.88%) for enhanced tumor.Experimental and qualitative results demonstrate the effectiveness of our methods for multi-dimensional medical image segmentation. By leveraging the efficiency of 2D networks and the robustness of 3D networks, our approach offers a promising solution for medical image analysis.
</details></li>
</ul>
<hr>
<h2 id="Catching-Elusive-Depression-via-Facial-Micro-Expression-Recognition"><a href="#Catching-Elusive-Depression-via-Facial-Micro-Expression-Recognition" class="headerlink" title="Catching Elusive Depression via Facial Micro-Expression Recognition"></a>Catching Elusive Depression via Facial Micro-Expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15862">http://arxiv.org/abs/2307.15862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohui Chen, Tie Luo</li>
<li>for: 您的报告是为了诊断隐藏性抑郁症？</li>
<li>methods: 您使用了面部微表情（FMEs）来检测和识别真正的情感？</li>
<li>results: 您的研究发现了一种使用面部特征点来解决检测极低强度和细微的FMEs的方法，并提出了一种低成本、隐私保护的自诊断解决方案，可以在家庭环境中使用手持式移动设备进行诊断。<details>
<summary>Abstract</summary>
Depression is a common mental health disorder that can cause consequential symptoms with continuously depressed mood that leads to emotional distress. One category of depression is Concealed Depression, where patients intentionally or unintentionally hide their genuine emotions through exterior optimism, thereby complicating and delaying diagnosis and treatment and leading to unexpected suicides. In this paper, we propose to diagnose concealed depression by using facial micro-expressions (FMEs) to detect and recognize underlying true emotions. However, the extremely low intensity and subtle nature of FMEs make their recognition a tough task. We propose a facial landmark-based Region-of-Interest (ROI) approach to address the challenge, and describe a low-cost and privacy-preserving solution that enables self-diagnosis using portable mobile devices in a personal setting (e.g., at home). We present results and findings that validate our method, and discuss other technical challenges and future directions in applying such techniques to real clinical settings.
</details>
<details>
<summary>摘要</summary>
抑郁是一种常见的心理健康问题，可能导致持续的沮丧情绪，从而引起情感压力。一种类型的抑郁是隐藏的抑郁，病人可能有意或无意地隐藏真实的情感，从而使诊断和治疗受阻，并可能导致意外的自杀。在这篇论文中，我们提议使用表情微表情（FMEs）来检测和识别隐藏的抑郁。然而，FMEs的非常低敏感度和细微的特征使其识别成为一项困难的任务。我们提议一种面部特征点-基于的区域引用（ROI）方法，以解决这个挑战。我们描述了一种低成本、隐私保护的解决方案，允许个人在家中使用可携带的移动设备进行自诊断。我们提供的结果和发现证明了我们的方法的有效性，并讨论了在实际临床设置中应用这种技术的其他技术挑战和未来方向。
</details></li>
</ul>
<hr>
<h2 id="What-can-Discriminator-do-Towards-Box-free-Ownership-Verification-of-Generative-Adversarial-Network"><a href="#What-can-Discriminator-do-Towards-Box-free-Ownership-Verification-of-Generative-Adversarial-Network" class="headerlink" title="What can Discriminator do? Towards Box-free Ownership Verification of Generative Adversarial Network"></a>What can Discriminator do? Towards Box-free Ownership Verification of Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15860">http://arxiv.org/abs/2307.15860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziheng Huang, Boheng Li, Yan Cai, Run Wang, Shangwei Guo, Liming Fang, Jing Chen, Lina Wang</li>
<li>for: 本研究旨在防止生成器模型被非法盗用或泄露，通过在不选择输入的情况下进行所有权验证。</li>
<li>methods: 我们提出了一种基于权威识别器的IP保护方案，通过训练权威识别器学习一个圆柱体来捕捉生成器唯一的分布。</li>
<li>results: 我们的方案在两个受欢迎的GAN任务和多达10个GAN架构上进行了广泛的评估，并 показа出高效地验证所有权。此外，我们的方案还能够抵御输入基于的移除攻击和其他已有攻击。<details>
<summary>Abstract</summary>
In recent decades, Generative Adversarial Network (GAN) and its variants have achieved unprecedented success in image synthesis. However, well-trained GANs are under the threat of illegal steal or leakage. The prior studies on remote ownership verification assume a black-box setting where the defender can query the suspicious model with specific inputs, which we identify is not enough for generation tasks. To this end, in this paper, we propose a novel IP protection scheme for GANs where ownership verification can be done by checking outputs only, without choosing the inputs (i.e., box-free setting). Specifically, we make use of the unexploited potential of the discriminator to learn a hypersphere that captures the unique distribution learned by the paired generator. Extensive evaluations on two popular GAN tasks and more than 10 GAN architectures demonstrate our proposed scheme to effectively verify the ownership. Our proposed scheme shown to be immune to popular input-based removal attacks and robust against other existing attacks. The source code and models are available at https://github.com/AbstractTeen/gan_ownership_verification
</details>
<details>
<summary>摘要</summary>
近年来，生成对抗网络（GAN）和其变种在图像生成方面取得了历史上无 precedent 的成功。然而，已经训练过的 GAN 受到非法窃取或泄露的威胁。先前的研究中，攻击者可以通过特定输入来访问异常模型，这种黑盒 Setting 我们发现是不够的 для生成任务。为此，在本文中，我们提出了一种新的知识Property protection scheme for GANs，可以通过检查输出来验证所有权，不需要选择输入（即无盒 Setting）。我们利用了generator和discriminator之间的可能性，通过学习一个捕捉生成器学习的唯一分布的卷积。我们的提议方案在两个流行的 GAN 任务和多于10个 GAN 架构上进行了广泛的评估，并示出了有效的所有权验证。我们的提议方案具有免疫输入基于移除攻击和其他现有攻击的特点。source code和模型可以在 <https://github.com/AbstractTeen/gan_ownership_verification> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Seeing-Behind-Dynamic-Occlusions-with-Event-Cameras"><a href="#Seeing-Behind-Dynamic-Occlusions-with-Event-Cameras" class="headerlink" title="Seeing Behind Dynamic Occlusions with Event Cameras"></a>Seeing Behind Dynamic Occlusions with Event Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15829">http://arxiv.org/abs/2307.15829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rong Zou, Manasi Muglikar, Nico Messikommer, Davide Scaramuzza</li>
<li>for: 提高计算机视觉系统的性能，解决干扰物（如尘埃、雨滴、雪花）对计算机视觉系统的影响</li>
<li>methods: 组合传统摄像机和事件摄像机，利用事件提供高时间分辨率的背景内容重建</li>
<li>results: 比image填充方法高3dB的PSNR提高，在我们的数据集上表现出色<details>
<summary>Abstract</summary>
Unwanted camera occlusions, such as debris, dust, rain-drops, and snow, can severely degrade the performance of computer-vision systems. Dynamic occlusions are particularly challenging because of the continuously changing pattern. Existing occlusion-removal methods currently use synthetic aperture imaging or image inpainting. However, they face issues with dynamic occlusions as these require multiple viewpoints or user-generated masks to hallucinate the background intensity. We propose a novel approach to reconstruct the background from a single viewpoint in the presence of dynamic occlusions. Our solution relies for the first time on the combination of a traditional camera with an event camera. When an occlusion moves across a background image, it causes intensity changes that trigger events. These events provide additional information on the relative intensity changes between foreground and background at a high temporal resolution, enabling a truer reconstruction of the background content. We present the first large-scale dataset consisting of synchronized images and event sequences to evaluate our approach. We show that our method outperforms image inpainting methods by 3dB in terms of PSNR on our dataset.
</details>
<details>
<summary>摘要</summary>
不想要的摄像头干扰，如垃圾、尘埃、雨滴和雪，可能严重降低计算机视觉系统的性能。动态干扰特别困难，因为它们的模式在不断变化。现有的干扰除方法使用合成光学投影或图像填充。然而，它们在动态干扰情况下存在问题，因为它们需要多个视点或用户生成的面积图来描述背景强度。我们提出了一种新的方法，使用传统摄像头和事件摄像头的组合来重建背景。当干扰物移动过背景图像时，它会导致强度变化，这些变化会触发事件。这些事件提供高度准确的时间分辨率中的背景内容的重建信息。我们提供了首个大规模的同步图像和事件序列数据集，以评估我们的方法。我们表明，我们的方法在我们的数据集上比图像填充方法高3dB的PSNR。
</details></li>
</ul>
<hr>
<h2 id="Multi-growth-stage-plant-recognition-a-case-study-of-Palmer-amaranth-Amaranthus-palmeri-in-cotton-Gossypium-hirsutum"><a href="#Multi-growth-stage-plant-recognition-a-case-study-of-Palmer-amaranth-Amaranthus-palmeri-in-cotton-Gossypium-hirsutum" class="headerlink" title="Multi-growth stage plant recognition: a case study of Palmer amaranth (Amaranthus palmeri) in cotton (Gossypium hirsutum)"></a>Multi-growth stage plant recognition: a case study of Palmer amaranth (Amaranthus palmeri) in cotton (Gossypium hirsutum)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15816">http://arxiv.org/abs/2307.15816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy RY Coleman, Matthew Kutugata, Michael J Walsh, Muthukumar Bagavathiannan</li>
<li>For: The paper is focused on developing and testing a method for recognizing growth stages of Amaranthus palmeri (a weed plant in cotton production) using convolutional neural networks (CNNs) and the You Only Look Once (YOLO) architecture.* Methods: The authors use 26 different architecture variants from YOLO v3, v5, v6, v6 3.0, v7, and v8 to recognize eight different growth stages of A. palmeri. They compare the performance of these architectures on an eight-class growth stage dataset and use class activation maps (CAM) to understand model attention on the complex dataset.* Results: The highest mAP@[0.5:0.95] for recognition of all growth stage classes was 47.34% achieved by v8-X, with inter-class confusion across visually similar growth stages. With all growth stages grouped as a single class, performance increased, with a maximum mAP@[0.5:0.95] of 67.05% achieved by v7-Original. Single class recall of up to 81.42% was achieved by v5-X, and precision of up to 89.72% was achieved by v8-X.<details>
<summary>Abstract</summary>
Many advanced, image-based precision agricultural technologies for plant breeding, field crop research, and site-specific crop management hinge on the reliable detection and phenotyping of plants across highly variable morphological growth stages. Convolutional neural networks (CNNs) have shown promise for image-based plant phenotyping and weed recognition, but their ability to recognize growth stages, often with stark differences in appearance, is uncertain. Amaranthus palmeri (Palmer amaranth) is a particularly challenging weed plant in cotton (Gossypium hirsutum) production, exhibiting highly variable plant morphology both across growth stages over a growing season, as well as between plants at a given growth stage due to high genetic diversity. In this paper, we investigate eight-class growth stage recognition of A. palmeri in cotton as a challenging model for You Only Look Once (YOLO) architectures. We compare 26 different architecture variants from YOLO v3, v5, v6, v6 3.0, v7, and v8 on an eight-class growth stage dataset of A. palmeri. The highest mAP@[0.5:0.95] for recognition of all growth stage classes was 47.34% achieved by v8-X, with inter-class confusion across visually similar growth stages. With all growth stages grouped as a single class, performance increased, with a maximum mean average precision (mAP@[0.5:0.95]) of 67.05% achieved by v7-Original. Single class recall of up to 81.42% was achieved by v5-X, and precision of up to 89.72% was achieved by v8-X. Class activation maps (CAM) were used to understand model attention on the complex dataset. Fewer classes, grouped by visual or size features improved performance over the ground-truth eight-class dataset. Successful growth stage detection highlights the substantial opportunity for improving plant phenotyping and weed recognition technologies with open-source object detection architectures.
</details>
<details>
<summary>摘要</summary>
Many advanced, image-based precision agricultural technologies for plant breeding, field crop research, and site-specific crop management rely on the reliable detection and phenotyping of plants across highly variable morphological growth stages. Convolutional neural networks (CNNs) have shown promise for image-based plant phenotyping and weed recognition, but their ability to recognize growth stages, often with stark differences in appearance, is uncertain. Amaranthus palmeri (Palmer amaranth) is a particularly challenging weed plant in cotton (Gossypium hirsutum) production, exhibiting highly variable plant morphology both across growth stages over a growing season and between plants at a given growth stage due to high genetic diversity. In this paper, we investigate eight-class growth stage recognition of A. palmeri in cotton as a challenging model for You Only Look Once (YOLO) architectures. We compare 26 different architecture variants from YOLO v3, v5, v6, v6 3.0, v7, and v8 on an eight-class growth stage dataset of A. palmeri. The highest mAP@[0.5:0.95] for recognition of all growth stage classes was 47.34% achieved by v8-X, with inter-class confusion across visually similar growth stages. With all growth stages grouped as a single class, performance increased, with a maximum mean average precision (mAP@[0.5:0.95]) of 67.05% achieved by v7-Original. Single class recall of up to 81.42% was achieved by v5-X, and precision of up to 89.72% was achieved by v8-X. Class activation maps (CAM) were used to understand model attention on the complex dataset. Fewer classes, grouped by visual or size features improved performance over the ground-truth eight-class dataset. Successful growth stage detection highlights the substantial opportunity for improving plant phenotyping and weed recognition technologies with open-source object detection architectures.
</details></li>
</ul>
<hr>
<h2 id="VPP-Efficient-Conditional-3D-Generation-via-Voxel-Point-Progressive-Representation"><a href="#VPP-Efficient-Conditional-3D-Generation-via-Voxel-Point-Progressive-Representation" class="headerlink" title="VPP: Efficient Conditional 3D Generation via Voxel-Point Progressive Representation"></a>VPP: Efficient Conditional 3D Generation via Voxel-Point Progressive Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16605">http://arxiv.org/abs/2307.16605</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qizekun/vpp">https://github.com/qizekun/vpp</a></li>
<li>paper_authors: Zekun Qi, Muzhou Yu, Runpei Dong, Kaisheng Ma</li>
<li>for: 这篇论文主要用于提高3D生成的效率和质量。</li>
<li>methods: 该论文提出了一种进步的生成方法，即精细体量进行渐进生成（VPP），该方法结合了精细体量表示法和点云拼接技术，以实现高效的多类Object生成。</li>
<li>results: 实验表明，VPP可以高效地生成高质量的8K点云数据，并且可以 Transfer Learning 到多种3D下渠道任务，如生成、编辑、完成和预训练。<details>
<summary>Abstract</summary>
Conditional 3D generation is undergoing a significant advancement, enabling the free creation of 3D content from inputs such as text or 2D images. However, previous approaches have suffered from low inference efficiency, limited generation categories, and restricted downstream applications. In this work, we revisit the impact of different 3D representations on generation quality and efficiency. We propose a progressive generation method through Voxel-Point Progressive Representation (VPP). VPP leverages structured voxel representation in the proposed Voxel Semantic Generator and the sparsity of unstructured point representation in the Point Upsampler, enabling efficient generation of multi-category objects. VPP can generate high-quality 8K point clouds within 0.2 seconds. Additionally, the masked generation Transformer allows for various 3D downstream tasks, such as generation, editing, completion, and pre-training. Extensive experiments demonstrate that VPP efficiently generates high-fidelity and diverse 3D shapes across different categories, while also exhibiting excellent representation transfer performance. Codes will be released on https://github.com/qizekun/VPP.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>Conditional 3D生成在进行了重要的进步，允许根据文本或2D图像生成3D内容。然而，之前的方法受到了低效率推理、有限的生成类别和下游应用的限制。在这种工作中，我们重新评估了不同的3D表示方式对生成质量和效率的影响。我们提议一种逐步生成方法，通过精细 voxel 表示法和简单点表示法的结合，实现高效的多类对象生成。我们称之为精细点进程表示（VPP）。VPP可以在0.2秒内生成8K点云，并且可以进行多种3D下游任务，如生成、编辑、完成和预训练。广泛的实验表明，VPP可以高效地生成多种不同类别的高质量3D形状，同时也展现出了优秀的表示转移性能。代码将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Object-Detection-in-the-Open-World"><a href="#Semi-Supervised-Object-Detection-in-the-Open-World" class="headerlink" title="Semi-Supervised Object Detection in the Open World"></a>Semi-Supervised Object Detection in the Open World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15710">http://arxiv.org/abs/2307.15710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Garvita Allabadi, Ana Lucic, Peter Pao-Huang, Yu-Xiong Wang, Vikram Adve</li>
<li>for: 本研究旨在开普世 semi-supervised object detection 领域，解决预设固定类目存在于训练和无标签数据中的问题。</li>
<li>methods: 我们提出了 Open World Semi-supervised Detection 框架 (OWSSD)，可以有效地探测异distribution（OOD）样本，同时还可以从这些样本中学习。我们还提出了一个ensemble基于自动编码网络，通过仅使用固定类目数据进行训练。</li>
<li>results: 我们通过广泛的评估表明，我们的方法可以与现状的OOD探测算法竞争，同时也可以在开放世界enario中显著提高 semi-supervised 学习性能。<details>
<summary>Abstract</summary>
Existing approaches for semi-supervised object detection assume a fixed set of classes present in training and unlabeled datasets, i.e., in-distribution (ID) data. The performance of these techniques significantly degrades when these techniques are deployed in the open-world, due to the fact that the unlabeled and test data may contain objects that were not seen during training, i.e., out-of-distribution (OOD) data. The two key questions that we explore in this paper are: can we detect these OOD samples and if so, can we learn from them? With these considerations in mind, we propose the Open World Semi-supervised Detection framework (OWSSD) that effectively detects OOD data along with a semi-supervised learning pipeline that learns from both ID and OOD data. We introduce an ensemble based OOD detector consisting of lightweight auto-encoder networks trained only on ID data. Through extensive evalulation, we demonstrate that our method performs competitively against state-of-the-art OOD detection algorithms and also significantly boosts the semi-supervised learning performance in open-world scenarios.
</details>
<details>
<summary>摘要</summary>
现有的半指导object detection方法假设training和无标据数据集中有固定的类型存在，即在distribution（ID）数据。在开放世界中部署这些技术时，其性能会受到很大的影响，因为测试数据可能包含不同于训练数据中的对象，即out-of-distribution（OOD）数据。我们在这篇论文中考虑以下两个关键问题：可以探测OOD样本吗，如果可以，我们可以学习吗？针对这些考虑，我们提出了开放世界半指导检测框架（OWSSD），可以有效地检测OOD数据，同时还可以从ID和OOD数据中学习。我们提出了一种 ensemble 基于 auto-encoder 网络，只在 ID 数据上训练。经过广泛的评估，我们示出了我们的方法与现有的OOD检测算法相比，性能具有竞争力，同时还可以在开放世界 scenarios 中提高半指导学习性能。
</details></li>
</ul>
<hr>
<h2 id="MeMOTR-Long-Term-Memory-Augmented-Transformer-for-Multi-Object-Tracking"><a href="#MeMOTR-Long-Term-Memory-Augmented-Transformer-for-Multi-Object-Tracking" class="headerlink" title="MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking"></a>MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15700">http://arxiv.org/abs/2307.15700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mcg-nju/memotr">https://github.com/mcg-nju/memotr</a></li>
<li>paper_authors: Ruopeng Gao, Limin Wang</li>
<li>for: 本研究旨在提高多目标追踪（MOT）中的时间信息捕捉效果，尤其是在短时间内的目标追踪 tasks 上。</li>
<li>methods: 我们提出了一种基于Transformer的长期记忆扩展方法，named MeMOTR，通过将长期记忆注入到自定义的记忆注意力层中，使同一个目标的追踪嵌入更加稳定和区别化。</li>
<li>results: 我们在DanceTrack上进行了实验，结果显示MeMOTR比前一代方法提高7.9%和13.0%的HOTA和AssA指标，并且在MOT17和BDD100K上也超越了其他Transformer-based方法的协议性表现。<details>
<summary>Abstract</summary>
As a video task, Multiple Object Tracking (MOT) is expected to capture temporal information of targets effectively. Unfortunately, most existing methods only explicitly exploit the object features between adjacent frames, while lacking the capacity to model long-term temporal information. In this paper, we propose MeMOTR, a long-term memory-augmented Transformer for multi-object tracking. Our method is able to make the same object's track embedding more stable and distinguishable by leveraging long-term memory injection with a customized memory-attention layer. This significantly improves the target association ability of our model. Experimental results on DanceTrack show that MeMOTR impressively surpasses the state-of-the-art method by 7.9% and 13.0% on HOTA and AssA metrics, respectively. Furthermore, our model also outperforms other Transformer-based methods on association performance on MOT17 and generalizes well on BDD100K. Code is available at https://github.com/MCG-NJU/MeMOTR.
</details>
<details>
<summary>摘要</summary>
作为视频任务，多目标跟踪（MOT）预期能够有效捕捉目标的时间信息。然而，大多数现有方法只是直接利用邻帧对象特征，而忽略了长期时间信息的模型。在这篇论文中，我们提出了MeMOTR，一种带有长期记忆的Transformer型多目标跟踪方法。我们的方法可以使同一个目标的跟踪嵌入更加稳定和分化，通过自定义的记忆注意力层进行长期记忆注入。这意味着我们的模型可以更好地进行目标相关性能。实验结果表明，MeMOTR在DanceTrack上表现出色，与比较方法相比，提高了7.9%和13.0%的HOTA和AssA指标。此外，我们的模型还超过了其他基于Transformer的方法在相关性能上，并在MOT17和BDD100K上进行了良好的总体化。代码可以在https://github.com/MCG-NJU/MeMOTR中找到。
</details></li>
</ul>
<hr>
<h2 id="SimDETR-Simplifying-self-supervised-pretraining-for-DETR"><a href="#SimDETR-Simplifying-self-supervised-pretraining-for-DETR" class="headerlink" title="SimDETR: Simplifying self-supervised pretraining for DETR"></a>SimDETR: Simplifying self-supervised pretraining for DETR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15697">http://arxiv.org/abs/2307.15697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Maniadis Metaxas, Adrian Bulat, Ioannis Patras, Brais Martinez, Georgios Tzimiropoulos</li>
<li>for: 提高 DETR 基于检测器的效果，提高 sample efficiency 和速度</li>
<li>methods: 使用无监督预训练，使用高级特征图生成更加具有 semantics 的初始提案，使用对象pseudo标签进行推训，进行自我训练</li>
<li>results: 比对先前的预训练方法，我们的预训练方法在全数据和低数据 régime 都具有显著的提升，可以直接在复杂图像 datasets 上预训练 DETR 从头开始<details>
<summary>Abstract</summary>
DETR-based object detectors have achieved remarkable performance but are sample-inefficient and exhibit slow convergence. Unsupervised pretraining has been found to be helpful to alleviate these impediments, allowing training with large amounts of unlabeled data to improve the detector's performance. However, existing methods have their own limitations, like keeping the detector's backbone frozen in order to avoid performance degradation and utilizing pretraining objectives misaligned with the downstream task. To overcome these limitations, we propose a simple pretraining framework for DETR-based detectors that consists of three simple yet key ingredients: (i) richer, semantics-based initial proposals derived from high-level feature maps, (ii) discriminative training using object pseudo-labels produced via clustering, (iii) self-training to take advantage of the improved object proposals learned by the detector. We report two main findings: (1) Our pretraining outperforms prior DETR pretraining works on both the full and low data regimes by significant margins. (2) We show we can pretrain DETR from scratch (including the backbone) directly on complex image datasets like COCO, paving the path for unsupervised representation learning directly using DETR.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>基于高级特征图像的丰富Semantic-based初始提案，来提高检测器的性能。2. 使用对象 Pseudo-标签生成的推理训练，以提高检测器的精度。3. 使用自我训练，以利用改进的对象提案来提高检测器的性能。我们的研究发现：1. 我们的预训练在完整数据集和低数据集上都比过去的 DETR 预训练工作表现出了显著的改善。2. 我们可以直接在复杂的图像 datasets 上预训练 DETR，从而开展无监督表征学学习。</details></li>
</ol>
<hr>
<h2 id="PatchMixer-Rethinking-network-design-to-boost-generalization-for-3D-point-cloud-understanding"><a href="#PatchMixer-Rethinking-network-design-to-boost-generalization-for-3D-point-cloud-understanding" class="headerlink" title="PatchMixer: Rethinking network design to boost generalization for 3D point cloud understanding"></a>PatchMixer: Rethinking network design to boost generalization for 3D point cloud understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15692">http://arxiv.org/abs/2307.15692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davideboscaini/patchmixer">https://github.com/davideboscaini/patchmixer</a></li>
<li>paper_authors: Davide Boscaini, Fabio Poiesi</li>
<li>for: 本研究旨在评估深度学习方法对3D点云理解的能力，并提出一种简单 yet effective的方法来扩展MLP-Mixer纸质。</li>
<li>methods: 本方法使用本地小块处理而不是整个形状，以促进对部分点云的稳定性，并使用MLP进行局部特征聚合。</li>
<li>results: 我们在形态分类和部分分割任务中评估了我们的方法，与一些相关的深度架构进行比较，得到了更好的总体适应性表现。<details>
<summary>Abstract</summary>
The recent trend in deep learning methods for 3D point cloud understanding is to propose increasingly sophisticated architectures either to better capture 3D geometries or by introducing possibly undesired inductive biases. Moreover, prior works introducing novel architectures compared their performance on the same domain, devoting less attention to their generalization to other domains. We argue that the ability of a model to transfer the learnt knowledge to different domains is an important feature that should be evaluated to exhaustively assess the quality of a deep network architecture. In this work we propose PatchMixer, a simple yet effective architecture that extends the ideas behind the recent MLP-Mixer paper to 3D point clouds. The novelties of our approach are the processing of local patches instead of the whole shape to promote robustness to partial point clouds, and the aggregation of patch-wise features using an MLP as a simpler alternative to the graph convolutions or the attention mechanisms that are used in prior works. We evaluated our method on the shape classification and part segmentation tasks, achieving superior generalization performance compared to a selection of the most relevant deep architectures.
</details>
<details>
<summary>摘要</summary>
现代深度学习方法的趋势是不断提出更加复杂的架构，以更好地捕捉3D形态或者引入可能不希望的逻辑偏见。然而，先前的工作通常只在同一个领域中评估其性能，对其在其他领域的泛化性能 menos 关注。我们认为，深度网络模型在不同领域中的泛化性能是一项重要的评估标准。在这篇文章中，我们提出了PatchMixer，一种简单 yet effective的架构，扩展了最近的 MLP-Mixer 文献中的想法，并对3D 点云进行处理。我们的方法的创新之处在于处理本地小块而不是整个形态，以便增强对部分点云的Robustness，以及通过 MLP 作为更简单的替代品，对于先前的图像 convolution 或者注意力机制进行聚合。我们对 shape classification 和部分 segmentation 任务进行评估，并实现了相比一些最相关的深度架构的更好的泛化性能。
</details></li>
</ul>
<hr>
<h2 id="TrackAgent-6D-Object-Tracking-via-Reinforcement-Learning"><a href="#TrackAgent-6D-Object-Tracking-via-Reinforcement-Learning" class="headerlink" title="TrackAgent: 6D Object Tracking via Reinforcement Learning"></a>TrackAgent: 6D Object Tracking via Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15671">http://arxiv.org/abs/2307.15671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantin Röhrl, Dominik Bauer, Timothy Patten, Markus Vincze</li>
<li>for:  Tracking an object’s 6D pose in robotics and augmented reality applications, while either the object or the observing camera is moving.</li>
<li>methods:  Simplify object tracking to a reinforced point cloud (depth only) alignment task, using a streamlined approach with limited amounts of sparse 3D point clouds and a reinforcement learning (RL) agent that jointly solves for both objectives.</li>
<li>results:  The RL agent’s uncertainty and a rendering-based mask propagation are effective reinitialization triggers, and the proposed method outperforms previous RGB(D)-based methods in terms of computational efficiency and robustness to tracking loss.<details>
<summary>Abstract</summary>
Tracking an object's 6D pose, while either the object itself or the observing camera is moving, is important for many robotics and augmented reality applications. While exploiting temporal priors eases this problem, object-specific knowledge is required to recover when tracking is lost. Under the tight time constraints of the tracking task, RGB(D)-based methods are often conceptionally complex or rely on heuristic motion models. In comparison, we propose to simplify object tracking to a reinforced point cloud (depth only) alignment task. This allows us to train a streamlined approach from scratch with limited amounts of sparse 3D point clouds, compared to the large datasets of diverse RGBD sequences required in previous works. We incorporate temporal frame-to-frame registration with object-based recovery by frame-to-model refinement using a reinforcement learning (RL) agent that jointly solves for both objectives. We also show that the RL agent's uncertainty and a rendering-based mask propagation are effective reinitialization triggers.
</details>
<details>
<summary>摘要</summary>
Tracking an object's 6D pose, while either the object itself or the observing camera is moving, is important for many robotics and augmented reality applications. While exploiting temporal priors eases this problem, object-specific knowledge is required to recover when tracking is lost. Under the tight time constraints of the tracking task, RGB(D)-based methods are often conceptionally complex or rely on heuristic motion models. In comparison, we propose to simplify object tracking to a reinforced point cloud (depth only) alignment task. This allows us to train a streamlined approach from scratch with limited amounts of sparse 3D point clouds, compared to the large datasets of diverse RGBD sequences required in previous works. We incorporate temporal frame-to-frame registration with object-based recovery by frame-to-model refinement using a reinforcement learning (RL) agent that jointly solves for both objectives. We also show that the RL agent's uncertainty and a rendering-based mask propagation are effective reinitialization triggers.Translation notes:* "6D pose" is translated as "6D位姿" (liù dì wèi xìng)* "RGB(D)" is translated as "RGB(D)" (RGB(D) séqùì)* "point cloud" is translated as "点云" (diǎn yú)* "reinforced" is translated as "加强" (jiā qiáng)* "streamlined" is translated as "流畅" (liú chóng)* "sparse" is translated as "稀疏" (xī shōu)* "temporal" is translated as "时间" (shí jian)* "frame-to-frame" is translated as "帧到帧" (kuàng dào kuàng)* "object-based" is translated as "基于物体" (jī yú wù tǐ)* "reinitialization" is translated as "重新初始化" (zhòng xīn chū shí huà)* "uncertainty" is translated as "不确定性" (bù jì dì xìng)* "rendering-based" is translated as "基于渲染" (jī yú yǎo chéng)
</details></li>
</ul>
<hr>
<h2 id="Multi-layer-Aggregation-as-a-key-to-feature-based-OOD-detection"><a href="#Multi-layer-Aggregation-as-a-key-to-feature-based-OOD-detection" class="headerlink" title="Multi-layer Aggregation as a key to feature-based OOD detection"></a>Multi-layer Aggregation as a key to feature-based OOD detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15647">http://arxiv.org/abs/2307.15647</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/benolmbrt/MedicOOD">https://github.com/benolmbrt/MedicOOD</a></li>
<li>paper_authors: Benjamin Lambert, Florence Forbes, Senan Doyle, Michel Dojat</li>
<li>for: 本研究旨在探讨基于深度学习模型的异常检测方法，以提高医学图像分析中的精度和可靠性。</li>
<li>methods: 本研究使用的方法包括单层方法和多层方法，单层方法根据特定层获得的特征图进行检测，多层方法则使用模型生成的ensemble特征图进行检测。</li>
<li>results: 本研究对20种异常类型（约7800个3D MRI）进行了大规模的异常检测实验，结果表明多层方法在异常检测中表现更好，而单层方法则具有不一致的行为，具体取决于异常类型。此外，研究还发现了模型的结构对异常检测性能产生了很大影响。<details>
<summary>Abstract</summary>
Deep Learning models are easily disturbed by variations in the input images that were not observed during the training stage, resulting in unpredictable predictions. Detecting such Out-of-Distribution (OOD) images is particularly crucial in the context of medical image analysis, where the range of possible abnormalities is extremely wide. Recently, a new category of methods has emerged, based on the analysis of the intermediate features of a trained model. These methods can be divided into 2 groups: single-layer methods that consider the feature map obtained at a fixed, carefully chosen layer, and multi-layer methods that consider the ensemble of the feature maps generated by the model. While promising, a proper comparison of these algorithms is still lacking. In this work, we compared various feature-based OOD detection methods on a large spectra of OOD (20 types), representing approximately 7800 3D MRIs. Our experiments shed the light on two phenomenons. First, multi-layer methods consistently outperform single-layer approaches, which tend to have inconsistent behaviour depending on the type of anomaly. Second, the OOD detection performance highly depends on the architecture of the underlying neural network.
</details>
<details>
<summary>摘要</summary>
深度学习模型容易受到输入图像的变化所影响，导致预测结果不可预测。在医学图像分析中，检测这些外围数据（Out-of-Distribution，OOD）图像特别重要。近期，一种新的分类方法在发展，基于模型的中间特征分析。这些方法可以分为两类：单层方法，利用特定层的特征图，和多层方法，利用模型生成的特征图的ensemble。虽有承诺，但对这些算法进行正确的比较仍然缺乏。在这项工作中，我们对20种OOD类型（约7800个3D MRI）进行了大规模的比较。我们的实验揭示了两个现象：首先，多层方法在不同类型的异常情况下表现更好，单层方法则具有不一致的行为；其次，OOD检测性能强烈依赖于下面的神经网络架构。
</details></li>
</ul>
<hr>
<h2 id="Scale-aware-Test-time-Click-Adaptation-for-Pulmonary-Nodule-and-Mass-Segmentation"><a href="#Scale-aware-Test-time-Click-Adaptation-for-Pulmonary-Nodule-and-Mass-Segmentation" class="headerlink" title="Scale-aware Test-time Click Adaptation for Pulmonary Nodule and Mass Segmentation"></a>Scale-aware Test-time Click Adaptation for Pulmonary Nodule and Mass Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15645">http://arxiv.org/abs/2307.15645</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/splinterli/sattca">https://github.com/splinterli/sattca</a></li>
<li>paper_authors: Zhihao Li, Jiancheng Yang, Yongchao Xu, Li Zhang, Wenhui Dong, Bo Du</li>
<li>for: 针对肺癌检测中肺脏异常的尺寸管理，提出了一种基于多尺度神经网络的尺度意识适应测试方法。</li>
<li>methods: 提出了一种基于简单的Click适应方法，通过使用轻松获得的病理点击来提高分 segmentation 性能，特别是大型肿瘤的检测。</li>
<li>results: EXTENSIVE EXPERIMENTS ON BOTH OPEN-SOURCE AND IN-HOUSE DATASETS CONSISTENTLY DEMONSTRATE THE EFFECTIVENESS OF THE PROPOSED METHOD OVER SOME CNN AND TRANSFORMER-BASED SEGMENTATION METHODS。<details>
<summary>Abstract</summary>
Pulmonary nodules and masses are crucial imaging features in lung cancer screening that require careful management in clinical diagnosis. Despite the success of deep learning-based medical image segmentation, the robust performance on various sizes of lesions of nodule and mass is still challenging. In this paper, we propose a multi-scale neural network with scale-aware test-time adaptation to address this challenge. Specifically, we introduce an adaptive Scale-aware Test-time Click Adaptation method based on effortlessly obtainable lesion clicks as test-time cues to enhance segmentation performance, particularly for large lesions. The proposed method can be seamlessly integrated into existing networks. Extensive experiments on both open-source and in-house datasets consistently demonstrate the effectiveness of the proposed method over some CNN and Transformer-based segmentation methods. Our code is available at https://github.com/SplinterLi/SaTTCA
</details>
<details>
<summary>摘要</summary>
肺部肿瘤和质量是链球癌检测中重要的图像特征，需要精确的诊断管理。尽管深度学习基础的医疗图像分类已经取得成功，但是不同大小的肿瘤和质量的性能仍然是挑战。在这篇论文中，我们提出了一个多尺度神经网络，并导入了渠道对应的测试时间适应方法。具体来说，我们引入了适应状态测试时间Click整合方法，根据易доступible的肿瘤点击作为测试时间启发，以提高分类性能，特别是大肿瘤。提案的方法可以与现有的网络集成。我们在多个开源和内部数据集上进行了广泛的实验，结果显示了提案方法的效果，比如CNN和Transformer基础的分类方法。我们的代码可以在https://github.com/SplinterLi/SaTTCA中下载。
</details></li>
</ul>
<hr>
<h2 id="CLIP-Brings-Better-Features-to-Visual-Aesthetics-Learners"><a href="#CLIP-Brings-Better-Features-to-Visual-Aesthetics-Learners" class="headerlink" title="CLIP Brings Better Features to Visual Aesthetics Learners"></a>CLIP Brings Better Features to Visual Aesthetics Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15640">http://arxiv.org/abs/2307.15640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liwu Xu, Jinjin Xu, Yuzhe Yang, Yijie Huang, Yanchun Xie, Yaqian Li<br>for:The paper is written for the application of image aesthetics assessment (IAA), which has a subjective and expensive labeling procedure.methods:The proposed method uses a two-phase approach, which integrates and leverages a multi-source unlabeled dataset to align rich features between a given visual encoder and an off-the-shelf CLIP image encoder via feature alignment loss.results:The proposed method achieves state-of-the-art performance on multiple widely used IAA benchmarks, alleviating the feature collapse issue and showcasing the necessity of feature alignment instead of training directly based on CLIP image encoder.Here is the Chinese version of the three key points:for:这篇论文是为了应用图像美学评估（IAA），这是一个主观和昂贵的标注过程。methods:提议的方法使用了两个阶段的方法，即将多源无标记数据集集成并利用，以实现给定视觉编码器和Off-the-shelf CLIP图像编码器之间的特征匹配，并通过特征匹配损失来进行协调。results:提议的方法在多个广泛使用的IAA标准准点上达到了状态机器人的性能，解决了特征塌积问题，并证明了在不直接基于CLIP图像编码器进行训练的情况下，特征匹配是必不可少的。<details>
<summary>Abstract</summary>
The success of pre-training approaches on a variety of downstream tasks has revitalized the field of computer vision. Image aesthetics assessment (IAA) is one of the ideal application scenarios for such methods due to subjective and expensive labeling procedure. In this work, an unified and flexible two-phase \textbf{C}LIP-based \textbf{S}emi-supervised \textbf{K}nowledge \textbf{D}istillation paradigm is proposed, namely \textbf{\textit{CSKD}. Specifically, we first integrate and leverage a multi-source unlabeled dataset to align rich features between a given visual encoder and an off-the-shelf CLIP image encoder via feature alignment loss. Notably, the given visual encoder is not limited by size or structure and, once well-trained, it can seamlessly serve as a better visual aesthetic learner for both student and teacher. In the second phase, the unlabeled data is also utilized in semi-supervised IAA learning to further boost student model performance when applied in latency-sensitive production scenarios. By analyzing the attention distance and entropy before and after feature alignment, we notice an alleviation of feature collapse issue, which in turn showcase the necessity of feature alignment instead of training directly based on CLIP image encoder. Extensive experiments indicate the superiority of CSKD, which achieves state-of-the-art performance on multiple widely used IAA benchmarks.
</details>
<details>
<summary>摘要</summary>
随着预训练方法在多种下游任务上的成功，计算机视觉领域得到了新的动力。图像美学评价（IAA）是适用于这些方法的理想应用场景，因为评价标签过程是主观且昂贵的。在这项工作中，我们提出了一种统一和灵活的两阶段\textbf{C}LIP-基于\textbf{S}emi-supervised \textbf{K}nowledge \textbf{D}istillation（CSKD）方法。具体来说，我们首先将多种无标注数据集集成并利用，以确保图像编码器和CLIP图像编码器之间的特征相似性。注意，给定的图像编码器没有尺寸或结构的限制，一旦它很好地训练，它就可以成为更好的图像美学学习者。在第二阶段，我们还利用无标注数据集进行 semi-supervised IAA 学习，以进一步提高学生模型在响应时间敏感的生产环境中的性能。通过分析特征距离和熵之前和之后准对，我们发现了特征坍塌问题的缓解，这反映了特征准对的重要性，而不是直接基于CLIP图像编码器进行训练。我们进行了广泛的实验，并证明了 CSKD 的优越性，在多个广泛使用的 IAA benchmark 上达到了状态之最的表现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/29/cs.CV_2023_07_29/" data-id="clpahu72f00h83h88cwbx2udx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/29/cs.AI_2023_07_29/" class="article-date">
  <time datetime="2023-07-29T12:00:00.000Z" itemprop="datePublished">2023-07-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/29/cs.AI_2023_07_29/">cs.AI - 2023-07-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Marrying-Dialogue-Systems-with-Data-Visualization-Interactive-Data-Visualization-Generation-from-Natural-Language-Conversations"><a href="#Marrying-Dialogue-Systems-with-Data-Visualization-Interactive-Data-Visualization-Generation-from-Natural-Language-Conversations" class="headerlink" title="Marrying Dialogue Systems with Data Visualization: Interactive Data Visualization Generation from Natural Language Conversations"></a>Marrying Dialogue Systems with Data Visualization: Interactive Data Visualization Generation from Natural Language Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16013">http://arxiv.org/abs/2307.16013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanfeng Song, Xuefang Zhao, Raymond Chi-Wing Wong</li>
<li>for: 本研究旨在提高数据视化（DV）系统的使用效率，通过自动化DV任务，如自然语言问题（NLQ）到视化翻译（formally called text-to-vis）。</li>
<li>methods: 本研究提出了一个新任务名为CoVis，即对话式文本到视化，旨在通过用户和系统之间的多次交互来构建DV。</li>
<li>results: 研究人员建立了一个名为Dial-NVBench的benchmark dataset，并提出了一种多模式神经网络名为MMCoVisNet，可以回答DV相关的问题。MMCoVisNet使用对话 контекст进行全面理解，然后使用适应性decoder提供相应的回答。实验结果表明，MMCoVisNet在比较基eline上表现出色，达到了状态略。<details>
<summary>Abstract</summary>
Data visualization (DV) has become the prevailing tool in the market due to its effectiveness into illustrating insights in vast amounts of data. To lower the barrier of using DVs, automatic DV tasks, such as natural language question (NLQ) to visualization translation (formally called text-to-vis), have been investigated in the research community. However, text-to-vis assumes the NLQ to be well-organized and expressed in a single sentence. However, in real-world settings, complex DV is needed through consecutive exchanges between the DV system and the users. In this paper, we propose a new task named CoVis, short for Conversational text-to-Visualization, aiming at constructing DVs through a series of interactions between users and the system. Since it is the task which has not been studied in the literature, we first build a benchmark dataset named Dial-NVBench, including dialogue sessions with a sequence of queries from a user and responses from the system. Then, we propose a multi-modal neural network named MMCoVisNet to answer these DV-related queries. In particular, MMCoVisNet first fully understands the dialogue context and determines the corresponding responses. Then, it uses adaptive decoders to provide the appropriate replies: (i) a straightforward text decoder is used to produce general responses, (ii) an SQL-form decoder is applied to synthesize data querying responses, and (iii) a DV-form decoder tries to construct the appropriate DVs. We comparatively evaluate MMCoVisNet with other baselines over our proposed benchmark dataset. Experimental results validate that MMCoVisNet performs better than existing baselines and achieves a state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
数据视化（DV）已成为市场上最受欢迎的工具，因为它能够快速和有效地表示大量数据中的意见。为了降低使用DV的门槛，研究者们已经对自动DV任务进行了详细的研究，如自然语言问题（NLQ）到视觉翻译（文本至图）。然而，文本至图假设NLQ是结束的和整洁的单句话。在实际应用中，需要通过多次交互来构建复杂的DV。在这篇论文中，我们提出了一个新的任务，即对话式文本至视觉（CoVis），旨在通过用户和系统之间的多次交互来构建DV。由于这是Literature中没有研究过的任务，我们首先构建了一个名为Dial-NVBench的 benchmark数据集，包括用户和系统之间的对话会话，以及一系列关于DV的查询。然后，我们提出了一种多模态神经网络名为MMCoVisNet，用于回答这些DV相关的查询。具体来说，MMCoVisNet首先完全理解对话上下文，然后确定相应的回答。接着，它使用适应编码器提供相应的答案，包括：（i）一般文本编码器用于生成通用答案，（ii）SQL形式编码器用于生成数据查询答案，（iii）DV形式编码器用于构建适当的DV。我们对MMCoVisNet进行了与其他基准点进行比较的实验，并证明它在我们提出的benchmark数据集上表现出色。
</details></li>
</ul>
<hr>
<h2 id="RoCar-A-Relationship-Network-based-Evaluation-Method-to-Large-Language-Models"><a href="#RoCar-A-Relationship-Network-based-Evaluation-Method-to-Large-Language-Models" class="headerlink" title="RoCar: A Relationship Network-based Evaluation Method to Large Language Models"></a>RoCar: A Relationship Network-based Evaluation Method to Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15997">http://arxiv.org/abs/2307.15997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neu-datamining/rocar">https://github.com/neu-datamining/rocar</a></li>
<li>paper_authors: Ming Wang, Wenfang Wu, Chongyun Gao, Daling Wang, Shi Feng, Yifei Zhang</li>
<li>for: 评估大语言模型（LLMs）的能力</li>
<li>methods: 使用定义的基本模式Random constructions of task graphs and generates natural language evaluation tasks to evaluate LLMs’ reasoning and memory abilities</li>
<li>results:  Ensures fairness of evaluation method by preventing LLMs from directly learning the evaluation tasks<details>
<summary>Abstract</summary>
Large language models (LLMs) have received increasing attention. However, due to the complexity of its capabilities, how to rationally evaluate the capabilities of LLMs is still a task to be solved. We propose the RoCar method, which utilizes the defined basic schemas to randomly construct a task graph and generates natural language evaluation tasks based on the task graph to evaluate the reasoning and memory abilities of LLMs respectively. Due to the very large randomness of the task construction process, it is possible to ensure that none of the LLMs to be tested has directly learned the evaluation tasks, guaranteeing the fairness of the evaluation method.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已经获得了越来越多的关注。然而，由于它们的能力相当复杂，如何合理评估它们的能力仍然是一个需要解决的任务。我们提议使用RoCar方法，它利用定义的基本模板来随机建立任务图和生成基于任务图的自然语言评估任务，以评估LLMs的推理和记忆能力。由于随机任务建构过程的非常大的Randomness，因此可以保证none of the LLMs to be tested haven't directly learned the evaluation tasks，确保评估方法的公平性。
</details></li>
</ul>
<hr>
<h2 id="UPFL-Unsupervised-Personalized-Federated-Learning-towards-New-Clients"><a href="#UPFL-Unsupervised-Personalized-Federated-Learning-towards-New-Clients" class="headerlink" title="UPFL: Unsupervised Personalized Federated Learning towards New Clients"></a>UPFL: Unsupervised Personalized Federated Learning towards New Clients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15994">http://arxiv.org/abs/2307.15994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiandi Ye, Cen Chen, Yinggui Wang, Xiang Li, Ming Gao</li>
<li>for: addressing the challenge of providing personalized models for new clients in federated learning settings</li>
<li>methods: extends adaptive risk minimization technique to unsupervised personalized federated learning, with two optimization strategies (proxy regularization and early-stopping) and a knowledge distillation loss specifically designed for FedTTA</li>
<li>results: extensive experiments on five datasets against eleven baselines demonstrate the effectiveness of the proposed FedTTA and its variants<details>
<summary>Abstract</summary>
Personalized federated learning has gained significant attention as a promising approach to address the challenge of data heterogeneity. In this paper, we address a relatively unexplored problem in federated learning. When a federated model has been trained and deployed, and an unlabeled new client joins, providing a personalized model for the new client becomes a highly challenging task. To address this challenge, we extend the adaptive risk minimization technique into the unsupervised personalized federated learning setting and propose our method, FedTTA. We further improve FedTTA with two simple yet effective optimization strategies: enhancing the training of the adaptation model with proxy regularization and early-stopping the adaptation through entropy. Moreover, we propose a knowledge distillation loss specifically designed for FedTTA to address the device heterogeneity. Extensive experiments on five datasets against eleven baselines demonstrate the effectiveness of our proposed FedTTA and its variants. The code is available at: https://github.com/anonymous-federated-learning/code.
</details>
<details>
<summary>摘要</summary>
个人化联合学习已经吸引了广泛关注，作为数据不同性的解决方案。在这篇论文中，我们解决了联合学习中较少研究的问题。当一个联合模型已经训练并部署后，新客户加入时，为新客户提供个性化模型是一个非常具有挑战性的任务。为解决这个挑战，我们将适应风险最小化技术推广到无标签联合学习设置中，并提出我们的方法，FedTTA。我们还通过两种简单却有效的优化策略来进一步提高FedTTA：在适应模型训练中添加代理规则，并在适应过程中使用熵来停止。此外，我们还提出了特有的知识传播损失，用于解决设备不同性。我们在五个数据集上对十一个基准进行了广泛的实验，并证明了我们提出的FedTTA和其变种的效果。代码可以在以下地址获取：https://github.com/anonymous-federated-learning/code。
</details></li>
</ul>
<hr>
<h2 id="Ultrasound-Image-Reconstruction-with-Denoising-Diffusion-Restoration-Models"><a href="#Ultrasound-Image-Reconstruction-with-Denoising-Diffusion-Restoration-Models" class="headerlink" title="Ultrasound Image Reconstruction with Denoising Diffusion Restoration Models"></a>Ultrasound Image Reconstruction with Denoising Diffusion Restoration Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15990">http://arxiv.org/abs/2307.15990</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuxin-zhang-jasmine/drus-v1">https://github.com/yuxin-zhang-jasmine/drus-v1</a></li>
<li>paper_authors: Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus</li>
<li>for: 这个论文是为了解决超声影像重建问题，通过学习前知识来提高重建质量。</li>
<li>methods: 这篇论文使用了学习前知识的权重，在Denosing Diffusion Restoration Models（DDRM）框架下实现了超声影像重建。提出了两种 modificates of DDRM，DRUS和WDRUS，并对合成数据和PICMUS数据进行了测试。</li>
<li>results: 该方法可以从单个平面波开始，并且可以达到或更好于DAS和当前最佳方法的图像质量。可以在<a target="_blank" rel="noopener" href="https://github.com/Yuxin-Zhang-Jasmine/DRUS-v1%E4%B8%AD%E4%B8%8B%E8%BD%BD%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/Yuxin-Zhang-Jasmine/DRUS-v1中下载代码。</a><details>
<summary>Abstract</summary>
Ultrasound image reconstruction can be approximately cast as a linear inverse problem that has traditionally been solved with penalized optimization using the $l_1$ or $l_2$ norm, or wavelet-based terms. However, such regularization functions often struggle to balance the sparsity and the smoothness. A promising alternative is using learned priors to make the prior knowledge closer to reality. In this paper, we rely on learned priors under the framework of Denoising Diffusion Restoration Models (DDRM), initially conceived for restoration tasks with natural images. We propose and test two adaptions of DDRM to ultrasound inverse problem models, DRUS and WDRUS. Our experiments on synthetic and PICMUS data show that from a single plane wave our method can achieve image quality comparable to or better than DAS and state-of-the-art methods. The code is available at: https://github.com/Yuxin-Zhang-Jasmine/DRUS-v1.
</details>
<details>
<summary>摘要</summary>
ultrasound图像重建可以 aproximately 看作一个线性 inverse 问题，传统上使用 $l_1$ 或 $l_2$ 范数或浪涌基元的 regularization 函数来解决。但这些 regularization 函数经常坚持不够平衡稀疏性和稳定性。一种有前途的替代方案是使用学习的 prior 来让 prior 更加接近 reality。在这篇论文中，我们利用学习的 prior 在 Denoising Diffusion Restoration Models（DDRM）框架下，DDRM 最初是为静止图像修复任务设计的。我们提出并测试了 two 种适应 DDRM 到 ultrasound inverse problem 模型的变体，DRUS 和 WDRUS。我们的实验表明，从单个扩散波的数据中，我们的方法可以 achieved 图像质量与 DAS 和现有方法相当或更高。代码可以在：https://github.com/Yuxin-Zhang-Jasmine/DRUS-v1 中找到。
</details></li>
</ul>
<hr>
<h2 id="Freespace-Optical-Flow-Modeling-for-Automated-Driving"><a href="#Freespace-Optical-Flow-Modeling-for-Automated-Driving" class="headerlink" title="Freespace Optical Flow Modeling for Automated Driving"></a>Freespace Optical Flow Modeling for Automated Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15989">http://arxiv.org/abs/2307.15989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Feng, Ruge Zhang, Jiayuan Du, Qijun Chen, Rui Fan</li>
<li>for: 这篇论文的目的是为自动驾驶视觉识别提出一个新的方法，具体来说是计算车辆在驾驶环境中的运动流。</li>
<li>methods: 这篇论文使用了一种新的方法，即在三维驾驶环境中利用几何信息来模型光流。这个方法利用了碰撞范围（也称为可动范围或简单地说是“自由空间”）中的几何信息，以便更好地利用环境信息和几何制约。</li>
<li>results: 这篇论文的实验结果显示了新的光流模型的高精度和可靠性。此外，这个模型还有许多应用在自动驾驶领域，例如顶对应探测、车辆位置探测等。实验结果显示了这个模型在不同的公共数据集上的高性能。另外，作者们还提供了一个公开的源代码，让其他研究人员可以免费地使用。<details>
<summary>Abstract</summary>
Optical flow and disparity are two informative visual features for autonomous driving perception. They have been used for a variety of applications, such as obstacle and lane detection. The concept of "U-V-Disparity" has been widely explored in the literature, while its counterpart in optical flow has received relatively little attention. Traditional motion analysis algorithms estimate optical flow by matching correspondences between two successive video frames, which limits the full utilization of environmental information and geometric constraints. Therefore, we propose a novel strategy to model optical flow in the collision-free space (also referred to as drivable area or simply freespace) for intelligent vehicles, with the full utilization of geometry information in a 3D driving environment. We provide explicit representations of optical flow and deduce the quadratic relationship between the optical flow component and the vertical coordinate. Through extensive experiments on several public datasets, we demonstrate the high accuracy and robustness of our model. Additionally, our proposed freespace optical flow model boasts a diverse array of applications within the realm of automated driving, providing a geometric constraint in freespace detection, vehicle localization, and more. We have made our source code publicly available at https://mias.group/FSOF.
</details>
<details>
<summary>摘要</summary>
优化流和差异是自动驾驶视觉感知中的两种有用特征。它们已经用于许多应用程序，如障碍物和车道检测。在文献中，“U-V-差异”概念已经广泛探讨，而其对优化流的匹配相对较少。传统的运动分析算法在两帧视频之间匹配对应点，这限制了环境信息和几何约束的完全利用。因此，我们提出了一种新的策略，在碰撞自由空间（也称为可驾驶空间或简单地 freespace）中模型优化流，充分利用3D驾驶环境中的几何信息。我们提供了优化流的Explicit表示，并证明了优化流组件与垂直坐标之间的 quadratic关系。经过对多个公共数据集的广泛实验，我们示出了我们的模型具有高准确性和稳定性。此外，我们提出的免碰撞自由流模型在自动驾驶领域中拥有多种应用，包括免碰撞自由空间检测、车辆定位和更多。我们的源代码已经公开在https://mias.group/FSOF。
</details></li>
</ul>
<hr>
<h2 id="You-Can-Backdoor-Personalized-Federated-Learning"><a href="#You-Can-Backdoor-Personalized-Federated-Learning" class="headerlink" title="You Can Backdoor Personalized Federated Learning"></a>You Can Backdoor Personalized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15971">http://arxiv.org/abs/2307.15971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiandi Ye, Cen Chen, Yinggui Wang, Xiang Li, Ming Gao<br>for: This paper focuses on backdoor attacks in personalized federated learning (pFL) scenarios, where each client constructs a personalized model based on its local data.methods: The paper proposes three backdoor attack methods: BapFL, BapFL+, and Gen-BapFL, which can effectively attack pFL methods by maintaining clean local parameters while implanting the backdoor into the global parameters, and by introducing Gaussian noise to the local parameters.results: The paper demonstrates the effectiveness of the proposed attack methods against two classic pFL methods with partial model-sharing, FedPer and LG-FedAvg, on four FL benchmark datasets. Additionally, the paper assesses the defense efficacy of various defense strategies against the proposed attacks and finds that Gradient Norm-Clipping is particularly effective.<details>
<summary>Abstract</summary>
Backdoor attacks pose a significant threat to the security of federated learning systems. However, existing research primarily focuses on backdoor attacks and defenses within the generic FL scenario, where all clients collaborate to train a single global model. \citet{qin2023revisiting} conduct the first study of backdoor attacks in the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In this paper, we whistleblow that pFL methods with partial model-sharing are still vulnerable to backdoor attacks in the absence of any defense. We propose three backdoor attack methods: BapFL, BapFL+, and Gen-BapFL, and we empirically demonstrate that they can effectively attack the pFL methods. Specifically, the key principle of BapFL lies in maintaining clean local parameters while implanting the backdoor into the global parameters. BapFL+ generalizes the attack success to benign clients by introducing Gaussian noise to the local parameters. Furthermore, we assume the collaboration of malicious clients and propose Gen-BapFL, which leverages meta-learning techniques to further enhances attack generalization. We evaluate our proposed attack methods against two classic pFL methods with partial model-sharing, FedPer and LG-FedAvg. Extensive experiments on four FL benchmark datasets demonstrate the effectiveness of our proposed attack methods. Additionally, we assess the defense efficacy of various defense strategies against our proposed attacks and find that Gradient Norm-Clipping is particularly effective. It is crucial to note that pFL method is not always secure in the presence of backdoor attacks, and we hope to inspire further research on attack and defense in pFL scenarios.
</details>
<details>
<summary>摘要</summary>
背门攻击对联合学习系统安全性提出了严重的威胁。然而，现有的研究主要集中在背门攻击和防御在通用 Federated Learning（FL）场景中，其中所有客户端协力训练单一的全球模型。 however, 某些客户端协力训练个人化 Federated Learning（pFL）场景，每个客户端都会根据本地数据建立个人化的模型。不ably, 这些研究显示了pFL方法在部分模型分享情况下可以大幅提高防御背门攻击的能力。在这篇文章中，我们宣布pFL方法在部分模型分享情况下仍然受到背门攻击的威胁，在没有任何防御措施的情况下。我们提出了三种背门攻击方法：BapFL、BapFL+和Gen-BapFL，并经过实验显示了它们可以有效地攻击pFL方法。具体来说，BapFL的关键原理是维持清洁的本地参数，同时将背门嵌入到全球参数中。BapFL+扩展了攻击成功到良好的客户端，通过引入 Gaussian 噪声到本地参数中。此外，我们假设了合作的黑客端，并提出了Gen-BapFL，利用了元学习技术以进一步增强攻击扩展。我们对两个类型的pFL方法进行了广泛的实验，评估了我们所提出的攻击方法的效果。我们还评估了不同防御策略对我们所提出的攻击方法的防御效果，发现Gradient Norm-Clipping particularly effective。需要注意的是，pFL方法不一定在背门攻击下安全，我们希望透过这篇文章启发更多的研究背门攻击和防御在pFL场景中。
</details></li>
</ul>
<hr>
<h2 id="Graph-Condensation-for-Inductive-Node-Representation-Learning"><a href="#Graph-Condensation-for-Inductive-Node-Representation-Learning" class="headerlink" title="Graph Condensation for Inductive Node Representation Learning"></a>Graph Condensation for Inductive Node Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15967">http://arxiv.org/abs/2307.15967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Gao, Tong Chen, Yilong Zang, Wentao Zhang, Quoc Viet Hung Nguyen, Kai Zheng, Hongzhi Yin</li>
<li>for: 提高大型图的 Graph Neural Networks (GNNs) 的计算效率，以便在多种应用中使用。</li>
<li>methods: 使用 Graph Condensation 技术，将大型图构建成小型的 sintetic graph，以便训练 GNNs。同时，通过学习一对多节点映射，使新节点可以直接在 sintetic graph 上进行信息传递。</li>
<li>results: 在 Reddit  dataset 上，使用 MCond 方法可以 achieve up to 121.5x 的推理速度增幅和 55.9x 的存储需求减少，比对方法 Based on 原始图更高效。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for inductive representation learning. This enables direct information propagation on the synthetic graph, which is much more efficient than on the original large graph. Specifically, MCond employs an alternating optimization scheme with innovative loss terms from transductive and inductive perspectives, facilitating the mutual promotion between graph condensation and node mapping learning. Extensive experiments demonstrate the efficacy of our approach in inductive inference. On the Reddit dataset, MCond achieves up to 121.5x inference speedup and 55.9x reduction in storage requirements compared with counterparts based on the original graph.
</details>
<details>
<summary>摘要</summary>
GRAPH NEURAL NETWORKS (GNNs) 面临大规模图处理中的 significiant 计算挑战，这限制了它们在多种应用中的效果。为解决这些限制，图简化技术 emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for inductive representation learning. This enables direct information propagation on the synthetic graph, which is much more efficient than on the original large graph. Specifically, MCond employs an alternating optimization scheme with innovative loss terms from transductive and inductive perspectives, facilitating the mutual promotion between graph condensation and node mapping learning. Extensive experiments demonstrate the efficacy of our approach in inductive inference. On the Reddit dataset, MCond achieves up to 121.5x inference speedup and 55.9x reduction in storage requirements compared with counterparts based on the original graph.
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Visualization-of-Aggregated-Class-Activation-Maps-to-Analyse-the-Global-Contribution-of-Class-Features"><a href="#Towards-the-Visualization-of-Aggregated-Class-Activation-Maps-to-Analyse-the-Global-Contribution-of-Class-Features" class="headerlink" title="Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features"></a>Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00710">http://arxiv.org/abs/2308.00710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Igor Cherepanov, David Sessler, Alex Ulmer, Hendrik Lücke-Tieke, Jörn Kohlhammer</li>
<li>for: 这篇论文旨在解释深度学习（DL）模型在分类任务中的决策过程，以便在高风险应用中使用DL模型。</li>
<li>methods: 我们对recent Class Activation Maps（CAMs）方法进行了扩展，以visualize每个数据样本中对分类决策的重要性。我们将多个样本的CAMs进行聚合，以提供一个全局的解释视图，并为每个特征添加了一个方块图示，以显示该特征对分类决策的影响。</li>
<li>results: 我们的视觉表示方法可以帮助分析者了解DL模型在高维数据中做出决策的重要特征，并提供了一种交互式 histogram 来筛选样本和细化 CAM，以便进一步分析 interessing 特征。<details>
<summary>Abstract</summary>
Deep learning (DL) models achieve remarkable performance in classification tasks. However, models with high complexity can not be used in many risk-sensitive applications unless a comprehensible explanation is presented. Explainable artificial intelligence (xAI) focuses on the research to explain the decision-making of AI systems like DL. We extend a recent method of Class Activation Maps (CAMs) which visualizes the importance of each feature of a data sample contributing to the classification. In this paper, we aggregate CAMs from multiple samples to show a global explanation of the classification for semantically structured data. The aggregation allows the analyst to make sophisticated assumptions and analyze them with further drill-down visualizations. Our visual representation for the global CAM illustrates the impact of each feature with a square glyph containing two indicators. The color of the square indicates the classification impact of this feature. The size of the filled square describes the variability of the impact between single samples. For interesting features that require further analysis, a detailed view is necessary that provides the distribution of these values. We propose an interactive histogram to filter samples and refine the CAM to show relevant samples only. Our approach allows an analyst to detect important features of high-dimensional data and derive adjustments to the AI model based on our global explanation visualization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-effect-of-network-topologies-on-fully-decentralized-learning-a-preliminary-investigation"><a href="#The-effect-of-network-topologies-on-fully-decentralized-learning-a-preliminary-investigation" class="headerlink" title="The effect of network topologies on fully decentralized learning: a preliminary investigation"></a>The effect of network topologies on fully decentralized learning: a preliminary investigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15947">http://arxiv.org/abs/2307.15947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luigi Palmieri, Lorenzo Valerio, Chiara Boldrini, Andrea Passarella</li>
<li>for: 这篇论文研究了在分布式机器学习系统中节点之间的网络拓扑如何影响模型的训练和性能。</li>
<li>methods: 作者使用了直接各节点间协作来训练机器学习模型，并研究了不同网络拓扑的影响。</li>
<li>results: 研究发现，即使网络组件之间存在较弱的连接，也可以快速传播信息，但是不足以传播知识。另外，研究还发现，核心节点（hubs）在传播知识方面扮演着更重要的角色，而叶节点（leaves）的作用则取决于分布的重要性。最后，研究还发现，紧密结构的社区会很大程度地阻碍知识的传播。<details>
<summary>Abstract</summary>
In a decentralized machine learning system, data is typically partitioned among multiple devices or nodes, each of which trains a local model using its own data. These local models are then shared and combined to create a global model that can make accurate predictions on new data. In this paper, we start exploring the role of the network topology connecting nodes on the performance of a Machine Learning model trained through direct collaboration between nodes. We investigate how different types of topologies impact the "spreading of knowledge", i.e., the ability of nodes to incorporate in their local model the knowledge derived by learning patterns in data available in other nodes across the networks. Specifically, we highlight the different roles in this process of more or less connected nodes (hubs and leaves), as well as that of macroscopic network properties (primarily, degree distribution and modularity). Among others, we show that, while it is known that even weak connectivity among network components is sufficient for information spread, it may not be sufficient for knowledge spread. More intuitively, we also find that hubs have a more significant role than leaves in spreading knowledge, although this manifests itself not only for heavy-tailed distributions but also when "hubs" have only moderately more connections than leaves. Finally, we show that tightly knit communities severely hinder knowledge spread.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Theory-for-Emergence-of-Complex-Skills-in-Language-Models"><a href="#A-Theory-for-Emergence-of-Complex-Skills-in-Language-Models" class="headerlink" title="A Theory for Emergence of Complex Skills in Language Models"></a>A Theory for Emergence of Complex Skills in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15936">http://arxiv.org/abs/2307.15936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dia2018/What-is-the-Difference-Between-AI-and-Machine-Learning">https://github.com/dia2018/What-is-the-Difference-Between-AI-and-Machine-Learning</a></li>
<li>paper_authors: Sanjeev Arora, Anirudh Goyal</li>
<li>for: 本研究旨在解释语言模型新技能的emergence现象，当参数集和训练数据集scale up时emergence现象的机制尚未得到充分理解。</li>
<li>methods: 本研究使用了著名的Scaling Laws of LLMs和简单的统计分析方法来分析emergence现象。</li>
<li>results: 研究发现，透MDb的损失函数和语言任务的基本技能之间存在 Statistical关系，而Scaling Laws imply strong inductive bias，allowing pre-trained模型在efficiently learn new skills。例如，在$k$-tuple skills任务中，模型可以 Essentially at the same scaling and rate as learning elementary skills themselves。<details>
<summary>Abstract</summary>
A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing tasks involving $k$-tuples of skills emerges essentially at the same scaling and same rate as competence on the elementary skills themselves.
</details>
<details>
<summary>摘要</summary>
现代AI产品的一个主要驱动力是语言模型新的技能的出现，当其参数集和训练 Corpora 的大小增加时。这种现象还不够了解，而且使用梯度基本训练的数学分析还 seems difficult。本文采用了一种不同的方法，通过著名的（empirical）涨大法律和简单的统计框架来分析出现。本文的贡献包括：(a) 一种统计框架，将语言任务下的基本技能的杂合 entropy loss 与语言模型的 competed 关系。(b) 数学分析，显示了涨大法律的强形 inductive bias，使得预训练模型可以非常高效地学习。我们 Informally 称这为“箭头泛化”，因为从直观来看，它看起来会让模型在不同的任务上达到高效的 competed 水平。(c) 一个重要的例子，即在执行包含 $k $-tuple 技能的任务时，语言模型的 competed 水平会出现在基本技能的 competed 水平之上，并且在同样的涨大程度和速度上进行。
</details></li>
</ul>
<hr>
<h2 id="Language-models-as-master-equation-solvers"><a href="#Language-models-as-master-equation-solvers" class="headerlink" title="Language models as master equation solvers"></a>Language models as master equation solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02514">http://arxiv.org/abs/2308.02514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Chuanbo Liu, Jin Wang</li>
<li>for: 解决幂等方程（master equation），即模拟随机动力系统的基本方程。</li>
<li>methods: 使用语言模型（language model）作为机器学习方法，将率参数、初始条件和时间值映射到状态共享分布中，即确切匹配输入上下文。</li>
<li>results: 对多模块和高维系统进行了示例应用，并观察到高准确率和扩展性。通过这种方法，可以使用单个预训练大模型解决任何幂等方程。<details>
<summary>Abstract</summary>
Master equations are of fundamental importance in modeling stochastic dynamical systems.However, solving master equations is challenging due to the exponential increase in the number of possible states or trajectories with the dimension of the state space. In this study, we propose repurposing language models as a machine learning approach to solve master equations. We design a prompt-based neural network to map rate parameters, initial conditions, and time values directly to the state joint probability distribution that exactly matches the input contexts. In this way, we approximate the solution of the master equation in its most general form. We train the network using the policy gradient algorithm within the reinforcement learning framework, with feedback rewards provided by a set of variational autoregressive models. By applying this approach to representative examples, we observe high accuracy for both multi-module and high-dimensional systems. The trained network also exhibits extrapolating ability, extending its predictability to unseen data. Our findings establish the connection between language models and master equations, highlighting the possibility of using a single pretrained large model to solve any master equation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ATESA-BAERT-A-Heterogeneous-Ensemble-Learning-Model-for-Aspect-Based-Sentiment-Analysis"><a href="#ATESA-BAERT-A-Heterogeneous-Ensemble-Learning-Model-for-Aspect-Based-Sentiment-Analysis" class="headerlink" title="ATESA-BÆRT: A Heterogeneous Ensemble Learning Model for Aspect-Based Sentiment Analysis"></a>ATESA-BÆRT: A Heterogeneous Ensemble Learning Model for Aspect-Based Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15920">http://arxiv.org/abs/2307.15920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elena-Simona Apostol, Alin-Georgian Pisică, Ciprian-Octavian Truică</li>
<li>for: 本研究旨在提高在线评论的分析精度，通过确定用户对不同产品和服务的意见。</li>
<li>methods: 本文提出了一种基于矩阵优化的多元搜索模型，可以同时处理多个凝聚因素。</li>
<li>results: 实验结果表明，该模型在两个 datasets 上具有更高的准确率和更好的精度，比现有方法更有优势。<details>
<summary>Abstract</summary>
The increasing volume of online reviews has made possible the development of sentiment analysis models for determining the opinion of customers regarding different products and services. Until now, sentiment analysis has proven to be an effective tool for determining the overall polarity of reviews. To improve the granularity at the aspect level for a better understanding of the service or product, the task of aspect-based sentiment analysis aims to first identify aspects and then determine the user's opinion about them. The complexity of this task lies in the fact that the same review can present multiple aspects, each with its own polarity. Current solutions have poor performance on such data. We address this problem by proposing ATESA-B{\AE}RT, a heterogeneous ensemble learning model for Aspect-Based Sentiment Analysis. Firstly, we divide our problem into two sub-tasks, i.e., Aspect Term Extraction and Aspect Term Sentiment Analysis. Secondly, we use the \textit{argmax} multi-class classification on six transformers-based learners for each sub-task. Initial experiments on two datasets prove that ATESA-B{\AE}RT outperforms current state-of-the-art solutions while solving the many aspects problem.
</details>
<details>
<summary>摘要</summary>
随着在线评论的量的增加，可以开发出情感分析模型，以确定客户对不同产品和服务的看法。到目前为止，情感分析已经证明是一个有效的工具，用于确定评论的总性。为了提高各个方面的细化，以便更好地理解产品或服务，分别针对每个方面进行情感分析是一项有挑战性的任务。这是因为同一篇评论可能会涵盖多个方面，每个方面都有其自己的正面或负面。现有的解决方案在处理这类数据时表现不佳。我们解决这个问题，提出了ATESA-B{\AE}RT，一种多样性ensemble学习模型，用于各个方面的情感分析。首先，我们将问题分为两个子任务：一是方面术语提取，二是方面术语情感分析。其次，我们使用六个基于转换器的学习器进行每个子任务的\textit{argmax}多类分类。初步实验表明，ATESA-B{\AE}RT在两个数据集上的表现优于当前状态的最佳解决方案，并解决了多个方面问题。
</details></li>
</ul>
<hr>
<h2 id="Opportunistic-Air-Quality-Monitoring-and-Forecasting-with-Expandable-Graph-Neural-Networks"><a href="#Opportunistic-Air-Quality-Monitoring-and-Forecasting-with-Expandable-Graph-Neural-Networks" class="headerlink" title="Opportunistic Air Quality Monitoring and Forecasting with Expandable Graph Neural Networks"></a>Opportunistic Air Quality Monitoring and Forecasting with Expandable Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15916">http://arxiv.org/abs/2307.15916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingwei Zuo, Wenbin Li, Michele Baldo, Hakim Hacid</li>
<li>for: 本研究旨在提出一种可扩展的图注意网络模型（EGAT），用于融合不同空间结构的数据采集，以提高空气质量预测的灵活性和准确性。</li>
<li>methods: 本研究使用了一种名为EGAT的图注意网络模型，可以处理不同空间结构的数据采集，并且可以与现有的预测模型结合使用。</li>
<li>results: 研究者通过使用EGAT模型，在实际的空气质量数据集上进行了验证，结果表明EGAT模型可以提高空气质量预测的灵活性和准确性。<details>
<summary>Abstract</summary>
Air Quality Monitoring and Forecasting has been a popular research topic in recent years. Recently, data-driven approaches for air quality forecasting have garnered significant attention, owing to the availability of well-established data collection facilities in urban areas. Fixed infrastructures, typically deployed by national institutes or tech giants, often fall short in meeting the requirements of diverse personalized scenarios, e.g., forecasting in areas without any existing infrastructure. Consequently, smaller institutes or companies with limited budgets are compelled to seek tailored solutions by introducing more flexible infrastructures for data collection. In this paper, we propose an expandable graph attention network (EGAT) model, which digests data collected from existing and newly-added infrastructures, with different spatial structures. Additionally, our proposal can be embedded into any air quality forecasting models, to apply to the scenarios with evolving spatial structures. The proposal is validated over real air quality data from PurpleAir.
</details>
<details>
<summary>摘要</summary>
近年来，空气质量监测和预测已成为科研领域的热点话题。现在，基于数据驱动的空气质量预测方法受到了广泛关注，因为城市地区的数据收集设施已经成熔化了。 fixed 的基础设施，通常由国家机构或科技巨头部署，经常无法满足个性化的情况，例如预测没有任何基础设施的地区。因此，小型机构或公司具有有限预算的情况下，需要寻找更灵活的基础设施来采集数据。在这篇论文中，我们提出了一种可扩展的图注意网络（EGAT）模型，该模型可以处理来自现有和新增的基础设施的数据，并且具有不同的空间结构。此外，我们的提议可以融入任何空气质量预测模型中，以适应不断发展的空间结构。我们的提议被验证通过实际的紫色空气数据。
</details></li>
</ul>
<hr>
<h2 id="Moisesdb-A-dataset-for-source-separation-beyond-4-stems"><a href="#Moisesdb-A-dataset-for-source-separation-beyond-4-stems" class="headerlink" title="Moisesdb: A dataset for source separation beyond 4-stems"></a>Moisesdb: A dataset for source separation beyond 4-stems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15913">http://arxiv.org/abs/2307.15913</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moises-ai/moises-db">https://github.com/moises-ai/moises-db</a></li>
<li>paper_authors: Igor Pereira, Felipe Araújo, Filip Korzeniowski, Richard Vogl</li>
<li>for: 本研究 introduce了 Musical Source Separation 领域的 MoisesDB 数据集，用于驱动和评估精细音源分离系统的发展。</li>
<li>methods: 本研究使用了一个二级层次的 taxonomy 组织音频源，并提供了一个简单易用的 Python 库来下载、处理和使用 MoisesDB。</li>
<li>results: 本研究提供了不同精细度的开源分离模型的基准结果，并对数据集的内容进行了详细的文档和分析。<details>
<summary>Abstract</summary>
In this paper, we introduce the MoisesDB dataset for musical source separation. It consists of 240 tracks from 45 artists, covering twelve musical genres. For each song, we provide its individual audio sources, organized in a two-level hierarchical taxonomy of stems. This will facilitate building and evaluating fine-grained source separation systems that go beyond the limitation of using four stems (drums, bass, other, and vocals) due to lack of data. To facilitate the adoption of this dataset, we publish an easy-to-use Python library to download, process and use MoisesDB. Alongside a thorough documentation and analysis of the dataset contents, this work provides baseline results for open-source separation models for varying separation granularities (four, five, and six stems), and discuss their results.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了Musical Source Separation的MoisesDB数据集。它包含240首歌曲，来自45位艺术家，涵盖了12种音乐类型。每首歌曲都有其自己的声音来源，以两级层次的概念分类为stems。这将有助于建立和评估精细的音乐来源分离系统，超越使用四个声音来源（鼓、贝斯、其他和 vocals）的限制，因为缺乏数据。为了促进这个数据集的采用，我们在Python库中发布了一个易于使用的下载、处理和使用MoisesDB的工具。同时，我们还提供了数据集的详细文档和分析，以及不同的分离精度（四、五、六个声音来源）的基准结果。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-Under-Probabilistic-Spatio-Temporal-Constraints-with-Time-Windows"><a href="#Reinforcement-Learning-Under-Probabilistic-Spatio-Temporal-Constraints-with-Time-Windows" class="headerlink" title="Reinforcement Learning Under Probabilistic Spatio-Temporal Constraints with Time Windows"></a>Reinforcement Learning Under Probabilistic Spatio-Temporal Constraints with Time Windows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15910">http://arxiv.org/abs/2307.15910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoshan Lin, Abbasali Koochakzadeh, Yasin Yazicioglu, Derya Aksaray</li>
<li>for: 本文提出了一种自动机理论方法，用于在复杂的空间时间约束下进行强化学习（RL）。</li>
<li>methods: 本文使用Markov决策过程下的 bounded temporal logic约束来形式化问题，并使用总自动机来翻译这个约束。文章还采用了基于可用的历史过程概率信息的方法来避免”危险”的动作。</li>
<li>results: 本文提供了关于约束满足的概率的理论保证，并提供了一个具有 periodic pick-up和交付任务的enario的数值结果，以证明本方法的有效性。<details>
<summary>Abstract</summary>
We propose an automata-theoretic approach for reinforcement learning (RL) under complex spatio-temporal constraints with time windows. The problem is formulated using a Markov decision process under a bounded temporal logic constraint. Different from existing RL methods that can eventually learn optimal policies satisfying such constraints, our proposed approach enforces a desired probability of constraint satisfaction throughout learning. This is achieved by translating the bounded temporal logic constraint into a total automaton and avoiding "unsafe" actions based on the available prior information regarding the transition probabilities, i.e., a pair of upper and lower bounds for each transition probability. We provide theoretical guarantees on the resulting probability of constraint satisfaction. We also provide numerical results in a scenario where a robot explores the environment to discover high-reward regions while fulfilling some periodic pick-up and delivery tasks that are encoded as temporal logic constraints.
</details>
<details>
<summary>摘要</summary>
我们提出一个自动化理论方法来解决具有复杂时空范围的强化学习（RL）问题。问题是以Markov决策过程形式ulated，并受到紧存时逻规范例的约束。与现有RL方法不同的是，我们的提议方法可以在学习过程中确保满足这些约束的条件，并且可以在学习过程中确保这些约束的满意度。这是通过转换紧存时逻规范例为总自动aton来实现的。我们提供了对结果的概率满意度的理论保证。我们还提供了一个实际应用的数据，该数据显示一个 robot 在环境中探索高奖区域，并且遵循一些periodic pick-up和交付任务，这些任务是通过时间逻规范例表示的。
</details></li>
</ul>
<hr>
<h2 id="UniBriVL-Robust-Universal-Representation-and-Generation-of-Audio-Driven-Diffusion-Models"><a href="#UniBriVL-Robust-Universal-Representation-and-Generation-of-Audio-Driven-Diffusion-Models" class="headerlink" title="UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models"></a>UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15898">http://arxiv.org/abs/2307.15898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sen Fang, Bowen Gao, Yangjian Wu, Jingwen Cai, Teik Toe Teoh</li>
<li>for: 这篇论文的目的是提出一种基于视与语言的语言表示学习方法，以实现多模态应用程序的开发。</li>
<li>methods: 该方法基于bridging-vision-and-language（BriVL），将语音、图像和文本 embedding到共享空间中，解决了语音和图像表示学习中的主要挑战，同时能够有效地捕捉语音和图像之间的相互关系。</li>
<li>results: 实验结果表明，UniBriVL可以在下游任务中达到优异的效果，并且可以根据音频生成相应的图像。这种方法有很多应用前景，如语音识别、音乐信号处理和标题生成等。<details>
<summary>Abstract</summary>
Multimodal large models have been recognized for their advantages in various performance and downstream tasks. The development of these models is crucial towards achieving general artificial intelligence in the future. In this paper, we propose a novel universal language representation learning method called UniBriVL, which is based on Bridging-Vision-and-Language (BriVL). Universal BriVL embeds audio, image, and text into a shared space, enabling the realization of various multimodal applications. Our approach addresses major challenges in robust language (both text and audio) representation learning and effectively captures the correlation between audio and image. Additionally, we demonstrate the qualitative evaluation of the generated images from UniBriVL, which serves to highlight the potential of our approach in creating images from audio. Overall, our experimental results demonstrate the efficacy of UniBriVL in downstream tasks and its ability to choose appropriate images from audio. The proposed approach has the potential for various applications such as speech recognition, music signal processing, and captioning systems.
</details>
<details>
<summary>摘要</summary>
多modal大型模型已被认可其在多种性能和下游任务中的优势。这种模型的发展对于实现未来的通用人工智能是关键。本文提出一种新的通用语言表示学习方法，称为UniBriVL，它基于bridging-vision-and-language（BriVL）。这种 универсаルBriVL嵌入音频、图像和文本到共享空间中，使得实现多种多modal应用程序变得可能。我们的方法解决了语言表示学习中的重要挑战，并有效地捕捉音频和图像之间的相关性。此外，我们还进行了生成图像的质量评估，以展示我们的方法在创建图像从音频中的可能性。总的来说，我们的实验结果表明UniBriVL在下游任务中的效果，并且可以选择适当的图像从音频中。这种方法在语音识别、音乐信号处理和描述系统等应用中具有潜在的潜力。
</details></li>
</ul>
<hr>
<h2 id="A-new-Gradient-TD-Algorithm-with-only-One-Step-size-Convergence-Rate-Analysis-using-L-λ-Smoothness"><a href="#A-new-Gradient-TD-Algorithm-with-only-One-Step-size-Convergence-Rate-Analysis-using-L-λ-Smoothness" class="headerlink" title="A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$λ$ Smoothness"></a>A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$λ$ Smoothness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15892">http://arxiv.org/abs/2307.15892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengshuai Yao</li>
<li>for: 这种论文是关于减少TD更新的维度($d$)的第一个$O(d)$算法，以及这种算法的 konvergence 约束，以及这种约束的证明。</li>
<li>methods: 这种论文使用了GTD算法，以及其两个步长参数。此外，这种论文还使用了一种新的单时间尺度GTD算法，以及一种基于$L$-$\lambda$  гладкость的证明。</li>
<li>results: 这种论文证明了新的GTD算法（即Impression GTD）可以在$O(1&#x2F;t)$的速度下 konvergence，并且可以在更强的假设下提供更快的 konvergence 约束。此外，这种论文还对四种GTD算法的 konvergence 约束进行了证明，并且提供了实验结果，证明Impression GTD在Random walks、Boyan chain和Baird counterexample中 konvergence  faster than其他GTD算法。<details>
<summary>Abstract</summary>
Gradient Temporal Difference (GTD) algorithms (Sutton et al., 2008, 2009) are the first $O(d)$ ($d$ is the number features) algorithms that have convergence guarantees for off-policy learning with linear function approximation. Liu et al. (2015) and Dalal et. al. (2018) proved the convergence rates of GTD, GTD2 and TDC are $O(t^{-\alpha/2})$ for some $\alpha \in (0,1)$. This bound is tight (Dalal et al., 2020), and slower than $O(1/\sqrt{t})$. GTD algorithms also have two step-size parameters, which are difficult to tune. In literature, there is a "single-time-scale" formulation of GTD. However, this formulation still has two step-size parameters.   This paper presents a truly single-time-scale GTD algorithm for minimizing the Norm of Expected td Update (NEU) objective, and it has only one step-size parameter. We prove that the new algorithm, called Impression GTD, converges at least as fast as $O(1/t)$. Furthermore, based on a generalization of the expected smoothness (Gower et al. 2019), called $L$-$\lambda$ smoothness, we are able to prove that the new GTD converges even faster, in fact, with a linear rate. Our rate actually also improves Gower et al.'s result with a tighter bound under a weaker assumption. Besides Impression GTD, we also prove the rates of three other GTD algorithms, one by Yao and Liu (2008), another called A-transpose-TD (Sutton et al., 2008), and a counterpart of A-transpose-TD. The convergence rates of all the four GTD algorithms are proved in a single generic GTD framework to which $L$-$\lambda$ smoothness applies. Empirical results on Random walks, Boyan chain, and Baird counterexample show that Impression GTD converges much faster than existing GTD algorithms for both on-policy and off-policy learning problems, with well-performing step-sizes in a big range.
</details>
<details>
<summary>摘要</summary>
gradient temporal difference（GTD）算法（Sutton et al., 2008, 2009）是首个 $O(d)$ ($d$ 是特征数) 的算法，具有离线学习 linear function approximation 的 convergence guarantee。Liu et al. (2015) 和 Dalal et al. (2018) 证明 GTD、GTD2 和 TDC 的 convergence rate 为 $O(t^{-\alpha/2})$，其中 $\alpha \in (0,1)$。这个 bound 是紧张的（Dalal et al., 2020），并且比 $O(1/\sqrt{t})$ 更慢。GTD 算法还有两个步长参数，这些参数难以调整。在文献中，有一种 "single-time-scale" 的 GTD 表述，但这种表述仍然有两个步长参数。这篇文章提出了一种真正的 single-time-scale GTD 算法，用于最小化 Norm of Expected td Update（NEU）目标函数，并且只有一个步长参数。我们证明该新算法，称为 Impression GTD，在 $O(1/t)$ 的速度下 converges。此外，基于预期的平滑（Gower et al. 2019）的一种推广，称为 $L$-$\lambda$ smoothness，我们能够证明 Impression GTD 的速度实际更快，实际上是 linear 速度。我们的速度实际也超越 Gower et al. 的结果，并且在较弱的假设下提供了更紧张的 bound。此外，我们还证明了三个 GTD 算法的 convergence rate，分别是 Yao and Liu (2008) 的一种算法，Sutton et al. (2008) 的 A-transpose-TD 算法，以及它的对应算法。所有四个 GTD 算法的 convergence rate 在一个通用的 GTD 框架中证明，该框架下 $L$-$\lambda$ smoothness 适用。empirical results 表明，Impression GTD 在Random walks、Boyan chain 和 Baird counterexample 问题中 converge  much faster than existing GTD algorithms，并且步长在大范围内表现良好。
</details></li>
</ul>
<hr>
<h2 id="Point-Annotation-Probability-Map-Towards-Dense-Object-Counting-by-Tolerating-Annotation-Noise"><a href="#Point-Annotation-Probability-Map-Towards-Dense-Object-Counting-by-Tolerating-Annotation-Noise" class="headerlink" title="Point Annotation Probability Map: Towards Dense Object Counting by Tolerating Annotation Noise"></a>Point Annotation Probability Map: Towards Dense Object Counting by Tolerating Annotation Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00530">http://arxiv.org/abs/2308.00530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuehai Chen</li>
<li>for: 这个研究旨在提高计算机视觉中对拥挤场景中物体的数量测量的精度和韧性。</li>
<li>methods: 这个研究使用了一种基于深度学习的方法，即将物体检测任务转化为一个 Gaussian 概率回归问题。然而，这种方法可能不能正确地考虑人工标注过程中的注释噪声，从而导致不同的分布。为了提高 robustness，这个研究使用了一种通用 Gaussian 分布函数（GGD）来形成学习目标点概率图像（PAPM）。</li>
<li>results: 对比于传统的手动设计 PAPM 方法（HD-PAPM）和适应学习 PAPM 方法（AL-PAPM），这个研究的方法在抗注释噪声方面显示出了更高的精度和韧性。此外，通过使用一种基于 GGD 的有效交通成本函数，这个研究还提出了一种可靠的交通框架，从而实现了更好的 PAPM 表现。<details>
<summary>Abstract</summary>
Counting objects in crowded scenes remains a challenge to computer vision. The current deep learning based approach often formulate it as a Gaussian density regression problem. Such a brute-force regression, though effective, may not consider the annotation noise properly which arises from the human annotation process and may lead to different distributions. We conjecture that it would be beneficial to consider the annotation noise in the dense object counting task. To obtain strong robustness against annotation noise, generalized Gaussian distribution (GGD) function with a tunable bandwidth and shape parameter is exploited to form the learning target point annotation probability map, PAPM. Specifically, we first present a hand-designed PAPM method (HD-PAPM), in which we design a function based on GGD to tolerate the annotation noise. For end-to-end training, the hand-designed PAPM may not be optimal for the particular network and dataset. An adaptively learned PAPM method (AL-PAPM) is proposed. To improve the robustness to annotation noise, we design an effective transport cost function based on GGD. With such transport cost constraints, a better PAPM presentation could be adaptively learned with an optimal transport framework from point annotation in an end-to-end manner. Extensive experiments show the superiority of our proposed methods.
</details>
<details>
<summary>摘要</summary>
计算对象在增强的场景中的数量 remains a challenge to computer vision. 现有的深度学习基于方法 often formulate it as a Gaussian density regression problem. 这种粗野的回归，虽然有效，可能不会正确地考虑人工标注过程中的标注噪音。 We conjecture that it would be beneficial to consider the annotation noise in the dense object counting task. To obtain strong robustness against annotation noise, we exploit the generalized Gaussian distribution (GGD) function with a tunable bandwidth and shape parameter to form the learning target point annotation probability map, PAPM. Specifically, we first present a hand-designed PAPM method (HD-PAPM), in which we design a function based on GGD to tolerate the annotation noise. For end-to-end training, the hand-designed PAPM may not be optimal for the particular network and dataset. An adaptively learned PAPM method (AL-PAPM) is proposed. To improve the robustness to annotation noise, we design an effective transport cost function based on GGD. With such transport cost constraints, a better PAPM presentation could be adaptively learned with an optimal transport framework from point annotation in an end-to-end manner. Extensive experiments show the superiority of our proposed methods.
</details></li>
</ul>
<hr>
<h2 id="Recent-neutrino-oscillation-result-with-the-IceCube-experiment"><a href="#Recent-neutrino-oscillation-result-with-the-IceCube-experiment" class="headerlink" title="Recent neutrino oscillation result with the IceCube experiment"></a>Recent neutrino oscillation result with the IceCube experiment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15855">http://arxiv.org/abs/2307.15855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiqi Yu, Jessie Micallef</li>
<li>for: 探测TeV中微子发射的天体物理源</li>
<li>methods: 使用Convolutional Neural Networks重构中微子交互</li>
<li>results: 对大气μ中微子消失的新result和现有全球测量进行比较<details>
<summary>Abstract</summary>
The IceCube South Pole Neutrino Observatory is a Cherenkov detector instrumented in a cubic kilometer of ice at the South Pole. IceCube's primary scientific goal is the detection of TeV neutrino emissions from astrophysical sources. At the lower center of the IceCube array, there is a subdetector called DeepCore, which has a denser configuration that makes it possible to lower the energy threshold of IceCube and observe GeV-scale neutrinos, opening the window to atmospheric neutrino oscillations studies. Advances in physics sensitivity have recently been achieved by employing Convolutional Neural Networks to reconstruct neutrino interactions in the DeepCore detector. In this contribution, the recent IceCube result from the atmospheric muon neutrino disappearance analysis using the CNN-reconstructed neutrino sample is presented and compared to the existing worldwide measurements.
</details>
<details>
<summary>摘要</summary>
南极冰矿 neutrino观测站是一种液气切变仪器，位于南极的冰中一公顷范围内。南极冰矿的主要科学目标是探测astrophysical sources的TeV neutrino发射。南极冰矿的下部中心有一个名为DeepCore的仪器，具有更密集的配置，使得可以降低iceCube的能量阈值，观测GeV级别的 neutrinos，开启大气中 neutrino振荡的研究窗口。最近，employning Convolutional Neural Networks（CNN）重建 neutrino互动的技术进行了进一步的物理敏感度提高。本贡献中将公布ICEube最新的大气μ neutrino消失分析结果，使用CNN重建的neutrino样本，与全球各地的现有测量进行比较。
</details></li>
</ul>
<hr>
<h2 id="Dimensionless-Policies-based-on-the-Buckingham-π-Theorem-Is-it-a-good-way-to-Generalize-Numerical-Results"><a href="#Dimensionless-Policies-based-on-the-Buckingham-π-Theorem-Is-it-a-good-way-to-Generalize-Numerical-Results" class="headerlink" title="Dimensionless Policies based on the Buckingham $π$ Theorem: Is it a good way to Generalize Numerical Results?"></a>Dimensionless Policies based on the Buckingham $π$ Theorem: Is it a good way to Generalize Numerical Results?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15852">http://arxiv.org/abs/2307.15852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandre Girard</li>
<li>for: 这 paper 是为了解决一种动力控制问题，即使用约束限制的倒挂pendulum swing-up问题。</li>
<li>methods: 该 paper 使用数字方法计算优化控制法，并利用约束限制的倒挂pendulum swing-up问题的数据生成优化控制器。</li>
<li>results: 研究发现，通过修改问题表述使用约束限制的倒挂pendulum swing-up问题的数据生成优化控制器可以在相似的系统上 reuse。此外，研究还发现了一种称为” режим”的概念，可以帮助relax约束限制的条件。最后，研究还讨论了将输入和输出缩放到相似系统上的约束限制的问题。<details>
<summary>Abstract</summary>
Yes if the context, the list of variables defining the motion control problem, is dimensionally similar. Here we show that by modifying the problem formulation using dimensionless variables, we can re-use the optimal control law generated numerically for a specific system to a sub-space of dimensionally similar systems. This is demonstrated, with numerically generated optimal controllers, for the classic motion control problem of swinging-up a torque-limited inverted pendulum. We also discuss the concept of regime, a region in the space of context variables, that can help relax the condition on dimensional similarity. Futhermore, we discuss how applying dimensionnal scaling of the input and output of a context-specific policy is equivalent to substituing the new systems parameters in an analytical equation for dimentionnaly similar systems. It remains to be seen if this approach can also help generalizing policies for more complex high-dimensional problems.
</details>
<details>
<summary>摘要</summary>
如果上下文中的变量集合定义的运动控制问题的维度相似，那么我们可以通过修改问题定义使用约束维度相似的系统来重用numerically生成的优化控制器。这种方法在 класси的倾斜挠子问题上实现了，并通过 numerically生成的优化控制器来证明。我们还讨论了“ режим”这个概念，它是上下文变量空间中的一个区域，可以帮助降低维度相似性的条件。此外，我们还讨论了将输入和输出缩放到上下文特定策略中的维度相似系统的方法，这与将新系统参数substitued into an analytical equation for dimensionally similar systems中的方法相同。未知是这种方法还可以扩展到更复杂的高维度问题上。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Algorithm-Portfolio-Evaluation-using-Item-Response-Theory"><a href="#Comprehensive-Algorithm-Portfolio-Evaluation-using-Item-Response-Theory" class="headerlink" title="Comprehensive Algorithm Portfolio Evaluation using Item Response Theory"></a>Comprehensive Algorithm Portfolio Evaluation using Item Response Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15850">http://arxiv.org/abs/2307.15850</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sevvandi/airt-scripts">https://github.com/sevvandi/airt-scripts</a></li>
<li>paper_authors: Sevvandi Kandanaarachchi, Kate Smith-Miles</li>
<li>for: 评估机器学习算法表现，包括评估算法在不同数据集上的一致性和异常性。</li>
<li>methods: 基于改进的Item Response Theory（IRT）模型，使用卷积神经网络对数据集进行分类。</li>
<li>results: 提供了一种简单、可解释的方法来评估机器学习算法竞争力，并且可以同时评估算法在不同数据集上的表现。<details>
<summary>Abstract</summary>
Item Response Theory (IRT) has been proposed within the field of Educational Psychometrics to assess student ability as well as test question difficulty and discrimination power. More recently, IRT has been applied to evaluate machine learning algorithm performance on a single classification dataset, where the student is now an algorithm, and the test question is an observation to be classified by the algorithm. In this paper we present a modified IRT-based framework for evaluating a portfolio of algorithms across a repository of datasets, while simultaneously eliciting a richer suite of characteristics - such as algorithm consistency and anomalousness - that describe important aspects of algorithm performance. These characteristics arise from a novel inversion and reinterpretation of the traditional IRT model without requiring additional dataset feature computations. We test this framework on algorithm portfolios for a wide range of applications, demonstrating the broad applicability of this method as an insightful algorithm evaluation tool. Furthermore, the explainable nature of IRT parameters yield an increased understanding of algorithm portfolios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Primitive-Skill-based-Robot-Learning-from-Human-Evaluative-Feedback"><a href="#Primitive-Skill-based-Robot-Learning-from-Human-Evaluative-Feedback" class="headerlink" title="Primitive Skill-based Robot Learning from Human Evaluative Feedback"></a>Primitive Skill-based Robot Learning from Human Evaluative Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15801">http://arxiv.org/abs/2307.15801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayano Hiranaka, Minjune Hwang, Sharon Lee, Chen Wang, Li Fei-Fei, Jiajun Wu, Ruohan Zhang</li>
<li>for: 提高RL算法在真实环境中执行长期机器人 manipulate 任务的效率和安全性。</li>
<li>methods: 利用RL from human feedback（RLHF）和基本技能基于RL两种方法，可以有效地解决稀缺奖励问题和长期任务的复杂性。</li>
<li>results: 对五种 manipulate 任务进行了广泛的实验，比较了SEED与当前RL算法的性能，得到了显著的提高 sample efficiency 和安全性，同时也比其他RLHF方法具有更少的人工干预。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) algorithms face significant challenges when dealing with long-horizon robot manipulation tasks in real-world environments due to sample inefficiency and safety issues. To overcome these challenges, we propose a novel framework, SEED, which leverages two approaches: reinforcement learning from human feedback (RLHF) and primitive skill-based reinforcement learning. Both approaches are particularly effective in addressing sparse reward issues and the complexities involved in long-horizon tasks. By combining them, SEED reduces the human effort required in RLHF and increases safety in training robot manipulation with RL in real-world settings. Additionally, parameterized skills provide a clear view of the agent's high-level intentions, allowing humans to evaluate skill choices before they are executed. This feature makes the training process even safer and more efficient. To evaluate the performance of SEED, we conducted extensive experiments on five manipulation tasks with varying levels of complexity. Our results show that SEED significantly outperforms state-of-the-art RL algorithms in sample efficiency and safety. In addition, SEED also exhibits a substantial reduction of human effort compared to other RLHF methods. Further details and video results can be found at https://seediros23.github.io/.
</details>
<details>
<summary>摘要</summary>
Reinforcement learning (RL) 算法在实际环境中完成长期机器人操作任务时面临重大挑战，主要是因为样本不充分和安全问题。为了解决这些问题，我们提出了一个新的框架，即 SEED，该框架利用了两种方法：人类反馈学习（RLHF）和基本技能基于学习。这两种方法都能够有效地解决罕见奖励问题和长期任务的复杂性。通过结合这两种方法，SEED可以减少人类努力需要在RLHF中，并在实际训练机器人操作中增加安全性。此外，参数化技能提供了机器人高级意图的明确视图，allowing humans to evaluate skill choices before they are executed。这个特点使得训练过程更加安全和高效。为了评估 SEED 的表现，我们进行了对五种 manipulate 任务的广泛实验。我们的结果表明，SEED 在样本效率和安全性方面明显超过了现有的RL算法。此外，SEED 还表现出了与其他 RLHF 方法相比明显减少的人类努力。更多细节和视频结果可以在 <https://seediros23.github.io/> 找到。
</details></li>
</ul>
<hr>
<h2 id="Summaries-Highlights-and-Action-items-Design-implementation-and-evaluation-of-an-LLM-powered-meeting-recap-system"><a href="#Summaries-Highlights-and-Action-items-Design-implementation-and-evaluation-of-an-LLM-powered-meeting-recap-system" class="headerlink" title="Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system"></a>Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15793">http://arxiv.org/abs/2307.15793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumit Asthana, Sagih Hilleli, Pengcheng He, Aaron Halfaker</li>
<li>for: 这个论文的目的是提高在线计算机媒体空间中的会议体验，使用大语言模型进行会议摘要，以减少个人会议负担和提高会议输出的明确度和一致性。</li>
<li>methods: 该论文使用大语言模型进行会议摘要，并开发了一个基于对话摘要的会议摘要系统。系统包括两种突出的会议摘要表示方式：重要高亮和结构化层次视图。</li>
<li>results: 该论文通过与7名用户进行实验，发现使用大语言模型进行会议摘要可以提高会议体验，但还存在个人重要性和摘要质量等问题。研究结果表明，高质量的会议摘要可以帮助建立共享摘要文档，并且可以通过与用户合作来进一步改进摘要质量和个人重要性。<details>
<summary>Abstract</summary>
Meetings play a critical infrastructural role in the coordination of work. In recent years, due to shift to hybrid and remote work, more meetings are moving to online Computer Mediated Spaces. This has led to new problems (e.g. more time spent in less engaging meetings) and new opportunities (e.g. automated transcription/captioning and recap support). Recent advances in large language models (LLMs) for dialog summarization have the potential to improve the experience of meetings by reducing individuals' meeting load and increasing the clarity and alignment of meeting outputs. Despite this potential, they face technological limitation due to long transcripts and inability to capture diverse recap needs based on user's context. To address these gaps, we design, implement and evaluate in-context a meeting recap system. We first conceptualize two salient recap representations -- important highlights, and a structured, hierarchical minutes view. We develop a system to operationalize the representations with dialogue summarization as its building blocks. Finally, we evaluate the effectiveness of the system with seven users in the context of their work meetings. Our findings show promise in using LLM-based dialogue summarization for meeting recap and the need for both representations in different contexts. However, we find that LLM-based recap still lacks an understanding of whats personally relevant to participants, can miss important details, and mis-attributions can be detrimental to group dynamics. We identify collaboration opportunities such as a shared recap document that a high quality recap enables. We report on implications for designing AI systems to partner with users to learn and improve from natural interactions to overcome the limitations related to personal relevance and summarization quality.
</details>
<details>
<summary>摘要</summary>
To address these gaps, we designed, implemented, and evaluated an in-context meeting recap system. We conceptualized two salient recap representations: important highlights and a structured, hierarchical minutes view. We developed a system to operationalize these representations using dialogue summarization as its building blocks. We evaluated the effectiveness of the system with seven users in the context of their work meetings. Our findings show promise in using LLM-based dialogue summarization for meeting recap, but we also identified limitations, such as a lack of understanding of what is personally relevant to participants, missing important details, and misattributions that can be detrimental to group dynamics.We suggest collaboration opportunities, such as a shared recap document, that a high-quality recap enables. We also identify the need for AI systems to partner with users to learn and improve from natural interactions to overcome the limitations related to personal relevance and summarization quality. Our findings have implications for designing AI systems for meeting support and other applications where summarization and personal relevance are important.
</details></li>
</ul>
<hr>
<h2 id="SAFE-Saliency-Aware-Counterfactual-Explanations-for-DNN-based-Automated-Driving-Systems"><a href="#SAFE-Saliency-Aware-Counterfactual-Explanations-for-DNN-based-Automated-Driving-Systems" class="headerlink" title="SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems"></a>SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15786">http://arxiv.org/abs/2307.15786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Samadi, Amir Shirian, Konstantinos Koufos, Kurt Debattista, Mehrdad Dianati</li>
<li>for: 本文提出了一种新的CF解释方法，用于生成更加有用的CF示例，以便更好地理解黑盒模型的决策过程。</li>
<li>methods: 本文使用了照明地图来生成CF示例，并且通过分析照明地图来确定CF示例的有用性。</li>
<li>results: 实验结果表明，本文提出的CF解释方法可以生成更加有用的CF示例，并且可以帮助理解黑盒模型的决策过程。Translation:</li>
<li>for: This paper proposes a new approach to CF explanations, which generates more informative CF examples to better understand the decision-making process of black-box models.</li>
<li>methods: The proposed method uses saliency maps to generate CF examples and evaluates their usefulness.</li>
<li>results: Experimental results show that the proposed CF explanation method can generate more informative CF examples and help understand the decision-making process of black-box models.<details>
<summary>Abstract</summary>
A CF explainer identifies the minimum modifications in the input that would alter the model's output to its complement. In other words, a CF explainer computes the minimum modifications required to cross the model's decision boundary. Current deep generative CF models often work with user-selected features rather than focusing on the discriminative features of the black-box model. Consequently, such CF examples may not necessarily lie near the decision boundary, thereby contradicting the definition of CFs. To address this issue, we propose in this paper a novel approach that leverages saliency maps to generate more informative CF explanations. Source codes are available at: https://github.com/Amir-Samadi//Saliency_Aware_CF.
</details>
<details>
<summary>摘要</summary>
一种 CF 解释器可以确定输入中最小的修改，使模型的输出变为其 complement。即，CF 解释器计算模型决策边界上需要的最小修改。现有的深度生成 CF 模型通常使用用户选择的特征而不是黑盒模型的激发特征，因此 CF 示例可能不会位于决策边界附近，从而违反 CF 的定义。为解决这个问题，我们在这篇论文中提出了一种新的方法，利用 Saliency 地图生成更有用的 CF 解释。代码可以在：https://github.com/Amir-Samadi//Saliency_Aware_CF 中找到。
</details></li>
</ul>
<hr>
<h2 id="Spherical-and-Hyperbolic-Toric-Topology-Based-Codes-On-Graph-Embedding-for-Ising-MRF-Models-Classical-and-Quantum-Topology-Machine-Learning"><a href="#Spherical-and-Hyperbolic-Toric-Topology-Based-Codes-On-Graph-Embedding-for-Ising-MRF-Models-Classical-and-Quantum-Topology-Machine-Learning" class="headerlink" title="Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning"></a>Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15778">http://arxiv.org/abs/2307.15778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Lcrypto/Topology-Signal-Processing">https://github.com/Lcrypto/Topology-Signal-Processing</a></li>
<li>paper_authors: Vasiliy Usatyuk, Sergey Egorov, Denis Sapozhnikov</li>
<li>for: 这篇论文探讨了应用信息几何来描述铁森模型的稳态状态。</li>
<li>methods: 该方法利用了环境和球体上的多面体代数来使用迪迪诺-莫里雷LLDPC码的自动同构和圆柱体代数来实现这一点。</li>
<li>results: 该研究显示了一种将深度学习架构与误差修正编码相关的新嵌入方法，以及一种使用统计物理和数学几何来优化误差修正编码的方法。这些方法有助于提高深度学习架构的设计、有效硬件设计和物理科学等领域的进步。<details>
<summary>Abstract</summary>
The paper introduces the application of information geometry to describe the ground states of Ising models. This is achieved by utilizing parity-check matrices of cyclic and quasi-cyclic codes on toric and spherical topologies. The approach establishes a connection between machine learning and error-correcting coding, specifically in terms of automorphism and the size of the circulant of the quasi-cyclic code. This proposed approach has implications for the development of new embedding methods based on trapping sets. Statistical physics and number geometry are utilized to optimize error-correcting codes, leading to these embedding and sparse factorization methods. The paper establishes a direct connection between DNN architecture and error-correcting coding by demonstrating how state-of-the-art DNN architectures (ChordMixer, Mega, Mega-chunk, CDIL, ...) from the long-range arena can be equivalent to specific types (Cage-graph, Repeat Accumulate) of block and convolutional LDPC codes. QC codes correspond to certain types of chemical elements, with the carbon element being represented by the mixed automorphism Shu-Lin-Fossorier QC-LDPC code. The Quantum Approximate Optimization Algorithm (QAOA) used in the Sherrington-Kirkpatrick Ising model can be seen as analogous to the back-propagation loss function landscape in training DNNs. This similarity creates a comparable problem with TS pseudo-codeword, resembling the belief propagation method. Additionally, the layer depth in QAOA correlates to the number of decoding belief propagation iterations in the Wiberg decoding tree. Overall, this work has the potential to advance multiple fields, from Information Theory, DNN architecture design (sparse and structured prior graph topology), efficient hardware design for Quantum and Classical DPU/TPU (graph, quantize and shift register architect.) to Materials Science and beyond.
</details>
<details>
<summary>摘要</summary>
文章介绍了使用信息几何来描述碰声模型的稳定态。这是通过利用循环和各种圆柱形码的自带矩阵来实现的，特别是在拓扑和球形上。这种方法可以将机器学习和错误修复编码相连接，并且在自动同构和循环码的大小之间建立关系。这种提议的方法可以用于开发新的嵌入方法，基于拦截集。统计物理和数字几何在错误修复编码中进行优化，导致这些嵌入和稀疏因子化方法。文章还证明了深度学习架构与错误修复编码之间的直接关系，并且显示了状态艺术架构（ChordMixer、Mega、Mega-chunk、CDIL等）与特定类型（团格raph、重复积累）块和 convolutional LDPC 码之间的等价关系。QC 码对应于某些化学元素，而碳元素则被表示为混合自带矩阵 Shu-Lin-Fossorier QC-LDPC 码。Quantum Approximate Optimization Algorithm（QAOA）在希林-基瑞泽曼-碰声模型中可以被看作类似于反射传播损失函数顺序地形态，这种相似性创造了相似的问题，与TS pseudo-codeword相似，类似于信念传播方法。此外，QAOA层数与反射传播循环数在Wiberg解码树中相关。总之，这项工作有可能推动多个领域的进步，从信息理论、深度学习架构设计（稀疏和结构化前 Graph 拓扑）、高效的古驱肾设计（图形、量化和移位注册架构）到材料科学和更远的领域。
</details></li>
</ul>
<hr>
<h2 id="Select-and-Augment-Enhanced-Dense-Retrieval-Knowledge-Graph-Augmentation"><a href="#Select-and-Augment-Enhanced-Dense-Retrieval-Knowledge-Graph-Augmentation" class="headerlink" title="Select and Augment: Enhanced Dense Retrieval Knowledge Graph Augmentation"></a>Select and Augment: Enhanced Dense Retrieval Knowledge Graph Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15776">http://arxiv.org/abs/2307.15776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Micheal Abaho, Yousef H. Alfaifi</li>
<li>for: 提高知识 graphs（KG）中任务的性能，例如链接预测等</li>
<li>methods: 使用多任务框架，选择合适的文本描述来增强KG表示，并将文本描述与KG表示进行对应或增强</li>
<li>results: 在链接预测任务上，与传统CNN方法相比，提高了5.5%和3.5%的 Mean Reciprocal Rank（MRR）和Hits@10分数Note: The above information is in Simplified Chinese text.<details>
<summary>Abstract</summary>
Injecting textual information into knowledge graph (KG) entity representations has been a worthwhile expedition in terms of improving performance in KG oriented tasks within the NLP community. External knowledge often adopted to enhance KG embeddings ranges from semantically rich lexical dependency parsed features to a set of relevant key words to entire text descriptions supplied from an external corpus such as wikipedia and many more. Despite the gains this innovation (Text-enhanced KG embeddings) has made, the proposal in this work suggests that it can be improved even further. Instead of using a single text description (which would not sufficiently represent an entity because of the inherent lexical ambiguity of text), we propose a multi-task framework that jointly selects a set of text descriptions relevant to KG entities as well as align or augment KG embeddings with text descriptions. Different from prior work that plugs formal entity descriptions declared in knowledge bases, this framework leverages a retriever model to selectively identify richer or highly relevant text descriptions to use in augmenting entities. Furthermore, the framework treats the number of descriptions to use in augmentation process as a parameter, which allows the flexibility of enumerating across several numbers before identifying an appropriate number. Experiment results for Link Prediction demonstrate a 5.5% and 3.5% percentage increase in the Mean Reciprocal Rank (MRR) and Hits@10 scores respectively, in comparison to text-enhanced knowledge graph augmentation methods using traditional CNNs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>> injecting textual information into knowledge graph (KG) entity representations has been a worthwhile expedition in terms of improving performance in KG oriented tasks within the NLP community. external knowledge often adopted to enhance KG embeddings ranges from semantically rich lexical dependency parsed features to a set of relevant key words to entire text descriptions supplied from an external corpus such as wikipedia and many more. despite the gains this innovation (text-enhanced KG embeddings) has made, the proposal in this work suggests that it can be improved even further. instead of using a single text description (which would not sufficiently represent an entity because of the inherent lexical ambiguity of text), we propose a multi-task framework that jointly selects a set of text descriptions relevant to KG entities as well as align or augment KG embeddings with text descriptions. different from prior work that plugs formal entity descriptions declared in knowledge bases, this framework leverages a retriever model to selectively identify richer or highly relevant text descriptions to use in augmenting entities. furthermore, the framework treats the number of descriptions to use in augmentation process as a parameter, which allows the flexibility of enumerating across several numbers before identifying an appropriate number. experiment results for link prediction demonstrate a 5.5% and 3.5% percentage increase in the mean reciprocal rank (mrr) and hits@10 scores respectively, in comparison to text-enhanced knowledge graph augmentation methods using traditional cnns.Here's the translation:<<SYS>>translate the following text into Simplified Chinese<</SYS>>通过注入文本信息，知识图（KG）实体表示得到了NLP社区中进行KG oriented任务的改进。外部知识通常采用了具有semantic richness的 lexical dependency parsed feature以及一组相关的关键词来增强KG嵌入。尽管text-enhanced KG embeddings已经带来了一些进步，但是本文提议可以进一步改进。而不是使用单个文本描述（这将不足以表示实体，因为文本的内在ambiguity），我们提议一个多任务框架，它同时选择KG实体相关的文本描述，并将KG嵌入与文本描述进行对齐或扩充。与之前的方法不同，这个框架不使用知识库中声明的正式实体描述，而是利用一个检索模型，选择更加富有或高度相关的文本描述来增强实体。此外，框架对增强过程中的文本描述数量作为参数，允许在增强过程中列举多个数据，以便选择合适的数量。实验结果表明，在链接预测任务中，与传统CNN使用的text-enhanced KG增强方法相比，本方法可以提高MRR和Hits@10分数的平均reciprocal rank和Hits@10分数。
</details></li>
</ul>
<hr>
<h2 id="The-Hydra-Effect-Emergent-Self-repair-in-Language-Model-Computations"><a href="#The-Hydra-Effect-Emergent-Self-repair-in-Language-Model-Computations" class="headerlink" title="The Hydra Effect: Emergent Self-repair in Language Model Computations"></a>The Hydra Effect: Emergent Self-repair in Language Model Computations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15771">http://arxiv.org/abs/2307.15771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, Shane Legg</li>
<li>for: 本研究使用 causal 分析探讨语言模型计算的内部结构。</li>
<li>methods: 本研究使用ablation研究方法探讨语言模型层次结构的相互作用。</li>
<li>results: 研究发现语言模型层次结构具有自适应计算和抵消功能，即“卷舌效应”和“下游MLP层的减退功能”。这些效应在不含dropout的语言模型中也存在。<details>
<summary>Abstract</summary>
We investigate the internal structure of language model computations using causal analysis and demonstrate two motifs: (1) a form of adaptive computation where ablations of one attention layer of a language model cause another layer to compensate (which we term the Hydra effect) and (2) a counterbalancing function of late MLP layers that act to downregulate the maximum-likelihood token. Our ablation studies demonstrate that language model layers are typically relatively loosely coupled (ablations to one layer only affect a small number of downstream layers). Surprisingly, these effects occur even in language models trained without any form of dropout. We analyse these effects in the context of factual recall and consider their implications for circuit-level attribution in language models.
</details>
<details>
<summary>摘要</summary>
我团队 Investigating 语言模型计算机制的内部结构，使用 causal 分析发现了两种模式：（1）一种适应 computation 的形式，其中剪除一层注意力层会导致另一层补偿（我们称之为“卷积效应”），以及（2）一种补偿函数，它使得晚期 MLP 层下降抑制最大可能性token。我们的ablation 研究表明，语言模型层通常是相对松散耦合的（剪除一层只会影响少量下游层）。奇怪的是，这些效果在没有任何 dropout 训练的情况下仍然出现。我们对这些效果在事实记忆中进行分析，并考虑它们对语言模型的征义归属的影响。
</details></li>
</ul>
<hr>
<h2 id="CHATREPORT-Democratizing-Sustainability-Disclosure-Analysis-through-LLM-based-Tools"><a href="#CHATREPORT-Democratizing-Sustainability-Disclosure-Analysis-through-LLM-based-Tools" class="headerlink" title="CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools"></a>CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15770">http://arxiv.org/abs/2307.15770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edisonni-hku/chatreport">https://github.com/edisonni-hku/chatreport</a></li>
<li>paper_authors: Jingwei Ni, Julia Bingler, Chiara Colesanti-Senni, Mathias Kraus, Glen Gostlow, Tobias Schimanski, Dominik Stammbach, Saeid Ashraf Vaghefi, Qian Wang, Nicolas Webersinke, Tobias Wekhof, Tingyu Yu, Markus Leippold</li>
<li>for: The paper aims to provide a novel LLM-based system for automating the analysis of corporate sustainability reports, with the goal of improving transparency and stakeholder empowerment.</li>
<li>methods: The system, called ChatReport, uses large language models (LLMs) to analyze sustainability reports and generate analyses, while addressing two key challenges: hallucination and the inefficiency of involving domain experts in the development loop.</li>
<li>results: The authors provide a methodology, annotated datasets, and generated analyses of 1015 reports to demonstrate the effectiveness of ChatReport. The results show that the system can provide accurate and traceable analyses of sustainability reports, empowering stakeholders and improving transparency in sustainability reporting.<details>
<summary>Abstract</summary>
In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) actively involving domain experts in the development loop. We make our methodology, annotated datasets, and generated analyses of 1015 reports publicly available.
</details>
<details>
<summary>摘要</summary>
在气候变化的面前，公司是否实际做出了更加可持续的操作？回答需要探索企业可持续报告的含义，但这些报告的量和复杂性使人工分析成本高昂。因此，只有少数世界各国的机构有能力大规模分析这些报告，这导致可持续报告的透明度不足。为了解决这个问题，我们提出了一种基于自然语言处理（LLM）的自动分析系统——ChatReport。我们的系统可以帮助投资者、消费者和其他关注可持续发展的各种利益相关者更好地理解企业的可持续发展情况。我们的系统可以解决现有挑战，包括：1. LLM的幻觉：由于LLM的幻觉问题，自动分析系统可能会产生错误的结论。我们的系统可以使答案traceable，以减少幻觉的影响。2. 域专家的参与：在开发LLM模型时，域专家的参与是关键。我们的系统可以 актив地吸引域专家参与开发过程，以提高模型的准确性和可靠性。我们的方法、标注数据集和对1015份报告的自动分析结果都公开 disponibles。通过我们的系统，您可以快速和高效地获得可持续发展的信息，以帮助您做出更加 Informed 的决策。
</details></li>
</ul>
<hr>
<h2 id="Goodness-of-Fit-of-Attributed-Probabilistic-Graph-Generative-Models"><a href="#Goodness-of-Fit-of-Attributed-Probabilistic-Graph-Generative-Models" class="headerlink" title="Goodness-of-Fit of Attributed Probabilistic Graph Generative Models"></a>Goodness-of-Fit of Attributed Probabilistic Graph Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03773">http://arxiv.org/abs/2308.03773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Robles-Granda, Katherine Tsai, Oluwasanmi Koyejo</li>
<li>for: 这篇论文主要用于描述如何评估 probabilistic generative models of graphs 的goodness of fit。</li>
<li>methods: 该论文使用了 mean square contingency coefficient 作为评估标准，并提供了一种方法来确保 strutture of learned attributed graph 的质量。</li>
<li>results: 该论文通过应用这种方法来评估了不同种类的图模型的表示能力。<details>
<summary>Abstract</summary>
Probabilistic generative models of graphs are important tools that enable representation and sampling. Many recent works have created probabilistic models of graphs that are capable of representing not only entity interactions but also their attributes. However, given a generative model of random attributed graph(s), the general conditions that establish goodness of fit are not clear a-priori. In this paper, we define goodness of fit in terms of the mean square contingency coefficient for random binary networks. For this statistic, we outline a procedure for assessing the quality of the structure of a learned attributed graph by ensuring that the discrepancy of the mean square contingency coefficient (constant, or random) is minimal with high probability. We apply these criteria to verify the representation capability of a probabilistic generative model for various popular types of graph models.
</details>
<details>
<summary>摘要</summary>
probabilistic生成模型可以用来表示和采样图像。近期的许多研究都创建了可以表示实体互动以及其属性的概率模型。但是，给定一个生成模型，确定好的适应性是不明确的。在这篇论文中，我们定义适应性是指随机二元网络的mean square contingency coefficient的平均值。我们还详细介绍了一种方法，以确保学习的嵌入图像的结构质量高，这种方法是通过确保mean square contingency coefficient的差异（常数或随机）是最小的来实现。我们应用这些标准来验证不同种类的图像模型的表示能力。
</details></li>
</ul>
<hr>
<h2 id="Lessons-in-Reproducibility-Insights-from-NLP-Studies-in-Materials-Science"><a href="#Lessons-in-Reproducibility-Insights-from-NLP-Studies-in-Materials-Science" class="headerlink" title="Lessons in Reproducibility: Insights from NLP Studies in Materials Science"></a>Lessons in Reproducibility: Insights from NLP Studies in Materials Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15759">http://arxiv.org/abs/2307.15759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyun Lei, Edward Kim, Viktoriia Baibakova, Shijing Sun</li>
<li>for: 本研究对两篇开创性论文进行了可重复性分析，即 “机器学习和编码 synthesis parameters of oxide materials” 和 “自主学习 word embeddings capture latent knowledge from materials science literature”。</li>
<li>methods: 这两篇论文都提供了完整的工作流程，整洁的代码库，以及丰富的评估指南。这使得复制他们的结果更加容易，并部分地复制他们的发现。</li>
<li>results: 我们的分析表明，这两篇论文设置了可贵的标准，使得未来的材料科学发表物可以借鉴。然而，我们还发现了一些需要改进的地方，如提供可 копи得训练数据，更加透明的模型架构和训练过程，以及软件依赖关系版本的详细说明。此外，我们还比较了这两篇论文中的 word embedding 模型，发现了一些关键的可重复性和交叉兼容性问题，这些问题与模型本身外部的设计选择有关。<details>
<summary>Abstract</summary>
Natural Language Processing (NLP), a cornerstone field within artificial intelligence, has been increasingly utilized in the field of materials science literature. Our study conducts a reproducibility analysis of two pioneering works within this domain: "Machine-learned and codified synthesis parameters of oxide materials" by Kim et al., and "Unsupervised word embeddings capture latent knowledge from materials science literature" by Tshitoyan et al. We aim to comprehend these studies from a reproducibility perspective, acknowledging their significant influence on the field of materials informatics, rather than critiquing them. Our study indicates that both papers offered thorough workflows, tidy and well-documented codebases, and clear guidance for model evaluation. This makes it easier to replicate their results successfully and partially reproduce their findings. In doing so, they set commendable standards for future materials science publications to aspire to. However, our analysis also highlights areas for improvement such as to provide access to training data where copyright restrictions permit, more transparency on model architecture and the training process, and specifications of software dependency versions. We also cross-compare the word embedding models between papers, and find that some key differences in reproducibility and cross-compatibility are attributable to design choices outside the bounds of the models themselves. In summary, our study appreciates the benchmark set by these seminal papers while advocating for further enhancements in research reproducibility practices in the field of NLP for materials science. This balance of understanding and continuous improvement will ultimately propel the intersecting domains of NLP and materials science literature into a future of exciting discoveries.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）是人工智能的一个核心领域，在材料科学文献中越来越广泛应用。我们的研究对两篇先锋性论文进行了可重现性分析：“机器学习和编码的材料合成参数” by Kim et al., 和“自然语言模型捕捉材料科学文献中隐知知识” by Tshitoyan et al。我们的研究目的是理解这两篇论文的可重现性，而不是批评它们。我们发现这两篇论文都提供了完整的工作流程、整洁的代码基础和详细的模型评估指南。这使得复制其结果成功并部分复制其发现更加容易。这两篇论文在设置标准的同时，也释放了一些可以进一步改进的提示，例如提供版权限制允许的训练数据访问，更加透明的模型架构和训练过程，以及软件依赖版本的详细说明。我们还将这两篇论文中的词嵌入模型进行比较，发现它们在可重现性和交互兼容性方面有一些关键的差异，这些差异可以归因于模型设计的选择。总之，我们的研究对这两篇论文进行了评价，同时强调了在NLP和材料科学文献领域的研究可重现性实践的进一步提高。这种平衡的理解和不断改进将最终推动这两个领域的发现。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-in-Natural-Language-Generation-From-Theory-to-Applications"><a href="#Uncertainty-in-Natural-Language-Generation-From-Theory-to-Applications" class="headerlink" title="Uncertainty in Natural Language Generation: From Theory to Applications"></a>Uncertainty in Natural Language Generation: From Theory to Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15703">http://arxiv.org/abs/2307.15703</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Rastaman4e/-1">https://github.com/Rastaman4e/-1</a></li>
<li>paper_authors: Joris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li, Raquel Fernández, Barbara Plank, Rico Sennrich, Chrysoula Zerva, Wilker Aziz</li>
<li>for: 这篇论文旨在探讨如何使自然语言生成（NLG）系统更加可靠和可信，以满足不同人群的需求。</li>
<li>methods: 论文提出了一种基于不确定性理论的NLG系统设计方法，包括表示不确定性的基本概念、框架和 vocabulary，以及从语言角度描述NLG中的主要不确定性来源。</li>
<li>results: 论文认为，在NLG系统中处理不确定性可以帮助创建更加适应人群需求的系统和评估协议，并提出了一些有前途的研究方向，如通过不确定性来强化解码、可控生成、自我评估、选择回答、活动学习等。<details>
<summary>Abstract</summary>
Recent advances of powerful Language Models have allowed Natural Language Generation (NLG) to emerge as an important technology that can not only perform traditional tasks like summarisation or translation, but also serve as a natural language interface to a variety of applications. As such, it is crucial that NLG systems are trustworthy and reliable, for example by indicating when they are likely to be wrong; and supporting multiple views, backgrounds and writing styles -- reflecting diverse human sub-populations. In this paper, we argue that a principled treatment of uncertainty can assist in creating systems and evaluation protocols better aligned with these goals. We first present the fundamental theory, frameworks and vocabulary required to represent uncertainty. We then characterise the main sources of uncertainty in NLG from a linguistic perspective, and propose a two-dimensional taxonomy that is more informative and faithful than the popular aleatoric/epistemic dichotomy. Finally, we move from theory to applications and highlight exciting research directions that exploit uncertainty to power decoding, controllable generation, self-assessment, selective answering, active learning and more.
</details>
<details>
<summary>摘要</summary>
First, we present the fundamental theory, frameworks, and vocabulary required to represent uncertainty. We then characterize the main sources of uncertainty in NLG from a linguistic perspective, and propose a two-dimensional taxonomy that is more informative and faithful than the popular aleatoric/epistemic dichotomy. Finally, we move from theory to applications and highlight exciting research directions that exploit uncertainty to power decoding, controllable generation, self-assessment, selective answering, active learning, and more.Translated into Simplified Chinese:最近的强大语言模型的进步使得自然语言生成（NLG）成为一种重要的技术，不仅可以完成传统任务如摘要或翻译，还可以作为多种应用程序的自然语言 интер法。因此，NLG 系统的可靠性和可预测性是非常重要的，例如指示它们可能会错误，并支持多个视点、背景和写作风格，反映人类亚群体。在这篇文章中，我们 argue That a principled treatment of uncertainty can assist in creating systems and evaluation protocols better aligned with these goals.我们首先提出了需要表示不确定性的基本理论、框架和术语。然后，我们从语言学 perspective Characterize the main sources of uncertainty in NLG, and propose a two-dimensional taxonomy that is more informative and faithful than the popular aleatoric/epistemic dichotomy. Finally, we move from theory to applications and highlight exciting research directions that exploit uncertainty to power decoding, controllable generation, self-assessment, selective answering, active learning, and more.Translated into Traditional Chinese:最近的强大语言模型的进步使得自然语言生成（NLG）成为一种重要的技术，不仅可以完成传统任务如摘要或翻译，还可以作为多种应用程序的自然语言 інтер法。因此，NLG 系统的可靠性和可预测性是非常重要的，例如指示它们可能会错误，并支持多个视点、背景和写作风格，反映人类亚群体。在这篇文章中，我们 argue That a principled treatment of uncertainty can assist in creating systems and evaluation protocols better aligned with these goals.我们首先提出了需要表示不确定性的基本理论、框架和术语。然后，我们从语言学 perspective Characterize the main sources of uncertainty in NLG, and propose a two-dimensional taxonomy that is more informative and faithful than the popular aleatoric/epistemic dichotomy. Finally, we move from theory to applications and highlight exciting research directions that exploit uncertainty to power decoding, controllable generation, self-assessment, selective answering, active learning, and more.
</details></li>
</ul>
<hr>
<h2 id="AI-for-Anticipatory-Action-Moving-Beyond-Climate-Forecasting"><a href="#AI-for-Anticipatory-Action-Moving-Beyond-Climate-Forecasting" class="headerlink" title="AI for Anticipatory Action: Moving Beyond Climate Forecasting"></a>AI for Anticipatory Action: Moving Beyond Climate Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15727">http://arxiv.org/abs/2307.15727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Q. Huynh, Mathew V. Kiang</li>
<li>for: 这篇论文旨在提供关于气候预测转移到预测行动的概述，并评估机器学习模型在气候预测中的应用。</li>
<li>methods: 论文评论了机器学习模型在气候预测中的应用，并发现了一些难题，例如如何使机器学习模型更好地支持预测行动。</li>
<li>results: 论文高亮了机器学习模型在气候预测中的应用可以帮助减轻气候变化对最容易受到影响的人群的影响，但还需要更多的研究来解决方法上的难题。<details>
<summary>Abstract</summary>
Disaster response agencies have been shifting from a paradigm of climate forecasting towards one of anticipatory action: assessing not just what the climate will be, but how it will impact specific populations, thereby enabling proactive response and resource allocation. Machine learning models are becoming exceptionally powerful at climate forecasting, but methodological gaps remain in terms of facilitating anticipatory action. Here we provide an overview of anticipatory action, review relevant applications of machine learning, identify common challenges, and highlight areas where machine learning can uniquely contribute to advancing disaster response for populations most vulnerable to climate change.
</details>
<details>
<summary>摘要</summary>
气候灾害机构正在从气候预测模式向一种预期行动模式转移：不仅评估气候会如何发展，而且评估气候对特定人口的影响，以便进行积极的应对和资源分配。机器学习模型在气候预测方面已经非常强大，但在实施预期行动方面还存在一些方法学挑战。本文提供了预期行动的概述，浏览了相关的机器学习应用，确认了常见的挑战，并强调了机器学习在气候变化影响最容易受到影响的人口群体中的独特贡献。
</details></li>
</ul>
<hr>
<h2 id="A-supervised-hybrid-quantum-machine-learning-solution-to-the-emergency-escape-routing-problem"><a href="#A-supervised-hybrid-quantum-machine-learning-solution-to-the-emergency-escape-routing-problem" class="headerlink" title="A supervised hybrid quantum machine learning solution to the emergency escape routing problem"></a>A supervised hybrid quantum machine learning solution to the emergency escape routing problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15682">http://arxiv.org/abs/2307.15682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Haboury, Mo Kordzanganeh, Sebastian Schmitt, Ayush Joshi, Igor Tokarev, Lukas Abdallah, Andrii Kurkin, Basil Kyriacou, Alexey Melnikov</li>
<li>for: 这篇研究探讨了如何使用监督式混合量子机器学习来优化自然灾害发生时的紧急撤退计划。</li>
<li>methods: 研究使用了一种新的混合式监督学习方法，融合了量子和传统的FiLM ней罗网络，并在一个实验城市graph上进行训练。</li>
<li>results: 研究发现，将量子和传统的FiLM ней罗网络融合在一起可以提高整体模型的表达力，并在训练 dataset上预测 Navigation 任务的成功率提高7%。<details>
<summary>Abstract</summary>
Managing the response to natural disasters effectively can considerably mitigate their devastating impact. This work explores the potential of using supervised hybrid quantum machine learning to optimize emergency evacuation plans for cars during natural disasters. The study focuses on earthquake emergencies and models the problem as a dynamic computational graph where an earthquake damages an area of a city. The residents seek to evacuate the city by reaching the exit points where traffic congestion occurs. The situation is modeled as a shortest-path problem on an uncertain and dynamically evolving map. We propose a novel hybrid supervised learning approach and test it on hypothetical situations on a concrete city graph. This approach uses a novel quantum feature-wise linear modulation (FiLM) neural network parallel to a classical FiLM network to imitate Dijkstra's node-wise shortest path algorithm on a deterministic dynamic graph. Adding the quantum neural network in parallel increases the overall model's expressivity by splitting the dataset's harmonic and non-harmonic features between the quantum and classical components. The hybrid supervised learning agent is trained on a dataset of Dijkstra's shortest paths and can successfully learn the navigation task. The hybrid quantum network improves over the purely classical supervised learning approach by 7% in accuracy. We show that the quantum part has a significant contribution of 45.(3)% to the prediction and that the network could be executed on an ion-based quantum computer. The results demonstrate the potential of supervised hybrid quantum machine learning in improving emergency evacuation planning during natural disasters.
</details>
<details>
<summary>摘要</summary>
natural disasters 的回应可以减轻其破坏性的影响。这项工作探讨使用监督式量子机器学习优化自然灾害期间撤离车辆的紧急计划。研究将地震灾害作为研究对象，将问题模型为一个动态计算图，地震破坏城市区域，居民寻求离开城市达到出口点，解决方案是一个短路问题在不确定和动态发展的地图上。我们提出了一种新的半监督学习方法，并在假设情况下测试其在具有具体城市图的情况下。这种方法使用一种新的量子特征WISE linear modulation（FiLM）神经网络并行地与一个经典FiLM神经网络相似，用于模拟在决定性动态图上Dijkstra的节点短路算法。在加入量子神经网络后，总模型的表达能力得到提高，因为将数据集的幂律和非幂律特征分别传递给量子和经典组件。半监督学习代理人通过一个包含Dijkstra短路的数据集进行训练，并成功学习导航任务。半量子网络在纯经典监督学习方法的基础上提高了准确率，提高了7%。我们发现量子部分对预测做出了重要贡献，占总预测概率的45.(3)%。我们还证明了这种网络可以在离子基础上执行量子计算机。结果表明，半量子机器学习在自然灾害期间emergency evacuation planning中具有潜在的优势。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Anomaly-Detection-System-on-various-Jetson-Edge-Devices"><a href="#Benchmarking-Anomaly-Detection-System-on-various-Jetson-Edge-Devices" class="headerlink" title="Benchmarking Anomaly Detection System on various Jetson Edge Devices"></a>Benchmarking Anomaly Detection System on various Jetson Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16834">http://arxiv.org/abs/2307.16834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoang Viet Pham, Thinh Gia Tran, Chuong Dinh Le, An Dinh Le, Hien Bich Vo</li>
<li>for: 增强市民安全和福祉的监控视频异常事件捕捉</li>
<li>methods: 应用EdgeAI技术，实现端到端犯罪现场异常检测系统</li>
<li>results: 比其他状态艺术算法竞争的异常检测模型，并在多个Jetson边缘设备上测试并部署AI系统，并提供了使用Docker技术进行系统性能改进的经验。<details>
<summary>Abstract</summary>
Capturing the abnormal event from surveillance videos enhances the safety and well-being of the citizens. The application of EdgeAI (Edge computing-based Artificial Intelligent ) meets the strict latency requirements for security. In this paper, we apply weakly supervised video anomaly detection called Robust Temporal Feature Magnitude Learning (RTFM) to an end-to-end crime-scene anomaly detection system from the surveillance cameras with the help of edge computing technology. The system is tested directly on multiple Jetson edge devices combined with TensorRT as the software developer kit from NVIDIA for system performance enhancement. The experience of an AI-based system deployment on various Jetson Edge devices with Docker technology is also provided. The anomaly detection model yields competitive results compared to other state-of-the-art (SOTA) algorithms on available datasets such as UCF-Crime and UIT VNAnomaly. The approach system reaches 47.56 frames per second (FPS) inference speed on a Jetson edge device with only 3.11 GB RAM usage total. We also discover the promising Jetson device that the AI system achieves 15% better performance than the previous version of Jetson devices while consuming 50% less energy power.
</details>
<details>
<summary>摘要</summary>
capturing the abnormal event from surveillance videos enhances the safety and well-being of the citizens. The application of EdgeAI (Edge computing-based Artificial Intelligence) meets the strict latency requirements for security. In this paper, we apply weakly supervised video anomaly detection called Robust Temporal Feature Magnitude Learning (RTFM) to an end-to-end crime-scene anomaly detection system from the surveillance cameras with the help of edge computing technology. The system is tested directly on multiple Jetson edge devices combined with TensorRT as the software developer kit from NVIDIA for system performance enhancement. The experience of an AI-based system deployment on various Jetson Edge devices with Docker technology is also provided. The anomaly detection model yields competitive results compared to other state-of-the-art (SOTA) algorithms on available datasets such as UCF-Crime and UIT VNAnomaly. The approach system reaches 47.56 frames per second (FPS) inference speed on a Jetson edge device with only 3.11 GB RAM usage total. We also discover the promising Jetson device that the AI system achieves 15% better performance than the previous version of Jetson devices while consuming 50% less energy power.
</details></li>
</ul>
<hr>
<h2 id="Case-Studies-of-Causal-Discovery-from-IT-Monitoring-Time-Series"><a href="#Case-Studies-of-Causal-Discovery-from-IT-Monitoring-Time-Series" class="headerlink" title="Case Studies of Causal Discovery from IT Monitoring Time Series"></a>Case Studies of Causal Discovery from IT Monitoring Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15678">http://arxiv.org/abs/2307.15678</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Ali Aït-Bachir, Charles K. Assaad, Christophe de Bignicourt, Emilie Devijver, Simon Ferreira, Eric Gaussier, Hosein Mohanna, Lei Zan</li>
<li>for: 本研究旨在应用 causal discovery 算法于 IT 监控数据，以获得系统的 causal 关系。</li>
<li>methods: 本研究使用了不同的 causal discovery 算法，包括 PC 算法、 FCI 算法和 Causal Additive Model (CAM) 算法，并对不同的 IT 监控数据进行了实验。</li>
<li>results: 研究发现，这些 causal discovery 算法可以帮助获得系统的 causal 关系，但是也存在一些挑战，例如时间序列不一致、睡眠时间序列、时间戳不准确和欠数据等。<details>
<summary>Abstract</summary>
Information technology (IT) systems are vital for modern businesses, handling data storage, communication, and process automation. Monitoring these systems is crucial for their proper functioning and efficiency, as it allows collecting extensive observational time series data for analysis. The interest in causal discovery is growing in IT monitoring systems as knowing causal relations between different components of the IT system helps in reducing downtime, enhancing system performance and identifying root causes of anomalies and incidents. It also allows proactive prediction of future issues through historical data analysis. Despite its potential benefits, applying causal discovery algorithms on IT monitoring data poses challenges, due to the complexity of the data. For instance, IT monitoring data often contains misaligned time series, sleeping time series, timestamp errors and missing values. This paper presents case studies on applying causal discovery algorithms to different IT monitoring datasets, highlighting benefits and ongoing challenges.
</details>
<details>
<summary>摘要</summary>
信息技术（IT）系统是现代企业中不可或缺的，它们负责数据存储、通信和自动化过程。监控这些系统非常重要，因为它可以收集广泛的观察时间序数据，用于分析。随着 causal discovery 的兴趣在 IT 监控系统中增长，因为它可以帮助发现 IT 系统中不同组件之间的 causal 关系，从而降低停机时间，提高系统性能和识别异常和事件的根本原因。此外，它还允许预测未来问题的预测，通过历史数据分析。尽管它拥有这些优点，但是在应用 causal discovery 算法于 IT 监控数据时，还存在一些挑战，例如 IT 监控数据中的时间序列误差、休眠时间序列、时间戳错误和缺失值。这篇文章介绍了不同 IT 监控数据集的 case study，探讨了这些挑战和 beneficial 效果。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Data-Generation-in-Vision-and-Language-Navigation"><a href="#Scaling-Data-Generation-in-Vision-and-Language-Navigation" class="headerlink" title="Scaling Data Generation in Vision-and-Language Navigation"></a>Scaling Data Generation in Vision-and-Language Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15644">http://arxiv.org/abs/2307.15644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wz0919/scalevln">https://github.com/wz0919/scalevln</a></li>
<li>paper_authors: Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, Yu Qiao</li>
<li>for: 提高普通环境下的语言导航agent性能</li>
<li>methods: 使用HM3D和Gibson dataset中的1200+真实照片环境和网络资源进行数据生成，并使用这些数据进行预训练和微调</li>
<li>results: 使用这些数据可以提高现有agent的性能 (+11%相对于之前的SoTA)，并将未看过环境下的游走成功率降低到&lt;1%（相比之前的8%）。此外，这种方法还使得不同的模型在CVDN、REVERIE和R2R中实现了新的状态计算导航结果。<details>
<summary>Abstract</summary>
Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervision for training generalizable agents. To tackle the common data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which applies 1200+ photo-realistic environments from HM3D and Gibson datasets and synthesizes 4.9 million instruction trajectory pairs using fully-accessible resources on the web. Importantly, we investigate the influence of each component in this paradigm on the agent's performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The long-lasting generalization gap between navigating in seen and unseen environments is also reduced to less than 1% (versus 8% in the previous best method). Moreover, our paradigm also facilitates different models to achieve new state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous environments.
</details>
<details>
<summary>摘要</summary>
近期研究 языковой导向视觉导航已经表明了广泛环境多样性和训练总体代理人的强需求。为了解决现有视觉语言导航数据集中的常见数据缺乏问题，我们提出了一种有效的数据生成模型，该模型应用了1200+的真实照片环境从HM3D和Gibson数据集，并使用了全面访问网络资源生成490万条 instruciton trajectory对。我们进一步研究这种模型中每个组件对代理人性能的影响，并研究如何正确地应用扩展数据来预训练和精度调整代理人。感谢我们的大规模数据集，已有代理人的性能可以被提高 (+11%绝对与之前的SoTA) 到一个新的最佳值80%单次成功率在R2R测试分割。此外，我们的模型也使得不同的模型在CVDN、REVERIE和R2R在连续环境中实现新的导航成绩。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/29/cs.AI_2023_07_29/" data-id="clpahu6xp001h3h885a015ajj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/29/cs.CL_2023_07_29/" class="article-date">
  <time datetime="2023-07-29T11:00:00.000Z" itemprop="datePublished">2023-07-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/29/cs.CL_2023_07_29/">cs.CL - 2023-07-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Codable-Text-Watermarking-for-Large-Language-Models"><a href="#Towards-Codable-Text-Watermarking-for-Large-Language-Models" class="headerlink" title="Towards Codable Text Watermarking for Large Language Models"></a>Towards Codable Text Watermarking for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15992">http://arxiv.org/abs/2307.15992</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lancopku/codable-watermarking-for-llm">https://github.com/lancopku/codable-watermarking-for-llm</a></li>
<li>paper_authors: Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, Fandong Meng, Jie Zhou, Xu Sun</li>
<li>for: 防止大语言模型（LLM）的滥用，通过植入隐藏模式到生成的文本中来识别文本是否由LLM生成。</li>
<li>methods: 利用文本水印技术，在LLM生成的文本中植入隐藏模式以识别文本来源。</li>
<li>results: 提出了首个系统性研究，对LLM水印技术进行了首个系统性研究，并提出了一种可编程文本水印技术（CTWL），可以在不同的LLM应用场景中具有更多自定义信息编码需求。<details>
<summary>Abstract</summary>
As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns into the generated texts. However, we argue that existing watermarking methods for LLMs are encoding-inefficient (only contain one bit of information - whether it is generated from an LLM or not) and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.) in different LLMs application scenarios. In this work, we conduct the first systematic study on the topic of Codable Text Watermarking for LLMs (CTWL) that allows text watermarks to carry more customizable information. First of all, we study the taxonomy of LLM watermarking technology and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive evaluation system for CTWL: (1) watermarking success rate, (2) robustness against various corruptions, (3) coding rate of payload information, (4) encoding and decoding efficiency, (5) impacts on the quality of the generated text. To meet the requirements of these non-Pareto-improving metrics, we devise a CTWL method named Balance-Marking, based on the motivation of ensuring that available and unavailable vocabularies for encoding information have approximately equivalent probabilities. Compared to the random vocabulary partitioning extended from the existing work, a probability-balanced vocabulary partition can significantly improve the quality of the generated text. Extensive experimental results have shown that our method outperforms a direct baseline under comprehensive evaluation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）生成的文本流行度在增长，需要识别文本来防止LLM的滥用。文本水印技术已经证明可以有效地 отличи出LLM生成的文本，但我们认为现有的LLM水印方法存在编码不fficient（只包含一bit信息，即是否由LLM生成）和不能适应不同的LLM应用场景中的多样化信息编码需求。在这项工作中，我们进行了首次系统性的研究，探讨 codable text watermarking for LLM（CTWL）技术，允许文本水印包含更多自定义信息。首先，我们研究了LLM水印技术的分类和CTWL的数学表述。此外，我们提供了对CTWL的全面评价方法：（1）水印成功率，（2）对各种损害的Robustness，（3）payload信息的编码率，（4）编码和解码效率，（5）对生成文本质量的影响。为满足这些不可比较的度量，我们提出了一种名为Balance-Marking的CTWL方法，基于保证可用和不可用词汇的编码信息有相近的概率。与现有的随机词汇分 partitioning 方法相比，一个probability-balanced词汇分可以显著改善生成文本的质量。我们的方法在全面的实验结果中胜过直接基eline。
</details></li>
</ul>
<hr>
<h2 id="GeneMask-Fast-Pretraining-of-Gene-Sequences-to-Enable-Few-Shot-Learning"><a href="#GeneMask-Fast-Pretraining-of-Gene-Sequences-to-Enable-Few-Shot-Learning" class="headerlink" title="GeneMask: Fast Pretraining of Gene Sequences to Enable Few-Shot Learning"></a>GeneMask: Fast Pretraining of Gene Sequences to Enable Few-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15933">http://arxiv.org/abs/2307.15933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/roysoumya/genemask">https://github.com/roysoumya/genemask</a></li>
<li>paper_authors: Soumyadeep Roy, Jonas Wallat, Sowmya S Sundaram, Wolfgang Nejdl, Niloy Ganguly</li>
<li>for: 学习优化基因表示，用于基因序列分类。</li>
<li>methods: 提出了一种新的masking算法， GeneMask，用于基因序列中的MASKED语言模型训练。</li>
<li>results: 在四个基因序列分类数据集上，GeneMask-based模型在五种几个shot Setting中显著超过了SOTA模型（DNABert和LOGO），而且可以在训练时间上减少一半。此外，我们还发现了 conserved DNA sequence motifs和top-ranked PMI tokens之间的强相关性，可能表明了基因序列中的隐藏信息的包含。<details>
<summary>Abstract</summary>
Large-scale language models such as DNABert and LOGO aim to learn optimal gene representations and are trained on the entire Human Reference Genome. However, standard tokenization schemes involve a simple sliding window of tokens like k-mers that do not leverage any gene-based semantics and thus may lead to (trivial) masking of easily predictable sequences and subsequently inefficient Masked Language Modeling (MLM) training. Therefore, we propose a novel masking algorithm, GeneMask, for MLM training of gene sequences, where we randomly identify positions in a gene sequence as mask centers and locally select the span around the mask center with the highest Normalized Pointwise Mutual Information (NPMI) to mask. We observe that in the absence of human-understandable semantics in the genomics domain (in contrast, semantic units like words and phrases are inherently available in NLP), GeneMask-based models substantially outperform the SOTA models (DNABert and LOGO) over four benchmark gene sequence classification datasets in five few-shot settings (10 to 1000-shot). More significantly, the GeneMask-based DNABert model is trained for less than one-tenth of the number of epochs of the original SOTA model. We also observe a strong correlation between top-ranked PMI tokens and conserved DNA sequence motifs, which may indicate the incorporation of latent genomic information. The codes (including trained models) and datasets are made publicly available at https://github.com/roysoumya/GeneMask.
</details>
<details>
<summary>摘要</summary>
大规模语言模型如DNABert和LOGO目标学习优化的基因表示，并在人类参照基因组中进行训练。然而，标准的分割方案通常使用简单的滑块窗口的 токен，不利用基因基本 semantics，可能导致（轻松）遮盲易predictable的序列，并最终导致不fficient的Masked Language Modeling（MLM）训练。因此，我们提出了一种新的遮盲算法， called GeneMask，用于 MLM 训练基因序列，我们在基因序列中随机选择位置作为遮盲中心，然后在遮盲中心周围选择 highest Normalized Pointwise Mutual Information（NPMI） 的 span来遮盲。我们发现，在 genomics 领域缺乏人类理解的 semantics（与 NLP 领域中的 semantic units 不同），GeneMask 基于模型在四个基因序列分类数据集上表现出优于 SOTA 模型（DNABert 和 LOGO）的五种 few-shot Setting（10 到 1000 个批）中表现出优。此外，我们发现 GeneMask 基于 DNABert 模型在训练时间上的减少为原始 SOTA 模型的一半，并且我们发现顶尖 PMI token 与保守的 DNA 序列模式之间存在强相关性，这可能表明 incorporation 隐藏的 genomic 信息。我们将codes（包括训练模型）和数据集公开发布在 GitHub 上，请参考 <https://github.com/roysoumya/GeneMask>。
</details></li>
</ul>
<hr>
<h2 id="Analysing-the-Resourcefulness-of-the-Paragraph-for-Precedence-Retrieval"><a href="#Analysing-the-Resourcefulness-of-the-Paragraph-for-Precedence-Retrieval" class="headerlink" title="Analysing the Resourcefulness of the Paragraph for Precedence Retrieval"></a>Analysing the Resourcefulness of the Paragraph for Precedence Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01203">http://arxiv.org/abs/2308.01203</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bhoomeendra/paragraph_resourcefulness">https://github.com/bhoomeendra/paragraph_resourcefulness</a></li>
<li>paper_authors: Bhoomeendra Singh Sisodiya, Narendra Babu Unnam, P. Krishna Reddy, Apala Das, K. V. K. Santhy, V. Balakista Reddy</li>
<li>for:  aid legal practitioners in retrieving relevant legal information</li>
<li>methods:  analyzed the resourcefulness of paragraph-level information in capturing similarity among judgments</li>
<li>results:  found that paragraph-level methods could capture similarity with only a few paragraph interactions and exhibit more discriminating power over baseline document-level method, with comparable performance to state-of-the-art methods.Here is the same information in Simplified Chinese text:</li>
<li>for:  aid法律启用者在获取相关法律信息方面</li>
<li>methods: 利用判例文本中段级信息来捕捉判例之间的相似性</li>
<li>results: 发现段级方法可以通过只需几个段交互来捕捉判例之间的相似性，并且比基线文档级方法具有更高的分化力，与现状的状态艺术方法相当。<details>
<summary>Abstract</summary>
Developing methods for extracting relevant legal information to aid legal practitioners is an active research area. In this regard, research efforts are being made by leveraging different kinds of information, such as meta-data, citations, keywords, sentences, paragraphs, etc. Similar to any text document, legal documents are composed of paragraphs. In this paper, we have analyzed the resourcefulness of paragraph-level information in capturing similarity among judgments for improving the performance of precedence retrieval. We found that the paragraph-level methods could capture the similarity among the judgments with only a few paragraph interactions and exhibit more discriminating power over the baseline document-level method. Moreover, the comparison results on two benchmark datasets for the precedence retrieval on the Indian supreme court judgments task show that the paragraph-level methods exhibit comparable performance with the state-of-the-art methods
</details>
<details>
<summary>摘要</summary>
研究抽取有关法律信息以帮助法律专业人士是一个活跃的研究领域。在这个方面，研究团队通过不同的信息类型，如元数据、引用、关键词、句子、段落等进行探索。与任何文本文档一样，法律文档也是由段落组成。本文通过分析了法律文档段落级别信息的资源fulness，发现段落级别方法可以通过只有几个段落互动 capture judgments的相似性，并且与基eline文档级别方法相比，段落级别方法具有更高的分化力。此外，对印度最高法院判决任务的两个标准数据集进行比较研究表明，段落级别方法与当前领域的状态态方法相比，具有相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Dialogue-Shaping-Empowering-Agents-through-NPC-Interaction"><a href="#Dialogue-Shaping-Empowering-Agents-through-NPC-Interaction" class="headerlink" title="Dialogue Shaping: Empowering Agents through NPC Interaction"></a>Dialogue Shaping: Empowering Agents through NPC Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15833">http://arxiv.org/abs/2307.15833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhou, Xiangyu Peng, Mark Riedl</li>
<li>for: 本研究旨在使用人工智能技术快速让RL代理人在文本游戏环境中学习优化策略，特别是在大量步骤训练过程中。</li>
<li>methods: 本研究使用大型自然语言模型（LLMs）与NPCE交互获取关键信息，并使用知识图（KGs）和Story Shaping将该信息integrate到RL代理人的训练中。</li>
<li>results: 研究表明，通过与NPCE交互获取关键信息，可以帮助RL代理人更快地 converge 到优化策略，提高训练效率。<details>
<summary>Abstract</summary>
One major challenge in reinforcement learning (RL) is the large amount of steps for the RL agent needs to converge in the training process and learn the optimal policy, especially in text-based game environments where the action space is extensive. However, non-player characters (NPCs) sometimes hold some key information about the game, which can potentially help to train RL agents faster. Thus, this paper explores how to interact and converse with NPC agents to get the key information using large language models (LLMs), as well as incorporate this information to speed up RL agent's training using knowledge graphs (KGs) and Story Shaping.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在强制学习（RL）是训练过程中RL机器人需要很多步骤才能学习最佳策略，特别在文本基于游戏环境中，动作空间很广泛。然而，非玩家角色（NPC）有时会拥有游戏中的一些关键信息，这可能能够帮助快速训练RL机器人。因此，这篇论文探讨如何与NPC代理交互获取关键信息，以及如何使用知识图（KG）和 Story Shaping  incorporate这些信息以加速RL机器人的训练。
</details></li>
</ul>
<hr>
<h2 id="RT-2-Vision-Language-Action-Models-Transfer-Web-Knowledge-to-Robotic-Control"><a href="#RT-2-Vision-Language-Action-Models-Transfer-Web-Knowledge-to-Robotic-Control" class="headerlink" title="RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"></a>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15818">http://arxiv.org/abs/2307.15818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich</li>
<li>for: 这个论文的目标是将视力语言模型直接integrated到终端控制中，以提高总结和允许emergent semantic reasoning。</li>
<li>methods: 作者提出了一种简单的、通用的方法来实现这个目标：表示动作为文本token，并将其直接 incorporated into the training set of the model。</li>
<li>results: 作者的方法导致了高效的机器人策略和优秀的总结能力，并允许模型获得了一系列的emergent capabilities，如对 novel object generalization、不存在于机器人训练数据中的命令理解以及multi-stage semantic reasoning。<details>
<summary>Abstract</summary>
We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).
</details>
<details>
<summary>摘要</summary>
我们研究如何使用互联网规模的数据进行训练，以实现普适的机器人控制和出现意义的推理。我们的目标是让单个终端训练的模型可以同时学习机器人观察到的动作和大规模预训练的语言和视觉语言数据。为达到这个目标，我们提议并行练习现状的视觉语言模型，包括机器人轨迹数据和互联网规模的视觉语言任务，如视觉问答。我们的方法是将机器人动作表示为文本符号，并将其直接添加到模型的训练集中，与自然语言符号一样。我们称之为视觉语言动作模型（VLA），并实现了一个例子，称为RT-2。我们的广泛评估（6000次评估试验）表明，我们的方法可以实现高效的机器人政策，并使RT-2能够获得互联网规模训练的许多emergent能力，包括对新物品的广泛适应、对机器人培训数据中未出现的命令的解释、以及对用户命令的简单推理。此外，我们还证明了将链条思维包含在VLA中可以实现多stage意义推理，例如选择哪个物品作为锤子（一个岩石），或者选择哪种饮料适合某人（一种能量饮料）。
</details></li>
</ul>
<hr>
<h2 id="Resume-Evaluation-through-Latent-Dirichlet-Allocation-and-Natural-Language-Processing-for-Effective-Candidate-Selection"><a href="#Resume-Evaluation-through-Latent-Dirichlet-Allocation-and-Natural-Language-Processing-for-Effective-Candidate-Selection" class="headerlink" title="Resume Evaluation through Latent Dirichlet Allocation and Natural Language Processing for Effective Candidate Selection"></a>Resume Evaluation through Latent Dirichlet Allocation and Natural Language Processing for Effective Candidate Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15752">http://arxiv.org/abs/2307.15752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vidhita Jagwani, Smit Meghani, Krishna Pai, Sudhir Dhage</li>
<li>for: 提出一种基于Latent Dirichlet Allocation(LDA)和实体检测（SpaCy）的简历评分方法，以提高简历评分的内容准确性。</li>
<li>methods: 方法首先使用SpaCy的Named Entity Recognition（NER）提取简历中有用的实体，然后使用LDA模型将这些实体分配到不同的主题中，并计算每个实体的主题概率。</li>
<li>results: 使用LDA模型，我们的提出的系统可以将简历分解成 latent topic，并提取有意义的semantic representation。在考虑技能、学历、工作经验等多个属性时，我们的模型达到了77%的准确率（仅考虑技能）和82%的总准确率。<details>
<summary>Abstract</summary>
In this paper, we propose a method for resume rating using Latent Dirichlet Allocation (LDA) and entity detection with SpaCy. The proposed method first extracts relevant entities such as education, experience, and skills from the resume using SpaCy's Named Entity Recognition (NER). The LDA model then uses these entities to rate the resume by assigning topic probabilities to each entity. Furthermore, we conduct a detailed analysis of the entity detection using SpaCy's NER and report its evaluation metrics. Using LDA, our proposed system breaks down resumes into latent topics and extracts meaningful semantic representations. With a vision to define our resume score to be more content-driven rather than a structure and keyword match driven, our model has achieved 77% accuracy with respect to only skills in consideration and an overall 82% accuracy with all attributes in consideration. (like college name, work experience, degree and skills)
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种使用Latent Dirichlet Allocation（LDA）和实体检测（SpaCy）来评分简历的方法。我们的方法首先从简历中提取有用的实体，如教育经验和技能，使用SpaCy的命名实体识别（NER）。然后，LDA模型使用这些实体来评分简历，并将每个实体分配话题概率。此外，我们还进行了NER的Entity detection的详细分析，并对其评估指标进行了报告。使用LDA，我们的提出的系统可以将简历分解成含义上的话题，并提取有意义的语义表示。我们的目标是使简历评分更加内容驱动，而不是基于结构和关键词匹配，我们的模型在只考虑技能时的准确率为77%，并在所有特征（包括学院名、工作经验、学位和技能）的情况下的总准确率为82%。
</details></li>
</ul>
<hr>
<h2 id="Context-VQA-Towards-Context-Aware-and-Purposeful-Visual-Question-Answering"><a href="#Context-VQA-Towards-Context-Aware-and-Purposeful-Visual-Question-Answering" class="headerlink" title="Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering"></a>Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15745">http://arxiv.org/abs/2307.15745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nandita Naik, Christopher Potts, Elisa Kreiss</li>
<li>for: 使网络更加可交互地访问，帮助无法看到图像的人们问题图像。</li>
<li>methods: 引入了 Context-VQA  dataset，该 dataset将图像与上下文（例如购物网站）对应。</li>
<li>results: 发现不同上下文下的问题类型系统性异，例如旅游上的图像引起 2 倍多的 “Where?” 问题，社交媒体和新闻上的图像引起 2.8 倍多的 “Who?” 问题。Context 对 VQA 模型的性能有重要影响，特别在无法看到图像的情况下。<details>
<summary>Abstract</summary>
Visual question answering (VQA) has the potential to make the Internet more accessible in an interactive way, allowing people who cannot see images to ask questions about them. However, multiple studies have shown that people who are blind or have low-vision prefer image explanations that incorporate the context in which an image appears, yet current VQA datasets focus on images in isolation. We argue that VQA models will not fully succeed at meeting people's needs unless they take context into account. To further motivate and analyze the distinction between different contexts, we introduce Context-VQA, a VQA dataset that pairs images with contexts, specifically types of websites (e.g., a shopping website). We find that the types of questions vary systematically across contexts. For example, images presented in a travel context garner 2 times more "Where?" questions, and images on social media and news garner 2.8 and 1.8 times more "Who?" questions than the average. We also find that context effects are especially important when participants can't see the image. These results demonstrate that context affects the types of questions asked and that VQA models should be context-sensitive to better meet people's needs, especially in accessibility settings.
</details>
<details>
<summary>摘要</summary>
视觉问答（VQA）有可能使互联网变得更加访问ible，让无法看到图像的人可以通过问题来了解图像。然而，多个研究表明，盲人或有低视力的人更喜欢图像解释包含图像的上下文，然而当前VQA数据集却主要关注图像的孤立显示。我们认为VQA模型不会完全满足人们的需求， Unless they take context into account。为了进一步驱动和分析不同上下文的差异，我们引入了Context-VQA，一个对图像与上下文进行对应的VQA数据集。我们发现，不同的上下文中的问题类型系统atically vary。例如，在旅游上下文中出现的图像会引发2倍的“Where?”问题，而社交媒体和新闻上下文中出现的图像会引发2.8倍的“Who?”问题。我们还发现，上下文效应在参与者无法看到图像时特别重要。这些结果表明上下文对问题类型的影响，并且VQA模型应该sensitive to context，以更好地满足人们的需求，特别是在Accessibility设置下。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/29/cs.CL_2023_07_29/" data-id="clpahu70000973h88c2xncft8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/29/cs.LG_2023_07_29/" class="article-date">
  <time datetime="2023-07-29T10:00:00.000Z" itemprop="datePublished">2023-07-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/29/cs.LG_2023_07_29/">cs.LG - 2023-07-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Monaural-Multi-Speaker-Speech-Separation-Using-Efficient-Transformer-Model"><a href="#Monaural-Multi-Speaker-Speech-Separation-Using-Efficient-Transformer-Model" class="headerlink" title="Monaural Multi-Speaker Speech Separation Using Efficient Transformer Model"></a>Monaural Multi-Speaker Speech Separation Using Efficient Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00010">http://arxiv.org/abs/2308.00010</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Rijal, R. Neupane, S. P. Mainali, S. K. Regmi, S. Maharjan</li>
<li>for: 这篇论文是为了解决cocktail party问题而写的，该问题存在多个说话者的杂音混合中分离 individu speaker的困难。</li>
<li>methods: 该论文基于Transformer架构，使用了有效的形式来实现单频多说话者语音分离。模型在使用LibriMix数据集进行训练，可以分离出2个不同的说话者的源音。</li>
<li>results: 该模型可以减少语音分离模型的计算复杂性，而不是与传统语音分离模型的性能there is a significant trade-off。这个项目预期将在计算效率为核心的speech separation研究中做出重要贡献。<details>
<summary>Abstract</summary>
Cocktail party problem is the scenario where it is difficult to separate or distinguish individual speaker from a mixed speech from several speakers. There have been several researches going on in this field but the size and complexity of the model is being traded off with the accuracy and robustness of speech separation. "Monaural multi-speaker speech separation" presents a speech-separation model based on the Transformer architecture and its efficient forms. The model has been trained with the LibriMix dataset containing diverse speakers' utterances. The model separates 2 distinct speaker sources from a mixed audio input. The developed model approaches the reduction in computational complexity of the speech separation model, with minimum tradeoff with the performance of prevalent speech separation model and it has shown significant movement towards that goal. This project foresees, a rise in contribution towards the ongoing research in the field of speech separation with computational efficiency at its core.
</details>
<details>
<summary>摘要</summary>
干预吧问题是一种场景，在混合语音中分 apart或 distinguishing  individu speaker 很困难。有很多研究在这个领域，但是模型的大小和复杂度与准确性和可靠性之间存在负面关系。“单频多 speaker speech separation”提出了一种基于 Transformer 架构的 speech-separation 模型，并且其高效的形式。该模型在使用 LibriMix 数据集中训练，可以从混合音频输入中分离出两个不同的 speaker 源。该模型减少了speech separation 模型的计算复杂度，同时保持了与传统 speech separation 模型的性能相似的水平。这个项目预计会对激进的 speech separation 研究做出重要贡献，计算效率为核心。
</details></li>
</ul>
<hr>
<h2 id="A-3D-deep-learning-classifier-and-its-explainability-when-assessing-coronary-artery-disease"><a href="#A-3D-deep-learning-classifier-and-its-explainability-when-assessing-coronary-artery-disease" class="headerlink" title="A 3D deep learning classifier and its explainability when assessing coronary artery disease"></a>A 3D deep learning classifier and its explainability when assessing coronary artery disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00009">http://arxiv.org/abs/2308.00009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wing Keung Cheung, Jeremy Kalindjian, Robert Bell, Arjun Nair, Leon J. Menezes, Riyaz Patel, Simon Wan, Kacy Chou, Jiahang Chen, Ryo Torii, Rhodri H. Davies, James C. Moon, Daniel C. Alexander, Joseph Jacob</li>
<li>for: 预测和诊断抑阻性心血管疾病 (CAD)，以保存生命和减少医疗成本。</li>
<li>methods: 使用3D Resnet-50深度学习模型直接将正常人群和CAD患者分类在计算机Tomography coronary angiography图像上。</li>
<li>results: 比2D Resnet-50模型提高23.65%的准确率，同时提供了 Grad-GAM解释性。此外，还将3D CAD分类连接到2D两类 semantic segmentation，以提高解释性和精确的畸形定位。<details>
<summary>Abstract</summary>
Early detection and diagnosis of coronary artery disease (CAD) could save lives and reduce healthcare costs. In this study, we propose a 3D Resnet-50 deep learning model to directly classify normal subjects and CAD patients on computed tomography coronary angiography images. Our proposed method outperforms a 2D Resnet-50 model by 23.65%. Explainability is also provided by using a Grad-GAM. Furthermore, we link the 3D CAD classification to a 2D two-class semantic segmentation for improved explainability and accurate abnormality localisation.
</details>
<details>
<summary>摘要</summary>
早期发现和诊断心络动脉疾病（CAD）可以拯救生命和减少医疗成本。在这项研究中，我们提出了一种基于3D Resnet-50深度学习模型的直接分类正常人和CAD患者的计算机Tomography coronary angiography图像。我们的提议方法比2D Resnet-50模型高出23.65%。此外，我们还使用Grad-GAM来提供解释性。此外，我们将3D CAD分类与2D两个分类semantic segmentation相连接，以提高解释性和精确的异常位置定位。Here's the breakdown of the translation:* 早期发现 (early detection) becomes 早期发现 (zhāo qī fāxìn)* 诊断 (diagnosis) becomes 诊断 (diànfǎng)* 心络动脉疾病 (coronary artery disease) becomes 心络动脉疾病 (xīn liàng dòng mǎi byōng bìng)*  computed tomography coronary angiography (CTCA) becomes 计算机Tomography coronary angiography (jìsuànjī Tomography coronary angiography)* 模型 (model) becomes 模型 (módelì)* 直接分类 (direct classification) becomes 直接分类 (zhíxí fānglè)* 正常人 (normal subjects) becomes 正常人 (zhèngzhèng rén)* CAD patients becomes CAD患者 (CAD huàyè)* 解释性 (explainability) becomes 解释性 (jiějīngxìng)* Grad-GAM becomes Grad-GAM (Grad-GAM)* 2D two-class semantic segmentation becomes 2D两个分类semantic segmentation (2D liǎnggè fānglè semantic segmentation)
</details></li>
</ul>
<hr>
<h2 id="A-data-centric-deep-learning-approach-to-airway-segmentation"><a href="#A-data-centric-deep-learning-approach-to-airway-segmentation" class="headerlink" title="A data-centric deep learning approach to airway segmentation"></a>A data-centric deep learning approach to airway segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00008">http://arxiv.org/abs/2308.00008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wing Keung Cheung, Ashkan Pakzad, Nesrin Mogulkoc, Sarah Needleman, Bojidar Rangelov, Eyjolfur Gudmundsson, An Zhao, Mariam Abbas, Davina McLaverty, Dimitrios Asimakopoulos, Robert Chapman, Recep Savas, Sam M Janes, Yipeng Hu, Daniel C. Alexander, John R Hurst, Joseph Jacob</li>
<li>for: 这种研究用于鉴别和诊断各种慢性呼吸疾病中的气道束缚畸形和分布特征，以便估算疾病的EXTENT和严重程度。</li>
<li>methods: 该研究提出了一种数据驱动的深度学习技术，用于 segmenting the airway tree。该技术利用了 interpolate 和 image split，以提高数据的有用性和质量。然后，我们实现了一种 ensemble learning 策略，以集成不同缩放的 segmented airway trees。</li>
<li>results: 与基线模型相比，我们的方法在使用 combineloss 时，平均提高了 segmentation performance（dice similarity coefficient）的表现，高于基线模型的平均值2.5%。此外，我们的提posed technique具有低的GPU使用量和高的灵活性，可以在任何2D深度学习模型上部署。<details>
<summary>Abstract</summary>
The morphology and distribution of airway tree abnormalities enables diagnosis and disease characterisation across a variety of chronic respiratory conditions. In this regard, airway segmentation plays a critical role in the production of the outline of the entire airway tree to enable estimation of disease extent and severity. In this study, we propose a data-centric deep learning technique to segment the airway tree. The proposed technique utilises interpolation and image split to improve data usefulness and quality. Then, an ensemble learning strategy is implemented to aggregate the segmented airway trees at different scales. In terms of segmentation performance (dice similarity coefficient), our method outperforms the baseline model by 2.5% on average when a combined loss is used. Further, our proposed technique has a low GPU usage and high flexibility enabling it to be deployed on any 2D deep learning model.
</details>
<details>
<summary>摘要</summary>
《气道树异常 morphology 和分布》能够用于诊断和疾病特征化多种慢性呼吸疾病。在这种情况下，气道分 segmentation 扮演着关键的角色，以生成整个气道树的轮廓，以便估算疾病的扩散和严重程度。本研究提出了一种基于数据的深度学习技术，用于气道分 segmentation。该技术利用 interpolate 和图像分割来提高数据的有用性和质量。然后，我们实施了一种 ensemble learning 策略，将不同缩放的气道树分割结果聚合 together。在 segmentation 性能（ dice 相似度）方面，我们的方法在使用 combinated loss 时比基准模型高出 2.5% 的平均值。此外，我们的提议方法具有低 GPU 使用率和高灵活性，可以在任何 2D 深度学习模型上部署。
</details></li>
</ul>
<hr>
<h2 id="UPFL-Unsupervised-Personalized-Federated-Learning-towards-New-Clients"><a href="#UPFL-Unsupervised-Personalized-Federated-Learning-towards-New-Clients" class="headerlink" title="UPFL: Unsupervised Personalized Federated Learning towards New Clients"></a>UPFL: Unsupervised Personalized Federated Learning towards New Clients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15994">http://arxiv.org/abs/2307.15994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiandi Ye, Cen Chen, Yinggui Wang, Xiang Li, Ming Gao</li>
<li>for: 这篇论文targets the problem of providing personalized models for new clients in federated learning, when the existing model has already been trained and deployed.</li>
<li>methods: 本论文提出了一种基于 adaptive risk minimization 技术的方法，named FedTTA，以及两个优化策略：proxy regularization和early-stopping。此外，本论文还提出了一个特有的知识传授损失，用于Addressing device heterogeneity。</li>
<li>results: 实验结果显示，FedTTA和其变型获得了优秀的性能，在五个数据集上比基eline eleven 倍进步。code可以在：<a target="_blank" rel="noopener" href="https://github.com/anonymous-federated-learning/code%E3%80%82">https://github.com/anonymous-federated-learning/code。</a><details>
<summary>Abstract</summary>
Personalized federated learning has gained significant attention as a promising approach to address the challenge of data heterogeneity. In this paper, we address a relatively unexplored problem in federated learning. When a federated model has been trained and deployed, and an unlabeled new client joins, providing a personalized model for the new client becomes a highly challenging task. To address this challenge, we extend the adaptive risk minimization technique into the unsupervised personalized federated learning setting and propose our method, FedTTA. We further improve FedTTA with two simple yet effective optimization strategies: enhancing the training of the adaptation model with proxy regularization and early-stopping the adaptation through entropy. Moreover, we propose a knowledge distillation loss specifically designed for FedTTA to address the device heterogeneity. Extensive experiments on five datasets against eleven baselines demonstrate the effectiveness of our proposed FedTTA and its variants. The code is available at: https://github.com/anonymous-federated-learning/code.
</details>
<details>
<summary>摘要</summary>
<<SYS>>个人化联合学习已经吸引了广泛的注意力，作为数据不一致性的解决方案。在这篇论文中，我们解决了联合学习中较为未经探索的问题。当一个联合模型已经训练和部署后，新的客户端加入，提供个性化模型 для新客户端是一项非常具有挑战性的任务。为解决这个挑战，我们将适应风险最小化技术扩展到无监督个性化联合学习设置中，并提出我们的方法FedTTA。此外，我们还提出了两种简单 yet 有效的优化策略：在适应模型训练中使用代理约束和在适应过程中使用Entropy来停止。此外，我们还提出了专门为FedTTA设计的知识塑化损失来Address设备不同性。我们在五个数据集上对十一个基eline进行了广泛的实验，并证明了我们的提议FedTTA和其变种的效果。代码可以在：https://github.com/anonymous-federated-learning/code中找到。
</details></li>
</ul>
<hr>
<h2 id="Feature-Reweighting-for-EEG-based-Motor-Imagery-Classification"><a href="#Feature-Reweighting-for-EEG-based-Motor-Imagery-Classification" class="headerlink" title="Feature Reweighting for EEG-based Motor Imagery Classification"></a>Feature Reweighting for EEG-based Motor Imagery Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02515">http://arxiv.org/abs/2308.02515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taveena Lotey, Prateek Keserwani, Debi Prosad Dogra, Partha Pratim Roy</li>
<li>for: 这个研究旨在使用非侵入性的电enzephalographic (EEG) 信号进行 motor imagery (MI) 的分类，以预测用户的手部运动意图。</li>
<li>methods: 这个研究使用了卷积神经网络 (CNN) 方法进行 MI-EEG 信号的分类，并提出了一个具有降噪特性的特征重新权重方法，以缓解对于 MI-EEG 信号的训练中的问题。</li>
<li>results: 实验结果显示，提出的方法可以对 Physionet EEG-MMIDB 和 BCI Competition IV 2a 资料集进行了有效的分类，与现有方法相比，提高了9.34% 和 3.82% 的分类精度。<details>
<summary>Abstract</summary>
Classification of motor imagery (MI) using non-invasive electroencephalographic (EEG) signals is a critical objective as it is used to predict the intention of limb movements of a subject. In recent research, convolutional neural network (CNN) based methods have been widely utilized for MI-EEG classification. The challenges of training neural networks for MI-EEG signals classification include low signal-to-noise ratio, non-stationarity, non-linearity, and high complexity of EEG signals. The features computed by CNN-based networks on the highly noisy MI-EEG signals contain irrelevant information. Subsequently, the feature maps of the CNN-based network computed from the noisy and irrelevant features contain irrelevant information. Thus, many non-contributing features often mislead the neural network training and degrade the classification performance. Hence, a novel feature reweighting approach is proposed to address this issue. The proposed method gives a noise reduction mechanism named feature reweighting module that suppresses irrelevant temporal and channel feature maps. The feature reweighting module of the proposed method generates scores that reweight the feature maps to reduce the impact of irrelevant information. Experimental results show that the proposed method significantly improved the classification of MI-EEG signals of Physionet EEG-MMIDB and BCI Competition IV 2a datasets by a margin of 9.34% and 3.82%, respectively, compared to the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
“ motor 幻想（MI）使用非侵入性电энцефалографи（EEG）信号的分类是一个关键的目标，因为它可以预测用户的肢体运动意图。在latest的研究中，卷积神经网络（CNN）基本方法广泛应用于MI-EEG分类。MI-EEG信号分类训练 neural network 的挑战包括低信号噪声率、非站ARY、非线性和高复杂性的EEG信号。CNN基本方法在高噪声MI-EEG信号上计算的特征包含无关信息。因此，CNN基本方法计算的特征图中含有无关信息。这些多余的特征通常会误导神经网络训练并下降分类性能。因此，一种新的特征重要性评分方法被提出，以解决这个问题。该方法包括一种干扰降低机制，名为特征重要性评分模块，该模块可以减少无关的时间和通道特征图中的干扰。特征重要性评分模块生成的分数可以重新评估特征图中的重要性，从而降低无关信息的影响。实验结果表明，提出的方法可以显著改善Physionet EEG-MMIDB和BCI Competition IV 2a数据集中MI-EEG信号的分类性能，相比之下state-of-the-art方法的margin为9.34%和3.82%。”
</details></li>
</ul>
<hr>
<h2 id="RGB-D-Fusion-Image-Conditioned-Depth-Diffusion-of-Humanoid-Subjects"><a href="#RGB-D-Fusion-Image-Conditioned-Depth-Diffusion-of-Humanoid-Subjects" class="headerlink" title="RGB-D-Fusion: Image Conditioned Depth Diffusion of Humanoid Subjects"></a>RGB-D-Fusion: Image Conditioned Depth Diffusion of Humanoid Subjects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15988">http://arxiv.org/abs/2307.15988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sascha Kirch, Valeria Olyunina, Jan Ondřej, Rafael Pagés, Sergio Martin, Clara Pérez-Molina</li>
<li>for: 生成高分辨率深度图从低分辨率单色RGB图像中</li>
<li>methods: 使用图像conditioned杂度泛化概率模型生成低分辨率深度图，然后使用第二个杂度泛化概率模型conditioned在低分辨率RGB-D图像上upsample depth map</li>
<li>results: 提出了一种多模态conditioned杂度泛化概率模型，可以高效地生成高分辨率深度图从低分辨率单色RGB图像中<details>
<summary>Abstract</summary>
We present RGB-D-Fusion, a multi-modal conditional denoising diffusion probabilistic model to generate high resolution depth maps from low-resolution monocular RGB images of humanoid subjects. RGB-D-Fusion first generates a low-resolution depth map using an image conditioned denoising diffusion probabilistic model and then upsamples the depth map using a second denoising diffusion probabilistic model conditioned on a low-resolution RGB-D image. We further introduce a novel augmentation technique, depth noise augmentation, to increase the robustness of our super-resolution model.
</details>
<details>
<summary>摘要</summary>
我们介绍RGB-D-Fusion，一种多模态条件杂化推敲模型，用于从低分辨率单色RGB图像中生成高分辨率深度图。RGB-D-Fusion首先使用一种图像conditioned杂化推敲probabilistic模型生成低分辨率深度图，然后使用第二个杂化推敲probabilistic模型，conditioned on low-resolution RGB-D图像，进行upsampling。我们还介绍了一种新的扩展技术，深度噪声增强，以提高我们的超分辨率模型的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Vehicle-Price-Prediction-By-Aggregating-decision-tree-model-With-Boosting-Model"><a href="#Vehicle-Price-Prediction-By-Aggregating-decision-tree-model-With-Boosting-Model" class="headerlink" title="Vehicle Price Prediction By Aggregating decision tree model With Boosting Model"></a>Vehicle Price Prediction By Aggregating decision tree model With Boosting Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15982">http://arxiv.org/abs/2307.15982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Auwal Tijjani Amshi</li>
<li>for: 这个研究的目的是预测二手车价格，这是一个有趣且需要的问题，因为车价预测具有许多特征，需要考虑多个因素以确定准确的预测结果。</li>
<li>methods: 这个研究使用的方法包括python脚本建立数据Normalization、标准化和清洁，以避免机器学习算法中的噪音。</li>
<li>results: 该研究使用的模型是决策树模型和梯度提升预测模型，这两种模型被组合以实现更加准确的预测结果。研究发现，该模型在预测二手车价格方面表现了良好的性能。未来的车价预测研究可以使用同一数据集，并采用不同的预测技术来进行研究。<details>
<summary>Abstract</summary>
Predicting the price of used vehicles is a more interesting and needed problem by many users. Vehicle price prediction can be a challenging task due to the high number of attributes that should be considered for accurate prediction. The major step in the prediction process is the collection and pre-processing of the data. In this project, python scripts were built to normalize, standardize, and clean data to avoid unnecessary noise for machine learning algorithms. The data set used in this project can be very valuable in conducting similar research using different prediction techniques. Many assumptions were made on the basis of the data set. The proposed system uses a Decision tree model and Gradient boosting predictive model, which are combined in other to get closed to accurate prediction, the proposed model was evaluated and it gives a promising performance. The future price prediction of used vehicles with the help of the same data set will comprise different models.
</details>
<details>
<summary>摘要</summary>
预测二手车价格是一项更有趣且需要的问题，对多个用户来说。预测二手车价格是一项复杂的任务，因为需要考虑大量的特征来确定准确的预测。主要在预测过程中的一步是数据收集和处理。在该项目中，使用Python脚本来 норма化、标准化和清洁数据，以避免机器学习算法中的无用噪音。使用的数据集可以在进行类似研究中发挥重要作用，使用不同的预测技术进行研究。在项目中，提出了许多假设，基于数据集。提议的系统使用决策树模型和梯度拟合预测模型，这两种模型结合使用，以达到更加准确的预测。该模型在评估中表现良好，未来预测二手车价格将使用同一个数据集进行不同的模型。
</details></li>
</ul>
<hr>
<h2 id="Initial-State-Interventions-for-Deconfounded-Imitation-Learning"><a href="#Initial-State-Interventions-for-Deconfounded-Imitation-Learning" class="headerlink" title="Initial State Interventions for Deconfounded Imitation Learning"></a>Initial State Interventions for Deconfounded Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15980">http://arxiv.org/abs/2307.15980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Pfrommer, Yatong Bai, Hyunin Lee, Somayeh Sojoudi</li>
<li>for: 这个论文旨在解决imitative learning中的 causal confusion问题，即学习策略会因为对于不直接影响专家行为的特征而产生低开放式监督损失，但是在投入后表现不佳。</li>
<li>methods: 这篇论文提出了一种新的掩码算法，用于在分离的 observable space 中掩码 observable 特征，以避免 causal confusion。该算法不需要专家问题、专家奖励函数或 causal 图Specification。在某些假设下，论文 theoretically 证明了这种算法是保守的，不会错过 causally 影响专家的观察。</li>
<li>results: 论文通过应用掩码算法到 CartPole 和 Reacher 两个示例控制系统中，实践证明了该算法可以有效地避免 causal confusion，并提高 open-loop 监督损失。<details>
<summary>Abstract</summary>
Imitation learning suffers from causal confusion. This phenomenon occurs when learned policies attend to features that do not causally influence the expert actions but are instead spuriously correlated. Causally confused agents produce low open-loop supervised loss but poor closed-loop performance upon deployment. We consider the problem of masking observed confounders in a disentangled representation of the observation space. Our novel masking algorithm leverages the usual ability to intervene in the initial system state, avoiding any requirement involving expert querying, expert reward functions, or causal graph specification. Under certain assumptions, we theoretically prove that this algorithm is conservative in the sense that it does not incorrectly mask observations that causally influence the expert; furthermore, intervening on the initial state serves to strictly reduce excess conservatism. The masking algorithm is applied to behavior cloning for two illustrative control systems: CartPole and Reacher.
</details>
<details>
<summary>摘要</summary>
模仿学习受到 causal 混乱的影响。这种现象发生在学习的策略会注意到不会导致专家行为的特征，而是与专家行为间corrrelate的。 causally 混乱的代理人会 prodduce low open-loop 监督损失，但是在投入中表现不佳。我们考虑了隐藏观察空间中的观察器的问题。我们的新的masking算法利用了 usual 能够 intervene在初始系统状态上，不需要专家查询、专家奖励函数或 causal graph specification。在某些假设下，我们理论上证明了这个算法是 conservative的，即不会错чно mask 观察到 causally 影响专家的观察; 而且， intervene 在初始状态上会 strict 减少过度保守。 masking 算法应用于 two 个 ilustrative 控制系统： CartPole 和 Reacher。
</details></li>
</ul>
<hr>
<h2 id="Blockchain-empowered-Federated-Learning-for-Healthcare-Metaverses-User-centric-Incentive-Mechanism-with-Optimal-Data-Freshness"><a href="#Blockchain-empowered-Federated-Learning-for-Healthcare-Metaverses-User-centric-Incentive-Mechanism-with-Optimal-Data-Freshness" class="headerlink" title="Blockchain-empowered Federated Learning for Healthcare Metaverses: User-centric Incentive Mechanism with Optimal Data Freshness"></a>Blockchain-empowered Federated Learning for Healthcare Metaverses: User-centric Incentive Mechanism with Optimal Data Freshness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15975">http://arxiv.org/abs/2307.15975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawen Kang, Jinbo Wen, Dongdong Ye, Bingkun Lai, Tianhao Wu, Zehui Xiong, Jiangtian Nie, Dusit Niyato, Yang Zhang, Shengli Xie</li>
<li>for: 这篇论文旨在为健康Metaverse（metaverse）开发出用户中心的隐私保护框架，以提高Metaverse的安全性和数据新鲜度。</li>
<li>methods: 该论文提出了一种基于分布式学习（Federated Learning，FL）的用户中心隐私保护框架，并在此基础上提出了一种跨链接强化的FL框架，以提高感知数据的安全性。</li>
<li>results: 数字实验结果表明，提出的方案可以有效地保护Metaverse的感知数据，并且可以提高服务提供者的数据分享利益。<details>
<summary>Abstract</summary>
Given the revolutionary role of metaverses, healthcare metaverses are emerging as a transformative force, creating intelligent healthcare systems that offer immersive and personalized services. The healthcare metaverses allow for effective decision-making and data analytics for users. However, there still exist critical challenges in building healthcare metaverses, such as the risk of sensitive data leakage and issues with sensing data security and freshness, as well as concerns around incentivizing data sharing. In this paper, we first design a user-centric privacy-preserving framework based on decentralized Federated Learning (FL) for healthcare metaverses. To further improve the privacy protection of healthcare metaverses, a cross-chain empowered FL framework is utilized to enhance sensing data security. This framework utilizes a hierarchical cross-chain architecture with a main chain and multiple subchains to perform decentralized, privacy-preserving, and secure data training in both virtual and physical spaces. Moreover, we utilize Age of Information (AoI) as an effective data-freshness metric and propose an AoI-based contract theory model under Prospect Theory (PT) to motivate sensing data sharing in a user-centric manner. This model exploits PT to better capture the subjective utility of the service provider. Finally, our numerical results demonstrate the effectiveness of the proposed schemes for healthcare metaverses.
</details>
<details>
<summary>摘要</summary>
随着metaverse的革命性发展，健康metaverse正在成为一种转型力量，创造出智能健康系统，提供 immerse和个性化服务。健康metaverse允许用户进行有效的决策和数据分析。然而，建构健康metaverse还存在一些挑战，如敏感数据泄露的风险和感知数据安全和新鲜度的问题，以及数据分享的激励问题。在这篇论文中，我们首先设计了一个基于分布式学习（FL）的用户中心隐私保护框架，以帮助健康metaverse解决这些挑战。为了进一步提高健康metaverse的隐私保护，我们利用了跨链 empowered FL框架，以提高感知数据的安全性。这个框架利用了一个层次结构的跨链架构，包括主链和多个子链，以进行分布式、隐私保护和安全的数据训练 both in virtual and physical spaces。此外，我们利用了Age of Information（AoI）作为有效的新鲜度指标，并提出了基于Prospect Theory（PT）的合约理论模型，以激励感知数据分享。这个模型利用PT来更好地捕捉服务提供者的主观价值。最后，我们的数据示出了健康metaverse中提议的方案的效果。
</details></li>
</ul>
<hr>
<h2 id="Graph-Condensation-for-Inductive-Node-Representation-Learning"><a href="#Graph-Condensation-for-Inductive-Node-Representation-Learning" class="headerlink" title="Graph Condensation for Inductive Node Representation Learning"></a>Graph Condensation for Inductive Node Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15967">http://arxiv.org/abs/2307.15967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Gao, Tong Chen, Yilong Zang, Wentao Zhang, Quoc Viet Hung Nguyen, Kai Zheng, Hongzhi Yin</li>
<li>for: 提高大型图的计算效率，使图神经网络（GNNs）能够更好地应用于多种应用场景。</li>
<li>methods: 使用mapping-aware graph condensation（MCond）技术，实际学习原节点与新节点之间的一对多映射，从而将新节点直接 integrate into 简化后的图中进行表示学习。</li>
<li>results: 在 inductive 推理中，MCond 可以减少计算开销和存储需求，在 Reddit 数据集上达到了121.5倍的推理速度提升和55.9倍的存储需求减少。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for inductive representation learning. This enables direct information propagation on the synthetic graph, which is much more efficient than on the original large graph. Specifically, MCond employs an alternating optimization scheme with innovative loss terms from transductive and inductive perspectives, facilitating the mutual promotion between graph condensation and node mapping learning. Extensive experiments demonstrate the efficacy of our approach in inductive inference. On the Reddit dataset, MCond achieves up to 121.5x inference speedup and 55.9x reduction in storage requirements compared with counterparts based on the original graph.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在处理大规模图时遇到了重要的计算挑战，这限制了其在多种应用场景中的效果。为解决这 limitation，图简化技术 emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle unseen data. Therefore, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), which explicitly learns the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for inductive representation learning. This enables direct information propagation on the synthetic graph, which is much more efficient than on the original large graph. Specifically, MCond employs an alternating optimization scheme with innovative loss terms from transductive and inductive perspectives, facilitating the mutual promotion between graph condensation and node mapping learning. Extensive experiments demonstrate the efficacy of our approach in inductive inference. On the Reddit dataset, MCond achieves up to 121.5x inference speedup and 55.9x reduction in storage requirements compared with counterparts based on the original graph.
</details></li>
</ul>
<hr>
<h2 id="Recommendation-Unlearning-via-Matrix-Correction"><a href="#Recommendation-Unlearning-via-Matrix-Correction" class="headerlink" title="Recommendation Unlearning via Matrix Correction"></a>Recommendation Unlearning via Matrix Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15960">http://arxiv.org/abs/2307.15960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Liu, Dongsheng Li, Hansu Gu, Tun Lu, Jiongran Wu, Peng Zhang, Li Shang, Ning Gu</li>
<li>for: 提供个性化服务，但大量用户数据带来隐私、安全和实用性问题。</li>
<li>methods: 使用推荐解启（unlearning）方法，允许忘记特定数据和模型，以降低敏感&#x2F;恶意&#x2F;毒害用户数据的风险。</li>
<li>results: 提出一种基于交互和映射矩阵 corrections（IMCorrect）方法，可以增强推荐解启的完整性、实用性和效率，而无需重新训练模型。实验结果表明，IMCorrect 在多种推荐解启场景中具有更高的完整性、实用性和效率，并且可以逐步学习新数据，进一步提高实际应用性。<details>
<summary>Abstract</summary>
Recommender systems are important for providing personalized services to users, but the vast amount of collected user data has raised concerns about privacy (e.g., sensitive data), security (e.g., malicious data) and utility (e.g., toxic data). To address these challenges, recommendation unlearning has emerged as a promising approach, which allows specific data and models to be forgotten, mitigating the risks of sensitive/malicious/toxic user data. However, existing methods often struggle to balance completeness, utility, and efficiency, i.e., compromising one for the other, leading to suboptimal recommendation unlearning. In this paper, we propose an Interaction and Mapping Matrices Correction (IMCorrect) method for recommendation unlearning. Firstly, we reveal that many collaborative filtering (CF) algorithms can be formulated as mapping-based approach, in which the recommendation results can be obtained by multiplying the user-item interaction matrix with a mapping matrix. Then, IMCorrect can achieve efficient recommendation unlearning by correcting the interaction matrix and enhance the completeness and utility by correcting the mapping matrix, all without costly model retraining. Unlike existing methods, IMCorrect is a whitebox model that offers greater flexibility in handling various recommendation unlearning scenarios. Additionally, it has the unique capability of incrementally learning from new data, which further enhances its practicality. We conducted comprehensive experiments to validate the effectiveness of IMCorrect and the results demonstrate that IMCorrect is superior in completeness, utility, and efficiency, and is applicable in many recommendation unlearning scenarios.
</details>
<details>
<summary>摘要</summary>
我们发现许多集成过滤（CF）算法可以表示为映射基本的方法，在这种方法中，推荐结果可以通过用户-项目互动矩阵与映射矩阵的乘法来获得。然后，IMCorrect可以通过修正互动矩阵和映射矩阵来实现高效的推荐忘记，同时提高完整性和有用性，而不需要费时重新训练模型。与现有方法不同，IMCorrect是一种白盒模型，可以更好地处理各种推荐忘记场景。此外，它还具有逐步学习新数据的能力，这使其在实际应用中更加具有实用性。我们对IMCorrect的效果进行了广泛的实验验证，结果表明，IMCorrect在完整性、有用性和效率方面均较为出色，并且可以在许多推荐忘记场景中应用。
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Visualization-of-Aggregated-Class-Activation-Maps-to-Analyse-the-Global-Contribution-of-Class-Features"><a href="#Towards-the-Visualization-of-Aggregated-Class-Activation-Maps-to-Analyse-the-Global-Contribution-of-Class-Features" class="headerlink" title="Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features"></a>Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00710">http://arxiv.org/abs/2308.00710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Igor Cherepanov, David Sessler, Alex Ulmer, Hendrik Lücke-Tieke, Jörn Kohlhammer</li>
<li>for: 这 paper 的目的是解释深度学习模型在分类任务中的决策过程。</li>
<li>methods: 该 paper 使用了 Class Activation Maps (CAMs) 方法，该方法可以视觉化每个数据样本中对分类决策的重要性。</li>
<li>results: 该 paper 通过对多个样本的 CAMs 的聚合，提供了一个全局的解释视觉化，可以帮助分析员了解深度学习模型的决策过程。<details>
<summary>Abstract</summary>
Deep learning (DL) models achieve remarkable performance in classification tasks. However, models with high complexity can not be used in many risk-sensitive applications unless a comprehensible explanation is presented. Explainable artificial intelligence (xAI) focuses on the research to explain the decision-making of AI systems like DL. We extend a recent method of Class Activation Maps (CAMs) which visualizes the importance of each feature of a data sample contributing to the classification. In this paper, we aggregate CAMs from multiple samples to show a global explanation of the classification for semantically structured data. The aggregation allows the analyst to make sophisticated assumptions and analyze them with further drill-down visualizations. Our visual representation for the global CAM illustrates the impact of each feature with a square glyph containing two indicators. The color of the square indicates the classification impact of this feature. The size of the filled square describes the variability of the impact between single samples. For interesting features that require further analysis, a detailed view is necessary that provides the distribution of these values. We propose an interactive histogram to filter samples and refine the CAM to show relevant samples only. Our approach allows an analyst to detect important features of high-dimensional data and derive adjustments to the AI model based on our global explanation visualization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-effect-of-network-topologies-on-fully-decentralized-learning-a-preliminary-investigation"><a href="#The-effect-of-network-topologies-on-fully-decentralized-learning-a-preliminary-investigation" class="headerlink" title="The effect of network topologies on fully decentralized learning: a preliminary investigation"></a>The effect of network topologies on fully decentralized learning: a preliminary investigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15947">http://arxiv.org/abs/2307.15947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luigi Palmieri, Lorenzo Valerio, Chiara Boldrini, Andrea Passarella</li>
<li>for: 这个论文研究了一种分布式机器学习系统中 nodes 之间的网络拓扑对模型的性能影响。</li>
<li>methods: 作者使用了 direct collaboration between nodes 方法，并调查了不同类型的网络拓扑对 “知识传播” 的影响。</li>
<li>results: 研究发现，即使网络组件之间存在只有弱连接，也可以传输信息；但是，这并不意味着知识可以快速传播。 另外，研究发现，核心节点（hubs）在传播知识方面扮演着更重要的角色，而叶节点（leaves）的影响相对较小。 最后，研究发现，紧密结合的社区会干扰知识传播。<details>
<summary>Abstract</summary>
In a decentralized machine learning system, data is typically partitioned among multiple devices or nodes, each of which trains a local model using its own data. These local models are then shared and combined to create a global model that can make accurate predictions on new data. In this paper, we start exploring the role of the network topology connecting nodes on the performance of a Machine Learning model trained through direct collaboration between nodes. We investigate how different types of topologies impact the "spreading of knowledge", i.e., the ability of nodes to incorporate in their local model the knowledge derived by learning patterns in data available in other nodes across the networks. Specifically, we highlight the different roles in this process of more or less connected nodes (hubs and leaves), as well as that of macroscopic network properties (primarily, degree distribution and modularity). Among others, we show that, while it is known that even weak connectivity among network components is sufficient for information spread, it may not be sufficient for knowledge spread. More intuitively, we also find that hubs have a more significant role than leaves in spreading knowledge, although this manifests itself not only for heavy-tailed distributions but also when "hubs" have only moderately more connections than leaves. Finally, we show that tightly knit communities severely hinder knowledge spread.
</details>
<details>
<summary>摘要</summary>
Specifically, we examine the roles of more or less connected nodes (hubs and leaves) and macroscopic network properties (such as degree distribution and modularity) in this process. We find that while even weak connectivity among network components is sufficient for information spread, it may not be sufficient for knowledge spread. Additionally, we show that hubs play a more significant role than leaves in spreading knowledge, and that tightly knit communities hinder knowledge spread.我们在这篇论文中开始探讨了一个分布式机器学习系统中数据的分区和节点之间的直接协作对机器学习模型的性能的影响。我们研究了不同类型的网络拓扑如何影响“知识散布”，即每个节点通过学习数据中的学习模式来 incorporate 其他节点中的知识。我们发现，尽管even weak connectivity among network components is sufficient for information spread，但可能不够 для知识散布。此外，我们发现主要的节点（hubs）比叶子节点（leaves）更有助于散布知识，并且这种效应不仅限于重 tailed distribution，而且在“hubs”只有moderately more connections than leaves时也manifests itself。最后，我们发现，紧密结合的社区会严重阻碍知识散布。
</details></li>
</ul>
<hr>
<h2 id="PIMbot-Policy-and-Incentive-Manipulation-for-Multi-Robot-Reinforcement-Learning-in-Social-Dilemmas"><a href="#PIMbot-Policy-and-Incentive-Manipulation-for-Multi-Robot-Reinforcement-Learning-in-Social-Dilemmas" class="headerlink" title="PIMbot: Policy and Incentive Manipulation for Multi-Robot Reinforcement Learning in Social Dilemmas"></a>PIMbot: Policy and Incentive Manipulation for Multi-Robot Reinforcement Learning in Social Dilemmas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15944">http://arxiv.org/abs/2307.15944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahab Nikkhoo, Zexin Li, Aritra Samanta, Yufei Li, Cong Liu</li>
<li>for: 这 paper 的目的是探讨如何通过 manipulate 多机器人之间的交流，以达到更好的协作效果。</li>
<li>methods: 这 paper 使用了一种新的 manipulate 方法，即 PIMbot，可以在多机器人协作中 manipulate 奖励函数，从而影响 outcome。</li>
<li>results: 实验结果表明，PIMbot 可以有效地 manipulate 多机器人协作环境，并且可以影响任务结果的负面和正面效果。<details>
<summary>Abstract</summary>
Recent research has demonstrated the potential of reinforcement learning (RL) in enabling effective multi-robot collaboration, particularly in social dilemmas where robots face a trade-off between self-interests and collective benefits. However, environmental factors such as miscommunication and adversarial robots can impact cooperation, making it crucial to explore how multi-robot communication can be manipulated to achieve different outcomes. This paper presents a novel approach, namely PIMbot, to manipulating the reward function in multi-robot collaboration through two distinct forms of manipulation: policy and incentive manipulation. Our work introduces a new angle for manipulation in recent multi-agent RL social dilemmas that utilize a unique reward function for incentivization. By utilizing our proposed PIMbot mechanisms, a robot is able to manipulate the social dilemma environment effectively. PIMbot has the potential for both positive and negative impacts on the task outcome, where positive impacts lead to faster convergence to the global optimum and maximized rewards for any chosen robot. Conversely, negative impacts can have a detrimental effect on the overall task performance. We present comprehensive experimental results that demonstrate the effectiveness of our proposed methods in the Gazebo-simulated multi-robot environment. Our work provides insights into how inter-robot communication can be manipulated and has implications for various robotic applications. %, including robotics, transportation, and manufacturing.
</details>
<details>
<summary>摘要</summary>
近期研究表明了强化学习（RL）在多机器人协作中的潜力，特别是在社会冲突中机器人面临自身利益和集体利益之间的负担。然而，环境因素如沟通错误和反对机器人可能会影响合作，使得探索如何 manipulate multi-robot communication 以实现不同的结果变得非常重要。这篇论文提出了一种新的方法，即 PIMbot，用于 manipulate 多机器人协作中的奖励函数。我们的工作描述了两种不同的欺诈方式：策略欺诈和激励欺诈。我们的 PIMbot 机制可以在多机器人社会冲突环境中有效地操纵环境。PIMbot 有可能导致任务结果的正面和负面影响，其中正面影响可以使任务结果更快地 converges 到全局最优解和最大化任务奖励。然而，负面影响可能会对整体任务性能产生负面影响。我们在 Gazebo  simulate 多机器人环境中进行了广泛的实验研究，并提供了具有深入意义的结论。我们的工作为 robotics、运输和制造等领域提供了新的思路和方法，并且有助于我们更好地理解如何 manipulate inter-robot communication。
</details></li>
</ul>
<hr>
<h2 id="Continual-Learning-in-Predictive-Autoscaling"><a href="#Continual-Learning-in-Predictive-Autoscaling" class="headerlink" title="Continual Learning in Predictive Autoscaling"></a>Continual Learning in Predictive Autoscaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15941">http://arxiv.org/abs/2307.15941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonymousaccountx/DMSHM">https://github.com/anonymousaccountx/DMSHM</a></li>
<li>paper_authors: Hongyan Hao, Zhixuan Chu, Shiyi Zhu, Gangwei Jiang, Yan Wang, Caigao Jiang, James Zhang, Wei Jiang, Siqiao Xue, Jun Zhou</li>
<li>for: 预测云服务器负载和预备资源以保证服务水平目标 (SLOs) 在动态云环境中。</li>
<li>methods: 提出了一种基于重播学习的 kontinual learning 方法，即密度基于内存选择和提示基于网络学习模型 (DMSHM)，只使用历史记录的一小部分来实现准确预测。</li>
<li>results: 在公共和工业数据集上进行了实验，证明了我们提出的方法在内存容量和预测精度两个方面比现状态静学习方法更高效，并在实际工业应用中表现出了remarkable的实用性。<details>
<summary>Abstract</summary>
Predictive Autoscaling is used to forecast the workloads of servers and prepare the resources in advance to ensure service level objectives (SLOs) in dynamic cloud environments. However, in practice, its prediction task often suffers from performance degradation under abnormal traffics caused by external events (such as sales promotional activities and applications re-configurations), for which a common solution is to re-train the model with data of a long historical period, but at the expense of high computational and storage costs. To better address this problem, we propose a replay-based continual learning method, i.e., Density-based Memory Selection and Hint-based Network Learning Model (DMSHM), using only a small part of the historical log to achieve accurate predictions. First, we discover the phenomenon of sample overlap when applying replay-based continual learning in prediction tasks. In order to surmount this challenge and effectively integrate new sample distribution, we propose a density-based sample selection strategy that utilizes kernel density estimation to calculate sample density as a reference to compute sample weight, and employs weight sampling to construct a new memory set. Then we implement hint-based network learning based on hint representation to optimize the parameters. Finally, we conduct experiments on public and industrial datasets to demonstrate that our proposed method outperforms state-of-the-art continual learning methods in terms of memory capacity and prediction accuracy. Furthermore, we demonstrate remarkable practicability of DMSHM in real industrial applications.
</details>
<details>
<summary>摘要</summary>
predictive autoscaling 是用于预测服务器劳动负荷和预先准备资源，以确保云环境中的服务水平目标 (SLO)。然而，在实践中，其预测任务经常受到外部事件（如销售推广活动和应用程序重新配置）引起的异常流量的影响，导致性能下降。为了更好地解决这个问题，我们提议一种基于重温学习的再启用学习方法，即密度基于内存选择和提示基于网络学习模型 (DMSHM)，只需使用历史记录中的一小部分来实现准确的预测。首先，我们发现在应用重温学习的预测任务中存在样本重叠现象。为了超越这个挑战并有效地 интегра新样本分布，我们提议一种密度基于样本选择策略，利用核函数密度估计计算样本密度，并employs weight sampling将其构成为新的内存集。然后，我们实现提示基于提示表示来优化参数。最后，我们在公共和工业数据集上进行了实验，并证明了我们提议的方法在内存容量和预测精度两个方面比现状前方法更高。此外，我们还证明了DMSHM在实际工业应用中具有很好的实用性。
</details></li>
</ul>
<hr>
<h2 id="A-Theory-for-Emergence-of-Complex-Skills-in-Language-Models"><a href="#A-Theory-for-Emergence-of-Complex-Skills-in-Language-Models" class="headerlink" title="A Theory for Emergence of Complex Skills in Language Models"></a>A Theory for Emergence of Complex Skills in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15936">http://arxiv.org/abs/2307.15936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dia2018/What-is-the-Difference-Between-AI-and-Machine-Learning">https://github.com/dia2018/What-is-the-Difference-Between-AI-and-Machine-Learning</a></li>
<li>paper_authors: Sanjeev Arora, Anirudh Goyal</li>
<li>for: 本研究旨在解释语言模型新技能的出现是因为参数集和训练 Corpora 的扩大。</li>
<li>methods: 本研究使用了知名的 Scaling Laws of LLMs 和简单的统计分析框架来分析emergence。</li>
<li>results: 研究发现，通过增加参数集和训练 Corpora，语言模型会出现强大的 inductive bias，使其可以快速学习。此外，研究还发现，这种 inductive bias 可以使语言模型在执行包含多个技能的任务时表现出高水平的能力。<details>
<summary>Abstract</summary>
A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing tasks involving $k$-tuples of skills emerges essentially at the same scaling and same rate as competence on the elementary skills themselves.
</details>
<details>
<summary>摘要</summary>
现代AI产品的主要驱动力之一是语言模型新的技能出现，当它们的参数集和训练 Corpora 的大小增加时。这种现象尚未得到充分理解，而且通过梯度基本训练的数学分析难以提供机制性解释。本文采取了一种不同的方法，通过著名的扩展法律和简单的统计框架来分析emergence。本文的贡献包括：(a) 一种统计框架，将语言任务下的基本技能的混合损失与语言模型的泛化能力相关联。(b) 数学分析表明，扩展法律Imply一种强大的推导偏见，使预训练模型在学习过程中非常高效。我们称之为“飞弹泛化”，因为在常规泛化理论下看来，模型能够学习出违背常规的技能水平。(c) 一个重要的例子， Competence at executing tasks involving $k$-tuples of skills emerges essentially at the same scaling and same rate as competence on the elementary skills themselves。Translation notes:* "Scaling Laws" is translated as "扩展法律" (fāng zhì fǎ) in Chinese, which is a literal translation of the English phrase.* "LLMs" is translated as "语言模型" (yǔ yán mó del) in Chinese, which is a common abbreviation for "language models" in the field of natural language processing.* "competence" is translated as "泛化能力" (fān huà néng lì) in Chinese, which refers to the ability of a language model to perform a task or a set of tasks.* "elementary skills" is translated as "基本技能" (jī běn jì néng) in Chinese, which refers to the basic skills or abilities that underlie language tasks.* "slingshot generalization" is translated as "飞弹泛化" (fēi dàn fān huà) in Chinese, which is a literal translation of the English phrase.
</details></li>
</ul>
<hr>
<h2 id="A-Noisy-Label-Learning-Formulation-for-Immune-Repertoire-Classification-and-Disease-Associated-Immune-Receptor-Sequence-Identification"><a href="#A-Noisy-Label-Learning-Formulation-for-Immune-Repertoire-Classification-and-Disease-Associated-Immune-Receptor-Sequence-Identification" class="headerlink" title="A Noisy-Label-Learning Formulation for Immune Repertoire Classification and Disease-Associated Immune Receptor Sequence Identification"></a>A Noisy-Label-Learning Formulation for Immune Repertoire Classification and Disease-Associated Immune Receptor Sequence Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15934">http://arxiv.org/abs/2307.15934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tencentailabhealthcare/nll-irc">https://github.com/tencentailabhealthcare/nll-irc</a></li>
<li>paper_authors: Mingcai Chen, Yu Zhao, Zhonghuang Wang, Bing He, Jianhua Yao</li>
<li>for: 革命性贡献对新药和免疫疗法的研究，即 computeational biology 领域的前沿研究。</li>
<li>methods: 提出了一种噪声标签学习方法，解决了传统的多例空间 MIL 问题，即直接将袋级标签分配给实例。</li>
<li>results: 实现了高精度的序列级分类和抗体组织级分类，并在 CMV 和癌症数据集上进行了实验，得到了显著的性能提升。<details>
<summary>Abstract</summary>
Immune repertoire classification, a typical multiple instance learning (MIL) problem, is a frontier research topic in computational biology that makes transformative contributions to new vaccines and immune therapies. However, the traditional instance-space MIL, directly assigning bag-level labels to instances, suffers from the massive amount of noisy labels and extremely low witness rate. In this work, we propose a noisy-label-learning formulation to solve the immune repertoire classification task. To remedy the inaccurate supervision of repertoire-level labels for a sequence-level classifier, we design a robust training strategy: The initial labels are smoothed to be asymmetric and are progressively corrected using the model's predictions throughout the training process. Furthermore, two models with the same architecture but different parameter initialization are co-trained simultaneously to remedy the known "confirmation bias" problem in the self-training-like schema. As a result, we obtain accurate sequence-level classification and, subsequently, repertoire-level classification. Experiments on the Cytomegalovirus (CMV) and Cancer datasets demonstrate our method's effectiveness and superior performance on sequence-level and repertoire-level tasks.
</details>
<details>
<summary>摘要</summary>
免疫质量分类是生物计算中一个前沿研究领域，它对新肇病变和免疫治疗做出了革命性的贡献。然而，传统的实例空间MIL（多个实例学习）问题，直接将袋级标签 assigning 到实例，受到巨大量的噪音标签和极低的证人率问题。在这个工作中，我们提出了噪音标签学习形式来解决免疫质量分类任务。为了缓解实例级标签的不精确指导，我们设计了一个预处理策略：初始标签被填充了偏好的差异，并在训练过程中逐渐更正使用模型预测。此外，我们同时培训两个同样的架构，但不同的参数初始化，以缓解自然语言训练中知道的“证人偏见”问题。最终，我们获得了精确的序列级分类和免疫质量分类。实验结果显示，我们的方法在CMV和癌症数据集上具有高效性和优良性，并在序列级和免疫质量任务上获得了佳绩。
</details></li>
</ul>
<hr>
<h2 id="Language-models-as-master-equation-solvers"><a href="#Language-models-as-master-equation-solvers" class="headerlink" title="Language models as master equation solvers"></a>Language models as master equation solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02514">http://arxiv.org/abs/2308.02514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Chuanbo Liu, Jin Wang</li>
<li>for: 解决复杂随机动力系统的精确解方法</li>
<li>methods: 使用语言模型学习方法，通过提示基金网络将率参数、初始条件和时间值映射到状态联合概率分布中</li>
<li>results: 对多模块和高维系统进行了示例应用，观察了高精度和抽象能力，并证明了使用单个预训练大型模型可以解决任何精确解方程。<details>
<summary>Abstract</summary>
Master equations are of fundamental importance in modeling stochastic dynamical systems.However, solving master equations is challenging due to the exponential increase in the number of possible states or trajectories with the dimension of the state space. In this study, we propose repurposing language models as a machine learning approach to solve master equations. We design a prompt-based neural network to map rate parameters, initial conditions, and time values directly to the state joint probability distribution that exactly matches the input contexts. In this way, we approximate the solution of the master equation in its most general form. We train the network using the policy gradient algorithm within the reinforcement learning framework, with feedback rewards provided by a set of variational autoregressive models. By applying this approach to representative examples, we observe high accuracy for both multi-module and high-dimensional systems. The trained network also exhibits extrapolating ability, extending its predictability to unseen data. Our findings establish the connection between language models and master equations, highlighting the possibility of using a single pretrained large model to solve any master equation.
</details>
<details>
<summary>摘要</summary>
We designed a prompt-based neural network that maps rate parameters, initial conditions, and time values to the joint probability distribution of the state, which exactly matches the input context. This approach can approximate the solution of the master equation in its most general form.We trained the network using the policy gradient algorithm within the reinforcement learning framework, with feedback rewards provided by a set of variational autoregressive models. We applied this approach to representative examples and found that the trained network had high accuracy for both multi-module and high-dimensional systems. Additionally, the trained network was able to extrapolate to unseen data, demonstrating its predictive power.Our findings establish a connection between language models and master equations, showing that a single pretrained large model can be used to solve any master equation. This approach has the potential to revolutionize the field of stochastic dynamical systems modeling.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-deep-reinforcement-learning-algorithm-in-Partially-Observed-Markov-Decision-Processes"><a href="#Dynamic-deep-reinforcement-learning-algorithm-in-Partially-Observed-Markov-Decision-Processes" class="headerlink" title="Dynamic deep-reinforcement-learning algorithm in Partially Observed Markov Decision Processes"></a>Dynamic deep-reinforcement-learning algorithm in Partially Observed Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15931">http://arxiv.org/abs/2307.15931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saki Omi, Hyo-Sang Shin, Namhoon Cho, Antonios Tsourdos</li>
<li>for: 解决Partially Observable Markov Decision Process (POMDP)中agent的性能难以保持问题。</li>
<li>methods: 提出了一些结构和方法来扩展latest deep reinforcement learning algorithms with LSTM networks，以提高控制性能对不同类型的外部干扰的Robustness。</li>
<li>results: 研究表明，包含动作序列的情况可以解决POMDP中agent的性能问题，并且提出了一些结构和方法来扩展latest deep reinforcement learning algorithms with LSTM networks，以提高控制性能对不同类型的外部干扰的Robustness。<details>
<summary>Abstract</summary>
Reinforcement learning has been greatly improved in recent studies and an increased interest in real-world implementation has emerged in recent years. In many cases, due to the non-static disturbances, it becomes challenging for the agent to keep the performance. The disturbance results in the environment called Partially Observable Markov Decision Process. In common practice, Partially Observable Markov Decision Process is handled by introducing an additional estimator, or Recurrent Neural Network is utilized in the context of reinforcement learning. Both of the cases require to process sequential information on the trajectory. However, there are only a few studies investigating the effect of information to consider and the network structure to handle them. This study shows the benefit of action sequence inclusion in order to solve Partially Observable Markov Decision Process. Several structures and approaches are proposed to extend one of the latest deep reinforcement learning algorithms with LSTM networks. The developed algorithms showed enhanced robustness of controller performance against different types of external disturbances that are added to observation.
</details>
<details>
<summary>摘要</summary>
现在的研究中，人工智能学习（Reinforcement Learning）已经得到了 significativo 改进，而且在实际应用中的兴趣也在不断增长。然而，由于环境中的非静态干扰，agent often 难以维持性能。这种干扰会导致环境中的Partially Observable Markov Decision Process（POMDP）。在常见的做法中，POMDP 通常通过添加额外估计器或 Recurrent Neural Network（RNN）来处理。两者都需要处理序列信息的扩展。然而，只有一些研究探讨了考虑信息的影响和网络结构的处理方式。本研究表明，包含动作序列的包含可以解决POMDP。本研究提出了一些结构和方法，以扩展最新的深度学习控制算法，包括Long Short-Term Memory（LSTM）网络。研究发现，通过包含动作序列，可以提高控制性能的鲁棒性对不同类型的外部干扰。
</details></li>
</ul>
<hr>
<h2 id="Opportunistic-Air-Quality-Monitoring-and-Forecasting-with-Expandable-Graph-Neural-Networks"><a href="#Opportunistic-Air-Quality-Monitoring-and-Forecasting-with-Expandable-Graph-Neural-Networks" class="headerlink" title="Opportunistic Air Quality Monitoring and Forecasting with Expandable Graph Neural Networks"></a>Opportunistic Air Quality Monitoring and Forecasting with Expandable Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15916">http://arxiv.org/abs/2307.15916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingwei Zuo, Wenbin Li, Michele Baldo, Hakim Hacid</li>
<li>for: 这篇论文主要目的是提出一种可扩展的图注意网络模型（EGAT），用于融合不同空间结构的数据收集，以提高空气质量预测的精度。</li>
<li>methods: 该模型使用了图注意网络技术，可以将现有和新增的基础设施数据融合，以满足不同个人化enario的需求。此外，该模型还可以与现有的空气质量预测模型结合使用，以适应变化的空间结构。</li>
<li>results: 对实际空气质量数据进行验证，EGAT模型可以提高空气质量预测的精度，并且可以适应不同的空间结构变化。<details>
<summary>Abstract</summary>
Air Quality Monitoring and Forecasting has been a popular research topic in recent years. Recently, data-driven approaches for air quality forecasting have garnered significant attention, owing to the availability of well-established data collection facilities in urban areas. Fixed infrastructures, typically deployed by national institutes or tech giants, often fall short in meeting the requirements of diverse personalized scenarios, e.g., forecasting in areas without any existing infrastructure. Consequently, smaller institutes or companies with limited budgets are compelled to seek tailored solutions by introducing more flexible infrastructures for data collection. In this paper, we propose an expandable graph attention network (EGAT) model, which digests data collected from existing and newly-added infrastructures, with different spatial structures. Additionally, our proposal can be embedded into any air quality forecasting models, to apply to the scenarios with evolving spatial structures. The proposal is validated over real air quality data from PurpleAir.
</details>
<details>
<summary>摘要</summary>
《空气质量监测和预测研究在最近几年内得到了广泛关注。最近，基于数据驱动的空气质量预测方法受到了广泛关注，因为城市地区的数据收集设施已经成熔化了。固定基础设施，通常由国家机构或科技巨头部署，经常无法满足多样化个性化场景的需求，例如预测没有任何基础设施的地区。因此，更小的机构或公司具有有限预算，需要寻找适合自己的解决方案，例如引入更灵活的数据收集基础设施。在本文中，我们提出了可扩展图注意网络（EGAT）模型，该模型可以处理现有和新增的基础设施，具有不同的空间结构。此外，我们的提议可以与任何空气质量预测模型结合使用，以适应发展中的空间结构。实验 validate 在实际空气质量数据上进行。》Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="An-Automata-Theoretic-Approach-to-Synthesizing-Binarized-Neural-Networks"><a href="#An-Automata-Theoretic-Approach-to-Synthesizing-Binarized-Neural-Networks" class="headerlink" title="An Automata-Theoretic Approach to Synthesizing Binarized Neural Networks"></a>An Automata-Theoretic Approach to Synthesizing Binarized Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15907">http://arxiv.org/abs/2307.15907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye Tao, Wanwei Liu, Fu Song, Zhen Liang, Ji Wang, Hongxu Zhu</li>
<li>for: 这个论文的目的是提出一个自动化方法来Synthesize Binarized Neural Networks (BNNs)，以满足特定的特性。</li>
<li>methods: 这个方法使用了形式语言BLTL来定义特性，并使用自动机来实现。在Synthesis过程中，使用SMT解析器来检查网络是否存在。</li>
<li>results: 这个方法可以对BNNs进行自动化Synthesis，并提高个人公平和本地类别价值的表现，同时保持准确性。<details>
<summary>Abstract</summary>
Deep neural networks, (DNNs, a.k.a. NNs), have been widely used in various tasks and have been proven to be successful. However, the accompanied expensive computing and storage costs make the deployments in resource-constrained devices a significant concern. To solve this issue, quantization has emerged as an effective way to reduce the costs of DNNs with little accuracy degradation by quantizing floating-point numbers to low-width fixed-point representations. Quantized neural networks (QNNs) have been developed, with binarized neural networks (BNNs) restricted to binary values as a special case. Another concern about neural networks is their vulnerability and lack of interpretability. Despite the active research on trustworthy of DNNs, few approaches have been proposed to QNNs. To this end, this paper presents an automata-theoretic approach to synthesizing BNNs that meet designated properties. More specifically, we define a temporal logic, called BLTL, as the specification language. We show that each BLTL formula can be transformed into an automaton on finite words. To deal with the state-explosion problem, we provide a tableau-based approach in real implementation. For the synthesis procedure, we utilize SMT solvers to detect the existence of a model (i.e., a BNN) in the construction process. Notably, synthesis provides a way to determine the hyper-parameters of the network before training.Moreover, we experimentally evaluate our approach and demonstrate its effectiveness in improving the individual fairness and local robustness of BNNs while maintaining accuracy to a great extent.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs，即NNs）在各种任务中广泛应用，并证明了其成功。然而，随之而来的计算和存储成本使得在有限资源设备中部署DNNs成为了一项重要问题。为解决这个问题，量化得到了广泛应用的方法，通过将浮点数转换为低宽定点表示来减少DNNs的成本，同时减少精度下降。量化神经网络（QNNs）已经开发出来，其中二进制神经网络（BNNs）的特殊情况是强制限制为二进制值。另一个神经网络的问题是它的不安全性和解释性的缺失。尽管有大量的研究在深度神经网络的可靠性方面，但对QNNs的研究尚未充分。因此，这篇论文提出了一种自动机理论方法来生成满足特定性质的BNNs。更具体地说，我们定义了一种时间逻辑（BLTL）作为规定语言。我们证明了每个BLTL公式都可以转换为一个在有限字符串上的自动机。为了解决状态爆发问题，我们提供了一种表格基本方法。在Synthesis过程中，我们利用SMT解决器来检测存在一个模型（即BNN）。另外，Synthesis还提供了一种方法来确定网络的各种参数（例如，层数、权重等）在建立过程中。同时，我们还进行了实验评估，并证明了我们的方法可以大幅提高BNNs的个人公平和本地Robustness，保持精度的程度。
</details></li>
</ul>
<hr>
<h2 id="Multi-view-Sparse-Laplacian-Eigenmaps-for-nonlinear-Spectral-Feature-Selection"><a href="#Multi-view-Sparse-Laplacian-Eigenmaps-for-nonlinear-Spectral-Feature-Selection" class="headerlink" title="Multi-view Sparse Laplacian Eigenmaps for nonlinear Spectral Feature Selection"></a>Multi-view Sparse Laplacian Eigenmaps for nonlinear Spectral Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15905">http://arxiv.org/abs/2307.15905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaurav Srivastava, Mahesh Jangid</li>
<li>for:  Addressing the challenges of high-dimensional datasets in machine learning, such as overfitting and computational complexity, by identifying an informative subset of features.</li>
<li>methods:  Multi-view Sparse Laplacian Eigenmaps (MSLE) for feature selection, combining multiple views of the data, enforcing sparsity constraints, and using a scalable optimization algorithm to identify a reduced feature set.</li>
<li>results:  Reduced the feature space by 10 to 90% while maintaining an error rate of 2.72% with Support Vector Machine (SVM), and achieved an accuracy of 96.69% with an 80% reduction in the overall feature space.<details>
<summary>Abstract</summary>
The complexity of high-dimensional datasets presents significant challenges for machine learning models, including overfitting, computational complexity, and difficulties in interpreting results. To address these challenges, it is essential to identify an informative subset of features that captures the essential structure of the data. In this study, the authors propose Multi-view Sparse Laplacian Eigenmaps (MSLE) for feature selection, which effectively combines multiple views of the data, enforces sparsity constraints, and employs a scalable optimization algorithm to identify a subset of features that capture the fundamental data structure. MSLE is a graph-based approach that leverages multiple views of the data to construct a more robust and informative representation of high-dimensional data. The method applies sparse eigendecomposition to reduce the dimensionality of the data, yielding a reduced feature set. The optimization problem is solved using an iterative algorithm alternating between updating the sparse coefficients and the Laplacian graph matrix. The sparse coefficients are updated using a soft-thresholding operator, while the graph Laplacian matrix is updated using the normalized graph Laplacian. To evaluate the performance of the MSLE technique, the authors conducted experiments on the UCI-HAR dataset, which comprises 561 features, and reduced the feature space by 10 to 90%. Our results demonstrate that even after reducing the feature space by 90%, the Support Vector Machine (SVM) maintains an error rate of 2.72%. Moreover, the authors observe that the SVM exhibits an accuracy of 96.69% with an 80% reduction in the overall feature space.
</details>
<details>
<summary>摘要</summary>
高维数据集的复杂性对机器学习模型提出了 significante挑战，包括过拟合、计算复杂性以及解释结果的困难。为了解决这些挑战，必须 identificainformative的特征子集，以捕捉数据的基本结构。本研究提出了多视图稀疏勋略 Laplacian Eigenmaps（MSLE）来选择特征，它可以有效地结合多个视图的数据，实施稀疏约束，并使用可扩展的优化算法来确定一个捕捉数据基本结构的特征子集。MSLE是基于图的方法，利用多个视图的数据构建更加robust和有用的数据表示。方法使用稀疏勋略减少数据维度，生成减少特征集。优化问题使用迭代更新稀疏系数和Laplacian图matrix的iterative算法解决。稀疏系数使用软阈值运算符更新，而Laplacian图matrix使用正则化Laplacian图。为评估MSLE技术的性能，作者在UCIDataset上进行了实验，UCIDataset包括561个特征，并将特征空间减少到10%-90%。我们的结果显示，即使特征空间减少到90%，Support Vector Machine（SVM）的错误率仅为2.72%。此外，作者发现，将特征空间减少到80%后，SVM的准确率达96.69%。
</details></li>
</ul>
<hr>
<h2 id="Online-Matching-A-Real-time-Bandit-System-for-Large-scale-Recommendations"><a href="#Online-Matching-A-Real-time-Bandit-System-for-Large-scale-Recommendations" class="headerlink" title="Online Matching: A Real-time Bandit System for Large-scale Recommendations"></a>Online Matching: A Real-time Bandit System for Large-scale Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15893">http://arxiv.org/abs/2307.15893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyang Yi, Shao-Chuan Wang, Ruining He, Hariharan Chandrasekaran, Charles Wu, Lukasz Heldt, Lichan Hong, Minmin Chen, Ed H. Chi</li>
<li>for: 提高大规模推荐系统中的新内容发现和用户兴趣探索能力</li>
<li>methods: 采用协同online+offline学习方法，提出Diag-LinUCB算法来实现分布式更新带ит参数</li>
<li>results: 通过实验示例，在YouTube平台上实现了在线学习系统的可扩展性和实时性，提高了新内容发现和用户兴趣探索的能力<details>
<summary>Abstract</summary>
The last decade has witnessed many successes of deep learning-based models for industry-scale recommender systems. These models are typically trained offline in a batch manner. While being effective in capturing users' past interactions with recommendation platforms, batch learning suffers from long model-update latency and is vulnerable to system biases, making it hard to adapt to distribution shift and explore new items or user interests. Although online learning-based approaches (e.g., multi-armed bandits) have demonstrated promising theoretical results in tackling these challenges, their practical real-time implementation in large-scale recommender systems remains limited. First, the scalability of online approaches in servicing a massive online traffic while ensuring timely updates of bandit parameters poses a significant challenge. Additionally, exploring uncertainty in recommender systems can easily result in unfavorable user experience, highlighting the need for devising intricate strategies that effectively balance the trade-off between exploitation and exploration. In this paper, we introduce Online Matching: a scalable closed-loop bandit system learning from users' direct feedback on items in real time. We present a hybrid "offline + online" approach for constructing this system, accompanied by a comprehensive exposition of the end-to-end system architecture. We propose Diag-LinUCB -- a novel extension of the LinUCB algorithm -- to enable distributed updates of bandits parameter in a scalable and timely manner. We conduct live experiments in YouTube and show that Online Matching is able to enhance the capabilities of fresh content discovery and item exploration in the present platform.
</details>
<details>
<summary>摘要</summary>
过去一个 décennial 内，深度学习基于模型在行业级推荐系统中取得了许多成功。这些模型通常在批量方式进行训练。虽然能够充分捕捉用户在推荐平台上的过去交互，但批量学习受到系统偏见的影响，难以适应分布Shift和探索新的用户 интере点。虽然在线学习基于方法（例如多臂投手）有许多理性的研究成果，但在大规模推荐系统中实际实施仍然受限。首先，在面临巨大在线流量的情况下，在线学习方法的可扩展性和实时更新投手参数的挑战是不可或缺的。其次，探索推荐系统中的不确定性容易导致用户体验不佳，高亮了需要制定复杂的策略，以有效地平衡利用和探索之间的负荷。在本文中，我们介绍了在线匹配：一种可扩展的关闭Loop投手系统，通过用户直接反馈ITEMS的实时反馈来学习。我们提出了一种“离线+在线”的方法，并提供了整体系统架构的详细介绍。我们还提出了Diag-LinUCB算法，用于在分布式环境中有效地更新投手参数。我们在 YouTube 上进行了实验，并证明了在线匹配可以增强现有的新内容发现和项目探索。
</details></li>
</ul>
<hr>
<h2 id="A-new-Gradient-TD-Algorithm-with-only-One-Step-size-Convergence-Rate-Analysis-using-L-λ-Smoothness"><a href="#A-new-Gradient-TD-Algorithm-with-only-One-Step-size-Convergence-Rate-Analysis-using-L-λ-Smoothness" class="headerlink" title="A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$λ$ Smoothness"></a>A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$λ$ Smoothness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15892">http://arxiv.org/abs/2307.15892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengshuai Yao</li>
<li>for: 本文研究了一种新的凸优化 gradient temporal difference（GTD）算法，用于解决强化学习中的偏离策略学习问题。</li>
<li>methods: 本文使用了 GTD 算法，并提出了一种新的单时间尺度 GTD 算法，具有只有一个步长参数。此外，本文还使用了 $L$-$\lambda$ 稳定性来证明新算法的 convergency 速率。</li>
<li>results: 本文通过实验表明，新提出的 Impression GTD 算法在 Random walks、Boyan chain 和 Baird counterexample 等问题上具有更高的 convergency 速率，并且可以在各种不同的步长参数下实现好的性能。<details>
<summary>Abstract</summary>
Gradient Temporal Difference (GTD) algorithms (Sutton et al., 2008, 2009) are the first $O(d)$ ($d$ is the number features) algorithms that have convergence guarantees for off-policy learning with linear function approximation. Liu et al. (2015) and Dalal et. al. (2018) proved the convergence rates of GTD, GTD2 and TDC are $O(t^{-\alpha/2})$ for some $\alpha \in (0,1)$. This bound is tight (Dalal et al., 2020), and slower than $O(1/\sqrt{t})$. GTD algorithms also have two step-size parameters, which are difficult to tune. In literature, there is a "single-time-scale" formulation of GTD. However, this formulation still has two step-size parameters.   This paper presents a truly single-time-scale GTD algorithm for minimizing the Norm of Expected td Update (NEU) objective, and it has only one step-size parameter. We prove that the new algorithm, called Impression GTD, converges at least as fast as $O(1/t)$. Furthermore, based on a generalization of the expected smoothness (Gower et al. 2019), called $L$-$\lambda$ smoothness, we are able to prove that the new GTD converges even faster, in fact, with a linear rate. Our rate actually also improves Gower et al.'s result with a tighter bound under a weaker assumption. Besides Impression GTD, we also prove the rates of three other GTD algorithms, one by Yao and Liu (2008), another called A-transpose-TD (Sutton et al., 2008), and a counterpart of A-transpose-TD. The convergence rates of all the four GTD algorithms are proved in a single generic GTD framework to which $L$-$\lambda$ smoothness applies. Empirical results on Random walks, Boyan chain, and Baird counterexample show that Impression GTD converges much faster than existing GTD algorithms for both on-policy and off-policy learning problems, with well-performing step-sizes in a big range.
</details>
<details>
<summary>摘要</summary>
Gradient Temporal Difference（GTD）算法（Sutton et al., 2008, 2009）是第一个 $O(d)$ ($d$ 是特征数) 拥有确定性 guarantees 的偏离策略学习算法。Liu et al.（2015）和Dalal et al.（2018）证明 GTD、GTD2 和 TDC 的收敛率为 $O(t^{-\alpha/2})$，其中 $\alpha \in (0,1)$。这个 bound 是紧张的（Dalal et al., 2020），并且 slower than $O(1/\sqrt{t})$。GTD 算法还有两个步长参数，这些参数难以调整。在文献中，有一种“单时间尺度”的 GTD 形式化，但这种形式化仍有两个步长参数。这篇文章提出了一个真正的单时间尺度 GTD 算法，用于最小化 Norm of Expected td Update（NEU）目标函数，并且具有只有一个步长参数。我们证明该新算法，称为 Impression GTD，在至少 $O(1/t)$ 的速度下收敛。此外，基于预期平滑性（Gower et al., 2019）的一种扩展，称为 $L$-$\lambda$ 平滑性，我们能够证明 Impression GTD 在实际情况下收敛更快，具体来说是线性收敛率。我们的速率实际上还超过 Gower et al. 的结果，并且在较弱的假设下提供了更紧张的 bound。除了 Impression GTD 之外，我们还证明了三种 GTD 算法的收敛率，分别是 Yao 和 Liu（2008）的一种 GTD 算法、Sutton et al.（2008）的 A-transpose-TD 算法，以及它的对应者。所有四种 GTD 算法的收敛率在一个通用的 GTD 框架中证明，该框架下 $L$-$\lambda$ 平滑性适用。实际实验表明，Impression GTD 在Random walks、Boyan chain 和 Baird counterexample 等问题上收敛 much faster than 现有 GTD 算法，并且步长在大范围内表现良好。
</details></li>
</ul>
<hr>
<h2 id="First-order-Policy-Optimization-for-Robust-Policy-Evaluation"><a href="#First-order-Policy-Optimization-for-Robust-Policy-Evaluation" class="headerlink" title="First-order Policy Optimization for Robust Policy Evaluation"></a>First-order Policy Optimization for Robust Policy Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15890">http://arxiv.org/abs/2307.15890</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Yan Li, Guanghui Lan</li>
<li>for: 该论文关注了Markov决策过程中的稳定性和不确定性问题，提出了一种基于政策优化的方法来评估政策的性能。</li>
<li>methods: 该论文使用了首领策略评估（FRPE）方法，该方法在幂等设定下提供了对于精确和不确定情况下的政策评估的一个统一框架，可以应用于表格表示或通用函数近似。</li>
<li>results: 该论文证明了FRPE方法在幂等设定下具有线性减少性，并且在随机设定下具有$\mathcal{O}(1&#x2F;\epsilon^2)$的样本复杂度。此外，FRPE还可以自然地扩展到评估不确定状态动作价值函数。<details>
<summary>Abstract</summary>
We adopt a policy optimization viewpoint towards policy evaluation for robust Markov decision process with $\mathrm{s}$-rectangular ambiguity sets. The developed method, named first-order policy evaluation (FRPE), provides the first unified framework for robust policy evaluation in both deterministic (offline) and stochastic (online) settings, with either tabular representation or generic function approximation. In particular, we establish linear convergence in the deterministic setting, and $\tilde{\mathcal{O}(1/\epsilon^2)$ sample complexity in the stochastic setting. FRPE also extends naturally to evaluating the robust state-action value function with $(\mathrm{s}, \mathrm{a})$-rectangular ambiguity sets. We discuss the application of the developed results for stochastic policy optimization of large-scale robust MDPs.
</details>
<details>
<summary>摘要</summary>
我们采用一个政策优化的看法来评估政策的强健Markov决策过程（MDP）中的政策评估。我们发展的方法，名为首轮政策评估（FRPE），提供了强健政策评估的第一个一致性框架，可以在决定（离线）和随机（在线）设置中进行，并且可以使用表格表示或通用函数近似。具体来说，我们证明了决定性设置中的线性传播，并且在随机设置中实现了$\mathcal{O}(1/\epsilon^2)$的样本复杂度。FRPE还自然地扩展到评估强健状态行为函数的($\mathrm{s}$, $\mathrm{a}$)-矩形不确定集。我们讨论了对大规模强健MDP的随机政策优化的应用。
</details></li>
</ul>
<hr>
<h2 id="Explaining-Full-disk-Deep-Learning-Model-for-Solar-Flare-Prediction-using-Attribution-Methods"><a href="#Explaining-Full-disk-Deep-Learning-Model-for-Solar-Flare-Prediction-using-Attribution-Methods" class="headerlink" title="Explaining Full-disk Deep Learning Model for Solar Flare Prediction using Attribution Methods"></a>Explaining Full-disk Deep Learning Model for Solar Flare Prediction using Attribution Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15878">http://arxiv.org/abs/2307.15878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/gsudmlab/explainfdvgg16">https://bitbucket.org/gsudmlab/explainfdvgg16</a></li>
<li>paper_authors: Chetraj Pandey, Rafal A. Angryk, Berkay Aydin</li>
<li>for: 这研究投入了深度学习方法来预测日冕物理学上的太阳折射风暴，尤其是受到严重忽略的近缘环区风暴。</li>
<li>methods: 这paper使用了一个使用每小时全盘线对视图图像的深度学习模型，并采用了二分类预测模式来预测将在下一个24小时期内发生的M级或更大的风暴。为了解决类别不均衡问题，这paper使用了数据拓展和类型权重技术。</li>
<li>results: 这paper的分析发现，全盘预测太阳风暴与活跃区（ARs）相关的特征都能够准确预测近缘环区风暴。具体来说，这paper的深度学习模型在24小时内预测M级或更大的风暴的True Skill Statistics（TSS）和Heidke Skill Score（HSS）分别为0.51和0.35，而且模型的解释分析表明，模型可以通过从全盘磁图像中提取与ARs相关的特征来做出对应预测。<details>
<summary>Abstract</summary>
This paper contributes to the growing body of research on deep learning methods for solar flare prediction, primarily focusing on highly overlooked near-limb flares and utilizing the attribution methods to provide a post hoc qualitative explanation of the model's predictions. We present a solar flare prediction model, which is trained using hourly full-disk line-of-sight magnetogram images and employs a binary prediction mode to forecast $\geq$M-class flares that may occur within the following 24-hour period. To address the class imbalance, we employ a fusion of data augmentation and class weighting techniques; and evaluate the overall performance of our model using the true skill statistic (TSS) and Heidke skill score (HSS). Moreover, we applied three attribution methods, namely Guided Gradient-weighted Class Activation Mapping, Integrated Gradients, and Deep Shapley Additive Explanations, to interpret and cross-validate our model's predictions with the explanations. Our analysis revealed that full-disk prediction of solar flares aligns with characteristics related to active regions (ARs). In particular, the key findings of this study are: (1) our deep learning models achieved an average TSS=0.51 and HSS=0.35, and the results further demonstrate a competent capability to predict near-limb solar flares and (2) the qualitative analysis of the model explanation indicates that our model identifies and uses features associated with ARs in central and near-limb locations from full-disk magnetograms to make corresponding predictions. In other words, our models learn the shape and texture-based characteristics of flaring ARs even at near-limb areas, which is a novel and critical capability with significant implications for operational forecasting.
</details>
<details>
<summary>摘要</summary>
In simplified Chinese:这篇论文为深入学习方法的太阳风暴预测研究做出了贡献，主要集中在快速减少的近边风暴预测方面，并使用负责任分析方法提供后果质量的解释。我们提出了一种太阳风暴预测模型，该模型通过每小时的全盘视线磁agram图像训练，并使用二进制预测模式预测下一个24小时内可能发生的M级风暴。为了解决类别不均衡问题，我们使用数据扩充和分类权重技术。我们使用真实技能统计学（TSS）和海德克技能统计学（HSS）来评估模型的总性表现。此外，我们还应用了三种负责任分析方法，即导航梯度权重映射、整合梯度和深度凝聚式加法解释，以解释和证明我们的模型预测的解释。我们的分析发现，我们的深度学习模型可以准确预测近边风暴，并且可以从全盘磁agram图像中提取活跃区域（ARs）的特征。具体来说，我们的模型可以在中央和近边位置中找到ARs的形状和文本特征，这是一种新的和重要的能力，具有深刻的运营预测意义。
</details></li>
</ul>
<hr>
<h2 id="GraphDAC-A-Graph-Analytic-Approach-to-Dynamic-Airspace-Configuration"><a href="#GraphDAC-A-Graph-Analytic-Approach-to-Dynamic-Airspace-Configuration" class="headerlink" title="GraphDAC: A Graph-Analytic Approach to Dynamic Airspace Configuration"></a>GraphDAC: A Graph-Analytic Approach to Dynamic Airspace Configuration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15876">http://arxiv.org/abs/2307.15876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kefenge2022/graphdac">https://github.com/kefenge2022/graphdac</a></li>
<li>paper_authors: Ke Feng, Dahai Liu, Yongxin Liu, Hong Liu, Houbing Song</li>
<li>for: 提高航空交通能力和应急响应能力</li>
<li>methods: 使用图像学算法和聚类分析生成协同机场组和均衡工作负担</li>
<li>results: 在不同交通情况下，实现了减少工作负担差异50%Here’s a brief explanation of each point:</li>
<li>for: The paper aims to improve the capacity and responsiveness of the current National Airspace System (NAS) by proposing a more dynamic airspace configuration approach.</li>
<li>methods: The proposed approach uses a constraints-embedded graph, dimensional compression, and spectral clustering-enabled adaptive algorithm to generate collaborative airport groups and evenly distribute workloads among them.</li>
<li>results: The experiments demonstrate a 50% reduction in workload imbalances under various traffic conditions, indicating the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
The current National Airspace System (NAS) is reaching capacity due to increased air traffic, and is based on outdated pre-tactical planning. This study proposes a more dynamic airspace configuration (DAC) approach that could increase throughput and accommodate fluctuating traffic, ideal for emergencies. The proposed approach constructs the airspace as a constraints-embedded graph, compresses its dimensions, and applies a spectral clustering-enabled adaptive algorithm to generate collaborative airport groups and evenly distribute workloads among them. Under various traffic conditions, our experiments demonstrate a 50\% reduction in workload imbalances. This research could ultimately form the basis for a recommendation system for optimized airspace configuration. Code available at https://github.com/KeFenge2022/GraphDAC.git
</details>
<details>
<summary>摘要</summary>
现有的国家航空系统（NAS）因为增加的航空交通量而达到了容量限制，并且基于先前的战略规划。本研究提出了一种更动态航空配置（DAC）方法，可以提高通过率和应对峰值交通时间的适应性。该方法将空间构建为约束嵌入图，压缩其维度，并通过特征集群算法来生成协作机场组和均衡工作负担。在不同的交通情况下，我们的实验表明，可以降低工作负担不均的幅度达50%。这些研究可能最终形成一个优化航空配置的建议系统。代码可以在https://github.com/KeFenge2022/GraphDAC.git中下载。
</details></li>
</ul>
<hr>
<h2 id="Cross-dimensional-transfer-learning-in-medical-image-segmentation-with-deep-learning"><a href="#Cross-dimensional-transfer-learning-in-medical-image-segmentation-with-deep-learning" class="headerlink" title="Cross-dimensional transfer learning in medical image segmentation with deep learning"></a>Cross-dimensional transfer learning in medical image segmentation with deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15872">http://arxiv.org/abs/2307.15872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hic-messaoudi/cross-dimensional-transfer-learning-in-medical-image-segmentation-with-deep-learning">https://github.com/hic-messaoudi/cross-dimensional-transfer-learning-in-medical-image-segmentation-with-deep-learning</a></li>
<li>paper_authors: Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem, Pierre-Henri Conze</li>
<li>for: 这篇论文的目的是将2D类别网络转移到2D和3D多模式医疗影像分类中，以提高医疗影像分类的精度和效率。</li>
<li>methods: 本论文提出了两个关键原则：首先，通过嵌入2D预训网络的重要特征，实现重要特征的转移，以提高2D类别网络的精度和效率。其次，通过扩展2D类别网络到更高维度，以实现维度的转移，以提高2D和3D多模式医疗影像分类的精度和效率。</li>
<li>results: 本论文的实验和质感结果显示，这些方法可以优化2D和3D多模式医疗影像分类的精度和效率。特别是，在CAMUS挑战中，这篇论文的2D网络排名第一，超过了现有的state-of-the-art。在CHAOS挑战中，这篇论文的2D&#x2F;3D MR和CT腹部影像的分类结果优于其他2D基于方法，并在Dice、RAVD、ASSD和MSSD分类指标中跨越了前一代。在BraTS 2022比赛中，这篇论文的3D网络也获得了良好的结果，平均Dice分类指标为91.69%（91.22%）、核心部分为83.23%（84.77%）和增强部分为81.75%（83.88%）。<details>
<summary>Abstract</summary>
Over the last decade, convolutional neural networks have emerged and advanced the state-of-the-art in various image analysis and computer vision applications. The performance of 2D image classification networks is constantly improving and being trained on databases made of millions of natural images. However, progress in medical image analysis has been hindered by limited annotated data and acquisition constraints. These limitations are even more pronounced given the volumetry of medical imaging data. In this paper, we introduce an efficient way to transfer the efficiency of a 2D classification network trained on natural images to 2D, 3D uni- and multi-modal medical image segmentation applications. In this direction, we designed novel architectures based on two key principles: weight transfer by embedding a 2D pre-trained encoder into a higher dimensional U-Net, and dimensional transfer by expanding a 2D segmentation network into a higher dimension one. The proposed networks were tested on benchmarks comprising different modalities: MR, CT, and ultrasound images. Our 2D network ranked first on the CAMUS challenge dedicated to echo-cardiographic data segmentation and surpassed the state-of-the-art. Regarding 2D/3D MR and CT abdominal images from the CHAOS challenge, our approach largely outperformed the other 2D-based methods described in the challenge paper on Dice, RAVD, ASSD, and MSSD scores and ranked third on the online evaluation platform. Our 3D network applied to the BraTS 2022 competition also achieved promising results, reaching an average Dice score of 91.69% (91.22%) for the whole tumor, 83.23% (84.77%) for the tumor core, and 81.75% (83.88%) for enhanced tumor using the approach based on weight (dimensional) transfer. Experimental and qualitative results illustrate the effectiveness of our methods for multi-dimensional medical image segmentation.
</details>
<details>
<summary>摘要</summary>
过去一个十年， convolutional neural networks（CNN）在各种图像分析和计算视觉应用中得到了提升和进步。2D图像分类网络的性能不断提高，并在包含数百万个自然图像的数据库上进行训练。然而，医疗图像分析的进步受到有限的标注数据和获取限制的影响。这些限制在医疗图像数据的量度上更加明显。在这篇文章中，我们介绍了一种高效的方法，将2D自然图像分类网络的效率传递到2D、3D单模和多模医疗图像分割应用中。为此，我们设计了基于以下两个关键原则的新架构：1）通过嵌入2D预训练encoder来实现重量传递，2）通过扩展2D分割网络到更高维度来实现维度传递。我们在不同模式的数据集上进行测试，包括MR、CT和ultrasound图像。我们的2D网络在Cardio- computed tomography（CAMUS）挑战中以echo-cardiographic数据分割的方式 ranked first，超过了现状。在2D/3D MR和CT腹部图像上，我们的方法在CHAOS挑战中较其他2D基于方法的Dice、RAVD、ASSD和MSSD分数上表现出色，并在在线评估平台上排名第三。我们的3D网络在BraTS 2022比赛中也实现了良好的结果，取得了91.69%（91.22%）的总肿瘤Dice分数，83.23%（84.77%）的肿瘤核心Dice分数和81.75%（83.88%）的增强肿瘤Dice分数。实验和质量分析结果表明，我们的方法在多维度医疗图像分 segmentation 中具有有效性。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Semi-Supervised-Federated-Learning-for-Heterogeneous-Participants"><a href="#Efficient-Semi-Supervised-Federated-Learning-for-Heterogeneous-Participants" class="headerlink" title="Efficient Semi-Supervised Federated Learning for Heterogeneous Participants"></a>Efficient Semi-Supervised Federated Learning for Heterogeneous Participants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15870">http://arxiv.org/abs/2307.15870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhipeng Sun, Yang Xu, Hongli Xu, Zhiyuan Wang<br>for: This paper proposes a novel system for training machine learning models in scenarios where labeled data reside on the server, called Pseudo-Clustering Semi-SFL.methods: The proposed system leverages semi-supervised techniques and clustering regularization to improve model performance under data non-IIDness. Additionally, a control algorithm for global updating frequency adaptation is developed to mitigate the training inconsistency.results: The proposed system achieves a 3.3x speed-up in training time and reduces the communication cost by about 80.1% while reaching the target accuracy, and achieves up to 6.9% improvement in accuracy under non-IID scenarios compared to the state-of-the-art.Here is the simplified Chinese version:for: 这篇论文提出了一种基于服务器上的标签数据的机器学习模型训练系统，名为 Pseudo-Clustering Semi-SFL。methods: 该系统利用了半指导的技术和帮助Regularization来提高模型在非同分布场景下的性能。此外，还开发了一种控制算法来调整全局更新频率，以避免训练不一致。results: 该系统实现了训练时间的3.3倍减少和通信成本的约80.1%减少，同时达到目标准确率，并在非同分布场景下实现了6.9%的提升。<details>
<summary>Abstract</summary>
Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. However, training and deploying large models for broader applications is challenging in resource-constrained environments. Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients SFL often assumes labeled data for local training on clients, however, it is not the case in practice.Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. Herein, we propose Pseudo-Clustering Semi-SFL, a novel system for training models in scenarios where labeled data reside on the server. By introducing Clustering Regularization, model performance under data non-IIDness can be improved. Besides, our theoretical and experimental investigations into model convergence reveal that the inconsistent training processes on labeled and unlabeled data impact the effectiveness of clustering regularization. Upon this, we develop a control algorithm for global updating frequency adaptation, which dynamically adjusts the number of supervised training iterations to mitigate the training inconsistency. Extensive experiments on benchmark models and datasets show that our system provides a 3.3x speed-up in training time and reduces the communication cost by about 80.1% while reaching the target accuracy, and achieves up to 6.9% improvement in accuracy under non-IID scenarios compared to the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 已经出现以 Allow Multiple clients 共同训练机器学习模型 On Their Private Data 。然而，在有限资源环境中训练和部署大型模型 для更广泛的应用是具有挑战性。幸运的是，Split Federated Learning (SFL) 提供了一个优秀的解决方案，减轻客户端的计算和通信压力。SFL 通常假设客户端上有标注数据进行本地训练，但在实践中并不是这样。先前的工作已经采用了 semi-supervised 技术来利用无标注数据，但是数据非标一致性又是一个困难。在这种情况下，我们提出了 Pseudo-Clustering Semi-SFL，一种用于在客户端上训练模型的新系统。我们引入了集群 regularization，可以在非标一致性情况下提高模型性能。此外，我们对模型融合的理论和实验调查表明，在标注和无标注数据上不一致的训练过程会影响集群 regularization 的效果。为此，我们开发了一种控制算法，可以动态调整全局更新频率，以抵消训练不一致性的影响。我们对标准模型和数据集进行了广泛的实验，结果显示，我们的系统可以提高训练时间速度3.3倍，降低通信成本约80.1%，同时达到目标准确率。此外，我们的系统在非标一致场景下可以提高准确率达6.9%。
</details></li>
</ul>
<hr>
<h2 id="Faster-Stochastic-Algorithms-for-Minimax-Optimization-under-Polyak–Lojasiewicz-Conditions"><a href="#Faster-Stochastic-Algorithms-for-Minimax-Optimization-under-Polyak–Lojasiewicz-Conditions" class="headerlink" title="Faster Stochastic Algorithms for Minimax Optimization under Polyak–Łojasiewicz Conditions"></a>Faster Stochastic Algorithms for Minimax Optimization under Polyak–Łojasiewicz Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15868">http://arxiv.org/abs/2307.15868</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/truenobility303/spider-gda">https://github.com/truenobility303/spider-gda</a></li>
<li>paper_authors: Lesi Chen, Boyuan Yao, Luo Luo</li>
<li>for: 这个论文考虑了随机首次算法的最大化问题，它们可以在Polyak-{\L}ojasiewicz（PL）条件下进行优化。</li>
<li>methods: 我们提出了一种名为SPIDER-GDA的算法来解决具有finite-sum形式的问题，即 $\min_x \max_y f(x,y)\triangleq \frac{1}{n} \sum_{i&#x3D;1}^n f_i(x,y)$，其中目标函数$f(x,y)$是$\mu_x$-PL在$x$上和$\mu_y$-PL在$y$上，每个$f_i(x,y)$是$L$-平滑的。我们证明了SPIDER-GDA可以在${\mathcal O}\left((n + \sqrt{n},\kappa_x\kappa_y^2)\log (1&#x2F;\epsilon)\right)$个随机首次oracle（SFO）复杂度内找到$\epsilon$-优解，比现状态艺术法的SFOUpper bound更好。</li>
<li>results: 我们的算法可以在糟糕条件下提供更高效的算法，其SFOUpper bound为$\tilde{\mathcal O}\big((n+\sqrt{n},\kappa_x\kappa_y)\log^2 (1&#x2F;\epsilon)\big)$，当$\kappa_y \gtrsim \sqrt{n}$时。我们的想法还可以应用于更一般的情况下，即目标函数只满足PL条件一个变量。实验 validate了我们的提案的优越性。<details>
<summary>Abstract</summary>
This paper considers stochastic first-order algorithms for minimax optimization under Polyak--{\L}ojasiewicz (PL) conditions. We propose SPIDER-GDA for solving the finite-sum problem of the form $\min_x \max_y f(x,y)\triangleq \frac{1}{n} \sum_{i=1}^n f_i(x,y)$, where the objective function $f(x,y)$ is $\mu_x$-PL in $x$ and $\mu_y$-PL in $y$; and each $f_i(x,y)$ is $L$-smooth. We prove SPIDER-GDA could find an $\epsilon$-optimal solution within ${\mathcal O}\left((n + \sqrt{n}\,\kappa_x\kappa_y^2)\log (1/\epsilon)\right)$ stochastic first-order oracle (SFO) complexity, which is better than the state-of-the-art method whose SFO upper bound is ${\mathcal O}\big((n + n^{2/3}\kappa_x\kappa_y^2)\log (1/\epsilon)\big)$, where $\kappa_x\triangleq L/\mu_x$ and $\kappa_y\triangleq L/\mu_y$. For the ill-conditioned case, we provide an accelerated algorithm to reduce the computational cost further. It achieves $\tilde{\mathcal O}\big((n+\sqrt{n}\,\kappa_x\kappa_y)\log^2 (1/\epsilon)\big)$ SFO upper bound when $\kappa_y \gtrsim \sqrt{n}$. Our ideas also can be applied to the more general setting that the objective function only satisfies PL condition for one variable. Numerical experiments validate the superiority of proposed methods.
</details>
<details>
<summary>摘要</summary>
本文考虑了随机首次算法 для最小最大化问题，其中目标函数 $f(x,y)$ 满足 Polyak-{\L}ojasiewicz（PL）条件，并且每个 $f_i(x,y)$ 是 $L$ 平滑的。我们提出了 SPIDER-GDA 算法来解决 finite-sum 问题 $\min_x \max_y f(x,y) \triangleq \frac{1}{n} \sum_{i=1}^n f_i(x,y)$，其中 $x$ 和 $y$ 都是维度为 $n$ 的变量。我们证明了 SPIDER-GDA 可以在 ${\mathcal O}\left((n + \sqrt{n}\,\kappa_x\kappa_y^2)\log (1/\epsilon)\right)$ 随机首次访问（SFO）复杂度内找到 $\epsilon$-优解，这比现状最佳方法的 SFO upper bound 更好，其为 ${\mathcal O}\big((n + n^{2/3}\kappa_x\kappa_y^2)\log (1/\epsilon)\big)$。在坏条件下，我们提供了一个加速算法，可以将计算复杂度降低到 $\tilde{\mathcal O}\big((n+\sqrt{n}\,\kappa_x\kappa_y)\log^2 (1/\epsilon)\big)$。我们的想法也可以应用于更一般的情况，其中目标函数只满足 PL 条件一个变量。实验 validate 我们提出的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Catching-Elusive-Depression-via-Facial-Micro-Expression-Recognition"><a href="#Catching-Elusive-Depression-via-Facial-Micro-Expression-Recognition" class="headerlink" title="Catching Elusive Depression via Facial Micro-Expression Recognition"></a>Catching Elusive Depression via Facial Micro-Expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15862">http://arxiv.org/abs/2307.15862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohui Chen, Tie Luo</li>
<li>for: 这项研究旨在识别隐藏型抑郁症（Concealed Depression），通过识别面部微表情（Facial Micro-Expressions，FMEs）来检测和识别真正的情感表达。</li>
<li>methods: 该研究提出了一种基于面部特征点（Facial Landmarks）的区域 интерес点（Region-of-Interest，ROI）方法，以解决识别FMEs的挑战。此外，该研究还提出了一种低成本、隐私保护的解决方案，允许用户在个人 Setting（如家中）进行自诊断，使用可携带的移动设备。</li>
<li>results: 研究结果和发现表明，该方法可以有效地识别和检测隐藏型抑郁症。然而，在实际临床设置中，还需要解决一些技术挑战，以确保方法的可靠性和精度。<details>
<summary>Abstract</summary>
Depression is a common mental health disorder that can cause consequential symptoms with continuously depressed mood that leads to emotional distress. One category of depression is Concealed Depression, where patients intentionally or unintentionally hide their genuine emotions through exterior optimism, thereby complicating and delaying diagnosis and treatment and leading to unexpected suicides. In this paper, we propose to diagnose concealed depression by using facial micro-expressions (FMEs) to detect and recognize underlying true emotions. However, the extremely low intensity and subtle nature of FMEs make their recognition a tough task. We propose a facial landmark-based Region-of-Interest (ROI) approach to address the challenge, and describe a low-cost and privacy-preserving solution that enables self-diagnosis using portable mobile devices in a personal setting (e.g., at home). We present results and findings that validate our method, and discuss other technical challenges and future directions in applying such techniques to real clinical settings.
</details>
<details>
<summary>摘要</summary>
抑郁是一种常见的心理健康问题，可能导致严重的情感不适和情绪压力。一种类型的抑郁是隐藏型抑郁，病人通过表面上的乐观情绪隐藏真实的情感，从而复杂和延迟诊断和治疗，导致意外的自杀。在这篇论文中，我们提议使用表情微表情（FMEs）来检测和识别隐藏的真实情感。然而，表情微表情的非常低敏感和细腻性使其识别成为一项困难的任务。我们提议使用面部特征点的区域利用方法（ROI）解决这个挑战，并描述一种低成本、隐私保护的解决方案，允许自我诊断在家庭环境（如家中）使用手持式移动设备进行。我们展示了结果和发现，并讨论了其他技术挑战和未来方向在实际临床设置中应用such techniques。
</details></li>
</ul>
<hr>
<h2 id="Multi-output-Headed-Ensembles-for-Product-Item-Classification"><a href="#Multi-output-Headed-Ensembles-for-Product-Item-Classification" class="headerlink" title="Multi-output Headed Ensembles for Product Item Classification"></a>Multi-output Headed Ensembles for Product Item Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15858">http://arxiv.org/abs/2307.15858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hotaka Shiokawa, Pradipto Das, Arthur Toth, Justin Chiu</li>
<li>for: The paper is written for the problem of product item classification for large-scale e-commerce catalogs, specifically addressing the issue of poor generalization performance due to the unavailability of sizable curated training sets.</li>
<li>methods: The paper proposes an extensible deep learning based classification model framework that combines multiple classifiers and uses metadata features and low-level feature engineering to boost classification performance.</li>
<li>results: The paper shows improvements in classification performance against robust industry standard baseline models using hyperparameter optimization, and also proposes a novel way to evaluate model performance using user sessions that provides better insights in addition to traditional measures of precision and recall.<details>
<summary>Abstract</summary>
In this paper, we revisit the problem of product item classification for large-scale e-commerce catalogs. The taxonomy of e-commerce catalogs consists of thousands of genres to which are assigned items that are uploaded by merchants on a continuous basis. The genre assignments by merchants are often wrong but treated as ground truth labels in automatically generated training sets, thus creating a feedback loop that leads to poorer model quality over time. This problem of taxonomy classification becomes highly pronounced due to the unavailability of sizable curated training sets.   Under such a scenario it is common to combine multiple classifiers to combat poor generalization performance from a single classifier. We propose an extensible deep learning based classification model framework that benefits from the simplicity and robustness of averaging ensembles and fusion based classifiers. We are also able to use metadata features and low-level feature engineering to boost classification performance. We show these improvements against robust industry standard baseline models that employ hyperparameter optimization.   Additionally, due to continuous insertion, deletion and updates to real-world high-volume e-commerce catalogs, assessing model performance for deployment using A/B testing and/or manual annotation becomes a bottleneck. To this end, we also propose a novel way to evaluate model performance using user sessions that provides better insights in addition to traditional measures of precision and recall.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们重新回到大规模电商目录中的产品项目分类问题上。电商目录的分类系统包含数千个分类，这些分类被商户上传的商品项目分配给了。商户将这些分类分配是经常错误的，但是这些分配被视为真实的标签，因此在自动生成的训练集中创建了一个反馈循环，导致模型质量变得更加低下。这个问题在没有大量精心编辑的训练集时特别突出来。在这种情况下，常见的方法是将多个分类器组合起来，以避免单个分类器的差异性。我们提出了一个可扩展的深度学习基于分类模型框架，该框架具有简单性和鲁棒性，可以使用averaging ensemble和 fusión基类ifiers。此外，我们还可以使用元数据特征和低级特征工程来提高分类性能。我们在使用超参优化的基础模型上显示了这些改进。此外，由于高量电商目录中的不断插入、删除和更新，使得在部署时使用A/B测试和/或手动标注来评估模型性能变得困难。为此，我们还提出了一种新的评估模型性能的方法，使用用户会话，该方法可以提供更好的投影，并且与传统的准确率和受损率相加。
</details></li>
</ul>
<hr>
<h2 id="Improving-Realistic-Worst-Case-Performance-of-NVCiM-DNN-Accelerators-through-Training-with-Right-Censored-Gaussian-Noise"><a href="#Improving-Realistic-Worst-Case-Performance-of-NVCiM-DNN-Accelerators-through-Training-with-Right-Censored-Gaussian-Noise" class="headerlink" title="Improving Realistic Worst-Case Performance of NVCiM DNN Accelerators through Training with Right-Censored Gaussian Noise"></a>Improving Realistic Worst-Case Performance of NVCiM DNN Accelerators through Training with Right-Censored Gaussian Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15853">http://arxiv.org/abs/2307.15853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyu Yan, Yifan Qin, Wujie Wen, Xiaobo Sharon Hu, Yiyu Shi</li>
<li>for: 提高深度神经网络（DNN）加速器的可靠性和稳定性，适用于安全关键应用场景如自动驾驶车辆。</li>
<li>methods: 利用k-th percentile性能（KPP）来捕捉DNN模型在 compute-in-Memory（CiM）加速器上的准确最差性能，并通过 formal 分析和噪声插入法来提高KPP。</li>
<li>results: 比起现有方法，提出一种自动确定插入右 censored Gaussian 噪声的方法，可以达到26%的KPP提高。<details>
<summary>Abstract</summary>
Compute-in-Memory (CiM), built upon non-volatile memory (NVM) devices, is promising for accelerating deep neural networks (DNNs) owing to its in-situ data processing capability and superior energy efficiency. Unfortunately, the well-trained model parameters, after being mapped to NVM devices, can often exhibit large deviations from their intended values due to device variations, resulting in notable performance degradation in these CiM-based DNN accelerators. There exists a long list of solutions to address this issue. However, they mainly focus on improving the mean performance of CiM DNN accelerators. How to guarantee the worst-case performance under the impact of device variations, which is crucial for many safety-critical applications such as self-driving cars, has been far less explored. In this work, we propose to use the k-th percentile performance (KPP) to capture the realistic worst-case performance of DNN models executing on CiM accelerators. Through a formal analysis of the properties of KPP and the noise injection-based DNN training, we demonstrate that injecting a novel right-censored Gaussian noise, as opposed to the conventional Gaussian noise, significantly improves the KPP of DNNs. We further propose an automated method to determine the optimal hyperparameters for injecting this right-censored Gaussian noise during the training process. Our method achieves up to a 26% improvement in KPP compared to the state-of-the-art methods employed to enhance DNN robustness under the impact of device variations.
</details>
<details>
<summary>摘要</summary>
计算在内存（CiM），基于不可塑性存储器（NVM）设备，对深度神经网络（DNN）进行加速，因其可在位置处理数据和能效率具有优势。然而，将训练好的模型参数映射到NVM设备后，由于设备变化而导致的巨大偏差，可能会导致DNN加速器的性能下降。当前的解决方法主要关注提高CiM DNN加速器的平均性能。然而，对于许多安全关键应用，如自动驾驶车辆，保证最坏情况性能是非常重要。在这种情况下，我们提出使用k-th percentile性能（KPP）来捕捉DNN模型在CiM加速器上的实际最坏情况性能。我们通过对KPP和噪声注入式DNN训练的形式分析，表明在插入右 censored Gaussian噪声时，DNN的KPP有显著改善。此外，我们还提出一种自动确定在训练过程中插入这种右 censored Gaussian噪声的优化参数的方法。我们的方法可以与现有的状态艺法相比，提高KPP达26%。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Algorithm-Portfolio-Evaluation-using-Item-Response-Theory"><a href="#Comprehensive-Algorithm-Portfolio-Evaluation-using-Item-Response-Theory" class="headerlink" title="Comprehensive Algorithm Portfolio Evaluation using Item Response Theory"></a>Comprehensive Algorithm Portfolio Evaluation using Item Response Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15850">http://arxiv.org/abs/2307.15850</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sevvandi/airt-scripts">https://github.com/sevvandi/airt-scripts</a></li>
<li>paper_authors: Sevvandi Kandanaarachchi, Kate Smith-Miles</li>
<li>for: 评估机器学习算法的表现 across a repository of datasets，同时描述算法的一般特征和异常性。</li>
<li>methods: 使用修改后的 Item Response Theory（IRT）模型，无需更多的数据特征计算，以获得更加具体的算法性能特征。</li>
<li>results: 在各种应用领域中测试了算法股投资，并证明了这种方法的广泛适用性和可解释性。<details>
<summary>Abstract</summary>
Item Response Theory (IRT) has been proposed within the field of Educational Psychometrics to assess student ability as well as test question difficulty and discrimination power. More recently, IRT has been applied to evaluate machine learning algorithm performance on a single classification dataset, where the student is now an algorithm, and the test question is an observation to be classified by the algorithm. In this paper we present a modified IRT-based framework for evaluating a portfolio of algorithms across a repository of datasets, while simultaneously eliciting a richer suite of characteristics - such as algorithm consistency and anomalousness - that describe important aspects of algorithm performance. These characteristics arise from a novel inversion and reinterpretation of the traditional IRT model without requiring additional dataset feature computations. We test this framework on algorithm portfolios for a wide range of applications, demonstrating the broad applicability of this method as an insightful algorithm evaluation tool. Furthermore, the explainable nature of IRT parameters yield an increased understanding of algorithm portfolios.
</details>
<details>
<summary>摘要</summary>
item response theory (IRT) 在教育心理测量领域提出来评估学生能力以及测试题目难度和抗択力。更加最近，IRT 被应用于评估单一分类 dataset 上机器学习算法的性能，其中学生现在是一个算法，测试题目是一个需要被分类的观察。在这篇文章中，我们提出了一种基于 IRT 的修改后的框架，用于评估一个数据库中的算法投资组，同时同时抽取一系列特征，例如算法一致性和异常性，这些特征描述了算法性能中重要的一些方面。这些特征来自于传统 IRT 模型的新的倒推和重新解释，不需要额外的数据特征计算。我们在各种应用领域测试了这种框架， demonstarting its 广泛适用性作为一种深入的算法评估工具。此外，可解释的 IRT 参数带来了对算法投资组的更好的理解。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Kernel-Estimation-With-Neutral-Atoms-For-Supervised-Classification-A-Gate-Based-Approach"><a href="#Quantum-Kernel-Estimation-With-Neutral-Atoms-For-Supervised-Classification-A-Gate-Based-Approach" class="headerlink" title="Quantum Kernel Estimation With Neutral Atoms For Supervised Classification: A Gate-Based Approach"></a>Quantum Kernel Estimation With Neutral Atoms For Supervised Classification: A Gate-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15840">http://arxiv.org/abs/2307.15840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Russo, Edoardo Giusto, Bartolomeo Montrucchio</li>
<li>for: 本文提出了一种基于量子计算机的kernel估计技术（量子卷积kernel估计，QKE），用于训练支持向量机（SVM）。由于实现特征映射需要大量的2本地运算，因此需要较高的量子比特数连接。而现代超导器device不可能实现这种连接。因此，本文使用中性原子量子计算机，因为它们允许更多的自由度。</li>
<li>methods: 本文提出了一种基于门odel的通用方法，包括1个和2个门的门。然后，通过实验计算kernel矩阵，从数据集中获得了高准确率。此外，本文还将这种过程推广到N个量子比特上，利用中性原子device的更 flexible的排列方式。</li>
<li>results: 本文的实验结果表明，使用中性原子量子计算机和基于门odel的方法可以实现高准确率的支持向量机训练，即使数据集小并且分离度低。这是首先提出了一种可以在中性原子device上实现通用 kernel估计的文献。<details>
<summary>Abstract</summary>
Quantum Kernel Estimation (QKE) is a technique based on leveraging a quantum computer to estimate a kernel function that is classically difficult to calculate, which is then used by a classical computer for training a Support Vector Machine (SVM). Given the high number of 2-local operators necessary for realizing a feature mapping hard to simulate classically, a high qubit connectivity is needed, which is not currently possible on superconducting devices. For this reason, neutral atom quantum computers can be used, since they allow to arrange the atoms with more freedom. Examples of neutral-atom-based QKE can be found in the literature, but they are focused on graph learning and use the analogue approach. In this paper, a general method based on the gate model is presented. After deriving 1-qubit and 2-qubit gates starting from laser pulses, a parameterized sequence for feature mapping on 3 qubits is realized. This sequence is then used to empirically compute the kernel matrix starting from a dataset, which is finally used to train the SVM. It is also shown that this process can be generalized up to N qubits taking advantage of the more flexible arrangement of atoms that this technology allows. The accuracy is shown to be high despite the small dataset and the low separation. This is the first paper that not only proposes an algorithm for explicitly deriving a universal set of gates but also presents a method of estimating quantum kernels on neutral atom devices for general problems using the gate model.
</details>
<details>
<summary>摘要</summary>
量子均衡估计（QKE）是一种基于使用量子计算机来估计类比Difficult to calculate classical kernel function，然后使用类型计算机进行培训支持向量机（SVM）的技术。由于实现特征映射所需的2本本操作数量很高，因此需要高 qubit 连接度，这 Currently not possible on superconducting devices. 因此，中性原子量子计算机可以使用，它们允许 atoms 的更多自由排序。文中提到了中性原子基于QKE的例子，但是它们主要关注于图学学习和使用分析方法。本文则提出了一种基于门模型的通用方法。通过从激光脉冲开始 derive 1 qubit 和 2 qubit 门，实现了基于3 qubits的特征映射序列。这个序列然后用来实际计算基于数据集的kernel矩阵，最后用来培训SVM。此外，文中还证明了这种过程可以扩展到N qubits，利用中性原子技术允许的更 flexible atoms 排序。具体来说，文中通过使用小型数据集和低分离度来证明这种方法的高准确率。这是第一篇不仅提出了一种算法来直接 derivation 一组 universal gates，而且还提出了使用门模型来估计中性原子设备上的量子kernels的方法。
</details></li>
</ul>
<hr>
<h2 id="Holistic-Survey-of-Privacy-and-Fairness-in-Machine-Learning"><a href="#Holistic-Survey-of-Privacy-and-Fairness-in-Machine-Learning" class="headerlink" title="Holistic Survey of Privacy and Fairness in Machine Learning"></a>Holistic Survey of Privacy and Fairness in Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15838">http://arxiv.org/abs/2307.15838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sina Shaham, Arash Hajisafi, Minh K Quan, Dinh C Nguyen, Bhaskar Krishnamachari, Charith Peris, Gabriel Ghinita, Cyrus Shahabi, Pubudu N. Pathirana</li>
<li>for: 本文旨在探讨负责任人工智能（AI）和可靠机器学习（ML）中的隐私和公平问题，以及这两个目标如何同时 integrate into ML 模型中。</li>
<li>methods: 本文通过对隐私和公平在 ML 中的研究，包括指导、不指导、半指导和奖励学习等多种方法，以及这些方法在应用领域的交互。</li>
<li>results: 本文结合了现有的研究成果，提出了隐私和公平在 ML 中的影响关系，以及如何同时实现这两个目标而减少功能损失。 However, the paper also identifies research challenges in achieving privacy and fairness concurrently in large language models.<details>
<summary>Abstract</summary>
Privacy and fairness are two crucial pillars of responsible Artificial Intelligence (AI) and trustworthy Machine Learning (ML). Each objective has been independently studied in the literature with the aim of reducing utility loss in achieving them. Despite the significant interest attracted from both academia and industry, there remains an immediate demand for more in-depth research to unravel how these two objectives can be simultaneously integrated into ML models. As opposed to well-accepted trade-offs, i.e., privacy-utility and fairness-utility, the interrelation between privacy and fairness is not well-understood. While some works suggest a trade-off between the two objective functions, there are others that demonstrate the alignment of these functions in certain scenarios. To fill this research gap, we provide a thorough review of privacy and fairness in ML, including supervised, unsupervised, semi-supervised, and reinforcement learning. After examining and consolidating the literature on both objectives, we present a holistic survey on the impact of privacy on fairness, the impact of fairness on privacy, existing architectures, their interaction in application domains, and algorithms that aim to achieve both objectives while minimizing the utility sacrificed. Finally, we identify research challenges in achieving privacy and fairness concurrently in ML, particularly focusing on large language models.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:Privacy and fairness are two crucial pillars of responsible Artificial Intelligence (AI) and trustworthy Machine Learning (ML). Each objective has been independently studied in the literature with the aim of reducing utility loss in achieving them. Despite the significant interest attracted from both academia and industry, there remains an immediate demand for more in-depth research to unravel how these two objectives can be simultaneously integrated into ML models. As opposed to well-accepted trade-offs, i.e., privacy-utility and fairness-utility, the interrelation between privacy and fairness is not well-understood. While some works suggest a trade-off between the two objective functions, there are others that demonstrate the alignment of these functions in certain scenarios. To fill this research gap, we provide a thorough review of privacy and fairness in ML, including supervised, unsupervised, semi-supervised, and reinforcement learning. After examining and consolidating the literature on both objectives, we present a holistic survey on the impact of privacy on fairness, the impact of fairness on privacy, existing architectures, their interaction in application domains, and algorithms that aim to achieve both objectives while minimizing the utility sacrificed. Finally, we identify research challenges in achieving privacy and fairness concurrently in ML, particularly focusing on large language models.Translation:<<SYS>>隐私和公正是负责任人工智能（AI）和可靠机器学习（ML）中的两个关键柱子。每个目标都已经独立地在文献中研究，以减少实现它们的利用损失。尽管学术界和industry都对这两个目标表示了极大的兴趣，但是还有一个立即需要更深入的研究，以了解这两个目标如何同时integrated into ML模型。与well-accepted的交易所不同，i.e., 隐私-实用和公正-实用，隐私和公正之间的关系还不够了解。一些工作表明了这两个目标函数之间的交易，而其他一些则表明了这两个目标函数在某些场景下的alignment。为了填补这个研究漏洞，我们提供了隐私和公正在ML中的经过系统性的综述，包括supervised, unsupervised, semi-supervised,和reinforcement learning。我们结合了文献中关于这两个目标的所有研究，并提供了一个总体的回顾，探讨了隐私对公正的影响，公正对隐私的影响，现有的架构，其交互在应用领域中，以及可以实现这两个目标的Algorithms，同时尽量减少实用损失。最后，我们确定了在ML中实现隐私和公正的研究挑战，特别是关注大型语言模型。
</details></li>
</ul>
<hr>
<h2 id="Mean-Estimation-with-User-level-Privacy-under-Data-Heterogeneity"><a href="#Mean-Estimation-with-User-level-Privacy-under-Data-Heterogeneity" class="headerlink" title="Mean Estimation with User-level Privacy under Data Heterogeneity"></a>Mean Estimation with User-level Privacy under Data Heterogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15835">http://arxiv.org/abs/2307.15835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rachel Cummings, Vitaly Feldman, Audra McMillan, Kunal Talwar</li>
<li>for:  Handle heterogeneous user data with different distribution and quantity of data while preserving user-level differential privacy.</li>
<li>methods:  Propose a simple model of heterogeneous user data and an estimator that achieves asymptotic optimality with proven lower bounds on error.</li>
<li>results:  Demonstrate the effectiveness of the proposed method through theoretical analysis and prove the asymptotic optimality and lower bounds on error.<details>
<summary>Abstract</summary>
A key challenge in many modern data analysis tasks is that user data are heterogeneous. Different users may possess vastly different numbers of data points. More importantly, it cannot be assumed that all users sample from the same underlying distribution. This is true, for example in language data, where different speech styles result in data heterogeneity. In this work we propose a simple model of heterogeneous user data that allows user data to differ in both distribution and quantity of data, and provide a method for estimating the population-level mean while preserving user-level differential privacy. We demonstrate asymptotic optimality of our estimator and also prove general lower bounds on the error achievable in the setting we introduce.
</details>
<details>
<summary>摘要</summary>
现代数据分析任务中一个关键挑战是用户数据异ogeneous。不同的用户可能拥有极其不同的数据点数。更重要的是，不能假设所有用户来自同一个下面分布。这是语言数据中的不同说话风格导致数据异ogeneous的一个例子。在这个工作中，我们提出了一种简单的异ogeneous用户数据模型，允许用户数据在分布和数据量方面不同，并提供了保持用户级别隐私的方法来估算人口级别的 mean。我们证明了我们的估计器在我们所引入的设定下是 asymptotic optimality 的，并且证明了该设定下的一般下界。
</details></li>
</ul>
<hr>
<h2 id="DeepTSF-Codeless-machine-learning-operations-for-time-series-forecasting"><a href="#DeepTSF-Codeless-machine-learning-operations-for-time-series-forecasting" class="headerlink" title="DeepTSF: Codeless machine learning operations for time series forecasting"></a>DeepTSF: Codeless machine learning operations for time series forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00709">http://arxiv.org/abs/2308.00709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sotiris Pelekis, Evangelos Karakolis, Theodosios Pountridis, George Kormpakis, George Lampropoulos, Spiros Mouzakits, Dimitris Askounis</li>
<li>for: 这篇论文旨在提供一个通用的机器学习操作（MLOps）框架，以创新时间序列预测（TS）领域。</li>
<li>methods: 这篇论文使用了深度学习（DL）和机器学习（ML）方法，并自动化了运算和模型化的过程，以提高资料科学家和机器学习工程师的生产力和效率。</li>
<li>results: 这篇论文在实际应用中已经证明了 DeepTSF 的有效性，并且在电力和能源系统领域中展示了它的重要加值。<details>
<summary>Abstract</summary>
This paper presents DeepTSF, a comprehensive machine learning operations (MLOps) framework aiming to innovate time series forecasting through workflow automation and codeless modeling. DeepTSF automates key aspects of the ML lifecycle, making it an ideal tool for data scientists and MLops engineers engaged in machine learning (ML) and deep learning (DL)-based forecasting. DeepTSF empowers users with a robust and user-friendly solution, while it is designed to seamlessly integrate with existing data analysis workflows, providing enhanced productivity and compatibility. The framework offers a front-end user interface (UI) suitable for data scientists, as well as other higher-level stakeholders, enabling comprehensive understanding through insightful visualizations and evaluation metrics. DeepTSF also prioritizes security through identity management and access authorization mechanisms. The application of DeepTSF in real-life use cases of the I-NERGY project has already proven DeepTSF's efficacy in DL-based load forecasting, showcasing its significant added value in the electrical power and energy systems domain.
</details>
<details>
<summary>摘要</summary>
DeepTSF provides a robust and user-friendly solution that seamlessly integrates with existing data analysis workflows, enhancing productivity and compatibility. The framework offers a front-end user interface (UI) suitable for data scientists and other higher-level stakeholders, providing insightful visualizations and evaluation metrics for comprehensive understanding.In addition, DeepTSF prioritizes security through identity management and access authorization mechanisms. The application of DeepTSF in real-life use cases of the I-NERGY project has already demonstrated its efficacy in DL-based load forecasting, showcasing its significant added value in the electrical power and energy systems domain.
</details></li>
</ul>
<hr>
<h2 id="A-Distance-Correlation-Based-Approach-to-Characterize-the-Effectiveness-of-Recurrent-Neural-Networks-for-Time-Series-Forecasting"><a href="#A-Distance-Correlation-Based-Approach-to-Characterize-the-Effectiveness-of-Recurrent-Neural-Networks-for-Time-Series-Forecasting" class="headerlink" title="A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecasting"></a>A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15830">http://arxiv.org/abs/2307.15830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Salazar, Ashis G. Banerjee</li>
<li>for: 这个论文主要针对时间序列预测问题，尤其是使用循环神经网络（RNN）模型来解决这个问题。</li>
<li>methods: 该论文使用距离相关度指标来链接时间序列特征和RNN活动层的组件，以便解释和解释RNN的性能。</li>
<li>results: 研究发现，RNN活动层可以良好地学习时间序列的延迟结构，但是随着层数的增加，这些信息会逐渐丢失，导致时间序列预测质量下降。此外，活动层也无法完善地模拟平均移动和不均等时间序列过程。<details>
<summary>Abstract</summary>
Time series forecasting has received a lot of attention with recurrent neural networks (RNNs) being one of the widely used models due to their ability to handle sequential data. Prior studies of RNNs for time series forecasting yield inconsistent results with limited insights as to why the performance varies for different datasets. In this paper, we provide an approach to link the characteristics of time series with the components of RNNs via the versatile metric of distance correlation. This metric allows us to examine the information flow through the RNN activation layers to be able to interpret and explain their performance. We empirically show that the RNN activation layers learn the lag structures of time series well. However, they gradually lose this information over a span of a few consecutive layers, thereby worsening the forecast quality for series with large lag structures. We also show that the activation layers cannot adequately model moving average and heteroskedastic time series processes. Last, we generate heatmaps for visual comparisons of the activation layers for different choices of the network hyperparameters to identify which of them affect the forecast performance. Our findings can, therefore, aid practitioners in assessing the effectiveness of RNNs for given time series data without actually training and evaluating the networks.
</details>
<details>
<summary>摘要</summary>
时间序列预测已经受到了很多关注，回归神经网络（RNN）是一种广泛使用的模型，因为它们可以处理序列数据。先前的研究表明，RNN在不同的数据集上的性能异常各异，具体原因不够清晰。在这篇论文中，我们提出了一种方法，通过距离相关度的灵活度量来连接时间序列的特征和RNN的组件。这种度量允许我们检查RNN活动层中信息的流动，以便解释和解释它们的性能。我们实际证明了，RNN活动层可以良好地学习时间序列的延迟结构。然而，它们随着连续层的数量增加，慢慢地失去这些信息，从而使时间序列预测质量下降。此外，我们还证明了，活动层不能够合理地模型平均和不均时间序列过程。最后，我们生成了不同网络参数选择的热图，以便比较活动层的效果。我们的发现可以帮助实践者评估RNN在给定时间序列数据上的效果，而不需要实际训练和评估网络。
</details></li>
</ul>
<hr>
<h2 id="RT-2-Vision-Language-Action-Models-Transfer-Web-Knowledge-to-Robotic-Control"><a href="#RT-2-Vision-Language-Action-Models-Transfer-Web-Knowledge-to-Robotic-Control" class="headerlink" title="RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"></a>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15818">http://arxiv.org/abs/2307.15818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich</li>
<li>for: 这个论文的目标是把视力语言模型直接 integrate into end-to-end robotic control，以提高总结和允许emergent semantic reasoning。</li>
<li>methods: 作者提出了一种简单的、通用的方法，即将 robotic actions 表示为文本token，并将其直接 incorporated into the training set of the model。</li>
<li>results: 作者的方法导致了高性能的 робо控制策略，并允许模型获得了一系列的emergent capabilities，如对新物体的总结、理解不在机器人培训数据中的命令（如将物体放置在特定的数字或图标上）、以及对用户命令的简单逻辑处理（如选择最小或最大的物体、或者最近的物体）。<details>
<summary>Abstract</summary>
We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).
</details>
<details>
<summary>摘要</summary>
我们研究如何将互联网规模数据上训练的视力语言模型直接应用到终端控制中，以提高泛化和启动Semantic Reasoning。我们的目标是将单一的终端训练模型，能够将机器人观察到动作映射到动作，并且从互联网的视力语言数据获益。为了实现这一目标，我们提出了一个简单的普遍方法：将机器人动作表示为文本 токен，并将它们直接添加到模型的训练集中，与自然语言 токен一样。我们称这种类型的模型为视力语言动作模型（VLA），并实现了一个简单的示例，即 RT-2。我们的广泛评估（6000次评估）表明，我们的方法将带来高效的机器人政策，并允许 RT-2 获得互联网训练中的许多类型的能力。这包括对新物品的泛化、对机器人训练数据中没有的命令（如将物品放在特定数字或图示上）的理解，以及对使用者命令进行基本的推理（如选择最小或最大的物品，或者最近的物品）。我们进一步显示，将链接思维理论添加到 VLA 中，允许它执行多阶层Semantic Reasoning，例如选择哪样的物品用作做扩展钻（一个岩石），或者选择哪种饮料适合疲劳的人（一种能量饮料）。
</details></li>
</ul>
<hr>
<h2 id="Multi-growth-stage-plant-recognition-a-case-study-of-Palmer-amaranth-Amaranthus-palmeri-in-cotton-Gossypium-hirsutum"><a href="#Multi-growth-stage-plant-recognition-a-case-study-of-Palmer-amaranth-Amaranthus-palmeri-in-cotton-Gossypium-hirsutum" class="headerlink" title="Multi-growth stage plant recognition: a case study of Palmer amaranth (Amaranthus palmeri) in cotton (Gossypium hirsutum)"></a>Multi-growth stage plant recognition: a case study of Palmer amaranth (Amaranthus palmeri) in cotton (Gossypium hirsutum)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15816">http://arxiv.org/abs/2307.15816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy RY Coleman, Matthew Kutugata, Michael J Walsh, Muthukumar Bagavathiannan</li>
<li>for: 这个论文旨在测试不同版本的YOLO框架在识别不同生长阶段的amaranthus palmeri中的性能。</li>
<li>methods: 该论文使用了YOLO框架的26种不同变体，并对其进行了测试和比较，以评估它们在识别不同生长阶段的表现。</li>
<li>results: 研究发现，使用最新版本的YOLO框架（v8）可以达到47.34%的识别精度，而将所有生长阶段 grouped为一个类型可以提高性能，最高的mean average precision（mAP）为67.05%。此外，使用不同的分割方法和权重也可以提高模型的性能。<details>
<summary>Abstract</summary>
Many advanced, image-based precision agricultural technologies for plant breeding, field crop research, and site-specific crop management hinge on the reliable detection and phenotyping of plants across highly variable morphological growth stages. Convolutional neural networks (CNNs) have shown promise for image-based plant phenotyping and weed recognition, but their ability to recognize growth stages, often with stark differences in appearance, is uncertain. Amaranthus palmeri (Palmer amaranth) is a particularly challenging weed plant in cotton (Gossypium hirsutum) production, exhibiting highly variable plant morphology both across growth stages over a growing season, as well as between plants at a given growth stage due to high genetic diversity. In this paper, we investigate eight-class growth stage recognition of A. palmeri in cotton as a challenging model for You Only Look Once (YOLO) architectures. We compare 26 different architecture variants from YOLO v3, v5, v6, v6 3.0, v7, and v8 on an eight-class growth stage dataset of A. palmeri. The highest mAP@[0.5:0.95] for recognition of all growth stage classes was 47.34% achieved by v8-X, with inter-class confusion across visually similar growth stages. With all growth stages grouped as a single class, performance increased, with a maximum mean average precision (mAP@[0.5:0.95]) of 67.05% achieved by v7-Original. Single class recall of up to 81.42% was achieved by v5-X, and precision of up to 89.72% was achieved by v8-X. Class activation maps (CAM) were used to understand model attention on the complex dataset. Fewer classes, grouped by visual or size features improved performance over the ground-truth eight-class dataset. Successful growth stage detection highlights the substantial opportunity for improving plant phenotyping and weed recognition technologies with open-source object detection architectures.
</details>
<details>
<summary>摘要</summary>
多种高级图像基于精准农业技术，如植物选择、田间考核和场景特定作物管理，都需要可靠地检测和phenotyping植物。 convolutional neural networks (CNNs) 已经在图像基于植物phenotyping和苔藿识别中展示了抢夺性，但它们在不同生长阶段之间的形态差异 recognition 的能力尚未得到证明。 Amaranthus palmeri（Palmer amaranth）是在棉花（Gossypium hirsutum）生产中 particualrly 挑战人工智能，它的植物形态具有高度变化和 между植物之间的高遗传多样性。 在这篇论文中，我们Investigate  Amaranthus palmeri 在棉花中的八个生长阶段识别，作为YOLO 架构的挑战模型。我们对 YOLO v3、v5、v6、v6 3.0、v7 和 v8 中的26种不同架构variant进行比较，并获得了最高的mAP@[0.5:0.95] 值为47.34%，由 v8-X 实现。在所有生长阶段被 grouped 为一个单一类时，性能提高，最高的mAP@[0.5:0.95] 值为67.05%，由 v7-Original 实现。单个类回归率可达81.42%，由 v5-X 实现，而特征精度可达89.72%，由 v8-X 实现。通过使用类活动图（CAM）来理解模型在复杂数据集上的注意力。 fewer classes， grouped by visual or size features 可以提高性能。成功的生长阶段检测表明了开源物体检测架构在植物phenotyping和苔藿识别技术中的潜在潜力。
</details></li>
</ul>
<hr>
<h2 id="Anomaly-Detection-in-Industrial-Machinery-using-IoT-Devices-and-Machine-Learning-a-Systematic-Mapping"><a href="#Anomaly-Detection-in-Industrial-Machinery-using-IoT-Devices-and-Machine-Learning-a-Systematic-Mapping" class="headerlink" title="Anomaly Detection in Industrial Machinery using IoT Devices and Machine Learning: a Systematic Mapping"></a>Anomaly Detection in Industrial Machinery using IoT Devices and Machine Learning: a Systematic Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15807">http://arxiv.org/abs/2307.15807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sérgio F. Chevtchenko, Elisson da Silva Rocha, Monalisa Cristina Moura Dos Santos, Ricardo Lins Mota, Diego Moura Vieira, Ermeson Carneiro de Andrade, Danilo Ricardo Barbosa de Araújo</li>
<li>For: This paper is written for researchers and practitioners who are interested in Anomaly Detection for industrial machinery using IoT devices and ML algorithms.* Methods: The paper uses a systematic mapping study to evaluate 84 relevant studies from 2016 to 2023, providing an extensive review of Anomaly Detection research in industrial machinery. The study covers the most commonly used algorithms, preprocessing techniques, and sensor types.* Results: The paper identifies the application areas and points to future challenges and research opportunities in Anomaly Detection for industrial machinery using IoT devices and ML algorithms.Here is the same information in Simplified Chinese text:* For: 这篇论文是为研究者和实践者而写的，他们关心工业机器使用互联网物联网设备和机器学习算法进行异常检测。* Methods: 这篇论文使用系统性的映射研究来评估84篇相关研究，从2016年到2023年，提供了工业机器异常检测研究的广泛审视。研究涵盖了最常用的算法、预处理技术和传感器类型。* Results: 这篇论文 indentifies了应用领域和未来挑战和研究机会在工业机器使用互联网物联网设备和机器学习算法进行异常检测。<details>
<summary>Abstract</summary>
Anomaly detection is critical in the smart industry for preventing equipment failure, reducing downtime, and improving safety. Internet of Things (IoT) has enabled the collection of large volumes of data from industrial machinery, providing a rich source of information for Anomaly Detection. However, the volume and complexity of data generated by the Internet of Things ecosystems make it difficult for humans to detect anomalies manually. Machine learning (ML) algorithms can automate anomaly detection in industrial machinery by analyzing generated data. Besides, each technique has specific strengths and weaknesses based on the data nature and its corresponding systems. However, the current systematic mapping studies on Anomaly Detection primarily focus on addressing network and cybersecurity-related problems, with limited attention given to the industrial sector. Additionally, these studies do not cover the challenges involved in using ML for Anomaly Detection in industrial machinery within the context of the IoT ecosystems. This paper presents a systematic mapping study on Anomaly Detection for industrial machinery using IoT devices and ML algorithms to address this gap. The study comprehensively evaluates 84 relevant studies spanning from 2016 to 2023, providing an extensive review of Anomaly Detection research. Our findings identify the most commonly used algorithms, preprocessing techniques, and sensor types. Additionally, this review identifies application areas and points to future challenges and research opportunities.
</details>
<details>
<summary>摘要</summary>
“异常探测是智能产业中的关键任务，可以预防设备故障、减少停机时间和提高安全性。互联网物件（IoT）已经允许了对工业机械的大量数据收集，提供了丰富的数据来源供异常探测。然而，由于互联网物件生态系统所生成的数据量和复杂度，使得人类手动探测异常具有困难。机器学习（ML）算法可以自动探测工业机械中的异常，通过分析生成的数据。然而，目前的系统性映射研究主要集中在网络和预防网络攻击等方面，对于工业 сектору的关注相对较少。此外，这些研究并未考虑使用ML探测工业机械中的异常在互联网物件生态系统中的挑战。本文提出了一个系统性映射研究，涵盖2016年至2023年间84份相关的研究，提供了广泛的异常探测研究评估。我们的发现显示了最常使用的算法、处理前置技术和感应器类型。此外，这个评估还点出了应用领域和未来挑战和研究机会。”
</details></li>
</ul>
<hr>
<h2 id="On-Single-Index-Models-beyond-Gaussian-Data"><a href="#On-Single-Index-Models-beyond-Gaussian-Data" class="headerlink" title="On Single Index Models beyond Gaussian Data"></a>On Single Index Models beyond Gaussian Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15804">http://arxiv.org/abs/2307.15804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joan Bruna, Loucas Pillaud-Vivien, Aaron Zweig</li>
<li>for: 本文研究了在非标准分布 Setting下，使用批处理梯度下降法（SGD）来修复潜在的隐藏向量 $\theta^*$。</li>
<li>methods: 本文基于 \cite{arous2020online} 的框架，并对非标准分布 Setting进行扩展。</li>
<li>results: 本文的主要结果表明，在高维度 Setting下，SGD 可以有效地修复隐藏向量 $\theta^*$，提供了对previous works \cite{yehudai2020learning,wu2022learning} 的扩展。<details>
<summary>Abstract</summary>
Sparse high-dimensional functions have arisen as a rich framework to study the behavior of gradient-descent methods using shallow neural networks, showcasing their ability to perform feature learning beyond linear models. Amongst those functions, the simplest are single-index models $f(x) = \phi( x \cdot \theta^*)$, where the labels are generated by an arbitrary non-linear scalar link function $\phi$ applied to an unknown one-dimensional projection $\theta^*$ of the input data. By focusing on Gaussian data, several recent works have built a remarkable picture, where the so-called information exponent (related to the regularity of the link function) controls the required sample complexity. In essence, these tools exploit the stability and spherical symmetry of Gaussian distributions. In this work, building from the framework of \cite{arous2020online}, we explore extensions of this picture beyond the Gaussian setting, where both stability or symmetry might be violated. Focusing on the planted setting where $\phi$ is known, our main results establish that Stochastic Gradient Descent can efficiently recover the unknown direction $\theta^*$ in the high-dimensional regime, under assumptions that extend previous works ~\cite{yehudai2020learning,wu2022learning}.
</details>
<details>
<summary>摘要</summary>
稀疏高维函数已成为研究梯度下降方法使用浅层神经网络的理想平台，展示它们在超 linear 模型中进行特征学习。这些函数中最简单的是单指数模型 $f(x) = \phi(x \cdot \theta^*)$，其中标签由一个未知的一维投影 $\theta^*$ 和一个任意非线性的链接函数 $\phi$ 生成。通过对 Gaussian 数据进行研究，一些最近的工作已经构建了一幅很出色的图像，其中信息指数（相关于链接函数的正则性）控制了样本复杂性。这些工具利用了 Gaussian 分布的稳定性和圆涂函数的圆涂性。在本文中，我们基于 \cite{arous2020online} 的框架，探讨这个图像在非 Gaussian 设置下的扩展。我们主要研究在植入 Setting 下，其中 $\phi$ 是已知的，Stochastic Gradient Descent 能够高维化 Recover 未知方向 $\theta^*$，并且我们的主要结果表明，在一些适用于前工作 \cite{yehudai2020learning,wu2022learning} 的假设下，Stochastic Gradient Descent 可以高效地进行梯度下降。
</details></li>
</ul>
<hr>
<h2 id="SAFE-Saliency-Aware-Counterfactual-Explanations-for-DNN-based-Automated-Driving-Systems"><a href="#SAFE-Saliency-Aware-Counterfactual-Explanations-for-DNN-based-Automated-Driving-Systems" class="headerlink" title="SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems"></a>SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15786">http://arxiv.org/abs/2307.15786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Samadi, Amir Shirian, Konstantinos Koufos, Kurt Debattista, Mehrdad Dianati</li>
<li>for: 本研究的目的是提出一种新的CF解释方法，即使用saliency map来生成更有用的CF解释。</li>
<li>methods: 本研究使用了现有的深度生成CF模型，并提出了一种基于saliency map的CF解释方法，该方法可以更好地考虑黑盒模型的权重分布。</li>
<li>results: 研究发现，使用saliency map可以生成更有用的CF解释，并且可以更好地考虑黑盒模型的权重分布。此外，研究还发现了一些相关的CF特征，可以用于更好地理解黑盒模型的决策过程。<details>
<summary>Abstract</summary>
A CF explainer identifies the minimum modifications in the input that would alter the model's output to its complement. In other words, a CF explainer computes the minimum modifications required to cross the model's decision boundary. Current deep generative CF models often work with user-selected features rather than focusing on the discriminative features of the black-box model. Consequently, such CF examples may not necessarily lie near the decision boundary, thereby contradicting the definition of CFs. To address this issue, we propose in this paper a novel approach that leverages saliency maps to generate more informative CF explanations. Source codes are available at: https://github.com/Amir-Samadi//Saliency_Aware_CF.
</details>
<details>
<summary>摘要</summary>
一种 CF 解释器可以确定输入中最小的修改，使模型的输出变为其补做。即使是深度生成的 CF 模型通常使用用户选择的特征而不是黑盒模型的激发特征，因此 CF 示例可能不会位于决策边界附近，从而违反 CF 的定义。为解决这个问题，我们在这篇论文中提出了一种新的方法，利用saliency map生成更有用的 CF 解释。代码可以在 GitHub 上找到：https://github.com/Amir-Samadi//Saliency_Aware_CF.
</details></li>
</ul>
<hr>
<h2 id="Spherical-and-Hyperbolic-Toric-Topology-Based-Codes-On-Graph-Embedding-for-Ising-MRF-Models-Classical-and-Quantum-Topology-Machine-Learning"><a href="#Spherical-and-Hyperbolic-Toric-Topology-Based-Codes-On-Graph-Embedding-for-Ising-MRF-Models-Classical-and-Quantum-Topology-Machine-Learning" class="headerlink" title="Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning"></a>Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15778">http://arxiv.org/abs/2307.15778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Lcrypto/Topology-Signal-Processing">https://github.com/Lcrypto/Topology-Signal-Processing</a></li>
<li>paper_authors: Vasiliy Usatyuk, Sergey Egorov, Denis Sapozhnikov</li>
<li>for: 本研究探讨了用信息 геометрия来描述爱丁顿模型的基态。</li>
<li>methods: 该方法利用了矩阵检查法和自动同构的概念，并与机器学习和错误检查编码之间的联系。</li>
<li>results: 该研究发现了一种将深度神经网络架构与错误检查编码相关的方法，并提出了一种基于捕获集的嵌入和稀疏分解方法。此外，研究还发现了一种将量子近似优化算法与深度神经网络架构相关的方法。<details>
<summary>Abstract</summary>
The paper introduces the application of information geometry to describe the ground states of Ising models. This is achieved by utilizing parity-check matrices of cyclic and quasi-cyclic codes on toric and spherical topologies. The approach establishes a connection between machine learning and error-correcting coding, specifically in terms of automorphism and the size of the circulant of the quasi-cyclic code. This proposed approach has implications for the development of new embedding methods based on trapping sets. Statistical physics and number geometry are utilized to optimize error-correcting codes, leading to these embedding and sparse factorization methods. The paper establishes a direct connection between DNN architecture and error-correcting coding by demonstrating how state-of-the-art DNN architectures (ChordMixer, Mega, Mega-chunk, CDIL, ...) from the long-range arena can be equivalent to specific types (Cage-graph, Repeat Accumulate) of block and convolutional LDPC codes. QC codes correspond to certain types of chemical elements, with the carbon element being represented by the mixed automorphism Shu-Lin-Fossorier QC-LDPC code. The Quantum Approximate Optimization Algorithm (QAOA) used in the Sherrington-Kirkpatrick Ising model can be seen as analogous to the back-propagation loss function landscape in training DNNs. This similarity creates a comparable problem with TS pseudo-codeword, resembling the belief propagation method. Additionally, the layer depth in QAOA correlates to the number of decoding belief propagation iterations in the Wiberg decoding tree. Overall, this work has the potential to advance multiple fields, from Information Theory, DNN architecture design (sparse and structured prior graph topology), efficient hardware design for Quantum and Classical DPU/TPU (graph, quantize and shift register architect.) to Materials Science and beyond.
</details>
<details>
<summary>摘要</summary>
文章介绍了使用信息 геометрии来描述铁模型的基态。这是通过利用cyclic和quasi-cyclic codes的parity-check矩阵在toric和spherical topologies上进行实现的。该方法确立了机器学习和错误修正编码之间的连接，特别是自动orf和 quasi-cyclic code的 circulant 的大小。这个提议的方法可以用于开发新的嵌入方法，基于拦束集。物理统计学和数字几何被用来优化错误修正编码，导致这些嵌入和稀疏因子化方法。文章显示了如何将state-of-the-art DNN架构（ChordMixer、Mega、Mega-chunk、CDIL等）与error-correcting coding相关联，并证明了这些架构可以被视为特定类型（Cage-graph、Repeat Accumulate）的块和 convolutional LDPC codes。QC codes与某些类型的化学元素相对应，如碳元素被表示为Shu-Lin-Fossorier QC-LDPC code的混合自动orf。Quantum Approximate Optimization Algorithm（QAOA）在Sherrington-Kirkpatrick Ising模型中可以被视为back-propagation loss function landscape在训练DNN时的相似。这种相似性创造了一个相似的问题，与TS pseudo-codeword相似，类似于 belief propagation 方法。此外，QAOA层深度与 belief propagation 迭代数相关。总之，这种工作有可能推动多个领域的进步，从信息理论、DNN架构设计（稀疏和结构化图前 topology）、高效的古典和量子 DPU/TPU 设计（图、量化和移动register 架构）到材料科学和更远的领域。
</details></li>
</ul>
<hr>
<h2 id="Seeking-the-Yield-Barrier-High-Dimensional-SRAM-Evaluation-Through-Optimal-Manifold"><a href="#Seeking-the-Yield-Barrier-High-Dimensional-SRAM-Evaluation-Through-Optimal-Manifold" class="headerlink" title="Seeking the Yield Barrier: High-Dimensional SRAM Evaluation Through Optimal Manifold"></a>Seeking the Yield Barrier: High-Dimensional SRAM Evaluation Through Optimal Manifold</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15773">http://arxiv.org/abs/2307.15773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanfang Liu, Guohao Dai, Wei W. Xing</li>
<li>for: 该研究目标是提高高级规模的SRAM组件失败概率估计的效率和准确性。</li>
<li>methods: 该研究基于经典的 нор minimization 方法，并将其扩展到无穷Components和得到新的优化 manifold 概念，这个概念连接了代理基本和重要抽样（IS）估计方法。然后，提出了一种不良边缘aware的落囊采样方法，并使用神经 Coupling 流（可以学习从样本如surrogate模型）作为 IS 提案分布。</li>
<li>results: 该研究结果显示，OPTIMIS 方法可以具有与 SOTA 方法相同的性能和稳定性，同时具有更高的效率和准确性。在高维度 SRAM 评估中，OPTIMIS 方法可以提高效率达 3.5倍，并提高准确性达 3倍。<details>
<summary>Abstract</summary>
Being able to efficiently obtain an accurate estimate of the failure probability of SRAM components has become a central issue as model circuits shrink their scale to submicrometer with advanced technology nodes. In this work, we revisit the classic norm minimization method. We then generalize it with infinite components and derive the novel optimal manifold concept, which bridges the surrogate-based and importance sampling (IS) yield estimation methods. We then derive a sub-optimal manifold, optimal hypersphere, which leads to an efficient sampling method being aware of the failure boundary called onion sampling. Finally, we use a neural coupling flow (which learns from samples like a surrogate model) as the IS proposal distribution. These combinations give rise to a novel yield estimation method, named Optimal Manifold Important Sampling (OPTIMIS), which keeps the advantages of the surrogate and IS methods to deliver state-of-the-art performance with robustness and consistency, with up to 3.5x in efficiency and 3x in accuracy over the best of SOTA methods in High-dimensional SRAM evaluation.
</details>
<details>
<summary>摘要</summary>
“能够效率地获得SRAM ком ponent的失败概率估计已成为技术迁移到 submicrometer 级别的 central issue。在这种工作中，我们回到了经典的 нор minimization 方法。然后，我们推广它到无限组件，并 derive novel optimal manifold 概念，该概念将 surrogate-based 和 importance sampling （IS） yield estimation 方法相连接。然后，我们 deriv sub-optimal manifold，optimal hypersphere，这导致了一种高效的抽样方法，意识到失败边界called onion sampling。最后，我们使用 neural coupling flow（学习从样本如 surrogate model）作为 IS 提案分布。这些组合在OPTIMIS 方法中，可以带来一种 novel yield estimation 方法，具有 surrogate 和 IS 方法的优点，可以提供 state-of-the-art 性能，同时具有 robustness 和一致性，高效率和高准确率，相比 SOTA 方法，可以提高到 3.5x 的效率和 3x 的准确率。”
</details></li>
</ul>
<hr>
<h2 id="Weighted-variation-spaces-and-approximation-by-shallow-ReLU-networks"><a href="#Weighted-variation-spaces-and-approximation-by-shallow-ReLU-networks" class="headerlink" title="Weighted variation spaces and approximation by shallow ReLU networks"></a>Weighted variation spaces and approximation by shallow ReLU networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15772">http://arxiv.org/abs/2307.15772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronald DeVore, Robert D. Nowak, Rahul Parhi, Jonathan W. Siegel</li>
<li>for: 本研究探讨了使用单层ReLU神经网络 approximate 函数 $f$ 在固定域 $\Omega$ 的表示方法。</li>
<li>methods: 本研究使用了单层ReLU神经网络来approximate 函数 $f$ 的表示方法。</li>
<li>results: 研究发现，使用这种方法可以在固定域 $\Omega$ 上获得更高精度的函数表示，并且这些表示的精度不受维度的影响。<details>
<summary>Abstract</summary>
We investigate the approximation of functions $f$ on a bounded domain $\Omega\subset \mathbb{R}^d$ by the outputs of single-hidden-layer ReLU neural networks of width $n$. This form of nonlinear $n$-term dictionary approximation has been intensely studied since it is the simplest case of neural network approximation (NNA). There are several celebrated approximation results for this form of NNA that introduce novel model classes of functions on $\Omega$ whose approximation rates avoid the curse of dimensionality. These novel classes include Barron classes, and classes based on sparsity or variation such as the Radon-domain BV classes.   The present paper is concerned with the definition of these novel model classes on domains $\Omega$. The current definition of these model classes does not depend on the domain $\Omega$. A new and more proper definition of model classes on domains is given by introducing the concept of weighted variation spaces. These new model classes are intrinsic to the domain itself. The importance of these new model classes is that they are strictly larger than the classical (domain-independent) classes. Yet, it is shown that they maintain the same NNA rates.
</details>
<details>
<summary>摘要</summary>
我们研究函数 $f$ 在受限区域 $\Omega$ 上的近似方法，使用单层ReLU神经网络的输出。这种单一神经网络近似方法已经广泛研究，因为它是最简单的神经网络近似方法（NNA）的案例。有很多著名的近似结果，包括Barron类和基于稀疏或变化的类，如Radon域BV类。本文关注这些新的模型类在领域 $\Omega$ 上的定义。现有的定义不依赖于领域 $\Omega$。我们引入权重变化空间的概念，并提出一种新的、更加适当的模型类定义。这些新的模型类与传统的领域独立的类相比，更加强大，但它们保持了同样的NNA率。
</details></li>
</ul>
<hr>
<h2 id="The-Hydra-Effect-Emergent-Self-repair-in-Language-Model-Computations"><a href="#The-Hydra-Effect-Emergent-Self-repair-in-Language-Model-Computations" class="headerlink" title="The Hydra Effect: Emergent Self-repair in Language Model Computations"></a>The Hydra Effect: Emergent Self-repair in Language Model Computations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15771">http://arxiv.org/abs/2307.15771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, Shane Legg</li>
<li>for: 本研究使用 causal 分析探讨语言模型计算的内部结构。</li>
<li>methods: 研究使用了ablation studying和counterfactual reasoning来探讨语言模型层次结构的作用。</li>
<li>results: 研究发现，语言模型层次结构具有adaptive computation和counterbalancing功能，即一种叫做“响应层补做”的现象，以及一种叫做“较量层下降”的现象。这些效果存在于不含dropout的语言模型中，并且层次结构相对较为松散。这些结果对于语言模型的审计和归因具有重要意义。<details>
<summary>Abstract</summary>
We investigate the internal structure of language model computations using causal analysis and demonstrate two motifs: (1) a form of adaptive computation where ablations of one attention layer of a language model cause another layer to compensate (which we term the Hydra effect) and (2) a counterbalancing function of late MLP layers that act to downregulate the maximum-likelihood token. Our ablation studies demonstrate that language model layers are typically relatively loosely coupled (ablations to one layer only affect a small number of downstream layers). Surprisingly, these effects occur even in language models trained without any form of dropout. We analyse these effects in the context of factual recall and consider their implications for circuit-level attribution in language models.
</details>
<details>
<summary>摘要</summary>
我们使用 causal 分析 investigate 语言模型计算的内部结构，并发现了两种模式：（1）一种适应计算，其中剪除一层注意力层会导致另一层补偿（我们称之为“哈迪拉效应”），以及（2）一种延迟多层扩散（MLP）层的抵消功能，它会降低最大可能性的 Token。我们的剪除研究表明，语言模型层通常是相对松散 Coupled（剪除一层只会影响一小部分下游层）。奇怪的是，这些效果会在没有任何dropout的情况下出现。我们在事实记忆中分析这些效果，并考虑它们对语言模型的征ircuit-level 归因的影响。
</details></li>
</ul>
<hr>
<h2 id="Goodness-of-Fit-of-Attributed-Probabilistic-Graph-Generative-Models"><a href="#Goodness-of-Fit-of-Attributed-Probabilistic-Graph-Generative-Models" class="headerlink" title="Goodness-of-Fit of Attributed Probabilistic Graph Generative Models"></a>Goodness-of-Fit of Attributed Probabilistic Graph Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03773">http://arxiv.org/abs/2308.03773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Robles-Granda, Katherine Tsai, Oluwasanmi Koyejo</li>
<li>for: 这篇论文主要用于描述如何评估Random Attributed Graph模型的合适性。</li>
<li>methods: 该论文使用了 Mean Square Contingency Coefficient 来评估模型的合适性，并提供了一种验证过程来确保模型的结构具有最低的偏差。</li>
<li>results: 该论文通过应用这些标准来验证各种流行的图模型的表示能力。<details>
<summary>Abstract</summary>
Probabilistic generative models of graphs are important tools that enable representation and sampling. Many recent works have created probabilistic models of graphs that are capable of representing not only entity interactions but also their attributes. However, given a generative model of random attributed graph(s), the general conditions that establish goodness of fit are not clear a-priori. In this paper, we define goodness of fit in terms of the mean square contingency coefficient for random binary networks. For this statistic, we outline a procedure for assessing the quality of the structure of a learned attributed graph by ensuring that the discrepancy of the mean square contingency coefficient (constant, or random) is minimal with high probability. We apply these criteria to verify the representation capability of a probabilistic generative model for various popular types of graph models.
</details>
<details>
<summary>摘要</summary>
probabilistic生成模型是重要工具，它们可以 representation和采样。在最近的许多研究中，人们已经创建了可以表示不只是实体交互，还有属性的 probabilistic模型。然而，给定一个生成模型的random attributed graph，通用的goodness of fit的条件并不明确。在这篇论文中，我们定义goodness of fit为random binary network的mean square contingency coefficient的 Statistics。我们详细说明了验证学习 attributed graph的结构质量的方法，确保discrepancy的mean square contingency coefficient（随机或常数）是最小的，并且具有高概率。我们应用这些标准来验证不同类型的图模型的表示能力。
</details></li>
</ul>
<hr>
<h2 id="Resume-Evaluation-through-Latent-Dirichlet-Allocation-and-Natural-Language-Processing-for-Effective-Candidate-Selection"><a href="#Resume-Evaluation-through-Latent-Dirichlet-Allocation-and-Natural-Language-Processing-for-Effective-Candidate-Selection" class="headerlink" title="Resume Evaluation through Latent Dirichlet Allocation and Natural Language Processing for Effective Candidate Selection"></a>Resume Evaluation through Latent Dirichlet Allocation and Natural Language Processing for Effective Candidate Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15752">http://arxiv.org/abs/2307.15752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vidhita Jagwani, Smit Meghani, Krishna Pai, Sudhir Dhage</li>
<li>for: 这 paper 是为了提出一种基于 Latent Dirichlet Allocation (LDA) 和 SpaCy 实体检测的简历评分方法。</li>
<li>methods: 该方法首先使用 SpaCy 的Named Entity Recognition (NER) 提取简历中的相关实体，例如教育、工作经验和技能。然后，LDA 模型使用这些实体对简历进行评分，并将每个实体分配一个主题概率。</li>
<li>results: 我们的提出的系统使用 LDA 分解简历为 latent topics，并提取有意义的 semantic representations。在尝试使用只考虑技能的情况下，我们的模型达到了 77% 的准确率；在考虑所有属性的情况下，我们的模型达到了 82% 的准确率。<details>
<summary>Abstract</summary>
In this paper, we propose a method for resume rating using Latent Dirichlet Allocation (LDA) and entity detection with SpaCy. The proposed method first extracts relevant entities such as education, experience, and skills from the resume using SpaCy's Named Entity Recognition (NER). The LDA model then uses these entities to rate the resume by assigning topic probabilities to each entity. Furthermore, we conduct a detailed analysis of the entity detection using SpaCy's NER and report its evaluation metrics. Using LDA, our proposed system breaks down resumes into latent topics and extracts meaningful semantic representations. With a vision to define our resume score to be more content-driven rather than a structure and keyword match driven, our model has achieved 77% accuracy with respect to only skills in consideration and an overall 82% accuracy with all attributes in consideration. (like college name, work experience, degree and skills)
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种使用Latent Dirichlet Allocation（LDA）和实体检测（SpaCy）来评分简历的方法。我们的方法首先从简历中提取有关的实体，如教育、经验和技能，使用SpaCy的命名实体识别（NER）。然后，LDA模型使用这些实体来评分简历，并将每个实体分配话题概率。此外，我们还进行了NER的实体检测的详细分析，并发布了评估指标。使用LDA，我们的提案系统将简历分解成了隐藏主题，并提取了有意义的语义表示。我们的模型的目标是通过对简历的内容进行评估，而不是仅仅是结构和关键词匹配，因此我们的模型在只考虑技能方面达到了77%的准确率，而在所有属性方面达到了82%的准确率。（包括学院名、工作经验、学位和技能）
</details></li>
</ul>
<hr>
<h2 id="How-regularization-affects-the-geometry-of-loss-functions"><a href="#How-regularization-affects-the-geometry-of-loss-functions" class="headerlink" title="How regularization affects the geometry of loss functions"></a>How regularization affects the geometry of loss functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15744">http://arxiv.org/abs/2307.15744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathaniel Bottman, Y. Cooper, Antonio Lerario</li>
<li>for: 研究深度神经网络如何学习，即使用不同的正则化方法。</li>
<li>methods: 研究不同的正则化方法如何改变损失函数的几何结构。</li>
<li>results: 发现在权重 decay 等正则化方法下，损失函数可能会变成 Morse 函数，这意味着神经网络可能会更好地学习。<details>
<summary>Abstract</summary>
What neural networks learn depends fundamentally on the geometry of the underlying loss function. We study how different regularizers affect the geometry of this function. One of the most basic geometric properties of a smooth function is whether it is Morse or not. For nonlinear deep neural networks, the unregularized loss function $L$ is typically not Morse. We consider several different regularizers, including weight decay, and study for which regularizers the regularized function $L_\epsilon$ becomes Morse.
</details>
<details>
<summary>摘要</summary>
<lang=zh-CN></SYS> neuronal networks 的学习听取函数的geometry的基本属性。我们研究不同的正则化对函数的geometry的影响。Morse函数是一种最基本的几何属性，我们研究非线性深度神经网络中未正则化的损失函数$L$是否为Morse函数。我们考虑了多种正则化器，包括权重减少，并研究哪些正则化器使得正则化后的函数$L_\epsilon$变为Morse函数。Note: "Morse function" is a mathematical concept, not a term commonly used in deep learning. In this text, it is used to refer to a smooth function that has a single global minimum.Here's the translation with some additional explanations:neuronal networks 的学习听取函数的geometry的基本属性。我们研究不同的正则化对函数的geometry的影响。Morse函数是一种最基本的几何属性，我们研究非线性深度神经网络中未正则化的损失函数$L$是否为Morse函数。我们考虑了多种正则化器，包括权重减少，并研究哪些正则化器使得正则化后的函数$L_\epsilon$变为Morse函数。In this text, the authors are studying the geometry of the loss function in deep neural networks, specifically how different regularizers affect the geometry of the function. They use the concept of a Morse function, which is a smooth function that has a single global minimum, to describe the geometry of the loss function. They consider various regularizers, including weight decay, and investigate which regularizers cause the regularized function $L_\epsilon$ to become a Morse function.
</details></li>
</ul>
<hr>
<h2 id="Quantum-noise-limited-optical-neural-networks-operating-at-a-few-quanta-per-activation"><a href="#Quantum-noise-limited-optical-neural-networks-operating-at-a-few-quanta-per-activation" class="headerlink" title="Quantum-noise-limited optical neural networks operating at a few quanta per activation"></a>Quantum-noise-limited optical neural networks operating at a few quanta per activation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15712">http://arxiv.org/abs/2307.15712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shi-Yuan Ma, Tianyu Wang, Jérémie Laydevant, Logan G. Wright, Peter L. McMahon</li>
<li>for: 这个论文是研究激光神经网络在低功率 режи响应下的性能的，特别是在层次结构中使用单 photon来触发神经元的情况下。</li>
<li>methods: 作者使用了一种直接模型摄像头检测器的随机行为来训练激光神经网络，以实现高精度的图像分类任务。</li>
<li>results: 实验结果显示，使用这种方法可以在低功率 régime下实现高精度的图像分类，并且使用的光能量相对于前一个state-of-the-art低光能量示范项目减少了&gt;40倍。<details>
<summary>Abstract</summary>
Analog physical neural networks, which hold promise for improved energy efficiency and speed compared to digital electronic neural networks, are nevertheless typically operated in a relatively high-power regime so that the signal-to-noise ratio (SNR) is large (>10). What happens if an analog system is instead operated in an ultra-low-power regime, in which the behavior of the system becomes highly stochastic and the noise is no longer a small perturbation on the signal? In this paper, we study this question in the setting of optical neural networks operated in the limit where some layers use only a single photon to cause a neuron activation. Neuron activations in this limit are dominated by quantum noise from the fundamentally probabilistic nature of single-photon detection of weak optical signals. We show that it is possible to train stochastic optical neural networks to perform deterministic image-classification tasks with high accuracy in spite of the extremely high noise (SNR ~ 1) by using a training procedure that directly models the stochastic behavior of photodetection. We experimentally demonstrated MNIST classification with a test accuracy of 98% using an optical neural network with a hidden layer operating in the single-photon regime; the optical energy used to perform the classification corresponds to 0.008 photons per multiply-accumulate (MAC) operation, which is equivalent to 0.003 attojoules of optical energy per MAC. Our experiment used >40x fewer photons per inference than previous state-of-the-art low-optical-energy demonstrations, to achieve the same accuracy of >90%. Our work shows that some extremely stochastic analog systems, including those operating in the limit where quantum noise dominates, can nevertheless be used as layers in neural networks that deterministically perform classification tasks with high accuracy if they are appropriately trained.
</details>
<details>
<summary>摘要</summary>
аналог物理神经网络，它们在能效率和速度方面比数字电子神经网络有前景，然而通常在高功率 режи干（SNR > 10）下运行。如果这样的 аналог系统反而在超低功率 режи干下运行，那么系统的行为会变得极其抽象和随机，而噪声不再是信号的小杂音。在这篇论文中，我们研究了这个问题，具体来说是在使用单 photon 来触发神经元的光学神经网络中。在这种情况下，神经元活动受到光学信号的渐进性和单 photon 的探测的抽象性的限制。我们表明，可以通过直接模型单 photon 探测的随机行为来训练随机光学神经网络，以实现高精度的图像分类任务。我们在 MNIST 分类任务上进行了实验，测试精度达 98%，使用的光学能量为 0.008 photons/MAC 操作，相当于 0.003 attojoules/MAC 的光学能量。我们的实验使用了 >40x  fewer photons per inference than previous state-of-the-art low-optical-energy demonstrations，以达到同样的准确率（>90%）。我们的工作表明，一些极其随机的 аналог系统，包括在噪声dominates的情况下，可以作为神经网络的层使用，以实现高精度的图像分类任务，只要采用相应的训练方法。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Object-Detection-in-the-Open-World"><a href="#Semi-Supervised-Object-Detection-in-the-Open-World" class="headerlink" title="Semi-Supervised Object Detection in the Open World"></a>Semi-Supervised Object Detection in the Open World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15710">http://arxiv.org/abs/2307.15710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Garvita Allabadi, Ana Lucic, Peter Pao-Huang, Yu-Xiong Wang, Vikram Adve</li>
<li>for: 本研究旨在 Addressing the challenges of open-world semi-supervised object detection, where the model must detect out-of-distribution (OOD) samples and learn from both in-distribution (ID) and OOD data.</li>
<li>methods: 我们提出了 Open World Semi-supervised Detection 框架 (OWSSD), which combines an OOD detector based on lightweight auto-encoder networks trained only on ID data, along with a semi-supervised learning pipeline that learns from both ID and OOD data.</li>
<li>results: 我们通过广泛的评估表明，我们的方法可以与现状最佳的OOD检测算法竞争，同时也可以在开放世界场景下提高 semi-supervised 学习性能。<details>
<summary>Abstract</summary>
Existing approaches for semi-supervised object detection assume a fixed set of classes present in training and unlabeled datasets, i.e., in-distribution (ID) data. The performance of these techniques significantly degrades when these techniques are deployed in the open-world, due to the fact that the unlabeled and test data may contain objects that were not seen during training, i.e., out-of-distribution (OOD) data. The two key questions that we explore in this paper are: can we detect these OOD samples and if so, can we learn from them? With these considerations in mind, we propose the Open World Semi-supervised Detection framework (OWSSD) that effectively detects OOD data along with a semi-supervised learning pipeline that learns from both ID and OOD data. We introduce an ensemble based OOD detector consisting of lightweight auto-encoder networks trained only on ID data. Through extensive evalulation, we demonstrate that our method performs competitively against state-of-the-art OOD detection algorithms and also significantly boosts the semi-supervised learning performance in open-world scenarios.
</details>
<details>
<summary>摘要</summary>
现有的半超vised对象检测方法假设训练和无标据数据集中的类集是固定的，即在distribution（ID）数据。这些技术在开放世界中部署时，其性能会受到很大降低，因为测试和无标据数据可能包含训练中没有看到的对象，即out-of-distribution（OOD）数据。我们在这篇文章中考虑了两个关键问题：我们可以检测OOD样本，并且如果可以，我们可以学习它们吗？为此，我们提出了开放世界半超vised检测框架（OWSSD），可以有效地检测OOD数据，同时还可以通过半超vised学习来学习ID和OOD数据。我们提出了一个ensemble基于自动编码网络，该网络只在ID数据上训练。经过广泛的评估，我们发现我们的方法可以与当前的OOD检测算法竞争，同时还可以在开放世界 scenarios中显著提高半超vised学习性能。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-in-Natural-Language-Generation-From-Theory-to-Applications"><a href="#Uncertainty-in-Natural-Language-Generation-From-Theory-to-Applications" class="headerlink" title="Uncertainty in Natural Language Generation: From Theory to Applications"></a>Uncertainty in Natural Language Generation: From Theory to Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15703">http://arxiv.org/abs/2307.15703</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Rastaman4e/-1">https://github.com/Rastaman4e/-1</a></li>
<li>paper_authors: Joris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li, Raquel Fernández, Barbara Plank, Rico Sennrich, Chrysoula Zerva, Wilker Aziz</li>
<li>for: 本研究旨在提高自然语言生成（NLG）系统的可靠性和可靠性，使其能够更好地满足人们的需求。</li>
<li>methods: 本文提出了一种基于理论的不确定处理方法，以提高NLG系统的可靠性和多样性。</li>
<li>results: 本研究提出了一种两维分类方法，可以更好地捕捉NLG系统中的不确定性。此外，本文还提出了一些实际应用的研究方向，如推理、自我评估、活动学习等。<details>
<summary>Abstract</summary>
Recent advances of powerful Language Models have allowed Natural Language Generation (NLG) to emerge as an important technology that can not only perform traditional tasks like summarisation or translation, but also serve as a natural language interface to a variety of applications. As such, it is crucial that NLG systems are trustworthy and reliable, for example by indicating when they are likely to be wrong; and supporting multiple views, backgrounds and writing styles -- reflecting diverse human sub-populations. In this paper, we argue that a principled treatment of uncertainty can assist in creating systems and evaluation protocols better aligned with these goals. We first present the fundamental theory, frameworks and vocabulary required to represent uncertainty. We then characterise the main sources of uncertainty in NLG from a linguistic perspective, and propose a two-dimensional taxonomy that is more informative and faithful than the popular aleatoric/epistemic dichotomy. Finally, we move from theory to applications and highlight exciting research directions that exploit uncertainty to power decoding, controllable generation, self-assessment, selective answering, active learning and more.
</details>
<details>
<summary>摘要</summary>
We first present the fundamental theory, frameworks, and vocabulary required to represent uncertainty. We then characterize the main sources of uncertainty in NLG from a linguistic perspective and propose a two-dimensional taxonomy that is more informative and faithful than the popular aleatoric/epistemic dichotomy. Finally, we move from theory to applications and highlight exciting research directions that exploit uncertainty to power decoding, controllable generation, self-assessment, selective answering, active learning, and more.Here's the text in Simplified Chinese:近期，具有强大语言模型的进步，使自然语言生成（NLG）成为一种重要的技术，不仅可以完成传统任务如概要或翻译，而且可以作为一种自然语言界面，与多种应用集成。因此，NLG系统需要可靠和可信，例如指示它们可能错误的时候，并支持多个视角、背景和写作风格——反映人类子 популяции的多样性。在这篇论文中，我们 argue That a principled treatment of uncertainty can assist in creating systems and evaluation protocols better aligned with these goals.我们首先介绍了需要表示不确定性的基本理论、框架和术语。然后，我们从语言学角度描述了NLG中的主要不确定性来源，并提出了一个两个维度的分类，比较有用和忠实于流行的 aleatoric/epistemic 对立。最后，我们从理论到应用，高亮了利用不确定性来实现排码、可控生成、自我评估、选择答案、活动学习等研究方向。
</details></li>
</ul>
<hr>
<h2 id="Universal-Recurrent-Event-Memories-for-Streaming-Data"><a href="#Universal-Recurrent-Event-Memories-for-Streaming-Data" class="headerlink" title="Universal Recurrent Event Memories for Streaming Data"></a>Universal Recurrent Event Memories for Streaming Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15694">http://arxiv.org/abs/2307.15694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Dou, Jose Principe</li>
<li>for: 这个论文提出了一种新的事件记忆架构（MemNet），用于 Recurrent Neural Networks（RNN），可以处理不同类型的时间序列数据，包括标量、多变量和符号时间序列。</li>
<li>methods:  MemNet 使用键值对来存储信息，这种方式可以提高表示力，同时也避免了模型状态构建的缺点。 MemNet 使用线性适应映射函数实现非线性运算。</li>
<li>results: MemNet 可以应用于不同的应用领域，包括混沌时间序列、符号运算任务和问答任务（bAbI），并在所有应用领域中达到了状态对抗网络和外部储存网络的性能水平。 MemNet 需要 fewer 的训练参数和更小的空间复杂度，使得注意机制更加有效率，开门到 IoT 应用领域。<details>
<summary>Abstract</summary>
In this paper, we propose a new event memory architecture (MemNet) for recurrent neural networks, which is universal for different types of time series data such as scalar, multivariate or symbolic. Unlike other external neural memory architectures, it stores key-value pairs, which separate the information for addressing and for content to improve the representation, as in the digital archetype. Moreover, the key-value pairs also avoid the compromise between memory depth and resolution that applies to memories constructed by the model state. One of the MemNet key characteristics is that it requires only linear adaptive mapping functions while implementing a nonlinear operation on the input data. MemNet architecture can be applied without modifications to scalar time series, logic operators on strings, and also to natural language processing, providing state-of-the-art results in all application domains such as the chaotic time series, the symbolic operation tasks, and the question-answering tasks (bAbI). Finally, controlled by five linear layers, MemNet requires a much smaller number of training parameters than other external memory networks as well as the transformer network. The space complexity of MemNet equals a single self-attention layer. It greatly improves the efficiency of the attention mechanism and opens the door for IoT applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的事件记忆架构（MemNet），用于逻辑神经网络，可以 universal 地应用于不同类型的时间序列数据，如scalar、多变量或 симвоlic。与其他外部神经网络记忆架构不同，MemNet 存储 key-value 对，以分离地址和内容信息，从而提高表示能力，类似于数字原型。此外，key-value 对还避免了模型状态构建的记忆深度和分辨率之间的compromise。MemNet 的一个关键特点是只需要线性适应映射函数，实现输入数据的非线性操作。MemNet 架构可以无需修改应用于scalar时间序列、逻辑运算符 strings 和自然语言处理等领域，并在所有应用领域中达到了state-of-the-art 结果，如混沌时间序列、符号操作任务和问答任务（bAbI）。最后，由五个线性层控制，MemNet 需要训练参数的数量比其他外部记忆网络和 transformer 网络要少得多。MemNet 的空间复杂度等于单个自我注意层。它大大提高了注意机制的效率，开启了对 IoT 应用的大门。
</details></li>
</ul>
<hr>
<h2 id="ODTlearn-A-Package-for-Learning-Optimal-Decision-Trees-for-Prediction-and-Prescription"><a href="#ODTlearn-A-Package-for-Learning-Optimal-Decision-Trees-for-Prediction-and-Prescription" class="headerlink" title="ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription"></a>ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15691">http://arxiv.org/abs/2307.15691</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/d3m-research-group/odtlearn">https://github.com/d3m-research-group/odtlearn</a></li>
<li>paper_authors: Patrick Vossler, Sina Aghaei, Nathan Justin, Nathanael Jo, Andrés Gómez, Phebe Vayanos</li>
<li>for: 这个论文主要针对高风险预测和规划任务中的优化决策树问题，提供了一个基于杂Integer优化（MIO）框架的开源Python包。</li>
<li>methods: 论文提出了一种基于MIO框架的优化决策树算法，以及其 extensions。包括优化分类树、优化公平分类树、分类树对 distribuition shift 的Robustness、和优化规划树等。</li>
<li>results: 论文提供了一个开源的Python包，名为ODTLearn，可以帮助用户快速地学习优化决策树。包括对 observational data 的学习、分类和规划等任务。<details>
<summary>Abstract</summary>
ODTLearn is an open-source Python package that provides methods for learning optimal decision trees for high-stakes predictive and prescriptive tasks based on the mixed-integer optimization (MIO) framework proposed in Aghaei et al. (2019) and several of its extensions. The current version of the package provides implementations for learning optimal classification trees, optimal fair classification trees, optimal classification trees robust to distribution shifts, and optimal prescriptive trees from observational data. We have designed the package to be easy to maintain and extend as new optimal decision tree problem classes, reformulation strategies, and solution algorithms are introduced. To this end, the package follows object-oriented design principles and supports both commercial (Gurobi) and open source (COIN-OR branch and cut) solvers. The package documentation and an extensive user guide can be found at https://d3m-research-group.github.io/odtlearn/. Additionally, users can view the package source code and submit feature requests and bug reports by visiting https://github.com/D3M-Research-Group/odtlearn.
</details>
<details>
<summary>摘要</summary>
ODTLearn 是一个开源的 Python 包，提供了用于学习优化决策树的方法，用于高风险预测和指导任务基于混合整数优化（MIO）框架，如 Aghaei et al. (2019) 等扩展。目前版本的包提供了学习优化分类树、优化公平分类树、分类树对分布变化强度 Robust 和指导树的实现。我们设计了该包，以便轻松维护和扩展，随着新的优化决策树问题类型、重新表述策略和解决算法的引入。为此，该包遵循 объек oriented 设计原则，并支持商业（Gurobi）和开源（COIN-OR branch and cut）解决方案。包的文档和详细用户指南可以在 https://d3m-research-group.github.io/odtlearn/ 找到。此外，用户可以在 https://github.com/D3M-Research-Group/odtlearn 查看包源代码，提交功能需求和错误报告。
</details></li>
</ul>
<hr>
<h2 id="AI-for-Anticipatory-Action-Moving-Beyond-Climate-Forecasting"><a href="#AI-for-Anticipatory-Action-Moving-Beyond-Climate-Forecasting" class="headerlink" title="AI for Anticipatory Action: Moving Beyond Climate Forecasting"></a>AI for Anticipatory Action: Moving Beyond Climate Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15727">http://arxiv.org/abs/2307.15727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Q. Huynh, Mathew V. Kiang</li>
<li>for: 该论文主要旨在探讨气候预测转向预先行动的趋势，以及机器学习模型在气候预测中的应用和挑战。</li>
<li>methods: 论文详细介绍了预先行动的概念和实践，并评估了现有机器学习模型在气候预测中的应用。</li>
<li>results: 论文指出，机器学习模型在气候预测中具有极高的准确率和可靠性，但在实现预先行动方面存在一些挑战和限制。<details>
<summary>Abstract</summary>
Disaster response agencies have been shifting from a paradigm of climate forecasting towards one of anticipatory action: assessing not just what the climate will be, but how it will impact specific populations, thereby enabling proactive response and resource allocation. Machine learning models are becoming exceptionally powerful at climate forecasting, but methodological gaps remain in terms of facilitating anticipatory action. Here we provide an overview of anticipatory action, review relevant applications of machine learning, identify common challenges, and highlight areas where machine learning can uniquely contribute to advancing disaster response for populations most vulnerable to climate change.
</details>
<details>
<summary>摘要</summary>
气候灾害机构正在从气候预测 парадигshift towards一个anticipatory action：评估不仅气候将如何发展，而且如何影响特定的人口，从而实现先进的应急响应和资源分配。机器学习模型在气候预测方面已经非常强大，但在实现anticipatory action方面还存在方法学挑战。本文提供了anticipatory action的概述，评估了相关的机器学习应用，描述了常见的挑战，并强调机器学习在对气候变化最容易受影响的人口进行应对方面的独特贡献。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Offline-Reinforcement-Learning-on-Real-Robot-Hardware"><a href="#Benchmarking-Offline-Reinforcement-Learning-on-Real-Robot-Hardware" class="headerlink" title="Benchmarking Offline Reinforcement Learning on Real-Robot Hardware"></a>Benchmarking Offline Reinforcement Learning on Real-Robot Hardware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15690">http://arxiv.org/abs/2307.15690</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rr-learning/trifinger_rl_datasets">https://github.com/rr-learning/trifinger_rl_datasets</a></li>
<li>paper_authors: Nico Gürtler, Sebastian Blaes, Pavel Kolev, Felix Widmaier, Manuel Wüthrich, Stefan Bauer, Bernhard Schölkopf, Georg Martius</li>
<li>for: 这篇论文旨在提出一个关于基于先前记录的数据学习的策略，用于实际Robotics任务。</li>
<li>methods: 论文使用大量多样数据和离线权威学习来解决dexterous manipulation问题。</li>
<li>results: 论文提供了一个大量数据的集合，包括在离线学习中学习的策略，以及一个可以在实际Robotics系统和模拟器上调试的选项。<details>
<summary>Abstract</summary>
Learning policies from previously recorded data is a promising direction for real-world robotics tasks, as online learning is often infeasible. Dexterous manipulation in particular remains an open problem in its general form. The combination of offline reinforcement learning with large diverse datasets, however, has the potential to lead to a breakthrough in this challenging domain analogously to the rapid progress made in supervised learning in recent years. To coordinate the efforts of the research community toward tackling this problem, we propose a benchmark including: i) a large collection of data for offline learning from a dexterous manipulation platform on two tasks, obtained with capable RL agents trained in simulation; ii) the option to execute learned policies on a real-world robotic system and a simulation for efficient debugging. We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets and provide a reproducible experimental setup for offline reinforcement learning on real systems.
</details>
<details>
<summary>摘要</summary>
学习政策从前期录制的数据中提取知识是一个有前途的方向，因为在线学习经常不可能。灵活的操作特别是一个打开的问题。 combining offline reinforcement learning with large and diverse datasets has the potential to make significant progress in this challenging domain, much like the rapid progress made in supervised learning in recent years. To coordinate the efforts of the research community towards tackling this problem, we propose a benchmark that includes:* A large collection of data for offline learning from a dexterous manipulation platform on two tasks, obtained with capable RL agents trained in simulation;* The option to execute learned policies on a real-world robotic system and a simulation for efficient debugging.We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets and provide a reproducible experimental setup for offline reinforcement learning on real systems.
</details></li>
</ul>
<hr>
<h2 id="A-supervised-hybrid-quantum-machine-learning-solution-to-the-emergency-escape-routing-problem"><a href="#A-supervised-hybrid-quantum-machine-learning-solution-to-the-emergency-escape-routing-problem" class="headerlink" title="A supervised hybrid quantum machine learning solution to the emergency escape routing problem"></a>A supervised hybrid quantum machine learning solution to the emergency escape routing problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15682">http://arxiv.org/abs/2307.15682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Haboury, Mo Kordzanganeh, Sebastian Schmitt, Ayush Joshi, Igor Tokarev, Lukas Abdallah, Andrii Kurkin, Basil Kyriacou, Alexey Melnikov<br>methods: 该论文使用了一种新的混合监督学习方法，其包括一个量子神经网络并与一个经典神经网络并行运行。results: 该研究表明，使用混合监督学习方法可以提高急救规划的准确率，相比之下，纯经典监督学习方法的准确率仅高于7%。此外，研究还表明，量子神经网络在预测中占据了45.(3)%的比重。<details>
<summary>Abstract</summary>
Managing the response to natural disasters effectively can considerably mitigate their devastating impact. This work explores the potential of using supervised hybrid quantum machine learning to optimize emergency evacuation plans for cars during natural disasters. The study focuses on earthquake emergencies and models the problem as a dynamic computational graph where an earthquake damages an area of a city. The residents seek to evacuate the city by reaching the exit points where traffic congestion occurs. The situation is modeled as a shortest-path problem on an uncertain and dynamically evolving map. We propose a novel hybrid supervised learning approach and test it on hypothetical situations on a concrete city graph. This approach uses a novel quantum feature-wise linear modulation (FiLM) neural network parallel to a classical FiLM network to imitate Dijkstra's node-wise shortest path algorithm on a deterministic dynamic graph. Adding the quantum neural network in parallel increases the overall model's expressivity by splitting the dataset's harmonic and non-harmonic features between the quantum and classical components. The hybrid supervised learning agent is trained on a dataset of Dijkstra's shortest paths and can successfully learn the navigation task. The hybrid quantum network improves over the purely classical supervised learning approach by 7% in accuracy. We show that the quantum part has a significant contribution of 45.(3)% to the prediction and that the network could be executed on an ion-based quantum computer. The results demonstrate the potential of supervised hybrid quantum machine learning in improving emergency evacuation planning during natural disasters.
</details>
<details>
<summary>摘要</summary>
naturale 灾害的回应可以优化它们的影响，这个工作探讨使用监督式量子机器学习来优化自然灾害时的紧急避难计划。研究专注在地震紧急情况下，模型问题为一个动态计算图，地震会对城市区域造成破坏。居民尝试通过到达城市边缘的出口点，以避免交通堵塞。这个问题被模型为一个短est-path问题，在一个不确定和动态变化的地图上。我们提出了一种新的复合监督学习方法，并在假设情况下进行了实验。这种方法使用了一个新的量子特征wise线性调整（FiLM）神经网络，与一个 классиical FiLM 神经网络并行，以模仿迪克斯特拉的节点短est-path算法。将量子神经网络加入平行增加了整个模型的表达能力，并将数据集的几何和非几何特征分别分配到量子和классиical  ком成分中。复合监督学习代理被训练在一个短est-path 数据集上，并成功学习到 Navigation 任务。复合量子网络与仅使用классиical 监督学习方法相比，提高了7%的准确性。我们显示出量子部分对预测的贡献为45.(3)%，并且显示了这个网络可以在钠基数量子电脑上执行。结果显示了超级监督量子机器学习在自然灾害时的紧急避难规划中的潜力。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Anomaly-Detection-System-on-various-Jetson-Edge-Devices"><a href="#Benchmarking-Anomaly-Detection-System-on-various-Jetson-Edge-Devices" class="headerlink" title="Benchmarking Anomaly Detection System on various Jetson Edge Devices"></a>Benchmarking Anomaly Detection System on various Jetson Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16834">http://arxiv.org/abs/2307.16834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoang Viet Pham, Thinh Gia Tran, Chuong Dinh Le, An Dinh Le, Hien Bich Vo<br>for:This paper focuses on developing an end-to-end crime-scene anomaly detection system using weakly supervised video anomaly detection called Robust Temporal Feature Magnitude Learning (RTFM) and edge computing technology.methods:The system uses edge computing technology and TensorRT as the software developer kit from NVIDIA for system performance enhancement, and is tested directly on multiple Jetson edge devices with Docker technology.results:The anomaly detection model yields competitive results compared to other state-of-the-art (SOTA) algorithms on available datasets such as UCF-Crime and UIT VNAnomaly, with an inference speed of 47.56 frames per second (FPS) on a Jetson edge device with only 3.11 GB RAM usage total. Additionally, the AI system achieves 15% better performance than the previous version of Jetson devices while consuming 50% less energy power.Here is the format you requested:for: 这篇论文关注开发一个结束点犯罪场景异常检测系统，使用弱有监督视频异常检测方法 called Robust Temporal Feature Magnitude Learning (RTFM) 和边缘计算技术。methods: 该系统使用边缘计算技术和NVIDIA的TensorRT软件开发工具包进行性能优化，并直接在多个Jetson边缘设备上进行测试，使用Docker技术进行系统部署。results: 异常检测模型与其他状态艺术算法在可用的 datasets such as UCF-Crime和UIT VNAnomaly 上达到竞争性的结果，推理速度达到47.56帧每秒 (FPS) 在Jetson边缘设备上，具有只有3.11 GB RAM 的总用量。此外，AI系统在不同的Jetson设备上 achieves 15% 更好的性能，同时占用50%  menos的能源电力。<details>
<summary>Abstract</summary>
Capturing the abnormal event from surveillance videos enhances the safety and well-being of the citizens. The application of EdgeAI (Edge computing-based Artificial Intelligent ) meets the strict latency requirements for security. In this paper, we apply weakly supervised video anomaly detection called Robust Temporal Feature Magnitude Learning (RTFM) to an end-to-end crime-scene anomaly detection system from the surveillance cameras with the help of edge computing technology. The system is tested directly on multiple Jetson edge devices combined with TensorRT as the software developer kit from NVIDIA for system performance enhancement. The experience of an AI-based system deployment on various Jetson Edge devices with Docker technology is also provided. The anomaly detection model yields competitive results compared to other state-of-the-art (SOTA) algorithms on available datasets such as UCF-Crime and UIT VNAnomaly. The approach system reaches 47.56 frames per second (FPS) inference speed on a Jetson edge device with only 3.11 GB RAM usage total. We also discover the promising Jetson device that the AI system achieves 15% better performance than the previous version of Jetson devices while consuming 50% less energy power.
</details>
<details>
<summary>摘要</summary>
capturing the abnormal event from surveillance videos enhances the safety and well-being of the citizens. The application of EdgeAI (Edge computing-based Artificial Intelligent) meets the strict latency requirements for security. In this paper, we apply weakly supervised video anomaly detection called Robust Temporal Feature Magnitude Learning (RTFM) to an end-to-end crime-scene anomaly detection system from the surveillance cameras with the help of edge computing technology. The system is tested directly on multiple Jetson edge devices combined with TensorRT as the software developer kit from NVIDIA for system performance enhancement. The experience of an AI-based system deployment on various Jetson Edge devices with Docker technology is also provided. The anomaly detection model yields competitive results compared to other state-of-the-art (SOTA) algorithms on available datasets such as UCF-Crime and UIT VNAnomaly. The approach system reaches 47.56 frames per second (FPS) inference speed on a Jetson edge device with only 3.11 GB RAM usage total. We also discover the promising Jetson device that the AI system achieves 15% better performance than the previous version of Jetson devices while consuming 50% less energy power.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Analysis-and-an-Eigen-Initializer-for-Recurrent-Neural-Networks"><a href="#Dynamic-Analysis-and-an-Eigen-Initializer-for-Recurrent-Neural-Networks" class="headerlink" title="Dynamic Analysis and an Eigen Initializer for Recurrent Neural Networks"></a>Dynamic Analysis and an Eigen Initializer for Recurrent Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15679">http://arxiv.org/abs/2307.15679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Dou, Jose Principe</li>
<li>for: 本研究探讨了深度循环神经网络中隐藏状态的动态行为，尤其是长期依赖问题。</li>
<li>methods: 我们采用了一种基于weight矩阵 eigen分解的新视角来分析隐藏状态空间。我们首先使用线性状态空间模型进行分析，并解释了激活函数如何保持信息。我们还对长期依赖进行了解释，并发现了不同任务类型下的独特行为。</li>
<li>results: 我们提出了一种新的初始化方法，可以在vanilla-RNN、LSTM和GRU等深度循环神经网络中提高表现。这种初始化方法在多个 datasets 上（如 Tomita Grammars、 pixel-by-pixel MNIST 数据集和 machine translation 数据集）进行了测试，并与 Xavier 初始izer 和 kaiming 初始izer 以及其他 RNN-only 初始izer  LIKE IRNN 和 sp-RNN 相比，在多个任务中具有更高的表现。<details>
<summary>Abstract</summary>
In recurrent neural networks, learning long-term dependency is the main difficulty due to the vanishing and exploding gradient problem. Many researchers are dedicated to solving this issue and they proposed many algorithms. Although these algorithms have achieved great success, understanding how the information decays remains an open problem. In this paper, we study the dynamics of the hidden state in recurrent neural networks. We propose a new perspective to analyze the hidden state space based on an eigen decomposition of the weight matrix. We start the analysis by linear state space model and explain the function of preserving information in activation functions. We provide an explanation for long-term dependency based on the eigen analysis. We also point out the different behavior of eigenvalues for regression tasks and classification tasks. From the observations on well-trained recurrent neural networks, we proposed a new initialization method for recurrent neural networks, which improves consistently performance. It can be applied to vanilla-RNN, LSTM, and GRU. We test on many datasets, such as Tomita Grammars, pixel-by-pixel MNIST datasets, and machine translation datasets (Multi30k). It outperforms the Xavier initializer and kaiming initializer as well as other RNN-only initializers like IRNN and sp-RNN in several tasks.
</details>
<details>
<summary>摘要</summary>
在回归神经网络中，长期依赖是主要挑战，主要因为衰减和爆炸梯度问题。许多研究人员努力解决这个问题，并提出了多种算法。尽管这些算法取得了很大成功，但我们还未完全理解信息如何衰减。在这篇论文中，我们研究了回归神经网络中隐藏状态的动态。我们提出了一新的视角来分析隐藏状态空间，基于权重矩阵的归一化分解。我们从线性状态空间模型开始，解释隐藏状态中的信息保持功能，并对长期依赖进行解释。我们还发现了不同任务类型的欧拉值之间的差异。从已经训练过的回归神经网络的观察结果来看，我们提出了一种新的初始化方法，可以提高回归神经网络的性能。它可以应用于普通RNN、LSTM和GRU。我们在多个数据集上进行了测试，包括Tomita Grammar、像素级MNIST数据集和机器翻译数据集（Multi30k），并且在多个任务上超越了Xavier初始化器、kaiming初始izer以及其他RNN专用的初始化器如IRNN和sp-RNN。
</details></li>
</ul>
<hr>
<h2 id="Case-Studies-of-Causal-Discovery-from-IT-Monitoring-Time-Series"><a href="#Case-Studies-of-Causal-Discovery-from-IT-Monitoring-Time-Series" class="headerlink" title="Case Studies of Causal Discovery from IT Monitoring Time Series"></a>Case Studies of Causal Discovery from IT Monitoring Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15678">http://arxiv.org/abs/2307.15678</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Ali Aït-Bachir, Charles K. Assaad, Christophe de Bignicourt, Emilie Devijver, Simon Ferreira, Eric Gaussier, Hosein Mohanna, Lei Zan</li>
<li>for: 这篇论文是为了探讨在现代企业中IT系统的监控和缓解问题，以及通过对历史数据进行分析，预测未来问题的可能性。</li>
<li>methods: 这篇论文使用了 causal discovery 算法来分析 IT 监控数据，并提出了一些对应的挑战，如时序列不对齐、睡眠时序列、时间戳错误和缺失值等。</li>
<li>results: 该论文通过对不同 IT 监控数据集的应用，显示了 causal discovery 算法的好处，但也描述了当前的挑战和未解决的问题。<details>
<summary>Abstract</summary>
Information technology (IT) systems are vital for modern businesses, handling data storage, communication, and process automation. Monitoring these systems is crucial for their proper functioning and efficiency, as it allows collecting extensive observational time series data for analysis. The interest in causal discovery is growing in IT monitoring systems as knowing causal relations between different components of the IT system helps in reducing downtime, enhancing system performance and identifying root causes of anomalies and incidents. It also allows proactive prediction of future issues through historical data analysis. Despite its potential benefits, applying causal discovery algorithms on IT monitoring data poses challenges, due to the complexity of the data. For instance, IT monitoring data often contains misaligned time series, sleeping time series, timestamp errors and missing values. This paper presents case studies on applying causal discovery algorithms to different IT monitoring datasets, highlighting benefits and ongoing challenges.
</details>
<details>
<summary>摘要</summary>
信息技术（IT）系统是现代企业的重要组成部分，负责数据存储、通信和自动化进程。监测这些系统非常重要，因为它可以收集广泛的观察时间序列数据，用于分析。在IT监测系统中，探索 causal 关系的兴趣在增长，因为它可以帮助降低系统停机时间、提高系统性能和识别异常和事件的根本原因。此外，它还允许预测未来问题的预测通过历史数据分析。虽然它拥有很多利点，但是应用 causal 探索算法在IT监测数据中存在很多挑战，例如IT监测数据中时间序列偏移、睡眠时间序列、时间戳错误和缺失值。本文通过不同的IT监测数据集的案例研究，highlights 这些挑战和继续挑战。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-training-for-tabular-data-with-attack-propagation"><a href="#Adversarial-training-for-tabular-data-with-attack-propagation" class="headerlink" title="Adversarial training for tabular data with attack propagation"></a>Adversarial training for tabular data with attack propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15677">http://arxiv.org/abs/2307.15677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiago Leon Melo, João Bravo, Marco O. P. Sampaio, Paolo Romano, Hugo Ferreira, João Tiago Ascensão, Pedro Bizarro</li>
<li>For: 防止机器学习模型受到攻击，防止恶意攻击者误导模型为非法活动预测为合法，降低系统维护人员的劳动负担。* Methods: 提出了一种新的对抗训练方法，在训练循环中带动攻击在两个空间中传播。* Results: 通过实验表明，该方法可以防止约30%的性能下降，并在非常攻击性下是必要的，但是存在一定的性能损失。<details>
<summary>Abstract</summary>
Adversarial attacks are a major concern in security-centered applications, where malicious actors continuously try to mislead Machine Learning (ML) models into wrongly classifying fraudulent activity as legitimate, whereas system maintainers try to stop them. Adversarially training ML models that are robust against such attacks can prevent business losses and reduce the work load of system maintainers. In such applications data is often tabular and the space available for attackers to manipulate undergoes complex feature engineering transformations, to provide useful signals for model training, to a space attackers cannot access. Thus, we propose a new form of adversarial training where attacks are propagated between the two spaces in the training loop. We then test this method empirically on a real world dataset in the domain of credit card fraud detection. We show that our method can prevent about 30% performance drops under moderate attacks and is essential under very aggressive attacks, with a trade-off loss in performance under no attacks smaller than 7%.
</details>
<details>
<summary>摘要</summary>
“对于安全应用程序而言，对抗攻击是一项重要的挑战，恶意攻击者不断尝试让机器学习（ML）模型错误地分类为合法的活动，而系统维护人员则努力阻止他们。使 ML 模型通过对抗训练得到鲜度的Robustness可以防止业务损失和减轻系统维护人员的劳重。在这些应用程序中，数据经常是表格式的，攻击者可以在复杂的特征工程转换下进行操作，以提供有用的信号 для模型训练。因此，我们提出了一种新的对抗训练方法，在训练循环中传递攻击。我们在实际世界数据集上进行了empirical测试，显示我们的方法可以在中等攻击下预防约30%的性能下降，并在非常攻击下是必要的，与无攻击下的性能损失比例小于7%。”
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Time-Series-Classifier-for-Decoding-Simple-Visual-Stimuli-from-Intracranial-Neural-Activity"><a href="#Bayesian-Time-Series-Classifier-for-Decoding-Simple-Visual-Stimuli-from-Intracranial-Neural-Activity" class="headerlink" title="Bayesian Time-Series Classifier for Decoding Simple Visual Stimuli from Intracranial Neural Activity"></a>Bayesian Time-Series Classifier for Decoding Simple Visual Stimuli from Intracranial Neural Activity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15672">http://arxiv.org/abs/2307.15672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Ziaei, Reza Saadatifard, Ali Yousefi, Behzad Nazari, Sydney S. Cash, Angelique C. Paulk</li>
<li>For:  This paper is written to address the need for developing analytical tools that can handle limited data and intrinsic stochasticity present in neural data, with the goal of understanding how external stimuli are encoded in distributed neural activity.* Methods:  The proposed Bayesian time series classifier (BTsC) model is used to classify neural data and decode colors in a visual task. The model is based on a straightforward approach that maintains a high level of interpretability.* Results:  The BTsC model exhibits consistent and reliable average performance of 75.55% on 4 patients’ dataset, improving upon state-of-the-art machine learning techniques by about 3.0 percent. The proposed solution provides interpretable results, making it a valuable tool to study neural activity in various tasks and categories.<details>
<summary>Abstract</summary>
Understanding how external stimuli are encoded in distributed neural activity is of significant interest in clinical and basic neuroscience. To address this need, it is essential to develop analytical tools capable of handling limited data and the intrinsic stochasticity present in neural data. In this study, we propose a straightforward Bayesian time series classifier (BTsC) model that tackles these challenges whilst maintaining a high level of interpretability. We demonstrate the classification capabilities of this approach by utilizing neural data to decode colors in a visual task. The model exhibits consistent and reliable average performance of 75.55% on 4 patients' dataset, improving upon state-of-the-art machine learning techniques by about 3.0 percent. In addition to its high classification accuracy, the proposed BTsC model provides interpretable results, making the technique a valuable tool to study neural activity in various tasks and categories. The proposed solution can be applied to neural data recorded in various tasks, where there is a need for interpretable results and accurate classification accuracy.
</details>
<details>
<summary>摘要</summary>
理解外部刺激如何在神经活动中被编码是клиниче和基础神经科学中的一项关键问题。为了解决这个问题，需要开发可以处理有限数据和神经数据的内在噪声的分析工具。在这个研究中，我们提出了一种简单的抽象时间序列分类器（BTsC）模型，该模型可以解决这些挑战，同时保持高度的可读性。我们通过使用神经数据来解码视觉任务中的颜色，示出了该模型的分类能力。模型在4名病人的数据集上显示了平均性和可靠性的75.55%的分类精度，比前一个状态的机器学习技术提高约3.0%。此外，我们的提议的BTsC模型不仅具有高的分类精度，还提供了可读的结果，使得该技术成为各种任务和类别中神经活动研究的有价值工具。该解决方案可以应用于神经数据记录在各种任务中，需要可读的结果和高的分类精度。
</details></li>
</ul>
<hr>
<h2 id="CoRe-Optimizer-An-All-in-One-Solution-for-Machine-Learning"><a href="#CoRe-Optimizer-An-All-in-One-Solution-for-Machine-Learning" class="headerlink" title="CoRe Optimizer: An All-in-One Solution for Machine Learning"></a>CoRe Optimizer: An All-in-One Solution for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15663">http://arxiv.org/abs/2307.15663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Marco Eckhoff, Markus Reiher</li>
<li>for: 训练机器学习模型的优化算法和其超参数可以对训练速度和模型准确率产生重要影响。</li>
<li>methods: 本文使用了10种优化算法，包括Adam优化器和抗衰减反propagation（RPROP），并对不同的机器学习任务进行了广泛的性能比较。</li>
<li>results: 研究发现，CoRe优化器在各种机器学习任务中表现最佳或与其他优化器竞争，而只需要根据mini-batch或批处理学习而改变一个超参数。<details>
<summary>Abstract</summary>
The optimization algorithm and its hyperparameters can significantly affect the training speed and resulting model accuracy in machine learning applications. The wish list for an ideal optimizer includes fast and smooth convergence to low error, low computational demand, and general applicability. Our recently introduced continual resilient (CoRe) optimizer has shown superior performance compared to other state-of-the-art first-order gradient-based optimizers for training lifelong machine learning potentials. In this work we provide an extensive performance comparison of the CoRe optimizer and nine other optimization algorithms including the Adam optimizer and resilient backpropagation (RPROP) for diverse machine learning tasks. We analyze the influence of different hyperparameters and provide generally applicable values. The CoRe optimizer yields best or competitive performance in every investigated application, while only one hyperparameter needs to be changed depending on mini-batch or batch learning.
</details>
<details>
<summary>摘要</summary>
优化算法和其超参数可以很大地影响机器学习模型的训练速度和结果准确率。理想的优化器的愿望列表包括快速和平滑地 converges to 低误差，低计算成本，通用性。我们最近引入的连续强健（CoRe）优化器在训练持续学习潜力方面显示出优于其他当前状态艺术首频导导优化器。在这个工作中，我们对CoRe优化器和9种其他优化算法，包括Adam优化器和快速反演（RPROP）进行了广泛的性能比较。我们分析了不同的超参数对应用的影响，并提供了通用的值。CoRe优化器在所有调查应用中表现最佳或竞争力强，而只需要根据mini-batch或批处理学习而变化一个超参数。
</details></li>
</ul>
<hr>
<h2 id="Multi-layer-Aggregation-as-a-key-to-feature-based-OOD-detection"><a href="#Multi-layer-Aggregation-as-a-key-to-feature-based-OOD-detection" class="headerlink" title="Multi-layer Aggregation as a key to feature-based OOD detection"></a>Multi-layer Aggregation as a key to feature-based OOD detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15647">http://arxiv.org/abs/2307.15647</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/benolmbrt/MedicOOD">https://github.com/benolmbrt/MedicOOD</a></li>
<li>paper_authors: Benjamin Lambert, Florence Forbes, Senan Doyle, Michel Dojat</li>
<li>For: 本研究旨在探讨 Deep Learning 模型对输入图像变化的抗干扰性，尤其是在医学图像分析中， где范围内的可能的异常非常广泛。* Methods: 本研究使用了基于模型中间特征的新一代方法，可以分为单层方法和多层方法。单层方法考虑在固定、仔细选择的层获得的特征图，而多层方法考虑模型生成的特征图ensemble。* Results: 本研究对20种异常类型（对应约7800个3D MRI）进行了大规模的对比，发现多层方法在各种异常类型中都有更高的抗干扰性，而单层方法则具有不一致的行为，具体取决于异常类型。此外，本研究还发现了基于模型网络架构的 OOD 检测性能强度的关系。<details>
<summary>Abstract</summary>
Deep Learning models are easily disturbed by variations in the input images that were not observed during the training stage, resulting in unpredictable predictions. Detecting such Out-of-Distribution (OOD) images is particularly crucial in the context of medical image analysis, where the range of possible abnormalities is extremely wide. Recently, a new category of methods has emerged, based on the analysis of the intermediate features of a trained model. These methods can be divided into 2 groups: single-layer methods that consider the feature map obtained at a fixed, carefully chosen layer, and multi-layer methods that consider the ensemble of the feature maps generated by the model. While promising, a proper comparison of these algorithms is still lacking. In this work, we compared various feature-based OOD detection methods on a large spectra of OOD (20 types), representing approximately 7800 3D MRIs. Our experiments shed the light on two phenomenons. First, multi-layer methods consistently outperform single-layer approaches, which tend to have inconsistent behaviour depending on the type of anomaly. Second, the OOD detection performance highly depends on the architecture of the underlying neural network.
</details>
<details>
<summary>摘要</summary>
深度学习模型容易受到训练阶段未见到的输入图像变化的影响，导致预测结果不可预测。在医学图像分析上，检测这些外围（Out-of-Distribution，OOD）图像特别重要。最近，一种新的类型的方法出现了，基于模型的中间特征分析。这些方法可以分为两个组：单层方法，考虑模型在固定、仔细选择的层获得的特征图，以及多层方法，考虑模型生成的特征图ensemble。虽然有承诺，但是这些算法之间的比较仍然缺乏。在这项工作中，我们对各种特征基于OOD检测方法进行了大规模的测试，包括20种类型的OOD图像，代表约7800个3D MRI图像。我们的实验揭示了两种现象：首先，多层方法在单层方法中具有更高的检测性能，而且这些单层方法在异常类型之间存在不一致的行为。其次，OOD检测性能强烈取决于下游神经网络的架构。
</details></li>
</ul>
<hr>
<h2 id="Scale-aware-Test-time-Click-Adaptation-for-Pulmonary-Nodule-and-Mass-Segmentation"><a href="#Scale-aware-Test-time-Click-Adaptation-for-Pulmonary-Nodule-and-Mass-Segmentation" class="headerlink" title="Scale-aware Test-time Click Adaptation for Pulmonary Nodule and Mass Segmentation"></a>Scale-aware Test-time Click Adaptation for Pulmonary Nodule and Mass Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15645">http://arxiv.org/abs/2307.15645</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/splinterli/sattca">https://github.com/splinterli/sattca</a></li>
<li>paper_authors: Zhihao Li, Jiancheng Yang, Yongchao Xu, Li Zhang, Wenhui Dong, Bo Du</li>
<li>for: 这篇论文是为了提高lung cancer screening中图像分割的精度，特别是处理不同大小的肺脏病变所写的。</li>
<li>methods: 这篇论文提出了一种基于多尺度神经网络的test-time adaptiveClick权重调整方法，使得分割性能特别是对大肺脏病变进行了改进。</li>
<li>results:  experiments表明，这种方法可以与一些CNN和Transformer基于的分割方法进行比较，并且可以很好地处理不同大小的肺脏病变。<details>
<summary>Abstract</summary>
Pulmonary nodules and masses are crucial imaging features in lung cancer screening that require careful management in clinical diagnosis. Despite the success of deep learning-based medical image segmentation, the robust performance on various sizes of lesions of nodule and mass is still challenging. In this paper, we propose a multi-scale neural network with scale-aware test-time adaptation to address this challenge. Specifically, we introduce an adaptive Scale-aware Test-time Click Adaptation method based on effortlessly obtainable lesion clicks as test-time cues to enhance segmentation performance, particularly for large lesions. The proposed method can be seamlessly integrated into existing networks. Extensive experiments on both open-source and in-house datasets consistently demonstrate the effectiveness of the proposed method over some CNN and Transformer-based segmentation methods. Our code is available at https://github.com/SplinterLi/SaTTCA
</details>
<details>
<summary>摘要</summary>
肺脏结核和肿块是肺癌检测中重要的成像特征，需要仔细的诊断管理。尽管深度学习基于医疗图像分割的技术取得了成功，但是对不同大小的肿块和结核的性能仍然是挑战。在这篇论文中，我们提出了一种多尺度神经网络以及Scale-aware Test-time Click Adaptation方法，以提高分割性能，特别是大肿块的分割。这种方法可以轻松地与现有网络集成。我们在开源和自有数据集上进行了广泛的实验，并经过了一系列的比较，结果表明我们提出的方法在一些CNN和Transformer基于的分割方法之上表现更加有力。我们的代码可以在https://github.com/SplinterLi/SaTTCA上获取。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Data-Generation-in-Vision-and-Language-Navigation"><a href="#Scaling-Data-Generation-in-Vision-and-Language-Navigation" class="headerlink" title="Scaling Data Generation in Vision-and-Language Navigation"></a>Scaling Data Generation in Vision-and-Language Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15644">http://arxiv.org/abs/2307.15644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wz0919/scalevln">https://github.com/wz0919/scalevln</a></li>
<li>paper_authors: Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, Yu Qiao</li>
<li>for: 提高语言导航agent的总体性和可靠性</li>
<li>methods: 使用HM3D和Gibson数据集中的1200多个真实照片环境，以及网络上可以访问的全部资源，生成490万个指令轨迹对，并对这些数据进行预训练和精度调整</li>
<li>results: 使用这些扩大数据，提高了现有agent的性能，单次成功率提高11%，与前一个SoTA的比较达到了80%的单次成功率，同时将不 familier环境下的性能差距降低到0.1%（相比前一个方法的8%），并且这种方法可以让不同的模型在CVDN、REVERIE和R2R中实现新的顶峰导航成绩。<details>
<summary>Abstract</summary>
Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervision for training generalizable agents. To tackle the common data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which applies 1200+ photo-realistic environments from HM3D and Gibson datasets and synthesizes 4.9 million instruction trajectory pairs using fully-accessible resources on the web. Importantly, we investigate the influence of each component in this paradigm on the agent's performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The long-lasting generalization gap between navigating in seen and unseen environments is also reduced to less than 1% (versus 8% in the previous best method). Moreover, our paradigm also facilitates different models to achieve new state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous environments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/29/cs.LG_2023_07_29/" data-id="clpahu75400p93h88ckcq6o92" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/29/eess.IV_2023_07_29/" class="article-date">
  <time datetime="2023-07-29T09:00:00.000Z" itemprop="datePublished">2023-07-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/29/eess.IV_2023_07_29/">eess.IV - 2023-07-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Why-is-thermal-imaging-textureless"><a href="#Why-is-thermal-imaging-textureless" class="headerlink" title="Why is thermal imaging textureless"></a>Why is thermal imaging textureless</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15800">http://arxiv.org/abs/2307.15800</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanglinbao/hadar">https://github.com/fanglinbao/hadar</a></li>
<li>paper_authors: Fanglin Bao, Shubhankar Jape, Andrew Schramka, Junjie Wang, Tim E. McGraw, Zubin Jacob</li>
<li>for: 这个论文旨在探讨靠热成像实现夜视的问题，以及如何使用TeX视力来缓解阴影效应。</li>
<li>methods: 该论文使用了实验和计算模拟来研究非对称温度场下的纹理恢复问题，并比较了传统温度成像和TeX视力的性能。</li>
<li>results: 研究发现，在非对称温度场下，传统温度成像不能正确地恢复纹理，而TeX视力则能够成功地缓解阴影效应并恢复纹理。此外，研究还发现了一些关键尚未研究的TeX视力理论问题，并在实际可行的Bayer滤光器设置下实现了一种真正的夜视如白天的效果。<details>
<summary>Abstract</summary>
Thermal imaging can enable night vision but is usually textureless, well-known as the ghosting effect. The mechanism of this ghosting effect has recently been explained, and TeX vision has been proposed to overcome the ghosting effect. However, it is still unknown for realistic scenarios with non-uniform temperature whether TeX vision can correctly recover geometric textures and how its performance is compared with traditional thermal imaging. Here, we focus on the interplay of geometric textures and non-uniform temperature which is common in realistic thermal imaging, and demonstrate the failure of traditional approaches while TeX vision successfully recovers geometric textures. We also analyze important yet unexplored aspects of the TeX vision theory, and demonstrate a true night vision like broad daylight with the experimentally more feasible Bayer-filter setup. This deepens the understanding of the ghosting effect and bridges the gap between the TeX vision theory and the consumer thermal-imaging market.
</details>
<details>
<summary>摘要</summary>
热成像可以启用夜视，但通常无Texture，这被称为幽灵效果。这种幽灵效果的机制最近才得到了解释，而TeX视力被提议以消除幽灵效果。然而，在非一致温度的实际场景中，TeX视力是否能正确地恢复Geometric texture和与传统热成像相比如何perform的还未得到了确定的答案。在这里，我们关注了Geometric texture和非一致温度之间的互动，并证明传统方法失败，而TeX视力成功地恢复Geometric texture。我们还分析了TeX vision理论中尚未被探索的重要方面，并在 Bayer-filter 设置下实现了真正的夜视如白天一般。这有助于深入理解幽灵效果，并将TeX vision理论和消费者热成像市场之间的空难 bridged。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/29/eess.IV_2023_07_29/" data-id="clpahu7co018p3h88cp4q7zdg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/cs.SD_2023_07_28/" class="article-date">
  <time datetime="2023-07-28T15:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/28/cs.SD_2023_07_28/">cs.SD - 2023-07-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="All-for-One-and-One-For-All-Deep-learning-based-feature-fusion-for-Synthetic-Speech-Detection"><a href="#All-for-One-and-One-For-All-Deep-learning-based-feature-fusion-for-Synthetic-Speech-Detection" class="headerlink" title="All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection"></a>All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15555">http://arxiv.org/abs/2307.15555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Mari, Davide Salvi, Paolo Bestagini, Simone Milani</li>
<li>for: 防止深度学习和计算机视觉技术的滥用，尤其是在语音频段，预防恶意用户利用深度模拟技术生成假语音，进而导致诈骗或身份盗窃等问题。</li>
<li>methods: 基于文献中提出的三种特征集，实现了一种将这三种特征集融合的模型，以实现对现有方法的改进。</li>
<li>results: 对不同场景和数据集进行测试，证明了该系统具有防御反反馈攻击和泛化能力。<details>
<summary>Abstract</summary>
Recent advances in deep learning and computer vision have made the synthesis and counterfeiting of multimedia content more accessible than ever, leading to possible threats and dangers from malicious users. In the audio field, we are witnessing the growth of speech deepfake generation techniques, which solicit the development of synthetic speech detection algorithms to counter possible mischievous uses such as frauds or identity thefts. In this paper, we consider three different feature sets proposed in the literature for the synthetic speech detection task and present a model that fuses them, achieving overall better performances with respect to the state-of-the-art solutions. The system was tested on different scenarios and datasets to prove its robustness to anti-forensic attacks and its generalization capabilities.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:最近的深度学习和计算机视觉技术的进步使得制作和伪造多媒体内容变得更加容易，可能导致恶意用户的威胁和危险。在音频领域，我们目睹到了语音深度伪造生成技术的增长，这使得适用于防止可能的欺诈和身份盗窃的伪造语音检测算法的开发成为了一项重要的任务。在这篇论文中，我们考虑了Literature中提出的三种不同的特征集，并提出了一种将其融合的模型，实现了与当前最佳解决方案的更好的性能。该系统在不同的场景和数据集上进行了测试，以证明其对反法医攻击的抗性和泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Automated-approach-for-source-location-in-shallow-waters"><a href="#Automated-approach-for-source-location-in-shallow-waters" class="headerlink" title="Automated approach for source location in shallow waters"></a>Automated approach for source location in shallow waters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15491">http://arxiv.org/abs/2307.15491</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/niclas-angele/source_localization">https://github.com/niclas-angele/source_localization</a></li>
<li>paper_authors: Angèle Niclas, Josselin Garnier</li>
<li>for: 这篇论文是为了描述一种完全自动化的 shallow water 中源点和媒体参数的恢复方法。</li>
<li>methods: 该方法使用了 teoretic 工具来理解扭变方法的稳定性，并提出了一种自动分解记录信号中的模态组分的方法。</li>
<li>results: 该方法在实验数据中展示了对实际场景中的右鲸鸟枪响和燃烧声源的有效性。<details>
<summary>Abstract</summary>
This paper proposes a fully automated method for recovering the location of a source and medium parameters in shallow waters. The scenario involves an unknown source emitting low-frequency sound waves in a shallow water environment, and a single hydrophone recording the signal. Firstly, theoretical tools are introduced to understand the robustness of the warping method and to propose and analyze an automated way to separate the modal components of the recorded signal. Secondly, using the spectrogram of each modal component, the paper investigates the best way to recover the modal travel times and provides stability estimates. Finally, a penalized minimization algorithm is presented to recover estimates of the source location and medium parameters. The proposed method is tested on experimental data of right whale gunshot and combustive sound sources, demonstrating its effectiveness in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种完全自动化的方法，用于在浅水中回归源点和媒体参数。情况是一个未知的源在浅水环境中发出低频声波，并且单个水微phone记录了信号。首先，论文介绍了理论工具，以理解扭曲方法的稳定性，并提出了自动分解记录信号的模态组分的方法。其次，使用每个模态组分的spectrogram，论文研究了最好的方法来回归模态旅行时间，并提供了稳定估计。最后，论文提出了一种惩罚最小化算法，用于回归源点和媒体参数的估计。这种方法在实验数据中进行了右鲸鱼枪声和燃烧声源的测试，证明其在实际情况下的有效性。
</details></li>
</ul>
<hr>
<h2 id="Minimally-Supervised-Speech-Synthesis-with-Conditional-Diffusion-Model-and-Language-Model-A-Comparative-Study-of-Semantic-Coding"><a href="#Minimally-Supervised-Speech-Synthesis-with-Conditional-Diffusion-Model-and-Language-Model-A-Comparative-Study-of-Semantic-Coding" class="headerlink" title="Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding"></a>Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15484">http://arxiv.org/abs/2307.15484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyu Qiang, Hao Li, Hao Ni, He Qu, Ruibo Fu, Tao Wang, Longbiao Wang, Jianwu Dang</li>
<li>for: 这些纸张是用于提高文本识别和自动语音合成的方法。</li>
<li>methods: 这些方法使用了扩散模型和变量自动编码器来提高提示表示能力，以及扩散模型来解决高维度和声波扭曲问题。</li>
<li>results: 对比基eline方法，这些方法表现更好，并且可以生成多种 expresión 的语音。Here’s the same information in Simplified Chinese:</li>
<li>for: 这些论文是用于提高文本识别和自动语音合成的方法。</li>
<li>methods: 这些方法使用了扩散模型和变量自动编码器来提高提示表示能力，以及扩散模型来解决高维度和声波扭曲问题。</li>
<li>results: 对比基eline方法，这些方法表现更好，并且可以生成多种 expresión 的语音。<details>
<summary>Abstract</summary>
Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. To address the challenges associated with high dimensionality and waveform distortion in discrete representations, we propose Diff-LM-Speech, which models semantic embeddings into mel-spectrogram based on diffusion models and introduces a prompt encoder structure based on variational autoencoders and prosody bottlenecks to improve prompt representation capabilities. Autoregressive language models often suffer from missing and repeated words, while non-autoregressive frameworks face expression averaging problems due to duration prediction models. To address these issues, we propose Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse prosodic expressions. While we expect the information content of semantic coding to be between that of text and acoustic coding, existing models extract semantic coding with a lot of redundant information and dimensionality explosion. To verify that semantic coding is not necessary, we propose Tri-Diff-Speech. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples.
</details>
<details>
<summary>摘要</summary>
近期，有越来越多的关注TEXT-TO-SPEECH（TTS）方法，可以通过最小监督来训练，通过组合两种类型的不同的语音表示方式，并使用两个序列-TO-序列任务来解耦TTS。为了Address高维度和波形扭曲在不同表示方式中的挑战，我们提出了Diff-LM-Speech，它模型语意嵌入 mel-spectrogram 基于扩散模型，并引入了提高描述符能力的提问编码结构，以及基于变量自动编码器和表达瓶颈的 prosody 瓶颈。非autoregressive 框架常常面临缺失和重复的单词问题，而autoregressive 模型则面临表达平均化问题，这是因为duration prediction 模型。为了解决这些问题，我们提出了Tetra-Diff-Speech，它设计了一个扩散duration模型，以实现多种表达的多样化。我们预期semantic coding 信息的内容与文本和声音编码信息之间存在一定的相似性，但现有模型通常会提取大量的重复信息和维度爆炸。为了验证semantic coding 是否真的不必要，我们提出了Tri-Diff-Speech。我们的提议方法在实验中表现出了超越基eline方法的成绩。我们提供了一个网站，包含了各种音频样本。
</details></li>
</ul>
<hr>
<h2 id="The-FlySpeech-Audio-Visual-Speaker-Diarization-System-for-MISP-Challenge-2022"><a href="#The-FlySpeech-Audio-Visual-Speaker-Diarization-System-for-MISP-Challenge-2022" class="headerlink" title="The FlySpeech Audio-Visual Speaker Diarization System for MISP Challenge 2022"></a>The FlySpeech Audio-Visual Speaker Diarization System for MISP Challenge 2022</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15400">http://arxiv.org/abs/2307.15400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Zhang, Huan Zhao, Yue Li, Bowen Pang, Yannan Wang, Hongji Wang, Wei Rao, Qing Wang, Lei Xie</li>
<li>for: 这个论文描述了在ICASSP 2022 上举行的第二届多Modal信息基于语音处理~(\textbf{MISP}) 挑战中提交的 FlySpeech 说话人分类系统。</li>
<li>methods: 我们开发了一个端到端的音频视频说话人分类系统（AVSD），该系统包括一个唇编码器、一个说话人编码器和一个音频视频解码器。具体来说，我们为了解决分类性能下降的问题，将说话人编码器和音频视频解码器进行共同训练。此外，我们还利用大量预训练的说话人提取器来初始化说话人编码器。</li>
<li>results: 我们的实验结果表明，我们的AVSD系统在不同的说话人数量和背景噪音水平下都具有良好的性能，并且与其他参与者的系统进行比较，我们的系统在大多数情况下具有更高的准确率。<details>
<summary>Abstract</summary>
This paper describes the FlySpeech speaker diarization system submitted to the second \textbf{M}ultimodal \textbf{I}nformation Based \textbf{S}peech \textbf{P}rocessing~(\textbf{MISP}) Challenge held in ICASSP 2022. We develop an end-to-end audio-visual speaker diarization~(AVSD) system, which consists of a lip encoder, a speaker encoder, and an audio-visual decoder. Specifically, to mitigate the degradation of diarization performance caused by separate training, we jointly train the speaker encoder and the audio-visual decoder. In addition, we leverage the large-data pretrained speaker extractor to initialize the speaker encoder.
</details>
<details>
<summary>摘要</summary>
这篇论文描述了我们在ICASSP 2022年度第二届多模态信息基于语音处理~(\textbf{MISP}) 挑战中提交的飞语音 speaker 分类系统。我们开发了一个端到端的音频视频 speaker 分类系统（AVSD），该系统包括一个唇编码器、一个说话者编码器和一个音频视频解码器。具体来说，为了解决分离训练导致的分类性能下降，我们在说话者编码器和音频视频解码器之间进行了联合训练。此外，我们还利用了大量预训练的说话者抽取器来初始化说话者编码器。
</details></li>
</ul>
<hr>
<h2 id="Improving-Audio-Text-Retrieval-via-Hierarchical-Cross-Modal-Interaction-and-Auxiliary-Captions"><a href="#Improving-Audio-Text-Retrieval-via-Hierarchical-Cross-Modal-Interaction-and-Auxiliary-Captions" class="headerlink" title="Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions"></a>Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15344">http://arxiv.org/abs/2307.15344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Xin, Yuexian Zou</li>
<li>for: 提高音频文本相关 retrieval（ATR）性能， ignore 细腻的cross-modal关系。</li>
<li>methods: 引入层次跨modal交互（HCI）方法，同时探索clip-sentence、segment-phrase和frame-word关系，实现多modal含义比较。</li>
<li>results: 实验显示，我们的HCI方法可以提高ATR性能，同时，我们的auxiliary captions（AC）框架可以提供更好的音频表示，并且可以作为训练数据增强。<details>
<summary>Abstract</summary>
Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained cross-modal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical cross-modal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets.
</details>
<details>
<summary>摘要</summary>
现有的音频文本检索（ATR）方法通常是构建整个音频clip和完整的caption句子的对比对，而忽略细致的交叉模态关系，例如短段和短语或帧和单词。在这篇论文中，我们介绍了一种层次跨模态交互（HCI）方法，同时探索clip-sentence、segment-phrase和frame-word关系，实现了多模态semantic比较的全面评估。此外，我们还提出了一种新的ATR框架，利用预训练的captioner生成的auxiliary captions（AC）来实现音频和生成caption之间的特征交互，这对audio表示具有进一步提高的性能，并且与原始ATR匹配分支相комplementary。音频和生成caption还可以组成新的音频-文本对，用于训练的数据增强。实验结果表明，我们的HCI方法具有显著的提升效果，而我们的AC框架也在多个dataset上表现稳定。
</details></li>
</ul>
<hr>
<h2 id="PCNN-A-Lightweight-Parallel-Conformer-Neural-Network-for-Efficient-Monaural-Speech-Enhancement"><a href="#PCNN-A-Lightweight-Parallel-Conformer-Neural-Network-for-Efficient-Monaural-Speech-Enhancement" class="headerlink" title="PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement"></a>PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15251">http://arxiv.org/abs/2307.15251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinmeng Xu, Weiping Tu, Yuhong Yang</li>
<li>for: 提高语音增强效果</li>
<li>methods: 利用CNN和Transformer两种架构，实现并行搅动和自我注意力的结合，并设计了多支杆堆叠 convolution和自我时频频率注意力模块</li>
<li>results: 与现有方法相比，本方法在大多数评估标准中表现出色，同时具有最低的模型参数数量<details>
<summary>Abstract</summary>
Convolutional neural networks (CNN) and Transformer have wildly succeeded in multimedia applications. However, more effort needs to be made to harmonize these two architectures effectively to satisfy speech enhancement. This paper aims to unify these two architectures and presents a Parallel Conformer for speech enhancement. In particular, the CNN and the self-attention (SA) in the Transformer are fully exploited for local format patterns and global structure representations. Based on the small receptive field size of CNN and the high computational complexity of SA, we specially designed a multi-branch dilated convolution (MBDC) and a self-channel-time-frequency attention (Self-CTFA) module. MBDC contains three convolutional layers with different dilation rates for the feature from local to non-local processing. Experimental results show that our method performs better than state-of-the-art methods in most evaluation criteria while maintaining the lowest model parameters.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）和变换器（Transformer）在多媒体应用中取得了很大成功。然而，为了有效融合这两种架构，还需要更多的努力。这篇论文目标是将这两种架构融合在一起，并提出了并行转换器（Parallel Conformer） для speech enhancement。特别是，我们完全利用了 CNN 和 Transformer 中的自注意力（SA）来捕捉本地格式模式和全球结构表示。基于小覆盖区域大小和自注意力的计算复杂性，我们专门设计了多支分支扩展 convolution（MBDC）和自频时间频率注意力（Self-CTFA）模块。MBDC 包括三层扩展 convolution Layer  WITH different dilation rates，用于从本地到非本地处理。实验结果表明，我们的方法在大多数评价标准下表现更好于现有方法，同时保持最低的模型参数。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Visual-Acoustic-Matching"><a href="#Self-Supervised-Visual-Acoustic-Matching" class="headerlink" title="Self-Supervised Visual Acoustic Matching"></a>Self-Supervised Visual Acoustic Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15064">http://arxiv.org/abs/2307.15064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Somayazulu, Changan Chen, Kristen Grauman</li>
<li>for: 用于实现自然语言处理任务，特别是听说场景中的声音重新synthesize。</li>
<li>methods: 提出了一种自动学习的方法，使用目标场景图像和声音进行联合学习，包括一种新的度量器来衡量剩余的声音信息。</li>
<li>results: 在多个Difficult的数据集上进行训练，与当前最佳方法进行比较，达到了更高的性能。<details>
<summary>Abstract</summary>
Acoustic matching aims to re-synthesize an audio clip to sound as if it were recorded in a target acoustic environment. Existing methods assume access to paired training data, where the audio is observed in both source and target environments, but this limits the diversity of training data or requires the use of simulated data or heuristics to create paired samples. We propose a self-supervised approach to visual acoustic matching where training samples include only the target scene image and audio -- without acoustically mismatched source audio for reference. Our approach jointly learns to disentangle room acoustics and re-synthesize audio into the target environment, via a conditional GAN framework and a novel metric that quantifies the level of residual acoustic information in the de-biased audio. Training with either in-the-wild web data or simulated data, we demonstrate it outperforms the state-of-the-art on multiple challenging datasets and a wide variety of real-world audio and environments.
</details>
<details>
<summary>摘要</summary>
它目标是使一个音频片段在目标听取环境中重新生成，以让它听起来像是在目标环境中录制的。现有的方法假设有对应的训练数据，其中包括音频在源和目标环境中的观察记录，但这限制了训练数据的多样性或需要使用模拟数据或启发法生成对应的样本。我们提出了一种自然语言处理的自主超vised Approach，其中训练样本只包括目标场景图像和音频，而不需要对应的听取环境音频作为参考。我们的方法通过一种Conditional GAN框架和一种新的度量来共同学习分离房间听取特性和重新生成音频到目标环境中，并在多个挑战性数据集和真实世界音频和环境中表现出色。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/28/cs.SD_2023_07_28/" data-id="clpahu77v00x63h8815s01iqz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/eess.AS_2023_07_28/" class="article-date">
  <time datetime="2023-07-28T14:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/28/eess.AS_2023_07_28/">eess.AS - 2023-07-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Acoustic-Echo-Suppression-with-Condition-Aware-Training"><a href="#Efficient-Acoustic-Echo-Suppression-with-Condition-Aware-Training" class="headerlink" title="Efficient Acoustic Echo Suppression with Condition-Aware Training"></a>Efficient Acoustic Echo Suppression with Condition-Aware Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15630">http://arxiv.org/abs/2307.15630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernst Seidel, Pejman Mowlaee, Tim Fingscheidt</li>
<li>for: 这篇论文主要关注deep acoustic echo control (DAEC)方法的改进，以提高双语言听取的效果。</li>
<li>methods: 论文使用了卷积循环神经网络 (CRN)，该网络包括卷积编码和解oder，并且包含了回传瓶颈，以保留双语言听取中的nearend speech。</li>
<li>results: 该网络在双语言听取中实现了更好的效果，比起FCRN和CRUSE这两个基eline架构，不仅储存parameters和计算复杂度少，而且也表现更好。<details>
<summary>Abstract</summary>
The topic of deep acoustic echo control (DAEC) has seen many approaches with various model topologies in recent years. Convolutional recurrent networks (CRNs), consisting of a convolutional encoder and decoder encompassing a recurrent bottleneck, are repeatedly employed due to their ability to preserve nearend speech even in double-talk (DT) condition. However, past architectures are either computationally complex or trade off smaller model sizes with a decrease in performance. We propose an improved CRN topology which, compared to other realizations of this class of architectures, not only saves parameters and computational complexity, but also shows improved performance in DT, outperforming both baseline architectures FCRN and CRUSE. Striving for a condition-aware training, we also demonstrate the importance of a high proportion of double-talk and the missing value of nearend-only speech in DAEC training data. Finally, we show how to control the trade-off between aggressive echo suppression and near-end speech preservation by fine-tuning with condition-aware component loss functions.
</details>
<details>
<summary>摘要</summary>
针对深度听音回声控制（DAEC）的话题在过去几年内有很多方法和不同的模型架构被提出。卷积回声网络（CRN），它由卷积编码器和解码器、回声瓶颈部分组成，因其能够在双语（DT）条件下保持近端语音的能力而被广泛采用。然而，过去的架构都是计算复杂或是减少模型大小的代价是性能下降。我们提出了改进的 CRN 架构，与其他类似架构相比，不仅减少参数和计算复杂度，而且在DT条件下表现更好，超过了基eline FCRN 和 CRUSE 架构。为了实现状态感知训练，我们也表明了高比例的双语和近端只有语音在 DAEC 训练数据中的重要性。最后，我们表明了如何通过condition-aware组件损失函数来控制对强制回声消除和近端语音保持的负面交互。
</details></li>
</ul>
<hr>
<h2 id="A-Time-Frequency-Generative-Adversarial-based-method-for-Audio-Packet-Loss-Concealment"><a href="#A-Time-Frequency-Generative-Adversarial-based-method-for-Audio-Packet-Loss-Concealment" class="headerlink" title="A Time-Frequency Generative Adversarial based method for Audio Packet Loss Concealment"></a>A Time-Frequency Generative Adversarial based method for Audio Packet Loss Concealment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15611">http://arxiv.org/abs/2307.15611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aircarlo/bin2bin-gan-plc">https://github.com/aircarlo/bin2bin-gan-plc</a></li>
<li>paper_authors: Carlo Aironi, Samuele Cornell, Luca Serafini, Stefano Squartini</li>
<li>for: 这篇论文 targets the problem of voice quality degradation in VoIP transmissions caused by packet loss, and proposes a generative adversarial approach to repair lost fragments during audio stream transmission.</li>
<li>methods: The proposed method, called bin2bin, is based on an improved pix2pix framework and uses a combination of two STFT-based loss functions and a modified PatchGAN structure as discriminator to translate magnitude spectrograms of audio frames with lost packets to noncorrupted speech spectrograms.</li>
<li>results: Experimental results show that the proposed method has obvious advantages compared to current state-of-the-art methods, particularly in handling high packet loss rates and large gaps.<details>
<summary>Abstract</summary>
Packet loss is a major cause of voice quality degradation in VoIP transmissions with serious impact on intelligibility and user experience. This paper describes a system based on a generative adversarial approach, which aims to repair the lost fragments during the transmission of audio streams. Inspired by the powerful image-to-image translation capability of Generative Adversarial Networks (GANs), we propose bin2bin, an improved pix2pix framework to achieve the translation task from magnitude spectrograms of audio frames with lost packets, to noncorrupted speech spectrograms. In order to better maintain the structural information after spectrogram translation, this paper introduces the combination of two STFT-based loss functions, mixed with the traditional GAN objective. Furthermore, we employ a modified PatchGAN structure as discriminator and we lower the concealment time by a proper initialization of the phase reconstruction algorithm. Experimental results show that the proposed method has obvious advantages when compared with the current state-of-the-art methods, as it can better handle both high packet loss rates and large gaps.
</details>
<details>
<summary>摘要</summary>
packet loss 是 VoIP 传输中音质下降的主要原因，对于智能指南和用户体验产生严重的影响。这篇论文描述了基于生成对抗方法的系统，用于在音频流传输过程中修复丢失的封包。取得了生成对抗网络（GAN）的强大图像对图像翻译能力的灵感，我们提议了bin2bin，一个改进的 pix2pix 框架，用于从丢失封包的音频帧矩阵中翻译到非损音频矩阵。为了更好地保持翻译后的结构信息，这篇论文提出了两个 STFT-based 损失函数的组合，混合传统 GAN 目标。此外，我们使用修改后的 PatchGAN 结构来担任探测器，并通过适当初始化相位重建算法来降低隐藏时间。实验结果表明，提议的方法在与当前状态艺术方法相比有显著优势，可以更好地处理高 packet loss 率和大 gap。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/28/eess.AS_2023_07_28/" data-id="clpahu7af01413h8828vr3u15" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/cs.CV_2023_07_28/" class="article-date">
  <time datetime="2023-07-28T13:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/28/cs.CV_2023_07_28/">cs.CV - 2023-07-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TriadNet-Sampling-free-predictive-intervals-for-lesional-volume-in-3D-brain-MR-images"><a href="#TriadNet-Sampling-free-predictive-intervals-for-lesional-volume-in-3D-brain-MR-images" class="headerlink" title="TriadNet: Sampling-free predictive intervals for lesional volume in 3D brain MR images"></a>TriadNet: Sampling-free predictive intervals for lesional volume in 3D brain MR images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15638">http://arxiv.org/abs/2307.15638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/benolmbrt/TriadNet">https://github.com/benolmbrt/TriadNet</a></li>
<li>paper_authors: Benjamin Lambert, Florence Forbes, Senan Doyle, Michel Dojat</li>
<li>for: 评估脑肿瘤（如血栓或 tumor）的体积是诊断病人状况的重要指标，可以用来导引治疗策略。</li>
<li>methods: 利用深度卷积神经网络（CNN）进行分割，目前是状态体系的方法。</li>
<li>results: 提出了TriadNet方法，可同时提供肿瘤体积和相关预测 интерVAL simultanously，仅需一秒钟。在BraTS 2021大规模 MRI glioblastoma 图像数据库上，TriadNet方法显示出优势。<details>
<summary>Abstract</summary>
The volume of a brain lesion (e.g. infarct or tumor) is a powerful indicator of patient prognosis and can be used to guide the therapeutic strategy. Lesional volume estimation is usually performed by segmentation with deep convolutional neural networks (CNN), currently the state-of-the-art approach. However, to date, few work has been done to equip volume segmentation tools with adequate quantitative predictive intervals, which can hinder their usefulness and acceptation in clinical practice. In this work, we propose TriadNet, a segmentation approach relying on a multi-head CNN architecture, which provides both the lesion volumes and the associated predictive intervals simultaneously, in less than a second. We demonstrate its superiority over other solutions on BraTS 2021, a large-scale MRI glioblastoma image database.
</details>
<details>
<summary>摘要</summary>
病变Volume (例如血栓或肿瘤) 是一个重要的病人 прогностиic indicator，可以用于导引治疗策略。 lesional volume 估计通常使用深度卷积神经网络（CNN）进行，现在是状态的方法。然而，到目前为止，有很少的工作是在 volume segmentation 工具中添加适当的量化预测范围，这会限制它们在临床实践中的使用和接受度。在这种情况下，我们提出了 TriadNet，一种基于多头CNN架构的分割方法，可以同时提供lesion volume和相关的预测范围，并且在一秒钟内完成。我们在 BraTS 2021 大规模 MRI  glioblastoma 图像数据库上展示了它的优势。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Deep-Learning-in-Medical-Image-Registration-New-Technologies-Uncertainty-Evaluation-Metrics-and-Beyond"><a href="#A-Survey-on-Deep-Learning-in-Medical-Image-Registration-New-Technologies-Uncertainty-Evaluation-Metrics-and-Beyond" class="headerlink" title="A Survey on Deep Learning in Medical Image Registration: New Technologies, Uncertainty, Evaluation Metrics, and Beyond"></a>A Survey on Deep Learning in Medical Image Registration: New Technologies, Uncertainty, Evaluation Metrics, and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15615">http://arxiv.org/abs/2307.15615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Chen, Yihao Liu, Shuwen Wei, Zhangxing Bian, Shalini Subramanian, Aaron Carass, Jerry L. Prince, Yong Du</li>
<li>for: 本文提供了深度学习技术在医学图像registratin中的最新进展。</li>
<li>methods: 本文提出了多种新的网络架构、特异度函数和误差估计方法，以及适用于评估深度学习模型在registratin任务中的评价指标。</li>
<li>results: 本文对deep learning-based registratin的应用进行了实质性的探讨，包括多Atlas建构、多Atlas分割、运动估计和2D-3D registratin等。<details>
<summary>Abstract</summary>
Over the past decade, deep learning technologies have greatly advanced the field of medical image registration. The initial developments, such as ResNet-based and U-Net-based networks, laid the groundwork for deep learning-driven image registration. Subsequent progress has been made in various aspects of deep learning-based registration, including similarity measures, deformation regularizations, and uncertainty estimation. These advancements have not only enriched the field of deformable image registration but have also facilitated its application in a wide range of tasks, including atlas construction, multi-atlas segmentation, motion estimation, and 2D-3D registration. In this paper, we present a comprehensive overview of the most recent advancements in deep learning-based image registration. We begin with a concise introduction to the core concepts of deep learning-based image registration. Then, we delve into innovative network architectures, loss functions specific to registration, and methods for estimating registration uncertainty. Additionally, this paper explores appropriate evaluation metrics for assessing the performance of deep learning models in registration tasks. Finally, we highlight the practical applications of these novel techniques in medical imaging and discuss the future prospects of deep learning-based image registration.
</details>
<details>
<summary>摘要</summary>
In this paper, we present a comprehensive overview of the most recent advancements in deep learning-based image registration. We begin with a concise introduction to the core concepts of deep learning-based image registration. Then, we delve into innovative network architectures, loss functions specific to registration, and methods for estimating registration uncertainty. Additionally, this paper explores appropriate evaluation metrics for assessing the performance of deep learning models in registration tasks. Finally, we highlight the practical applications of these novel techniques in medical imaging and discuss the future prospects of deep learning-based image registration.Translated into Simplified Chinese:过去十年，深度学习技术在医疗图像注册领域已经做出了很大的进步。初期的发展，如ResNet基于和U-Net基于的网络，为深度学习驱动的图像注册奠定了基础。后续的进步包括相似度量、形变规范和注册不确定性的估计等方面。这些进步不仅涌现了图像注册领域的多样性，还促进了其应用于多种任务，如建立图像 атла斯、多个 атла斯分割、运动估计和2D-3D注册等。在这篇论文中，我们提供了深度学习基于图像注册的最新进展的全面概述。我们从核心概念的入门开始，然后探讨了新的网络架构、特定于注册的损失函数和注册不确定性的估计方法。此外，这篇论文还探讨了注册任务中深度学习模型的评价指标，以及这些新技术在医疗图像中的实际应用和未来前景。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Digital-Reconstruction-of-Welded-Components-Supporting-Improved-Fatigue-Life-Prediction"><a href="#Integrated-Digital-Reconstruction-of-Welded-Components-Supporting-Improved-Fatigue-Life-Prediction" class="headerlink" title="Integrated Digital Reconstruction of Welded Components: Supporting Improved Fatigue Life Prediction"></a>Integrated Digital Reconstruction of Welded Components: Supporting Improved Fatigue Life Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15604">http://arxiv.org/abs/2307.15604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anders Faarbæk Mikkelstrup, Morten Kristiansen</li>
<li>for: 提高钢结构疲劳性能</li>
<li>methods: 使用自动化高频机械冲击处理，并利用数字重建技术来改进质量和生产效率</li>
<li>results: 实现了成本效果、可重用、灵活和快速的数字重建方法，以援助Component设计、全面质量监测和HFMI处理ocumentation<details>
<summary>Abstract</summary>
In the design of offshore jacket foundations, fatigue life is crucial. Post-weld treatment has been proposed to enhance the fatigue performance of welded joints, where particularly high-frequency mechanical impact (HFMI) treatment has been shown to improve fatigue performance significantly. Automated HFMI treatment has improved quality assurance and can lead to cost-effective design when combined with accurate fatigue life prediction. However, the finite element method (FEM), commonly used for predicting fatigue life in complex or multi-axial joints, relies on a basic CAD depiction of the weld, failing to consider the actual weld geometry and defects. Including the actual weld geometry in the FE model improves fatigue life prediction and possible crack location prediction but requires a digital reconstruction of the weld. Current digital reconstruction methods are time-consuming or require specialised scanning equipment and potential component relocation. The proposed framework instead uses an industrial manipulator combined with a line scanner to integrate digital reconstruction as part of the automated HFMI treatment setup. This approach applies standard image processing, simple filtering techniques, and non-linear optimisation for aligning and merging overlapping scans. A screened Poisson surface reconstruction finalises the 3D model to create a meshed surface. The outcome is a generic, cost-effective, flexible, and rapid method that enables generic digital reconstruction of welded parts, aiding in component design, overall quality assurance, and documentation of the HFMI treatment.
</details>
<details>
<summary>摘要</summary>
在海上钻井基础设计中，疲劳寿命是关键。POST-WELD处理已经被提议来提高焊接缝合的疲劳性能，其中高频机械冲击（HFMI）处理得到了显著提高疲劳性能的结果。自动化HFMI处理可以提高质量控制和可靠性，并可以通过精准的疲劳寿命预测来实现成本效益。但是，通用的Finite Element方法（FEM），常用于预测复杂或多轴缝合的疲劳寿命，基于焊接部的基本CAD描述，而不考虑实际焊接geometry和缺陷。包含实际焊接geometry在FEM模型中可以提高疲劳寿命预测和可能的裂解位置预测，但需要对焊接部进行数字重建。现有的数字重建方法有时间consuming或需要特殊的扫描设备，并且可能需要部件重新位置。提案的框架使用工业机械人与线扫描器结合来集成数字重建作为自动HFMI处理设置的一部分。这种方法使用标准的图像处理、简单的滤波技术和非线性优化来对 overlap的扫描进行协调和拼接。一个屏幕化的波峰表面重建最后生成3D模型，以创建一个可重用、成本效益、灵活的方法，以帮助 generic数字重建焊接部件，以便于组件设计、总质量控制和HFMI处理的文档。
</details></li>
</ul>
<hr>
<h2 id="OAFuser-Towards-Omni-Aperture-Fusion-for-Light-Field-Semantic-Segmentation-of-Road-Scenes"><a href="#OAFuser-Towards-Omni-Aperture-Fusion-for-Light-Field-Semantic-Segmentation-of-Road-Scenes" class="headerlink" title="OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation of Road Scenes"></a>OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation of Road Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15588">http://arxiv.org/abs/2307.15588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feibryantkit/oafuser">https://github.com/feibryantkit/oafuser</a></li>
<li>paper_authors: Fei Teng, Jiaming Zhang, Kunyu Peng, Kailun Yang, Yaonan Wang, Rainer Stiefelhagen</li>
<li>for: 提高自动驾驶场景理解的图像semantic segmentation，利用光场相机提供的丰富的角度和空间信息。</li>
<li>methods: 提出Omni-Aperture Fusion模型（OAFuser），利用中心视图的dense Context和子镜头图像中的角度信息生成semantically-consistent的结果，并提出Sub-Aperture Fusion Module（SAFM）来嵌入子镜头图像到角度特征中，无需额外存储成本。</li>
<li>results: 在UrbanLF-Real和-Syn数据集上达到了当前最佳性能，mIoU达84.93%，与原始数据集上的最佳性能增加+4.53%。<details>
<summary>Abstract</summary>
Light field cameras can provide rich angular and spatial information to enhance image semantic segmentation for scene understanding in the field of autonomous driving. However, the extensive angular information of light field cameras contains a large amount of redundant data, which is overwhelming for the limited hardware resource of intelligent vehicles. Besides, inappropriate compression leads to information corruption and data loss. To excavate representative information, we propose an Omni-Aperture Fusion model (OAFuser), which leverages dense context from the central view and discovers the angular information from sub-aperture images to generate a semantically-consistent result. To avoid feature loss during network propagation and simultaneously streamline the redundant information from the light field camera, we present a simple yet very effective Sub-Aperture Fusion Module (SAFM) to embed sub-aperture images into angular features without any additional memory cost. Furthermore, to address the mismatched spatial information across viewpoints, we present Center Angular Rectification Module (CARM) realized feature resorting and prevent feature occlusion caused by asymmetric information. Our proposed OAFuser achieves state-of-the-art performance on the UrbanLF-Real and -Syn datasets and sets a new record of 84.93% in mIoU on the UrbanLF-Real Extended dataset, with a gain of +4.53%. The source code of OAFuser will be made publicly available at https://github.com/FeiBryantkit/OAFuser.
</details>
<details>
<summary>摘要</summary>
光场相机可以提供丰富的angular和空间信息，以提高自动驾驶场景理解。然而，广泛的angular信息光场相机包含大量的重复数据，对智能汽车硬件资源的限制是过载。此外，不当压缩会导致信息损害和数据损失。为了挖掘代表性信息，我们提出了 Omni-Aperture Fusion 模型（OAFuser），它利用中心视图的 dense context 和子镜像中的angular信息来生成具有semantic consistency的结果。为了避免网络传播过程中的特征损失并同时压缩红外相机中的重复信息，我们提出了一种简单 yet 高效的 Sub-Aperture Fusion Module（SAFM），可以在不添加额外存储成本下将子镜像 embed 到angular特征中。此外，为了 Address the mismatched spatial information across viewpoints，我们提出了 Center Angular Rectification Module（CARM），实现了Feature resorting 和避免了由不同视角信息干扰所致的特征遮挡。我们的提出的 OAFuser 在 UrbanLF-Real 和 UrbanLF-Syn 数据集上达到了状态机器人�왕的性能，并在 UrbanLF-Real Extended 数据集上 achieved 84.93% 的 mIoU 记录，与前一个记录 (+4.53%) 相比。我们将在 GitHub 上发布 OAFuser 的源代码，访问 https://github.com/FeiBryantkit/OAFuser。
</details></li>
</ul>
<hr>
<h2 id="Point-Clouds-Are-Specialized-Images-A-Knowledge-Transfer-Approach-for-3D-Understanding"><a href="#Point-Clouds-Are-Specialized-Images-A-Knowledge-Transfer-Approach-for-3D-Understanding" class="headerlink" title="Point Clouds Are Specialized Images: A Knowledge Transfer Approach for 3D Understanding"></a>Point Clouds Are Specialized Images: A Knowledge Transfer Approach for 3D Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15569">http://arxiv.org/abs/2307.15569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachen Kang, Wenjing Jia, Xiangjian He, Kin Man Lam</li>
<li>for: 这篇论文是针对点云理解进行自我指导的 representation learning (SSRL) 问题，以 addressed 3D 数据缺乏和高notation costs 的挑战。</li>
<li>methods: 这篇论文提出了 PCExpert，一种新的 SSRL 方法，将点云视为 “特殊化的图像”，这个概念Shift 允许 PCExpert 可以更直接地和更深入地利用大规模的图像特征，通过在多种Transformer架构中共享参数。</li>
<li>results: PCExpert 可以在多种任务中表现出色，包括 ScanObjectNN 等，并且只需要训练少量的参数，而且其性能可以与全模型 fine-tuning 的结果相似（92.66%），这表明 PCExpert 具有强大和可靠的表示能力。<details>
<summary>Abstract</summary>
Self-supervised representation learning (SSRL) has gained increasing attention in point cloud understanding, in addressing the challenges posed by 3D data scarcity and high annotation costs. This paper presents PCExpert, a novel SSRL approach that reinterprets point clouds as "specialized images". This conceptual shift allows PCExpert to leverage knowledge derived from large-scale image modality in a more direct and deeper manner, via extensively sharing the parameters with a pre-trained image encoder in a multi-way Transformer architecture. The parameter sharing strategy, combined with a novel pretext task for pre-training, i.e., transformation estimation, empowers PCExpert to outperform the state of the arts in a variety of tasks, with a remarkable reduction in the number of trainable parameters. Notably, PCExpert's performance under LINEAR fine-tuning (e.g., yielding a 90.02% overall accuracy on ScanObjectNN) has already approached the results obtained with FULL model fine-tuning (92.66%), demonstrating its effective and robust representation capability.
</details>
<details>
<summary>摘要</summary>
自适应表示学习（SSRL）在点云理解方面受到越来越多的关注，总是面临3D数据罕见和高标注成本的挑战。本文提出PCExpert，一种新的SSRL方法，它将点云视为“专业化图像”，这种概念转换允许PCExpert在更直接和深入的方式利用大规模图像领域的知识，通过在多种Transformer架构中广泛共享参数和一种新的预测任务来进行预训练。这种参数共享策略，加之预训练中的变换估计任务，使得PCExpert能够超越当前状态的表现，并在多种任务中具有很好的灵活性和稳定性。值得一提的是，PCExpert在线性微调（例如在ScanObjectNN上达到90.02%的总准确率）的性能已经接近了全模型微调（92.66%）的结果，这显示PCExpert的表示能力是有效和可靠的。
</details></li>
</ul>
<hr>
<h2 id="Panoptic-Scene-Graph-Generation-with-Semantics-prototype-Learning"><a href="#Panoptic-Scene-Graph-Generation-with-Semantics-prototype-Learning" class="headerlink" title="Panoptic Scene Graph Generation with Semantics-prototype Learning"></a>Panoptic Scene Graph Generation with Semantics-prototype Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15567">http://arxiv.org/abs/2307.15567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Li, Wei Ji, Yiming Wu, Mengze Li, You Qin, Lina Wei, Roger Zimmermann</li>
<li>for: 提高 PSG 模型在实际应用中的表现，解决固有偏见导致 PSG 模型在构建准确的决策平面上遇到困难。</li>
<li>methods: 提出了一种名为 ADTrans 的新框架，用于适应性地传输偏见 predicate 笔记到有用和统一的笔记。通过保证每个 predicate 类划 representation 的一致性和准确性，学习不偏 predicate 的聚类表示。同时，通过不断测量每个表现与其聚类表示之间的分布变化，不断屏选掉潜在的偏见数据。</li>
<li>results: 实验显示，ADTrans 可以显著提高 benchmark 模型的表现，实现新的州OF-the-art 性能，并在多个数据集上显示出极高的一致性和效果。<details>
<summary>Abstract</summary>
Panoptic Scene Graph Generation (PSG) parses objects and predicts their relationships (predicate) to connect human language and visual scenes. However, different language preferences of annotators and semantic overlaps between predicates lead to biased predicate annotations in the dataset, i.e. different predicates for same object pairs. Biased predicate annotations make PSG models struggle in constructing a clear decision plane among predicates, which greatly hinders the real application of PSG models. To address the intrinsic bias above, we propose a novel framework named ADTrans to adaptively transfer biased predicate annotations to informative and unified ones. To promise consistency and accuracy during the transfer process, we propose to measure the invariance of representations in each predicate class, and learn unbiased prototypes of predicates with different intensities. Meanwhile, we continuously measure the distribution changes between each presentation and its prototype, and constantly screen potential biased data. Finally, with the unbiased predicate-prototype representation embedding space, biased annotations are easily identified. Experiments show that ADTrans significantly improves the performance of benchmark models, achieving a new state-of-the-art performance, and shows great generalization and effectiveness on multiple datasets.
</details>
<details>
<summary>摘要</summary>
全面Scene图生成（PSG）解析物体并预测其关系（ predicate），将语言和视觉场景连接起来。然而，annotator的语言偏好和 predicate的semantic overlap导致数据集中的 predicate 注解受到偏见，例如对象对的不同 predicate。这种偏见使PSG模型在构建准确的决策平面上陷入困难，从而限制PSG模型的实际应用。为了解决上述内在偏见，我们提出了一种名为 ADTrans 的框架，用于自适应地传输偏见 predicate 注解到有用和统一的一个。为保证转移过程中的一致性和准确性，我们提议测量每个 predicate 类别中的表示不变性，并学习不偏 predicate 的 прототипы。同时，我们连续测量每个表现和其prototype之间的分布变化，并持续屏蔽潜在偏见数据。最终，通过不偏 predicate-prototype表示空间，偏见注解得到了轻松地识别。实验表明，ADTrans 可以显著提高 benchmark 模型的性能，达到新的状态对应性，并在多个数据集上显示出极大的一致性和效果。
</details></li>
</ul>
<hr>
<h2 id="Beating-Backdoor-Attack-at-Its-Own-Game"><a href="#Beating-Backdoor-Attack-at-Its-Own-Game" class="headerlink" title="Beating Backdoor Attack at Its Own Game"></a>Beating Backdoor Attack at Its Own Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15539">http://arxiv.org/abs/2307.15539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damianliumin/non-adversarial_backdoor">https://github.com/damianliumin/non-adversarial_backdoor</a></li>
<li>paper_authors: Min Liu, Alberto Sangiovanni-Vincentelli, Xiangyu Yue</li>
<li>for: 防御深度神经网络（DNNs）受到后门攻击，不会影响网络在干净数据上的性能，但会 manipulate 网络行为一旦添加触发模式。</li>
<li>methods: 我们提出了一种简单 yet highly effective 防御框架，通过在恶意样本上注入非对抗性后门来抑制攻击者的后门。</li>
<li>results: 我们在多个 benchmark 上进行了广泛的实验，结果显示我们的方法可以 достичь现状最佳的防御效果，同时具有最低的干净数据性能下降。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network's performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on multiple benchmarks with different architectures and representative attacks. Results demonstrate that our method achieves state-of-the-art defense effectiveness with by far the lowest performance drop on clean data. Considering the surprising defense ability displayed by our framework, we call for more attention to utilizing backdoor for backdoor defense. Code is available at https://github.com/damianliumin/non-adversarial_backdoor.
</details>
<details>
<summary>摘要</summary>
Inspired by the stealthiness and effectiveness of backdoor attacks, we propose a simple but highly effective defense framework that injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attacks, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data.The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on multiple benchmarks with different architectures and representative attacks. Results demonstrate that our method achieves state-of-the-art defense effectiveness with by far the lowest performance drop on clean data.Considering the surprising defense ability displayed by our framework, we call for more attention to utilizing backdoors for backdoor defense. Code is available at https://github.com/damianliumin/non-adversarial_backdoor.
</details></li>
</ul>
<hr>
<h2 id="YOLOv8-for-Defect-Inspection-of-Hexagonal-Directed-Self-Assembly-Patterns-A-Data-Centric-Approach"><a href="#YOLOv8-for-Defect-Inspection-of-Hexagonal-Directed-Self-Assembly-Patterns-A-Data-Centric-Approach" class="headerlink" title="YOLOv8 for Defect Inspection of Hexagonal Directed Self-Assembly Patterns: A Data-Centric Approach"></a>YOLOv8 for Defect Inspection of Hexagonal Directed Self-Assembly Patterns: A Data-Centric Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15516">http://arxiv.org/abs/2307.15516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enrique Dehaerne, Bappaditya Dey, Hossein Esfandiar, Lander Verstraete, Hyo Seon Suh, Sandip Halder, Stefan De Gendt</li>
<li>for: 本文旨在提出一种方法，以便在 Directed self-assembly (DSA)  Patterning 中获得高质量的报告标签，以便用于supervised Machine Learning 模型的准确性检测。</li>
<li>methods: 本文使用了一种基于 Machine Learning 的 SEM 图像分析方法，以便自动检测 DSA  Patterning 中的缺陷。</li>
<li>results: 本文的实验结果表明，使用 YOLOv8  neural network 可以在 DSA  Patterning 中达到精度更高于 0.9 mAP 的缺陷检测精度。<details>
<summary>Abstract</summary>
Shrinking pattern dimensions leads to an increased variety of defect types in semiconductor devices. This has spurred innovation in patterning approaches such as Directed self-assembly (DSA) for which no traditional, automatic defect inspection software exists. Machine Learning-based SEM image analysis has become an increasingly popular research topic for defect inspection with supervised ML models often showing the best performance. However, little research has been done on obtaining a dataset with high-quality labels for these supervised models. In this work, we propose a method for obtaining coherent and complete labels for a dataset of hexagonal contact hole DSA patterns while requiring minimal quality control effort from a DSA expert. We show that YOLOv8, a state-of-the-art neural network, achieves defect detection precisions of more than 0.9 mAP on our final dataset which best reflects DSA expert defect labeling expectations. We discuss the strengths and limitations of our proposed labeling approach and suggest directions for future work in data-centric ML-based defect inspection.
</details>
<details>
<summary>摘要</summary>
缩小模式维度会导致半导体设备中的缺陷类型多样化增加。这种情况推动了半导体 Patterning 的创新，如指导自assembly（DSA），但是传统的自动缺陷检测软件没有适用。机器学习基于 SEM 图像分析已成为半导体缺陷检测的流行研究话题，但是有少量研究关于获得高质量标签的方法。在这种工作中，我们提出一种方法，可以获得 coherent 和完整的标签集，用于半导体 DSA 模式的缺陷检测，而不需要 DSA 专家投入大量时间进行质量控制。我们显示，使用 YOLOv8  neural network，可以在我们的最终数据集上达到缺陷检测精度超过 0.9 mAP，这与 DSA 专家的标签预期相 closest 。我们讨论了我们的标签获取方法的优势和局限性，以及未来在数据驱动的 ML 基于缺陷检测方面的发展方向。
</details></li>
</ul>
<hr>
<h2 id="Improving-Image-Quality-of-Sparse-view-Lung-Cancer-CT-Images-with-a-Convolutional-Neural-Network"><a href="#Improving-Image-Quality-of-Sparse-view-Lung-Cancer-CT-Images-with-a-Convolutional-Neural-Network" class="headerlink" title="Improving Image Quality of Sparse-view Lung Cancer CT Images with a Convolutional Neural Network"></a>Improving Image Quality of Sparse-view Lung Cancer CT Images with a Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15506">http://arxiv.org/abs/2307.15506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Annika Ries, Tina Dorosti, Johannes Thalhammer, Daniel Sasse, Andreas Sauter, Felix Meurer, Ashley Benne, Franz Pfeiffer, Daniela Pfeiffer</li>
<li>For: The paper aims to improve the image quality of sparse-view computed tomography (CT) images for lung cancer detection and to determine the best trade-off between number of views, image quality, and diagnostic confidence.* Methods: The paper uses a U-Net to improve the image quality of sparse-view CT images and evaluates the effectiveness of different levels of undersampling (16, 32, 64, 128, 256, and 512 views) on image quality and diagnostic confidence.* Results: The paper shows that 64-projection sparse-view images result in high image quality and diagnostic confidence, while fewer views lead to insufficient quality. Post-processing the sparse-view images with the U-Net further improves image quality and diagnostic confidence.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 该研究旨在提高 sparse-view CT 图像质量，以便更好地检测肺癌病变，并确定最佳投射视图数量、图像质量和诊断自信的平衡点。</li>
<li>methods: 该研究使用 U-Net 来提高 sparse-view CT 图像质量，并评估不同的投射视图数量（16, 32, 64, 128, 256, 512 个视图）对图像质量和诊断自信的影响。</li>
<li>results: 研究显示，64 投射 sparse-view CT 图像可以保持高质量和诊断自信，而更少的投射视图会导致图像质量下降。通过将 sparse-view CT 图像后处理 U-Net 模型，可以进一步提高图像质量和诊断自信。<details>
<summary>Abstract</summary>
Purpose: To improve the image quality of sparse-view computed tomography (CT) images with a U-Net for lung cancer detection and to determine the best trade-off between number of views, image quality, and diagnostic confidence.   Methods: CT images from 41 subjects (34 with lung cancer, seven healthy) were retrospectively selected (01.2016-12.2018) and forward projected onto 2048-view sinograms. Six corresponding sparse-view CT data subsets at varying levels of undersampling were reconstructed from sinograms using filtered backprojection with 16, 32, 64, 128, 256, and 512 views, respectively. A dual-frame U-Net was trained and evaluated for each subsampling level on 8,658 images from 22 diseased subjects. A representative image per scan was selected from 19 subjects (12 diseased, seven healthy) for a single-blinded reader study. The selected slices, for all levels of subsampling, with and without post-processing by the U-Net model, were presented to three readers. Image quality and diagnostic confidence were ranked using pre-defined scales. Subjective nodule segmentation was evaluated utilizing sensitivity (Se) and Dice Similarity Coefficient (DSC) with 95% confidence intervals (CI).   Results: The 64-projection sparse-view images resulted in Se = 0.89 and DSC = 0.81 [0.75,0.86] while their counterparts, post-processed with the U-Net, had improved metrics (Se = 0.94, DSC = 0.85 [0.82,0.87]). Fewer views lead to insufficient quality for diagnostic purposes. For increased views, no substantial discrepancies were noted between the sparse-view and post-processed images.   Conclusion: Projection views can be reduced from 2048 to 64 while maintaining image quality and the confidence of the radiologists on a satisfactory level.
</details>
<details>
<summary>摘要</summary>
Methods: 从 2016年1月至2018年12月收集到的41名病人（34名有肺癌，7名健康）的 CT 图像，并将它们前向投影到 2048 个视角的信号gram。将这些视角投影到稀畴视角 CT 数据集中，并使用缓冲后投影来重建图像。为每个抽象级别，使用 filtered backprojection 重建图像，并使用 dual-frame U-Net 训练和评估。在 8,658 张图像上进行了 22 名病人的评估。选择了每个扫描的一个代表图像，并在 19 名病人（12 名病人、7 名健康）中选择了一个代表图像。这些选择的slice被presented给三名读者。评估图像质量和诊断自信使用预定的级别。使用敏感度（Se）和 dice 相似度系数（DSC）来评估分割结果，并计算95% 的信任区间（CI）。Results: 64 个视角稀畴视角图像的 Se = 0.89 和 DSC = 0.81 [0.75,0.86]，而其对应的 U-Net 后处理图像的 metric 得到了改善（Se = 0.94, DSC = 0.85 [0.82,0.87]）。 fewer views 不够用于诊断purpose。随着视角数量的增加，没有显著的差异被注意到 между sparse-view 和 U-Net 后处理图像。Conclusion: 可以将 projection views 从 2048 缩减到 64，而不会影响图像质量和诊断人员对图像质量的自信。
</details></li>
</ul>
<hr>
<h2 id="Local-and-Global-Information-in-Obstacle-Detection-on-Railway-Tracks"><a href="#Local-and-Global-Information-in-Obstacle-Detection-on-Railway-Tracks" class="headerlink" title="Local and Global Information in Obstacle Detection on Railway Tracks"></a>Local and Global Information in Obstacle Detection on Railway Tracks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15478">http://arxiv.org/abs/2307.15478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Brucker, Andrei Cramariuc, Cornelius von Einem, Roland Siegwart, Cesar Cadena</li>
<li>for: 避免铁路交通事故，提高列车安全性。</li>
<li>methods: 利用浅网络学习铁路分割，采用局部感知和控制global信息。</li>
<li>results: 比基eline方法高效，在自定义铁路图像集上评估。<details>
<summary>Abstract</summary>
Reliable obstacle detection on railways could help prevent collisions that result in injuries and potentially damage or derail the train. Unfortunately, generic object detectors do not have enough classes to account for all possible scenarios, and datasets featuring objects on railways are challenging to obtain. We propose utilizing a shallow network to learn railway segmentation from normal railway images. The limited receptive field of the network prevents overconfident predictions and allows the network to focus on the locally very distinct and repetitive patterns of the railway environment. Additionally, we explore the controlled inclusion of global information by learning to hallucinate obstacle-free images. We evaluate our method on a custom dataset featuring railway images with artificially augmented obstacles. Our proposed method outperforms other learning-based baseline methods.
</details>
<details>
<summary>摘要</summary>
可靠的铁路障碍检测可以帮助避免因collision而导致的伤害和可能地损坏或脱轨列车。然而，通用对象探测器并不具备足够的类型来覆盖所有可能的场景，而 dataset featuring objects on railways 也是具有挑战性的。我们提议利用一个浅网络学习铁路分割 FROM normal railway images。网络的有限接受区域防止过于自信的预测，allowing the network to focus on the locally very distinct and repetitive patterns of the railway environment。此外，我们还探索控制了包含全局信息的学习方法，通过学习生成障碍物free images。我们对自定义的 dataset featuring railway images with artificially augmented obstacles 进行评估，并证明了我们的提议方法在其他学习基础方法的比较中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Defocus-Blur-Synthesis-and-Deblurring-via-Interpolation-and-Extrapolation-in-Latent-Space"><a href="#Defocus-Blur-Synthesis-and-Deblurring-via-Interpolation-and-Extrapolation-in-Latent-Space" class="headerlink" title="Defocus Blur Synthesis and Deblurring via Interpolation and Extrapolation in Latent Space"></a>Defocus Blur Synthesis and Deblurring via Interpolation and Extrapolation in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15461">http://arxiv.org/abs/2307.15461</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nis-research/linear-latent-blur">https://github.com/nis-research/linear-latent-blur</a></li>
<li>paper_authors: Ioana Mazilu, Shunxin Wang, Sven Dummer, Raymond Veldhuis, Christoph Brune, Nicola Strisciuglio</li>
<li>for: 这篇论文的目的是提高微scopic图像的质量，以便进一步的处理和分析疾病。</li>
<li>methods: 该论文提出了一种方法，可以对图像进行恢复和合成不同程度的模糊效果。该方法使用自适应卷积神经网络，并采用了隐式和显式正则化技术来强制抽象关系在准确空间中的线性关系。</li>
<li>results: 该方法可以有效地模拟不同程度的模糊效果，从而提高数据的多样性，并提高微scopic图像的质量。这种方法可以作为数据增强技术，并且可以提高疾病的诊断和分析。<details>
<summary>Abstract</summary>
Though modern microscopes have an autofocusing system to ensure optimal focus, out-of-focus images can still occur when cells within the medium are not all in the same focal plane, affecting the image quality for medical diagnosis and analysis of diseases. We propose a method that can deblur images as well as synthesize defocus blur. We train autoencoders with implicit and explicit regularization techniques to enforce linearity relations among the representations of different blur levels in the latent space. This allows for the exploration of different blur levels of an object by linearly interpolating/extrapolating the latent representations of images taken at different focal planes. Compared to existing works, we use a simple architecture to synthesize images with flexible blur levels, leveraging the linear latent space. Our regularized autoencoders can effectively mimic blur and deblur, increasing data variety as a data augmentation technique and improving the quality of microscopic images, which would be beneficial for further processing and analysis.
</details>
<details>
<summary>摘要</summary>
现代微镜已经搭载了自动对焦系统，但是在细胞medium中不同的细胞不在同一个 фокус平面上，可能导致图像质量下降，影响医学诊断和疾病分析。我们提出了一种方法，可以恢复图像和生成杂推模灵。我们使用自动编码器，并通过显式和隐式正则化技术来强制抽象关系在 latent space 中的线性关系。这允许我们通过线性 interpolate/extrapolate latent representation来探索不同的杂推模灵水平。与现有方法相比，我们使用简单的架构来生成具有灵活杂推模灵的图像，利用 latent space 的线性性。我们的正则化自动编码器可以有效地模拟杂推模灵和恢复，增加数据的多样性，并提高微镜图像的质量，这将对进一步处理和分析产生有利影响。
</details></li>
</ul>
<hr>
<h2 id="ERCPMP-An-Endoscopic-Image-and-Video-Dataset-for-Colorectal-Polyps-Morphology-and-Pathology"><a href="#ERCPMP-An-Endoscopic-Image-and-Video-Dataset-for-Colorectal-Polyps-Morphology-and-Pathology" class="headerlink" title="ERCPMP: An Endoscopic Image and Video Dataset for Colorectal Polyps Morphology and Pathology"></a>ERCPMP: An Endoscopic Image and Video Dataset for Colorectal Polyps Morphology and Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15444">http://arxiv.org/abs/2307.15444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojgan Forootan, Mohsen Rajabnia, Ahmad R Mafi, Hamed Azhdari Tehrani, Erfan Ghadirzadeh, Mahziar Setayeshfar, Zahra Ghaffari, Mohammad Tashakoripour, Mohammad Reza Zali, Hamidreza Bolhasani</li>
<li>For: The paper is written for developing accurate algorithms for medical prediction, detection, diagnosis, treatment, and prognosis, specifically for colorectal polyps.* Methods: The paper uses a dataset called ERCPMP, which contains demographic, morphological, and pathological data, endoscopic images, and videos of 191 patients with colorectal polyps. The dataset includes data on the diagnosis of the polyps, such as Tubular, Villous, Tubulovillous, Hyperplastic, Serrated, Inflammatory, and Adenocarcinoma with Dysplasia Grade &amp; Differentiation.* Results: The paper provides a dataset that can be used for developing accurate algorithms for medical prediction, detection, diagnosis, treatment, and prognosis of colorectal polyps. The dataset includes a wide range of data, including demographic, morphological, and pathological data, endoscopic images, and videos, which can be used to train and test machine learning and deep learning models.Here is the information in Simplified Chinese text:* 用途：这篇论文用于开发准确的医学预测、检测、诊断、治疗和预后预测算法，特别是对于肠Rectal 肿瘤。* 方法：论文使用一个名为ERCPMP的Endoscopic Image和Video Dataset，该Dataset包括191名患者的肠Rectal 肿瘤的人口统计、形态学数据、病理学数据、Endoscopic 图像和视频。该Dataset包括肿瘤的诊断，如管状、 villous、 tubulovillous、 hyperplastic、 serrated、 inflammatory 和adenocarcinoma with dysplasia grade &amp; differentiation。* 结果：论文提供了一个可以用于开发准确的医学预测、检测、诊断、治疗和预后预测算法的Dataset。该Dataset包括词语、形态学数据、病理学数据、Endoscopic 图像和视频，可以用于训练和测试机器学习和深度学习模型。<details>
<summary>Abstract</summary>
In the recent years, artificial intelligence (AI) and its leading subtypes, machine learning (ML) and deep learning (DL) and their applications are spreading very fast in various aspects such as medicine. Today the most important challenge of developing accurate algorithms for medical prediction, detection, diagnosis, treatment and prognosis is data. ERCPMP is an Endoscopic Image and Video Dataset for Recognition of Colorectal Polyps Morphology and Pathology. This dataset contains demographic, morphological and pathological data, endoscopic images and videos of 191 patients with colorectal polyps. Morphological data is included based on the latest international gastroenterology classification references such as Paris, Pit and JNET classification. Pathological data includes the diagnosis of the polyps including Tubular, Villous, Tubulovillous, Hyperplastic, Serrated, Inflammatory and Adenocarcinoma with Dysplasia Grade & Differentiation. The current version of this dataset is published and available on Elsevier Mendeley Dataverse and since it is under development, the latest version is accessible via: https://databiox.com.
</details>
<details>
<summary>摘要</summary>
recent 年们，人工智能（AI）和其主要子类型，机器学习（ML）和深度学习（DL）以及其应用在各个领域都在快速扩散，其中医学领域也是如此。 currently, the most important challenge of developing accurate algorithms for medical prediction, detection, diagnosis, treatment, and prognosis is data. ERCPMP 是一个 Endoscopic Image and Video Dataset for Recognition of Colorectal Polyps Morphology and Pathology。 This dataset contains demographic, morphological, and pathological data, endoscopic images and videos of 191 patients with colorectal polyps. Morphological data is based on the latest international gastroenterology classification references such as Paris, Pit, and JNET classification. Pathological data includes the diagnosis of the polyps, including Tubular, Villous, Tubulovillous, Hyperplastic, Serrated, Inflammatory, and Adenocarcinoma with Dysplasia Grade & Differentiation. The current version of this dataset is published and available on Elsevier Mendeley Dataverse, and the latest version can be accessed via: <https://databiox.com>.
</details></li>
</ul>
<hr>
<h2 id="Automated-Visual-Monitoring-of-Nocturnal-Insects-with-Light-based-Camera-Traps"><a href="#Automated-Visual-Monitoring-of-Nocturnal-Insects-with-Light-based-Camera-Traps" class="headerlink" title="Automated Visual Monitoring of Nocturnal Insects with Light-based Camera Traps"></a>Automated Visual Monitoring of Nocturnal Insects with Light-based Camera Traps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15433">http://arxiv.org/abs/2307.15433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitri Korsch, Paul Bodesheim, Gunnar Brehm, Joachim Denzler</li>
<li>for: 这个论文的目的是为了提供一个自动化的摄像头辅助的蜥蜉数量估计方法，以便更好地理解和对抗现在的蜥蜉减少趋势。</li>
<li>methods: 这个论文使用了一个两stage的检测和分类管道，使用了公民科学家手动捕捉的EU-Moths数据集，并对其进行了训练和评估。此外，它还介绍了一个自动化视觉监测系统的原型，并对其进行了评估。</li>
<li>results: 这个论文提供了这两个数据集的第一个检测和分类基线，并鼓励其他科学家使用这些公共可用的数据进行进一步的研究。<details>
<summary>Abstract</summary>
Automatic camera-assisted monitoring of insects for abundance estimations is crucial to understand and counteract ongoing insect decline. In this paper, we present two datasets of nocturnal insects, especially moths as a subset of Lepidoptera, photographed in Central Europe. One of the datasets, the EU-Moths dataset, was captured manually by citizen scientists and contains species annotations for 200 different species and bounding box annotations for those. We used this dataset to develop and evaluate a two-stage pipeline for insect detection and moth species classification in previous work. We further introduce a prototype for an automated visual monitoring system. This prototype produced the second dataset consisting of more than 27,000 images captured on 95 nights. For evaluation and bootstrapping purposes, we annotated a subset of the images with bounding boxes enframing nocturnal insects. Finally, we present first detection and classification baselines for these datasets and encourage other scientists to use this publicly available data.
</details>
<details>
<summary>摘要</summary>
自动摄像头助记 insect 数量的估计是理解和逆转正在进行的昆虫衰退的关键。在这篇论文中，我们介绍了中欧地区的两个昆虫数据集。其中一个数据集是由公民科学家手动捕捉的 EU-Moths 数据集，包含了 200 种不同物种的标注和 bounding box 标注。我们在过去的工作中使用了这个数据集来开发和评估一种昆虫检测和蛾类种类分类的两个阶段管道。我们还介绍了一种自动视觉监测系统的原型，这个原型在 95 个夜晚中拍摄了 более 27,000 张图像。为评估和启动目的，我们将一 subset 的图像标注为涵盖夜晚昆虫的 bounding box。最后，我们展示了这些数据集的第一个检测和分类基线，并邀请其他科学家使用这些公共可用的数据进行研究。
</details></li>
</ul>
<hr>
<h2 id="Implicit-neural-representation-for-change-detection"><a href="#Implicit-neural-representation-for-change-detection" class="headerlink" title="Implicit neural representation for change detection"></a>Implicit neural representation for change detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15428">http://arxiv.org/abs/2307.15428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Naylor, Diego Di Carlo, Arianna Traviglia, Makoto Yamada, Marco Fiorucci</li>
<li>for: 检测三维空间飞行LiDAR点云中发生的变化，特别是因为不匹配的空间支持和采集系统噪声。</li>
<li>methods: 我们提出了一种无监督方法，包括两个组成部分：神经场（NF） для连续形态重建和高斯混合模型 для分类变化。NF提供了不固定格式的表示方式，可以增加高频环境和减少噪声。</li>
<li>results: 我们在一个 benchmark 数据集上进行了测试，并证明了我们的方法可以与当前状态的艺术之前提高检测能力。此外，我们还应用了我们的方法于一个实际场景，并证明了它们与场景专家的发现相符。<details>
<summary>Abstract</summary>
Detecting changes that occurred in a pair of 3D airborne LiDAR point clouds, acquired at two different times over the same geographical area, is a challenging task because of unmatching spatial supports and acquisition system noise. Most recent attempts to detect changes on point clouds are based on supervised methods, which require large labelled data unavailable in real-world applications. To address these issues, we propose an unsupervised approach that comprises two components: Neural Field (NF) for continuous shape reconstruction and a Gaussian Mixture Model for categorising changes. NF offer a grid-agnostic representation to encode bi-temporal point clouds with unmatched spatial support that can be regularised to increase high-frequency details and reduce noise. The reconstructions at each timestamp are compared at arbitrary spatial scales, leading to a significant increase in detection capabilities. We apply our method to a benchmark dataset of simulated LiDAR point clouds for urban sprawling. The dataset offers different challenging scenarios with different resolutions, input modalities and noise levels, allowing a multi-scenario comparison of our method with the current state-of-the-art. We boast the previous methods on this dataset by a 10% margin in intersection over union metric. In addition, we apply our methods to a real-world scenario to identify illegal excavation (looting) of archaeological sites and confirm that they match findings from field experts.
</details>
<details>
<summary>摘要</summary>
检测两个3D空中探测点云之间的变化是一项具有挑战性的任务，因为这两个点云在不同的时间被获取，并且具有不匹配的空间支持和探测系统噪声。大多数最新的变化检测方法基于指导方法，需要大量的标注数据，这些数据在实际应用中很难获得。为了解决这些问题，我们提出了一种不supervised方法，它包括两个组成部分：神经场（NF）和高斯混合模型。NF提供了不受格子限制的表示方式，用于编码不匹配的时间点云，并可以通过增强高频率细节和减少噪声来增强高精度。在每个时间戳点上对重建的点云进行比较，可以在不同的空间缩放比例上进行比较，从而大幅提高检测能力。我们在一个 simulate LiDAR点云 benchmark dataset上应用了我们的方法，该dataset包括不同的具有不同分辨率、输入模式和噪声水平的场景。我们在这些场景中跟比现状态的方法，增加了10%的交叉部分精度。此外，我们还应用了我们的方法于一个实际场景，即考古遗产挖掘（looting），并证明与场地专家的发现相匹配。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Pipeline-for-Automated-Visual-Moth-Monitoring-Insect-Localization-and-Species-Classification"><a href="#Deep-Learning-Pipeline-for-Automated-Visual-Moth-Monitoring-Insect-Localization-and-Species-Classification" class="headerlink" title="Deep Learning Pipeline for Automated Visual Moth Monitoring: Insect Localization and Species Classification"></a>Deep Learning Pipeline for Automated Visual Moth Monitoring: Insect Localization and Species Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15427">http://arxiv.org/abs/2307.15427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitri Korsch, Paul Bodesheim, Joachim Denzler</li>
<li>for: 本研究旨在开发一个基于深度学习的苹果蛾自动识别系统，以帮助生物多样性监测。</li>
<li>methods: 本研究使用了一个基于蛾虫检测器和分类器的深度学习管线来分析蛾虫扫描仪上的图像。</li>
<li>results: 研究表明，将检测器和分类器结合使用可以提高蛾虫图像标识率从79.62%提高到88.05%。<details>
<summary>Abstract</summary>
Biodiversity monitoring is crucial for tracking and counteracting adverse trends in population fluctuations. However, automatic recognition systems are rarely applied so far, and experts evaluate the generated data masses manually. Especially the support of deep learning methods for visual monitoring is not yet established in biodiversity research, compared to other areas like advertising or entertainment. In this paper, we present a deep learning pipeline for analyzing images captured by a moth scanner, an automated visual monitoring system of moth species developed within the AMMOD project. We first localize individuals with a moth detector and afterward determine the species of detected insects with a classifier. Our detector achieves up to 99.01% mean average precision and our classifier distinguishes 200 moth species with an accuracy of 93.13% on image cutouts depicting single insects. Combining both in our pipeline improves the accuracy for species identification in images of the moth scanner from 79.62% to 88.05%.
</details>
<details>
<summary>摘要</summary>
生物多样性监测是追踪和抵消人口波动的关键。然而，自动识别系统 rarely 被应用，专家们仍然手动评估生成的数据量。特别是在生物多样性研究中，深度学习方法的视觉监测支持还没有得到广泛应用，相比其他领域如广告或娱乐业。本文提出了一个深度学习管道，用于分析由 moth scanner 捕捉的图像。我们首先使用 moth 检测器来 Localize 检测到的 insects，然后使用分类器来确定检测到的昆虫种类。我们的检测器可以达到 99.01% 的平均精度，分类器可以在单个昆虫图像中分类出 200 种昆虫，准确率为 93.13%。将两个模块结合在一起可以提高图像中的种类鉴定精度，从 79.62% 提高到 88.05%。
</details></li>
</ul>
<hr>
<h2 id="MLIC-Linear-Complexity-Multi-Reference-Entropy-Modeling-for-Learned-Image-Compression"><a href="#MLIC-Linear-Complexity-Multi-Reference-Entropy-Modeling-for-Learned-Image-Compression" class="headerlink" title="MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression"></a>MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15421">http://arxiv.org/abs/2307.15421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiangweibeta/mlic">https://github.com/jiangweibeta/mlic</a></li>
<li>paper_authors: Wei Jiang, Ronggang Wang</li>
<li>for: 这个论文是为了提出一种基于多参考 entropy 模型的学习型图像压缩方法，以提高图像压缩的效率和质量。</li>
<li>methods: 该方法使用 linear complexity 来捕捉全局相关性，而不是之前的 attention 方法，以降低复杂性。具体来说，它使用了 softmax  decomposion 来实现 linear complexity 的捕捉。</li>
<li>results:  compared to VTM-17.0，这种 MLIC$^{++}$ 方法可以提供12.44%的BD-rate 下降，并且在 PSNR 上具有较高的效率。Here’s the translation in English:</li>
<li>for: This paper proposes a learned image compression method based on multi-reference entropy modeling, to improve the efficiency and quality of image compression.</li>
<li>methods: The method uses linear complexity to capture global correlations, instead of the previous attention method, to reduce complexity. Specifically, it uses softmax decomposition to achieve linear complexity.</li>
<li>results: Compared to VTM-17.0, the proposed MLIC$^{++}$ method can provide a 12.44% reduction in BD-rate and higher efficiency in PSNR.<details>
<summary>Abstract</summary>
Recently, multi-reference entropy model has been proposed, which captures channel-wise, local spatial, and global spatial correlations. Previous works adopt attention for global correlation capturing, however, the quadratic cpmplexity limits the potential of high-resolution image coding. In this paper, we propose the linear complexity global correlations capturing, via the decomposition of softmax operation. Based on it, we propose the MLIC$^{++}$, a learned image compression with linear complexity for multi-reference entropy modeling. Our MLIC$^{++}$ is more efficient and it reduces BD-rate by 12.44% on the Kodak dataset compared to VTM-17.0 when measured in PSNR. Code will be available at https://github.com/JiangWeibeta/MLIC.
</details>
<details>
<summary>摘要</summary>
最近，多参照 entropy 模型已经被提出，它捕捉了通道 wise、本地空间和全局空间相关性。先前的工作采用了注意力来捕捉全局相关性，但 quadratic complexity 限制了高分辨率图像编码的潜力。在这篇文章中，我们提出了线性复杂度全局相关性捕捉，通过软MAX操作的分解。基于其，我们提出了 MLIC$^{++} $，一种学习图像压缩的线性复杂度多参照 entropy 模型。我们的 MLIC$^{++} $ 比 VTM-17.0 在 PSNR 下降12.44% 的 Kodak 数据集上更高效，代码将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-Unsupervised-Multi-Object-Tracking"><a href="#Uncertainty-aware-Unsupervised-Multi-Object-Tracking" class="headerlink" title="Uncertainty-aware Unsupervised Multi-Object Tracking"></a>Uncertainty-aware Unsupervised Multi-Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15409">http://arxiv.org/abs/2307.15409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Liu, Sheng Jin, Zhihang Fu, Ze Chen, Rongxin Jiang, Jieping Ye</li>
<li>for: 提高无监督多 объек tracking的性能</li>
<li>methods: 采用自我监督技术，并开发了一个uncertainty-based metric来验证和修正危险关系</li>
<li>results: 实现了高性能的无监督多对象跟踪，并在MOT-Challenges和VisDrone-MOT benchmark上达到了最高级别的表现<details>
<summary>Abstract</summary>
Without manually annotated identities, unsupervised multi-object trackers are inferior to learning reliable feature embeddings. It causes the similarity-based inter-frame association stage also be error-prone, where an uncertainty problem arises. The frame-by-frame accumulated uncertainty prevents trackers from learning the consistent feature embedding against time variation. To avoid this uncertainty problem, recent self-supervised techniques are adopted, whereas they failed to capture temporal relations. The interframe uncertainty still exists. In fact, this paper argues that though the uncertainty problem is inevitable, it is possible to leverage the uncertainty itself to improve the learned consistency in turn. Specifically, an uncertainty-based metric is developed to verify and rectify the risky associations. The resulting accurate pseudo-tracklets boost learning the feature consistency. And accurate tracklets can incorporate temporal information into spatial transformation. This paper proposes a tracklet-guided augmentation strategy to simulate tracklets' motion, which adopts a hierarchical uncertainty-based sampling mechanism for hard sample mining. The ultimate unsupervised MOT framework, namely U2MOT, is proven effective on MOT-Challenges and VisDrone-MOT benchmark. U2MOT achieves a SOTA performance among the published supervised and unsupervised trackers.
</details>
<details>
<summary>摘要</summary>
Without manually annotated identities, unsupervised multi-object trackers are inferior to learning reliable feature embeddings. This causes the similarity-based inter-frame association stage to also be error-prone, resulting in an uncertainty problem. The frame-by-frame accumulated uncertainty prevents trackers from learning the consistent feature embedding against time variation. To avoid this uncertainty problem, recent self-supervised techniques are adopted, but they failed to capture temporal relations. The interframe uncertainty still exists. In fact, this paper argues that though the uncertainty problem is inevitable, it is possible to leverage the uncertainty itself to improve the learned consistency in turn. Specifically, an uncertainty-based metric is developed to verify and rectify the risky associations. The resulting accurate pseudo-tracklets boost learning the feature consistency. And accurate tracklets can incorporate temporal information into spatial transformation. This paper proposes a tracklet-guided augmentation strategy to simulate tracklets' motion, which adopts a hierarchical uncertainty-based sampling mechanism for hard sample mining. The ultimate unsupervised MOT framework, namely U2MOT, is proven effective on MOT-Challenges and VisDrone-MOT benchmark. U2MOT achieves a SOTA performance among the published supervised and unsupervised trackers.
</details></li>
</ul>
<hr>
<h2 id="Task-Oriented-Channel-Attention-for-Fine-Grained-Few-Shot-Classification"><a href="#Task-Oriented-Channel-Attention-for-Fine-Grained-Few-Shot-Classification" class="headerlink" title="Task-Oriented Channel Attention for Fine-Grained Few-Shot Classification"></a>Task-Oriented Channel Attention for Fine-Grained Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00093">http://arxiv.org/abs/2308.00093</a></li>
<li>repo_url: None</li>
<li>paper_authors: SuBeen Lee, WonJun Moon, Hyun Seok Seong, Jae-Pil Heo</li>
<li>for:  fine-grained image classification with limited training data</li>
<li>methods:  Task Discrepancy Maximization (TDM) with Support Attention Module (SAM) and Query Attention Module (QAM)</li>
<li>results:  accurate class-sensitive similarity measure and instance-wise highlighting of object-relevant channels<details>
<summary>Abstract</summary>
The difficulty of the fine-grained image classification mainly comes from a shared overall appearance across classes. Thus, recognizing discriminative details, such as eyes and beaks for birds, is a key in the task. However, this is particularly challenging when training data is limited. To address this, we propose Task Discrepancy Maximization (TDM), a task-oriented channel attention method tailored for fine-grained few-shot classification with two novel modules Support Attention Module (SAM) and Query Attention Module (QAM). SAM highlights channels encoding class-wise discriminative features, while QAM assigns higher weights to object-relevant channels of the query. Based on these submodules, TDM produces task-adaptive features by focusing on channels encoding class-discriminative details and possessed by the query at the same time, for accurate class-sensitive similarity measure between support and query instances. While TDM influences high-level feature maps by task-adaptive calibration of channel-wise importance, we further introduce Instance Attention Module (IAM) operating in intermediate layers of feature extractors to instance-wisely highlight object-relevant channels, by extending QAM. The merits of TDM and IAM and their complementary benefits are experimentally validated in fine-grained few-shot classification tasks. Moreover, IAM is also shown to be effective in coarse-grained and cross-domain few-shot classifications.
</details>
<details>
<summary>摘要</summary>
Fine-grained图像分类的困难主要来自于类别之间共同的整体外观。因此，认izable找到分类特征，如鸟类的眼睛和嘴，是关键。然而，当训练数据有限时，这变得非常困难。为 Addressing this challenge, we propose Task Discrepancy Maximization (TDM), a task-oriented channel attention method tailored for fine-grained few-shot classification with two novel modules Support Attention Module (SAM) and Query Attention Module (QAM). SAM highlights channels encoding class-wise discriminative features, while QAM assigns higher weights to object-relevant channels of the query. Based on these submodules, TDM produces task-adaptive features by focusing on channels encoding class-discriminative details and possessed by the query at the same time, for accurate class-sensitive similarity measure between support and query instances. While TDM influences high-level feature maps by task-adaptive calibration of channel-wise importance, we further introduce Instance Attention Module (IAM) operating in intermediate layers of feature extractors to instance-wisely highlight object-relevant channels, by extending QAM. The merits of TDM and IAM and their complementary benefits are experimentally validated in fine-grained few-shot classification tasks. Moreover, IAM is also shown to be effective in coarse-grained and cross-domain few-shot classifications.
</details></li>
</ul>
<hr>
<h2 id="AffineGlue-Joint-Matching-and-Robust-Estimation"><a href="#AffineGlue-Joint-Matching-and-Robust-Estimation" class="headerlink" title="AffineGlue: Joint Matching and Robust Estimation"></a>AffineGlue: Joint Matching and Robust Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15381">http://arxiv.org/abs/2307.15381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Barath, Dmytro Mishkin, Luca Cavalli, Paul-Edouard Sarlin, Petr Hruby, Marc Pollefeys</li>
<li>for: 本文提出了AffineGlue方法，用于 JOINT two-view 特征匹配和稳定估计，从而减少了问题的可能性级别。</li>
<li>methods: AffineGlue 使用单点最小解方法选择可能的匹配，并使用导航匹配来找到与模型相符的匹配。此外，我们还提出了一种新的 minimal solver for homography estimation，只需要一个 affine correspondence (AC) 和一个重力优先级。</li>
<li>results: AffineGlue 在实际 dataset 上表现优于 SOTA，即使假设重力方向下降。在 PhotoTourism 上，AUC@10° 分数提高了6.6个点 compared to SOTA。在 ScanNet 上，AffineGlue 使得 SuperPoint 和 SuperGlue 与无探测 LoFTR  achieve 类似的准确率。<details>
<summary>Abstract</summary>
We propose AffineGlue, a method for joint two-view feature matching and robust estimation that reduces the combinatorial complexity of the problem by employing single-point minimal solvers. AffineGlue selects potential matches from one-to-many correspondences to estimate minimal models. Guided matching is then used to find matches consistent with the model, suffering less from the ambiguities of one-to-one matches. Moreover, we derive a new minimal solver for homography estimation, requiring only a single affine correspondence (AC) and a gravity prior. Furthermore, we train a neural network to reject ACs that are unlikely to lead to a good model. AffineGlue is superior to the SOTA on real-world datasets, even when assuming that the gravity direction points downwards. On PhotoTourism, the AUC@10{\deg} score is improved by 6.6 points compared to the SOTA. On ScanNet, AffineGlue makes SuperPoint and SuperGlue achieve similar accuracy as the detector-free LoFTR.
</details>
<details>
<summary>摘要</summary>
我们提出了AffineGlue方法，它是一种能够同时实现二视图特征匹配和稳定估计的方法，通过单点最小解决方案来减少问题的 combinatorial 复杂性。AffineGlue选择一个可能的匹配点，并使用导向匹配来找到与模型一致的匹配。此外，我们还 derivated一种新的单 Affine 匹配（AC）的 homography 估计方法，只需要一个Affine对应性（AC）和重力 prior。此外，我们还训练了一个神经网络来拒绝不可能导致良好模型的AC。Compared to the state-of-the-art（SOTA），AffineGlue在实际数据集上表现更优异，即使gravity方向下降。在PhotoTourism上，AffineGlue在10度的AUC得分上提高了6.6分，比SOTA更高。在ScanNet上，AffineGlue使得SuperPoint和SuperGlue达到了无检测器LoFTR的同等准确率。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Guided-Transformer-for-Multi-Task-Dense-Prediction"><a href="#Prompt-Guided-Transformer-for-Multi-Task-Dense-Prediction" class="headerlink" title="Prompt Guided Transformer for Multi-Task Dense Prediction"></a>Prompt Guided Transformer for Multi-Task Dense Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15362">http://arxiv.org/abs/2307.15362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Lu, Shalayiding Sirejiding, Yue Ding, Chunlin Wang, Hongtao Lu</li>
<li>for: 本文targets the problem of trading off performance and model parameters in task-conditional architecture, and proposes a simple and lightweight task-conditional model called Prompt Guided Transformer (PGT) to optimize this challenge.</li>
<li>methods: 本文提出了一种名为Prompt-conditioned Transformer block的新块，该块在自我注意机制中加入了任务特定的提示，以实现全球相互关系模型和参数效率的特征适应。此外，本文还提出了一种轻量级的解码器，以降低参数数量。</li>
<li>results: EXTENSIVE experiments on two multi-task dense prediction benchmarks, PASCAL-Context and NYUD-v2, show that our approach achieves state-of-the-art results among task-conditional methods while using fewer parameters, and maintains a significant balance between performance and parameter size.<details>
<summary>Abstract</summary>
Task-conditional architecture offers advantage in parameter efficiency but falls short in performance compared to state-of-the-art multi-decoder methods. How to trade off performance and model parameters is an important and difficult problem. In this paper, we introduce a simple and lightweight task-conditional model called Prompt Guided Transformer (PGT) to optimize this challenge. Our approach designs a Prompt-conditioned Transformer block, which incorporates task-specific prompts in the self-attention mechanism to achieve global dependency modeling and parameter-efficient feature adaptation across multiple tasks. This block is integrated into both the shared encoder and decoder, enhancing the capture of intra- and inter-task features. Moreover, we design a lightweight decoder to further reduce parameter usage, which accounts for only 2.7% of the total model parameters. Extensive experiments on two multi-task dense prediction benchmarks, PASCAL-Context and NYUD-v2, demonstrate that our approach achieves state-of-the-art results among task-conditional methods while using fewer parameters, and maintains a significant balance between performance and parameter size.
</details>
<details>
<summary>摘要</summary>
任务条件架构具有参数效率优势，但在性能方面与现有多decoder方法相比，表现略为下降。在这篇论文中，我们提出了一种简单且轻量级的任务条件模型，即提示导向 transformer（PGT），以优化这个挑战。我们的方法设计了一个任务特定提示块，将任务特定的提示 incorporated 在自我注意机制中，以实现全局依赖关系和参数效率的特征适应。这个块在共享encoder和decoder中都有所整合，从而提高了内部和外部任务特征的捕捉。此外，我们还设计了一个轻量级decoder，以进一步减少参数使用量，这个decoder占总模型参数的2.7%。我们在两个多任务稠密预测 benchmark，PASCAL-Context和NYUD-v2，进行了广泛的实验，结果表明，我们的方法在任务条件方法中实现了最佳的结果，同时具有更好的参数大小协调。
</details></li>
</ul>
<hr>
<h2 id="Supervised-Homography-Learning-with-Realistic-Dataset-Generation"><a href="#Supervised-Homography-Learning-with-Realistic-Dataset-Generation" class="headerlink" title="Supervised Homography Learning with Realistic Dataset Generation"></a>Supervised Homography Learning with Realistic Dataset Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15353">http://arxiv.org/abs/2307.15353</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jianghaiscu/realsh">https://github.com/jianghaiscu/realsh</a></li>
<li>paper_authors: Hai Jiang, Haipeng Li, Songchen Han, Haoqiang Fan, Bing Zeng, Shuaicheng Liu</li>
<li>For: 提出一种迭代框架，包括两个阶段：生成阶段和训练阶段，用于生成真实的训练数据并提取一个监督的投影网络。* Methods: 使用预估的主导平面屏障和投影对一个无标签图像对来生成一个新的标注过的训练对，并使用这些生成的数据进行训练监督投影网络。在训练阶段，使用内容一致模块和质量评估模块来进行数据的细化和评估。* Results: 实验结果表明，我们的方法可以达到现有最佳性能，并可以在生成的数据集上提高现有的监督方法的性能。代码和数据集可以在<a target="_blank" rel="noopener" href="https://github.com/JianghaiSCU/RealSH%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/JianghaiSCU/RealSH上下载。</a><details>
<summary>Abstract</summary>
In this paper, we propose an iterative framework, which consists of two phases: a generation phase and a training phase, to generate realistic training data and yield a supervised homography network. In the generation phase, given an unlabeled image pair, we utilize the pre-estimated dominant plane masks and homography of the pair, along with another sampled homography that serves as ground truth to generate a new labeled training pair with realistic motion. In the training phase, the generated data is used to train the supervised homography network, in which the training data is refined via a content consistency module and a quality assessment module. Once an iteration is finished, the trained network is used in the next data generation phase to update the pre-estimated homography. Through such an iterative strategy, the quality of the dataset and the performance of the network can be gradually and simultaneously improved. Experimental results show that our method achieves state-of-the-art performance and existing supervised methods can be also improved based on the generated dataset. Code and dataset are available at https://github.com/JianghaiSCU/RealSH.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种迭代框架，它包括两个阶段：生成阶段和训练阶段，用于生成真实的训练数据并生成一个监督式投影网络。在生成阶段，给定一个无标注的图像对，我们利用该对的先预计算的主要平面面积和投影，以及另一个随机选择的投影作为真实的参照，生成一个新的标注过的训练对。在训练阶段，生成的数据被用来训练监督式投影网络，其中生成的数据被修正 via 内容一致模块和质量评估模块。一旦一个迭代结束，训练完成后，用于下一次数据生成阶段的网络被更新。通过如此的迭代策略，数据集的质量和网络的性能可以逐渐提高。实验结果表明，我们的方法可以 дости得现状势最佳性能，并且可以根据生成的数据来改进现有的监督式方法。代码和数据可以在https://github.com/JianghaiSCU/RealSH中下载。
</details></li>
</ul>
<hr>
<h2 id="The-Radon-Signed-Cumulative-Distribution-Transform-and-its-applications-in-classification-of-Signed-Images"><a href="#The-Radon-Signed-Cumulative-Distribution-Transform-and-its-applications-in-classification-of-Signed-Images" class="headerlink" title="The Radon Signed Cumulative Distribution Transform and its applications in classification of Signed Images"></a>The Radon Signed Cumulative Distribution Transform and its applications in classification of Signed Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15339">http://arxiv.org/abs/2307.15339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rohdelab/PyTransKit">https://github.com/rohdelab/PyTransKit</a></li>
<li>paper_authors: Le Gong, Shiying Li, Naqib Sad Pathan, Mohammad Shifat-E-Rabbi, Gustavo K. Rohde, Abu Hasnat Mohammad Rubaiyat, Sumati Thareja</li>
<li>for: 该研究提出了一种基于运输 mathematics 和最优运输的新图像表示技术。</li>
<li>methods: 该方法结合了Radon transform 和 Signed Cumulative Distribution Transform 两种已知的图像表示方法，并将其推广到任意函数（图像）上，因此可以用于更多应用场景。</li>
<li>results: 研究人员对实验和模拟数据进行了比较，发现新的 transform 能够更准确地表示签名图像中的信息内容，因此可以获得更高的分类精度。<details>
<summary>Abstract</summary>
Here we describe a new image representation technique based on the mathematics of transport and optimal transport. The method relies on the combination of the well-known Radon transform for images and a recent signal representation method called the Signed Cumulative Distribution Transform. The newly proposed method generalizes previous transport-related image representation methods to arbitrary functions (images), and thus can be used in more applications. We describe the new transform, and some of its mathematical properties and demonstrate its ability to partition image classes with real and simulated data. In comparison to existing transport transform methods, as well as deep learning-based classification methods, the new transform more accurately represents the information content of signed images, and thus can be used to obtain higher classification accuracies. The implementation of the proposed method in Python language is integrated as a part of the software package PyTransKit, available on Github.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的图像表示技术，基于运输学和最优运输学 mathematics。该方法通过结合已知的射频变换和最近的signal representation方法called Signed Cumulative Distribution Transform而组合。新提出的方法可以对任意函数（图像）进行扩展，因此可以在更多的应用中使用。我们描述了新的变换，以及一些其数学性质和示例数据。与现有的运输变换方法和深度学习基于分类方法相比，新的变换更好地表示签名图像中的信息内容，因此可以实现更高的分类精度。我们在Python语言中实现了该方法，并将其 integrate到PyTransKit软件包中，可以在Github上下载。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-PlenOctree-for-Adaptive-Sampling-Refinement-in-Explicit-NeRF"><a href="#Dynamic-PlenOctree-for-Adaptive-Sampling-Refinement-in-Explicit-NeRF" class="headerlink" title="Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF"></a>Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15333">http://arxiv.org/abs/2307.15333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Bai, Yiqi Lin, Yize Chen, Lin Wang</li>
<li>for: 这个研究是为了提高Explicit NeRF的训练和测试效率，以应对虚拟现实和游戏等领域。</li>
<li>methods: 这个研究使用的方法是Dynamic PlenOctree DOT，它是一个可靠的、高效的Octree表现，可以适应场景的变化。</li>
<li>results: 相比POT，DOT可以提高视觉质量，减少超过55.15%&#x2F;$68.84%$的参数，并提供1.7&#x2F;1.9倍的FPS дляNeRF-synthetic和Tanks $&amp;$ Temples。<details>
<summary>Abstract</summary>
The explicit neural radiance field (NeRF) has gained considerable interest for its efficient training and fast inference capabilities, making it a promising direction such as virtual reality and gaming. In particular, PlenOctree (POT)[1], an explicit hierarchical multi-scale octree representation, has emerged as a structural and influential framework. However, POT's fixed structure for direct optimization is sub-optimal as the scene complexity evolves continuously with updates to cached color and density, necessitating refining the sampling distribution to capture signal complexity accordingly. To address this issue, we propose the dynamic PlenOctree DOT, which adaptively refines the sample distribution to adjust to changing scene complexity. Specifically, DOT proposes a concise yet novel hierarchical feature fusion strategy during the iterative rendering process. Firstly, it identifies the regions of interest through training signals to ensure adaptive and efficient refinement. Next, rather than directly filtering out valueless nodes, DOT introduces the sampling and pruning operations for octrees to aggregate features, enabling rapid parameter learning. Compared with POT, our DOT outperforms it by enhancing visual quality, reducing over $55.15$/$68.84\%$ parameters, and providing 1.7/1.9 times FPS for NeRF-synthetic and Tanks $\&$ Temples, respectively. Project homepage:https://vlislab22.github.io/DOT.   [1] Yu, Alex, et al. "Plenoctrees for real-time rendering of neural radiance fields." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
</details>
<details>
<summary>摘要</summary>
Explicit NeRF 技术在虚拟现实和游戏领域得到了广泛关注，因为它具有高效的训练和快速推断能力。特别是PlenOctree（POT）这种显式层次多规格树表示方法，在许多情况下成为了一种重要的框架。然而，POT 的固定结构导致了直接优化的问题，因为场景复杂度在缓存颜色和浓度更新的过程中不断变化，需要根据信号复杂度进行适应性的改进。为解决这个问题，我们提出了动态PlenOctree DOT，它可以动态调整样本分布，以适应场景复杂度的变化。具体来说，DOT提出了一种新的层次特征融合策略，在迭代渲染过程中对Region of Interest进行训练，以确保高效和适应的改进。而不是直接过滤无用的节点，DOT引入了采样和剪除操作，以协助快速学习参数。相比POT，我们的DOT在提高视觉质量、减少参数数量和提供更高的帧率方面表现出色，具体来说，DOT的视觉质量提高了55.15%/68.84%，参数减少了55.15%/68.84%，并且在NeRF-synthetic和Tanks $\&$ Temples等场景下提供了1.7/1.9倍的帧率。项目主页：https://vlislab22.github.io/DOT。[1] Yu, Alex, et al. "Plenoctrees for real-time rendering of neural radiance fields." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
</details></li>
</ul>
<hr>
<h2 id="Staging-E-Commerce-Products-for-Online-Advertising-using-Retrieval-Assisted-Image-Generation"><a href="#Staging-E-Commerce-Products-for-Online-Advertising-using-Retrieval-Assisted-Image-Generation" class="headerlink" title="Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation"></a>Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15326">http://arxiv.org/abs/2307.15326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueh-Ning Ku, Mikhail Kuznetsov, Shaunak Mishra, Paloma de Juan</li>
<li>for: 这个论文是关于如何使用生成对抗网络（GAN）和检索助手GAN（Retrieval Assisted GAN，RAGAN）来增强电子商务平台上的动态产品广告（DPA）图像的。</li>
<li>methods: 这个论文提出了一种基于GAN和检索助手GAN的复制粘贴stagging方法，该方法首先从目录中检索与输入产品相似的已stagged产品，然后将其背景复制到输入图像中，并使用GAN基于填充模型来填充复制后的孔隙。</li>
<li>results: 论文通过在线度量和人工评估来证明了该复制粘贴stagging方法的效果，同时还展示了如何使用该方法生成产品动画。<details>
<summary>Abstract</summary>
Online ads showing e-commerce products typically rely on the product images in a catalog sent to the advertising platform by an e-commerce platform. In the broader ads industry such ads are called dynamic product ads (DPA). It is common for DPA catalogs to be in the scale of millions (corresponding to the scale of products which can be bought from the e-commerce platform). However, not all product images in the catalog may be appealing when directly re-purposed as an ad image, and this may lead to lower click-through rates (CTRs). In particular, products just placed against a solid background may not be as enticing and realistic as a product staged in a natural environment. To address such shortcomings of DPA images at scale, we propose a generative adversarial network (GAN) based approach to generate staged backgrounds for un-staged product images. Generating the entire staged background is a challenging task susceptible to hallucinations. To get around this, we introduce a simpler approach called copy-paste staging using retrieval assisted GANs. In copy paste staging, we first retrieve (from the catalog) staged products similar to the un-staged input product, and then copy-paste the background of the retrieved product in the input image. A GAN based in-painting model is used to fill the holes left after this copy-paste operation. We show the efficacy of our copy-paste staging method via offline metrics, and human evaluation. In addition, we show how our staging approach can enable animations of moving products leading to a video ad from a product image.
</details>
<details>
<summary>摘要</summary>
在线广告通常会使用电商平台提供的产品图片，这些图片通常会被称为动态产品广告（DPA）。DPA目录通常有数百万个图片，但不 все图片都能够直接复用为广告图片，这可能导致更低的键盘 clicks（CTR）。特别是，产品只有在固定背景下显示可能不那么吸引人和真实。为解决DPA图片的缺点，我们提出了基于生成对抗网络（GAN）的方法，生成产品在自然环境中的摄影。然而，整个生成整个场景是一项复杂的任务，易于生成幻觉。为此，我们提出了一种更简单的方法：复制粘贴配置。在复制粘贴配置中，我们首先从目录中检索与输入产品相似的已有的stage产品，然后将其中的背景复制到输入图片中。使用GAN基于的填充模型来填充复制后的孔隙。我们通过线上指标和人工评估表明了我们的配置方法的有效性。此外，我们还展示了如何使用我们的配置方法生成动画。
</details></li>
</ul>
<hr>
<h2 id="TaskExpert-Dynamically-Assembling-Multi-Task-Representations-with-Memorial-Mixture-of-Experts"><a href="#TaskExpert-Dynamically-Assembling-Multi-Task-Representations-with-Memorial-Mixture-of-Experts" class="headerlink" title="TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts"></a>TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15324">http://arxiv.org/abs/2307.15324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prismformore/multi-task-transformer">https://github.com/prismformore/multi-task-transformer</a></li>
<li>paper_authors: Hanrong Ye, Dan Xu</li>
<li>for: 这个研究是为了解决多项任务学习中，同一个背景特征（例如从backbone层中的特征）中同时学习多个明确任务特有的特征的问题。</li>
<li>methods: 这个研究使用了一种名为TaskExpert的多项任务混合专家模型，它可以学习多个代表任务特有的特征空间，并在动态的方式下将任务特有的特征解析为多个专家网络。</li>
<li>results: 实验结果显示，TaskExpert在两个竞争性多项任务学习 benchmark（PASCAL-Context和NYUD-v2）上的9个指标中，均超越了前一代最佳方法。<details>
<summary>Abstract</summary>
Learning discriminative task-specific features simultaneously for multiple distinct tasks is a fundamental problem in multi-task learning. Recent state-of-the-art models consider directly decoding task-specific features from one shared task-generic feature (e.g., feature from a backbone layer), and utilize carefully designed decoders to produce multi-task features. However, as the input feature is fully shared and each task decoder also shares decoding parameters for different input samples, it leads to a static feature decoding process, producing less discriminative task-specific representations. To tackle this limitation, we propose TaskExpert, a novel multi-task mixture-of-experts model that enables learning multiple representative task-generic feature spaces and decoding task-specific features in a dynamic manner. Specifically, TaskExpert introduces a set of expert networks to decompose the backbone feature into several representative task-generic features. Then, the task-specific features are decoded by using dynamic task-specific gating networks operating on the decomposed task-generic features. Furthermore, to establish long-range modeling of the task-specific representations from different layers of TaskExpert, we design a multi-task feature memory that updates at each layer and acts as an additional feature expert for dynamic task-specific feature decoding. Extensive experiments demonstrate that our TaskExpert clearly outperforms previous best-performing methods on all 9 metrics of two competitive multi-task learning benchmarks for visual scene understanding (i.e., PASCAL-Context and NYUD-v2). Codes and models will be made publicly available at https://github.com/prismformore/Multi-Task-Transformer
</details>
<details>
<summary>摘要</summary>
学习多个不同任务的特征同时是多任务学习的基本问题。现今的状态提取模型直接从一个共享任务普适特征（例如，底层层次特征）中提取任务特征，并使用特别设计的解码器生成多任务特征。然而，由于输入特征完全共享，每个任务解码器也共享解码参数 для不同的输入样本，这会导致静态特征解码过程，生成更少的特征分解。为了解决这些限制，我们提出了TaskExpert，一种新的多任务混合专家模型，允许学习多个代表任务普适特征空间和动态解码任务特征。特别是，TaskExpert引入了一组专家网络将底层特征分解成多个代表任务普适特征。然后，每个任务特征被解码器使用动态任务特征闭合网络在不同的任务普适特征空间中解码。此外，为了在不同层次的TaskExpert中建立长距离模型化任务特征表示，我们设计了一个多任务特征记忆，在每层更新并作为多任务特征解码器的额外特征专家。广泛的实验证明了我们的TaskExpert明显超过了之前最佳表现的方法在图像Scene理解两个竞争性多任务学习 benchmark 上（即 PASCAL-Context 和 NYUD-v2）。代码和模型将在https://github.com/prismformore/Multi-Task-Transformer 上公开。
</details></li>
</ul>
<hr>
<h2 id="DocDeshadower-Frequency-aware-Transformer-for-Document-Shadow-Removal"><a href="#DocDeshadower-Frequency-aware-Transformer-for-Document-Shadow-Removal" class="headerlink" title="DocDeshadower: Frequency-aware Transformer for Document Shadow Removal"></a>DocDeshadower: Frequency-aware Transformer for Document Shadow Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15318">http://arxiv.org/abs/2307.15318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shenghong Luo, Ruifeng Xu, Xuhang Chen, Zinuo Li, Chi-Man Pun, Shuqiang Wang</li>
<li>for: 提高扫描文档中的阴影 removing 效果</li>
<li>methods: 使用多频Transformer模型、Attention-Aggregation Network和Gated Multi-scale Fusion Transformer来除阴影</li>
<li>results: 在质量和量化两个方面都超越现有的状态之最方法<details>
<summary>Abstract</summary>
The presence of shadows significantly impacts the visual quality of scanned documents. However, the existing traditional techniques and deep learning methods used for shadow removal have several limitations. These methods either rely heavily on heuristics, resulting in suboptimal performance, or require large datasets to learn shadow-related features. In this study, we propose the DocDeshadower, a multi-frequency Transformer-based model built on Laplacian Pyramid. DocDeshadower is designed to remove shadows at different frequencies in a coarse-to-fine manner. To achieve this, we decompose the shadow image into different frequency bands using Laplacian Pyramid. In addition, we introduce two novel components to this model: the Attention-Aggregation Network and the Gated Multi-scale Fusion Transformer. The Attention-Aggregation Network is designed to remove shadows in the low-frequency part of the image, whereas the Gated Multi-scale Fusion Transformer refines the entire image at a global scale with its large perceptive field. Our extensive experiments demonstrate that DocDeshadower outperforms the current state-of-the-art methods in both qualitative and quantitative terms.
</details>
<details>
<summary>摘要</summary>
文本中的阴影对可读性有着重要的影响，但现有的传统方法和深度学习方法used for shadow removal有一些局限性。这些方法可能会依赖于规则，导致性能下降，或者需要大量的数据来学习阴影相关的特征。在这项研究中，我们提出了DocDeshadower，一种多频Transformer基于Laplacian Pyramid的模型。DocDeshadower通过在不同频率带进行均衡处理来去除阴影。为此，我们使用Laplacian Pyramid将阴影图像分解成不同频率带。此外，我们还提出了两个新的组件：协调汇集网络和灵活多scale混合transformer。协调汇集网络用于在低频部分中去除阴影，而灵活多scale混合transformer则在全图上进行全局级别的细化处理，其大见范field允许它在不同频率带上进行细化处理。我们的广泛实验表明，DocDeshadower在可读性和量化上都超过了当前状态的方法。
</details></li>
</ul>
<hr>
<h2 id="Attentive-Multimodal-Fusion-for-Optical-and-Scene-Flow"><a href="#Attentive-Multimodal-Fusion-for-Optical-and-Scene-Flow" class="headerlink" title="Attentive Multimodal Fusion for Optical and Scene Flow"></a>Attentive Multimodal Fusion for Optical and Scene Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15301">http://arxiv.org/abs/2307.15301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiesico/fusionraft">https://github.com/jiesico/fusionraft</a></li>
<li>paper_authors: Youjie Zhou, Guofeng Mei, Yiming Wang, Fabio Poiesi, Yi Wan</li>
<li>for: 这paper是为了解决RGB模式下的视觉和Scene flow估计问题，尤其是在噪声或低照度环境下。</li>
<li>methods: 这paper提出了一种基于深度学习的FusionRAFT方法，使得早期模式融合可以更好地利用两个感知模式（RGB和深度）的优势。该方法包括自我和交叉关注层，以建立有用的特征，以便更好地利用两个模式的优势。</li>
<li>results: 通过比较性试验，这paper表明FusionRAFT方法在Flyingthings3DSynthetic数据集和KITTI实际数据集上表现更好，并且在噪声和低照度条件下表现更加稳定和可靠。<details>
<summary>Abstract</summary>
This paper presents an investigation into the estimation of optical and scene flow using RGBD information in scenarios where the RGB modality is affected by noise or captured in dark environments. Existing methods typically rely solely on RGB images or fuse the modalities at later stages, which can result in lower accuracy when the RGB information is unreliable. To address this issue, we propose a novel deep neural network approach named FusionRAFT, which enables early-stage information fusion between sensor modalities (RGB and depth). Our approach incorporates self- and cross-attention layers at different network levels to construct informative features that leverage the strengths of both modalities. Through comparative experiments, we demonstrate that our approach outperforms recent methods in terms of performance on the synthetic dataset Flyingthings3D, as well as the generalization on the real-world dataset KITTI. We illustrate that our approach exhibits improved robustness in the presence of noise and low-lighting conditions that affect the RGB images. We release the code, models and dataset at https://github.com/jiesico/FusionRAFT.
</details>
<details>
<summary>摘要</summary>
中文翻译：本文研究了基于RGBD信息的光学和场景流计算，在RGB信息受到噪音或低光照影响的场景下。现有方法通常只采用RGB图像或在后续阶段进行模式融合，这可能会导致RGB信息不可靠时的性能下降。为解决这问题，我们提出了一种新的深度神经网络方法，即FusionRAFT，它在感知Modalities（RGB和深度）之间进行早期融合。我们的方法包括自身和交叉关注层，以不同的网络层次构建有用的特征，以利用两种模式之间的优势。通过比较实验，我们证明了我们的方法在Flyingthings3D sintetic dataset和KITTI实验室 dataset上的性能较高，并且在噪音和低光照条件下表现更加稳定。我们将代码、模型和数据集发布在https://github.com/jiesico/FusionRAFT上。
</details></li>
</ul>
<hr>
<h2 id="AC-Norm-Effective-Tuning-for-Medical-Image-Analysis-via-Affine-Collaborative-Normalization"><a href="#AC-Norm-Effective-Tuning-for-Medical-Image-Analysis-via-Affine-Collaborative-Normalization" class="headerlink" title="AC-Norm: Effective Tuning for Medical Image Analysis via Affine Collaborative Normalization"></a>AC-Norm: Effective Tuning for Medical Image Analysis via Affine Collaborative Normalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15282">http://arxiv.org/abs/2307.15282</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/endoluminalsurgicalvision-imr/acnorm">https://github.com/endoluminalsurgicalvision-imr/acnorm</a></li>
<li>paper_authors: Chuyan Zhang, Yuncheng Yang, Hao Zheng, Yun Gu<br>for:  This paper focuses on enhancing the performance of clinical applications with limited annotations using self-supervised learning (SSL) and the “pretraining-then-finetuning” paradigm.methods:  The proposed method, Affine Collaborative Normalization (AC-Norm), utilizes the trainable affine parameters of batch normalization (BN) layers to dynamically recalibrate the channels in the target model according to the cross-domain channel-wise correlations, without adding extra parameters.results:  The proposed AC-Norm method outperformed the vanilla finetuning method by up to 4% improvement in various transfer learning tasks, including diabetic retinopathy grade classification, retinal vessel segmentation, CT lung nodule segmentation&#x2F;classification, CT liver-tumor segmentation, and MRI cardiac segmentation. Additionally, AC-Norm was found to be capable of fast transferability estimation.<details>
<summary>Abstract</summary>
Driven by the latest trend towards self-supervised learning (SSL), the paradigm of "pretraining-then-finetuning" has been extensively explored to enhance the performance of clinical applications with limited annotations. Previous literature on model finetuning has mainly focused on regularization terms and specific policy models, while the misalignment of channels between source and target models has not received sufficient attention. In this work, we revisited the dynamics of batch normalization (BN) layers and observed that the trainable affine parameters of BN serve as sensitive indicators of domain information. Therefore, Affine Collaborative Normalization (AC-Norm) is proposed for finetuning, which dynamically recalibrates the channels in the target model according to the cross-domain channel-wise correlations without adding extra parameters. Based on a single-step backpropagation, AC-Norm can also be utilized to measure the transferability of pretrained models. We evaluated AC-Norm against the vanilla finetuning and state-of-the-art fine-tuning methods on transferring diverse pretrained models to the diabetic retinopathy grade classification, retinal vessel segmentation, CT lung nodule segmentation/classification, CT liver-tumor segmentation and MRI cardiac segmentation tasks. Extensive experiments demonstrate that AC-Norm unanimously outperforms the vanilla finetuning by up to 4% improvement, even under significant domain shifts where the state-of-the-art methods bring no gains. We also prove the capability of AC-Norm in fast transferability estimation. Our code is available at https://github.com/EndoluminalSurgicalVision-IMR/ACNorm.
</details>
<details>
<summary>摘要</summary>
受最新的自动学习（SSL）趋势驱动，“预训练后finetuning”的方法在医疗应用中得到了广泛的探索，以提高limited annotations的性能。之前的模型finetuning研究主要集中在常规化项和特定策略模型上，而频道之间的偏移问题尚未得到了充分的注意。在这项工作中，我们重新探讨了批量 нормализа（BN）层的动力学，并发现了BN层的可调参数作为域信息的敏感指标。因此，我们提出了Affine Collaborative Normalization（AC-Norm），用于finetuning，可以在目标模型中动态重新准确channel，无需添加额外参数。基于单步反射，AC-Norm还可以用于评估预训练模型的传输性。我们对AC-Norm与常规finetuning和现有的精细调整方法进行了对比，在不同的频道偏移 task 上进行了广泛的实验。结果表明，AC-Norm在频道偏移情况下可以达到4%的提升，而在其他方法无法提供任何提升的情况下。我们还证明了AC-Norm的快速传输性能测试能力。代码可以在https://github.com/EndoluminalSurgicalVision-IMR/ACNorm上下载。
</details></li>
</ul>
<hr>
<h2 id="Recovering-high-quality-FODs-from-a-reduced-number-of-diffusion-weighted-images-using-a-model-driven-deep-learning-architecture"><a href="#Recovering-high-quality-FODs-from-a-reduced-number-of-diffusion-weighted-images-using-a-model-driven-deep-learning-architecture" class="headerlink" title="Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture"></a>Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15273">http://arxiv.org/abs/2307.15273</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jbartlett6/sdnet">https://github.com/jbartlett6/sdnet</a></li>
<li>paper_authors: J Bartlett, C E Davey, L A Johnston, J Duan</li>
<li>for: 该研究旨在提出一种基于深度学习的材料方向分布（FOD）重建方法，可以从少量的扩散束图像（DWI）中生成高精度的FOD。</li>
<li>methods: 该方法使用深度学习网络，使用扩散获取的Diffusion-weighted image（DWI）信号作为输入，并通过一种圆拟合网络来重建FOD。</li>
<li>results: 研究表明，该模型基于深度学习的FOD重建方法可以与现有的FOD超分辨率网络相比，并且可以通过调整约束来提高下游的fixel分类精度。代码可以在<a target="_blank" rel="noopener" href="https://github.com/Jbartlett6/SDNet%E4%B8%AD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Jbartlett6/SDNet中获取。</a><details>
<summary>Abstract</summary>
Fibre orientation distribution (FOD) reconstruction using deep learning has the potential to produce accurate FODs from a reduced number of diffusion-weighted images (DWIs), decreasing total imaging time. Diffusion acquisition invariant representations of the DWI signals are typically used as input to these methods to ensure that they can be applied flexibly to data with different b-vectors and b-values; however, this means the network cannot condition its output directly on the DWI signal. In this work, we propose a spherical deconvolution network, a model-driven deep learning FOD reconstruction architecture, that ensures intermediate and output FODs produced by the network are consistent with the input DWI signals. Furthermore, we implement a fixel classification penalty within our loss function, encouraging the network to produce FODs that can subsequently be segmented into the correct number of fixels and improve downstream fixel-based analysis. Our results show that the model-based deep learning architecture achieves competitive performance compared to a state-of-the-art FOD super-resolution network, FOD-Net. Moreover, we show that the fixel classification penalty can be tuned to offer improved performance with respect to metrics that rely on accurately segmented of FODs. Our code is publicly available at https://github.com/Jbartlett6/SDNet .
</details>
<details>
<summary>摘要</summary>
《纤维方向分布（FOD）重建使用深度学习有可能生成准确的FOD，从一小量的扩散束图像（DWI）中减少总成像时间。通常使用扩散获取的不变表示来作为输入，以确保这些方法可以适应不同的b-向量和b值;然而，这意味着网络无法直接Conditional Output在DWI信号。在这种工作中，我们提议一种圆柱体抽象网络，一种驱动深度学习FOD重建架构，以确保输入DWI信号和输出FOD之间的一致。此外，我们实施了一种纤维分类罚金在我们的损失函数中，让网络生成FOD，可以随后被正确分割为纤维。我们的结果表明，模型驱动的深度学习架构与现状的FOD超分辨网络FOD-Net具有竞争性。此外，我们还证明了纤维分类罚金可以调整以提高基于纤维分割的下游分析的表现。我们的代码在https://github.com/Jbartlett6/SDNet上公开。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Anatomy-Aware-Lymph-Node-Detection-in-Chest-CT-using-Implicit-Station-Stratification"><a href="#Anatomy-Aware-Lymph-Node-Detection-in-Chest-CT-using-Implicit-Station-Stratification" class="headerlink" title="Anatomy-Aware Lymph Node Detection in Chest CT using Implicit Station Stratification"></a>Anatomy-Aware Lymph Node Detection in Chest CT using Implicit Station Stratification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15271">http://arxiv.org/abs/2307.15271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Yan, Dakai Jin, Dazhou Guo, Minfeng Xu, Na Shen, Xian-Sheng Hua, Xianghua Ye, Le Lu<br>for: 这个研究的目的是提高于医疗影像中发现异常淋巴节的自动检测性能。methods: 本研究提出了一个 novel 的终端框架，通过利用淋巴节的站信息来提高淋巴节检测性能。我们设计了多头检测器，并将每个头专注于区分淋巴节和非淋巴节结构的不同站信息。在训练过程中，我们使用了多任务学习，将淋巴节站信息作为多个类别的标签生成，因此在测试过程中不需要另外的Explicit LN站预测模型。results: 我们在82名肺癌患者和91名食道癌患者的CT影像检查中评估了我们的算法。结果显示，我们的方法可以从65.1%提高到71.4%和80.3%提高到85.5%，对于每个患者2个错误的淋巴节检测性能，均有明显的改善。相比于 existed 的多种基eline技术，如nnUNet、nnDetection和LENS，我们的方法具有更高的检测性能。<details>
<summary>Abstract</summary>
Finding abnormal lymph nodes in radiological images is highly important for various medical tasks such as cancer metastasis staging and radiotherapy planning. Lymph nodes (LNs) are small glands scattered throughout the body. They are grouped or defined to various LN stations according to their anatomical locations. The CT imaging appearance and context of LNs in different stations vary significantly, posing challenges for automated detection, especially for pathological LNs. Motivated by this observation, we propose a novel end-to-end framework to improve LN detection performance by leveraging their station information. We design a multi-head detector and make each head focus on differentiating the LN and non-LN structures of certain stations. Pseudo station labels are generated by an LN station classifier as a form of multi-task learning during training, so we do not need another explicit LN station prediction model during inference. Our algorithm is evaluated on 82 patients with lung cancer and 91 patients with esophageal cancer. The proposed implicit station stratification method improves the detection sensitivity of thoracic lymph nodes from 65.1% to 71.4% and from 80.3% to 85.5% at 2 false positives per patient on the two datasets, respectively, which significantly outperforms various existing state-of-the-art baseline techniques such as nnUNet, nnDetection and LENS.
</details>
<details>
<summary>摘要</summary>
找到不同常规图像中的异常淋巴节点是医疗领域中非常重要的各种任务中的一个，如癌细胞肿瘤stage和放疗规划。淋巴节点（LN）是身体中散布的小腺体，根据其生理位置分为不同的LN站。不同的LN站在CT图像的出现和背景下有很大的差异，这会提高自动检测的挑战，特别是对于病理LN。为了解决这个问题，我们提出了一种新的综合框架，利用LN站信息来提高淋巴节点检测性能。我们设计了多头检测器，每个头都专门用于区分LN和非LN结构。在训练时，我们使用LN站分类器生成pseudo站标签，以实现多任务学习，因此在推断时不需要另外的explicit LN站预测模型。我们的算法在82名肺癌患者和91名食道癌患者的数据集上进行了评估，并显示了提高了脊梗淋巴节点检测感度，从65.1%提高到71.4%和80.3%提高到85.5%，在2个false positive每个患者时，分别有显著的提高。与多种现有的基线技术相比，我们的方法显示出了显著的优势。
</details></li>
</ul>
<hr>
<h2 id="RSGPT-A-Remote-Sensing-Vision-Language-Model-and-Benchmark"><a href="#RSGPT-A-Remote-Sensing-Vision-Language-Model-and-Benchmark" class="headerlink" title="RSGPT: A Remote Sensing Vision Language Model and Benchmark"></a>RSGPT: A Remote Sensing Vision Language Model and Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15266">http://arxiv.org/abs/2307.15266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, Xiang Li<br>for: 本研究旨在开发特有的大视语言模型（VLM），用于数据分析领域中的远程感知（RS）应用。methods: 本研究使用了人工标注的卫星图像描述集（RSICap），以及卫星图像评估集（RSIEval），用于评估和训练大视语言模型。results: 研究发现，通过使用高质量的卫星图像描述集（RSICap）和卫星图像评估集（RSIEval），可以帮助开发大视语言模型（VLM），并且可以在RS应用中达到比较出色的性能。<details>
<summary>Abstract</summary>
The emergence of large-scale large language models, with GPT-4 as a prominent example, has significantly propelled the rapid advancement of artificial general intelligence and sparked the revolution of Artificial Intelligence 2.0. In the realm of remote sensing (RS), there is a growing interest in developing large vision language models (VLMs) specifically tailored for data analysis in this domain. However, current research predominantly revolves around visual recognition tasks, lacking comprehensive, large-scale image-text datasets that are aligned and suitable for training large VLMs, which poses significant challenges to effectively training such models for RS applications. In computer vision, recent research has demonstrated that fine-tuning large vision language models on small-scale, high-quality datasets can yield impressive performance in visual and language understanding. These results are comparable to state-of-the-art VLMs trained from scratch on massive amounts of data, such as GPT-4. Inspired by this captivating idea, in this work, we build a high-quality Remote Sensing Image Captioning dataset (RSICap) that facilitates the development of large VLMs in the RS field. Unlike previous RS datasets that either employ model-generated captions or short descriptions, RSICap comprises 2,585 human-annotated captions with rich and high-quality information. This dataset offers detailed descriptions for each image, encompassing scene descriptions (e.g., residential area, airport, or farmland) as well as object information (e.g., color, shape, quantity, absolute position, etc). To facilitate the evaluation of VLMs in the field of RS, we also provide a benchmark evaluation dataset called RSIEval. This dataset consists of human-annotated captions and visual question-answer pairs, allowing for a comprehensive assessment of VLMs in the context of RS.
</details>
<details>
<summary>摘要</summary>
大规模的大语言模型，如GPT-4，对人工通用智能的发展产生了巨大的推动，并促使了人工智能2.0的革命。在远程感知（RS）领域，有增加兴趣在开发特定于数据分析的大视语言模型（VLMs）。然而，当前的研究主要集中在视觉认知任务上，缺乏大规模、一致的图像文本数据集，这会对培育大VLMs的训练带来很大的挑战。在计算机视觉领域，最近的研究表明， fine-tuning大视语言模型在小规模、高质量数据集上可以获得出色的视觉和语言理解性能。这些结果与 state-of-the-art VLMs 训练自零开始大量数据，如GPT-4，相当。 inspirited by this captivating idea，在这项工作中，我们构建了高质量的远程感知图像描述集（RSICap），以便在RS领域开发大VLMs。与前期RS datasets不同，RSICap包含2,585个人注解的描述，其中包括Scene描述（例如：居民区、机场、农业等）以及物体信息（例如：颜色、形状、数量、绝对位置等）。为便于RS领域中VLMs的评估，我们还提供了一个名为RSIEval的基准评估集，该集包含人注解的描述和视觉问答对，以便对VLMs在RS领域进行全面的评估。
</details></li>
</ul>
<hr>
<h2 id="Learning-with-Constraint-Learning-New-Perspective-Solution-Strategy-and-Various-Applications"><a href="#Learning-with-Constraint-Learning-New-Perspective-Solution-Strategy-and-Various-Applications" class="headerlink" title="Learning with Constraint Learning: New Perspective, Solution Strategy and Various Applications"></a>Learning with Constraint Learning: New Perspective, Solution Strategy and Various Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15257">http://arxiv.org/abs/2307.15257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Risheng Liu, Jiaxin Gao, Xuan Liu, Xin Fan</li>
<li>for: 解决复杂的机器学习和计算机视觉问题，包括生成对抗网络（GAN）和其变种、多任务和元学习、超参数学习以及各种实际应用。</li>
<li>methods: 提出了一种新的框架——学习干预学习（LwCL），可以一元化地探讨这些多样化的学习和视觉问题。LwCL 采用了一种高级别的层次优化模型，能够捕捉这些多样化的学习和视觉问题的本质。</li>
<li>results: 对于多种学习和视觉应用，LwCL 提供了一个广泛的解决方案，包括三类和九种问题类型。实验表明，LwCL 可以有效地解决各种复杂的机器学习和计算机视觉问题，并 bridge 理论和实践之间的差距。<details>
<summary>Abstract</summary>
The complexity of learning problems, such as Generative Adversarial Network (GAN) and its variants, multi-task and meta-learning, hyper-parameter learning, and a variety of real-world vision applications, demands a deeper understanding of their underlying coupling mechanisms. Existing approaches often address these problems in isolation, lacking a unified perspective that can reveal commonalities and enable effective solutions. Therefore, in this work, we proposed a new framework, named Learning with Constraint Learning (LwCL), that can holistically examine challenges and provide a unified methodology to tackle all the above-mentioned complex learning and vision problems. Specifically, LwCL is designed as a general hierarchical optimization model that captures the essence of these diverse learning and vision problems. Furthermore, we develop a gradient-response based fast solution strategy to overcome optimization challenges of the LwCL framework. Our proposed framework efficiently addresses a wide range of applications in learning and vision, encompassing three categories and nine different problem types. Extensive experiments on synthetic tasks and real-world applications verify the effectiveness of our approach. The LwCL framework offers a comprehensive solution for tackling complex machine learning and computer vision problems, bridging the gap between theory and practice.
</details>
<details>
<summary>摘要</summary>
“复杂的学习问题，如生成对抗网络（GAN）和其变种，多任务和元学习，参数学习，以及各种现实世界视觉应用，需要更深刻的理解它们的基础机制。现有方法通常对这些问题进行隔离处理，缺乏一个综合视角，这使得它们的解决方法受限。因此，在这项工作中，我们提出了一个新的框架，名为学习约束学（LwCL），可以总结这些多样化的学习和视觉问题。具体来说，LwCL是一种通用的层次优化模型，捕捉这些多样化的学习和视觉问题的核心。此外，我们开发了基于梯度响应的快速解决策略，以解决LwCL框架中的优化挑战。我们的提出的框架可以有效地解决各种学习和视觉问题，涵盖三个类别和九种不同的问题类型。广泛的实验证明了我们的方法的有效性，LwCL框架可以凝聚理论和实践之间的差距，为复杂的机器学习和计算机视觉问题提供一个普适的解决方案。”
</details></li>
</ul>
<hr>
<h2 id="A-Solution-to-Co-occurrence-Bias-Attributes-Disentanglement-via-Mutual-Information-Minimization-for-Pedestrian-Attribute-Recognition"><a href="#A-Solution-to-Co-occurrence-Bias-Attributes-Disentanglement-via-Mutual-Information-Minimization-for-Pedestrian-Attribute-Recognition" class="headerlink" title="A Solution to Co-occurrence Bias: Attributes Disentanglement via Mutual Information Minimization for Pedestrian Attribute Recognition"></a>A Solution to Co-occurrence Bias: Attributes Disentanglement via Mutual Information Minimization for Pedestrian Attribute Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15252">http://arxiv.org/abs/2307.15252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdret/a-solution-to-co-occurence-bias-in-pedestrian-attribute-recognition">https://github.com/sdret/a-solution-to-co-occurence-bias-in-pedestrian-attribute-recognition</a></li>
<li>paper_authors: Yibo Zhou, Hai-Miao Hu, Jinzuo Yu, Zhenbo Xu, Weiqing Lu, Yuran Cao</li>
<li>for: 提高pedestrian attribute recognition的Robustness和Generalization能力</li>
<li>methods: 提出了一种Attributes-disentangled feature learning方法，通过mutual information minimization来解耦特征之间的相关性</li>
<li>results: 在实际场景中提高了baseline的性能，并在PETAzs和RAPzs等 dataset上实现了State-of-the-artresult<details>
<summary>Abstract</summary>
Recent studies on pedestrian attribute recognition progress with either explicit or implicit modeling of the co-occurrence among attributes. Considering that this known a prior is highly variable and unforeseeable regarding the specific scenarios, we show that current methods can actually suffer in generalizing such fitted attributes interdependencies onto scenes or identities off the dataset distribution, resulting in the underlined bias of attributes co-occurrence. To render models robust in realistic scenes, we propose the attributes-disentangled feature learning to ensure the recognition of an attribute not inferring on the existence of others, and which is sequentially formulated as a problem of mutual information minimization. Rooting from it, practical strategies are devised to efficiently decouple attributes, which substantially improve the baseline and establish state-of-the-art performance on realistic datasets like PETAzs and RAPzs. Code is released on https://github.com/SDret/A-Solution-to-Co-occurence-Bias-in-Pedestrian-Attribute-Recognition.
</details>
<details>
<summary>摘要</summary>
近期研究人员发现，人行AttributeRecognition的进步通常采用显式或隐式表示人行Attribute的相互关系。然而，这种已知的假设对特定场景的变化和不可预测，可能导致现有方法在不同场景下表现不佳，具有人行Attribute相互关系的偏见。为确保模型在真实场景中 robust，我们提议使用Attribute分离特征学习，以确保一个特征不受另一个特征的存在影响。这个问题可以看作是 mutual information minimization 问题。从而，我们提出了实用的策略来快速分离特征，并在实际数据集上达到了基eline和状态arp的表现。代码可以在 https://github.com/SDret/A-Solution-to-Co-occurence-Bias-in-Pedestrian-Attribute-Recognition 上找到。
</details></li>
</ul>
<hr>
<h2 id="D2S-Representing-local-descriptors-and-global-scene-coordinates-for-camera-relocalization"><a href="#D2S-Representing-local-descriptors-and-global-scene-coordinates-for-camera-relocalization" class="headerlink" title="D2S: Representing local descriptors and global scene coordinates for camera relocalization"></a>D2S: Representing local descriptors and global scene coordinates for camera relocalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15250">http://arxiv.org/abs/2307.15250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bach-Thuan Bui, Dinh-Tuan Tran, Joo-Ho Lee</li>
<li>For: 本研究提出了一种基于直接学习的视地标记方法，用于解决现有的视地标记方法具有较高的计算成本和存储量的问题。* Methods: 本方法使用一个简单的神经网络 named D2S，用于表示本地描述符和场景坐标。 D2S 使用一种简单的损失函数和图注意机制，选择性地关注 robust 的描述符，而忽略某些不可靠的区域，如云、树和动态对象。* Results: 本方法在室内和室外环境中的场景坐标回归任务中表现出色，超过了现有的 CNN 基于方法。 它能够在不具备标注数据源的情况下，通过自然语言描述和自动分类来泛化到不同的场景中，包括从白天到黑夜的过渡和频率域转换。I hope that helps! Let me know if you have any further questions or if there’s anything else I can help with.<details>
<summary>Abstract</summary>
State-of-the-art visual localization methods mostly rely on complex procedures to match local descriptors and 3D point clouds. However, these procedures can incur significant cost in terms of inference, storage, and updates over time. In this study, we propose a direct learning-based approach that utilizes a simple network named D2S to represent local descriptors and their scene coordinates. Our method is characterized by its simplicity and cost-effectiveness. It solely leverages a single RGB image for localization during the testing phase and only requires a lightweight model to encode a complex sparse scene. The proposed D2S employs a combination of a simple loss function and graph attention to selectively focus on robust descriptors while disregarding areas such as clouds, trees, and several dynamic objects. This selective attention enables D2S to effectively perform a binary-semantic classification for sparse descriptors. Additionally, we propose a new outdoor dataset to evaluate the capabilities of visual localization methods in terms of scene generalization and self-updating from unlabeled observations. Our approach outperforms the state-of-the-art CNN-based methods in scene coordinate regression in indoor and outdoor environments. It demonstrates the ability to generalize beyond training data, including scenarios involving transitions from day to night and adapting to domain shifts, even in the absence of the labeled data sources. The source code, trained models, dataset, and demo videos are available at the following link: https://thpjp.github.io/d2s
</details>
<details>
<summary>摘要</summary>
现代视觉地标方法通常需要复杂的过程来匹配本地描述符和3D点云。然而，这些过程可能会带来较大的计算成本、存储成本和时间更新成本。在本研究中，我们提出了一种直接学习基于的方法，利用名为D2S的简单网络来表示本地描述符和其场景坐标。我们的方法具有简单性和成本效果。它仅在测试阶段使用单个RGB图像进行地标，并且仅需要一个轻量级模型来编码复杂的稀疏场景。提出的D2S使用一种简单的损失函数和图像注意力来选择ively关注可靠的描述符，而忽略云、树和一些动态对象。这种选择性注意力使得D2S可以有效地进行二分类Semantic地标。此外，我们还提出了一个新的户外数据集来评估视觉地标方法的场景总结和自动更新能力。我们的方法在室内和户外环境中超越了当前最佳CNN基于方法的场景坐标回归。它能够总结超出训练数据，包括从日到夜的过渡和适应频率变化，甚至在没有标注数据源的情况下。source code、训练模型、数据集和示例视频可以在以下链接获取：https://thpjp.github.io/d2s。
</details></li>
</ul>
<hr>
<h2 id="TROPHY-A-Topologically-Robust-Physics-Informed-Tracking-Framework-for-Tropical-Cyclones"><a href="#TROPHY-A-Topologically-Robust-Physics-Informed-Tracking-Framework-for-Tropical-Cyclones" class="headerlink" title="TROPHY: A Topologically Robust Physics-Informed Tracking Framework for Tropical Cyclones"></a>TROPHY: A Topologically Robust Physics-Informed Tracking Framework for Tropical Cyclones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15243">http://arxiv.org/abs/2307.15243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yan, Hanqi Guo, Thomas Peterka, Bei Wang, Jiali Wang</li>
<li>for: 本研究旨在提出一种基于物理知识的高效的TC跟踪方法，以提高大规模气象数据集中TC跟踪的计算效率。</li>
<li>methods: 本方法首先提出了一种基于物理知识的特征选择策略，以筛选出高稳定性和长期存在的TC kritical points。然后，在多层Robustness计算中，我们对TC kritical points进行了物理约束，以确保计算的TC跟踪结果具有物理意义。</li>
<li>results: 我们对30年的2D风场数据进行了实验，并通过对比观察轨迹和已有的TC跟踪算法，示出了TROPHY可以准确地跟踪TC的特征，并且有时even better than已有的TC跟踪算法。<details>
<summary>Abstract</summary>
Tropical cyclones (TCs) are among the most destructive weather systems. Realistically and efficiently detecting and tracking TCs are critical for assessing their impacts and risks. Recently, a multilevel robustness framework has been introduced to study the critical points of time-varying vector fields. The framework quantifies the robustness of critical points across varying neighborhoods. By relating the multilevel robustness with critical point tracking, the framework has demonstrated its potential in cyclone tracking. An advantage is that it identifies cyclonic features using only 2D wind vector fields, which is encouraging as most tracking algorithms require multiple dynamic and thermodynamic variables at different altitudes. A disadvantage is that the framework does not scale well computationally for datasets containing a large number of cyclones. This paper introduces a topologically robust physics-informed tracking framework (TROPHY) for TC tracking. The main idea is to integrate physical knowledge of TC to drastically improve the computational efficiency of multilevel robustness framework for large-scale climate datasets. First, during preprocessing, we propose a physics-informed feature selection strategy to filter 90% of critical points that are short-lived and have low stability, thus preserving good candidates for TC tracking. Second, during in-processing, we impose constraints during the multilevel robustness computation to focus only on physics-informed neighborhoods of TCs. We apply TROPHY to 30 years of 2D wind fields from reanalysis data in ERA5 and generate a number of TC tracks. In comparison with the observed tracks, we demonstrate that TROPHY can capture TC characteristics that are comparable to and sometimes even better than a well-validated TC tracking algorithm that requires multiple dynamic and thermodynamic scalar fields.
</details>
<details>
<summary>摘要</summary>
热带风暴（TC）是气候系统中最破坏性的天气系统之一。有效地探测和跟踪TC是评估其影响和风险的关键。最近，一种多级坚定性框架已经被提出来研究时变向量场中的关键点。这个框架可以量化不同级别的坚定性，并与多级坚定性相关的 kritical point tracking 进行比较。这个框架在风暴跟踪中表现出了潜力。它可以通过只使用2D风向场来识别风暴特征，这是有利的，因为大多数跟踪算法需要不同高度和层次的动力和 термодинамиче变量。然而，这个框架的计算效率不太好，特别是对大规模气候数据进行处理。这篇文章介绍了一种基于物理知识的逻辑坚定性物理协调跟踪框架（TROPHY），用于风暴跟踪。TROPHY的主要想法是通过将物理知识 integrate 到多级坚定性框架中，以提高计算效率。我们的方法包括：一、在预处理阶段，我们提出了物理学习Feature选择策略，以过滤90%的不稳定和短暂的关键点，保留适合风暴跟踪的好andidates。二、在进程阶段，我们在多级坚定性计算中强制实施物理学习的约束，只考虑物理学习所支持的TC约束。我们在ERA5的2D风向场数据上应用TROPHY，并生成了30年的风暴跟踪。与观测跟踪相比，我们示出TROPHY可以捕捉风暴特征，并且在一些情况下，甚至比一种已经证明有效的TC跟踪算法（需要多个动力和 термодинамичеscalar场）更好。
</details></li>
</ul>
<hr>
<h2 id="Fast-Dust-Sand-Image-Enhancement-Based-on-Color-Correction-and-New-Membership-Function"><a href="#Fast-Dust-Sand-Image-Enhancement-Based-on-Color-Correction-and-New-Membership-Function" class="headerlink" title="Fast Dust Sand Image Enhancement Based on Color Correction and New Membership Function"></a>Fast Dust Sand Image Enhancement Based on Color Correction and New Membership Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15230">http://arxiv.org/abs/2307.15230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Hakem Alsaeedi, Suha Mohammed Hadi, Yarub Alazzawi</li>
<li>for: 提高灰尘照片质量</li>
<li>methods: 使用色彩修正和新成员函数进行颜色偏移 correction，采用Adaptive Dark Channel Prior（A-DCP）进行雾化 removal，基于Contrast Limited Adaptive Histogram Equalization（CLAHE）进行对比度限制和图像亮度提高</li>
<li>results: 比现有研究更高效地除去红色和黄色投影，提供高质量和量灰尘照片<details>
<summary>Abstract</summary>
Images captured in dusty environments suffering from poor visibility and quality. Enhancement of these images such as sand dust images plays a critical role in various atmospheric optics applications. In this work, proposed a new model based on Color Correction and new membership function to enhance san dust images. The proposed model consists of three phases: correction of color shift, removal of haze, and enhancement of contrast and brightness. The color shift is corrected using a new membership function to adjust the values of U and V in the YUV color space. The Adaptive Dark Channel Prior (A-DCP) is used for haze removal. The stretching contrast and improving image brightness are based on Contrast Limited Adaptive Histogram Equalization (CLAHE). The proposed model tests and evaluates through many real sand dust images. The experimental results show that the proposed solution is outperformed the current studies in terms of effectively removing the red and yellow cast and provides high quality and quantity dust images.
</details>
<details>
<summary>摘要</summary>
图像捕捉在尘埃环境中，由于visibility和质量受到限制。尘埃图像加强在大气光学应用中扮演关键角色。本工作提出了一种基于颜色修正和新成员函数的图像加强模型。该模型包括三个阶段：色差修正、雾气除净和对比和亮度提高。色差修正使用新的成员函数调整YUV颜色空间中U和V值。使用适应黑道通道优先(A-DCP)进行雾气除净。对比和亮度提高基于对比限定适应 histogram平衡(CLAHE)。提出的模型在多个真实的沙尘图像上进行测试和评估。实验结果表明，提出的解决方案在效果上超越当前研究，可以有效地除掉红色和黄色投影，提供高质量和量的尘埃图像。
</details></li>
</ul>
<hr>
<h2 id="Sustainable-Transparency-in-Recommender-Systems-Bayesian-Ranking-of-Images-for-Explainability"><a href="#Sustainable-Transparency-in-Recommender-Systems-Bayesian-Ranking-of-Images-for-Explainability" class="headerlink" title="Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability"></a>Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01196">http://arxiv.org/abs/2308.01196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Paz-Ruza, Amparo Alonso-Betanzos, Berta Guijarro-Berdiñas, Brais Cancela, Carlos Eiras-Franco</li>
<li>for: 提高推荐系统的透明度和用户信任度</li>
<li>methods: 使用用户创建的视觉内容生成个性化解释</li>
<li>results: 比前方法更高效，减少了75%的CO${_2}$排放和模型尺寸，在六个实际数据集上达到了一致性superior表现，而且具有remarkable efficiency和小型模型优势。<details>
<summary>Abstract</summary>
Recommender Systems have become crucial in the modern world, commonly guiding users towards relevant content or products, and having a large influence over the decisions of users and citizens. However, ensuring transparency and user trust in these systems remains a challenge; personalized explanations have emerged as a solution, offering justifications for recommendations. Among the existing approaches for generating personalized explanations, using visual content created by the users is one particularly promising option, showing a potential to maximize transparency and user trust. Existing models for explaining recommendations in this context face limitations: sustainability has been a critical concern, as they often require substantial computational resources, leading to significant carbon emissions comparable to the Recommender Systems where they would be integrated. Moreover, most models employ surrogate learning goals that do not align with the objective of ranking the most effective personalized explanations for a given recommendation, leading to a suboptimal learning process and larger model sizes. To address these limitations, we present BRIE, a novel model designed to tackle the existing challenges by adopting a more adequate learning goal based on Bayesian Pairwise Ranking, enabling it to achieve consistently superior performance than state-of-the-art models in six real-world datasets, while exhibiting remarkable efficiency, emitting up to 75% less CO${_2}$ during training and inference with a model up to 64 times smaller than previous approaches.
</details>
<details>
<summary>摘要</summary>
现有的解释推荐模型在这个上下文中存在限制：它们通常需要大量的计算资源，导致显著的碳排放和大型模型，与推荐系统集成时的碳排放相比。此外，大多数模型使用代理学习目标，这些目标与个性化解释排名的目标不一致，导致学习过程不优化和模型较大。为解决这些限制，我们提出了 BRIE，一种新的模型，通过采用更适合的学习目标基于 bayesian pairwise ranking，实现了与现状最佳的性能，在六个实际数据集上表现出了明显的优势，同时具有很好的效率和较小的模型大小。在训练和推理过程中，BRIE可以减少75%的碳排放，并且模型可以达到64倍小于现有方法。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-for-Medical-Imaging-extending-the-MONAI-Framework"><a href="#Generative-AI-for-Medical-Imaging-extending-the-MONAI-Framework" class="headerlink" title="Generative AI for Medical Imaging: extending the MONAI Framework"></a>Generative AI for Medical Imaging: extending the MONAI Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15208">http://arxiv.org/abs/2307.15208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/project-monai/generativemodels">https://github.com/project-monai/generativemodels</a></li>
<li>paper_authors: Walter H. L. Pinaya, Mark S. Graham, Eric Kerfoot, Petru-Daniel Tudosiu, Jessica Dafflon, Virginia Fernandez, Pedro Sanchez, Julia Wolleb, Pedro F. da Costa, Ashay Patel, Hyungjin Chung, Can Zhao, Wei Peng, Zelong Liu, Xueyan Mei, Oeslle Lucena, Jong Chul Ye, Sotirios A. Tsaftaris, Prerna Dogra, Andrew Feng, Marc Modat, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso</li>
<li>for: 本研究旨在提供一个开源平台，专门用于训练、评估和部署生成模型和相关应用。</li>
<li>methods: 本研究使用了多种生成模型，包括扩散模型、自动推导 трансформа器和GANs，并在一个通用的方式下实现了这些模型。</li>
<li>results: 本研究可以将生成模型应用到不同的领域，包括医疗影像的问题检测、图像对图像翻译、干扰除和MRI重建。 results表明，这些模型可以在不同的领域中实现高效和精准的结果。<details>
<summary>Abstract</summary>
Recent advances in generative AI have brought incredible breakthroughs in several areas, including medical imaging. These generative models have tremendous potential not only to help safely share medical data via synthetic datasets but also to perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due to the complexity of these models, their implementation and reproducibility can be difficult. This complexity can hinder progress, act as a use barrier, and dissuade the comparison of new methods with existing works. In this study, we present MONAI Generative Models, a freely available open-source platform that allows researchers and developers to easily train, evaluate, and deploy generative models and related applications. Our platform reproduces state-of-art studies in a standardised way involving different architectures (such as diffusion models, autoregressive transformers, and GANs), and provides pre-trained models for the community. We have implemented these models in a generalisable fashion, illustrating that their results can be extended to 2D or 3D scenarios, including medical images with different modalities (like CT, MRI, and X-Ray data) and from different anatomical areas. Finally, we adopt a modular and extensible approach, ensuring long-term maintainability and the extension of current applications for future features.
</details>
<details>
<summary>摘要</summary>
In this study, we present MONAI Generative Models, a freely available open-source platform that allows researchers and developers to easily train, evaluate, and deploy generative models and related applications. Our platform reproduces state-of-the-art studies in a standardized way involving different architectures (such as diffusion models, autoregressive transformers, and GANs), and provides pre-trained models for the community. We have implemented these models in a generalizable fashion, illustrating that their results can be extended to 2D or 3D scenarios, including medical images with different modalities (such as CT, MRI, and X-ray data) and from different anatomical areas.Finally, we adopt a modular and extensible approach, ensuring long-term maintainability and the extension of current applications for future features.
</details></li>
</ul>
<hr>
<h2 id="Small-but-important-Traffic-light-proposals-for-detecting-small-traffic-lights-and-beyond"><a href="#Small-but-important-Traffic-light-proposals-for-detecting-small-traffic-lights-and-beyond" class="headerlink" title="Small, but important: Traffic light proposals for detecting small traffic lights and beyond"></a>Small, but important: Traffic light proposals for detecting small traffic lights and beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15191">http://arxiv.org/abs/2307.15191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Sanitz, Christian Wilms, Simone Frintrop</li>
<li>for: 提高小型交通灯的检测精度</li>
<li>methods: 提出了一种新的交通灯检测系统，包括基于通用物体提案生成的新的交通灯提案生成器，以及细致多尺度特征和注意力机制，以提高检测效果。</li>
<li>results: 对三个公共可用的数据集进行评估，与六种方法进行比较，结果显示小型交通灯的检测精度提高至少12.6%，并在所有交通灯大小上表现优异。<details>
<summary>Abstract</summary>
Traffic light detection is a challenging problem in the context of self-driving cars and driver assistance systems. While most existing systems produce good results on large traffic lights, detecting small and tiny ones is often overlooked. A key problem here is the inherent downsampling in CNNs, leading to low-resolution features for detection. To mitigate this problem, we propose a new traffic light detection system, comprising a novel traffic light proposal generator that utilizes findings from general object proposal generation, fine-grained multi-scale features, and attention for efficient processing. Moreover, we design a new detection head for classifying and refining our proposals. We evaluate our system on three challenging, publicly available datasets and compare it against six methods. The results show substantial improvements of at least $12.6\%$ on small and tiny traffic lights, as well as strong results across all sizes of traffic lights.
</details>
<details>
<summary>摘要</summary>
干货灯检测是自驾车和驾驶助手系统中的一个挑战。大多数现有系统可以在大型干货灯上提供良好的结果，但检测小型和微型干货灯通常被忽略。这里的关键问题在于卷积神经网络中的自然下采样问题，导致检测特征的解析精度低下。为解决这个问题，我们提出了一个新的干货灯检测系统，包括一个新的干货灯提案生成器，该生成器利用通用物体提案生成的发现，以及细腻多尺度特征和注意力来实现高效处理。此外，我们还设计了一个新的检测头来分类和精细地修正我们的提案。我们在三个公共可用的数据集上评估了我们的系统，并与六种方法进行比较。结果显示，我们的系统在小型和微型干货灯上提供了至少12.6%的提升，并在所有干货灯大小上达到了强劲的结果。
</details></li>
</ul>
<hr>
<h2 id="EnSolver-Uncertainty-Aware-CAPTCHA-Solver-Using-Deep-Ensembles"><a href="#EnSolver-Uncertainty-Aware-CAPTCHA-Solver-Using-Deep-Ensembles" class="headerlink" title="EnSolver: Uncertainty-Aware CAPTCHA Solver Using Deep Ensembles"></a>EnSolver: Uncertainty-Aware CAPTCHA Solver Using Deep Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15180">http://arxiv.org/abs/2307.15180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hoangcongduc/ensolver">https://github.com/hoangcongduc/ensolver</a></li>
<li>paper_authors: Duc C. Hoang, Cuong V. Nguyen, Amin Kharraz</li>
<li>for: 保护网站从自动化机器人的攻击中，通过使用文本基于的 CAPTCHA 安全机制。</li>
<li>methods: 使用深度学习技术建立 CAPTCHA 解决器，并使用深度ensemble不确定性估计来检测和跳过不符合预期的样本。</li>
<li>results: 使用对象检测模型和实验结果表明，EnSolver 可以在不同样本中具有高度的准确率和成功率，达到98.1% 和93% 分别。<details>
<summary>Abstract</summary>
The popularity of text-based CAPTCHA as a security mechanism to protect websites from automated bots has prompted researches in CAPTCHA solvers, with the aim of understanding its failure cases and subsequently making CAPTCHAs more secure. Recently proposed solvers, built on advances in deep learning, are able to crack even the very challenging CAPTCHAs with high accuracy. However, these solvers often perform poorly on out-of-distribution samples that contain visual features different from those in the training set. Furthermore, they lack the ability to detect and avoid such samples, making them susceptible to being locked out by defense systems after a certain number of failed attempts. In this paper, we propose EnSolver, a novel CAPTCHA solver that utilizes deep ensemble uncertainty estimation to detect and skip out-of-distribution CAPTCHAs, making it harder to be detected. We demonstrate the use of our solver with object detection models and show empirically that it performs well on both in-distribution and out-of-distribution data, achieving up to 98.1% accuracy when detecting out-of-distribution data and up to 93% success rate when solving in-distribution CAPTCHAs.
</details>
<details>
<summary>摘要</summary>
受欢迎的文本基于CAPTCHA作为网站自动化软件保护机制的流行性，使得研究人员努力开发CAPTCHA解决方案，以了解其失败情况，并使CAPTCHA更加安全。最近提出的解决方案基于深度学习技术，能够解决even the very challenging CAPTCHAs with high accuracy。然而，这些解决方案经常在不同于训练集的视觉特征的样本上表现不佳，并且缺乏检测和避免这些样本的能力，使其容易被防御系统锁定。在本文中，我们提出EnSolver，一种新的CAPTCHA解决方案，利用深度ensemble uncertainty estimation来检测和跳过不同于训练集的CAPTCHAs，使其更难被检测。我们使用对象检测模型来实现我们的解决方案，并证明了其在各种数据上的良好性，包括在分布型和不同分布型数据上的性能，达到了98.1%的检测精度和93%的成功率。
</details></li>
</ul>
<hr>
<h2 id="R-LPIPS-An-Adversarially-Robust-Perceptual-Similarity-Metric"><a href="#R-LPIPS-An-Adversarially-Robust-Perceptual-Similarity-Metric" class="headerlink" title="R-LPIPS: An Adversarially Robust Perceptual Similarity Metric"></a>R-LPIPS: An Adversarially Robust Perceptual Similarity Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15157">http://arxiv.org/abs/2307.15157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saraghazanfari/r-lpips">https://github.com/saraghazanfari/r-lpips</a></li>
<li>paper_authors: Sara Ghazanfari, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, Alexandre Araujo</li>
<li>for: 本研究旨在提出一种robust learned perceptual image patch similarity（R-LPIPS）度量，以提高图像相似度评估中的安全性。</li>
<li>methods: 该度量使用了 adversarially trained deep features，并通过了一系列实验证明其比 классиical LPIPS 度量更加稳定和可靠。</li>
<li>results: 研究表明，R-LPIPS 度量能够更好地抗击 adversarial examples，并且在大规模应用中具有更高的安全性。<details>
<summary>Abstract</summary>
Similarity metrics have played a significant role in computer vision to capture the underlying semantics of images. In recent years, advanced similarity metrics, such as the Learned Perceptual Image Patch Similarity (LPIPS), have emerged. These metrics leverage deep features extracted from trained neural networks and have demonstrated a remarkable ability to closely align with human perception when evaluating relative image similarity. However, it is now well-known that neural networks are susceptible to adversarial examples, i.e., small perturbations invisible to humans crafted to deliberately mislead the model. Consequently, the LPIPS metric is also sensitive to such adversarial examples. This susceptibility introduces significant security concerns, especially considering the widespread adoption of LPIPS in large-scale applications. In this paper, we propose the Robust Learned Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages adversarially trained deep features. Through a comprehensive set of experiments, we demonstrate the superiority of R-LPIPS compared to the classical LPIPS metric. The code is available at https://github.com/SaraGhazanfari/R-LPIPS.
</details>
<details>
<summary>摘要</summary>
Computer vision 中的相似度度量有着重要的作用，用于捕捉图像的含义。近年来，高级相似度度量，如学习的 Perceptual Image Patch Similarity（LPIPS），得到了广泛应用。这些度量利用训练过的神经网络提取的深度特征，并在评估图像相似性时表现出了人类视觉的惊人能力。然而，现在已经公认的是，神经网络受到攻击性例子的威胁，即通过小型的隐蔽的扰动量让模型做出错误的判断。这种敏感性引入了重要的安全问题，特别是在大规模应用中。在这篇论文中，我们提出了Robust Learned Perceptual Image Patch Similarity（R-LPIPS）度量，一种新的度量，利用攻击性训练的深度特征。通过全面的实验，我们证明了R-LPIPS在相似性评估中的优越性，相比于经典的LPIPS度量。代码可以在https://github.com/SaraGhazanfari/R-LPIPS中找到。
</details></li>
</ul>
<hr>
<h2 id="R-Block-Regularized-Block-of-Dropout-for-convolutional-networks"><a href="#R-Block-Regularized-Block-of-Dropout-for-convolutional-networks" class="headerlink" title="R-Block: Regularized Block of Dropout for convolutional networks"></a>R-Block: Regularized Block of Dropout for convolutional networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15150">http://arxiv.org/abs/2307.15150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liqi Wang, Qiya Hu</li>
<li>for: 本研究旨在提出一种基于对比学习的卷积层正则化技术，以提高卷积神经网络的性能。</li>
<li>methods: 本研究使用了一种名为R-Block的对比学习训练策略，其目的是在卷积层中对两个不同采样的输出进行匹配。具体来说，R-Block将两个不同采样的卷积层输出的分布差用来最小化训练集的损失。我们还提出了两种构建子模型的方法。</li>
<li>results: 我们的实验结果显示，R-Block在比较其他结构化抛出变体时表现更好，而且我们的子模型构建方法也超过了其他方法。<details>
<summary>Abstract</summary>
Dropout as a regularization technique is widely used in fully connected layers while is less effective in convolutional layers. Therefore more structured forms of dropout have been proposed to regularize convolutional networks. The disadvantage of these methods is that the randomness introduced causes inconsistency between training and inference. In this paper, we apply a mutual learning training strategy for convolutional layer regularization, namely R-Block, which forces two outputs of the generated difference maximizing sub models to be consistent with each other. Concretely, R-Block minimizes the losses between the output distributions of two sub models with different drop regions for each sample in the training dataset. We design two approaches to construct such sub models. Our experiments demonstrate that R-Block achieves better performance than other existing structured dropout variants. We also demonstrate that our approaches to construct sub models outperforms others.
</details>
<details>
<summary>摘要</summary>
Dropout 作为一种常用的正则化技术，通常在全连接层中使用，而在卷积层中效果较差。因此，为了正则化卷积网络，更结构化的Dropout变体被提议。然而，这些方法的Randomness引入会导致训练和测试过程中的不一致。在这篇论文中，我们采用了相互学习训练策略，即R-Block，使得两个由生成的差分最大化子模型输出的结果彼此一致。具体来说，R-Block将每个训练集中的样本的输出分布between两个不同掉除区域的子模型Minimize the loss。我们设计了两种方法来构建子模型。我们的实验表明，R-Block在其他已有结构化Dropout变体的比较中表现更好。此外，我们的子模型构建方法也超过了其他方法。
</details></li>
</ul>
<hr>
<h2 id="Online-Clustered-Codebook"><a href="#Online-Clustered-Codebook" class="headerlink" title="Online Clustered Codebook"></a>Online Clustered Codebook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15139">http://arxiv.org/abs/2307.15139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lyndonzheng/cvq-vae">https://github.com/lyndonzheng/cvq-vae</a></li>
<li>paper_authors: Chuanxia Zheng, Andrea Vedaldi</li>
<li>for: 这 paper 的目的是提出一种简单的在线代码库学习方法，以解决现有 VQ-VAE 中的代码Vector 归一化问题。</li>
<li>methods: 这 paper 使用 Clustering VQ-VAE (CVQ-VAE) 方法，选择编码特征作为更新“死亡”的代码Vector 的参考点，同时使用原始损失来优化代码库。</li>
<li>results: 该 paper 的 CVQ-VAE 方法可以广泛验证在不同的 dataset、任务（如重建和生成）和架构（如 VQ-VAE、VQGAN、LDM）上，并且可以轻松地与现有模型集成。<details>
<summary>Abstract</summary>
Vector Quantisation (VQ) is experiencing a comeback in machine learning, where it is increasingly used in representation learning. However, optimizing the codevectors in existing VQ-VAE is not entirely trivial. A problem is codebook collapse, where only a small subset of codevectors receive gradients useful for their optimisation, whereas a majority of them simply ``dies off'' and is never updated or used. This limits the effectiveness of VQ for learning larger codebooks in complex computer vision tasks that require high-capacity representations. In this paper, we present a simple alternative method for online codebook learning, Clustering VQ-VAE (CVQ-VAE). Our approach selects encoded features as anchors to update the ``dead'' codevectors, while optimising the codebooks which are alive via the original loss. This strategy brings unused codevectors closer in distribution to the encoded features, increasing the likelihood of being chosen and optimized. We extensively validate the generalization capability of our quantiser on various datasets, tasks (e.g. reconstruction and generation), and architectures (e.g. VQ-VAE, VQGAN, LDM). Our CVQ-VAE can be easily integrated into the existing models with just a few lines of code.
</details>
<details>
<summary>摘要</summary>
vector量化（VQ）在机器学习中经受着重新发现，现在越来越在表示学习中使用。然而，在现有的VQ-VAE中优化codevector并不是完全懒散的。一个问题是codebook塌缩，只有一小部分的codevector会收到有用的梯度更新，而大多数codevector会“死亡”并从未更新或使用。这限制了VQ在学习更大的codebook时的效iveness，特别是在复杂的计算机视觉任务中需要高容量表示。在这篇论文中，我们提出了一种简单的在线代码库学习方法，即Clustering VQ-VAE（CVQ-VAE）。我们的方法选择编码特征作为更新“死亡” codevector的锚点，同时通过原始损失来优化活跃的代码库。这种策略使得无用的codevector更近于编码特征的分布，提高了它们的选择和优化的可能性。我们广泛验证了我们的量化器在不同的数据集、任务（例如重建和生成）和结构（例如VQ-VAE、VQGAN、LDM）上的通用能力。我们的CVQ-VAE可以轻松地与现有模型集成，只需要几行代码。
</details></li>
</ul>
<hr>
<h2 id="Seal-3D-Interactive-Pixel-Level-Editing-for-Neural-Radiance-Fields"><a href="#Seal-3D-Interactive-Pixel-Level-Editing-for-Neural-Radiance-Fields" class="headerlink" title="Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields"></a>Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15131">http://arxiv.org/abs/2307.15131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/windingwind/seal-3d">https://github.com/windingwind/seal-3d</a></li>
<li>paper_authors: Xiangyu Wang, Jingsen Zhu, Qi Ye, Yuchi Huo, Yunlong Ran, Zhihua Zhong, Jiming Chen</li>
<li>for: 这个论文旨在提供一种可交互地编辑神经表示的方法，以便在受限的编辑灵活性、质量和速度等方面提高NeRF编辑的效果。</li>
<li>methods: 该方法使用一种新的教师-学生训练策略和本地预训练和全球精度调整，将编辑指令映射到原始NeRF模型空间，以实现直接响应编辑指令并快速预览编辑效果。</li>
<li>results: 该方法可以实现各种编辑效果，并且可以在约1秒钟的交互速度下达到出色的编辑效果。<details>
<summary>Abstract</summary>
With the popularity of implicit neural representations, or neural radiance fields (NeRF), there is a pressing need for editing methods to interact with the implicit 3D models for tasks like post-processing reconstructed scenes and 3D content creation. While previous works have explored NeRF editing from various perspectives, they are restricted in editing flexibility, quality, and speed, failing to offer direct editing response and instant preview. The key challenge is to conceive a locally editable neural representation that can directly reflect the editing instructions and update instantly. To bridge the gap, we propose a new interactive editing method and system for implicit representations, called Seal-3D, which allows users to edit NeRF models in a pixel-level and free manner with a wide range of NeRF-like backbone and preview the editing effects instantly. To achieve the effects, the challenges are addressed by our proposed proxy function mapping the editing instructions to the original space of NeRF models and a teacher-student training strategy with local pretraining and global finetuning. A NeRF editing system is built to showcase various editing types. Our system can achieve compelling editing effects with an interactive speed of about 1 second.
</details>
<details>
<summary>摘要</summary>
《 neural radiance fields (NeRF) 的 популярность导致了对 implicit 3D 模型的编辑方法的强需求，以便在重建场景后处理和创建3D内容中进行交互式编辑。而过去的作品都已经在不同的角度探索了 NeRF 编辑，但它们的编辑灵活性、质量和速度受到了限制，无法提供直接的编辑回应和即时预览。关键挑战是总结一种可以直接反映编辑指令并快速更新的 neural representation。为了bridging这个差距，我们提出了一种新的交互式编辑方法和系统，叫做 Seal-3D，允许用户在像素级别和自由地编辑 NeRF 模型，并在即时预览编辑效果。以解决这些挑战，我们提出了一种代理函数，将编辑指令映射到原始 NeRF 模型的空间，以及一种教师学生训练策略，包括本地预训练和全球调整。我们建立了一个 NeRF 编辑系统，以示出多种编辑类型。我们的系统可以实现吸引人的编辑效果，编辑速度约为1秒。》
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Remote-Sensing-Change-Detection-of-Unregistered-Bi-temporal-Images-for-Natural-Disasters"><a href="#End-to-end-Remote-Sensing-Change-Detection-of-Unregistered-Bi-temporal-Images-for-Natural-Disasters" class="headerlink" title="End-to-end Remote Sensing Change Detection of Unregistered Bi-temporal Images for Natural Disasters"></a>End-to-end Remote Sensing Change Detection of Unregistered Bi-temporal Images for Natural Disasters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15128">http://arxiv.org/abs/2307.15128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guiqin Zhao, Lianlei Shan, Weiqiang Wang</li>
<li>for: 本研究旨在针对自然灾害区域内的建筑物损害检测，通过远程感知图像进行检测。</li>
<li>methods: 本研究使用了深度网络，并提出了一种无需注册的端到端变化检测网络（E2ECDNet），可以处理不匹配的双时间图像对。</li>
<li>results: 实验结果表明，E2ECDNet 能够在不匹配的双时间图像对上提供高精度的变化检测结果，并且与现有的注册变化检测方法相比，具有更高的精度和更快的运算速度。<details>
<summary>Abstract</summary>
Change detection based on remote sensing images has been a prominent area of interest in the field of remote sensing. Deep networks have demonstrated significant success in detecting changes in bi-temporal remote sensing images and have found applications in various fields. Given the degradation of natural environments and the frequent occurrence of natural disasters, accurately and swiftly identifying damaged buildings in disaster-stricken areas through remote sensing images holds immense significance. This paper aims to investigate change detection specifically for natural disasters. Considering that existing public datasets used in change detection research are registered, which does not align with the practical scenario where bi-temporal images are not matched, this paper introduces an unregistered end-to-end change detection synthetic dataset called xBD-E2ECD. Furthermore, we propose an end-to-end change detection network named E2ECDNet, which takes an unregistered bi-temporal image pair as input and simultaneously generates the flow field prediction result and the change detection prediction result. It is worth noting that our E2ECDNet also supports change detection for registered image pairs, as registration can be seen as a special case of non-registration. Additionally, this paper redefines the criteria for correctly predicting a positive case and introduces neighborhood-based change detection evaluation metrics. The experimental results have demonstrated significant improvements.
</details>
<details>
<summary>摘要</summary>
改变探测基于远程感知图像已经成为远程感知领域的一个主要领域。深度网络在比时图像之间进行改变探测中表现出了显著的成功，并在不同领域找到了应用。随着自然环境的衰退和自然灾害的频繁发生，通过远程感知图像快速和准确地确定灾难hit buildings是非常重要的。本文旨在研究自然灾害中的改变探测。由于现有的公共数据集在改变探测研究中使用的是注册的，这并不符合实际情况，在这种情况下，本文引入了一个无注册的综合改变探测数据集called xBD-E2ECD。此外，我们提议一种综合改变探测网络，称为E2ECDNet，该网络可以将无注册的双时图像对作为输入，并同时生成流场预测结果和改变探测预测结果。需要注意的是，我们的E2ECDNet还支持注册图像对的改变探测，因为注册可以看作特殊的非注册情况。此外，本文还重新定义了正确预测正例的标准，并引入了邻居基于改变探测评价指标。实验结果表明了显著的改进。
</details></li>
</ul>
<hr>
<h2 id="To-Adapt-or-Not-to-Adapt-Real-Time-Adaptation-for-Semantic-Segmentation"><a href="#To-Adapt-or-Not-to-Adapt-Real-Time-Adaptation-for-Semantic-Segmentation" class="headerlink" title="To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation"></a>To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15063">http://arxiv.org/abs/2307.15063</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MarcBotet/hamlet">https://github.com/MarcBotet/hamlet</a></li>
<li>paper_authors: Marc Botet Colomer, Pier Luigi Dovesi, Theodoros Panagiotakopoulos, Joao Frederico Carvalho, Linus Härenstam-Nielsen, Hossein Azizpour, Hedvig Kjellström, Daniel Cremers, Matteo Poggi</li>
<li>for: 该论文旨在解决在部署时出现不可预期的领域变化，如突发天气事件，以实现 semantic segmentation 的在线领域适应。</li>
<li>methods: 该论文提出了一种基于硬件意识的模块最低成本训练框架（HAMLET），包括硬件意识反推协调器（HAMT）和特有的领域偏移探测器（LT），以实现实时领域适应。</li>
<li>results: 该论文的方法可以在单个consumer-grade GPU上达到更高于29帧&#x2F;秒的同时进行 semantic segmentation 和领域适应，并且在 OnDA 和 SHIFT 标准准则上实现了鼓舞人的准确率和速度质量平衡。<details>
<summary>Abstract</summary>
The goal of Online Domain Adaptation for semantic segmentation is to handle unforeseeable domain changes that occur during deployment, like sudden weather events. However, the high computational costs associated with brute-force adaptation make this paradigm unfeasible for real-world applications. In this paper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training framework for real-time domain adaptation. Our approach includes a hardware-aware back-propagation orchestration agent (HAMT) and a dedicated domain-shift detector that enables active control over when and how the model is adapted (LT). Thanks to these advancements, our approach is capable of performing semantic segmentation while simultaneously adapting at more than 29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy and speed trade-off is demonstrated on OnDA and SHIFT benchmarks through experimental results.
</details>
<details>
<summary>摘要</summary>
goal of Online Domain Adaptation for semantic segmentation is to handle unforeseeable domain changes that occur during deployment, like sudden weather events. However, the high computational costs associated with brute-force adaptation make this paradigm unfeasible for real-world applications. In this paper, we propose HAMLET, a Hardware-Aware Modular Least Expensive Training framework for real-time domain adaptation. Our approach includes a hardware-aware back-propagation orchestration agent (HAMT) and a dedicated domain-shift detector that enables active control over when and how the model is adapted (LT). Thanks to these advancements, our approach is capable of performing semantic segmentation while simultaneously adapting at more than 29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy and speed trade-off is demonstrated on OnDA and SHIFT benchmarks through experimental results.Here's the translation in Traditional Chinese:goal of Online Domain Adaptation for semantic segmentation is to handle unforeseeable domain changes that occur during deployment, like sudden weather events. However, the high computational costs associated with brute-force adaptation make this paradigm unfeasible for real-world applications. In this paper, we propose HAMLET, a Hardware-Aware Modular Least Expensive Training framework for real-time domain adaptation. Our approach includes a hardware-aware back-propagation orchestration agent (HAMT) and a dedicated domain-shift detector that enables active control over when and how the model is adapted (LT). Thanks to these advancements, our approach is capable of performing semantic segmentation while simultaneously adapting at more than 29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy and speed trade-off is demonstrated on OnDA and SHIFT benchmarks through experimental results.
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Visual-Acoustic-Matching"><a href="#Self-Supervised-Visual-Acoustic-Matching" class="headerlink" title="Self-Supervised Visual Acoustic Matching"></a>Self-Supervised Visual Acoustic Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15064">http://arxiv.org/abs/2307.15064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Somayazulu, Changan Chen, Kristen Grauman</li>
<li>for: 用于自然语言处理和媒体生成等应用场景，恢复和重新生成受到环境影响的音频clip。</li>
<li>methods: 提出一种自监督的方法，只使用目标场景图像和音频，不需要匹配的源音频作为参考。通过 Conditional GAN 框架和一种新的评价指标，学习抽象房间声学特征和重新生成音频。</li>
<li>results: 在多个挑战性的数据集上，与当前状态势最高，并在各种真实世界的音频和环境下表现出色。<details>
<summary>Abstract</summary>
Acoustic matching aims to re-synthesize an audio clip to sound as if it were recorded in a target acoustic environment. Existing methods assume access to paired training data, where the audio is observed in both source and target environments, but this limits the diversity of training data or requires the use of simulated data or heuristics to create paired samples. We propose a self-supervised approach to visual acoustic matching where training samples include only the target scene image and audio -- without acoustically mismatched source audio for reference. Our approach jointly learns to disentangle room acoustics and re-synthesize audio into the target environment, via a conditional GAN framework and a novel metric that quantifies the level of residual acoustic information in the de-biased audio. Training with either in-the-wild web data or simulated data, we demonstrate it outperforms the state-of-the-art on multiple challenging datasets and a wide variety of real-world audio and environments.
</details>
<details>
<summary>摘要</summary>
听音匹配目标是重新synthesize一个音频片段，使其在目标听音环境中 зву律。现有方法假设有对称的训练数据，其中包括源和目标环境中的音频，但这限制了训练数据的多样性或需要使用模拟数据或规则来生成对应的样本。我们提出了一种无监督的方法，通过jointly学习抽离房间听音和重新synthesize音频到目标环境中，使用条件GAAN框架和一个新的度量量化听音信息的减噪度。我们在使用实际网络数据或模拟数据进行训练后，示出其在多个挑战性 dataset 和真实世界的听音和环境中表现出色。
</details></li>
</ul>
<hr>
<h2 id="The-RoboDepth-Challenge-Methods-and-Advancements-Towards-Robust-Depth-Estimation"><a href="#The-RoboDepth-Challenge-Methods-and-Advancements-Towards-Robust-Depth-Estimation" class="headerlink" title="The RoboDepth Challenge: Methods and Advancements Towards Robust Depth Estimation"></a>The RoboDepth Challenge: Methods and Advancements Towards Robust Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15061">http://arxiv.org/abs/2307.15061</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ldkong1205/robodepth">https://github.com/ldkong1205/robodepth</a></li>
<li>paper_authors: Lingdong Kong, Yaru Niu, Shaoyuan Xie, Hanjiang Hu, Lai Xing Ng, Benoit R. Cottereau, Ding Zhao, Liangjun Zhang, Hesheng Wang, Wei Tsang Ooi, Ruijie Zhu, Ziyang Song, Li Liu, Tianzhu Zhang, Jun Yu, Mohan Jing, Pengwei Li, Xiaohua Qi, Cheng Jin, Yingfeng Chen, Jie Hou, Jie Zhang, Zhen Kan, Qiang Ling, Liang Peng, Minglei Li, Di Xu, Changpeng Yang, Yuanqi Yao, Gang Wu, Jian Kuai, Xianming Liu, Junjun Jiang, Jiamian Huang, Baojun Li, Jiale Chen, Shuang Zhang, Sun Ao, Zhenyu Li, Runze Chen, Haiyong Luo, Fang Zhao, Jingze Yu</li>
<li>for: 提高安全应用中深度估计的可靠性，如在不良天气、传感器故障和噪声污染等情况下提供可靠的深度预测。</li>
<li>methods: 使用自然语言处理、图像修复、超解析、对抗训练、扩散噪声消除、视觉语言预训练、学习模型ensemble和层次特征强化等方法来提高深度估计的 Robustness 和可靠性。</li>
<li>results: 通过RoboDepth Challenge的学术竞赛，发现了9种top-performing解决方案，包括空间-和频率域扩充、面罩模型、图像修复和超解析、对抗训练、扩散噪声消除、视觉语言预训练、学习模型ensemble和层次特征强化等方法，这些方法能够提高深度估计的可靠性和Robustness。<details>
<summary>Abstract</summary>
Accurate depth estimation under out-of-distribution (OoD) scenarios, such as adverse weather conditions, sensor failure, and noise contamination, is desirable for safety-critical applications. Existing depth estimation systems, however, suffer inevitably from real-world corruptions and perturbations and are struggled to provide reliable depth predictions under such cases. In this paper, we summarize the winning solutions from the RoboDepth Challenge -- an academic competition designed to facilitate and advance robust OoD depth estimation. This challenge was developed based on the newly established KITTI-C and NYUDepth2-C benchmarks. We hosted two stand-alone tracks, with an emphasis on robust self-supervised and robust fully-supervised depth estimation, respectively. Out of more than two hundred participants, nine unique and top-performing solutions have appeared, with novel designs ranging from the following aspects: spatial- and frequency-domain augmentations, masked image modeling, image restoration and super-resolution, adversarial training, diffusion-based noise suppression, vision-language pre-training, learned model ensembling, and hierarchical feature enhancement. Extensive experimental analyses along with insightful observations are drawn to better understand the rationale behind each design. We hope this challenge could lay a solid foundation for future research on robust and reliable depth estimation and beyond. The datasets, competition toolkit, workshop recordings, and source code from the winning teams are publicly available on the challenge website.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>实时深度估计在不同的应用场景中具有重要的意义，如恶劣天气、传感器故障和噪声污染等情况下，却存在现实世界中的干扰和损害，使得现有的深度估计系统很难提供可靠的深度预测。在这篇论文中，我们总结了RoboDepth Challenge中的胜利解决方案，这是一个学术竞赛，旨在促进和进步强健的应用场景外的深度估计。这个竞赛基于新建的KITTI-C和NYUDepth2-C标准准则。我们设置了两个独立的轨道，强调强健自我超vised和强健全supervised深度估计。共有超过200名参与者，9个独特和表现出色的解决方案出现在了，其中包括空间和频率域扩充、掩码图像模型、图像修复和超分辨率、对抗训练、扩散型噪声消除、视语预训练、学习集成和层次特征增强等方法。我们进行了广泛的实验分析和深入的观察，以更好地理解每种设计的原理。我们希望这次竞赛可以为未来的强健和可靠的深度估计奠定坚实的基础，并超出这个领域。竞赛网站上公开了数据集、竞赛工具箱、学术会录音和胜利团队的源代码。
</details></li>
</ul>
<hr>
<h2 id="MARS-An-Instance-aware-Modular-and-Realistic-Simulator-for-Autonomous-Driving"><a href="#MARS-An-Instance-aware-Modular-and-Realistic-Simulator-for-Autonomous-Driving" class="headerlink" title="MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous Driving"></a>MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15058">http://arxiv.org/abs/2307.15058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/open-air-sun/mars">https://github.com/open-air-sun/mars</a></li>
<li>paper_authors: Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao, Chao Hou, Haozhe Lou, Yuantao Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan, Yongliang Shi, Yiyi Liao, Hao Zhao</li>
<li>For: The paper proposes an autonomous driving simulator based on neural radiance fields (NeRFs) to solve remaining corner cases and improve realism in simulation.* Methods: The simulator models foreground instances and background environments separately with independent networks, allowing for flexible switching between different NeRF-related backbones, sampling strategies, input modalities, etc.* Results: The simulator achieves state-of-the-art photo-realism results and will be open-sourced, while most counterparts are not.Here’s the simplified Chinese text for the three key points:* For: 该论文提出了基于神经采样场（NeRF）的自动驾驶模拟器，以解决剩下的角落情况并提高模拟的真实性。* Methods: 该模拟器将前景实体和背景环境分别模型为独立的网络，允许自由地换换不同的NeRF相关脊梁、采样策略、输入模式等。* Results: 该模拟器实现了状态机器人化的图像真实性结果，并将被开源发布，而大多数对手不是。<details>
<summary>Abstract</summary>
Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is widely recognized that realistic sensor simulation will play a critical role in solving remaining corner cases by simulating them. To this end, we propose an autonomous driving simulator based upon neural radiance fields (NeRFs). Compared with existing works, ours has three notable features: (1) Instance-aware. Our simulator models the foreground instances and background environments separately with independent networks so that the static (e.g., size and appearance) and dynamic (e.g., trajectory) properties of instances can be controlled separately. (2) Modular. Our simulator allows flexible switching between different modern NeRF-related backbones, sampling strategies, input modalities, etc. We expect this modular design to boost academic progress and industrial deployment of NeRF-based autonomous driving simulation. (3) Realistic. Our simulator set new state-of-the-art photo-realism results given the best module selection. Our simulator will be open-sourced while most of our counterparts are not. Project page: https://open-air-sun.github.io/mars/.
</details>
<details>
<summary>摘要</summary>
现在，自动驾驶车可以平稳驾驶在常见情况下，而实际感知器模拟将在解决剩下的角度情况中扮演关键角色。为此，我们提出了基于神经辐射场（NeRFs）的自动驾驶模拟器。与现有作品相比，我们的模拟器具有以下三个特点：1. 实例感知：我们的模拟器将背景环境和前景实例分别模型为独立的网络，以便分别控制实例的静态特性（如大小和外观）和动态特性（如轨迹）。2. 模块化：我们的模拟器允许自由地换换不同的现代NeRF相关脊梁、采样策略、输入Modalities等。我们期望这种模块化设计能促进学术进步和实业应用NeRF相关自动驾驶模拟。3. 实际：我们的模拟器在选择最佳模块时创造了新的state-of-the-art的 фото实实alomResults。我们的模拟器将被开源，而大多数对手不是。项目页面：<https://open-air-sun.github.io/mars/>。
</details></li>
</ul>
<hr>
<h2 id="PointOdyssey-A-Large-Scale-Synthetic-Dataset-for-Long-Term-Point-Tracking"><a href="#PointOdyssey-A-Large-Scale-Synthetic-Dataset-for-Long-Term-Point-Tracking" class="headerlink" title="PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking"></a>PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15055">http://arxiv.org/abs/2307.15055</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/y-zheng18/point_odyssey">https://github.com/y-zheng18/point_odyssey</a></li>
<li>paper_authors: Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wetzstein, Leonidas J. Guibas</li>
<li>for: 本研究旨在提高长期细化跟踪算法的状态前方，强调自然化的运动。</li>
<li>methods: 该研究使用真实世界动作捕捉数据来动画可变形人物，建立3D场景，并使用结构从动视觉来渲染摄像头视点。其中，人物的外观、动作特征、物理、照明和大气效果都进行了随机化。</li>
<li>results: 研究人员通过对现有算法进行修改，使其在PointOdyssey数据集上表现更好，并在两个真实世界benchmark上表现出色。此外，研究人员还提出了一种改进PIPs点跟踪方法，使其在时间上具有更广泛的感知范围，并在PointOdyssey数据集和两个真实世界benchmark上提高了表现。<details>
<summary>Abstract</summary>
We introduce PointOdyssey, a large-scale synthetic dataset, and data generation framework, for the training and evaluation of long-term fine-grained tracking algorithms. Our goal is to advance the state-of-the-art by placing emphasis on long videos with naturalistic motion. Toward the goal of naturalism, we animate deformable characters using real-world motion capture data, we build 3D scenes to match the motion capture environments, and we render camera viewpoints using trajectories mined via structure-from-motion on real videos. We create combinatorial diversity by randomizing character appearance, motion profiles, materials, lighting, 3D assets, and atmospheric effects. Our dataset currently includes 104 videos, averaging 2,000 frames long, with orders of magnitude more correspondence annotations than prior work. We show that existing methods can be trained from scratch in our dataset and outperform the published variants. Finally, we introduce modifications to the PIPs point tracking method, greatly widening its temporal receptive field, which improves its performance on PointOdyssey as well as on two real-world benchmarks. Our data and code are publicly available at: https://pointodyssey.com
</details>
<details>
<summary>摘要</summary>
我们介绍PointOdyssey，一个大规模的人工数据集和数据生成框架，用于长期精细跟踪算法的训练和评估。我们的目标是提高状态的艺术性，因此我们使用了真实的动作捕捉数据来动画可变的人物，建立了基于动作捕捉环境的3D场景，并使用了从结构 FROM 运动中挖掘的轨迹来渲染摄像头视点。我们创造了多样性的组合，通过随机化人物的外观、动作特征、物理、照明、3D资产和大气效果来创造多样性。我们的数据集目前包含104个视频，每个视频平均2,000帧长，与先前的工作相比有几个数量级的更多的对应笔记注解。我们示出了在我们数据集中可以从头开始训练现有方法，并在PointOdyssey上以及两个真实的benchmark上表现出色。最后，我们对PIPs点跟踪方法进行修改，使其 temporal 感知场景得到了大幅提高，这也提高了它的表现在PointOdyssey上以及两个真实的benchmark上。我们的数据和代码在https://pointodyssey.com 上公开 available。
</details></li>
</ul>
<hr>
<h2 id="Learning-Depth-Estimation-for-Transparent-and-Mirror-Surfaces"><a href="#Learning-Depth-Estimation-for-Transparent-and-Mirror-Surfaces" class="headerlink" title="Learning Depth Estimation for Transparent and Mirror Surfaces"></a>Learning Depth Estimation for Transparent and Mirror Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15052">http://arxiv.org/abs/2307.15052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Costanzino, Pierluigi Zama Ramirez, Matteo Poggi, Fabio Tosi, Stefano Mattoccia, Luigi Di Stefano</li>
<li>for: 估算透明或镜面（ToM）表面深度是一个困难的任务，需要感知器、算法或深度网络。</li>
<li>methods: 我们提出了一个简单的管道，使用神经网络来学习正确地估算ToM表面深度，无需任何真实标注。我们解释了如何获取可靠的 pseudo标签，通过在图像中填充ToM对象并使用单视深度估算模型来处理它们。这些标签可以用于练化现有的单视或双视网络，让它们学习如何处理ToM表面。</li>
<li>results: 在Booster数据集上进行实验，我们发现我们的简单提案具有很大的改进作用。<details>
<summary>Abstract</summary>
Inferring the depth of transparent or mirror (ToM) surfaces represents a hard challenge for either sensors, algorithms, or deep networks. We propose a simple pipeline for learning to estimate depth properly for such surfaces with neural networks, without requiring any ground-truth annotation. We unveil how to obtain reliable pseudo labels by in-painting ToM objects in images and processing them with a monocular depth estimation model. These labels can be used to fine-tune existing monocular or stereo networks, to let them learn how to deal with ToM surfaces. Experimental results on the Booster dataset show the dramatic improvements enabled by our remarkably simple proposal.
</details>
<details>
<summary>摘要</summary>
描述透明或镜面（ToM）表面的深度很难以由感知器、算法或深度网络正确地推断。我们提出了一个简单的管道，通过神经网络来学习对ToM表面的深度估计，无需任何真实标注。我们解释了如何获得可靠的pseudo标签，通过在图像中填充ToM对象并使用单目深度估计模型处理它们。这些标签可以用来练化现有的单目或双目网络，让它们学习如何处理ToM表面。实验结果表明，我们的非常简单的建议带来了 dramatic improvement。
</details></li>
</ul>
<hr>
<h2 id="Regularized-Mask-Tuning-Uncovering-Hidden-Knowledge-in-Pre-trained-Vision-Language-Models"><a href="#Regularized-Mask-Tuning-Uncovering-Hidden-Knowledge-in-Pre-trained-Vision-Language-Models" class="headerlink" title="Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models"></a>Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15049">http://arxiv.org/abs/2307.15049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kecheng Zheng, Wei Wu, Ruili Feng, Kai Zhu, Jiawei Liu, Deli Zhao, Zheng-Jun Zha, Wei Chen, Yujun Shen</li>
<li>for: 这个研究想要将预训练的潜在视觉语言模型（VLM）转移到不同的下游任务上。</li>
<li>methods: 我们提出了一种新型的调整方法，即弹性掩蔽调整法，它通过学习选择器来掩蔽网络参数。我们参考了神经通路，认为预训练过程中隐藏在网络参数中的知识，可以透过调整这些参数来把其恢复到光。我们首先选择一个下游任务中需要的参数集，然后将这些参数给掩蔽，最后在下游数据上优化这些掩蔽。当更新掩蔽时，我们引入了一种新的梯度减少策略，以调整参数选择，以避免模型忘记过去的知识并过滤下游数据。</li>
<li>results: 我们在11个数据集上进行实验，结果显示我们的方法在前一代的方法上具有优越的性能。特别是，我们可以透过仅将2.56%的参数掩蔽，实现18.73%的性能提升，比零基elineCLIP更高。此外，我们的方法可以与大多数现有的参数效率调整方法相互作用，可以将其表现提升。详细信息可以查看我们的项目页面（<a target="_blank" rel="noopener" href="https://wuw2019.github.io/R-AMT/%EF%BC%89%E3%80%82">https://wuw2019.github.io/R-AMT/）。</a><details>
<summary>Abstract</summary>
Prompt tuning and adapter tuning have shown great potential in transferring pre-trained vision-language models (VLMs) to various downstream tasks. In this work, we design a new type of tuning method, termed as regularized mask tuning, which masks the network parameters through a learnable selection. Inspired by neural pathways, we argue that the knowledge required by a downstream task already exists in the pre-trained weights but just gets concealed in the upstream pre-training stage. To bring the useful knowledge back into light, we first identify a set of parameters that are important to a given downstream task, then attach a binary mask to each parameter, and finally optimize these masks on the downstream data with the parameters frozen. When updating the mask, we introduce a novel gradient dropout strategy to regularize the parameter selection, in order to prevent the model from forgetting old knowledge and overfitting the downstream data. Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives. It is noteworthy that we manage to deliver 18.73% performance improvement compared to the zero-shot CLIP via masking an average of only 2.56% parameters. Furthermore, our method is synergistic with most existing parameter-efficient tuning methods and can boost the performance on top of them. Project page can be found here (https://wuw2019.github.io/R-AMT/).
</details>
<details>
<summary>摘要</summary>
Prompt tuning和adapter tuning已经在将预训练的视觉语言模型（VLM）转移到多种下游任务中显示了很大的潜力。在这项工作中，我们设计了一种新的调参方法，称为正则化面 Selection Tuning（R-AMT）。受神经网络的启发，我们认为预训练阶段中隐藏在预训练模型中的知识已经存在于下游任务中。为了让这些知识重新浮现，我们首先确定了一个下游任务中重要的参数集，然后将每个参数添加一个二进制面，最后在下游数据上优化这些面。在更新面时，我们引入了一种新的梯度抑制策略，以避免模型忘记原来的知识并遇到下游数据上的过拟合。实验结果在11个数据集上表明，我们的方法与之前的方法相比具有显著的优势。具体来说，我们通过面 Selection Tuning仅占用2.56%的参数平均提高了CLIP的性能18.73%。此外，我们的方法可以与大多数现有的参数效率调参方法相结合，并可以提高它们的性能。相关页面可以在这里找到（https://wuw2019.github.io/R-AMT/）。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-based-Approach-for-Arabic-Offline-Handwritten-Text-Recognition"><a href="#A-Transformer-based-Approach-for-Arabic-Offline-Handwritten-Text-Recognition" class="headerlink" title="A Transformer-based Approach for Arabic Offline Handwritten Text Recognition"></a>A Transformer-based Approach for Arabic Offline Handwritten Text Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15045">http://arxiv.org/abs/2307.15045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saleh Momeni, Bagher BabaAli</li>
<li>for: 本研究旨在提高 Offline 阿拉伯手写文本识别精度。</li>
<li>methods: 我们提出了两种新的架构：Transformer Transducer 和标准sequence-to-sequence Transformer，并对其表现进行比较。这两种架构均利用了注意力机制，可以更好地模型语言依赖关系，并且更容易并行化。</li>
<li>results: 我们的方法在 Arabic KHATT 数据集上的评估中表现出色，超越了现有的状态之作。<details>
<summary>Abstract</summary>
Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains. In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text. Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation. However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks. Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy. To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and speed. Our approach can model language dependencies and relies only on the attention mechanism, thereby making it more parallelizable and less complex. We employ pre-trained Transformers for both image understanding and language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that our proposed method outperforms the current state-of-the-art approaches for recognizing offline Arabic handwritten text.
</details>
<details>
<summary>摘要</summary>
手写识别是Pattern recognition和机器学习领域中的一个挑战和重要问题，其应用范围广泛，包括文本识别、语音识别、图像识别等领域。在本文中，我们将关注特定的问题是 Offline 阿拉伯手写文本识别。现有的方法通常使用混合 convolutional neural networks  для图像特征提取和 recurrent neural networks  для时间模型化，并使用 connectionist temporal classification  для文本生成。然而，这些方法受到缺乏并行化的约束，以及不能考虑语言规则的限制，因此需要在后期处理阶段使用外部语言模型以提高准确性。为了解决这些问题，我们提出了两种 altenative 架构：Transformer Transducer 和标准 sequence-to-sequence Transformer。我们 comparing 这两种架构的性能，包括准确率和速度。我们的方法可以模型语言依赖关系，只靠注意机制进行并行化，因此更加简单和高效。我们使用预训练的 Transformers 来进行图像理解和语言模型化。我们的评估结果表明，我们的提议方法在 Offline 阿拉伯手写文本识别领域的现状顶峰性能。
</details></li>
</ul>
<hr>
<h2 id="TEDi-Temporally-Entangled-Diffusion-for-Long-Term-Motion-Synthesis"><a href="#TEDi-Temporally-Entangled-Diffusion-for-Long-Term-Motion-Synthesis" class="headerlink" title="TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis"></a>TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15042">http://arxiv.org/abs/2307.15042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Zhang, Richard Liu, Kfir Aberman, Rana Hanocka</li>
<li>for: 提出一种基于渐进噪声泛化的动作序列 sintesis 模型，以满足长期动作synthesis的应用需求。</li>
<li>methods: 利用渐进噪声泛化概率模型（DDPM）的核心思想，在时间轴上实现渐进噪声泛化，并在动作序列中随时间变化。</li>
<li>results: 通过实验表明，提出的方法可以生成高质量的动作序列，并且可以应用于人物动画和其他领域。<details>
<summary>Abstract</summary>
The gradual nature of a diffusion process that synthesizes samples in small increments constitutes a key ingredient of Denoising Diffusion Probabilistic Models (DDPM), which have presented unprecedented quality in image synthesis and been recently explored in the motion domain. In this work, we propose to adapt the gradual diffusion concept (operating along a diffusion time-axis) into the temporal-axis of the motion sequence. Our key idea is to extend the DDPM framework to support temporally varying denoising, thereby entangling the two axes. Using our special formulation, we iteratively denoise a motion buffer that contains a set of increasingly-noised poses, which auto-regressively produces an arbitrarily long stream of frames. With a stationary diffusion time-axis, in each diffusion step we increment only the temporal-axis of the motion such that the framework produces a new, clean frame which is removed from the beginning of the buffer, followed by a newly drawn noise vector that is appended to it. This new mechanism paves the way towards a new framework for long-term motion synthesis with applications to character animation and other domains.
</details>
<details>
<summary>摘要</summary>
《慢慢进程的扩散过程 Synthesize 样本在小幅度 increments 中是关键组成部分，这种方法被称为 Denoising Diffusion Probabilistic Models (DDPM)，它在图像生成中提供了无 precedent 的质量，并在近期被探索在运动频谱中。在这项工作中，我们提议将慢慢扩散概念（在扩散时间轴上进行）扩展到运动序列的 temporal 轴。我们的关键想法是在 DDPM 框架中支持时间变化的净化，从而将两个轴相互束缚。使用我们的特殊形式ulation，我们在每个扩散步骤中只 increments 运动序列中的时间轴，以生成一个新的、干净的帧，并将其从运动缓冲中移除。然后，我们随机生成一个新的噪声向量，并将其追加到缓冲中。这种新机制开 up a new 框架，可以用于长期运动生成，并具有应用于人物动画和其他领域的潜在应用。]Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Detecting-Morphing-Attacks-via-Continual-Incremental-Training"><a href="#Detecting-Morphing-Attacks-via-Continual-Incremental-Training" class="headerlink" title="Detecting Morphing Attacks via Continual Incremental Training"></a>Detecting Morphing Attacks via Continual Incremental Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15105">http://arxiv.org/abs/2307.15105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Pellegrini, Guido Borghi, Annalisa Franco, Davide Maltoni</li>
<li>for: 这个论文旨在解决数据传输和存储限制下，难以组合单一数据集，以执行批处理训练方法。</li>
<li>methods: 这篇论文使用了不同数据源的Continual Learning（CL）方法， simulate一个随着新数据块的到达，模型在每一轮训练中更新的场景。</li>
<li>results: 实验结果显示，Learning without Forgetting（LwF）方法在这种场景下表现最佳，并且在 Morphing Attack Detection 和 Object Classification 任务中进行了详细的调研和优化。<details>
<summary>Abstract</summary>
Scenarios in which restrictions in data transfer and storage limit the possibility to compose a single dataset -- also exploiting different data sources -- to perform a batch-based training procedure, make the development of robust models particularly challenging. We hypothesize that the recent Continual Learning (CL) paradigm may represent an effective solution to enable incremental training, even through multiple sites. Indeed, a basic assumption of CL is that once a model has been trained, old data can no longer be used in successive training iterations and in principle can be deleted. Therefore, in this paper, we investigate the performance of different Continual Learning methods in this scenario, simulating a learning model that is updated every time a new chunk of data, even of variable size, is available. Experimental results reveal that a particular CL method, namely Learning without Forgetting (LwF), is one of the best-performing algorithms. Then, we investigate its usage and parametrization in Morphing Attack Detection and Object Classification tasks, specifically with respect to the amount of new training data that became available.
</details>
<details>
<summary>摘要</summary>
具有限制数据传输和存储的场景下，组成单一数据集并在多个数据源上进行批处理训练过程中存在很大挑战。我们假设，最近的不间断学习（Continual Learning，CL） paradigm可能是一个有效的解决方案，以允许逐步训练，即使在多个站点之间。实际上，CL的基本假设是，一旦模型已经训练过，那么过去的数据不能在后续训练过程中再次使用，并且可以被删除。因此，在这篇论文中，我们 investigate CL方法在这种enario下的表现，通过模拟一个可以在新数据批量可用时更新的学习模型。实验结果表明，一种特定的CL方法，即不忘学习（Learning without Forgetting，LwF）是最佳性能的算法之一。然后，我们进一步调查其在形态攻击检测和对象分类任务中的使用和参数化情况，具体是关于可用新训练数据的量。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Inpainting-and-Editing-with-GAN-Inversion"><a href="#Diverse-Inpainting-and-Editing-with-GAN-Inversion" class="headerlink" title="Diverse Inpainting and Editing with GAN Inversion"></a>Diverse Inpainting and Editing with GAN Inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15033">http://arxiv.org/abs/2307.15033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmet Burak Yildirim, Hamza Pehlivan, Bahri Batuhan Bilecen, Aysegul Dundar</li>
<li>for: 实现从 StyleGAN 的潜在空间还原被删除的图像，并在这些图像上进行许多编辑。</li>
<li>methods: 我们提出了一个将删除图像的潜在代码与 StyleGAN 的映射特征结合的混合网络，以及一个使用生成的数据进行训练的新设计。</li>
<li>results: 我们的方法与现有的逆向和填充方法进行比较，实现了较好的质量和多样性。<details>
<summary>Abstract</summary>
Recent inversion methods have shown that real images can be inverted into StyleGAN's latent space and numerous edits can be achieved on those images thanks to the semantically rich feature representations of well-trained GAN models. However, extensive research has also shown that image inversion is challenging due to the trade-off between high-fidelity reconstruction and editability. In this paper, we tackle an even more difficult task, inverting erased images into GAN's latent space for realistic inpaintings and editings. Furthermore, by augmenting inverted latent codes with different latent samples, we achieve diverse inpaintings. Specifically, we propose to learn an encoder and mixing network to combine encoded features from erased images with StyleGAN's mapped features from random samples. To encourage the mixing network to utilize both inputs, we train the networks with generated data via a novel set-up. We also utilize higher-rate features to prevent color inconsistencies between the inpainted and unerased parts. We run extensive experiments and compare our method with state-of-the-art inversion and inpainting methods. Qualitative metrics and visual comparisons show significant improvements.
</details>
<details>
<summary>摘要</summary>
现在的倒转方法已经证明了真实图像可以被StyleGAN的含义空间内转换，并且可以通过训练过的GAN模型中的含义强的特征表示来实现多个编辑。然而，广泛的研究也表明，图像倒转是一项复杂的任务，因为存在高精度重建和编辑之间的负担。在这篇论文中，我们面临更加困难的任务：将抹消图像转换到GAN模型的含义空间中，以获得真实的填充和编辑。此外，我们还利用不同的含义样本来扩展 reverted 的含义代码，以实现多样化的填充。具体来说，我们提议使用编码器和混合网络将抹消图像的编码特征与StyleGAN的映射特征混合在一起。为了让混合网络使用两个输入，我们在网络训练时使用生成的数据进行新的设置。我们还利用更高的比率特征，以避免在填充和未抹消部分之间的颜色不一致。我们进行了广泛的实验，并与当前的倒转和填充方法进行比较。质量指标和视觉比较表明存在显著的改进。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Segmentation-Network-for-Scene-Text-Detection"><a href="#Adaptive-Segmentation-Network-for-Scene-Text-Detection" class="headerlink" title="Adaptive Segmentation Network for Scene Text Detection"></a>Adaptive Segmentation Network for Scene Text Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15029">http://arxiv.org/abs/2307.15029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guiqin Zhao</li>
<li>for: 提高场景文本检测器的性能，解决手动调整参数的繁琐问题，并且能够处理文本实例的极大比例和方向。</li>
<li>methods: 提出自适应分 segmentation 阈值学习方法，自动分辨文本像素和背景像素，并且使用全球信息增强特征层网络（GE-FPN）捕捉文本实例的macro大小和极大比例。其后，引入层次优化结构进一步精细化文本实例。</li>
<li>results: 通过提出的阈值学习策略和文本检测结构，实现了场景文本检测器的state-of-the-art性能，并且通过缺失实验证明了我们的贡献的有效性。<details>
<summary>Abstract</summary>
Inspired by deep convolution segmentation algorithms, scene text detectors break the performance ceiling of datasets steadily. However, these methods often encounter threshold selection bottlenecks and have poor performance on text instances with extreme aspect ratios. In this paper, we propose to automatically learn the discriminate segmentation threshold, which distinguishes text pixels from background pixels for segmentation-based scene text detectors and then further reduces the time-consuming manual parameter adjustment. Besides, we design a Global-information Enhanced Feature Pyramid Network (GE-FPN) for capturing text instances with macro size and extreme aspect ratios. Following the GE-FPN, we introduce a cascade optimization structure to further refine the text instances. Finally, together with the proposed threshold learning strategy and text detection structure, we design an Adaptive Segmentation Network (ASNet) for scene text detection. Extensive experiments are carried out to demonstrate that the proposed ASNet can achieve the state-of-the-art performance on four text detection benchmarks, i.e., ICDAR 2015, MSRA-TD500, ICDAR 2017 MLT and CTW1500. The ablation experiments also verify the effectiveness of our contributions.
</details>
<details>
<summary>摘要</summary>
深受深度卷积分割算法的启发，场景文本检测器不断突破数据集的性能峰值。然而，这些方法经常遇到选择阈值的瓶颈和文本实例的极大比例问题。在这篇论文中，我们提议自动学习分割 segmentation 阈值，以分别文本像素和背景像素，从而降低时间消耗的手动参数调整。此外，我们设计了全球信息增强特征层网络（GE-FPN），用于捕捉文本实例的大小和极大比例。接着，我们引入了层次优化结构，以进一步细化文本实例。最后，我们结合提出的阈值学习策略和文本检测结构，设计了适应性网络（ASNet） для场景文本检测。广泛的实验表明，我们的ASNet可以在四个文本检测标准准则上达到领先性状态，即ICDAR 2015、MSRA-TD500、ICDAR 2017 MLT 和 CTW1500。另外，我们的拓展实验也证明了我们的贡献的有效性。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Graph-Transformer-for-Deepfake-Detection"><a href="#Self-Supervised-Graph-Transformer-for-Deepfake-Detection" class="headerlink" title="Self-Supervised Graph Transformer for Deepfake Detection"></a>Self-Supervised Graph Transformer for Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15019">http://arxiv.org/abs/2307.15019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aminollah Khormali, Jiann-Shiun Yuan</li>
<li>For: 本研究提出了一种深伪检测框架，用于检测视频中的深伪。* Methods: 该框架包括一个基于视觉转换器架构的特征提取器，一个基于Transformer推理器的图 convolutional网络，以及一个图Transformer相关图。* Results: 研究人员通过进行多种难题的实验，包括在不同数据集上进行测试、跨数据集检测、跨操作检测和对常见后期处理抖散的检测，得到了该框架的优秀效果，超过了当前状态艺术方法。<details>
<summary>Abstract</summary>
Deepfake detection methods have shown promising results in recognizing forgeries within a given dataset, where training and testing take place on the in-distribution dataset. However, their performance deteriorates significantly when presented with unseen samples. As a result, a reliable deepfake detection system must remain impartial to forgery types, appearance, and quality for guaranteed generalizable detection performance. Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur. Hence, this study introduces a deepfake detection framework, leveraging a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability. The framework comprises three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning methodology, a graph convolution network coupled with a Transformer discriminator, and a graph Transformer relevancy map that provides a better understanding of manipulated regions and further explains the model's decision. To assess the effectiveness of the proposed framework, several challenging experiments are conducted, including in-data distribution performance, cross-dataset, cross-manipulation generalization, and robustness against common post-production perturbations. The results achieved demonstrate the remarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
深度伪造检测方法已经在给定的数据集上显示出了可靠的结果，但是在见到过去的数据集上进行测试时，其表现会受到很大的损害。因此，一个可靠的深度伪造检测系统必须保持不偏向于伪造类型、外观和质量，以确保普遍可靠的检测性能。尽管有许多尝试来提高跨数据集通用性，这个问题仍然是挑战，特别是在面对常见的后期处理扰动时。因此，本研究将提出一个深度伪造检测框架，利用一个自我超vised的预训练模型，具有出色的通用能力，抵抗常见的扰动和提供功能解释。这个框架包括三个关键 комponent：一个基于视觉对应架构的Feature Extractor，一个与对应架构的Transformer Discriminator，以及一个基于对应架构的Graph Transformer Relevancy Map。为了评估提案的效果，本研究将进行许多挑战性的实验，包括在原始数据集上的性能、跨数据集通用性、跨修改通用性和对常见后期处理扰动的Robustness。实验结果显示，提案的深度伪造检测框架具有卓越的效果，超过现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Verifiable-Feature-Attributions-A-Bridge-between-Post-Hoc-Explainability-and-Inherent-Interpretability"><a href="#Verifiable-Feature-Attributions-A-Bridge-between-Post-Hoc-Explainability-and-Inherent-Interpretability" class="headerlink" title="Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability"></a>Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15007">http://arxiv.org/abs/2307.15007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Usha Bhalla, Suraj Srinivas, Himabindu Lakkaraju</li>
<li>for: 这 paper 的目的是解释机器学习模型的行为，提高模型的可解释性。</li>
<li>methods: 这 paper 使用了两种主要的解释策略：post hoc 解释和嵌入式可解释模型。post hoc 解释方法可以解释复杂的黑盒模型的行为，但是这些解释可能不准确，而且无法验证。嵌入式可解释模型则可以自动编码解释到模型结构中，解释是自然的、可靠的和验证的，但是它们通常具有较强的表达能力。这 paper 提出了一种方法，即 Verifiability Tuning (VerT)，可以将黑盒模型转化成可靠、可验证的解释模型。</li>
<li>results: 这 paper 的实验结果表明，VerT 可以将黑盒模型转化成可靠、可验证的解释模型，并且这些解释模型可以correctly 和可靠地解释模型的行为。同时，VerT 还可以保持黑盒模型的预测性能。<details>
<summary>Abstract</summary>
With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them. Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we aim to bridge the gap between the aforementioned strategies by proposing Verifiability Tuning (VerT), a method that transforms black-box models into models that naturally yield faithful and verifiable feature attributions. We begin by introducing a formal theoretical framework to understand verifiability and show that attributions produced by standard models cannot be verified. We then leverage this framework to propose a method to build verifiable models and feature attributions out of fully trained black-box models. Finally, we perform extensive experiments on semi-synthetic and real-world datasets, and show that VerT produces models that (1) yield explanations that are correct and verifiable and (2) are faithful to the original black-box models they are meant to explain.
</details>
<details>
<summary>摘要</summary>
随着机器学习模型在各种实际应用中的广泛部署，研究人员和实践者们一样强调了模型行为的解释的必要性。为此，先前的文献中提出了两种广泛的解释策略：后期解释方法通过强调模型预测中关键的特征来解释复杂黑盒模型的行为，但是先前的研究表明这些解释可能不准确，甚至更加担忧的是无法确认这些贡献的正确性。而内置可解释模型则通过显式地编码解释到模型建构中，因此其解释自然地准确和可靠，但它们通常具有有限的表达能力，导致预测性能不佳。在这个工作中，我们希望bridge这两种策略的差异，提出一种名为Verifiability Tuning（VerT）的方法，可以将黑盒模型转化为可以自然地生成准确和可靠的特征贡献的模型。我们首先引入了一个正式的理论框架，以理解可靠性的概念，并证明标准模型生成的贡献无法被验证。然后，我们利用这个框架，提出一种建立可靠模型和特征贡献的方法，并在 semi-synthetic 和实际数据集上进行了广泛的实验，结果表明VerT可以生成准确和可靠的特征贡献，同时保持和原始黑盒模型相同的预测性能。
</details></li>
</ul>
<hr>
<h2 id="MapNeRF-Incorporating-Map-Priors-into-Neural-Radiance-Fields-for-Driving-View-Simulation"><a href="#MapNeRF-Incorporating-Map-Priors-into-Neural-Radiance-Fields-for-Driving-View-Simulation" class="headerlink" title="MapNeRF: Incorporating Map Priors into Neural Radiance Fields for Driving View Simulation"></a>MapNeRF: Incorporating Map Priors into Neural Radiance Fields for Driving View Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14981">http://arxiv.org/abs/2307.14981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenming Wu, Jiadai Sun, Zhelun Shen, Liangjun Zhang</li>
<li>for: 用于自动驾驶测试中测试摄像头感知器。</li>
<li>methods: 利用map假设来将神经辐射场与不确定摄像头位置整合，以确保多视角一致性。</li>
<li>results: 实验结果显示，我们的方法可以在偏离路径上维持 semantic consistency。详细视频可以在<a target="_blank" rel="noopener" href="https://youtu.be/jEQWr-Rfh3A%E4%B8%AD%E6%A3%80%E8%A7%86%E3%80%82">https://youtu.be/jEQWr-Rfh3A中检视。</a><details>
<summary>Abstract</summary>
Simulating camera sensors is a crucial task in autonomous driving. Although neural radiance fields are exceptional at synthesizing photorealistic views in driving simulations, they still fail to generate extrapolated views. This paper proposes to incorporate map priors into neural radiance fields to synthesize out-of-trajectory driving views with semantic road consistency. The key insight is that map information can be utilized as a prior to guiding the training of the radiance fields with uncertainty. Specifically, we utilize the coarse ground surface as uncertain information to supervise the density field and warp depth with uncertainty from unknown camera poses to ensure multi-view consistency. Experimental results demonstrate that our approach can produce semantic consistency in deviated views for vehicle camera simulation. The supplementary video can be viewed at https://youtu.be/jEQWr-Rfh3A.
</details>
<details>
<summary>摘要</summary>
simulate 摄像头传感器是自动驾驶中非常重要的任务。尽管神经辐射场是在驾驶 simulations 中生成 photorealistic 视图的非常好的方法，但它们仍然无法生成 extrapolated 视图。这篇论文提议将地图约束 incorporated 到神经辐射场中，以生成 deviated 视图的 semantic 路况一致性。关键的思想是利用地图信息作为训练神经辐射场的先验知识，以确保多视图一致性。特别是，我们利用 unknown 相机 pose 中的不确定信息来监督density field 和折叠深度的学习，以确保多视图一致性。实验结果表明，我们的方法可以在 deviated 视图中保持 semantic 路况一致性。补充视频可以在 https://youtu.be/jEQWr-Rfh3A 上查看。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/28/cs.CV_2023_07_28/" data-id="clpahu72e00h43h889tnl0mub" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/cs.AI_2023_07_28/" class="article-date">
  <time datetime="2023-07-28T12:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/28/cs.AI_2023_07_28/">cs.AI - 2023-07-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Evaluating-the-structure-of-cognitive-tasks-with-transfer-learning"><a href="#Evaluating-the-structure-of-cognitive-tasks-with-transfer-learning" class="headerlink" title="Evaluating the structure of cognitive tasks with transfer learning"></a>Evaluating the structure of cognitive tasks with transfer learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02408">http://arxiv.org/abs/2308.02408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruno Aristimunha, Raphael Y. de Camargo, Walter H. Lopez Pinaya, Sylvain Chevallier, Alexandre Gramfort, Cedric Rommel</li>
<li>for: 这项研究旨在 investigate deep learning representations 的可传递性在不同的EEG解oding任务中。</li>
<li>methods: 研究者使用了现有的decoding模型和两个最新发布的EEG数据集（ERP CORE和M$^3$CV），包含140个人和11种不同的认知任务。他们测试了在一个任务上预训练深度神经网络后，其能够decode到后续任务的能力。</li>
<li>results: 研究结果表明，即使使用线性探测传输，也可以获得显著的提高，与纯粹的超vised方法相比，提高了最多28%。此外，研究者发现了一些解oding方案会释放特定和窄的脑活动，而其他解oding方案则需要预训练在广泛的表征上。这些发现有助于解决EEG解oding中的数据稀缺问题，并且提供了减少数据稀缺的实际应用。同时，生成的传输图也提供了认知任务之间的层次关系的理解，从 neuroscientific 的角度来看。<details>
<summary>Abstract</summary>
Electroencephalography (EEG) decoding is a challenging task due to the limited availability of labelled data. While transfer learning is a promising technique to address this challenge, it assumes that transferable data domains and task are known, which is not the case in this setting. This study investigates the transferability of deep learning representations between different EEG decoding tasks. We conduct extensive experiments using state-of-the-art decoding models on two recently released EEG datasets, ERP CORE and M$^3$CV, containing over 140 subjects and 11 distinct cognitive tasks. We measure the transferability of learned representations by pre-training deep neural networks on one task and assessing their ability to decode subsequent tasks. Our experiments demonstrate that, even with linear probing transfer, significant improvements in decoding performance can be obtained, with gains of up to 28% compare with the pure supervised approach. Additionally, we discover evidence that certain decoding paradigms elicit specific and narrow brain activities, while others benefit from pre-training on a broad range of representations. By revealing which tasks transfer well and demonstrating the benefits of transfer learning for EEG decoding, our findings have practical implications for mitigating data scarcity in this setting. The transfer maps generated also provide insights into the hierarchical relations between cognitive tasks, hence enhancing our understanding of how these tasks are connected from a neuroscientific standpoint.
</details>
<details>
<summary>摘要</summary>
电enzephalography（EEG）解oding是一个具有挑战性的任务，主要因为数据的有限可用性。而转移学习是一种有前途的技术，可以解决这个问题，但它假设了可以确定的数据领域和任务，而这不是这个设置的情况。这个研究探讨了EEG解oding任务之间的转移学习表示的可行性。我们在两个最新发布的EEG数据集，ERP CORE和M$^3$CV中，使用现有的解oding模型进行了广泛的实验。我们测量了转移学习中学习的表示的可行性，通过在一个任务上预训练深度神经网络，然后评估它对后续任务的解码性能的能力。我们的实验结果表明，即使使用线性探索传输，也可以获得显著改善，与纯粹supervised方法相比，改善率可达28%。此外，我们发现了一些解oding方法引起特定和窄的脑活动，而其他方法则受到预训练在广泛的表示上的 beneficial。我们的发现可以有实际意义，帮助解决EEG解oding数据的缺乏问题，同时还可以提供有关认知科学方面的task之间的层次关系的新的视角。
</details></li>
</ul>
<hr>
<h2 id="We-are-all-Individuals-The-Role-of-Robot-Personality-and-Human-Traits-in-Trustworthy-Interaction"><a href="#We-are-all-Individuals-The-Role-of-Robot-Personality-and-Human-Traits-in-Trustworthy-Interaction" class="headerlink" title="We are all Individuals: The Role of Robot Personality and Human Traits in Trustworthy Interaction"></a>We are all Individuals: The Role of Robot Personality and Human Traits in Trustworthy Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15568">http://arxiv.org/abs/2307.15568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mei Yii Lim, José David Aguas Lopes, David A. Robb, Bruce W. Wilson, Meriam Moujahid, Emanuele De Pellegrin, Helen Hastie</li>
<li>for: 这个论文旨在研究 робо类型在人类社会中的表现，以及人类对 robot的偏好和信任度。</li>
<li>methods: 该论文采用了量化和质化的方法，通过 vocal cues 和语言特征来描述 robot 的个性，并通过询问参与者对不同 robot 个性的偏好和信任度来评估 robot 的表现。</li>
<li>results: 研究发现，在 Robo-Barista 中， extrovert robot 被人类参与者更加信任和喜欢，无论参与者自己的人格 trait 如何。此外，研究还发现，人类对 robot 的态度和先天偏好对 human-robot interaction  Study 中的信任度有重要影响。<details>
<summary>Abstract</summary>
As robots take on roles in our society, it is important that their appearance, behaviour and personality are appropriate for the job they are given and are perceived favourably by the people with whom they interact. Here, we provide an extensive quantitative and qualitative study exploring robot personality but, importantly, with respect to individual human traits. Firstly, we show that we can accurately portray personality in a social robot, in terms of extroversion-introversion using vocal cues and linguistic features. Secondly, through garnering preferences and trust ratings for these different robot personalities, we establish that, for a Robo-Barista, an extrovert robot is preferred and trusted more than an introvert robot, regardless of the subject's own personality. Thirdly, we find that individual attitudes and predispositions towards robots do impact trust in the Robo-Baristas, and are therefore important considerations in addition to robot personality, roles and interaction context when designing any human-robot interaction study.
</details>
<details>
<summary>摘要</summary>
As robots take on roles in our society, it is important that their appearance, behavior, and personality are appropriate for the job they are given and are perceived favorably by the people with whom they interact. Here, we provide an extensive quantitative and qualitative study exploring robot personality, but importantly, with respect to individual human traits. Firstly, we show that we can accurately portray personality in a social robot, in terms of extroversion-introversion using vocal cues and linguistic features. Secondly, through garnering preferences and trust ratings for these different robot personalities, we establish that, for a Robo-Barista, an extrovert robot is preferred and trusted more than an introvert robot, regardless of the subject's own personality. Thirdly, we find that individual attitudes and predispositions towards robots do impact trust in the Robo-Baristas, and are therefore important considerations in addition to robot personality, roles, and interaction context when designing any human-robot interaction study.
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Image-Classification-based-on-Gradual-Machine-Learning"><a href="#Few-shot-Image-Classification-based-on-Gradual-Machine-Learning" class="headerlink" title="Few-shot Image Classification based on Gradual Machine Learning"></a>Few-shot Image Classification based on Gradual Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15524">http://arxiv.org/abs/2307.15524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Na Chen, Xianming Kuang, Feiyu Liu, Kehao Wang, Qun Chen</li>
<li>for: 这个论文的目的是提高几个标注样本的图像分类精度。</li>
<li>methods: 这个论文使用非同一个分布（Non-i.i.d）的渐进机器学习（GML）方法，从只有几个标注样本开始，然后逐渐将目标图像标注为增加难度的顺序，通过迭代因子推理在因子图中。</li>
<li>results: 该方法可以提高比较精度（SOTA）性能，在测试集上进行了比较研究，并证明了其在图像分类任务中的优越性。特别是，该方法可以在Query集大小增加时，保持性能的提高，而深度模型的性能则很可能会保持不变或者变差。<details>
<summary>Abstract</summary>
Few-shot image classification aims to accurately classify unlabeled images using only a few labeled samples. The state-of-the-art solutions are built by deep learning, which focuses on designing increasingly complex deep backbones. Unfortunately, the task remains very challenging due to the difficulty of transferring the knowledge learned in training classes to new ones. In this paper, we propose a novel approach based on the non-i.i.d paradigm of gradual machine learning (GML). It begins with only a few labeled observations, and then gradually labels target images in the increasing order of hardness by iterative factor inference in a factor graph. Specifically, our proposed solution extracts indicative feature representations by deep backbones, and then constructs both unary and binary factors based on the extracted features to facilitate gradual learning. The unary factors are constructed based on class center distance in an embedding space, while the binary factors are constructed based on k-nearest neighborhood. We have empirically validated the performance of the proposed approach on benchmark datasets by a comparative study. Our extensive experiments demonstrate that the proposed approach can improve the SOTA performance by 1-5% in terms of accuracy. More notably, it is more robust than the existing deep models in that its performance can consistently improve as the size of query set increases while the performance of deep models remains essentially flat or even becomes worse.
</details>
<details>
<summary>摘要</summary>
《几个样本图像分类》目的是使用只有几个标注样本来高精度地分类无标注图像。现状的解决方案基于深度学习，强调设计越来越复杂的深度背bone。然而，任务仍然非常困难，因为在训练类别之间传递知识的困难。在这篇论文中，我们提出了一种新的方法，基于异步度学习（GML）的异步机器学习（gradual machine learning）的 paradigm。它从只有几个标注样本开始，然后逐渐将目标图像标注为增加难度的顺序，通过迭代因子推理在因子图中。具体来说，我们的提出的方法首先提取特征表示，然后根据提取的特征构建 both unicode 和二进制因子，以便进行慢学习。unicode 因子是根据 embedding 空间中的类中心距离构建的，而二进制因子是根据 k-最近邻居构建的。我们在标准 benchmark 数据集上进行了比较研究，并经验 validate 了我们的方法的性能。我们的广泛的实验表明，我们的方法可以提高 SOTA 性能，并且比现有的深度模型更加稳定。 Specifically, our proposed approach can improve the SOTA performance by 1-5% in terms of accuracy, and it is more robust than the existing deep models in that its performance can consistently improve as the size of query set increases while the performance of deep models remains essentially flat or even becomes worse.Note: "SOTA" stands for "State of the Art", which means the current best performance in a particular field or task.
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Fully-Convolutional-Geometric-Features-for-Object-6D-Pose-Estimation"><a href="#Revisiting-Fully-Convolutional-Geometric-Features-for-Object-6D-Pose-Estimation" class="headerlink" title="Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation"></a>Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15514">http://arxiv.org/abs/2307.15514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaime Corsetti, Davide Boscaini, Fabio Poiesi</li>
<li>for: 6D object pose estimation</li>
<li>methods: Fully Convolutional Geometric Features (FCGF) with sparse convolutions and hardest contrastive loss, and key modifications to the loss and input data representations, as well as careful tuning of training strategies and data augmentations</li>
<li>results: state-of-the-art performance on popular benchmarks, with outperformance of recent competitors<details>
<summary>Abstract</summary>
Recent works on 6D object pose estimation focus on learning keypoint correspondences between images and object models, and then determine the object pose through RANSAC-based algorithms or by directly regressing the pose with end-to-end optimisations. We argue that learning point-level discriminative features is overlooked in the literature. To this end, we revisit Fully Convolutional Geometric Features (FCGF) and tailor it for object 6D pose estimation to achieve state-of-the-art performance. FCGF employs sparse convolutions and learns point-level features using a fully-convolutional network by optimising a hardest contrastive loss. We can outperform recent competitors on popular benchmarks by adopting key modifications to the loss and to the input data representations, by carefully tuning the training strategies, and by employing data augmentations suitable for the underlying problem. We carry out a thorough ablation to study the contribution of each modification.
</details>
<details>
<summary>摘要</summary>
最近的6D物体 pose 估计研究关注学习图像和物体模型之间关键点匹配，然后使用RANSAC算法或直接使用整体优化来确定物体pose。我们认为在文献中学习点级特征是被忽略的。为此，我们回顾了全面卷积Geometric Features（FCGF），并对其进行修改以适应物体6D pose估计，以达到领先的性能。FCGF使用稀疏卷积，通过全面卷积网络学习点级特征，并通过最难的对比损失来优化。我们可以通过采用修改loss和输入数据表示，精心调整训练策略，以及适合下面问题的数据增强来超越最近的竞争对手。我们进行了严格的拟合来研究每个修改的贡献。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Format-Consistency-for-Instruction-Tuning"><a href="#Exploring-Format-Consistency-for-Instruction-Tuning" class="headerlink" title="Exploring Format Consistency for Instruction Tuning"></a>Exploring Format Consistency for Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15504">http://arxiv.org/abs/2307.15504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shihao Liang, Kunlun Zhu, Runchu Tian, Yujia Qin, Huadong Wang, Xin Cong, Zhiyuan Liu, Xiaojiang Liu, Maosong Sun</li>
<li>for: 提高大语言模型 seguir las instrucciones de los humanos</li>
<li>methods: 使用 OpenAI APIs 自动转换format instruction tuning 数据集，并提出一种基于抽象搅拌的干扰除法以降低自动转换中的噪音</li>
<li>results: 研究表明，UIT 框架可以提高 instruction tuning 中的泛化性能，并且在实际应用中可以降低成本Here is the full text in Simplified Chinese, with the three key points highlighted:</li>
<li>for: 本文旨在提高大语言模型 seguir las instrucciones de los humanos，具体来说是通过增加不同 instrucciones 和 instrucciones 集合的训练数据来提高模型的泛化性能。</li>
<li>methods: 我们提出了一种基于 OpenAI APIs 的自动转换format instruction tuning 数据集的框架，并提出了一种基于抽象搅拌的干扰除法以降低自动转换中的噪音。</li>
<li>results: 我们的研究表明，UIT 框架可以提高 instruction tuning 中的泛化性能，并且在实际应用中可以降低成本。<details>
<summary>Abstract</summary>
Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, we study how format inconsistency may impact the performance of instruction tuning. We propose a framework called "Unified Instruction Tuning" (UIT), which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets. We show that UIT successfully improves the generalization performance on unseen instructions, which highlights the importance of format consistency for instruction tuning. To make the UIT framework more practical, we further propose a novel perplexity-based denoising method to reduce the noise of automatic format transfer. We also train a smaller offline model that achieves comparable format transfer capability than OpenAI APIs to reduce costs in practice.
</details>
<details>
<summary>摘要</summary>
<SYS> translate_text="Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, we study how format inconsistency may impact the performance of instruction tuning. We propose a framework called "Unified Instruction Tuning" (UIT), which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets. We show that UIT successfully improves the generalization performance on unseen instructions, which highlights the importance of format consistency for instruction tuning. To make the UIT framework more practical, we further propose a novel perplexity-based denoising method to reduce the noise of automatic format transfer. We also train a smaller offline model that achieves comparable format transfer capability than OpenAI APIs to reduce costs in practice."</SYS>Here's the translation in Simplified Chinese: instrucion 调整有 emerged 为大语言模型遵循人类指令的一种有前途的方法。增加多样性和数量的指令在训练数据中可以一直提高总体性能，这有助于最近的努力，收集各种指令并将现有的指令调整数据集合入大型集合。然而，不同的用户有各自的指令表达方式，而指令集合中的指令风格和格式经常存在差异，即格式不一致。在这种情况下，我们研究了格式不一致如何影响指令调整的性能。我们提出了一个名为 "统一指令调整"（UIT）的框架，通过OpenAI API进行自动格式传输。我们发现，UIT可以成功地提高未见指令的总体性能，这说明了格式一致性对指令调整的重要性。为了使UIT框架更实用，我们进一步提出了一种基于抽象率的减噪方法，以减少自动格式传输中的噪声。此外，我们还训练了一个较小的离线模型，可以实现与OpenAI API相同的格式传输能力，以降低在实践中的成本。
</details></li>
</ul>
<hr>
<h2 id="Curiosity-Driven-Reinforcement-Learning-based-Low-Level-Flight-Control"><a href="#Curiosity-Driven-Reinforcement-Learning-based-Low-Level-Flight-Control" class="headerlink" title="Curiosity-Driven Reinforcement Learning based Low-Level Flight Control"></a>Curiosity-Driven Reinforcement Learning based Low-Level Flight Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15724">http://arxiv.org/abs/2307.15724</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a-ramezani/cdrl-l2fc_u_hcm">https://github.com/a-ramezani/cdrl-l2fc_u_hcm</a></li>
<li>paper_authors: Amir Ramezani Dooraki, Alexandros Iosifidis</li>
<li>for: 这个论文的目的是提出一种基于好奇性的自主学习算法，用于控制quadcopter navigating through obstacles。</li>
<li>methods: 该算法使用了好奇性的prediction errorapproach，并与基于奖励学习的算法结合使用。</li>
<li>results: 测试结果显示，该算法可以学习优化策略，最大化奖励，其他算法无法达成的目标。<details>
<summary>Abstract</summary>
Curiosity is one of the main motives in many of the natural creatures with measurable levels of intelligence for exploration and, as a result, more efficient learning. It makes it possible for humans and many animals to explore efficiently by searching for being in states that make them surprised with the goal of learning more about what they do not know. As a result, while being curious, they learn better. In the machine learning literature, curiosity is mostly combined with reinforcement learning-based algorithms as an intrinsic reward. This work proposes an algorithm based on the drive of curiosity for autonomous learning to control by generating proper motor speeds from odometry data. The quadcopter controlled by our proposed algorithm can pass through obstacles while controlling the Yaw direction of the quad-copter toward the desired location. To achieve that, we also propose a new curiosity approach based on prediction error. We ran tests using on-policy, off-policy, on-policy plus curiosity, and the proposed algorithm and visualized the effect of curiosity in evolving exploration patterns. Results show the capability of the proposed algorithm to learn optimal policy and maximize reward where other algorithms fail to do so.
</details>
<details>
<summary>摘要</summary>
寻Curiosity是许多自然 creature 的主要动机，包括许多智能生物，以探索和学习为主要目的。它使得人类和许多动物能够效率地探索，并且在过程中获得更多的知识。在机器学习文献中，Curiosity通常与征募学习算法结合，作为自然选择的一种内生奖励。本工作提出一个基于寻Curiosity的自主学习控制算法，可以将四辐游戏机器人控制到避免障碍而飞行，并且控制机器人的转向方向以 дости其目标位置。为了实现这一目标，我们还提出了一种新的寻Curiosity方法，基于预测误差。我们在实验中使用了在政策、不在政策、在政策加上寻Curiosity、以及我们的提案中进行试验，并将寻Curiosity的影响在演化探索模式中 visualized。结果显示了我们的提案算法可以学习并实现最佳策略，其他算法无法实现。
</details></li>
</ul>
<hr>
<h2 id="ETHER-Aligning-Emergent-Communication-for-Hindsight-Experience-Replay"><a href="#ETHER-Aligning-Emergent-Communication-for-Hindsight-Experience-Replay" class="headerlink" title="ETHER: Aligning Emergent Communication for Hindsight Experience Replay"></a>ETHER: Aligning Emergent Communication for Hindsight Experience Replay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15494">http://arxiv.org/abs/2307.15494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Denamganaï, Daniel Hernandez, Ozan Vardal, Sondess Missaoui, James Alfred Walker</li>
<li>for: 本研究旨在提高自然语言指令驱动的人工智能机器人的合作能力。</li>
<li>methods: 本研究使用自然语言conditioned reinforcement learning（RL）agent，利用自然语言的特性，如 компози�，提供强 inductive bias 来学习复杂的策略。</li>
<li>results: 研究表明，通过使用 referential game 作为 auxiliary task，可以使RL Agent 更好地利用语言信息，并且可以在不具备 oracle  predicate function 的情况下提高性能和数据效率。<details>
<summary>Abstract</summary>
Natural language instruction following is paramount to enable collaboration between artificial agents and human beings. Natural language-conditioned reinforcement learning (RL) agents have shown how natural languages' properties, such as compositionality, can provide a strong inductive bias to learn complex policies. Previous architectures like HIGhER combine the benefit of language-conditioning with Hindsight Experience Replay (HER) to deal with sparse rewards environments. Yet, like HER, HIGhER relies on an oracle predicate function to provide a feedback signal highlighting which linguistic description is valid for which state. This reliance on an oracle limits its application. Additionally, HIGhER only leverages the linguistic information contained in successful RL trajectories, thus hurting its final performance and data-efficiency. Without early successful trajectories, HIGhER is no better than DQN upon which it is built. In this paper, we propose the Emergent Textual Hindsight Experience Replay (ETHER) agent, which builds on HIGhER and addresses both of its limitations by means of (i) a discriminative visual referential game, commonly studied in the subfield of Emergent Communication (EC), used here as an unsupervised auxiliary task and (ii) a semantic grounding scheme to align the emergent language with the natural language of the instruction-following benchmark. We show that the referential game's agents make an artificial language emerge that is aligned with the natural-like language used to describe goals in the BabyAI benchmark and that it is expressive enough so as to also describe unsuccessful RL trajectories and thus provide feedback to the RL agent to leverage the linguistic, structured information contained in all trajectories. Our work shows that EC is a viable unsupervised auxiliary task for RL and provides missing pieces to make HER more widely applicable.
</details>
<details>
<summary>摘要</summary>
自然语言指导following是人工智能和人类合作的关键。自然语言conditioned reinforcement learning（RL）代理人们已经证明了自然语言的特性，如compositionality，可以为学习复杂政策提供强大的逻辑导向。先前的架构如HIGhER将语言conditioning与Hindsight Experience Replay（HER）结合以处理罕见奖励环境。然而，如HER，HIGhER依赖oracle predicate函数提供一个反馈信号，用于指示哪些语言描述是哪个状态的有效描述。这种依赖oracle限制了其应用。此外，HIGhER只利用RL trajectory中的语言信息，因此在最终性和数据效率方面受到限制。在absence of early successful trajectories，HIGhER与DQN相比，没有优势。在这篇论文中，我们提出了Emergent Textual Hindsight Experience Replay（ETHER）代理人，它基于HIGhER并解决了它的两个限制。我们使用了一种推理视觉游戏，通常在Emergent Communication（EC）中被研究，作为一种无监督任务。此外，我们还使用了一种semantic grounding scheme来将自然语言与RL benchmark中的目标描述相对应。我们发现，EC中的代理人在学习一种与自然语言相关的人工语言，并且这种语言足够表达，以便描述失败的RL trajectory，并提供给RL代理人以利用语言、结构化信息来改进性能。我们的工作表明，EC是一种可靠的无监督任务，可以为RL提供 missing pieces，使HER更加广泛应用。
</details></li>
</ul>
<hr>
<h2 id="A-Semantic-Approach-to-Decidability-in-Epistemic-Planning-Extended-Version"><a href="#A-Semantic-Approach-to-Decidability-in-Epistemic-Planning-Extended-Version" class="headerlink" title="A Semantic Approach to Decidability in Epistemic Planning (Extended Version)"></a>A Semantic Approach to Decidability in Epistemic Planning (Extended Version)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15485">http://arxiv.org/abs/2307.15485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Burigana, Paolo Felli, Marco Montali, Nicolas Troquard</li>
<li>for: 这篇论文主要探讨了在多智能计划中使用动态纽标逻辑（DEL）的可 decidability问题。</li>
<li>methods: 作者采用了一种新的semantic方法来实现可 decidability，而不是通过语法上的限制。具体来说，作者增强了知识逻辑S5$_n$的axioms，并添加了一个交互axioms（知识共同性），以控制代理人对别人知识的无限推理能力。</li>
<li>results: 作者首先证明了这种epistemic planning问题是可 decidable的。此外，作者还研究了不同的通常性axioms的推广，以实现更expressive的DEL Fragment的可 decidability。最后，作者证明了两个常见的epistemic planning系统，基于action templates，在知识下的设定下是可 decidable的。<details>
<summary>Abstract</summary>
The use of Dynamic Epistemic Logic (DEL) in multi-agent planning has led to a widely adopted action formalism that can handle nondeterminism, partial observability and arbitrary knowledge nesting. As such expressive power comes at the cost of undecidability, several decidable fragments have been isolated, mainly based on syntactic restrictions of the action formalism. In this paper, we pursue a novel semantic approach to achieve decidability. Namely, rather than imposing syntactical constraints, the semantic approach focuses on the axioms of the logic for epistemic planning. Specifically, we augment the logic of knowledge S5$_n$ and with an interaction axiom called (knowledge) commutativity, which controls the ability of agents to unboundedly reason on the knowledge of other agents. We then provide a threefold contribution. First, we show that the resulting epistemic planning problem is decidable. In doing so, we prove that our framework admits a finitary non-fixpoint characterization of common knowledge, which is of independent interest. Second, we study different generalizations of the commutativity axiom, with the goal of obtaining decidability for more expressive fragments of DEL. Finally, we show that two well-known epistemic planning systems based on action templates, when interpreted under the setting of knowledge, conform to the commutativity axiom, hence proving their decidability.
</details>
<details>
<summary>摘要</summary>
使用动态эпистемологи（DEL）在多代理规划中得到了广泛采用的行动 formalism，可以处理不确定性、部分可见性和嵌套知识。然而，这么高度表达力带来了不可解决性问题，因此有很多可 decidable fragments 已经被隔离出来，主要基于动态 formalism 的语法约束。在这篇论文中，我们采用一种新的semantic方法来实现可解决性。具体来说，我们在知识逻辑S5$_n$中添加了一个交互axioms（知识 commutativity），该axioms控制代理者对别人知识的无限推理能力。然后，我们提供了三项贡献：1. 我们表明了这种epistemic planning问题的可解决性。在这个过程中，我们证明了我们的框架有一个 finitary non-fixpoint characterization of common knowledge，这是独立的有趣的。2. 我们研究了不同的generalisations of the commutativity axiom，以实现更expressive fragments of DEL的可解决性。3. 我们证明了两个常见的epistemic planning系统，基于action templates，在知识下被解释，符合 commutativity axiom，因此其可解决性。
</details></li>
</ul>
<hr>
<h2 id="Minimally-Supervised-Speech-Synthesis-with-Conditional-Diffusion-Model-and-Language-Model-A-Comparative-Study-of-Semantic-Coding"><a href="#Minimally-Supervised-Speech-Synthesis-with-Conditional-Diffusion-Model-and-Language-Model-A-Comparative-Study-of-Semantic-Coding" class="headerlink" title="Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding"></a>Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15484">http://arxiv.org/abs/2307.15484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyu Qiang, Hao Li, Hao Ni, He Qu, Ruibo Fu, Tao Wang, Longbiao Wang, Jianwu Dang</li>
<li>for: 这个论文旨在提出一种基于扩散模型和语言模型的文本译语音系统，以提高文本译语音的质量和自然性。</li>
<li>methods: 该论文使用了两种不同类型的扩散Speech表示，并使用两个序列到序列任务来解耦文本译语音。它还引入了一个提示编码结构，以提高提示表示能力。</li>
<li>results: 实验结果显示，该论文提出的方法比基eline方法表现出色，并提供了一个网站的音频样本。<details>
<summary>Abstract</summary>
Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. To address the challenges associated with high dimensionality and waveform distortion in discrete representations, we propose Diff-LM-Speech, which models semantic embeddings into mel-spectrogram based on diffusion models and introduces a prompt encoder structure based on variational autoencoders and prosody bottlenecks to improve prompt representation capabilities. Autoregressive language models often suffer from missing and repeated words, while non-autoregressive frameworks face expression averaging problems due to duration prediction models. To address these issues, we propose Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse prosodic expressions. While we expect the information content of semantic coding to be between that of text and acoustic coding, existing models extract semantic coding with a lot of redundant information and dimensionality explosion. To verify that semantic coding is not necessary, we propose Tri-Diff-Speech. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples.
</details>
<details>
<summary>摘要</summary>
最近，有越来越多关注可以通过最小监督学习的文本识别（TTS）方法。我们提议一种叫做Diff-LM-Speech的方法，它利用扩散模型将含义编码作为mel-spectrogram中的semantic embedding，并通过变量自动编码器和谱瓣瓶逻辑来改善提示表示能力。而autoregressive语言模型经常会出现缺失和重复的单词问题，而非autoregressive框架则会面临表达均衡问题，这是因为duration预测模型的问题。为解决这些问题，我们提议Tetra-Diff-Speech，它使用扩散模型来实现多种表达方式的多样性。尽管我们预期含义编码的信息内容在文本和音频编码之间，现有的模型通常会提取很多无用的信息和维度爆炸。为验证这一点，我们提议Tri-Diff-Speech。实验结果表明，我们的提议方法在比基eline方法有更好的表现。我们提供了一个网站，包含了各种音频样本。
</details></li>
</ul>
<hr>
<h2 id="Non-invasive-Diabetes-Detection-using-Gabor-Filter-A-Comparative-Analysis-of-Different-Cameras"><a href="#Non-invasive-Diabetes-Detection-using-Gabor-Filter-A-Comparative-Analysis-of-Different-Cameras" class="headerlink" title="Non-invasive Diabetes Detection using Gabor Filter: A Comparative Analysis of Different Cameras"></a>Non-invasive Diabetes Detection using Gabor Filter: A Comparative Analysis of Different Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15480">http://arxiv.org/abs/2307.15480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christina A. Garcia, Patricia Angela R. Abu, Rosula SJ. Reyes</li>
<li>for: 这个论文旨在比较和探讨使用移动设备摄像头和笔记型电脑摄像头来捕捉非侵入性诊断糖尿病（DM）的图像，并使用facial block texture特征进行识别。</li>
<li>methods: 该论文使用了12mp和7mp移动设备摄像头以及笔记型电脑摄像头，在正常照明下拍摄图像。 extracted facial blocks被分类使用k-最近邻和支持向量机。</li>
<li>results: 系统的性能被测量为准确率96.7%，特异性93%和敏感性100%，最佳性能来自12mp后置摄像头使用支持向量机，使用100张图像。<details>
<summary>Abstract</summary>
This paper compares and explores the performance of both mobile device camera and laptop camera as convenient tool for capturing images for non-invasive detection of Diabetes Mellitus (DM) using facial block texture features. Participants within age bracket 20 to 79 years old were chosen for the dataset. 12mp and 7mp mobile cameras, and a laptop camera were used to take the photo under normal lighting condition. Extracted facial blocks were classified using k-Nearest Neighbors (k-NN) and Support Vector Machine (SVM). 100 images were captured, preprocessed, filtered using Gabor, and iterated. Performance of the system was measured in terms of accuracy, specificity, and sensitivity. Best performance of 96.7% accuracy, 100% sensitivity, and 93% specificity were achieved from 12mp back camera using SVM with 100 images.
</details>
<details>
<summary>摘要</summary>
这篇论文比较了手持设备摄像头和笔记型电脑摄像头作为轻便的捕捉照片用于不侵入性诊断糖尿病（DM）的方法。选择的参与者年龄在20岁至79岁之间。使用1200万像素和700万像素手持设备摄像头以及笔记型电脑摄像头，在正常照明条件下拍摄照片。提取的脸部块被分类使用k-最近邻和支持向量机（SVM）。 captured 100 张照片，预处理、筛选using Gabor, iterated。系统性能测量的指标包括准确率、特异性和敏感度。使用1200万像素后摄像头和SVM， achieved 96.7%的准确率、100%的敏感度和93%的特异性。
</details></li>
</ul>
<hr>
<h2 id="FeedbackLogs-Recording-and-Incorporating-Stakeholder-Feedback-into-Machine-Learning-Pipelines"><a href="#FeedbackLogs-Recording-and-Incorporating-Stakeholder-Feedback-into-Machine-Learning-Pipelines" class="headerlink" title="FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines"></a>FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15475">http://arxiv.org/abs/2307.15475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Barker, Emma Kallina, Dhananjay Ashok, Katherine M. Collins, Ashley Casovan, Adrian Weller, Ameet Talwalkar, Valerie Chen, Umang Bhatt</li>
<li>for: 这个论文是为了提供一种方法来记录和 incorporate 多个潜在参与者的反馈，以便更好地了解 ML 管道的影响。</li>
<li>methods: 这篇论文提出了一种名为 FeedbackLogs 的新方法，用于跟踪 ML 管道中不同参与者的反馈。每个 FeedbackLog 都记录了反馈收集过程中的重要细节，以及反馈本身和如何将反馈纳入 ML 管道中。</li>
<li>results: 这篇论文提供了一些具体的使用案例，例如使用 FeedbackLogs 作为算法审核的证据，以及用于记录基于潜在参与者反馈的更新。<details>
<summary>Abstract</summary>
Even though machine learning (ML) pipelines affect an increasing array of stakeholders, there is little work on how input from stakeholders is recorded and incorporated. We propose FeedbackLogs, addenda to existing documentation of ML pipelines, to track the input of multiple stakeholders. Each log records important details about the feedback collection process, the feedback itself, and how the feedback is used to update the ML pipeline. In this paper, we introduce and formalise a process for collecting a FeedbackLog. We also provide concrete use cases where FeedbackLogs can be employed as evidence for algorithmic auditing and as a tool to record updates based on stakeholder feedback.
</details>
<details>
<summary>摘要</summary>
即使机器学习（ML）管道影响到越来越多的利益者，有很少关于如何记录和 incorporate 利益者的输入的研究。我们提议使用 FeedbackLogs，加入现有的 ML 管道文档，跟踪多个利益者的反馈。每个日志记录了反馈收集过程中重要的细节，反馈本身，以及如何使用反馈更新 ML 管道。在这篇论文中，我们介绍了和形式化了收集FeedbackLog的过程。我们还提供了具体的应用场景，其中FeedbackLog可以作为算法审核的证据，以及用于记录基于利益者反馈的更新。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Noisy-Label-Learning-in-Real-world-Annotation-Scenarios-from-the-Noise-type-Perspective"><a href="#Rethinking-Noisy-Label-Learning-in-Real-world-Annotation-Scenarios-from-the-Noise-type-Perspective" class="headerlink" title="Rethinking Noisy Label Learning in Real-world Annotation Scenarios from the Noise-type Perspective"></a>Rethinking Noisy Label Learning in Real-world Annotation Scenarios from the Noise-type Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16889">http://arxiv.org/abs/2307.16889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fuxiailab/protosemi">https://github.com/fuxiailab/protosemi</a></li>
<li>paper_authors: Renyu Zhu, Haoyu Liu, Runze Wu, Minmin Lin, Tangjie Lv, Changjie Fan, Haobo Wang</li>
<li>for:  investigate the problem of learning with noisy labels in real-world annotation scenarios</li>
<li>methods: propose a novel sample selection-based approach for noisy label learning called Proto-semi</li>
<li>results: demonstrate the effectiveness of Proto-semi in handling the problem of learning from noisy labels, and show that the prototype-based repartitioning strategy is effective in mitigating the adverse impact of label noise.Here is the summary in Traditional Chinese:</li>
<li>for: 研究实际标签条件下的学习噪音标签问题</li>
<li>methods: 提出一个基于选择体系的噪音标签学习方法 called Proto-semi</li>
<li>results: 验证 Proto-semi 能够实现噪音标签学习问题，并显示基于几何的重新分配策略有效地减少噪音标签的影响。<details>
<summary>Abstract</summary>
In this paper, we investigate the problem of learning with noisy labels in real-world annotation scenarios, where noise can be categorized into two types: factual noise and ambiguity noise. To better distinguish these noise types and utilize their semantics, we propose a novel sample selection-based approach for noisy label learning, called Proto-semi. Proto-semi initially divides all samples into the confident and unconfident datasets via warm-up. By leveraging the confident dataset, prototype vectors are constructed to capture class characteristics. Subsequently, the distances between the unconfident samples and the prototype vectors are calculated to facilitate noise classification. Based on these distances, the labels are either corrected or retained, resulting in the refinement of the confident and unconfident datasets. Finally, we introduce a semi-supervised learning method to enhance training. Empirical evaluations on a real-world annotated dataset substantiate the robustness of Proto-semi in handling the problem of learning from noisy labels. Meanwhile, the prototype-based repartitioning strategy is shown to be effective in mitigating the adverse impact of label noise. Our code and data are available at https://github.com/fuxiAIlab/ProtoSemi.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了在实际注释场景中学习受损标签的问题，其中噪声可以分为两类：事实噪声和模糊噪声。为了更好地 отличи出这两种噪声类型并利用其 semantics，我们提议了一种基于样本选择的受损标签学习方法，称为Proto-semi。Proto-semi首先将所有样本分为自信量高和自信量低两个集合via warm-up。然后，通过利用自信量集合，构建 prototype vectors，以捕捉类征特征。接着，计算不确定样本与 prototype vectors 之间的距离，以便噪声分类。根据这些距离，将标签更正或保留，从而对自信量集合和不确定集合进行修正。最后，我们引入了一种半supervised学习方法，以提高训练。empirical evaluations 表明，Proto-semi 能够有效地处理实际注释中的受损标签学习问题。同时，我们的 prototype-based repartitioning 策略能够减轻噪声对标签学习的负面影响。我们的代码和数据可以在 <https://github.com/fuxiAIlab/ProtoSemi> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Testing-the-Depth-of-ChatGPT’s-Comprehension-via-Cross-Modal-Tasks-Based-on-ASCII-Art-GPT3-5’s-Abilities-in-Regard-to-Recognizing-and-Generating-ASCII-Art-Are-Not-Totally-Lacking"><a href="#Testing-the-Depth-of-ChatGPT’s-Comprehension-via-Cross-Modal-Tasks-Based-on-ASCII-Art-GPT3-5’s-Abilities-in-Regard-to-Recognizing-and-Generating-ASCII-Art-Are-Not-Totally-Lacking" class="headerlink" title="Testing the Depth of ChatGPT’s Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5’s Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking"></a>Testing the Depth of ChatGPT’s Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5’s Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16806">http://arxiv.org/abs/2307.16806</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Bayani</li>
<li>for: 这篇论文探讨了 GPT3.5 模型在视觉任务中的能力，包括图像识别、图像分割和图像生成等。</li>
<li>methods: 该论文使用了 GPT3.5 模型，并对其进行了不同的变换和修改，以测试其在视觉任务中的表现。</li>
<li>results: 研究发现，GPT3.5 模型在图像识别和图像分割任务中表现不佳，但在图像生成任务中表现较为出色。<details>
<summary>Abstract</summary>
Over the eight months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche-industry of papers have emerged examining the scope of capabilities these models possess, the information fed to and extracted from these networks has been either natural language text or stylized, code-like language. Drawing inspiration from the prowess we expect a truly human-level intelligent agent to have across multiple signal modalities, in this work we examine GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary. We conduct experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings, trials investigating knowledge of image parts, and tasks covering image generation.
</details>
<details>
<summary>摘要</summary>
Over the past eight months since its release, ChatGPT and its underlying model, GPT3.5, have received massive attention due to their powerful combination of capabilities and accessibility. While a niche industry of papers has emerged examining the scope of capabilities these models possess, the information fed to and extracted from these networks has been limited to natural language text or stylized, code-like language. Inspired by the versatility we would expect from a truly human-level intelligent agent, in this work we explore GPT3.5's ability to perform visual tasks, using ASCII art as input without any explicit linguistic summaries. We conduct experiments analyzing the model's performance on image recognition tasks, image part recognition, and image generation.
</details></li>
</ul>
<hr>
<h2 id="Worrisome-Properties-of-Neural-Network-Controllers-and-Their-Symbolic-Representations"><a href="#Worrisome-Properties-of-Neural-Network-Controllers-and-Their-Symbolic-Representations" class="headerlink" title="Worrisome Properties of Neural Network Controllers and Their Symbolic Representations"></a>Worrisome Properties of Neural Network Controllers and Their Symbolic Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15456">http://arxiv.org/abs/2307.15456</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mimuw-rl/worrisome-nn">https://github.com/mimuw-rl/worrisome-nn</a></li>
<li>paper_authors: Jacek Cyranka, Kevin E M Church, Jean-Philippe Lessard</li>
<li>for: 本研究探讨控制器在简单强化学习问题中的稳定性问题。</li>
<li>methods: 本研究使用神经网络控制器和其低神经级别和符号抽象。</li>
<li>results: 研究发现， Typical controller 可以达到高均返回值，但仍然生成大量的持续低返回解决方案，这是一个非常不жела的性能，易被敌对者利用。 更加简单的控制器会承认更多的持续坏解决方案。 研究提供了一种系统性 robustness 研究的算法，并证明存在持续解决方案和、在某些情况下， periodic orbits 的存在，使用计算机支持的证明方法。<details>
<summary>Abstract</summary>
We raise concerns about controllers' robustness in simple reinforcement learning benchmark problems. We focus on neural network controllers and their low neuron and symbolic abstractions. A typical controller reaching high mean return values still generates an abundance of persistent low-return solutions, which is a highly undesirable property, easily exploitable by an adversary. We find that the simpler controllers admit more persistent bad solutions. We provide an algorithm for a systematic robustness study and prove existence of persistent solutions and, in some cases, periodic orbits, using a computer-assisted proof methodology.
</details>
<details>
<summary>摘要</summary>
我们有关控制器的Robustness在简单的征务学习问题上表达出关注。我们专注于神经网络控制器的低神经和符号抽象。一般情况下，一个高均返回值的控制器仍然生成丰富的持续性低返回解，这是非常不愿意的危难，易于敌人利用。我们发现简单的控制器承认更多持续性坏解。我们提供了一个系统atic robustness研究的算法，并证明存在持续解和，在一些情况下， periodic orbit，使用了电脑辅助证明方法。
</details></li>
</ul>
<hr>
<h2 id="From-Probabilistic-Programming-to-Complexity-based-Programming"><a href="#From-Probabilistic-Programming-to-Complexity-based-Programming" class="headerlink" title="From Probabilistic Programming to Complexity-based Programming"></a>From Probabilistic Programming to Complexity-based Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15453">http://arxiv.org/abs/2307.15453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Sileno, Jean-Louis Dessalles</li>
<li>for: 本文提出了一种新的计算框架，名为CompLog，受 probabilistic programming 系统ProbLog的启发，基于 simplicity theory 的推理机制，通过计算两个kolmogorov复杂度来代替概率推理。</li>
<li>methods: 本文使用了两个kolmogorov复杂度来计算后期和前期意外程度，即后期和前期主观概率。计算基于世界和心理模型的假设，通过 causa 和descriptive 关系来Weight predicates的复杂度。</li>
<li>results: 本文提供了一些应用示例，包括生成相关描述和提供谱析和否定的不同方法。<details>
<summary>Abstract</summary>
The paper presents the main characteristics and a preliminary implementation of a novel computational framework named CompLog. Inspired by probabilistic programming systems like ProbLog, CompLog builds upon the inferential mechanisms proposed by Simplicity Theory, relying on the computation of two Kolmogorov complexities (here implemented as min-path searches via ASP programs) rather than probabilistic inference. The proposed system enables users to compute ex-post and ex-ante measures of unexpectedness of a certain situation, mapping respectively to posterior and prior subjective probabilities. The computation is based on the specification of world and mental models by means of causal and descriptive relations between predicates weighted by complexity. The paper illustrates a few examples of application: generating relevant descriptions, and providing alternative approaches to disjunction and to negation.
</details>
<details>
<summary>摘要</summary>
文章介绍了一种新的计算框架，名为CompLog，它受到概率编程系统ProbLog的启发，基于 simplicity theory 中的推理机制，通过计算两个可读性复杂度（在 ASP 程序中实现为最短路寻找）而不是概率推理。该系统可以为用户计算出不同情况的预后和预先抽象度，即后 posting 和前 posting Subjective 概率。计算基于世界和心理模型的干扰和描述关系，这些关系由 predicate 的复杂度Weight。文章还给出了一些应用示例，如生成相关的描述和提供了许多不同的补做法。
</details></li>
</ul>
<hr>
<h2 id="DELPHIC-Practical-DEL-Planning-via-Possibilities-Extended-Version"><a href="#DELPHIC-Practical-DEL-Planning-via-Possibilities-Extended-Version" class="headerlink" title="DELPHIC: Practical DEL Planning via Possibilities (Extended Version)"></a>DELPHIC: Practical DEL Planning via Possibilities (Extended Version)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15451">http://arxiv.org/abs/2307.15451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Burigana, Paolo Felli, Marco Montali</li>
<li>for: This paper aims to improve the practicality of Dynamic Epistemic Logic (DEL) planning by questioning the traditional semantics and proposing an alternative, more compact approach called DELPHIC.</li>
<li>methods: The paper uses a new semantics defined using possibilities, which are non-well-founded objects representing both factual properties and what agents consider to be possible. The authors implement the DELPHIC approach in Answer Set Programming (ASP) and compare it with the traditional Kripke-based approach.</li>
<li>results: The experimental evaluation shows that DELPHIC outperforms the traditional approach in terms of space and time.<details>
<summary>Abstract</summary>
Dynamic Epistemic Logic (DEL) provides a framework for epistemic planning that is capable of representing non-deterministic actions, partial observability, higher-order knowledge and both factual and epistemic change. The high expressivity of DEL challenges existing epistemic planners, which typically can handle only restricted fragments of the whole framework. The goal of this work is to push the envelop of practical DEL planning, ultimately aiming for epistemic planners to be able to deal with the full range of features offered by DEL. Towards this goal, we question the traditional semantics of DEL, defined in terms on Kripke models. In particular, we propose an equivalent semantics defined using, as main building block, so-called possibilities: non well-founded objects representing both factual properties of the world, and what agents consider to be possible. We call the resulting framework DELPHIC. We argue that DELPHIC indeed provides a more compact representation of epistemic states. To substantiate this claim, we implement both approaches in ASP and we set up an experimental evaluation to compare DELPHIC with the traditional, Kripke-based approach. The evaluation confirms that DELPHIC outperforms the traditional approach in space and time.
</details>
<details>
<summary>摘要</summary>
dynamically epistemic logic (DEL) 提供了一个架构，可以表示非决定性行为、部分可观察性、高阶知识和事实和知识改变。 DEL 的表达力问题，使得现有的 epistemic 观察者通常只能处理 restriction 的 fragment。 在这个工作中，我们质疑传统 DEL 的 semantics，定义为基于 Kripke 模型。 具体来说，我们提出了一个相等的 semantics，使用 so-called possibilities：非对数世界的性质，以及 agents 认为可能的东西。 我们称这个框架为 DELPHIC。 我们认为 DELPHIC 可以提供更 компакт的 epistemic 状态表示。 为了证明这个主张，我们将实现这两种方法，并设置了一个实验评估，以比较 DELPHIC 和传统、基于 Kripke 的方法。 评估确认 DELPHIC 在空间和时间方面的表现比 traditional 方法更好。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Alignment-of-Temporal-Knowledge-Bases"><a href="#Optimal-Alignment-of-Temporal-Knowledge-Bases" class="headerlink" title="Optimal Alignment of Temporal Knowledge Bases"></a>Optimal Alignment of Temporal Knowledge Bases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15439">http://arxiv.org/abs/2307.15439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Fernandez-Gil, Fabio Patrizi, Giuseppe Perelli, Anni-Yasmin Turhan</li>
<li>for: 本研究旨在实现基于ontology的情境识别，并且解决在知识库中收集的数据不准确导致重要查询答案被遗弃的问题。</li>
<li>methods: 本文引入了TKBAlignment问题，该问题计算一个变体的TKB，以最小改变TKB，但能使得给定的temporal CQ得到答案，并且是这种（成本-)优化的。</li>
<li>results: 本文对ALC TKBs和 conjunctive queries with LTL operators进行研究，并提出了一种解决TKBAlignment问题的方法，该方法基于 propositional LTL over finite traces的对应技术，可以 Compute (cost-optimal) alignments of TKBs。<details>
<summary>Abstract</summary>
Answering temporal CQs over temporalized Description Logic knowledge bases (TKB) is a main technique to realize ontology-based situation recognition. In case the collected data in such a knowledge base is inaccurate, important query answers can be missed. In this paper we introduce the TKB Alignment problem, which computes a variant of the TKB that minimally changes the TKB, but entails the given temporal CQ and is in that sense (cost-)optimal. We investigate this problem for ALC TKBs and conjunctive queries with LTL operators and devise a solution technique to compute (cost-optimal) alignments of TKBs that extends techniques for the alignment problem for propositional LTL over finite traces.
</details>
<details>
<summary>摘要</summary>
Answering temporal CQs over temporalized Description Logic knowledge bases (TKB) is a main technique to realize ontology-based situation recognition. If the collected data in such a knowledge base is inaccurate, important query answers can be missed. In this paper, we introduce the TKB Alignment problem, which computes a variant of the TKB that minimally changes the TKB, but entails the given temporal CQ and is in that sense (cost-)optimal. We investigate this problem for ALC TKBs and conjunctive queries with LTL operators and devise a solution technique to compute (cost-optimal) alignments of TKBs that extends techniques for the alignment problem for propositional LTL over finite traces.Here's the word-for-word translation:回答 temporal CQs over temporalized Description Logic knowledge bases (TKB) 是实现 ontology-based situation recognition 的主要技术。如果 collected data 中的 TKB 不准确，重要的查询答案就可能会丢失。在这篇论文中，我们介绍 TKB Alignment problem，该问题计算一个 TKB 的变体，使其最小地改变 TKB，但涵盖给定的 temporal CQ，并且是Cost-optimal的。我们对 ALC TKBs 和 conjunctive queries with LTL operators 进行调查，并提出一种 compute (cost-optimal) alignments of TKBs 的解决方案，该方案基于 propositional LTL over finite traces 的对应技术。
</details></li>
</ul>
<hr>
<h2 id="Improvable-Gap-Balancing-for-Multi-Task-Learning"><a href="#Improvable-Gap-Balancing-for-Multi-Task-Learning" class="headerlink" title="Improvable Gap Balancing for Multi-Task Learning"></a>Improvable Gap Balancing for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15429">http://arxiv.org/abs/2307.15429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanqidai/igb4mtl">https://github.com/yanqidai/igb4mtl</a></li>
<li>paper_authors: Yanqi Dai, Nanyi Fei, Zhiwu Lu</li>
<li>For: 这篇论文主要关注多任务学习（MTL）中的梯度平衡和损失平衡两种方法，以及它们在不同任务间的对应关系。* Methods: 本篇论文提出了两种新的改进梯度平衡（IGB）算法，其中一种运用了简单的规律，另一种则是通过深度强化学习来实现MTL中的改进梯度平衡。* Results: 实验结果显示，IGB算法在MTL中实现了最佳的结果，并且与梯度平衡结合使用可以获得进一步的改进。<details>
<summary>Abstract</summary>
In multi-task learning (MTL), gradient balancing has recently attracted more research interest than loss balancing since it often leads to better performance. However, loss balancing is much more efficient than gradient balancing, and thus it is still worth further exploration in MTL. Note that prior studies typically ignore that there exist varying improvable gaps across multiple tasks, where the improvable gap per task is defined as the distance between the current training progress and desired final training progress. Therefore, after loss balancing, the performance imbalance still arises in many cases. In this paper, following the loss balancing framework, we propose two novel improvable gap balancing (IGB) algorithms for MTL: one takes a simple heuristic, and the other (for the first time) deploys deep reinforcement learning for MTL. Particularly, instead of directly balancing the losses in MTL, both algorithms choose to dynamically assign task weights for improvable gap balancing. Moreover, we combine IGB and gradient balancing to show the complementarity between the two types of algorithms. Extensive experiments on two benchmark datasets demonstrate that our IGB algorithms lead to the best results in MTL via loss balancing and achieve further improvements when combined with gradient balancing. Code is available at https://github.com/YanqiDai/IGB4MTL.
</details>
<details>
<summary>摘要</summary>
在多任务学习（MTL）中，梯度均衡在最近几年内吸引了更多的研究兴趣，因为它经常会导致更好的性能。然而，损失均衡是梯度均衡的更加有效的方法，因此仍然值得进一步的探索。尽管先前的研究通常忽略了多任务中存在的不同可改善差距，其中每个任务的可改善差距定义为从当前训练进度到期望的最终训练进度之间的距离。因此，在进行损失均衡后，性能差距仍然出现在许多情况下。在本文中，我们采用损失均衡框架，提出了两种新的可改善差距均衡（IGB）算法 для MTL：一个使用简单的启发，另一个（这是第一次）使用深度强化学习。特别是，不直接在 MTL 中平衡损失，而是动态分配任务权重以进行可改善差距均衡。此外，我们将 IGB 和梯度均衡相结合，以示两者之间的补充性。广泛的实验表明，我们的 IGB 算法在 MTL 中通过损失均衡得到最佳结果，并在结合梯度均衡时获得进一步的改进。代码可以在 <https://github.com/YanqiDai/IGB4MTL> 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Review-of-Large-Language-Models-Sensitivity-Bias-and-the-Path-Toward-Specialized-AI"><a href="#A-Critical-Review-of-Large-Language-Models-Sensitivity-Bias-and-the-Path-Toward-Specialized-AI" class="headerlink" title="A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI"></a>A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15425">http://arxiv.org/abs/2307.15425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arash Hajikhani, Carolyn Cole</li>
<li>for: 本研究探讨了一种专门编译的语言模型和一个通用模型如OpenAI的GPT-3.5在文本数据中检测SDGs的比较效果。</li>
<li>methods: 本研究使用了大语言模型（LLMs），探讨了对偏见和敏感性的挑战。研究强调了特殊训练的重要性以实现精确和不偏的分析。</li>
<li>results: 研究发现，专门的SDG检测模型在公司描述 dataset 中比GPT-3.5更加精准地检测SDGs，并且可以快速地提供高度相关的SDGs。研究认为，在执行任务时应选择合适的模型，考虑任务的需求、成本、复杂度和可见性。<details>
<summary>Abstract</summary>
This paper examines the comparative effectiveness of a specialized compiled language model and a general-purpose model like OpenAI's GPT-3.5 in detecting SDGs within text data. It presents a critical review of Large Language Models (LLMs), addressing challenges related to bias and sensitivity. The necessity of specialized training for precise, unbiased analysis is underlined. A case study using a company descriptions dataset offers insight into the differences between the GPT-3.5 and the specialized SDG detection model. While GPT-3.5 boasts broader coverage, it may identify SDGs with limited relevance to the companies' activities. In contrast, the specialized model zeroes in on highly pertinent SDGs. The importance of thoughtful model selection is emphasized, taking into account task requirements, cost, complexity, and transparency. Despite the versatility of LLMs, the use of specialized models is suggested for tasks demanding precision and accuracy. The study concludes by encouraging further research to find a balance between the capabilities of LLMs and the need for domain-specific expertise and interpretability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Social-Media-Popularity-Prediction-with-Multiple-Post-Dependencies"><a href="#Improving-Social-Media-Popularity-Prediction-with-Multiple-Post-Dependencies" class="headerlink" title="Improving Social Media Popularity Prediction with Multiple Post Dependencies"></a>Improving Social Media Popularity Prediction with Multiple Post Dependencies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15413">http://arxiv.org/abs/2307.15413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhizhen Zhang, Xiaohui Xie, Mengyu Yang, Ye Tian, Yong Jiang, Yong Cui</li>
<li>for: 预测社交媒体帖子的 популяр度，以提高推荐系统和多媒体广告等应用的效果。</li>
<li>methods: 提出了一种名为受依关系探测网络（DSN）的新预测框架，利用了帖子之间和帖子内的多个依存关系，以提高预测精度。</li>
<li>results: 对社交媒体帖子Popularity Dataset进行实验，比现有的模型表现更优异。<details>
<summary>Abstract</summary>
Social Media Popularity Prediction has drawn a lot of attention because of its profound impact on many different applications, such as recommendation systems and multimedia advertising. Despite recent efforts to leverage the content of social media posts to improve prediction accuracy, many existing models fail to fully exploit the multiple dependencies between posts, which are important to comprehensively extract content information from posts. To tackle this problem, we propose a novel prediction framework named Dependency-aware Sequence Network (DSN) that exploits both intra- and inter-post dependencies. For intra-post dependency, DSN adopts a multimodal feature extractor with an efficient fine-tuning strategy to obtain task-specific representations from images and textual information of posts. For inter-post dependency, DSN uses a hierarchical information propagation method to learn category representations that could better describe the difference between posts. DSN also exploits recurrent networks with a series of gating layers for more flexible local temporal processing abilities and multi-head attention for long-term dependencies. The experimental results on the Social Media Popularity Dataset demonstrate the superiority of our method compared to existing state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
For intra-post dependency, DSN uses a multimodal feature extractor with an efficient fine-tuning strategy to obtain task-specific representations from images and textual information of posts. For inter-post dependency, DSN employs a hierarchical information propagation method to learn category representations that can better capture the differences between posts. Additionally, DSN utilizes recurrent networks with a series of gating layers for more flexible local temporal processing abilities and multi-head attention for long-term dependencies.The experimental results on the Social Media Popularity Dataset demonstrate the superiority of our method compared to existing state-of-the-art models.
</details></li>
</ul>
<hr>
<h2 id="Agent-Based-Model-Simulating-a-Virus-Expansion-Based-on-the-Acceptance-of-Containment-Measures"><a href="#Agent-Based-Model-Simulating-a-Virus-Expansion-Based-on-the-Acceptance-of-Containment-Measures" class="headerlink" title="Agent-Based Model: Simulating a Virus Expansion Based on the Acceptance of Containment Measures"></a>Agent-Based Model: Simulating a Virus Expansion Based on the Acceptance of Containment Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15723">http://arxiv.org/abs/2307.15723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alejandro Rodríguez-Arias, Amparo Alonso-Betanzos, Bertha Guijarro-Berdiñas, Noelia Sánchez-Marroño</li>
<li>for: 这个研究旨在描述一种基于代理模型（ABM）的社会系统分析方法，用于研究流行病在社会中的传播和控制。</li>
<li>methods: 该研究使用了修改后的SEIRD模型和公民决策模型，以模拟公民在流行病爆发期间的行为和决策。</li>
<li>results: 研究发现，公民的行为和决策对抗流行病的传播有重要影响，而且这种影响可以通过分析各个公民的行为和决策来了解。<details>
<summary>Abstract</summary>
Compartmental epidemiological models categorize individuals based on their disease status, such as the SEIRD model (Susceptible-Exposed-Infected-Recovered-Dead). These models determine the parameters that influence the magnitude of an outbreak, such as contagion and recovery rates. However, they don't account for individual characteristics or population actions, which are crucial for assessing mitigation strategies like mask usage in COVID-19 or condom distribution in HIV. Additionally, studies highlight the role of citizen solidarity, interpersonal trust, and government credibility in explaining differences in contagion rates between countries. Agent-Based Modeling (ABM) offers a valuable approach to study complex systems by simulating individual components, their actions, and interactions within an environment. ABM provides a useful tool for analyzing social phenomena. In this study, we propose an ABM architecture that combines an adapted SEIRD model with a decision-making model for citizens. In this paper, we propose an ABM architecture that allows us to analyze the evolution of virus infections in a society based on two components: 1) an adaptation of the SEIRD model and 2) a decision-making model for citizens. In this way, the evolution of infections is affected, in addition to the spread of the virus itself, by individual behavior when accepting or rejecting public health measures. We illustrate the designed model by examining the progression of SARS-CoV-2 infections in A Coru\~na, Spain. This approach makes it possible to analyze the effect of the individual actions of citizens during an epidemic on the spread of the virus.
</details>
<details>
<summary>摘要</summary>
《组室传染学模型（SEIRD模型）分类人员根据疾病状况，但这些模型不会考虑个体特征或人口行为，这些因素对控制疫情策略的评估非常重要。例如，面罩使用和HIV抗原分发等疫情控制措施的效果。学者们指出，公民团结、人际信任和政府信用度在不同国家的传染率之间存在关系。基于代理模型（ABM）可以研究复杂系统，模拟个体组件、其行为和互动环境中的交互。在本研究中，我们提出了一种ABM架构，将SEIRD模型与公民决策模型相结合，以分析病毒传播在社会中的演化。我们通过对SARS-CoV-2在西班牙加的库恩省的传播进行示例分析，以示出这种方法的效果。这种方法可以评估疫情期间公民个体行为对病毒传播的影响。》Note: The translation is provided using the Google Translate tool, and may not be entirely accurate or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Co-attention-Graph-Pooling-for-Efficient-Pairwise-Graph-Interaction-Learning"><a href="#Co-attention-Graph-Pooling-for-Efficient-Pairwise-Graph-Interaction-Learning" class="headerlink" title="Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning"></a>Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15377">http://arxiv.org/abs/2307.15377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leejunhyun/coattentiongraphpooling">https://github.com/leejunhyun/coattentiongraphpooling</a></li>
<li>paper_authors: Junhyun Lee, Bumsoo Kim, Minji Jeon, Jaewoo Kang</li>
<li>for: 处理和学习图structured数据</li>
<li>methods: 使用 co-attention 在图 pooling 中提取交互作表示</li>
<li>results: 在实际数据集上，与现有方法相比，我们的方法具有更高的精度和更低的计算成本<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have proven to be effective in processing and learning from graph-structured data. However, previous works mainly focused on understanding single graph inputs while many real-world applications require pair-wise analysis for graph-structured data (e.g., scene graph matching, code searching, and drug-drug interaction prediction). To this end, recent works have shifted their focus to learning the interaction between pairs of graphs. Despite their improved performance, these works were still limited in that the interactions were considered at the node-level, resulting in high computational costs and suboptimal performance. To address this issue, we propose a novel and efficient graph-level approach for extracting interaction representations using co-attention in graph pooling. Our method, Co-Attention Graph Pooling (CAGPool), exhibits competitive performance relative to existing methods in both classification and regression tasks using real-world datasets, while maintaining lower computational complexity.
</details>
<details>
<summary>摘要</summary>
格子神经网络（GNNs）已经证明能够有效地处理和学习具有格式结构的数据。然而，先前的工作主要集中在单个图像输入的理解上，而现实世界中许多应用需要对图像数据进行对比分析（例如场景图匹配、代码搜索和药物交互预测）。为此，最近的工作已经转移注意力到对图像对的交互进行学习。虽然这些方法提高了性能，但是它们仍然受到节点级别的交互限制，导致计算成本高并且性能不佳。为解决这个问题，我们提出了一种新的和高效的图像水平的交互表示提取方法，即协同注意力图集（CAGPool）。我们的方法在实际 dataset 上表现竞争性，同时保持计算复杂度较低。
</details></li>
</ul>
<hr>
<h2 id="Confident-Feature-Ranking"><a href="#Confident-Feature-Ranking" class="headerlink" title="Confident Feature Ranking"></a>Confident Feature Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15361">http://arxiv.org/abs/2307.15361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bitya Neuhof, Yuval Benjamini</li>
<li>for: 本研究旨在提供一种基于对比测试的后处方法，以确定特征重要性值的稳定排名。</li>
<li>methods: 本研究使用对比测试方法，对特征重要性值进行重新排名，并生成相应的置信区间。</li>
<li>results: 本研究确保了对特征重要性值的排名具有高概率包含真实排名，并且允许选择top-k集。<details>
<summary>Abstract</summary>
Interpretation of feature importance values often relies on the relative order of the features rather than on the value itself, referred to as ranking. However, the order may be unstable due to the small sample sizes used in calculating the importance values. We propose that post-hoc importance methods produce a ranking and simultaneous confident intervals for the rankings. Based on pairwise comparisons of the feature importance values, our method is guaranteed to include the ``true'' (infinite sample) ranking with high probability and allows for selecting top-k sets.
</details>
<details>
<summary>摘要</summary>
常用的特征重要性值的解释通常是通过特征之间的相对排名而进行，而不是直接查看值的大小。然而，排名的稳定性可能会受到小样本大小的影响。我们提议使用 posterior 重要性方法生成排名和同时的信任范围，以确保包含“真实”（无限大样本）排名，并允许选择 top-k 集。基于特征之间的对比，我们的方法能够保证包含“真实”排名，并且可以选择 top-k 集。Note: "posterior" in Chinese is "后验" (hòu yì).
</details></li>
</ul>
<hr>
<h2 id="Med-HALT-Medical-Domain-Hallucination-Test-for-Large-Language-Models"><a href="#Med-HALT-Medical-Domain-Hallucination-Test-for-Large-Language-Models" class="headerlink" title="Med-HALT: Medical Domain Hallucination Test for Large Language Models"></a>Med-HALT: Medical Domain Hallucination Test for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15343">http://arxiv.org/abs/2307.15343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Logesh Kumar Umapathi, Ankit Pal, Malaikannan Sankarasubbu</li>
<li>for: The paper is written to address the challenges of hallucinations in large language models (LLMs) in the medical domain, and to propose a new benchmark and dataset (Med-HALT) to evaluate and reduce hallucinations.</li>
<li>methods: The paper proposes a new benchmark and dataset (Med-HALT) that includes reasoning and memory-based hallucination tests to assess LLMs’ problem-solving and information retrieval abilities.</li>
<li>results: The study evaluates leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, and reveals significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility.<details>
<summary>Abstract</summary>
This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.   Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Skeleton-of-Thought-Large-Language-Models-Can-Do-Parallel-Decoding"><a href="#Skeleton-of-Thought-Large-Language-Models-Can-Do-Parallel-Decoding" class="headerlink" title="Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"></a>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15337">http://arxiv.org/abs/2307.15337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, Yu Wang</li>
<li>for: 降低大语言模型（LLMs）的终端生成延迟。</li>
<li>methods: 提出了“思维skeleton”（SoT），让LLMs先生成答案的框架，然后并行API调用或批处理解码完成每个框架点的内容。</li>
<li>results: 对11种不同的LLMs进行测试，得到了 considerable 的速度提升（最高达2.39倍），并且可能会在某些问题类型上提高答案质量。<details>
<summary>Abstract</summary>
This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.
</details>
<details>
<summary>摘要</summary>
这项工作的目标是减少大语言模型（LLM）的端到端生成延迟。一个主要的延迟原因是大多数现状的LLM采用的是顺序解码方法。在这项工作中，我们受人类思维和写作过程的 inspirited by，提议了“思想骨架”（SoT），帮助LLM首先生成答案的框架，然后在平行API调用或批处理decode中完善每个骨架点。不仅SoT可以提供显著的速度增加（最多2.39倍于11个不同的LLM），而且也可能提高答案质量在一些问题类型上，包括多样性和相关性。SoT是数据驱动优化的初步尝试，揭示了推动LLM思考更像人类的答案质量的可能性。
</details></li>
</ul>
<hr>
<h2 id="Tutorials-on-Stance-Detection-using-Pre-trained-Language-Models-Fine-tuning-BERT-and-Prompting-Large-Language-Models"><a href="#Tutorials-on-Stance-Detection-using-Pre-trained-Language-Models-Fine-tuning-BERT-and-Prompting-Large-Language-Models" class="headerlink" title="Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models"></a>Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15331">http://arxiv.org/abs/2307.15331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Shiuan Chuang</li>
<li>for: 本文提供了两个自包含的教程，用于在推特数据中进行立场检测，使用BERT精度和大型自然语言模型（LLM）的启发。</li>
<li>methods: 本教程涵盖了BERT体系和Tokenization，并导导用户在训练、调参和评估标准和域pecificBERT模型的方法。</li>
<li>results: 教程使用了多种提示策略，并使用混淆矩阵和macro F1分数来评估。结果显示，不需要精度调整的ChatGPT和FLAN-T5可以在几个例子下表现出优于精度调整的BERT。<details>
<summary>Abstract</summary>
This paper presents two self-contained tutorials on stance detection in Twitter data using BERT fine-tuning and prompting large language models (LLMs). The first tutorial explains BERT architecture and tokenization, guiding users through training, tuning, and evaluating standard and domain-specific BERT models with HuggingFace transformers. The second focuses on constructing prompts and few-shot examples to elicit stances from ChatGPT and open-source FLAN-T5 without fine-tuning. Various prompting strategies are implemented and evaluated using confusion matrices and macro F1 scores. The tutorials provide code, visualizations, and insights revealing the strengths of few-shot ChatGPT and FLAN-T5 which outperform fine-tuned BERTs. By covering both model fine-tuning and prompting-based techniques in an accessible, hands-on manner, these tutorials enable learners to gain applied experience with cutting-edge methods for stance detection.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文提供了使用BERT精细化和大型自然语言模型（LLM）的两个自包含的教程，用于推断推特上的立场检测。第一个教程介绍了BERT的架构和token化，并引导用户通过训练、调整和评估标准和域pecificBERT模型的方法。第二个教程专注于构建提示和几个示例来引发ChatGPT和开源FLAN-T5的立场，而不需要精细化。多种提示策略被实现和评估使用混淆矩阵和macro F1分数。这些教程提供了代码、视觉化和概念，揭示了几个批处的强点，其中几个几个示例超出了精细化BERT的性能。通过覆盖模型精细化和提示基本技术，这些教程帮助学习者获得应用最新方法的实践经验。
</details></li>
</ul>
<hr>
<h2 id="Robust-Visual-Sim-to-Real-Transfer-for-Robotic-Manipulation"><a href="#Robust-Visual-Sim-to-Real-Transfer-for-Robotic-Manipulation" class="headerlink" title="Robust Visual Sim-to-Real Transfer for Robotic Manipulation"></a>Robust Visual Sim-to-Real Transfer for Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15320">http://arxiv.org/abs/2307.15320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Garcia, Robin Strudel, Shizhe Chen, Etienne Arlaud, Ivan Laptev, Cordelia Schmid</li>
<li>for: 这个论文的目的是探索视觉动力策略在模拟环境中学习，以优化在真实世界中的机器人控制。</li>
<li>methods: 这个论文使用了域随机化（DR）方法来bridge模拟和真实数据之间的观察者随机化（sim-to-real）问题。</li>
<li>results: 研究人员通过使用DR方法，在一个rich的机器人抓取任务集中系统地探索了视觉域随机化策略，并证明了DR参数对off-line代理任务和on-line策略均有类似的影响。此外，研究人员还表明了在真实场景中的视觉变化robustness的优势。<details>
<summary>Abstract</summary>
Learning visuomotor policies in simulation is much safer and cheaper than in the real world. However, due to discrepancies between the simulated and real data, simulator-trained policies often fail when transferred to real robots. One common approach to bridge the visual sim-to-real domain gap is domain randomization (DR). While previous work mainly evaluates DR for disembodied tasks, such as pose estimation and object detection, here we systematically explore visual domain randomization methods and benchmark them on a rich set of challenging robotic manipulation tasks. In particular, we propose an off-line proxy task of cube localization to select DR parameters for texture randomization, lighting randomization, variations of object colors and camera parameters. Notably, we demonstrate that DR parameters have similar impact on our off-line proxy task and on-line policies. We, hence, use off-line optimized DR parameters to train visuomotor policies in simulation and directly apply such policies to a real robot. Our approach achieves 93% success rate on average when tested on a diverse set of challenging manipulation tasks. Moreover, we evaluate the robustness of policies to visual variations in real scenes and show that our simulator-trained policies outperform policies learned using real but limited data. Code, simulation environment, real robot datasets and trained models are available at https://www.di.ens.fr/willow/research/robust_s2r/.
</details>
<details>
<summary>摘要</summary>
学习视motor策略在模拟中 Much safer and cheaper than in the real world. However, due to discrepancies between the simulated and real data, simulator-trained policies often fail when transferred to real robots. One common approach to bridge the visual sim-to-real domain gap is domain randomization (DR). While previous work mainly evaluates DR for disembodied tasks, such as pose estimation and object detection, here we systematically explore visual domain randomization methods and benchmark them on a rich set of challenging robotic manipulation tasks. In particular, we propose an off-line proxy task of cube localization to select DR parameters for texture randomization, lighting randomization, variations of object colors and camera parameters. Notably, we demonstrate that DR parameters have similar impact on our off-line proxy task and on-line policies. We, hence, use off-line optimized DR parameters to train visuomotor policies in simulation and directly apply such policies to a real robot. Our approach achieves 93% success rate on average when tested on a diverse set of challenging manipulation tasks. Moreover, we evaluate the robustness of policies to visual variations in real scenes and show that our simulator-trained policies outperform policies learned using real but limited data. 代码、模拟环境、实际机器人数据和训练模型可以在https://www.di.ens.fr/willow/research/robust_s2r/ obtained。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Reality-The-Pivotal-Role-of-Generative-AI-in-the-Metaverse"><a href="#Beyond-Reality-The-Pivotal-Role-of-Generative-AI-in-the-Metaverse" class="headerlink" title="Beyond Reality: The Pivotal Role of Generative AI in the Metaverse"></a>Beyond Reality: The Pivotal Role of Generative AI in the Metaverse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06272">http://arxiv.org/abs/2308.06272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinay Chamola, Gaurang Bansal, Tridib Kumar Das, Vikas Hassija, Naga Siva Sai Reddy, Jiacheng Wang, Sherali Zeadally, Amir Hussain, F. Richard Yu, Mohsen Guizani, Dusit Niyato</li>
<li>for: 这篇论文探讨了如何通过生成人工智能技术实现虚拟世界的演进和互动性，以及这些技术在虚拟世界中的应用。</li>
<li>methods: 论文描述了各种生成人工智能技术，包括文本生成模型ChatGPT和GPT-3、图像生成模型DALL-E和MidJourney、以及3D模型生成技术Point-E和Lumirithmic。</li>
<li>results: 论文总结了这些技术在虚拟世界中的应用和发展前景，同时也评估了这些技术的挑战和伦理问题。<details>
<summary>Abstract</summary>
Imagine stepping into a virtual world that's as rich, dynamic, and interactive as our physical one. This is the promise of the Metaverse, and it's being brought to life by the transformative power of Generative Artificial Intelligence (AI). This paper offers a comprehensive exploration of how generative AI technologies are shaping the Metaverse, transforming it into a dynamic, immersive, and interactive virtual world. We delve into the applications of text generation models like ChatGPT and GPT-3, which are enhancing conversational interfaces with AI-generated characters. We explore the role of image generation models such as DALL-E and MidJourney in creating visually stunning and diverse content. We also examine the potential of 3D model generation technologies like Point-E and Lumirithmic in creating realistic virtual objects that enrich the Metaverse experience. But the journey doesn't stop there. We also address the challenges and ethical considerations of implementing these technologies in the Metaverse, offering insights into the balance between user control and AI automation. This paper is not just a study, but a guide to the future of the Metaverse, offering readers a roadmap to harnessing the power of generative AI in creating immersive virtual worlds.
</details>
<details>
<summary>摘要</summary>
imagine 进入一个丰富、动态、互动的虚拟世界，这是metaverse的推荐，并且这个虚拟世界正在被生成人工智能（AI）的变革力所实现。本文将进行全面的探讨，描述如何使用生成AI技术将metaverse变成一个动态、内在、互动的虚拟世界。我们将探讨文本生成模型如ChatGPT和GPT-3，它们在虚拟世界中创建了AI生成的人物，让用户能够在虚拟世界中互动。我们也将探讨图像生成模型如DALL-E和MidJourney，它们在创建丰富多样的内容方面发挥了重要作用。此外，我们还将探讨3D模型生成技术如Point-E和Lumirithmic，它们将实现虚拟物品的实际化，增强metaverse的体验。但我们的旅程不止于此。我们还需要处理在metaverse中实施这些技术的挑战和伦理考虑。本文不仅是一篇研究，更是 metaverse 未来的路径，帮助读者实现在虚拟世界中使用生成AI的潜力。
</details></li>
</ul>
<hr>
<h2 id="DiffKendall-A-Novel-Approach-for-Few-Shot-Learning-with-Differentiable-Kendall’s-Rank-Correlation"><a href="#DiffKendall-A-Novel-Approach-for-Few-Shot-Learning-with-Differentiable-Kendall’s-Rank-Correlation" class="headerlink" title="DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall’s Rank Correlation"></a>DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall’s Rank Correlation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15317">http://arxiv.org/abs/2307.15317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaipeng Zheng, Huishuai Zhang, Weiran Huang</li>
<li>for: 这个论文主要针对几何学习中的测试类别不为模型所见过的问题进行适应。</li>
<li>methods: 这个论文使用了几何 similarity 度量来衡量两个特征之间的Semantic相似性，并将 Kendall rank correlation 作为替代的度量。</li>
<li>results: 这个论文的实验结果显示，使用 Kendall rank correlation 来替代几何 similarity 度量可以对几何学习中的测试类别进行更好的适应，并且可以实现更高的性能。<details>
<summary>Abstract</summary>
Few-shot learning aims to adapt models trained on the base dataset to novel tasks where the categories are not seen by the model before. This often leads to a relatively uniform distribution of feature values across channels on novel classes, posing challenges in determining channel importance for novel tasks. Standard few-shot learning methods employ geometric similarity metrics such as cosine similarity and negative Euclidean distance to gauge the semantic relatedness between two features. However, features with high geometric similarities may carry distinct semantics, especially in the context of few-shot learning. In this paper, we demonstrate that the importance ranking of feature channels is a more reliable indicator for few-shot learning than geometric similarity metrics. We observe that replacing the geometric similarity metric with Kendall's rank correlation only during inference is able to improve the performance of few-shot learning across a wide range of datasets with different domains. Furthermore, we propose a carefully designed differentiable loss for meta-training to address the non-differentiability issue of Kendall's rank correlation. Extensive experiments demonstrate that the proposed rank-correlation-based approach substantially enhances few-shot learning performance.
</details>
<details>
<summary>摘要</summary>
通过几拍学习适应基 dataset 中的任务，目标是使模型能够适应 novel 任务中的类别未经过模型训练。这经常导致 novel 类通道的特征值分布呈相对均勋的形式，从而增加了决定通道重要性的挑战。标准的几拍学习方法通常使用 геометрические相似度度量，如 косину斯相似度和负 Euclidian 距离，来衡量两个特征之间的 semantic 相似性。但是，具有高 geometric 相似度的特征可能会拥有不同的 semantics，特别是在几拍学习上。在这篇文章中，我们表明通道的重要性排名是几拍学习中更可靠的指标，而不是 geometric 相似度度量。我们发现，在推理时将 geometric 相似度度量替换为 Kendall 排名相关性可以在不同领域的 dataset 上提高几拍学习性能。此外，我们还提出了一种特殊的可导式损失函数，用于在 meta-training 中处理 Kendall 排名相关性的不导数问题。广泛的实验表明，我们的排名相关性基于的方法可以显著提高几拍学习性能。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Multiuser-AI-Downloading-via-Reusable-Knowledge-Broadcasting"><a href="#Efficient-Multiuser-AI-Downloading-via-Reusable-Knowledge-Broadcasting" class="headerlink" title="Efficient Multiuser AI Downloading via Reusable Knowledge Broadcasting"></a>Efficient Multiuser AI Downloading via Reusable Knowledge Broadcasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15316">http://arxiv.org/abs/2307.15316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Wu, Qunsong Zeng, Kaibin Huang</li>
<li>for: 这 paper 的目的是解决 sixth-generation (6G) 移动网络中的即时适应人工智能（AI）模型下载问题，以减少无线链路上的通信开销。</li>
<li>methods: 这 paper 提出了一个名为 Model Broadcasting and Assembling (MBA) 框架，该框架利用可重用知识（shared parameters among tasks）来启用参数广播，从而减少通信开销。MBA 框架包括两个关键组件：MBA 协议和参数选择和功率控制（PS-PC）的共同设计。</li>
<li>results: 该 paper 的实验结果表明，相比传统模型下载方法，MBA 框架可以减少下载时间开销，提高设备的模型性能。<details>
<summary>Abstract</summary>
For the 6G mobile networks, in-situ model downloading has emerged as an important use case to enable real-time adaptive artificial intelligence on edge devices. However, the simultaneous downloading of diverse and high-dimensional models to multiple devices over wireless links presents a significant communication bottleneck. To overcome the bottleneck, we propose the framework of model broadcasting and assembling (MBA), which represents the first attempt on leveraging reusable knowledge, referring to shared parameters among tasks, to enable parameter broadcasting to reduce communication overhead. The MBA framework comprises two key components. The first, the MBA protocol, defines the system operations including parameter selection from a model library, power control for broadcasting, and model assembling at devices. The second component is the joint design of parameter-selection-and-power-control (PS-PC), which provides guarantees on devices' model performance and minimizes the downloading latency. The corresponding optimization problem is simplified by decomposition into the sequential PS and PC sub-problems without compromising its optimality. The PS sub-problem is solved efficiently by designing two efficient algorithms. On one hand, the low-complexity algorithm of greedy parameter selection features the construction of candidate model sets and a selection metric, both of which are designed under the criterion of maximum reusable knowledge among tasks. On the other hand, the optimal tree-search algorithm gains its efficiency via the proposed construction of a compact binary tree pruned using model architecture constraints and an intelligent branch-and-bound search. Given optimal PS, the optimal PC policy is derived in closed form. Extensive experiments demonstrate the substantial reduction in downloading latency achieved by the proposed MBA compared to traditional model downloading.
</details>
<details>
<summary>摘要</summary>
The MBA framework consists of two key components:1. MBA Protocol: This defines the system operations, including parameter selection from a model library, power control for broadcasting, and model assembling at devices.2. Joint Design of Parameter-Selection-and-Power-Control (PS-PC): This provides guarantees on devices' model performance and minimizes downloading latency. The optimization problem is simplified by decomposing it into sequential PS and PC sub-problems without compromising optimality.The PS sub-problem is solved efficiently using two efficient algorithms:1. Greedy Parameter Selection: This features the construction of candidate model sets and a selection metric, both designed under the criterion of maximum reusable knowledge among tasks.2. Optimal Tree-Search Algorithm: This gains efficiency via a proposed construction of a compact binary tree pruned using model architecture constraints and an intelligent branch-and-bound search.Given optimal PS, the optimal PC policy is derived in closed form. Extensive experiments demonstrate that the proposed MBA achieves substantial reduction in downloading latency compared to traditional model downloading.
</details></li>
</ul>
<hr>
<h2 id="WC-SBERT-Zero-Shot-Text-Classification-via-SBERT-with-Self-Training-for-Wikipedia-Categories"><a href="#WC-SBERT-Zero-Shot-Text-Classification-via-SBERT-with-Self-Training-for-Wikipedia-Categories" class="headerlink" title="WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories"></a>WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15293">http://arxiv.org/abs/2307.15293</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seventychi/wc-sbert">https://github.com/seventychi/wc-sbert</a></li>
<li>paper_authors: Te-Yu Chi, Yu-Meng Tang, Chia-Wen Lu, Qiu-Xia Zhang, Jyh-Shing Roger Jang</li>
<li>for: 解决 zero-shot 文本分类问题，尤其是自适应自动学习策略。</li>
<li>methods: 提议一种使用标签而不是文本进行训练的新型自适应策略，利用 Wikipedia 中的类别作为训练集，并使用 SBERT 预训练模型建立文本中对应的相互关系，以便 associative 训练。</li>
<li>results: 实验结果表明，这种方法可以在 minutes 内将模型适应目标数据集，并在 Yahoo Topic 和 AG News 数据集上达到了状态元的 результаTS。相比其他 BERT 基于 transformer 模型，我们的方法可以减少训练数据量，提高训练效率，并且在不同数据集上进行快速的精度调整和推理。<details>
<summary>Abstract</summary>
Our research focuses on solving the zero-shot text classification problem in NLP, with a particular emphasis on innovative self-training strategies. To achieve this objective, we propose a novel self-training strategy that uses labels rather than text for training, significantly reducing the model's training time. Specifically, we use categories from Wikipedia as our training set and leverage the SBERT pre-trained model to establish positive correlations between pairs of categories within the same text, facilitating associative training. For new test datasets, we have improved the original self-training approach, eliminating the need for prior training and testing data from each target dataset. Instead, we adopt Wikipedia as a unified training dataset to better approximate the zero-shot scenario. This modification allows for rapid fine-tuning and inference across different datasets, greatly reducing the time required for self-training. Our experimental results demonstrate that this method can adapt the model to the target dataset within minutes. Compared to other BERT-based transformer models, our approach significantly reduces the amount of training data by training only on labels, not the actual text, and greatly improves training efficiency by utilizing a unified training set. Additionally, our method achieves state-of-the-art results on both the Yahoo Topic and AG News datasets.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:我们的研究集中于解决NLP中的零例文本分类问题，强调创新自动训练策略。为达到这个目标，我们提议一种新的自动训练策略，使用标签而不是文本进行训练，减少模型训练时间。特别是，我们使用Wikipedia中的类别作为我们的训练集，利用SBERT预训练模型来建立文本中类别之间的相互关联，促进相关训练。对于新的测试集，我们改进了原始自动训练方法，消除了每个目标集需要的先前训练和测试数据。而是采用Wikipedia作为一个统一的训练集，更好地逼近零例场景。这种修改允许快速的细化和推理，大幅减少自动训练的时间。我们的实验结果表明，这种方法可以在分钟内适应目标集。相比其他BERT基于转换器模型，我们的方法可以减少训练数据量，只训练标签而不是实际文本，并且大幅提高训练效率。此外，我们的方法在Yahoo主题和AG新闻集上达到了状态对的结果。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-before-Responding-Integrating-Commonsense-based-Causality-Explanation-for-Empathetic-Response-Generation"><a href="#Reasoning-before-Responding-Integrating-Commonsense-based-Causality-Explanation-for-Empathetic-Response-Generation" class="headerlink" title="Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation"></a>Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00085">http://arxiv.org/abs/2308.00085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yahui Fu, Koji Inoue, Chenhui Chu, Tatsuya Kawahara</li>
<li>for: 提高对用户情感的理解和回应</li>
<li>methods:  incorporating commonsense knowledge and reasoning about the causes of emotions, integrating in-context learning with commonsense knowledge, and integrating with ChatGPT and T5-based models</li>
<li>results: outperforms other comparable methods on both automatic and human evaluations<details>
<summary>Abstract</summary>
Recent approaches to empathetic response generation try to incorporate commonsense knowledge or reasoning about the causes of emotions to better understand the user's experiences and feelings. However, these approaches mainly focus on understanding the causalities of context from the user's perspective, ignoring the system's perspective. In this paper, we propose a commonsense-based causality explanation approach for diverse empathetic response generation that considers both the user's perspective (user's desires and reactions) and the system's perspective (system's intentions and reactions). We enhance ChatGPT's ability to reason for the system's perspective by integrating in-context learning with commonsense knowledge. Then, we integrate the commonsense-based causality explanation with both ChatGPT and a T5-based model. Experimental evaluations demonstrate that our method outperforms other comparable methods on both automatic and human evaluations.
</details>
<details>
<summary>摘要</summary>
现代方法 для生成同情响应尝试包含常识知识或理智来更好地理解用户的经验和情感。然而，这些方法主要关注用户的视角，忽略系统的视角。在这篇论文中，我们提出一种基于常识的 causality 解释方法 для多样化同情响应生成，考虑用户的视角（用户的愿望和反应）和系统的视角（系统的意图和反应）。我们通过将审计学习与常识知识集成到ChatGPT中，提高其理解系统视角的能力。然后，我们将commonsense-based causality explanation与ChatGPT和基于T5的模型集成。实验评估表明，我们的方法在自动和人类评估中都高于其他相似方法。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Whole-Slide-Image-Classification"><a href="#Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Whole-Slide-Image-Classification" class="headerlink" title="Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification"></a>Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15254">http://arxiv.org/abs/2307.15254</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dearcaat/mhim-mil">https://github.com/dearcaat/mhim-mil</a></li>
<li>paper_authors: Wenhao Tang, Sheng Huang, Xiaoxian Zhang, Fengtao Zhou, Yi Zhang, Bo Liu</li>
<li>for: 这篇论文是关于多个实例学习（MIL）问题的解决方案。</li>
<li>methods: 这篇论文使用了一种新的实例遮盖策略（Masked Hard Instance Mining，MHIM），它使用一个SIAMESE结构（教师-学生）和一个准确性约束来探索可能的困难实例。</li>
<li>results: 实验结果表明，使用MHIM-MIL方法可以在CAMELYON-16和TCGA肺癌数据集上超过其他最新的方法，并且具有更好的性能和训练成本。Here’s the breakdown of each point in more detail:</li>
<li>for: The paper is about solving the Multiple Instance Learning (MIL) problem, which is a common problem in medical image analysis.</li>
<li>methods: The proposed method uses a novel instance masking strategy called Masked Hard Instance Mining (MHIM), which combines a Siamese structure with a consistency constraint to explore potential hard instances.</li>
<li>results: The experimental results on the CAMELYON-16 and TCGA Lung Cancer datasets show that the proposed MHIM-MIL method outperforms other state-of-the-art methods in terms of performance and training cost.<details>
<summary>Abstract</summary>
The whole slide image (WSI) classification is often formulated as a multiple instance learning (MIL) problem. Since the positive tissue is only a small fraction of the gigapixel WSI, existing MIL methods intuitively focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting hard-to-classify instances. Some literature has revealed that hard examples are beneficial for modeling a discriminative boundary accurately. By applying such an idea at the instance level, we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which uses a Siamese structure (Teacher-Student) with a consistency constraint to explore the potential hard instances. With several instance masking strategies based on attention scores, MHIM-MIL employs a momentum teacher to implicitly mine hard instances for training the student model, which can be any attention-based MIL model. This counter-intuitive strategy essentially enables the student to learn a better discriminating boundary. Moreover, the student is used to update the teacher with an exponential moving average (EMA), which in turn identifies new hard instances for subsequent training iterations and stabilizes the optimization. Experimental results on the CAMELYON-16 and TCGA Lung Cancer datasets demonstrate that MHIM-MIL outperforms other latest methods in terms of performance and training cost. The code is available at: https://github.com/DearCaat/MHIM-MIL.
</details>
<details>
<summary>摘要</summary>
整个滤镜图像（WSI）分类经常被视为多例学习（MIL）问题。由于正例组占整个多个吉比特像素的只有一小部分，现有的MIL方法倾向于通过注意力机制来标识突出的实例。然而，这会导致模型偏好易于分类的实例，而忽略困难分类的实例。一些文献表明，困难的实例对模型准确地界定边框具有重要作用。我们在实例层次上运用这一想法，提出了一种新的MIL框架——偏挥硬实例挖掘（MHIM-MIL）。该框架使用了SIAMESE结构（教师-学生），并通过一致性约束来探索潜在的困难实例。通过多种实例层次的掩码策略，MHIM-MIL使用了掩码硬实例来训练学生模型，该模型可以是任何注意力基于的MIL模型。这种Counter-intuitive策略使得学生能够学习更好的分类边界。此外，学生模型被用来更新教师模型，并使用了指数移动平均（EMA）来识别新的困难实例，以便在后续训练迭代中进行更新。实验结果表明，MHIM-MIL在CAMELYON-16和TCGA肺癌数据集上表现出色，比latest方法更高的性能和训练成本。代码可以在：https://github.com/DearCaat/MHIM-MIL。
</details></li>
</ul>
<hr>
<h2 id="An-Overview-Of-Temporal-Commonsense-Reasoning-and-Acquisition"><a href="#An-Overview-Of-Temporal-Commonsense-Reasoning-and-Acquisition" class="headerlink" title="An Overview Of Temporal Commonsense Reasoning and Acquisition"></a>An Overview Of Temporal Commonsense Reasoning and Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00002">http://arxiv.org/abs/2308.00002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Wenzel, Adam Jatowt</li>
<li>for: 本文旨在探讨大语言模型在时间常识逻辑 reasoning 方面的表现，并提出了一些增强语言模型表现的方法。</li>
<li>methods: 本文使用了多种增强方法，包括数据增强、随机隐藏状态和随机掩码等，以提高语言模型的时间常识逻辑能力。</li>
<li>results:  despite the use of these augmentations, the models still struggle to approach human performance on reasoning tasks over temporal common sense properties, such as the typical occurrence times, orderings, or durations of events.<details>
<summary>Abstract</summary>
Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning tasks over temporal common sense properties, such as the typical occurrence times, orderings, or durations of events. We further emphasize the need for careful interpretation of research to guard against overpromising evaluation results in light of the shallow reasoning present in transformers. This can be achieved by appropriately preparing datasets and suitable evaluation metrics.
</details>
<details>
<summary>摘要</summary>
时间常识逻辑指的是理解phrase、action和event的典型时间上下文，并使用这些知识来解决问题。这种 trait 是 temporal natural language processing 任务的关键特征，可能的应用包括时间线概要、时间问答和时间自然语言推理。Recent research 表明，虽然大语言模型能够生成正确的语法结构和解决分类任务，但它们经常采取短cuts 的思维方式，容易受到simple linguistic traps 的影响。本文提供了 temporal commonsense reasoning 领域的研究概述，特别是通过多种加强和其评估在不断增长的数据集上。然而，这些加强模型仍然无法 approached human performance 在时间常识性Property上，如事件的典型发生时间、顺序或持续时间。我们进一步强调需要在研究中进行仔细的解释，以避免因 transformers 的浅层理解而导致的误导。这可以通过适当的数据准备和评估 metric 来实现。
</details></li>
</ul>
<hr>
<h2 id="A-Practical-Recipe-for-Federated-Learning-Under-Statistical-Heterogeneity-Experimental-Design"><a href="#A-Practical-Recipe-for-Federated-Learning-Under-Statistical-Heterogeneity-Experimental-Design" class="headerlink" title="A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design"></a>A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15245">http://arxiv.org/abs/2307.15245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mmorafah/fedzoo-bench">https://github.com/mmorafah/fedzoo-bench</a></li>
<li>paper_authors: Mahdi Morafah, Weijia Wang, Bill Lin</li>
<li>for: 本研究旨在探讨 Federated Learning (FL) 在数据不同性下的应用，并提供一个可比较的和有奖励的实验设置。</li>
<li>methods: 本研究使用了多种 FL 方法，包括 22 种state-of-the-art 方法，并提供了一个开源库 PyTorch 的实现。</li>
<li>results: 研究发现了 FL 特有的实验变量对性能的影响，并提供了一些建议和标准化的特性，以帮助设计更加有意义和有奖励的 FL 实验设置。<details>
<summary>Abstract</summary>
Federated Learning (FL) has been an area of active research in recent years. There have been numerous studies in FL to make it more successful in the presence of data heterogeneity. However, despite the existence of many publications, the state of progress in the field is unknown. Many of the works use inconsistent experimental settings and there are no comprehensive studies on the effect of FL-specific experimental variables on the results and practical insights for a more comparable and consistent FL experimental setup. Furthermore, the existence of several benchmarks and confounding variables has further complicated the issue of inconsistency and ambiguity. In this work, we present the first comprehensive study on the effect of FL-specific experimental variables in relation to each other and performance results, bringing several insights and recommendations for designing a meaningful and well-incentivized FL experimental setup. We further aid the community by releasing FedZoo-Bench, an open-source library based on PyTorch with pre-implementation of 22 state-of-the-art methods, and a broad set of standardized and customizable features available at https://github.com/MMorafah/FedZoo-Bench. We also provide a comprehensive comparison of several state-of-the-art (SOTA) methods to better understand the current state of the field and existing limitations.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 是近年来的一个热点领域，有很多研究来使其在数据不同性时更成功。然而，尽管有很多论文，但现状的进步还未得到了一个全面的了解。许多研究使用不一致的实验设置，并没有系统性的研究FL特有的实验变量对结果和实践建议。此外，存在多个标准准则和干扰变量，导致了不一致和混乱的问题。在这项工作中，我们提供了FL特有实验变量与其他变量之间的首次全面研究，从而获得了许多新的发现和建议，以及设计一个有意义和有奖励的FL实验设置。此外，我们还发布了FedZoo-Bench，一个基于PyTorch的开源库，包含22种当前领导的方法的预实现，以及一个广泛的标准化和自定义功能，可以在https://github.com/MMorafah/FedZoo-Bench中获取。此外，我们还提供了多种当前领导方法的比较，以更好地了解现场的状况和存在的限制。
</details></li>
</ul>
<hr>
<h2 id="BOURNE-Bootstrapped-Self-supervised-Learning-Framework-for-Unified-Graph-Anomaly-Detection"><a href="#BOURNE-Bootstrapped-Self-supervised-Learning-Framework-for-Unified-Graph-Anomaly-Detection" class="headerlink" title="BOURNE: Bootstrapped Self-supervised Learning Framework for Unified Graph Anomaly Detection"></a>BOURNE: Bootstrapped Self-supervised Learning Framework for Unified Graph Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15244">http://arxiv.org/abs/2307.15244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jackson117/BOURNE">https://github.com/Jackson117/BOURNE</a></li>
<li>paper_authors: Jie Liu, Mengting He, Xuequn Shang, Jieming Shi, Bin Cui, Hongzhi Yin</li>
<li>for: 这篇论文的目的是提出一个统一的图像异常检测方法，以检测图像中的节点和边异常。</li>
<li>methods: 本论文使用的方法包括图像观察中心的节点和边异常检测模型，以及一个bootstrapped自我监督学习架构（BOURNE）。BOURNE使用图像和图像对映的方法来捕捉节点和边的表现，并通过节点和边之间的对映来实现节点和边异常的互相检测。</li>
<li>results: 实验结果显示，BOURNE在6个benchmark dataset上具有较高的异常检测效果和效率，并且可以处理大型图像。<details>
<summary>Abstract</summary>
Graph anomaly detection (GAD) has gained increasing attention in recent years due to its critical application in a wide range of domains, such as social networks, financial risk management, and traffic analysis. Existing GAD methods can be categorized into node and edge anomaly detection models based on the type of graph objects being detected. However, these methods typically treat node and edge anomalies as separate tasks, overlooking their associations and frequent co-occurrences in real-world graphs. As a result, they fail to leverage the complementary information provided by node and edge anomalies for mutual detection. Additionally, state-of-the-art GAD methods, such as CoLA and SL-GAD, heavily rely on negative pair sampling in contrastive learning, which incurs high computational costs, hindering their scalability to large graphs. To address these limitations, we propose a novel unified graph anomaly detection framework based on bootstrapped self-supervised learning (named BOURNE). We extract a subgraph (graph view) centered on each target node as node context and transform it into a dual hypergraph (hypergraph view) as edge context. These views are encoded using graph and hypergraph neural networks to capture the representations of nodes, edges, and their associated contexts. By swapping the context embeddings between nodes and edges and measuring the agreement in the embedding space, we enable the mutual detection of node and edge anomalies. Furthermore, we adopt a bootstrapped training strategy that eliminates the need for negative sampling, enabling BOURNE to handle large graphs efficiently. Extensive experiments conducted on six benchmark datasets demonstrate the superior effectiveness and efficiency of BOURNE in detecting both node and edge anomalies.
</details>
<details>
<summary>摘要</summary>
GRAPH anomaly detection (GAD) 在过去几年内得到了越来越多的关注，因为它在各种领域中具有重要的应用，如社交网络、金融风险管理和交通分析。现有的 GAD 方法可以分为基于图对象类型的节点和边异常检测模型。然而，这些方法通常将节点和边异常视为分开的任务，忽略了它们在实际图中的相互关系和常见的共occurrence。这会导致它们无法利用节点和边异常的相互信息进行互助检测。此外，现有的 GAD 方法，如 CoLA 和 SL-GAD，通常依赖于负样本采样，这会增加计算成本，使其不可扩展到大型图。为解决这些限制，我们提出了一种基于自我超vision learning的新的统一图异常检测框架（名为 BOURNE）。我们将目标节点所在的子图（图视图）中心化为节点上下文，并将其转换成双向图（双向图视图）来表示边上下文。这些视图被图和双向图神经网络编码，以Capture图节点、边和其相关上下文的表示。通过交换节点和边上下文嵌入的协调，我们实现了节点和边异常之间的互助检测。此外，我们采用了自我超视learning的培训策略，不需要负样本，从而使 BOURNE 可以高效地处理大型图。我们在六个 benchmark 数据集上进行了广泛的实验，结果表明 BOURNE 能够高效地检测节点和边异常。
</details></li>
</ul>
<hr>
<h2 id="Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures"><a href="#Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures" class="headerlink" title="Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures"></a>Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15220">http://arxiv.org/abs/2307.15220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/camma-public/surgvlp">https://github.com/camma-public/surgvlp</a></li>
<li>paper_authors: Kun Yuan, Vinkle Srivastav, Tong Yu, Joel Lavanchy, Pietro Mascagni, Nassir Navab, Nicolas Padoy</li>
<li>for: 本研究旨在使用开放式 Laparoscopic surgery 视频教程提供有效的超级视觉语言学习指导，无需人工标注。</li>
<li>methods: 我们使用多种自动语音识别系统生成视频lecture 的文本转写，并提出了一种新的多模态表示学习方法——SurgVLP，用于对视频和文本进行共同表示学习。</li>
<li>results: 我们在多种视觉语言任务上展示了我们的方法的表示能力，包括文本基于视频检索、时间活动固定和视频描述等。此外，我们还证明了我们的方法可以无需人工标注来进行传统的视觉下游任务，如手术工具、阶段和 triplet 识别。<details>
<summary>Abstract</summary>
Recent advancements in surgical computer vision applications have been driven by fully-supervised methods, primarily using only visual data. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. SurgVLP constructs a new contrastive learning objective to align video clip embeddings with the corresponding multiple text embeddings by bringing them together within a joint latent space. To effectively show the representation capability of the learned joint latent space, we introduce several vision-and-language tasks for surgery, such as text-based video retrieval, temporal activity grounding, and video captioning, as benchmarks for evaluation. We further demonstrate that without using any labeled ground truth, our approach can be employed for traditional vision-only surgical downstream tasks, such as surgical tool, phase, and triplet recognition. The code will be made available at https://github.com/CAMMA-public/SurgVLP
</details>
<details>
<summary>摘要</summary>
近期的手术计算机视觉应用程序得到了完全监督的方法驱动，主要使用视觉数据。这些方法依赖于手术视频的手动标注来预测固定的对象类别，这限制了它们对未经见过手术过程和下游任务的泛化能力。在这种工作中，我们提出了使用开放手术电子学习平台上的手术视频课程来提供有效的监督信号，以实现多模态表示学习而无需手动标注。我们对手术视频课程中存在的手术语言特有挑战采用多种自动语音识别系统来生成文本转录。然后，我们提出了一种新的多模态表示学习方法——手术视语言预训练（SurgVLP）。SurgVLP构建了一个新的对比学习目标，将视频剪辑embedding与相应的多个文本embedding集成在一个共同的latent空间中。为了有效地展示学习得到的共同空间表示能力，我们引入了许多视频和语言任务，如文本基于视频检索、时间活动固定和视频描述，作为评估标准。此外，我们还证明了我们的方法无需使用任何标注数据，可以用于传统的视觉下游任务，如手术工具、阶段和 triplet 识别。代码将在https://github.com/CAMMA-public/SurgVLP 上公开。
</details></li>
</ul>
<hr>
<h2 id="Reachability-Poorman-Discrete-Bidding-Games"><a href="#Reachability-Poorman-Discrete-Bidding-Games" class="headerlink" title="Reachability Poorman Discrete-Bidding Games"></a>Reachability Poorman Discrete-Bidding Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15218">http://arxiv.org/abs/2307.15218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Avni, Tobias Meggendorfer, Suman Sadhukhan, Josef Tkadlec, Đorđe Žikelić</li>
<li>for: 本研究是关于“拍卖游戏”（bidding games），特别是在图格上进行的两个玩家零点游戏。</li>
<li>methods: 本研究使用“贫人精确拍卖”（poorman discrete-bidding）机制，其中竞拍奖金是限制的，高得分玩家将奖金支付给银行。</li>
<li>results: 研究发现，在图DAGs中，贫人预算的阈值可以在某些情况下提供误差 bounds，并且具有周期性。在特定情况下，我们还发现了关闭式解决方案。我们还实现了一种算法来找到阈值预算。<details>
<summary>Abstract</summary>
We consider {\em bidding games}, a class of two-player zero-sum {\em graph games}. The game proceeds as follows. Both players have bounded budgets. A token is placed on a vertex of a graph, in each turn the players simultaneously submit bids, and the higher bidder moves the token, where we break bidding ties in favor of Player 1. Player 1 wins the game iff the token visits a designated target vertex. We consider, for the first time, {\em poorman discrete-bidding} in which the granularity of the bids is restricted and the higher bid is paid to the bank. Previous work either did not impose granularity restrictions or considered {\em Richman} bidding (bids are paid to the opponent). While the latter mechanisms are technically more accessible, the former is more appealing from a practical standpoint. Our study focuses on {\em threshold budgets}, which is the necessary and sufficient initial budget required for Player 1 to ensure winning against a given Player 2 budget. We first show existence of thresholds. In DAGs, we show that threshold budgets can be approximated with error bounds by thresholds under continuous-bidding and that they exhibit a periodic behavior. We identify closed-form solutions in special cases. We implement and experiment with an algorithm to find threshold budgets.
</details>
<details>
<summary>摘要</summary>
我们考虑{\em 拍卖游戏}，一种两player零余{\em 图形游戏}。游戏进行如下：两名玩家都有固定预算。一个 токен被放在一个图形上的顶点上，在每次转折时，两名玩家同时提交拍卖，高拍卖者可以移动 токен，并且在拍卖僵固时，将拍卖赢家决定为 Player 1。Player 1 赢得游戏，只要 токен到达一个指定的目标顶点。我们在这篇研究中，以前无法实现的{\em 穷人粗糙拍卖}，即拍卖时的价格粗糙限制，并且不同于前一些研究，不允许玩家在拍卖时付出费用。我们的研究集中在{\em 阈值预算}，即玩家必须具备的最低预算，以确保在对某名玩家的预算下获得胜利。我们首先证明存在阈值。在DAGs中，我们证明阈值预算可以准确地 aproximated ，并且它们展现了一个周期性的行为。我们还发现了特殊情况下的关闭式解。我们实现了一个算法，以找到阈值预算。
</details></li>
</ul>
<hr>
<h2 id="Open-Problems-and-Fundamental-Limitations-of-Reinforcement-Learning-from-Human-Feedback"><a href="#Open-Problems-and-Fundamental-Limitations-of-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"></a>Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15217">http://arxiv.org/abs/2307.15217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell</li>
<li>for: 这篇论文旨在探讨人工智能系统RLHF的问题和限制，以及如何更好地开发更安全的AI系统。</li>
<li>methods: 论文使用了RLHF和相关方法的评估和改进方法，以及如何在实践中使用这些方法。</li>
<li>results: 论文提出了RLHF和相关方法的开放问题和基本限制，并提出了优化和补偿这些方法的建议。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
</details>
<details>
<summary>摘要</summary>
人工智能强化学习（RLHF）是一种训练人工智能系统以实现人类目标的技术。RLHF已经成为现代大语言模型（LLM）的训练方法的中心。尽管如此，RLHF的问题和限制得到了相对较少的公共研究。在这篇论文中，我们（1）survey了RLHF和相关方法的开放问题和基本限制;（2）介绍了RLHF在实践中的理解、改进和补充方法;（3）提出了审核和披露标准，以提高RLHF系统的社会监管。我们的工作强调RLHF的限制，并高调了在RLHF系统的开发中采取多方面的方法，以建立更安全的人工智能系统。
</details></li>
</ul>
<hr>
<h2 id="PromptStyler-Prompt-driven-Style-Generation-for-Source-free-Domain-Generalization"><a href="#PromptStyler-Prompt-driven-Style-Generation-for-Source-free-Domain-Generalization" class="headerlink" title="PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization"></a>PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15199">http://arxiv.org/abs/2307.15199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhyeong Cho, Gilhyun Nam, Sungyeon Kim, Hunmin Yang, Suha Kwak</li>
<li>for: 这个论文旨在提出一种不需要任何图像的源自由频谱适应方法，以便在视觉语言空间中生成多种风格特征。</li>
<li>methods: 该方法使用提示来生成多种风格特征，并使用可学习的风格词vector来 Represent these styles。为确保风格特征不会扭曲内容信息，该方法在视觉语言空间中强制风格-内容特征之间的相互靠近。</li>
<li>results: 该方法在PACS、VLCS、OfficeHome和DomainNet等四个 datasets上达到了状态对的最佳性能，而不需要任何图像进行训练。<details>
<summary>Abstract</summary>
In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Also, a recent study has demonstrated the cross-modal transferability phenomenon of this joint space. From these observations, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. The proposed method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, even though it does not require any images for training.
</details>
<details>
<summary>摘要</summary>
在共同视语空间中，文本特征（例如来自“狗照片”）可以有效表示相关的图像特征（例如狗照片中的特征）。此外，一项latest study发现了这个共同空间的各模态传递现象。基于这些观察，我们提出了PromptStyler，它在共同空间中通过提示 simulate various distribution shifts，无需使用任何图像进行源自无图像领域泛化。我们的方法学习生成多种风格特征（例如“S*风格”） via 可学习的风格词vecorts for pseudo-words S*。为确保学习的风格不会扭曲内容信息，我们强制风格-内容特征（例如“S*风格的[类别]”）在共同视语空间中与其相应的内容特征（例如“[类别]”）相 nearby。之后，我们使用生成的风格-内容特征进行线性分类。PromptStyler实现了在PACS、VLCS、OfficeHome和DomainNet上的state of the art，即使没有使用任何图像进行训练。
</details></li>
</ul>
<hr>
<h2 id="One-shot-Joint-Extraction-Registration-and-Segmentation-of-Neuroimaging-Data"><a href="#One-shot-Joint-Extraction-Registration-and-Segmentation-of-Neuroimaging-Data" class="headerlink" title="One-shot Joint Extraction, Registration and Segmentation of Neuroimaging Data"></a>One-shot Joint Extraction, Registration and Segmentation of Neuroimaging Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15198">http://arxiv.org/abs/2307.15198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonymous4545/jers">https://github.com/anonymous4545/jers</a></li>
<li>paper_authors: Yao Su, Zhentian Qian, Lei Ma, Lifang He, Xiangnan Kong</li>
<li>for: 本研究旨在开发一种基于单个标注图像（即Atlas）和几个未标注原始图像的一拟合批处理方法，以提高脑成像数据中的抽取、准确和分割等预处理步骤的效果。</li>
<li>methods: 本研究提出了一种统一的端到端框架，称为JERS，用于联合优化抽取、准确和分割任务。该框架使用了一组抽取、准确和分割模块，通过自我超级视觉来互相强化和促进Feedback。</li>
<li>results: 实验结果表明，我们提出的方法在抽取、准确和分割任务中表现出色，并且可以在实际 dataset 上减少人工干预和标注量。 codes 和数据可以在<a target="_blank" rel="noopener" href="https://github.com/Anonymous4545/JERS">https://github.com/Anonymous4545/JERS</a> 找到。<details>
<summary>Abstract</summary>
Brain extraction, registration and segmentation are indispensable preprocessing steps in neuroimaging studies. The aim is to extract the brain from raw imaging scans (i.e., extraction step), align it with a target brain image (i.e., registration step) and label the anatomical brain regions (i.e., segmentation step). Conventional studies typically focus on developing separate methods for the extraction, registration and segmentation tasks in a supervised setting. The performance of these methods is largely contingent on the quantity of training samples and the extent of visual inspections carried out by experts for error correction. Nevertheless, collecting voxel-level labels and performing manual quality control on high-dimensional neuroimages (e.g., 3D MRI) are expensive and time-consuming in many medical studies. In this paper, we study the problem of one-shot joint extraction, registration and segmentation in neuroimaging data, which exploits only one labeled template image (a.k.a. atlas) and a few unlabeled raw images for training. We propose a unified end-to-end framework, called JERS, to jointly optimize the extraction, registration and segmentation tasks, allowing feedback among them. Specifically, we use a group of extraction, registration and segmentation modules to learn the extraction mask, transformation and segmentation mask, where modules are interconnected and mutually reinforced by self-supervision. Empirical results on real-world datasets demonstrate that our proposed method performs exceptionally in the extraction, registration and segmentation tasks. Our code and data can be found at https://github.com/Anonymous4545/JERS
</details>
<details>
<summary>摘要</summary>
脑部提取、注册和分割是 neuroscience 研究中不可或缺的前processing 步骤。目的是从 raw 成像扫描中提取脑部（i.e., 提取步骤），将其与目标脑部图像（i.e., 注册步骤）进行对接，并将脑部区域标注为不同的 анатомические区域（i.e., 分割步骤）。传统的研究通常会对提取、注册和分割任务进行分别的开发，并在超级vised Setting中进行训练。然而，收集 voxel-level 标签和进行手动质量控制高维度 neuroscience 成像数据（例如 3D MRI）是许多医学研究中的昂贵和时间consuming。在这篇论文中，我们研究了一种一遍性的脑部提取、注册和分割方法，该方法只需要一个标注图像（即 atlas）和一些 raw 成像数据进行训练。我们提出了一个统一的端到端框架，称之为 JERS，以同时优化提取、注册和分割任务，并允许Feedback among them。具体来说，我们使用一组提取、注册和分割模块，通过自我监督来学习提取 маMask，变换和分割 mask，这些模块之间存在相互之间的连接和互相强化。我们在实际数据上进行了实验，结果表明我们的提案方法在提取、注册和分割任务中表现出色。我们的代码和数据可以在 https://github.com/Anonymous4545/JERS 找到。
</details></li>
</ul>
<hr>
<h2 id="Learning-in-Repeated-Multi-Unit-Pay-As-Bid-Auctions"><a href="#Learning-in-Repeated-Multi-Unit-Pay-As-Bid-Auctions" class="headerlink" title="Learning in Repeated Multi-Unit Pay-As-Bid Auctions"></a>Learning in Repeated Multi-Unit Pay-As-Bid Auctions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15193">http://arxiv.org/abs/2307.15193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rigel Galgana, Negin Golrezaei</li>
<li>For: The paper is written for learning how to bid in repeated multi-unit pay-as-bid auctions, with the goal of maximizing revenue.* Methods: The paper uses dynamic programming and online learning algorithms with polynomial time and space complexity under full information and bandit feedback settings.* Results: The paper achieves an upper bound on regret of $O(M\sqrt{T\log |\mathcal{B}|})$ and $O(M\sqrt{|\mathcal{B}|T\log |\mathcal{B}|})$ respectively, and demonstrates through numerical results that the resulting market dynamics converge to a welfare maximizing equilibrium where bidders submit uniform bids. Additionally, the paper shows that the pay-as-bid auction consistently generates significantly higher revenue compared to its popular alternative, the uniform price auction.Here is the same information in Simplified Chinese:* For: 这篇论文是为了学习在重复的多个单位付款为投标的拍卖中，以最大化收益为目的。* Methods: 论文使用动态规划和在线学习算法，具有对数时间和空间复杂度的优化。* Results: 论文实现了对 regret 的Upper bound，并通过数值实验表明，市场动态 converge 到一个最大化利益的均衡点，投标者提交均匀投标。此外，论文还表明，付款拍卖可以与通常的固定价格拍卖相比，一直高得多。<details>
<summary>Abstract</summary>
Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under full information and bandit feedback settings. We achieve an upper bound on regret of $O(M\sqrt{T\log |\mathcal{B}|})$ and $O(M\sqrt{|\mathcal{B}|T\log |\mathcal{B}|})$ respectively, where $M$ is the number of units demanded by the bidder, $T$ is the total number of auctions, and $|\mathcal{B}|$ is the size of the discretized bid space. We accompany these results with a regret lower bound, which match the linear dependency in $M$. Our numerical results suggest that when all agents behave according to our proposed no regret learning algorithms, the resulting market dynamics mainly converge to a welfare maximizing equilibrium where bidders submit uniform bids. Lastly, our experiments demonstrate that the pay-as-bid auction consistently generates significantly higher revenue compared to its popular alternative, the uniform price auction.
</details>
<details>
<summary>摘要</summary>
受到碳排放交易制度、储蓄拍卖和采购拍卖的启发，我们考虑了在重复的多单位付出拍卖中学习投标的问题。在这些拍卖中，大量相同的物品需要分配给最大的提交投标价格，其中每个赢得投标价格都等于投标价格本身。投标在付出拍卖中的问题具有 combinatorial 性，这使得问题更加挑战。我们通过关注线上设置，即bidder在过去其他投标者提交的投标中仅有访问 Vector of bids 的问题来解决这个挑战。我们表明了在线上问题的优化解决方案可以在 polynomial time 内完成。我们利用 DP 算法的结构来设计在线学习算法，其时间复杂度和空间复杂度均为 O(M\*sqrt(T\*log(|\mathcal{B}|)))，其中 M 是投标者需要的单位数量，T 是总的拍卖数量，并且 $|\mathcal{B}|$ 是投标空间中的精度。我们还提供了一个 regret 下界，它与 M 的线性相似。我们的数值结果表明，当所有代理人按照我们建议的无恐学习算法进行投标时，市场动态会主要向积极的均衡点转化，其中投标者会提交均匀投标。最后，我们的实验表明，付出拍卖routinely 生成较高的收益，相比于其受欢迎的替代方案 uniform price auction。
</details></li>
</ul>
<hr>
<h2 id="Med-Flamingo-a-Multimodal-Medical-Few-shot-Learner"><a href="#Med-Flamingo-a-Multimodal-Medical-Few-shot-Learner" class="headerlink" title="Med-Flamingo: a Multimodal Medical Few-shot Learner"></a>Med-Flamingo: a Multimodal Medical Few-shot Learner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15189">http://arxiv.org/abs/2307.15189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snap-stanford/med-flamingo">https://github.com/snap-stanford/med-flamingo</a></li>
<li>paper_authors: Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Cyril Zakka, Yash Dalmia, Eduardo Pontes Reis, Pranav Rajpurkar, Jure Leskovec</li>
<li>for: 这个研究旨在提出一个适应医疗领域多Modal Few-shot Learning的模型，以满足医疗应用中资料稀少的问题。</li>
<li>methods: 本研究基于OpenFlamingo-9B，继续进行预训练，使用医疗图像和文本数据库，并实现了几个医疗问题的解释。</li>
<li>results: 研究结果显示，Med-Flamingo可以在几个医疗问题上表现出优秀的生成能力，并且可以提供解释，具体来说是20%的提升在医生评价中。此外，本研究首次进行了人类评价，并发现Med-Flamingo可以在不同的医疗问题上提供更好的解释。<details>
<summary>Abstract</summary>
Medicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models (VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering (VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluation for generative medical VQA where physicians review the problems and blinded generations in an interactive app. Med-Flamingo improves performance in generative medical VQA by up to 20\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation. We release our model, code, and evaluation app under https://github.com/snap-stanford/med-flamingo.
</details>
<details>
<summary>摘要</summary>
医学是一个多方面的领域，需要将多种模式的信息集成起来。医学生成视语模型（VLM）可以作为第一步，并且承诺了许多临床应用。然而，现有模型通常需要在大量下游数据集上练习，这会限制其在医学应用中的使用，因为在许多医学应用中数据是稀缺的，需要能够从少量示例中学习。在这里，我们提出了医学鹭鸟（Med-Flamingo），一种适应医学领域的多Modal几个shot学习者。基于OpenFlamingo-9B，我们继续预训练在医学图像和文本数据集上，并且在医学图像和文本数据集上进行了交叉和混合预训练。Med-Flamingo实现了几个shot的生成医学视觉问答（VQA）能力，我们评估了这些能力在多个数据集上，包括一个新的开放式VQA数据集，这些数据集包括视频USMLE风格问题。此外，我们进行了第一次的人类评估 для生成医学VQA， Physicians review了问题和潜在的生成，并在交互应用中进行了评估。Med-Flamingo提高了生成医学VQA的性能，提高了临床评估员的评分，并且首次实现了多Modal医学几个shot适应。我们将我们的模型、代码和评估应用发布在https://github.com/snap-stanford/med-flamingo。
</details></li>
</ul>
<hr>
<h2 id="Rotation-Invariant-Random-Features-Provide-a-Strong-Baseline-for-Machine-Learning-on-3D-Point-Clouds"><a href="#Rotation-Invariant-Random-Features-Provide-a-Strong-Baseline-for-Machine-Learning-on-3D-Point-Clouds" class="headerlink" title="Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds"></a>Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06271">http://arxiv.org/abs/2308.06271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meliao/rotation-invariant-random-features">https://github.com/meliao/rotation-invariant-random-features</a></li>
<li>paper_authors: Owen Melia, Eric Jonas, Rebecca Willett</li>
<li>for: 这个论文的目的是研究三维点云数据上的征要学习方法，以实现具有旋转对称性的函数学习。</li>
<li>methods: 这个论文使用了随机特征方法，并对其进行了三维旋转对称性的扩展，以便快速评估点云数据上的函数。</li>
<li>results: 实验表明，这个方法可以与通用的旋转不变深度神经网络相比或超越其性能，并且具有许多任务的通用性和快速评估特点。<details>
<summary>Abstract</summary>
Rotational invariance is a popular inductive bias used by many fields in machine learning, such as computer vision and machine learning for quantum chemistry. Rotation-invariant machine learning methods set the state of the art for many tasks, including molecular property prediction and 3D shape classification. These methods generally either rely on task-specific rotation-invariant features, or they use general-purpose deep neural networks which are complicated to design and train. However, it is unclear whether the success of these methods is primarily due to the rotation invariance or the deep neural networks. To address this question, we suggest a simple and general-purpose method for learning rotation-invariant functions of three-dimensional point cloud data using a random features approach. Specifically, we extend the random features method of Rahimi & Recht 2007 by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point cloud data. We show through experiments that our method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. We also show that our method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task. Finally, we show that our method has an order of magnitude smaller prediction latency than competing kernel methods.
</details>
<details>
<summary>摘要</summary>
rotational invariance 是机器学习中广泛使用的一种印度预测，如计算机视觉和量子化学机器学习。无需旋转的机器学习方法已经设置了许多任务的州OF-the-art，包括分子性质预测和3D形状分类。这些方法通常 Either rely on task-specific rotation-invariant features or use general-purpose deep neural networks, which are complicated to design and train. However, it is unclear whether the success of these methods is primarily due to the rotation invariance or the deep neural networks. To address this question, we propose a simple and general-purpose method for learning rotation-invariant functions of three-dimensional point cloud data using a random features approach. Specifically, we extend the random features method of Rahimi & Recht 2007 by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point cloud data. We show through experiments that our method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. We also show that our method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task. Finally, we show that our method has an order of magnitude smaller prediction latency than competing kernel methods.
</details></li>
</ul>
<hr>
<h2 id="RCT-Rejection-Sampling-for-Causal-Estimation-Evaluation"><a href="#RCT-Rejection-Sampling-for-Causal-Estimation-Evaluation" class="headerlink" title="RCT Rejection Sampling for Causal Estimation Evaluation"></a>RCT Rejection Sampling for Causal Estimation Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15176">http://arxiv.org/abs/2307.15176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kakeith/rct_rejection_sampling">https://github.com/kakeith/rct_rejection_sampling</a></li>
<li>paper_authors: Katherine A. Keith, Sergey Feldman, David Jurgens, Jonathan Bragg, Rohit Bhattacharya</li>
<li>for: 这个论文旨在提高对 observational data 中 causal effect 的估计，并解决高维 covariate 的干扰问题。</li>
<li>methods: 该论文提出了一种基于机器学习方法的 adjustment 方法，用于解决 causal estimation 中的干扰问题。 authors 还提出了一种新的抽样算法，称为 RCT rejection sampling，并提供了理论保证， garanting causal identification 在 observational data 中。</li>
<li>results: 通过使用 simulate data， authors 证明了其算法在 oracle estimators 上的低偏度性。 In addition, authors 还 highlighted 一些 finite data 考虑因素，以便在实际应用中使用 RCT rejection sampling。 as a proof of concept, authors 实现了一个 example evaluation pipeline, 并详细介绍了这些 finite data 考虑因素。<details>
<summary>Abstract</summary>
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -- researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm indeed results in low bias when oracle estimators are evaluated on the confounded samples, which is not always the case for a previously proposed algorithm. In addition to this identification result, we highlight several finite data considerations for evaluation designers who plan to use RCT rejection sampling on their own datasets. As a proof of concept, we implement an example evaluation pipeline and walk through these finite data considerations with a novel, real-world RCT -- which we release publicly -- consisting of approximately 70k observations and text data as high-dimensional covariates. Together, these contributions build towards a broader agenda of improved empirical evaluation for causal estimation.
</details>
<details>
<summary>摘要</summary>
干扰是观察数据中 causal 效应的重要障碍。在高维 covariate 的设置下（如文本数据、 genomics 或行为社会科学），研究人员已经提出了适应机器学习方法以便 causal 估计的调整方法。然而，实际评估这些调整方法的困难和有限。在这项工作中，我们基于一种有前途的评估策略，即使用 randomized controlled trials (RCTs) 的平均 causal 效应作为真实参照值，并提出了一种新的抽样算法，称为 RCT 拒绝抽样。我们提供了理论保证，表明在观察数据中， causal 标识是可行的，从而允许有效地与参照值 RCT 进行比较。使用 sintetic 数据，我们证明了我们的算法在 oracle 估计器中的低偏误。此外，我们还提出了一些实际评估设计师应该考虑的有限数据问题。作为证明，我们实现了一个示例评估管道，并详细介绍了这些有限数据问题。作为证明，我们发布了一个新的、实际存在的 RCT，包含约 70k 个观察和文本数据作为高维 covariate。总的来说，这些贡献共同推动了观察数据中 causal 估计的有效评估。
</details></li>
</ul>
<hr>
<h2 id="VISU-at-WASSA-2023-Shared-Task-Detecting-Emotions-in-Reaction-to-News-Stories-Leveraging-BERT-and-Stacked-Embeddings"><a href="#VISU-at-WASSA-2023-Shared-Task-Detecting-Emotions-in-Reaction-to-News-Stories-Leveraging-BERT-and-Stacked-Embeddings" class="headerlink" title="VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings"></a>VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15164">http://arxiv.org/abs/2307.15164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Kumar, Sushmita Singh, Prayag Tiwari</li>
<li>for: 这篇论文是为了探讨情感识别FROM essays written in reaction to news articles的问题而写的。</li>
<li>methods: 这篇论文使用了深度学习（DL）模型，将词嵌入表示与特化的预处理策略相结合，以捕捉表达的情感细节。实验使用了静止和上下文嵌入（个体和堆叠），以及BIaLSTM和Transformer基本模型。</li>
<li>results: 这篇论文在WASSA 2023 Shared Task（3）中的情感识别任务中获得了rank十的成绩，即Macro F1-Score为0.2717，证明了我们实施的方法在小型和不均衡数据集中的效果。<details>
<summary>Abstract</summary>
Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion Classification from essays written in reaction to news articles. Emotion detection from complex dialogues is challenging and often requires context/domain understanding. Therefore in this research, we have focused on developing deep learning (DL) models using the combination of word embedding representations with tailored prepossessing strategies to capture the nuances of emotions expressed. Our experiments used static and contextual embeddings (individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and Transformer based models. We occupied rank tenth in the emotion detection task by scoring a Macro F1-Score of 0.2717, validating the efficacy of our implemented approaches for small and imbalanced datasets with mixed categories of target emotions.
</details>
<details>
<summary>摘要</summary>
我们的系统，VISU，参加了2023年WASSA分享任务（3）的情感分类从新闻文章中的反应文章。情感检测从复杂对话中是挑战，因此在这项研究中，我们将重点发展深度学习（DL）模型，使用词嵌入表示和特制预处理策略来捕捉表达出的情感含义。我们的实验使用静态和上下文嵌入（个体和堆叠）以及双向长短Memory（BiLSTM）和转换器基于模型。我们在情感检测任务中占据了排名第十的位置，取得了macro F1分数0.2717，证明我们实施的方法对小数据集和混合类目标情感任务具有效果。
</details></li>
</ul>
<hr>
<h2 id="Distilled-Feature-Fields-Enable-Few-Shot-Language-Guided-Manipulation"><a href="#Distilled-Feature-Fields-Enable-Few-Shot-Language-Guided-Manipulation" class="headerlink" title="Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation"></a>Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07931">http://arxiv.org/abs/2308.07931</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, Phillip Isola</li>
<li>for:  bridges the 2D-to-3D gap for robotic manipulation</li>
<li>methods:  leverages distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models</li>
<li>results:  achieves in-the-wild generalization to unseen objects using few-shot learning method for 6-DOF grasping and placing<details>
<summary>Abstract</summary>
Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
</details>
<details>
<summary>摘要</summary>
自我监督和语言监督的图像模型含有重要的世界知识，这对总化非常重要。然而，许多 робоaxi tasks需要精准的三维几何理解，而图像特征通常缺乏这种知识。这个工作将两个维度之间的 gap  bridged ，使用精炼的特征场来结合准确的三维几何和丰富的语言特征，以实现在野外进行6个自由度抓取和放置的几何学掌握。使用从视觉语言模型CLIP中提取出的特征，我们提出了一种通过自然语言文本来指定新的物体 для操作的方法，并证明其能够通过未经见过的表达和新类别的物体进行总化。
</details></li>
</ul>
<hr>
<h2 id="Matching-Patients-to-Clinical-Trials-with-Large-Language-Models"><a href="#Matching-Patients-to-Clinical-Trials-with-Large-Language-Models" class="headerlink" title="Matching Patients to Clinical Trials with Large Language Models"></a>Matching Patients to Clinical Trials with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15051">http://arxiv.org/abs/2307.15051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiao Jin, Zifeng Wang, Charalampos S. Floudas, Jimeng Sun, Zhiyong Lu</li>
<li>For: The paper aims to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection, using large language models (LLMs) to predict criterion-level eligibility with detailed explanations.* Methods: The paper introduces TrialGPT, a novel architecture that employs LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes.* Results: The experimental results demonstrate that TrialGPT achieves high criterion-level prediction accuracy with faithful explanations, and the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. The scores are also effective in ranking clinical trials and excluding ineligible candidates, but the paper acknowledges that current LLMs still make some mistakes due to limited medical knowledge and domain-specific context understanding.<details>
<summary>Abstract</summary>
Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scores prove effective in ranking clinical trials and exclude ineligible candidates. Our error analysis suggests that current LLMs still make some mistakes due to limited medical knowledge and domain-specific context understanding. Nonetheless, we believe the explanatory capabilities of LLMs are highly valuable. Future research is warranted on how such AI assistants can be integrated into the routine trial matching workflow in real-world settings to improve its efficiency.
</details>
<details>
<summary>摘要</summary>
临床试验是药物开发和基于证据的医学发展的关键，但患者招募困难往往阻碍其成功。在这项工作中，我们调查了大语言模型（LLM）在帮助个人患者和推荐医生选择适合的临床试验中的潜在作用。我们介绍了一种新的建筑方案，称为TrialGPT，它使用LLM来预测临床试验权威性的详细解释，然后将这些解释聚合为排名和排除不适的临床试验。我们在三个公共可用的群组中进行了184名患者和18238个临床试验的评估。实验结果表明了以下几点：首先，TrialGPT在权威性预测中达到了高精度和详细的解释。其次，聚合的临床试验级TrialGPT分数与专家证明的可参与性注释高度相关。最后，这些分数能够有效地排名临床试验和排除不适的参与者。我们的错误分析表明，当前的LLM仍然由于医学知识和域pecific上下文理解有一些错误。然而，我们认为LLM的解释能力很值得。未来的研究应该关注如何在实际试验匹配过程中集成这些AI助手，以提高其效率。
</details></li>
</ul>
<hr>
<h2 id="Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models"><a href="#Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models" class="headerlink" title="Universal and Transferable Adversarial Attacks on Aligned Language Models"></a>Universal and Transferable Adversarial Attacks on Aligned Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15043">http://arxiv.org/abs/2307.15043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llm-attacks/llm-attacks">https://github.com/llm-attacks/llm-attacks</a></li>
<li>paper_authors: Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson<br>for: 这个论文的目的是提出一种简单而有效的攻击方法，使得已经被调整的语言模型产生不适的行为。methods: 这个论文使用的方法包括批处理和梯度下降搜索技术，自动生成攻击 suffix。results: 这个论文的实验结果表明，使用这种攻击方法可以让已经被调整的语言模型产生不适的行为，并且这种攻击方法可以在黑盒模型上也有效。<details>
<summary>Abstract</summary>
Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.
</details>
<details>
<summary>摘要</summary>
因为"out-of-the-box"大语言模型可以生成很多不适的内容，因此最近的工作都在尝试对这些模型进行对齐，以避免不适的生成。虽然有一些成功的尝试（即“监狱”对LLMs），但这些攻击需要人类的创造力，并且在实践中较脆弱。在这篇论文中，我们提出了一种简单而有效的攻击方法，使得对齐的语言模型生成不适的行为。具体来说，我们的方法找到一个适用于各种查询的 suffix，以使模型生成有利的答案（而不是拒绝回答）。而不是人工工程，我们的方法通过滥览和梯度基于搜索技术自动生成这些反对性词组，并且超过了过去的自动提示生成方法。 surprisingly，我们发现了这些反对性词组在黑盒、公共释放的LLMs中也是可转移的。我们在多个提问（即请求多种不适的内容）和多个模型（我们的案例是Vicuna-7B和13B）上训练了攻击 suffix，并且在ChatGPT、Bard和Claude等公共接口上也能够引起不适的内容。总的来说，这项工作提高了对齐语言模型的反对攻击的状态艺术，提出了如何避免这些系统生成不适的信息的问题。代码可以在github.com/llm-attacks/llm-attacks中找到。
</details></li>
</ul>
<hr>
<h2 id="AI-Literature-Review-Suite"><a href="#AI-Literature-Review-Suite" class="headerlink" title="AI Literature Review Suite"></a>AI Literature Review Suite</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02443">http://arxiv.org/abs/2308.02443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/datovar4/ai_literature_review_suite">https://github.com/datovar4/ai_literature_review_suite</a></li>
<li>paper_authors: David A. Tovar</li>
<li>for:  automate and optimize the process of literature review in academic and industrial research</li>
<li>methods:  leverages open access science, large language models (LLMs), natural language processing, semantic search queries, text embeddings, and summarization</li>
<li>results:  provides a comprehensive literature review, enables searching, downloading, and organizing of PDF files, and extracts content from articles with succinct summaries<details>
<summary>Abstract</summary>
The process of conducting literature reviews is often time-consuming and labor-intensive. To streamline this process, I present an AI Literature Review Suite that integrates several functionalities to provide a comprehensive literature review. This tool leverages the power of open access science, large language models (LLMs) and natural language processing to enable the searching, downloading, and organizing of PDF files, as well as extracting content from articles. Semantic search queries are used for data retrieval, while text embeddings and summarization using LLMs present succinct literature reviews. Interaction with PDFs is enhanced through a user-friendly graphical user interface (GUI). The suite also features integrated programs for bibliographic organization, interaction and query, and literature review summaries. This tool presents a robust solution to automate and optimize the process of literature review in academic and industrial research.
</details>
<details>
<summary>摘要</summary>
Literature reviews 通常是时间和劳动密集的过程。为了减少这个过程的复杂性，我们提出了一个基于人工智能的文献评估套件（AI Literature Review Suite），该套件集成了多种功能以提供全面的文献评估。这个工具利用了开放科学、大语言模型（LLM）和自然语言处理技术来实现PDF文档的搜索、下载和组织，以及文章中的内容抽取。使用semantic search queries进行数据检索，并使用文本嵌入和摘要使用LLM来提供简洁的文献评估。用户可以通过用户友好的图形用户界面（GUI）进行交互，并且套件还包括了一个集成的bibliographic组织、交互和查询程序，以及文献评估摘要。这个工具为学术和工业研究中的文献评估带来了一个强大的自动化和优化解决方案。
</details></li>
</ul>
<hr>
<h2 id="SuperCLUE-A-Comprehensive-Chinese-Large-Language-Model-Benchmark"><a href="#SuperCLUE-A-Comprehensive-Chinese-Large-Language-Model-Benchmark" class="headerlink" title="SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark"></a>SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15020">http://arxiv.org/abs/2307.15020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Xu, Anqi Li, Lei Zhu, Hang Xue, Changtai Zhu, Kangkang Zhao, Haonan He, Xuanwei Zhang, Qiyue Kang, Zhenzhong Lan</li>
<li>for: 这 paper 的目的是为了评估大语言模型在实际应用中的性能，而不是仅仅是测试其精度。</li>
<li>methods: 这 paper 使用了一个全面的中文测试套件 SuperCLUE，包括 CArena、OPEN 和 CLOSE 三个子任务。</li>
<li>results: 这 paper 的研究结果表明，关闭式问题的答案准确率不足以反映人类的偏好，但是它们可以补充对话来预测实际用户的偏好。此外，GPT-4 可以自动评估中文语言模型在开放式问题上的人类偏好。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown the potential to be integrated into human daily lives. Therefore, user preference is the most critical criterion for assessing LLMs' performance in real-world scenarios. However, existing benchmarks mainly focus on measuring models' accuracy using multi-choice questions, which limits the understanding of their capabilities in real applications. We fill this gap by proposing a comprehensive Chinese benchmark SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE encompasses three sub-tasks: actual users' queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE). Our study shows that accuracy on closed-ended questions is insufficient to reflect human preferences achieved on open-ended ones. At the same time, they can complement each other to predict actual user preferences. We also demonstrate that GPT-4 is a reliable judge to automatically evaluate human preferences on open-ended questions in a Chinese context. Our benchmark will be released at https://www.CLUEbenchmarks.com
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经展示了在人类日常生活中的潜在应用前景。因此，用户偏好成为评估LLM在实际应用场景中的表现的关键因素。然而，现有的标准约束主要是通过多选问题来衡量模型的准确率，这限制了我们对其在实际应用中的能力的理解。我们填补了这一漏洞，提出了一个全面的中文标准准备SuperCLUE，名称来自另一个流行的中文LLM标准准备CLUE。SuperCLUE包括三个子任务：实际用户的问题和评分来自LLM战场平台（CArena），开放式问题（OPEN）和关闭式问题（CLOSE）。我们的研究显示，关闭式问题的准确率不充分反映人类偏好，而且可以补充each other来预测实际用户的偏好。此外，我们还证明了GPT-4可以自动评估中文上开放式问题的人类偏好。我们的标准将在https://www.CLUEbenchmarks.com上发布。
</details></li>
</ul>
<hr>
<h2 id="How-Good-is-Google-Bard’s-Visual-Understanding-An-Empirical-Study-on-Open-Challenges"><a href="#How-Good-is-Google-Bard’s-Visual-Understanding-An-Empirical-Study-on-Open-Challenges" class="headerlink" title="How Good is Google Bard’s Visual Understanding? An Empirical Study on Open Challenges"></a>How Good is Google Bard’s Visual Understanding? An Empirical Study on Open Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15016">http://arxiv.org/abs/2307.15016</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htqin/googlebard-visunderstand">https://github.com/htqin/googlebard-visunderstand</a></li>
<li>paper_authors: Haotong Qin, Ge-Peng Ji, Salman Khan, Deng-Ping Fan, Fahad Shahbaz Khan, Luc Van Gool</li>
<li>for: This paper explores the ability of Google Bard to understand and interpret visual data (images) conditioned by text questions, with the goal of evaluating its performance in various task scenarios and identifying areas for improvement.</li>
<li>methods: The paper uses Google Bard to process text and image inputs and evaluate its performance in 15 diverse task scenarios, including regular, camouflaged, medical, under-water, and remote sensing data.</li>
<li>results: The primary finding of the study is that Bard struggles in vision scenarios, highlighting the significant gap in vision-based understanding that needs to be bridged in future developments. The study provides valuable insights for advancing future models and improving their capabilities in comprehending and interpreting fine-grained visual data.Here is the same information in Simplified Chinese:</li>
<li>for: 这个研究用Google Bard来处理文本和图像输入，以评估其在不同任务场景中的表现，并找到改进的方向。</li>
<li>methods: 这篇论文使用Google Bard处理文本和图像输入，并在15种多样化任务场景中评估其表现，包括常见、掩蔽、医疗、水下和Remote感知数据。</li>
<li>results: 研究发现，Bard在视觉场景中表现不佳，这显示了未来模型需要覆盖视觉理解的巨大差距。这项实验带来了价值的发现，可以帮助未来的模型在细致的视觉数据上增强其能力。<details>
<summary>Abstract</summary>
Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs to be bridged in future developments. We expect that this empirical study will prove valuable in advancing future models, leading to enhanced capabilities in comprehending and interpreting fine-grained visual data. Our project is released on https://github.com/htqin/GoogleBard-VisUnderstand
</details>
<details>
<summary>摘要</summary>
Google的Bard在协作AI领域已经成为OpenAI的ChatGPT的强有力竞争对手。特别是最近Bard更新以处理图像和文本提示的对话。由于Bard在文本输入方面的卓越表现，我们探索了它在理解和解释图像数据（图像）的能力。这种探索具有探索新的发现和挑战，特别是在解决复杂计算机视觉问题方面。在这项研究中，我们选择了15种多样化任务场景，包括常见、掩体、医疗、水下和远程感知数据，以全面评估Bard的性能。我们的主要发现表明Bard在视觉场景中仍然努力，反映了需要在未来发展中覆盖视觉基础知识的巨大差距。我们预计这项实验性研究会对未来模型的发展产生重要影响，导致它们在理解和解释细致的视觉数据方面增强其能力。我们的项目在https://github.com/htqin/GoogleBard-VisUnderstand上发布。
</details></li>
</ul>
<hr>
<h2 id="Improved-Neural-Radiance-Fields-Using-Pseudo-depth-and-Fusion"><a href="#Improved-Neural-Radiance-Fields-Using-Pseudo-depth-and-Fusion" class="headerlink" title="Improved Neural Radiance Fields Using Pseudo-depth and Fusion"></a>Improved Neural Radiance Fields Using Pseudo-depth and Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03772">http://arxiv.org/abs/2308.03772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingliang Li, Qiang Zhou, Chaohui Yu, Zhengda Lu, Jun Xiao, Zhibin Wang, Fan Wang</li>
<li>for: 本研究旨在提高Neural Radiance Fields（NeRF）模型的渲染精度和视角渲染能力，特别是在实际场景中存在多种大小对象&#x2F;结构的情况下。</li>
<li>methods: 我们提出了一种使用多尺度编码量表示 scene中对象的几何信息，并将其提供给NeRF模型。我们还提出了同时进行深度预测和场景重建，以使得构造的量表示更加准确。此外，我们还提出了基于深度导航的点云特征协同拼接，以提高点云特征的准确性。</li>
<li>results: 我们的方法在novel view synthesis和dense geometry modeling中表现出了superior的性能，无需Scene-specific优化。<details>
<summary>Abstract</summary>
Since the advent of Neural Radiance Fields, novel view synthesis has received tremendous attention. The existing approach for the generalization of radiance field reconstruction primarily constructs an encoding volume from nearby source images as additional inputs. However, these approaches cannot efficiently encode the geometric information of real scenes with various scale objects/structures. In this work, we propose constructing multi-scale encoding volumes and providing multi-scale geometry information to NeRF models. To make the constructed volumes as close as possible to the surfaces of objects in the scene and the rendered depth more accurate, we propose to perform depth prediction and radiance field reconstruction simultaneously. The predicted depth map will be used to supervise the rendered depth, narrow the depth range, and guide points sampling. Finally, the geometric information contained in point volume features may be inaccurate due to occlusion, lighting, etc. To this end, we propose enhancing the point volume feature from depth-guided neighbor feature fusion. Experiments demonstrate the superior performance of our method in both novel view synthesis and dense geometry modeling without per-scene optimization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Thinker-Learning-to-Plan-and-Act"><a href="#Thinker-Learning-to-Plan-and-Act" class="headerlink" title="Thinker: Learning to Plan and Act"></a>Thinker: Learning to Plan and Act</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14993">http://arxiv.org/abs/2307.14993</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonymous-scrl/thinker">https://github.com/anonymous-scrl/thinker</a></li>
<li>paper_authors: Stephen Chung, Ivan Anokhin, David Krueger</li>
<li>for: 这个论文的目的是开发一种新的推奖学习算法，帮助推奖学习代理人自主地使用学习到的世界模型进行规划。</li>
<li>methods: 这个算法使用了包装环境在世界模型中的方法，并提出了一些特定的模型交互动作，让代理人可以通过对世界模型进行规划，选择更好的行动。</li>
<li>results: 经验结果表明，这个算法在扮演游戏和Atari 2600测试中都达到了状态之 искусственный智能表现的最佳效果和竞争性表现。代理人训练后的视觉化表示它们已经学习了如何有效地规划使用世界模型选择更好的行动。<details>
<summary>Abstract</summary>
We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. The algorithm's generality opens a new research direction on how a world model can be used in reinforcement learning and how planning can be seamlessly integrated into an agent's decision-making process.
</details>
<details>
<summary>摘要</summary>
我们提出了思考算法（Thinker algorithm），一种新的方法，让强化学习代理人能够自主地与学习的世界模型互动。思考算法将环境包装在世界模型中，并增加了特定 для与世界模型互动的动作。这些模型互动动作使代理人能够在世界模型中进行规划，提出不同的计划供世界模型评估，然后选择最佳的行动进行环境中执行。这种方法扩展了强化学习的可能性，并且让代理人能够自主地学习规划技巧，同时亦可以轻松地将其计划视觉化。我们透过实验结果在拓扑游戏和Atari 2600测试中证明了思考算法的有效性，并且在这两个测试中获得了竞争性的结果。代理人训练了思考算法后的视觉化结果表明，它们已经学会了从世界模型中选择更好的动作。这个算法的通用性开启了一个新的研究方向，即如何在强化学习中使用世界模型，以及如何将规划自透地整合到代理人的决策过程中。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Code-Co-Evolution-Using-Large-Language-Models"><a href="#Multilingual-Code-Co-Evolution-Using-Large-Language-Models" class="headerlink" title="Multilingual Code Co-Evolution Using Large Language Models"></a>Multilingual Code Co-Evolution Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14991">http://arxiv.org/abs/2307.14991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiyang Zhang, Pengyu Nie, Junyi Jessy Li, Milos Gligoric</li>
<li>for: 本研究旨在解决跨编程语言代码更新的问题，通过大语言模型（LLMs）来实现代码更新。</li>
<li>methods: 本研究使用大语言模型（LLMs）来模型代码更新为编辑序列，并学习代码更新之间的相关性。</li>
<li>results: 对于6,613个对齐的代码更新样本，codeditor与状态之前的方法相比，得到了大幅度的提高。此外，codeditor与现有的生成型模型相结合，能够实现更高的性能。<details>
<summary>Abstract</summary>
Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#). Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics. Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.
</details>
<details>
<summary>摘要</summary>
许多软件项目会实现API和算法在多种编程语言中。维护这些项目是疲劳的，因为开发者必须确保任何更改（例如，bug fix或新功能）在其他编程语言中得到有效地传播，并且不会出现错误。在软件世界中，使用规则基于的翻译工具（例如，转换器）或机器学习模型来翻译代码从一种语言到另一种语言提供有限的价值。每次翻译整个代码库从一种语言到另一种语言都不是开发者的工作方式。在这篇论文中，我们target一个新任务：将代码更改从一种编程语言到另一种编程语言使用大语言模型（LLM）进行翻译。我们设计并实现了第一个LLM，名为Codeditor，以解决这个任务。Codeditor显式模型代码更改为编辑序列，并学习代码更改之间的相互关系。为了评估Codeditor，我们收集了6,613个对齐的代码更改从8对开源软件项目中，这些项目在两种编程语言（Java和C#）中实现了相同的功能。结果表明，Codeditor在所有常用的自动指标上都高于现有的状态艺术方法。我们的工作还发现，Codeditor与现有的生成基于模型相结合，可以确保更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Take-A-Photo-3D-to-2D-Generative-Pre-training-of-Point-Cloud-Models"><a href="#Take-A-Photo-3D-to-2D-Generative-Pre-training-of-Point-Cloud-Models" class="headerlink" title="Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models"></a>Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14971">http://arxiv.org/abs/2307.14971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangzy22/tap">https://github.com/wangzy22/tap</a></li>
<li>paper_authors: Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, Jiwen Lu</li>
<li>for: 提高3D视觉模型的性能</li>
<li>methods: 使用交叉注意机制生成视图图像作为预训练方案</li>
<li>results: 超过前一代预训练方法的表现，并且可以提高建筑型approaches的性能<details>
<summary>Abstract</summary>
With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boosting the performance of architecture-oriented approaches, achieving state-of-the-art performance when fine-tuning on ScanObjectNN classification and ShapeNetPart segmentation tasks. Code is available at https://github.com/wangzy22/TAP.
</details>
<details>
<summary>摘要</summary>
在MAE领导的面孔图模型化潮流中，生成预训练显示了强大的可能性，以提高2D视觉基本模型的性能。然而，在3D视觉中，基于Transformer的幕后和点云的顺序性限制了生成预训练的进一步发展。在这篇论文中，我们提出了一种适用于任意点云模型的3D-to-2D生成预训练方法。我们提议通过交叉注意机制来生成不同指导姿态的视图图像作为预训练方案。生成视图图像的精确超级vision比点云对应的点云更加精准，因此帮助3D背部更好地理解点云的几何结构和立体关系。实验结果表明了我们提议的3D-to-2D生成预训练的优越性，并在ScanObjectNN分类和ShapeNetPart segmentation任务上实现了最佳性能。代码可以在https://github.com/wangzy22/TAP上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/28/cs.AI_2023_07_28/" data-id="clpahu6xo001f3h886js1hvv8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/cs.CL_2023_07_28/" class="article-date">
  <time datetime="2023-07-28T11:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/28/cs.CL_2023_07_28/">cs.CL - 2023-07-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Robust-Distortion-free-Watermarks-for-Language-Models"><a href="#Robust-Distortion-free-Watermarks-for-Language-Models" class="headerlink" title="Robust Distortion-free Watermarks for Language Models"></a>Robust Distortion-free Watermarks for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15593">http://arxiv.org/abs/2307.15593</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jthickstun/watermark">https://github.com/jthickstun/watermark</a></li>
<li>paper_authors: Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, Percy Liang</li>
<li>for: 这个论文目的是为了植入水印到文本中，以防止不当使用文本生成模型。</li>
<li>methods: 这个论文使用了一种基于杂素分布的水印方法，通过将随机数据序列映射到语言模型中生成的文本中，以达到水印的目的。</li>
<li>results: 实验结果表明，这种水印方法可以在不改变文本的分布下，对文本进行植入水印，并且可以在不同的攻击方式下保持水印的可读性。具体来说，对于OPT-1.3B和LLaMA-7B模型，可以在40%-50%的杂素替换、插入和删除攻击下，在35个字符的文本中仍可以可靠地检测水印（p&lt;&#x3D;0.01）。对于Alpaca-7B模型，由于响应的 entropy 较低，检测是更加困难的，但仍可以在25%的响应中检测到水印（p&lt;&#x3D;0.01）。<details>
<summary>Abstract</summary>
We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around $25\%$ of the responses -- whose median length is around $100$ tokens -- are detectable with $p \leq 0.01$, and the watermark is also less robust to certain automated paraphrasing attacks we implement.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于在文本中植入抗干扰的水印，不会改变文本的分布，直到最大生成预算为止。我们生成水印文本，通过将一个序列Random numbers（我们使用随机水印密钥计算）映射到语言模型的样本。为检测水印文本，任何知道密钥的人可以将文本与Random number序列进行对齐。我们实现了我们的水印方法，使用两种采样方案：反转采样和最小值采样。我们在三个语言模型（OPT-1.3B、LLaMA-7B和Alpaca-7B）上实验 validate its statistical power and robustness to various paraphrasing attacks。我们发现，对OPT-1.3B和LLaMA-7B模型，我们可以在35个字符之前（也就是说，在40-50%的字符被随机编辑后）检测水印文本（p ≤ 0.01）。对Alpaca-7B模型，我们进行了一项研究，探讨是否可以在用户指令的回答中植入水印。由于回答的低 entropy，检测变得更加困难：约25%的回答（ median length around 100 tokens）可以在p ≤ 0.01的情况下检测，而水印也更易受到一些自动生成的修改攻击。
</details></li>
</ul>
<hr>
<h2 id="When-to-generate-hedges-in-peer-tutoring-interactions"><a href="#When-to-generate-hedges-in-peer-tutoring-interactions" class="headerlink" title="When to generate hedges in peer-tutoring interactions"></a>When to generate hedges in peer-tutoring interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15582">http://arxiv.org/abs/2307.15582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuromaancer/hedge_prediction">https://github.com/neuromaancer/hedge_prediction</a></li>
<li>paper_authors: Alafate Abulimiti, Chloé Clavel, Justine Cassell</li>
<li>for: 这篇论文探讨了机器学习技术在辅导互动中预测幂值的应用。</li>
<li>methods: 这篇研究使用自然的面对面数据集，并对自然语言转折、对话策略、辅导策略和非语言表达进行标注。这些元素被转换为上一句话的向量表示，并作为机器学习模型的输入。</li>
<li>results: 研究发现，使用嵌入层（捕捉上一句话的语义信息）可以显著提高模型的性能。此外，研究还提供了关于不同特征（如人际关系和非语言表达）在预测幂值方面的重要性的视觉值解释。研究发现，辅导和学生双方的视线强烈关系到幂值预测。这一观察得到了验证通过后续减少研究。<details>
<summary>Abstract</summary>
This paper explores the application of machine learning techniques to predict where hedging occurs in peer-tutoring interactions. The study uses a naturalistic face-to-face dataset annotated for natural language turns, conversational strategies, tutoring strategies, and nonverbal behaviours. These elements are processed into a vector representation of the previous turns, which serves as input to several machine learning models. Results show that embedding layers, that capture the semantic information of the previous turns, significantly improves the model's performance. Additionally, the study provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviours, in predicting hedges by using Shapley values for feature explanation. We discover that the eye gaze of both the tutor and the tutee has a significant impact on hedge prediction. We further validate this observation through a follow-up ablation study.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="All-for-One-and-One-For-All-Deep-learning-based-feature-fusion-for-Synthetic-Speech-Detection"><a href="#All-for-One-and-One-For-All-Deep-learning-based-feature-fusion-for-Synthetic-Speech-Detection" class="headerlink" title="All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection"></a>All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15555">http://arxiv.org/abs/2307.15555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Mari, Davide Salvi, Paolo Bestagini, Simone Milani</li>
<li>for: 防止声音深冒险攻击和身份盗窃	+ The paper is written to address the issue of synthetic speech detection in order to prevent frauds and identity thefts.</li>
<li>methods: 结合了三种文献中提出的特征集	+ The paper uses a fusion of three different feature sets proposed in the literature to improve the performance of synthetic speech detection.</li>
<li>results: 在不同的场景和数据集上实现了更好的总体性能	+ The paper presents a model that fuses the three feature sets and achieves better overall performance compared to state-of-the-art solutions, with robustness to anti-forensic attacks and generalization capabilities.Here is the information in Simplified Chinese text:</li>
<li>for: 防止声音深冒险攻击和身份盗窃</li>
<li>methods: 结合了三种文献中提出的特征集</li>
<li>results: 在不同的场景和数据集上实现了更好的总体性能<details>
<summary>Abstract</summary>
Recent advances in deep learning and computer vision have made the synthesis and counterfeiting of multimedia content more accessible than ever, leading to possible threats and dangers from malicious users. In the audio field, we are witnessing the growth of speech deepfake generation techniques, which solicit the development of synthetic speech detection algorithms to counter possible mischievous uses such as frauds or identity thefts. In this paper, we consider three different feature sets proposed in the literature for the synthetic speech detection task and present a model that fuses them, achieving overall better performances with respect to the state-of-the-art solutions. The system was tested on different scenarios and datasets to prove its robustness to anti-forensic attacks and its generalization capabilities.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)最近的深度学习和计算机视觉技术的进步，使得 multimedia 内容的合成和伪造变得更加容易，可能导致来自黑客的威胁和危险。在音频领域，我们目睹到深度语音生成技术的快速发展，这使得对于可能的欺诈或身份盗用而需要开发深层 speech 检测算法。在这篇论文中，我们考虑了 literature 中提出的三种不同的特征集，并提出一种将其融合的模型，实现了与当前的解决方案相比的更好的性能。系统在不同的场景和数据集上进行了测试，以证明其对抗反科学攻击和泛化能力的Robustness。
</details></li>
</ul>
<hr>
<h2 id="‘What-are-you-referring-to-’-Evaluating-the-Ability-of-Multi-Modal-Dialogue-Models-to-Process-Clarificational-Exchanges"><a href="#‘What-are-you-referring-to-’-Evaluating-the-Ability-of-Multi-Modal-Dialogue-Models-to-Process-Clarificational-Exchanges" class="headerlink" title="‘What are you referring to?’ Evaluating the Ability of Multi-Modal Dialogue Models to Process Clarificational Exchanges"></a>‘What are you referring to?’ Evaluating the Ability of Multi-Modal Dialogue Models to Process Clarificational Exchanges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15554">http://arxiv.org/abs/2307.15554</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jchiyah/what-are-you-referring-to">https://github.com/jchiyah/what-are-you-referring-to</a></li>
<li>paper_authors: Javier Chiyah-Garcia, Alessandro Suglia, Arash Eshghi, Helen Hastie</li>
<li>For: 这篇论文主要针对对话中的referential ambiguity问题，即当引用表达不唯一确定所指的对象时，谈话中的冲突和修复机制。* Methods: 该论文使用SIMMC 2.0数据集来评估不同状态艺术模型对Clarificational Exchanges (CE)的处理能力，包括对对话历史相关的CE进行处理。* Results: 研究发现，语言基于模型可以编码多Modal semantic information，并处理一些CE；而多Modal模型可以通过额外学习目标获得分离的对象表示，这在处理多modal referential ambiguity中发挥了关键作用。<details>
<summary>Abstract</summary>
Referential ambiguities arise in dialogue when a referring expression does not uniquely identify the intended referent for the addressee. Addressees usually detect such ambiguities immediately and work with the speaker to repair it using meta-communicative, Clarificational Exchanges (CE): a Clarification Request (CR) and a response. Here, we argue that the ability to generate and respond to CRs imposes specific constraints on the architecture and objective functions of multi-modal, visually grounded dialogue models. We use the SIMMC 2.0 dataset to evaluate the ability of different state-of-the-art model architectures to process CEs, with a metric that probes the contextual updates that arise from them in the model. We find that language-based models are able to encode simple multi-modal semantic information and process some CEs, excelling with those related to the dialogue history, whilst multi-modal models can use additional learning objectives to obtain disentangled object representations, which become crucial to handle complex referential ambiguities across modalities overall.
</details>
<details>
<summary>摘要</summary>
优先级意图杂化出现在对话中当referring表达不唯一地标识目标对象时。对话参与者通常立即发现这种杂化并与对话者使用meta-communicative, Clarificational Exchanges（CE）进行修复，包括一个Clarification Request（CR）和回应。我们 argue that能生成和回应CE强制要求对多模态、视觉固定对话模型的架构和目标函数做出特定的限制。我们使用SIMMC 2.0数据集来评估不同状态 искусственного智能模型的处理CE能力，并使用一个度量测试模型中的上下文更新。我们发现语言基于模型可以编码简单的多模态Semantic信息并处理一些CE，在对话历史相关的CE方面表现出色，而多模态模型可以通过额外学习目标函数获得分离的对象表示，这些表示在多modal杂化中扮演重要角色。
</details></li>
</ul>
<hr>
<h2 id="Oracle-Computability-and-Turing-Reducibility-in-the-Calculus-of-Inductive-Constructions"><a href="#Oracle-Computability-and-Turing-Reducibility-in-the-Calculus-of-Inductive-Constructions" class="headerlink" title="Oracle Computability and Turing Reducibility in the Calculus of Inductive Constructions"></a>Oracle Computability and Turing Reducibility in the Calculus of Inductive Constructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15543">http://arxiv.org/abs/2307.15543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannick Forster, Dominik Kirst, Niklas Mück</li>
<li>for: 这个论文是为了研究 oracle 计算和图灵下降的概念和关系而写的。</li>
<li>methods: 这篇论文使用了 Calculus of Inductive Constructions (CIC) 和 Coq 证明助手来定义和研究 oracle 计算和图灵下降。 它采用了基于 meta-level 函数的定义方式，而不是基于对象级模型的 computation。</li>
<li>results: 这篇论文得到了以下结果：Turing 下降形成上半semilattice，传输 decidability，并且比 truth-table 下降更加强大表达能力。此外，当 predicate $p$ 和其 complement 都是对 oracle $q$ 的 semi-decidable 时，then $p$ Turing-reduces to $q$.<details>
<summary>Abstract</summary>
We develop synthetic notions of oracle computability and Turing reducibility in the Calculus of Inductive Constructions (CIC), the constructive type theory underlying the Coq proof assistant. As usual in synthetic approaches, we employ a definition of oracle computations based on meta-level functions rather than object-level models of computation, relying on the fact that in constructive systems such as CIC all definable functions are computable by construction. Such an approach lends itself well to machine-checked proofs, which we carry out in Coq.   There is a tension in finding a good synthetic rendering of the higher-order notion of oracle computability. On the one hand, it has to be informative enough to prove central results, ensuring that all notions are faithfully captured. On the other hand, it has to be restricted enough to benefit from axioms for synthetic computability, which usually concern first-order objects. Drawing inspiration from a definition by Andrej Bauer based on continuous functions in the effective topos, we use a notion of sequential continuity to characterise valid oracle computations.   As main technical results, we show that Turing reducibility forms an upper semilattice, transports decidability, and is strictly more expressive than truth-table reducibility, and prove that whenever both a predicate $p$ and its complement are semi-decidable relative to an oracle $q$, then $p$ Turing-reduces to $q$.
</details>
<details>
<summary>摘要</summary>
我们在Calculus of Inductive Constructions（CIC）中发展了一种干预计算和图灵可reducible的概念。与传统的 sintética方法不同，我们使用基于高级函数而不是对象水平模型的计算定义 oracle computations。这种方法适合机器检查证明，我们在Coq中进行了证明。在高级层次上定义干预计算的问题存在一种矛盾。一方面，它必须够精细以证明中心结果，确保所有概念都能够准确地捕捉。另一方面，它必须够简单以便利用 axioms for synthetic computability，这些axioms通常只关注第一级对象。 draw inspiration from Andrej Bauer 基于有效幂论中的连续函数的定义，我们使用sequential continuity来 caracterize valid oracle computations。我们的主要技术结果包括：1. Turing reducibility forms an upper semilattice。2. Turing reducibility transports decidability。3. Turing reducibility is strictly more expressive than truth-table reducibility。4. If both a predicate $p$ and its complement are semi-decidable relative to an oracle $q$, then $p$ Turing-reduces to $q$.注意：以下是简化中文版本，如果需要更加详细的解释，请咨询专业人士。
</details></li>
</ul>
<hr>
<h2 id="The-Road-to-Quality-is-Paved-with-Good-Revisions-A-Detailed-Evaluation-Methodology-for-Revision-Policies-in-Incremental-Sequence-Labelling"><a href="#The-Road-to-Quality-is-Paved-with-Good-Revisions-A-Detailed-Evaluation-Methodology-for-Revision-Policies-in-Incremental-Sequence-Labelling" class="headerlink" title="The Road to Quality is Paved with Good Revisions: A Detailed Evaluation Methodology for Revision Policies in Incremental Sequence Labelling"></a>The Road to Quality is Paved with Good Revisions: A Detailed Evaluation Methodology for Revision Policies in Incremental Sequence Labelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15508">http://arxiv.org/abs/2307.15508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/briemadu/inc-eval-revisions">https://github.com/briemadu/inc-eval-revisions</a></li>
<li>paper_authors: Brielen Madureira, Patrick Kahardipraja, David Schlangen</li>
<li>for: 这篇论文主要是关于增量Sequence Labeling中的编辑和修订策略。</li>
<li>methods: 该论文提出了一种形式化和Characterize edits和修订策略，并对三种基于Transformer的encoder进行了 Profile。</li>
<li>results: 研究发现，这些encoder在不同任务中的增量行为都具有不同的特点，这可以帮助改进修订策略。<details>
<summary>Abstract</summary>
Incremental dialogue model components produce a sequence of output prefixes based on incoming input. Mistakes can occur due to local ambiguities or to wrong hypotheses, making the ability to revise past outputs a desirable property that can be governed by a policy. In this work, we formalise and characterise edits and revisions in incremental sequence labelling and propose metrics to evaluate revision policies. We then apply our methodology to profile the incremental behaviour of three Transformer-based encoders in various tasks, paving the road for better revision policies.
</details>
<details>
<summary>摘要</summary>
转换文本为简化中文：增量对话模型组件生成基于输入的输出前缀序列。由于地方冲突或错误假设，可能会出现错误，因此能够修改过去输出的能力是一个感irable的属性，可以由策略控制。在这项工作中，我们将增量编辑和修订在增量序列标记中进行正式化和特征化，并提出修订策略评价指标。然后，我们将方法应用于三种基于Transformer的编码器在不同任务中的增量行为进行 profiling，为更好的修订策略开出道路。
</details></li>
</ul>
<hr>
<h2 id="The-timing-bottleneck-Why-timing-and-overlap-are-mission-critical-for-conversational-user-interfaces-speech-recognition-and-dialogue-systems"><a href="#The-timing-bottleneck-Why-timing-and-overlap-are-mission-critical-for-conversational-user-interfaces-speech-recognition-and-dialogue-systems" class="headerlink" title="The timing bottleneck: Why timing and overlap are mission-critical for conversational user interfaces, speech recognition and dialogue systems"></a>The timing bottleneck: Why timing and overlap are mission-critical for conversational user interfaces, speech recognition and dialogue systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15493">http://arxiv.org/abs/2307.15493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Liesenfeld, Alianda Lopez, Mark Dingemanse</li>
<li>for: 这些研究是为了评估现有的商业语音识别系统在对话场景中的性能。</li>
<li>methods: 研究者使用了5种主要的商业语音识别系统，对于6种语言的自然对话数据进行了评估。</li>
<li>results: 研究发现，对话数据中的单词错误率仍然很高，而 overlap 问题是对话识别的关键挑战。这些结果有助于评估当前的对话语音识别技术的状态，并且可以帮助建立更加可靠的对话speech技术。<details>
<summary>Abstract</summary>
Speech recognition systems are a key intermediary in voice-driven human-computer interaction. Although speech recognition works well for pristine monologic audio, real-life use cases in open-ended interactive settings still present many challenges. We argue that timing is mission-critical for dialogue systems, and evaluate 5 major commercial ASR systems for their conversational and multilingual support. We find that word error rates for natural conversational data in 6 languages remain abysmal, and that overlap remains a key challenge (study 1). This impacts especially the recognition of conversational words (study 2), and in turn has dire consequences for downstream intent recognition (study 3). Our findings help to evaluate the current state of conversational ASR, contribute towards multidimensional error analysis and evaluation, and identify phenomena that need most attention on the way to build robust interactive speech technologies.
</details>
<details>
<summary>摘要</summary>
speech recognition systems 是人机交互中的关键中间件，尽管speech recognition在纯净的对话中工作得很好，但实际的生活中的开放式交互场景仍然存在许多挑战。我们认为时间是对话系统的关键因素，并评估了5个主要的商业ASR系统的对话和多语言支持。我们发现，在6种自然的对话语言中，word error rate remain extremely high，并且 overlap是关键挑战（研究1）。这继而影响了对话词的识别（研究2），并 ultimately affects downstream intent recognition（研究3）。我们的发现可以评估当前的对话ASR的状态，帮助建立多维度的错误分析和评估，并 indentify需要特别注意的现象，以建立可靠的对话speech技术。
</details></li>
</ul>
<hr>
<h2 id="Cross-Modal-Concept-Learning-and-Inference-for-Vision-Language-Models"><a href="#Cross-Modal-Concept-Learning-and-Inference-for-Vision-Language-Models" class="headerlink" title="Cross-Modal Concept Learning and Inference for Vision-Language Models"></a>Cross-Modal Concept Learning and Inference for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15460">http://arxiv.org/abs/2307.15460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Zhang, Ce Zhang, Yushun Tang, Zhihai He</li>
<li>for: 本研究旨在提高现有 fine-tuning 方法的性能，解决图像中的不同Semantic object和概念之间的关系问题。</li>
<li>methods: 我们提出了一种新的方法，即跨模型概念学习和推理（CCLI），利用 CLIP 强大的文本-图像相关能力，自动学习图像中的大量特征特征，并根据这些特征构建了分类图像的描述表示，并学习一个概念推理网络进行下游图像分类任务。</li>
<li>results: 我们的 CCLI 方法在 few-shot learning 和领域泛化等下游任务中表现出了显著的提升，比如与当前状态艺术法相比，提高了8.0%的性能。<details>
<summary>Abstract</summary>
Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP, establish the correlation between texts and images, achieving remarkable success on various downstream tasks with fine-tuning. In existing fine-tuning methods, the class-specific text description is matched against the whole image. We recognize that this whole image matching is not effective since images from the same class often contain a set of different semantic objects, and an object further consists of a set of semantic parts or concepts. Individual semantic parts or concepts may appear in image samples from different classes. To address this issue, in this paper, we develop a new method called cross-model concept learning and inference (CCLI). Using the powerful text-image correlation capability of CLIP, our method automatically learns a large set of distinctive visual concepts from images using a set of semantic text concepts. Based on these visual concepts, we construct a discriminative representation of images and learn a concept inference network to perform downstream image classification tasks, such as few-shot learning and domain generalization. Extensive experimental results demonstrate that our CCLI method is able to improve the performance upon the current state-of-the-art methods by large margins, for example, by up to 8.0% improvement on few-shot learning and by up to 1.3% for domain generalization.
</details>
<details>
<summary>摘要</summary>
大规模预训练视觉语言模型（VLM），如CLIP，已经建立了文本和图像之间的相关性，在多种下游任务上达到了非常成功的结果。现有的精细调整方法中，通常将类型特定的文本描述与整个图像进行匹配，但我们认为这种整个图像匹配并不有效，因为图像从同一类型的图像中可能包含多个不同的semantic对象，而每个semantic对象都可能包含多个semantic部分或概念。在不同类型的图像中，这些semantic部分或概念可能会出现。为解决这个问题，在这篇论文中，我们开发了一种新的方法 called cross-model concept learning and inference（CCLI）。使用CLIP强大的文本-图像相关能力，我们自动学习了一个大量的特异性视觉概念从图像中，并使用这些视觉概念构建了一个描述性图像表示，并学习一个概念推理网络来进行下游图像分类任务，如少量学习和领域泛化。我们的CCLI方法在多种实验结果中表现出了大幅提升的性能，比如在少量学习任务上提升了8.0%，在领域泛化任务上提升了1.3%。
</details></li>
</ul>
<hr>
<h2 id="Trie-NLG-Trie-Context-Augmentation-to-Improve-Personalized-Query-Auto-Completion-for-Short-and-Unseen-Prefixes"><a href="#Trie-NLG-Trie-Context-Augmentation-to-Improve-Personalized-Query-Auto-Completion-for-Short-and-Unseen-Prefixes" class="headerlink" title="Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes"></a>Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15455">http://arxiv.org/abs/2307.15455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaushal Kumar Maurya, Maunendra Sankar Desarkar, Manish Gupta, Puneet Agrawal</li>
<li>for: 提高 Query Auto-completion（QAC）系统的完ten completion accuracy，尤其是对短前缀和未看到的前缀进行提高。</li>
<li>methods: 提议一种基于 Trie 和 Natural Language Generation（NLG）模型的新方法，可以同时利用前一个会话中的查询和 Trie 中的 популярity 信号来提高 QAC 的性能。</li>
<li>results: 通过对两个大型 QAC 数据集进行评估，发现该方法可以提高 QAC 系统的 MRR 指标的表现，相比之下 Popular Trie-based Lookup 和 BART-based Baseline 方法，平均提高了大约 57% 和 14%。<details>
<summary>Abstract</summary>
Query auto-completion (QAC) aims at suggesting plausible completions for a given query prefix. Traditionally, QAC systems have leveraged tries curated from historical query logs to suggest most popular completions. In this context, there are two specific scenarios that are difficult to handle for any QAC system: short prefixes (which are inherently ambiguous) and unseen prefixes. Recently, personalized Natural Language Generation (NLG) models have been proposed to leverage previous session queries as context for addressing these two challenges. However, such NLG models suffer from two drawbacks: (1) some of the previous session queries could be noisy and irrelevant to the user intent for the current prefix, and (2) NLG models cannot directly incorporate historical query popularity. This motivates us to propose a novel NLG model for QAC, Trie-NLG, which jointly leverages popularity signals from trie and personalization signals from previous session queries. We train the Trie-NLG model by augmenting the prefix with rich context comprising of recent session queries and top trie completions. This simple modeling approach overcomes the limitations of trie-based and NLG-based approaches and leads to state-of-the-art performance. We evaluate the Trie-NLG model using two large QAC datasets. On average, our model achieves huge ~57% and ~14% boost in MRR over the popular trie-based lookup and the strong BART-based baseline methods, respectively. We make our code publicly available.
</details>
<details>
<summary>摘要</summary>
Query 自动完成（QAC）的目标是为给定的查询前缀提供可能的完成方案。传统上，QAC 系统都是基于历史查询记录中的尝试来建议最受欢迎的完成方案。在这种情况下，短前缀（即查询符）和未看到的前缀是两个困难的场景。最近，人性化的自然语言生成（NLG）模型被提议用于解决这两个挑战。然而，这些 NLG 模型受到两个缺点：（1）一些前一个会话中的查询可能是噪音和无关于用户意图的查询，和（2）NLG 模型不能直接包含历史查询的流行性信号。这种情况引发我们提出一种新的 NLG 模型，即 Trie-NLG，它同时利用尝试和前一个会话中的查询来提供可能的完成方案。我们在训练 Trie-NLG 模型时，将前缀添加了丰富的上下文，包括最近的会话中的查询和 top 尝试。这种简单的模型方法超越了尝试基于和 NLG 基于的方法，并带来了状态之最好的性能。我们使用两个大的 QAC 数据集来评估 Trie-NLG 模型。在 average 的情况下，我们的模型在 MRR 方面获得了大约 57% 和 14% 的提升，相比于流行的尝试基于的查看和强大的 BART 基eline 方法。我们将代码公开。
</details></li>
</ul>
<hr>
<h2 id="CFN-ESA-A-Cross-Modal-Fusion-Network-with-Emotion-Shift-Awareness-for-Dialogue-Emotion-Recognition"><a href="#CFN-ESA-A-Cross-Modal-Fusion-Network-with-Emotion-Shift-Awareness-for-Dialogue-Emotion-Recognition" class="headerlink" title="CFN-ESA: A Cross-Modal Fusion Network with Emotion-Shift Awareness for Dialogue Emotion Recognition"></a>CFN-ESA: A Cross-Modal Fusion Network with Emotion-Shift Awareness for Dialogue Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15432">http://arxiv.org/abs/2307.15432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiang Li, Yingjian Liu, Xiaoping Wang, Zhigang Zeng</li>
<li>for: 这篇研究旨在提出一个跨Modal融合网络，具有情感变化意识（CFN-ESA），用于多modal情感识别（ERC）。</li>
<li>methods: 该方法使用文本modalities作为主要情感信息来源，而视觉和声音modalities则被视为次要来源。此外，该方法还包括情感变化模块（LESM），以捕捉情感变化信息，并将其与主要任务进行相互适应。</li>
<li>results: 实验结果显示，CFN-ESA可以优化ERC的表现，并与现有模型相比，得到了remarkable的进步。<details>
<summary>Abstract</summary>
Multimodal Emotion Recognition in Conversation (ERC) has garnered growing attention from research communities in various fields. In this paper, we propose a cross-modal fusion network with emotion-shift awareness (CFN-ESA) for ERC. Extant approaches employ each modality equally without distinguishing the amount of emotional information, rendering it hard to adequately extract complementary and associative information from multimodal data. To cope with this problem, in CFN-ESA, textual modalities are treated as the primary source of emotional information, while visual and acoustic modalities are taken as the secondary sources. Besides, most multimodal ERC models ignore emotion-shift information and overfocus on contextual information, leading to the failure of emotion recognition under emotion-shift scenario. We elaborate an emotion-shift module to address this challenge. CFN-ESA mainly consists of the unimodal encoder (RUME), cross-modal encoder (ACME), and emotion-shift module (LESM). RUME is applied to extract conversation-level contextual emotional cues while pulling together the data distributions between modalities; ACME is utilized to perform multimodal interaction centered on textual modality; LESM is used to model emotion shift and capture related information, thereby guide the learning of the main task. Experimental results demonstrate that CFN-ESA can effectively promote performance for ERC and remarkably outperform the state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
多modal情感识别在对话（ERC）领域已经吸引了不同领域的研究者的关注。在这篇论文中，我们提出了跨modal融合网络 WITH emotion-shift 意识（CFN-ESA） для ERC。现有的方法均视每个模式都是平等的，无法准确地EXTRACT complementary和associative information FROM multimodal data。为了解决这个问题，在CFN-ESA中，文本模式被视为情感信息的主要来源，而视觉和声音模式则被视为次要来源。此外，大多数多modal ERC模型忽视情感转换信息并过度关注上下文信息，导致情感识别下情感转换场景失败。我们提出了情感转换模块来解决这个挑战。CFN-ESA主要由RUME、ACME和LESM三部分组成。RUME用于EXTRACT对话水平的情感cue，并将多个模式之间的数据分布相互紧密连接起来；ACME用于在文本模式为中心进行多模式交互；LESM用于模型情感转换， capture相关信息，以导引主任务的学习。实验结果表明，CFN-ESA可以有效提高ERC的性能，并remarkably exceed state-of-the-art模型。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Learning-Behaviour-of-In-context-Learning-A-Comparison-with-Supervised-Learning"><a href="#Investigating-the-Learning-Behaviour-of-In-context-Learning-A-Comparison-with-Supervised-Learning" class="headerlink" title="Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning"></a>Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15411">http://arxiv.org/abs/2307.15411</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xdwang0726/icl_ll">https://github.com/xdwang0726/icl_ll</a></li>
<li>paper_authors: Xindi Wang, Yufei Wang, Can Xu, Xiubo Geng, Bowen Zhang, Chongyang Tao, Frank Rudzicz, Robert E. Mercer, Daxin Jiang</li>
<li>for: 这 paper 的目的是 investigate the learning behavior of in-context learning (ICL) and compare it with supervised learning (SL) under label perturbations.</li>
<li>methods: 作者使用了同一个 demonstration example 进行 ICL 和 SL 训练，并研究其下游性能在 classification tasks 中受到 label perturbations 的影响。</li>
<li>results: AUTHORS 发现，gold labels 对下游 ICL 性能有显著影响，特别是对大语言模型; 然而，不均衡标签对 ICL 的影响几乎无关紧要。此外，作者还发现，相比 SL，ICL 对标签扰乱更为敏感，但是随着模型大小增加，ICL 逐渐达到 SL 的性能水平。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown remarkable capacity for in-context learning (ICL), where learning a new task from just a few training examples is done without being explicitly pre-trained. However, despite the success of LLMs, there has been little understanding of how ICL learns the knowledge from the given prompts. In this paper, to make progress toward understanding the learning behaviour of ICL, we train the same LLMs with the same demonstration examples via ICL and supervised learning (SL), respectively, and investigate their performance under label perturbations (i.e., noisy labels and label imbalance) on a range of classification tasks. First, via extensive experiments, we find that gold labels have significant impacts on the downstream in-context performance, especially for large language models; however, imbalanced labels matter little to ICL across all model sizes. Second, when comparing with SL, we show empirically that ICL is less sensitive to label perturbations than SL, and ICL gradually attains comparable performance to SL as the model size increases.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经表现出杰出的内容学习（ICL）能力，即从极少的训练示例中学习新任务，而不需要预先训练。然而， despite 成功的 LLM， there has been little understanding of how ICL learns the knowledge from the given prompts. 在这篇论文中，我们将同一个 LLM 训练 via ICL 和监督学习（SL），并调查它们在标签噪音（i.e., 杂凑标签和标签不均）的情况下的性能。首先，通过广泛的实验，我们发现 gold labels 对 downstream in-context performance 有很大的影响，特别是 для large language models; however, imbalanced labels matter little to ICL across all model sizes。其次，我们比较 SL 和 ICL，我们显示了实践中 ICL 比 SL 更敏感于标签噪音，并且 ICL 逐渐实现了与 SL 相同的性能，随着模型大小增加。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Fully-Unsupervised-Framework-for-Intent-Induction-in-Customer-Support-Dialogues"><a href="#Towards-a-Fully-Unsupervised-Framework-for-Intent-Induction-in-Customer-Support-Dialogues" class="headerlink" title="Towards a Fully Unsupervised Framework for Intent Induction in Customer Support Dialogues"></a>Towards a Fully Unsupervised Framework for Intent Induction in Customer Support Dialogues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15410">http://arxiv.org/abs/2307.15410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rita Costa, Bruno Martins, Sérgio Viana, Luisa Coheur</li>
<li>for: 本研究旨在提出一种 completelly unsupervised 的意图推论框架，以便在对话中进行意图推论。</li>
<li>methods: 本研究使用了对话 corpora 的预处理技术，以提高结果的准确性。同时，通过investigating the most common sequences，提取对话的意图流程。</li>
<li>results: 本研究在 MultiWOZ  dataset 上进行了测试，并获得了可靠的结果。这种框架不仅可以应用于 MultiWOZ  dataset，还可以应用于任何可能的用 caso，例如实际世界中的客户支持应用。<details>
<summary>Abstract</summary>
State of the art models in intent induction require annotated datasets. However, annotating dialogues is time-consuming, laborious and expensive. In this work, we propose a completely unsupervised framework for intent induction within a dialogue. In addition, we show how pre-processing the dialogue corpora can improve results. Finally, we show how to extract the dialogue flows of intentions by investigating the most common sequences. Although we test our work in the MultiWOZ dataset, the fact that this framework requires no prior knowledge make it applicable to any possible use case, making it very relevant to real world customer support applications across industry.
</details>
<details>
<summary>摘要</summary>
现代模型对意向推干需要标注数据集。然而，标注对话是时间费时的、劳苦的和昂费的。在这个工作中，我们提出了一个 completly 无监控的框架，用于对对话中的意向进行推干。此外，我们显示了如何对对话数据库进行预processing，以改善结果。最后，我们显示了如何从最常见的sequences中提取对话流程的意向。我们在MultiWOZ dataset上进行了测试，但由于这个框架不需任何先前知识，因此它适用于任何可能的用 caso，使其在实际世界中的客户支持应用程序中非常有 relevance。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Tourist-Assistance-using-ChatGPT-Comparing-Capabilities-in-Hindi-Telugu-and-Kannada"><a href="#Multilingual-Tourist-Assistance-using-ChatGPT-Comparing-Capabilities-in-Hindi-Telugu-and-Kannada" class="headerlink" title="Multilingual Tourist Assistance using ChatGPT: Comparing Capabilities in Hindi, Telugu, and Kannada"></a>Multilingual Tourist Assistance using ChatGPT: Comparing Capabilities in Hindi, Telugu, and Kannada</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15376">http://arxiv.org/abs/2307.15376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjana Kolar, Rohit Kumar</li>
<li>for: 这项研究旨在评估OpenAI提供的ChatGPT语言模型在翻译英语到印地语、telugu和 kannada语言方面的效果，以帮助印度的旅游者在语言多样性的环境中。</li>
<li>methods: 该研究使用了50个多样化的问题集，包括一般知识、美食和旅游等领域，并由5名志愿者评分翻译的准确性和流畅性。这些分数最后被转换为BLEU分数，以衡量机器生成的翻译质量。</li>
<li>results: 研究发现，印地语翻译表现出色，具有更高的准确性和流畅性，而telugu翻译则落后于其他语言。人工评分者对翻译的准确性和流畅性进行评估，提供了全面的语言模型性能评估。<details>
<summary>Abstract</summary>
This research investigates the effectiveness of ChatGPT, an AI language model by OpenAI, in translating English into Hindi, Telugu, and Kannada languages, aimed at assisting tourists in India's linguistically diverse environment. To measure the translation quality, a test set of 50 questions from diverse fields such as general knowledge, food, and travel was used. These were assessed by five volunteers for accuracy and fluency, and the scores were subsequently converted into a BLEU score. The BLEU score evaluates the closeness of a machine-generated translation to a human translation, with a higher score indicating better translation quality. The Hindi translations outperformed others, showcasing superior accuracy and fluency, whereas Telugu translations lagged behind. Human evaluators rated both the accuracy and fluency of translations, offering a comprehensive perspective on the language model's performance.
</details>
<details>
<summary>摘要</summary>
这项研究探讨了OpenAI开发的语言模型ChatGPT在将英语翻译成印地语、telugu和 kannada语言方面的效果，以帮助印度语言多样性环境中的旅游者。为衡量翻译质量，研究使用了50个多学科知识、食物和旅行的问题集，由5名志愿者评测准确性和流畅性，并将得分转换为BLEU分数。BLEU分数评估机器生成翻译与人工翻译之间的相似性，高分数表示更高的翻译质量。印地语翻译表现出色，准确性和流畅性都较高，而telugu翻译则落后。人工评测器对翻译准确性和流畅性进行评估，为语言模型表现提供了全面的视角。
</details></li>
</ul>
<hr>
<h2 id="Teach-Me-How-to-Improve-My-Argumentation-Skills-A-Survey-on-Feedback-in-Argumentation"><a href="#Teach-Me-How-to-Improve-My-Argumentation-Skills-A-Survey-on-Feedback-in-Argumentation" class="headerlink" title="Teach Me How to Improve My Argumentation Skills: A Survey on Feedback in Argumentation"></a>Teach Me How to Improve My Argumentation Skills: A Survey on Feedback in Argumentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15341">http://arxiv.org/abs/2307.15341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Camélia Guerraoui, Paul Reisert, Naoya Inoue, Farjana Sultana Mim, Shoichi Naito, Jungmin Choi, Irfan Robbani, Wenzhi Wang, Kentaro Inui</li>
<li>for: 这篇论文旨在探讨计算机模型在推理方面的反馈方式，以帮助学生提高批判性思维能力。</li>
<li>methods: 论文使用现有的计算机模型来评估论证质量，并探讨这些模型是否能够提供有用的反馈，以帮助学生进行改进。</li>
<li>results: 论文发现，现有的计算机模型可以提供较为 ricH 的反馈，但是这些反馈通常无法解释为什么某个论证是低质量的，这限制了对学生的反馈提供 constructive 的feedback。<details>
<summary>Abstract</summary>
The use of argumentation in education has been shown to improve critical thinking skills for end-users such as students, and computational models for argumentation have been developed to assist in this process. Although these models are useful for evaluating the quality of an argument, they oftentimes cannot explain why a particular argument is considered poor or not, which makes it difficult to provide constructive feedback to users to strengthen their critical thinking skills. In this survey, we aim to explore the different dimensions of feedback (Richness, Visualization, Interactivity, and Personalization) provided by the current computational models for argumentation, and the possibility of enhancing the power of explanations of such models, ultimately helping learners improve their critical thinking skills.
</details>
<details>
<summary>摘要</summary>
使用辩论在教育中有助于提高学生的批判性思维能力，计算机模型也已经为这个过程而开发。虽然这些模型有用于评估论证质量，但它们往往无法解释特定论证为何不好或者不合理，这使得给用户提供有用的反馈很困难，从而难以帮助学生提高批判性思维能力。在这份调查中，我们计划探讨现有的计算机模型feedback维度（丰富性、可视化、交互性和个性化），以及可能通过增强这些模型的解释力来帮助学生提高批判性思维能力。
</details></li>
</ul>
<hr>
<h2 id="BARTPhoBEiT-Pre-trained-Sequence-to-Sequence-and-Image-Transformers-Models-for-Vietnamese-Visual-Question-Answering"><a href="#BARTPhoBEiT-Pre-trained-Sequence-to-Sequence-and-Image-Transformers-Models-for-Vietnamese-Visual-Question-Answering" class="headerlink" title="BARTPhoBEiT: Pre-trained Sequence-to-Sequence and Image Transformers Models for Vietnamese Visual Question Answering"></a>BARTPhoBEiT: Pre-trained Sequence-to-Sequence and Image Transformers Models for Vietnamese Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15335">http://arxiv.org/abs/2307.15335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khiem Vinh Tran, Kiet Van Nguyen, Ngan Luu Thuy Nguyen</li>
<li>for: 本研究旨在提出一个基于 transformer 的越南语模型，以解决英语资源充足的问题，并在越南语 VQA dataset 上进行评估。</li>
<li>methods: 本研究使用了预训Sequence-to-Sequence和 bidirectional encoder representation from Image Transformers，并在越南语中进行训练。</li>
<li>results: 实验结果显示，我们的提案模型在六个指标中出performing better than 强基eline，包括 Accuracy、Precision、Recall、F1-score、WUPS 0.0 和 WUPS 0.9。<details>
<summary>Abstract</summary>
Visual Question Answering (VQA) is an intricate and demanding task that integrates natural language processing (NLP) and computer vision (CV), capturing the interest of researchers. The English language, renowned for its wealth of resources, has witnessed notable advancements in both datasets and models designed for VQA. However, there is a lack of models that target specific countries such as Vietnam. To address this limitation, we introduce a transformer-based Vietnamese model named BARTPhoBEiT. This model includes pre-trained Sequence-to-Sequence and bidirectional encoder representation from Image Transformers in Vietnamese and evaluates Vietnamese VQA datasets. Experimental results demonstrate that our proposed model outperforms the strong baseline and improves the state-of-the-art in six metrics: Accuracy, Precision, Recall, F1-score, WUPS 0.0, and WUPS 0.9.
</details>
<details>
<summary>摘要</summary>
视觉问答（VQA）是一项复杂且需求高的任务，涉及自然语言处理（NLP）和计算机视觉（CV），吸引了研究者们的关注。英语，因其资源丰富，在VQA领域已经取得了显著进步，但是尚未有专门针对特定国家的模型。为了解决这一限制，我们提出了一个基于变换器的越南语模型，名为BARTPhoBEiT。这个模型包括预训练的序列到序列和双向编码器表示图像变换器在越南语中，并评估越南语VQA数据集。实验结果表明，我们提议的模型超越强基线和提高了状态之册的六个指标：准确率、精度、回归率、F1分数、WUPS 0.0和WUPS 0.9。
</details></li>
</ul>
<hr>
<h2 id="SAP-sLDA-An-Interpretable-Interface-for-Exploring-Unstructured-Text"><a href="#SAP-sLDA-An-Interpretable-Interface-for-Exploring-Unstructured-Text" class="headerlink" title="SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text"></a>SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01420">http://arxiv.org/abs/2308.01420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charumathi Badrinath, Weiwei Pan, Finale Doshi-Velez</li>
<li>for: 用于改进文档抽象的维度减少算法，以便更好地捕捉人类理解的文档相似性关系。</li>
<li>methods: 基于Latent Dirichlet Allocation（LDA）的半指导式人工智能-循环方法，通过允许用户提供一些标签来学习主题。</li>
<li>results: 在synthetic corpora上，我们的方法可以生成更加理解的抽象，并且只需要提供一部分标签。在实际 corpora 上，我们获得了相似的结果。<details>
<summary>Abstract</summary>
A common way to explore text corpora is through low-dimensional projections of the documents, where one hopes that thematically similar documents will be clustered together in the projected space. However, popular algorithms for dimensionality reduction of text corpora, like Latent Dirichlet Allocation (LDA), often produce projections that do not capture human notions of document similarity. We propose a semi-supervised human-in-the-loop LDA-based method for learning topics that preserve semantically meaningful relationships between documents in low-dimensional projections. On synthetic corpora, our method yields more interpretable projections than baseline methods with only a fraction of labels provided. On a real corpus, we obtain qualitatively similar results.
</details>
<details>
<summary>摘要</summary>
通常来说，探索文本 corpus 的方式是通过低维度投影文档，希望在投影空间中 clusters  similar documents 。然而，流行的文本探索维度减少算法，如 Latent Dirichlet Allocation (LDA)，经常生成投影不符合人类意义的文档相似性。我们提议一种 semi-supervised 人在循环 LDA 基于方法，以学习保持含义相似性的文档关系在低维度投影中。在 sintetic corpus 上，我们的方法可以提供更加 interpretable 的投影，只需提供一部分标签。在真实 corpus 上，我们获得了类似的结果。
</details></li>
</ul>
<hr>
<h2 id="TrafficSafetyGPT-Tuning-a-Pre-trained-Large-Language-Model-to-a-Domain-Specific-Expert-in-Transportation-Safety"><a href="#TrafficSafetyGPT-Tuning-a-Pre-trained-Large-Language-Model-to-a-Domain-Specific-Expert-in-Transportation-Safety" class="headerlink" title="TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety"></a>TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15311">http://arxiv.org/abs/2307.15311</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ozheng1993/trafficsafetygpt">https://github.com/ozheng1993/trafficsafetygpt</a></li>
<li>paper_authors: Ou Zheng, Mohamed Abdel-Aty, Dongdong Wang, Chenzhu Wang, Shengxuan Ding</li>
<li>for: 这种研究是为了提高大型自然语言处理模型（LLMs）在交通安全领域任务中的表现，并且强调了特殊的交通安全专业知识的重要性。</li>
<li>methods: 这种研究使用了一种基于LLAMA模型的新型模型，并在这种模型上进行了监督微调，使用了由政府生成的指导书和ChatGPT生成的指令输出对的人工标签。</li>
<li>results: 研究发现，这种TrafficSafetyGPT模型在交通安全领域任务中表现出色，并且可以减轻特殊的交通安全专业知识的需求。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown remarkable effectiveness in various general-domain natural language processing (NLP) tasks. However, their performance in transportation safety domain tasks has been suboptimal, primarily attributed to the requirement for specialized transportation safety expertise in generating accurate responses [1]. To address this challenge, we introduce TrafficSafetyGPT, a novel LLAMA-based model, which has undergone supervised fine-tuning using TrafficSafety-2K dataset which has human labels from government produced guiding books and ChatGPT-generated instruction-output pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset are accessible at https://github.com/ozheng1993/TrafficSafetyGPT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ChatHome-Development-and-Evaluation-of-a-Domain-Specific-Language-Model-for-Home-Renovation"><a href="#ChatHome-Development-and-Evaluation-of-a-Domain-Specific-Language-Model-for-Home-Renovation" class="headerlink" title="ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation"></a>ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15290">http://arxiv.org/abs/2307.15290</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lianjiatech/belle">https://github.com/lianjiatech/belle</a></li>
<li>paper_authors: Cheng Wen, Xianghui Sun, Shuaijiang Zhao, Xiaoquan Fang, Liangyu Chen, Wei Zou</li>
<li>for: 这篇论文旨在开发和评估一种专门为家居改造领域设计的域专语言模型（DSLM）。</li>
<li>methods: 该研究采用了域适应预训练和指令调整的方法，使用了一个广泛的数据集，包括专业文章、标准文档和网络内容相关于家居改造。</li>
<li>results: 实验表明，ChatHome不仅提高了域专功能，还保持了其通用性。<details>
<summary>Abstract</summary>
This paper presents the development and evaluation of ChatHome, a domain-specific language model (DSLM) designed for the intricate field of home renovation. Considering the proven competencies of large language models (LLMs) like GPT-4 and the escalating fascination with home renovation, this study endeavors to reconcile these aspects by generating a dedicated model that can yield high-fidelity, precise outputs relevant to the home renovation arena. ChatHome's novelty rests on its methodology, fusing domain-adaptive pretraining and instruction-tuning over an extensive dataset. This dataset includes professional articles, standard documents, and web content pertinent to home renovation. This dual-pronged strategy is designed to ensure that our model can assimilate comprehensive domain knowledge and effectively address user inquiries. Via thorough experimentation on diverse datasets, both universal and domain-specific, including the freshly introduced "EvalHome" domain dataset, we substantiate that ChatHome not only amplifies domain-specific functionalities but also preserves its versatility.
</details>
<details>
<summary>摘要</summary>
ChatHome's novelty lies in its methodology, which fuses domain-adaptive pretraining and instruction-tuning over an extensive dataset. This dataset includes professional articles, standard documents, and web content related to home renovation. This dual-pronged approach is designed to ensure that our model can absorb comprehensive domain knowledge and effectively address user inquiries.Through thorough experimentation on diverse datasets, including the newly introduced "EvalHome" domain dataset, we demonstrate that ChatHome not only enhances domain-specific functionalities but also preserves its versatility.
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Lexical-Simplification-via-Paraphrase-Generation"><a href="#Multilingual-Lexical-Simplification-via-Paraphrase-Generation" class="headerlink" title="Multilingual Lexical Simplification via Paraphrase Generation"></a>Multilingual Lexical Simplification via Paraphrase Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15286">http://arxiv.org/abs/2307.15286</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kpkqwq/lspg">https://github.com/kpkqwq/lspg</a></li>
<li>paper_authors: Kang Liu, Jipeng Qiang, Yun Li, Yunhao Yuan, Yi Zhu, Kaixun Hua</li>
<li>for: 提高 lexical simplification 方法的效果，使其能够生成更加准确和多样化的替代词。</li>
<li>methods: 提出了一种基于 paraphrase 生成的多语言 lexical simplification 方法，通过将句子作为输入，生成具有多种词替的替代词。</li>
<li>results: 实验结果表明，我们的方法在英语、西班牙语和葡萄牙语等语言上显著超过 BERT 基于方法和零批 GPT3 基于方法。<details>
<summary>Abstract</summary>
Lexical simplification (LS) methods based on pretrained language models have made remarkable progress, generating potential substitutes for a complex word through analysis of its contextual surroundings. However, these methods require separate pretrained models for different languages and disregard the preservation of sentence meaning. In this paper, we propose a novel multilingual LS method via paraphrase generation, as paraphrases provide diversity in word selection while preserving the sentence's meaning. We regard paraphrasing as a zero-shot translation task within multilingual neural machine translation that supports hundreds of languages. After feeding the input sentence into the encoder of paraphrase modeling, we generate the substitutes based on a novel decoding strategy that concentrates solely on the lexical variations of the complex word. Experimental results demonstrate that our approach surpasses BERT-based methods and zero-shot GPT3-based method significantly on English, Spanish, and Portuguese.
</details>
<details>
<summary>摘要</summary>
Lexical simplification（LS）方法基于预训练语言模型已经做出了很大的进步，通过分析词语上下文环境来生成可能的替换词。然而，这些方法需要单独的预训练模型来支持不同的语言，并且忽略了保持句子意义的要求。在这篇论文中，我们提出了一种新的多语言LS方法，通过句子重写来提供多样性的词选择，同时保持句子意义的完整性。我们将重写视为一种零批译翻译任务，利用多语言神经翻译模型支持多种语言。在输入句子被编码器模型处理后，我们通过一种新的解码策略来生成替换词，专注于复杂词语的语言变换。实验结果显示，我们的方法在英语、西班牙语和葡萄牙语等语言上超越BERT基于方法和零批GPT3基于方法。
</details></li>
</ul>
<hr>
<h2 id="f-Divergence-Minimization-for-Sequence-Level-Knowledge-Distillation"><a href="#f-Divergence-Minimization-for-Sequence-Level-Knowledge-Distillation" class="headerlink" title="f-Divergence Minimization for Sequence-Level Knowledge Distillation"></a>f-Divergence Minimization for Sequence-Level Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15190">http://arxiv.org/abs/2307.15190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manga-uofa/fdistill">https://github.com/manga-uofa/fdistill</a></li>
<li>paper_authors: Yuqiao Wen, Zichao Li, Wenyu Du, Lili Mou</li>
<li>for: 本文主要针对语言处理领域中的知识填充问题，即将大型模型中的知识转移到小型模型中。</li>
<li>methods: 本文提出了一个f-DISTILL框架，该框架将序列级知识填充转化为最小化一个泛化f- divergence函数的问题。文章还提出了四种填充变种，其中包括SeqKD和ENGINE等已有的方法。</li>
<li>results: 实验结果显示，本文提出的方法可以超越现有的填充方法，并且使用 симметриック的填充损失可以更好地让学生模型学习教师分布。<details>
<summary>Abstract</summary>
Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an f-DISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our f-DISTILL methods. We further derive step-wise decomposition for our f-DISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.
</details>
<details>
<summary>摘要</summary>
知识填充（KD）是将知识从大型模型传递到小型模型的过程。随着自然语言处理领域中模型的不断扩大，KD已经受到了越来越多的关注。在这项工作中，我们提出了f-DISTILL框架，它将序列级知识填充形式化为最小化一个通用f-散度函数。我们提出了四种填充变体，并证明了现有的SeqKD和ENGINE方法是f-DISTILL方法的近似方法。我们还 deriv了 step-wise 分解，将不可 tractable的序列级散度降到单词级损失，可以在可追踪的方式上计算。实验结果表明，我们的方法在四个数据集上表现出色，并且我们的对称填充损失可以更好地让学生学习老师分布。
</details></li>
</ul>
<hr>
<h2 id="A-Geometric-Notion-of-Causal-Probing"><a href="#A-Geometric-Notion-of-Causal-Probing" class="headerlink" title="A Geometric Notion of Causal Probing"></a>A Geometric Notion of Causal Probing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15054">http://arxiv.org/abs/2307.15054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clément Guerner, Anej Svete, Tianyu Liu, Alexander Warstadt, Ryan Cotterell</li>
<li>for: 本文提出了一种 formal definition of $\textit{intrinsic}$ information in a subspace of a language model’s representation space, 以便从某些方面控制语言模型的预测结果。</li>
<li>methods: 本文使用了一种counterfactual approach，通过独立处理潜在相关的组件来避免偶极 correlations 问题（Kumar et al., 2022）。此外，本文还提出了一种 $\textit{causal}$ concept subspace，可以优化information在子空间中的搜索。</li>
<li>results: 实验表明，使用R-LACE（Ravfogel et al., 2022）返回的一维子空间可以控制语言模型生成的字符串的概念值。在本文的 causal controlled intervention 中，我们发现，对于至少一个模型，可以使用 R-LACE 返回的子空间来准确地控制生成的字符串中的概念值。<details>
<summary>Abstract</summary>
Large language models rely on real-valued representations of text to make their predictions. These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender. A growing body of work has considered removing information about concepts such as these using orthogonal projections onto subspaces of the representation space. We contribute to this body of work by proposing a formal definition of $\textit{intrinsic}$ information in a subspace of a language model's representation space. We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently. We show that our counterfactual notion of information in a subspace is optimized by a $\textit{causal}$ concept subspace. Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the value of the conceptual component of a representation. Empirically, we find that R-LACE (Ravfogel et al., 2022) returns a one-dimensional subspace containing roughly half of total concept information under our framework. Our causal controlled intervention shows that, for at least one model, the subspace returned by R-LACE can be used to manipulate the concept value of the generated word with precision.
</details>
<details>
<summary>摘要</summary>
大型语言模型通常使用实数valued表示文本来进行预测。这些表示包含模型在训练数据中学习的信息，包括语言性质和人口偏见（如性别）等。一个快速增长的研究领域是去除这些信息，使用正交投影onto subspace of representation space。我们对这些研究进行贡献，提出了一个正式的内在信息定义在语言模型表示空间中的概念。我们提出了一种避免假 correlate 失败模式（Kumar et al., 2022）的对照方法，该方法将subspace和其正交补做独立处理。我们显示了我们的对照方法可以优化内在信息在subspace中。此外，这种干预还允许我们通过控制概念的值来进行概念控制生成。我们的实验表明，R-LACE（Ravfogel et al., 2022）返回了一个一维子空间，包含约半个总概念信息。我们的 causal 控制干预表明，至少一个模型中，R-LACE返回的子空间可以准确地控制生成词的概念值。
</details></li>
</ul>
<hr>
<h2 id="Gzip-versus-bag-of-words-for-text-classification"><a href="#Gzip-versus-bag-of-words-for-text-classification" class="headerlink" title="Gzip versus bag-of-words for text classification"></a>Gzip versus bag-of-words for text classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15002">http://arxiv.org/abs/2307.15002</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flipz357/npc_gzip_exp">https://github.com/flipz357/npc_gzip_exp</a></li>
<li>paper_authors: Juri Opitz</li>
<li>for: 这份研究旨在证明 bag-of-words 方法可以达到类似或更好的结果，并更高效。</li>
<li>methods: 这篇论文使用了 bag-of-words 方法，并进行了 compression 来提高效率。</li>
<li>results: 研究发现，bag-of-words 方法可以达到类似或更好的结果，而且更高效。<details>
<summary>Abstract</summary>
The effectiveness of compression in text classification ('gzip') has recently garnered lots of attention. In this note we show that `bag-of-words' approaches can achieve similar or better results, and are more efficient.
</details>
<details>
<summary>摘要</summary>
“压缩（gzip）在文本分类中的效果在最近引起了很多关注。在这个笔记中，我们展示了 bag-of-words 方法可以达到类似或更好的结果，并且更高效。”Note:* "压缩" (gzip) is translated as "压缩" in Simplified Chinese.* "bag-of-words" is translated as "bag-of-words" in Simplified Chinese.* "文本分类" (text classification) is translated as "文本分类" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Scaling-TransNormer-to-175-Billion-Parameters"><a href="#Scaling-TransNormer-to-175-Billion-Parameters" class="headerlink" title="Scaling TransNormer to 175 Billion Parameters"></a>Scaling TransNormer to 175 Billion Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14995">http://arxiv.org/abs/2307.14995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, Yu Qiao, Yiran Zhong</li>
<li>For: The paper proposes a new linear attention-based Large Language Model (LLM) called TransNormerLLM, which outperforms conventional softmax attention-based models in terms of both accuracy and efficiency.* Methods: The paper introduces several advanced modifications to the previous linear attention architecture TransNormer, including positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization. The paper also proposes a new technique called Lightning Attention to accelerate linear attention.* Results: The paper achieves impressive acceleration of over 20% and reduces memory usage by a remarkable four times. The model also shows superior efficiency during both training and inference stages, and is scalable for seamless deployment on large-scale clusters. The paper also demonstrates the effectiveness of the model through comprehensive experiments on a self-collected corpus exceeding 6TB and containing over 2 trillion tokens.<details>
<summary>Abstract</summary>
We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over 20%. Furthermore, we have developed a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. Scalability is at the heart of our model's design, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, all while maintaining outstanding performance metrics. Rigorous validation of our model design is achieved through a series of comprehensive experiments on our self-collected corpus, boasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure data quality and relevance, we implement a new self-cleaning strategy to filter our collected data. Our pre-trained models will be released to foster community advancements in efficient LLMs.
</details>
<details>
<summary>摘要</summary>
我们介绍TransNormerLLM，世界上首个运算式注意力基于大语言模型（LLM），在精度和效率方面都能超越传统软max注意力基于模型。TransNormerLLM继承了之前的线性注意架构TransNormer，通过进一步改进，包括位置嵌入、加速线性注意、闸门机制、tensor normalization、推理加速和稳定。具体来说，我们使用LRPE和指数衰退来避免注意力扩散问题，同时让模型保留字元之间的全局互动。此外，我们提出了Lightning Attention技术，可以在runtime中更多地加速线性注意，并降低内存使用量，实现了四倍以上的加速。为了进一步提高TransNormer的性能，我们导入了闸门机制来缓和训练，并使用新的tensor normalization scheme来加速模型，实现了20%以上的加速。此外，我们开发了一个可靠的推理算法，可以在训练和推理阶段都保持数据稳定和一致的推理速度，无视序列长度。 scalability是我们模型的设计核心，使得可以顺利部署在大规模集群上，并且可以进一步扩展到更大的模型，同时保持出色的性能 Metrics。我们透过一系列严谨的实验，证明了我们的模型设计是正确的。我们将预训练的模型发布，以促进社区对效率LLM的发展。
</details></li>
</ul>
<hr>
<h2 id="Incrementally-Computable-Neural-Networks-Efficient-Inference-for-Dynamic-Inputs"><a href="#Incrementally-Computable-Neural-Networks-Efficient-Inference-for-Dynamic-Inputs" class="headerlink" title="Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs"></a>Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14988">http://arxiv.org/abs/2307.14988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Or Sharir, Anima Anandkumar</li>
<li>for: 这个论文的目的是提出一种高效的增量计算方法，以便在感知数据或用户输入的变化下进行深度学习模型的更新。</li>
<li>methods: 该论文使用了vector量化方法，以便在深度网络中重用计算结果。具体来说，它使用了densely connected transformers架构，并通过减少不必要的计算来实现增量计算。</li>
<li>results: 实验结果表明，使用该方法可以在文档编辑过程中实现高效的增量计算，并且可以保持与批处理模型相同的准确率。具体来说，相比于OPT-125M预训练语言模型，该方法可以降低12.1倍（中位数）的操作数量。<details>
<summary>Abstract</summary>
Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs. For example, an AI writing assistant is required to update its suggestions in real time as a document is edited. Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization. Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change. However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexity proportional to the fraction of the modified inputs. Our experiments with adapting the OPT-125M pre-trained language model demonstrate comparable accuracy on document classification while requiring 12.1X (median) fewer operations for processing sequences of atomic edits.
</details>
<details>
<summary>摘要</summary>
为解决这个问题，我们使用 вектор量化来精度化 intermediate 值在网络中，从而过滤无用和干扰的修改。这使得可以 reuse hidden neurons 的值，从而实现高效的逐步计算算法。我们应用这种方法到 transformers 架构上，创造了一种高效的逐步计算算法，其复杂度与修改输入的 Fraction 成正比。我们的实验表明，可以在文档编辑中实现相同的准确率，而且需要 fewer operations（ median 为 12.1X）来处理序列中的 atomic 修改。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/28/cs.CL_2023_07_28/" data-id="clpahu701009b3h88erxhcmc7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/78/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/77/">77</a><a class="page-number" href="/page/78/">78</a><span class="page-number current">79</span><a class="page-number" href="/page/80/">80</a><a class="page-number" href="/page/81/">81</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/80/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
