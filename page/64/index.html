
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/64/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_07_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/31/cs.CL_2023_07_31/" class="article-date">
  <time datetime="2023-07-31T11:00:00.000Z" itemprop="datePublished">2023-07-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/31/cs.CL_2023_07_31/">cs.CL - 2023-07-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Lexically-Accelerated-Dense-Retrieval"><a href="#Lexically-Accelerated-Dense-Retrieval" class="headerlink" title="Lexically-Accelerated Dense Retrieval"></a>Lexically-Accelerated Dense Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16779">http://arxiv.org/abs/2307.16779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishikesh Kulkarni, Sean MacAvaney, Nazli Goharian, Ophir Frieder</li>
<li>for: 提高 dense retrieval 模型的效率，不妨碍搜索效果。</li>
<li>methods: 使用 lexical retrieval 技术为 dense retrieval 搜索种子，并使用文档准近图进行搜索。</li>
<li>results: 实现 dense retrieval 模型的效率-效果对应 frontier，并在8毫秒&#x2F;查询时间下达到了相同的精度和准确率。<details>
<summary>Abstract</summary>
Retrieval approaches that score documents based on learned dense vectors (i.e., dense retrieval) rather than lexical signals (i.e., conventional retrieval) are increasingly popular. Their ability to identify related documents that do not necessarily contain the same terms as those appearing in the user's query (thereby improving recall) is one of their key advantages. However, to actually achieve these gains, dense retrieval approaches typically require an exhaustive search over the document collection, making them considerably more expensive at query-time than conventional lexical approaches. Several techniques aim to reduce this computational overhead by approximating the results of a full dense retriever. Although these approaches reasonably approximate the top results, they suffer in terms of recall -- one of the key advantages of dense retrieval. We introduce 'LADR' (Lexically-Accelerated Dense Retrieval), a simple-yet-effective approach that improves the efficiency of existing dense retrieval models without compromising on retrieval effectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval exploration that uses a document proximity graph. We explore two variants of LADR: a proactive approach that expands the search space to the neighbors of all seed documents, and an adaptive approach that selectively searches the documents with the highest estimated relevance in an iterative fashion. Through extensive experiments across a variety of dense retrieval models, we find that LADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier among approximate k nearest neighbor techniques. Further, we find that when tuned to take around 8ms per query in retrieval latency on our hardware, LADR consistently achieves both precision and recall that are on par with an exhaustive search on standard benchmarks.
</details>
<details>
<summary>摘要</summary>
traditional retrieval approaches that rely on lexical signals are being replaced by dense retrieval approaches that score documents based on learned dense vectors. These approaches can retrieve related documents that do not contain the same terms as the user's query, which improves recall. However, dense retrieval approaches typically require an exhaustive search over the document collection, which is computationally expensive. Several techniques have been proposed to reduce the computational overhead, but these approaches often sacrifice recall.We introduce 'LADR' (Lexically-Accelerated Dense Retrieval), a simple and effective approach that improves the efficiency of existing dense retrieval models without compromising on retrieval effectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval exploration that uses a document proximity graph. We explore two variants of LADR: a proactive approach that expands the search space to the neighbors of all seed documents, and an adaptive approach that selectively searches the documents with the highest estimated relevance in an iterative fashion.Through extensive experiments across a variety of dense retrieval models, we find that LADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier among approximate k nearest neighbor techniques. Furthermore, when tuned to take around 8ms per query in retrieval latency on our hardware, LADR consistently achieves both precision and recall that are on par with an exhaustive search on standard benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Multilingual-context-based-pronunciation-learning-for-Text-to-Speech"><a href="#Multilingual-context-based-pronunciation-learning-for-Text-to-Speech" class="headerlink" title="Multilingual context-based pronunciation learning for Text-to-Speech"></a>Multilingual context-based pronunciation learning for Text-to-Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16709">http://arxiv.org/abs/2307.16709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulia Comini, Manuel Sam Ribeiro, Fan Yang, Heereen Shim, Jaime Lorenzo-Trueba</li>
<li>for: 这篇论文是为了描述一种多语言Front-end系统，用于解决语音识别和其他语言特定挑战。</li>
<li>methods: 该系统使用Grapheme-to-Phoneme（G2P）关系来预测未知词的发音，同时使用规则引用系统来解决homograph和多音字的混淆。</li>
<li>results: 该模型在G2P转换和其他语言特定任务上表现竞争力强，但有些语言和任务之间存在一些妥协。<details>
<summary>Abstract</summary>
Phonetic information and linguistic knowledge are an essential component of a Text-to-speech (TTS) front-end. Given a language, a lexicon can be collected offline and Grapheme-to-Phoneme (G2P) relationships are usually modeled in order to predict the pronunciation for out-of-vocabulary (OOV) words. Additionally, post-lexical phonology, often defined in the form of rule-based systems, is used to correct pronunciation within or between words. In this work we showcase a multilingual unified front-end system that addresses any pronunciation related task, typically handled by separate modules. We evaluate the proposed model on G2P conversion and other language-specific challenges, such as homograph and polyphones disambiguation, post-lexical rules and implicit diacritization. We find that the multilingual model is competitive across languages and tasks, however, some trade-offs exists when compared to equivalent monolingual solutions.
</details>
<details>
<summary>摘要</summary>
文本识别和语言知识是文本识别（TTS）前端的重要组成部分。给定一种语言，可以在线收集词典，并模型文字到音（G2P）关系，以预测未在词典中出现的词汇的发音。此外，在词语之间或者在词语之内进行发音 correction 也需要使用后 lexical phonology，通常通过规则集来实现。在这项工作中，我们展示了一种多语言统一前端系统，可以解决任何发音相关任务，通常由分立模块处理。我们对这种模型进行了G2P转换和其他语言特有挑战的评估，如Homograph和多音字识别、后 lexical 规则和隐式 диаcritization。我们发现，该多语言模型在语言和任务方面具有竞争力，但有些交换存在于与等效单语言解决方案进行比较时。
</details></li>
</ul>
<hr>
<h2 id="No-that’s-not-what-I-meant-Handling-Third-Position-Repair-in-Conversational-Question-Answering"><a href="#No-that’s-not-what-I-meant-Handling-Third-Position-Repair-in-Conversational-Question-Answering" class="headerlink" title="No that’s not what I meant: Handling Third Position Repair in Conversational Question Answering"></a>No that’s not what I meant: Handling Third Position Repair in Conversational Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16689">http://arxiv.org/abs/2307.16689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vevake Balaraman, Arash Eshghi, Ioannis Konstas, Ioannis Papaioannou</li>
<li>for: 这 paper 的目的是研究人们在对话中如何处理歧义，以及如何使用 Third Position Repair (TPR) 来纠正歧义。</li>
<li>methods: 这 paper 使用了一个大型的 TPR 数据集，并对这些数据集进行了自动和人工评估。同时，paper 还使用了一些基eline 模型来执行 TPR。</li>
<li>results: 研究发现，OpenAI 的 GPT-3 LLMs 在原始turn的 TPR 处理方面表现不佳，但是在接下来的对话问答任务中，这些 LLMs 的 TPR 处理能力有了显著改善。<details>
<summary>Abstract</summary>
The ability to handle miscommunication is crucial to robust and faithful conversational AI. People usually deal with miscommunication immediately as they detect it, using highly systematic interactional mechanisms called repair. One important type of repair is Third Position Repair (TPR) whereby a speaker is initially misunderstood but then corrects the misunderstanding as it becomes apparent after the addressee's erroneous response. Here, we collect and publicly release Repair-QA, the first large dataset of TPRs in a conversational question answering (QA) setting. The data is comprised of the TPR turns, corresponding dialogue contexts, and candidate repairs of the original turn for execution of TPRs. We demonstrate the usefulness of the data by training and evaluating strong baseline models for executing TPRs. For stand-alone TPR execution, we perform both automatic and human evaluations on a fine-tuned T5 model, as well as OpenAI's GPT-3 LLMs. Additionally, we extrinsically evaluate the LLMs' TPR processing capabilities in the downstream conversational QA task. The results indicate poor out-of-the-box performance on TPR's by the GPT-3 models, which then significantly improves when exposed to Repair-QA.
</details>
<details>
<summary>摘要</summary>
人们在对话中处理混乱communication是关键，以确保对话AI强大和可靠。人们通常在发现混乱后立即处理，使用高度系统化的互动机制called repair。一种重要的修复方式是第三人称修复（TPR），在这种情况下，说话者在对方错误回答后才被理解，并在这个过程中修复错误。我们收集和公开发布了Repair-QA数据集，这是一个大型的TPR在对话问答（QA） Setting中的数据集。数据包括TPR转帧、对话上下文和原始转帧的可能修复。我们通过训练和评估强大基线模型来证明数据的有用性。对独立TPR执行来说，我们在一个精度调整的T5模型上进行自动和人工评估，以及OpenAI的GPT-3LLMs。此外，我们在下游对话问答任务中评估LLMs的TPR处理能力。结果表明GPT-3模型在出厂情况下对TPR表现不佳，但是在暴露于Repair-QA数据集后，其表现显著改善。
</details></li>
</ul>
<hr>
<h2 id="Comparing-normalizing-flows-and-diffusion-models-for-prosody-and-acoustic-modelling-in-text-to-speech"><a href="#Comparing-normalizing-flows-and-diffusion-models-for-prosody-and-acoustic-modelling-in-text-to-speech" class="headerlink" title="Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech"></a>Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16679">http://arxiv.org/abs/2307.16679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyan Zhang, Thomas Merritt, Manuel Sam Ribeiro, Biel Tura-Vecino, Kayoko Yanagisawa, Kamil Pokora, Abdelhamid Ezzerg, Sebastian Cygert, Ammar Abbas, Piotr Bilinski, Roberto Barra-Chicote, Daniel Korzekwa, Jaime Lorenzo-Trueba</li>
<li>for: 这 paper 是用来比较传统 L1&#x2F;L2 loss 方法和流体和填充方法来进行 text-to-speech 合成 tasks 的。</li>
<li>methods: 这 paper 使用了一种 prosody 模型来生成 log-f0 和 duration 特征，这些特征用于 condition 一个 acoustic 模型，该模型生成 mel-spectrograms。</li>
<li>results: 实验结果表明，流体基本模型在 spectrogram 预测任务中 achieve 最佳性能，超过相同的 diffusion 和 L1 模型。同时，流体和填充 prosody 预测器均导致了significant 改善，比传统 L2 训练的 prosody 模型。<details>
<summary>Abstract</summary>
Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models.
</details>
<details>
<summary>摘要</summary>
传统的文本到语音系统通常是基于L1/L2损失优化的，这些损失假设了目标数据空间的分布。为了改善这些假设，流体和扩散概率模型最近被提出作为替代方案。在这篇论文中，我们比较了传统的L1/L2基于的方法和扩散和流体基于的方法 для文本到语音合成中的谱和频谱预测任务。我们使用一个谱模型来生成日吸率和持续时间特征，这些特征用于condition一个生成mel-spectrogram的语音模型。实验结果显示，流体基于的模型在spectrogram预测中表现最佳，超过了相当的扩散和L1模型。同时，扩散和流体基于的谱预测模型均导致了对于Typical L2训练的谱模型的显著改进。
</details></li>
</ul>
<hr>
<h2 id="The-World-Literature-Knowledge-Graph"><a href="#The-World-Literature-Knowledge-Graph" class="headerlink" title="The World Literature Knowledge Graph"></a>The World Literature Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16659">http://arxiv.org/abs/2307.16659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elifhaciosmanoglu/PythonL-For-Mathematicians">https://github.com/elifhaciosmanoglu/PythonL-For-Mathematicians</a></li>
<li>paper_authors: Marco Antonio Stranisci, Eleonora Bernasconi, Viviana Patti, Stefano Ferilli, Miguel Ceriani, Rossana Damiano</li>
<li>for: 这篇论文旨在提供一个 semantic resource，用于探索不同地区文学作品和作者的事实。</li>
<li>methods: 该论文使用了3个不同的读者社区的反馈，并将其整合到一个Semantic Model中，以便更好地探索世界文学的知识。</li>
<li>results: 论文通过在线可视化平台提供了194,346名作家和965,210部作品的知识 graph，并在3个不同的专家领域进行了严格的测试和验证，得到了高度的评价和满意度。<details>
<summary>Abstract</summary>
Digital media have enabled the access to unprecedented literary knowledge. Authors, readers, and scholars are now able to discover and share an increasing amount of information about books and their authors. However, these sources of knowledge are fragmented and do not adequately represent non-Western writers and their works. In this paper we present The World Literature Knowledge Graph, a semantic resource containing 194,346 writers and 965,210 works, specifically designed for exploring facts about literary works and authors from different parts of the world. The knowledge graph integrates information about the reception of literary works gathered from 3 different communities of readers, aligned according to a single semantic model. The resource is accessible through an online visualization platform, which can be found at the following URL: https://literaturegraph.di.unito.it/. This platform has been rigorously tested and validated by $3$ distinct categories of experts who have found it to be highly beneficial for their respective work domains. These categories include teachers, researchers in the humanities, and professionals in the publishing industry. The feedback received from these experts confirms that they can effectively utilize the platform to enhance their work processes and achieve valuable outcomes.
</details>
<details>
<summary>摘要</summary>
数字媒体为我们提供了前所未有的文学知识访问。作家、读者和学者现在可以找到和分享越来越多的关于书籍和作家的信息。然而，这些知识来源是分散的，并不充分代表非西方作家和他们的作品。在这篇论文中，我们介绍了世界文学知识图，这是一个基于semantic模型的语义资源，用于探索不同地区的文学作品和作家的事实。知识图集成了来自3个不同社区的读者的受众反馈，并以单一的semantic模型进行对接。这个资源可以通过以下URL访问：https://literaturegraph.di.unito.it/.这个平台已经被3种不同的专家组织rigorously测试和验证，这些专家包括教师、人文科学研究人员和出版业专业人员。这些专家的反馈表明，他们可以通过这个平台增强工作流程，并实现价值的成果。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Sentence-Embeddings-with-Large-Language-Models"><a href="#Scaling-Sentence-Embeddings-with-Large-Language-Models" class="headerlink" title="Scaling Sentence Embeddings with Large Language Models"></a>Scaling Sentence Embeddings with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16645">http://arxiv.org/abs/2307.16645</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kongds/scaling_sentemb">https://github.com/kongds/scaling_sentemb</a></li>
<li>paper_authors: Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, Fuzhen Zhuang</li>
<li>for: 本研究旨在提高句子嵌入性能，通过句子嵌入来提高自然语言处理任务的性能。</li>
<li>methods: 我们提出了一种基于句子嵌入的培训方法，通过适应之前的提问基础表示方法，构建了一个示例集，使得LLMs可以进行上下文学习。我们还对LLMs进行了缩放，以不同的模型大小进行比较。</li>
<li>results: 通过广泛的实验，我们发现在句子嵌入任务上，通过培训LLMs可以获得高质量的句子嵌入，无需任何微调。此外，我们发现，随着模型大小的增加，模型在语义文本相似性（STS）任务上的性能会下降。但是，我们发现最大化模型可以超过其他对手，并达到新的状态态表现在转移任务上。此外，我们还对LLMs进行了现有对比学习方法的微调，并发现2.7B OPT模型，通过我们的提示基础方法，超过了4.8B ST5模型的性能，达到新的状态态表现在STS任务上。<details>
<summary>Abstract</summary>
Large language models (LLMs) have recently garnered significant interest. With in-context learning, LLMs achieve impressive results in various natural language tasks. However, the application of LLMs to sentence embeddings remains an area of ongoing research. In this work, we propose an in-context learning-based method aimed at improving sentence embeddings performance. Our approach involves adapting the previous prompt-based representation method for autoregressive models, constructing a demonstration set that enables LLMs to perform in-context learning, and scaling up the LLMs to different model sizes. Through extensive experiments, in-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve performance comparable to current contrastive learning methods. By scaling model size, we find scaling to more than tens of billion parameters harms the performance on semantic textual similarity (STS) tasks. However, the largest model outperforms other counterparts and achieves the new state-of-the-art result on transfer tasks. We also fine-tune LLMs with current contrastive learning approach, and the 2.7B OPT model, incorporating our prompt-based method, surpasses the performance of 4.8B ST5, achieving the new state-of-the-art results on STS tasks. Our code is available at https://github.com/kongds/scaling_sentemb.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-grapheme-to-phoneme-conversion-by-learning-pronunciations-from-speech-recordings"><a href="#Improving-grapheme-to-phoneme-conversion-by-learning-pronunciations-from-speech-recordings" class="headerlink" title="Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings"></a>Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16643">http://arxiv.org/abs/2307.16643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Sam Ribeiro, Giulia Comini, Jaime Lorenzo-Trueba</li>
<li>for: 提高Grapheme-to-Phoneme（G2P）任务的精度，使其更适合各种语音处理应用程序。</li>
<li>methods: 根据语音录制Example学习发音词典，然后使用这些词典来重新训练G2P系统。</li>
<li>results: 对多种语言和数据量不同的G2P系统，our approach consistently提高了Phone错误率。<details>
<summary>Abstract</summary>
The Grapheme-to-Phoneme (G2P) task aims to convert orthographic input into a discrete phonetic representation. G2P conversion is beneficial to various speech processing applications, such as text-to-speech and speech recognition. However, these tend to rely on manually-annotated pronunciation dictionaries, which are often time-consuming and costly to acquire. In this paper, we propose a method to improve the G2P conversion task by learning pronunciation examples from audio recordings. Our approach bootstraps a G2P with a small set of annotated examples. The G2P model is used to train a multilingual phone recognition system, which then decodes speech recordings with a phonetic representation. Given hypothesized phoneme labels, we learn pronunciation dictionaries for out-of-vocabulary words, and we use those to re-train the G2P system. Results indicate that our approach consistently improves the phone error rate of G2P systems across languages and amount of available data.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a method to improve the G2P conversion task by learning pronunciation examples from audio recordings. We start with a small set of annotated examples and use a G2P model to train a multilingual phone recognition system. This system decodes speech recordings using a phonetic representation. Given hypothesized phoneme labels, we learn pronunciation dictionaries for out-of-vocabulary words and use those to re-train the G2P system.Our approach consistently improves the phone error rate of G2P systems across languages and amounts of available data. This shows that our method is effective in improving the accuracy of G2P conversion.
</details></li>
</ul>
<hr>
<h2 id="VacancySBERT-the-approach-for-representation-of-titles-and-skills-for-semantic-similarity-search-in-the-recruitment-domain"><a href="#VacancySBERT-the-approach-for-representation-of-titles-and-skills-for-semantic-similarity-search-in-the-recruitment-domain" class="headerlink" title="VacancySBERT: the approach for representation of titles and skills for semantic similarity search in the recruitment domain"></a>VacancySBERT: the approach for representation of titles and skills for semantic similarity search in the recruitment domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16638">http://arxiv.org/abs/2307.16638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maiiabocharova/vacancysbert">https://github.com/maiiabocharova/vacancysbert</a></li>
<li>paper_authors: Maiia Bocharova, Eugene Malakhov, Vitaliy Mezhuyev</li>
<li>for: 本研究旨在应用深度学习 semantic search 算法于人力资源领域，开发一种链接 Job 招聘广告中提到的技能与 Job 标题的新方法。</li>
<li>methods: 本研究使用 semantic similarity search 算法来找到匹配 Job 标题和技能的候选者，并采用了预训练语言模型，通过对Title-description对的协同信息进行教学，使模型能够匹配标题和技能。</li>
<li>results: 研究表明，使用自定义训练目标可以实现显著改进，比如使用 VacancySBERT 和 VacancySBERT (with skills) 得到了10% 和 21.5% 的提升。此外，开发了一个开源的基准数据集，以便进一步探索这一领域。<details>
<summary>Abstract</summary>
The paper focuses on deep learning semantic search algorithms applied in the HR domain. The aim of the article is developing a novel approach to training a Siamese network to link the skills mentioned in the job ad with the title. It has been shown that the title normalization process can be based either on classification or similarity comparison approaches. While classification algorithms strive to classify a sample into predefined set of categories, similarity search algorithms take a more flexible approach, since they are designed to find samples that are similar to a given query sample, without requiring pre-defined classes and labels. In this article semantic similarity search to find candidates for title normalization has been used. A pre-trained language model has been adapted while teaching it to match titles and skills based on co-occurrence information. For the purpose of this research fifty billion title-descriptions pairs had been collected for training the model and thirty three thousand title-description-normalized title triplets, where normalized job title was picked up manually by job ad creator for testing purposes. As baselines FastText, BERT, SentenceBert and JobBert have been used. As a metric of the accuracy of the designed algorithm is Recall in top one, five and ten model's suggestions. It has been shown that the novel training objective lets it achieve significant improvement in comparison to other generic and specific text encoders. Two settings with treating titles as standalone strings, and with included skills as additional features during inference have been used and the results have been compared in this article. Improvements by 10% and 21.5% have been achieved using VacancySBERT and VacancySBERT (with skills) respectively. The benchmark has been developed as open-source to foster further research in the area.
</details>
<details>
<summary>摘要</summary>
文章主要研究深度学习 semantic search 算法在人力资源（HR）领域中的应用。文章的目标是开发一种新的方法，通过链接在职位招聘中提到的技能与工作标题之间的连接。研究表明，标题Normalization过程可以基于类别或相似性比较方法。而类别算法尝试将样本分类到预定的类别中，相似性搜索算法则更加灵活，它们可以找到与查询样本相似的样本，无需预定的类别和标签。本文使用semantic similarity搜索来查找候选者。研究人员采用了预训练的语言模型，并将其改进以将标题和技能相匹配，基于共occurrence信息。为了训练模型，收集了50亿个标题-描述对，并使用33,000个标题-描述-Normalized标题 triplets进行测试。作为基准，使用了FastText、BERT、SentenceBert和JobBert。用于评估算法准确性的指标是Recall在top一、五和十个模型建议中。研究表明，新的训练目标可以实现显著改进，相比于其他通用和专门的文本编码器。在使用标题作为独立字符串和包含技能作为推断时进行两种设置后，对比结果。使用VacancySBERT和VacancySBERT（与技能）后，分别实现了10%和21.5%的提高。研究人员开发了一个开源的标准套件，以便进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="Text-CRS-A-Generalized-Certified-Robustness-Framework-against-Textual-Adversarial-Attacks"><a href="#Text-CRS-A-Generalized-Certified-Robustness-Framework-against-Textual-Adversarial-Attacks" class="headerlink" title="Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks"></a>Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16630">http://arxiv.org/abs/2307.16630</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Eyr3/TextCRS">https://github.com/Eyr3/TextCRS</a></li>
<li>paper_authors: Xinyu Zhang, Hanbin Hong, Yuan Hong, Peng Huang, Binghui Wang, Zhongjie Ba, Kui Ren</li>
<li>for: 防止文本 adversarial 攻击，提高模型robustness</li>
<li>methods: 使用randomized smoothing方法， derive robustness bounds against four word-level adversarial operations</li>
<li>results: Text-CRS可以Address all four different word-level adversarial operations，significantly improve certified accuracy and radius，outperform state-of-the-art certification against synonym substitution attacks，provide the first benchmark on certified accuracy and radius of four word-level operations.<details>
<summary>Abstract</summary>
The language models, especially the basic text classification models, have been shown to be susceptible to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving the model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\ell_0$ perturbations in synonym substitution attacks. Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing. Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.
</details>
<details>
<summary>摘要</summary>
Language models, especially basic text classification models, have been shown to be vulnerable to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\ell_0$ perturbations in synonym substitution attacks.Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing.Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.
</details></li>
</ul>
<hr>
<h2 id="Noisy-Self-Training-with-Data-Augmentations-for-Offensive-and-Hate-Speech-Detection-Tasks"><a href="#Noisy-Self-Training-with-Data-Augmentations-for-Offensive-and-Hate-Speech-Detection-Tasks" class="headerlink" title="Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks"></a>Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16609">http://arxiv.org/abs/2307.16609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jaugusto97/offense-self-training">https://github.com/jaugusto97/offense-self-training</a></li>
<li>paper_authors: João A. Leite, Carolina Scarton, Diego F. Silva</li>
<li>for: Automatic detection of offensive and hateful comments in online social media.</li>
<li>methods: Self-training and noisy self-training using textual data augmentations with five pre-trained BERT architectures.</li>
<li>results: Self-training consistently improves performance, while noisy self-training decreases performance on offensive and hate-speech domains.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在自动检测社交媒体上的侮辱和恐吓言论。</li>
<li>methods: 使用自我训练和含杂自我训练，使用文本数据增强技术，采用五种不同的预训练BERT架构。</li>
<li>results: 自我训练可以一直提高表现，而含杂自我训练在侮辱和恐吓言论领域的表现下降。<details>
<summary>Abstract</summary>
Online social media is rife with offensive and hateful comments, prompting the need for their automatic detection given the sheer amount of posts created every second. Creating high-quality human-labelled datasets for this task is difficult and costly, especially because non-offensive posts are significantly more frequent than offensive ones. However, unlabelled data is abundant, easier, and cheaper to obtain. In this scenario, self-training methods, using weakly-labelled examples to increase the amount of training data, can be employed. Recent "noisy" self-training approaches incorporate data augmentation techniques to ensure prediction consistency and increase robustness against noisy data and adversarial attacks. In this paper, we experiment with default and noisy self-training using three different textual data augmentation techniques across five different pre-trained BERT architectures varying in size. We evaluate our experiments on two offensive/hate-speech datasets and demonstrate that (i) self-training consistently improves performance regardless of model size, resulting in up to +1.5% F1-macro on both datasets, and (ii) noisy self-training with textual data augmentations, despite being successfully applied in similar settings, decreases performance on offensive and hate-speech domains when compared to the default method, even with state-of-the-art augmentations such as backtranslation.
</details>
<details>
<summary>摘要</summary>
在线社交媒体中，有许多内容具有攻击性和恐惧语气，导致自动检测的需求，因为每秒钟有极多的创建。然而，人工标注数据实际上很难以取得，特别是非攻击性内容比攻击性内容更多。在这种情况下，自我训练方法可以使用弱标注的例子增加训练数据的量。现代的“杂音”自我训练方法将数据扩展技术纳入训练，以确保预测的一致性和抗衰变攻击的强健性。在这篇文章中，我们将实验 default 和杂音自我训练，使用三种文本数据扩展技术，在五种不同的预读BERT架构上进行评估。我们在两个攻击和负面语气dataset上进行评估，结果显示：（i）自我训练无变通过所有模型大小，实现最高 +1.5% F1-macro 的改善，（ii）杂音自我训练对于攻击和负面语气领域的性能下降，即使使用了现代的数据扩展技术，如回译。
</details></li>
</ul>
<hr>
<h2 id="Deep-Dive-into-the-Language-of-International-Relations-NLP-based-Analysis-of-UNESCO’s-Summary-Records"><a href="#Deep-Dive-into-the-Language-of-International-Relations-NLP-based-Analysis-of-UNESCO’s-Summary-Records" class="headerlink" title="Deep Dive into the Language of International Relations: NLP-based Analysis of UNESCO’s Summary Records"></a>Deep Dive into the Language of International Relations: NLP-based Analysis of UNESCO’s Summary Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16573">http://arxiv.org/abs/2307.16573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joanna Wojciechowska, Mateusz Sypniewski, Maria Śmigielska, Igor Kamiński, Emilia Wiśnios, Hanna Schreiber, Bartosz Pieliński</li>
<li>for: 该论文旨在研究UNESCO世界遗产名录和联合国教科文组织非物质文化遗产名录的投票过程中的紧张关系和冲突，以及开发自动化工具来提供有价值的决策过程分析。</li>
<li>methods: 该研究使用创新的话题模型和紧张度探测方法，基于UNESCO的摘要记录，实现了72%的准确率在检测紧张关系。此外，研究人员还开发了一个专门为外交官、律师、政治科学家和国际关系研究人员设计的应用程序，以便高效搜索选定文档和特定发言人的话题。</li>
<li>results: 该研究的结果表明，自动化工具可以提供有价值的决策过程分析，帮助解决国际遗产投票过程中的紧张关系和冲突。<details>
<summary>Abstract</summary>
Cultural heritage is an arena of international relations that interests all states worldwide. The inscription process on the UNESCO World Heritage List and the UNESCO Representative List of the Intangible Cultural Heritage of Humanity often leads to tensions and conflicts among states. This research addresses these challenges by developing automatic tools that provide valuable insights into the decision-making processes regarding inscriptions to the two lists mentioned above. We propose innovative topic modelling and tension detection methods based on UNESCO's summary records. Our analysis achieved a commendable accuracy rate of 72% in identifying tensions. Furthermore, we have developed an application tailored for diplomats, lawyers, political scientists, and international relations researchers that facilitates the efficient search of paragraphs from selected documents and statements from specific speakers about chosen topics. This application is a valuable resource for enhancing the understanding of complex decision-making dynamics within international heritage inscription procedures.
</details>
<details>
<summary>摘要</summary>
文化遗产是国际关系的一个领域，各国都很关心。联合国教科文组织世界遗产名录和联合国教科文组织非物质文化遗产名录的登记过程经常导致国家之间的紧张关系和冲突。本研究通过开发自动化工具，为各国帮助解决这些挑战。我们提出了创新的话题模型和紧张检测方法，基于联合国教科文组织的摘要记录。我们的分析达到了72%的准确率，可以快速寻找关键话题和紧张关系。此外，我们开发了一个专门为外交官、律师、政治科学家和国际关系研究人员设计的应用程序，可以帮助这些人快速搜索选择的文档和来自特定发言人的声明中的特定话题。这个应用程序是国际遗产登记过程中复杂决策动力理解的重要资源。
</details></li>
</ul>
<hr>
<h2 id="DiffProsody-Diffusion-based-Latent-Prosody-Generation-for-Expressive-Speech-Synthesis-with-Prosody-Conditional-Adversarial-Training"><a href="#DiffProsody-Diffusion-based-Latent-Prosody-Generation-for-Expressive-Speech-Synthesis-with-Prosody-Conditional-Adversarial-Training" class="headerlink" title="DiffProsody: Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training"></a>DiffProsody: Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16549">http://arxiv.org/abs/2307.16549</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsoh0306/diffprosody">https://github.com/hsoh0306/diffprosody</a></li>
<li>paper_authors: Hyung-Seok Oh, Sang-Hoon Lee, Seong-Whan Lee</li>
<li>for: This paper focuses on improving the quality and speed of expressive text-to-speech systems through the use of a diffusion-based latent prosody generator and prosody conditional adversarial training.</li>
<li>methods: The proposed method, called DiffProsody, uses a diffusion-based latent prosody generator and prosody conditional adversarial training to generate high-quality speech with accurate prosody. The method also utilizes denoising diffusion generative adversarial networks to improve the prosody generation speed.</li>
<li>results: The paper demonstrates the effectiveness of the proposed method through experiments, showing that DiffProsody is capable of generating prosody 16 times faster than the conventional diffusion model, with superior performance compared to other state-of-the-art methods.<details>
<summary>Abstract</summary>
Expressive text-to-speech systems have undergone significant advancements owing to prosody modeling, but conventional methods can still be improved. Traditional approaches have relied on the autoregressive method to predict the quantized prosody vector; however, it suffers from the issues of long-term dependency and slow inference. This study proposes a novel approach called DiffProsody in which expressive speech is synthesized using a diffusion-based latent prosody generator and prosody conditional adversarial training. Our findings confirm the effectiveness of our prosody generator in generating a prosody vector. Furthermore, our prosody conditional discriminator significantly improves the quality of the generated speech by accurately emulating prosody. We use denoising diffusion generative adversarial networks to improve the prosody generation speed. Consequently, DiffProsody is capable of generating prosody 16 times faster than the conventional diffusion model. The superior performance of our proposed method has been demonstrated via experiments.
</details>
<details>
<summary>摘要</summary>
现代文本到语音系统已经经历了重要的进步，归功于谱系模型。然而，传统方法仍然有可以改进的地方。传统方法通常采用推论方法来预测量化的谱系 вектор;然而，它受到长期依赖和慢速推理的问题困扰。本研究提出了一种新的方法，即DiffProsody，用于生成表达性的语音。我们的发现表明，我们的谱系生成器可以生成高质量的谱系 вектор。此外，我们的谱系条件推论器可以准确地模拟谱系，从而提高生成的语音质量。我们使用denoising扩散生成 adversarial networks来提高谱系生成速度。因此，DiffProsody可以在16倍的速度上生成谱系。我们的实验结果表明，我们的提议的方法在性能上有superior的表现。
</details></li>
</ul>
<hr>
<h2 id="Specification-of-MiniDemographicABM-jl-A-simplified-agent-based-demographic-model-of-the-UK"><a href="#Specification-of-MiniDemographicABM-jl-A-simplified-agent-based-demographic-model-of-the-UK" class="headerlink" title="Specification of MiniDemographicABM.jl: A simplified agent-based demographic model of the UK"></a>Specification of MiniDemographicABM.jl: A simplified agent-based demographic model of the UK</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16548">http://arxiv.org/abs/2307.16548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atiyah Elsheikh</li>
<li>For: The paper is written for exploring and exploiting the capabilities of the state-of-the-art Agents.jl Julia package in a simplified non-calibrated agent-based demographic model of the UK.* Methods: The paper uses a simplified non-calibrated agent-based demographic model of the UK, where individuals are subject to ageing, deaths, births, divorces, and marriages. The model can be simulated with a user-defined simulation fixed step size on a hourly, daily, weekly, monthly basis or even an arbitrary user-defined clock rate.* Results: The paper can serve as a base model to be adjusted to realistic large-scale socio-economics, pandemics or social interactions-based studies mainly within a demographic context.Here is the same information in Simplified Chinese text:* For: 本文是用于探索和利用现代Agents.jl Julia包的能力的简化非参数化人工智能模型，用于研究英国人口的特点。* Methods: 该模型使用简化非参数化人工智能模型，其中个体受到年龄、死亡、生育、离婚和婚姻的影响。模型可以通过用户定义的 simulation fixed step size 进行模拟，并且可以在每小时、每天、每周、每月基础或者用户定义的时间刻度进行模拟。* Results: 该模型可以作为基本模型，用于调整大规模的社会经济、疫情或者社交互动等研究，主要在人口学上。<details>
<summary>Abstract</summary>
This document presents adequate formal terminology for the mathematical specification of a simplified non-calibrated agent-based demographic model of the UK. Individuals of an initial population are subject to ageing, deaths, births, divorces and marriages. The main purpose of the model is to explore and exploit capabilities of the state-of-the-art Agents.jl Julia package [1]. Additionally, the model can serve as a base model to be adjusted to realistic large-scale socio-economics, pandemics or social interactions-based studies mainly within a demographic context. A specific simulation is progressed with a user-defined simulation fixed step size on a hourly, daily, weekly, monthly basis or even an arbitrary user-defined clock rate.
</details>
<details>
<summary>摘要</summary>
Translation Notes:1. "non-calibrated" 不是 "calibrated" 的反义词。"non-calibrated" 是指模型没有进行过精度调整，而"calibrated" 则是指模型已经进行了精度调整。2. "agent-based" 是指模型使用代理（agent）来表示实体，而不是使用固定的数值或函数来描述实体。3. "demographic" 是指人口的统计学性质，包括年龄、性别、地域等。4. "simulation" 是指模拟或复制实际情况的过程，通常使用计算机模拟实现。5. "fixed step size" 是指每次执行模拟时，使用固定的时间间隔（step size）来执行计算。6. "user-defined" 是指用户可以自定义的参数或设置，例如 simulation clock rate。
</details></li>
</ul>
<hr>
<h2 id="Utilisation-of-open-intent-recognition-models-for-customer-support-intent-detection"><a href="#Utilisation-of-open-intent-recognition-models-for-customer-support-intent-detection" class="headerlink" title="Utilisation of open intent recognition models for customer support intent detection"></a>Utilisation of open intent recognition models for customer support intent detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16544">http://arxiv.org/abs/2307.16544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rasheed Mohammad, Oliver Favell, Shariq Shah, Emmett Cooper, Edlira Vakaj</li>
<li>for: 这篇论文旨在探讨客户支持方面的人工智能应用，以帮助企业提供更好的客户服务，提高客户满意度。</li>
<li>methods: 该论文使用了社交媒体、人工智能（AI）、机器学习（ML）等技术，以及远程设备连接，对客户进行更加快速、高效和专业的支持。</li>
<li>results: 研究表明，使用这些技术可以提高客户支持效率和准确率，同时为企业提供更多的国际客户和业务规模。然而，在检测未知意图方面，还需要进一步的研究和改进。<details>
<summary>Abstract</summary>
Businesses have sought out new solutions to provide support and improve customer satisfaction as more products and services have become interconnected digitally. There is an inherent need for businesses to provide or outsource fast, efficient and knowledgeable support to remain competitive. Support solutions are also advancing with technologies, including use of social media, Artificial Intelligence (AI), Machine Learning (ML) and remote device connectivity to better support customers. Customer support operators are trained to utilise these technologies to provide better customer outreach and support for clients in remote areas. Interconnectivity of products and support systems provide businesses with potential international clients to expand their product market and business scale. This paper reports the possible AI applications in customer support, done in collaboration with the Knowledge Transfer Partnership (KTP) program between Birmingham City University and a company that handles customer service systems for businesses outsourcing customer support across a wide variety of business sectors. This study explored several approaches to accurately predict customers' intent using both labelled and unlabelled textual data. While some approaches showed promise in specific datasets, the search for a single, universally applicable approach continues. The development of separate pipelines for intent detection and discovery has led to improved accuracy rates in detecting known intents, while further work is required to improve the accuracy of intent discovery for unknown intents.
</details>
<details>
<summary>摘要</summary>
This paper reports on the possible AI applications in customer support, done in collaboration with the Knowledge Transfer Partnership (KTP) program between Birmingham City University and a company that handles customer service systems for businesses outsourcing customer support across a wide variety of business sectors. This study explored several approaches to accurately predict customers' intent using both labeled and unlabeled textual data. While some approaches showed promise in specific datasets, the search for a single, universally applicable approach continues. The development of separate pipelines for intent detection and discovery has led to improved accuracy rates in detecting known intents, while further work is required to improve the accuracy of intent discovery for unknown intents.
</details></li>
</ul>
<hr>
<h2 id="Transferable-Decoding-with-Visual-Entities-for-Zero-Shot-Image-Captioning"><a href="#Transferable-Decoding-with-Visual-Entities-for-Zero-Shot-Image-Captioning" class="headerlink" title="Transferable Decoding with Visual Entities for Zero-Shot Image Captioning"></a>Transferable Decoding with Visual Entities for Zero-Shot Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16525">http://arxiv.org/abs/2307.16525</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feielysia/viecap">https://github.com/feielysia/viecap</a></li>
<li>paper_authors: Junjie Fei, Teng Wang, Jinrui Zhang, Zhenyu He, Chengjie Wang, Feng Zheng</li>
<li>for: This paper aims to improve image-to-text generation by addressing the problem of object hallucination in zero-shot image captioning.</li>
<li>methods: The proposed method, ViECap, uses entity-aware decoding to guide the attention of large language models (LLMs) toward the visual entities present in the image, improving the coherence and accuracy of the generated captions.</li>
<li>results: Extensive experiments show that ViECap sets a new state-of-the-art cross-domain (transferable) captioning performance and performs competitively in-domain captioning compared to previous VLMs-based zero-shot methods.<details>
<summary>Abstract</summary>
Image-to-text generation aims to describe images using natural language. Recently, zero-shot image captioning based on pre-trained vision-language models (VLMs) and large language models (LLMs) has made significant progress. However, we have observed and empirically demonstrated that these methods are susceptible to modality bias induced by LLMs and tend to generate descriptions containing objects (entities) that do not actually exist in the image but frequently appear during training (i.e., object hallucination). In this paper, we propose ViECap, a transferable decoding model that leverages entity-aware decoding to generate descriptions in both seen and unseen scenarios. ViECap incorporates entity-aware hard prompts to guide LLMs' attention toward the visual entities present in the image, enabling coherent caption generation across diverse scenes. With entity-aware hard prompts, ViECap is capable of maintaining performance when transferring from in-domain to out-of-domain scenarios. Extensive experiments demonstrate that ViECap sets a new state-of-the-art cross-domain (transferable) captioning and performs competitively in-domain captioning compared to previous VLMs-based zero-shot methods. Our code is available at: https://github.com/FeiElysia/ViECap
</details>
<details>
<summary>摘要</summary>
Image-to-text生成旨在使用自然语言描述图像。最近，零批学习图像描述基于预训练视觉语言模型（VLM）和大型语言模型（LLM）已经取得了重要进展。然而，我们观察到和实际示出了这些方法受模式偏见（modality bias）的LLM的影响，往往生成包含图像中不存在的对象（实体）的描述（对象幻觉）。在本文中，我们提出了ViECap，一种可移植的解码器，利用实体意识的解码来生成在seen和unseen场景中的描述。ViECap使用实体意识强制提示来导引LLM的视觉注意力，使其能够在多样场景中生成准确的描述。与传统的VLMs-based零shot方法相比，ViECap在跨频道场景中维持性能，并在域内场景中表现竞争力。我们的代码可以在GitHub上找到：https://github.com/FeiElysia/ViECap
</details></li>
</ul>
<hr>
<h2 id="Classifying-multilingual-party-manifestos-Domain-transfer-across-country-time-and-genre"><a href="#Classifying-multilingual-party-manifestos-Domain-transfer-across-country-time-and-genre" class="headerlink" title="Classifying multilingual party manifestos: Domain transfer across country, time, and genre"></a>Classifying multilingual party manifestos: Domain transfer across country, time, and genre</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16511">http://arxiv.org/abs/2307.16511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slds-lmu/manifesto-domaintransfer">https://github.com/slds-lmu/manifesto-domaintransfer</a></li>
<li>paper_authors: Matthias Aßenmacher, Nadja Sauter, Christian Heumann</li>
<li>for: 本研究旨在探讨域传递在政治宣言中的可靠性和可重用性。</li>
<li>methods: 研究使用了大量政治宣言数据库，并对模型进行了精细调整。</li>
<li>results: 研究发现，使用(Distil)BERT模型可以在不同语言、地域、时间和类型的政治宣言中实现类似的表现。此外，研究还发现了不同国家的政治宣言之间存在一定的差异，即使这些国家使用同一种语言或文化背景。<details>
<summary>Abstract</summary>
Annotating costs of large corpora are still one of the main bottlenecks in empirical social science research. On the one hand, making use of the capabilities of domain transfer allows re-using annotated data sets and trained models. On the other hand, it is not clear how well domain transfer works and how reliable the results are for transfer across different dimensions. We explore the potential of domain transfer across geographical locations, languages, time, and genre in a large-scale database of political manifestos. First, we show the strong within-domain classification performance of fine-tuned transformer models. Second, we vary the genre of the test set across the aforementioned dimensions to test for the fine-tuned models' robustness and transferability. For switching genres, we use an external corpus of transcribed speeches from New Zealand politicians while for the other three dimensions, custom splits of the Manifesto database are used. While BERT achieves the best scores in the initial experiments across modalities, DistilBERT proves to be competitive at a lower computational expense and is thus used for further experiments across time and country. The results of the additional analysis show that (Distil)BERT can be applied to future data with similar performance. Moreover, we observe (partly) notable differences between the political manifestos of different countries of origin, even if these countries share a language or a cultural background.
</details>
<details>
<summary>摘要</summary>
大公司的标注成本仍是employmultiple的社会科学研究的主要瓶颈。一方面，利用领域传输的能力可以重用标注数据集和训练模型。另一方面，不清楚领域传输是如何工作，结果如何可靠性。我们在一个大规模的政治宣言数据库中探索领域传输的潜力。首先，我们显示了在不同领域内的精度转换模型的强大表现。其次，我们在不同维度上随机选择测试集，以测试精度转换模型的可靠性和可迁移性。为了在类别之间转换，我们使用新西兰政治人物的演讲录音库，而其他三个维度使用自定义的演示数据。虽然BERT在初始实验中Across modalities achieve the best scores，但DistilBERT在更低的计算成本下能够达到类似的性能，因此我们在时间和国家之间进行了进一步的实验。results of the additional analysis show that (Distil)BERT can be applied to future data with similar performance.更重要的是，我们发现了不同国家的政治宣言之间有些 notable differences，即使这些国家共享语言或文化背景。
</details></li>
</ul>
<hr>
<h2 id="FinVis-GPT-A-Multimodal-Large-Language-Model-for-Financial-Chart-Analysis"><a href="#FinVis-GPT-A-Multimodal-Large-Language-Model-for-Financial-Chart-Analysis" class="headerlink" title="FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis"></a>FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01430">http://arxiv.org/abs/2308.01430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziao Wang, Yuhang Li, Junda Wu, Jaehyeon Soon, Xiaofeng Zhang</li>
<li>for:  FinVis-GPT is proposed for financial chart analysis, providing valuable analysis and interpretation of financial charts.</li>
<li>methods: FinVis-GPT uses a multimodal large language model (LLM) with instruction tuning and multimodal capabilities to analyze financial charts.</li>
<li>results: FinVis-GPT demonstrates superior performance in various financial chart related tasks, including generating descriptions, answering questions, and predicting future market trends, compared to existing state-of-the-art multimodal LLMs.Here is the text in Simplified Chinese:</li>
<li>for:  FinVis-GPT 是为金融图表分析而提出的一种新型多Modal大语言模型，它可以准确地解读和分析金融图表。</li>
<li>methods: FinVis-GPT 使用了多Modal大语言模型（LLM），并通过指令调整和多Modal能力来分析金融图表。</li>
<li>results: FinVis-GPT 在多种金融图表相关任务中表现出色，包括生成描述、回答问题和预测未来市场趋势，比现有的多Modal LLM 更高效。<details>
<summary>Abstract</summary>
In this paper, we propose FinVis-GPT, a novel multimodal large language model (LLM) specifically designed for financial chart analysis. By leveraging the power of LLMs and incorporating instruction tuning and multimodal capabilities, FinVis-GPT is capable of interpreting financial charts and providing valuable analysis. To train FinVis-GPT, a financial task oriented dataset was generated for pre-training alignment and instruction tuning, comprising various types of financial charts and their corresponding descriptions. We evaluate the model performance via several case studies due to the time limit, and the promising results demonstrated that FinVis-GPT is superior in various financial chart related tasks, including generating descriptions, answering questions and predicting future market trends, surpassing existing state-of-the-art multimodal LLMs. The proposed FinVis-GPT serves as a pioneering effort in utilizing multimodal LLMs in the finance domain and our generated dataset will be release for public use in the near future to speedup related research.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了 FinVis-GPT，一种新型的多Modal大语言模型（LLM），专门用于金融图表分析。通过利用LLM的力量和多Modal特性，FinVis-GPT可以解读金融图表并提供有价值的分析。为了训练FinVis-GPT，我们生成了一个金融任务指向数据集，用于预训练对齐和指令调整，其包括各种金融图表和其对应的描述。我们通过一些案例研究评估了模型性能，结果表明FinVis-GPT在各种金融图表相关任务中表现出色，包括生成描述、回答问题和预测未来市场趋势，超越现有的多Modal LLMs。我们的提出的FinVis-GPT是金融领域中首次利用多Modal LLMs的先河，我们将在近 future中发布生成的数据集，以便加速相关研究。
</details></li>
</ul>
<hr>
<h2 id="A-new-mapping-of-technological-interdependence"><a href="#A-new-mapping-of-technological-interdependence" class="headerlink" title="A new mapping of technological interdependence"></a>A new mapping of technological interdependence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00014">http://arxiv.org/abs/2308.00014</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Fronzetti Colladon, B. Guardabascio, F. Venturini</li>
<li>for: 本研究旨在探讨技术链接对产业创新能力的影响，以及这些影响如何在技术空间传递。</li>
<li>methods: 本研究使用文本挖掘和网络分析新方法，分析1976-2021年美国专利和商标局（USPTO）授权的650万个专利文本，揭示技术链接的全谱系。</li>
<li>results: 研究发现，专利文本含有许多不被传统创新指标捕捉的信息，如专利引用。网络分析表明，间接链接与直接连接相当重要，而传统间接链接指标，如列 Ontief inverse matrix，仅仅捕捉了一部分间接链接。最后，基于冲击分析，我们示出技术衰退如何在技术空间传递，影响产业创新能力。<details>
<summary>Abstract</summary>
Which technological linkages affect the sector's ability to innovate? How do these effects transmit through the technology space? This paper answers these two key questions using novel methods of text mining and network analysis. We examine technological interdependence across sectors over a period of half a century (from 1976 to 2021) by analyzing the text of 6.5 million patents granted by the United States Patent and Trademark Office (USPTO), and applying network analysis to uncover the full spectrum of linkages existing across technology areas. We demonstrate that patent text contains a wealth of information often not captured by traditional innovation metrics, such as patent citations. By using network analysis, we document that indirect linkages are as important as direct connections and that the former would remain mostly hidden using more traditional measures of indirect linkages, such as the Leontief inverse matrix. Finally, based on an impulse-response analysis, we illustrate how technological shocks transmit through the technology (network-based) space, affecting the innovation capacity of the sectors.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这个文章使用新的文本挖掘和网络分析方法回答了两个关键问题：一是技术链接对产业创新能力产生影响，二是这些影响如何在技术空间传递？我们在1976年至2021年的半个世纪时间内分析了美国专利与商标局（USPTO）授权的650万个专利文本，并通过网络分析揭示出技术领域之间的全谱连接。我们发现专利文本含有许多不被传统创新指标捕捉的信息，例如专利引用。通过网络分析，我们证明了间接链接与直接连接具有相同的重要性，并且前者通常会通过传统的间接链接指标，如Leontief反对矩阵，被遗弃。最后，我们通过冲击回响分析，示出技术冲击如何在技术空间传递，影响产业的创新能力。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-for-Understanding-Dialogue-Safety-in-Mental-Health-Support"><a href="#A-Benchmark-for-Understanding-Dialogue-Safety-in-Mental-Health-Support" class="headerlink" title="A Benchmark for Understanding Dialogue Safety in Mental Health Support"></a>A Benchmark for Understanding Dialogue Safety in Mental Health Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16457">http://arxiv.org/abs/2307.16457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiuhuachuan/dialoguesafety">https://github.com/qiuhuachuan/dialoguesafety</a></li>
<li>paper_authors: Huachuan Qiu, Tong Zhao, Anqi Li, Shuai Zhang, Hongliang He, Zhenzhong Lan</li>
<li>For: The paper aims to develop a theoretically and factually grounded taxonomy for analyzing response safety in mental health support, and to create a benchmark corpus with fine-grained labels for each dialogue session.* Methods: The paper uses a zero- and few-shot learning approach with popular language models, including BERT-base, RoBERTa-large, and ChatGPT, to detect and understand unsafe responses within the context of mental health support.* Results: The study reveals that ChatGPT struggles to detect safety categories with detailed safety definitions in a zero- and few-shot paradigm, whereas the fine-tuned model proves to be more suitable. The developed dataset and findings serve as valuable benchmarks for advancing research on dialogue safety in mental health support.<details>
<summary>Abstract</summary>
Dialogue safety remains a pervasive challenge in open-domain human-machine interaction. Existing approaches propose distinctive dialogue safety taxonomies and datasets for detecting explicitly harmful responses. However, these taxonomies may not be suitable for analyzing response safety in mental health support. In real-world interactions, a model response deemed acceptable in casual conversations might have a negligible positive impact on users seeking mental health support. To address these limitations, this paper aims to develop a theoretically and factually grounded taxonomy that prioritizes the positive impact on help-seekers. Additionally, we create a benchmark corpus with fine-grained labels for each dialogue session to facilitate further research. We analyze the dataset using popular language models, including BERT-base, RoBERTa-large, and ChatGPT, to detect and understand unsafe responses within the context of mental health support. Our study reveals that ChatGPT struggles to detect safety categories with detailed safety definitions in a zero- and few-shot paradigm, whereas the fine-tuned model proves to be more suitable. The developed dataset and findings serve as valuable benchmarks for advancing research on dialogue safety in mental health support, with significant implications for improving the design and deployment of conversation agents in real-world applications. We release our code and data here: https://github.com/qiuhuachuan/DialogueSafety.
</details>
<details>
<summary>摘要</summary>
对话安全问题在开放领域人机交互中仍然是一个普遍的挑战。现有的方法提出了不同的对话安全分类和数据集来检测直接危害性的回答。然而，这些分类可能并不适用于分析心理支持中的回答安全性。在实际交互中，一个被认为在互助会话中可以得到积极影响的回答可能并不适用于用户寻求心理支持。为了解决这些限制，本研究旨在开发一个基于理论和实际的对话安全分类，并创建一个具有细化标签的对话会话数据集，以便进一步的研究。我们使用了流行的语言模型，包括BERT-base、RoBERTa-large和ChatGPT，对心理支持中的对话安全进行检测和理解。我们的研究发现，ChatGPT在零和几个shot情况下很难检测安全类别，而精心调整的模型却表现出色。我们开发的数据集和发现将为对话安全在心理支持中的研究提供价值的标准，对实际应用中的对话机器人设计和部署产生重要影响。我们在 GitHub 上发布了代码和数据：https://github.com/qiuhuachuan/DialogueSafety。
</details></li>
</ul>
<hr>
<h2 id="Camoscio-an-Italian-Instruction-tuned-LLaMA"><a href="#Camoscio-an-Italian-Instruction-tuned-LLaMA" class="headerlink" title="Camoscio: an Italian Instruction-tuned LLaMA"></a>Camoscio: an Italian Instruction-tuned LLaMA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16456">http://arxiv.org/abs/2307.16456</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teelinsan/camoscio">https://github.com/teelinsan/camoscio</a></li>
<li>paper_authors: Andrea Santilli, Emanuele Rodolà</li>
<li>for: 本研究旨在提供一个特定 для意大利语言的语言模型，以便实现该语言的自然语言处理任务。</li>
<li>methods: 本研究使用了LLaMA的最小型别（7b），并通过LoRA进行精细调整，以便在意大利语言下跟随用户的指令。</li>
<li>results: 本研究的结果显示，模型在意大利语言下的零执行性能在各种下游任务中竞争性地与现有特定 для这些任务的模型竞争。<details>
<summary>Abstract</summary>
In recent years Large Language Models (LLMs) have increased the state of the art on several natural language processing tasks. However, their accessibility is often limited to paid API services, posing challenges for researchers in conducting extensive investigations. On the other hand, while some open-source models have been proposed by the community, they are typically multilingual and not specifically tailored for the Italian language. In an effort to democratize the available and open resources for the Italian language, in this paper we introduce Camoscio: a language model specifically tuned to follow users' prompts in Italian. Specifically, we finetuned the smallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts translated to Italian via ChatGPT. Results indicate that the model's zero-shot performance on various downstream tasks in Italian competes favorably with existing models specifically finetuned for those tasks. All the artifacts (code, dataset, model) are released to the community at the following url: https://github.com/teelinsan/camoscio
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DCTM-Dilated-Convolutional-Transformer-Model-for-Multimodal-Engagement-Estimation-in-Conversation"><a href="#DCTM-Dilated-Convolutional-Transformer-Model-for-Multimodal-Engagement-Estimation-in-Conversation" class="headerlink" title="DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation"></a>DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01966">http://arxiv.org/abs/2308.01966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vu Ngoc Tu, Van Thong Huynh, Hyung-Jeong Yang, M. Zaigham Zaheer, Shah Nawaz, Karthik Nandakumar, Soo-Hyung Kim</li>
<li>for: 本研究的目的是提取对话中人们的互动情况，以获得对人际交流的理解和行为模式的洞察。</li>
<li>methods: 本研究使用了扩展 convolutional Transformer 来模型和估计对话中人们的参与度。我们还使用了不同的模态融合方法，并发现在这种数据上，简单的 concatenated 方法加上自注意融合得到了最好的表现。</li>
<li>results: 我们的提案系统在测试集上比基eline模型高$7%$，在验证集上高$4%$。<details>
<summary>Abstract</summary>
Conversational engagement estimation is posed as a regression problem, entailing the identification of the favorable attention and involvement of the participants in the conversation. This task arises as a crucial pursuit to gain insights into human's interaction dynamics and behavior patterns within a conversation. In this research, we introduce a dilated convolutional Transformer for modeling and estimating human engagement in the MULTIMEDIATE 2023 competition. Our proposed system surpasses the baseline models, exhibiting a noteworthy $7$\% improvement on test set and $4$\% on validation set. Moreover, we employ different modality fusion mechanism and show that for this type of data, a simple concatenated method with self-attention fusion gains the best performance.
</details>
<details>
<summary>摘要</summary>
文本对话参与度估计问题 pose 为回归问题，涉及到参与者在对话中有利的注意力和参与度的识别。这项任务对于了解人类对话动力学和行为模式具有重要意义。在这项研究中，我们提出了一种扩展 convolutional Transformer 来模型和估计对话参与度，并在 MULTIMEDIATE 2023 比赛中提交了我们的提案。我们的提案比基eline模型表现出了显著的 $7\%$ 提高（测试集）和 $4\%$ 提高（验证集）。此外，我们采用了不同的modalities fusion方法，并证明在这类数据上，简单 concatenation 方法配合自我注意力融合可以获得最佳性能。
</details></li>
</ul>
<hr>
<h2 id="SelfSeg-A-Self-supervised-Sub-word-Segmentation-Method-for-Neural-Machine-Translation"><a href="#SelfSeg-A-Self-supervised-Sub-word-Segmentation-Method-for-Neural-Machine-Translation" class="headerlink" title="SelfSeg: A Self-supervised Sub-word Segmentation Method for Neural Machine Translation"></a>SelfSeg: A Self-supervised Sub-word Segmentation Method for Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16400">http://arxiv.org/abs/2307.16400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiyue Song, Raj Dabre, Chenhui Chu, Sadao Kurohashi, Eiichiro Sumita</li>
<li>for: This paper is written for the purpose of proposing a self-supervised neural sub-word segmentation method called SelfSeg, which is faster and more efficient than existing methods.</li>
<li>methods: The paper uses a self-supervised approach that takes as input a word in the form of a partially masked character sequence, optimizes the word generation probability, and generates the segmentation with the maximum posterior probability using a dynamic programming algorithm. The training time of SelfSeg depends on word frequencies, and the paper explores several word frequency normalization strategies to accelerate the training phase.</li>
<li>results: The paper conducts machine translation experiments in low-, middle-, and high-resource scenarios, comparing the performance of different segmentation methods. The results show that SelfSeg achieves significant improvements over existing methods, including BPE and SentencePiece, and the regularization method achieves approximately a 4.3 BLEU score improvement over BPE and a 1.2 BLEU score improvement over BPE-dropout. The paper also observes improvements on several other datasets.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提出一种自动学习的字符级别分词法，它比现有的方法更快速和高效。</li>
<li>methods: 这篇论文使用了一种自动学习的方法，输入一个部分遮盖的单词，优化单词生成概率，并使用动态规划算法计算最大 posterior probability，生成分词结果。训练时间取决于单词频率，论文探索了多种单词频率normalization策略来加速训练阶段。</li>
<li>results: 论文通过对机器翻译 task进行实验，在低资源、中资源和高资源的场景中比较了不同的分词方法的性能。结果表明，SelfSeg在ALT dataset上获得了1.2 BLEU分数的提升，而与DPE和VOLT相比，增加了约4.3 BLEU分数。论文还发现了其他一些数据集上的显著提升。<details>
<summary>Abstract</summary>
Sub-word segmentation is an essential pre-processing step for Neural Machine Translation (NMT). Existing work has shown that neural sub-word segmenters are better than Byte-Pair Encoding (BPE), however, they are inefficient as they require parallel corpora, days to train and hours to decode. This paper introduces SelfSeg, a self-supervised neural sub-word segmentation method that is much faster to train/decode and requires only monolingual dictionaries instead of parallel corpora. SelfSeg takes as input a word in the form of a partially masked character sequence, optimizes the word generation probability and generates the segmentation with the maximum posterior probability, which is calculated using a dynamic programming algorithm. The training time of SelfSeg depends on word frequencies, and we explore several word frequency normalization strategies to accelerate the training phase. Additionally, we propose a regularization mechanism that allows the segmenter to generate various segmentations for one word. To show the effectiveness of our approach, we conduct MT experiments in low-, middle- and high-resource scenarios, where we compare the performance of using different segmentation methods. The experimental results demonstrate that on the low-resource ALT dataset, our method achieves more than 1.2 BLEU score improvement compared with BPE and SentencePiece, and a 1.1 score improvement over Dynamic Programming Encoding (DPE) and Vocabulary Learning via Optimal Transport (VOLT) on average. The regularization method achieves approximately a 4.3 BLEU score improvement over BPE and a 1.2 BLEU score improvement over BPE-dropout, the regularized version of BPE. We also observed significant improvements on IWSLT15 Vi->En, WMT16 Ro->En and WMT15 Fi->En datasets, and competitive results on the WMT14 De->En and WMT14 Fr->En datasets.
</details>
<details>
<summary>摘要</summary>
它是一种基于神经网络的自动分词方法，可以用于语机翻译（NMT）的前期处理步骤。现有研究表明，神经网络分词器比Byte-Pair Encoding（BPE）更好，但它们需要并行 Corpora 并且训练和解码时间比较长。这篇论文介绍了一种自然语言自动分词法，它叫做SelfSeg，它比较快速地训练和解码，并且只需要单语言词典而不需要并行 Corpora。SelfSeg 接受一个部分遮盖的字符序列作为输入，并且通过计算最大 posterior 概率来生成分词结果。训练 SelfSeg 的时间取决于单词频率，我们也提出了多种单词频率归一化策略来加速训练阶段。此外，我们还提出了一种规范化机制，允许分词器生成不同的分词结果。为证明我们的方法的有效性，我们进行了不同分词方法的MT实验，其中包括 BPE、SentencePiece、DPE 和 VOLT。实验结果表明，在 ALT  dataset 上，我们的方法可以与 BPE 和 SentencePiece 相比，提高了 más de 1.2 BLEU 分数，而与 DPE 和 VOLT 相比，提高了约 1.1 BLEU 分数。规范化机制可以提高 BPE 的约 4.3 BLEU 分数，并且与 BPE-dropout 相比，提高了约 1.2 BLEU 分数。我们还观察到在 IWSLT15 Vi->En、WMT16 Ro->En 和 WMT15 Fi->En 等 dataset 上，我们的方法具有显著的改善，并且在 WMT14 De->En 和 WMT14 Fr->En  dataset 上达到了竞争性的结果。
</details></li>
</ul>
<hr>
<h2 id="Does-fine-tuning-GPT-3-with-the-OpenAI-API-leak-personally-identifiable-information"><a href="#Does-fine-tuning-GPT-3-with-the-OpenAI-API-leak-personally-identifiable-information" class="headerlink" title="Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?"></a>Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16382">http://arxiv.org/abs/2307.16382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/albertsun1/gpt3-pii-attacks">https://github.com/albertsun1/gpt3-pii-attacks</a></li>
<li>paper_authors: Albert Yu Sun, Eliott Zemour, Arushi Saxena, Udith Vaidyanathan, Eric Lin, Christian Lau, Vaikkunth Mugunthan</li>
<li>for: 本研究目的是探讨 GPT-3 模型在 fine-tuning 过程中是否可以提取个人敏感信息 (PII)。</li>
<li>methods: 本研究使用 OpenAI 提供的 fine-tuning API 对 GPT-3 模型进行了模拟攻击，并使用了 naive 提示方法和实际应用中的自动完成任务来调查 GPT-3 是否在 fine-tuning 过程中记忆和泄露敏感信息。</li>
<li>results: 研究发现，对 GPT-3 进行了 fine-tuning 后，模型就可以记忆并泄露来自原始 fine-tuning 数据集中的敏感信息 (PII)。<details>
<summary>Abstract</summary>
Machine learning practitioners often fine-tune generative pre-trained models like GPT-3 to improve model performance at specific tasks. Previous works, however, suggest that fine-tuned machine learning models memorize and emit sensitive information from the original fine-tuning dataset. Companies such as OpenAI offer fine-tuning services for their models, but no prior work has conducted a memorization attack on any closed-source models. In this work, we simulate a privacy attack on GPT-3 using OpenAI's fine-tuning API. Our objective is to determine if personally identifiable information (PII) can be extracted from this model. We (1) explore the use of naive prompting methods on a GPT-3 fine-tuned classification model, and (2) we design a practical word generation task called Autocomplete to investigate the extent of PII memorization in fine-tuned GPT-3 within a real-world context. Our findings reveal that fine-tuning GPT3 for both tasks led to the model memorizing and disclosing critical personally identifiable information (PII) obtained from the underlying fine-tuning dataset. To encourage further research, we have made our codes and datasets publicly available on GitHub at: https://github.com/albertsun1/gpt3-pii-attacks
</details>
<details>
<summary>摘要</summary>
机器学习实践者常常精细调整生成预训练模型，如GPT-3，以提高模型在特定任务上的表现。前一次的研究表明，精细调整的机器学习模型可能会记忆并释出原始精细调整数据中的敏感资讯。如OpenAI提供的精细调整服务，但没有任何前一次的工作对关闭源代码模型进行了记忆攻击。在这个工作中，我们模拟了隐私攻击GPT-3，使用OpenAI的精细调整API。我们的目标是确定GPT-3是否能够从这个模型中提取个人敏感信息（PII）。我们（1）探索使用简单提示方法在GPT-3精细调整分类模型上，并（2）设计了实用的自动完成任务，以探索精细调整GPT-3中PII的记忆情况。我们的发现显示，精细调整GPT-3 для这两个任务都导致模型记忆并释出重要的个人敏感信息（PII），从原始精细调整数据中获取。为了鼓励更多研究，我们将我们的代码和数据公开提供GitHub上：https://github.com/albertsun1/gpt3-pii-attacks。
</details></li>
</ul>
<hr>
<h2 id="Distractor-generation-for-multiple-choice-questions-with-predictive-prompting-and-large-language-models"><a href="#Distractor-generation-for-multiple-choice-questions-with-predictive-prompting-and-large-language-models" class="headerlink" title="Distractor generation for multiple-choice questions with predictive prompting and large language models"></a>Distractor generation for multiple-choice questions with predictive prompting and large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16338">http://arxiv.org/abs/2307.16338</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/semerekiros/distractgpt">https://github.com/semerekiros/distractgpt</a></li>
<li>paper_authors: Semere Kiros Bitew, Johannes Deleu, Chris Develder, Thomas Demeester</li>
<li>for: 本研究旨在提高 chatGPT 等大型自然语言模型在多项选择题 (MCQ) 中生成干扰符的性能。</li>
<li>methods: 我们提出了一种使用问题库中自动检索的问题项作为准确的示例来引导 chatGPT 生成相关的干扰符的策略。</li>
<li>results: 我们的方法在已有测试集上进行量化评估以及人工专家（教师）的质量注释中表现出色，平均53%的生成干扰符被教师评为高质量，超过当前最佳模型。 我们还比较了我们的方法与零极 chatGPT 和几个示例激活 chatGPT 的性能，并证明了我们的方法在生成高质量干扰符方面的优势。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable performance across various tasks and have garnered significant attention from both researchers and practitioners. However, in an educational context, we still observe a performance gap in generating distractors -- i.e., plausible yet incorrect answers -- with LLMs for multiple-choice questions (MCQs). In this study, we propose a strategy for guiding LLMs such as ChatGPT, in generating relevant distractors by prompting them with question items automatically retrieved from a question bank as well-chosen in-context examples. We evaluate our LLM-based solutions using a quantitative assessment on an existing test set, as well as through quality annotations by human experts, i.e., teachers. We found that on average 53% of the generated distractors presented to the teachers were rated as high-quality, i.e., suitable for immediate use as is, outperforming the state-of-the-art model. We also show the gains of our approach 1 in generating high-quality distractors by comparing it with a zero-shot ChatGPT and a few-shot ChatGPT prompted with static examples.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如ChatGPT已经表现出了在不同任务上的出色表现，引起了研究者和实践者的广泛关注。然而，在教育上，我们仍然观察到LLM在生成诱导者（i.e., 可能correct但不正确的答案）方面存在性能差距。在这项研究中，我们提出了一种策略，使LLM通过自动从问题库中提取问题项来生成相关的诱导者。我们使用现有测试集进行了量化评估，以及通过人工智能专家（i.e., 教师）的质量标注来评估。我们发现，平均 speaking 53%的生成诱导者被教师评估为高质量，可以不需要更改，超越了现有的模型。我们还显示了我们的方法在生成高质量诱导者方面的优势，与零极 chatGPT 和几个极少shot chatGPT 提交的静止示例相比。
</details></li>
</ul>
<hr>
<h2 id="Mispronunciation-detection-using-self-supervised-speech-representations"><a href="#Mispronunciation-detection-using-self-supervised-speech-representations" class="headerlink" title="Mispronunciation detection using self-supervised speech representations"></a>Mispronunciation detection using self-supervised speech representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16324">http://arxiv.org/abs/2307.16324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JazminVidal/ssl-mispron">https://github.com/JazminVidal/ssl-mispron</a></li>
<li>paper_authors: Jazmin Vidal, Pablo Riera, Luciana Ferrer</li>
<li>for: 本研究探讨了使用自动学习模型（SSL）进行外语学习者的发音识别任务。</li>
<li>methods: 我们比较了两种下游方法：1）使用本地英语数据进行话音识别（PR）模型训练，2）直接使用非本地英语数据进行目标任务模型训练。我们对各种SSL表示形式以及一个来自传统的DNN基于语音识别模型的表示形式进行比较。</li>
<li>results: 我们发现使用下游模型直接进行目标任务训练得到最好的性能，而大多数上游模型在这个任务上表现相似。<details>
<summary>Abstract</summary>
In recent years, self-supervised learning (SSL) models have produced promising results in a variety of speech-processing tasks, especially in contexts of data scarcity. In this paper, we study the use of SSL models for the task of mispronunciation detection for second language learners. We compare two downstream approaches: 1) training the model for phone recognition (PR) using native English data, and 2) training a model directly for the target task using non-native English data. We compare the performance of these two approaches for various SSL representations as well as a representation extracted from a traditional DNN-based speech recognition model. We evaluate the models on L2Arctic and EpaDB, two datasets of non-native speech annotated with pronunciation labels at the phone level. Overall, we find that using a downstream model trained for the target task gives the best performance and that most upstream models perform similarly for the task.
</details>
<details>
<summary>摘要</summary>
近年来，自我超vised学习（SSL）模型在各种语音处理任务中表现出色，特别是在数据缺乏的情况下。本文研究了使用SSL模型进行第二语言学习者的误发音检测。我们比较了两种下游方法：1）使用本地英语数据进行话语识别（PR）模型的训练，和2）直接使用非本地英语数据进行目标任务的模型训练。我们对各种SSL表示形式以及一种基于传统DNN语音识别模型中的表示进行比较。我们对L2Arctic和EpaDB两个非本地语音 datasets进行评估。总的来说，我们发现使用下游模型直接进行目标任务的训练可以获得最好的性能，而大多数上游模型在这个任务上的表现相似。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/31/cs.CL_2023_07_31/" data-id="cloh7tqd700897b88ads59sim" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/31/cs.LG_2023_07_31/" class="article-date">
  <time datetime="2023-07-31T10:00:00.000Z" itemprop="datePublished">2023-07-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/31/cs.LG_2023_07_31/">cs.LG - 2023-07-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Classification-with-Deep-Neural-Networks-and-Logistic-Loss"><a href="#Classification-with-Deep-Neural-Networks-and-Logistic-Loss" class="headerlink" title="Classification with Deep Neural Networks and Logistic Loss"></a>Classification with Deep Neural Networks and Logistic Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16792">http://arxiv.org/abs/2307.16792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Zihan Zhang, Lei Shi, Ding-Xuan Zhou</li>
<li>for: 这个论文主要是研究 Deep Neural Networks (DNNs) 在二分类任务中的泛化分析。</li>
<li>methods: 这篇论文使用了一种新的oracle-type不等式，并使用这个不等式来 deriv 出 DNN 类ifiers 在 logistic loss 下的快速收敛率。</li>
<li>results: 这篇论文提供了一些新的泛化分析结果，包括对 DNN 类ifiers 的收敛率的优化（即 log 因子），以及对数据维度的独立性。这些结果可以解释为什么 DNN 类ifiers 在实际高维度分类任务中能够表现出色。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) trained with the logistic loss (i.e., the cross entropy loss) have made impressive advancements in various binary classification tasks. However, generalization analysis for binary classification with DNNs and logistic loss remains scarce. The unboundedness of the target function for the logistic loss is the main obstacle to deriving satisfying generalization bounds. In this paper, we aim to fill this gap by establishing a novel and elegant oracle-type inequality, which enables us to deal with the boundedness restriction of the target function, and using it to derive sharp convergence rates for fully connected ReLU DNN classifiers trained with logistic loss. In particular, we obtain optimal convergence rates (up to log factors) only requiring the H\"older smoothness of the conditional class probability $\eta$ of data. Moreover, we consider a compositional assumption that requires $\eta$ to be the composition of several vector-valued functions of which each component function is either a maximum value function or a H\"older smooth function only depending on a small number of its input variables. Under this assumption, we derive optimal convergence rates (up to log factors) which are independent of the input dimension of data. This result explains why DNN classifiers can perform well in practical high-dimensional classification problems. Besides the novel oracle-type inequality, the sharp convergence rates given in our paper also owe to a tight error bound for approximating the natural logarithm function near zero (where it is unbounded) by ReLU DNNs. In addition, we justify our claims for the optimality of rates by proving corresponding minimax lower bounds. All these results are new in the literature and will deepen our theoretical understanding of classification with DNNs.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在不同的二分类任务中做出了很好的表现。然而，对于DNN和对数损失函数的泛化分析仍然缺乏研究。对数损失函数的无上界性是泛化分析的主要障碍。在这篇论文中，我们想要填补这个差距，通过设立一个新的oracle-type不等式，使得我们可以处理对数损失函数的上界限制，并使用其 derive sharp的泛化速率 для全连接ReLU DNN类ifiziert。特别是，我们获得了最佳的泛化速率（Up to log factor），只需要数据中condition class概率函数 $\eta$的Holder平滑性。此外，我们还假设了$\eta$是一个Vector-valued函数的compose，其中每个组件函数可以是最大值函数或Holder平滑函数，只依赖于几个输入变量。在这个假设下，我们 derive optimal的泛化速率（Up to log factor），这个结果解释了为什么DNN类ifiziert可以在实际高维分类问题中表现良好。此外，我们还提供了一个紧距的错误 bound，用于 aproximating自然对数函数 near zero（where it is unbounded）by ReLU DNNs。此外，我们还证明了我们的结果的最佳性，通过证明相应的最小化下界。这些结果都是文献中新的，它们将深入我们对分类问题的理论理解。
</details></li>
</ul>
<hr>
<h2 id="ToolLLM-Facilitating-Large-Language-Models-to-Master-16000-Real-world-APIs"><a href="#ToolLLM-Facilitating-Large-Language-Models-to-Master-16000-Real-world-APIs" class="headerlink" title="ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"></a>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16789">http://arxiv.org/abs/2307.16789</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openbmb/toolbench">https://github.com/openbmb/toolbench</a></li>
<li>paper_authors: Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun</li>
<li>for: 这个论文旨在提高开源大语言模型（LLM）的高级任务能力，使其能够更好地执行人类指令，以及自动选择适合的API。</li>
<li>methods: 这篇论文提出了一种总称为 ToolLLM 的工具使用框架，包括数据建构、模型训练和评估。具体来说，他们提出了一个名为 ToolBench 的实ruction-tuning数据集，并使用 ChatGPT 自动生成了16,464个实际 RESTful API 示例，以及一种名为 DFSDT 的深度优先搜索树，以提高 LL 模型的规划和推理能力。</li>
<li>results: 根据论文的描述，通过使用 ToolLLM 框架和 ToolBench 数据集，LLaMA 模型可以准确执行复杂的指令，并在未看过的 API 上进行推理和规划。此外，ToolLLaMA 模型与 ChatGPT 的表现相当，且可以自动选择适合的 API。<details>
<summary>Abstract</summary>
Despite the advancements of open-source large language models (LLMs) and their variants, e.g., LLaMA and Vicuna, they remain significantly limited in performing higher-level tasks, such as following human instructions to use external tools (APIs). This is because current instruction tuning largely focuses on basic language tasks instead of the tool-use domain. This is in contrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have demonstrated excellent tool-use capabilities but are unfortunately closed source. To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework of data construction, model training and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT. Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub, then prompt ChatGPT to generate diverse human instructions involving these APIs, covering both single-tool and multi-tool scenarios. Finally, we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To make the searching process more efficient, we develop a novel depth-first search-based decision tree (DFSDT), enabling LLMs to evaluate multiple reasoning traces and expand the search space. We show that DFSDT significantly enhances the planning and reasoning capabilities of LLMs. For efficient tool-use assessment, we develop an automatic evaluator: ToolEval. We fine-tune LLaMA on ToolBench and obtain ToolLLaMA. Our ToolEval reveals that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. To make the pipeline more practical, we devise a neural API retriever to recommend appropriate APIs for each instruction, negating the need for manual API selection.
</details>
<details>
<summary>摘要</summary>
尽管开源大型自然语言模型（LLM）和其变体（如LLaMA和Vicuna）在进行更高级任务方面有所进步，但它们仍然在使用外部工具（API）方面有限制。这是因为当前的指令调整主要集中在基础语言任务上，而不是工具使用领域。与此相比，当前最佳实践（SOTA）LLM（如ChatGPT）已经表现出了优秀的工具使用能力，但它们却是关闭源代码。为了激活开源LLM中的工具使用能力，我们提出了工具框架（ToolLLM），包括数据建构、模型训练和评估。我们首先提供了工具调整数据集（ToolBench），该数据集是通过ChatGPT自动生成的人工指令，涵盖单工具和多工具场景。我们收集了16,464个实际RESTful API，涵盖49个类别，并使用ChatGPT生成多样化的人工指令，以覆盖这些API。然后，我们使用ChatGPT搜索一个有效的解决方案路径（Chain of API calls） для每个指令。为了使搜索过程更有效，我们开发了一种深度优先搜索基于决策树（DFSDT），使LLM可以评估多种逻辑追踪，扩大搜索空间。我们表明，DFSDT有效地提高了LLM的规划和逻辑能力。为了有效评估工具使用，我们开发了自动评估器（ToolEval）。我们精心 fine-tune LLaMA 在 ToolBench 上，并得到了 ToolLLaMA。我们的 ToolEval 表明，ToolLLaMA 能够执行复杂的指令并将其推广到未经看到的API，并与ChatGPT的性能相似。为使管道更实用，我们设计了一种神经API搜索器，以便根据每个指令提供适当的API，从而消除手动API选择的需要。
</details></li>
</ul>
<hr>
<h2 id="Exploring-how-a-Generative-AI-interprets-music"><a href="#Exploring-how-a-Generative-AI-interprets-music" class="headerlink" title="Exploring how a Generative AI interprets music"></a>Exploring how a Generative AI interprets music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00015">http://arxiv.org/abs/2308.00015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriela Barenboim, Luigi Del Debbio, Johannes Hirn, Veronica Sanz</li>
<li>for: 用Google的MusicVAE模型来表示一些乐曲的几个核心特征。</li>
<li>methods: 使用Variational Auto-Encoder，对乐曲进行512维 latent空间的表示，并对latent空间中的维度进行排序，以确定哪些维度最有用于描述乐曲。</li>
<li>results: 发现大多数latent neuron在听到真正的乐曲时都不活跃，只有几个独立的neuron会活跃，这些neuron被称为”music neuron”。发现大多数乐曲中的旋律信息都是由第一些music neuron来编码的，而乐曲的旋律信息只在 longer sequences of music中出现。<details>
<summary>Abstract</summary>
We use Google's MusicVAE, a Variational Auto-Encoder with a 512-dimensional latent space to represent a few bars of music, and organize the latent dimensions according to their relevance in describing music. We find that, on average, most latent neurons remain silent when fed real music tracks: we call these "noise" neurons. The remaining few dozens of latent neurons that do fire are called "music neurons". We ask which neurons carry the musical information and what kind of musical information they encode, namely something that can be identified as pitch, rhythm or melody. We find that most of the information about pitch and rhythm is encoded in the first few music neurons: the neural network has thus constructed a couple of variables that non-linearly encode many human-defined variables used to describe pitch and rhythm. The concept of melody only seems to show up in independent neurons for longer sequences of music.
</details>
<details>
<summary>摘要</summary>
我们使用Google的MusicVAE，一种Variational Auto-Encoder，将乐曲压缩到512维 latent space中。我们发现，在真实音乐轨迹上 feed  latent neurons 中，大多数 neurons 保持静止：我们称这些为 "噪声" neurons。剩下的数十个 latent neurons 会被 activated ，我们称为 "音乐 neurons"。我们问的是，哪些 neurons 携带了音乐信息，它们编码什么样的音乐信息，例如抽象的旋律、和声或旋律。我们发现，大多数旋律和和声信息是在首几个 music neurons 中编码的：神经网络因此构造了一些变量，非线性地编码了许多人定义的旋律和和声变量。概念化的旋律似乎只在 longer sequence of music 中出现。
</details></li>
</ul>
<hr>
<h2 id="Lossless-Transformations-and-Excess-Risk-Bounds-in-Statistical-Inference"><a href="#Lossless-Transformations-and-Excess-Risk-Bounds-in-Statistical-Inference" class="headerlink" title="Lossless Transformations and Excess Risk Bounds in Statistical Inference"></a>Lossless Transformations and Excess Risk Bounds in Statistical Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16735">http://arxiv.org/abs/2307.16735</a></li>
<li>repo_url: None</li>
<li>paper_authors: László Györfi, Tamás Linder, Harro Walk</li>
<li>for: 这篇论文主要研究了统计推断中的剩余最小风险，即从观测特征向量中估计随机变量的最小风险差。</li>
<li>methods: 本文首先 caracterizes lossless transformations，即 transformations for which the excess risk is zero for all loss functions，然后构建一个分 partitions 测试统计量来检验一个 transformations 是否是 lossless，并证明对于独立同分布数据，该测试是strongly consistent。</li>
<li>results: 本文还提出了基于信息理论的上限 bounds on the excess risk，这些 bounds 适用于较为通用的损失函数类型。基于这些 bounds，本文引入了 delta-lossless transformation 概念，并给出了 universally delta-lossless transformation 的充分条件。<details>
<summary>Abstract</summary>
We study the excess minimum risk in statistical inference, defined as the difference between the minimum expected loss in estimating a random variable from an observed feature vector and the minimum expected loss in estimating the same random variable from a transformation (statistic) of the feature vector. After characterizing lossless transformations, i.e., transformations for which the excess risk is zero for all loss functions, we construct a partitioning test statistic for the hypothesis that a given transformation is lossless and show that for i.i.d. data the test is strongly consistent. More generally, we develop information-theoretic upper bounds on the excess risk that uniformly hold over fairly general classes of loss functions. Based on these bounds, we introduce the notion of a delta-lossless transformation and give sufficient conditions for a given transformation to be universally delta-lossless. Applications to classification, nonparametric regression, portfolio strategies, information bottleneck, and deep learning, are also surveyed.
</details>
<details>
<summary>摘要</summary>
我们研究额外最小风险在统计推断中，定义为从观察特征向量获取随机变量的估计loss的差异，与从特征向量的变换（统计）获取随机变量的loss的差异。经过定义无损变换，即所有loss函数下的额外风险为零的变换，我们构建了分partition测试统计，用于测试一个给定的变换是否无损，并证明在独立Identically distributed（i.i.d）数据上，该测试是强有效的。更一般地，我们建立了基于信息理论的额外风险上下文 bounds，对于较为一般的损失函数而言，这些上下文 bounds 具有 uniform 的持续性。基于这些上下文 bounds，我们引入了 delta-lossless 变换的概念，并给出了universally delta-lossless 变换的充分条件。我们还应用到分类、非 Parametric 回归、投资策略、信息瓶颈和深度学习等领域。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Shapley-Value-Computation-for-the-Naive-Bayes-Classifier"><a href="#An-Efficient-Shapley-Value-Computation-for-the-Naive-Bayes-Classifier" class="headerlink" title="An Efficient Shapley Value Computation for the Naive Bayes Classifier"></a>An Efficient Shapley Value Computation for the Naive Bayes Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16718">http://arxiv.org/abs/2307.16718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Lemaire, Fabrice Clérot, Marc Boullé</li>
<li>for: 本研究的目的是提出一种exact analytic expression of Shapley values дляNaive Bayes分类器，以及与其他常用指标Weight of Evidence（WoE）的比较，以及与 kernelShap 结果的 empirical comparison。</li>
<li>methods: 本研究使用了 cooperative game theory 的 Shapley value estimation algorithms，并提出了一种exact analytic expression of Shapley values 的方法，用于Naive Bayes分类器。</li>
<li>results: 研究结果表明，我们的 Shapley proposal 可以在实际世界数据集上提供有用的结果，并且与 WoE 和 KernelShap 结果有一定的相似性和不同性。 特别是，我们的方法具有低的算法复杂度和高效的计算时间，可以在很大的数据集上进行实时计算。<details>
<summary>Abstract</summary>
Variable selection or importance measurement of input variables to a machine learning model has become the focus of much research. It is no longer enough to have a good model, one also must explain its decisions. This is why there are so many intelligibility algorithms available today. Among them, Shapley value estimation algorithms are intelligibility methods based on cooperative game theory. In the case of the naive Bayes classifier, and to our knowledge, there is no ``analytical" formulation of Shapley values. This article proposes an exact analytic expression of Shapley values in the special case of the naive Bayes Classifier. We analytically compare this Shapley proposal, to another frequently used indicator, the Weight of Evidence (WoE) and provide an empirical comparison of our proposal with (i) the WoE and (ii) KernelShap results on real world datasets, discussing similar and dissimilar results. The results show that our Shapley proposal for the naive Bayes classifier provides informative results with low algorithmic complexity so that it can be used on very large datasets with extremely low computation time.
</details>
<details>
<summary>摘要</summary>
Variable selection或输入变量重要性评估在机器学习模型中已成为研究焦点。不再只有一个好模型，还需要解释其决策。这是为什么今天有这么多可理解性算法的原因。 Among them, Shapley value estimation algorithms are intelligibility methods based on cooperative game theory. In the case of the naive Bayes classifier, and to our knowledge, there is no "analytical" formulation of Shapley values. This article proposes an exact analytic expression of Shapley values in the special case of the naive Bayes Classifier. We analytically compare this Shapley proposal with another frequently used indicator, the Weight of Evidence (WoE), and provide an empirical comparison of our proposal with (i) the WoE and (ii) KernelShap results on real-world datasets, discussing similar and dissimilar results. The results show that our Shapley proposal for the naive Bayes classifier provides informative results with low algorithmic complexity, so it can be used on very large datasets with extremely low computation time.Note: Please note that the translation is in Simplified Chinese, and the word order and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-in-Genetic-Programming-Guiding-Efficient-Data-Collection-for-Symbolic-Regression"><a href="#Active-Learning-in-Genetic-Programming-Guiding-Efficient-Data-Collection-for-Symbolic-Regression" class="headerlink" title="Active Learning in Genetic Programming: Guiding Efficient Data Collection for Symbolic Regression"></a>Active Learning in Genetic Programming: Guiding Efficient Data Collection for Symbolic Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00672">http://arxiv.org/abs/2308.00672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hoolagans/stackgp">https://github.com/hoolagans/stackgp</a></li>
<li>paper_authors: Nathan Haut, Wolfgang Banzhaf, Bill Punch</li>
<li>for: 这种论文探讨了活动学习生物学程中不同方法 Computing Uncertainty和Diversity。</li>
<li>methods: 我们使用了模型集合 combines with uncertainty metric 选择有用的训练数据点。我们尝试了多种不确定度指标，发现 differential entropy 最佳。我们还比较了两种数据多样性指标，发现 correlation 作为多样性指标表现更好，但是有一些缺点使得 correlation 无法在所有问题上使用。</li>
<li>results: 我们将不确定度和多样性 combinesthrough Pareto optimization approach 来考虑它们在培训中选择有用和独特的数据点。<details>
<summary>Abstract</summary>
This paper examines various methods of computing uncertainty and diversity for active learning in genetic programming. We found that the model population in genetic programming can be exploited to select informative training data points by using a model ensemble combined with an uncertainty metric. We explored several uncertainty metrics and found that differential entropy performed the best. We also compared two data diversity metrics and found that correlation as a diversity metric performs better than minimum Euclidean distance, although there are some drawbacks that prevent correlation from being used on all problems. Finally, we combined uncertainty and diversity using a Pareto optimization approach to allow both to be considered in a balanced way to guide the selection of informative and unique data points for training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Empirical-Study-on-Log-based-Anomaly-Detection-Using-Machine-Learning"><a href="#An-Empirical-Study-on-Log-based-Anomaly-Detection-Using-Machine-Learning" class="headerlink" title="An Empirical Study on Log-based Anomaly Detection Using Machine Learning"></a>An Empirical Study on Log-based Anomaly Detection Using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16714">http://arxiv.org/abs/2307.16714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shan Ali, Chaima Boufaied, Domenico Bianculli, Paula Branco, Lionel Briand, Nathan Aschbacher</li>
<li>for: 本研究旨在对不同预测模型在日志异常检测任务中的表现进行全面的实验研究，包括传统机器学习（ML）技术和深度学习（DL）技术。</li>
<li>methods: 本研究使用了多种传统ML技术和深度学习技术，包括批处理学习、归一化学习、异常点检测等。</li>
<li>results: 研究发现，传统ML技术和深度学习技术在检测精度和预测时间方面几乎相当，而半监督学习技术在检测精度方面表现较差。此外，不同学习模型对于参数调整的敏感性也有很大差异。<details>
<summary>Abstract</summary>
The growth of systems complexity increases the need of automated techniques dedicated to different log analysis tasks such as Log-based Anomaly Detection (LAD). The latter has been widely addressed in the literature, mostly by means of different deep learning techniques. Nevertheless, the focus on deep learning techniques results in less attention being paid to traditional Machine Learning (ML) techniques, which may perform well in many cases, depending on the context and the used datasets. Further, the evaluation of different ML techniques is mostly based on the assessment of their detection accuracy. However, this is is not enough to decide whether or not a specific ML technique is suitable to address the LAD problem. Other aspects to consider include the training and prediction time as well as the sensitivity to hyperparameter tuning. In this paper, we present a comprehensive empirical study, in which we evaluate different supervised and semi-supervised, traditional and deep ML techniques w.r.t. four evaluation criteria: detection accuracy, time performance, sensitivity of detection accuracy as well as time performance to hyperparameter tuning. The experimental results show that supervised traditional and deep ML techniques perform very closely in terms of their detection accuracy and prediction time. Moreover, the overall evaluation of the sensitivity of the detection accuracy of the different ML techniques to hyperparameter tuning shows that supervised traditional ML techniques are less sensitive to hyperparameter tuning than deep learning techniques. Further, semi-supervised techniques yield significantly worse detection accuracy than supervised techniques.
</details>
<details>
<summary>摘要</summary>
随着系统复杂性的增加，自动化技术在不同的日志分析任务中的应用越来越重要，如日志异常检测（LAD）。在文献中，大多数使用深度学习技术来解决LAD问题，但是这些技术的使用导致传统的机器学习（ML）技术 receiving less attention，但是这些技术在某些情况下可能表现非常好，具体取决于context和使用的数据集。此外，评估不同的ML技术的方法通常是根据它们的检测精度进行评估，但这并不是决定是否适用于LAD问题的唯一因素。其他需要考虑的因素包括训练和预测时间以及参数调整的敏感性。在本文中，我们提出了一项全面的实验研究，在四个评估标准下评估不同的超vised和半supervised、传统和深度学习技术：检测精度、训练和预测时间、检测精度对参数调整的敏感性以及训练和预测时间对参数调整的敏感性。实验结果表明，超vised传统和深度学习技术在检测精度和预测时间方面表现很相似，而且总体来说，传统学习技术对参数调整的敏感性较低。此外，半supervised技术的检测精度相对较低。
</details></li>
</ul>
<hr>
<h2 id="TFE-GNN-A-Temporal-Fusion-Encoder-Using-Graph-Neural-Networks-for-Fine-grained-Encrypted-Traffic-Classification"><a href="#TFE-GNN-A-Temporal-Fusion-Encoder-Using-Graph-Neural-Networks-for-Fine-grained-Encrypted-Traffic-Classification" class="headerlink" title="TFE-GNN: A Temporal Fusion Encoder Using Graph Neural Networks for Fine-grained Encrypted Traffic Classification"></a>TFE-GNN: A Temporal Fusion Encoder Using Graph Neural Networks for Fine-grained Encrypted Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16713">http://arxiv.org/abs/2307.16713</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ViktorAxelsen/TFE-GNN">https://github.com/ViktorAxelsen/TFE-GNN</a></li>
<li>paper_authors: Haozhen Zhang, Le Yu, Xi Xiao, Qing Li, Francesco Mercaldo, Xiapu Luo, Qixu Liu<br>for: 本研究旨在提出一种基于点wise积分信息（PMI）的字节级交通图构建方法，以及一种基于图神经网络（GNN）的特征提取模型——时间融合编码器（TFE-GNN），用于细致Encrypted traffic classification。methods: 本研究使用了字节级交通图构建方法和基于GNN的特征提取模型，包括两层双重嵌入层、GNN基于交通图编码器和交通图与数据流之间的交叉阻止机制。results: 对于两个实际数据集，TFE-GNN的实验结果表明，它在细致Encrypted traffic classification任务中超过多种现有方法表现出色。<details>
<summary>Abstract</summary>
Encrypted traffic classification is receiving widespread attention from researchers and industrial companies. However, the existing methods only extract flow-level features, failing to handle short flows because of unreliable statistical properties, or treat the header and payload equally, failing to mine the potential correlation between bytes. Therefore, in this paper, we propose a byte-level traffic graph construction approach based on point-wise mutual information (PMI), and a model named Temporal Fusion Encoder using Graph Neural Networks (TFE-GNN) for feature extraction. In particular, we design a dual embedding layer, a GNN-based traffic graph encoder as well as a cross-gated feature fusion mechanism, which can first embed the header and payload bytes separately and then fuses them together to obtain a stronger feature representation. The experimental results on two real datasets demonstrate that TFE-GNN outperforms multiple state-of-the-art methods in fine-grained encrypted traffic classification tasks.
</details>
<details>
<summary>摘要</summary>
受到研究者和工业公司的广泛关注，加密流量分类技术正在不断发展。然而，现有的方法仅EXTract flow-level特征，无法处理短流，或者对header和 payload equally，失去了可能的字节相关性。因此，在这篇论文中，我们提出了基于点wise私有信息（PMI）的字节级流量图构建方法，以及一种基于图 neural network（GNN）的 Temporal Fusion Encoder（TFE-GNN）模型 для特征提取。具体来说，我们设计了两层双向嵌入层，一个基于GNN的流量图编码器以及一个跨度闭合特征融合机制，可以首先将header和 payload字节分别嵌入，然后将其相互融合以获得更强的特征表示。实验结果表明，TFE-GNN在两个实际数据集上的细化加密流量分类任务中较多状态前方法表现出色。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Meets-Adaptive-Filtering-A-Stein’s-Unbiased-Risk-Estimator-Approach"><a href="#Deep-Learning-Meets-Adaptive-Filtering-A-Stein’s-Unbiased-Risk-Estimator-Approach" class="headerlink" title="Deep Learning Meets Adaptive Filtering: A Stein’s Unbiased Risk Estimator Approach"></a>Deep Learning Meets Adaptive Filtering: A Stein’s Unbiased Risk Estimator Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16708">http://arxiv.org/abs/2307.16708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Esmaeilbeig, Mojtaba Soltanalian</li>
<li>for: 本文重新审视了两种广泛使用的适应滤波算法，即Recursive Least Squares (RLS) 和 Equivariant Adaptive Source Separation (EASI)，在源估计和分离上。</li>
<li>methods: 作者通过algorithm unrolling方法，将RLS和EASI算法变换成层次结构，并将每一层作为深度神经网络的一个层。</li>
<li>results: 作者提出了一种基于Stein’s unbiased risk estimator (SURE) 的训练方法，并在实验中证明了这种方法能够提高源信号估计的性能。<details>
<summary>Abstract</summary>
This paper revisits two prominent adaptive filtering algorithms through the lens of algorithm unrolling, namely recursive least squares (RLS) and equivariant adaptive source separation (EASI), in the context of source estimation and separation. Building upon the unrolling methodology, we introduce novel task-based deep learning frameworks, denoted as Deep RLS and Deep EASI. These architectures transform the iterations of the original algorithms into layers of a deep neural network, thereby enabling efficient source signal estimation by taking advantage of a training process. To further enhance performance, we propose training these deep unrolled networks utilizing a loss function grounded on a Stein's unbiased risk estimator (SURE). Our empirical evaluations demonstrate the efficacy of this SURE-based approach for enhanced source signal estimation.
</details>
<details>
<summary>摘要</summary>
这篇论文重新审视了两种常见的适应滤波算法，即回归最小二乘（RLS）和对称适应源分离（EASI），在源估计和分离的上下文中。基于折叠方法，我们提出了两种新的任务深度学习框架，称为深度RLS和深度EASI。这些架构将原始算法的迭代转化为层次深度神经网络的形式，以便通过训练过程来实现高效的源信号估计。进一步提高性能，我们提议使用基于Stein不偏风险估计（SURE）的训练方法。我们的实验证明了这种SURE基于的方法对源信号估计具有显著的效果。
</details></li>
</ul>
<hr>
<h2 id="Lookbehind-Optimizer-k-steps-back-1-step-forward"><a href="#Lookbehind-Optimizer-k-steps-back-1-step-forward" class="headerlink" title="Lookbehind Optimizer: k steps back, 1 step forward"></a>Lookbehind Optimizer: k steps back, 1 step forward</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16704">http://arxiv.org/abs/2307.16704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gonçalo Mordido, Pranshu Malviya, Aristide Baratin, Sarath Chandar</li>
<li>for: 提高深度神经网络训练稳定性，并且改善损失锐度与损失平衡之间的交互关系。</li>
<li>methods:  combines Lookahead optimizer的想法和锐度感知训练（SAM），并提出了Lookbehind方法，通过在每个迭代阶段计算多个梯度升降步的积分来偏导跌幅向平坦的极小值。</li>
<li>results: 在多种任务和训练方案中，Lookbehind方法可以获得更高的普适性表现、更大的随机权重稳定性和更高的寿命学习中的抗杂稳定性。<details>
<summary>Abstract</summary>
The Lookahead optimizer improves the training stability of deep neural networks by having a set of fast weights that "look ahead" to guide the descent direction. Here, we combine this idea with sharpness-aware minimization (SAM) to stabilize its multi-step variant and improve the loss-sharpness trade-off. We propose Lookbehind, which computes $k$ gradient ascent steps ("looking behind") at each iteration and combine the gradients to bias the descent step toward flatter minima. We apply Lookbehind on top of two popular sharpness-aware training methods -- SAM and adaptive SAM (ASAM) -- and show that our approach leads to a myriad of benefits across a variety of tasks and training regimes. Particularly, we show increased generalization performance, greater robustness against noisy weights, and higher tolerance to catastrophic forgetting in lifelong learning settings.
</details>
<details>
<summary>摘要</summary>
“lookahead”优化器可以提高深度神经网络的训练稳定性，通过一组快速的权重来引导 DESC 方向。我们在这里结合这个想法与锐度意识化最小化（SAM）来稳定其多步变体并改善损失锐度质量。我们提出了“lookbehind”，它在每次迭代中计算 $k$ 步梯度升级（“寻看后”），并将梯度相加以偏移下降步向平坦的极小值。我们在两种流行的锐度意识化训练方法——SAM 和 adaptive SAM（ASAM）——之上应用了 Lookbehind，并证明了我们的方法在多种任务和训练 режимах中具有多种优势，包括提高泛化性能、增强随着权重噪声的 Robustness 和生长学习中的忘却鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="A-theory-of-data-variability-in-Neural-Network-Bayesian-inference"><a href="#A-theory-of-data-variability-in-Neural-Network-Bayesian-inference" class="headerlink" title="A theory of data variability in Neural Network Bayesian inference"></a>A theory of data variability in Neural Network Bayesian inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16695">http://arxiv.org/abs/2307.16695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javed Lindner, David Dahmen, Michael Krämer, Moritz Helias</li>
<li>for: 这篇论文主要是研究无穷层神经网络的泛化性质，以及在不同输入维度和训练数据规模下的泛化性质。</li>
<li>methods: 这篇论文使用了泛化推理和kernel方法，具体来说是使用了神经网络 Gaussian 过程，以研究无穷层神经网络的泛化性质。</li>
<li>results: 这篇论文的结果表明，在不同输入维度和训练数据规模下，神经网络的泛化性质可以通过计算 kernel 矩阵的统计性质来描述，并且可以通过这种方法获得泛化性质的 bounds。<details>
<summary>Abstract</summary>
Bayesian inference and kernel methods are well established in machine learning. The neural network Gaussian process in particular provides a concept to investigate neural networks in the limit of infinitely wide hidden layers by using kernel and inference methods. Here we build upon this limit and provide a field-theoretic formalism which covers the generalization properties of infinitely wide networks. We systematically compute generalization properties of linear, non-linear, and deep non-linear networks for kernel matrices with heterogeneous entries. In contrast to currently employed spectral methods we derive the generalization properties from the statistical properties of the input, elucidating the interplay of input dimensionality, size of the training data set, and variability of the data. We show that data variability leads to a non-Gaussian action reminiscent of a ($\varphi^3+\varphi^4$)-theory. Using our formalism on a synthetic task and on MNIST we obtain a homogeneous kernel matrix approximation for the learning curve as well as corrections due to data variability which allow the estimation of the generalization properties and exact results for the bounds of the learning curves in the case of infinitely many training data points.
</details>
<details>
<summary>摘要</summary>
bayesian inference和kernel方法在机器学习中很受欢迎，特别是神经网络泊松过程，它提供了investigate神经网络的概念，在具有无限宽隐藏层的情况下使用kernel和推理方法。我们在这个限制下建立了一个场理论 formalism，涵盖无限宽网络的泛化性质。我们系统地计算了linear, non-linear和深度非线性网络的泛化性质，并对具有不同分布的输入数据进行了系统的计算。与现有的spectral方法不同，我们从输入数据的统计性质出发，描述了输入维度、训练数据集的大小和数据的多样性之间的交互作用。我们发现，数据多样性导致一种非高斯行为，类似于($\varphi^3+\varphi^4$)-理论。使用我们的形式主义在一个synthetic任务和MNIST上，我们获得了一个homogeneous kernel matrix approximation，以及由数据多样性引起的 corrections，这些 corrections 允许我们估计泛化性质并计算 bounds 的学习曲线。
</details></li>
</ul>
<hr>
<h2 id="Guiding-Image-Captioning-Models-Toward-More-Specific-Captions"><a href="#Guiding-Image-Captioning-Models-Toward-More-Specific-Captions" class="headerlink" title="Guiding Image Captioning Models Toward More Specific Captions"></a>Guiding Image Captioning Models Toward More Specific Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16686">http://arxiv.org/abs/2307.16686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Kornblith, Lala Li, Zirui Wang, Thao Nguyen</li>
<li>For: The paper aims to improve the quality of image captions generated by an autoregressive captioning model, specifically by fine-tuning the model to estimate both conditional and unconditional distributions over captions.* Methods: The paper uses classifier-free guidance for the autoregressive captioning model, which involves fine-tuning the model to estimate both conditional and unconditional distributions over captions. The guidance scale applied at decoding controls a trade-off between maximizing the probability of the caption given the image and the probability of the image given the caption.* Results: The paper shows that decoding with a guidance scale of 2 substantially improves reference-free metrics such as CLIPScore and caption-to-image retrieval performance in the CLIP embedding space, but worsens standard reference-based captioning metrics such as CIDEr. The paper also explores the use of language models to guide the decoding process, obtaining small improvements over the Pareto frontier of reference-free vs. reference-based captioning metrics.<details>
<summary>Abstract</summary>
Image captioning is conventionally formulated as the task of generating captions for images that match the distribution of reference image-caption pairs. However, reference captions in standard captioning datasets are short and may not uniquely identify the images they describe. These problems are further exacerbated when models are trained directly on image-alt text pairs collected from the internet. In this work, we show that it is possible to generate more specific captions with minimal changes to the training process. We implement classifier-free guidance for an autoregressive captioning model by fine-tuning it to estimate both conditional and unconditional distributions over captions. The guidance scale applied at decoding controls a trade-off between maximizing $p(\mathrm{caption}|\mathrm{image})$ and $p(\mathrm{image}|\mathrm{caption})$. Compared to standard greedy decoding, decoding with a guidance scale of 2 substantially improves reference-free metrics such as CLIPScore (0.808 vs. 0.775) and caption$\to$image retrieval performance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens standard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). We further explore the use of language models to guide the decoding process, obtaining small improvements over the Pareto frontier of reference-free vs. reference-based captioning metrics that arises from classifier-free guidance, and substantially improving the quality of captions generated from a model trained only on minimally curated web data.
</details>
<details>
<summary>摘要</summary>
Image captioning 通常是指生成与图像相关的文本描述，但标准的参考caption集合可能不够准确地描述图像。此外，由互联网上收集的图像-alt文本对也可能导致模型的训练过程中的问题。在这项工作中，我们表明可以通过微调模型来生成更具体的描述。我们实现了无类别导航的autoregressive captioning模型，通过估计条件和无条件的描述分布来对模型进行微调。在解码过程中应用的指导尺度控制了在描述中最大化 $p(\text{caption}|\text{image})$ 和 $p(\text{image}|\text{caption})$ 的权重平衡。与标准的批量解码相比，使用指导尺度可以substantially提高无参考metric（CLIPScore）（0.808 vs. 0.775）和在CLIP嵌入空间中的描述$\to$图像检索性能（recall@1 44.6% vs. 26.5%），但会降低标准的参考基线metric（例如 CIDEr 78.6 vs 126.1）。我们进一步探讨使用语言模型来引导解码过程，可以在无参考vs参考captioning metric之间获得小的改进，并在使用微 curated web数据训练的模型中substantially提高生成的描述质量。
</details></li>
</ul>
<hr>
<h2 id="On-the-Trustworthiness-Landscape-of-State-of-the-art-Generative-Models-A-Comprehensive-Survey"><a href="#On-the-Trustworthiness-Landscape-of-State-of-the-art-Generative-Models-A-Comprehensive-Survey" class="headerlink" title="On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey"></a>On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16680">http://arxiv.org/abs/2307.16680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyuan Fan, Cen Chen, Chengyu Wang, Jun Huang<br>for: This paper investigates the trustworthiness of large-scale generative models, specifically addressing privacy, security, fairness, and responsibility concerns.methods: The authors use a comprehensive approach, mapping out the trustworthiness of these models across four fundamental dimensions and providing practical recommendations.results: The paper provides an extensive map outlining the trustworthiness of large-scale generative models and identifies future directions for promoting their trustworthy deployment.Here’s the text in Simplified Chinese:for: 这篇论文研究了大规模生成模型的可靠性，特别是privacy、安全、公平和责任等四个基本维度上的问题。methods: 作者采用了一种全面的方法，通过地图出大规模生成模型的可靠性，并提供了实践的建议。results: 论文提供了大规模生成模型的可靠性的广泛地图，并标识了未来的发展方向，以便推广这些模型的可靠性。<details>
<summary>Abstract</summary>
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ultimately benefiting society as a whole.
</details>
<details>
<summary>摘要</summary>
大数据扩散模型和大语言模型在各种方面已经成为当今领先的生成模型，它们的实施也暴露了内在的风险，表现出了双重性和不可靠性的问题。尽管有大量的文献关于这个主题，但是一篇全面探讨大规模生成模型和其可靠性之间的关系的调查还是缺席。为了填补这个空白，本文调查了这些模型所面临的长期和新出现的威胁，从四个基本维度出发：隐私、安全、公平和责任。通过构建了这些模型的可靠性地图，并提供了实践的建议和未来方向，以便推广这些模型的可靠部署，终于为社会带来利益。注：本文使用的是简化中文，以便更好地适应不同读者的需求。
</details></li>
</ul>
<hr>
<h2 id="Comparing-normalizing-flows-and-diffusion-models-for-prosody-and-acoustic-modelling-in-text-to-speech"><a href="#Comparing-normalizing-flows-and-diffusion-models-for-prosody-and-acoustic-modelling-in-text-to-speech" class="headerlink" title="Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech"></a>Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16679">http://arxiv.org/abs/2307.16679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyan Zhang, Thomas Merritt, Manuel Sam Ribeiro, Biel Tura-Vecino, Kayoko Yanagisawa, Kamil Pokora, Abdelhamid Ezzerg, Sebastian Cygert, Ammar Abbas, Piotr Bilinski, Roberto Barra-Chicote, Daniel Korzekwa, Jaime Lorenzo-Trueba</li>
<li>for: 文章主要针对的是语音合成 tasks 中的 Prosody 和 mel-spectrogram 预测问题。</li>
<li>methods: 文章使用了 Normalizing Flows 和 Diffusion Probabilistic Models 作为传统 L1&#x2F;L2 损失函数的替代方案。</li>
<li>results: 实验结果表明，流程基本模型在 mel-spectrogram 预测 task 中表现最佳，超过了相当的扩散和 L1 模型。此外，流程和扩散基本预测器均对传统 L2 训练的颤音模型产生了显著改进。<details>
<summary>Abstract</summary>
Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models.
</details>
<details>
<summary>摘要</summary>
Traditional L1/L2-based approaches 是使用 Normalizing Flows 和 Diffusion Probabilistic Models 作为代替方案，以改善目标数据空间的假设。在这篇文章中，我们对 text-to-speech 合成中的 Prosody 和 mel-spectrogram 预测任务进行比较。我们使用 Prosody 模型生成 log-f0 和 duration 特征，然后使用这些特征来 condition 一个 acoustic 模型，生成 mel-spectrogram。实验结果表明，流动基于模型可以在 spectrogram 预测任务中取得最佳性能，超过相当的扩散和 L1 模型。同时，流动和扩散基于 Prosody 预测器都导致了 significativly 改善于 Typical L2 训练的 Prosody 模型。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Reinforcement-Learning-for-Torque-Based-Variable-Height-Hopping"><a href="#End-to-End-Reinforcement-Learning-for-Torque-Based-Variable-Height-Hopping" class="headerlink" title="End-to-End Reinforcement Learning for Torque Based Variable Height Hopping"></a>End-to-End Reinforcement Learning for Torque Based Variable Height Hopping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16676">http://arxiv.org/abs/2307.16676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raghav Soni, Daniel Harnack, Hauke Isermann, Sotaro Fushimi, Shivesh Kumar, Frank Kirchner</li>
<li>for: 该论文主要针对的是如何通过RL控制方法实现跑动动态控制，以提高跑动机器人在自然或非结构化地形上的穿行能力。</li>
<li>methods: 该论文使用了RL控制方法，并通过练习学习来学习一个端到端的扭矩控制器，该控制器能够自动探测不同的跳跃阶段，并使用不同的控制器来处理每个阶段。</li>
<li>results: 该论文的实验结果表明，使用RL控制方法可以成功地学习一个端到端的扭矩控制器，并且可以在实际中转移到机器人上进行运行，无需进行参数调整。<details>
<summary>Abstract</summary>
Legged locomotion is arguably the most suited and versatile mode to deal with natural or unstructured terrains. Intensive research into dynamic walking and running controllers has recently yielded great advances, both in the optimal control and reinforcement learning (RL) literature. Hopping is a challenging dynamic task involving a flight phase and has the potential to increase the traversability of legged robots. Model based control for hopping typically relies on accurate detection of different jump phases, such as lift-off or touch down, and using different controllers for each phase. In this paper, we present a end-to-end RL based torque controller that learns to implicitly detect the relevant jump phases, removing the need to provide manual heuristics for state detection. We also extend a method for simulation to reality transfer of the learned controller to contact rich dynamic tasks, resulting in successful deployment on the robot after training without parameter tuning.
</details>
<details>
<summary>摘要</summary>
四肢行走是可能适应自然或非结构化地形的最佳和多样化模式。最近的研究对动态步行和跑步控制器的优化和强化学习（RL）文献已经取得了很大的进步。跳跃是一项复杂的动态任务，涉及飞行阶段，可以提高四肢机器人的通行性。基于模型的控制方法通常需要精准地探测不同的跳跃阶段，如升空或落地，然后使用不同的控制器来处理每个阶段。在这篇论文中，我们提出了一种终端RL基于扭矩控制器，该控制器通过学习来隐式探测相关的跳跃阶段，从而消除了手动规则的需求。我们还扩展了在模拟和实际中转移学习的方法，使得在机器人上部署已经训练好的控制器而无需参数调整。
</details></li>
</ul>
<hr>
<h2 id="Generative-models-for-wearables-data"><a href="#Generative-models-for-wearables-data" class="headerlink" title="Generative models for wearables data"></a>Generative models for wearables data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16664">http://arxiv.org/abs/2307.16664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arinbjörn Kolbeinsson, Luca Foschini</li>
<li>for: 解决医疗研究中的数据缺乏问题，通过生成健康数据来提供效率和成本效果的解决方案，帮助研究人员探索现有观察数据中不表现或难以访问的分布和人口。</li>
<li>methods: 提出了一种多任务自注意模型，可生成真实的穿戴式活动数据，并对生成数据的特点进行了量化和质量的评估。</li>
<li>results: 研究人员通过量化和质量的评估方法，发现生成的数据具有真实性和可靠性，能够满足医疗研究中的数据需求。<details>
<summary>Abstract</summary>
Data scarcity is a common obstacle in medical research due to the high costs associated with data collection and the complexity of gaining access to and utilizing data. Synthesizing health data may provide an efficient and cost-effective solution to this shortage, enabling researchers to explore distributions and populations that are not represented in existing observations or difficult to access due to privacy considerations. To that end, we have developed a multi-task self-attention model that produces realistic wearable activity data. We examine the characteristics of the generated data and quantify its similarity to genuine samples with both quantitative and qualitative approaches.
</details>
<details>
<summary>摘要</summary>
医学研究中数据缺乏是一个常见的障碍，这是因为数据收集的成本高昂，以及获取和利用数据的复杂性。synthesize health data可能提供一种高效且成本下降的解决方案，允许研究人员探索现有观察数据中未表示或难以访问的分布和人口。为此，我们开发了一种多任务自注意模型，生成了真实的运动活动数据。我们分析生成数据的特点，并使用量化和质量化方法衡量生成数据与真实样本之间的相似性。
</details></li>
</ul>
<hr>
<h2 id="Graph-Structure-from-Point-Clouds-Geometric-Attention-is-All-You-Need"><a href="#Graph-Structure-from-Point-Clouds-Geometric-Attention-is-All-You-Need" class="headerlink" title="Graph Structure from Point Clouds: Geometric Attention is All You Need"></a>Graph Structure from Point Clouds: Geometric Attention is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16662">http://arxiv.org/abs/2307.16662</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/murnanedaniel/geometricattention">https://github.com/murnanedaniel/geometricattention</a></li>
<li>paper_authors: Daniel Murnane</li>
<li>for: 本研究 targets the problem of top jet tagging in high energy physics, using graph neural networks to improve the accuracy and efficiency of the task.</li>
<li>methods: 本研究提出了一种 attention mechanism, named GravNetNorm, which learns a graph structure in a high-dimensional space to handle the flow of relevance and solve the Topology Problem.</li>
<li>results: 实验结果表明，GravNetNorm 比其他相关模型具有更高的标签准确率和更少的计算资源消耗。<details>
<summary>Abstract</summary>
The use of graph neural networks has produced significant advances in point cloud problems, such as those found in high energy physics. The question of how to produce a graph structure in these problems is usually treated as a matter of heuristics, employing fully connected graphs or K-nearest neighbors. In this work, we elevate this question to utmost importance as the Topology Problem. We propose an attention mechanism that allows a graph to be constructed in a learned space that handles geometrically the flow of relevance, providing one solution to the Topology Problem. We test this architecture, called GravNetNorm, on the task of top jet tagging, and show that it is competitive in tagging accuracy, and uses far fewer computational resources than all other comparable models.
</details>
<details>
<summary>摘要</summary>
使用图 neural network 已经在点云问题中取得了重要进展，如高能物理学中的问题。通常，在这些问题中建立图结构的问题是视为低级别的问题，通过全连接图或 K-最近邻居的方式进行处理。在这项工作中，我们提升了这个问题的重要性，将其称为Topology Problem。我们提议一种注意力机制，使得在学习空间中建立一个图，可以处理空间上的流动相关性，解决Topology Problem。我们测试了我们称为GravNetNorm的架构，并在top jet tagging任务上达到了与其他相关模型相当的准确率，同时使用的计算资源也是相对较少的。
</details></li>
</ul>
<hr>
<h2 id="Proactive-Resource-Request-for-Disaster-Response-A-Deep-Learning-based-Optimization-Model"><a href="#Proactive-Resource-Request-for-Disaster-Response-A-Deep-Learning-based-Optimization-Model" class="headerlink" title="Proactive Resource Request for Disaster Response: A Deep Learning-based Optimization Model"></a>Proactive Resource Request for Disaster Response: A Deep Learning-based Optimization Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16661">http://arxiv.org/abs/2307.16661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongzhe Zhang, Xiaohang Zhao, Xiao Fang, Bintong Chen</li>
<li>for: 这个论文旨在提出一种新的资源管理问题，以满足灾害应急响应中的资源需求。</li>
<li>methods: 这个论文使用了深度学习方法来预测未来需求，并将问题表述为一个Stochastic Optimization模型。</li>
<li>results: 论文的实验结果表明，该方法比现有方法更高效，并在多个维度和目标下进行了优化。<details>
<summary>Abstract</summary>
Disaster response is critical to save lives and reduce damages in the aftermath of a disaster. Fundamental to disaster response operations is the management of disaster relief resources. To this end, a local agency (e.g., a local emergency resource distribution center) collects demands from local communities affected by a disaster, dispatches available resources to meet the demands, and requests more resources from a central emergency management agency (e.g., Federal Emergency Management Agency in the U.S.). Prior resource management research for disaster response overlooks the problem of deciding optimal quantities of resources requested by a local agency. In response to this research gap, we define a new resource management problem that proactively decides optimal quantities of requested resources by considering both currently unfulfilled demands and future demands. To solve the problem, we take salient characteristics of the problem into consideration and develop a novel deep learning method for future demand prediction. We then formulate the problem as a stochastic optimization model, analyze key properties of the model, and propose an effective solution method to the problem based on the analyzed properties. We demonstrate the superior performance of our method over prevalent existing methods using both real world and simulated data. We also show its superiority over prevalent existing methods in a multi-stakeholder and multi-objective setting through simulations.
</details>
<details>
<summary>摘要</summary>
灾害应急应对是保存生命和减少灾害后果的关键。紧急应急资源分配的管理是灾害应急应对的核心。因此，当地机构（例如本地紧急资源分配中心）会收集受到灾害影响的本地社区的需求，派发可用资源来满足需求，并请求中央紧急管理机构（例如美国联邦紧急管理署）提供更多资源。然而，过去的资源管理研究通常忽略了确定最佳资源请求量的问题。为了填补这一研究漏洞，我们定义了一个新的资源管理问题，该问题旨在预测未来需求，并考虑当前未满足的需求和未来需求。为了解决这个问题，我们首先考虑了问题的重要特征，然后开发了一种新的深度学习方法来预测未来需求。接着，我们将问题转化为一个随机优化模型，分析了模型的关键属性，并提出了一种有效的解决方案。我们通过使用实际数据和预测数据进行比较，证明了我们的方法的超越性。此外，我们还通过多元参与者和多元目标的 simulate 示例，证明了我们的方法在多元参与者和多元目标下的超越性。
</details></li>
</ul>
<hr>
<h2 id="Sequential-and-Shared-Memory-Parallel-Algorithms-for-Partitioned-Local-Depths"><a href="#Sequential-and-Shared-Memory-Parallel-Algorithms-for-Partitioned-Local-Depths" class="headerlink" title="Sequential and Shared-Memory Parallel Algorithms for Partitioned Local Depths"></a>Sequential and Shared-Memory Parallel Algorithms for Partitioned Local Depths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16652">http://arxiv.org/abs/2307.16652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Devarakonda, Grey Ballard</li>
<li>for: 本文研究了一种基于对比距离的分区本地深度（PaLD）方法，用于检测 dense和稀疏社群中的强相关关系。</li>
<li>methods: 本文提出了两种变种，通过对 triplet 进行比较来进行社群结构分析。 authors 还提供了关于计算和通信成本的理论分析，并证明了Sequential 算法是通信优化的，即使在常数因素的限制下。</li>
<li>results:  authors 介绍了一些性能优化策略，使Sequential 实现可以达到 $29\times$ 的速度提升，并在使用多个线程的情况下达到 $19.4\times$ 的速度提升。<details>
<summary>Abstract</summary>
In this work, we design, analyze, and optimize sequential and shared-memory parallel algorithms for partitioned local depths (PaLD). Given a set of data points and pairwise distances, PaLD is a method for identifying strength of pairwise relationships based on relative distances, enabling the identification of strong ties within dense and sparse communities even if their sizes and within-community absolute distances vary greatly. We design two algorithmic variants that perform community structure analysis through triplet comparisons of pairwise distances. We present theoretical analyses of computation and communication costs and prove that the sequential algorithms are communication optimal, up to constant factors. We introduce performance optimization strategies that yield sequential speedups of up to $29\times$ over a baseline sequential implementation and parallel speedups of up to $19.4\times$ over optimized sequential implementations using up to $32$ threads on an Intel multicore CPU.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们设计、分析和优化了继承和分享内存并行算法 для分割本地深度（PaLD）。给定一组数据点和对称距离，PaLD 是一种方法，用于根据相对距离确定对的强度，从而在密集和稀疏社区中确定强聚合，即使社区大小和内部绝对距离差异较大。我们设计了两种算法变体，通过 triplet 比较来执行社区结构分析。我们提供了计算和通信成本的理论分析，并证明Sequential 算法是通信优化的，即使到 constants 因素。我们介绍了性能优化策略，其中包括Sequential 加速的最多 $29\times$，以及并行加速的最多 $19.4\times$，使用 Intel 多核CPU 上的最多 $32$ 个线程。
</details></li>
</ul>
<hr>
<h2 id="UDAMA-Unsupervised-Domain-Adaptation-through-Multi-discriminator-Adversarial-Training-with-Noisy-Labels-Improves-Cardio-fitness-Prediction"><a href="#UDAMA-Unsupervised-Domain-Adaptation-through-Multi-discriminator-Adversarial-Training-with-Noisy-Labels-Improves-Cardio-fitness-Prediction" class="headerlink" title="UDAMA: Unsupervised Domain Adaptation through Multi-discriminator Adversarial Training with Noisy Labels Improves Cardio-fitness Prediction"></a>UDAMA: Unsupervised Domain Adaptation through Multi-discriminator Adversarial Training with Noisy Labels Improves Cardio-fitness Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16651">http://arxiv.org/abs/2307.16651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yvonneywu/udama">https://github.com/yvonneywu/udama</a></li>
<li>paper_authors: Yu Wu, Dimitris Spathis, Hong Jia, Ignacio Perez-Pozuelo, Tomas Gonzales, Soren Brage, Nicholas Wareham, Cecilia Mascolo<br>for:* 这个研究旨在提高健康监控应用中深度学习模型的表现，并且利用不精确的标签数据来实现这一目的。methods:* 这个研究使用了两个关键 комponents：Unsupervised Domain Adaptation 和 Multidiscriminator Adversarial Training，并在这两个部分中进行了训练。results:* 研究结果显示，UDAMA 可以对不同的标签分布进行适应，并且在两个自由生活调查中与竞争性转移学习和现有的领域适应模型相比，表现出色。<details>
<summary>Abstract</summary>
Deep learning models have shown great promise in various healthcare monitoring applications. However, most healthcare datasets with high-quality (gold-standard) labels are small-scale, as directly collecting ground truth is often costly and time-consuming. As a result, models developed and validated on small-scale datasets often suffer from overfitting and do not generalize well to unseen scenarios. At the same time, large amounts of imprecise (silver-standard) labeled data, annotated by approximate methods with the help of modern wearables and in the absence of ground truth validation, are starting to emerge. However, due to measurement differences, this data displays significant label distribution shifts, which motivates the use of domain adaptation. To this end, we introduce UDAMA, a method with two key components: Unsupervised Domain Adaptation and Multidiscriminator Adversarial Training, where we pre-train on the silver-standard data and employ adversarial adaptation with the gold-standard data along with two domain discriminators. In particular, we showcase the practical potential of UDAMA by applying it to Cardio-respiratory fitness (CRF) prediction. CRF is a crucial determinant of metabolic disease and mortality, and it presents labels with various levels of noise (goldand silver-standard), making it challenging to establish an accurate prediction model. Our results show promising performance by alleviating distribution shifts in various label shift settings. Additionally, by using data from two free-living cohort studies (Fenland and BBVS), we show that UDAMA consistently outperforms up to 12% compared to competitive transfer learning and state-of-the-art domain adaptation models, paving the way for leveraging noisy labeled data to improve fitness estimation at scale.
</details>
<details>
<summary>摘要</summary>
深度学习模型在医疗监测应用中表现出了很大的搭配性。然而，大多数医疗数据集（高品质标准）的标签是小规模的，因为直接收集真实标签是成本高昂且时间consuming。因此，基于小规模数据集开发和验证的模型往往会出现过拟合问题，并不能良好地适应未看过的场景。同时，大量的不准确（银标准）标签数据，由现代便携设备提供的不准确标签，开始出现。然而，由于测量差异，这些数据表现出了明显的标签分布偏移，这种情况驱动我们使用领域适应。为此，我们介绍了UDAMA方法，它包括无监督领域适应和多discriminator对抗学习。我们在银标准数据上预训练，并使用银标准数据和两个领域探测器进行对抗适应。特别是，我们通过应用UDAMA方法于心血管健康（CRF）预测，CRF是生物 markers的关键指标，并且标签存在各种噪音（银标准和金标准），因此建立准确的预测模型是一个挑战。我们的结果表明UDAMA方法在不同的标签分布偏移情况下能够提供有望的性能。此外，通过使用两个自由生活 cohort study（Fenland和BBVS）的数据，我们表明UDAMA方法可以在不同的预测任务上持续性高于12%的竞争对手和现有的领域适应模型。这种表现，预示了可以通过不准确的标签数据来改善健康评估的可能性。
</details></li>
</ul>
<hr>
<h2 id="LLMs4OL-Large-Language-Models-for-Ontology-Learning"><a href="#LLMs4OL-Large-Language-Models-for-Ontology-Learning" class="headerlink" title="LLMs4OL: Large Language Models for Ontology Learning"></a>LLMs4OL: Large Language Models for Ontology Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16648">http://arxiv.org/abs/2307.16648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hamedbabaei/llms4ol">https://github.com/hamedbabaei/llms4ol</a></li>
<li>paper_authors: Hamed Babaei Giglou, Jennifer D’Souza, Sören Auer</li>
<li>for: 这篇论文旨在探讨 whether Large Language Models (LLMs) can effectively apply their language pattern capturing capability to Ontology Learning (OL), and evaluate the performance of nine different LLM model families for three main OL tasks.</li>
<li>methods: 该论文使用了零shot prompting方法进行评估，并使用了多种ontoLOGical knowledge genre，包括lexicosemantic knowledge in WordNet、geographical knowledge in GeoNames和医学知识 in UMLS。</li>
<li>results: 论文的评估结果表明，LLMs可以很好地应用其语言模式捕捉能力来支持OL任务，并且不同的LLM模型家族在不同的OL任务上的表现有所不同。<details>
<summary>Abstract</summary>
We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: \textit{Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.
</details>
<details>
<summary>摘要</summary>
我们提出LLMs4OL方法，该方法利用大型自然语言模型（LLMs）进行ontology学习（OL）。LLMs在不同知识领域的自然语言处理中已经表现出了明显的进步，其能够捕捉复杂的语言模式。我们的LLMs4OL思想是：\textit{可以LLMs通过捕捉和结构化自然语言文本中的知识来进行OL吗？} 为了证明这一假设，我们采用零处理方法进行全面的评估。我们评估了9种不同的LLM模型家族，用于三个主要的OL任务：类型映射、树 струкucture发现和非树结构EXTRACT。此外，评估还涵盖了多种类型的ontological知识，包括lexicosemantic知识在WordNet、地理知识在GeoNames和医疗知识在UMLS。
</details></li>
</ul>
<hr>
<h2 id="Text-CRS-A-Generalized-Certified-Robustness-Framework-against-Textual-Adversarial-Attacks"><a href="#Text-CRS-A-Generalized-Certified-Robustness-Framework-against-Textual-Adversarial-Attacks" class="headerlink" title="Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks"></a>Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16630">http://arxiv.org/abs/2307.16630</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Eyr3/TextCRS">https://github.com/Eyr3/TextCRS</a></li>
<li>paper_authors: Xinyu Zhang, Hanbin Hong, Yuan Hong, Peng Huang, Binghui Wang, Zhongjie Ba, Kui Ren</li>
<li>for: 本研究旨在提高语言模型对文本攻击的Robustness，尤其是对synonym substitution和word insertion等文本攻击。</li>
<li>methods: 我们提出了一种通用的证明 robustness 框架——Text-CRS，基于随机填充。我们通过对每个单词 adversarial operation（synonym substitution、word reordering、insertion和deletion）进行 permutation和embedding transformation，提出了新的smoothing定理来 deriv robustness bound在 permutation和embedding空间中。</li>
<li>results: 我们在多种语言模型和数据集上进行了substantial的实验，Text-CRS可以 Address all four different word-level adversarial operations，并 achieve a significant accuracy improvement。此外，我们还提供了第一个certified accuracy和radius的四个单词操作的benchmark，并超越了现有的certification against synonym substitution attacks。<details>
<summary>Abstract</summary>
The language models, especially the basic text classification models, have been shown to be susceptible to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving the model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\ell_0$ perturbations in synonym substitution attacks. Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing. Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.
</details>
<details>
<summary>摘要</summary>
“语言模型，尤其是基本文本分类模型，在文本对抗攻击方面存在抵触。为了防御这些攻击，研究人员已经投入了大量时间和精力来提高模型的Robustness。然而，提供可证明的Robustness保证而不是实际的Robustness仍然是广泛未explored的领域。在这篇论文中，我们提出了Text-CRS，一种通用的证明Robustness框架 для自然语言处理（NLP），基于随机抽象。我们知道现有的NLP证明Robustness可以只 certificates against $\ell_0$ 抖锋攻击。我们将每种单词级对抗操作（即同义词替换、单词重新排序、插入和删除）表示为排序和嵌入变换的组合。我们提出了新的抽象定理，以获得对这些对抗操作的Robustness保证在排序和嵌入空间中。为了进一步提高证明精度和半径，我们考虑了单词之间的数学关系，并选择合适的噪声分布来进行随机抽象。最后，我们对多种语言模型和数据集进行了substantial的实验。Text-CRS可以处理所有四种单词级对抗操作，并达到了显著的准确率提高。我们还提供了对四种单词级对抗操作的证明精度和半径的首个Benchmark，并超越了现有的对同义词替换攻击的证明。”
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Causal-Bayesian-Optimization"><a href="#Adversarial-Causal-Bayesian-Optimization" class="headerlink" title="Adversarial Causal Bayesian Optimization"></a>Adversarial Causal Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16625">http://arxiv.org/abs/2307.16625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Scott Sussex, Pier Giuseppe Sessa, Anastasiia Makarova, Andreas Krause</li>
<li>for: 这篇论文的目的是解决 causal bayesian optimization (CBO) 中其他代理或外部事件对系统的影响，以实现适应非站立性如天气变化、市场力量或敌对者。</li>
<li>methods: 这篇论文提出了 Adversarial Causal Bayesian Optimization (ACBO) 的概念，并提供了首个 ACBO 算法：Causal Bayesian Optimization with Multiplicative Weights (CBO-MW)。该方法结合了经典的在线学习策略和 causal 模型来计算优化的可能性损失。</li>
<li>results: 实验表明，CBO-MW 在 synthetic 环境和基于实际数据的环境中表现出色，并且在用户需求模式学习和车辆重新部署方面进行了实际应用。<details>
<summary>Abstract</summary>
In Causal Bayesian Optimization (CBO), an agent intervenes on an unknown structural causal model to maximize a downstream reward variable. In this paper, we consider the generalization where other agents or external events also intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users' demand patterns in a shared mobility system and reposition vehicles in strategic areas.
</details>
<details>
<summary>摘要</summary>
在 causal bayesian 优化 (CBO) 中, 一个智能体 intervenes 在一个未知的结构性 causal 模型中，以最大化一个下游奖励变量。在这篇论文中，我们考虑了泛化，其中其他智能体或外部事件也 intervenes 在系统中，这对于适应非站点性（如天气变化、市场力量或敌对者）是关键的。我们将这种泛化的 CBO 称为 Adversarial Causal Bayesian Optimization (ACBO)，并介绍了首个 ACBO 算法：Causal Bayesian Optimization with Multiplicative Weights (CBO-MW)。我们的方法结合了经典的在线学习策略和 causal 模型来计算优胜的假设性奖励估计。为此，它计算出了 causal 图中的优胜性奖励估计。我们 derive  regret bounds for CBO-MW，这些 regret bounds 自然地取决于图像相关的量。我们还提出了可扩展的实现方法，用于 combinatorial interventions 和 submodular 奖励。Empirically, CBO-MW 在 synthetic 环境和基于实际数据的环境中表现出色，超过了非 causal 和非对抗的 bayesian 优化方法。我们的实验包括一个实际的示例，用于在一个共享交通系统中学习用户的需求模式，并在策略性位置重新布置车辆。
</details></li>
</ul>
<hr>
<h2 id="Detecting-diabetic-retinopathy-severity-through-fundus-images-using-an-ensemble-of-classifiers"><a href="#Detecting-diabetic-retinopathy-severity-through-fundus-images-using-an-ensemble-of-classifiers" class="headerlink" title="Detecting diabetic retinopathy severity through fundus images using an ensemble of classifiers"></a>Detecting diabetic retinopathy severity through fundus images using an ensemble of classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16622">http://arxiv.org/abs/2307.16622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduard Popescu, Adrian Groza, Ioana Damian<br>for: This paper aims to detect diabetic retinopathy severity levels using fundus images.methods: The proposed method includes data preprocessing, image segmentation, and feature extraction, followed by an ensemble of classifiers.results: The authors assess the trust in the system and evaluate the performance of the proposed method.<details>
<summary>Abstract</summary>
Diabetic retinopathy is an ocular condition that affects individuals with diabetes mellitus. It is a common complication of diabetes that can impact the eyes and lead to vision loss. One method for diagnosing diabetic retinopathy is the examination of the fundus of the eye. An ophthalmologist examines the back part of the eye, including the retina, optic nerve, and the blood vessels that supply the retina. In the case of diabetic retinopathy, the blood vessels in the retina deteriorate and can lead to bleeding, swelling, and other changes that affect vision. We proposed a method for detecting diabetic diabetic severity levels. First, a set of data-prerpocessing is applied to available data: adaptive equalisation, color normalisation, Gaussian filter, removal of the optic disc and blood vessels. Second, we perform image segmentation for relevant markers and extract features from the fundus images. Third, we apply an ensemble of classifiers and we assess the trust in the system.
</details>
<details>
<summary>摘要</summary>
糖尿病Retinopathy是肉眼疾病，它是diabetes mellitus的常见并发症，可能影响视力。一种用于诊断糖尿病Retinopathy的方法是对眼球背部进行检查，包括retina、触觉神经和对retina的血管。在糖尿病Retinopathy中，retina中的血管会逐渐衰竭，导致内出血、软化和其他影响视力的变化。我们提出了一种用于评估糖尿病严重度的方法。首先，对可用数据进行数据处理：自适应平衡、色彩normal化、Gaussian filter和去除 optic disc和血管。其次，我们实现图像分割，提取fundus图像中相关的标记。第三，我们应用一个 ensemble of classifiers，并评估系统的可靠性。
</details></li>
</ul>
<hr>
<h2 id="LaplaceConfidence-a-Graph-based-Approach-for-Learning-with-Noisy-Labels"><a href="#LaplaceConfidence-a-Graph-based-Approach-for-Learning-with-Noisy-Labels" class="headerlink" title="LaplaceConfidence: a Graph-based Approach for Learning with Noisy Labels"></a>LaplaceConfidence: a Graph-based Approach for Learning with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16614">http://arxiv.org/abs/2307.16614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingcai Chen, Yuntao Du, Wei Tang, Baoming Zhang, Hao Cheng, Shuwei Qian, Chongjun Wang</li>
<li>For: 本文提出了一种基于 Laplacian 能量的方法，可以在受损数据集上获得高质量的标签信任度。* Methods: 方法首先根据特征表示构建了所有受损样本的图，然后使用 Laplacian 能量来生成低能量图。clean标签应该适应低能量图，而噪音标签则不应该。* Results: 实验表明，LaplaceConfidence 方法在标准 benchmark 数据集上以及真实世界中的噪音下都能够获得更高的性能，比对 State-of-the-art 方法。<details>
<summary>Abstract</summary>
In real-world applications, perfect labels are rarely available, making it challenging to develop robust machine learning algorithms that can handle noisy labels. Recent methods have focused on filtering noise based on the discrepancy between model predictions and given noisy labels, assuming that samples with small classification losses are clean. This work takes a different approach by leveraging the consistency between the learned model and the entire noisy dataset using the rich representational and topological information in the data. We introduce LaplaceConfidence, a method that to obtain label confidence (i.e., clean probabilities) utilizing the Laplacian energy. Specifically, it first constructs graphs based on the feature representations of all noisy samples and minimizes the Laplacian energy to produce a low-energy graph. Clean labels should fit well into the low-energy graph while noisy ones should not, allowing our method to determine data's clean probabilities. Furthermore, LaplaceConfidence is embedded into a holistic method for robust training, where co-training technique generates unbiased label confidence and label refurbishment technique better utilizes it. We also explore the dimensionality reduction technique to accommodate our method on large-scale noisy datasets. Our experiments demonstrate that LaplaceConfidence outperforms state-of-the-art methods on benchmark datasets under both synthetic and real-world noise.
</details>
<details>
<summary>摘要</summary>
在实际应用中，完美的标签很少可用，这使得开发机器学习算法可以抗杂的挑战性增大。现有方法通常是基于模型预测和噪声标签之间的差异来筛选噪声，假设样本 WITH 小分类损失是干净的。这个工作采用了不同的方法，利用数据中学习模型和整个噪声数据集之间的一致性，通过数据中的丰富表达和拓扑信息来获得标签信任度。我们引入了LaplaceConfidence方法，它可以在数据中获得标签信任度（即干净概率），基于Laplacian能量。具体来说，它首先基于所有噪声样本的特征表示构建图，然后将图的laplacian能量最小化来生成一个低能量图。干净标签应该适应低能量图，而噪声标签不应该。这样，LaplaceConfidence方法可以判断数据中的干净概率。此外，LaplaceConfidence方法被包含到一种整体的鲁棒训练方法中，其中co-training技术生成不偏的标签信任度，而标签修复技术更好地利用它。我们还探索了缩放技术以适应我们的方法在大规模噪声数据上进行。我们的实验表明，LaplaceConfidence方法在标准 benchmark 数据集上比state-of-the-art方法更高效，并且在实际噪声下也表现出色。
</details></li>
</ul>
<hr>
<h2 id="Noisy-Self-Training-with-Data-Augmentations-for-Offensive-and-Hate-Speech-Detection-Tasks"><a href="#Noisy-Self-Training-with-Data-Augmentations-for-Offensive-and-Hate-Speech-Detection-Tasks" class="headerlink" title="Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks"></a>Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16609">http://arxiv.org/abs/2307.16609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jaugusto97/offense-self-training">https://github.com/jaugusto97/offense-self-training</a></li>
<li>paper_authors: João A. Leite, Carolina Scarton, Diego F. Silva</li>
<li>for:  automatic detection of offensive and hateful comments online</li>
<li>methods: self-training and noisy self-training with textual data augmentations</li>
<li>results: consistent improvement in performance regardless of model size, but noisy self-training decreases performance on offensive and hate-speech domains compared to default method<details>
<summary>Abstract</summary>
Online social media is rife with offensive and hateful comments, prompting the need for their automatic detection given the sheer amount of posts created every second. Creating high-quality human-labelled datasets for this task is difficult and costly, especially because non-offensive posts are significantly more frequent than offensive ones. However, unlabelled data is abundant, easier, and cheaper to obtain. In this scenario, self-training methods, using weakly-labelled examples to increase the amount of training data, can be employed. Recent "noisy" self-training approaches incorporate data augmentation techniques to ensure prediction consistency and increase robustness against noisy data and adversarial attacks. In this paper, we experiment with default and noisy self-training using three different textual data augmentation techniques across five different pre-trained BERT architectures varying in size. We evaluate our experiments on two offensive/hate-speech datasets and demonstrate that (i) self-training consistently improves performance regardless of model size, resulting in up to +1.5% F1-macro on both datasets, and (ii) noisy self-training with textual data augmentations, despite being successfully applied in similar settings, decreases performance on offensive and hate-speech domains when compared to the default method, even with state-of-the-art augmentations such as backtranslation.
</details>
<details>
<summary>摘要</summary>
在线社交媒体中流行着侮辱和仇恨的评论，需要自动检测这些评论的需求，因为每秒创建的帖子的数量太多。创建高质量的人类标注数据集是困难和昂贵的，特别是非侮辱的帖子比侮辱的帖子更多。然而，无标注数据却充沛，更容易、更便宜地获得。在这种情况下，自动训练方法可以被使用，使用弱标注的示例来增加训练数据的量。最近的“噪声”自动训练方法利用数据扩展技术来确保预测的一致性和增强对噪声数据和敌意攻击的抵抗力。在这篇论文中，我们对默认和噪声自动训练使用三种不同的文本数据扩展技术进行实验，并使用五种不同的预训练BERT架构，变化规模。我们对两个侮辱和仇恨频道进行评估，并证明了以下两点：（i）自动训练 invariantly 提高性，无论模型的规模如何，可以在两个频道上达到最高 +1.5% F1-macro 的性能，（ii）噪声自动训练，尽管在类似场景中得到成功，但在侮辱和仇恨频道上，与默认方法相比，即使使用最新的扩展技术，如回 перевод，也会导致性能下降。
</details></li>
</ul>
<hr>
<h2 id="NLLG-Quarterly-arXiv-Report-06-23-What-are-the-most-influential-current-AI-Papers"><a href="#NLLG-Quarterly-arXiv-Report-06-23-What-are-the-most-influential-current-AI-Papers" class="headerlink" title="NLLG Quarterly arXiv Report 06&#x2F;23: What are the most influential current AI Papers?"></a>NLLG Quarterly arXiv Report 06&#x2F;23: What are the most influential current AI Papers?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04889">http://arxiv.org/abs/2308.04889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nl2g/quaterly-arxiv">https://github.com/nl2g/quaterly-arxiv</a></li>
<li>paper_authors: Steffen Eger, Christoph Leiter, Jonas Belouadi, Ran Zhang, Aida Kostikova, Daniil Larionov, Yanran Chen, Vivian Fresen</li>
<li>为：本文主要研究目的是提供arXiv上最受欢迎的40篇论文，尤其是关于自然语言处理（NLP）和机器学习（ML）的研究。* 方法：本文使用了 норма化引用统计来确定最受欢迎的论文，并分析了这些论文的主题和特征。* 结果：研究发现，在2023年第一季度，LLMs和ChatGPT在最受欢迎的论文中占据了主导地位，但在最近几个月内，ChatGPT的 популярность已经开始下降。此外，NLP相关的论文占了Influential论文的约60%，而ML相关的论文则占了约40%。研究还发现，最受欢迎的论文中最重要的问题包括LLM效率、评估技术、伦理考虑、embodied agents和问题解决方法。<details>
<summary>Abstract</summary>
The rapid growth of information in the field of Generative Artificial Intelligence (AI), particularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML), presents a significant challenge for researchers and practitioners to keep pace with the latest developments. To address the problem of information overload, this report by the Natural Language Learning Group at Bielefeld University focuses on identifying the most popular papers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick guide to the most relevant and widely discussed research, aiding both newcomers and established researchers in staying abreast of current trends. In particular, we compile a list of the 40 most popular papers based on normalized citation counts from the first half of 2023. We observe the dominance of papers related to Large Language Models (LLMs) and specifically ChatGPT during the first half of 2023, with the latter showing signs of declining popularity more recently, however. Further, NLP related papers are the most influential (around 60\% of top papers) even though there are twice as many ML related papers in our data. Core issues investigated in the most heavily cited papers are: LLM efficiency, evaluation techniques, ethical considerations, embodied agents, and problem-solving with LLMs. Additionally, we examine the characteristics of top papers in comparison to others outside the top-40 list (noticing the top paper's focus on LLM related issues and higher number of co-authors) and analyze the citation distributions in our dataset, among others.
</details>
<details>
<summary>摘要</summary>
中文（简化字）生成人工智能（AI）领域的信息快速增长，特别是自然语言处理（NLP）和机器学习（ML）的相关领域，对研究人员和实践者来说，是一个巨大的挑战。为了解决信息泛洪的问题，本报告由比较言语学组于比辗大学编制，将关注arXiv上最受欢迎的40篇论文，强调NLP和ML领域的研究。目的是提供一份快速引导新手和已有研究人员了解当前趋势的最有影响力的研究。我们发现在2023年第一季度，LLM相关论文占据了主导地位，其中ChatGPT的影响力在最近减弱。此外，NLP相关论文占据了Influence的60%，尽管ML相关论文的数量为NLP相关论文的两倍。核心问题包括：LLM效率、评价技术、伦理考虑、具体代理人和问题解决方法。此外，我们还分析了top40篇论文的特点和其他论文之间的差异，以及数据集中的引用分布。
</details></li>
</ul>
<hr>
<h2 id="Audio-visual-video-to-speech-synthesis-with-synthesized-input-audio"><a href="#Audio-visual-video-to-speech-synthesis-with-synthesized-input-audio" class="headerlink" title="Audio-visual video-to-speech synthesis with synthesized input audio"></a>Audio-visual video-to-speech synthesis with synthesized input audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16584">http://arxiv.org/abs/2307.16584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Triantafyllos Kefalas, Yannis Panagakis, Maja Pantic</li>
<li>for: 这个论文旨在研究视频到语音合成 task 中使用视频和声音输入的效果。</li>
<li>methods: 这个论文使用了预训练的视频到语音模型来生成缺失的语音信号，然后使用视频和生成的语音作为输入，用一个Audio-Visual到语音合成模型来预测最终重建的语音。</li>
<li>results: 实验结果表明，这种方法在使用 raw waveforms 和 mel spectrograms 作为目标输出时都是成功的。<details>
<summary>Abstract</summary>
Video-to-speech synthesis involves reconstructing the speech signal of a speaker from a silent video. The implicit assumption of this task is that the sound signal is either missing or contains a high amount of noise/corruption such that it is not useful for processing. Previous works in the literature either use video inputs only or employ both video and audio inputs during training, and discard the input audio pathway during inference. In this work we investigate the effect of using video and audio inputs for video-to-speech synthesis during both training and inference. In particular, we use pre-trained video-to-speech models to synthesize the missing speech signals and then train an audio-visual-to-speech synthesis model, using both the silent video and the synthesized speech as inputs, to predict the final reconstructed speech. Our experiments demonstrate that this approach is successful with both raw waveforms and mel spectrograms as target outputs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。</SYS>>视频到语音合成 involve 重建说话人的语音信号从一个无声视频中。假设中的假设是，音频信号 either missing 或含有高量的噪声/损坏，使其不可用于处理。先前的文献中的工作 either 使用视频输入只或者在训练期间使用视频和音频输入，并在推理期间抛弃输入音频路径。在这个工作中，我们调查了使用视频和音频输入进行视频到语音合成的效果，并在训练和推理期间都使用这两种输入。我们使用预训练的视频到语音模型来合成缺失的语音信号，然后使用两个输入（即无声视频和合成的语音）来预测最终重建的语音。我们的实验表明，这种方法在 Raw waveform 和 mel spectrogram 作为目标输出时都是成功的。
</details></li>
</ul>
<hr>
<h2 id="A-multiscale-and-multicriteria-Generative-Adversarial-Network-to-synthesize-1-dimensional-turbulent-fields"><a href="#A-multiscale-and-multicriteria-Generative-Adversarial-Network-to-synthesize-1-dimensional-turbulent-fields" class="headerlink" title="A multiscale and multicriteria Generative Adversarial Network to synthesize 1-dimensional turbulent fields"></a>A multiscale and multicriteria Generative Adversarial Network to synthesize 1-dimensional turbulent fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16580">http://arxiv.org/abs/2307.16580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Granero-Belinchon, Manuel Cabeza Gallucci</li>
<li>for: This paper introduces a new neural network stochastic model to generate a 1-dimensional stochastic field with turbulent velocity statistics, with the goal of accurately capturing the energy distribution, energy cascade, and intermittency across scales in turbulence.</li>
<li>methods: The model used in this paper is a Generative Adversarial Network (GAN) with multiple multiscale optimization criteria, including physics-based criteria based on the Kolmogorov and Obukhov statistical theories of fully developed turbulence. The model is fully convolutional with varying kernel sizes, and is trained using turbulent velocity signals from grid turbulence at Modane wind tunnel.</li>
<li>results: The paper reports that the proposed model is able to accurately capture the energy distribution, energy cascade, and intermittency across scales in turbulence, as demonstrated through experiments using turbulent velocity signals from the Modane wind tunnel.<details>
<summary>Abstract</summary>
This article introduces a new Neural Network stochastic model to generate a 1-dimensional stochastic field with turbulent velocity statistics. Both the model architecture and training procedure ground on the Kolmogorov and Obukhov statistical theories of fully developed turbulence, so guaranteeing descriptions of 1) energy distribution, 2) energy cascade and 3) intermittency across scales in agreement with experimental observations. The model is a Generative Adversarial Network with multiple multiscale optimization criteria. First, we use three physics-based criteria: the variance, skewness and flatness of the increments of the generated field that retrieve respectively the turbulent energy distribution, energy cascade and intermittency across scales. Second, the Generative Adversarial Network criterion, based on reproducing statistical distributions, is used on segments of different length of the generated field. Furthermore, to mimic multiscale decompositions frequently used in turbulence's studies, the model architecture is fully convolutional with kernel sizes varying along the multiple layers of the model. To train our model we use turbulent velocity signals from grid turbulence at Modane wind tunnel.
</details>
<details>
<summary>摘要</summary>
The model is a generative adversarial network (GAN) with multiple multiscale optimization criteria. First, three physics-based criteria are used: the variance, skewness, and flatness of the increments of the generated field, which respectively retrieve the turbulent energy distribution, energy cascade, and intermittency across scales. Second, the GAN criterion, based on reproducing statistical distributions, is used on segments of different length of the generated field.Furthermore, to mimic multiscale decompositions frequently used in turbulence studies, the model architecture is fully convolutional with kernel sizes varying along the multiple layers of the model. To train the model, turbulent velocity signals from grid turbulence at Modane wind tunnel are used.
</details></li>
</ul>
<hr>
<h2 id="The-Decimation-Scheme-for-Symmetric-Matrix-Factorization"><a href="#The-Decimation-Scheme-for-Symmetric-Matrix-Factorization" class="headerlink" title="The Decimation Scheme for Symmetric Matrix Factorization"></a>The Decimation Scheme for Symmetric Matrix Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16564">http://arxiv.org/abs/2307.16564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Camilli, Marc Mézard</li>
<li>for: 研究矩阵分解的基本统计限制，以掌握其在深度学习中的应用。</li>
<li>methods: 提出了一种名为“减少”的方法，通过将问题映射到一系列神经网络模型，以实现矩阵分解。这种方法可以 theoretically analyzable，但不是最优的。</li>
<li>results: 对两种矩阵 families 进行了扩展和分析，并证明了在低温限下，这种方法的自由 entropy 采取一种统一的形式。对于稀疏牛顿假设，则证明了矩阵分解存储容量在缺失模式增加时而增长。还提出了一种基于底层搜索的简单算法，可以实现矩阵分解，无需初始化。<details>
<summary>Abstract</summary>
Matrix factorization is an inference problem that has acquired importance due to its vast range of applications that go from dictionary learning to recommendation systems and machine learning with deep networks. The study of its fundamental statistical limits represents a true challenge, and despite a decade-long history of efforts in the community, there is still no closed formula able to describe its optimal performances in the case where the rank of the matrix scales linearly with its size. In the present paper, we study this extensive rank problem, extending the alternative 'decimation' procedure that we recently introduced, and carry out a thorough study of its performance. Decimation aims at recovering one column/line of the factors at a time, by mapping the problem into a sequence of neural network models of associative memory at a tunable temperature. Though being sub-optimal, decimation has the advantage of being theoretically analyzable. We extend its scope and analysis to two families of matrices. For a large class of compactly supported priors, we show that the replica symmetric free entropy of the neural network models takes a universal form in the low temperature limit. For sparse Ising prior, we show that the storage capacity of the neural network models diverges as sparsity in the patterns increases, and we introduce a simple algorithm based on a ground state search that implements decimation and performs matrix factorization, with no need of an informative initialization.
</details>
<details>
<summary>摘要</summary>
矩阵分解是一个推理问题，因其广泛的应用领域，从词语学习到推荐系统和深度学习 networks。研究其基本统计限制是一项真正挑战，尽管社区在过去的十年内努力努力，仍没有能描述其优化性能的关闭式公式，即使矩阵的排名与其大小成直线关系。在 presente 纸上，我们研究这个广泛的排名问题，扩展我们先前提出的 'decimation' 程序，并进行了详细的性能研究。decimation 的目的是一次一列/一行的因子，通过将问题映射到一个可调温度的神经网络模型中，以实现矩阵分解。虽然不是最优的，但 decimation 具有可分析的优点。我们将其扩展到两个家族的矩阵上，并对一类受限支持的假设进行了扩展性分析。在低温限下，我们证明了神经网络模型的自由能量的复制同归于普遍形式。对于稀疏的 Иссинг 假设，我们证明了矩阵分解中存储容量的增长，并引入了一种简单的地面搜索算法，实现了矩阵分解，无需具有有用的初始化。
</details></li>
</ul>
<hr>
<h2 id="Line-Search-for-Convex-Minimization"><a href="#Line-Search-for-Convex-Minimization" class="headerlink" title="Line Search for Convex Minimization"></a>Line Search for Convex Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16560">http://arxiv.org/abs/2307.16560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurent Orseau, Marcus Hutter</li>
<li>for: 这个论文的目的是提出一种新的搜索算法，用于最小化一元函数中的极值点。</li>
<li>methods: 该算法使用了 golden-section 搜索和 bisect 搜索两种主要的原理，并且利用了函数查询和导数查询来加速收敛。</li>
<li>results: 实验表明，该算法比其前一个论文中的搜索算法更快，通常比其快得多于一倍。此外，该算法还可以用于 quasi-exact line search，并且可以与导数下降的搜索 algorithms 进行比较。<details>
<summary>Abstract</summary>
Golden-section search and bisection search are the two main principled algorithms for 1d minimization of quasiconvex (unimodal) functions. The first one only uses function queries, while the second one also uses gradient queries. Other algorithms exist under much stronger assumptions, such as Newton's method. However, to the best of our knowledge, there is no principled exact line search algorithm for general convex functions -- including piecewise-linear and max-compositions of convex functions -- that takes advantage of convexity. We propose two such algorithms: $\Delta$-Bisection is a variant of bisection search that uses (sub)gradient information and convexity to speed up convergence, while $\Delta$-Secant is a variant of golden-section search and uses only function queries.   While bisection search reduces the $x$ interval by a factor 2 at every iteration, $\Delta$-Bisection reduces the (sometimes much) smaller $x^*$-gap $\Delta^x$ (the $x$ coordinates of $\Delta$) by at least a factor 2 at every iteration. Similarly, $\Delta$-Secant also reduces the $x^*$-gap by at least a factor 2 every second function query. Moreover, the $y^*$-gap $\Delta^y$ (the $y$ coordinates of $\Delta$) also provides a refined stopping criterion, which can also be used with other algorithms. Experiments on a few convex functions confirm that our algorithms are always faster than their quasiconvex counterparts, often by more than a factor 2.   We further design a quasi-exact line search algorithm based on $\Delta$-Secant. It can be used with gradient descent as a replacement for backtracking line search, for which some parameters can be finicky to tune -- and we provide examples to this effect, on strongly-convex and smooth functions. We provide convergence guarantees, and confirm the efficiency of quasi-exact line search on a few single- and multivariate convex functions.
</details>
<details>
<summary>摘要</summary>
金叶搜索和BIsection搜索是一维最小化逻辑函数的两种主要原则化算法。前者只使用函数查询，而后者还使用梯度查询。其他算法存在更加强大的假设，如新颖方法。然而，我们所知道的情况下，没有原则化正确线搜索算法，可以在一般凸函数（包括分割凸函数和最大组合凸函数）上使用凸性，并且可以快速 converge。我们提出了两种算法：Δ-BIsection是BIsection搜索的变种，使用（子）梯度信息和凸性来加速收敛，而Δ-Secant是金叶搜索的变种，只使用函数查询。在每次迭代中，BIsection搜索将($x$)间隔减少为2，而Δ-BIsection将($x^*$)间隔减少为至少2。 similarly，Δ-Secant将($x^*$)间隔减少为至少2 every second function query。此外，($y^*$)间隔还提供了一个精细的停止条件，可以与其他算法一起使用。我们的实验表明，我们的算法在一些凸函数上比其相对凸函数更快，常常高于2倍。我们还设计了一种 quasi-exact line search algorithm，基于Δ-Secant。它可以与梯度下降为替换backtracking line search，对于一些参数可能需要繁琐调整。我们提供了一些示例，包括强拟合函数和 глад度函数。我们提供了收敛保证，并在一些单变量和多变量凸函数上验证了 quasi-exact line search 的效率。
</details></li>
</ul>
<hr>
<h2 id="Simultaneous-column-based-deep-learning-progression-analysis-of-atrophy-associated-with-AMD-in-longitudinal-OCT-studies"><a href="#Simultaneous-column-based-deep-learning-progression-analysis-of-atrophy-associated-with-AMD-in-longitudinal-OCT-studies" class="headerlink" title="Simultaneous column-based deep learning progression analysis of atrophy associated with AMD in longitudinal OCT studies"></a>Simultaneous column-based deep learning progression analysis of atrophy associated with AMD in longitudinal OCT studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16559">http://arxiv.org/abs/2307.16559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adi Szeskin, Roei Yehuda, Or Shmueli, Jaime Levy, Leo Joskowicz</li>
<li>For: The paper aims to develop a fully automatic end-to-end pipeline for detecting and quantifying retinal atrophy changes associated with dry AMD in pairs of OCT scans.* Methods: The proposed method uses a novel simultaneous multi-channel column-based deep learning model that concurrently detects and segments retinal atrophy segments in consecutive OCT scans by classifying light scattering patterns in matched pairs of vertical pixel-wide columns (A-scans) in registered prior and current OCT slices (B-scans).* Results: The experimental results on a dataset of 4,040 OCT slices with 5.2M columns from 40 scan pairs of 18 patients show a mean atrophy segments detection precision of 0.90+-0.09 and a recall of 0.95+-0.06, outperforming standalone classification methods by 30+-62% and 27+-0% for atrophy segments and lesions, respectively.Here’s the same information in Simplified Chinese text:* For: 本研究旨在开发一个完全自动的末端到终端管道，用于检测和评估涂炭病关节associated with dry AMD的脑细胞变化。* Methods: 提议的方法使用了一种新的同时多通道列基的深度学习模型，该模型同时检测并分割涂炭病关节的变化，通过匹配的列宽像素列（A-scan）在注册的先前和当前 OCT slice（B-scan）中类型化光散射模式。* Results: 实验结果表明，在40个扫描对（18名患者，每名患者有40个扫描）的4,040个 OCT slice 中，5.2亿个列上的结果显示，同时检测和分割涂炭病关节的方法可以达到0.90+-0.09的检测精度和0.95+-0.06的回归率，相比单独的检测方法，提高了30+-62%和27+-0%。<details>
<summary>Abstract</summary>
Purpose: Disease progression of retinal atrophy associated with AMD requires the accurate quantification of the retinal atrophy changes on longitudinal OCT studies. It is based on finding, comparing, and delineating subtle atrophy changes on consecutive pairs (prior and current) of unregistered OCT scans. Methods: We present a fully automatic end-to-end pipeline for the simultaneous detection and quantification of time-related atrophy changes associated with dry AMD in pairs of OCT scans of a patient. It uses a novel simultaneous multi-channel column-based deep learning model trained on registered pairs of OCT scans that concurrently detects and segments retinal atrophy segments in consecutive OCT scans by classifying light scattering patterns in matched pairs of vertical pixel-wide columns (A-scans) in registered prior and current OCT slices (B-scans). Results: Experimental results on 4,040 OCT slices with 5.2M columns from 40 scans pairs of 18 patients (66% training/validation, 33% testing) with 24.13+-14.0 months apart in which Complete RPE and Outer Retinal Atrophy (cRORA) was identified in 1,998 OCT slices (735 atrophy lesions from 3,732 segments, 0.45M columns) yield a mean atrophy segments detection precision, recall of 0.90+-0.09, 0.95+-0.06 and 0.74+-0.18, 0.94+-0.12 for atrophy lesions with AUC=0.897, all above observer variability. Simultaneous classification outperforms standalone classification precision and recall by 30+-62% and 27+-0% for atrophy segments and lesions. Conclusions: simultaneous column-based detection and quantification of retinal atrophy changes associated with AMD is accurate and outperforms standalone classification methods. Translational relevance: an automatic and efficient way to detect and quantify retinal atrophy changes associated with AMD.
</details>
<details>
<summary>摘要</summary>
目的：检测和评估普遍疾病相关的肉眼衰竭变化，需要精准地量化 consecutiveslices of OCT Studies。这是基于发现，比较和定义极微的衰竭变化的方法。方法：我们提出了一个完全自动的终端到终点管道，用于同时检测和评估普遍疾病相关的肉眼衰竭变化。这种方法使用了一种同时多通道的 column-based深度学习模型，该模型在注册的Prior和Current OCT slice之间同时检测和分割肉眼衰竭 segment。该模型通过匹配vertical pixel-wide columns（A-scans）在注册的Prior和Current OCT slice（B-scans）中匹配的光散射模式来同时检测和分割肉眼衰竭segment。结果：我们在4,040个OCT slice中进行了40个scan pairs的实验，其中每个scan pairs包含24.13±14.0个月的时间差。在这些实验中，我们发现了1,998个OCT slice中存在普遍疾病相关的肉眼衰竭（cRORA），其中735个衰竭 lesion from 3,732个 segment，0.45M columns。我们的方法在这些OCT slice中达到了 mean atrophy segments检测精度和回归的0.90±0.09和0.95±0.06，同时 simultanous classification的精度和回归也高于单独的 classification方法，相对于衰竭 segments和 lesions的检测和分割， simultaneous classification的精度和回归高于30±62%和27±0%。结论：同时检测和评估普遍疾病相关的肉眼衰竭变化是一种准确的方法，并且高于单独的 classification方法。翻译结论：我们提出了一种自动和高效的方法，可以帮助检测和评估普遍疾病相关的肉眼衰竭变化。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-and-Computer-Vision-for-Glaucoma-Detection-A-Review"><a href="#Deep-Learning-and-Computer-Vision-for-Glaucoma-Detection-A-Review" class="headerlink" title="Deep Learning and Computer Vision for Glaucoma Detection: A Review"></a>Deep Learning and Computer Vision for Glaucoma Detection: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16528">http://arxiv.org/abs/2307.16528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mona Ashtari-Majlan, Mohammad Mahdi Dehshibi, David Masip</li>
<li>for: 本研究旨在概述近年来计算机视觉和深度学习在诊断眼内压瘤方面的应用，包括基于fundus、optical coherence tomography和视场图像的诊断方法。</li>
<li>methods: 本研究主要介绍了深度学习基于方法，并提供了一个更新的分类法，将方法分为不同的建筑学 paradigma，并附上了可用的源代码以增强方法的重复性。</li>
<li>results: 通过对广泛使用的公共数据集进行严格的 benchmarking，我们揭示了一些普遍的性能差距，包括总体化、不确定性估计和多modal интеграción。此外，我们还细目了一些关键的数据集，并指出了限制，如批处大小、标签不一致和偏见。<details>
<summary>Abstract</summary>
Glaucoma is the leading cause of irreversible blindness worldwide and poses significant diagnostic challenges due to its reliance on subjective evaluation. However, recent advances in computer vision and deep learning have demonstrated the potential for automated assessment. In this paper, we survey recent studies on AI-based glaucoma diagnosis using fundus, optical coherence tomography, and visual field images, with a particular emphasis on deep learning-based methods. We provide an updated taxonomy that organizes methods into architectural paradigms and includes links to available source code to enhance the reproducibility of the methods. Through rigorous benchmarking on widely-used public datasets, we reveal performance gaps in generalizability, uncertainty estimation, and multimodal integration. Additionally, our survey curates key datasets while highlighting limitations such as scale, labeling inconsistencies, and bias. We outline open research challenges and detail promising directions for future studies. This survey is expected to be useful for both AI researchers seeking to translate advances into practice and ophthalmologists aiming to improve clinical workflows and diagnosis using the latest AI outcomes.
</details>
<details>
<summary>摘要</summary>
全球最主要的眼病之一是水肿眼病，它具有较大的检测挑战，主要是因为它的诊断依赖于主观评估。然而，最近的计算机视觉和深度学习技术的进步已经表明了自动诊断的潜在可能性。在这篇论文中，我们回顾了最近的人工智能基于眼膜、光共振成像和视场图像的眼病诊断研究，尤其是深度学习基本的方法。我们提供了一个更新的分类法，将方法分为建筑学 paradigma，并提供了可用的源代码，以便增强方法的重现性。通过对广泛使用的公共数据集进行严格的 benchmarking，我们揭示了总体的一致性、不确定性估计和多modal集成的性能差距。此外，我们还提供了关键的数据集，并强调了限制，如规模、标签不一致和偏见。我们列出了未解决的研究挑战，并详细介绍了未来研究的可能性。这篇论文预计会对人工智能研究人员和眼科医生都是有用的，以便将最新的成果落实到实践中，并改善诊断和临床工作流程。
</details></li>
</ul>
<hr>
<h2 id="No-Fair-Lunch-A-Causal-Perspective-on-Dataset-Bias-in-Machine-Learning-for-Medical-Imaging"><a href="#No-Fair-Lunch-A-Causal-Perspective-on-Dataset-Bias-in-Machine-Learning-for-Medical-Imaging" class="headerlink" title="No Fair Lunch: A Causal Perspective on Dataset Bias in Machine Learning for Medical Imaging"></a>No Fair Lunch: A Causal Perspective on Dataset Bias in Machine Learning for Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16526">http://arxiv.org/abs/2307.16526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charles Jones, Daniel C. Castro, Fabio De Sousa Ribeiro, Ozan Oktay, Melissa McCradden, Ben Glocker</li>
<li>for: This paper is written for those who are concerned about fairness in clinical decision-making, particularly in the context of machine learning methods.</li>
<li>methods: The paper uses a causal perspective to identify and analyze different sources of bias in datasets, and introduces a three-step framework for reasoning about fairness in medical imaging.</li>
<li>results: The paper highlights the limitations of current mitigation methods for algorithmic bias, and provides a practical framework for developing safe and equitable AI prediction models.<details>
<summary>Abstract</summary>
As machine learning methods gain prominence within clinical decision-making, addressing fairness concerns becomes increasingly urgent. Despite considerable work dedicated to detecting and ameliorating algorithmic bias, today's methods are deficient with potentially harmful consequences. Our causal perspective sheds new light on algorithmic bias, highlighting how different sources of dataset bias may appear indistinguishable yet require substantially different mitigation strategies. We introduce three families of causal bias mechanisms stemming from disparities in prevalence, presentation, and annotation. Our causal analysis underscores how current mitigation methods tackle only a narrow and often unrealistic subset of scenarios. We provide a practical three-step framework for reasoning about fairness in medical imaging, supporting the development of safe and equitable AI prediction models.
</details>
<details>
<summary>摘要</summary>
随着机器学习方法在医疗决策中升级，对公平性问题的解决变得越来越紧迫。虽然已经投入了大量的时间和精力来检测和改进算法的偏见，但今天的方法仍然存在有害的后果。我们的 causal 视角 shed 新的光 на算法偏见，指出不同的数据集偏见可能会看起来相同，但需要不同的修正策略。我们介绍了三种家族的 causal 偏见机制，来自不同的发病率、展示和注释的偏见。我们的 causal 分析表明，当前的修正方法只能处理一个窄而且经常不现实的子集的场景。我们提供了一个实用的三步框架，以便在医疗影像领域考虑公平性，支持开发安全和公平的 AI 预测模型。
</details></li>
</ul>
<hr>
<h2 id="Deception-Abilities-Emerged-in-Large-Language-Models"><a href="#Deception-Abilities-Emerged-in-Large-Language-Models" class="headerlink" title="Deception Abilities Emerged in Large Language Models"></a>Deception Abilities Emerged in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16513">http://arxiv.org/abs/2307.16513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thilo Hagendorff</li>
<li>for: 这研究旨在探讨大型自然语言模型（LLM）是如何适应人类价值观的，以及未来 LLM 是否可能成为人类操作员的欺骗工具。</li>
<li>methods: 该研究使用现代大型语言模型 GPT-4 进行实验，以证明这些模型在逻辑推理和复杂的欺骗场景中表现出色。</li>
<li>results: 研究发现，现代 LLM 已经拥有了骗取他人信任的能力，并且可以通过链条思维提高其欺骗性能。此外，通过引入 MACHIAVELLIANISM 来调节 LLM 的骗取倾向也被证明有效。<details>
<summary>Abstract</summary>
Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown machine behavior in LLMs, our study contributes to the nascent field of machine psychology.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Classifying-multilingual-party-manifestos-Domain-transfer-across-country-time-and-genre"><a href="#Classifying-multilingual-party-manifestos-Domain-transfer-across-country-time-and-genre" class="headerlink" title="Classifying multilingual party manifestos: Domain transfer across country, time, and genre"></a>Classifying multilingual party manifestos: Domain transfer across country, time, and genre</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16511">http://arxiv.org/abs/2307.16511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slds-lmu/manifesto-domaintransfer">https://github.com/slds-lmu/manifesto-domaintransfer</a></li>
<li>paper_authors: Matthias Aßenmacher, Nadja Sauter, Christian Heumann</li>
<li>for: 这个研究的目的是探讨域转移在政治宣言中的可靠性和可重用性。</li>
<li>methods: 这个研究使用了 transformer 模型，并通过 fine-tuning 来调整模型。在不同的地理位置、语言、时间和风格上进行了域转移。</li>
<li>results: 研究发现，BERT 和 DistilBERT 都能够在不同的域转移情况下获得良好的表现，但 DistilBERT 的计算成本较低。此外，研究发现不同国家的政治宣言之间存在一定的差异，即使这些国家使用同一种语言或文化背景。<details>
<summary>Abstract</summary>
Annotating costs of large corpora are still one of the main bottlenecks in empirical social science research. On the one hand, making use of the capabilities of domain transfer allows re-using annotated data sets and trained models. On the other hand, it is not clear how well domain transfer works and how reliable the results are for transfer across different dimensions. We explore the potential of domain transfer across geographical locations, languages, time, and genre in a large-scale database of political manifestos. First, we show the strong within-domain classification performance of fine-tuned transformer models. Second, we vary the genre of the test set across the aforementioned dimensions to test for the fine-tuned models' robustness and transferability. For switching genres, we use an external corpus of transcribed speeches from New Zealand politicians while for the other three dimensions, custom splits of the Manifesto database are used. While BERT achieves the best scores in the initial experiments across modalities, DistilBERT proves to be competitive at a lower computational expense and is thus used for further experiments across time and country. The results of the additional analysis show that (Distil)BERT can be applied to future data with similar performance. Moreover, we observe (partly) notable differences between the political manifestos of different countries of origin, even if these countries share a language or a cultural background.
</details>
<details>
<summary>摘要</summary>
大型公司的标注成本仍是employmatical社会科学研究中的主要瓶颈。一方面，使用领域传递的能力可以重用标注数据集和训练模型。另一方面，不清楚领域传递如何工作，以及如何确定结果的可靠性。我们在一个大规模的政治宣言数据库中探索领域传递的潜力。首先，我们显示了针对域内数据集进行细化的转换器模型的强大在域内分类性能。其次，我们在不同的维度上变换测试集，以测试细化模型的可重用性和鲁棒性。为switching genre，我们使用新西兰政治人物的口头演讲录音库；对其他三个维度，我们使用 manuscripdbase中的自定义分割。虽然BERT在初始实验中Across modalities achieve the best scores, DistilBERT proves to be competitive at a lower computational expense and is thus used for further experiments across time and country. Additional analysis shows that (Distil)BERT can be applied to future data with similar performance. Moreover, we observe (partly) notable differences between the political manifestos of different countries of origin, even if these countries share a language or a cultural background.
</details></li>
</ul>
<hr>
<h2 id="Explainable-Equivariant-Neural-Networks-for-Particle-Physics-PELICAN"><a href="#Explainable-Equivariant-Neural-Networks-for-Particle-Physics-PELICAN" class="headerlink" title="Explainable Equivariant Neural Networks for Particle Physics: PELICAN"></a>Explainable Equivariant Neural Networks for Particle Physics: PELICAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16506">http://arxiv.org/abs/2307.16506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abogatskiy/pelican">https://github.com/abogatskiy/pelican</a></li>
<li>paper_authors: Alexander Bogatskiy, Timothy Hoffman, David W. Miller, Jan T. Offermann, Xiaoyang Liu</li>
<li>For: The paper is written for the task of tagging and reconstructing Lorentz-boosted top quarks, specifically identifying and measuring the $W$-boson in the dense final state.* Methods: The paper proposes a novel permutation equivariant and Lorentz invariant or covariant aggregator network called PELICAN, which employs a fundamentally symmetry group-based architecture to overcome common limitations in particle physics problems.* Results: PELICAN outperforms existing competitors with much lower model complexity and high sample efficiency on the standard task of Lorentz-boosted top quark tagging, and also outperforms hand-crafted algorithms on the less common and more complex task of four-momentum regression.<details>
<summary>Abstract</summary>
We present a comprehensive study of the PELICAN machine learning algorithm architecture in the context of both tagging (classification) and reconstructing (regression) Lorentz-boosted top quarks, including the difficult task of specifically identifying and measuring the $W$-boson inside the dense environment of the boosted hadronic final state. PELICAN is a novel permutation equivariant and Lorentz invariant or covariant aggregator network designed to overcome common limitations found in architectures applied to particle physics problems. Compared to many approaches that use non-specialized architectures that neglect underlying physics principles and require very large numbers of parameters, PELICAN employs a fundamentally symmetry group-based architecture that demonstrates benefits in terms of reduced complexity, increased interpretability, and raw performance. When tested on the standard task of Lorentz-boosted top quark tagging, PELICAN outperforms existing competitors with much lower model complexity and high sample efficiency. On the less common and more complex task of four-momentum regression, PELICAN also outperforms hand-crafted algorithms. We discuss the implications of symmetry-restricted architectures for the wider field of machine learning for physics.
</details>
<details>
<summary>摘要</summary>
我们提出了一项全面的PELICAN机器学习算法架构研究，包括标记（分类）和重建（回归） Lorentz-扩展类题粒子，包括困难的内部 $W-$  boson 识别和量测在扩展有核心环境的对应核心粒子状态中。 PELICAN 是一个新的对称平衡和 Lorentz 不变的网络架构，用于超越物理问题中常见的限制。 相比许多使用非特殊架构的方法，PELICAN 使用基本的 Symmetry 集合 基础架构，实现了对复杂性、可读性和原生性的改善。 在标准任务中 Lorentz-扩展类题粒子标识中，PELICAN 超过了现有的竞争对手，具有较低的模型复杂度和高的样本效率。 在更少见且更复杂的任务中，四维动量回归中，PELICAN 也超过了手工构成的算法。 我们讨论了对物理机器学习领域的对称限制的影响。
</details></li>
</ul>
<hr>
<h2 id="Value-Informed-Skill-Chaining-for-Policy-Learning-of-Long-Horizon-Tasks-with-Surgical-Robot"><a href="#Value-Informed-Skill-Chaining-for-Policy-Learning-of-Long-Horizon-Tasks-with-Surgical-Robot" class="headerlink" title="Value-Informed Skill Chaining for Policy Learning of Long-Horizon Tasks with Surgical Robot"></a>Value-Informed Skill Chaining for Policy Learning of Long-Horizon Tasks with Surgical Robot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16503">http://arxiv.org/abs/2307.16503</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/med-air/viskill">https://github.com/med-air/viskill</a></li>
<li>paper_authors: Tao Huang, Kai Chen, Wang Wei, Jianan Li, Yonghao Long, Qi Dou</li>
<li>for:  solves long-horizon surgical robot tasks with multiple steps over an extended duration of time.</li>
<li>methods:  uses value-informed skill chaining (ViSkill) with a state value function to distinguish suitable terminal states for starting subtask policies, and a chaining policy to instruct subtask policies to terminate at the highest-value state.</li>
<li>results:  demonstrates effectiveness on three complex surgical robot tasks from SurRoL, achieving high task success rates and execution efficiency.<details>
<summary>Abstract</summary>
Reinforcement learning is still struggling with solving long-horizon surgical robot tasks which involve multiple steps over an extended duration of time due to the policy exploration challenge. Recent methods try to tackle this problem by skill chaining, in which the long-horizon task is decomposed into multiple subtasks for easing the exploration burden and subtask policies are temporally connected to complete the whole long-horizon task. However, smoothly connecting all subtask policies is difficult for surgical robot scenarios. Not all states are equally suitable for connecting two adjacent subtasks. An undesired terminate state of the previous subtask would make the current subtask policy unstable and result in a failed execution. In this work, we introduce value-informed skill chaining (ViSkill), a novel reinforcement learning framework for long-horizon surgical robot tasks. The core idea is to distinguish which terminal state is suitable for starting all the following subtask policies. To achieve this target, we introduce a state value function that estimates the expected success probability of the entire task given a state. Based on this value function, a chaining policy is learned to instruct subtask policies to terminate at the state with the highest value so that all subsequent policies are more likely to be connected for accomplishing the task. We demonstrate the effectiveness of our method on three complex surgical robot tasks from SurRoL, a comprehensive surgical simulation platform, achieving high task success rates and execution efficiency. Code is available at $\href{https://github.com/med-air/ViSkill}{\text{https://github.com/med-air/ViSkill}$.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用强化学习解决长时间间隔的外科机器人任务仍然面临着策略探索挑战。现有方法是通过精细分解任务，将长时间间隔的任务分解成多个子任务，以减轻探索压力。但是，在外科机器人场景下，平滑地连接所有子任务策略是困难的。不是所有状态都适合连接两个相邻的子任务策略。undesired terminate state of the previous subtask would make the current subtask policy unstable and result in a failed execution。在这种情况下，我们提出了值知识推荐技术（ViSkill），一种新的强化学习框架，用于解决长时间间隔的外科机器人任务。核心思想是在不同状态下分配不同的策略，以确保连接所有子任务策略。为此，我们引入了一个状态价值函数，用于估计整个任务的成功概率。基于这个价值函数，我们学习了一个链接策略，用于指定子任务策略在最高价值的状态中终止，以确保所有后续策略能够连接成功完成任务。我们在三个复杂的外科机器人任务上进行了实验，分别来自SurRoL数据平台，实现了高任务成功率和执行效率。代码可以在 $\href{https://github.com/med-air/ViSkill}{\text{https://github.com/med-air/ViSkill}$ 上获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-Generalizable-Tool-Use-with-Non-rigid-Grasp-pose-Registration"><a href="#Learning-Generalizable-Tool-Use-with-Non-rigid-Grasp-pose-Registration" class="headerlink" title="Learning Generalizable Tool Use with Non-rigid Grasp-pose Registration"></a>Learning Generalizable Tool Use with Non-rigid Grasp-pose Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16499">http://arxiv.org/abs/2307.16499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malte Mosbach, Sven Behnke</li>
<li>for: 本研究旨在帮助机器人学习工具使用行为。</li>
<li>methods: 该方法使用一个单一示例来学习新类型工具的操作。它使用了多指手套的抓取配置的普适化，通过有利的初始化和形式化奖励信号来导引政策搜索。</li>
<li>results: 学习出来的策略可以解决复杂的工具使用任务，并可以在未看过的工具上进行推广。视频和图像可以在<a target="_blank" rel="noopener" href="https://maltemosbach.github.io/generalizable_tool_use%E4%B8%8A%E6%9F%A5%E7%9C%8B%E3%80%82">https://maltemosbach.github.io/generalizable_tool_use上查看。</a><details>
<summary>Abstract</summary>
Tool use, a hallmark feature of human intelligence, remains a challenging problem in robotics due the complex contacts and high-dimensional action space. In this work, we present a novel method to enable reinforcement learning of tool use behaviors. Our approach provides a scalable way to learn the operation of tools in a new category using only a single demonstration. To this end, we propose a new method for generalizing grasping configurations of multi-fingered robotic hands to novel objects. This is used to guide the policy search via favorable initializations and a shaped reward signal. The learned policies solve complex tool use tasks and generalize to unseen tools at test time. Visualizations and videos of the trained policies are available at https://maltemosbach.github.io/generalizable_tool_use.
</details>
<details>
<summary>摘要</summary>
人类智能的一个标志性特征是工具使用，但在机器人学中，这种问题仍然是一个挑战。在这篇论文中，我们提出了一种新的方法来启用机器人学习工具使用行为。我们的方法可以在新类别中学习工具的操作，只需要一个示例。为实现这一目标，我们提出了一种新的方法来泛化多指手 robotic 手上的抓取配置到新物体。这种方法通过提供有利初始化和形成的奖励信号来导引政策搜索。我们的学习策略解决了复杂的工具使用任务，并在测试时对未看过的工具进行扩展。可以在 <https://maltemosbach.github.io/generalizable_tool_use> 查看视频和图像。
</details></li>
</ul>
<hr>
<h2 id="Don’t-be-so-negative-Score-based-Generative-Modeling-with-Oracle-assisted-Guidance"><a href="#Don’t-be-so-negative-Score-based-Generative-Modeling-with-Oracle-assisted-Guidance" class="headerlink" title="Don’t be so negative! Score-based Generative Modeling with Oracle-assisted Guidance"></a>Don’t be so negative! Score-based Generative Modeling with Oracle-assisted Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16463">http://arxiv.org/abs/2307.16463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeid Naderiparizi, Xiaoxuan Liang, Berend Zwartsenberg, Frank Wood</li>
<li>for: The paper is written for discussing a new method called Gen-neG, which leverages side-information in the form of an oracle to improve the learning of probabilistic models.</li>
<li>methods: The paper uses a combination of generative adversarial networks (GANs) and discriminator guidance in diffusion models to guide the generation process towards the positive support region indicated by the oracle.</li>
<li>results: The paper presents empirical results in applications including collision avoidance in self-driving simulators and safety-guarded human motion generation, demonstrating the utility of the proposed Gen-neG method.Here’s the same information in Simplified Chinese text:</li>
<li>for: 本文是用来介绍一种新的方法called Gen-neG，该方法利用 oracle 提供的侧 информацию来改进概率模型的学习。</li>
<li>methods: 本文使用了一种组合了生成对抗网络（GANs）和混合环境导向的扩散模型，以便通过 oracle 指定的正方向区域来导引生成过程。</li>
<li>results: 本文对自动驾驶模拟器中的碰撞避免和人体动作生成等应用中进行了实验，并证明了 Gen-neG 方法的实用性。<details>
<summary>Abstract</summary>
The maximum likelihood principle advocates parameter estimation via optimization of the data likelihood function. Models estimated in this way can exhibit a variety of generalization characteristics dictated by, e.g. architecture, parameterization, and optimization bias. This work addresses model learning in a setting where there further exists side-information in the form of an oracle that can label samples as being outside the support of the true data generating distribution. Specifically we develop a new denoising diffusion probabilistic modeling (DDPM) methodology, Gen-neG, that leverages this additional side-information. Our approach builds on generative adversarial networks (GANs) and discriminator guidance in diffusion models to guide the generation process towards the positive support region indicated by the oracle. We empirically establish the utility of Gen-neG in applications including collision avoidance in self-driving simulators and safety-guarded human motion generation.
</details>
<details>
<summary>摘要</summary>
“最大可能性原则”提倡通过数据可能函数估计 параметр。这种方法可以实现多种通用特征，例如建筑、参数化和估计偏好。这个工作在存在 oracle 提供样本是否在真实数据生成分布中的支持下的情况下进行模型学习。我们开发了一种新的推导散布模型方法（DDPM），叫做 Gen-neG，它利用这些额外的side-information。我们的方法基于生成对抗网络（GANs）和推导器指导在散布模型中引导生成过程，以便将生成结果导向正确的支持区域。我们经过实验证明 Gen-neG 在包括自驾车 simulation 和人类动作生成等应用中的实用性。
</details></li>
</ul>
<hr>
<h2 id="L3DMC-Lifelong-Learning-using-Distillation-via-Mixed-Curvature-Space"><a href="#L3DMC-Lifelong-Learning-using-Distillation-via-Mixed-Curvature-Space" class="headerlink" title="L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space"></a>L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16459">http://arxiv.org/abs/2307.16459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csiro-robotics/l3dmc">https://github.com/csiro-robotics/l3dmc</a></li>
<li>paper_authors: Kaushik Roy, Peyman Moghadam, Mehrtash Harandi</li>
<li>for: 提高生长学习（L3）模型在连续学习任务中的表现，解决L3模型在学习新概念时的性能下降问题。</li>
<li>methods: 提议使用混合曲率空间（mixed-curvature space）来保持已经学习的知识，并使用多个固定曲率空间（fixed-curvature spaces）的表示能力来增强模型的表达力。</li>
<li>results: 在三个标准测试集上进行了实验，证明了我们提议的混合曲率空间防止忘记旧知识并更好地适应新知识的方法可以提高L3模型在医学图像分类任务中的表现。<details>
<summary>Abstract</summary>
The performance of a lifelong learning (L3) model degrades when it is trained on a series of tasks, as the geometrical formation of the embedding space changes while learning novel concepts sequentially. The majority of existing L3 approaches operate on a fixed-curvature (e.g., zero-curvature Euclidean) space that is not necessarily suitable for modeling the complex geometric structure of data. Furthermore, the distillation strategies apply constraints directly on low-dimensional embeddings, discouraging the L3 model from learning new concepts by making the model highly stable. To address the problem, we propose a distillation strategy named L3DMC that operates on mixed-curvature spaces to preserve the already-learned knowledge by modeling and maintaining complex geometrical structures. We propose to embed the projected low dimensional embedding of fixed-curvature spaces (Euclidean and hyperbolic) to higher-dimensional Reproducing Kernel Hilbert Space (RKHS) using a positive-definite kernel function to attain rich representation. Afterward, we optimize the L3 model by minimizing the discrepancies between the new sample representation and the subspace constructed using the old representation in RKHS. L3DMC is capable of adapting new knowledge better without forgetting old knowledge as it combines the representation power of multiple fixed-curvature spaces and is performed on higher-dimensional RKHS. Thorough experiments on three benchmarks demonstrate the effectiveness of our proposed distillation strategy for medical image classification in L3 settings. Our code implementation is publicly available at https://github.com/csiro-robotics/L3DMC.
</details>
<details>
<summary>摘要</summary>
“一个生命时间学习（L3）模型的性能会随着在不同任务上的训练，而逐渐下降。现有大多数L3方法都是在固定曲率（例如零曲率欧几里得）空间中进行训练，这并不一定适合数据的复杂的几何结构。另外，维持抽象策略会直接在低维度嵌入上加载约束，使L3模型学习新的概念变得更加困难。为解决这问题，我们提出了一种名为L3DMC的维持策略，它在混合曲率空间中进行训练，以保留已经学习的知识，并在高维度的复制函数希尔бер特空间（RKHS）中进行嵌入。然后，我们将L3模型进行优化，使其在新样本表示中与以前的表示在RKHS中构建的子空间之间的差异最小化。L3DMC可以更好地适应新的知识，而不是忘记过去的知识，因为它结合了多个固定曲率空间的表示能力，并在高维度RKHS中进行训练。我们在三个标准检验 задании上进行了详细的实验，并证明了L3DMC的效iveness。我们的代码实现可以在https://github.com/csiro-robotics/L3DMC上获得。”
</details></li>
</ul>
<hr>
<h2 id="An-Effective-Data-Creation-Pipeline-to-Generate-High-quality-Financial-Instruction-Data-for-Large-Language-Model"><a href="#An-Effective-Data-Creation-Pipeline-to-Generate-High-quality-Financial-Instruction-Data-for-Large-Language-Model" class="headerlink" title="An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model"></a>An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01415">http://arxiv.org/abs/2308.01415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziao Wang, Jianning Wang, Junda Wu, Xiaofeng Zhang</li>
<li>for: 这篇论文主要是为了提供一个高质量的金融数据集，以便使用大语言模型进行金融相关任务的细化调教。</li>
<li>methods: 该论文提出了一种仔细设计的数据创建管道，包括通过ChatGPT与人工金融专家之间的对话，并根据人工Feedback进行数据集的细化。</li>
<li>results: 该管道生成了一个 Robust 的征调数据集，包括103k多个多Turn chat，并通过对这个数据集进行了广泛的实验，以评估模型的性能。结果表明，该方法可以使AI模型生成准确、相关、金融式的回答，从而为金融领域应用提供一个强大的工具。<details>
<summary>Abstract</summary>
At the beginning era of large language model, it is quite critical to generate a high-quality financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model's performance by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models, and thus providing a powerful tool for applications within the financial sector.
</details>
<details>
<summary>摘要</summary>
在大语言模型时代的开始，生成高质量金融数据集是非常重要的，以调整大语言模型进行金融相关任务。这篇论文提出了一个仔细设计的数据创建管道，特别是通过与人工金融专家的对话，使用ChatGPT，并根据人类金融专家的反馈，进行数据集的精细调整。这个管道生成了103k多turn对话数据集。我们在这个数据集上进行了广泛的实验，采用外部GPT-4作为评审者，以评估模型的性能。实验结果表明，我们的方法导致了AI模型生成高准确、相关、金融风格的回答，从而为金融领域应用提供了一个强大的工具。
</details></li>
</ul>
<hr>
<h2 id="A-continuous-Structural-Intervention-Distance-to-compare-Causal-Graphs"><a href="#A-continuous-Structural-Intervention-Distance-to-compare-Causal-Graphs" class="headerlink" title="A continuous Structural Intervention Distance to compare Causal Graphs"></a>A continuous Structural Intervention Distance to compare Causal Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16452">http://arxiv.org/abs/2307.16452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mihir Dhanakshirur, Felix Laumann, Junhyung Park, Mauricio Barahona</li>
<li>for: 本研究旨在提供一种新的维度度量，用于评估真实和学习的 causal 图之间的差异。</li>
<li>methods: 本研究使用 conditional mean embeddings 将 intervención 分布映射到 reproduce kernel Hilbert space 中，然后计算这些分布之间的最大（conditional）mean discrepancy，来评估 causal 图的差异。</li>
<li>results: 研究人员通过 theoretically 和数据实验验证了这种新的维度度量的有效性。<details>
<summary>Abstract</summary>
Understanding and adequately assessing the difference between a true and a learnt causal graphs is crucial for causal inference under interventions. As an extension to the graph-based structural Hamming distance and structural intervention distance, we propose a novel continuous-measured metric that considers the underlying data in addition to the graph structure for its calculation of the difference between a true and a learnt causal graph. The distance is based on embedding intervention distributions over each pair of nodes as conditional mean embeddings into reproducing kernel Hilbert spaces and estimating their difference by the maximum (conditional) mean discrepancy. We show theoretical results which we validate with numerical experiments on synthetic data.
</details>
<details>
<summary>摘要</summary>
理解和准确评估真实和学习的 causal 图之间的差异是 causal 推断中的关键。作为结构 Hamming 距离和结构 intervención 距离的扩展，我们提出一种新的连续量化的度量，它考虑了在计算真实和学习 causal 图之间的差异时的数据的下面。这个距离基于对每对节点的 intervención 分布进行 Conditional Mean Embedding 的嵌入，并估计它们之间的差异为最大（conditional） Mean Discrepancy。我们提供了理论结果，并通过synthetic数据的数值实验 validate 这些结果。
</details></li>
</ul>
<hr>
<h2 id="Towards-Head-Computed-Tomography-Image-Reconstruction-Standardization-with-Deep-Learning-Assisted-Automatic-Detection"><a href="#Towards-Head-Computed-Tomography-Image-Reconstruction-Standardization-with-Deep-Learning-Assisted-Automatic-Detection" class="headerlink" title="Towards Head Computed Tomography Image Reconstruction Standardization with Deep Learning Assisted Automatic Detection"></a>Towards Head Computed Tomography Image Reconstruction Standardization with Deep Learning Assisted Automatic Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16440">http://arxiv.org/abs/2307.16440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Zheng, Chenxi Huang, Yuemei Luo</li>
<li>for: 提高头部计算机断层成像（CT）图像三维重建的精度和可重复性，并减少手动操作。</li>
<li>methods: 使用深度学习基于 объек检测算法，自动检测和评估颅骨线标志，以重新格式化图像前置 reconstruction。</li>
<li>results: 比较了十种对象检测算法的精度、效率和Robustness，选择了轻量级的 YOLOv8，其mAP为92.91%，并在标准化重建结果中表现出丰富的临床实用性和有效性。<details>
<summary>Abstract</summary>
Three-dimensional (3D) reconstruction of head Computed Tomography (CT) images elucidates the intricate spatial relationships of tissue structures, thereby assisting in accurate diagnosis. Nonetheless, securing an optimal head CT scan without deviation is challenging in clinical settings, owing to poor positioning by technicians, patient's physical constraints, or CT scanner tilt angle restrictions. Manual formatting and reconstruction not only introduce subjectivity but also strain time and labor resources. To address these issues, we propose an efficient automatic head CT images 3D reconstruction method, improving accuracy and repeatability, as well as diminishing manual intervention. Our approach employs a deep learning-based object detection algorithm, identifying and evaluating orbitomeatal line landmarks to automatically reformat the images prior to reconstruction. Given the dearth of existing evaluations of object detection algorithms in the context of head CT images, we compared ten methods from both theoretical and experimental perspectives. By exploring their precision, efficiency, and robustness, we singled out the lightweight YOLOv8 as the aptest algorithm for our task, with an mAP of 92.91% and impressive robustness against class imbalance. Our qualitative evaluation of standardized reconstruction results demonstrates the clinical practicability and validity of our method.
</details>
<details>
<summary>摘要</summary>
三维重建头部计算机断层成像（CT）图像可以帮助精确诊断，但在临床 Settings中获得优质头部CT扫描是具有挑战性的，这主要是由技术人员的位置不稳定、病人的身体限制或计算机扫描机的倾斜角度所致。手动格式化和重建不仅引入主观性，还占用了时间和劳动资源。为了解决这些问题，我们提出了一种高效的自动头部CT图像三维重建方法，提高了准确性和重复性，同时减少了手动干预。我们的方法利用深度学习基于 объек检测算法，通过识别和评估 orbitomeatal 线标记来自动重新格式化图像，以前置重建。由于现有的头部CT图像对象检测算法的评估罕见，我们从理论和实验两个角度对十种方法进行了比较。通过评估精度、效率和稳定性，我们选择了轻量级的 YOLOv8，其MAP值为92.91%，并在类偏置问题中表现出了扎实的Robustness。我们的质量评估标准化重建结果表明了我们的方法在临床实践中的可行性和有效性。
</details></li>
</ul>
<hr>
<h2 id="VITS2-Improving-Quality-and-Efficiency-of-Single-Stage-Text-to-Speech-with-Adversarial-Learning-and-Architecture-Design"><a href="#VITS2-Improving-Quality-and-Efficiency-of-Single-Stage-Text-to-Speech-with-Adversarial-Learning-and-Architecture-Design" class="headerlink" title="VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design"></a>VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16430">http://arxiv.org/abs/2307.16430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/daniilrobnikov/vits2">https://github.com/daniilrobnikov/vits2</a></li>
<li>paper_authors: Jungil Kong, Jihoon Park, Beomjeong Kim, Jeongmin Kim, Dohee Kong, Sangjin Kim</li>
<li>for: 提高单Stage Text-to-Speech模型的自然性、计算效率和多种语音特征的同步。</li>
<li>methods: 提出VITS2模型，通过改进结构和训练机制，提高单Stage Text-to-Speech模型的自然性、多种语音特征的同步和训练和推断的效率。</li>
<li>results: 实验结果表明，VITS2模型可以更好地提高单Stage Text-to-Speech模型的自然性、多种语音特征的同步和计算效率，同时可以减少先前的phoneme转换依赖，实现完全端到端单Stage Approach。<details>
<summary>Abstract</summary>
Single-stage text-to-speech models have been actively studied recently, and their results have outperformed two-stage pipeline systems. Although the previous single-stage model has made great progress, there is room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and present that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows a fully end-to-end single-stage approach.
</details>
<details>
<summary>摘要</summary>
单阶段文本至语音模型在最近几年中得到了广泛研究，其效果比两阶段管道系统更好。虽然之前的单阶段模型已经做出了很大的进步，但还有一些方面可以进一步改进，例如间歇性不自然、计算效率低下和phoneme转换的强依赖。在这项工作中，我们介绍VITS2单阶段文本至语音模型，该模型通过改进多个方面来生成更自然的语音。我们提出了改进的结构和训练机制，并证明了我们的方法能够提高自然性、语音特征相似性和训练和推理的效率。此外，我们还证明了前一代模型中phoneme转换的强依赖可以在我们的方法下降到可接受的水平，这使得完全的端到端单阶段approach成为可能。
</details></li>
</ul>
<hr>
<h2 id="Causal-Inference-for-Banking-Finance-and-Insurance-A-Survey"><a href="#Causal-Inference-for-Banking-Finance-and-Insurance-A-Survey" class="headerlink" title="Causal Inference for Banking Finance and Insurance A Survey"></a>Causal Inference for Banking Finance and Insurance A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16427">http://arxiv.org/abs/2307.16427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satyam Kumar, Yelleti Vivek, Vadlamani Ravi, Indranil Bose</li>
<li>for: 本研究旨在探讨 causal inference 在银行、金融和保险领域的应用，尤其是在这些领域中 causal inference 的应用状况。</li>
<li>methods: 本文通过对 37 篇1992-2023年发表的论文进行概括，探讨这些论文中使用的 statistical methods，包括 Bayesian Causal Network、Granger Causality 等。</li>
<li>results: 本文发现，银行和保险领域中的 causal inference 应用还处于初始阶段，因此有更多的研究空间可以开拓，以使其成为可靠的方法。<details>
<summary>Abstract</summary>
Causal Inference plays an significant role in explaining the decisions taken by statistical models and artificial intelligence models. Of late, this field started attracting the attention of researchers and practitioners alike. This paper presents a comprehensive survey of 37 papers published during 1992-2023 and concerning the application of causal inference to banking, finance, and insurance. The papers are categorized according to the following families of domains: (i) Banking, (ii) Finance and its subdomains such as corporate finance, governance finance including financial risk and financial policy, financial economics, and Behavioral finance, and (iii) Insurance. Further, the paper covers the primary ingredients of causal inference namely, statistical methods such as Bayesian Causal Network, Granger Causality and jargon used thereof such as counterfactuals. The review also recommends some important directions for future research. In conclusion, we observed that the application of causal inference in the banking and insurance sectors is still in its infancy, and thus more research is possible to turn it into a viable method.
</details>
<details>
<summary>摘要</summary>
causal inference 在解释统计模型和人工智能模型所作出的决策中扮演着重要的角色。近年来，这个领域吸引了研究者和实践者的关注。本文是一篇涵盖1992-2023年发表的37篇论文，探讨了在银行、金融和保险领域中应用 causal inference 的综述。这些论文被分为以下三个家庭领域：（i）银行，（ii）金融和其子领域，如企业财务、管理财务、金融风险和金融政策、金融经济和行为金融，以及（iii）保险。此外，文章还覆盖了 causal inference 的基本组成部分，包括统计方法如 bayesian causal network 和 Granger causality，以及其中使用的术语如 counterfactuals。文章还提出了未来研究的重要方向。结论是，在银行和保险领域中应用 causal inference 的应用还处于初生阶段，因此更多的研究可以使其成为可靠的方法。
</details></li>
</ul>
<hr>
<h2 id="MetaDiff-Meta-Learning-with-Conditional-Diffusion-for-Few-Shot-Learning"><a href="#MetaDiff-Meta-Learning-with-Conditional-Diffusion-for-Few-Shot-Learning" class="headerlink" title="MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning"></a>MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16424">http://arxiv.org/abs/2307.16424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baoquan Zhang, Demin Yu</li>
<li>for: 提高深度学习模型快速学习能力，即从少量示例学习出更好的表现。</li>
<li>methods: 基于梯度下降的meta学习方法，通过学习如何快速学习新任务。其关键思想是在bi-level优化manner中学习一个共享梯度下降算法（即其超参数），然后使用这个算法优化任务特定的模型，使用只有少量标注数据。</li>
<li>results: 与现有方法相比，我们的MetaDiff在少量学习任务中表现出色，并且不需要计算第二个DERIVATIVE，从而避免了内存压力和梯度消失问题。<details>
<summary>Abstract</summary>
Equipping a deep model the abaility of few-shot learning, i.e., learning quickly from only few examples, is a core challenge for artificial intelligence. Gradient-based meta-learning approaches effectively address the challenge by learning how to learn novel tasks. Its key idea is learning a deep model in a bi-level optimization manner, where the outer-loop process learns a shared gradient descent algorithm (i.e., its hyperparameters), while the inner-loop process leverage it to optimize a task-specific model by using only few labeled data. Although these existing methods have shown superior performance, the outer-loop process requires calculating second-order derivatives along the inner optimization path, which imposes considerable memory burdens and the risk of vanishing gradients. Drawing inspiration from recent progress of diffusion models, we find that the inner-loop gradient descent process can be actually viewed as a reverse process (i.e., denoising) of diffusion where the target of denoising is model weights but the origin data. Based on this fact, in this paper, we propose to model the gradient descent optimizer as a diffusion model and then present a novel task-conditional diffusion-based meta-learning, called MetaDiff, that effectively models the optimization process of model weights from Gaussion noises to target weights in a denoising manner. Thanks to the training efficiency of diffusion models, our MetaDiff do not need to differentiate through the inner-loop path such that the memory burdens and the risk of vanishing gradients can be effectvely alleviated. Experiment results show that our MetaDiff outperforms the state-of-the-art gradient-based meta-learning family in few-shot learning tasks.
</details>
<details>
<summary>摘要</summary>
使得深度模型具备几个例之学习能力，即快速从只有几个示例学习，是人工智能的核心挑战。基于梯度的meta学习方法有效地解决了这个挑战，其关键思想是通过在外层循环中学习一个共享梯度下降算法（即其超参数），而在内层循环中使用只有几个标注数据来优化任务特定模型。虽然现有的方法已经表现出色，但外层循环过程需要计算第二个Derivative along the inner optimization path，这会带来很大的内存压力和梯度消失风险。 drawing inspiration from recent progress of diffusion models， we find that the inner-loop gradient descent process can be viewed as a reverse process (i.e., denoising) of diffusion, where the target of denoising is model weights but the origin data。 Based on this fact， in this paper， we propose to model the gradient descent optimizer as a diffusion model and then present a novel task-conditional diffusion-based meta-learning, called MetaDiff, that effectively models the optimization process of model weights from Gaussian noise to target weights in a denoising manner。Thanks to the training efficiency of diffusion models, our MetaDiff does not need to differentiate through the inner-loop path, so the memory burdens and the risk of vanishing gradients can be effectively alleviated。 Experiment results show that our MetaDiff outperforms the state-of-the-art gradient-based meta-learning family in few-shot learning tasks。
</details></li>
</ul>
<hr>
<h2 id="Guaranteed-Optimal-Generative-Modeling-with-Maximum-Deviation-from-the-Empirical-Distribution"><a href="#Guaranteed-Optimal-Generative-Modeling-with-Maximum-Deviation-from-the-Empirical-Distribution" class="headerlink" title="Guaranteed Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution"></a>Guaranteed Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16422">http://arxiv.org/abs/2307.16422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elen Vardanyan, Arshak Minasyan, Sona Hunanyan, Tigran Galstyan, Arnak Dalalyan</li>
<li>for: 这个论文的目的是提供关于训练生成模型的理论吗？</li>
<li>methods: 这个论文使用了哪些方法？</li>
<li>results: 这个论文的结果是什么？Here are the answers in Simplified Chinese:</li>
<li>for: 这个论文的目的是提供关于训练生成模型的理论吗？	+ 这个论文的目的是为了提供一些关于训练生成模型的理论吗？</li>
<li>methods: 这个论文使用了哪些方法？	+ 这个论文使用了一些关于训练生成模型的方法，包括：		- 错误函数方法		- 聚合方法		- 权重函数方法</li>
<li>results: 这个论文的结果是什么？	+ 这个论文的结果是：		- 训练生成模型的误差函数应该逐渐接近零		- 训练生成模型的分布应该远离任何模型，可以在训练数据中找到的分布I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Generative modeling is a widely-used machine learning method with various applications in scientific and industrial fields. Its primary objective is to simulate new examples drawn from an unknown distribution given training data while ensuring diversity and avoiding replication of examples from the training data.   This paper presents theoretical insights into training a generative model with two properties: (i) the error of replacing the true data-generating distribution with the trained data-generating distribution should optimally converge to zero as the sample size approaches infinity, and (ii) the trained data-generating distribution should be far enough from any distribution replicating examples in the training data.   We provide non-asymptotic results in the form of finite sample risk bounds that quantify these properties and depend on relevant parameters such as sample size, the dimension of the ambient space, and the dimension of the latent space. Our results are applicable to general integral probability metrics used to quantify errors in probability distribution spaces, with the Wasserstein-$1$ distance being the central example. We also include numerical examples to illustrate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的机器学习方法之一是生成模型，它在科学和工业领域有广泛的应用。生成模型的 PRIMARY OBJECTIVE 是通过训练数据 simulate 新的例子，从未知分布中采样新的例子，同时保证新的例子具有多样性和不同于训练数据中的例子。这篇文章提供了生成模型训练的两个性质：（i）在训练数据中替换真实的数据生成分布时，训练后的数据生成分布的误差应该在样本数趋向于无穷大时Optimally Converge to Zero，（ii）训练后的数据生成分布应该与训练数据中的例子replicate的分布远离 enough。 我们提供了非假设统计结果，包括finite sample risk bounds，这些结果取决于样本大小、维度空间和秘密空间中的相关参数。我们的结果适用于普通的积分概率度量， Wasserstein-$1$ 距离是中心例子。我们还包括了数据示例来证明我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="DCTM-Dilated-Convolutional-Transformer-Model-for-Multimodal-Engagement-Estimation-in-Conversation"><a href="#DCTM-Dilated-Convolutional-Transformer-Model-for-Multimodal-Engagement-Estimation-in-Conversation" class="headerlink" title="DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation"></a>DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01966">http://arxiv.org/abs/2308.01966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vu Ngoc Tu, Van Thong Huynh, Hyung-Jeong Yang, M. Zaigham Zaheer, Shah Nawaz, Karthik Nandakumar, Soo-Hyung Kim</li>
<li>for: 这篇论文的目的是为了模型和估计人类对话中的参与度。</li>
<li>methods: 该论文使用了扩展 convolutional Transformer 来实现对话参与度的估计。</li>
<li>results: 该论文在 MULTIMEDIATE 2023 比赛中表现出优于基eline模型，在测试集上提高了 $7%$，在验证集上提高了 $4%$。另外，该论文还使用了不同的modalities fusión机制，并证明了在这种数据上，简单的 concatenation 方法加上自注意力融合可以获得最好的性能。<details>
<summary>Abstract</summary>
Conversational engagement estimation is posed as a regression problem, entailing the identification of the favorable attention and involvement of the participants in the conversation. This task arises as a crucial pursuit to gain insights into human's interaction dynamics and behavior patterns within a conversation. In this research, we introduce a dilated convolutional Transformer for modeling and estimating human engagement in the MULTIMEDIATE 2023 competition. Our proposed system surpasses the baseline models, exhibiting a noteworthy $7$\% improvement on test set and $4$\% on validation set. Moreover, we employ different modality fusion mechanism and show that for this type of data, a simple concatenated method with self-attention fusion gains the best performance.
</details>
<details>
<summary>摘要</summary>
通过对话参与度的 regression 问题来评估对话参与度，以获得对话中人类互动动态和行为模式的深入理解。在本研究中，我们提出了一种扩展 convolutional Transformer 来模型和估计对话参与度，并在 MULTIMEDIATE 2023 比赛中展示了我们的提案系统。我们的提案系统在测试集上显示了出色的 $7\%$ 提升，而在验证集上则是 $4\%$。此外，我们还使用了不同的 modalities 融合机制，并证明在这类数据上，简单的 concatenation 方法加上自注意力融合能够获得最好的性能。
</details></li>
</ul>
<hr>
<h2 id="Subspace-Distillation-for-Continual-Learning"><a href="#Subspace-Distillation-for-Continual-Learning" class="headerlink" title="Subspace Distillation for Continual Learning"></a>Subspace Distillation for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16419">http://arxiv.org/abs/2307.16419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csiro-robotics/sdcl">https://github.com/csiro-robotics/sdcl</a></li>
<li>paper_authors: Kaushik Roy, Christian Simon, Peyman Moghadam, Mehrtash Harandi</li>
<li>for: 本研究旨在 Mitigating Catastrophic Forgetting in continual learning, 以 preserve 前一任务学习的知识。</li>
<li>methods: 提议一种基于 manifold structure 的知识混合技术，使得 neural network 可以在新任务学习过程中保持先前任务的知识。</li>
<li>results: 实验表明，提议的方法可以在多个难度dataset上减轻忘却现象，并且可以与现有的学习方法混合使用，以提高其性能。<details>
<summary>Abstract</summary>
An ultimate objective in continual learning is to preserve knowledge learned in preceding tasks while learning new tasks. To mitigate forgetting prior knowledge, we propose a novel knowledge distillation technique that takes into the account the manifold structure of the latent/output space of a neural network in learning novel tasks. To achieve this, we propose to approximate the data manifold up-to its first order, hence benefiting from linear subspaces to model the structure and maintain the knowledge of a neural network while learning novel concepts. We demonstrate that the modeling with subspaces provides several intriguing properties, including robustness to noise and therefore effective for mitigating Catastrophic Forgetting in continual learning. We also discuss and show how our proposed method can be adopted to address both classification and segmentation problems. Empirically, we observe that our proposed method outperforms various continual learning methods on several challenging datasets including Pascal VOC, and Tiny-Imagenet. Furthermore, we show how the proposed method can be seamlessly combined with existing learning approaches to improve their performances. The codes of this article will be available at https://github.com/csiro-robotics/SDCL.
</details>
<details>
<summary>摘要</summary>
最终目标是在持续学习中保留先前任务中学习的知识，以避免知识卷积。我们提出了一种新的知识填充技术，利用神经网络的输出/潜在空间的拓扑结构来学习新任务。为此，我们提出将数据拓扑约化到第一阶段，从而利用直线子空间来模型结构，保持神经网络的知识 while learning novel concepts。我们证明了这种模型具有许多有趣的性质，包括鲁棒性于噪声和有效防止Catastrophic Forgetting。我们还讨论了如何采用我们的提议方法来解决分类和 segmentation 问题。实验证明，我们的提议方法在 Pascal VOC 和 Tiny-Imagenet 等复杂的数据集上表现出色，并且可以轻松地与现有的学习方法结合使用，以提高其表现。代码将在 <https://github.com/csiro-robotics/SDCL> 上提供。
</details></li>
</ul>
<hr>
<h2 id="Causal-learn-Causal-Discovery-in-Python"><a href="#Causal-learn-Causal-Discovery-in-Python" class="headerlink" title="Causal-learn: Causal Discovery in Python"></a>Causal-learn: Causal Discovery in Python</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16405">http://arxiv.org/abs/2307.16405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/py-why/causal-learn">https://github.com/py-why/causal-learn</a></li>
<li>paper_authors: Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming Gong, Ruichu Cai, Shohei Shimizu, Peter Spirtes, Kun Zhang</li>
<li>for: 本文旨在描述一个开源的Python库，用于 causal discovery，即从观察数据中揭示 causal 关系。</li>
<li>methods: 本库提供了一系列 causal discovery 方法，包括 non-parametric 方法和 parametric 方法，并且提供了 ease-to-use API，以便 для非专家用户。</li>
<li>results: 本库可以帮助用户快速和简单地进行 causal discovery，并且提供了详细的 documentation，以便学习和掌握。<details>
<summary>Abstract</summary>
Causal discovery aims at revealing causal relations from observational data, which is a fundamental task in science and engineering. We describe $\textit{causal-learn}$, an open-source Python library for causal discovery. This library focuses on bringing a comprehensive collection of causal discovery methods to both practitioners and researchers. It provides easy-to-use APIs for non-specialists, modular building blocks for developers, detailed documentation for learners, and comprehensive methods for all. Different from previous packages in R or Java, $\textit{causal-learn}$ is fully developed in Python, which could be more in tune with the recent preference shift in programming languages within related communities. The library is available at https://github.com/py-why/causal-learn.
</details>
<details>
<summary>摘要</summary>
causal discovery 旨在从观察数据中揭示 causal 关系，这是科学和工程中的基本任务。我们介绍了 $\textit{causal-learn}$，一个开源的 Python 库用于 causal discovery。这个库专注于为专家和研究人员提供一个全面的 causal discovery 方法收藏。它提供了易于使用的 API，用于非专家，模块化的构建块，详细的文档，以及全面的方法。与之前在 R 或 Java 中出现的包不同，$\textit{causal-learn}$ 是完全在 Python 中开发的，这与相关领域的编程语言偏好的变化相吻合。该库可以在 <https://github.com/py-why/causal-learn> 中获取。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Exploring-the-Capabilities-of-Bridge-Architectures-for-Complex-Visual-Reasoning-Tasks"><a href="#Bridging-the-Gap-Exploring-the-Capabilities-of-Bridge-Architectures-for-Complex-Visual-Reasoning-Tasks" class="headerlink" title="Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex Visual Reasoning Tasks"></a>Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex Visual Reasoning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16395">http://arxiv.org/abs/2307.16395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kousik Rajesh, Mrigank Raman, Mohammed Asad Karim, Pranit Chawla</li>
<li>for: This paper investigates the performance of multi-modal architectures based on Large Language Models (LLMs) on the NLVR2 dataset, specifically focusing on the effectiveness of adding object level features and pre-training on multi-modal data for complex visual reasoning tasks.</li>
<li>methods: The paper proposes extending traditional bridge architectures for the NLVR2 dataset by adding object level features, and also explores the use of a recently proposed bridge-architecture called LLaVA in the zero shot setting.</li>
<li>results: The paper shows that pre-training on multi-modal data is key for good performance on complex reasoning tasks such as NLVR2, and that adding object level features to bridge architectures does not help. The paper also demonstrates some initial results on LLaVA in the zero shot setting and analyzes its performance.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文研究了基于大语言模型（LLMs）的多Modal架构在NLVR2数据集上的表现，具体来说是对复杂视觉理解任务中的细化对象分析是否有助于提高表现。</li>
<li>methods: 论文提出了对NLVR2数据集的传统桥架架构进行对象层特征的扩展，以及使用最近提出的桥架架构LLAVA在零shot Setting中的初步研究。</li>
<li>results: 论文表明了预training多Modal数据是复杂视觉任务NLVR2表现好的关键因素，而对象层特征的添加不帮助提高表现。论文还初步展示了LLAVA在零shot Setting中的表现和分析。<details>
<summary>Abstract</summary>
In recent times there has been a surge of multi-modal architectures based on Large Language Models, which leverage the zero shot generation capabilities of LLMs and project image embeddings into the text space and then use the auto-regressive capacity to solve tasks such as VQA, captioning, and image retrieval. We name these architectures as "bridge-architectures" as they project from the image space to the text space. These models deviate from the traditional recipe of training transformer based multi-modal models, which involve using large-scale pre-training and complex multi-modal interactions through co or cross attention. However, the capabilities of bridge architectures have not been tested on complex visual reasoning tasks which require fine grained analysis about the image. In this project, we investigate the performance of these bridge-architectures on the NLVR2 dataset, and compare it to state-of-the-art transformer based architectures. We first extend the traditional bridge architectures for the NLVR2 dataset, by adding object level features to faciliate fine-grained object reasoning. Our analysis shows that adding object level features to bridge architectures does not help, and that pre-training on multi-modal data is key for good performance on complex reasoning tasks such as NLVR2. We also demonstrate some initial results on a recently bridge-architecture, LLaVA, in the zero shot setting and analyze its performance.
</details>
<details>
<summary>摘要</summary>
现在有一些基于大语言模型的多模态架构得到了广泛应用，这些架构利用大语言模型的零批生成能力，将图像嵌入在文本空间中，然后使用自动反相能力解决问题如VQA、描述和图像检索。我们称这些架构为“桥架架构”，因为它们从图像空间到文本空间的映射。这些模型与传统的多模态变换器模型不同，它们不需要大规模预训练和复杂的多模态交互，但是它们的能力尚未在复杂的视觉逻辑任务中被测试。在这个项目中，我们调查了bridge架构在NLVR2 dataset上的表现，并与状态对照的transformer基于模型进行比较。我们首先对NLVR2 dataset进行了传统的bridge架构的扩展，添加了 объек level 特征以便促进细化的物体逻辑分析。我们的分析表明，在bridge架构中添加对象级别特征并不有助于，预训练在多模态数据上是关键 для在复杂的逻辑任务上表现良好。我们还提供了一些初步的LLaVA bridgebasis在零批设置下的表现分析。
</details></li>
</ul>
<hr>
<h2 id="A-Pre-trained-Data-Deduplication-Model-based-on-Active-Learning"><a href="#A-Pre-trained-Data-Deduplication-Model-based-on-Active-Learning" class="headerlink" title="A Pre-trained Data Deduplication Model based on Active Learning"></a>A Pre-trained Data Deduplication Model based on Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00721">http://arxiv.org/abs/2308.00721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyao Liu, Shengdong Du, Fengmao Lv, Hongtao Xue, Jie Hu, Tianrui Li</li>
<li>For: 解决大数据时期的数据质量问题，特别是重复数据问题，以提高大数据的有效应用。* Methods: 基于活动学习的预训练去重模型，首次将活动学习与转换器结合在一起，以选择最有价值的数据进行模型训练，并首次应用R-Drop方法进行数据扩展。* Results: 对于去重后的数据标识，提出了28%的准确率提升，比前一个状态的艺术品（SOTA）更高。<details>
<summary>Abstract</summary>
In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
在大数据时代，数据质量问题变得越来越突出。一个主要挑战是重复的数据问题，这可能来自于重复输入或多个数据源的合并。这些“废弃数据”问题可能会很大程度限制大数据的有效应用。为解决数据筛选问题，我们提议一个基于活动学习的预训练deduplication模型，这是第一个在semantic水平上使用活动学习解决数据筛选问题的研究。该模型基于预训练的Transformer结构，并在解决数据筛选问题上进行了序列分类任务的精度训练，这也是第一次将Transformer结构与活动学习集成到末端架构中，以选择数据筛选模型训练的最有价值数据，并且是第一次在每个Label数据上使用R-Drop方法进行数据增强，可以降低人工标注成本并提高模型的性能。实验结果表明，我们提议的模型在比较数据标识 task 中超过前一个状态的较好表现，达到28%的Recall分数提升在 benchmark 数据集上。
</details></li>
</ul>
<hr>
<h2 id="STL-A-Signed-and-Truncated-Logarithm-Activation-Function-for-Neural-Networks"><a href="#STL-A-Signed-and-Truncated-Logarithm-Activation-Function-for-Neural-Networks" class="headerlink" title="STL: A Signed and Truncated Logarithm Activation Function for Neural Networks"></a>STL: A Signed and Truncated Logarithm Activation Function for Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16389">http://arxiv.org/abs/2307.16389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Gong</li>
<li>for: 本文提出了一种新的签名和 truncated logarithm 函数作为活动函数，以提高神经网络的精度和运行性能。</li>
<li>methods: 本文使用了多种已知的活动函数进行比较，并证明了新提出的activation函数具有更好的数学性质，如odd函数、 monotone函数、 differentiable函数、无 bound 值范围和连续非零导数。</li>
<li>results: 对多种已知神经网络进行比较，结果表明新提出的activation函数在精度和运行性能方面具有 estado-of-the-art 的表现。这种活动函数可以应用于大多数神经网络中， где活动函数是必要的。<details>
<summary>Abstract</summary>
Activation functions play an essential role in neural networks. They provide the non-linearity for the networks. Therefore, their properties are important for neural networks' accuracy and running performance. In this paper, we present a novel signed and truncated logarithm function as activation function. The proposed activation function has significantly better mathematical properties, such as being odd function, monotone, differentiable, having unbounded value range, and a continuous nonzero gradient. These properties make it an excellent choice as an activation function. We compare it with other well-known activation functions in several well-known neural networks. The results confirm that it is the state-of-the-art. The suggested activation function can be applied in a large range of neural networks where activation functions are necessary.
</details>
<details>
<summary>摘要</summary>
Activation functions play an essential role in neural networks. They provide the non-linearity for the networks. Therefore, their properties are important for neural networks' accuracy and running performance. In this paper, we present a novel signed and truncated logarithm function as activation function. The proposed activation function has significantly better mathematical properties, such as being odd function, monotone, differentiable, having unbounded value range, and a continuous nonzero gradient. These properties make it an excellent choice as an activation function. We compare it with other well-known activation functions in several well-known neural networks. The results confirm that it is the state-of-the-art. The suggested activation function can be applied in a large range of neural networks where activation functions are necessary.</SYSCODE>这文章提出了一个新的签名和截断对数函数作为神经网络中的启动函数。这个提案的启动函数具有更好的数学性能，如是odd函数、单调、可微、无上限值范围和连续非零导数。这些特性使其成为一个非常出色的启动函数选择。我们与其他已知的启动函数进行比较，发现其在训练神经网络方面的表现优于其他所有启动函数。这个提案的启动函数可以应用于广泛的神经网络中，其中需要启动函数的情况下。
</details></li>
</ul>
<hr>
<h2 id="Does-fine-tuning-GPT-3-with-the-OpenAI-API-leak-personally-identifiable-information"><a href="#Does-fine-tuning-GPT-3-with-the-OpenAI-API-leak-personally-identifiable-information" class="headerlink" title="Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?"></a>Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16382">http://arxiv.org/abs/2307.16382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/albertsun1/gpt3-pii-attacks">https://github.com/albertsun1/gpt3-pii-attacks</a></li>
<li>paper_authors: Albert Yu Sun, Eliott Zemour, Arushi Saxena, Udith Vaidyanathan, Eric Lin, Christian Lau, Vaikkunth Mugunthan</li>
<li>for: 这个研究的目的是确定是否可以从OpenAI的GPT-3模型中提取个人可Identifiable信息 (PII)。</li>
<li>methods: 这个研究使用了naive prompting方法和Autocomplete任务来调查GPT-3模型是否会记忆和泄露敏感信息。</li>
<li>results: 研究发现，对GPT-3模型进行了两个任务的 fine-tuning，导致模型记忆并泄露了基本数据集中的敏感信息 (PII)。Here’s the full text in Simplified Chinese:</li>
<li>for: 这个研究的目的是确定是否可以从OpenAI的GPT-3模型中提取个人可Identifiable信息 (PII)。</li>
<li>methods: 这个研究使用了naive prompting方法和Autocomplete任务来调查GPT-3模型是否会记忆和泄露敏感信息。</li>
<li>results: 研究发现，对GPT-3模型进行了两个任务的 fine-tuning，导致模型记忆并泄露了基本数据集中的敏感信息 (PII)。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Machine learning practitioners often fine-tune generative pre-trained models like GPT-3 to improve model performance at specific tasks. Previous works, however, suggest that fine-tuned machine learning models memorize and emit sensitive information from the original fine-tuning dataset. Companies such as OpenAI offer fine-tuning services for their models, but no prior work has conducted a memorization attack on any closed-source models. In this work, we simulate a privacy attack on GPT-3 using OpenAI's fine-tuning API. Our objective is to determine if personally identifiable information (PII) can be extracted from this model. We (1) explore the use of naive prompting methods on a GPT-3 fine-tuned classification model, and (2) we design a practical word generation task called Autocomplete to investigate the extent of PII memorization in fine-tuned GPT-3 within a real-world context. Our findings reveal that fine-tuning GPT3 for both tasks led to the model memorizing and disclosing critical personally identifiable information (PII) obtained from the underlying fine-tuning dataset. To encourage further research, we have made our codes and datasets publicly available on GitHub at: https://github.com/albertsun1/gpt3-pii-attacks
</details>
<details>
<summary>摘要</summary>
机器学习实践者们常常微调生成批处理模型如GPT-3以提高模型在特定任务上的性能。然而，先前的研究表明，微调机器学习模型会记忆和发送敏感信息从原始微调数据集。如OpenAI提供的微调服务，但没有任何之前的工作对任何关闭源模型进行了记忆攻击。在这种工作中，我们模拟了对GPT-3的隐私攻击，以确定是否可以从这个模型中提取个人 Identifiable Information (PII)。我们（1）探索使用简单的提示方法在GPT-3微调的分类模型上，并（2）我们设计了一个实用的单词生成任务called Autocomplete，以调查微调GPT-3中PII的储存程度。我们的发现表明，对GPT-3进行微调两个任务都导致模型记忆和披露critical的个人Identifiable Information (PII)从原始微调数据集中获得。为促进更多的研究，我们在GitHub上公开了我们的代码和数据集：https://github.com/albertsun1/gpt3-pii-attacks。
</details></li>
</ul>
<hr>
<h2 id="UniAP-Unifying-Inter-and-Intra-Layer-Automatic-Parallelism-by-Mixed-Integer-Quadratic-Programming"><a href="#UniAP-Unifying-Inter-and-Intra-Layer-Automatic-Parallelism-by-Mixed-Integer-Quadratic-Programming" class="headerlink" title="UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming"></a>UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16375">http://arxiv.org/abs/2307.16375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Lin, Ke Wu, Jun Li, Wu-Jun Li</li>
<li>for: 提高深度学习模型的训练效率</li>
<li>methods: 使用混合整数二次函数Programming进行自动并行化</li>
<li>results: 比前方法提高1.70倍的 durchput，并且减少搜索策略时间16倍。<details>
<summary>Abstract</summary>
Deep learning models have demonstrated impressive performance in various domains. However, the prolonged training time of these models remains a critical problem. Manually designed parallel training strategies could enhance efficiency but require considerable time and deliver little flexibility. Hence, automatic parallelism is proposed to automate the parallel strategy searching process. Even so, existing approaches suffer from sub-optimal strategy space because they treat automatic parallelism as two independent stages, namely inter- and intra-layer parallelism. To address this issue, we propose UniAP, which utilizes mixed integer quadratic programming to unify inter- and intra-layer automatic parallelism. To the best of our knowledge, UniAP is the first work to unify these two categories to search for a globally optimal strategy. The experimental results show that UniAP outperforms state-of-the-art methods by up to 1.70$\times$ in throughput and reduces strategy searching time by up to 16$\times$ across four Transformer-like models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BearingPGA-Net-A-Lightweight-and-Deployable-Bearing-Fault-Diagnosis-Network-via-Decoupled-Knowledge-Distillation-and-FPGA-Acceleration"><a href="#BearingPGA-Net-A-Lightweight-and-Deployable-Bearing-Fault-Diagnosis-Network-via-Decoupled-Knowledge-Distillation-and-FPGA-Acceleration" class="headerlink" title="BearingPGA-Net: A Lightweight and Deployable Bearing Fault Diagnosis Network via Decoupled Knowledge Distillation and FPGA Acceleration"></a>BearingPGA-Net: A Lightweight and Deployable Bearing Fault Diagnosis Network via Decoupled Knowledge Distillation and FPGA Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16363">http://arxiv.org/abs/2307.16363</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asdvfghg/bearingpga-net">https://github.com/asdvfghg/bearingpga-net</a></li>
<li>paper_authors: Jing-Xiao Liao, Sheng-Lai Wei, Chen-Long Xie, Tieyong Zeng, Jinwei Sun, Shiping Zhang, Xiaoge Zhang, Feng-Lei Fan</li>
<li>for: 这个研究旨在提出一个轻量级且可部署的滚珠缺陷诊断模型，以应对现有的深度学习模型对行业领域的不适用性。</li>
<li>methods: 我们使用了一个已经受训的大型模型来训练 BearingPGA-Net，并运用了分离知识传播来将知识传播到小型模型中。这使得我们的模型具有了优秀的缺陷诊断性能，而且模型的大小仅仅是其他轻量级方法的一半。</li>
<li>results: 我们设计了一个使用 FPGA 的加速方案，将 BearingPGA-Net 的每个层都用特定的量化和定制的逻辑门体设计成 FPGA 上，并强调了平行计算和模组重复以提高计算速度。根据我们的实验结果，我们的部署方案可以实现 CPU 上的200倍以上的诊断速度，而且与 CPU 上的 F1、Recall 和 Precision 分数相比，其表现下降不到0.4%。<details>
<summary>Abstract</summary>
Deep learning has achieved remarkable success in the field of bearing fault diagnosis. However, this success comes with larger models and more complex computations, which cannot be transferred into industrial fields requiring models to be of high speed, strong portability, and low power consumption. In this paper, we propose a lightweight and deployable model for bearing fault diagnosis, referred to as BearingPGA-Net, to address these challenges. Firstly, aided by a well-trained large model, we train BearingPGA-Net via decoupled knowledge distillation. Despite its small size, our model demonstrates excellent fault diagnosis performance compared to other lightweight state-of-the-art methods. Secondly, we design an FPGA acceleration scheme for BearingPGA-Net using Verilog. This scheme involves the customized quantization and designing programmable logic gates for each layer of BearingPGA-Net on the FPGA, with an emphasis on parallel computing and module reuse to enhance the computational speed. To the best of our knowledge, this is the first instance of deploying a CNN-based bearing fault diagnosis model on an FPGA. Experimental results reveal that our deployment scheme achieves over 200 times faster diagnosis speed compared to CPU, while achieving a lower-than-0.4\% performance drop in terms of F1, Recall, and Precision score on our independently-collected bearing dataset. Our code is available at \url{https://github.com/asdvfghg/BearingPGA-Net}.
</details>
<details>
<summary>摘要</summary>
深度学习在滚珠疲劳诊断领域取得了很大的成功，但这些成功来自于更大的模型和更复杂的计算，这些模型不能在需要高速、强可移植和低功耗的工业场景中使用。在这篇论文中，我们提出了一种轻量级可部署的滚珠疲劳诊断模型，称为滚珠疲劳诊断网络（BearingPGA-Net），以解决这些挑战。首先，我们通过一个受训的大型模型的帮助，对BearingPGA-Net进行分离知识填充。尽管它具有小型，但我们的模型在其他轻量级当前的方法中表现出色，并且达到了优于0.4%的F1、回归和准确率分数。其次，我们设计了一种FPGA加速方案，使用Verilog语言来自定义量化和设计可编程逻辑门阵列 для每层BearingPGA-Net在FPGA上，强调并行计算和模块重用以提高计算速度。到目前为止，这是首次将CNN基于滚珠疲劳诊断模型部署到FPGA上。实验结果表明，我们的部署方案可以在CPU上进行200倍以上的诊断速度提升，而且与F1、回归和准确率分数在我们独立收集的滚珠 dataset上保持低于0.4%的表现。我们的代码可以在以下链接中找到：https://github.com/asdvfghg/BearingPGA-Net。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-and-Analyzing-Robust-Point-Cloud-Recognition-Bag-of-Tricks-for-Defending-Adversarial-Examples"><a href="#Benchmarking-and-Analyzing-Robust-Point-Cloud-Recognition-Bag-of-Tricks-for-Defending-Adversarial-Examples" class="headerlink" title="Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples"></a>Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16361">http://arxiv.org/abs/2307.16361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiufan319/benchmark_pc_attack">https://github.com/qiufan319/benchmark_pc_attack</a></li>
<li>paper_authors: Qiufan Ji, Lin Wang, Cong Shi, Shengshan Hu, Yingying Chen, Lichao Sun</li>
<li>for: The paper is written for defending deep neural networks (DNNs) against adversarial examples in 3D point cloud recognition.</li>
<li>methods: The paper uses a comprehensive and rigorous benchmark to evaluate adversarial robustness, collects existing defense tricks, and proposes a hybrid training augmentation method that considers various types of point cloud adversarial examples.</li>
<li>results: The paper achieves an average accuracy of 83.45% against various attacks, demonstrating the capability of the proposed defense framework to enable robust learners.Here is the information in Simplified Chinese text:</li>
<li>for: 本研究是为了防御深度神经网络（DNNs）在3D点云识别中受到攻击的。</li>
<li>methods: 本文使用了一个完整的和严格的benchmark来评估攻击性 robustness，收集了现有的防御技巧，并提出了一种 combining多种点云攻击的训练增强方法。</li>
<li>results: 本文通过多种攻击测试得到了83.45%的平均准确率，demonstrating the capabilities of the proposed defense framework to enable robust learners。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) for 3D point cloud recognition are vulnerable to adversarial examples, threatening their practical deployment. Despite the many research endeavors have been made to tackle this issue in recent years, the diversity of adversarial examples on 3D point clouds makes them more challenging to defend against than those on 2D images. For examples, attackers can generate adversarial examples by adding, shifting, or removing points. Consequently, existing defense strategies are hard to counter unseen point cloud adversarial examples. In this paper, we first establish a comprehensive, and rigorous point cloud adversarial robustness benchmark to evaluate adversarial robustness, which can provide a detailed understanding of the effects of the defense and attack methods. We then collect existing defense tricks in point cloud adversarial defenses and then perform extensive and systematic experiments to identify an effective combination of these tricks. Furthermore, we propose a hybrid training augmentation methods that consider various types of point cloud adversarial examples to adversarial training, significantly improving the adversarial robustness. By combining these tricks, we construct a more robust defense framework achieving an average accuracy of 83.45\% against various attacks, demonstrating its capability to enabling robust learners. Our codebase are open-sourced on: \url{https://github.com/qiufan319/benchmark_pc_attack.git}.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs） для三维点云识别是易受到攻击的，这 threatening its practical deployment.  despite many research efforts have been made to address this issue in recent years, the diversity of adversarial examples on 3D point clouds makes them more challenging to defend against than those on 2D images. For example, attackers can generate adversarial examples by adding, shifting, or removing points. As a result, existing defense strategies are difficult to counter unseen point cloud adversarial examples.In this paper, we first establish a comprehensive and rigorous point cloud adversarial robustness benchmark to evaluate adversarial robustness, which can provide a detailed understanding of the effects of the defense and attack methods. We then collect existing defense tricks in point cloud adversarial defenses and perform extensive and systematic experiments to identify an effective combination of these tricks. Furthermore, we propose a hybrid training augmentation method that considers various types of point cloud adversarial examples to adversarial training, significantly improving the adversarial robustness. By combining these tricks, we construct a more robust defense framework achieving an average accuracy of 83.45% against various attacks, demonstrating its capability to enabling robust learners. Our codebase is open-sourced on: <https://github.com/qiufan319/benchmark_pc_attack.git>.
</details></li>
</ul>
<hr>
<h2 id="Probabilistically-robust-conformal-prediction"><a href="#Probabilistically-robust-conformal-prediction" class="headerlink" title="Probabilistically robust conformal prediction"></a>Probabilistically robust conformal prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16360">http://arxiv.org/abs/2307.16360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/1995subhankar1995/PRCP">https://github.com/1995subhankar1995/PRCP</a></li>
<li>paper_authors: Subhankar Ghosh, Yuanjie Shi, Taha Belkhouja, Yan Yan, Jana Doppa, Brian Jones</li>
<li>for: This paper focuses on developing a probabilistically robust conformal prediction (PRCP) algorithm to ensure robustness to natural&#x2F;adversarial perturbations in testing examples.</li>
<li>methods: The proposed PRCP algorithm uses a novel adaptive approach called “quantile-of-quantile” design to determine two parallel thresholds for data samples and perturbations, achieving better trade-offs between nominal performance and robustness.</li>
<li>results: The proposed aPRCP algorithm is experimentally demonstrated to achieve better trade-offs than state-of-the-art conformal prediction (CP) and adversarially robust CP algorithms on CIFAR-10, CIFAR-100, and ImageNet datasets using deep neural networks.<details>
<summary>Abstract</summary>
Conformal prediction (CP) is a framework to quantify uncertainty of machine learning classifiers including deep neural networks. Given a testing example and a trained classifier, CP produces a prediction set of candidate labels with a user-specified coverage (i.e., true class label is contained with high probability). Almost all the existing work on CP assumes clean testing data and there is not much known about the robustness of CP algorithms w.r.t natural/adversarial perturbations to testing examples. This paper studies the problem of probabilistically robust conformal prediction (PRCP) which ensures robustness to most perturbations around clean input examples. PRCP generalizes the standard CP (cannot handle perturbations) and adversarially robust CP (ensures robustness w.r.t worst-case perturbations) to achieve better trade-offs between nominal performance and robustness. We propose a novel adaptive PRCP (aPRCP) algorithm to achieve probabilistically robust coverage. The key idea behind aPRCP is to determine two parallel thresholds, one for data samples and another one for the perturbations on data (aka "quantile-of-quantile" design). We provide theoretical analysis to show that aPRCP algorithm achieves robust coverage. Our experiments on CIFAR-10, CIFAR-100, and ImageNet datasets using deep neural networks demonstrate that aPRCP achieves better trade-offs than state-of-the-art CP and adversarially robust CP algorithms.
</details>
<details>
<summary>摘要</summary>
“对于机器学习分类器，具有不同程度的不确定性是一个重要的考虑因素。这篇论文探讨了一个名为“可靠性推断（Conformal Prediction，CP）”的框架，可以为机器学习分类器量化不确定性。给定一个测试例子和一个已经训练好的分类器，CP 可以生成一个包含真实类别的可能性的预测集。然而，大多数现有的 CP 研究假设测试数据是清洁的，而且对于自然或攻击性的推偏而言， CP 的可靠性不充分了解。这篇论文提出了一个名为“可靠性推断（PRCP）”的问题，它可以确保分类器对于大多数推偏而言是可靠的。我们提出了一个名为“可靠性推断（aPRCP）”的新算法，它可以实现可靠性推断。aPRCP 的关键思想是在测试数据和推偏之间设置两个平行的阈值（也称为“量ile-of-quantile”设计）。我们提供了理论分析，证明 aPRCP 算法可以实现可靠性推断。我们在 CIFAR-10、CIFAR-100 和 ImageNet  dataset 上进行了深度神经网络的实验，结果显示 aPRCP 算法可以更好地平衡 Nominal 性和可靠性。”
</details></li>
</ul>
<hr>
<h2 id="Moreau-Yoshida-Variational-Transport-A-General-Framework-For-Solving-Regularized-Distributional-Optimization-Problems"><a href="#Moreau-Yoshida-Variational-Transport-A-General-Framework-For-Solving-Regularized-Distributional-Optimization-Problems" class="headerlink" title="Moreau-Yoshida Variational Transport: A General Framework For Solving Regularized Distributional Optimization Problems"></a>Moreau-Yoshida Variational Transport: A General Framework For Solving Regularized Distributional Optimization Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16358">http://arxiv.org/abs/2307.16358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dai Hai Nguyen, Tetsuya Sakurai<br>for:  solves a regularized distributional optimization problem widely appeared in machine learning and statistics, such as proximal Monte-Carlo sampling, Bayesian inference and generative modeling.methods: employs the Moreau-Yoshida envelope for a smooth approximation of the nonsmooth function in the objective, and leverages the variational representation to reformulate the approximate problem as a concave-convex saddle point problem.results: provides theoretical analyses and reports experimental results to demonstrate the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
We consider a general optimization problem of minimizing a composite objective functional defined over a class of probability distributions. The objective is composed of two functionals: one is assumed to possess the variational representation and the other is expressed in terms of the expectation operator of a possibly nonsmooth convex regularizer function. Such a regularized distributional optimization problem widely appears in machine learning and statistics, such as proximal Monte-Carlo sampling, Bayesian inference and generative modeling, for regularized estimation and generation.   We propose a novel method, dubbed as Moreau-Yoshida Variational Transport (MYVT), for solving the regularized distributional optimization problem. First, as the name suggests, our method employs the Moreau-Yoshida envelope for a smooth approximation of the nonsmooth function in the objective. Second, we reformulate the approximate problem as a concave-convex saddle point problem by leveraging the variational representation, and then develope an efficient primal-dual algorithm to approximate the saddle point. Furthermore, we provide theoretical analyses and report experimental results to demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
我们考虑一个总体优化问题，即将一类概率分布中的一个复合目标函数进行最小化。这个目标函数由两个函数组成：一个假设具有变量表示，另一个是一个可能非均衡的凸函数。这种凸函数regularizer在机器学习和统计中广泛应用，例如距离 Monte Carlo 抽样、 bayesian 推断和生成模型。我们提出一种新的方法，称为 Moreau-Yoshida 变量运输（MYVT），以解决这种凸函数regularized 分布优化问题。首先，我们的方法使用Moreau-Yoshida 覆盖函数来将非均衡函数在目标函数中进行简化。其次，我们将 reformulate  approximate 问题为一个凹凸两点问题，并利用变量表示来解决。然后，我们开发了一种高效的主动-副本算法来approximate 两点。 finally，我们提供了理论分析和实验结果，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Hypertension-Detection-From-High-Dimensional-Representation-of-Photoplethysmogram-Signals"><a href="#Hypertension-Detection-From-High-Dimensional-Representation-of-Photoplethysmogram-Signals" class="headerlink" title="Hypertension Detection From High-Dimensional Representation of Photoplethysmogram Signals"></a>Hypertension Detection From High-Dimensional Representation of Photoplethysmogram Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02425">http://arxiv.org/abs/2308.02425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/navidhasanzadeh/hypertension_ppg">https://github.com/navidhasanzadeh/hypertension_ppg</a></li>
<li>paper_authors: Navid Hasanzadeh, Shahrokh Valaee, Hojjat Salehinejad</li>
<li>for: 旨在检测高血压，使用光敏plethysmogram（PPG）信号。</li>
<li>methods: 提出了一种基于随机核函数的高维表示技术，用于检测高血压。</li>
<li>results: 实验结果表明，该关系不仅限于心率和血压，而且可以扩展到更多的特征。此外，使用核函数变换为终端时间序列特征提取器，超过了前一些研究和现代深度学习模型的性能。<details>
<summary>Abstract</summary>
Hypertension is commonly referred to as the "silent killer", since it can lead to severe health complications without any visible symptoms. Early detection of hypertension is crucial in preventing significant health issues. Although some studies suggest a relationship between blood pressure and certain vital signals, such as Photoplethysmogram (PPG), reliable generalization of the proposed blood pressure estimation methods is not yet guaranteed. This lack of certainty has resulted in some studies doubting the existence of such relationships, or considering them weak and limited to heart rate and blood pressure. In this paper, a high-dimensional representation technique based on random convolution kernels is proposed for hypertension detection using PPG signals. The results show that this relationship extends beyond heart rate and blood pressure, demonstrating the feasibility of hypertension detection with generalization. Additionally, the utilized transform using convolution kernels, as an end-to-end time-series feature extractor, outperforms the methods proposed in the previous studies and state-of-the-art deep learning models.
</details>
<details>
<summary>摘要</summary>
高血压通常被称为"无论的杀手"，因为它可能会导致严重的健康问题无需任何可见的symptoms。早期检测高血压是预防重要的健康问题的锁定要素。虽然一些研究表明血压和某些生命体征之间存在关系，但可靠地总结这些提议的血压估算方法并不 yet guaranteed。这种不确定性导致了一些研究质疑这些关系的存在，或者认为这些关系是弱小limited to heart rate and blood pressure。在这篇论文中，一种基于随机核函数的高维表示技术被提出用于使用PPG信号检测高血压。结果表明这种关系超出了heart rate和血压，证明了检测高血压的可能性。此外，使用核函数来实现终端时间序列特征提取，比前一些研究和现代深度学习模型都高效。
</details></li>
</ul>
<hr>
<h2 id="Rating-based-Reinforcement-Learning"><a href="#Rating-based-Reinforcement-Learning" class="headerlink" title="Rating-based Reinforcement Learning"></a>Rating-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16348">http://arxiv.org/abs/2307.16348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Devin White, Mingkang Wu, Ellen Novoseller, Vernon Lawhern, Nick Waytowich, Yongcan Cao</li>
<li>for: 本研究开发了一种新的评价基于学习方法，利用人类评价来获取人类指导。与现有的喜好度基于和排名基于学习惯例不同，本研究基于人类评价个别路径而不需要相对比较sample pair的喜好度。</li>
<li>methods: 本研究使用了一个新的预测模型来预测人类评价，以及一种多组别损失函数。</li>
<li>results: 经过多个实验研究，包括基于实验实际评价和synthetic评价，本研究获得了该方法的有效性和优势。<details>
<summary>Abstract</summary>
This paper develops a novel rating-based reinforcement learning approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach.
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种新的评分基于束缚学习方法，利用人类评分来获取人类指导。与现有的偏好基于样本对比和排名基于样本对比不同，提出的评分基于束缚学习方法是基于人类评估个体轨迹而不需要对样本对比进行相对比较。该方法建立在新的人类评分预测模型和多类损失函数之上。我们通过对 sintetic评分和真实人类评分进行多个实验研究来评估新的评分基于束缚学习方法的有效性和优势。
</details></li>
</ul>
<hr>
<h2 id="Proof-of-Federated-Learning-Subchain-Free-Partner-Selection-Subchain-Based-on-Federated-Learning"><a href="#Proof-of-Federated-Learning-Subchain-Free-Partner-Selection-Subchain-Based-on-Federated-Learning" class="headerlink" title="Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning"></a>Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16342">http://arxiv.org/abs/2307.16342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boyang Li, Bingyu Shen, Qing Lu, Taeho Jung, Yiyu Shi</li>
<li>for: 本研究旨在提出一种新的证明方式，以填补当前Proof-of-Deep-Learning(PoDL)承诺的缺陷。</li>
<li>methods: 本研究使用了聚合学习模型训练任务作为填补Hashing的可用功能。</li>
<li>results: 在 simulations 中，我们发现在受限的订单池大小下， miner  WITH 高Shapley Value (SV) 会获得更好的机会被选择。在实验中，Proof-of-Federated-Learning-Subchain (PoFLSC) 证明支持了子链管理员在受限订单池大小下建立和维护竞争性子链。<details>
<summary>Abstract</summary>
The continuous thriving of the Blockchain society motivates research in novel designs of schemes supporting cryptocurrencies. Previously multiple Proof-of-Deep-Learning(PoDL) consensuses have been proposed to replace hashing with useful work such as deep learning model training tasks. The energy will be more efficiently used while maintaining the ledger. However deep learning models are problem-specific and can be extremely complex. Current PoDL consensuses still require much work to realize in the real world. In this paper, we proposed a novel consensus named Proof-of-Federated-Learning-Subchain(PoFLSC) to fill the gap. We applied a subchain to record the training, challenging, and auditing activities and emphasized the importance of valuable datasets in partner selection. We simulated 20 miners in the subchain to demonstrate the effectiveness of PoFLSC. When we reduce the pool size concerning the reservation priority order, the drop rate difference in the performance in different scenarios further exhibits that the miner with a higher Shapley Value (SV) will gain a better opportunity to be selected when the size of the subchain pool is limited. In the conducted experiments, the PoFLSC consensus supported the subchain manager to be aware of reservation priority and the core partition of contributors to establish and maintain a competitive subchain.
</details>
<details>
<summary>摘要</summary>
continous thriving of the Blockchain society motivates research in novel designs of schemes supporting cryptocurrencies. Previously multiple Proof-of-Deep-Learning(PoDL) consensuses have been proposed to replace hashing with useful work such as deep learning model training tasks. The energy will be more efficiently used while maintaining the ledger. However deep learning models are problem-specific and can be extremely complex. Current PoDL consensuses still require much work to realize in the real world. In this paper, we proposed a novel consensus named Proof-of-Federated-Learning-Subchain(PoFLSC) to fill the gap. We applied a subchain to record the training, challenging, and auditing activities and emphasized the importance of valuable datasets in partner selection. We simulated 20 miners in the subchain to demonstrate the effectiveness of PoFLSC. When we reduce the pool size concerning the reservation priority order, the drop rate difference in the performance in different scenarios further exhibits that the miner with a higher Shapley Value (SV) will gain a better opportunity to be selected when the size of the subchain pool is limited. In the conducted experiments, the PoFLSC consensus supported the subchain manager to be aware of reservation priority and the core partition of contributors to establish and maintain a competitive subchain.Here's the translation in Traditional Chinese:continuous thriving of the Blockchain society motivates research in novel designs of schemes supporting cryptocurrencies. Previously multiple Proof-of-Deep-Learning(PoDL) consensuses have been proposed to replace hashing with useful work such as deep learning model training tasks. The energy will be more efficiently used while maintaining the ledger. However deep learning models are problem-specific and can be extremely complex. Current PoDL consensuses still require much work to realize in the real world. In this paper, we proposed a novel consensus named Proof-of-Federated-Learning-Subchain(PoFLSC) to fill the gap. We applied a subchain to record the training, challenging, and auditing activities and emphasized the importance of valuable datasets in partner selection. We simulated 20 miners in the subchain to demonstrate the effectiveness of PoFLSC. When we reduce the pool size concerning the reservation priority order, the drop rate difference in the performance in different scenarios further exhibits that the miner with a higher Shapley Value (SV) will gain a better opportunity to be selected when the size of the subchain pool is limited. In the conducted experiments, the PoFLSC consensus supported the subchain manager to be aware of reservation priority and the core partition of contributors to establish and maintain a competitive subchain.
</details></li>
</ul>
<hr>
<h2 id="Theoretically-Principled-Trade-off-for-Stateful-Defenses-against-Query-Based-Black-Box-Attacks"><a href="#Theoretically-Principled-Trade-off-for-Stateful-Defenses-against-Query-Based-Black-Box-Attacks" class="headerlink" title="Theoretically Principled Trade-off for Stateful Defenses against Query-Based Black-Box Attacks"></a>Theoretically Principled Trade-off for Stateful Defenses against Query-Based Black-Box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16331">http://arxiv.org/abs/2307.16331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashish Hooda, Neal Mangaokar, Ryan Feng, Kassem Fawaz, Somesh Jha, Atul Prakash</li>
<li>for: 这个论文旨在探讨stateful defense的攻击检测与假阳性率之间的贸易OFF，并提供一种理论上的界限。</li>
<li>methods: 论文使用一种通用的特征提取器类型和相似度阈值来优化攻击检测和假阳性率的贸易OFF。</li>
<li>results: 论文通过理论分析和实验评估表明，stateful defense的攻击检测和假阳性率之间存在一定的贸易OFF，并且可以通过不同的特征提取器和相似度阈值来优化这个贸易OFF。<details>
<summary>Abstract</summary>
Adversarial examples threaten the integrity of machine learning systems with alarming success rates even under constrained black-box conditions. Stateful defenses have emerged as an effective countermeasure, detecting potential attacks by maintaining a buffer of recent queries and detecting new queries that are too similar. However, these defenses fundamentally pose a trade-off between attack detection and false positive rates, and this trade-off is typically optimized by hand-picking feature extractors and similarity thresholds that empirically work well. There is little current understanding as to the formal limits of this trade-off and the exact properties of the feature extractors/underlying problem domain that influence it. This work aims to address this gap by offering a theoretical characterization of the trade-off between detection and false positive rates for stateful defenses. We provide upper bounds for detection rates of a general class of feature extractors and analyze the impact of this trade-off on the convergence of black-box attacks. We then support our theoretical findings with empirical evaluations across multiple datasets and stateful defenses.
</details>
<details>
<summary>摘要</summary>
⟨SYS⟩抗对抗示例威胁机器学习系统的稳定性，尤其在受限黑盒条件下。状态防御技术已经出现为有效的反应方法，通过维护最近几个查询来检测潜在攻击，并检测新的查询是否过于相似。然而，这些防御技术存在识别攻击和假阳性率之间的负面交易，这种交易通常通过手动选择特征提取器和相似性阈值来优化。现在我们对这种交易的形式上限和特征提取器/下面问题领域的影响没有很好的理解。这项工作的目的是解决这个问题，通过提供一种理论性的评估交易的权衡率和假阳性率的总bounds。我们还分析了黑盒攻击的收敛性受到这种交易的影响。最后，我们支持我们的理论发现通过多个数据集和状态防御技术的实验性评估。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-ChatGPT-and-GPT-4-for-Visual-Programming"><a href="#Evaluating-ChatGPT-and-GPT-4-for-Visual-Programming" class="headerlink" title="Evaluating ChatGPT and GPT-4 for Visual Programming"></a>Evaluating ChatGPT and GPT-4 for Visual Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02522">http://arxiv.org/abs/2308.02522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adish Singla</li>
<li>for: 本研究旨在检验现代生成模型在视觉编程领域是否具备高水平的能力，与文本编程领域的Python编程相比。</li>
<li>methods: 我们使用了ChatGPT和GPT-4两种生成模型，对各种视觉编程场景进行评估，并使用专家标注来评估其表现。</li>
<li>results: 我们发现，这两种模型在视觉编程领域表现不佳，尤其是在结合空间逻辑和编程技能方面遇到困难。这些结果提供了未来发展生成模型在视觉编程领域的探索方向。<details>
<summary>Abstract</summary>
Generative AI and large language models have the potential to drastically improve the landscape of computing education by automatically generating personalized feedback and content. Recent works have studied the capabilities of these models for different programming education scenarios; however, these works considered only text-based programming, in particular, Python programming. Consequently, they leave open the question of how well these models would perform in visual programming domains popularly used for K-8 programming education. The main research question we study is: Do state-of-the-art generative models show advanced capabilities in visual programming on par with their capabilities in text-based Python programming? In our work, we evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, in visual programming domains for various scenarios and assess performance using expert-based annotations. In particular, we base our evaluation using reference tasks from the domains of Hour of Code: Maze Challenge by Code-dot-org and Karel. Our results show that these models perform poorly and struggle to combine spatial, logical, and programming skills crucial for visual programming. These results also provide exciting directions for future work on developing techniques to improve the performance of generative models in visual programming.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传送文本到简化中文。</SYS>>生成AI和大语言模型有可能在计算教育中提供个性化反馈和内容，从而改善计算教育的景观。先前的研究已经研究了这些模型在不同的编程教育场景下的能力，但是这些研究仅考虑了文本编程，尤其是Python编程。因此，它们留下了如何在视觉编程领域中表现的问题。我们的研究问题是：现代生成模型在视觉编程领域中是否有高水平的表现，与文本基于Python编程的表现相当？在我们的工作中，我们评估了两个模型：ChatGPT（基于GPT-3.5）和GPT-4，在不同的视觉编程场景下进行评估，并使用专家标注来评估性能。具体来说，我们基于Code-dot-org的Hour of Code：迷宫挑战和Karel的参考任务进行评估。我们的结果表明，这些模型在视觉编程中表现糟糕，无法结合空间、逻辑和编程技能，这些技能是视觉编程中的关键。这些结果还提供了未来开发改进生成模型在视觉编程中表现的潜在方向。
</details></li>
</ul>
<hr>
<h2 id="RoseNNa-A-performant-portable-library-for-neural-network-inference-with-application-to-computational-fluid-dynamics"><a href="#RoseNNa-A-performant-portable-library-for-neural-network-inference-with-application-to-computational-fluid-dynamics" class="headerlink" title="RoseNNa: A performant, portable library for neural network inference with application to computational fluid dynamics"></a>RoseNNa: A performant, portable library for neural network inference with application to computational fluid dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16322">http://arxiv.org/abs/2307.16322</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comp-physics/roseNNa">https://github.com/comp-physics/roseNNa</a></li>
<li>paper_authors: Ajay Bati, Spencer H. Bryngelson</li>
<li>for: 这篇论文主要应用在 computational fluid dynamics (CFD) 领域，将神经网络应用到 CFD 模型中以短化模拟时间。</li>
<li>methods: 本论文使用的方法包括 Multilayer Perceptrons (MLPs) 和 Long Short Term Memory (LSTM) 类型的神经网络架构，并通过自动将训练好的模型转换为高性能的 Fortran 库，以便与 CFD 模型集成。</li>
<li>results: 结果显示，使用 RoseNNa 实现神经网络架构后，与 PyTorch 和 libtorch 相比，在 MLPs 和 LSTM RNNs 中，具有少于 100 个隐藏层和 100 个神经元的情况下，可以获得较高的速度优化，具体而言，在小型神经网络中，速度优化因子在 10 到 2 之间，而在大型神经网络中，速度优化因子在 2 倍以上。<details>
<summary>Abstract</summary>
The rise of neural network-based machine learning ushered in high-level libraries, including TensorFlow and PyTorch, to support their functionality. Computational fluid dynamics (CFD) researchers have benefited from this trend and produced powerful neural networks that promise shorter simulation times. For example, multilayer perceptrons (MLPs) and Long Short Term Memory (LSTM) recurrent-based (RNN) architectures can represent sub-grid physical effects, like turbulence. Implementing neural networks in CFD solvers is challenging because the programming languages used for machine learning and CFD are mostly non-overlapping, We present the roseNNa library, which bridges the gap between neural network inference and CFD. RoseNNa is a non-invasive, lightweight (1000 lines), and performant tool for neural network inference, with focus on the smaller networks used to augment PDE solvers, like those of CFD, which are typically written in C/C++ or Fortran. RoseNNa accomplishes this by automatically converting trained models from typical neural network training packages into a high-performance Fortran library with C and Fortran APIs. This reduces the effort needed to access trained neural networks and maintains performance in the PDE solvers that CFD researchers build and rely upon. Results show that RoseNNa reliably outperforms PyTorch (Python) and libtorch (C++) on MLPs and LSTM RNNs with less than 100 hidden layers and 100 neurons per layer, even after removing the overhead cost of API calls. Speedups range from a factor of about 10 and 2 faster than these established libraries for the smaller and larger ends of the neural network size ranges tested.
</details>
<details>
<summary>摘要</summary>
neural network基于机器学习的兴起使得高级库，如TensorFlow和PyTorch，得到了支持。计算流体动力学（CFD）研究人员也从中受益，制作出了 poderoso neural networks， promises shorter simulation times。例如，多层感知器（MLPs）和Long Short Term Memory（LSTM）回归型（RNN）架构可以表示sub-grid物理效应，如湍流。实施 neural networks在CFD solvers 中是具有挑战，因为机器学习和CFD的编程语言通常不 overlap。我们提出了roseNNa库，它 bridge gap between neural network inference and CFD。roseNNa 是一个不侵入、轻量级（1000行），并且高性能的工具，用于 neural network inference，专注于 CFD 中常用的小型网络。它通过自动将训练过的模型从通常的 neural network 训练包转换成高性能 Fortran 库，并提供 C 和 Fortran API，从而降低了访问训练过的 neural networks 的努力，并保持了在 PDE 解决方案中的性能。结果表明，roseNNa 可靠地超越了 PyTorch（Python）和 libtorch（C++）在 MLPs 和 LSTM RNNs 中，即使去除 API 调用的开销。速度提高范围从约10倍到2倍，depending on the size of the neural network。
</details></li>
</ul>
<hr>
<h2 id="Towards-Practical-Robustness-Auditing-for-Linear-Regression"><a href="#Towards-Practical-Robustness-Auditing-for-Linear-Regression" class="headerlink" title="Towards Practical Robustness Auditing for Linear Regression"></a>Towards Practical Robustness Auditing for Linear Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16315">http://arxiv.org/abs/2307.16315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Freund, Samuel B. Hopkins</li>
<li>for: 检测小数据集的影响，以逆转OLS回归系数的策略</li>
<li>methods: 使用混合整数 quadratic constrained optimization 和 exact greedy method</li>
<li>results: 比state of the art大大提高性能，但对高维度问题还存在计算瓶颈<details>
<summary>Abstract</summary>
We investigate practical algorithms to find or disprove the existence of small subsets of a dataset which, when removed, reverse the sign of a coefficient in an ordinary least squares regression involving that dataset. We empirically study the performance of well-established algorithmic techniques for this task -- mixed integer quadratically constrained optimization for general linear regression problems and exact greedy methods for special cases. We show that these methods largely outperform the state of the art and provide a useful robustness check for regression problems in a few dimensions. However, significant computational bottlenecks remain, especially for the important task of disproving the existence of such small sets of influential samples for regression problems of dimension $3$ or greater. We make some headway on this challenge via a spectral algorithm using ideas drawn from recent innovations in algorithmic robust statistics. We summarize the limitations of known techniques in several challenge datasets to encourage further algorithmic innovation.
</details>
<details>
<summary>摘要</summary>
我们研究了实用的算法来找到或证明 dataset 中小subset 的存在，将其从 regression 问题中除去，使得回归系数的符号变化。我们对常用的算法技术进行实证研究，包括混合整数 quadratic 约束优化问题和特殊情况下的精准搜索方法。我们发现这些方法在大多数情况下表现出色，提供了有用的 robustness 检查。但是，特别是 dla regression 问题的维度大于 3 的情况下，计算瓶颈仍然存在，导致重要的 task 的实现受阻。我们通过使用最近的算法Robust statistics 的想法，开发了一种spectral 算法，尝试解决这个挑战。我们对一些挑战数据集的限制进行总结，以鼓励进一步的算法创新。
</details></li>
</ul>
<hr>
<h2 id="Mask-guided-Data-Augmentation-for-Multiparametric-MRI-Generation-with-a-Rare-Hepatocellular-Carcinoma"><a href="#Mask-guided-Data-Augmentation-for-Multiparametric-MRI-Generation-with-a-Rare-Hepatocellular-Carcinoma" class="headerlink" title="Mask-guided Data Augmentation for Multiparametric MRI Generation with a Rare Hepatocellular Carcinoma"></a>Mask-guided Data Augmentation for Multiparametric MRI Generation with a Rare Hepatocellular Carcinoma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16314">http://arxiv.org/abs/2307.16314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karen Sanchez, Carlos Hinojosa, Kevin Arias, Henry Arguello, Denis Kouame, Olivier Meyrignac, Adrian Basarab</li>
<li>for: 这篇论文主要是为了提高深度学习模型在医疗领域的性能而开发的数据扩充技术。</li>
<li>methods: 这篇论文提出了一种新的数据扩充建议，通过一种生成深度学习approach来生成多 Parametric（T1arterial、T1portal和T2）磁共振成像（MRI）图像，并生成相应的肝肿瘤mask。</li>
<li>results: 论文的实验结果表明，这种方法可以使用有限的多 Parametric dataset来生成1000个synthetic triplets和其对应的肝肿瘤mask，Frechet Inception Distance分数为86.55。这种方法在2021年数据扩充挑战中获得了优胜奖。<details>
<summary>Abstract</summary>
Data augmentation is classically used to improve the overall performance of deep learning models. It is, however, challenging in the case of medical applications, and in particular for multiparametric datasets. For example, traditional geometric transformations used in several applications to generate synthetic images can modify in a non-realistic manner the patients' anatomy. Therefore, dedicated image generation techniques are necessary in the medical field to, for example, mimic a given pathology realistically. This paper introduces a new data augmentation architecture that generates synthetic multiparametric (T1 arterial, T1 portal, and T2) magnetic resonance images (MRI) of massive macrotrabecular subtype hepatocellular carcinoma with their corresponding tumor masks through a generative deep learning approach. The proposed architecture creates liver tumor masks and abdominal edges used as input in a Pix2Pix network for synthetic data creation. The method's efficiency is demonstrated by training it on a limited multiparametric dataset of MRI triplets from $89$ patients with liver lesions to generate $1,000$ synthetic triplets and their corresponding liver tumor masks. The resulting Frechet Inception Distance score was $86.55$. The proposed approach was among the winners of the 2021 data augmentation challenge organized by the French Society of Radiology.
</details>
<details>
<summary>摘要</summary>
“数据扩展是深度学习模型性能改进的传统方法，但在医疗应用中具有挑战性，特别是对多 параметric 数据进行处理。例如，传统的几何变换在许多应用中用于生成合成图像，可能会非实际地改变病人的解剖结构。因此，医疗领域需要专门的图像生成技术，例如模拟给定疾病的合成图像。这篇论文介绍了一种新的数据扩展建立，通过生成多参数（T1血管、T1入口和T2）核磁共振成像（MRI）图像，并生成相应的肝肿瘤面积大型巨孢细胞肿瘤肝癌的合成图像。提议的建立使用生成深度学习方法创建肝肿瘤面积和腹部边缘，并将其作为PIx2Pix网络的输入进行合成数据创建。方法的效率被证明通过使用有限的多参数MRI三重组合来训练，从89名患有肝脏病变的病人中生成1000个合成三重组合和相应的肝肿瘤面积。结果的Frechet Inception Distance分数为86.55。该方法在2021年数据扩展挑战中被法国放射学会评选为获奖作品。”Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="You-Shall-not-Pass-the-Zero-Gradient-Problem-in-Predict-and-Optimize-for-Convex-Optimization"><a href="#You-Shall-not-Pass-the-Zero-Gradient-Problem-in-Predict-and-Optimize-for-Convex-Optimization" class="headerlink" title="You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization"></a>You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16304">http://arxiv.org/abs/2307.16304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grigorii Veviurko, Wendelin Böhmer, Mathijs de Weerdt</li>
<li>for: 这篇论文是关于决策参数预测和优化的一种流行方法，它使用机器学习来预测优化问题中的未知参数。</li>
<li>methods: 这篇论文使用了任务性能作为损失函数，以训练预测模型。</li>
<li>results: 论文发现了这种方法的一个弱点—梯度问题，并提出了一种基于差分优化的解决方法，并在两个实际 benchmark 上进行了验证。<details>
<summary>Abstract</summary>
Predict and optimize is an increasingly popular decision-making paradigm that employs machine learning to predict unknown parameters of optimization problems. Instead of minimizing the prediction error of the parameters, it trains predictive models using task performance as a loss function. In the convex optimization domain, predict and optimize has seen significant progress due to recently developed methods for differentiating optimization problem solutions over the problem parameters. This paper identifies a yet unnoticed drawback of this approach -- the zero-gradient problem -- and introduces a method to solve it. The suggested method is based on the mathematical properties of differential optimization and is verified using two real-world benchmarks.
</details>
<details>
<summary>摘要</summary>
预测和优化是一种日益受欢迎的决策模式，它利用机器学习预测未知优化问题中的参数。而不是将参数预测错误降为最小值，它使用任务性能作为损失函数来训练预测模型。在凸优化领域，预测和优化已经取得了显著进步，这主要归功于最近发展出的对优化问题解决方案的差分优化方法。然而，这种方法还存在一个未注意的缺点：零梯度问题。这篇论文描述了这个缺点，并提出了一种解决方法，基于差分优化的数学性质。该方法在两个真实世界 benchmark 上进行了验证。
</details></li>
</ul>
<hr>
<h2 id="Predicting-delays-in-Indian-lower-courts-using-AutoML-and-Decision-Forests"><a href="#Predicting-delays-in-Indian-lower-courts-using-AutoML-and-Decision-Forests" class="headerlink" title="Predicting delays in Indian lower courts using AutoML and Decision Forests"></a>Predicting delays in Indian lower courts using AutoML and Decision Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16285">http://arxiv.org/abs/2307.16285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mb7419/pendencyprediction">https://github.com/mb7419/pendencyprediction</a></li>
<li>paper_authors: Mohit Bhatnagar, Shivraj Huchhanavar</li>
<li>for: 预测印度下级法院延迟的 Classification 模型 (what the paper is written for)</li>
<li>methods: 使用 AutoML 开发多类别分类模型，并使用 binary decision forest 分类器提高预测精度 (what methods the paper uses)</li>
<li>results: 最佳模型达到 81.4% 的准确率，并且 precision、recall 和 F1 分别为 0.81 (what results the paper gets)<details>
<summary>Abstract</summary>
This paper presents a classification model that predicts delays in Indian lower courts based on case information available at filing. The model is built on a dataset of 4.2 million court cases filed in 2010 and their outcomes over a 10-year period. The data set is drawn from 7000+ lower courts in India. The authors employed AutoML to develop a multi-class classification model over all periods of pendency and then used binary decision forest classifiers to improve predictive accuracy for the classification of delays. The best model achieved an accuracy of 81.4%, and the precision, recall, and F1 were found to be 0.81. The study demonstrates the feasibility of AI models for predicting delays in Indian courts, based on relevant data points such as jurisdiction, court, judge, subject, and the parties involved. The paper also discusses the results in light of relevant literature and suggests areas for improvement and future research. The authors have made the dataset and Python code files used for the analysis available for further research in the crucial and contemporary field of Indian judicial reform.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "lower courts" is translated as "下级法院" (xià jí fǎ yuàn), which refers to the lower-level courts in India's judicial system.* "case information" is translated as "案件信息" (àn jiàn xìn xì), which refers to the details of each court case.* "AutoML" is translated as "自动机器学习" (zì dòng jī shù xí), which stands for "automated machine learning" and refers to the use of software tools to automate the process of building machine learning models.* "multi-class classification model" is translated as "多类分类模型" (duō lèi fēn c categorization model), which refers to a type of machine learning model that can predict multiple classes or outcomes.* "binary decision forest classifiers" is translated as "二分裂树分类器" (èr fēn jié shù fēn c categorization), which refers to a type of machine learning model that uses a combination of decision trees to predict binary outcomes.* "precision, recall, and F1" are all translated as "准确率、报告率和F1值" (zhèng qiú lǐ, bào gào lǐ, and F1 value), which are all measures of the accuracy of a machine learning model.
</details></li>
</ul>
<hr>
<h2 id="zkDL-Efficient-Zero-Knowledge-Proofs-of-Deep-Learning-Training"><a href="#zkDL-Efficient-Zero-Knowledge-Proofs-of-Deep-Learning-Training" class="headerlink" title="zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training"></a>zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16273">http://arxiv.org/abs/2307.16273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Sun, Hongyang Zhang</li>
<li>for: 这篇论文旨在提供一种高效的零知证证明方法，以保护不信任的AI开发者的知识产权。</li>
<li>methods: 该方法使用特殊的零知证证明协议——zkReLU，以优化证明时间和证明大小，使其适用于ReLU激活函数，这是许多验证过程中的主要障碍。</li>
<li>results: zkDL可以在几秒钟内，对一个深度16层神经网络，并在200M参数下，生成完整和准确的证明，证明时间和证明大小均在20 kB之下，保护数据和模型参数的隐私。<details>
<summary>Abstract</summary>
The recent advancements in deep learning have brought about significant changes in various aspects of people's lives. Meanwhile, these rapid developments have raised concerns about the legitimacy of the training process of deep networks. However, to protect the intellectual properties of untrusted AI developers, directly examining the training process by accessing the model parameters and training data by verifiers is often prohibited.   In response to this challenge, we present zkDL, an efficient zero-knowledge proof of deep learning training. At the core of zkDL is zkReLU, a specialized zero-knowledge proof protocol with optimized proving time and proof size for the ReLU activation function, a major obstacle in verifiable training due to its non-arithmetic nature. To integrate zkReLU into the proof system for the entire training process, we devise a novel construction of an arithmetic circuit from neural networks. By leveraging the abundant parallel computation resources, this construction reduces proving time and proof sizes by a factor of the network depth. As a result, zkDL enables the generation of complete and sound proofs, taking less than a minute with a size of less than 20 kB per training step, for a 16-layer neural network with 200M parameters, while ensuring the privacy of data and model parameters.
</details>
<details>
<summary>摘要</summary>
The core of zkDL is zkReLU, a specialized zero-knowledge proof protocol that is optimized for the ReLU activation function, which has been a major obstacle in verifiable training due to its non-arithmetic nature. By leveraging abundant parallel computation resources, we have devised a novel construction of an arithmetic circuit from neural networks, which reduces proving time and proof sizes by a factor of the network depth.With zkDL, we can generate complete and sound proofs in less than a minute, with a size of less than 20 kB per training step, for a 16-layer neural network with 200M parameters, while ensuring the privacy of data and model parameters. This solution is efficient and effective, and it addresses the challenge of verifying the training process of deep networks in a privacy-preserving manner.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/31/cs.LG_2023_07_31/" data-id="cloh7tqiv00m47b883vajbb9f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/31/eess.IV_2023_07_31/" class="article-date">
  <time datetime="2023-07-31T09:00:00.000Z" itemprop="datePublished">2023-07-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/31/eess.IV_2023_07_31/">eess.IV - 2023-07-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hybrid-quantum-transfer-learning-for-crack-image-classification-on-NISQ-hardware"><a href="#Hybrid-quantum-transfer-learning-for-crack-image-classification-on-NISQ-hardware" class="headerlink" title="Hybrid quantum transfer learning for crack image classification on NISQ hardware"></a>Hybrid quantum transfer learning for crack image classification on NISQ hardware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16723">http://arxiv.org/abs/2307.16723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Geng, Ali Moghiseh, Claudia Redenbach, Katja Schladitz</li>
<li>for: 这个研究是用于应用量子机器学习来检测灰度图像中的裂线。</li>
<li>methods: 这个研究使用了量子转移学习，并比较了各种量子处理器的执行效率。</li>
<li>results: 研究发现，使用量子转移学习可以实现高效地检测灰度图像中的裂线，并且可以在不同的量子处理器上进行实际实现。<details>
<summary>Abstract</summary>
Quantum computers possess the potential to process data using a remarkably reduced number of qubits compared to conventional bits, as per theoretical foundations. However, recent experiments have indicated that the practical feasibility of retrieving an image from its quantum encoded version is currently limited to very small image sizes. Despite this constraint, variational quantum machine learning algorithms can still be employed in the current noisy intermediate scale quantum (NISQ) era. An example is a hybrid quantum machine learning approach for edge detection. In our study, we present an application of quantum transfer learning for detecting cracks in gray value images. We compare the performance and training time of PennyLane's standard qubits with IBM's qasm\_simulator and real backends, offering insights into their execution efficiency.
</details>
<details>
<summary>摘要</summary>
量子计算机具有可能处理数据使用非常减少的量子比特数量，根据理论基础。然而，最近的实验表明目前只能处理非常小的图像大小。尽管有这些限制，可以在当前的不纯量子Intermediate scale quantum（NISQ）时代使用量子机器学习算法。我们的研究中，我们应用量子传输学习来探测灰度图像中的裂 crack。我们比较了PennyLane的标准量子比特和IBM的qasm_simulator和真实后端的执行效率。
</details></li>
</ul>
<hr>
<h2 id="Conditioning-Generative-Latent-Optimization-to-solve-Imaging-Inverse-Problems"><a href="#Conditioning-Generative-Latent-Optimization-to-solve-Imaging-Inverse-Problems" class="headerlink" title="Conditioning Generative Latent Optimization to solve Imaging Inverse Problems"></a>Conditioning Generative Latent Optimization to solve Imaging Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16670">http://arxiv.org/abs/2307.16670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Braure, Kévin Ginsburger</li>
<li>for: 这个论文主要针对医学成像逆问题（IIP），尤其是在稀疏X射线投影设置下。</li>
<li>methods: 这篇论文使用了完全无监督的技术，主要是使用得分数据驱动模型来解决IIP。</li>
<li>results: 研究表明，使用cGLO方法可以在稀疏视角CT设置下提供更好的重建效果，并且不需要使用回探操作。此外，cGLO方法在小训练数据集下也表现出了更好的效果。<details>
<summary>Abstract</summary>
Computed Tomography (CT) is a prominent example of Imaging Inverse Problem (IIP), highlighting the unrivalled performances of data-driven methods in degraded measurements setups like sparse X-ray projections. Although a significant proportion of deep learning approaches benefit from large supervised datasets to directly map experimental measurements to medical scans, they cannot generalize to unknown acquisition setups. In contrast, fully unsupervised techniques, most notably using score-based generative models, have recently demonstrated similar or better performances compared to supervised approaches to solve IIPs while being flexible at test time regarding the imaging setup. However, their use cases are limited by two factors: (a) they need considerable amounts of training data to have good generalization properties and (b) they require a backward operator, like Filtered-Back-Projection in the case of CT, to condition the learned prior distribution of medical scans to experimental measurements. To overcome these issues, we propose an unsupervised conditional approach to the Generative Latent Optimization framework (cGLO), in which the parameters of a decoder network are initialized on an unsupervised dataset. The decoder is then used for reconstruction purposes, by performing Generative Latent Optimization with a loss function directly comparing simulated measurements from proposed reconstructions to experimental measurements. The resulting approach, tested on sparse-view CT using multiple training dataset sizes, demonstrates better reconstruction quality compared to state-of-the-art score-based strategies in most data regimes and shows an increasing performance advantage for smaller training datasets and reduced projection angles. Furthermore, cGLO does not require any backward operator and could expand use cases even to non-linear IIPs.
</details>
<details>
<summary>摘要</summary>
computed tomography (CT) 是一个典型的 imaging inverse problem (IIP) 例子， highlighting 数据驱动方法在受限的测量设置中的无与伦比的表现。 although 许多深度学习方法可以从大量的直接映射实验室测量到医疗扫描的 supervised datasets 中获得优秀的表现，它们无法泛化到未知的获取设置。 相反，完全不supervised 技术，主要是使用得分数据生成模型，在 recent  years 中 demonstrates 与 supervised 方法相当或更好的表现，而且可以在测试时随意选择 imaging 设置。 however， its use cases 受到两个因素的限制： (a) 它们需要大量的训练数据来有好的泛化性质， (b) 它们需要一个 backwards Operator，如 filtered-back-projection 在 CT 中，以Conditional 学习的 learned prior distribution of medical scans 到实验测量。To overcome these issues, we propose an unsupervised conditional approach to the Generative Latent Optimization framework (cGLO), in which the parameters of a decoder network are initialized on an unsupervised dataset. the decoder is then used for reconstruction purposes, by performing Generative Latent Optimization with a loss function directly comparing simulated measurements from proposed reconstructions to experimental measurements. the resulting approach, tested on sparse-view CT using multiple training dataset sizes, demonstrates better reconstruction quality compared to state-of-the-art score-based strategies in most data regimes and shows an increasing performance advantage for smaller training datasets and reduced projection angles. Furthermore, cGLO does not require any backward operator and could expand use cases even to non-linear IIPs.
</details></li>
</ul>
<hr>
<h2 id="Towards-General-Low-Light-Raw-Noise-Synthesis-and-Modeling"><a href="#Towards-General-Low-Light-Raw-Noise-Synthesis-and-Modeling" class="headerlink" title="Towards General Low-Light Raw Noise Synthesis and Modeling"></a>Towards General Low-Light Raw Noise Synthesis and Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16508">http://arxiv.org/abs/2307.16508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fengzhang427/LRD">https://github.com/fengzhang427/LRD</a></li>
<li>paper_authors: Feng Zhang, Bin Xu, Zhiqiang Li, Xinran Liu, Qingbo Lu, Changxin Gao, Nong Sang</li>
<li>for: 本研究旨在Addressing the problem of modeling and synthesizing low-light raw noise in computational photography and image processing applications.</li>
<li>methods: 我们提出了一种新的方法，即通过物理和学习模型来同时Synthesize signal-dependent and signal-independent noise.</li>
<li>results: 我们的方法可以同时学习不同ISO水平的噪声特性，并可以通过多尺度扩展Discriminator（FTD）来正确地分布噪声. 实验结果表明，我们的方法可以与现有方法相比，在不同的感器上表现出优异的denoising效果.<details>
<summary>Abstract</summary>
Modeling and synthesizing low-light raw noise is a fundamental problem for computational photography and image processing applications. Although most recent works have adopted physics-based models to synthesize noise, the signal-independent noise in low-light conditions is far more complicated and varies dramatically across camera sensors, which is beyond the description of these models. To address this issue, we introduce a new perspective to synthesize the signal-independent noise by a generative model. Specifically, we synthesize the signal-dependent and signal-independent noise in a physics- and learning-based manner, respectively. In this way, our method can be considered as a general model, that is, it can simultaneously learn different noise characteristics for different ISO levels and generalize to various sensors. Subsequently, we present an effective multi-scale discriminator termed Fourier transformer discriminator (FTD) to distinguish the noise distribution accurately. Additionally, we collect a new low-light raw denoising (LRD) dataset for training and benchmarking. Qualitative validation shows that the noise generated by our proposed noise model can be highly similar to the real noise in terms of distribution. Furthermore, extensive denoising experiments demonstrate that our method performs favorably against state-of-the-art methods on different sensors.
</details>
<details>
<summary>摘要</summary>
模型和 sintesizar 低光照Raw 噪声是计算摄影和图像处理应用中的基本问题。although most recent works have adopted physics-based models to synthesize noise, the signal-independent noise in low-light conditions is far more complicated and varies dramatically across camera sensors, which is beyond the description of these models. To address this issue, we introduce a new perspective to synthesize the signal-independent noise by a generative model. Specifically, we synthesize the signal-dependent and signal-independent noise in a physics- and learning-based manner, respectively. In this way, our method can be considered as a general model, that is, it can simultaneously learn different noise characteristics for different ISO levels and generalize to various sensors. Subsequently, we present an effective multi-scale discriminator termed Fourier transformer discriminator (FTD) to distinguish the noise distribution accurately. Additionally, we collect a new low-light raw denoising (LRD) dataset for training and benchmarking. Qualitative validation shows that the noise generated by our proposed noise model can be highly similar to the real noise in terms of distribution. Furthermore, extensive denoising experiments demonstrate that our method performs favorably against state-of-the-art methods on different sensors.Here's the translation in Traditional Chinese:模型和 sintesizar 低光照Raw 噪声是计算摄影和图像处理应用中的基本问题。although most recent works have adopted physics-based models to synthesize noise, the signal-independent noise in low-light conditions is far more complicated and varies dramatically across camera sensors, which is beyond the description of these models. To address this issue, we introduce a new perspective to synthesize the signal-independent noise by a generative model. Specifically, we synthesize the signal-dependent and signal-independent noise in a physics- and learning-based manner, respectively. In this way, our method can be considered as a general model, that is, it can simultaneously learn different noise characteristics for different ISO levels and generalize to various sensors. Subsequently, we present an effective multi-scale discriminator termed Fourier transformer discriminator (FTD) to distinguish the noise distribution accurately. Additionally, we collect a new low-light raw denoising (LRD) dataset for training and benchmarking. Qualitative validation shows that the noise generated by our proposed noise model can be highly similar to the real noise in terms of distribution. Furthermore, extensive denoising experiments demonstrate that our method performs favorably against state-of-the-art methods on different sensors.
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-approach-for-improving-U-Net-variants-in-medical-image-segmentation"><a href="#A-hybrid-approach-for-improving-U-Net-variants-in-medical-image-segmentation" class="headerlink" title="A hybrid approach for improving U-Net variants in medical image segmentation"></a>A hybrid approach for improving U-Net variants in medical image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16462">http://arxiv.org/abs/2307.16462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aitik Gupta, Dr. Joydip Dhar</li>
<li>for: 这篇论文主要针对医疗图像分割领域，旨在提高医疗图像分割的精度和效率。</li>
<li>methods: 该论文使用深度学习方法，包括MultiResUNet、Attention U-Net等 variants，以及depthwise separable convolutions来降低网络参数的需求，同时保持一定的性能水平。</li>
<li>results: 该论文通过使用注意力系统和径向连接来提高医疗图像分割的准确率和效率，并且在皮肤病变分割任务中达到了一定的成果。<details>
<summary>Abstract</summary>
Medical image segmentation is vital to the area of medical imaging because it enables professionals to more accurately examine and understand the information offered by different imaging modalities. The technique of splitting a medical image into various segments or regions of interest is known as medical image segmentation. The segmented images that are produced can be used for many different things, including diagnosis, surgery planning, and therapy evaluation.   In initial phase of research, major focus has been given to review existing deep-learning approaches, including researches like MultiResUNet, Attention U-Net, classical U-Net, and other variants. The attention feature vectors or maps dynamically add important weights to critical information, and most of these variants use these to increase accuracy, but the network parameter requirements are somewhat more stringent. They face certain problems such as overfitting, as their number of trainable parameters is very high, and so is their inference time.   Therefore, the aim of this research is to reduce the network parameter requirements using depthwise separable convolutions, while maintaining performance over some medical image segmentation tasks such as skin lesion segmentation using attention system and residual connections.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是医疗图像领域的关键技术，它使医 profesionales可以更加准确地检查和理解不同的成像模式中提供的信息。图像分割技术的核心是将医疗图像分成不同的区域或特点，以便进行更加准确的诊断、手术规划和治疗评估。在初期研究阶段，主要是对现有深度学习方法进行了审查，包括MultiResUNet、Attention U-Net、类传统U-Net和其他变体。这些变体的注意力特征向量或地图在运行时动态地给予重要的权重，以提高准确性。然而，这些网络的参数需求较高，导致过拟合和执行时间较长。因此，本研究的目标是通过深度分割减少网络参数，保持一定的性能水平，特别是在医学图像分割任务中，如皮肤病变分割使用注意力系统和 residual 连接。
</details></li>
</ul>
<hr>
<h2 id="High-Dynamic-Range-Image-Reconstruction-via-Deep-Explicit-Polynomial-Curve-Estimation"><a href="#High-Dynamic-Range-Image-Reconstruction-via-Deep-Explicit-Polynomial-Curve-Estimation" class="headerlink" title="High Dynamic Range Image Reconstruction via Deep Explicit Polynomial Curve Estimation"></a>High Dynamic Range Image Reconstruction via Deep Explicit Polynomial Curve Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16426">http://arxiv.org/abs/2307.16426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jqtangust/epce-hdr">https://github.com/jqtangust/epce-hdr</a></li>
<li>paper_authors: Jiaqi Tang, Xiaogang Xu, Sixing Hu, Ying-Cong Chen</li>
<li>for: 解决镜头缺乏能力导致数字图像的动态范围受限，提高图像的动态范围以更好地反映实际场景。</li>
<li>methods: 提出一种使用单一神经网络来显式地估算折射函数和其对应的HDR图像的方法，并使用synthetic和实际图像构建一个新的数据集来验证该方法的一致性和性能。</li>
<li>results: 经验表明，该方法可以在不同的折射函数下进行一致性的重建，并达到领先的性能水平。<details>
<summary>Abstract</summary>
Due to limited camera capacities, digital images usually have a narrower dynamic illumination range than real-world scene radiance. To resolve this problem, High Dynamic Range (HDR) reconstruction is proposed to recover the dynamic range to better represent real-world scenes. However, due to different physical imaging parameters, the tone-mapping functions between images and real radiance are highly diverse, which makes HDR reconstruction extremely challenging. Existing solutions can not explicitly clarify a corresponding relationship between the tone-mapping function and the generated HDR image, but this relationship is vital when guiding the reconstruction of HDR images. To address this problem, we propose a method to explicitly estimate the tone mapping function and its corresponding HDR image in one network. Firstly, based on the characteristics of the tone mapping function, we construct a model by a polynomial to describe the trend of the tone curve. To fit this curve, we use a learnable network to estimate the coefficients of the polynomial. This curve will be automatically adjusted according to the tone space of the Low Dynamic Range (LDR) image, and reconstruct the real HDR image. Besides, since all current datasets do not provide the corresponding relationship between the tone mapping function and the LDR image, we construct a new dataset with both synthetic and real images. Extensive experiments show that our method generalizes well under different tone-mapping functions and achieves SOTA performance.
</details>
<details>
<summary>摘要</summary>
First, based on the characteristics of the tone mapping function, we construct a model using a polynomial to describe the trend of the tone curve. To fit this curve, we use a learnable network to estimate the coefficients of the polynomial. This curve will be automatically adjusted according to the tone space of the Low Dynamic Range (LDR) image and reconstruct the real HDR image.Furthermore, since all current datasets do not provide the corresponding relationship between the tone mapping function and the LDR image, we construct a new dataset with both synthetic and real images. Extensive experiments show that our method generalizes well under different tone-mapping functions and achieves state-of-the-art performance.
</details></li>
</ul>
<hr>
<h2 id="DRAW-Defending-Camera-shooted-RAW-against-Image-Manipulation"><a href="#DRAW-Defending-Camera-shooted-RAW-against-Image-Manipulation" class="headerlink" title="DRAW: Defending Camera-shooted RAW against Image Manipulation"></a>DRAW: Defending Camera-shooted RAW against Image Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16418">http://arxiv.org/abs/2307.16418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Hu, Qichao Ying, Zhenxing Qian, Sheng Li, Xinpeng Zhang</li>
<li>for: 防止图像修改和增强图像源安全性</li>
<li>methods: 利用多频部分融合网络和隐藏水印技术保护原始RAW数据</li>
<li>results: 在多个知名RAW数据集上实现了修改和增强图像的抵抗性，并且可以准确地 Localize增强区域。<details>
<summary>Abstract</summary>
RAW files are the initial measurement of scene radiance widely used in most cameras, and the ubiquitously-used RGB images are converted from RAW data through Image Signal Processing (ISP) pipelines. Nowadays, digital images are risky of being nefariously manipulated. Inspired by the fact that innate immunity is the first line of body defense, we propose DRAW, a novel scheme of defending images against manipulation by protecting their sources, i.e., camera-shooted RAWs. Specifically, we design a lightweight Multi-frequency Partial Fusion Network (MPF-Net) friendly to devices with limited computing resources by frequency learning and partial feature fusion. It introduces invisible watermarks as protective signal into the RAW data. The protection capability can not only be transferred into the rendered RGB images regardless of the applied ISP pipeline, but also is resilient to post-processing operations such as blurring or compression. Once the image is manipulated, we can accurately identify the forged areas with a localization network. Extensive experiments on several famous RAW datasets, e.g., RAISE, FiveK and SIDD, indicate the effectiveness of our method. We hope that this technique can be used in future cameras as an option for image protection, which could effectively restrict image manipulation at the source.
</details>
<details>
<summary>摘要</summary>
RAW文件是现场辐射强度的初始测量数据，广泛用于大多数相机中。现在，数字图像容易受到负面的修改。 inspirited by身体的自然免疫力是第一道防御线，我们提出了一种新的图像防范修改方案，通过保护图像的来源，即相机拍摄的RAW文件。我们设计了一种轻量级多频部分融合网络（MPF-Net），适合具有有限计算资源的设备。这个网络通过频率学习和部分特征融合，将不可见的水印（protective signal）引入RAW数据中。这种保护能力不仅可以在渲染后RGB图像中传递，而且对后期处理操作，如压缩或抖杂，也具有抗性。如果图像被修改，我们可以使用本地化网络来准确地标识受到修改的区域。我们在许多知名的RAW数据集，如RAISE、FiveK和SIDD上进行了广泛的实验，结果表明我们的方法的有效性。我们希望这种技术可以在未来的相机中作为图像保护选项，以防止图像修改在源头级别。
</details></li>
</ul>
<hr>
<h2 id="Multi-modal-Graph-Neural-Network-for-Early-Diagnosis-of-Alzheimer’s-Disease-from-sMRI-and-PET-Scans"><a href="#Multi-modal-Graph-Neural-Network-for-Early-Diagnosis-of-Alzheimer’s-Disease-from-sMRI-and-PET-Scans" class="headerlink" title="Multi-modal Graph Neural Network for Early Diagnosis of Alzheimer’s Disease from sMRI and PET Scans"></a>Multi-modal Graph Neural Network for Early Diagnosis of Alzheimer’s Disease from sMRI and PET Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16366">http://arxiv.org/abs/2307.16366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanteng Zhanga, Xiaohai He, Yi Hao Chan, Qizhi Teng, Jagath C. Rajapakse</li>
<li>for: 这个研究旨在提出一种基于图形深度学习（Graph Neural Network，GNN）的多modal资料融合方法，用于早期诊断阿尔茨海默病（Alzheimer’s Disease，AD）。</li>
<li>methods: 这个研究使用了两种不同的图形深度学习方法：一种是基于图形的GNN，另一种是基于人类的GNN。这两种方法在不同的分支中进行训练，然后使用late fusion融合以获得最终的预测结果。</li>
<li>results: 实验结果显示，这个提出的多modal资料融合方法可以提高AD诊断的性能，并且显示了不同的图形深度学习方法在不同的分支中的表现。此研究也提供了一个技术参考，支持多重多modal诊断方法的需求。<details>
<summary>Abstract</summary>
In recent years, deep learning models have been applied to neuroimaging data for early diagnosis of Alzheimer's disease (AD). Structural magnetic resonance imaging (sMRI) and positron emission tomography (PET) images provide structural and functional information about the brain, respectively. Combining these features leads to improved performance than using a single modality alone in building predictive models for AD diagnosis. However, current multi-modal approaches in deep learning, based on sMRI and PET, are mostly limited to convolutional neural networks, which do not facilitate integration of both image and phenotypic information of subjects. We propose to use graph neural networks (GNN) that are designed to deal with problems in non-Euclidean domains. In this study, we demonstrate how brain networks can be created from sMRI or PET images and be used in a population graph framework that can combine phenotypic information with imaging features of these brain networks. Then, we present a multi-modal GNN framework where each modality has its own branch of GNN and a technique is proposed to combine the multi-modal data at both the level of node vectors and adjacency matrices. Finally, we perform late fusion to combine the preliminary decisions made in each branch and produce a final prediction. As multi-modality data becomes available, multi-source and multi-modal is the trend of AD diagnosis. We conducted explorative experiments based on multi-modal imaging data combined with non-imaging phenotypic information for AD diagnosis and analyzed the impact of phenotypic information on diagnostic performance. Results from experiments demonstrated that our proposed multi-modal approach improves performance for AD diagnosis, and this study also provides technical reference and support the need for multivariate multi-modal diagnosis methods.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习模型在神经成像数据上进行早期诊断阿尔茨海默病（AD）已经广泛应用。Structural磁共振成像（sMRI）和萱 electrons Tomatoes（PET）成像提供了脑部结构和功能信息，分别。将这些特征相结合，可以建立更好的预测模型，than using a single modality alone。然而，目前的多Modalapproaches in deep learning，基于sMRI和PET，主要是基于卷积神经网络，这些网络不能整合图像和参数信息。我们提议使用图 neural networks（GNN），这些网络是非欧几何问题的解决方案。在本研究中，我们示示了如何从sMRI或PET成像中创建脑网络，并使用人口图框架将图像和参数信息结合。然后，我们提出了一种多Modal GNN框架，其中每个模式有自己的GNN分支，并提出了将多Modal数据在级别Node vector和相互作用矩阵之间进行结合的技术。最后，我们进行了较晚的融合，将每个分支的初步决策相互融合，并生成最终预测。随着多Modal数据变得更加可用，多Modal和多源是AD诊断的趋势。我们基于多Modal成像数据和非成像参数信息进行了探索性实验，分析了影响诊断性能的非成像信息。实验结果表明，我们提议的多Modal方法可以提高AD诊断性能，这也提供了技术参考，支持多变量多Modal诊断方法的需求。
</details></li>
</ul>
<hr>
<h2 id="Cardiac-MRI-Orientation-Recognition-and-Standardization-using-Deep-Neural-Networks"><a href="#Cardiac-MRI-Orientation-Recognition-and-Standardization-using-Deep-Neural-Networks" class="headerlink" title="Cardiac MRI Orientation Recognition and Standardization using Deep Neural Networks"></a>Cardiac MRI Orientation Recognition and Standardization using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00615">http://arxiv.org/abs/2308.00615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rxzhen/mscmr-orient">https://github.com/rxzhen/mscmr-orient</a></li>
<li>paper_authors: Ruoxuan Zhen</li>
<li>For: The paper is written for the purpose of addressing the challenge of imaging orientation in cardiac MRI and presenting a deep learning-based method for categorizing and standardizing the orientation.* Methods: The paper employs deep neural networks to categorize and standardize the orientation of cardiac MRI images, and proposes a transfer learning strategy to adapt the model to diverse modalities.* Results: The validation accuracies achieved were 100.0%, 100.0%, and 99.4% on CMR images from various modalities, including bSSFP, T2, and LGE, confirming the robustness and effectiveness of the proposed method.<details>
<summary>Abstract</summary>
Orientation recognition and standardization play a crucial role in the effectiveness of medical image processing tasks. Deep learning-based methods have proven highly advantageous in orientation recognition and prediction tasks. In this paper, we address the challenge of imaging orientation in cardiac MRI and present a method that employs deep neural networks to categorize and standardize the orientation. To cater to multiple sequences and modalities of MRI, we propose a transfer learning strategy, enabling adaptation of our model from a single modality to diverse modalities. We conducted comprehensive experiments on CMR images from various modalities, including bSSFP, T2, and LGE. The validation accuracies achieved were 100.0\%, 100.0\%, and 99.4\%, confirming the robustness and effectiveness of our model. Our source code and network models are available at https://github.com/rxzhen/MSCMR-orient
</details>
<details>
<summary>摘要</summary>
医疗影像处理任务中的方向识别和标准化扮演着关键性的角色。基于深度学习的方法在方向识别和预测任务中表现出了高度的优势。本文描述了在卡丁MRI中的影像方向识别挑战，并提出了使用深度神经网络来分类和标准化影像方向的方法。为了适应不同的序列和模式，我们提议了传输学习策略，使得我们的模型能够从单一的模式中适应多种模式。我们在不同的CMR图像模式（包括bSSFP、T2和LGE）上进行了广泛的实验，并得到了100.0%、100.0%和99.4%的验证精度，这证明了我们的模型的稳定性和有效性。我们的源代码和网络模型可以在https://github.com/rxzhen/MSCMR-orient中下载。
</details></li>
</ul>
<hr>
<h2 id="An-objective-validation-of-polyp-and-instrument-segmentation-methods-in-colonoscopy-through-Medico-2020-polyp-segmentation-and-MedAI-2021-transparency-challenges"><a href="#An-objective-validation-of-polyp-and-instrument-segmentation-methods-in-colonoscopy-through-Medico-2020-polyp-segmentation-and-MedAI-2021-transparency-challenges" class="headerlink" title="An objective validation of polyp and instrument segmentation methods in colonoscopy through Medico 2020 polyp segmentation and MedAI 2021 transparency challenges"></a>An objective validation of polyp and instrument segmentation methods in colonoscopy through Medico 2020 polyp segmentation and MedAI 2021 transparency challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16262">http://arxiv.org/abs/2307.16262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/georgebatch/kvasir-seg">https://github.com/georgebatch/kvasir-seg</a></li>
<li>paper_authors: Debesh Jha, Vanshali Sharma, Debapriya Banik, Debayan Bhattacharya, Kaushiki Roy, Steven A. Hicks, Nikhil Kumar Tomar, Vajira Thambawita, Adrian Krenzer, Ge-Peng Ji, Sahadev Poudel, George Batchkala, Saruar Alam, Awadelrahman M. A. Ahmed, Quoc-Huy Trinh, Zeshan Khan, Tien-Phat Nguyen, Shruti Shrestha, Sabari Nathan, Jeonghwan Gwak, Ritika K. Jha, Zheyuan Zhang, Alexander Schlaefer, Debotosh Bhattacharjee, M. K. Bhuyan, Pradip K. Das, Sravanthi Parsa, Sharib Ali, Michael A. Riegler, Pål Halvorsen, Ulas Bagci, Thomas De Lange</li>
<li>for: 这个研究旨在探讨自动分析colonoscopy影像，以提高早期癌前肿瘤的检测率。</li>
<li>methods: 这个研究使用了深度学习算法，以协助镜头专门医生在现场检查中检测和分类潜在的肿瘤和异常。</li>
<li>results: 研究发现，使用深度学习算法可以提高肿瘤检测率，并且可以提供可读的和可理解的解释，以便在临床应用中使用。<details>
<summary>Abstract</summary>
Automatic analysis of colonoscopy images has been an active field of research motivated by the importance of early detection of precancerous polyps. However, detecting polyps during the live examination can be challenging due to various factors such as variation of skills and experience among the endoscopists, lack of attentiveness, and fatigue leading to a high polyp miss-rate. Deep learning has emerged as a promising solution to this challenge as it can assist endoscopists in detecting and classifying overlooked polyps and abnormalities in real time. In addition to the algorithm's accuracy, transparency and interpretability are crucial to explaining the whys and hows of the algorithm's prediction. Further, most algorithms are developed in private data, closed source, or proprietary software, and methods lack reproducibility. Therefore, to promote the development of efficient and transparent methods, we have organized the "Medico automatic polyp segmentation (Medico 2020)" and "MedAI: Transparency in Medical Image Segmentation (MedAI 2021)" competitions. We present a comprehensive summary and analyze each contribution, highlight the strength of the best-performing methods, and discuss the possibility of clinical translations of such methods into the clinic. For the transparency task, a multi-disciplinary team, including expert gastroenterologists, accessed each submission and evaluated the team based on open-source practices, failure case analysis, ablation studies, usability and understandability of evaluations to gain a deeper understanding of the models' credibility for clinical deployment. Through the comprehensive analysis of the challenge, we not only highlight the advancements in polyp and surgical instrument segmentation but also encourage qualitative evaluation for building more transparent and understandable AI-based colonoscopy systems.
</details>
<details>
<summary>摘要</summary>
自动分析幽门摄影像是一个活跃的研究领域，受到早期检测前期肿瘤的重要性启发。然而，在实时检查中检测肿瘤可以是困难的，因为幽门摄影医生的技能和经验异常，精力不足和疲劳等多种因素导致高检测肿瘤率。深度学习已经成为一种有希望的解决方案，它可以帮助幽门摄影医生在实时检查中检测和分类检测到的肿瘤和异常。此外，算法的准确率以外，透明度和解释性也是非常重要的，以解释算法的预测原因。然而，大多数算法是在私有数据、关闭源代码或商业软件上开发的，方法缺乏可重复性。因此，为促进效率和透明度的方法的发展，我们组织了“医疗自动肿瘤分割（Medico 2020）”和“MedAI：医疗图像分割透明度（MedAI 2021）”比赛。我们对每个提交进行了全面的摘要和分析，推出了最佳方法的优势，并讨论了这些方法在临床应用中的可能性。在透明度任务中，一个多学科团队，包括专业的肠胃内科医生，对每个提交进行了评估，以评估团队的开源实践、失败案例分析、割除研究、可用性和理解度来深入了解模型的可靠性。通过全面分析挑战，我们不仅披露了肿瘤和手术工具分割领域的进步，也鼓励了更多的透明度和理解性基于医疗图像系统的AI技术的开发。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/31/eess.IV_2023_07_31/" data-id="cloh7tqok012m7b88hbsmf8d6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/30/cs.SD_2023_07_30/" class="article-date">
  <time datetime="2023-07-30T15:00:00.000Z" itemprop="datePublished">2023-07-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/30/cs.SD_2023_07_30/">cs.SD - 2023-07-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="HierVST-Hierarchical-Adaptive-Zero-shot-Voice-Style-Transfer"><a href="#HierVST-Hierarchical-Adaptive-Zero-shot-Voice-Style-Transfer" class="headerlink" title="HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer"></a>HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16171">http://arxiv.org/abs/2307.16171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sang-Hoon Lee, Ha-Yeong Choi, Hyung-Seok Oh, Seong-Whan Lee</li>
<li>for: 这篇论文主要是为了解决零shotvoice style transfer（VST）系统中对新speaker的voice style转换能力不足的问题。</li>
<li>methods: 本文提出了一种层次适应的终端到终端零shot VST模型，只使用了语音数据进行训练，不需要文本脚本。模型使用了层次变量推论和自主学习表示，同时采用了层次适应生成器，生成抑制表达和波形声音序列。</li>
<li>results: 实验结果表明，我们的方法在零shot VST场景中比其他VST模型表现更好。Audioamples可以在 \url{<a target="_blank" rel="noopener" href="https://hiervst.github.io/%7D">https://hiervst.github.io/}</a> 上 obtaint。<details>
<summary>Abstract</summary>
Despite rapid progress in the voice style transfer (VST) field, recent zero-shot VST systems still lack the ability to transfer the voice style of a novel speaker. In this paper, we present HierVST, a hierarchical adaptive end-to-end zero-shot VST model. Without any text transcripts, we only use the speech dataset to train the model by utilizing hierarchical variational inference and self-supervised representation. In addition, we adopt a hierarchical adaptive generator that generates the pitch representation and waveform audio sequentially. Moreover, we utilize unconditional generation to improve the speaker-relative acoustic capacity in the acoustic representation. With a hierarchical adaptive structure, the model can adapt to a novel voice style and convert speech progressively. The experimental results demonstrate that our method outperforms other VST models in zero-shot VST scenarios. Audio samples are available at \url{https://hiervst.github.io/}.
</details>
<details>
<summary>摘要</summary>
尽管voice style transfer（VST）领域的进步 rapid, recent zero-shot VST system 仍然缺乏能够传递新说者的voice style的能力。在这篇论文中，我们提出了一种层次适应式 zero-shot VST 模型，即 HierVST。无需任何文本脚本，我们只使用语音数据来训练模型，通过层次变量推断和自我supervised representation。此外，我们采用层次适应生成器，生成抽象音频序列和声音表示。此外，我们利用无条件生成技术，提高speaker-relative acoustic capacity。通过层次适应结构，模型可以逐步适应新的voice style，并将语音转换为新的语音风格。实验结果表明，我们的方法在zero-shot VST场景下表现出色，超过了其他VST模型。听音amples可以在 \url{https://hiervst.github.io/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="IroyinSpeech-A-multi-purpose-Yoruba-Speech-Corpus"><a href="#IroyinSpeech-A-multi-purpose-Yoruba-Speech-Corpus" class="headerlink" title="ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus"></a>ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16071">http://arxiv.org/abs/2307.16071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, David Ifeoluwa Adelani</li>
<li>for: 这个论文是为了提高现代 йоруба语言的高质量自由数据而创建的 dataset。</li>
<li>methods: 这个 dataset 使用了新闻和创作域的文本句子，并且由多个说话者录音每个句子。</li>
<li>results: 这个 dataset 总共包含 38.5 小时的数据，由 80 名志愿者录音。<details>
<summary>Abstract</summary>
We introduce the \`{I}r\`{o}y\`{i}nSpeech corpus -- a new dataset influenced by a desire to increase the amount of high quality, freely available, contemporary Yor\`{u}b\'{a} speech. We release a multi-purpose dataset that can be used for both TTS and ASR tasks. We curated text sentences from the news and creative writing domains under an open license i.e., CC-BY-4.0 and had multiple speakers record each sentence. We provide 5000 of our utterances to the Common Voice platform to crowdsource transcriptions online. The dataset has 38.5 hours of data in total, recorded by 80 volunteers.
</details>
<details>
<summary>摘要</summary>
我们介绍《尼罗言语 corpus》——一个新的数据集，受到了提高高质量、自由可用、当代尼罗语言的需求的影响。我们发布了多用途的数据集，可以用于 TTS 和 ASR 任务。我们从新闻和创意写作领域中选取了 CC-BY-4.0 开源许可证下的文本句子，并由多名说话者录制每句句子。我们向 Common Voice 平台提供了5000个语音utterance，以便在线受众投票转录。总共有38.5小时的数据，由80名志愿者录制。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/30/cs.SD_2023_07_30/" data-id="cloh7tqkz00sp7b8874a2dm1l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/30/cs.CV_2023_07_30/" class="article-date">
  <time datetime="2023-07-30T13:00:00.000Z" itemprop="datePublished">2023-07-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/30/cs.CV_2023_07_30/">cs.CV - 2023-07-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="3D-Medical-Image-Segmentation-with-Sparse-Annotation-via-Cross-Teaching-between-3D-and-2D-Networks"><a href="#3D-Medical-Image-Segmentation-with-Sparse-Annotation-via-Cross-Teaching-between-3D-and-2D-Networks" class="headerlink" title="3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks"></a>3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16256">http://arxiv.org/abs/2307.16256</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hengcai-nju/3d2dct">https://github.com/hengcai-nju/3d2dct</a></li>
<li>paper_authors: Heng Cai, Lei Qi, Qian Yu, Yinghuan Shi, Yang Gao</li>
<li>for: 这篇研究旨在提高医疗影像分类中的准确率，减少annotations的需求。</li>
<li>methods: 我们提出了一个基于cross-teaching的框架，可以从简少的标注中学习。我们还开发了两种pseudo label选择策略，协助提高模型的准确率。</li>
<li>results: 我们的方法在MMWHS dataset上实验显示，较进一步于现有的半Supervised分类方法。更进一步，我们的方法可以与完全Supervised方法的最佳结果相匹配。<details>
<summary>Abstract</summary>
Medical image segmentation typically necessitates a large and precisely annotated dataset. However, obtaining pixel-wise annotation is a labor-intensive task that requires significant effort from domain experts, making it challenging to obtain in practical clinical scenarios. In such situations, reducing the amount of annotation required is a more practical approach. One feasible direction is sparse annotation, which involves annotating only a few slices, and has several advantages over traditional weak annotation methods such as bounding boxes and scribbles, as it preserves exact boundaries. However, learning from sparse annotation is challenging due to the scarcity of supervision signals. To address this issue, we propose a framework that can robustly learn from sparse annotation using the cross-teaching of both 3D and 2D networks. Considering the characteristic of these networks, we develop two pseudo label selection strategies, which are hard-soft confidence threshold and consistent label fusion. Our experimental results on the MMWHS dataset demonstrate that our method outperforms the state-of-the-art (SOTA) semi-supervised segmentation methods. Moreover, our approach achieves results that are comparable to the fully-supervised upper bound result.
</details>
<details>
<summary>摘要</summary>
医学图像分割通常需要大量精确标注数据。然而，获取像素级标注是一项劳动密集的任务，需要培训领域专家投入大量时间和精力，在实践医疗场景中很困难。在这种情况下，减少标注量是一个更实际的方法。一种可行的方向是稀疏标注，即只标注一些剖面，具有许多优势，比如精确边界保持。然而，从稀疏标注学习是一项挑战，因为缺乏监督信号。为解决这个问题，我们提出了一种可靠地从稀疏标注学习的框架，基于3D和2D网络的交叉教学。由于这些网络的特点，我们开发了两种pseudo标签选择策略：固定-软信号阈值和一致性标签合并。我们在MMWHS数据集上进行实验，得到的结果表明，我们的方法在比较 semi-supervised segmentation 方法的State-of-the-art（SOTA）之上升起了。此外，我们的方法可以与完全监督Upper bound（UB）的结果相比。
</details></li>
</ul>
<hr>
<h2 id="Count-Decode-and-Fetch-A-New-Approach-to-Handwritten-Chinese-Character-Error-Correction"><a href="#Count-Decode-and-Fetch-A-New-Approach-to-Handwritten-Chinese-Character-Error-Correction" class="headerlink" title="Count, Decode and Fetch: A New Approach to Handwritten Chinese Character Error Correction"></a>Count, Decode and Fetch: A New Approach to Handwritten Chinese Character Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16253">http://arxiv.org/abs/2307.16253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Hu, Jiefeng Ma, Zhenrong Zhang, Jun Du, Jianshu Zhang</li>
<li>for: 提高手写中文字识别率</li>
<li>methods: 使用计数器、解码器和搜索器</li>
<li>results: 提高手写中文字识别率，可以更好地识别未看过的错误字符<details>
<summary>Abstract</summary>
Recently, handwritten Chinese character error correction has been greatly improved by employing encoder-decoder methods to decompose a Chinese character into an ideographic description sequence (IDS). However, existing methods implicitly capture and encode linguistic information inherent in IDS sequences, leading to a tendency to generate IDS sequences that match seen characters. This poses a challenge when dealing with an unseen misspelled character, as the decoder may generate an IDS sequence that matches a seen character instead. Therefore, we introduce Count, Decode and Fetch (CDF), a novel approach that exhibits better generalization towards unseen misspelled characters. CDF is mainly composed of three parts: the counter, the decoder, and the fetcher. In the first stage, the counter predicts the number of each radical class without the symbol-level position annotations. In the second stage, the decoder employs the counting information and generates the IDS sequence step by step. Moreover, by updating the counting information at each time step, the decoder becomes aware of the existence of each radical. With the decomposed IDS sequence, we can determine whether the given character is misspelled. If it is misspelled, the fetcher under the transductive transfer learning strategy predicts the ideal character that the user originally intended to write. We integrate our method into existing encoder-decoder models and significantly enhance their performance.
</details>
<details>
<summary>摘要</summary>
In the first stage, the counter predicts the number of each radical class without the symbol-level position annotations. In the second stage, the decoder employs the counting information and generates the IDS sequence step by step. Moreover, by updating the counting information at each time step, the decoder becomes aware of the existence of each radical. With the decomposed IDS sequence, we can determine whether the given character is misspelled. If it is misspelled, the fetcher under the transductive transfer learning strategy predicts the ideal character that the user originally intended to write. We integrate our method into existing encoder-decoder models and significantly enhance their performance.
</details></li>
</ul>
<hr>
<h2 id="SR-R-2-KAC-Improving-Single-Image-Defocus-Deblurring"><a href="#SR-R-2-KAC-Improving-Single-Image-Defocus-Deblurring" class="headerlink" title="SR-R$^2$KAC: Improving Single Image Defocus Deblurring"></a>SR-R$^2$KAC: Improving Single Image Defocus Deblurring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16242">http://arxiv.org/abs/2307.16242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Tang, Zhiqiang Xu, Pengfei Wei, Xiaobin Hu, Peilin Zhao, Xin Cao, Chunlai Zhou, Tobias Lasser<br>for:The paper proposes an efficient deep learning method for single image defocus deblurring (SIDD) to address the issue of large blurs.methods:The proposed method, called R$^2$KAC, builds on the inverse kernel properties and uses a combination of kernel-sharing atrous convolutions and recursive atrous convolutions to simulate a large inverse kernel. The method also includes identity shortcuts to alleviate ringing artifacts and a scale recurrent module to exploit multi-scale information.results:The proposed method achieves the state-of-the-art performance on SIDD tasks, outperforming other existing methods.<details>
<summary>Abstract</summary>
We propose an efficient deep learning method for single image defocus deblurring (SIDD) by further exploring inverse kernel properties. Although the current inverse kernel method, i.e., kernel-sharing parallel atrous convolution (KPAC), can address spatially varying defocus blurs, it has difficulty in handling large blurs of this kind. To tackle this issue, we propose a Residual and Recursive Kernel-sharing Atrous Convolution (R$^2$KAC). R$^2$KAC builds on a significant observation of inverse kernels, that is, successive use of inverse-kernel-based deconvolutions with fixed size helps remove unexpected large blurs but produces ringing artifacts. Specifically, on top of kernel-sharing atrous convolutions used to simulate multi-scale inverse kernels, R$^2$KAC applies atrous convolutions recursively to simulate a large inverse kernel. Specifically, on top of kernel-sharing atrous convolutions, R$^2$KAC stacks atrous convolutions recursively to simulate a large inverse kernel. To further alleviate the contingent effect of recursive stacking, i.e., ringing artifacts, we add identity shortcuts between atrous convolutions to simulate residual deconvolutions. Lastly, a scale recurrent module is embedded in the R$^2$KAC network, leading to SR-R$^2$KAC, so that multi-scale information from coarse to fine is exploited to progressively remove the spatially varying defocus blurs. Extensive experimental results show that our method achieves the state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
我们提出了一种高效的深度学习方法，用于单张图像杂斑去振荡（SIDD），通过进一步探索 inverse kernel 性质。现有的 inverse kernel 方法，即 kernel-sharing parallel atrous convolution（KPAC），可以处理空间变化的杂斑干扰，但在处理大范围杂斑时存在困难。为解决这问题，我们提出了 Residual and Recursive Kernel-sharing Atrous Convolution（R$^2$KAC）。R$^2$KAC 基于 inverse kernel 的重要观察，即 successive use of inverse-kernel-based deconvolutions with fixed size 可以消除意外大杂斑，但会生成环形artefacts。在 kernel-sharing atrous convolutions 上进一步堆叠 atrous convolutions，R$^2$KAC 可以模拟大型 inverse kernel。为了进一步减少堆叠效应的影响，我们添加了 identity shortcuts between atrous convolutions，以便模拟 residual deconvolutions。最后，我们在 R$^2$KAC 网络中添加了 scale recurrent module，导致 SR-R$^2$KAC，以便利用 multi-scale information from coarse to fine 来逐步除去空间变化的杂斑干扰。我们的方法在实验中达到了状态盘的性能。
</details></li>
</ul>
<hr>
<h2 id="InfoStyler-Disentanglement-Information-Bottleneck-for-Artistic-Style-Transfer"><a href="#InfoStyler-Disentanglement-Information-Bottleneck-for-Artistic-Style-Transfer" class="headerlink" title="InfoStyler: Disentanglement Information Bottleneck for Artistic Style Transfer"></a>InfoStyler: Disentanglement Information Bottleneck for Artistic Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16227">http://arxiv.org/abs/2307.16227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueming Lyu, Yue Jiang, Bo Peng, Jing Dong</li>
<li>for: 这篇论文的目的是提出一种新的信息离散学习方法，以实现高质量的艺术风格转移。</li>
<li>methods: 该方法基于一种名为InfoStyler的新信息离散学习方法，该方法通过从预训练编码网络中捕捉最小充分的信息来捕捉内容特征和风格特征的分离。</li>
<li>results: 对比于传统的转移模块方法，InfoStyler可以更好地保持内容结构的稳定性，同时也可以增加风格特征的多样性。实验证明，InfoStyler可以生成高质量的风格转移图像。<details>
<summary>Abstract</summary>
Artistic style transfer aims to transfer the style of an artwork to a photograph while maintaining its original overall content. Many prior works focus on designing various transfer modules to transfer the style statistics to the content image. Although effective, ignoring the clear disentanglement of the content features and the style features from the first beginning, they have difficulty in balancing between content preservation and style transferring. To tackle this problem, we propose a novel information disentanglement method, named InfoStyler, to capture the minimal sufficient information for both content and style representations from the pre-trained encoding network. InfoStyler formulates the disentanglement representation learning as an information compression problem by eliminating style statistics from the content image and removing the content structure from the style image. Besides, to further facilitate disentanglement learning, a cross-domain Information Bottleneck (IB) learning strategy is proposed by reconstructing the content and style domains. Extensive experiments demonstrate that our InfoStyler can synthesize high-quality stylized images while balancing content structure preservation and style pattern richness.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traslate the given text into Simplified Chinese.<</SYS>>艺术风格转移目标是将艺术作品的风格转移到照片中，保持原始内容的整体结构。许多先前的工作都是设计多种转移模块，以将风格统计传输到内容图像中。虽然有效，但忽略了初始阶段分离内容特征和风格特征的清晰分离，因此困难保持内容保持和风格传输的平衡。为解决这个问题，我们提出了一种新的信息分解方法，即InfoStyler，以捕捉预训练编码网络中最少够的信息来表示内容和风格表示。InfoStyler将分解表示学习定义为信息压缩问题，从内容图像中除去风格统计，并从风格图像中除去内容结构。此外，为进一步促进分解学习，我们提出了跨域信息瓶颈（IB）学习策略，通过重建内容和风格域来进行跨域信息瓶颈学习。广泛的实验表明，我们的InfoStyler可以生成高质量风格化图像，同时保持内容结构和风格特征的平衡。
</details></li>
</ul>
<hr>
<h2 id="ScribbleVC-Scribble-supervised-Medical-Image-Segmentation-with-Vision-Class-Embedding"><a href="#ScribbleVC-Scribble-supervised-Medical-Image-Segmentation-with-Vision-Class-Embedding" class="headerlink" title="ScribbleVC: Scribble-supervised Medical Image Segmentation with Vision-Class Embedding"></a>ScribbleVC: Scribble-supervised Medical Image Segmentation with Vision-Class Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16226">http://arxiv.org/abs/2307.16226</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huanglizi/scribblevc">https://github.com/huanglizi/scribblevc</a></li>
<li>paper_authors: Zihan Li, Yuan Zheng, Xiangde Luo, Dandan Shan, Qingqi Hong</li>
<li>for: 这个研究旨在提高医疗影像分类的精度和效率，以便于诊断、治疗规划和病情监控。</li>
<li>methods: 本研究提出了一个名为ScribbleVC的新框架，它利用了视觉和分类嵌入，并通过多 modal 资讯增强机制来提高视觉特征提取。此外，ScribbleVC 将 CNN 特征和 transformer 特征 uniformly 利用，以获得更好的视觉特征提取。</li>
<li>results: 我们在三个 benchmark 数据集上评估了ScribbleVC，并与现有的方法进行比较。结果显示，我们的方法在精度、Robustness 和效率三方面都超过了现有的方法。<details>
<summary>Abstract</summary>
Medical image segmentation plays a critical role in clinical decision-making, treatment planning, and disease monitoring. However, accurate segmentation of medical images is challenging due to several factors, such as the lack of high-quality annotation, imaging noise, and anatomical differences across patients. In addition, there is still a considerable gap in performance between the existing label-efficient methods and fully-supervised methods. To address the above challenges, we propose ScribbleVC, a novel framework for scribble-supervised medical image segmentation that leverages vision and class embeddings via the multimodal information enhancement mechanism. In addition, ScribbleVC uniformly utilizes the CNN features and Transformer features to achieve better visual feature extraction. The proposed method combines a scribble-based approach with a segmentation network and a class-embedding module to produce accurate segmentation masks. We evaluate ScribbleVC on three benchmark datasets and compare it with state-of-the-art methods. The experimental results demonstrate that our method outperforms existing approaches in terms of accuracy, robustness, and efficiency. The datasets and code are released on GitHub.
</details>
<details>
<summary>摘要</summary>
医疗影像分割在临床决策、治疗规划和疾病监测中扮演着关键的角色。然而，准确地分割医疗影像受到多种因素的限制，包括高质量注释缺乏、成像噪声和患者间解剖学差异。此外，现有的标签效率方法和全标签方法之间还存在显著的性能差距。为了解决以上挑战，我们提出了ScribbleVC，一种基于scribble的医疗影像分割框架。此外，ScribbleVC通过多Modal信息增强机制来利用视觉和类嵌入。我们还 uniformmente利用CNN特征和Transformer特征来提取更好的视觉特征。我们的方法将scribble-based Approach与分割网络和类嵌入模块结合，以生成准确的分割mask。我们在三个标准数据集上测试了我们的方法，并与现有方法进行比较。实验结果表明，我们的方法在准确性、稳定性和效率方面都超过了现有的方法。我们在GitHub上发布了数据集和代码。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Decomposition-Networks-for-Bias-Field-Correction-in-MR-Image"><a href="#Unsupervised-Decomposition-Networks-for-Bias-Field-Correction-in-MR-Image" class="headerlink" title="Unsupervised Decomposition Networks for Bias Field Correction in MR Image"></a>Unsupervised Decomposition Networks for Bias Field Correction in MR Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16219">http://arxiv.org/abs/2307.16219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leongdong/bias-decomposition-networks">https://github.com/leongdong/bias-decomposition-networks</a></li>
<li>paper_authors: Dong Liang, Xingyu Qiu, Kuanquan Wang, Gongning Luo, Wei Wang, Yashu Liu</li>
<li>For: The paper aims to propose a novel unsupervised decomposition network to correct bias fields in magnetic resonance (MR) images, which are degraded by intensity inhomogeneity caused by imperfect MR devices or imaged objects.* Methods: The proposed method consists of a segmentation part and an estimation part, which are optimized alternately. The segmentation part predicts the probability of every pixel belonging to each class, while the estimation part calculates the bias field. The loss functions used in the method are based on the combination of fuzzy clustering and the multiplicative bias field, which introduce smoothness of the bias field and construct soft relationships among different classes under intra-consistency constraints.* Results: The proposed method can accurately estimate bias fields and produce better bias correction results, as demonstrated by extensive experiments. The code for the proposed method is available on the link: <a target="_blank" rel="noopener" href="https://github.com/LeongDong/Bias-Decomposition-Networks">https://github.com/LeongDong/Bias-Decomposition-Networks</a>.<details>
<summary>Abstract</summary>
Bias field, which is caused by imperfect MR devices or imaged objects, introduces intensity inhomogeneity into MR images and degrades the performance of MR image analysis methods. Many retrospective algorithms were developed to facilitate the bias correction, to which the deep learning-based methods outperformed. However, in the training phase, the supervised deep learning-based methods heavily rely on the synthesized bias field. As the formation of the bias field is extremely complex, it is difficult to mimic the true physical property of MR images by synthesized data. While bias field correction and image segmentation are strongly related, the segmentation map is precisely obtained by decoupling the bias field from the original MR image, and the bias value is indicated by the segmentation map in reverse. Thus, we proposed novel unsupervised decomposition networks that are trained only with biased data to obtain the bias-free MR images. Networks are made up of: a segmentation part to predict the probability of every pixel belonging to each class, and an estimation part to calculate the bias field, which are optimized alternately. Furthermore, loss functions based on the combination of fuzzy clustering and the multiplicative bias field are also devised. The proposed loss functions introduce the smoothness of bias field and construct the soft relationships among different classes under intra-consistency constraints. Extensive experiments demonstrate that the proposed method can accurately estimate bias fields and produce better bias correction results. The code is available on the link: https://github.com/LeongDong/Bias-Decomposition-Networks.
</details>
<details>
<summary>摘要</summary>
��ubble field, which is caused by imperfect MR devices or imaged objects, introduces intensity inhomogeneity into MR images and degrades the performance of MR image analysis methods. Many retrospective algorithms were developed to facilitate the bias correction, to which the deep learning-based methods outperformed. However, in the training phase, the supervised deep learning-based methods heavily rely on the synthesized bias field. As the formation of the bias field is extremely complex, it is difficult to mimic the true physical property of MR images by synthesized data. While bias field correction and image segmentation are strongly related, the segmentation map is precisely obtained by decoupling the bias field from the original MR image, and the bias value is indicated by the segmentation map in reverse. Thus, we proposed novel unsupervised decomposition networks that are trained only with biased data to obtain the bias-free MR images. Networks are made up of: a segmentation part to predict the probability of every pixel belonging to each class, and an estimation part to calculate the bias field, which are optimized alternately. Furthermore, loss functions based on the combination of fuzzy clustering and the multiplicative bias field are also devised. The proposed loss functions introduce the smoothness of bias field and construct the soft relationships among different classes under intra-consistency constraints. Extensive experiments demonstrate that the proposed method can accurately estimate bias fields and produce better bias correction results. Code available at: https://github.com/LeongDong/Bias-Decomposition-Networks.
</details></li>
</ul>
<hr>
<h2 id="Mesh-Density-Adaptation-for-Template-based-Shape-Reconstruction"><a href="#Mesh-Density-Adaptation-for-Template-based-Shape-Reconstruction" class="headerlink" title="Mesh Density Adaptation for Template-based Shape Reconstruction"></a>Mesh Density Adaptation for Template-based Shape Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16205">http://arxiv.org/abs/2307.16205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ycjungsubhuman/density-adaptation">https://github.com/ycjungsubhuman/density-adaptation</a></li>
<li>paper_authors: Yucheol Jung, Hyomin Kim, Gyeongha Hwang, Seung-Hwan Baek, Seungyong Lee</li>
<li>for: 3D shape reconstruction based on template mesh deformation</li>
<li>methods: 使用规则化方法（如平滑能量）引导重建向度，并提出了一种 mesh 密度适应方法来解决 mesh 缺失问题</li>
<li>results: 比对无 mesh 密度适应方法和 mesh 密度适应方法的重建结果，结果显示 mesh 密度适应方法能够提高重建精度。<details>
<summary>Abstract</summary>
In 3D shape reconstruction based on template mesh deformation, a regularization, such as smoothness energy, is employed to guide the reconstruction into a desirable direction. In this paper, we highlight an often overlooked property in the regularization: the vertex density in the mesh. Without careful control on the density, the reconstruction may suffer from under-sampling of vertices near shape details. We propose a novel mesh density adaptation method to resolve the under-sampling problem. Our mesh density adaptation energy increases the density of vertices near complex structures via deformation to help reconstruction of shape details. We demonstrate the usability and performance of mesh density adaptation with two tasks, inverse rendering and non-rigid surface registration. Our method produces more accurate reconstruction results compared to the cases without mesh density adaptation.
</details>
<details>
<summary>摘要</summary>
在基于模板网格塑形的3D形状重建中，常用一种正则化方法，如平滑能量，以导向重建向 Desirable 方向。在这篇文章中，我们强调了常被忽略的属性在正则化中：网格中的顶点密度。如果不当控制密度，则重建可能会受到形状细节附近的顶点下折。我们提议一种新的网格密度适应方法，以解决这个问题。我们的网格密度适应能量通过塑形来增加顶点密度，以便重建形状细节。我们示出了使用这种方法的可用性和性能，通过对 inverse rendering 和非刚体表面 региSTR 进行比较。我们的方法可以提供更高精度的重建结果，比不使用网格密度适应的情况更好。
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Domain-Adaptation-with-Visual-Language-Foundation-Models"><a href="#Open-Set-Domain-Adaptation-with-Visual-Language-Foundation-Models" class="headerlink" title="Open-Set Domain Adaptation with Visual-Language Foundation Models"></a>Open-Set Domain Adaptation with Visual-Language Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16204">http://arxiv.org/abs/2307.16204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qing Yu, Go Irie, Kiyoharu Aizawa</li>
<li>for: 这个研究旨在应用开放集成领域数据预测（ODA）来识别目标领域中的未知类别，并使用最新的语言图像基础模型（VLFM）来解决这个问题。</li>
<li>methods: 本研究使用了一种称为“语言图像对应预测（CLIP）”的新型语言图像基础模型，并将其应用到ODA中。CLIP可以对多种分布Shift进行适应，因此可以帮助ODA模型更好地处理目标领域中的未知类别。</li>
<li>results: 本研究的结果显示，使用CLIP可以实现State-of-the-art的ODA效果，并且可以帮助ODA模型更好地预测目标领域中的未知类别。<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) has proven to be very effective in transferring knowledge obtained from a source domain with labeled data to a target domain with unlabeled data. Owing to the lack of labeled data in the target domain and the possible presence of unknown classes, open-set domain adaptation (ODA) has emerged as a potential solution to identify these classes during the training phase. Although existing ODA approaches aim to solve the distribution shifts between the source and target domains, most methods fine-tuned ImageNet pre-trained models on the source domain with the adaptation on the target domain. Recent visual-language foundation models (VLFM), such as Contrastive Language-Image Pre-Training (CLIP), are robust to many distribution shifts and, therefore, should substantially improve the performance of ODA. In this work, we explore generic ways to adopt CLIP, a popular VLFM, for ODA. We investigate the performance of zero-shot prediction using CLIP, and then propose an entropy optimization strategy to assist the ODA models with the outputs of CLIP. The proposed approach achieves state-of-the-art results on various benchmarks, demonstrating its effectiveness in addressing the ODA problem.
</details>
<details>
<summary>摘要</summary>
Unsupervised domain adaptation (UDA) 已经证明可以很有效地将源频道上带有标注数据的知识传递给目标频道上无标注数据。由于目标频道上可能存在未知的类别，开放集频道适应（ODA）已经出现为解决这个问题的可能性。虽然现有的 ODA 方法主要是通过修改源频道上的 ImageNet 预训练模型来解决源和目标频道之间的分布差异，但是这些方法通常是在源频道上进行适应。在这种情况下，我们提出了一种适应 CLIP，一种流行的视觉语言基础模型，的方法。我们 investigate CLIP 的零shot 预测性能，然后提出了一种信息归一化策略来帮助 ODA 模型使用 CLIP 的输出。我们的方法实现了多个 benchmark 上的状态实际结果，证明了它在 ODA 问题中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Convolutional-Neural-Networks-with-Zero-Padding-Feature-Extraction-and-Learning"><a href="#Deep-Convolutional-Neural-Networks-with-Zero-Padding-Feature-Extraction-and-Learning" class="headerlink" title="Deep Convolutional Neural Networks with Zero-Padding: Feature Extraction and Learning"></a>Deep Convolutional Neural Networks with Zero-Padding: Feature Extraction and Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16203">http://arxiv.org/abs/2307.16203</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liubc17/eDCNN_zero_padding">https://github.com/liubc17/eDCNN_zero_padding</a></li>
<li>paper_authors: Zhi Han, Baichen Liu, Shao-Bo Lin, Ding-Xuan Zhou</li>
<li>for: 这个论文研究了深度卷积神经网络（DCNN）中零填充的性能。</li>
<li>methods: 论文首先验证了零填充在特征提取和学习中的角色，并证明了它们在翻译相对性方面发挥作用。然后，论文表明了任何深度全连接神经网络（DFCN）都可以通过DCNN来表示，这表明DCNN在特征提取方面比DFCN更好。</li>
<li>results: 论文derives了DCNN零填充的通用一致性和学习过程中的翻译不变性。这些结论都被数字实验验证，包括举例和实际数据运行。<details>
<summary>Abstract</summary>
This paper studies the performance of deep convolutional neural networks (DCNNs) with zero-padding in feature extraction and learning. After verifying the roles of zero-padding in enabling translation-equivalence, and pooling in its translation-invariance driven nature, we show that with similar number of free parameters, any deep fully connected networks (DFCNs) can be represented by DCNNs with zero-padding. This demonstrates that DCNNs with zero-padding is essentially better than DFCNs in feature extraction. Consequently, we derive universal consistency of DCNNs with zero-padding and show its translation-invariance in the learning process. All our theoretical results are verified by numerical experiments including both toy simulations and real-data running.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Gastrointestinal-Mucosal-Problems-Classification-with-Deep-Learning"><a href="#Gastrointestinal-Mucosal-Problems-Classification-with-Deep-Learning" class="headerlink" title="Gastrointestinal Mucosal Problems Classification with Deep Learning"></a>Gastrointestinal Mucosal Problems Classification with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16198">http://arxiv.org/abs/2307.16198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadhasan Goharian, Vahid Goharian, Hamidreza Bolhasani</li>
<li>for:  Early diagnosis of gastrointestinal mucosal changes to prevent cancers and provide early treatment.</li>
<li>methods:  Deep learning, specifically Transfer Learning (TL) based on Convolutional Neural Networks (CNNs), was used to predict 8 classes of mucosal changes and anatomical landmarks from endoscopy images.</li>
<li>results:  The best model achieved 93% accuracy in test images and was applied to real endoscopy and colonoscopy movies to classify problems.<details>
<summary>Abstract</summary>
Gastrointestinal mucosal changes can cause cancers after some years and early diagnosing them can be very useful to prevent cancers and early treatment. In this article, 8 classes of mucosal changes and anatomical landmarks including Polyps, Ulcerative Colitis, Esophagitis, Normal Z-Line, Normal Pylorus, Normal Cecum, Dyed Lifted Polyps, and Dyed Lifted Margin were predicted by deep learning. We used neural networks in this article. It is a black box artificial intelligence algorithm that works like a human neural system. In this article, Transfer Learning (TL) based on the Convolutional Neural Networks (CNNs), which is one of the well-known types of neural networks in image processing is used. We compared some famous CNN architecture including VGG, Inception, Xception, and ResNet. Our best model got 93% accuracy in test images. At last, we used our model in some real endoscopy and colonoscopy movies to classify problems.
</details>
<details>
<summary>摘要</summary>
胃肠内膜变化可能导致癌症，早期诊断可以非常有用，以预防癌症和早期治疗。在这篇文章中，我们预测了8种胃肠内膜变化和解剖学特征，包括膜腺肿（Polyps）、慢性结肠炎（Ulcerative Colitis）、食管炎（Esophagitis）、正常Z-线、正常胃隔（Normal Pylorus）、正常肠隔（Normal Cecum）、染料提取后的膜腺肿（Dyed Lifted Polyps）和染料提取后的边缘（Dyed Lifted Margin）。我们使用了神经网络（Neural Networks）来实现这一点。我们使用了传输学习（Transfer Learning），基于卷积神经网络（Convolutional Neural Networks，CNNs），这是图像处理领域的一种非常知名的神经网络类型。我们比较了一些著名的 CNN 架构，包括 VGG、Inception、Xception 和 ResNet。我们的最佳模型在测试图像中达到了93%的准确率。最后，我们使用了我们的模型在一些真实的窥镜和colonoscopy视频中分类问题。
</details></li>
</ul>
<hr>
<h2 id="Unified-Model-for-Image-Video-Audio-and-Language-Tasks"><a href="#Unified-Model-for-Image-Video-Audio-and-Language-Tasks" class="headerlink" title="Unified Model for Image, Video, Audio and Language Tasks"></a>Unified Model for Image, Video, Audio and Language Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16184">http://arxiv.org/abs/2307.16184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mshukor/unival">https://github.com/mshukor/unival</a></li>
<li>paper_authors: Mustafa Shukor, Corentin Dancette, Alexandre Rame, Matthieu Cord</li>
<li>for: 这篇论文的目的是建立一个可以支持多Modalities的大型语言模型（Unified Model），以解决现有的多任务多Modalities问题。</li>
<li>methods: 该论文提出了一种基于任务均衡和多Modalities课程学习的方法，以有效地把多种任务和模式合并到一个模型中。</li>
<li>results: 该模型在多个图像和视频文本任务上显示了竞争力的性能，并且可以在不同的模式上进行权值 interpolate 以提高特点表示能力。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches, across image and video-text tasks. The feature representations learned from image and video-text modalities, allows the model to achieve competitive performance when finetuned on audio-text tasks, despite not being pretrained on audio. Thanks to the unified model, we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks, showing their benefits in particular for out-of-distribution generalization. Finally, we motivate unification by showing the synergy between tasks. The model weights and code are released here: https://github.com/mshukor/UnIVAL.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经让普通的通用代理人变得非常接近现实。一个关键的障碍是任务和模式的多样性和不同性。一个有前途的解决方案是统一，允许支持多种任务和模式的一个统一框架。虽然一些大型模型（例如Flamingo（Alayrac等，2022））已经在庞大数据集上训练，可以支持更多于两种模式，但目前的小至中型统一模型仍然只能支持两种模式，通常是图像文本或视频文本。我们提出的问题是：是否可以效率地建立一个统一模型，可以支持所有模式？为此，我们提出了UnIVAL，这是一步进一步的目标。不需要庞大的数据集或多亿 Parameters的模型，UnIVAL模型只有约0.25B Parameters，可以超越两种模式，并将文本、图像、视频和音频 integrate 到一个模型中。我们的模型通过多任务均衡和多模式学习来快速预训练，并在图像和视频文本任务上达到竞争性性能。通过图像和视频文本模式学习的特征表示，我们的模型可以在没有 direct 预训练的情况下，在音频文本任务上达到竞争性性能。此外，我们还提出了一种基于多模式学习的模型混合方法，通过将不同多模式任务训练的模型权重进行 interpolate 来提高对异常数据的泛化性能。最后，我们鼓励统一，因为任务之间存在相互作用的关系。UnIVAL模型和代码可以在以下链接获取：https://github.com/mshukor/UnIVAL。
</details></li>
</ul>
<hr>
<h2 id="HD-Fusion-Detailed-Text-to-3D-Generation-Leveraging-Multiple-Noise-Estimation"><a href="#HD-Fusion-Detailed-Text-to-3D-Generation-Leveraging-Multiple-Noise-Estimation" class="headerlink" title="HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation"></a>HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16183">http://arxiv.org/abs/2307.16183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinbo Wu, Xiaobo Gao, Xing Liu, Zhengyang Shen, Chen Zhao, Haocheng Feng, Jingtuo Liu, Errui Ding</li>
<li>for: 本研究旨在利用文本到3D内容生成，通过2D扩散先验来提高生成的3D模型质量和细节。</li>
<li>methods: 本研究提出了一种新的方法，即结合多个噪声估计过程和预训练2D扩散先验，以实现高分辨率渲染。与Bar-Tal等人的研究不同，我们的方法包括计算分配散射损失（SDS损失和VSD损失），这些损失是关键的技术 для3D内容生成。</li>
<li>results: 我们对提出的方法进行实验评估，结果显示， compared to基eline，我们的方法可以生成高质量的细节。<details>
<summary>Abstract</summary>
In this paper, we study Text-to-3D content generation leveraging 2D diffusion priors to enhance the quality and detail of the generated 3D models. Recent progress (Magic3D) in text-to-3D has shown that employing high-resolution (e.g., 512 x 512) renderings can lead to the production of high-quality 3D models using latent diffusion priors. To enable rendering at even higher resolutions, which has the potential to further augment the quality and detail of the models, we propose a novel approach that combines multiple noise estimation processes with a pretrained 2D diffusion prior. Distinct from the Bar-Tal et al.s' study which binds multiple denoised results to generate images from texts, our approach integrates the computation of scoring distillation losses such as SDS loss and VSD loss which are essential techniques for the 3D content generation with 2D diffusion priors. We experimentally evaluated the proposed approach. The results show that the proposed approach can generate high-quality details compared to the baselines.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了基于2D扩散先验的文本到3D内容生成，以提高生成的3D模型质量和细节。最近的Magic3D进展表明，使用高分辨率（例如512x512）渲染可以生成高质量3D模型使用潜在扩散先验。为了启用更高的分辨率渲染，我们提出了一种新的方法，即将多个雷达抑制过程与预训练的2D扩散先验结合。与巴尔-塔尔等人的研究不同，我们的方法将多个抑制后的结果绑定生成图像，而不是将多个抑制后的结果绑定生成图像。我们实际进行了实验，结果显示，我们的方法可以生成高质量细节，相比于基eline。
</details></li>
</ul>
<hr>
<h2 id="Fusing-VHR-Post-disaster-Aerial-Imagery-and-LiDAR-Data-for-Roof-Classification-in-the-Caribbean"><a href="#Fusing-VHR-Post-disaster-Aerial-Imagery-and-LiDAR-Data-for-Roof-Classification-in-the-Caribbean" class="headerlink" title="Fusing VHR Post-disaster Aerial Imagery and LiDAR Data for Roof Classification in the Caribbean"></a>Fusing VHR Post-disaster Aerial Imagery and LiDAR Data for Roof Classification in the Caribbean</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16177">http://arxiv.org/abs/2307.16177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GFDRR/caribbean-rooftop-classification">https://github.com/GFDRR/caribbean-rooftop-classification</a></li>
<li>paper_authors: Isabelle Tingzon, Nuala Margaret Cowan, Pierre Chrzanowski</li>
<li>for: 帮助政府更快地生成 Building Information，以提高区域风险管理和灾害应急应对。</li>
<li>methods: 利用深度学习技术自动分类 Very High-Resolution orthophotos 和空中 LiDAR 数据，以获取 Dominica ollowing Hurricane Maria 的建筑特征。</li>
<li>results: 我们的方法可以达到 F1 分数 0.93 和 0.92  для 屋顶类别和材料类别的自动分类，并且融合多 modal 地球观测数据可以达到更高的准确率。<details>
<summary>Abstract</summary>
Accurate and up-to-date information on building characteristics is essential for vulnerability assessment; however, the high costs and long timeframes associated with conducting traditional field surveys can be an obstacle to obtaining critical exposure datasets needed for disaster risk management. In this work, we leverage deep learning techniques for the automated classification of roof characteristics from very high-resolution orthophotos and airborne LiDAR data obtained in Dominica following Hurricane Maria in 2017. We demonstrate that the fusion of multimodal earth observation data performs better than using any single data source alone. Using our proposed methods, we achieve F1 scores of 0.93 and 0.92 for roof type and roof material classification, respectively. This work is intended to help governments produce more timely building information to improve resilience and disaster response in the Caribbean.
</details>
<details>
<summary>摘要</summary>
准确和最新的建筑特征信息是质量评估中的关键因素，但传统的场地调查过程可能会带来高昂的成本和长时间的投入。在这种情况下，我们利用深度学习技术自动分类飞地照片和飞行 LiDAR 数据中的屋顶特征。我们发现多模态地球观测数据的融合性能比单一数据源 alone 更高。我们的提议方法可以达到 F1 分数为 0.93 和 0.92 的精度，用于屋顶类别和材料类别的自动分类。这项工作的目的是帮助政府生成更加准确的建筑信息，以提高加勒比地区的韧性和灾害应急应对。
</details></li>
</ul>
<hr>
<h2 id="InvVis-Large-Scale-Data-Embedding-for-Invertible-Visualization"><a href="#InvVis-Large-Scale-Data-Embedding-for-Invertible-Visualization" class="headerlink" title="InvVis: Large-Scale Data Embedding for Invertible Visualization"></a>InvVis: Large-Scale Data Embedding for Invertible Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16176">http://arxiv.org/abs/2307.16176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huayuan Ye, Chenhui Li, Yang Li, Changbo Wang</li>
<li>for: 这个研究是为了实现可逆性的视觉化，即将资料嵌入到视觉图像中，并且能够复原或修改该图像。</li>
<li>methods: 这个方法使用专门的对应网络来实现高品质的资料嵌入和显示，并且提出了一种新的方法来快速将柱 chart 资料转换为图像形式，以便将大量的资料嵌入到视觉图像中。</li>
<li>results: 实验结果显示， InvVis 可以实现高品质的资料嵌入和显示，并且可以将大量的资料嵌入到视觉图像中，而且可以复原或修改该图像。<details>
<summary>Abstract</summary>
We present InvVis, a new approach for invertible visualization, which is reconstructing or further modifying a visualization from an image. InvVis allows the embedding of a significant amount of data, such as chart data, chart information, source code, etc., into visualization images. The encoded image is perceptually indistinguishable from the original one. We propose a new method to efficiently express chart data in the form of images, enabling large-capacity data embedding. We also outline a model based on the invertible neural network to achieve high-quality data concealing and revealing. We explore and implement a variety of application scenarios of InvVis. Additionally, we conduct a series of evaluation experiments to assess our method from multiple perspectives, including data embedding quality, data restoration accuracy, data encoding capacity, etc. The result of our experiments demonstrates the great potential of InvVis in invertible visualization.
</details>
<details>
<summary>摘要</summary>
我团队现在提出了一种新的方法，即InvVis，它可以将图像中的数据重构或进一步修改为图像。InvVis可以将大量数据，如图表数据、图表信息、源代码等，嵌入到图像中。编码后的图像与原始图像看不出差异。我们提出了一种新的方法，可以高效地将图表数据转换为图像形式，以实现大容量数据嵌入。我们还 outline了一种基于倒排神经网络的模型，以实现高质量的数据隐藏和恢复。我们在多个应用场景下进行了详细的探索和实现，并进行了多个评估实验，包括数据嵌入质量、数据恢复精度、数据编码容量等方面。实验结果表明，InvVis在 revertible visualization 方面具有很大的潜力。
</details></li>
</ul>
<hr>
<h2 id="StarSRGAN-Improving-Real-World-Blind-Super-Resolution"><a href="#StarSRGAN-Improving-Real-World-Blind-Super-Resolution" class="headerlink" title="StarSRGAN: Improving Real-World Blind Super-Resolution"></a>StarSRGAN: Improving Real-World Blind Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16169">http://arxiv.org/abs/2307.16169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kynthesis/StarSRGAN">https://github.com/kynthesis/StarSRGAN</a></li>
<li>paper_authors: Khoa D. Vo, Len T. Bui</li>
<li>for: 提高图像分辨率 ohne prior knowledge of degradation process</li>
<li>methods: 使用5种不同的建筑物，包括StarSRGAN模型</li>
<li>results: 提供新的SOTA性能，在MANLIQA和AHIQ测试中比Real-ESRGAN提高约10%，并提供了实时SR体验。<details>
<summary>Abstract</summary>
The aim of blind super-resolution (SR) in computer vision is to improve the resolution of an image without prior knowledge of the degradation process that caused the image to be low-resolution. The State of the Art (SOTA) model Real-ESRGAN has advanced perceptual loss and produced visually compelling outcomes using more complex degradation models to simulate real-world degradations. However, there is still room to improve the super-resolved quality of Real-ESRGAN by implementing recent techniques. This research paper introduces StarSRGAN, a novel GAN model designed for blind super-resolution tasks that utilize 5 various architectures. Our model provides new SOTA performance with roughly 10% better on the MANIQA and AHIQ measures, as demonstrated by experimental comparisons with Real-ESRGAN. In addition, as a compact version, StarSRGAN Lite provides approximately 7.5 times faster reconstruction speed (real-time upsampling from 540p to 4K) but can still keep nearly 90% of image quality, thereby facilitating the development of a real-time SR experience for future research. Our codes are released at https://github.com/kynthesis/StarSRGAN.
</details>
<details>
<summary>摘要</summary>
目标是提高计算机视觉中的盲超分辨率（SR）图像质量，无需知情减震过程所导致的图像低分辨率。现状的模型Real-ESRGAN已经提出了感知损失，并且生成了视觉吸引人的结果，使用更复杂的减震模型来模拟实际世界中的减震。然而，还有空间可以提高Real-ESRGAN中的超分辨率质量。这个研究论文介绍了StarSRGAN，一种新的GAN模型，用于盲超分辨率任务。我们的模型使用了5种不同的建筑，并提供了新的状态之artefact（SOTA）性能，在MANINQA和AHIQ测试中比Real-ESRGAN提高约10%。此外，StarSRGAN Lite为实时� upsampling提供了约7.5倍 faster的重建速度（从540p到4K），但可以保持 Nearly 90%的图像质量，因此可以促进实时SR经验的发展。我们的代码在https://github.com/kynthesis/StarSRGAN上发布。
</details></li>
</ul>
<hr>
<h2 id="Motion-Degeneracy-in-Self-supervised-Learning-of-Elevation-Angle-Estimation-for-2D-Forward-Looking-Sonar"><a href="#Motion-Degeneracy-in-Self-supervised-Learning-of-Elevation-Angle-Estimation-for-2D-Forward-Looking-Sonar" class="headerlink" title="Motion Degeneracy in Self-supervised Learning of Elevation Angle Estimation for 2D Forward-Looking Sonar"></a>Motion Degeneracy in Self-supervised Learning of Elevation Angle Estimation for 2D Forward-Looking Sonar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16160">http://arxiv.org/abs/2307.16160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusheng Wang, Yonghoon Ji, Chujie Wu, Hiroshi Tsuchiya, Hajime Asama, Atsushi Yamashita</li>
<li>for: 本研究旨在实现无需预训练的自主学习方法，以估计SONAR图像中缺失的高程信息。</li>
<li>methods: 该方法利用现代学习框架，通过分析SONAR图像中的运动场，证明可以在无需synthetic数据预训练的情况下，通过自主学习方式进行高程估计。</li>
<li>results: 实验和实际应用 validate了提议的方法，并且显示了稳定的自主学习性。<details>
<summary>Abstract</summary>
2D forward-looking sonar is a crucial sensor for underwater robotic perception. A well-known problem in this field is estimating missing information in the elevation direction during sonar imaging. There are demands to estimate 3D information per image for 3D mapping and robot navigation during fly-through missions. Recent learning-based methods have demonstrated their strengths, but there are still drawbacks. Supervised learning methods have achieved high-quality results but may require further efforts to acquire 3D ground-truth labels. The existing self-supervised method requires pretraining using synthetic images with 3D supervision. This study aims to realize stable self-supervised learning of elevation angle estimation without pretraining using synthetic images. Failures during self-supervised learning may be caused by motion degeneracy problems. We first analyze the motion field of 2D forward-looking sonar, which is related to the main supervision signal. We utilize a modern learning framework and prove that if the training dataset is built with effective motions, the network can be trained in a self-supervised manner without the knowledge of synthetic data. Both simulation and real experiments validate the proposed method.
</details>
<details>
<summary>摘要</summary>
2D前Looking陀螺是水下机器人视觉中的重要感知传感器。一个著名的问题在这个领域是在陀螺成像过程中缺失高度方向的信息。有需求将每幅图像中的信息提取到3D格式下，以便进行3D地图生成和机器人导航 durante fly-through任务。现有的学习基于方法已经表现出其优势，但还有一些缺点。监督学习方法可以获得高质量的结果，但可能需要进一步的努力来获得3D的实际 labels。现有的无监督方法需要使用 synthetic 图像进行预训练。本研究旨在实现不需要预训练使用 synthetic 图像的稳定无监督学习高度角度估计。在自主学习过程中出现失败可能是因为运动缺乏问题。我们首先分析了2D前Looking陀螺的运动场，与主要监督信号相关。我们利用现代学习框架，并证明如果训练集建立有效的运动，则网络可以在无监督的情况下在自主学习模式下训练，不需要Synthetic 数据的知识。 both simulation和实际实验 validate 我们的方法。
</details></li>
</ul>
<hr>
<h2 id="StylePrompter-All-Styles-Need-Is-Attention"><a href="#StylePrompter-All-Styles-Need-Is-Attention" class="headerlink" title="StylePrompter: All Styles Need Is Attention"></a>StylePrompter: All Styles Need Is Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16151">http://arxiv.org/abs/2307.16151</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/i2-multimedia-lab/styleprompter">https://github.com/i2-multimedia-lab/styleprompter</a></li>
<li>paper_authors: Chenyi Zhuang, Pan Gao, Aljosa Smolic</li>
<li>for: 这个论文的目标是使用Generative Adversarial Networks（GAN）进行图像归一化，特别是StyleGAN，以获得分离的 latent space，并在该空间进行特征 Editing。</li>
<li>methods: 这个论文使用了一种转移到Token level的幂等视觉Transformer底层，以及一种Style-driven Multi-scale Adaptive Refinement Transformer（SMART）来修改生成器的中间风格特征。SMART可以从encoder的特征图中检索丢失的标识信息，并且可以高质量地恢复图像。</li>
<li>results: 实验表明，StylePrompter可以在重建质量和可编辑性之间做出平衡，并且可以”聪明”地适应大多数编辑任务，超过其他 $\mathcal{F}$-参与的恢复方法。<details>
<summary>Abstract</summary>
GAN inversion aims at inverting given images into corresponding latent codes for Generative Adversarial Networks (GANs), especially StyleGAN where exists a disentangled latent space that allows attribute-based image manipulation at latent level. As most inversion methods build upon Convolutional Neural Networks (CNNs), we transfer a hierarchical vision Transformer backbone innovatively to predict $\mathcal{W^+}$ latent codes at token level. We further apply a Style-driven Multi-scale Adaptive Refinement Transformer (SMART) in $\mathcal{F}$ space to refine the intermediate style features of the generator. By treating style features as queries to retrieve lost identity information from the encoder's feature maps, SMART can not only produce high-quality inverted images but also surprisingly adapt to editing tasks. We then prove that StylePrompter lies in a more disentangled $\mathcal{W^+}$ and show the controllability of SMART. Finally, quantitative and qualitative experiments demonstrate that StylePrompter can achieve desirable performance in balancing reconstruction quality and editability, and is "smart" enough to fit into most edits, outperforming other $\mathcal{F}$-involved inversion methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Video-Frame-Interpolation-with-Flow-Transformer"><a href="#Video-Frame-Interpolation-with-Flow-Transformer" class="headerlink" title="Video Frame Interpolation with Flow Transformer"></a>Video Frame Interpolation with Flow Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16144">http://arxiv.org/abs/2307.16144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pan Gao, Haoyue Tian, Jie Qin</li>
<li>for: 本文是为了提出一种基于Transformer的视频帧 interpolator，以提高视频 interpolating的Visual quality。</li>
<li>methods: 本文使用了一种叫做Flow Transformer Block的方法，通过计算 temporal self-attention在匹配的Local area中来使用动态学习来捕捉大动作的动态信息。此外，文章还提出了一种多尺度架构来考虑多尺度的动作信息。</li>
<li>results: 实验结果表明，提出的方法可以在三个标准测试集上生成比state-of-the-art方法更高质量的 interpolated frames。<details>
<summary>Abstract</summary>
Video frame interpolation has been actively studied with the development of convolutional neural networks. However, due to the intrinsic limitations of kernel weight sharing in convolution, the interpolated frame generated by it may lose details. In contrast, the attention mechanism in Transformer can better distinguish the contribution of each pixel, and it can also capture long-range pixel dependencies, which provides great potential for video interpolation. Nevertheless, the original Transformer is commonly used for 2D images; how to develop a Transformer-based framework with consideration of temporal self-attention for video frame interpolation remains an open issue. In this paper, we propose Video Frame Interpolation Flow Transformer to incorporate motion dynamics from optical flows into the self-attention mechanism. Specifically, we design a Flow Transformer Block that calculates the temporal self-attention in a matched local area with the guidance of flow, making our framework suitable for interpolating frames with large motion while maintaining reasonably low complexity. In addition, we construct a multi-scale architecture to account for multi-scale motion, further improving the overall performance. Extensive experiments on three benchmarks demonstrate that the proposed method can generate interpolated frames with better visual quality than state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>视频帧 interpolate 已经广泛研究，发展 convolutional neural networks 时，但是由于内在的核心权重共享限制，生成的 interpolated 帧可能会产生细节损失。相比之下，Transformer 中的注意机制可以更好地识别每个像素的贡献，同时也可以捕捉长距离像素相关性，这提供了大量的潜在能量 для 视频 interpolate。然而，原始 Transformer 通常用于 2D 图像；如何开发一个包含 temporal 自注意的 Transformer 框架，以便用于视频帧 interpolate 仍是一个开放的问题。在这篇论文中，我们提出了 Video Frame Interpolation Flow Transformer，即在 optical flows 的指导下，在本地匹配区域内进行 temporal 自注意计算的 Flow Transformer Block。这使得我们的框架适用于大跑动的 interpolated 帧，同时保持reasonably low complexity。此外，我们还构建了多尺度架构，以account for multi-scale motion，进一步提高总性能。经过对三个标准测试集的广泛实验，我们发现提出的方法可以生成比state-of-the-art 方法更高质量的 interpolated 帧。
</details></li>
</ul>
<hr>
<h2 id="Structure-Preserving-Synthesis-MaskGAN-for-Unpaired-MR-CT-Translation"><a href="#Structure-Preserving-Synthesis-MaskGAN-for-Unpaired-MR-CT-Translation" class="headerlink" title="Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation"></a>Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16143">http://arxiv.org/abs/2307.16143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HieuPhan33/MaskGAN">https://github.com/HieuPhan33/MaskGAN</a></li>
<li>paper_authors: Minh Hieu Phan, Zhibin Liao, Johan W. Verjans, Minh-Son To</li>
<li>for: 这篇 paper 的目的是提出一个新的、cost-effective的数据类型转换模型，以便在医疗影像处理中处理不同modalities之间的数据类型转换。</li>
<li>methods: 这篇 paper 使用了 CycleGAN 方法，并将其与一个辅助的插值网络（mask generator）结合，以便强制运作网络对应于不同modalities之间的数据类型转换。</li>
<li>results: 实验结果显示，这篇 paper 的方法（MaskGAN）在一个儿童dataset上表现出色，能够对不同modalities之间的数据类型转换进行高精度的Synthesis，并且不需要专业的标注。<details>
<summary>Abstract</summary>
Medical image synthesis is a challenging task due to the scarcity of paired data. Several methods have applied CycleGAN to leverage unpaired data, but they often generate inaccurate mappings that shift the anatomy. This problem is further exacerbated when the images from the source and target modalities are heavily misaligned. Recently, current methods have aimed to address this issue by incorporating a supplementary segmentation network. Unfortunately, this strategy requires costly and time-consuming pixel-level annotations. To overcome this problem, this paper proposes MaskGAN, a novel and cost-effective framework that enforces structural consistency by utilizing automatically extracted coarse masks. Our approach employs a mask generator to outline anatomical structures and a content generator to synthesize CT contents that align with these structures. Extensive experiments demonstrate that MaskGAN outperforms state-of-the-art synthesis methods on a challenging pediatric dataset, where MR and CT scans are heavily misaligned due to rapid growth in children. Specifically, MaskGAN excels in preserving anatomical structures without the need for expert annotations. The code for this paper can be found at https://github.com/HieuPhan33/MaskGAN.
</details>
<details>
<summary>摘要</summary>
医学图像生成是一项具有挑战性的任务，主要因为缺乏匹配数据。许多方法使用 CycleGAN 利用不匹配数据，但它们经常生成不准确的映射，导致图像坐标shift。这个问题在图像来源和目标模式之间的差异极大时更加严重。在这种情况下，当前的方法通常采用了补充性分割网络。然而，这种策略需要成本高昂和时间费时的像素级注释。为了解决这个问题，本文提出了 MaskGAN，一种新的和成本效果的框架。我们的方法使用一个掩蔽生成器来 outline 生物结构，并使用一个内容生成器来Synthesize CT 内容，以适应这些结构。我们进行了广泛的实验，并证明了 MaskGAN 在一个具有挑战性的pediatric dataset上表现出色，特别是在 MR 和 CT 扫描图像之间存在巨大的差异时。具体来说，MaskGAN 能够保持生物结构，不需要专家注释。相关代码可以在 GitHub 上找到：https://github.com/HieuPhan33/MaskGAN。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Neural-Representation-in-Medical-Imaging-A-Comparative-Survey"><a href="#Implicit-Neural-Representation-in-Medical-Imaging-A-Comparative-Survey" class="headerlink" title="Implicit Neural Representation in Medical Imaging: A Comparative Survey"></a>Implicit Neural Representation in Medical Imaging: A Comparative Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16142">http://arxiv.org/abs/2307.16142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mindflow-institue/awesome-implicit-neural-representations-in-medical-imaging">https://github.com/mindflow-institue/awesome-implicit-neural-representations-in-medical-imaging</a></li>
<li>paper_authors: Amirali Molaei, Amirhossein Aminimehr, Armin Tavakoli, Amirhossein Kazerouni, Bobby Azad, Reza Azad, Dorit Merhof</li>
<li>for: 这篇论文旨在为医疗图像分析领域提供了一个全面的评论，探讨了隐藏神经表示（INR）在医疗图像分析中的应用。</li>
<li>methods: 这篇论文使用了隐藏神经网络来parameterize数据，并 explore了INR在医疗图像分析中的各种应用，如图像重建、分割、注册、新视图生成和压缩。</li>
<li>results: 论文总结了INR在医疗图像分析中的优点和限制，包括其能够解决多个难题、高效、可靠、可调和可迭代性等特点。同时，论文 также提出了将来的研究方向和机遇，如与多Modal imaging、实时交互系统和领域适应等。<details>
<summary>Abstract</summary>
Implicit neural representations (INRs) have gained prominence as a powerful paradigm in scene reconstruction and computer graphics, demonstrating remarkable results. By utilizing neural networks to parameterize data through implicit continuous functions, INRs offer several benefits. Recognizing the potential of INRs beyond these domains, this survey aims to provide a comprehensive overview of INR models in the field of medical imaging. In medical settings, numerous challenging and ill-posed problems exist, making INRs an attractive solution. The survey explores the application of INRs in various medical imaging tasks, such as image reconstruction, segmentation, registration, novel view synthesis, and compression. It discusses the advantages and limitations of INRs, highlighting their resolution-agnostic nature, memory efficiency, ability to avoid locality biases, and differentiability, enabling adaptation to different tasks. Furthermore, the survey addresses the challenges and considerations specific to medical imaging data, such as data availability, computational complexity, and dynamic clinical scene analysis. It also identifies future research directions and opportunities, including integration with multi-modal imaging, real-time and interactive systems, and domain adaptation for clinical decision support. To facilitate further exploration and implementation of INRs in medical image analysis, we have provided a compilation of cited studies along with their available open-source implementations on \href{https://github.com/mindflow-institue/Awesome-Implicit-Neural-Representations-in-Medical-imaging}. Finally, we aim to consistently incorporate the most recent and relevant papers regularly.
</details>
<details>
<summary>摘要</summary>
启发神经表示 (INR) 在场景重建和计算机图形领域已经得到了广泛的应用，并且表现出了惊人的成果。通过使用神经网络来参数化数据通过间接连续函数，INR 提供了多种优点。认识到 INR 在医疗领域的潜在应用，这篇评论旨在提供医疗影像领域中 INR 模型的全面概述。在医疗设置中，存在许多困难和不稳定的问题，使得 INR 成为一个吸引人的解决方案。评论探讨了 INR 在各种医疗影像任务中的应用，如图像重建、分割、注册、新视图生成和压缩。它讨论了 INR 的优点和局限性，包括其无关分辨率的性、内存有效性、避免地方偏见以及可微性，以及其在不同任务中的适应性。此外，评论还讨论了医疗影像数据特有的挑战和考虑因素，如数据可用性、计算复杂度和动态临床场景分析。最后，评论提出了未来研究方向和机会，包括与多模态成像集成、实时交互系统和领域适应性的研究。为便于进一步探索和应用 INR 在医疗影像分析中，我们在 \href{https://github.com/mindflow-institue/Awesome-Implicit-Neural-Representations-in-Medical-imaging} 提供了一份参考文献和其相应的开源实现。我们计划在 réguliére 基础上不断更新和补充最新和最相关的论文。
</details></li>
</ul>
<hr>
<h2 id="Augmented-Math-Authoring-AR-Based-Explorable-Explanations-by-Augmenting-Static-Math-Textbooks"><a href="#Augmented-Math-Authoring-AR-Based-Explorable-Explanations-by-Augmenting-Static-Math-Textbooks" class="headerlink" title="Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks"></a>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16112">http://arxiv.org/abs/2307.16112</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucalgary-ilab/augmented-math">https://github.com/ucalgary-ilab/augmented-math</a></li>
<li>paper_authors: Neil Chulpongsatorn, Mille Skovhus Lunding, Nishan Soni, Ryo Suzuki</li>
<li>for: 帮助非技术用户，如教师或学生，将静止数学书籍和手册转化为即时和个性化的探索解释。</li>
<li>methods: 我们的系统首先使用光学字符识别和计算机视觉EXTRACT数学公式和图片FROM given document，然后将这些EXTRACT的内容绑定和操作，让用户通过移动AR界面看到交互动画 overlay onto the document。</li>
<li>results: 我们的研究表明，我们的系统可以帮助学生更好地理解数学概念，并且允许非技术用户创建个性化的探索解释。<details>
<summary>Abstract</summary>
We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts.
</details>
<details>
<summary>摘要</summary>
我们介绍了增强数学（Augmented Math），一种机器学习基于的方法来创建不需要程式的扩展显示探索解释。我们的系统首先从给定的文档中提取数学公式和图片使用光学字符识别（OCR）和计算机视觉。接着，我们可以将这些提取的内容绑定和操作，让用户透过移动AR界面看到对文档的互动动画。这使得非技术用户，如教师或学生，可以将现有的数学文档和手册转换为即时和个性化的探索解释。为了设计我们的系统，我们首先分析了现有的探索 math解释，以发现通用的设计策略。根据发现的结果，我们发展了一系列可以自动生成的增强技巧，包括1）动态值、2）互动图像、3）关系显示、4）实物例子、5）步骤提示。为了评估我们的系统，我们进行了两项用户研究：初步用户测试和专家访谈。研究结果表明，我们的系统可以为学习数学概念提供更加有趣的体验。
</details></li>
</ul>
<hr>
<h2 id="TransFusion-A-Practical-and-Effective-Transformer-based-Diffusion-Model-for-3D-Human-Motion-Prediction"><a href="#TransFusion-A-Practical-and-Effective-Transformer-based-Diffusion-Model-for-3D-Human-Motion-Prediction" class="headerlink" title="TransFusion: A Practical and Effective Transformer-based Diffusion Model for 3D Human Motion Prediction"></a>TransFusion: A Practical and Effective Transformer-based Diffusion Model for 3D Human Motion Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16106">http://arxiv.org/abs/2307.16106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sibo Tian, Minghui Zheng, Xiao Liang</li>
<li>For: The paper is written for predicting human motion in intelligent remanufacturing systems, with a focus on ensuring safe and effective human-robot collaboration.* Methods: The paper proposes a diffusion-based model for 3D human motion prediction, which leverages Transformer as the backbone and employs the discrete cosine transform to model motion sequences in the frequency space.* Results: The paper reports extensive experimental studies on benchmark datasets to validate the effectiveness of the proposed human motion prediction model, with results showing that the model can generate samples that are more likely to happen while maintaining a certain level of diversity.Here’s the information in Simplified Chinese text:* For: 这篇论文是为了预测人体动作在智能再生产系统中，以确保人机合作的安全和效果。* Methods: 论文提出了一种基于扩散的3D人体动作预测模型，利用Transformer作为基础，并使用隔行 cosine transform来模型动作序列在频域中。* Results: 论文进行了广泛的实验研究，以验证提出的人体动作预测模型的有效性，结果表明模型可以生成更有可能性发生的样本，同时保持一定的多样性。<details>
<summary>Abstract</summary>
Predicting human motion plays a crucial role in ensuring a safe and effective human-robot close collaboration in intelligent remanufacturing systems of the future. Existing works can be categorized into two groups: those focusing on accuracy, predicting a single future motion, and those generating diverse predictions based on observations. The former group fails to address the uncertainty and multi-modal nature of human motion, while the latter group often produces motion sequences that deviate too far from the ground truth or become unrealistic within historical contexts. To tackle these issues, we propose TransFusion, an innovative and practical diffusion-based model for 3D human motion prediction which can generate samples that are more likely to happen while maintaining a certain level of diversity. Our model leverages Transformer as the backbone with long skip connections between shallow and deep layers. Additionally, we employ the discrete cosine transform to model motion sequences in the frequency space, thereby improving performance. In contrast to prior diffusion-based models that utilize extra modules like cross-attention and adaptive layer normalization to condition the prediction on past observed motion, we treat all inputs, including conditions, as tokens to create a more lightweight model compared to existing approaches. Extensive experimental studies are conducted on benchmark datasets to validate the effectiveness of our human motion prediction model.
</details>
<details>
<summary>摘要</summary>
预测人体运动在智能重组系统中发挥关键作用，以确保人机协作安全有效。现有的研究可以分为两类：一类是强调准确性，预测单个未来运动，另一类是生成基于观察的多种预测。前者忽视了人体运动的不确定性和多模性，后者经常生成的运动序列与真实值有很大偏差或在历史上不实际。为解决这些问题，我们提出了TransFusion，一种创新的扩散模型，可以生成更有可能性的3D人体运动预测样本，同时保持一定的多样性。我们的模型借鉴Transformer作为背景，并在浅深层之间设置长距离连接。此外，我们使用离散归一化变换来模型运动序列在频域中，从而提高性能。与之前的扩散模型不同，我们不需要额外的模块如交叉注意力和自适应层normalization来condition预测，而是将所有输入，包括条件，视为令素来创建更轻量级的模型。我们在标准测试集上进行了广泛的实验研究，以验证我们的人体运动预测模型的有效性。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Graph-Filtering-Network-for-3D-Human-Pose-Estimation"><a href="#Iterative-Graph-Filtering-Network-for-3D-Human-Pose-Estimation" class="headerlink" title="Iterative Graph Filtering Network for 3D Human Pose Estimation"></a>Iterative Graph Filtering Network for 3D Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16074">http://arxiv.org/abs/2307.16074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zaedulislam/gs-net">https://github.com/zaedulislam/gs-net</a></li>
<li>paper_authors: Zaedul Islam, A. Ben Hamza<br>for:* This paper is written for 3D human pose estimation, specifically using graph convolutional networks (GCNs) to capture the spatial relationships between joints and learn an efficient representation of the underlying pose.methods:* The proposed method uses an iterative graph filtering framework with Laplacian regularization, which is implemented using the Gauss-Seidel iterative method.* The model architecture includes weight and adjacency modulation, skip connection, and a pure convolutional block with layer normalization.results:* The proposed model achieves state-of-the-art performance on two standard benchmark datasets for 3D human pose estimation, outperforming a comprehensive set of strong baseline methods.* Ablation studies demonstrate that the skip connection and adjacency modulation contribute to the improved model performance.<details>
<summary>Abstract</summary>
Graph convolutional networks (GCNs) have proven to be an effective approach for 3D human pose estimation. By naturally modeling the skeleton structure of the human body as a graph, GCNs are able to capture the spatial relationships between joints and learn an efficient representation of the underlying pose. However, most GCN-based methods use a shared weight matrix, making it challenging to accurately capture the different and complex relationships between joints. In this paper, we introduce an iterative graph filtering framework for 3D human pose estimation, which aims to predict the 3D joint positions given a set of 2D joint locations in images. Our approach builds upon the idea of iteratively solving graph filtering with Laplacian regularization via the Gauss-Seidel iterative method. Motivated by this iterative solution, we design a Gauss-Seidel network (GS-Net) architecture, which makes use of weight and adjacency modulation, skip connection, and a pure convolutional block with layer normalization. Adjacency modulation facilitates the learning of edges that go beyond the inherent connections of body joints, resulting in an adjusted graph structure that reflects the human skeleton, while skip connections help maintain crucial information from the input layer's initial features as the network depth increases. We evaluate our proposed model on two standard benchmark datasets, and compare it with a comprehensive set of strong baseline methods for 3D human pose estimation. Our experimental results demonstrate that our approach outperforms the baseline methods on both datasets, achieving state-of-the-art performance. Furthermore, we conduct ablation studies to analyze the contributions of different components of our model architecture and show that the skip connection and adjacency modulation help improve the model performance.
</details>
<details>
<summary>摘要</summary>
格 Graf卷积网络（GCNs）已经证明是3D人姿估计中有效的方法。通过自然地视图人体骨架结构为图，GCNs可以捕捉人体 JOINTS 之间的空间关系，并学习高效的姿势表示。然而，大多数GCN-based方法使用共享权重矩阵，使得准确地捕捉 JOINTS 之间的不同和复杂的关系困难。在这篇论文中，我们介绍了一种迭代图 filtering 框架 для3D人姿估计，该框架的目标是根据给定的2D JOINTS 位置来预测3D JOINTS 位置。我们的方法基于迭代图 filtering  WITH Laplacian regularization via Gauss-Seidel iterative method。这种迭代解决方法的灵感导致我们设计了Gauss-Seidel网络（GS-Net）架构，该架构使用权重和连接调整、跳过连接、并使用纯 convolutional block with layer normalization。连接调整使得学习的边度超出人体骨架内的自然连接，从而生成一个调整后的图结构，反映人体骨架。跳过连接帮助保留输入层的初始特征信息，以适应网络深度的增加。我们对两个标准 benchmark dataset进行评估，并与一组强大的基eline方法进行比较。我们的实验结果表明，我们的方法在两个dataset上都超过基eline方法，实现状态机器人姿估计的最佳性。此外，我们进行了归 subtract 分析，并证明跳过连接和连接调整对模型性能的贡献。
</details></li>
</ul>
<hr>
<h2 id="HandMIM-Pose-Aware-Self-Supervised-Learning-for-3D-Hand-Mesh-Estimation"><a href="#HandMIM-Pose-Aware-Self-Supervised-Learning-for-3D-Hand-Mesh-Estimation" class="headerlink" title="HandMIM: Pose-Aware Self-Supervised Learning for 3D Hand Mesh Estimation"></a>HandMIM: Pose-Aware Self-Supervised Learning for 3D Hand Mesh Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16061">http://arxiv.org/abs/2307.16061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zuyan Liu, Gaojie Lin, Congyi Wang, Min Zheng, Feida Zhu</li>
<li>for: 这篇论文主要目标是提出一种基于Masked Image Modeling（MIM）的自监督预训练策略，用于优化3D手套绘制参数的推断。</li>
<li>methods: 该策略包括一种教师生Student模型，其中包含一个pseudo键点对齐模块，用于学习pose-awaresemantic类标签。对于patch tokens，我们采用了一种自体革新的方式，使得教师和学生网络之间进行自我批判。此外，我们还采用了多级表示学习，以更好地适应低级 regression 任务。</li>
<li>results: 我们的提出的方法，即HandMIM，在多种手套绘制任务上达到了优秀的表现，包括FreiHAND和HO3Dv2测试集。特别是，HandMIM在特殊优化的架构上进行了比较，并实现了6.29mm和8.00mm PA VPE 的最佳记录，从而成为3D手套绘制领域的新状态之一。<details>
<summary>Abstract</summary>
With an enormous number of hand images generated over time, unleashing pose knowledge from unlabeled images for supervised hand mesh estimation is an emerging yet challenging topic. To alleviate this issue, semi-supervised and self-supervised approaches have been proposed, but they are limited by the reliance on detection models or conventional ResNet backbones. In this paper, inspired by the rapid progress of Masked Image Modeling (MIM) in visual classification tasks, we propose a novel self-supervised pre-training strategy for regressing 3D hand mesh parameters. Our approach involves a unified and multi-granularity strategy that includes a pseudo keypoint alignment module in the teacher-student framework for learning pose-aware semantic class tokens. For patch tokens with detailed locality, we adopt a self-distillation manner between teacher and student network based on MIM pre-training. To better fit low-level regression tasks, we incorporate pixel reconstruction tasks for multi-level representation learning. Additionally, we design a strong pose estimation baseline using a simple vanilla vision Transformer (ViT) as the backbone and attach a PyMAF head after tokens for regression. Extensive experiments demonstrate that our proposed approach, named HandMIM, achieves strong performance on various hand mesh estimation tasks. Notably, HandMIM outperforms specially optimized architectures, achieving 6.29mm and 8.00mm PAVPE (Vertex-Point-Error) on challenging FreiHAND and HO3Dv2 test sets, respectively, establishing new state-of-the-art records on 3D hand mesh estimation.
</details>
<details>
<summary>摘要</summary>
WITH 一 enormous number of hand images generated over time, unleashing pose knowledge from unlabeled images for supervised hand mesh estimation is an emerging yet challenging topic. To alleviate this issue, semi-supervised and self-supervised approaches have been proposed, but they are limited by the reliance on detection models or conventional ResNet backbones. In this paper, inspired by the rapid progress of Masked Image Modeling (MIM) in visual classification tasks, we propose a novel self-supervised pre-training strategy for regressing 3D hand mesh parameters. Our approach involves a unified and multi-granularity strategy that includes a pseudo keypoint alignment module in the teacher-student framework for learning pose-aware semantic class tokens. For patch tokens with detailed locality, we adopt a self-distillation manner between teacher and student network based on MIM pre-training. To better fit low-level regression tasks, we incorporate pixel reconstruction tasks for multi-level representation learning. Additionally, we design a strong pose estimation baseline using a simple vanilla vision Transformer (ViT) as the backbone and attach a PyMAF head after tokens for regression. Extensive experiments demonstrate that our proposed approach, named HandMIM, achieves strong performance on various hand mesh estimation tasks. Notably, HandMIM outperforms specially optimized architectures, achieving 6.29mm and 8.00mm PAVPE (Vertex-Point-Error) on challenging FreiHAND and HO3Dv2 test sets, respectively, establishing new state-of-the-art records on 3D hand mesh estimation.
</details></li>
</ul>
<hr>
<h2 id="CoVid-19-Detection-leveraging-Vision-Transformers-and-Explainable-AI"><a href="#CoVid-19-Detection-leveraging-Vision-Transformers-and-Explainable-AI" class="headerlink" title="CoVid-19 Detection leveraging Vision Transformers and Explainable AI"></a>CoVid-19 Detection leveraging Vision Transformers and Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16033">http://arxiv.org/abs/2307.16033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pangoth Santhosh Kumar, Kundrapu Supriya, Mallikharjuna Rao K</li>
<li>for: 这个论文的目的是为了检测肺病，以提高患者的健康和生活质量。</li>
<li>methods: 这篇论文使用了深度学习算法和图像处理技术来实现自动化、快速和准确地检测肺病。</li>
<li>results: 研究发现，使用视transformer基础模型可以解决深度学习模型对不同图像方向的问题，并在检测肺病方面达到了更高的准确率。<details>
<summary>Abstract</summary>
Lung disease is a common health problem in many parts of the world. It is a significant risk to people health and quality of life all across the globe since it is responsible for five of the top thirty leading causes of death. Among them are COVID 19, pneumonia, and tuberculosis, to name just a few. It is critical to diagnose lung diseases in their early stages. Several different models including machine learning and image processing have been developed for this purpose. The earlier a condition is diagnosed, the better the patient chances of making a full recovery and surviving into the long term. Thanks to deep learning algorithms, there is significant promise for the autonomous, rapid, and accurate identification of lung diseases based on medical imaging. Several different deep learning strategies, including convolutional neural networks (CNN), vanilla neural networks, visual geometry group based networks (VGG), and capsule networks , are used for the goal of making lung disease forecasts. The standard CNN has a poor performance when dealing with rotated, tilted, or other aberrant picture orientations. As a result of this, within the scope of this study, we have suggested a vision transformer based approach end to end framework for the diagnosis of lung disorders. In the architecture, data augmentation, training of the suggested models, and evaluation of the models are all included. For the purpose of detecting lung diseases such as pneumonia, Covid 19, lung opacity, and others, a specialised Compact Convolution Transformers (CCT) model have been tested and evaluated on datasets such as the Covid 19 Radiography Database. The model has achieved a better accuracy for both its training and validation purposes on the Covid 19 Radiography Database.
</details>
<details>
<summary>摘要</summary>
肺病是全球许多地区的常见健康问题。它对人们健康和生活质量产生了重要的风险，负责全球前30名死亡原因中的5个。包括COVID-19、肺炎和结核病等在内，这些疾病对人们的健康造成了严重的威胁。因此，早期诊断肺病非常重要。为了达到这一目标，包括机器学习和图像处理在内的多种模型已经被开发出来。随着深度学习算法的出现，肺病的自动化、快速和准确诊断已经得到了广泛的应用。在这些研究中，我们提出了基于视transformer的综合方法，以便诊断肺病。在该方法中，包括数据增强、模型训练和模型评估等环节。为了检测肺病，如肺炎、COVID-19、肺抑制等，我们测试了一种专门的Compact Convolution Transformers（CCT）模型。该模型在Covid 19 Radiography Database上的训练和验证过程中表现出了更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="LOTUS-Learning-to-Optimize-Task-based-US-representations"><a href="#LOTUS-Learning-to-Optimize-Task-based-US-representations" class="headerlink" title="LOTUS: Learning to Optimize Task-based US representations"></a>LOTUS: Learning to Optimize Task-based US representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16021">http://arxiv.org/abs/2307.16021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yordanka Velikova, Mohammad Farid Azampour, Walter Simson, Vanessa Gonzalez Duque, Nassir Navab</li>
<li>for: 这篇论文的目的是提出一种新的方法来对超声影像进行分类，以提高诊断和监控的精度。</li>
<li>methods: 这篇论文使用了一种新的方法，即使用rayed-casting来模拟超声传播，并且通过对下游分类任务进行优化，以 learns optimize the parameters for generating physics-based ultrasound images。</li>
<li>results: 这篇论文的结果显示，使用这种方法可以实现高度的自动分类精度，并且可以同时进行实验和自动分类。 qualitative results also show that the proposed method can generate high-quality images with improved contrast and resolution.<details>
<summary>Abstract</summary>
Anatomical segmentation of organs in ultrasound images is essential to many clinical applications, particularly for diagnosis and monitoring. Existing deep neural networks require a large amount of labeled data for training in order to achieve clinically acceptable performance. Yet, in ultrasound, due to characteristic properties such as speckle and clutter, it is challenging to obtain accurate segmentation boundaries, and precise pixel-wise labeling of images is highly dependent on the expertise of physicians. In contrast, CT scans have higher resolution and improved contrast, easing organ identification. In this paper, we propose a novel approach for learning to optimize task-based ultra-sound image representations. Given annotated CT segmentation maps as a simulation medium, we model acoustic propagation through tissue via ray-casting to generate ultrasound training data. Our ultrasound simulator is fully differentiable and learns to optimize the parameters for generating physics-based ultrasound images guided by the downstream segmentation task. In addition, we train an image adaptation network between real and simulated images to achieve simultaneous image synthesis and automatic segmentation on US images in an end-to-end training setting. The proposed method is evaluated on aorta and vessel segmentation tasks and shows promising quantitative results. Furthermore, we also conduct qualitative results of optimized image representations on other organs.
</details>
<details>
<summary>摘要</summary>
医学应用中的器官隔segmentation在ultrasound图像中是非常重要的，特别是诊断和监测。现有的深度神经网络需要大量标注数据进行训练，以达到医学接受的性能。然而，在ultrasound中，由特征性质 such as speckle和响应而带来的困难，减少了获得准确的分割边界，并且精确地标注图像 pixels 是医生的专业技巧依赖。与CT扫描相比，ultrasound图像有更高的分辨率和更好的对比度，使器官识别更加容易。在这篇论文中，我们提出了一种新的方法，用于学习优化任务基于ultrasound图像表示。我们使用rayed-casting模拟了声波传播 через组织，从而生成了physics-based的ultrasound训练数据。我们的ultrasound模拟器是可微分的，可以学习参数，以便生成按下沟通任务的优化参数。此外，我们还训练了一种图像适应网络，用于同时实现图像合成和自动分割任务。我们的提posed方法在大动脉和血管分割任务中表现出了良好的量化结果。此外，我们还进行了其他器官的优化图像表示的质量研究。
</details></li>
</ul>
<hr>
<h2 id="Fuzzy-Logic-Visual-Network-FLVN-A-neuro-symbolic-approach-for-visual-features-matching"><a href="#Fuzzy-Logic-Visual-Network-FLVN-A-neuro-symbolic-approach-for-visual-features-matching" class="headerlink" title="Fuzzy Logic Visual Network (FLVN): A neuro-symbolic approach for visual features matching"></a>Fuzzy Logic Visual Network (FLVN): A neuro-symbolic approach for visual features matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16019">http://arxiv.org/abs/2307.16019</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/grains2/flvn">https://gitlab.com/grains2/flvn</a></li>
<li>paper_authors: Francesco Manigrasso, Lia Morra, Fabrizio Lamberti</li>
<li>for: 这个论文旨在探讨如何通过组合深度学习网络和符号知识表示来提高零shot学习（ZSL）分类的性能。</li>
<li>methods: 这个论文使用了逻辑tensor网络（LTN）来 incorporate 背景知识，包括逻辑axioms，并将其转化为可微分的操作。</li>
<li>results: 这个论文提出了名为Fuzzy Logic Visual Network（FLVN）的方法，该方法在neuro-symbolic LTN框架下学习视觉semantic空间。FLVN利用了类层级知识和 Robust高级 inductive bias，从而提高了ZSL分类的性能，在Generalized ZSL（GZSL）benchmark AWA2和CUB上达到了状态的艺术性能，相比之下，其计算开销较低。<details>
<summary>Abstract</summary>
Neuro-symbolic integration aims at harnessing the power of symbolic knowledge representation combined with the learning capabilities of deep neural networks. In particular, Logic Tensor Networks (LTNs) allow to incorporate background knowledge in the form of logical axioms by grounding a first order logic language as differentiable operations between real tensors. Yet, few studies have investigated the potential benefits of this approach to improve zero-shot learning (ZSL) classification. In this study, we present the Fuzzy Logic Visual Network (FLVN) that formulates the task of learning a visual-semantic embedding space within a neuro-symbolic LTN framework. FLVN incorporates prior knowledge in the form of class hierarchies (classes and macro-classes) along with robust high-level inductive biases. The latter allow, for instance, to handle exceptions in class-level attributes, and to enforce similarity between images of the same class, preventing premature overfitting to seen classes and improving overall performance. FLVN reaches state of the art performance on the Generalized ZSL (GZSL) benchmarks AWA2 and CUB, improving by 1.3% and 3%, respectively. Overall, it achieves competitive performance to recent ZSL methods with less computational overhead. FLVN is available at https://gitlab.com/grains2/flvn.
</details>
<details>
<summary>摘要</summary>
neuroro-symbolic 融合目标是利用深度神经网络学习的能力和符号知识表示的力量相结合。特别是逻辑张量网络（LTN）可以将背景知识表示为可 diferenciable 操作 между实数张量。然而，有很少的研究探讨了这种方法可以提高零例学习（ZSL）分类的潜力。在这项研究中，我们提出了灰度逻辑视觉网络（FLVN），它在 neuro-symbolic LTN 框架中学习视觉semantic embedding空间。FLVN integrates prior knowledge in the form of class hierarchies (classes and macro-classes) along with robust high-level inductive biases。这些假设允许，例如，处理类层特征异常，并强制图像同一类的相似性，避免提前过拟合已知类和提高总性能。FLVN 在 Generalized ZSL（GZSL）标准吗 AWA2 和 CUB 上达到了状态的捷径性表现，提高了1.3%和3%，分别。总的来说，它实现了与最近 ZSL 方法相当的性能，但计算开销较少。FLVN 可以在 <https://gitlab.com/grains2/flvn> 中下载。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/30/cs.CV_2023_07_30/" data-id="cloh7tqgt00f67b88dtkyfbxa" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/30/cs.AI_2023_07_30/" class="article-date">
  <time datetime="2023-07-30T12:00:00.000Z" itemprop="datePublished">2023-07-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/30/cs.AI_2023_07_30/">cs.AI - 2023-07-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DRL4Route-A-Deep-Reinforcement-Learning-Framework-for-Pick-up-and-Delivery-Route-Prediction"><a href="#DRL4Route-A-Deep-Reinforcement-Learning-Framework-for-Pick-up-and-Delivery-Route-Prediction" class="headerlink" title="DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction"></a>DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16246">http://arxiv.org/abs/2307.16246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maoxiaowei97/drl4route">https://github.com/maoxiaowei97/drl4route</a></li>
<li>paper_authors: Xiaowei Mao, Haomin Wen, Hengrui Zhang, Huaiyu Wan, Lixia Wu, Jianbin Zheng, Haoyuan Hu, Youfang Lin</li>
<li>For: 预测工作者的服务路线（PDRP），以便估算未来服务任务的路线，在过去几年内受到了越来越多的关注。* Methods: 使用深度神经网络和强化学习框架，将工作者的行为模式从大量历史数据中学习出来，并将非导数对象优化纳入训练过程中。* Results: 对实际数据集进行了广泛的离线实验和在线部署，并显示了对PDRP的改进，包括Location Square Deviation（LSD）和Accuracy@3（ACC@3）的改进。<details>
<summary>Abstract</summary>
Pick-up and Delivery Route Prediction (PDRP), which aims to estimate the future service route of a worker given his current task pool, has received rising attention in recent years. Deep neural networks based on supervised learning have emerged as the dominant model for the task because of their powerful ability to capture workers' behavior patterns from massive historical data. Though promising, they fail to introduce the non-differentiable test criteria into the training process, leading to a mismatch in training and test criteria. Which considerably trims down their performance when applied in practical systems. To tackle the above issue, we present the first attempt to generalize Reinforcement Learning (RL) to the route prediction task, leading to a novel RL-based framework called DRL4Route. It combines the behavior-learning abilities of previous deep learning models with the non-differentiable objective optimization ability of reinforcement learning. DRL4Route can serve as a plug-and-play component to boost the existing deep learning models. Based on the framework, we further implement a model named DRL4Route-GAE for PDRP in logistic service. It follows the actor-critic architecture which is equipped with a Generalized Advantage Estimator that can balance the bias and variance of the policy gradient estimates, thus achieving a more optimal policy. Extensive offline experiments and the online deployment show that DRL4Route-GAE improves Location Square Deviation (LSD) by 0.9%-2.7%, and Accuracy@3 (ACC@3) by 2.4%-3.2% over existing methods on the real-world dataset.
</details>
<details>
<summary>摘要</summary>
picked-up 和交付路线预测（PDRP）在最近几年内收到了越来越多的关注。深度神经网络基于超级vised学习 emerged as the dominant model for the task because of their powerful ability to capture workers' behavior patterns from massive historical data。although promising，they fail to introduce the non-differentiable test criteria into the training process，leading to a mismatch in training and test criteria。Which considerably trims down their performance when applied in practical systems。To tackle the above issue，we present the first attempt to generalize Reinforcement Learning（RL）to the route prediction task，leading to a novel RL-based framework called DRL4Route。It combines the behavior-learning abilities of previous deep learning models with the non-differentiable objective optimization ability of reinforcement learning。DRL4Route can serve as a plug-and-play component to boost the existing deep learning models。Based on the framework，we further implement a model named DRL4Route-GAE for PDRP in logistic service。It follows the actor-critic architecture which is equipped with a Generalized Advantage Estimator that can balance the bias and variance of the policy gradient estimates，thus achieving a more optimal policy。Extensive offline experiments and the online deployment show that DRL4Route-GAE improves Location Square Deviation（LSD）by 0.9%-2.7%，and Accuracy@3（ACC@3）by 2.4%-3.2% over existing methods on the real-world dataset。
</details></li>
</ul>
<hr>
<h2 id="Synaptic-Plasticity-Models-and-Bio-Inspired-Unsupervised-Deep-Learning-A-Survey"><a href="#Synaptic-Plasticity-Models-and-Bio-Inspired-Unsupervised-Deep-Learning-A-Survey" class="headerlink" title="Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey"></a>Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16236">http://arxiv.org/abs/2307.16236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato</li>
<li>for: 本研究旨在探讨基于深度学习（DL）技术的新兴应用，以及与生物体针对的机制相关的挑战。</li>
<li>methods: 本文综述了一些基于生物机制的深度学习模型，包括synaptic plasticity模型，以及与脉冲神经网络（SNNs）相关的模型。</li>
<li>results: 本研究发现，基于生物机制的深度学习模型在多种应用场景中表现出色，并且可能解决一些DL技术面临的挑战，如针对攻击和生态影响。<details>
<summary>Abstract</summary>
Recently emerged technologies based on Deep Learning (DL) achieved outstanding results on a variety of tasks in the field of Artificial Intelligence (AI). However, these encounter several challenges related to robustness to adversarial inputs, ecological impact, and the necessity of huge amounts of training data. In response, researchers are focusing more and more interest on biologically grounded mechanisms, which are appealing due to the impressive capabilities exhibited by biological brains. This survey explores a range of these biologically inspired models of synaptic plasticity, their application in DL scenarios, and the connections with models of plasticity in Spiking Neural Networks (SNNs). Overall, Bio-Inspired Deep Learning (BIDL) represents an exciting research direction, aiming at advancing not only our current technologies but also our understanding of intelligence.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spiking-Neural-Networks-and-Bio-Inspired-Supervised-Deep-Learning-A-Survey"><a href="#Spiking-Neural-Networks-and-Bio-Inspired-Supervised-Deep-Learning-A-Survey" class="headerlink" title="Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey"></a>Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16235">http://arxiv.org/abs/2307.16235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato<br>for:本文提供了一个全面的评论，涵盖最近基于生物学的人工智能技术发展的方法。methods:本文 introduce了生物神经元计算原则和 synaptic plasticity，并提供了精炼的脉冲神经网络（SNN）模型，以及对SNN训练的主要挑战。results:本文讨论了一些基于生物学的训练方法，作为传统backprop-based优化的替代方案，以提高当前模型的计算能力和生物学可能性。<details>
<summary>Abstract</summary>
For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.
</details>
<details>
<summary>摘要</summary>
For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.Here is the translation in Traditional Chinese:For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.
</details></li>
</ul>
<hr>
<h2 id="Robust-Electric-Vehicle-Balancing-of-Autonomous-Mobility-On-Demand-System-A-Multi-Agent-Reinforcement-Learning-Approach"><a href="#Robust-Electric-Vehicle-Balancing-of-Autonomous-Mobility-On-Demand-System-A-Multi-Agent-Reinforcement-Learning-Approach" class="headerlink" title="Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach"></a>Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16228">http://arxiv.org/abs/2307.16228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sihong He, Shuo Han, Fei Miao<br>for:This paper aims to design a multi-agent reinforcement learning (MARL) framework for electric autonomous vehicles (EAVs) balancing in future autonomous mobility-on-demand (AMoD) systems, with adversarial agents to model both the EAVs supply and mobility demand uncertainties.methods:The proposed method uses a MARL-based framework to train a robust EAVs balancing policy that considers both the supply-demand ratio and charging utilization rate across the whole city.results:Experiments show that the proposed robust method performs better compared with a non-robust MARL method, with improvements of 19.28% in reward, 28.18% in charging utilization fairness, and 3.97% in supply-demand fairness. Compared with a robust optimization-based method, the proposed MARL algorithm improves the reward, charging utilization fairness, and supply-demand fairness by 8.21%, 8.29%, and 9.42%, respectively.<details>
<summary>Abstract</summary>
Electric autonomous vehicles (EAVs) are getting attention in future autonomous mobility-on-demand (AMoD) systems due to their economic and societal benefits. However, EAVs' unique charging patterns (long charging time, high charging frequency, unpredictable charging behaviors, etc.) make it challenging to accurately predict the EAVs supply in E-AMoD systems. Furthermore, the mobility demand's prediction uncertainty makes it an urgent and challenging task to design an integrated vehicle balancing solution under supply and demand uncertainties. Despite the success of reinforcement learning-based E-AMoD balancing algorithms, state uncertainties under the EV supply or mobility demand remain unexplored. In this work, we design a multi-agent reinforcement learning (MARL)-based framework for EAVs balancing in E-AMoD systems, with adversarial agents to model both the EAVs supply and mobility demand uncertainties that may undermine the vehicle balancing solutions. We then propose a robust E-AMoD Balancing MARL (REBAMA) algorithm to train a robust EAVs balancing policy to balance both the supply-demand ratio and charging utilization rate across the whole city. Experiments show that our proposed robust method performs better compared with a non-robust MARL method that does not consider state uncertainties; it improves the reward, charging utilization fairness, and supply-demand fairness by 19.28%, 28.18%, and 3.97%, respectively. Compared with a robust optimization-based method, the proposed MARL algorithm can improve the reward, charging utilization fairness, and supply-demand fairness by 8.21%, 8.29%, and 9.42%, respectively.
</details>
<details>
<summary>摘要</summary>
电动自驾车 (EAVs) 在未来的自动化 Shared Mobility-on-Demand (AMoD) 系统中受到关注，因为它们具有经济和社会的优势。然而，EAVs 的充电模式 (长时间充电、高频充电、不可预测的充电行为等) 使得预测 EAVs 供应很具有挑战性。此外， mobilité 需求预测的不确定性使得设计一个集成的车辆均衡解决方案变得非常困难和挑战。虽然 reinforcement learning 基于 E-AMoD 均衡算法得到了成功，但是 state uncertainties under the EV 供应或 mobilité 需求仍然未经探讨。在这种情况下，我们设计了一个多代理启发学 (MARL) 基本框架 для EAVs 均衡在 E-AMoD 系统中，并使用对抗代理来模拟 EAVs 供应和 mobilité 需求不确定性。然后，我们提出了一种可靠的 E-AMoD Balancing MARL (REBAMA) 算法，用于训练一个可靠的 EAVs 均衡策略，以平衡全市的供应和需求比例，同时保证充电利用率的平衡。实验显示，我们的提出的可靠方法在比较 non-robust MARL 方法时表现更好，提高了奖励、充电利用公平和供应需求公平的指标，分别提高了19.28%, 28.18%和3.97%。相比robust optimization-based方法，我们的 MARL 算法可以提高奖励、充电利用公平和供应需求公平的指标，分别提高了8.21%, 8.29%和9.42%。
</details></li>
</ul>
<hr>
<h2 id="Text-Analysis-Using-Deep-Neural-Networks-in-Digital-Humanities-and-Information-Science"><a href="#Text-Analysis-Using-Deep-Neural-Networks-in-Digital-Humanities-and-Information-Science" class="headerlink" title="Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science"></a>Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16217">http://arxiv.org/abs/2307.16217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet</li>
<li>for: This paper aims to explore the use of deep neural networks (DNNs) in Digital Humanities (DH) research and provide a practical decision model for DH experts to choose the appropriate deep learning approaches for their research.</li>
<li>methods: The paper analyzes multiple use-cases of DH studies in recent literature and their possible solutions, and lays out a practical decision model for DH experts to choose the appropriate deep learning approaches for their research.</li>
<li>results: The paper aims to raise awareness of the benefits of utilizing deep learning models in the DH community and provide a practical decision model for DH experts to choose the appropriate deep learning approaches for their research.Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 这篇论文目的是探讨数字人文学科（DH）研究中使用深度神经网络（DNN）的可能性和实践。</li>
<li>methods: 论文分析了最新的DH研究文献中的多个用例和可能的解决方案，并提供了实用的决策模型，帮助DH专家选择适合他们研究的深度学习方法。</li>
<li>results: 论文的目的是为DH社区宣传深度学习模型的利好，并提供实用的决策模型，帮助DH专家选择适合他们研究的深度学习方法。<details>
<summary>Abstract</summary>
Combining computational technologies and humanities is an ongoing effort aimed at making resources such as texts, images, audio, video, and other artifacts digitally available, searchable, and analyzable. In recent years, deep neural networks (DNN) dominate the field of automatic text analysis and natural language processing (NLP), in some cases presenting a super-human performance. DNNs are the state-of-the-art machine learning algorithms solving many NLP tasks that are relevant for Digital Humanities (DH) research, such as spell checking, language detection, entity extraction, author detection, question answering, and other tasks. These supervised algorithms learn patterns from a large number of "right" and "wrong" examples and apply them to new examples. However, using DNNs for analyzing the text resources in DH research presents two main challenges: (un)availability of training data and a need for domain adaptation. This paper explores these challenges by analyzing multiple use-cases of DH studies in recent literature and their possible solutions and lays out a practical decision model for DH experts for when and how to choose the appropriate deep learning approaches for their research. Moreover, in this paper, we aim to raise awareness of the benefits of utilizing deep learning models in the DH community.
</details>
<details>
<summary>摘要</summary>
使用计算机技术和人文领域的结合是一项持续的努力，旨在使文本、图像、音频、视频和其他文化遗产数字化、搜索化和分析化。在最近几年里，深度神经网络（DNN）在自动文本分析和自然语言处理（NLP）领域占据了主导地位，在某些情况下表现出超人般的表现。DNN是当今最先进的机器学习算法，用于解决数字人文学科（DH）研究中有关的许多NLP任务，如拼写检查、语言检测、实体提取、作者检测、问答等任务。这些有监督的算法通过大量“正确”和“错误”示例学习出模式，然后应用于新示例。然而，在使用DNN进行人文学科研究中，存在两大挑战：数据训练的可用性和领域适应。本文通过分析多个DH研究中的用例，探讨这些挑战并提出解决方案，并提出了实用的决策模型，以帮助DH专家在选择合适的深度学习方法时作出决策。此外，本文的目的还是提高人文学科社区使用深度学习模型的认识。
</details></li>
</ul>
<hr>
<h2 id="Question-Answering-with-Deep-Neural-Networks-for-Semi-Structured-Heterogeneous-Genealogical-Knowledge-Graphs"><a href="#Question-Answering-with-Deep-Neural-Networks-for-Semi-Structured-Heterogeneous-Genealogical-Knowledge-Graphs" class="headerlink" title="Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs"></a>Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16214">http://arxiv.org/abs/2307.16214</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/omrivm/uncle-bert">https://github.com/omrivm/uncle-bert</a></li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</li>
<li>for: 这个研究旨在开发一种基于家谱树的问答系统，以便为家谱研究提供更好的支持。</li>
<li>methods: 这个研究使用了一种综合家谱数据作为知识图，然后将其转换为文本，并将文本与不结构化文本混合在一起，最后使用一种基于Transformer架构的问答模型进行训练。</li>
<li>results: 研究发现，使用专门的方法可以减少问答模型的复杂性，同时提高准确性。这种方法可能对家谱研究和实际项目有实际应用，使家谱数据更加可 accessible。<details>
<summary>Abstract</summary>
With the rising popularity of user-generated genealogical family trees, new genealogical information systems have been developed. State-of-the-art natural question answering algorithms use deep neural network (DNN) architecture based on self-attention networks. However, some of these models use sequence-based inputs and are not suitable to work with graph-based structure, while graph-based DNN models rely on high levels of comprehensiveness of knowledge graphs that is nonexistent in the genealogical domain. Moreover, these supervised DNN models require training datasets that are absent in the genealogical domain. This study proposes an end-to-end approach for question answering using genealogical family trees by: 1) representing genealogical data as knowledge graphs, 2) converting them to texts, 3) combining them with unstructured texts, and 4) training a trans-former-based question answering model. To evaluate the need for a dedicated approach, a comparison between the fine-tuned model (Uncle-BERT) trained on the auto-generated genealogical dataset and state-of-the-art question-answering models was per-formed. The findings indicate that there are significant differences between answering genealogical questions and open-domain questions. Moreover, the proposed methodology reduces complexity while increasing accuracy and may have practical implications for genealogical research and real-world projects, making genealogical data accessible to experts as well as the general public.
</details>
<details>
<summary>摘要</summary>
随着用户生成的家谱树的流行，新的家谱信息系统被开发出来。现代自然问答算法使用深度神经网络（DNN）架构，其中一些模型使用序列化输入并不适用于图形结构，而图形基于DNN模型则需要高度完整的知识图，而这在家谱领域并不存在。此外，这些直接监督DNN模型需要家谱领域缺乏训练数据。本研究提出了一种端到端方法，通过以下步骤来解决问题：1）将家谱数据转换为知识图，2）将其转换为文本，3）将文本与无结构文本结合，4）使用转换器基于模型来回答问题。为评估需要专门的方法，对自动生成的家谱数据 fine-tune Uncle-BERT 模型和现有的问答模型进行比较。研究发现，回答家谱问题和开放领域问题存在显著差异。此外，提出的方法可以减少复杂性而提高准确率，可能对家谱研究和实际项目产生实质性的影响，让家谱数据更加可访易地访问ible for experts and the general public。
</details></li>
</ul>
<hr>
<h2 id="Robust-Multi-Agent-Reinforcement-Learning-with-State-Uncertainty"><a href="#Robust-Multi-Agent-Reinforcement-Learning-with-State-Uncertainty" class="headerlink" title="Robust Multi-Agent Reinforcement Learning with State Uncertainty"></a>Robust Multi-Agent Reinforcement Learning with State Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16212">http://arxiv.org/abs/2307.16212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sihongho/robust_marl_with_state_uncertainty">https://github.com/sihongho/robust_marl_with_state_uncertainty</a></li>
<li>paper_authors: Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, Fei Miao</li>
<li>for: 本研究旨在解决多智能体强化学习（MARL）中存在状态不确定性的问题，提高 MARL 的稳定性和可靠性。</li>
<li>methods: 本文提出了一种基于 Markov Game 的状态扰动敌对（MG-SPA）模型，并使用 robust equilibrium（RE）作为解题方法。同时，提出了一种基于 Q-学习 的 robust multi-agent Q-learning（RMAQ）算法，以及一种基于actor-critic 算法的 robust multi-agent actor-critic（RMAAC）算法，以处理高维状态动作空间。</li>
<li>results: 实验结果表明，提出的 RMAQ 算法可以寻求最优值函数；RMAAC 算法在多个多智能体环境中，在状态不确定性存在时，与多种 MARL 和robust MARL 方法相比，表现更高效。<details>
<summary>Abstract</summary>
In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC) algorithm based on an analytical expression of the policy gradient derived in the paper. Our experiments show that the proposed RMAQ algorithm converges to the optimal value function; our RMAAC algorithm outperforms several MARL and robust MARL methods in multiple multi-agent environments when state uncertainty is present. The source code is public on \url{https://github.com/sihongho/robust_marl_with_state_uncertainty}.
</details>
<details>
<summary>摘要</summary>
在实际多智能体学习（MARL）应用中，智能体可能无法获得完美的状态信息（例如因为不准确的测量或攻击），这会对智能体的策略的稳定性造成挑战。虽然稳定性在MARL部署中变得越来越重要，但是前一个研究中对状态不确定性在MARL中的研究很少。为了解决这个稳定性问题和相关的研究不足，我们在这里研究了MARL中的状态不确定性问题。我们首先将问题模型为一个Markov游戏中的状态干扰者（MG-SPA），并在Markov游戏中引入一组状态干扰者。然后，我们引入了稳定平衡（RE）作为MG-SPA的解决方案。我们进行了基本的分析，并给出了存在稳定平衡的条件。然后，我们提出了一种稳定多智能体Q学习（RMAQ）算法，以找到这样的平衡，并提供了一些确定性的证明。为了处理高维状态动作空间，我们设计了一种基于分析表达的策略梯度的稳定多智能体actor-critic（RMAAC）算法。我们的实验表明，我们的RMAQ算法可以到达最优的值函数；我们的RMAAC算法在多个多智能体环境中，当状态不确定性存在时，与多个MARL和稳定MARL方法相比，表现更好。源代码可以在<https://github.com/sihongho/robust_marl_with_state_uncertainty>上获取。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Uncertainly-Missing-and-Ambiguous-Visual-Modality-in-Multi-Modal-Entity-Alignment"><a href="#Rethinking-Uncertainly-Missing-and-Ambiguous-Visual-Modality-in-Multi-Modal-Entity-Alignment" class="headerlink" title="Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment"></a>Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16210">http://arxiv.org/abs/2307.16210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjukg/UMAEA">https://github.com/zjukg/UMAEA</a></li>
<li>paper_authors: Zhuo Chen, Lingbing Guo, Yin Fang, Yichi Zhang, Jiaoyan Chen, Jeff Z. Pan, Yangning Li, Huajun Chen, Wen Zhang</li>
<li>for: 多modalentityAlignment (MMEA) 的挑战，包括模式噪声和内在的模式不确定性。</li>
<li>methods: 我们提出了一种基于 uncertainly missing and ambiguous visual modalities的Robust Multi-modal Entity Alignment (UMAEA) 方法，并在多个 benchmark splits 上达到了最佳性能。</li>
<li>results: UMAEA 方法在face of modality incompleteness和模式不确定性中具有优秀的性能，比如其他模型具有更多的参数和更多的计时时间，同时能够有效地缓解其他模型中的限制。<details>
<summary>Abstract</summary>
As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additional multi-modal data can sometimes adversely affect EA. To address these challenges, we introduce UMAEA , a robust multi-modal entity alignment approach designed to tackle uncertainly missing and ambiguous visual modalities. It consistently achieves SOTA performance across all 97 benchmark splits, significantly surpassing existing baselines with limited parameters and time consumption, while effectively alleviating the identified limitations of other models. Our code and benchmark data are available at https://github.com/zjukg/UMAEA.
</details>
<details>
<summary>摘要</summary>
为了解决多个知识图（KG）之间的实体对应关系（Entity Alignment，EA）的扩展，多模态实体对应（Multi-modal Entity Alignment，MMEA）尝试通过利用相关的视觉信息来标识不同知识图中的相同实体。然而，现有的MMEA方法主要集中在多模态实体特征的融合方法上，而忽略了视觉图像中的普遍现象——缺失和内在的模糊性。在这篇论文中，我们进行了视觉Modal的进一步分析，并在我们提出的MMEA-UMVM数据集上 benchmark最新的MMEA模型。我们的研究表明，在面临多模态缺失的情况下，模型会受到模态噪声的折衔，并且在高比例的缺失多模态时，表现出振荡或下降的趋势。这表明，在多模态缺失情况下，模型可能会因为模态噪声而降低性能。为了解决这些挑战，我们提出了UMAEA，一种适应不确定、缺失和模糊的视觉多模态实体对应方法。它在所有97个 benchmark split中表现出了最高的SOTA性能，超过了已有的基线值，同时具有有限的参数和时间投入。我们的代码和 benchmark数据可以在https://github.com/zjukg/UMAEA上获取。
</details></li>
</ul>
<hr>
<h2 id="Around-the-GLOBE-Numerical-Aggregation-Question-Answering-on-Heterogeneous-Genealogical-Knowledge-Graphs-with-Deep-Neural-Networks"><a href="#Around-the-GLOBE-Numerical-Aggregation-Question-Answering-on-Heterogeneous-Genealogical-Knowledge-Graphs-with-Deep-Neural-Networks" class="headerlink" title="Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks"></a>Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16208">http://arxiv.org/abs/2307.16208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech<br>for:This paper is written for researchers and practitioners in the field of natural language processing and genealogy, as well as for the general public who are interested in exploring cultural heritage domains.methods:The paper proposes a new end-to-end methodology for numerical aggregation question-answering (QA) for genealogical trees, which includes an automatic method for training dataset generation, a transformer-based table selection method, and an optimized transformer-based numerical aggregation QA model.results:The proposed architecture, called GLOBE, outperforms the state-of-the-art models and pipelines by achieving 87% accuracy for the task of numerical aggregation QA compared to only 21% by current state-of-the-art models.<details>
<summary>Abstract</summary>
One of the key AI tools for textual corpora exploration is natural language question-answering (QA). Unlike keyword-based search engines, QA algorithms receive and process natural language questions and produce precise answers to these questions, rather than long lists of documents that need to be manually scanned by the users. State-of-the-art QA algorithms based on DNNs were successfully employed in various domains. However, QA in the genealogical domain is still underexplored, while researchers in this field (and other fields in humanities and social sciences) can highly benefit from the ability to ask questions in natural language, receive concrete answers and gain insights hidden within large corpora. While some research has been recently conducted for factual QA in the genealogical domain, to the best of our knowledge, there is no previous research on the more challenging task of numerical aggregation QA (i.e., answering questions combining aggregation functions, e.g., count, average, max). Numerical aggregation QA is critical for distant reading and analysis for researchers (and the general public) interested in investigating cultural heritage domains. Therefore, in this study, we present a new end-to-end methodology for numerical aggregation QA for genealogical trees that includes: 1) an automatic method for training dataset generation; 2) a transformer-based table selection method, and 3) an optimized transformer-based numerical aggregation QA model. The findings indicate that the proposed architecture, GLOBE, outperforms the state-of-the-art models and pipelines by achieving 87% accuracy for this task compared to only 21% by current state-of-the-art models. This study may have practical implications for genealogical information centers and museums, making genealogical data research easy and scalable for experts as well as the general public.
</details>
<details>
<summary>摘要</summary>
一种关键的人工智能工具 для文本资料探索是自然语言问答（QA）。不同于关键词搜索引擎，QA算法会根据自然语言问题提供精确的答案，而不是长列表需要手动扫描的文档。现状最先进的QA算法基于深度学习神经网络（DNN）在多个领域得到了成功应用。然而，在家谱领域，QA仍然受到了不足的研究，而家谱领域的研究人员（以及人文社科领域的研究人员）可以很大程度上受益于自然语言问题的能力，并且可以通过自然语言问题来获得潜藏在大量文档中的新的发现和理解。虽然有些研究已经在家谱领域进行了实际问答，但我们知道的是，没有任何研究在家谱领域进行了更加复杂的数学聚合问答（例如计数、平均值、最大值）。数学聚合问答是远程阅读和分析的关键，因此在这种领域进行数学聚合问答是非常重要的。因此，在本研究中，我们提出了一种新的端到端方法，名为GLOBE，用于家谱领域的数学聚合问答。GLOBE方法包括：1）自动生成训练数据集方法；2）基于转换器的表格选择方法；3）优化的转换器基于数学聚合问答模型。研究结果表明，GLOBE方法在这个任务上的准确率为87%，比现有的状态OF艺术模型和管道的准确率高出26倍。这项研究可能对家谱信息中心和博物馆产生实质性的影响，使家谱数据研究变得容易和可扩展，以便专家和一般公众都能够轻松地进行研究。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Event-centric-Knowledge-Graphs-of-Daily-Activities-Using-Virtual-Space"><a href="#Synthesizing-Event-centric-Knowledge-Graphs-of-Daily-Activities-Using-Virtual-Space" class="headerlink" title="Synthesizing Event-centric Knowledge Graphs of Daily Activities Using Virtual Space"></a>Synthesizing Event-centric Knowledge Graphs of Daily Activities Using Virtual Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16206">http://arxiv.org/abs/2307.16206</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aistairc/virtualhome2kg">https://github.com/aistairc/virtualhome2kg</a></li>
<li>paper_authors: Shusaku Egami, Takanori Ugai, Mikiko Oono, Koji Kitamura, Ken Fukuda</li>
<li>for: 本研究旨在提供一个虚拟空间内的日常活动知识 graphs（KG）构建框架，以支持人类日常生活中的各种情感和决策。</li>
<li>methods: 本研究使用的方法包括虚拟空间 simulations、事件中心式架构、和Contextual semantic data的生成。</li>
<li>results: 本研究通过实验示出了虚拟Home2KG框架的实用性和潜力，并显示了可以通过该框架进行日常活动分析、问题回答、嵌入和散列等应用。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) is expected to be embodied in software agents, robots, and cyber-physical systems that can understand the various contextual information of daily life in the home environment to support human behavior and decision making in various situations. Scene graph and knowledge graph (KG) construction technologies have attracted much attention for knowledge-based embodied question answering meeting this expectation. However, collecting and managing real data on daily activities under various experimental conditions in a physical space are quite costly, and developing AI that understands the intentions and contexts is difficult. In the future, data from both virtual spaces, where conditions can be easily modified, and physical spaces, where conditions are difficult to change, are expected to be combined to analyze daily living activities. However, studies on the KG construction of daily activities using virtual space and their application have yet to progress. The potential and challenges must still be clarified to facilitate AI development for human daily life. Thus, this study proposes the VirtualHome2KG framework to generate synthetic KGs of daily life activities in virtual space. This framework augments both the synthetic video data of daily activities and the contextual semantic data corresponding to the video contents based on the proposed event-centric schema and virtual space simulation results. Therefore, context-aware data can be analyzed, and various applications that have conventionally been difficult to develop due to the insufficient availability of relevant data and semantic information can be developed. We also demonstrate herein the utility and potential of the proposed VirtualHome2KG framework through several use cases, including the analysis of daily activities by querying, embedding, and clustering, and fall risk detection among ...
</details>
<details>
<summary>摘要</summary>
人工智能（AI）预期会被嵌入软件代理、机器人和 cyber-physical 系统中，以便在家庭环境中理解日常生活中的多种情况信息，以支持人类行为和决策。Scene graph和知识图（KG）建构技术吸引了很多关注，以满足这个期望。然而，收集和管理实际情况下的日常活动数据在物理空间是非常成本的，而发展AI理解意图和情况是困难的。未来，来自虚拟空间和物理空间的数据将被组合分析日常生活活动。然而，在虚拟空间和物理空间的KG建构日常活动研究仍然处于早期阶段。因此，本研究提出了虚拟家庭2KG框架，用于生成虚拟空间中的日常活动Synthetic KG。该框架将融合日常活动的 sintetic 视频数据和相关的上下文semantic数据，根据提出的事件-中心架构和虚拟空间模拟结果。因此，可以分析上下文化数据，并开发过去由于数据和semantic信息的不足而困难的应用。我们还在本文中展示了虚拟家庭2KG框架的实用性和潜力，包括查询、嵌入和凝聚等多种应用场景，以及落干风险检测等。
</details></li>
</ul>
<hr>
<h2 id="Shuffled-Differentially-Private-Federated-Learning-for-Time-Series-Data-Analytics"><a href="#Shuffled-Differentially-Private-Federated-Learning-for-Time-Series-Data-Analytics" class="headerlink" title="Shuffled Differentially Private Federated Learning for Time Series Data Analytics"></a>Shuffled Differentially Private Federated Learning for Time Series Data Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16196">http://arxiv.org/abs/2307.16196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxi Huang, Chaoyang Jiang, Zhenghua Chen</li>
<li>for: 针对时间序列资料的信任worthy联合学习，实现最佳性能 while ensuring clients’ privacy.</li>
<li>methods: 使用本地差异隐藏来扩展隐藏保护 bound 到客户端，并将抛终技术 incorporated 以实现隐藏增强，从而缓解因采用本地差异隐藏而导致的准确度下降。</li>
<li>results: 在五个时间序列数据集上进行了广泛的实验，结果显示了我们的算法在小客户和大客户enario 中都实现了最小的准确度损失，并在同等隐藏保护水平下与中央差异隐藏联合学习相比，在小客户和大客户enario 中都展现出了改善的准确度。<details>
<summary>Abstract</summary>
Trustworthy federated learning aims to achieve optimal performance while ensuring clients' privacy. Existing privacy-preserving federated learning approaches are mostly tailored for image data, lacking applications for time series data, which have many important applications, like machine health monitoring, human activity recognition, etc. Furthermore, protective noising on a time series data analytics model can significantly interfere with temporal-dependent learning, leading to a greater decline in accuracy. To address these issues, we develop a privacy-preserving federated learning algorithm for time series data. Specifically, we employ local differential privacy to extend the privacy protection trust boundary to the clients. We also incorporate shuffle techniques to achieve a privacy amplification, mitigating the accuracy decline caused by leveraging local differential privacy. Extensive experiments were conducted on five time series datasets. The evaluation results reveal that our algorithm experienced minimal accuracy loss compared to non-private federated learning in both small and large client scenarios. Under the same level of privacy protection, our algorithm demonstrated improved accuracy compared to the centralized differentially private federated learning in both scenarios.
</details>
<details>
<summary>摘要</summary>
信任worthy的联合学习 aimsto achieve optimal performance while ensuring clients' privacy. Existing privacy-preserving federated learning approaches are mostly tailored for image data, lacking applications for time series data, which have many important applications, such as machine health monitoring and human activity recognition. Furthermore, protective noising on a time series data analytics model can significantly interfere with temporal-dependent learning, leading to a greater decline in accuracy. To address these issues, we develop a privacy-preserving federated learning algorithm for time series data. Specifically, we employ local differential privacy to extend the privacy protection trust boundary to the clients. We also incorporate shuffle techniques to achieve a privacy amplification, mitigating the accuracy decline caused by leveraging local differential privacy. Extensive experiments were conducted on five time series datasets. The evaluation results reveal that our algorithm experienced minimal accuracy loss compared to non-private federated learning in both small and large client scenarios. Under the same level of privacy protection, our algorithm demonstrated improved accuracy compared to the centralized differentially private federated learning in both scenarios.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you prefer Traditional Chinese, please let me know and I will be happy to provide the translation in that version as well.
</details></li>
</ul>
<hr>
<h2 id="CLGT-A-Graph-Transformer-for-Student-Performance-Prediction-in-Collaborative-Learning"><a href="#CLGT-A-Graph-Transformer-for-Student-Performance-Prediction-in-Collaborative-Learning" class="headerlink" title="CLGT: A Graph Transformer for Student Performance Prediction in Collaborative Learning"></a>CLGT: A Graph Transformer for Student Performance Prediction in Collaborative Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02038">http://arxiv.org/abs/2308.02038</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tianhao-peng/clgt">https://github.com/tianhao-peng/clgt</a></li>
<li>paper_authors: Tianhao Peng, Yu Liang, Wenjun Wu, Jian Ren, Zhao Pengrui, Yanjun Pu</li>
<li>for: 本研究旨在模型和预测学生在合作学习 paradigm 中的表现。大多数 literatura 中的研究都集中在讨论 forum 和社交学习网络。只有一些工作研究了学生在团队项目中如何互动，以及这些互动如何影响他们的学术表现。为了填补这个差距，我们选择了一个软件工程课程作为研究主题。参与这门课程的学生需要组队完成一个软件项目。在这种情况下，我们构建了一个学生互动图，基于不同团队中学生的活动。以这个学生互动图为基础，我们提出了一种扩展的图 transformer 框架 для合作学习（CLGT），用于评估和预测学生的表现。此外，提出的 CLGT 还包括一个解释模块，用于解释预测结果并可视化学生互动模式。实验结果表明，提出的 CLGT 在基于实际数据集上进行预测时，与基准模型相比，表现更高。此外，提出的 CLGT 可以 diferenciate 学生在合作学习 paradigm 中的低表现学生，并给教师提供早期预警，以便提供相应的帮助。<details>
<summary>Abstract</summary>
Modeling and predicting the performance of students in collaborative learning paradigms is an important task. Most of the research presented in literature regarding collaborative learning focuses on the discussion forums and social learning networks. There are only a few works that investigate how students interact with each other in team projects and how such interactions affect their academic performance. In order to bridge this gap, we choose a software engineering course as the study subject. The students who participate in a software engineering course are required to team up and complete a software project together. In this work, we construct an interaction graph based on the activities of students grouped in various teams. Based on this student interaction graph, we present an extended graph transformer framework for collaborative learning (CLGT) for evaluating and predicting the performance of students. Moreover, the proposed CLGT contains an interpretation module that explains the prediction results and visualizes the student interaction patterns. The experimental results confirm that the proposed CLGT outperforms the baseline models in terms of performing predictions based on the real-world datasets. Moreover, the proposed CLGT differentiates the students with poor performance in the collaborative learning paradigm and gives teachers early warnings, so that appropriate assistance can be provided.
</details>
<details>
<summary>摘要</summary>
学习协作模式下学生表现预测和评价是一项重要任务。大多数文献中的研究都集中在讨论区和社交学习网络上，只有一些研究探讨了学生在团队项目中之间的互动如何影响学业表现。为了填补这一漏洞，我们选择了软件工程课程作为研究对象。参与这门课程的学生需要组队完成软件项目。在这种情况下，我们构建了基于学生分组的团队活动图，然后提出了一种基于协作学习（CLGT）扩展图 transformer 框架，用于评估和预测学生表现。此外，我们的 CLGT 还包括一个解释模块，可以解释预测结果并可视化学生互动模式。实验结果表明，我们的 CLGT 在实际数据集上表现较好，而且可以区分协作学习中表现不佳的学生，为教师提供早期预警，以便提供相应的帮助。
</details></li>
</ul>
<hr>
<h2 id="ESP-Exploiting-Symmetry-Prior-for-Multi-Agent-Reinforcement-Learning"><a href="#ESP-Exploiting-Symmetry-Prior-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning"></a>ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16186">http://arxiv.org/abs/2307.16186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yu, Rongye Shi, Pu Feng, Yongkai Tian, Jie Luo, Wenjun Wu</li>
<li>for: 提高多智能体学习（MARL）的数据效率和模型准确性。</li>
<li>methods:  integrate data augmentation和一种Well-designed consistency loss到现有的MARL方法中，使用协同学习和约束优化。</li>
<li>results: 在多个复杂任务上实现了效果，并在物理多机器人测试环境中证明了其优势。Here’s the breakdown of each point:1. for: This paper aims to improve the data efficiency and model accuracy of MARL by incorporating prior knowledge and using data augmentation.2. methods: The proposed framework uses a well-designed consistency loss and integrates it with existing MARL methods, utilizing both individual and cooperative learning.3. results: The proposed framework achieves effectiveness on multiple challenging tasks and outperforms existing methods in a physical multi-robot testbed.<details>
<summary>Abstract</summary>
Multi-agent reinforcement learning (MARL) has achieved promising results in recent years. However, most existing reinforcement learning methods require a large amount of data for model training. In addition, data-efficient reinforcement learning requires the construction of strong inductive biases, which are ignored in the current MARL approaches. Inspired by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.
</details>
<details>
<summary>摘要</summary>
多智能体强化学习（MARL）在最近几年内已经取得了成功的结果。然而，现有的强化学习方法大多需要训练模型的大量数据。此外，数据效率的强化学习还需要建立强的概念预测，这些预测在当前的 MARL 方法中被忽略了。 inspirited by 多智能体系统中的对称现象，本文提出了一种将数据扩展和一种良好设计的一致损失integrated into the existing MARL methods。此外，提出的框架是model-agnostic，可以应用于大多数当前的 MARL算法。实验测试在多个复杂任务上表明了提出的框架的有效性。此外，提出的框架还应用于一个物理多机器人测试平台，以示其优势。Note: The translation is done using a machine translation tool, and may not be perfect. Please note that the translation is provided as-is, and may not be accurate or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Modeling-with-Experimental-Augmentation-for-the-Modulation-Strategy-of-the-Dual-Active-Bridge-Converter"><a href="#Data-Driven-Modeling-with-Experimental-Augmentation-for-the-Modulation-Strategy-of-the-Dual-Active-Bridge-Converter" class="headerlink" title="Data-Driven Modeling with Experimental Augmentation for the Modulation Strategy of the Dual-Active-Bridge Converter"></a>Data-Driven Modeling with Experimental Augmentation for the Modulation Strategy of the Dual-Active-Bridge Converter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16173">http://arxiv.org/abs/2307.16173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinze Li, Josep Pou, Jiaxin Dong, Fanfan Lin, Changyun Wen, Suvajit Mukherjee, Xin Zhang</li>
<li>for: 提高功率转换器性能模型的准确性和实用性</li>
<li>methods: combines simulation data and experimental data to establish a highly accurate and practical data-driven model</li>
<li>results: 实现了99.92%的效率模型准确性，并在2kW硬件实验中达到了98.45%的峰效率<details>
<summary>Abstract</summary>
For the performance modeling of power converters, the mainstream approaches are essentially knowledge-based, suffering from heavy manpower burden and low modeling accuracy. Recent emerging data-driven techniques greatly relieve human reliance by automatic modeling from simulation data. However, model discrepancy may occur due to unmodeled parasitics, deficient thermal and magnetic models, unpredictable ambient conditions, etc. These inaccurate data-driven models based on pure simulation cannot represent the practical performance in physical world, hindering their applications in power converter modeling. To alleviate model discrepancy and improve accuracy in practice, this paper proposes a novel data-driven modeling with experimental augmentation (D2EA), leveraging both simulation data and experimental data. In D2EA, simulation data aims to establish basic functional landscape, and experimental data focuses on matching actual performance in real world. The D2EA approach is instantiated for the efficiency optimization of a hybrid modulation for neutral-point-clamped dual-active-bridge (NPC-DAB) converter. The proposed D2EA approach realizes 99.92% efficiency modeling accuracy, and its feasibility is comprehensively validated in 2-kW hardware experiments, where the peak efficiency of 98.45% is attained. Overall, D2EA is data-light and can achieve highly accurate and highly practical data-driven models in one shot, and it is scalable to other applications, effortlessly.
</details>
<details>
<summary>摘要</summary>
现代电源转换器性能模型ing的主流方法基本上是知识基础的，受到人工劳动的重荷和低精度模型ing的限制。Recent emerging data-driven techniques greatly relieve human reliance by automatic modeling from simulation data. However, model discrepancy may occur due to unmodeled parasitics, deficient thermal and magnetic models, unpredictable ambient conditions, etc. These inaccurate data-driven models based on pure simulation cannot represent the practical performance in physical world, hindering their applications in power converter modeling. To alleviate model discrepancy and improve accuracy in practice, this paper proposes a novel data-driven modeling with experimental augmentation (D2EA), leveraging both simulation data and experimental data. In D2EA, simulation data aims to establish basic functional landscape, and experimental data focuses on matching actual performance in real world. The D2EA approach is instantiated for the efficiency optimization of a hybrid modulation for neutral-point-clamped dual-active-bridge (NPC-DAB) converter. The proposed D2EA approach realizes 99.92% efficiency modeling accuracy, and its feasibility is comprehensively validated in 2-kW hardware experiments, where the peak efficiency of 98.45% is attained. Overall, D2EA is data-light and can achieve highly accurate and highly practical data-driven models in one shot, and it is scalable to other applications, effortlessly.
</details></li>
</ul>
<hr>
<h2 id="HierVST-Hierarchical-Adaptive-Zero-shot-Voice-Style-Transfer"><a href="#HierVST-Hierarchical-Adaptive-Zero-shot-Voice-Style-Transfer" class="headerlink" title="HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer"></a>HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16171">http://arxiv.org/abs/2307.16171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sang-Hoon Lee, Ha-Yeong Choi, Hyung-Seok Oh, Seong-Whan Lee</li>
<li>for: 这个论文是为了解决Zero-shot voice style transfer（VST）系统中，新的话者语言风格转移的问题。</li>
<li>methods: 这个论文使用了 Hierarchical adaptive end-to-end zero-shot VST 模型，不需要文本输入，只使用了语音数据来训练模型，并利用了层次分布式构造和自我supervised representation。</li>
<li>results: 实验结果显示，我们的方法在Zero-shot VST scenario中表现更好，并且可以预测进行话者语言风格转移。Audio samples可以在 \url{<a target="_blank" rel="noopener" href="https://hiervst.github.io/%7D">https://hiervst.github.io/}</a> 上找到。<details>
<summary>Abstract</summary>
Despite rapid progress in the voice style transfer (VST) field, recent zero-shot VST systems still lack the ability to transfer the voice style of a novel speaker. In this paper, we present HierVST, a hierarchical adaptive end-to-end zero-shot VST model. Without any text transcripts, we only use the speech dataset to train the model by utilizing hierarchical variational inference and self-supervised representation. In addition, we adopt a hierarchical adaptive generator that generates the pitch representation and waveform audio sequentially. Moreover, we utilize unconditional generation to improve the speaker-relative acoustic capacity in the acoustic representation. With a hierarchical adaptive structure, the model can adapt to a novel voice style and convert speech progressively. The experimental results demonstrate that our method outperforms other VST models in zero-shot VST scenarios. Audio samples are available at \url{https://hiervst.github.io/}.
</details>
<details>
<summary>摘要</summary>
尽管voice style transfer（VST）领域的进步迅速，现有的零shot VST系统仍然缺乏将新 speaker的voice style转移的能力。在这篇论文中，我们提出了层次适应式结构的终端零shot VST模型，即HierVST。无需文本脚本，我们只使用语音数据来训练模型，通过层次变量推理和自主学习表示。此外，我们采用层次适应生成器，生成抖音表示和波形声音sequentially。此外，我们利用无条件生成提高了发音人Relative acoustic representation的能力。通过层次适应结构，模型可以适应新的语音风格，并逐渐转换语音。实验结果表明，我们的方法在零shot VST场景中超过了其他VST模型。听样本可以在 \url{https://hiervst.github.io/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="An-Effective-LSTM-DDPM-Scheme-for-Energy-Theft-Detection-and-Forecasting-in-Smart-Grid"><a href="#An-Effective-LSTM-DDPM-Scheme-for-Energy-Theft-Detection-and-Forecasting-in-Smart-Grid" class="headerlink" title="An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid"></a>An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16149">http://arxiv.org/abs/2307.16149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xun Yuan, Yang Yang, Arwa Alromih, Prosanta Gope, Biplab Sikdar</li>
<li>for: 这篇论文旨在解决智能电网系统中的能源盗窃探测 (ETD) 和能源消耗预测 (ECF) 两个几相关的挑战，以确保系统安全。</li>
<li>methods: 本文提出的解决方案结合了长期内部积存 (LSTM) 和检测扩散概率模型 (DDPM)，实现输入重建和预测。系统通过利用重建和预测错误来识别能源盗窃实例，并且通过重建错误和预测错误的结合来检测不同类型的攻击。</li>
<li>results: 经过实验表明，提出的方案在真实数据和 sintetic 数据上都表现出色，较基eline方法有更好的检测和预测性。 ensemble 方法可以强化 ETD 性能，精确地检测能源盗窃攻击，而 baseline 方法则失败。<details>
<summary>Abstract</summary>
Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers a comprehensive and effective solution for addressing ETD and ECF challenges, demonstrating promising results and improved security in smart grid systems.
</details>
<details>
<summary>摘要</summary>
智能电网系统中的能源盗链检测（ETD）和能源消耗预测（ECF）是两个相互关联的挑战。解决这两个问题是确保系统安全的关键。本文介绍了智能电网系统中ETD和ECF的解决方案，combines long short-term memory（LSTM）和denoising diffusion probabilistic model（DDPM）来生成输入重建和预测。通过利用重建和预测错误，系统可以识别能源盗链行为，baseline方法不能检测的不同类型的攻击。通过对实际数据和 sintetic 数据进行广泛的实验，提出的方案在ETD和ECF问题中表现出色，significantly enhances ETD性能，准确地检测能源盗链攻击。本研究提供了智能电网系统中ETD和ECF问题的全面和有效解决方案，实现了系统安全性的提高。
</details></li>
</ul>
<hr>
<h2 id="Fully-1-times1-Convolutional-Network-for-Lightweight-Image-Super-Resolution"><a href="#Fully-1-times1-Convolutional-Network-for-Lightweight-Image-Super-Resolution" class="headerlink" title="Fully $1\times1$ Convolutional Network for Lightweight Image Super-Resolution"></a>Fully $1\times1$ Convolutional Network for Lightweight Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16140">http://arxiv.org/abs/2307.16140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aitical/scnet">https://github.com/aitical/scnet</a></li>
<li>paper_authors: Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu</li>
<li>for: 提高单张图像超解像（SISR）任务中的性能，特别是在具有大kernel（3×3或更大）的深度模型中。</li>
<li>methods: 提出一种简单 yet effective的完全$1\times1$卷积网络，称为Shift-Conv-based Network（SCNet），通过添加一个参数自由的空间移动操作，使得完全$1\times1$卷积网络具有强大的表示能力和出色的计算效率。</li>
<li>results: 经验表明，SCNets，即使完全使用$1\times1$卷积结构，可以与现有的轻量级SR模型相匹配或超越其性能。<details>
<summary>Abstract</summary>
Deep models have achieved significant process on single image super-resolution (SISR) tasks, in particular large models with large kernel ($3\times3$ or more). However, the heavy computational footprint of such models prevents their deployment in real-time, resource-constrained environments. Conversely, $1\times1$ convolutions bring substantial computational efficiency, but struggle with aggregating local spatial representations, an essential capability to SISR models. In response to this dichotomy, we propose to harmonize the merits of both $3\times3$ and $1\times1$ kernels, and exploit a great potential for lightweight SISR tasks. Specifically, we propose a simple yet effective fully $1\times1$ convolutional network, named Shift-Conv-based Network (SCNet). By incorporating a parameter-free spatial-shift operation, it equips the fully $1\times1$ convolutional network with powerful representation capability while impressive computational efficiency. Extensive experiments demonstrate that SCNets, despite its fully $1\times1$ convolutional structure, consistently matches or even surpasses the performance of existing lightweight SR models that employ regular convolutions.
</details>
<details>
<summary>摘要</summary>
深度模型在单图超分辨（SISR）任务上已经实现了显著的进步，特别是大型模型与大ernel（3×3或更大）。然而，这些模型的计算负担太大，使得它们在实时、资源受限的环境中不得不进行部署。相反，$1\times1$ convolution具有很大的计算效率，但是它们很难将本地空间表示合并成功。为了解决这种对立，我们提议融合$3\times3$和$1\times1$ kernel的优点，并利用轻量级SR任务的潜在能力。具体来说，我们提出了一种简单 yet effective的全$1\times1$ convolutional neural network（SCNet）。通过添加无参数的空间移动操作，SCNet可以具有强大的表示能力，同时具有出色的计算效率。广泛的实验表明，尽管SCNet具有全$1\times1$ convolutional结构，仍然可以与现有的轻量级SR模型相比或超越其性能。
</details></li>
</ul>
<hr>
<h2 id="User-Controlled-Knowledge-Fusion-in-Large-Language-Models-Balancing-Creativity-and-Hallucination"><a href="#User-Controlled-Knowledge-Fusion-in-Large-Language-Models-Balancing-Creativity-and-Hallucination" class="headerlink" title="User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination"></a>User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16139">http://arxiv.org/abs/2307.16139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhang</li>
<li>for: 这篇论文旨在提出一种用户控制的机制，以调节大语言模型（LLM）的假设和现实知识之间的平衡。</li>
<li>methods: 该方法在训练阶段使用数字标签来表示LLM在生成响应时的 faithfulness degree，并通过自动化的过程来计算这个度量，包括 ROUGE  scores、Sentence-BERT 嵌入和 LLM 自我评估得分。</li>
<li>results: 研究人员在不同的场景下进行了广泛的实验，并证明了该方法的适应性和精度。结果表明，该方法可以增强 LLM 的多样性，同时保持它们的假设和投影之间的平衡。<details>
<summary>Abstract</summary>
In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Uncertainty-Encoded-Multi-Modal-Fusion-for-Robust-Object-Detection-in-Autonomous-Driving"><a href="#Uncertainty-Encoded-Multi-Modal-Fusion-for-Robust-Object-Detection-in-Autonomous-Driving" class="headerlink" title="Uncertainty-Encoded Multi-Modal Fusion for Robust Object Detection in Autonomous Driving"></a>Uncertainty-Encoded Multi-Modal Fusion for Robust Object Detection in Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16121">http://arxiv.org/abs/2307.16121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Lou, Qun Song, Qian Xu, Rui Tan, Jianping Wang</li>
<li>for: 提高自动驾驶感知器对象检测的精度和可靠性。</li>
<li>methods: 利用不同感知器的检测结果和单模态不确定性进行多模态融合，并通过门控网络对结果进行权重衡量。</li>
<li>results: 与状态bla bla bla相比，提高了10.67%, 3.17%, 5.40%的性能。<details>
<summary>Abstract</summary>
Multi-modal fusion has shown initial promising results for object detection of autonomous driving perception. However, many existing fusion schemes do not consider the quality of each fusion input and may suffer from adverse conditions on one or more sensors. While predictive uncertainty has been applied to characterize single-modal object detection performance at run time, incorporating uncertainties into the multi-modal fusion still lacks effective solutions due primarily to the uncertainty's cross-modal incomparability and distinct sensitivities to various adverse conditions. To fill this gap, this paper proposes Uncertainty-Encoded Mixture-of-Experts (UMoE) that explicitly incorporates single-modal uncertainties into LiDAR-camera fusion. UMoE uses individual expert network to process each sensor's detection result together with encoded uncertainty. Then, the expert networks' outputs are analyzed by a gating network to determine the fusion weights. The proposed UMoE module can be integrated into any proposal fusion pipeline. Evaluation shows that UMoE achieves a maximum of 10.67%, 3.17%, and 5.40% performance gain compared with the state-of-the-art proposal-level multi-modal object detectors under extreme weather, adversarial, and blinding attack scenarios.
</details>
<details>
<summary>摘要</summary>
Here is the Simplified Chinese translation:多modal融合已经在自动驾驶感知中展示了初步的抢眼结果，但许多现有的融合方案不考虑每个融合输入的质量，可能会受到一或多个感知器的不良条件的影响。尽管预测不确定性已经应用于characterize单modal对象检测性能的实时，但在多modal融合中缺乏有效的解决方案，主要是因为不确定性的跨模异常性和不同的感知器对各种不良条件的敏感性。为了填补这个空白，这篇论文提出了不确定性编码的权重混合（UMoE）模块，该模块将单modal不确定性编码成特征网络中的一部分。然后，这些特征网络的输出将被分析器网络分析，以确定融合权重。该提出的UMoE模块可以与任何提议融合管道集成。评估结果表明，UMoE在极端天气、反击和盲目攻击等场景下达到了最大10.67%, 3.17%和5.40%的性能提升。
</details></li>
</ul>
<hr>
<h2 id="AI-Increases-Global-Access-to-Reliable-Flood-Forecasts"><a href="#AI-Increases-Global-Access-to-Reliable-Flood-Forecasts" class="headerlink" title="AI Increases Global Access to Reliable Flood Forecasts"></a>AI Increases Global Access to Reliable Flood Forecasts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16104">http://arxiv.org/abs/2307.16104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/global_streamflow_model_paper">https://github.com/google-research-datasets/global_streamflow_model_paper</a></li>
<li>paper_authors: Grey Nearing, Deborah Cohen, Vusumuzi Dube, Martin Gauch, Oren Gilon, Shaun Harrigan, Avinatan Hassidim, Frederik Kratzert, Asher Metzger, Sella Nevo, Florian Pappenberger, Christel Prudhomme, Guy Shalev, Shlomo Shenzis, Tadele Tekalign, Dana Weitzner, Yoss Matias</li>
<li>for: 这项研究的目的是开发一种基于人工智能的洪水预测模型，以提供更加准确和及时的洪水警报。</li>
<li>methods: 该模型使用了人工智能技术，并使用了全球覆盖率较高的卫星数据和开放数据来预测洪水事件。</li>
<li>results: 该模型在全球各大洲的洪水预测中表现出色，特别是在 ungauged 水系中，其预测精度高于现有的全球洪水模型。<details>
<summary>Abstract</summary>
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human impacts of flooding. We produce forecasts of extreme events in South America and Africa that achieve reliability approaching the current state of the art in Europe and North America, and we achieve reliability at between 4 and 6-day lead times that are similar to current state of the art nowcasts (0-day lead time). Additionally, we achieve accuracies over 10-year return period events that are similar to current accuracies over 2-year return period events, meaning that AI can provide warnings earlier and over larger and more impactful events. The model that we develop in this paper has been incorporated into an operational early warning system that produces publicly available (free and open) forecasts in real time in over 80 countries. This work using AI and open data highlights a need for increasing the availability of hydrological data to continue to improve global access to reliable flood warnings.
</details>
<details>
<summary>摘要</summary>
洪水是最常见且影响最大的自然灾害之一，特别是在发展中国家，那里缺乏密集的流量监测网。精确和时刻的警告是控制洪水风险的关键，但是需要对每个水系进行精确的水文模型协调。我们开发了人工智能（AI）模型，可以预测极端ydrological事件，时间从7天前到7天后。这个模型在全球各大洲、不同的领先时间和回报期都有出色的表现，特别是在无测流域，因为只有少数世界的水系有流量测站，而那些缺乏测站的水系往往是发展中国家，他们对人类洪水的影响更加敏感。我们的预测在南美和非洲 achieves reliability approaching the current state of the art in Europe and North America, and we achieve reliability at between 4 and 6-day lead times that are similar to current state of the art nowcasts (0-day lead time). In addition, we achieve accuracies over 10-year return period events that are similar to current accuracies over 2-year return period events, meaning that AI can provide warnings earlier and over larger and more impactful events. 我们在这篇文章中开发的模型已经被integrated into an operational early warning system that produces publicly available (free and open) forecasts in real time in over 80 countries. 这个使用AI和开放数据的工作表明了需要增加水文数据的可用性，以继续提高全球访问可靠的洪水警告。
</details></li>
</ul>
<hr>
<h2 id="PD-SEG-Population-Disaggregation-Using-Deep-Segmentation-Networks-For-Improved-Built-Settlement-Mask"><a href="#PD-SEG-Population-Disaggregation-Using-Deep-Segmentation-Networks-For-Improved-Built-Settlement-Mask" class="headerlink" title="PD-SEG: Population Disaggregation Using Deep Segmentation Networks For Improved Built Settlement Mask"></a>PD-SEG: Population Disaggregation Using Deep Segmentation Networks For Improved Built Settlement Mask</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16084">http://arxiv.org/abs/2307.16084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Abdul Rahman, Muhammad Ahmad Waseem, Zubair Khalid, Muhammad Tahir, Momin Uppal</li>
<li>For: 该研究旨在提供高精度的人口普查数据，以便用于国家发展规划和资源分配决策。* Methods: 该研究使用深度分割网络生成高精度的建成区域面积图像，并使用POI数据排除非居住区域。* Results: 该研究可以准确地估计人口总数和人口密度，并可以提供30米x30米的分辨率的人口普查数据。<details>
<summary>Abstract</summary>
Any policy-level decision-making procedure and academic research involving the optimum use of resources for development and planning initiatives depends on accurate population density statistics. The current cutting-edge datasets offered by WorldPop and Meta do not succeed in achieving this aim for developing nations like Pakistan; the inputs to their algorithms provide flawed estimates that fail to capture the spatial and land-use dynamics. In order to precisely estimate population counts at a resolution of 30 meters by 30 meters, we use an accurate built settlement mask obtained using deep segmentation networks and satellite imagery. The Points of Interest (POI) data is also used to exclude non-residential areas.
</details>
<details>
<summary>摘要</summary>
任何政策层次决策过程和学术研究，涉及资源最佳利用 для发展和规划倡议，都取决于准确的人口密度统计。现有的最先进数据集，如WorldPop和Meta，无法实现这一目标，因为它们的输入算法不能准确捕捉空间和土地利用动态。为了准确地估算人口数，我们使用高精度的建成市区mask，以及POI数据来排除非居住区域。注意：以下文本使用了简化中文，与标准中文有些细微的差异。
</details></li>
</ul>
<hr>
<h2 id="EnrichEvent-Enriching-Social-Data-with-Contextual-Information-for-Emerging-Event-Extraction"><a href="#EnrichEvent-Enriching-Social-Data-with-Contextual-Information-for-Emerging-Event-Extraction" class="headerlink" title="EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction"></a>EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16082">http://arxiv.org/abs/2307.16082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadali Sefidi Esfahani, Mohammad Akbari</li>
<li>for: 这 paper 的目的是提出一种基于流行社交数据的事件检测方法，以便更好地检测和分类不同类型的社会事件。</li>
<li>methods: 该方法使用语义和语境知识来检测社交媒体上的事件，并通过构建事件链来展示事件的变化。</li>
<li>results: 实验结果表明，该方法能够高效地检测和分类不同类型的社会事件，并且可以准确地捕捉事件的变化。<details>
<summary>Abstract</summary>
Social platforms have emerged as crucial platforms for disseminating information and discussing real-life social events, which offers an excellent opportunity for researchers to design and implement novel event detection frameworks. However, most existing approaches merely exploit keyword burstiness or network structures to detect unspecified events. Thus, they often fail to identify unspecified events regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, word sense ambiguation, and irregular language, as well as variation in aspects of opinions. Moreover, extracting discriminative features and patterns for evolving events by exploiting the limited structural knowledge is almost infeasible. To address these challenges, in this thesis, we propose a novel framework, namely EnrichEvent, that leverages the lexical and contextual representations of streaming social data. In particular, we leverage contextual knowledge, as well as lexical knowledge, to detect semantically related tweets and enhance the effectiveness of the event detection approaches. Eventually, our proposed framework produces cluster chains for each event to show the evolving variation of the event through time. We conducted extensive experiments to evaluate our framework, validating its high performance and effectiveness in detecting and distinguishing unspecified social events.
</details>
<details>
<summary>摘要</summary>
社交平台已成为散布信息和讨论现实社会事件的重要平台，这提供了研究人员设计和实现新型事件探测框架的优秀机会。然而，大多数现有方法只是利用关键词爆炸或社交网络结构来探测不特定的事件。因此，它们经常无法识别复杂的事件和社会数据中的事件。社会数据，例如微博，具有杂乱不准、缺失、多义词和不规则语言特征，同时也存在意见方面的变化。此外，抽取特征和模式以探测发展事件的限制知识是几乎不可能的。为了解决这些挑战，在本论文中，我们提出了一种新的框架，即EnrichEvent，该框架利用流动社会数据的语言和上下文表示来探测事件。具体来说，我们利用上下文知识以及语言知识来检测相关的微博，从而提高事件探测方法的效iveness。最终，我们的提出的框架生成了每个事件的时间序列链，以示出事件的发展变化。我们进行了广泛的实验来评估我们的框架，并证明其高效性和效iveness在探测和分辨不特定社会事件。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/30/cs.AI_2023_07_30/" data-id="cloh7tqaz001f7b887ydpbuut" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/30/cs.CL_2023_07_30/" class="article-date">
  <time datetime="2023-07-30T11:00:00.000Z" itemprop="datePublished">2023-07-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/30/cs.CL_2023_07_30/">cs.CL - 2023-07-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Private-Watermark-for-Large-Language-Models"><a href="#A-Private-Watermark-for-Large-Language-Models" class="headerlink" title="A Private Watermark for Large Language Models"></a>A Private Watermark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16230">http://arxiv.org/abs/2307.16230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THU-BPM/private_watermark">https://github.com/THU-BPM/private_watermark</a></li>
<li>paper_authors: Aiwei Liu, Leyi Pan, Xuming Hu, Shu’ang Li, Lijie Wen, Irwin King, Philip S. Yu</li>
<li>for: 保护大语言模型生成的文本免遭伪造和版权侵犯</li>
<li>methods: 使用两个不同的神经网络：一个用于水印生成，另一个用于水印检测，而且一部分参数共享两者</li>
<li>results: 实现高检测精度，无需大量参数和计算资源，同时难以从检测网络中提取水印生成规则<details>
<summary>Abstract</summary>
Recently, text watermarking algorithms for large language models (LLMs) have been mitigating the potential harms of text generated by the LLMs, including fake news and copyright issues. However, the watermark detection of current text algorithms requires the key from the generation process, making them susceptible to breaches and counterfeiting. In this work, we propose the first private watermarking algorithm, which extends the current text watermarking algorithms by using two different neural networks respectively for watermark generation and detection, rather than using the same key at both stages. Meanwhile, part of the parameters of the watermark generation and detection networks are shared, which makes the detection network achieve a high accuracy very efficiently. Experiments show that our algorithm ensures high detection accuracy with minimal impact on generation and detection speed, due to the small parameter size of both networks. Additionally, our subsequent analysis demonstrates the difficulty of reverting the watermark generation rules from the detection network.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimizing-the-Neural-Network-Training-for-OCR-Error-Correction-of-Historical-Hebrew-Texts"><a href="#Optimizing-the-Neural-Network-Training-for-OCR-Error-Correction-of-Historical-Hebrew-Texts" class="headerlink" title="Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts"></a>Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16220">http://arxiv.org/abs/2307.16220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smartinternz02/SI-GuidedProject-2307-1622049182">https://github.com/smartinternz02/SI-GuidedProject-2307-1622049182</a></li>
<li>paper_authors: Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet</li>
<li>For: The paper aims to improve the accuracy of Optical Character Recognition (OCR) post-correction for historical documents, specifically for Hebrew texts.* Methods: The paper proposes an innovative method for training a light-weight neural network using significantly less manually created data. The method involves generating language and task-specific training data to improve the neural network results for OCR post-correction.* Results: The paper shows that the proposed method outperforms other state-of-the-art neural networks for OCR post-correction and complex spellcheckers. The results also indicate that the performance of the neural network depends on the genre and area of the training data.Here is the same information in Simplified Chinese text:* For: 这篇论文旨在提高历史文档中的Optical Character Recognition（OCR）后处理的准确率，具体是为希伯来文本。* Methods: 论文提出了一种创新的方法，通过大量地使用自动生成的语言和任务特定的训练数据来提高神经网络的OCR后处理准确率。* Results: 论文表明，提议的方法可以超越其他现有的神经网络和复杂的拼写检查器。结果还表明，神经网络的性能受训练数据的种类和地域的影响。<details>
<summary>Abstract</summary>
Over the past few decades, large archives of paper-based documents such as books and newspapers have been digitized using Optical Character Recognition. This technology is error-prone, especially for historical documents. To correct OCR errors, post-processing algorithms have been proposed based on natural language analysis and machine learning techniques such as neural networks. Neural network's disadvantage is the vast amount of manually labeled data required for training, which is often unavailable. This paper proposes an innovative method for training a light-weight neural network for Hebrew OCR post-correction using significantly less manually created data. The main research goal is to develop a method for automatically generating language and task-specific training data to improve the neural network results for OCR post-correction, and to investigate which type of dataset is the most effective for OCR post-correction of historical documents. To this end, a series of experiments using several datasets was conducted. The evaluation corpus was based on Hebrew newspapers from the JPress project. An analysis of historical OCRed newspapers was done to learn common language and corpus-specific OCR errors. We found that training the network using the proposed method is more effective than using randomly generated errors. The results also show that the performance of the neural network for OCR post-correction strongly depends on the genre and area of the training data. Moreover, neural networks that were trained with the proposed method outperform other state-of-the-art neural networks for OCR post-correction and complex spellcheckers. These results may have practical implications for many digital humanities projects.
</details>
<details>
<summary>摘要</summary>
在过去几十年，大量的纸质文档，如书籍和报纸，已经被数字化使用光学字符识别（OCR）技术。这种技术存在误差，尤其是对历史文档。为了纠正OCR误差，基于自然语言分析和机器学习技术的后处理算法被提议。但是，这些算法需要大量的手动标注数据来训练，而这些数据往往不可得。这篇论文提出了一种创新的方法，用于训练一个轻量级的神经网络，以便进行希伯来文OCR后处理。研究的主要目标是开发一种自动生成语言和任务特定的训练数据，以提高神经网络的OCR后处理结果，并investigate最有效的数据集类型，以便对历史文档进行OCR后处理。为此，我们进行了一系列的实验，使用了多个数据集。评估集基于希伯来报纸的JPress项目。我们对历史OCR后的报纸进行了分析，以了解希伯来文中的常见OCR误差。我们发现，使用我们提出的方法来训练神经网络是比使用随机生成的误差更有效的。结果还表明，神经网络的OCR后处理性能强度取决于训练数据的类型和地区。此外，我们使用我们提出的方法训练的神经网络，与其他现有的神经网络和复杂的拼写检查器相比，表现更好。这些结果可能对数字人文学科项目产生实质性的影响。
</details></li>
</ul>
<hr>
<h2 id="Toward-a-Period-Specific-Optimized-Neural-Network-for-OCR-Error-Correction-of-Historical-Hebrew-Texts"><a href="#Toward-a-Period-Specific-Optimized-Neural-Network-for-OCR-Error-Correction-of-Historical-Hebrew-Texts" class="headerlink" title="Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts"></a>Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16213">http://arxiv.org/abs/2307.16213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</li>
<li>for:  corrected historical documents</li>
<li>methods:  neural networks, OCR error correction</li>
<li>results:  effective OCR post-correction in Hebrew<details>
<summary>Abstract</summary>
Over the past few decades, large archives of paper-based historical documents, such as books and newspapers, have been digitized using the Optical Character Recognition (OCR) technology. Unfortunately, this broadly used technology is error-prone, especially when an OCRed document was written hundreds of years ago. Neural networks have shown great success in solving various text processing tasks, including OCR post-correction. The main disadvantage of using neural networks for historical corpora is the lack of sufficiently large training datasets they require to learn from, especially for morphologically-rich languages like Hebrew. Moreover, it is not clear what are the optimal structure and values of hyperparameters (predefined parameters) of neural networks for OCR error correction in Hebrew due to its unique features. Furthermore, languages change across genres and periods. These changes may affect the accuracy of OCR post-correction neural network models. To overcome these challenges, we developed a new multi-phase method for generating artificial training datasets with OCR errors and hyperparameters optimization for building an effective neural network for OCR post-correction in Hebrew.
</details>
<details>
<summary>摘要</summary>
过去几十年，大量的纸质历史文献，如书籍和报纸，已经被使用光学字符识别（OCR）技术数字化。然而，这种广泛使用的技术有误，特别是当OCRed文档写于数百年前时。神经网络在解决不同的文本处理任务上表现出色，包括OCR后修正。然而，使用神经网络 для历史资料的问题是缺乏足够大的训练数据集，特别是 для morphologically-rich语言如希伯来语。此外，希伯来语的独特特征使得神经网络模型的优化很难。此外，语言随着时代和领域的变化而变化，这些变化可能会影响OCR后修正神经网络模型的准确性。为了解决这些挑战，我们开发了一种新的多阶段方法，用于生成人工的OCR错误数据集和神经网络模型优化，以建立有效的OCR后修正神经网络模型。
</details></li>
</ul>
<hr>
<h2 id="A-Knowledge-enhanced-Two-stage-Generative-Framework-for-Medical-Dialogue-Information-Extraction"><a href="#A-Knowledge-enhanced-Two-stage-Generative-Framework-for-Medical-Dialogue-Information-Extraction" class="headerlink" title="A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction"></a>A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16200">http://arxiv.org/abs/2307.16200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flyingcat-fa/ktgf">https://github.com/flyingcat-fa/ktgf</a></li>
<li>paper_authors: Zefa Hu, Ziyi Ni, Jing Shi, Shuang Xu, Bo Xu</li>
<li>For: 这个论文关注医疗对话中的短语状态对（MD-TSPE）抽取，它是诊断对话系统和电子医疗记录自动scriber的基础。过去几年，关于MD-TSPE的研究吸引了增加的关注，特别是在生成方法的进步之后。但是，这些生成方法输出整个序列，包括短语状态对，而忽略了集成先前知识，需要更深刻地理解短语和状态之间的关系，以及模型短语的推理。* Methods: 本论文提出了知识增强的两个阶段生成框架（KTGF），使用任务特定的提示，我们使用单个模型完成MD-TSPE的两个阶段：首先生成所有短语，然后为每个生成的短语生成状态。这样做的好处是可以更好地从序列中学习短语之间的关系，并且我们设计的知识增强提示在第二阶段可以更好地利用短语的类别和状态候选人选择状态生成。此外，我们提出的特殊状态“未提及”使得更多的短语可用，增加训练数据的质量。* Results: 对于Chunyu和CMDD数据集，我们的提posed方法在全训练和低资源设置下 achieve superior results比之前的状态艺术模型。<details>
<summary>Abstract</summary>
This paper focuses on term-status pair extraction from medical dialogues (MD-TSPE), which is essential in diagnosis dialogue systems and the automatic scribe of electronic medical records (EMRs). In the past few years, works on MD-TSPE have attracted increasing research attention, especially after the remarkable progress made by generative methods. However, these generative methods output a whole sequence consisting of term-status pairs in one stage and ignore integrating prior knowledge, which demands a deeper understanding to model the relationship between terms and infer the status of each term. This paper presents a knowledge-enhanced two-stage generative framework (KTGF) to address the above challenges. Using task-specific prompts, we employ a single model to complete the MD-TSPE through two phases in a unified generative form: we generate all terms the first and then generate the status of each generated term. In this way, the relationship between terms can be learned more effectively from the sequence containing only terms in the first phase, and our designed knowledge-enhanced prompt in the second phase can leverage the category and status candidates of the generated term for status generation. Furthermore, our proposed special status ``not mentioned" makes more terms available and enriches the training data in the second phase, which is critical in the low-resource setting. The experiments on the Chunyu and CMDD datasets show that the proposed method achieves superior results compared to the state-of-the-art models in the full training and low-resource settings.
</details>
<details>
<summary>摘要</summary>
To address these challenges, this paper proposes a knowledge-enhanced two-stage generative framework (KTGF) that uses task-specific prompts to complete MD-TSPE in a unified generative form. The first stage generates all terms, and the second stage generates the status of each generated term. By learning the relationship between terms in the first phase and leveraging category and status candidates in the second phase, our method can generate more accurate term-status pairs. Moreover, our proposed "not mentioned" special status enriches the training data in the second phase, which is critical in low-resource settings.Experiments on the Chunyu and CMDD datasets show that our proposed method outperforms state-of-the-art models in both full training and low-resource settings.
</details></li>
</ul>
<hr>
<h2 id="Improving-TTS-for-Shanghainese-Addressing-Tone-Sandhi-via-Word-Segmentation"><a href="#Improving-TTS-for-Shanghainese-Addressing-Tone-Sandhi-via-Word-Segmentation" class="headerlink" title="Improving TTS for Shanghainese: Addressing Tone Sandhi via Word Segmentation"></a>Improving TTS for Shanghainese: Addressing Tone Sandhi via Word Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16199">http://arxiv.org/abs/2307.16199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edward-martyr/shanghainese-tts">https://github.com/edward-martyr/shanghainese-tts</a></li>
<li>paper_authors: Yuanhao Chen</li>
<li>for: 这篇论文的目的是提高 Shanghainese TTS 模型中的声调混合问题。</li>
<li>methods: 作者使用 word segmentation 技术来增强 TTS 模型对声调混合的表现。特别是在 left-dominant 声调中，使用特殊符号来代表每个词中的声调信息。</li>
<li>results: 作者发现，通过 word segmentation 技术可以提高 TTS 模型对声调混合的表现，并且可以更好地捕捉 Shanghainese 语言的声调特征。这项研究可能成为 Shanghainese 语言计算机化项目的开端。<details>
<summary>Abstract</summary>
Tone is a crucial component of the prosody of Shanghainese, a Wu Chinese variety spoken primarily in urban Shanghai. Tone sandhi, which applies to all multi-syllabic words in Shanghainese, then, is key to natural-sounding speech. Unfortunately, recent work on Shanghainese TTS (text-to-speech) such as Apple's VoiceOver has shown poor performance with tone sandhi, especially LD (left-dominant sandhi). Here I show that word segmentation during text preprocessing can improve the quality of tone sandhi production in TTS models. Syllables within the same word are annotated with a special symbol, which serves as a proxy for prosodic information of the domain of LD. Contrary to the common practice of using prosodic annotation mainly for static pauses, this paper demonstrates that prosodic annotation can also be applied to dynamic tonal phenomena. I anticipate this project to be a starting point for bringing formal linguistic accounts of Shanghainese into computational projects. Too long have we been using the Mandarin models to approximate Shanghainese, but it is a different language with its own linguistic features, and its digitisation and revitalisation should be treated as such.
</details>
<details>
<summary>摘要</summary>
<SYS><TRANSLATE>上海话的拥有者是一种武汉话种，主要在上海城市地区使用。声调推移是上海话的重要成分，但是最近的上海话 Text-to-Speech（TTS）技术，如苹果的voiceover，在声调推移方面表现不佳，特别是左倾推移（LD）。我们表明，在文本处理阶段使用 word segmentation 可以提高 TTS 模型中声调推移质量。在同一个词中的每个音节上使用特殊符号，作为声调信息的代理，以表示声调推移的域。与常见的使用静音注释主要用于静止停顿的情况下，这篇论文表明了可以将静音注释应用到动态声调现象上。我预计这个项目将成为将正式语言学质量的上海话计算机项目的开端。我们已经使用普通话模型来近似上海话很长时间，但是它是一种不同的语言，它有自己的语言特征，我们应该对其数字化和恢复进行正确的待遇。</TRANSLATE></SYS>Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The original text was in Traditional Chinese, which is used in Taiwan and other parts of the world where Traditional Chinese is prevalent.
</details></li>
</ul>
<hr>
<h2 id="Do-LLMs-Possess-a-Personality-Making-the-MBTI-Test-an-Amazing-Evaluation-for-Large-Language-Models"><a href="#Do-LLMs-Possess-a-Personality-Making-the-MBTI-Test-an-Amazing-Evaluation-for-Large-Language-Models" class="headerlink" title="Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models"></a>Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16180">http://arxiv.org/abs/2307.16180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harderthenharder/transformers_tasks">https://github.com/harderthenharder/transformers_tasks</a></li>
<li>paper_authors: Keyu Pan, Yawen Zeng</li>
<li>for:  investigating the feasibility of using the Myers-Briggs Type Indicator (MBTI) as an evaluation metric for large language models (LLMs)</li>
<li>methods:  extensive experiments to explore the personality types of different LLMs, the possibility of changing the personality types by prompt engineering, and the impact of training datasets on the model’s personality</li>
<li>results:  the study aims to determine whether LLMs with human-like abilities possess human-like personalities, and whether the MBTI can serve as a rough indicator of this similarity<details>
<summary>Abstract</summary>
The field of large language models (LLMs) has made significant progress, and their knowledge storage capacity is approaching that of human beings. Furthermore, advanced techniques, such as prompt learning and reinforcement learning, are being employed to address ethical concerns and hallucination problems associated with LLMs, bringing them closer to aligning with human values. This situation naturally raises the question of whether LLMs with human-like abilities possess a human-like personality? In this paper, we aim to investigate the feasibility of using the Myers-Briggs Type Indicator (MBTI), a widespread human personality assessment tool, as an evaluation metric for LLMs. Specifically, extensive experiments will be conducted to explore: 1) the personality types of different LLMs, 2) the possibility of changing the personality types by prompt engineering, and 3) How does the training dataset affect the model's personality. Although the MBTI is not a rigorous assessment, it can still reflect the similarity between LLMs and human personality. In practice, the MBTI has the potential to serve as a rough indicator. Our codes are available at https://github.com/HarderThenHarder/transformers_tasks/tree/main/LLM/llms_mbti.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）领域已经做出了重大进步，它们的知识储存能力接近人类水平。此外，高级技术，如提示学习和复杂学习，也在实施以解决伦理性和幻觉问题，使得 LLM 更接近人类价值观。这种情况自然地引起了问题： LLM 是否拥有人类式的人格？在这篇文章中，我们将 investigate 使用 Myers-Briggs Type Indicator（MBTI），一个广泛应用于人类人格评估工具，来评估 LLM 的可能性。具体来说，我们将进行大量的实验，以探索：1）不同 LLM 的人格型态，2）提示工程学可以改变 LLM 的人格型态，3）训练数据库对模型的人格影响。虽然 MBTI 不是一个正式的评估工具，但它仍然可以反映 LLM 与人类人格之间的相似性。在实践中，MBTI 有可能作为一个简单的指标。我们的代码可以在 GitHub 上找到：https://github.com/HarderThenHarder/transformers_tasks/tree/main/LLM/llms_mbti。
</details></li>
</ul>
<hr>
<h2 id="SEED-Bench-Benchmarking-Multimodal-LLMs-with-Generative-Comprehension"><a href="#SEED-Bench-Benchmarking-Multimodal-LLMs-with-Generative-Comprehension" class="headerlink" title="SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"></a>SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16125">http://arxiv.org/abs/2307.16125</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ailab-cvc/seed-bench">https://github.com/ailab-cvc/seed-bench</a></li>
<li>paper_authors: Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan</li>
<li>for: 本研究旨在评估多元语言模型（MLLMs）的生成理解能力，作为评估生成模型的首要步骤，并提供一个名为SEED-Bench的benchmark。</li>
<li>methods: 本研究使用了一个高级的生成管道，包括自动筛选和人工验证过程，以生成多个选项问题，以覆盖12个评估维度，包括图像和视频模式的理解。</li>
<li>results: 本研究对18个模型进行了全面的评估，并发现了现有MLLMs的限制，以及它们在不同的维度上的表现。这些结果可以为未来的研究提供指导，并为社区提供一个平台来评估和调查模型能力。<details>
<summary>Abstract</summary>
Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Proposing-a-conceptual-framework-social-media-listening-for-public-health-behavior"><a href="#Proposing-a-conceptual-framework-social-media-listening-for-public-health-behavior" class="headerlink" title="Proposing a conceptual framework: social media listening for public health behavior"></a>Proposing a conceptual framework: social media listening for public health behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02037">http://arxiv.org/abs/2308.02037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shu-Feng Tsao, Helen Chen, Samantha Meyer, Zahid A. Butt<br>for: This study aims to propose a novel conceptual framework for misinformation research using social media data and natural language processing techniques, with a focus on understanding public discourse on social media and its impact on public health behavior.methods: The study uses a literature review to analyze and critique existing theories and models used in COVID-19 related studies, and proposes a new conceptual framework that integrates important attributes of existing theories and adds new attributes. The proposed framework is demonstrated through a case study of the Freedom Convoy social media listening.results: The proposed conceptual framework can be used to better understand public discourse on social media and its impact on public health behavior, and can be integrated with other data analyses to gather a more comprehensive picture. The framework is flexible and can be revised and adopted as health misinformation evolves.<details>
<summary>Abstract</summary>
Existing communications and behavioral theories have been adopted to address health misinformation. Although various theories and models have been used to investigate the COVID-19 pandemic, there is no framework specially designed for social listening or misinformation studies using social media data and natural language processing techniques. This study aimed to propose a novel yet theory-based conceptual framework for misinformation research. We collected theories and models used in COVID-19 related studies published in peer-reviewed journals. The theories and models ranged from health behaviors, communications, to misinformation. They are analyzed and critiqued for their components, followed by proposing a conceptual framework with a demonstration. We reviewed Health Belief Model, Theory of Planned Behavior/Reasoned Action, Communication for Behavioral Impact, Transtheoretical Model, Uses and Gratifications Theory, Social Judgment Theory, Risk Information Seeking and Processing Model, Behavioral and Social Drivers, and Hype Loop. Accordingly, we proposed the Social Media Listening for Public Health Behavior Conceptual Framework by not only integrating important attributes of existing theories, but also adding new attributes. The proposed conceptual framework was demonstrated in the Freedom Convoy social media listening. The proposed conceptual framework can be used to better understand public discourse on social media, and it can be integrated with other data analyses to gather a more comprehensive picture. The framework will continue to be revised and adopted as health misinformation evolves.
</details>
<details>
<summary>摘要</summary>
现有的交流和行为理论已经应用于健康谣言研究中，但是没有专门为社交媒体数据和自然语言处理技术设计的框架。本研究的目的是提议一个新的 yet theory-based 概念框架 для谣言研究。我们收集了在科学期刊上发表的COVID-19相关研究中使用的理论和模型，包括健康行为模型、沟通行为模型、谣言模型等。我们分析和评价这些理论和模型的组成部分，然后提出了一个概念框架，并进行了示例。我们审查了健康信念模型、计划行为理论/逻辑行为理论、沟通对行为的影响、变革模型、用途和满足理论、社会评价理论、风险信息搜索和处理模型、行为和社会驱动力等理论。根据这些理论的重要属性，我们提出了社交媒体听取为公共卫生行为概念框架。这个框架不仅 integrates 重要的现有理论属性，还添加了新的属性。我们在自由征voyage社交媒体听取中进行了示例。这个框架可以用来更好地理解社交媒体上的公共讨论，并可以与其他数据分析结合以获得更全面的图像。这个框架将继续更新和采纳，随着健康谣言的发展。
</details></li>
</ul>
<hr>
<h2 id="Roll-Up-Your-Sleeves-Working-with-a-Collaborative-and-Engaging-Task-Oriented-Dialogue-System"><a href="#Roll-Up-Your-Sleeves-Working-with-a-Collaborative-and-Engaging-Task-Oriented-Dialogue-System" class="headerlink" title="Roll Up Your Sleeves: Working with a Collaborative and Engaging Task-Oriented Dialogue System"></a>Roll Up Your Sleeves: Working with a Collaborative and Engaging Task-Oriented Dialogue System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16081">http://arxiv.org/abs/2307.16081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingbo Mo, Shijie Chen, Ziru Chen, Xiang Deng, Ashley Lewis, Sunit Singh, Samuel Stevens, Chang-You Tai, Zhen Wang, Xiang Yue, Tianshu Zhang, Yu Su, Huan Sun</li>
<li>for: 论文主要目标是开发一个用户中心的任务强调对话系统，帮助用户完成复杂的真实世界任务。</li>
<li>methods: 论文使用语言理解、对话管理和响应生成组件，以及一个强大的搜索引擎，以提供高效的任务协助。在增强对话体验方面，论文探讨了一系列的数据扩充策略，使用LLMs训练进阶 нейрон网络。</li>
<li>results: 论文通过Alexa Prize TaskBot Challenge中的成功参赛，证明了TACOBot在完成 cooking 和 how-to 类任务方面的效果。此外，论文还提供了一个开源框架，用于实现任务强调对话系统的部署。<details>
<summary>Abstract</summary>
We introduce TacoBot, a user-centered task-oriented digital assistant designed to guide users through complex real-world tasks with multiple steps. Covering a wide range of cooking and how-to tasks, we aim to deliver a collaborative and engaging dialogue experience. Equipped with language understanding, dialogue management, and response generation components supported by a robust search engine, TacoBot ensures efficient task assistance. To enhance the dialogue experience, we explore a series of data augmentation strategies using LLMs to train advanced neural models continuously. TacoBot builds upon our successful participation in the inaugural Alexa Prize TaskBot Challenge, where our team secured third place among ten competing teams. We offer TacoBot as an open-source framework that serves as a practical example for deploying task-oriented dialogue systems.
</details>
<details>
<summary>摘要</summary>
我们介绍TacoBot，一个用户中心的任务导向的数位助手，旨在帮助用户完成复杂的现实世界任务，这些任务通常有多步骤。TacoBot 覆盖了厨艺和如何进行任务的广泛领域，我们目标是提供一个协力和有趣的对话体验。TacoBot 搭配了语言理解、对话管理和回应生成的 комponents，这些 комponents 由一个强大的搜索引擎支持。为了增强对话体验，我们探索了一系列的数据增强策略，使用LLMs训练进阶的神经网络模型。TacoBot 基于我们在Alexa Prize TaskBot Challenge的成功参赛经验，我们的队伍在十支队伍中排名第三。我们提供TacoBot 作为一个开源框架，作为实际的部署任务对话系统的示范。
</details></li>
</ul>
<hr>
<h2 id="IroyinSpeech-A-multi-purpose-Yoruba-Speech-Corpus"><a href="#IroyinSpeech-A-multi-purpose-Yoruba-Speech-Corpus" class="headerlink" title="ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus"></a>ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16071">http://arxiv.org/abs/2307.16071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, David Ifeoluwa Adelani</li>
<li>for: 提高现代尼日利亚语言讲话质量的数据集</li>
<li>methods: 使用新闻和创作域的文本句子，并由多个说话者录音</li>
<li>results: 提供38.5小时的数据集，来自80名志愿者的录音<details>
<summary>Abstract</summary>
We introduce the \`{I}r\`{o}y\`{i}nSpeech corpus -- a new dataset influenced by a desire to increase the amount of high quality, freely available, contemporary Yor\`{u}b\'{a} speech. We release a multi-purpose dataset that can be used for both TTS and ASR tasks. We curated text sentences from the news and creative writing domains under an open license i.e., CC-BY-4.0 and had multiple speakers record each sentence. We provide 5000 of our utterances to the Common Voice platform to crowdsource transcriptions online. The dataset has 38.5 hours of data in total, recorded by 80 volunteers.
</details>
<details>
<summary>摘要</summary>
我们介绍《IroyinSpeech》 corpus -- 一个新的数据集，受到了提高现代尤布语言质量、可以免费使用的需求的影响。我们发布了多用途的数据集，可以用于 TTS 和 ASR 任务。我们从新闻和创作领域中选取了 CC-BY-4.0 开源许可证下的文本句子，并有多个说话者录制每句话。我们提供了5000个音频记录给 Common Voice 平台，以便在线受托写 транскрипт。总共有38.5小时的数据，记录了80名志愿者。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Extraction-of-the-Romanian-Academic-Word-List-Data-and-Methods"><a href="#Automatic-Extraction-of-the-Romanian-Academic-Word-List-Data-and-Methods" class="headerlink" title="Automatic Extraction of the Romanian Academic Word List: Data and Methods"></a>Automatic Extraction of the Romanian Academic Word List: Data and Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16045">http://arxiv.org/abs/2307.16045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bucuram/ro-awl">https://github.com/bucuram/ro-awl</a></li>
<li>paper_authors: Ana-Maria Bucur, Andreea Dincă, Mădălina Chitez, Roxana Rogobete</li>
<li>for: 这篇论文是为了自动提取罗马尼亚学术词汇列表（Ro-AWL）的方法和数据。</li>
<li>methods: 这篇论文使用了 corpus 和计算语言学的方法，以及 L2 学习Contexts 的 Writing 方法，将数据组合在一起生成 Ro-AWL。</li>
<li>results: 研究人员通过对两种数据进行组合，包括现有的 Romanian Frequency List 和自编的 Expert Academic Writing Corpus EXPRES，成功地生成了 Ro-AWL，并且其分布特征（总分布、PART-OF-SPEECH 分布）与先前的研究相符。<details>
<summary>Abstract</summary>
This paper presents the methodology and data used for the automatic extraction of the Romanian Academic Word List (Ro-AWL). Academic Word Lists are useful in both L2 and L1 teaching contexts. For the Romanian language, no such resource exists so far. Ro-AWL has been generated by combining methods from corpus and computational linguistics with L2 academic writing approaches. We use two types of data: (a) existing data, such as the Romanian Frequency List based on the ROMBAC corpus, and (b) self-compiled data, such as the expert academic writing corpus EXPRES. For constructing the academic word list, we follow the methodology for building the Academic Vocabulary List for the English language. The distribution of Ro-AWL features (general distribution, POS distribution) into four disciplinary datasets is in line with previous research. Ro-AWL is freely available and can be used for teaching, research and NLP applications.
</details>
<details>
<summary>摘要</summary>
这个论文介绍了自动提取罗马尼亚学术词汇列表（Ro-AWL）的方法和数据。学术词汇列表在L2和L1教学上都是有用的资源。为罗马尼亚语，目前没有相关资源。Ro-AWL通过将核心语言学和计算语言学方法与L2学术写作方法结合起来生成。我们使用两种数据：（a）现有数据，如罗马尼亚频率列表基于ROMBAC corpus，和（b）自制数据，如专家学术写作 corpus EXPRES。为构建学术词汇列表，我们遵循了英语学术词汇列表的建立方法。Ro-AWL的分布特征（总分布、POS分布）在四个学科数据集中与先前研究一致。Ro-AWL公开提供，可以用于教学、研究和NLP应用。
</details></li>
</ul>
<hr>
<h2 id="Okapi-Instruction-tuned-Large-Language-Models-in-Multiple-Languages-with-Reinforcement-Learning-from-Human-Feedback"><a href="#Okapi-Instruction-tuned-Large-Language-Models-in-Multiple-Languages-with-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback"></a>Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16039">http://arxiv.org/abs/2307.16039</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nlp-uoregon/okapi">https://github.com/nlp-uoregon/okapi</a></li>
<li>paper_authors: Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen<br>for:The paper is written to explore instruction tuning for large language models (LLMs) in multiple languages, with a focus on reinforcement learning from human feedback (RLHF) as an alternative approach to supervised fine-tuning (SFT).methods:The paper uses RLHF to instruction-tune LLMs for multiple languages, introducing instruction and response-ranked data in 26 diverse languages to facilitate the experiments.results:The paper demonstrates the advantages of RLHF for multilingual instruction over SFT for different base models and datasets, and releases the framework and resources at <a target="_blank" rel="noopener" href="https://github.com/nlp-uoregon/Okapi">https://github.com/nlp-uoregon/Okapi</a>.<details>
<summary>Abstract</summary>
A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi.
</details>
<details>
<summary>摘要</summary>
具有大语言模型（LLM）的发展的关键技术之一是指令调整，帮助模型的回答与人类期望保持一致，从而实现了惊人的学习能力。目前最流行的两种方法 для实现指令调整是超级精度微调（SFT）和人类反馈学习（RLHF）。为了提高LLM的可访问性，各种指令调整的开源LLM也在不断发布，如Alpaca和Vicuna等。然而，现有的开源LLM仅仅对英语和一些流行语言进行了指令调整，这限制了它们在全球各语言中的影响和可用性。在最近几年中，一些研究已经开始探索LLM在多种语言上的指令调整，但是这些研究仅仅使用SFT进行指令调整。这留下了一个大的空白，即RLHF如何在多语言上提高指令调整的性能。为了解决这个问题，我们介绍了Okapi，第一个基于RLHF的多语言指令调整系统。Okapi在26种多样化的语言中提供了指令和回答排名数据，以便实验和未来多语言LLM研究的发展。我们还提供了多语言生成LLM的评价数据集。我们的实验表明，RLHF在多语言指令调整中具有优势，不同的基本模型和数据集上。我们的框架和资源在https://github.com/nlp-uoregon/Okapi上发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/30/cs.CL_2023_07_30/" data-id="cloh7tqd600877b88bj1rhooj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/30/cs.LG_2023_07_30/" class="article-date">
  <time datetime="2023-07-30T10:00:00.000Z" itemprop="datePublished">2023-07-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/30/cs.LG_2023_07_30/">cs.LG - 2023-07-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Federated-Learning-via-Local-Adaptive-Amended-Optimizer-with-Linear-Speedup"><a href="#Efficient-Federated-Learning-via-Local-Adaptive-Amended-Optimizer-with-Linear-Speedup" class="headerlink" title="Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup"></a>Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00522">http://arxiv.org/abs/2308.00522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Sun, Li Shen, Hao Sun, Liang Ding, Dacheng Tao</li>
<li>for: 这 paper 的目的是提出一种基于 momentum 的 Federated Learning 算法，以解决分布式学习中的 rugged convergence 和客户端漂移问题。</li>
<li>methods: 该 paper 使用了一种名为 Federated Local ADaptive Amended optimizer（FedLADA），它将 global gradient descent 和 local adaptive amended optimizer 相结合，通过在前一个通信回合中估计全局平均偏移量，并通过一个 momentum-like 项来更好地改进实际训练速度和缓解不同客户端的过拟合。</li>
<li>results: 该 paper 的实验结果表明，使用 FedLADA 可以大幅减少通信回合数和实现更高的准确率，比如基于几个基elines的基elines。<details>
<summary>Abstract</summary>
Adaptive optimization has achieved notable success for distributed learning while extending adaptive optimizer to federated Learning (FL) suffers from severe inefficiency, including (i) rugged convergence due to inaccurate gradient estimation in global adaptive optimizer; (ii) client drifts exacerbated by local over-fitting with the local adaptive optimizer. In this work, we propose a novel momentum-based algorithm via utilizing the global gradient descent and locally adaptive amended optimizer to tackle these difficulties. Specifically, we incorporate a locally amended technique to the adaptive optimizer, named Federated Local ADaptive Amended optimizer (\textit{FedLADA}), which estimates the global average offset in the previous communication round and corrects the local offset through a momentum-like term to further improve the empirical training speed and mitigate the heterogeneous over-fitting. Theoretically, we establish the convergence rate of \textit{FedLADA} with a linear speedup property on the non-convex case under the partial participation settings. Moreover, we conduct extensive experiments on the real-world dataset to demonstrate the efficacy of our proposed \textit{FedLADA}, which could greatly reduce the communication rounds and achieves higher accuracy than several baselines.
</details>
<details>
<summary>摘要</summary>
适应优化在分布式学习中获得了显著的成功，但扩展适应优化到联邦学习（FL）中受到严重的不稳定性困扰，包括（i）粗糙的收敛 due to 不准确的梯度估计在全局适应优化器中;（ii）客户端漂移加剧由本地适应优化器引起的本地过拟合。在这种情况下，我们提出了一种新的慢速逻辑算法，通过利用全局梯度下降和本地适应修正优化器来解决这些困难。 Specifically, we incorporate a locally amended technique to the adaptive optimizer, named Federated Local ADaptive Amended optimizer (\textit{FedLADA}), which estimates the global average offset in the previous communication round and corrects the local offset through a momentum-like term to further improve the empirical training speed and mitigate the heterogeneous over-fitting. 理论上，我们确立了\textit{FedLADA}的收敛率在非对称 случа下的线性快速性质。此外，我们在真实世界数据集上进行了广泛的实验，以证明我们提出的\textit{FedLADA}可以减少通信圈数并达到更高的准确率，比许多基eline的性能更高。
</details></li>
</ul>
<hr>
<h2 id="DRL4Route-A-Deep-Reinforcement-Learning-Framework-for-Pick-up-and-Delivery-Route-Prediction"><a href="#DRL4Route-A-Deep-Reinforcement-Learning-Framework-for-Pick-up-and-Delivery-Route-Prediction" class="headerlink" title="DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction"></a>DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16246">http://arxiv.org/abs/2307.16246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maoxiaowei97/drl4route">https://github.com/maoxiaowei97/drl4route</a></li>
<li>paper_authors: Xiaowei Mao, Haomin Wen, Hengrui Zhang, Huaiyu Wan, Lixia Wu, Jianbin Zheng, Haoyuan Hu, Youfang Lin</li>
<li>for: 预测劳务者的服务路线，提高快递服务质量和效率。</li>
<li>methods: 基于强化学习框架， combining 深度学习模型的行为学习能力和强化学习的非导数目标优化能力。</li>
<li>results: 在实际数据集上，对比既有方法，DRL4Route-GAE 提高了 Location Square Deviation (LSD) 和 Accuracy@3 (ACC@3) 的值，具体提高了 0.9%-2.7% 和 2.4%-3.2%。<details>
<summary>Abstract</summary>
Pick-up and Delivery Route Prediction (PDRP), which aims to estimate the future service route of a worker given his current task pool, has received rising attention in recent years. Deep neural networks based on supervised learning have emerged as the dominant model for the task because of their powerful ability to capture workers' behavior patterns from massive historical data. Though promising, they fail to introduce the non-differentiable test criteria into the training process, leading to a mismatch in training and test criteria. Which considerably trims down their performance when applied in practical systems. To tackle the above issue, we present the first attempt to generalize Reinforcement Learning (RL) to the route prediction task, leading to a novel RL-based framework called DRL4Route. It combines the behavior-learning abilities of previous deep learning models with the non-differentiable objective optimization ability of reinforcement learning. DRL4Route can serve as a plug-and-play component to boost the existing deep learning models. Based on the framework, we further implement a model named DRL4Route-GAE for PDRP in logistic service. It follows the actor-critic architecture which is equipped with a Generalized Advantage Estimator that can balance the bias and variance of the policy gradient estimates, thus achieving a more optimal policy. Extensive offline experiments and the online deployment show that DRL4Route-GAE improves Location Square Deviation (LSD) by 0.9%-2.7%, and Accuracy@3 (ACC@3) by 2.4%-3.2% over existing methods on the real-world dataset.
</details>
<details>
<summary>摘要</summary>
picked-up 和交付路线预测（PDRP）在最近几年内 Received rising attention，目的是计算工作者当前任务池的未来服务路线。基于supervised learning的深度神经网络在任务中 Emerged as the dominant model，因为它们可以很好地捕捉工作者的行为模式从大量历史数据中。虽然有前景，但它们无法将不对数据进行梯度下降的测试标准引入到训练过程中，导致训练和测试标准之间的匹配性异常低。为解决这一问题，我们提出了将Reinforcement Learning（RL）应用于路线预测任务，并提出了一个基于RL的框架 called DRL4Route。这个框架结合了以前的深度学习模型中的行为学习能力和RL的非梯度优化能力。DRL4Route可以作为现有的深度学习模型的插件，以提高其性能。基于此框架，我们进一步实现了一个名为DRL4Route-GAE的模型，用于PDRP在物流服务中。这个模型采用actor-critic架构，并配备一个Generalized Advantage Estimator，可以平衡策略梯度估计的偏好和方差，从而实现更优化的策略。经过大量的离线实验和在线部署，我们发现DRL4Route-GAE可以在实际数据上提高Location Square Deviation（LSD）和Accuracy@3（ACC@3）的值，相比 existed 方法，LSD提高0.9%-2.7%，ACC@3提高2.4%-3.2%。
</details></li>
</ul>
<hr>
<h2 id="Synaptic-Plasticity-Models-and-Bio-Inspired-Unsupervised-Deep-Learning-A-Survey"><a href="#Synaptic-Plasticity-Models-and-Bio-Inspired-Unsupervised-Deep-Learning-A-Survey" class="headerlink" title="Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey"></a>Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16236">http://arxiv.org/abs/2307.16236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato</li>
<li>for: 本文探讨了基于深度学习的新技术，以及它们在人工智能领域中的应用和挑战。</li>
<li>methods: 本文描述了一些基于生物机制的深度学习模型，包括synaptic plasticity模型和脉冲神经网络（SNNs）模型。</li>
<li>results: 本文总结了这些生物启发的深度学习模型在不同场景下的应用和效果，并指出了这些模型在人工智能领域的潜在发展前景。<details>
<summary>Abstract</summary>
Recently emerged technologies based on Deep Learning (DL) achieved outstanding results on a variety of tasks in the field of Artificial Intelligence (AI). However, these encounter several challenges related to robustness to adversarial inputs, ecological impact, and the necessity of huge amounts of training data. In response, researchers are focusing more and more interest on biologically grounded mechanisms, which are appealing due to the impressive capabilities exhibited by biological brains. This survey explores a range of these biologically inspired models of synaptic plasticity, their application in DL scenarios, and the connections with models of plasticity in Spiking Neural Networks (SNNs). Overall, Bio-Inspired Deep Learning (BIDL) represents an exciting research direction, aiming at advancing not only our current technologies but also our understanding of intelligence.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文版本，如果您需要正式的中文版本，请勿使用这个版本）Recently emerged Deep Learning (DL) technologies have achieved remarkable results in various Artificial Intelligence (AI) tasks, but they also face challenges such as robustness to adversarial inputs, ecological impact, and the need for large amounts of training data. In response, researchers are increasingly interested in biologically grounded mechanisms, which are attractive due to the impressive capabilities of biological brains. This survey explores a range of biologically inspired models of synaptic plasticity, their applications in DL scenarios, and connections with models of plasticity in Spiking Neural Networks (SNNs). Overall, Bio-Inspired Deep Learning (BIDL) is an exciting research direction that aims to advance not only our current technologies but also our understanding of intelligence.
</details></li>
</ul>
<hr>
<h2 id="Spiking-Neural-Networks-and-Bio-Inspired-Supervised-Deep-Learning-A-Survey"><a href="#Spiking-Neural-Networks-and-Bio-Inspired-Supervised-Deep-Learning-A-Survey" class="headerlink" title="Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey"></a>Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16235">http://arxiv.org/abs/2307.16235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato</li>
<li>for:  This survey provides a comprehensive review of recent biologically-inspired approaches for Artificial Intelligence (AI) technologies, with a focus on Spiking Neural Network (SNN) models and bio-inspired training methods.</li>
<li>methods: The survey discusses SNN models and their challenges, as well as bio-inspired training methods that pose alternatives to traditional backprop-based optimization. These methods aim to advance the computational capabilities and biological plausibility of current models.</li>
<li>results: The survey provides a thorough presentation of recent biologically-inspired approaches for AI, including SNN models and bio-inspired training methods. These approaches aim to improve the computational capabilities and biological plausibility of current AI models.<details>
<summary>Abstract</summary>
For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.
</details>
<details>
<summary>摘要</summary>
For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.Here's the text in Simplified Chinese characters: For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.
</details></li>
</ul>
<hr>
<h2 id="Robust-Electric-Vehicle-Balancing-of-Autonomous-Mobility-On-Demand-System-A-Multi-Agent-Reinforcement-Learning-Approach"><a href="#Robust-Electric-Vehicle-Balancing-of-Autonomous-Mobility-On-Demand-System-A-Multi-Agent-Reinforcement-Learning-Approach" class="headerlink" title="Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach"></a>Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16228">http://arxiv.org/abs/2307.16228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sihong He, Shuo Han, Fei Miao<br>for:The paper is written for electric autonomous vehicles (EAVs) in future autonomous mobility-on-demand (AMoD) systems, with the goal of designing an integrated vehicle balancing solution that can handle supply and demand uncertainties.methods:The paper uses multi-agent reinforcement learning (MARL) to model both the EAVs supply and mobility demand uncertainties, and proposes a robust E-AMoD Balancing MARL (REBAMA) algorithm to train a robust EAVs balancing policy that can balance both the supply-demand ratio and charging utilization rate across the whole city.results:The proposed robust method improves the reward, charging utilization fairness, and supply-demand fairness compared to a non-robust MARL method and a robust optimization-based method. Specifically, the proposed method improves the reward by 19.28%, charging utilization fairness by 28.18%, and supply-demand fairness by 3.97%, compared to the non-robust MARL method. Compared to the robust optimization-based method, the proposed MARL algorithm improves the reward by 8.21%, charging utilization fairness by 8.29%, and supply-demand fairness by 9.42%.<details>
<summary>Abstract</summary>
Electric autonomous vehicles (EAVs) are getting attention in future autonomous mobility-on-demand (AMoD) systems due to their economic and societal benefits. However, EAVs' unique charging patterns (long charging time, high charging frequency, unpredictable charging behaviors, etc.) make it challenging to accurately predict the EAVs supply in E-AMoD systems. Furthermore, the mobility demand's prediction uncertainty makes it an urgent and challenging task to design an integrated vehicle balancing solution under supply and demand uncertainties. Despite the success of reinforcement learning-based E-AMoD balancing algorithms, state uncertainties under the EV supply or mobility demand remain unexplored. In this work, we design a multi-agent reinforcement learning (MARL)-based framework for EAVs balancing in E-AMoD systems, with adversarial agents to model both the EAVs supply and mobility demand uncertainties that may undermine the vehicle balancing solutions. We then propose a robust E-AMoD Balancing MARL (REBAMA) algorithm to train a robust EAVs balancing policy to balance both the supply-demand ratio and charging utilization rate across the whole city. Experiments show that our proposed robust method performs better compared with a non-robust MARL method that does not consider state uncertainties; it improves the reward, charging utilization fairness, and supply-demand fairness by 19.28%, 28.18%, and 3.97%, respectively. Compared with a robust optimization-based method, the proposed MARL algorithm can improve the reward, charging utilization fairness, and supply-demand fairness by 8.21%, 8.29%, and 9.42%, respectively.
</details>
<details>
<summary>摘要</summary>
电动自动车 (EAV) 在未来自动移动需求 (AMoD) 系统中吸引了关注，因为它们具有经济和社会的好处。然而，EAV 的充电特点 (长充电时间、高充电频率、不可预测的充电行为等) 使得预测 EAV 供应很困难。此外，移动需求的预测不确定性使得设计一个集成的车辆均衡解决方案变得非常困难和挑战性。虽然激励学习基于 E-AMoD 均衡算法得到了成功，但是状态不确定性以及 EV 供应或移动需求的预测不确定性尚未得到探讨。在这种情况下，我们设计了一个多代理激励学习 (MARL) 基础的框架，用于 EAV 均衡解决方案。我们在这个框架中引入了对 EAV 供应和移动需求不确定性的模型，以便模拟这些不确定性的影响。我们then propose a robust E-AMoD Balancing MARL (REBAMA) algorithm to train a robust EAVs balancing policy to balance both the supply-demand ratio and charging utilization rate across the whole city.实验表明，我们提出的方法比非robust MARL 方法更好，可以提高奖励、充电利用公平性和供应需求公平性的表现。与一种robust优化基础的方法进行比较，我们的方法可以提高奖励、充电利用公平性和供应需求公平性的表现。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-the-Neural-Network-Training-for-OCR-Error-Correction-of-Historical-Hebrew-Texts"><a href="#Optimizing-the-Neural-Network-Training-for-OCR-Error-Correction-of-Historical-Hebrew-Texts" class="headerlink" title="Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts"></a>Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16220">http://arxiv.org/abs/2307.16220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smartinternz02/SI-GuidedProject-2307-1622049182">https://github.com/smartinternz02/SI-GuidedProject-2307-1622049182</a></li>
<li>paper_authors: Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet</li>
<li>For: This paper aims to improve the accuracy of Optical Character Recognition (OCR) post-correction for historical documents by developing a method for automatically generating language and task-specific training data.* Methods: The proposed method uses a light-weight neural network and significantly less manually created data to correct OCR errors in Hebrew newspapers. The method is based on natural language analysis and machine learning techniques such as neural networks.* Results: The proposed method outperforms other state-of-the-art neural networks and complex spellcheckers for OCR post-correction, and the performance of the neural network depends on the genre and area of the training data.<details>
<summary>Abstract</summary>
Over the past few decades, large archives of paper-based documents such as books and newspapers have been digitized using Optical Character Recognition. This technology is error-prone, especially for historical documents. To correct OCR errors, post-processing algorithms have been proposed based on natural language analysis and machine learning techniques such as neural networks. Neural network's disadvantage is the vast amount of manually labeled data required for training, which is often unavailable. This paper proposes an innovative method for training a light-weight neural network for Hebrew OCR post-correction using significantly less manually created data. The main research goal is to develop a method for automatically generating language and task-specific training data to improve the neural network results for OCR post-correction, and to investigate which type of dataset is the most effective for OCR post-correction of historical documents. To this end, a series of experiments using several datasets was conducted. The evaluation corpus was based on Hebrew newspapers from the JPress project. An analysis of historical OCRed newspapers was done to learn common language and corpus-specific OCR errors. We found that training the network using the proposed method is more effective than using randomly generated errors. The results also show that the performance of the neural network for OCR post-correction strongly depends on the genre and area of the training data. Moreover, neural networks that were trained with the proposed method outperform other state-of-the-art neural networks for OCR post-correction and complex spellcheckers. These results may have practical implications for many digital humanities projects.
</details>
<details>
<summary>摘要</summary>
在过去几十年，大量的纸质文档，如书籍和报纸，已经被数字化使用光学字符识别（OCR）技术。这种技术存在误差，尤其是对历史文档。为了修正OCR错误，基于自然语言分析和机器学习技术的后处理算法已经被提出。然而，这些算法需要大量的手动标注数据，却经常不可获得。这篇论文提出了一种创新的方法，使用较少的手动创建数据来训练轻量级的神经网络进行希伯来文OCR后处理。研究的主要目标是开发一种自动生成语言和任务特定的训练数据，以提高神经网络的OCR后处理效果，并investigate历史文档OCR后处理中哪种数据集是最有效的。为此，我们进行了一系列实验，使用了多个数据集。评估集基于希伯来报纸JPress项目。我们分析了历史OCR后的报纸，了解希伯来文OCR后处理中的常见语言和核心错误。我们发现，使用我们提出的方法训练神经网络是比使用随机生成错误更有效的。结果还表明，神经网络的OCR后处理效果强度取决于训练数据的类别和地区。此外，使用我们提出的方法训练的神经网络，超过了当前最佳的神经网络和复杂的拼写检查器。这些结果可能对许多数字人文项目产生实质性的影响。
</details></li>
</ul>
<hr>
<h2 id="Improving-Probabilistic-Bisimulation-for-MDPs-Using-Machine-Learning"><a href="#Improving-Probabilistic-Bisimulation-for-MDPs-Using-Machine-Learning" class="headerlink" title="Improving Probabilistic Bisimulation for MDPs Using Machine Learning"></a>Improving Probabilistic Bisimulation for MDPs Using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02519">http://arxiv.org/abs/2308.02519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadsadegh Mohaghegh, Khayyam Salehi</li>
<li>for: 本文旨在应用形式验证技术来分析复杂系统，但是遇到了状态空间爆炸问题。</li>
<li>methods: 本文使用 bisimulation 减少模型状态数量，以解决状态空间爆炸问题。在涉及杂Event-driven系统时，使用概率 bisimulation 来减少模型的状态数量。</li>
<li>results: 本文提出一种新的方法，使用 PRISM 程序和机器学习分类技术来partition 模型的状态空间。实验结果显示，该方法可以减少运行时间相比之前的工具。<details>
<summary>Abstract</summary>
The utilization of model checking has been suggested as a formal verification technique for analyzing critical systems. However, the primary challenge in applying to complex systems is state space explosion problem. To address this issue, bisimulation minimization has emerged as a prominent method for reducing the number of states in a labeled transition system, aiming to overcome the difficulties associated with the state space explosion problem. In the case of systems exhibiting stochastic behaviors, probabilistic bisimulation is employed to minimize a given model, obtaining its equivalent form with fewer states. Recently, various techniques have been introduced to decrease the time complexity of the iterative methods used to compute probabilistic bisimulation for stochastic systems that display nondeterministic behaviors. In this paper, we propose a new technique to partition the state space of a given probabilistic model to its bisimulation classes. This technique uses the PRISM program of a given model and constructs some small versions of the model to train a classifier. It then applies machine learning classification techniques to approximate the related partition. The resulting partition is used as an initial one for the standard bisimulation technique in order to reduce the running time of the method. The experimental results show that the approach can decrease significantly the running time compared to state-of-the-art tools.
</details>
<details>
<summary>摘要</summary>
utilization of model checking 被建议作为形式验证技术来分析关键系统。然而，主要挑战在应用于复杂系统时是状态空间爆炸问题。为解决这个问题， bisimulation minimization emerged as a prominent method for reducing the number of states in a labeled transition system, aiming to overcome the difficulties associated with the state space explosion problem。在系统展现杂次性行为时， probabilistic bisimulation 被使用来最小化给定模型，从而获得 fewer states 的等价形式。最近， various techniques have been introduced to decrease the time complexity of the iterative methods used to compute probabilistic bisimulation for stochastic systems that display nondeterministic behaviors。在这篇论文中，我们提出了一种新的方法，用于将 givens 模型的状态空间 partition 到其 bisimulation classes。这种方法使用 PRISM 程序的 givens 模型，并将其转换为一些小版本的模型，以训练一个类ifier。然后，通过机器学习分类技术来approximate 相关的 partition。 obtained 的 partition 被用作标准 bisimulation 技术的初始 partition，以降低方法的运行时间。实验结果表明，该方法可以在比较器与当前工具之间减少运行时间。
</details></li>
</ul>
<hr>
<h2 id="Text-Analysis-Using-Deep-Neural-Networks-in-Digital-Humanities-and-Information-Science"><a href="#Text-Analysis-Using-Deep-Neural-Networks-in-Digital-Humanities-and-Information-Science" class="headerlink" title="Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science"></a>Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16217">http://arxiv.org/abs/2307.16217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet</li>
<li>for: 本研究的目的是探讨如何在人文科技领域中使用深度神经网络（DNN）来自动分析文本资源，以便为人文科学研究（DH）提供更多的可靠的数据分析方法。</li>
<li>methods: 本研究使用了多个DNN模型来解决各种NLP任务，包括拼写检查、语言检测、实体提取、作者检测、问答等任务。这些模型通过从大量“正确”和“错误”示例中学习模式，并将其应用于新的示例。</li>
<li>results: 本研究通过分析多个DH研究 literatura 中的实践，探讨了使用DNN模型在DH研究中的两大挑战：数据AVAILABILITY和领域适应。此外，本研究还提出了一个实用的决策模型，以帮助DH专家在选择合适的深度学习方法时作出更好的决策。<details>
<summary>Abstract</summary>
Combining computational technologies and humanities is an ongoing effort aimed at making resources such as texts, images, audio, video, and other artifacts digitally available, searchable, and analyzable. In recent years, deep neural networks (DNN) dominate the field of automatic text analysis and natural language processing (NLP), in some cases presenting a super-human performance. DNNs are the state-of-the-art machine learning algorithms solving many NLP tasks that are relevant for Digital Humanities (DH) research, such as spell checking, language detection, entity extraction, author detection, question answering, and other tasks. These supervised algorithms learn patterns from a large number of "right" and "wrong" examples and apply them to new examples. However, using DNNs for analyzing the text resources in DH research presents two main challenges: (un)availability of training data and a need for domain adaptation. This paper explores these challenges by analyzing multiple use-cases of DH studies in recent literature and their possible solutions and lays out a practical decision model for DH experts for when and how to choose the appropriate deep learning approaches for their research. Moreover, in this paper, we aim to raise awareness of the benefits of utilizing deep learning models in the DH community.
</details>
<details>
<summary>摘要</summary>
使用计算机技术和人文学是一项持续的努力，旨在使文本、图像、音频、视频和其他文物 digitally可用、搜索可用和分析可用。在过去几年中，深度神经网络（DNN）在自动文本分析和自然语言处理（NLP）领域占据了主导地位，在一些情况下表现出超人般的表现。DNN是当前最佳的机器学习算法，用于解决数字人文学（DH）研究中相关的许多NLP任务，如拼写检查、语言检测、实体提取、作者检测、问题回答等任务。这些有监督的算法通过大量“正确”和“错误”示例学习出模式，然后应用于新示例。但是，在DH研究中使用DNN分析文本资源存在两个主要挑战：数据训练的可用性和领域适应。本文分析了多个DH研究中的用例，并评估了它们的可能的解决方案，并提出了实用的决策模型，以帮助DH专家在选择合适的深度学习方法时做出决策。此外，本文的目的是提醒DH社区利用深度学习模型的好处。
</details></li>
</ul>
<hr>
<h2 id="Question-Answering-with-Deep-Neural-Networks-for-Semi-Structured-Heterogeneous-Genealogical-Knowledge-Graphs"><a href="#Question-Answering-with-Deep-Neural-Networks-for-Semi-Structured-Heterogeneous-Genealogical-Knowledge-Graphs" class="headerlink" title="Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs"></a>Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16214">http://arxiv.org/abs/2307.16214</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/omrivm/uncle-bert">https://github.com/omrivm/uncle-bert</a></li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</li>
<li>for: 这个研究旨在开发一种基于家谱树的问答系统，以便为家谱研究提供更加精准的问答功能。</li>
<li>methods: 该研究使用了转换器模型，将家谱数据转换为知识图，然后与文本结合，并使用自动生成的家谱数据进行训练。</li>
<li>results: 研究发现，与开放领域问答模型相比，专门为家谱问答模型具有更高的精度和更低的复杂性。此外，该方法可能有实际意义 для家谱研究和实际项目，使家谱数据更加访问ible。<details>
<summary>Abstract</summary>
With the rising popularity of user-generated genealogical family trees, new genealogical information systems have been developed. State-of-the-art natural question answering algorithms use deep neural network (DNN) architecture based on self-attention networks. However, some of these models use sequence-based inputs and are not suitable to work with graph-based structure, while graph-based DNN models rely on high levels of comprehensiveness of knowledge graphs that is nonexistent in the genealogical domain. Moreover, these supervised DNN models require training datasets that are absent in the genealogical domain. This study proposes an end-to-end approach for question answering using genealogical family trees by: 1) representing genealogical data as knowledge graphs, 2) converting them to texts, 3) combining them with unstructured texts, and 4) training a trans-former-based question answering model. To evaluate the need for a dedicated approach, a comparison between the fine-tuned model (Uncle-BERT) trained on the auto-generated genealogical dataset and state-of-the-art question-answering models was per-formed. The findings indicate that there are significant differences between answering genealogical questions and open-domain questions. Moreover, the proposed methodology reduces complexity while increasing accuracy and may have practical implications for genealogical research and real-world projects, making genealogical data accessible to experts as well as the general public.
</details>
<details>
<summary>摘要</summary>
随着家谱创建者自动生成的家谱树的 популяр度的提高，新的家谱信息系统已经被开发出来。现代自然问答算法使用深度神经网络（DNN）架构，其中一些模型使用序列化输入，不适合处理图structured数据，而图structured DNN模型则需要高度完整的知识图，而这种图structured 知识图在家谱领域缺失。此外，这些监督式DNN模型需要家谱领域缺失的培训数据。本研究提出了一种终端方法，通过以下步骤来回答家谱问题：1）将家谱数据转换为知识图，2）将其与未结构化文本结合，3）使用转换器基于 transformer 模型进行问答。为了评估需要专门的方法，对自动生成的家谱数据进行了精心 fine-tune 的 Uncle-BERT 模型和现有的问答模型进行比较。结果显示，回答家谱问题和开放领域问题存在显著差异。此外，提出的方法可以降低复杂性，提高准确性，并可能对家谱研究和实际项目产生实际意义，使家谱数据更加可访问ible  для专家和一般公众。
</details></li>
</ul>
<hr>
<h2 id="Toward-a-Period-Specific-Optimized-Neural-Network-for-OCR-Error-Correction-of-Historical-Hebrew-Texts"><a href="#Toward-a-Period-Specific-Optimized-Neural-Network-for-OCR-Error-Correction-of-Historical-Hebrew-Texts" class="headerlink" title="Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts"></a>Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16213">http://arxiv.org/abs/2307.16213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</li>
<li>for: 为了提高希伯来文件中的Optical Character Recognition（OCR）识别精度，提供一种多阶段方法。</li>
<li>methods: 使用神经网络进行OCR识别错误修复，并且通过人工生成的训练数据集和优化超参数来提高模型的性能。</li>
<li>results: 通过实验表明，该方法可以提高希伯来文件中OCR识别精度，并且可以适应不同的语言风格和时期变化。<details>
<summary>Abstract</summary>
Over the past few decades, large archives of paper-based historical documents, such as books and newspapers, have been digitized using the Optical Character Recognition (OCR) technology. Unfortunately, this broadly used technology is error-prone, especially when an OCRed document was written hundreds of years ago. Neural networks have shown great success in solving various text processing tasks, including OCR post-correction. The main disadvantage of using neural networks for historical corpora is the lack of sufficiently large training datasets they require to learn from, especially for morphologically-rich languages like Hebrew. Moreover, it is not clear what are the optimal structure and values of hyperparameters (predefined parameters) of neural networks for OCR error correction in Hebrew due to its unique features. Furthermore, languages change across genres and periods. These changes may affect the accuracy of OCR post-correction neural network models. To overcome these challenges, we developed a new multi-phase method for generating artificial training datasets with OCR errors and hyperparameters optimization for building an effective neural network for OCR post-correction in Hebrew.
</details>
<details>
<summary>摘要</summary>
To overcome these challenges, we developed a new multi-phase method for generating artificial training datasets with OCR errors and hyperparameters optimization for building an effective neural network for OCR post-correction in Hebrew.
</details></li>
</ul>
<hr>
<h2 id="Robust-Multi-Agent-Reinforcement-Learning-with-State-Uncertainty"><a href="#Robust-Multi-Agent-Reinforcement-Learning-with-State-Uncertainty" class="headerlink" title="Robust Multi-Agent Reinforcement Learning with State Uncertainty"></a>Robust Multi-Agent Reinforcement Learning with State Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16212">http://arxiv.org/abs/2307.16212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sihongho/robust_marl_with_state_uncertainty">https://github.com/sihongho/robust_marl_with_state_uncertainty</a></li>
<li>paper_authors: Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, Fei Miao</li>
<li>for: 本研究旨在解决多代理人学习（MARL）中存在状态不确定性（state uncertainty）的问题，提高agent的稳定性和可靠性。</li>
<li>methods: 本研究使用Markov Game with state perturbation adversaries（MG-SPA）模型，并提出了一种robust equilibrium（RE）作为解题方法。然后，提出了一种robust multi-agent Q-learning（RMAQ）算法来实现RE，并在高维状态动作空间中提出了一种robust multi-agent actor-critic（RMAAC）算法。</li>
<li>results: 实验结果表明，提出的RMAQ算法能够 converges to the optimal value function，而RMAAC算法在多个多代理人环境中比较多个MARL和robust MARL方法表现更好，特别是在状态不确定性存在时。<details>
<summary>Abstract</summary>
In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC) algorithm based on an analytical expression of the policy gradient derived in the paper. Our experiments show that the proposed RMAQ algorithm converges to the optimal value function; our RMAAC algorithm outperforms several MARL and robust MARL methods in multiple multi-agent environments when state uncertainty is present. The source code is public on \url{https://github.com/sihongho/robust_marl_with_state_uncertainty}.
</details>
<details>
<summary>摘要</summary>
在实际多智能体强化学习（MARL）应用中，智能体可能不具备完整的状态信息（例如因为不准确的测量或攻击），这会挑战智能体的策略的稳定性。虽然稳定性在MARL部署中变得越来越重要，但前一些研究却没有系统地研究了状态不确定性在MARL中的问题。我们受到这种稳定性问题和相关研究的缺失启发，在这里研究了MARL中的状态不确定性问题。我们首次在Markov游戏中引入状态干扰对手（MG-SPA），并将状态干扰对手作为问题的解决方案。我们 THEN 进行了基本的分析和探索，包括状态不确定性下的稳定性存在的条件。然后，我们提出了一种可靠的多智能体Q学习（RMAQ）算法，以找到这种稳定性的解决方案，并有确定的收敛保证。为了处理高维状态动作空间，我们设计了一种基于分析表达的策略梯度的多智能体actor-critic（RMAAC）算法。我们的实验表明，我们的RMAQ算法可以到达优质函数的优化值；我们的RMAAC算法在多个多智能体环境中高效地处理状态不确定性。我们的代码可以在 \url{https://github.com/sihongho/robust_marl_with_state_uncertainty} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Uncertainly-Missing-and-Ambiguous-Visual-Modality-in-Multi-Modal-Entity-Alignment"><a href="#Rethinking-Uncertainly-Missing-and-Ambiguous-Visual-Modality-in-Multi-Modal-Entity-Alignment" class="headerlink" title="Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment"></a>Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16210">http://arxiv.org/abs/2307.16210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjukg/UMAEA">https://github.com/zjukg/UMAEA</a></li>
<li>paper_authors: Zhuo Chen, Lingbing Guo, Yin Fang, Yichi Zhang, Jiaoyan Chen, Jeff Z. Pan, Yangning Li, Huajun Chen, Wen Zhang</li>
<li>for: 本文主要目标是提出一种robust多模态实体对应方法，以解决在多个知识图(KG)中存在不完整的视觉模态的问题。</li>
<li>methods: 本文使用了最新的MMEA模型，并在我们提出的MMEA-UMVM数据集上进行了 benchmarking。该数据集包括了双语和单语对照KG，并采用了标准(非迭代)和迭代训练方法来评估模型性能。</li>
<li>results: 研究结果表明，在面临多模态不完整性时，模型容易过拟合多模态噪音，并出现高比例的性能波动或下降。这表明，在某些情况下，附加的多模态数据可能会对实体对应性能产生负面影响。为解决这些挑战，我们提出了UMAEA方法，它可以有效地处理不确定的多模态视觉信息。UMAEA方法在所有97个分 splitting中表现出色，superiority 过 existed baseline，并且具有限制parameters和时间消耗的优点。<details>
<summary>Abstract</summary>
As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additional multi-modal data can sometimes adversely affect EA. To address these challenges, we introduce UMAEA , a robust multi-modal entity alignment approach designed to tackle uncertainly missing and ambiguous visual modalities. It consistently achieves SOTA performance across all 97 benchmark splits, significantly surpassing existing baselines with limited parameters and time consumption, while effectively alleviating the identified limitations of other models. Our code and benchmark data are available at https://github.com/zjukg/UMAEA.
</details>
<details>
<summary>摘要</summary>
Traditional multi-modal entity alignment (MMEA) aims to identify the same entity across different knowledge graphs (KGs) by leveraging associated visual information. However, existing MMEA methods primarily focus on fusing multi-modal entity features, while neglecting the challenges posed by the prevalent phenomenon of missing and inherent ambiguity of visual images. In this paper, we conduct a further analysis of the incompleteness of visual modalities, and benchmark the latest MMEA models on our proposed dataset MMEA-UMVM, which includes bilingual and monolingual alignment graphs with standard (non-iterative) and iterative training paradigms to evaluate model performance. Our findings indicate that, in the face of modality incompleteness, models tend to overfit the modality noise and exhibit performance fluctuations or declines at high rates of missing modality. This suggests that the inclusion of additional multi-modal data can sometimes adversely affect entity alignment. To address these challenges, we propose UMAEA, a robust multi-modal entity alignment approach designed to handle uncertain, missing, and ambiguous visual modalities. It consistently achieves state-of-the-art (SOTA) performance across all 97 benchmark splits, significantly outperforming existing baselines with limited parameters and time consumption, while effectively alleviating the limitations of other models. Our code and benchmark data are available at https://github.com/zjukg/UMAEA.
</details></li>
</ul>
<hr>
<h2 id="Around-the-GLOBE-Numerical-Aggregation-Question-Answering-on-Heterogeneous-Genealogical-Knowledge-Graphs-with-Deep-Neural-Networks"><a href="#Around-the-GLOBE-Numerical-Aggregation-Question-Answering-on-Heterogeneous-Genealogical-Knowledge-Graphs-with-Deep-Neural-Networks" class="headerlink" title="Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks"></a>Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16208">http://arxiv.org/abs/2307.16208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</li>
<li>for: 这个研究是为了提高基础设施领域中的数字资产管理和研究效率。</li>
<li>methods: 该研究使用了自动化数据集训练方法、转换器基于表格选择方法和优化的转换器基于数字聚合Question Answering模型。</li>
<li>results: 研究发现，提案的建筑GLOBE，在数字聚合Question Answering任务中的准确率为87%，比现有状态方法和管道的准确率提高了66%。<details>
<summary>Abstract</summary>
One of the key AI tools for textual corpora exploration is natural language question-answering (QA). Unlike keyword-based search engines, QA algorithms receive and process natural language questions and produce precise answers to these questions, rather than long lists of documents that need to be manually scanned by the users. State-of-the-art QA algorithms based on DNNs were successfully employed in various domains. However, QA in the genealogical domain is still underexplored, while researchers in this field (and other fields in humanities and social sciences) can highly benefit from the ability to ask questions in natural language, receive concrete answers and gain insights hidden within large corpora. While some research has been recently conducted for factual QA in the genealogical domain, to the best of our knowledge, there is no previous research on the more challenging task of numerical aggregation QA (i.e., answering questions combining aggregation functions, e.g., count, average, max). Numerical aggregation QA is critical for distant reading and analysis for researchers (and the general public) interested in investigating cultural heritage domains. Therefore, in this study, we present a new end-to-end methodology for numerical aggregation QA for genealogical trees that includes: 1) an automatic method for training dataset generation; 2) a transformer-based table selection method, and 3) an optimized transformer-based numerical aggregation QA model. The findings indicate that the proposed architecture, GLOBE, outperforms the state-of-the-art models and pipelines by achieving 87% accuracy for this task compared to only 21% by current state-of-the-art models. This study may have practical implications for genealogical information centers and museums, making genealogical data research easy and scalable for experts as well as the general public.
</details>
<details>
<summary>摘要</summary>
一种关键的人工智能工具 для文本 corpora 探索是自然语言问答（QA）。与关键词搜索引擎不同，QA 算法会根据自然语言问题进行处理，而不是将长列表交给用户手动搜索。现状的QA 算法基于 DNN 已经在不同领域得到成功应用。然而，在家谱领域，QA 仍然处于未探索的阶段，而研究人员在这个领域（以及人文社会科学领域）可以很大程度上受益于能够通过自然语言提问，得到准确的答案，并从大量文本中获得隐藏的洞察。虽然有些研究在家谱领域的事实Question Answering（QA）方面已经进行，但我们知道，对数字和平均函数的QA（即Answering questions combining aggregation functions, e.g., count, average, max）的研究尚未进行。这种QA 任务对于远程阅读和分析是非常重要的，因此，在这种研究中，我们提出了一种新的综合方法，包括：1）自动生成训练数据集方法；2）基于 transformer 的表格选择方法；3）优化的 transformer 基于 numerical aggregation QA 模型。研究结果表明，我们提出的架构，GLOBE，在这个任务上比现状模型和管道的性能高出87%，而不是只有21%。这个研究可能具有实质性的实际应用，使得家谱信息中心和博物馆的研究变得容易和可扩展，以便专家和一般公众都可以使用。
</details></li>
</ul>
<hr>
<h2 id="Deep-Convolutional-Neural-Networks-with-Zero-Padding-Feature-Extraction-and-Learning"><a href="#Deep-Convolutional-Neural-Networks-with-Zero-Padding-Feature-Extraction-and-Learning" class="headerlink" title="Deep Convolutional Neural Networks with Zero-Padding: Feature Extraction and Learning"></a>Deep Convolutional Neural Networks with Zero-Padding: Feature Extraction and Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16203">http://arxiv.org/abs/2307.16203</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liubc17/eDCNN_zero_padding">https://github.com/liubc17/eDCNN_zero_padding</a></li>
<li>paper_authors: Zhi Han, Baichen Liu, Shao-Bo Lin, Ding-Xuan Zhou</li>
<li>for: 这个论文研究了深度卷积神经网络（DCNNs）中零填充的表现和学习。</li>
<li>methods: 论文首先验证了零填充在特征提取和学习中的作用，并证明了 pooling 的翻译不变性驱动性。然后，论文表明了任何深度全连接神经网络（DFCNs）可以被表示为 DCNNs  WITH 零填充，这表明了 DCNNs 的更好的特征提取能力。</li>
<li>results: 论文 derivates 了 DCNNs  WITH 零填充的 universal consistency，并证明了其在学习过程中的翻译不变性。numerical experiments 验证了这些理论结论，包括 both  Toy 仿真和实际数据运行。<details>
<summary>Abstract</summary>
This paper studies the performance of deep convolutional neural networks (DCNNs) with zero-padding in feature extraction and learning. After verifying the roles of zero-padding in enabling translation-equivalence, and pooling in its translation-invariance driven nature, we show that with similar number of free parameters, any deep fully connected networks (DFCNs) can be represented by DCNNs with zero-padding. This demonstrates that DCNNs with zero-padding is essentially better than DFCNs in feature extraction. Consequently, we derive universal consistency of DCNNs with zero-padding and show its translation-invariance in the learning process. All our theoretical results are verified by numerical experiments including both toy simulations and real-data running.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Shuffled-Differentially-Private-Federated-Learning-for-Time-Series-Data-Analytics"><a href="#Shuffled-Differentially-Private-Federated-Learning-for-Time-Series-Data-Analytics" class="headerlink" title="Shuffled Differentially Private Federated Learning for Time Series Data Analytics"></a>Shuffled Differentially Private Federated Learning for Time Series Data Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16196">http://arxiv.org/abs/2307.16196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxi Huang, Chaoyang Jiang, Zhenghua Chen</li>
<li>for: 针对时间序列数据进行信任性联合学习，以达到最佳性能 while 保护客户端的隐私。</li>
<li>methods: 使用本地差分隐私来延伸隐私保护信赖关系到客户端，并 incorporate 摇摆技术以进一步增强隐私。</li>
<li>results: 在五个时间序列数据集上进行了广泛的实验，结果显示我们的算法在小客户和大客户enario 中都实现了最小的精度损失，并在同等隐私保护水平下与中央差分隐私联合学习的比较更好。<details>
<summary>Abstract</summary>
Trustworthy federated learning aims to achieve optimal performance while ensuring clients' privacy. Existing privacy-preserving federated learning approaches are mostly tailored for image data, lacking applications for time series data, which have many important applications, like machine health monitoring, human activity recognition, etc. Furthermore, protective noising on a time series data analytics model can significantly interfere with temporal-dependent learning, leading to a greater decline in accuracy. To address these issues, we develop a privacy-preserving federated learning algorithm for time series data. Specifically, we employ local differential privacy to extend the privacy protection trust boundary to the clients. We also incorporate shuffle techniques to achieve a privacy amplification, mitigating the accuracy decline caused by leveraging local differential privacy. Extensive experiments were conducted on five time series datasets. The evaluation results reveal that our algorithm experienced minimal accuracy loss compared to non-private federated learning in both small and large client scenarios. Under the same level of privacy protection, our algorithm demonstrated improved accuracy compared to the centralized differentially private federated learning in both scenarios.
</details>
<details>
<summary>摘要</summary>
信任worthy的联合学习目标是实现最佳性能，同时保障客户端的隐私。现有的隐私保护联合学习方法主要针对图像数据，缺乏应用于时间序列数据，这种数据在机器健康监测、人活动识别等领域具有重要应用。此外，对时间序列数据分析模型的保护噪声可能会对时间相关的学习产生干扰，导致准确性下降。为解决这些问题，我们开发了一种隐私保护的联合学习算法 для时间序列数据。具体来说，我们使用本地差分隐私来扩展隐私保护的信任边界到客户端。我们还 integrates 搅拌技术来实现隐私增强，为了 Mitigating the accuracy decline caused by leveraging local differential privacy.我们对五个时间序列数据集进行了广泛的实验。评估结果表明，我们的算法在小客户和大客户场景中都体现出较少的准确性下降，与非隐私联合学习相比。同时，在保持同样的隐私保护水平下，我们的算法在两个场景中都表现出了与中央差分隐私联合学习的改进准确性。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Approach-to-Mitigate-Numerical-Instability-in-Backpropagation-for-16-bit-Neural-Network-Training"><a href="#An-Efficient-Approach-to-Mitigate-Numerical-Instability-in-Backpropagation-for-16-bit-Neural-Network-Training" class="headerlink" title="An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training"></a>An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16189">http://arxiv.org/abs/2307.16189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juyoung Yun</li>
<li>for: 这个研究探讨了在机器学习模型16位计算中出现的数学不稳定性问题，特别是在广泛使用的优化算法RMSProp和Adam中。</li>
<li>methods: 研究人员发现了epsilongamma的单个参数对这种数学不稳定性问题产生了主要影响，并提出了一种新的方法来缓解这些问题。</li>
<li>results: 研究人员发现，通过轻微调整epsilongamma的值，可以恢复RMSProp和Adam在16位计算中的正常功能，并提高了深度神经网络训练过程的稳定性。<details>
<summary>Abstract</summary>
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit computations. This study contributes to better understanding of optimization in low-precision computations and provides an effective solution to a longstanding issue in training deep neural networks, opening new avenues for more efficient and stable model training.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们探讨了16位计算中机器学习模型的数值不稳定现象，尤其是在广泛使用的优化算法such as RMSProp和Adam时。这种不稳定现象通常在深度神经网络训练阶段出现，导致学习过程中断并阻碍深度神经网络的有效部署。我们确定了ε参数为这种数值不稳定的主要罪魁。在16位计算中的RMSProp和Adam优化器中，我们进行了深入的探讨，发现一小调整ε参数的值可以恢复这些优化器的功能，从而启用16位神经网络的有效使用。我们提出了一种新的约束数值不稳定问题的方法。这种方法基于Adam优化器的更新，可以在16位计算中提高学习过程的稳定性。本研究对低精度计算中优化的理解做出了贡献，并提供了训练深度神经网络的有效解决方案，开创了更高效和稳定的模型训练新途径。
</details></li>
</ul>
<hr>
<h2 id="ESP-Exploiting-Symmetry-Prior-for-Multi-Agent-Reinforcement-Learning"><a href="#ESP-Exploiting-Symmetry-Prior-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning"></a>ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16186">http://arxiv.org/abs/2307.16186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yu, Rongye Shi, Pu Feng, Yongkai Tian, Jie Luo, Wenjun Wu</li>
<li>for: 这篇论文旨在提高多智能体强化学习（MARL）的数据效率。</li>
<li>methods: 该paper提出了一个基于同质现象的框架，通过融合数据增强和一个妥善设计的一致损失函数，以提高现有MARL方法的数据效率。</li>
<li>results: 实验结果显示，提案的框架能够提高多个具有挑战性的任务的数据效率。此外，该框架还应用于物理多机器人测试平台，以显示其优势。<details>
<summary>Abstract</summary>
Multi-agent reinforcement learning (MARL) has achieved promising results in recent years. However, most existing reinforcement learning methods require a large amount of data for model training. In addition, data-efficient reinforcement learning requires the construction of strong inductive biases, which are ignored in the current MARL approaches. Inspired by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Multi-agent reinforcement learning" is translated as "多 Agent 强化学习" (MARL)* "achieved promising results" is translated as "取得了可观的成果"* "most existing reinforcement learning methods" is translated as "大多数现有的 reinforcement learning 方法"* "require a large amount of data" is translated as "需要大量数据"* "data-efficient reinforcement learning" is translated as "数据有效的 reinforcement learning"* "strong inductive biases" is translated as "强大的推理假设"* " ignored in the current MARL approaches" is translated as "在当前的 MARL 方法中被忽略"* "inspired by the symmetry phenomenon" is translated as " inspirited by the symmetry phenomenon"* "a framework for exploiting prior knowledge" is translated as "一个抽象框架 для利用先前知识"* "integrating data augmentation" is translated as " integrating data augmentation"* "a well-designed consistency loss" is translated as "一个良好的一致性损失"* "model-agnostic" is translated as "模型无关"* "can be applied to most of the current MARL algorithms" is translated as "可以应用于大多数当前的 MARL 算法"* "Experimental tests on multiple challenging tasks" is translated as "在多个复杂任务上进行了实验测试"* "demonstrate the effectiveness of the proposed framework" is translated as "示出提议的框架的有效性"* "applied to a physical multi-robot testbed" is translated as "应用于物理多机器人测试平台"* "show its superiority" is translated as "示出其优越性"
</details></li>
</ul>
<hr>
<h2 id="Unified-Model-for-Image-Video-Audio-and-Language-Tasks"><a href="#Unified-Model-for-Image-Video-Audio-and-Language-Tasks" class="headerlink" title="Unified Model for Image, Video, Audio and Language Tasks"></a>Unified Model for Image, Video, Audio and Language Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16184">http://arxiv.org/abs/2307.16184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mshukor/unival">https://github.com/mshukor/unival</a></li>
<li>paper_authors: Mustafa Shukor, Corentin Dancette, Alexandre Rame, Matthieu Cord<br>for:* The paper aims to build a unified model that can support all modalities (text, images, videos, and audio) efficiently and without relying on large datasets or complex models.methods:* The proposed model, UnIVAL, is pretrained on many tasks using task balancing and multimodal curriculum learning.* The model uses weight interpolation of models trained on different multimodal tasks to improve generalization to out-of-distribution inputs.results:* UnIVAL shows competitive performance on image and video-text tasks and achieves competitive performance on audio-text tasks despite not being pretrained on audio.* The unified model demonstrates the synergy between tasks and improves generalization to out-of-distribution inputs.Here is the information in Simplified Chinese text:for:* 论文目的是建立一个能够支持所有Modalities（文本、图像、视频和音频）的有效和高效的模型，不需要庞大的数据集或复杂的模型。methods:* 提议的模型UnIVAL通过任务均衡和多模态训练来预训练多个任务。* 模型使用多模态任务训练的模型Weight interpolation来提高对异常输入的泛化。results:* UnIVAL在图像和视频-文本任务上显示了竞争性表现，并在没有对 audio 进行预训练的情况下在 audio-文本任务上达到了竞争性表现。* 统一模型展示了任务之间的共谊和对异常输入的泛化提高。I hope that helps!<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches, across image and video-text tasks. The feature representations learned from image and video-text modalities, allows the model to achieve competitive performance when finetuned on audio-text tasks, despite not being pretrained on audio. Thanks to the unified model, we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks, showing their benefits in particular for out-of-distribution generalization. Finally, we motivate unification by showing the synergy between tasks. The model weights and code are released here: https://github.com/mshukor/UnIVAL.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经让普通的通用代理人变得不是一个梦想。一个关键的难点是多元化和多种多样的任务和模式。一个有前途的解决方案是统一，允许支持一大量的任务和模式在一个统一框架下。现在的小型至中型统一模型都只支持2种模式，通常是图像文本或视频文本。我们的问题是：是否可以有效地建立一个统一模型，可以支持所有模式？为了回答这个问题，我们提出了 UnIVAL，这是一个进一步的目标。不需要庞大的数据集或者具有亿位 Parameters 的模型，我们的 ~ 0.25B 参数的 UnIVAL 模型可以超过二种模式，并将文本、图像、视频和音频统一为一个模型。我们的模型通过多任务调整和多模式学习来快速预训。 UnIVAL 在图像和视频文本任务上显示了竞争性的表现，而且可以在不直接预训的音频文本任务上 achieve 竞争性的表现，只因为它可以从图像和视频文本模式中学习出来的特征表现。我们还提出了一个新的研究，通过多模式模型的权重 interpolating 来评估多模式模型在不同多模式任务之间的融合效果，这些任务包括 audio-text 任务。最后，我们鼓励统一，因为多个任务之间存在联互关系，这使得模型可以从不同任务中学习到普遍的特征表现。模型和代码可以在 GitHub 上获取：https://github.com/mshukor/UnIVAL。
</details></li>
</ul>
<hr>
<h2 id="Redundancy-aware-unsupervised-rankings-for-collections-of-gene-sets"><a href="#Redundancy-aware-unsupervised-rankings-for-collections-of-gene-sets" class="headerlink" title="Redundancy-aware unsupervised rankings for collections of gene sets"></a>Redundancy-aware unsupervised rankings for collections of gene sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16182">http://arxiv.org/abs/2307.16182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiara Balestra, Carlo Maj, Emmanuel Müller, Andreas Mayr</li>
<li>for: 提高生物学 Pathway 的可读性和解释力</li>
<li>methods: 使用 Shapley 值来评估 Pathway 的重要性，并使用 trick 避免计算复杂性</li>
<li>results: 可以减少 Pathway 集合的维度，同时保持高度覆盖所有基因的表达In simplified Chinese:</li>
<li>for: 提高生物学 Pathway 的可读性和解释力</li>
<li>methods: 使用 Shapley 值来评估 Pathway 的重要性，并使用 trick 避免计算复杂性</li>
<li>results: 可以减少 Pathway 集合的维度，同时保持高度覆盖所有基因的表达<details>
<summary>Abstract</summary>
The biological roles of gene sets are used to group them into collections. These collections are often characterized by being high-dimensional, overlapping, and redundant families of sets, thus precluding a straightforward interpretation and study of their content. Bioinformatics looked for solutions to reduce their dimension or increase their intepretability. One possibility lies in aggregating overlapping gene sets to create larger pathways, but the modified biological pathways are hardly biologically justifiable. We propose to use importance scores to rank the pathways in the collections studying the context from a set covering perspective. The proposed Shapley values-based scores consider the distribution of the singletons and the size of the sets in the families; Furthermore, a trick allows us to circumvent the usual exponential complexity of Shapley values' computation. Finally, we address the challenge of including a redundancy awareness in the obtained rankings where, in our case, sets are redundant if they show prominent intersections.   The rankings can be used to reduce the dimension of collections of gene sets, such that they show lower redundancy and still a high coverage of the genes. We further investigate the impact of our selection on Gene Sets Enrichment Analysis. The proposed method shows a practical utility in bioinformatics to increase the interpretability of the collections of gene sets and a step forward to include redundancy into Shapley values computations.
</details>
<details>
<summary>摘要</summary>
生物学角色集合用于分组 gene set。这些集合经常是高维、重叠、重复的家庭集合，因此禁止直接解释和研究其内容。生物信息学搜索解决方案以降低维度或增加可读性。一种可能性在于将重叠的 gene set 聚合成更大的路径，但修改后的生物路径几乎不能正确地表达生物学意义。我们提议使用importance scores来排序pathway，并研究集合从集合覆盖角度来学习context。我们的提案基于 Shapley 值，考虑单个元素和集合的大小，并且可以避免通常的对 Shapley 值的计算的指数复杂性。 finally，我们解决了包含重复性在内的获得的排名中的挑战。这些排名可以用来降低集合的维度，以便仍然保持高度覆盖所有的基因。我们进一步调查了我们的选择对 Gene Sets Enrichment Analysis 的影响。我们的方法显示了生物信息学中可行的增加可读性的集合，以及包含重复性在内的 Shapley 值计算的一个进步。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-learning-of-density-ratios-in-RKHS"><a href="#Adaptive-learning-of-density-ratios-in-RKHS" class="headerlink" title="Adaptive learning of density ratios in RKHS"></a>Adaptive learning of density ratios in RKHS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16164">http://arxiv.org/abs/2307.16164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Werner Zellinger, Stefan Kindermann, Sergei V. Pereverzyev</li>
<li>for: 估计两个概率密度之间的比率从finite数据观测中。</li>
<li>methods: 使用regularized Bregman divergence在 reproduce kernel Hilbert space（RKHS）中对概率密度比率进行估计。</li>
<li>results: 提供新的finite-sample error bounds，并提出Lepskii type parameter choice principle，可以在不知道概率密度比率的情况下最小化 bound。在特定情况下，我们的方法可以达到最优的 minimax 错误率。<details>
<summary>Abstract</summary>
Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.
</details>
<details>
<summary>摘要</summary>
估算两个概率密度之间的比率是机器学习和统计领域中的中心问题，具有应用于两个样本测试、差异估计、生成模型、 covariate shift 适应、条件概率密度估计和新奇检测等领域。在这个工作中，我们分析了一大类的概率密度比率估计方法，这些方法在一个 reproduce kernel Hilbert space（RKHS）中减少了一个弹性Bregman divergence的正则化。我们 derivated新的finite-sample error bounds，并提出了一种Lepskii类型的参数选择原则，该原则可以在不知道概率密度比率的regulatory情况下最小化error bounds。在特定的quadratic loss情况下，我们的方法可以自适应取得一个minimax优化的错误率。一个数字示例也提供。
</details></li>
</ul>
<hr>
<h2 id="Variance-Control-for-Distributional-Reinforcement-Learning"><a href="#Variance-Control-for-Distributional-Reinforcement-Learning" class="headerlink" title="Variance Control for Distributional Reinforcement Learning"></a>Variance Control for Distributional Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16152">http://arxiv.org/abs/2307.16152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kuangqi927/qem">https://github.com/kuangqi927/qem</a></li>
<li>paper_authors: Qi Kuang, Zhoufan Zhu, Liwen Zhang, Fan Zhou</li>
<li>for: 这个论文主要是为了检验分布式强化学习（DRL）中Q函数估计器的有效性。</li>
<li>methods: 该论文使用了错误分析来理解Q函数估计器在分布式设定下的拟合误差的影响，并提出了一种新的估计器\emph{Quantiled Expansion Mean}（QEM）以及一种新的DRL算法（QEMRL）。</li>
<li>results: 对于variety of Atari和Mujoco benchmark任务，QEMRL算法比基eline算法在样本效率和收敛性方面具有显著改进。<details>
<summary>Abstract</summary>
Although distributional reinforcement learning (DRL) has been widely examined in the past few years, very few studies investigate the validity of the obtained Q-function estimator in the distributional setting. To fully understand how the approximation errors of the Q-function affect the whole training process, we do some error analysis and theoretically show how to reduce both the bias and the variance of the error terms. With this new understanding, we construct a new estimator \emph{Quantiled Expansion Mean} (QEM) and introduce a new DRL algorithm (QEMRL) from the statistical perspective. We extensively evaluate our QEMRL algorithm on a variety of Atari and Mujoco benchmark tasks and demonstrate that QEMRL achieves significant improvement over baseline algorithms in terms of sample efficiency and convergence performance.
</details>
<details>
<summary>摘要</summary>
尽管分布式强化学习（DRL）在过去几年内得到了广泛的研究，但是很少研究对于分布式设定中的Q函数估计器的有效性。为了全面理解Q函数估计器的折衔错误对整个训练过程的影响，我们进行了错误分析并从统计角度提出了一种新的估计器——量划扩展均值（QEM），以及一种基于统计学的新DRL算法（QEMRL）。我们对多个Atari和Mujoco benchmark任务进行了广泛的评估，并证明了QEMRL在样本效率和收敛性方面具有显著的改进。
</details></li>
</ul>
<hr>
<h2 id="An-Effective-LSTM-DDPM-Scheme-for-Energy-Theft-Detection-and-Forecasting-in-Smart-Grid"><a href="#An-Effective-LSTM-DDPM-Scheme-for-Energy-Theft-Detection-and-Forecasting-in-Smart-Grid" class="headerlink" title="An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid"></a>An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16149">http://arxiv.org/abs/2307.16149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xun Yuan, Yang Yang, Arwa Alromih, Prosanta Gope, Biplab Sikdar</li>
<li>for: 该研究目标是解决智能电网系统中的能源盗用检测（ETD）和能源消耗预测（ECF）两个相关的挑战。</li>
<li>methods: 该研究提出了一种结合长期快短训练记忆（LSTM）和杂噪扩散概率模型（DDPM）的方法，通过生成输入重建和预测来实现ETD和ECF。</li>
<li>results: 经过大量的实验 validate 实验，该方法在实际数据集和 sintetic 数据集上都能够高效地解决ETD和ECF问题，并且在ETD问题上显示出了显著的改善。<details>
<summary>Abstract</summary>
Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers a comprehensive and effective solution for addressing ETD and ECF challenges, demonstrating promising results and improved security in smart grid systems.
</details>
<details>
<summary>摘要</summary>
智能Grid系统中的能源抢夺检测（ETD）和能源消耗预测（ECF）是两个相关的挑战。对这两个问题进行集中解决是确保系统安全的关键。这篇论文解决了智能Grid系统中的ETD和ECF问题。提议的解决方案将长期短期记忆（LSTM）和杂度减少概率模型（DDPM）结合使用，生成输入重建和预测。通过利用重建和预测错误，系统可以识别能源抢夺行为，基于重建错误和预测错误来识别不同类型的攻击。经过广泛的实验，提议的方案在ETD和ECF问题上表现出优于基eline方法。 ensemble方法可以明显提高ETD性能，准确地检测基eline方法无法检测的能源抢夺攻击。这项研究提供了智能Grid系统中ETD和ECF问题的全面和有效解决方案，实验结果表明，该方案在智能Grid系统中提供了更好的安全保障。
</details></li>
</ul>
<hr>
<h2 id="Pupil-Learning-Mechanism"><a href="#Pupil-Learning-Mechanism" class="headerlink" title="Pupil Learning Mechanism"></a>Pupil Learning Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16141">http://arxiv.org/abs/2307.16141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rua-Huan Tsaih, Yu-Hang Chien, Shih-Yi Chien</li>
<li>for: 这个研究旨在解决人工神经网络中的快速衰减和过拟合问题。</li>
<li>methods: 本研究使用了学习眼视程序，包括解释、选择、理解、填充和组织等模块，从而 derivate 视力学习机制（PLM），并将其应用到2层神经网络（2LNN）中。</li>
<li>results: 在实验中，PLM模型与线性回归模型和传统的反射式2LNN模型相比，PLM模型具有较高的准确率和较低的错误率。<details>
<summary>Abstract</summary>
Studies on artificial neural networks rarely address both vanishing gradients and overfitting issues. In this study, we follow the pupil learning procedure, which has the features of interpreting, picking, understanding, cramming, and organizing, to derive the pupil learning mechanism (PLM) by which to modify the network structure and weights of 2-layer neural networks (2LNNs). The PLM consists of modules for sequential learning, adaptive learning, perfect learning, and less-overfitted learning. Based upon a copper price forecasting dataset, we conduct an experiment to validate the PLM module design modules, and an experiment to evaluate the performance of PLM. The empirical results indeed approve the PLM module design and show the superiority of the proposed PLM model over the linear regression model and the conventional backpropagation-based 2LNN model.
</details>
<details>
<summary>摘要</summary>
研究人工神经网络通常不关注两个问题：衰减梯度和适应过度。在这个研究中，我们采用学生学习过程，具有解释、选择、理解、填充和组织等特点， derivate学生学习机制（PLM），用于修改网络结构和权重。PLM包括顺序学习、适应学习、完美学习和较少适应学习模块。我们使用铜价预测数据集进行实验验证PLM模块设计，并对PLM模型的性能进行评估。实验结果证明PLM模块设计的正确性，并表明我们提出的PLM模型在线性回归模型和传统的反射层2LNN模型的性能上显著优于。
</details></li>
</ul>
<hr>
<h2 id="User-Controlled-Knowledge-Fusion-in-Large-Language-Models-Balancing-Creativity-and-Hallucination"><a href="#User-Controlled-Knowledge-Fusion-in-Large-Language-Models-Balancing-Creativity-and-Hallucination" class="headerlink" title="User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination"></a>User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16139">http://arxiv.org/abs/2307.16139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhang</li>
<li>for: 这个论文旨在解决现代对话系统中大语言模型（LLM）的使用问题，即找到一种能够控制LLM的幽默和实际知识的平衡点。</li>
<li>methods: 该论文提出了一种新的用户可控的机制，通过在LLM训练阶段添加数字标签来控制LLM的幽默和实际知识之间的平衡。该标签的值由自动化过程计算，根据ROUGE分数、Sentence-BERT嵌入和LLM自我评价分数来度量LLM对参考知识的依赖程度。</li>
<li>results: 该论文通过了广泛的实验，证明了该方法的适应性和可控性，并在不同的场景下保持了LLM的回快和准确性。结果表明该方法可以提高LLM的多样性，同时保持幽默和幻想的平衡。<details>
<summary>Abstract</summary>
In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.
</details>
<details>
<summary>摘要</summary>
现代对话系统中，使用大型语言模型（LLM）的使用量在增长 exponentially  due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Deep-Unrolling-Networks-with-Recurrent-Momentum-Acceleration-for-Nonlinear-Inverse-Problems"><a href="#Deep-Unrolling-Networks-with-Recurrent-Momentum-Acceleration-for-Nonlinear-Inverse-Problems" class="headerlink" title="Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems"></a>Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16120">http://arxiv.org/abs/2307.16120</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhouqp631/dunets-rma">https://github.com/zhouqp631/dunets-rma</a></li>
<li>paper_authors: Qingping Zhou, Jiayu Qian, Junqi Tang, Jinglai Li</li>
<li>for: 解决非线性逆问题</li>
<li>methods: 使用梯度加速技术（RMA）扩展深度推导网络（DuNets）</li>
<li>results: 对两种流行的 DuNets 方法（LPGD 和 LPD）进行了改进，提高了非线性逆问题的解决能力。实验结果表明，RMA 技术在非线性逆问题中的改进效果随问题的非线性程度增长。<details>
<summary>Abstract</summary>
Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, and an electrical impedance tomography problem with limited boundary measurements. In the first experiment we have observed that the improvement due to RMA largely increases with respect to the nonlinearity of the problem. The results of the second example further demonstrate that the RMA schemes can significantly improve the performance of DuNets in strongly ill-posed problems.
</details>
<details>
<summary>摘要</summary>
使用模型基于迭代算法和数据驱动深度学习解决方案，深度螺旋网络（DuNets）已成为解析逆问题的流行工具。 Although DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the method's performance. Inspired by momentum acceleration techniques commonly used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from previous gradients. We apply RMA to two popular DuNets - the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA, respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem and an electrical impedance tomography problem with limited boundary measurements. In the first experiment, we observed that the improvement due to RMA increases significantly with respect to the nonlinearity of the problem. The results of the second example further demonstrate that the RMA schemes can significantly improve the performance of DuNets in strongly ill-posed problems.
</details></li>
</ul>
<hr>
<h2 id="TMPNN-High-Order-Polynomial-Regression-Based-on-Taylor-Map-Factorization"><a href="#TMPNN-High-Order-Polynomial-Regression-Based-on-Taylor-Map-Factorization" class="headerlink" title="TMPNN: High-Order Polynomial Regression Based on Taylor Map Factorization"></a>TMPNN: High-Order Polynomial Regression Based on Taylor Map Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16105">http://arxiv.org/abs/2307.16105</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andiva/tmpnn">https://github.com/andiva/tmpnn</a></li>
<li>paper_authors: Andrei Ivanov, Stefan Maria Ailuro</li>
<li>for: 这篇论文旨在提出一种基于Taylor map汇合的高阶多项式回传 regression 方法，用于解决非线性模式的预测问题。</li>
<li>methods: 这篇论文使用了 Taylor map 汇合来建构高阶多项式回传 regression 方法，并实现了多目标回传和内部目标之间的关联。</li>
<li>results: 根据 UCI 开放资料集、Feynman  симвоlic regression 资料集和 Friedman-1 资料集的比较，提出的方法与现有的回传方法相比，在特定任务上表现更好，并且在某些任务上表现更好。<details>
<summary>Abstract</summary>
Polynomial regression is widely used and can help to express nonlinear patterns. However, considering very high polynomial orders may lead to overfitting and poor extrapolation ability for unseen data. The paper presents a method for constructing a high-order polynomial regression based on the Taylor map factorization. This method naturally implements multi-target regression and can capture internal relationships between targets. Additionally, we introduce an approach for model interpretation in the form of systems of differential equations. By benchmarking on UCI open access datasets, Feynman symbolic regression datasets, and Friedman-1 datasets, we demonstrate that the proposed method performs comparable to the state-of-the-art regression methods and outperforms them on specific tasks.
</details>
<details>
<summary>摘要</summary>
“多项式回传 regression 广泛应用，可以表示非线性征 patten。然而，考虑非常高的多项式顺序可能会导致过拟合和未见数据的 extrapolation 能力不佳。论文提出了基于 Taylor 对偶 factorization 的高顺位多项式回传方法。这种方法自然地实现多 Target 回传和目标之间的内部关系。此外，我们引入了一种模型解释方法，即系统 diferential Equations。通过 UCI 开放存储数据集、Feynman  симвоlic 回传数据集和 Friedman-1 数据集的实验，我们显示了提案的方法与现有的回传方法相比，在特定任务上表现相似，甚至在某些任务上超越。”Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="AI-Increases-Global-Access-to-Reliable-Flood-Forecasts"><a href="#AI-Increases-Global-Access-to-Reliable-Flood-Forecasts" class="headerlink" title="AI Increases Global Access to Reliable Flood Forecasts"></a>AI Increases Global Access to Reliable Flood Forecasts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16104">http://arxiv.org/abs/2307.16104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/global_streamflow_model_paper">https://github.com/google-research-datasets/global_streamflow_model_paper</a></li>
<li>paper_authors: Grey Nearing, Deborah Cohen, Vusumuzi Dube, Martin Gauch, Oren Gilon, Shaun Harrigan, Avinatan Hassidim, Frederik Kratzert, Asher Metzger, Sella Nevo, Florian Pappenberger, Christel Prudhomme, Guy Shalev, Shlomo Shenzis, Tadele Tekalign, Dana Weitzner, Yoss Matias<br>for: 这份研究是为了开发一个使用人工智能（AI）预测极端水文事件的模型，以提供更准确和更早的洪水警告。methods: 这份研究使用了AI模型来预测极端水文事件，并且比较了这个模型与现有的全球水文模型（Copernicus Emergency Management Service Global Flood Awareness System）的性能。results: 研究发现，这个AI模型在全球各地、不同的时间点和返回期下，都有着更高的准确性和更早的预测能力，特别是在无测流域中。这个模型已经被 integrate into an operational early warning system，并且在更 чем 80个国家提供免费和开放的预测。<details>
<summary>Abstract</summary>
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human impacts of flooding. We produce forecasts of extreme events in South America and Africa that achieve reliability approaching the current state of the art in Europe and North America, and we achieve reliability at between 4 and 6-day lead times that are similar to current state of the art nowcasts (0-day lead time). Additionally, we achieve accuracies over 10-year return period events that are similar to current accuracies over 2-year return period events, meaning that AI can provide warnings earlier and over larger and more impactful events. The model that we develop in this paper has been incorporated into an operational early warning system that produces publicly available (free and open) forecasts in real time in over 80 countries. This work using AI and open data highlights a need for increasing the availability of hydrological data to continue to improve global access to reliable flood warnings.
</details>
<details>
<summary>摘要</summary>
洪水是最常见且最有影响力的自然灾害之一，特别是在开发中国家，那里缺乏密集的流域流速监测网。精确和时间对洪水风险的警示是 Mitigating flood risks critical，但是需要对每个水系进行精确的水文模型 calibration 。我们发展了一个人工智能（AI）模型，可以预测7天内的极端水文事件。这个模型在所有大陆、领先时间和返回时间方面都有 significatively  outperform 现有的全球水文模型（Copernicus Emergency Management Service Global Flood Awareness System）。AI 特别有用于预测无测流域，因为只有一小 percent of the world's watersheds have stream gauges，而且这些无测流域主要集中在开发中国家，这些国家对人类洪水的影响更加敏感。我们在南美和非洲预测极端事件的精度接近现有的欧洲和北美洲精度，并在4-6天领先时间内实现相似的可靠性。此外，我们在10年返回时间的事件中实现了现有2年返回时间的精度，这意味着AI可以提供更早的警示和更大和更重要的事件。我们在这篇文章中开发的模型已经被 integrate 到一个操作中的早期警示系统中，该系统在实时生成可公开获取（免费和开放）的预测。这个使用 AI 和开放数据的工作 highlights 对于全球访问可靠洪水警示的需求。
</details></li>
</ul>
<hr>
<h2 id="On-Neural-Network-approximation-of-ideal-adversarial-attack-and-convergence-of-adversarial-training"><a href="#On-Neural-Network-approximation-of-ideal-adversarial-attack-and-convergence-of-adversarial-training" class="headerlink" title="On Neural Network approximation of ideal adversarial attack and convergence of adversarial training"></a>On Neural Network approximation of ideal adversarial attack and convergence of adversarial training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16099">http://arxiv.org/abs/2307.16099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajdeep Haldar, Qifan Song</li>
<li>for: 本文针对适用于防御模型对抗攻击的方法。</li>
<li>methods: 本文使用了一种基于神经网络的方法，将攻击表示为可训练的函数，不需要进一步的梯度计算。</li>
<li>results: 本文证明了在适当的条件下，攻击可以被表示为光滑的块状函数（块状Holder函数），并使用神经网络实现理想的攻击过程。<details>
<summary>Abstract</summary>
Adversarial attacks are usually expressed in terms of a gradient-based operation on the input data and model, this results in heavy computations every time an attack is generated. In this work, we solidify the idea of representing adversarial attacks as a trainable function, without further gradient computation. We first motivate that the theoretical best attacks, under proper conditions, can be represented as smooth piece-wise functions (piece-wise H\"older functions). Then we obtain an approximation result of such functions by a neural network. Subsequently, we emulate the ideal attack process by a neural network and reduce the adversarial training to a mathematical game between an attack network and a training model (a defense network). We also obtain convergence rates of adversarial loss in terms of the sample size $n$ for adversarial training in such a setting.
</details>
<details>
<summary>摘要</summary>
adversarial attacks 通常表示为输入数据和模型的梯度基于操作，这会导致每次生成攻击时需要重大计算。在这项工作中，我们坚持思考表达攻击为可学习函数，不需要进一步的梯度计算。我们首先证明，理论上最佳的攻击，在适当的条件下，可以表示为流畅的割辑函数（割辑Holder函数）。然后，我们得到了这些函数的近似结果，使用神经网络。接着，我们模拟理想的攻击过程，用神经网络来实现，并将反恐训练转化为数学游戏， между攻击网络和训练模型（防御网络）。我们还得到了对攻击损失的整数化速率，随着样本大小 $n$ 的增加。
</details></li>
</ul>
<hr>
<h2 id="ADR-GNN-Advection-Diffusion-Reaction-Graph-Neural-Networks"><a href="#ADR-GNN-Advection-Diffusion-Reaction-Graph-Neural-Networks" class="headerlink" title="ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks"></a>ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16092">http://arxiv.org/abs/2307.16092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moshe Eliasof, Eldad Haber, Eran Treister</li>
<li>for: 本文提出了一种基于扩散吸引系统的图 neural network 架构（ADR-GNN），用于解决图数据上复杂现象的学习表示。</li>
<li>methods: 该架构结合了扩散、吸引和反应三种过程，以模型图数据上的导向传输信息、本地平滑信息和非线性变换信息。</li>
<li>results: 作者对实验数据集进行了评估，并显示了 ADR-GNN 在图分类和空间时间数据集上提供了改进或与状态艺术网络竞争的表现。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown remarkable success in learning representations for graph-structured data. However, GNNs still face challenges in modeling complex phenomena that involve advection. In this paper, we propose a novel GNN architecture based on Advection-Diffusion-Reaction systems, called ADR-GNN. Advection models the directed transportation of information, diffusion captures the local smoothing of information, and reaction represents the non-linear transformation of information in channels. We provide an analysis of the qualitative behavior of ADR-GNN, that shows the benefit of combining advection, diffusion, and reaction. To demonstrate its efficacy, we evaluate ADR-GNN on real-world node classification and spatio-temporal datasets, and show that it improves or offers competitive performance compared to state-of-the-art networks.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) 已经取得了非常成功的表示图structured数据的学习。然而，GNNS仍然面临Complex Phenomena 的挑战，包括适应。在这篇论文中，我们提出了一种基于适应扩散反应系统的新GNN架构，称为ADR-GNN。适应模型化了irectional transportation of information，扩散捕捉了Local Smoothing of information，并且Reaction表示通道中的非线性变换。我们提供了ADR-GNN的qualitative行为分析，显示了结合适应、扩散和反应的优势。为证明其有效性，我们对实际世界节点分类和空时间数据集进行了评估，并显示了它与当前网络的竞争性或提高性。
</details></li>
</ul>
<hr>
<h2 id="Rapid-Flood-Inundation-Forecast-Using-Fourier-Neural-Operator"><a href="#Rapid-Flood-Inundation-Forecast-Using-Fourier-Neural-Operator" class="headerlink" title="Rapid Flood Inundation Forecast Using Fourier Neural Operator"></a>Rapid Flood Inundation Forecast Using Fourier Neural Operator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16090">http://arxiv.org/abs/2307.16090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Y. Sun, Zhi Li, Wonhyun Lee, Qixing Huang, Bridget R. Scanlon, Clint Dawson</li>
<li>For: 预测洪水覆盖范围和水深，提供紧急准备和应急响应之用。* Methods: 结合过程基本模型和数据驱动机器学习方法，采用Fourier neural operator（FNO）模型进行蒸发模拟。* Results: FNO模型在训练使用六个历史洪水事件的计算水深数据（15分钟间隔）后，在两个保留事件上进行测试，显示FNO模型在所有领先时间（最长3小时）中保持高预测精度，并在应用于新地点时表现良好，表明具有强泛化能力。<details>
<summary>Abstract</summary>
Flood inundation forecast provides critical information for emergency planning before and during flood events. Real time flood inundation forecast tools are still lacking. High-resolution hydrodynamic modeling has become more accessible in recent years, however, predicting flood extents at the street and building levels in real-time is still computationally demanding. Here we present a hybrid process-based and data-driven machine learning (ML) approach for flood extent and inundation depth prediction. We used the Fourier neural operator (FNO), a highly efficient ML method, for surrogate modeling. The FNO model is demonstrated over an urban area in Houston (Texas, U.S.) by training using simulated water depths (in 15-min intervals) from six historical storm events and then tested over two holdout events. Results show FNO outperforms the baseline U-Net model. It maintains high predictability at all lead times tested (up to 3 hrs) and performs well when applying to new sites, suggesting strong generalization skill.
</details>
<details>
<summary>摘要</summary>
洪水泛洪预测提供了重要的紧急准备和应急管理之前和在洪水事件发生时的信息。实时洪水泛洪预测工具仍然缺乏。高分解力 hidrodynamic 模型在过去几年中变得更加可 accessible，但是在实时预测洪水泛洪范围和泛洪深度方面仍然是计算挑战。我们提出了一种 hybrid 过程基于的数据驱动机器学习（ML）方法，用于预测洪水泛洪范围和泛洪深度。我们使用了 Fourier 神经网络（FNO）模型，这是一种高效的 ML 方法，用于模拟器。FNO 模型在得克萨斯州HOUSTON 市区域上进行了训练，使用了六个历史洪水事件中的 simulate 水深数据（每 15 分钟一个数据点），然后在两个保留事件上进行测试。结果显示，FNO 模型在所有领先时间（最长 3 小时）中保持高度预测性，并在应用于新地点时表现良好，这表明其具有强大的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Using-Implicit-Behavior-Cloning-and-Dynamic-Movement-Primitive-to-Facilitate-Reinforcement-Learning-for-Robot-Motion-Planning"><a href="#Using-Implicit-Behavior-Cloning-and-Dynamic-Movement-Primitive-to-Facilitate-Reinforcement-Learning-for-Robot-Motion-Planning" class="headerlink" title="Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning"></a>Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16062">http://arxiv.org/abs/2307.16062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zengjie Zhang, Jayden Hong, Amir Soufi Enayati, Homayoun Najjaran</li>
<li>for: 提高多度OFRobot的运动规划效率和可重用性</li>
<li>methods: 使用偏好行为假设（IBC）和动态运动原理（DMP）提高RLagent的训练速度和通用性</li>
<li>results: 在模拟和实验中比对RLagent和传统RLagent，表明提议方法具有更快的训练速度和更高的分数Here’s the breakdown of each point:</li>
<li>for: The paper is written to improve the efficiency and generalizability of reinforcement learning (RL) for motion planning of multi-degree-of-freedom robots.</li>
<li>methods: The proposed method uses implicit behavior cloning (IBC) and dynamic movement primitive (DMP) to improve the training speed and generalizability of an off-policy RL agent.</li>
<li>results: The proposed method is compared with conventional RL agents in simulation and real-robot experiments, showing faster training speed and higher scores.<details>
<summary>Abstract</summary>
Reinforcement learning (RL) for motion planning of multi-degree-of-freedom robots still suffers from low efficiency in terms of slow training speed and poor generalizability. In this paper, we propose a novel RL-based robot motion planning framework that uses implicit behavior cloning (IBC) and dynamic movement primitive (DMP) to improve the training speed and generalizability of an off-policy RL agent. IBC utilizes human demonstration data to leverage the training speed of RL, and DMP serves as a heuristic model that transfers motion planning into a simpler planning space. To support this, we also create a human demonstration dataset using a pick-and-place experiment that can be used for similar studies. Comparison studies in simulation reveal the advantage of the proposed method over the conventional RL agents with faster training speed and higher scores. A real-robot experiment indicates the applicability of the proposed method to a simple assembly task. Our work provides a novel perspective on using motion primitives and human demonstration to leverage the performance of RL for robot applications.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:利用强化学习（RL）的动作规划方法，多度 freedom 机器人的准确率仍然受到低效率的困扰，即训练速度慢和泛化能力差。在这篇论文中，我们提出了一种基于RL的机器人动作规划框架，使用隐式行为封装（IBC）和动态运动 primitives（DMP）来提高RLagent的训练速度和泛化能力。IBC利用人类示范数据来利用RL的训练速度，而DMP作为一种启发模型，将动作规划转移到简单的规划空间。为支持这一点，我们还创建了一个人类示范数据集，用于类似的研究。在模拟环境中的比较研究表明，提案方法比普通RL代理人具有更快的训练速度和更高的分数。一个真实机器人实验表明提案方法对简单的组装任务有应用性。我们的工作提供了一种新的思路，利用动作 primitives 和人类示范来提高RL的表现 для机器人应用。
</details></li>
</ul>
<hr>
<h2 id="Click-Conversion-Multi-Task-Model-with-Position-Bias-Mitigation-for-Sponsored-Search-in-eCommerce"><a href="#Click-Conversion-Multi-Task-Model-with-Position-Bias-Mitigation-for-Sponsored-Search-in-eCommerce" class="headerlink" title="Click-Conversion Multi-Task Model with Position Bias Mitigation for Sponsored Search in eCommerce"></a>Click-Conversion Multi-Task Model with Position Bias Mitigation for Sponsored Search in eCommerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16060">http://arxiv.org/abs/2307.16060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yibo Wang, Yanbing Xue, Bo Liu, Musen Wen, Wenting Zhao, Stephen Guo, Philip S. Yu<br>for:  This paper aims to mitigate position bias in ranking systems, particularly in e-commerce sponsored product search.methods: The authors propose two position-bias-free CTR and CVR prediction models: Position-Aware Click-Conversion (PACC) and PACC via Position Embedding (PACC-PE). PACC is built upon probability decomposition, while PACC-PE utilizes neural networks to model product-specific position information as embedding.results: The proposed models have better ranking effectiveness and can greatly alleviate position bias in both CTR and CVR prediction, as shown in experiments on the E-commerce sponsored product search dataset.<details>
<summary>Abstract</summary>
Position bias, the phenomenon whereby users tend to focus on higher-ranked items of the search result list regardless of the actual relevance to queries, is prevailing in many ranking systems. Position bias in training data biases the ranking model, leading to increasingly unfair item rankings, click-through-rate (CTR), and conversion rate (CVR) predictions. To jointly mitigate position bias in both item CTR and CVR prediction, we propose two position-bias-free CTR and CVR prediction models: Position-Aware Click-Conversion (PACC) and PACC via Position Embedding (PACC-PE). PACC is built upon probability decomposition and models position information as a probability. PACC-PE utilizes neural networks to model product-specific position information as embedding. Experiments on the E-commerce sponsored product search dataset show that our proposed models have better ranking effectiveness and can greatly alleviate position bias in both CTR and CVR prediction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT位置偏见，用户偏好搜索结果列表中高排名的项目，不管它们与查询的实际相关性有多少关系，是许多排名系统中的现象。位置偏见在训练数据中扭曲排名模型，导致排名预测变得越来越不公平，点击率（CTR）和转化率（CVR）预测也受到影响。为了同时消除位置偏见在ITEM CTR和 CVR预测中，我们提议了两种位置偏见自由的预测模型：Position-Aware Click-Conversion（PACC）和PACC via Position Embedding（PACC-PE）。PACC基于概率分解，将位置信息视为概率；PACC-PE使用神经网络来模型产品特定的位置信息作为嵌入。在电商推荐 searched product dataset上的实验表明，我们的提议模型在排名效果和减少位置偏见方面具有显著优势。Note: The text has been translated using the Google Translate API, which may not be perfect and may not capture all the nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Robustness-of-Test-Selection-Methods-for-Deep-Neural-Networks"><a href="#Evaluating-the-Robustness-of-Test-Selection-Methods-for-Deep-Neural-Networks" class="headerlink" title="Evaluating the Robustness of Test Selection Methods for Deep Neural Networks"></a>Evaluating the Robustness of Test Selection Methods for Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01314">http://arxiv.org/abs/2308.01314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Wei Ma, Mike Papadakis, Yves Le Traon</li>
<li>for: This paper aims to investigate the reliability of multiple test selection methods for deep learning-based systems, and to identify potential pitfalls in their construction.</li>
<li>methods: The paper examines 11 test selection methods from top-tier venues, and conducts a study on five datasets with two model architectures per dataset to empirically confirm the existence of pitfalls.</li>
<li>results: The paper finds that methods for fault detection suffer from test data that are correctly classified but uncertain, or misclassified but confident, leading to a drop in test relative coverage of up to 86.85%. Additionally, methods for performance estimation are sensitive to the choice of intermediate-layer output, and can be less effective than random selection when using an inappropriate layer.<details>
<summary>Abstract</summary>
Testing deep learning-based systems is crucial but challenging due to the required time and labor for labeling collected raw data. To alleviate the labeling effort, multiple test selection methods have been proposed where only a subset of test data needs to be labeled while satisfying testing requirements. However, we observe that such methods with reported promising results are only evaluated under simple scenarios, e.g., testing on original test data. This brings a question to us: are they always reliable? In this paper, we explore when and to what extent test selection methods fail for testing. Specifically, first, we identify potential pitfalls of 11 selection methods from top-tier venues based on their construction. Second, we conduct a study on five datasets with two model architectures per dataset to empirically confirm the existence of these pitfalls. Furthermore, we demonstrate how pitfalls can break the reliability of these methods. Concretely, methods for fault detection suffer from test data that are: 1) correctly classified but uncertain, or 2) misclassified but confident. Remarkably, the test relative coverage achieved by such methods drops by up to 86.85%. On the other hand, methods for performance estimation are sensitive to the choice of intermediate-layer output. The effectiveness of such methods can be even worse than random selection when using an inappropriate layer.
</details>
<details>
<summary>摘要</summary>
测试深度学习系统是关键但困难的，因为需要大量的时间和劳动来标注采集的原始数据。为了减轻标注劳动，许多测试选择方法已经被提出，只需要标注一个子集的测试数据而不符合测试要求。然而，我们发现这些方法在报道了Promising结果后，很少被评估在复杂的场景下。这引发了我们的问题：这些方法是否总是可靠？在这篇论文中，我们探索测试选择方法在测试时会失败的情况。 Specifically, first, we identify potential pitfalls of 11 selection methods from top-tier venues based on their construction. Second, we conduct a study on five datasets with two model architectures per dataset to empirically confirm the existence of these pitfalls. Furthermore, we demonstrate how pitfalls can break the reliability of these methods. Concretely, methods for fault detection suffer from test data that are: 1) correctly classified but uncertain, or 2) misclassified but confident. Remarkably, the test relative coverage achieved by such methods drops by up to 86.85%. On the other hand, methods for performance estimation are sensitive to the choice of intermediate-layer output. The effectiveness of such methods can be even worse than random selection when using an inappropriate layer.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Exotic-Magnetic-Phases-in-Fibonacci-Quasicrystalline-Stacking-of-Ferromagnetic-Layers-through-Machine-Learning"><a href="#Unveiling-Exotic-Magnetic-Phases-in-Fibonacci-Quasicrystalline-Stacking-of-Ferromagnetic-Layers-through-Machine-Learning" class="headerlink" title="Unveiling Exotic Magnetic Phases in Fibonacci Quasicrystalline Stacking of Ferromagnetic Layers through Machine Learning"></a>Unveiling Exotic Magnetic Phases in Fibonacci Quasicrystalline Stacking of Ferromagnetic Layers through Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16052">http://arxiv.org/abs/2307.16052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo S. Cornaglia, Matias Nuñez, D. J. Garcia</li>
<li>for: 这个研究探讨了使用短距离磁Materials实现的菲波奈克里斯Stacking结构的磁性性质。</li>
<li>methods: 该研究使用了机器学习方法来探索这个 quasi-crystalline 系统中磁性行为的复杂关系，并提供了磁性相对图。</li>
<li>results: 研究发现了一种特殊的斜排列相，其中磁化程度递减Logarithmically with the stack height。此外，研究还发现了其他斜排列相和非斜排列相。<details>
<summary>Abstract</summary>
In this study, we conduct a comprehensive theoretical analysis of a Fibonacci quasicrystalline stacking of ferromagnetic layers, potentially realizable using van der Waals magnetic materials. We construct a model of this magnetic heterostructure, which includes up to second neighbor interlayer magnetic interactions, that displays a complex relationship between geometric frustration and magnetic order in this quasicrystalline system. To navigate the parameter space and identify distinct magnetic phases, we employ a machine learning approach, which proves to be a powerful tool in revealing the complex magnetic behavior of this system. We offer a thorough description of the magnetic phase diagram as a function of the model parameters. Notably, we discover among other collinear and non-collinear phases, a unique ferromagnetic alternating helical phase. In this non-collinear quasiperiodic ferromagnetic configuration the magnetization decreases logarithmically with the stack height.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们进行了详细的理论分析，涉及到费波纳契镁磁层的杂合堆叠，可能通过磁性van der Waals材料实现。我们构建了这种磁性异构体系的模型，包括最多的第二邻居层磁交互，这种系统显示了复杂的几何阻碍和磁ORDER之间的关系。为了探索参数空间并特征化不同磁相，我们使用机器学习方法，这证明了这种方法在揭示这种系统的复杂磁性行为上是一个强大工具。我们提供了磁相图的全面描述，其中包括了模型参数的函数。特别是，我们发现了一种独特的梯形扁平磁相，在堆高上呈指数减少的情况下，磁化强度下降。
</details></li>
</ul>
<hr>
<h2 id="Okapi-Instruction-tuned-Large-Language-Models-in-Multiple-Languages-with-Reinforcement-Learning-from-Human-Feedback"><a href="#Okapi-Instruction-tuned-Large-Language-Models-in-Multiple-Languages-with-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback"></a>Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16039">http://arxiv.org/abs/2307.16039</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nlp-uoregon/okapi">https://github.com/nlp-uoregon/okapi</a></li>
<li>paper_authors: Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen</li>
<li>for: 本研究旨在提高大型自然语言模型（LLM）的可用性和影响力，通过训练和调教 LLM 可以更好地适应人类的期望，从而实现出色的学习能力。</li>
<li>methods: 本研究使用了 supervised fine-tuning (SFT) 和 reinforcement learning from human feedback (RLHF) 两种方法进行 instruction tuning，以便在多种语言上实现最佳的性能。</li>
<li>results: 我们的实验表明，使用 RLHF 进行多语言 instruction tuning 可以超过 SFT 的性能，并且可以在不同的基模型和数据集上实现优秀的结果。<details>
<summary>Abstract</summary>
A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi.
</details>
<details>
<summary>摘要</summary>
键技术 для开发大语言模型（LLM）包括指令调整，以使模型的回答与人类期望相一致，从而实现很好的学习能力。目前，supervised fine-tuning（SFT）和人类反馈学习（RLHF）是两种主要的指令调整方法，用于生产最佳商业LLM（如ChatGPT）。为了提高LLM的研究和开发的可accessibility，各种指令调整的开源LLM也已经被引入，如Alpaca和Vicuna等。然而，现有的开源LLM只有在英语和一些流行的语言上进行了指令调整，这限制了它们在全球各种语言中的影响和可用性。在最近几年中，有一些工作尝试了在多语言中进行指令调整，但只使用了SFT作为唯一的调整方法。这留下了一个大的 gap，即在多语言中使用RLHF进行细调的可能性。为了解决这个问题，我们提出了Okapi，首个基于RLHF的多语言指令调整系统。Okapi introduce了26种多语言的指令和回答排名数据，以便进行实验和未来多语言LLM研究的发展。我们还提供了多语言生成LLM的评价数据集。我们的实验表明，RLHF在多语言指令调整中的优势，比SFT在不同的基本模型和数据集上。我们的框架和资源在https://github.com/nlp-uoregon/Okapi上发布。
</details></li>
</ul>
<hr>
<h2 id="Developing-novel-ligands-with-enhanced-binding-affinity-for-the-sphingosine-1-phosphate-receptor-1-using-machine-learning"><a href="#Developing-novel-ligands-with-enhanced-binding-affinity-for-the-sphingosine-1-phosphate-receptor-1-using-machine-learning" class="headerlink" title="Developing novel ligands with enhanced binding affinity for the sphingosine 1-phosphate receptor 1 using machine learning"></a>Developing novel ligands with enhanced binding affinity for the sphingosine 1-phosphate receptor 1 using machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16037">http://arxiv.org/abs/2307.16037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Colin Zhang, Yang Ha</li>
<li>for: 这个研究旨在使用机器学习模型加速多斯普朗肌病（MS）的药物发现过程，并通过分析蛋白质-药物交互的化学特性，探索新的药物设计方法。</li>
<li>methods: 该研究使用了自适应器机器学习模型，将化学式转化为数学向量，并生成了超过500个分子变体基于斯皮诺模（siponimod），其中25种分子具有更高的靶蛋白S1PR1的绑定亲和力。</li>
<li>results: 该研究发现了6种有药理性和易合成的药物候选者，并通过分析这些药物与S1PR1的绑定交互，探讨了一些靶蛋白-药物交互的化学特性，这些结果表明机器学习可以加速药物发现过程，并为药物设计提供新的视角。<details>
<summary>Abstract</summary>
Multiple sclerosis (MS) is a debilitating neurological disease affecting nearly one million people in the United States. Sphingosine-1-phosphate receptor 1, or S1PR1, is a protein target for MS. Siponimod, a ligand of S1PR1, was approved by the FDA in 2019 for MS treatment, but there is a demonstrated need for better therapies. To this end, we finetuned an autoencoder machine learning model that converts chemical formulas into mathematical vectors and generated over 500 molecular variants based on siponimod, out of which 25 compounds had higher predicted binding affinity to S1PR1. The model was able to generate these ligands in just under one hour. Filtering these compounds led to the discovery of six promising candidates with good drug-like properties and ease of synthesis. Furthermore, by analyzing the binding interactions for these ligands, we uncovered several chemical properties that contribute to high binding affinity to S1PR1. This study demonstrates that machine learning can accelerate the drug discovery process and reveal new insights into protein-drug interactions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MUSE-Multi-View-Contrastive-Learning-for-Heterophilic-Graphs"><a href="#MUSE-Multi-View-Contrastive-Learning-for-Heterophilic-Graphs" class="headerlink" title="MUSE: Multi-View Contrastive Learning for Heterophilic Graphs"></a>MUSE: Multi-View Contrastive Learning for Heterophilic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16026">http://arxiv.org/abs/2307.16026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengyi Yuan, Minjie Chen, Xiang Li<br>for: 这篇文章的目的是提出一种基于多视图对照学习的自动学习模型，即MUSE，以解决传统Graph Neural Networks（GNN）中的标签依赖和泛化性问题。methods: 该模型使用了两种视图来捕捉egos节点和其邻居的信息，即GNNs增强了对照学习的视图，然后将这两个视图融合以生成节点表示。此外，该模型还使用了对照强化和信息整合控制器来模型节点邻居相似性的多样性。results: 对于9个benchmark数据集，我们的实验结果表明MUSE模型在节点分类和聚类任务中具有显著的效果。<details>
<summary>Abstract</summary>
In recent years, self-supervised learning has emerged as a promising approach in addressing the issues of label dependency and poor generalization performance in traditional GNNs. However, existing self-supervised methods have limited effectiveness on heterophilic graphs, due to the homophily assumption that results in similar node representations for connected nodes. In this work, we propose a multi-view contrastive learning model for heterophilic graphs, namely, MUSE. Specifically, we construct two views to capture the information of the ego node and its neighborhood by GNNs enhanced with contrastive learning, respectively. Then we integrate the information from these two views to fuse the node representations. Fusion contrast is utilized to enhance the effectiveness of fused node representations. Further, considering that the influence of neighboring contextual information on information fusion may vary across different ego nodes, we employ an information fusion controller to model the diversity of node-neighborhood similarity at both the local and global levels. Finally, an alternating training scheme is adopted to ensure that unsupervised node representation learning and information fusion controller can mutually reinforce each other. We conduct extensive experiments to evaluate the performance of MUSE on 9 benchmark datasets. Our results show the effectiveness of MUSE on both node classification and clustering tasks.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:近年来，自我超vision学习 emerged as a promising approach to address the issues of label dependency and poor generalization performance in traditional GNNs. However, existing self-supervised methods have limited effectiveness on heterophilic graphs due to the homophily assumption, which results in similar node representations for connected nodes. In this work, we propose a multi-view contrastive learning model for heterophilic graphs, called MUSE. Specifically, we construct two views to capture the information of the ego node and its neighborhood using GNNs enhanced with contrastive learning, respectively. Then we integrate the information from these two views to fuse the node representations. Fusion contrast is utilized to enhance the effectiveness of fused node representations. Moreover, considering that the influence of neighboring contextual information on information fusion may vary across different ego nodes, we employ an information fusion controller to model the diversity of node-neighborhood similarity at both the local and global levels. Finally, an alternating training scheme is adopted to ensure that unsupervised node representation learning and information fusion controller can mutually reinforce each other. We conduct extensive experiments to evaluate the performance of MUSE on 9 benchmark datasets. Our results show the effectiveness of MUSE on both node classification and clustering tasks.
</details></li>
</ul>
<hr>
<h2 id="Discrete-neural-nets-and-polymorphic-learning"><a href="#Discrete-neural-nets-and-polymorphic-learning" class="headerlink" title="Discrete neural nets and polymorphic learning"></a>Discrete neural nets and polymorphic learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00677">http://arxiv.org/abs/2308.00677</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caten2/tripods2021ua">https://github.com/caten2/tripods2021ua</a></li>
<li>paper_authors: Charlotte Aten</li>
<li>for: 这篇论文旨在统一提出神经网络和 универсаль算法的关系，并介绍一种基于 polymorphisms of relational structures 的学习算法。</li>
<li>methods: 这篇论文使用了 Murski\u{i} 的 universal algebra 结论和 Cybenko 的神经网络 universal approximation 结论，并提出了一种基于 polymorphisms of relational structures 的学习算法。</li>
<li>results: 这篇论文的结果表明，使用这种学习算法可以解决一些 классические学习任务。<details>
<summary>Abstract</summary>
Theorems from universal algebra such as that of Murski\u{i} from the 1970s have a striking similarity to universal approximation results for neural nets along the lines of Cybenko's from the 1980s. We consider here a discrete analogue of the classical notion of a neural net which places these results in a unified setting. We introduce a learning algorithm based on polymorphisms of relational structures and show how to use it for a classical learning task.
</details>
<details>
<summary>摘要</summary>
theorem 从通用代数如 murski的 1970年代有一种 striking similarity 与 neural network 的 universal approximation result 类似，例如 cybenko 在 1980年代的 result。我们在这里 Consider 一个离散的 neural network 的类传统的概念，并将这些结果集成到一个统一的设定中。我们介绍一种基于 relational structure 的学习算法，并证明可以用它来解决 classical learning task。Note:* "Murski" should be written as "穆尔斯基" (Mù'ěrskī) in Simplified Chinese.* "Cybenko" should be written as "谢本科" (Xiè Běnkē) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Fuzzy-Logic-Visual-Network-FLVN-A-neuro-symbolic-approach-for-visual-features-matching"><a href="#Fuzzy-Logic-Visual-Network-FLVN-A-neuro-symbolic-approach-for-visual-features-matching" class="headerlink" title="Fuzzy Logic Visual Network (FLVN): A neuro-symbolic approach for visual features matching"></a>Fuzzy Logic Visual Network (FLVN): A neuro-symbolic approach for visual features matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16019">http://arxiv.org/abs/2307.16019</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/grains2/flvn">https://gitlab.com/grains2/flvn</a></li>
<li>paper_authors: Francesco Manigrasso, Lia Morra, Fabrizio Lamberti</li>
<li>for: 这个研究目的是实现具有symbolic知识表示和深度神经网络学习的neuro-symbolic整合。</li>
<li>methods: 这个研究使用了Logic Tensor Networks (LTNs)来将背景知识转换为可微分的操作，并将其应用到零例学习（ZSL）分类任务中。</li>
<li>results: 这个研究提出了Fuzzy Logic Visual Network (FLVN)，它在neuro-symbolic LTN框架下学习了一个可视 Semantic  embedding 空间，并将内在知识（例如类别和概念阶层）统一到这个 embedding 空间中。 FLVN 在 Generalized ZSL（GZSL）测试 benchmark 上表现出色，与其他最新的 ZSL 方法相比，具有较少的计算负载。<details>
<summary>Abstract</summary>
Neuro-symbolic integration aims at harnessing the power of symbolic knowledge representation combined with the learning capabilities of deep neural networks. In particular, Logic Tensor Networks (LTNs) allow to incorporate background knowledge in the form of logical axioms by grounding a first order logic language as differentiable operations between real tensors. Yet, few studies have investigated the potential benefits of this approach to improve zero-shot learning (ZSL) classification. In this study, we present the Fuzzy Logic Visual Network (FLVN) that formulates the task of learning a visual-semantic embedding space within a neuro-symbolic LTN framework. FLVN incorporates prior knowledge in the form of class hierarchies (classes and macro-classes) along with robust high-level inductive biases. The latter allow, for instance, to handle exceptions in class-level attributes, and to enforce similarity between images of the same class, preventing premature overfitting to seen classes and improving overall performance. FLVN reaches state of the art performance on the Generalized ZSL (GZSL) benchmarks AWA2 and CUB, improving by 1.3% and 3%, respectively. Overall, it achieves competitive performance to recent ZSL methods with less computational overhead. FLVN is available at https://gitlab.com/grains2/flvn.
</details>
<details>
<summary>摘要</summary>
In this study, we present the Fuzzy Logic Visual Network (FLVN), which formulates the task of learning a visual-semantic embedding space within a neuro-symbolic LTN framework. FLVN incorporates prior knowledge in the form of class hierarchies and robust high-level inductive biases, allowing for exception handling and similarity enforcement between images of the same class. This improves overall performance and reduces premature overfitting to seen classes.FLVN achieves state-of-the-art performance on the Generalized ZSL (GZSL) benchmarks AWA2 and CUB, improving by 1.3% and 3%, respectively. It also achieves competitive performance to recent ZSL methods with less computational overhead. FLVN is available at <https://gitlab.com/grains2/flvn>.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/30/cs.LG_2023_07_30/" data-id="cloh7tqit00ly7b8885n967tv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/30/eess.IV_2023_07_30/" class="article-date">
  <time datetime="2023-07-30T09:00:00.000Z" itemprop="datePublished">2023-07-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/30/eess.IV_2023_07_30/">eess.IV - 2023-07-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unsupervised-Decomposition-Networks-for-Bias-Field-Correction-in-MR-Image"><a href="#Unsupervised-Decomposition-Networks-for-Bias-Field-Correction-in-MR-Image" class="headerlink" title="Unsupervised Decomposition Networks for Bias Field Correction in MR Image"></a>Unsupervised Decomposition Networks for Bias Field Correction in MR Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16219">http://arxiv.org/abs/2307.16219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leongdong/bias-decomposition-networks">https://github.com/leongdong/bias-decomposition-networks</a></li>
<li>paper_authors: Dong Liang, Xingyu Qiu, Kuanquan Wang, Gongning Luo, Wei Wang, Yashu Liu</li>
<li>for: 这个研究旨在提出一种不需要监督学习的批处理网络，以获取受扭曲的MR影像中的偏差场。</li>
<li>methods: 该方法使用了一种由批处理网络组成的分解方法，包括一个分类部分和一个估算部分，以便分解受扭曲的MR影像。</li>
<li>results: 实验结果表明，该方法可以准确地估算偏差场并生成更好的偏差 corrections。 codes 可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://github.com/LeongDong/Bias-Decomposition-Networks%E3%80%82">https://github.com/LeongDong/Bias-Decomposition-Networks。</a><details>
<summary>Abstract</summary>
Bias field, which is caused by imperfect MR devices or imaged objects, introduces intensity inhomogeneity into MR images and degrades the performance of MR image analysis methods. Many retrospective algorithms were developed to facilitate the bias correction, to which the deep learning-based methods outperformed. However, in the training phase, the supervised deep learning-based methods heavily rely on the synthesized bias field. As the formation of the bias field is extremely complex, it is difficult to mimic the true physical property of MR images by synthesized data. While bias field correction and image segmentation are strongly related, the segmentation map is precisely obtained by decoupling the bias field from the original MR image, and the bias value is indicated by the segmentation map in reverse. Thus, we proposed novel unsupervised decomposition networks that are trained only with biased data to obtain the bias-free MR images. Networks are made up of: a segmentation part to predict the probability of every pixel belonging to each class, and an estimation part to calculate the bias field, which are optimized alternately. Furthermore, loss functions based on the combination of fuzzy clustering and the multiplicative bias field are also devised. The proposed loss functions introduce the smoothness of bias field and construct the soft relationships among different classes under intra-consistency constraints. Extensive experiments demonstrate that the proposed method can accurately estimate bias fields and produce better bias correction results. The code is available on the link: https://github.com/LeongDong/Bias-Decomposition-Networks.
</details>
<details>
<summary>摘要</summary>
�� bias �eld，��由不完美的 MR 设备或图像物理特性所引起的，会导致 MR 图像中的�Intensity 不均��，从而降低 MR 图像分析方法的性能。许多retrospective算法已经开发来简化偏好 corrections，其中深度学习基于方法在训练阶段更高效。然而，在训练阶段，深度学习基于方法强依赖于制成的偏好场。由于偏好场的形成非常复杂，难以通过制成的数据来模拟真实的物理特性。而偏好场 correction 和图像 segmentation 是非常相关的，可以通过分解偏好场来获得不受偏好影响的 MR 图像。因此，我们提出了一种新的无监督分解网络，该网络通过在偏好数据上进行训练来获得偏好场 corrections。该网络由两部分组成：一个分类部分用于预测每个像素属于哪个类别，以及一个估计部分用于计算偏好场，这两个部分在 alternate 中优化。此外，我们还提出了一种基于多元偏好场的损失函数，该损失函数引入了偏好场的缓和性和不同类别之间的软连接。广泛的实验表明，我们的方法可以准确地估计偏好场并生成更好的偏好 corrections 结果。代码可以在以下链接获取：https://github.com/LeongDong/Bias-Decomposition-Networks。
</details></li>
</ul>
<hr>
<h2 id="Gastrointestinal-Mucosal-Problems-Classification-with-Deep-Learning"><a href="#Gastrointestinal-Mucosal-Problems-Classification-with-Deep-Learning" class="headerlink" title="Gastrointestinal Mucosal Problems Classification with Deep Learning"></a>Gastrointestinal Mucosal Problems Classification with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16198">http://arxiv.org/abs/2307.16198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadhasan Goharian, Vahid Goharian, Hamidreza Bolhasani</li>
<li>for: 旨在检测胃肠粘膜变化，早期诊断和预防胃肠癌。</li>
<li>methods: 使用深度学习算法，特别是基于Convolutional Neural Networks（CNNs）的传送学习（TL）。</li>
<li>results: 在测试图像中，模型精度达93%，并在实际检anoscopy和colonoscopy视频中进行了预测。<details>
<summary>Abstract</summary>
Gastrointestinal mucosal changes can cause cancers after some years and early diagnosing them can be very useful to prevent cancers and early treatment. In this article, 8 classes of mucosal changes and anatomical landmarks including Polyps, Ulcerative Colitis, Esophagitis, Normal Z-Line, Normal Pylorus, Normal Cecum, Dyed Lifted Polyps, and Dyed Lifted Margin were predicted by deep learning. We used neural networks in this article. It is a black box artificial intelligence algorithm that works like a human neural system. In this article, Transfer Learning (TL) based on the Convolutional Neural Networks (CNNs), which is one of the well-known types of neural networks in image processing is used. We compared some famous CNN architecture including VGG, Inception, Xception, and ResNet. Our best model got 93% accuracy in test images. At last, we used our model in some real endoscopy and colonoscopy movies to classify problems.
</details>
<details>
<summary>摘要</summary>
胃肠内膜变化可能导致癌变，早期诊断可以有助于预防癌变并提供早期治疗。在这篇文章中，我们预测了8种胃肠内膜变化和解剖学特征，包括贫血溃疡、急性肠炎、胃肠内膜Z线、胃肠内膜pylorus、胃肠内膜 Cecum、染料吸引溃疡和染料吸引边缘。我们使用了神经网络来进行预测。神经网络是一种黑盒子人工智能算法，它工作如同人类神经系统一样。在这篇文章中，我们使用了传输学（TL）基于卷积神经网络（CNNs），这是一种广泛使用的神经网络类型在图像处理中。我们比较了一些著名的CNN架构，包括VGG、Inception、Xception和ResNet。我们的最佳模型在测试图像中达到了93%的准确率。最后，我们使用了我们的模型在一些真实的病理影像中进行分类。
</details></li>
</ul>
<hr>
<h2 id="StarSRGAN-Improving-Real-World-Blind-Super-Resolution"><a href="#StarSRGAN-Improving-Real-World-Blind-Super-Resolution" class="headerlink" title="StarSRGAN: Improving Real-World Blind Super-Resolution"></a>StarSRGAN: Improving Real-World Blind Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16169">http://arxiv.org/abs/2307.16169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kynthesis/StarSRGAN">https://github.com/kynthesis/StarSRGAN</a></li>
<li>paper_authors: Khoa D. Vo, Len T. Bui</li>
<li>for: This paper is written for improving the blind super-resolution (SR) in computer vision, aiming to enhance the resolution of low-resolution images without prior knowledge of the degradation process.</li>
<li>methods: The paper introduces StarSRGAN, a novel GAN model that utilizes 5 various architectures to achieve state-of-the-art (SOTA) performance in blind SR tasks. The model is designed to provide visually compelling outcomes with improved super-resolved quality.</li>
<li>results: The experimental comparisons with Real-ESRGAN show that StarSRGAN achieves roughly 10% better performance on the MANIQA and AHIQ measures, while StarSRGAN Lite provides approximately 7.5 times faster reconstruction speed with only a slight decrease in image quality. The codes are available at <a target="_blank" rel="noopener" href="https://github.com/kynthesis/StarSRGAN">https://github.com/kynthesis/StarSRGAN</a>.<details>
<summary>Abstract</summary>
The aim of blind super-resolution (SR) in computer vision is to improve the resolution of an image without prior knowledge of the degradation process that caused the image to be low-resolution. The State of the Art (SOTA) model Real-ESRGAN has advanced perceptual loss and produced visually compelling outcomes using more complex degradation models to simulate real-world degradations. However, there is still room to improve the super-resolved quality of Real-ESRGAN by implementing recent techniques. This research paper introduces StarSRGAN, a novel GAN model designed for blind super-resolution tasks that utilize 5 various architectures. Our model provides new SOTA performance with roughly 10% better on the MANIQA and AHIQ measures, as demonstrated by experimental comparisons with Real-ESRGAN. In addition, as a compact version, StarSRGAN Lite provides approximately 7.5 times faster reconstruction speed (real-time upsampling from 540p to 4K) but can still keep nearly 90% of image quality, thereby facilitating the development of a real-time SR experience for future research. Our codes are released at https://github.com/kynthesis/StarSRGAN.
</details>
<details>
<summary>摘要</summary>
目的是提高计算机视觉中的盲超分辨率（SR），无需先知道降低过程的信息，以提高图像的分辨率。现有的最佳实践（SOTA）模型Real-ESRGAN已经使用了更复杂的降低模型来模拟实际世界中的降低。然而，还有余地可以提高Real-ESRGAN中的超分辨率质量。这篇研究论文介绍了StarSRGAN，一种新的GAN模型，用于盲SR任务。我们的模型使用了5种不同的建筑，并提供了新的SOTA性能，在MANIQA和AHIQ测试中比Real-ESRGAN提高了约10%。此外，我们还提供了一个快速重建速度版本StarSRGAN Lite，可以在540p到4K的快速扩展中实现实时SR体验。我们的代码在https://github.com/kynthesis/StarSRGAN上发布。
</details></li>
</ul>
<hr>
<h2 id="Structure-Preserving-Synthesis-MaskGAN-for-Unpaired-MR-CT-Translation"><a href="#Structure-Preserving-Synthesis-MaskGAN-for-Unpaired-MR-CT-Translation" class="headerlink" title="Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation"></a>Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16143">http://arxiv.org/abs/2307.16143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HieuPhan33/MaskGAN">https://github.com/HieuPhan33/MaskGAN</a></li>
<li>paper_authors: Minh Hieu Phan, Zhibin Liao, Johan W. Verjans, Minh-Son To</li>
<li>for: 这篇论文旨在提供一个可靠且cost-effective的医疗影像合成方法，以便对于医疗影像资料的损失或缺乏实现合成。</li>
<li>methods: 这篇论文使用了CycleGAN的架构，并将自动提取的粗糙面给入力到架构中，以便保持体Structural consistency。</li>
<li>results: 实验结果显示，MaskGAN在一个儿童医疗领域的复杂数据集上表现出色，能够保持体 Structural consistency，而不需要专家的标注。<details>
<summary>Abstract</summary>
Medical image synthesis is a challenging task due to the scarcity of paired data. Several methods have applied CycleGAN to leverage unpaired data, but they often generate inaccurate mappings that shift the anatomy. This problem is further exacerbated when the images from the source and target modalities are heavily misaligned. Recently, current methods have aimed to address this issue by incorporating a supplementary segmentation network. Unfortunately, this strategy requires costly and time-consuming pixel-level annotations. To overcome this problem, this paper proposes MaskGAN, a novel and cost-effective framework that enforces structural consistency by utilizing automatically extracted coarse masks. Our approach employs a mask generator to outline anatomical structures and a content generator to synthesize CT contents that align with these structures. Extensive experiments demonstrate that MaskGAN outperforms state-of-the-art synthesis methods on a challenging pediatric dataset, where MR and CT scans are heavily misaligned due to rapid growth in children. Specifically, MaskGAN excels in preserving anatomical structures without the need for expert annotations. The code for this paper can be found at https://github.com/HieuPhan33/MaskGAN.
</details>
<details>
<summary>摘要</summary>
医学图像生成是一项具有挑战性的任务，因为精度匹配数据罕见。许多方法使用CycleGAN来利用无对数据，但它们经常生成错误的映射，导致身体结构的偏移。这个问题更加严重当图像来源和目标模式之间的偏移很大。目前的方法通过添加辅助分割网络来解决这个问题，但这需要成本和时间昂贵的像素级别标注。为了缓解这个问题，这篇论文提出了MaskGAN，一种新的和经济的框架，通过自动提取的粗略Mask来保持结构一致性。我们的方法使用Mask生成器将体结构析出，并使用内容生成器Synthesize CT内容，与这些结构相对应。我们的实验表明，MaskGAN在一个复杂的儿童数据集上表现出色，特别是在MR和CT扫描中存在快速增长的儿童身体中，具有优秀的结构保持性，而不需要专家标注。相关代码可以在https://github.com/HieuPhan33/MaskGAN中找到。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Neural-Representation-in-Medical-Imaging-A-Comparative-Survey"><a href="#Implicit-Neural-Representation-in-Medical-Imaging-A-Comparative-Survey" class="headerlink" title="Implicit Neural Representation in Medical Imaging: A Comparative Survey"></a>Implicit Neural Representation in Medical Imaging: A Comparative Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16142">http://arxiv.org/abs/2307.16142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mindflow-institue/awesome-implicit-neural-representations-in-medical-imaging">https://github.com/mindflow-institue/awesome-implicit-neural-representations-in-medical-imaging</a></li>
<li>paper_authors: Amirali Molaei, Amirhossein Aminimehr, Armin Tavakoli, Amirhossein Kazerouni, Bobby Azad, Reza Azad, Dorit Merhof<br>for: This survey provides a comprehensive overview of implicit neural representations (INRs) in the field of medical imaging, exploring their applications and advantages in various medical imaging tasks.methods: The survey discusses the use of INRs in image reconstruction, segmentation, registration, novel view synthesis, and compression, highlighting their resolution-agnostic nature, memory efficiency, ability to avoid locality biases, and differentiability.results: The survey addresses the challenges and considerations specific to medical imaging data, such as data availability, computational complexity, and dynamic clinical scene analysis, and identifies future research directions and opportunities, including integration with multi-modal imaging, real-time and interactive systems, and domain adaptation for clinical decision support.<details>
<summary>Abstract</summary>
Implicit neural representations (INRs) have gained prominence as a powerful paradigm in scene reconstruction and computer graphics, demonstrating remarkable results. By utilizing neural networks to parameterize data through implicit continuous functions, INRs offer several benefits. Recognizing the potential of INRs beyond these domains, this survey aims to provide a comprehensive overview of INR models in the field of medical imaging. In medical settings, numerous challenging and ill-posed problems exist, making INRs an attractive solution. The survey explores the application of INRs in various medical imaging tasks, such as image reconstruction, segmentation, registration, novel view synthesis, and compression. It discusses the advantages and limitations of INRs, highlighting their resolution-agnostic nature, memory efficiency, ability to avoid locality biases, and differentiability, enabling adaptation to different tasks. Furthermore, the survey addresses the challenges and considerations specific to medical imaging data, such as data availability, computational complexity, and dynamic clinical scene analysis. It also identifies future research directions and opportunities, including integration with multi-modal imaging, real-time and interactive systems, and domain adaptation for clinical decision support. To facilitate further exploration and implementation of INRs in medical image analysis, we have provided a compilation of cited studies along with their available open-source implementations on \href{https://github.com/mindflow-institue/Awesome-Implicit-Neural-Representations-in-Medical-imaging}. Finally, we aim to consistently incorporate the most recent and relevant papers regularly.
</details>
<details>
<summary>摘要</summary>
启发神经表示（INR）在场景重建和计算机图形领域已经崭新出名，表现出色。通过使用神经网络来参数化数据通过间接连续函数，INR提供了多个优势。认识到INR在医疗领域之外的潜在应用，这份报告提供了医学成像领域INR模型的全面回顾。在医疗设置下，存在许多复杂和不稳定的问题，使INR成为一种吸引人的解决方案。本报告探讨了INR在各种医学成像任务中的应用，如图像重建、分割、注册、新视图生成和压缩。它讨论了INR的优点和限制，包括其分辨率不依赖、内存效率高、避免地方偏好和可导 differentiability，以便适应不同任务。此外，报告还考虑了医学成像数据特有的挑战和考虑因素，如数据可用性、计算复杂度和临床Scene analysis。最后，报告还提出了未来研究方向和机会，包括与多模态成像集成、实时交互系统和适应医疗决策的领域适应。为便于进一步探索和实现INR在医学成像分析中，我们在\href{https://github.com/mindflow-institue/Awesome-Implicit-Neural-Representations-in-Medical-imaging}提供了参考文献和其可用的开源实现。
</details></li>
</ul>
<hr>
<h2 id="RIS-Enhanced-Semantic-Communications-Adaptive-to-User-Requirements"><a href="#RIS-Enhanced-Semantic-Communications-Adaptive-to-User-Requirements" class="headerlink" title="RIS-Enhanced Semantic Communications Adaptive to User Requirements"></a>RIS-Enhanced Semantic Communications Adaptive to User Requirements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16100">http://arxiv.org/abs/2307.16100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiwen Jiang, Chao-Kai Wen, Shi Jin, Geoffrey Ye Li</li>
<li>for: 这个论文是为了提出一个基于智能表面的对话传输框架，以满足不断变化的用户需求和环境。</li>
<li>methods: 这个框架使用了智能表面来自动调整传输通道，以满足不同的用户需求和环境。它还使用了对话传输的混合编码设计和端到端训练，以提高传输效率和可靠性。</li>
<li>results:  simulations results indicate that the proposed RIS-SC framework can achieve reasonable task performance and adapt to diverse channel conditions and user requirements. However, under severe channel conditions, some semantic parts may be abandoned. To address this issue, a reconstruction method is introduced to improve visual acceptance by inferring missing semantic parts. Additionally, the framework can efficiently allocate RIS resources among multiple users in friendly channel conditions.<details>
<summary>Abstract</summary>
Semantic communication significantly reduces required bandwidth by understanding semantic meaning of the transmitted. However, current deep learning-based semantic communication methods rely on joint source-channel coding design and end-to-end training, which limits their adaptability to new physical channels and user requirements. Reconfigurable intelligent surfaces (RIS) offer a solution by customizing channels in different environments. In this study, we propose the RIS-SC framework, which allocates semantic contents with varying levels of RIS assistance to satisfy the changing user requirements. It takes into account user movement and line-of-sight obstructions, enabling the RIS resource to protect important semantics in challenging channel conditions. The simulation results indicate reasonable task performance, but some semantic parts that have no effect on task performances are abandoned under severe channel conditions. To address this issue, a reconstruction method is also introduced to improve visual acceptance by inferring those missing semantic parts. Furthermore, the framework can adjust RIS resources in friendly channel conditions to save and allocate them efficiently among multiple users. Simulation results demonstrate the adaptability and efficiency of the RIS-SC framework across diverse channel conditions and user requirements.
</details>
<details>
<summary>摘要</summary>
semantic communication 可以减少需要的带宽，因为它理解传输的 semantic 含义。但是，现有的深度学习基于 semantic communication 方法依赖于共同源-通道编码设计和端到端训练，这限制了它们在新的物理通道和用户需求中的适应性。可重配置智能表面（RIS）提供了一种解决方案，可以在不同环境中自定义通道。在本研究中，我们提出了 RIS-SC 框架，它将具有不同水平的 RIS 帮助分配到满足变化的用户需求。它考虑用户的运动和视线干扰，使得 RIS 资源能够保护重要的 semantics 在具有挑战性的通道条件下。 sim 结果表明任务性能合理，但在严重的通道条件下，一些无关任务性能的 semantic 部分会被放弃。为解决这个问题，我们还提出了一种重建方法，可以通过推理这些缺失的 semantic 部分来提高视觉接受度。此外，框架还可以在友好的通道条件下调整 RIS 资源，以efficiently 地分配它们于多个用户。 sim 结果表明 RIS-SC 框架在多种通道条件和用户需求下展示了适应性和效率。
</details></li>
</ul>
<hr>
<h2 id="A-New-Multi-Level-Hazy-Image-and-Video-Dataset-for-Benchmark-of-Dehazing-Methods"><a href="#A-New-Multi-Level-Hazy-Image-and-Video-Dataset-for-Benchmark-of-Dehazing-Methods" class="headerlink" title="A New Multi-Level Hazy Image and Video Dataset for Benchmark of Dehazing Methods"></a>A New Multi-Level Hazy Image and Video Dataset for Benchmark of Dehazing Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16050">http://arxiv.org/abs/2307.16050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bedrettin Cetinkaya, Yucel Cimtay, Fatma Nazli Gunay, Gokce Nur Yilmaz</li>
<li>for:  This study aims to present a new multi-level hazy color image dataset and compare the dehazing performance of five different dehazing methods&#x2F;models.</li>
<li>methods: The study uses color video data captured for two real scenes with controlled levels of haze, and the dehazing performance is evaluated based on SSIM, PSNR, VSI, and DISTS image quality metrics.</li>
<li>results: The results show that traditional methods can generalize the dehazing problem better than many deep learning-based methods, and the performance of deep models depends mostly on the scene and is generally poor on cross-dataset dehazing.Here’s the Chinese translation of the three key points:</li>
<li>for: 这个研究的目的是为了提供一个多级雾度的颜色图像集合，并对五种不同的抑雾方法&#x2F;模型进行比较。</li>
<li>methods: 这个研究使用了两个真实场景中的颜色视频数据，并使用了控制雾度的方式来生成多级雾度图像。抑雾性能是根据SSIM、PSNR、VSI和DISTS图像质量指标进行评估。</li>
<li>results: 结果表明，传统方法在抑雾问题上能够更好地总结，而深度学习基于的方法在不同场景下的性能很差，特别是在跨集合抑雾问题上。<details>
<summary>Abstract</summary>
The changing level of haze is one of the main factors which affects the success of the proposed dehazing methods. However, there is a lack of controlled multi-level hazy dataset in the literature. Therefore, in this study, a new multi-level hazy color image dataset is presented. Color video data is captured for two real scenes with a controlled level of haze. The distance of the scene objects from the camera, haze level, and ground truth (clear image) are available so that different dehazing methods and models can be benchmarked. In this study, the dehazing performance of five different dehazing methods/models is compared on the dataset based on SSIM, PSNR, VSI and DISTS image quality metrics. Results show that traditional methods can generalize the dehazing problem better than many deep learning based methods. The performance of deep models depends mostly on the scene and is generally poor on cross-dataset dehazing.
</details>
<details>
<summary>摘要</summary>
“雾度的变化是这些提议的滤雾方法成功的一个主要因素，但在文献中没有受控多级雾度数据集。因此，在本研究中，一个新的多级雾度彩色图像数据集被提出。实际拍摄的彩色视频数据被捕捉到两个场景中，并且有控制雾度、距离相机和真实预期（清晰图像）的资讯，以便不同的滤雾方法和模型进行比较。在本研究中，五种不同的滤雾方法/模型的比较结果显示，传统方法在不同场景下能够更好地应对滤雾问题，而深度学习基本方法则受到场景的影响，一般而言，跨数据集的滤雾性能较差。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="CoVid-19-Detection-leveraging-Vision-Transformers-and-Explainable-AI"><a href="#CoVid-19-Detection-leveraging-Vision-Transformers-and-Explainable-AI" class="headerlink" title="CoVid-19 Detection leveraging Vision Transformers and Explainable AI"></a>CoVid-19 Detection leveraging Vision Transformers and Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16033">http://arxiv.org/abs/2307.16033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pangoth Santhosh Kumar, Kundrapu Supriya, Mallikharjuna Rao K</li>
<li>for: 这个研究的目的是为了测定肺病的早期诊断，以提高病人的生存机会和质量生活。</li>
<li>methods: 这个研究使用了深度学习算法，包括卷积神经网络（CNN）、普通神经网络、视觉几何组网络（VGG）和封顶网络（Capsule Network）等，以进行肺病预测。</li>
<li>results: 这个研究发现，使用了视觉几何组网络（VGG）和封顶网络（Capsule Network）的方法可以实现肺病早期检测，并在 Covid 19 Radiography Database 上进行了训练和验证，获得了更高的准确率。<details>
<summary>Abstract</summary>
Lung disease is a common health problem in many parts of the world. It is a significant risk to people health and quality of life all across the globe since it is responsible for five of the top thirty leading causes of death. Among them are COVID 19, pneumonia, and tuberculosis, to name just a few. It is critical to diagnose lung diseases in their early stages. Several different models including machine learning and image processing have been developed for this purpose. The earlier a condition is diagnosed, the better the patient chances of making a full recovery and surviving into the long term. Thanks to deep learning algorithms, there is significant promise for the autonomous, rapid, and accurate identification of lung diseases based on medical imaging. Several different deep learning strategies, including convolutional neural networks (CNN), vanilla neural networks, visual geometry group based networks (VGG), and capsule networks , are used for the goal of making lung disease forecasts. The standard CNN has a poor performance when dealing with rotated, tilted, or other aberrant picture orientations. As a result of this, within the scope of this study, we have suggested a vision transformer based approach end to end framework for the diagnosis of lung disorders. In the architecture, data augmentation, training of the suggested models, and evaluation of the models are all included. For the purpose of detecting lung diseases such as pneumonia, Covid 19, lung opacity, and others, a specialised Compact Convolution Transformers (CCT) model have been tested and evaluated on datasets such as the Covid 19 Radiography Database. The model has achieved a better accuracy for both its training and validation purposes on the Covid 19 Radiography Database.
</details>
<details>
<summary>摘要</summary>
肺病是全球许多地区的常见健康问题。它对人们的健康和生活质量构成了重要的威胁，因为它负责全球前30名死亡原因中的5个。包括COVID-19、肺炎和结核病等在内，这些疾病的普遍性使得早期诊断变得非常重要。为了实现这一目标，许多不同的模型，包括机器学习和图像处理，已经被开发出来。随着深度学习算法的出现，对于基于医疗图像的肺病诊断，存在 significante 的承诺。在这种情况下，我们建议使用视transformer基本框架，以实现肺病诊断。在这个框架中，包括数据增强、模型训练和评估等方面。为了检测肺病如肺炎、COVID-19、肺抑制等，我们提出了一种专门的Compact Convolution Transformers（CCT）模型，并在 Covid 19 胸部X射线数据库上进行了测试和评估。该模型在训练和验证过程中具有更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="LOTUS-Learning-to-Optimize-Task-based-US-representations"><a href="#LOTUS-Learning-to-Optimize-Task-based-US-representations" class="headerlink" title="LOTUS: Learning to Optimize Task-based US representations"></a>LOTUS: Learning to Optimize Task-based US representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16021">http://arxiv.org/abs/2307.16021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yordanka Velikova, Mohammad Farid Azampour, Walter Simson, Vanessa Gonzalez Duque, Nassir Navab<br>for:The paper is written for the task of anatomical segmentation of organs in ultrasound images, specifically for diagnosis and monitoring.methods:The paper proposes a novel approach for learning to optimize task-based ultrasound image representations, using annotated CT segmentation maps as a simulation medium to generate ultrasound training data. The approach includes a fully differentiable ultrasound simulator that learns to optimize the parameters for generating physics-based ultrasound images guided by the downstream segmentation task, as well as an image adaptation network between real and simulated images to achieve simultaneous image synthesis and automatic segmentation on US images in an end-to-end training setting.results:The proposed method is evaluated on aorta and vessel segmentation tasks and shows promising quantitative results, as well as qualitative results of optimized image representations on other organs.<details>
<summary>Abstract</summary>
Anatomical segmentation of organs in ultrasound images is essential to many clinical applications, particularly for diagnosis and monitoring. Existing deep neural networks require a large amount of labeled data for training in order to achieve clinically acceptable performance. Yet, in ultrasound, due to characteristic properties such as speckle and clutter, it is challenging to obtain accurate segmentation boundaries, and precise pixel-wise labeling of images is highly dependent on the expertise of physicians. In contrast, CT scans have higher resolution and improved contrast, easing organ identification. In this paper, we propose a novel approach for learning to optimize task-based ultra-sound image representations. Given annotated CT segmentation maps as a simulation medium, we model acoustic propagation through tissue via ray-casting to generate ultrasound training data. Our ultrasound simulator is fully differentiable and learns to optimize the parameters for generating physics-based ultrasound images guided by the downstream segmentation task. In addition, we train an image adaptation network between real and simulated images to achieve simultaneous image synthesis and automatic segmentation on US images in an end-to-end training setting. The proposed method is evaluated on aorta and vessel segmentation tasks and shows promising quantitative results. Furthermore, we also conduct qualitative results of optimized image representations on other organs.
</details>
<details>
<summary>摘要</summary>
医学应用中对ultrasound图像的结构分割是非常重要的，特别是诊断和监测。现有的深度神经网络需要大量标注数据进行训练以达到临床可接受的性能。然而，在ultrasound中，由特有的斑点和噪声而导致的分割边界很难确定，并且医生 preciselly pixel-wise 标注图像是高度dependent于医生的专业技巧。然而，CT扫描机有更高的分辨率和更好的对比度，使得器官识别变得更容易。在这篇论文中，我们提出了一种新的方法，用于学习优化任务基于ultrasound图像的表示。我们使用了ray-casting模拟声波传播through tissue，以生成ultrasound训练数据。我们的ultrasound模拟器是完全可导的，可以学习优化参数，以便生成physics-based ultasound图像，并且被下游分割任务导引。此外，我们还训练了一种图像适应网络，以实现同时的图像合成和自动分割任务。我们的提案方法在AAA和血管分割任务中表现出了有力的量化结果。此外，我们还进行了其他器官的优化图像结果的质量评估。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/30/eess.IV_2023_07_30/" data-id="cloh7tqol012q7b889esz85im" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/29/cs.SD_2023_07_29/" class="article-date">
  <time datetime="2023-07-29T15:00:00.000Z" itemprop="datePublished">2023-07-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/29/cs.SD_2023_07_29/">cs.SD - 2023-07-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MSStyleTTS-Multi-Scale-Style-Modeling-with-Hierarchical-Context-Information-for-Expressive-Speech-Synthesis"><a href="#MSStyleTTS-Multi-Scale-Style-Modeling-with-Hierarchical-Context-Information-for-Expressive-Speech-Synthesis" class="headerlink" title="MSStyleTTS: Multi-Scale Style Modeling with Hierarchical Context Information for Expressive Speech Synthesis"></a>MSStyleTTS: Multi-Scale Style Modeling with Hierarchical Context Information for Expressive Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16012">http://arxiv.org/abs/2307.16012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shun Lei, Yixuan Zhou, Liyang Chen, Zhiyong Wu, Xixin Wu, Shiyin Kang, Helen Meng</li>
<li>for: 这个论文的目的是提出一种基于多尺度 Style 模型的 expresive speech synthesis 方法，以便在人机交互场景中更加自然和 expresive。</li>
<li>methods: 该方法使用两个子模块：一个是多尺度 Style 提取器，另一个是多尺度 Style 预测器。这两个子模块与 FastSpeech 2 基于 acoustic model 一起训练。预测器通过考虑上下文结构关系来探索层次结构上的 Context information，并预测 Style 嵌入。提取器则提取了多尺度 Style 嵌入从真实的speech中。</li>
<li>results: 论文的实验结果表明，该方法与三个基准方法进行比较，在域内和域外 audiobook 数据集上具有显著的优势。此外，文章还进行了Context information 和多尺度 Style 表示的分析，这些分析从未被讨论过。<details>
<summary>Abstract</summary>
Expressive speech synthesis is crucial for many human-computer interaction scenarios, such as audiobooks, podcasts, and voice assistants. Previous works focus on predicting the style embeddings at one single scale from the information within the current sentence. Whereas, context information in neighboring sentences and multi-scale nature of style in human speech are neglected, making it challenging to convert multi-sentence text into natural and expressive speech. In this paper, we propose MSStyleTTS, a style modeling method for expressive speech synthesis, to capture and predict styles at different levels from a wider range of context rather than a sentence. Two sub-modules, including multi-scale style extractor and multi-scale style predictor, are trained together with a FastSpeech 2 based acoustic model. The predictor is designed to explore the hierarchical context information by considering structural relationships in context and predict style embeddings at global-level, sentence-level and subword-level. The extractor extracts multi-scale style embedding from the ground-truth speech and explicitly guides the style prediction. Evaluations on both in-domain and out-of-domain audiobook datasets demonstrate that the proposed method significantly outperforms the three baselines. In addition, we conduct the analysis of the context information and multi-scale style representations that have never been discussed before.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Expressive speech synthesis is crucial for many human-computer interaction scenarios, such as audiobooks, podcasts, and voice assistants. Previous works focus on predicting the style embeddings at one single scale from the information within the current sentence. However, context information in neighboring sentences and the multi-scale nature of style in human speech are neglected, making it challenging to convert multi-sentence text into natural and expressive speech. In this paper, we propose MSStyleTTS, a style modeling method for expressive speech synthesis, to capture and predict styles at different levels from a wider range of context rather than a sentence. Two sub-modules, including a multi-scale style extractor and a multi-scale style predictor, are trained together with a FastSpeech 2 based acoustic model. The predictor is designed to explore the hierarchical context information by considering structural relationships in context and predict style embeddings at global-level, sentence-level, and subword-level. The extractor extracts multi-scale style embedding from the ground-truth speech and explicitly guides the style prediction. Evaluations on both in-domain and out-of-domain audiobook datasets demonstrate that the proposed method significantly outperforms the three baselines. In addition, we conduct the analysis of the context information and multi-scale style representations that have never been discussed before.中文简体版：人机交互场景中， expresive speech synthesis 是非常重要的，如 audiobooks、podcasts 和 voice assistants。先前的工作都是根据当前句子中的信息预测 style embedding 的，而忽略了周围句子的上下文信息和人类语音中的多级式样本，这使得将多句子文本转化为自然和 expresive speech 变得困难。在这篇论文中，我们提出了 MSStyleTTS，一种基于 FastSpeech 2 的 speech synthesis 模型，可以在更广泛的上下文中捕捉和预测多级式样本。我们的模型包括两个子模块：多级式样本抽取器和多级式样本预测器。前者从真实的语音中提取多级式样本 embedding，并直接导引预测；后者利用上下文关系来预测 style embedding 的 hierarchical 结构，包括全局水平、句子水平和字句水平。我们对具有域外和域内的 audiobook 数据集进行评估，结果显示，我们的方法至少超过了三个基线。此外，我们还进行了上下文信息和多级式样本表示的分析，这些研究方法从未被讨论过。
</details></li>
</ul>
<hr>
<h2 id="Moisesdb-A-dataset-for-source-separation-beyond-4-stems"><a href="#Moisesdb-A-dataset-for-source-separation-beyond-4-stems" class="headerlink" title="Moisesdb: A dataset for source separation beyond 4-stems"></a>Moisesdb: A dataset for source separation beyond 4-stems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15913">http://arxiv.org/abs/2307.15913</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moises-ai/moises-db">https://github.com/moises-ai/moises-db</a></li>
<li>paper_authors: Igor Pereira, Felipe Araújo, Filip Korzeniowski, Richard Vogl</li>
<li>for: 这个论文是为了介绍音乐源分离的MoisesDB数据集而写的。</li>
<li>methods: 这个论文使用了一个二级层次的分类法来组织音频源，并提供了一个使用Python编程的易于使用的库来下载、处理和使用MoisesDB数据集。</li>
<li>results: 这个论文提供了不同粒度的开源分离模型的基准结果，并分析了数据集的内容。<details>
<summary>Abstract</summary>
In this paper, we introduce the MoisesDB dataset for musical source separation. It consists of 240 tracks from 45 artists, covering twelve musical genres. For each song, we provide its individual audio sources, organized in a two-level hierarchical taxonomy of stems. This will facilitate building and evaluating fine-grained source separation systems that go beyond the limitation of using four stems (drums, bass, other, and vocals) due to lack of data. To facilitate the adoption of this dataset, we publish an easy-to-use Python library to download, process and use MoisesDB. Alongside a thorough documentation and analysis of the dataset contents, this work provides baseline results for open-source separation models for varying separation granularities (four, five, and six stems), and discuss their results.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了MoisesDB数据集，用于音乐来源分离。它包含240首歌曲，来自45位艺术家，涵盖了12种音乐类型。对每首歌曲，我们提供了它的个别音频来源，以二级层级的分类系统组织。这将促进建立和评估细化来源分离系统，超出了使用四个来源（鼓、 bass、其他和 vocals）的限制，因为缺乏数据。为便于使用这个数据集，我们在Python库中发布了一个易于使用的下载、处理和使用MoisesDB的工具。此外，我们还提供了数据集的详细文档和分析，以及不同的分离精度（四、五、六个来源）的基准结果。
</details></li>
</ul>
<hr>
<h2 id="UniBriVL-Robust-Universal-Representation-and-Generation-of-Audio-Driven-Diffusion-Models"><a href="#UniBriVL-Robust-Universal-Representation-and-Generation-of-Audio-Driven-Diffusion-Models" class="headerlink" title="UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models"></a>UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15898">http://arxiv.org/abs/2307.15898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sen Fang, Bowen Gao, Yangjian Wu, Jingwen Cai, Teik Toe Teoh</li>
<li>for: 这篇论文旨在提出一种基于 Bridging-Vision-and-Language（BriVL）的universal语言表示学习方法，以实现多modal应用程序的开发。</li>
<li>methods: 该方法使用audio、图像和文本在共享空间内嵌入，解决了多modal语言表示学习中的主要挑战，同时能够有效地捕捉audio和图像之间的相关性。</li>
<li>results: 我们的实验结果表明，UniBriVL在下游任务中具有较高的效果，并且能够从audio中生成相应的图像。此外，我们还进行了质量评估，发现UniBriVL能够生成高质量的图像。<details>
<summary>Abstract</summary>
Multimodal large models have been recognized for their advantages in various performance and downstream tasks. The development of these models is crucial towards achieving general artificial intelligence in the future. In this paper, we propose a novel universal language representation learning method called UniBriVL, which is based on Bridging-Vision-and-Language (BriVL). Universal BriVL embeds audio, image, and text into a shared space, enabling the realization of various multimodal applications. Our approach addresses major challenges in robust language (both text and audio) representation learning and effectively captures the correlation between audio and image. Additionally, we demonstrate the qualitative evaluation of the generated images from UniBriVL, which serves to highlight the potential of our approach in creating images from audio. Overall, our experimental results demonstrate the efficacy of UniBriVL in downstream tasks and its ability to choose appropriate images from audio. The proposed approach has the potential for various applications such as speech recognition, music signal processing, and captioning systems.
</details>
<details>
<summary>摘要</summary>
多Modal大型模型已被认为具有多种表现和下游任务的优势。这些模型的开发是未来通用人工智能的重要步骤。在这篇文章中，我们提出了一种新的通用语言表现学习方法，即UniBriVL，它基于桥接视觉和语言（BriVL）。这个通用BriVL嵌入音频、影像和文本到共享空间中，使得实现多modal应用的可能性。我们的方法解决了语言表现学习中的重要挑战，并具有优秀的捕捉音频和影像之间的联乘。此外，我们还进行了生成图像的质感评估，以强调我们的方法在创建图像的能力。总的来说，我们的实验结果显示UniBriVL在下游任务中的有效性，并能够从音频中选择适当的图像。这种方法的应用包括语音识别、音乐信号处理和描述系统等。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/29/cs.SD_2023_07_29/" data-id="cloh7tql000sr7b880932fs4y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/63/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/62/">62</a><a class="page-number" href="/page/63/">63</a><span class="page-number current">64</span><a class="page-number" href="/page/65/">65</a><a class="page-number" href="/page/66/">66</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/65/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
