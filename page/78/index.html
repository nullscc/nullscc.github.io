
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/78/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_07_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/11/cs.LG_2023_07_11/" class="article-date">
  <time datetime="2023-07-11T10:00:00.000Z" itemprop="datePublished">2023-07-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/11/cs.LG_2023_07_11/">cs.LG - 2023-07-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sports-Betting-an-application-of-neural-networks-and-modern-portfolio-theory-to-the-English-Premier-League"><a href="#Sports-Betting-an-application-of-neural-networks-and-modern-portfolio-theory-to-the-English-Premier-League" class="headerlink" title="Sports Betting: an application of neural networks and modern portfolio theory to the English Premier League"></a>Sports Betting: an application of neural networks and modern portfolio theory to the English Premier League</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13807">http://arxiv.org/abs/2307.13807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vélez Jiménez, Román Alberto, Lecuanda Ontiveros, José Manuel, Edgar Possani</li>
<li>for: 这篇研究旨在优化运动赌博策略，应用 Von Neumann-Morgenstern 预期价理论、深度学习技术和进阶的 Kelly 条件。</li>
<li>methods: 研究结合神经网络模型和资产估值优化，实现了20&#x2F;21英超联赛第二季后的135.8%资产增值。</li>
<li>results: 研究获得了实用的对预测足球赛果的神经网络模型，并评估了完整和限制策略的性能、风险管理和多样化。<details>
<summary>Abstract</summary>
This paper presents a novel approach for optimizing betting strategies in sports gambling by integrating Von Neumann-Morgenstern Expected Utility Theory, deep learning techniques, and advanced formulations of the Kelly Criterion. By combining neural network models with portfolio optimization, our method achieved remarkable profits of 135.8% relative to the initial wealth during the latter half of the 20/21 season of the English Premier League. We explore complete and restricted strategies, evaluating their performance, risk management, and diversification. A deep neural network model is developed to forecast match outcomes, addressing challenges such as limited variables. Our research provides valuable insights and practical applications in the field of sports betting and predictive modeling.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Von Neumann-Morgenstern Expected Utility Theory" is translated as "尼采曼-摩根斯特恩预期风险理论" (Niàimèi-Mògēngshìyì Expected Utility Theory)* "deep learning techniques" is translated as "深度学习技术" (shēngrán xuéxí jīshù)* "advanced formulations of the Kelly Criterion" is translated as "凯利 критери亮的高级形式" (Kēlǐ Críterion de gāojí xíngshì)* "English Premier League" is translated as "英超联赛" (Yīngcháo Liánshì)* "match outcomes" is translated as "赛事结果" (sài shì jiéguǒ)
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Nested-Compositional-Bi-level-Optimization-for-Robust-Feature-Learning"><a href="#Stochastic-Nested-Compositional-Bi-level-Optimization-for-Robust-Feature-Learning" class="headerlink" title="Stochastic Nested Compositional Bi-level Optimization for Robust Feature Learning"></a>Stochastic Nested Compositional Bi-level Optimization for Robust Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05384">http://arxiv.org/abs/2307.05384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuxing Chen, Krishnakumar Balasubramanian, Saeed Ghadimi</li>
<li>for: 解决嵌套compose函数的二级优化问题</li>
<li>methods: 使用stochev approximation algorithm，不需要matrix inversion或mini-batch</li>
<li>results: 可以达到$\epsilon$-站点解的解决方案，复杂度约为$\tilde{O}_T(1&#x2F;\epsilon^2)$<details>
<summary>Abstract</summary>
We develop and analyze stochastic approximation algorithms for solving nested compositional bi-level optimization problems. These problems involve a nested composition of $T$ potentially non-convex smooth functions in the upper-level, and a smooth and strongly convex function in the lower-level. Our proposed algorithm does not rely on matrix inversions or mini-batches and can achieve an $\epsilon$-stationary solution with an oracle complexity of approximately $\tilde{O}_T(1/\epsilon^{2})$, assuming the availability of stochastic first-order oracles for the individual functions in the composition and the lower-level, which are unbiased and have bounded moments. Here, $\tilde{O}_T$ hides polylog factors and constants that depend on $T$. The key challenge we address in establishing this result relates to handling three distinct sources of bias in the stochastic gradients. The first source arises from the compositional nature of the upper-level, the second stems from the bi-level structure, and the third emerges due to the utilization of Neumann series approximations to avoid matrix inversion. To demonstrate the effectiveness of our approach, we apply it to the problem of robust feature learning for deep neural networks under covariate shift, showcasing the benefits and advantages of our methodology in that context.
</details>
<details>
<summary>摘要</summary>
我们开发和分析Stochastic Approximation Algorithm，用于解决嵌套组合的 би-级优化问题。这些问题包含 $T$ 个可能不准确的凸函数的嵌套作用在上级，以及一个准确的凸函数在下级。我们的提议的算法不需要矩阵反转或 mini-batch，可以在 $\epsilon$ 静态环境下获得 $\tilde{O}_T(1/\epsilon^{2})$ 精度，假设有随机首ORDER oracle 对各个函数的拟合和下级，这些 oracle 是无偏见的和有 bounded moments。在这里， $\tilde{O}_T$ 隐藏了 polylog 因子和常数，它们取决于 $T$。我们在实现这个结果时面临的主要挑战是处理 Stochastic Gradient 中的三种偏差。第一种来自于嵌套结构，第二种来自于 bi-level 结构，第三种来自于使用 Neumann 系列近似来避免矩阵反转。为了证明我们的方法的效果，我们将它应用于深度神经网络中的 Robust Feature Learning 问题，并通过这个例子展示了我们的方法的优势和优点。
</details></li>
</ul>
<hr>
<h2 id="Combating-Data-Imbalances-in-Federated-Semi-supervised-Learning-with-Dual-Regulators"><a href="#Combating-Data-Imbalances-in-Federated-Semi-supervised-Learning-with-Dual-Regulators" class="headerlink" title="Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators"></a>Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05358">http://arxiv.org/abs/2307.05358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sikai Bai, Shuaicheng Li, Weiming Zhuang, Jie Zhang, Song Guo, Kunlin Yang, Jun Hou, Shuai Zhang, Junyu Gao, Shuai Yi</li>
<li>for: 这个论文targets federated semi-supervised learning (FSSL) in decentralized heterogeneous data, with a focus on addressing the challenge of non-identical data distribution within and across clients.</li>
<li>methods: 该论文提出了一种新的FSSL框架，called FedDure，which uses dual regulators (C-reg和F-reg) to address the assumption of independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client.</li>
<li>results: 该论文的实验表明，FedDure比现有方法在各种场景中表现出色，尤其是在CIFAR-10和CINIC-10 datasets上，提高了更多于11%的性能。<details>
<summary>Abstract</summary>
Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.} FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored for unlabeled instances in each client. We further formulate the client model training as bi-level optimization that adaptively optimizes the model in the client with two regulators. Theoretically, we show the convergence guarantee of the dual regulators. Empirically, we demonstrate that FedDure is superior to the existing methods across a wide range of settings, notably by more than 11% on CIFAR-10 and CINIC-10 datasets.
</details>
<details>
<summary>摘要</summary>
federated 学习已经成为了learn from 分布式不同数据的popular方法。 federated 半supervised 学习（FSSL）emerges 用于从小量标签数据中训练模型，由于分布式客户端上标签数据的稀缺。 existing FSSL 方法假设客户端上的标签数据是独立同分布的（IID），并且在客户端内部标签和未标签数据的分布相同。 this work studies 一种更实际和挑战的scenario of FSSL，where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure。FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored for unlabeled instances in each client. We further formulate the client model training as bi-level optimization that adaptively optimizes the model in the client with two regulators. Theoretically, we show the convergence guarantee of the dual regulators. Empirically, we demonstrate that FedDure is superior to the existing methods across a wide range of settings, notably by more than 11% on CIFAR-10 and CINIC-10 datasets.
</details></li>
</ul>
<hr>
<h2 id="Tracking-Most-Significant-Shifts-in-Nonparametric-Contextual-Bandits"><a href="#Tracking-Most-Significant-Shifts-in-Nonparametric-Contextual-Bandits" class="headerlink" title="Tracking Most Significant Shifts in Nonparametric Contextual Bandits"></a>Tracking Most Significant Shifts in Nonparametric Contextual Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05341">http://arxiv.org/abs/2307.05341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joe Suk, Samory Kpotufe</li>
<li>For: 这篇论文主要研究非参数contextual bandit，其中 lipschitz均值函数可能随时间变化。* Methods: 作者首先确定了这种情况下的最佳动态快悔率率，并证明现有方法在这种设定下是不优秀的。然后，他们提出了一种新的定义，即“经验性变化”，以更好地考虑本地性。* Results: 作者的主要结果是证明这种更宽容的定义可以实际应用于 adaptive 算法。<details>
<summary>Abstract</summary>
We study nonparametric contextual bandits where Lipschitz mean reward functions may change over time. We first establish the minimax dynamic regret rate in this less understood setting in terms of number of changes $L$ and total-variation $V$, both capturing all changes in distribution over context space, and argue that state-of-the-art procedures are suboptimal in this setting.   Next, we tend to the question of an adaptivity for this setting, i.e. achieving the minimax rate without knowledge of $L$ or $V$. Quite importantly, we posit that the bandit problem, viewed locally at a given context $X_t$, should not be affected by reward changes in other parts of context space $\cal X$. We therefore propose a notion of change, which we term experienced significant shifts, that better accounts for locality, and thus counts considerably less changes than $L$ and $V$. Furthermore, similar to recent work on non-stationary MAB (Suk & Kpotufe, 2022), experienced significant shifts only count the most significant changes in mean rewards, e.g., severe best-arm changes relevant to observed contexts.   Our main result is to show that this more tolerant notion of change can in fact be adapted to.
</details>
<details>
<summary>摘要</summary>
我们研究非参数化上下文带狗，其中 lipschitz 平均奖励函数可能随时间变化。我们首先将最佳动态对抗 regret 率定义为函数数量 $L$ 和总变化 $V$，这两个量都捕捉了上下文空间中的所有变化，并证明了现有的方法在这个设定下是不佳的。接下来，我们对这个设定进行了适应，即在不知道 $L$ 或 $V$ 的情况下 achieving 最佳动态对抗 regret 率。我们认为在上下文空间中的带狗问题，应该忽略其他部分上下文空间中的奖励变化。因此，我们提出了一个称为“体验了大幅度变化”的概念，这个概念更好地考虑了地方性，因此只计算了上下文空间中的一部分变化。此外，我们还与最近的非站点带狗研究（Suk & Kpotufe, 2022）相似，只计算了最重要的奖励变化，例如在观察到的上下文中的严重最好臂变化。我们的主要结果是证明这个更宽容的变化定义可以实际地适应。
</details></li>
</ul>
<hr>
<h2 id="Predicting-small-molecules-solubilities-on-endpoint-devices-using-deep-ensemble-neural-networks"><a href="#Predicting-small-molecules-solubilities-on-endpoint-devices-using-deep-ensemble-neural-networks" class="headerlink" title="Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks"></a>Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05318">http://arxiv.org/abs/2307.05318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ur-whitelab/mol.dev">https://github.com/ur-whitelab/mol.dev</a></li>
<li>paper_authors: Mayk Caldas Ramos, Andrew D. White</li>
<li>for: 这个研究旨在提高溶解性预测的精度和计算效率，同时解决使用群体基于的方法的使用问题。</li>
<li>methods: 该研究使用深度学习模型来预测溶解性，并通过提供Predictive uncertainty来衡量模型的不确定性。</li>
<li>results: 研究结果表明，该模型可以得到满意的溶解性预测结果，并且可以帮助创建溶解性预测模型，既能够考虑不确定性，又能够使用者友好。<details>
<summary>Abstract</summary>
Aqueous solubility is a valuable yet challenging property to predict. Computing solubility using first-principles methods requires accounting for the competing effects of entropy and enthalpy, resulting in long computations for relatively poor accuracy. Data-driven approaches, such as deep learning, offer improved accuracy and computational efficiency but typically lack uncertainty quantification. Additionally, ease of use remains a concern for any computational technique, resulting in the sustained popularity of group-based contribution methods. In this work, we addressed these problems with a deep learning model with predictive uncertainty that runs on a static website (without a server). This approach moves computing needs onto the website visitor without requiring installation, removing the need to pay for and maintain servers. Our model achieves satisfactory results in solubility prediction. Furthermore, we demonstrate how to create molecular property prediction models that balance uncertainty and ease of use. The code is available at \url{https://github.com/ur-whitelab/mol.dev}, and the model is usable at \url{https://mol.dev}.
</details>
<details>
<summary>摘要</summary>
溶解性是一个有价值又具有挑战性的性质，计算溶解性使用基于原理方法需要考虑竞争的 entropy 和 enthalpy 效应，导致计算效率低下，准确性也不高。数据驱动方法，如深度学习，可以提高准确性和计算效率，但通常缺乏 uncertainty 评估。此外，使用容易性也是一个关键问题，导致群组基于的贡献方法仍然具有广泛的应用。在这种情况下，我们采用了一种深度学习模型，具有预测 uncertainty，运行在静态网站（无需服务器）上。这种方法将计算需求卷积到网站访问者身上，不需要安装和维护服务器。我们的模型可以达到溶解性预测的满意结果。此外，我们还示出了如何创建分子性质预测模型，既具有 uncertainty 也具有容易使用性。代码可以在 \url{https://github.com/ur-whitelab/mol.dev} 上获取，模型可以在 \url{https://mol.dev} 上使用。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Symbolic-Laws-Directly-from-Trajectories-with-Hamiltonian-Graph-Neural-Networks"><a href="#Discovering-Symbolic-Laws-Directly-from-Trajectories-with-Hamiltonian-Graph-Neural-Networks" class="headerlink" title="Discovering Symbolic Laws Directly from Trajectories with Hamiltonian Graph Neural Networks"></a>Discovering Symbolic Laws Directly from Trajectories with Hamiltonian Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05299">http://arxiv.org/abs/2307.05299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suresh Bishnoi, Ravinder Bhattoo, Jayadeva, Sayan Ranu, N M Anoop Krishnan</li>
<li>for: 本研究旨在通过数学方法发现自然系统的互动规律。</li>
<li>methods: 本研究使用了加权图 neural network (HGNN) 来学习物理系统的动态。HGNN 是一种符合物理规则的神经网络，可以直接从系统的轨迹中学习动态。</li>
<li>results: 研究发现，HGNN 可以很好地适应不同的物理系统，并且可以从小量数据中学习出高度一致的动态。此外，HGNN 还可以扩展到更大的系统大小和混合系统中，并且可以通过符号 regresion 推导出互动方程。<details>
<summary>Abstract</summary>
The time evolution of physical systems is described by differential equations, which depend on abstract quantities like energy and force. Traditionally, these quantities are derived as functionals based on observables such as positions and velocities. Discovering these governing symbolic laws is the key to comprehending the interactions in nature. Here, we present a Hamiltonian graph neural network (HGNN), a physics-enforced GNN that learns the dynamics of systems directly from their trajectory. We demonstrate the performance of HGNN on n-springs, n-pendulums, gravitational systems, and binary Lennard Jones systems; HGNN learns the dynamics in excellent agreement with the ground truth from small amounts of data. We also evaluate the ability of HGNN to generalize to larger system sizes, and to hybrid spring-pendulum system that is a combination of two original systems (spring and pendulum) on which the models are trained independently. Finally, employing symbolic regression on the learned HGNN, we infer the underlying equations relating the energy functionals, even for complex systems such as the binary Lennard-Jones liquid. Our framework facilitates the interpretable discovery of interaction laws directly from physical system trajectories. Furthermore, this approach can be extended to other systems with topology-dependent dynamics, such as cells, polydisperse gels, or deformable bodies.
</details>
<details>
<summary>摘要</summary>
Physical systems' 时间演化是用导函数方程表示的，这些方程取决于抽象量如能量和力。传统上，这些量是基于观察量如位置和速度来 derivation 的。找出这些指导符号法则是理解自然系统的交互的键。我们现在提出了一种哈密顿图 neural network（HGNN），这是一种符合物理规则的图 neural network，可以直接从系统轨迹中学习系统的动力学。我们在 n-spring、n-pendulum、重力系统和二元 Lenard-Jones 系统上测试了 HGNN，它在小数据量下与真实值一致地学习了系统的动力学。我们还评估了 HGNN 的扩展性和可重复性，以及将两个独立训练的系统（春和振荡）结合在一起的 hybrid 春振荡系统的性能。最后，我们使用符号回归来推导出在 energy 函数上的下文依赖的方程，包括复杂的二元 Lenard-Jones 液体系统。我们的框架可以直接从物理系统轨迹中可读地找到交互的法则，并且可以扩展到其他具有体系依赖动力学的系统，如细胞、多种 gel 或可变形体。
</details></li>
</ul>
<hr>
<h2 id="On-the-Need-for-a-Language-Describing-Distribution-Shifts-Illustrations-on-Tabular-Datasets"><a href="#On-the-Need-for-a-Language-Describing-Distribution-Shifts-Illustrations-on-Tabular-Datasets" class="headerlink" title="On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets"></a>On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05284">http://arxiv.org/abs/2307.05284</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/namkoong-lab/whyshift">https://github.com/namkoong-lab/whyshift</a></li>
<li>paper_authors: Jiashuo Liu, Tianyu Wang, Peng Cui, Hongseok Namkoong</li>
<li>for: This paper aims to investigate natural shifts in tabular datasets and the impact of these shifts on algorithmic performance.</li>
<li>methods: The authors use a thorough investigation of 5 tabular datasets and 86,000 model configurations to identify the most prevalent types of distribution shifts, specifically $Y|X$-shifts. They also build an empirical testbed called WhyShift to characterize and benchmark performance over different types of shifts.</li>
<li>results: The authors find that $Y|X$-shifts are the most prevalent type of shift in tabular settings, and they identify covariate regions that suffer the biggest $Y|X$-shifts. They discuss the implications of these shifts for algorithmic and data-based interventions.<details>
<summary>Abstract</summary>
Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithmic and data-based interventions. Our testbed highlights the importance of future research that builds an understanding of how distributions differ.
</details>
<details>
<summary>摘要</summary>
不同的分布偏移需要不同的算法和操作干预。方法研究必须基于具体的偏移来定制。虽然初始的标准模型提供了一个有前途的基础，但它们默认地关注 covariate 偏移，并且研究成果的有效性取决于偏移的类型，例如，之前对算法性能的评估可能无法在 $Y|X$ 分布变化时保持有效。我们对 5 个表格数据集进行了全面的调查，并发现了 $Y|X$-偏移是最普遍的。为促进研究人员开发更加细化的分布偏移语言，我们建立了 WhyShift，一个基于实际情况的偏移测试床，我们在这里characterize了我们测试性能的类型的偏移。由于 tabular 设置中 $Y|X$-偏移最普遍，我们将covariate 区域分析出最大的 $Y|X$-偏移，并讨论了对算法和数据基于干预的影响。我们的测试床表明未来研究应该建立一个对分布的不同而建立的理解，以便更好地适应具体的应用场景。
</details></li>
</ul>
<hr>
<h2 id="CareFall-Automatic-Fall-Detection-through-Wearable-Devices-and-AI-Methods"><a href="#CareFall-Automatic-Fall-Detection-through-Wearable-Devices-and-AI-Methods" class="headerlink" title="CareFall: Automatic Fall Detection through Wearable Devices and AI Methods"></a>CareFall: Automatic Fall Detection through Wearable Devices and AI Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05275">http://arxiv.org/abs/2307.05275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Carlos Ruiz-Garcia, Ruben Tolosana, Ruben Vera-Rodriguez, Carlos Moro</li>
<li>for: 这篇研究旨在开发一个自动检测跌倒的系统，以减轻老年人跌倒所带来的负面影响。</li>
<li>methods: 这篇研究使用了智能手表上的加速度和陀螺仪时间信号，并使用人工智能方法进行特征提取和类别。</li>
<li>results: 实验结果显示，使用机器学习方法结合加速度和陀螺仪信息的方法，在准确性、敏感度和特异度方面都高于阈值基本方法。<details>
<summary>Abstract</summary>
The aging population has led to a growing number of falls in our society, affecting global public health worldwide. This paper presents CareFall, an automatic Fall Detection System (FDS) based on wearable devices and Artificial Intelligence (AI) methods. CareFall considers the accelerometer and gyroscope time signals extracted from a smartwatch. Two different approaches are used for feature extraction and classification: i) threshold-based, and ii) machine learning-based. Experimental results on two public databases show that the machine learning-based approach, which combines accelerometer and gyroscope information, outperforms the threshold-based approach in terms of accuracy, sensitivity, and specificity. This research contributes to the design of smart and user-friendly solutions to mitigate the negative consequences of falls among older people.
</details>
<details>
<summary>摘要</summary>
随着人口老龄化，社会中的滥落事件数量在全球范围内呈增加趋势。本文介绍了一种基于智能手表和人工智能方法的自动滥落检测系统（FDS），称为CareFall。CareFall利用智能手表上的加速度和自转器时间信号进行特征提取和分类，并使用两种不同的方法：一种是基于阈值的方法，另一种是基于机器学习的方法。在两个公共数据库上进行了实验，结果显示，基于加速度和自转器信息的机器学习方法的检测精度、敏感度和特征鲁平性比基于阈值的方法高。这项研究增加了设计智能和易用的解决方案，以降低老年人滥落的负面影响。
</details></li>
</ul>
<hr>
<h2 id="U-CREAT-Unsupervised-Case-Retrieval-using-Events-extrAcTion"><a href="#U-CREAT-Unsupervised-Case-Retrieval-using-Events-extrAcTion" class="headerlink" title="U-CREAT: Unsupervised Case Retrieval using Events extrAcTion"></a>U-CREAT: Unsupervised Case Retrieval using Events extrAcTion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05260">http://arxiv.org/abs/2307.05260</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploration-lab/il-pcr">https://github.com/exploration-lab/il-pcr</a></li>
<li>paper_authors: Abhinav Joshi, Akshat Sharma, Sai Kiran Tanikella, Ashutosh Modi</li>
<li>For: The paper is written for the task of Prior Case Retrieval (PCR) in the legal domain, specifically proposing a new large benchmark (IL-PCR corpus) and exploring the role of events in legal case retrieval.* Methods: The paper proposes an unsupervised retrieval method-based pipeline called U-CREAT (Unsupervised Case Retrieval using Events Extraction), which significantly increases performance compared to BM25 and makes retrieval faster by a considerable margin.* Results: The proposed system is generic and shows state-of-the-art performance on the benchmarks for both the Indian and Canadian legal systems (IL-PCR and COLIEE corpora).Here are the three points in Simplified Chinese text:* For: 这篇论文是为了legal domain中的 Prior Case Retrieval (PCR)任务而写的，具体来说是提出一个新的大型benchmark（IL-PCR corpus），并探讨法律案例之间的事件的角色。* Methods: 这篇论文提出了一个无监督的检索方法基于管道called U-CREAT (Unsupervised Case Retrieval using Events Extraction)，它与BM25相比显著提高了性能，并且使检索速度减少了较大的margin，因此适用于实时案例检索系统。* Results: 提posed系统是通用的，并在两个不同的法律系统（印度和加拿大）的benchmark上达到了状态计算机表现。<details>
<summary>Abstract</summary>
The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance and the long size of legal documents, BM25 remains a strong baseline for ranking the cited prior documents. In this work, we explore the role of events in legal case retrieval and propose an unsupervised retrieval method-based pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find that the proposed unsupervised retrieval method significantly increases performance compared to BM25 and makes retrieval faster by a considerable margin, making it applicable to real-time case retrieval systems. Our proposed system is generic, we show that it generalizes across two different legal systems (Indian and Canadian), and it shows state-of-the-art performance on the benchmarks for both the legal systems (IL-PCR and COLIEE corpora).
</details>
<details>
<summary>摘要</summary>
PCR任务在法律领域是自动提供相关的前例案例，以便在查询案例中提供有用的信息。为了进一步推动PCR研究，在这篇论文中，我们提出了一个新的大量benchmark（英文） дляPCR任务：IL-PCR（印度法律前例案例采集） corpora。由于案例相关性的复杂性和法律文档的长度，BM25仍然是PCR任务中的强基线。在这种工作中，我们探讨了法律案例中事件的角色，并提出了一种无监督检索方法 pipeline U-CREAT（无监督案例检索使用事件提取）。我们发现，我们提出的无监督检索方法可以在BM25和事件提取方法之间提高性能，并且可以在实时案例检索系统中提高检索速度，使其成为实时案例检索系统中的可靠选择。我们的提出的系统是通用的，我们表明它可以在印度和加拿大两个不同的法律系统中实现状态的表现，并且在IL-PCR和COLIEE corpora上达到了状态的性能。
</details></li>
</ul>
<hr>
<h2 id="MAP-and-MLE-Based-Teaching"><a href="#MAP-and-MLE-Based-Teaching" class="headerlink" title="MAP- and MLE-Based Teaching"></a>MAP- and MLE-Based Teaching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05252">http://arxiv.org/abs/2307.05252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hans Ulrich Simon, Jan Arne Telle</li>
<li>for: 这个论文主要研究的是学习概率的概念推理问题，具体来说是learner L从一个观察集Z中INFER一个隐藏的概念。</li>
<li>methods: 该论文基于 Ferri et al.的工作，假设learner L被参数化为 prior P(c)和c-conditioned likelihoods P(z|c)，其中c是一个给定的概念集C中的一个概念，z是一个观察集Z中的一个观察。learner L被称为MAP-learner（resp. MLE-learner），如果它将观察集S视为一个随机抽样，并返回最大a posteriori probabilities（resp.最大c-conditional likelihood）。</li>
<li>results: 该论文的主要结果是，这种教学模型具有一些愉悦的 monotonicity 性质，并且可以通过不同的抽样方式来关联ogether。在特定的情况下（即概念是集合，观察是0,1-标记的示例）， authors 还得到了一些额外的结果，例如，MAP-和MLE-教学维度可以被图 theoretically characterize，并且可以通过VC-dimension和其他 combinatorial parameters来Upper bound。<details>
<summary>Abstract</summary>
Imagine a learner L who tries to infer a hidden concept from a collection of observations. Building on the work [4] of Ferri et al., we assume the learner to be parameterized by priors P(c) and by c-conditional likelihoods P(z|c) where c ranges over all concepts in a given class C and z ranges over all observations in an observation set Z. L is called a MAP-learner (resp. an MLE-learner) if it thinks of a collection S of observations as a random sample and returns the concept with the maximum a-posteriori probability (resp. the concept which maximizes the c-conditional likelihood of S). Depending on whether L assumes that S is obtained from ordered or unordered sampling resp. from sampling with or without replacement, we can distinguish four different sampling modes. Given a target concept c in C, a teacher for a MAP-learner L aims at finding a smallest collection of observations that causes L to return c. This approach leads in a natural manner to various notions of a MAP- or MLE-teaching dimension of a concept class C. Our main results are: We show that this teaching model has some desirable monotonicity properties. We clarify how the four sampling modes are related to each other. As for the (important!) special case, where concepts are subsets of a domain and observations are 0,1-labeled examples, we obtain some additional results. First of all, we characterize the MAP- and MLE-teaching dimension associated with an optimally parameterized MAP-learner graph-theoretically. From this central result, some other ones are easy to derive. It is shown, for instance, that the MLE-teaching dimension is either equal to the MAP-teaching dimension or exceeds the latter by 1. It is shown furthermore that these dimensions can be bounded from above by the so-called antichain number, the VC-dimension and related combinatorial parameters. Moreover they can be computed in polynomial time.
</details>
<details>
<summary>摘要</summary>
学生L将尝试对一个集合观察做推理，建立在[4] Ferri等人的工作之上。我们假设学生L是受到先前知识P(c)和c- conditional likelihoods P(z|c)的参数化学生，其中c是所有概念集C中的一个概念，z是所有观察集Z中的一个观察。L被称为MAP-学生（resp. MLE-学生），如果它视观察集S为一个随机抽样，并返回概念中的最大a posteriori probabilities（resp. 最大c-conditional likelihood）。对于L是否假设S是顺序或无顺序抽样，或者是否从抽样中删除或不删除某些观察，我们可以区别出四种抽样模式。对于目标概念c在C中，教师 дляMAP-学生L的目标是找到一个最小的观察集，使L返回c。这个模型具有一些愉悦的弹性性质。我们还证明了这些抽样模式之间的相关性。对于特别的情况，其中概念是域中的子集和观察是0,1-标注的例子，我们获得了一些额外的结果。例如，我们可以Characterize MAP-和MLE-教育dimension associated with an optimally parameterized MAP-learner graph-theoretically。从这个中央结果，一些其他的结果是容易 derivable。例如，我们可以证明MAP-和MLE-教育dimension是等于或高于对方，并且可以通过 antichain number、VC-dimension和相关的 combinatorial parameters bound from above。此外，这些教育dimension可以在 polynomial time 内计算。
</details></li>
</ul>
<hr>
<h2 id="DRMC-A-Generalist-Model-with-Dynamic-Routing-for-Multi-Center-PET-Image-Synthesis"><a href="#DRMC-A-Generalist-Model-with-Dynamic-Routing-for-Multi-Center-PET-Image-Synthesis" class="headerlink" title="DRMC: A Generalist Model with Dynamic Routing for Multi-Center PET Image Synthesis"></a>DRMC: A Generalist Model with Dynamic Routing for Multi-Center PET Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05249">http://arxiv.org/abs/2307.05249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwen Yang, Yang Zhou, Hui Zhang, Bingzheng Wei, Yubo Fan, Yan Xu</li>
<li>For: 多中心 positron射tomography（PET）图像合成问题的目的是 recuperate low-dose PET图像从多个不同中心。* Methods: 我们开发了一种通用模型，该模型在不同中心共享结构和参数，以便利用多中心之间的共同知识。但是，这种通用模型可能会受到中心间干扰问题的影响，即不同中心的梯度方向可能不一致或 même 相反。为 Mitigate such interference, we introduce a novel dynamic routing strategy with cross-layer connections that routes data from different centers to different experts.* Results: 我们的通用模型与动态路由（DRMC）在多中心之间表现出了优秀的通用性。<details>
<summary>Abstract</summary>
Multi-center positron emission tomography (PET) image synthesis aims at recovering low-dose PET images from multiple different centers. The generalizability of existing methods can still be suboptimal for a multi-center study due to domain shifts, which result from non-identical data distribution among centers with different imaging systems/protocols. While some approaches address domain shifts by training specialized models for each center, they are parameter inefficient and do not well exploit the shared knowledge across centers. To address this, we develop a generalist model that shares architecture and parameters across centers to utilize the shared knowledge. However, the generalist model can suffer from the center interference issue, \textit{i.e.} the gradient directions of different centers can be inconsistent or even opposite owing to the non-identical data distribution. To mitigate such interference, we introduce a novel dynamic routing strategy with cross-layer connections that routes data from different centers to different experts. Experiments show that our generalist model with dynamic routing (DRMC) exhibits excellent generalizability across centers. Code and data are available at: https://github.com/Yaziwel/Multi-Center-PET-Image-Synthesis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-From-Distributed-Machine-Learning-to-Distributed-Deep-Learning"><a href="#A-Survey-From-Distributed-Machine-Learning-to-Distributed-Deep-Learning" class="headerlink" title="A Survey From Distributed Machine Learning to Distributed Deep Learning"></a>A Survey From Distributed Machine Learning to Distributed Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05232">http://arxiv.org/abs/2307.05232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Mohammad Dehghani, Zahra Yazdanparast</li>
<li>for: 本文总结了当前分布式机器学习领域的最新进展，包括分类和聚类（传统机器学习）、深度学习和深度强化学习等方法。</li>
<li>methods: 本文对分布式机器学习算法进行了详细的概述，并将其分为分类和聚类（传统机器学习）、深度学习和深度强化学习等类别。</li>
<li>results: 本文对各种分布式机器学习算法进行了评估，并将其分为深度学习和传统机器学习两类。深度学习在分布式机器学习中占据了主导地位，大多数研究都集中在这一方面。<details>
<summary>Abstract</summary>
Artificial intelligence has achieved significant success in handling complex tasks in recent years. This success is due to advances in machine learning algorithms and hardware acceleration. In order to obtain more accurate results and solve more complex problems, algorithms must be trained with more data. This huge amount of data could be time-consuming to process and require a great deal of computation. This solution could be achieved by distributing the data and algorithm across several machines, which is known as distributed machine learning. There has been considerable effort put into distributed machine learning algorithms, and different methods have been proposed so far. In this article, we present a comprehensive summary of the current state-of-the-art in the field through the review of these algorithms. We divide this algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has gained more attention in recent years and most of studies worked on this algorithms. As a result, most of the articles we discussed here belong to this category. Based on our investigation of algorithms, we highlight limitations that should be addressed in future research.
</details>
<details>
<summary>摘要</summary>
人工智能在最近几年内已经取得了重要的成功，这种成功主要归功于机器学习算法和硬件加速。为了获得更高准确率和解决更复杂的问题，算法需要更多的数据进行训练。这些庞大数据可能需要很长时间来处理，并需要巨量的计算资源。为了解决这个问题，人们提出了分布式机器学习的想法。在这篇文章中，我们对当前领域的状况做了全面的概述，包括分类和聚类（传统机器学习）、深度学习和深度强化学习等方法。分布式深度学习在最近几年内得到了更多的关注， więc大多数研究都是在这个领域进行的。根据我们对算法的调查，我们指出了未来研究中应该解决的一些限制。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Controlled-Dialogue-Prompting"><a href="#Attribute-Controlled-Dialogue-Prompting" class="headerlink" title="Attribute Controlled Dialogue Prompting"></a>Attribute Controlled Dialogue Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05228">http://arxiv.org/abs/2307.05228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runcheng Liu, Ahmad Rashid, Ivan Kobyzev, Mehdi Rezagholizadeh, Pascal Poupart</li>
<li>for: 这篇研究是为了提出一种新的、具体化的问题训练算法，用于控制对话生成。</li>
<li>methods: 这篇研究使用了基于实例级控制代码的问题训练算法，而不是基于对话历史。</li>
<li>results: 实验结果显示，该方法与问题训练基线比较，并且与只使用5%-6%的总参数进行精致训练相当。<details>
<summary>Abstract</summary>
Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.
</details>
<details>
<summary>摘要</summary>
启发调整已成为大型预训练语言模型适应下游任务的常用方法，但是两种精确提示和连续提示都假设每个数据样本内任务中的提示是固定的，而忽视了一些任务，如开放领域对话生成，输入可能很大的变化。在这篇论文中，我们提出了一种新的实例特定的提示调整算法，具体来说，我们基于实例级控制代码而不是对话历史来生成提示，以探索它们对控制对话生成的影响。我们在流行的开放领域对话Dataset上进行了实验，并通过自动评价指标和人工评价来评价我们的方法。结果表明，我们的方法在比基准提示和只使用5%-6%的总参数进行精度训练时具有优异表现。
</details></li>
</ul>
<hr>
<h2 id="Supervised-Attention-Using-Homophily-in-Graph-Neural-Networks"><a href="#Supervised-Attention-Using-Homophily-in-Graph-Neural-Networks" class="headerlink" title="Supervised Attention Using Homophily in Graph Neural Networks"></a>Supervised Attention Using Homophily in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05217">http://arxiv.org/abs/2307.05217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michail Chatzianastasis, Giannis Nikolentzos, Michalis Vazirgiannis</li>
<li>for: 本研究旨在提高图像学习中Graph Attention Networks（GATs）的性能，以提高图像分类和图像推荐等任务的效果。</li>
<li>methods: 我们提出了一种新的技术，可以与任何图像学习模型结合使用，以增强GATs中节点之间的关注分布。该技术可以提高节点之间的关注分布，从而提高图像分类和图像推荐等任务的效果。</li>
<li>results: 我们在多个节点分类数据集上进行了评估，并证明了我们的方法可以提高GATs的性能，并且在某些任务上超过标准基eline模型。<details>
<summary>Abstract</summary>
Graph neural networks have become the standard approach for dealing with learning problems on graphs. Among the different variants of graph neural networks, graph attention networks (GATs) have been applied with great success to different tasks. In the GAT model, each node assigns an importance score to its neighbors using an attention mechanism. However, similar to other graph neural networks, GATs aggregate messages from nodes that belong to different classes, and therefore produce node representations that are not well separated with respect to the different classes, which might hurt their performance. In this work, to alleviate this problem, we propose a new technique that can be incorporated into any graph attention model to encourage higher attention scores between nodes that share the same class label. We evaluate the proposed method on several node classification datasets demonstrating increased performance over standard baseline models.
</details>
<details>
<summary>摘要</summary>
格raph神经网络已经成为处理图structured learning问题的标准方法。其中，graph attention网络（GATs）在不同任务上得到了成功应用。在GAT模型中，每个节点通过注意机制对其邻居进行重要性分配。然而，与其他图神经网络一样，GATs将来自不同类别的节点的消息聚合，因此生成的节点表示可能并不是根据不同类别而分离得到的，这可能会影响其性能。在这种情况下，我们提出了一种可以在任何图注意模型中应用的新技术，以促进同类别节点之间的高度注意分数。我们在多个节点分类数据集上评估了该方法，并证明了它与标准基准模型相比表现更好。
</details></li>
</ul>
<hr>
<h2 id="Score-Function-Gradient-Estimation-to-Widen-the-Applicability-of-Decision-Focused-Learning"><a href="#Score-Function-Gradient-Estimation-to-Widen-the-Applicability-of-Decision-Focused-Learning" class="headerlink" title="Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning"></a>Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05213">http://arxiv.org/abs/2307.05213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mattia Silvestri, Senne Berden, Jayanta Mandi, Ali İrfan Mahmutoğulları, Maxime Mulamba, Allegra De Filippo, Tias Guns, Michele Lombardi<br>for: 这个论文主要用于提出一种基于决策学习的优化方法，用于解决具有未知参数的实际优化问题。methods: 这种方法基于直接最小化下游任务损失来训练机器学习模型，而不是直接最大化预测精度。它采用分布预测和分数函数梯度估计（SFGE）来计算决策学习更新，以扩展决策学习的应用范围。results: 经过实验，这种方法可以：（1）处理在目标函数和约束中都有预测的情况；（2）有效地解决两个阶段随机优化问题。<details>
<summary>Abstract</summary>
Many real-world optimization problems contain unknown parameters that must be predicted prior to solving. To train the predictive machine learning (ML) models involved, the commonly adopted approach focuses on maximizing predictive accuracy. However, this approach does not always lead to the minimization of the downstream task loss. Decision-focused learning (DFL) is a recently proposed paradigm whose goal is to train the ML model by directly minimizing the task loss. However, state-of-the-art DFL methods are limited by the assumptions they make about the structure of the optimization problem (e.g., that the problem is linear) and by the fact that can only predict parameters that appear in the objective function. In this work, we address these limitations by instead predicting \textit{distributions} over parameters and adopting score function gradient estimation (SFGE) to compute decision-focused updates to the predictive model, thereby widening the applicability of DFL. Our experiments show that by using SFGE we can: (1) deal with predictions that occur both in the objective function and in the constraints; and (2) effectively tackle two-stage stochastic optimization problems.
</details>
<details>
<summary>摘要</summary>
许多实际优化问题中含有未知参数，需要预测才能解决。现有的方法通常是通过提高预测精度来训练预测机器学习（ML）模型。然而，这种方法不一定能够最小化下游任务损失。决策驱动学习（DFL）是一种最近提出的方法，其目标是通过直接最小化任务损失来训练 ML 模型。然而，现有的 DFL 方法受到问题结构假设（例如，问题是线性的）和仅能预测出现在目标函数中的参数的限制。在这项工作中，我们解决这些限制，而不是直接预测参数，而是预测参数的分布，并采用分数函数梯度估计（SFGE）来计算决策关注更新，从而扩展 DFL 的应用范围。我们的实验表明，通过使用 SFGE，我们可以：（1）处理目标函数中的预测和约束中的预测；（2）有效地解决两阶段随机优化问题。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Pre-Planning-on-Reward-Machine-Abstractions-for-Enhanced-Transfer-in-Deep-Reinforcement-Learning"><a href="#Contextual-Pre-Planning-on-Reward-Machine-Abstractions-for-Enhanced-Transfer-in-Deep-Reinforcement-Learning" class="headerlink" title="Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning"></a>Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05209">http://arxiv.org/abs/2307.05209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Azran, Mohamad H. Danesh, Stefano V. Albrecht, Sarah Keren</li>
<li>for: 提高深度强化学习代理的适应能力和学习效率，使其能够更好地适应未看过的任务和环境变化。</li>
<li>methods: 使用奖机器（RM）来表示当前任务，通过生成符号表示法提供代理 Symbolic 表示当前任务的优质转移，并在多个任务之间共享这些表示，使代理可以利用已经遇到的符号和转移来增强转移。</li>
<li>results: 在多个领域中进行了实验，证明了我们的方法可以提高代理的样本效率和几个shot转移，从而提高深度强化学习代理的适应能力和学习效率。<details>
<summary>Abstract</summary>
Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reject-option-models-comprising-out-of-distribution-detection"><a href="#Reject-option-models-comprising-out-of-distribution-detection" class="headerlink" title="Reject option models comprising out-of-distribution detection"></a>Reject option models comprising out-of-distribution detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05199">http://arxiv.org/abs/2307.05199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vojtech Franc, Daniel Prusa, Jakub Paplham</li>
<li>for: 本研究旨在解决机器学习中的 OUT-OF-DISTRIBUTION（OOD）设置问题，提出了多种贡献。</li>
<li>methods: 本文提出了三种拒绝选项模型 для OOD 设置：Cost-based 模型、Bounded TPR-FPR 模型和 Bounded Precision-Recall 模型。这些模型将标准的拒绝选项模型扩展到非 OOD 设置，并定义了理想的 OOD 选择类фика器的概念。我们证明所提出的모든模型，尽管它们的不同形式化，都共享一个公共的优化策略。</li>
<li>results: 实验结果表明，使用两个选择的 OOD 检测器的uncertainty 分数来进行双重分数 OOD 方法，具有较高的性能。此外，我们提出了基于定义优化策略的新评价指标，以提供全面和可靠地评价 OOD 方法。<details>
<summary>Abstract</summary>
The optimal prediction strategy for out-of-distribution (OOD) setups is a fundamental question in machine learning. In this paper, we address this question and present several contributions. We propose three reject option models for OOD setups: the Cost-based model, the Bounded TPR-FPR model, and the Bounded Precision-Recall model. These models extend the standard reject option models used in non-OOD setups and define the notion of an optimal OOD selective classifier. We establish that all the proposed models, despite their different formulations, share a common class of optimal strategies. Motivated by the optimal strategy, we introduce double-score OOD methods that leverage uncertainty scores from two chosen OOD detectors: one focused on OOD/ID discrimination and the other on misclassification detection. The experimental results consistently demonstrate the superior performance of this simple strategy compared to state-of-the-art methods. Additionally, we propose novel evaluation metrics derived from the definition of the optimal strategy under the proposed OOD rejection models. These new metrics provide a comprehensive and reliable assessment of OOD methods without the deficiencies observed in existing evaluation approaches.
</details>
<details>
<summary>摘要</summary>
OPTIMAL PREDICTION STRATEGY FOR OUT-OF-DISTRIBUTION SETUPS 是机器学习领域的基本问题。在这篇论文中，我们回答这个问题，并提出了多种贡献。我们提出了三种拒绝选项模型 для OOD 设置：成本基于模型、TPR-FPR 边界值模型和精度-准确率边界值模型。这些模型将标准的非 OOD 拒绝选项模型推广到 OOD 设置，并定义了最佳 OOD 选择类фика器的概念。我们证明了所提出的모든模型，尽管它们的不同形式ulation，都共享一个共同的优化策略。受到优化策略的激发，我们提出了双分数 OOD 方法，利用两个选择的 OOD 探测器的uncertainty scores：一个专注于 OOD/ID 识别，另一个专注于错误探测。实验结果逐 consistently 表明这种简单的策略的 superior performance 与当前方法相比。此外，我们提出了基于定义最佳策略的新评价指标，用于评价 OOD 方法。这些新指标提供了全面和可靠的评价方法，不受现有评价方法的缺陷。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Statistical-Inference-through-β-Divergence-One-Posterior-Sampling"><a href="#Differentially-Private-Statistical-Inference-through-β-Divergence-One-Posterior-Sampling" class="headerlink" title="Differentially Private Statistical Inference through $β$-Divergence One Posterior Sampling"></a>Differentially Private Statistical Inference through $β$-Divergence One Posterior Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05194">http://arxiv.org/abs/2307.05194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Jewson, Sahra Ghalebikesabi, Chris Holmes</li>
<li>for: 本研究旨在提供一种可靠地保护敏感数据的统计分析结果的隐私保护机制，无需改变数据生成过程。</li>
<li>methods: 本研究使用 Bayesian  posterior sampling 方法，通过采样 Bayesian  posterior distribution 来生成私有的估计结果，而不需要人工添加噪声。</li>
<li>results: 本研究表明，使用 $\beta$D-Bayes 方法可以实现更高精度的隐私保护，同时可以用于复杂的分类器和连续回归模型，如神经网络。<details>
<summary>Abstract</summary>
Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the $\beta$-divergence between the model and the data generating process. This provides private estimation that is generally applicable without requiring changes to the underlying model and consistently learns the data generating parameter. We show that $\beta$D-Bayes produces more precise inference estimation for the same privacy guarantees, and further facilitates differentially private estimation via posterior sampling for complex classifiers and continuous regression models such as neural networks for the first time.
</details>
<details>
<summary>摘要</summary>
dif·fer·en·tial pri·va·cy guar·an·tees allow the results of a stat·is·tical analy·sis in·volv·ing sen·sen·tive data to be re·leased with·out com·pro·mis·ing the pri·va·cy of any in·di·vid·ual tak·ing part. Achiev·ing such guar·an·tees gen·er·al·ly re·quires the in·jec·tion of noise, either di·rect·ly into para·me·ter es·ti·mates or into the es·ti·ma·tion process. In·stead of ar·ti·fi·cial·ly in·tro·duc·ing per·tur·ba·tions, sam·pling from Bay·esian po·st·erior dis·trib·u·tions has been shown to be a spe·cial case of the ex·po·nen·tial mech·a·nism, pro·duc·ing con·sis·tent, and ef·fi·cient pri·va·te es·ti·mates with·out al·ter·ing the data gen·er·a·tive process. The ap·pli·ca·tion of cur·rent ap·proach·es has, how·ev·er, been lim·it·ed by their strong bound·ing as·sump·tions which do not hold for ba·sic mod·els, such as sim·ple lin·ear re·gressors. To ame·li·or·ate this, we pro·pose $\beta$D-Bayes, a po·ster·ior sam·pling scheme from a gen·er·al·ized po·ster·ior tar·get·ing the mi·ni·miza·tion of the $\beta$-di·ver·gence be·tween the model and the data gen·er·a·tive process. This pro·vides pri·va·te es·ti·ma·tion that is gen·er·al·ly ap·plic·a·ble with·out re·quir·ing changes to the un·der·ly·ing mo·del and con·sis·tent·ly learns the data gen·er·a·tive pa·ra·me·ter. We show that $\beta$D-Bayes pro·duces more pre·cise in·fer·ence es·ti·mates for the same pri·va·cy guar·an·tees, and fur·ther fa·cil·i·tates di·ffer·en·tially pri·va·te es·ti·ma·tion via po·ster·ior sam·pling for com·plex class·i·fi·ers and con·tin·u·ous re·gress·ion mod·els such as neu·ral net·works for the first time.
</details></li>
</ul>
<hr>
<h2 id="Membership-Inference-Attacks-on-DNNs-using-Adversarial-Perturbations"><a href="#Membership-Inference-Attacks-on-DNNs-using-Adversarial-Perturbations" class="headerlink" title="Membership Inference Attacks on DNNs using Adversarial Perturbations"></a>Membership Inference Attacks on DNNs using Adversarial Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05193">http://arxiv.org/abs/2307.05193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hassanalikhatim/amia">https://github.com/hassanalikhatim/amia</a></li>
<li>paper_authors: Hassan Ali, Adnan Qayyum, Ala Al-Fuqaha, Junaid Qadir</li>
<li>for: 这个论文主要目标是提出一种高置信度成员检测算法，以便在深度神经网络（DNN）训练后进行成员检测。</li>
<li>methods: 这个论文使用了现有的成员检测攻击（MI attack），并提出了两种新的攻击方法： adversarial membership inference attack（AMIA）和 enhance AMIA（E-AMIA）。这些攻击方法利用了subject的会员和非会员信息，并在一定的散度范围内进行了对准损失函数的最小化。</li>
<li>results: 论文的实验结果表明，AMIA和E-AMIA在Fashion-MNIST和MNIST datasets上的真正阳性率分别达到6%和8%，而现有的LiRA和EMIA在这些 datasets上的真正阳性率几乎为0。此外，这些攻击方法还能够在不同的训练方法和环境下进行 Transfer Learning，并且比现有的攻击方法更加稳定和可靠。<details>
<summary>Abstract</summary>
Several membership inference (MI) attacks have been proposed to audit a target DNN. Given a set of subjects, MI attacks tell which subjects the target DNN has seen during training. This work focuses on the post-training MI attacks emphasizing high confidence membership detection -- True Positive Rates (TPR) at low False Positive Rates (FPR). Current works in this category -- likelihood ratio attack (LiRA) and enhanced MI attack (EMIA) -- only perform well on complex datasets (e.g., CIFAR-10 and Imagenet) where the target DNN overfits its train set, but perform poorly on simpler datasets (0% TPR by both attacks on Fashion-MNIST, 2% and 0% TPR respectively by LiRA and EMIA on MNIST at 1% FPR). To address this, firstly, we unify current MI attacks by presenting a framework divided into three stages -- preparation, indication and decision. Secondly, we utilize the framework to propose two novel attacks: (1) Adversarial Membership Inference Attack (AMIA) efficiently utilizes the membership and the non-membership information of the subjects while adversarially minimizing a novel loss function, achieving 6% TPR on both Fashion-MNIST and MNIST datasets; and (2) Enhanced AMIA (E-AMIA) combines EMIA and AMIA to achieve 8% and 4% TPRs on Fashion-MNIST and MNIST datasets respectively, at 1% FPR. Thirdly, we introduce two novel augmented indicators that positively leverage the loss information in the Gaussian neighborhood of a subject. This improves TPR of all four attacks on average by 2.5% and 0.25% respectively on Fashion-MNIST and MNIST datasets at 1% FPR. Finally, we propose simple, yet novel, evaluation metric, the running TPR average (RTA) at a given FPR, that better distinguishes different MI attacks in the low FPR region. We also show that AMIA and E-AMIA are more transferable to the unknown DNNs (other than the target DNN) and are more robust to DP-SGD training as compared to LiRA and EMIA.
</details>
<details>
<summary>摘要</summary>
多种会员推测（MI）攻击已经提议用于审核目标神经网络（DNN）。给定一组主题，MI攻击可以确定目标DNN在训练过程中训练过的主题。这项工作专注于增强后期MI攻击，强调高置信度会员检测——准确率（TPR）在低 FALSE POSITIVE RATE（FPR）下。现有的工作在这个类别——概率比例攻击（LiRA）和增强MI攻击（EMIA）——只在复杂的数据集（如CIFAR-10和Imagenet）上表现出色，但在简单的数据集（Fashion-MNIST中的0% TPR，MNIST中的2%和0% TPR）上表现不佳。为解决这个问题，我们首先：一、将当前MI攻击统一为一个框架，分为三个阶段：准备阶段、指示阶段和决策阶段。二、使用框架提出两种新的攻击：（1）敌意会员推测攻击（AMIA）利用会员和非会员主题的信息，同时 adversarially 最小化一个新的损失函数，在Fashion-MNIST和MNIST数据集上达到6%的TPR；（2）增强AMIA（E-AMIA）结合EMIA和AMIA，在Fashion-MNIST和MNIST数据集上达到8%和4%的TPR，即1%的FPR。三、我们介绍两种新的扩展指标，利用损失函数的梯度信息在Gaussian neighborhood中提高TPR的平均值。这将在1% FPR下提高Fashion-MNIST和MNIST数据集的TPR平均值 by 2.5%和0.25%。最后，我们提出一个简单 yet novel的评价指标——运行TPR平均值（RTA）——更好地 отлича出不同的MI攻击在低FPR区域。我们还证明AMIA和E-AMIA在未知DNN上（与目标DNN不同）更加可转移和DP-SGD训练中更加稳定。
</details></li>
</ul>
<hr>
<h2 id="Using-Linear-Regression-for-Iteratively-Training-Neural-Networks"><a href="#Using-Linear-Regression-for-Iteratively-Training-Neural-Networks" class="headerlink" title="Using Linear Regression for Iteratively Training Neural Networks"></a>Using Linear Regression for Iteratively Training Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05189">http://arxiv.org/abs/2307.05189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Harshad Khadilkar</li>
<li>for: 这 paper 的目的是提出一种基于线性回归的神经网络学习方法，作为标准梯度下降法的一种alternative。</li>
<li>methods: 本 paper 使用的方法是基于输入层的线性组合和神经元参数（ weights 和 biases）来学习神经网络的参数。作者们提出了一种可靠和快速的算法，通过工作 backwards 从输出来计算理想的输入值，并在每个神经元上更新参数和活动值。</li>
<li>results: 作者们表明，对小 проблеmlarge, more complex architectures。 In addition, the approach is more stable and faster than gradient-based methods.<details>
<summary>Abstract</summary>
We present a simple linear regression based approach for learning the weights and biases of a neural network, as an alternative to standard gradient based backpropagation. The present work is exploratory in nature, and we restrict the description and experiments to (i) simple feedforward neural networks, (ii) scalar (single output) regression problems, and (iii) invertible activation functions. However, the approach is intended to be extensible to larger, more complex architectures. The key idea is the observation that the input to every neuron in a neural network is a linear combination of the activations of neurons in the previous layer, as well as the parameters (weights and biases) of the layer. If we are able to compute the ideal total input values to every neuron by working backwards from the output, we can formulate the learning problem as a linear least squares problem which iterates between updating the parameters and the activation values. We present an explicit algorithm that implements this idea, and we show that (at least for small problems) the approach is more stable and faster than gradient-based methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于线性回归的方法来学习 neural network 的权重和偏置。这项工作是探索性的，我们限制了描述和实验到（i）简单的前向 нейрон网络，（ii）具有唯一输出（单输出）回归问题，以及（iii）可逆活化函数。然而，该方法适用于更大、更复杂的结构。关键思想是观察每个神经元在神经网络中的输入是前一层神经元的活化值和当前层参数（权重和偏置）的线性组合。如果我们可以计算每个神经元的理想总输入值，那么我们可以将学习问题转化为一个线性最小二乘问题，这个问题可以通过更新参数和活化值进行迭代来解决。我们提供了一个明确的算法，并证明（至少对小问题）该方法比梯度基本方法更稳定和更快。
</details></li>
</ul>
<hr>
<h2 id="Decorrelation-using-Optimal-Transport"><a href="#Decorrelation-using-Optimal-Transport" class="headerlink" title="Decorrelation using Optimal Transport"></a>Decorrelation using Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05187">http://arxiv.org/abs/2307.05187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/malteal/ot-decorrelation">https://github.com/malteal/ot-decorrelation</a></li>
<li>paper_authors: Malte Algren, John Andrew Raine, Tobias Golling</li>
<li>for: decorrelate a continuous feature space against protected attributes with optimal transport</li>
<li>methods: Convex Neural Optimal Transport Solvers (Cnots)</li>
<li>results: achieved state-of-the-art performance in binary classification, and significantly better performance in multiclass outputs compared to the state-of-the-art.<details>
<summary>Abstract</summary>
Being able to decorrelate a feature space from protected attributes is an area of active research and study in ethics, fairness, and also natural sciences. We introduce a novel decorrelation method using Convex Neural Optimal Transport Solvers (Cnots) that is able to decorrelate a continuous feature space against protected attributes with optimal transport. We demonstrate how well it performs in the context of jet classification in high energy physics, where classifier scores are desired to be decorrelated from the mass of a jet. The decorrelation achieved in binary classification approaches the levels achieved by the state-of-the-art using conditional normalising flows. When moving to multiclass outputs the optimal transport approach performs significantly better than the state-of-the-art, suggesting substantial gains at decorrelating multidimensional feature spaces.
</details>
<details>
<summary>摘要</summary>
<trans-type>simplified-chinese</trans-type><text>能够减除保护属性相关的特征空间是一个活跃的研究领域，包括伦理、公平性和自然科学。我们介绍了一种使用凸神经最优运输算法（CNOTS）来减除连续特征空间与保护属性的距离最优运输方法。我们在高能物理中的扩散分类中示出了该方法的性能，并达到了 conditional normalizing flows 的水平。在多类输出情况下，优化运输方法表现更好， suggesting substantial gains in decorrelating multidimensional feature spaces.</text></sys>
</details></li>
</ul>
<hr>
<h2 id="A-Mapping-Study-of-Machine-Learning-Methods-for-Remaining-Useful-Life-Estimation-of-Lead-Acid-Batteries"><a href="#A-Mapping-Study-of-Machine-Learning-Methods-for-Remaining-Useful-Life-Estimation-of-Lead-Acid-Batteries" class="headerlink" title="A Mapping Study of Machine Learning Methods for Remaining Useful Life Estimation of Lead-Acid Batteries"></a>A Mapping Study of Machine Learning Methods for Remaining Useful Life Estimation of Lead-Acid Batteries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05163">http://arxiv.org/abs/2307.05163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sérgio F Chevtchenko, Elisson da Silva Rocha, Bruna Cruz, Ermeson Carneiro de Andrade, Danilo Ricardo Barbosa de Araújo</li>
<li>for: 这篇论文主要针对的是锂离子电池的状态评估和剩余用生命时间的机器学习方法。</li>
<li>methods: 本论文使用了多种机器学习算法来评估锂离子电池的状态和剩余用生命时间，并评估了这些算法的准确率和计算时间。</li>
<li>results: 本论文通过分析不同应用中使用的感知器组合，发现了一些常见的感知器组合，并评估了这些组合的性能。同时，本论文还发现了未来研究的潜在空白和机遇。<details>
<summary>Abstract</summary>
Energy storage solutions play an increasingly important role in modern infrastructure and lead-acid batteries are among the most commonly used in the rechargeable category. Due to normal degradation over time, correctly determining the battery's State of Health (SoH) and Remaining Useful Life (RUL) contributes to enhancing predictive maintenance, reliability, and longevity of battery systems. Besides improving the cost savings, correct estimation of the SoH can lead to reduced pollution though reuse of retired batteries. This paper presents a mapping study of the state-of-the-art in machine learning methods for estimating the SoH and RUL of lead-acid batteries. These two indicators are critical in the battery management systems of electric vehicles, renewable energy systems, and other applications that rely heavily on this battery technology. In this study, we analyzed the types of machine learning algorithms employed for estimating SoH and RUL, and evaluated their performance in terms of accuracy and inference time. Additionally, this mapping identifies and analyzes the most commonly used combinations of sensors in specific applications, such as vehicular batteries. The mapping concludes by highlighting potential gaps and opportunities for future research, which lays the foundation for further advancements in the field.
</details>
<details>
<summary>摘要</summary>
This study uses machine learning methods to estimate the SoH and RUL of lead-acid batteries. These indicators are critical in battery management systems for electric vehicles, renewable energy systems, and other applications that rely heavily on this battery technology. The study analyzed the types of machine learning algorithms used for estimating SoH and RUL, and evaluated their performance in terms of accuracy and inference time. Additionally, the study identified and analyzed the most commonly used combinations of sensors in specific applications, such as vehicular batteries.The study concludes by highlighting potential gaps and opportunities for future research, providing a foundation for further advancements in the field.
</details></li>
</ul>
<hr>
<h2 id="SuryaKiran-at-MEDIQA-Sum-2023-Leveraging-LoRA-for-Clinical-Dialogue-Summarization"><a href="#SuryaKiran-at-MEDIQA-Sum-2023-Leveraging-LoRA-for-Clinical-Dialogue-Summarization" class="headerlink" title="SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization"></a>SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05162">http://arxiv.org/abs/2307.05162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunal Suri, Prakhar Mishra, Saumajit Saha, Atul Singh</li>
<li>for: 本研究旨在探讨 Parametric Efficient Fine Tuning (PEFT) 方法可以提高适用于域域特定应用场景的语言模型的性能。</li>
<li>methods: 本研究使用的是 Low Rank Adaptation (LoRA) 方法，它在保持大语言模型为固定基础之前，添加额外层次，并使用 PEFT 方法进行微调。</li>
<li>results: 实验结果显示，LoRA 方法在临床对话概要SUMMARIZATION 任务上达到了与端到端微调相当的性能水平。<details>
<summary>Abstract</summary>
Finetuning Large Language Models helps improve the results for domain-specific use cases. End-to-end finetuning of large language models is time and resource intensive and has high storage requirements to store the finetuned version of the large language model. Parameter Efficient Fine Tuning (PEFT) methods address the time and resource challenges by keeping the large language model as a fixed base and add additional layers, which the PEFT methods finetune. This paper demonstrates the evaluation results for one such PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization. The evaluation results show that LoRA works at par with end-to-end finetuning for a large language model. The paper presents the evaluations done for solving both the Subtask A and B from ImageCLEFmedical {https://www.imageclef.org/2023/medical}
</details>
<details>
<summary>摘要</summary>
适应域pecific用 caso 中的语言模型细化可以提高结果。整个终端细化大语言模型需要很多时间和资源，同时存储细化后的大语言模型也需要很大的存储空间。Parameter Efficient Fine Tuning (PEFT) 方法解决了这些时间和资源挑战，它保留了大语言模型作为固定基础，并在其上添加了额外层，这些层由 PEFT 方法进行细化。本文介绍了一种名为 Low Rank Adaptation (LoRA) 的 PEFT 方法，用于临床对话摘要。试验结果表明，LoRA 与终端细化大语言模型相当。文章介绍了解决 ImageCLEF 医学 {https://www.imageclef.org/2023/medical} 中的两个任务 A 和 B 的评估结果。
</details></li>
</ul>
<hr>
<h2 id="Multiobjective-Hydropower-Reservoir-Operation-Optimization-with-Transformer-Based-Deep-Reinforcement-Learning"><a href="#Multiobjective-Hydropower-Reservoir-Operation-Optimization-with-Transformer-Based-Deep-Reinforcement-Learning" class="headerlink" title="Multiobjective Hydropower Reservoir Operation Optimization with Transformer-Based Deep Reinforcement Learning"></a>Multiobjective Hydropower Reservoir Operation Optimization with Transformer-Based Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05643">http://arxiv.org/abs/2307.05643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rixin Wu, Ran Wang, Jie Hao, Qiang Wu, Ping Wang<br>for: 这个研究的目的是为了对多座水库系统进行调和，以确保发电、生态保护和居民用水的平衡。methods: 这篇研究使用了深度强化学习法，并将 transformer 框架组合到了运算决策中。多头注意机制和多贝勒网络实现了储存和居民区域资讯的提取，并生成了适当的运作决策。results: 实验结果显示，使用这种方法可以生成适当的运作结果，比较于现有方法提高发电量10.11%，减少调整年度调整流量差异39.69%，增加水货收益4.10%。因此，这种方法可以对多座水库系统进行有效的调和运作。<details>
<summary>Abstract</summary>
Due to shortage of water resources and increasing water demands, the joint operation of multireservoir systems for balancing power generation, ecological protection, and the residential water supply has become a critical issue in hydropower management. However, the numerous constraints and nonlinearity of multiple reservoirs make solving this problem time-consuming. To address this challenge, a deep reinforcement learning approach that incorporates a transformer framework is proposed. The multihead attention mechanism of the encoder effectively extracts information from reservoirs and residential areas, and the multireservoir attention network of the decoder generates suitable operational decisions. The proposed method is applied to Lake Mead and Lake Powell in the Colorado River Basin. The experimental results demonstrate that the transformer-based deep reinforcement learning approach can produce appropriate operational outcomes. Compared to a state-of-the-art method, the operation strategies produced by the proposed approach generate 10.11% more electricity, reduce the amended annual proportional flow deviation by 39.69%, and increase water supply revenue by 4.10%. Consequently, the proposed approach offers an effective method for the multiobjective operation of multihydropower reservoir systems.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:由于水资源短缺和增长水需求， JOINT操作多个水电堤system为平衡发电、生态保护和居民用水供应成为水力管理中的核心问题。然而，多个堤坝的约束和非线性使得解决这个问题占用了很多时间。为 Addressing this challenge, a deep reinforcement learning approach that incorporates a transformer framework is proposed. The multi-head attention mechanism of the encoder effectively extracts information from reservoirs and residential areas, and the multi-reservoir attention network of the decoder generates suitable operational decisions. The proposed method is applied to Lake Mead and Lake Powell in the Colorado River Basin. The experimental results demonstrate that the transformer-based deep reinforcement learning approach can produce appropriate operational outcomes. Compared to a state-of-the-art method, the operation strategies produced by the proposed approach generate 10.11% more electricity, reduce the amended annual proportional flow deviation by 39.69%, and increase water supply revenue by 4.10%. Therefore, the proposed approach offers an effective method for the multiobjective operation of multihydropower reservoir systems.
</details></li>
</ul>
<hr>
<h2 id="On-the-Effectiveness-of-Speech-Self-supervised-Learning-for-Music"><a href="#On-the-Effectiveness-of-Speech-Self-supervised-Learning-for-Music" class="headerlink" title="On the Effectiveness of Speech Self-supervised Learning for Music"></a>On the Effectiveness of Speech Self-supervised Learning for Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05161">http://arxiv.org/abs/2307.05161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinghao Ma, Ruibin Yuan, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Ruibo Liu, Gus Xia, Roger Dannenberg, Yike Guo, Jie Fu</li>
<li>for: 本研究探讨了自主学习（SSL）在音乐信息检索（MIR）领域的应用，并评估了两种不同的speech相关模型在音乐数据上的适用性。</li>
<li>methods: 本研究使用了两种不同的speech相关模型，namely data2vec1.0和Hubert，并在不同的预训练配置下训练了12个SSL模型，共计95M个参数。</li>
<li>results: 研究发现，通过训练音乐数据可以改善MIR任务的性能，即使使用了speech相关的模型。然而，研究发现现有的speech导向的设计在处理多重音频信息方面存在局限性。基于实验结果，本研究还提出了未来音乐SSL策略和模式的设计建议。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent speech models such as wav2vec2.0 have shown promise in music modelling. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train $12$ SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech. However, we identify the limitations of such existing speech-oriented designs, especially in modelling polyphonic information. Based on the experimental results, empirical suggestions are also given for designing future musical SSL strategies and paradigms.
</details>
<details>
<summary>摘要</summary>
我们探索了将 Speech SSL 模型应用于音乐录制的音乐适应 (SSL)，并使用了 two distinctive speech-related models, data2vec1.0和Hubert，并将其称为music2vec和musicHuBERT，respectively. We train $12$ SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech. However, we identify the limitations of such existing speech-oriented designs, especially in modeling polyphonic information. Based on the experimental results, we also provide empirical suggestions for designing future musical SSL strategies and paradigms.
</details></li>
</ul>
<hr>
<h2 id="Fast-Neural-Network-Inference-on-FPGAs-for-Triggering-on-Long-Lived-Particles-at-Colliders"><a href="#Fast-Neural-Network-Inference-on-FPGAs-for-Triggering-on-Long-Lived-Particles-at-Colliders" class="headerlink" title="Fast Neural Network Inference on FPGAs for Triggering on Long-Lived Particles at Colliders"></a>Fast Neural Network Inference on FPGAs for Triggering on Long-Lived Particles at Colliders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05152">http://arxiv.org/abs/2307.05152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Coccaro, Francesco Armando Di Bello, Stefano Giagu, Lucrezia Rambelli, Nicola Stocchetti</li>
<li>for: 这项研究是为了开发一种高效的触发和获取系统，以便更好地处理高能物理实验中的碰撞事件。</li>
<li>methods: 这项研究使用了FPGA卡来实现不同的计算方法，以提高触发策略的效率。</li>
<li>results: 研究发现，使用FPGA卡加速的机器学习算法可以保持高度的精度，而且加速时间较短。此外，对比CPU和GPU硬件设置，FPGA卡的加速效果更好。<details>
<summary>Abstract</summary>
Experimental particle physics demands a sophisticated trigger and acquisition system capable to efficiently retain the collisions of interest for further investigation. Heterogeneous computing with the employment of FPGA cards may emerge as a trending technology for the triggering strategy of the upcoming high-luminosity program of the Large Hadron Collider at CERN. In this context, we present two machine-learning algorithms for selecting events where neutral long-lived particles decay within the detector volume studying their accuracy and inference time when accelerated on commercially available Xilinx FPGA accelerator cards. The inference time is also confronted with a CPU- and GPU-based hardware setup. The proposed new algorithms are proven efficient for the considered benchmark physics scenario and their accuracy is found to not degrade when accelerated on the FPGA cards. The results indicate that all tested architectures fit within the latency requirements of a second-level trigger farm and that exploiting accelerator technologies for real-time processing of particle-physics collisions is a promising research field that deserves additional investigations, in particular with machine-learning models with a large number of trainable parameters.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated using the Simplified Chinese language model, which is a standardized form of Chinese that is used in mainland China and Singapore. The translation is written in the traditional Chinese characters and not in the simplified ones, which are used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="ConFL-Constraint-guided-Fuzzing-for-Machine-Learning-Framework"><a href="#ConFL-Constraint-guided-Fuzzing-for-Machine-Learning-Framework" class="headerlink" title="ConFL: Constraint-guided Fuzzing for Machine Learning Framework"></a>ConFL: Constraint-guided Fuzzing for Machine Learning Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05642">http://arxiv.org/abs/2307.05642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao Liu, Quanchen Zou, Tian Yu, Xuan Wang, Guozhu Meng, Kai Chen, Deyue Zhang</li>
<li>For: The paper aims to propose a constraint-guided fuzzer for machine learning (ML) frameworks to improve the efficiency and effectiveness of fuzzing.* Methods: The proposed ConFL fuzzer automatically extracts constraints from kernel codes without prior knowledge, and uses these constraints to generate valid inputs that can pass verification and explore deeper paths of kernel codes. The paper also designs a grouping technique to boost fuzzing efficiency.* Results: The paper evaluates the performance of ConFL mainly on Tensorflow and finds that it covers more code lines and generates more valid inputs than state-of-the-art fuzzers. ConFL also discovers 84 previously unknown vulnerabilities in different versions of Tensorflow, including 3 critical-severity and 13 high-severity vulnerabilities. The paper also extends ConFL to test PyTorch and Paddle, finding 7 vulnerabilities to date.Here is the simplified Chinese text for the three key points:* For: 本研究旨在提出一种基于约束的机器学习（ML）框架测试工具，以提高测试效率和可靠性。* Methods: 提议的 ConFL 测试工具可以自动从核心代码中提取约束，无需任何先驱知识。基于这些约束，ConFL 可以生成可通过验证的有效输入，并探索更深层次的核心代码。 paper 还提出了分组技术，以提高测试效率。* Results: 本研究主要测试 Tensorflow，发现 ConFL 能够覆盖更多的代码行数，并生成更多的有效输入。此外，ConFL 还发现了 Tensorflow 不同版本中的84个新的漏洞，其中有3个是严重性高的漏洞。 paper 还扩展了 ConFL 到测试 PyTorch 和 Paddle，到目前为止发现了7个漏洞。<details>
<summary>Abstract</summary>
As machine learning gains prominence in various sectors of society for automated decision-making, concerns have risen regarding potential vulnerabilities in machine learning (ML) frameworks. Nevertheless, testing these frameworks is a daunting task due to their intricate implementation. Previous research on fuzzing ML frameworks has struggled to effectively extract input constraints and generate valid inputs, leading to extended fuzzing durations for deep execution or revealing the target crash.   In this paper, we propose ConFL, a constraint-guided fuzzer for ML frameworks. ConFL automatically extracting constraints from kernel codes without the need for any prior knowledge. Guided by the constraints, ConFL is able to generate valid inputs that can pass the verification and explore deeper paths of kernel codes. In addition, we design a grouping technique to boost the fuzzing efficiency.   To demonstrate the effectiveness of ConFL, we evaluated its performance mainly on Tensorflow. We find that ConFL is able to cover more code lines, and generate more valid inputs than state-of-the-art (SOTA) fuzzers. More importantly, ConFL found 84 previously unknown vulnerabilities in different versions of Tensorflow, all of which were assigned with new CVE ids, of which 3 were critical-severity and 13 were high-severity. We also extended ConFL to test PyTorch and Paddle, 7 vulnerabilities are found to date.
</details>
<details>
<summary>摘要</summary>
Machine learning 在不同领域的自动化决策中受到推广，但是有关机器学习（ML）架构的可能漏洞问题却愈来愈严重。实际上，测试这些架构是一个艰辛的任务，因为它们的实现非常复杂。在这篇论文中，我们提出了 ConFL，一个基于条件的对ML架构的搜寻器。ConFL可以自动从核心代码中提取约束，不需要任何先前知识。根据这些约束，ConFL能够产生有效的输入，并让核心代码进行更深入的测试。此外，我们设计了一种分组技术，以提高搜寻效率。为证明 ConFL 的有效性，我们主要对 Tensorflow 进行评估。我们发现 ConFL 能够覆盖更多的代码行数，并产生更多的有效的输入，比起现有的 SOTA 搜寻器。更重要的是，ConFL 发现了 Tensorflow 不同版本中的84个未知漏洞，其中3个是严重性高的漏洞。我们还将 ConFL 扩展到 PyTorch 和 Paddle，发现了7个漏洞。
</details></li>
</ul>
<hr>
<h2 id="Learning-Active-Subspaces-and-Discovering-Important-Features-with-Gaussian-Radial-Basis-Functions-Neural-Networks"><a href="#Learning-Active-Subspaces-and-Discovering-Important-Features-with-Gaussian-Radial-Basis-Functions-Neural-Networks" class="headerlink" title="Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks"></a>Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05639">http://arxiv.org/abs/2307.05639</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dannyzx/grbf-nns">https://github.com/dannyzx/grbf-nns</a></li>
<li>paper_authors: Danny D’Agostino, Ilija Ilievski, Christine Annette Shoemaker</li>
<li>for: 提出了一种可以同时实现强预测性和人类可读性的机器学习模型，以解决机器学习研究中最大化预测性和人类可读性之间的矛盾。</li>
<li>methods: 提出了一种基于径向基函数神经网络模型的修改，将其核函数加载了一个学习的精度矩阵。通过训练模型后提取precision矩阵的谱спект的方式，可以提取有价值信息，包括方向敏感度最大化和输入变量的重要性排名。</li>
<li>results: 通过对回归、分类和特征选择任务进行数值实验，与其他机器学习模型和深度学习基于嵌入特征选择技术进行比较，结果显示该模型不仅在预测性方面与竞争者具有优异性，还提供了可读性高的结果，可能为实际应用中决策过程提供帮助。<details>
<summary>Abstract</summary>
Providing a model that achieves a strong predictive performance and at the same time is interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the Radial Basis Function Neural Network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the prediction task enhancing the model interpretability. We conducted numerical experiments for regression, classification, and feature selection tasks, comparing our model against popular machine learning models and the state-of-the-art deep learning-based embedding feature selection techniques. Our results demonstrate that the proposed model does not only yield an attractive prediction performance with respect to the competitors but also provides meaningful and interpretable results that potentially could assist the decision-making process in real-world applications. A PyTorch implementation of the model is available on GitHub at the following link. https://github.com/dannyzx/GRBF-NNs
</details>
<details>
<summary>摘要</summary>
提供一个具有强预测性能并且可以被人类理解的模型是机器学习研究中最大的挑战，这是因为这两个目标之间存在矛盾。为解决这个挑战，我们提议对卷积函数神经网络模型进行修改，并在其核函数中添加学习型精度矩阵。我们发现在训练过程完成后，模型的准确矩阵的谱有着极大的信息价值。特别是，准确矩阵的特征向量可以解释模型的最大敏感方向，披露活动子空间和可能的维度减少任务。同时，特征向量还可以描述输入和隐藏变量之间的绝对变化关系，从而提供输入变量的重要性排名，这有助于提高模型的解释性。我们对回归、分类和特征选择任务进行了数学实验，与流行的机器学习模型和深度学习基于嵌入特征选择技术进行比较。我们的结果表明，我们的模型不仅在竞争对手中具有吸引人的预测性能，而且提供了可读ible和可解释的结果，这些结果可能在实际应用中帮助决策过程。PyTorch实现的模型可以在GitHub上找到，请参考以下链接：https://github.com/dannyzx/GRBF-NNs。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-of-Deep-Transfer-Learning-for-Anomaly-Detection-in-Industrial-Time-Series-Methods-Applications-and-Directions"><a href="#A-Comprehensive-Survey-of-Deep-Transfer-Learning-for-Anomaly-Detection-in-Industrial-Time-Series-Methods-Applications-and-Directions" class="headerlink" title="A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions"></a>A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05638">http://arxiv.org/abs/2307.05638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Yan, Ahmed Abdulkadir, Matthias Rosenthal, Gerrit A. Schatte, Benjamin F. Grewe, Thilo Stadelmann</li>
<li>for:  This paper focuses on the application of deep transfer learning in industrial process monitoring and anomaly detection, with the goal of enhancing efficiency and optimizing quality.</li>
<li>methods: The paper reviews and examines the problem settings of transfer learning and classifies the prevailing deep transfer learning methods, with a focus on their applications in industrial contexts.</li>
<li>results: The paper discusses the challenges and limitations of deep transfer learning in industrial contexts, and provides practical directions for solution design and implementation, including specific, actionable suggestions.Here’s the Chinese translation of the three key information points:</li>
<li>for: 这篇论文关注于工业过程监测和异常检测中使用深度转移学习，以提高效率和质量。</li>
<li>methods: 论文检查和分类深度转移学习方法的问题设置，主要在工业上下文中应用。</li>
<li>results: 论文描述了深度转移学习在工业上下文中的挑战和局限性，并提供了实践的解决方案设计和实施方法，包括具体的、可行的建议。<details>
<summary>Abstract</summary>
Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, the algorithms demand large volumes of labeled training data. However, due to the dynamic nature of processes and the environment, it is impractical to acquire the needed data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, this learning framework solves new tasks even with little or no additional labeled data. The approach bypasses the need to retrain a model from scratch for every new setup and dramatically reduces the labeled data requirement. This survey provides an in-depth review of deep transfer learning, examining the problem settings of transfer learning and classifying the prevailing deep transfer learning methods. Moreover, we delve into applying deep transfer learning in the context of a broad spectrum of time series anomaly detection tasks prevalent in primary industrial domains, e.g., manufacturing process monitoring, predictive maintenance, energy management, and infrastructure facility monitoring. We conclude this survey by underlining the challenges and limitations of deep transfer learning in industrial contexts. We also provide practical directions for solution design and implementation for these tasks, leading to specific, actionable suggestions.
</details>
<details>
<summary>摘要</summary>
自动监测工业过程有可能提高效率和优化质量，通过及时检测异常事件，以便及时干预。深度学习，作为检测非平凡模式的能力，在这个过程中扮演着关键角色。标准的深度学习方法适用于特定任务和数据类型。在训练过程中，算法需要大量标注数据。然而，由于生产过程和环境的动态性，获得需要的数据是不可能的。深度传输学习提供了一个解决方案，通过利用相关任务的知识和考虑数据分布的差异，解决新任务，甚至无需额外的标注数据。这种学习框架绕过了对新设置的模型重新训练的需要，减少了标注数据的需求，从而减少了训练时间。本文提供了深度传输学习的深入审查，包括传输学习问题的设定和深度传输学习方法的分类。此外，我们还探讨了在主要工业领域中广泛存在的时间序列异常检测任务中的深度传输学习应用，例如制造过程监测、预测维护、能源管理和基础设施监测。我们 conclude this survey by highlighting the challenges and limitations of deep transfer learning in industrial contexts, and provide practical directions for solution design and implementation, leading to specific, actionable suggestions.
</details></li>
</ul>
<hr>
<h2 id="Deep-Probabilistic-Movement-Primitives-with-a-Bayesian-Aggregator"><a href="#Deep-Probabilistic-Movement-Primitives-with-a-Bayesian-Aggregator" class="headerlink" title="Deep Probabilistic Movement Primitives with a Bayesian Aggregator"></a>Deep Probabilistic Movement Primitives with a Bayesian Aggregator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05141">http://arxiv.org/abs/2307.05141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Przystupa, Faezeh Haghverd, Martin Jagersand, Samuele Tosatto</li>
<li>for: 本研究的目的是提出一种深度运动原理模型，可以执行先前的运动操作，包括时间调整、混合、终点决定和上下文决定。</li>
<li>methods: 本研究使用了深度学习模型来实现运动原理模型，并使用bayesianContext aggregator来实现更好的上下文决定和混合。</li>
<li>results: 实验结果表明，我们的方法可以在更多的输入选择下实现复杂的运动复制，而且与基eline运动原理模型提供的操作相当。<details>
<summary>Abstract</summary>
Movement primitives are trainable parametric models that reproduce robotic movements starting from a limited set of demonstrations. Previous works proposed simple linear models that exhibited high sample efficiency and generalization power by allowing temporal modulation of movements (reproducing movements faster or slower), blending (merging two movements into one), via-point conditioning (constraining a movement to meet some particular via-points) and context conditioning (generation of movements based on an observed variable, e.g., position of an object). Previous works have proposed neural network-based motor primitive models, having demonstrated their capacity to perform tasks with some forms of input conditioning or time-modulation representations. However, there has not been a single unified deep motor primitive's model proposed that is capable of all previous operations, limiting neural motor primitive's potential applications. This paper proposes a deep movement primitive architecture that encodes all the operations above and uses a Bayesian context aggregator that allows a more sound context conditioning and blending. Our results demonstrate our approach can scale to reproduce complex motions on a larger variety of input choices compared to baselines while maintaining operations of linear movement primitives provide.
</details>
<details>
<summary>摘要</summary>
movement primitives 是可训练的参数化模型，可以复制 Starting from a limited set of demonstrations, robotic movements. Previous works have proposed simple linear models that exhibited high sample efficiency and generalization power by allowing temporal modulation of movements (reproducing movements faster or slower), blending (merging two movements into one), via-point conditioning (constraining a movement to meet some particular via-points) and context conditioning (generation of movements based on an observed variable, e.g., position of an object). Previous works have proposed neural network-based motor primitive models, having demonstrated their capacity to perform tasks with some forms of input conditioning or time-modulation representations. However, there has not been a single unified deep motor primitive's model proposed that is capable of all previous operations, limiting neural motor primitive's potential applications. This paper proposes a deep movement primitive architecture that encodes all the operations above and uses a Bayesian context aggregator that allows a more sound context conditioning and blending. Our results demonstrate our approach can scale to reproduce complex motions on a larger variety of input choices compared to baselines while maintaining operations of linear movement primitives provide.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Speech-Diarization-and-ASR-with-GMM"><a href="#Speech-Diarization-and-ASR-with-GMM" class="headerlink" title="Speech Diarization and ASR with GMM"></a>Speech Diarization and ASR with GMM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05637">http://arxiv.org/abs/2307.05637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aayush Kumar Sharma, Vineet Bhavikatti, Amogh Nidawani, Dr. Siddappaji, Sanath P, Dr Geetishree Mishra</li>
<li>for: 这个研究论文主要探讨了语音分类和自动语音识别（ASR）问题。语音分类是将音频流中的各个发言者分类为不同的个体，使用ASR转文本来实现这一目标。</li>
<li>methods: 我们在语音分类方法中使用了 Gaussian Mixer Model（GMM）来表示语音段。计算间集距离基于GMM参数，使用距离阈值作为停止 criterion。</li>
<li>results: 我们的主要目标是开发一个可以最小化单词错误率（WER）度量的语音转文本模型。<details>
<summary>Abstract</summary>
In this research paper, we delve into the topics of Speech Diarization and Automatic Speech Recognition (ASR). Speech diarization involves the separation of individual speakers within an audio stream. By employing the ASR transcript, the diarization process aims to segregate each speaker's utterances, grouping them based on their unique audio characteristics. On the other hand, Automatic Speech Recognition refers to the capability of a machine or program to identify and convert spoken words and phrases into a machine-readable format. In our speech diarization approach, we utilize the Gaussian Mixer Model (GMM) to represent speech segments. The inter-cluster distance is computed based on the GMM parameters, and the distance threshold serves as the stopping criterion. ASR entails the conversion of an unknown speech waveform into a corresponding written transcription. The speech signal is analyzed using synchronized algorithms, taking into account the pitch frequency. Our primary objective typically revolves around developing a model that minimizes the Word Error Rate (WER) metric during speech transcription.
</details>
<details>
<summary>摘要</summary>
在这篇研究报告中，我们探讨了语音分类和自动语音识别（ASR）的话题。语音分类是将音频流中的各个说话人分割成不同的个体。通过使用ASR转cript，分类过程将每个说话人的讲话分组成为各自的音频特征。相反，自动语音识别是一种机器或程序能够识别和将口头语音转换成可读格式的能力。在我们的语音分类方法中，我们使用高斯混合模型（GMM）来表示语音段落。间隔距离是根据GMM参数计算的，并且距离阈值作为停止条件。ASR则是将未知的语音波形转换成相应的书面转cript。语音信号被同步的算法分析，考虑到抽屉频率。我们的主要目标通常是开发一个能够最小化单词错误率（WER）度量的模型。
</details></li>
</ul>
<hr>
<h2 id="TIAM-–-A-Metric-for-Evaluating-Alignment-in-Text-to-Image-Generation"><a href="#TIAM-–-A-Metric-for-Evaluating-Alignment-in-Text-to-Image-Generation" class="headerlink" title="TIAM – A Metric for Evaluating Alignment in Text-to-Image Generation"></a>TIAM – A Metric for Evaluating Alignment in Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05134">http://arxiv.org/abs/2307.05134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grimalpaul/tiam">https://github.com/grimalpaul/tiam</a></li>
<li>paper_authors: Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille</li>
<li>for: 本研究旨在评估文本到图像（T2I）模型生成图像的质量，特别是考虑提示中的重要内容是否正确反映在生成图像中。</li>
<li>methods: 我们提出了一种基于提示模板的新评价指标，可以更好地衡量生成图像与提示中的内容的对应关系，包括提示中的对象类型、数量和颜色等方面。</li>
<li>results: 我们通过对多种最新的T2I模型进行研究发现，图像质量受到种子图像的随机变化的影响很大，同时提示中的概念数量、顺序和颜色属性也会影响图像质量。此外，我们还发现了一些特定的种子图像可以生成更高质量的图像，开启了新的研究方向。<details>
<summary>Abstract</summary>
The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically depending on the latent noise used as a seed for the images. We also quantify the influence of the number of concepts in the prompt, their order as well as their (color) attributes. Finally, our method allows us to identify some latent seeds that produce better images than others, opening novel directions of research on this understudied topic.
</details>
<details>
<summary>摘要</summary>
“随着生成Synthetic图像的进步，它成为了评估图像质量的关键。多个指标有被提议来评估图像的渲染，但是Text-to-Image（T2I）模型，它们根据提示生成图像，需要考虑更多的方面，例如提示中重要内容和生成图像之间的对齐度。此外，生成图像通常是由Random Starting Point开始的，但是这个影响通常不被考虑。在这篇文章中，我们提出了一个基于提示模板的新指标，以研究提示中的内容和生成图像之间的对齐度。这允许我们更好地描述对齐度的类型、物件的数量和颜色等方面。我们对多个现代T2I模型进行了一系列研究，获得了一些有趣的结果。例如，图像质量可以很大程度上受到latent noise的影响，并且我们可以量化提示中的概念数量、顺序以及颜色属性的影响。最后，我们的方法允许我们识别一些latent seed的特定对图像质量的影响，开启了一些未经研究的方向。”
</details></li>
</ul>
<hr>
<h2 id="On-the-Use-of-Self-Supervised-Speech-Representations-in-Spontaneous-Speech-Synthesis"><a href="#On-the-Use-of-Self-Supervised-Speech-Representations-in-Spontaneous-Speech-Synthesis" class="headerlink" title="On the Use of Self-Supervised Speech Representations in Spontaneous Speech Synthesis"></a>On the Use of Self-Supervised Speech Representations in Spontaneous Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05132">http://arxiv.org/abs/2307.05132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyang Wang, Gustav Eje Henter, Joakim Gustafson, Éva Székely</li>
<li>For: The paper is focused on exploring the use of self-supervised learning (SSL) in spontaneous text-to-speech (TTS) and predicting the mean opinion scores (MOS) of synthesized speech.* Methods: The paper uses six different SSL models and three layers within each SSL model to evaluate their effectiveness in spontaneous TTS. The authors also extend an existing SSL-based MOS prediction framework to predict the quality of synthesized spontaneous speech.* Results: The paper presents comprehensive experimental results on the use of SSL in spontaneous TTS and MOS prediction, including the performance of different SSL models and layers in predicting the MOS of synthesized speech. The results show that certain SSL models and layers perform better than others in spontaneous TTS and MOS prediction.<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) speech representations learned from large amounts of diverse, mixed-quality speech data without transcriptions are gaining ground in many speech technology applications. Prior work has shown that SSL is an effective intermediate representation in two-stage text-to-speech (TTS) for both read and spontaneous speech. However, it is still not clear which SSL and which layer from each SSL model is most suited for spontaneous TTS. We address this shortcoming by extending the scope of comparison for SSL in spontaneous TTS to 6 different SSLs and 3 layers within each SSL. Furthermore, SSL has also shown potential in predicting the mean opinion scores (MOS) of synthesized speech, but this has only been done in read-speech MOS prediction. We extend an SSL-based MOS prediction framework previously developed for scoring read speech synthesis and evaluate its performance on synthesized spontaneous speech. All experiments are conducted twice on two different spontaneous corpora in order to find generalizable trends. Overall, we present comprehensive experimental results on the use of SSL in spontaneous TTS and MOS prediction to further quantify and understand how SSL can be used in spontaneous TTS. Audios samples: https://www.speech.kth.se/tts-demos/sp_ssl_tts
</details>
<details>
<summary>摘要</summary>
自我指导学习（SSL）所学习的语音表示法，从大量多样性的杂质语音数据中学习，无需转录。先前的研究表明，SSL 是两stage text-to-speech（TTS）中的有效中间表示，包括阅读和自发语音。然而，尚未确定哪些 SSL 和哪层在每个 SSL 模型最适合自发 TTS。我们解决这个缺陷，通过对 SSL 在自发 TTS 中进行扩展比较，包括 6 种 SSL 和每个 SSL 模型中的 3 层。此外，SSL 还显示了在synthesized speech的 mean opinion scores（MOS）预测中的潜力，但只有在阅读 speech MOS 预测中进行过。我们扩展了以前为阅读 speech synthesis 预测 MOS 的 SSL-based 框架，并对 synthesized spontaneous speech 进行评估。所有实验都在两个不同的自发语音 corpus 上进行了两次，以找到通用的趋势。总的来说，我们提供了 SSL 在自发 TTS 和 MOS 预测中的详细实验结果，以更好地量化和理解 SSL 在自发 TTS 中的使用。响应样本：https://www.speech.kth.se/tts-demos/sp_ssl_tts
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Continuous-Time-Series-Modelling-with-a-Latent-ODE-LSTM-Approach"><a href="#Enhancing-Continuous-Time-Series-Modelling-with-a-Latent-ODE-LSTM-Approach" class="headerlink" title="Enhancing Continuous Time Series Modelling with a Latent ODE-LSTM Approach"></a>Enhancing Continuous Time Series Modelling with a Latent ODE-LSTM Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05126">http://arxiv.org/abs/2307.05126</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Coelho, M. Fernanda P. Costa, L. L. Ferrás</li>
<li>for: 这个论文主要针对连续时间序列（CTS）的模型化，尤其是在 irregular sampling rate 和高频 sampling rate 下。</li>
<li>methods: 该论文提出了一种基于 ODE-RNN 和 Latent ODE 模型的新模型 - Latent ODE-LSTM，以解决 CTTS 模型化中的vanishing和exploding gradients问题。</li>
<li>results: 数值实验表明，新提出的 Latent ODE-LSTM 模型在模型 CTTS  WITH regular和irregular sampling rates 时，表现更好于 Latent ODE-RNNs，并能够避免在训练过程中的vanishing和exploding gradients问题。<details>
<summary>Abstract</summary>
Due to their dynamic properties such as irregular sampling rate and high-frequency sampling, Continuous Time Series (CTS) are found in many applications. Since CTS with irregular sampling rate are difficult to model with standard Recurrent Neural Networks (RNNs), RNNs have been generalised to have continuous-time hidden dynamics defined by a Neural Ordinary Differential Equation (Neural ODE), leading to the ODE-RNN model. Another approach that provides a better modelling is that of the Latent ODE model, which constructs a continuous-time model where a latent state is defined at all times. The Latent ODE model uses a standard RNN as the encoder and a Neural ODE as the decoder. However, since the RNN encoder leads to difficulties with missing data and ill-defined latent variables, a Latent ODE-RNN model has recently been proposed that uses a ODE-RNN model as the encoder instead. Both the Latent ODE and Latent ODE-RNN models are difficult to train due to the vanishing and exploding gradients problem. To overcome this problem, the main contribution of this paper is to propose and illustrate a new model based on a new Latent ODE using an ODE-LSTM (Long Short-Term Memory) network as an encoder -- the Latent ODE-LSTM model. To limit the growth of the gradients the Norm Gradient Clipping strategy was embedded on the Latent ODE-LSTM model. The performance evaluation of the new Latent ODE-LSTM (with and without Norm Gradient Clipping) for modelling CTS with regular and irregular sampling rates is then demonstrated. Numerical experiments show that the new Latent ODE-LSTM performs better than Latent ODE-RNNs and can avoid the vanishing and exploding gradients during training.
</details>
<details>
<summary>摘要</summary>
因为它们的动态属性，如不规则采样速率和高频采样， kontinuous time series (CTS) 在许多应用中出现。由于标准的回归神经网络 (RNN) 不能模型 CTs 的不规则采样速率，因此 RNN 被推广为有连续时间隐藏动力学定义的神经普通微分方程 (Neural Ordinary Differential Equation, ODE)，导致 ODE-RNN 模型的出现。另一种提供更好的模型是隐藏 ODE 模型，该模型在全部时间上定义了隐藏状态。隐藏 ODE 模型使用标准的 RNN 作为编码器和 ODE 作为解码器。然而，由 RNN 编码器导致缺失数据和不定义的隐藏变量，因此在最近提出了 Latent ODE-RNN 模型，该模型使用 ODE-RNN 作为编码器。两种 Latent ODE 和 Latent ODE-RNN 模型都困难于训练，主要是因为训练过程中的混血和爆炸梯度问题。为解决这个问题，本文的主要贡献是提出了一种基于新的隐藏 ODE 模型，使用 ODE-LSTM (Long Short-Term Memory) 网络作为编码器 - 隐藏 ODE-LSTM 模型。为限制梯度的增长，在隐藏 ODE-LSTM 模型中嵌入了 Norm Gradient Clipping 策略。对 CTs  WITH 规则和无规则采样速率进行表达评估，并对新模型的性能进行比较。数值实验显示，新的隐藏 ODE-LSTM 模型比 Latent ODE-RNN 更好，并且可以在训练过程中避免混血和爆炸梯度问题。
</details></li>
</ul>
<hr>
<h2 id="Transaction-Fraud-Detection-via-Spatial-Temporal-Aware-Graph-Transformer"><a href="#Transaction-Fraud-Detection-via-Spatial-Temporal-Aware-Graph-Transformer" class="headerlink" title="Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer"></a>Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05121">http://arxiv.org/abs/2307.05121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Tian, Guanjun Liu</li>
<li>for: 预防金融 transactions 诈骗</li>
<li>methods: 使用 Graph Neural Networks (GNNs) 和 transformer module 模型，capture temporal dependencies 和 learn local and global information</li>
<li>results: 在两个金融数据集上比较于常见 GNN 模型和 GNN-based fraud detectors 表现出色，有效地检测 transaction fraud<details>
<summary>Abstract</summary>
How to obtain informative representations of transactions and then perform the identification of fraudulent transactions is a crucial part of ensuring financial security. Recent studies apply Graph Neural Networks (GNNs) to the transaction fraud detection problem. Nevertheless, they encounter challenges in effectively learning spatial-temporal information due to structural limitations. Moreover, few prior GNN-based detectors have recognized the significance of incorporating global information, which encompasses similar behavioral patterns and offers valuable insights for discriminative representation learning. Therefore, we propose a novel heterogeneous graph neural network called Spatial-Temporal-Aware Graph Transformer (STA-GT) for transaction fraud detection problems. Specifically, we design a temporal encoding strategy to capture temporal dependencies and incorporate it into the graph neural network framework, enhancing spatial-temporal information modeling and improving expressive ability. Furthermore, we introduce a transformer module to learn local and global information. Pairwise node-node interactions overcome the limitation of the GNN structure and build up the interactions with the target node and long-distance ones. Experimental results on two financial datasets compared to general GNN models and GNN-based fraud detectors demonstrate that our proposed method STA-GT is effective on the transaction fraud detection task.
</details>
<details>
<summary>摘要</summary>
如何获得有用的交易表示并实现交易识别是金融安全的关键环节。现有研究利用图神经网络（GNN）解决交易诈骗问题。然而，它们在有效地学习空间-时间信息方面遇到了结构限制。此外，先前的GNN基本检测器很少认可全球信息的重要性，这些信息包括类似行为模式，它们提供了价值的表示学习意味。因此，我们提出了一种新的多类型图神经网络模型，即空间-时间感知图Transformer（STA-GT），用于交易诈骗检测问题。具体来说，我们设计了时间编码策略，以捕捉时间依赖关系，并将其 integrate into GNN框架中，提高空间-时间信息模型化的表达能力。此外，我们引入了Transformer模块，以学习本地和全球信息。对于目标节点和远程节点之间的对比交互，我们建立了对抗限制GNN结构的对抗。实验结果表明，我们提出的STA-GT方法在两个金融数据集上与通用GNN模型和GNN基本检测器相比，在交易诈骗检测任务上表现出色。
</details></li>
</ul>
<hr>
<h2 id="ell-p-Regression-in-the-Arbitrary-Partition-Model-of-Communication"><a href="#ell-p-Regression-in-the-Arbitrary-Partition-Model-of-Communication" class="headerlink" title="$\ell_p$-Regression in the Arbitrary Partition Model of Communication"></a>$\ell_p$-Regression in the Arbitrary Partition Model of Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05117">http://arxiv.org/abs/2307.05117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Li, Honghao Lin, David P. Woodruff</li>
<li>for: 这个论文研究了分布式 $\ell_p$-回归问题在协调器模型中的随机通信复杂度， $p\in (0,2]$。</li>
<li>methods: 作者使用了随机化通信复杂度来研究这个问题，并提供了更好的上界和下界。</li>
<li>results: 作者得到了更好的上界，特别是在 $p &#x3D; 2$ 情况下，提供了首个优等式 bound of $\tilde{\Theta}(sd^2 + sd&#x2F;\epsilon)$ 比特。 在 $p \in (1,2]$ 情况下，作者提供了 $\tilde{O}(sd^2&#x2F;\epsilon + sd&#x2F;\text{poly}(\epsilon))$ 上界，其中 $d$ 是数据维度。<details>
<summary>Abstract</summary>
We consider the randomized communication complexity of the distributed $\ell_p$-regression problem in the coordinator model, for $p\in (0,2]$. In this problem, there is a coordinator and $s$ servers. The $i$-th server receives $A^i\in\{-M, -M+1, \ldots, M\}^{n\times d}$ and $b^i\in\{-M, -M+1, \ldots, M\}^n$ and the coordinator would like to find a $(1+\epsilon)$-approximate solution to $\min_{x\in\mathbb{R}^n} \|(\sum_i A^i)x - (\sum_i b^i)\|_p$. Here $M \leq \mathrm{poly}(nd)$ for convenience. This model, where the data is additively shared across servers, is commonly referred to as the arbitrary partition model.   We obtain significantly improved bounds for this problem. For $p = 2$, i.e., least squares regression, we give the first optimal bound of $\tilde{\Theta}(sd^2 + sd/\epsilon)$ bits.   For $p \in (1,2)$,we obtain an $\tilde{O}(sd^2/\epsilon + sd/\mathrm{poly}(\epsilon))$ upper bound. Notably, for $d$ sufficiently large, our leading order term only depends linearly on $1/\epsilon$ rather than quadratically. We also show communication lower bounds of $\Omega(sd^2 + sd/\epsilon^2)$ for $p\in (0,1]$ and $\Omega(sd^2 + sd/\epsilon)$ for $p\in (1,2]$. Our bounds considerably improve previous bounds due to (Woodruff et al. COLT, 2013) and (Vempala et al., SODA, 2020).
</details>
<details>
<summary>摘要</summary>
我们考虑分布式 $\ell_p$ 回传问题的随机通信复杂度在协调模型中，其中 $p \in (0,2]$。在这个问题中，有一个协调者和 $s$ 个服务器。每个服务器 $i$ 获得 $A^i \in \{-M, -M+1, \ldots, M\}^{n \times d}$ 和 $b^i \in \{-M, -M+1, \ldots, M\}^n$，并且协调者想要找到 $(1+\epsilon)$-近似解的 $\min_{x \in \mathbb{R}^n} \|(\sum_i A^i)x - (\sum_i b^i)\|_p$。其中 $M \leq \text{poly}(nd)$ 是一个实数。这个模型，称为可以分配式模型，是由数据以添加方式分享到服务器。我们获得了非常改善的上界。对于 $p = 2$，即最小二乘回传，我们给出了首个 $\tilde{\Theta}(sd^2 + sd/\epsilon)$ 比特的上界。对于 $p \in (1,2)$，我们获得了 $\tilde{O}(sd^2/\epsilon + sd/\text{poly}(\epsilon))$ 的上界。特别是当 $d$ 够大时，我们的主要项只靠对应 $\frac{1}{\epsilon}$ 而不是 $\frac{1}{\epsilon^2}$。我们还证明了分布式通信下界，包括 $\Omega(sd^2 + sd/\epsilon^2)$ 和 $\Omega(sd^2 + sd/\epsilon)$。我们的下界明显超过先前的 bounds，例如 (Woodruff et al., COLT, 2013) 和 (Vempala et al., SODA, 2020)。
</details></li>
</ul>
<hr>
<h2 id="Conformalization-of-Sparse-Generalized-Linear-Models"><a href="#Conformalization-of-Sparse-Generalized-Linear-Models" class="headerlink" title="Conformalization of Sparse Generalized Linear Models"></a>Conformalization of Sparse Generalized Linear Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05109">http://arxiv.org/abs/2307.05109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/etashguha/sparse_conformal">https://github.com/etashguha/sparse_conformal</a></li>
<li>paper_authors: Etash Kumar Guha, Eugene Ndiaye, Xiaoming Huo</li>
<li>for: 这篇论文关注的是如何使用简单的线性模型和数字继续技术来实现可靠的预测集，以便在大数据量时进行高效的预测。</li>
<li>methods: 这篇论文使用的方法包括使用唯一的一些变量进行预测，并使用数字继续技术来精确地计算预测集。</li>
<li>results: 论文的结果表明，使用这种方法可以高效地生成预测集，并且可以在不同的数据集上进行高效的预测。<details>
<summary>Abstract</summary>
Given a sequence of observable variables $\{(x_1, y_1), \ldots, (x_n, y_n)\}$, the conformal prediction method estimates a confidence set for $y_{n+1}$ given $x_{n+1}$ that is valid for any finite sample size by merely assuming that the joint distribution of the data is permutation invariant. Although attractive, computing such a set is computationally infeasible in most regression problems. Indeed, in these cases, the unknown variable $y_{n+1}$ can take an infinite number of possible candidate values, and generating conformal sets requires retraining a predictive model for each candidate. In this paper, we focus on a sparse linear model with only a subset of variables for prediction and use numerical continuation techniques to approximate the solution path efficiently. The critical property we exploit is that the set of selected variables is invariant under a small perturbation of the input data. Therefore, it is sufficient to enumerate and refit the model only at the change points of the set of active features and smoothly interpolate the rest of the solution via a Predictor-Corrector mechanism. We show how our path-following algorithm accurately approximates conformal prediction sets and illustrate its performance using synthetic and real data examples.
</details>
<details>
<summary>摘要</summary>
In this paper, we focus on a sparse linear model with only a subset of variables for prediction and use numerical continuation techniques to approximate the solution path efficiently. The critical property we exploit is that the set of selected variables is invariant under a small perturbation of the input data. Therefore, it is sufficient to enumerate and refit the model only at the change points of the set of active features and smoothly interpolate the rest of the solution via a Predictor-Corrector mechanism.We show how our path-following algorithm accurately approximates conformal prediction sets and illustrate its performance using synthetic and real data examples.
</details></li>
</ul>
<hr>
<h2 id="Fundamental-limits-of-overparametrized-shallow-neural-networks-for-supervised-learning"><a href="#Fundamental-limits-of-overparametrized-shallow-neural-networks-for-supervised-learning" class="headerlink" title="Fundamental limits of overparametrized shallow neural networks for supervised learning"></a>Fundamental limits of overparametrized shallow neural networks for supervised learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05635">http://arxiv.org/abs/2307.05635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Camilli, Daria Tieplova, Jean Barbier</li>
<li>for: 这 paper 是关于两层神经网络在有限数据训练下的信息理论分析，以确定神经网络训练时的基本性能限制。</li>
<li>methods: 这 paper 使用了 teacher 网络生成的输入输出对来训练一个二层神经网络，并利用信息理论 bound 来描述神经网络的性能。</li>
<li>results: 这 paper 得到了一些 bounds，表明在有限数据训练下，神经网络的性能受到输入维度、隐藏层Unit 和训练样本数量的限制。这些 bounds 是对任何神经网络训练过程的基本性能限制，并且覆盖了所有网络参数的训练情况。<details>
<summary>Abstract</summary>
We carry out an information-theoretical analysis of a two-layer neural network trained from input-output pairs generated by a teacher network with matching architecture, in overparametrized regimes. Our results come in the form of bounds relating i) the mutual information between training data and network weights, or ii) the Bayes-optimal generalization error, to the same quantities but for a simpler (generalized) linear model for which explicit expressions are rigorously known. Our bounds, which are expressed in terms of the number of training samples, input dimension and number of hidden units, thus yield fundamental performance limits for any neural network (and actually any learning procedure) trained from limited data generated according to our two-layer teacher neural network model. The proof relies on rigorous tools from spin glasses and is guided by ``Gaussian equivalence principles'' lying at the core of numerous recent analyses of neural networks. With respect to the existing literature, which is either non-rigorous or restricted to the case of the learning of the readout weights only, our results are information-theoretic (i.e. are not specific to any learning algorithm) and, importantly, cover a setting where all the network parameters are trained.
</details>
<details>
<summary>摘要</summary>
我们进行了信息理论分析，对一个两层神经网络，从输入输出对生成的教师网络中训练，在过参数化的情况下。我们的结果以bounds的形式表达，关系于一、training数据和神经网络参数之间的共 informations，或二、最优化泛化误差的bayesian优化 bound。我们的 bound 表达式为数据训练样本数、输入维度和隐藏层单元数，从而获得任何神经网络（以及任何学习过程）从有限数据生成的基本性能上限。证明基于磁矿石的精确工具，受到“Gaussian equivalence principles”的引导，这些原理在神经网络分析中具有重要作用。相比现有文献，我们的结果是信息理论的（即不受任何学习算法限制），同时覆盖了所有网络参数都被训练的情况。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Dive-into-Perturbations-as-Evaluation-Technique-for-Time-Series-XAI"><a href="#A-Deep-Dive-into-Perturbations-as-Evaluation-Technique-for-Time-Series-XAI" class="headerlink" title="A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI"></a>A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05104">http://arxiv.org/abs/2307.05104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visual-xai-for-time-series/time-series-xai-perturbation-analysis">https://github.com/visual-xai-for-time-series/time-series-xai-perturbation-analysis</a></li>
<li>paper_authors: Udo Schlegel, Daniel A. Keim</li>
<li>for: 本研究旨在评估时序数据XAI技术中的质量，提供可靠和可解释的机器学习模型。</li>
<li>methods: 本研究使用扰动分析方法评估XAI技术中的质量，通过修改输入数据来评估XAI方法生成的权重。</li>
<li>results: 研究结果表明，扰动分析方法可以有效评估XAI技术中的质量，并提供时序数据XAI技术的强点和局限性。这种方法可以帮助选择适合时序数据的XAI方法，以及开发更可靠和可解释的机器学习模型。<details>
<summary>Abstract</summary>
Explainable Artificial Intelligence (XAI) has gained significant attention recently as the demand for transparency and interpretability of machine learning models has increased. In particular, XAI for time series data has become increasingly important in finance, healthcare, and climate science. However, evaluating the quality of explanations, such as attributions provided by XAI techniques, remains challenging. This paper provides an in-depth analysis of using perturbations to evaluate attributions extracted from time series models. A perturbation analysis involves systematically modifying the input data and evaluating the impact on the attributions generated by the XAI method. We apply this approach to several state-of-the-art XAI techniques and evaluate their performance on three time series classification datasets. Our results demonstrate that the perturbation analysis approach can effectively evaluate the quality of attributions and provide insights into the strengths and limitations of XAI techniques. Such an approach can guide the selection of XAI methods for time series data, e.g., focusing on return time rather than precision, and facilitate the development of more reliable and interpretable machine learning models for time series analysis.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>最近，可解释人工智能（XAI）已经受到了广泛关注，因为机器学习模型的透明度和可解释性的需求增加了。特别是在金融、医疗和气候科学等领域，XAI for time series data已经变得非常重要。然而，评估XAI技术提供的解释质量仍然是一个挑战。这篇论文提供了对使用扰动分析评估XAI方法提供的解释的深入分析。扰动分析通过系统地修改输入数据，并评估对XAI方法生成的解释产生的影响。我们对several state-of-the-art XAI技术进行了应用，并对三个时间序列分类 dataset进行了评估。我们的结果表明，扰动分析方法可以有效评估解释质量，并为XAI技术的选择和开发更可靠和可解释的机器学习模型提供了指导。
</details></li>
</ul>
<hr>
<h2 id="PePNet-A-Periodicity-Perceived-Workload-Prediction-Network-Supporting-Rare-Occurrence-of-Heavy-Workload"><a href="#PePNet-A-Periodicity-Perceived-Workload-Prediction-Network-Supporting-Rare-Occurrence-of-Heavy-Workload" class="headerlink" title="PePNet: A Periodicity-Perceived Workload Prediction Network Supporting Rare Occurrence of Heavy Workload"></a>PePNet: A Periodicity-Perceived Workload Prediction Network Supporting Rare Occurrence of Heavy Workload</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01917">http://arxiv.org/abs/2308.01917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feiyi Chen, Zhen Qin, Hailiang Zhao, Mengchu Zhou, Shuiguang Deng</li>
<li>for: 提高云服务器的工作负载预测精度，尤其是高工作负载。</li>
<li>methods: 使用统计方法和神经网络方法，并将两种方法结合使用。</li>
<li>results: 对Alibaba2018、SMD数据集和Dinda的数据集进行了广泛的实验，并显示了与现有方法相比的MAPЭ值下降20.0%，特别是对高工作负载下的MAPЭ值下降23.9%。<details>
<summary>Abstract</summary>
Cloud providers can greatly benefit from accurate workload prediction. However, the workload of cloud servers is highly variable, with occasional heavy workload bursts. This makes workload prediction challenging.   There are mainly two categories of workload prediction methods: statistical methods and neural-network-based ones. The former ones rely on strong mathematical assumptions and have reported low accuracy when predicting highly variable workload. The latter ones offer higher overall accuracy, yet they are vulnerable to data imbalance between heavy workload and common one. This impairs the prediction accuracy of neural network-based models on heavy workload.   Either the overall inaccuracy of statistic methods or the heavy-workload inaccuracy of neural-network-based models can cause service level agreement violations.   Thus, we propose PePNet to improve overall especially heavy workload prediction accuracy. It has two distinctive characteristics:   (i) A Periodicity-Perceived Mechanism to detect the existence of periodicity and the length of one period automatically, without any priori knowledge. Furthermore, it fuses periodic information adaptively, which is suitable for periodic, lax periodic and aperiodic time series.   (ii) An Achilles' Heel Loss Function iteratively optimizing the most under-fitting part in predicting sequence for each step, which significantly improves the prediction accuracy of heavy load.   Extensive experiments conducted on Alibaba2018, SMD dataset and Dinda's dataset demonstrate that PePNet improves MAPE for overall workload by 20.0% on average, compared with state-of-the-art methods. Especially, PePNet improves MAPE for heavy workload by 23.9% on average.
</details>
<details>
<summary>摘要</summary>
云提供商可以受益很大地由于准确的工作负载预测。然而，云服务器的工作负载很变化， occasional 强大的工作负载峰值。这使工作负载预测变得困难。根据文章，有两类主要的工作负载预测方法：统计方法和神经网络基于的方法。前者基于强大的数学假设，报告了低精度when predicting highly variable workload。后者提供更高的总精度，但它们对数据不均衡between heavy workload and common one而言，这会降低神经网络基于模型的预测精度。因此，文章提出了PePNet以提高特别是高负载预测精度。它有两个特点：（i）一种Periodicity-Perceived机制，自动检测工作负载是否存在周期性和一个周期的长度，无需任何先验知识。此外，它适应 periodic, lax periodic 和频繁无法时序列。（ii）一种Achilles' Heel Loss Function， iteratively 优化预测序列中最下降的部分，以提高高负载预测精度。根据文章的实验结果，PePNet在Alibaba2018、SMD dataset 和 Dinda's dataset上提高了MAPEs的平均值，相比之下state-of-the-art方法。尤其是，PePNet在高负载预测方面提高了MAPEs的平均值23.9%。
</details></li>
</ul>
<hr>
<h2 id="Transaction-Fraud-Detection-via-an-Adaptive-Graph-Neural-Network"><a href="#Transaction-Fraud-Detection-via-an-Adaptive-Graph-Neural-Network" class="headerlink" title="Transaction Fraud Detection via an Adaptive Graph Neural Network"></a>Transaction Fraud Detection via an Adaptive Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05633">http://arxiv.org/abs/2307.05633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Tian, Guanjun Liu, Jiacun Wang, Mengchu Zhou</li>
<li>for: 提高交易验证 fraud detection 精度，以保障个人和银行的金融安全。</li>
<li>methods: 提出了 Adaptive Sampling and Aggregation-based Graph Neural Network (ASA-GNN)，可以学习交易数据中的准确表示。</li>
<li>results: 对三个真实的金融数据集进行了广泛的实验，显示了 ASA-GNN 的提出方法在交易验证预测中的优于现有方法。<details>
<summary>Abstract</summary>
Many machine learning methods have been proposed to achieve accurate transaction fraud detection, which is essential to the financial security of individuals and banks. However, most existing methods leverage original features only or require manual feature engineering. They lack the ability to learn discriminative representations from transaction data. Moreover, criminals often commit fraud by imitating cardholders' behaviors, which causes the poor performance of existing detection models. In this paper, we propose an Adaptive Sampling and Aggregation-based Graph Neural Network (ASA-GNN) that learns discriminative representations to improve the performance of transaction fraud detection. A neighbor sampling strategy is performed to filter noisy nodes and supplement information for fraudulent nodes. Specifically, we leverage cosine similarity and edge weights to adaptively select neighbors with similar behavior patterns for target nodes and then find multi-hop neighbors for fraudulent nodes. A neighbor diversity metric is designed by calculating the entropy among neighbors to tackle the camouflage issue of fraudsters and explicitly alleviate the over-smoothing phenomena. Extensive experiments on three real financial datasets demonstrate that the proposed method ASA-GNN outperforms state-of-the-art ones.
</details>
<details>
<summary>摘要</summary>
多种机器学习方法已经被提议用于实现准确的交易诈骗检测，这是个人和银行的金融安全的关键。然而，大多数现有方法只使用原始特征或需要手动工程特征。它们缺乏学习特征表示的能力。此外，诈骗者们常常通过模仿卡户的行为进行诈骗，这导致现有的检测模型表现不佳。在这篇论文中，我们提出了一种适应采样和汇集基于图 neural network（ASA-GNN），用于提高交易诈骗检测的性能。我们使用cosine相似性和边重要性来适应选择target节点周围的相似行为模式的邻居，然后找到多跳邻居 для诈骗节点。我们还设计了邻居多样性度量，通过计算邻居Entropy来解决诈骗者的掩盖问题，并直接缓解过滤问题。我们在三个实际金融dataset上进行了广泛的实验，结果显示，我们提出的方法ASA-GNN在相比之前的方法上表现出优异的表现。
</details></li>
</ul>
<hr>
<h2 id="Estimating-label-quality-and-errors-in-semantic-segmentation-data-via-any-model"><a href="#Estimating-label-quality-and-errors-in-semantic-segmentation-data-via-any-model" class="headerlink" title="Estimating label quality and errors in semantic segmentation data via any model"></a>Estimating label quality and errors in semantic segmentation data via any model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05080">http://arxiv.org/abs/2307.05080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vedang Lad, Jonas Mueller</li>
<li>for: 提高 semantic segmentation 数据集的标注质量，减少人工标注错误。</li>
<li>methods: 使用 probabilistic 预测来评估标注质量，可以使用任何模型架构和训练方法。</li>
<li>results: 通过评估多种标注质量分数方法，发现使用soft-minimum 方法可以最 effectively 标识图像中的错误标注，并且适用于多种类型的标注错误。<details>
<summary>Abstract</summary>
The labor-intensive annotation process of semantic segmentation datasets is often prone to errors, since humans struggle to label every pixel correctly. We study algorithms to automatically detect such annotation errors, in particular methods to score label quality, such that the images with the lowest scores are least likely to be correctly labeled. This helps prioritize what data to review in order to ensure a high-quality training/evaluation dataset, which is critical in sensitive applications such as medical imaging and autonomous vehicles. Widely applicable, our label quality scores rely on probabilistic predictions from a trained segmentation model -- any model architecture and training procedure can be utilized. Here we study 7 different label quality scoring methods used in conjunction with a DeepLabV3+ or a FPN segmentation model to detect annotation errors in a version of the SYNTHIA dataset. Precision-recall evaluations reveal a score -- the soft-minimum of the model-estimated likelihoods of each pixel's annotated class -- that is particularly effective to identify images that are mislabeled, across multiple types of annotation error.
</details>
<details>
<summary>摘要</summary>
人工标注过程中的 semantic segmentation 数据集的劳动密集程度可能会导致错误，因为人们很难将每个像素都正确地标注。我们研究自动检测标注错误的算法，特别是计算标签质量分数，以便根据分数 lowest 优先级顺序对数据进行审核，以确保高质量的训练/评估数据集，这是敏感应用，如医学影像和自动驾驶等。我们的标签质量分数可以任意选择模型结构和训练方法。在这里，我们研究了7种不同的标签质量分数方法，与 DeepLabV3+ 或 FPN  segmentation模型结合使用，检测 SYNTHIA 数据集中的标注错误。精度-回归评估表明，使用模型估计每个像素的类别概率的软最小值是特别有效地标识错批标注， across multiple types of annotation error。
</details></li>
</ul>
<hr>
<h2 id="A-Theory-of-Bounded-Inductive-Rationality"><a href="#A-Theory-of-Bounded-Inductive-Rationality" class="headerlink" title="A Theory of Bounded Inductive Rationality"></a>A Theory of Bounded Inductive Rationality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05068">http://arxiv.org/abs/2307.05068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caspar Oesterheld, Abram Demski, Vincent Conitzer</li>
<li>for: 这篇论文是为了研究不假设完整知识的理性决策理论而写的。</li>
<li>methods: 论文使用了 inductive reasoning 和 infinitely often testing 来定义理性。</li>
<li>results: 论文提出了一种新的理性决策理论，并证明了这种理性决策可以在各种决策问题上达到 Desirable Properties，如值 random and pseudo-random lotteries at their expected reward。此外，论文还证明了在不同代理之间的竞争中， bounded rational inductive agents 可以 converges to certain strategies。<details>
<summary>Abstract</summary>
The dominant theories of rational choice assume logical omniscience. That is, they assume that when facing a decision problem, an agent can perform all relevant computations and determine the truth value of all relevant logical/mathematical claims. This assumption is unrealistic when, for example, we offer bets on remote digits of pi or when an agent faces a computationally intractable planning problem. Furthermore, the assumption of logical omniscience creates contradictions in cases where the environment can contain descriptions of the agent itself. Importantly, strategic interactions as studied in game theory are decision problems in which a rational agent is predicted by its environment (the other players). In this paper, we develop a theory of rational decision making that does not assume logical omniscience. We consider agents who repeatedly face decision problems (including ones like betting on digits of pi or games against other agents). The main contribution of this paper is to provide a sensible theory of rationality for such agents. Roughly, we require that a boundedly rational inductive agent tests each efficiently computable hypothesis infinitely often and follows those hypotheses that keep their promises of high rewards. We then prove that agents that are rational in this sense have other desirable properties. For example, they learn to value random and pseudo-random lotteries at their expected reward. Finally, we consider strategic interactions between different agents and prove a folk theorem for what strategies bounded rational inductive agents can converge to.
</details>
<details>
<summary>摘要</summary>
dominant 理论假设了推理完全性。即在面临决策问题时，一个代理人可以完成所有相关计算并确定所有有关逻辑/数学声明的真值。这个假设是不现实的，例如在购买远程数字π的赌注或面临计算不可解压缩的规划问题时。此外，假设推理完全性会导致环境中描述代理人本身的矛盾。在这篇论文中，我们开发了一种不假设推理完全性的决策理论。我们考虑代理人在面临决策问题（包括购买数字π的赌注或与其他代理人的游戏）时的行为。我们的主要贡献是提供了一种合理的决策理论，即代理人应该对每个可计算的假设进行无限次测试，并且在这些假设保证高奖励时采取这些假设。然后，我们证明了这种合理性具有其他愉悦的属性，例如对随机和 Pseudo-Random 抽签有价值。最后，我们考虑不同代理人之间的战略互动，并证明了这些代理人可以 converges 到的策略。
</details></li>
</ul>
<hr>
<h2 id="Portfolio-Optimization-A-Comparative-Study"><a href="#Portfolio-Optimization-A-Comparative-Study" class="headerlink" title="Portfolio Optimization: A Comparative Study"></a>Portfolio Optimization: A Comparative Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05048">http://arxiv.org/abs/2307.05048</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/riddhi927/Portfolio-Optimization">https://github.com/riddhi927/Portfolio-Optimization</a></li>
<li>paper_authors: Jaydip Sen, Subhasis Dasgupta</li>
<li>for: 这篇论文主要研究了股票组合设计方法的比较，包括mean-variance portfolio（MVP）、 hierarchical risk parity（HRP）基于的股票组合和自适应神经网络（Autoencoder）基于的股票组合。</li>
<li>methods: 这篇论文使用了历史股票价格数据，从国家证券交易所（NSE）的十个主题领域选择了股票。使用了股票价格数据从2018年1月1日至2022年12月31日，并对这些股票组合进行了测试。</li>
<li>results: 研究发现，MVP股票组合在对数据上的风险考虑返回最好。然而，Autoencoder股票组合在一年 Returns 方面表现更好。<details>
<summary>Abstract</summary>
Portfolio optimization has been an area that has attracted considerable attention from the financial research community. Designing a profitable portfolio is a challenging task involving precise forecasting of future stock returns and risks. This chapter presents a comparative study of three portfolio design approaches, the mean-variance portfolio (MVP), hierarchical risk parity (HRP)-based portfolio, and autoencoder-based portfolio. These three approaches to portfolio design are applied to the historical prices of stocks chosen from ten thematic sectors listed on the National Stock Exchange (NSE) of India. The portfolios are designed using the stock price data from January 1, 2018, to December 31, 2021, and their performances are tested on the out-of-sample data from January 1, 2022, to December 31, 2022. Extensive results are analyzed on the performance of the portfolios. It is observed that the performance of the MVP portfolio is the best on the out-of-sample data for the risk-adjusted returns. However, the autoencoder portfolios outperformed their counterparts on annual returns.
</details>
<details>
<summary>摘要</summary>
股票股票组合优化已经吸引了金融研究社区的广泛关注。设计一个有利可图的股票组合是一项复杂的任务，涉及精准预测未来股票收益和风险。这章介绍了三种股票组合设计方法：均值风险股票组合（MVP）、层次风险平衡基于股票组合（HRP）以及自动编码基于股票组合。这三种股票组合设计方法在印度国家证券交易所（NSE）上市的10个主题领域上的股票历史价格数据上进行了应用。这些股票组合使用的价格数据是从2018年1月1日至2021年12月31日，并在这些数据上测试了其性能。结果显示，MVP股票组合在审核数据上的风险衡量回报最佳。然而，自动编码股票组合在年度收益上超过了其对应的股票组合。
</details></li>
</ul>
<hr>
<h2 id="DNAGPT-A-Generalized-Pre-trained-Tool-for-Versatile-DNA-Sequence-Analysis-Tasks"><a href="#DNAGPT-A-Generalized-Pre-trained-Tool-for-Versatile-DNA-Sequence-Analysis-Tasks" class="headerlink" title="DNAGPT: A Generalized Pre-trained Tool for Versatile DNA Sequence Analysis Tasks"></a>DNAGPT: A Generalized Pre-trained Tool for Versatile DNA Sequence Analysis Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05628">http://arxiv.org/abs/2307.05628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daoan Zhang, Weitong Zhang, Bing He, Yu Zhao, Jianguo Zhang, Chenchen Qin, Jianhua Yao<br>for:DNAGPT is proposed to handle various DNA analysis tasks, including genomic signals and regions recognition, pseudo genomes generation, and mRNA abundance regression.methods:DNAGPT uses a pre-trained model with a modified architecture that includes binary classification and numerical regression tasks, as well as a comprehensive token language to encode sequence, number, and task-related information.results:DNAGPT demonstrates superior performance on various DNA analysis tasks, especially when compared to existing models specialized for specific downstream tasks.<details>
<summary>Abstract</summary>
GPT has been proven to be capable of extracting general information from language sequences, thereby benefiting all downstream tasks. This motivates us to use pre-trained models to explore the hidden inherent information in DNA sequences. However, data and task requirements in DNA sequence analyses are tasked in different formats such as generation, prediction and regression, and are complexity and involve different modalities, such as nucleotides sequences and, expression levels, etc. Existing BERT-based models are mostly for generation tasks and use sequence data as input and output, thus cannot easily handle various DNA analysis tasks in one single model. Herein, we propose a generalized DNA pre-training DNA model, DNAGPT, that was trained on over 200 billion base pairs from all the mammals. We enhance the classic GPT model by adding binary classification task (DNA sequence order) and numerical regression task (guanine-cytosine content prediction) in the pre-training period and enhancing the architecture with corresponding embedding layers and encoding heads. We also design a comprehensive token language to encode sequence, number and task related information in the same token space. Therefore, DNAGPT can handle versatile DNA analysis tasks and simultaneously process handle both sequence and numerical data. We have evaluated our model on genomic signals and regions recognition, pseudo genomes generation and mRNA abudance regression tasks. We demonstrate that benefiting from pre-training, DNAGPT can shows superior performance than the existing models specially designed for various downstreams tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Number-Systems-for-Deep-Neural-Network-Architectures-A-Survey"><a href="#Number-Systems-for-Deep-Neural-Network-Architectures-A-Survey" class="headerlink" title="Number Systems for Deep Neural Network Architectures: A Survey"></a>Number Systems for Deep Neural Network Architectures: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05035">http://arxiv.org/abs/2307.05035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ghada Alsuhli, Vasileios Sakellariou, Hani Saleh, Mahmoud Al-Qutayri, Baker Mohammad, Thanos Stouraitis</li>
<li>for: 本文主要探讨了深度神经网络（DNNs）中的数据表示方法，以提高DNNs的计算效率和能效性。</li>
<li>methods: 本文提出了多种非标准数学系统，包括循环乘法、基于扩展的Gaussian数学系统、基于循环的数学系统等，以优化DNNs的表示方法。</li>
<li>results: 本文结果表明，使用非标准数学系统可以提高DNNs的计算效率和能效性，同时也可以降低硬件设计的复杂性。但是，每种数学系统都有其缺点和挑战，需要进一步的研究和优化。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have become an enabling component for a myriad of artificial intelligence applications. DNNs have shown sometimes superior performance, even compared to humans, in cases such as self-driving, health applications, etc. Because of their computational complexity, deploying DNNs in resource-constrained devices still faces many challenges related to computing complexity, energy efficiency, latency, and cost. To this end, several research directions are being pursued by both academia and industry to accelerate and efficiently implement DNNs. One important direction is determining the appropriate data representation for the massive amount of data involved in DNN processing. Using conventional number systems has been found to be sub-optimal for DNNs. Alternatively, a great body of research focuses on exploring suitable number systems. This article aims to provide a comprehensive survey and discussion about alternative number systems for more efficient representations of DNN data. Various number systems (conventional/unconventional) exploited for DNNs are discussed. The impact of these number systems on the performance and hardware design of DNNs is considered. In addition, this paper highlights the challenges associated with each number system and various solutions that are proposed for addressing them. The reader will be able to understand the importance of an efficient number system for DNN, learn about the widely used number systems for DNN, understand the trade-offs between various number systems, and consider various design aspects that affect the impact of number systems on DNN performance. In addition, the recent trends and related research opportunities will be highlighted
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FairLay-ML-Intuitive-Remedies-for-Unfairness-in-Data-Driven-Social-Critical-Algorithms"><a href="#FairLay-ML-Intuitive-Remedies-for-Unfairness-in-Data-Driven-Social-Critical-Algorithms" class="headerlink" title="FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms"></a>FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05029">http://arxiv.org/abs/2307.05029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Normen Yu, Gang Tan, Saeid Tizpaz-Niari</li>
<li>For: This paper aims to provide a user-friendly interface for laypeople to understand and remedy unfairness in machine learning models.* Methods: The paper uses open-sourced machine learning model explanation tools, such as Local Interpretable Model-Agnostic Explanations (LIME), and integrates them with existing machine learning-focused graphical user interfaces (GUIs) like Python Streamlit.* Results: The paper tests the effectiveness of FairLay-ML, a proof-of-concept GUI, using models of various accuracy and fairness generated by an unfairness detector tool, Parfait-ML, and validates the results using Themis. The study finds that the technology stack used for FairLay-ML is easy to install and provides real-time black-box explanations of pre-trained models to users, and the explanations provided translate to actionable remedies.Here is the information in Simplified Chinese text:* For: 这项研究旨在提供一个用户友好的界面，使普通人能够理解和修复机器学习模型中的不公正性。* Methods: 这篇论文使用开源的机器学习模型解释工具，如Local Interpretable Model-Agnostic Explanations（LIME），并将其与现有的机器学习专注的图形用户界面（GUI）如Python Streamlit集成。* Results: 这篇论文测试了 FairLay-ML，一个证明性的 GUI，使用 Parfait-ML 生成的不公正度探测器生成的模型，并使用 Themis 验证结果。研究发现，FairLay-ML 使用的技术栈易于安装，可以在实时提供黑盒解释，并且解释提供了实际的纠正措施。<details>
<summary>Abstract</summary>
This thesis explores open-sourced machine learning (ML) model explanation tools to understand whether these tools can allow a layman to visualize, understand, and suggest intuitive remedies to unfairness in ML-based decision-support systems. Machine learning models trained on datasets biased against minority groups are increasingly used to guide life-altering social decisions, prompting the urgent need to study their logic for unfairness. Due to this problem's impact on vast populations of the general public, it is critical for the layperson -- not just subject matter experts in social justice or machine learning experts -- to understand the nature of unfairness within these algorithms and the potential trade-offs. Existing research on fairness in machine learning focuses mostly on the mathematical definitions and tools to understand and remedy unfair models, with some directly citing user-interactive tools as necessary for future work. This thesis presents FairLay-ML, a proof-of-concept GUI integrating some of the most promising tools to provide intuitive explanations for unfair logic in ML models by integrating existing research tools (e.g. Local Interpretable Model-Agnostic Explanations) with existing ML-focused GUI (e.g. Python Streamlit). We test FairLay-ML using models of various accuracy and fairness generated by an unfairness detector tool, Parfait-ML, and validate our results using Themis. Our study finds that the technology stack used for FairLay-ML makes it easy to install and provides real-time black-box explanations of pre-trained models to users. Furthermore, the explanations provided translate to actionable remedies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unleashing-the-Potential-of-Regularization-Strategies-in-Learning-with-Noisy-Labels"><a href="#Unleashing-the-Potential-of-Regularization-Strategies-in-Learning-with-Noisy-Labels" class="headerlink" title="Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels"></a>Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05025">http://arxiv.org/abs/2307.05025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Kang, Sheng Liu, Huaxi Huang, Jun Yu, Bo Han, Dadong Wang, Tongliang Liu</li>
<li>for: 本研究旨在探讨学习含杂标签时的稳定性和泛化性问题，并提出简单的基线方法来解决这个问题。</li>
<li>methods: 本研究使用了混合抑制技术，包括学习率衰退、模型权重平均、数据增强等方法来提高模型的稳定性和泛化性。</li>
<li>results: 研究发现，使用简单的基线方法可以超越当前的状态艺术方法，并且这些方法可以更好地发挥其潜力。这些结果 suggessts that employing a combination of regularization strategies can be more effective than intricate algorithms in tackling the challenges of learning with noisy labels.<details>
<summary>Abstract</summary>
In recent years, research on learning with noisy labels has focused on devising novel algorithms that can achieve robustness to noisy training labels while generalizing to clean data. These algorithms often incorporate sophisticated techniques, such as noise modeling, label correction, and co-training. In this study, we demonstrate that a simple baseline using cross-entropy loss, combined with widely used regularization strategies like learning rate decay, model weights average, and data augmentations, can outperform state-of-the-art methods. Our findings suggest that employing a combination of regularization strategies can be more effective than intricate algorithms in tackling the challenges of learning with noisy labels. While some of these regularization strategies have been utilized in previous noisy label learning research, their full potential has not been thoroughly explored. Our results encourage a reevaluation of benchmarks for learning with noisy labels and prompt reconsideration of the role of specialized learning algorithms designed for training with noisy labels.
</details>
<details>
<summary>摘要</summary>
在近年来，学习含杂标签的研究主要关注开发 novel 算法，以实现对含杂训练标签的Robustness 性，同时能够泛化到干净数据上。这些算法通常包括复杂的技术，如噪声模型、标签修正和合作学习。在这项研究中，我们发现，使用 Cross-Entropy 损失函数，并与通用的规则化策略（如学习率减少、模型权重平均和数据扩展）结合使用，可以超越当前的state-of-the-art 方法。我们的发现表明，结合规则化策略可以更有效地解决含杂标签学习中的挑战。虽然一些这些规则化策略在过去的含杂标签学习研究中已经被利用，但它们的潜力尚未得到了全面的探索。我们的结果鼓励我们重新评估含杂标签学习的标准准则，并重新考虑特殊的含杂标签学习算法。
</details></li>
</ul>
<hr>
<h2 id="Feature-Activation-Map-Visual-Explanation-of-Deep-Learning-Models-for-Image-Classification"><a href="#Feature-Activation-Map-Visual-Explanation-of-Deep-Learning-Models-for-Image-Classification" class="headerlink" title="Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification"></a>Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05017">http://arxiv.org/abs/2307.05017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Liao, Yongsheng Gao, Weichuan Zhang</li>
<li>for: 这 paper 的目的是为了解释 deep learning 模型不含全连接层的分类器的决策。</li>
<li>methods: 该 paper 提出了一种后处解释工具 named feature activation map (FAM)，可以用于解释不含全连接层的 deep learning 模型。 FAM 算法通过计算图像嵌入的相似度分布来 derive 通道 wise 贡献权重，然后将 activation map 与相应的正规化贡献权重进行线性组合，形成解释图。</li>
<li>results: 在十种 deep learning 模型上，包括几种 few-shot 图像分类、对比学习图像分类和图像检索任务，Quantitative 和 Qualitative 实验结果表明 FAM 算法的有效性。<details>
<summary>Abstract</summary>
Decisions made by convolutional neural networks(CNN) can be understood and explained by visualizing discriminative regions on images. To this end, Class Activation Map (CAM) based methods were proposed as powerful interpretation tools, making the prediction of deep learning models more explainable, transparent, and trustworthy. However, all the CAM-based methods (e.g., CAM, Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with fully-connected (FC) layers as a classifier. It is worth noting that many deep learning models classify images without FC layers, e.g., few-shot learning image classification, contrastive learning image classification, and image retrieval tasks. In this work, a post-hoc interpretation tool named feature activation map (FAM) is proposed, which can interpret deep learning models without FC layers as a classifier. In the proposed FAM algorithm, the channel-wise contribution weights are derived from the similarity scores between two image embeddings. The activation maps are linearly combined with the corresponding normalized contribution weights, forming the explanation map for visualization. The quantitative and qualitative experiments conducted on ten deep learning models for few-shot image classification, contrastive learning image classification and image retrieval tasks demonstrate the effectiveness of the proposed FAM algorithm.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN文本：深度学习模型的决策可以通过图像视觉化的方式来理解和解释。为此，基于图像活动映射（CAM）的方法被提出，使得深度学习模型的预测变得更加可解、透明和信任worthy。然而，所有的CAM基于方法（如CAM、Grad-CAM和Relevance-CAM）都只适用于深度学习模型中的完全连接（FC）层作为分类器。它们无法用于解释不含FC层的深度学习模型，例如几拍学习图像分类、对比学习图像分类和图像检索任务。在这种情况下，一种后期解释工具named feature activation map（FAM）被提出，可以解释不含FC层的深度学习模型。在提出的FAM算法中，通过两个图像嵌入的相似度分布来 derivate通道 wise的贡献重量。然后，将活动地图与相应的 норmal化贡献重量进行线性组合，形成解释地图 для视觉化。对于几拍学习图像分类、对比学习图像分类和图像检索任务中的十个深度学习模型，我们进行了量化和质量测试，结果表明FAM算法的效果。Translation:<<SYS>>translate_language: zh-CN文本：深度学习模型的决策可以通过图像视觉化的方式来理解和解释。为此，基于图像活动映射（CAM）的方法被提出，使得深度学习模型的预测变得更加可解、透明和信任worthy。然而，所有的CAM基于方法（如CAM、Grad-CAM和Relevance-CAM）都只适用于深度学习模型中的完全连接（FC）层作为分类器。它们无法用于解释不含FC层的深度学习模型，例如几拍学习图像分类、对比学习图像分类和图像检索任务。在这种情况下，一种后期解释工具named feature activation map（FAM）被提出，可以解释不含FC层的深度学习模型。在提出的FAM算法中，通过两个图像嵌入的相似度分布来 derivate通道 wise的贡献重量。然后，将活动地图与相应的 норmal化贡献重量进行线性组合，形成解释地图 для视觉化。对于几拍学习图像分类、对比学习图像分类和图像检索任务中的十个深度学习模型，我们进行了量化和质量测试，结果表明FAM算法的效果。
</details></li>
</ul>
<hr>
<h2 id="CILF-Causality-Inspired-Learning-Framework-for-Out-of-Distribution-Vehicle-Trajectory-Prediction"><a href="#CILF-Causality-Inspired-Learning-Framework-for-Out-of-Distribution-Vehicle-Trajectory-Prediction" class="headerlink" title="CILF:Causality Inspired Learning Framework for Out-of-Distribution Vehicle Trajectory Prediction"></a>CILF:Causality Inspired Learning Framework for Out-of-Distribution Vehicle Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05624">http://arxiv.org/abs/2307.05624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengyi Li, Qifan Xue, Yezhuo Zhang, Xuanpeng Li</li>
<li>for: 提高自动驾驶车辆的路径预测精度</li>
<li>methods: 提出了一种基于 causal graph 的 Out-of-Distribution Causal Graph (OOD-CG) 方法，并提出了一种基于这个 causal graph 的 Causal Inspired Learning Framework (CILF)</li>
<li>results: 在 NGSIM 和 INTERACTION 两个主流数据集上，CILF 实现了提高域间泛化性的表现<details>
<summary>Abstract</summary>
Trajectory prediction is critical for autonomous driving vehicles. Most existing methods tend to model the correlation between history trajectory (input) and future trajectory (output). Since correlation is just a superficial description of reality, these methods rely heavily on the i.i.d. assumption and evince a heightened susceptibility to out-of-distribution data. To address this problem, we propose an Out-of- Distribution Causal Graph (OOD-CG), which explicitly defines the underlying causal structure of the data with three entangled latent features: 1) domain-invariant causal feature (IC), 2) domain-variant causal feature (VC), and 3) domain-variant non-causal feature (VN ). While these features are confounded by confounder (C) and domain selector (D). To leverage causal features for prediction, we propose a Causal Inspired Learning Framework (CILF), which includes three steps: 1) extracting domain-invariant causal feature by means of an invariance loss, 2) extracting domain variant feature by domain contrastive learning, and 3) separating domain-variant causal and non-causal feature by encouraging causal sufficiency. We evaluate the performance of CILF in different vehicle trajectory prediction models on the mainstream datasets NGSIM and INTERACTION. Experiments show promising improvements in CILF on domain generalization.
</details>
<details>
<summary>摘要</summary>
几何预测是自动驾驶车辆中的关键技术。大多数现有方法都是根据历史轨迹（输入）和未来轨迹（输出）之间的相互相关性模型。由于相互相关性只是现象的表面描述，这些方法对于非常用数据的敏感性较高。为了解决这个问题，我们提出了一个 OUT-OF-DISTRIBUTION  causal graph（OOD-CG），它明确地定义了数据的底层 causal 结构，包括三个涉及的隐藏特征：1）域对称 causal 特征（IC），2）域特有 causal 特征（VC），和3）域特有 non-causal 特征（VN）。这些特征受到干扰因子（C）和域选择器（D）的混合影响。为了利用 causal 特征进行预测，我们提出了一个 causal 灵感学习框架（CILF），包括三个步骤：1）通过不对称损失提取域对称 causal 特征，2）通过域区别学习提取域特有 causal 特征，和3）通过鼓励 causal 充分性来分离域特有 causal 和 non-causal 特征。我们在主流的 NGSIM 和 INTERACTION 等数据集上评估了 CILF 的表现，实验结果显示了 CILF 在域泛化中的明显改进。
</details></li>
</ul>
<hr>
<h2 id="Test-Time-Training-on-Video-Streams"><a href="#Test-Time-Training-on-Video-Streams" class="headerlink" title="Test-Time Training on Video Streams"></a>Test-Time Training on Video Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05014">http://arxiv.org/abs/2307.05014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Renhao Wang, Yu Sun, Yossi Gandelsman, Xinlei Chen, Alexei A. Efros, Xiaolong Wang</li>
<li>for: 这个论文是为了提高在测试时使用已经训练好的模型的性能而设计的。</li>
<li>methods: 这个论文使用了在测试时使用自我监督任务，如图像重建使用压缩 autoencoders，来进行模型进一步改进。</li>
<li>results: 这个论文在三个实际 datasets 上对四个任务进行了实验，并取得了45% 和 66% 的相对提升。<details>
<summary>Abstract</summary>
Prior work has established test-time training (TTT) as a general framework to further improve a trained model at test time. Before making a prediction on each test instance, the model is trained on the same instance using a self-supervised task, such as image reconstruction with masked autoencoders. We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order. Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before. Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets. The relative improvement is 45% and 66% for instance and panoptic segmentation. Surprisingly, online TTT also outperforms its offline variant that accesses more information, training on all frames from the entire test video regardless of temporal order. This differs from previous findings using synthetic videos. We conceptualize locality as the advantage of online over offline TTT. We analyze the role of locality with ablations and a theory based on bias-variance trade-off.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:先前的研究已经确立了测试时训练（TTT）为一种通用的框架，以进一步改进已经训练的模型。在测试时，模型会被训练在每个测试实例上，使用自我指导任务，如图像重建 WITH 掩码 autoencoders。我们将 TTT 扩展到流处理设置，其中多个测试实例（视频帧）会在时间顺序下 arrive。我们的扩展是在线 TTT：当前模型将从前一个模型 initialized，然后在当前帧和当前时间范围内的一小Window of frames上进行训练。在线 TTT 与固定模型基eline 相比，显著提高了四个任务的性能，分别是实例和杂点分割。奇怪的是，在线 TTT 还超过了它的离线变体，即训练所有测试视频帧的整体信息。这与以往使用 sintetic videos 的发现不同。我们认为本地性是在线 TTT 的优势。我们通过减少和基准范变换来分析本地性的作用。
</details></li>
</ul>
<hr>
<h2 id="A-DeepLearning-Framework-for-Dynamic-Estimation-of-Origin-Destination-Sequence"><a href="#A-DeepLearning-Framework-for-Dynamic-Estimation-of-Origin-Destination-Sequence" class="headerlink" title="A DeepLearning Framework for Dynamic Estimation of Origin-Destination Sequence"></a>A DeepLearning Framework for Dynamic Estimation of Origin-Destination Sequence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05623">http://arxiv.org/abs/2307.05623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheli Xiong, Defu Lian, Enhong Chen, Gang Chen, Xiaomin Cheng<br>for: 本研究旨在解决交通领域中OD矩阵估算中的主要问题，即使用交通传感器测量信息来估算交通需求表示的OD矩阵。methods: 本研究提议一种集成方法，利用深度学习方法来推断OD序列的结构，并使用结构约束导引传统的数值优化。results: 实验表明，神经网络可以有效地推断OD序列的结构，并提供实用的约束 для数值优化以获得更好的结果。此外，实验还表明，提供的结构信息不仅包含OD矩阵的空间结构约束，还包含时间结构约束，可以有效解决延迟问题。<details>
<summary>Abstract</summary>
OD matrix estimation is a critical problem in the transportation domain. The principle method uses the traffic sensor measured information such as traffic counts to estimate the traffic demand represented by the OD matrix. The problem is divided into two categories: static OD matrix estimation and dynamic OD matrices sequence(OD sequence for short) estimation. The above two face the underdetermination problem caused by abundant estimated parameters and insufficient constraint information. In addition, OD sequence estimation also faces the lag challenge: due to different traffic conditions such as congestion, identical vehicle will appear on different road sections during the same observation period, resulting in identical OD demands correspond to different trips. To this end, this paper proposes an integrated method, which uses deep learning methods to infer the structure of OD sequence and uses structural constraints to guide traditional numerical optimization. Our experiments show that the neural network(NN) can effectively infer the structure of the OD sequence and provide practical constraints for numerical optimization to obtain better results. Moreover, the experiments show that provided structural information contains not only constraints on the spatial structure of OD matrices but also provides constraints on the temporal structure of OD sequence, which solve the effect of the lagging problem well.
</details>
<details>
<summary>摘要</summary>
OD矩阵估计是交通领域中的关键问题。主要方法使用交通仪器测量信息，如交通统计数据，来估计交通需求表示的OD矩阵。问题分为两类：静态OD矩阵估计和动态OD序列（简称OD序列）估计。两者都面临了不充分约束的问题，导致估计过多的参数。此外，OD序列估计还面临着延迟问题：由于不同的交通情况，如拥堵，同一段道路上的同一辆车辆在同一个观察时期出现，导致同一个OD需求对应不同的旅行。为解决这些问题，本文提出了一种集成方法，使用深度学习方法来推断OD序列的结构，并使用结构约束来导引传统的数值优化。我们的实验表明，神经网络（NN）可以有效地推断OD序列的结构，并为数值优化提供实用的约束。此外，实验还表明，提供的结构信息不仅包含OD矩阵的空间结构约束，还提供了OD序列的时间结构约束，这有效解决了延迟问题。
</details></li>
</ul>
<hr>
<h2 id="Improving-RNN-Transducers-with-Acoustic-LookAhead"><a href="#Improving-RNN-Transducers-with-Acoustic-LookAhead" class="headerlink" title="Improving RNN-Transducers with Acoustic LookAhead"></a>Improving RNN-Transducers with Acoustic LookAhead</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05006">http://arxiv.org/abs/2307.05006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinit S. Unni, Ashish Mittal, Preethi Jyothi, Sunita Sarawagi</li>
<li>for: 这篇论文是为了提高语音转文字的精度和流动性而写的。</li>
<li>methods: 这篇论文使用了RNN-T模型，并提出了一个名为LookAhead的技术来让文本表现更加声音背景测量。</li>
<li>results: 这篇论文的实验结果显示，使用LookAhead技术可以导致文本误差率降低5%-20%，包括在域内和域外评估集上。<details>
<summary>Abstract</summary>
RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end model for speech to text conversion because of their high accuracy and streaming capabilities. A typical RNN-T independently encodes the input audio and the text context, and combines the two encodings by a thin joint network. While this architecture provides SOTA streaming accuracy, it also makes the model vulnerable to strong LM biasing which manifests as multi-step hallucination of text without acoustic evidence. In this paper we propose LookAhead that makes text representations more acoustically grounded by looking ahead into the future within the audio input. This technique yields a significant 5%-20% relative reduction in word error rate on both in-domain and out-of-domain evaluation sets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Latent-Space-Perspicacity-and-Interpretation-Enhancement-LS-PIE-Framework"><a href="#Latent-Space-Perspicacity-and-Interpretation-Enhancement-LS-PIE-Framework" class="headerlink" title="Latent Space Perspicacity and Interpretation Enhancement (LS-PIE) Framework"></a>Latent Space Perspicacity and Interpretation Enhancement (LS-PIE) Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05620">http://arxiv.org/abs/2307.05620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jesse Stevens, Daniel N. Wilke, Itumeleng Setshedi</li>
<li>for: 这个论文的目的是提高线性隐 Variable 模型中的隐空间表示，以提高这些模型的可解释性。</li>
<li>methods: 这个论文提出了一个通用框架，用于自动对隐向量进行归类、缩放和排序，以提高每个隐向量的信息含量。这个框架还包括单通道和多通道数据源、数据预处理策略和特定 метри来自动确定隐向量的归类数量。</li>
<li>results: 在两个基础问题上，这个框架的效果被证明了，包括LR、LS和LCON等功能。这些功能可以帮助提高线性隐 Variable 模型的可解释性和应用范围。<details>
<summary>Abstract</summary>
Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). For example, PCA solvers usually rank the principal directions by explaining the most to least variance, while ICA solvers usually return independent directions unordered and often with single sources spread across multiple directions as multiple sub-sources, which is of severe detriment to their usability and interpretability.   This paper proposes a general framework to enhance latent space representations for improving the interpretability of linear latent spaces. Although the concepts in this paper are language agnostic, the framework is written in Python. This framework automates the clustering and ranking of latent vectors to enhance the latent information per latent vector, as well as, the interpretation of latent vectors. Several innovative enhancements are incorporated including latent ranking (LR), latent scaling (LS), latent clustering (LC), and latent condensing (LCON).   For a specified linear latent variable model, LR ranks latent directions according to a specified metric, LS scales latent directions according to a specified metric, LC automatically clusters latent directions into a specified number of clusters, while, LCON automatically determines an appropriate number of clusters into which to condense the latent directions for a given metric. Additional functionality of the framework includes single-channel and multi-channel data sources, data preprocessing strategies such as Hankelisation to seamlessly expand the applicability of linear latent variable models (LLVMs) to a wider variety of data.   The effectiveness of LR, LS, and LCON are showcased on two crafted foundational problems with two applied latent variable models, namely, PCA and ICA.
</details>
<details>
<summary>摘要</summary>
Linear 隐变量模型，如主成分分析（PCA）、独立成分分析（ICA）、共谱分析（CCA）和因素分析（FA），可以找到隐向量（或负荷）是有序还是无序的。然后将数据Project onto these latent directions to obtain their projected representations（或分数）。例如，PCA 解决器通常会根据解释最多变量来排序主方向，而 ICA 解决器通常会返回独立的方向，无序，经常有多个来源分散在多个方向中，这会影响其可用性和可读性。这篇文章提出了一种通用框架，用于提高线性隐空间表示的可解释性。尽管这些概念是语言无关的，但框架是写在Python语言中。这个框架可以自动将隐向量集中到减少隐信息的latent vector，以及提高隐向量的解释性。框架包含了多种创新的改进，包括隐向量排名（LR）、隐向量缩放（LS）、隐向量划分（LC）和隐向量压缩（LCON）。对于指定的线性隐变量模型，LR 可以根据指定的度量将隐方向排名，LS 可以根据指定的度量缩放隐方向，LC 可以自动将隐方向分为指定数量的集中，而 LCON 可以自动确定隐方向压缩到指定度量的最佳数量。框架还包括单通道和多通道数据源，以及数据预处理策略，例如束腾化来扩展线性隐变量模型（LLVMs）的应用范围。LR、LS和LCON 的效果在两个基本问题上进行了示例，这两个问题分别使用 PCA 和 ICA 作为应用隐变量模型。
</details></li>
</ul>
<hr>
<h2 id="Control-as-Probabilistic-Inference-as-an-Emergent-Communication-Mechanism-in-Multi-Agent-Reinforcement-Learning"><a href="#Control-as-Probabilistic-Inference-as-an-Emergent-Communication-Mechanism-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Control as Probabilistic Inference as an Emergent Communication Mechanism in Multi-Agent Reinforcement Learning"></a>Control as Probabilistic Inference as an Emergent Communication Mechanism in Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05004">http://arxiv.org/abs/2307.05004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomoaki Nakamura, Akira Taniguchi, Tadahiro Taniguchi</li>
<li>for: 这种论文旨在提出一种生成概率模型，整合emergent communication和多个代理人强化学习。</li>
<li>methods: 该模型使用概率推理进行控制，并通过隐藏变量和估计来实现代理人之间的交流。</li>
<li>results: 通过在网格环境中的实验，我们表明了该PGM可以推理出有意义的消息，以完成合作任务。<details>
<summary>Abstract</summary>
This paper proposes a generative probabilistic model integrating emergent communication and multi-agent reinforcement learning. The agents plan their actions by probabilistic inference, called control as inference, and communicate using messages that are latent variables and estimated based on the planned actions. Through these messages, each agent can send information about its actions and know information about the actions of another agent. Therefore, the agents change their actions according to the estimated messages to achieve cooperative tasks. This inference of messages can be considered as communication, and this procedure can be formulated by the Metropolis-Hasting naming game. Through experiments in the grid world environment, we show that the proposed PGM can infer meaningful messages to achieve the cooperative task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Selective-Sampling-and-Imitation-Learning-via-Online-Regression"><a href="#Selective-Sampling-and-Imitation-Learning-via-Online-Regression" class="headerlink" title="Selective Sampling and Imitation Learning via Online Regression"></a>Selective Sampling and Imitation Learning via Online Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04998">http://arxiv.org/abs/2307.04998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Sekhari, Karthik Sridharan, Wen Sun, Runzhe Wu</li>
<li>for: 本文提出了一种解决Imitation Learning（IL）问题的交互算法，使得在受到噪声专家反馈的情况下，IL可以更加成功。</li>
<li>methods: 本文使用选择样本算法，通过咨询噪声专家来获得反馈，以提高IL的性能。</li>
<li>results: 本文提供了一种新的选择样本算法，可以在涉及到多个动作和通用函数类型的情况下实现IL。这个算法的 regret bound和查询次数都与在线回归 oracle 相关，并且可以与噪声专家进行有限次的交互。<details>
<summary>Abstract</summary>
We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback. While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications. In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful. In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback. Our contributions are twofold: First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries. Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries.   Our algorithm for selective sampling leverages function approximation, and relies on an online regression oracle w.r.t.~the given model class to predict actions, and to decide whether to query the expert for its label. On the theoretical side, the regret bound of our algorithm is upper bounded by the regret of the online regression oracle, while the query complexity additionally depends on the eluder dimension of the model class. We complement this with a lower bound that demonstrates that our results are tight. We extend our selective sampling algorithm for IL with general function approximation and provide bounds on both the regret and the number of queries made to the noisy expert. A key novelty here is that our regret and query complexity bounds only depend on the number of times the optimal policy (and not the noisy expert, or the learner) go to states that have a small margin.
</details>
<details>
<summary>摘要</summary>
我们考虑伪模仿学习（IL）问题，通过活动地发送受惊访问来获得不精确的专家反馈。过往的大部分研究假设可以得到不受扰动的专家反馈，但这在许多应用中不实际。事实上，只有可以获得不精确的专家反馈时，基于专家反馈的非互动式IL（非互动式学习）的算法可以证明需要极大的样本数量才能成功。相比之下，在这个研究中，我们提出了一个互动式IL算法，使用选择性样本来活动地发送受惊访问。我们的贡献包括：首先，我们提出了一个新的选择性样本算法，适用于一般函数类别和多个动作。我们获得了最好的known bounds的 regret和询问数量。其次，我们将这一分析扩展到伪模仿学习问题中，提出了一个新的IL算法，使用选择性样本来获得有限的询问数量。我们的算法利用函数近似，并且透过在line上的 regression oracle 来预测动作，并决定是否需要受惊访问专家。从理论上看，我们的算法的 regret bound是由online regression oracle的 regret bound所 upper bounded，而且询问 complexity 还dependent于模型类别的eluder dimension。我们补充了一个下界，证明我们的结果是紧缩的。 finally，我们扩展了我们的选择性样本算法，提供了具有 regret和询问数量 bounds的IL算法，适用于一般函数类别和多个动作。这个新的特点是，我们的 regret和询问 complexity bounds仅dependent于Optimal policy（而不是专家、学习者）在状态空间中的小margin次数。
</details></li>
</ul>
<hr>
<h2 id="Empowering-recommender-systems-using-automatically-generated-Knowledge-Graphs-and-Reinforcement-Learning"><a href="#Empowering-recommender-systems-using-automatically-generated-Knowledge-Graphs-and-Reinforcement-Learning" class="headerlink" title="Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning"></a>Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04996">http://arxiv.org/abs/2307.04996</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GhanshyamVerma/Explainable-Recommender-System">https://github.com/GhanshyamVerma/Explainable-Recommender-System</a></li>
<li>paper_authors: Ghanshyam Verma, Shovon Sengupta, Simon Simanta, Huan Chen, Janos A. Perge, Devishree Pillai, John P. McCrae, Paul Buitelaar</li>
<li>For: This paper focuses on developing interpretable knowledge graph-based recommender systems for personalized article recommendations in financial services.* Methods: The authors propose two approaches: one using Reinforcement Learning and the other using XGBoost, both of which leverage a knowledge graph generated from structured and unstructured data. The Reinforcement Learning-based approach utilizes graph traversal paths to provide interpretations, while the XGBoost-based approach uses post-hoc methods like SHAP and ELI5 to provide explainable results.* Results: The approach offers explainable results, promoting better decision-making, and demonstrates the potential of combining advanced machine learning techniques with KG-driven insights for enhancing customer experience in relationship management.Here’s the Chinese version:* For: 这篇论文关注开发可解释知识图基于文章个性化推荐在金融服务中。* Methods: 作者提出了两种方法：一种使用奖励学习，另一种使用XGBoost，两者都利用了基于结构化和无结构化数据生成的知识图。奖励学习基本方法使用图 traversal 路径来提供解释，而 XGBoost 基本方法使用 SHAP 和 ELI5 等后置方法提供解释结果。* Results: 方法提供了可解释结果，促进更好的决策，并证明了结合先进机器学习技术和知识图驱动的想法可以增强客户关系管理的体验。<details>
<summary>Abstract</summary>
Personalized recommendations have a growing importance in direct marketing, which motivates research to enhance customer experiences by knowledge graph (KG) applications. For example, in financial services, companies may benefit from providing relevant financial articles to their customers to cultivate relationships, foster client engagement and promote informed financial decisions. While several approaches center on KG-based recommender systems for improved content, in this study we focus on interpretable KG-based recommender systems for decision making.To this end, we present two knowledge graph-based approaches for personalized article recommendations for a set of customers of a large multinational financial services company. The first approach employs Reinforcement Learning and the second approach uses the XGBoost algorithm for recommending articles to the customers. Both approaches make use of a KG generated from both structured (tabular data) and unstructured data (a large body of text data).Using the Reinforcement Learning-based recommender system we could leverage the graph traversal path leading to the recommendation as a way to generate interpretations (Path Directed Reasoning (PDR)). In the XGBoost-based approach, one can also provide explainable results using post-hoc methods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like I am Five).Importantly, our approach offers explainable results, promoting better decision-making. This study underscores the potential of combining advanced machine learning techniques with KG-driven insights to bolster experience in customer relationship management.
</details>
<details>
<summary>摘要</summary>
personalized recommendations 的重要性在直接市场营销中增长，这些研究旨在提高客户体验 durch 知识图（KG）应用。例如，在金融服务中，公司可能会从提供相关的金融文章来培养关系，促进客户参与度和提高客户做出的Financial 决策。虽然许多方法集中在 KG 基于的推荐系统中，但在这项研究中，我们关注可解释 KG 基于的推荐系统。为此，我们提出了两种基于知识图的方法，用于个性化文章推荐。首先，我们使用强化学习来实现 Reinforcement Learning 基于的推荐系统。这种方法可以利用知识图的搜索路径来生成可解释的结果（Path Directed Reasoning （PDR））。其次，我们使用 XGBoost 算法来推荐文章。这种方法可以通过后处方法，如 SHAP 和 ELI5，提供可解释的结果。我们的方法可以提供可解释的结果，这会促进更好的决策。这项研究证明了将高级机器学习技术与知识图驱动的 Insights 结合使用，可以提高客户关系管理的经验。
</details></li>
</ul>
<hr>
<h2 id="PowerFusion-A-Tensor-Compiler-with-Explicit-Data-Movement-Description-and-Instruction-level-Graph-IR"><a href="#PowerFusion-A-Tensor-Compiler-with-Explicit-Data-Movement-Description-and-Instruction-level-Graph-IR" class="headerlink" title="PowerFusion: A Tensor Compiler with Explicit Data Movement Description and Instruction-level Graph IR"></a>PowerFusion: A Tensor Compiler with Explicit Data Movement Description and Instruction-level Graph IR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04995">http://arxiv.org/abs/2307.04995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Ma, Haojie Wang, Jingze Xing, Liyan Zheng, Chen Zhang, Huanqi Cao, Kezhao Huang, Shizhi Tang, Penghan Wang, Jidong Zhai</li>
<li>for: 这个论文主要是为了提高深度神经网络（DNN）的计算效率，以及对不同领域的加速器上的代码生成。</li>
<li>methods: 这个论文提出了一个名为IntelliGen的tensor compiler，可以为memory-intensive操作生成高性能的代码，并考虑到 computation和data movement optimizations。</li>
<li>results: 在试验IntelliGen时，在NVIDIA GPU、AMD GPU和Cambricon MLU上得到了1.97倍、2.93倍和16.91倍的速度提升（在average上是1.28倍、1.23倍和2.31倍），较现有最高效的框架还要快。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are of critical use in different domains. To accelerate DNN computation, tensor compilers are proposed to generate efficient code on different domain-specific accelerators. Existing tensor compilers mainly focus on optimizing computation efficiency. However, memory access is becoming a key performance bottleneck because the computational performance of accelerators is increasing much faster than memory performance. The lack of direct description of memory access and data dependence in current tensor compilers' intermediate representation (IR) brings significant challenges to generate memory-efficient code.   In this paper, we propose IntelliGen, a tensor compiler that can generate high-performance code for memory-intensive operators by considering both computation and data movement optimizations. IntelliGen represent a DNN program using GIR, which includes primitives indicating its computation, data movement, and parallel strategies. This information will be further composed as an instruction-level dataflow graph to perform holistic optimizations by searching different memory access patterns and computation operations, and generating memory-efficient code on different hardware. We evaluate IntelliGen on NVIDIA GPU, AMD GPU, and Cambricon MLU, showing speedup up to 1.97x, 2.93x, and 16.91x(1.28x, 1.23x, and 2.31x on average), respectively, compared to current most performant frameworks.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在不同领域都有critical使用。为加速DNN计算，tensor编译器被提议，以生成在不同领域特定加速器上的高效代码。现有的tensor编译器主要关注计算效率。然而，内存访问已成为计算加速器的性能瓶颈，因为计算性能的提升速度比内存性能提升的速度要快得多。现有的tensor编译器中间表示（IR）缺乏直接描述内存访问和数据依赖，这使得生成内存高效代码带来重大挑战。为解决这个问题，我们提出了IntelliGen，一种tensor编译器，可以通过考虑计算和数据移动优化来生成高性能的内存高效代码。IntelliGen使用GIR表示DNN程序，GIR包括计算、数据移动和并行策略的元素。这些信息将被组合成为数字水平的数据流图，以实现整体优化。我们对IntelliGen进行了NVIDIA GPU、AMD GPU和Cambricon MLU的测试，并显示了与当前最高性能框架的比较，获得了速度提升为1.97倍、2.93倍和16.91倍（1.28倍、1.23倍和2.31倍的平均提升）。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-of-the-Virial-Black-Hole-Mass-with-Conformal-Prediction"><a href="#Uncertainty-Quantification-of-the-Virial-Black-Hole-Mass-with-Conformal-Prediction" class="headerlink" title="Uncertainty Quantification of the Virial Black Hole Mass with Conformal Prediction"></a>Uncertainty Quantification of the Virial Black Hole Mass with Conformal Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04993">http://arxiv.org/abs/2307.04993</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yongsukyee/uncertain_blackholemass">https://github.com/yongsukyee/uncertain_blackholemass</a></li>
<li>paper_authors: Suk Yee Yong, Cheng Soon Ong</li>
<li>for: 这个研究的目的是为了测量黑洞质量的精度，以了解黑洞和宿主 галактика之间的演化。</li>
<li>methods: 这个研究使用了对称化量划 regression (CQR) 来衡量黑洞预测的不确定性。</li>
<li>results: 研究发现，使用 CQR 方法可以提供更有用的预测 интерVAL指标，并且可以根据黑洞质量和相关属性进行调整。<details>
<summary>Abstract</summary>
Precise measurements of the black hole mass are essential to gain insight on the black hole and host galaxy co-evolution. A direct measure of the black hole mass is often restricted to nearest galaxies and instead, an indirect method using the single-epoch virial black hole mass estimation is used for objects at high redshifts. However, this method is subjected to biases and uncertainties as it is reliant on the scaling relation from a small sample of local active galactic nuclei. In this study, we propose the application of conformalised quantile regression (CQR) to quantify the uncertainties of the black hole predictions in a machine learning setting. We compare CQR with various prediction interval techniques and demonstrated that CQR can provide a more useful prediction interval indicator. In contrast to baseline approaches for prediction interval estimation, we show that the CQR method provides prediction intervals that adjust to the black hole mass and its related properties. That is it yields a tighter constraint on the prediction interval (hence more certain) for a larger black hole mass, and accordingly, bright and broad spectral line width source. Using a combination of neural network model and CQR framework, the recovered virial black hole mass predictions and uncertainties are comparable to those measured from the Sloan Digital Sky Survey. The code is publicly available at https://github.com/yongsukyee/uncertain_blackholemass.
</details>
<details>
<summary>摘要</summary>
精确测量黑洞质量是研究黑洞和宿主 галактики共EVOLUTION的关键。直接测量黑洞质量通常只能在最近的 галактиках中进行，而高红shift объек图使用单个epoch virial黑洞质量估计法则受到偏见和不确定性的限制。在这种情况下，我们提议使用尺度化量表 regression（CQR）来评估黑洞预测的不确定性。我们比较了CQR与various prediction interval技术，并证明了CQR可以提供更有用的预测间隔指标。与基eline方法相比，CQR方法提供的预测间隔变化随着黑洞质量和相关的属性而变化，即在更大的黑洞质量和明亮宽 spectral line width sources 时提供更紧张的预测间隔（更确定）。使用一种带有神经网络模型的CQR框架，我们 retrieved virial黑洞质量预测和不确定性结果与SDSS中的测量结果相符。代码可以在https://github.com/yongsukyee/uncertain_blackholemass上获取。
</details></li>
</ul>
<hr>
<h2 id="Monotone-deep-Boltzmann-machines"><a href="#Monotone-deep-Boltzmann-machines" class="headerlink" title="Monotone deep Boltzmann machines"></a>Monotone deep Boltzmann machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04990">http://arxiv.org/abs/2307.04990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Feng, Ezra Winston, J. Zico Kolter</li>
<li>for: This paper explores the possibility of efficient approximate inference in deep Boltzmann machines (DBMs) by developing a new class of restricted models called monotone DBMs.</li>
<li>methods: The paper uses tools from the recently-proposed monotone Deep Equilibrium model to develop a fixed-point iteration that gives a variational mean-field solution for monotone DBMs.</li>
<li>results: The paper demonstrates that the proposed approach allows for tasks such as joint completion and classification of images within a single deep probabilistic setting, while avoiding the pitfalls of mean-field inference in traditional RBMs.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文探讨了深度博尔茨曼机器 (DBM) 的有效减少概率 approximate inference 的可能性，通过开发一种新的受限模型，即 monotone DBM。</li>
<li>methods: 论文使用最近提出的 monotone Deep Equilibrium 模型的工具，开发了一种固有点迭代，以获得 monotone DBM 的变量场解。</li>
<li>results: 论文示出了该方法可以在单个深度概率设定下完成图像的联合完成和分类任务，而不是传统 RBM 中的mean-field inference 中的陷阱。<details>
<summary>Abstract</summary>
Deep Boltzmann machines (DBMs), one of the first ``deep'' learning methods ever studied, are multi-layered probabilistic models governed by a pairwise energy function that describes the likelihood of all variables/nodes in the network. In practice, DBMs are often constrained, i.e., via the \emph{restricted} Boltzmann machine (RBM) architecture (which does not permit intra-layer connections), in order to allow for more efficient inference. In this work, we revisit the generic DBM approach, and ask the question: are there other possible restrictions to their design that would enable efficient (approximate) inference? In particular, we develop a new class of restricted model, the monotone DBM, which allows for arbitrary self-connection in each layer, but restricts the \emph{weights} in a manner that guarantees the existence and global uniqueness of a mean-field fixed point. To do this, we leverage tools from the recently-proposed monotone Deep Equilibrium model and show that a particular choice of activation results in a fixed-point iteration that gives a variational mean-field solution. While this approach is still largely conceptual, it is the first architecture that allows for efficient approximate inference in fully-general weight structures for DBMs. We apply this approach to simple deep convolutional Boltzmann architectures and demonstrate that it allows for tasks such as the joint completion and classification of images, within a single deep probabilistic setting, while avoiding the pitfalls of mean-field inference in traditional RBMs.
</details>
<details>
<summary>摘要</summary>
深度博尔茨曼机（DBM）是一种多层概率模型，其中每个层都有一个对应的概率分布，用于描述网络中所有变量/节点的可能性。在实践中，DBM通常会被限制，例如通过使用Restricted Boltzmann Machine（RBM）架构，以便更加有效地进行推理。在这个工作中，我们回到了基本的DBM方法，并问：是否有其他可能的限制，以实现更加有效的推理？特别是，我们开发了一种新的受限模型，即偏好DBM，该模型允许每层任意自连接，但是限制权重的方式，以保证存在和全局唯一的均衡点。为此，我们利用了最近提出的偏好深度平衡模型的工具，并证明在某种特定的激活函数下，这种模型会导致一个均衡点逻辑的解。尽管这种方法仍然是概念上的，但它是DBM中第一种允许高效近似推理的建筑。我们应用这种方法于深度卷积博尔茨曼架构，并示出它可以在单个深度概率设定下完成图像的联合完成和分类任务，而不需要传统RBM的含义场推理。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Bayesian-Causal-Discovery-Methods-for-Downstream-Treatment-Effect-Estimation"><a href="#Benchmarking-Bayesian-Causal-Discovery-Methods-for-Downstream-Treatment-Effect-Estimation" class="headerlink" title="Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation"></a>Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04988">http://arxiv.org/abs/2307.04988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Chinenye Emezue, Alexandre Drouin, Tristan Deleu, Stefan Bauer, Yoshua Bengio</li>
<li>for: 评估 causal discovery 方法的下游 task 的效果，即 treatment effect estimation。</li>
<li>methods: 使用 seven 个基准方法，包括一种新提出的 GFlowNets 方法，对 causal discovery 方法的下游 task 进行评估。</li>
<li>results: 研究结果显示，一些算法能够有效地捕捉各种有用和多样的 ATE 模式，而其他一些算法往往学习低概率模式，影响 (不relax) 精度和准确性。<details>
<summary>Abstract</summary>
The practical utility of causality in decision-making is widespread and brought about by the intertwining of causal discovery and causal inference. Nevertheless, a notable gap exists in the evaluation of causal discovery methods, where insufficient emphasis is placed on downstream inference. To address this gap, we evaluate seven established baseline causal discovery methods including a newly proposed method based on GFlowNets, on the downstream task of treatment effect estimation. Through the implementation of a distribution-level evaluation, we offer valuable and unique insights into the efficacy of these causal discovery methods for treatment effect estimation, considering both synthetic and real-world scenarios, as well as low-data scenarios. The results of our study demonstrate that some of the algorithms studied are able to effectively capture a wide range of useful and diverse ATE modes, while some tend to learn many low-probability modes which impacts the (unrelaxed) recall and precision.
</details>
<details>
<summary>摘要</summary>
“ causality 在决策中的实际用途广泛，这与 causal discovery 和 causal inference 的相互关联有关。然而，评估 causal discovery 方法的一个显著的差距是在下游推理方面，现有的评估方法强调上游推理。为了解决这个差距，我们评估了七种已有的基准 causal discovery 方法，包括一种基于 GFlowNets 的新方法，在对医疗效果估计任务上。通过实施分布水平评估，我们提供了有价值和独特的洞察，探讨这些 causal discovery 方法在医疗效果估计任务中的效果，包括合成和实际场景，以及低数据场景。结果显示，一些算法可以有效地捕捉各种有用和多样的 ATE 模式，而其他些则往往学习低概率模式，影响（不压缩）准确率和准确率。”
</details></li>
</ul>
<hr>
<h2 id="Secrets-of-RLHF-in-Large-Language-Models-Part-I-PPO"><a href="#Secrets-of-RLHF-in-Large-Language-Models-Part-I-PPO" class="headerlink" title="Secrets of RLHF in Large Language Models Part I: PPO"></a>Secrets of RLHF in Large Language Models Part I: PPO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04964">http://arxiv.org/abs/2307.04964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openlmlab/moss-rlhf">https://github.com/openlmlab/moss-rlhf</a></li>
<li>paper_authors: Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang<br>for:这篇论文的目标是提出一种技术Alignment的方法，以便在人类中心的辅助下，使大型语言模型（LLMs）得到人类Feedback（RLHF）的改进。methods:该论文使用了 reward models 来测量人类的偏好，Proximal Policy Optimization（PPO）来优化策略模型的输出，以及 process supervision 来提高步骤逻辑能力。results:研究发现，Policy constraints 是 PPO 算法的关键因素，因此，他们提出了 PPO-max 算法，以提高策略模型的训练稳定性。基于主要结果，他们进行了RLHF的全面分析，并与 SFT 模型和 ChatGPT 进行了比较。<details>
<summary>Abstract</summary>
Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经制定了人工通用智能的发展蓝图。其主要目标是作为人acentric（帮助、诚实、无害）助手。与人类Alignment相当重要，而使用人类反馈学习（RLHF）成为了这一努力的核心技术。现有的技术路径通常包括了优先项目模型来度量人类喜好，使用Proximal Policy Optimization（PPO）来优化政策模型的输出，以及过程监控来提高步骤逻辑能力。但由于优先项目设计、环境互动和机器人训练等因素，加上大型语言模型的实验成本巨大，使得AI研究人员对LLM的技术Alignment和安全落地带出了很大的挑战。RLHF的稳定训练仍然是一个谜。在本报告中，我们分析RLHF的框架，重新评估PPO内部运作，并探索PPO算法中的不同部分对政策代理训练的影响。我们发现政策限制是PPO算法的关键因素。因此，我们探索了PPO-max，一种PPO算法的进阶版本，以提高政策模型训练的稳定性。根据我们的主要结果，我们进行了RLHF能力的全面分析，与SFT模型和ChatGPT进行比较。由于LLMs的开源实现缺乏，因此我们对LLMs的对齐做出了很大的挑战。因此，我们将释出技术报告、优先项目模型和PPO代码，以做出一定的贡献 LLMs的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="DyCL-Dynamic-Neural-Network-Compilation-Via-Program-Rewriting-and-Graph-Optimization"><a href="#DyCL-Dynamic-Neural-Network-Compilation-Via-Program-Rewriting-and-Graph-Optimization" class="headerlink" title="DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph Optimization"></a>DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04963">http://arxiv.org/abs/2307.04963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simin Chen, Shiyi Wei, Cong Liu, Wei Yang</li>
<li>for: 提高动态神经网络（DyNNs）的编译效率和性能。</li>
<li>methods: 提出一种通用的方法，使得现有的深度学习（DL）编译器可以成功编译DyNNs。该方法包括程序分析和程序转换技术，将动态神经网络转换为多个子神经网络。每个子神经网络独立编译，并且使用主机模块来模拟控制流。</li>
<li>results: 对多个动态神经网络进行编译，实现了100%的成功率。同时，生成的执行代码在一些场景下运行速度提高了1.12-20.21倍。<details>
<summary>Abstract</summary>
DL compiler's primary function is to translate DNN programs written in high-level DL frameworks such as PyTorch and TensorFlow into portable executables. These executables can then be flexibly executed by the deployed host programs. However, existing DL compilers rely on a tracing mechanism, which involves feeding a runtime input to a neural network program and tracing the program execution paths to generate the computational graph necessary for compilation. Unfortunately, this mechanism falls short when dealing with modern dynamic neural networks (DyNNs) that possess varying computational graphs depending on the inputs. Consequently, conventional DL compilers struggle to accurately compile DyNNs into executable code. To address this limitation, we propose \tool, a general approach that enables any existing DL compiler to successfully compile DyNNs. \tool tackles the dynamic nature of DyNNs by introducing a compilation mechanism that redistributes the control and data flow of the original DNN programs during the compilation process. Specifically, \tool develops program analysis and program transformation techniques to convert a dynamic neural network into multiple sub-neural networks. Each sub-neural network is devoid of conditional statements and is compiled independently. Furthermore, \tool synthesizes a host module that models the control flow of the DyNNs and facilitates the invocation of the sub-neural networks. Our evaluation demonstrates the effectiveness of \tool, achieving a 100\% success rate in compiling all dynamic neural networks. Moreover, the compiled executables generated by \tool exhibit significantly improved performance, running between $1.12\times$ and $20.21\times$ faster than the original DyNNs executed on general-purpose DL frameworks.
</details>
<details>
<summary>摘要</summary>
DL编译器的主要功能是将深度学习（DL）程序从高级框架such as PyTorch和TensorFlow转换为可移植的执行程序。这些执行程序可以在部署的主机程序上灵活执行。然而，现有的DL编译器都 rely on一种跟踪机制，即通过 feeding runtime输入到深度学习程序并跟踪程序执行路径来生成必要的计算图来进行编译。然而，这种机制在处理现代动态神经网络（DyNNs）时会遇到问题，因为DyNNs具有因输入而变化的计算图。因此，传统的DL编译器无法准确地编译DyNNs。为解决这个限制，我们提出了\tool，一种通用的方法，可以使任何现有的DL编译器成功编译DyNNs。\tool 处理动态神经网络的方式是通过在编译过程中重新分配控制和数据流的方式来转换动态神经网络。具体来说，\tool 开发了程序分析和程序转换技术，将动态神经网络转换为多个子神经网络。每个子神经网络都是无条件语句的，可以独立地编译。此外，\tool synthesizes主机模块，模拟动态神经网络的控制流，并且促进了子神经网络的邀请。我们的评估表明，\tool 的效果非常出色，所有的动态神经网络都成功编译。此外，由\tool 生成的执行程序表现出色，在特定的情况下，与原始 DyNNs 执行在通用DL框架上的性能相比，具有1.12倍至20.21倍的提升。
</details></li>
</ul>
<hr>
<h2 id="Intrinsically-motivated-graph-exploration-using-network-theories-of-human-curiosity"><a href="#Intrinsically-motivated-graph-exploration-using-network-theories-of-human-curiosity" class="headerlink" title="Intrinsically motivated graph exploration using network theories of human curiosity"></a>Intrinsically motivated graph exploration using network theories of human curiosity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04962">http://arxiv.org/abs/2307.04962</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spatank/GraphRL">https://github.com/spatank/GraphRL</a></li>
<li>paper_authors: Shubhankar P. Patankar, Mathieu Ouellet, Juan Cervino, Alejandro Ribeiro, Kieran A. Murphy, Dani S. Bassett</li>
<li>for: 这篇论文主要是为了解决如何在图structured数据中引导探索，而不需要外部奖励。</li>
<li>methods: 该论文提出了一种基于图神经网络学习的探索方法，使用了人类好奇的两种理论：信息差距理论和压缩进步理论。</li>
<li>results: 在多个synthetically生成的图上，训练过的代理人能够在更大的环境和更长的探索步骤上generalize，并且比较有效率地计算 topological feature。此外，好奇基于探索的推荐系统也比PageRank中心性更能预测人类行为，在MovieLens、Amazon Books和Wikispeedia等实际图 dataset上得到了证明。<details>
<summary>Abstract</summary>
Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bear particular relevance for recommender systems. We demonstrate that curiosity-based recommendations are more predictive of human behavior than PageRank centrality for several real-world graph datasets, including MovieLens, Amazon Books, and Wikispeedia.
</details>
<details>
<summary>摘要</summary>
天生有探索的恩恵，即使没有外部奖励，也可以有效地促进学习。当环境自然表示为图时，如何最好引导探索仍然是一个开放问题。在这项工作中，我们提议一种新的方法，通过利用访问节点所induced的子图特征来驱动graph neural network基于的奖励学习。我们使用这些提议的特征作为奖励，以便促进探索。在多种 sintetically生成的图上，我们发现训练的代理人可以在更大的环境和更长的探索步骤上generalize。我们的方法更加高效，而不是仅仅是评估相关的topological特征。我们的内在动机具有特别 relevance  для推荐系统。我们示示了curiosity-based推荐的更高predictive power than PageRank中心性for several real-world graph datasets, including MovieLens, Amazon Books, and Wikispeedia.
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-with-Non-Cumulative-Objective"><a href="#Reinforcement-Learning-with-Non-Cumulative-Objective" class="headerlink" title="Reinforcement Learning with Non-Cumulative Objective"></a>Reinforcement Learning with Non-Cumulative Objective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04957">http://arxiv.org/abs/2307.04957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/willtop/Reinforcement_Learning_With_Non-Cumulative_Objective">https://github.com/willtop/Reinforcement_Learning_With_Non-Cumulative_Objective</a></li>
<li>paper_authors: Wei Cui, Wei Yu</li>
<li>for:  solving optimal control and reinforcement learning problems with non-cumulative objectives</li>
<li>methods:  modifying existing algorithms using a generalized Bellman update rule and providing sufficient conditions for globally optimal convergence</li>
<li>results:  demonstrating the idea experimentally on classical tasks and network routing problems, and achieving better performance compared to traditional methods<details>
<summary>Abstract</summary>
In reinforcement learning, the objective is almost always defined as a \emph{cumulative} function over the rewards along the process. However, there are many optimal control and reinforcement learning problems in various application fields, especially in communications and networking, where the objectives are not naturally expressed as summations of the rewards. In this paper, we recognize the prevalence of non-cumulative objectives in various problems, and propose a modification to existing algorithms for optimizing such objectives. Specifically, we dive into the fundamental building block for many optimal control and reinforcement learning algorithms: the Bellman optimality equation. To optimize a non-cumulative objective, we replace the original summation operation in the Bellman update rule with a generalized operation corresponding to the objective. Furthermore, we provide sufficient conditions on the form of the generalized operation as well as assumptions on the Markov decision process under which the globally optimal convergence of the generalized Bellman updates can be guaranteed. We demonstrate the idea experimentally with the bottleneck objective, i.e., the objectives determined by the minimum reward along the process, on classical optimal control and reinforcement learning tasks, as well as on two network routing problems on maximizing the flow rates.
</details>
<details>
<summary>摘要</summary>
在再增强学习中，目标通常是一个累积函数，表示过程中的奖励的总和。然而，在各种应用领域中，特别是在通信和网络领域，有许多优化控制和再增强学习问题，其目标不是自然地表示为奖励的总和。在这篇论文中，我们认为这种非累积目标在各种问题中很普遍，并提出修改现有算法来优化这类目标的方法。specifically，我们探究了许多优化控制和再增强学习算法的基本构建块：贝尔曼优化Equation。为了优化非累积目标，我们在贝尔曼更新规则中replace原始的总和操作，使用一种通用化的操作，与目标相对应。此外，我们还提供了 garantía global optimal convergence of the generalized Bellman updates可以 garantía的条件和假设，即Markov decision process的形式和assumptions。我们在经典优化控制和再增强学习任务上，以及两个网络流量最大化问题上，通过实验证明了这个想法。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-hidden-Markov-LSTM-for-short-term-traffic-flow-prediction"><a href="#Hybrid-hidden-Markov-LSTM-for-short-term-traffic-flow-prediction" class="headerlink" title="Hybrid hidden Markov LSTM for short-term traffic flow prediction"></a>Hybrid hidden Markov LSTM for short-term traffic flow prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04954">http://arxiv.org/abs/2307.04954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnimitra Sengupta, Adway Das, S. Ilgin Guler</li>
<li>for: 预测交通流量</li>
<li>methods: 使用深度学习方法（如RNN和其变种）和hybrid hidden Markov-LSTM模型</li>
<li>results: 比使用传统方法（如Markov switching ARIMA和LSTM）表现出显著的性能提升<details>
<summary>Abstract</summary>
Deep learning (DL) methods have outperformed parametric models such as historical average, ARIMA and variants in predicting traffic variables into short and near-short future, that are critical for traffic management. Specifically, recurrent neural network (RNN) and its variants (e.g. long short-term memory) are designed to retain long-term temporal correlations and therefore are suitable for modeling sequences. However, multi-regime models assume the traffic system to evolve through multiple states (say, free-flow, congestion in traffic) with distinct characteristics, and hence, separate models are trained to characterize the traffic dynamics within each regime. For instance, Markov-switching models with a hidden Markov model (HMM) for regime identification is capable of capturing complex dynamic patterns and non-stationarity. Interestingly, both HMM and LSTM can be used for modeling an observation sequence from a set of latent or, hidden state variables. In LSTM, the latent variable is computed in a deterministic manner from the current observation and the previous latent variable, while, in HMM, the set of latent variables is a Markov chain. Inspired by research in natural language processing, a hybrid hidden Markov-LSTM model that is capable of learning complementary features in traffic data is proposed for traffic flow prediction. Results indicate significant performance gains in using hybrid architecture compared to conventional methods such as Markov switching ARIMA and LSTM.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）方法已经超过参数模型，如历史平均值、ARIMA和其变种，在预测交通变量的短期和近期未来方面表现出色。特别是，循环神经网络（RNN）和其变种（例如长短期记忆）能够保留长期时间的相关性，因此适用于序列模elling。然而，多态模型假设交通系统会逐渐发展到多个状态（例如自由流、堵塞），每个状态都具有独特的特征，因此需要分别训练特定的模型来描述交通动态。例如，Markov switching模型可以使用隐藏Markov模型（HMM）来确定状态转换，可以捕捉复杂的动态模式和非站ARY。在LSTM中，隐藏变量是基于当前观察和上一个隐藏变量的决定方式计算的，而在HMM中，隐藏变量是一个Markov链。受自然语言处理研究的启发，一种hybrid隐藏Markov-LSTM模型被提出，可以学习交通数据中的补充特征。结果表明，使用hybrid体系可以与传统方法，如Markov switching ARIMA和LSTM，相比较显著提高预测性能。
</details></li>
</ul>
<hr>
<h2 id="Compact-Twice-Fusion-Network-for-Edge-Detection"><a href="#Compact-Twice-Fusion-Network-for-Edge-Detection" class="headerlink" title="Compact Twice Fusion Network for Edge Detection"></a>Compact Twice Fusion Network for Edge Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04952">http://arxiv.org/abs/2307.04952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/li-yachuan/ctfn-pytorch-master">https://github.com/li-yachuan/ctfn-pytorch-master</a></li>
<li>paper_authors: Yachuan Li, Zongmin Li, Xavier Soria P., Chaozhi Yang, Qian Xiao, Yun Bai, Hua Li, Xiangdong Wang</li>
<li>for: 本研究旨在提出一种可靠的多尺度特征融合网络，以便实现高精度Edge detection的目标。</li>
<li>methods: 该网络使用了两种轻量级多尺度特征融合模块：一个具有semantic enhancement module（SEM），可以利用粗略度特征中的semantic信息来引导细粒度特征的学习；另一个具有pseudo pixel-level weighting（PPW）模块，可以将多尺度特征的补做weighting，以便更好地融合多尺度特征。</li>
<li>results: 该方法在BSDS500、NYUDv2和BIPEDv2等三个 dataset上进行了评估，与state-of-the-art方法相比，CTFN达到了竞争性的准确率，而且具有较少的参数和计算成本。特别是，除了基础模型外，CTFN只需要0.1M的额外参数，这使得其计算成本降低到了60%以下。<details>
<summary>Abstract</summary>
The significance of multi-scale features has been gradually recognized by the edge detection community. However, the fusion of multi-scale features increases the complexity of the model, which is not friendly to practical application. In this work, we propose a Compact Twice Fusion Network (CTFN) to fully integrate multi-scale features while maintaining the compactness of the model. CTFN includes two lightweight multi-scale feature fusion modules: a Semantic Enhancement Module (SEM) that can utilize the semantic information contained in coarse-scale features to guide the learning of fine-scale features, and a Pseudo Pixel-level Weighting (PPW) module that aggregate the complementary merits of multi-scale features by assigning weights to all features. Notwithstanding all this, the interference of texture noise makes the correct classification of some pixels still a challenge. For these hard samples, we propose a novel loss function, coined Dynamic Focal Loss, which reshapes the standard cross-entropy loss and dynamically adjusts the weights to correct the distribution of hard samples. We evaluate our method on three datasets, i.e., BSDS500, NYUDv2, and BIPEDv2. Compared with state-of-the-art methods, CTFN achieves competitive accuracy with less parameters and computational cost. Apart from the backbone, CTFN requires only 0.1M additional parameters, which reduces its computation cost to just 60% of other state-of-the-art methods. The codes are available at https://github.com/Li-yachuan/CTFN-pytorch-master.
</details>
<details>
<summary>摘要</summary>
“多尺度特征的重要性逐渐被edge detection社区所认可。然而，多尺度特征的融合增加模型的复杂度，不易应用。在这个工作中，我们提出了一个Compact Twice Fusion Network（CTFN），可以充分融合多尺度特征，同时保持模型的简洁性。CTFN包括两个轻量级多尺度特征融合模组：一个Semantic Enhancement Module（SEM）可以利用粗细度特征中的 semantics信息来导引细细度特征的学习，以及一个Pseudo Pixel-level Weighting（PPW）模组可以将多尺度特征中的 complementary advantages聚集到所有特征上。尽管如此，隐藏在文本腐败中的杂质项目仍然是一个挑战。为了解决这个问题，我们提出了一个新的损失函数，即Dynamic Focal Loss，它可以重新定义标准十进法损失函数，并动态调整权重，以正确处理困难样本。我们在BSDS500、NYUDv2和BIPEDv2三个 dataset上评估了我们的方法，与现有的方法相比，CTFN实现了竞争的精度，仅需0.1M的额外参数，对应computational cost的减少为60%。代码可以在https://github.com/Li-yachuan/CTFN-pytorch-master上获取。”
</details></li>
</ul>
<hr>
<h2 id="DDGM-Solving-inverse-problems-by-Diffusive-Denoising-of-Gradient-based-Minimization"><a href="#DDGM-Solving-inverse-problems-by-Diffusive-Denoising-of-Gradient-based-Minimization" class="headerlink" title="DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization"></a>DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04946">http://arxiv.org/abs/2307.04946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyle Luther, H. Sebastian Seung</li>
<li>for:  This paper is written for solving the inverse problem of tomographic reconstruction from electron micrographs acquired at multiple tilt angles.</li>
<li>methods: The paper proposes a simpler approach that combines traditional gradient-based minimization of reconstruction error with denoising, using a convolutional neural network (CNN) as a prior. The method adds noise at each step and uses an iterative dynamics resembling a Langevin or diffusion process, with the level of added noise and the size of the denoising step decaying exponentially with time.</li>
<li>results: The paper shows that high accuracy can be achieved with as few as 50 denoising steps, and compares the proposed method with more complex diffusion methods such as DDRM and DPS. The results demonstrate that the proposed method is more accurate (as measured by MSE and SSIM) for the tomography problem, and can be applied to reconstruction of arbitrary-sized images.<details>
<summary>Abstract</summary>
Inverse problems generally require a regularizer or prior for a good solution. A recent trend is to train a convolutional net to denoise images, and use this net as a prior when solving the inverse problem. Several proposals depend on a singular value decomposition of the forward operator, and several others backpropagate through the denoising net at runtime. Here we propose a simpler approach that combines the traditional gradient-based minimization of reconstruction error with denoising. Noise is also added at each step, so the iterative dynamics resembles a Langevin or diffusion process. Both the level of added noise and the size of the denoising step decay exponentially with time. We apply our method to the problem of tomographic reconstruction from electron micrographs acquired at multiple tilt angles. With empirical studies using simulated tilt views, we find parameter settings for our method that produce good results. We show that high accuracy can be achieved with as few as 50 denoising steps. We also compare with DDRM and DPS, more complex diffusion methods of the kinds mentioned above. These methods are less accurate (as measured by MSE and SSIM) for our tomography problem, even after the generation hyperparameters are optimized. Finally we extend our method to reconstruction of arbitrary-sized images and show results on 128 $\times$ 1568 pixel images
</details>
<details>
<summary>摘要</summary>
“倒Problems通常需要一个正规化或先验的方法以获得好的解决方案。现在的趋势是使用卷积网来去噪图像，并将这个网络用作解决倒Problem的先验。一些提案靠摄Singular value decomposition of the forward operator，另一些透过在Runtime backpropagating through the denoising net。我们提出了一种更简单的方法，它结合了传统的梯度基于的最小化重建错误和去噪。噪音也会在每步加入，因此迭代运算类似于兰格温或漫游过程。噪音水平和去噪步骤的减少阶段落逐渐呈指数衰减。我们将方法应用到电子显微镜中的 Tomographic Reconstruction问题上。通过实验使用模拟的倾斜角度，我们获得了适当的参数设定，并证明高精度可以在50个去噪步骤中获得。我们还与DDRM和DPS等更复杂的演化方法进行比较，这些方法在我们的Tomography问题上较低的Mean Squared Error和Structural Similarity Index Measure。最后，我们将方法扩展到任意大小的图像重建问题上，并在128 $\times$ 1568像素图像上显示结果。”
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Algorithms-for-Federated-Domain-Generalization"><a href="#Benchmarking-Algorithms-for-Federated-Domain-Generalization" class="headerlink" title="Benchmarking Algorithms for Federated Domain Generalization"></a>Benchmarking Algorithms for Federated Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04942">http://arxiv.org/abs/2307.04942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/inouye-lab/feddg_benchmark">https://github.com/inouye-lab/feddg_benchmark</a></li>
<li>paper_authors: Ruqi Bai, Saurabh Bagchi, David I. Inouye</li>
<li>for: This paper is written for evaluating the performance of Federated Domain Generalization (FedDG) methods, which is a new challenge in Federated Learning (FL) that involves dealing with diverse client datasets.</li>
<li>methods: The paper proposes a benchmark methodology for FedDG, which includes controlling the number and heterogeneity of clients and providing metrics for dataset difficulty. The authors also evaluate 13 FedDG methods, including centralized DG methods adapted to the FL context, FL methods that handle client heterogeneity, and methods designed specifically for FedDG.</li>
<li>results: The paper shows that despite some progress, there remain significant performance gaps in FedDG, particularly when evaluating with a large number of clients, high client heterogeneity, or more realistic datasets. The authors also find that the performance of FedDG methods can be improved by using a larger number of clients and more diverse datasets.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文是为评估 Federated Domain Generalization（FedDG）方法的性能，这是 Federated Learning（FL）中新的挑战，它涉及处理多个客户端数据集的多样性。</li>
<li>methods: 论文提出了一种 FedDG 评估方法方法，包括控制客户端数量和多样性，以及提供数据集困难度指标。作者还评估了 13 种 FedDG 方法，包括中央 DG 方法在 FL 上的修改，FL 方法可以处理客户端多样性，以及特制 для FedDG 的方法。</li>
<li>results: 论文显示，尽管有一些进步，但 FedDG 中的性能仍然存在显著的性能差距，特别是在评估多个客户端、高客户端多样性或更真实的数据集时。作者们还发现，通过使用更多的客户端和更多的多样的数据集，FedDG 方法的性能可以得到改进。<details>
<summary>Abstract</summary>
While prior domain generalization (DG) benchmarks consider train-test dataset heterogeneity, we evaluate Federated DG which introduces federated learning (FL) specific challenges. Additionally, we explore domain-based heterogeneity in clients' local datasets - a realistic Federated DG scenario. Prior Federated DG evaluations are limited in terms of the number or heterogeneity of clients and dataset diversity. To address this gap, we propose an Federated DG benchmark methodology that enables control of the number and heterogeneity of clients and provides metrics for dataset difficulty. We then apply our methodology to evaluate 13 Federated DG methods, which include centralized DG methods adapted to the FL context, FL methods that handle client heterogeneity, and methods designed specifically for Federated DG. Our results suggest that despite some progress, there remain significant performance gaps in Federated DG particularly when evaluating with a large number of clients, high client heterogeneity, or more realistic datasets. Please check our extendable benchmark code here: https://github.com/inouye-lab/FedDG_Benchmark.
</details>
<details>
<summary>摘要</summary>
“对于联边学习（Federated Learning，FL）中的网络统一化（Domain Generalization，DG），我们评估了联边网络统一化（Federated DG），并将联边学习特有的挑战纳入考虑。另外，我们还探索了客户端本地数据中的领域差异，这是现实中联边学习中的常见情况。过去的联边DG评估仅仅具有一些客户和数据的限制，无法反映现实中联边学习的多样性和问题。为了缓解这个问题，我们提出了一个联边DG评估方法，可以控制客户和数据的数量和多样性，并提供了评估数据的困难度的指标。我们运用这个方法评估了13种联边DG方法，包括中央化DG方法在FL上的适应，FL方法可以处理客户端的多样性，以及特地设计 для联边DG的方法。我们的结果显示，虽然有一些进步，但在处理大量客户、高客户多样性或更真实的数据时仍然存在较大的性能差距。您可以在以下链接中获取我们的可extendable benchmark代码：https://github.com/inouye-lab/FedDG_Benchmark。”
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Feature-Encoding-on-Malware-Classification-Explainability"><a href="#Impact-of-Feature-Encoding-on-Malware-Classification-Explainability" class="headerlink" title="Impact of Feature Encoding on Malware Classification Explainability"></a>Impact of Feature Encoding on Malware Classification Explainability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05614">http://arxiv.org/abs/2307.05614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elyes Manai, Mohamed Mejri, Jaouhar Fattahi</li>
<li>for: 这个论文研究了对于可解释人工智能（XAI）算法的特征编码技术的影响。</li>
<li>methods: 使用一个恶意软件分类 dataset，我们训练了一个 XGBoost 模型，并比较了两种特征编码方法：标签编码（LE）和一个热点编码（OHE）。</li>
<li>results: 我们发现，使用 OHE 相比 LE，表现只有微不足。但是，OHE 提供了更详细的解释，使得更深入的探究详细信息。此外，我们发现 OHE 使得解释文件更小，降低了人类分析员的分析时间。这些发现强调了考虑特征编码技术在 XAI 研究中的重要性，并建议进一步探索采用其他编码方法和创新视觉方法。<details>
<summary>Abstract</summary>
This paper investigates the impact of feature encoding techniques on the explainability of XAI (Explainable Artificial Intelligence) algorithms. Using a malware classification dataset, we trained an XGBoost model and compared the performance of two feature encoding methods: Label Encoding (LE) and One Hot Encoding (OHE). Our findings reveal a marginal performance loss when using OHE instead of LE. However, the more detailed explanations provided by OHE compensated for this loss. We observed that OHE enables deeper exploration of details in both global and local contexts, facilitating more comprehensive answers. Additionally, we observed that using OHE resulted in smaller explanation files and reduced analysis time for human analysts. These findings emphasize the significance of considering feature encoding techniques in XAI research and suggest potential for further exploration by incorporating additional encoding methods and innovative visualization approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Fair-Graph-Neural-Networks-via-Graph-Counterfactual"><a href="#Towards-Fair-Graph-Neural-Networks-via-Graph-Counterfactual" class="headerlink" title="Towards Fair Graph Neural Networks via Graph Counterfactual"></a>Towards Fair Graph Neural Networks via Graph Counterfactual</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04937">http://arxiv.org/abs/2307.04937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/timelovercc/caf-gnn">https://github.com/timelovercc/caf-gnn</a></li>
<li>paper_authors: Zhimeng Guo, Jialiang Li, Teng Xiao, Yao Ma, Suhang Wang</li>
<li>for: 本文针对 Graph Neural Networks (GNNs) 的偏见问题进行研究，尤其是 GNNs 在训练数据中继承和增强偏见的问题。</li>
<li>methods: 本文提出了一个名为 CAF 的新框架，它可以从训练数据中选择合理的 counterfactual，以避免非现实的 counterfactual，并将选择的 counterfactual 用于学习公平的node表示。</li>
<li>results: 实验结果显示，CAF 可以对 synthetic 和 real-world 数据进行优化，并且可以增强 GNNs 的公平性。<details>
<summary>Abstract</summary>
Graph neural networks have shown great ability in representation (GNNs) learning on graphs, facilitating various tasks. Despite their great performance in modeling graphs, recent works show that GNNs tend to inherit and amplify the bias from training data, causing concerns of the adoption of GNNs in high-stake scenarios. Hence, many efforts have been taken for fairness-aware GNNs. However, most existing fair GNNs learn fair node representations by adopting statistical fairness notions, which may fail to alleviate bias in the presence of statistical anomalies. Motivated by causal theory, there are several attempts utilizing graph counterfactual fairness to mitigate root causes of unfairness. However, these methods suffer from non-realistic counterfactuals obtained by perturbation or generation. In this paper, we take a causal view on fair graph learning problem. Guided by the casual analysis, we propose a novel framework CAF, which can select counterfactuals from training data to avoid non-realistic counterfactuals and adopt selected counterfactuals to learn fair node representations for node classification task. Extensive experiments on synthetic and real-world datasets show the effectiveness of CAF. Our code is available at https://github.com/TimeLovercc/CAF-GNN.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) 有出色的能力在图上学习 Representation, 促进多种任务。 despite their great performance in modeling graphs, recent works show that GNNs tend to inherit and amplify the bias from training data, causing concerns about the adoption of GNNs in high-stakes scenarios. Therefore, many efforts have been made for fairness-aware GNNs. However, most existing fair GNNs learn fair node representations by adopting statistical fairness notions, which may fail to alleviate bias in the presence of statistical anomalies. Motivated by causal theory, there are several attempts using graph counterfactual fairness to mitigate the root causes of unfairness. However, these methods suffer from non-realistic counterfactuals obtained by perturbation or generation.In this paper, we take a causal view on the fair graph learning problem. Guided by the causal analysis, we propose a novel framework CAF, which can select counterfactuals from training data to avoid non-realistic counterfactuals and adopt selected counterfactuals to learn fair node representations for node classification tasks. Extensive experiments on synthetic and real-world datasets show the effectiveness of CAF. Our code is available at <https://github.com/TimeLovercc/CAF-GNN>.Here's the translation in Simplified Chinese:GRNNs 有出色的能力在图上学习 Representation, 促进多种任务。  despite their great performance in modeling graphs, recent works show that GRNNs tend to inherit and amplify the bias from training data, causing concerns about the adoption of GRNNs in high-stakes scenarios. Therefore, many efforts have been made for fairness-aware GRNNs. However, most existing fair GRNNs learn fair node representations by adopting statistical fairness notions, which may fail to alleviate bias in the presence of statistical anomalies. Motivated by causal theory, there are several attempts using graph counterfactual fairness to mitigate the root causes of unfairness. However, these methods suffer from non-realistic counterfactuals obtained by perturbation or generation.In this paper, we take a causal view on the fair graph learning problem. Guided by the causal analysis, we propose a novel framework CAF, which can select counterfactuals from training data to avoid non-realistic counterfactuals and adopt selected counterfactuals to learn fair node representations for node classification tasks. Extensive experiments on synthetic and real-world datasets show the effectiveness of CAF. Our code is available at <https://github.com/TimeLovercc/CAF-GNN>.
</details></li>
</ul>
<hr>
<h2 id="Substance-or-Style-What-Does-Your-Image-Embedding-Know"><a href="#Substance-or-Style-What-Does-Your-Image-Embedding-Know" class="headerlink" title="Substance or Style: What Does Your Image Embedding Know?"></a>Substance or Style: What Does Your Image Embedding Know?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05610">http://arxiv.org/abs/2307.05610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cyrus Rashtchian, Charles Herrmann, Chun-Sung Ferng, Ayan Chakrabarti, Dilip Krishnan, Deqing Sun, Da-Cheng Juan, Andrew Tomkins</li>
<li>for: 这个论文是为了研究图像基础模型中的非 semantic信息，以及这些基础模型在不同下游任务中的表现。</li>
<li>methods: 作者使用了一系列的变换预测任务来测试图像基础模型中的非 semantic信息，包括图像风格、质量和自然&#x2F;人工变换等多个轴。</li>
<li>results: 研究发现，六种图像基础模型（包括SimCLR）中的embeddings含有许多非 semantic信息，可以识别多达数十种变换。此外，作者还发现，使用图像文本模型（CLIP和ALIGN）可以更好地识别新的样式转移示例，而使用面具模型（CAN和MAE）则更适合隐藏变换任务。<details>
<summary>Abstract</summary>
Probes are small networks that predict properties of underlying data from embeddings, and they provide a targeted, effective way to illuminate the information contained in embeddings. While analysis through the use of probes has become standard in NLP, there has been much less exploration in vision. Image foundation models have primarily been evaluated for semantic content. Better understanding the non-semantic information in popular embeddings (e.g., MAE, SimCLR, or CLIP) will shed new light both on the training algorithms and on the uses for these foundation models. We design a systematic transformation prediction task and measure the visual content of embeddings along many axes, including image style, quality, and a range of natural and artificial transformations. Surprisingly, six embeddings (including SimCLR) encode enough non-semantic information to identify dozens of transformations. We also consider a generalization task, where we group similar transformations and hold out several for testing. We find that image-text models (CLIP and ALIGN) are better at recognizing new examples of style transfer than masking-based models (CAN and MAE). Overall, our results suggest that the choice of pre-training algorithm impacts the types of information in the embedding, and certain models are better than others for non-semantic downstream tasks.
</details>
<details>
<summary>摘要</summary>
probes 是小型网络，可以预测嵌入数据中的性质，并提供一种targeted、有效的方式来照明嵌入数据中的信息。在 NLP 领域中， probes 已经成为标准的分析方法，而在视觉领域中， however, 只有很少的探索。 popular 的嵌入模型（如 MAE、SimCLR 和 CLIP）主要被评估为semantic content，但是更深入的理解这些嵌入模型中的非 semantic information（例如图像风格、质量等）会对训练算法和这些基础模型的应用有新的灯光。我们设计了一个系统性的变换预测任务，并测量嵌入中的视觉内容 along 多个轴，包括图像风格、质量和一些自然和人工变换。结果显示， six 个嵌入（包括 SimCLR）encode enough non-semantic information，可以识别多达 dozen 种变换。我们还考虑一个总结任务，将类似的变换分组，并将一些用作测试集。我们发现， image-text 模型（CLIP 和 ALIGN）在新的样式转移例子中表现更好，而 masking-based 模型（CAN 和 MAE）则更适合掩码转换。总之，我们的结果表明，选择预训练算法的选择会影响嵌入中的信息类型，而certain 模型在非 semantic 下游任务中表现更好。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Counterexample-Guidance-for-Safer-Reinforcement-Learning-Extended-Version"><a href="#Probabilistic-Counterexample-Guidance-for-Safer-Reinforcement-Learning-Extended-Version" class="headerlink" title="Probabilistic Counterexample Guidance for Safer Reinforcement Learning (Extended Version)"></a>Probabilistic Counterexample Guidance for Safer Reinforcement Learning (Extended Version)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04927">http://arxiv.org/abs/2307.04927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaotong Ji, Antonio Filieri</li>
<li>for: 本研究旨在解决RL在安全关键场景中的限制，因为失败可能导致高成本。</li>
<li>methods: 我们使用Counterexample-based safe exploration方法，把批处理和抽象模型结合在一起，以便在不同的环境中快速地训练agent。</li>
<li>results: 我们的方法可以在静止训练和在线探索中减少安全违反的风险，并且可以与QL和DQN标准算法和先前的相关工作相比，提高了安全性和总奖励的性能。<details>
<summary>Abstract</summary>
Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the risk of safety violations during the subsequent online exploration. We demonstrate our method's effectiveness in reducing safety violations during online exploration in preliminary experiments by an average of 40.3% compared with QL and DQN standard algorithms and 29.1% compared with previous related work, while achieving comparable cumulative rewards with respect to unrestricted exploration and alternative approaches.
</details>
<details>
<summary>摘要</summary>
安全探索targets addressing the limitations of reinforcement learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the risk of safety violations during the subsequent online exploration. We demonstrate our method's effectiveness in reducing safety violations during online exploration in preliminary experiments by an average of 40.3% compared with QL and DQN standard algorithms and 29.1% compared with previous related work, while achieving comparable cumulative rewards with respect to unrestricted exploration and alternative approaches.
</details></li>
</ul>
<hr>
<h2 id="SimpleMTOD-A-Simple-Language-Model-for-Multimodal-Task-Oriented-Dialogue-with-Symbolic-Scene-Representation"><a href="#SimpleMTOD-A-Simple-Language-Model-for-Multimodal-Task-Oriented-Dialogue-with-Symbolic-Scene-Representation" class="headerlink" title="SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation"></a>SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04907">http://arxiv.org/abs/2307.04907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bhathiya Hemanthage, Christian Dondrup, Phil Bartie, Oliver Lemon</li>
<li>for: 这篇论文主要是为了提出一种简单的语言模型，用于处理多modal任务对话。</li>
<li>methods: 该模型基于大规模的 transformer 框架，并利用了 transfer learning 技术，从 pre-trained GPT-2 中提取了知识。为了捕捉视觉场景的 semantics，该模型引入了本地和 де-本地 токен。</li>
<li>results: 该模型在 Response Generation 子任务上达到了 state-of-the-art BLEU 分数 (0.327)，并在其他多modal 子任务中表现良好，包括 Disambiguation、Coreference Resolution 和 Dialog State Tracking。这是 despite 该模型采取了一种 minimalist 的方法来提取视觉（以及非视觉）信息。<details>
<summary>Abstract</summary>
SimpleMTOD is a simple language model which recasts several sub-tasks in multimodal task-oriented dialogues as sequence prediction tasks. SimpleMTOD is built on a large-scale transformer-based auto-regressive architecture, which has already proven to be successful in uni-modal task-oriented dialogues, and effectively leverages transfer learning from pre-trained GPT-2. In-order to capture the semantics of visual scenes, we introduce both local and de-localized tokens for objects within a scene. De-localized tokens represent the type of an object rather than the specific object itself and so possess a consistent meaning across the dataset. SimpleMTOD achieves a state-of-the-art BLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0 test-std dataset while performing on par in other multimodal sub-tasks: Disambiguation, Coreference Resolution, and Dialog State Tracking. This is despite taking a minimalist approach for extracting visual (and non-visual) information. In addition the model does not rely on task-specific architectural changes such as classification heads.
</details>
<details>
<summary>摘要</summary>
SimpleMTOD 是一个简单的语言模型，它将多modal task-oriented对话中的多个子任务转化为序列预测任务。 SimpleMTOD 基于大规模的 transformer 自动生成架构，这种架构在单modal task-oriented对话中已经证明成功，并且有效地利用了预训练的 GPT-2 的转移学习。为了捕捉视觉场景的 semantics，我们引入了场景中对象的本地和非本地符号。非本地符号表示对象的类型而不是特定的对象，因此具有 dataset 中的一致性。 SimpleMTOD 在 SIMMC 2.0 测试标准数据集中的 Response Generation 子任务中 achieved state-of-the-art BLEU 分数 (0.327)，并在其他多modal 子任务中（歧义解决、核心引用解决和对话状态跟踪）表现良好，尽管使用了 minimalist 的方法来提取视觉（和非视觉）信息。此外，模型不需要任务特定的建筑性Changes，如分类头。
</details></li>
</ul>
<hr>
<h2 id="FedYolo-Augmenting-Federated-Learning-with-Pretrained-Transformers"><a href="#FedYolo-Augmenting-Federated-Learning-with-Pretrained-Transformers" class="headerlink" title="FedYolo: Augmenting Federated Learning with Pretrained Transformers"></a>FedYolo: Augmenting Federated Learning with Pretrained Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04905">http://arxiv.org/abs/2307.04905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuechen Zhang, Mingchen Li, Xiangyu Chang, Jiasi Chen, Amit K. Roy-Chowdhury, Ananda Theertha Suresh, Samet Oymak</li>
<li>for: 这个论文的目的是探讨如何使用预训练 трансформа器（PTF）来实现在移动设备和边缘设备上进行学习，以满足多样化的客户端目标和有限的不同数据的学习。</li>
<li>methods: 这个论文使用了联合学习（Federated Learning）和预训练 transformer（PTF）来实现在移动设备和边缘设备上进行学习，并 investigate了模型大小和模块化的影响。</li>
<li>results: 研究发现，可以通过增大模型规模和使用模块化来提高设备和网络约束下的学习效果，同时减少了通信轮次数。此外，模块化还可以使得客户端可以同时解决多个无关的任务，而不会出现归化问题。这些发现 inspirited a new federated learning approach called “You Only Load Once” (FedYolo)， clients可以通过通信有效的模块来更新模型，而不需要每次更新整个模型。<details>
<summary>Abstract</summary>
The growth and diversity of machine learning applications motivate a rethinking of learning with mobile and edge devices. How can we address diverse client goals and learn with scarce heterogeneous data? While federated learning aims to address these issues, it has challenges hindering a unified solution. Large transformer models have been shown to work across a variety of tasks achieving remarkable few-shot adaptation. This raises the question: Can clients use a single general-purpose model, rather than custom models for each task, while obeying device and network constraints? In this work, we investigate pretrained transformers (PTF) to achieve these on-device learning goals and thoroughly explore the roles of model size and modularity, where the latter refers to adaptation through modules such as prompts or adapters. Focusing on federated learning, we demonstrate that: (1) Larger scale shrinks the accuracy gaps between alternative approaches and improves heterogeneity robustness. Scale allows clients to run more local SGD epochs which can significantly reduce the number of communication rounds. At the extreme, clients can achieve respectable accuracy locally highlighting the potential of fully-local learning. (2) Modularity, by design, enables $>$100$\times$ less communication in bits. Surprisingly, it also boosts the generalization capability of local adaptation methods and the robustness of smaller PTFs. Finally, it enables clients to solve multiple unrelated tasks simultaneously using a single PTF, whereas full updates are prone to catastrophic forgetting. These insights on scale and modularity motivate a new federated learning approach we call "You Only Load Once" (FedYolo): The clients load a full PTF model once and all future updates are accomplished through communication-efficient modules with limited catastrophic-forgetting, where each task is assigned to its own module.
</details>
<details>
<summary>摘要</summary>
machine learning应用的增长和多样性，需要我们重新思考手持设备和边缘设备上的学习。如何处理多样化客户端目标和缺乏多样化数据的学习呢？联邦学习旨在解决这些问题，但它存在一些阻碍统一解决方案的挑战。大型转换器模型已经在多种任务上显示出惊人的几次适应性，这引起了问题：客户可以使用单一通用模型来满足每个任务，而不是为每个任务创建特定的模型吗？在这种情况下，我们 investigate pretrained transformers (PTF) 以实现这些在设备上学习的目标，并且全面探索模型大小和模块化的角色。我们将注重联邦学习，并证明：1. 规模的扩大可以降低客户端和网络约束下的准确性差距，同时提高多样性的鲁棒性。通过在本地进行更多的SGD迭代，客户可以减少通信轮次数。在极限情况下，客户可以在本地达到尽可能高的准确性，这 highlights 了本地学习的潜在可能性。2. 模块化设计可以减少通信量，并且意外地提高了本地适应方法的总体化能力和小型PTF的鲁棒性。此外，它还允许客户同时解决多个无关的任务，而不是通过全部更新而导致彻底忘记。这些策略对规模和模块化的影响，驱动我们提出一种新的联邦学习方法，我们称之为“只上载一次”（FedYolo）。在这种方法中，客户在第一次上载一个完整的PTF模型后，所有的未来更新都可以通过通信效率低的模块来完成，而不会导致彻底忘记。
</details></li>
</ul>
<hr>
<h2 id="Fast-dynamic-time-warping-and-clustering-in-C"><a href="#Fast-dynamic-time-warping-and-clustering-in-C" class="headerlink" title="Fast dynamic time warping and clustering in C++"></a>Fast dynamic time warping and clustering in C++</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04904">http://arxiv.org/abs/2307.04904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Volkan Kumtepeli, Rebecca Perriment, David A. Howey</li>
<li>for:  Computationally efficient dynamic time warping (DTW) and clustering of time-series data.</li>
<li>methods: Dynamic programming and mixed-integer programming (MIP) for DTW and clustering, with task-level parallelization for efficiency.</li>
<li>results: 33% faster than the next fastest option on average, with a 64% speedup for larger datasets (over 1000 time series). The MIP clustering is most effective for small numbers of longer time series.<details>
<summary>Abstract</summary>
We present an approach for computationally efficient dynamic time warping (DTW) and clustering of time-series data. The method frames the dynamic warping of time series datasets as an optimisation problem solved using dynamic programming, and then clusters time series data by solving a second optimisation problem using mixed-integer programming (MIP). There is also an option to use k-medoids clustering for increased speed, when a certificate for global optimality is not essential. The improved efficiency of our approach is due to task-level parallelisation of the clustering alongside DTW. Our approach was tested using the UCR Time Series Archive, and was found to be, on average, 33% faster than the next fastest option when using the same clustering method. This increases to 64% faster when considering only larger datasets (with more than 1000 time series). The MIP clustering is most effective on small numbers of longer time series, because the DTW computation is faster than other approaches, but the clustering problem becomes increasingly computationally expensive as the number of time series to be clustered increases.
</details>
<details>
<summary>摘要</summary>
我们提出了一种 computationally efficient 的动态时间扭曲（DTW）和时间序列数据 clustering 方法。该方法将动态扭曲时间序列数据集 frames 为一个优化问题，使用动态Programming 解决，然后使用杂Integer Programming（MIP）解决第二个优化问题，并且可以选择使用 k-medoids  clustering 以提高速度。我们的方法在UCR Time Series Archive 上进行测试，与其他相同 clustering 方法相比，平均提高了33%的速度，对于更大的数据集（包括1000个时间序列），则提高到64%。MIP clustering 对于少量 longer 时间序列表示最高效，因为 DTW 计算 faster than other approaches，但是 clustering 问题的计算成本随着时间序列数据集的数量增加。
</details></li>
</ul>
<hr>
<h2 id="Can-You-Improve-My-Code-Optimizing-Programs-with-Local-Search"><a href="#Can-You-Improve-My-Code-Optimizing-Programs-with-Local-Search" class="headerlink" title="Can You Improve My Code? Optimizing Programs with Local Search"></a>Can You Improve My Code? Optimizing Programs with Local Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05603">http://arxiv.org/abs/2307.05603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fatemehab/polis">https://github.com/fatemehab/polis</a></li>
<li>paper_authors: Fatemeh Abdollahi, Saqib Ameen, Matthew E. Taylor, Levi H. S. Lelis</li>
<li>for: 这篇论文目标是提高现有程序的性能，通过利用程序的结构和已有的猛烈合成算法。</li>
<li>methods: 该方法使用了本地搜索，不断地改进单个程序行，以提高程序的性能。</li>
<li>results: 经过27名参与者的用户研究，发现POLIS可以在两个单机游戏中（即月球降落和高速公路）提高参与者的程序性能。此外，对现有Stack Overflow代码进行了一个证明示例，表明POLIS在实际问题中也有应用价值。<details>
<summary>Abstract</summary>
This paper introduces a local search method for improving an existing program with respect to a measurable objective. Program Optimization with Locally Improving Search (POLIS) exploits the structure of a program, defined by its lines. POLIS improves a single line of the program while keeping the remaining lines fixed, using existing brute-force synthesis algorithms, and continues iterating until it is unable to improve the program's performance. POLIS was evaluated with a 27-person user study, where participants wrote programs attempting to maximize the score of two single-agent games: Lunar Lander and Highway. POLIS was able to substantially improve the participants' programs with respect to the game scores. A proof-of-concept demonstration on existing Stack Overflow code measures applicability in real-world problems. These results suggest that POLIS could be used as a helpful programming assistant for programming problems with measurable objectives.
</details>
<details>
<summary>摘要</summary>
The effectiveness of POLIS was evaluated through a 27-person user study, where participants wrote programs aiming to maximize the scores of two single-agent games: Lunar Lander and Highway. The results showed that POLIS was able to significantly improve the participants' programs with respect to game scores. Additionally, a proof-of-concept demonstration on existing Stack Overflow code demonstrated the applicability of POLIS in real-world problems. These findings suggest that POLIS could be a useful programming assistant for programming problems with measurable objectives.Translated into Simplified Chinese:这篇论文介绍了一种基于可测量目标的地方搜索方法，用于改进现有的程序。该方法称为程序优化与本地改进搜索（POLIS），利用程序的线程结构，在保持其余线程固定的情况下，使用现有的毫干搜索算法，对程序中的单个线程进行改进，并继续迭代直到无法提高程序的性能。为证明POLIS的有用性，该论文进行了27名用户参与的实验，参与者需要通过设计两个单机游戏的分数来尝试提高他们的程序：月球降落和高速公路。结果表明，POLIS能够有效地改进参与者们的程序，增加分数。此外，对现有的Stack Overflow代码进行了一个证明性示例，以证明POLIS在实际问题中的应用可行性。这些结果表明，POLIS可能成为程序问题中的有用编程助手。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Solve-Constraint-Satisfaction-Problems-with-Recurrent-Transformer"><a href="#Learning-to-Solve-Constraint-Satisfaction-Problems-with-Recurrent-Transformer" class="headerlink" title="Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer"></a>Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04895">http://arxiv.org/abs/2307.04895</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/azreasoners/recurrent_transformer">https://github.com/azreasoners/recurrent_transformer</a></li>
<li>paper_authors: Zhun Yang, Adam Ishay, Joohyung Lee</li>
<li>for: 解决干式约束问题 (Constraint Satisfaction Problems, CSPs)</li>
<li>methods: 使用Transformer扩展了回归，结合Visual input和逻辑知识进行端到端学习</li>
<li>results: 提出了一种新的方法，可以在端到端学习中解决CSPs，并且可以使用逻辑知识进行 semi-supervised learning 和 sample-efficient learning<details>
<summary>Abstract</summary>
Constraint satisfaction problems (CSPs) are about finding values of variables that satisfy the given constraints. We show that Transformer extended with recurrence is a viable approach to learning to solve CSPs in an end-to-end manner, having clear advantages over state-of-the-art methods such as Graph Neural Networks, SATNet, and some neuro-symbolic models. With the ability of Transformer to handle visual input, the proposed Recurrent Transformer can straightforwardly be applied to visual constraint reasoning problems while successfully addressing the symbol grounding problem. We also show how to leverage deductive knowledge of discrete constraints in the Transformer's inductive learning to achieve sample-efficient learning and semi-supervised learning for CSPs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Choosing-Well-Your-Opponents-How-to-Guide-the-Synthesis-of-Programmatic-Strategies"><a href="#Choosing-Well-Your-Opponents-How-to-Guide-the-Synthesis-of-Programmatic-Strategies" class="headerlink" title="Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies"></a>Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04893">http://arxiv.org/abs/2307.04893</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rubensolv/locallearnerijcai">https://github.com/rubensolv/locallearnerijcai</a></li>
<li>paper_authors: Rubens O. Moraes, David S. Aleixo, Lucas N. Ferreira, Levi H. S. Lelis</li>
<li>for: 这篇论文旨在提供一种用于指导搜索算法的参考策略集，以提高在两个玩家零风险游戏中搜索策略的效果。</li>
<li>methods: 本论文提出了一种名为本地学习（2L）算法，该算法可以活动选择一组参考策略，以提高搜索信号。</li>
<li>results: 实验表明，使用2L算法可以比较IBR、FP和DO算法更好地学习参考策略，并在Synthesizing策略中提高搜索效果。此外，我们还通过模拟一场MicroRTS比赛，发现使用2L算法 synthesizer 可以比较两个最近的MicroRTS比赛的赢家，这些赢家都是由人工程师编写的程序策略。<details>
<summary>Abstract</summary>
This paper introduces Local Learner (2L), an algorithm for providing a set of reference strategies to guide the search for programmatic strategies in two-player zero-sum games. Previous learning algorithms, such as Iterated Best Response (IBR), Fictitious Play (FP), and Double-Oracle (DO), can be computationally expensive or miss important information for guiding search algorithms. 2L actively selects a set of reference strategies to improve the search signal. We empirically demonstrate the advantages of our approach while guiding a local search algorithm for synthesizing strategies in three games, including MicroRTS, a challenging real-time strategy game. Results show that 2L learns reference strategies that provide a stronger search signal than IBR, FP, and DO. We also simulate a tournament of MicroRTS, where a synthesizer using 2L outperformed the winners of the two latest MicroRTS competitions, which were programmatic strategies written by human programmers.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)这篇论文介绍了本地学习器（2L），一种算法用于提供指导搜索算法的参考策略集。先前的学习算法，如趋同最佳回应（IBR）、虚拟游戏（FP）和双重oracle（DO），可能具有计算成本高或搜索信号不准确的缺陷。2L活动选择一组参考策略，以提高搜索信号。我们通过对三个游戏进行实验，包括实时战略游戏MicroRTS，证明了2L所学习的参考策略比IBR、FP和DO更强的搜索信号。此外，我们还在MicroRTS的赛事中使用2L进行合成策略，并在赛事中击败了最近两届MicroRTS的赛事冠军，这些赛事冠军是由人工程师编写的程序策略。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Domain-Adaptation-with-Deep-Neural-Network"><a href="#Unsupervised-Domain-Adaptation-with-Deep-Neural-Network" class="headerlink" title="Unsupervised Domain Adaptation with Deep Neural-Network"></a>Unsupervised Domain Adaptation with Deep Neural-Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05601">http://arxiv.org/abs/2307.05601</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jetwev/domain-adaptation">https://github.com/jetwev/domain-adaptation</a></li>
<li>paper_authors: Artem Bituitskii</li>
<li>for: 这篇论文为了解决不监督领域适应问题提供分析现有方法、推出新方法，并在不同领域下进行视觉识别任务的改进。</li>
<li>methods: 这篇论文使用了现有方法的分析和一种新的方法，用于解决不同领域下的视觉识别任务。</li>
<li>results: 这篇论文的结果预示了适应领域下的视觉识别任务可以通过提高现有方法的性能来改进。<details>
<summary>Abstract</summary>
This report contributes to the field of unsupervised domain adaptation by providing an analysis of existing methods, introducing a new approach, and demonstrating the potential for improving visual recognition tasks across different domains. The results of this study open up opportunities for further study and development of advanced methods in the field of domain adaptation.
</details>
<details>
<summary>摘要</summary>
这份报告对不监督领域适应进行了分析，提出了一种新的方法，并证明了在不同领域中进行视觉识别任务的可能性。这些研究结果为领域适应领域的进一步研究开创了新的可能性。Here's a breakdown of the translation:* 这份报告 (zhè fā bào gāo) - This report* 对 (duì) - To* 不监督 (bù jiān dǎo) - Unsupervised* 领域 (lǐng yù) - Domain* 适应 (shì yìng) - Adaptation* 进行 (jìn cè) - To perform* 分析 (fēn xī) - Analysis* 提出 (tím zhè) - To propose* 一种 (yī zhǒng) - A new* 方法 (fāng fá) - Method* 并 (bìng) - And* 证明 (zhèng míng) - To demonstrate* 在 (zhè) - In* 不同 (bù tiěng) - Different* 领域 (lǐng yù) - Domains* 进行 (jìn cè) - To perform* 视觉识别 (shì jìng bìa) - Visual recognition* 任务 (réng zhì) - Tasks* 可能性 (kě néng xìng) - Possibilities* 开创 (kāi chuàng) - New possibilitiesI hope this helps! Let me know if you have any further questions.
</details></li>
</ul>
<hr>
<h2 id="Accelerated-Discovery-of-Machine-Learned-Symmetries-Deriving-the-Exceptional-Lie-Groups-G2-F4-and-E6"><a href="#Accelerated-Discovery-of-Machine-Learned-Symmetries-Deriving-the-Exceptional-Lie-Groups-G2-F4-and-E6" class="headerlink" title="Accelerated Discovery of Machine-Learned Symmetries: Deriving the Exceptional Lie Groups G2, F4 and E6"></a>Accelerated Discovery of Machine-Learned Symmetries: Deriving the Exceptional Lie Groups G2, F4 and E6</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04891">http://arxiv.org/abs/2307.04891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roy T. Forestano, Konstantin T. Matchev, Katia Matcheva, Alexander Roman, Eyup B. Unlu, Sarunas Verner</li>
<li>for: 这些研究用超vised深度学习来找到保持数据标签的连续对称变换，以及对应的对称生成器的代数。</li>
<li>methods: 这封信使用两种改进的算法来加速对称变换的发现，并用 sparse 形式表示发现的生成器。</li>
<li>results: 这些新算法对标准方法的性能进行了比较，并在对称群 $G_2$, $F_4$, $E_6$ 中发现了完整的生成器集。<details>
<summary>Abstract</summary>
Recent work has applied supervised deep learning to derive continuous symmetry transformations that preserve the data labels and to obtain the corresponding algebras of symmetry generators. This letter introduces two improved algorithms that significantly speed up the discovery of these symmetry transformations. The new methods are demonstrated by deriving the complete set of generators for the unitary groups U(n) and the exceptional Lie groups $G_2$, $F_4$, and $E_6$. A third post-processing algorithm renders the found generators in sparse form. We benchmark the performance improvement of the new algorithms relative to the standard approach. Given the significant complexity of the exceptional Lie groups, our results demonstrate that this machine-learning method for discovering symmetries is completely general and can be applied to a wide variety of labeled datasets.
</details>
<details>
<summary>摘要</summary>
最近的工作已经使用监督深度学习来找到保持数据标签的连续 симметry 变换，并获得相应的symmetry生成器的代数。这封信件介绍了两种改进的算法，可以很快速地找到这些symmetry变换。新方法在U(n)和G_2、F_4、E_6等特例 Lie grupos中找到了完整的生成器集。此外，我们还提出了一种后处理算法，将找到的生成器变换成简约的形式。我们对标准方法和新方法进行了性能比较，结果显示，对特例 Lie grupos来说，这种机器学习方法可以完全应用于各种标注数据集。
</details></li>
</ul>
<hr>
<h2 id="Measuring-and-Mitigating-Interference-in-Reinforcement-Learning"><a href="#Measuring-and-Mitigating-Interference-in-Reinforcement-Learning" class="headerlink" title="Measuring and Mitigating Interference in Reinforcement Learning"></a>Measuring and Mitigating Interference in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04887">http://arxiv.org/abs/2307.04887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Liu, Han Wang, Ruo Yu Tao, Khurram Javed, Adam White, Martha White</li>
<li>for: 本研究旨在提供一种定义和衡量值基的强化学习方法中的干扰量的方法，以及一种测试这种干扰量和控制性能的方法。</li>
<li>methods: 本研究使用了 fitted Q-iteration 和 DQN 等值基强化学习方法，并提出了一种新的干扰量测试方法。</li>
<li>results: 研究发现，该测试方法与控制性能的变化高度相关，并且可以用来评估不同网络架构和学习算法对干扰量的影响。此外，研究还提出了一种名为 “online-aware” 的算法，可以减少干扰量，并且在一些经典控制环境中提高了稳定性和性能。<details>
<summary>Abstract</summary>
Catastrophic interference is common in many network-based learning systems, and many proposals exist for mitigating it. Before overcoming interference we must understand it better. In this work, we provide a definition and novel measure of interference for value-based reinforcement learning methods such as Fitted Q-Iteration and DQN. We systematically evaluate our measure of interference, showing that it correlates with instability in control performance, across a variety of network architectures. Our new interference measure allows us to ask novel scientific questions about commonly used deep learning architectures and study learning algorithms which mitigate interference. Lastly, we outline a class of algorithms which we call online-aware that are designed to mitigate interference, and show they do reduce interference according to our measure and that they improve stability and performance in several classic control environments.
</details>
<details>
<summary>摘要</summary>
Catastrophic interference is a common problem in many network-based learning systems, and many proposals have been made to mitigate it. Before we can overcome interference, we must first understand it better. In this study, we provide a definition and a novel measure of interference for value-based reinforcement learning methods such as Fitted Q-Iteration and DQN. We systematically evaluate our measure of interference and show that it correlates with instability in control performance across a variety of network architectures. Our new interference measure allows us to ask novel scientific questions about commonly used deep learning architectures and study learning algorithms that mitigate interference. Finally, we outline a class of algorithms we call "online-aware" that are designed to mitigate interference, and show that they do reduce interference according to our measure and improve stability and performance in several classic control environments.
</details></li>
</ul>
<hr>
<h2 id="AI-For-Global-Climate-Cooperation-2023-Competition-Proceedings"><a href="#AI-For-Global-Climate-Cooperation-2023-Competition-Proceedings" class="headerlink" title="AI For Global Climate Cooperation 2023 Competition Proceedings"></a>AI For Global Climate Cooperation 2023 Competition Proceedings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06951">http://arxiv.org/abs/2307.06951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshua Bengio, Prateek Gupta, Lu Li, Soham Phade, Sunil Srinivasa, Andrew Williams, Tianyu Zhang, Yang Zhang, Stephan Zheng</li>
<li>For: The paper aims to design international frameworks for mitigating climate change and promoting sustainable economic growth, using AI-driven integrated assessment models (IAM) and simulations.* Methods: The paper uses RICE-N, an AI-driven IAM that supports modeling regional decision-making using AI agents, to model the climate-economic impact of decisions into the future. The proposals were evaluated both quantitatively and qualitatively, with a combination of performance metrics and human expert evaluation.* Results: The paper seeks to provide a promising solution to the challenges of collaboration in mitigating climate change and promoting sustainable economic growth, by combining AI with climate-economic simulations and involving human experts from multiple disciplines. The results of the competition and the improvements to RICE-N are expected to contribute to the development of effective and sustainable international frameworks for climate cooperation.<details>
<summary>Abstract</summary>
The international community must collaborate to mitigate climate change and sustain economic growth. However, collaboration is hard to achieve, partly because no global authority can ensure compliance with international climate agreements. Combining AI with climate-economic simulations offers a promising solution to design international frameworks, including negotiation protocols and climate agreements, that promote and incentivize collaboration. In addition, these frameworks should also have policy goals fulfillment, and sustained commitment, taking into account climate-economic dynamics and strategic behaviors. These challenges require an interdisciplinary approach across machine learning, economics, climate science, law, policy, ethics, and other fields.   Towards this objective, we organized AI for Global Climate Cooperation, a Mila competition in which teams submitted proposals and analyses of international frameworks, based on (modifications of) RICE-N, an AI-driven integrated assessment model (IAM). In particular, RICE-N supports modeling regional decision-making using AI agents. Furthermore, the IAM then models the climate-economic impact of those decisions into the future.   Whereas the first track focused only on performance metrics, the proposals submitted to the second track were evaluated both quantitatively and qualitatively. The quantitative evaluation focused on a combination of (i) the degree of mitigation of global temperature rise and (ii) the increase in economic productivity. On the other hand, an interdisciplinary panel of human experts in law, policy, sociology, economics and environmental science, evaluated the solutions qualitatively. In particular, the panel considered the effectiveness, simplicity, feasibility, ethics, and notions of climate justice of the protocols. In the third track, the participants were asked to critique and improve RICE-N.
</details>
<details>
<summary>摘要</summary>
国际社区必须合作以 Mitigate  климатиче变化并保持经济增长。然而，合作困难，其中一个原因是没有全球权威机构可以确保国际气候协议的遵从性。通过将 AI 与气候经济仿真模型相结合，可以开发出国际框架，包括谈判协议和气候协议，以便促进和激励合作。此外，这些框架还应该包括政策目标实现和持续承诺，考虑气候经济动态和策略行为。这些挑战需要跨学科的方法，包括机器学习、经济学、气候科学、法律、政策、伦理和其他领域。为了实现这个目标，我们组织了 AI for Global Climate Cooperation 竞赛，其中团队提交了国际框架的建议和分析，基于（修改后）的 RICE-N 气候经济仿真模型（IAM）。特别是，RICE-N 支持用 AI 代理模型地区决策。而 IAM 则模拟了这些决策对未来气候经济的影响。在第一个轨道中，只评估性能指标。而在第二个轨道中，提交的提案被评估了 both quantitatively 和 qualitatively。量化评估包括（i）全球气温升高的减轻程度和（ii）经济生产力的增长。然而，人类专家组成的多学科评审团（包括法律、政策、社会学、经济学和环境科学）对解决方案进行了质量评估。特别是，评审团考虑了效果、简洁、可行性、伦理和气候正义方面的评估。在第三个轨道中，参与者被要求批评并改进 RICE-N。
</details></li>
</ul>
<hr>
<h2 id="Onion-Universe-Algorithm-Applications-in-Weakly-Supervised-Learning"><a href="#Onion-Universe-Algorithm-Applications-in-Weakly-Supervised-Learning" class="headerlink" title="Onion Universe Algorithm: Applications in Weakly Supervised Learning"></a>Onion Universe Algorithm: Applications in Weakly Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04870">http://arxiv.org/abs/2307.04870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Woojoo Na</li>
<li>for: 本研究旨在提出一种新的分类方法，即 Onion Universe Algorithm (OUA)，用于弱监督学习。</li>
<li>methods: OUA 基于弱信号空间的几何解释，不需要任何假设，可以快速实现并且简单易用。</li>
<li>results: 实验结果表明，OUA 在常见的标准数据集上表现出色，比既有的标签模型更好。<details>
<summary>Abstract</summary>
We introduce Onion Universe Algorithm (OUA), a novel classification method in ensemble learning. In particular, we show its applicability as a label model for weakly supervised learning. OUA offers simplicity in implementation, computational efficiency, and does not rely on any assumptions regarding the data or weak signals. The model is well suited for scenarios where fully labeled data is not available. Our method is built upon geometrical interpretation of the space spanned by weak signals. Empirical results support our analysis of the hidden geometric structure underlying general set of weak signals and also illustrates that OUA works well in practice. We show empirical evidence that OUA performs favorably on common benchmark datasets compared to existing label models for weakly supervised learning.
</details>
<details>
<summary>摘要</summary>
我团队介绍了葱宇宙算法（OUA），一种新型的集成学习分类方法。具体来说，我们证明了它在弱监督学习中的应用性。OUA具有简单的实现、计算效率和不假设数据或弱信号的特点。该模型适用于具有受限数据的场景。我们的方法基于弱信号空间的几何解释。我们的分析表明，OUA在通用的弱信号集中隐藏的几何结构下适用。实验证明了OUA在实践中表现良好，并且与现有的弱监督学习标签模型相比，OUA的性能较好。
</details></li>
</ul>
<hr>
<h2 id="Fed-CPrompt-Contrastive-Prompt-for-Rehearsal-Free-Federated-Continual-Learning"><a href="#Fed-CPrompt-Contrastive-Prompt-for-Rehearsal-Free-Federated-Continual-Learning" class="headerlink" title="Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning"></a>Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04869">http://arxiv.org/abs/2307.04869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaurav Bagwe, Xiaoyong Yuan, Miao Pan, Lan Zhang</li>
<li>for: 这篇论文是针对 Federated Continual Learning (FCL) 的研究，尤其是在不需要练习的情况下学习多个任务。</li>
<li>methods: 本论文使用了 Prompt Learning 技术，通过强制学习 Task-specific 的描述来解决 FCL 中的忘记问题。它还 introduce two key components： asynchronous prompt learning 和 contrastive continual loss，以 Handling asynchronous task arrival 和 heterogeneous data distributions 在 FCL 中。</li>
<li>results: 实验结果显示 Fed-CPrompt 可以实现 SOTA 的 rehearsal-free FCL 性能。<details>
<summary>Abstract</summary>
Federated continual learning (FCL) learns incremental tasks over time from confidential datasets distributed across clients. This paper focuses on rehearsal-free FCL, which has severe forgetting issues when learning new tasks due to the lack of access to historical task data. To address this issue, we propose Fed-CPrompt based on prompt learning techniques to obtain task-specific prompts in a communication-efficient way. Fed-CPrompt introduces two key components, asynchronous prompt learning, and contrastive continual loss, to handle asynchronous task arrival and heterogeneous data distributions in FCL, respectively. Extensive experiments demonstrate the effectiveness of Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
</details>
<details>
<summary>摘要</summary>
联邦不断学习（FCL）逐渐学习课程，随着时间的推移，从静态分布在客户端上的机密数据集中学习新任务。这篇论文专注于无卷重复FCL，由于缺乏历史任务数据，因此受到严重的忘记问题困扰。为解决这个问题，我们提议了Fed-CPrompt，基于提示学习技术来获得任务特定的提示。Fed-CPrompt具有异步提示学习和对异构数据分布的对比连续损失两个关键组件，可以有效地处理FCL中的异步任务到达和不同数据分布问题。广泛的实验表明Fed-CPrompt可以实现SOTA的无卷重复FCL性能。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-an-Alignment-Set-in-Tackling-Instance-Dependent-Label-Noise"><a href="#Leveraging-an-Alignment-Set-in-Tackling-Instance-Dependent-Label-Noise" class="headerlink" title="Leveraging an Alignment Set in Tackling Instance-Dependent Label Noise"></a>Leveraging an Alignment Set in Tackling Instance-Dependent Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04868">http://arxiv.org/abs/2307.04868</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MLD3/Instance_Dependent_Label_Noise">https://github.com/MLD3/Instance_Dependent_Label_Noise</a></li>
<li>paper_authors: Donna Tjandra, Jenna Wiens</li>
<li>for: 这篇论文是为了解决受标签错误影响的模型性能问题。</li>
<li>methods: 这篇论文提出了一个two-stage方法来在标签错误中学习。这个方法使用了“anchor points”，一小部分数据，其标签已知。</li>
<li>results: 这篇论文的方法在多个任务上实现了显著的改善（AUROC），同时减少了偏见（AUEOC）。例如，在预测MIMIC-IIIdataset上的严重呼吸系统失常开始时，这篇论文的方法取得了0.84（SD 0.01）的和谐平均值（AUROC和AUEOC），比下一个最佳基eline的0.81（SD 0.01）高。总的来说，这篇论文的方法可以提高精度，同时减少可能的偏见。<details>
<summary>Abstract</summary>
Noisy training labels can hurt model performance. Most approaches that aim to address label noise assume label noise is independent from the input features. In practice, however, label noise is often feature or \textit{instance-dependent}, and therefore biased (i.e., some instances are more likely to be mislabeled than others). E.g., in clinical care, female patients are more likely to be under-diagnosed for cardiovascular disease compared to male patients. Approaches that ignore this dependence can produce models with poor discriminative performance, and in many healthcare settings, can exacerbate issues around health disparities. In light of these limitations, we propose a two-stage approach to learn in the presence instance-dependent label noise. Our approach utilizes \textit{\anchor points}, a small subset of data for which we know the observed and ground truth labels. On several tasks, our approach leads to consistent improvements over the state-of-the-art in discriminative performance (AUROC) while mitigating bias (area under the equalized odds curve, AUEOC). For example, when predicting acute respiratory failure onset on the MIMIC-III dataset, our approach achieves a harmonic mean (AUROC and AUEOC) of 0.84 (SD [standard deviation] 0.01) while that of the next best baseline is 0.81 (SD 0.01). Overall, our approach improves accuracy while mitigating potential bias compared to existing approaches in the presence of instance-dependent label noise.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Compositional-Generalization-from-First-Principles"><a href="#Compositional-Generalization-from-First-Principles" class="headerlink" title="Compositional Generalization from First Principles"></a>Compositional Generalization from First Principles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05596">http://arxiv.org/abs/2307.05596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brendel-group/compositional-ood-generalization">https://github.com/brendel-group/compositional-ood-generalization</a></li>
<li>paper_authors: Thaddäus Wiedemer, Prasanna Mayilvahanan, Matthias Bethge, Wieland Brendel</li>
<li>for: 本研究旨在探讨机器学习中的compositional generalization问题，即如何使模型能够通过学习数据的组成结构来泛化到新的数据集。</li>
<li>methods: 我们采用了一种底层的方法，通过对数据生成过程的分析，将compositional generalization问题转化为了一种数据生成问题。然后，我们提出了一些某种条件，这些条件只需要支持Training distribution和模型结构，即可以确保模型的泛化能力。</li>
<li>results: 我们的研究结果表明，在实际场景中，我们的方法可以有效地推广模型的泛化能力。此外，我们还进行了一些实验来验证我们的理论结论，并得到了正面的结果。<details>
<summary>Abstract</summary>
Leveraging the compositional nature of our world to expedite learning and facilitate generalization is a hallmark of human perception. In machine learning, on the other hand, achieving compositional generalization has proven to be an elusive goal, even for models with explicit compositional priors. To get a better handle on compositional generalization, we here approach it from the bottom up: Inspired by identifiable representation learning, we investigate compositionality as a property of the data-generating process rather than the data itself. This reformulation enables us to derive mild conditions on only the support of the training distribution and the model architecture, which are sufficient for compositional generalization. We further demonstrate how our theoretical framework applies to real-world scenarios and validate our findings empirically. Our results set the stage for a principled theoretical study of compositional generalization.
</details>
<details>
<summary>摘要</summary>
利用世界的compositional nature来加速学习和推广是人类视觉的一种特征。在机器学习中， however，实现compositional generalization是一个困难的目标，即使模型具有显式的compositional priors。为了更好地理解compositional generalization，我们在这里从底向上方法： inspirited by identifiable representation learning, we investigate compositionality as a property of the data-generating process rather than the data itself.这种重新定义允许我们 derive mild conditions on only the support of the training distribution and the model architecture, which are sufficient for compositional generalization. we further demonstrate how our theoretical framework applies to real-world scenarios and validate our findings empirically. our results set the stage for a principled theoretical study of compositional generalization.Note: "compositional nature" is translated as "世界的compositional nature" in Simplified Chinese, where "世界" (shì jiè) means "world" and "compositional" is an adjective.
</details></li>
</ul>
<hr>
<h2 id="Automated-Detection-of-Gait-Events-and-Travel-Distance-Using-Waist-worn-Accelerometers-Across-a-Typical-Range-of-Walking-and-Running-Speeds"><a href="#Automated-Detection-of-Gait-Events-and-Travel-Distance-Using-Waist-worn-Accelerometers-Across-a-Typical-Range-of-Walking-and-Running-Speeds" class="headerlink" title="Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds"></a>Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04866">http://arxiv.org/abs/2307.04866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Albara Ah Ramli, Xin Liu, Kelly Berndt, Chen-Nee Chuah, Erica Goude, Lynea B. Kaethler, Amanda Lopez, Alina Nicorici, Corey Owens, David Rodriguez, Jane Wang, Daniel Aranki, Craig M. McDonald, Erik K. Henricson</li>
<li>For: The paper aims to evaluate the accuracy of using accelerometer data from commercially-available smartphones to measure clinical features of gait (CFs) in children with Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using machine learning (ML)-based methods.* Methods: The study used a multi-step machine learning-based process to extract CFs from accelerometer data collected from 15 children with DMD and 15 TDs during supervised clinical testing across a range of gait speeds, including 10 or 25m run&#x2F;walk (10MRW, 25MRW), 100m run&#x2F;walk (100MRW), 6-minute walk (6MWT) and free-walk (FW) evaluations.* Results: The study found that the estimates of CFs obtained from the accelerometer data showed a strong correlation with ground-truth observation data, with a mean (SD) percentage error of 1.49% (7.04%) for step counts, 1.18% (9.91%) for distance traveled, and 0.37% (7.52%) for step length.Here is the information in Simplified Chinese text:* For: 这个研究是用来评估使用商业可用的智能手机陀螺仪数据来评估 Duchenne muscular dystrophy（DMD）和 typically developing controls（TDs）中的步行临床特征（CFs）的准确性的。* Methods: 这个研究使用了多步机器学习基于的过程来从陀螺仪数据中提取CFs，并在15个DMD儿童和15个TD儿童的指导临床测试中进行了多种步速评估，包括10米&#x2F;25米跑步（10MRW、25MRW）、100米跑步（100MRW）、6分钟步行（6MWT）和自由步行（FW）测试。* Results: 研究发现，通过陀螺仪数据获取的CFs估算与实际观察数据之间呈现了强相关关系，其中step count、距离旅行和步长的估算 errors的mean（SD）为1.49%（7.04%）、1.18%（9.91%）和0.37%（7.52%）。<details>
<summary>Abstract</summary>
Background: Estimation of temporospatial clinical features of gait (CFs), such as step count and length, step duration, step frequency, gait speed and distance traveled is an important component of community-based mobility evaluation using wearable accelerometers. However, challenges arising from device complexity and availability, cost and analytical methodology have limited widespread application of such tools. Research Question: Can accelerometer data from commercially-available smartphones be used to extract gait CFs across a broad range of attainable gait velocities in children with Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using machine learning (ML)-based methods Methods: Fifteen children with DMD and 15 TDs underwent supervised clinical testing across a range of gait speeds using 10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT) and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer at the waist near the body's center of mass. Gait CFs were extracted from the accelerometer data using a multi-step machine learning-based process and results were compared to ground-truth observation data. Results: Model predictions vs. observed values for step counts, distance traveled, and step length showed a strong correlation (Pearson's r = -0.9929 to 0.9986, p<0.0001). The estimates demonstrated a mean (SD) percentage error of 1.49% (7.04%) for step counts, 1.18% (9.91%) for distance traveled, and 0.37% (7.52%) for step length compared to ground truth observations for the combined 6MWT, 100MRW, and FW tasks. Significance: The study findings indicate that a single accelerometer placed near the body's center of mass can accurately measure CFs across different gait speeds in both TD and DMD peers, suggesting that there is potential for accurately measuring CFs in the community with consumer-level smartphones.
</details>
<details>
<summary>摘要</summary>
背景：评估社区基尼行动（CF），如步数和长度、步duration、步频、走速和总距离是评估社区基尼行动评估器（CB-GAIT）的重要组成部分。然而，设备复杂性和可用性、成本和分析方法的问题有限制了这些工具的广泛应用。研究问题：可以使用商业化手机的加速度仪数据来提取CF在各种可行的步速下的儿童 Duchenne muscular dystrophy（DMD）和正常发育 controls（TD）中使用机器学习（ML）方法来提取CF。方法：15名DMD儿童和15名TD儿童在不同的步速下进行了监测，包括10米跑步/跑步（10MRW）、25米跑步/跑步（25MRW）、100米跑步/跑步（100MRW）、6分钟步行（6MWT）和自由步行（FW）测试，同时穿着在腰部近身体中心的手机加速度仪。CF从加速度数据中提取使用多步骤机器学习基本过程，并与实际观测数据进行比较。结果：模型预测与实际观测值之间的相关性（Pearson的r = -0.9929到0.9986，p < 0.0001），并且估计结果表明了步数、总距离和步长的mean（SD）百分比误差为1.49%（7.04%）、1.18%（9.91%）和0.37%（7.52%），与实际观测值相比。意义：这些结果表明，一个单一的加速度仪可以在不同的步速下准确地测量CF，并且这些测量可以在社区中使用consumer级别的智能手机进行。
</details></li>
</ul>
<hr>
<h2 id="Articulated-3D-Head-Avatar-Generation-using-Text-to-Image-Diffusion-Models"><a href="#Articulated-3D-Head-Avatar-Generation-using-Text-to-Image-Diffusion-Models" class="headerlink" title="Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models"></a>Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04859">http://arxiv.org/abs/2307.04859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander W. Bergman, Wang Yifan, Gordon Wetzstein</li>
<li>for: 本研究旨在提供一种基于文本描述的3D人物头部生成方法，以满足人工智能、虚拟现实、电影制作和教育等领域的需求。</li>
<li>methods: 本研究使用了已经训练过的2D文本到图像扩散模型，直接生成3D-多视图一致的辐射场，以生成3D人物头部。新的优化方法可以保持2D和3D的表情特征相对应。</li>
<li>results: 研究表明，使用 diffusion-based 方法可以生成高质量的3D人物头部，并且可以在特定的领域内操作，例如人类头部。与之前的CLIP方法相比，我们的方法可以提供更高的多样性和准确性。<details>
<summary>Abstract</summary>
The ability to generate diverse 3D articulated head avatars is vital to a plethora of applications, including augmented reality, cinematography, and education. Recent work on text-guided 3D object generation has shown great promise in addressing these needs. These methods directly leverage pre-trained 2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance fields of generic objects. However, due to the lack of geometry and texture priors, these methods have limited control over the generated 3D objects, making it difficult to operate inside a specific domain, e.g., human heads. In this work, we develop a new approach to text-guided 3D head avatar generation to address this limitation. Our framework directly operates on the geometry and texture of an articulable 3D morphable model (3DMM) of a head, and introduces novel optimization procedures to update the geometry and texture while keeping the 2D and 3D facial features aligned. The result is a 3D head avatar that is consistent with the text description and can be readily articulated using the deformation model of the 3DMM. We show that our diffusion-based articulated head avatars outperform state-of-the-art approaches for this task. The latter are typically based on CLIP, which is known to provide limited diversity of generation and accuracy for 3D object generation.
</details>
<details>
<summary>摘要</summary>
“三维人物头像生成能力是许多应用中的重要能力，包括增强现实、电影拍摄和教育。现有的文本导向三维物体生成研究已经展示了很大的应用潜力。这些方法直接利用预训的二维文本扩散模型来生成三维多视角具有颜色场的对应物体。但由于缺乏几何和纹理偏好，这些方法对生成的三维物体有限的控制，很难在特定领域内运作，例如人头。在这个工作中，我们开发了一新的文本导向三维头像生成方法，以解决这个限制。我们的框架直接运算在头像3DMM中的几何和纹理，并引入了新的优化程序以更新几何和纹理，并保持2D和3D脸部特征相互Alignment。结果是一个跟文本描述相符的3D头像，可以轻松地运动使用3DMM的扭变模型。我们显示了我们的扩散基于3DMM的头像比以前的方法更高效。这些方法通常基于CLIP，CLIP知道提供有限的多样性和精度 для三维物体生成。”
</details></li>
</ul>
<hr>
<h2 id="SHAP-k-Efficient-and-Probably-Approximately-Correct-PAC-Identification-of-Top-k-Features"><a href="#SHAP-k-Efficient-and-Probably-Approximately-Correct-PAC-Identification-of-Top-k-Features" class="headerlink" title="SHAP@k:Efficient and Probably Approximately Correct (PAC) Identification of Top-k Features"></a>SHAP@k:Efficient and Probably Approximately Correct (PAC) Identification of Top-k Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04850">http://arxiv.org/abs/2307.04850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjay Kariyappa, Leonidas Tsepenekas, Freddy Lécué, Daniele Magazzeni</li>
<li>for: 本研究的目的是解释模型预测结果的原因，通过计算特征重要性。</li>
<li>methods: 本研究使用了SHAP框架，并引入了Top-k标识问题（TkIP），以解决高级特征选择问题。</li>
<li>results: 研究人员通过引入多重采样和适应采样策略，提高了现有方法的样本效率和运行时间，平均提高了5倍。<details>
<summary>Abstract</summary>
The SHAP framework provides a principled method to explain the predictions of a model by computing feature importance. Motivated by applications in finance, we introduce the Top-k Identification Problem (TkIP), where the objective is to identify the k features with the highest SHAP values. While any method to compute SHAP values with uncertainty estimates (such as KernelSHAP and SamplingSHAP) can be trivially adapted to solve TkIP, doing so is highly sample inefficient. The goal of our work is to improve the sample efficiency of existing methods in the context of solving TkIP. Our key insight is that TkIP can be framed as an Explore-m problem--a well-studied problem related to multi-armed bandits (MAB). This connection enables us to improve sample efficiency by leveraging two techniques from the MAB literature: (1) a better stopping-condition (to stop sampling) that identifies when PAC (Probably Approximately Correct) guarantees have been met and (2) a greedy sampling scheme that judiciously allocates samples between different features. By adopting these methods we develop KernelSHAP@k and SamplingSHAP@k to efficiently solve TkIP, offering an average improvement of $5\times$ in sample-efficiency and runtime across most common credit related datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The SHAP framework provides a principled method to explain the predictions of a model by computing feature importance. Motivated by applications in finance, we introduce the Top-k Identification Problem (TkIP), where the objective is to identify the k features with the highest SHAP values. While any method to compute SHAP values with uncertainty estimates (such as KernelSHAP and SamplingSHAP) can be trivially adapted to solve TkIP, doing so is highly sample inefficient. The goal of our work is to improve the sample efficiency of existing methods in the context of solving TkIP. Our key insight is that TkIP can be framed as an Explore-m problem--a well-studied problem related to multi-armed bandits (MAB). This connection enables us to improve sample efficiency by leveraging two techniques from the MAB literature: (1) a better stopping-condition (to stop sampling) that identifies when PAC (Probably Approximately Correct) guarantees have been met and (2) a greedy sampling scheme that judiciously allocates samples between different features. By adopting these methods we develop KernelSHAP@k and SamplingSHAP@k to efficiently solve TkIP, offering an average improvement of $5\times$ in sample-efficiency and runtime across most common credit related datasets."中文翻译：<<SYS>>SHAP框架提供一种原则正的方法来解释模型预测的原因，通过计算特征重要性。在金融应用场景下，我们引入了Top-k标识问题（TkIP）， objective是Identify k 最高 SHAP 值的特征。尽管任何可以计算 SHAP 值的不确定性估计（如 KernelSHAP 和 SamplingSHAP）可以轻松地适应 TkIP，但这样做是高度样本不效率的。我们的目标是提高现有方法的样本效率，在 TkIP 的 контекス中。我们的关键发现是，TkIP 可以视为一个 Explore-m 问题，这是一个已经研究了多臂枪（MAB）中的问题。这种连接使我们可以通过利用 MAB  литературе中的两种技术来提高样本效率：（1）更好的停止条件（以确保 PAC （Probably Approximately Correct）的保证）和（2）贪婪的采样方案，智能分配样本 между不同的特征。通过采用这些方法，我们开发了 KernelSHAP@k 和 SamplingSHAP@k，以有效地解决 TkIP，在大多数常见的借款相关数据集上提供了平均提高 $5\times$ 的样本效率和运行时间。
</details></li>
</ul>
<hr>
<h2 id="SigOpt-Mulch-An-Intelligent-System-for-AutoML-of-Gradient-Boosted-Trees"><a href="#SigOpt-Mulch-An-Intelligent-System-for-AutoML-of-Gradient-Boosted-Trees" class="headerlink" title="SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees"></a>SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04849">http://arxiv.org/abs/2307.04849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aleksei Sorokin, Xinran Zhu, Eric Hans Lee, Bolong Cheng</li>
<li>for: 提高Gradient Boosted Trees（GBTs）模型的hyperparameter优化</li>
<li>methods: 使用meta学和多信度优化技术进行模型 aware的hyperparameter优化，自动学习performant的hyperparameter</li>
<li>results: 比现有系统更高效地优化GBTs hyperparameter，减少用户域知识的需求，提供更易用的用户体验<details>
<summary>Abstract</summary>
Gradient boosted trees (GBTs) are ubiquitous models used by researchers, machine learning (ML) practitioners, and data scientists because of their robust performance, interpretable behavior, and ease-of-use. One critical challenge in training GBTs is the tuning of their hyperparameters. In practice, selecting these hyperparameters is often done manually. Recently, the ML community has advocated for tuning hyperparameters through black-box optimization and developed state-of-the-art systems to do so. However, applying such systems to tune GBTs suffers from two drawbacks. First, these systems are not \textit{model-aware}, rather they are designed to apply to a \textit{generic} model; this leaves significant optimization performance on the table. Second, using these systems requires \textit{domain knowledge} such as the choice of hyperparameter search space, which is an antithesis to the automatic experimentation that black-box optimization aims to provide. In this paper, we present SigOpt Mulch, a model-aware hyperparameter tuning system specifically designed for automated tuning of GBTs that provides two improvements over existing systems. First, Mulch leverages powerful techniques in metalearning and multifidelity optimization to perform model-aware hyperparameter optimization. Second, it automates the process of learning performant hyperparameters by making intelligent decisions about the optimization search space, thus reducing the need for user domain knowledge. These innovations allow Mulch to identify good GBT hyperparameters far more efficiently -- and in a more seamless and user-friendly way -- than existing black-box hyperparameter tuning systems.
</details>
<details>
<summary>摘要</summary>
Gradient Boosted Trees (GBTs) 是研究人员、机器学习（ML）专业人员和数据科学家们广泛使用的模型，因其性能 Robust 、可解释性和易用性。但在训练 GBTs 时，一个关键挑战是调整它们的超参数。在实践中，选择这些超参数通常是手动进行的。近年来，ML 社区强调通过黑盒优化进行超参数调整，并开发出了state-of-the-art 系统。但是，对 GBTs 进行黑盒优化具有两个缺点。首先，这些系统不是 GBTs 模型具有的，而是为普通模型而设计的，这会吃到大量的优化性能。第二，使用这些系统需要域知识，如选择超参数搜索空间，这与黑盒优化的自动实验相对抵触。在本文中，我们介绍了 SigOpt Mulch，一个特地设计用于自动调整 GBTs 超参数的模型具有优化系统。相比现有系统，Mulch 具有两个优势：首先，Mulch 利用了强大的元学习和多 fidelt 优化技术来进行模型具有的超参数优化。其次，它自动化了超参数优化的过程，从而减少了用户需要域知识的需求。这些创新使得 Mulch 可以更加高效地找到好的 GBTs 超参数，并且在更加易用和愉悦的方式进行自动化。
</details></li>
</ul>
<hr>
<h2 id="Dynamics-of-Temporal-Difference-Reinforcement-Learning"><a href="#Dynamics-of-Temporal-Difference-Reinforcement-Learning" class="headerlink" title="Dynamics of Temporal Difference Reinforcement Learning"></a>Dynamics of Temporal Difference Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04841">http://arxiv.org/abs/2307.04841</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pehlevan-group/td-rl-dynamics">https://github.com/pehlevan-group/td-rl-dynamics</a></li>
<li>paper_authors: Blake Bordelon, Paul Masset, Henry Kuo, Cengiz Pehlevan</li>
<li>for: 这篇论文旨在研究反射学习中参数和状态表示方法如何控制学习动态。</li>
<li>methods: 这篇论文使用统计物理概念来研究价值函数学习的时间差分学习 Curves。</li>
<li>results: 研究发现，在抽样 espacio 的情况下，涉及到随机漫步的 Stochastic semi-gradient noise 会导致价值错误出现极大板块，而不同于传统的梯度下降动力学。研究还发现，学习率渐变和奖励调整可以改善学习动态和板块。<details>
<summary>Abstract</summary>
Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent dynamics. We study how learning dynamics and plateaus depend on feature structure, learning rate, discount factor, and reward function. We then analyze how strategies like learning rate annealing and reward shaping can favorably alter learning dynamics and plateaus. To conclude, our work introduces new tools to open a new direction towards developing a theory of learning dynamics in reinforcement learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>基于实验的成功，强化学习已经在具有罕见反馈的环境中的多种应用中成功。然而，虽然有这些实际成功，但是还没有充分的理论理解强化学习模型参数和表示状态的特征之间如何控制学习动态。在这项工作中，我们使用统计物理学的概念，研究延迟差值学习值函数的时间梯度学习曲线的典型情况。我们的理论基于 Gaussian 等价假设，将 episodic 随机轨迹的平均值替换为时间相关的 Gaussian 特征平均值，并在小规模 Markov Decision Processes 上验证我们的假设。我们发现，由于 episodic 随机轨迹的抽样而导致的随机半gradient 噪声会在传统的梯度下降动力学中引起显著的板块，而不同于传统的梯度下降动力学。我们研究学习动态和板块如何受特征结构、学习率、折扣因子和奖励函数的影响。然后，我们分析如何通过学习率减退和奖励修饰来改善学习动态和板块。综上所述，我们的工作开启了一个新的方向，以开发强化学习动态学的理论。
</details></li>
</ul>
<hr>
<h2 id="CREPE-Learnable-Prompting-With-CLIP-Improves-Visual-Relationship-Prediction"><a href="#CREPE-Learnable-Prompting-With-CLIP-Improves-Visual-Relationship-Prediction" class="headerlink" title="CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction"></a>CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04838">http://arxiv.org/abs/2307.04838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llnl/crepe">https://github.com/llnl/crepe</a></li>
<li>paper_authors: Rakshith Subramanyam, T. S. Jayram, Rushil Anirudh, Jayaraman J. Thiagarajan</li>
<li>for: 这篇论文探讨了使用视觉语言模型（VLM），尤其是CLIP，预测视觉对象关系的潜力，从图像中提取语言基于关系。</li>
<li>methods: 我们采用了UVTransE关系预测框架，该框架学习关系为图像中的翻译嵌入。我们系统地探索CLIP中的subject、object和union-box表示方法，并提出了CREPE（CLIP表示增强预测）。CREPE使用了文本基于表示，并引入了一种新的对比训练策略来自动推理union-box的文本提示。</li>
<li>results: 我们的方法在Visual Genome benchmark上实现了 predicate estimation的状态aru-the-art性能，mR@5 27.79，mR@20 31.95，与最近的状态aru-the-art在mR@20上提高15.3%。这个工作证明了CLIP在对象关系预测中的效果，并鼓励了更多的研究在这个挑战性的领域。<details>
<summary>Abstract</summary>
In this paper, we explore the potential of Vision-Language Models (VLMs), specifically CLIP, in predicting visual object relationships, which involves interpreting visual features from images into language-based relations. Current state-of-the-art methods use complex graphical models that utilize language cues and visual features to address this challenge. We hypothesize that the strong language priors in CLIP embeddings can simplify these graphical models paving for a simpler approach. We adopt the UVTransE relation prediction framework, which learns the relation as a translational embedding with subject, object, and union box embeddings from a scene. We systematically explore the design of CLIP-based subject, object, and union-box representations within the UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate Estimation). CREPE utilizes text-based representations for all three bounding boxes and introduces a novel contrastive training strategy to automatically infer the text prompt for union-box. Our approach achieves state-of-the-art performance in predicate estimation, mR@5 27.79, and mR@20 31.95 on the Visual Genome benchmark, achieving a 15.3\% gain in performance over recent state-of-the-art at mR@20. This work demonstrates CLIP's effectiveness in object relation prediction and encourages further research on VLMs in this challenging domain.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探索了视觉语言模型（VLM），尤其是CLIP，在预测视觉对象关系方面的潜力。当前领先方法使用复杂的图形模型，利用语言提示和视觉特征来解决这个挑战。我们假设CLIP的强语言优先可以简化这些图形模型，为更简单的方法提供条件。我们采用UVTransE关系预测框架，该框架学习关系为图像中的翻译嵌入。我们系统地探索CLIP基于表示的主体、对象和联合盒子表示的设计，并提出CREPE（CLIP表示增强预测Predicate）。CREPE使用文本基于表示 для所有三个盒子，并 introduce了一种新的对比训练策略，自动推断联合盒子的文本提示。我们的方法在Visual Genome标准 benchmark上实现了预测 predicate 的状态对应性，MR@5 27.79，MR@20 31.95，与最近领先方法相比提高了15.3%的性能。这项工作证明CLIP在对象关系预测中的效iveness，并鼓励进一步的VLM在这个领域的研究。
</details></li>
</ul>
<hr>
<h2 id="Graph-Representation-of-the-Magnetic-Field-Topology-in-High-Fidelity-Plasma-Simulations-for-Machine-Learning-Applications"><a href="#Graph-Representation-of-the-Magnetic-Field-Topology-in-High-Fidelity-Plasma-Simulations-for-Machine-Learning-Applications" class="headerlink" title="Graph Representation of the Magnetic Field Topology in High-Fidelity Plasma Simulations for Machine Learning Applications"></a>Graph Representation of the Magnetic Field Topology in High-Fidelity Plasma Simulations for Machine Learning Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09469">http://arxiv.org/abs/2307.09469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioanna Bouri, Fanni Franssila, Markku Alho, Giulia Cozzani, Ivan Zaitsev, Minna Palmroth, Teemu Roos</li>
<li>for: study of magnetic reconnection in three-dimensional magnetic vector fields</li>
<li>methods: scalable pipeline for topological data analysis and spatiotemporal graph representation</li>
<li>results: demonstration on simulations of the Earth’s magnetosphere produced by Vlasiator<details>
<summary>Abstract</summary>
Topological analysis of the magnetic field in simulated plasmas allows the study of various physical phenomena in a wide range of settings. One such application is magnetic reconnection, a phenomenon related to the dynamics of the magnetic field topology, which is difficult to detect and characterize in three dimensions. We propose a scalable pipeline for topological data analysis and spatiotemporal graph representation of three-dimensional magnetic vector fields. We demonstrate our methods on simulations of the Earth's magnetosphere produced by Vlasiator, a supercomputer-scale Vlasov theory-based simulation for near-Earth space. The purpose of this work is to challenge the machine learning community to explore graph-based machine learning approaches to address a largely open scientific problem with wide-ranging potential impact.
</details>
<details>
<summary>摘要</summary>
topological 分析 магнитного场在模拟的气体中允许研究各种物理现象在广泛的设置下。一种应用是 магнит重连，这是关于磁场拓扑结构动态的现象，Difficult to detect and characterize in three dimensions. We propose a scalable pipeline for topological data analysis and spatiotemporal graph representation of three-dimensional magnetic vector fields. We demonstrate our methods on simulations of the Earth's magnetosphere produced by Vlasiator, a supercomputer-scale Vlasov theory-based simulation for near-Earth space. The purpose of this work is to challenge the machine learning community to explore graph-based machine learning approaches to address a largely open scientific problem with wide-ranging potential impact.Note: Simplified Chinese is used here, which is a more casual and widely-used version of Chinese. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Functional-PCA-and-Deep-Neural-Networks-based-Bayesian-Inverse-Uncertainty-Quantification-with-Transient-Experimental-Data"><a href="#Functional-PCA-and-Deep-Neural-Networks-based-Bayesian-Inverse-Uncertainty-Quantification-with-Transient-Experimental-Data" class="headerlink" title="Functional PCA and Deep Neural Networks-based Bayesian Inverse Uncertainty Quantification with Transient Experimental Data"></a>Functional PCA and Deep Neural Networks-based Bayesian Inverse Uncertainty Quantification with Transient Experimental Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05592">http://arxiv.org/abs/2307.05592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyu Xie, Mahmoud Yaseen, Xu Wu</li>
<li>for: 这个论文的目的是开发一种逆Quantification of Uncertainty（UQ）过程，用于量化模型输入不确定性基于实验数据。</li>
<li>methods: 这篇论文使用了功能归一分析（PCA）和深度神经网络（DNN）来建立一个快速的Surrogate模型，从而减少计算成本。</li>
<li>results: 该研究比较了不同的逆UQ过程，并结果表明，提posed方法可以更好地减少TRACE транзиент计算模型的维度，并且预测结果与实验数据更加一致。<details>
<summary>Abstract</summary>
Inverse UQ is the process to inversely quantify the model input uncertainties based on experimental data. This work focuses on developing an inverse UQ process for time-dependent responses, using dimensionality reduction by functional principal component analysis (PCA) and deep neural network (DNN)-based surrogate models. The demonstration is based on the inverse UQ of TRACE physical model parameters using the FEBA transient experimental data. The measurement data is time-dependent peak cladding temperature (PCT). Since the quantity-of-interest (QoI) is time-dependent that corresponds to infinite-dimensional responses, PCA is used to reduce the QoI dimension while preserving the transient profile of the PCT, in order to make the inverse UQ process more efficient. However, conventional PCA applied directly to the PCT time series profiles can hardly represent the data precisely due to the sudden temperature drop at the time of quenching. As a result, a functional alignment method is used to separate the phase and amplitude information of the transient PCT profiles before dimensionality reduction. DNNs are then trained using PC scores from functional PCA to build surrogate models of TRACE in order to reduce the computational cost in Markov Chain Monte Carlo sampling. Bayesian neural networks are used to estimate the uncertainties of DNN surrogate model predictions. In this study, we compared four different inverse UQ processes with different dimensionality reduction methods and surrogate models. The proposed approach shows an improvement in reducing the dimension of the TRACE transient simulations, and the forward propagation of inverse UQ results has a better agreement with the experimental data.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换为简化中文：<</SYS>>反UQ是量化模型输入不确定性的过程，基于实验数据。这项工作关注于开发一种反UQ过程，用于时间相依 responses，使用函数 principales component analysis (PCA) 和深度神经网络 (DNN) 模型来替代模型。示例基于TRACE物理模型参数的反UQ，使用FEBA过程数据。测量数据是时间相依的皮层温度 (PCT)。由于QoI是无穷维度的响应，PCA 用于减少 QoI 维度，保持温度过程的演变轨迹，以便更有效地进行反UQ。然而，直接应用常规 PCA 到 PCT 时间序列profile 的数据可能无法准确表示数据，因为冷却过程中的温度快速下降。因此，我们使用函数对齐方法分离 PCT 时间序列profile 的频率和幅度信息。然后，使用 PC  scores 从函数 PCA 训练 DNN 模型，以减少计算成本。bayesian neural networks 用于估计 DNN 模型预测结果的不确定性。在这项研究中，我们比较了不同的反UQ过程，包括不同的维度减少方法和模型。我们的方法显示可以更好地减少 TRACE 过程的维度，并且反UQ结果的前向传播与实验数据更好匹配。
</details></li>
</ul>
<hr>
<h2 id="SITTA-A-Semantic-Image-Text-Alignment-for-Image-Captioning"><a href="#SITTA-A-Semantic-Image-Text-Alignment-for-Image-Captioning" class="headerlink" title="SITTA: A Semantic Image-Text Alignment for Image Captioning"></a>SITTA: A Semantic Image-Text Alignment for Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05591">http://arxiv.org/abs/2307.05591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ml-jku/semantic-image-text-alignment">https://github.com/ml-jku/semantic-image-text-alignment</a></li>
<li>paper_authors: Fabian Paischer, Thomas Adler, Markus Hofmarcher, Sepp Hochreiter<br>for: 这 paper 的目的是提供一种将 Semantic 信息从视觉模型传输到生成型语言模型中，以实现图像描述的详细语言能力。methods: 该 paper 使用了两种新的建构方法来实现将 Semantic 信息从视觉模型的 embedding space 传输到生成型语言模型中，包括通过token对应关系和使用额外数据来构建直接从视觉空间到语言空间的映射。results: 该 paper 使用了这两种方法，在 MS-COCO 和 Flickr30k 数据集上实现了强大的描述性能，甚至在有限的数据情况下，与零参数和训练参数的竞争对手相比，有部分表现出色。具体来说，使用该方法，只需要250M参数的语言模型就可以生成不错的描述。这种方法使得图像描述更加可 accessible  для机构具有限制的计算资源。<details>
<summary>Abstract</summary>
Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embedding space of the pretrained LM via token correspondences. The latter leverages additional data that consists of image-text pairs to construct the mapping directly from vision to language space. Using our semantic mappings, we unlock image captioning for LMs without access to gradient information. By using different sources of data we achieve strong captioning performance on MS-COCO and Flickr30k datasets. Even in the face of limited data, our method partly exceeds the performance of other zero-shot and even finetuned competitors. Our ablation studies show that even LMs at a scale of merely 250M parameters can generate decent captions employing our semantic mappings. Our approach makes image captioning more accessible for institutions with restricted computational resources.
</details>
<details>
<summary>摘要</summary>
文本和 semantic 理解图像是图像描述的关键。这种理解需要检测对象、模型图像中对象之间的关系、Scene 的 semantics 评估，然后将抽象到语言空间中。为了实现良好的语言功能并确保图像语言映射，我们使用预训练的语言模型（LM） conditioned 在预训练的多modal（图像文本）模型中。这需要将图像表示的多modal 语言encoder 与 LM 的语言表示空间进行Alignment。然而，不知道如何最好地传递多modal 视觉encoder 中检测到的 semantics 到 LM。我们提出了两种新的方法，它们可以成功地在多modal 和 LM 的 embedding 空间之间传递 semantics。第一种方法是将多modal 语言encoder 的 embedding 空间与预训练 LM 的 embedding 空间进行Token 对应。第二种方法是通过使用更多的数据， direkt 从视觉到语言空间进行映射。使用我们的semantic mapping，我们可以在 LM 没有梯度信息的情况下实现图像描述。通过使用不同的数据源，我们在 MS-COCO 和 Flickr30k 数据集上达到了强大的描述性性能。即使 faced 有限的数据，我们的方法可以比其他零 shot 和精度调整的竞争对手表现更好。我们的抽象研究表明，即使 LM 的参数数量只有 250M，我们的方法仍可以生成不错的描述。我们的方法使图像描述更加可 accessible  для机构具有限制的计算资源。
</details></li>
</ul>
<hr>
<h2 id="Information-decomposition-to-identify-relevant-variation-in-complex-systems-with-machine-learning"><a href="#Information-decomposition-to-identify-relevant-variation-in-complex-systems-with-machine-learning" class="headerlink" title="Information decomposition to identify relevant variation in complex systems with machine learning"></a>Information decomposition to identify relevant variation in complex systems with machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04755">http://arxiv.org/abs/2307.04755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kieran A. Murphy, Dani S. Bassett</li>
<li>for: 本研究旨在提供一种实用、有效和通用的方法，以解compose the information contained in a set of measurements，以便更好地理解复杂系统的行为。</li>
<li>methods: 该方法基于分布式信息瓶颈作为学习目标，通过lossily compressing each measurement，对measurements的变化进行分类，并对不同量的预测信息进行排序。</li>
<li>results: 研究表明，该方法可以帮助分解复杂系统的信息，并在不同的预测量上提供更多的细节。在两个典型的复杂系统中（Boolean circuit和塑性变形材料），研究人员可以通过查看学习 compression scheme 来了解系统中关键的变化，并从而更好地理解系统的行为。<details>
<summary>Abstract</summary>
One of the fundamental steps toward understanding a complex system is identifying variation at the scale of the system's components that is most relevant to behavior on a macroscopic scale. Mutual information is a natural means of linking variation across scales of a system due to its independence of the particular functional relationship between variables. However, estimating mutual information given high-dimensional, continuous-valued data is notoriously difficult, and the desideratum -- to reveal important variation in a comprehensible manner -- is only readily achieved through exhaustive search. Here we propose a practical, efficient, and broadly applicable methodology to decompose the information contained in a set of measurements by lossily compressing each measurement with machine learning. Guided by the distributed information bottleneck as a learning objective, the information decomposition sorts variation in the measurements of the system state by relevance to specified macroscale behavior, revealing the most important subsets of measurements for different amounts of predictive information. Additional granularity is achieved by inspection of the learned compression schemes: the variation transmitted during compression is composed of distinctions among measurement values that are most relevant to the macroscale behavior. We focus our analysis on two paradigmatic complex systems: a Boolean circuit and an amorphous material undergoing plastic deformation. In both examples, specific bits of entropy are identified out of the high entropy of the system state as most related to macroscale behavior for insight about the connection between micro- and macro- in the complex system. The identification of meaningful variation in data, with the full generality brought by information theory, is made practical for the study of complex systems.
</details>
<details>
<summary>摘要</summary>
一个基本的步骤到理解复杂系统是识别系统组件的变化 scales 最 relevante to macroscopic behavior. 互补信息是一种自然地将变化 across scales of a system link 的方法，但是估计高维数据 Continuous valued mutual information 是非常困难的。我们提出了一种实用的、高效的和通用的方法，通过使用机器学习来压缩每个测量，以实现信息剖析。我们的方法基于分布式信息瓶颈作为学习目标，通过对测量集进行损失压缩，对测量集中的信息进行分类，并对不同Amount of predictive information 的情况进行分类。通过 inspecting the learned compression schemes ，我们可以获得额外的细化，并识别出系统状态测量中最相关的变化。我们的分析涉及到两种典型的复杂系统：布尔电路和杂质材料在塑性变形过程中。在这两个例子中，我们可以通过对系统状态测量中的高 entropy 进行分类，并识别出与 macroscopic behavior 相关的特定比特 entropy。通过这种方法，我们可以实际地识别复杂系统数据中的有用变化，并且通过信息理论来获得全面的一般性。
</details></li>
</ul>
<hr>
<h2 id="Shelving-Stacking-Hanging-Relational-Pose-Diffusion-for-Multi-modal-Rearrangement"><a href="#Shelving-Stacking-Hanging-Relational-Pose-Diffusion-for-Multi-modal-Rearrangement" class="headerlink" title="Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement"></a>Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04751">http://arxiv.org/abs/2307.04751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Simeonov, Ankit Goyal, Lucas Manuelli, Lin Yen-Chen, Alina Sarmiento, Alberto Rodriguez, Pulkit Agrawal, Dieter Fox</li>
<li>for: 提供了一个系统来重新排序场景中的物品，以达到想要的物品-场景排序关系，如一本书插入开放架上的槽中。</li>
<li>methods: 使用了一个管线，处理了三维点云的变化，并从示范训练出来运作。系统可以处理不同的场景和物品的几种对称和位置，并且可以快速地处理多种不同的重新排序任务。</li>
<li>results: 系统可以实现多种不同的重新排序任务，包括处理多模式和物体形状和位置的变化。实验和真实世界中的评估结果显示，系统可以精确地完成重新排序任务，并且可以快速地处理多种不同的任务。<details>
<summary>Abstract</summary>
We propose a system for rearranging objects in a scene to achieve a desired object-scene placing relationship, such as a book inserted in an open slot of a bookshelf. The pipeline generalizes to novel geometries, poses, and layouts of both scenes and objects, and is trained from demonstrations to operate directly on 3D point clouds. Our system overcomes challenges associated with the existence of many geometrically-similar rearrangement solutions for a given scene. By leveraging an iterative pose de-noising training procedure, we can fit multi-modal demonstration data and produce multi-modal outputs while remaining precise and accurate. We also show the advantages of conditioning on relevant local geometric features while ignoring irrelevant global structure that harms both generalization and precision. We demonstrate our approach on three distinct rearrangement tasks that require handling multi-modality and generalization over object shape and pose in both simulation and the real world. Project website, code, and videos: https://anthonysimeonov.github.io/rpdiff-multi-modal/
</details>
<details>
<summary>摘要</summary>
我们提出了一个系统，用于将对象重新排序Scene以实现想要的对象-Scene的放置关系，如一本书插入开放架上的槽中。我们的管道可以普适到新的几何结构、姿态和布局，并从示例中训练直接操作3D点云。我们的系统可以超越对于给定场景中的多个几何相似的重新排序解。通过利用循环pose减噪训练过程，我们可以适应多模态示例数据，并且生成多模态输出，同时保持精度和准确。我们还发现，通过关注相关的本地几何特征，而忽略无关的全局结构，可以提高both泛化和精度。我们在三个不同的重新排序任务中证明了我们的方法的优势，这些任务需要处理多模态和对象形状和姿态的泛化。项目网站、代码和视频：https://anthonysimeonov.github.io/rpdiff-multi-modal/
</details></li>
</ul>
<hr>
<h2 id="Divide-Evaluate-and-Refine-Evaluating-and-Improving-Text-to-Image-Alignment-with-Iterative-VQA-Feedback"><a href="#Divide-Evaluate-and-Refine-Evaluating-and-Improving-Text-to-Image-Alignment-with-Iterative-VQA-Feedback" class="headerlink" title="Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback"></a>Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04749">http://arxiv.org/abs/2307.04749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaskirat Singh, Liang Zheng</li>
<li>for:  This paper aims to improve the accuracy of text-to-image alignment in latent diffusion models, which have shown remarkable progress in the field of text-conditioned image generation.</li>
<li>methods: The proposed approach uses a decompositional approach to evaluate and improve text-to-image alignment. This involves introducing a Decompositional-Alignment-Score, which decomposes a complex prompt into a set of disjoint assertions and measures the alignment of each assertion with generated images using a VQA model.</li>
<li>results: The proposed approach shows significantly higher correlation with human ratings compared to traditional CLIP and BLIP scores, and also provides useful feedback in the form of assertion level alignment scores. Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy.Here is the same information in Simplified Chinese text:</li>
<li>for: 这个研究旨在提高文本到图像对适性的准确性，文本conditioned图像生成领域内，latest扩散模型已经做出了无 precedent的进步。</li>
<li>methods: 提议的方法使用分解法来评估和改进文本到图像对适性。这里引入了Decompositional-Alignment-Score，将复杂的提示分解成一系列独立的声明，然后使用VQA模型测量每个声明与生成图像之间的对适性。</li>
<li>results: 提议的方法与人类评分之间显著相关性高于传统的CLIP和BLIP分数，同时也提供了有用的反馈，即声明级对适性分数。人类用户研究表明，提议的方法比前一代最佳方法提高了8.7%的全面文本到图像对适性精度。<details>
<summary>Abstract</summary>
The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as CLIP. To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. In particular, we first introduce a Decompositional-Alignment-Score which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. Experimental analysis reveals that the proposed alignment metric shows significantly higher correlation with human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also find that the assertion level alignment scores provide a useful feedback which can then be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs. Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy. Project page for our paper is available at https://1jsingh.github.io/divide-evaluate-and-refine
</details>
<details>
<summary>摘要</summary>
“文本调整图像生成领域在最近的几年内取得了无 precedent 的进步，特别是透过潜在散射模型。然而，当文本输入变得越来越复杂时，现代的散射模型可能仍然无法生成具有准确传递 semantics 的图像。此外，实验表明，这些不一致性常常会被先验的多modal模型如 CLIP 掩盖。为了解决这些问题，在本文中我们探索了一个简单 yet effective 的分解分析方法，包括：（1）将复杂的文本提示分解为一系列不耦合的宣告（Decompositional-Alignment-Score）；（2）使用 VQA 模型评估每个宣告与生成的图像之间的Alignment度；（3）将不同宣告的Alignment度联合后续构成最终的文本至图像Alignment度。实验分析显示，我们提出的Alignment度指标与人工评分呈现高度相关，并且比Traditional CLIP、BLIP 分数更高。此外，我们发现 assertion 阶层Alignment度也提供了有用的反馈，可以用于一个简单的迭代程序，逐步增加生成图像中不同宣告的表达。人工用户研究显示，我们的方法与前一代最佳状态差异8.7%。”Note: The translation is done using Google Translate, and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="RoCo-Dialectic-Multi-Robot-Collaboration-with-Large-Language-Models"><a href="#RoCo-Dialectic-Multi-Robot-Collaboration-with-Large-Language-Models" class="headerlink" title="RoCo: Dialectic Multi-Robot Collaboration with Large Language Models"></a>RoCo: Dialectic Multi-Robot Collaboration with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04738">http://arxiv.org/abs/2307.04738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MandiZhao/robot-collab">https://github.com/MandiZhao/robot-collab</a></li>
<li>paper_authors: Zhao Mandi, Shreeya Jain, Shuran Song</li>
<li>for: 这个论文旨在探讨多机器人合作的新方法，利用预训大量语言模型（LLM）来实现高层次通信和低层次路径规划。</li>
<li>methods: 在这个方法中，机器人具有LLM，可以集体推理和讨论任务策略。它们产生了子任务计划和任务空间径路径，这些路径被用于加速曲线规划。此外，环境反馈，例如碰撞检查，并提醒LLM代理从中 improvise它们的计划和径路。</li>
<li>results: 在 RoCoBench  benchmark 中，这个方法得到了高成功率，并能够适应任务 semantics 的变化。对话设置具有高可读性和灵活性，在实际世界实验中，RoCo 可以与人合作完成任务。<details>
<summary>Abstract</summary>
We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website https://project-roco.github.io for videos and code.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的多机器人合作方法，利用预训练的大型自然语言模型（LLM）来实现高级沟通和低级路径规划。机器人通过LLM进行集体理解任务策略，然后生成子任务计划和任务空间弧轨路径，这些路径被用于加速曲线规划。我们还提供了环境反馈，如碰撞检测，并让LLM代理更新其计划和弧轨路径。为评估，我们提出了RoCoBench，一个6个任务的benchmark，涵盖了多机器人合作场景的各种情况，并附带了文本数据集，用于代理表示和理解。我们的对话设置具有高可读性和灵活性，在实际世界实验中，我们示例了RoCo可以轻松地与人类在Loop合作，以完成任务。更多信息请访问项目网站https://project-roco.github.io。
</details></li>
</ul>
<hr>
<h2 id="A-unifying-framework-for-differentially-private-quantum-algorithms"><a href="#A-unifying-framework-for-differentially-private-quantum-algorithms" class="headerlink" title="A unifying framework for differentially private quantum algorithms"></a>A unifying framework for differentially private quantum algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04733">http://arxiv.org/abs/2307.04733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Armando Angrisani, Mina Doosti, Elham Kashefi</li>
<li>for: 本研究旨在提供一种通用的量子隐私定义，以保护敏感信息的处理。</li>
<li>methods: 本文提出了一种新的量子隐私定义，基于量子状态的近似性。此外，本文还提出了一种将类型和量子噪声添加到混合噪声中的方法，以提供更加紧密的隐私保障。</li>
<li>results: 本文的研究结果表明，使用该新的量子隐私定义和混合噪声方法可以提供更加紧密的隐私保障，同时具有较小的失真率。此外，本文还证明了量子隐私的先进共轨性，并应用到量子隐私中。<details>
<summary>Abstract</summary>
Differential privacy is a widely used notion of security that enables the processing of sensitive information. In short, differentially private algorithms map "neighbouring" inputs to close output distributions. Prior work proposed several quantum extensions of differential privacy, each of them built on substantially different notions of neighbouring quantum states. In this paper, we propose a novel and general definition of neighbouring quantum states. We demonstrate that this definition captures the underlying structure of quantum encodings and can be used to provide exponentially tighter privacy guarantees for quantum measurements. Our approach combines the addition of classical and quantum noise and is motivated by the noisy nature of near-term quantum devices. Moreover, we also investigate an alternative setting where we are provided with multiple copies of the input state. In this case, differential privacy can be ensured with little loss in accuracy combining concentration of measure and noise-adding mechanisms. En route, we prove the advanced joint convexity of the quantum hockey-stick divergence and we demonstrate how this result can be applied to quantum differential privacy. Finally, we complement our theoretical findings with an empirical estimation of the certified adversarial robustness ensured by differentially private measurements.
</details>
<details>
<summary>摘要</summary>
differential privacy 是一种广泛使用的安全概念，允许处理敏感信息。简而言之， differentially private 算法将 "邻近" 输入映射到相似的输出分布。先前的工作已经提出了多种量子扩展 differential privacy，每一种都基于不同的量子邻近状态的几何。在这篇论文中，我们提出了一个新的和通用的量子邻近状态定义。我们示出了这个定义捕捉了量子编码的下面结构，可以提供 exponentially 紧密的隐私保证 для量子测量。我们的方法 combinest  классической和量子噪声的添加，被动机是近期的量子设备的噪声性。此外，我们还 investigate了另一种情况，在这种情况下，我们被提供多份输入状态。在这种情况下， differential privacy 可以在减少精度的情况下确保。在路过中，我们证明了量子奶酪散度的高级联合几何性，并示出了如何将这一结果应用于量子隐私。最后，我们补充了我们的理论发现，通过实际的验证证明了 differentially private 测量确保的抗对抗性。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Policies-for-Out-of-Distribution-Generalization-in-Offline-Reinforcement-Learning"><a href="#Diffusion-Policies-for-Out-of-Distribution-Generalization-in-Offline-Reinforcement-Learning" class="headerlink" title="Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning"></a>Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04726">http://arxiv.org/abs/2307.04726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suzan Ece Ada, Erhan Oztop, Emre Ugur</li>
<li>for: 本研究旨在提高Offline Reinforcement Learning（RL）方法的灵活性和可靠性，以便在不同的环境中学习更好的策略。</li>
<li>methods: 本研究使用了Diffusion Policies，并添加了State Reconstruction Feature Learning来解决Out-of-distribution扩展问题。</li>
<li>results: 本研究在2D Multimodal Contextual Bandit环境中实现了State-of-the-art的性能，并在多个D4RLbenchmark任务上也达到了优秀的结果。<details>
<summary>Abstract</summary>
Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for experience collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to obtain expressive policies to represent multimodal behavior in the dataset. Nevertheless, they are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descriptive representation learning of states to alleviate the distribution shift incurred by the out-of-distribution states. We design a 2D Multimodal Contextual Bandit environment to demonstrate and evaluate our proposed model. We assess the performance of our model not only in this new environment but also on several D4RL benchmark tasks, achieving state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for experience collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to obtain expressive policies to represent multimodal behavior in the dataset. Nevertheless, they are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descriptive representation learning of states to alleviate the distribution shift incurred by the out-of-distribution states. We design a 2D Multimodal Contextual Bandit environment to demonstrate and evaluate our proposed model. We assess the performance of our model not only in this new environment but also on several D4RL benchmark tasks, achieving state-of-the-art results."into Simplified Chinese:<<SYS>>在线RL方法可以利用先前的经验学习更好的策略，而不是使用行为策略来收集经验。与行为克隆不同，在线RL可以处理非专家数据和多Modal行为策略。然而，在线RL算法面临着处理分布偏移和有效表达策略的挑战。先前的在线RL使用条件扩散模型获得表达性策略来表示数据集中的多Modal行为。然而，它们不是专门解决异常状态泛化问题。我们提出了一种新的方法，利用状态重建特征学习在最近的扩散策略中来解决异常状态泛化问题。状态重建损失使得状态表示学习更加描述性，以适应数据集中的分布偏移。我们设计了2D多Modal上下文随机策略环境，用以评估和评测我们的提议模型。我们不仅在这个新环境中评估我们的模型，还在D4RL benchmark任务上达到了最佳成绩。
</details></li>
</ul>
<hr>
<h2 id="AnimateDiff-Animate-Your-Personalized-Text-to-Image-Diffusion-Models-without-Specific-Tuning"><a href="#AnimateDiff-Animate-Your-Personalized-Text-to-Image-Diffusion-Models-without-Specific-Tuning" class="headerlink" title="AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning"></a>AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04725">http://arxiv.org/abs/2307.04725</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guoyww/animatediff">https://github.com/guoyww/animatediff</a></li>
<li>paper_authors: Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai</li>
<li>for: 提出了一个实用的框架，以便使得大多数现有的个性化文本到图像模型都能够通过一次性调整生成动画图像。</li>
<li>methods: 核心思想是插入一个新初始化的动态模型模块到冻结的文本到图像模型中，并在视频clip上进行训练，以储存合理的动作假设。</li>
<li>results: 对多种公共代表性的个性化文本到图像模型进行评估，并示出了该框架可以帮助这些模型生成满足时间滑动的动画clip，同时保持域和多样性的输出。<details>
<summary>Abstract</summary>
With the advance of text-to-image models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. Subsequently, there is a great demand for image animation techniques to further combine generated static images with motion dynamics. In this report, we propose a practical framework to animate most of the existing personalized text-to-image models once and for all, saving efforts in model-specific tuning. At the core of the proposed framework is to insert a newly initialized motion modeling module into the frozen text-to-image model and train it on video clips to distill reasonable motion priors. Once trained, by simply injecting this motion modeling module, all personalized versions derived from the same base T2I readily become text-driven models that produce diverse and personalized animated images. We conduct our evaluation on several public representative personalized text-to-image models across anime pictures and realistic photographs, and demonstrate that our proposed framework helps these models generate temporally smooth animation clips while preserving the domain and diversity of their outputs. Code and pre-trained weights will be publicly available at https://animatediff.github.io/ .
</details>
<details>
<summary>摘要</summary>
随着文本到图像模型（如稳定扩散）和相关个性化技术（如梦 Broadcast和LoRA）的发展，现在任何人都可以将想象力转化成高质量图像，并且在便宜的成本下进行。然而，随后需要图像动画技术来将生成的静止图像与动态运动结合起来。在这份报告中，我们提出一个实用的框架，可以将大多数现有的个性化文本到图像模型动画一遍，从而避免模型特定的调整。核心思想是插入一个新初始化的运动模型模块到冻结的文本到图像模型中，然后在视频clip上进行准则学习，以提取合理的运动偏好。一旦训练完成，只需将这个运动模型模块注入到基础T2I的所有个性化版本中，即使这些版本都是基于同一个基础T2I模型的，也可以通过文本驱动生成个性化的动画图像。我们在各种公共代表性的个性化文本到图像模型上进行了评估，包括漫画图片和真实摄影，并证明了我们提出的框架可以帮助这些模型生成 temporally Smooth的动画clip，保持域和多样性的输出。代码和预训练 веса将在https://animatediff.github.io/公共上发布。
</details></li>
</ul>
<hr>
<h2 id="Advances-and-Challenges-in-Meta-Learning-A-Technical-Review"><a href="#Advances-and-Challenges-in-Meta-Learning-A-Technical-Review" class="headerlink" title="Advances and Challenges in Meta-Learning: A Technical Review"></a>Advances and Challenges in Meta-Learning: A Technical Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04722">http://arxiv.org/abs/2307.04722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anna Vettoruzzo, Mohamed-Rafik Bouguelia, Joaquin Vanschoren, Thorsteinn Rögnvaldsson, KC Santosh</li>
<li>for: 这篇评论旨在提供一个全面的技术概述，探讨meta-learning在实际应用中的重要性，以及它如何帮助学习系统从多个任务中获得知识，以更快地适应和泛化到新任务。</li>
<li>methods: 评论涵盖了当前meta-learning领域的state-of-the-art方法，并探讨了meta-learning与多任务学习、传输学习、领域适应和泛化、自我指导学习、个性化联合学习以及持续学习的关系。</li>
<li>results: 评论总结了当前领域的最新研究发展，并提出了未解决的问题和挑战，以便未来研究者可以更好地发挥创新力和积极性。<details>
<summary>Abstract</summary>
Meta-learning empowers learning systems with the ability to acquire knowledge from multiple tasks, enabling faster adaptation and generalization to new tasks. This review provides a comprehensive technical overview of meta-learning, emphasizing its importance in real-world applications where data may be scarce or expensive to obtain. The paper covers the state-of-the-art meta-learning approaches and explores the relationship between meta-learning and multi-task learning, transfer learning, domain adaptation and generalization, self-supervised learning, personalized federated learning, and continual learning. By highlighting the synergies between these topics and the field of meta-learning, the paper demonstrates how advancements in one area can benefit the field as a whole, while avoiding unnecessary duplication of efforts. Additionally, the paper delves into advanced meta-learning topics such as learning from complex multi-modal task distributions, unsupervised meta-learning, learning to efficiently adapt to data distribution shifts, and continual meta-learning. Lastly, the paper highlights open problems and challenges for future research in the field. By synthesizing the latest research developments, this paper provides a thorough understanding of meta-learning and its potential impact on various machine learning applications. We believe that this technical overview will contribute to the advancement of meta-learning and its practical implications in addressing real-world problems.
</details>
<details>
<summary>摘要</summary>
Meta-学习授予学习系统多个任务知识的能力，以便更快地适应和泛化新任务。本文提供了关于Meta-学习的全面技术综述，强调其在实际应用中的重要性，特别是数据可能罕见或便宜得不到的情况下。文章涵盖了当前Meta-学习领域的state-of-the-art方法，并探讨Meta-学习和多任务学习、传输学习、领域适应和泛化、自动学习、个性化联合学习和持续学习之间的关系。文章指出这些话题之间的相互关系，并表明在一个领域进步可以对另一个领域产生积极影响，而不需要重复努力。此外，文章还探讨了Meta-学习高级主题，如从复杂多Modal任务分布学习、无监督Meta-学习、高效地适应数据分布变化学习和持续Meta-学习。最后，文章揭示了未解决的问题和未来研究的挑战。通过总结最新的研究发展，本文提供了Meta-学习的全面理解，以及其在不同机器学习应用中的实际影响。我们认为这种技术综述将对Meta-学习的进一步发展和实际应用产生贡献。
</details></li>
</ul>
<hr>
<h2 id="On-the-curvature-of-the-loss-landscape"><a href="#On-the-curvature-of-the-loss-landscape" class="headerlink" title="On the curvature of the loss landscape"></a>On the curvature of the loss landscape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04719">http://arxiv.org/abs/2307.04719</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Enosh-P/Study-on-Loss-Landscape-Geometry-for-Improving-Generalization-in-Adaptive-Optimization-Methods">https://github.com/Enosh-P/Study-on-Loss-Landscape-Geometry-for-Improving-Generalization-in-Adaptive-Optimization-Methods</a></li>
<li>paper_authors: Alison Pouplin, Hrittik Roy, Sidak Pal Singh, Georgios Arvanitidis</li>
<li>for: Understanding the generalization abilities of over-parameterized deep learning models.</li>
<li>methods: Analyzing the loss landscape as an embedded Riemannian manifold, focusing on the scalar curvature.</li>
<li>results: Connections between the scalar curvature and generalization in deep learning models.Here’s the same information in Simplified Chinese:</li>
<li>for: 了解深度学习模型的泛化能力。</li>
<li>methods: 分析损失ландшаф特为嵌入的里曼尼投影，关注scalar curvature。</li>
<li>results: 关于scalar curvature和泛化的连接。<details>
<summary>Abstract</summary>
One of the main challenges in modern deep learning is to understand why such over-parameterized models perform so well when trained on finite data. A way to analyze this generalization concept is through the properties of the associated loss landscape. In this work, we consider the loss landscape as an embedded Riemannian manifold and show that the differential geometric properties of the manifold can be used when analyzing the generalization abilities of a deep net. In particular, we focus on the scalar curvature, which can be computed analytically for our manifold, and show connections to several settings that potentially imply generalization.
</details>
<details>
<summary>摘要</summary>
（一个主要挑战在现代深度学习中是理解如何over-parameterized模型在finite数据上表现得如此出色。我们可以通过损失景观的属性来分析这种泛化能力。在这个工作中，我们将损失景观视为一个嵌入在Riemannian多重空间中的抽象 manifold，并证明了这种多重空间的几何属性可以用于分析深度网络的泛化能力。特别是，我们将关注scalar curvature，可以在我们的 manifold 上计算analytically，并显示了与several settings的连接，这些设置可能导致泛化。）
</details></li>
</ul>
<hr>
<h2 id="Cobalt-Optimizing-Mining-Rewards-in-Proof-of-Work-Network-Games"><a href="#Cobalt-Optimizing-Mining-Rewards-in-Proof-of-Work-Network-Games" class="headerlink" title="Cobalt: Optimizing Mining Rewards in Proof-of-Work Network Games"></a>Cobalt: Optimizing Mining Rewards in Proof-of-Work Network Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04695">http://arxiv.org/abs/2307.04695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arti Vedula, Abhishek Gupta, Shaileshh Bojja Venkatakrishnan</li>
<li>for: 提高挖矿 reward 的最佳方式</li>
<li>methods: 使用 combinatorial bandit 算法，利用网络坐标来学习网络结构</li>
<li>results: 对多种网络设置进行实验，提出的方法能够超过或匹配基线方法的性能<details>
<summary>Abstract</summary>
Mining in proof-of-work blockchains has become an expensive affair requiring specialized hardware capable of executing several megahashes per second at huge electricity costs. Miners earn a reward each time they mine a block within the longest chain, which helps offset their mining costs. It is therefore of interest to miners to maximize the number of mined blocks in the blockchain and increase revenue. A key factor affecting mining rewards earned is the connectivity between miners in the peer-to-peer network. To maximize rewards a miner must choose its network connections carefully, ensuring existence of paths to other miners that are on average of a lower latency compared to paths between other miners. We formulate the problem of deciding whom to connect to for miners as a combinatorial bandit problem. Each node picks its neighbors strategically to minimize the latency to reach 90\% of the hash power of the network relative to the 90-th percentile latency from other nodes. A key contribution of our work is the use of a network coordinates based model for learning the network structure within the bandit algorithm. Experimentally we show our proposed algorithm outperforming or matching baselines on diverse network settings.
</details>
<details>
<summary>摘要</summary>
钱币证明（proof-of-work）分布式区块链中的矿工活动已成为一项昂贵的 affair，需要特殊的硬件，可以每秒执行几百万次的Hash算法，并且需要巨大的电力成本。矿工每次在最长链中挖矿一个块，就会获得一定的奖励，这有助于 offset 矿工的矿工成本。因此，矿工想 maximize 矿工奖励 earned，需要选择网络连接优化。我们将这个问题定义为一个 combinatorial bandit problem。每个节点选择其邻居策略地，以最小化与90%的哈希能量网络的路径延迟相比，90%的哈希能量网络的路径延迟。我们的工作的一个重要贡献是使用基于网络坐标的模型来学习网络结构 dentro de  bandit 算法。我们在多种网络设置下进行实验，并证明我们的提议算法可以超过或与基eline相当。
</details></li>
</ul>
<hr>
<h2 id="FreeDrag-Point-Tracking-is-Not-What-You-Need-for-Interactive-Point-based-Image-Editing"><a href="#FreeDrag-Point-Tracking-is-Not-What-You-Need-for-Interactive-Point-based-Image-Editing" class="headerlink" title="FreeDrag: Point Tracking is Not What You Need for Interactive Point-based Image Editing"></a>FreeDrag: Point Tracking is Not What You Need for Interactive Point-based Image Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04684">http://arxiv.org/abs/2307.04684</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lpengyang/freedrag">https://github.com/lpengyang/freedrag</a></li>
<li>paper_authors: Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, Yi Jin</li>
<li>for: 提高图像修改精度和灵活性，解决DragGAN在点追踪方面存在缺陷和困难。</li>
<li>methods: 提出了一种基于特征的方法，即FreeDrag，通过适应模板特征、直线搜索和杂化本地化技术来解决点追踪困难。</li>
<li>results: 对 DragGAN 进行比较，FreeDrag 能够在具有相似结构、细节或多个目标点的场景下实现稳定和高效的点基型图像修改。<details>
<summary>Abstract</summary>
To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable. Recently, DragGAN has achieved impressive editing results through point-based manipulation. However, we have observed that DragGAN struggles with miss tracking, where DragGAN encounters difficulty in effectively tracking the desired handle points, and ambiguous tracking, where the tracked points are situated within other regions that bear resemblance to the handle points. To deal with the above issues, we propose FreeDrag, which adopts a feature-oriented approach to free the burden on point tracking within the point-oriented methodology of DragGAN. The FreeDrag incorporates adaptive template features, line search, and fuzzy localization techniques to perform stable and efficient point-based image editing. Extensive experiments demonstrate that our method is superior to the DragGAN and enables stable point-based editing in challenging scenarios with similar structures, fine details, or under multi-point targets.
</details>
<details>
<summary>摘要</summary>
为了满足图像编辑的复杂和多样化需求，图像内容精确和 flexible 的操作是不可或缺的。近期，DragGAN 已经实现了吸引人的编辑结果通过点基的操作。然而，我们发现 DragGAN 会遇到跟踪困难和模糊跟踪问题，其中 DragGAN 在跟踪感兴趣的执行点时遇到了困难，并且执行点可能会位于其他区域中，这些区域具有执行点的相似特征。为解决以上问题，我们提出了 FreeDrag，它采用了特征对应方法来解除 DragGAN 中点跟踪的压力。FreeDrag  integrate了适应模板特征、线搜索和杂化地址技术，以实现稳定和高效的点基图像编辑。广泛的实验表明，我们的方法比 DragGAN 更高效和稳定，能够在复杂的场景下，如同构件、细节等，或者多点目标下进行稳定的点基编辑。
</details></li>
</ul>
<hr>
<h2 id="Generalization-Error-of-First-Order-Methods-for-Statistical-Learning-with-Generic-Oracles"><a href="#Generalization-Error-of-First-Order-Methods-for-Statistical-Learning-with-Generic-Oracles" class="headerlink" title="Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles"></a>Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04679">http://arxiv.org/abs/2307.04679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Scaman, Mathieu Even, Laurent Massoulié</li>
<li>for: 本文提出了一种新的泛化误差分析框架，用于对首选优化算法的统计学学习中的泛化误差进行分析，当Gradient只能通过部分观察到了由oracle提供的时候。我们的分析基于数据样本的Regularity，并可以得到多种学习问题的泛化误差的近似上下限，包括超vised学习、转移学习、Robust学习、分布式学习和通信效率的学习使用Gradient量化。这些结果适用于光滑和强型优化问题，以及非光滑优化问题，只要满足Polyak-Lojasiewicz假设。特别是，我们的上下限和下限取决于一个新的量，它扩展了条件标准差的概念，并是让Optimization of the statistical learning objective几乎等于Estimation of its gradient的准确度。</li>
<li>methods: 我们的分析基于数据样本的Regularity，并使用了Conditional standard deviation的概念来扩展Gradient的近似。</li>
<li>results: 我们的结果显示，在标准超vised学习问题中，采用增加 batch size的mini-batch gradient descent，并使用了温start的优化策略可以 дости到最佳的泛化误差，即在一定的多项式因子下，与理论最佳值相同。这些结果鼓励使用这种优化策略在实际应用中。<details>
<summary>Abstract</summary>
In this paper, we provide a novel framework for the analysis of generalization error of first-order optimization algorithms for statistical learning when the gradient can only be accessed through partial observations given by an oracle. Our analysis relies on the regularity of the gradient w.r.t. the data samples, and allows to derive near matching upper and lower bounds for the generalization error of multiple learning problems, including supervised learning, transfer learning, robust learning, distributed learning and communication efficient learning using gradient quantization. These results hold for smooth and strongly-convex optimization problems, as well as smooth non-convex optimization problems verifying a Polyak-Lojasiewicz assumption. In particular, our upper and lower bounds depend on a novel quantity that extends the notion of conditional standard deviation, and is a measure of the extent to which the gradient can be approximated by having access to the oracle. As a consequence, our analysis provides a precise meaning to the intuition that optimization of the statistical learning objective is as hard as the estimation of its gradient. Finally, we show that, in the case of standard supervised learning, mini-batch gradient descent with increasing batch sizes and a warm start can reach a generalization error that is optimal up to a multiplicative factor, thus motivating the use of this optimization scheme in practical applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了一种新的框架来分析首领优化算法在统计学学习中的泛化误差。我们的分析基于数据样本的规则性，可以 derive near matching 上下 bounds for the generalization error of multiple learning problems, including supervised learning, transfer learning, robust learning, distributed learning and communication efficient learning using gradient quantization.这些结果适用于光滑和强 converges 优化问题，以及非 convex 优化问题， whenever the gradient can be approximated by having access to an oracle. In particular, our upper and lower bounds depend on a novel quantity that extends the notion of conditional standard deviation, and is a measure of the extent to which the gradient can be approximated by having access to the oracle.因此，我们的分析给出了一个准确的含义，即优化统计学学习 objective 是等价于 estimation of its gradient的困难。最后，我们表明，在标准的supervised learning情况下，采用增加 batch size 和 warm start的 mini-batch gradient descent可以达到一个泛化误差，是optimal up to a multiplicative factor，因此推荐使用这种优化策略在实际应用中。
</details></li>
</ul>
<hr>
<h2 id="LINFA-a-Python-library-for-variational-inference-with-normalizing-flow-and-annealing"><a href="#LINFA-a-Python-library-for-variational-inference-with-normalizing-flow-and-annealing" class="headerlink" title="LINFA: a Python library for variational inference with normalizing flow and annealing"></a>LINFA: a Python library for variational inference with normalizing flow and annealing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04675">http://arxiv.org/abs/2307.04675</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/desreslab/linfa">https://github.com/desreslab/linfa</a></li>
<li>paper_authors: Yu Wang, Emma R. Cobian, Jubilee Lee, Fang Liu, Jonathan D. Hauenstein, Daniele E. Schiavazzi</li>
<li>for: 这 paper 是为了提供一种用于变量推断的 Python 库，以便处理复杂的模型和难以采样的分布。</li>
<li>methods: 这 paper 使用了变量推断和均化流程来解决计算成本高和难以采样的问题。</li>
<li>results: 这 paper 在多个 benchmark 中表现出色，可以快速和高效地处理复杂的模型和分布。<details>
<summary>Abstract</summary>
Variational inference is an increasingly popular method in statistics and machine learning for approximating probability distributions. We developed LINFA (Library for Inference with Normalizing Flow and Annealing), a Python library for variational inference to accommodate computationally expensive models and difficult-to-sample distributions with dependent parameters. We discuss the theoretical background, capabilities, and performance of LINFA in various benchmarks. LINFA is publicly available on GitHub at https://github.com/desResLab/LINFA.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Variational inference is an increasingly popular method in statistics and machine learning for approximating probability distributions. We developed LINFA (Library for Inference with Normalizing Flow and Annealing), a Python library for variational inference to accommodate computationally expensive models and difficult-to-sample distributions with dependent parameters. We discuss the theoretical background, capabilities, and performance of LINFA in various benchmarks. LINFA is publicly available on GitHub at https://github.com/desResLab/LINFA." into Simplified Chinese.习惯性推论是现代统计学和机器学习中越来越受欢迎的方法，用于简化机会概率分布。我们发展了LINFA（对应流和气适化库），一个用于统计学和机器学习中的推论方法，以应对 computationally expensive 模型和困难样本分布。我们在不同的审核中讨论了 LINFA 的理论背景、能力和性能。 LINFA 可以在 GitHub 上遍历：https://github.com/desResLab/LINFA。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-the-Echo-Chamber-Effect-An-Embedding-Distance-based-Approach"><a href="#Quantifying-the-Echo-Chamber-Effect-An-Embedding-Distance-based-Approach" class="headerlink" title="Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach"></a>Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04668">http://arxiv.org/abs/2307.04668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/faalatawi/echo-chamber-score">https://github.com/faalatawi/echo-chamber-score</a></li>
<li>paper_authors: Faisal Alatawi, Paras Sheth, Huan Liu</li>
<li>for: 这 paper 的目的是开发一种量化 echo chamber 的方法，以便更好地理解在社交媒体平台上的信息传播和社会架构。</li>
<li>methods: 这 paper 使用了一种新的衡量方法，即 Echo Chamber Score (ECS)，可以不需要用户政治信仰的标签，同时不需要假设交互图的结构。具体来说，ECS 使用了一种自我超vised graph autoencoder-based 的用户嵌入模型，以便在嵌入空间中度量用户之间的距离。</li>
<li>results: 根据 Twitter 数据集的四个话题（两个极化话题和两个非极化话题），我们的结果表明 ECS 是一种有效的量化 echo chamber 的工具，可以帮助我们更好地理解在线讨论的动态。<details>
<summary>Abstract</summary>
The rise of social media platforms has facilitated the formation of echo chambers, which are online spaces where users predominantly encounter viewpoints that reinforce their existing beliefs while excluding dissenting perspectives. This phenomenon significantly hinders information dissemination across communities and fuels societal polarization. Therefore, it is crucial to develop methods for quantifying echo chambers. In this paper, we present the Echo Chamber Score (ECS), a novel metric that assesses the cohesion and separation of user communities by measuring distances between users in the embedding space. In contrast to existing approaches, ECS is able to function without labels for user ideologies and makes no assumptions about the structure of the interaction graph. To facilitate measuring distances between users, we propose EchoGAE, a self-supervised graph autoencoder-based user embedding model that leverages users' posts and the interaction graph to embed them in a manner that reflects their ideological similarity. To assess the effectiveness of ECS, we use a Twitter dataset consisting of four topics - two polarizing and two non-polarizing. Our results showcase ECS's effectiveness as a tool for quantifying echo chambers and shedding light on the dynamics of online discourse.
</details>
<details>
<summary>摘要</summary>
社交媒体平台的兴起使得听众圈（echo chamber）在线空间中增长，这些圈中听众主要遇到支持他们现有的信仰的观点，同时排除不同意见。这种现象严重阻碍信息在社区之间传播，促进社会分化。因此，我们需要开发方法来评估听众圈。在这篇论文中，我们提出了听众圈分数（ECS），一种新的评估方法，可以评估用户社区的凝聚和分化程度。与现有方法不同的是，ECS不需要用户政治信仰的标签，并且不 assumptions 关于交互图的结构。为了计算用户之间的距离，我们提出了 EchoGAE，一种基于用户帖子和交互图的自适应图自动encoder模型，可以帮助将用户嵌入到一个表示他们意识形态相似性的空间中。为了评估ECS的有效性，我们使用了一个 Twitter 数据集，包括四个话题：两个极化话题和两个非极化话题。我们的结果显示ECS 是一种有效的听众圈评估工具，可以揭示在线讨论的动态。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/11/cs.LG_2023_07_11/" data-id="clon21irc00l8r5886raxdfeu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/11/eess.IV_2023_07_11/" class="article-date">
  <time datetime="2023-07-11T09:00:00.000Z" itemprop="datePublished">2023-07-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/11/eess.IV_2023_07_11/">eess.IV - 2023-07-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="APRF-Anti-Aliasing-Projection-Representation-Field-for-Inverse-Problem-in-Imaging"><a href="#APRF-Anti-Aliasing-Projection-Representation-Field-for-Inverse-Problem-in-Imaging" class="headerlink" title="APRF: Anti-Aliasing Projection Representation Field for Inverse Problem in Imaging"></a>APRF: Anti-Aliasing Projection Representation Field for Inverse Problem in Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05270">http://arxiv.org/abs/2307.05270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Chen, Lingxiao Yang, Jianhuang Lai, Xiaohua Xie</li>
<li>for: 高品质 Computed Tomography (CT) 影像重建</li>
<li>methods: 使用 Implicit Neural Representations (INRs) 建立坐标基于的映射，以及自我监督 Anti-Aliasing Projection Representation Field (APRF) 方法</li>
<li>results: 比前方法更高效，能够提供更高品质的 CT 影像，并且具有更少的噪音和杂音Here’s a more detailed explanation of each point:</li>
<li>for: The paper is focused on developing a novel method for reconstructing high-quality CT images from sparse-view measurements.</li>
<li>methods: The proposed method uses Implicit Neural Representations (INRs) to establish a coordinate-based mapping between sinograms and CT images. Additionally, a self-supervised method called Anti-Aliasing Projection Representation Field (APRF) is proposed to improve the quality of the reconstructed images.</li>
<li>results: The proposed method outperforms state-of-the-art methods in terms of image quality, with fewer artifacts and more accurate details. The code for the proposed method will be publicly available soon.<details>
<summary>Abstract</summary>
Sparse-view Computed Tomography (SVCT) reconstruction is an ill-posed inverse problem in imaging that aims to acquire high-quality CT images based on sparsely-sampled measurements. Recent works use Implicit Neural Representations (INRs) to build the coordinate-based mapping between sinograms and CT images. However, these methods have not considered the correlation between adjacent projection views, resulting in aliasing artifacts on SV sinograms. To address this issue, we propose a self-supervised SVCT reconstruction method -- Anti-Aliasing Projection Representation Field (APRF), which can build the continuous representation between adjacent projection views via the spatial constraints. Specifically, APRF only needs SV sinograms for training, which first employs a line-segment sampling module to estimate the distribution of projection views in a local region, and then synthesizes the corresponding sinogram values using center-based line integral module. After training APRF on a single SV sinogram itself, it can synthesize the corresponding dense-view (DV) sinogram with consistent continuity. High-quality CT images can be obtained by applying re-projection techniques on the predicted DV sinograms. Extensive experiments on CT images demonstrate that APRF outperforms state-of-the-art methods, yielding more accurate details and fewer artifacts. Our code will be publicly available soon.
</details>
<details>
<summary>摘要</summary>
sparse-view 计算机断层成像（SVCT）重建是一个不定问题在成像中，旨在基于稀疏样本获取高质量 CT 图像。 current works 使用隐式神经表示（INRs）建立坐标基于的映射 между sinograms 和 CT 图像。 however, these methods have not considered the correlation between adjacent projection views, resulting in aliasing artifacts on SV sinograms. to address this issue, we propose a self-supervised SVCT reconstruction method -- Anti-Aliasing Projection Representation Field (APRF), which can build the continuous representation between adjacent projection views via the spatial constraints. specifically, APRF only needs SV sinograms for training, which first employs a line-segment sampling module to estimate the distribution of projection views in a local region, and then synthesizes the corresponding sinogram values using center-based line integral module. after training APRF on a single SV sinogram itself, it can synthesize the corresponding dense-view (DV) sinogram with consistent continuity. high-quality CT images can be obtained by applying re-projection techniques on the predicted DV sinograms. extensive experiments on CT images demonstrate that APRF outperforms state-of-the-art methods, yielding more accurate details and fewer artifacts. our code will be publicly available soon.Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Does-pre-training-on-brain-related-tasks-results-in-better-deep-learning-based-brain-age-biomarkers"><a href="#Does-pre-training-on-brain-related-tasks-results-in-better-deep-learning-based-brain-age-biomarkers" class="headerlink" title="Does pre-training on brain-related tasks results in better deep-learning-based brain age biomarkers?"></a>Does pre-training on brain-related tasks results in better deep-learning-based brain age biomarkers?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05241">http://arxiv.org/abs/2307.05241</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gama-ufsc/brain-age">https://github.com/gama-ufsc/brain-age</a></li>
<li>paper_authors: Bruno Machado Pacheco, Victor Hugo Rocha de Oliveira, Augusto Braga Fernandes Antunes, Saulo Domingos de Souza Pedro, Danilo Silva</li>
<li>for: 预测健康人脑年龄，作为脑健康指标和成功老化指标，以及疾病生物标志。</li>
<li>methods: 使用深度学习模型预测健康人脑年龄，并在训练过程中使用脑相关任务进行预处理。</li>
<li>results: 研究发现，使用脑相关任务预处理的深度学习模型可以在ADNI数据集上达到状态之作的结果，而且可以验证这些模型的脑年龄生物标志在轻度认知障碍和阿尔茨heimer病患者的图像上的效果。<details>
<summary>Abstract</summary>
Brain age prediction using neuroimaging data has shown great potential as an indicator of overall brain health and successful aging, as well as a disease biomarker. Deep learning models have been established as reliable and efficient brain age estimators, being trained to predict the chronological age of healthy subjects. In this paper, we investigate the impact of a pre-training step on deep learning models for brain age prediction. More precisely, instead of the common approach of pre-training on natural imaging classification, we propose pre-training the models on brain-related tasks, which led to state-of-the-art results in our experiments on ADNI data. Furthermore, we validate the resulting brain age biomarker on images of patients with mild cognitive impairment and Alzheimer's disease. Interestingly, our results indicate that better-performing deep learning models in terms of brain age prediction on healthy patients do not result in more reliable biomarkers.
</details>
<details>
<summary>摘要</summary>
��colon cancer prediction using neuroimaging data has shown great potential as an indicator of overall brain health and successful aging, as well as a disease biomarker. Deep learning models have been established as reliable and efficient brain age estimators, being trained to predict the chronological age of healthy subjects. In this paper, we investigate the impact of a pre-training step on deep learning models for brain age prediction. More precisely, instead of the common approach of pre-training on natural imaging classification, we propose pre-training the models on brain-related tasks, which led to state-of-the-art results in our experiments on ADNI data. Furthermore, we validate the resulting brain age biomarker on images of patients with mild cognitive impairment and Alzheimer's disease. Interestingly, our results indicate that better-performing deep learning models in terms of brain age prediction on healthy patients do not result in more reliable biomarkers.Here's the word-for-word translation:��colon cancer prediction using neuroimaging data has shown great potential as an indicator of overall brain health and successful aging, as well as a disease biomarker. Deep learning models have been established as reliable and efficient brain age estimators, being trained to predict the chronological age of healthy subjects. In this paper, we investigate the impact of a pre-training step on deep learning models for brain age prediction. More precisely, instead of the common approach of pre-training on natural imaging classification, we propose pre-training the models on brain-related tasks, which led to state-of-the-art results in our experiments on ADNI data. Furthermore, we validate the resulting brain age biomarker on images of patients with mild cognitive impairment and Alzheimer's disease. Interestingly, our results indicate that better-performing deep learning models in terms of brain age prediction on healthy patients do not result in more reliable biomarkers.
</details></li>
</ul>
<hr>
<h2 id="Encoder-Complexity-Control-in-SVT-AV1-by-Speed-Adaptive-Preset-Switching"><a href="#Encoder-Complexity-Control-in-SVT-AV1-by-Speed-Adaptive-Preset-Switching" class="headerlink" title="Encoder Complexity Control in SVT-AV1 by Speed-Adaptive Preset Switching"></a>Encoder Complexity Control in SVT-AV1 by Speed-Adaptive Preset Switching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05208">http://arxiv.org/abs/2307.05208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lena Eichermüller, Gaurang Chaudhari, Ioannis Katsavounidis, Zhijun Lei, Hassene Tmar, André Kaup, Christian Herglotz</li>
<li>for: 提高视频编码器的复杂度控制机制，以满足在线视频流量的增长和相关的编码需求。</li>
<li>methods: 使用速度自适应预设切换机制，在完整预设范围内实现用户定义的时间限制，准确率为8.9%。</li>
<li>results: 可以在编码过程中实现用户定义的时间限制，而无需添加额外延迟。<details>
<summary>Abstract</summary>
Current developments in video encoding technology lead to continuously improving compression performance but at the expense of increasingly higher computational demands. Regarding the online video traffic increases during the last years and the concomitant need for video encoding, encoder complexity control mechanisms are required to restrict the processing time to a sufficient extent in order to find a reasonable trade-off between performance and complexity. We present a complexity control mechanism in SVT-AV1 by using speed-adaptive preset switching to comply with the remaining time budget. This method enables encoding with a user-defined time constraint within the complete preset range with an average precision of 8.9 \% without introducing any additional latencies.
</details>
<details>
<summary>摘要</summary>
当前的视频编码技术发展趋势导致无限扩大压缩性能，但是同时也增加了计算负担。随着上传视频流量的增长和相关的编码需求，需要控制编码器复杂性来限制处理时间，以达到一个合理的性能和复杂性交互。我们在SVT-AV1中提出了一种复杂性控制机制，通过速度适应预设切换来遵守剩余时间预算。这种方法可以在完整的预设范围内进行编码，并且保证用户定义的时间约束，平均准确率为8.9%，不增加任何延迟。
</details></li>
</ul>
<hr>
<h2 id="HistoColAi-An-Open-Source-Web-Platform-for-Collaborative-Digital-Histology-Image-Annotation-with-AI-Driven-Predictive-Integration"><a href="#HistoColAi-An-Open-Source-Web-Platform-for-Collaborative-Digital-Histology-Image-Annotation-with-AI-Driven-Predictive-Integration" class="headerlink" title="HistoColAi: An Open-Source Web Platform for Collaborative Digital Histology Image Annotation with AI-Driven Predictive Integration"></a>HistoColAi: An Open-Source Web Platform for Collaborative Digital Histology Image Annotation with AI-Driven Predictive Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07525">http://arxiv.org/abs/2307.07525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cristian Camilo Pulgarín-Ospina, Rocío del Amor, Adrián Colomera, Julio Silva-Rodríguez, Valery Naranjo</li>
<li>for: 这篇论文是为了提供一种高效的在线工具，用于可见化和注释数字 histological 图像的开发而写的。</li>
<li>methods: 本论文使用了深度学习基于方法，用于图像分析，以帮助在数字patology中提高诊断精度。</li>
<li>results: 本论文包含了一个关于细胞皮肤肿瘤诊断的用例，并进行了多个注释者的可用性研究，以验证开发的工具的可行性。<details>
<summary>Abstract</summary>
Digital pathology has become a standard in the pathology workflow due to its many benefits. These include the level of detail of the whole slide images generated and the potential immediate sharing of cases between hospitals. Recent advances in deep learning-based methods for image analysis make them of potential aid in digital pathology. However, a major limitation in developing computer-aided diagnostic systems for pathology is the lack of an intuitive and open web application for data annotation. This paper proposes a web service that efficiently provides a tool to visualize and annotate digitized histological images. In addition, to show and validate the tool, in this paper we include a use case centered on the diagnosis of spindle cell skin neoplasm for multiple annotators. A usability study of the tool is also presented, showing the feasibility of the developed tool.
</details>
<details>
<summary>摘要</summary>
数字病理学已成为病理学工作流程中的标准，这主要归功于它的多种优点。这些优点包括整个报告图像的细节水平和可以立即将案例传递给医院。Recent advances in deep learning-based methods for image analysis make them of potential aid in digital pathology. However, a major limitation in developing computer-aided diagnostic systems for pathology is the lack of an intuitive and open web application for data annotation. This paper proposes a web service that efficiently provides a tool to visualize and annotate digitized histological images. In addition, to show and validate the tool, in this paper we include a use case centered on the diagnosis of spindle cell skin neoplasm for multiple annotators. A usability study of the tool is also presented, showing the feasibility of the developed tool.Here's the translation in Traditional Chinese: digitale pathology 已成为病理学工作流程中的标准，这主要从其许多优点中来。这些优点包括整个报告图像的细节水平和可以立即将案例传递到医院。Recent advances in deep learning-based methods for image analysis make them of potential aid in digital pathology. However, a major limitation in developing computer-aided diagnostic systems for pathology is the lack of an intuitive and open web application for data annotation. This paper proposes a web service that efficiently provides a tool to visualize and annotate digitized histological images. In addition, to show and validate the tool, in this paper we include a use case centered on the diagnosis of spindle cell skin neoplasm for multiple annotators. A usability study of the tool is also presented, showing the feasibility of the developed tool.
</details></li>
</ul>
<hr>
<h2 id="Super-resolution-imaging-through-a-multimode-fiber-the-physical-upsampling-of-speckle-driven"><a href="#Super-resolution-imaging-through-a-multimode-fiber-the-physical-upsampling-of-speckle-driven" class="headerlink" title="Super-resolution imaging through a multimode fiber: the physical upsampling of speckle-driven"></a>Super-resolution imaging through a multimode fiber: the physical upsampling of speckle-driven</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05138">http://arxiv.org/abs/2307.05138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuncheng Zhang, Tingting Liu, Zhihua Xie, Yu Wang, Tong Liu, Qian Chen, Xiubao Sui</li>
<li>for: 这个论文的目的是为了提高endooscopic imaging的精度和质量，以便更好地进行微创外科手术。</li>
<li>methods: 这个论文使用了一种基于深度学习的超分解影像方法，通过physically realizing the upsampling of low-resolution images来提高模型的感知能力。</li>
<li>results: 实验表明，这种方法可以有效地提高endooscopic imaging的精度和质量，并且可以补偿数据驱动方法中缺失的信息。<details>
<summary>Abstract</summary>
Following recent advancements in multimode fiber (MMF), miniaturization of imaging endoscopes has proven crucial for minimally invasive surgery in vivo. Recent progress enabled by super-resolution imaging methods with a data-driven deep learning (DL) framework has balanced the relationship between the core size and resolution. However, most of the DL approaches lack attention to the physical properties of the speckle, which is crucial for reconciling the relationship between the magnification of super-resolution imaging and the quality of reconstruction quality. In the paper, we find that the interferometric process of speckle formation is an essential basis for creating DL models with super-resolution imaging. It physically realizes the upsampling of low-resolution (LR) images and enhances the perceptual capabilities of the models. The finding experimentally validates the role played by the physical upsampling of speckle-driven, effectively complementing the lack of information in data-driven. Experimentally, we break the restriction of the poor reconstruction quality at great magnification by inputting the same size of the speckle with the size of the high-resolution (HR) image to the model. The guidance of our research for endoscopic imaging may accelerate the further development of minimally invasive surgery.
</details>
<details>
<summary>摘要</summary>
Recent advancements in multimode fiber (MMF) have made it crucial to miniaturize imaging endoscopes for minimally invasive surgery in vivo. Recent progress in super-resolution imaging methods with a data-driven deep learning (DL) framework has balanced the relationship between the core size and resolution. However, most DL approaches overlook the physical properties of speckle, which is essential for reconciling the relationship between the magnification of super-resolution imaging and the reconstruction quality. In our paper, we find that the interferometric process of speckle formation is a crucial basis for creating DL models with super-resolution imaging. It physically realizes the upsampling of low-resolution (LR) images and enhances the perceptual capabilities of the models. Our findings experimentally validate the role played by the physical upsampling of speckle-driven, effectively complementing the lack of information in data-driven. Experimentally, we break the restriction of poor reconstruction quality at great magnification by inputting the same size of the speckle with the size of the high-resolution (HR) image to the model. Our research guidance may accelerate the further development of minimally invasive surgery.
</details></li>
</ul>
<hr>
<h2 id="Offline-and-Online-Optical-Flow-Enhancement-for-Deep-Video-Compression"><a href="#Offline-and-Online-Optical-Flow-Enhancement-for-Deep-Video-Compression" class="headerlink" title="Offline and Online Optical Flow Enhancement for Deep Video Compression"></a>Offline and Online Optical Flow Enhancement for Deep Video Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05092">http://arxiv.org/abs/2307.05092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanbo Tang, Xihua Sheng, Zhuoyuan Li, Haotian Zhang, Li Li, Dong Liu</li>
<li>for: 提高深度视频压缩网络的效率，使其更加适应实际视频数据。</li>
<li>methods: 提出了两 stage 优化方法：离线阶段使用已训练的 optical flow 估计网络进行优化，在线阶段使用梯度下降算法对视频进行适应优化。</li>
<li>results: 实验结果显示，该方法可以在 tested 视频上实现平均 12.8% 的比特率减少，而无需增加解码器的模型或计算复杂度。<details>
<summary>Abstract</summary>
Video compression relies heavily on exploiting the temporal redundancy between video frames, which is usually achieved by estimating and using the motion information. The motion information is represented as optical flows in most of the existing deep video compression networks. Indeed, these networks often adopt pre-trained optical flow estimation networks for motion estimation. The optical flows, however, may be less suitable for video compression due to the following two factors. First, the optical flow estimation networks were trained to perform inter-frame prediction as accurately as possible, but the optical flows themselves may cost too many bits to encode. Second, the optical flow estimation networks were trained on synthetic data, and may not generalize well enough to real-world videos. We address the twofold limitations by enhancing the optical flows in two stages: offline and online. In the offline stage, we fine-tune a trained optical flow estimation network with the motion information provided by a traditional (non-deep) video compression scheme, e.g. H.266/VVC, as we believe the motion information of H.266/VVC achieves a better rate-distortion trade-off. In the online stage, we further optimize the latent features of the optical flows with a gradient descent-based algorithm for the video to be compressed, so as to enhance the adaptivity of the optical flows. We conduct experiments on a state-of-the-art deep video compression scheme, DCVC. Experimental results demonstrate that the proposed offline and online enhancement together achieves on average 12.8% bitrate saving on the tested videos, without increasing the model or computational complexity of the decoder side.
</details>
<details>
<summary>摘要</summary>
视频压缩听说很多地利用视频帧之间的时间重复性，通常通过估计和使用运动信息来实现。运动信息通常被表示为光流在大多数现有的深度视频压缩网络中。实际上，这些网络 часто采用预训练的光流估计网络进行运动估计。然而，光流可能对视频压缩不适用，因为以下两个因素：一是光流估计网络在尽可能准确地进行间帧预测，但光流本身可能cost太多比特来编码。二是光流估计网络在synthetic数据上进行训练，可能无法在实际视频中generalize好 enough。我们通过两个阶段进行优化来解决这两个限制：离线阶段和在线阶段。在离线阶段，我们使用一个已经训练过的光流估计网络，并在H.266/VVC中提供的运动信息的基础上进行微调。在在线阶段，我们使用一种基于梯度下降算法的优化方法，以适应视频压缩。我们在DCVC中进行实验，实际结果表明，我们的离线和在线优化结合使用，在测试视频上平均实现12.8%的比特率折损，无需增加解码器的模型或计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="SAR-NeRF-Neural-Radiance-Fields-for-Synthetic-Aperture-Radar-Multi-View-Representation"><a href="#SAR-NeRF-Neural-Radiance-Fields-for-Synthetic-Aperture-Radar-Multi-View-Representation" class="headerlink" title="SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View Representation"></a>SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05087">http://arxiv.org/abs/2307.05087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengxin Lei, Feng Xu, Jiangtao Wei, Feng Cai, Feng Wang, Ya-Qiu Jin</li>
<li>for: 本研究旨在提出一种基于NeRF的SAR图像生成模型，以便更好地利用SAR图像的特点和特殊性。</li>
<li>methods: 本研究使用了SAR图像机理和神经网络结合，提出了一种基于NeRF的SAR图像生成模型，通过映射和投影原理，将SAR图像模型为3D图像空间中的吸收率和散射强度的函数。</li>
<li>results: 通过多种数据集的量化实验，研究者发现SAR-NeRF模型能够具有良好的多视角表示和泛化能力，并且可以在几何学学习设置下提高SAR目标分类性能。<details>
<summary>Abstract</summary>
SAR images are highly sensitive to observation configurations, and they exhibit significant variations across different viewing angles, making it challenging to represent and learn their anisotropic features. As a result, deep learning methods often generalize poorly across different view angles. Inspired by the concept of neural radiance fields (NeRF), this study combines SAR imaging mechanisms with neural networks to propose a novel NeRF model for SAR image generation. Following the mapping and projection pinciples, a set of SAR images is modeled implicitly as a function of attenuation coefficients and scattering intensities in the 3D imaging space through a differentiable rendering equation. SAR-NeRF is then constructed to learn the distribution of attenuation coefficients and scattering intensities of voxels, where the vectorized form of 3D voxel SAR rendering equation and the sampling relationship between the 3D space voxels and the 2D view ray grids are analytically derived. Through quantitative experiments on various datasets, we thoroughly assess the multi-view representation and generalization capabilities of SAR-NeRF. Additionally, it is found that SAR-NeRF augumented dataset can significantly improve SAR target classification performance under few-shot learning setup, where a 10-type classification accuracy of 91.6\% can be achieved by using only 12 images per class.
</details>
<details>
<summary>摘要</summary>
SAR图像受观测配置影响很大，并且在不同视角下显示出显著的变化，这使得深度学习方法很难通过不同视角来泛化。以NeRF原理为灵感，本研究将SAR探测机制与神经网络结合，提出了一种新的NeRF模型 дляSAR图像生成。通过 mapping和projection原理，SAR-NeRF模型将被用来学习积分吸收率和散射强度的分布，其中每个小体积的SAR渲染公式和视角网格之间的样本关系被分析 derivation。通过多种数据集的量化实验，我们全面评估了SAR-NeRF模型在多视角表示和泛化能力方面。此外，我们发现SAR-NeRF数据集可以大幅提高SAR目标分类性能，特别是在少量学习setup下，只需使用12个图像per类可以 дости得10类分类精度达91.6%。
</details></li>
</ul>
<hr>
<h2 id="Towards-Anytime-Optical-Flow-Estimation-with-Event-Cameras"><a href="#Towards-Anytime-Optical-Flow-Estimation-with-Event-Cameras" class="headerlink" title="Towards Anytime Optical Flow Estimation with Event Cameras"></a>Towards Anytime Optical Flow Estimation with Event Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05033">http://arxiv.org/abs/2307.05033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaozhuwa/eva-flow">https://github.com/yaozhuwa/eva-flow</a></li>
<li>paper_authors: Yaozu Ye, Hao Shi, Kailun Yang, Ze Wang, Xiaoting Yin, Yaonan Wang, Kaiwei Wang</li>
<li>For: The paper is focused on developing a high-frame-rate, low-latency event representation for optical flow estimation using event cameras.* Methods: The proposed method, called EVA-Flow, uses a unified voxel grid to represent events and a stacked Spatiotemporal Motion Refinement (SMR) module to predict temporally-dense optical flow. The method also utilizes a Rectified Flow Warp Loss (RFWL) for unsupervised evaluation of intermediate optical flow.* Results: The proposed method achieves competitive performance, with super-low latency (5ms), fastest inference (9.2ms), time-dense motion estimation (200Hz), and strong generalization.Here is the information in Simplified Chinese text:</li>
<li>for: 本文主要针对使用事件摄像机进行光流估计。</li>
<li>methods: 提议的方法使用团结矩阵表示事件，并使用堆叠的空间时间运动级化模块预测时间密集的光流。方法还利用Rectified Flow Warp Loss（RFWL）进行不监督的中间光流评估。</li>
<li>results: 提议的方法实现了竞争性的性能，具有超低延迟（5ms）、最快的推理（9.2ms）、时间密集的运动估计（200Hz）和强大的泛化性。<details>
<summary>Abstract</summary>
Event cameras are capable of responding to log-brightness changes in microseconds. Its characteristic of producing responses only to the changing region is particularly suitable for optical flow estimation. In contrast to the super low-latency response speed of event cameras, existing datasets collected via event cameras, however, only provide limited frame rate optical flow ground truth, (e.g., at 10Hz), greatly restricting the potential of event-driven optical flow. To address this challenge, we put forward a high-frame-rate, low-latency event representation Unified Voxel Grid, sequentially fed into the network bin by bin. We then propose EVA-Flow, an EVent-based Anytime Flow estimation network to produce high-frame-rate event optical flow with only low-frame-rate optical flow ground truth for supervision. The key component of our EVA-Flow is the stacked Spatiotemporal Motion Refinement (SMR) module, which predicts temporally-dense optical flow and enhances the accuracy via spatial-temporal motion refinement. The time-dense feature warping utilized in the SMR module provides implicit supervision for the intermediate optical flow. Additionally, we introduce the Rectified Flow Warp Loss (RFWL) for the unsupervised evaluation of intermediate optical flow in the absence of ground truth. This is, to the best of our knowledge, the first work focusing on anytime optical flow estimation via event cameras. A comprehensive variety of experiments on MVSEC, DESC, and our EVA-FlowSet demonstrates that EVA-Flow achieves competitive performance, super-low-latency (5ms), fastest inference (9.2ms), time-dense motion estimation (200Hz), and strong generalization. Our code will be available at https://github.com/Yaozhuwa/EVA-Flow.
</details>
<details>
<summary>摘要</summary>
Event 摄像头可以在微秒级别响应日志亮度变化。它的特点是仅响应变化区域，特别适合光流估算。然而，现有的事件摄像头数据集仅提供有限制的帧率光流真实值（例如10Hz），大大限制了事件驱动的光流潜力。为解决这个挑战，我们提出了高帧率、低延迟事件表示 Unified Voxel Grid，顺序Feed into网络bin by bin。然后，我们提出了EVENT-based Anytime Flow estimation Network（EVA-Flow），以生成高帧率事件光流，只需低帧率光流真实值作为超vision。EVA-Flow的关键组件是堆叠的空间时间运动级化（SMR）模块，预测时间密集的光流并通过空间时间运动级化提高准确性。SMR模块使用的时间密集特征扭曲提供了隐式超vision для中间光流。此外，我们引入了Rectified Flow Warp Loss（RFWL），用于无监督评估中间光流。这是我们所知道的首个关注在事件摄像头上的任何时间光流估算工作。我们在MVSEC、DESC和我们自己的EVA-FlowSet上进行了广泛的实验，并证明了EVA-Flow可以实现竞争性表现，超低延迟（5ms），最快执行（9.2ms），时间密集运动估计（200Hz）和强大总体化。我们的代码将在https://github.com/Yaozhuwa/EVA-Flow中提供。
</details></li>
</ul>
<hr>
<h2 id="Count-Free-Single-Photon-3D-Imaging-with-Race-Logic"><a href="#Count-Free-Single-Photon-3D-Imaging-with-Race-Logic" class="headerlink" title="Count-Free Single-Photon 3D Imaging with Race Logic"></a>Count-Free Single-Photon 3D Imaging with Race Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04924">http://arxiv.org/abs/2307.04924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atul Ingle, David Maier</li>
<li>for: The paper is written for developing an online approach for distance estimation in single-photon cameras (SPCs) without explicitly storing photon counts.</li>
<li>methods: The paper uses race logic to process photon streams in the time-delay domain and constructs count-free equi-depth histograms using a binner element to represent the distribution of photons.</li>
<li>results: The paper shows that the proposed method can provide an order of magnitude reduction in bandwidth and power consumption while maintaining similar distance reconstruction accuracy as conventional processing methods.Here’s the Chinese version of the three points:</li>
<li>for: 这篇论文是为了开发一种在单 photon 相机（SPC）中实现距离估计而不需要直接存储光子计数的在线方法。</li>
<li>methods: 该论文使用了竞速逻辑处理单 photon 流水，并使用一个归一化元素构建无计数的等深分布图。</li>
<li>results: 论文表明，该方法可以减少带宽和功耗消耗量，同时保持与传统处理方法相同的距离重建精度。<details>
<summary>Abstract</summary>
Single-photon cameras (SPCs) have emerged as a promising technology for high-resolution 3D imaging. A single-photon 3D camera determines the round-trip time of a laser pulse by capturing the arrival of individual photons at each camera pixel. Constructing photon-timestamp histograms is a fundamental operation for a single-photon 3D camera. However, in-pixel histogram processing is computationally expensive and requires large amount of memory per pixel. Digitizing and transferring photon timestamps to an off-sensor histogramming module is bandwidth and power hungry. Here we present an online approach for distance estimation without explicitly storing photon counts. The two key ingredients of our approach are (a) processing photon streams using race logic, which maintains photon data in the time-delay domain, and (b) constructing count-free equi-depth histograms. Equi-depth histograms are a succinct representation for ``peaky'' distributions, such as those obtained by an SPC pixel from a laser pulse reflected by a surface. Our approach uses a binner element that converges on the median (or, more generally, to another quantile) of a distribution. We cascade multiple binners to form an equi-depth histogrammer that produces multi-bin histograms. Our evaluation shows that this method can provide an order of magnitude reduction in bandwidth and power consumption while maintaining similar distance reconstruction accuracy as conventional processing methods.
</details>
<details>
<summary>摘要</summary>
单 photon 摄像机（SPC）已成为高分辨率 3D 成像技术的承诺。单 photon 3D 摄像机通过记录每个像素的各个 фотоン的到达时间来确定激光脉冲的圆涂时间。构建 photon 时间频谱 Histogram 是单 photon 3D 摄像机的基本操作之一。然而，在每个像素中进行 Histogram 处理是计算昂贵的，需要大量的内存。将 photon 时间频谱数据转移到外部 Histogramming 模块进行处理也是带宽和功耗浪费。我们现在提出了一种在线方法，不需要直接存储 photon 计数。我们的方法包括以下两个关键组成部分：1. 使用竞赛逻辑处理 photon 流，以保持 photon 数据在时延频域中。2. 使用 equi-depth histogram 构建器，通过 converges onto the median （或更一般地，另一个量化）的方式，来生成具有 "peaky" 分布的 histogram。equi-depth histogram 是一种简洁的表示方式，用于描述由 SPC 像素反射激光脉冲后得到的分布。我们的方法使用一个 binner 元素，该元素 converge 到分布的中值（或更一般地，另一个量化）。我们将多个 binner 元素串接起来，形成一个 equi-depth histogrammer，该机制可以生成多个 histogram。我们的评估表明，这种方法可以在带宽和功耗上减少一个数量级，而保持与传统处理方法相似的距离重建精度。
</details></li>
</ul>
<hr>
<h2 id="Kinematically-Decoupled-Impedance-Control-for-Fast-Object-Visual-Servoing-and-Grasping-on-Quadruped-Manipulators"><a href="#Kinematically-Decoupled-Impedance-Control-for-Fast-Object-Visual-Servoing-and-Grasping-on-Quadruped-Manipulators" class="headerlink" title="Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators"></a>Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04918">http://arxiv.org/abs/2307.04918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Riccardo Parosi, Mattia Risiglione, Darwin G. Caldwell, Claudio Semini, Victor Barasuol</li>
<li>for: 这 paper 是为了提出一个控制管道，用于对物体进行搜索、接近和抓取（SAG），基于分离的机械臂链和阻力控制，并 integrate 图像基于视能服务（IBVS）。</li>
<li>methods: 该管道使用了分离的机械臂链和阻力控制，并 integrate 图像基于视能服务（IBVS）。</li>
<li>results: 经过各种实验，提出的方法能够在动态移动的四足机器人上实现稳定的搜索、接近和抓取操作，并且能够抗抵抗外部干扰。<details>
<summary>Abstract</summary>
We propose a control pipeline for SAG (Searching, Approaching, and Grasping) of objects, based on a decoupled arm kinematic chain and impedance control, which integrates image-based visual servoing (IBVS). The kinematic decoupling allows for fast end-effector motions and recovery that leads to robust visual servoing. The whole approach and pipeline can be generalized for any mobile platform (wheeled or tracked vehicles), but is most suitable for dynamically moving quadruped manipulators thanks to their reactivity against disturbances. The compliance of the impedance controller makes the robot safer for interactions with humans and the environment. We demonstrate the performance and robustness of the proposed approach with various experiments on our 140 kg HyQReal quadruped robot equipped with a 7-DoF manipulator arm. The experiments consider dynamic locomotion, tracking under external disturbances, and fast motions of the target object.
</details>
<details>
<summary>摘要</summary>
我们提出了一个SAG（搜索、接近和抓取）控制管道，基于分离式机械臂链和弹簧控制，并 integrates 图像基于视服务（IBVS）。机械链的分离使得结束器速度快，并且可以快速恢复，从而实现了可靠的视服务。整个方法和管道可以应用于任何移动平台（轮式或轨道车辆），但是最适合动态移动四足机械人，因为它们对干扰的反应更强。弹簧控制器的灵活性使得机器人在与人类和环境的互动中更安全。我们通过对我们7度自由度 manipulate 机械臂的HyQReal四足机械人进行多种实验，证明了我们的方法的性能和稳定性。实验包括动态移动、外部干扰追踪和目标物体快速移动。
</details></li>
</ul>
<hr>
<h2 id="Rapid-Deforestation-and-Burned-Area-Detection-using-Deep-Multimodal-Learning-on-Satellite-Imagery"><a href="#Rapid-Deforestation-and-Burned-Area-Detection-using-Deep-Multimodal-Learning-on-Satellite-Imagery" class="headerlink" title="Rapid Deforestation and Burned Area Detection using Deep Multimodal Learning on Satellite Imagery"></a>Rapid Deforestation and Burned Area Detection using Deep Multimodal Learning on Satellite Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04916">http://arxiv.org/abs/2307.04916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation">https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation</a></li>
<li>paper_authors: Gabor Fodor, Marcos V. Conde</li>
<li>for: 这个研究报告的目的是提出一种基于多模态卫星影像和远程感知技术的方法，用于估计亚马逊热带雨林中的Deforestation和野火检测。</li>
<li>methods: 该研究使用了 convolutional neural networks (CNNs) 和完整的数据处理技术来解决这些问题。 dataset 包括从 Sentinel、Landsat、VIIRS 和 MODIS 卫星获取的 curaed 图像和多个通道频率。</li>
<li>results: 该方法在未见图像上达到了高精度的Deforestation 估计和烧毁地带检测。Here’s the same information in English:</li>
<li>for: The purpose of this research paper is to propose a method based on multimodal satellite imagery and remote sensing technology for estimating deforestation and detecting wildfire in the Amazon region.</li>
<li>methods: The research uses convolutional neural networks (CNNs) and comprehensive data processing techniques to solve these problems. The dataset includes curated images and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites.</li>
<li>results: The method achieves high-precision deforestation estimation and burned area detection on unseen images from the region.<details>
<summary>Abstract</summary>
Deforestation estimation and fire detection in the Amazon forest poses a significant challenge due to the vast size of the area and the limited accessibility. However, these are crucial problems that lead to severe environmental consequences, including climate change, global warming, and biodiversity loss. To effectively address this problem, multimodal satellite imagery and remote sensing offer a promising solution for estimating deforestation and detecting wildfire in the Amazonia region. This research paper introduces a new curated dataset and a deep learning-based approach to solve these problems using convolutional neural networks (CNNs) and comprehensive data processing techniques. Our dataset includes curated images and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites. We design the dataset considering different spatial and temporal resolution requirements. Our method successfully achieves high-precision deforestation estimation and burned area detection on unseen images from the region. Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation
</details>
<details>
<summary>摘要</summary>
亚马逊森林的排除和野火检测 pose 一个 significative 挑战，因为该区域的面积很大，而访问也很困难。然而，这些问题会导致严重的环境后果，包括气候变化、全球变暖和生物多样性损失。为了有效地解决这个问题，多模态卫星成像和远程感知提供了一个有希望的解决方案，通过 convolutional neural networks (CNNs) 和全面的数据处理技术。我们的数据集包括手动准备的图像和多种通道频谱的卫星图像，包括 Sentinel、Landsat、VIIRS 和 MODIS 卫星。我们设计数据集，考虑不同的空间和时间分辨率要求。我们的方法在未见图像上达到了高精度的排除和烧毁地带检测。我们的代码、模型和数据集都是开源的，可以在 GitHub 上找到：https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation。
</details></li>
</ul>
<hr>
<h2 id="KU-DMIS-MSRA-at-RadSum23-Pre-trained-Vision-Language-Model-for-Radiology-Report-Summarization"><a href="#KU-DMIS-MSRA-at-RadSum23-Pre-trained-Vision-Language-Model-for-Radiology-Report-Summarization" class="headerlink" title="KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization"></a>KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07409">http://arxiv.org/abs/2307.07409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gangwoo Kim, Hajung Kim, Lei Ji, Seongsu Bae, Chanhwi Kim, Mujeen Sung, Hyunjae Kim, Kun Yan, Eric Chang, Jaewoo Kang</li>
<li>for: 这个研究是为了开发一个新的预训掌握视觉语言模型（VLM），用于胸部X射影领域。</li>
<li>methods: 这个模型首先在多个多元领域的资料上进行预训，然后转移到胸部X射影领域。我们将不同的领域对应 задачу聚合为一个简单的序列转换架构，让模型从有限资源中获得所需的知识和技能。</li>
<li>results: 我们的模型在 BioNLP 共享任务的评分数据上显示出了超过其他模型的表现，并在 RadSum23 隐藏测试集的领导排行板上获得了第一名。<details>
<summary>Abstract</summary>
In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema. It enables the model to effectively learn the required knowledge and skills from limited resources in the domain. Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task, our model benefits from its training across multiple tasks and domains. With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们引入CheXOFA，一种新的预训练视语言模型（VLM），用于胸X光领域。我们的模型首先在通用领域中预训练于多种多样的数据集，然后转移到胸X光领域。借鉴一些知名的VLM，我们将各种领域特定任务整合成简单的序列到sequence schema。这使得模型可以很好地从有限资源中学习需要的知识和技能。在BioNLP共享任务提供的标准 datasets上表现出色，我们的模型受益于跨任务和领域的训练。通过微妙的技巧，包括ensemble和事实抽象，我们的系统在RadSum23隐藏测试集上达到了第一名。
</details></li>
</ul>
<hr>
<h2 id="CVPR-MultiEarth-2023-Deforestation-Estimation-Challenge-SpaceVision4Amazon"><a href="#CVPR-MultiEarth-2023-Deforestation-Estimation-Challenge-SpaceVision4Amazon" class="headerlink" title="CVPR MultiEarth 2023 Deforestation Estimation Challenge:SpaceVision4Amazon"></a>CVPR MultiEarth 2023 Deforestation Estimation Challenge:SpaceVision4Amazon</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04715">http://arxiv.org/abs/2307.04715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunita Arya, S Manthira Moorthi, Debajyoti Dhar</li>
<li>for: 本研究提出了一种基于注意力指导UNet架构的森林砍伐估计方法，使用电Optical（EO）和Synthetic Aperture Radar（SAR）卫星影像。</li>
<li>methods: 用Landsat-8和Sentinel-1卫星数据进行训练和验证提议的模型，由于数据的时空尺度不可用，因此每架次训练一个模型。</li>
<li>results: 训练时LandSat-8模型达到了93.45%的训练和验证像素精度，Sentinel-2模型达到了83.87%的像素精度。测试集评估中，模型达到了84.70%的像素精度，F1-Score为0.79，IOU为0.69。<details>
<summary>Abstract</summary>
In this paper, we present a deforestation estimation method based on attention guided UNet architecture using Electro-Optical (EO) and Synthetic Aperture Radar (SAR) satellite imagery. For optical images, Landsat-8 and for SAR imagery, Sentinel-1 data have been used to train and validate the proposed model. Due to the unavailability of temporally and spatially collocated data, individual model has been trained for each sensor. During training time Landsat-8 model achieved training and validation pixel accuracy of 93.45% and Sentinel-2 model achieved 83.87% pixel accuracy. During the test set evaluation, the model achieved pixel accuracy of 84.70% with F1-Score of 0.79 and IoU of 0.69.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于注意力引导的UNet架构的森林伐木估计方法，使用电Optical（EO）和Synthetic Aperture Radar（SAR）卫星图像。对于光学图像，我们使用了Landstat-8数据进行训练和验证；对于SAR图像，我们使用了Sentinel-1数据。由于数据的 temporally和spatially不同，我们需要单独训练每款感知器。在训练时，Landstat-8模型在训练和验证像素精度方面达到了93.45%，而Sentinel-2模型在验证像素精度方面达到了83.87%。在测试集评估中，模型在像素精度方面达到了84.70%，F1-Score为0.79，IOU为0.69。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/11/eess.IV_2023_07_11/" data-id="clon21ix8011sr588ap7jayck" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/10/cs.SD_2023_07_10/" class="article-date">
  <time datetime="2023-07-10T15:00:00.000Z" itemprop="datePublished">2023-07-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/10/cs.SD_2023_07_10/">cs.SD - 2023-07-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Collaborative-Song-Dataset-CoSoD-An-annotated-dataset-of-multi-artist-collaborations-in-popular-music"><a href="#Collaborative-Song-Dataset-CoSoD-An-annotated-dataset-of-multi-artist-collaborations-in-popular-music" class="headerlink" title="Collaborative Song Dataset (CoSoD): An annotated dataset of multi-artist collaborations in popular music"></a>Collaborative Song Dataset (CoSoD): An annotated dataset of multi-artist collaborations in popular music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05588">http://arxiv.org/abs/2307.05588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michèle Duguay, Kate Mancey, Johanna Devaney</li>
<li>for: 本研究使用的 CoSoD 数据集包含 331 首多个艺人合作的歌曲，来自 2010-2019 年 Billboard “Hot 100” 年度榜单。</li>
<li>methods: 本研究使用的 annotation 方法包括 formal sections, 声音生成方法（包括延迟、层次、扬声和performer 的性别）以及相关的 metadata。</li>
<li>results: 研究发现，男性艺人的 vocals 通常具有 menos 的延迟和更窄的位置在声音混合中，与女性艺人的 vocals 相比。<details>
<summary>Abstract</summary>
The Collaborative Song Dataset (CoSoD) is a corpus of 331 multi-artist collaborations from the 2010-2019 Billboard "Hot 100" year-end charts. The corpus is annotated with formal sections, aspects of vocal production (including reverberation, layering, panning, and gender of the performers), and relevant metadata. CoSoD complements other popular music datasets by focusing exclusively on musical collaborations between independent acts. In addition to facilitating the study of song form and vocal production, CoSoD allows for the in-depth study of gender as it relates to various timbral, pitch, and formal parameters in musical collaborations. In this paper, we detail the contents of the dataset and outline the annotation process. We also present an experiment using CoSoD that examines how the use of reverberation, layering, and panning are related to the gender of the artist. In this experiment, we find that men's voices are on average treated with less reverberation and occupy a more narrow position in the stereo mix than women's voices.
</details>
<details>
<summary>摘要</summary>
《合作歌曲数据集（CoSoD）》是2010-2019年度 Billboard "Hot 100" 年度榜单中331首多位艺人合作的数据集。该数据集已经被标注了正式部分、声乐生产方面的属性（包括延迟、层次、扬声和performer的性别）以及相关的元数据。CoSoD 与其他流行音乐数据集相比，专注于独立艺人之间的音乐合作。此外，CoSoD 还允许对歌曲结构和声乐生产进行深入研究，以及对 gender 与不同的时间、抽象和形式参数之间的关系进行深入研究。在这篇文章中，我们详细介绍了数据集的内容和标注过程，以及使用 CoSoD 进行了一个实验，发现男性艺人的声音在延迟方面比女性艺人的声音更为低，并且男性艺人的声音在立体混合中占据更窄的位置。
</details></li>
</ul>
<hr>
<h2 id="The-NPU-MSXF-Speech-to-Speech-Translation-System-for-IWSLT-2023-Speech-to-Speech-Translation-Task"><a href="#The-NPU-MSXF-Speech-to-Speech-Translation-System-for-IWSLT-2023-Speech-to-Speech-Translation-Task" class="headerlink" title="The NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023 Speech-to-Speech Translation Task"></a>The NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023 Speech-to-Speech Translation Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04630">http://arxiv.org/abs/2307.04630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Song, Yi lei, Peikun Chen, Yiqing Cao, Kun Wei, Yongmao Zhang, Lei Xie, Ning Jiang, Guoqing Zhao</li>
<li>for: 这个论文旨在提出一个基于自动语音识别（ASR）、机器翻译（MT）和文本到语音（TTS）的语音到语音翻译（S2ST）系统，用于从英语语音多源到中文语音的翻译。</li>
<li>methods: 这个系统采用了多种数据增强策略和ROVER基于多个ASR模型输出的分数融合，以提高多源语音输入的Robustness。此外，为了更好地处理含杂ASR转文本的问题，我们提出了三个阶段精度调整策略。</li>
<li>results: 实验结果显示，我们的系统在翻译准确率、语音自然度、声音质量和 speaker相似性方面具有优秀的表现，同时也在多源数据下表现良好的Robustness。<details>
<summary>Abstract</summary>
This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech translation (S2ST) task which aims to translate from English speech of multi-source to Chinese speech. The system is built in a cascaded manner consisting of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS). We make tremendous efforts to handle the challenging multi-source input. Specifically, to improve the robustness to multi-source speech input, we adopt various data augmentation strategies and a ROVER-based score fusion on multiple ASR model outputs. To better handle the noisy ASR transcripts, we introduce a three-stage fine-tuning strategy to improve translation accuracy. Finally, we build a TTS model with high naturalness and sound quality, which leverages a two-stage framework, using network bottleneck features as a robust intermediate representation for speaker timbre and linguistic content disentanglement. Based on the two-stage framework, pre-trained speaker embedding is leveraged as a condition to transfer the speaker timbre in the source English speech to the translated Chinese speech. Experimental results show that our system has high translation accuracy, speech naturalness, sound quality, and speaker similarity. Moreover, it shows good robustness to multi-source data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploiting-an-External-Microphone-for-Binaural-RTF-Vector-Based-Direction-of-Arrival-Estimation-for-Multiple-Speakers"><a href="#Exploiting-an-External-Microphone-for-Binaural-RTF-Vector-Based-Direction-of-Arrival-Estimation-for-Multiple-Speakers" class="headerlink" title="Exploiting an External Microphone for Binaural RTF-Vector-Based Direction of Arrival Estimation for Multiple Speakers"></a>Exploiting an External Microphone for Binaural RTF-Vector-Based Direction of Arrival Estimation for Multiple Speakers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04460">http://arxiv.org/abs/2307.04460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Fejgin, Simon Doclo</li>
<li>for: 减少计算复杂性的听力器 DOA估计方法</li>
<li>methods: 使用外部麦克风和听力器麦克风的低相关性假设进行RTF向量估计，并使用covariance whitening方法获取RTF向量</li>
<li>results: 在听力器麦克风和外部麦克风的听音环境中，提出了一种低计算复杂性的DOA估计方法，并且在两个说话人和杂音环境下获得了相当于CW方法的DOA估计性能，而且具有较低的计算复杂性。<details>
<summary>Abstract</summary>
In hearing aid applications, an important objective is to accurately estimate the direction of arrival (DOA) of multiple speakers in noisy and reverberant environments. Recently, we proposed a binaural DOA estimation method, where the DOAs of the speakers are estimated by selecting the directions for which the so-called Hermitian angle spectrum between the estimated relative transfer function (RTF) vector and a database of prototype anechoic RTF vectors is maximized. The RTF vector is estimated using the covariance whitening (CW) method, which requires a computationally complex generalized eigenvalue decomposition. The spatial spectrum is obtained by only considering frequencies where it is likely that one speaker dominates over the other speakers, noise and reverberation. In this contribution, we exploit the availability of an external microphone that is spatially separated from the hearing aid microphones and consider a low-complexity RTF vector estimation method that assumes a low spatial coherence between the undesired components in the external microphone and the hearing aid microphones. Using recordings of two speakers and diffuse-like babble noise in acoustic environments with mild reverberation and low signal-to-noise ratio, simulation results show that the proposed method yields a comparable DOA estimation performance as the CW method at a lower computational complexity.
</details>
<details>
<summary>摘要</summary>
在听觉器应用中，一个重要的目标是准确估计多个说话人的方向来达（DOA）在噪音和反射环境中。最近，我们提出了一种双耳DOA估计方法，其中DOA的估计是通过选择RTFVector的 Hermitian角spectrum与一个数据库中的prototype静音RTFVector之间的最大匹配来实现。RTFVector是通过covariance whitening（CW）方法来估计，该方法需要计算复杂的普通值归一化分解。 spatial spectrum是通过只考虑可能有一个说话人在其他说话人、噪音和反射中占据主导地位的频率来获得。在这种贡献中，我们利用了一个外部麦克风，该麦克风与听觉器麦克风之间存在低空间协方差，并考虑了一种低复杂度RTFVector估计方法，该方法假设外部麦克风和听觉器麦克风之间的不desired ком分布具有低空间协方差。通过对两个说话人和杂音噪音的记录在带有柔和反射的Acoustic环境中进行模拟，结果表明，我们的方法可以与CW方法具有相同的DOA估计性能，但具有较低的计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="HCLAS-X-Hierarchical-and-Cascaded-Lyrics-Alignment-System-Using-Multimodal-Cross-Correlation"><a href="#HCLAS-X-Hierarchical-and-Cascaded-Lyrics-Alignment-System-Using-Multimodal-Cross-Correlation" class="headerlink" title="HCLAS-X: Hierarchical and Cascaded Lyrics Alignment System Using Multimodal Cross-Correlation"></a>HCLAS-X: Hierarchical and Cascaded Lyrics Alignment System Using Multimodal Cross-Correlation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04377">http://arxiv.org/abs/2307.04377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsung Kang, Soochul Park, Keunwoo Choi</li>
<li>for: 本研究targets the challenge of lyrics alignment, which involves aligning the lyrics and vocal components of songs.</li>
<li>methods: 我们提出了一种基于supervised learning的模型，利用歌词和音频模态之间的cross-correlation矩阵来适应这个挑战。我们的模型采用了层次结构和顺序串行的设计，首先在句子级别预测同步时间，然后在词语级别预测同步时间。</li>
<li>results: 我们的提议模型在比较 experiment中显示了significant improvement in mean average error，并且在 deploying 在多个音乐流媒体服务上的实践中也得到了良好的结果。<details>
<summary>Abstract</summary>
In this work, we address the challenge of lyrics alignment, which involves aligning the lyrics and vocal components of songs. This problem requires the alignment of two distinct modalities, namely text and audio. To overcome this challenge, we propose a model that is trained in a supervised manner, utilizing the cross-correlation matrix of latent representations between vocals and lyrics. Our system is designed in a hierarchical and cascaded manner. It predicts synced time first on a sentence-level and subsequently on a word-level. This design enables the system to process long sequences, as the cross-correlation uses quadratic memory with respect to sequence length. In our experiments, we demonstrate that our proposed system achieves a significant improvement in mean average error, showcasing its robustness in comparison to the previous state-of-the-art model. Additionally, we conduct a qualitative analysis of the system after successfully deploying it in several music streaming services.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们面临着歌词对齐挑战，即将歌词和声音组件对齐。这个问题需要对两种不同的modalities进行对齐，即文本和音频。为了解决这个挑战，我们提议一种在有监督下训练的模型，使用歌词和声音的横排相关矩阵来训练。我们的系统设计为层次和分层的结构，首先在句子水平预测同步时间，然后在单词水平进行精度调整。这种设计使得系统能够处理长序列，因为交叉相关使用了序列长度的二次memory。在我们的实验中，我们示出了我们提议的系统在平均误差中具有显著的改善，这反映了它在比前一个状态艺术模型时更加稳定。此外，我们还进行了音乐流服务中成功部署后的系统Qualitative分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/10/cs.SD_2023_07_10/" data-id="clon21itj00ror5882tey4ayn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/10/eess.AS_2023_07_10/" class="article-date">
  <time datetime="2023-07-10T14:00:00.000Z" itemprop="datePublished">2023-07-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/10/eess.AS_2023_07_10/">eess.AS - 2023-07-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Timbre-transfer-using-image-to-image-denoising-diffusion-implicit-models"><a href="#Timbre-transfer-using-image-to-image-denoising-diffusion-implicit-models" class="headerlink" title="Timbre transfer using image-to-image denoising diffusion implicit models"></a>Timbre transfer using image-to-image denoising diffusion implicit models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04586">http://arxiv.org/abs/2307.04586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Comanducci, Fabio Antonacci, Augusto Sarti</li>
<li>for: 这个论文的目的是将乐器的 зву色转移到另一种乐器上，保持音乐特征的内容。</li>
<li>methods: 这个论文使用了深度学习基于生成的Denosing Diffusion Models（DDMs）进行音色转移。具体来说，使用最近提出的Denosing Diffusion Implicit Models（DDIMs）加速采样过程。</li>
<li>results: 该论文通过 listening tests 和对象度量来证明提出的模型的效果。<details>
<summary>Abstract</summary>
Timbre transfer techniques aim at converting the sound of a musical piece generated by one instrument into the same one as if it was played by another instrument, while maintaining as much as possible the content in terms of musical characteristics such as melody and dynamics. Following their recent breakthroughs in deep learning-based generation, we apply Denoising Diffusion Models (DDMs) to perform timbre transfer. Specifically, we apply the recently proposed Denoising Diffusion Implicit Models (DDIMs) that enable to accelerate the sampling procedure. Inspired by the recent application of DDMs to image translation problems we formulate the timbre transfer task similarly, by first converting the audio tracks into log mel spectrograms and by conditioning the generation of the desired timbre spectrogram through the input timbre spectrogram. We perform both one-to-one and many-to-many timbre transfer, by converting audio waveforms containing only single instruments and multiple instruments, respectively. We compare the proposed technique with existing state-of-the-art methods both through listening tests and objective measures in order to demonstrate the effectiveness of the proposed model.
</details>
<details>
<summary>摘要</summary>
timbre 传送技术目的在于将一种乐器生成的音乐作品的声音转换成另一种乐器上的同样的声音，保持音乐特点如旋律和声 dynamics 等。我们通过深度学习基本的生成技术，应用 Denoising Diffusion Models (DDMs) 来实现 timbre 传送。具体来说，我们使用最近提出的 Denoising Diffusion Implicit Models (DDIMs)，以加速抽样过程。受图像翻译问题的应用启发，我们将 timbre 传送任务设置为类似的形式，首先将音频轨迹转换为宽度 spectrograms 并通过输入 timbre spectrogram 来控制生成 Desired  timbre spectrogram。我们进行了一对一和多对多 timbre 传送，将单个乐器和多个乐器的音频波形转换为另一种 timbre。我们将比较我们的提议方法与现有状态的方法，通过听测试和 объек 的度量来证明我们的模型的效果。
</details></li>
</ul>
<hr>
<h2 id="Study-on-the-Correlation-between-Objective-Evaluations-and-Subjective-Speech-Quality-and-Intelligibility"><a href="#Study-on-the-Correlation-between-Objective-Evaluations-and-Subjective-Speech-Quality-and-Intelligibility" class="headerlink" title="Study on the Correlation between Objective Evaluations and Subjective Speech Quality and Intelligibility"></a>Study on the Correlation between Objective Evaluations and Subjective Speech Quality and Intelligibility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04517">http://arxiv.org/abs/2307.04517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsin-Tien Chiang, Kuo-Hsuan Hung, Szu-Wei Fu, Heng-Cheng Kuo, Ming-Hsueh Tsai, Yu Tsao</li>
<li>for: 这项研究旨在评估中文语音质量和理解度的对象测量和人类识别能力之间的相关性，以及使用深度学习技术组合现有的对象测量方法预测语音质量和理解度。</li>
<li>methods: 本研究使用中文语音数据集，对常用的对象测量方法进行了评估，并提出了一种新的深度学习模型，该模型可以预测语音质量和理解度，同时减少了训练数据量。</li>
<li>results: 研究发现，新提出的深度学习模型可以准确预测语音质量和理解度，而且可以减少训练数据量。此外，包含主观语音质量评分在语音理解预测中的影响也得到了研究。<details>
<summary>Abstract</summary>
Subjective tests are the gold standard for evaluating speech quality and intelligibility, but they are time-consuming and expensive. Thus, objective measures that align with human perceptions are crucial. This study evaluates the correlation between commonly used objective measures and subjective speech quality and intelligibility using a Chinese speech dataset. Moreover, new objective measures are proposed combining current objective measures using deep learning techniques to predict subjective quality and intelligibility. The proposed deep learning model reduces the amount of training data without significantly impacting prediction performance. We interpret the deep learning model to understand how objective measures reflect subjective quality and intelligibility. We also explore the impact of including subjective speech quality ratings on speech intelligibility prediction. Our findings offer valuable insights into the relationship between objective measures and human perceptions.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Subjective tests" is translated as "主观测试" (zhǔ yì cè shì), which refers to tests that are evaluated subjectively by human raters.* "Objective measures" is translated as "对象评价指标" (duì yì bìng jí), which refers to measures that are evaluated objectively using quantitative methods.* "Speech quality" is translated as "语音质量" (yǔ yīn zhì liàng), which refers to the overall quality of speech.* "Intelligibility" is translated as "语音可识别度" (yǔ yīn kě shí bèi duō), which refers to the ability to understand speech.* "Deep learning model" is translated as "深度学习模型" (shēn dào xué xí mó delì), which refers to a type of machine learning model that uses artificial neural networks to analyze data.* "Subjective speech quality ratings" is translated as "主观语音质量评分" (zhǔ yì yǔ yīn zhì liàng píng fān), which refers to ratings given by human raters to evaluate the subjective quality of speech.
</details></li>
</ul>
<hr>
<h2 id="A-Demand-Driven-Perspective-on-Generative-Audio-AI"><a href="#A-Demand-Driven-Perspective-on-Generative-Audio-AI" class="headerlink" title="A Demand-Driven Perspective on Generative Audio AI"></a>A Demand-Driven Perspective on Generative Audio AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04292">http://arxiv.org/abs/2307.04292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangshin Oh, Minsung Kang, Hyeongi Moon, Keunwoo Choi, Ben Sangbae Chon</li>
<li>for: 本研究的目的是了解业界对AI研究的需求，以便成功地应用AI技术。</li>
<li>methods: 本研究使用询问专业音响工程师的方式确定研究优先级和定义多种研究任务。</li>
<li>results: 调查结果表明当前最大的瓶颈是数据集的可用性，以及音质和控制性的现有挑战。研究还提出了一些解决这些问题的可能性，并提供了empirical证据。<details>
<summary>Abstract</summary>
To achieve successful deployment of AI research, it is crucial to understand the demands of the industry. In this paper, we present the results of a survey conducted with professional audio engineers, in order to determine research priorities and define various research tasks. We also summarize the current challenges in audio quality and controllability based on the survey. Our analysis emphasizes that the availability of datasets is currently the main bottleneck for achieving high-quality audio generation. Finally, we suggest potential solutions for some revealed issues with empirical evidence.
</details>
<details>
<summary>摘要</summary>
要成功推广人工智能研究，我们需要了解行业的需求。在这篇论文中，我们通过询问专业音频工程师来确定研究优先级和定义各种研究任务。我们还总结了现有数据的可用性是达到高质量音频生成的主要瓶颈。最后，我们提出了一些解决问题的可能性，并提供了实证证据。Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/10/eess.AS_2023_07_10/" data-id="clon21ivx00y8r588emsedre8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/10/cs.CV_2023_07_10/" class="article-date">
  <time datetime="2023-07-10T13:00:00.000Z" itemprop="datePublished">2023-07-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/10/cs.CV_2023_07_10/">cs.CV - 2023-07-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Joint-Salient-Object-Detection-and-Camouflaged-Object-Detection-via-Uncertainty-aware-Learning"><a href="#Joint-Salient-Object-Detection-and-Camouflaged-Object-Detection-via-Uncertainty-aware-Learning" class="headerlink" title="Joint Salient Object Detection and Camouflaged Object Detection via Uncertainty-aware Learning"></a>Joint Salient Object Detection and Camouflaged Object Detection via Uncertainty-aware Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04651">http://arxiv.org/abs/2307.04651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baneitixiaomai/joint_sod_cod">https://github.com/baneitixiaomai/joint_sod_cod</a></li>
<li>paper_authors: Aixuan Li, Jing Zhang, Yunqiu Lv, Tong Zhang, Yiran Zhong, Mingyi He, Yuchao Dai<br>for: 这个论文主要针对的是��Salient Object Detection（SOD）和Camouflaged Object Detection（COD）两个任务的uncertainty-aware学习ipeline的研究。methods: 该论文提出了一种基于数据水平和任务水平的 contradiction modeling的方法，通过利用数据集之间的相互关系和任务之间的相互关系，以及一种joint-task contrastive learning框架，来减少模型的训练数据量，同时提高模型的 Robustness和 representation learning能力。results: 实验结果表明，该方法可以在标准 benchmark datasets 上达到状态革命的性能，同时也可以提供有用的uncertainty estimation。<details>
<summary>Abstract</summary>
Salient objects attract human attention and usually stand out clearly from their surroundings. In contrast, camouflaged objects share similar colors or textures with the environment. In this case, salient objects are typically non-camouflaged, and camouflaged objects are usually not salient. Due to this inherent contradictory attribute, we introduce an uncertainty-aware learning pipeline to extensively explore the contradictory information of salient object detection (SOD) and camouflaged object detection (COD) via data-level and task-wise contradiction modeling. We first exploit the dataset correlation of these two tasks and claim that the easy samples in the COD dataset can serve as hard samples for SOD to improve the robustness of the SOD model. Based on the assumption that these two models should lead to activation maps highlighting different regions of the same input image, we further introduce a contrastive module with a joint-task contrastive learning framework to explicitly model the contradictory attributes of these two tasks. Different from conventional intra-task contrastive learning for unsupervised representation learning, our contrastive module is designed to model the task-wise correlation, leading to cross-task representation learning. To better understand the two tasks from the perspective of uncertainty, we extensively investigate the uncertainty estimation techniques for modeling the main uncertainties of the two tasks, namely task uncertainty (for SOD) and data uncertainty (for COD), and aiming to effectively estimate the challenging regions for each task to achieve difficulty-aware learning. Experimental results on benchmark datasets demonstrate that our solution leads to both state-of-the-art performance and informative uncertainty estimation.
</details>
<details>
<summary>摘要</summary>
salient objects 吸引人类注意力，通常与周围环境分别出色。相反，涂抹物体通常与环境颜色或文化相似。在这种情况下，突出物体通常不是涂抹的，而涂抹物体通常不是突出的。由于这种内在的矛盾特征，我们介绍了一种不确定性意识学习管道，激进探索这两个任务之间的矛盾信息。我们首先利用这两个任务的数据集之间的相关性，并认为易Difficult样本在COD任务中可以为SOD任务提高稳健性。基于假设这两个模型应该导致输入图像的活动图标高亮不同的区域，我们还引入了一个集成任务对比学习框架，以显式地模型这两个任务之间的矛盾特征。与传统的同任务内部对比学习不同，我们的对比模块旨在模型这两个任务之间的任务相关性，导致跨任务表示学习。为更好地理解这两个任务的不确定性，我们进行了广泛的不确定性估计技术研究，包括任务不确定性（ для SOD）和数据不确定性（ для COD），并aspire to effectively estimate the challenging regions for each task to achieve difficulty-aware learning。实验结果表明，我们的解决方案可以同时实现状态机器人表现和有用的不确定性估计。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-brain-age-estimation-using-interpretable-adaptive-population-graph-learning"><a href="#Multimodal-brain-age-estimation-using-interpretable-adaptive-population-graph-learning" class="headerlink" title="Multimodal brain age estimation using interpretable adaptive population-graph learning"></a>Multimodal brain age estimation using interpretable adaptive population-graph learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04639">http://arxiv.org/abs/2307.04639</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bintsi/adaptive-graph-learning">https://github.com/bintsi/adaptive-graph-learning</a></li>
<li>paper_authors: Kyriaki-Margarita Bintsi, Vasileios Baltatzis, Rolandos Alexandros Potamias, Alexander Hammers, Daniel Rueckert</li>
<li>for: 这个论文主要是为了提出一种基于人口图的大脑年龄估计方法，以便在 neuroscience 中提供有价值的信息，特别是在抑郁性神经疾病方面。</li>
<li>methods: 该论文使用了人口图和图像学习网络（GCN），并提出了一种可学习的人口图结构，以优化下游任务的性能。该方法使用了注意力机制，将多modal 的成像特征和非成像特征（phenotypes）作为权重，进行边EXTRACTION。</li>
<li>results: 该论文在使用 UK Biobank 数据集进行大脑年龄估计和分类任务中，超过了竞争static graph方法和其他状态态 adaptive 方法。此外，通过可视化注意力权重，提高了图的可读性。<details>
<summary>Abstract</summary>
Brain age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer's. Population graphs, which include multimodal imaging information of the subjects along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks. A population graph is usually static and constructed manually using non-imaging information. However, graph construction is not a trivial task and might significantly affect the performance of the GCN, which is inherently very sensitive to the graph structure. In this work, we propose a framework that learns a population graph structure optimized for the downstream task. An attention mechanism assigns weights to a set of imaging and non-imaging features (phenotypes), which are then used for edge extraction. The resulting graph is used to train the GCN. The entire pipeline can be trained end-to-end. Additionally, by visualizing the attention weights that were the most important for the graph construction, we increase the interpretability of the graph. We use the UK Biobank, which provides a large variety of neuroimaging and non-imaging phenotypes, to evaluate our method on brain age regression and classification. The proposed method outperforms competing static graph approaches and other state-of-the-art adaptive methods. We further show that the assigned attention scores indicate that there are both imaging and non-imaging phenotypes that are informative for brain age estimation and are in agreement with the relevant literature.
</details>
<details>
<summary>摘要</summary>
�XV�urrency age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer's. Population graphs, which include multimodal imaging information of the subjects along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks. A population graph is usually static and constructed manually using non-imaging information. However, graph construction is not a trivial task and might significantly affect the performance of the GCN, which is inherently very sensitive to the graph structure. In this work, we propose a framework that learns a population graph structure optimized for the downstream task. An attention mechanism assigns weights to a set of imaging and non-imaging features (phenotypes), which are then used for edge extraction. The resulting graph is used to train the GCN. The entire pipeline can be trained end-to-end. Additionally, by visualizing the attention weights that were the most important for the graph construction, we increase the interpretability of the graph. We use the UK Biobank, which provides a large variety of neuroimaging and non-imaging phenotypes, to evaluate our method on brain age regression and classification. The proposed method outperforms competing static graph approaches and other state-of-the-art adaptive methods. We further show that the assigned attention scores indicate that there are both imaging and non-imaging phenotypes that are informative for brain age estimation and are in agreement with the relevant literature.Here's the translation in Traditional Chinese:��XV�urrency age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer's. Population graphs, which include multimodal imaging information of the subjects along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks. A population graph is usually static and constructed manually using non-imaging information. However, graph construction is not a trivial task and might significantly affect the performance of the GCN, which is inherently very sensitive to the graph structure. In this work, we propose a framework that learns a population graph structure optimized for the downstream task. An attention mechanism assigns weights to a set of imaging and non-imaging features (phenotypes), which are then used for edge extraction. The resulting graph is used to train the GCN. The entire pipeline can be trained end-to-end. Additionally, by visualizing the attention weights that were the most important for the graph construction, we increase the interpretability of the graph. We use the UK Biobank, which provides a large variety of neuroimaging and non-imaging phenotypes, to evaluate our method on brain age regression and classification. The proposed method outperforms competing static graph approaches and other state-of-the-art adaptive methods. We further show that the assigned attention scores indicate that there are both imaging and non-imaging phenotypes that are informative for brain age estimation and are in agreement with the relevant literature.
</details></li>
</ul>
<hr>
<h2 id="SPLAL-Similarity-based-pseudo-labeling-with-alignment-loss-for-semi-supervised-medical-image-classification"><a href="#SPLAL-Similarity-based-pseudo-labeling-with-alignment-loss-for-semi-supervised-medical-image-classification" class="headerlink" title="SPLAL: Similarity-based pseudo-labeling with alignment loss for semi-supervised medical image classification"></a>SPLAL: Similarity-based pseudo-labeling with alignment loss for semi-supervised medical image classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04610">http://arxiv.org/abs/2307.04610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Junaid Mahmood, Pranaw Raj, Divyansh Agarwal, Suruchi Kumari, Pravendra Singh</li>
<li>for: 这篇论文的目的是提出一个新的半监督学习（SSL）方法，以便在医疗影像分类中处理缺乏标签的资料。</li>
<li>methods: 本论文提出的方法是使用分类 prototype 和复数分类器的加权组合来预测不标的影像标签。此外，它还引入了平衡损失来减少模型偏好的问题。</li>
<li>results: 实验结果显示，我们的提案方法在两个公开的医疗影像分类 benchmark 数据集上（ISIC 2018 和 BCCD）表现出色，较前一些状态的 SSL 方法在不同的评估指标上表现出明显的改善。具体来说，我们的方法在 ISIC 2018 数据集上的 Accuracy 和 F1 分别提高了2.24% 和11.40%。此外，我们还进行了广泛的折冲实验，以验证方法的有效性。<details>
<summary>Abstract</summary>
Medical image classification is a challenging task due to the scarcity of labeled samples and class imbalance caused by the high variance in disease prevalence. Semi-supervised learning (SSL) methods can mitigate these challenges by leveraging both labeled and unlabeled data. However, SSL methods for medical image classification need to address two key challenges: (1) estimating reliable pseudo-labels for the images in the unlabeled dataset and (2) reducing biases caused by class imbalance. In this paper, we propose a novel SSL approach, SPLAL, that effectively addresses these challenges. SPLAL leverages class prototypes and a weighted combination of classifiers to predict reliable pseudo-labels over a subset of unlabeled images. Additionally, we introduce alignment loss to mitigate model biases toward majority classes. To evaluate the performance of our proposed approach, we conduct experiments on two publicly available medical image classification benchmark datasets: the skin lesion classification (ISIC 2018) and the blood cell classification dataset (BCCD). The experimental results empirically demonstrate that our approach outperforms several state-of-the-art SSL methods over various evaluation metrics. Specifically, our proposed approach achieves a significant improvement over the state-of-the-art approach on the ISIC 2018 dataset in both Accuracy and F1 score, with relative margins of 2.24\% and 11.40\%, respectively. Finally, we conduct extensive ablation experiments to examine the contribution of different components of our approach, validating its effectiveness.
</details>
<details>
<summary>摘要</summary>
医学图像分类是一项具有挑战性的任务，原因在于缺乏标注样本和类别偏度导致的高度变化。 semi-supervised learning（SSL）方法可以减轻这些挑战，通过利用标注和未标注数据。然而，SSL方法 для医学图像分类需要解决两个关键挑战：（1）估算未标注图像的可靠 Pseudo-标签和（2）减轻类别偏度引起的模型偏好。在这篇论文中，我们提出了一种新的SSL方法，称为SPLAL，可以有效地解决这些挑战。SPLAL利用类 prototype和权重组合的类ifier来预测未标注图像的可靠 Pseudo-标签。此外，我们引入了对齐损失来减轻模型偏好向主要类别。为评估我们提出的方法的性能，我们在两个公开可用的医学图像分类标准测试集上进行了实验：皮肤病诊断（ISIC 2018）和血球分类 dataset（BCCD）。实验结果证明，我们的方法在多种评价指标上超越了一些状态方法。具体来说，我们的方法在 ISIC 2018 数据集上取得了显著的改进，其中 Accuracy 和 F1 score 分别提高了2.24%和11.40%。最后，我们进行了广泛的减少实验，以验证我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Source-Free-Open-Set-Domain-Adaptation-for-Histopathological-Images-via-Distilling-Self-Supervised-Vision-Transformer"><a href="#Source-Free-Open-Set-Domain-Adaptation-for-Histopathological-Images-via-Distilling-Self-Supervised-Vision-Transformer" class="headerlink" title="Source-Free Open-Set Domain Adaptation for Histopathological Images via Distilling Self-Supervised Vision Transformer"></a>Source-Free Open-Set Domain Adaptation for Histopathological Images via Distilling Self-Supervised Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04596">http://arxiv.org/abs/2307.04596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lts5/proto-sf-osda">https://github.com/lts5/proto-sf-osda</a></li>
<li>paper_authors: Guillaume Vray, Devavrat Tomar, Behzad Bozorgtabar, Jean-Philippe Thiran</li>
<li>for: 这个论文的目的是提出一种可行的设定，以解决批量诊断图像分类中的开集难题，包括减轻批量诊断图像的标注负担，以及在不可访问源数据集的情况下，将知识从目标频谱中提取出来，并同时检测开集样本。</li>
<li>methods: 这个论文使用了一种自我超vised vision transformer，通过在目标频谱中进行自我超vised学习，提取出高度Contextualized embedding。然后，使用semantic相似性来分 grouped target images，并使用源模型提供的weak pseudo-labels来权重调整这些分 grouped image的类别代表。</li>
<li>results: 这个论文的实验结果表明，其方法可以significantly outperform previous methods，包括开集检测、测试时 Adaptation 和 SF-OSDA方法，在三个公共的 histopathological 数据集上（Kather-16、Kather-19和CRCTP） achieve state-of-the-art 性能。<details>
<summary>Abstract</summary>
There is a strong incentive to develop computational pathology models to i) ease the burden of tissue typology annotation from whole slide histological images; ii) transfer knowledge, e.g., tissue class separability from the withheld source domain to the distributionally shifted unlabeled target domain, and simultaneously iii) detect Open Set samples, i.e., unseen novel categories not present in the training source domain. This paper proposes a highly practical setting by addressing the abovementioned challenges in one fell swoop, i.e., source-free Open Set domain adaptation (SF-OSDA), which addresses the situation where a model pre-trained on the inaccessible source dataset can be adapted on the unlabeled target dataset containing Open Set samples. The central tenet of our proposed method is distilling knowledge from a self-supervised vision transformer trained in the target domain. We propose a novel style-based data augmentation used as hard positives for self-training a vision transformer in the target domain, yielding strongly contextualized embedding. Subsequently, semantically similar target images are clustered while the source model provides their corresponding weak pseudo-labels with unreliable confidence. Furthermore, we propose cluster relative maximum logit score (CRMLS) to rectify the confidence of the weak pseudo-labels and compute weighted class prototypes in the contextualized embedding space that are utilized for adapting the source model on the target domain. Our method significantly outperforms the previous methods, including open set detection, test-time adaptation, and SF-OSDA methods, setting the new state-of-the-art on three public histopathological datasets of colorectal cancer (CRC) assessment- Kather-16, Kather-19, and CRCTP. Our code is available at https://github.com/LTS5/Proto-SF-OSDA.
</details>
<details>
<summary>摘要</summary>
There is a strong incentive to develop computational pathology models to ease the burden of tissue typology annotation from whole slide histological images, transfer knowledge from the withheld source domain to the distributionally shifted unlabeled target domain, and simultaneously detect Open Set samples. This paper proposes a highly practical setting by addressing the above challenges in one fell swoop, i.e., source-free Open Set domain adaptation (SF-OSDA), which addresses the situation where a model pre-trained on the inaccessible source dataset can be adapted on the unlabeled target dataset containing Open Set samples. The central tenet of our proposed method is distilling knowledge from a self-supervised vision transformer trained in the target domain. We propose a novel style-based data augmentation used as hard positives for self-training a vision transformer in the target domain, yielding strongly contextualized embeddings. Subsequently, semantically similar target images are clustered while the source model provides their corresponding weak pseudo-labels with unreliable confidence. Furthermore, we propose cluster relative maximum logit score (CRMLS) to rectify the confidence of the weak pseudo-labels and compute weighted class prototypes in the contextualized embedding space that are utilized for adapting the source model on the target domain. Our method significantly outperforms the previous methods, including open set detection, test-time adaptation, and SF-OSDA methods, setting the new state-of-the-art on three public histopathological datasets of colorectal cancer (CRC) assessment- Kather-16, Kather-19, and CRCTP. Our code is available at https://github.com/LTS5/Proto-SF-OSDA.
</details></li>
</ul>
<hr>
<h2 id="DWA-Differential-Wavelet-Amplifier-for-Image-Super-Resolution"><a href="#DWA-Differential-Wavelet-Amplifier-for-Image-Super-Resolution" class="headerlink" title="DWA: Differential Wavelet Amplifier for Image Super-Resolution"></a>DWA: Differential Wavelet Amplifier for Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04593">http://arxiv.org/abs/2307.04593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian B. Moser, Stanislav Frolov, Federico Raue, Sebastian Palacio, Andreas Dengel</li>
<li>For: 提高图像超分辨率（SR）模型的可持续性和效能。* Methods: 使用Discrete Wavelet Transformation（DWT）和两个卷积 filters的差异来提高波лет频域中特征提取和噪声抑制。* Results: 通过与现有SR模型（如DWSR和MWCNN）结合， demonstarted improved SR performance in classical SR tasks, and also enabled direct application of these models to the input image space, reducing the DWT representation channel-wise.<details>
<summary>Abstract</summary>
This work introduces Differential Wavelet Amplifier (DWA), a drop-in module for wavelet-based image Super-Resolution (SR). DWA invigorates an approach recently receiving less attention, namely Discrete Wavelet Transformation (DWT). DWT enables an efficient image representation for SR and reduces the spatial area of its input by a factor of 4, the overall model size, and computation cost, framing it as an attractive approach for sustainable ML. Our proposed DWA model improves wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We show its effectiveness by integrating it into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear improvement in classical SR tasks. Moreover, DWA enables a direct application of DWSR and MWCNN to input image space, reducing the DWT representation channel-wise since it omits traditional DWT.
</details>
<details>
<summary>摘要</summary>
Our proposed DWA model enhances wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We demonstrate the effectiveness of DWA by integrating it into existing SR models, such as DWSR and MWCNN, and show a clear improvement in classical SR tasks.Moreover, DWA enables a direct application of DWSR and MWCNN to the input image space, reducing the DWT representation channel-wise since it omits traditional DWT. This simplifies the model architecture and reduces computational complexity, making it more sustainable and efficient.
</details></li>
</ul>
<hr>
<h2 id="A-Graph-Multi-separator-Problem-for-Image-Segmentation"><a href="#A-Graph-Multi-separator-Problem-for-Image-Segmentation" class="headerlink" title="A Graph Multi-separator Problem for Image Segmentation"></a>A Graph Multi-separator Problem for Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04592">http://arxiv.org/abs/2307.04592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannik Irmai, Shengxian Zhao, Jannik Presberger, Bjoern Andres</li>
<li>for: 本研究提出了一种新的图像分割任务抽象方法，称为多分隔问题（Multi-separator problem），用于寻找每个像素是否属于一个分割或分隔结构。</li>
<li>methods: 本研究使用了两种特殊情况的解法和两种本地搜索算法来解决多分隔问题。</li>
<li>results: 在模拟的三维图像中分割 foam cells 和 filaments 时，本研究的方法得到了有效的结果。<details>
<summary>Abstract</summary>
We propose a novel abstraction of the image segmentation task in the form of a combinatorial optimization problem that we call the multi-separator problem. Feasible solutions indicate for every pixel whether it belongs to a segment or a segment separator, and indicate for pairs of pixels whether or not the pixels belong to the same segment. This is in contrast to the closely related lifted multicut problem where every pixel is associated to a segment and no pixel explicitly represents a separating structure. While the multi-separator problem is NP-hard, we identify two special cases for which it can be solved efficiently. Moreover, we define two local search algorithms for the general case and demonstrate their effectiveness in segmenting simulated volume images of foam cells and filaments.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的图像分割任务抽象方法，即多分隔器问题（Multi-Separator Problem）。这个问题的可行解释是每个像素是否属于一个分割结构中，以及每对像素是否属于同一个分割结构。这与它与提高的多分隔器问题（Lifted Multicut Problem）相似，但不同的是，在这个问题中没有像素Explicitly表示分割结构。虽然多分隔器问题是NP困难的，但我们发现了两种特殊情况，可以有效解决。此外，我们还定义了两种本地搜索算法，并在模拟的壳体细胞和纤维物体图像中进行了效果证明。
</details></li>
</ul>
<hr>
<h2 id="AnyTeleop-A-General-Vision-Based-Dexterous-Robot-Arm-Hand-Teleoperation-System"><a href="#AnyTeleop-A-General-Vision-Based-Dexterous-Robot-Arm-Hand-Teleoperation-System" class="headerlink" title="AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System"></a>AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04577">http://arxiv.org/abs/2307.04577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk, Hao Su, Xiaolong Wang, Yu-Wei Chao, Dieter Fox</li>
<li>for: 提供多种机器人模型和运行环境下的智能远程操作系统</li>
<li>methods: 使用一个通用的和普适的 теле操作系统，支持多种臂、手、现实和摄像头配置</li>
<li>results: 在实际实验中，AnyTeleop可以比之前特定机器人硬件的系统更高的成功率，同时在虚拟实验中也可以比之前特定模拟器的系统更好的模仿学习性能<details>
<summary>Abstract</summary>
Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better imitation learning performance, compared with a previous system that is particularly designed for that simulator. Project page: http://anyteleop.com/.
</details>
<details>
<summary>摘要</summary>
“视觉基于的 теле操作提供了让机器人具备人类水平的智能，只需要低成本的摄像头传感器。然而，当前的视觉基于的 теле操作系统是为特定的机器人模型和部署环境设计和工程，这会导致系统在机器人模型池的扩展和操作环境的多样性增加时，缺乏扩展性。在这篇论文中，我们提出了 AnyTeleop，一个通用和总体的 теле操作系统，可以支持多种不同的机器人臂、手、现实和摄像头配置。尽管我们的系统设计了很大的灵活性，但它仍然可以达到高性能。在实际实验中， AnyTeleop 可以比一个特定机器人硬件的前一个系统高效性更高，使用同一个机器人。在 simulations 中， AnyTeleop 可以比一个特定 simulator 的前一个系统更好地学习模仿性。项目页面：http://anyteleop.com/。”
</details></li>
</ul>
<hr>
<h2 id="TFR-Texture-Defect-Detection-with-Fourier-Transform-using-Normal-Reconstructed-Template-of-Simple-Autoencoder"><a href="#TFR-Texture-Defect-Detection-with-Fourier-Transform-using-Normal-Reconstructed-Template-of-Simple-Autoencoder" class="headerlink" title="TFR: Texture Defect Detection with Fourier Transform using Normal Reconstructed Template of Simple Autoencoder"></a>TFR: Texture Defect Detection with Fourier Transform using Normal Reconstructed Template of Simple Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04574">http://arxiv.org/abs/2307.04574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jongwook Si, Sungyoung Kim</li>
<li>for: 本研究旨在检测图像中的纹理缺陷，以提高图像质量和检测精度。</li>
<li>methods: 本研究使用了简单的自动编码器和傅立叫变换，并将这两种方法组合使用。傅立叫变换是一种强大的图像分析工具，可以在频谱域中分析图像和信号。</li>
<li>results: 实验结果表明，该方法可以准确和有效地检测纹理缺陷。与现有方法相比，该方法的性能和精度更高。<details>
<summary>Abstract</summary>
Texture is an essential information in image representation, capturing patterns and structures. As a result, texture plays a crucial role in the manufacturing industry and is extensively studied in the fields of computer vision and pattern recognition. However, real-world textures are susceptible to defects, which can degrade image quality and cause various issues. Therefore, there is a need for accurate and effective methods to detect texture defects. In this study, a simple autoencoder and Fourier transform are employed for texture defect detection. The proposed method combines Fourier transform analysis with the reconstructed template obtained from the simple autoencoder. Fourier transform is a powerful tool for analyzing the frequency domain of images and signals. Moreover, since texture defects often exhibit characteristic changes in specific frequency ranges, analyzing the frequency domain enables effective defect detection. The proposed method demonstrates effectiveness and accuracy in detecting texture defects. Experimental results are presented to evaluate its performance and compare it with existing approaches.
</details>
<details>
<summary>摘要</summary>
Texture 是图像表示中的关键信息，捕捉 Patterns 和结构。因此，Texture 在制造业中扮演着重要的角色，在计算机视觉和模式识别领域广泛研究。然而，实际世界中的 Texture 容易受到缺陷的影响，这可能会下降图像质量并导致多种问题。因此，有一需要高精度和有效的方法来检测 Texture 缺陷。在本研究中，我们employs a simple autoencoder 和 Fourier transform 来检测 Texture 缺陷。我们的方法结合了 Fourier transform 分析和从 simple autoencoder 获得的重建模板。Fourier transform 是一个强大的图像和信号分析工具，而 texture 缺陷通常在特定频谱范围内表现出特征变化，因此在频谱域进行分析可以有效地检测缺陷。我们的方法在检测 Texture 缺陷方面表现出了高效和准确的性能。实验结果用于评估我们的方法和现有方法的比较。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Age-Estimation-Puzzle-Comparative-Analysis-of-Deep-Learning-Approaches-for-Facial-Age-Estimation"><a href="#Unraveling-the-Age-Estimation-Puzzle-Comparative-Analysis-of-Deep-Learning-Approaches-for-Facial-Age-Estimation" class="headerlink" title="Unraveling the Age Estimation Puzzle: Comparative Analysis of Deep Learning Approaches for Facial Age Estimation"></a>Unraveling the Age Estimation Puzzle: Comparative Analysis of Deep Learning Approaches for Facial Age Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04570">http://arxiv.org/abs/2307.04570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paplhjak/facial-age-estimation-benchmark">https://github.com/paplhjak/facial-age-estimation-benchmark</a></li>
<li>paper_authors: Jakub Paplham, Vojtech Franc</li>
<li>for: 本研究旨在解决不同年龄估计方法之间的比较困难，即由于benchmarking过程中的不一致性导致出版的结果不可靠。</li>
<li>methods: 本研究使用cross-entropy损失函数作为标准方法，并系统分析了影响年龄估计结果的多种因素，包括脸部对alignment、脸部覆盖率、图像分辨率、图像表示方式、模型架构以及数据量。</li>
<li>results: 研究发现，这些因素frequently exerts a more significant influence than the choice of age estimation method itself，而且使用一致的数据处理方法和建立标准benchmarks是 Ensure reliable and meaningful comparisons.<details>
<summary>Abstract</summary>
Comparing different age estimation methods poses a challenge due to the unreliability of published results, stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. We argue that, for age estimation tasks outside of the low-data regime, designing specialized methods is unnecessary, and the standard approach of utilizing cross-entropy loss is sufficient. This paper aims to address the benchmark shortcomings by evaluating state-of-the-art age estimation methods in a unified and comparable setting. We systematically analyze the impact of various factors, including facial alignment, facial coverage, image resolution, image representation, model architecture, and the amount of data on age estimation results. Surprisingly, these factors often exert a more significant influence than the choice of the age estimation method itself. We assess the generalization capability of each method by evaluating the cross-dataset performance for publicly available age estimation datasets. The results emphasize the importance of using consistent data preprocessing practices and establishing standardized benchmarks to ensure reliable and meaningful comparisons. The source code is available at https://github.com/paplhjak/Facial-Age-Estimation-Benchmark.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:与不同的年龄估计方法进行比较存在挑战，主要是因为发表的结果不可靠，这主要归结于测试过程中的不一致。先前的研究报告过去十年内，通过特殊方法进行改进，但我们的发现推翻了这些宣称。我们认为，在低数据域以外，特殊的方法设计是不必要的，使用权重损失是充分的。这篇论文旨在解决测试缺陷，对现有的年龄估计方法进行统一和可比较的评估。我们系统地分析了不同因素的影响，包括人脸匹配、人脸覆盖率、图像分辨率、图像表示方式、模型体系和数据量，这些因素经常对年龄估计结果产生更大的影响，而不是选择年龄估计方法本身。我们评估每种方法的通用能力，通过评估公共可用的年龄估计数据集上的交叉数据表现来评估。结果表明，使用一致的数据处理方法和建立标准化的标准是确保可靠和有意义的比较的重要。软件代码可以在https://github.com/paplhjak/Facial-Age-Estimation-Benchmark上获取。
</details></li>
</ul>
<hr>
<h2 id="Important-Clues-that-Facilitate-Visual-Emergence-Three-Psychological-Experiments"><a href="#Important-Clues-that-Facilitate-Visual-Emergence-Three-Psychological-Experiments" class="headerlink" title="Important Clues that Facilitate Visual Emergence: Three Psychological Experiments"></a>Important Clues that Facilitate Visual Emergence: Three Psychological Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10194">http://arxiv.org/abs/2307.10194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingmeng Li, Hui Wei</li>
<li>for: 本研究旨在探讨视觉 emergence 现象中的因素。</li>
<li>methods: 研究采用三个心理实验，以探索 emerging 图像的识别因素。</li>
<li>results: 研究发现，本地区域的斑点密度和一些关键斑点的排列有关于 emerging 图像的识别。<details>
<summary>Abstract</summary>
Visual emergence is the phenomenon in which the visual system obtains a holistic perception after grouping and reorganizing local signals. The picture Dalmatian dog is known for its use in explaining visual emergence. This type of image, which consists of a set of discrete black speckles (speckles), is called an emerging image. Not everyone can find the dog in Dalmatian dog, and among those who can, the time spent varies greatly. Although Gestalt theory summarizes perceptual organization into several principles, it remains ambiguous how these principles affect the perception of emerging images. This study, therefore, designed three psychological experiments to explore the factors that influence the perception of emerging images. In the first, we found that the density of speckles in the local area and the arrangements of some key speckles played a key role in the perception of an emerging case. We set parameters in the algorithm to characterize these two factors. We then automatically generated diversified emerging-test images (ETIs) through the algorithm and verified their effectiveness in two subsequent experiments.
</details>
<details>
<summary>摘要</summary>
“视觉突出现”是指视系统从局部信号集成和重新排序后获得整体视觉。达尔马提亚狗图例是用于解释视觉突出现的图像。这种图像由一组独立的黑点（点）组成，被称为突出现图像。不 everyone可以在达尔马提亚狗图中找到狗，而且那些找到狗的人之间的时间花费差异很大。虽然格式理论总结了感知组织化的几个原则，但是这些原则如何影响突出现图像的感知仍然抽象。这个研究因此设计了三个心理实验，探讨突出现图像的因素。在第一个实验中，我们发现了局部区域的密度和一些关键点的排列对突出现的感知产生了关键作用。我们在算法中设置了这两个因素的参数。然后我们通过算法自动生成多样化的突出现测试图像（ETIs），并在两个后续实验中证明了它们的有效性。
</details></li>
</ul>
<hr>
<h2 id="SparseVSR-Lightweight-and-Noise-Robust-Visual-Speech-Recognition"><a href="#SparseVSR-Lightweight-and-Noise-Robust-Visual-Speech-Recognition" class="headerlink" title="SparseVSR: Lightweight and Noise Robust Visual Speech Recognition"></a>SparseVSR: Lightweight and Noise Robust Visual Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04552">http://arxiv.org/abs/2307.04552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adriana Fernandez-Lopez, Honglie Chen, Pingchuan Ma, Alexandros Haliassos, Stavros Petridis, Maja Pantic</li>
<li>for: 这个研究旨在测试不同的几何基于簇节约技术，以生成轻量级的模型，并评估其对于视觉声译解的表现。</li>
<li>methods: 本研究使用了深度神经网络，并应用了不同的几何基于簇节约技术，以生成轻量级的模型。</li>
<li>results: 研究发现，使用几何基于簇节约技术可以实现轻量级的模型，并且在视觉声译解任务中表现更好，尤其是在视觉噪音存在的情况下。<details>
<summary>Abstract</summary>
Recent advances in deep neural networks have achieved unprecedented success in visual speech recognition. However, there remains substantial disparity between current methods and their deployment in resource-constrained devices. In this work, we explore different magnitude-based pruning techniques to generate a lightweight model that achieves higher performance than its dense model equivalent, especially under the presence of visual noise. Our sparse models achieve state-of-the-art results at 10% sparsity on the LRS3 dataset and outperform the dense equivalent up to 70% sparsity. We evaluate our 50% sparse model on 7 different visual noise types and achieve an overall absolute improvement of more than 2% WER compared to the dense equivalent. Our results confirm that sparse networks are more resistant to noise than dense networks.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的深度神经网络技术已经取得了无 precedent 的成功 в视觉语音识别领域。然而，目前的方法和其在资源受限的设备中的部署仍然存在很大的差距。在这项工作中，我们探索了不同的大小基于的剪除技术，以生成一个轻量级的模型，其性能高于其稠密模型等效的，尤其是在视觉噪声存在的情况下。我们的稀疏模型在 LRS3 数据集上达到了state-of-the-art 的结果，并在 10% 稀疏率下超过了稠密模型的性能。我们的 50% 稀疏模型在 7 种不同的视觉噪声类型下进行评估，实现了全部绝对改进超过 2% WER 的成果，相比稠密模型。我们的结果表明，稀疏网络比稠密网络更抗障压。
</details></li>
</ul>
<hr>
<h2 id="Customizing-Synthetic-Data-for-Data-Free-Student-Learning"><a href="#Customizing-Synthetic-Data-for-Data-Free-Student-Learning" class="headerlink" title="Customizing Synthetic Data for Data-Free Student Learning"></a>Customizing Synthetic Data for Data-Free Student Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04542">http://arxiv.org/abs/2307.04542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luoshiya/csd">https://github.com/luoshiya/csd</a></li>
<li>paper_authors: Shiya Luo, Defang Chen, Can Wang</li>
<li>for: 这篇论文目的是提出一种基于无数据知识的学生模型训练方法，即Customizing Synthetic Data for Data-Free Student Learning（CSD）。</li>
<li>methods: 该方法使用一种自我超vised的补充任务来估算学生学习能力，然后根据这个估算值进行数据生成动态调整，以生成难样本 для学生模型。</li>
<li>results: 实验结果表明，该方法可以在不同的数据集和教师-学生模型下达到效果，并且可以提高学生模型的性能。代码可以在： $\href{<a target="_blank" rel="noopener" href="https://github.com/luoshiya/CSD%7D%7Bhttps://github.com/luoshiya/CSD%7D$">https://github.com/luoshiya/CSD}{https://github.com/luoshiya/CSD}$</a> 获取。<details>
<summary>Abstract</summary>
Data-free knowledge distillation (DFKD) aims to obtain a lightweight student model without original training data. Existing works generally synthesize data from the pre-trained teacher model to replace the original training data for student learning. To more effectively train the student model, the synthetic data shall be customized to the current student learning ability. However, this is ignored in the existing DFKD methods and thus negatively affects the student training. To address this issue, we propose Customizing Synthetic Data for Data-Free Student Learning (CSD) in this paper, which achieves adaptive data synthesis using a self-supervised augmented auxiliary task to estimate the student learning ability. Specifically, data synthesis is dynamically adjusted to enlarge the cross entropy between the labels and the predictions from the self-supervised augmented task, thus generating hard samples for the student model. The experiments on various datasets and teacher-student models show the effectiveness of our proposed method. Code is available at: $\href{https://github.com/luoshiya/CSD}{https://github.com/luoshiya/CSD}$
</details>
<details>
<summary>摘要</summary>
“数据空间知识储备（DFKD）目的是获得轻量级学生模型无需原始训练数据。现有的方法通常将先行训练的教师模型中的数据进行合成，以替代学生学习的原始数据。但是现有的DFKD方法忽略了对学生学习能力的适应，这会负面影响学生模型的训练。为解决这个问题，我们在本文提出了适应数据生成 для无数据学习（CSD）方法，它通过自我监督增强任务来估算学生学习能力，并在数据生成中进行动态调整。具体来说，通过自我监督增强任务来增加label和预测值之间的cross entropy，从而生成硬样本 для学生模型。实验结果表明，我们的提议方法在不同的数据集和教师-学生模型上具有效果。代码可以在： $\href{https://github.com/luoshiya/CSD}{https://github.com/luoshiya/CSD}$”
</details></li>
</ul>
<hr>
<h2 id="Cluster-Induced-Mask-Transformers-for-Effective-Opportunistic-Gastric-Cancer-Screening-on-Non-contrast-CT-Scans"><a href="#Cluster-Induced-Mask-Transformers-for-Effective-Opportunistic-Gastric-Cancer-Screening-on-Non-contrast-CT-Scans" class="headerlink" title="Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans"></a>Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04525">http://arxiv.org/abs/2307.04525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingze Yuan, Yingda Xia, Xin Chen, Jiawen Yao, Junli Wang, Mingyan Qiu, Hexin Dong, Jingren Zhou, Bin Dong, Le Lu, Li Zhang, Zaiyi Liu, Ling Zhang<br>for: 这个研究旨在检测胃癌，它是全球第三大导致死亡癌症之一，但是没有建议的检测测试。现有的方法可能是侵入性高，成本高，敏感度低，无法检测早期胃癌。methods: 我们使用了一种深度学习方法，使用非对照CT扫描图进行胃癌检测。我们提出了一种新的归一化引导Transporter，该模型同时分割肿瘤并将异常性分类。我们的模型包括学习集的概率，使用自我和对自我的关注来交互对征特征。results: 我们的方法在一个测试集上达到了敏感性85.0%和特征性92.6%，比两名医生的平均敏感性73.5%和特征性84.3%高。此外，我们在一个外部测试集上获得了特征性97.7%。我们的方法与血液测试和endoscopia等现有的胃癌检测工具相当，同时具有更高的敏感性，可能成为一种新的、不侵入性、成本低、准确的胃癌creening方法。<details>
<summary>Abstract</summary>
Gastric cancer is the third leading cause of cancer-related mortality worldwide, but no guideline-recommended screening test exists. Existing methods can be invasive, expensive, and lack sensitivity to identify early-stage gastric cancer. In this study, we explore the feasibility of using a deep learning approach on non-contrast CT scans for gastric cancer detection. We propose a novel cluster-induced Mask Transformer that jointly segments the tumor and classifies abnormality in a multi-task manner. Our model incorporates learnable clusters that encode the texture and shape prototypes of gastric cancer, utilizing self- and cross-attention to interact with convolutional features. In our experiments, the proposed method achieves a sensitivity of 85.0% and specificity of 92.6% for detecting gastric tumors on a hold-out test set consisting of 100 patients with cancer and 148 normal. In comparison, two radiologists have an average sensitivity of 73.5% and specificity of 84.3%. We also obtain a specificity of 97.7% on an external test set with 903 normal cases. Our approach performs comparably to established state-of-the-art gastric cancer screening tools like blood testing and endoscopy, while also being more sensitive in detecting early-stage cancer. This demonstrates the potential of our approach as a novel, non-invasive, low-cost, and accurate method for opportunistic gastric cancer screening.
</details>
<details>
<summary>摘要</summary>
胃癌是全球第三大的癌症相关死亡原因，但没有任何指南推荐的检测试exist。现有方法可能是侵入的、昂贵的，并且缺乏感知到早期胃癌的能力。在这项研究中，我们研究了使用深度学习方法来非对照CT扫描图像中的胃癌检测。我们提出了一种新的归一化Transformer，它同时分割肿瘤并将异常性分类，使用多任务方式进行协同学习。我们的模型利用学习的归一化来编码胃癌的纹理和形态谱，通过自我和十字关注来交互与卷积特征。在我们的实验中，我们的方法达到了检测胃癌的敏感性85.0%和特征精度92.6%，与两名医生的平均检测率73.5%和84.3%相比。此外，我们还获得了97.7%的特征精度在外部测试集上，该测试集包含903个正常例。我们的方法与已有的胃癌检测工具如血液测试和内视镜相比，同时具有更高的敏感性，可能成为一种新的、不侵入的、低成本的和准确的胃癌creening方法。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Match-Pair-Retrieval-for-Large-scale-UAV-Images-via-Graph-Indexed-Global-Descriptor"><a href="#Efficient-Match-Pair-Retrieval-for-Large-scale-UAV-Images-via-Graph-Indexed-Global-Descriptor" class="headerlink" title="Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor"></a>Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04520">http://arxiv.org/abs/2307.04520</a></li>
<li>repo_url: None</li>
<li>paper_authors: San Jiang, Yichen Ma, Qingquan Li, Wanshou Jiang, Bingxuan Guo, Lelin Li, Lizhe Wang<br>for: 这 paper 是用于提高 UAV 图像姿态重建的 Structure from Motion (SfM) 方法的研究。methods: 该 paper 提出了一种高效的匹配对选择方法，包括在线训练个性化 Codebook，使用 VLAD 综合器将本地特征转换为高维度全局描述符，并使用 HNSW 图结构进行最近匹配搜索。results: 测试结果表明，提出的解决方案可以减少匹配对选择的计算量，并且在相对和绝对姿态重建中具有竞争力的准确率。<details>
<summary>Abstract</summary>
SfM (Structure from Motion) has been extensively used for UAV (Unmanned Aerial Vehicle) image orientation. Its efficiency is directly influenced by feature matching. Although image retrieval has been extensively used for match pair selection, high computational costs are consumed due to a large number of local features and the large size of the used codebook. Thus, this paper proposes an efficient match pair retrieval method and implements an integrated workflow for parallel SfM reconstruction. First, an individual codebook is trained online by considering the redundancy of UAV images and local features, which avoids the ambiguity of training codebooks from other datasets. Second, local features of each image are aggregated into a single high-dimension global descriptor through the VLAD (Vector of Locally Aggregated Descriptors) aggregation by using the trained codebook, which remarkably reduces the number of features and the burden of nearest neighbor searching in image indexing. Third, the global descriptors are indexed via the HNSW (Hierarchical Navigable Small World) based graph structure for the nearest neighbor searching. Match pairs are then retrieved by using an adaptive threshold selection strategy and utilized to create a view graph for divide-and-conquer based parallel SfM reconstruction. Finally, the performance of the proposed solution has been verified using three large-scale UAV datasets. The test results demonstrate that the proposed solution accelerates match pair retrieval with a speedup ratio ranging from 36 to 108 and improves the efficiency of SfM reconstruction with competitive accuracy in both relative and absolute orientation.
</details>
<details>
<summary>摘要</summary>
SfM (结构从运动) 已广泛应用于无人飞行器（UAV）图像方向。其效率直接取决于特征匹配。 although image retrieval has been extensively used for match pair selection, high computational costs are consumed due to a large number of local features and the large size of the used codebook. Therefore, this paper proposes an efficient match pair retrieval method and implements an integrated workflow for parallel SfM reconstruction.First, an individual codebook is trained online by considering the redundancy of UAV images and local features, which avoids the ambiguity of training codebooks from other datasets. Second, local features of each image are aggregated into a single high-dimensional global descriptor through the VLAD (Vector of Locally Aggregated Descriptors) aggregation by using the trained codebook, which remarkably reduces the number of features and the burden of nearest neighbor searching in image indexing.Third, the global descriptors are indexed via the HNSW (Hierarchical Navigable Small World) based graph structure for the nearest neighbor searching. Match pairs are then retrieved by using an adaptive threshold selection strategy and utilized to create a view graph for divide-and-conquer based parallel SfM reconstruction.Finally, the performance of the proposed solution has been verified using three large-scale UAV datasets. The test results demonstrate that the proposed solution accelerates match pair retrieval with a speedup ratio ranging from 36 to 108 and improves the efficiency of SfM reconstruction with competitive accuracy in both relative and absolute orientation.
</details></li>
</ul>
<hr>
<h2 id="An-Examination-of-Wearable-Sensors-and-Video-Data-Capture-for-Human-Exercise-Classification"><a href="#An-Examination-of-Wearable-Sensors-and-Video-Data-Capture-for-Human-Exercise-Classification" class="headerlink" title="An Examination of Wearable Sensors and Video Data Capture for Human Exercise Classification"></a>An Examination of Wearable Sensors and Video Data Capture for Human Exercise Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04516">http://arxiv.org/abs/2307.04516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashish Singh, Antonio Bevilacqua, Timilehin B. Aderinola, Thach Le Nguyen, Darragh Whelan, Martin O’Reilly, Brian Caulfield, Georgiana Ifrim<br>for:这种研究用于评估人体运动表现，并使用增强式计时系统（IMU）和视频技术进行分类。methods:这种研究使用了单个相机和5个IMU，并使用了多变量时间序列分类器来处理数据。results:研究发现，使用单个相机可以在平均上高于使用单个IMU的表现，而且至少需要3个IMU来与单个相机相比。此外，将数据从单个相机和单个IMU组合在一起可以达到更高的表现。<details>
<summary>Abstract</summary>
Wearable sensors such as Inertial Measurement Units (IMUs) are often used to assess the performance of human exercise. Common approaches use handcrafted features based on domain expertise or automatically extracted features using time series analysis. Multiple sensors are required to achieve high classification accuracy, which is not very practical. These sensors require calibration and synchronization and may lead to discomfort over longer time periods. Recent work utilizing computer vision techniques has shown similar performance using video, without the need for manual feature engineering, and avoiding some pitfalls such as sensor calibration and placement on the body. In this paper, we compare the performance of IMUs to a video-based approach for human exercise classification on two real-world datasets consisting of Military Press and Rowing exercises. We compare the performance using a single camera that captures video in the frontal view versus using 5 IMUs placed on different parts of the body. We observe that an approach based on a single camera can outperform a single IMU by 10 percentage points on average. Additionally, a minimum of 3 IMUs are required to outperform a single camera. We observe that working with the raw data using multivariate time series classifiers outperforms traditional approaches based on handcrafted or automatically extracted features. Finally, we show that an ensemble model combining the data from a single camera with a single IMU outperforms either data modality. Our work opens up new and more realistic avenues for this application, where a video captured using a readily available smartphone camera, combined with a single sensor, can be used for effective human exercise classification.
</details>
<details>
<summary>摘要</summary>
佩戴式感测器如力度测量单元（IMU）经常用于评估人类运动表现。常见的方法使用域专业知识或自动提取的特征来制定手动特征工程。使用多个感测器可以获得高分类精度，但是不实际。这些感测器需要协调和同步，并可能会导致身体部位的不适，尤其是在长时间内。现有的计算机视觉技术研究表示，通过视频来评估人类运动，可以达到类似的性能，而不需要手动特征工程，并避免一些障碍物，如感测器协调和身体部位的印象。在这篇论文中，我们对IMUs和视频方法进行了人类运动分类的比较，使用了两个实际数据集，一个是军事推举操作，另一个是舟行操作。我们发现，使用单个摄像头捕捉视频的方法可以在 average 上高于单个IMU的性能，并且至少需要3个IMUs以达到单个摄像头的性能。此外，我们发现，使用原始数据进行多变量时间序列分类可以超越传统的手动特征工程和自动提取特征的方法。最后，我们表明，将单个摄像头和单个IMU的数据 ensemble 起来可以超越任何一个数据类型。我们的工作开启了新的、更实际的应用途径，通过使用普遍可用的智能手机摄像头，并与单个感测器结合，可以实现有效的人类运动分类。
</details></li>
</ul>
<hr>
<h2 id="CoactSeg-Learning-from-Heterogeneous-Data-for-New-Multiple-Sclerosis-Lesion-Segmentation"><a href="#CoactSeg-Learning-from-Heterogeneous-Data-for-New-Multiple-Sclerosis-Lesion-Segmentation" class="headerlink" title="CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation"></a>CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04513">http://arxiv.org/abs/2307.04513</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ycwu1997/coactseg">https://github.com/ycwu1997/coactseg</a></li>
<li>paper_authors: Yicheng Wu, Zhonghua Wu, Hengcan Shi, Bjoern Picker, Winston Chong, Jianfei Cai<br>for:* 新型多发性纤维病（MS）诊断和治疗中的新损害分 segmentation 问题具有重要的实际意义，因为它可以帮助评估疾病的进程和治疗效果。methods:* 提出了一种协同分割（CoactSeg）框架，利用新损害注意点和所有损害注意点的同时注入，以提高新损害分 segmentation 的性能。* 提出了一种简单而有效的时间关系约束，以保证新损害和所有损害之间的长期关系，从而改进模型学习。results:* 利用多种数据（包括新损害注意点和所有损害注意点）和提出的时间关系约束，可以明显提高新损害分 segmentation 的性能。* 在评估新损害和所有损害分 segmentation 任务中，模型表现出色，并且在不同的数据上进行了广泛的验证。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
New lesion segmentation is essential to estimate the disease progression and therapeutic effects during multiple sclerosis (MS) clinical treatments. However, the expensive data acquisition and expert annotation restrict the feasibility of applying large-scale deep learning models. Since single-time-point samples with all-lesion labels are relatively easy to collect, exploiting them to train deep models is highly desirable to improve new lesion segmentation. Therefore, we proposed a coaction segmentation (CoactSeg) framework to exploit the heterogeneous data (i.e., new-lesion annotated two-time-point data and all-lesion annotated single-time-point data) for new MS lesion segmentation. The CoactSeg model is designed as a unified model, with the same three inputs (the baseline, follow-up, and their longitudinal brain differences) and the same three outputs (the corresponding all-lesion and new-lesion predictions), no matter which type of heterogeneous data is being used. Moreover, a simple and effective relation regularization is proposed to ensure the longitudinal relations among the three outputs to improve the model learning. Extensive experiments demonstrate that utilizing the heterogeneous data and the proposed longitudinal relation constraint can significantly improve the performance for both new-lesion and all-lesion segmentation tasks. Meanwhile, we also introduce an in-house MS-23v1 dataset, including 38 Oceania single-time-point samples with all-lesion labels. Codes and the dataset are released at https://github.com/ycwu1997/CoactSeg.
</details>
<details>
<summary>摘要</summary>
新的肿瘤分割是评估多发性硬化病（MS）治疗效果和病程进程中的关键。然而，收集数据和专家标注的成本限制了大规模深度学习模型的应用性。由于单时点检查结果具有所有肿瘤标注，因此利用它们来训练深度模型是非常有利的。因此，我们提出了合作分割（CoactSeg）框架，以利用不同类型的数据（包括新肿瘤标注的两个时点数据和所有肿瘤标注的单个时点数据）进行新肿瘤分割。CoactSeg模型采用同一个输入（基线、追加和其中间脑变化）和同一个输出（对应的所有肿瘤和新肿瘤预测），无论使用哪种不同类型的数据。此外，我们还提出了一种简单而有效的时间相关约束，以提高模型学习。我们的实验表明，利用多种数据和提议的时间相关约束可以显著提高新肿瘤和所有肿瘤分割任务的性能。此外，我们还推出了一个MS-23v1 dataset，包括38个澳大利亚单时点样本，每个样本均有所有肿瘤标注。代码和数据可以在https://github.com/ycwu1997/CoactSeg中下载。
</details></li>
</ul>
<hr>
<h2 id="Exact-Diffusion-Inversion-via-Bi-directional-Integration-Approximation"><a href="#Exact-Diffusion-Inversion-via-Bi-directional-Integration-Approximation" class="headerlink" title="Exact Diffusion Inversion via Bi-directional Integration Approximation"></a>Exact Diffusion Inversion via Bi-directional Integration Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10829">http://arxiv.org/abs/2307.10829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoqiang Zhang, J. P. Lewis, W. Bastiaan Kleijn</li>
<li>for: 减少Diffusion Inversion Matrix（DDIM）的计算复杂度，实现图像修改和重建。</li>
<li>methods: 提出了一种新的技术——双向集成近似（BDIA），通过两次DDIM更新过程来实现精确的扩散推算，并且可以在不同的ODE解析器上进行改进。</li>
<li>results: 通过实验证明，BDIA可以减少计算复杂度，并且在图像重建和修改中获得更好的性能。<details>
<summary>Abstract</summary>
Recently, different methods have been proposed to address the inconsistency issue of DDIM inversion to enable image editing, such as EDICT \cite{Wallace23EDICT} and Null-text inversion \cite{Mokady23NullTestInv}. However, the above methods introduce considerable computational overhead. In this paper, we propose a new technique, named \emph{bi-directional integration approximation} (BDIA), to perform exact diffusion inversion with neglible computational overhead. Suppose we would like to estimate the next diffusion state $\boldsymbol{z}_{i-1}$ at timestep $t_i$ with the historical information $(i,\boldsymbol{z}_i)$ and $(i+1,\boldsymbol{z}_{i+1})$. We first obtain the estimated Gaussian noise $\hat{\boldsymbol{\epsilon}(\boldsymbol{z}_i,i)$, and then apply the DDIM update procedure twice for approximating the ODE integration over the next time-slot $[t_i, t_{i-1}]$ in the forward manner and the previous time-slot $[t_i, t_{t+1}]$ in the backward manner. The DDIM step for the previous time-slot is used to refine the integration approximation made earlier when computing $\boldsymbol{z}_i$. One nice property with BDIA-DDIM is that the update expression for $\boldsymbol{z}_{i-1}$ is a linear combination of $(\boldsymbol{z}_{i+1}, \boldsymbol{z}_i, \hat{\boldsymbol{\epsilon}(\boldsymbol{z}_i,i))$. This allows for exact backward computation of $\boldsymbol{z}_{i+1}$ given $(\boldsymbol{z}_i, \boldsymbol{z}_{i-1})$, thus leading to exact diffusion inversion. Experiments on both image reconstruction and image editing were conducted, confirming our statement. BDIA can also be applied to improve the performance of other ODE solvers in addition to DDIM. In our work, it is found that applying BDIA to the EDM sampling procedure produces slightly better FID score over CIFAR10.
</details>
<details>
<summary>摘要</summary>
最近，有些方法已经被提出来解决了DDIM逆向问题的不一致性issue，如EDICT \cite{Wallace23EDICT}和Null-text逆向 \cite{Mokady23NullTestInv}。然而，这些方法会带来较大的计算开销。在这篇论文中，我们提出了一新的方法，即双向集成approximation（BDIA），以实现精确的扩散逆向。假设我们想要在时间步长$t_i$中计算下一个扩散状态$\boldsymbol{z}_{i-1}$，使用历史信息 $(i,\boldsymbol{z}_i)$ 和 $(i+1,\boldsymbol{z}_{i+1})$。我们首先获取估计的高斯噪声 $\hat{\boldsymbol{\epsilon}(\boldsymbol{z}_i,i)$，然后使用DDIM更新过程两次来approximate ODE  интеграção在下一个时间步长 $[t_i, t_{i-1}]$ 和上一个时间步长 $[t_i, t_{t+1}]$ 之间的integração。DDIM步骤用于上一个时间步长来精度地计算以前的integração，从而提高了integration的精度。BDIA-DDIM的更新表达式为$\boldsymbol{z}_{i-1}$ 是 $\boldsymbol{z}_{i+1}$, $\boldsymbol{z}_i$ 和 $\hat{\boldsymbol{\epsilon}(\boldsymbol{z}_i,i)$ 的线性组合。这使得可以精确地计算 $\boldsymbol{z}_{i+1}$  given $(\boldsymbol{z}_i, \boldsymbol{z}_{i-1})$，从而实现精确的扩散逆向。我们在图像重建和图像修改方面进行了实验，并证明了我们的说法。BDIA还可以用于提高其他ODE解析器的性能，而不仅仅是DDIM。在我们的工作中，我们发现了应用BDIA到EDM采样过程中会提高FID分数。
</details></li>
</ul>
<hr>
<h2 id="Partial-Vessels-Annotation-based-Coronary-Artery-Segmentation-with-Self-training-and-Prototype-Learning"><a href="#Partial-Vessels-Annotation-based-Coronary-Artery-Segmentation-with-Self-training-and-Prototype-Learning" class="headerlink" title="Partial Vessels Annotation-based Coronary Artery Segmentation with Self-training and Prototype Learning"></a>Partial Vessels Annotation-based Coronary Artery Segmentation with Self-training and Prototype Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04472">http://arxiv.org/abs/2307.04472</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangz7112/pva-cas">https://github.com/zhangz7112/pva-cas</a></li>
<li>paper_authors: Zheng Zhang, Xiaolei Zhang, Yaolei Qi, Guanyu Yang</li>
<li>for: 这个研究旨在提出一种 partial vessels annotation (PVA) based 的 label-efficient learning algorithm, 以应对 coronary artery segmentation 的挑战和临床诊断特点。</li>
<li>methods: 提出了一个 progressive weakly supervised learning 框架，包括 local feature learning、global structure learning 和 feature embedding 等步骤，以实现精确的 segmentation。</li>
<li>results: 在临床数据上的实验表明，提出的框架比竞争方法在 PVA 下表现出色 (24.29% 血管), 并且与基线模型使用完整注释 (100% 血管) 的表现相似。<details>
<summary>Abstract</summary>
Coronary artery segmentation on coronary-computed tomography angiography (CCTA) images is crucial for clinical use. Due to the expertise-required and labor-intensive annotation process, there is a growing demand for the relevant label-efficient learning algorithms. To this end, we propose partial vessels annotation (PVA) based on the challenges of coronary artery segmentation and clinical diagnostic characteristics. Further, we propose a progressive weakly supervised learning framework to achieve accurate segmentation under PVA. First, our proposed framework learns the local features of vessels to propagate the knowledge to unlabeled regions. Subsequently, it learns the global structure by utilizing the propagated knowledge, and corrects the errors introduced in the propagation process. Finally, it leverages the similarity between feature embeddings and the feature prototype to enhance testing outputs. Experiments on clinical data reveals that our proposed framework outperforms the competing methods under PVA (24.29% vessels), and achieves comparable performance in trunk continuity with the baseline model using full annotation (100% vessels).
</details>
<details>
<summary>摘要</summary>
coronary artery 分 segmentation on coronary-computed tomography angiography (CCTA) 图像是临床使用的关键。由于注意力和劳动力需求的注意力和劳动力需求，因此有一种增长的需求 für relevant label-efficient learning algorithms。为此，我们提出了partial vessels annotation (PVA)，基于 coronary artery segmentation 和临床诊断特点的挑战。然后，我们提出了一种进步的弱监督学习框架，以实现高精度的分 segmentation。首先，我们的提出的框架学习了local feature of vessels，以帮助它在无标注区域中传播知识。然后，它学习了全局结构，利用传播的知识，并 corrections 了在传播过程中引入的错误。最后，它利用特征嵌入和特征原型之间的相似性，提高测试输出。在临床数据上的实验表明，我们的提出的框架在 PVA (24.29% 血管) 下比竞争方法表现出色，并与基于全注解 (100% 血管) 的基eline模型具有相同的 trunk continuity 性能。
</details></li>
</ul>
<hr>
<h2 id="Test-Time-Adaptation-for-Nighttime-Color-Thermal-Semantic-Segmentation"><a href="#Test-Time-Adaptation-for-Nighttime-Color-Thermal-Semantic-Segmentation" class="headerlink" title="Test-Time Adaptation for Nighttime Color-Thermal Semantic Segmentation"></a>Test-Time Adaptation for Nighttime Color-Thermal Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04470">http://arxiv.org/abs/2307.04470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yexin Liu, Weiming Zhang, Guoyang Zhao, Jinjing Zhu, Athanasios Vasilakos, Lin Wang</li>
<li>For: The paper focuses on improving the performance of RGB-Thermal (RGB-T) semantic segmentation in nighttime scenes, which is a challenging task due to the large day-night gap and the inconsistent performance of RGB images at night.* Methods: The proposed method, called Night-TTA, uses a test-time adaptation (TTA) framework to address the challenges of nighttime RGB-T semantic segmentation without requiring access to the source (daytime) data during adaptation. The method consists of three key technical parts: Imaging Heterogeneity Refinement (IHR), Class Aware Refinement (CAR), and a specific learning scheme.* Results: The proposed method achieves state-of-the-art (SoTA) performance with a 13.07% boost in mean Intersection over Union (mIoU) compared to the baseline method.<details>
<summary>Abstract</summary>
The ability to scene understanding in adverse visual conditions, e.g., nighttime, has sparked active research for RGB-Thermal (RGB-T) semantic segmentation. However, it is essentially hampered by two critical problems: 1) the day-night gap of RGB images is larger than that of thermal images, and 2) the class-wise performance of RGB images at night is not consistently higher or lower than that of thermal images. we propose the first test-time adaptation (TTA) framework, dubbed Night-TTA, to address the problems for nighttime RGBT semantic segmentation without access to the source (daytime) data during adaptation. Our method enjoys three key technical parts. Firstly, as one modality (e.g., RGB) suffers from a larger domain gap than that of the other (e.g., thermal), Imaging Heterogeneity Refinement (IHR) employs an interaction branch on the basis of RGB and thermal branches to prevent cross-modal discrepancy and performance degradation. Then, Class Aware Refinement (CAR) is introduced to obtain reliable ensemble logits based on pixel-level distribution aggregation of the three branches. In addition, we also design a specific learning scheme for our TTA framework, which enables the ensemble logits and three student logits to collaboratively learn to improve the quality of predictions during the testing phase of our Night TTA. Extensive experiments show that our method achieves state-of-the-art (SoTA) performance with a 13.07% boost in mIoU.
</details>
<details>
<summary>摘要</summary>
《夜间RGB-热图（RGB-T）semantic segmentation的研究受到了日间和夜间视觉条件下的场景理解能力的激发。然而，这些研究受到了两个重要问题的限制：1）RGB图像的日夜差距比热图更大，2）夜间RGB图像的类别性能不一定高于或低于热图。我们提出了第一个测试时适应（TTA）框架，名为夜间TTA，以解决这些问题。我们的方法具有三个关键技术部分。首先，由于一种感知Modal（例如RGB）的频谱差异比另一种Modal（例如热图）更大，我们使用了Imaging Heterogeneity Refinement（IHR）模块，该模块基于RGB和热图分支来避免交互Modal差异并避免性能下降。其次，我们引入了Class Aware Refinement（CAR）模块，以基于像素级分布聚合来获取可靠的ensemble logits。此外，我们还设计了特殊的学习方案 для我们的TTA框架，这使得ensemble logits和三个学生logits可以共同学习以提高测试阶段的预测质量。广泛的实验表明，我们的方法可以 дости到当前最佳性能（SoTA），并提高了13.07%的mIoU。》Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SAM-IQA-Can-Segment-Anything-Boost-Image-Quality-Assessment"><a href="#SAM-IQA-Can-Segment-Anything-Boost-Image-Quality-Assessment" class="headerlink" title="SAM-IQA: Can Segment Anything Boost Image Quality Assessment?"></a>SAM-IQA: Can Segment Anything Boost Image Quality Assessment?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04455">http://arxiv.org/abs/2307.04455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hedlen/sam-iqa">https://github.com/hedlen/sam-iqa</a></li>
<li>paper_authors: Xinpeng Li, Ting Jiang, Haoqiang Fan, Shuaicheng Liu</li>
<li>for: 本研究旨在提高图像质量评估（IQA）的精度，通过使用大量数据进行训练以获得更加准确的预测结果。</li>
<li>methods: 本研究使用了 Segment Anything 模型的Encoder部分，以提取高级别的semantic特征。同时，通过施加 fourier 和标准卷积操作，以提取频域特征。</li>
<li>results: 实验结果表明，使用我们提议的方法可以在四个代表性的数据集上超越当前最佳状态（SOTA）， both qualitatively and quantitatively。这些实验证明了 Segment Anything 模型的强大特征提取能力，以及频域和空间域特征的组合在 IQA 任务中的价值。<details>
<summary>Abstract</summary>
Image Quality Assessment (IQA) is a challenging task that requires training on massive datasets to achieve accurate predictions. However, due to the lack of IQA data, deep learning-based IQA methods typically rely on pre-trained networks trained on massive datasets as feature extractors to enhance their generalization ability, such as the ResNet network trained on ImageNet. In this paper, we utilize the encoder of Segment Anything, a recently proposed segmentation model trained on a massive dataset, for high-level semantic feature extraction. Most IQA methods are limited to extracting spatial-domain features, while frequency-domain features have been shown to better represent noise and blur. Therefore, we leverage both spatial-domain and frequency-domain features by applying Fourier and standard convolutions on the extracted features, respectively. Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively and quantitatively. Our experiments confirm the powerful feature extraction capabilities of Segment Anything and highlight the value of combining spatial-domain and frequency-domain features in IQA tasks. Code: https://github.com/Hedlen/SAM-IQA
</details>
<details>
<summary>摘要</summary>
Image Quality Assessment (IQA) 是一个复杂的任务，需要训练大量数据来获得准确的预测。然而，由于缺乏 IQA 数据，深度学习基于 IQA 方法通常会使用预训练的网络作为特征提取器，以增强其泛化能力，如 ImageNet 上的 ResNet 网络。在这篇论文中，我们利用 Segment Anything 中的encoder，一个最近提出的分割模型，在大量数据上进行高级别semantic特征提取。大多数 IQA 方法只是提取空间频域特征，而频率频域特征已经被证明可以更好地表示噪音和模糊。因此，我们利用空间频域和频率频域特征，通过Fourier和标准 convolutions应用于提取的特征，分别进行特征提取。我们进行了广泛的实验，以证明所提出的所有组件的效果，结果表明我们的方法在四个代表性数据集上， both qualitatively and quantitatively 超越了状态前的最佳方法。我们的实验证明了 Segment Anything 的强大特征提取能力，并高亮了在 IQA 任务中，结合空间频域和频率频域特征的价值。代码：https://github.com/Hedlen/SAM-IQA
</details></li>
</ul>
<hr>
<h2 id="Search-time-Efficient-Device-Constraints-Aware-Neural-Architecture-Search"><a href="#Search-time-Efficient-Device-Constraints-Aware-Neural-Architecture-Search" class="headerlink" title="Search-time Efficient Device Constraints-Aware Neural Architecture Search"></a>Search-time Efficient Device Constraints-Aware Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04443">http://arxiv.org/abs/2307.04443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oshin Dutta, Tanu Kanvar, Sumeet Agarwal</li>
<li>for: 这篇论文目标是解决边缘计算领域中的深度学习问题，即使用边缘设备进行数据处理，而不需要依赖云计算。</li>
<li>methods: 该论文使用了神经网络搜索（NAS）技术自动构建适合边缘设备的特定任务深度学习架构，并包括了设备约束（如模型大小和浮点运算）。它还使用了共享权重和扁担束技术以加速搜索过程。</li>
<li>results: 根据实验表明，DCA-NAS比手动设计的架构更高效，并与流行的移动架构在多个图像分类数据集上达到了相似的性能水平。此外，通过在DARTS和NAS-Bench-201上进行搜索空间的测试，表明DCA-NAS具有广泛的通用能力。最后，通过在Hardware-NAS-Bench上进行评估，发现了适用于具体设备的特定架构，并达到了低于10ms的推理延迟和现场性能的国际前景。<details>
<summary>Abstract</summary>
Edge computing aims to enable edge devices, such as IoT devices, to process data locally instead of relying on the cloud. However, deep learning techniques like computer vision and natural language processing can be computationally expensive and memory-intensive. Creating manual architectures specialized for each device is infeasible due to their varying memory and computational constraints. To address these concerns, we automate the construction of task-specific deep learning architectures optimized for device constraints through Neural Architecture Search (NAS). We present DCA-NAS, a principled method of fast neural network architecture search that incorporates edge-device constraints such as model size and floating-point operations. It incorporates weight sharing and channel bottleneck techniques to speed up the search time. Based on our experiments, we see that DCA-NAS outperforms manual architectures for similar sized models and is comparable to popular mobile architectures on various image classification datasets like CIFAR-10, CIFAR-100, and Imagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the generalization capabilities of DCA-NAS. On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art performance were discovered.
</details>
<details>
<summary>摘要</summary>
Edge computing旨在让边缘设备，如IoT设备，在云上进行数据处理而不依赖于云。然而，深度学习技术如计算机视觉和自然语言处理可能会占用许多计算资源和内存。为了解决这些问题，我们自动化了任务特定的深度学习建筑，以适应设备的变化。我们称之为DCA-NAS，它是一种快速的神经网络建筑搜索方法，它包括设备约束such as模型大小和浮点运算。它还包括共享权重和扁拟束技术来加速搜索时间。根据我们的实验，DCA-NAS比相同大小的手动建筑更高效，与流行的移动设备建筑相当在多个图像分类数据集上，如CIFAR-10、CIFAR-100和Imagenet-1k。我们的实验还表明DCA-NAS可以在不同的搜索空间中进行普适化，例如DARTS和NAS-Bench-201。进一步评估我们的方法在Hardware-NAS-Bench上，我们发现了适用于具体设备的硬件特定建筑，具有低评估延迟和当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Automatic-diagnosis-of-knee-osteoarthritis-severity-using-Swin-transformer"><a href="#Automatic-diagnosis-of-knee-osteoarthritis-severity-using-Swin-transformer" class="headerlink" title="Automatic diagnosis of knee osteoarthritis severity using Swin transformer"></a>Automatic diagnosis of knee osteoarthritis severity using Swin transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04442">http://arxiv.org/abs/2307.04442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aymen Sekhri, Marouane Tliba, Mohamed Amine Kerkouri, Yassine Nasser, Aladine Chetouani, Alessandro Bruno, Rachid Jennane</li>
<li>for: 预测拟合股骨关节疾病（KOA）的严重程度</li>
<li>methods: 使用SwinTransformer自动预测KOA严重程度，使用公共可用的 radiographic数据集（Kellgren和Lawrence分数）进行早期检测和严重评估</li>
<li>results: 实验结果表明我们的方法可以准确预测KOA严重程度<details>
<summary>Abstract</summary>
Knee osteoarthritis (KOA) is a widespread condition that can cause chronic pain and stiffness in the knee joint. Early detection and diagnosis are crucial for successful clinical intervention and management to prevent severe complications, such as loss of mobility. In this paper, we propose an automated approach that employs the Swin Transformer to predict the severity of KOA. Our model uses publicly available radiographic datasets with Kellgren and Lawrence scores to enable early detection and severity assessment. To improve the accuracy of our model, we employ a multi-prediction head architecture that utilizes multi-layer perceptron classifiers. Additionally, we introduce a novel training approach that reduces the data drift between multiple datasets to ensure the generalization ability of the model. The results of our experiments demonstrate the effectiveness and feasibility of our approach in predicting KOA severity accurately.
</details>
<details>
<summary>摘要</summary>
《骨关节炎（KOA）是一种广泛存在的疾病，可以导致chronic pain和股关节僵硬。早期发现和诊断是成功临床干预和管理的关键，以避免严重的complications，如 mobilit产生 loss。在这篇论文中，我们提出了一种自动化的方法，使用Swin Transformer来预测KOA的严重程度。我们的模型使用公共可用的 radiographic数据集来实现早期发现和严重评估。为了提高模型的准确性，我们使用多个预测头 Architecture，并使用多层感知器来进行分类。此外，我们还引入了一种新的训练方法，以降低数据的漂移，以确保模型的泛化能力。实验结果表明我们的方法可以准确地预测KOA的严重程度。》Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Global-and-Local-Visual-Processing-Influence-of-Perceptual-Field-Variables"><a href="#Global-and-Local-Visual-Processing-Influence-of-Perceptual-Field-Variables" class="headerlink" title="Global and Local Visual Processing: Influence of Perceptual Field Variables"></a>Global and Local Visual Processing: Influence of Perceptual Field Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04435">http://arxiv.org/abs/2307.04435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Rezvani, Ali Katanforoush, Richard van Wezel, Hamidreza Pouretemad</li>
<li>for: 这个论文旨在探讨全球优势效应（GPE）在视觉过程中的普遍性，以及不同PERCEPTUAL FIELD VARIABLES（PFVs）对GPE的影响。</li>
<li>methods: 这个研究使用了各种实验方法，包括匹配判断和相似判断任务，以explore GPE在不同任务 paradigms下的表现。</li>
<li>results: 研究发现，CONGRUENCY和SIZE两个PFVs有显著的影响，而SPARSITY只有小的影响。此外，任务概念和PFVs之间的交互也有显著的效果。这些结果表明，任务概念在评估PFVs对GPE的影响中扮演着重要的角色。<details>
<summary>Abstract</summary>
The Global Precedence Effect (GPE) suggests that the processing of global properties of a visual stimulus precedes the processing of local properties. The generality of this theory was argued for four decades during different known Perceptual Field Variables. The effect size of various PFVs, regarding the findings during these four decades, were pooled in our recent meta-analysis study. Pursuing the study, in the present paper, we explore the effects of Congruency, Size, and Sparsity and their interaction on global advantage in two different experiments with different task paradigms; Matching judgment and Similarity judgment. Upon results of these experiments, Congruency and Size have significant effects and Sparsity has small effects. Also, the task paradigm and its interaction with other PFVs are shown significant effects in this study, which shows the prominence of the role of task paradigms in evaluating PFVs' effects on GPE. Also, we found that the effects of these parameters were not specific to the special condition that individuals were instructed to retinal stabilize. So, the experiments were more extendible to daily human behavior.
</details>
<details>
<summary>摘要</summary>
globally  precedes  processing  of  local  properties,  the  Global  Precedence  Effect  (GPE)  suggests.  这个理论四十年来在不同的知觉场景中得到了证明，包括多种知觉场景中的效果大小的Pooling。在当前的论文中，我们探究了 Congruency、Size 和 Sparsity 对全局优势的影响，并在不同的任务模式下进行了两个实验。结果显示，Congruency 和 Size 具有显著影响，而 Sparsity 具有微scopic 影响。此外，任务模式和其他知觉场景之间的交互也在这些实验中得到了显著影响，这表明任务模式在评估知觉场景的效果中扮演着重要的角色。此外，我们发现这些参数的影响不仅限于特殊的指导困难者保持视场的情况，而且可以推广到日常人类行为。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-Hemorrhage-and-Infarct-Lesions-on-Brain-CT-Images-using-Deep-Learning"><a href="#Identification-of-Hemorrhage-and-Infarct-Lesions-on-Brain-CT-Images-using-Deep-Learning" class="headerlink" title="Identification of Hemorrhage and Infarct Lesions on Brain CT Images using Deep Learning"></a>Identification of Hemorrhage and Infarct Lesions on Brain CT Images using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04425">http://arxiv.org/abs/2307.04425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arunkumar Govindarajan, Arjun Agarwal, Subhankar Chattoraj, Dennis Robert, Satish Golla, Ujjwal Upadhyay, Swetha Tanamala, Aarthi Govindarajan</li>
<li>for: This paper aims to evaluate the performance of a deep learning-based algorithm in identifying intracranial hemorrhage (ICH) and infarct from head non-contrast computed tomography (NCCT) scans.</li>
<li>methods: The paper uses a deep learning-based algorithm to automatically identify ICH and infarct from head-NCCT scans. The dataset used for validation consists of head-NCCT scans collected from multiple diagnostic imaging centers across India.</li>
<li>results: The study shows the potential and limitations of using a DL-based algorithm for identifying ICH and infarct from head-NCCT scans. The algorithm demonstrated high accuracy in identifying ICH and infarct, but the results also highlighted the limitations of using this approach in routine clinical practice due to factors such as image quality and dataset variability.<details>
<summary>Abstract</summary>
Head Non-contrast computed tomography (NCCT) scan remain the preferred primary imaging modality due to their widespread availability and speed. However, the current standard for manual annotations of abnormal brain tissue on head NCCT scans involves significant disadvantages like lack of cutoff standardization and degeneration identification. The recent advancement of deep learning-based computer-aided diagnostic (CAD) models in the multidisciplinary domain has created vast opportunities in neurological medical imaging. Significant literature has been published earlier in the automated identification of brain tissue on different imaging modalities. However, determining Intracranial hemorrhage (ICH) and infarct can be challenging due to image texture, volume size, and scan quality variability. This retrospective validation study evaluated a DL-based algorithm identifying ICH and infarct from head-NCCT scans. The head-NCCT scans dataset was collected consecutively from multiple diagnostic imaging centers across India. The study exhibits the potential and limitations of such DL-based software for introduction in routine workflow in extensive healthcare facilities.
</details>
<details>
<summary>摘要</summary>
<<SYS>>头部非对比计算机断层成像（NCCT）扫描仍然是首选的主要图像Modalität，因为它们的普及性和速度很高。然而，现有的手动标注不正常脑组织头部NCCT扫描的标准具有一些缺点，如标准化的分割标准和衰变标识。随着深度学习基于计算机支持的诊断（CAD）模型在多学科领域的发展，头部神经医学成像中的自动识别技术受到了广泛的关注。 Earlier literature has been published on the automated identification of brain tissue on different imaging modalities. However, determining intracranial hemorrhage (ICH) and infarct can be challenging due to image texture, volume size, and scan quality variability. This retrospective validation study evaluated a deep learning-based algorithm for identifying ICH and infarct from head-NCCT scans. The head-NCCT scans dataset was collected consecutively from multiple diagnostic imaging centers across India. The study exhibits the potential and limitations of such deep learning-based software for introduction in routine workflow in extensive healthcare facilities.<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Towards-Enabling-Cardiac-Digital-Twins-of-Myocardial-Infarction-Using-Deep-Computational-Models-for-Inverse-Inference"><a href="#Towards-Enabling-Cardiac-Digital-Twins-of-Myocardial-Infarction-Using-Deep-Computational-Models-for-Inverse-Inference" class="headerlink" title="Towards Enabling Cardiac Digital Twins of Myocardial Infarction Using Deep Computational Models for Inverse Inference"></a>Towards Enabling Cardiac Digital Twins of Myocardial Infarction Using Deep Computational Models for Inverse Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04421">http://arxiv.org/abs/2307.04421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Li, Julia Camps, Zhinuo, Wang, Abhirup Banerjee, Marcel Beetz, Blanca Rodriguez, Vicente Grau</li>
<li>for: 这个研究旨在开发一个基于电子参数血液征象（ECG）的个体心脏模型，以实现个体化诊断和治疗规划的个体化诊断和治疗规划。</li>
<li>methods: 这个研究使用了多modal的数据，例如心脏MRI和ECG，以增强推论的精度和可靠性。实验使用了电脑模拟，系统地探讨了损伤的位置、大小、传播度和电子活动的变化对QRS复议的影响，以建立方法的限制。</li>
<li>results: 研究发现，提出的深度计算模型可以有效地捕捉QRS复议和相应的损伤区域之间的复杂关系，具有推广应用的潜力。<details>
<summary>Abstract</summary>
Myocardial infarction (MI) demands precise and swift diagnosis. Cardiac digital twins (CDTs) have the potential to offer individualized evaluation of cardiac function in a non-invasive manner, making them a promising approach for personalized diagnosis and treatment planning of MI. The inference of accurate myocardial tissue properties is crucial in creating a reliable CDT platform, and particularly in the context of studying MI. In this work, we investigate the feasibility of inferring myocardial tissue properties from the electrocardiogram (ECG), focusing on the development of a comprehensive CDT platform specifically designed for MI. The platform integrates multi-modal data, such as cardiac MRI and ECG, to enhance the accuracy and reliability of the inferred tissue properties. We perform a sensitivity analysis based on computer simulations, systematically exploring the effects of infarct location, size, degree of transmurality, and electrical activity alteration on the simulated QRS complex of ECG, to establish the limits of the approach. We subsequently propose a deep computational model to infer infarct location and distribution from the simulated QRS. The in silico experimental results show that our model can effectively capture the complex relationships between the QRS signals and the corresponding infarct regions, with promising potential for clinical application in the future. The code will be released publicly once the manuscript is accepted for publication.
</details>
<details>
<summary>摘要</summary>
In this study, we investigate the feasibility of inferring myocardial tissue properties from the electrocardiogram (ECG) to develop a comprehensive CDT platform for MI. We integrate multi-modal data, including cardiac MRI and ECG, to enhance the accuracy and reliability of the inferred tissue properties.To evaluate the effectiveness of our approach, we perform a sensitivity analysis based on computer simulations. We systematically explore the effects of infarct location, size, degree of transmurality, and electrical activity alteration on the simulated QRS complex of ECG. Our results show that our model can effectively capture the complex relationships between the QRS signals and the corresponding infarct regions, with promising potential for clinical application in the future.Our code will be released publicly once the manuscript is accepted for publication, providing a valuable resource for researchers and clinicians working in the field of MI diagnosis and treatment.
</details></li>
</ul>
<hr>
<h2 id="FODVid-Flow-guided-Object-Discovery-in-Videos"><a href="#FODVid-Flow-guided-Object-Discovery-in-Videos" class="headerlink" title="FODVid: Flow-guided Object Discovery in Videos"></a>FODVid: Flow-guided Object Discovery in Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04392">http://arxiv.org/abs/2307.04392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silky Singh, Shripad Deshmukh, Mausoom Sarkar, Rishabh Jain, Mayur Hemani, Balaji Krishnamurthy</li>
<li>for:  solve Video Object Segmentation (VOS) in an unsupervised setting</li>
<li>methods:  incorporating intra-frame appearance and flow similarities, and inter-frame temporal continuation of the objects under consideration</li>
<li>results:  results comparable (within a range of ~2 mIoU) to the existing top approaches in unsupervised VOS<details>
<summary>Abstract</summary>
Segmentation of objects in a video is challenging due to the nuances such as motion blurring, parallax, occlusions, changes in illumination, etc. Instead of addressing these nuances separately, we focus on building a generalizable solution that avoids overfitting to the individual intricacies. Such a solution would also help us save enormous resources involved in human annotation of video corpora. To solve Video Object Segmentation (VOS) in an unsupervised setting, we propose a new pipeline (FODVid) based on the idea of guiding segmentation outputs using flow-guided graph-cut and temporal consistency. Basically, we design a segmentation model incorporating intra-frame appearance and flow similarities, and inter-frame temporal continuation of the objects under consideration. We perform an extensive experimental analysis of our straightforward methodology on the standard DAVIS16 video benchmark. Though simple, our approach produces results comparable (within a range of ~2 mIoU) to the existing top approaches in unsupervised VOS. The simplicity and effectiveness of our technique opens up new avenues for research in the video domain.
</details>
<details>
<summary>摘要</summary>
视频对象分割是一项复杂的任务，因为存在许多细节，如运动模糊、相对位移、遮挡、照明变化等。而不是单独处理这些细节，我们强调总体化解决方案，以避免过拟合特定细节。这样的解决方案不仅可以减少对视频资料的人工标注成本，还可以开拓新的研究领域。为解决无监督视频对象分割（VOS）问题，我们提出了一个新的管道（FODVid），基于流场导向图割和时间一致性。我们设计了一种包含内帧出现相似性和流动相似性的分割模型，并且在不同帧之间继续对对象进行时间一致性。我们对标准的DAVIS16视频benchmark进行了广泛的实验分析，并证明了我们的简单方法在无监督VOS中可以获得比较好的结果（与对比方法的误差在 ~2 mIoU 之间）。我们的简单 yet effective 的方法打开了新的研究领域，特别是在视频领域。
</details></li>
</ul>
<hr>
<h2 id="CT-based-Subchondral-Bone-Microstructural-Analysis-in-Knee-Osteoarthritis-via-MR-Guided-Distillation-Learning"><a href="#CT-based-Subchondral-Bone-Microstructural-Analysis-in-Knee-Osteoarthritis-via-MR-Guided-Distillation-Learning" class="headerlink" title="CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning"></a>CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04390">http://arxiv.org/abs/2307.04390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jackhu-bme/srrd">https://github.com/jackhu-bme/srrd</a></li>
<li>paper_authors: Yuqi Hu, Xiangyu Zhao, Gaowei Qing, Kai Xie, Chenglei Liu, Lichi Zhang</li>
<li>For: The paper aims to develop a novel method for subchondral bone microstructural analysis using easily-acquired CT images, which can enhance the accuracy of knee osteoarthritis classification.* Methods: The proposed method, named SRRD, leverages paired MR images to enhance the CT-based analysis model during training. The method uses a GAN-based generative model to transform MR images into CT images, and a distillation-learning technique to transfer MR structural information to the CT-based model.* Results: The proposed method achieved high reliability and validity in MR-CT registration, regression, and knee osteoarthritis classification, with an AUC score of 0.767 (95% CI, 0.681-0.853). The use of distillation learning significantly improved the performance of the CT-based knee osteoarthritis classification method using the CNN approach.<details>
<summary>Abstract</summary>
Background: MR-based subchondral bone effectively predicts knee osteoarthritis. However, its clinical application is limited by the cost and time of MR. Purpose: We aim to develop a novel distillation-learning-based method named SRRD for subchondral bone microstructural analysis using easily-acquired CT images, which leverages paired MR images to enhance the CT-based analysis model during training. Materials and Methods: Knee joint images of both CT and MR modalities were collected from October 2020 to May 2021. Firstly, we developed a GAN-based generative model to transform MR images into CT images, which was used to establish the anatomical correspondence between the two modalities. Next, we obtained numerous patches of subchondral bone regions of MR images, together with their trabecular parameters (BV / TV, Tb. Th, Tb. Sp, Tb. N) from the corresponding CT image patches via regression. The distillation-learning technique was used to train the regression model and transfer MR structural information to the CT-based model. The regressed trabecular parameters were further used for knee osteoarthritis classification. Results: A total of 80 participants were evaluated. CT-based regression results of trabecular parameters achieved intra-class correlation coefficients (ICCs) of 0.804, 0.773, 0.711, and 0.622 for BV / TV, Tb. Th, Tb. Sp, and Tb. N, respectively. The use of distillation learning significantly improved the performance of the CT-based knee osteoarthritis classification method using the CNN approach, yielding an AUC score of 0.767 (95% CI, 0.681-0.853) instead of 0.658 (95% CI, 0.574-0.742) (p<.001). Conclusions: The proposed SRRD method showed high reliability and validity in MR-CT registration, regression, and knee osteoarthritis classification, indicating the feasibility of subchondral bone microstructural analysis based on CT images.
</details>
<details>
<summary>摘要</summary>
Materials and Methods: Knee joint CT and MR images were collected from October 2020 to May 2021. A GAN-based generative model was used to transform MR images into CT images for anatomical correspondence. Regression was used to obtain trabecular parameters (BV/TV, Tb. Th, Tb. Sp, Tb. N) from CT image patches. Distillation learning was used to train the regression model and transfer MR structural information to the CT-based model. The regressed trabecular parameters were used for knee osteoarthritis classification.Results: 80 participants were evaluated. CT-based regression achieved intra-class correlation coefficients (ICCs) of 0.804, 0.773, 0.711, and 0.622 for BV/TV, Tb. Th, Tb. Sp, and Tb. N, respectively. Distillation learning significantly improved the performance of the CT-based knee osteoarthritis classification method using the CNN approach, yielding an AUC score of 0.767 (95% CI, 0.681-0.853) instead of 0.658 (95% CI, 0.574-0.742) (p<.001).Conclusions: The proposed SRRD method showed high reliability and validity in MR-CT registration, regression, and knee osteoarthritis classification, indicating the feasibility of subchondral bone microstructural analysis based on CT images.
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalizable-Diabetic-Retinopathy-Grading-in-Unseen-Domains"><a href="#Towards-Generalizable-Diabetic-Retinopathy-Grading-in-Unseen-Domains" class="headerlink" title="Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains"></a>Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04378">http://arxiv.org/abs/2307.04378</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chehx/dgdr">https://github.com/chehx/dgdr</a></li>
<li>paper_authors: Haoxuan Che, Yuhan Cheng, Haibo Jin, Hao Chen<br>for:This paper aims to address the domain generalization problem in deep learning-based automated diabetic retinopathy (DR) grading, which hinders the real-world deployment of DR grading systems.methods:The proposed method, called Generalizable Diabetic Retinopathity Grading Network (GDRNet), consists of three components: fundus visual-artifact augmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), and domain-class-aware re-balancing (DCR).results:GDRNet achieves state-of-the-art performance on a publicly available benchmark and demonstrates better generalization ability than existing methods through extensive comparison experiments and ablation studies.<details>
<summary>Abstract</summary>
Diabetic Retinopathy (DR) is a common complication of diabetes and a leading cause of blindness worldwide. Early and accurate grading of its severity is crucial for disease management. Although deep learning has shown great potential for automated DR grading, its real-world deployment is still challenging due to distribution shifts among source and target domains, known as the domain generalization problem. Existing works have mainly attributed the performance degradation to limited domain shifts caused by simple visual discrepancies, which cannot handle complex real-world scenarios. Instead, we present preliminary evidence suggesting the existence of three-fold generalization issues: visual and degradation style shifts, diagnostic pattern diversity, and data imbalance. To tackle these issues, we propose a novel unified framework named Generalizable Diabetic Retinopathy Grading Network (GDRNet). GDRNet consists of three vital components: fundus visual-artifact augmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), and domain-class-aware re-balancing (DCR). FundusAug generates realistic augmented images via visual transformation and image degradation, while DahLoss jointly leverages pixel-level consistency and image-level semantics to capture the diverse diagnostic patterns and build generalizable feature representations. Moreover, DCR mitigates the data imbalance from a domain-class view and avoids undesired over-emphasis on rare domain-class pairs. Finally, we design a publicly available benchmark for fair evaluations. Extensive comparison experiments against advanced methods and exhaustive ablation studies demonstrate the effectiveness and generalization ability of GDRNet.
</details>
<details>
<summary>摘要</summary>
糖尿病综述病 (DR) 是 диабеت tipo 2 的常见并发症，并是全球最大的盲人原因。 早期和准确地评估其严重程度是疾病管理的关键。 虽然深度学习已经表现出了自动DR评估的潜在能力，但是在实际应用中仍然存在域泛化问题，即域域识别问题。 现有的工作主要归结于域域识别问题，即限制域域识别问题，但是我们发现了三种普遍问题：视觉和衰落风格差异、诊断样本多样性和数据不均衡。 为解决这些问题，我们提出了一个全新的框架，即普遍DR评估网络（GDRNet）。 GDRNet包括三个重要组成部分：基于图像增强的视觉增强（FundusAug）、动态混合监督损失（DahLoss）和域类感知重新平衡（DCR）。 FundusAug通过视觉变换和图像衰落来生成真实的增强图像，而 DahLoss 同时captures多样的诊断模式和建立普遍的特征表示。 此外，DCR 消除了域类视图中的数据不均衡，以避免不必要地强调少数域类对的独特对。 最后，我们设计了一个公共可用的标准套件，以便公平的评估。 对于先进方法和详细的抽象研究，我们进行了广泛的比较和探索。 结果表明，GDRNet 具有普遍能力和高效性。
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Pruning-for-Fast-adapting-Pre-trained-Models-on-Devices"><a href="#One-Shot-Pruning-for-Fast-adapting-Pre-trained-Models-on-Devices" class="headerlink" title="One-Shot Pruning for Fast-adapting Pre-trained Models on Devices"></a>One-Shot Pruning for Fast-adapting Pre-trained Models on Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04365">http://arxiv.org/abs/2307.04365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiyan Zhao, Guodong Long</li>
<li>for: 这篇 paper 的目的是提出一个可扩展的一体化推断运算方法，将大规模预训练模型部署到低能量设备上。</li>
<li>methods: 方法是根据类似任务的删除模型来提取适当大小的子网络，以便快速适束新任务。</li>
<li>results: 实验分析显示，提案的方法可以在不同的数据集上实现高精度和高效率，并且在不同的下游任务和设备上表现更好 than 对等的删除基eline方法。<details>
<summary>Abstract</summary>
Large-scale pre-trained models have been remarkably successful in resolving downstream tasks. Nonetheless, deploying these models on low-capability devices still requires an effective approach, such as model pruning. However, pruning the model from scratch can pose a practical challenge given the limited resources of each downstream task or device. To tackle this issue, we present a scalable one-shot pruning method that leverages pruned knowledge of similar tasks to extract a sub-network from the pre-trained model for a new task. Specifically, we create a score mask using the pruned models of similar tasks to identify task-specific filters/nodes in the pre-trained model for the new task. Based on this mask, we conduct a single round of pruning to extract a suitably-sized sub-network that can quickly adapt to the new task with only a few training iterations. Our experimental analysis demonstrates the effectiveness of the proposed method on the convolutional neural networks (CNNs) and vision transformers (ViT) with various datasets. The proposed method consistently outperforms popular pruning baseline methods in terms of accuracy and efficiency when dealing with diverse downstream tasks with different memory constraints.
</details>
<details>
<summary>摘要</summary>
大规模预训练模型已经在下游任务中表现惊人成功。然而，在低能力设备上部署这些模型仍然需要有效的方法，如模型剪除。然而，从零开始剪除模型可能是实际挑战，因为每个下游任务或设备的资源都是有限的。为解决这个问题，我们提出了可扩展的一键剪除方法，利用预剪的相似任务知识来提取预训练模型中的子网络 для新任务。特别是，我们创建了一个分数面罩，使用预剪模型中的相似任务来标识新任务中的任务特有的滤波器/节点。根据这个面罩，我们在单轮剪除中提取了适当大小的子网络，可以快速适应新任务，只需几个训练轮。我们的实验分析表明，我们的方法在 convolutional neural networks (CNNs) 和 vision transformers (ViT) 中对各种数据集表现出了效果。我们的方法在处理不同的下游任务时，常常超越流行的剪除基准方法，在准确性和效率方面。
</details></li>
</ul>
<hr>
<h2 id="InfLoR-SNN-Reducing-Information-Loss-for-Spiking-Neural-Networks"><a href="#InfLoR-SNN-Reducing-Information-Loss-for-Spiking-Neural-Networks" class="headerlink" title="InfLoR-SNN: Reducing Information Loss for Spiking Neural Networks"></a>InfLoR-SNN: Reducing Information Loss for Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04356">http://arxiv.org/abs/2307.04356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Guo, Yuanpei Chen, Liwen Zhang, Xiaode Liu, Xinyi Tong, Yuanyuan Ou, Xuhui Huang, Zhe Ma</li>
<li>for: 这个论文是为了提出一种基于超viscous reset的激活函数的神经网络模型，以提高神经网络的能效性和精度。</li>
<li>methods: 该论文使用了基于加法的信号传输方式，并提出了一种基于动态重置电压的”软重置”机制，以及一种名为”Rectifier”的电压重定义器，以减少量化误差。</li>
<li>results: 实验结果表明，使用”软重置”机制和”Rectifier”组合可以在静止和动态数据集上超过原始神经网络的性能。<details>
<summary>Abstract</summary>
The Spiking Neural Network (SNN) has attracted more and more attention recently. It adopts binary spike signals to transmit information. Benefitting from the information passing paradigm of SNNs, the multiplications of activations and weights can be replaced by additions, which are more energy-efficient. However, its "Hard Reset" mechanism for the firing activity would ignore the difference among membrane potentials when the membrane potential is above the firing threshold, causing information loss. Meanwhile, quantifying the membrane potential to 0/1 spikes at the firing instants will inevitably introduce the quantization error thus bringing about information loss too. To address these problems, we propose to use the "Soft Reset" mechanism for the supervised training-based SNNs, which will drive the membrane potential to a dynamic reset potential according to its magnitude, and Membrane Potential Rectifier (MPR) to reduce the quantization error via redistributing the membrane potential to a range close to the spikes. Results show that the SNNs with the "Soft Reset" mechanism and MPR outperform their vanilla counterparts on both static and dynamic datasets.
</details>
<details>
<summary>摘要</summary>
《刺激神经网络（SNN）》在最近几年内受到了越来越多的关注。它使用二进制刺激信号来传输信息。由于SNN的信息传递模式，活动和权重的乘法可以被替换为加法，这会更加能效。然而，SNN的“硬重置”机制会忽略细胞电位的差异，当细胞电位超过刺激阈值时，这会导致信息损失。同时，将细胞电位量化为0/1刺激在刺激实时会导致量化误差，从而再次导致信息损失。为解决这些问题，我们提议使用“软重置”机制来进行监督式训练的SNN，将细胞电位驱动到一个动态重置电位，根据其大小，并使用细胞电位重定向器（MPR）来减少量化误差。结果表明，使用“软重置”机制和MPR的SNN比其原始版本在静态和动态数据集上表现更佳。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Semantic-Tree-Concept-Whitening-for-Interpretable-Image-Classification"><a href="#Hierarchical-Semantic-Tree-Concept-Whitening-for-Interpretable-Image-Classification" class="headerlink" title="Hierarchical Semantic Tree Concept Whitening for Interpretable Image Classification"></a>Hierarchical Semantic Tree Concept Whitening for Interpretable Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04343">http://arxiv.org/abs/2307.04343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haixing Dai, Lu Zhang, Lin Zhao, Zihao Wu, Zhengliang Liu, David Liu, Xiaowei Yu, Yanjun Lyu, Changying Li, Ninghao Liu, Tianming Liu, Dajiang Zhu</li>
<li>for: 提高深度神经网络（DNNs）模型解释性</li>
<li>methods: 使用层次结构的semantic概念存储知识，在训练深度模型时进行正则化，使模型表示含义更加精炼</li>
<li>results: 提高模型解释性，提取Semantic概念的分离，不增加模型分类性能的损失<details>
<summary>Abstract</summary>
With the popularity of deep neural networks (DNNs), model interpretability is becoming a critical concern. Many approaches have been developed to tackle the problem through post-hoc analysis, such as explaining how predictions are made or understanding the meaning of neurons in middle layers. Nevertheless, these methods can only discover the patterns or rules that naturally exist in models. In this work, rather than relying on post-hoc schemes, we proactively instill knowledge to alter the representation of human-understandable concepts in hidden layers. Specifically, we use a hierarchical tree of semantic concepts to store the knowledge, which is leveraged to regularize the representations of image data instances while training deep models. The axes of the latent space are aligned with the semantic concepts, where the hierarchical relations between concepts are also preserved. Experiments on real-world image datasets show that our method improves model interpretability, showing better disentanglement of semantic concepts, without negatively affecting model classification performance.
</details>
<details>
<summary>摘要</summary>
随着深度神经网络（DNN）的普及，模型解释性变得越来越重要。许多方法已经被开发来解决这个问题，如解释预测如何进行或者理解中间层神经元的含义。然而，这些方法只能找到模型中自然存在的模式或规则。在这项工作中，我们不再仰仗后期分析方法，而是主动填充了人理解的概念知识，以影响深度模型中的表示。 Specifically，我们使用一个层次结构的semantic概念树来存储知识，并将其用于训练深度模型中的数据实例表示。模型的幂等空间轴与semantic概念之间的层次关系保持一致。实验表明，我们的方法可以提高模型解释性，提取 semantic概念的分离度更高，不会影响模型的分类性能。
</details></li>
</ul>
<hr>
<h2 id="New-Variants-of-Frank-Wolfe-Algorithm-for-Video-Co-localization-Problem"><a href="#New-Variants-of-Frank-Wolfe-Algorithm-for-Video-Co-localization-Problem" class="headerlink" title="New Variants of Frank-Wolfe Algorithm for Video Co-localization Problem"></a>New Variants of Frank-Wolfe Algorithm for Video Co-localization Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04319">http://arxiv.org/abs/2307.04319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamid Nazari</li>
<li>for: 解决图像和视频中同类对象的同时本地化问题</li>
<li>methods: 基于Frank-Wolfe算法和conditional gradient sliding算法（CGS）的新变种</li>
<li>results: 在YouTube-Objects数据集上实现比较高的效率，并与 conditional gradient sliding算法（CGS）进行比较<details>
<summary>Abstract</summary>
The co-localization problem is a model that simultaneously localizes objects of the same class within a series of images or videos. In \cite{joulin2014efficient}, authors present new variants of the Frank-Wolfe algorithm (aka conditional gradient) that increase the efficiency in solving the image and video co-localization problems. The authors show the efficiency of their methods with the rate of decrease in a value called the Wolfe gap in each iteration of the algorithm. In this project, inspired by the conditional gradient sliding algorithm (CGS) \cite{CGS:Lan}, We propose algorithms for solving such problems and demonstrate the efficiency of the proposed algorithms through numerical experiments. The efficiency of these methods with respect to the Wolfe gap is compared with implementing them on the YouTube-Objects dataset for videos.
</details>
<details>
<summary>摘要</summary>
《协同地ocalization问题》是一种同时对同一类图像或视频中的对象进行本地化的模型。在《\cite{joulin2014efficient}》中，作者们提出了新的Frank-Wolfe算法（即条件Gradient）的变种，以提高解决图像和视频协同地ocalization问题的效率。作者们通过每次算法的Wolfe差值下降率来证明方法的效果。在这个项目中，我们Draw inspiration from conditional gradient sliding algorithm（CGS）《\cite{CGS:Lan}》，并提出了解决这些问题的算法。我们通过数值实验证明了我们的方法的效率，并与实现在YouTube-Objects dataset上进行比较。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Multiple-Descriptive-Features-for-Robust-Few-shot-Image-Learning"><a href="#Leveraging-Multiple-Descriptive-Features-for-Robust-Few-shot-Image-Learning" class="headerlink" title="Leveraging Multiple Descriptive Features for Robust Few-shot Image Learning"></a>Leveraging Multiple Descriptive Features for Robust Few-shot Image Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04317">http://arxiv.org/abs/2307.04317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Feng, Anna Bair, J. Zico Kolter</li>
<li>for: 本研究旨在提高图像分类的准确率和robustness，通过自动生成多个视觉描述来代表每个类别。</li>
<li>methods: 研究使用大型语言模型（LLM）生成多个视觉描述，然后使用视图图像模型将描述转化为多个视觉特征，最后使用稀疏逻辑回归选择相关的特征来分类图像。</li>
<li>results: 研究表明，使用这种方法可以在几个shot learning Setting中提高图像分类的准确率和Robustness，并且在训练和测试集之间的混合性能也达到了状态艺术的水平。<details>
<summary>Abstract</summary>
Modern image classification is based upon directly predicting model classes via large discriminative networks, making it difficult to assess the intuitive visual ``features'' that may constitute a classification decision. At the same time, recent works in joint visual language models such as CLIP provide ways to specify natural language descriptions of image classes but typically focus on providing single descriptions for each class. In this work, we demonstrate that an alternative approach, arguably more akin to our understanding of multiple ``visual features'' per class, can also provide compelling performance in the robust few-shot learning setting. In particular, we automatically enumerate multiple visual descriptions of each class -- via a large language model (LLM) -- then use a vision-image model to translate these descriptions to a set of multiple visual features of each image; we finally use sparse logistic regression to select a relevant subset of these features to classify each image. This both provides an ``intuitive'' set of relevant features for each class, and in the few-shot learning setting, outperforms standard approaches such as linear probing. When combined with finetuning, we also show that the method is able to outperform existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance.
</details>
<details>
<summary>摘要</summary>
现代图像分类基于直接预测模型类 via 大量抽象网络，使得分类决策的视觉特征难以评估。同时， recient works in  JOINT 视觉语言模型（CLIP）提供了提供单个描述每个类的方法，但通常将精力集中在单个描述每个类。在这种工作中，我们示出了一种不同的方法，更加符合我们对多个“视觉特征”per 类的认知。具体来说，我们使用大语言模型（LLM）自动生成每个类的多个视觉描述，然后使用视觉图像模型将这些描述翻译成每个图像的多个视觉特征集；最后，我们使用稀疏逻辑回归选择每个图像的相关 subset 的特征进行分类。这种方法不仅提供了每个类的“直观”的视觉特征集，而且在少数shot learning setting中也能够超越标准方法，如线性探测。当与 fine-tuning 结合使用时，我们还示出了该方法可以超越现有的状态之artefinetuning 方法在 both in-distribution 和 out-of-distribution 性能上。
</details></li>
</ul>
<hr>
<h2 id="Robust-Feature-Learning-Against-Noisy-Labels"><a href="#Robust-Feature-Learning-Against-Noisy-Labels" class="headerlink" title="Robust Feature Learning Against Noisy Labels"></a>Robust Feature Learning Against Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04312">http://arxiv.org/abs/2307.04312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tsung-Ming Tai, Yun-Jie Jhang, Wen-Jyi Hwang</li>
<li>for: 这篇论文的目的是提出一种能够有效地适应噪音标签的深度神经网络学习方法。</li>
<li>methods: 这篇论文使用了不supervised augmentation restoration和cluster regularization来学习具有响应力的特征表现，并且引入进步自我发展来减少噪音标签的负面影响。</li>
<li>results: 实验结果显示，这篇论文提出的方法可以高效地提高模型对噪音标签的抗衰强性。<details>
<summary>Abstract</summary>
Supervised learning of deep neural networks heavily relies on large-scale datasets annotated by high-quality labels. In contrast, mislabeled samples can significantly degrade the generalization of models and result in memorizing samples, further learning erroneous associations of data contents to incorrect annotations. To this end, this paper proposes an efficient approach to tackle noisy labels by learning robust feature representation based on unsupervised augmentation restoration and cluster regularization. In addition, progressive self-bootstrapping is introduced to minimize the negative impact of supervision from noisy labels. Our proposed design is generic and flexible in applying to existing classification architectures with minimal overheads. Experimental results show that our proposed method can efficiently and effectively enhance model robustness under severely noisy labels.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对深度神经网络的超级vised学习 heavily relies on large-scale datasets annotated with high-quality labels. However, mislabeled samples can significantly degrade the generalization of models and result in memorizing samples, leading to learning erroneous associations of data contents to incorrect annotations. To address this issue, this paper proposes an efficient approach to tackle noisy labels by learning robust feature representation based on unsupervised augmentation restoration and cluster regularization. Additionally, progressive self-bootstrapping is introduced to minimize the negative impact of supervision from noisy labels. Our proposed design is generic and flexible, and can be applied to existing classification architectures with minimal overheads. Experimental results show that our proposed method can efficiently and effectively enhance model robustness under severely noisy labels.Translation notes:* "supervised learning" is translated as "超级vised学习" (with "超级" meaning "super" and "vised" meaning "learning")* "deep neural networks" is translated as "深度神经网络"* "large-scale datasets" is translated as "大规模 datasets"* "high-quality labels" is translated as "高质量 labels"* "mislabeled samples" is translated as "错abeled samples"* "noisy labels" is translated as "噪音 labels"* "robust feature representation" is translated as "鲁棒特征表示"* "unsupervised augmentation restoration" is translated as "无监督增强 Restoration"* "cluster regularization" is translated as "群集 regularization"* "progressive self-bootstrapping" is translated as "进行式自我启动"* "minimal overheads" is translated as "最小的开销"Please note that the translation is done in Simplified Chinese, which is the most widely used standard for Chinese writing. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="K-Space-Aware-Cross-Modality-Score-for-Synthesized-Neuroimage-Quality-Assessment"><a href="#K-Space-Aware-Cross-Modality-Score-for-Synthesized-Neuroimage-Quality-Assessment" class="headerlink" title="K-Space-Aware Cross-Modality Score for Synthesized Neuroimage Quality Assessment"></a>K-Space-Aware Cross-Modality Score for Synthesized Neuroimage Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04296">http://arxiv.org/abs/2307.04296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinbao Wang, Guoyang Xie, Yawen Huang, Jiayi Lyu, Feng Zheng, Yefeng Zheng, Yaochu Jin</li>
<li>for: 评估多Modalität医学影像合成的问题尚未得到充分研究。现有的度量方法，如PSNR和SSIM，主要关注结构特征，忽略了医学影像中重要的病变位置和基本k空间特点。</li>
<li>methods: 我们提出了一个新的度量方法K-CROSS，以促进这个问题的解决。K-CROSS使用预训练的多Modalität分割网络预测病变位置，并使用肿瘤编码器表示特征，如текстура细节和亮度强度。为了更好地反映医学影像原理中的频率特征，我们获得了k空间特征和视觉特征，并将其集成到了我们的全面编码器中。</li>
<li>results: 我们的方法在NIRPS数据集中表现出色，特别是与放射学专业人员相比。我们的实验结果表明，K-CROSS度量方法可以更好地评估多Modalität医学影像合成的质量。<details>
<summary>Abstract</summary>
The problem of how to assess cross-modality medical image synthesis has been largely unexplored. The most used measures like PSNR and SSIM focus on analyzing the structural features but neglect the crucial lesion location and fundamental k-space speciality of medical images. To overcome this problem, we propose a new metric K-CROSS to spur progress on this challenging problem. Specifically, K-CROSS uses a pre-trained multi-modality segmentation network to predict the lesion location, together with a tumor encoder for representing features, such as texture details and brightness intensities. To further reflect the frequency-specific information from the magnetic resonance imaging principles, both k-space features and vision features are obtained and employed in our comprehensive encoders with a frequency reconstruction penalty. The structure-shared encoders are designed and constrained with a similarity loss to capture the intrinsic common structural information for both modalities. As a consequence, the features learned from lesion regions, k-space, and anatomical structures are all captured, which serve as our quality evaluators. We evaluate the performance by constructing a large-scale cross-modality neuroimaging perceptual similarity (NIRPS) dataset with 6,000 radiologist judgments. Extensive experiments demonstrate that the proposed method outperforms other metrics, especially in comparison with the radiologists on NIRPS.
</details>
<details>
<summary>摘要</summary>
医学多Modal图像合成评估问题尚未得到广泛的研究。现有的度量方法，如PSNR和SSIM，主要关注医学图像的结构特征，而忽略了病变位置和基本扫描空间特点。为解决这问题，我们提出了一种新的度量方法，即K-CROSS。具体来说，K-CROSS使用预训练的多Modal Segmentation网络预测病变位置，同时使用肿瘤编码器来表示特征，如文字特征和亮度强度。为了更好地反映医学图像的频率特征，我们获得了k空间特征和视觉特征，并将其与共同Encoder结合，并使用频率重建罚。结构共享Encoder受到相似损失的限制，以捕捉两种模式之间的共同结构信息。因此，我们可以从病变区域、k空间和解剖结构中获得特征，这些特征作为我们评估质量的依据。我们通过构建6000个医生评估的大规模跨Modal神经成像相似度（NIRPS）数据集来评估性能。广泛的实验表明，我们提出的方法在比较其他度量方法时，特别是与医生的NIRPS表现出色。
</details></li>
</ul>
<hr>
<h2 id="Convex-Decomposition-of-Indoor-Scenes"><a href="#Convex-Decomposition-of-Indoor-Scenes" class="headerlink" title="Convex Decomposition of Indoor Scenes"></a>Convex Decomposition of Indoor Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04246">http://arxiv.org/abs/2307.04246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaibhav Vavilala, David Forsyth</li>
<li>for: 这个论文的目的是解决复杂、杂乱的室内场景 parsing 问题，以获得简洁的场景结构抽象。</li>
<li>methods: 该方法使用学习回归过程将RGBD输入解决为固定数量的简单 convex  primitives，可选择接受 segmentation 以提高分解。然后，使用 descendant 方法进行调整和优化，以 producenextremely good fit。</li>
<li>results: 论文的结果表明，使用该方法可以达到与单个图像预测深度的误差相似的精度。<details>
<summary>Abstract</summary>
We describe a method to parse a complex, cluttered indoor scene into primitives which offer a parsimonious abstraction of scene structure. Our primitives are simple convexes. Our method uses a learned regression procedure to parse a scene into a fixed number of convexes from RGBD input, and can optionally accept segmentations to improve the decomposition. The result is then polished with a descent method which adjusts the convexes to produce a very good fit, and greedily removes superfluous primitives. Because the entire scene is parsed, we can evaluate using traditional depth, normal, and segmentation error metrics. Our evaluation procedure demonstrates that the error from our primitive representation is comparable to that of predicting depth from a single image.
</details>
<details>
<summary>摘要</summary>
我们描述了一种方法，用于将复杂敷地室内场景分解成基本元素，这些基本元素提供了场景结构的简洁抽象。我们的基本元素是简单的凹陷体。我们的方法使用学习回归过程将RGBD输入 parsed into a fixed number of convexes，并可选地接受分割来改进分解。结果 subsequentially polished with a descent method，以达到非常好的适应。因为整个场景都被分解，我们可以使用传统的深度、 норма和分割错误度量来评估。我们的评估过程表明，我们的基本表示错误与从单个图像预测深度的错误相当。
</details></li>
</ul>
<hr>
<h2 id="Mx2M-Masked-Cross-Modality-Modeling-in-Domain-Adaptation-for-3D-Semantic-Segmentation"><a href="#Mx2M-Masked-Cross-Modality-Modeling-in-Domain-Adaptation-for-3D-Semantic-Segmentation" class="headerlink" title="Mx2M: Masked Cross-Modality Modeling in Domain Adaptation for 3D Semantic Segmentation"></a>Mx2M: Masked Cross-Modality Modeling in Domain Adaptation for 3D Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04231">http://arxiv.org/abs/2307.04231</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zbx0810/Mx2M">https://github.com/zbx0810/Mx2M</a></li>
<li>paper_authors: Boxiang Zhang, Zunran Wang, Yonggen Ling, Yuanyuan Guan, Shenghao Zhang, Wenhui Li</li>
<li>for: 该研究旨在解决cross-modal domain adaptation for 3D semantic segmentation task中的缺乏监督问题，提高结果的精度和可靠性。</li>
<li>methods: 该方法基于masked modeling和cross-modal removal和预测（xMRP）技术，以及一种新的cross-modal feature matching方法——动态cross-modal filter（DxMF）。</li>
<li>results: 对于三个DA场景（Day&#x2F;Night、USA&#x2F;Singapore和A2D2&#x2F;SemanticKITTI）的评估，该方法比前一代方法在许多指标上带来了大幅提升。<details>
<summary>Abstract</summary>
Existing methods of cross-modal domain adaptation for 3D semantic segmentation predict results only via 2D-3D complementarity that is obtained by cross-modal feature matching. However, as lacking supervision in the target domain, the complementarity is not always reliable. The results are not ideal when the domain gap is large. To solve the problem of lacking supervision, we introduce masked modeling into this task and propose a method Mx2M, which utilizes masked cross-modality modeling to reduce the large domain gap. Our Mx2M contains two components. One is the core solution, cross-modal removal and prediction (xMRP), which makes the Mx2M adapt to various scenarios and provides cross-modal self-supervision. The other is a new way of cross-modal feature matching, the dynamic cross-modal filter (DxMF) that ensures the whole method dynamically uses more suitable 2D-3D complementarity. Evaluation of the Mx2M on three DA scenarios, including Day/Night, USA/Singapore, and A2D2/SemanticKITTI, brings large improvements over previous methods on many metrics.
</details>
<details>
<summary>摘要</summary>
现有的跨Modal域适应方法 для 3D semantic segmentation 仅通过2D-3D complementarity来预测结果，而这些 complementarity 通常由跨Modal feature matching 获得。然而，由于缺乏目标域的监督，这些 complementarity 不一定可靠。在差异大的域内，结果并不理想。为解决缺乏监督的问题，我们引入 masked modeling 到这个任务中，并提出了一种方法 Mx2M，该方法利用 masked cross-modality modeling 来减少大域差。Mx2M 包含两个组件。一个是核心解决方案，cross-modal removal and prediction (xMRP)，该方法使得 Mx2M 适应不同的场景，并提供 cross-modal self-supervision。另一个是一种新的跨Modal feature matching 方法，dynamic cross-modal filter (DxMF)，该方法确保整个方法在运行时能够使用更适合的 2D-3D complementarity。对 Mx2M 在三个 DA 场景中进行评估，包括 Day/Night、USA/Singapore 和 A2D2/SemanticKITTI，实现了大幅提高过去方法的多个纪录。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/10/cs.CV_2023_07_10/" data-id="clon21ip300e6r5883k7uentu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/10/cs.AI_2023_07_10/" class="article-date">
  <time datetime="2023-07-10T12:00:00.000Z" itemprop="datePublished">2023-07-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/10/cs.AI_2023_07_10/">cs.AI - 2023-07-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Learning-for-Video-Classification-with-Frame-Level-Queries"><a href="#Active-Learning-for-Video-Classification-with-Frame-Level-Queries" class="headerlink" title="Active Learning for Video Classification with Frame Level Queries"></a>Active Learning for Video Classification with Frame Level Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05587">http://arxiv.org/abs/2307.05587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debanjan Goswami, Shayok Chakraborty</li>
<li>for: 这篇论文的目的是提出一个新的活跃学习框架，以对影片分类 зада目中减少人类标注努力。</li>
<li>methods: 我们的框架使用了不确定性和多样性的标准来选择有用的影片和具有代表性的几帧帧帧，以便只需要人类标注几帧帧帧，而不是标注整个影片。</li>
<li>results: 我们的框架可以对影片分类任务中减少人类标注努力，并且可以获得较高的准确率。<details>
<summary>Abstract</summary>
Deep learning algorithms have pushed the boundaries of computer vision research and have depicted commendable performance in a variety of applications. However, training a robust deep neural network necessitates a large amount of labeled training data, acquiring which involves significant time and human effort. This problem is even more serious for an application like video classification, where a human annotator has to watch an entire video end-to-end to furnish a label. Active learning algorithms automatically identify the most informative samples from large amounts of unlabeled data; this tremendously reduces the human annotation effort in inducing a machine learning model, as only the few samples that are identified by the algorithm, need to be labeled manually. In this paper, we propose a novel active learning framework for video classification, with the goal of further reducing the labeling onus on the human annotators. Our framework identifies a batch of exemplar videos, together with a set of informative frames for each video; the human annotator needs to merely review the frames and provide a label for each video. This involves much less manual work than watching the complete video to come up with a label. We formulate a criterion based on uncertainty and diversity to identify the informative videos and exploit representative sampling techniques to extract a set of exemplar frames from each video. To the best of our knowledge, this is the first research effort to develop an active learning framework for video classification, where the annotators need to inspect only a few frames to produce a label, rather than watching the end-to-end video.
</details>
<details>
<summary>摘要</summary>
深度学习算法在计算机视觉领域的研究中提出了新的挑战，并表现出了卓越的表现。然而，训练一个强大的深度神经网络需要大量的标注训练数据，获取这些数据需要很多时间和人工劳动。这个问题对于视频分类应用来说更加严重，因为人类标注者需要从头到尾播放整个视频，以为每个视频提供标签。活动学习算法可以自动从大量的未标注数据中选择最有用的样本，这将减少人工标注劳动的强度，只需要人类标注者为每个视频提供标签。在这篇论文中，我们提出了一个新的活动学习框架 для视频分类，目的是进一步减少人类标注劳动。我们的框架可以同时选择一批表型视频和每个视频的一些有用帧，人类标注者只需要审查这些帧并为每个视频提供标签。这需要的人工劳动比较少，只需要看到每个视频的一些帧，而不是从头到尾播放整个视频。我们基于不确定性和多样性的准则来选择有用的视频和帧，并使用代表性抽样技术来从每个视频中提取有用的帧。根据我们所知，这是第一篇对视频分类的活动学习框架进行研究，人类标注者只需要审查一些帧来生成标签，而不是从头到尾播放整个视频。
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-positional-contrastive-learning-application-to-cirrhosis-classification"><a href="#Weakly-supervised-positional-contrastive-learning-application-to-cirrhosis-classification" class="headerlink" title="Weakly-supervised positional contrastive learning: application to cirrhosis classification"></a>Weakly-supervised positional contrastive learning: application to cirrhosis classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04617">http://arxiv.org/abs/2307.04617</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guerbet-ai/wsp-contrastive">https://github.com/guerbet-ai/wsp-contrastive</a></li>
<li>paper_authors: Emma Sarfati, Alexandre Bône, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch</li>
<li>for: 这个研究是为了提高医疗影像分类的精度和效率，使用低信度标签和弱标签来驱动模型的训练。</li>
<li>methods: 这个研究使用了一种弱相关学习策略，即将医疗影像的空间上下文和低信度标签 integrate 到一个通用的贝叶斯损失函数中，以提高模型的分类精度。</li>
<li>results: 研究结果表明，使用提议的方法可以提高医疗影像分类的分类AUC by 5%（相比基eline模型）和26%（相比公共的LIHC数据集）。<details>
<summary>Abstract</summary>
Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations, and small strongly-labeled (i.e., high-confidence) datasets. The proposed model improves the classification AUC by 5% with respect to a baseline model on our internal dataset, and by 26% on the public LIHC dataset from the Cancer Genome Atlas. The code is available at: https://github.com/Guerbet-AI/wsp-contrastive.
</details>
<details>
<summary>摘要</summary>
大量医疗成像数据可以便宜地和快速地被低信任度、软标签（例如放射学分数）注释。高信任度标签，如 histology-based 诊断，是罕见和昂贵的。无监督策略，如对比学习（CL）方法，可以利用无标签或软标签数据。这些方法通常需要大批量大小，但是在Full Resolution 3D图像的情况下，由于 GPU 内存限制，这可能会增加困难。然而，Volume 的位置信息可以在某些医学应用中是非常重要的。在这种情况下，我们提出了一种高效的软监督位置（WSP）对比学习策略，其中我们将通过一种通用的核心函数基于损失函数来整合每个2Dslice的空间上下文和软标签。我们在 cirrhosis 预测中使用了一个大量的软标签图像，即放射学低信任签名，以及一个小的强 confidence（高信任度）数据集。我们的模型可以与基eline模型相比提高分类AUC值5%，并在公共的LIHC数据集上提高26%。代码可以在：https://github.com/Guerbet-AI/wsp-contrastive 中找到。
</details></li>
</ul>
<hr>
<h2 id="Code-Generation-for-Machine-Learning-using-Model-Driven-Engineering-and-SysML"><a href="#Code-Generation-for-Machine-Learning-using-Model-Driven-Engineering-and-SysML" class="headerlink" title="Code Generation for Machine Learning using Model-Driven Engineering and SysML"></a>Code Generation for Machine Learning using Model-Driven Engineering and SysML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05584">http://arxiv.org/abs/2307.05584</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sraedler/MDE_for_ML_Generation">https://github.com/sraedler/MDE_for_ML_Generation</a></li>
<li>paper_authors: Simon Raedler, Matthias Rupp, Eugen Rigger, Stefanie Rinderle-Ma</li>
<li>for: 这个论文的目的是提高工程系统的数据驱动工程实践。</li>
<li>methods: 这个论文使用机器学习模型转换来生成可执行代码，以便在实践中更好地实现数据驱动工程。</li>
<li>results: 研究表明，这种方法可以提高模型转换的可 modify性和维护性，并且可以减少实现的努力。此外，这种方法还可以提供一个理论基础，用于标准化数据驱动工程实践。<details>
<summary>Abstract</summary>
Data-driven engineering refers to systematic data collection and processing using machine learning to improve engineering systems. Currently, the implementation of data-driven engineering relies on fundamental data science and software engineering skills. At the same time, model-based engineering is gaining relevance for the engineering of complex systems. In previous work, a model-based engineering approach integrating the formalization of machine learning tasks using the general-purpose modeling language SysML is presented. However, formalized machine learning tasks still require the implementation in a specialized programming languages like Python. Therefore, this work aims to facilitate the implementation of data-driven engineering in practice by extending the previous work of formalizing machine learning tasks by integrating model transformation to generate executable code. The method focuses on the modifiability and maintainability of the model transformation so that extensions and changes to the code generation can be integrated without requiring modifications to the code generator. The presented method is evaluated for feasibility in a case study to predict weather forecasts. Based thereon, quality attributes of model transformations are assessed and discussed. Results demonstrate the flexibility and the simplicity of the method reducing efforts for implementation. Further, the work builds a theoretical basis for standardizing data-driven engineering implementation in practice.
</details>
<details>
<summary>摘要</summary>
数据驱动工程指的是通过机器学习系统化收集和处理数据来提高工程系统。现在，数据驱动工程的实施仍然需要基础数据科学和软件工程技能。同时，模型基于工程在复杂系统工程中得到了应用。在前一项工作中，一种基于SysML通用模型语言的模型基于工程方法 integrates formalized机器学习任务的表现。然而，正式机器学习任务仍然需要通过专门的编程语言如Python进行实现。因此，本工作的目标是通过扩展前一项工作中的形式化机器学习任务，并将模型转换 integrate into executable code来促进数据驱动工程在实践中的实现。该方法强调模型转换的可 modify和维护性，以便在扩展和修改代码生成器时不需要修改代码。在一个案例研究中，通过预测天气预报来评估和讨论模型转换的质量特征。结果表明该方法的灵活性和简洁性，可以降低实现的努力。此外，该工作为数据驱动工程实践中的标准化提供了理论基础。
</details></li>
</ul>
<hr>
<h2 id="MiVOLO-Multi-input-Transformer-for-Age-and-Gender-Estimation"><a href="#MiVOLO-Multi-input-Transformer-for-Age-and-Gender-Estimation" class="headerlink" title="MiVOLO: Multi-input Transformer for Age and Gender Estimation"></a>MiVOLO: Multi-input Transformer for Age and Gender Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04616">http://arxiv.org/abs/2307.04616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wildchlamydia/mivolo">https://github.com/wildchlamydia/mivolo</a></li>
<li>paper_authors: Maksim Kuprashevich, Irina Tolstykh<br>for: 这篇论文的目的是提出一种基于视觉变换器的年龄和性别识别方法，以解决在野外的年龄和性别识别任务中存在的挑战。methods: 该方法使用最新的视觉变换器，并将年龄和性别识别任务集成到一个共同的双输入&#x2F;双出口模型中，利用了人脸信息以及人像数据。results: 经过实验表明，该模型在四个流行的benchmark上 achieve state-of-the-art表现，并且能够实现实时处理。此外，我们还介绍了一个基于Open Images Dataset的新的benchmark，其 annotations是由人工注解员 manually generated，因此得到了高准确率的答案。最后，我们公开了我们的模型，以及验证和推理代码。<details>
<summary>Abstract</summary>
Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded. We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer. Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data. This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image. To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities. Additionally, we introduce a novel benchmark based on images from the Open Images Dataset. The ground truth annotations for this benchmark have been meticulously generated by human annotators, resulting in high accuracy answers due to the smart aggregation of votes. Furthermore, we compare our model's age recognition performance with human-level accuracy and demonstrate that it significantly outperforms humans across a majority of age ranges. Finally, we grant public access to our models, along with the code for validation and inference. In addition, we provide extra annotations for used datasets and introduce our new benchmark.
</details>
<details>
<summary>摘要</summary>
ages and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded. We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer. Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data. This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image. To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities. Additionally, we introduce a novel benchmark based on images from the Open Images Dataset. The ground truth annotations for this benchmark have been meticulously generated by human annotators, resulting in high accuracy answers due to the smart aggregation of votes. Furthermore, we compare our model's age recognition performance with human-level accuracy and demonstrate that it significantly outperforms humans across a majority of age ranges. Finally, we grant public access to our models, along with the code for validation and inference. In addition, we provide extra annotations for used datasets and introduce our new benchmark.
</details></li>
</ul>
<hr>
<h2 id="Learning-Interpretable-Heuristics-for-WalkSAT"><a href="#Learning-Interpretable-Heuristics-for-WalkSAT" class="headerlink" title="Learning Interpretable Heuristics for WalkSAT"></a>Learning Interpretable Heuristics for WalkSAT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04608">http://arxiv.org/abs/2307.04608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannet Interian, Sara Bernardini</li>
<li>for: 这 paper 是解决大型、困难的满足问题 (SAT) 的地方搜索算法的研究。</li>
<li>methods: 这 paper 使用了 reinforcement learning 来学习变量评价函数和噪音参数的优化策略。</li>
<li>results: 实验结果表明，与 WalkSAT 基准和另一种本地搜索学习的策略相比，这种方法能够获得更好的表现。<details>
<summary>Abstract</summary>
Local search algorithms are well-known methods for solving large, hard instances of the satisfiability problem (SAT). The performance of these algorithms crucially depends on heuristics for setting noise parameters and scoring variables. The optimal setting for these heuristics varies for different instance distributions. In this paper, we present an approach for learning effective variable scoring functions and noise parameters by using reinforcement learning. We consider satisfiability problems from different instance distributions and learn specialized heuristics for each of them. Our experimental results show improvements with respect to both a WalkSAT baseline and another local search learned heuristic.
</details>
<details>
<summary>摘要</summary>
<<SYS>>本文提出了一种使用再征学习来学习有效的变量评分函数和噪声参数，以解决大规模、困难的满足问题（SAT）。这些算法的性能取决于设置噪声参数和评分变量的规则。不同的实例分布下，optimal setting for these heuristics 会有所不同。本文 presente una aproximación para aprender heurísticas especializadas para cada distribución de instancias. Our experimental results show improvements over a WalkSAT baseline and another local search learned heuristic.Note: "WalkSAT" is a local search algorithm for solving SAT problems.
</details></li>
</ul>
<hr>
<h2 id="A-Memristor-Inspired-Computation-for-Epileptiform-Signals-in-Spheroids"><a href="#A-Memristor-Inspired-Computation-for-Epileptiform-Signals-in-Spheroids" class="headerlink" title="A Memristor-Inspired Computation for Epileptiform Signals in Spheroids"></a>A Memristor-Inspired Computation for Epileptiform Signals in Spheroids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04607">http://arxiv.org/abs/2307.04607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iván Díez de los Ríos, John Wesley Ephraim, Gemma Palazzolo, Teresa Serrano-Gotarredona, Gabriella Panuccio, Bernabé Linares-Barranco</li>
<li>for: 这种研究是为了开发一种基于折射器的计算方法，用于实时计算和较低的计算成本下的癫病活动特征或指纹。</li>
<li>methods: 这种计算方法基于折射器的原理，使用微电极阵系统记录的癫病活动数据进行计算。</li>
<li>results: 这种计算方法可以在实时中计算并生成癫病事件开始的警示信号，并且可以减少计算成本。<details>
<summary>Abstract</summary>
In this paper we present a memristor-inspired computational method for obtaining a type of running spectrogram or fingerprint of epileptiform activity generated by rodent hippocampal spheroids. It can be used to compute on the fly and with low computational cost an alert-level signal for epileptiform events onset. Here, we describe the computational method behind this fingerprint technique and illustrate it using epileptiform events recorded from hippocampal spheroids using a microelectrode array system.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种基于抗晶阻器的计算方法，用于获得来自小鼠脊梁块的 épileptiform 活动的类型运动 спектрограм或指纹。这种方法可以在实时计算和低计算成本下生成 épileptiform 事件开始的警示信号。我们将在这篇论文中介绍这种指纹技术的计算方法，并使用来自脊梁块的 épileptiform 事件记录系统来说明。
</details></li>
</ul>
<hr>
<h2 id="DBFed-Debiasing-Federated-Learning-Framework-based-on-Domain-Independent"><a href="#DBFed-Debiasing-Federated-Learning-Framework-based-on-Domain-Independent" class="headerlink" title="DBFed: Debiasing Federated Learning Framework based on Domain-Independent"></a>DBFed: Debiasing Federated Learning Framework based on Domain-Independent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05582">http://arxiv.org/abs/2307.05582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiale Li, Zhixin Li, Yibo Wang, Yao Li, Lei Wang</li>
<li>for: 这篇论文主要是关于如何使用 federated learning 技术保护数据隐私和解决数据岛屿问题，同时mitigate model bias caused by data differences in quality.</li>
<li>methods: 该论文提出了一种基于domain-independent的 debiasing federated learning 框架， named DBFed，可以在客户端上训练带有敏感特征的模型，并且可以Explicitlyencode sensitive attributes to mitigate model bias.</li>
<li>results: 该论文在三个实际数据集上进行了实验，并使用了五个评价指标来衡量模型的准确率和公平性。结果显示，DBFed 的评价指标比三种比较方法更高，充分表明 DBFed 可以减少模型偏见问题。<details>
<summary>Abstract</summary>
As digital transformation continues, enterprises are generating, managing, and storing vast amounts of data, while artificial intelligence technology is rapidly advancing. However, it brings challenges in information security and data security. Data security refers to the protection of digital information from unauthorized access, damage, theft, etc. throughout its entire life cycle. With the promulgation and implementation of data security laws and the emphasis on data security and data privacy by organizations and users, Privacy-preserving technology represented by federated learning has a wide range of application scenarios. Federated learning is a distributed machine learning computing framework that allows multiple subjects to train joint models without sharing data to protect data privacy and solve the problem of data islands. However, the data among multiple subjects are independent of each other, and the data differences in quality may cause fairness issues in federated learning modeling, such as data bias among multiple subjects, resulting in biased and discriminatory models. Therefore, we propose DBFed, a debiasing federated learning framework based on domain-independent, which mitigates model bias by explicitly encoding sensitive attributes during client-side training. This paper conducts experiments on three real datasets and uses five evaluation metrics of accuracy and fairness to quantify the effect of the model. Most metrics of DBFed exceed those of the other three comparative methods, fully demonstrating the debiasing effect of DBFed.
</details>
<details>
<summary>摘要</summary>
如 digital transformation 继续发展，企业在生产、管理和存储庞大数据的同时，人工智能技术也在快速发展。然而，这些技术带来了信息安全和数据安全的挑战。数据安全指的是保护数字信息不被未经授权的访问、损害、盗窃等等的整个生命周期。随着数据安全法规的推出和组织和用户对数据安全和隐私的强调，隐私保护技术如联合学习得到了广泛的应用场景。联合学习是一种分布式机器学习计算框架，允许多个主体共同训练无需分享数据，以保护数据隐私和解决数据孤岛问题。然而，多个主体的数据归并独立，数据质量差异可能导致公平性问题，如多主体数据的偏见问题，导致模型偏见和歧视。因此，我们提出了DBFed，一种基于领域独立的偏见降低联合学习框架，通过客户端训练中显式编码敏感特征来mitigate模型偏见。这篇论文在三个实际数据集上进行了实验，并使用了五个评价指标来衡量模型的效果。大多数DBFed的评价指标超过了其他三种比较方法的评价指标，完全展示了DBFed的偏见降低效果。
</details></li>
</ul>
<hr>
<h2 id="Model-Driven-Engineering-for-Artificial-Intelligence-–-A-Systematic-Literature-Review"><a href="#Model-Driven-Engineering-for-Artificial-Intelligence-–-A-Systematic-Literature-Review" class="headerlink" title="Model-Driven Engineering for Artificial Intelligence – A Systematic Literature Review"></a>Model-Driven Engineering for Artificial Intelligence – A Systematic Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04599">http://arxiv.org/abs/2307.04599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sraedler/model-driven-engineering4artificial-intelligence">https://github.com/sraedler/model-driven-engineering4artificial-intelligence</a></li>
<li>paper_authors: Simon Raedler, Luca Berardinelli, Karolin Winter, Abbas Rahimi, Stefanie Rinderle-Ma</li>
<li>for: 本研究目的是为了探讨Model-Driven Engineering（MDE）在人工智能（AI）领域的现有知识体系，以锻炼未来的研究方向和定义当前领域的惯性。</li>
<li>methods: 本研究采用了系统性Literature Review（SLR）方法，从五个主要数据库中收集了703个候选研究，最终选择了15个主要研究。每个主要研究都会被评估和讨论，以对MDE原则和实践的采用情况进行分析，同时也会对CRISP-DM方法ологиraph中的不同阶段进行对应。</li>
<li>results: 研究发现，使用MDE在AI领域的应用还处于早期阶段，没有一个广泛使用的工具或方法。现有的方法主要集中在特定的开发阶段，而不是涵盖整个开发过程。此外，训练和定制AI算法是主要的AI相关问题，而数据集准备则受到较少的注意。早期项目阶段，如CRISP-DM《企业理解》阶段，很少被反映。<details>
<summary>Abstract</summary>
Objective: This study aims to investigate the existing body of knowledge in the field of Model-Driven Engineering MDE in support of AI (MDE4AI) to sharpen future research further and define the current state of the art.   Method: We conducted a Systemic Literature Review (SLR), collecting papers from five major databases resulting in 703 candidate studies, eventually retaining 15 primary studies. Each primary study will be evaluated and discussed with respect to the adoption of (1) MDE principles and practices and (2) the phases of AI development support aligned with the stages of the CRISP-DM methodology.   Results: The study's findings show that the pillar concepts of MDE (metamodel, concrete syntax and model transformation), are leveraged to define domain-specific languages (DSL) explicitly addressing AI concerns. Different MDE technologies are used, leveraging different language workbenches. The most prominent AI-related concerns are training and modeling of the AI algorithm, while minor emphasis is given to the time-consuming preparation of the data sets. Early project phases that support interdisciplinary communication of requirements, such as the CRISP-DM \textit{Business Understanding} phase, are rarely reflected.   Conclusion: The study found that the use of MDE for AI is still in its early stages, and there is no single tool or method that is widely used. Additionally, current approaches tend to focus on specific stages of development rather than providing support for the entire development process. As a result, the study suggests several research directions to further improve the use of MDE for AI and to guide future research in this area.
</details>
<details>
<summary>摘要</summary>
目的：本研究旨在调查MDE（Model-Driven Engineering）在支持人工智能（AI）领域的现有知识体系，以锐化未来研究和定义当前领域的惯例。方法：我们进行了系统性文献综述（SLR），从五个主要数据库收集了703份候选学术论文，最终选择了15个首论。每个首论都将被评估和讨论，对MDE原则和实践的采纳以及CRISP-DM方法ологии中的不同阶段进行分析。结果：研究发现，MDE柱论（模型模型、具体语法和模型转换）在定义适用于人工智能问题的域特定语言（DSL）方面发挥了重要作用。不同的MDE技术被使用，利用不同的语言工具箱。AI algoritirthmic的训练和模型化收到了主要的注意，而数据集的准备则很少被考虑。CRISP-DM方法ологии的营运理解阶段（Business Understanding） rare reflection。结论：研究发现，MDE在人工智能领域的使用仍在早期阶段，没有一个广泛使用的工具或方法。当前的approaches倾向于特定的开发阶段，而不是为整个开发过程提供支持。因此，研究建议了 severaltudesearch directions，以进一步改进MDE在人工智能领域的使用，并引导未来的研究。
</details></li>
</ul>
<hr>
<h2 id="A-Semi-Automated-Solution-Approach-Selection-Tool-for-Any-Use-Case-via-Scopus-and-OpenAI-a-Case-Study-for-AI-ML-in-Oncology"><a href="#A-Semi-Automated-Solution-Approach-Selection-Tool-for-Any-Use-Case-via-Scopus-and-OpenAI-a-Case-Study-for-AI-ML-in-Oncology" class="headerlink" title="A Semi-Automated Solution Approach Selection Tool for Any Use Case via Scopus and OpenAI: a Case Study for AI&#x2F;ML in Oncology"></a>A Semi-Automated Solution Approach Selection Tool for Any Use Case via Scopus and OpenAI: a Case Study for AI&#x2F;ML in Oncology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04573">http://arxiv.org/abs/2307.04573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deniz Kenan Kılıç, Alex Elkjær Vasegaard, Aurélien Desoeuvres, Peter Nielsen</li>
<li>for: 本研究提出了一个半自动化的工具，用于方案方法评估和选择，并且适用于研究者、实践者以及决策者。同时，这个工具可以作为未来研究的benchmark。</li>
<li>methods: 本研究使用了三个模组：（1）选择和分数计算 papers，使用关键词选择方案查询Scopus API， compute relevancy;（2）从 papers中提取方案方法，使用OpenAI API;（3）敏感分析和后analyzes。</li>
<li>results: 本研究显示出了方法方案的趋势、相关文献和方法。在肿瘤领域的实例和多个使用案中，这个工具获得了Promising的结果，与手动ground truth比较。<details>
<summary>Abstract</summary>
In today's vast literature landscape, a manual review is very time-consuming. To address this challenge, this paper proposes a semi-automated tool for solution method review and selection. It caters to researchers, practitioners, and decision-makers while serving as a benchmark for future work. The tool comprises three modules: (1) paper selection and scoring, using a keyword selection scheme to query Scopus API and compute relevancy; (2) solution method extraction in papers utilizing OpenAI API; (3) sensitivity analysis and post-analyzes. It reveals trends, relevant papers, and methods. AI in the oncology case study and several use cases are presented with promising results, comparing the tool to manual ground truth.
</details>
<details>
<summary>摘要</summary>
今天的文献景观中，手动审查非常占时。为解决这个挑战，这篇论文提出了一种半自动化工具，用于解决方法审查和选择。它适用于研究人员、实践者和决策者，同时作为未来工作的标准。工具包括三个模块：（1）文献选择和分数计算，使用关键词选择方案来查询Scopus API计算相关性；（2）解决方法提取在论文中使用OpenAI API；（3）敏感分析和后期分析。它显示出了趋势、相关论文和方法。在肿瘤学案例和多个实践案例中，这种工具取得了有前提的结果，与手动真实比较。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Surgery-for-One-shot-Unlearning-on-Generative-Model"><a href="#Gradient-Surgery-for-One-shot-Unlearning-on-Generative-Model" class="headerlink" title="Gradient Surgery for One-shot Unlearning on Generative Model"></a>Gradient Surgery for One-shot Unlearning on Generative Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04550">http://arxiv.org/abs/2307.04550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seohui Bae, Seoyoon Kim, Hyemin Jung, Woohyung Lim</li>
<li>for: 本研究旨在提出一种简单又有效的方法，用于从深度生成模型中移除特定样本的影响。</li>
<li>methods: 本方法基于多任务学习的想法，通过将梯度投影到梯度的正常平面来规范梯度的影响。</li>
<li>results: 本研究提供了 theoretically analysis，并与现有基eline相比，实现了超出现有基eline的表现。<details>
<summary>Abstract</summary>
Recent regulation on right-to-be-forgotten emerges tons of interest in unlearning pre-trained machine learning models. While approximating a straightforward yet expensive approach of retrain-from-scratch, recent machine unlearning methods unlearn a sample by updating weights to remove its influence on the weight parameters. In this paper, we introduce a simple yet effective approach to remove a data influence on the deep generative model. Inspired by works in multi-task learning, we propose to manipulate gradients to regularize the interplay of influence among samples by projecting gradients onto the normal plane of the gradients to be retained. Our work is agnostic to statistics of the removal samples, outperforming existing baselines while providing theoretical analysis for the first time in unlearning a generative model.
</details>
<details>
<summary>摘要</summary>
近期的“忘记权”规定吸引了大量关注，训练过的机器学习模型的“忘记”问题受到了广泛关注。而现有的机器学习忘记方法通常是将数据重新训练，这种方法可能是复杂且昂贵的。在这篇论文中，我们介绍了一个简单又有效的方法，可以将数据的影响力从深度生成模型中移除。我们参考了多任务学习的研究，我们提出了将梯度调控到梯度的正常平面上，以regularize数据之间的影响相互关系。我们的方法不依赖排除数据的统计特征，而且超越现有的基elines，并提供了理论分析。
</details></li>
</ul>
<hr>
<h2 id="Hate-Speech-Detection-via-Dual-Contrastive-Learning"><a href="#Hate-Speech-Detection-via-Dual-Contrastive-Learning" class="headerlink" title="Hate Speech Detection via Dual Contrastive Learning"></a>Hate Speech Detection via Dual Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05578">http://arxiv.org/abs/2307.05578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Lu, Hongfei Lin, Xiaokun Zhang, Zhaoqing Li, Tongyue Zhang, Linlin Zong, Fenglong Ma, Bo Xu</li>
<li>for: 本研究旨在探讨社交媒体上 hate speech 的快速传播对互联网环境和社会造成的影响，以及探索现代自然语言处理技术中 hate speech 检测的问题。</li>
<li>methods: 本研究提出了一种基于 dual contrastive learning（DCL）的新方法，通过同时优化自动supervised和supervised contrastive learning损失函数，捕捉 span-level 信息，超过传统模型所使用的 token-level 情感 semantics。此外，我们还将 focal loss 集成到 DCL 框架中，以解决数据不均衡问题。</li>
<li>results: 我们在两个公开的英文数据集上进行实验，结果表明，提出的模型在 hate speech 检测任务中具有比例性和精度，并 precisely 地检测了 hate speeches。<details>
<summary>Abstract</summary>
The fast spread of hate speech on social media impacts the Internet environment and our society by increasing prejudice and hurting people. Detecting hate speech has aroused broad attention in the field of natural language processing. Although hate speech detection has been addressed in recent work, this task still faces two inherent unsolved challenges. The first challenge lies in the complex semantic information conveyed in hate speech, particularly the interference of insulting words in hate speech detection. The second challenge is the imbalanced distribution of hate speech and non-hate speech, which may significantly deteriorate the performance of models. To tackle these challenges, we propose a novel dual contrastive learning (DCL) framework for hate speech detection. Our framework jointly optimizes the self-supervised and the supervised contrastive learning loss for capturing span-level information beyond the token-level emotional semantics used in existing models, particularly detecting speech containing abusive and insulting words. Moreover, we integrate the focal loss into the dual contrastive learning framework to alleviate the problem of data imbalance. We conduct experiments on two publicly available English datasets, and experimental results show that the proposed model outperforms the state-of-the-art models and precisely detects hate speeches.
</details>
<details>
<summary>摘要</summary>
“社交媒体上快速传播的仇恨言论影响了网络环境和我们的社会，增加了偏见和伤害人。检测仇恨言论已经受到了学界的广泛关注，但这个任务仍然面临两个困难。第一个挑战是仇恨言论中的复杂 semantics，特别是攻击性评语的干扰。第二个挑战是仇恨言论和非仇恨言论之间的数据分布不均，这可能导致模型的性能下降。为了解决这些挑战，我们提出了一个新的双重对照学习（DCL）框架 для仇恨言论检测。我们的框架同时运行自我超vised和supervised对照学习损失，以捕捉 span-level 信息，超过了现有模型中的单词水平情感学。此外，我们将 focal loss 整合到 dual contrastive learning 框架中，以解决数据不均的问题。我们在两个公开的英文数据集上进行实验，实验结果显示，我们提出的模型比现有的模型高效，精准地检测到了仇恨言论。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Large-Margin-Sparse-Embeddings-for-Open-Set-Medical-Diagnosis"><a href="#Learning-Large-Margin-Sparse-Embeddings-for-Open-Set-Medical-Diagnosis" class="headerlink" title="Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis"></a>Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04541">http://arxiv.org/abs/2307.04541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyuan Liu, Lu Xu, Jicong Zhang</li>
<li>for: 这篇论文目的是解决医学数据集搜集不充分时，算法遇到多种挑战，其中一个重要的挑战是开集识别（OSR）。</li>
<li>methods: 这篇论文提出了两个机制来解决OSR：一个叫做Margin Loss with Adaptive Scale（MLAS），它引入了角度margin来增强内组距离和间组分离，同时还有一个适应因子来强化通用能力；另一个叫做Open-Space Suppression（OSS），它开辟了分类器，让尚未被训练的范围被视为未知。</li>
<li>results: 与现有方法比较，这篇论文的MLAS方法实现了superior表现，测量方式包括ACC、AUROC和OSCR。<details>
<summary>Abstract</summary>
Fueled by deep learning, computer-aided diagnosis achieves huge advances. However, out of controlled lab environments, algorithms could face multiple challenges. Open set recognition (OSR), as an important one, states that categories unseen in training could appear in testing. In medical fields, it could derive from incompletely collected training datasets and the constantly emerging new or rare diseases. OSR requires an algorithm to not only correctly classify known classes, but also recognize unknown classes and forward them to experts for further diagnosis. To tackle OSR, we assume that known classes could densely occupy small parts of the embedding space and the remaining sparse regions could be recognized as unknowns. Following it, we propose Open Margin Cosine Loss (OMCL) unifying two mechanisms. The former, called Margin Loss with Adaptive Scale (MLAS), introduces angular margin for reinforcing intra-class compactness and inter-class separability, together with an adaptive scaling factor to strengthen the generalization capacity. The latter, called Open-Space Suppression (OSS), opens the classifier by recognizing sparse embedding space as unknowns using proposed feature space descriptors. Besides, since medical OSR is still a nascent field, two publicly available benchmark datasets are proposed for comparison. Extensive ablation studies and feature visualization demonstrate the effectiveness of each design. Compared with state-of-the-art methods, MLAS achieves superior performances, measured by ACC, AUROC, and OSCR.
</details>
<details>
<summary>摘要</summary>
We propose Open Margin Cosine Loss (OMCL), which combines two mechanisms:1. Margin Loss with Adaptive Scale (MLAS): This introduces angular margin for improving intra-class compactness and inter-class separability, along with an adaptive scaling factor to enhance generalization capacity.2. Open-Space Suppression (OSS): This opens the classifier by recognizing sparse embedding space as unknowns using proposed feature space descriptors.To evaluate the effectiveness of OMCL, we use two publicly available benchmark datasets for comparison. Extensive ablation studies and feature visualization demonstrate the superior performance of MLAS, as measured by ACC, AUROC, and OSCR. Compared to state-of-the-art methods, MLAS achieves better performance in recognizing unknown classes and forwarding them to experts for further diagnosis.
</details></li>
</ul>
<hr>
<h2 id="Pathway-toward-prior-knowledge-integrated-machine-learning-in-engineering"><a href="#Pathway-toward-prior-knowledge-integrated-machine-learning-in-engineering" class="headerlink" title="Pathway toward prior knowledge-integrated machine learning in engineering"></a>Pathway toward prior knowledge-integrated machine learning in engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06950">http://arxiv.org/abs/2307.06950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xia Chen, Philipp Geyer</li>
<li>for:  This study aims to integrate multidisciplinary domain professions into machine acknowledgeable, data-driven processes.</li>
<li>methods: The study examines information uncertainty sources in knowledge representation and explores knowledge decomposition with a three-tier knowledge-integrated machine learning paradigm.</li>
<li>results: The approach balances holist and reductionist perspectives in the engineering domain.Here is the same information in Simplified Chinese text, as requested:</li>
<li>for: 这种研究旨在将多学科领域专业integrated into机器可知的数据驱动过程中。</li>
<li>methods: 研究检查知识表示中的信息不确定源和知识归纳三层知识结合机器学习模型。</li>
<li>results: 该方法在工程领域实现了平衡整体和分解的观点。<details>
<summary>Abstract</summary>
Despite the digitalization trend and data volume surge, first-principles models (also known as logic-driven, physics-based, rule-based, or knowledge-based models) and data-driven approaches have existed in parallel, mirroring the ongoing AI debate on symbolism versus connectionism. Research for process development to integrate both sides to transfer and utilize domain knowledge in the data-driven process is rare. This study emphasizes efforts and prevailing trends to integrate multidisciplinary domain professions into machine acknowledgeable, data-driven processes in a two-fold organization: examining information uncertainty sources in knowledge representation and exploring knowledge decomposition with a three-tier knowledge-integrated machine learning paradigm. This approach balances holist and reductionist perspectives in the engineering domain.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "first-principles models" 被翻译成 "逻辑驱动、物理驱动、规则驱动或知识驱动模型"* "data-driven approaches" 被翻译成 "数据驱动方法"* "domain knowledge" 被翻译成 "领域知识"* "machine acknowledgeable" 被翻译成 "可识别的机器"* "three-tier knowledge-integrated machine learning paradigm" 被翻译成 "三层知识集成机器学习模型"Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Q-YOLOP-Quantization-aware-You-Only-Look-Once-for-Panoptic-Driving-Perception"><a href="#Q-YOLOP-Quantization-aware-You-Only-Look-Once-for-Panoptic-Driving-Perception" class="headerlink" title="Q-YOLOP: Quantization-aware You Only Look Once for Panoptic Driving Perception"></a>Q-YOLOP: Quantization-aware You Only Look Once for Panoptic Driving Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04537">http://arxiv.org/abs/2307.04537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chi-Chih Chang, Wei-Cheng Lin, Pei-Shuo Wang, Sheng-Feng Yu, Yu-Chen Lu, Kuan-Cheng Lin, Kai-Chiang Wu</li>
<li>for: 本文提出了一种高效、量化化感知模型（Q- YOLOP），用于自动驾驶场景中的对象检测、 drivable 区域分 segmentation 和车道线分 segmentation。</li>
<li>methods: 该模型使用 Efficient Layer Aggregation Network（ELAN）作为底层，并采用任务特定的头来实现每个任务。训练过程包括四个阶段：预训练于 BDD100K 数据集、精度调整于 BDD100K 和 iVS 数据集、量化感知训练（QAT）于 BDD100K 数据集，并使用强大的数据增强技术和多个数据集训练。</li>
<li>results: 提出的模型在对象检测和分 segmentation  tasks 中实现了状态的术语性性能（<a href="mailto:&#x6d;&#65;&#x50;&#x40;&#x30;&#46;&#x35;">&#x6d;&#65;&#x50;&#x40;&#x30;&#46;&#x35;</a> &#x3D; 0.622）和低计算和存储需求。<details>
<summary>Abstract</summary>
In this work, we present an efficient and quantization-aware panoptic driving perception model (Q- YOLOP) for object detection, drivable area segmentation, and lane line segmentation, in the context of autonomous driving. Our model employs the Efficient Layer Aggregation Network (ELAN) as its backbone and task-specific heads for each task. We employ a four-stage training process that includes pretraining on the BDD100K dataset, finetuning on both the BDD100K and iVS datasets, and quantization-aware training (QAT) on BDD100K. During the training process, we use powerful data augmentation techniques, such as random perspective and mosaic, and train the model on a combination of the BDD100K and iVS datasets. Both strategies enhance the model's generalization capabilities. The proposed model achieves state-of-the-art performance with an mAP@0.5 of 0.622 for object detection and an mIoU of 0.612 for segmentation, while maintaining low computational and memory requirements.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种高效和量化意识的泛型驾驶视觉模型（Q-YOLOP），用于对象检测、驾驶区域分割和车道线分割，在自动驾驶上下文中。我们的模型采用了高效层堆网络（ELAN）作为基础，并在每个任务上采用专门的任务头。我们采用了四个阶段的训练过程，包括预训练于BDD100K数据集、finetuning于BDD100K和iVS数据集，以及量化意识训练（QAT）于BDD100K。在训练过程中，我们使用了强大的数据扩展技术，如随机投影和拼图，并在BDD100K和iVS数据集上训练模型。这两种策略都会增强模型的通用化能力。我们提出的模型在对象检测和分割任务中具有状态机器的表现，即mAP@0.5为0.622，而且保持了低的计算和存储需求。
</details></li>
</ul>
<hr>
<h2 id="QBitOpt-Fast-and-Accurate-Bitwidth-Reallocation-during-Training"><a href="#QBitOpt-Fast-and-Accurate-Bitwidth-Reallocation-during-Training" class="headerlink" title="QBitOpt: Fast and Accurate Bitwidth Reallocation during Training"></a>QBitOpt: Fast and Accurate Bitwidth Reallocation during Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04535">http://arxiv.org/abs/2307.04535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorn Peters, Marios Fournarakis, Markus Nagel, Mart van Baalen, Tijmen Blankevoort</li>
<li>for: 这篇论文的目的是提出一个新的算法来更新量化网络中层的比特宽，以提高量化网络在移动和嵌入式设备上的效率。</li>
<li>methods: 这篇论文使用了量化网络的训练方法，并将层的比特宽更新为获得更高的任务性能。</li>
<li>results: 根据评估 ImageNet 的结果，这篇论文的算法可以在严格的比特宽限制下，与现有的固定和混合精度方法相比，实现更高的任务性能。<details>
<summary>Abstract</summary>
Quantizing neural networks is one of the most effective methods for achieving efficient inference on mobile and embedded devices. In particular, mixed precision quantized (MPQ) networks, whose layers can be quantized to different bitwidths, achieve better task performance for the same resource constraint compared to networks with homogeneous bitwidths. However, finding the optimal bitwidth allocation is a challenging problem as the search space grows exponentially with the number of layers in the network. In this paper, we propose QBitOpt, a novel algorithm for updating bitwidths during quantization-aware training (QAT). We formulate the bitwidth allocation problem as a constraint optimization problem. By combining fast-to-compute sensitivities with efficient solvers during QAT, QBitOpt can produce mixed-precision networks with high task performance guaranteed to satisfy strict resource constraints. This contrasts with existing mixed-precision methods that learn bitwidths using gradients and cannot provide such guarantees. We evaluate QBitOpt on ImageNet and confirm that we outperform existing fixed and mixed-precision methods under average bitwidth constraints commonly found in the literature.
</details>
<details>
<summary>摘要</summary>
quantizing neural networks是一种最有效的方法来实现移动和嵌入式设备上的高效推理。特别是杂性精度归一化（MPQ）网络，它们的层可以被归一化到不同的比特宽，可以在相同的资源约束下实现更好的任务性能。然而，寻找最佳比特宽分配是一个复杂的问题，因为搜索空间会随着网络层数的增加而增长 exponentially。在这篇论文中，我们提出了QBitOpt算法，它是一种在量化训练中更新比特宽的算法。我们将比特宽分配问题定义为一个约束优化问题。通过将快速计算的敏感度与高效的解决方案结合在一起，QBitOpt可以生成杂性精度网络，保证任务性能和严格的资源约束之间的均衡。这与现有的杂性精度方法不同，它们通过梯度来学习比特宽，但无法提供类似的保证。我们对ImageNet进行了评估，并证明了我们在常见的平均比特宽约束下超过了现有的固定和杂性精度方法。
</details></li>
</ul>
<hr>
<h2 id="Preventing-Errors-in-Person-Detection-A-Part-Based-Self-Monitoring-Framework"><a href="#Preventing-Errors-in-Person-Detection-A-Part-Based-Self-Monitoring-Framework" class="headerlink" title="Preventing Errors in Person Detection: A Part-Based Self-Monitoring Framework"></a>Preventing Errors in Person Detection: A Part-Based Self-Monitoring Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04533">http://arxiv.org/abs/2307.04533</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fraunhoferiks/smf-object-detection">https://github.com/fraunhoferiks/smf-object-detection</a></li>
<li>paper_authors: Franziska Schwaiger, Andrea Matic, Karsten Roscher, Stephan Günnemann</li>
<li>for: 提高自主系统在真实应用中的可靠性，尤其是人体检测 tasks，以避免错误检测。</li>
<li>methods: 提出了一种自动监控框架，让感知系统在运行时进行可能性检查。通过添加人体部分检测组件，可以显著减少基线设置中的人体检测错失率，最多减少9倍。同时，通过同时训练人体和其部分，可以减少假阳检测率，最多减少50%。</li>
<li>results: 通过在公共可用的数据集DensePose和Pascal VOC上进行广泛的实验，证明了我们的框架的效iveness。代码可以在<a target="_blank" rel="noopener" href="https://github.com/FraunhoferIKS/smf-object-detection%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/FraunhoferIKS/smf-object-detection上下载。</a><details>
<summary>Abstract</summary>
The ability to detect learned objects regardless of their appearance is crucial for autonomous systems in real-world applications. Especially for detecting humans, which is often a fundamental task in safety-critical applications, it is vital to prevent errors. To address this challenge, we propose a self-monitoring framework that allows for the perception system to perform plausibility checks at runtime. We show that by incorporating an additional component for detecting human body parts, we are able to significantly reduce the number of missed human detections by factors of up to 9 when compared to a baseline setup, which was trained only on holistic person objects. Additionally, we found that training a model jointly on humans and their body parts leads to a substantial reduction in false positive detections by up to 50% compared to training on humans alone. We performed comprehensive experiments on the publicly available datasets DensePose and Pascal VOC in order to demonstrate the effectiveness of our framework. Code is available at https://github.com/ FraunhoferIKS/smf-object-detection.
</details>
<details>
<summary>摘要</summary>
自动化系统在实际应用中需要检测已经学习的对象，不管它们的外观如何。尤其是在检测人类时，这是安全关键任务，避免错误是非常重要。为解决这个挑战，我们提出了一个 runtime 进行可靠性检查的自我监控框架。我们发现，通过在 runtime 添加人体部分检测的附加组件，可以大幅减少与基准设置相比（只在人体整体对象上进行训练）的人类检测错过率，达到了9倍的提高。此外，我们发现，同时训练人体和人体部分模型，可以将假阳性检测错误量降低至50%以上，比训练只在人体上进行训练要好。我们在公共可用的 dataset 上进行了广泛的实验，以示效果。代码可以在 https://github.com/FraunhoferIKS/smf-object-detection 上获取。
</details></li>
</ul>
<hr>
<h2 id="SAGC-A68-a-space-access-graph-dataset-for-the-classification-of-spaces-and-space-elements-in-apartment-buildings"><a href="#SAGC-A68-a-space-access-graph-dataset-for-the-classification-of-spaces-and-space-elements-in-apartment-buildings" class="headerlink" title="SAGC-A68: a space access graph dataset for the classification of spaces and space elements in apartment buildings"></a>SAGC-A68: a space access graph dataset for the classification of spaces and space elements in apartment buildings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04515">http://arxiv.org/abs/2307.04515</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a2amir/sagc-a68">https://github.com/a2amir/sagc-a68</a></li>
<li>paper_authors: Amir Ziaee, Georg Suter</li>
<li>For: The paper aims to provide a dataset and a method for automated classification of spaces and space elements in digital 3D models of apartment buildings, using Graph Deep Learning (GDL) techniques.* Methods: The paper introduces a new dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings. The dataset is well-suited for developing GDL models for space function and space element classification. The paper also employs a graph attention network (GAT) to predict 22 space function and 6 space element classes using the dataset.* Results: The paper demonstrates the potential of the dataset and the GAT method by achieving high accuracy in predicting space functions and space elements in digital 3D models of apartment buildings. The dataset and code used in the experiment are available online.<details>
<summary>Abstract</summary>
The analysis of building models for usable area, building safety, and energy use requires accurate classification data of spaces and space elements. To reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable. A barrier hindering the utilization of Graph Deep Learning (GDL) methods to space function and space element classification is a lack of suitable datasets. To bridge this gap, we introduce a dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings. This graph-based dataset is well-suited for developing GDL models for space function and space element classification. To demonstrate the potential of the dataset, we employ it to train and evaluate a graph attention network (GAT) that predicts 22 space function and 6 space element classes. The dataset and code used in the experiment are available online. https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68.
</details>
<details>
<summary>摘要</summary>
“建筑模型中的usable area、建筑安全和能源使用的分析需要准确的空间和空间元素分类数据。为了减少输入模型准备和错误，自动化空间和空间元素分类是非常有利的。然而，使用图深度学习（GDL）方法进行空间功能和空间元素分类时存在一个障碍，即缺乏适当的数据集。为了缓解这个问题，我们介绍了一个数据集，即SAGC-A68，该数据集包含68个数字3D模型的空间布局图像。这个图像基于的数据集非常适合用于开发GDL模型进行空间功能和空间元素分类。为了证明数据集的潜力，我们使用它来训练和评估一个图注意网络（GAT），该网络预测22个空间功能和6个空间元素类别。数据集和实验代码在线可用，请参考https://doi.org/10.5281/zenodo.7805872和https://github.com/A2Amir/SAGC-A68。”Note: Simplified Chinese is used in this translation, as it is the most widely used variety of Chinese in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Improving-Heterogeneous-Graph-Learning-with-Weighted-Mixed-Curvature-Product-Manifold"><a href="#Improving-Heterogeneous-Graph-Learning-with-Weighted-Mixed-Curvature-Product-Manifold" class="headerlink" title="Improving Heterogeneous Graph Learning with Weighted Mixed-Curvature Product Manifold"></a>Improving Heterogeneous Graph Learning with Weighted Mixed-Curvature Product Manifold</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04514">http://arxiv.org/abs/2307.04514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sharecodesubmission/weighted_product_manifold">https://github.com/sharecodesubmission/weighted_product_manifold</a></li>
<li>paper_authors: Tuc Nguyen-Van, Dung D. Le, The-Anh Ta</li>
<li>for: 这 paper 的目的是学习不同结构的图像 embedding，以便在多种下游任务中提高表示能力。</li>
<li>methods: 这 paper 使用 product manifold 的方法，其中每个组件空间在 product 空间中有不同的贡献，因此需要根据数据自动确定每个组件的权重。</li>
<li>results: 这 paper 的实验表明，使用 weighted product manifold 方法可以从输入数据中学习更好的图像表示，并在多个下游任务中表现更好，例如 word similarity learning、top-$k$ recommendation 和知识图 embedding。<details>
<summary>Abstract</summary>
In graph representation learning, it is important that the complex geometric structure of the input graph, e.g. hidden relations among nodes, is well captured in embedding space. However, standard Euclidean embedding spaces have a limited capacity in representing graphs of varying structures. A promising candidate for the faithful embedding of data with varying structure is product manifolds of component spaces of different geometries (spherical, hyperbolic, or euclidean). In this paper, we take a closer look at the structure of product manifold embedding spaces and argue that each component space in a product contributes differently to expressing structures in the input graph, hence should be weighted accordingly. This is different from previous works which consider the roles of different components equally. We then propose WEIGHTED-PM, a data-driven method for learning embedding of heterogeneous graphs in weighted product manifolds. Our method utilizes the topological information of the input graph to automatically determine the weight of each component in product spaces. Extensive experiments on synthetic and real-world graph datasets demonstrate that WEIGHTED-PM is capable of learning better graph representations with lower geometric distortion from input data, and performs better on multiple downstream tasks, such as word similarity learning, top-$k$ recommendation, and knowledge graph embedding.
</details>
<details>
<summary>摘要</summary>
在图表示学习中，将复杂的图像结构（如节点之间的隐藏关系） faithful embedding 到嵌入空间中是非常重要的。然而，标准的欧几何嵌入空间具有表示图像的不同结构的局限性。一种有前途的候选人是Product manifold embedding 空间，其中每个分量空间具有不同的几何结构（球形、尖锐形或欧几何形）。在这篇论文中，我们更加细详地研究Product manifold embedding 空间的结构，并论证每个分量空间在Product manifold中对输入图像的表示具有不同的贡献，因此应该被权衡。这与之前的工作不同，那些假设所有分量空间具有相同的贡献。我们然后提出了WEIGHTED-PM，一种基于数据的方法，用于学习异构图像的嵌入。我们的方法利用输入图像的topological信息自动确定每个分量空间的权重。我们对 synthetic 和实际世界图像集进行了广泛的实验，结果表明WEIGHTED-PM 能够从输入数据中学习更好的图像表示，并且在多个下游任务上表现更好，如单词相似学习、top-$k$ 推荐和知识图像嵌入。
</details></li>
</ul>
<hr>
<h2 id="Improving-Factuality-of-Abstractive-Summarization-via-Contrastive-Reward-Learning"><a href="#Improving-Factuality-of-Abstractive-Summarization-via-Contrastive-Reward-Learning" class="headerlink" title="Improving Factuality of Abstractive Summarization via Contrastive Reward Learning"></a>Improving Factuality of Abstractive Summarization via Contrastive Reward Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04507">http://arxiv.org/abs/2307.04507</a></li>
<li>repo_url: None</li>
<li>paper_authors: I-Chun Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig</li>
<li>for: 提高报告摘要的准确性和可靠性</li>
<li>methods: 利用最新的奖励学习和实际度指标，实现对Feedback的报告摘要学习</li>
<li>results: 经验研究表明，提出的框架可以通过对实际度指标的反馈学习，从而生成更加准确和可靠的报告摘要。<details>
<summary>Abstract</summary>
Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deductive-Controller-Synthesis-for-Probabilistic-Hyperproperties"><a href="#Deductive-Controller-Synthesis-for-Probabilistic-Hyperproperties" class="headerlink" title="Deductive Controller Synthesis for Probabilistic Hyperproperties"></a>Deductive Controller Synthesis for Probabilistic Hyperproperties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04503">http://arxiv.org/abs/2307.04503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Andriushchenko, Ezio Bartocci, Milan Ceska, Francesco Pontiggia, Sarah Sallinger</li>
<li>for: 这个论文旨在解决Markov决策过程（MDP）和概率协同特性（HyperPCTL）的控制 synthesis问题。</li>
<li>methods: 该方法使用逻辑HyperPCTL和结构约束来描述控制器，并使用抽象减少策略来缩小搜索空间。</li>
<li>results: 实验结果表明，该方法与HyperProb相比，可以耗时更短、更高效地解决控制 synthesis问题，并且可以同时满足概率协同特性和内部控制器约束（例如部分可观测性）以及间接控制器约束（例如共同行动协议）。<details>
<summary>Abstract</summary>
Probabilistic hyperproperties specify quantitative relations between the probabilities of reaching different target sets of states from different initial sets of states. This class of behavioral properties is suitable for capturing important security, privacy, and system-level requirements. We propose a new approach to solve the controller synthesis problem for Markov decision processes (MDPs) and probabilistic hyperproperties. Our specification language builds on top of the logic HyperPCTL and enhances it with structural constraints over the synthesized controllers. Our approach starts from a family of controllers represented symbolically and defined over the same copy of an MDP. We then introduce an abstraction refinement strategy that can relate multiple computation trees and that we employ to prune the search space deductively. The experimental evaluation demonstrates that the proposed approach considerably outperforms HyperProb, a state-of-the-art SMT-based model checking tool for HyperPCTL. Moreover, our approach is the first one that is able to effectively combine probabilistic hyperproperties with additional intra-controller constraints (e.g. partial observability) as well as inter-controller constraints (e.g. agreements on a common action).
</details>
<details>
<summary>摘要</summary>
probabistic 特性指定了从不同的初始状态集到达不同的目标状态集的概率关系。这类行为特性适用于捕捉安全性、隐私和系统级别的需求。我们提出了一种新的控制器Synthesis问题的解决方案，用于Markov决策过程（MDP）和概率特性。我们的规定语言基于逻辑HyperPCTL并增加了对synthesized控制器的结构约束。我们的方法从一个家族控制器的符号表示开始，这些控制器都是基于同一个MDP的。我们然后引入一种抽象减少策略，可以关联多个计算树，并通过推理减少搜索空间。实验评估表明，我们的方法在比 HyperProb，一个现有的SMT基于模板检查工具 для HyperPCTL 的性能上明显优于。此外，我们的方法可以有效地结合概率特性与内部控制器约束（例如半观察性）以及间接控制器约束（例如共同行动协议）。
</details></li>
</ul>
<hr>
<h2 id="Model-Driven-Engineering-Method-to-Support-the-Formalization-of-Machine-Learning-using-SysML"><a href="#Model-Driven-Engineering-Method-to-Support-the-Formalization-of-Machine-Learning-using-SysML" class="headerlink" title="Model-Driven Engineering Method to Support the Formalization of Machine Learning using SysML"></a>Model-Driven Engineering Method to Support the Formalization of Machine Learning using SysML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04495">http://arxiv.org/abs/2307.04495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Raedler, Juergen Mangler, Stefanie Rinderle-Ma<br>for:This paper aims to support the collaborative definition of machine learning tasks by leveraging model-based engineering in the formalization of the systems modeling language SysML.methods:The method introduced in this paper uses SysML to formalize knowledge from various domains, identify and integrate data sources, define semantic connections between data attributes, and define data processing steps within the machine learning support.results:The method is evaluated through two use cases and a user study, demonstrating the potential of integrating machine learning-specific properties in systems engineering techniques to support non-data scientists in defining specific aspects of a machine learning problem and documenting knowledge on the data. The results show that the method can consolidate knowledge from various domains and support the integration of machine learning in industry by involving several stakeholders.Here is the same information in Simplified Chinese:for:这篇论文目的是支持机器学习任务的共同定义，通过基于模型工程的形式化语言SysML的扩展。methods:这篇论文引入的方法使用SysML来形式化各个领域的知识，标识和集成数据源，定义数据属性之间的含义连接，以及在机器学习支持中定义数据处理步骤。results:这篇论文通过两个使用场景和一个用户研究，展示了将机器学习特有的属性 integrate into 系统工程技术中的可能性，以支持非数据科学家在定义机器学习问题中的参与。研究结果表明，这种方法可以汇集各个领域的知识，并且支持机器学习在产业中的整合，并且能够涉及多个团队成员。<details>
<summary>Abstract</summary>
Methods: This work introduces a method supporting the collaborative definition of machine learning tasks by leveraging model-based engineering in the formalization of the systems modeling language SysML. The method supports the identification and integration of various data sources, the required definition of semantic connections between data attributes, and the definition of data processing steps within the machine learning support.   Results: By consolidating the knowledge of domain and machine learning experts, a powerful tool to describe machine learning tasks by formalizing knowledge using the systems modeling language SysML is introduced. The method is evaluated based on two use cases, i.e., a smart weather system that allows to predict weather forecasts based on sensor data, and a waste prevention case for 3D printer filament that cancels the printing if the intended result cannot be achieved (image processing). Further, a user study is conducted to gather insights of potential users regarding perceived workload and usability of the elaborated method.   Conclusion: Integrating machine learning-specific properties in systems engineering techniques allows non-data scientists to understand formalized knowledge and define specific aspects of a machine learning problem, document knowledge on the data, and to further support data scientists to use the formalized knowledge as input for an implementation using (semi-) automatic code generation. In this respect, this work contributes by consolidating knowledge from various domains and therefore, fosters the integration of machine learning in industry by involving several stakeholders.
</details>
<details>
<summary>摘要</summary>
方法：本研究将机器学习任务支持使用系统模型语言（SysML）进行形式化，以便推广专业人员之间的协作定义。这种方法可以支持资料来源的识别和集成，并定义资料属性之间的semantic连接，以及机器学习支持中的数据处理步骤。结果：通过将领域专家和机器学习专家的知识结构化使用SysML，我们提出了一个具有强大描述机器学习任务的工具。这种方法在两个使用案例中进行评估，包括一个预测天气预报基于传感器数据的智能天气系统，以及一个根据图像处理的废物预防系统。此外，我们还进行了用户研究，以获得潜在用户对方法的感受和使用预期。结论：通过将机器学习特有的属性 integrate到系统工程技术中，非数据科学家可以理解形式化知识，定义特定的机器学习问题的方面，将资料处理步骤文档化，并且支持（半）自动代码生成。在这种方式下，本研究对各个领域的知识集成，因此推动了机器学习在业界的整合，并让多个潜在用户参与。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Large-Language-Model-for-Graph-Data-Understanding-in-Online-Job-Recommendations"><a href="#Exploring-Large-Language-Model-for-Graph-Data-Understanding-in-Online-Job-Recommendations" class="headerlink" title="Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations"></a>Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05722">http://arxiv.org/abs/2307.05722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, Enhong Chen</li>
<li>for: This paper aims to explore the potential of large language models (LLMs) in understanding behavior graphs and enhancing job recommendations in online recruitment, including the promotion of out-of-distribution (OOD) applications.</li>
<li>methods: The proposed framework leverages the rich contextual information and semantic representations provided by LLMs to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, it uses a meta-path prompt constructor to understand behavior graphs and a path augmentation module to alleviate prompt bias.</li>
<li>results: The approach is evaluated on a comprehensive dataset and demonstrates improved relevance and quality of recommended jobs compared to traditional path-based sequence input methods. The findings contribute to the growing field of natural language processing and offer practical implications for enhancing job search experiences.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for behavior graph understanding in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including the promotion of out-of-distribution (OOD) application. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that leverages LLM recommender to understand behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By leveraging this capability, our framework enables personalized and accurate job recommendations for individual users. We evaluate the effectiveness of our approach on a comprehensive dataset and demonstrate its ability to improve the relevance and quality of recommended quality. This research not only sheds light on the untapped potential of large language models but also provides valuable insights for developing advanced recommendation systems in the recruitment market. The findings contribute to the growing field of natural language processing and offer practical implications for enhancing job search experiences.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经革命化了自然语言处理任务，展示了其在各个领域的出色表现。然而，它们在行为图理解方面的潜在力还尚未得到充分利用。这篇论文探讨了大型语言模型在理解行为图方面的能力，并利用这种理解来提高在线招聘中的推荐。我们提出了一种新的框架，利用大型语言模型提供的背景信息和含义表示来分析行为图，揭示下面的模式和关系。我们提出了一种基于LLM推荐的元路提取器，用于理解行为图，并设计了一种相应的路径增强模块，以减少路径基于序列输入引入的提示偏见。通过利用这种能力，我们的框架可以为个人用户提供个性化和准确的职业推荐。我们对一个完整的数据集进行了评估，并证明了我们的方法可以提高推荐的 relevance 和质量。这些发现不仅探讨了大型语言模型的未利用潜力，还为自然语言处理领域的进步提供了有价值的 suggessions，以及改善employme搜索经验。
</details></li>
</ul>
<hr>
<h2 id="Proceeding-of-the-1st-Workshop-on-Social-Robots-Personalisation-At-the-crossroads-between-engineering-and-humanities-CONCATENATE"><a href="#Proceeding-of-the-1st-Workshop-on-Social-Robots-Personalisation-At-the-crossroads-between-engineering-and-humanities-CONCATENATE" class="headerlink" title="Proceeding of the 1st Workshop on Social Robots Personalisation At the crossroads between engineering and humanities (CONCATENATE)"></a>Proceeding of the 1st Workshop on Social Robots Personalisation At the crossroads between engineering and humanities (CONCATENATE)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12777">http://arxiv.org/abs/2307.12777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Imene Tarakli, Georgios Angelopoulos, Mehdi Hellou, Camille Vindolet, Boris Abramovic, Rocco Limongelli, Dimitri Lacroix, Andrea Bertolini, Silvia Rossi, Alessandro Di Nuovo, Angelo Cangelosi, Gordon Cheng</li>
<li>for: This paper aims to discuss and propose guidelines for personalization in robotics, addressing questions such as how to define it, how to achieve it, and how it should be guided to fit legal and ethical requirements.</li>
<li>methods: The paper uses an interdisciplinary approach, bringing together researchers from various fields to discuss and propose guidelines for personalization in robotics.</li>
<li>results: The paper aims to provide a comprehensive understanding of personalization in robotics, including its definition, achievement, and ethical considerations, to ensure the large-scale adoption of social robotics.<details>
<summary>Abstract</summary>
Nowadays, robots are expected to interact more physically, cognitively, and socially with people. They should adapt to unpredictable contexts alongside individuals with various behaviours. For this reason, personalisation is a valuable attribute for social robots as it allows them to act according to a specific user's needs and preferences and achieve natural and transparent robot behaviours for humans. If correctly implemented, personalisation could also be the key to the large-scale adoption of social robotics. However, achieving personalisation is arduous as it requires us to expand the boundaries of robotics by taking advantage of the expertise of various domains. Indeed, personalised robots need to analyse and model user interactions while considering their involvement in the adaptative process. It also requires us to address ethical and socio-cultural aspects of personalised HRI to achieve inclusive and diverse interaction and avoid deception and misplaced trust when interacting with the users. At the same time, policymakers need to ensure regulations in view of possible short-term and long-term adaptive HRI. This workshop aims to raise an interdisciplinary discussion on personalisation in robotics. It aims at bringing researchers from different fields together to propose guidelines for personalisation while addressing the following questions: how to define it - how to achieve it - and how it should be guided to fit legal and ethical requirements.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:现在，机器人被期望能更加physically、cognitively和社会地与人们互动。它们应该适应不可预测的情况，与具有不同行为的人们一起互动。因此，个性化成为社交机器人的有价值特性，允许它们根据特定用户的需求和偏好进行行为，并为人类带来自然和透明的机器人行为。如果正确实施，个性化也可能成为社交机器人的大规模采用的关键。然而，实现个性化是困难的，因为它需要我们拓宽机器人领域，利用不同领域的专家知识。实际上，个性化机器人需要分析和模型用户互动，同时考虑用户参与过程中的适应性。此外，我们还需要考虑人工智能与人类社会的伦理和文化方面，以实现包容和多样化的互动，避免欺骗和不当信任。同时，政策制定者需要确保法规，以适应可能的短期和长期适应HRI。这个工作室希望通过跨学科的交流，让研究人员从不同领域集结一起，提出个性化指南，并解决以下问题：如何定义它？如何实现它？如何使其遵循法律和伦理要求？
</details></li>
</ul>
<hr>
<h2 id="PapagAI-Automated-Feedback-for-Reflective-Essays"><a href="#PapagAI-Automated-Feedback-for-Reflective-Essays" class="headerlink" title="PapagAI:Automated Feedback for Reflective Essays"></a>PapagAI:Automated Feedback for Reflective Essays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07523">http://arxiv.org/abs/2307.07523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Veronika Solopova, Adrian Gruszczynski, Eiad Rostom, Fritz Cremer, Sascha Witte, Chengming Zhang, Fernando Ramos López Lea Plößl, Florian Hofmann, Ralf Romeike, Michaela Gläser-Zikuda, Christoph Benzmüller, Tim Landgraf</li>
<li>for: 提高学生学习成绩和讲师教学 activites的补充</li>
<li>methods: 基于教学理论的自动反馈工具， hybrid AI系统</li>
<li>results: 提供了一种开源自动反馈工具，可以补充讲师的反馈活动，提高学生的学习成绩<details>
<summary>Abstract</summary>
Written reflective practice is a regular exercise pre-service teachers perform during their higher education. Usually, their lecturers are expected to provide individual feedback, which can be a challenging task to perform on a regular basis. In this paper, we present the first open-source automated feedback tool based on didactic theory and implemented as a hybrid AI system. We describe the components and discuss the advantages and disadvantages of our system compared to the state-of-art generative large language models. The main objective of our work is to enable better learning outcomes for students and to complement the teaching activities of lecturers.
</details>
<details>
<summary>摘要</summary>
高等教育中的写作反思实践是未来教师的常规课程之一。通常，课堂教师会提供个人反馈，但这可能是一项困难的任务。在这篇论文中，我们介绍了第一个开源自动反馈工具，基于教学理论而实现，并作为混合式人工智能系统。我们说明了它的组成部分，并讨论了我们的系统与现有的生成大语言模型之间的优劣点。我们的主要目的是帮助学生学习更好，并辅助课堂教师的教学活动。
</details></li>
</ul>
<hr>
<h2 id="Digital-Modeling-for-Everyone-Exploring-How-Novices-Approach-Voice-Based-3D-Modeling"><a href="#Digital-Modeling-for-Everyone-Exploring-How-Novices-Approach-Voice-Based-3D-Modeling" class="headerlink" title="Digital Modeling for Everyone: Exploring How Novices Approach Voice-Based 3D Modeling"></a>Digital Modeling for Everyone: Exploring How Novices Approach Voice-Based 3D Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04481">http://arxiv.org/abs/2307.04481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Desolda, Andrea Esposito, Florian Müller, Sebastian Feger</li>
<li>for: 这项研究的目的是提高voice assistant的用户体验，使得更多人可以通过语音指令制作3D模型。</li>
<li>methods: 研究采用了高严译Wizard of Oz方法，与22名参与者进行实验，以了解初级用户的心理模型如何在语音基于3D模型创建中表现。</li>
<li>results: 研究发现，初级用户通常会发送模糊、不完整或错误的语音指令，因此voice助手需要处理这些指令并提供相应的帮助。 In addition, the study found that users need a set of straightforward commands to shape simple and composite objects, and different strategies to select 3D objects.<details>
<summary>Abstract</summary>
Manufacturing tools like 3D printers have become accessible to the wider society, making the promise of digital fabrication for everyone seemingly reachable. While the actual manufacturing process is largely automated today, users still require knowledge of complex design applications to produce ready-designed objects and adapt them to their needs or design new objects from scratch. To lower the barrier to the design and customization of personalized 3D models, we explored novice mental models in voice-based 3D modeling by conducting a high-fidelity Wizard of Oz study with 22 participants. We performed a thematic analysis of the collected data to understand how the mental model of novices translates into voice-based 3D modeling. We conclude with design implications for voice assistants. For example, they have to: deal with vague, incomplete and wrong commands; provide a set of straightforward commands to shape simple and composite objects; and offer different strategies to select 3D objects.
</details>
<details>
<summary>摘要</summary>
现代制造工具如3D打印机已成为更广泛的社会可达，使得数字制造的承诺似乎已经实现可行。虽然今天的制造过程已经大多是自动化的，但用户仍需具备复杂的设计应用程序知识来生产预设的物体和根据自己的需求或从头开始设计新的物体。为降低个性化3D模型的设计和自定义的门槛，我们进行了一项高效精准的奥瑞奥斯研究，与22名参与者进行了高精准的研究。我们通过主题分析收集的数据来理解 novice 的心理模型如何翻译为语音基于3D模型。我们结论出了设计启示，如：处理欠准确、不完整的指令，提供简单明了的命令来形成简单和复杂物体，以及不同的策略来选择3D对象。
</details></li>
</ul>
<hr>
<h2 id="Some-Preliminary-Steps-Towards-Metaverse-Logic"><a href="#Some-Preliminary-Steps-Towards-Metaverse-Logic" class="headerlink" title="Some Preliminary Steps Towards Metaverse Logic"></a>Some Preliminary Steps Towards Metaverse Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05574">http://arxiv.org/abs/2307.05574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio L. Furtado, Marco A. Casanova, Edirlei Soares de Lima</li>
<li>for: 这个论文的目的是开发一种能够处理现实世界和虚拟世界应用领域中的不稳定行为的逻辑。</li>
<li>methods: 这篇论文使用了非常ventional的扩展，尝试绘制一种最小化composite逻辑策略。</li>
<li>results: 论文通过使用 chatGPT AI agent，并appealing to common-sense approach，提出了一种可以用于处理不稳定行为的逻辑策略。<details>
<summary>Abstract</summary>
Assuming that the term 'metaverse' could be understood as a computer-based implementation of multiverse applications, we started to look in the present work for a logic that would be powerful enough to handle the situations arising both in the real and in the fictional underlying application domains. Realizing that first-order logic fails to account for the unstable behavior of even the most simpleminded information system domains, we resorted to non-conventional extensions, in an attempt to sketch a minimal composite logic strategy. The discussion was kept at a rather informal level, always trying to convey the intuition behind the theoretical notions in natural language terms, and appealing to an AI agent, namely ChatGPT, in the hope that algorithmic and common-sense approaches can be usefully combined.
</details>
<details>
<summary>摘要</summary>
假设“metaverse”可以理解为基于计算机实现的多元宇宙应用程序，我们在 presente 工作中寻找了一种逻辑能够处理现实世界和 fictional 下的应用领域中出现的情况。因为首领逻辑无法考虑even the simplest information system 的不稳定行为，我们转而使用非标准扩展，以尝试绘制最小的复合逻辑策略。在讨论中，我们尽量使用自然语言来表达理论概念，并征用一个 AI 代理人，即 ChatGPT，希望可以将算法和通俗的方法相结合。
</details></li>
</ul>
<hr>
<h2 id="Designing-Novel-Cognitive-Diagnosis-Models-via-Evolutionary-Multi-Objective-Neural-Architecture-Search"><a href="#Designing-Novel-Cognitive-Diagnosis-Models-via-Evolutionary-Multi-Objective-Neural-Architecture-Search" class="headerlink" title="Designing Novel Cognitive Diagnosis Models via Evolutionary Multi-Objective Neural Architecture Search"></a>Designing Novel Cognitive Diagnosis Models via Evolutionary Multi-Objective Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04429">http://arxiv.org/abs/2307.04429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/devilyangs/emo-nas-cd">https://github.com/devilyangs/emo-nas-cd</a></li>
<li>paper_authors: Shangshang Yang, Haiping Ma, Cheng Zhen, Ye Tian, Limiao Zhang, Yaochu Jin, Xingyi Zhang</li>
<li>for: 本研究旨在提出一种自动设计智能诊断模型，以提高现有智能教育平台中学生知识概念诊断的精度和效果。</li>
<li>methods: 本研究使用了进化多目标神经网络架构搜索（NAS）方法自动设计智能诊断模型，并采用了多目标遗传编程（MOGP）法来探索搜索空间。每个建筑物被转化为树结构，并使用树来容易优化。此外，还提出了一种初始化策略，以加速共振。</li>
<li>results: 实验结果显示，由提出的方法搜索的诊断模型在两个实际数据集上表现出了显著更好的性能，并且与人工设计的模型一样有效。<details>
<summary>Abstract</summary>
Cognitive diagnosis plays a vital role in modern intelligent education platforms to reveal students' proficiency in knowledge concepts for subsequent adaptive tasks. However, due to the requirement of high model interpretability, existing manually designed cognitive diagnosis models hold too simple architectures to meet the demand of current intelligent education systems, where the bias of human design also limits the emergence of effective cognitive diagnosis models. In this paper, we propose to automatically design novel cognitive diagnosis models by evolutionary multi-objective neural architecture search (NAS). Specifically, we observe existing models can be represented by a general model handling three given types of inputs and thus first design an expressive search space for the NAS task in cognitive diagnosis. Then, we propose multi-objective genetic programming (MOGP) to explore the NAS task's search space by maximizing model performance and interpretability. In the MOGP design, each architecture is transformed into a tree architecture and encoded by a tree for easy optimization, and a tailored genetic operation based on four sub-genetic operations is devised to generate offspring effectively. Besides, an initialization strategy is also suggested to accelerate the convergence by evolving half of the population from existing models' variants. Experiments on two real-world datasets demonstrate that the cognitive diagnosis models searched by the proposed approach exhibit significantly better performance than existing models and also hold as good interpretability as human-designed models.
</details>
<details>
<summary>摘要</summary>
现代智能教育平台中，认知诊断扮演着关键的角色，以揭示学生知识概念的熟练程度，以便进行适应性任务。然而，由于需要高度模型可读性，现有的手动设计的认知诊断模型具有过于简单的结构，无法满足现代智能教育系统的需求。在这篇论文中，我们提议使用进化多目标神经网络搜索（NAS）自动设计新的认知诊断模型。 Specifically，我们发现现有模型可以表示为三种输入类型的通用模型，因此首先设计了表达力强的搜索空间 для NAS任务。然后，我们提出了多目标遗传编程（MOGP）来探索 NAS 任务的搜索空间，并寻找最佳的模型性能和可读性。在 MOGP 设计中，每个建筑物被转换为树结构，并通过树来编码，以便优化。此外，我们还提出了一种 initialization 策略，以加速共振。实验结果表明，由我们提出的方法搜索出的认知诊断模型在两个真实世界数据集上表现出色，并且与人类设计的模型相当可读。
</details></li>
</ul>
<hr>
<h2 id="FedDCT-A-Dynamic-Cross-Tier-Federated-Learning-Scheme-in-Wireless-Communication-Networks"><a href="#FedDCT-A-Dynamic-Cross-Tier-Federated-Learning-Scheme-in-Wireless-Communication-Networks" class="headerlink" title="FedDCT: A Dynamic Cross-Tier Federated Learning Scheme in Wireless Communication Networks"></a>FedDCT: A Dynamic Cross-Tier Federated Learning Scheme in Wireless Communication Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04420">http://arxiv.org/abs/2307.04420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Liu, Youquan Xian, Chuanjian Yao, Xiaoyun Gan, Lianghaojie Zhou, Jianyong Jiang, Dongcheng Li</li>
<li>for: 这个研究旨在提高无线通信网络上的联合学习系统表现和准确性。</li>
<li>methods: 我们提出了一个novel的动态跨层联合学习方案（FedDCT），使用一个层分算法将客户端分为不同的层，并将每个层分配不同的超时阈值以减少训练时间。我们还引入了一个跨层客户选择算法，可以有效地选择层和参与者。</li>
<li>results: 我们的实验结果显示，我们的方案可以让模型更快地读数和 дости得更高的准确性在无线通信网络上。<details>
<summary>Abstract</summary>
With the rapid proliferation of Internet of Things (IoT) devices and the growing concern for data privacy among the public, Federated Learning (FL) has gained significant attention as a privacy-preserving machine learning paradigm. FL enables the training of a global model among clients without exposing local data. However, when a federated learning system runs on wireless communication networks, limited wireless resources, heterogeneity of clients, and network transmission failures affect its performance and accuracy. In this study, we propose a novel dynamic cross-tier FL scheme, named FedDCT to increase training accuracy and performance in wireless communication networks. We utilize a tiering algorithm that dynamically divides clients into different tiers according to specific indicators and assigns specific timeout thresholds to each tier to reduce the training time required. To improve the accuracy of the model without increasing the training time, we introduce a cross-tier client selection algorithm that can effectively select the tiers and participants. Simulation experiments show that our scheme can make the model converge faster and achieve a higher accuracy in wireless communication networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unmasking-the-giant-A-comprehensive-evaluation-of-ChatGPT’s-proficiency-in-coding-algorithms-and-data-structures"><a href="#Unmasking-the-giant-A-comprehensive-evaluation-of-ChatGPT’s-proficiency-in-coding-algorithms-and-data-structures" class="headerlink" title="Unmasking the giant: A comprehensive evaluation of ChatGPT’s proficiency in coding algorithms and data structures"></a>Unmasking the giant: A comprehensive evaluation of ChatGPT’s proficiency in coding algorithms and data structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05360">http://arxiv.org/abs/2307.05360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayed Erfan Arefin, Tasnia Ashrafi Heya, Hasan Al-Qudah, Ynes Ineza, Abdul Serwadda</li>
<li>For: The paper evaluates the coding capabilities of ChatGPT, a large language model, in the Python programming language, specifically focusing on data structures and algorithms.* Methods: The paper uses a comprehensive evaluation of ChatGPT’s coding capabilities based on a large catalog of coding challenges, and investigates the quality of ChatGPT’s code, the nature of run-time errors, and whether ChatGPT might have directly memorized some of the data used to train it.* Results: The paper investigates the above questions from the context of both the underlying learning models (GPT-3.5 and GPT-4) and on a vast array of sub-topics within the main topics, and compares with human performance whenever feasible.<details>
<summary>Abstract</summary>
The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these kinds of situations. To infer whether ChatGPT might have directly memorized some of the data that was used to train it, we methodically design an experiment to investigate this phenomena. Making comparisons with human performance whenever feasible, we investigate all the above questions from the context of both its underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics within the main topics, and on problems having varying degrees of difficulty.
</details>
<details>
<summary>摘要</summary>
LLMs 的变革性影响在人工智能技术领域是极其深远的。其中，ChatGPT 在多回交流中表现出色，并在多种语言中 exhibit 代码技巧。在这篇论文中，我们对 ChatGPT 的编程能力进行了全面的评估，以及在 Python 编程语言和数据结构、算法两个领域中的问题。我们评估 ChatGPT 是否能够生成正确的解决方案，代码质量和运行时错误的特点。当 ChatGPT 的代码成功执行但未能解决问题时，我们 investigate 到哪些测试用例中的征性。为了判断 ChatGPT 是否直接Memorized 一些训练数据，我们采用了一种系统atic experiment。我们在可能的情况下与人类性能进行比较，并 investigate  LLMs 的下面几个方面：GPT-3.5 和 GPT-4 的下面几个学习模型，多个子题下的问题，以及问题的Difficulty 度。
</details></li>
</ul>
<hr>
<h2 id="Ethicist-Targeted-Training-Data-Extraction-Through-Loss-Smoothed-Soft-Prompting-and-Calibrated-Confidence-Estimation"><a href="#Ethicist-Targeted-Training-Data-Extraction-Through-Loss-Smoothed-Soft-Prompting-and-Calibrated-Confidence-Estimation" class="headerlink" title="Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation"></a>Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04401">http://arxiv.org/abs/2307.04401</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-coai/targeted-data-extraction">https://github.com/thu-coai/targeted-data-extraction</a></li>
<li>paper_authors: Zhexin Zhang, Jiaxin Wen, Minlie Huang</li>
<li>for: 本研究旨在提出一种名为“Ethicist”的目标数据提取方法，以帮助避免语言模型内存化训练数据，从而降低隐私风险。</li>
<li>methods: 本方法使用损失平滑化和准确性估计来引导模型具有更好的特征，并通过固定模型进行调整软提示 embedding，以便诱导模型内存化训练数据。另外，我们还提出了一种简化损失函数，以使得抽取更加容易。</li>
<li>results: 我们的实验结果表明，使用 Ethicist 方法可以显著改善目标数据提取性能，并且可以调查各种因素（如排序策略、模型规模、前缀长度和后缀长度）对数据提取性能的影响。代码可以在 GitHub 上找到。<details>
<summary>Abstract</summary>
Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is available at https://github.com/thu-coai/Targeted-Data-Extraction.
</details>
<details>
<summary>摘要</summary>
大型预训语言模型在多种任务上实现了很好的结果，但是最近的研究发现，这些预训语言模型可能会记忆一部分的训练数据，导致隐私风险的信息泄露。在这篇论文中，我们提出了一种方法 named Ethicist，通过loss smoothed soft prompting和准确性调整，实现目标数据提取。我们在给定一个prefix时，通过调整软提示 embedding来引起模型的记忆。我们还提出了一种平滑损失，用于平滑训练数据中的 suffix token 的损失分布，以便更容易从 Correct 的 suffix 中采样。为了选择最有可能性的 suffix 从一组采样的 suffix 中，并估计预测结果的可信度，我们提出了一种准确性调整方法，通过地方估计来 норmalize 生成的 suffix 的可信度。我们表明，Ethicist 可以显著提高目标数据提取的性能，并对多种因素的影响进行了调查，包括解码策略、模型缩放、prefix 长度和 suffix 长度。我们的代码可以在 <https://github.com/thu-coai/Targeted-Data-Extraction> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning-A-Survey"><a href="#Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning-A-Survey" class="headerlink" title="Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey"></a>Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04370">http://arxiv.org/abs/2307.04370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pranav-chib/recent-advancements-in-end-to-end-autonomous-driving-using-deep-learning">https://github.com/pranav-chib/recent-advancements-in-end-to-end-autonomous-driving-using-deep-learning</a></li>
<li>paper_authors: Pranav Singh Chib, Pravendra Singh</li>
<li>for: This paper provides a comprehensive review of the End-to-End autonomous driving stack, including the entire driving process from perception to control.</li>
<li>methods: The paper employs neural networks in an End-to-End manner, addressing key challenges encountered in real-world applications.</li>
<li>results: The paper discusses recent developments in End-to-End autonomous driving, including sensorial input, main and auxiliary output, learning approaches, and model evaluation techniques.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文提供了综合的End-to-End自动驾驶栈评论，包括整个驾驶过程从感知到控制。</li>
<li>methods: 论文使用神经网络在End-to-End方式下进行整合，解决了实际应用中的主要挑战。</li>
<li>results: 论文介绍了最新的End-to-End自动驾驶发展，包括感知输入、主要和辅助输出、学习方法从模仿到强化学习、评估技术等。<details>
<summary>Abstract</summary>
End-to-End driving is a promising paradigm as it circumvents the drawbacks associated with modular systems, such as their overwhelming complexity and propensity for error propagation. Autonomous driving transcends conventional traffic patterns by proactively recognizing critical events in advance, ensuring passengers' safety and providing them with comfortable transportation, particularly in highly stochastic and variable traffic settings. This paper presents a comprehensive review of the End-to-End autonomous driving stack. It provides a taxonomy of automated driving tasks wherein neural networks have been employed in an End-to-End manner, encompassing the entire driving process from perception to control, while addressing key challenges encountered in real-world applications. Recent developments in End-to-End autonomous driving are analyzed, and research is categorized based on underlying principles, methodologies, and core functionality. These categories encompass sensorial input, main and auxiliary output, learning approaches ranging from imitation to reinforcement learning, and model evaluation techniques. The survey incorporates a detailed discussion of the explainability and safety aspects. Furthermore, it assesses the state-of-the-art, identifies challenges, and explores future possibilities. We maintained the latest advancements and their corresponding open-source implementations at https://github.com/Pranav-chib/Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning.
</details>
<details>
<summary>摘要</summary>
结束到结束驾驶是一种有前途的思路，因为它 circumvents 模块化系统的缺点，如它们的束缚性和错误传递。自动驾驶超越了传统的交通模式，因为它可以主动认识到关键事件的发生，保证乘客的安全，并为他们提供舒适的交通，特别是在高度随机和变量的交通Setting中。本文提供了结束到结束自动驾驶栈的全面回顾。它提供了自动驾驶任务中使用神经网络的End-to-End方式，涵盖整个驾驶过程从感知到控制，并解决了实际应用中遇到的关键挑战。最新的结束到结束自动驾驶发展分析，并将研究分为基本原理、方法论和核心功能方面。这些类别包括感知输入、主要和辅助输出、学习方法从仿制到强化学习，以及模型评估技术。文章还包括对解释性和安全方面的详细讨论，以及现状、挑战和未来可能性的评估。我们在 GitHub 上维护最新的进展和相应的开源实现，请参考 <https://github.com/Pranav-chib/Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning>。
</details></li>
</ul>
<hr>
<h2 id="ECS-–-an-Interactive-Tool-for-Data-Quality-Assurance"><a href="#ECS-–-an-Interactive-Tool-for-Data-Quality-Assurance" class="headerlink" title="ECS – an Interactive Tool for Data Quality Assurance"></a>ECS – an Interactive Tool for Data Quality Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04368">http://arxiv.org/abs/2307.04368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Sieberichs, Simon Geerkens, Alexander Braun, Thomas Waschulzik</li>
<li>for:  Ensuring high-quality data for use in safety-critical systems</li>
<li>methods:  Novel approach using mathematical basics and multiple examples to detect potentially harmful data points</li>
<li>results:  Detection of data points with potentially harmful properties for use in safety-critical systems.Here is the summary in traditional Chinese characters:</li>
<li>for:  Ensuring高品质数据 для安全重要系统</li>
<li>methods:  Novel方法使用数学基础和多个例子检测潜在危险数据点</li>
<li>results:  Detected潜在危险数据点for use in safety-critical systems.<details>
<summary>Abstract</summary>
With the increasing capabilities of machine learning systems and their potential use in safety-critical systems, ensuring high-quality data is becoming increasingly important. In this paper we present a novel approach for the assurance of data quality. For this purpose, the mathematical basics are first discussed and the approach is presented using multiple examples. This results in the detection of data points with potentially harmful properties for the use in safety-critical systems.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:随着机器学习系统的能力的提高和其在安全关键系统中的潜在使用，保证数据质量的重要性也在增加。在这篇论文中，我们提出了一种新的数据质量保障方法。在这里，我们首先介绍了数学基础，然后通过多个示例介绍了我们的方法。这种方法可以检测数据点中可能有害的特性，以便在安全关键系统中使用。
</details></li>
</ul>
<hr>
<h2 id="RLTF-Reinforcement-Learning-from-Unit-Test-Feedback"><a href="#RLTF-Reinforcement-Learning-from-Unit-Test-Feedback" class="headerlink" title="RLTF: Reinforcement Learning from Unit Test Feedback"></a>RLTF: Reinforcement Learning from Unit Test Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04349">http://arxiv.org/abs/2307.04349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zyq-scut/rltf">https://github.com/zyq-scut/rltf</a></li>
<li>paper_authors: Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, Deheng Ye</li>
<li>for: 提高大型自然语言模型（LLM）的代码生成性能，通过强化学习策略。</li>
<li>methods: 使用在线渐进学习策略，并在训练过程中接受多级别单元测试反馈。</li>
<li>results: 在APPS和MBPP测试benchmark上实现了状态前的性能。<details>
<summary>Abstract</summary>
The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, these RL methods have only used offline frameworks, limiting their exploration of new sample spaces. Additionally, current approaches that utilize unit test signals are rather simple, not accounting for specific error locations within the code. To address these issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code can be found at: https://github.com/Zyq-scut/RLTF.
</details>
<details>
<summary>摘要</summary>
目的是代码生成或程序生成，是根据给定的描述生成可执行的代码。最近，有越来越多的研究使用强化学习（RL）来提高大型自然语言模型（LLM）的代码质量。然而，这些RL方法只使用了离线框架，限制了新样本空间的探索。另外，当前的使用单元测试信号的方法比较简单，没有考虑特定的代码错误位置。为解决这些问题，我们提出了RLTF，即基于单元测试反馈的在线RL框架，用于细化代码LLM。我们的方法在训练中生成数据并同时使用多级别的反馈信号来引导模型生成更高质量的代码。我们的实验表明，RLTF在APPS和MBPP测试准则上达到了状态 искусственный智能的性能。您可以在以下GitHub上找到我们的代码：https://github.com/Zyq-scut/RLTF。
</details></li>
</ul>
<hr>
<h2 id="Injecting-Logical-Constraints-into-Neural-Networks-via-Straight-Through-Estimators"><a href="#Injecting-Logical-Constraints-into-Neural-Networks-via-Straight-Through-Estimators" class="headerlink" title="Injecting Logical Constraints into Neural Networks via Straight-Through Estimators"></a>Injecting Logical Constraints into Neural Networks via Straight-Through Estimators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04347">http://arxiv.org/abs/2307.04347</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/azreasoners/cl-ste">https://github.com/azreasoners/cl-ste</a></li>
<li>paper_authors: Zhun Yang, Joohyung Lee, Chiyoun Park</li>
<li>for: 本文旨在探讨把逻辑约束直接插入神经网络学习中的挑战。</li>
<li>methods: 我们提出了一种系统地将逻辑约束表示为损失函数，使用梯度下降via直通估计器更新神经网络的权重，使神经网络满足逻辑约束。</li>
<li>results: 我们的方法可以利用GPU和批处理训练，与现有的神经符号计算方法相比，具有更好的扩展性和可扩展性。此外，我们还证明了我们的方法可以应用于不同类型的神经网络，如MLP、CNN和GNN，使它们可以通过直接学习已知约束而不需要大量标注数据。<details>
<summary>Abstract</summary>
Injecting discrete logical constraints into neural network learning is one of the main challenges in neuro-symbolic AI. We find that a straight-through-estimator, a method introduced to train binary neural networks, could effectively be applied to incorporate logical constraints into neural network learning. More specifically, we design a systematic way to represent discrete logical constraints as a loss function; minimizing this loss using gradient descent via a straight-through-estimator updates the neural network's weights in the direction that the binarized outputs satisfy the logical constraints. The experimental results show that by leveraging GPUs and batch training, this method scales significantly better than existing neuro-symbolic methods that require heavy symbolic computation for computing gradients. Also, we demonstrate that our method applies to different types of neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer labeled data by learning directly from known constraints.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使得神经网络学习中插入离散逻辑约束是新 символи� AI 中的主要挑战。我们发现，一种直接推导器，一种用于训练 binary 神经网络的方法，可以有效地插入逻辑约束到神经网络学习中。更 Specifically，我们设计了一种系统的方法来表示离散逻辑约束，将其作为损失函数来折算；使用梯度下降via straight-through-estimator更新神经网络的权重，以便使得 binarized 输出满足逻辑约束。实验结果表明，通过利用 GPU 和批处理训练，这种方法可以比既存的新 simulate 方法更好地缩放，而且我们的方法可以应用到不同类型的神经网络，如 MLP、CNN 和 GNN，使它们可以通过不或少的标注数据学习。Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide that instead.
</details></li>
</ul>
<hr>
<h2 id="Continual-Learning-as-Computationally-Constrained-Reinforcement-Learning"><a href="#Continual-Learning-as-Computationally-Constrained-Reinforcement-Learning" class="headerlink" title="Continual Learning as Computationally Constrained Reinforcement Learning"></a>Continual Learning as Computationally Constrained Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04345">http://arxiv.org/abs/2307.04345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, Benjamin Van Roy</li>
<li>for: 本文旨在解决人工智能领域中长期积累知识的问题，以提高人工智能能力的前iers。</li>
<li>methods: 本文使用了一种概念框架和工具集，以促进对 continual learning 的进一步研究。</li>
<li>results: 本文提出了一种概念框架和工具集，可以帮助研究人员更好地理解和解决 continual learning 问题。<details>
<summary>Abstract</summary>
An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities. The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning. This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research.
</details>
<details>
<summary>摘要</summary>
一个智能代理人，能够有效地积累知识，以发展逐渐加强的技能，可能会推动人工智能技术的前沿。这种代理人的设计，仍然是人工智能领域的长期挑战。这本善本帮助解决这个问题，提供了一个框架和一套工具，以便进一步探索。
</details></li>
</ul>
<hr>
<h2 id="Stroke-Extraction-of-Chinese-Character-Based-on-Deep-Structure-Deformable-Image-Registration"><a href="#Stroke-Extraction-of-Chinese-Character-Based-on-Deep-Structure-Deformable-Image-Registration" class="headerlink" title="Stroke Extraction of Chinese Character Based on Deep Structure Deformable Image Registration"></a>Stroke Extraction of Chinese Character Based on Deep Structure Deformable Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04341">http://arxiv.org/abs/2307.04341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mengli-l1/strokeextraction">https://github.com/mengli-l1/strokeextraction</a></li>
<li>paper_authors: Meng Li, Yahan Yu, Yi Yang, Guanghao Ren, Jian Wang</li>
<li>for: 提高中文字符识别和生成中的roke抽取精度</li>
<li>methods: 使用深度学习和 prior 信息，包括图像注册、图像semantic segmentation和高精度单roke抽取</li>
<li>results: 比基eline方法高效，可以准确抽取中文字符的rokeHere’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the accuracy of stroke extraction for Chinese characters, which is an important step in character recognition and generation.</li>
<li>methods: The proposed method uses deep learning and prior information to extract strokes. It consists of three parts: image registration-based stroke registration, image semantic segmentation-based stroke segmentation, and high-precision extraction of single strokes. The method uses a structure deformable image registration network to achieve structure-deformable transformation while maintaining the stable morphology of single strokes.</li>
<li>results: The experimental results show that the proposed method outperforms the baselines, demonstrating its effectiveness in stroke extraction for Chinese characters.<details>
<summary>Abstract</summary>
Stroke extraction of Chinese characters plays an important role in the field of character recognition and generation. The most existing character stroke extraction methods focus on image morphological features. These methods usually lead to errors of cross strokes extraction and stroke matching due to rarely using stroke semantics and prior information. In this paper, we propose a deep learning-based character stroke extraction method that takes semantic features and prior information of strokes into consideration. This method consists of three parts: image registration-based stroke registration that establishes the rough registration of the reference strokes and the target as prior information; image semantic segmentation-based stroke segmentation that preliminarily separates target strokes into seven categories; and high-precision extraction of single strokes. In the stroke registration, we propose a structure deformable image registration network to achieve structure-deformable transformation while maintaining the stable morphology of single strokes for character images with complex structures. In order to verify the effectiveness of the method, we construct two datasets respectively for calligraphy characters and regular handwriting characters. The experimental results show that our method strongly outperforms the baselines. Code is available at https://github.com/MengLi-l1/StrokeExtraction.
</details>
<details>
<summary>摘要</summary>
stroke extraction of Chinese characters plays an important role in the field of character recognition and generation. Most existing character stroke extraction methods focus on image morphological features, which often lead to errors in cross stroke extraction and stroke matching due to the lack of stroke semantics and prior information. In this paper, we propose a deep learning-based character stroke extraction method that considers stroke semantics and prior information. This method consists of three parts: image registration-based stroke registration, image semantic segmentation-based stroke segmentation, and high-precision extraction of single strokes. In the stroke registration, we propose a structure deformable image registration network to achieve structure-deformable transformation while maintaining the stable morphology of single strokes for character images with complex structures. To verify the effectiveness of the method, we construct two datasets respectively for calligraphy characters and regular handwriting characters. The experimental results show that our method significantly outperforms the baselines. Code is available at https://github.com/MengLi-l1/StrokeExtraction.Here's the translation breakdown:* roke extraction: roke抽取 (stroke extraction)* Chinese characters: 中文字符 (Chinese characters)* plays an important role: 扮演重要的角色 (plays an important role)* field of character recognition and generation: 字符识别和生成领域 (field of character recognition and generation)* most existing methods: 大多数现有方法 (most existing methods)* focus on image morphological features: 强调图像形态特征 (focus on image morphological features)* often lead to errors: 常常导致错误 (often lead to errors)* cross strokes extraction: 交叉roke抽取 (cross strokes extraction)* stroke matching: roke匹配 (stroke matching)* due to the lack of stroke semantics and prior information: 由于缺乏roke semantics和先前信息 (due to the lack of stroke semantics and prior information)* propose a deep learning-based method: 提出一种基于深度学习的方法 (propose a deep learning-based method)* consists of three parts: 分为三部分 (consists of three parts)* image registration-based stroke registration: 图像registratiooon基于stroke registration (image registration-based stroke registration)* image semantic segmentation-based stroke segmentation: 图像semantic segmentation基于stroke segmentation (image semantic segmentation-based stroke segmentation)* high-precision extraction of single strokes: 高精度的单roke抽取 (high-precision extraction of single strokes)* propose a structure deformable image registration network: 提出一种结构可变的图像registratiooon网络 (propose a structure deformable image registration network)* achieve structure-deformable transformation: 实现结构可变的变换 (achieve structure-deformable transformation)* maintaining the stable morphology of single strokes: 保持单roke的稳定形态 (maintaining the stable morphology of single strokes)* for character images with complex structures:  для字符图像 WITH complex structures (for character images with complex structures)* construct two datasets: 构建两个数据集 (construct two datasets)* respectively for calligraphy characters and regular handwriting characters: 分别为楷书和常规手写字符 (respectively for calligraphy characters and regular handwriting characters)* the experimental results show that our method strongly outperforms the baselines: 实验结果表明我们的方法明显超越基线 (the experimental results show that our method strongly outperforms the baselines)* Code is available at: 代码可以在https://github.com/MengLi-l1/StrokeExtraction上获取 (Code is available at)
</details></li>
</ul>
<hr>
<h2 id="Miriam-Exploiting-Elastic-Kernels-for-Real-time-Multi-DNN-Inference-on-Edge-GPU"><a href="#Miriam-Exploiting-Elastic-Kernels-for-Real-time-Multi-DNN-Inference-on-Edge-GPU" class="headerlink" title="Miriam: Exploiting Elastic Kernels for Real-time Multi-DNN Inference on Edge GPU"></a>Miriam: Exploiting Elastic Kernels for Real-time Multi-DNN Inference on Edge GPU</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04339">http://arxiv.org/abs/2307.04339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihe Zhao, Neiwen Ling, Nan Guan, Guoliang Xing</li>
<li>for: 多个深度学习神经网络（DNN）的同时运行，如自动驾驶和增强现实，需要考虑不同级别的实时性要求。然而，边缘GPU上进行多个DNN任务的协调仍然是一个未经研究的领域。</li>
<li>methods: Miriam是一个针对边缘GPU进行多个DNN任务协调的框架，包括两个主要组成部分：灵活kernel生成器和运行时动态kernel协调器。该框架支持混合承载重要任务。</li>
<li>results: 对于两个边缘GPU平台， Miriam可以提高系统吞吐量92%，同时仅增加了 less than 10%的延迟 overhead，对于关键任务。与状态对照基eline相比， Miriam可以提高系统性能。<details>
<summary>Abstract</summary>
Many applications such as autonomous driving and augmented reality, require the concurrent running of multiple deep neural networks (DNN) that poses different levels of real-time performance requirements. However, coordinating multiple DNN tasks with varying levels of criticality on edge GPUs remains an area of limited study. Unlike server-level GPUs, edge GPUs are resource-limited and lack hardware-level resource management mechanisms for avoiding resource contention. Therefore, we propose Miriam, a contention-aware task coordination framework for multi-DNN inference on edge GPU. Miriam consolidates two main components, an elastic-kernel generator, and a runtime dynamic kernel coordinator, to support mixed critical DNN inference. To evaluate Miriam, we build a new DNN inference benchmark based on CUDA with diverse representative DNN workloads. Experiments on two edge GPU platforms show that Miriam can increase system throughput by 92% while only incurring less than 10\% latency overhead for critical tasks, compared to state of art baselines.
</details>
<details>
<summary>摘要</summary>
很多应用程序，如自动驾驶和增强现实，需要同时运行多个深度神经网络（DNN），它们具有不同的实时性要求。然而，在边缘GPU上协调多个DNN任务的协调仍然是一个有限的研究领域。与服务器级GPU不同，边缘GPU具有资源限制，缺乏硬件层次资源管理机制，以避免资源竞争。因此，我们提出了 Miriam，一个协调感知的多DNN推理框架 для边缘GPU。Miriam包括两个主要组件：灵活kernel生成器和运行时动态kernel协调器，以支持混合重要DNN推理。为评估Miriam，我们创建了基于CUDA的多DNN推理测试 benchmark，包含多种代表性DNN工作负荷。实验结果表明，Miriam可以提高系统吞吐量92%，只带来少于10%的延迟增加，相比于状态 искусственный基eline。
</details></li>
</ul>
<hr>
<h2 id="Source-Aware-Embedding-Training-on-Heterogeneous-Information-Networks"><a href="#Source-Aware-Embedding-Training-on-Heterogeneous-Information-Networks" class="headerlink" title="Source-Aware Embedding Training on Heterogeneous Information Networks"></a>Source-Aware Embedding Training on Heterogeneous Information Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04336">http://arxiv.org/abs/2307.04336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tsai Hor Chan, Chi Ho Wong, Jiajun Shen, Guosheng Yin</li>
<li>for: Scalable Unsupervised Multi-Source Heterogeneous Information Network Embedding (SUMSHINE) is proposed to address the issue of distribution discrepancy among subgraphs in Heterogeneous Information Networks (HINs) from multiple sources.</li>
<li>methods: SUMSHINE uses a scalable unsupervised framework to align the embedding distributions among multiple sources of an HIN.</li>
<li>results: Experimental results on real-world datasets in a variety of downstream tasks validate the performance of SUMSHINE over the state-of-the-art heterogeneous information network embedding algorithms.Here’s the text in Simplified Chinese:</li>
<li>for: SUMSHINE 是为了解决多源 heterogeneous information network (HIN) 中各个子图的分布差异而提出的Scalable Unsupervised Multi-Source Heterogeneous Information Network Embedding方法。</li>
<li>methods: SUMSHINE 使用了一种可扩展的无监督框架，将多源 HIN 中各个子图的嵌入分布进行对齐。</li>
<li>results: 在实际世界数据集上，SUMSHINE 在多种下游任务中表现出色，比之前的多源 HIN 嵌入算法更高效。<details>
<summary>Abstract</summary>
Heterogeneous information networks (HINs) have been extensively applied to real-world tasks, such as recommendation systems, social networks, and citation networks. While existing HIN representation learning methods can effectively learn the semantic and structural features in the network, little awareness was given to the distribution discrepancy of subgraphs within a single HIN. However, we find that ignoring such distribution discrepancy among subgraphs from multiple sources would hinder the effectiveness of graph embedding learning algorithms. This motivates us to propose SUMSHINE (Scalable Unsupervised Multi-Source Heterogeneous Information Network Embedding) -- a scalable unsupervised framework to align the embedding distributions among multiple sources of an HIN. Experimental results on real-world datasets in a variety of downstream tasks validate the performance of our method over the state-of-the-art heterogeneous information network embedding algorithms.
</details>
<details>
<summary>摘要</summary>
各种不同类型的信息网络（HINs）已经广泛应用于实际任务中，如推荐系统、社交网络和引用网络。而现有的HIN表示学习方法可以有效地学习网络中的 semantic和结构特征。然而，我们发现忽略了不同来源的子图分布差异会妨碍图像学习算法的效果。这个问题 Motivates我们提出SUMSHINE（可扩展无监督多源多类信息网络嵌入）——一种可扩展的无监督框架，用于对多个来源的HIN嵌入的对齐。实际实验结果表明，我们的方法在多种实际任务中表现更好于现有的多源多类信息网络嵌入算法。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Adversarial-Robustness-via-Score-Based-Optimization"><a href="#Enhancing-Adversarial-Robustness-via-Score-Based-Optimization" class="headerlink" title="Enhancing Adversarial Robustness via Score-Based Optimization"></a>Enhancing Adversarial Robustness via Score-Based Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04333">http://arxiv.org/abs/2307.04333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boya Zhang, Weijian Luo, Zhihua Zhang</li>
<li>for: 防止深度神经网络分类器受到攻击，提高人工智能的安全性。</li>
<li>methods: 使用分布模型进行防御，在测试时优化抗击amples，以循着分数based的先验知识向原始的干净数据方向进行优化。</li>
<li>results: 在多个dataset上，包括CIFAR10、CIFAR100和ImageNet，OUR方法比既有的抗击方法更高效，同时也能够提高鲁棒性和推理速度。<details>
<summary>Abstract</summary>
Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.
</details>
<details>
<summary>摘要</summary>
深度神经网络分类器面临着敌意攻击的威胁，这些攻击可以通过小量的修改引起分类器的误差。为确保人工智能的安全使用，开发有效的防御策略是非常重要。 latest studies suggest that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results.在本文中，我们提出了一种新的防御方案，即 ScoreOpt，它在测试时对原始的干净数据进行优化，以遵循分布式优化的方向。我们在多个数据集上进行了广泛的实验，包括CIFAR10、CIFAR100和ImageNet。我们的实验结果表明，我们的方法在鲁棒性性能和推理速度两个方面都超过了现有的防御策略。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Generate-Equitable-Text-in-Dialogue-from-Biased-Training-Data"><a href="#Learning-to-Generate-Equitable-Text-in-Dialogue-from-Biased-Training-Data" class="headerlink" title="Learning to Generate Equitable Text in Dialogue from Biased Training Data"></a>Learning to Generate Equitable Text in Dialogue from Biased Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04303">http://arxiv.org/abs/2307.04303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anthonysicilia/equitable-dialogue-acl2023">https://github.com/anthonysicilia/equitable-dialogue-acl2023</a></li>
<li>paper_authors: Anthony Sicilia, Malihe Alikhani</li>
<li>for: 这个论文主要研究了对话系统决策过程中嵌入的公平原则的影响，以及这些原则如何影响用户参与度、满意度和任务完成度。</li>
<li>methods: 该论文使用计算学习理论来研究公平文本生成问题，提供了公平文本生成的正式定义，并证明了学习人类化和学习公平之间的正式联系。</li>
<li>results: 该论文通过empirical测试验证了其理论，并证明了一些算法在生成公平文本时的性能。<details>
<summary>Abstract</summary>
The ingrained principles of fairness in a dialogue system's decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system. For example, misusing pronouns in a user interaction may cause ambiguity about the intended subject. Yet, there is no comprehensive study of equitable text generation in dialogue. Aptly, in this work, we use theories of computational learning to study this problem. We provide formal definitions of equity in text generation, and further, prove formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data). With this insight, we also formulate reasonable conditions under which text generation algorithms can learn to generate equitable text without any modifications to the biased training data on which they learn. To exemplify our theory in practice, we look at a group of algorithms for the GuessWhat?! visual dialogue game and, using this example, test our theory empirically. Our theory accurately predicts relative-performance of multiple algorithms in generating equitable text as measured by both human and automated evaluation.
</details>
<details>
<summary>摘要</summary>
“对话系统决策过程中嵌入的公平原则是关键，以确保用户参与度、满意度和任务完成度。缺乏公平和包容的原则可能导致共同基础的形成受阻，从而影响整体系统的性能。例如，在用户交互中错误使用代名词可能导致对象的涉及性 ambiguity。然而，现有的研究中没有全面研究对话系统的公平文本生成。在这种情况下，我们使用计算学理论来研究这个问题。我们提供了对equity in text generation的正式定义，并证明了学习人类化的算法实际上可以降低到学习公平的算法（在增强数据上）。这个发现使我们还能提出了可行的条件，即文本生成算法可以在不修改恶势所训练数据上学习生成公平文本。为了证明我们的理论在实践中的正确性，我们选择了一些GuessWhat?!视觉对话游戏中的算法，并使用这个例子进行实验。我们的理论准确预测了多种算法在生成公平文本方面的相对性能，并被人工和自动评估都验证。”
</details></li>
</ul>
<hr>
<h2 id="A-Demand-Driven-Perspective-on-Generative-Audio-AI"><a href="#A-Demand-Driven-Perspective-on-Generative-Audio-AI" class="headerlink" title="A Demand-Driven Perspective on Generative Audio AI"></a>A Demand-Driven Perspective on Generative Audio AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04292">http://arxiv.org/abs/2307.04292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangshin Oh, Minsung Kang, Hyeongi Moon, Keunwoo Choi, Ben Sangbae Chon</li>
<li>for: 本研究的目的是了解音频研究的产业需求，以便成功地应用人工智能技术。</li>
<li>methods: 本研究使用了访问专业音响工程师的问卷调查，以确定研究优先级和定义各种研究任务。</li>
<li>results: 调查显示当前音频质量和可控性的主要瓶颈是数据集的可用性，而且提供了一些解决这些问题的可能性。<details>
<summary>Abstract</summary>
To achieve successful deployment of AI research, it is crucial to understand the demands of the industry. In this paper, we present the results of a survey conducted with professional audio engineers, in order to determine research priorities and define various research tasks. We also summarize the current challenges in audio quality and controllability based on the survey. Our analysis emphasizes that the availability of datasets is currently the main bottleneck for achieving high-quality audio generation. Finally, we suggest potential solutions for some revealed issues with empirical evidence.
</details>
<details>
<summary>摘要</summary>
要成功部署人工智能研究，非常重要了解行业的需求。在这篇论文中，我们通过询问专业音频工程师，确定研究优先级和定义各种研究任务。我们还总结了现有数据的可用性是实现高质量音频生成的主要瓶颈。最后，我们提出了一些解决问题的可能性，并提供了实证证明。
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Graph-ODE-for-Learning-Complex-System-Dynamics-across-Environments"><a href="#Generalizing-Graph-ODE-for-Learning-Complex-System-Dynamics-across-Environments" class="headerlink" title="Generalizing Graph ODE for Learning Complex System Dynamics across Environments"></a>Generalizing Graph ODE for Learning Complex System Dynamics across Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04287">http://arxiv.org/abs/2307.04287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Huang, Yizhou Sun, Wei Wang</li>
<li>for: 学习多体系统动力学，特别是生物分子动力学等实际应用场景。</li>
<li>methods: 我们提出了一种机器学习框架，即通用图Ordinary Differential Equations（GG-ODE），用于学习连续多体系统动力学。我们使用图神经网络（GNNs）来parametrize neural ordinary differential equations（ODE），以捕捉多体系统中连续交互的特性。</li>
<li>results: 我们的模型可以准确预测系统动力学，特别是在远距离下。我们还发现我们的模型可以在新系统中很好地泛化，即使只有几个观察数据。<details>
<summary>Abstract</summary>
Learning multi-agent system dynamics has been extensively studied for various real-world applications, such as molecular dynamics in biology. Most of the existing models are built to learn single system dynamics from observed historical data and predict the future trajectory. In practice, however, we might observe multiple systems that are generated across different environments, which differ in latent exogenous factors such as temperature and gravity. One simple solution is to learn multiple environment-specific models, but it fails to exploit the potential commonalities among the dynamics across environments and offers poor prediction results where per-environment data is sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary Differential Equations), a machine learning framework for learning continuous multi-agent system dynamics across environments. Our model learns system dynamics using neural ordinary differential equations (ODE) parameterized by Graph Neural Networks (GNNs) to capture the continuous interaction among agents. We achieve the model generalization by assuming the dynamics across different environments are governed by common physics laws that can be captured via learning a shared ODE function. The distinct latent exogenous factors learned for each environment are incorporated into the ODE function to account for their differences. To improve model performance, we additionally design two regularization losses to (1) enforce the orthogonality between the learned initial states and exogenous factors via mutual information minimization; and (2) reduce the temporal variance of learned exogenous factors within the same system via contrastive learning. Experiments over various physical simulations show that our model can accurately predict system dynamics, especially in the long range, and can generalize well to new systems with few observations.
</details>
<details>
<summary>摘要</summary>
学习多体系统动力学已经广泛研究了多种实际应用，如生物分子动力学。大多数现有模型都是从观察历史数据学习单个系统动力学并预测未来轨迹。然而，在实践中，我们可能会观察到不同环境中生成的多个系统，这些系统之间的潜在因素不同，如温度和重力。一种简单的解决方案是学习每个环境专门的模型，但这会忽略系统动力学之间的共同特性并提供差异环境数据罕见或有限的预测结果。这里，我们提出了GG-ODE（普适图 ordinary differential equations）机器学习框架，用于学习连续多体系统动力学 across environments。我们的模型通过使用图ael neural ordinary differential equations（GNNs）参数化的神经网络学习系统动力学，以捕捉连续间的代理人之间的交互。我们实现模型通用性 by assuming不同环境的动力学都受到同一个物理法则管理，可以通过学习共享的 ODE 函数来捕捉这些法则。每个环境的特性学习的潜在隐藏因素被 incorporated into the ODE function 以考虑它们的差异。为了提高模型性能，我们还设计了两种 regularization loss 来（1）使得学习的初始状态和隐藏因素彼此正交via 信息减少Minimization; 和（2）在同一个系统中减少学习的隐藏因素的时间变差via 对比学习。在多种物理 simulations 中，我们的模型可以准确预测系统动力学，特别是在长距离下，并可以良好地通用到新系统中。
</details></li>
</ul>
<hr>
<h2 id="Cloud-Render-Farm-Services-Discovery-Using-NLP-And-Ontology-Based-Knowledge-Graph"><a href="#Cloud-Render-Farm-Services-Discovery-Using-NLP-And-Ontology-Based-Knowledge-Graph" class="headerlink" title="Cloud Render Farm Services Discovery Using NLP And Ontology Based Knowledge Graph"></a>Cloud Render Farm Services Discovery Using NLP And Ontology Based Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13604">http://arxiv.org/abs/2307.13604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruby Annette, Aisha Banu, Sharon Priya, Subash Chandran</li>
<li>for: 该研究旨在提供一个基于 ontology 的云 render farm 服务发现引擎，以便更好地找到符合项目需求的云 render farm 服务。</li>
<li>methods: 该研究使用了知识基础 reasoning 算法，包括概念相似度理解、等价理解和数值相似度理解，来确定云 render farm 服务之间的相似性。</li>
<li>results: 研究发现，使用 ontology 基于的服务发现引擎可以更好地找到符合项目需求的云 render farm 服务，并且比不使用 ontology 和使用通用搜索引擎更高效。<details>
<summary>Abstract</summary>
Cloud render farm services are the animation domain specific cloud services Platform-as-a-Service (PaaS) type of cloud services that provides a complete platform to render the animation files. However, identifying the render farm services that is cost effective and also matches the functional requirements that changes for almost every project like the animation software, plug-ins required etc., is a challenge. This research work proposes an ontology-based service discovery engine named RenderSelect for the cloud render farm services. The cloud render farm ontology semantically defines the relationship among the cloud render farm services. The knowledge-based reasoning algorithms namely, the Concept similarity reasoning, Equivalent reasoning and the Numerical similarity reasoning have been applied to determine the similarity among the cloud services. The service discovery engine was evaluated for finding the services under three different scenarios namely a) with help of the ontology, b) without the help of the ontology and c) using a common search engine on the internet. The results show that the proposed service discovery engine which is specifically designed for the cloud render farm services using the ontology performs significantly better than the other two.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate text into Simplified Chinese云 render farm 服务是动画领域专门的云服务平台协议（PaaS）类云服务，提供了完整的平台来渲染动画文件。然而，确定云 render farm 服务的成本效果和匹配动画软件、插件等功能要求的挑战。这项研究工作提出了基于 ontology 的服务发现引擎 named RenderSelect  для云 render farm 服务。云 render farm ontology Semantically defines the relationship among the cloud render farm services。使用知识库reasoning算法，包括概念相似理解、相等理解和数值相似理解，对云服务进行相似性判断。服务发现引擎在三个不同的场景下进行测试，分别是：a) 使用 ontology，b) 无ontology和c) 使用互联网搜索引擎。结果表明，特定 для云 render farm 服务使用 ontology 的服务发现引擎在三个场景下表现出显著优异性。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="RidgeBase-A-Cross-Sensor-Multi-Finger-Contactless-Fingerprint-Dataset"><a href="#RidgeBase-A-Cross-Sensor-Multi-Finger-Contactless-Fingerprint-Dataset" class="headerlink" title="RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset"></a>RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05563">http://arxiv.org/abs/2307.05563</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bhavinjawade/RidgeBase_Fingerprint_Camera_App">https://github.com/bhavinjawade/RidgeBase_Fingerprint_Camera_App</a></li>
<li>paper_authors: Bhavin Jawade, Deen Dayal Mohan, Srirangaraj Setlur, Nalini Ratha, Venu Govindaraju</li>
<li>for: 本研究旨在提高无接触指纹识别系统的实用性和可靠性，并且提供一个大规模的实际数据集来推动此类研究。</li>
<li>methods: 本研究使用了智能手机摄像头捕获的无接触指纹图像，并提出了一种基于多图像匹配的集成匹配协议，以提高无接触指纹识别的精度和可靠性。</li>
<li>results: 研究人员通过对RidgeBase数据集进行实验，发现该协议可以减少无接触指纹识别中的灵活性和背景影响，并且可以提高识别率和准确率。<details>
<summary>Abstract</summary>
Contactless fingerprint matching using smartphone cameras can alleviate major challenges of traditional fingerprint systems including hygienic acquisition, portability and presentation attacks. However, development of practical and robust contactless fingerprint matching techniques is constrained by the limited availability of large scale real-world datasets. To motivate further advances in contactless fingerprint matching across sensors, we introduce the RidgeBase benchmark dataset. RidgeBase consists of more than 15,000 contactless and contact-based fingerprint image pairs acquired from 88 individuals under different background and lighting conditions using two smartphone cameras and one flatbed contact sensor. Unlike existing datasets, RidgeBase is designed to promote research under different matching scenarios that include Single Finger Matching and Multi-Finger Matching for both contactless- to-contactless (CL2CL) and contact-to-contactless (C2CL) verification and identification. Furthermore, due to the high intra-sample variance in contactless fingerprints belonging to the same finger, we propose a set-based matching protocol inspired by the advances in facial recognition datasets. This protocol is specifically designed for pragmatic contactless fingerprint matching that can account for variances in focus, polarity and finger-angles. We report qualitative and quantitative baseline results for different protocols using a COTS fingerprint matcher (Verifinger) and a Deep CNN based approach on the RidgeBase dataset. The dataset can be downloaded here: https://www.buffalo.edu/cubs/research/datasets/ridgebase-benchmark-dataset.html
</details>
<details>
<summary>摘要</summary>
contactless指纹匹配使用智能手机摄像头可以解决传统指纹系统中的主要挑战，包括卫生性获取、可搬性和演示攻击。然而，实现有用和可靠的无接触指纹匹配技术的发展受到有限的大规模实际数据的限制。为了激励更多的无接触指纹匹配技术的发展，我们介绍了RidgeBase数据集。RidgeBase包含了 более于15,000个无接触和接触基本指纹图像对，从88名个体下获得的不同背景和照明条件下的两个智能手机摄像头和一个平板式接触感测器中获得。与现有数据集不同的是，RidgeBase是设计用于促进不同匹配enario，包括单指匹配和多指匹配，以及CL2CL和C2CL验证和识别。此外，由于无接触指纹图像中的同一个手指之间的高内样异，我们提议一种基于facial recognition数据集的集合匹配协议。这种协议特别适用于实用无接触指纹匹配，可以考虑封闭、极性和手刃角的变化。我们在RidgeBase数据集上运行了一种COTS指纹匹配器（Verifinger）和一种深度 CNN 基于方法的基线结果。数据集可以在以下链接下下载：https://www.buffalo.edu/cubs/research/datasets/ridgebase-benchmark-dataset.html。
</details></li>
</ul>
<hr>
<h2 id="The-Future-of-Fundamental-Science-Led-by-Generative-Closed-Loop-Artificial-Intelligence"><a href="#The-Future-of-Fundamental-Science-Led-by-Generative-Closed-Loop-Artificial-Intelligence" class="headerlink" title="The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence"></a>The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07522">http://arxiv.org/abs/2307.07522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hector Zenil, Jesper Tegnér, Felipe S. Abrahão, Alexander Lavin, Vipin Kumar, Jeremy G. Frey, Adrian Weller, Larisa Soldatova, Alan R. Bundy, Nicholas R. Jennings, Koichi Takahashi, Lawrence Hunter, Saso Dzeroski, Andrew Briggs, Frederick D. Gregory, Carla P. Gomes, Christopher K. I. Williams, Jon Rowe, James Evans, Hiroaki Kitano, Joshua B. Tenenbaum, Ross King</li>
<li>for: The paper explores the potential of AI-driven automation in scientific discovery, particularly in fundamental deep science, and aims to mitigate current problems in the scientific process such as replication of findings and systematic production of data.</li>
<li>methods: The paper proposes an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space.</li>
<li>results: The paper holds the promise to unleash AI’s potential for searching and discovering the fundamental structure of our world beyond what human scientists have been able to achieve, and could open doors for technological innovation to tackle some of the greatest challenges facing humanity today.<details>
<summary>Abstract</summary>
Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Integrating AI-driven automation into the practice of science would mitigate current problems, including the replication of findings, systematic production of data, and ultimately democratisation of the scientific process. Realising these possibilities requires a vision for augmented AI coupled with a diversity of AI approaches able to deal with fundamental aspects of causality analysis and model discovery while enabling unbiased search across the space of putative explanations. These advances hold the promise to unleash AI's potential for searching and discovering the fundamental structure of our world beyond what human scientists have been able to achieve. Such a vision would push the boundaries of new fundamental science rather than automatize current workflows and instead open doors for technological innovation to tackle some of the greatest challenges facing humanity today.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ChatGPT-in-the-Age-of-Generative-AI-and-Large-Language-Models-A-Concise-Survey"><a href="#ChatGPT-in-the-Age-of-Generative-AI-and-Large-Language-Models-A-Concise-Survey" class="headerlink" title="ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey"></a>ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04251">http://arxiv.org/abs/2307.04251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iamgmujtaba/scholar_search">https://github.com/iamgmujtaba/scholar_search</a></li>
<li>paper_authors: Salman Mohamadi, Ghulam Mujtaba, Ngan Le, Gianfranco Doretto, Donald A. Adjeroh<br>for: 这个论文的主要目标是为了提供一份简洁的论文来概括现代 chatGPT 的研究进展和演化。methods: 本论文使用了两种视角来研究 chatGPT：玻璃盒视角（glass box view）和黑盒视角（black box view）。玻璃盒视角旨在理解技术的内部结构和工作机制，而黑盒视角则是将技术视为一个复杂系统，研究其输入、输出和影响。results: 本论文提供了一个全面的概述，涵盖了 chatGPT 的组件和基础元素，以及其应用、影响和意义。文章还提供了关于 LLM 和 GAI 的基本文献，并评估了现有和缺失的研究方向。最后，文章还探讨了 chatGPT 在不同领域的广泛应用和重要问题。<details>
<summary>Abstract</summary>
ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs, outputs, and effects. This paves the way for a comprehensive exploration of the technology and provides a road map for further research and experimentation. We also lay out essential foundational literature on LLMs and GAI in general and their connection with ChatGPT. This overview sheds light on existing and missing research lines in the emerging field of LLMs, benefiting both public users and developers. Furthermore, the paper delves into the broad spectrum of applications and significant concerns in fields such as education, research, healthcare, finance, etc.
</details>
<details>
<summary>摘要</summary>
chatGPT是一个大型自然语言模型（LLM），由OpenAI公司认真训练了大量数据。它已经革命化了自然语言处理（NLP）领域，并推动了LLM能力的边缘。chatGPT使得大规模的人工智能生成（GAI）与公众进行交互，并且激发了研究人员开发类似技术的兴趣，以及对其应用和影响的研究。在这篇论文中，我们的主要目标是为chatGPT的当前研究进行简要的报告。我们包括了glass box和black box两种视角，涵盖了技术的组件和基础元素，以及其应用、影响和意义。glass box方法强调了技术的内部工作，而black box方法则是视chatGPT为一个复杂系统，从输入、输出和效果的角度进行研究。这种方法使得我们可以全面探讨技术，并为未来的研究和实验提供路线图。我们还提供了LLM和GAI的基础文献，这些文献 shed light on存在和缺失的研究方向，这有助于公众和开发者。此外，论文还探讨了LLM在教育、研究、医疗、金融等领域的广泛应用和重要问题。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Pipeline-for-Improving-Optical-Character-Recognition-through-Post-processing-Using-Natural-Language-Processing"><a href="#A-Novel-Pipeline-for-Improving-Optical-Character-Recognition-through-Post-processing-Using-Natural-Language-Processing" class="headerlink" title="A Novel Pipeline for Improving Optical Character Recognition through Post-processing Using Natural Language Processing"></a>A Novel Pipeline for Improving Optical Character Recognition through Post-processing Using Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04245">http://arxiv.org/abs/2307.04245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aishik Rakshit, Samyak Mehta, Anirban Dasgupta</li>
<li>for: 这篇论文是用于提高 Optical Character Recognition (OCR) 技术的精度，特别是对于手写文本和印刷文本的应用。</li>
<li>methods: 该论文使用了 OCR 技术，并将其与自然语言处理 (NLP) 技术结合，以提高 OCR 的精度。</li>
<li>results: 该论文提出了一个端到端管道，首先使用 OCR 技术将手写或印刷文本转化为电子文本，然后使用 NLP 技术进行后处理，以提高 OCR 的精度。<details>
<summary>Abstract</summary>
Optical Character Recognition (OCR) technology finds applications in digitizing books and unstructured documents, along with applications in other domains such as mobility statistics, law enforcement, traffic, security systems, etc. The state-of-the-art methods work well with the OCR with printed text on license plates, shop names, etc. However, applications such as printed textbooks and handwritten texts have limited accuracy with existing techniques. The reason may be attributed to similar-looking characters and variations in handwritten characters. Since these issues are challenging to address with OCR technologies exclusively, we propose a post-processing approach using Natural Language Processing (NLP) tools. This work presents an end-to-end pipeline that first performs OCR on the handwritten or printed text and then improves its accuracy using NLP.
</details>
<details>
<summary>摘要</summary>
оптическое характерное признание (OCR)技术在数字化书籍和无结构文档方面找到了应用，同时还有其他领域的应用，如交通统计、刑事调查、安全系统等。现状的方法在处理OCR打印文本的许多应用场景中表现良好，但是印刷书籍和手写文本的准确率受限，主要是因为类似的字符和手写文本中的变化。这些问题难以通过OCR技术alone解决，因此我们提议一种基于自然语言处理（NLP）的后处理方法。本文介绍了一个端到端管道，首先使用OCR技术将手写或印刷文本转化为电子文档，然后使用NLP工具进行后处理，以提高准确率。
</details></li>
</ul>
<hr>
<h2 id="TransPose-A-Transformer-based-6D-Object-Pose-Estimation-Network-with-Depth-Refinement"><a href="#TransPose-A-Transformer-based-6D-Object-Pose-Estimation-Network-with-Depth-Refinement" class="headerlink" title="TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement"></a>TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05561">http://arxiv.org/abs/2307.05561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Abdulsalam, Nabil Aouf</li>
<li>for: 本研究旨在提高视觉基于6DoF姿态估计的精度，以便在自动化操作中提高机器人抓取应用的精度。</li>
<li>methods: 本文提出了一种基于Transformer的6DoF姿态估计方法，包括一个新的深度估计网络和一个改进的推测网络。</li>
<li>results: 对比其他文献方法，本研究的结果表明，该方法在果园抓取应用中的精度明显高于其他方法。<details>
<summary>Abstract</summary>
As demand for robotics manipulation application increases, accurate vision-based 6D pose estimation becomes essential for autonomous operations. Convolutional Neural Networks (CNNs) based approaches for pose estimation have been previously introduced. However, the quest for better performance still persists especially for accurate robotics manipulation. This quest extends to the Agri-robotics domain. In this paper, we propose TransPose, an improved Transformer-based 6D pose estimation with a depth refinement module. The architecture takes in only an RGB image as input with no additional supplementing modalities such as depth or thermal images. The architecture encompasses an innovative lighter depth estimation network that estimates depth from an RGB image using feature pyramid with an up-sampling method. A transformer-based detection network with additional prediction heads is proposed to directly regress the object's centre and predict the 6D pose of the target. A novel depth refinement module is then used alongside the predicted centers, 6D poses and depth patches to refine the accuracy of the estimated 6D pose. We extensively compared our results with other state-of-the-art methods and analysed our results for fruit-picking applications. The results we achieved show that our proposed technique outperforms the other methods available in the literature.
</details>
<details>
<summary>摘要</summary>
随着 роботи库操作应用的需求增加，精准视觉基于6D姿态估计变得非常重要。先前，基于卷积神经网络（CNN）的 pose estimation 方法已经得到了推出。然而，为了更好地实现自动化操作，特别是在农业机器人领域，仍然存在寻求更高的性能。在这篇论文中，我们提出了 TransPose，一种改进的 transformer 基于 6D 姿态估计方法。该架构只接受 RGB 图像作为输入，不需要附加的补充Modalities 如深度或热力图像。架构包括一个创新的轻量级深度估计网络，该网络使用特征层次来估计 RGB 图像中的深度，并使用更正方法进行升采样。我们还提出了一个基于 transformer 的探测网络，该网络直接将目标对象的中心点和6D 姿态估计作为输出。最后，我们提出了一种新的深度修正模块，该模块与预测的中心点、6D 姿态和深度裁剪图像一起，来修正估计的6D姿态的准确性。我们对其他文献中的结果进行了广泛比较，并对果物摘取应用进行了分析。我们所得到的结果显示，我们的提出的技术超过了现有文献中的方法。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Human-Detection-in-Fire-Scenarios-using-Infrared-and-Thermal-Imaging-Fusion"><a href="#Real-time-Human-Detection-in-Fire-Scenarios-using-Infrared-and-Thermal-Imaging-Fusion" class="headerlink" title="Real-time Human Detection in Fire Scenarios using Infrared and Thermal Imaging Fusion"></a>Real-time Human Detection in Fire Scenarios using Infrared and Thermal Imaging Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04223">http://arxiv.org/abs/2307.04223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Truong-Dong Do, Nghe-Nhan Truong, My-Ha Le</li>
<li>for: 提高人员搜救效率，使用视觉基于人体检测系统在低可见性enario中提高生存机会。</li>
<li>methods: 利用多个摄像头捕获图像，通过推热和红外混合技术，提取有用的特征进行人体检测。</li>
<li>results: 实验结果显示，提posed方法可以在理想的速度下处理，并在0.5MAP中达到95%的表现。<details>
<summary>Abstract</summary>
Fire is considered one of the most serious threats to human lives which results in a high probability of fatalities. Those severe consequences stem from the heavy smoke emitted from a fire that mostly restricts the visibility of escaping victims and rescuing squad. In such hazardous circumstances, the use of a vision-based human detection system is able to improve the ability to save more lives. To this end, a thermal and infrared imaging fusion strategy based on multiple cameras for human detection in low-visibility scenarios caused by smoke is proposed in this paper. By processing with multiple cameras, vital information can be gathered to generate more useful features for human detection. Firstly, the cameras are calibrated using a Light Heating Chessboard. Afterward, the features extracted from the input images are merged prior to being passed through a lightweight deep neural network to perform the human detection task. The experiments conducted on an NVIDIA Jetson Nano computer demonstrated that the proposed method can process with reasonable speed and can achieve favorable performance with a mAP@0.5 of 95%.
</details>
<details>
<summary>摘要</summary>
火是人类生命最严重的威胁之一，可能导致高probability of fatalities。这些严重的后果来自于大量烟雾所带来的视线受阻，使得逃生者和救援队伍的视线受到了严重限制。在这种危险情况下，使用视觉基本人体检测系统可以提高生存的可能性。因此，本文提出了一种基于多个摄像头的热成像和 инфра红成像融合策略，用于人体检测在低视野情况下。通过处理多个摄像头的输入图像，可以收集更多的有用特征进行人体检测。首先，摄像头被准确使用一个光热棋盘进行准确。然后，输入图像中提取的特征被合并并传递给一个轻量级深度神经网络进行人体检测任务。实验结果表明，提议的方法可以在合理的速度下进行处理，并且可以在0.5的MAP值上达到95%的表现。
</details></li>
</ul>
<hr>
<h2 id="LakeBench-Benchmarks-for-Data-Discovery-over-Data-Lakes"><a href="#LakeBench-Benchmarks-for-Data-Discovery-over-Data-Lakes" class="headerlink" title="LakeBench: Benchmarks for Data Discovery over Data Lakes"></a>LakeBench: Benchmarks for Data Discovery over Data Lakes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04217">http://arxiv.org/abs/2307.04217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kavitha Srinivas, Julian Dolby, Ibrahim Abdelaziz, Oktie Hassanzadeh, Harsha Kokel, Aamod Khatiwada, Tejaswini Pedapati, Subhajit Chaudhury, Horst Samulowitz</li>
<li>for: 本研究旨在提供数据湖中数据发现的基准测试集，以便用于评估不同的表Foundational模型在数据发现任务中的表现。</li>
<li>methods: 本研究使用了多种数据源，包括政府数据平台CKAN、Socrata和欧洲中央银行，从而构建了多个 benchmark tasks。然后，对4种公共可用的表Foundational模型进行了比较性evaluation。</li>
<li>results: 研究发现，现有的表Foundational模型尚未被专门用于数据发现任务的训练，其表现表现出了明显的改进空间。结果建议，建立这些基准测试集可能对社区有所帮助，以建立适用于数据湖中数据发现的表Foundational模型。<details>
<summary>Abstract</summary>
Within enterprises, there is a growing need to intelligently navigate data lakes, specifically focusing on data discovery. Of particular importance to enterprises is the ability to find related tables in data repositories. These tables can be unionable, joinable, or subsets of each other. There is a dearth of benchmarks for these tasks in the public domain, with related work targeting private datasets. In LakeBench, we develop multiple benchmarks for these tasks by using the tables that are drawn from a diverse set of data sources such as government data from CKAN, Socrata, and the European Central Bank. We compare the performance of 4 publicly available tabular foundational models on these tasks. None of the existing models had been trained on the data discovery tasks that we developed for this benchmark; not surprisingly, their performance shows significant room for improvement. The results suggest that the establishment of such benchmarks may be useful to the community to build tabular models usable for data discovery in data lakes.
</details>
<details>
<summary>摘要</summary>
在企业中，有一个增长的需求是智能导航数据湖，尤其是数据发现。企业中对于找到相关的表格在数据存储中的能力是非常重要。这些表格可能是可合并的、可连接的或彼此之间的子集。现有的公共领域中没有很多对这些任务的标准准则，相关的工作都是针对私有数据集进行的。在 LakeBench 中，我们开发了多个对这些任务的标准准则，使用从多种数据源中抽出的多个表格，如政府数据平台 CKAN、Socrata 和欧洲中央银行。我们对这些任务的表现进行了比较，发现现有的公共可用基础模型中没有任何一个已经在这些任务上进行过训练。结果表明，建立这些准则可能对社区有用，以建立适用于数据发现的表格模型。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Autoencoder-based-Lossy-Compression-for-Large-scale-High-resolution-Scientific-Data"><a href="#Hierarchical-Autoencoder-based-Lossy-Compression-for-Large-scale-High-resolution-Scientific-Data" class="headerlink" title="Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data"></a>Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04216">http://arxiv.org/abs/2307.04216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieutrungle/data-slim">https://github.com/hieutrungle/data-slim</a></li>
<li>paper_authors: Hieu Le, Hernan Santos, Jian Tao</li>
<li>for: 大规模科学数据压缩</li>
<li>methods: 使用自适应神经网络进行压缩</li>
<li>results: 实现了高压缩率并保持高重建质量Here’s a more detailed explanation of each point:</li>
<li>for: The paper is focused on compressing large-scale scientific data, which is a growing challenge in many domains. The authors propose a neural network-based approach to address this issue.</li>
<li>methods: The proposed method uses an Autoencoder-based neural network to compress the data. The network is trained to reconstruct the original data from the compressed representation, allowing for efficient compression and reconstruction.</li>
<li>results: The authors test their method on several benchmark data sets and achieve a high compression ratio (140) without compromising the reconstruction quality. They also apply the method to a large-scale high-resolution climate modeling data set and achieve a compression ratio of 200 with negligible reconstruction error.<details>
<summary>Abstract</summary>
Lossy compression has become an important technique to reduce data size in many domains. This type of compression is especially valuable for large-scale scientific data, whose size ranges up to several petabytes. Although Autoencoder-based models have been successfully leveraged to compress images and videos, such neural networks have not widely gained attention in the scientific data domain. Our work presents a neural network that not only significantly compresses large-scale scientific data but also maintains high reconstruction quality. The proposed model is tested with scientific benchmark data available publicly and applied to a large-scale high-resolution climate modeling data set. Our model achieves a compression ratio of 140 on several benchmark data sets without compromising the reconstruction quality. Simulation data from the High-Resolution Community Earth System Model (CESM) Version 1.3 over 500 years are also being compressed with a compression ratio of 200 while the reconstruction error is negligible for scientific analysis.
</details>
<details>
<summary>摘要</summary>
丢弃性压缩已成为许多领域中减少数据大小的重要技术。特别是在大规模科学数据领域，数据的大小可以达到几百亿字节级别。虽然基于自适应神经网络的模型已成功应用于图像和视频压缩，但这些神经网络在科学数据领域并未受到广泛关注。我们的工作推出了一种可以高效压缩大规模科学数据，同时保持高重建质量的神经网络模型。我们的模型在公共可用的科学数据 benchmark 上进行测试，并应用于大规模高分辨率气候模拟数据集。我们的模型在多个 benchmark 数据集上实现了压缩率为140，而且重建质量仍然保持在高水平。同时，我们还对高分辨率地球系统模型（CESM）版本1.3的500年大量数据进行压缩，压缩率达200，而且重建错误几乎可以忽略不计。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/10/cs.AI_2023_07_10/" data-id="clon21ii8000br588ajskf1jk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/10/cs.CL_2023_07_10/" class="article-date">
  <time datetime="2023-07-10T11:00:00.000Z" itemprop="datePublished">2023-07-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/10/cs.CL_2023_07_10/">cs.CL - 2023-07-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="BeaverTails-Towards-Improved-Safety-Alignment-of-LLM-via-a-Human-Preference-Dataset"><a href="#BeaverTails-Towards-Improved-Safety-Alignment-of-LLM-via-a-Human-Preference-Dataset" class="headerlink" title="BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"></a>BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04657">http://arxiv.org/abs/2307.04657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, Yaodong Yang</li>
<li>for: 这个论文旨在鼓励大语言模型（LLM）的安全对齐研究，提供了一个名为BeaverTails的数据集。</li>
<li>methods: 这个数据集独特地将问答对的注释分为帮助性和无害性两个方面，因此可以为这两个关键特征提供多个视角。</li>
<li>results: 作者们编译了30,207个问答对的安全元标和30,144个专家对比数据，并在内容审核和人工反馈学习（RLHF）中应用了BeaverTails，证明了其在LLM的实际安全措施方面的潜在价值。<details>
<summary>Abstract</summary>
In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了BeaverTails数据集，用于推动大语言模型（LLM）的安全对齐研究。这个数据集独特地将问答对的有用性和无害性标注分开，因此可以提供不同的视角。总共，我们对30,207个问答对进行了安全元标注，并收集了30,144对专家比较数据，以便在有用性和无害性metric上进行评估。我们还展示了在内容审核和人工回馈学习（RLHF）中使用BeaverTails的应用，强调其在LLM的安全实施中的潜在价值。我们认为这个数据集为研究者提供了重要的资源，帮助开发和部署LLM的安全。项目页面的URL为：https://sites.google.com/view/pku-beavertails。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Lexical-Diversity-in-Texts-The-Twofold-Length-Problem"><a href="#Measuring-Lexical-Diversity-in-Texts-The-Twofold-Length-Problem" class="headerlink" title="Measuring Lexical Diversity in Texts: The Twofold Length Problem"></a>Measuring Lexical Diversity in Texts: The Twofold Length Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04626">http://arxiv.org/abs/2307.04626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yves Bestgen</li>
<li>for: 评估语言学习者文本 lexical diversity 的方法和问题</li>
<li>methods: 使用probabilistic或algorithmic方法来缩短文本长度，但都未能解决第二个问题：敏感度参数的影响</li>
<li>results: 三个英语语言学习者文本 dataset 的分析显示，使用这些方法可以解决长度问题，但都未能解决第二个问题<details>
<summary>Abstract</summary>
The impact of text length on the estimation of lexical diversity has captured the attention of the scientific community for more than a century. Numerous indices have been proposed, and many studies have been conducted to evaluate them, but the problem remains. This methodological review provides a critical analysis not only of the most commonly used indices in language learning studies, but also of the length problem itself, as well as of the methodology for evaluating the proposed solutions. The analysis of three datasets of English language-learners' texts revealed that indices that reduce all texts to the same length using a probabilistic or an algorithmic approach solve the length dependency problem; however, all these indices failed to address the second problem, which is their sensitivity to the parameter that determines the length to which the texts are reduced. The paper concludes with recommendations for optimizing lexical diversity analysis.
</details>
<details>
<summary>摘要</summary>
Text length的影响对语言多样性的估计已经在科学社区内引起了超过一个世纪的关注。许多指标已经被提出，但问题仍然存在。本方法评论提供了不只是语言学研究中最常用的指标的重要分析，还包括长度问题本身以及评估提出的解决方案的方法学分析。分析三个英语学习者的文本数据表明，使用概率或算法方法减小所有文本到同一个长度可以解决长度依赖问题，但所有这些指标都无法解决第二个问题，即它们对参数的敏感性。文章结束于优化语言多样性分析的建议。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computational-Modeling-of-Meaning-Embodied-Cognition-Intertwined-with-Emotion"><a href="#On-the-Computational-Modeling-of-Meaning-Embodied-Cognition-Intertwined-with-Emotion" class="headerlink" title="On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion"></a>On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04518">http://arxiv.org/abs/2307.04518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Casey Kennington</li>
<li>for: 本文探讨了语言意义如何形成，尤其关注儿童语言学习。</li>
<li>methods: 本文使用了历史风格，汇总了作者在不同时间发现的想法，并描述了儿童语言学习的设置、身体知识和情感知识的重要性。</li>
<li>results: 本文提出了语言学习机器人需要满足的一些要求，以及对未来语言模型的建议。<details>
<summary>Abstract</summary>
This document chronicles this author's attempt to explore how words come to mean what they do, with a particular focus on child language acquisition and what that means for models of language understanding.\footnote{I say \emph{historical} because I synthesize the ideas based on when I discovered them and how those ideas influenced my later thinking.} I explain the setting for child language learning, how embodiment -- being able to perceive and enact in the world, including knowledge of concrete and abstract concepts -- is crucial, and how emotion and cognition relate to each other and the language learning process. I end with what I think are some of the requirements for a language-learning agent that learns language in a setting similar to that of children. This paper can act as a potential guide for ongoing and future work in modeling language.
</details>
<details>
<summary>摘要</summary>
这份文档记录作者对语言意义的探索，尤其是儿童语言学习的过程。作者提出了一些关于语言理解的模型，并在文档中解释了儿童语言学习的背景和embodiment的重要性。文档还讨论了情感和认知之间的关系，以及语言学习过程中的感知和行为。最后，作者提出了一些对于模拟语言学习的agent来说的需求。这份文档可以作为未来语言模型研究的指南。
</details></li>
</ul>
<hr>
<h2 id="Detecting-LLM-Generated-Text-in-Computing-Education-A-Comparative-Study-for-ChatGPT-Cases"><a href="#Detecting-LLM-Generated-Text-in-Computing-Education-A-Comparative-Study-for-ChatGPT-Cases" class="headerlink" title="Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases"></a>Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07411">http://arxiv.org/abs/2307.07411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Sheinman Orenstrakh, Oscar Karnalim, Carlos Anibal Suarez, Michael Liut</li>
<li>for: This paper aims to evaluate the effectiveness of eight publicly-available LLM-generated text detectors in detecting LLM-generated text in computer science submissions.</li>
<li>methods: The authors collected 124 submissions from computer science students and generated 40 ChatGPT submissions to evaluate the eight LLM-generated text detectors using accuracy, false positives, and resilience measures.</li>
<li>results: The results show that CopyLeaks is the most accurate LLM-generated text detector, GPTKit is the best LLM-generated text detector to reduce false positives, and GLTR is the most resilient LLM-generated text detector. However, the authors also note that all LLM-generated text detectors are less accurate with code, other languages, and after the use of paraphrasing tools.Here’s the same information in Simplified Chinese:</li>
<li>for: 这个论文目的是评估现有的8种公共可用LLM生成文本检测器在计算机科学提交中检测LLM生成文本的有效性。</li>
<li>methods: 作者收集了124名计算机科学学生的提交和生成40个ChatGPT提交来评估8种LLM生成文本检测器，使用准确率、假阳性和抗耗力三个指标评估。</li>
<li>results: 结果显示，CopyLeaks是LLM生成文本检测器中最准确的，GPTKit可以减少假阳性，而GLTR是最有抗耗力的LLM生成文本检测器。然而，作者还注意到，所有LLM生成文本检测器在代码、其他语言和使用篇章工具（如QuillBot）后都有减少准确性的问题。<details>
<summary>Abstract</summary>
Due to the recent improvements and wide availability of Large Language Models (LLMs), they have posed a serious threat to academic integrity in education. Modern LLM-generated text detectors attempt to combat the problem by offering educators with services to assess whether some text is LLM-generated. In this work, we have collected 124 submissions from computer science students before the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this data to evaluate eight publicly-available LLM-generated text detectors through the measures of accuracy, false positives, and resilience. The purpose of this work is to inform the community of what LLM-generated text detectors work and which do not, but also to provide insights for educators to better maintain academic integrity in their courses. Our results find that CopyLeaks is the most accurate LLM-generated text detector, GPTKit is the best LLM-generated text detector to reduce false positives, and GLTR is the most resilient LLM-generated text detector. We also express concerns over 52 false positives (of 114 human written submissions) generated by GPTZero. Finally, we note that all LLM-generated text detectors are less accurate with code, other languages (aside from English), and after the use of paraphrasing tools (like QuillBot). Modern detectors are still in need of improvements so that they can offer a full-proof solution to help maintain academic integrity. Further, their usability can be improved by facilitating a smooth API integration, providing clear documentation of their features and the understandability of their model(s), and supporting more commonly used languages.
</details>
<details>
<summary>摘要</summary>
因为最近的大语言模型（LLM）的改进和普遍可用性，它们对教育的学术 integrity 造成了严重的威胁。现代 LLM 生成文本检测器尝试通过为教师提供检测 LLM 生成文本的服务，以确保学术 integrity 的维护。在这项工作中，我们收集了 124 篇计算机科学学生的作业，然后生成了 40 篇 ChatGPT 作业。我们使用这些数据来评估 eight 个公共可用的 LLM 生成文本检测器，通过准确率、假阳性和抗耗能力三个指标进行评估。本研究的目的是通过检测器的评估，了解哪些 LLM 生成文本检测器效果好、哪些需要改进，以便为教育行业提供更好的学术 integrity 维护方案。我们的结果显示，CopyLeaks 是最准确的 LLM 生成文本检测器，GPTKit 是减少假阳性的最佳选择，GLTR 是最有抗耗能力的 LLM 生成文本检测器。此外，我们还发现 GPTZero 对 114 篇人工写作中的 52 个假阳性存在问题。最后，我们注意到所有 LLM 生成文本检测器都对代码、其他语言（除英语外）和使用副作业工具（如 QuillBot）后的文本准确率较低。现代检测器仍需进一步改进，以提供不可攻击的解决方案，并且可以提高使用者体验，例如通过简单的 API 集成、清晰的功能和模型文档、以及支持更常用的语言。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Biomedical-Text-Summarization-and-Question-Answering-On-the-Utility-of-Domain-Specific-Pre-Training"><a href="#Enhancing-Biomedical-Text-Summarization-and-Question-Answering-On-the-Utility-of-Domain-Specific-Pre-Training" class="headerlink" title="Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training"></a>Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04412">http://arxiv.org/abs/2307.04412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dima Galat, Marian-Andrei Rizoiu</li>
<li>for: 这个论文旨在解决生物医学摘要 generation 需要大量数据训练的问题。</li>
<li>methods: 论文使用了转移学习，并证明了在 BioASQ 摘要任务中，不一定需要域专的预训练。作者提出了一种适合模型体系，并在这种模型体系下进行了任务特定细化，从而提出了一种三步细化方法，只需要千个域内示例即可。</li>
<li>results: 研究结果表明，没有域专预训练的大语言模型在某些域pecific生物医学文本生成任务中可以具有显著优势。<details>
<summary>Abstract</summary>
Biomedical summarization requires large datasets to train for text generation. We show that while transfer learning offers a viable option for addressing this challenge, an in-domain pre-training does not always offer advantages in a BioASQ summarization task. We identify a suitable model architecture and use it to show a benefit of a general-domain pre-training followed by a task-specific fine-tuning in the context of a BioASQ summarization task, leading to a novel three-step fine-tuning approach that works with only a thousand in-domain examples. Our results indicate that a Large Language Model without domain-specific pre-training can have a significant edge in some domain-specific biomedical text generation tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TIM-Teaching-Large-Language-Models-to-Translate-with-Comparison"><a href="#TIM-Teaching-Large-Language-Models-to-Translate-with-Comparison" class="headerlink" title="TIM: Teaching Large Language Models to Translate with Comparison"></a>TIM: Teaching Large Language Models to Translate with Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04408">http://arxiv.org/abs/2307.04408</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lemon0830/tim">https://github.com/lemon0830/tim</a></li>
<li>paper_authors: Jiali Zeng, Fandong Meng, Yongjing Yin, Jie Zhou</li>
<li>for: 提高大型自然语言模型（LLM）在翻译任务中的表现</li>
<li>methods: 使用比较例子来教育LLM学习翻译</li>
<li>results: 比较例子学习翻译的方法可以超越现有的方法，并提高LLM在翻译任务中的表现<details>
<summary>Abstract</summary>
Open-sourced large language models (LLMs) have demonstrated remarkable efficacy in various tasks with instruction tuning. However, these models can sometimes struggle with tasks that require more specialized knowledge such as translation. One possible reason for such deficiency is that instruction tuning aims to generate fluent and coherent text that continues from a given instruction without being constrained by any task-specific requirements. Moreover, it can be more challenging for tuning smaller LLMs with lower-quality training data. To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation. Our approach involves presenting the model with examples of correct and incorrect translations and using a preference loss to guide the model's learning. We evaluate our method on WMT2022 test sets and show that it outperforms existing methods. Our findings offer a new perspective on fine-tuning LLMs for translation tasks and provide a promising solution for generating high-quality translations. Please refer to Github for more details: https://github.com/lemon0830/TIM.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Cross-lingual-Transfer-via-Phonemic-Transcription-Integration"><a href="#Enhancing-Cross-lingual-Transfer-via-Phonemic-Transcription-Integration" class="headerlink" title="Enhancing Cross-lingual Transfer via Phonemic Transcription Integration"></a>Enhancing Cross-lingual Transfer via Phonemic Transcription Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04361">http://arxiv.org/abs/2307.04361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nhhoang96/phonemic_xlingual">https://github.com/nhhoang96/phonemic_xlingual</a></li>
<li>paper_authors: Hoang H. Nguyen, Chenwei Zhang, Tao Zhang, Eugene Rohrbaugh, Philip S. Yu</li>
<li>for: 本研究旨在提高cross-lingual transfer的效果，特别是为了 bridging the gap among Chinese-Japanese-Korean-Vietnamese (CJKV) languages.</li>
<li>methods: 本研究使用了一个叫PhoneXL的框架，它将phonemic transcriptions作为cross-lingual transfer中的一个额外语言特征，以便优化cross-lingual transfer。PhoneXL使用了不同的对Alignment objectives，包括本地一对一对 alignment、多模态上下文对 alignment以及多语言上下文对 alignment。</li>
<li>results: 本研究的试验结果表明，PhoneXL可以提高cross-lingual transfer的效果，特别是在CJKV语言之间。在Named Entity Recognition和Part-of-Speech Tagging两个token-level任务上，PhoneXL可以实现了Consistent improvements over orthographic-based multilingual PLMs。<details>
<summary>Abstract</summary>
Previous cross-lingual transfer methods are restricted to orthographic representation learning via textual scripts. This limitation hampers cross-lingual transfer and is biased towards languages sharing similar well-known scripts. To alleviate the gap between languages from different writing scripts, we propose PhoneXL, a framework incorporating phonemic transcriptions as an additional linguistic modality beyond the traditional orthographic transcriptions for cross-lingual transfer. Particularly, we propose unsupervised alignment objectives to capture (1) local one-to-one alignment between the two different modalities, (2) alignment via multi-modality contexts to leverage information from additional modalities, and (3) alignment via multilingual contexts where additional bilingual dictionaries are incorporated. We also release the first phonemic-orthographic alignment dataset on two token-level tasks (Named Entity Recognition and Part-of-Speech Tagging) among the understudied but interconnected Chinese-Japanese-Korean-Vietnamese (CJKV) languages. Our pilot study reveals phonemic transcription provides essential information beyond the orthography to enhance cross-lingual transfer and bridge the gap among CJKV languages, leading to consistent improvements on cross-lingual token-level tasks over orthographic-based multilingual PLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Local one-to-one alignment between the two modalities2. Alignment via multi-modality contexts to leverage information from additional modalities3. Alignment via multilingual contexts using bilingual dictionariesWe also release the first phonemic-orthographic alignment dataset on two token-level tasks (Named Entity Recognition and Part-of-Speech Tagging) among the understudied but interconnected Chinese-Japanese-Korean-Vietnamese (CJKV) languages. Our pilot study shows that phonemic transcriptions provide essential information beyond the orthography to enhance cross-lingual transfer and bridge the gap among CJKV languages, leading to consistent improvements on cross-lingual token-level tasks over orthographic-based multilingual PLMs.Translation notes:* “orthographic representation” is translated as “文字表示” (wén zì biǎo yì)* “phonemic transcriptions” is translated as “phoneme  транскрипции” (fōnēm yīn xiǎng)* “alignment” is translated as “对应” (duì yì)* “modality” is translated as “modalities” is translated as “语言 modalities” (yǔ yán modalities)* “token-level tasks” is translated as “токен级任务” (tuō kēn jīn yè)* “multilingual PLMs” is translated as “多语言 PLMs” (duō yǔ yán PLMs)</details></li>
</ol>
<hr>
<h2 id="Event-Extraction-as-Question-Generation-and-Answering"><a href="#Event-Extraction-as-Question-Generation-and-Answering" class="headerlink" title="Event Extraction as Question Generation and Answering"></a>Event Extraction as Question Generation and Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05567">http://arxiv.org/abs/2307.05567</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dataminr-ai/event-extraction-as-question-generation-and-answering">https://github.com/dataminr-ai/event-extraction-as-question-generation-and-answering</a></li>
<li>paper_authors: Di Lu, Shihao Ran, Joel Tetreault, Alejandro Jaimes</li>
<li>for: 这篇论文的目的是提出一种基于问答模型的事件抽取方法，以提高事件抽取的准确率和效率。</li>
<li>methods: 这篇论文使用了问题生成模型（QG）和动态模板来生成更加具有Contextual information的问题，从而提高事件抽取的准确率。</li>
<li>results: 实验表明， compared to prior single-task-based models, QGA-EE在ACE05英语 dataset上的表现更高， indicating that the proposed method can effectively improve the accuracy and efficiency of event extraction.<details>
<summary>Abstract</summary>
Recent work on Event Extraction has reframed the task as Question Answering (QA), with promising results. The advantage of this approach is that it addresses the error propagation issue found in traditional token-based classification approaches by directly predicting event arguments without extracting candidates first. However, the questions are typically based on fixed templates and they rarely leverage contextual information such as relevant arguments. In addition, prior QA-based approaches have difficulty handling cases where there are multiple arguments for the same role. In this paper, we propose QGA-EE, which enables a Question Generation (QG) model to generate questions that incorporate rich contextual information instead of using fixed templates. We also propose dynamic templates to assist the training of QG model. Experiments show that QGA-EE outperforms all prior single-task-based models on the ACE05 English dataset.
</details>
<details>
<summary>摘要</summary>
最近的Event Extraction研究已经将任务重新定义为问题回答（QA），并取得了良好的结果。这种方法可以直接预测事件参数而不是首先提取候选人选。然而，问题通常基于固定模板，rarely leveraging上下文信息如相关参数。此外，先前的QA-based方法很难处理多个参数的同一个角色情况。在这篇论文中，我们提议QGA-EE，即使用问题生成（QG）模型生成含有丰富上下文信息的问题，而不是使用固定模板。我们还提出了动态模板，以帮助QG模型的训练。实验显示，QGA-EE在ACE05英语数据集上的单任务模型都超过了所有之前的模型。
</details></li>
</ul>
<hr>
<h2 id="HistRED-A-Historical-Document-Level-Relation-Extraction-Dataset"><a href="#HistRED-A-Historical-Document-Level-Relation-Extraction-Dataset" class="headerlink" title="HistRED: A Historical Document-Level Relation Extraction Dataset"></a>HistRED: A Historical Document-Level Relation Extraction Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04285">http://arxiv.org/abs/2307.04285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Soyoung/HistRED">https://huggingface.co/datasets/Soyoung/HistRED</a></li>
<li>paper_authors: Soyoung Yang, Minseok Choi, Youngwoo Cho, Jaegul Choo</li>
<li>for: 本研究的目的是促进历史关系抽取（RE）研究，探讨历史数据中的潜在应用场景。</li>
<li>methods: 该研究使用了 Yeonhaengnok 集成了的 HistRED 数据集，该数据集包含了 Hanja 和 Korean 文本的双语注释，以支持历史 RE 任务的研究。</li>
<li>results: 研究提出了一种双语 RE 模型，利用了 Korean 和 Hanja 文本上的上下文来预测实体之间的关系。模型在 HistRED 数据集上表现出色，超过了单语基elines，表明使用多语言上下文可以补充 RE 预测。Here’s the simplified Chinese text for each point:</li>
<li>for: 这个研究的目的是推动历史关系抽取（RE）研究，探索历史数据中的潜在应用场景。</li>
<li>methods: 该研究使用了《연향록》集成的 HistRED 数据集，该数据集包含了汉字和韩语文本的双语注释，以支持历史 RE 任务的研究。</li>
<li>results: 研究提出了一种双语 RE 模型，利用了韩语和汉字文本上的上下文来预测实体之间的关系。模型在 HistRED 数据集上表现出色，超过了单语基elines，表明使用多语言上下文可以补充 RE 预测。<details>
<summary>Abstract</summary>
Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years. To promote the historical RE research, we present HistRED constructed from Yeonhaengnok. Yeonhaengnok is a collection of records originally written in Hanja, the classical Chinese writing, which has later been translated into Korean. HistRED provides bilingual annotations such that RE can be performed on Korean and Hanja texts. In addition, HistRED supports various self-contained subtexts with different lengths, from a sentence level to a document level, supporting diverse context settings for researchers to evaluate the robustness of their RE models. To demonstrate the usefulness of our dataset, we propose a bilingual RE model that leverages both Korean and Hanja contexts to predict relations between entities. Our model outperforms monolingual baselines on HistRED, showing that employing multiple language contexts supplements the RE predictions. The dataset is publicly available at: https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.
</details>
<details>
<summary>摘要</summary>
尽管关系提取（RE）任务在不同领域得到了广泛应用，但历史上的应用尚未得到了充分的研究。为推动历史RE研究，我们现在提出了 HistRED，它是基于《연해нг록》的一个建构。《연해нг록》是一种原始写于汉字的记录，后来被翻译成朝鲜语。HistRED提供了双语注释，使得RE可以在朝鲜语和汉字文本之间进行。此外，HistRED支持多种自 contenido Subtexts，其中 lengths 从句子级到文档级，以支持多种上下文设置，以便研究人员可以评估其RE模型的可靠性。为了证明我们的数据集的有用性，我们提出了一种双语RE模型，该模型利用了朝鲜语和汉字上下文来预测实体之间的关系。我们的模型在 HistRED 上表现出色，超过了单语基eline，显示了employna多语言上下文可以补充RE预测。该数据集现在可以在以下链接获取：https://huggingface.co/datasets/Soyoung/HistRED， unter CC BY-NC-ND 4.0 license。
</details></li>
</ul>
<hr>
<h2 id="Automated-Essay-Scoring-in-Argumentative-Writing-DeBERTeachingAssistant"><a href="#Automated-Essay-Scoring-in-Argumentative-Writing-DeBERTeachingAssistant" class="headerlink" title="Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant"></a>Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04276">http://arxiv.org/abs/2307.04276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yann Hicke, Tonghua Tian, Karan Jha, Choong Hee Kim</li>
<li>for: This paper aims to improve the assessment of argumentative writing by developing a transformer-based architecture that can annotate discourse elements for their persuasiveness quality.</li>
<li>methods: The proposed method uses a transformer-based architecture to analyze argumentative writing and provide annotations for the persuasiveness quality of various discourse elements.</li>
<li>results: The proposed method achieved above-human accuracy in annotating argumentative writing discourse elements for their persuasiveness quality.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文的目的是改进口头写作评价，通过开发基于转换器的架构，对口头写作中的语言元素进行评估，并提供有关语言元素的评价结果。</li>
<li>methods: 提议的方法使用基于转换器的架构来分析口头写作，并提供对语言元素的评价结果。</li>
<li>results: 提议的方法在评估口头写作中的语言元素评价上达到了人类以上的准确率。<details>
<summary>Abstract</summary>
Automated Essay scoring has been explored as a research and industry problem for over 50 years. It has drawn a lot of attention from the NLP community because of its clear educational value as a research area that can engender the creation of valuable time-saving tools for educators around the world. Yet, these tools are generally focused on detecting good grammar, spelling mistakes, and organization quality but tend to fail at incorporating persuasiveness features in their final assessment. The responsibility to give actionable feedback to the student to improve the strength of their arguments is left solely on the teacher's shoulders. In this work, we present a transformer-based architecture capable of achieving above-human accuracy in annotating argumentative writing discourse elements for their persuasiveness quality and we expand on planned future work investigating the explainability of our model so that actionable feedback can be offered to the student and thus potentially enable a partnership between the teacher's advice and the machine's advice.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Augmenters-at-SemEval-2023-Task-1-Enhancing-CLIP-in-Handling-Compositionality-and-Ambiguity-for-Zero-Shot-Visual-WSD-through-Prompt-Augmentation-and-Text-To-Image-Diffusion"><a href="#Augmenters-at-SemEval-2023-Task-1-Enhancing-CLIP-in-Handling-Compositionality-and-Ambiguity-for-Zero-Shot-Visual-WSD-through-Prompt-Augmentation-and-Text-To-Image-Diffusion" class="headerlink" title="Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion"></a>Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05564">http://arxiv.org/abs/2307.05564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie S. Li, Yow-Ting Shiue, Yong-Siang Shih, Jonas Geiping</li>
<li>for: 这篇论文主要是为了解决英语视觉单词意思异步问题。</li>
<li>methods: 论文提出了两种解决方案：增强CLIP和稳定扩散抽样（SD Sampling）。增强CLIP通过使用大语言模型（LLMs）生成句子，以增强CLIP模型对文本中词语的理解能力。SD Sampling使用文本到图像的稳定扩散来生成多个图像，提高图像和文本的匹配机会。</li>
<li>results: 实验结果表明，增强CLIP和SD Sampling可以提高图像和文本的匹配率，并且可以减少多对多的问题。<details>
<summary>Abstract</summary>
This paper describes our zero-shot approaches for the Visual Word Sense Disambiguation (VWSD) Task in English. Our preliminary study shows that the simple approach of matching candidate images with the phrase using CLIP suffers from the many-to-many nature of image-text pairs. We find that the CLIP text encoder may have limited abilities in capturing the compositionality in natural language. Conversely, the descriptive focus of the phrase varies from instance to instance. We address these issues in our two systems, Augment-CLIP and Stable Diffusion Sampling (SD Sampling). Augment-CLIP augments the text prompt by generating sentences that contain the context phrase with the help of large language models (LLMs). We further explore CLIP models in other languages, as the an ambiguous word may be translated into an unambiguous one in the other language. SD Sampling uses text-to-image Stable Diffusion to generate multiple images from the given phrase, increasing the likelihood that a subset of images match the one that paired with the text.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Assessing-the-efficacy-of-large-language-models-in-generating-accurate-teacher-responses"><a href="#Assessing-the-efficacy-of-large-language-models-in-generating-accurate-teacher-responses" class="headerlink" title="Assessing the efficacy of large language models in generating accurate teacher responses"></a>Assessing the efficacy of large language models in generating accurate teacher responses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04274">http://arxiv.org/abs/2307.04274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yann Hicke, Abhishek Masand, Wentao Guo, Tushaar Gangavarapu</li>
<li>for: 这个研究是为了评估大语言模型在教育对话中的生成能力，以便模拟知gable teacher。</li>
<li>methods: 研究使用了多种标准的生成模型，包括GPT-4（少量学习、在场景学习）、精度调整的GPT-2和DialoGPT，并使用了强化学习来优化Flan-T5模型。</li>
<li>results: 研究发现GPT-4在Techer-Student Chatroom Corpus子集上表现出色， measured using BERTScore和DialogRPT。  Additionally, the study found that certain dataset characteristics, such as sampling, representativeness, and dialog completeness, can pose challenges to fine-tuning and contribute to the poor generalizability of the fine-tuned models.<details>
<summary>Abstract</summary>
(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.   We hypothesize that several dataset characteristics, including sampling, representativeness, and dialog completeness, pose significant challenges to fine-tuning, thus contributing to the poor generalizability of the fine-tuned models. Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills.
</details>
<details>
<summary>摘要</summary>
我们认为， dataset 特性，包括采样、 representativeness 和对话完整性，对于调整带来了 significi cant 挑战，这些挑战对于调整模型的泛化性具有负面影响。最后，我们注意到，为了评估这些生成模型，需要使用一种指标，不仅考虑对话 coherence 和模型语言分布的匹配，还需要考虑模型在教学技巧方面的表现。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Coding-at-Scale-Design-and-Deployment-of-a-Nationwide-System-for-Normalizing-Referrals-in-the-Chilean-Public-Healthcare-System"><a href="#Automatic-Coding-at-Scale-Design-and-Deployment-of-a-Nationwide-System-for-Normalizing-Referrals-in-the-Chilean-Public-Healthcare-System" class="headerlink" title="Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System"></a>Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05560">http://arxiv.org/abs/2307.05560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabián Villena, Matías Rojas, Felipe Arias, Jorge Pacheco, Paulina Vera, Jocelyn Dunstan</li>
<li>for: 该论文主要目的是提出一种自动将疾病提取出 из医疗文档，以便进行 epidemiological studies 等研究。</li>
<li>methods: 该论文提出了一种两步方法，首先使用 state-of-the-art NER 模型来识别疾病提到，然后使用基于 Elasticsearch 的搜索引擎系统将最相关的疾病代码分配给疾病提到。</li>
<li>results: 论文的实验结果表明，该系统可以准确地自动分配疾病代码，MAP 得分为 0.63 和 0.83 分别在 subcategory 和 category 两个水平上。<details>
<summary>Abstract</summary>
The disease coding task involves assigning a unique identifier from a controlled vocabulary to each disease mentioned in a clinical document. This task is relevant since it allows information extraction from unstructured data to perform, for example, epidemiological studies about the incidence and prevalence of diseases in a determined context. However, the manual coding process is subject to errors as it requires medical personnel to be competent in coding rules and terminology. In addition, this process consumes a lot of time and energy, which could be allocated to more clinically relevant tasks. These difficulties can be addressed by developing computational systems that automatically assign codes to diseases. In this way, we propose a two-step system for automatically coding diseases in referrals from the Chilean public healthcare system. Specifically, our model uses a state-of-the-art NER model for recognizing disease mentions and a search engine system based on Elasticsearch for assigning the most relevant codes associated with these disease mentions. The system's performance was evaluated on referrals manually coded by clinical experts. Our system obtained a MAP score of 0.63 for the subcategory level and 0.83 for the category level, close to the best-performing models in the literature. This system could be a support tool for health professionals, optimizing the coding and management process. Finally, to guarantee reproducibility, we publicly release the code of our models and experiments.
</details>
<details>
<summary>摘要</summary>
疾病编码任务是将每个在临床文档中提到的疾病分配一个从控制词汇中获取的唯一标识符。这项任务非常重要，因为它允许从无结构数据中提取信息，以进行例如，疾病发生率和患病率的评估。然而，手动编码过程受到误差的影响，因为医疗人员需要熟悉编码规则和术语。此外，这个过程需要很多时间和能量，这些资源可以用于更有价值的临床任务。为解决这些困难，我们提出了一种自动将疾病编码为疾病名称的两步系统。具体来说，我们的模型使用了当前领域的最佳NER模型，以识别疾病提到的文本，并使用基于Elasticsearch的搜索引擎系统，将疾病提到的最相关的编码词归类。我们的系统在专家手动编码的referral上进行评估，我们的系统在分类层次上获得了MAP分数为0.63，在类别层次上获得了MAP分数为0.83，与文献中最佳模型几乎相同。这个系统可以作为医疗专业人员的支持工具，优化编码和管理过程。最后，为保证可重现性，我们在线上公开发布了我们的模型和实验。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/10/cs.CL_2023_07_10/" data-id="clon21imz0078r588ga4i8wkj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/10/cs.LG_2023_07_10/" class="article-date">
  <time datetime="2023-07-10T10:00:00.000Z" itemprop="datePublished">2023-07-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/10/cs.LG_2023_07_10/">cs.LG - 2023-07-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="On-the-power-of-graph-neural-networks-and-the-role-of-the-activation-function"><a href="#On-the-power-of-graph-neural-networks-and-the-role-of-the-activation-function" class="headerlink" title="On the power of graph neural networks and the role of the activation function"></a>On the power of graph neural networks and the role of the activation function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04661">http://arxiv.org/abs/2307.04661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sammy Khalife, Amitabh Basu</li>
<li>for: 本研究探讨了图 neural network (GNN) 的表达能力。</li>
<li>methods: 作者使用了对凑分多项式活化函数的研究，证明了任何具有固定大小的 GNN 无法在任意数量的迭代过程中分辨两个非同构的根树。</li>
<li>results: 研究发现，允许Activation function 不是固定多项式的 GNN 在两轮迭代过程中可以分辨任何两个非同构的根树。此外，研究还回答了 [Grohe, 2021] 提出的一个开问，并证明了 bounded 和 unbounded 大小 GNN 之间存在势必的分化。<details>
<summary>Abstract</summary>
In this article we present new results about the expressivity of Graph Neural Networks (GNNs). We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations. The proof relies on tools from the algebra of symmetric polynomials. In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations. Our results imply a strict separation between bounded and unbounded size GNNs, answering an open question formulated by [Grohe, 2021]. We next prove that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of depth two (our results hold for activations like the sigmoid, hyperbolic tan and others). This shows how the power of graph neural networks can change drastically if one changes the activation function of the neural networks. The proof of this result utilizes the Lindemann-Weierstrauss theorem from transcendental number theory.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了新的结果关于图神经网络（GNNs）的表达能力。我们证明了，对任何具有分割 polynomials 活化函数的 GNN，其架构大小不随图像大小增长，那么存在两个非同构的根树，其中根节点不可以在无限多轮 iterations 中被 GNN  отличить。证明基于同态多项式代数的工具。在对比之下，已知无穷 GNNs（允许架构大小随图像大小变化）可以在两轮 iterations 中分辨这两个根节点。我们的结果表明，有穷 GNNs 和无穷 GNNs 之间存在彻底的分化，解答了 [Grohe, 2021] 提出的开问。我们接着证明，允许非分割 polynomials 活化函数的情况下，在两轮 iterations 中，单个神经元某元素权重网络可以分辨任何两个非同构的深度两个根树的根节点（我们的结果适用于如 сиги模、恒下弯和其他活化函数）。这示cases how the power of graph neural networks can change drastically if one changes the activation function of the neural networks. The proof of this result utilizes the Lindemann-Weierstrauss theorem from transcendental number theory.
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-for-Video-Classification-with-Frame-Level-Queries"><a href="#Active-Learning-for-Video-Classification-with-Frame-Level-Queries" class="headerlink" title="Active Learning for Video Classification with Frame Level Queries"></a>Active Learning for Video Classification with Frame Level Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05587">http://arxiv.org/abs/2307.05587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debanjan Goswami, Shayok Chakraborty</li>
<li>for: 这个研究目的是为了更进一步削减人工标注员的努力，使得将机器学习模型训练需要的标注数量更加少。</li>
<li>methods: 我们提出了一个新的活动学习框架，使用了不确定度和多样性的标准来选择具有代表性的视频和具有价值的几帧帧。</li>
<li>results: 我们的方法可以将人工标注员的努力削减至只需要审核几帧帧，而不是观看整个视频。这些结果表明了我们的方法可以帮助将机器学习模型训练需要的标注数量降低，并且可以更好地利用人工标注员的时间和努力。<details>
<summary>Abstract</summary>
Deep learning algorithms have pushed the boundaries of computer vision research and have depicted commendable performance in a variety of applications. However, training a robust deep neural network necessitates a large amount of labeled training data, acquiring which involves significant time and human effort. This problem is even more serious for an application like video classification, where a human annotator has to watch an entire video end-to-end to furnish a label. Active learning algorithms automatically identify the most informative samples from large amounts of unlabeled data; this tremendously reduces the human annotation effort in inducing a machine learning model, as only the few samples that are identified by the algorithm, need to be labeled manually. In this paper, we propose a novel active learning framework for video classification, with the goal of further reducing the labeling onus on the human annotators. Our framework identifies a batch of exemplar videos, together with a set of informative frames for each video; the human annotator needs to merely review the frames and provide a label for each video. This involves much less manual work than watching the complete video to come up with a label. We formulate a criterion based on uncertainty and diversity to identify the informative videos and exploit representative sampling techniques to extract a set of exemplar frames from each video. To the best of our knowledge, this is the first research effort to develop an active learning framework for video classification, where the annotators need to inspect only a few frames to produce a label, rather than watching the end-to-end video.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multimodal-brain-age-estimation-using-interpretable-adaptive-population-graph-learning"><a href="#Multimodal-brain-age-estimation-using-interpretable-adaptive-population-graph-learning" class="headerlink" title="Multimodal brain age estimation using interpretable adaptive population-graph learning"></a>Multimodal brain age estimation using interpretable adaptive population-graph learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04639">http://arxiv.org/abs/2307.04639</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bintsi/adaptive-graph-learning">https://github.com/bintsi/adaptive-graph-learning</a></li>
<li>paper_authors: Kyriaki-Margarita Bintsi, Vasileios Baltatzis, Rolandos Alexandros Potamias, Alexander Hammers, Daniel Rueckert</li>
<li>for: 本研究旨在提出一种基于图 convolutional neural network (GCN) 的人脑年龄估计方法，以便在诊断阿尔ц海默病等nehrodegenerative疾病中提供有价值的信息。</li>
<li>methods: 本研究使用了图建构机制（GCN）和注意力机制（Attention），通过学习人脑图的结构，从而提高人脑年龄估计的准确性。</li>
<li>results: 比较static图建构方法和其他适应方法，本研究的方法在人脑年龄估计和分类任务上表现出色，并且通过图像和非图像特征（phenotypes）的权重赋值，提高了图建构的可读性。<details>
<summary>Abstract</summary>
Brain age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer's. Population graphs, which include multimodal imaging information of the subjects along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks. A population graph is usually static and constructed manually using non-imaging information. However, graph construction is not a trivial task and might significantly affect the performance of the GCN, which is inherently very sensitive to the graph structure. In this work, we propose a framework that learns a population graph structure optimized for the downstream task. An attention mechanism assigns weights to a set of imaging and non-imaging features (phenotypes), which are then used for edge extraction. The resulting graph is used to train the GCN. The entire pipeline can be trained end-to-end. Additionally, by visualizing the attention weights that were the most important for the graph construction, we increase the interpretability of the graph. We use the UK Biobank, which provides a large variety of neuroimaging and non-imaging phenotypes, to evaluate our method on brain age regression and classification. The proposed method outperforms competing static graph approaches and other state-of-the-art adaptive methods. We further show that the assigned attention scores indicate that there are both imaging and non-imaging phenotypes that are informative for brain age estimation and are in agreement with the relevant literature.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用Graph Convolutional Networks (GCNs)在文献中使用人口图（population graph），其中包括多Modal imaging信息和人口之间的关系，已经证明是有用的 для多种医学影像任务。然而，人口图的建构并不是一个轻松的任务，可能会影响GCN的性能，GCN本身是非常敏感于图结构。在这种情况下，我们提出了一个框架，可以学习优化的人口图结构，用于下游任务。我们使用一个注意力机制，将多Modal imaging和非 imaging特征（phenotypes）分配给权重，然后用这些权重进行边EXTRACTION。得到的图可以用来训练GCN。整个管道可以被训练END-to-END。此外，通过Visual化注意力权重，我们提高了图的可读性。我们使用UK Biobank，该提供了丰富的神经成像和非成像特征，来评估我们的方法在脑年龄回归和分类任务中的性能。我们的方法超过了相同的静止图方法和其他状态的适应方法。我们进一步表明，分配的注意力分数指示，有 both imaging和非 imaging特征是脑年龄估计中有用的，与相关文献一致。<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-positional-contrastive-learning-application-to-cirrhosis-classification"><a href="#Weakly-supervised-positional-contrastive-learning-application-to-cirrhosis-classification" class="headerlink" title="Weakly-supervised positional contrastive learning: application to cirrhosis classification"></a>Weakly-supervised positional contrastive learning: application to cirrhosis classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04617">http://arxiv.org/abs/2307.04617</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guerbet-ai/wsp-contrastive">https://github.com/guerbet-ai/wsp-contrastive</a></li>
<li>paper_authors: Emma Sarfati, Alexandre Bône, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch</li>
<li>for: 这个研究是为了提出一种高效的弱监督位置学习（WSP）对称学习策略，以搭配弱标签和空间上的排序信息来提高医学影像分类的精度。</li>
<li>methods: 本研究使用了一种具有内部积分函数的通用核心损失函数，将弱标签和空间上的排序信息融合到一起，以提高医学影像分类的精度。</li>
<li>results: 研究结果显示，与基准模型相比，提案的模型在内部数据集上提高了分类AUC的值 by 5%，并在公共的LIHC数据集上提高了分类AUC的值 by 26%。<details>
<summary>Abstract</summary>
Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations, and small strongly-labeled (i.e., high-confidence) datasets. The proposed model improves the classification AUC by 5% with respect to a baseline model on our internal dataset, and by 26% on the public LIHC dataset from the Cancer Genome Atlas. The code is available at: https://github.com/Guerbet-AI/wsp-contrastive.
</details>
<details>
<summary>摘要</summary>
大量医学成像数据集可以便宜地和快速地进行低信度、弱标注（例如，放射学分数）的标注。高信度标签，如 histology-based 诊断，则非常罕见和昂贵。预训练策略，如对冲学习（CL）方法，可以利用无标签或弱标签数据集。这些方法通常需要大批量大小，但是在大量3D图像的全分辨率下，由于 GPU 内存限制，这可能会增加困难。然而，三维位势信息可以在某些医学应用中非常重要。在这种情况下，我们提出了一种高效的弱指导位势对比（WSP）对比学习策略，该策略通过一种通用的 kernel-based 损失函数来整合每个2D slice 的空间上下文和弱标签。我们在 cirrhosis 预测 task 上使用了一大量的弱标签图像，即放射学低信度标注，以及一些小型、强标签（即高信度标签）数据集。我们的模型与基线模型在我们的内部数据集上提高了分类 AUC 值5%，并在 LIHC 数据集上提高了26%。代码可以在：https://github.com/Guerbet-AI/wsp-contrastive 中找到。
</details></li>
</ul>
<hr>
<h2 id="MiVOLO-Multi-input-Transformer-for-Age-and-Gender-Estimation"><a href="#MiVOLO-Multi-input-Transformer-for-Age-and-Gender-Estimation" class="headerlink" title="MiVOLO: Multi-input Transformer for Age and Gender Estimation"></a>MiVOLO: Multi-input Transformer for Age and Gender Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04616">http://arxiv.org/abs/2307.04616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wildchlamydia/mivolo">https://github.com/wildchlamydia/mivolo</a></li>
<li>paper_authors: Maksim Kuprashevich, Irina Tolstykh</li>
<li>for: 这篇论文旨在提出一种基于视觉变换器的年龄和性别认知方法，以便在野外环境中进行年龄和性别预测。</li>
<li>methods: 该方法使用最新的视觉变换器，并将年龄和性别两个任务集成到一个共同的双输入&#x2F;输出模型中，以利用人脸信息和人像数据。</li>
<li>results: 经过实验表明，该模型在四个流行的标准benchmark上达到了状态机器人性能，并且在实时处理方面表现出色。此外，我们还引入了一个基于Open Images Dataset的新的benchmark，并提供了高精度的人工筛选的标注数据。最终，我们比较了该模型的年龄认知性能与人类水平，并证明它在大多数年龄范围内有显著优势。<details>
<summary>Abstract</summary>
Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded. We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer. Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data. This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image. To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities. Additionally, we introduce a novel benchmark based on images from the Open Images Dataset. The ground truth annotations for this benchmark have been meticulously generated by human annotators, resulting in high accuracy answers due to the smart aggregation of votes. Furthermore, we compare our model's age recognition performance with human-level accuracy and demonstrate that it significantly outperforms humans across a majority of age ranges. Finally, we grant public access to our models, along with the code for validation and inference. In addition, we provide extra annotations for used datasets and introduce our new benchmark.
</details>
<details>
<summary>摘要</summary>
在野外中，年龄和性别识别是一项非常具有挑战性的任务：除了条件的变化、姿势复杂度和图像质量的变化外，还有情况下面部或完全被遮盖。我们介绍了 MiVOLO（多输入VOLO），一种简单的方法，使用最新的视觉变换器进行年龄和性别估算。我们的方法将这两个任务集成到一个统一的双输入/输出模型中，利用人像数据以及脸部信息。这会提高我们的模型的总体化能力，使其能够在图像中不可见的脸部时还能达到满意的结果。为评估我们提出的模型，我们进行了四个流行的benchmark测试，并实现了实时处理能力。此外，我们还创建了一个基于Open Images Dataset的新的benchmark，其中的ground truth标注由人工标注员仔细生成，因此得到了高度的准确率。此外，我们比较了我们的年龄识别性能与人类水平的准确率，并证明它在大多数年龄范围内Significantly Outperform humans。最后，我们向公众开放了我们的模型，同时提供了验证和推理代码。此外，我们还提供了其他的标注，并引入了我们的新benchmark。
</details></li>
</ul>
<hr>
<h2 id="EchoVest-Real-Time-Sound-Classification-and-Depth-Perception-Expressed-through-Transcutaneous-Electrical-Nerve-Stimulation"><a href="#EchoVest-Real-Time-Sound-Classification-and-Depth-Perception-Expressed-through-Transcutaneous-Electrical-Nerve-Stimulation" class="headerlink" title="EchoVest: Real-Time Sound Classification and Depth Perception Expressed through Transcutaneous Electrical Nerve Stimulation"></a>EchoVest: Real-Time Sound Classification and Depth Perception Expressed through Transcutaneous Electrical Nerve Stimulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04604">http://arxiv.org/abs/2307.04604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jesse Choe, Siddhant Sood, Ryan Park</li>
<li>for: The paper aims to develop an assistive device for blind&#x2F;deaf individuals to enhance their awareness of their environment, with a focus on sound classification and localization.</li>
<li>methods: The paper employs a novel audio pipeline that combines the Audio Spectrogram Transformer (AST) model and Fast Fourier Transforms for noise reduction, as well as Otsu’s Method for background noise sound filtering and Complex Time Difference of Arrival algorithms for direction and depth calculation.</li>
<li>results: The final algorithm achieved state-of-the-art results on numerous checkpoints, including a 95.7% accuracy on the ESC-50 dataset for environmental sound classification.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文目标是开发一种助听设备，帮助失明聋听人更好地了解环境，特色在声音分类和本地化。</li>
<li>methods: 这篇论文采用了一种新的音频管线，将Audio Spectrogram Transformer（AST）模型和快速傅里埃变换（FFT）组合用于声音干扰reduction，以及OTSU的方法来过滤背景噪声，并使用复杂时差到达算法来计算方向和深度。</li>
<li>results: 最终算法在多个检查点上达到了顶尖的结果，包括ESC-50数据集上的声音分类准确率95.7%。<details>
<summary>Abstract</summary>
Over 1.5 billion people worldwide live with hearing impairment. Despite various technologies that have been created for individuals with such disabilities, most of these technologies are either extremely expensive or inaccessible for everyday use in low-medium income countries. In order to combat this issue, we have developed a new assistive device, EchoVest, for blind/deaf people to intuitively become more aware of their environment. EchoVest transmits vibrations to the user's body by utilizing transcutaneous electric nerve stimulation (TENS) based on the source of the sounds. EchoVest also provides various features, including sound localization, sound classification, noise reduction, and depth perception. We aimed to outperform CNN-based machine-learning models, the most commonly used machine learning model for classification tasks, in accuracy and computational costs. To do so, we developed and employed a novel audio pipeline that adapts the Audio Spectrogram Transformer (AST) model, an attention-based model, for our sound classification purposes, and Fast Fourier Transforms for noise reduction. The application of Otsu's Method helped us find the optimal thresholds for background noise sound filtering and gave us much greater accuracy. In order to calculate direction and depth accurately, we applied Complex Time Difference of Arrival algorithms and SOTA localization. Our last improvement was to use blind source separation to make our algorithms applicable to multiple microphone inputs. The final algorithm achieved state-of-the-art results on numerous checkpoints, including a 95.7\% accuracy on the ESC-50 dataset for environmental sound classification.
</details>
<details>
<summary>摘要</summary>
全球1.5亿人口中有听力障碍，尽管有许多为这些人群开发的技术，但大多数这些技术是非常昂贵或在低中收入国家中不可 accessing。为解决这个问题，我们开发了一个新的助手设备——EchoVest，用于听力障碍人群更好地了解环境。EchoVest通过使用皮肤电刺激（TENS）技术，将声音传递给用户的身体，并提供了多种功能，如声音地图、声音分类、干扰reduction和深度感知。我们想要在精度和计算成本两个方面超越基于Convolutional Neural Networks（CNN）的机器学习模型，因此我们开发了一个新的音频管道，使用Audio Spectrogram Transformer（AST）模型，并使用快速傅立叶变换来减少干扰。使用欧氏方法，我们可以查找最佳背景声音滤波的阈值，从而提高了准确性。为了计算方向和深度，我们应用了复杂时间差分解算法和当今最佳地址算法。最后，我们使用盲源分离来使我们的算法适用于多个麦克风输入。最终算法达到了现有最佳结果，包括ESC-50数据集上的95.7%准确率。
</details></li>
</ul>
<hr>
<h2 id="DBFed-Debiasing-Federated-Learning-Framework-based-on-Domain-Independent"><a href="#DBFed-Debiasing-Federated-Learning-Framework-based-on-Domain-Independent" class="headerlink" title="DBFed: Debiasing Federated Learning Framework based on Domain-Independent"></a>DBFed: Debiasing Federated Learning Framework based on Domain-Independent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05582">http://arxiv.org/abs/2307.05582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiale Li, Zhixin Li, Yibo Wang, Yao Li, Lei Wang</li>
<li>for: 本研究旨在提出一种基于领域独立的 federated learning 框架，以解决多个主体数据岛化问题，并 mitigate 模型偏见问题。</li>
<li>methods: 本研究使用了客户端生成的敏感特征来Explicitly 编码敏感特征，以避免模型偏见问题。</li>
<li>results: 实验结果显示，DBFed 比三种比较方法中的大多数指标都高于，这 fully  demonstrably 表明 DBFed 的偏见纠正效果。<details>
<summary>Abstract</summary>
As digital transformation continues, enterprises are generating, managing, and storing vast amounts of data, while artificial intelligence technology is rapidly advancing. However, it brings challenges in information security and data security. Data security refers to the protection of digital information from unauthorized access, damage, theft, etc. throughout its entire life cycle. With the promulgation and implementation of data security laws and the emphasis on data security and data privacy by organizations and users, Privacy-preserving technology represented by federated learning has a wide range of application scenarios. Federated learning is a distributed machine learning computing framework that allows multiple subjects to train joint models without sharing data to protect data privacy and solve the problem of data islands. However, the data among multiple subjects are independent of each other, and the data differences in quality may cause fairness issues in federated learning modeling, such as data bias among multiple subjects, resulting in biased and discriminatory models. Therefore, we propose DBFed, a debiasing federated learning framework based on domain-independent, which mitigates model bias by explicitly encoding sensitive attributes during client-side training. This paper conducts experiments on three real datasets and uses five evaluation metrics of accuracy and fairness to quantify the effect of the model. Most metrics of DBFed exceed those of the other three comparative methods, fully demonstrating the debiasing effect of DBFed.
</details>
<details>
<summary>摘要</summary>
为数统计 револю过程中，企业创生、管理和储存大量数据，同时人工智能技术快速发展。然而，这带来数据安全和资讯安全的挑战。数据安全指的是保护数据 digitization 过程中的数据免被未经授权的存取、损坏、窃取等行为。随着数据安全法规的推广和组织和用户对数据安全和隐私的重视，隐私保持技术如联邦学习被应用在各个应用场景中。联邦学习是一种分布式机器学习计算框架，允许多个主题共同训练无需分享数据，以保护数据隐私和解决数据岛问题。然而，各个主题的数据独立于对方，并且各个数据质量的不同可能导致该课程优化问题，如主题间的数据偏见，从而导致不公正和歧视的模型。因此，我们提出了DBFed，一个基于领域独立的偏见调整联邦学习框架，通过客边端训练中明确地编码敏感特征，以 Mitigate 模型偏见。本文对三个真实数据集进行实验，使用五个评估 метри来衡量模型的影响。大多数DBFed的 métriques exceed 其他三种比较方法的 métriques，全面展示了DBFed的偏见调整效果。
</details></li>
</ul>
<hr>
<h2 id="AnyTeleop-A-General-Vision-Based-Dexterous-Robot-Arm-Hand-Teleoperation-System"><a href="#AnyTeleop-A-General-Vision-Based-Dexterous-Robot-Arm-Hand-Teleoperation-System" class="headerlink" title="AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System"></a>AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04577">http://arxiv.org/abs/2307.04577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk, Hao Su, Xiaolong Wang, Yu-Wei Chao, Dieter Fox</li>
<li>for: 这篇论文目的是提出一个通用的视觉控制系统，以支持多种不同的机器人臂、手、现实和摄像头配置，并且可以在单个系统中实现。</li>
<li>methods: 该系统使用了一种通用的视觉控制方法，可以在多种不同的simulator和实际环境中进行操作。</li>
<li>results: 实际实验和虚拟实验中，AnyTeleop系统可以达到更高的成功率和更好的模仿学习性能，相比之前特定设计 для该机器人硬件的系统。<details>
<summary>Abstract</summary>
Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better imitation learning performance, compared with a previous system that is particularly designed for that simulator. Project page: http://anyteleop.com/.
</details>
<details>
<summary>摘要</summary>
“视觉基于的 теле操作可以赋予机器人人类水平的智能，同时只需要低成本的摄像头感知器。然而，当前的视觉基于的 теле操作系统是为特定的机器人模型和部署环境设计和工程，这会随着机器人模型池的扩展和运行环境的多样化而扩展不佳。在这篇论文中，我们提议了 AnyTeleop，一个通用和普适的 теле操作系统，可以支持多种不同的臂、手、现实和摄像头配置。尽管我们的系统设计了大量的灵活性，但它仍然可以实现高性能。在实际实验中， AnyTeleop 可以比一个特定机器人硬件设计的系统更高的成功率，使用同样的机器人。在模拟中， AnyTeleop 比特定适用于该模拟器的系统更好的学习效果。项目页面：http://anyteleop.com/。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Semi-Automated-Solution-Approach-Selection-Tool-for-Any-Use-Case-via-Scopus-and-OpenAI-a-Case-Study-for-AI-ML-in-Oncology"><a href="#A-Semi-Automated-Solution-Approach-Selection-Tool-for-Any-Use-Case-via-Scopus-and-OpenAI-a-Case-Study-for-AI-ML-in-Oncology" class="headerlink" title="A Semi-Automated Solution Approach Selection Tool for Any Use Case via Scopus and OpenAI: a Case Study for AI&#x2F;ML in Oncology"></a>A Semi-Automated Solution Approach Selection Tool for Any Use Case via Scopus and OpenAI: a Case Study for AI&#x2F;ML in Oncology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04573">http://arxiv.org/abs/2307.04573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deniz Kenan Kılıç, Alex Elkjær Vasegaard, Aurélien Desoeuvres, Peter Nielsen</li>
<li>for: 本研究提出了一个半自动化的工具，用于方案方法评估和选择，供研究者、实践者和决策者使用，同时作为未来研究的benchmark。</li>
<li>methods: 本工具包括三个模组：(1) 文献选择和分数计算，使用关键词选择方案进行Scopus API查询和 Compute relevancy; (2) 方案方法提取在文献中使用OpenAI API; (3) 敏感分析和后analyzes。</li>
<li>results: 研究显示，这个工具可以实现半自动化的方案方法评估和选择，并提供了各种应用场景的敏感分析和后analyzes。在肿瘤领域的 caso study 和多个使用案例中，该工具获得了有前提的结果，与手动真实比较。<details>
<summary>Abstract</summary>
In today's vast literature landscape, a manual review is very time-consuming. To address this challenge, this paper proposes a semi-automated tool for solution method review and selection. It caters to researchers, practitioners, and decision-makers while serving as a benchmark for future work. The tool comprises three modules: (1) paper selection and scoring, using a keyword selection scheme to query Scopus API and compute relevancy; (2) solution method extraction in papers utilizing OpenAI API; (3) sensitivity analysis and post-analyzes. It reveals trends, relevant papers, and methods. AI in the oncology case study and several use cases are presented with promising results, comparing the tool to manual ground truth.
</details>
<details>
<summary>摘要</summary>
今天的文献景观中， manual review 非常时间consuming。为解决这个挑战，这篇论文提出了一种半自动化工具 для方法评估和选择。它适用于研究人员、实践者和决策者，同时也作为未来工作的标准。工具包括三个模块：（1）文献选择和分数计算，使用关键词选择方案查询Scopus API，计算相关性；（2）解决方法提取在论文中使用OpenAI API；（3）敏感分析和后置分析。它揭示了趋势、相关论文和方法。在肿瘤 случа子研究和一些实践案例中，提出了有前提的结果，与手动参照相比。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Age-Estimation-Puzzle-Comparative-Analysis-of-Deep-Learning-Approaches-for-Facial-Age-Estimation"><a href="#Unraveling-the-Age-Estimation-Puzzle-Comparative-Analysis-of-Deep-Learning-Approaches-for-Facial-Age-Estimation" class="headerlink" title="Unraveling the Age Estimation Puzzle: Comparative Analysis of Deep Learning Approaches for Facial Age Estimation"></a>Unraveling the Age Estimation Puzzle: Comparative Analysis of Deep Learning Approaches for Facial Age Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04570">http://arxiv.org/abs/2307.04570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paplhjak/facial-age-estimation-benchmark">https://github.com/paplhjak/facial-age-estimation-benchmark</a></li>
<li>paper_authors: Jakub Paplham, Vojtech Franc</li>
<li>for: 本研究旨在解决不同年龄估计方法的比较带来的挑战，即发布结果的不可靠性，归因于测试过程中的不一致性。先前的研究报告了过去十年内特殊方法的不断改进，但我们的发现证明这些报告是不准确的。</li>
<li>methods: 本研究使用了跨Entropy损失函数作为标准方法，并系统分析了影响年龄估计结果的多种因素，包括脸部对齐、脸部覆盖率、图像分辨率、图像表示方式、模型架构和数据量。</li>
<li>results: 我们发现，这些因素通常对年龄估计结果产生更大的影响，而不是选择年龄估计方法本身。我们还评估了每种方法的泛化能力，通过评估每种方法在公共可用的年龄估计数据集上的跨数据集性能。结果强调了使用一致的数据处理方法和建立标准 benchmarks，以确保可靠和意义的比较。<details>
<summary>Abstract</summary>
Comparing different age estimation methods poses a challenge due to the unreliability of published results, stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. We argue that, for age estimation tasks outside of the low-data regime, designing specialized methods is unnecessary, and the standard approach of utilizing cross-entropy loss is sufficient. This paper aims to address the benchmark shortcomings by evaluating state-of-the-art age estimation methods in a unified and comparable setting. We systematically analyze the impact of various factors, including facial alignment, facial coverage, image resolution, image representation, model architecture, and the amount of data on age estimation results. Surprisingly, these factors often exert a more significant influence than the choice of the age estimation method itself. We assess the generalization capability of each method by evaluating the cross-dataset performance for publicly available age estimation datasets. The results emphasize the importance of using consistent data preprocessing practices and establishing standardized benchmarks to ensure reliable and meaningful comparisons. The source code is available at https://github.com/paplhjak/Facial-Age-Estimation-Benchmark.
</details>
<details>
<summary>摘要</summary>
This paper aims to address benchmark shortcomings by evaluating state-of-the-art age estimation methods in a unified and comparable setting. We systematically analyze the impact of various factors, including facial alignment, facial coverage, image resolution, image representation, model architecture, and the amount of data on age estimation results. Surprisingly, these factors often have a more significant influence than the choice of age estimation method.We assess the generalization capability of each method by evaluating cross-dataset performance for publicly available age estimation datasets. The results emphasize the importance of using consistent data preprocessing practices and establishing standardized benchmarks to ensure reliable and meaningful comparisons. The source code is available at https://github.com/paplhjak/Facial-Age-Estimation-Benchmark.
</details></li>
</ul>
<hr>
<h2 id="Interpreting-and-generalizing-deep-learning-in-physics-based-problems-with-functional-linear-models"><a href="#Interpreting-and-generalizing-deep-learning-in-physics-based-problems-with-functional-linear-models" class="headerlink" title="Interpreting and generalizing deep learning in physics-based problems with functional linear models"></a>Interpreting and generalizing deep learning in physics-based problems with functional linear models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04569">http://arxiv.org/abs/2307.04569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Arzani, Lingxiao Yuan, Pania Newell, Bei Wang</li>
<li>for: 本研究旨在提供一种可解释的深度学习模型，以提高其在不同数据集上的泛化能力和可读性。</li>
<li>methods: 我们提议使用函数数据分析（FDA）的概念，设计一种通用函数线性模型来替代已经训练过的深度学习模型。我们可以使用不同的kernel函数库，并使用稀疏回归来找到一个可解释的替代模型。</li>
<li>results: 我们的模型可以与深度学习模型具有相同的准确率，同时提高对于不同数据集的泛化能力，并且可以提供更多的可读性和可解释性。我们在固体力学、流体力学和运输等领域进行了测试，并得到了良好的结果。<details>
<summary>Abstract</summary>
Although deep learning has achieved remarkable success in various scientific machine learning applications, its black-box nature poses concerns regarding interpretability and generalization capabilities beyond the training data. Interpretability is crucial and often desired in modeling physical systems. Moreover, acquiring extensive datasets that encompass the entire range of input features is challenging in many physics-based learning tasks, leading to increased errors when encountering out-of-distribution (OOD) data. In this work, motivated by the field of functional data analysis (FDA), we propose generalized functional linear models as an interpretable surrogate for a trained deep learning model. We demonstrate that our model could be trained either based on a trained neural network (post-hoc interpretation) or directly from training data (interpretable operator learning). A library of generalized functional linear models with different kernel functions is considered and sparse regression is used to discover an interpretable surrogate model that could be analytically presented. We present test cases in solid mechanics, fluid mechanics, and transport. Our results demonstrate that our model can achieve comparable accuracy to deep learning and can improve OOD generalization while providing more transparency and interpretability. Our study underscores the significance of interpretability in scientific machine learning and showcases the potential of functional linear models as a tool for interpreting and generalizing deep learning.
</details>
<details>
<summary>摘要</summary>
Inspired by the field of functional data analysis (FDA), we propose generalized functional linear models as an interpretable surrogate for a trained deep learning model. Our model can be trained either based on a trained neural network (post-hoc interpretation) or directly from training data (interpretable operator learning). We consider a library of generalized functional linear models with different kernel functions and use sparse regression to discover an interpretable surrogate model that can be analytically presented.We demonstrate our model's effectiveness through test cases in solid mechanics, fluid mechanics, and transport. Our results show that our model can achieve comparable accuracy to deep learning and can improve OOD generalization while providing more transparency and interpretability. Our study highlights the importance of interpretability in scientific machine learning and showcases the potential of functional linear models as a tool for interpreting and generalizing deep learning.
</details></li>
</ul>
<hr>
<h2 id="Automatically-detecting-activities-of-daily-living-from-in-home-sensors-as-indicators-of-routine-behaviour-in-an-older-population"><a href="#Automatically-detecting-activities-of-daily-living-from-in-home-sensors-as-indicators-of-routine-behaviour-in-an-older-population" class="headerlink" title="Automatically detecting activities of daily living from in-home sensors as indicators of routine behaviour in an older population"></a>Automatically detecting activities of daily living from in-home sensors as indicators of routine behaviour in an older population</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04563">http://arxiv.org/abs/2307.04563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claire M. Timon, Pamela Hussey, Hyowon Lee, Catriona Murphy, Harsh Vardan Rai, and Alan F. Smeaton</li>
<li>for: 这个研究是为了开发一个基于互联网物 Things（IoT）系统，与数据分析，来提供不干扰的健康和待遇监控，以支持年长者在家中独立生活。</li>
<li>methods: 这个研究使用了一个Action Research Cycle（ARC）试验，试验了23名参与者，每个参与者有约20个IoT仪器在家中。在ARC试验中，参与者参加了两次数据告诉会，每次都会显示参与者的家中活动。这些会议也收集了参与者对检测活动的准确性的反馈。</li>
<li>results: 这个研究发现，使用了联合规则探索，可以独立地检测参与者的每天生活活动（ADL），并且可以使用单一的规则来检测各个参与者的ADL。这意味着，可以降低参与者提供训练数据的必要性，并且可以让更多的参与者加入系统。<details>
<summary>Abstract</summary>
Objective: The NEX project has developed an integrated Internet of Things (IoT) system coupled with data analytics to offer unobtrusive health and wellness monitoring supporting older adults living independently at home. Monitoring {currently} involves visualising a set of automatically detected activities of daily living (ADLs) for each participant. The detection of ADLs is achieved {} to allow the incorporation of additional participants whose ADLs are detected without re-training the system.   Methods: Following an extensive User Needs and Requirements study involving 426 participants, a pilot trial and a friendly trial of the deployment, an Action Research Cycle (ARC) trial was completed. This involved 23 participants over a 10-week period each with c.20 IoT sensors in their homes. During the ARC trial, participants each took part in two data-informed briefings which presented visualisations of their own in-home activities. The briefings also gathered training data on the accuracy of detected activities. Association rule mining was then used on the combination of data from sensors and participant feedback to improve the automatic detection of ADLs.   Results: Association rule mining was used to detect a range of ADLs for each participant independently of others and was then used to detect ADLs across participants using a single set of rules {for each ADL}. This allows additional participants to be added without the necessity of them providing training data.   Conclusions: Additional participants can be added to the NEX system without the necessity to re-train the system for automatic detection of the set of their activities of daily living.
</details>
<details>
<summary>摘要</summary>
目标：NEX项目已经开发了一个集成互联网智能（IoT）系统，并结合数据分析，以提供不间断的健康和休闲监测，支持年轻者在家中独立生活。监测当前包括图示每名参与者的自动检测的日常生活活动（ADLs）的集成。方法：经过了426名参与者的用户需求和要求研究，飞行试验和友好试验的部署，完成了一个Action Research Cycle（ARC）试验。这 involve了23名参与者，每名参与者在10周期内有约20个IoT传感器在家中。在ARC试验期间，参与者每人参加了两次数据驱动的会议，其中显示了每名参与者的家中活动的视觉化。这些会议还收集了参与者对检测的准确性的反馈，以便进一步改进自动检测ADLs的方法。结论：通过对传感器和参与者反馈的组合使用关联规则挖掘，可以独立地检测每名参与者的ADLs，并且可以在所有参与者之间共享同一组则。这意味着可以在不需要更多的训练数据的情况下，将更多的参与者添加到NEX系统中。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Surgery-for-One-shot-Unlearning-on-Generative-Model"><a href="#Gradient-Surgery-for-One-shot-Unlearning-on-Generative-Model" class="headerlink" title="Gradient Surgery for One-shot Unlearning on Generative Model"></a>Gradient Surgery for One-shot Unlearning on Generative Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04550">http://arxiv.org/abs/2307.04550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seohui Bae, Seoyoon Kim, Hyemin Jung, Woohyung Lim</li>
<li>for: 这篇论文目的是为了解释如何将数据的影响力除去 deep generative model 中。</li>
<li>methods: 本文提出了一个简单又有效的方法，通过将条件变数调整到正常方向，以干扰条件变数的影响。</li>
<li>results: 本文比较了现有的基eline，并提供了理论分析，证明了本方法可以高效地删除数据的影响。<details>
<summary>Abstract</summary>
Recent regulation on right-to-be-forgotten emerges tons of interest in unlearning pre-trained machine learning models. While approximating a straightforward yet expensive approach of retrain-from-scratch, recent machine unlearning methods unlearn a sample by updating weights to remove its influence on the weight parameters. In this paper, we introduce a simple yet effective approach to remove a data influence on the deep generative model. Inspired by works in multi-task learning, we propose to manipulate gradients to regularize the interplay of influence among samples by projecting gradients onto the normal plane of the gradients to be retained. Our work is agnostic to statistics of the removal samples, outperforming existing baselines while providing theoretical analysis for the first time in unlearning a generative model.
</details>
<details>
<summary>摘要</summary>
最近的强制忘记法规则促使大量关注在强制忘记前训练机器学习模型上。而 aproximating一个直接又昂贵的重新训练方法，现代机器忘记方法在一个样本上忘记，通过更新权重来消除其影响于权重参数。在这篇论文中，我们介绍了一种简单又有效的方法，用于从深度生成模型中移除数据的影响。受到多任务学习的启发，我们提议在欠拟合的情况下，对权重参数进行抑制，以 regularize各个样本之间的影响相互作用。我们的方法不依赖于废弃样本的统计特征，超越现有的基eline，并提供了理论分析。
</details></li>
</ul>
<hr>
<h2 id="StyleGAN2-based-Out-of-Distribution-Detection-for-Medical-Imaging"><a href="#StyleGAN2-based-Out-of-Distribution-Detection-for-Medical-Imaging" class="headerlink" title="StyleGAN2-based Out-of-Distribution Detection for Medical Imaging"></a>StyleGAN2-based Out-of-Distribution Detection for Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10193">http://arxiv.org/abs/2307.10193</a></li>
<li>repo_url: None</li>
<li>paper_authors: McKell Woodland, John Wood, Caleb O’Connor, Ankit B. Patel, Kristy K. Brock</li>
<li>for: 这个研究旨在探讨一种基于生成敌方网络（GAN）的方法，用于检测显像处理器中的非常训练分布（out-of-distribution，OOD）图像。</li>
<li>methods: 这个研究使用了StyleGAN2-ADA架构，并使用了反射推导来重建图像。测试数据包括了脑、头颈、肺、阴道和异常肝脏的 Computed Tomography（CT）图像。</li>
<li>results: 研究发现，这个方法可以对CT图像进行非常好的分类，并且完全无法重建肝脏异常，如针和痰液。AUROC值高于90%，这表明这个方法可以实现非常好的OOD检测。<details>
<summary>Abstract</summary>
One barrier to the clinical deployment of deep learning-based models is the presence of images at runtime that lie far outside the training distribution of a given model. We aim to detect these out-of-distribution (OOD) images with a generative adversarial network (GAN). Our training dataset was comprised of 3,234 liver-containing computed tomography (CT) scans from 456 patients. Our OOD test data consisted of CT images of the brain, head and neck, lung, cervix, and abnormal livers. A StyleGAN2-ADA architecture was employed to model the training distribution. Images were reconstructed using backpropagation. Reconstructions were evaluated using the Wasserstein distance, mean squared error, and the structural similarity index measure. OOD detection was evaluated with the area under the receiver operating characteristic curve (AUROC). Our paradigm distinguished between liver and non-liver CT with greater than 90% AUROC. It was also completely unable to reconstruct liver artifacts, such as needles and ascites.
</details>
<details>
<summary>摘要</summary>
一个阻碍深度学习模型在临床应用的问题是运行时存在远离训练分布的图像。我们使用生成对抗网络（GAN）来探测这些外部分布（OOD）图像。我们的训练集包括3234个liver-包含 computed tomography（CT）扫描图像，来自456名患者。我们的OOD测试数据包括CT图像的脑、头颈、肺、颈部和异常liver。我们采用了StyleGAN2-ADA架构来模型训练分布。图像使用反射推导重建。重建图像的评估方法包括 Wasserstein 距离、平均方差和结构相似度指标。OOD检测的评估方法包括接受操作特征分布曲线（AUROC）。我们的方法可以在liver和非liver CT之间进行分类，AUROC高于90%。此外，我们的方法完全无法重建liver残留物，如针和液体。
</details></li>
</ul>
<hr>
<h2 id="Pathway-toward-prior-knowledge-integrated-machine-learning-in-engineering"><a href="#Pathway-toward-prior-knowledge-integrated-machine-learning-in-engineering" class="headerlink" title="Pathway toward prior knowledge-integrated machine learning in engineering"></a>Pathway toward prior knowledge-integrated machine learning in engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06950">http://arxiv.org/abs/2307.06950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xia Chen, Philipp Geyer</li>
<li>for: 本研究旨在整合多学科领域专业知识和数据驱动过程，以便将域知识传递到机器可识别的数据驱动过程中。</li>
<li>methods: 本研究使用多学科领域专业知识的整合和数据驱动技术，包括信息不确定性源的检查和知识归约三层知识集成机器学习平台。</li>
<li>results: 本研究可以均衡全景和分解视角，满足不同领域专业人员的需求，同时也能够利用域知识来提高数据驱动过程的精度和可靠性。<details>
<summary>Abstract</summary>
Despite the digitalization trend and data volume surge, first-principles models (also known as logic-driven, physics-based, rule-based, or knowledge-based models) and data-driven approaches have existed in parallel, mirroring the ongoing AI debate on symbolism versus connectionism. Research for process development to integrate both sides to transfer and utilize domain knowledge in the data-driven process is rare. This study emphasizes efforts and prevailing trends to integrate multidisciplinary domain professions into machine acknowledgeable, data-driven processes in a two-fold organization: examining information uncertainty sources in knowledge representation and exploring knowledge decomposition with a three-tier knowledge-integrated machine learning paradigm. This approach balances holist and reductionist perspectives in the engineering domain.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:不withstanding数字化趋势和数据量的急剧增长，首 принциples模型（也称为逻辑驱动、物理学驱动、规则驱动或知识驱动模型）和数据驱动方法在平行的方式存在，反映了人工智能中符号主义VS连接主义的讨论。对于过程发展来 integrate both sides的研究 rare。这种研究强调在机器可识别的数据驱动过程中 Multidisciplinary domain professions的集成，使用 two-fold 组织方式：对知识表示中的信息不确定源进行检查，并 explore knowledge decomposition with a three-tier knowledge-integrated machine learning paradigm。这种方法平衡了整体和分解的视角在工程领域。
</details></li>
</ul>
<hr>
<h2 id="DADO-–-Low-Cost-Selection-Strategies-for-Deep-Active-Design-Optimization"><a href="#DADO-–-Low-Cost-Selection-Strategies-for-Deep-Active-Design-Optimization" class="headerlink" title="DADO – Low-Cost Selection Strategies for Deep Active Design Optimization"></a>DADO – Low-Cost Selection Strategies for Deep Active Design Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04536">http://arxiv.org/abs/2307.04536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens Decke, Christian Gruhl, Lukas Rauch, Bernhard Sick</li>
<li>for: 降低计算成本的数值仿真 simulate computationally expensive numerical simulations.</li>
<li>methods: 使用深度学习自动学习来优化设计。</li>
<li>results: 提高设计优化的效率，减少计算成本。<details>
<summary>Abstract</summary>
In this experience report, we apply deep active learning to the field of design optimization to reduce the number of computationally expensive numerical simulations. We are interested in optimizing the design of structural components, where the shape is described by a set of parameters. If we can predict the performance based on these parameters and consider only the promising candidates for simulation, there is an enormous potential for saving computing power. We present two selection strategies for self-optimization to reduce the computational cost in multi-objective design optimization problems. Our proposed methodology provides an intuitive approach that is easy to apply, offers significant improvements over random sampling, and circumvents the need for uncertainty estimation. We evaluate our strategies on a large dataset from the domain of fluid dynamics and introduce two new evaluation metrics to determine the model's performance. Findings from our evaluation highlights the effectiveness of our selection strategies in accelerating design optimization. We believe that the introduced method is easily transferable to other self-optimization problems.
</details>
<details>
<summary>摘要</summary>
在这份经验报告中，我们运用深度活动学来降低计算成本的数字实验 simulation 的数量。我们关注设计结构元件的优化，其形状由一组参数描述。如果我们可以根据这些参数预测性能，只考虑计算成本较低的候选者，那么可以很大减少计算力量。我们提出了两种选择策略来减少多目标设计优化问题中的计算成本。我们的提出的方法是INTUITIVE的，易于实施，比Random Sampling更好，无需 uncertainty estimation。我们对大量的流体动力学数据集进行了评估，并引入了两种新的评价指标来评估模型的性能。我们的评估结果表明，我们的选择策略在加速设计优化方面具有显著的效果。我们认为引入的方法可以轻松地应用于其他自动优化问题。
</details></li>
</ul>
<hr>
<h2 id="QBitOpt-Fast-and-Accurate-Bitwidth-Reallocation-during-Training"><a href="#QBitOpt-Fast-and-Accurate-Bitwidth-Reallocation-during-Training" class="headerlink" title="QBitOpt: Fast and Accurate Bitwidth Reallocation during Training"></a>QBitOpt: Fast and Accurate Bitwidth Reallocation during Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04535">http://arxiv.org/abs/2307.04535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorn Peters, Marios Fournarakis, Markus Nagel, Mart van Baalen, Tijmen Blankevoort</li>
<li>for: 提高 mobil 和嵌入式设备上的准确性，量化神经网络是一种非常有效的方法。特别是混合精度量化（MPQ）网络，其层可以被量化到不同的位宽，可以实现对同等资源限制下的任务性能更好的表现。</li>
<li>methods: 我们提出了一种名为QBitOpt的新算法，用于在量化感知训练（QAT）中更新位宽。我们将位宽分配问题解释为一个约束优化问题，并通过将快速计算敏感度与高效的解决器结合在一起，可以在QAT中生成混合精度网络，保证任务性能和严格的资源约束之间具有高度的匹配。</li>
<li>results: 我们在ImageNet上测试了QBitOpt，并证明了我们可以比存在fixed和混合精度方法的情况下，在常见的位宽约束下表现更好。<details>
<summary>Abstract</summary>
Quantizing neural networks is one of the most effective methods for achieving efficient inference on mobile and embedded devices. In particular, mixed precision quantized (MPQ) networks, whose layers can be quantized to different bitwidths, achieve better task performance for the same resource constraint compared to networks with homogeneous bitwidths. However, finding the optimal bitwidth allocation is a challenging problem as the search space grows exponentially with the number of layers in the network. In this paper, we propose QBitOpt, a novel algorithm for updating bitwidths during quantization-aware training (QAT). We formulate the bitwidth allocation problem as a constraint optimization problem. By combining fast-to-compute sensitivities with efficient solvers during QAT, QBitOpt can produce mixed-precision networks with high task performance guaranteed to satisfy strict resource constraints. This contrasts with existing mixed-precision methods that learn bitwidths using gradients and cannot provide such guarantees. We evaluate QBitOpt on ImageNet and confirm that we outperform existing fixed and mixed-precision methods under average bitwidth constraints commonly found in the literature.
</details>
<details>
<summary>摘要</summary>
“量化神经网络是一种最有效的方法来实现移动和嵌入式设备上快速的推理。特别是杂比例量化（MPQ）网络，它的层可以被量化到不同的比特宽，可以在同样的资源约束下提高任务性能。然而，寻找最佳比特宽分配是一个复杂的问题，因为搜索空间随着网络层数的增加而呈指数增长。在这篇论文中，我们提出了QBitOpt算法，它是一种在量化推理期间更新比特宽的算法。我们将比特宽分配问题形式化为一个约束优化问题。通过将快速计算的敏感度与高效的解决方案结合在一起，QBitOpt可以生成杂比例网络，保证任务性能高，同时也能够遵守常见的比特宽约束。这与现有的杂比例方法不同，它们通过梯度来学习比特宽，不能提供相同的保证。我们对ImageNet进行了评估，并证明了我们在平均比特宽约束下出perform existing固定和杂比例方法。”
</details></li>
</ul>
<hr>
<h2 id="Self-Expanding-Neural-Networks"><a href="#Self-Expanding-Neural-Networks" class="headerlink" title="Self Expanding Neural Networks"></a>Self Expanding Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04526">http://arxiv.org/abs/2307.04526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferzaad/Diabetes">https://github.com/ferzaad/Diabetes</a></li>
<li>paper_authors: Rupert Mitchell, Martin Mundt, Kristian Kersting</li>
<li>for: 这个研究旨在提出一种自适应的神经网络架构，可以灵活地调整网络的大小和深度，以减少训练过程中的损失。</li>
<li>methods: 本研究使用自然几何基于的方法，将网络的宽度和深度适当地调整，以降低训练过程中的损失。 authors 并证明了网络扩展的速率和扩展分数的下界。</li>
<li>results: 研究人员透过实验证明了自适应神经网络的有效性，在分类和回授问题中均有好的表现。此外， authors 还证明了这种自适应神经网络可以在网络的大小和深度不确定情况下表现良好。<details>
<summary>Abstract</summary>
The results of training a neural network are heavily dependent on the architecture chosen; and even a modification of only the size of the network, however small, typically involves restarting the training process. In contrast to this, we begin training with a small architecture, only increase its capacity as necessary for the problem, and avoid interfering with previous optimization while doing so. We thereby introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss. We prove an upper bound on the "rate" at which neurons are added, and a computationally cheap lower bound on the expansion score. We illustrate the benefits of such Self-Expanding Neural Networks in both classification and regression problems, including those where the appropriate architecture size is substantially uncertain a priori.
</details>
<details>
<summary>摘要</summary>
training一个神经网络的结果很受网络结构的影响，而 même一些小的修改会导致重新开始训练。相比之下，我们开始训练时选择小的网络 architecture，逐渐增加其容量，并避免在过程中对前一个优化器进行干扰。我们提出一种自然偏导的方法，其可以在可能减少假设的训练损失的情况下，自然地扩展神经网络的宽度和深度。我们证明了某种准则下的神经元数目增加的Upper bound，以及一种计算效率低的下界。我们还证明了这种自适应神经网络在分类和回归问题中的优势，包括一些预先不确定适应网络大小的问题。
</details></li>
</ul>
<hr>
<h2 id="Cluster-Induced-Mask-Transformers-for-Effective-Opportunistic-Gastric-Cancer-Screening-on-Non-contrast-CT-Scans"><a href="#Cluster-Induced-Mask-Transformers-for-Effective-Opportunistic-Gastric-Cancer-Screening-on-Non-contrast-CT-Scans" class="headerlink" title="Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans"></a>Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04525">http://arxiv.org/abs/2307.04525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingze Yuan, Yingda Xia, Xin Chen, Jiawen Yao, Junli Wang, Mingyan Qiu, Hexin Dong, Jingren Zhou, Bin Dong, Le Lu, Li Zhang, Zaiyi Liu, Ling Zhang</li>
<li>for: 寻找一种可靠、低成本、质量高的胃癌检测方法，以帮助提高胃癌检测率和降低误 диагности Ratio。</li>
<li>methods: 我们提出了一种基于深度学习的方法，使用非对照CT扫描图像进行胃癌检测。我们的模型具有学习的团集，可以快速识别胃癌的特征，并且可以减少误分类的风险。</li>
<li>results: 我们的方法在一个包含100名癌症患者和148名正常人的测试集上实现了感知率85.0%和特征率92.6%。与两名医生的平均感知率（73.5%）和特征率（84.3%）相比，我们的方法表现更好。此外，我们在一个外部测试集上实现了特征率97.7%。这表明我们的方法可以作为一种新、不侵入、低成本、高精度的胃癌检测方法。<details>
<summary>Abstract</summary>
Gastric cancer is the third leading cause of cancer-related mortality worldwide, but no guideline-recommended screening test exists. Existing methods can be invasive, expensive, and lack sensitivity to identify early-stage gastric cancer. In this study, we explore the feasibility of using a deep learning approach on non-contrast CT scans for gastric cancer detection. We propose a novel cluster-induced Mask Transformer that jointly segments the tumor and classifies abnormality in a multi-task manner. Our model incorporates learnable clusters that encode the texture and shape prototypes of gastric cancer, utilizing self- and cross-attention to interact with convolutional features. In our experiments, the proposed method achieves a sensitivity of 85.0% and specificity of 92.6% for detecting gastric tumors on a hold-out test set consisting of 100 patients with cancer and 148 normal. In comparison, two radiologists have an average sensitivity of 73.5% and specificity of 84.3%. We also obtain a specificity of 97.7% on an external test set with 903 normal cases. Our approach performs comparably to established state-of-the-art gastric cancer screening tools like blood testing and endoscopy, while also being more sensitive in detecting early-stage cancer. This demonstrates the potential of our approach as a novel, non-invasive, low-cost, and accurate method for opportunistic gastric cancer screening.
</details>
<details>
<summary>摘要</summary>
Gastic cancer 是全球第三大的肿瘤癌症死亡原因，但没有任何指南推荐的检测试验。现有的方法可能是侵入的，昂贵的，并且缺乏感知力来检测早期肿瘤。在这项研究中，我们探索了使用深度学习方法来非侵入性的检测肿瘤。我们提出了一种新的帧Transformer，它同时分割肿瘤和识别异常。我们的模型包含学习的团队，这些团队编码了肿瘤癌症的文本和形状原型，并利用自我和交叉关注来交互 WITH  convolutional 特征。在我们的实验中，我们的方法实现了检测肿瘤的感知率为85.0%，特征率为92.6%。与两名放射学家的平均感知率（73.5%）和特征率（84.3%）相比，我们的方法表现出色。此外，我们在一个外部测试集上获得了特征率为97.7%，这个测试集包含903个正常的案例。我们的方法与已知的肿瘤癌症检测工具如血液测试和endoscopic一样有效，同时具有更高的感知力和更早的识别能力。这表明了我们的方法的潜在优势，作为一种新、非侵入的、低成本的、准确的肿瘤检测方法。
</details></li>
</ul>
<hr>
<h2 id="SAGC-A68-a-space-access-graph-dataset-for-the-classification-of-spaces-and-space-elements-in-apartment-buildings"><a href="#SAGC-A68-a-space-access-graph-dataset-for-the-classification-of-spaces-and-space-elements-in-apartment-buildings" class="headerlink" title="SAGC-A68: a space access graph dataset for the classification of spaces and space elements in apartment buildings"></a>SAGC-A68: a space access graph dataset for the classification of spaces and space elements in apartment buildings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04515">http://arxiv.org/abs/2307.04515</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a2amir/sagc-a68">https://github.com/a2amir/sagc-a68</a></li>
<li>paper_authors: Amir Ziaee, Georg Suter</li>
<li>for: The paper is written for researchers and practitioners who are interested in developing Graph Deep Learning (GDL) models for space function and space element classification in the context of building design and analysis.</li>
<li>methods: The paper introduces a new dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings. The authors use this dataset to train and evaluate a graph attention network (GAT) that predicts 22 space function and 6 space element classes.</li>
<li>results: The authors demonstrate the potential of the dataset and the GAT model by achieving high accuracy rates on the test set. They also show that the GAT model outperforms other baseline models, indicating the effectiveness of using GDL methods for space function and space element classification.<details>
<summary>Abstract</summary>
The analysis of building models for usable area, building safety, and energy use requires accurate classification data of spaces and space elements. To reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable. A barrier hindering the utilization of Graph Deep Learning (GDL) methods to space function and space element classification is a lack of suitable datasets. To bridge this gap, we introduce a dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings. This graph-based dataset is well-suited for developing GDL models for space function and space element classification. To demonstrate the potential of the dataset, we employ it to train and evaluate a graph attention network (GAT) that predicts 22 space function and 6 space element classes. The dataset and code used in the experiment are available online. https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68.
</details>
<details>
<summary>摘要</summary>
analysis of building models for usable area, building safety, and energy use requires accurate classification data of spaces and space elements. to reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable. a barrier hindering the utilization of graph deep learning (gdl) methods to space function and space element classification is a lack of suitable datasets. to bridge this gap, we introduce a dataset, sagc-a68, which comprises access graphs automatically generated from 68 digital 3d models of space layouts of apartment buildings. this graph-based dataset is well-suited for developing gdl models for space function and space element classification. to demonstrate the potential of the dataset, we employ it to train and evaluate a graph attention network (gat) that predicts 22 space function and 6 space element classes. the dataset and code used in the experiment are available online. https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68.Here's the translation in Traditional Chinese:分析建筑模型的用able面积、建筑安全和能源使用需要准确的分类数据空间和空间元素。为了减少输入模型准备的劳动和错误，自动分类空间和空间元素是可能的。 however，使用图深度学习（GDL）方法进行空间功能和空间元素分类存在一个障碍：缺乏适合的数据集。为了跨越这一障碍，我们介绍了一个数据集，SAGC-A68，该数据集包括自动生成的68个数字3D模型的空间布局的访问图。这个图基本数据集非常适合开发GDL模型进行空间功能和空间元素分类。为了证明该数据集的潜力，我们使用它来训练和评估一个图注意力网络（GAT），该网络预测22个空间功能和6个空间元素类型。该数据集和实验代码在线可用。https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68。
</details></li>
</ul>
<hr>
<h2 id="Improving-Heterogeneous-Graph-Learning-with-Weighted-Mixed-Curvature-Product-Manifold"><a href="#Improving-Heterogeneous-Graph-Learning-with-Weighted-Mixed-Curvature-Product-Manifold" class="headerlink" title="Improving Heterogeneous Graph Learning with Weighted Mixed-Curvature Product Manifold"></a>Improving Heterogeneous Graph Learning with Weighted Mixed-Curvature Product Manifold</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04514">http://arxiv.org/abs/2307.04514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sharecodesubmission/weighted_product_manifold">https://github.com/sharecodesubmission/weighted_product_manifold</a></li>
<li>paper_authors: Tuc Nguyen-Van, Dung D. Le, The-Anh Ta</li>
<li>for: 该论文主要研究 graphs 的 embedding 问题，即如何将 graphs 转换为低维度的 vector 空间中的表示，以便进行后续的计算和分析。</li>
<li>methods: 该论文提出了一种基于 weighted product manifolds 的方法，即 WEIGHTED-PM，来解决 graphs 的 embedding 问题。该方法利用输入 graph 的 topological 信息自动地确定每个 ком成分空间在 product spaces 中的权重，从而更好地表示输入 graph 的复杂结构。</li>
<li>results: 该论文通过 extensively 的实验表明，WEIGHTED-PM 方法可以从输入数据中学习更好的 graph 表示，并且在多个下游任务中表现更好，如 word similarity learning、top-$k$ recommendation 和 knowledge graph embedding。<details>
<summary>Abstract</summary>
In graph representation learning, it is important that the complex geometric structure of the input graph, e.g. hidden relations among nodes, is well captured in embedding space. However, standard Euclidean embedding spaces have a limited capacity in representing graphs of varying structures. A promising candidate for the faithful embedding of data with varying structure is product manifolds of component spaces of different geometries (spherical, hyperbolic, or euclidean). In this paper, we take a closer look at the structure of product manifold embedding spaces and argue that each component space in a product contributes differently to expressing structures in the input graph, hence should be weighted accordingly. This is different from previous works which consider the roles of different components equally. We then propose WEIGHTED-PM, a data-driven method for learning embedding of heterogeneous graphs in weighted product manifolds. Our method utilizes the topological information of the input graph to automatically determine the weight of each component in product spaces. Extensive experiments on synthetic and real-world graph datasets demonstrate that WEIGHTED-PM is capable of learning better graph representations with lower geometric distortion from input data, and performs better on multiple downstream tasks, such as word similarity learning, top-$k$ recommendation, and knowledge graph embedding.
</details>
<details>
<summary>摘要</summary>
在图表示学中，capturing复杂的图形结构，如隐藏的节点关系，在嵌入空间中是非常重要的。然而，标准的欧几何嵌入空间有限的表示能力，无法捕捉不同结构的图。一种有前途的方法是使用不同geometry的组件空间的产品拓扑（球面、尖锥形、欧几何）。在这篇文章中，我们做了产品拓扑 embedding 空间的结构分析，并 argue that每个组件空间在产品中都有不同的表达效果，因此应该给予不同的权重。这与之前的作品不同，一般认为所有组件都应该具有相同的角色。我们然后提出了WEIGHTED-PM，一种数据驱动的方法，用于学习异类图的嵌入。我们的方法利用输入图的topological信息来自动确定每个组件在产品空间中的权重。我们的实验表明，WEIGHTED-PM可以从输入数据中学习更好的图表示，并且在多个下游任务中表现更好，如word相似度学习、top-$k$推荐和知识图嵌入。
</details></li>
</ul>
<hr>
<h2 id="An-Algorithm-with-Optimal-Dimension-Dependence-for-Zero-Order-Nonsmooth-Nonconvex-Stochastic-Optimization"><a href="#An-Algorithm-with-Optimal-Dimension-Dependence-for-Zero-Order-Nonsmooth-Nonconvex-Stochastic-Optimization" class="headerlink" title="An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization"></a>An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04504">http://arxiv.org/abs/2307.04504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Kornowski, Ohad Shamir</li>
<li>for: 本研究探讨了使用杂变函数评估的$( \delta, \epsilon )$-稳定点优化问题的复杂性，并提出了一种 faster algorithm，其复杂度为 $O(d\delta^{-1}\epsilon^{-3})$，这是对维度 $d$ 和精度参数 $\delta,\epsilon$ 的优化。</li>
<li>methods: 本研究使用了随机零次算法来解决该问题，并提出了一种新的方法，该方法的复杂度为 $O(d\delta^{-1}\epsilon^{-3})$，这是对维度 $d$ 和精度参数 $\delta,\epsilon$ 的优化。</li>
<li>results: 本研究的结果表明，我们的算法可以在几乎所有情况下达到最优的 convergence rate，并且在满足certain condition下，我们的算法可以在随机扰动下达到最优的 convergence rate。此外，我们的分析还证明了在非 convex 随机零次设定下，nonsmooth 优化与 smooth 优化之间存在等价关系。<details>
<summary>Abstract</summary>
We study the complexity of producing $(\delta,\epsilon)$-stationary points of Lipschitz objectives which are possibly neither smooth nor convex, using only noisy function evaluations. Recent works proposed several stochastic zero-order algorithms that solve this task, all of which suffer from a dimension-dependence of $\Omega(d^{3/2})$ where $d$ is the dimension of the problem, which was conjectured to be optimal. We refute this conjecture by providing a faster algorithm that has complexity $O(d\delta^{-1}\epsilon^{-3})$, which is optimal (up to numerical constants) with respect to $d$ and also optimal with respect to the accuracy parameters $\delta,\epsilon$, thus solving an open question due to Lin et al. (NeurIPS'22). Moreover, the convergence rate achieved by our algorithm is also optimal for smooth objectives, proving that in the nonconvex stochastic zero-order setting, nonsmooth optimization is as easy as smooth optimization. We provide algorithms that achieve the aforementioned convergence rate in expectation as well as with high probability. Our analysis is based on a simple yet powerful geometric lemma regarding the Goldstein-subdifferential set, which allows utilizing recent advancements in first-order nonsmooth nonconvex optimization.
</details>
<details>
<summary>摘要</summary>
我们研究生成($\delta$, $\epsilon$)-稳定点的复杂性，这些目标可能不对称，并且可能不 глад。我们使用仅有抽象函数评估来进行。近期的研究提出了一些测量零次法，解决了这个任务，但是所有这些方法都受到了维度($d$)的依赖性，即$\Omega(d^{3/2})$。我们推翻了这个推论，提出了一个更快的算法，其复杂度为$O(d\delta^{-1}\epsilon^{-3})$，它在$d$和精度参数$\delta,\epsilon$方面都是最佳（即对应的常数），解决了林等人（NeurIPS'22）所提出的开问题。此外，我们的算法还可以在光滑目标上 достичь最佳的凝固率，证明了在非对称零次法设定中，非光滑优化是与光滑优化一样容易。我们提供了在预期中和高概率下实现的算法，我们的分析基于一个简单又强大的几何学 lemma，允许我们利用最近的非光滑非对称非凝固优化的进步。
</details></li>
</ul>
<hr>
<h2 id="Geometric-Constraints-in-Probabilistic-Manifolds-A-Bridge-from-Molecular-Dynamics-to-Structured-Diffusion-Processes"><a href="#Geometric-Constraints-in-Probabilistic-Manifolds-A-Bridge-from-Molecular-Dynamics-to-Structured-Diffusion-Processes" class="headerlink" title="Geometric Constraints in Probabilistic Manifolds: A Bridge from Molecular Dynamics to Structured Diffusion Processes"></a>Geometric Constraints in Probabilistic Manifolds: A Bridge from Molecular Dynamics to Structured Diffusion Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04493">http://arxiv.org/abs/2307.04493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Diamond, Markus Lill</li>
<li>for: 这篇论文是为了解决生物复杂体系的宏观特征特征模型中的精度和特点问题，具体来说是在Euclidean空间中采样特定子集的问题。</li>
<li>methods: 该论文提出了一种将约束 projetion 算法纳入杜伦抑杂概率模型中的方法，以便在Euclidean空间中采样受约束的分布。</li>
<li>results: 该方法可以帮助实现在深度学习基础上的药物设计中维护特定分子对话的需要，以确保治疗效果和安全性。<details>
<summary>Abstract</summary>
Understanding the macroscopic characteristics of biological complexes demands precision and specificity in statistical ensemble modeling. One of the primary challenges in this domain lies in sampling from particular subsets of the state-space, driven either by existing structural knowledge or specific areas of interest within the state-space. We propose a method that enables sampling from distributions that rigorously adhere to arbitrary sets of geometric constraints in Euclidean spaces. This is achieved by integrating a constraint projection operator within the well-regarded architecture of Denoising Diffusion Probabilistic Models, a framework founded in generative modeling and probabilistic inference. The significance of this work becomes apparent, for instance, in the context of deep learning-based drug design, where it is imperative to maintain specific molecular profile interactions to realize the desired therapeutic outcomes and guarantee safety.
</details>
<details>
<summary>摘要</summary>
理解生物复杂体系的宏观特征需要精度和特点的统计ensemble模型。这个领域的主要挑战在于采样特定的子集state-space，由于现有的结构知识或特定区域的 интересы。我们提出了一种方法，可以准确地采样符合任意的几何约束的分布在欧几何空间中。这是通过将约束投影Operator интеGRATE到了Denosing Diffusion Probabilistic Models的可靠的建筑基础上实现的。这种方法的重要性在深度学习基础设计中 particualry evident，因为需要保持特定的分子 profil interactio来实现所需的治疗效果和安全性。
</details></li>
</ul>
<hr>
<h2 id="Invertible-Low-Dimensional-Modelling-of-X-ray-Absorption-Spectra-for-Potential-Applications-in-Spectral-X-ray-Imaging"><a href="#Invertible-Low-Dimensional-Modelling-of-X-ray-Absorption-Spectra-for-Potential-Applications-in-Spectral-X-ray-Imaging" class="headerlink" title="Invertible Low-Dimensional Modelling of X-ray Absorption Spectra for Potential Applications in Spectral X-ray Imaging"></a>Invertible Low-Dimensional Modelling of X-ray Absorption Spectra for Potential Applications in Spectral X-ray Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04484">http://arxiv.org/abs/2307.04484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raziye Kubra Kumrular, Thomas Blumensath</li>
<li>for: 这个论文主要是为了提出一种新的非线性模型，用于 spectral X-ray imaging 中的数据压缩、噪声除除、spectral 估计和材料组成的量化测量。</li>
<li>methods: 该论文提出了一种新的模型，它combines一个深度神经网络自编器和一个最佳的线性模型基于 Singular Value Decomposition (SVD)。</li>
<li>results: 作者比较了这种新方法与其他线性和非线性方法，包括一种简单的模型和一种另一种深度学习模型。结果显示，该新方法在模拟 X-ray 吸收谱中的 K-edge 区域时表现更优异。<details>
<summary>Abstract</summary>
X-ray interaction with matter is an energy-dependent process that is contingent on the atomic structure of the constituent material elements. The most advanced models to capture this relationship currently rely on Monte Carlo (MC) simulations. Whilst these very accurate models, in many problems in spectral X-ray imaging, such as data compression, noise removal, spectral estimation, and the quantitative measurement of material compositions, these models are of limited use, as these applications typically require the efficient inversion of the model, that is, they require the estimation of the best model parameters for a given spectral measurement. Current models that can be easily inverted however typically only work when modelling spectra in regions away from their K-edges, so they have limited utility when modelling a wider range of materials. In this paper, we thus propose a novel, non-linear model that combines a deep neural network autoencoder with an optimal linear model based on the Singular Value Decomposition (SVD). We compare our new method to other alternative linear and non-linear approaches, a sparse model and an alternative deep learning model. We demonstrate the advantages of our method over traditional models, especially when modelling X-ray absorption spectra that contain K-edges in the energy range of interest.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们因此提出了一种新的、非线性模型，它结合了深度神经网络自适应器和基于Singular Value Decomposition（SVD）的最佳线性模型。我们与其他的线性和非线性方法进行比较，包括一种稀疏模型和一种另一种深度学习模型。我们示出了我们的方法在传统模型的基础上具有优势，特别是当模型X射线吸收谱在能量范围内时。
</details></li>
</ul>
<hr>
<h2 id="Badgers-generating-data-quality-deficits-with-Python"><a href="#Badgers-generating-data-quality-deficits-with-Python" class="headerlink" title="Badgers: generating data quality deficits with Python"></a>Badgers: generating data quality deficits with Python</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04468">http://arxiv.org/abs/2307.04468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fraunhofer-iese/badgers">https://github.com/fraunhofer-iese/badgers</a></li>
<li>paper_authors: Julien Siebert, Daniel Seifert, Patricia Kelbert, Michael Kläs, Adam Trendowicz</li>
<li>for: 这篇论文的目的是为了实验性评估数据驱动（人工智能（AI）或机器学习（ML））应用中的数据质量问题。</li>
<li>methods: 这篇论文使用了一个名为“badgers”的Python库，该库可以生成不同类型的数据质量缺陷（例如异常值、数据不均衡、漂移等），以便对数据驱动应用的数据质量进行实验性评估。</li>
<li>results: 这篇论文通过使用“badgers”库，可以生成不同类型的数据质量缺陷，以便对数据驱动应用的数据质量进行实验性评估。 documentation 可以在<a target="_blank" rel="noopener" href="https://fraunhofer-iese.github.io/badgers/">https://fraunhofer-iese.github.io/badgers/</a> 中找到，源代码可以在<a target="_blank" rel="noopener" href="https://github.com/Fraunhofer-IESE/badgers">https://github.com/Fraunhofer-IESE/badgers</a> 中找到。<details>
<summary>Abstract</summary>
Generating context specific data quality deficits is necessary to experimentally assess data quality of data-driven (artificial intelligence (AI) or machine learning (ML)) applications. In this paper we present badgers, an extensible open-source Python library to generate data quality deficits (outliers, imbalanced data, drift, etc.) for different modalities (tabular data, time-series, text, etc.). The documentation is accessible at https://fraunhofer-iese.github.io/badgers/ and the source code at https://github.com/Fraunhofer-IESE/badgers
</details>
<details>
<summary>摘要</summary>
生成模式特定数据质量缺陷是评估数据驱动（人工智能（AI）或机器学习（ML））应用的实验需求。本文介绍badgers，一个可扩展的开源Python库，用于生成不同类型的数据质量缺陷（异常值、不均衡数据、漂移等）。文档可以在https://fraunhofer-iese.github.io/badgers/中查看，代码在https://github.com/Fraunhofer-IESE/badgers上可以查看。
</details></li>
</ul>
<hr>
<h2 id="Multi-modal-Graph-Learning-over-UMLS-Knowledge-Graphs"><a href="#Multi-modal-Graph-Learning-over-UMLS-Knowledge-Graphs" class="headerlink" title="Multi-modal Graph Learning over UMLS Knowledge Graphs"></a>Multi-modal Graph Learning over UMLS Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04461">http://arxiv.org/abs/2307.04461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Burger, Gunnar Rätsch, Rita Kuznetsova</li>
<li>for: 预测病人患病进程的多次医院访问（Multiple Hospital Visits）</li>
<li>methods: 使用图 neural network 学习医学概念的有意义表示，并将其组合为代表整个病人访问的表示。</li>
<li>results: 比较于现有的建筑方案，提高了表示多模态医学概念的性能，并且证明了在医学知识的支持下，多模态医学概念表示具有重要的意义。In English, this means:</li>
<li>for: Predicting the progression of patient illnesses across multiple hospital visits</li>
<li>methods: Using graph neural networks to learn meaningful representations of medical concepts, and combining them to represent entire patient visits.</li>
<li>results: Outperforming existing architectures in representing multiple modalities of medical concepts, and demonstrating the significance of incorporating prior medical knowledge.<details>
<summary>Abstract</summary>
Clinicians are increasingly looking towards machine learning to gain insights about patient evolutions. We propose a novel approach named Multi-Modal UMLS Graph Learning (MMUGL) for learning meaningful representations of medical concepts using graph neural networks over knowledge graphs based on the unified medical language system. These representations are aggregated to represent entire patient visits and then fed into a sequence model to perform predictions at the granularity of multiple hospital visits of a patient. We improve performance by incorporating prior medical knowledge and considering multiple modalities. We compare our method to existing architectures proposed to learn representations at different granularities on the MIMIC-III dataset and show that our approach outperforms these methods. The results demonstrate the significance of multi-modal medical concept representations based on prior medical knowledge.
</details>
<details>
<summary>摘要</summary>
临床医生们正在寻求机器学习技术以获取病人发展情况的洞察。我们提出了一种新的方法名为多modal UMLS图像学习（MMUGL），该方法利用知识图库基于统一医学语言系统（UMLS）中的医学概念进行图神经网络学习，以获取有用的医学概念表示。这些表示被聚合以表示整个病人访问，然后通过顺序模型进行预测，以达到多个医院访问病人的级别。我们通过 incorporating 先前的医学知识和考虑多种感知modalities来提高性能。我们对MIMIC-III数据集中的不同层次的表示学习体系进行比较，并显示了我们的方法在这些方法之上出performances。结果表明多modal医学概念表示基于先前的医学知识具有重要性。
</details></li>
</ul>
<hr>
<h2 id="Invex-Programs-First-Order-Algorithms-and-Their-Convergence"><a href="#Invex-Programs-First-Order-Algorithms-and-Their-Convergence" class="headerlink" title="Invex Programs: First Order Algorithms and Their Convergence"></a>Invex Programs: First Order Algorithms and Their Convergence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04456">http://arxiv.org/abs/2307.04456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adarsh Barik, Suvrit Sra, Jean Honorio</li>
<li>for: 解决非对称问题，即具有全局最小值的每个站点点。</li>
<li>methods: 提出了新的一代首选法，并证明其 converge 的条件和速率。</li>
<li>results: 提供了一种新的投影法，可以解决约束的非对称问题，并提供了速率 guarantees。<details>
<summary>Abstract</summary>
Invex programs are a special kind of non-convex problems which attain global minima at every stationary point. While classical first-order gradient descent methods can solve them, they converge very slowly. In this paper, we propose new first-order algorithms to solve the general class of invex problems. We identify sufficient conditions for convergence of our algorithms and provide rates of convergence. Furthermore, we go beyond unconstrained problems and provide a novel projected gradient method for constrained invex programs with convergence rate guarantees. We compare and contrast our results with existing first-order algorithms for a variety of unconstrained and constrained invex problems. To the best of our knowledge, our proposed algorithm is the first algorithm to solve constrained invex programs.
</details>
<details>
<summary>摘要</summary>
“inx problems是一种特殊的非凸问题，它们在每个稳定点上取得全球最小值。 classical first-order gradient descent方法可以解决它们，但是它们对应的速度很慢。在这篇论文中，我们提出了一些新的first-order算法来解决一般的inx问题。我们识别出了它们的充分条件，并提供了速度 guarantee。此外，我们不仅处理不受限制的问题，而且提出了一个新的对应投影法，它具有速度 guarantee。我们与现有的first-order算法进行比较和对照，并证明了我们的提案是第一个可以解决约束inx问题的算法。”Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Graph-Convolutional-Networks-for-Simulating-Multi-phase-Flow-and-Transport-in-Porous-Media"><a href="#Graph-Convolutional-Networks-for-Simulating-Multi-phase-Flow-and-Transport-in-Porous-Media" class="headerlink" title="Graph Convolutional Networks for Simulating Multi-phase Flow and Transport in Porous Media"></a>Graph Convolutional Networks for Simulating Multi-phase Flow and Transport in Porous Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04449">http://arxiv.org/abs/2307.04449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiamin Jiang, Bo Guo</li>
<li>for:  numerical simulation of multi-phase fluid dynamics in porous media</li>
<li>methods:  data-driven surrogate modeling using Graph Convolutional Networks (GCNs)</li>
<li>results:  high accuracy in predicting pressure and saturation states, and generalization to irregular domain geometries and unstructured meshes.Here is the same information in Simplified Chinese:</li>
<li>for:  numerical simulation of多相流体动力学在材料孔隙中</li>
<li>methods: 基于图像 convolutional neural networks (GCNs) 的数据驱动模拟</li>
<li>results: 高精度地预测压力和湿度状态的演变，并可以通过不同的域几何和粗粒度网格来扩展。<details>
<summary>Abstract</summary>
Numerical simulation of multi-phase fluid dynamics in porous media is critical for many subsurface applications. Data-driven surrogate modeling provides computationally inexpensive alternatives to high-fidelity numerical simulators. While the commonly used convolutional neural networks (CNNs) are powerful in approximating partial differential equation solutions, it remains challenging for CNNs to handle irregular and unstructured simulation meshes. However, subsurface simulation models often involve unstructured meshes with complex mesh geometries, which limits the application of CNNs. To address this challenge, here we construct surrogate models based on Graph Convolutional Networks (GCNs) to approximate the spatial-temporal solutions of multi-phase flow and transport processes. We propose a new GCN architecture suited to the hyperbolic character of the coupled PDE system, to better capture the saturation dynamics. Results of 2D heterogeneous test cases show that our surrogates predict the evolutions of the pressure and saturation states with high accuracy, and the predicted rollouts remain stable for multiple timesteps. Moreover, the GCN-based models generalize well to irregular domain geometries and unstructured meshes that are unseen in the training dataset.
</details>
<details>
<summary>摘要</summary>
numerically simulating multi-phase fluid dynamics in porous media is crucial for many subsurface applications. data-driven surrogate modeling provides computationally inexpensive alternatives to high-fidelity numerical simulators. although commonly used convolutional neural networks (CNNs) are powerful in approximating partial differential equation solutions, it remains challenging for CNNs to handle irregular and unstructured simulation meshes. however, subsurface simulation models often involve unstructured meshes with complex mesh geometries, which limits the application of CNNs. to address this challenge, we construct surrogate models based on graph convolutional networks (GCNs) to approximate the spatial-temporal solutions of multi-phase flow and transport processes. we propose a new GCN architecture suited to the hyperbolic character of the coupled PDE system, to better capture the saturation dynamics. results of 2D heterogeneous test cases show that our surrogates predict the evolutions of the pressure and saturation states with high accuracy, and the predicted rollouts remain stable for multiple timesteps. moreover, the GCN-based models generalize well to irregular domain geometries and unstructured meshes that are unseen in the training dataset.
</details></li>
</ul>
<hr>
<h2 id="Learning-Behavioral-Representations-of-Routines-From-Large-scale-Unlabeled-Wearable-Time-series-Data-Streams-using-Hawkes-Point-Process"><a href="#Learning-Behavioral-Representations-of-Routines-From-Large-scale-Unlabeled-Wearable-Time-series-Data-Streams-using-Hawkes-Point-Process" class="headerlink" title="Learning Behavioral Representations of Routines From Large-scale Unlabeled Wearable Time-series Data Streams using Hawkes Point Process"></a>Learning Behavioral Representations of Routines From Large-scale Unlabeled Wearable Time-series Data Streams using Hawkes Point Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04445">http://arxiv.org/abs/2307.04445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiantian Feng, Brandon M Booth, Shrikanth Narayanan</li>
<li>for: 这篇论文旨在探讨如何通过不带任何标签的穿戴式传感器数据来探索人们的日常活动模式。</li>
<li>methods: 该论文提出了一种基于时间序列 clustering 和 Hawkes 点过程学习算法的新方法，可以从无标签时间序列数据中探索人们的日常行为模式。</li>
<li>results: 研究人员通过使用该方法，成功地探索了100多名参与者的日常活动模式，并发现了每天的活动转换关系。此外，该方法还能够捕捉到各个人的个性特征和情绪变化。<details>
<summary>Abstract</summary>
Continuously-worn wearable sensors enable researchers to collect copious amounts of rich bio-behavioral time series recordings of real-life activities of daily living, offering unprecedented opportunities to infer novel human behavior patterns during daily routines. Existing approaches to routine discovery through bio-behavioral data rely either on pre-defined notions of activities or use additional non-behavioral measurements as contexts, such as GPS location or localization within the home, presenting risks to user privacy. In this work, we propose a novel wearable time-series mining framework, Hawkes point process On Time series clusters for ROutine Discovery (HOT-ROD), for uncovering behavioral routines from completely unlabeled wearable recordings. We utilize a covariance-based method to generate time-series clusters and discover routines via the Hawkes point process learning algorithm. We empirically validate our approach for extracting routine behaviors using a completely unlabeled time-series collected continuously from over 100 individuals both in and outside of the workplace during a period of ten weeks. Furthermore, we demonstrate this approach intuitively captures daily transitional relationships between physical activity states without using prior knowledge. We also show that the learned behavioral patterns can assist in illuminating an individual's personality and affect.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用不间断搭配的便携式传感器，研究人员可以收集大量的丰富的生物行为时序记录，以获得前无之例的人类行为模式发现机会。现有的 Routine 发现方法通过生物行为数据，都是基于先前定义的活动或者使用其他非行为测量，如 GPS 位置或家庭内部的位置，这些方法存在用户隐私风险。在这项工作中，我们提出了一种基于 Hawkes 点过程的时序序分 clustering 框架，称为 HOT-ROD，用于从未标记的便携式记录中探索行为 Routine。我们使用 covariance 基本方法生成时序序 clusters，通过 Hawkes 点过程学习算法发现 Routine。我们经验 Validate 我们的方法可以从未标记的时序记录中提取行为 Routine。我们使用了来自100名参与者的完全无标记时序记录，在工作场所和外部环境中进行了10周的观察。此外，我们还示出了我们的方法可以直观地捕捉日常physical activity状态之间的关系，无需使用先前知识。最后，我们还示出了学习的行为模式可以帮助描述个体的人性和情感。
</details></li>
</ul>
<hr>
<h2 id="Search-time-Efficient-Device-Constraints-Aware-Neural-Architecture-Search"><a href="#Search-time-Efficient-Device-Constraints-Aware-Neural-Architecture-Search" class="headerlink" title="Search-time Efficient Device Constraints-Aware Neural Architecture Search"></a>Search-time Efficient Device Constraints-Aware Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04443">http://arxiv.org/abs/2307.04443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oshin Dutta, Tanu Kanvar, Sumeet Agarwal</li>
<li>for: 这篇论文的目的是提出一种自动化建立适合边缘设备的对应深度学习架构，以减少边缘设备的资源需求。</li>
<li>methods: 这篇论文使用了对应深度学习架构搜索（Neural Architecture Search，NAS）来自动化建立适合边缘设备的架构。它还使用了Weight sharing和Channel bottleneck技术来加速搜索过程。</li>
<li>results: 根据实验结果，DCA-NAS比对应深度学习架构更好地适应边缘设备的硬件限制，并且与流行的 mobil 架构相比，在不同的图像识别 datasets 上具有相似的表现。此外，DCA-NAS 还可以在 Hardware-NAS-Bench 上进行硬件特定的架构搜索，实现低 inference 延迟和现场表现。<details>
<summary>Abstract</summary>
Edge computing aims to enable edge devices, such as IoT devices, to process data locally instead of relying on the cloud. However, deep learning techniques like computer vision and natural language processing can be computationally expensive and memory-intensive. Creating manual architectures specialized for each device is infeasible due to their varying memory and computational constraints. To address these concerns, we automate the construction of task-specific deep learning architectures optimized for device constraints through Neural Architecture Search (NAS). We present DCA-NAS, a principled method of fast neural network architecture search that incorporates edge-device constraints such as model size and floating-point operations. It incorporates weight sharing and channel bottleneck techniques to speed up the search time. Based on our experiments, we see that DCA-NAS outperforms manual architectures for similar sized models and is comparable to popular mobile architectures on various image classification datasets like CIFAR-10, CIFAR-100, and Imagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the generalization capabilities of DCA-NAS. On further evaluating our approach on Hardware-NAS-Bench, device-specific architectures with low inference latency and state-of-the-art performance were discovered.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)边计算旨在让边缘设备，如物联网设备，直接处理本地数据而不依赖于云端。然而，深度学习技术如计算机视觉和自然语言处理可能具有计算成本和内存占用的问题。为了解决这些问题，我们自动构建任务特定的深度学习架构，以适应边缘设备的约束。我们提出了DCA-NAS，一种基于约束的快速神经网络架构搜索方法，该方法包括边缘设备的模型大小和浮点运算数量等约束。它还包括权重共享和通道瓶颈技术来加速搜索时间。根据我们的实验，DCA-NAS在相同大小的模型中表现更好，与流行的移动设备架构相当，并在多个图像分类 datasets  like CIFAR-10, CIFAR-100, Imagenet-1k 上达到了类似的性能。我们的方法在 DARTS 和 NAS-Bench-201 的搜索空间上进行了广泛的测试，并证明了DCA-NAS的普适性。在进一步评估我们的方法在 Hardware-NAS-Bench 上时，我们发现了适用于具体的设备的低延迟和状态公平的设备特定架构。
</details></li>
</ul>
<hr>
<h2 id="Designing-Novel-Cognitive-Diagnosis-Models-via-Evolutionary-Multi-Objective-Neural-Architecture-Search"><a href="#Designing-Novel-Cognitive-Diagnosis-Models-via-Evolutionary-Multi-Objective-Neural-Architecture-Search" class="headerlink" title="Designing Novel Cognitive Diagnosis Models via Evolutionary Multi-Objective Neural Architecture Search"></a>Designing Novel Cognitive Diagnosis Models via Evolutionary Multi-Objective Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04429">http://arxiv.org/abs/2307.04429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/devilyangs/emo-nas-cd">https://github.com/devilyangs/emo-nas-cd</a></li>
<li>paper_authors: Shangshang Yang, Haiping Ma, Cheng Zhen, Ye Tian, Limiao Zhang, Yaochu Jin, Xingyi Zhang</li>
<li>for: 提高现代智能教育平台中学生知识概念掌握的精度和效率，通过自动设计新的认知诊断模型。</li>
<li>methods: 使用进化多目标神经网络架构搜索（NAS）自动设计新的认知诊断模型，并maximize模型性能和可解释性。</li>
<li>results: 通过实验表明，提posed方法可以寻找比现有模型更好的认知诊断模型，同时保持与人类设计的模型相同的可解释性。<details>
<summary>Abstract</summary>
Cognitive diagnosis plays a vital role in modern intelligent education platforms to reveal students' proficiency in knowledge concepts for subsequent adaptive tasks. However, due to the requirement of high model interpretability, existing manually designed cognitive diagnosis models hold too simple architectures to meet the demand of current intelligent education systems, where the bias of human design also limits the emergence of effective cognitive diagnosis models. In this paper, we propose to automatically design novel cognitive diagnosis models by evolutionary multi-objective neural architecture search (NAS). Specifically, we observe existing models can be represented by a general model handling three given types of inputs and thus first design an expressive search space for the NAS task in cognitive diagnosis. Then, we propose multi-objective genetic programming (MOGP) to explore the NAS task's search space by maximizing model performance and interpretability. In the MOGP design, each architecture is transformed into a tree architecture and encoded by a tree for easy optimization, and a tailored genetic operation based on four sub-genetic operations is devised to generate offspring effectively. Besides, an initialization strategy is also suggested to accelerate the convergence by evolving half of the population from existing models' variants. Experiments on two real-world datasets demonstrate that the cognitive diagnosis models searched by the proposed approach exhibit significantly better performance than existing models and also hold as good interpretability as human-designed models.
</details>
<details>
<summary>摘要</summary>
现代智能教育平台中，认知诊断发挥了关键作用，以揭示学生知识概念的熟练程度，以便进行后续适应任务。然而，由于需要高度的模型解释性，现有的手动设计的认知诊断模型具有过于简单的结构，无法满足当前智能教育系统的需求。在这篇论文中，我们提议使用EVOLUTIONARY MULTI-OBJECTIVE NEURAL ARCHITECTURE SEARCH（NAS）自动设计新的认知诊断模型。 Specifically, we observe that existing models can be represented by a general model handling three given types of inputs, and thus first design an expressive search space for the NAS task in cognitive diagnosis. Then, we propose multi-objective genetic programming (MOGP) to explore the NAS task's search space by maximizing model performance and interpretability. In the MOGP design, each architecture is transformed into a tree architecture and encoded by a tree for easy optimization, and a tailored genetic operation based on four sub-genetic operations is devised to generate offspring effectively. Besides, an initialization strategy is also suggested to accelerate the convergence by evolving half of the population from existing models' variants. Experiments on two real-world datasets demonstrate that the cognitive diagnosis models searched by the proposed approach exhibit significantly better performance than existing models and also hold as good interpretability as human-designed models.
</details></li>
</ul>
<hr>
<h2 id="Observation-of-high-energy-neutrinos-from-the-Galactic-plane"><a href="#Observation-of-high-energy-neutrinos-from-the-Galactic-plane" class="headerlink" title="Observation of high-energy neutrinos from the Galactic plane"></a>Observation of high-energy neutrinos from the Galactic plane</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04427">http://arxiv.org/abs/2307.04427</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Abbasi, M. Ackermann, J. Adams, J. A. Aguilar, M. Ahlers, M. Ahrens, J. M. Alameddine, A. A. Alves Jr., N. M. Amin, K. Andeen, T. Anderson, G. Anton, C. Argüelles, Y. Ashida, S. Athanasiadou, S. Axani, X. Bai, A. Balagopal V., S. W. Barwick, V. Basu, S. Baur, R. Bay, J. J. Beatty, K. -H. Becker, J. Becker Tjus, J. Beise, C. Bellenghi, S. Benda, S. BenZvi, D. Berley, E. Bernardini, D. Z. Besson, G. Binder, D. Bindig, E. Blaufuss, S. Blot, M. Boddenberg, F. Bontempo, J. Y. Book, J. Borowka, S. Böser, O. Botner, J. Böttcher, E. Bourbeau, F. Bradascio, J. Braun, B. Brinson, S. Bron, J. Brostean-Kaiser, R. T. Burley, R. S. Busse, M. A. Campana, E. G. Carnie-Bronca, C. Chen, Z. Chen, D. Chirkin, K. Choi, B. A. Clark, K. Clark, L. Classen, A. Coleman, G. H. Collin, A. Connolly, J. M. Conrad, P. Coppin, P. Correa, D. F. Cowen, R. Cross, C. Dappen, P. Dave, C. De Clercq, J. J. DeLaunay, D. Delgado López, H. Dembinski, K. Deoskar, A. Desai, P. Desiati, K. D. de Vries, G. de Wasseige, T. DeYoung, A. Diaz, J. C. Díaz-Vélez, M. Dittmer, H. Dujmovic, M. Dunkman, M. A. DuVernois, T. Ehrhardt, P. Eller, R. Engel, H. Erpenbeck, J. Evans, P. A. Evenson, K. L. Fan, A. R. Fazely, A. Fedynitch, N. Feigl, S. Fiedlschuster, A. T. Fienberg, C. Finley, L. Fischer, D. Fox, A. Franckowiak, E. Friedman, A. Fritz, P. Fürst, T. K. Gaisser, J. Gallagher, E. Ganster, A. Garcia, S. Garrappa, L. Gerhardt, A. Ghadimi, C. Glaser, T. Glauch, T. Glüsenkamp, N. Goehlke, A. Goldschmidt, J. G. Gonzalez, S. Goswami, D. Grant, T. Grégoire, S. Griswold, C. Günther, P. Gutjahr, C. Haack, A. Hallgren, R. Halliday, L. Halve, F. Halzen, M. Ha Minh, K. Hanson, J. Hardin, A. A. Harnisch, A. Haungs, K. Helbing, F. Henningsen, E. C. Hettinger, S. Hickford, J. Hignight, C. Hill, G. C. Hill, K. D. Hoffman, K. Hoshina, W. Hou, F. Huang, M. Huber, T. Huber, K. Hultqvist, M. Hünnefeld, R. Hussain, K. Hymon, S. In, N. Iovine, A. Ishihara, M. Jansson, G. S. Japaridze, M. Jeong, M. Jin, B. J. P. Jones, D. Kang, W. Kang, X. Kang, A. Kappes, D. Kappesser, L. Kardum, T. Karg, M. Karl, A. Karle, U. Katz, M. Kauer, M. Kellermann, J. L. Kelley, A. Kheirandish, K. Kin, J. Kiryluk, S. R. Klein, A. Kochocki, R. Koirala, H. Kolanoski, T. Kontrimas, L. Köpke, C. Kopper, S. Kopper, D. J. Koskinen, P. Koundal, M. Kovacevich, M. Kowalski, T. Kozynets, E. Krupczak, E. Kun, N. Kurahashi, N. Lad, C. Lagunas Gualda, J. L. Lanfranchi, M. J. Larson, F. Lauber, J. P. Lazar, J. W. Lee, K. Leonard, A. Leszczyńska, Y. Li, M. Lincetto, Q. R. Liu, M. Liubarska, E. Lohfink, C. J. Lozano Mariscal, L. Lu, F. Lucarelli, A. Ludwig, W. Luszczak, Y. Lyu, W. Y. Ma, J. Madsen, K. B. M. Mahn, Y. Makino, S. Mancina, I. C. Mariş, I. Martinez-Soler, R. Maruyama, S. McCarthy, T. McElroy, F. McNally, J. V. Mead, K. Meagher, S. Mechbal, A. Medina, M. Meier, S. Meighen-Berger, Y. Merckx, J. Micallef, D. Mockler, T. Montaruli, R. W. Moore, K. Morik, R. Morse, M. Moulai, T. Mukherjee, R. Naab, R. Nagai, R. Nahnhauer, U. Naumann, J. Necker, L. V. Nguyen, H. Niederhausen, M. U. Nisa, S. C. Nowicki, D. Nygren, A. Obertacke Pollmann, M. Oehler, B. Oeyen, A. Olivas, E. O’Sullivan, H. Pandya, D. V. Pankova, N. Park, G. K. Parker, E. N. Paudel, L. Paul, C. Pérez de los Heros, L. Peters, J. Peterson, S. Philippen, S. Pieper, A. Pizzuto, M. Plum, Y. Popovych, A. Porcelli, M. Prado Rodriguez, B. Pries, G. T. Przybylski, C. Raab, J. Rack-Helleis, A. Raissi, M. Rameez, K. Rawlins, I. C. Rea, Z. Rechav, A. Rehman, P. Reichherzer, R. Reimann, G. Renzi, E. Resconi, S. Reusch, W. Rhode, M. Richman, B. Riedel, E. J. Roberts, S. Robertson, G. Roellinghoff, M. Rongen, C. Rott, T. Ruhe, D. Ryckbosch, D. Rysewyk Cantu, I. Safa, J. Saffer, D. Salazar-Gallegos, P. Sampathkumar, S. E. Sanchez Herrera, A. Sandrock, M. Santander, S. Sarkar, S. Sarkar, K. Satalecka, M. Schaufel, H. Schieler, S. Schindler, T. Schmidt, A. Schneider, J. Schneider, F. G. Schröder, L. Schumacher, G. Schwefer, S. Sclafani, D. Seckel, S. Seunarine, A. Sharma, S. Shefali, N. Shimizu, M. Silva, B. Skrzypek, B. Smithers, R. Snihur, J. Soedingrekso, A. Sogaard, D. Soldin, C. Spannfellner, G. M. Spiczak, C. Spiering, M. Stamatikos, T. Stanev, R. Stein, J. Stettner, T. Stezelberger, B. Stokstad, T. Stürwald, T. Stuttard, G. W. Sullivan, I. Taboada, S. Ter-Antonyan, J. Thwaites, S. Tilav, F. Tischbein, K. Tollefson, C. Tönnis, S. Toscano, D. Tosi, A. Trettin, M. Tselengidou, C. F. Tung, A. Turcati, R. Turcotte, C. F. Turley, J. P. Twagirayezu, B. Ty, M. A. Unland Elorrieta, N. Valtonen-Mattila, J. Vandenbroucke, N. van Eijndhoven, D. Vannerom, J. van Santen, J. Veitch-Michaelis, S. Verpoest, C. Walck, W. Wang, T. B. Watson, C. Weaver, P. Weigel, A. Weindl, M. J. Weiss, J. Weldert, C. Wendt, J. Werthebach, M. Weyrauch, N. Whitehorn, C. H. Wiebusch, N. Willey, D. R. Williams, M. Wolf, G. Wrede, J. Wulff, X. W. Xu, J. P. Yanez, E. Yildizci, S. Yoshida, S. Yu, T. Yuan, Z. Zhang, P. Zhelnin</li>
<li>for: 本研究的目的是找到高能宇宙射线的起源。</li>
<li>methods: 这个研究使用机器学习技术分析了十年的冰穹神 neutrino telescope 数据，寻找宇宙射线的辐射。</li>
<li>results: 研究发现了来自银河平面的 neutrino 辐射，stats 相对于背景只是4.5$\sigma$ 的水平，但这些辐射也可能来自一群未能解决的点源。<details>
<summary>Abstract</summary>
The origin of high-energy cosmic rays, atomic nuclei that continuously impact Earth's atmosphere, has been a mystery for over a century. Due to deflection in interstellar magnetic fields, cosmic rays from the Milky Way arrive at Earth from random directions. However, near their sources and during propagation, cosmic rays interact with matter and produce high-energy neutrinos. We search for neutrino emission using machine learning techniques applied to ten years of data from the IceCube Neutrino Observatory. We identify neutrino emission from the Galactic plane at the 4.5$\sigma$ level of significance, by comparing diffuse emission models to a background-only hypothesis. The signal is consistent with modeled diffuse emission from the Galactic plane, but could also arise from a population of unresolved point sources.
</details>
<details>
<summary>摘要</summary>
高能宇宙射线的起源已经是一个谜题超过一个世纪。由于介电场的折射，宇宙射线从银河系 arrive at Earth from random directions。然而，在源头附近和传播过程中，宇宙射线与物质进行交互，生成高能中微子。我们使用机器学习技术对 IceCube 中微子观测器十年的数据进行搜索。我们在background-only假设下，在4.5σ水平上发现了来自银河平面的中微子发射。该信号与模型的散发辐射相一致，但也可能来自一群未解决的点源。
</details></li>
</ul>
<hr>
<h2 id="Handling-Group-Fairness-in-Federated-Learning-Using-Augmented-Lagrangian-Approach"><a href="#Handling-Group-Fairness-in-Federated-Learning-Using-Augmented-Lagrangian-Approach" class="headerlink" title="Handling Group Fairness in Federated Learning Using Augmented Lagrangian Approach"></a>Handling Group Fairness in Federated Learning Using Augmented Lagrangian Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04417">http://arxiv.org/abs/2307.04417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerry Windiarto Mohamad Dunda, Shenghui Song</li>
<li>for: 提高 Federated Learning 系统中群体公平性的方法</li>
<li>methods: 提出了一种基于 local differential privacy 和 client-side aggregation 的新 Federated Learning 算法，可以有效地提高群体公平性，同时具有较少的通信成本和负载。</li>
<li>results: 在 CelebA 和 ImSitu 数据集上实验表明，提出的方法可以在统计上不同的客户端数量和随机性下，Quantitatively 和 Qualitatively 提高群体公平性，同时具有较少的减少精度损失。<details>
<summary>Abstract</summary>
Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature. Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models might be biased towards sensitive factors such as race or gender, even if they are trained using a legally compliant process. To redress this concern, this paper proposes a novel FL algorithm designed explicitly to address group fairness issues. We show empirically on CelebA and ImSitu datasets that the proposed method can improve fairness both quantitatively and qualitatively with minimal loss in accuracy in the presence of statistical heterogeneity and with different numbers of clients. Besides improving fairness, the proposed FL algorithm is compatible with local differential privacy (LDP), has negligible communication costs, and results in minimal overhead when migrating existing FL systems from the common FL protocol such as FederatedAveraging (FedAvg). We also provide the theoretical convergence rate guarantee for the proposed algorithm and the required noise level of the Gaussian mechanism to achieve desired LDP. This innovative approach holds significant potential to enhance the fairness and effectiveness of FL systems, particularly in sensitive applications such as healthcare or criminal justice.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 已经引起了广泛的关注，主要是因为它具有隐私保护的特点。然而，由于用户数据的管理不具有完全的自由性，这可能会导致群体公平问题，模型可能会偏爱敏感因素 such as 种族或性别，即使使用了合法的过程进行训练。为了解决这个问题，这篇论文提出了一种新的 Federated Learning 算法，用于直接地 Addressing Group Fairness Issues。我们通过实验表明，在 celebA 和 ImSitu 数据集上，我们的方法可以改善公平性 both quantitatively and qualitatively，即使在统计不同性和不同客户端数量的情况下。此外，我们的 FL 算法兼容地方 differential privacy (LDP)，通信成本很低，并且在将现有 FL 系统从 Common FL Protocol such as FederatedAveraging (FedAvg) 迁移到我们的方法时，不会带来很大的干扰。我们还提供了对我们的算法的理论收敛率保证和需要的 Gaussian 机制的噪音水平来实现 Desired LDP。这种创新的方法可能会提高 FL 系统的公平性和效果，特别是在敏感应用 such as 医疗或刑事正义领域。
</details></li>
</ul>
<hr>
<h2 id="Episodic-Gaussian-Process-Based-Learning-Control-with-Vanishing-Tracking-Errors"><a href="#Episodic-Gaussian-Process-Based-Learning-Control-with-Vanishing-Tracking-Errors" class="headerlink" title="Episodic Gaussian Process-Based Learning Control with Vanishing Tracking Errors"></a>Episodic Gaussian Process-Based Learning Control with Vanishing Tracking Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04415">http://arxiv.org/abs/2307.04415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Armin Lederer, Jonas Umlauft, Sandra Hirche</li>
<li>for: 这个论文是为了解决技术系统中减少精细模型的问题，通过超visirotting机器学习来推断模型。</li>
<li>methods: 这篇论文使用了 Gaussian process regression，具有高数据效率和明确的uncertainty表示，从而 derivation of prediction error bounds。</li>
<li>results: 论文提出了一种 Bayesian prediction error bound，其与数据密度相关，并证明了时间变化的跟踪准确性保证。通过这种 bound，可以实现vanishing tracking error with increasing data density。<details>
<summary>Abstract</summary>
Due to the increasing complexity of technical systems, accurate first principle models can often not be obtained. Supervised machine learning can mitigate this issue by inferring models from measurement data. Gaussian process regression is particularly well suited for this purpose due to its high data-efficiency and its explicit uncertainty representation, which allows the derivation of prediction error bounds. These error bounds have been exploited to show tracking accuracy guarantees for a variety of control approaches, but their direct dependency on the training data is generally unclear. We address this issue by deriving a Bayesian prediction error bound for GP regression, which we show to decay with the growth of a novel, kernel-based measure of data density. Based on the prediction error bound, we prove time-varying tracking accuracy guarantees for learned GP models used as feedback compensation of unknown nonlinearities, and show to achieve vanishing tracking error with increasing data density. This enables us to develop an episodic approach for learning Gaussian process models, such that an arbitrary tracking accuracy can be guaranteed. The effectiveness of the derived theory is demonstrated in several simulations.
</details>
<details>
<summary>摘要</summary>
To address this issue, we derive a Bayesian prediction error bound for GP regression, which we show to decay with the growth of a novel, kernel-based measure of data density. Based on this bound, we prove time-varying tracking accuracy guarantees for learned GP models used as feedback compensation of unknown nonlinearities, and show that the tracking error vanishes as the data density increases. This enables us to develop an episodic approach for learning Gaussian process models, such that an arbitrary tracking accuracy can be guaranteed. The effectiveness of the derived theory is demonstrated through several simulations.
</details></li>
</ul>
<hr>
<h2 id="Comparison-of-Point-Cloud-and-Image-based-Models-for-Calorimeter-Fast-Simulation"><a href="#Comparison-of-Point-Cloud-and-Image-based-Models-for-Calorimeter-Fast-Simulation" class="headerlink" title="Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation"></a>Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04780">http://arxiv.org/abs/2307.04780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/viniciusmikuni/calo4eic">https://github.com/viniciusmikuni/calo4eic</a></li>
<li>paper_authors: Fernando Torales Acosta, Vinicius Mikuni, Benjamin Nachman, Miguel Arratia, Bishnu Karki, Ryan Milton, Piyush Karande, Aaron Angerami</li>
<li>for: 这个论文是为了研究Score基于生成模型，以准确地生成高维度calorimeter数据集。</li>
<li>methods: 这两个状态对比的Score基于模型都是使用同一个calorimeter simulate数据集训练的。</li>
<li>results: 研究发现，使用点云来表示calorimeter shower比使用3D voxels更加自然地处理稀疏数据集，并可以使用更加紧凑的模型和数据文件。<details>
<summary>Abstract</summary>
Score based generative models are a new class of generative models that have been shown to accurately generate high dimensional calorimeter datasets. Recent advances in generative models have used images with 3D voxels to represent and model complex calorimeter showers. Point clouds, however, are likely a more natural representation of calorimeter showers, particularly in calorimeters with high granularity. Point clouds preserve all of the information of the original simulation, more naturally deal with sparse datasets, and can be implemented with more compact models and data files. In this work, two state-of-the-art score based models are trained on the same set of calorimeter simulation and directly compared.
</details>
<details>
<summary>摘要</summary>
分数基于的生成模型是一种新的生成模型，已经证明可以准确地生成高维度的椭圆仪数据。最近的生成模型使用图像的3D矩阵来表示和模型复杂的椭圆仪涨潮。然而，点云更可能是椭圆仪涨潮的自然表示，特别是高精度的椭圆仪中。点云保留原始模拟中的所有信息，更自然地处理稀疏数据，并可以通过更 компакт的模型和数据文件实现。在这项工作中，两种当前领先的分数基于模型被直接对比训练。
</details></li>
</ul>
<hr>
<h2 id="CT-based-Subchondral-Bone-Microstructural-Analysis-in-Knee-Osteoarthritis-via-MR-Guided-Distillation-Learning"><a href="#CT-based-Subchondral-Bone-Microstructural-Analysis-in-Knee-Osteoarthritis-via-MR-Guided-Distillation-Learning" class="headerlink" title="CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning"></a>CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04390">http://arxiv.org/abs/2307.04390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jackhu-bme/srrd">https://github.com/jackhu-bme/srrd</a></li>
<li>paper_authors: Yuqi Hu, Xiangyu Zhao, Gaowei Qing, Kai Xie, Chenglei Liu, Lichi Zhang</li>
<li>For: The paper aims to develop a novel method for subchondral bone microstructural analysis using easily-acquired CT images, which can help diagnose knee osteoarthritis.* Methods: The proposed method, named SRRD, uses a distillation-learning-based approach to transfer MR structural information to a CT-based model, and leverages paired MR images to enhance the CT-based analysis model during training.* Results: The proposed method achieved high reliability and validity in MR-CT registration, regression, and knee osteoarthritis classification, with an AUC score of 0.767 (95% CI, 0.681-0.853) compared to 0.658 (95% CI, 0.574-0.742) using the CNN approach.<details>
<summary>Abstract</summary>
Background: MR-based subchondral bone effectively predicts knee osteoarthritis. However, its clinical application is limited by the cost and time of MR. Purpose: We aim to develop a novel distillation-learning-based method named SRRD for subchondral bone microstructural analysis using easily-acquired CT images, which leverages paired MR images to enhance the CT-based analysis model during training. Materials and Methods: Knee joint images of both CT and MR modalities were collected from October 2020 to May 2021. Firstly, we developed a GAN-based generative model to transform MR images into CT images, which was used to establish the anatomical correspondence between the two modalities. Next, we obtained numerous patches of subchondral bone regions of MR images, together with their trabecular parameters (BV / TV, Tb. Th, Tb. Sp, Tb. N) from the corresponding CT image patches via regression. The distillation-learning technique was used to train the regression model and transfer MR structural information to the CT-based model. The regressed trabecular parameters were further used for knee osteoarthritis classification. Results: A total of 80 participants were evaluated. CT-based regression results of trabecular parameters achieved intra-class correlation coefficients (ICCs) of 0.804, 0.773, 0.711, and 0.622 for BV / TV, Tb. Th, Tb. Sp, and Tb. N, respectively. The use of distillation learning significantly improved the performance of the CT-based knee osteoarthritis classification method using the CNN approach, yielding an AUC score of 0.767 (95% CI, 0.681-0.853) instead of 0.658 (95% CI, 0.574-0.742) (p<.001). Conclusions: The proposed SRRD method showed high reliability and validity in MR-CT registration, regression, and knee osteoarthritis classification, indicating the feasibility of subchondral bone microstructural analysis based on CT images.
</details>
<details>
<summary>摘要</summary>
背景：MR基于subchondral骨的效果可以预测膝关节炎，但其临床应用受到MR成本和时间的限制。目的：我们希望开发一种基于截然学习的方法，名为SRRD，用于CT图像上的subchondral骨微结构分析，利用MR图像来增强CT图像分析模型的训练。材料和方法：膝关节图像 Both CT和MR模式自2020年10月至2021年5月收集。首先，我们开发了一个GAN基于生成模型，将MR图像转换成CT图像，以建立两种模态之间的解剖相对性。接着，我们从MR图像中提取了许多subchondral骨区域的补丁，并将其与CT图像中的相应区域进行对比，以获取 trabecular参数（BV / TV、Tb. Th、Tb. Sp、Tb. N）的 regression。使用截然学习技术来训练回归模型，以将MR结构信息传递给CT基本的模型。重新计算的 trabecular参数被用于膝关节炎分类。结果：总共评估了80名参与者。CT基于 regression 结果中的 trabecular参数 achieved intra-class correlation coefficients (ICCs) of 0.804, 0.773, 0.711, and 0.622 for BV / TV, Tb. Th, Tb. Sp, and Tb. N, respectively。使用截然学习对CT基本的膝关节炎分类方法进行了显著改进，其AUC score为0.767（95% CI, 0.681-0.853），比之前的0.658（95% CI, 0.574-0.742）（p<.001）。结论：我们的SRRD方法在MR-CT регистраción、回归和膝关节炎分类中表现了高可靠性和有效性，表明基于CT图像的subchondral骨微结构分析是可能的。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Identify-Graphs-from-Node-Trajectories-in-Multi-Robot-Networks"><a href="#Learning-to-Identify-Graphs-from-Node-Trajectories-in-Multi-Robot-Networks" class="headerlink" title="Learning to Identify Graphs from Node Trajectories in Multi-Robot Networks"></a>Learning to Identify Graphs from Node Trajectories in Multi-Robot Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04374">http://arxiv.org/abs/2307.04374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Sebastian, Thai Duong, Nikolay Atanasov, Eduardo Montijano, Carlos Sagues</li>
<li>for:  Identifying the interactions among nodes in a network given their state&#x2F;feature trajectories.</li>
<li>methods: Combines a strongly convex program with a self-attention encoder to learn the graph topology and appropriate regularizers for optimization.</li>
<li>results: Can identify the graph topology of unseen networks with new configurations in terms of number of nodes, connectivity, or state trajectories, and demonstrates effectiveness in multi-robot formation and flocking tasks.Here’s the text in Simplified Chinese:</li>
<li>for:  Identifying网络中节点之间的互动关系，给出状态&#x2F;特征轨迹。</li>
<li>methods: 组合强型凸程程序和自注意编码器，学习网络架构和优化适当的正则化项。</li>
<li>results: 可以 Identify未看到的网络架构，包括节点数量、连接性和状态轨迹等新配置，并在多机器formation和群集任务中表现效果。<details>
<summary>Abstract</summary>
The graph identification problem consists of discovering the interactions among nodes in a network given their state/feature trajectories. This problem is challenging because the behavior of a node is coupled to all the other nodes by the unknown interaction model. Besides, high-dimensional and nonlinear state trajectories make difficult to identify if two nodes are connected. Current solutions rely on prior knowledge of the graph topology and the dynamic behavior of the nodes, and hence, have poor generalization to other network configurations. To address these issues, we propose a novel learning-based approach that combines (i) a strongly convex program that efficiently uncovers graph topologies with global convergence guarantees and (ii) a self-attention encoder that learns to embed the original state trajectories into a feature space and predicts appropriate regularizers for the optimization program. In contrast to other works, our approach can identify the graph topology of unseen networks with new configurations in terms of number of nodes, connectivity or state trajectories. We demonstrate the effectiveness of our approach in identifying graphs in multi-robot formation and flocking tasks.
</details>
<details>
<summary>摘要</summary>
《网络图标识问题》的解决方法是找出网络中节点之间的交互关系， giventheir状态/特征轨迹。这个问题具有挑战性，因为每个节点的行为都是所有其他节点的未知交互模型的各自响应。此外，高维和非线性的状态轨迹使得判断两个节点是否连接的很困难。现有的解决方案均基于节点的动态行为和网络拓扑结构的先验知识，因此具有poor泛化性。为了解决这些问题，我们提出了一种新的学习型方法，其包括（i）一个强 convex 程序，可以高效地揭示网络拓扑结构，并且具有全局收敛保证；（ii）一个自注意编码器，可以将原始状态轨迹编码到特征空间中，并预测适当的正则化项来优化程序。与其他工作不同，我们的方法可以在未看到的网络配置下，适应新的节点数、连接性和状态轨迹。我们在多机器人formation和群集任务中证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning-A-Survey"><a href="#Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning-A-Survey" class="headerlink" title="Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey"></a>Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04370">http://arxiv.org/abs/2307.04370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pranav-chib/recent-advancements-in-end-to-end-autonomous-driving-using-deep-learning">https://github.com/pranav-chib/recent-advancements-in-end-to-end-autonomous-driving-using-deep-learning</a></li>
<li>paper_authors: Pranav Singh Chib, Pravendra Singh</li>
<li>for: 本研究旨在提供一份涵盖整个自动驾驶栈的全面回顾，探讨了深度学习在自动驾驶中的应用，以及适应实际应用环境中的挑战。</li>
<li>methods: 本研究使用了深度学习来实现自动驾驶栈的全面控制，包括感知到控制的全流程，并Addressed key challenges in real-world applications, such as explainability and safety aspects.</li>
<li>results: 本研究对自动驾驶领域的最新发展进行了分类和评估，并提供了一个包含最新开源实现的GitHub仓库。<details>
<summary>Abstract</summary>
End-to-End driving is a promising paradigm as it circumvents the drawbacks associated with modular systems, such as their overwhelming complexity and propensity for error propagation. Autonomous driving transcends conventional traffic patterns by proactively recognizing critical events in advance, ensuring passengers' safety and providing them with comfortable transportation, particularly in highly stochastic and variable traffic settings. This paper presents a comprehensive review of the End-to-End autonomous driving stack. It provides a taxonomy of automated driving tasks wherein neural networks have been employed in an End-to-End manner, encompassing the entire driving process from perception to control, while addressing key challenges encountered in real-world applications. Recent developments in End-to-End autonomous driving are analyzed, and research is categorized based on underlying principles, methodologies, and core functionality. These categories encompass sensorial input, main and auxiliary output, learning approaches ranging from imitation to reinforcement learning, and model evaluation techniques. The survey incorporates a detailed discussion of the explainability and safety aspects. Furthermore, it assesses the state-of-the-art, identifies challenges, and explores future possibilities. We maintained the latest advancements and their corresponding open-source implementations at https://github.com/Pranav-chib/Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning.
</details>
<details>
<summary>摘要</summary>
END-TO-END自驾系统是一种优秀的思想，因为它绕过模块化系统的缺点，如它们的复杂性和错误卷积。自驾系统超越了传统的交通模式，通过积极地预测重要事件，确保乘客的安全和提供舒适的交通方式，特别是在高度随机和变化的交通环境中。这篇论文提供了END-TO-END自驾栈的全面回顾。它提供了自动驾驶任务中使用神经网络的End-to-End方式，涵盖整个驾驶过程从感知到控制，并解决了实际应用中遇到的主要挑战。最新的END-TO-END自驾技术发展分析，并按照基本原理、方法论和核心功能分类。这些类别包括感知输入、主要和辅助输出、学习方法从模仿到强化学习，以及模型评估技术。文章还包括对解释性和安全方面的详细讨论，以及现状、挑战和未来可能性的评估。我们将latestadvances和它们相应的开源实现存储在https://github.com/Pranav-chib/Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning上。
</details></li>
</ul>
<hr>
<h2 id="ECS-–-an-Interactive-Tool-for-Data-Quality-Assurance"><a href="#ECS-–-an-Interactive-Tool-for-Data-Quality-Assurance" class="headerlink" title="ECS – an Interactive Tool for Data Quality Assurance"></a>ECS – an Interactive Tool for Data Quality Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04368">http://arxiv.org/abs/2307.04368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Sieberichs, Simon Geerkens, Alexander Braun, Thomas Waschulzik</li>
<li>for: 这项研究旨在提高机器学习系统的可靠性和安全性，通过确保数据质量高。</li>
<li>methods: 该paper提出了一种新的数据质量保证方法，通过数学基础和多个示例来检测数据中可能有害的特性。</li>
<li>results: 该方法可以检测出可能对安全系统具有害的数据点，从而提高机器学习系统的可靠性和安全性。<details>
<summary>Abstract</summary>
With the increasing capabilities of machine learning systems and their potential use in safety-critical systems, ensuring high-quality data is becoming increasingly important. In this paper we present a novel approach for the assurance of data quality. For this purpose, the mathematical basics are first discussed and the approach is presented using multiple examples. This results in the detection of data points with potentially harmful properties for the use in safety-critical systems.
</details>
<details>
<summary>摘要</summary>
随着机器学习系统的能力不断提高，它们的潜在应用在安全关键系统中也在增加。为此，保证数据质量的重要性也在增加。在这篇论文中，我们提出了一种新的数据质量保证方法。为此，我们首先介绍了数学基础，然后通过多个示例介绍了该方法。这将导致检测数据点中可能有害的特性，因此不适用于安全关键系统的使用。
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Pruning-for-Fast-adapting-Pre-trained-Models-on-Devices"><a href="#One-Shot-Pruning-for-Fast-adapting-Pre-trained-Models-on-Devices" class="headerlink" title="One-Shot Pruning for Fast-adapting Pre-trained Models on Devices"></a>One-Shot Pruning for Fast-adapting Pre-trained Models on Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04365">http://arxiv.org/abs/2307.04365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiyan Zhao, Guodong Long</li>
<li>for: 这篇论文旨在解决大规模预训练模型在低能力设备上部署的问题，通过实现一种可扩展的一击适束方法。</li>
<li>methods: 方法是使用相似任务中删除过的模型删除知识，创建任务特有的权重对，以对原始模型进行删除，从而提取适当大小的子网络，并将其适束到新任务。</li>
<li>results: 实验分析显示，提案方法在各种数据集和任务下，与各种删除基准方法进行比较，具有较高的精度和效率。<details>
<summary>Abstract</summary>
Large-scale pre-trained models have been remarkably successful in resolving downstream tasks. Nonetheless, deploying these models on low-capability devices still requires an effective approach, such as model pruning. However, pruning the model from scratch can pose a practical challenge given the limited resources of each downstream task or device. To tackle this issue, we present a scalable one-shot pruning method that leverages pruned knowledge of similar tasks to extract a sub-network from the pre-trained model for a new task. Specifically, we create a score mask using the pruned models of similar tasks to identify task-specific filters/nodes in the pre-trained model for the new task. Based on this mask, we conduct a single round of pruning to extract a suitably-sized sub-network that can quickly adapt to the new task with only a few training iterations. Our experimental analysis demonstrates the effectiveness of the proposed method on the convolutional neural networks (CNNs) and vision transformers (ViT) with various datasets. The proposed method consistently outperforms popular pruning baseline methods in terms of accuracy and efficiency when dealing with diverse downstream tasks with different memory constraints.
</details>
<details>
<summary>摘要</summary>
Specifically, we create a score mask using the pruned models of similar tasks to identify task-specific filters/nodes in the pre-trained model for the new task. Based on this mask, we conduct a single round of pruning to extract a suitably-sized sub-network that can quickly adapt to the new task with only a few training iterations. Our experimental analysis demonstrates the effectiveness of the proposed method on convolutional neural networks (CNNs) and vision transformers (ViT) with various datasets. The proposed method consistently outperforms popular pruning baseline methods in terms of accuracy and efficiency when dealing with diverse downstream tasks with different memory constraints.
</details></li>
</ul>
<hr>
<h2 id="False-Sense-of-Security-Leveraging-XAI-to-Analyze-the-Reasoning-and-True-Performance-of-Context-less-DGA-Classifiers"><a href="#False-Sense-of-Security-Leveraging-XAI-to-Analyze-the-Reasoning-and-True-Performance-of-Context-less-DGA-Classifiers" class="headerlink" title="False Sense of Security: Leveraging XAI to Analyze the Reasoning and True Performance of Context-less DGA Classifiers"></a>False Sense of Security: Leveraging XAI to Analyze the Reasoning and True Performance of Context-less DGA Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04358">http://arxiv.org/abs/2307.04358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/rwth-itsec/explainability-analyzed-dga-models">https://gitlab.com/rwth-itsec/explainability-analyzed-dga-models</a></li>
<li>paper_authors: Arthur Drichel, Ulrike Meyer</li>
<li>for: 本研究旨在解决使用Domain Generation Algorithm（DGA）检测 botnet 活动时存在的假 positives 问题，通过使用深度学习分类器达到精度超过 99.9%。</li>
<li>methods: 本研究使用 Explainable Artificial Intelligence（XAI）方法分析深度学习分类器的 reasoning，并系统地揭露了这些分类器的偏见。</li>
<li>results:  eliminating 偏见后，DGA 分类器的性能明显下降，但我们提出了一种 context-aware 检测系统，能够维持 state-of-the-art 深度学习分类器的检测率，并提供了一种可视化分析系统，帮助更好地理解分类器的 reasoning，从而提高检测方法的信任和透明度。<details>
<summary>Abstract</summary>
The problem of revealing botnet activity through Domain Generation Algorithm (DGA) detection seems to be solved, considering that available deep learning classifiers achieve accuracies of over 99.9%. However, these classifiers provide a false sense of security as they are heavily biased and allow for trivial detection bypass. In this work, we leverage explainable artificial intelligence (XAI) methods to analyze the reasoning of deep learning classifiers and to systematically reveal such biases. We show that eliminating these biases from DGA classifiers considerably deteriorates their performance. Nevertheless we are able to design a context-aware detection system that is free of the identified biases and maintains the detection rate of state-of-the art deep learning classifiers. In this context, we propose a visual analysis system that helps to better understand a classifier's reasoning, thereby increasing trust in and transparency of detection methods and facilitating decision-making.
</details>
<details>
<summary>摘要</summary>
“botnet活动探测透过类别生成算法（DGA）探测的问题似乎已经解决了，因为可用的深度学习分类器可以达到99.9%以上的准确率。但是这些分类器具有严重的偏见，容易被轻松地逃脱检测。在这个工作中，我们利用可解释人工智能（XAI）方法来分析深度学习分类器的思考过程，并系统地揭露这些偏见。我们发现，从DGA分类器中除掉这些偏见后，其性能会很差。但我们能够设计一个具有上述偏见的 контекст感知检测系统，并维持现有深度学习分类器的检测率。在这个上下文中，我们提出了一个可观分析系统，帮助更好地理解分类器的思考过程，从而增加检测方法的信任和透明度，并且方便决策。”
</details></li>
</ul>
<hr>
<h2 id="Formulating-A-Strategic-Plan-Based-On-Statistical-Analyses-And-Applications-For-Financial-Companies-Through-A-Real-World-Use-Case"><a href="#Formulating-A-Strategic-Plan-Based-On-Statistical-Analyses-And-Applications-For-Financial-Companies-Through-A-Real-World-Use-Case" class="headerlink" title="Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case"></a>Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04778">http://arxiv.org/abs/2307.04778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saman Sarraf</li>
<li>for: 这篇论文是为了提出一个基于统计分析的战略计划，以帮助金融公司LendingClub实施数据驱动的决策过程。</li>
<li>methods: 论文使用了不同的统计分析方法，包括机器学习技术，来构建更好的总体数据驱动预测模型。</li>
<li>results: 研究发现，贷款数量对borrower债务抵押率有很大影响，而提出的战略计划可以帮助LendingClub提高收入，同时减少贷款风险。<details>
<summary>Abstract</summary>
Business statistics play a crucial role in implementing a data-driven strategic plan at the enterprise level to employ various analytics where the outcomes of such a plan enable an enterprise to enhance the decision-making process or to mitigate risks to the organization. In this work, a strategic plan informed by the statistical analysis is introduced for a financial company called LendingClub, where the plan is comprised of exploring the possibility of onboarding a big data platform along with advanced feature selection capacities. The main objectives of such a plan are to increase the company's revenue while reducing the risks of granting loans to borrowers who cannot return their loans. In this study, different hypotheses formulated to address the company's concerns are studied, where the results reveal that the amount of loans profoundly impacts the number of borrowers charging off their loans. Also, the proposed strategic plan includes onboarding advanced analytics such as machine learning technologies that allow the company to build better generalized data-driven predictive models.
</details>
<details>
<summary>摘要</summary>
企业统计在实施数据驱动策略方面发挥关键作用，以使用不同的统计分析来帮助企业做出更好的决策或减少对组织的风险。在这个工作中，我们将介绍一份基于统计分析的战略计划，用于一家名为LendingClub的金融公司，以增加公司的收入，同时减少向借款者发放贷款的风险。在这个研究中，我们提出了一些用于解决公司的问题的假设，其结果表明，贷款的数量对借款者养成贷款的数量产生很大的影响。此外，我们的战略计划还包括在boarding高级分析工具，如机器学习技术，以帮助公司建立更好的通用数据驱动预测模型。
</details></li>
</ul>
<hr>
<h2 id="Policy-Finetuning-in-Reinforcement-Learning-via-Design-of-Experiments-using-Offline-Data"><a href="#Policy-Finetuning-in-Reinforcement-Learning-via-Design-of-Experiments-using-Offline-Data" class="headerlink" title="Policy Finetuning in Reinforcement Learning via Design of Experiments using Offline Data"></a>Policy Finetuning in Reinforcement Learning via Design of Experiments using Offline Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04354">http://arxiv.org/abs/2307.04354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiqi Zhang, Andrea Zanette</li>
<li>for: 本研究是用于提高强化学习策略质量的数据采集和处理方法。</li>
<li>methods: 本研究使用了一种可证明的算法，利用已有的离线数据集来设计单一不反应的探索策略。</li>
<li>results: 研究表明，该算法可以在离线数据集的本地覆盖率和额外数据量的情况下提供可证明的质量保证。<details>
<summary>Abstract</summary>
In some applications of reinforcement learning, a dataset of pre-collected experience is already available but it is also possible to acquire some additional online data to help improve the quality of the policy. However, it may be preferable to gather additional data with a single, non-reactive exploration policy and avoid the engineering costs associated with switching policies.   In this paper we propose an algorithm with provable guarantees that can leverage an offline dataset to design a single non-reactive policy for exploration. We theoretically analyze the algorithm and measure the quality of the final policy as a function of the local coverage of the original dataset and the amount of additional data collected.
</details>
<details>
<summary>摘要</summary>
在某些应用中的强化学习中，已经有一个准备好的经验数据集，但也可以获得一些在线数据来提高政策质量。然而，可能更好的选择是使用单一、不反应的探索策略来收集更多数据，而不是在政策之间切换。在这篇论文中，我们提出了一个可证明的 garantía 的算法，可以利用已有的离线数据集来设计单一的不反应策略。我们对算法进行了理论分析，并测量了最终政策质量与原始数据集的地方覆盖率以及额外收集的数据量之间的关系。
</details></li>
</ul>
<hr>
<h2 id="On-Sufficient-Graphical-Models"><a href="#On-Sufficient-Graphical-Models" class="headerlink" title="On Sufficient Graphical Models"></a>On Sufficient Graphical Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04353">http://arxiv.org/abs/2307.04353</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Beuleup93/ProjetPythonSISE">https://github.com/Beuleup93/ProjetPythonSISE</a></li>
<li>paper_authors: Bing Li, Kyongwon Kim</li>
<li>For: This paper is written for researchers and practitioners who work with high-dimensional data and are interested in evaluating conditional independence in a nonparametric manner.* Methods: The paper uses recently developed nonlinear sufficient dimension reduction techniques to introduce a sufficient graphical model for evaluating conditional independence. The model is nonparametric and does not make distributional assumptions, but it is based on conditional independence given a set of sufficient predictors with a reduced dimension.* Results: The paper demonstrates that the proposed method outperforms existing methods when the Gaussian or copula Gaussian assumptions are violated, and its performance remains excellent in high-dimensional settings. The method is also shown to be consistent in variable selection.<details>
<summary>Abstract</summary>
We introduce a sufficient graphical model by applying the recently developed nonlinear sufficient dimension reduction techniques to the evaluation of conditional independence. The graphical model is nonparametric in nature, as it does not make distributional assumptions such as the Gaussian or copula Gaussian assumptions. However, unlike a fully nonparametric graphical model, which relies on the high-dimensional kernel to characterize conditional independence, our graphical model is based on conditional independence given a set of sufficient predictors with a substantially reduced dimension. In this way we avoid the curse of dimensionality that comes with a high-dimensional kernel. We develop the population-level properties, convergence rate, and variable selection consistency of our estimate. By simulation comparisons and an analysis of the DREAM 4 Challenge data set, we demonstrate that our method outperforms the existing methods when the Gaussian or copula Gaussian assumptions are violated, and its performance remains excellent in the high-dimensional setting.
</details>
<details>
<summary>摘要</summary>
我们引入一种充分的图形模型，通过最近发展的非线性充分维度减少技术来评估条件独立性。这个图形模型是非 Parametric 性的，意味着它不会对条件独立性进行分布假设，如 Gaussian 或 copula Gaussian 假设。然而，与完全非 Parametric 图形模型不同，我们的图形模型基于条件独立性给定一组充分的预测变量，具有减少维度的优点。这样可以避免高维度 kernel 中的困惑。我们研究这个方法的人口级特性、整体速度和变量选择一致性。通过对比实验和分析 DREAM 4 Challenge 数据集，我们示出了我们的方法在假设不满足 Gaussian 或 copula Gaussian 假设时表现出色，并在高维度设置中保持出色的表现。
</details></li>
</ul>
<hr>
<h2 id="MD-HIT-Machine-learning-for-materials-property-prediction-with-dataset-redundancy-control"><a href="#MD-HIT-Machine-learning-for-materials-property-prediction-with-dataset-redundancy-control" class="headerlink" title="MD-HIT: Machine learning for materials property prediction with dataset redundancy control"></a>MD-HIT: Machine learning for materials property prediction with dataset redundancy control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04351">http://arxiv.org/abs/2307.04351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/usccolumbia/md-hit">https://github.com/usccolumbia/md-hit</a></li>
<li>paper_authors: Qin Li, Nihang Fu, Sadman Sadeed Omee, Jianjun Hu</li>
<li>For: 本研究旨在解决物料数据集中的重复样本问题，以提高物料预测性能的准确性。* Methods: 本文提出了一种物料数据集重复样本减少算法（MD-HIT），并对其进行了评估。* Results: 研究表明，通过使用 MD-HIT 减少样本重复，可以更好地反映物料预测性能的准确性。<details>
<summary>Abstract</summary>
Materials datasets are usually featured by the existence of many redundant (highly similar) materials due to the tinkering material design practice over the history of materials research. For example, the materials project database has many perovskite cubic structure materials similar to SrTiO$_3$. This sample redundancy within the dataset makes the random splitting of machine learning model evaluation to fail so that the ML models tend to achieve over-estimated predictive performance which is misleading for the materials science community. This issue is well known in the field of bioinformatics for protein function prediction, in which a redundancy reduction procedure (CD-Hit) is always applied to reduce the sample redundancy by ensuring no pair of samples has a sequence similarity greater than a given threshold. This paper surveys the overestimated ML performance in the literature for both composition based and structure based material property prediction. We then propose a material dataset redundancy reduction algorithm called MD-HIT and evaluate it with several composition and structure based distance threshold sfor reducing data set sample redundancy. We show that with this control, the predicted performance tends to better reflect their true prediction capability. Our MD-hit code can be freely accessed at https://github.com/usccolumbia/MD-HIT
</details>
<details>
<summary>摘要</summary>
In this paper, we survey the overestimated ML performance in the literature for both composition-based and structure-based material property prediction. We then propose a material dataset redundancy reduction algorithm called MD-HIT and evaluate it with several composition and structure-based distance thresholds for reducing data set sample redundancy. Our results show that with this control, the predicted performance tends to better reflect their true prediction capability. The MD-hit code can be freely accessed at https://github.com/usccolumbia/MD-HIT.
</details></li>
</ul>
<hr>
<h2 id="RLTF-Reinforcement-Learning-from-Unit-Test-Feedback"><a href="#RLTF-Reinforcement-Learning-from-Unit-Test-Feedback" class="headerlink" title="RLTF: Reinforcement Learning from Unit Test Feedback"></a>RLTF: Reinforcement Learning from Unit Test Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04349">http://arxiv.org/abs/2307.04349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zyq-scut/rltf">https://github.com/zyq-scut/rltf</a></li>
<li>paper_authors: Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, Deheng Ye</li>
<li>for: 提高大型自然语言模型（LLM）的代码生成性能。</li>
<li>methods: 使用强化学习（RL）方法，并在训练过程中接受多级别单元测试反馈。</li>
<li>results: 在APPS和MBPPbenchmark上实现了状态的最佳性能。<details>
<summary>Abstract</summary>
The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, these RL methods have only used offline frameworks, limiting their exploration of new sample spaces. Additionally, current approaches that utilize unit test signals are rather simple, not accounting for specific error locations within the code. To address these issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code can be found at: https://github.com/Zyq-scut/RLTF.
</details>
<details>
<summary>摘要</summary>
目标是使程序生成器、代码生成器或代码生成器可以根据给定的描述生成可执行的代码。近些年来，有越来越多的研究使用强化学习（RL）来提高大型自然语言模型（LLM）的代码生成性能。然而，这些RL方法只使用了离线框架，限制了新样本空间的探索。此外，现有的使用单元测试信号的方法相对简单，没有考虑特定的错误位置在代码中。为解决这些问题，我们提出了RLTF，即基于单元测试反馈的强化学习框架。我们的方法在训练过程中生成数据并同时使用多级别的反馈信号来引导模型生成更高质量的代码。我们的实验表明，RLTF可以在APPS和MBPPbenchmark上达到当前最佳性能。我们的代码可以在github上找到：https://github.com/Zyq-scut/RLTF。
</details></li>
</ul>
<hr>
<h2 id="Injecting-Logical-Constraints-into-Neural-Networks-via-Straight-Through-Estimators"><a href="#Injecting-Logical-Constraints-into-Neural-Networks-via-Straight-Through-Estimators" class="headerlink" title="Injecting Logical Constraints into Neural Networks via Straight-Through Estimators"></a>Injecting Logical Constraints into Neural Networks via Straight-Through Estimators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04347">http://arxiv.org/abs/2307.04347</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/azreasoners/cl-ste">https://github.com/azreasoners/cl-ste</a></li>
<li>paper_authors: Zhun Yang, Joohyung Lee, Chiyoun Park</li>
<li>for: 将逻辑约束直接注入到神经网络学习中</li>
<li>methods: 使用直通估计器将逻辑约束转化为损失函数，使神经网络通过梯度下降更新参数，以满足逻辑约束</li>
<li>results: 在使用GPU和批处理训练时，方法可以规模更好地比较现有的神经符号计算方法，同时可以在不同类型的神经网络上学习，无需或减少标注数据<details>
<summary>Abstract</summary>
Injecting discrete logical constraints into neural network learning is one of the main challenges in neuro-symbolic AI. We find that a straight-through-estimator, a method introduced to train binary neural networks, could effectively be applied to incorporate logical constraints into neural network learning. More specifically, we design a systematic way to represent discrete logical constraints as a loss function; minimizing this loss using gradient descent via a straight-through-estimator updates the neural network's weights in the direction that the binarized outputs satisfy the logical constraints. The experimental results show that by leveraging GPUs and batch training, this method scales significantly better than existing neuro-symbolic methods that require heavy symbolic computation for computing gradients. Also, we demonstrate that our method applies to different types of neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer labeled data by learning directly from known constraints.
</details>
<details>
<summary>摘要</summary>
injecting discrete logical constraints into neural network learning is one of the main challenges in neuro-symbolic AI. we find that a straight-through-estimator, a method introduced to train binary neural networks, could effectively be applied to incorporate logical constraints into neural network learning. more specifically, we design a systematic way to represent discrete logical constraints as a loss function; minimizing this loss using gradient descent via a straight-through-estimator updates the neural network's weights in the direction that the binarized outputs satisfy the logical constraints. the experimental results show that by leveraging GPUs and batch training, this method scales significantly better than existing neuro-symbolic methods that require heavy symbolic computation for computing gradients. also, we demonstrate that our method applies to different types of neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer labeled data by learning directly from known constraints.Here's the translation in Traditional Chinese as well:injecting discrete logical constraints into neural network learning is one of the main challenges in neuro-symbolic AI. we find that a straight-through-estimator, a method introduced to train binary neural networks, could effectively be applied to incorporate logical constraints into neural network learning. more specifically, we design a systematic way to represent discrete logical constraints as a loss function; minimizing this loss using gradient descent via a straight-through-estimator updates the neural network's weights in the direction that the binarized outputs satisfy the logical constraints. the experimental results show that by leveraging GPUs and batch training, this method scales significantly better than existing neuro-symbolic methods that require heavy symbolic computation for computing gradients. also, we demonstrate that our method applies to different types of neural networks, such as MLP, CNN, and GNN, making them learn with no or fewer labeled data by learning directly from known constraints.
</details></li>
</ul>
<hr>
<h2 id="Continual-Learning-as-Computationally-Constrained-Reinforcement-Learning"><a href="#Continual-Learning-as-Computationally-Constrained-Reinforcement-Learning" class="headerlink" title="Continual Learning as Computationally Constrained Reinforcement Learning"></a>Continual Learning as Computationally Constrained Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04345">http://arxiv.org/abs/2307.04345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, Benjamin Van Roy</li>
<li>for: 本研究旨在开发一种能够高效储存知识，逐渐提高人工智能能力的智能代理人。</li>
<li>methods: 本文提出了一种概念和工具集，用于探讨智能代理人的持续学习问题。</li>
<li>results: 本研究提供了一个明确和正式的定义和框架，以促进持续学习领域的进一步研究。<details>
<summary>Abstract</summary>
An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities. The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning. This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research.
</details>
<details>
<summary>摘要</summary>
一个智能代理人，能够有效地积累知识，开发越来越复杂的技能，可能会推动人工智能技能的前沿。该代理人的设计，仍然是人工智能领域的长期挑战。本论文将 clarify和正式化持续学习的概念，提出一个框架和一组工具，以促进更多的研究。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-Graph-Machine-Learning-from-Data-to-Computation-A-Survey"><a href="#Privacy-Preserving-Graph-Machine-Learning-from-Data-to-Computation-A-Survey" class="headerlink" title="Privacy-Preserving Graph Machine Learning from Data to Computation: A Survey"></a>Privacy-Preserving Graph Machine Learning from Data to Computation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04338">http://arxiv.org/abs/2307.04338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongqi Fu, Wenxuan Bao, Ross Maciejewski, Hanghang Tong, Jingrui He</li>
<li>for: 这篇论文主要关注于保护敏感信息的图机器学习隐私技术。</li>
<li>methods: 论文系统地回顾了图数据生成的隐私技术，以及在多方合作下传输隐私保护的图模型参数的方法。</li>
<li>results: 论文详细介绍了现有的隐私保护技术和软件工具，同时也提出了未来研究的挑战和可能性。最终，论文概述了一个简单、通用的安全图机器学习系统。<details>
<summary>Abstract</summary>
In graph machine learning, data collection, sharing, and analysis often involve multiple parties, each of which may require varying levels of data security and privacy. To this end, preserving privacy is of great importance in protecting sensitive information. In the era of big data, the relationships among data entities have become unprecedentedly complex, and more applications utilize advanced data structures (i.e., graphs) that can support network structures and relevant attribute information. To date, many graph-based AI models have been proposed (e.g., graph neural networks) for various domain tasks, like computer vision and natural language processing. In this paper, we focus on reviewing privacy-preserving techniques of graph machine learning. We systematically review related works from the data to the computational aspects. We first review methods for generating privacy-preserving graph data. Then we describe methods for transmitting privacy-preserved information (e.g., graph model parameters) to realize the optimization-based computation when data sharing among multiple parties is risky or impossible. In addition to discussing relevant theoretical methodology and software tools, we also discuss current challenges and highlight several possible future research opportunities for privacy-preserving graph machine learning. Finally, we envision a unified and comprehensive secure graph machine learning system.
</details>
<details>
<summary>摘要</summary>
在图机器学习中，数据采集、分享和分析经常涉及多方面，每个方面可能需要不同的数据安全和隐私保护。因此，保护隐私是图机器学习中非常重要的一环。在大数据时代，数据之间的关系变得无前例地复杂，更多的应用利用高级数据结构（即图）来支持网络结构和相关属性信息。到目前为止，许多基于图的AI模型（如图神经网络）已经被提出，用于各种领域任务，如计算机视觉和自然语言处理。在本文中，我们关注图机器学习中的隐私保护技术的评审。我们系统地回顾相关的数据和计算方面的方法。我们首先回顾如何生成隐私保护的图数据。然后我们描述如何将隐私保持的信息（如图模型参数）传输，以实现在多方共享数据时的优化计算。除了讨论相关的理论方法和软件工具外，我们还讨论当前的挑战和可能的未来研究机会。最后，我们拟定了一个统一和完整的安全图机器学习系统。
</details></li>
</ul>
<hr>
<h2 id="Source-Aware-Embedding-Training-on-Heterogeneous-Information-Networks"><a href="#Source-Aware-Embedding-Training-on-Heterogeneous-Information-Networks" class="headerlink" title="Source-Aware Embedding Training on Heterogeneous Information Networks"></a>Source-Aware Embedding Training on Heterogeneous Information Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04336">http://arxiv.org/abs/2307.04336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tsai Hor Chan, Chi Ho Wong, Jiajun Shen, Guosheng Yin</li>
<li>for: 这篇论文旨在提出一种可扩展的无监督多源异构信息网络嵌入方法，以解决现有的异构信息网络嵌入方法忽略多个来源子图的分布差异问题。</li>
<li>methods: 该方法基于一种新的嵌入空间分解技术，可以适应不同来源的异构信息网络，并通过一种扩展的聚合推理来将各个来源的嵌入空间调整到同一个空间中。</li>
<li>results: 实验结果表明，SUMSHINE方法可以在实际世界数据集上达到现有方法的性能水平，同时具有更好的扩展性和灵活性。<details>
<summary>Abstract</summary>
Heterogeneous information networks (HINs) have been extensively applied to real-world tasks, such as recommendation systems, social networks, and citation networks. While existing HIN representation learning methods can effectively learn the semantic and structural features in the network, little awareness was given to the distribution discrepancy of subgraphs within a single HIN. However, we find that ignoring such distribution discrepancy among subgraphs from multiple sources would hinder the effectiveness of graph embedding learning algorithms. This motivates us to propose SUMSHINE (Scalable Unsupervised Multi-Source Heterogeneous Information Network Embedding) -- a scalable unsupervised framework to align the embedding distributions among multiple sources of an HIN. Experimental results on real-world datasets in a variety of downstream tasks validate the performance of our method over the state-of-the-art heterogeneous information network embedding algorithms.
</details>
<details>
<summary>摘要</summary>
各种多源多类网络（HINs）已经广泛应用于现实世界任务中，如推荐系统、社交网络和引用网络。 existing HIN表示学习方法可以有效学习网络的semantic和结构特征，但是忽略了各个子图在单个HIN中的分布差异。然而，我们发现忽略这些分布差异会降低图像学习算法的效果。这种情况 Motivates我们提出 SUMSHINE（可扩展无监督多源多类网络嵌入）——一种可扩展无监督框架，用于对多个HIN的嵌入分布进行对齐。实验结果表明，我们的方法在真实世界数据上的多种下游任务中表现出色，胜过现状的多种多样化网络嵌入算法。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Adversarial-Robustness-via-Score-Based-Optimization"><a href="#Enhancing-Adversarial-Robustness-via-Score-Based-Optimization" class="headerlink" title="Enhancing Adversarial Robustness via Score-Based Optimization"></a>Enhancing Adversarial Robustness via Score-Based Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04333">http://arxiv.org/abs/2307.04333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boya Zhang, Weijian Luo, Zhihua Zhang</li>
<li>for: 本研究旨在提出一种新的防御机制，以提高深度神经网络分类器对势力攻击的抵抗能力。</li>
<li>methods: 该方法使用分Diffusion模型来优化对抗样本，以便在测试时更好地抵抗势力攻击。</li>
<li>results: 实验结果表明，ScoreOpt方法可以在多个数据集上（包括CIFAR10、CIFAR100和ImageNet）击败现有的防御方法，both in terms of robustness performance和推理速度。<details>
<summary>Abstract</summary>
Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.
</details>
<details>
<summary>摘要</summary>
深度神经网络分类器可能会被抗击攻击诱导，引入微小的扰动。为确保人工智能的安全使用，开发有效的抗击攻击算法是非常重要。latest studies suggest that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.Here's the translation in Traditional Chinese:深度神经网络分类器可能会被抗击攻击诱导，引入微小的扰动。为确保人工智能的安全使用，开发有效的抗击攻击算法是非常重要。latest studies suggest that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Multiple-Descriptive-Features-for-Robust-Few-shot-Image-Learning"><a href="#Leveraging-Multiple-Descriptive-Features-for-Robust-Few-shot-Image-Learning" class="headerlink" title="Leveraging Multiple Descriptive Features for Robust Few-shot Image Learning"></a>Leveraging Multiple Descriptive Features for Robust Few-shot Image Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04317">http://arxiv.org/abs/2307.04317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Feng, Anna Bair, J. Zico Kolter</li>
<li>for: 这篇论文旨在提出一种新的图像分类方法，以便更好地评估图像分类决策中的视觉特征。</li>
<li>methods: 这篇论文使用了一种大型语言模型（LLM）来自动生成每个分类的多个视觉描述，然后使用视图图像模型将这些描述翻译成每个图像的多个视觉特征。最后，使用稀疏逻辑回归选择每个图像的相关特征进行分类。</li>
<li>results:  compared to标准方法such as linear probing, this method outperforms in the few-shot learning setting, and when combined with fine-tuning, it also outperforms existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance.<details>
<summary>Abstract</summary>
Modern image classification is based upon directly predicting model classes via large discriminative networks, making it difficult to assess the intuitive visual ``features'' that may constitute a classification decision. At the same time, recent works in joint visual language models such as CLIP provide ways to specify natural language descriptions of image classes but typically focus on providing single descriptions for each class. In this work, we demonstrate that an alternative approach, arguably more akin to our understanding of multiple ``visual features'' per class, can also provide compelling performance in the robust few-shot learning setting. In particular, we automatically enumerate multiple visual descriptions of each class -- via a large language model (LLM) -- then use a vision-image model to translate these descriptions to a set of multiple visual features of each image; we finally use sparse logistic regression to select a relevant subset of these features to classify each image. This both provides an ``intuitive'' set of relevant features for each class, and in the few-shot learning setting, outperforms standard approaches such as linear probing. When combined with finetuning, we also show that the method is able to outperform existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance.
</details>
<details>
<summary>摘要</summary>
现代图像分类基于直接预测模型类via大量推理网络，这使得评估视觉“特征”的概念变得更加困难。同时，现有的 JOINT 视觉语言模型，如 CLIP，可以提供每个类型的自然语言描述，但通常只focus on提供每个类型的单一描述。在这个工作中，我们展示了一种alternative方法，更加类似于我们对多个“视觉特征”的认知，可以在robust few-shot learning setting中提供出色的表现。具体来说，我们使用大型语言模型（LLM）自动生成每个类型的多个视觉描述，然后使用视觉图像模型将这些描述翻译成每个图像的多个视觉特征；最后，我们使用稀疏逻辑回归选择每个图像的相关子集特征进行分类。这种方法不仅提供了每个类型的“直观”相关特征，而且在few-shot learning setting中，也超越了标准方法such as linear probing。当与finetuning结合使用时，我们还证明了该方法可以超越现有的state-of-the-art finetuning方法，包括在分布式和非分布式情况下的表现。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-Nonlinear-Parametric-Model-Order-Reduction-Framework-using-Deep-Hierarchical-Variational-Autoencoder"><a href="#Data-driven-Nonlinear-Parametric-Model-Order-Reduction-Framework-using-Deep-Hierarchical-Variational-Autoencoder" class="headerlink" title="Data-driven Nonlinear Parametric Model Order Reduction Framework using Deep Hierarchical Variational Autoencoder"></a>Data-driven Nonlinear Parametric Model Order Reduction Framework using Deep Hierarchical Variational Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06816">http://arxiv.org/abs/2307.06816</a></li>
<li>repo_url: None</li>
<li>paper_authors: SiHun Lee, Sangmin Lee, Kijoo Jang, Haeseong Cho, SangJoon Shin</li>
<li>for: 提出了一种基于深度人工神经网络的数据驱动参数缩放方法，用于非线性动力系统的参数 interpolating。</li>
<li>methods: 使用了一种叫做Least-Squares Hierarchical Variational Autoencoder（LSH-VAE）的深度人工神经网络，该网络能够进行非线性参数缩放。LSH-VAE 通过两种主要改进：嵌入了一个嵌入的深度结构和一个权重加权概率损失函数。这两种改进使得LSH-VAE 的准确性和稳定性与传统的非线性参数缩放方法、自动Encoder和Variational Autoencoder相比有了显著提高。</li>
<li>results: 基于LSH-VAE，提出了一种基于圆拟 interpolating 的参数缩放框架。该框架在三个非线性多物理动力系统上进行了验证和评估，并与传统的非线性参数缩放方法进行了比较。结果显示，LSH-VAE 能够在准确性和速度两个方面与传统方法相比有显著优势。<details>
<summary>Abstract</summary>
A data-driven parametric model order reduction (MOR) method using a deep artificial neural network is proposed. The present network, which is the least-squares hierarchical variational autoencoder (LSH-VAE), is capable of performing nonlinear MOR for the parametric interpolation of a nonlinear dynamic system with a significant number of degrees of freedom. LSH-VAE exploits two major changes to the existing networks: a hierarchical deep structure and a hybrid weighted, probabilistic loss function. The enhancements result in a significantly improved accuracy and stability compared against the conventional nonlinear MOR methods, autoencoder, and variational autoencoder. Upon LSH-VAE, a parametric MOR framework is presented based on the spherically linear interpolation of the latent manifold. The present framework is validated and evaluated on three nonlinear and multiphysics dynamic systems. First, the present framework is evaluated on the fluid-structure interaction benchmark problem to assess its efficiency and accuracy. Then, a highly nonlinear aeroelastic phenomenon, limit cycle oscillation, is analyzed. Finally, the present framework is applied to a three-dimensional fluid flow to demonstrate its capability of efficiently analyzing a significantly large number of degrees of freedom. The performance of LSH-VAE is emphasized by comparing its results against that of the widely used nonlinear MOR methods, convolutional autoencoder, and $\beta$-VAE. The present framework exhibits a significantly enhanced accuracy to the conventional methods while still exhibiting a large speed-up factor.
</details>
<details>
<summary>摘要</summary>
提出了一种基于深度人工神经网络的数据驱动参数化模型简化方法（MOR）。该网络为Least-Squares Hierarchical Variational Autoencoder（LSH-VAE），可以实现非线性MOR，用于 interpolating非线性动力系统中的多个自由度。LSH-VAE利用了两个主要改进：层次深度结构和权重加权概率损失函数。这些改进使得MOR的准确性和稳定性得到了明显提高，相比于传统的非线性MOR方法、自动encoder和variational autoencoder。基于LSH-VAE，一种基于圆拟 interpolating latent manifold的参数化MOR框架被提出。该框架在三个非线性和多物理动力系统上进行了验证和评估。首先，该框架在流体-结构交互问题上进行了效率和准确性的评估。然后，一种高度非线性的风动现象，限цик尔振荡，进行了分析。最后，该框架在三维流体流中进行了efficient地分析一个相对较大的自由度。LSH-VAE的性能被与传统的非线性MOR方法、卷积自动encoder和β-VAE进行了比较，其准确性与传统方法相比具有明显提高，同时仍然具有大快速因子。
</details></li>
</ul>
<hr>
<h2 id="CT-BERT-Learning-Better-Tabular-Representations-Through-Cross-Table-Pre-training"><a href="#CT-BERT-Learning-Better-Tabular-Representations-Through-Cross-Table-Pre-training" class="headerlink" title="CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training"></a>CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04308">http://arxiv.org/abs/2307.04308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, Junbo Zhao<br>for: 这个论文的目的是为了提出一种可以在大规模表格数据上预训练表格数据的方法，以便在表格数据上实现通用表示。methods: 这个论文使用了一种名为 CT-BERT 的新框架，该框架可以在跨表格上进行预训练。 CT-BERT 可以与 both 监督学习和自监学习方法一起使用，并且提出了一种基于对比学习的表格模型目标函数。results: 论文的实验结果显示，CT-BERT 在 15 个 dataset 上达到了州际前进的性能，其中包括监督学习和自监学习两种不同的设置。 CT-BERT 的性能都高于先前的方法。<details>
<summary>Abstract</summary>
Tabular data -- also known as structured data -- is one of the most common data forms in existence, thanks to the stable development and scaled deployment of database systems in the last few decades. At present however, despite the blast brought by large pre-trained models in other domains such as ChatGPT or SAM, how can we extract common knowledge across tables at a scale that may eventually lead to generalizable representation for tabular data remains a full blank. Indeed, there have been a few works around this topic. Most (if not all) of them are limited in the scope of a single table or fixed form of a schema. In this work, we first identify the crucial research challenges behind tabular data pre-training, particularly towards the cross-table scenario. We position the contribution of this work in two folds: (i)-we collect and curate nearly 2k high-quality tabular datasets, each of which is guaranteed to possess clear semantics, clean labels, and other necessary meta information. (ii)-we propose a novel framework that allows cross-table pre-training dubbed as CT-BERT. Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is fully compatible with both supervised and self-supervised schemes, where the specific instantiation of CT-BERT is very much dependent on the downstream tasks. We further propose and implement a contrastive-learning-based and masked table modeling (MTM) objective into CT-BERT, that is inspired from computer vision and natural language processing communities but sophistically tailored to tables. The extensive empirical results on 15 datasets demonstrate CT-BERT's state-of-the-art performance, where both its supervised and self-supervised setups significantly outperform the prior approaches.
</details>
<details>
<summary>摘要</summary>
表格数据 -- 也称为结构化数据 -- 是现代数据的最常见形式，这主要归功于过去几十年内Database系统的稳定发展和大规模部署。然而，当前，即使大型预训模型在其他领域，如ChatGPT或SAM，所带来的冲击，总之，如何在大规模的表格数据中提取通用知识，以达到可generalizable的表格数据表示尚未得到解决。事实上，有一些相关的研究工作。大多数（如果不是所有）这些工作都受到单个表格或固定的表格Schema的限制。在这种工作中，我们首先 indentified the crucial research challenges behind tabular data pre-training, particularly in the cross-table scenario。我们的贡献在两个方面：（i）我们收集和精心整理了nearly 2k高质量的表格数据集，每个数据集都具有明确的 semantics、clean labels和其他必要的元信息。（ii）我们提出了一种新的框架，称为 CT-BERT，可以在跨表格上进行预训。另外，我们还提出了一种基于对比学习和遮盖表格模型（MTM）的目标函数，这种目标函数是从计算机视觉和自然语言处理社区中吸取的，但是它在表格上进行了精心修改。我们的实验结果表明，CT-BERT在15个数据集上 display state-of-the-art performance，其中包括supervised和self-supervised设置。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Piano-Transcription-with-Hierarchical-Frequency-Time-Transformer"><a href="#Automatic-Piano-Transcription-with-Hierarchical-Frequency-Time-Transformer" class="headerlink" title="Automatic Piano Transcription with Hierarchical Frequency-Time Transformer"></a>Automatic Piano Transcription with Hierarchical Frequency-Time Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04305">http://arxiv.org/abs/2307.04305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sony/hft-transformer">https://github.com/sony/hft-transformer</a></li>
<li>paper_authors: Keisuke Toyama, Taketo Akama, Yukara Ikemiya, Yuhta Takida, Wei-Hsiang Liao, Yuki Mitsufuji</li>
<li>for:  automatic piano transcription, especially for determining the precise onset and offset of each note in polyphonic piano content.</li>
<li>methods:  hFT-Transformer, a two-level hierarchical frequency-time Transformer architecture that captures long-term dependencies in the frequency and time axes using self-attention mechanism.</li>
<li>results:  state-of-the-art performance on all F1-scores of metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations, as demonstrated on the widely used MAPS and MAESTRO v3.0.0 datasets.<details>
<summary>Abstract</summary>
Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription. This is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content. In this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes. In this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture. The first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis. The output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis. We evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the F1-scores of the metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations.
</details>
<details>
<summary>摘要</summary>
需要考虑长期 spectral 和 temporal 依赖性，以便自动识别钢琴乐谱。特别是在确定每个乐谱中的精确开始和结束时间点时，长期依赖性对于多重钢琴内容非常重要。在这种情况下，我们可以利用 Transformer 模型中的自我注意力机制，以捕捉在频谱和时间轴上的长期依赖性。在这项工作中，我们提出了 hFT-Transformer，这是一种使用两级叠加频谱-时间 Transformer 架构的自动音乐识别方法。第一层包括时间轴中的卷积块，频谱轴中的 Transformer 编码器，以及将频谱维度转换为时间轴的 Transformer 解码器。输出然后被 fed 到第二层，该层包括另一个时间轴中的 Transformer 编码器。我们对 widely 使用的 MAPS 和 MAESTRO v3.0.0 数据集进行了评估，并示出了所有 F1-scores 的 metric 中的状态顶峰性能。
</details></li>
</ul>
<hr>
<h2 id="Edge-Storage-Management-Recipe-with-Zero-Shot-Data-Compression-for-Road-Anomaly-Detection"><a href="#Edge-Storage-Management-Recipe-with-Zero-Shot-Data-Compression-for-Road-Anomaly-Detection" class="headerlink" title="Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection"></a>Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04298">http://arxiv.org/abs/2307.04298</a></li>
<li>repo_url: None</li>
<li>paper_authors: YeongHyeon Park, Uju Gim, Myung Jin Kim</li>
<li>for: 本研究旨在提出一种高效的存储管理方法，以便在Edge计算环境中进行噪声检测系统的实现。</li>
<li>methods: 本研究提出了一种使用预训练 autoencoder 进行数据压缩的方法，以便减少数据的存储和传输成本。</li>
<li>results: 经过对比性测试，我们发现该方法可以保持噪声检测性能，同时提高存储和传输效率。<details>
<summary>Abstract</summary>
Recent studies show edge computing-based road anomaly detection systems which may also conduct data collection simultaneously. However, the edge computers will have small data storage but we need to store the collected audio samples for a long time in order to update existing models or develop a novel method. Therefore, we should consider an approach for efficient storage management methods while preserving high-fidelity audio. A hardware-perspective approach, such as using a low-resolution microphone, is an intuitive way to reduce file size but is not recommended because it fundamentally cuts off high-frequency components. On the other hand, a computational file compression approach that encodes collected high-resolution audio into a compact code should be recommended because it also provides a corresponding decoding method. Motivated by this, we propose a way of simple yet effective pre-trained autoencoder-based data compression method. The pre-trained autoencoder is trained for the purpose of audio super-resolution so it can be utilized to encode or decode any arbitrary sampling rate. Moreover, it will reduce the communication cost for data transmission from the edge to the central server. Via the comparative experiments, we confirm that the zero-shot audio compression and decompression highly preserve anomaly detection performance while enhancing storage and transmission efficiency.
</details>
<details>
<summary>摘要</summary>
近期研究显示基于边缘计算的公路异常检测系统可能同时进行数据收集。然而，边缘计算机器具有小容量数据存储，我们需要长期保存收集的音频采样以更新现有模型或开发新方法。因此，我们应该考虑一种高效存储管理方法，同时保持高质量音频。一种硬件视角的方法，如使用低分辨率 Mikrofon，不建议使用，因为它基本上切断高频组件。相反，一种计算机件压缩方法，通过编码收集的高分辨率音频为紧凑的编码，应该得到推荐。受到这一点的激励，我们提出了简单又有效的预训练自动编码器基于数据压缩方法。预训练自动编码器是为了音频超分辨率而训练的，因此可以用于编码或解码任何采样率。此外，它还会降低边缘到中央服务器的数据传输成本。通过比较实验，我们证明了零aser音频压缩和解压缩可以高度保持异常检测性能，同时提高存储和传输效率。
</details></li>
</ul>
<hr>
<h2 id="Online-Ad-Procurement-in-Non-stationary-Autobidding-Worlds"><a href="#Online-Ad-Procurement-in-Non-stationary-Autobidding-Worlds" class="headerlink" title="Online Ad Procurement in Non-stationary Autobidding Worlds"></a>Online Ad Procurement in Non-stationary Autobidding Worlds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05698">http://arxiv.org/abs/2307.05698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Cheuk Nam Liang, Haihao Lu, Baoyu Zhou</li>
<li>for: 本研究旨在帮助在线广告主动ynamically优化广告平台的杠杆决策，以便更好地管理媒体购买预算和竞争对手的影响，并在长期环境下减少 regret。</li>
<li>methods: 本研究使用了在线学习框架，并引入了 primal-dual 算法来解决多维决策变量、飞行反馈和长期不确定约束的问题。</li>
<li>results: 本研究表明，我们的算法在不知道飞行过程的情况下可以 achieve low regret in many worlds，并且可以在不同类型的飞行过程中提供优化的杠杆决策。<details>
<summary>Abstract</summary>
Today's online advertisers procure digital ad impressions through interacting with autobidding platforms: advertisers convey high level procurement goals via setting levers such as budget, target return-on-investment, max cost per click, etc.. Then ads platforms subsequently procure impressions on advertisers' behalf, and report final procurement conversions (e.g. click) to advertisers. In practice, advertisers may receive minimal information on platforms' procurement details, and procurement outcomes are subject to non-stationary factors like seasonal patterns, occasional system corruptions, and market trends which make it difficult for advertisers to optimize lever decisions effectively. Motivated by this, we present an online learning framework that helps advertisers dynamically optimize ad platform lever decisions while subject to general long-term constraints in a realistic bandit feedback environment with non-stationary procurement outcomes. In particular, we introduce a primal-dual algorithm for online decision making with multi-dimension decision variables, bandit feedback and long-term uncertain constraints. We show that our algorithm achieves low regret in many worlds when procurement outcomes are generated through procedures that are stochastic, adversarial, adversarially corrupted, periodic, and ergodic, respectively, without having to know which procedure is the ground truth. Finally, we emphasize that our proposed algorithm and theoretical results extend beyond the applications of online advertising.
</details>
<details>
<summary>摘要</summary>
今天的在线广告主通过交互式自动拍卖平台购买数字广告印象：广告主通过设置杠杆如预算、目标回报率、最高单击成本等来传递高级购买目标。然后广告平台会在广告主的 behalf 购买印象，并将最终购买转化（例如点击）报告给广告主。在实践中，广告主可能会收到最小的平台购买详细信息，并且购买结果受到不同因素的影响，如季节性模式、 occasional 系统腐蚀和市场趋势，这使得广告主难以有效地优化杠杆决策。为了解决这个问题，我们提出了一个在线学习框架，帮助广告主在面临实际的链接环境下动态优化广告平台杠杆决策，同时遵循长期不确定的约束。具体来说，我们引入了 primal-dual 算法，用于在线决策中的多维决策变量、链接反馈和长期不确定约束。我们证明了我们的算法在许多世界中具有低念悟，无需知道采用哪种程序是真实的。最后，我们强调了我们的提出的算法和理论结果超出了在线广告应用的限制，可以应用于其他领域。
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Graph-ODE-for-Learning-Complex-System-Dynamics-across-Environments"><a href="#Generalizing-Graph-ODE-for-Learning-Complex-System-Dynamics-across-Environments" class="headerlink" title="Generalizing Graph ODE for Learning Complex System Dynamics across Environments"></a>Generalizing Graph ODE for Learning Complex System Dynamics across Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04287">http://arxiv.org/abs/2307.04287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Huang, Yizhou Sun, Wei Wang</li>
<li>for: 学习多体系统动力学，特别是在生物学中的分子动力学，以便更好地预测系统的未来轨迹。</li>
<li>methods: 我们提出了一种机器学习框架，名为GG-ODE（通用图Ordinary Differential Equations），可以学习不同环境中系统动力学的连续性。我们使用图神经网络（GNNs）参数化神经普通 diferencial equations（ODEs），以捕捉多体系统之间的连续交互。为了实现模型泛化，我们假设不同环境下的动力学都受到共同的物理法则控制，可以通过学习共享的ODE函数来捕捉。</li>
<li>results: 我们的模型可以准确预测系统动力学，特别是在长距离上，并且可以将新系统中的少量观测数据泛化到整个系统。在多种物理 simulate 中进行了实验，我们的模型能够准确预测系统动力学，特别是在长距离上，并且可以将新系统中的少量观测数据泛化到整个系统。<details>
<summary>Abstract</summary>
Learning multi-agent system dynamics has been extensively studied for various real-world applications, such as molecular dynamics in biology. Most of the existing models are built to learn single system dynamics from observed historical data and predict the future trajectory. In practice, however, we might observe multiple systems that are generated across different environments, which differ in latent exogenous factors such as temperature and gravity. One simple solution is to learn multiple environment-specific models, but it fails to exploit the potential commonalities among the dynamics across environments and offers poor prediction results where per-environment data is sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary Differential Equations), a machine learning framework for learning continuous multi-agent system dynamics across environments. Our model learns system dynamics using neural ordinary differential equations (ODE) parameterized by Graph Neural Networks (GNNs) to capture the continuous interaction among agents. We achieve the model generalization by assuming the dynamics across different environments are governed by common physics laws that can be captured via learning a shared ODE function. The distinct latent exogenous factors learned for each environment are incorporated into the ODE function to account for their differences. To improve model performance, we additionally design two regularization losses to (1) enforce the orthogonality between the learned initial states and exogenous factors via mutual information minimization; and (2) reduce the temporal variance of learned exogenous factors within the same system via contrastive learning. Experiments over various physical simulations show that our model can accurately predict system dynamics, especially in the long range, and can generalize well to new systems with few observations.
</details>
<details>
<summary>摘要</summary>
学习多智能体系统动态已经广泛研究了各种现实世界应用，如生物分子动力学。大多数现有模型都是建立来学习单个系统动态从观察到数据，预测未来轨迹。然而，在实践中，我们可能会观察到来自不同环境的多个系统，这些系统之间的潜在因素不同，如温度和重力。一个简单的解决方案是学习每个环境特定的模型，但这会忽略系统动态之间的共同特征，并且预测结果在每个环境数据稀缺或有限时会不佳。我们提出了GG-ODE（通用图 ordininary differential equations）机器学习框架，用于学习连续多智能体系统动态。我们的模型通过使用图神经网络（GNNs）参数化神经ordinary differential equations（ODE）来捕捉连续智能体之间的交互。我们通过假设不同环境的动态都受到共同的物理法则所控制，以学习共同的ODE函数来泛化模型。每个环境的潜在隐藏因素被学习到ODE函数中，以考虑它们之间的差异。为了提高模型性能，我们还设计了两种常见化loss，即（1）通过对学习的初始状态和隐藏因素进行相互信息减少来保持对隐藏因素的正交性; 和（2）在同一个系统中减少学习的时间异谱。通过对各种物理 simulate experiment 进行测试，我们发现我们的模型可以准确预测系统动态，特别是在长距离内，并且可以良好地泛化到新系统。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-efficacy-of-large-language-models-in-generating-accurate-teacher-responses"><a href="#Assessing-the-efficacy-of-large-language-models-in-generating-accurate-teacher-responses" class="headerlink" title="Assessing the efficacy of large language models in generating accurate teacher responses"></a>Assessing the efficacy of large language models in generating accurate teacher responses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04274">http://arxiv.org/abs/2307.04274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yann Hicke, Abhishek Masand, Wentao Guo, Tushaar Gangavarapu</li>
<li>for: 这个研究旨在评估大语言模型在提供教育性帮助和指导中的生成能力，以采用知识able teacher的角色进行 simulate。</li>
<li>methods: 研究使用了多个标准的生成模型，包括GPT-4（少量示例学习）、精度调整GPT-2和DialogGPT，以及通过强化学习来优化Flan-T5模型。</li>
<li>results: 实验结果表明，GPT-4在BERTScore和DialogRPT上的表现较高，而其他精度调整模型表现较差，这些结果提示了 dataset 特性的影响，如采样、代表性和对话完整性，对于精度调整具有 significanteffect。<details>
<summary>Abstract</summary>
(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.   We hypothesize that several dataset characteristics, including sampling, representativeness, and dialog completeness, pose significant challenges to fine-tuning, thus contributing to the poor generalizability of the fine-tuned models. Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills.
</details>
<details>
<summary>摘要</summary>
我们认为，共同任务的 dataset 特点，包括采样、代表性和对话完整性，对于调整 pose  significiant 挑战，从而导致调整模型的泛化性不佳。最后，我们注意到，为了评估这些生成模型，需要使用一个指标，不仅考虑对话 coherence 和语言模型的匹配分布，还需要考虑模型的教学技能展示能力。
</details></li>
</ul>
<hr>
<h2 id="MentalHealthAI-Utilizing-Personal-Health-Device-Data-to-Optimize-Psychiatry-Treatment"><a href="#MentalHealthAI-Utilizing-Personal-Health-Device-Data-to-Optimize-Psychiatry-Treatment" class="headerlink" title="MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment"></a>MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04777">http://arxiv.org/abs/2307.04777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manan Shukla, Oshani Seneviratne</li>
<li>for: 提供个性化心理健康跟踪和情绪预测系统，以帮助心理医生更好地了解患有心理疾病的患者的情况，并提供更有效的心理疾病治疗和管理方法。</li>
<li>methods: 利用个体 физиологи数据，combines transfer和 federated机器学习概念，使用智能合约，保持数据在用户设备上，实现隐私感知和负责任的心理健康跟踪。</li>
<li>results: 使用流行的心理健康数据集，实现了Promising results，表明我们的方法可以提供有效的心理健康跟踪和预测。<details>
<summary>Abstract</summary>
Mental health disorders remain a significant challenge in modern healthcare, with diagnosis and treatment often relying on subjective patient descriptions and past medical history. To address this issue, we propose a personalized mental health tracking and mood prediction system that utilizes patient physiological data collected through personal health devices. Our system leverages a decentralized learning mechanism that combines transfer and federated machine learning concepts using smart contracts, allowing data to remain on users' devices and enabling effective tracking of mental health conditions for psychiatric treatment and management in a privacy-aware and accountable manner. We evaluate our model using a popular mental health dataset that demonstrates promising results. By utilizing connected health systems and machine learning models, our approach offers a novel solution to the challenge of providing psychiatrists with further insight into their patients' mental health outside of traditional office visits.
</details>
<details>
<summary>摘要</summary>
精神健康问题仍然是现代医疗中的主要挑战，诊断和治疗往往基于患者主观描述和医疗历史。为解决这一问题，我们提出了个性化精神健康跟踪和情绪预测系统，该系统利用通过个人健康设备收集的患者生物数据。我们的系统采用分布式学习机制，结合了转移和联邦机器学习概念，使得数据能够留在用户设备上，并且有效地跟踪精神健康状况，供心理医生进行精神疾病治疗和管理，同时具有隐私和负责任的特点。我们使用了一个流行的精神健康数据集，并得到了批判性的结果。通过连接医疗系统和机器学习模型，我们的方法提供了一种新的解决方案，即为心理医生在传统办公室访问之外提供更多的精神健康情况的信息。
</details></li>
</ul>
<hr>
<h2 id="RidgeBase-A-Cross-Sensor-Multi-Finger-Contactless-Fingerprint-Dataset"><a href="#RidgeBase-A-Cross-Sensor-Multi-Finger-Contactless-Fingerprint-Dataset" class="headerlink" title="RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset"></a>RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05563">http://arxiv.org/abs/2307.05563</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bhavinjawade/RidgeBase_Fingerprint_Camera_App">https://github.com/bhavinjawade/RidgeBase_Fingerprint_Camera_App</a></li>
<li>paper_authors: Bhavin Jawade, Deen Dayal Mohan, Srirangaraj Setlur, Nalini Ratha, Venu Govindaraju</li>
<li>For: 这 paper 的目的是提出一个大规模实际数据集，以促进无接触指纹识别技术的进一步发展。* Methods: 这 paper 使用了两种摄像头和一个平板式接触传感器来收集 contactless 和 contact-based 指纹图像，并提出了一种基于 facial recognition 数据集的集成匹配协议来处理 intra-sample 差异。* Results: 这 paper 的结果显示，使用 COTS 指纹匹配器和深度 CNN 方法在 RidgeBase 数据集上实现了高度准确的指纹识别。<details>
<summary>Abstract</summary>
Contactless fingerprint matching using smartphone cameras can alleviate major challenges of traditional fingerprint systems including hygienic acquisition, portability and presentation attacks. However, development of practical and robust contactless fingerprint matching techniques is constrained by the limited availability of large scale real-world datasets. To motivate further advances in contactless fingerprint matching across sensors, we introduce the RidgeBase benchmark dataset. RidgeBase consists of more than 15,000 contactless and contact-based fingerprint image pairs acquired from 88 individuals under different background and lighting conditions using two smartphone cameras and one flatbed contact sensor. Unlike existing datasets, RidgeBase is designed to promote research under different matching scenarios that include Single Finger Matching and Multi-Finger Matching for both contactless- to-contactless (CL2CL) and contact-to-contactless (C2CL) verification and identification. Furthermore, due to the high intra-sample variance in contactless fingerprints belonging to the same finger, we propose a set-based matching protocol inspired by the advances in facial recognition datasets. This protocol is specifically designed for pragmatic contactless fingerprint matching that can account for variances in focus, polarity and finger-angles. We report qualitative and quantitative baseline results for different protocols using a COTS fingerprint matcher (Verifinger) and a Deep CNN based approach on the RidgeBase dataset. The dataset can be downloaded here: https://www.buffalo.edu/cubs/research/datasets/ridgebase-benchmark-dataset.html
</details>
<details>
<summary>摘要</summary>
请注意，以下文本将使用简化中文表示法。无接触指纹识别使用智能手机镜头可以解决传统指纹系统中的主要挑战，包括医疗安全、可移植性和展示攻击。然而，发展实用且可靠的无接触指纹识别技术受到实际数据的有限可用性所限制。为了鼓励进一步的无接触指纹识别技术发展，我们介绍了RidgeBase参考 dataset。RidgeBase 包含了15,000多个无接触和接触基于指纹图像的对照项目，来自88名参与者，在不同的背景和照明条件下使用两款智能手机镜头和一款平板式接触感应器所取得。与现有数据集不同的是，RidgeBase 是设计来鼓励研究不同的匹配enario，包括单指纹匹配和多指纹匹配，并且包括CL2CL和C2CL验证和识别。此外，由于无接触指纹内部的同 fingers 的标本之间的高同一样性，我们提出了一个基于人脸识别数据集的集合匹配协议。这个协议特别适用于实用无接触指纹识别，可以考虑到专注、极化和手均角度的变化。我们将在RidgeBase dataset上报告基于不同协议的 qualitative 和量化基eline结果，使用商业 fingerprint 匹配软件（Verifinger）和深度 CNN 基础的方法。数据可以在以下网址下载：https://www.buffalo.edu/cubs/research/datasets/ridgebase-benchmark-dataset.html。
</details></li>
</ul>
<hr>
<h2 id="The-Future-of-Fundamental-Science-Led-by-Generative-Closed-Loop-Artificial-Intelligence"><a href="#The-Future-of-Fundamental-Science-Led-by-Generative-Closed-Loop-Artificial-Intelligence" class="headerlink" title="The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence"></a>The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07522">http://arxiv.org/abs/2307.07522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hector Zenil, Jesper Tegnér, Felipe S. Abrahão, Alexander Lavin, Vipin Kumar, Jeremy G. Frey, Adrian Weller, Larisa Soldatova, Alan R. Bundy, Nicholas R. Jennings, Koichi Takahashi, Lawrence Hunter, Saso Dzeroski, Andrew Briggs, Frederick D. Gregory, Carla P. Gomes, Christopher K. I. Williams, Jon Rowe, James Evans, Hiroaki Kitano, Joshua B. Tenenbaum, Ross King</li>
<li>For: The paper explores the potential of AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space.* Methods: The paper discusses the use of Generative AI and Large Language Models to augment and accelerate the scientific discovery of fundamental deep science with quantitative models.* Results: The paper suggests that integrating AI-driven automation into the practice of science could mitigate current problems, including the replication of findings, systematic production of data, and ultimately democratise the scientific process.<details>
<summary>Abstract</summary>
Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Integrating AI-driven automation into the practice of science would mitigate current problems, including the replication of findings, systematic production of data, and ultimately democratisation of the scientific process. Realising these possibilities requires a vision for augmented AI coupled with a diversity of AI approaches able to deal with fundamental aspects of causality analysis and model discovery while enabling unbiased search across the space of putative explanations. These advances hold the promise to unleash AI's potential for searching and discovering the fundamental structure of our world beyond what human scientists have been able to achieve. Such a vision would push the boundaries of new fundamental science rather than automatize current workflows and instead open doors for technological innovation to tackle some of the greatest challenges facing humanity today.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ChatGPT-in-the-Age-of-Generative-AI-and-Large-Language-Models-A-Concise-Survey"><a href="#ChatGPT-in-the-Age-of-Generative-AI-and-Large-Language-Models-A-Concise-Survey" class="headerlink" title="ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey"></a>ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04251">http://arxiv.org/abs/2307.04251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iamgmujtaba/scholar_search">https://github.com/iamgmujtaba/scholar_search</a></li>
<li>paper_authors: Salman Mohamadi, Ghulam Mujtaba, Ngan Le, Gianfranco Doretto, Donald A. Adjeroh<br>for: 这 paper 的主要目的是为了提供一份简洁的survey关于 ChatGPT 的当前研究进展和演化。methods: 这 paper 使用了两种视角来研究 ChatGPT：玻璃盒视角（glass box view）和黑盒视角（black box view）。玻璃盒视角专注于理解技术的内部工作方式，而黑盒视角则视其为一个复杂系统，从输入、输出和效果的角度进行研究。results: 这 paper 提供了一份全面的探究 ChatGPT 技术的研究进展和应用前景，同时也提出了进一步研究的必要性和潜在应用领域。此外， paper 还 shed light on  LLMS 和GAI 的基础文献，并探讨了这些技术在教育、研究、医疗、金融等领域的应用和关键问题。<details>
<summary>Abstract</summary>
ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs, outputs, and effects. This paves the way for a comprehensive exploration of the technology and provides a road map for further research and experimentation. We also lay out essential foundational literature on LLMs and GAI in general and their connection with ChatGPT. This overview sheds light on existing and missing research lines in the emerging field of LLMs, benefiting both public users and developers. Furthermore, the paper delves into the broad spectrum of applications and significant concerns in fields such as education, research, healthcare, finance, etc.
</details>
<details>
<summary>摘要</summary>
chatGPT是一个大型自然语言模型（LLM），由OpenAI精心训练了大量数据。它对自然语言处理（NLP）领域产生了革命性的变革，并推动了LLM的可能性的极限。chatGPT使得大规模的人工智能生成（GAI）与公众进行交互，并且引发了研究人员对类似技术的开发和应用的兴趣。在这篇论文中，我们的主要目标是提供对chatGPT的当前研究进展和演化的简短概述。我们包括了“玻璃盒”和“黑盒”两种视角，即理解技术的内部结构和行为，以及对其输入、输出和影响的研究。这种方法使得我们可以全面探索技术，并为进一步的研究和实验提供了道路图。此外，我们还提供了LLM和GAI的基础文献，这些文献对于公众和开发者都是必读的。此概述照明了LLM领域的现有和缺失的研究方向，并且探讨了GAI在各个领域的应用和关注点，如教育、研究、医疗、金融等。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-learning-for-blending-gridded-satellite-and-gauge-measured-precipitation-data"><a href="#Ensemble-learning-for-blending-gridded-satellite-and-gauge-measured-precipitation-data" class="headerlink" title="Ensemble learning for blending gridded satellite and gauge-measured precipitation data"></a>Ensemble learning for blending gridded satellite and gauge-measured precipitation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06840">http://arxiv.org/abs/2307.06840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgia Papacharalampous, Hristos Tyralis, Nikolaos Doulamis, Anastasios Doulamis</li>
<li>for: 提高卫星降水产品的准确性</li>
<li>methods: 使用11种新的ensemble学习算法，包括多变量适应回归splines、多变量适应多项式splines、随机森林、梯度提升机、极大梯度提升和 bayesian常量化神经网络</li>
<li>results: 对整个美国陆地区和15年时间段进行了广泛的比较，并发现ensemble学习算法可以提高卫星降水产品的准确性。<details>
<summary>Abstract</summary>
Regression algorithms are regularly used for improving the accuracy of satellite precipitation products. In this context, ground-based measurements are the dependent variable and the satellite data are the predictor variables, together with topography factors. Alongside this, it is increasingly recognised in many fields that combinations of algorithms through ensemble learning can lead to substantial predictive performance improvements. Still, a sufficient number of ensemble learners for improving the accuracy of satellite precipitation products and their large-scale comparison are currently missing from the literature. In this work, we fill this specific gap by proposing 11 new ensemble learners in the field and by extensively comparing them for the entire contiguous United States and for a 15-year period. We use monthly data from the PERSIANN (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks) and IMERG (Integrated Multi-satellitE Retrievals for GPM) gridded datasets. We also use gauge-measured precipitation data from the Global Historical Climatology Network monthly database, version 2 (GHCNm). The ensemble learners combine the predictions by six regression algorithms (base learners), namely the multivariate adaptive regression splines (MARS), multivariate adaptive polynomial splines (poly-MARS), random forests (RF), gradient boosting machines (GBM), extreme gradient boosting (XGBoost) and Bayesian regularized neural networks (BRNN), and each of them is based on a different combiner. The combiners include the equal-weight combiner, the median combiner, two best learners and seven variants of a sophisticated stacking method. The latter stacks a regression algorithm on the top of the base learners to combine their independent predictions...
</details>
<details>
<summary>摘要</summary>
干涉算法常用于提高卫星降水产品的准确性。在这个上下文中，地面测量是dependent变量，而卫星数据和地形因素是预测变量。此外，逐渐认识到，将算法组合在ensemble学习中可以导致重要的预测性能提高。然而，卫星降水产品的精度 improvemen要求足够多的ensemble学习者，而现有文献中缺乏这些学习者。在这项工作中，我们填充这个空白，并提出11种新的ensemble学习者，并对整个大陆和15年时间进行了广泛的比较。我们使用PERSIANN（Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks）和IMERG（Integrated Multi-satellitE Retrievals for GPM）的月度数据，以及GHCNm（Global Historical Climatology Network monthly database, version 2）中的 gauge-measured precipitation数据。ensemble学习者将base learner（六种回归算法）的预测结果进行组合，其中包括平均值 combiner、 median combiner、两个best learner和七种堆叠方法的七种变种。这些堆叠方法都基于不同的combiner。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Bayesian-travel-time-tomography-with-geologically-complex-priors-using-sensitivity-informed-polynomial-chaos-expansion-and-deep-generative-networks"><a href="#Efficient-Bayesian-travel-time-tomography-with-geologically-complex-priors-using-sensitivity-informed-polynomial-chaos-expansion-and-deep-generative-networks" class="headerlink" title="Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks"></a>Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04228">http://arxiv.org/abs/2307.04228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Angelo Meles, Macarena Amaya, Shiran Levy, Stefano Marelli, Niklas Linde</li>
<li>for: This paper focuses on developing a strategy for Bayesian travel-time tomography using Monte Carlo Markov Chain (MCMC) methods, which can accurately characterize the prior distribution and efficiently evaluate the likelihood.</li>
<li>methods: The paper combines the use of principal component analysis (PCA) and polynomial chaos expansion (PCE) to develop a surrogate model for the forward problem, and leverages variational autoencoders (VAEs) to represent the prior distribution.</li>
<li>results: The proposed method enables accurate reconstruction of the true travel times and provides a viable alternative to traditional MCMC methods, which can be computationally expensive and challenging to implement.<details>
<summary>Abstract</summary>
Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental challenges: the accurate characterization of the prior distribution and the efficient evaluation of the likelihood. In the context of Bayesian studies on tomography, principal component analysis (PCA) can in some cases facilitate the straightforward definition of the prior distribution, while simultaneously enabling the implementation of accurate surrogate models based on polynomial chaos expansion (PCE) to replace computationally intensive full-physics forward solvers. When faced with scenarios where PCA does not offer a direct means of easily defining the prior distribution alternative methods like deep generative models (e.g., variational autoencoders (VAEs)), can be employed as viable options. However, accurately producing a surrogate capable of capturing the intricate non-linear relationship between the latent parameters of a VAE and the outputs of forward modeling presents a notable challenge. Indeed, while PCE models provide high accuracy when the input-output relationship can be effectively approximated by relatively low-degree multivariate polynomials, this condition is typically unmet when utilizing latent variables derived from deep generative models. In this contribution, we present a strategy that combines the excellent reconstruction performances of VAE in terms of prio representation with the accuracy of PCA-PCE surrogate modeling in the context of Bayesian ground penetrating radar (GPR) travel-time tomography. Within the MCMC process, the parametrization of the VAE is leveraged for prior exploration and sample proposal. Concurrently, modeling is conducted using PCE, which operates on either globally or locally defined principal components of the VAE samples under examination.
</details>
<details>
<summary>摘要</summary>
Monte Carlo Markov Chain（MCMC）方法常遇到两个基本挑战：准确地 characteryrization of the prior distribution和高效地评估 likelihood。在扩展学中的Tomography研究中，主成分分析（PCA）可以在一些情况下使得归一化分布的定义变得更加直观，同时允许通过多项式混合（PCE）来替代 computationally intensive full-physics forward solvers。然而，在PCA无法直接定义归一化分布的情况下，可以使用深度生成模型（例如变量自动编码器（VAEs））作为可行的选择。然而，生成高精度的surrogate模型，以capture latent parameters of VAE和前向模型之间的复杂非线性关系，则成为一大挑战。实际上，PCE模型在输入-输出关系可以高度有效地被近似为低度多ivariate polynomials时，具有高准确性。然而，这种条件通常不满足使用来自深度生成模型 derivated的latent variables。在这篇论文中，我们提出一种策略，将VAE的很好的重建性与PCA-PCE模型的准确性相结合，以进行 Bayesian ground penetrating radar（GPR） travel-time tomography。在MCMC过程中，VAE的 Parametrization被用于归一化和样本提议。同时，使用PCE进行模型化， operate on either globally or locally defined principal components of VAE samples under examination。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Autoencoder-based-Lossy-Compression-for-Large-scale-High-resolution-Scientific-Data"><a href="#Hierarchical-Autoencoder-based-Lossy-Compression-for-Large-scale-High-resolution-Scientific-Data" class="headerlink" title="Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data"></a>Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04216">http://arxiv.org/abs/2307.04216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieutrungle/data-slim">https://github.com/hieutrungle/data-slim</a></li>
<li>paper_authors: Hieu Le, Hernan Santos, Jian Tao<br>for:  compress large-scale scientific data while maintaining high reconstruction qualitymethods:  uses a neural network-based approach with Autoencoder architectureresults:  achieves a compression ratio of 140 on several benchmark data sets, and 200 on simulation data from the High-Resolution Community Earth System Model (CESM) Version 1.3 with negligible reconstruction error.<details>
<summary>Abstract</summary>
Lossy compression has become an important technique to reduce data size in many domains. This type of compression is especially valuable for large-scale scientific data, whose size ranges up to several petabytes. Although Autoencoder-based models have been successfully leveraged to compress images and videos, such neural networks have not widely gained attention in the scientific data domain. Our work presents a neural network that not only significantly compresses large-scale scientific data but also maintains high reconstruction quality. The proposed model is tested with scientific benchmark data available publicly and applied to a large-scale high-resolution climate modeling data set. Our model achieves a compression ratio of 140 on several benchmark data sets without compromising the reconstruction quality. Simulation data from the High-Resolution Community Earth System Model (CESM) Version 1.3 over 500 years are also being compressed with a compression ratio of 200 while the reconstruction error is negligible for scientific analysis.
</details>
<details>
<summary>摘要</summary>
lossy compression技术在许多领域中已成为重要的数据压缩方法。特别是在大规模科学数据领域，数据的大小可以达到数十个petabyte级别。虽然基于Autoencoder的模型已成功地压缩图像和视频，但这些神经网络在科学数据领域尚未得到广泛关注。我们的工作推出了一种能够高效压缩大规模科学数据，同时保持高重建质量的神经网络模型。我们的模型在公共预测数据集上进行测试，并应用于大规模高分辨率气候模拟数据集。我们的模型在多个 benchmark 数据集上实现了压缩率为140，而且重建质量几乎不受影响。同时，我们还对高分辨率社区地球系统模型（CESM）版本1.3的500年 simultion 数据进行压缩，压缩率达200，重建错误几乎可以忽略不计。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Action-based-Ball-Recovery-Model-using-360-circ-data"><a href="#Generalized-Action-based-Ball-Recovery-Model-using-360-circ-data" class="headerlink" title="Generalized Action-based Ball Recovery Model using 360$^\circ$ data"></a>Generalized Action-based Ball Recovery Model using 360$^\circ$ data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04215">http://arxiv.org/abs/2307.04215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Furbino Marques do Nascimento, Hugo M. R. Rios-Neto</li>
<li>for: 本研究的目的是答 answer the question of what actions lead to a change in possession, and how a team’s positioning affects ball recovery.</li>
<li>methods: 本研究使用了Statsbomb 360$^\circ$ 数据创建了一个通用的动作基于球 Possession 模型（GABR）。</li>
<li>results: 研究发现，在各种不同的防御方式下，球 Possession 的变化是由几种不同的动作引起的，而这些动作与防御方式之间存在着正相关关系。此外，研究还发现了一些球队的位置对球 Possession 的影响。<details>
<summary>Abstract</summary>
Even though having more possession does not necessarily lead to winning, teams like Manchester City, Liverpool, and Leeds United notably have tried to recover the ball quickly after they lost it over the past few years. Nowadays, some of the top managers in the world apply high-pressing styles, and concepts such as the five-second rule, usually credited to Guardiola, have been spreading out [9][10], becoming a fundamental part of how lots of teams have played over the recent years. Expressions like "don't let them breathe" and "get the ball back as soon as possible" are often heard in the media [4][5][6], but what are the actions that most lead to a change in possession? What is the influence of a team's positioning on the ball recovery? Which are the players that more often collapse when under pressure? Can we evaluate the defensive dynamics of teams that do not necessarily press the player in possession as intensely as those mentioned above? We try to answer those and other questions in this paper by creating a Generalized Action based Ball Recovery model (GABR) using Statsbomb 360$^\circ$ data.
</details>
<details>
<summary>摘要</summary>
即使 possessed 更多不一定会导致赢球，但是球队如曼城、利物浦和利兹联等在过去几年来尝试快速回夺球的情况仍然很常见。目前，世界上许多顶尖教练会应用高压风格，并且概念如五秒规则，通常被归功于加多达（Guardiola），在过去几年间成为了许多队伍的基本战斗方式。媒体中经常听到“不让他呼吸”和“尽快回夺球”的表达（4][5][6），但是哪些行为最可能导致 possession 的变化？队伍的位置如何影响球回夺？哪些球员在压力下更容易塌陷？我们通过创建一个通用行动基于球回夺模型（GABR），使用Statsbomb 360$^\circ$ 数据来回答这些问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/10/cs.LG_2023_07_10/" data-id="clon21ira00l0r588emdq6idc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/10/eess.IV_2023_07_10/" class="article-date">
  <time datetime="2023-07-10T09:00:00.000Z" itemprop="datePublished">2023-07-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/10/eess.IV_2023_07_10/">eess.IV - 2023-07-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DWA-Differential-Wavelet-Amplifier-for-Image-Super-Resolution"><a href="#DWA-Differential-Wavelet-Amplifier-for-Image-Super-Resolution" class="headerlink" title="DWA: Differential Wavelet Amplifier for Image Super-Resolution"></a>DWA: Differential Wavelet Amplifier for Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04593">http://arxiv.org/abs/2307.04593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian B. Moser, Stanislav Frolov, Federico Raue, Sebastian Palacio, Andreas Dengel</li>
<li>for: 这篇论文是为了提出一种 Drop-in 模块，即 diferencial wavelet amplifier (DWA)，用于提高wavelet-based image Super-Resolution (SR)。</li>
<li>methods: DWA 使用了Discrete Wavelet Transformation (DWT)，一种已经受到较少关注的方法，来实现高效的图像表示和减少输入空间大小。</li>
<li>results: 根据实验结果，DWA 可以提高wavelet-based SR 模型的性能，特别是在经典的 SR 任务中，如 DWSR 和 MWCNN。此外，DWA 可以直接应用于输入图像空间，从而避免了传统 DWT 的频道化表示。<details>
<summary>Abstract</summary>
This work introduces Differential Wavelet Amplifier (DWA), a drop-in module for wavelet-based image Super-Resolution (SR). DWA invigorates an approach recently receiving less attention, namely Discrete Wavelet Transformation (DWT). DWT enables an efficient image representation for SR and reduces the spatial area of its input by a factor of 4, the overall model size, and computation cost, framing it as an attractive approach for sustainable ML. Our proposed DWA model improves wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We show its effectiveness by integrating it into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear improvement in classical SR tasks. Moreover, DWA enables a direct application of DWSR and MWCNN to input image space, reducing the DWT representation channel-wise since it omits traditional DWT.
</details>
<details>
<summary>摘要</summary>
Our proposed DWA model enhances wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain. This approach emphasizes local contrasts and suppresses common noise in the input signals. We demonstrate the effectiveness of DWA by integrating it into existing SR models, such as DWSR and MWCNN, and show a clear improvement in classical SR tasks.Moreover, DWA enables a direct application of DWSR and MWCNN to the input image space, reducing the DWT representation channel-wise since it omits traditional DWT. This simplifies the model architecture and reduces the computational cost, making it more sustainable and efficient.
</details></li>
</ul>
<hr>
<h2 id="TFR-Texture-Defect-Detection-with-Fourier-Transform-using-Normal-Reconstructed-Template-of-Simple-Autoencoder"><a href="#TFR-Texture-Defect-Detection-with-Fourier-Transform-using-Normal-Reconstructed-Template-of-Simple-Autoencoder" class="headerlink" title="TFR: Texture Defect Detection with Fourier Transform using Normal Reconstructed Template of Simple Autoencoder"></a>TFR: Texture Defect Detection with Fourier Transform using Normal Reconstructed Template of Simple Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04574">http://arxiv.org/abs/2307.04574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jongwook Si, Sungyoung Kim</li>
<li>for:  detection of texture defects in real-world images</li>
<li>methods:  simple autoencoder + Fourier transform analysis</li>
<li>results:  effective and accurate defect detection, demonstrated through experimental results<details>
<summary>Abstract</summary>
Texture is an essential information in image representation, capturing patterns and structures. As a result, texture plays a crucial role in the manufacturing industry and is extensively studied in the fields of computer vision and pattern recognition. However, real-world textures are susceptible to defects, which can degrade image quality and cause various issues. Therefore, there is a need for accurate and effective methods to detect texture defects. In this study, a simple autoencoder and Fourier transform are employed for texture defect detection. The proposed method combines Fourier transform analysis with the reconstructed template obtained from the simple autoencoder. Fourier transform is a powerful tool for analyzing the frequency domain of images and signals. Moreover, since texture defects often exhibit characteristic changes in specific frequency ranges, analyzing the frequency domain enables effective defect detection. The proposed method demonstrates effectiveness and accuracy in detecting texture defects. Experimental results are presented to evaluate its performance and compare it with existing approaches.
</details>
<details>
<summary>摘要</summary>
Texture 是图像表示中的重要信息，捕捉pattern和结构。由于这些Texture在生产业中扮演着重要的角色，因此在计算机视觉和模式识别领域中进行了广泛的研究。然而，实际世界中的Texture受到defect的影响，这会导致图像质量下降和多种问题。因此，需要一种准确和有效的方法来检测Texture defect。在本研究中，使用了简单的自适应神经网络和快推trasform来检测Texture defect。该方法将快推trasform分析与自适应神经网络重建的模板结合使用。快推trasform是图像和信号频谱分析的powerful工具，而Texture defects通常在特定频谱范围内表现出 caracteristic 变化，因此在频谱分析中可以实现有效的检测。该方法的实验结果表明其效果和准确性在检测Texture defect方面具有优势。与现有方法进行比较的实验结果也是如此。
</details></li>
</ul>
<hr>
<h2 id="CoactSeg-Learning-from-Heterogeneous-Data-for-New-Multiple-Sclerosis-Lesion-Segmentation"><a href="#CoactSeg-Learning-from-Heterogeneous-Data-for-New-Multiple-Sclerosis-Lesion-Segmentation" class="headerlink" title="CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation"></a>CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04513">http://arxiv.org/abs/2307.04513</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ycwu1997/coactseg">https://github.com/ycwu1997/coactseg</a></li>
<li>paper_authors: Yicheng Wu, Zhonghua Wu, Hengcan Shi, Bjoern Picker, Winston Chong, Jianfei Cai</li>
<li>for: 提高多发形股病（MS）临床治疗中新出现的肿瘤分割精度，以估计疾病进程和治疗效果。</li>
<li>methods: 利用不同时点样本的异类数据（新出现肿瘤注解两个时点样本和全部肿瘤注解单个时点样本），提出了一种协作分割（CoactSeg）框架，以提高新肿瘤分割精度。</li>
<li>results: 对于新肿瘤和全部肿瘤分割任务，通过利用不同时点样本和提出的关系常量约束，可以显著提高分割精度。同时，还提供了一个MS-23v1 dataset，包括38个澳大利亚单个时点样本的全部肿瘤标签。<details>
<summary>Abstract</summary>
New lesion segmentation is essential to estimate the disease progression and therapeutic effects during multiple sclerosis (MS) clinical treatments. However, the expensive data acquisition and expert annotation restrict the feasibility of applying large-scale deep learning models. Since single-time-point samples with all-lesion labels are relatively easy to collect, exploiting them to train deep models is highly desirable to improve new lesion segmentation. Therefore, we proposed a coaction segmentation (CoactSeg) framework to exploit the heterogeneous data (i.e., new-lesion annotated two-time-point data and all-lesion annotated single-time-point data) for new MS lesion segmentation. The CoactSeg model is designed as a unified model, with the same three inputs (the baseline, follow-up, and their longitudinal brain differences) and the same three outputs (the corresponding all-lesion and new-lesion predictions), no matter which type of heterogeneous data is being used. Moreover, a simple and effective relation regularization is proposed to ensure the longitudinal relations among the three outputs to improve the model learning. Extensive experiments demonstrate that utilizing the heterogeneous data and the proposed longitudinal relation constraint can significantly improve the performance for both new-lesion and all-lesion segmentation tasks. Meanwhile, we also introduce an in-house MS-23v1 dataset, including 38 Oceania single-time-point samples with all-lesion labels. Codes and the dataset are released at https://github.com/ycwu1997/CoactSeg.
</details>
<details>
<summary>摘要</summary>
新的肿瘤分割是 Multiple Sclerosis (MS) 诊断和治疗中的关键，但是收集大规模的数据和专家标注的成本限制了应用大规模深度学习模型的可能性。由于单个时间点样本中的所有肿瘤标签比较容易获得，因此可以利用它们来训练深度模型以提高新的肿瘤分割。为此，我们提出了一个合作分割（CoactSeg）框架，可以利用不同类型的数据（新的肿瘤标签的两个时间点数据和所有肿瘤标签的单个时间点数据）来进行新的肿瘤分割。CoactSeg 模型设计为一个统一的模型，三个输入（基线、追视和它们之间的脑部差异）和三个输出（相应的所有肿瘤和新肿瘤预测），不管使用哪种不同类型的数据。此外，我们还提出了一种简单而有效的时间相关约束，以保证新肿瘤和所有肿瘤之间的长期关系，从而提高模型学习。我们的实验表明，利用不同类型的数据和提议的时间相关约束可以大幅提高新肿瘤和所有肿瘤分割任务的表现。此外，我们还释放了一个 MS-23v1 数据集，包括 38 个澳大利亚单个时间点样本，每个样本均有所有肿瘤标签。代码和数据可以在 <https://github.com/ycwu1997/CoactSeg> 上下载。
</details></li>
</ul>
<hr>
<h2 id="SAM-IQA-Can-Segment-Anything-Boost-Image-Quality-Assessment"><a href="#SAM-IQA-Can-Segment-Anything-Boost-Image-Quality-Assessment" class="headerlink" title="SAM-IQA: Can Segment Anything Boost Image Quality Assessment?"></a>SAM-IQA: Can Segment Anything Boost Image Quality Assessment?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04455">http://arxiv.org/abs/2307.04455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hedlen/sam-iqa">https://github.com/hedlen/sam-iqa</a></li>
<li>paper_authors: Xinpeng Li, Ting Jiang, Haoqiang Fan, Shuaicheng Liu</li>
<li>for: 本研究旨在提高图像质量评估（IQA） task 的准确性，通过使用大量数据进行训练。</li>
<li>methods: 本研究使用 Segment Anything 模型的encoder部分进行高级别 semantic feature extraction，并利用 Fourier 和标准卷积来提取频域特征。</li>
<li>results: 对四个代表性的数据集进行了广泛的实验，结果表明我们的方法可以比STATE-OF-THE-ART 高效， both qualitatively 和 quantitatively。<details>
<summary>Abstract</summary>
Image Quality Assessment (IQA) is a challenging task that requires training on massive datasets to achieve accurate predictions. However, due to the lack of IQA data, deep learning-based IQA methods typically rely on pre-trained networks trained on massive datasets as feature extractors to enhance their generalization ability, such as the ResNet network trained on ImageNet. In this paper, we utilize the encoder of Segment Anything, a recently proposed segmentation model trained on a massive dataset, for high-level semantic feature extraction. Most IQA methods are limited to extracting spatial-domain features, while frequency-domain features have been shown to better represent noise and blur. Therefore, we leverage both spatial-domain and frequency-domain features by applying Fourier and standard convolutions on the extracted features, respectively. Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively and quantitatively. Our experiments confirm the powerful feature extraction capabilities of Segment Anything and highlight the value of combining spatial-domain and frequency-domain features in IQA tasks. Code: https://github.com/Hedlen/SAM-IQA
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Identification-of-Hemorrhage-and-Infarct-Lesions-on-Brain-CT-Images-using-Deep-Learning"><a href="#Identification-of-Hemorrhage-and-Infarct-Lesions-on-Brain-CT-Images-using-Deep-Learning" class="headerlink" title="Identification of Hemorrhage and Infarct Lesions on Brain CT Images using Deep Learning"></a>Identification of Hemorrhage and Infarct Lesions on Brain CT Images using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04425">http://arxiv.org/abs/2307.04425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arunkumar Govindarajan, Arjun Agarwal, Subhankar Chattoraj, Dennis Robert, Satish Golla, Ujjwal Upadhyay, Swetha Tanamala, Aarthi Govindarajan</li>
<li>for: 鉴定非contrast computed tomography（NCCT）头部影像的潜在疾病</li>
<li>methods: 使用深度学习（DL）基本的计算机助临 diagnosis（CAD）模型</li>
<li>results: 对头部NCCT影像的自动识别出血肿和损伤的可能性和局限性<details>
<summary>Abstract</summary>
Head Non-contrast computed tomography (NCCT) scan remain the preferred primary imaging modality due to their widespread availability and speed. However, the current standard for manual annotations of abnormal brain tissue on head NCCT scans involves significant disadvantages like lack of cutoff standardization and degeneration identification. The recent advancement of deep learning-based computer-aided diagnostic (CAD) models in the multidisciplinary domain has created vast opportunities in neurological medical imaging. Significant literature has been published earlier in the automated identification of brain tissue on different imaging modalities. However, determining Intracranial hemorrhage (ICH) and infarct can be challenging due to image texture, volume size, and scan quality variability. This retrospective validation study evaluated a DL-based algorithm identifying ICH and infarct from head-NCCT scans. The head-NCCT scans dataset was collected consecutively from multiple diagnostic imaging centers across India. The study exhibits the potential and limitations of such DL-based software for introduction in routine workflow in extensive healthcare facilities.
</details>
<details>
<summary>摘要</summary>
head non-contrast computed tomography (NCCT) 扫描仍然是 primary imaging modality 的首选方式，因为它们在可用性和速度方面具有广泛的优势。然而，现有的手动标注病理脑组织在 head NCCT 扫描中存在一些缺点，如标准化标注的缺乏和衰变识别。Recent Advances 在多学科领域的 computer-aided diagnostic (CAD) 模型中，已经创造了巨大的机会，特别是在神经科医学影像领域。 Earlier literature 已经发表了自动识别不同 imaging modalities 中的脑组织。然而，由于图像 текстура、体积大小和扫描质量的变化，识别Intracranial hemorrhage (ICH) 和衰变可以是困难的。本回顾验证研究检查了一种基于深度学习 (DL) 的算法，可以从 head-NCCT 扫描中识别 ICH 和衰变。该 dataset 是从印度多个诊断影像中心收集的 consecutively。研究表明了这种 DL-based 软件的潜在和局限性，并探讨了在广泛的医疗设施中的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Towards-Enabling-Cardiac-Digital-Twins-of-Myocardial-Infarction-Using-Deep-Computational-Models-for-Inverse-Inference"><a href="#Towards-Enabling-Cardiac-Digital-Twins-of-Myocardial-Infarction-Using-Deep-Computational-Models-for-Inverse-Inference" class="headerlink" title="Towards Enabling Cardiac Digital Twins of Myocardial Infarction Using Deep Computational Models for Inverse Inference"></a>Towards Enabling Cardiac Digital Twins of Myocardial Infarction Using Deep Computational Models for Inverse Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04421">http://arxiv.org/abs/2307.04421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Li, Julia Camps, Zhinuo, Wang, Abhirup Banerjee, Marcel Beetz, Blanca Rodriguez, Vicente Grau</li>
<li>for: 这个论文的目的是开发一个基于电cardiac digital twins (CDTs)的个性化诊断和治疗规划系统，以便更准确地诊断心肺病。</li>
<li>methods: 该论文使用了多Modal数据，包括心肺成像和电cardiacogram (ECG)，以提高推断肉粉组织特性的准确性和可靠性。具体来说，该论文采用了一种深度计算模型，通过对QRS信号和相应的损肉区域之间的复杂关系进行捕捉，来推断损肉的位置和分布。</li>
<li>results: 在计算实验中，该模型能够有效地捕捉QRS信号和相应的损肉区域之间的复杂关系，并且在未来的临床应用中具有扎实的潜在性。<details>
<summary>Abstract</summary>
Myocardial infarction (MI) demands precise and swift diagnosis. Cardiac digital twins (CDTs) have the potential to offer individualized evaluation of cardiac function in a non-invasive manner, making them a promising approach for personalized diagnosis and treatment planning of MI. The inference of accurate myocardial tissue properties is crucial in creating a reliable CDT platform, and particularly in the context of studying MI. In this work, we investigate the feasibility of inferring myocardial tissue properties from the electrocardiogram (ECG), focusing on the development of a comprehensive CDT platform specifically designed for MI. The platform integrates multi-modal data, such as cardiac MRI and ECG, to enhance the accuracy and reliability of the inferred tissue properties. We perform a sensitivity analysis based on computer simulations, systematically exploring the effects of infarct location, size, degree of transmurality, and electrical activity alteration on the simulated QRS complex of ECG, to establish the limits of the approach. We subsequently propose a deep computational model to infer infarct location and distribution from the simulated QRS. The in silico experimental results show that our model can effectively capture the complex relationships between the QRS signals and the corresponding infarct regions, with promising potential for clinical application in the future. The code will be released publicly once the manuscript is accepted for publication.
</details>
<details>
<summary>摘要</summary>
In this study, we investigate the feasibility of inferring myocardial tissue properties from the electrocardiogram (ECG) to develop a comprehensive CDT platform specifically designed for MI. Our platform integrates multi-modal data, such as cardiac MRI and ECG, to enhance the accuracy and reliability of the inferred tissue properties.We performed a sensitivity analysis based on computer simulations to explore the effects of infarct location, size, degree of transmurality, and electrical activity alteration on the simulated QRS complex of ECG. Our results show that our approach has limitations, but our deep computational model can effectively capture the complex relationships between the QRS signals and the corresponding infarct regions, with promising potential for clinical application in the future. The code will be publicly released once the manuscript is accepted for publication.
</details></li>
</ul>
<hr>
<h2 id="K-Space-Aware-Cross-Modality-Score-for-Synthesized-Neuroimage-Quality-Assessment"><a href="#K-Space-Aware-Cross-Modality-Score-for-Synthesized-Neuroimage-Quality-Assessment" class="headerlink" title="K-Space-Aware Cross-Modality Score for Synthesized Neuroimage Quality Assessment"></a>K-Space-Aware Cross-Modality Score for Synthesized Neuroimage Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04296">http://arxiv.org/abs/2307.04296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinbao Wang, Guoyang Xie, Yawen Huang, Jiayi Lyu, Feng Zheng, Yefeng Zheng, Yaochu Jin</li>
<li>for: This paper aims to address the problem of assessing cross-modality medical image synthesis, which has been largely unexplored and neglected by existing measures such as PSNR and SSIM.</li>
<li>methods: The proposed method, called K-CROSS, uses a pre-trained multi-modality segmentation network to predict lesion locations, together with a tumor encoder to represent features such as texture details and brightness intensities. Both k-space features and vision features are obtained and employed in comprehensive encoders with a frequency reconstruction penalty. The structure-shared encoders are designed and constrained with a similarity loss to capture the intrinsic common structural information for both modalities.</li>
<li>results: The proposed method outperforms other metrics, especially when compared with radiologists on a large-scale cross-modality neuroimaging perceptual similarity (NIRPS) dataset with 6,000 radiologist judgments.<details>
<summary>Abstract</summary>
The problem of how to assess cross-modality medical image synthesis has been largely unexplored. The most used measures like PSNR and SSIM focus on analyzing the structural features but neglect the crucial lesion location and fundamental k-space speciality of medical images. To overcome this problem, we propose a new metric K-CROSS to spur progress on this challenging problem. Specifically, K-CROSS uses a pre-trained multi-modality segmentation network to predict the lesion location, together with a tumor encoder for representing features, such as texture details and brightness intensities. To further reflect the frequency-specific information from the magnetic resonance imaging principles, both k-space features and vision features are obtained and employed in our comprehensive encoders with a frequency reconstruction penalty. The structure-shared encoders are designed and constrained with a similarity loss to capture the intrinsic common structural information for both modalities. As a consequence, the features learned from lesion regions, k-space, and anatomical structures are all captured, which serve as our quality evaluators. We evaluate the performance by constructing a large-scale cross-modality neuroimaging perceptual similarity (NIRPS) dataset with 6,000 radiologist judgments. Extensive experiments demonstrate that the proposed method outperforms other metrics, especially in comparison with the radiologists on NIRPS.
</details>
<details>
<summary>摘要</summary>
医疗影像合成的跨Modalidad评估问题一直受到了相对少数研究。现有的度量方法，如PSNR和SSIM，主要关注医疗影像的结构特征，而忽略了重要的疾病位置和基本的k-空间特点。为了解决这个问题，我们提出了一个新的度量方法，即K-CROSS。Specifically，K-CROSS使用一个预训练的多Modalidad分割网络预测疾病位置，并使用一个恶性编码器来表示特征，如纹理细节和明亮度。为了更好地反映医疗影像的频率特征，我们在我们的全面编码器中使用频率重建罚款。同时，我们设计了结构共享编码器，并使用一个相似损失来捕捉两Modalidad之间的共同结构信息。因此，我们可以从疾病区域、k-空间和解剖结构中 capture所有的特征，这些特征作为我们的质量评估器。我们通过构建一个大规模的跨Modalidad神经成像相似度（NIRPS）数据集，并对6,000名医生判断进行评估，来评估性能。广泛的实验表明，我们提出的方法在比较其他度量方法时，尤其是与医生在NIRPS上的评估中，表现出了优异。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/10/eess.IV_2023_07_10/" data-id="clon21ix7011or5887tm13cjl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/cs.SD_2023_07_09/" class="article-date">
  <time datetime="2023-07-09T15:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/09/cs.SD_2023_07_09/">cs.SD - 2023-07-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Can-Generative-Large-Language-Models-Perform-ASR-Error-Correction"><a href="#Can-Generative-Large-Language-Models-Perform-ASR-Error-Correction" class="headerlink" title="Can Generative Large Language Models Perform ASR Error Correction?"></a>Can Generative Large Language Models Perform ASR Error Correction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04172">http://arxiv.org/abs/2307.04172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rao Ma, Mengjie Qian, Potsawee Manakul, Mark Gales, Kate Knill</li>
<li>for: 提高 ASR 系统的表现，使其更加准确和有效。</li>
<li>methods: 使用 ChatGPT 大语言模型进行零次或一次学习，对 ASR N-best 列表进行错误修正。</li>
<li>results: 对 Conformer-Transducer 模型和预训练的 Whisper 模型进行错误修正，可以大幅提高 ASR 系统的表现。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
ASR error correction continues to serve as an important part of post-processing for speech recognition systems. Traditionally, these models are trained with supervised training using the decoding results of the underlying ASR system and the reference text. This approach is computationally intensive and the model needs to be re-trained when switching the underlying ASR model. Recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. In this paper, we take ChatGPT as an example to examine its ability to perform ASR error correction in the zero-shot or 1-shot settings. We use the ASR N-best list as model input and propose unconstrained error correction and N-best constrained error correction methods. Results on a Conformer-Transducer model and the pre-trained Whisper model show that we can largely improve the ASR system performance with error correction using the powerful ChatGPT model.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>ASR错误修正仍然serve as重要的后处理步骤 для语音识别系统。传统上，这些模型通过指导学习使用下面ASR系统的解码结果和参考文本进行训练。这种方法是计算机程序昂贵，模型需要在Switching beneath ASR模型时重新训练。 recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. In this paper, we take ChatGPT as an example to examine its ability to perform ASR error correction in the zero-shot or 1-shot settings. We use the ASR N-best list as model input and propose unconstrained error correction and N-best constrained error correction methods. Results on a Conformer-Transducer model and the pre-trained Whisper model show that we can largely improve the ASR system performance with error correction using the powerful ChatGPT model.Note: The translation is done using Google Translate, which may not be perfect, but it should give you a good idea of the content in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Emotion-Guided-Music-Accompaniment-Generation-Based-on-Variational-Autoencoder"><a href="#Emotion-Guided-Music-Accompaniment-Generation-Based-on-Variational-Autoencoder" class="headerlink" title="Emotion-Guided Music Accompaniment Generation Based on Variational Autoencoder"></a>Emotion-Guided Music Accompaniment Generation Based on Variational Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04015">http://arxiv.org/abs/2307.04015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/duoluoluos/emotion-guided-music-accompaniment-generation">https://github.com/duoluoluos/emotion-guided-music-accompaniment-generation</a></li>
<li>paper_authors: Qi Wang, Shubing Zhang, Li Zhou</li>
<li>for: 本研究旨在解决人工智能在音乐创作过程中困难地 integrate 人类情感创作美妙的伴奏乐曲。</li>
<li>methods: 我们提议使用一种易于表示情感流程的曲线模型，即Valence&#x2F;Arousal Curve，以实现模型内部情感信息的兼容，并使用变量自动编码器作为模型结构以提高情感因素的解释性。 而Relative Self-Attention 技术也用于保持音乐句子水平结构和生成更加丰富的伴奏乐曲。</li>
<li>results: 我们的方法可以增强音乐创作过程中 AI 的情感创作能力，并生成更加美妙的伴奏乐曲。<details>
<summary>Abstract</summary>
Music accompaniment generation is a crucial aspect in the composition process. Deep neural networks have made significant strides in this field, but it remains a challenge for AI to effectively incorporate human emotions to create beautiful accompaniments. Existing models struggle to effectively characterize human emotions within neural network models while composing music. To address this issue, we propose the use of an easy-to-represent emotion flow model, the Valence/Arousal Curve, which allows for the compatibility of emotional information within the model through data transformation and enhances interpretability of emotional factors by utilizing a Variational Autoencoder as the model structure. Further, we used relative self-attention to maintain the structure of the music at music phrase level and to generate a richer accompaniment when combined with the rules of music theory.
</details>
<details>
<summary>摘要</summary>
音乐伴奏生成是作曲过程中的一个重要方面。深度神经网络在这个领域已经做出了很大的进步，但是AI还未能够有效地涵盖人类情感以创造美妙的伴奏。现有的模型很难准确地捕捉人类情感信息在神经网络模型中，而且通常会导致模型难以理解和描述情感因素。为解决这个问题，我们提议使用一种容易表达情感流程的曲线模型，即情感值/刺激曲线，该模型通过数据转换来兼容情感信息，并通过使用变量自动编码器结构来提高情感因素的解释性。此外，我们使用相对自注意力来保持音乐段级结构和生成更加丰富的伴奏，并与音乐理论规则相结合。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/cs.SD_2023_07_09/" data-id="clon21iti00rmr5884ds48dgr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/77/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/76/">76</a><a class="page-number" href="/page/77/">77</a><span class="page-number current">78</span><a class="page-number" href="/page/79/">79</a><a class="page-number" href="/page/80/">80</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/79/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">113</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">63</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
