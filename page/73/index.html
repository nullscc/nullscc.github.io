
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/73/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_07_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/17/cs.CV_2023_07_17/" class="article-date">
  <time datetime="2023-07-17T13:00:00.000Z" itemprop="datePublished">2023-07-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/cs.CV_2023_07_17/">cs.CV - 2023-07-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Identity-Preserving-Aging-of-Face-Images-via-Latent-Diffusion-Models"><a href="#Identity-Preserving-Aging-of-Face-Images-via-Latent-Diffusion-Models" class="headerlink" title="Identity-Preserving Aging of Face Images via Latent Diffusion Models"></a>Identity-Preserving Aging of Face Images via Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08585">http://arxiv.org/abs/2307.08585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudipta Banerjee, Govind Mittal, Ameya Joshi, Chinmay Hegde, Nasir Memon</li>
<li>for: 这个论文是为了提高自动人脸识别系统的性能而写的。</li>
<li>methods: 这个论文使用了文本到图像扩散模型来 sintetically 年轻和年老人脸图像。</li>
<li>results: 这个方法可以通过几个尝试的训练来实现高度的视觉实际性和生物度的精度。在两个标准测试集（CelebA和AgeDB）上，这个方法比现有的状态调学基eline减少了约44%的False Non-Match Rate。<details>
<summary>Abstract</summary>
The performance of automated face recognition systems is inevitably impacted by the facial aging process. However, high quality datasets of individuals collected over several years are typically small in scale. In this work, we propose, train, and validate the use of latent text-to-image diffusion models for synthetically aging and de-aging face images. Our models succeed with few-shot training, and have the added benefit of being controllable via intuitive textual prompting. We observe high degrees of visual realism in the generated images while maintaining biometric fidelity measured by commonly used metrics. We evaluate our method on two benchmark datasets (CelebA and AgeDB) and observe significant reduction (~44%) in the False Non-Match Rate compared to existing state-of the-art baselines.
</details>
<details>
<summary>摘要</summary>
“自动人脸识别系统的表现必然受到人脸年龄变化的影响。然而，高质量的个人数据库，收集了几年，通常规模不大。在这项工作中，我们提议、训练和验证了使用潜在的文本到图像扩散模型来人工增加和减少人脸图像的年龄。我们的模型在几次训练后达到了高度的视觉实际性和生物特征准确度，而且通过直观的文本提示来控制。我们对两个标准数据集（CelebA和AgeDB）进行评估，与现有的州态艺法基elines进行比较， Observation  False Non-Match Rate 下降约44%。”Note: Please keep in mind that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and other countries.
</details></li>
</ul>
<hr>
<h2 id="Scale-Aware-Modulation-Meet-Transformer"><a href="#Scale-Aware-Modulation-Meet-Transformer" class="headerlink" title="Scale-Aware Modulation Meet Transformer"></a>Scale-Aware Modulation Meet Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08579">http://arxiv.org/abs/2307.08579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/afeng-x/smt">https://github.com/afeng-x/smt</a></li>
<li>paper_authors: Weifeng Lin, Ziheng Wu, Jiayu Chen, Jun Huang, Lianwen Jin</li>
<li>For: The paper proposes a new vision Transformer called Scale-Aware Modulation Transformer (SMT) that can handle various downstream tasks efficiently by combining convolutional networks and vision Transformers.* Methods: The proposed SMT includes two novel designs: Multi-Head Mixed Convolution (MHMC) and Scale-Aware Aggregation (SAA) modules. These modules enhance convolutional modulation and allow the network to capture multi-scale features and fuse information effectively.* Results: The proposed SMT significantly outperforms existing state-of-the-art models across a wide range of visual tasks, including image classification, object detection, and semantic segmentation. Specifically, SMT achieves 82.2% and 84.3% top-1 accuracy on ImageNet-1K, and outperforms the Swin Transformer counterpart by 4.2 and 1.3 mAP on COCO for object detection and 2.0 and 1.1 mIoU on ADE20K for semantic segmentation.<details>
<summary>Abstract</summary>
This paper presents a new vision Transformer, Scale-Aware Modulation Transformer (SMT), that can handle various downstream tasks efficiently by combining the convolutional network and vision Transformer. The proposed Scale-Aware Modulation (SAM) in the SMT includes two primary novel designs. Firstly, we introduce the Multi-Head Mixed Convolution (MHMC) module, which can capture multi-scale features and expand the receptive field. Secondly, we propose the Scale-Aware Aggregation (SAA) module, which is lightweight but effective, enabling information fusion across different heads. By leveraging these two modules, convolutional modulation is further enhanced. Furthermore, in contrast to prior works that utilized modulations throughout all stages to build an attention-free network, we propose an Evolutionary Hybrid Network (EHN), which can effectively simulate the shift from capturing local to global dependencies as the network becomes deeper, resulting in superior performance. Extensive experiments demonstrate that SMT significantly outperforms existing state-of-the-art models across a wide range of visual tasks. Specifically, SMT with 11.5M / 2.4GFLOPs and 32M / 7.7GFLOPs can achieve 82.2% and 84.3% top-1 accuracy on ImageNet-1K, respectively. After pretrained on ImageNet-22K in 224^2 resolution, it attains 87.1% and 88.1% top-1 accuracy when finetuned with resolution 224^2 and 384^2, respectively. For object detection with Mask R-CNN, the SMT base trained with 1x and 3x schedule outperforms the Swin Transformer counterpart by 4.2 and 1.3 mAP on COCO, respectively. For semantic segmentation with UPerNet, the SMT base test at single- and multi-scale surpasses Swin by 2.0 and 1.1 mIoU respectively on the ADE20K.
</details>
<details>
<summary>摘要</summary>
In contrast to prior works that use modulations throughout all stages to build an attention-free network, the proposed Evolutionary Hybrid Network (EHN) can effectively simulate the shift from capturing local to global dependencies as the network becomes deeper, resulting in superior performance.Extensive experiments show that SMT significantly outperforms existing state-of-the-art models across a wide range of visual tasks. Specifically, SMT with 11.5M parameters and 2.4GFLOPs can achieve 82.2% top-1 accuracy on ImageNet-1K, while SMT with 32M parameters and 7.7GFLOPs can achieve 84.3% top-1 accuracy. After pretraining on ImageNet-22K in 224^2 resolution, the model can achieve 87.1% and 88.1% top-1 accuracy when finetuned with resolution 224^2 and 384^2, respectively.In object detection with Mask R-CNN, the SMT base trained with 1x and 3x schedule outperforms the Swin Transformer counterpart by 4.2 and 1.3 mAP on COCO, respectively. For semantic segmentation with UPerNet, the SMT base test at single- and multi-scale surpasses Swin by 2.0 and 1.1 mIoU, respectively, on the ADE20K.
</details></li>
</ul>
<hr>
<h2 id="On-the-Fly-Neural-Style-Smoothing-for-Risk-Averse-Domain-Generalization"><a href="#On-the-Fly-Neural-Style-Smoothing-for-Risk-Averse-Domain-Generalization" class="headerlink" title="On the Fly Neural Style Smoothing for Risk-Averse Domain Generalization"></a>On the Fly Neural Style Smoothing for Risk-Averse Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08551">http://arxiv.org/abs/2307.08551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akshaymehra24/riskaversedg">https://github.com/akshaymehra24/riskaversedg</a></li>
<li>paper_authors: Akshay Mehra, Yunbei Zhang, Bhavya Kailkhura, Jihun Hamm</li>
<li>For: 该论文目的是提出一种测试时 neural style smoothing（TT-NSS）方法，以提高预测不同域的风险敏感性。* Methods: 该方法使用一个“风格平滑”的 DG 分类器进行测试时预测，并使用 neural style transfer 模块来快速地在测试图像上实现风格平滑。* Results: 实验结果表明，TT-NSS 和 NSS 可以提高 DG 分类器在未经见过的域上的预测精度和风险敏感性。<details>
<summary>Abstract</summary>
Achieving high accuracy on data from domains unseen during training is a fundamental challenge in domain generalization (DG). While state-of-the-art DG classifiers have demonstrated impressive performance across various tasks, they have shown a bias towards domain-dependent information, such as image styles, rather than domain-invariant information, such as image content. This bias renders them unreliable for deployment in risk-sensitive scenarios such as autonomous driving where a misclassification could lead to catastrophic consequences. To enable risk-averse predictions from a DG classifier, we propose a novel inference procedure, Test-Time Neural Style Smoothing (TT-NSS), that uses a "style-smoothed" version of the DG classifier for prediction at test time. Specifically, the style-smoothed classifier classifies a test image as the most probable class predicted by the DG classifier on random re-stylizations of the test image. TT-NSS uses a neural style transfer module to stylize a test image on the fly, requires only black-box access to the DG classifier, and crucially, abstains when predictions of the DG classifier on the stylized test images lack consensus. Additionally, we propose a neural style smoothing (NSS) based training procedure that can be seamlessly integrated with existing DG methods. This procedure enhances prediction consistency, improving the performance of TT-NSS on non-abstained samples. Our empirical results demonstrate the effectiveness of TT-NSS and NSS at producing and improving risk-averse predictions on unseen domains from DG classifiers trained with SOTA training methods on various benchmark datasets and their variations.
</details>
<details>
<summary>摘要</summary>
Specifically, the style-smoothed classifier predicts the most probable class based on the DG classifier's predictions on random re-stylizations of the test image. TT-NSS uses a neural style transfer module to stylize the test image on the fly, requires only black-box access to the DG classifier, and abstains when the DG classifier's predictions on the stylized test images lack consensus. Additionally, we propose a neural style smoothing (NSS) based training procedure that can be integrated with existing DG methods. This procedure enhances prediction consistency, improving the performance of TT-NSS on non-abstained samples.Our empirical results show that TT-NSS and NSS are effective in producing and improving risk-averse predictions on unseen domains for DG classifiers trained with state-of-the-art methods on various benchmark datasets and their variations.
</details></li>
</ul>
<hr>
<h2 id="Improving-Data-Efficiency-for-Plant-Cover-Prediction-with-Label-Interpolation-and-Monte-Carlo-Cropping"><a href="#Improving-Data-Efficiency-for-Plant-Cover-Prediction-with-Label-Interpolation-and-Monte-Carlo-Cropping" class="headerlink" title="Improving Data Efficiency for Plant Cover Prediction with Label Interpolation and Monte-Carlo Cropping"></a>Improving Data Efficiency for Plant Cover Prediction with Label Interpolation and Monte-Carlo Cropping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08559">http://arxiv.org/abs/2307.08559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Körschens, Solveig Franziska Bucher, Christine Römermann, Joachim Denzler<br>for: 这个论文主要针对的是如何使用自动摄像头系统和深度学习算法对植被plot进行自动分类。methods: 这篇论文使用了自动摄像头系统收集高分辨率图像，然后使用深度学习算法对图像进行分类。另外，论文还引入了一种新的 Monte-Carlo Cropping 方法，用于处理高分辨率图像，并且可以增加训练数据集的大小。results: 论文的实验结果表明，使用自动摄像头系统和深度学习算法可以对植被plot进行高精度的自动分类，并且可以提高种类、社区和分割指标。此外， Monte-Carlo Cropping 方法也能够提高训练数据集的大小和模型的性能。<details>
<summary>Abstract</summary>
The plant community composition is an essential indicator of environmental changes and is, for this reason, usually analyzed in ecological field studies in terms of the so-called plant cover. The manual acquisition of this kind of data is time-consuming, laborious, and prone to human error. Automated camera systems can collect high-resolution images of the surveyed vegetation plots at a high frequency. In combination with subsequent algorithmic analysis, it is possible to objectively extract information on plant community composition quickly and with little human effort. An automated camera system can easily collect the large amounts of image data necessary to train a Deep Learning system for automatic analysis. However, due to the amount of work required to annotate vegetation images with plant cover data, only few labeled samples are available. As automated camera systems can collect many pictures without labels, we introduce an approach to interpolate the sparse labels in the collected vegetation plot time series down to the intermediate dense and unlabeled images to artificially increase our training dataset to seven times its original size. Moreover, we introduce a new method we call Monte-Carlo Cropping. This approach trains on a collection of cropped parts of the training images to deal with high-resolution images efficiently, implicitly augment the training images, and speed up training. We evaluate both approaches on a plant cover dataset containing images of herbaceous plant communities and find that our methods lead to improvements in the species, community, and segmentation metrics investigated.
</details>
<details>
<summary>摘要</summary>
plant 社区组成是环境变化的重要指标，因此通常在生态场景研究中用 Plant cover 来进行分析。 however， manual acquisition of this kind of data is time-consuming, laborious, and prone to human error。 automatic camera systems can collect high-resolution images of the surveyed vegetation plots at a high frequency，并且可以使用后续的算法分析以获取植物社区组成的信息。 due to the amount of work required to annotate vegetation images with plant cover data， only a few labeled samples are available。 therefore， we introduce an approach to interpolate the sparse labels in the collected vegetation plot time series down to the intermediate dense and unlabeled images to artificially increase our training dataset to seven times its original size。 furthermore， we introduce a new method called Monte-Carlo Cropping。 this approach trains on a collection of cropped parts of the training images to deal with high-resolution images efficiently， implicitly augment the training images， and speed up training。we evaluate both approaches on a plant cover dataset containing images of herbaceous plant communities and find that our methods lead to improvements in the species， community， and segmentation metrics investigated。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Reconstructed-Convolution-Module-Based-Look-Up-Tables-for-Efficient-Image-Super-Resolution"><a href="#Reconstructed-Convolution-Module-Based-Look-Up-Tables-for-Efficient-Image-Super-Resolution" class="headerlink" title="Reconstructed Convolution Module Based Look-Up Tables for Efficient Image Super-Resolution"></a>Reconstructed Convolution Module Based Look-Up Tables for Efficient Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08544">http://arxiv.org/abs/2307.08544</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuguandu/rc-lut">https://github.com/liuguandu/rc-lut</a></li>
<li>paper_authors: Guandu Liu, Yukang Ding, Mading Li, Ming Sun, Xing Wen, Bin Wang</li>
<li>for: 提高单图超分辨率（SR）任务中的灵活性和效率。</li>
<li>methods: 提出了一种新的重构 convolution（RC）模块，通过分离通道和空间计算来减少 LUT 的存储量，同时可以扩大 RF 的大小。</li>
<li>results: 比对于 state-of-the-art LUT-based SR 方法，提出的 RCLUT 方法可以在五个 популяр的benchmark dataset上 achieve 9 倍的 RF 大小扩展和优秀的性能，并且可以作为 LUT-based SR 方法的插件来提高其效果。<details>
<summary>Abstract</summary>
Look-up table(LUT)-based methods have shown the great efficacy in single image super-resolution (SR) task. However, previous methods ignore the essential reason of restricted receptive field (RF) size in LUT, which is caused by the interaction of space and channel features in vanilla convolution. They can only increase the RF at the cost of linearly increasing LUT size. To enlarge RF with contained LUT sizes, we propose a novel Reconstructed Convolution(RC) module, which decouples channel-wise and spatial calculation. It can be formulated as $n^2$ 1D LUTs to maintain $n\times n$ receptive field, which is obviously smaller than $n\times n$D LUT formulated before. The LUT generated by our RC module reaches less than 1/10000 storage compared with SR-LUT baseline. The proposed Reconstructed Convolution module based LUT method, termed as RCLUT, can enlarge the RF size by 9 times than the state-of-the-art LUT-based SR method and achieve superior performance on five popular benchmark dataset. Moreover, the efficient and robust RC module can be used as a plugin to improve other LUT-based SR methods. The code is available at https://github.com/liuguandu/RC-LUT.
</details>
<details>
<summary>摘要</summary>
Look-up table（LUT）基本方法在单图超解像（SR）任务中表现出色。然而，之前的方法忽视了LUT中受限的接收场（RF）大小的重要原因，这是因为混合空间和通道特征导致的vanilla convolution中的交互作用。它们只能通过linearly增加LUT大小来增加RF。为了使RF增加而不是LUT大小，我们提出了一种新的Reconstructed Convolution（RC）模块，它可以将通道和空间计算解耦。它可以表示为n^2个1D LUT，以维持n×n的接收场，这比之前的n×nD LUT更小。我们的RC模块生成的LUT可以达到与SR-LUT基准值的less than 1/10000的存储量。我们提出的Reconstructed Convolution模块基于LUT方法，称为RCLUT，可以将RF大小提高9倍于当前LUT基本SR方法，并在五个流行的benchmark dataset上达到更高的性能。此外，RC模块可以作为LUT基本SR方法的插件来改进其性能。代码可以在https://github.com/liuguandu/RC-LUT中找到。
</details></li>
</ul>
<hr>
<h2 id="Variational-Probabilistic-Fusion-Network-for-RGB-T-Semantic-Segmentation"><a href="#Variational-Probabilistic-Fusion-Network-for-RGB-T-Semantic-Segmentation" class="headerlink" title="Variational Probabilistic Fusion Network for RGB-T Semantic Segmentation"></a>Variational Probabilistic Fusion Network for RGB-T Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08536">http://arxiv.org/abs/2307.08536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baihong Lin, Zengrong Lin, Yulan Guo, Yulan Zhang, Jianxiao Zou, Shicai Fan</li>
<li>for: 本研究旨在提高RGB-T semantic segmentation的精度和稳定性，以应对具有差异光照条件的困难景象。</li>
<li>methods: 本研究提出了一种新的Variational Probabilistic Fusion Network（VPFNet），它视融合特征为Random Variables，通过多个抽象样本来实现稳定的分类。在VPFNet中，Variational Feature Fusion Module（VFFM）通过差异注意力来实现随机样本生成。此外，为了避免类归一致和模式偏好，我们使用了Weighted Cross-Entropy损失函数，并在VFFM中引入了灯光和类别的先验信息。</li>
<li>results: 实验结果表明，提出的VPFNet可以在MFNet和PST900 dataset上达到当今最佳的分类性能。<details>
<summary>Abstract</summary>
RGB-T semantic segmentation has been widely adopted to handle hard scenes with poor lighting conditions by fusing different modality features of RGB and thermal images. Existing methods try to find an optimal fusion feature for segmentation, resulting in sensitivity to modality noise, class-imbalance, and modality bias. To overcome the problems, this paper proposes a novel Variational Probabilistic Fusion Network (VPFNet), which regards fusion features as random variables and obtains robust segmentation by averaging segmentation results under multiple samples of fusion features. The random samples generation of fusion features in VPFNet is realized by a novel Variational Feature Fusion Module (VFFM) designed based on variation attention. To further avoid class-imbalance and modality bias, we employ the weighted cross-entropy loss and introduce prior information of illumination and category to control the proposed VFFM. Experimental results on MFNet and PST900 datasets demonstrate that the proposed VPFNet can achieve state-of-the-art segmentation performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-class-point-cloud-completion-networks-for-3D-cardiac-anatomy-reconstruction-from-cine-magnetic-resonance-images"><a href="#Multi-class-point-cloud-completion-networks-for-3D-cardiac-anatomy-reconstruction-from-cine-magnetic-resonance-images" class="headerlink" title="Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images"></a>Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08535">http://arxiv.org/abs/2307.08535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Beetz, Abhirup Banerjee, Julius Ossenberg-Engels, Vicente Grau</li>
<li>For: 这个论文的目的是提出一种全自动的三维心脏形态重建方法，以便从硬件磁共振成像（cine MRI）获得三维心脏形态模型。* Methods: 这个方法使用了一种多类点云完成网络（PCCN）来解决3D重建任务中的稀疏性和重合性问题。PCCN在大量的synthetic数据集上进行了评估，并与标准 Referenced anatomy 之间的Chamfer距离在不同的扭曲程度下都在或类似于图像分辨率下。此外，与3D U-Net Referenced 模型相比，PCCN减少了重建错误的比例，即 Hausdorff 距离和平均表面距离减少了32%和24%。* Results: 然后，作者使用PCCN作为自动重建管道的一部分，对UK Biobank研究中的1000名参与者进行了cross-domain传送。结果显示，PCCN可以重建 precisemedical 和可靠的三维心脏形态模型，并与之前的 литераature 中的临床指标相符。此外，作者还调查了该方法的稳定性，并发现它可以成功处理多种常见异常情况。<details>
<summary>Abstract</summary>
Cine magnetic resonance imaging (MRI) is the current gold standard for the assessment of cardiac anatomy and function. However, it typically only acquires a set of two-dimensional (2D) slices of the underlying three-dimensional (3D) anatomy of the heart, thus limiting the understanding and analysis of both healthy and pathological cardiac morphology and physiology. In this paper, we propose a novel fully automatic surface reconstruction pipeline capable of reconstructing multi-class 3D cardiac anatomy meshes from raw cine MRI acquisitions. Its key component is a multi-class point cloud completion network (PCCN) capable of correcting both the sparsity and misalignment issues of the 3D reconstruction task in a unified model. We first evaluate the PCCN on a large synthetic dataset of biventricular anatomies and observe Chamfer distances between reconstructed and gold standard anatomies below or similar to the underlying image resolution for multiple levels of slice misalignment. Furthermore, we find a reduction in reconstruction error compared to a benchmark 3D U-Net by 32% and 24% in terms of Hausdorff distance and mean surface distance, respectively. We then apply the PCCN as part of our automated reconstruction pipeline to 1000 subjects from the UK Biobank study in a cross-domain transfer setting and demonstrate its ability to reconstruct accurate and topologically plausible biventricular heart meshes with clinical metrics comparable to the previous literature. Finally, we investigate the robustness of our proposed approach and observe its capacity to successfully handle multiple common outlier conditions.
</details>
<details>
<summary>摘要</summary>
临床磁共振成像（MRI）是当前的黄金标准 для心脏解剖和功能评估。然而，它通常只能获取心脏的二维（2D）slice图像，因此限制了我们对健康和疾病心脏解剖和physiology的理解和分析。在这篇论文中，我们提出了一种全自动的表面重建管线，可以从raw磁共振成像获取多类3D心脏解剖模型。其关键组件是一种多类点云完成网络（PCCN），可以同时 corrrect both the sparsity和misalignment Issues of the 3D reconstruction task in a unified model。我们首先在大量的synthetic数据集上评估PCCN，并观察到Chamfer距离与 golden standard anatomy之间的距离在多个slice misalignment情况下都在或类似于图像分辨率下。此外，我们发现PCCN比 benchmark 3D U-Net的重建误差下降32%和24%。我们然后将PCCN作为自动重建管线的一部分应用于UK Biobank研究中的1000名研究对象，并示出了它能够重建准确和topologically plausible的心脏解剖模型，与前一代 литераature中的临床 metric相当。最后，我们调查了我们提议的方法的稳定性，并发现它可以成功处理多种常见异常情况。
</details></li>
</ul>
<hr>
<h2 id="Multi-Domain-Learning-with-Modulation-Adapters"><a href="#Multi-Domain-Learning-with-Modulation-Adapters" class="headerlink" title="Multi-Domain Learning with Modulation Adapters"></a>Multi-Domain Learning with Modulation Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08528">http://arxiv.org/abs/2307.08528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekaterina Iakovleva, Karteek Alahari, Jakob Verbeek</li>
<li>for: 这篇论文是为了解决多个领域的图像分类问题，以及对不同领域的图像进行共同训练。</li>
<li>methods: 这篇论文使用了卷积 neural network，并将卷积权重更新为每个任务的对应领域特有的权重。</li>
<li>results: 这篇论文在Visual Decathlon挑战和ImageNet-to-Sketch benchmark上取得了出色的结果，其精度与现有的州先进方法相似或更高。<details>
<summary>Abstract</summary>
Deep convolutional networks are ubiquitous in computer vision, due to their excellent performance across different tasks for various domains. Models are, however, often trained in isolation for each task, failing to exploit relatedness between tasks and domains to learn more compact models that generalise better in low-data regimes. Multi-domain learning aims to handle related tasks, such as image classification across multiple domains, simultaneously. Previous work on this problem explored the use of a pre-trained and fixed domain-agnostic base network, in combination with smaller learnable domain-specific adaptation modules. In this paper, we introduce Modulation Adapters, which update the convolutional filter weights of the model in a multiplicative manner for each task. Parameterising these adaptation weights in a factored manner allows us to scale the number of per-task parameters in a flexible manner, and to strike different parameter-accuracy trade-offs. We evaluate our approach on the Visual Decathlon challenge, composed of ten image classification tasks across different domains, and on the ImageNet-to-Sketch benchmark, which consists of six image classification tasks. Our approach yields excellent results, with accuracies that are comparable to or better than those of existing state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
深度卷积网络在计算机视觉领域具有广泛的应用，这是因为它们在不同任务和领域上表现出色。然而，模型们通常是单独进行每个任务的训练，不利用任务和领域之间的相互关系，以学习更加紧凑的模型，并在低数据 regime 中更好地泛化。多元领域学习旨在处理相关的任务，如图像分类领域中的多个任务。先前的工作中使用了预训练和固定的域名不同的基础网络，并与更小的可学习的域pecific适应模块进行组合。在这篇论文中，我们引入了调整器（Modulation Adapters），它们可以在每个任务中对卷积权重进行更新，并在多任务情况下共享参数。我们可以在 фактор化的方式下参数化这些适应权重，以便在灵活的方式下增加每个任务的参数数量，并在精度和准确性之间进行负担平衡。我们在Visual Decathlon挑战和ImageNet-to-Sketch benchmark上进行评估，并取得了出色的结果，与现有状态的方法相比，我们的方法的准确率更高。
</details></li>
</ul>
<hr>
<h2 id="BUS-Efficient-and-Effective-Vision-language-Pre-training-with-Bottom-Up-Patch-Summarization"><a href="#BUS-Efficient-and-Effective-Vision-language-Pre-training-with-Bottom-Up-Patch-Summarization" class="headerlink" title="BUS:Efficient and Effective Vision-language Pre-training with Bottom-Up Patch Summarization"></a>BUS:Efficient and Effective Vision-language Pre-training with Bottom-Up Patch Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08504">http://arxiv.org/abs/2307.08504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoya Jiang, Haiyang Xu, Wei Ye, Qinghao Ye, Chenliang Li, Ming Yan, Bin Bi, Shikun Zhang, Fei Huang, Songfang Huang</li>
<li>for: 这个论文主要旨在提高 ViT 模型在视觉语言理解和生成任务中的训练效率，而不 sacrifi 性能。</li>
<li>methods: 该论文提出了一种底向概括方法，称为 Bottom-Up Patch Summarization (BUS)，它在 ViT 底层EXTRACT 和顶层 ABSTRACT 之间协调底层EXTRACT 和顶层 ABSTRACT，以学习高效的视觉概括。</li>
<li>results: 该论文在多个视觉语言理解和生成任务中表现竞争力强，而且可以提高训练效率达 50%，同时保持或者提高效果。此外，该模型在增加输入图像分辨率时可以不增加计算成本，而达到领先的性能。<details>
<summary>Abstract</summary>
Vision Transformer (ViT) based Vision-Language Pre-training (VLP) models have demonstrated impressive performance in various tasks. However, the lengthy visual token sequences fed into ViT can lead to training inefficiency and ineffectiveness. Existing efforts address the challenge by either bottom-level patch extraction in the ViT backbone or top-level patch abstraction outside, not balancing training efficiency and effectiveness well. Inspired by text summarization in natural language processing, we propose a Bottom-Up Patch Summarization approach named BUS, coordinating bottom-level extraction and top-level abstraction to learn a concise summary of lengthy visual token sequences efficiently. Specifically, We incorporate a Text-Semantics-Aware Patch Selector (TSPS) into the ViT backbone to perform a coarse-grained visual token extraction and then attach a flexible Transformer-based Patch Abstraction Decoder (PAD) upon the backbone for top-level visual abstraction. This bottom-up collaboration enables our BUS to yield high training efficiency while maintaining or even improving effectiveness. We evaluate our approach on various visual-language understanding and generation tasks and show competitive downstream task performance while boosting the training efficiency by 50\%. Additionally, our model achieves state-of-the-art performance on many downstream tasks by increasing input image resolution without increasing computational costs over baselines.
</details>
<details>
<summary>摘要</summary>
视Transformer（ViT）基于视力语言预训练（VLP）模型在不同任务中表现出色。然而，在ViT中长时间的视觉 токен序列可能会导致训练不fficient和不effective。现有的尝试解决这个挑战是通过ViT backbone中的底层 patch抽取或者外部的top-level patch抽象，但这些方法并不能很好地寻求训练效率和效果的平衡。以文本概要为引yles，我们提出了底层 patch概要approach（BUS），通过协同底层抽取和顶层抽象来学习长时间的视觉 токен序列高效简洁的概要。specifically，我们在ViT backbone中添加了文本 semantics-aware patch selector（TSPS），以进行粗粒度的视觉 токен抽取，然后在backbone上添加一个灵活的 transformer-based patch abstraction decoder（PAD），以进行顶层视觉抽象。这种底层协同使得我们的BUS可以高效地进行训练，同时保持或者提高效果。我们在不同的视力语言理解和生成任务上评估了我们的方法，并显示了与基eline相比的50%的训练效率提升，同时在许多下游任务上达到了state-of-the-art的性能。此外，我们的模型可以在不变 computational costs的情况下，提高输入图像的分辨率，从而实现更好的下游任务性能。
</details></li>
</ul>
<hr>
<h2 id="Study-of-Vision-Transformers-for-Covid-19-Detection-from-Chest-X-rays"><a href="#Study-of-Vision-Transformers-for-Covid-19-Detection-from-Chest-X-rays" class="headerlink" title="Study of Vision Transformers for Covid-19 Detection from Chest X-rays"></a>Study of Vision Transformers for Covid-19 Detection from Chest X-rays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09402">http://arxiv.org/abs/2307.09402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandeep Angara, Sharath Thirunagaru</li>
<li>for: 这种研究旨在检测 COVID-19 使用视transformer 技术，以提高检测效率和准确率。</li>
<li>methods: 本研究使用了许多最新的 transformer 模型，包括 Vision Transformer (ViT)、Swin-transformer、Max vision transformer (MViT) 和 Pyramid Vision transformer (PVT)，通过转移学习IMAGENET 的 weights，实现了惊人的准确率范围为 98.75% 到 99.5%。</li>
<li>results: 实验结果表明，视transformer 在 COVID-19 检测中达到了 estado-of-the-art 性能，高于传统方法和 Even Convolutional Neural Networks (CNNs)，这些结果表明了视transformer 的潜在力量作为 COVID-19 检测工具，有助于提高检测和诊断的效率和准确率在临床设置中。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has led to a global health crisis, highlighting the need for rapid and accurate virus detection. This research paper examines transfer learning with vision transformers for COVID-19 detection, known for its excellent performance in image recognition tasks. We leverage the capability of Vision Transformers to capture global context and learn complex patterns from chest X-ray images. In this work, we explored the recent state-of-art transformer models to detect Covid-19 using CXR images such as vision transformer (ViT), Swin-transformer, Max vision transformer (MViT), and Pyramid Vision transformer (PVT). Through the utilization of transfer learning with IMAGENET weights, the models achieved an impressive accuracy range of 98.75% to 99.5%. Our experiments demonstrate that Vision Transformers achieve state-of-the-art performance in COVID-19 detection, outperforming traditional methods and even Convolutional Neural Networks (CNNs). The results highlight the potential of Vision Transformers as a powerful tool for COVID-19 detection, with implications for improving the efficiency and accuracy of screening and diagnosis in clinical settings.
</details>
<details>
<summary>摘要</summary>
COVID-19 流行病毒引起全球健康危机，强调了快速和准确的病毒检测的需求。这篇研究论文研究了将转移学习应用于视Transformers 中的 COVID-19 检测，其在图像识别任务中表现出色。我们利用了视Transformers 的全球上下文捕捉和学习复杂模式的能力，对胸部X射像进行检测。在这个工作中，我们探索了最新的转移学习模型，包括视Transformer（ViT）、Swin-transformer、Maxvision transformer（MViT）和Pyramid Vision transformer（PVT）。通过使用IMAGENET 权重进行转移学习，这些模型在 COVID-19 检测中实现了98.75% 到 99.5% 的准确率范围。我们的实验表明，视Transformers 在 COVID-19 检测中达到了状态 искусственный智能的性能，超越传统方法和卷积神经网络（CNNs）。这些结果表明，视Transformers 可以作为COVID-19 检测的有力工具，并有可能提高诊断和检测的效率和准确率在临床设置中。
</details></li>
</ul>
<hr>
<h2 id="Cumulative-Spatial-Knowledge-Distillation-for-Vision-Transformers"><a href="#Cumulative-Spatial-Knowledge-Distillation-for-Vision-Transformers" class="headerlink" title="Cumulative Spatial Knowledge Distillation for Vision Transformers"></a>Cumulative Spatial Knowledge Distillation for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08500">http://arxiv.org/abs/2307.08500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Borui Zhao, Renjie Song, Jiajun Liang</li>
<li>for: 本研究旨在提高vision transformer（ViT）的性能，通过吸取 convolutional neural networks（CNN）的知识。</li>
<li>methods: 本研究提出了Cumulative Spatial Knowledge Distillation（CSKD）方法，通过从CNN的相应的空间响应中提取空间知识，对ViT的所有patchtoken进行适应。此外，CSKD还使用了Cumulative Knowledge Fusion（CKF）模块，通过在训练过程中逐渐增加CNN的全局响应的重要性，使得ViT能够在训练早期充分利用CNN的地方适应，在训练后期更好地利用ViT的全局能力。</li>
<li>results: 对于ImageNet-1k和下游 dataset，CSKD获得了超越原始ViT的性能。 code将公开。<details>
<summary>Abstract</summary>
Distilling knowledge from convolutional neural networks (CNNs) is a double-edged sword for vision transformers (ViTs). It boosts the performance since the image-friendly local-inductive bias of CNN helps ViT learn faster and better, but leading to two problems: (1) Network designs of CNN and ViT are completely different, which leads to different semantic levels of intermediate features, making spatial-wise knowledge transfer methods (e.g., feature mimicking) inefficient. (2) Distilling knowledge from CNN limits the network convergence in the later training period since ViT's capability of integrating global information is suppressed by CNN's local-inductive-bias supervision. To this end, we present Cumulative Spatial Knowledge Distillation (CSKD). CSKD distills spatial-wise knowledge to all patch tokens of ViT from the corresponding spatial responses of CNN, without introducing intermediate features. Furthermore, CSKD exploits a Cumulative Knowledge Fusion (CKF) module, which introduces the global response of CNN and increasingly emphasizes its importance during the training. Applying CKF leverages CNN's local inductive bias in the early training period and gives full play to ViT's global capability in the later one. Extensive experiments and analysis on ImageNet-1k and downstream datasets demonstrate the superiority of our CSKD. Code will be publicly available.
</details>
<details>
<summary>摘要</summary>
精炼知识从卷积神经网络（CNN）是视transformer（ViT）的双刃剑。它会提高性能，因为图像友好的本地推导性（local-inductive bias）在CNN中帮助ViT更快速地学习和提高性能，但也存在两个问题：（1）CNN和ViT的网络设计完全不同，导致它们的中间特征semantic level不同，使得空间知识传递方法（例如特征模仿）不efficient。（2）从CNN精炼知识限制了ViT的网络迁移在后期训练中，因为ViT的全球信息整合能力被CNN的本地推导性supervise。为此，我们提出了积累的空间知识填充（CSKD）。CSKD将从CNN的相应空间响应中精炼到ViT的所有patchtoken中的空间知识，而不需要中间特征。此外，CSKD还利用了积累知识融合（CKF）模块，该模块在训练中逐渐增加CNN的全球响应的重要性，从而利用CNN的本地推导性在初期训练中，并让ViT在后期训练中发挥全球能力。我们在ImageNet-1k和下游数据集上进行了广泛的实验和分析，并证明了我们的CSKD的优越性。代码将在公共可用。
</details></li>
</ul>
<hr>
<h2 id="SVDFormer-Complementing-Point-Cloud-via-Self-view-Augmentation-and-Self-structure-Dual-generator"><a href="#SVDFormer-Complementing-Point-Cloud-via-Self-view-Augmentation-and-Self-structure-Dual-generator" class="headerlink" title="SVDFormer: Complementing Point Cloud via Self-view Augmentation and Self-structure Dual-generator"></a>SVDFormer: Complementing Point Cloud via Self-view Augmentation and Self-structure Dual-generator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08492">http://arxiv.org/abs/2307.08492</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/czvvd/svdformer">https://github.com/czvvd/svdformer</a></li>
<li>paper_authors: Zhe Zhu, Honghua Chen, Xing He, Weiming Wang, Jing Qin, Mingqiang Wei</li>
<li>for: 本文提出了一种新型网络SVDFormer，用于解决 incomplete point cloud 的两个特定挑战：理解完整的全球形状和生成高精度的本地结构。现有方法通常只使用三维坐标来识别形状模式，或者带有良好准备的颜色图像来引导geometry estimation的缺失部分。但这些方法并不总是能充分利用cross-modal自身结构来完成高质量的点云完成。</li>
<li>methods: 我们首先设计了一个Self-view Fusion Network，利用多视图深度图像信息来观察不完整的自身形状并生成一个紧凑的全球形状。然后，我们引入了一个改进模块，叫Self-structure Dual-generator，其中我们将学习的形状假设和地理自相似性 incorporated into producing new points。通过识别每个点的不完整性，我们实现了DUAL-PATH设计，使得各种精度的精细结构可以被独立地识别和修复。</li>
<li>results: 我们的方法在 widely-used benchmarks 上 achieve state-of-the-art performance。代码将在 <a target="_blank" rel="noopener" href="https://github.com/czvvd/SVDFormer">https://github.com/czvvd/SVDFormer</a> 上发布。<details>
<summary>Abstract</summary>
In this paper, we propose a novel network, SVDFormer, to tackle two specific challenges in point cloud completion: understanding faithful global shapes from incomplete point clouds and generating high-accuracy local structures. Current methods either perceive shape patterns using only 3D coordinates or import extra images with well-calibrated intrinsic parameters to guide the geometry estimation of the missing parts. However, these approaches do not always fully leverage the cross-modal self-structures available for accurate and high-quality point cloud completion. To this end, we first design a Self-view Fusion Network that leverages multiple-view depth image information to observe incomplete self-shape and generate a compact global shape. To reveal highly detailed structures, we then introduce a refinement module, called Self-structure Dual-generator, in which we incorporate learned shape priors and geometric self-similarities for producing new points. By perceiving the incompleteness of each point, the dual-path design disentangles refinement strategies conditioned on the structural type of each point. SVDFormer absorbs the wisdom of self-structures, avoiding any additional paired information such as color images with precisely calibrated camera intrinsic parameters. Comprehensive experiments indicate that our method achieves state-of-the-art performance on widely-used benchmarks. Code will be available at https://github.com/czvvd/SVDFormer.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的网络，即SVDFormer，以解决Point cloud completion中的两个特定挑战：理解完整的全球形态从不完整的点云中，并生成高精度的本地结构。现有方法可以通过只使用3D坐标来识别形态模式，或者从外部Import预先calibrated的颜色图像来导航点云中缺失部分的准确性。但这些方法并不总能充分利用点云之间的自同构信息，以实现高质量的完成。为此，我们首先设计了一个自我融合网络，利用多视图深度图像信息来观察不完整的自身形态，并生成一个紧凑的全球形态。为了揭示高精度的结构，我们然后引入了一个改进模块，即自身结构双生成器，在其中我们利用学习的形态规范和几何自相似性来生成新的点。通过识别每个点的不完整性，我们的双路设计分离了各种精度的修正策略。SVDFormer利用自身结构的智慧，不需要任何额外的对应信息，如预先calibrated的颜色图像。广泛的实验表明，我们的方法在常用的 benchmark 上达到了领先的性能。代码将在https://github.com/czvvd/SVDFormer 上提供。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Transportation-Pruning"><a href="#Differentiable-Transportation-Pruning" class="headerlink" title="Differentiable Transportation Pruning"></a>Differentiable Transportation Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08483">http://arxiv.org/abs/2307.08483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunqiang Li, Jan C. van Gemert, Torsten Hoefler, Bert Moons, Evangelos Eleftheriou, Bram-Ernst Verhoef</li>
<li>for: 该论文旨在提出一种高效的深度学习模型压缩方法，以便在边缘设备上部署深度学习模型。</li>
<li>methods: 该方法使用了一种高效的优化交通算法，该算法通过自动调整搜索-利用行为来找到精准的稀疏子网络。</li>
<li>results: 该方法可以在3个不同的数据集上，使用5种不同的模型，在各种压缩比例下，与之前的压缩方法进行比较，并且可以在不同的稀疏预算和压缩粒度下实现状态的最佳性能。<details>
<summary>Abstract</summary>
Deep learning algorithms are increasingly employed at the edge. However, edge devices are resource constrained and thus require efficient deployment of deep neural networks. Pruning methods are a key tool for edge deployment as they can improve storage, compute, memory bandwidth, and energy usage. In this paper we propose a novel accurate pruning technique that allows precise control over the output network size. Our method uses an efficient optimal transportation scheme which we make end-to-end differentiable and which automatically tunes the exploration-exploitation behavior of the algorithm to find accurate sparse sub-networks. We show that our method achieves state-of-the-art performance compared to previous pruning methods on 3 different datasets, using 5 different models, across a wide range of pruning ratios, and with two types of sparsity budgets and pruning granularities.
</details>
<details>
<summary>摘要</summary>
深度学习算法在边缘部署中越来越普遍。然而，边缘设备具有限制的资源，因此需要有效地部署深度神经网络。剪辑方法是边缘部署中关键的工具，可以提高存储、计算、内存带宽和能源使用。在这篇论文中，我们提出了一种新的精准剪辑技术，可以准确控制输出网络大小。我们的方法使用高效的优化交通方案，我们使其成为终端到终端可微分的，自动调整搜索-利用行为，以找到精准的稀疏子网络。我们展示了我们的方法与之前的剪辑方法相比，在三个 datasets 上，使用五种模型，在各种剪辑比率、两种稀疏预算和剪辑粒度上均达到了状态 искусственный智能的表现。
</details></li>
</ul>
<hr>
<h2 id="SkeletonMAE-Graph-based-Masked-Autoencoder-for-Skeleton-Sequence-Pre-training"><a href="#SkeletonMAE-Graph-based-Masked-Autoencoder-for-Skeleton-Sequence-Pre-training" class="headerlink" title="SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training"></a>SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08476">http://arxiv.org/abs/2307.08476</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hongyan1123/skeletonmae">https://github.com/hongyan1123/skeletonmae</a></li>
<li>paper_authors: Hong Yan, Yang Liu, Yushen Wei, Zhen Li, Guanbin Li, Liang Lin</li>
<li>for: 这 paper 的目的是提出一种高效的人体序列学习框架，以便在不同的数据集上进行自动识别人体动作。</li>
<li>methods: 该 paper 使用了一种异symmetric graph-based encoder-decoder预训练架构，named SkeletonMAE，以及一种Spatiotemporal Representation Learning（STRL）模块，以完全捕捉人体姿势和获得有效的人体序列表示。</li>
<li>results: 该 paper 的实验结果表明，该方法可以在不同的数据集上提供优秀的自动识别人体动作性能，并且与一些完全监督的方法相当。<details>
<summary>Abstract</summary>
Skeleton sequence representation learning has shown great advantages for action recognition due to its promising ability to model human joints and topology. However, the current methods usually require sufficient labeled data for training computationally expensive models, which is labor-intensive and time-consuming. Moreover, these methods ignore how to utilize the fine-grained dependencies among different skeleton joints to pre-train an efficient skeleton sequence learning model that can generalize well across different datasets. In this paper, we propose an efficient skeleton sequence learning framework, named Skeleton Sequence Learning (SSL). To comprehensively capture the human pose and obtain discriminative skeleton sequence representation, we build an asymmetric graph-based encoder-decoder pre-training architecture named SkeletonMAE, which embeds skeleton joint sequence into Graph Convolutional Network (GCN) and reconstructs the masked skeleton joints and edges based on the prior human topology knowledge. Then, the pre-trained SkeletonMAE encoder is integrated with the Spatial-Temporal Representation Learning (STRL) module to build the SSL framework. Extensive experimental results show that our SSL generalizes well across different datasets and outperforms the state-of-the-art self-supervised skeleton-based action recognition methods on FineGym, Diving48, NTU 60 and NTU 120 datasets. Additionally, we obtain comparable performance to some fully supervised methods. The code is avaliable at https://github.com/HongYan1123/SkeletonMAE.
</details>
<details>
<summary>摘要</summary>
skeleton sequence representation learning 对于动作认识表现出了非常出色的优势，这是因为它可以模型人体关节和结构。然而，当前的方法通常需要大量标注数据进行训练计算昂贵的模型，这是劳动密集和时间消耗的。此外，这些方法忽略了如何使用细腻的关节间依赖关系来预训练高效的skeleton sequence学习模型，以便在不同的数据集上进行泛化。在这篇论文中，我们提出了一种高效的skeleton sequence学习框架，名为Skeleton Sequence Learning（SSL）。为了全面捕捉人体姿势和获得特征的skeleton sequence表示，我们设计了一种偏 asymmetric graph-based encoder-decoder预训练架构，名为SkeletonMAE，它将skeleton关节序列嵌入图像 convolutional neural network（GCN）中，并在基于人体 topology知识的前提下重建屏蔽的关节和边。然后，我们将预训练的SkeletonMAE编码器与空间-时间表示学习（STRL）模块集成，构建了SSL框架。我们对多个数据集进行了广泛的实验，结果显示，我们的SSL可以在不同的数据集上进行泛化，并且比 estado-of-the-art的自我监督skeleton-based动作认识方法在FineGym、Diving48、NTU 60和NTU 120数据集上表现出色。此外，我们还获得了与一些完全监督方法相同的性能。代码可以在https://github.com/HongYan1123/SkeletonMAE中找到。
</details></li>
</ul>
<hr>
<h2 id="EGE-UNet-an-Efficient-Group-Enhanced-UNet-for-skin-lesion-segmentation"><a href="#EGE-UNet-an-Efficient-Group-Enhanced-UNet-for-skin-lesion-segmentation" class="headerlink" title="EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation"></a>EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08473">http://arxiv.org/abs/2307.08473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jcruan519/ege-unet">https://github.com/jcruan519/ege-unet</a></li>
<li>paper_authors: Jiacheng Ruan, Mingye Xie, Jingsheng Gao, Ting Liu, Yuzhuo Fu</li>
<li>for: 这篇研究旨在提出一个更有效的医疗影像分类方法，以应对现有的过滤器和其变体在医疗应用中的问题。</li>
<li>methods: 本研究提出了一个名为Efficient Group Enhanced UNet（EGE-UNet）的方法，具有轻量级的设计和实现。EGE-UNet具有Group multi-axis Hadamard Product Attention（GHPA）和Group Aggregation Bridge（GAB）两个模块，可以实现多轴 Hadamard Product Attention  Mechanism 和多尺度信息聚合。</li>
<li>results: 实验结果显示，EGE-UNet在 ISIC2017 和 ISIC2018  datasets 上的分类性能比较现有的状态顶对方法高，同时对应用程序的参数和计算负载也有了显著的减少（494倍和160倍）。此外，EGE-UNet 的参数数量只有 50KB，这是现有方法中首次出现的最小化参数数量。<details>
<summary>Abstract</summary>
Transformer and its variants have been widely used for medical image segmentation. However, the large number of parameter and computational load of these models make them unsuitable for mobile health applications. To address this issue, we propose a more efficient approach, the Efficient Group Enhanced UNet (EGE-UNet). We incorporate a Group multi-axis Hadamard Product Attention module (GHPA) and a Group Aggregation Bridge module (GAB) in a lightweight manner. The GHPA groups input features and performs Hadamard Product Attention mechanism (HPA) on different axes to extract pathological information from diverse perspectives. The GAB effectively fuses multi-scale information by grouping low-level features, high-level features, and a mask generated by the decoder at each stage. Comprehensive experiments on the ISIC2017 and ISIC2018 datasets demonstrate that EGE-UNet outperforms existing state-of-the-art methods. In short, compared to the TransFuse, our model achieves superior segmentation performance while reducing parameter and computation costs by 494x and 160x, respectively. Moreover, to our best knowledge, this is the first model with a parameter count limited to just 50KB. Our code is available at https://github.com/JCruan519/EGE-UNet.
</details>
<details>
<summary>摘要</summary>
transformer 和其变种在医学影像 segmentation 中广泛应用，但这些模型的参数数量和计算负担使其不适用于移动健康应用。为解决这个问题，我们提议一种更有效的方法，即高效组增强 U-Net (EGE-UNet)。我们在轻量级的情况下 интеGRoup 多轴 Hadamard Product Attention 模块 (GHPA) 和 Group Aggregation Bridge 模块 (GAB)。GHPA 将输入特征分组并在不同轴上执行 Hadamard Product Attention 机制 (HPA)，以提取多个视角的疾病信息。GAB 有效地融合多尺度信息，通过分组低级特征、高级特征和每个阶段生成的掩码。我们在 ISIC2017 和 ISIC2018 数据集上进行了全面的实验，结果表明 EGE-UNet 超过了现有的状态数据。简而言之，与 TransFuse 相比，我们的模型实现了更高的分 segmentation 性能，同时减少参数和计算成本494倍和160倍，分别。此外，我们知道这是首个参数数量限制在50KB之下的模型。我们的代码可以在 <https://github.com/JCruan519/EGE-UNet> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Riesz-feature-representation-scale-equivariant-scattering-network-for-classification-tasks"><a href="#Riesz-feature-representation-scale-equivariant-scattering-network-for-classification-tasks" class="headerlink" title="Riesz feature representation: scale equivariant scattering network for classification tasks"></a>Riesz feature representation: scale equivariant scattering network for classification tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08467">http://arxiv.org/abs/2307.08467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tin Barisin, Jesus Angulo, Katja Schladitz, Claudia Redenbach</li>
<li>for: 文章主要用于提出一种基于里茨变换的特征表示方法，以避免采样缩scale维度的问题，并且具有等级平衡性。</li>
<li>methods: 本文使用里茨变换定义了一种新的特征表示方法，并详细分析了这种表示方法的数学基础。这种表示方法具有等级平衡性，并且与传统的散射网络相比，减少了特征数量四分之一。</li>
<li>results: 作者通过对 текстуre 分类和数字分类两个任务进行实验，证明了该方法可以具有比较好的性能，并且在不同的缩scale下保持稳定性。特别是在训练数据中不包含的缩scale下，该方法可以具有更好的性能。<details>
<summary>Abstract</summary>
Scattering networks yield powerful and robust hierarchical image descriptors which do not require lengthy training and which work well with very few training data. However, they rely on sampling the scale dimension. Hence, they become sensitive to scale variations and are unable to generalize to unseen scales. In this work, we define an alternative feature representation based on the Riesz transform. We detail and analyze the mathematical foundations behind this representation. In particular, it inherits scale equivariance from the Riesz transform and completely avoids sampling of the scale dimension. Additionally, the number of features in the representation is reduced by a factor four compared to scattering networks. Nevertheless, our representation performs comparably well for texture classification with an interesting addition: scale equivariance. Our method yields superior performance when dealing with scales outside of those covered by the training dataset. The usefulness of the equivariance property is demonstrated on the digit classification task, where accuracy remains stable even for scales four times larger than the one chosen for training. As a second example, we consider classification of textures.
</details>
<details>
<summary>摘要</summary>
扫描网络生成强大和稳定的层次图像描述符，不需要长时间训练，并且可以使用非常少的训练数据。然而，它们依赖于采样Scale维度，因此对于不同的Scale变化会变得敏感，无法泛化到未经训练的Scale。在这项工作中，我们定义了一种基于Riesz变换的特征表示方法。我们详细介绍了这种表示方法的数学基础，特别是它从Riesz变换继承了Scale相对 invariants属性，完全避免了采样Scale维度。此外，与扫描网络相比，我们的表示方法减少了特征数量的四倍。然而，我们的方法与扫描网络的性能相似，并且具有一个有趣的附加特点：Scale相对 invariants。我们的方法在 digit 分类任务中表现出色，即使用于训练 dataset 中的Scale四倍大的Scale也能保持稳定的准确率。作为第二个例子，我们考虑了 Texture 分类任务。Note: Simplified Chinese is used here, as it is the most widely used variety of Chinese in mainland China. However, if you prefer Traditional Chinese, I can also provide the translation.
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Classification-of-UHF-Partial-Discharge-Signals-in-Gas-Insulated-HVDC-Systems-Using-Neural-Networks"><a href="#Generalizable-Classification-of-UHF-Partial-Discharge-Signals-in-Gas-Insulated-HVDC-Systems-Using-Neural-Networks" class="headerlink" title="Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks"></a>Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08466">http://arxiv.org/abs/2307.08466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steffen Seitz, Thomas Götz, Christopher Lindenberg, Ronald Tetzlaff, Stephan Schlegel</li>
<li>for: 本研究旨在提出一种基于神经网络的方法，用于分类HVDC GIS中的部分磁发（PD）信号，而不需要基于振荡序列分析特征。</li>
<li>methods: 本研究使用神经网络模型进行PD信号分类，并对时域和频域输入信号进行比较，以及不同Normalization方法的影响。</li>
<li>results: 研究结果表明，使用神经网络模型可以有效地分类PD信号，并且可以普适到不同的输入振荡频率和电压倍数。<details>
<summary>Abstract</summary>
Undetected partial discharges (PDs) are a safety critical issue in high voltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under AC voltage is well-established, the analysis of PDs under DC voltage remains an active research field. A key focus of these investigations is the classification of different PD sources to enable subsequent sophisticated analysis.   In this paper, we propose and analyze a neural network-based approach for classifying PD signals caused by metallic protrusions and conductive particles on the insulator of HVDC GIS, without relying on pulse sequence analysis features. In contrast to previous approaches, our proposed model can discriminate the studied PD signals obtained at negative and positive potentials, while also generalizing to unseen operating voltage multiples. Additionally, we compare the performance of time- and frequency-domain input signals and explore the impact of different normalization schemes to mitigate the influence of free-space path loss between the sensor and defect location.
</details>
<details>
<summary>摘要</summary>
Undetected partial discharges (PDs) are a safety critical issue in high voltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under AC voltage is well-established, the analysis of PDs under DC voltage remains an active research field. A key focus of these investigations is the classification of different PD sources to enable subsequent sophisticated analysis.  In this paper, we propose and analyze a neural network-based approach for classifying PD signals caused by metallic protrusions and conductive particles on the insulator of HVDC GIS, without relying on pulse sequence analysis features. In contrast to previous approaches, our proposed model can discriminate the studied PD signals obtained at negative and positive potentials, while also generalizing to unseen operating voltage multiples. Additionally, we compare the performance of time- and frequency-domain input signals and explore the impact of different normalization schemes to mitigate the influence of free-space path loss between the sensor and defect location.Here's the translation in Traditional Chinese:Undetected partial discharges (PDs) are a safety critical issue in high voltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under AC voltage is well-established, the analysis of PDs under DC voltage remains an active research field. A key focus of these investigations is the classification of different PD sources to enable subsequent sophisticated analysis.  In this paper, we propose and analyze a neural network-based approach for classifying PD signals caused by metallic protrusions and conductive particles on the insulator of HVDC GIS, without relying on pulse sequence analysis features. In contrast to previous approaches, our proposed model can discriminate the studied PD signals obtained at negative and positive potentials, while also generalizing to unseen operating voltage multiples. Additionally, we compare the performance of time- and frequency-domain input signals and explore the impact of different normalization schemes to mitigate the influence of free-space path loss between the sensor and defect location.
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-using-Silver-Standard-Masks-for-Lateral-Ventricle-Segmentation-in-FLAIR-MRI"><a href="#Domain-Adaptation-using-Silver-Standard-Masks-for-Lateral-Ventricle-Segmentation-in-FLAIR-MRI" class="headerlink" title="Domain Adaptation using Silver Standard Masks for Lateral Ventricle Segmentation in FLAIR MRI"></a>Domain Adaptation using Silver Standard Masks for Lateral Ventricle Segmentation in FLAIR MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08456">http://arxiv.org/abs/2307.08456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Owen Crystal, Pejman J. Maralani, Sandra Black, Alan R. Moody, April Khademi<br>for:This paper presents a new method for segmenting lateral ventricular volume (LVV) in fluid-attenuated inversion recovery (FLAIR) MRI images.methods:The proposed method uses transfer learning and domain adaptation to improve the accuracy of LVV segmentation. It uses a novel image processing algorithm to generate silver standard (SS) masks from the target domain, which are then used to supplement the gold standard (GS) data from the source domain.results:The proposed method achieved the best and most consistent performance on four different datasets, with a mean Dice similarity coefficient (DSC) of 0.89 and a coefficient of variation (CoV) of 0.05. The method significantly outperformed the GS-only model on three target domains, and the results suggest that pre-training with noisy labels from the target domain and fine-tuning with GS masks allows the model to adapt to the dataset-specific characteristics and provide robust parameter initialization.<details>
<summary>Abstract</summary>
Lateral ventricular volume (LVV) is an important biomarker for clinical investigation. We present the first transfer learning-based LVV segmentation method for fluid-attenuated inversion recovery (FLAIR) MRI. To mitigate covariate shifts between source and target domains, this work proposes an domain adaptation method that optimizes performance on three target datasets. Silver standard (SS) masks were generated from the target domain using a novel conventional image processing ventricular segmentation algorithm and used to supplement the gold standard (GS) data from the source domain, Canadian Atherosclerosis Imaging Network (CAIN). Four models were tested on held-out test sets from four datasets: 1) SS+GS: trained on target SS masks and fine-tuned on source GS masks, 2) GS+SS: trained on source GS masks and fine-tuned on target SS masks, 3) trained on source GS (GS CAIN Only) and 4) trained on target SS masks (SS Only). The SS+GS model had the best and most consistent performance (mean DSC = 0.89, CoV = 0.05) and showed significantly (p < 0.05) higher DSC compared to the GS-only model on three target domains. Results suggest pre-training with noisy labels from the target domain allows the model to adapt to the dataset-specific characteristics and provides robust parameter initialization while fine-tuning with GS masks allows the model to learn detailed features. This method has wide application to other medical imaging problems where labeled data is scarce, and can be used as a per-dataset calibration method to accelerate wide-scale adoption.
</details>
<details>
<summary>摘要</summary>
“ lateral ventricular volume (LVV) 是一个重要的临床探险标的。本研究提出了首个基于传播学习的 LVV 分类方法，用于 fluido-attenuated inversion recovery (FLAIR) MRI。为了缓解对应领域的变化，这个工作提出了一个领域适应方法，将表达性提高到三个目标 dataset 上。silver standard (SS) mask 由目标领域中的一个新的传统影像处理方法生成，并用来补充来自源领域的 gold standard (GS) 数据，Canadian Atherosclerosis Imaging Network (CAIN)。我们测试了四个模型，分别是：1) SS+GS：在目标 SS masks 上练习并在源 GS masks 上精确化，2) GS+SS：在源 GS masks 上练习并在目标 SS masks 上精确化，3) 在源 GS (GS CAIN Only) 上练习，4) 在目标 SS masks (SS Only) 上练习。SS+GS 模型表现最好，其 mean DSC 为 0.89，CoV 为 0.05，并在三个目标领域中表现出最高的 DSC，并与 GS-only 模型在所有三个目标领域中有 statistically significant (p < 0.05) 的差异。结果显示，在目标领域的噪音标签上进行预训可以让模型适应到dataset-specific的特征，并提供了稳定的初始化，而精确化在目标 SS masks 上可以让模型学习更多的细部特征。这种方法可以广泛应用于医疗影像问题中，where labeled data 是稀有的，并可以用来加速广泛的采纳。”
</details></li>
</ul>
<hr>
<h2 id="Not-All-Steps-are-Created-Equal-Selective-Diffusion-Distillation-for-Image-Manipulation"><a href="#Not-All-Steps-are-Created-Equal-Selective-Diffusion-Distillation-for-Image-Manipulation" class="headerlink" title="Not All Steps are Created Equal: Selective Diffusion Distillation for Image Manipulation"></a>Not All Steps are Created Equal: Selective Diffusion Distillation for Image Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08448">http://arxiv.org/abs/2307.08448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andysonys/selective-diffusion-distillation">https://github.com/andysonys/selective-diffusion-distillation</a></li>
<li>paper_authors: Luozhou Wang, Shuai Yang, Shu Liu, Ying-cong Chen</li>
<li>for: 提高图像修改任务中的精度和可修改性</li>
<li>methods: 提出了一种新的框架 Selective Diffusion Distillation (SDD)，通过训练一个Feedforward图像修改网络，以及一个有效的时间步选择器，使得图像修改任务中精度和可修改性同时得到提高。</li>
<li>results: 经验证明，该框架可以成功解决图像修改任务中的质量与可修改性之间的衡量问题，并且在多个任务上达到了优秀的效果。<details>
<summary>Abstract</summary>
Conditional diffusion models have demonstrated impressive performance in image manipulation tasks. The general pipeline involves adding noise to the image and then denoising it. However, this method faces a trade-off problem: adding too much noise affects the fidelity of the image while adding too little affects its editability. This largely limits their practical applicability. In this paper, we propose a novel framework, Selective Diffusion Distillation (SDD), that ensures both the fidelity and editability of images. Instead of directly editing images with a diffusion model, we train a feedforward image manipulation network under the guidance of the diffusion model. Besides, we propose an effective indicator to select the semantic-related timestep to obtain the correct semantic guidance from the diffusion model. This approach successfully avoids the dilemma caused by the diffusion process. Our extensive experiments demonstrate the advantages of our framework. Code is released at https://github.com/AndysonYs/Selective-Diffusion-Distillation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>Conditional diffusion models 在图像修饰任务中表现出色。通常的管道包括在图像上添加噪声并 затем去噪。然而，这种方法面临一个负担选择问题：添加过多噪声会影响图像的准确性，而添加过少噪声则会影响图像的修饰性。这主要限制了它们的实际应用。在这篇论文中，我们提出了一个新的框架，选择性填充分离（SDD），以确保图像的准确性和修饰性。而不是直接使用扩散模型修饰图像，我们在扩散模型的指导下训练了一个Feedforward图像修饰网络。此外，我们提出了一个有效的指标选择 semantic相关的时间步骤，以获取正确的semantic指导从扩散模型。这种方法成功避免了扩散过程所导致的困难。我们的广泛实验表明了我们的框架的优势。代码发布在https://github.com/AndysonYs/Selective-Diffusion-Distillation。
</details></li>
</ul>
<hr>
<h2 id="DOT-A-Distillation-Oriented-Trainer"><a href="#DOT-A-Distillation-Oriented-Trainer" class="headerlink" title="DOT: A Distillation-Oriented Trainer"></a>DOT: A Distillation-Oriented Trainer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08436">http://arxiv.org/abs/2307.08436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Borui Zhao, Quan Cui, Renjie Song, Jiajun Liang</li>
<li>for: 本研究旨在提高知识传播过程中学生模型的优化属性，以提高模型的泛化能力。</li>
<li>methods: 本研究使用了知识传播策略，并增加了精益损失来加速学生模型的优化。</li>
<li>results: 实验表明，使用 Distillation-Oriented Trainer (DOT) 可以破坏知识传播中的负面交互，并提高学生模型的泛化能力。 DOT 在 ImageNet-1k 上 Achieves a +2.59% accuracy improvement for the ResNet50-MobileNetV1 pair.<details>
<summary>Abstract</summary>
Knowledge distillation transfers knowledge from a large model to a small one via task and distillation losses. In this paper, we observe a trade-off between task and distillation losses, i.e., introducing distillation loss limits the convergence of task loss. We believe that the trade-off results from the insufficient optimization of distillation loss. The reason is: The teacher has a lower task loss than the student, and a lower distillation loss drives the student more similar to the teacher, then a better-converged task loss could be obtained. To break the trade-off, we propose the Distillation-Oriented Trainer (DOT). DOT separately considers gradients of task and distillation losses, then applies a larger momentum to distillation loss to accelerate its optimization. We empirically prove that DOT breaks the trade-off, i.e., both losses are sufficiently optimized. Extensive experiments validate the superiority of DOT. Notably, DOT achieves a +2.59% accuracy improvement on ImageNet-1k for the ResNet50-MobileNetV1 pair. Conclusively, DOT greatly benefits the student's optimization properties in terms of loss convergence and model generalization. Code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
知识塑化是将知识从大模型传递到小模型的过程，通过任务和塑化损失来实现。在这篇论文中，我们观察到任务损失和塑化损失之间存在负面关系，即在引入塑化损失时，任务损失的收敛被限制。我们认为这种负面关系的原因是塑化损失的优化不充分。理由是：教师模型的任务损失低于学生模型，且塑化损失驱动学生模型更加相似于教师模型，然后可以获得更好的任务损失收敛。为解决这种负面关系，我们提出了塑化导向训练器（DOT）。DOT分别考虑了任务和塑化损失的梯度，然后将塑化损失的梯度应用更大的滚动矩阵，以加速其优化。我们经验证明，DOT可以破坏这种负面关系，即任务损失和塑化损失都得到了足够的优化。广泛的实验证明了DOT的优越性。特别是，DOT在ImageNet-1k上为ResNet50-MobileNetV1对得到了+2.59%的准确率提升。结论是，DOT对学生模型的优化质量有着很大的改善，包括损失收敛和模型泛化。代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Dense-Affinity-Matching-for-Few-Shot-Segmentation"><a href="#Dense-Affinity-Matching-for-Few-Shot-Segmentation" class="headerlink" title="Dense Affinity Matching for Few-Shot Segmentation"></a>Dense Affinity Matching for Few-Shot Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08434">http://arxiv.org/abs/2307.08434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Yonghan Dong, Zheming Lu, Yunlong Yu, Yingming Li, Jungong Han, Zhongfei Zhang</li>
<li>for: 这篇论文目的是提出一个几shot segmentation（FSS）方法，用于分类新的图像类型，只需要几个标注类别的数据。</li>
<li>methods: 这篇论文提出了一个紧密的相似性匹配（DAM）框架，通过密集的Pixel-to-Pixel和Pixel-to-Patch关系捕捉，以及对应的3D潜在神经网络，实现了支持-询问的互动。</li>
<li>results: 实验结果显示，DAM在十个benchmark上的性能很竞争，特别是在跨类、跨数据和跨领域的FSS任务中，仅需0.68M个parameters，表明DAM的效iveness和效率。<details>
<summary>Abstract</summary>
Few-Shot Segmentation (FSS) aims to segment the novel class images with a few annotated samples. In this paper, we propose a dense affinity matching (DAM) framework to exploit the support-query interaction by densely capturing both the pixel-to-pixel and pixel-to-patch relations in each support-query pair with the bidirectional 3D convolutions. Different from the existing methods that remove the support background, we design a hysteretic spatial filtering module (HSFM) to filter the background-related query features and retain the foreground-related query features with the assistance of the support background, which is beneficial for eliminating interference objects in the query background. We comprehensively evaluate our DAM on ten benchmarks under cross-category, cross-dataset, and cross-domain FSS tasks. Experimental results demonstrate that DAM performs very competitively under different settings with only 0.68M parameters, especially under cross-domain FSS tasks, showing its effectiveness and efficiency.
</details>
<details>
<summary>摘要</summary>
几个示例图像分割（FSS）目标是将新类图像分割成几个示例图像。在这篇论文中，我们提出了密集相似匹配（DAM）框架，利用支持Query的互动来密集捕捉每个支持Query对的像素到像素和像素到补做的关系，使用双向三维卷积来实现。与现有方法不同的是，我们设计了一种弹性空间筛选模块（HSFM），用于筛选查询背景相关的特征，保留查询背景相关的特征，以帮助消除查询背景中的干扰对象。我们在十个benchmark上进行了广泛的评估，包括跨类、跨数据集和跨领域的FSS任务。实验结果表明，DAM在不同的设置下表现非常竞争力，特别是在跨领域FSS任务中，表明其效果和效率。
</details></li>
</ul>
<hr>
<h2 id="Divide-Classify-Fine-Grained-Classification-for-City-Wide-Visual-Place-Recognition"><a href="#Divide-Classify-Fine-Grained-Classification-for-City-Wide-Visual-Place-Recognition" class="headerlink" title="Divide&amp;Classify: Fine-Grained Classification for City-Wide Visual Place Recognition"></a>Divide&amp;Classify: Fine-Grained Classification for City-Wide Visual Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08417">http://arxiv.org/abs/2307.08417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ga1i13o/Divide-and-Classify">https://github.com/ga1i13o/Divide-and-Classify</a></li>
<li>paper_authors: Gabriele Trivigno, Gabriele Berton, Carlo Masone, Juan Aragon, Barbara Caputo</li>
<li>for: 本研究旨在解决Visual Place recognition问题，即图像检索问题。</li>
<li>methods: 本研究使用分类方法，而不是传统的相似性搜索方法，以减少计算时间。</li>
<li>results: 研究提出了一种新的分类方法，称为Divide&amp;Classify（D&amp;C），可以快速和准确地进行计算，并且与现有的检索方法结合使用可以提高计算速度。<details>
<summary>Abstract</summary>
Visual Place recognition is commonly addressed as an image retrieval problem. However, retrieval methods are impractical to scale to large datasets, densely sampled from city-wide maps, since their dimension impact negatively on the inference time. Using approximate nearest neighbour search for retrieval helps to mitigate this issue, at the cost of a performance drop. In this paper we investigate whether we can effectively approach this task as a classification problem, thus bypassing the need for a similarity search. We find that existing classification methods for coarse, planet-wide localization are not suitable for the fine-grained and city-wide setting. This is largely due to how the dataset is split into classes, because these methods are designed to handle a sparse distribution of photos and as such do not consider the visual aliasing problem across neighbouring classes that naturally arises in dense scenarios. Thus, we propose a partitioning scheme that enables a fast and accurate inference, preserving a simple learning procedure, and a novel inference pipeline based on an ensemble of novel classifiers that uses the prototypes learned via an angular margin loss. Our method, Divide&Classify (D&C), enjoys the fast inference of classification solutions and an accuracy competitive with retrieval methods on the fine-grained, city-wide setting. Moreover, we show that D&C can be paired with existing retrieval pipelines to speed up computations by over 20 times while increasing their recall, leading to new state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
通常情况下，视觉地点识别被视为一个图像检索问题。然而，检索方法在大量数据集上是不可行的，因为它们的维度会导致推断时间增加。使用近似最似 neighboor search 进行检索可以减轻这个问题，但是会导致性能下降。在这篇论文中，我们研究了是否可以通过将这个任务转化为一个分类问题，从而减少需要的相似性检索。我们发现现有的分类方法不适用于细致的城市范围内的地点识别任务，主要是因为数据集被分成的类别不适合处理稠密的场景中的视觉假设问题。因此，我们提出了一种分类方案，即 Divide&Classify (D&C)，它可以快速和准确地进行推断，同时保持简单的学习过程。此外，我们还提出了一种新的推断管线，基于一个 ensemble 的新分类器，使用学习 angular margin loss 的抽象。我们的方法 D&C 可以快速地进行分类，并且与现有的检索管线结合使用可以提高计算速度，从而实现新的领先结果。
</details></li>
</ul>
<hr>
<h2 id="Monocular-3D-Object-Detection-with-LiDAR-Guided-Semi-Supervised-Active-Learning"><a href="#Monocular-3D-Object-Detection-with-LiDAR-Guided-Semi-Supervised-Active-Learning" class="headerlink" title="Monocular 3D Object Detection with LiDAR Guided Semi Supervised Active Learning"></a>Monocular 3D Object Detection with LiDAR Guided Semi Supervised Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08415">http://arxiv.org/abs/2307.08415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aral Hekimoglu, Michael Schmidt, Alvaro Marcos-Ramiro</li>
<li>for: 这个论文旨在提出一种基于 semi-supervised active learning 的精灵活的 LiDAR 引导的单目3D对象检测框架 (MonoLiG)，以利用收集的所有数据模式进行模型开发。</li>
<li>methods: 论文使用 LiDAR 作为指导，在训练单目3D检测器时不添加任何执行阶段的开销。在训练中，我们利用 LiDAR 教师和单目学生跨模态批处理法从 semi-supervised learning 中提取不标注数据中的信息作为 Pseudo-labels。</li>
<li>results: 我们的选择策略可以在 KITTI 和 Waymo 数据集上广泛地实现，并且在 state-of-the-art active learning 基础上减少标注成本至少 17%。我们的训练策略在 KITTI 3D 和 birds-eye-view (BEV) 单目对象检测官方 benchmark 中获得了前一名，提高了 BEV 平均准确率 (AP) 2.02。<details>
<summary>Abstract</summary>
We propose a novel semi-supervised active learning (SSAL) framework for monocular 3D object detection with LiDAR guidance (MonoLiG), which leverages all modalities of collected data during model development. We utilize LiDAR to guide the data selection and training of monocular 3D detectors without introducing any overhead in the inference phase. During training, we leverage the LiDAR teacher, monocular student cross-modal framework from semi-supervised learning to distill information from unlabeled data as pseudo-labels. To handle the differences in sensor characteristics, we propose a data noise-based weighting mechanism to reduce the effect of propagating noise from LiDAR modality to monocular. For selecting which samples to label to improve the model performance, we propose a sensor consistency-based selection score that is also coherent with the training objective. Extensive experimental results on KITTI and Waymo datasets verify the effectiveness of our proposed framework. In particular, our selection strategy consistently outperforms state-of-the-art active learning baselines, yielding up to 17% better saving rate in labeling costs. Our training strategy attains the top place in KITTI 3D and birds-eye-view (BEV) monocular object detection official benchmarks by improving the BEV Average Precision (AP) by 2.02.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的半监督学习框架（SSAL），用于单目3D物体检测，利用了所有数据收集时的模式。我们使用激光准备数据选择和训练单目3D检测器，无需在检测阶段添加任何负担。在训练过程中，我们利用激光师，单目学生交叉模式自动学习法从无标签数据中提取信息，作为pseudo标签。为了处理感知器特征的差异，我们提议一种数据噪音基于权重机制，以减少激光模式噪音对单目检测器的影响。为选择需要标注的样本以提高模型性能，我们提议一种感知器一致性基于选择分数，与训练目标含义一致。我们的选择策略在KITTI和Waymo数据集上进行了广泛的实验，并证明了我们的提议的有效性。特别是，我们的选择策略在活动学习基elines上一直保持状态的最佳，可以在KITTI 3D和bird's-eye-view（BEV）单目物体检测官方benchmark中提高BEV均值精度（AP）by 2.02。
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-for-Object-Detection-with-Non-Redundant-Informative-Sampling"><a href="#Active-Learning-for-Object-Detection-with-Non-Redundant-Informative-Sampling" class="headerlink" title="Active Learning for Object Detection with Non-Redundant Informative Sampling"></a>Active Learning for Object Detection with Non-Redundant Informative Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08414">http://arxiv.org/abs/2307.08414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aral Hekimoglu, Adrian Brucker, Alper Kagan Kayali, Michael Schmidt, Alvaro Marcos-Ramiro</li>
<li>for: 提高2D对象检测器的性能，建立一个有代表性和多样性的数据集</li>
<li>methods: 使用不同样本之间的差异和不确定性来选择样本，并计算样本集中样本之间的信息共同分数</li>
<li>results: 比Random选择更高效，可以减少标注成本20%和30%，并且可以建立多样化的对象类型、形状和角度的数据集<details>
<summary>Abstract</summary>
Curating an informative and representative dataset is essential for enhancing the performance of 2D object detectors. We present a novel active learning sampling strategy that addresses both the informativeness and diversity of the selections. Our strategy integrates uncertainty and diversity-based selection principles into a joint selection objective by measuring the collective information score of the selected samples. Specifically, our proposed NORIS algorithm quantifies the impact of training with a sample on the informativeness of other similar samples. By exclusively selecting samples that are simultaneously informative and distant from other highly informative samples, we effectively avoid redundancy while maintaining a high level of informativeness. Moreover, instead of utilizing whole image features to calculate distances between samples, we leverage features extracted from detected object regions within images to define object features. This allows us to construct a dataset encompassing diverse object types, shapes, and angles. Extensive experiments on object detection and image classification tasks demonstrate the effectiveness of our strategy over the state-of-the-art baselines. Specifically, our selection strategy achieves a 20% and 30% reduction in labeling costs compared to random selection for PASCAL-VOC and KITTI, respectively.
</details>
<details>
<summary>摘要</summary>
curating an informative and representative dataset is crucial for enhancing the performance of 2D object detectors. we present a novel active learning sampling strategy that addresses both the informativeness and diversity of the selections. our strategy integrates uncertainty and diversity-based selection principles into a joint selection objective by measuring the collective information score of the selected samples. specifically, our proposed NORIS algorithm quantifies the impact of training with a sample on the informativeness of other similar samples. by exclusively selecting samples that are simultaneously informative and distant from other highly informative samples, we effectively avoid redundancy while maintaining a high level of informativeness. moreover, instead of utilizing whole image features to calculate distances between samples, we leverage features extracted from detected object regions within images to define object features. this allows us to construct a dataset encompassing diverse object types, shapes, and angles. extensive experiments on object detection and image classification tasks demonstrate the effectiveness of our strategy over the state-of-the-art baselines. specifically, our selection strategy achieves a 20% and 30% reduction in labeling costs compared to random selection for PASCAL-VOC and KITTI, respectively.
</details></li>
</ul>
<hr>
<h2 id="CLIP-Guided-StyleGAN-Inversion-for-Text-Driven-Real-Image-Editing"><a href="#CLIP-Guided-StyleGAN-Inversion-for-Text-Driven-Real-Image-Editing" class="headerlink" title="CLIP-Guided StyleGAN Inversion for Text-Driven Real Image Editing"></a>CLIP-Guided StyleGAN Inversion for Text-Driven Real Image Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08397">http://arxiv.org/abs/2307.08397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/johnberg1/CLIPInverter">https://github.com/johnberg1/CLIPInverter</a></li>
<li>paper_authors: Ahmet Canberk Baykal, Abdul Basit Anees, Duygu Ceylan, Erkut Erdem, Aykut Erdem, Deniz Yuret</li>
<li>for: 用于实现基于自然语言描述的图像编辑</li>
<li>methods: 使用 StyleGAN 模型和 CLIP embedding 进行图像编辑，并使用 novel 的文本条件 adapter 层来实现多属性变化</li>
<li>results: 比其他方法更高效和精准地完成多属性变化，并且在不同领域（人脸、猫、鸟等）表现出更高的推理精度和图像真实性<details>
<summary>Abstract</summary>
Researchers have recently begun exploring the use of StyleGAN-based models for real image editing. One particularly interesting application is using natural language descriptions to guide the editing process. Existing approaches for editing images using language either resort to instance-level latent code optimization or map predefined text prompts to some editing directions in the latent space. However, these approaches have inherent limitations. The former is not very efficient, while the latter often struggles to effectively handle multi-attribute changes. To address these weaknesses, we present CLIPInverter, a new text-driven image editing approach that is able to efficiently and reliably perform multi-attribute changes. The core of our method is the use of novel, lightweight text-conditioned adapter layers integrated into pretrained GAN-inversion networks. We demonstrate that by conditioning the initial inversion step on the CLIP embedding of the target description, we are able to obtain more successful edit directions. Additionally, we use a CLIP-guided refinement step to make corrections in the resulting residual latent codes, which further improves the alignment with the text prompt. Our method outperforms competing approaches in terms of manipulation accuracy and photo-realism on various domains including human faces, cats, and birds, as shown by our qualitative and quantitative results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Revisiting-Scene-Text-Recognition-A-Data-Perspective"><a href="#Revisiting-Scene-Text-Recognition-A-Data-Perspective" class="headerlink" title="Revisiting Scene Text Recognition: A Data Perspective"></a>Revisiting Scene Text Recognition: A Data Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08723">http://arxiv.org/abs/2307.08723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Mountchicken/Union14M">https://github.com/Mountchicken/Union14M</a></li>
<li>paper_authors: Qing Jiang, Jiapeng Wang, Dezhi Peng, Chongyu Liu, Lianwen Jin</li>
<li>for: 本研究旨在从数据驱动的角度重新评估场景文本识别（STR）。</li>
<li>methods: 我们首先回顾了场景文本识别领域的六个常用标准 benchmark，并发现了性能饱和现象，即仅有2.91%的标准图像无法由13种表征模型准确识别。</li>
<li>results: 我们的实验表明，13种模型在400万个标注图像上的平均准确率只有66.53%， indicating that STR still faces numerous challenges in real-world scenarios。<details>
<summary>Abstract</summary>
This paper aims to re-assess scene text recognition (STR) from a data-oriented perspective. We begin by revisiting the six commonly used benchmarks in STR and observe a trend of performance saturation, whereby only 2.91% of the benchmark images cannot be accurately recognized by an ensemble of 13 representative models. While these results are impressive and suggest that STR could be considered solved, however, we argue that this is primarily due to the less challenging nature of the common benchmarks, thus concealing the underlying issues that STR faces. To this end, we consolidate a large-scale real STR dataset, namely Union14M, which comprises 4 million labeled images and 10 million unlabeled images, to assess the performance of STR models in more complex real-world scenarios. Our experiments demonstrate that the 13 models can only achieve an average accuracy of 66.53% on the 4 million labeled images, indicating that STR still faces numerous challenges in the real world. By analyzing the error patterns of the 13 models, we identify seven open challenges in STR and develop a challenge-driven benchmark consisting of eight distinct subsets to facilitate further progress in the field. Our exploration demonstrates that STR is far from being solved and leveraging data may be a promising solution. In this regard, we find that utilizing the 10 million unlabeled images through self-supervised pre-training can significantly improve the robustness of STR model in real-world scenarios and leads to state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文旨在从数据驱动的角度重新评估场景文本识别（STR）。我们开始是查看常用的六个STR benchmark，并观察到表现饱和的趋势，只有2.91%的benchmark图像无法被13种代表性模型准确地识别。虽然这些结果吸引人并建议STR可以被视为解决的，但我们认为这主要归结于常用的benchmark图像的更容易识别性，因此隐藏STR实际面临的问题。为此，我们整合了大规模的实际STR数据集，即Union14M，该数据集包括400万标注图像和1000万无标注图像，以评估STR模型在更复杂的实际场景中的性能。我们的实验表明，13种模型只能在400万标注图像上 achieve an average accuracy of 66.53%，表明STR在实际场景中仍面临许多挑战。通过分析13种模型的错误模式，我们确定了七个STR中的开放挑战，并开发了一个基于这些挑战的挑战驱动benchmark，包括八个不同的子集，以促进STR领域的进一步进步。我们的探索表明，STR远未被解决，并且利用数据可能是一个有希望的解决方案。在这种情况下，我们发现通过对1000万无标注图像进行自主学习预训练可以在实际场景中显著改善STR模型的Robustness，并达到状态的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Snake-Convolution-based-on-Topological-Geometric-Constraints-for-Tubular-Structure-Segmentation"><a href="#Dynamic-Snake-Convolution-based-on-Topological-Geometric-Constraints-for-Tubular-Structure-Segmentation" class="headerlink" title="Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation"></a>Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08388">http://arxiv.org/abs/2307.08388</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaoleiqi/dscnet">https://github.com/yaoleiqi/dscnet</a></li>
<li>paper_authors: Yaolei Qi, Yuting He, Xiaoming Qi, Yuan Zhang, Guanyu Yang<br>for: 这种研究旨在提高 tubular 结构 segmentation 任务中的准确性和效率，这些结构包括血管和道路等。methods: 该研究使用了动态蛇卷 convolution 技术来正确地捕捉 tubular 结构的特征，并提出了多视角特征融合策略以保持多种全球形态的重要信息。results: 实验表明，使用 DSCNet 可以在 2D 和 3D 数据集上提供更高的准确性和连续性，比较常见的方法更好。<details>
<summary>Abstract</summary>
Accurate segmentation of topological tubular structures, such as blood vessels and roads, is crucial in various fields, ensuring accuracy and efficiency in downstream tasks. However, many factors complicate the task, including thin local structures and variable global morphologies. In this work, we note the specificity of tubular structures and use this knowledge to guide our DSCNet to simultaneously enhance perception in three stages: feature extraction, feature fusion, and loss constraint. First, we propose a dynamic snake convolution to accurately capture the features of tubular structures by adaptively focusing on slender and tortuous local structures. Subsequently, we propose a multi-view feature fusion strategy to complement the attention to features from multiple perspectives during feature fusion, ensuring the retention of important information from different global morphologies. Finally, a continuity constraint loss function, based on persistent homology, is proposed to constrain the topological continuity of the segmentation better. Experiments on 2D and 3D datasets show that our DSCNet provides better accuracy and continuity on the tubular structure segmentation task compared with several methods. Our codes will be publicly available.
</details>
<details>
<summary>摘要</summary>
Accurate segmentation of topological tubular structures, such as blood vessels and roads, is crucial in various fields, ensuring accuracy and efficiency in downstream tasks. However, many factors complicate the task, including thin local structures and variable global morphologies. In this work, we note the specificity of tubular structures and use this knowledge to guide our DSCNet to simultaneously enhance perception in three stages: feature extraction, feature fusion, and loss constraint. First, we propose a dynamic snake convolution to accurately capture the features of tubular structures by adaptively focusing on slender and tortuous local structures. Subsequently, we propose a multi-view feature fusion strategy to complement the attention to features from multiple perspectives during feature fusion, ensuring the retention of important information from different global morphologies. Finally, a continuity constraint loss function, based on persistent homology, is proposed to constrain the topological continuity of the segmentation better. Experiments on 2D and 3D datasets show that our DSCNet provides better accuracy and continuity on the tubular structure segmentation task compared with several methods. Our codes will be publicly available.Here's the translation in Traditional Chinese as well:同样，精确的分类 tubular structures, such as blood vessels and roads, 在多个领域是重要的，以确保下游任务的精度和效率。然而，多个因素会复杂这个任务，包括细部本地结构和变化的全球形态。在这个工作中，我们注意到 tubular structures 的特有性，并将这些知识用于导引我们的 DSCNet，以同时增强特性在三个阶段：特征提取、特征融合和损失约束。首先，我们提出了动态蛇条件，以精确地捕捉 tubular structures 的特征，并适应细长和迂回的本地结构。接着，我们提出了多观点特征融合策略，以补充多个观点的特征，以确保保留不同全球形态中的重要信息。最后，我们提出了基于 persistent homology 的连续约束损失函数，以更好地限制分类的topological continuity。实验结果显示，我们的 DSCNet 在 tubular structure 分类任务中提供了更好的精度和连续性，较于多种方法。我们的代码将会公开。
</details></li>
</ul>
<hr>
<h2 id="Distributed-bundle-adjustment-with-block-based-sparse-matrix-compression-for-super-large-scale-datasets"><a href="#Distributed-bundle-adjustment-with-block-based-sparse-matrix-compression-for-super-large-scale-datasets" class="headerlink" title="Distributed bundle adjustment with block-based sparse matrix compression for super large scale datasets"></a>Distributed bundle adjustment with block-based sparse matrix compression for super large scale datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08383">http://arxiv.org/abs/2307.08383</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MozartZheng/DistributedBA">https://github.com/MozartZheng/DistributedBA</a></li>
<li>paper_authors: Maoteng Zheng, Nengcheng Chen, Junfeng Zhu, Xiaoru Zeng, Huanbin Qiu, Yuyao Jiang, Xingyue Lu, Hao Qu</li>
<li>for: 这篇论文主要是为了解决大规模数据集中的摄像头系统Bundle Adjustment（BA）问题。</li>
<li>methods: 该方法使用精确的Levenberg-Marquardt（LM）算法来实现分布式摄像头系统（DBA），而不是使用估计算法来适应平行框架。它还使用块基于稀疏矩阵压缩格式（BSMC）来压缩大规模的摄像头系统（RCS），以便分布式存储和更新。</li>
<li>results: 经过评估和比较，该方法在各种数据集上显示了高效的内存使用和广泛的可扩展性，比基eline上的方法更高效。首次在实际数据集上实现了平行BA使用LM算法，处理118万张图像和1000万张图像（相对于状态艺术LPM-based BA的500倍）。<details>
<summary>Abstract</summary>
We propose a distributed bundle adjustment (DBA) method using the exact Levenberg-Marquardt (LM) algorithm for super large-scale datasets. Most of the existing methods partition the global map to small ones and conduct bundle adjustment in the submaps. In order to fit the parallel framework, they use approximate solutions instead of the LM algorithm. However, those methods often give sub-optimal results. Different from them, we utilize the exact LM algorithm to conduct global bundle adjustment where the formation of the reduced camera system (RCS) is actually parallelized and executed in a distributed way. To store the large RCS, we compress it with a block-based sparse matrix compression format (BSMC), which fully exploits its block feature. The BSMC format also enables the distributed storage and updating of the global RCS. The proposed method is extensively evaluated and compared with the state-of-the-art pipelines using both synthetic and real datasets. Preliminary results demonstrate the efficient memory usage and vast scalability of the proposed method compared with the baselines. For the first time, we conducted parallel bundle adjustment using LM algorithm on a real datasets with 1.18 million images and a synthetic dataset with 10 million images (about 500 times that of the state-of-the-art LM-based BA) on a distributed computing system.
</details>
<details>
<summary>摘要</summary>
我们提议一种分布式束适应（DBA）方法，使用精确的Levenberg-Marquardt（LM）算法进行超大规模数据集处理。现有的方法通常将全球地图分割成小地图，并在子地图中进行束适应。为适应并行框架，它们通常使用估计而不是LM算法。然而，这些方法通常会给出低于优化的结果。与之不同的是，我们利用精确的LM算法来进行全球束适应，并将Camera系统的减少（RCS）实际上并行并在分布式环境中执行。为存储大RCS，我们使用块基本稀疏矩阵压缩格式（BSMC），这种格式充分利用了块特点。BSMC格式还允许分布式存储和更新全球RCS。我们提出的方法与现有的管道进行了广泛的评估和比较，使用了真实和 sintetic 数据集。初步结果表明我们的方法具有高效的内存使用和广泛的扩展性，与基eline相比。此外，我们首次在真实数据集上进行了并行束适应，使用LM算法，并处理1.18万张图像和10万张图像（约500倍于现有LM基于BA的状态）在分布式计算系统上。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Monocular-Depth-Estimation-Let’s-Talk-About-The-Weather"><a href="#Self-supervised-Monocular-Depth-Estimation-Let’s-Talk-About-The-Weather" class="headerlink" title="Self-supervised Monocular Depth Estimation: Let’s Talk About The Weather"></a>Self-supervised Monocular Depth Estimation: Let’s Talk About The Weather</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08357">http://arxiv.org/abs/2307.08357</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kieran514/robustdepth">https://github.com/kieran514/robustdepth</a></li>
<li>paper_authors: Kieran Saunders, George Vogiatzis, Luis Manso<br>for:这篇论文旨在提出一种 Pseudo-supervised 方法，使得自助学习深度估计模型能够在不同天气和光照条件下进行高性能的估计。methods:该方法使用计算机图形和生成模型来对现有的晴天数据进行数据增强，以模拟不利天气效果。此外，该方法还使用 Pseudo-supervised 损失函数，以提高depth和pose估计的性能。results:测试结果表明，该方法（Robust-Depth）在 KITTI 数据集上达到了 State-of-the-Art 性能，而在具有困难天气条件的数据集上，如 DrivingStereo、Foggy CityScape 和 NuScenes-Night，则有 significanly 高的性能。<details>
<summary>Abstract</summary>
Current, self-supervised depth estimation architectures rely on clear and sunny weather scenes to train deep neural networks. However, in many locations, this assumption is too strong. For example in the UK (2021), 149 days consisted of rain. For these architectures to be effective in real-world applications, we must create models that can generalise to all weather conditions, times of the day and image qualities. Using a combination of computer graphics and generative models, one can augment existing sunny-weather data in a variety of ways that simulate adverse weather effects. While it is tempting to use such data augmentations for self-supervised depth, in the past this was shown to degrade performance instead of improving it. In this paper, we put forward a method that uses augmentations to remedy this problem. By exploiting the correspondence between unaugmented and augmented data we introduce a pseudo-supervised loss for both depth and pose estimation. This brings back some of the benefits of supervised learning while still not requiring any labels. We also make a series of practical recommendations which collectively offer a reliable, efficient framework for weather-related augmentation of self-supervised depth from monocular video. We present extensive testing to show that our method, Robust-Depth, achieves SotA performance on the KITTI dataset while significantly surpassing SotA on challenging, adverse condition data such as DrivingStereo, Foggy CityScape and NuScenes-Night. The project website can be found here https://kieran514.github.io/Robust-Depth-Project/.
</details>
<details>
<summary>摘要</summary>
当前的自助学深度估算架构假设需要清晰的天气和日光照明来训练深度学习模型。然而，在许多地方，这个假设是太强大。例如在英国（2021年），有149天雨天。为了使这些架构在实际应用中效果，我们需要创建可以总结到所有天气条件、时间和图像质量的模型。使用计算机图形和生成模型，我们可以对现有的晴天数据进行多种修改，以模拟不利的天气效果。尽管这可能看起来有趣，但在过去，这些数据修改方法实际上会降低性能而不是提高它。在这篇论文中，我们提出了一种使用修改来解决这个问题的方法。通过利用未修改和修改数据之间的对应关系，我们引入了一种假超级vised损失函数，用于估算深度和pose。这种方法可以带来一些supervised学习的好处，而不需要任何标签。我们还提供了一系列实用的建议，这些建议共同组成一个可靠、高效的气候相关数据修改框架，用于自助学深度从单光视频中的估算。我们对KITTI数据集进行了广泛的测试，并证明了我们的方法Robust-Depth可以在KITTI数据集上达到SotA性能，并在抗气候条件数据集上（如DrivingStereo、Foggy CityScape和NuScenes-Night）表现出显著超过SotA。 project网站的地址为https://kieran514.github.io/Robust-Depth-Project/.
</details></li>
</ul>
<hr>
<h2 id="Box-DETR-Understanding-and-Boxing-Conditional-Spatial-Queries"><a href="#Box-DETR-Understanding-and-Boxing-Conditional-Spatial-Queries" class="headerlink" title="Box-DETR: Understanding and Boxing Conditional Spatial Queries"></a>Box-DETR: Understanding and Boxing Conditional Spatial Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08353">http://arxiv.org/abs/2307.08353</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tiny-smart/box-detr">https://github.com/tiny-smart/box-detr</a></li>
<li>paper_authors: Wenze Liu, Hao Lu, Yuliang Liu, Zhiguo Cao</li>
<li>for: 提高DETR的快速启动和检测性能</li>
<li>methods: 使用 conditional spatial queries 和 conditional linear projection，并将盒子信息转化为头specific agent points</li>
<li>results: 提高了启动速度和检测性能，例如使用 ResNet-50 的单个尺度模型达到了 $44.2$ APHere’s a brief summary of the paper in English:The paper proposes a method called Box Agent to improve the performance of DETR, a popular object detection framework. The method condenses the box information into head-specific agent points, allowing the conditional cross-attention to search for positions from a more reasonable starting point. This reduces the burden of the conditional linear projection and leads to faster convergence and improved detection performance. The method requires minor modifications to the code and has negligible computational workload.<details>
<summary>Abstract</summary>
Conditional spatial queries are recently introduced into DEtection TRansformer (DETR) to accelerate convergence. In DAB-DETR, such queries are modulated by the so-called conditional linear projection at each decoder stage, aiming to search for positions of interest such as the four extremities of the box. Each decoder stage progressively updates the box by predicting the anchor box offsets, while in cross-attention only the box center is informed as the reference point. The use of only box center, however, leaves the width and height of the previous box unknown to the current stage, which hinders accurate prediction of offsets. We argue that the explicit use of the entire box information in cross-attention matters. In this work, we propose Box Agent to condense the box into head-specific agent points. By replacing the box center with the agent point as the reference point in each head, the conditional cross-attention can search for positions from a more reasonable starting point by considering the full scope of the previous box, rather than always from the previous box center. This significantly reduces the burden of the conditional linear projection. Experimental results show that the box agent leads to not only faster convergence but also improved detection performance, e.g., our single-scale model achieves $44.2$ AP with ResNet-50 based on DAB-DETR. Our Box Agent requires minor modifications to the code and has negligible computational workload. Code is available at https://github.com/tiny-smart/box-detr.
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate into Simplified Chinese Conditional spatial queries 是在 Detection Transformer (DETR) 中最近引入的，以加速减速。在 DAB-DETR 中，这些查询被称为 conditional linear projection 的模ulates，以每个解码器阶段进行搜索，以找到包括四个顶点的盒体的位置。每个解码器阶段都会逐渐更新盒体，通过预测盒体偏移量，而在跨attenion中只有盒体中心作为参考点。然而，使用只有盒体中心作为参考点，会使得上一个盒体的宽度和高度无法知道当前阶段，从而阻碍精确预测偏移量。我们认为，Explicitly 使用整个盒体信息在 cross-attention 中 matters。在这种情况下，我们提议使用 Box Agent 来压缩盒体到 head-specific agent point。通过在每个 head 中将盒体中心点 replaced 为代理点作为参考点， conditional cross-attention 可以从更加合理的起始点开始搜索，而不是总是从上一个盒体中心点开始。这会减轻 conditional linear projection 的负担。我们的 Box Agent 需要对代码进行 minor 的修改，并且计算工作负担几乎是零。代码可以在 https://github.com/tiny-smart/box-detr 上获取。Experimental results show that our single-scale model achieves $44.2$ AP with ResNet-50 based on DAB-DETR. Our Box Agent has two main advantages: faster convergence and improved detection performance. By using the entire box information in cross-attention, the agent can search for positions from a more reasonable starting point, rather than always from the previous box center. This significantly reduces the burden of the conditional linear projection. In addition, our Box Agent requires minor modifications to the code and has negligible computational workload, making it easy to implement and deploy.
</details></li>
</ul>
<hr>
<h2 id="Neural-Modulation-Fields-for-Conditional-Cone-Beam-Neural-Tomography"><a href="#Neural-Modulation-Fields-for-Conditional-Cone-Beam-Neural-Tomography" class="headerlink" title="Neural Modulation Fields for Conditional Cone Beam Neural Tomography"></a>Neural Modulation Fields for Conditional Cone Beam Neural Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08351">http://arxiv.org/abs/2307.08351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/samuelepapa/cond-cbnt">https://github.com/samuelepapa/cond-cbnt</a></li>
<li>paper_authors: Samuele Papa, David M. Knigge, Riccardo Valperga, Nikita Moriakov, Miltos Kofinas, Jan-Jakob Sonke, Efstratios Gavves</li>
<li>for: 提高CBCT重建精度</li>
<li>methods: 使用深度学习方法，包括conditional neural fields和Neural Modulation Field</li>
<li>results: 在不同数量的投影下，Conditional Cone Beam Neural Tomography表现更好，包括降低误差和提高精度<details>
<summary>Abstract</summary>
Conventional Computed Tomography (CT) methods require large numbers of noise-free projections for accurate density reconstructions, limiting their applicability to the more complex class of Cone Beam Geometry CT (CBCT) reconstruction. Recently, deep learning methods have been proposed to overcome these limitations, with methods based on neural fields (NF) showing strong performance, by approximating the reconstructed density through a continuous-in-space coordinate based neural network. Our focus is on improving such methods, however, unlike previous work, which requires training an NF from scratch for each new set of projections, we instead propose to leverage anatomical consistencies over different scans by training a single conditional NF on a dataset of projections. We propose a novel conditioning method where local modulations are modeled per patient as a field over the input domain through a Neural Modulation Field (NMF). The resulting Conditional Cone Beam Neural Tomography (CondCBNT) shows improved performance for both high and low numbers of available projections on noise-free and noisy data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adaptive-Local-Basis-Functions-for-Shape-Completion"><a href="#Adaptive-Local-Basis-Functions-for-Shape-Completion" class="headerlink" title="Adaptive Local Basis Functions for Shape Completion"></a>Adaptive Local Basis Functions for Shape Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08348">http://arxiv.org/abs/2307.08348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yinghdb/adaptive-local-basis-functions">https://github.com/yinghdb/adaptive-local-basis-functions</a></li>
<li>paper_authors: Hui Ying, Tianjia Shao, He Wang, Yin Yang, Kun Zhou</li>
<li>for: 这个论文的目的是完成部分点云数据的3D形状完成任务，使用深度隐函数。</li>
<li>methods: 该方法使用适应本地基函数，不受限制于特定的函数形式，通过这些基函数实现本地到本地的形状完成框架。</li>
<li>results: 该方法比现有方法更高效，能够保留本地几何细节，涵盖更多的形状，并且可以在未看过的几何上进行扩展。<details>
<summary>Abstract</summary>
In this paper, we focus on the task of 3D shape completion from partial point clouds using deep implicit functions. Existing methods seek to use voxelized basis functions or the ones from a certain family of functions (e.g., Gaussians), which leads to high computational costs or limited shape expressivity. On the contrary, our method employs adaptive local basis functions, which are learned end-to-end and not restricted in certain forms. Based on those basis functions, a local-to-local shape completion framework is presented. Our algorithm learns sparse parameterization with a small number of basis functions while preserving local geometric details during completion. Quantitative and qualitative experiments demonstrate that our method outperforms the state-of-the-art methods in shape completion, detail preservation, generalization to unseen geometries, and computational cost. Code and data are at https://github.com/yinghdb/Adaptive-Local-Basis-Functions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注3D形状完成从部分点云使用深度隐函数的任务。现有方法通常使用块化基函数或一定家族函数（例如高斯函数），这会导致高计算成本或局部形态表达力有限。相反，我们的方法使用适应地ocal基函数，这些基函数通过端到端学习而不受限制。基于这些基函数，我们提出了一种本地到本地的形状完成框架。我们的算法可以学习少量的基函数参数，同时保留完成过程中的地方准确性。量化和质量实验表明，我们的方法在形状完成、准确性、未经见过的几何体 generale和计算成本方面都高于当前的方法。代码和数据可以在https://github.com/yinghdb/Adaptive-Local-Basis-Functions上找到。
</details></li>
</ul>
<hr>
<h2 id="Soft-Curriculum-for-Learning-Conditional-GANs-with-Noisy-Labeled-and-Uncurated-Unlabeled-Data"><a href="#Soft-Curriculum-for-Learning-Conditional-GANs-with-Noisy-Labeled-and-Uncurated-Unlabeled-Data" class="headerlink" title="Soft Curriculum for Learning Conditional GANs with Noisy-Labeled and Uncurated Unlabeled Data"></a>Soft Curriculum for Learning Conditional GANs with Noisy-Labeled and Uncurated Unlabeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08319">http://arxiv.org/abs/2307.08319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Katsumata, Duc Minh Vo, Tatsuya Harada, Hideki Nakayama</li>
<li>for: 用于提高 conditional generative adversarial network 的训练，使其能够处理含有噪声和无标签数据的情况。</li>
<li>methods: 提出了一种新的 Conditional Image Generation 框架，该框架在训练时接受噪声和无标签数据，并使用 soft curriculum learning 来杜绝噪声和无标签数据的影响。</li>
<li>results: 对比 semi-supervised 和 label-noise 鲁棒方法，提出的方法在量化和质量上均达到了更高的表现。特别是，该方法能够与少于半个标注数据的情况下匹配 semi-supervised GANs 的表现。<details>
<summary>Abstract</summary>
Label-noise or curated unlabeled data is used to compensate for the assumption of clean labeled data in training the conditional generative adversarial network; however, satisfying such an extended assumption is occasionally laborious or impractical. As a step towards generative modeling accessible to everyone, we introduce a novel conditional image generation framework that accepts noisy-labeled and uncurated unlabeled data during training: (i) closed-set and open-set label noise in labeled data and (ii) closed-set and open-set unlabeled data. To combat it, we propose soft curriculum learning, which assigns instance-wise weights for adversarial training while assigning new labels for unlabeled data and correcting wrong labels for labeled data. Unlike popular curriculum learning, which uses a threshold to pick the training samples, our soft curriculum controls the effect of each training instance by using the weights predicted by the auxiliary classifier, resulting in the preservation of useful samples while ignoring harmful ones. Our experiments show that our approach outperforms existing semi-supervised and label-noise robust methods in terms of both quantitative and qualitative performance. In particular, the proposed approach is able to match the performance of (semi-) supervised GANs even with less than half the labeled data.
</details>
<details>
<summary>摘要</summary>
文本中的描述：用于资料准备的标签噪声或精心挑选的无标签数据被用来补偿 conditional generative adversarial network 的假设，但满足这种扩展的假设 occasional 是劳动ious 或 impractical。为了实现 everyone 可以接触的生成模型，我们介绍了一种新的 conditional 图像生成框架，该框架在训练时接受噪声标签和无标签数据：（i） closed-set 和 open-set 标签噪声在标签数据中，（ii） closed-set 和 open-set 无标签数据。为了解决这个问题，我们提出了软预科学学习，它在对抗式训练中分配每个实例的权重，并为无标签数据分配新的标签，并对标签数据中的错误标签进行更正。与传统的预科学学习不同，我们的软预科学学习不使用阈值来选择训练样本，而是使用辅助分类器预测的权重来控制每个训练实例的影响，从而保留有用的样本，而忽略有害的样本。我们的实验显示，我们的方法在量化和质量上都超过了现有的半支持和标签噪声鲁棒方法。特别是，我们的方法能够与少于半个标签数据相当的性能。
</details></li>
</ul>
<hr>
<h2 id="Airway-Label-Prediction-in-Video-Bronchoscopy-Capturing-Temporal-Dependencies-Utilizing-Anatomical-Knowledge"><a href="#Airway-Label-Prediction-in-Video-Bronchoscopy-Capturing-Temporal-Dependencies-Utilizing-Anatomical-Knowledge" class="headerlink" title="Airway Label Prediction in Video Bronchoscopy: Capturing Temporal Dependencies Utilizing Anatomical Knowledge"></a>Airway Label Prediction in Video Bronchoscopy: Capturing Temporal Dependencies Utilizing Anatomical Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08318">http://arxiv.org/abs/2307.08318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ron Keuth, Mattias Heinrich, Martin Eichenlaub, Marian Himstedt<br>for:本研究旨在提供无需电磁跟踪和特定病人CT扫描的视觉导航，以便在肺部手术中进行其他应用程序，如医学护理室。methods:本研究使用单帧图像分类和肺部模型来实现视觉导航，而不需要电磁跟踪和特定病人CT扫描。研究者们通过 incorporating sequences of CNN-based airway likelihoods into a Hidden Markov Model 来使用 topological bronchoscope localization 和 anatomical constraints 来提高导航精度。results:研究者们通过多个实验在肺部模型中评估了该方法，并发现该方法可以提高导航精度至0.98，比之前的0.81（加权平均值：0.98 vs 0.81）。这表明， combining CNN-based single image classification of airway segments with anatomical constraints and temporal HMM-based inference 可以提供高度的视觉导航。<details>
<summary>Abstract</summary>
Purpose: Navigation guidance is a key requirement for a multitude of lung interventions using video bronchoscopy. State-of-the-art solutions focus on lung biopsies using electromagnetic tracking and intraoperative image registration w.r.t. preoperative CT scans for guidance. The requirement of patient-specific CT scans hampers the utilisation of navigation guidance for other applications such as intensive care units.   Methods: This paper addresses navigation guidance solely incorporating bronchosopy video data. In contrast to state-of-the-art approaches we entirely omit the use of electromagnetic tracking and patient-specific CT scans. Guidance is enabled by means of topological bronchoscope localization w.r.t. an interpatient airway model. Particularly, we take maximally advantage of anatomical constraints of airway trees being sequentially traversed. This is realized by incorporating sequences of CNN-based airway likelihoods into a Hidden Markov Model.   Results: Our approach is evaluated based on multiple experiments inside a lung phantom model. With the consideration of temporal context and use of anatomical knowledge for regularization, we are able to improve the accuracy up to to 0.98 compared to 0.81 (weighted F1: 0.98 compared to 0.81) for a classification based on individual frames.   Conclusion: We combine CNN-based single image classification of airway segments with anatomical constraints and temporal HMM-based inference for the first time. Our approach renders vision-only guidance for bronchoscopy interventions in the absence of electromagnetic tracking and patient-specific CT scans possible.
</details>
<details>
<summary>摘要</summary>
目的：用视频镜头导航是肺部内部手术中的关键需求，现代解决方案主要采用电磁 tracking和实时 CT 图像对比为导航。但这些方法受到patient-specific CT 图像的限制，不能用于医学加护部门。方法：本文提出一种具有唯视导航的方法，与现有方法不同之处在于完全不使用电磁 tracking和patient-specific CT 图像。我们通过基于隐藏 Markov 模型的空间排序和 CNN 网络来实现导航。特别是，我们利用隐藏 Markov 模型中的排序和 CNN 网络来使用排序的 temporal 上下文和空间上下文来进行补做，从而提高导航的准确性。结果：我们在肺部模型中进行了多个实验，结果表明，我们的方法可以提高准确性至 0.98，比对 individual 帧的分类结果更高（weighted F1 分数为 0.98，对比 0.81）。结论：我们结合了 CNN 网络基于单个图像分类和空间排序的方法，并利用 temporal HMM 模型来进行推理。这种方法可以在没有电磁 tracking和patient-specific CT 图像的情况下实现肺部内部手术的视野导航。
</details></li>
</ul>
<hr>
<h2 id="AltFreezing-for-More-General-Video-Face-Forgery-Detection"><a href="#AltFreezing-for-More-General-Video-Face-Forgery-Detection" class="headerlink" title="AltFreezing for More General Video Face Forgery Detection"></a>AltFreezing for More General Video Face Forgery Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08317">http://arxiv.org/abs/2307.08317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhendongwang6/altfreezing">https://github.com/zhendongwang6/altfreezing</a></li>
<li>paper_authors: Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Houqiang Li</li>
<li>for: 这个论文主要应用于面伪造检测，对于已知的攻击方法进行防护。</li>
<li>methods: 本文提出了一个捷径的方法，通过结合空间和时间特征以检测面伪造。具体来说，是使用3D ConvNet来捕捉空间和时间特征，并通过AltFreezing训练策略来鼓励模型对于空间和时间类型的伪造进行检测。</li>
<li>results: 实验结果显示，该方法能够超越现有的方法，具有更好的扩展性和应用性。<details>
<summary>Abstract</summary>
Existing face forgery detection models try to discriminate fake images by detecting only spatial artifacts (e.g., generative artifacts, blending) or mainly temporal artifacts (e.g., flickering, discontinuity). They may experience significant performance degradation when facing out-domain artifacts. In this paper, we propose to capture both spatial and temporal artifacts in one model for face forgery detection. A simple idea is to leverage a spatiotemporal model (3D ConvNet). However, we find that it may easily rely on one type of artifact and ignore the other. To address this issue, we present a novel training strategy called AltFreezing for more general face forgery detection. The AltFreezing aims to encourage the model to detect both spatial and temporal artifacts. It divides the weights of a spatiotemporal network into two groups: spatial-related and temporal-related. Then the two groups of weights are alternately frozen during the training process so that the model can learn spatial and temporal features to distinguish real or fake videos. Furthermore, we introduce various video-level data augmentation methods to improve the generalization capability of the forgery detection model. Extensive experiments show that our framework outperforms existing methods in terms of generalization to unseen manipulations and datasets. Code is available at https: //github.com/ZhendongWang6/AltFreezing.
</details>
<details>
<summary>摘要</summary>
现有的面孔伪造检测模型通常仅仅检测到空间artefacts（例如生成artefacts、融合）或主要是时间artefacts（例如闪烁、缺失连续性）。它们可能会在面对不同领域artefacts时表现出显著性能下降。在这篇论文中，我们提议一种捕捉空间和时间artefacts的一体化模型 для面孔伪造检测。一种简单的想法是利用三维ConvNet。然而，我们发现它可能会很容易依赖于一种类型的artefact并忽略另一种。为了解决这个问题，我们提出了一种新的训练策略called AltFreezing，旨在促进模型检测空间和时间artefacts。它将把一个三维网络的Weight分为两组：空间相关和时间相关。然后，这两组的Weight在训练过程中被 alternate 冻结，以便模型可以学习空间和时间特征来 distinguish real or fake videos。此外，我们还引入了多种视频级数据增强方法，以提高伪造检测模型的通用性。广泛的实验结果表明，我们的框架在面对未seen manipulations和数据集时表现出优于现有方法。代码可以在 <https://github.com/ZhendongWang6/AltFreezing> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Multi-Level-Cross-Modality-Joint-Alignment-for-Visible-Infrared-Person-Re-Identification"><a href="#Bridging-the-Gap-Multi-Level-Cross-Modality-Joint-Alignment-for-Visible-Infrared-Person-Re-Identification" class="headerlink" title="Bridging the Gap: Multi-Level Cross-Modality Joint Alignment for Visible-Infrared Person Re-Identification"></a>Bridging the Gap: Multi-Level Cross-Modality Joint Alignment for Visible-Infrared Person Re-Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08316">http://arxiv.org/abs/2307.08316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tengfei Liang, Yi Jin, Wu Liu, Tao Wang, Songhe Feng, Yidong Li</li>
<li>for: 解决可见光和infrared摄像头之间的人识别问题，即人识别任务中的跨模态图像检索问题。</li>
<li>methods: 提出了一种简单而有效的方法，即多级跨模态共同准备（MCJA），它通过修正模态和目标水平的差距，解决了跨模态图像检索问题。</li>
<li>results: 在实验中，该方法通过增加模态匹配级联augmenation和跨模态检索损失，实现了提高跨模态图像检索的性能，并可以作为VI-ReID领域的强大基线方法。<details>
<summary>Abstract</summary>
Visible-Infrared person Re-IDentification (VI-ReID) is a challenging cross-modality image retrieval task that aims to match pedestrians' images across visible and infrared cameras. To solve the modality gap, existing mainstream methods adopt a learning paradigm converting the image retrieval task into an image classification task with cross-entropy loss and auxiliary metric learning losses. These losses follow the strategy of adjusting the distribution of extracted embeddings to reduce the intra-class distance and increase the inter-class distance. However, such objectives do not precisely correspond to the final test setting of the retrieval task, resulting in a new gap at the optimization level. By rethinking these keys of VI-ReID, we propose a simple and effective method, the Multi-level Cross-modality Joint Alignment (MCJA), bridging both modality and objective-level gap. For the former, we design the Modality Alignment Augmentation, which consists of three novel strategies, the weighted grayscale, cross-channel cutmix, and spectrum jitter augmentation, effectively reducing modality discrepancy in the image space. For the latter, we introduce a new Cross-Modality Retrieval loss. It is the first work to constrain from the perspective of the ranking list, aligning with the goal of the testing stage. Moreover, based on the global feature only, our method exhibits good performance and can serve as a strong baseline method for the VI-ReID community.
</details>
<details>
<summary>摘要</summary>
visible-infrared人Re-IDentification（VI-ReID）是一个复杂的跨模态图像检索任务，旨在匹配人员的图像在可见和红外摄像头之间。为解决模态差距，现有主流方法采用学习做法，将图像检索任务转化为图像分类任务，使用十字积分损失和辅助度量学习损失。这些损失采用缩短内类距离和增加间类距离的策略，但这些目标不准确反映最终测试阶段的检索任务，导致新的优化差距。通过重新思考VI-ReID的关键，我们提出了一种简单有效的方法：多级跨模态联合准确（MCJA）。它通过以下三种新策略来减少模态差距：1. 模态准确增强：对于每个图像，使用权重规则来增强模态准确性。2. 交叉通道CMix：在不同模态之间进行交叉通道的混合，以提高模态之间的匹配度。3. 谱谱异常增强：在不同模态之间进行谱谱异常的增强，以提高模态之间的匹配度。此外，我们还引入了一种新的跨模态检索损失，它是根据排序列表来约束的，与测试阶段的目标相匹配。我们的方法只使用全球特征，可以在VI-ReID领域中作为一个强大基线方法。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Intersection-Over-Union-for-Small-Object-Detection-in-Few-Shot-Regime"><a href="#Rethinking-Intersection-Over-Union-for-Small-Object-Detection-in-Few-Shot-Regime" class="headerlink" title="Rethinking Intersection Over Union for Small Object Detection in Few-Shot Regime"></a>Rethinking Intersection Over Union for Small Object Detection in Few-Shot Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09562">http://arxiv.org/abs/2307.09562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Le Jeune, Anissa Mokraoui</li>
<li>for: 提高小 объек的检测精度</li>
<li>methods: 使用Scale-adaptive Intersection over Union（SIoU）作为评价指标和训练损失函数</li>
<li>results: 在小 объек检测 task 中，SIoU 可以大幅提高模型的性能，特别是在 aerial 图像中，其达到了新的顶峰性能Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to improve the accuracy of detecting small objects in few-shot object detection (FSOD) tasks.</li>
<li>methods: The paper proposes using a novel box similarity measure called Scale-adaptive Intersection over Union (SIoU) as an evaluation criterion and a loss function to prioritize small objects during training.</li>
<li>results: The paper shows that SIoU improves significantly the performance of small object detection in both natural (Pascal VOC and COCO datasets) and aerial images (DOTA and DIOR), especially in the aerial imagery where small objects are critical, and achieves new state-of-the-art FSOD performance on DOTA and DIOR.<details>
<summary>Abstract</summary>
In Few-Shot Object Detection (FSOD), detecting small objects is extremely difficult. The limited supervision cripples the localization capabilities of the models and a few pixels shift can dramatically reduce the Intersection over Union (IoU) between the ground truth and predicted boxes for small objects. To this end, we propose Scale-adaptive Intersection over Union (SIoU), a novel box similarity measure. SIoU changes with the objects' size, it is more lenient with small object shifts. We conducted a user study and SIoU better aligns than IoU with human judgment. Employing SIoU as an evaluation criterion helps to build more user-oriented models. SIoU can also be used as a loss function to prioritize small objects during training, outperforming existing loss functions. SIoU improves small object detection in the non-few-shot regime, but this setting is unrealistic in the industry as annotated detection datasets are often too expensive to acquire. Hence, our experiments mainly focus on the few-shot regime to demonstrate the superiority and versatility of SIoU loss. SIoU improves significantly FSOD performance on small objects in both natural (Pascal VOC and COCO datasets) and aerial images (DOTA and DIOR). In aerial imagery, small objects are critical and SIoU loss achieves new state-of-the-art FSOD on DOTA and DIOR.
</details>
<details>
<summary>摘要</summary>
几个框架内部对象检测（FSOD）中，检测小对象非常困难。有限的监督使得模型的地方化能力受到限制，几个像素的偏移可以导致对真实值和预测框之间的交集覆盖率（IoU）减少很多。为了解决这个问题，我们提出了适应缩放交集覆盖率（SIoU），一种新的框 similarity度量。SIoU随对象的大小变化，对小对象的偏移更加宽容。我们进行了用户研究，发现SIoU与人类判断更加一致。使用SIoU作为评价标准可以建立更用户 oriented的模型。SIoU还可以作为训练 criterion，以优先级驱动模型在训练中学习小对象。SIoU在几何shot regime中显著提高了小对象检测性能，但这种设定是在实际应用中不切实际的，因为检测框 datasets通常是非常昂贵的。因此，我们的实验主要集中在几何shot regime中，以示SIoU损失的优越性和多样性。SIoU在自然图像（Pascal VOC和COCO datasets）和航空图像（DOTA和DIOR）上显著提高了小对象检测性能。在航空图像中，小对象非常重要，SIoU损失实现了新的状态的法Socket的FSOD。
</details></li>
</ul>
<hr>
<h2 id="RCM-Fusion-Radar-Camera-Multi-Level-Fusion-for-3D-Object-Detection"><a href="#RCM-Fusion-Radar-Camera-Multi-Level-Fusion-for-3D-Object-Detection" class="headerlink" title="RCM-Fusion: Radar-Camera Multi-Level Fusion for 3D Object Detection"></a>RCM-Fusion: Radar-Camera Multi-Level Fusion for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10249">http://arxiv.org/abs/2307.10249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jisong Kim, Minjae Seong, Geonho Bang, Dongsuk Kum, Jun Won Choi</li>
<li>for: 本研究旨在提出一种基于雷达和摄像头的多级融合方法（RCM-Fusion），以完全利用雷达信息并提高3D对象检测性能。</li>
<li>methods: 本方法在feature级和实例级进行了雷达和摄像头的多级融合，包括Radar Guided BEV Encoder和Radar Grid Point Refinement module。Radar Guided BEV Encoder利用雷达 Bird’s-Eye-View特征将图像特征转换为精确的BEV表示，然后适应性地组合了雷达和摄像头的BEV特征。Radar Grid Point Refinement模块通过考虑雷达点云特征来减少本地化错误。</li>
<li>results: 在公共的nuScenes数据集上进行了实验，并证明了我们的提出的RCM-Fusion方法与摄像头只的基准模型相比，提高了11.8%的nuScenes检测得分（NDS），并在nuScenes 3D对象检测 benchmark中实现了雷达-摄像头融合方法的州际之最性能。<details>
<summary>Abstract</summary>
While LiDAR sensors have been succesfully applied to 3D object detection, the affordability of radar and camera sensors has led to a growing interest in fusiong radars and cameras for 3D object detection. However, previous radar-camera fusion models have not been able to fully utilize radar information in that initial 3D proposals were generated based on the camera features only and the instance-level fusion is subsequently conducted. In this paper, we propose radar-camera multi-level fusion (RCM-Fusion), which fuses radar and camera modalities at both the feature-level and instance-level to fully utilize radar information. At the feature-level, we propose a Radar Guided BEV Encoder which utilizes radar Bird's-Eye-View (BEV) features to transform image features into precise BEV representations and then adaptively combines the radar and camera BEV features. At the instance-level, we propose a Radar Grid Point Refinement module that reduces localization error by considering the characteristics of the radar point clouds. The experiments conducted on the public nuScenes dataset demonstrate that our proposed RCM-Fusion offers 11.8% performance gain in nuScenes detection score (NDS) over the camera-only baseline model and achieves state-of-the-art performaces among radar-camera fusion methods in the nuScenes 3D object detection benchmark. Code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
而LiDAR感知器已经成功应用于3D物体检测中，但由于雷达和摄像头感知器的可Affordability，有关 fusion 雷达和摄像头的研究在紧张起来。然而，之前的雷达-摄像头融合模型尚未能充分利用雷达信息，因为初始的3D提案都是基于摄像头特征来生成的，然后进行了实例级融合。在这篇论文中，我们提议了雷达-摄像头多级融合（RCM-Fusion）模型，该模型在特征级和实例级都进行雷达和摄像头模态的融合，以完全利用雷达信息。在特征级上，我们提出了雷达导航BEV编码器，该编码器利用雷达 bird's-eye-view（BEV）特征将图像特征转换为准确的BEV表示，然后适应性地合并雷达和摄像头BEV特征。在实例级上，我们提出了雷达网点精度修正模块，该模块通过考虑雷达点云特征来减少局部定位错误。我们在公共的 nuScenes 数据集上进行了实验，结果显示，我们提出的 RCM-Fusion 与摄像头基eline模型相比，提高 nuScenes 检测分数（NDS）11.8%，并在 nuScenes 3D物体检测比赛中实现了雷达-摄像头融合方法的状态器。代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Combiner-and-HyperCombiner-Networks-Rules-to-Combine-Multimodality-MR-Images-for-Prostate-Cancer-Localisation"><a href="#Combiner-and-HyperCombiner-Networks-Rules-to-Combine-Multimodality-MR-Images-for-Prostate-Cancer-Localisation" class="headerlink" title="Combiner and HyperCombiner Networks: Rules to Combine Multimodality MR Images for Prostate Cancer Localisation"></a>Combiner and HyperCombiner Networks: Rules to Combine Multimodality MR Images for Prostate Cancer Localisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08279">http://arxiv.org/abs/2307.08279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Yan, Bernard Chiu, Ziyi Shen, Qianye Yang, Tom Syer, Zhe Min, Shonit Punwani, Mark Emberton, David Atkinson, Dean C. Barratt, Yipeng Hu</li>
<li>for: 这种研究的目的是使用报告系统PI-RADS v2.1，评估multiparametric MR扫描图像中的肾癌风险。</li>
<li>methods: 这种研究使用了低维度Parametric模型，模型PI-RADS决策规则，以及HyperCombiner网络来训练一个单一的图像分割网络。</li>
<li>results: 实验结果基于850名患者的数据，表明，使用Combiner网络可以提高图像分割的效率，同时可以获得和解释个体图像模式的线性权重或征兆，以及评估图像可用性、重要性和规则发现等临床应用。<details>
<summary>Abstract</summary>
One of the distinct characteristics in radiologists' reading of multiparametric prostate MR scans, using reporting systems such as PI-RADS v2.1, is to score individual types of MR modalities, T2-weighted, diffusion-weighted, and dynamic contrast-enhanced, and then combine these image-modality-specific scores using standardised decision rules to predict the likelihood of clinically significant cancer. This work aims to demonstrate that it is feasible for low-dimensional parametric models to model such decision rules in the proposed Combiner networks, without compromising the accuracy of predicting radiologic labels: First, it is shown that either a linear mixture model or a nonlinear stacking model is sufficient to model PI-RADS decision rules for localising prostate cancer. Second, parameters of these (generalised) linear models are proposed as hyperparameters, to weigh multiple networks that independently represent individual image modalities in the Combiner network training, as opposed to end-to-end modality ensemble. A HyperCombiner network is developed to train a single image segmentation network that can be conditioned on these hyperparameters during inference, for much improved efficiency. Experimental results based on data from 850 patients, for the application of automating radiologist labelling multi-parametric MR, compare the proposed combiner networks with other commonly-adopted end-to-end networks. Using the added advantages of obtaining and interpreting the modality combining rules, in terms of the linear weights or odds-ratios on individual image modalities, three clinical applications are presented for prostate cancer segmentation, including modality availability assessment, importance quantification and rule discovery.
</details>
<details>
<summary>摘要</summary>
一个 radiologists 在多 Parametric prostate MR 扫描结果中的一个特征是，使用如 PI-RADS v2.1 的报告系统，对不同的 MR 模式（T2 重度、Diffusion 重度和动力刺激）进行分数，然后使用标准化的决策规则来预测肿瘤的可能性。这个工作的目的是证明可以使用低维度 Parametric 模型来模型这些决策规则，无需损失预测 радиологи labels 的准确性。首先，证明了线性混合模型或非线性堆叠模型都可以模型 PI-RADS 决策规则，用于Localizing 肿瘤。其次，通过将这些（总体）线性模型的参数作为权重来，以便在 Combiner 网络训练中对多个网络进行权重合并。在执行时，通过将这些参数作为 Condition 来，可以 conditioning 这些参数来提高效率。基于850名患者的数据，对多 Parametric MR 自动标注的 radiologist 标注进行比较，提出了Combiner 网络和其他常见的端到端网络之间的比较。通过获得和解释模式结合规则的优点，包括对单个图像模式的线性权重或抽象比率，对肿瘤 segmentation 进行三个临床应用：评估模式可用性、重要性评估和规则发现。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-on-Traffic-Sign-Recognition-A-Survey"><a href="#Adversarial-Attacks-on-Traffic-Sign-Recognition-A-Survey" class="headerlink" title="Adversarial Attacks on Traffic Sign Recognition: A Survey"></a>Adversarial Attacks on Traffic Sign Recognition: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08278">http://arxiv.org/abs/2307.08278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Svetlana Pavlitska, Nico Lambing, J. Marius Zöllner</li>
<li>for: 这篇论文主要针对的是自动驾驶车辆的视觉系统中的交通标志识别问题，以及这个问题如何受到深度神经网络（DNNs）的攻击。</li>
<li>methods: 该论文主要采用了现有的深度神经网络（DNNs）进行交通标志识别和分类，并对这些模型进行了数字和实际攻击。</li>
<li>results: 该论文提供了现有的攻击研究的概述，并指出了需要进一步研究的领域。<details>
<summary>Abstract</summary>
Traffic sign recognition is an essential component of perception in autonomous vehicles, which is currently performed almost exclusively with deep neural networks (DNNs). However, DNNs are known to be vulnerable to adversarial attacks. Several previous works have demonstrated the feasibility of adversarial attacks on traffic sign recognition models. Traffic signs are particularly promising for adversarial attack research due to the ease of performing real-world attacks using printed signs or stickers. In this work, we survey existing works performing either digital or real-world attacks on traffic sign detection and classification models. We provide an overview of the latest advancements and highlight the existing research areas that require further investigation.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆的见识功能中，交通标志识别是一个重要的组成部分，目前大多数使用深度神经网络（DNNs）进行实现。但是，DNNs已知容易受到对抗攻击。许多前期工作已经证明了对交通标志识别模型的攻击的可行性。由于交通标志的易于获得和修改，交通标志识别模型在实际攻击中具有极高的潜在危害性。在这种情况下，我们对交通标志检测和分类模型的攻击进行了评估和概述，并 highlighted 需要进一步研究的领域。
</details></li>
</ul>
<hr>
<h2 id="Liver-Tumor-Screening-and-Diagnosis-in-CT-with-Pixel-Lesion-Patient-Network"><a href="#Liver-Tumor-Screening-and-Diagnosis-in-CT-with-Pixel-Lesion-Patient-Network" class="headerlink" title="Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network"></a>Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08268">http://arxiv.org/abs/2307.08268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Yan, Xiaoli Yin, Yingda Xia, Fakai Wang, Shu Wang, Yuan Gao, Jiawen Yao, Chunli Li, Xiaoyu Bai, Jingren Zhou, Ling Zhang, Le Lu, Yu Shi</li>
<li>for: liver tumor segmentation and classification in non-contrast and dynamic contrast-enhanced CT images</li>
<li>methods: mask transformer with improved anchor queries and foreground-enhanced sampling loss, and an image-wise classifier to aggregate global information</li>
<li>results: high accuracy in tumor screening and lesion segmentation, and on par with a senior human radiologist in a reader study<details>
<summary>Abstract</summary>
Liver tumor segmentation and classification are important tasks in computer aided diagnosis. We aim to address three problems: liver tumor screening and preliminary diagnosis in non-contrast computed tomography (CT), and differential diagnosis in dynamic contrast-enhanced CT. A novel framework named Pixel-Lesion-pAtient Network (PLAN) is proposed. It uses a mask transformer to jointly segment and classify each lesion with improved anchor queries and a foreground-enhanced sampling loss. It also has an image-wise classifier to effectively aggregate global information and predict patient-level diagnosis. A large-scale multi-phase dataset is collected containing 939 tumor patients and 810 normal subjects. 4010 tumor instances of eight types are extensively annotated. On the non-contrast tumor screening task, PLAN achieves 95% and 96% in patient-level sensitivity and specificity. On contrast-enhanced CT, our lesion-level detection precision, recall, and classification accuracy are 92%, 89%, and 86%, outperforming widely used CNN and transformers for lesion segmentation. We also conduct a reader study on a holdout set of 250 cases. PLAN is on par with a senior human radiologist, showing the clinical significance of our results.
</details>
<details>
<summary>摘要</summary>
liver tumor segmentation和分类是计算机辅助诊断中的重要任务。我们想要解决三个问题：肝肿征检测和初步诊断在不含对比 computed tomography（CT）图像，以及在动态对比增强CT图像中的差异诊断。我们提出了一个名为Pixel-Lesion-pAtient Network（PLAN）的框架。它使用一个面对transformer来同时段和类别每个肿瘤，并使用改进的锚点查询和前景增强抽象损失来提高精度。它还有一个图像级别分类器，可以有效地聚合全局信息并预测患者级别诊断。我们收集了一个大规模多阶段数据集，包括939名患者和810名正常人。4010个肿瘤实例中有八种类型得到了广泛的注释。在非对比肿征检测任务上，PLAN达到了95%和96%的患者级别敏感性和特异性。在对比CT图像上，我们的肿瘤水平检测精度、回归率和分类精度分别为92%, 89%和86%，超越了广泛使用的CNN和transformers для肿瘤 segmentation。我们还进行了一次读者研究，并证明PLAN与一名高级人类Radiologist在250个案例中的表现相当。
</details></li>
</ul>
<hr>
<h2 id="Extreme-Image-Compression-using-Fine-tuned-VQGAN-Models"><a href="#Extreme-Image-Compression-using-Fine-tuned-VQGAN-Models" class="headerlink" title="Extreme Image Compression using Fine-tuned VQGAN Models"></a>Extreme Image Compression using Fine-tuned VQGAN Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08265">http://arxiv.org/abs/2307.08265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Mao, Tinghan Yang, Yinuo Zhang, Shuyin Pan, Meng Wang, Shiqi Wang, Siwei Ma</li>
<li>for: 提高压缩数据的感知质量，特别是在低比特率下。</li>
<li>methods: 引入vector quantization（VQ）基于生成模型，将图像表示为VQ指标。</li>
<li>results: 提出了一种简单 yet有效的编码框架，可以在低比特率下保持图像重建质量。并通过对大规模代码库进行划分，实现图像可以被表示为多个不同的VQ指标，从而实现可变比特率和不同水平的重建质量。<details>
<summary>Abstract</summary>
Recent advances in generative compression methods have demonstrated remarkable progress in enhancing the perceptual quality of compressed data, especially in scenarios with low bitrates. Nevertheless, their efficacy and applicability in achieving extreme compression ratios ($<0.1$ bpp) still remain constrained. In this work, we propose a simple yet effective coding framework by introducing vector quantization (VQ)-based generative models into the image compression domain. The main insight is that the codebook learned by the VQGAN model yields strong expressive capacity, facilitating efficient compression of continuous information in the latent space while maintaining reconstruction quality. Specifically, an image can be represented as VQ-indices by finding the nearest codeword, which can be encoded using lossless compression methods into bitstreams. We then propose clustering a pre-trained large-scale codebook into smaller codebooks using the K-means algorithm. This enables images to be represented as diverse ranges of VQ-indices maps, resulting in variable bitrates and different levels of reconstruction quality. Extensive qualitative and quantitative experiments on various datasets demonstrate that the proposed framework outperforms the state-of-the-art codecs in terms of perceptual quality-oriented metrics and human perception under extremely low bitrates.
</details>
<details>
<summary>摘要</summary>
The main idea is to use the codebook learned by the VQGAN model to efficiently compress continuous information in the latent space while maintaining reconstruction quality. Specifically, an image can be represented as VQ-indices by finding the nearest codeword, which can be encoded using lossless compression methods into bitstreams.To further improve the efficiency of the framework, we propose clustering a pre-trained large-scale codebook into smaller codebooks using the K-means algorithm. This enables images to be represented as diverse ranges of VQ-indices maps, resulting in variable bitrates and different levels of reconstruction quality.Extensive experiments on various datasets show that the proposed framework outperforms state-of-the-art codecs in terms of perceptual quality-oriented metrics and human perception under extremely low bitrates.
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Spatiotemporal-Transformers-for-Video-Object-Segmentation"><a href="#Hierarchical-Spatiotemporal-Transformers-for-Video-Object-Segmentation" class="headerlink" title="Hierarchical Spatiotemporal Transformers for Video Object Segmentation"></a>Hierarchical Spatiotemporal Transformers for Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08263">http://arxiv.org/abs/2307.08263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun-Sang Yoo, Hongjae Lee, Seung-Won Jung</li>
<li>for: 这篇论文探讨了一个新的框架，即HST，用于半监督类别影像对象分割 (VOS)。</li>
<li>methods: 这篇论文使用了最新的Swin Transformer和Video Swin Transformer来提取影像和影片特征，并将它们视为问题和内存，以获得高效的对象掩模数据。</li>
<li>results: HST在处理具有遮盾和快速移动的物体，以及压缩背景的情况下表现出色，并在多个知名的测试benchmark上表现出比以前的竞争对手更高的效果。具体来说，HST-B在YouTube-VOS（85.0%）、DAVIS 2017（85.9%）和DAVIS 2016（94.0%）等多个知名测试benchmark上表现出比以前的竞争对手更高的效果。<details>
<summary>Abstract</summary>
This paper presents a novel framework called HST for semi-supervised video object segmentation (VOS). HST extracts image and video features using the latest Swin Transformer and Video Swin Transformer to inherit their inductive bias for the spatiotemporal locality, which is essential for temporally coherent VOS. To take full advantage of the image and video features, HST casts image and video features as a query and memory, respectively. By applying efficient memory read operations at multiple scales, HST produces hierarchical features for the precise reconstruction of object masks. HST shows effectiveness and robustness in handling challenging scenarios with occluded and fast-moving objects under cluttered backgrounds. In particular, HST-B outperforms the state-of-the-art competitors on multiple popular benchmarks, i.e., YouTube-VOS (85.0%), DAVIS 2017 (85.9%), and DAVIS 2016 (94.0%).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Scale-Person-Detection-and-Localization-using-Overhead-Fisheye-Cameras"><a href="#Large-Scale-Person-Detection-and-Localization-using-Overhead-Fisheye-Cameras" class="headerlink" title="Large-Scale Person Detection and Localization using Overhead Fisheye Cameras"></a>Large-Scale Person Detection and Localization using Overhead Fisheye Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08252">http://arxiv.org/abs/2307.08252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lu Yang, Liulei Li, Xueshi Xin, Yifan Sun, Qing Song, Wenguan Wang</li>
<li>for: 本研究旨在提供一种基于折射镜相机的人员位置测定方法，以满足现代生活中的各种应用需求。</li>
<li>methods: 该方法使用了一种基于折射镜的人体探测网络，利用折射镜的扭转对称性进行培训策略，并通过数值解决方法计算实际人员位置。</li>
<li>results: 实验结果表明， compared to先前方法，该方法的折射镜人体探测器有superiority，并且整个折射镜位置测定方法可以在0.5米的准确精度下，在0.1秒钟之内确定所有人员在FOV的位置。<details>
<summary>Abstract</summary>
Location determination finds wide applications in daily life. Instead of existing efforts devoted to localizing tourist photos captured by perspective cameras, in this article, we focus on devising person positioning solutions using overhead fisheye cameras. Such solutions are advantageous in large field of view (FOV), low cost, anti-occlusion, and unaggressive work mode (without the necessity of cameras carried by persons). However, related studies are quite scarce, due to the paucity of data. To stimulate research in this exciting area, we present LOAF, the first large-scale overhead fisheye dataset for person detection and localization. LOAF is built with many essential features, e.g., i) the data cover abundant diversities in scenes, human pose, density, and location; ii) it contains currently the largest number of annotated pedestrian, i.e., 457K bounding boxes with groundtruth location information; iii) the body-boxes are labeled as radius-aligned so as to fully address the positioning challenge. To approach localization, we build a fisheye person detection network, which exploits the fisheye distortions by a rotation-equivariant training strategy and predict radius-aligned human boxes end-to-end. Then, the actual locations of the detected persons are calculated by a numerical solution on the fisheye model and camera altitude data. Extensive experiments on LOAF validate the superiority of our fisheye detector w.r.t. previous methods, and show that our whole fisheye positioning solution is able to locate all persons in FOV with an accuracy of 0.5 m, within 0.1 s.
</details>
<details>
<summary>摘要</summary>
Location determination has numerous applications in daily life. Instead of previous efforts focused on localizing tourist photos captured by perspective cameras, this article focuses on developing person positioning solutions using overhead fisheye cameras. These solutions have several advantages, including a large field of view (FOV), low cost, resistance to occlusion, and a non-intrusive work mode (without the need for cameras carried by individuals). However, there is a lack of related studies due to the scarcity of data. To promote research in this exciting area, we present LOAF, the first large-scale overhead fisheye dataset for person detection and localization. LOAF features several essential aspects, including:1. Diverse scenes, human poses, densities, and locations are covered in the data.2. It contains the largest number of annotated pedestrians, with 457,000 bounding boxes and ground truth location information.3. The body boxes are labeled as radius-aligned to fully address the positioning challenge.To perform localization, we develop a fisheye person detection network that leverages fisheye distortions using a rotation-equivariant training strategy. The network predicts radius-aligned human boxes end-to-end. Then, the actual locations of the detected persons are calculated using a numerical solution on the fisheye model and camera altitude data. Extensive experiments on LOAF demonstrate the superiority of our fisheye detector compared to previous methods, and show that our entire fisheye positioning solution can accurately locate all persons in the FOV within 0.5 meters and within 0.1 seconds.
</details></li>
</ul>
<hr>
<h2 id="Random-Boxes-Are-Open-world-Object-Detectors"><a href="#Random-Boxes-Are-Open-world-Object-Detectors" class="headerlink" title="Random Boxes Are Open-world Object Detectors"></a>Random Boxes Are Open-world Object Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08249">http://arxiv.org/abs/2307.08249</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scuwyh2000/randbox">https://github.com/scuwyh2000/randbox</a></li>
<li>paper_authors: Yanghao Wang, Zhongqi Yue, Xian-Sheng Hua, Hanwang Zhang</li>
<li>for: 本文目的是提出一种基于随机区域提议的Open-world Object Detection（OWOD）方法，以提高不知对象的检测精度。</li>
<li>methods: 本文使用的方法包括Random Box（RandBox）架构，基于Faster R-CNN和Transformer的基础，通过随机提议来增强模型的泛化能力。</li>
<li>results: 对 Pascal-VOC&#x2F;MS-COCO 和 LVIS 两个底层 benchmark 进行了评估， RandBox 在所有指标中显著超过了之前的状态方法。 codes 可以在 <a target="_blank" rel="noopener" href="https://github.com/scuwyh2000/RandBox">https://github.com/scuwyh2000/RandBox</a> 上获取。<details>
<summary>Abstract</summary>
We show that classifiers trained with random region proposals achieve state-of-the-art Open-world Object Detection (OWOD): they can not only maintain the accuracy of the known objects (w/ training labels), but also considerably improve the recall of unknown ones (w/o training labels). Specifically, we propose RandBox, a Fast R-CNN based architecture trained on random proposals at each training iteration, surpassing existing Faster R-CNN and Transformer based OWOD. Its effectiveness stems from the following two benefits introduced by randomness. First, as the randomization is independent of the distribution of the limited known objects, the random proposals become the instrumental variable that prevents the training from being confounded by the known objects. Second, the unbiased training encourages more proposal explorations by using our proposed matching score that does not penalize the random proposals whose prediction scores do not match the known objects. On two benchmarks: Pascal-VOC/MS-COCO and LVIS, RandBox significantly outperforms the previous state-of-the-art in all metrics. We also detail the ablations on randomization and loss designs. Codes are available at https://github.com/scuwyh2000/RandBox.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Randomization independence from the distribution of limited known objects: Random proposals serve as an instrumental variable that prevents training from being confounded by known objects.2. Unbiased training encourages proposal exploration: Our proposed matching score does not penalize random proposals with incorrect prediction scores, encouraging more exploration.On Pascal-VOC&#x2F;MS-COCO and LVIS benchmarks, RandBox significantly outperforms previous state-of-the-art in all metrics. We also conduct ablation studies on randomization and loss designs. The codes are available at <a target="_blank" rel="noopener" href="https://github.com/scuwyh2000/RandBox">https://github.com/scuwyh2000/RandBox</a>.</details></li>
</ol>
<hr>
<h2 id="Uncertainty-aware-State-Space-Transformer-for-Egocentric-3D-Hand-Trajectory-Forecasting"><a href="#Uncertainty-aware-State-Space-Transformer-for-Egocentric-3D-Hand-Trajectory-Forecasting" class="headerlink" title="Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting"></a>Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08243">http://arxiv.org/abs/2307.08243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Bao, Lele Chen, Libing Zeng, Zhong Li, Yi Xu, Junsong Yuan, Yu Kong</li>
<li>for: 预测人工手势 trajectory from egocentric views，以便快速理解人与AR&#x2F;VR系统的互动意图。</li>
<li>methods: 提出了一种基于RGB视频在首人视角下的 egocentric 3D手势轨迹预测任务，使用了不确定性意识的状态空间变换器（USST），并可以通过速度约束和视觉提示调整（VPT）进一步改进。</li>
<li>results: 在H2O和EgoPAT3D数据集上实现了USST的超越性，并且可以对2D和3D轨迹预测进行比较。代码和数据集公开发布在GitHub上：<a target="_blank" rel="noopener" href="https://github.com/Cogito2012/USST">https://github.com/Cogito2012/USST</a>。<details>
<summary>Abstract</summary>
Hand trajectory forecasting from egocentric views is crucial for enabling a prompt understanding of human intentions when interacting with AR/VR systems. However, existing methods handle this problem in a 2D image space which is inadequate for 3D real-world applications. In this paper, we set up an egocentric 3D hand trajectory forecasting task that aims to predict hand trajectories in a 3D space from early observed RGB videos in a first-person view. To fulfill this goal, we propose an uncertainty-aware state space Transformer (USST) that takes the merits of the attention mechanism and aleatoric uncertainty within the framework of the classical state-space model. The model can be further enhanced by the velocity constraint and visual prompt tuning (VPT) on large vision transformers. Moreover, we develop an annotation workflow to collect 3D hand trajectories with high quality. Experimental results on H2O and EgoPAT3D datasets demonstrate the superiority of USST for both 2D and 3D trajectory forecasting. The code and datasets are publicly released: https://github.com/Cogito2012/USST.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified Chinese人体轨迹预测从自центриック视角是虚拟现实/扩展现实系统中关键的一部分，以便快速理解人类意图。然而，现有方法在2D图像空间中处理这个问题，这不适用于3D实际应用。在这篇论文中，我们设置了一个 Egocentric 3D 手轨迹预测任务，旨在从早期观察到RGB视频的第一人称视角中预测手轨迹在3D空间。为实现这个目标，我们提议一种不确定性意识状态空间变换器（USST），它将注意力机制和不确定性因素内置在классиical状态空间模型中。此外，我们还提出了速度约束和视觉提示调整（VPT），以提高大规模视觉变换器的性能。此外，我们还开发了一种高质量3D手轨迹注释工作流程。实验结果表明，USST在H2O和EgoPAT3D数据集上对2D和3D轨迹预测均有superiority。代码和数据集公共发布：https://github.com/Cogito2012/USST。
</details></li>
</ul>
<hr>
<h2 id="Unified-Open-Vocabulary-Dense-Visual-Prediction"><a href="#Unified-Open-Vocabulary-Dense-Visual-Prediction" class="headerlink" title="Unified Open-Vocabulary Dense Visual Prediction"></a>Unified Open-Vocabulary Dense Visual Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08238">http://arxiv.org/abs/2307.08238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengcan Shi, Munawar Hayat, Jianfei Cai</li>
<li>for: 这篇论文旨在提出一种统一开 vocabulary 网络（UOVN），以jointly Address four 常见的dense prediction 任务。</li>
<li>methods: 该论文提出了一种多Modal, multi-scale和多任务（MMM）解码机制，以更好地利用多modal数据。此外，它还提出了一种UOVN 训练机制，以降低不同任务和领域之间的差距。</li>
<li>results: 实验结果表明，UOVN 可以有效地 Address four  datasets 上的 dense prediction 任务。<details>
<summary>Abstract</summary>
In recent years, open-vocabulary (OV) dense visual prediction (such as OV object detection, semantic, instance and panoptic segmentations) has attracted increasing research attention. However, most of existing approaches are task-specific and individually tackle each task. In this paper, we propose a Unified Open-Vocabulary Network (UOVN) to jointly address four common dense prediction tasks. Compared with separate models, a unified network is more desirable for diverse industrial applications. Moreover, OV dense prediction training data is relatively less. Separate networks can only leverage task-relevant training data, while a unified approach can integrate diverse training data to boost individual tasks. We address two major challenges in unified OV prediction. Firstly, unlike unified methods for fixed-set predictions, OV networks are usually trained with multi-modal data. Therefore, we propose a multi-modal, multi-scale and multi-task (MMM) decoding mechanism to better leverage multi-modal data. Secondly, because UOVN uses data from different tasks for training, there are significant domain and task gaps. We present a UOVN training mechanism to reduce such gaps. Experiments on four datasets demonstrate the effectiveness of our UOVN.
</details>
<details>
<summary>摘要</summary>
Recently, open-vocabulary (OV) dense visual prediction (such as OV object detection, semantic, instance and panoptic segmentations) has attracted increasing research attention. However, most existing approaches are task-specific and individually tackle each task. In this paper, we propose a Unified Open-Vocabulary Network (UOVN) to jointly address four common dense prediction tasks. Compared with separate models, a unified network is more desirable for diverse industrial applications. Moreover, OV dense prediction training data is relatively less. Separate networks can only leverage task-relevant training data, while a unified approach can integrate diverse training data to boost individual tasks. We address two major challenges in unified OV prediction. Firstly, unlike unified methods for fixed-set predictions, OV networks are usually trained with multi-modal data. Therefore, we propose a multi-modal, multi-scale and multi-task (MMM) decoding mechanism to better leverage multi-modal data. Secondly, because UOVN uses data from different tasks for training, there are significant domain and task gaps. We present a UOVN training mechanism to reduce such gaps. Experiments on four datasets demonstrate the effectiveness of our UOVN.Here's the word-for-word translation of the text into Simplified Chinese:近年来，开放词汇（OV）密集预测（如OV物体检测、 semantics、实例和杂alu segmentation）在研究中吸引了越来越多的注意力。然而，大多数现有的方法都是任务特定的，每个任务都是单独处理的。在这篇论文中，我们提出了一个统一开放词汇网络（UOVN），用于同时处理四种常见的密集预测任务。相比之下，分开的模型更加适合各种工业应用。此外，OV密集预测的训练数据相对较少。分开的网络只能利用任务相关的训练数据，而统一的方法可以更好地利用多种数据来提高个个任务。我们解决了两个主要挑战：一是与统一方法不同，OV网络通常在多模式数据上训练。因此，我们提出了一种多模式、多尺度和多任务（MMM）解码机制，以更好地利用多模式数据。二是由于UOVN在训练时使用不同任务的数据，存在域和任务漏报。我们提出了一种UOVN训练机制，以减少这些漏报。实验结果表明，我们的UOVN具有效果。
</details></li>
</ul>
<hr>
<h2 id="Video-Frame-Interpolation-with-Stereo-Event-and-Intensity-Camera"><a href="#Video-Frame-Interpolation-with-Stereo-Event-and-Intensity-Camera" class="headerlink" title="Video Frame Interpolation with Stereo Event and Intensity Camera"></a>Video Frame Interpolation with Stereo Event and Intensity Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08228">http://arxiv.org/abs/2307.08228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Ding, Mingyuan Lin, Haijian Zhang, Jianzhuang Liu, Lei Yu</li>
<li>for: 解决实时视频 interpolate 中困难很多的 cross-modality parallax 问题，提高 Event-based Video Frame Interpolation (E-VFI) 的性能。</li>
<li>methods: 提出了一种 novel Stereo Event-based VFI (SE-VFI) 网络 (SEVFI-Net)，通过Feature Aggregation Module (FAM) 缓解 parallax，并通过综合了 optical flow 和 disparity estimation 来生成高质量的中间帧和相关的分辨率信息。</li>
<li>results: 对于实际世界的复杂动作和不同深度的场景，我们的提出的 SEVFI-Net 可以与现有的 E-VFI 方法相比，在多个公共实际三视图数据集（DSEC和MVSEC）和我们自己收集的 Stereo Event-Intensity Dataset (SEID) 上达到了显著的性能提升。<details>
<summary>Abstract</summary>
The stereo event-intensity camera setup is widely applied to leverage the advantages of both event cameras with low latency and intensity cameras that capture accurate brightness and texture information. However, such a setup commonly encounters cross-modality parallax that is difficult to be eliminated solely with stereo rectification especially for real-world scenes with complex motions and varying depths, posing artifacts and distortion for existing Event-based Video Frame Interpolation (E-VFI) approaches. To tackle this problem, we propose a novel Stereo Event-based VFI (SE-VFI) network (SEVFI-Net) to generate high-quality intermediate frames and corresponding disparities from misaligned inputs consisting of two consecutive keyframes and event streams emitted between them. Specifically, we propose a Feature Aggregation Module (FAM) to alleviate the parallax and achieve spatial alignment in the feature domain. We then exploit the fused features accomplishing accurate optical flow and disparity estimation, and achieving better interpolated results through flow-based and synthesis-based ways. We also build a stereo visual acquisition system composed of an event camera and an RGB-D camera to collect a new Stereo Event-Intensity Dataset (SEID) containing diverse scenes with complex motions and varying depths. Experiments on public real-world stereo datasets, i.e., DSEC and MVSEC, and our SEID dataset demonstrate that our proposed SEVFI-Net outperforms state-of-the-art methods by a large margin.
</details>
<details>
<summary>摘要</summary>
这个双摄频码设置是广泛应用，以利用两种事件摄频的优点，即低延迟和精确的光度和 texture 信息。但是，这种设置通常会面临跨modalità 偏移，这是对单独使用摄频补偿所困难以解决，特别是 для 实际世界的场景中的复杂运动和不同的深度，导致现有的 Event-based Video Frame Interpolation (E-VFI) 方法中的缺陷和扭曲。为了解决这个问题，我们提出了一个新的双摄频基于 VFI (SE-VFI) 网络（SEVFI-Net），用于从不一致的两个关键帧和事件流中生成高品质的中频帧和相应的偏移。具体来说，我们提出了一个 Feature Aggregation Module (FAM)，以解决偏移和在特征领域进行空间Alignment。然后，我们利用融合的特征来完成精确的光流和偏移估测，并通过流动基于和synthesis基于的方法来取得更好的 interpolated 结果。我们还建立了一个双摄频视觉采集系统，该系统包括一个事件摄频和一个RGB-D 摄频，以收集一个新的双摄频 Intensity Dataset (SEID)，该dataset包括多样化的场景中的复杂运动和不同的深度。实验结果显示，我们的提案的 SEVFI-Net 在公共的实际世界双摄频dataset上（DSEC和MVSEC）和我们的 SEID dataset上都表现出色，与现有的方法相比，具有较大的改善空间。
</details></li>
</ul>
<hr>
<h2 id="Ada3D-Exploiting-the-Spatial-Redundancy-with-Adaptive-Inference-for-Efficient-3D-Object-Detection"><a href="#Ada3D-Exploiting-the-Spatial-Redundancy-with-Adaptive-Inference-for-Efficient-3D-Object-Detection" class="headerlink" title="Ada3D : Exploiting the Spatial Redundancy with Adaptive Inference for Efficient 3D Object Detection"></a>Ada3D : Exploiting the Spatial Redundancy with Adaptive Inference for Efficient 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08209">http://arxiv.org/abs/2307.08209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchen Zhao, Xuefei Ning, Ke Hong, Zhongyuan Qiu, Pu Lu, Yali Zhao, Linfeng Zhang, Lipu Zhou, Guohao Dai, Huazhong Yang, Yu Wang</li>
<li>for: 这个研究旨在提高自驾车中3D物体检测的效率，使其能够在资源有限的车辆上运行。</li>
<li>methods: 这个研究使用了适应推理框架，将输入中的空间重复点滤除，以提高模型的效率。此外，它还利用了2D BEV特征 map 的自然稀畴性，实现了缓存和computational cost的减少。</li>
<li>results: 这个研究获得了40%的缩减在3D voxels上，并将2D BEV特征 map 的密度从100%降低到20%，而无需对准确性作出牺牲。此外，这个方法可以降低模型的computational cost和缓存价值，并且实现了端到端 GPU 延迟和 GPU 峰值内存优化。<details>
<summary>Abstract</summary>
Voxel-based methods have achieved state-of-the-art performance for 3D object detection in autonomous driving. However, their significant computational and memory costs pose a challenge for their application to resource-constrained vehicles. One reason for this high resource consumption is the presence of a large number of redundant background points in Lidar point clouds, resulting in spatial redundancy in both 3D voxel and dense BEV map representations. To address this issue, we propose an adaptive inference framework called Ada3D, which focuses on exploiting the input-level spatial redundancy. Ada3D adaptively filters the redundant input, guided by a lightweight importance predictor and the unique properties of the Lidar point cloud. Additionally, we utilize the BEV features' intrinsic sparsity by introducing the Sparsity Preserving Batch Normalization. With Ada3D, we achieve 40% reduction for 3D voxels and decrease the density of 2D BEV feature maps from 100% to 20% without sacrificing accuracy. Ada3D reduces the model computational and memory cost by 5x, and achieves 1.52x/1.45x end-to-end GPU latency and 1.5x/4.5x GPU peak memory optimization for the 3D and 2D backbone respectively.
</details>
<details>
<summary>摘要</summary>
voxel-based方法已经实现了自动驾驶场景中3D对象检测的状态机器。但是，它们的计算和内存成本却成为应用于有限资源的车辆中的挑战。一个原因是Lidar点云中的背景点的大量重复，导致3D voxel和稠密的BEV地图表示中的空间重复。为解决这个问题，我们提议了一种适应性推理框架，称之为Ada3D。Ada3D通过在输入水平进行适应性滤波，以避免不必要的计算。此外，我们利用BEV特征的自然稀畴性，通过引入稀畴保持批处理normalization。与Ada3D相比，我们实现了3D voxels的40%减少和2D BEV特征地图的density从100%降至20%，无需牺牲准确性。Ada3D降低了模型的计算和内存成本，并实现了3D和2D核心的5x缩放和1.5x/4.5x GPU峰值内存优化。
</details></li>
</ul>
<hr>
<h2 id="Unbiased-Image-Synthesis-via-Manifold-Driven-Sampling-in-Diffusion-Models"><a href="#Unbiased-Image-Synthesis-via-Manifold-Driven-Sampling-in-Diffusion-Models" class="headerlink" title="Unbiased Image Synthesis via Manifold-Driven Sampling in Diffusion Models"></a>Unbiased Image Synthesis via Manifold-Driven Sampling in Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08199">http://arxiv.org/abs/2307.08199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingzhe Su, Yi Ren, Wenwen Qiang, Zeen Song, Hang Gao, Fengge Wu, Changwen Zheng<br>for: 这个研究旨在Addressing data bias in diffusion models, especially when the training data does not accurately represent the true data distribution and exhibits skewed or imbalanced patterns.methods: 我们提出了一种新的方法，即利用构造指导来减少Diffusion models中的数据偏见。我们的关键思想是使用无supervised方法估计训练数据的构造，然后使其导引Diffusion models中的抽样过程。这样可以使生成的图像在数据构造上具备均匀分布，不需要更改模型架构或重新训练。results: 我们的理论分析和实验证明，该方法可以对Diffusion models进行改善图像生成质量和不偏性。 Specifically, our method can generate more diverse and balanced images compared to standard diffusion models, and can also improve the robustness of downstream applications.<details>
<summary>Abstract</summary>
Diffusion models are a potent class of generative models capable of producing high-quality images. However, they can face challenges related to data bias, favoring specific modes of data, especially when the training data does not accurately represent the true data distribution and exhibits skewed or imbalanced patterns. For instance, the CelebA dataset contains more female images than male images, leading to biased generation results and impacting downstream applications. To address this issue, we propose a novel method that leverages manifold guidance to mitigate data bias in diffusion models. Our key idea is to estimate the manifold of the training data using an unsupervised approach, and then use it to guide the sampling process of diffusion models. This encourages the generated images to be uniformly distributed on the data manifold without altering the model architecture or necessitating labels or retraining. Theoretical analysis and empirical evidence demonstrate the effectiveness of our method in improving the quality and unbiasedness of image generation compared to standard diffusion models.
</details>
<details>
<summary>摘要</summary>
文本翻译成简化中文：Diffusion模型是一种强大的生成模型，能够生成高质量图像。然而，它们可能面临数据偏袋问题，尤其当训练数据不准确反映真实数据分布，并且呈现偏斜或不均匀的模式时。例如，CelebA数据集中有更多的女性图像 than male images，这会导致生成结果偏斜，并影响下游应用。为解决这个问题，我们提出了一种新的方法，利用拓扑导航来减轻 diffusion models 中的数据偏袋。我们的关键思想是通过不supervised的方式来估计训练数据的拓扑 manifold，然后使用它来导引 diffusion models 中的采样过程。这会使生成的图像在数据拓扑上均匀分布，而不需要修改模型结构或者需要标签或重新训练。我们的理论分析和实验证据表明，我们的方法可以提高图像生成的质量和不偏袋性，相比标准的 diffusion models。
</details></li>
</ul>
<hr>
<h2 id="On-Point-Affiliation-in-Feature-Upsampling"><a href="#On-Point-Affiliation-in-Feature-Upsampling" class="headerlink" title="On Point Affiliation in Feature Upsampling"></a>On Point Affiliation in Feature Upsampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08198">http://arxiv.org/abs/2307.08198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tiny-smart/sapa">https://github.com/tiny-smart/sapa</a></li>
<li>paper_authors: Wenze Liu, Hao Lu, Yuliang Liu, Zhiguo Cao</li>
<li>For:	+ The paper is written for improving feature upsampling in dense prediction tasks, specifically addressing the problem of point affiliation.* Methods:	+ The paper introduces the notion of point affiliation and presents a novel, lightweight, and universal upsampling solution called Similarity-Aware Point Affiliation (SAPA).	+ SAPA uses a generic formulation for generating similarity-aware upsampling kernels, which encourage not only semantic smoothness but also boundary sharpness.* Results:	+ The paper shows that SAPA outperforms prior upsamplers and consistently improves performance on a number of dense prediction tasks, including semantic segmentation, object detection, instance segmentation, panoptic segmentation, image matting, and depth estimation.<details>
<summary>Abstract</summary>
We introduce the notion of point affiliation into feature upsampling. By abstracting a feature map into non-overlapped semantic clusters formed by points of identical semantic meaning, feature upsampling can be viewed as point affiliation -- designating a semantic cluster for each upsampled point. In the framework of kernel-based dynamic upsampling, we show that an upsampled point can resort to its low-res decoder neighbors and high-res encoder point to reason the affiliation, conditioned on the mutual similarity between them. We therefore present a generic formulation for generating similarity-aware upsampling kernels and prove that such kernels encourage not only semantic smoothness but also boundary sharpness. This formulation constitutes a novel, lightweight, and universal upsampling solution, Similarity-Aware Point Affiliation (SAPA). We show its working mechanism via our preliminary designs with window-shape kernel. After probing the limitations of the designs on object detection, we reveal additional insights for upsampling, leading to SAPA with the dynamic kernel shape. Extensive experiments demonstrate that SAPA outperforms prior upsamplers and invites consistent performance improvements on a number of dense prediction tasks, including semantic segmentation, object detection, instance segmentation, panoptic segmentation, image matting, and depth estimation. Code is made available at: https://github.com/tiny-smart/sapa
</details>
<details>
<summary>摘要</summary>
我们引入点聚合（point affiliation）into feature upsampling。我们抽象特征图into non-overlapped semantic clusters formed by points of identical semantic meaning，可以视为点聚合——为每个upsampled点分配一个semantic cluster。在基于kernel的动态upsampling框架中，我们表明upsampled点可以借助其low-res decoder neighbors和高-res encoder point来理解聚合，conditioned on它们之间的相似性。我们因此提出了一种通用的形式化方法，生成相似度感知upsampling kernel，并证明这些kernel不仅激发semantic smoothness，还激发boundary sharpness。这种形式化方法被称为Similarity-Aware Point Affiliation（SAPA）。我们通过我们的初步设计中的窗口形状kernel来示出它的工作机制。在对对象检测 task进行评估后，我们揭示了更多的增强方法，导致SAPA with dynamic kernel shape。广泛的实验表明SAPA比前一代的upsamplers有更好的性能，并在多个 dense prediction task 上具有一致的表现提升，包括semantic segmentation、object detection、instance segmentation、panoptic segmentation、image matting和depth estimation。代码可以在https://github.com/tiny-smart/sapa上获取。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Image-Harmonization-with-Generative-Model-Prior"><a href="#Zero-Shot-Image-Harmonization-with-Generative-Model-Prior" class="headerlink" title="Zero-Shot Image Harmonization with Generative Model Prior"></a>Zero-Shot Image Harmonization with Generative Model Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08182">http://arxiv.org/abs/2307.08182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/windvchen/diff-harmonization">https://github.com/windvchen/diff-harmonization</a></li>
<li>paper_authors: Jianqi Chen, Zhengxia Zou, Yilan Zhang, Keyan Chen, Zhenwei Shi</li>
<li>for: 本文旨在提出一种零shot图像协调方法，不需要大量的合成图像训练。</li>
<li>methods: 我们 Draw lessons from human behavior，使用预训练的生成模型来模拟人类对协调图像的偏好。我们还提出了一种Attention-Constraint Text来引导协调方向。</li>
<li>results: 我们的方法可以具有高效性和一致性，并且可以保持前景内容结构。广泛的实验证明了我们的方法的有效性，并且我们还探索了一些有趣的应用场景。<details>
<summary>Abstract</summary>
Recent image harmonization methods have demonstrated promising results. However, due to their heavy reliance on a large number of composite images, these works are expensive in the training phase and often fail to generalize to unseen images. In this paper, we draw lessons from human behavior and come up with a zero-shot image harmonization method. Specifically, in the harmonization process, a human mainly utilizes his long-term prior on harmonious images and makes a composite image close to that prior. To imitate that, we resort to pretrained generative models for the prior of natural images. For the guidance of the harmonization direction, we propose an Attention-Constraint Text which is optimized to well illustrate the image environments. Some further designs are introduced for preserving the foreground content structure. The resulting framework, highly consistent with human behavior, can achieve harmonious results without burdensome training. Extensive experiments have demonstrated the effectiveness of our approach, and we have also explored some interesting applications.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:最近的图像协调方法已经展示出了有前途的结果，但它们往往因依赖大量的组合图像而成本高于训练阶段，而且常常无法泛化到未看过的图像。在这篇论文中，我们听从人类行为，提出了一种零shot图像协调方法。具体来说，在协调过程中，人类主要利用了长期的偏好 towards Harmonious images，使得组合图像更接近于这种偏好。为了模仿这种情况，我们采用了预训练的生成模型来定义自然图像的先验。为了指导协调方向，我们提出了一种注意力约束文本，该文本在图像环境中得到了优化。此外，我们还引入了一些保持前景内容结构的further designs。得到的框架与人类行为高度一致，可以无需压力的训练实现和谐的结果。我们进行了广泛的实验，并explored了一些有趣的应用。
</details></li>
</ul>
<hr>
<h2 id="Boundary-weighted-logit-consistency-improves-calibration-of-segmentation-networks"><a href="#Boundary-weighted-logit-consistency-improves-calibration-of-segmentation-networks" class="headerlink" title="Boundary-weighted logit consistency improves calibration of segmentation networks"></a>Boundary-weighted logit consistency improves calibration of segmentation networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08163">http://arxiv.org/abs/2307.08163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neerav Karani, Neel Dey, Polina Golland</li>
<li>for: 这个论文是为了解决神经网络预测概率和准确率之间的强相关性问题，以及图像分割数据中的固有标签抽象问题。</li>
<li>methods: 该论文使用了随机变换的稳定性来防止过于自信的预测，并提出了边界权重扩展来提供最佳的准确性调整。</li>
<li>results: 该论文对抑制癌细胞和心脏MRI分割问题取得了state-of-the-art的准确性。<details>
<summary>Abstract</summary>
Neural network prediction probabilities and accuracy are often only weakly-correlated. Inherent label ambiguity in training data for image segmentation aggravates such miscalibration. We show that logit consistency across stochastic transformations acts as a spatially varying regularizer that prevents overconfident predictions at pixels with ambiguous labels. Our boundary-weighted extension of this regularizer provides state-of-the-art calibration for prostate and heart MRI segmentation.
</details>
<details>
<summary>摘要</summary>
神经网络预测概率和准确率通常只有weakly相关。在图像分割训练数据中的自然标签抽象使得这种误差更加严重。我们表明，在Stochastic变换中的Logit一致性作为空间分布的variational regularizer，可以防止在杂 Label pixels上的过于自信。我们的边界权重扩展可以提供state-of-the-art的均衡。Note: "Simplified Chinese" refers to the written form of Chinese that uses simplified characters and grammar, which is commonly used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Self-Attention-Based-Generative-Adversarial-Networks-For-Unsupervised-Video-Summarization"><a href="#Self-Attention-Based-Generative-Adversarial-Networks-For-Unsupervised-Video-Summarization" class="headerlink" title="Self-Attention Based Generative Adversarial Networks For Unsupervised Video Summarization"></a>Self-Attention Based Generative Adversarial Networks For Unsupervised Video Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08145">http://arxiv.org/abs/2307.08145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Nektaria Minaidi, Charilaos Papaioannou, Alexandros Potamianos</li>
<li>for: 这 paper 的目的是提出一种基于无监督学习的视频摘要生成方法，使其能够生成具有代表性的摘要，与原始视频无法区分。</li>
<li>methods: 该方法基于一个流行的 Generative Adversarial Network (GAN) 的建立，通过在选择、编码和解码视频帧中引入注意力机制，以模型视频之间的时间关系。提出了 SUM-GAN-AED 模型，combines self-attention mechanism for frame selection with LSTMs for encoding and decoding.</li>
<li>results: 对 SumMe、TVSum 和 COGNIMUSE 等数据集进行了实验，结果表明，使用自注意机制作为帧选择机制，在 SumMe 上表现出色，与其他状态态的比较对 TVSum 和 COGNIMUSE 的表现相对较好。<details>
<summary>Abstract</summary>
In this paper, we study the problem of producing a comprehensive video summary following an unsupervised approach that relies on adversarial learning. We build on a popular method where a Generative Adversarial Network (GAN) is trained to create representative summaries, indistinguishable from the originals. The introduction of the attention mechanism into the architecture for the selection, encoding and decoding of video frames, shows the efficacy of self-attention and transformer in modeling temporal relationships for video summarization. We propose the SUM-GAN-AED model that uses a self-attention mechanism for frame selection, combined with LSTMs for encoding and decoding. We evaluate the performance of the SUM-GAN-AED model on the SumMe, TVSum and COGNIMUSE datasets. Experimental results indicate that using a self-attention mechanism as the frame selection mechanism outperforms the state-of-the-art on SumMe and leads to comparable to state-of-the-art performance on TVSum and COGNIMUSE.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了一种不需要监督的视频概要生成方法，基于对抗学习。我们建立在一种受欢迎的方法之上，其中一个生成概要网络（GAN）在创造可信任的概要时进行训练。我们通过将注意力机制引入网络架构中，选择、编码和解码视频帧时，以表明自我注意力和变换器在视频概要模型中的有效性。我们提出了SUM-GAN-AED模型，它使用自我注意力机制来选择帧，并使用LSTM来编码和解码。我们在SumMe、TVSum和COGNIMUSE数据集上评估SUM-GAN-AED模型的性能。实验结果表明，使用自我注意力机制来选择帧比预先的状态对SUMMe数据集表现更好，并且在TVSum和COGNIMUSE数据集上表现相当于预先的状态。
</details></li>
</ul>
<hr>
<h2 id="Neural-Stream-Functions"><a href="#Neural-Stream-Functions" class="headerlink" title="Neural Stream Functions"></a>Neural Stream Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08142">http://arxiv.org/abs/2307.08142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skywolf829/neuralstreamfunction">https://github.com/skywolf829/neuralstreamfunction</a></li>
<li>paper_authors: Skylar Wolfgang Wurster, Hanqi Guo, Tom Peterka, Han-Wei Shen</li>
<li>for: 这个论文是为了计算流函数的，流函数是一个scalar函数，其梯度与给定的vector field垂直。</li>
<li>methods: 这个论文使用神经网络方法来学习流函数，输入是vector field，神经网络会学习将输入坐标映射到流函数值上。</li>
<li>results: 这个论文的结果表明，使用神经网络方法可以高效地计算流函数，并且可以根据输入vector field的不同来生成不同的流函数解。此外，论文还提出了一些可选的约束来生成流函数解，以便在流场的拟合中提高计算的精度。<details>
<summary>Abstract</summary>
We present a neural network approach to compute stream functions, which are scalar functions with gradients orthogonal to a given vector field. As a result, isosurfaces of the stream function extract stream surfaces, which can be visualized to analyze flow features. Our approach takes a vector field as input and trains an implicit neural representation to learn a stream function for that vector field. The network learns to map input coordinates to a stream function value by minimizing the inner product of the gradient of the neural network's output and the vector field. Since stream function solutions may not be unique, we give optional constraints for the network to learn particular stream functions of interest. Specifically, we introduce regularizing loss functions that can optionally be used to generate stream function solutions whose stream surfaces follow the flow field's curvature, or that can learn a stream function that includes a stream surface passing through a seeding rake. We also discuss considerations for properly visualizing the trained implicit network and extracting artifact-free surfaces. We compare our results with other implicit solutions and present qualitative and quantitative results for several synthetic and simulated vector fields.
</details>
<details>
<summary>摘要</summary>
我们提出了一种神经网络方法来计算流函数，它是一个Scalar函数的梯度垂直于给定的vector field。因此，isoSurface流函数EXTRACT流面，可以用来分析流体特性。我们的方法通过将vector field作为输入，训练一个隐式神经表示来学习一个流函数 для该vector field。神经网络学习将输入坐标映射到流函数值上，通过内积 Inner product的梯度和vector field的梯度来最小化。由于流函数解可能不唯一，我们可以选择性地添加约束来学习特定的流函数解。例如，我们可以添加一个正则化损失函数，使流函数解的流面与流体场的弯曲度相符，或者学习一个流函数解包含一个流面通过种子排 sowing rake。我们还讨论了可视化训练后的隐式网络和EXTRACT artifact-free 流面的注意事项。我们与其他隐式解相比较，并对一些Synthetic和Simulated vector field进行了质量和量化的比较结果。
</details></li>
</ul>
<hr>
<h2 id="Adaptively-Placed-Multi-Grid-Scene-Representation-Networks-for-Large-Scale-Data-Visualization"><a href="#Adaptively-Placed-Multi-Grid-Scene-Representation-Networks-for-Large-Scale-Data-Visualization" class="headerlink" title="Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization"></a>Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02494">http://arxiv.org/abs/2308.02494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skywolf829/apmgsrn">https://github.com/skywolf829/apmgsrn</a></li>
<li>paper_authors: Skylar Wolfgang Wurster, Tianyu Xiong, Han-Wei Shen, Hanqi Guo, Tom Peterka<br>for: 这篇论文是为了提出一种适应性的Scene Representation Networks（SRN），以便更好地压缩和可视化科学数据。methods: 该论文使用了多个空间自适应特征网格（APMGSRN），并提出了域分解训练和推理技术，以加速多GPU系统上的训练。results: 该论文提出的APMGSRN架构可以在不需要昂贵的八个树叶树、剪枝和搜索的情况下，提高SRN的重建精度。此外，论文还提供了一个开源的神经网络volume渲染应用程序，可以与任何PyTorch基于SRN进行插件式渲染。<details>
<summary>Abstract</summary>
Scene representation networks (SRNs) have been recently proposed for compression and visualization of scientific data. However, state-of-the-art SRNs do not adapt the allocation of available network parameters to the complex features found in scientific data, leading to a loss in reconstruction quality. We address this shortcoming with an adaptively placed multi-grid SRN (APMGSRN) and propose a domain decomposition training and inference technique for accelerated parallel training on multi-GPU systems. We also release an open-source neural volume rendering application that allows plug-and-play rendering with any PyTorch-based SRN. Our proposed APMGSRN architecture uses multiple spatially adaptive feature grids that learn where to be placed within the domain to dynamically allocate more neural network resources where error is high in the volume, improving state-of-the-art reconstruction accuracy of SRNs for scientific data without requiring expensive octree refining, pruning, and traversal like previous adaptive models. In our domain decomposition approach for representing large-scale data, we train an set of APMGSRNs in parallel on separate bricks of the volume to reduce training time while avoiding overhead necessary for an out-of-core solution for volumes too large to fit in GPU memory. After training, the lightweight SRNs are used for realtime neural volume rendering in our open-source renderer, where arbitrary view angles and transfer functions can be explored. A copy of this paper, all code, all models used in our experiments, and all supplemental materials and videos are available at https://github.com/skywolf829/APMGSRN.
</details>
<details>
<summary>摘要</summary>
Scene representation networks (SRNs) 有最近提出用于压缩和可视化科学数据的方法。然而，当前的 SRNs 不会根据科学数据中复杂的特征进行分配可用的网络参数，导致重建质量下降。我们解决这个缺点，提出了适应地位的多格rid SRN (APMGSRN) 和多 GPU 系统上加速并行训练的领域分解训练和推理技术。我们还发布了基于 PyTorch 的开源神经体积渲染应用程序，允许任意的 PyTorch-based SRN 插件式渲染。我们的提议的 APMGSRN 架构使用多个空间自适应特征网格，学习在域内的位置，以动态分配更多神经网络资源，以提高 SRNs 的重建精度。在我们的域ode decomposition 方法中，我们在分解大规模数据时使用多个独立的块训练 APMGSRN，以降低训练时间，而不需要费时的 Octree 修正、剪辑和搜索。之后，我们使用轻量级 SRN 进行实时神经体积渲染，并且支持任意的视角和转换函数。详细的报告、所有代码、所有在实验中使用的模型、以及所有补充材料和视频都可以在 https://github.com/skywolf829/APMGSRN 上获取。
</details></li>
</ul>
<hr>
<h2 id="GastroVision-A-Multi-class-Endoscopy-Image-Dataset-for-Computer-Aided-Gastrointestinal-Disease-Detection"><a href="#GastroVision-A-Multi-class-Endoscopy-Image-Dataset-for-Computer-Aided-Gastrointestinal-Disease-Detection" class="headerlink" title="GastroVision: A Multi-class Endoscopy Image Dataset for Computer Aided Gastrointestinal Disease Detection"></a>GastroVision: A Multi-class Endoscopy Image Dataset for Computer Aided Gastrointestinal Disease Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08140">http://arxiv.org/abs/2307.08140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/debeshjha/gastrovision">https://github.com/debeshjha/gastrovision</a></li>
<li>paper_authors: Debesh Jha, Vanshali Sharma, Neethi Dasu, Nikhil Kumar Tomar, Steven Hicks, M. K. Bhuyan, Pradip K. Das, Michael A. Riegler, Pål Halvorsen, Ulas Bagci, Thomas de Lange</li>
<li>For: 这个研究旨在提供一个大规模、精确标注的胃肠内镜数据集，以便用于胃肠疾病检测和分类的人工智能（AI）系统开发。* Methods: 这个研究使用了多中心开放存取的胃肠内镜数据集，包括不同的生物学特征、疾病症状、肿瘤除去 caso和正常发现（总共27个类别）。数据集包含8,000幅从挪威巴鲁姆医院和瑞典卡罗琳斯卡大学医院所取得的胃肠内镜图像，并由经验丰富的胃肠镜诊师进行了标注和验证。* Results: 研究人员验证了数据集的重要性，使用了具有普遍受欢迎的深度学习基eline模型进行了广泛的比较。他们认为这个数据集可以促进胃肠疾病检测和分类的AI系统开发。<details>
<summary>Abstract</summary>
Integrating real-time artificial intelligence (AI) systems in clinical practices faces challenges such as scalability and acceptance. These challenges include data availability, biased outcomes, data quality, lack of transparency, and underperformance on unseen datasets from different distributions. The scarcity of large-scale, precisely labeled, and diverse datasets are the major challenge for clinical integration. This scarcity is also due to the legal restrictions and extensive manual efforts required for accurate annotations from clinicians. To address these challenges, we present \textit{GastroVision}, a multi-center open-access gastrointestinal (GI) endoscopy dataset that includes different anatomical landmarks, pathological abnormalities, polyp removal cases and normal findings (a total of 27 classes) from the GI tract. The dataset comprises 8,000 images acquired from B{\ae}rum Hospital in Norway and Karolinska University Hospital in Sweden and was annotated and verified by experienced GI endoscopists. Furthermore, we validate the significance of our dataset with extensive benchmarking based on the popular deep learning based baseline models. We believe our dataset can facilitate the development of AI-based algorithms for GI disease detection and classification. Our dataset is available at \url{https://osf.io/84e7f/}.
</details>
<details>
<summary>摘要</summary>
临床应用人工智能（AI）系统的整合面临着扩展性和接受性的挑战。这些挑战包括数据可用性、偏见结果、数据质量、不透明度和不同分布下的表现不佳。lack of large-scale, precisely labeled, and diverse datasets is the major challenge for clinical integration. This scarcity is also due to the legal restrictions and extensive manual efforts required for accurate annotations from clinicians. To address these challenges, we present \textit{GastroVision}, a multi-center open-access gastrointestinal (GI) endoscopy dataset that includes different anatomical landmarks, pathological abnormalities, polyp removal cases and normal findings (a total of 27 classes) from the GI tract. The dataset comprises 8,000 images acquired from B{\ae}rum Hospital in Norway and Karolinska University Hospital in Sweden and was annotated and verified by experienced GI endoscopists. Furthermore, we validate the significance of our dataset with extensive benchmarking based on the popular deep learning based baseline models. We believe our dataset can facilitate the development of AI-based algorithms for GI disease detection and classification. Our dataset is available at \url{https://osf.io/84e7f/}.Here's the word-for-word translation of the text into Simplified Chinese:临床应用人工智能（AI）系统的整合面临着扩展性和接受性的挑战。这些挑战包括数据可用性、偏见结果、数据质量、不透明度和不同分布下的表现不佳。lack of large-scale, precisely labeled, and diverse datasets is the major challenge for clinical integration. This scarcity is also due to the legal restrictions and extensive manual efforts required for accurate annotations from clinicians. To address these challenges, we present \textit{GastroVision}, a multi-center open-access gastrointestinal (GI) endoscopy dataset that includes different anatomical landmarks, pathological abnormalities, polyp removal cases and normal findings (a total of 27 classes) from the GI tract. The dataset comprises 8,000 images acquired from B{\ae}rum Hospital in Norway and Karolinska University Hospital in Sweden and was annotated and verified by experienced GI endoscopists. Furthermore, we validate the significance of our dataset with extensive benchmarking based on the popular deep learning based baseline models. We believe our dataset can facilitate the development of AI-based algorithms for GI disease detection and classification. Our dataset is available at \url{https://osf.io/84e7f/}.
</details></li>
</ul>
<hr>
<h2 id="Solving-Inverse-Problems-with-Latent-Diffusion-Models-via-Hard-Data-Consistency"><a href="#Solving-Inverse-Problems-with-Latent-Diffusion-Models-via-Hard-Data-Consistency" class="headerlink" title="Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency"></a>Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08123">http://arxiv.org/abs/2307.08123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, Liyue Shen</li>
<li>for: 解决 inverse problems 的泛化问题</li>
<li>methods: 使用 pre-trained latent diffusion models 和 hard data consistency 技术</li>
<li>results: 可以 reconstruction high-quality images, 比如 Linear and non-linear inverse problems 的解决方案<details>
<summary>Abstract</summary>
Diffusion models have recently emerged as powerful generative priors for solving inverse problems. However, training diffusion models in the pixel space are both data intensive and computationally demanding, which restricts their applicability as priors in domains such as medical imaging. Latent diffusion models, which operate in a much lower-dimensional space, offer a solution to these challenges. Though, their direct application to solving inverse problems remains an unsolved technical challenge due to the nonlinearity of the encoder and decoder. To address this issue,we propose ReSample, an algorithm that solves general inverse problems with pre-trained latent diffusion models. Our algorithm incorporates data consistency by solving an optimization problem during the reverse sampling process, a concept that we term as hard data consistency. Upon solving this optimization problem, we propose a novel resampling scheme to map the measurement-consistent sample back onto the correct data manifold. Our approach offers both memory efficiency and considerable flexibility in the sense that (1) it can be readily adapted to various inverse problems using the same pre-trained model as it does not assume any fixed forward measurement operator during training, and (2) it can be generalized to different domains by simply fine-tuning the latent diffusion model with a minimal amount of data samples. Our empirical results on both linear and non-linear inverse problems demonstrate that our approach can reconstruct high-quality images even compared to state-of-the-art works that operate in the pixel space.
</details>
<details>
<summary>摘要</summary>
Diffusion models 已经在 solves inverse problems 中出现为强大的生成假设。然而，在像素空间中训练 diffusion models 是数据充足和计算挑战性的，这限制了它们在医学成像等领域的应用。 latent diffusion models 可以解决这些挑战，它们在低维度空间中运行。然而，它们的直接应用于解决 inverse problems 还是技术挑战，因为推导器和解码器都是非线性的。为解决这个问题，我们提出了 ReSample 算法，它可以解决一般的 inverse problems with pre-trained latent diffusion models。我们的算法包括数据一致性，通过解决反推问题时的优化问题，我们称之为 hard data consistency。在解决这个优化问题后，我们提出了一种新的抽样方案，用于将 measurement-consistent sample 映射回正确的数据拟合。我们的方法具有内存效率和可变性，它可以轻松适应不同的 inverse problems，并且可以通过简单地微调 latent diffusion model 来适应不同的领域。我们的实验结果表明，我们的方法可以重建高质量的图像，甚至比针对像素空间进行训练的现状技术更高效。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalisation-with-Bidirectional-Encoder-Representations-from-Vision-Transformers"><a href="#Domain-Generalisation-with-Bidirectional-Encoder-Representations-from-Vision-Transformers" class="headerlink" title="Domain Generalisation with Bidirectional Encoder Representations from Vision Transformers"></a>Domain Generalisation with Bidirectional Encoder Representations from Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08117">http://arxiv.org/abs/2307.08117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sw-packages/d23c4b6afa05094a23071333bd230aceceec08117355003f5c0ea958e60c9c98">https://github.com/sw-packages/d23c4b6afa05094a23071333bd230aceceec08117355003f5c0ea958e60c9c98</a></li>
<li>paper_authors: Hamza Riaz, Alan F. Smeaton</li>
<li>for: 这篇论文旨在应用领域普遍化（Domain Generalization）技术，将知识从来源领域（Source Domain）转移到未见领域（Target Domain），以扩展深度学习模型的应用范围。</li>
<li>methods: 这篇论文使用了视觉对映器（Vision Transformer）进行领域普遍化，并评估了四种不同的视觉对映器架构（ViT、LeViT、DeiT、BEIT）在对于不同的资料分布进行测试。</li>
<li>results: 根据结果显示，使用了bidirectional encoder representation from image transformers（BEIT）架构，在三个benchmark（PACS、Home-Office、DomainNet）上实现了显著的验证和测试准确率改善，并且在对于未见领域的测试中具有较好的表现。<details>
<summary>Abstract</summary>
Domain generalisation involves pooling knowledge from source domain(s) into a single model that can generalise to unseen target domain(s). Recent research in domain generalisation has faced challenges when using deep learning models as they interact with data distributions which differ from those they are trained on. Here we perform domain generalisation on out-of-distribution (OOD) vision benchmarks using vision transformers. Initially we examine four vision transformer architectures namely ViT, LeViT, DeiT, and BEIT on out-of-distribution data. As the bidirectional encoder representation from image transformers (BEIT) architecture performs best, we use it in further experiments on three benchmarks PACS, Home-Office and DomainNet. Our results show significant improvements in validation and test accuracy and our implementation significantly overcomes gaps between within-distribution and OOD data.
</details>
<details>
<summary>摘要</summary>
域�总结是将来源域的知识汇集到一个可以总结目标域的模型中。Recent research in domain generalization has faced challenges when using deep learning models as they interact with data distributions that differ from those they are trained on. Here we perform domain generalization on out-of-distribution (OOD) vision benchmarks using vision transformers. Initially we examine four vision transformer architectures, namely ViT, LeViT, DeiT, and BEIT, on out-of-distribution data. As the bidirectional encoder representation from image transformers (BEIT) architecture performs best, we use it in further experiments on three benchmarks PACS, Home-Office, and DomainNet. Our results show significant improvements in validation and test accuracy, and our implementation significantly overcomes gaps between within-distribution and OOD data.
</details></li>
</ul>
<hr>
<h2 id="Polarization-Multi-Image-Synthesis-with-Birefringent-Metasurfaces"><a href="#Polarization-Multi-Image-Synthesis-with-Birefringent-Metasurfaces" class="headerlink" title="Polarization Multi-Image Synthesis with Birefringent Metasurfaces"></a>Polarization Multi-Image Synthesis with Birefringent Metasurfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08106">http://arxiv.org/abs/2307.08106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deanhazineh/multi-image-synthesis">https://github.com/deanhazineh/multi-image-synthesis</a></li>
<li>paper_authors: Dean Hazineh, Soon Wei Daniel Lim, Qi Guo, Federico Capasso, Todd Zickler</li>
<li>For:	+ The paper is written for the task of incoherent opto-electronic filtering, which is a new application of optical metasurfaces in computational imaging systems.	+ The paper aims to demonstrate a new system that uses a birefringent metasurface with a polarizer-mosaicked photosensor to capture four optically-coded measurements in a single exposure.* Methods:	+ The paper uses a birefringent metasurface with a polarizer-mosaicked photosensor to capture four optically-coded measurements in a single exposure.	+ The paper introduces a new form of gradient descent with a novel regularizer that encourages light efficiency and a high signal-to-noise ratio to find a metasurface that can realize a set of user-specified spatial filters.* Results:	+ The paper demonstrates several examples in simulation and with fabricated prototypes, including some with spatial filters that have prescribed variations with respect to depth and wavelength.Here is the answer in Simplified Chinese text:* For:	+ 这篇论文是为了实现不同的空间滤波器而设计的，这是计算成像系统中新的应用。	+ 论文描述了一种使用具有 polarizer-mosaicked photosensor 的折射率元件，在单次曝光中捕获四个光学编码量。* Methods:	+ 论文使用具有折射率元件和 polarizer-mosaicked photosensor 的新方法来捕获四个光学编码量。	+ 论文引入了一种新的梯度下降算法，该算法通过鼓励光效率和信号噪声比来找到一个实现用户指定的空间滤波器的元件。* Results:	+ 论文通过实验和制造的证明，展示了一些实现了用户指定的空间滤波器的例子，包括一些具有不同深度和波长的滤波器。<details>
<summary>Abstract</summary>
Optical metasurfaces composed of precisely engineered nanostructures have gained significant attention for their ability to manipulate light and implement distinct functionalities based on the properties of the incident field. Computational imaging systems have started harnessing this capability to produce sets of coded measurements that benefit certain tasks when paired with digital post-processing. Inspired by these works, we introduce a new system that uses a birefringent metasurface with a polarizer-mosaicked photosensor to capture four optically-coded measurements in a single exposure. We apply this system to the task of incoherent opto-electronic filtering, where digital spatial-filtering operations are replaced by simpler, per-pixel sums across the four polarization channels, independent of the spatial filter size. In contrast to previous work on incoherent opto-electronic filtering that can realize only one spatial filter, our approach can realize a continuous family of filters from a single capture, with filters being selected from the family by adjusting the post-capture digital summation weights. To find a metasurface that can realize a set of user-specified spatial filters, we introduce a form of gradient descent with a novel regularizer that encourages light efficiency and a high signal-to-noise ratio. We demonstrate several examples in simulation and with fabricated prototypes, including some with spatial filters that have prescribed variations with respect to depth and wavelength.   Visit the Project Page at https://deanhazineh.github.io/publications/Multi_Image_Synthesis/MIS_Home.html
</details>
<details>
<summary>摘要</summary>
依据精心设计的奈米结构，光学元面 composites 已经吸引了广泛关注，因为它们可以 manipulate 光子并实现基于入射场的特性而具有不同的功能性。计算成像系统已经开始利用这种能力生成具有特定任务需求的编码测量集，并通过数字后处理来实现。我们基于这些工作，引入了一种新的系统，使用偏振元面和分割成多个极化通道的探测器来在单个曝光中捕捉四个光学编码测量。我们应用这种系统于不同激光的吸收过滤器任务中，取代了传统的数字空间滤波操作，并且可以实现连续的家族filters，从单个捕捉中选择filters。为找到实现用户指定的空间滤波的元面，我们引入了一种新的迭代 descent 算法，其中包含了光效率和信号噪声比的新正则化项。我们在 simulate 和实验中示出了许多示例，包括一些具有深度和波长的特定变化的空间滤波。更多信息请参考 <https://deanhazineh.github.io/publications/Multi_Image_Synthesis/MIS_Home.html>
</details></li>
</ul>
<hr>
<h2 id="FourierHandFlow-Neural-4D-Hand-Representation-Using-Fourier-Query-Flow"><a href="#FourierHandFlow-Neural-4D-Hand-Representation-Using-Fourier-Query-Flow" class="headerlink" title="FourierHandFlow: Neural 4D Hand Representation Using Fourier Query Flow"></a>FourierHandFlow: Neural 4D Hand Representation Using Fourier Query Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08100">http://arxiv.org/abs/2307.08100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihyun Lee, Junbong Jang, Donghwan Kim, Minhyuk Sung, Tae-Kyun Kim</li>
<li>for: 本研究旨在学习RGB视频中的人手四维形态，以实现高效精准的人手重建和动作估计。</li>
<li>methods: 本方法使用Fourier série来表示 Query Flow，并将3D占据场与动作相关的Query Flow组合起来，以实现四维形态的精准重建。</li>
<li>results: 在实验中，本方法可以实现 estado-of-the-art 的Result on video-based 4D reconstruction，同时 computationally more efficient than existing 3D&#x2F;4D implicit shape representations。 Additionally, the learned correspondences of implicit shapes can be used for motion inter- and extrapolation and texture transfer.<details>
<summary>Abstract</summary>
Recent 4D shape representations model continuous temporal evolution of implicit shapes by (1) learning query flows without leveraging shape and articulation priors or (2) decoding shape occupancies separately for each time value. Thus, they do not effectively capture implicit correspondences between articulated shapes or regularize jittery temporal deformations. In this work, we present FourierHandFlow, which is a spatio-temporally continuous representation for human hands that combines a 3D occupancy field with articulation-aware query flows represented as Fourier series. Given an input RGB sequence, we aim to learn a fixed number of Fourier coefficients for each query flow to guarantee smooth and continuous temporal shape dynamics. To effectively model spatio-temporal deformations of articulated hands, we compose our 4D representation based on two types of Fourier query flow: (1) pose flow that models query dynamics influenced by hand articulation changes via implicit linear blend skinning and (2) shape flow that models query-wise displacement flow. In the experiments, our method achieves state-of-the-art results on video-based 4D reconstruction while being computationally more efficient than the existing 3D/4D implicit shape representations. We additionally show our results on motion inter- and extrapolation and texture transfer using the learned correspondences of implicit shapes. To the best of our knowledge, FourierHandFlow is the first neural 4D continuous hand representation learned from RGB videos. The code will be publicly accessible.
</details>
<details>
<summary>摘要</summary>
最近的4D形态表示模型连续时间演化的隐式形态 by (1) 学习无关形态和肢体约束的查询流或 (2) 分解每个时间值的形态占用。因此，它们不能有效地捕捉隐式相关性 между 动体形态或正则化颤动幅。在这种工作中，我们提出了FourierHandFlow，它是一种包含3D占用场和形态相关的查询流 Fourier系列的四维表示。给输入的RGB序列，我们希望学习固定数量的Fourier系数来保证平滑和连续的时间形态动态。为了有效地模型动体形态的空间时间变换，我们将我们的4D表示分为两种类型的查询流：（1）pose流，它模型查询动态受到手肢变化的影响via隐式线性混合皮肤和（2）形态流，它模型查询点 wise的移动流。在实验中，我们的方法实现了视频基于4D重建的状态对齐的结果，而且与现有的3D/4D隐式形态表示更加计算效率。我们还展示了使用学习的隐式形态对应关系进行动作间隔和外部逼近，以及纹理传输。根据我们所知，FourierHandFlow是首次由RGB视频学习的神经网络4D连续手表示。代码将公开访问。
</details></li>
</ul>
<hr>
<h2 id="CalibNet-Dual-branch-Cross-modal-Calibration-for-RGB-D-Salient-Instance-Segmentation"><a href="#CalibNet-Dual-branch-Cross-modal-Calibration-for-RGB-D-Salient-Instance-Segmentation" class="headerlink" title="CalibNet: Dual-branch Cross-modal Calibration for RGB-D Salient Instance Segmentation"></a>CalibNet: Dual-branch Cross-modal Calibration for RGB-D Salient Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08098">http://arxiv.org/abs/2307.08098</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pjlallen/calibnet">https://github.com/pjlallen/calibnet</a></li>
<li>paper_authors: Jialun Pei, Tao Jiang, He Tang, Nian Liu, Yueming Jin, Deng-Ping Fan, Pheng-Ann Heng</li>
<li>for: 这个论文主要针对RGB-D图像中的精度实例分割问题，提出了一种基于双树 Cross-Modal Feature Calibration Architecture（CalibNet）的新方法。</li>
<li>methods: 该方法使用了三个简单模块：动态交互卷积（DIK）、重量共享 fusión（WSF）和深度相似度评估（DSA），这三个模块共同工作以生成有效的实例相关卷积和融合cross-modal特征。</li>
<li>results: 对于三个挑战性评价标准，该方法实现了出色的结果，即COME15K-N测试集上的AP为58.0%，比替代方案更高。<details>
<summary>Abstract</summary>
We propose a novel approach for RGB-D salient instance segmentation using a dual-branch cross-modal feature calibration architecture called CalibNet. Our method simultaneously calibrates depth and RGB features in the kernel and mask branches to generate instance-aware kernels and mask features. CalibNet consists of three simple modules, a dynamic interactive kernel (DIK) and a weight-sharing fusion (WSF), which work together to generate effective instance-aware kernels and integrate cross-modal features. To improve the quality of depth features, we incorporate a depth similarity assessment (DSA) module prior to DIK and WSF. In addition, we further contribute a new DSIS dataset, which contains 1,940 images with elaborate instance-level annotations. Extensive experiments on three challenging benchmarks show that CalibNet yields a promising result, i.e., 58.0% AP with 320*480 input size on the COME15K-N test set, which significantly surpasses the alternative frameworks. Our code and dataset are available at: https://github.com/PJLallen/CalibNet.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的RGB-D突出实例分割方法，基于双极分支交叉模式特征均衡架构，即CalibNet。我们的方法同时均衡了深度和RGB特征在核心和面罩分支中，以生成实例相关的核心和面罩特征。CalibNet由三个简单模块组成：动态互动核心（DIK）、重量共享融合（WSF）以及深度相似评估（DSA）模块。这些模块共同工作，以生成有效的实例相关核心和融合交叉特征。此外，我们还提供了一个新的DSIS数据集，包含1940张图像，每张图像均有详细的实例级别注解。我们的实验表明，CalibNet在三个挑战性的benchmark上实现了优秀的结果，即COME15K-N测试集上的58.0% AP值，与其他框架相比有显著提高。我们的代码和数据集可以在：https://github.com/PJLallen/CalibNet上获取。
</details></li>
</ul>
<hr>
<h2 id="Semi-DETR-Semi-Supervised-Object-Detection-with-Detection-Transformers"><a href="#Semi-DETR-Semi-Supervised-Object-Detection-with-Detection-Transformers" class="headerlink" title="Semi-DETR: Semi-Supervised Object Detection with Detection Transformers"></a>Semi-DETR: Semi-Supervised Object Detection with Detection Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08095">http://arxiv.org/abs/2307.08095</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/semi_det/semi_detr">https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/semi_det/semi_detr</a></li>
<li>paper_authors: Jiacheng Zhang, Xiangru Lin, Wei Zhang, Kuo Wang, Xiao Tan, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li</li>
<li>for:  semi-supervised object detection (SSOD)</li>
<li>methods:  DETR-based framework with Stage-wise Hybrid Matching strategy and Crossview Query Consistency method</li>
<li>results:  outperforms all state-of-the-art methods by clear margins on all SSOD settings of both COCO and Pascal VOC benchmark datasets.Here is the Chinese translation of the three key information points:</li>
<li>for:  semi-supervised物体检测 (SSOD)</li>
<li>methods:  DETR基于的框架，具有Stage-wise Hybrid Matching策略和 Crossview Query Consistency方法</li>
<li>results: 在所有 SSOD 设定下，包括 COCO 和 Pascal VOC 数据集的所有 benchmark 数据集上，与所有现有方法均以明显的差距超越。<details>
<summary>Abstract</summary>
We analyze the DETR-based framework on semi-supervised object detection (SSOD) and observe that (1) the one-to-one assignment strategy generates incorrect matching when the pseudo ground-truth bounding box is inaccurate, leading to training inefficiency; (2) DETR-based detectors lack deterministic correspondence between the input query and its prediction output, which hinders the applicability of the consistency-based regularization widely used in current SSOD methods. We present Semi-DETR, the first transformer-based end-to-end semi-supervised object detector, to tackle these problems. Specifically, we propose a Stage-wise Hybrid Matching strategy that combines the one-to-many assignment and one-to-one assignment strategies to improve the training efficiency of the first stage and thus provide high-quality pseudo labels for the training of the second stage. Besides, we introduce a Crossview Query Consistency method to learn the semantic feature invariance of object queries from different views while avoiding the need to find deterministic query correspondence. Furthermore, we propose a Cost-based Pseudo Label Mining module to dynamically mine more pseudo boxes based on the matching cost of pseudo ground truth bounding boxes for consistency training. Extensive experiments on all SSOD settings of both COCO and Pascal VOC benchmark datasets show that our Semi-DETR method outperforms all state-of-the-art methods by clear margins. The PaddlePaddle version code1 is at https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/semi_det/semi_detr.
</details>
<details>
<summary>摘要</summary>
我们分析基于DETR的框架在半指导下的物体检测（SSOD）中，发现了两个问题：（1）一对一对应策略可能导致训练不精确，因为伪真的 bounding box 精度不高；（2）基于DETR的检测器缺乏对输入查询与其预测输出之间的决定性对匹配，这限制了现有的一般SSOD方法中的一致性基础训练的应用。我们提出了半DETR，第一个基于transformer的端到端半指导物体检测器，以解决这些问题。具体来说，我们提出了阶段匹配策略，让一个查询与多个预测结果之间进行匹配，从而提高了训练的效率，并且为第二阶段的训练提供高质量的伪标签。此外，我们引入了跨观查询内容一致性方法，以学习查询从不同观点的物体特征内在性，而不需要寻找决定性的查询匹配。最后，我们提出了一个基于成本的伪标签采矿模组，以动态地采矿更多的伪标签，以便实现一致性训练。实验结果显示，我们的半DETR方法在所有SSOD设定下，都比所有现有方法优化了明显。PaddlePaddle版本代码可以在以下链接中找到：https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/semi_det/semi_detr。
</details></li>
</ul>
<hr>
<h2 id="Cross-Ray-Neural-Radiance-Fields-for-Novel-view-Synthesis-from-Unconstrained-Image-Collections"><a href="#Cross-Ray-Neural-Radiance-Fields-for-Novel-view-Synthesis-from-Unconstrained-Image-Collections" class="headerlink" title="Cross-Ray Neural Radiance Fields for Novel-view Synthesis from Unconstrained Image Collections"></a>Cross-Ray Neural Radiance Fields for Novel-view Synthesis from Unconstrained Image Collections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08093">http://arxiv.org/abs/2307.08093</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yifyang993/cr-nerf-pytorch">https://github.com/yifyang993/cr-nerf-pytorch</a></li>
<li>paper_authors: Yifan Yang, Shuhai Zhang, Zixiong Huang, Yubing Zhang, Mingkui Tan</li>
<li>for: 用于Synthesizing occlusion-free novel views from unconstrained image collections, addressing challenges such as dynamic changes in appearance and transient objects.</li>
<li>methods: 使用Cross-Ray NeRF (CR-NeRF)方法，利用多个ray的交互信息来模型变化的外观，并通过图像特征covariance和图像外观的统计方式来recover外观。此外，还提出了适应物体排除和网格采样策略来避免 occlusion 问题。</li>
<li>results: 经过广泛的实验 validate CR-NeRF 的有效性，能够Synthesize high-quality novel views with the same appearances as the input images, even in the presence of dynamic changes and transient objects.<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRF) is a revolutionary approach for rendering scenes by sampling a single ray per pixel and it has demonstrated impressive capabilities in novel-view synthesis from static scene images. However, in practice, we usually need to recover NeRF from unconstrained image collections, which poses two challenges: 1) the images often have dynamic changes in appearance because of different capturing time and camera settings; 2) the images may contain transient objects such as humans and cars, leading to occlusion and ghosting artifacts. Conventional approaches seek to address these challenges by locally utilizing a single ray to synthesize a color of a pixel. In contrast, humans typically perceive appearance and objects by globally utilizing information across multiple pixels. To mimic the perception process of humans, in this paper, we propose Cross-Ray NeRF (CR-NeRF) that leverages interactive information across multiple rays to synthesize occlusion-free novel views with the same appearances as the images. Specifically, to model varying appearances, we first propose to represent multiple rays with a novel cross-ray feature and then recover the appearance by fusing global statistics, i.e., feature covariance of the rays and the image appearance. Moreover, to avoid occlusion introduced by transient objects, we propose a transient objects handler and introduce a grid sampling strategy for masking out the transient objects. We theoretically find that leveraging correlation across multiple rays promotes capturing more global information. Moreover, extensive experimental results on large real-world datasets verify the effectiveness of CR-NeRF.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (nerf) 是一种革命性的方法，通过每个像素只采样一个光线来渲染场景，并在不同视图 synthesis 中显示出优异的能力。然而，在实际应用中，我们通常需要从无结构图像集中恢复 nerf，这两个挑战：1）图像经常具有不同拍摄时间和摄像机设置导致的变化的外观; 2）图像可能包含过渡性的对象，如人和车辆，导致干扰和幻影 artifacts。传统的方法通过地方使用单个光线来synthesize 一个像素的颜色来解决这些挑战。与人类的感知过程不同，我们在这篇论文中提出了跨光线 nerf (cr-nerf)，它利用多个光线之间的互动信息来synthesize 干扰和幻影 artifacts 自由的新视图，同时保持与图像的外观一致。为了模型不同的外观，我们首先提出了一种新的跨光线特征表示，然后通过拼接全局统计，即光线特征相关矩阵和图像外观，来回归出现在图像中的外观。此外，我们还提出了一种过渡性对象处理器，并引入了网格采样策略，以masking 出过渡性对象。我们理论上发现，通过多个光线之间的互动信息，可以更好地捕捉全局信息。此外，我们在大量实际数据上进行了广泛的实验，并证明了cr-nerf的效果。
</details></li>
</ul>
<hr>
<h2 id="Gait-Data-Augmentation-using-Physics-Based-Biomechanical-Simulation"><a href="#Gait-Data-Augmentation-using-Physics-Based-Biomechanical-Simulation" class="headerlink" title="Gait Data Augmentation using Physics-Based Biomechanical Simulation"></a>Gait Data Augmentation using Physics-Based Biomechanical Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08092">http://arxiv.org/abs/2307.08092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mritula Chandrasekaran, Jarek Francik, Dimitrios Makris</li>
<li>for: Addressing the problem of data scarcity for gait analysis</li>
<li>methods: Using OpenSIM, a physics-based simulator, to synthesize biomechanically plausible walking sequences for gait data augmentation</li>
<li>results: Improved performance of model-based gait classifiers and state-of-the-art results for gait-based person identification with an accuracy of up to 96.11% on the CASIA-B dataset.Here’s the full Chinese text:</li>
<li>for:  solves the problem of gait analysis data scarcity</li>
<li>methods: 使用OpenSIM，一个基于物理的 simulator，Synthesize biomechanically plausible walking sequences for gait data augmentation</li>
<li>results: 提高基于模型的步行分类器的性能，并在CASIA-B dataset上实现了人体步行识别的最佳结果，准确率高达96.11%。<details>
<summary>Abstract</summary>
This paper focuses on addressing the problem of data scarcity for gait analysis. Standard augmentation methods may produce gait sequences that are not consistent with the biomechanical constraints of human walking. To address this issue, we propose a novel framework for gait data augmentation by using OpenSIM, a physics-based simulator, to synthesize biomechanically plausible walking sequences. The proposed approach is validated by augmenting the WBDS and CASIA-B datasets and then training gait-based classifiers for 3D gender gait classification and 2D gait person identification respectively. Experimental results indicate that our augmentation approach can improve the performance of model-based gait classifiers and deliver state-of-the-art results for gait-based person identification with an accuracy of up to 96.11% on the CASIA-B dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Untrained-neural-network-embedded-Fourier-phase-retrieval-from-few-measurements"><a href="#Untrained-neural-network-embedded-Fourier-phase-retrieval-from-few-measurements" class="headerlink" title="Untrained neural network embedded Fourier phase retrieval from few measurements"></a>Untrained neural network embedded Fourier phase retrieval from few measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08717">http://arxiv.org/abs/2307.08717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyuan-2000/trad">https://github.com/liyuan-2000/trad</a></li>
<li>paper_authors: Liyuan Ma, Hongxia Wang, Ningyi Leng, Ziyang Yuan</li>
<li>for: 这篇论文目的是解决快速干扰分析（FPR）问题，即从傅里叶频谱测量获得未知信号的重建问题。</li>
<li>methods: 该论文提出了一种基于替换方向方法的多分量加速器（ADMM）框架的无学习神经网络（NN）嵌入算法，用于解决FPR问题。该算法使用生成网络来表示要重建的图像，从而限制图像在网络结构中的空间。此外，该算法还添加了总变量（TV）正则化，以便更好地恢复图像中的本地结构。</li>
<li>results: 实验结果表明，提出的算法在计算资源更少的情况下，能够超越现有的无学习NN基于算法，甚至与有学习NN基于算法相比表现竞争力强。<details>
<summary>Abstract</summary>
Fourier phase retrieval (FPR) is a challenging task widely used in various applications. It involves recovering an unknown signal from its Fourier phaseless measurements. FPR with few measurements is important for reducing time and hardware costs, but it suffers from serious ill-posedness. Recently, untrained neural networks have offered new approaches by introducing learned priors to alleviate the ill-posedness without requiring any external data. However, they may not be ideal for reconstructing fine details in images and can be computationally expensive. This paper proposes an untrained neural network (NN) embedded algorithm based on the alternating direction method of multipliers (ADMM) framework to solve FPR with few measurements. Specifically, we use a generative network to represent the image to be recovered, which confines the image to the space defined by the network structure. To improve the ability to represent high-frequency information, total variation (TV) regularization is imposed to facilitate the recovery of local structures in the image. Furthermore, to reduce the computational cost mainly caused by the parameter updates of the untrained NN, we develop an accelerated algorithm that adaptively trades off between explicit and implicit regularization. Experimental results indicate that the proposed algorithm outperforms existing untrained NN-based algorithms with fewer computational resources and even performs competitively against trained NN-based algorithms.
</details>
<details>
<summary>摘要</summary>
傅里叶阶段恢复（FPR）是一个广泛应用的挑战任务，它的目标是从傅里叶无法测量中恢复未知的信号。FPR WITH few measurements 是一个重要的应用，可以降低时间和硬件成本，但它受到严重的非uniqueness问题的困扰。最近，未训练的神经网络（NN）已经提供了新的approaches，它们通过引入学习的约束来缓解非uniqueness问题，不需要任何外部数据。然而，它们可能不适合重建图像中的细节。这篇论文提出了一种基于 alternating direction method of multipliers（ADMM）框架的未训练NN算法来解决FPR WITH few measurements。 Specifically, we use a generative network to represent the image to be recovered, which confines the image to the space defined by the network structure。 To improve the ability to represent high-frequency information, total variation（TV）正则化是应用于促进图像中的地方结构的恢复。更over, to reduce the computational cost mainly caused by the parameter updates of the untrained NN, we develop an accelerated algorithm that adaptively trades off between explicit and implicit regularization。实验结果表明，提出的算法在计算资源更少的情况下表现出了更好的性能，甚至与训练NN-based algorithm相当竞争。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/17/cs.CV_2023_07_17/" data-id="cloimip8900egs488fwzlenrl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/17/cs.AI_2023_07_17/" class="article-date">
  <time datetime="2023-07-17T12:00:00.000Z" itemprop="datePublished">2023-07-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/cs.AI_2023_07_17/">cs.AI - 2023-07-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="BuboGPT-Enabling-Visual-Grounding-in-Multi-Modal-LLMs"><a href="#BuboGPT-Enabling-Visual-Grounding-in-Multi-Modal-LLMs" class="headerlink" title="BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"></a>BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08581">http://arxiv.org/abs/2307.08581</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/magic-research/bubogpt">https://github.com/magic-research/bubogpt</a></li>
<li>paper_authors: Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, Bingyi Kang</li>
<li>for: 这paper的目的是提出一种多模态LLM，可以在语言、视觉和声音三种模式之间进行交互，并提供细化的对象位置理解。</li>
<li>methods: 这paper使用了一种基于SAM的视觉定位模块，以及一种两个阶段训练方案和指令集来授命多模态理解。</li>
<li>results: 实验表明，BuboGPT在与人类交互时表现出了卓越的多模态理解和视觉定位能力，并在不同的模式组合（有Alignment和无Alignment）下表现consistently well。<details>
<summary>Abstract</summary>
LLMs have demonstrated remarkable abilities at interacting with humans through language, especially with the usage of instruction-following data. Recent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further enlarge their abilities by incorporating multi-modal inputs, including image, video, and speech. Despite their effectiveness at generating precise and detailed language understanding of the given modality signal, these LLMs give up the ability to ground specific parts of inputs, thus only constructing a coarse-grained mapping. However, explicit and informative correspondence between text and other modalities will not only improve the user experience but also help to expand the application scenario of multi-modal LLMs. Therefore, we propose BuboGPT, a multi-modal LLM with visual grounding that can perform cross-modal interaction between vision, audio and language, providing fine-grained understanding of visual objects and other given modalities. As a result, BuboGPT is able to point out the specific location of an object in the image, when it is generating response or description for that object. Our contributions are two-fold: 1) An off-the-shelf visual grounding module based on SAM that extracts entities in a sentence and find corresponding masks in the image. 2) A two-stage training scheme and instruction dataset to endow joint text-image-audio understanding. Our experiments show that BuboGPT achieves impressive multi-modality understanding and visual grounding abilities during the interaction with human. It performs consistently well when provided by arbitrary modality combinations (either aligned or unaligned). Our code, model and dataset are available at https://bubo-gpt.github.io .
</details>
<details>
<summary>摘要</summary>
LLMs 已经表现出了与人类语言交互的异常出色能力，特别是在使用指令数据时。最新的 LLMs，如 MiniGPT-4、LLaVA 和 X-LLM，进一步扩展了它们的能力，通过包括图像、视频和语音多种输入模式。尽管这些 LLMs 可以生成精准和详细的语言理解输入信号，但它们失去了对特定输入部分的地址映射的能力，因此只能建立粗糙的映射。然而，显式和有用的输入模式与文本之间的对应关系不仅会提高用户体验，还可以扩展多模态 LLMs 的应用场景。因此，我们提出了 BuboGPT，一种具有视觉定位的多模态 LLM，可以在视觉、语音和文本之间进行跨模态交互，提供细化的视觉对象理解和其他输入模式的精准地址映射。因此，BuboGPT 能够在生成响应或描述某个对象时指出该对象在图像中的具体位置。我们的贡献包括：1. 基于 SAM 的Visual Grounding Module，可以从 sentence 中提取实体并在图像中找到匹配的面征。2. 一种两stage 训练方案和指令数据集，以激活joint text-image-audio理解。我们的实验表明，BuboGPT 在与人类交互时表现出了很好的多模态理解和视觉定位能力，能够在不同的模式组合（可能是对齐或不对齐）下表现稳定。我们的代码、模型和数据集可以在 <https://bubo-gpt.github.io> 获取。
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Processing-with-Linear-Optics"><a href="#Nonlinear-Processing-with-Linear-Optics" class="headerlink" title="Nonlinear Processing with Linear Optics"></a>Nonlinear Processing with Linear Optics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08533">http://arxiv.org/abs/2307.08533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustafa Yildirim, Niyazi Ulas Dinc, Ilker Oguz, Demetri Psaltis, Christophe Moser</li>
<li>for: 这项研究旨在提高神经网络的能效性和速度，通过利用光学实现多层神经网络，而不需要低功率光学非线性元件。</li>
<li>methods: 该研究提出了一种新的框架，通过多散射实现程序可编程的线性和非线性变换，并且可以在低光力率下实现非线性光学计算。</li>
<li>results: 理论和实验研究显示，通过重复数据的散射可以实现低功率连续波光学计算，并且可以同时实现线性和非线性变换。<details>
<summary>Abstract</summary>
Deep neural networks have achieved remarkable breakthroughs by leveraging multiple layers of data processing to extract hidden representations, albeit at the cost of large electronic computing power. To enhance energy efficiency and speed, the optical implementation of neural networks aims to harness the advantages of optical bandwidth and the energy efficiency of optical interconnections. In the absence of low-power optical nonlinearities, the challenge in the implementation of multilayer optical networks lies in realizing multiple optical layers without resorting to electronic components. In this study, we present a novel framework that uses multiple scattering that is capable of synthesizing programmable linear and nonlinear transformations concurrently at low optical power by leveraging the nonlinear relationship between the scattering potential, represented by data, and the scattered field. Theoretical and experimental investigations show that repeating the data by multiple scattering enables non-linear optical computing at low power continuous wave light.
</details>
<details>
<summary>摘要</summary>
深度神经网络已经取得了非常出色的突破，通过多层数据处理来抽取隐藏表示，尽管在电子计算能力上付出了很大的代价。为了提高能效性和速度，光学实现神经网络尝试利用光学带宽和光学连接的能效性。在没有低功率光学非线性的情况下，实现多层光学网络的挑战在于不使用电子组件来实现多层光学层。在这种研究中，我们提出了一种新的框架，使用多散射来实现可编程的线性和非线性变换，并在低光力短波光下实现多散射。理论和实验研究表明，通过多次散射来重复数据，可以实现低功率连续波光学计算。
</details></li>
</ul>
<hr>
<h2 id="LuckyMera-a-Modular-AI-Framework-for-Building-Hybrid-NetHack-Agents"><a href="#LuckyMera-a-Modular-AI-Framework-for-Building-Hybrid-NetHack-Agents" class="headerlink" title="LuckyMera: a Modular AI Framework for Building Hybrid NetHack Agents"></a>LuckyMera: a Modular AI Framework for Building Hybrid NetHack Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08532">http://arxiv.org/abs/2307.08532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pervasive-ai-lab/luckymera">https://github.com/pervasive-ai-lab/luckymera</a></li>
<li>paper_authors: Luigi Quarantiello, Simone Marzeddu, Antonio Guzzi, Vincenzo Lomonaco</li>
<li>for: 这个研究目的是为了开发一个轻松易用、可扩展的人工智能框架，用于玩家在roguelike游戏中实现高水平的游戏表现。</li>
<li>methods: 这个研究使用了NetHack游戏来测试和训练人工智能代理，并提供了一个高阶的游戏策略设计界面。研究人员还使用了 симвоlic和神经网络模块（称为“技能”），以及实验评估和训练神经网络模型的功能。</li>
<li>results: 这个研究显示了一个强大的基准代理，可以在完整的NetHack游戏中实现州际级的表现。此外，研究人员还提供了一个可扩展的框架，可以实现更多的游戏和策略设计。<details>
<summary>Abstract</summary>
In the last few decades we have witnessed a significant development in Artificial Intelligence (AI) thanks to the availability of a variety of testbeds, mostly based on simulated environments and video games. Among those, roguelike games offer a very good trade-off in terms of complexity of the environment and computational costs, which makes them perfectly suited to test AI agents generalization capabilities. In this work, we present LuckyMera, a flexible, modular, extensible and configurable AI framework built around NetHack, a popular terminal-based, single-player roguelike video game. This library is aimed at simplifying and speeding up the development of AI agents capable of successfully playing the game and offering a high-level interface for designing game strategies. LuckyMera comes with a set of off-the-shelf symbolic and neural modules (called "skills"): these modules can be either hard-coded behaviors, or neural Reinforcement Learning approaches, with the possibility of creating compositional hybrid solutions. Additionally, LuckyMera comes with a set of utility features to save its experiences in the form of trajectories for further analysis and to use them as datasets to train neural modules, with a direct interface to the NetHack Learning Environment and MiniHack. Through an empirical evaluation we validate our skills implementation and propose a strong baseline agent that can reach state-of-the-art performances in the complete NetHack game. LuckyMera is open-source and available at https://github.com/Pervasive-AI-Lab/LuckyMera.
</details>
<details>
<summary>摘要</summary>
最近几十年内，人工智能（AI）领域所经历的发展非常 significativeto，主要归功于各种测试环境和游戏的可用性。而roguelike游戏又提供了一个非常好的平衡点，即环境复杂度和计算成本之间的融合，使其成为AI测试agent普遍化能力的最佳选择。在这项工作中，我们提出了LuckyMera框架，这是一个基于NetHackterminal型单player roguelike游戏的flexible、可Module、可Configurable和可扩展的AI框架。该框架旨在简化和加速AI测试agent的开发，并提供高级接口来设计游戏策略。LuckyMera具有内置的符号学和神经网络模块（称为“技能”），这些模块可以是硬编码的行为或神经网络学习方法，同时还可以创建Hybrid解决方案。此外，LuckyMera还提供了一些实用功能，如保存经验的轨迹，用于后续分析和训练神经网络模块，直接与NetHack学习环境和MiniHack进行交互。通过实验证明，我们 validate our skills实现和提出了一个强大的基线代理，可以在完整的NetHack游戏中达到顶尖性能。LuckyMera是开源的，可以在https://github.com/Pervasive-AI-Lab/LuckyMera上获取。
</details></li>
</ul>
<hr>
<h2 id="Image-Captions-are-Natural-Prompts-for-Text-to-Image-Models"><a href="#Image-Captions-are-Natural-Prompts-for-Text-to-Image-Models" class="headerlink" title="Image Captions are Natural Prompts for Text-to-Image Models"></a>Image Captions are Natural Prompts for Text-to-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08526">http://arxiv.org/abs/2307.08526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiye Lei, Hao Chen, Sen Zhang, Bo Zhao, Dacheng Tao</li>
<li>for: 增强文本生成模型在生成训练数据方面的表现，特别是在面临数据稀缺和隐私泄露问题时。</li>
<li>methods: 提出了一种简单 yet effective的方法，通过使用高级captioning模型对实际图像进行描述，从而生成更有信息和多样化的训练数据。</li>
<li>results: 在ImageNette、ImageNet-100和ImageNet-1K等 dataset上进行了广泛的实验，结果显示，我们的方法可以significantly improve模型在生成训练数据上的表现，即平均提高10%的分类精度。<details>
<summary>Abstract</summary>
With the rapid development of Artificial Intelligence Generated Content (AIGC), it has become common practice in many learning tasks to train or fine-tune large models on synthetic data due to the data-scarcity and privacy leakage problems. Albeit promising with unlimited data generation, owing to massive and diverse information conveyed in real images, it is challenging for text-to-image generative models to synthesize informative training data with hand-crafted prompts, which usually leads to inferior generalization performance when training downstream models. In this paper, we theoretically analyze the relationship between the training effect of synthetic data and the synthetic data distribution induced by prompts. Then we correspondingly propose a simple yet effective method that prompts text-to-image generative models to synthesize more informative and diverse training data. Specifically, we caption each real image with the advanced captioning model to obtain informative and faithful prompts that extract class-relevant information and clarify the polysemy of class names. The image captions and class names are concatenated to prompt generative models for training image synthesis. Extensive experiments on ImageNette, ImageNet-100, and ImageNet-1K verify that our method significantly improves the performance of models trained on synthetic training data, i.e., 10% classification accuracy improvements on average.
</details>
<details>
<summary>摘要</summary>
随着人工智能生成内容（AIGC）的快速发展，在许多学习任务中通常使用合成数据进行训练或细化大型模型，因为实际数据的缺乏和隐私泄露问题。虽然有普遍的可访问性和多样性的实际图像信息，但文本生成模型很难通过手工提示生成有用的训练数据，通常会导致下游模型的训练性能不佳。在这篇论文中，我们 theoretically 分析了印杂数据训练的效果和提示所引起的数据分布关系。然后，我们对应提出了一种简单 yet effective 的方法，使文本生成模型在训练图像生成时生成更有用和多样的数据。具体来说，我们使用进步的描述模型将实际图像描述成 faithful 和有用的提示，提取类相关信息并清晰地表达类名的多义性。图像描述和类名被 concatenate 以提交给生成模型进行训练图像生成。广泛的实验结果表明，我们的方法可以在 ImageNette、ImageNet-100 和 ImageNet-1K 上提高模型在合成训练数据上的性能，即平均提高10%的分类精度。
</details></li>
</ul>
<hr>
<h2 id="Does-Visual-Pretraining-Help-End-to-End-Reasoning"><a href="#Does-Visual-Pretraining-Help-End-to-End-Reasoning" class="headerlink" title="Does Visual Pretraining Help End-to-End Reasoning?"></a>Does Visual Pretraining Help End-to-End Reasoning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08506">http://arxiv.org/abs/2307.08506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Sun, Calvin Luo, Xingyi Zhou, Anurag Arnab, Cordelia Schmid</li>
<li>for:  investigate whether end-to-end learning of visual reasoning can be achieved with general-purpose neural networks, and challenge the common belief that explicit visual abstraction is essential for compositional generalization on visual reasoning.</li>
<li>methods: propose a simple and general self-supervised framework which “compresses” each video frame into a small set of tokens with a transformer network, and reconstructs the remaining frames based on the compressed temporal context.</li>
<li>results: observe that pretraining is essential to achieve compositional generalization for end-to-end visual reasoning, and our proposed framework outperforms traditional supervised pretraining, including image classification and explicit object detection, by large margins.<details>
<summary>Abstract</summary>
We aim to investigate whether end-to-end learning of visual reasoning can be achieved with general-purpose neural networks, with the help of visual pretraining. A positive result would refute the common belief that explicit visual abstraction (e.g. object detection) is essential for compositional generalization on visual reasoning, and confirm the feasibility of a neural network "generalist" to solve visual recognition and reasoning tasks. We propose a simple and general self-supervised framework which "compresses" each video frame into a small set of tokens with a transformer network, and reconstructs the remaining frames based on the compressed temporal context. To minimize the reconstruction loss, the network must learn a compact representation for each image, as well as capture temporal dynamics and object permanence from temporal context. We perform evaluation on two visual reasoning benchmarks, CATER and ACRE. We observe that pretraining is essential to achieve compositional generalization for end-to-end visual reasoning. Our proposed framework outperforms traditional supervised pretraining, including image classification and explicit object detection, by large margins.
</details>
<details>
<summary>摘要</summary>
我们的目标是研究是否可以使用通用神经网络来学习视觉逻辑，并且通过视觉预处理来帮助。如果得到正面的结果，那么这将证明通过显式的视觉抽象（例如物体检测）不是必需的，并且确认神经网络"通用"可以解决视觉识别和逻辑任务。我们提出了一个简单和通用的自我超vised框架，使得每帧视频都可以被压缩成一小组token，并使用变换器网络重建剩下的帧。为了减少重建损失，网络必须学习每幅图像的紧凑表示，同时捕捉时间上下文中的动态和物体的永久性。我们在CATER和ACRE两个视觉逻辑benchmark上进行评估，发现预处理是必要的，以实现结构化总结。我们的提议的框架在超过传统的直接监督预处理，包括图像分类和显式物体检测，的情况下表现出大的优势。
</details></li>
</ul>
<hr>
<h2 id="Can-We-Trust-Race-Prediction"><a href="#Can-We-Trust-Race-Prediction" class="headerlink" title="Can We Trust Race Prediction?"></a>Can We Trust Race Prediction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08496">http://arxiv.org/abs/2307.08496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cangyuanli/pyethnicity">https://github.com/cangyuanli/pyethnicity</a></li>
<li>paper_authors: Cangyuan Li</li>
<li>for: 本研究的目的是提高选民登记数据中的预测性能，并构建美国各州选民登记数据的全面数据库。</li>
<li>methods: 本研究使用bidirectional LSTM模型，并将其组合成ensemble模型，可以达到Literature中最高的36.8%的OOS F1分数。</li>
<li>results: 本研究构建了美国各州选民登记数据的最全面的数据库，并提供了高质量的比较基准数据集，以帮助未来的模型开发者。<details>
<summary>Abstract</summary>
In the absence of sensitive race and ethnicity data, researchers, regulators, and firms alike turn to proxies. In this paper, I train a Bidirectional Long Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data from all 50 US states and create an ensemble that achieves up to 36.8% higher out of sample (OOS) F1 scores than the best performing machine learning models in the literature. Additionally, I construct the most comprehensive database of first and surname distributions in the US in order to improve the coverage and accuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved Firstname Surname Geocoding (BIFSG). Finally, I provide the first high-quality benchmark dataset in order to fairly compare existing models and aid future model developers.
</details>
<details>
<summary>摘要</summary>
在敏感的种族和民族数据缺失的情况下，研究人员、规则制定者和企业都会倾向于使用代理。在这篇论文中，我使用bidirectional long short-term memory（BiLSTM）模型训练了一个新的选民注册数据集，并创建了一个 ensemble，其在外测（OOS） F1 分数上达到了36.8%的提高。此外，我还构建了美国首次和姓氏分布的最全面的数据库，以提高 Bayesian Improved Surname Geocoding（BISG）和 Bayesian Improved Firstname Surname Geocoding（BIFSG）的覆盖率和准确率。最后，我提供了首个高质量的 referential dataset，以公平地比较现有模型并帮助未来的模型开发者。
</details></li>
</ul>
<hr>
<h2 id="Navigating-Fairness-Measures-and-Trade-Offs"><a href="#Navigating-Fairness-Measures-and-Trade-Offs" class="headerlink" title="Navigating Fairness Measures and Trade-Offs"></a>Navigating Fairness Measures and Trade-Offs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08484">http://arxiv.org/abs/2307.08484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Buijsman</li>
<li>for: 本研究旨在为AI系统中的偏见监测和预防提供一个基础。</li>
<li>methods: 本研究使用Rawls的正义为公平性提供了一个基础，以帮助决策公平性指标和准确率之间的贸易OFF。</li>
<li>results: 研究发现，使用Rawls的正义来导航公平性指标和准确率之间的贸易OFF，可以创造一个基于理论的决策方法，帮助关注最抢夺的群体和对该群体产生最大影响的公平性指标。<details>
<summary>Abstract</summary>
In order to monitor and prevent bias in AI systems we can use a wide range of (statistical) fairness measures. However, it is mathematically impossible to optimize for all of these measures at the same time. In addition, optimizing a fairness measure often greatly reduces the accuracy of the system (Kozodoi et al, 2022). As a result, we need a substantive theory that informs us how to make these decisions and for what reasons. I show that by using Rawls' notion of justice as fairness, we can create a basis for navigating fairness measures and the accuracy trade-off. In particular, this leads to a principled choice focusing on both the most vulnerable groups and the type of fairness measure that has the biggest impact on that group. This also helps to close part of the gap between philosophical accounts of distributive justice and the fairness literature that has been observed (Kuppler et al, 2021) and to operationalise the value of fairness.
</details>
<details>
<summary>摘要</summary>
要监测和预防人工智能系统中的偏见，我们可以使用一系列（统计）公平度量。然而，从数学角度来看，同时优化所有这些公平度量是不可能的。此外，优化公平度量通常会很大减少系统的准确率（Kozodoi等，2022）。因此，我们需要一种有产物的理论，以帮助我们做出这些决策，并且为什么做出这些决策。我显示，通过使用罗尔斯的公平度量观，我们可以创建一个基于公平度量和准确率之间的平衡的基础。特别是，这会导致一种原则性的选择，集中于最容易受到影响的群体和对该群体有最大影响的公平度量。这也有助于将哲学财富分配正义和公平文献之间的差距降低（Kuppler等，2021），并将公平的价值实践化。
</details></li>
</ul>
<hr>
<h2 id="Derivation-Graph-Based-Characterizations-of-Decidable-Existential-Rule-Sets"><a href="#Derivation-Graph-Based-Characterizations-of-Decidable-Existential-Rule-Sets" class="headerlink" title="Derivation-Graph-Based Characterizations of Decidable Existential Rule Sets"></a>Derivation-Graph-Based Characterizations of Decidable Existential Rule Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08481">http://arxiv.org/abs/2307.08481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim S. Lyon, Sebastian Rudolph</li>
<li>for: 这篇论文目的是为了建立表达力强的类型规则集的代表性定义。</li>
<li>methods: 论文使用了 derivation graph 的概念和证明论证来研究存在规则的分析逻辑。</li>
<li>results: 论文得到了 gbts 和 cdgs 之间的等价关系，以及 wgbts 和 wcdgs 之间的等价关系。这些结果将有助于深化我们对存在规则的分析逻辑的理解。<details>
<summary>Abstract</summary>
This paper establishes alternative characterizations of very expressive classes of existential rule sets with decidable query entailment. We consider the notable class of greedy bounded-treewidth sets (gbts) and a new, generalized variant, called weakly gbts (wgbts). Revisiting and building on the notion of derivation graphs, we define (weakly) cycle-free derivation graph sets ((w)cdgs) and employ elaborate proof-theoretic arguments to obtain that gbts and cdgs coincide, as do wgbts and wcdgs. These novel characterizations advance our analytic proof-theoretic understanding of existential rules and will likely be instrumental in practice.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Clarifying-the-Half-Full-or-Half-Empty-Question-Multimodal-Container-Classification"><a href="#Clarifying-the-Half-Full-or-Half-Empty-Question-Multimodal-Container-Classification" class="headerlink" title="Clarifying the Half Full or Half Empty Question: Multimodal Container Classification"></a>Clarifying the Half Full or Half Empty Question: Multimodal Container Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08471">http://arxiv.org/abs/2307.08471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josua Spisak, Matthias Kerzel, Stefan Wermter</li>
<li>for: 这篇论文主要是为了研究多模态融合的问题，以提高机器人的感知能力。</li>
<li>methods: 本论文使用了不同的融合策略，将视觉、感觉和自带感知数据融合在一起，以便在分类容器和其内容时使用多模态信息。</li>
<li>results: 研究发现，使用多模态融合策略可以提高分类精度，比使用单一感知信息高出15%。<details>
<summary>Abstract</summary>
Multimodal integration is a key component of allowing robots to perceive the world. Multimodality comes with multiple challenges that have to be considered, such as how to integrate and fuse the data. In this paper, we compare different possibilities of fusing visual, tactile and proprioceptive data. The data is directly recorded on the NICOL robot in an experimental setup in which the robot has to classify containers and their content. Due to the different nature of the containers, the use of the modalities can wildly differ between the classes. We demonstrate the superiority of multimodal solutions in this use case and evaluate three fusion strategies that integrate the data at different time steps. We find that the accuracy of the best fusion strategy is 15% higher than the best strategy using only one singular sense.
</details>
<details>
<summary>摘要</summary>
设置为简化中文。<</SYS>>多modal integration 是让机器人感知世界的关键组件。多modal 存在多种挑战，如如何集成和融合数据。本文比较了不同的融合视觉、感觉和 proprioceptive 数据的可能性。数据直接记录在 NICOL 机器人上，在一个实验室中，机器人需要分类容器和其内容。由于容器的不同性，使用不同的感知方式可以在类别中有很大差异。我们示出了多modal 解决方案在这种用例中的优越性，并评估了三种融合策略，在不同的时间步骤上集成数据。我们发现，使用多modal 策略的最佳准确率比使用单一感知方式最佳策略高出了15%。
</details></li>
</ul>
<hr>
<h2 id="Towards-eXplainable-AI-for-Mobility-Data-Science"><a href="#Towards-eXplainable-AI-for-Mobility-Data-Science" class="headerlink" title="Towards eXplainable AI for Mobility Data Science"></a>Towards eXplainable AI for Mobility Data Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08461">http://arxiv.org/abs/2307.08461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anahid Jalali, Anita Graser, Clemens Heistracher</li>
<li>for: 本研究目的是为了实现行动数据科学应用中的可解释性模型，即可以从稠密轨迹数据，如汽车和船用的GPS轨迹数据中学习的模型，并提供可理解的解释。</li>
<li>methods: 本研究使用了时间图 neural networks (GNNs)和 counterfactuals 来实现可解释的模型，并评估了这些方法在不同的数据集上的性能。</li>
<li>results: 本研究提出了一种研究路径，以便实现行动数据科学中的可解释性模型，并评估了现有的 GeoXAI 研究，认为需要更加人类中心的解释方法。<details>
<summary>Abstract</summary>
This paper presents our ongoing work towards XAI for Mobility Data Science applications, focusing on explainable models that can learn from dense trajectory data, such as GPS tracks of vehicles and vessels using temporal graph neural networks (GNNs) and counterfactuals. We review the existing GeoXAI studies, argue the need for comprehensible explanations with human-centered approaches, and outline a research path toward XAI for Mobility Data Science.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Long-range-Dependency-based-Multi-Layer-Perceptron-for-Heterogeneous-Information-Networks"><a href="#Long-range-Dependency-based-Multi-Layer-Perceptron-for-Heterogeneous-Information-Networks" class="headerlink" title="Long-range Dependency based Multi-Layer Perceptron for Heterogeneous Information Networks"></a>Long-range Dependency based Multi-Layer Perceptron for Heterogeneous Information Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08430">http://arxiv.org/abs/2307.08430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jhl-hust/ldmlp">https://github.com/jhl-hust/ldmlp</a></li>
<li>paper_authors: Chao Li, Zijie Guo, Qiuting He, Hao Xu, Kun He</li>
<li>for: 这篇论文旨在解决现有的多型 graphs neuronal networks (HGNNs) 中的长距离依赖性 utilization 问题，以提高 HGNNs 的性能和效率。</li>
<li>methods: 本文提出了一种 Long-range Dependency based Multi-Layer Perceptron (LDMLP)，通过自动找到有效的meta-paths来解决高 computation 和 memory 成本问题。LDMLP 还利用了一个简单的架构，将 multi-layer perceptions 用于搜寻阶段，以提高搜寻结果的一致性。</li>
<li>results: 实验结果显示，LDMLP 可以在八个多型 graphs 数据集上取得 state-of-the-art 的性能，同时具有高效率和一致性，特别是在罕见 HINs 上。此外，LDMLP 还可以提高其他 HGNNs 的性能，如 HAN 和 SeHGNN。<details>
<summary>Abstract</summary>
Existing heterogeneous graph neural networks (HGNNs) have achieved great success in utilizing the rich semantic information in heterogeneous information networks (HINs). However, few works have delved into the utilization of long-range dependencies in HINs, which is extremely valuable as many real-world HINs are sparse, and each node has only a few directly connected neighbors. Although some HGNNs can utilize distant neighbors by stacking multiple layers or leveraging long meta-paths, the exponentially increased number of nodes in the receptive field or the number of meta-paths incurs high computation and memory costs. To address these issues, we investigate the importance of different meta-paths and propose Long-range Dependency based Multi-Layer Perceptron (LDMLP). Specifically, to solve the high-cost problem of leveraging long-range dependencies, LDMLP adopts a search stage to discover effective meta-paths automatically, reducing the exponentially increased number of meta-paths to a constant. To avoid the influence of specific modules on search results, LDMLP utilizes a simple architecture with only multi-layer perceptions in the search stage, improving the generalization of searched meta-paths. As a result, the searched meta-paths not only perform well in LDMLP but also enable other HGNNs like HAN and SeHGNN to perform better. Extensive experiments on eight heterogeneous datasets demonstrate that LDMLP achieves state-of-the-art performance while enjoying high efficiency and generalization, especially on sparse HINs.
</details>
<details>
<summary>摘要</summary>
现有的异种图 neural network (HGNN) 已经在异种信息网络 (HIN) 中获得了很大的成功，但是很少的研究者在 HIN 中利用长距离依赖关系，这对于许多实际世界 HIN 来说是非常有价值的，因为每个节点通常只有几个直接连接的邻居。虽然一些 HGNN 可以利用远程邻居，但是通过堆叠多层或利用长媒体路径来实现，导致计算和存储成本随着节点数量的增加而呈指数增长。为解决这些问题，我们调查不同的媒体路径的重要性并提出了 Long-range Dependency based Multi-Layer Perceptron (LDMLP)。具体来说，为解决长距离依赖关系的高计算成本问题，LDMLP 采用了搜索阶段自动发现有效的媒体路径，从而将 exponentially 增加的节点数量降低到常数。此外，为确保搜索结果不受特定模块的影响，LDMLP 使用了简单的架构，只有多层感知，从而提高了搜索的通用性。因此，搜索到的媒体路径不仅在 LDMLP 中表现良好，还可以使得其他 HGNN 如 HAN 和 SeHGNN 表现更好。我们在八个异种数据集进行了广泛的实验，结果表明，LDMLP 可以 дости得状态 искусственный智能性的表现，同时具有高效性和通用性，特别是在稀有 HIN 上。
</details></li>
</ul>
<hr>
<h2 id="Unstoppable-Attack-Label-Only-Model-Inversion-via-Conditional-Diffusion-Model"><a href="#Unstoppable-Attack-Label-Only-Model-Inversion-via-Conditional-Diffusion-Model" class="headerlink" title="Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model"></a>Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08424">http://arxiv.org/abs/2307.08424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongke Liu<br>for:The paper is written to address the issue of model inversion attacks (MIAs) in deep learning models, specifically in black-box scenarios where the attacker does not have access to the model’s parameters.methods:The paper proposes a novel method of MIA using a conditional diffusion model to recover the precise sample of the target without any extra optimization. The method uses two primary techniques: selecting an auxiliary dataset that is relevant to the target model task, and using the target labels and random standard normally distributed noise as conditions to guide the training process.results:The paper demonstrates that the proposed method can generate similar and accurate data to the target without optimization and outperforms generators of previous approaches in the label-only scenario. The method is evaluated using Learned Perceptual Image Patch Similarity (LPIPS) as one of the evaluation metrics, and the results show that the method achieves high attack accuracy, realism, and similarity.<details>
<summary>Abstract</summary>
Model inversion attacks (MIAs) are aimed at recovering private data from a target model's training set, which poses a threat to the privacy of deep learning models. MIAs primarily focus on the white-box scenario where the attacker has full access to the structure and parameters of the target model. However, practical applications are black-box, it is not easy for adversaries to obtain model-related parameters, and various models only output predicted labels. Existing black-box MIAs primarily focused on designing the optimization strategy, and the generative model is only migrated from the GAN used in white-box MIA. Our research is the pioneering study of feasible attack models in label-only black-box scenarios, to the best of our knowledge.   In this paper, we develop a novel method of MIA using the conditional diffusion model to recover the precise sample of the target without any extra optimization, as long as the target model outputs the label. Two primary techniques are introduced to execute the attack. Firstly, select an auxiliary dataset that is relevant to the target model task, and the labels predicted by the target model are used as conditions to guide the training process. Secondly, target labels and random standard normally distributed noise are input into the trained conditional diffusion model, generating target samples with pre-defined guidance strength. We then filter out the most robust and representative samples. Furthermore, we propose for the first time to use Learned Perceptual Image Patch Similarity (LPIPS) as one of the evaluation metrics for MIA, with systematic quantitative and qualitative evaluation in terms of attack accuracy, realism, and similarity. Experimental results show that this method can generate similar and accurate data to the target without optimization and outperforms generators of previous approaches in the label-only scenario.
</details>
<details>
<summary>摘要</summary>
模型反推攻击（MIA）是target模型的训练集中私人数据的恢复，这种攻击对深度学习模型的隐私造成了威胁。MIA主要在白盒enario中进行，攻击者可以完全访问目标模型的结构和参数。然而，在实际应用中，敌方通常无法获得模型相关的参数，只有输出预测标签。现有的黑盒MIA主要关注于设计优化策略，而模型migrated from GAN在白盒MIA中使用。我们的研究是黑盒MIA的开拓性研究，到我们所知道的范围内是首次。在这篇论文中，我们开发了一种使用conditional diffusion模型来恢复target模型的准确样本，不需要任何额外优化。只要target模型输出标签，我们就可以通过以下两种技术来执行攻击。首先，选择一个相关的auxiliary dataset，并使用目标模型预测的标签作为准则来导引训练过程。其次，通过对已经训练的conditional diffusion模型中的标签和随机标准差分布噪声输入，生成具有预定的导向强度的目标样本。然后，我们将过滤出最Robust和代表性最高的样本。此外，我们还提出了在MIA中使用Learned Perceptual Image Patch Similarity（LPIPS）作为评价指标，并进行系统atic quantitative和质量评价，包括攻击准确率、真实性和相似性。实验结果表明，这种方法可以生成与目标模型无需优化的准确和真实的数据，并且在黑盒scenario中超越了前一代方法的生成器。
</details></li>
</ul>
<hr>
<h2 id="Systematic-Comparison-of-Software-Agents-and-Digital-Twins-Differences-Similarities-and-Synergies-in-Industrial-Production"><a href="#Systematic-Comparison-of-Software-Agents-and-Digital-Twins-Differences-Similarities-and-Synergies-in-Industrial-Production" class="headerlink" title="Systematic Comparison of Software Agents and Digital Twins: Differences, Similarities, and Synergies in Industrial Production"></a>Systematic Comparison of Software Agents and Digital Twins: Differences, Similarities, and Synergies in Industrial Production</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08421">http://arxiv.org/abs/2307.08421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lasse Matthias Reinpold, Lukas Peter Wagner, Felix Gehlhoff, Malte Ramonat, Maximilian Kilthau, Milapji Singh Gill, Jonathan Tobias Reif, Vincent Henkel, Lena Scholz, Alexander Fay<br>for:This paper compares and contrasts the use of Software Agents (Agents) and Digital Twins (DTs) in industrial applications, with the goal of determining their differences, similarities, and potential synergies.methods:The comparison is based on the purposes for which Agents and DTs are applied, their properties and capabilities, and how they can be allocated within the Reference Architecture Model Industry 4.0.results:The study finds that Agents are commonly employed in the collaborative planning and execution of production processes, while DTs typically play a more passive role in monitoring production resources and processing information. The analysis suggests that a combination of Agents and DTs would demonstrate high degrees of intelligence, autonomy, sociability, and fidelity, but further standardization is required, particularly in the field of DTs.<details>
<summary>Abstract</summary>
To achieve a highly agile and flexible production, it is envisioned that industrial production systems gradually become more decentralized, interconnected, and intelligent. Within this vision, production assets collaborate with each other, exhibiting a high degree of autonomy. Furthermore, knowledge about individual production assets is readily available throughout their entire life-cycles. To realize this vision, adequate use of information technology is required. Two commonly applied software paradigms in this context are Software Agents (referred to as Agents) and Digital Twins (DTs). This work presents a systematic comparison of Agents and DTs in industrial applications. The goal of the study is to determine the differences, similarities, and potential synergies between the two paradigms. The comparison is based on the purposes for which Agents and DTs are applied, the properties and capabilities exhibited by these software paradigms, and how they can be allocated within the Reference Architecture Model Industry 4.0. The comparison reveals that Agents are commonly employed in the collaborative planning and execution of production processes, while DTs typically play a more passive role in monitoring production resources and processing information. Although these observations imply characteristic sets of capabilities and properties for both Agents and DTs, a clear and definitive distinction between the two paradigms cannot be made. Instead, the analysis indicates that production assets utilizing a combination of Agents and DTs would demonstrate high degrees of intelligence, autonomy, sociability, and fidelity. To achieve this, further standardization is required, particularly in the field of DTs.
</details>
<details>
<summary>摘要</summary>
The comparison reveals that Agents are commonly employed in the collaborative planning and execution of production processes, while DTs typically play a more passive role in monitoring production resources and processing information. Although these observations imply characteristic sets of capabilities and properties for both Agents and DTs, a clear and definitive distinction between the two paradigms cannot be made. Instead, the analysis indicates that production assets utilizing a combination of Agents and DTs would demonstrate high degrees of intelligence, autonomy, sociability, and fidelity. To achieve this, further standardization is required, particularly in the field of DTs.
</details></li>
</ul>
<hr>
<h2 id="Neurosymbolic-AI-for-Reasoning-on-Biomedical-Knowledge-Graphs"><a href="#Neurosymbolic-AI-for-Reasoning-on-Biomedical-Knowledge-Graphs" class="headerlink" title="Neurosymbolic AI for Reasoning on Biomedical Knowledge Graphs"></a>Neurosymbolic AI for Reasoning on Biomedical Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08411">http://arxiv.org/abs/2307.08411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauren Nicole DeLong, Ramon Fernández Mir, Zonglin Ji, Fiona Niamh Coulter Smith, Jacques D. Fleuriot</li>
<li>for: 这种论文是为了探讨基因组谱的完成问题，以及如何使用符号智能技术来解决这个问题。</li>
<li>methods: 这种论文使用了一种混合的符号智能技术，包括规则基本实体、嵌入式实体和逻辑回归等方法，以解决基因组谱的完成问题。</li>
<li>results: 这种论文的研究结果表明，使用符号智能技术可以提高基因组谱的完成率和准确率，并且可以更好地捕捉基因组谱中的复杂关系和特征。<details>
<summary>Abstract</summary>
Biomedical datasets are often modeled as knowledge graphs (KGs) because they capture the multi-relational, heterogeneous, and dynamic natures of biomedical systems. KG completion (KGC), can, therefore, help researchers make predictions to inform tasks like drug repositioning. While previous approaches for KGC were either rule-based or embedding-based, hybrid approaches based on neurosymbolic artificial intelligence are becoming more popular. Many of these methods possess unique characteristics which make them even better suited toward biomedical challenges. Here, we survey such approaches with an emphasis on their utilities and prospective benefits for biomedicine.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Novel-Multiagent-Flexibility-Aggregation-Framework"><a href="#A-Novel-Multiagent-Flexibility-Aggregation-Framework" class="headerlink" title="A Novel Multiagent Flexibility Aggregation Framework"></a>A Novel Multiagent Flexibility Aggregation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08401">http://arxiv.org/abs/2307.08401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stavros Orfanoudakis, Georgios Chalkiadakis</li>
<li>for: 提高Distributed Energy Resources（DERs）在智能电网中的有效利用，建立一种智能多代理框架来管理DERs。</li>
<li>methods: 提出了一种新的DER聚合框架，包括多代理体系和不同类型的机制来有效地 инте integrating DERs into the Grid。</li>
<li>results: 实验表明，该框架可以有效地将各种不同的DERs集成到Grid中，并且可以提高参与者的平均支付。使用CRPS分数规则选择机制可以提高参与者的预测准确率。<details>
<summary>Abstract</summary>
The increasing number of Distributed Energy Resources (DERs) in the emerging Smart Grid, has created an imminent need for intelligent multiagent frameworks able to utilize these assets efficiently. In this paper, we propose a novel DER aggregation framework, encompassing a multiagent architecture and various types of mechanisms for the effective management and efficient integration of DERs in the Grid. One critical component of our architecture is the Local Flexibility Estimators (LFEs) agents, which are key for offloading the Aggregator from serious or resource-intensive responsibilities -- such as addressing privacy concerns and predicting the accuracy of DER statements regarding their offered demand response services. The proposed framework allows the formation of efficient LFE cooperatives. To this end, we developed and deployed a variety of cooperative member selection mechanisms, including (a) scoring rules, and (b) (deep) reinforcement learning. We use data from the well-known PowerTAC simulator to systematically evaluate our framework. Our experiments verify its effectiveness for incorporating heterogeneous DERs into the Grid in an efficient manner. In particular, when using the well-known probabilistic prediction accuracy-incentivizing CRPS scoring rule as a selection mechanism, our framework results in increased average payments for participants, when compared with traditional commercial aggregators.
</details>
<details>
<summary>摘要</summary>
随着分布式能源资源（DERs）在智能电网中的增加，需要有效地利用这些资源已成为一项紧迫的需求。本文提出了一种新的DER集成框架，包括多智能体架构和不同类型的机制，以便有效管理和有效地吸收DERs在电网中。本文中的一个关键组件是地方flexibility估计（LFEs）代理，它们可以减轻集成器的负担，例如处理隐私问题和预测DERs提供的需求回应服务的准确性。我们的框架允许形成高效的LFE合作社。为此，我们开发了和部署了多种合作社员选择机制，包括（a）分数规则，以及（b）深度鼓励学习。我们使用PowerTAC simulator中的数据进行系统性评估我们的框架。我们的实验表明，当使用CRPS分数规则作为选择机制时，我们的框架可以有效地吸收不同类型的DERs。
</details></li>
</ul>
<hr>
<h2 id="Gender-mobility-in-the-labor-market-with-skills-based-matching-models"><a href="#Gender-mobility-in-the-labor-market-with-skills-based-matching-models" class="headerlink" title="Gender mobility in the labor market with skills-based matching models"></a>Gender mobility in the labor market with skills-based matching models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08368">http://arxiv.org/abs/2307.08368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajaya Adhikari, Steven Vethman, Daan Vos, Marc Lenz, Ioana Cocu, Ioannis Tolios, Cor J. Veenman</li>
<li>for: 本研究旨在探讨基于技能匹配的劳动市场流动性是否会促进性别分布的调整。</li>
<li>methods: 研究使用了语言模型和监督学习方法，包括bag of words、word2vec和BERT语言表示，以及不同的距离度量（静止和机器学习基于的）。</li>
<li>results: 研究发现，基于技能匹配的模型会传递性别分布偏见，而不同的语言表示和距离度量可能会影响模型的匹配性和风险。<details>
<summary>Abstract</summary>
Skills-based matching promises mobility of workers between different sectors and occupations in the labor market. In this case, job seekers can look for jobs they do not yet have experience in, but for which they do have relevant skills. Currently, there are multiple occupations with a skewed gender distribution. For skills-based matching, it is unclear if and how a shift in the gender distribution, which we call gender mobility, between occupations will be effected. It is expected that the skills-based matching approach will likely be data-driven, including computational language models and supervised learning methods.   This work, first, shows the presence of gender segregation in language model-based skills representation of occupations. Second, we assess the use of these representations in a potential application based on simulated data, and show that the gender segregation is propagated by various data-driven skills-based matching models.These models are based on different language representations (bag of words, word2vec, and BERT), and distance metrics (static and machine learning-based). Accordingly, we show how skills-based matching approaches can be evaluated and compared on matching performance as well as on the risk of gender segregation. Making the gender segregation bias of models more explicit can help in generating healthy trust in the use of these models in practice.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用技能匹配的承诺，工作者可以在劳动市场中移动 между不同的领域和职业。在这种情况下，招聘人员可以寻找他们没有经验的工作，但具有相关技能。目前，有多个职业存在倾斜性的性别分布。对于技能匹配方法，不确定性是 gender  mobilty 会如何改变。预计技能匹配方法将会是数据驱动的，包括计算机语言模型和监督学习方法。这项工作首先显示了语言模型基于职业技能表示中的性别分布。其次，我们评估了这些表示在基于验证数据的应用中的使用，并显示了这些模型在各种数据驱动技能匹配模型中的性别分布倾斜。这些模型基于不同的语言表示（袋式、word2vec和BERT）和距离度量（静止和机器学习基于）。因此，我们可以评估和比较技能匹配方法的匹配性和性别分布倾斜。使得模型中的性别分布倾斜更加明确，可以帮助在实践中健康地信任这些模型。>>
</details></li>
</ul>
<hr>
<h2 id="M-FLAG-Medical-Vision-Language-Pre-training-with-Frozen-Language-Models-and-Latent-Space-Geometry-Optimization"><a href="#M-FLAG-Medical-Vision-Language-Pre-training-with-Frozen-Language-Models-and-Latent-Space-Geometry-Optimization" class="headerlink" title="M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization"></a>M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08347">http://arxiv.org/abs/2307.08347</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cheliu-computation/m-flag-miccai2023">https://github.com/cheliu-computation/m-flag-miccai2023</a></li>
<li>paper_authors: Che Liu, Sibo Cheng, Chen Chen, Mengyun Qiao, Weitong Zhang, Anand Shah, Wenjia Bai, Rossella Arcucci</li>
<li>for: 这篇论文旨在提出一种新的医疗影像语言模型预训方法，以提高医疗影像和临床文本之间的联合学习。</li>
<li>methods: 提案方法称为医疗影像语言预训（M-FLAG），利用冻结的语言模型来稳定训练过程，并导入一个新的正交对角对映损失函数来调和隐藏空间几何。</li>
<li>results: 实验结果显示，M-FLAG可以与现有的医疗影像语言预训方法相比，在三个下游任务中表现出色，包括医疗影像分类、分割和物体检测。尤其是在分割任务中，M-FLAG只使用RSNA数据集的1%，仍能超越已经精通ImageNet预训模型的 Fine-tuning 。<details>
<summary>Abstract</summary>
Medical vision-language models enable co-learning and integrating features from medical imaging and clinical text. However, these models are not easy to train and the latent representation space can be complex. Here we propose a novel way for pre-training and regularising medical vision-language models. The proposed method, named Medical vision-language pre-training with Frozen language models and Latent spAce Geometry optimization (M-FLAG), leverages a frozen language model for training stability and efficiency and introduces a novel orthogonality loss to harmonize the latent space geometry. We demonstrate the potential of the pre-trained model on three downstream tasks: medical image classification, segmentation, and object detection. Extensive experiments across five public datasets demonstrate that M-FLAG significantly outperforms existing medical vision-language pre-training approaches and reduces the number of parameters by 78\%. Notably, M-FLAG achieves outstanding performance on the segmentation task while using only 1\% of the RSNA dataset, even outperforming ImageNet pre-trained models that have been fine-tuned using 100\% of the data.
</details>
<details>
<summary>摘要</summary>
医疗视力语言模型可以同时学习医疗影像和临床文本特征。然而，这些模型不易于训练，其潜在表示空间也可能复杂。在这篇文章中，我们提出了一种新的医疗视力语言预训练方法，名为医疗视力语言预训练器（M-FLAG）。M-FLAG利用冻结的语言模型来保证训练稳定性和效率，并引入一种新的正交准则来融和潜在表示空间的几何结构。我们在三个下游任务上进行了广泛的实验：医疗影像分类、 segmentation 和物体检测。结果表明，M-FLAG在这些任务上显著超过了现有的医疗视力语言预训练方法，并将参数数量减少了78%。尤其是在分割任务上，M-FLAG只使用了RSNA数据集的1%，而且even outperform ImageNet预训练模型，这些模型在100%的数据上进行了细化。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Cross-Modality-Attention-Fusion-for-2D-Object-Detection"><a href="#Multi-Task-Cross-Modality-Attention-Fusion-for-2D-Object-Detection" class="headerlink" title="Multi-Task Cross-Modality Attention-Fusion for 2D Object Detection"></a>Multi-Task Cross-Modality Attention-Fusion for 2D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08339">http://arxiv.org/abs/2307.08339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huawei Sun, Hao Feng, Georg Stettinger, Lorenzo Servadei, Robert Wille</li>
<li>for: 本研究旨在提高自动驾驶中的精准和可靠对象检测，尤其是在不良天气和夜间场景下。</li>
<li>methods: 本研究提出了两种新的雷达处理技术，以更好地与摄像头数据相匹配。此外，我们还提出了一种多任务交叉模态注意力融合网络（MCAF-Net），用于对象检测和自由空间分割。</li>
<li>results: 我们的方法在nuScenes数据集上比现有的雷达摄像头融合基于对象检测器表现更好，特别是在不良天气和夜间场景下。我们的方法还能够更好地利用特征地图信息，从而提高对象检测的精度和可靠性。<details>
<summary>Abstract</summary>
Accurate and robust object detection is critical for autonomous driving. Image-based detectors face difficulties caused by low visibility in adverse weather conditions. Thus, radar-camera fusion is of particular interest but presents challenges in optimally fusing heterogeneous data sources. To approach this issue, we propose two new radar preprocessing techniques to better align radar and camera data. In addition, we introduce a Multi-Task Cross-Modality Attention-Fusion Network (MCAF-Net) for object detection, which includes two new fusion blocks. These allow for exploiting information from the feature maps more comprehensively. The proposed algorithm jointly detects objects and segments free space, which guides the model to focus on the more relevant part of the scene, namely, the occupied space. Our approach outperforms current state-of-the-art radar-camera fusion-based object detectors in the nuScenes dataset and achieves more robust results in adverse weather conditions and nighttime scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>自动驾驶需要精准和可靠的对象检测，图像基于的检测器在不利的天气条件下会遇到困难。因此，雷达-相机融合非常有优势，但是将异构数据源合并优化又是一个挑战。为解决这个问题，我们提出了两种新的雷达预处理技术，以更好地对准雷达和相机数据。此外，我们还介绍了一种多任务跨模态注意力融合网络（MCAF-Net），用于对象检测，其中包括两种新的融合块。这些块使得可以更好地利用特征地图中的信息。我们的方法同时检测对象和分割空间，使模型能够更好地专注于场景中更重要的部分，即占用空间。我们的方法在nuScenes数据集中比现有的雷达-相机融合基于对象检测器更高效和更稳定，在不利的天气和夜晚 scenarios 中也达到了更好的效果。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Impact-of-Adversarial-Examples-on-Explainable-Machine-Learning"><a href="#Analyzing-the-Impact-of-Adversarial-Examples-on-Explainable-Machine-Learning" class="headerlink" title="Analyzing the Impact of Adversarial Examples on Explainable Machine Learning"></a>Analyzing the Impact of Adversarial Examples on Explainable Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08327">http://arxiv.org/abs/2307.08327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prathyusha Devabhakthini, Sasmita Parida, Raj Mani Shukla, Suvendu Chandan Nayak</li>
<li>for: 本研究探讨了因为对深度学习模型的抗击攻击而导致的模型解释性的影响，尤其是在文本分类问题上。</li>
<li>methods: 我们开发了一个基于机器学习的文本数据分类模型，然后引入了对文本数据的抗击偏移来评估模型的分类性能 после攻击。</li>
<li>results: 我们发现了对文本数据的抗击偏移会导致模型的解释性受到影响，并且我们可以通过分析模型的解释来理解攻击后模型的性能下降的原因。<details>
<summary>Abstract</summary>
Adversarial attacks are a type of attack on machine learning models where an attacker deliberately modifies the inputs to cause the model to make incorrect predictions. Adversarial attacks can have serious consequences, particularly in applications such as autonomous vehicles, medical diagnosis, and security systems. Work on the vulnerability of deep learning models to adversarial attacks has shown that it is very easy to make samples that make a model predict things that it doesn't want to. In this work, we analyze the impact of model interpretability due to adversarial attacks on text classification problems. We develop an ML-based classification model for text data. Then, we introduce the adversarial perturbations on the text data to understand the classification performance after the attack. Subsequently, we analyze and interpret the model's explainability before and after the attack
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Adversarial attacks are a type of attack on machine learning models where an attacker deliberately modifies the inputs to cause the model to make incorrect predictions. Adversarial attacks can have serious consequences, particularly in applications such as autonomous vehicles, medical diagnosis, and security systems. Work on the vulnerability of deep learning models to adversarial attacks has shown that it is very easy to make samples that make a model predict things that it doesn't want to. In this work, we analyze the impact of model interpretability due to adversarial attacks on text classification problems. We develop an ML-based classification model for text data. Then, we introduce the adversarial perturbations on the text data to understand the classification performance after the attack. Subsequently, we analyze and interpret the model's explainability before and after the attack." into 简化字 Simplified Chinese.Here's the translation: Adversarial attacks 是一种对机器学习模型的攻击，攻击者故意修改输入，使模型作出错误预测。这些攻击可能有严重的后果，特别是在自动驾驶、医疗诊断和安全系统等应用中。工作表明，深度学习模型对 adversarial attacks 的抵触性很强，可以轻松地制造出导致模型预测错误的样本。在这种情况下，我们分析了文本分类问题中模型解释性受到 adversarial attacks 的影响。我们开发了一个基于机器学习的文本分类模型，然后引入了对文本数据的perturbations，以理解攻击后的分类性能。接着，我们分析和解释模型之前和之后攻击的解释性。
</details></li>
</ul>
<hr>
<h2 id="LogPrecis-Unleashing-Language-Models-for-Automated-Shell-Log-Analysis"><a href="#LogPrecis-Unleashing-Language-Models-for-Automated-Shell-Log-Analysis" class="headerlink" title="LogPrécis: Unleashing Language Models for Automated Shell Log Analysis"></a>LogPrécis: Unleashing Language Models for Automated Shell Log Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08309">http://arxiv.org/abs/2307.08309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Boffa, Rodolfo Vieira Valentim, Luca Vassio, Danilo Giordano, Idilio Drago, Marco Mellia, Zied Ben Houidi</li>
<li>for: 本研究旨在利用语言模型（LM）自动分析文本类 Unix shell 攻击日志，以提高安全专家对攻击行为的理解和诊断。</li>
<li>methods: 本研究使用了当今最佳的 LM 技术，开发了名为 LogPr&#39;ecis 的系统，可以对 Unix shell 会话进行自动分析，并将攻击者策略分配给每个会话部分。</li>
<li>results: 对两个大数据集，包含约400,000个 Unix shell 攻击，LogPr&#39;ecis 可以将其缩减为约3,000个指纹，每个指纹都是Session中的攻击者策略的序列。LogPr&#39;ecis 提供的抽象可以帮助分析员更好地理解攻击，识别指纹，检测新型攻击、连接相似攻击和跟踪家族和变化。<details>
<summary>Abstract</summary>
The collection of security-related logs holds the key to understanding attack behaviors and diagnosing vulnerabilities. Still, their analysis remains a daunting challenge. Recently, Language Models (LMs) have demonstrated unmatched potential in understanding natural and programming languages. The question arises whether and how LMs could be also useful for security experts since their logs contain intrinsically confused and obfuscated information. In this paper, we systematically study how to benefit from the state-of-the-art in LM to automatically analyze text-like Unix shell attack logs. We present a thorough design methodology that leads to LogPr\'ecis. It receives as input raw shell sessions and automatically identifies and assigns the attacker tactic to each portion of the session, i.e., unveiling the sequence of the attacker's goals. We demonstrate LogPr\'ecis capability to support the analysis of two large datasets containing about 400,000 unique Unix shell attacks. LogPr\'ecis reduces them into about 3,000 fingerprints, each grouping sessions with the same sequence of tactics. The abstraction it provides lets the analyst better understand attacks, identify fingerprints, detect novelty, link similar attacks, and track families and mutations. Overall, LogPr\'ecis, released as open source, paves the way for better and more responsive defense against cyberattacks.
</details>
<details>
<summary>摘要</summary>
集成安全相关的日志可能是了解攻击者行为和诊断漏洞的钥匙。然而，它们的分析仍然是一项挑战。现在，语言模型（LM）已经在理解自然语言和编程语言方面展现出无与伦比的潜力。问题在于何时和如何使用LM来帮助安全专家分析含有各种各样信息的日志。在这篇论文中，我们系统地研究了如何利用当前的LM来自动分析文本类 Unix shell 攻击日志。我们提出了一种完整的设计方法，即LogPr\'ecis。它接受 raw shell 会话作为输入，并自动将攻击者策略分配到每个会话中的每个部分，即揭示攻击者的目标顺序。我们示例了LogPr\'ecis 对两个大数据集（包含约400,000个Uniix shell 攻击）的分析。LogPr\'ecis 将这些数据缩减成约3,000个指纹，每个指纹集成了与同样的策略序列相关的会话。这种抽象使得分析员更好地理解攻击，识别指纹，检测新特征，连接相似的攻击，跟踪家族和变化。总之，LogPr\'ecis，作为开源软件，为防御 против网络攻击提供了更好和更快的反应。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Multi-Task-Model-Imitating-Dermatologists-for-Accurate-Differential-Diagnosis-of-Skin-Diseases-in-Clinical-Images"><a href="#A-Novel-Multi-Task-Model-Imitating-Dermatologists-for-Accurate-Differential-Diagnosis-of-Skin-Diseases-in-Clinical-Images" class="headerlink" title="A Novel Multi-Task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images"></a>A Novel Multi-Task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08308">http://arxiv.org/abs/2307.08308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan-Jie Zhou, Wei Liu, Yuan Gao, Jing Xu, Le Lu, Yuping Duan, Hao Cheng, Na Jin, Xiaoyong Man, Shuang Zhao, Yu Wang</li>
<li>for: 这个研究旨在提出一个具有执行力的电脑支持 skin 疾病诊断方法，以帮助皮肤科医生和患者更好地诊断皮肤疾病。</li>
<li>methods: 本研究提出了一个名为 DermImitFormer 的多任务模型，该模型通过多任务学习同时预测身体部位和肿瘤特征以及疾病本身，从而提高诊断精度和诊断解释性。此外，研究还提出了一个精确地 zoom-in 到肿瘤特征的选择模组，以及一个模型 complicated 的诊断推理之间的交互模块。</li>
<li>results: 实验结果显示，DermImitFormer 在三个不同的数据集上均能够实现顶尖的识别性能，并且在诊断皮肤疾病中具有更高的精度和解释性。<details>
<summary>Abstract</summary>
Skin diseases are among the most prevalent health issues, and accurate computer-aided diagnosis methods are of importance for both dermatologists and patients. However, most of the existing methods overlook the essential domain knowledge required for skin disease diagnosis. A novel multi-task model, namely DermImitFormer, is proposed to fill this gap by imitating dermatologists' diagnostic procedures and strategies. Through multi-task learning, the model simultaneously predicts body parts and lesion attributes in addition to the disease itself, enhancing diagnosis accuracy and improving diagnosis interpretability. The designed lesion selection module mimics dermatologists' zoom-in action, effectively highlighting the local lesion features from noisy backgrounds. Additionally, the presented cross-interaction module explicitly models the complicated diagnostic reasoning between body parts, lesion attributes, and diseases. To provide a more robust evaluation of the proposed method, a large-scale clinical image dataset of skin diseases with significantly more cases than existing datasets has been established. Extensive experiments on three different datasets consistently demonstrate the state-of-the-art recognition performance of the proposed approach.
</details>
<details>
<summary>摘要</summary>
皮肤病是现代医学中最常见的健康问题，而计算机助成诊断方法对于皮肤科医生和患者都是非常重要的。然而，现有的大多数方法忽略了皮肤病诊断中所需的基本领域知识。本文提出了一种新的多任务模型，namely DermImitFormer，以模仿皮肤科医生的诊断过程和策略。通过多任务学习，模型同时预测身体部位和肿瘤特征以及疾病本身，从而提高诊断准确性和诊断可读性。设计的肿瘤选择模块模仿了皮肤科医生的缩进操作，有效地强调背景中的肿瘤特征。此外，提出的交叉交互模块Explicitly模型了肿瘤特征、身体部位和疾病之间的复杂的诊断关系。为了更加Robust地评估提议方法，我们建立了一个大规模的皮肤病图像数据集，包含了现有数据集的多倍的患者。广泛的实验表明，提议的方法在三个不同的数据集上具有现代识别性能。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Computation-of-Counterfactual-Bounds"><a href="#Efficient-Computation-of-Counterfactual-Bounds" class="headerlink" title="Efficient Computation of Counterfactual Bounds"></a>Efficient Computation of Counterfactual Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08304">http://arxiv.org/abs/2307.08304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Zaffalon, Alessandro Antonucci, Rafael Cabañas, David Huber, Dario Azzimonti</li>
<li>for: 本研究的目的是计算基于部分可Identifiable counterfactual queries的上下文 bounds。</li>
<li>methods: 本研究使用了从Structural causal model maps to credal nets的方法，以及基于 credal nets的算法来计算 exact counterfactual bounds。</li>
<li>results: 研究表明，使用 causal EM scheme可以得到准确的approximate bounds，并且通过提供credible intervals来评估其准确性。Synthetic benchmark表明，EM scheme在一定数量的运行中能够实现准确的结果。此外，研究还指出了一种常neglected的限制，即counterfactual bounds计算不需要知道结构方程的情况下是不可靠的。<details>
<summary>Abstract</summary>
We assume to be given structural equations over discrete variables inducing a directed acyclic graph, namely, a structural causal model, together with data about its internal nodes. The question we want to answer is how we can compute bounds for partially identifiable counterfactual queries from such an input. We start by giving a map from structural casual models to credal networks. This allows us to compute exact counterfactual bounds via algorithms for credal nets on a subclass of structural causal models. Exact computation is going to be inefficient in general given that, as we show, causal inference is NP-hard even on polytrees. We target then approximate bounds via a causal EM scheme. We evaluate their accuracy by providing credible intervals on the quality of the approximation; we show through a synthetic benchmark that the EM scheme delivers accurate results in a fair number of runs. In the course of the discussion, we also point out what seems to be a neglected limitation to the trending idea that counterfactual bounds can be computed without knowledge of the structural equations. We also present a real case study on palliative care to show how our algorithms can readily be used for practical purposes.
</details>
<details>
<summary>摘要</summary>
我们假设我们获得了结构方程模型，即直接数据图，以及这个模型内部节点的数据。我们想要解答一个问题：如何从这个输入中计算对 partly 可识别的 counterfactual 查询中的范围。我们开始通过将结构 causal 模型映射到信义网络中，以便通过信义网络的算法来计算精确的 counterfactual 范围。但是，我们表明，因为我们展示的是 causal 推理是 NP-hard 的，因此精确的计算通常是不可能的。我们遂提出一个近似的 bounds 方法，基于 causal EM 架构。我们评估了这个方法的准确性，通过提供信义interval 来评估近似的质量。我们透过一个 sintetic  benchmark 表明，EM 架构实际上可以获得正确的结果。在讨论中，我们还指出了一个忽略的限制，即 counterfactual 范围可以computed without 知情 structural 方程的假设。我们还提供了一个实际应用的例子，关于palliative care。
</details></li>
</ul>
<hr>
<h2 id="Soft-Prompt-Tuning-for-Augmenting-Dense-Retrieval-with-Large-Language-Models"><a href="#Soft-Prompt-Tuning-for-Augmenting-Dense-Retrieval-with-Large-Language-Models" class="headerlink" title="Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models"></a>Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08303">http://arxiv.org/abs/2307.08303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhiyuanpeng/sptar">https://github.com/zhiyuanpeng/sptar</a></li>
<li>paper_authors: Zhiyuan Peng, Xuyang Wu, Yi Fang<br>for:这篇论文主要针对的是提高 dense retrieval（DR）模型的性能，特别是在没有域pecific的训练数据的情况下。methods:这篇论文提出了一种基于 soft prompt tuning（SPTAR）的方法，通过优化任务特定的软提问来提高 LLMs 的表现，并使用这些提问来标注无标注文档。results:实验表明，SPTAR 在无监督基elines上超越 BM25 和 latest 提出的 LLMs-based 增强方法。<details>
<summary>Abstract</summary>
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific dense retrievers. We design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries. To the best of our knowledge, there is no prior work utilizing soft prompt tuning to augment DR models. The experiments demonstrate that SPTAR outperforms the unsupervised baselines BM25 and the recently proposed LLMs-based augmentation method for DR.
</details>
<details>
<summary>摘要</summary>
dense retrieval (DR) 将查询和文档转换为密集表示并测量查询和文档在向量空间的相似性。DR模型的一个挑战是缺乏域专的训练数据。虽然DR模型可以通过转移学习从大规模公共数据集如MS MARCO进行学习，但证据表明不 todos los DR模型和领域都可以从转移学习中受益。在这些研究中，一些研究人员使用大型自然语言模型（LLM）来改进零shot和几shot DR模型。然而，使用这些工作中的硬提问或人工写的提问无法保证生成的弱查询的质量。为了解决这个问题，我们提出了软提问调整 для增强DR（SPTAR）：对每个任务，我们利用软提问调整来优化任务特定的软提问，然后使用LLM进行标注未标注的文档，生成弱查询。我们设计了一个筛选器，以选择高质量的示例文档-查询对，进一步改进弱标记的查询质量。到目前为止，没有先前的工作使用软提问调整来增强DR模型。实验结果表明，SPTAR超过了无监督基eline和最近提出的LLMs-based增强方法 дляDR。
</details></li>
</ul>
<hr>
<h2 id="ShiftNAS-Improving-One-shot-NAS-via-Probability-Shift"><a href="#ShiftNAS-Improving-One-shot-NAS-via-Probability-Shift" class="headerlink" title="ShiftNAS: Improving One-shot NAS via Probability Shift"></a>ShiftNAS: Improving One-shot NAS via Probability Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08300">http://arxiv.org/abs/2307.08300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bestfleer/shiftnas">https://github.com/bestfleer/shiftnas</a></li>
<li>paper_authors: Mingyang Zhang, Xinyi Yu, Haodong Zhao, Linlin Ou</li>
<li>for: 一种时间效率的 neural architecture search (NAS) 方法，可以在不同的复杂度情况下获得最优的子网架构和参数，只需要训练一次。</li>
<li>methods: 我们使用 shiftNAS，一种可以根据子网的复杂度调整抽象概率的方法，以及一种可以准确地提供子网的架构的建立方法。</li>
<li>results: 我们在多种视觉网络模型，包括卷积神经网络 (CNNs) 和视transformers (ViTs) 上进行了实验，并证明了 shiftNAS 是模型无关的。实验结果表明，shiftNAS 可以在 ImageNet 上提高一键 NAS 的性能，而无需额外的资源消耗。<details>
<summary>Abstract</summary>
One-shot Neural architecture search (One-shot NAS) has been proposed as a time-efficient approach to obtain optimal subnet architectures and weights under different complexity cases by training only once. However, the subnet performance obtained by weight sharing is often inferior to the performance achieved by retraining. In this paper, we investigate the performance gap and attribute it to the use of uniform sampling, which is a common approach in supernet training. Uniform sampling concentrates training resources on subnets with intermediate computational resources, which are sampled with high probability. However, subnets with different complexity regions require different optimal training strategies for optimal performance. To address the problem of uniform sampling, we propose ShiftNAS, a method that can adjust the sampling probability based on the complexity of subnets. We achieve this by evaluating the performance variation of subnets with different complexity and designing an architecture generator that can accurately and efficiently provide subnets with the desired complexity. Both the sampling probability and the architecture generator can be trained end-to-end in a gradient-based manner. With ShiftNAS, we can directly obtain the optimal model architecture and parameters for a given computational complexity. We evaluate our approach on multiple visual network models, including convolutional neural networks (CNNs) and vision transformers (ViTs), and demonstrate that ShiftNAS is model-agnostic. Experimental results on ImageNet show that ShiftNAS can improve the performance of one-shot NAS without additional consumption. Source codes are available at https://github.com/bestfleer/ShiftNAS.
</details>
<details>
<summary>摘要</summary>
一种叫做One-shot Neural architecture search（One-shot NAS）的方法已经被提出，可以在不同的复杂度情况下获得优化的子网架构和参数，只需要训练一次。然而，通过重复使用的方法可以获得更好的性能。在这篇论文中，我们研究了这个性能差距，并归因于使用uniform sampling方法。uniform sampling会将训练资源集中在中等计算资源的子网上，这些子网具有高概率被采样。然而，不同的复杂度区域需要不同的优化训练策略以实现最佳性能。为了解决uniform sampling问题，我们提出了ShiftNAS方法。ShiftNAS可以根据子网的复杂度调整采样概率，以便直接从一个给定的计算复杂度中获得最佳的模型架构和参数。我们可以通过评估不同复杂度下子网的性能变化，并设计一个可以准确和高效地提供子网的架构生成器。采样概率和架构生成器都可以通过梯度下降方式进行END-TO-END训练。ShiftNAS是模型无关的，我们在多种视觉网络模型，包括卷积神经网络（CNN）和视Transformers（ViTs）中进行了实验，并证明了ShiftNAS可以提高一键 NAS的性能。实验结果表明，ShiftNAS可以在ImageNet上提高性能，而不需要额外的资源。代码可以在https://github.com/bestfleer/ShiftNAS上下载。
</details></li>
</ul>
<hr>
<h2 id="Abductive-Reasoning-with-the-GPT-4-Language-Model-Case-studies-from-criminal-investigation-medical-practice-scientific-research"><a href="#Abductive-Reasoning-with-the-GPT-4-Language-Model-Case-studies-from-criminal-investigation-medical-practice-scientific-research" class="headerlink" title="Abductive Reasoning with the GPT-4 Language Model: Case studies from criminal investigation, medical practice, scientific research"></a>Abductive Reasoning with the GPT-4 Language Model: Case studies from criminal investigation, medical practice, scientific research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10250">http://arxiv.org/abs/2307.10250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Remo Pareschi</li>
<li>for: 这项研究评估了GPT-4大语言模型在复杂领域如医学诊断、刑事学和 cosmology 中的推理能力。</li>
<li>methods: 这项研究使用了交互式采访 Format，AI助手表现了可靠性在生成和选择假设方面。</li>
<li>results: 研究发现，GPT-4大语言模型可靠地生成和选择假设，并在医学诊断、刑事学和 cosmology 中提供了可能的医疗诊断、刑事原因和 cosmology 解释。<details>
<summary>Abstract</summary>
This study evaluates the GPT-4 Large Language Model's abductive reasoning in complex fields like medical diagnostics, criminology, and cosmology. Using an interactive interview format, the AI assistant demonstrated reliability in generating and selecting hypotheses. It inferred plausible medical diagnoses based on patient data and provided potential causes and explanations in criminology and cosmology. The results highlight the potential of LLMs in complex problem-solving and the need for further research to maximize their practical applications.
</details>
<details>
<summary>摘要</summary>
Note:* "GPT-4" 被翻译为 "GPT-4 Large Language Model"* "abductive reasoning" 被翻译为 "推理"* "complex fields" 被翻译为 "复杂的领域"* "interactive interview format" 被翻译为 "互动式采访形式"* "hypotheses" 被翻译为 "假设"* "patient data" 被翻译为 "病人数据"* "cosmology" 被翻译为 " cosmology"
</details></li>
</ul>
<hr>
<h2 id="Going-Beyond-Linear-Mode-Connectivity-The-Layerwise-Linear-Feature-Connectivity"><a href="#Going-Beyond-Linear-Mode-Connectivity-The-Layerwise-Linear-Feature-Connectivity" class="headerlink" title="Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity"></a>Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08286">http://arxiv.org/abs/2307.08286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanpeng Zhou, Yongyi Yang, Xiaojiang Yang, Junchi Yan, Wei Hu<br>for: 这研究探讨了神经网络训练中复杂的损失 landscape 和训练 dynamics 中的一些新领域现象, 其中 Linear Mode Connectivity (LMC) 引起了广泛的关注，因为它表明不同的解可以在参数空间中 Linear 连接，保持 nearly constant 的训练和测试损失。methods: 这研究引入了一种更强的线性连接概念，即层wise Linear Feature Connectivity (LLFC)，它表明每层的特征图在不同训练网络中是线性连接的。研究提供了广泛的实验证据，证明当两个训练网络满足 LMC （通过生成或排序方法）时，它们也总是满足 LLFC 在大多数层。results: 研究发现，LLFC 在各层中的实现，对于不同的训练方法和模型来说，都是一个通用的现象。这些结果不仅证明了 LMC 和 LLFC 之间的关系，还探讨了这两种方法的内在逻辑。<details>
<summary>Abstract</summary>
Recent work has revealed many intriguing empirical phenomena in neural network training, despite the poorly understood and highly complex loss landscapes and training dynamics. One of these phenomena, Linear Mode Connectivity (LMC), has gained considerable attention due to the intriguing observation that different solutions can be connected by a linear path in the parameter space while maintaining near-constant training and test losses. In this work, we introduce a stronger notion of linear connectivity, Layerwise Linear Feature Connectivity (LLFC), which says that the feature maps of every layer in different trained networks are also linearly connected. We provide comprehensive empirical evidence for LLFC across a wide range of settings, demonstrating that whenever two trained networks satisfy LMC (via either spawning or permutation methods), they also satisfy LLFC in nearly all the layers. Furthermore, we delve deeper into the underlying factors contributing to LLFC, which reveal new insights into the spawning and permutation approaches. The study of LLFC transcends and advances our understanding of LMC by adopting a feature-learning perspective.
</details>
<details>
<summary>摘要</summary>
最近的研究发现了许多神秘的实际现象在神经网络训练中，尽管损失地形和训练动态还未完全理解。一种这些现象是线性模式连接（LMC），它因为训练和测试损失保持相对常数的情况下，连接不同解的线性路径在参数空间而吸引了广泛的关注。在这篇文章中，我们引入了一种更强的线性连接概念，层次线性特征连接（LLFC），它表示每个层的特征图在不同训练网络中是线性连接的。我们提供了广泛的实验证据，证明在大多数情况下，当两个训练网络满足LMC（通过生成或排序方法）时，它们也满足LLFC在大多数层。此外，我们还探究了LLFC的下面因素，这些因素揭示了生成和排序方法的新的视角。研究LLFC超越了和掌握LMC的理解，采用特征学习的视角。
</details></li>
</ul>
<hr>
<h2 id="Deep-Neural-Networks-and-Brain-Alignment-Brain-Encoding-and-Decoding-Survey"><a href="#Deep-Neural-Networks-and-Brain-Alignment-Brain-Encoding-and-Decoding-Survey" class="headerlink" title="Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)"></a>Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10246">http://arxiv.org/abs/2307.10246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subba Reddy Oota, Manish Gupta, Raju S. Bapi, Gael Jobard, Frederic Alexandre, Xavier Hinaut</li>
<li>for: 这个论文的目的是为了研究大脑如何表示不同的信息模式。</li>
<li>methods: 这篇论文使用了functional magnetic resonance imaging（fMRI）记录来研究大脑的记忆和语言处理。</li>
<li>results: 这篇论文提出了一些深度学习模型来解释大脑如何处理语言、视觉和听觉信息。<details>
<summary>Abstract</summary>
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for designing brain-machine or brain-computer interfaces. Inspired by the effectiveness of deep learning models for natural language processing, computer vision, and speech, recently several neural encoding and decoding models have been proposed. In this survey, we will first discuss popular representations of language, vision and speech stimuli, and present a summary of neuroscience datasets. Further, we will review popular deep learning based encoding and decoding architectures and note their benefits and limitations. Finally, we will conclude with a brief summary and discussion about future trends. Given the large amount of recently published work in the `computational cognitive neuroscience' community, we believe that this survey nicely organizes the plethora of work and presents it as a coherent story.
</details>
<details>
<summary>摘要</summary>
如何使潜意识表示不同的信息？可以通过研究大脑磁共振成像（fMRI）来回答这些问题。作为一个第一步，神经科学社区已经提供了许多大脑认知神经科学数据集，关于静止阅读/听写/观看概念词、故事、图片和电影。使用这些数据集，已经提出了许多过去两十年的编码和解码模型。这些模型可以用于基础研究神经科学和认知科学。编码模型的目标是自动生成fMRI大脑表示，它们有许多实际应用，如评估和诊断神经系统疾病，以及设计神经系统损伤的治疗。解码模型的目标是使用fMRI重建刺激，它们可以用于设计大脑机器或大脑计算机界面。受到深度学习模型在自然语言处理、计算机视觉和语音处理方面的成功，最近几年内，有许多神经编码和解码模型被提出。在这篇评论中，我们将首先讨论语言、视觉和听说刺激的流行表示方法，并提供大脑认知神经科学数据集的总览。然后，我们将回顾深度学习基于编码和解码架构的优点和局限性。最后，我们将结束 WITH 一个简短的总结和讨论，关于未来趋势。由于最近出版的大量研究在`计算认知神经科学`社区中，我们认为这篇评论非常有用，可以将这些研究组织成一个听起来的故事。
</details></li>
</ul>
<hr>
<h2 id="Team-Badminseok-at-IJCAI-CoachAI-Badminton-Challenge-2023-Multi-Layer-Multi-Input-Transformer-Network-MuLMINet-with-Weighted-Loss"><a href="#Team-Badminseok-at-IJCAI-CoachAI-Badminton-Challenge-2023-Multi-Layer-Multi-Input-Transformer-Network-MuLMINet-with-Weighted-Loss" class="headerlink" title="Team Badminseok at IJCAI CoachAI Badminton Challenge 2023: Multi-Layer Multi-Input Transformer Network (MuLMINet) with Weighted Loss"></a>Team Badminseok at IJCAI CoachAI Badminton Challenge 2023: Multi-Layer Multi-Input Transformer Network (MuLMINet) with Weighted Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08262">http://arxiv.org/abs/2307.08262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stan5dard/IJCAI-CoachAI-Challenge-2023">https://github.com/stan5dard/IJCAI-CoachAI-Challenge-2023</a></li>
<li>paper_authors: Minwoo Seong, Jeongseok Oh, SeungJun Kim</li>
<li>for: 这个研究是为了使用人工智能技术（AI）来分析羽毛球比赛的资料，以便更好地评估策略和训练计划。</li>
<li>methods: 这个研究使用了多层多输入变数推导器网络（Multi-Layer Multi-Input Transformer Network，简称MuLMINet），利用了职业羽毛球选手比赛资料来准确地预测未来的球型和位置坐标。</li>
<li>results: 这个研究的结果是在IJCAI CoachAI Badminton Challenge 2023, Track 2中获得亚军（第二名）。此外，我们也将我们的代码公开在线上，以便对更广泛的研究社区做出贡献，并帮助进一步推动人工智能在体育分析领域的发展。<details>
<summary>Abstract</summary>
The increasing use of artificial intelligence (AI) technology in turn-based sports, such as badminton, has sparked significant interest in evaluating strategies through the analysis of match video data. Predicting future shots based on past ones plays a vital role in coaching and strategic planning. In this study, we present a Multi-Layer Multi-Input Transformer Network (MuLMINet) that leverages professional badminton player match data to accurately predict future shot types and area coordinates. Our approach resulted in achieving the runner-up (2nd place) in the IJCAI CoachAI Badminton Challenge 2023, Track 2. To facilitate further research, we have made our code publicly accessible online, contributing to the broader research community's knowledge and advancements in the field of AI-assisted sports analysis.
</details>
<details>
<summary>摘要</summary>
人工智能技术在回合性体育运动，如羽毛球，的应用越来越普遍，导致评估策略通过对比赛视频数据进行分析得到了广泛的关注。预测未来的击球种类和位置坐标是训练和战略规划中非常重要的一环。在这种研究中，我们介绍了一种多层多输入变换器网络（MuLMINet），利用专业羽毛球运动员的比赛数据来准确预测未来的击球种类和位置坐标。我们的方法在IJCAI CoachAI Badminton Challenge 2023年度赛事中获得了亚军（第二名），轨迹2。为了促进进一步的研究，我们在线上公开了我们的代码，对广泛的研究社区的知识和进步在人工智能辅助体育分析领域做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Transferable-Graph-Neural-Fingerprint-Models-for-Quick-Response-to-Future-Bio-Threats"><a href="#Transferable-Graph-Neural-Fingerprint-Models-for-Quick-Response-to-Future-Bio-Threats" class="headerlink" title="Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats"></a>Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01921">http://arxiv.org/abs/2308.01921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Chen, Yihui Ren, Ai Kagawa, Matthew R. Carbone, Samuel Yen-Chi Chen, Xiaohui Qu, Shinjae Yoo, Austin Clyde, Arvind Ramanathan, Rick L. Stevens, Hubertus J. J. van Dam, Deyu Liu<br>for:This paper aims to develop a high-throughput virtual screening method for COVID-19 drug discovery using graph neural fingerprints.methods:The authors use a dataset of 300,000 drug candidates and 23 coronavirus protein targets to train graph neural fingerprint docking models, which show high prediction accuracy with a mean squared error of less than 0.21 kcal&#x2F;mol. They also propose a transferable graph neural fingerprint method trained on multiple targets, which exhibits comparable accuracy to target-specific models with superior training and data efficiency.results:The authors achieve significant improvement over conventional circular fingerprint methods in predicting docking scores, and demonstrate the transferability of their approach to unknown targets. They highlight the potential of their method for fast virtual ligand screening in the future battle against bio-threats.<details>
<summary>Abstract</summary>
Fast screening of drug molecules based on the ligand binding affinity is an important step in the drug discovery pipeline. Graph neural fingerprint is a promising method for developing molecular docking surrogates with high throughput and great fidelity. In this study, we built a COVID-19 drug docking dataset of about 300,000 drug candidates on 23 coronavirus protein targets. With this dataset, we trained graph neural fingerprint docking models for high-throughput virtual COVID-19 drug screening. The graph neural fingerprint models yield high prediction accuracy on docking scores with the mean squared error lower than $0.21$ kcal/mol for most of the docking targets, showing significant improvement over conventional circular fingerprint methods. To make the neural fingerprints transferable for unknown targets, we also propose a transferable graph neural fingerprint method trained on multiple targets. With comparable accuracy to target-specific graph neural fingerprint models, the transferable model exhibits superb training and data efficiency. We highlight that the impact of this study extends beyond COVID-19 dataset, as our approach for fast virtual ligand screening can be easily adapted and integrated into a general machine learning-accelerated pipeline to battle future bio-threats.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China. The translation may not be exact, and some nuances or idioms may be lost in translation.
</details></li>
</ul>
<hr>
<h2 id="Where-Did-the-President-Visit-Last-Week-Detecting-Celebrity-Trips-from-News-Articles"><a href="#Where-Did-the-President-Visit-Last-Week-Detecting-Celebrity-Trips-from-News-Articles" class="headerlink" title="Where Did the President Visit Last Week? Detecting Celebrity Trips from News Articles"></a>Where Did the President Visit Last Week? Detecting Celebrity Trips from News Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08721">http://arxiv.org/abs/2307.08721</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangdatalab/celetrip">https://github.com/zhangdatalab/celetrip</a></li>
<li>paper_authors: Kai Peng, Ying Zhang, Shuai Ling, Zhaoru Ke, Haipeng Zhang</li>
<li>for: 这篇论文的目的是开发一种自动检测明星行程的工具，以便进行大规模和网络化分析。</li>
<li>methods: 论文使用文本内容图模型和注意力机制来处理新闻文章中的旅行信息，并采用特殊的pooling层和节点相似性来减少不相关信息。</li>
<li>results: 论文的提出方法（CeleTrip）在比较baseline模型的测试中，实现了82.53%的F1指标。<details>
<summary>Abstract</summary>
Celebrities' whereabouts are of pervasive importance. For instance, where politicians go, how often they visit, and who they meet, come with profound geopolitical and economic implications. Although news articles contain travel information of celebrities, it is not possible to perform large-scale and network-wise analysis due to the lack of automatic itinerary detection tools. To design such tools, we have to overcome difficulties from the heterogeneity among news articles: 1)One single article can be noisy, with irrelevant people and locations, especially when the articles are long. 2)Though it may be helpful if we consider multiple articles together to determine a particular trip, the key semantics are still scattered across different articles intertwined with various noises, making it hard to aggregate them effectively. 3)Over 20% of the articles refer to the celebrities' trips indirectly, instead of using the exact celebrity names or location names, leading to large portions of trips escaping regular detecting algorithms. We model text content across articles related to each candidate location as a graph to better associate essential information and cancel out the noises. Besides, we design a special pooling layer based on attention mechanism and node similarity, reducing irrelevant information from longer articles. To make up the missing information resulted from indirect mentions, we construct knowledge sub-graphs for named entities (person, organization, facility, etc.). Specifically, we dynamically update embeddings of event entities like the G7 summit from news descriptions since the properties (date and location) of the event change each time, which is not captured by the pre-trained event representations. The proposed CeleTrip jointly trains these modules, which outperforms all baseline models and achieves 82.53% in the F1 metric.
</details>
<details>
<summary>摘要</summary>
celebrities的行踪具有普遍重要性。例如，政要的行程、他们多少次去过、和他们会见的人，都有深刻的地opolitical和经济意义。尽管新闻文章中包含了明星的旅行信息，但由于缺乏自动旅行计划检测工具，因此无法进行大规模的网络化分析。为了设计这些工具，我们需要超越新闻文章中的差异性：1. 一篇文章可能含有噪音，包括不相关的人和地点，特别是文章长。2. 虽然考虑多篇文章可以确定一次行程，但关键 semantics 仍然分散在不同文章中，困难于有效地聚合。3. 更 than 20% 的文章通过间接提到明星的行程，而不是使用明星名称或地点名称，导致大量行程逃逸常规检测算法。我们将文章内容相关的每个候选地点文本内容模型为一个图，以更好地关联关键信息并抑制噪音。此外，我们还设计了基于注意力机制的特殊池化层，以减少长文章中的噪音。为了补做间接提到的信息，我们构建了名实体知识图，包括人名、组织机构、设施等。具体来说，我们在新闻描述中动态更新事件实体表示，以适应每次事件的不同属性（日期和地点），这些属性不是预处理的事件表示所能捕捉。我们提出的 CeleTrip 模型jointly 训练这些模块，并超越所有基准模型，达到了 82.53% 的 F1 度量。
</details></li>
</ul>
<hr>
<h2 id="Lifted-Sequential-Planning-with-Lazy-Constraint-Generation-Solvers"><a href="#Lifted-Sequential-Planning-with-Lazy-Constraint-Generation-Solvers" class="headerlink" title="Lifted Sequential Planning with Lazy Constraint Generation Solvers"></a>Lifted Sequential Planning with Lazy Constraint Generation Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08242">http://arxiv.org/abs/2307.08242</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anubhav-cs/Lcg-Plan">https://github.com/anubhav-cs/Lcg-Plan</a></li>
<li>paper_authors: Anubhav Singh, Miquel Ramirez, Nir Lipovetzky, Peter J. Stuckey</li>
<li>for: 这篇论文探讨了使用惰 clause Generation（LCG）基于方法的含义Programming（CP）来解决序列类 плани约。</li>
<li>methods: 我们提出了一种新的CP模型，基于启示性的卷积编码方法，不需要落实，选择函数和动作架构的选择成为计划设计的一部分。此编码方法不需要编码框架axioms，并不直接将状态表示为决策变量。我们还提出了一种宣传过程，以示LCG可以扩展计划中的推理方法的可能性。</li>
<li>results: 我们对经典IPC和最新提出的benchmark测试了编码和宣传过程，发现对需要 fewer plan step的计划问题，我们的方法与现有最佳序列计划方法相比，表现很好。<details>
<summary>Abstract</summary>
This paper studies the possibilities made open by the use of Lazy Clause Generation (LCG) based approaches to Constraint Programming (CP) for tackling sequential classical planning. We propose a novel CP model based on seminal ideas on so-called lifted causal encodings for planning as satisfiability, that does not require grounding, as choosing groundings for functions and action schemas becomes an integral part of the problem of designing valid plans. This encoding does not require encoding frame axioms, and does not explicitly represent states as decision variables for every plan step. We also present a propagator procedure that illustrates the possibilities of LCG to widen the kind of inference methods considered to be feasible in planning as (iterated) CSP solving. We test encodings and propagators over classic IPC and recently proposed benchmarks for lifted planning, and report that for planning problem instances requiring fewer plan steps our methods compare very well with the state-of-the-art in optimal sequential planning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ROFusion-Efficient-Object-Detection-using-Hybrid-Point-wise-Radar-Optical-Fusion"><a href="#ROFusion-Efficient-Object-Detection-using-Hybrid-Point-wise-Radar-Optical-Fusion" class="headerlink" title="ROFusion: Efficient Object Detection using Hybrid Point-wise Radar-Optical Fusion"></a>ROFusion: Efficient Object Detection using Hybrid Point-wise Radar-Optical Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08233">http://arxiv.org/abs/2307.08233</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuliu-55/rofusion">https://github.com/liuliu-55/rofusion</a></li>
<li>paper_authors: Liu Liu, Shuaifeng Zhi, Zhenhua Du, Li Liu, Xinyu Zhang, Kai Huo, Weidong Jiang</li>
<li>for: 这篇论文是针对自动驾驶和智能代理的Radar感知技术进行研究，以提高Radar感知的精度和可靠性。</li>
<li>methods: 本研究采用混合点子标准方法，融合Radar和摄像头数据，以获得多Modal特征表现。此外，本研究还提出了一个新的本地坐标表示方法，实现了对物体检测任务的对象中心坐标。</li>
<li>results: 实验结果显示，与光学图像获得信息相结合后，我们可以实现97.69%的检测精度（与最新的State-of-the-art方法FFT-RadNet的82.86% recall相比）。实验结果还显示了我们的设计选择和实现可行性。<details>
<summary>Abstract</summary>
Radars, due to their robustness to adverse weather conditions and ability to measure object motions, have served in autonomous driving and intelligent agents for years. However, Radar-based perception suffers from its unintuitive sensing data, which lack of semantic and structural information of scenes. To tackle this problem, camera and Radar sensor fusion has been investigated as a trending strategy with low cost, high reliability and strong maintenance. While most recent works explore how to explore Radar point clouds and images, rich contextual information within Radar observation are discarded. In this paper, we propose a hybrid point-wise Radar-Optical fusion approach for object detection in autonomous driving scenarios. The framework benefits from dense contextual information from both the range-doppler spectrum and images which are integrated to learn a multi-modal feature representation. Furthermore, we propose a novel local coordinate formulation, tackling the object detection task in an object-centric coordinate. Extensive results show that with the information gained from optical images, we could achieve leading performance in object detection (97.69\% recall) compared to recent state-of-the-art methods FFT-RadNet (82.86\% recall). Ablation studies verify the key design choices and practicability of our approach given machine generated imperfect detections. The code will be available at https://github.com/LiuLiu-55/ROFusion.
</details>
<details>
<summary>摘要</summary>
雷达因其在不利天气条件下的强健性和能量测量对象运动而服务了多年。然而，雷达感知数据缺乏场景中semantic和结构信息，这使得雷达基于感知难以取得准确的对象检测结果。为解决这问题，Camera和雷达传感器融合已被视为一种流行的策略，具有低成本、高可靠性和强维护性。然而，大多数最新的研究都在探索如何探索雷达点云和图像，而忽略了雷达观测中的丰富contextual信息。在这篇论文中，我们提出了一种混合点位雷达-光学拟合方法，用于自动驾驶场景中的对象检测。该框架利用雷达和光学图像中的稠密contextual信息，将其集成到一个多Modal特征表示中。此外，我们还提出了一种新的本地坐标系表示方法，解决对象检测任务在对象中心坐标系中进行。我们的实验结果显示，通过在光学图像中获取更多的信息，我们可以在自动驾驶场景中实现97.69%的回归率（相比最新的状态艺术法FFT-RadNet的82.86%回归率）。我们还进行了ablation研究，以验证我们的设计方案和实现的可行性。我们的代码将在https://github.com/LiuLiu-55/ROFusion上提供。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Scalable-Transactional-Stream-Processing-for-Managing-Large-Language-Models-Vision"><a href="#Harnessing-Scalable-Transactional-Stream-Processing-for-Managing-Large-Language-Models-Vision" class="headerlink" title="Harnessing Scalable Transactional Stream Processing for Managing Large Language Models [Vision]"></a>Harnessing Scalable Transactional Stream Processing for Managing Large Language Models [Vision]</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08225">http://arxiv.org/abs/2307.08225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuhao Zhang, Xianzhi Zeng, Yuhao Wu, Zhonghao Yang</li>
<li>for: 本研究旨在探讨大语言模型（LLM）在实时决策环境中的应用，以提高快速、准确、并并发响应的能力。</li>
<li>methods: 本研究提出了一种名为 TStreamLLM 的新框架，它将流处理（TSP）和 LLM 管理集成在一起，以实现高可扩展性和低延迟。</li>
<li>results: 实验结果表明，TStreamLLM 可以高效地处理连续并发的 LLM 更新和使用请求，并且可以在实时患者监测和智能交通管理等应用中提供remarkable的性能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated extraordinary performance across a broad array of applications, from traditional language processing tasks to interpreting structured sequences like time-series data. Yet, their effectiveness in fast-paced, online decision-making environments requiring swift, accurate, and concurrent responses poses a significant challenge. This paper introduces TStreamLLM, a revolutionary framework integrating Transactional Stream Processing (TSP) with LLM management to achieve remarkable scalability and low latency. By harnessing the scalability, consistency, and fault tolerance inherent in TSP, TStreamLLM aims to manage continuous & concurrent LLM updates and usages efficiently. We showcase its potential through practical use cases like real-time patient monitoring and intelligent traffic management. The exploration of synergies between TSP and LLM management can stimulate groundbreaking developments in AI and database research. This paper provides a comprehensive overview of challenges and opportunities in this emerging field, setting forth a roadmap for future exploration and development.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Self-Assembling-Artificial-Neural-Networks-through-Neural-Developmental-Programs"><a href="#Towards-Self-Assembling-Artificial-Neural-Networks-through-Neural-Developmental-Programs" class="headerlink" title="Towards Self-Assembling Artificial Neural Networks through Neural Developmental Programs"></a>Towards Self-Assembling Artificial Neural Networks through Neural Developmental Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08197">http://arxiv.org/abs/2307.08197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elias Najarro, Shyam Sudhakaran, Sebastian Risi</li>
<li>for: 这个论文的目的是研究如何使用自适应的 neural network 进行自我组织和增长，以优化机器学习性能。</li>
<li>methods: 这个论文使用的方法是通过 Neural Developmental Program (NDP) 来引导 neural network 的发展和自我组织，NDP 通过本地通信来操作。</li>
<li>results: 研究发现，通过使用 NDP 来引导 neural network 的发展和自我组织，可以在不同的机器学习任务和优化方法（包括演化训练、在线 RL、离线 RL 和监督学习）中获得优化的性能。<details>
<summary>Abstract</summary>
Biological nervous systems are created in a fundamentally different way than current artificial neural networks. Despite its impressive results in a variety of different domains, deep learning often requires considerable engineering effort to design high-performing neural architectures. By contrast, biological nervous systems are grown through a dynamic self-organizing process. In this paper, we take initial steps toward neural networks that grow through a developmental process that mirrors key properties of embryonic development in biological organisms. The growth process is guided by another neural network, which we call a Neural Developmental Program (NDP) and which operates through local communication alone. We investigate the role of neural growth on different machine learning benchmarks and different optimization methods (evolutionary training, online RL, offline RL, and supervised learning). Additionally, we highlight future research directions and opportunities enabled by having self-organization driving the growth of neural networks.
</details>
<details>
<summary>摘要</summary>
（biological nervous systems are created in a fundamentally different way than current artificial neural networks, deep learning often requires considerable engineering effort to design high-performing neural architectures, but biological nervous systems are grown through a dynamic self-organizing process, in this paper, we take initial steps toward neural networks that grow through a developmental process that mirrors key properties of embryonic development in biological organisms, the growth process is guided by another neural network called Neural Developmental Program (NDP) and which operates through local communication alone, we investigate the role of neural growth on different machine learning benchmarks and different optimization methods, and highlight future research directions and opportunities enabled by having self-organization driving the growth of neural networks）
</details></li>
</ul>
<hr>
<h2 id="HOPE-High-order-Polynomial-Expansion-of-Black-box-Neural-Networks"><a href="#HOPE-High-order-Polynomial-Expansion-of-Black-box-Neural-Networks" class="headerlink" title="HOPE: High-order Polynomial Expansion of Black-box Neural Networks"></a>HOPE: High-order Polynomial Expansion of Black-box Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08192">http://arxiv.org/abs/2307.08192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harrypotterxtx/hope">https://github.com/harrypotterxtx/hope</a></li>
<li>paper_authors: Tingxiong Xiao, Weihang Zhang, Yuxiao Cheng, Jinli Suo</li>
<li>for: 提高深度神经网络的可解释性和应用广泛性</li>
<li>methods: 使用高阶多项式扩展法拓展神经网络，计算高阶导数规则，并从导数中获得神经网络的本地解释</li>
<li>results: 提出了一种高精度、低计算复杂度、好 converges的方法，并在深度学习中应用于功能探索、快速推理和特征选择等领域<details>
<summary>Abstract</summary>
Despite their remarkable performance, deep neural networks remain mostly ``black boxes'', suggesting inexplicability and hindering their wide applications in fields requiring making rational decisions. Here we introduce HOPE (High-order Polynomial Expansion), a method for expanding a network into a high-order Taylor polynomial on a reference input. Specifically, we derive the high-order derivative rule for composite functions and extend the rule to neural networks to obtain their high-order derivatives quickly and accurately. From these derivatives, we can then derive the Taylor polynomial of the neural network, which provides an explicit expression of the network's local interpretations. Numerical analysis confirms the high accuracy, low computational complexity, and good convergence of the proposed method. Moreover, we demonstrate HOPE's wide applications built on deep learning, including function discovery, fast inference, and feature selection. The code is available at https://github.com/HarryPotterXTX/HOPE.git.
</details>
<details>
<summary>摘要</summary>
尽管深度神经网络表现很出色，但它们仍然被称为“黑盒子”，表明它们的工作机制不够清晰，从而限制了它们在需要作出合理决策的领域的广泛应用。在这里，我们介绍了一种方法 called HOPE（高阶多项式扩展），它可以将神经网络扩展成一个高阶泰勒多项式在参考输入上。我们 derivated the high-order derivative rule for composite functions, and extend the rule to neural networks to obtain their high-order derivatives quickly and accurately. From these derivatives, we can then derive the Taylor polynomial of the neural network, which provides an explicit expression of the network's local interpretations.数值分析表明我们的方法具有高准确性、低计算复杂性和好 converge 性。此外，我们还证明了HOPE的广泛应用基础深度学习，包括函数发现、快速推理和特征选择。代码可以在https://github.com/HarryPotterXTX/HOPE.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Mini-Giants-“Small”-Language-Models-and-Open-Source-Win-Win"><a href="#Mini-Giants-“Small”-Language-Models-and-Open-Source-Win-Win" class="headerlink" title="Mini-Giants: “Small” Language Models and Open Source Win-Win"></a>Mini-Giants: “Small” Language Models and Open Source Win-Win</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08189">http://arxiv.org/abs/2307.08189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengping Zhou, Lezhi Li, Xinxi Chen, Andy Li</li>
<li>for: 本文主要针对小语言模型的研究和应用。</li>
<li>methods: 本文使用了开源社区如Kaggle和小语言模型的技术实现。</li>
<li>results: 本文对小语言模型的比较和评估，并介绍了其应用场景在现实世界中。Here’s a more detailed explanation of each point:</li>
<li>for: The paper is primarily focused on the research and application of small language models.</li>
<li>methods: The paper uses open source communities like Kaggle and small language models to achieve its goals.</li>
<li>results: The paper compares and evaluates small language models, and introduces their application scenarios in the real world.<details>
<summary>Abstract</summary>
ChatGPT is phenomenal. However, it is prohibitively expensive to train and refine such giant models. Fortunately, small language models are flourishing and becoming more and more competent. We call them "mini-giants". We argue that open source community like Kaggle and mini-giants will win-win in many ways, technically, ethically and socially. In this article, we present a brief yet rich background, discuss how to attain small language models, present a comparative study of small language models and a brief discussion of evaluation methods, discuss the application scenarios where small language models are most needed in the real world, and conclude with discussion and outlook.
</details>
<details>
<summary>摘要</summary>
chatgpt是非常出色的，但它的训练和精细化成本却 prohibitively expensive。幸运的是，小语言模型在繁殖和成熔中进步不断。我们称之为“小巨人”。我们认为，开源社区如Kaggle和小巨人在技术、伦理和社会层次上都将取得胜利。在这篇文章中，我们将提供简洁而丰富的背景，讨论如何获得小语言模型，进行小语言模型的比较研究， briefly discuss evaluation methods，讨论实际场景中小语言模型最需要的应用场景，并结束 WITH discussion and outlook。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Investigation-of-Pre-trained-Model-Selection-for-Out-of-Distribution-Generalization-and-Calibration"><a href="#An-Empirical-Investigation-of-Pre-trained-Model-Selection-for-Out-of-Distribution-Generalization-and-Calibration" class="headerlink" title="An Empirical Investigation of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration"></a>An Empirical Investigation of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08187">http://arxiv.org/abs/2307.08187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiroki Naganuma, Ryuichiro Hataya</li>
<li>for: 提高out-of-distribution泛化性能和预测不确定性</li>
<li>methods: 研究预训练模型选择对finetuning中的out-of-distribution性能和预测不确定性的影响</li>
<li>results: 结果表明预训练模型选择对out-of-distribution性能有显著影响，大型模型表现较好，但需要进一步研究memorization和真正的泛化之间的平衡。<details>
<summary>Abstract</summary>
In the realm of out-of-distribution generalization tasks, finetuning has risen as a key strategy. While the most focus has been on optimizing learning algorithms, our research highlights the influence of pre-trained model selection in finetuning on out-of-distribution performance and inference uncertainty. Balancing model size constraints of a single GPU, we examined the impact of varying pre-trained datasets and model parameters on performance metrics like accuracy and expected calibration error. Our findings underscore the significant influence of pre-trained model selection, showing marked performance improvements over algorithm choice. Larger models outperformed others, though the balance between memorization and true generalization merits further investigation. Ultimately, our research emphasizes the importance of pre-trained model selection for enhancing out-of-distribution generalization.
</details>
<details>
<summary>摘要</summary>
在外域泛化普通化任务中，微调得到了关键策略的地位。虽然大多数注意力集中在学习算法优化上，但我们的研究表明预训练模型选择对于外域性能和推理不确定性具有重要影响。我们在单个GPU的模型大小限制下，研究了不同预训练数据集和模型参数对性能指标如准确率和预期抽象误差的影响。我们的发现表明预训练模型选择具有显著的影响，大型模型表现较好，但是要权衡记忆和真正的普通化问题还需要进一步研究。最终，我们的研究强调预训练模型选择对于提高外域普通化的重要性。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Faithfulness-in-Chain-of-Thought-Reasoning"><a href="#Measuring-Faithfulness-in-Chain-of-Thought-Reasoning" class="headerlink" title="Measuring Faithfulness in Chain-of-Thought Reasoning"></a>Measuring Faithfulness in Chain-of-Thought Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13702">http://arxiv.org/abs/2307.13702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, Ethan Perez</li>
<li>for: 这个论文的目的是研究语言模型（LLMs）是否在回答问题时能够提供 faithful（忠实）的解释。</li>
<li>methods: 这个论文使用 intervening 方法来检查 LLMS 是否真正地使用 chain-of-thought（CoT）reasoning 来回答问题。</li>
<li>results: 研究发现，LLMS 在不同任务上 exhibit 大量的差异，有时会强烈依赖 CoT，有时却忽略它。CoT 的性能提升不仅来自于 CoT 的额外计算，还有来自于模型的其他因素。在大型模型和更强大的能力下，LLMS 的 faithful reasoning 减退。总之，我们的结果表明，CoT 可以是 faithful 的，只要选择合适的任务和模型。<details>
<summary>Abstract</summary>
Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)大型语言模型（LLM）在回答问题时会表现更好，但是不清楚它们的具体逻辑是否 faithful（即回答问题的过程）。我们 investigate hypothesis 表明 CoT 逻辑可能不 faithful，通过对 CoT 进行干预（如添加错误或重塑它）来评估模型的预测变化。我们发现模型在不同任务上对 CoT 的依赖程度有很大的变化，有时产生 heavily 依赖 CoT，有时几乎忽略它。CoT 的性能提升不仅不来自 CoT 的添加测试时计算alone 还是从编码在 CoT 中的信息。随着模型的增大和能力的提高，它们在大多数任务上表现出 less faithful reasoning。总之，我们的结果表明，CoT 可以是 faithful，只要选择合适的任务和模型大小。
</details></li>
</ul>
<hr>
<h2 id="Question-Decomposition-Improves-the-Faithfulness-of-Model-Generated-Reasoning"><a href="#Question-Decomposition-Improves-the-Faithfulness-of-Model-Generated-Reasoning" class="headerlink" title="Question Decomposition Improves the Faithfulness of Model-Generated Reasoning"></a>Question Decomposition Improves the Faithfulness of Model-Generated Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11768">http://arxiv.org/abs/2307.11768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anthropics/decompositionfaithfulnesspaper">https://github.com/anthropics/decompositionfaithfulnesspaper</a></li>
<li>paper_authors: Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Sam McCandlish, Sheer El Showk, Tamera Lanham, Tim Maxwell, Venkatesa Chandrasekaran, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, Ethan Perez</li>
<li>for: 帮助验证大型自然语言模型（LLM）的正确性和安全性。</li>
<li>methods: 使用Chain-of-Thought（CoT）来询问模型，并让模型生成步骤 reasoning 来回答问题。</li>
<li>results: 通过划分问题为子问题来提高模型生成的 reasoning 的准确性，并在一些最近提出的指标上达到了类似于 CoT 的性能，同时改善了模型生成的 reasoning 的准确性。<details>
<summary>Abstract</summary>
As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.
</details>
<details>
<summary>摘要</summary>
To improve the faithfulness of CoT reasoning, we have developed a method that involves decomposing questions into subquestions. This approach achieves strong performance on question-answering tasks and sometimes approaches the performance of CoT while improving the faithfulness of the model's stated reasoning on several recently proposed metrics.By forcing the model to answer simpler subquestions in separate contexts, we significantly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show that it is possible to improve the faithfulness of model-generated reasoning, and continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.
</details></li>
</ul>
<hr>
<h2 id="In-IDE-Generation-based-Information-Support-with-a-Large-Language-Model"><a href="#In-IDE-Generation-based-Information-Support-with-a-Large-Language-Model" class="headerlink" title="In-IDE Generation-based Information Support with a Large Language Model"></a>In-IDE Generation-based Information Support with a Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08177">http://arxiv.org/abs/2307.08177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, Brad Myers</li>
<li>for: 这个论文的目的是研究一种基于大语言模型（LLM）的代码理解UI，以帮助开发者更好地理解代码。</li>
<li>methods: 该论文使用了OpenAI的GPT-3.5和GPT-4模型，通过在IDE中直接建立一个启用对话UI，让开发者可以通过高级请求（ без需要写明文）来请求模型对代码进行解释、提供API调用详细信息、解释领域特有术语以及提供API使用示例。</li>
<li>results: 该论文的用户研究显示，使用该系统可以帮助开发者更快速地完成任务，并且在开发者中间的学生和专业人员之间存在显著的使用和感受差异。研究结果表明，在IDE中基于LLM的启用对话UI是未来工具建造的有前途的方向。<details>
<summary>Abstract</summary>
Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5 and GPT-4 models with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ significantly between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.
</details>
<details>
<summary>摘要</summary>
理解代码是困难的，尤其是在新和复杂的开发环境中。代码注释和文档可以帮助，但通常罕见或难以浏览。大型自然语言模型（LLM）正在改变代码写作的过程。可以做到同样的事情吗？在这个研究中，我们提供了一个基于 LLM 的对话式 UI，用于在 IDE 中帮助理解代码。我们的 IDE 插件会向 OpenAI 的 GPT-3.5 和 GPT-4 模型提交四种高级请求，无需用户写明文提示：解释选中代码段落，提供 API 调用使用情况，解释领域特定术语，并提供 API 使用示例。插件还允许开放式提示，这些提示会自动Contextualized 到 LLM 中的当前编辑程序。我们在32名参与者的用户研究中证明，使用我们的插件可以提高任务完成度比 web 搜索更高。我们还进行了对系统的使用和认可的全面分析，其中发现了开发者的使用和认可方式存在很大差异，学生和专业人员的使用和利用方式不同。我们认为，在 IDE 中无需提交提示的 LLM 交互是未来工具制造者的承诺。
</details></li>
</ul>
<hr>
<h2 id="Credit-Assignment-Challenges-and-Opportunities-in-Developing-Human-like-AI-Agents"><a href="#Credit-Assignment-Challenges-and-Opportunities-in-Developing-Human-like-AI-Agents" class="headerlink" title="Credit Assignment: Challenges and Opportunities in Developing Human-like AI Agents"></a>Credit Assignment: Challenges and Opportunities in Developing Human-like AI Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08171">http://arxiv.org/abs/2307.08171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thuy Ngoc Nguyen, Chase McDonald, Cleotilde Gonzalez</li>
<li>for: 这种研究旨在探讨人类如何处理延迟反馈，以及计算机方法如TD方法在人工智能中是否准确反映人类行为。</li>
<li>methods: 该研究使用了一种基于经验决策理论的认知模型，Instance-Based Learning Theory (IBLT)，测试不同的信任分配机制在目标寻找 Navigation 任务中的表现。</li>
<li>results: 研究发现，(1) 一个给所有决策平等信任分配的 IBLT 模型能够更好地匹配人类表现，比其他模型更高效；(2) IBL-TD 和 Q-学习模型在开始时 initially 下遇到困难，但 eventually 超越人类表现；(3) 人类决策 Complexity 会影响决策，而模型则不会。<details>
<summary>Abstract</summary>
Temporal credit assignment is crucial for learning and skill development in natural and artificial intelligence. While computational methods like the TD approach in reinforcement learning have been proposed, it's unclear if they accurately represent how humans handle feedback delays. Cognitive models intend to represent the mental steps by which humans solve problems and perform a number of tasks, but limited research in cognitive science has addressed the credit assignment problem in humans and cognitive models. Our research uses a cognitive model based on a theory of decisions from experience, Instance-Based Learning Theory (IBLT), to test different credit assignment mechanisms in a goal-seeking navigation task with varying levels of decision complexity. Instance-Based Learning (IBL) models simulate the process of making sequential choices with different credit assignment mechanisms, including a new IBL-TD model that combines the IBL decision mechanism with the TD approach. We found that (1) An IBL model that gives equal credit assignment to all decisions is able to match human performance better than other models, including IBL-TD and Q-learning; (2) IBL-TD and Q-learning models underperform compared to humans initially, but eventually, they outperform humans; (3) humans are influenced by decision complexity, while models are not. Our study provides insights into the challenges of capturing human behavior and the potential opportunities to use these models in future AI systems to support human activities.
</details>
<details>
<summary>摘要</summary>
时间归属是学习和技能发展中非常重要的因素，而计算方法如TD方法在强化学习中已经被提出，但是不清楚这些方法是否准确地表现出人类对延迟反馈的处理方式。认知模型旨在表现人类解决问题和完成任务的心理步骤，但是认知科学中对归属问题的研究很少。我们的研究使用基于经验决策理论的认知模型（Instance-Based Learning Theory，IBLT）测试不同的归属机制在具有不同决策复杂度的目标寻找 Navigation 任务中的表现。Instance-Based Learning（IBL）模型模拟了在发送连续选择时使用不同归属机制的过程，其中包括一种新的IBL-TD模型，该模型结合IBL决策机制和TD方法。我们发现：1. 给所有决策平等的归属机制的IBL模型能够更好地与人类表现相符，比其他模型，包括IBL-TD和Q学习模型。2. IBL-TD和Q学习模型在初期比人类表现更差，但是在后期它们超越了人类表现。3. 人类受到决策复杂度的影响，而模型则不是。我们的研究提供了人类行为的挑战和未来AI系统中使用这些模型的可能性。
</details></li>
</ul>
<hr>
<h2 id="Computing-the-gradients-with-respect-to-all-parameters-of-a-quantum-neural-network-using-a-single-circuit"><a href="#Computing-the-gradients-with-respect-to-all-parameters-of-a-quantum-neural-network-using-a-single-circuit" class="headerlink" title="Computing the gradients with respect to all parameters of a quantum neural network using a single circuit"></a>Computing the gradients with respect to all parameters of a quantum neural network using a single circuit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08167">http://arxiv.org/abs/2307.08167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gphehub/grad2210">https://github.com/gphehub/grad2210</a></li>
<li>paper_authors: Guang Ping He</li>
<li>for: 计算量子神经网络参数shift规则中的函数值时，需要计算两次函数值，一次为单个可变参数的gradient。当总参数数量高时，量子电路需要调整和运行多次。我们提出了一种方法，可以通过单个电路计算所有的gradient，减少电路深度和经典注册数量。</li>
<li>methods: 我们提出的方法使用单个电路计算所有的gradient，减少电路深度和经典注册数量。</li>
<li>results: 我们实验表明，使用我们的方法可以在量子硬件和模拟器上减少 compile时间，从而提高总时间的速度。<details>
<summary>Abstract</summary>
When computing the gradients of a quantum neural network using the parameter-shift rule, the cost function needs to be calculated twice for the gradient with respect to a single adjustable parameter of the network. When the total number of parameters is high, the quantum circuit for the computation has to be adjusted and run for many times. Here we propose an approach to compute all the gradients using a single circuit only, with a much reduced circuit depth and less classical registers. We also demonstrate experimentally, on both real quantum hardware and simulator, that our approach has the advantages that the circuit takes a significantly shorter time to compile than the conventional approach, resulting in a speedup on the total runtime.
</details>
<details>
<summary>摘要</summary>
当计算量子神经网络的梯度使用参数调整规则时，需要计算两次函数值，即每个可调参数的梯度。当总参数数量很高时，量子电路需要调整并运行多次。我们提出了一种方法，可以通过单个电路计算所有梯度，减少电路深度和类别 региsters。我们还实验ally示出，使用我们的方法可以在真正的量子硬件和模拟器上实现速度减少。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Quality-of-Multiple-Choice-Questions-Using-GPT-4-and-Rule-Based-Methods"><a href="#Assessing-the-Quality-of-Multiple-Choice-Questions-Using-GPT-4-and-Rule-Based-Methods" class="headerlink" title="Assessing the Quality of Multiple-Choice Questions Using GPT-4 and Rule-Based Methods"></a>Assessing the Quality of Multiple-Choice Questions Using GPT-4 and Rule-Based Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08161">http://arxiv.org/abs/2307.08161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevenjamesmoore/ectel23">https://github.com/stevenjamesmoore/ectel23</a></li>
<li>paper_authors: Steven Moore, Huy A. Nguyen, Tianying Chen, John Stamper</li>
<li>for:  This paper aims to assess the quality of multiple-choice questions and identify common item-writing flaws present in student-generated questions.</li>
<li>methods:  The paper compares the performance of a rule-based method and a machine-learning based method (GPT-4) in automatically assessing multiple-choice questions for item-writing flaws.</li>
<li>results:  The rule-based method correctly detected 91% of the flaws identified by human annotators, outperforming GPT-4 which detected 79% of the flaws. The study demonstrates the effectiveness of the two methods in identifying common item-writing flaws present in student-generated questions across different subject areas.<details>
<summary>Abstract</summary>
Multiple-choice questions with item-writing flaws can negatively impact student learning and skew analytics. These flaws are often present in student-generated questions, making it difficult to assess their quality and suitability for classroom usage. Existing methods for evaluating multiple-choice questions often focus on machine readability metrics, without considering their intended use within course materials and their pedagogical implications. In this study, we compared the performance of a rule-based method we developed to a machine-learning based method utilizing GPT-4 for the task of automatically assessing multiple-choice questions based on 19 common item-writing flaws. By analyzing 200 student-generated questions from four different subject areas, we found that the rule-based method correctly detected 91% of the flaws identified by human annotators, as compared to 79% by GPT-4. We demonstrated the effectiveness of the two methods in identifying common item-writing flaws present in the student-generated questions across different subject areas. The rule-based method can accurately and efficiently evaluate multiple-choice questions from multiple domains, outperforming GPT-4 and going beyond existing metrics that do not account for the educational use of such questions. Finally, we discuss the potential for using these automated methods to improve the quality of questions based on the identified flaws.
</details>
<details>
<summary>摘要</summary>
多个选项问题的编写问题可以负面影响学生学习和估计数据。这些问题经常出现在学生自己编写的问题中，使得评估其质量和教学意义困难。现有的评估多个选项问题方法通常会专注于机器可读性指标，不考虑它们在课程材料中的用途和教学意义。在这个研究中，我们比较了我们开发的规则基于方法和GPT-4机器学习方法在自动评估多个选项问题中的表现。通过分析4个不同学科的200个学生自己编写的问题，我们发现了规则基于方法可以准确地检测91%的人类标注员标记的问题，比GPT-4的79%高。我们证明了两种方法在不同学科的学生自己编写的问题中具有普遍性和可靠性，并超过了不考虑教学用途的现有指标。最后，我们讨论了使用自动方法来改进问题的质量基于发现的潜在问题。
</details></li>
</ul>
<hr>
<h2 id="POA-Passable-Obstacles-Aware-Path-planning-Algorithm-for-Navigation-of-a-Two-wheeled-Robot-in-Highly-Cluttered-Environments"><a href="#POA-Passable-Obstacles-Aware-Path-planning-Algorithm-for-Navigation-of-a-Two-wheeled-Robot-in-Highly-Cluttered-Environments" class="headerlink" title="POA: Passable Obstacles Aware Path-planning Algorithm for Navigation of a Two-wheeled Robot in Highly Cluttered Environments"></a>POA: Passable Obstacles Aware Path-planning Algorithm for Navigation of a Two-wheeled Robot in Highly Cluttered Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08141">http://arxiv.org/abs/2307.08141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Petrovsky, Yomna Youssef, Kirill Myasoedov, Artem Timoshenko, Vladimir Guneavoi, Ivan Kalinov, Dzmitry Tsetserukou</li>
<li>for: 这个论文是为了提出一种新的导航方法，使两轮机器人在受限的环境中能够穿越障碍物。</li>
<li>methods: 这个算法可以探测和分类障碍物，并将障碍物分为两类：可通过和不可通过。该算法允许两轮机器人找到通过障碍物的路径。</li>
<li>results: 与标准导航算法相比，这个方法可以降低路径长度和总旅行时间，最多降低43%和39%。<details>
<summary>Abstract</summary>
This paper focuses on Passable Obstacles Aware (POA) planner - a novel navigation method for two-wheeled robots in a highly cluttered environment. The navigation algorithm detects and classifies objects to distinguish two types of obstacles - passable and unpassable. Our algorithm allows two-wheeled robots to find a path through passable obstacles. Such a solution helps the robot working in areas inaccessible to standard path planners and find optimal trajectories in scenarios with a high number of objects in the robot's vicinity. The POA planner can be embedded into other planning algorithms and enables them to build a path through obstacles. Our method decreases path length and the total travel time to the final destination up to 43% and 39%, respectively, comparing to standard path planners such as GVD, A*, and RRT*
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Heterogeneous-graphs-model-spatial-relationships-between-biological-entities-for-breast-cancer-diagnosis"><a href="#Heterogeneous-graphs-model-spatial-relationships-between-biological-entities-for-breast-cancer-diagnosis" class="headerlink" title="Heterogeneous graphs model spatial relationships between biological entities for breast cancer diagnosis"></a>Heterogeneous graphs model spatial relationships between biological entities for breast cancer diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08132">http://arxiv.org/abs/2307.08132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akhila Krishna K, Ravi Kant Gupta, Nikhil Cherian Kurian, Pranav Jeevan, Amit Sethi</li>
<li>for: 这篇论文的目的是提高肝癌预后评估和治疗选择的准确性，通过使用对照网络（GNN）模型来捕捉组织内部细胞和组织之间的空间关系。</li>
<li>methods: 这篇论文使用了一种多元GNN模型，它可以捕捉组织内部细胞和组织之间的空间和层次关系，并且与对照网络（CNN）模型进行比较，以评估其表现。</li>
<li>results: 这篇论文的模型在三个公开可用的肝癌数据集（BRIGHT、BreakHis和BACH）上表现出色，其中包括更高的准确性和较少的参数数量，相比于使用 transformer 架构的现有方法。<details>
<summary>Abstract</summary>
The heterogeneity of breast cancer presents considerable challenges for its early detection, prognosis, and treatment selection. Convolutional neural networks often neglect the spatial relationships within histopathological images, which can limit their accuracy. Graph neural networks (GNNs) offer a promising solution by coding the spatial relationships within images. Prior studies have investigated the modeling of histopathological images as cell and tissue graphs, but they have not fully tapped into the potential of extracting interrelationships between these biological entities. In this paper, we present a novel approach using a heterogeneous GNN that captures the spatial and hierarchical relations between cell and tissue graphs to enhance the extraction of useful information from histopathological images. We also compare the performance of a cross-attention-based network and a transformer architecture for modeling the intricate relationships within tissue and cell graphs. Our model demonstrates superior efficiency in terms of parameter count and achieves higher accuracy compared to the transformer-based state-of-the-art approach on three publicly available breast cancer datasets -- BRIGHT, BreakHis, and BACH.
</details>
<details>
<summary>摘要</summary>
乳癌病例的多样性呈现出了早期发现、预后和治疗选择的很大挑战。卷积神经网络经常忽略图像中的空间关系，这限制了它们的准确性。图гра夫神经网络（GNNs）提供了一个有优势的解决方案，它可以编码图像中的空间关系。先前的研究曾经对压缩细胞和组织图进行模型化，但是它们没有充分利用了抽取生物体系间关系的潜力。在本文中，我们提出了一种新的方法，使用多样性GNN来捕捉图像中细胞和组织图中的空间和层次关系，以提高从图像中提取有用信息的能力。我们还对一个混合注意力网络和一个变换器架构进行比较，以评估它们在模型细胞和组织图中的关系。我们的模型在三个公共可用的乳癌数据集（BRIGHT、BreakHis和BACH）上达到了更高的准确率，并且 Parameters 的数量更少。
</details></li>
</ul>
<hr>
<h2 id="INFLECT-DGNN-Influencer-Prediction-with-Dynamic-Graph-Neural-Networks"><a href="#INFLECT-DGNN-Influencer-Prediction-with-Dynamic-Graph-Neural-Networks" class="headerlink" title="INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks"></a>INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08131">http://arxiv.org/abs/2307.08131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/banking-analytics-lab/inflect">https://github.com/banking-analytics-lab/inflect</a></li>
<li>paper_authors: Elena Tiukhova, Emiliano Penaloza, María Óskarsdóttir, Bart Baesens, Monique Snoeck, Cristián Bravo</li>
<li>For: 本研究旨在适用于referral和targeted marketing中的influencer检测领域，利用动态网络表示来提高预测性能。* Methods: 本研究提出了一种新的framework，名为INFLECT-DGNN，它将Graph Neural Networks (GNN)和Recurrent Neural Networks (RNN)结合使用，并采用Weighted loss函数、Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data，以及一种精心设计的rolling-window策略。* Results: 通过使用RNN来编码时间特征，并与GNNs结合使用，可以显著提高预测性能。对于不同的模型进行比较，研究发现，捕捉图表示、时间相关性和使用财务驱动的评价方法均是非常重要的。<details>
<summary>Abstract</summary>
Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships. To elaborate this idea, we introduce INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural Networks (RNN) with weighted loss functions, the Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted rolling-window strategy. To evaluate predictive performance, we utilize a unique corporate data set with networks of three cities and derive a profit-driven evaluation methodology for influencer prediction. Our results show how using RNN to encode temporal attributes alongside GNNs significantly improves predictive performance. We compare the results of various models to demonstrate the importance of capturing graph representation, temporal dependencies, and using a profit-driven methodology for evaluation.
</details>
<details>
<summary>摘要</summary>
利用网络信息进行预测已经在多个领域广泛应用。在推荐和targeted市场营销方面，Influencer检测是一个可以受益于动态网络表示的领域。为了详细说明这个想法，我们提出了INFLECT-DGNN框架，它将Graph Neural Networks (GNN)和Recurrent Neural Networks (RNN)结合，并使用负权重函数，SMOTE算法适应图数据，以及一种精心制定的滚动窗口策略。为了评估预测性能，我们使用了一个独特的企业数据集，并 derivate了一种基于利润的评估方法ology for influencer prediction。我们的结果表明，使用RNN来编码时间特征并与GNNs结合可以明显提高预测性能。我们对不同的模型进行比较，以示出capturing图表示、时间依赖和使用利润驱动的评估方法ology的重要性。
</details></li>
</ul>
<hr>
<h2 id="A-max-affine-spline-approximation-of-neural-networks-using-the-Legendre-transform-of-a-convex-concave-representation"><a href="#A-max-affine-spline-approximation-of-neural-networks-using-the-Legendre-transform-of-a-convex-concave-representation" class="headerlink" title="A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation"></a>A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09602">http://arxiv.org/abs/2307.09602</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adamgoodtime/legendre_net">https://github.com/adamgoodtime/legendre_net</a></li>
<li>paper_authors: Adam Perrett, Danny Wood, Gavin Brown</li>
<li>for: 本研究提出了一种新的神经网络转换算法，用于将神经网络转换成spline表示形式。与前一些研究不同，这种算法不需要几何和分割平面的约束，而是只需要函数在某些区域具有 bounded 和有定义的第二导数。</li>
<li>methods: 本研究使用了一种新的算法，可以在整个神经网络中进行spline转换，而不是只在每层级独立进行转换。这种算法还可以覆盖整个神经网络，而不是只是在某些层级进行。</li>
<li>results: 实验表明，这种算法可以准确地将神经网络转换成spline表示形式，并且可以在不同的神经网络架构中进行应用。此外，这种算法还可以提取神经网络特征图，从而帮助更好地理解神经网络的工作机理。<details>
<summary>Abstract</summary>
This work presents a novel algorithm for transforming a neural network into a spline representation. Unlike previous work that required convex and piecewise-affine network operators to create a max-affine spline alternate form, this work relaxes this constraint. The only constraint is that the function be bounded and possess a well-define second derivative, although this was shown experimentally to not be strictly necessary. It can also be performed over the whole network rather than on each layer independently. As in previous work, this bridges the gap between neural networks and approximation theory but also enables the visualisation of network feature maps. Mathematical proof and experimental investigation of the technique is performed with approximation error and feature maps being extracted from a range of architectures, including convolutional neural networks.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种新的算法，用于将神经网络转换为spline表示。与前一些工作不同，这个算法不需要几何和分割 affine网络运算符来创建一个最大 affine spline alternate form。而是放宽了这些约束。只需要函数是受限的并具有明确的二阶导数，但是这在实验中未能够很好地证明。此外，这种方法还可以在整个网络中进行，而不仅仅是在每层独立进行。这种方法将神经网络和近似理论相连接，并且允许Feature map的可视化。数学证明和实验研究这种技术，包括卷积神经网络，进行了错误估计和特征图Extraction。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-of-Forgetting-in-Deep-Learning-Beyond-Continual-Learning"><a href="#A-Comprehensive-Survey-of-Forgetting-in-Deep-Learning-Beyond-Continual-Learning" class="headerlink" title="A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning"></a>A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09218">http://arxiv.org/abs/2307.09218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ennengyang/awesome-forgetting-in-deep-learning">https://github.com/ennengyang/awesome-forgetting-in-deep-learning</a></li>
<li>paper_authors: Zhenyi Wang, Enneng Yang, Li Shen, Heng Huang</li>
<li>for: This paper aims to provide a comprehensive survey of forgetting in deep learning, exploring its various manifestations and challenges, and highlighting its potential advantages in certain cases.</li>
<li>methods: The paper draws upon ideas and approaches from various fields that have dealt with forgetting, including continual learning, generative models, and federated learning.</li>
<li>results: The paper presents a nuanced understanding of forgetting and highlights its potential advantages in certain scenarios, such as privacy-preserving scenarios. It also provides a comprehensive list of papers about forgetting in various research fields for future reference.<details>
<summary>Abstract</summary>
Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context, we aim to present a more nuanced understanding of this phenomenon and highlight its potential advantages. Through this comprehensive survey, we aspire to uncover potential solutions by drawing upon ideas and approaches from various fields that have dealt with forgetting. By examining forgetting beyond its conventional boundaries, in future work, we hope to encourage the development of novel strategies for mitigating, harnessing, or even embracing forgetting in real applications. A comprehensive list of papers about forgetting in various research fields is available at \url{https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning}.
</details>
<details>
<summary>摘要</summary>
忘却（Forgetting）是指先前学习或知识的失去或衰退。 existed 的学习surveys 主要集中在持续学习中，但忘却是深度学习其他研究领域中的普遍现象。忘却在生成模型中的 generator shifts 和联合学习中的客户端数据分布不同性等研究领域中出现。 Addressing忘却包括保持过去任务知识与快速学习新任务的 equilibrio, 管理任务干扰与 conflicting goals, 以及防止隐私泄露等挑战。另外，大多数已有的学习surveys 假设忘却总是有害的。然而，我们的survey 认为忘却是一把双刃剑，在某些情况下可以是有利的，如隐私保护场景。通过探讨忘却在更广泛的上下文中，我们希望提供一个更加细腻的理解，并强调其 potential advantages。通过这种全面的survey，我们希望探索可以利用不同领域中的想法和方法来 mitigate、利用或甚至欢迎忘却的实际应用。一个包含各种研究领域中忘却的完整列表可以在 \url{https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning} 上找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/17/cs.AI_2023_07_17/" data-id="cloimip43000ts488egqbhpup" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/17/cs.CL_2023_07_17/" class="article-date">
  <time datetime="2023-07-17T11:00:00.000Z" itemprop="datePublished">2023-07-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/cs.CL_2023_07_17/">cs.CL - 2023-07-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Syntax-Aware-Complex-Valued-Neural-Machine-Translation"><a href="#Syntax-Aware-Complex-Valued-Neural-Machine-Translation" class="headerlink" title="Syntax-Aware Complex-Valued Neural Machine Translation"></a>Syntax-Aware Complex-Valued Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08586">http://arxiv.org/abs/2307.08586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Liu, Yuexian Hou</li>
<li>for: 提高 neural machine translation (NMT) 的翻译性能</li>
<li>methods: 使用复杂值 Encoder-Decoder 架构，并将 syntax 信息直接 интегрирован到 NMT 模型中，使用注意机制来学习 word-level 和 syntax-level 注意力分数</li>
<li>results: 实验结果表明，提出的方法可以在两个数据集上提高 BLEU 分数，尤其是在语言对的 sintactic 差异较大的翻译任务中获得更大的改善。<details>
<summary>Abstract</summary>
Syntax has been proven to be remarkably effective in neural machine translation (NMT). Previous models obtained syntax information from syntactic parsing tools and integrated it into NMT models to improve translation performance. In this work, we propose a method to incorporate syntax information into a complex-valued Encoder-Decoder architecture. The proposed model jointly learns word-level and syntax-level attention scores from the source side to the target side using an attention mechanism. Importantly, it is not dependent on specific network architectures and can be directly integrated into any existing sequence-to-sequence (Seq2Seq) framework. The experimental results demonstrate that the proposed method can bring significant improvements in BLEU scores on two datasets. In particular, the proposed method achieves a greater improvement in BLEU scores in translation tasks involving language pairs with significant syntactic differences.
</details>
<details>
<summary>摘要</summary>
syntax 已经被证明可以在神经机器翻译（NMT）中发挥 Remarkably 的效果。在过去的模型中，从 sintactic parsing 工具中获取了 syntax 信息，然后将其集成到 NMT 模型中，以提高翻译性能。在这项工作中，我们提议一种将 syntax 信息 incorporated 到复杂值 Encoder-Decoder 架构中的方法。该方法使用注意力机制，在源侧到目标侧的翻译过程中同时学习 word-level 和 syntax-level 注意力分数。很重要的是，该方法不依赖于特定的网络架构，可以直接integrated 到任何现有的 sequence-to-sequence（Seq2Seq）框架中。实验结果表明，我们提议的方法可以在两个 dataset 上提供显著的改善，特别是在语言对应的语言对中进行翻译任务时。
</details></li>
</ul>
<hr>
<h2 id="The-Resume-Paradox-Greater-Language-Differences-Smaller-Pay-Gaps"><a href="#The-Resume-Paradox-Greater-Language-Differences-Smaller-Pay-Gaps" class="headerlink" title="The Resume Paradox: Greater Language Differences, Smaller Pay Gaps"></a>The Resume Paradox: Greater Language Differences, Smaller Pay Gaps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08580">http://arxiv.org/abs/2307.08580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua R. Minot, Marc Maier, Bradford Demarest, Nicholas Cheney, Christopher M. Danforth, Peter Sheridan Dodds, Morgan R. Frank</li>
<li>for: 这种研究旨在探讨工作者自我表达如何影响 gender pay gap.</li>
<li>methods: 研究使用美国工作者数百万个简历语言分析gender pay gap.</li>
<li>results: 研究发现，在不同行业中，男女简历语言差异对 gender pay gap 的影响相对较小，但是尽管如此，具有更高语言差异的行业却有较低的gender pay gap. 每年增加两倍语言差异，女性工作者的平均年薪增加2,797美元。<details>
<summary>Abstract</summary>
Over the past decade, the gender pay gap has remained steady with women earning 84 cents for every dollar earned by men on average. Many studies explain this gap through demand-side bias in the labor market represented through employers' job postings. However, few studies analyze potential bias from the worker supply-side. Here, we analyze the language in millions of US workers' resumes to investigate how differences in workers' self-representation by gender compare to differences in earnings. Across US occupations, language differences between male and female resumes correspond to 11% of the variation in gender pay gap. This suggests that females' resumes that are semantically similar to males' resumes may have greater wage parity. However, surprisingly, occupations with greater language differences between male and female resumes have lower gender pay gaps. A doubling of the language difference between female and male resumes results in an annual wage increase of $2,797 for the average female worker. This result holds with controls for gender-biases of resume text and we find that per-word bias poorly describes the variance in wage gap. The results demonstrate that textual data and self-representation are valuable factors for improving worker representations and understanding employment inequities.
</details>
<details>
<summary>摘要</summary>
We find that language differences between male and female resumes account for 11% of the variation in the gender pay gap. Specifically, we find that female resumes that are semantically similar to male resumes are associated with greater wage parity. However, surprisingly, occupations with greater language differences between male and female resumes have lower gender pay gaps.Furthermore, we find that a doubling of the language difference between female and male resumes results in an annual wage increase of $2,797 for the average female worker. This result holds even when controlling for gender-biases of resume text, and we find that per-word bias poorly describes the variance in wage gap.Overall, our results demonstrate that textual data and self-representation are valuable factors for improving worker representations and understanding employment inequities.
</details></li>
</ul>
<hr>
<h2 id="Discovering-collective-narratives-shifts-in-online-discussions"><a href="#Discovering-collective-narratives-shifts-in-online-discussions" class="headerlink" title="Discovering collective narratives shifts in online discussions"></a>Discovering collective narratives shifts in online discussions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08541">http://arxiv.org/abs/2307.08541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanying Zhao, Fiona Guo, Kristina Lerman, Yong-Yeol Ahn</li>
<li>for: This paper aims to develop a systematic and computational understanding of online narratives, specifically in the context of social media, to better understand how they emerge, spread, and die.</li>
<li>methods: The proposed framework combines change point detection, semantic role labeling (SRL), and automatic aggregation of narrative fragments into narrative networks to reliably and automatically extract narratives from massive amounts of text data.</li>
<li>results: The proposed approach is evaluated using synthetic and empirical data from two Twitter corpora related to COVID-19 and the 2017 French Election, and the results demonstrate that the approach can recover major narrative shifts that correspond to significant events.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是为了提供一种系统的计算机理解社交媒体上的故事，以更好地理解它们如何出现、传播和消亡。</li>
<li>methods: 该提议的框架结合变化点检测、Semantic Role Labeling（SRL）和自动聚合故事片断到故事网络，以可靠地和自动地从大量文本数据中提取故事。</li>
<li>results: 该提议的方法在使用生成的和实际数据两个Twitter corpora相关于COVID-19和2017法国大选进行评估，结果表明该方法可以回归主要的故事变化，与主要事件相吻合。<details>
<summary>Abstract</summary>
Narrative is a foundation of human cognition and decision making. Because narratives play a crucial role in societal discourses and spread of misinformation and because of the pervasive use of social media, the narrative dynamics on social media can have profound societal impact. Yet, systematic and computational understanding of online narratives faces critical challenge of the scale and dynamics; how can we reliably and automatically extract narratives from massive amount of texts? How do narratives emerge, spread, and die? Here, we propose a systematic narrative discovery framework that fill this gap by combining change point detection, semantic role labeling (SRL), and automatic aggregation of narrative fragments into narrative networks. We evaluate our model with synthetic and empirical data two-Twitter corpora about COVID-19 and 2017 French Election. Results demonstrate that our approach can recover major narrative shifts that correspond to the major events.
</details>
<details>
<summary>摘要</summary>
叙述是人类认知和决策的基础。因为叙述在社会话语中发挥重要作用，并且在社会媒体上广泛传播谣言，因此在社会媒体上的叙述动态可能有深远的社会影响。然而，系统化和计算化地理解社会媒体上的叙述遇到了重要的挑战，即如何可靠地和自动地提取叙述？叙述是如何产生、传播和死亡的？我们提出了一个系统的叙述发现框架，通过结合变点检测、semantic role labeling（SRL）和自动聚合叙述碎片而填补这个空白。我们使用了两个Twitter数据集，一个是关于COVID-19的，另一个是关于2017年法国大选。结果表明，我们的方法可以重现主要的叙述变化，与主要事件相吻合。
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-Evaluation-of-Topic-Models-and-Dimensionality-Reduction-Methods-for-2D-Text-Spatialization"><a href="#Large-Scale-Evaluation-of-Topic-Models-and-Dimensionality-Reduction-Methods-for-2D-Text-Spatialization" class="headerlink" title="Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization"></a>Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11770">http://arxiv.org/abs/2307.11770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cgshpi/topic-models-and-dimensionality-reduction-benchmark">https://github.com/cgshpi/topic-models-and-dimensionality-reduction-benchmark</a></li>
<li>paper_authors: Daniel Atzberger, Tim Cech, Willy Scheibel, Matthias Trapp, Rico Richter, Jürgen Döllner, Tobias Schreck</li>
<li>For: The paper is written for deriving spatializations for text corpora using topic models and dimensionality reduction methods, and evaluating the effectiveness of these methods for creating high-quality layouts.* Methods: The paper uses a combination of topic models and dimensionality reduction methods, including Latent Dirichlet Allocation (LDA) and t-Distributed Stochastic Neighbor Embedding (t-SNE), to create two-dimensional scatter plots of text corpora.* Results: The paper presents a large-scale computational evaluation of the effectiveness of these methods, using a set of corpora and quality metrics to quantify the preservation of local and global properties and the perceptual effectiveness of the resulting layouts. The results show that interpretable topic models are beneficial for capturing the structure of text corpora, and that t-SNE is a good choice for subsequent dimensionality reduction.<details>
<summary>Abstract</summary>
Topic models are a class of unsupervised learning algorithms for detecting the semantic structure within a text corpus. Together with a subsequent dimensionality reduction algorithm, topic models can be used for deriving spatializations for text corpora as two-dimensional scatter plots, reflecting semantic similarity between the documents and supporting corpus analysis. Although the choice of the topic model, the dimensionality reduction, and their underlying hyperparameters significantly impact the resulting layout, it is unknown which particular combinations result in high-quality layouts with respect to accuracy and perception metrics. To investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots (or basis for landscape-type visualizations), we present a large-scale, benchmark-based computational evaluation. Our evaluation consists of (1) a set of corpora, (2) a set of layout algorithms that are combinations of topic models and dimensionality reductions, and (3) quality metrics for quantifying the resulting layout. The corpora are given as document-term matrices, and each document is assigned to a thematic class. The chosen metrics quantify the preservation of local and global properties and the perceptual effectiveness of the two-dimensional scatter plots. By evaluating the benchmark on a computing cluster, we derived a multivariate dataset with over 45 000 individual layouts and corresponding quality metrics. Based on the results, we propose guidelines for the effective design of text spatializations that are based on topic models and dimensionality reductions. As a main result, we show that interpretable topic models are beneficial for capturing the structure of text corpora. We furthermore recommend the use of t-SNE as a subsequent dimensionality reduction.
</details>
<details>
<summary>摘要</summary>
Topic models 是一类不监督学习算法，用于检测文本集合中的 semantic structure。与随后的维度减少算法结合使用，topic models 可以用于生成文本集合的 two-dimensional scatter plot，反映文档之间的 semantic similarity，并支持文档集合分析。although the choice of topic model, dimensionality reduction, and their underlying hyperparameters have a significant impact on the resulting layout, it is unknown which particular combinations result in high-quality layouts with respect to accuracy and perception metrics.To investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots (or basis for landscape-type visualizations), we present a large-scale, benchmark-based computational evaluation. Our evaluation consists of (1) a set of corpora, (2) a set of layout algorithms that are combinations of topic models and dimensionality reductions, and (3) quality metrics for quantifying the resulting layout. The corpora are given as document-term matrices, and each document is assigned to a thematic class. The chosen metrics quantify the preservation of local and global properties and the perceptual effectiveness of the two-dimensional scatter plots. By evaluating the benchmark on a computing cluster, we derived a multivariate dataset with over 45 000 individual layouts and corresponding quality metrics. Based on the results, we propose guidelines for the effective design of text spatializations that are based on topic models and dimensionality reductions. As a main result, we show that interpretable topic models are beneficial for capturing the structure of text corpora. We furthermore recommend the use of t-SNE as a subsequent dimensionality reduction.
</details></li>
</ul>
<hr>
<h2 id="Latent-Jailbreak-A-Test-Suite-for-Evaluating-Both-Text-Safety-and-Output-Robustness-of-Large-Language-Models"><a href="#Latent-Jailbreak-A-Test-Suite-for-Evaluating-Both-Text-Safety-and-Output-Robustness-of-Large-Language-Models" class="headerlink" title="Latent Jailbreak: A Test Suite for Evaluating Both Text Safety and Output Robustness of Large Language Models"></a>Latent Jailbreak: A Test Suite for Evaluating Both Text Safety and Output Robustness of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08487">http://arxiv.org/abs/2307.08487</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiuhuachuan/latent-jailbreak">https://github.com/qiuhuachuan/latent-jailbreak</a></li>
<li>paper_authors: Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, Zhenzhong Lan</li>
<li>for: 这种论文旨在评估大语言模型（LLM）是否能够遵循人类价值观和生成安全文本。</li>
<li>methods: 作者提出了一个新的评估标准，以评估 LLM 的安全性和可靠性。该标准包括使用潜在的监禁提示集，以评估模型在完成任务时的 robustness。</li>
<li>results: 研究发现，当前的 LLM 不仅会优先使用某些指令词，还会在不同的指令词上 exhibit 不同的监禁率。<details>
<summary>Abstract</summary>
Considerable research efforts have been devoted to ensuring that large language models (LLMs) align with human values and generate safe text. However, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks. Previous benchmarks for jailbreaking LLMs have primarily focused on evaluating the safety of the models without considering their robustness. In this paper, we propose a benchmark that assesses both the safety and robustness of LLMs, emphasizing the need for a balanced approach. To comprehensively study text safety and output robustness, we introduce a latent jailbreak prompt dataset, each involving malicious instruction embedding. Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions. To further analyze safety and robustness, we design a hierarchical annotation framework. We present a systematic analysis of the safety and robustness of LLMs regarding the position of explicit normal instructions, word replacements (verbs in explicit normal instructions, target groups in malicious instructions, cue words for explicit normal instructions), and instruction replacements (different explicit normal instructions). Our results demonstrate that current LLMs not only prioritize certain instruction verbs but also exhibit varying jailbreak rates for different instruction verbs in explicit normal instructions. Code and data are available at https://github.com/qiuhuachuan/latent-jailbreak.
</details>
<details>
<summary>摘要</summary>
很多研究工作已经投入到确保大语言模型（LLM）与人类价值观 align的问题上。然而，过度强调某些话题的敏感性可能会削弱模型的完成任务能力，从而影响其总体性能。以前的 LLM 破狱 benchmark 主要关注模型的安全性而忽略了其可靠性。在这篇论文中，我们提出一个旨在评估 LLM 的安全性和可靠性的 benchmark，强调了平衡的方法。为了全面研究文本安全性和输出可靠性，我们提出了一个潜在破狱 prompt 数据集，每个包含恶意命令嵌入。为了进一步分析安全性和可靠性，我们设计了一个层次化注释框架。我们对 LLM 的安全性和可靠性进行了系统性分析，包括表示正常指令的位置、词替换（表示正常指令中的词语）、指令替换（不同的正常指令）等方面。我们的结果显示，当前 LLM 不仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅仅��
</details></li>
</ul>
<hr>
<h2 id="Domain-Knowledge-Distillation-from-Large-Language-Model-An-Empirical-Study-in-the-Autonomous-Driving-Domain"><a href="#Domain-Knowledge-Distillation-from-Large-Language-Model-An-Empirical-Study-in-the-Autonomous-Driving-Domain" class="headerlink" title="Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain"></a>Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11769">http://arxiv.org/abs/2307.11769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Tang, Antonio A. Bruto da Costa, Jason Zhang, Irvine Patrick, Siddartha Khastgir, Paul Jennings</li>
<li>for:  automatize engineering processes in knowledge-based systems</li>
<li>methods:  prompt engineering and ChatGPT language model</li>
<li>results:  empirical assessment in autonomous driving domain, improved efficiency and output quality with human supervision<details>
<summary>Abstract</summary>
Engineering knowledge-based (or expert) systems require extensive manual effort and domain knowledge. As Large Language Models (LLMs) are trained using an enormous amount of cross-domain knowledge, it becomes possible to automate such engineering processes. This paper presents an empirical automation and semi-automation framework for domain knowledge distillation using prompt engineering and the LLM ChatGPT. We assess the framework empirically in the autonomous driving domain and present our key observations. In our implementation, we construct the domain knowledge ontology by "chatting" with ChatGPT. The key finding is that while fully automated domain ontology construction is possible, human supervision and early intervention typically improve efficiency and output quality as they lessen the effects of response randomness and the butterfly effect. We, therefore, also develop a web-based distillation assistant enabling supervision and flexible intervention at runtime. We hope our findings and tools could inspire future research toward revolutionizing the engineering of knowledge-based systems across application domains.
</details>
<details>
<summary>摘要</summary>
工程知识基础（或专家）系统需要广泛的手动努力和领域知识。由于大型自然语言模型（LLM）在巨量跨领域知识训练中得到了很多经验，因此可以自动化工程过程。这篇论文提出了一种实践和半自动化框架，用于领域知识蒸馏，通过提问工程和LLM ChatGPT进行建构领域知识 ontology。我们在自动驾驶领域进行了实证测试，并提出了我们的关键观察。在我们的实现中，我们通过与 ChatGPT "聊天" 构建了领域知识 ontology。我们发现，完全自动化领域知识构建是可能的，但是人工监督和早期干预通常可以提高效率和输出质量，因为它们可以减少响应随机性和蝴蝶效应。因此，我们还开发了一个基于网络的蒸馏助手，以便在运行时进行监督和灵活干预。我们希望我们的发现和工具可以激励未来的工程系统知识工程研究，以推动应用领域的工程系统工程技术的革命。
</details></li>
</ul>
<hr>
<h2 id="Improving-End-to-End-Speech-Translation-by-Imitation-Based-Knowledge-Distillation-with-Synthetic-Transcripts"><a href="#Improving-End-to-End-Speech-Translation-by-Imitation-Based-Knowledge-Distillation-with-Synthetic-Transcripts" class="headerlink" title="Improving End-to-End Speech Translation by Imitation-Based Knowledge Distillation with Synthetic Transcripts"></a>Improving End-to-End Speech Translation by Imitation-Based Knowledge Distillation with Synthetic Transcripts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08426">http://arxiv.org/abs/2307.08426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hubreb/imitkd_ast">https://github.com/hubreb/imitkd_ast</a></li>
<li>paper_authors: Rebekka Hubert, Artem Sokolov, Stefan Riezler</li>
<li>for: This paper focuses on improving end-to-end automatic speech translation (AST) systems by using imitation learning to correct errors made by a student model.</li>
<li>methods: The authors use a teacher NMT system to correct the errors of an AST student model without relying on manual transcripts.</li>
<li>results: The NMT teacher is able to recover from errors in automatic transcriptions and correct erroneous translations of the AST student, leading to improvements of about 4 BLEU points over the standard AST end-to-end baseline on two datasets.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文关注改进端到端自动语音翻译（AST）系统，使用模仿学习方法来更正学生模型中的错误。</li>
<li>methods: 作者使用一个教师NMT系统来更正学生AST模型中的错误，而不需要人工译文。</li>
<li>results: NMT教师能够从自动译文中恢复错误，并更正学生AST模型中的错误翻译，在两个数据集上提高约4个BLEU分。<details>
<summary>Abstract</summary>
End-to-end automatic speech translation (AST) relies on data that combines audio inputs with text translation outputs. Previous work used existing large parallel corpora of transcriptions and translations in a knowledge distillation (KD) setup to distill a neural machine translation (NMT) into an AST student model. While KD allows using larger pretrained models, the reliance of previous KD approaches on manual audio transcripts in the data pipeline restricts the applicability of this framework to AST. We present an imitation learning approach where a teacher NMT system corrects the errors of an AST student without relying on manual transcripts. We show that the NMT teacher can recover from errors in automatic transcriptions and is able to correct erroneous translations of the AST student, leading to improvements of about 4 BLEU points over the standard AST end-to-end baseline on the English-German CoVoST-2 and MuST-C datasets, respectively. Code and data are publicly available.\footnote{\url{https://github.com/HubReb/imitkd_ast/releases/tag/v1.1}
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Supervised-Learning-with-Contrastive-Markings-in-Neural-Machine-Translation-Training"><a href="#Enhancing-Supervised-Learning-with-Contrastive-Markings-in-Neural-Machine-Translation-Training" class="headerlink" title="Enhancing Supervised Learning with Contrastive Markings in Neural Machine Translation Training"></a>Enhancing Supervised Learning with Contrastive Markings in Neural Machine Translation Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08416">http://arxiv.org/abs/2307.08416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathaniel Berger, Miriam Exel, Matthias Huck, Stefan Riezler</li>
<li>for: 提高 neural machine translation（NMT）中的监督学习过程中的探索性能力</li>
<li>methods: 使用对比标记目标来提供自动生成的增强训练信号，对比系统假设与参考文本进行比较，并对正确&#x2F;错误字符进行权重调整</li>
<li>results: 训练 WITH contrastive markings 可以提高 NMT 的性能，特别是在学习从 postedits 中的情况下，contrastive markings 可以指示人工错误纠正。<details>
<summary>Abstract</summary>
Supervised learning in Neural Machine Translation (NMT) typically follows a teacher forcing paradigm where reference tokens constitute the conditioning context in the model's prediction, instead of its own previous predictions. In order to alleviate this lack of exploration in the space of translations, we present a simple extension of standard maximum likelihood estimation by a contrastive marking objective. The additional training signals are extracted automatically from reference translations by comparing the system hypothesis against the reference, and used for up/down-weighting correct/incorrect tokens. The proposed new training procedure requires one additional translation pass over the training set per epoch, and does not alter the standard inference setup. We show that training with contrastive markings yields improvements on top of supervised learning, and is especially useful when learning from postedits where contrastive markings indicate human error corrections to the original hypotheses. Code is publicly released.
</details>
<details>
<summary>摘要</summary>
通常情况下，超级vised学习在神经机器翻译（NMT）中通常采用教师强制方法，其中参考令符出现作为模型预测的conditioningContext。为了解决这种预测缺乏探索的问题，我们提出了一个简单的扩展方法，通过对参考翻译进行自动提取的对照标记目标。这些额外训练信号通过比较系统假设与参考之间的比较，来升/降重量正确/错误的字符。我们的新训练方法不会改变标准的推理设置，只需要在每个轮次上进行一次训练集的一个额外翻译过程。我们展示了在supervised学习之上进行训练，并且在postedits中学习时，对于人类错误纠正的对照标记具有特别的用处。代码公共发布。
</details></li>
</ul>
<hr>
<h2 id="On-the-application-of-Large-Language-Models-for-language-teaching-and-assessment-technology"><a href="#On-the-application-of-Large-Language-Models-for-language-teaching-and-assessment-technology" class="headerlink" title="On the application of Large Language Models for language teaching and assessment technology"></a>On the application of Large Language Models for language teaching and assessment technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08393">http://arxiv.org/abs/2307.08393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Caines, Luca Benedetto, Shiva Taslimipoor, Christopher Davis, Yuan Gao, Oeistein Andersen, Zheng Yuan, Mark Elliott, Russell Moore, Christopher Bryant, Marek Rei, Helen Yannakoudakis, Andrew Mullooly, Diane Nicholls, Paula Buttery</li>
<li>for: 这个论文主要目的是研究大型自然语言处理模型在语言教学和评估系统中的应用潜力。</li>
<li>methods: 这篇论文使用了大型自然语言处理模型，包括PaLM和GPT-4，进行文本生成和自动评分等任务的研究。</li>
<li>results: 研究发现，大型语言模型可以在文本生成任务中提供更好的表现，但是在自动评分和语法错误检测任务中，它们并没有超越现有的state-of-the-art结果。<details>
<summary>Abstract</summary>
The recent release of very large language models such as PaLM and GPT-4 has made an unprecedented impact in the popular media and public consciousness, giving rise to a mixture of excitement and fear as to their capabilities and potential uses, and shining a light on natural language processing research which had not previously received so much attention. The developments offer great promise for education technology, and in this paper we look specifically at the potential for incorporating large language models in AI-driven language teaching and assessment systems. We consider several research areas and also discuss the risks and ethical considerations surrounding generative AI in education technology for language learners. Overall we find that larger language models offer improvements over previous models in text generation, opening up routes toward content generation which had not previously been plausible. For text generation they must be prompted carefully and their outputs may need to be reshaped before they are ready for use. For automated grading and grammatical error correction, tasks whose progress is checked on well-known benchmarks, early investigations indicate that large language models on their own do not improve on state-of-the-art results according to standard evaluation metrics. For grading it appears that linguistic features established in the literature should still be used for best performance, and for error correction it may be that the models can offer alternative feedback styles which are not measured sensitively with existing methods. In all cases, there is work to be done to experiment with the inclusion of large language models in education technology for language learners, in order to properly understand and report on their capacities and limitations, and to ensure that foreseeable risks such as misinformation and harmful bias are mitigated.
</details>
<details>
<summary>摘要</summary>
Recently released large language models such as PaLM and GPT-4 have caused a stir in popular media and public consciousness, with both excitement and fear about their capabilities and potential uses. This has shone a light on natural language processing research, which had not previously received so much attention. These developments offer great promise for education technology, and in this paper we explore the potential for incorporating large language models in AI-driven language teaching and assessment systems. We examine several research areas and also discuss the risks and ethical considerations surrounding generative AI in education technology for language learners.We find that larger language models offer improvements over previous models in text generation, opening up new possibilities for content generation. However, for text generation, careful prompting is necessary, and the outputs may need to be reshaped before they are ready for use. In terms of automated grading and grammatical error correction, early investigations suggest that large language models on their own do not improve on state-of-the-art results according to standard evaluation metrics. For grading, it appears that linguistic features established in the literature should still be used for best performance, and for error correction, the models may offer alternative feedback styles that are not measured sensitively with existing methods.In all cases, there is work to be done to experiment with the inclusion of large language models in education technology for language learners, in order to properly understand and report on their capacities and limitations, and to mitigate foreseeable risks such as misinformation and harmful bias.
</details></li>
</ul>
<hr>
<h2 id="How-do-software-citation-formats-evolve-over-time-A-longitudinal-analysis-of-R-programming-language-packages"><a href="#How-do-software-citation-formats-evolve-over-time-A-longitudinal-analysis-of-R-programming-language-packages" class="headerlink" title="How do software citation formats evolve over time? A longitudinal analysis of R programming language packages"></a>How do software citation formats evolve over time? A longitudinal analysis of R programming language packages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09390">http://arxiv.org/abs/2307.09390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhuo Wang, Kai Li</li>
<li>for: 本研究旨在探讨软件引用的复杂性，以便更好地理解软件引用的政策和基础设施。</li>
<li>methods: 本研究使用长期数据集，对2021年和2022年所有R包的引用格式进行比较和分析，以了解R语言包的引用格式，这些包是开源软件家族中重要的成员，以及引用格式是如何发展起来的。</li>
<li>results: 研究发现，不同的文档类型下的引用格式存在差异，而且在不同时间点，Metadata元素在引用格式中的变化也存在差异。此外，研究还发现了软件纸引用的专业性。通过这项研究，我们希望能够为软件引用政策和基础设施提供更好的理解。<details>
<summary>Abstract</summary>
Under the data-driven research paradigm, research software has come to play crucial roles in nearly every stage of scientific inquiry. Scholars are advocating for the formal citation of software in academic publications, treating it on par with traditional research outputs. However, software is hardly consistently cited: one software entity can be cited as different objects, and the citations can change over time. These issues, however, are largely overlooked in existing empirical research on software citation. To fill the above gaps, the present study compares and analyzes a longitudinal dataset of citation formats of all R packages collected in 2021 and 2022, in order to understand the citation formats of R-language packages, important members in the open-source software family, and how the citations evolve over time. In particular, we investigate the different document types underlying the citations and what metadata elements in the citation formats changed over time. Furthermore, we offer an in-depth analysis of the disciplinarity of journal articles cited as software (software papers). By undertaking this research, we aim to contribute to a better understanding of the complexities associated with software citation, shedding light on future software citation policies and infrastructure.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation may not be perfect, and some nuances of the original text may be lost in translation.)
</details></li>
</ul>
<hr>
<h2 id="Legal-Syllogism-Prompting-Teaching-Large-Language-Models-for-Legal-Judgment-Prediction"><a href="#Legal-Syllogism-Prompting-Teaching-Large-Language-Models-for-Legal-Judgment-Prediction" class="headerlink" title="Legal Syllogism Prompting: Teaching Large Language Models for Legal Judgment Prediction"></a>Legal Syllogism Prompting: Teaching Large Language Models for Legal Judgment Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08321">http://arxiv.org/abs/2307.08321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cong Jiang, Xiaolei Yang</li>
<li>for: 本研究旨在开发一种简单的提示方法，以教育大型自然语言处理模型（LLM）在法律推理中进行判断预测。</li>
<li>methods: 本研究使用的是“法律推理提示”（LoT）方法，它只教育模型在法律推理中，主 premise 是法律， minor premise 是事实，结论是判断。</li>
<li>results: 在 CAIL2018 中国刑事案例集上，我们使用 GPT-3 模型进行零例判断预测实验，结果显示 LLM 使用 LoT 方法可以在多种推理任务上表现更好 than 基eline 和链式思维提示方法。 LoT 方法使得模型能够吸取到关键信息 relevante 于判断，正确理解法律规定的意义，与其他方法相比更加精准。<details>
<summary>Abstract</summary>
Legal syllogism is a form of deductive reasoning commonly used by legal professionals to analyze cases. In this paper, we propose legal syllogism prompting (LoT), a simple prompting method to teach large language models (LLMs) for legal judgment prediction. LoT teaches only that in the legal syllogism the major premise is law, the minor premise is the fact, and the conclusion is judgment. Then the models can produce a syllogism reasoning of the case and give the judgment without any learning, fine-tuning, or examples. On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment prediction experiments with GPT-3 models. Our results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks. LoT enables the model to concentrate on the key information relevant to the judgment and to correctly understand the legal meaning of acts, as compared to other methods. Our method enables LLMs to predict judgment along with law articles and justification, which significantly enhances the explainability of models.
</details>
<details>
<summary>摘要</summary>
法律逻辑是法律专业人员常用的推理方式，用于分析案例。在这篇论文中，我们提出了法律逻辑提示（LoT），一种简单的提示方法，用于教育大型自然语言模型（LLM）进行法律判断预测。LoT只教导了在法律逻辑中，主 Premise 是法律，次 Premise 是事实，结论是判断。然后模型可以生成案例的逻辑推理，并给出判断。在 CAIL2018 中国刑事案例集上，我们进行了零shot 判断预测实验，使用 GPT-3 模型。我们的结果显示， LLMS  WITH LoT 在多种推理任务上表现更好 than 基eline 和链条思维提示方法。LoT 使得模型能够专注于关键信息 relevante 到判断，正确理解法律 act 的法律含义，与其他方法相比。我们的方法允许 LLMS 预测判断，同时提供法律条文和 justify，这显著提高了模型的解释性。
</details></li>
</ul>
<hr>
<h2 id="IterLara-A-Turing-Complete-Algebra-for-Big-Data-AI-Scientific-Computing-and-Database"><a href="#IterLara-A-Turing-Complete-Algebra-for-Big-Data-AI-Scientific-Computing-and-Database" class="headerlink" title="IterLara: A Turing Complete Algebra for Big Data, AI, Scientific Computing, and Database"></a>IterLara: A Turing Complete Algebra for Big Data, AI, Scientific Computing, and Database</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08315">http://arxiv.org/abs/2307.08315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongxiao Li, Wanling Gao, Lei Wang, Jianfeng Zhan<br>for: This paper aims to provide an algebraic model that unifies operations in general-purpose computing, such as big data, AI, scientific computing, and database.methods: The paper proposes \textsc{IterLara}, an extension of \textsc{Lara} with iterative operators, to achieve this goal.results: The paper studies the expressive ability of \textsc{Lara} and \textsc{IterLara} and proves that \textsc{IterLara} with aggregation functions can represent matrix inversion and determinant. Additionally, the paper shows that \textsc{IterLara} with no limitation of function utility is Turing complete, and proposes the Operation Count (OP) as a metric of computation amount for \textsc{IterLara}.<details>
<summary>Abstract</summary>
\textsc{Lara} is a key-value algebra that aims at unifying linear and relational algebra with three types of operation abstraction. The study of \textsc{Lara}'s expressive ability reports that it can represent relational algebra and most linear algebra operations. However, several essential computations, such as matrix inversion and determinant, cannot be expressed in \textsc{Lara}. \textsc{Lara} cannot represent global and iterative computation, either. This article proposes \textsc{IterLara}, extending \textsc{Lara} with iterative operators, to provide an algebraic model that unifies operations in general-purpose computing, like big data, AI, scientific computing, and database. We study the expressive ability of \textsc{Lara} and \textsc{IterLara} and prove that \textsc{IterLara} with aggregation functions can represent matrix inversion, determinant. Besides, we demonstrate that \textsc{IterLara} with no limitation of function utility is Turing complete. We also propose the Operation Count (OP) as a metric of computation amount for \textsc{IterLara} and ensure that the OP metric is in accordance with the existing computation metrics.
</details>
<details>
<summary>摘要</summary>
\begin{blockquote}\textsc{Lara} 是一种键值代数，旨在将线性代数和关系代数合一，并提供三种操作抽象类型。研究 \textsc{Lara} 的表达能力发现，它可以表示关系代数和大多数线性代数操作。然而，一些基本计算，如矩阵逆元和 determinant，无法在 \textsc{Lara} 中表达。此外， \textsc{Lara} 也无法表示全局和迭代计算。这篇文章提出 \textsc{IterLara}， extending \textsc{Lara}  WITH 迭代操作，以提供一种通用计算中的代数模型，包括大数据、人工智能、科学计算和数据库。我们研究 \textsc{Lara} 和 \textsc{IterLara} 的表达能力，并证明 \textsc{IterLara}  WITH 聚合函数可以表示矩阵逆元和 determinant。此外，我们还证明 \textsc{IterLara}  WITH 无限函数Utility 是 Turing 完全的。我们还提出了 Operation Count（OP）作为 \textsc{IterLara} 的计算量度，并证明 OP 度量与现有计算度量相符。\end{blockquoteNote that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="CoAD-Automatic-Diagnosis-through-Symptom-and-Disease-Collaborative-Generation"><a href="#CoAD-Automatic-Diagnosis-through-Symptom-and-Disease-Collaborative-Generation" class="headerlink" title="CoAD: Automatic Diagnosis through Symptom and Disease Collaborative Generation"></a>CoAD: Automatic Diagnosis through Symptom and Disease Collaborative Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08290">http://arxiv.org/abs/2307.08290</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kwanwaichung/coad">https://github.com/kwanwaichung/coad</a></li>
<li>paper_authors: Huimin Wang, Wai-Chung Kwan, Kam-Fai Wong, Yefeng Zheng</li>
<li>for: 该研究旨在提高自动诊断（AD）的精度，以帮助医生更加精确地诊断疾病。</li>
<li>methods: 该方法使用Transformer架构，将症状序列作为输入，透过自动重构来预测疾病。</li>
<li>results: 该研究获得了2.3%的提升，与前一代最佳结果相比。<details>
<summary>Abstract</summary>
Automatic diagnosis (AD), a critical application of AI in healthcare, employs machine learning techniques to assist doctors in gathering patient symptom information for precise disease diagnosis. The Transformer-based method utilizes an input symptom sequence, predicts itself through auto-regression, and employs the hidden state of the final symptom to determine the disease. Despite its simplicity and superior performance demonstrated, a decline in disease diagnosis accuracy is observed caused by 1) a mismatch between symptoms observed during training and generation, and 2) the effect of different symptom orders on disease prediction. To address the above obstacles, we introduce the CoAD, a novel disease and symptom collaborative generation framework, which incorporates several key innovations to improve AD: 1) aligning sentence-level disease labels with multiple possible symptom inquiry steps to bridge the gap between training and generation; 2) expanding symptom labels for each sub-sequence of symptoms to enhance annotation and eliminate the effect of symptom order; 3) developing a repeated symptom input schema to effectively and efficiently learn the expanded disease and symptom labels. We evaluate the CoAD framework using four datasets, including three public and one private, and demonstrate that it achieves an average 2.3% improvement over previous state-of-the-art results in automatic disease diagnosis. For reproducibility, we release the code and data at https://github.com/KwanWaiChung/coad.
</details>
<details>
<summary>摘要</summary>
自动诊断（AD），医疗领域中critical应用的人工智能技术，利用机器学习技术帮助医生收集病人症状信息，以确定精准的疾病诊断。transformer基本方法使用输入症状序列，通过自动回归，并使用最终症状隐藏状态来确定疾病。despite its simplicity and superior performance demonstrated, a decline in disease diagnosis accuracy is observed caused by 1) a mismatch between symptoms observed during training and generation, and 2) the effect of different symptom orders on disease prediction. To address the above obstacles, we introduce the CoAD, a novel disease and symptom collaborative generation framework, which incorporates several key innovations to improve AD: 1) aligning sentence-level disease labels with multiple possible symptom inquiry steps to bridge the gap between training and generation; 2) expanding symptom labels for each sub-sequence of symptoms to enhance annotation and eliminate the effect of symptom order; 3) developing a repeated symptom input schema to effectively and efficiently learn the expanded disease and symptom labels. We evaluate the CoAD framework using four datasets, including three public and one private, and demonstrate that it achieves an average 2.3% improvement over previous state-of-the-art results in automatic disease diagnosis. For reproducibility, we release the code and data at https://github.com/KwanWaiChung/coad.Here's the translation in Traditional Chinese:自动诊断（AD），医疗领域中critical应用的人工智能技术，利用机器学习技术帮助医生收集病人症状信息，以确定精确的疾病诊断。transformer基本方法使用输入症状序列，通过自动回归，并使用最终症状隐藏状态来确定疾病。despite its simplicity and superior performance demonstrated, a decline in disease diagnosis accuracy is observed caused by 1) a mismatch between symptoms observed during training and generation, and 2) the effect of different symptom orders on disease prediction. To address the above obstacles, we introduce the CoAD, a novel disease and symptom collaborative generation framework, which incorporates several key innovations to improve AD: 1) aligning sentence-level disease labels with multiple possible symptom inquiry steps to bridge the gap between training and generation; 2) expanding symptom labels for each sub-sequence of symptoms to enhance annotation and eliminate the effect of symptom order; 3) developing a repeated symptom input schema to effectively and efficiently learn the expanded disease and symptom labels. We evaluate the CoAD framework using four datasets, including three public and one private, and demonstrate that it achieves an average 2.3% improvement over previous state-of-the-art results in automatic disease diagnosis. For reproducibility, we release the code and data at https://github.com/KwanWaiChung/coad.
</details></li>
</ul>
<hr>
<h2 id="Automated-Action-Model-Acquisition-from-Narrative-Texts"><a href="#Automated-Action-Model-Acquisition-from-Narrative-Texts" class="headerlink" title="Automated Action Model Acquisition from Narrative Texts"></a>Automated Action Model Acquisition from Narrative Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10247">http://arxiv.org/abs/2307.10247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiqi Li, Leyang Cui, Songtuan Lin, Patrik Haslum</li>
<li>for: 本研究旨在提高人工智能代理人的决策能力，通过自动从叙述文本中提取结构化事件和生成 планинг语言风格的动作模型。</li>
<li>methods: 本研究使用了自动从叙述文本中提取结构化事件的方法，并基于预测常识事件关系、文本矛盾和相似性，生成了 планинг语言风格的动作模型。</li>
<li>results: 实验结果显示，NaRuto可以在经典叙述规划领域中生成高质量的动作模型，与现有的完全自动方法相当，甚至与半自动方法相当。<details>
<summary>Abstract</summary>
Action models, which take the form of precondition/effect axioms, facilitate causal and motivational connections between actions for AI agents. Action model acquisition has been identified as a bottleneck in the application of planning technology, especially within narrative planning. Acquiring action models from narrative texts in an automated way is essential, but challenging because of the inherent complexities of such texts. We present NaRuto, a system that extracts structured events from narrative text and subsequently generates planning-language-style action models based on predictions of commonsense event relations, as well as textual contradictions and similarities, in an unsupervised manner. Experimental results in classical narrative planning domains show that NaRuto can generate action models of significantly better quality than existing fully automated methods, and even on par with those of semi-automated methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>使用减少的语言表达，模型可以帮助人工智能代理人进行计划和决策。但是获取这些模型的步骤是困难的，特别是在叙述计划领域。我们提出了一种系统，它可以自动从叙述文本中提取结构化事件，并根据预测的常识事件关系、文本矛盾和相似性，生成计划语言风格的行动模型，而不需要人工干预。我们的实验结果表明，NaRuto可以在经典叙述计划领域生成的行动模型质量比既有的完全自动方法更高，甚至与半自动方法相当。</SYS>Here's the translation in Traditional Chinese: <<SYS>使用简化的语言表达，模型可以帮助人工智能代理人进行计划和决策。但是获取这些模型的步骤是困难的，特别是在叙述计划领域。我们提出了一个系统，它可以自动从叙述文本中提取结构化事件，并根据预测的常识事件关系、文本矛盾和相似性，生成计划语言风格的行动模型，而不需要人工干预。我们的实验结果表明，NaRuto可以在经典叙述计划领域生成的行动模型质量比既有的完全自动方法更高，甚至与半自动方法相当。</SYS>
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-is-Good-but-Bing-Chat-is-Better-for-Vietnamese-Students"><a href="#ChatGPT-is-Good-but-Bing-Chat-is-Better-for-Vietnamese-Students" class="headerlink" title="ChatGPT is Good but Bing Chat is Better for Vietnamese Students"></a>ChatGPT is Good but Bing Chat is Better for Vietnamese Students</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08272">http://arxiv.org/abs/2307.08272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan-Quy Dao, Ngoc-Bich Le</li>
<li>for: 这个研究旨在探讨两个现代大语言模型（LLMs），即ChatGPT和Microsoft Bing Chat（BingChat），在越南学生的需求下是否有效。</li>
<li>methods: 我们进行了这两个LLMs在不同学科中的比较分析，包括数学、文学、英语、物理、化学、生物、历史、地理和公民教育等。</li>
<li>results: 我们的研究结果表明，BingChat在各种学科中表现出色，只有文学领域是ChatGPT表现得更好。此外，BingChat使用了更高级的GPT-4技术，而ChatGPT是基于GPT-3.5技术。这使得BingChat可以提高其理解、判断和创造性文本生成能力。此外，BingChat在越南可用并具有内置的链接和参考，这也为其superiority做出了贡献。<details>
<summary>Abstract</summary>
This study examines the efficacy of two SOTA large language models (LLMs), namely ChatGPT and Microsoft Bing Chat (BingChat), in catering to the needs of Vietnamese students. Although ChatGPT exhibits proficiency in multiple disciplines, Bing Chat emerges as the more advantageous option. We conduct a comparative analysis of their academic achievements in various disciplines, encompassing mathematics, literature, English language, physics, chemistry, biology, history, geography, and civic education. The results of our study suggest that BingChat demonstrates superior performance compared to ChatGPT across a wide range of subjects, with the exception of literature, where ChatGPT exhibits better performance. Additionally, BingChat utilizes the more advanced GPT-4 technology in contrast to ChatGPT, which is built upon GPT-3.5. This allows BingChat to improve to comprehension, reasoning and generation of creative and informative text. Moreover, the fact that BingChat is accessible in Vietnam and its integration of hyperlinks and citations within responses serve to reinforce its superiority. In our analysis, it is evident that while ChatGPT exhibits praiseworthy qualities, BingChat presents a more apdated solutions for Vietnamese students.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这个研究检查了两个现代大型自然语言处理器（LLM），即ChatGPT和Microsoft Bing Chat（BingChat），在越南学生的需求下是否有效。虽然ChatGPT在多个领域展现出色，但BingChat emerges as the more advantageous option。我们进行了多个学科的比较分析，包括数学、文学、英语、物理、化学、生物、历史、地理和公民教育。我们的研究结果表明，BingChat在多个学科中表现出色，只有文学领域，ChatGPT的表现更好。此外，BingChat使用更先进的GPT-4技术，可以提高对话、理解和创造性文本的能力。此外，BingChat在越南可用，并且在回答中包含链接和参考文献，这些因素都服务于加强其优势。在我们的分析中，可以看到，虽然ChatGPT具有称赞的特点，但BingChat提供了更先进的解决方案 для越南学生。
</details></li>
</ul>
<hr>
<h2 id="Extending-the-Frontier-of-ChatGPT-Code-Generation-and-Debugging"><a href="#Extending-the-Frontier-of-ChatGPT-Code-Generation-and-Debugging" class="headerlink" title="Extending the Frontier of ChatGPT: Code Generation and Debugging"></a>Extending the Frontier of ChatGPT: Code Generation and Debugging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08260">http://arxiv.org/abs/2307.08260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fardin Ahsan Sakib, Saadat Hasan Khan, A. H. M. Rezaul Karim</li>
<li>for: 这篇论文旨在研究 ChatGPT 是否可以解决编程问题，并评估其解决问题的正确性和效率。</li>
<li>methods: 该论文使用 ChatGPT 解决 Leetcode 上的编程问题，并对其解决的问题进行了评估。</li>
<li>results: 论文发现，ChatGPT 的总成功率为 71.875%，表明它可以成功解决大多数编程问题。它在结构化问题上表现出了优异，但在接受反馈改进解决方案时表现不佳。<details>
<summary>Abstract</summary>
Large-scale language models (LLMs) have emerged as a groundbreaking innovation in the realm of question-answering and conversational agents. These models, leveraging different deep learning architectures such as Transformers, are trained on vast corpora to predict sentences based on given queries. Among these LLMs, ChatGPT, developed by OpenAI, has ushered in a new era by utilizing artificial intelligence (AI) to tackle diverse problem domains, ranging from composing essays and biographies to solving intricate mathematical integrals. The versatile applications enabled by ChatGPT offer immense value to users. However, assessing the performance of ChatGPT's output poses a challenge, particularly in scenarios where queries lack clear objective criteria for correctness. For instance, evaluating the quality of generated essays becomes arduous and relies heavily on manual labor, in stark contrast to evaluating solutions to well-defined, closed-ended questions such as mathematical problems. This research paper delves into the efficacy of ChatGPT in solving programming problems, examining both the correctness and the efficiency of its solution in terms of time and memory complexity. The research reveals a commendable overall success rate of 71.875\%, denoting the proportion of problems for which ChatGPT was able to provide correct solutions that successfully satisfied all the test cases present in Leetcode. It exhibits strengths in structured problems and shows a linear correlation between its success rate and problem acceptance rates. However, it struggles to improve solutions based on feedback, pointing to potential shortcomings in debugging tasks. These findings provide a compact yet insightful glimpse into ChatGPT's capabilities and areas for improvement.
</details>
<details>
<summary>摘要</summary>
大规模语言模型（LLM）已经成为问答和对话机器人领域的创新之一。这些模型，利用不同的深度学习架构，如转换器，在庞大的文献中进行训练，以预测基于给定查询的句子。中的ChatGPT，由OpenAI开发，对多种问题领域进行了应用，从编写文章和传记到解决复杂的数学 интеграル。这些应用具有巨大的价值，但评估ChatGPT的输出性能具有挑战，尤其是在查询缺乏明确的对象标准的情况下。例如，评估生成的文章的质量变得困难，需要大量的人工劳动，与解决具有明确对象标准的关闭式问题，如数学问题，存在很大的区别。本研究评估了ChatGPT在编程问题上的效果，包括正确性和时间复杂度、内存复杂度。研究发现，ChatGPT在Leetcode上的总成功率为71.875%，表示它可以成功解决的问题数。它在结构化问题上表现出了优异，与问题接受率之间存在直线相关性。但是，它在反馈基础上改进解决方案存在困难，表明可能存在调试任务的缺陷。这些发现为ChatGPT的能力和改进提供了一个紧凑 yet 深入的视角。
</details></li>
</ul>
<hr>
<h2 id="PAT-Parallel-Attention-Transformer-for-Visual-Question-Answering-in-Vietnamese"><a href="#PAT-Parallel-Attention-Transformer-for-Visual-Question-Answering-in-Vietnamese" class="headerlink" title="PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese"></a>PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08247">http://arxiv.org/abs/2307.08247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nghia Hieu Nguyen, Kiet Van Nguyen</li>
<li>for: 本文提出了一种新的多模态学习方法，称为并行注意机制。</li>
<li>methods: 本文提出了一种新的语言特征提取模块，即干扰语言特征提取器（Hierarchical Linguistic Features Extractor，HLFE），以及一种基于Transformer架构的并行注意机制（Parallel Attention Transformer，PAT）。</li>
<li>results: 根据本文的实验结果，PAT在ViVQA数据集上达到了所有基准值和其他SOTA方法（包括SAAA和MCAN）的最高准确率。<details>
<summary>Abstract</summary>
We present in this paper a novel scheme for multimodal learning named the Parallel Attention mechanism. In addition, to take into account the advantages of grammar and context in Vietnamese, we propose the Hierarchical Linguistic Features Extractor instead of using an LSTM network to extract linguistic features. Based on these two novel modules, we introduce the Parallel Attention Transformer (PAT), achieving the best accuracy compared to all baselines on the benchmark ViVQA dataset and other SOTA methods including SAAA and MCAN.
</details>
<details>
<summary>摘要</summary>
我们在这篇论文中提出了一种新的多Modal学习方案，称为并行注意机制。此外，为了利用越南语的语法和语言上下文的优势，我们提议使用层次语言特征提取器而不是LSTM网络来提取语言特征。基于这两种新模块，我们介绍了并行注意变换器（PAT），在 benchmark ViVQA 数据集和其他 SOTA 方法，包括 SAAA 和 MCAN，中达到最高准确率。
</details></li>
</ul>
<hr>
<h2 id="ivrit-ai-A-Comprehensive-Dataset-of-Hebrew-Speech-for-AI-Research-and-Development"><a href="#ivrit-ai-A-Comprehensive-Dataset-of-Hebrew-Speech-for-AI-Research-and-Development" class="headerlink" title="ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development"></a>ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08720">http://arxiv.org/abs/2307.08720</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yairl/ivrit.ai">https://github.com/yairl/ivrit.ai</a></li>
<li>paper_authors: Yanir Marmor, Kinneret Misgav, Yair Lifshitz</li>
<li>for: 提高希伯来语自动语音识别技术的研究和开发</li>
<li>methods: 使用大量希伯来语 speech数据，包括未经过音响活动检测和部分转录数据，以满足不同研究需求</li>
<li>results: 提供了大量、多样化的希伯来语 speech数据资源，可以帮助研究人员、开发者和商业机构提高希伯来语自动语音识别技术的水平Translation:</li>
<li>for: 提高希伯来语自动语音识别技术的研究和开发</li>
<li>methods: 使用大量希伯来语 speech数据，包括未经过音响活动检测和部分转录数据，以满足不同研究需求</li>
<li>results: 提供了大量、多样化的希伯来语 speech数据资源，可以帮助研究人员、开发者和商业机构提高希伯来语自动语音识别技术的水平<details>
<summary>Abstract</summary>
We introduce "ivrit.ai", a comprehensive Hebrew speech dataset, addressing the distinct lack of extensive, high-quality resources for advancing Automated Speech Recognition (ASR) technology in Hebrew. With over 3,300 speech hours and a over a thousand diverse speakers, ivrit.ai offers a substantial compilation of Hebrew speech across various contexts. It is delivered in three forms to cater to varying research needs: raw unprocessed audio; data post-Voice Activity Detection, and partially transcribed data. The dataset stands out for its legal accessibility, permitting use at no cost, thereby serving as a crucial resource for researchers, developers, and commercial entities. ivrit.ai opens up numerous applications, offering vast potential to enhance AI capabilities in Hebrew. Future efforts aim to expand ivrit.ai further, thereby advancing Hebrew's standing in AI research and technology.
</details>
<details>
<summary>摘要</summary>
我们介绍“ivrit.ai”，一个全面的 hébrew 语音数据集，纠正了 hébrew 语音识别技术的缺乏大量、高质量资源。 ivrit.ai 包含了超过 3,300 小时的语音数据和数百个多样化的说话人，在不同的场景下提供了广泛的 hébrew 语音资料。该数据集提供三种形式，以适应不同的研究需求： raw 未处理的音频数据、 after Voice Activity Detection 的数据和部分转录的数据。ivrit.ai 的法律可 accessing，免费使用，因此成为研究人员、开发者和商业机构的重要资源。 ivrit.ai 开启了许多应用程序，提供了巨大的潜在space ，以提高 hébrew 语言在人工智能技术中的地位。未来努力将 ivrit.ai 进一步扩展，以推动 hébrew 语言在人工智能研究和技术中的发展。
</details></li>
</ul>
<hr>
<h2 id="BASS-Block-wise-Adaptation-for-Speech-Summarization"><a href="#BASS-Block-wise-Adaptation-for-Speech-Summarization" class="headerlink" title="BASS: Block-wise Adaptation for Speech Summarization"></a>BASS: Block-wise Adaptation for Speech Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08217">http://arxiv.org/abs/2307.08217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, Bhiksha Raj</li>
<li>for: 提高端到端语音摘要模型的性能，解决训练时间过长导致模型质量下降的问题。</li>
<li>methods: 采用块式训练方法，通过分割输入序列进行逐步训练，使模型能够在很长的输入序列上进行学习。</li>
<li>results: 在How2 dataset上实现了块式训练方法，比 truncated input baseline 高出3点级ROUGE-L。<details>
<summary>Abstract</summary>
End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline." into Simplified Chinese.中文简体版：END-TO-END语音摘要模型已经显示在提高性能的情况下超过顺序基elines。然而，这些模型在非常长的输入（分钟或小时级别）上受到计算限制，因此通常使用 truncated 模型输入进行训练。这会导致模型较差，解决这个问题需要块式模型化，即在每个块（一段时间）中处理输入框架。在这篇论文中，我们开发了一种允许在非常长序列上逐步训练摘要模型的方法。在流动处理中，每个块基于新的音频信息更新假设摘要。我们提出并测试了在块之间传递 semantics 上下文的策略。在 How2 数据集上，我们的块式训练方法与 truncated 输入基线相比，提高了 ROUGE-L 指标的3个绝对点。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Dataset-Annotation-Quality-Management-in-the-Wild"><a href="#Analyzing-Dataset-Annotation-Quality-Management-in-the-Wild" class="headerlink" title="Analyzing Dataset Annotation Quality Management in the Wild"></a>Analyzing Dataset Annotation Quality Management in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08153">http://arxiv.org/abs/2307.08153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan-Christoph Klie, Richard Eckart de Castilho, Iryna Gurevych</li>
<li>for: 这些论文的目的是要研究数据质量对机器学习模型训练和评估的影响，以及现有数据集中是否存在错误注释、偏见或注释 artifacts。</li>
<li>methods: 这些论文使用了文献综述和建议的方法来描述数据集创建时的质量管理实践，以及如何应用这些建议。然后，它们编译了591篇科学论文引用的文本数据集，并对其进行了质量相关的注释。</li>
<li>results: 这些论文发现大多数注释的工作采用了良好或非常良好的质量管理方法，但有30%的工作只有较差的质量管理。分析还显示了一些常见的错误，特别是在使用间对注释协议和计算注释错误率时出现的错误。<details>
<summary>Abstract</summary>
Data quality is crucial for training accurate, unbiased, and trustworthy machine learning models and their correct evaluation. Recent works, however, have shown that even popular datasets used to train and evaluate state-of-the-art models contain a non-negligible amount of erroneous annotations, bias or annotation artifacts. There exist best practices and guidelines regarding annotation projects. But to the best of our knowledge, no large-scale analysis has been performed as of yet on how quality management is actually conducted when creating natural language datasets and whether these recommendations are followed. Therefore, we first survey and summarize recommended quality management practices for dataset creation as described in the literature and provide suggestions on how to apply them. Then, we compile a corpus of 591 scientific publications introducing text datasets and annotate it for quality-related aspects, such as annotator management, agreement, adjudication or data validation. Using these annotations, we then analyze how quality management is conducted in practice. We find that a majority of the annotated publications apply good or very good quality management. However, we deem the effort of 30% of the works as only subpar. Our analysis also shows common errors, especially with using inter-annotator agreement and computing annotation error rates.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translation into Simplified Chinese文本质量对机器学习模型的训练和评估是关键。然而，当今最新的研究表明，即使使用最新的模型训练和评估的数据集中也包含一定数量的错误注释、偏见或注释遗留。现有的最佳实践和指南可以帮助确保数据集的质量。然而，我们知道的是，到目前为止没有任何大规模的分析，检查在创建自然语言数据集时是否实际遵循这些建议。因此，我们首先对文献中描述的质量管理做出survey和摘要，然后提供应用这些建议的建议。接着，我们编译了591篇科学论文引用的文本数据集，并对其进行质量相关的注释，包括注释员管理、一致性、仲裁或数据验证。使用这些注释，我们然后分析了在实践中是否实际遵循质量管理的方法。我们发现大多数注释的工作采用良好或非常良好的质量管理方法。然而，我们认为30%的工作仅为低水平。我们的分析还发现了一些常见的错误，特别是使用间接注释者一致性和计算注释错误率。
</details></li>
</ul>
<hr>
<h2 id="The-Potential-and-Pitfalls-of-using-a-Large-Language-Model-such-as-ChatGPT-or-GPT-4-as-a-Clinical-Assistant"><a href="#The-Potential-and-Pitfalls-of-using-a-Large-Language-Model-such-as-ChatGPT-or-GPT-4-as-a-Clinical-Assistant" class="headerlink" title="The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant"></a>The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08152">http://arxiv.org/abs/2307.08152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingqing Zhang, Kai Sun, Akshay Jagadeesh, Mahta Ghahfarokhi, Deepa Gupta, Ashok Gupta, Vibhor Gupta, Yike Guo</li>
<li>for: 这两篇研究用于评估ChatGPT和GPT-4在实际医疗数据库中的表现，以及它们在诊断助理方面的使用可行性。</li>
<li>methods: 这两篇研究使用了ChatGPT和GPT-4，分别在实际医疗数据库中进行了医疗诊断和诊断助理任务。</li>
<li>results: GPT-4在医疗诊断和诊断助理任务中的表现可以达到96%的F1分数，但是有些提示中含有误导性的资讯，遗传医疗发现和建议不必要的检测和治疗。这些问题，加上医疗资料保护问题，使这些模型目前不适合实际医疗应用。<details>
<summary>Abstract</summary>
Recent studies have demonstrated promising performance of ChatGPT and GPT-4 on several medical domain tasks. However, none have assessed its performance using a large-scale real-world electronic health record database, nor have evaluated its utility in providing clinical diagnostic assistance for patients across a full range of disease presentation. We performed two analyses using ChatGPT and GPT-4, one to identify patients with specific medical diagnoses using a real-world large electronic health record database and the other, in providing diagnostic assistance to healthcare workers in the prospective evaluation of hypothetical patients. Our results show that GPT-4 across disease classification tasks with chain of thought and few-shot prompting can achieve performance as high as 96% F1 scores. For patient assessment, GPT-4 can accurately diagnose three out of four times. However, there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use. However, limited data and time needed for prompt engineering in comparison to configuration of conventional machine learning workflows highlight their potential for scalability across healthcare applications.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese: latest studies have shown that ChatGPT and GPT-4 have performed well on medical tasks, but no one has tested their performance on a large, real-world electronic health record database or evaluated their ability to provide clinical diagnostic assistance for patients with a wide range of symptoms. We conducted two studies using ChatGPT and GPT-4. One study used a real-world large electronic health record database to identify patients with specific medical diagnoses, and the other study provided diagnostic assistance to healthcare workers for hypothetical patients. Our results show that GPT-4 achieved high performance on disease classification tasks with chain of thought and few-shot prompting, with F1 scores as high as 96%. However, GPT-4 also made factually incorrect statements, overlooked crucial medical findings, and recommended unnecessary investigations and overtreatment. These issues, combined with privacy concerns, make these models currently unsuitable for real-world clinical use. However, the limited data and time required for prompt engineering compared to configuring conventional machine learning workflows highlight their potential for scalability across healthcare applications.
</details></li>
</ul>
<hr>
<h2 id="It’s-All-Relative-Interpretable-Models-for-Scoring-Bias-in-Documents"><a href="#It’s-All-Relative-Interpretable-Models-for-Scoring-Bias-in-Documents" class="headerlink" title="It’s All Relative: Interpretable Models for Scoring Bias in Documents"></a>It’s All Relative: Interpretable Models for Scoring Bias in Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08139">http://arxiv.org/abs/2307.08139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aswin Suresh, Chi-Hsuan Wu, Matthias Grossglauser</li>
<li>for: 这篇论文的目的是提出一种可解释的模型，用于评分网络文档中的偏见。</li>
<li>methods: 这种模型基于布莱德利-泰勒axioms，并通过对同一篇Wikipedia文章的多个修订版本进行比较，以学习偏见的评分。</li>
<li>results: 模型可以准确地评分偏见，并且可以解释模型的参数，找到偏见的指标字。此外，模型还在不同的设置下进行了应用，包括研究Wikipedia文章的时间演化、比较新闻来源的偏见、以及评分法律修订的偏见。<details>
<summary>Abstract</summary>
We propose an interpretable model to score the bias present in web documents, based only on their textual content. Our model incorporates assumptions reminiscent of the Bradley-Terry axioms and is trained on pairs of revisions of the same Wikipedia article, where one version is more biased than the other. While prior approaches based on absolute bias classification have struggled to obtain a high accuracy for the task, we are able to develop a useful model for scoring bias by learning to perform pairwise comparisons of bias accurately. We show that we can interpret the parameters of the trained model to discover the words most indicative of bias. We also apply our model in three different settings - studying the temporal evolution of bias in Wikipedia articles, comparing news sources based on bias, and scoring bias in law amendments. In each case, we demonstrate that the outputs of the model can be explained and validated, even for the two domains that are outside the training-data domain. We also use the model to compare the general level of bias between domains, where we see that legal texts are the least biased and news media are the most biased, with Wikipedia articles in between. Given its high performance, simplicity, interpretability, and wide applicability, we hope the model will be useful for a large community, including Wikipedia and news editors, political and social scientists, and the general public.
</details>
<details>
<summary>摘要</summary>
我们提出一种可解释的模型，用于评分网络文档中的偏见。我们的模型具有布莱德利-泰勒axioms的假设，并基于同一篇Wikipedia文章的修订版本进行训练。而且，我们的模型可以准确地比较修订版本之间的偏见水平。我们表明了我们可以解释模型中学习的参数，以便发现偏见的关键词。此外，我们还应用了我们的模型在三个不同的场景中：研究Wikipedia文章的时间演化、比较新闻来源的偏见水平以及评分法律修订的偏见水平。在每个场景中，我们都能够解释和验证模型的输出结果，包括在训练数据外的两个领域中。此外，我们还使用模型对不同领域的总体偏见水平进行比较，发现法律文档最少偏见，新闻媒体最多偏见，Wikipedia文章处于中间。考虑其高性能、简单、可解释和广泛应用，我们希望这种模型能够为大量社区提供帮助，包括Wikipedia编辑、新闻编辑、政治和社会科学家以及一般公众。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/17/cs.CL_2023_07_17/" data-id="cloimip64007js488e1bsahs9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/17/cs.LG_2023_07_17/" class="article-date">
  <time datetime="2023-07-17T10:00:00.000Z" itemprop="datePublished">2023-07-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/cs.LG_2023_07_17/">cs.LG - 2023-07-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Study-on-the-Performance-of-Generative-Pre-trained-Transformer-GPT-in-Simulating-Depressed-Individuals-on-the-Standardized-Depressive-Symptom-Scale"><a href="#A-Study-on-the-Performance-of-Generative-Pre-trained-Transformer-GPT-in-Simulating-Depressed-Individuals-on-the-Standardized-Depressive-Symptom-Scale" class="headerlink" title="A Study on the Performance of Generative Pre-trained Transformer (GPT) in Simulating Depressed Individuals on the Standardized Depressive Symptom Scale"></a>A Study on the Performance of Generative Pre-trained Transformer (GPT) in Simulating Depressed Individuals on the Standardized Depressive Symptom Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08576">http://arxiv.org/abs/2307.08576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijin Cai, Nanfeng Zhang, Jiaying Zhu, Yanjie Liu, Yongjin Zhou<br>for:* 这个论文的目的是评估GPT技术在诊断抑郁症中的潜力。methods:* 这个论文使用了三种抑郁症评估工具（HAMD-17、SDS、GDS-15），并在两个实验中使用GPT模拟抑郁症和正常个体的响应。results:* GPT在抑郁症评估中表现出了准确的性能，能够与正常个体和抑郁症个体的反应保持一致。* GPT在不同的抑郁程度下的表现存在一些差异，其表现更好的是在高敏感度的评估工具上。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Background: Depression is a common mental disorder with societal and economic burden. Current diagnosis relies on self-reports and assessment scales, which have reliability issues. Objective approaches are needed for diagnosing depression. Objective: Evaluate the potential of GPT technology in diagnosing depression. Assess its ability to simulate individuals with depression and investigate the influence of depression scales. Methods: Three depression-related assessment tools (HAMD-17, SDS, GDS-15) were used. Two experiments simulated GPT responses to normal individuals and individuals with depression. Compare GPT's responses with expected results, assess its understanding of depressive symptoms, and performance differences under different conditions. Results: GPT's performance in depression assessment was evaluated. It aligned with scoring criteria for both individuals with depression and normal individuals. Some performance differences were observed based on depression severity. GPT performed better on scales with higher sensitivity. Conclusion: GPT accurately simulates individuals with depression and normal individuals during depression-related assessments. Deviations occur when simulating different degrees of depression, limiting understanding of mild and moderate cases. GPT performs better on scales with higher sensitivity, indicating potential for developing more effective depression scales. GPT has important potential in depression assessment, supporting clinicians and patients.
</details>
<details>
<summary>摘要</summary>
背景：抑郁是一种常见的心理疾病，带来社会和经济的负担。现在的诊断仍然基于自我报告和评估工具，这些工具的可靠性存在问题。需要更Objective的方法来诊断抑郁。目标：评估GPT技术在诊断抑郁方面的潜力。判断它能够模拟抑郁症状和调查不同程度的抑郁症状对响应。方法：使用三种抑郁相关的评估工具（HAMD-17、SDS、GDS-15）。进行两个实验，通过GPT对正常人和抑郁人群的响应进行模拟，与预期结果进行比较，评估它对抑郁症状的理解程度和不同情况下的表现差异。结果：GPT在抑郁诊断方面的表现被评估。它与正常人和抑郁人群的评分标准相符。有些情况下，随着抑郁的严重程度不同，GPT的表现会有所不同。GPT在感知度较高的评价工具上表现更好，表明GPT可能有助于开发更有效的抑郁评价工具。结论：GPT可以准确模拟正常人和抑郁人群在抑郁相关评估中的表现，但是在不同程度的抑郁中会出现一些差异。GPT在感知度较高的评价工具上表现更好，表明它可能有助于开发更有效的抑郁评价工具，支持临床医生和患者。
</details></li>
</ul>
<hr>
<h2 id="FedCME-Client-Matching-and-Classifier-Exchanging-to-Handle-Data-Heterogeneity-in-Federated-Learning"><a href="#FedCME-Client-Matching-and-Classifier-Exchanging-to-Handle-Data-Heterogeneity-in-Federated-Learning" class="headerlink" title="FedCME: Client Matching and Classifier Exchanging to Handle Data Heterogeneity in Federated Learning"></a>FedCME: Client Matching and Classifier Exchanging to Handle Data Heterogeneity in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08574">http://arxiv.org/abs/2307.08574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Nie, Danyang Xiao, Lei Yang, Weigang Wu</li>
<li>for: 本研究的目的是解决 federated learning 中的数据不同性问题，以提高模型的全球性性和性能。</li>
<li>methods: 本研究提出了一种新的 federated learning 框架，即 FedCME，通过客户端匹配和分类器交换来解决数据不同性问题。在本方法中，客户端的匹配和分类器交换可以更好地调整本地模型的训练方向，从而缓解本地更新偏移。此外，本研究还提出了一种特征对齐方法来增强特征提取器的训练。</li>
<li>results: 实验结果表明，FedCME 比 FedAvg、FedProx、MOON 和 FedRS 在 FMNIST 和 CIFAR10 等常用 federated learning 测试 benchmark 上表现更好，特别在数据不同性的情况下。<details>
<summary>Abstract</summary>
Data heterogeneity across clients is one of the key challenges in Federated Learning (FL), which may slow down the global model convergence and even weaken global model performance. Most existing approaches tackle the heterogeneity by constraining local model updates through reference to global information provided by the server. This can alleviate the performance degradation on the aggregated global model. Different from existing methods, we focus the information exchange between clients, which could also enhance the effectiveness of local training and lead to generate a high-performance global model. Concretely, we propose a novel FL framework named FedCME by client matching and classifier exchanging. In FedCME, clients with large differences in data distribution will be matched in pairs, and then the corresponding pair of clients will exchange their classifiers at the stage of local training in an intermediate moment. Since the local data determines the local model training direction, our method can correct update direction of classifiers and effectively alleviate local update divergence. Besides, we propose feature alignment to enhance the training of the feature extractor. Experimental results demonstrate that FedCME performs better than FedAvg, FedProx, MOON and FedRS on popular federated learning benchmarks including FMNIST and CIFAR10, in the case where data are heterogeneous.
</details>
<details>
<summary>摘要</summary>
“数据不同性是联邦学习（FL）中的一个关键挑战，可能会 slow down 全球模型的融合和甚至弱化全球模型的性能。现有的方法通常通过对本地模型更新进行约束，通过服务器提供的全球信息进行参考。这可以减轻全球模型的性能下降。与现有方法不同，我们将注重客户端之间的信息交换，可以提高本地训练的效iveness，并通过生成高性能的全球模型。具体来说，我们提出了一种新的联邦学习框架，名为FedCME。在FedCME中，客户端之间的数据分布差异较大的将被匹配，并在本地训练阶段进行交换类型的分类器。由于本地数据决定本地模型训练方向，我们的方法可以正确更新类ifiers，有效地缓解本地更新偏移。此外，我们还提出了特征对齐来强化特征提取器的训练。实验结果表明，FedCME在FMNIST和CIFAR10等流行的联邦学习 benchmark 上表现比 FedAvg、FedProx、MOON 和 FedRS 更好，即使数据具有不同性。”
</details></li>
</ul>
<hr>
<h2 id="Revisiting-the-Robustness-of-the-Minimum-Error-Entropy-Criterion-A-Transfer-Learning-Case-Study"><a href="#Revisiting-the-Robustness-of-the-Minimum-Error-Entropy-Criterion-A-Transfer-Learning-Case-Study" class="headerlink" title="Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study"></a>Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08572">http://arxiv.org/abs/2307.08572</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lpsilvestrin/mee-finetune">https://github.com/lpsilvestrin/mee-finetune</a></li>
<li>paper_authors: Luis Pedro Silvestrin, Shujian Yu, Mark Hoogendoorn</li>
<li>for: 本研究旨在探讨在实际任务中常见的分布偏移问题下，使用传输学习方法达到良好性能。</li>
<li>methods: 本研究使用了最小错误Entropy（MEE） criterion，一种广泛用于统计信号处理中对非高斯噪声的优化目标，并investigated its feasibility和usefulness在实际传输学习回归任务中。</li>
<li>results: 研究发现，通过简单地将基本传输学习算法中的MSE损失换为MEE，可以达到与现状技术传输学习算法相当的性能。<details>
<summary>Abstract</summary>
Coping with distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-of-the-art transfer learning algorithms. We justify our arguments on both synthetic data and 5 real-world time-series data.
</details>
<details>
<summary>摘要</summary>
handle distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-of-the-art transfer learning algorithms. We justify our arguments on both synthetic data and 5 real-world time-series data.Here's the translation in Traditional Chinese:handle distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-of-the-art transfer learning algorithms. We justify our arguments on both synthetic data and 5 real-world time-series data.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-with-Passive-Optical-Nonlinear-Mapping"><a href="#Deep-Learning-with-Passive-Optical-Nonlinear-Mapping" class="headerlink" title="Deep Learning with Passive Optical Nonlinear Mapping"></a>Deep Learning with Passive Optical Nonlinear Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08558">http://arxiv.org/abs/2307.08558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Xia, Kyungduk Kim, Yaniv Eliezer, Liam Shaughnessy, Sylvain Gigan, Hui Cao</li>
<li>for: 这项研究旨在开发一种基于光学加速器的 Deep Learning 系统，以提高人工智能的性能和能效性。</li>
<li>methods: 该研究使用了多散射在反射室中的技术，实现了无需加速器的光学非线性随机映射，以提高计算性能。</li>
<li>results: 研究发现，通过光学数据压缩和数字解码器，可以实现高性能、高压缩比的实时人体检测和其他计算任务。<details>
<summary>Abstract</summary>
Deep learning has fundamentally transformed artificial intelligence, but the ever-increasing complexity in deep learning models calls for specialized hardware accelerators. Optical accelerators can potentially offer enhanced performance, scalability, and energy efficiency. However, achieving nonlinear mapping, a critical component of neural networks, remains challenging optically. Here, we introduce a design that leverages multiple scattering in a reverberating cavity to passively induce optical nonlinear random mapping, without the need for additional laser power. A key advantage emerging from our work is that we show we can perform optical data compression, facilitated by multiple scattering in the cavity, to efficiently compress and retain vital information while also decreasing data dimensionality. This allows rapid optical information processing and generation of low dimensional mixtures of highly nonlinear features. These are particularly useful for applications demanding high-speed analysis and responses such as in edge computing devices. Utilizing rapid optical information processing capabilities, our optical platforms could potentially offer more efficient and real-time processing solutions for a broad range of applications. We demonstrate the efficacy of our design in improving computational performance across tasks, including classification, image reconstruction, key-point detection, and object detection, all achieved through optical data compression combined with a digital decoder. Notably, we observed high performance, at an extreme compression ratio, for real-time pedestrian detection. Our findings pave the way for novel algorithms and architectural designs for optical computing.
</details>
<details>
<summary>摘要</summary>
A key advantage emerging from our work is that we show we can perform optical data compression, facilitated by multiple scattering in the cavity, to efficiently compress and retain vital information while also decreasing data dimensionality. This allows rapid optical information processing and generation of low-dimensional mixtures of highly nonlinear features. These are particularly useful for applications demanding high-speed analysis and responses such as in edge computing devices.Utilizing rapid optical information processing capabilities, our optical platforms could potentially offer more efficient and real-time processing solutions for a broad range of applications. We demonstrate the efficacy of our design in improving computational performance across tasks, including classification, image reconstruction, key-point detection, and object detection, all achieved through optical data compression combined with a digital decoder. Notably, we observed high performance, at an extreme compression ratio, for real-time pedestrian detection.Our findings pave the way for novel algorithms and architectural designs for optical computing.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-based-Colorectal-Tissue-Classification-via-Acoustic-Resolution-Photoacoustic-Microscopy"><a href="#Machine-Learning-based-Colorectal-Tissue-Classification-via-Acoustic-Resolution-Photoacoustic-Microscopy" class="headerlink" title="Machine-Learning-based Colorectal Tissue Classification via Acoustic Resolution Photoacoustic Microscopy"></a>Machine-Learning-based Colorectal Tissue Classification via Acoustic Resolution Photoacoustic Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08556">http://arxiv.org/abs/2307.08556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shangqing Tong, Peng Ge, Yanan Jiao, Zhaofu Ma, Ziye Li, Longhai Liu, Feng Gao, Xiaohui Du, Fei Gao</li>
<li>for: 检测肠癌的有效方法</li>
<li>methods: 使用机器学习基于ARPAM技术进行肠部细胞分类</li>
<li>results: 通过多种机器学习方法对肠部细胞进行分类，并对结果进行量化和质量分析以评估方法效果<details>
<summary>Abstract</summary>
Colorectal cancer is a deadly disease that has become increasingly prevalent in recent years. Early detection is crucial for saving lives, but traditional diagnostic methods such as colonoscopy and biopsy have limitations. Colonoscopy cannot provide detailed information within the tissues affected by cancer, while biopsy involves tissue removal, which can be painful and invasive. In order to improve diagnostic efficiency and reduce patient suffering, we studied machine-learningbased approach for colorectal tissue classification that uses acoustic resolution photoacoustic microscopy (ARPAM). With this tool, we were able to classify benign and malignant tissue using multiple machine learning methods. Our results were analyzed both quantitatively and qualitatively to evaluate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
COLERECTAL CANCER 是一种致命的疾病，在最近几年内逐渐增加。早期检测是保存生命的关键，但传统的诊断方法，如colonoscopy和biopsy，有限制。colonoscopy无法提供癌变组织中的详细信息，而biopsy则需要组织切除，可能很痛苦和侵入性。为了改善诊断效率和减少患者的痛苦，我们研究了机器学习基于的抑制癌变组织分类方法，使用高分辨率光子振荡显微镜（ARPAM）。我们使用多种机器学习方法来分类健康和癌变组织。我们的结果经过了量化和质数分析，以评估我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Multi-class-point-cloud-completion-networks-for-3D-cardiac-anatomy-reconstruction-from-cine-magnetic-resonance-images"><a href="#Multi-class-point-cloud-completion-networks-for-3D-cardiac-anatomy-reconstruction-from-cine-magnetic-resonance-images" class="headerlink" title="Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images"></a>Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08535">http://arxiv.org/abs/2307.08535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Beetz, Abhirup Banerjee, Julius Ossenberg-Engels, Vicente Grau</li>
<li>for: 这篇论文是为了提出一种全自动的三维心脏形态重建pipeline，用于从硬件磁共振成像（cine MRI）数据中提取多类三维心脏形态模型。</li>
<li>methods: 该ipeline使用了一种新型的多类点云完成网络（PCCN）来解决三维重建任务中的稀疏性和不对称性问题，并且在大量的Synthetic数据集上进行了评估。</li>
<li>results: 研究发现，使用PCCN可以在不同程度的扫描角度下实现下面或相似于原始图像分辨率的 Chamfer距离，并且相比于参考的3D U-Net模型，减少了32%和24%的重建误差。此外，该ipeline在UK Biobank研究中的1000个主题中实现了准确且 topological plausible的双心脏形态模型，并且与之前的文献中的临床指标相似。 finally, 研究发现该方法在多种常见异常条件下的稳定性。<details>
<summary>Abstract</summary>
Cine magnetic resonance imaging (MRI) is the current gold standard for the assessment of cardiac anatomy and function. However, it typically only acquires a set of two-dimensional (2D) slices of the underlying three-dimensional (3D) anatomy of the heart, thus limiting the understanding and analysis of both healthy and pathological cardiac morphology and physiology. In this paper, we propose a novel fully automatic surface reconstruction pipeline capable of reconstructing multi-class 3D cardiac anatomy meshes from raw cine MRI acquisitions. Its key component is a multi-class point cloud completion network (PCCN) capable of correcting both the sparsity and misalignment issues of the 3D reconstruction task in a unified model. We first evaluate the PCCN on a large synthetic dataset of biventricular anatomies and observe Chamfer distances between reconstructed and gold standard anatomies below or similar to the underlying image resolution for multiple levels of slice misalignment. Furthermore, we find a reduction in reconstruction error compared to a benchmark 3D U-Net by 32% and 24% in terms of Hausdorff distance and mean surface distance, respectively. We then apply the PCCN as part of our automated reconstruction pipeline to 1000 subjects from the UK Biobank study in a cross-domain transfer setting and demonstrate its ability to reconstruct accurate and topologically plausible biventricular heart meshes with clinical metrics comparable to the previous literature. Finally, we investigate the robustness of our proposed approach and observe its capacity to successfully handle multiple common outlier conditions.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging (MRI) 是当今心脏形态和功能评估的标准金属。然而，它通常只取得心脏三维形态的二维图像，因此限制了对健康和疾病心脏形态和physiology的理解和分析。在这篇论文中，我们提议一种全自动表面重建管道，可以从raw cine MRI获得多类三维心脏形态矩阵。其关键组件是一种多类点云完成网络（PCCN），可以在三维重建任务中解决缺失和不对称问题。我们首先在大量的人工数据集上评估PCCN，并观察到下同图像分辨率下的Chamfer距离。此外，我们发现与参考3D U-Net模型相比，PCCN可以降低重建错误的 Hausdorff距离和平均表面距离，分别降低32%和24%。然后，我们将PCCN作为自动重建管道的一部分应用于UK Biobank研究中的1000名参与者，并证明它能够重建准确和可靠的两个心脏宫室表面矩阵，与前一代文献的临床指标相符。最后，我们调查了我们提议的方法的稳定性，并发现它可以成功处理多种常见异常情况。
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Processing-with-Linear-Optics"><a href="#Nonlinear-Processing-with-Linear-Optics" class="headerlink" title="Nonlinear Processing with Linear Optics"></a>Nonlinear Processing with Linear Optics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08533">http://arxiv.org/abs/2307.08533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustafa Yildirim, Niyazi Ulas Dinc, Ilker Oguz, Demetri Psaltis, Christophe Moser</li>
<li>for: 这个论文旨在实现多层光网络，并且解决在不使用电子组件的情况下实现多层光网络的挑战。</li>
<li>methods: 这篇论文提出了一种新的框架，使用多散射来实现可编程的线性和非线性变换，并且可以在低光功率 kontinuierender CW 光中实现非线性光计算。</li>
<li>results: 理论和实验研究表明，通过多散射重复数据可以实现低功率连续波光的非线性计算。<details>
<summary>Abstract</summary>
Deep neural networks have achieved remarkable breakthroughs by leveraging multiple layers of data processing to extract hidden representations, albeit at the cost of large electronic computing power. To enhance energy efficiency and speed, the optical implementation of neural networks aims to harness the advantages of optical bandwidth and the energy efficiency of optical interconnections. In the absence of low-power optical nonlinearities, the challenge in the implementation of multilayer optical networks lies in realizing multiple optical layers without resorting to electronic components. In this study, we present a novel framework that uses multiple scattering that is capable of synthesizing programmable linear and nonlinear transformations concurrently at low optical power by leveraging the nonlinear relationship between the scattering potential, represented by data, and the scattered field. Theoretical and experimental investigations show that repeating the data by multiple scattering enables non-linear optical computing at low power continuous wave light.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：深度神经网络在数据处理多层级处理中提取隐藏表示的成就很大，但是需要大量电子计算能力。为了提高能效性和速度， оптиче实现神经网络寻求利用光波宽频带和光通信的优点。在缺乏低功率光非线性下，实现多层光网络的挑战在于不使用电子组件实现多个光层。本研究提出了一种新的框架，利用多散射实现可编程的线性和非线性变换，并在低光力连续波光下实现非线性光计算。理论和实验研究表明，通过多散射复制数据可实现低功率连续波光非线性计算。</SYS>Here is the translation of the text into Simplified Chinese:深度神经网络在数据处理多层级处理中提取隐藏表示的成就很大，但是需要大量电子计算能力。为了提高能效性和速度， оптиче实现神经网络寻求利用光波宽频带和光通信的优点。在缺乏低功率光非线性下，实现多层光网络的挑战在于不使用电子组件实现多个光层。本研究提出了一种新的框架，利用多散射实现可编程的线性和非线性变换，并在低光力连续波光下实现非线性光计算。理论和实验研究表明，通过多散射复制数据可实现低功率连续波光非线性计算。
</details></li>
</ul>
<hr>
<h2 id="LuckyMera-a-Modular-AI-Framework-for-Building-Hybrid-NetHack-Agents"><a href="#LuckyMera-a-Modular-AI-Framework-for-Building-Hybrid-NetHack-Agents" class="headerlink" title="LuckyMera: a Modular AI Framework for Building Hybrid NetHack Agents"></a>LuckyMera: a Modular AI Framework for Building Hybrid NetHack Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08532">http://arxiv.org/abs/2307.08532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pervasive-ai-lab/luckymera">https://github.com/pervasive-ai-lab/luckymera</a></li>
<li>paper_authors: Luigi Quarantiello, Simone Marzeddu, Antonio Guzzi, Vincenzo Lomonaco</li>
<li>for: 这个论文的目的是提出一个可 configurable、可扩展的 AI 框架，用于在 NetHack 游戏中测试和训练 AI 代理人。</li>
<li>methods: 这个框架使用了 симвоlic 和神经网络学习方法，并提供了一些实用功能来保存经验和用于训练神经网络。</li>
<li>results: 经验证明，这个框架可以实现 state-of-the-art 的表现在完整的 NetHack 游戏中，并且提供了一个强大的基线代理人。<details>
<summary>Abstract</summary>
In the last few decades we have witnessed a significant development in Artificial Intelligence (AI) thanks to the availability of a variety of testbeds, mostly based on simulated environments and video games. Among those, roguelike games offer a very good trade-off in terms of complexity of the environment and computational costs, which makes them perfectly suited to test AI agents generalization capabilities. In this work, we present LuckyMera, a flexible, modular, extensible and configurable AI framework built around NetHack, a popular terminal-based, single-player roguelike video game. This library is aimed at simplifying and speeding up the development of AI agents capable of successfully playing the game and offering a high-level interface for designing game strategies. LuckyMera comes with a set of off-the-shelf symbolic and neural modules (called "skills"): these modules can be either hard-coded behaviors, or neural Reinforcement Learning approaches, with the possibility of creating compositional hybrid solutions. Additionally, LuckyMera comes with a set of utility features to save its experiences in the form of trajectories for further analysis and to use them as datasets to train neural modules, with a direct interface to the NetHack Learning Environment and MiniHack. Through an empirical evaluation we validate our skills implementation and propose a strong baseline agent that can reach state-of-the-art performances in the complete NetHack game. LuckyMera is open-source and available at https://github.com/Pervasive-AI-Lab/LuckyMera.
</details>
<details>
<summary>摘要</summary>
在过去几十年中，人工智能（AI）领域已经经历了 significative 的发展，很大一部分归功于各种测试环境和游戏的可用性。 Among them, roguelike 游戏提供了一个非常好的复杂性环境和计算成本的trade-off，使其成为测试 AI 代理的 идеal 选择。在这项工作中，我们介绍了 LuckyMera，一个灵活、可模块化、可扩展和配置化的 AI 框架，基于 NetHack terminal 基于单player 游戏。这个库的目标是为 AI 代理的开发提供简单化和加速的方式，并提供一个高级接口来设计游戏策略。LuckyMera 包含一些固定的符号学模块（called "skills"）和神经网络学习方法，以及可创建compositional 混合解决方案。此外，LuckyMera 还提供了一些实用功能，以保存其经验为 trajectories 进行后续分析，并直接与 NetHack Learning Environment 和 MiniHack 进行交互。通过实验 validate 我们的技能实现，并提出了一个强大的基线代理，可以在完整的 NetHack 游戏中达到状态艺术表现。LuckyMera 是开源的，可以在 https://github.com/Pervasive-AI-Lab/LuckyMera 上获取。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Lagrangian-Turbulence-by-Generative-Diffusion-Models"><a href="#Synthetic-Lagrangian-Turbulence-by-Generative-Diffusion-Models" class="headerlink" title="Synthetic Lagrangian Turbulence by Generative Diffusion Models"></a>Synthetic Lagrangian Turbulence by Generative Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08529">http://arxiv.org/abs/2307.08529</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smartturb/diffusion-lagr">https://github.com/smartturb/diffusion-lagr</a></li>
<li>paper_authors: Tianyi Li, Luca Biferale, Fabio Bonaccorso, Martino Andrea Scarpolini, Michele Buzzicotti</li>
<li>For: The paper aims to generate single-particle trajectories in three-dimensional turbulence at high Reynolds numbers using a machine learning approach.* Methods: The paper proposes a state-of-the-art Diffusion Model to generate the trajectories, which bypasses the need for direct numerical simulations or experiments to obtain reliable Lagrangian data.* Results: The model demonstrates the ability to quantitatively reproduce all relevant statistical benchmarks over the entire range of time scales, including the presence of fat tails distribution for the velocity increments, anomalous power law, and enhancement of intermittency around the dissipative scale. The model also exhibits good generalizability for extreme events, achieving unprecedented intensity and rarity.<details>
<summary>Abstract</summary>
Lagrangian turbulence lies at the core of numerous applied and fundamental problems related to the physics of dispersion and mixing in engineering, bio-fluids, atmosphere, oceans, and astrophysics. Despite exceptional theoretical, numerical, and experimental efforts conducted over the past thirty years, no existing models are capable of faithfully reproducing statistical and topological properties exhibited by particle trajectories in turbulence. We propose a machine learning approach, based on a state-of-the-art Diffusion Model, to generate single-particle trajectories in three-dimensional turbulence at high Reynolds numbers, thereby bypassing the need for direct numerical simulations or experiments to obtain reliable Lagrangian data. Our model demonstrates the ability to quantitatively reproduce all relevant statistical benchmarks over the entire range of time scales, including the presence of fat tails distribution for the velocity increments, anomalous power law, and enhancement of intermittency around the dissipative scale. The model exhibits good generalizability for extreme events, achieving unprecedented intensity and rarity. This paves the way for producing synthetic high-quality datasets for pre-training various downstream applications of Lagrangian turbulence.
</details>
<details>
<summary>摘要</summary>
拉格朗日流动在许多应用和基础问题中扮演重要角色，包括物理杂化和混合在工程、生物流体、大气、海洋和astrophysics中。尽管过去三十年来有过 Exceptional theoretical, numerical, and experimental efforts，但现有的模型无法准确地复制流动中粒子轨迹的统计和 тополоڤ���IC Properties。我们提出一种基于当前最佳Diffusion Model的机器学习方法，可以在高 Reynolds 数下生成三维流动中的单粒子轨迹，并且不需要直接进行数值 simulate or experiment to obtain reliable Lagrangian data。我们的模型能够准确地复制所有有关时间尺度的统计标准，包括粒子增量的轮廓分布、罕见的功率律和在混合度下的增强。这种模型具有良好的通用性，能够生成extreme events，达到了历史上最高的Intensity和罕见性。这些Synthetic高质量数据可以用于PRE-TRAINING various downstream应用程序。
</details></li>
</ul>
<hr>
<h2 id="Multi-Domain-Learning-with-Modulation-Adapters"><a href="#Multi-Domain-Learning-with-Modulation-Adapters" class="headerlink" title="Multi-Domain Learning with Modulation Adapters"></a>Multi-Domain Learning with Modulation Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08528">http://arxiv.org/abs/2307.08528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekaterina Iakovleva, Karteek Alahari, Jakob Verbeek</li>
<li>For: The paper is written for computer vision tasks, specifically for image classification across multiple domains.* Methods: The paper introduces Modulation Adapters, which update the convolutional filter weights of the model in a multiplicative manner for each task. The adaptation weights are parameterized in a factored manner, allowing for flexible scaling of the number of per-task parameters and different parameter-accuracy trade-offs.* Results: The approach yields excellent results on the Visual Decathlon challenge and the ImageNet-to-Sketch benchmark, with accuracies that are comparable to or better than those of existing state-of-the-art approaches.<details>
<summary>Abstract</summary>
Deep convolutional networks are ubiquitous in computer vision, due to their excellent performance across different tasks for various domains. Models are, however, often trained in isolation for each task, failing to exploit relatedness between tasks and domains to learn more compact models that generalise better in low-data regimes. Multi-domain learning aims to handle related tasks, such as image classification across multiple domains, simultaneously. Previous work on this problem explored the use of a pre-trained and fixed domain-agnostic base network, in combination with smaller learnable domain-specific adaptation modules. In this paper, we introduce Modulation Adapters, which update the convolutional filter weights of the model in a multiplicative manner for each task. Parameterising these adaptation weights in a factored manner allows us to scale the number of per-task parameters in a flexible manner, and to strike different parameter-accuracy trade-offs. We evaluate our approach on the Visual Decathlon challenge, composed of ten image classification tasks across different domains, and on the ImageNet-to-Sketch benchmark, which consists of six image classification tasks. Our approach yields excellent results, with accuracies that are comparable to or better than those of existing state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Image-Captions-are-Natural-Prompts-for-Text-to-Image-Models"><a href="#Image-Captions-are-Natural-Prompts-for-Text-to-Image-Models" class="headerlink" title="Image Captions are Natural Prompts for Text-to-Image Models"></a>Image Captions are Natural Prompts for Text-to-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08526">http://arxiv.org/abs/2307.08526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiye Lei, Hao Chen, Sen Zhang, Bo Zhao, Dacheng Tao</li>
<li>for: 提高文本到图像生成模型的训练数据 Informative 性和多样性</li>
<li>methods: 使用高级captioning模型生成图像描述，并将描述与分类名称 concatenate 用作生成模型的训练数据</li>
<li>results: 对ImageNette、ImageNet-100和ImageNet-1K进行了广泛的实验，并 verify 了我们的方法可以significantly improve 模型在 sintetic 训练数据上的表现，即平均提高10%的分类精度。<details>
<summary>Abstract</summary>
With the rapid development of Artificial Intelligence Generated Content (AIGC), it has become common practice in many learning tasks to train or fine-tune large models on synthetic data due to the data-scarcity and privacy leakage problems. Albeit promising with unlimited data generation, owing to massive and diverse information conveyed in real images, it is challenging for text-to-image generative models to synthesize informative training data with hand-crafted prompts, which usually leads to inferior generalization performance when training downstream models. In this paper, we theoretically analyze the relationship between the training effect of synthetic data and the synthetic data distribution induced by prompts. Then we correspondingly propose a simple yet effective method that prompts text-to-image generative models to synthesize more informative and diverse training data. Specifically, we caption each real image with the advanced captioning model to obtain informative and faithful prompts that extract class-relevant information and clarify the polysemy of class names. The image captions and class names are concatenated to prompt generative models for training image synthesis. Extensive experiments on ImageNette, ImageNet-100, and ImageNet-1K verify that our method significantly improves the performance of models trained on synthetic training data, i.e., 10% classification accuracy improvements on average.
</details>
<details>
<summary>摘要</summary>
随着人工智能生成内容（AIGC）的快速发展，在许多学习任务中通常是通过人工生成的数据进行训练或细化大型模型，因为实际数据的缺乏和隐私泄露问题。虽然人工生成的数据具有无限数据的优势，但是由于实际图像中含有庞大和多样化的信息， Text-to-image生成模型很难以通过手工提示生成有用的训练数据，这通常会导致下游模型训练时的泛化性能差。在这篇论文中，我们 theoretically 分析了人工生成数据训练效果和提示数据分布之间的关系，然后对应提出了一种简单 yet effective的方法。具体来说，我们使用高级描述模型将每个实际图像描述为 faithful 和有用的提示，以提取类别相关的信息并减少类名的多义性。图像描述和类别名称 concatenated 作为提示生成模型进行训练图像生成。我们在 ImageNette、ImageNet-100 和 ImageNet-1K 进行了广泛的实验，结果显示，我们的方法可以在使用人工生成数据进行训练时提高模型的性能，即平均提高10%的分类精度。
</details></li>
</ul>
<hr>
<h2 id="Results-on-Counterfactual-Invariance"><a href="#Results-on-Counterfactual-Invariance" class="headerlink" title="Results on Counterfactual Invariance"></a>Results on Counterfactual Invariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08519">http://arxiv.org/abs/2307.08519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jake Fawkes, Robin J. Evans</li>
<li>for: 本文提供了对Counterfactual invariants的理论分析。</li>
<li>methods: 文章presented a variety of existing definitions, studied their relationships and graphical implications.</li>
<li>results: 文章showed that counterfactual invariance implies conditional independence, but conditional independence does not provide any information about the likelihood of satisfying counterfactual invariance. Additionally, for discrete causal models, counterfactually invariant functions are often restricted to being functions of specific variables or constant.Here’s the same information in Traditional Chinese:</li>
<li>for: 本文的目的是提供Counterfactual invariants的理论分析。</li>
<li>methods: 文章使用了多种现有的定义，研究它们之间的关系和图形 implications。</li>
<li>results: 文章显示了Counterfactual invariance implies conditional independence, but conditional independence does not provide any information about the likelihood of satisfying counterfactual invariance. In addition, for discrete causal models, counterfactually invariant functions are often restricted to being functions of specific variables or constant.<details>
<summary>Abstract</summary>
In this paper we provide a theoretical analysis of counterfactual invariance. We present a variety of existing definitions, study how they relate to each other and what their graphical implications are. We then turn to the current major question surrounding counterfactual invariance, how does it relate to conditional independence? We show that whilst counterfactual invariance implies conditional independence, conditional independence does not give any implications about the degree or likelihood of satisfying counterfactual invariance. Furthermore, we show that for discrete causal models counterfactually invariant functions are often constrained to be functions of particular variables, or even constant.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提供了对Counterfactual invariants的理论分析。我们提供了多种现有的定义，研究它们之间的关系以及它们在图形上的含义。然后，我们转向现在主要关注的问题：Counterfactual invariants与Conditional independence之间的关系。我们表明，Counterfactual invariants imply Conditional independence,但Conditional independence不能提供关于满足Counterfactual invariants的度或概率的任何信息。此外，我们表明，对于排序 causal模型，Counterfactually invariants的函数frequently是特定变量或常数。
</details></li>
</ul>
<hr>
<h2 id="Kernel-Based-Testing-for-Single-Cell-Differential-Analysis"><a href="#Kernel-Based-Testing-for-Single-Cell-Differential-Analysis" class="headerlink" title="Kernel-Based Testing for Single-Cell Differential Analysis"></a>Kernel-Based Testing for Single-Cell Differential Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08509">http://arxiv.org/abs/2307.08509</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anthoozier/kernel_testsda">https://github.com/anthoozier/kernel_testsda</a></li>
<li>paper_authors: Anthony Ozier-Lafontaine, Camille Fourneaux, Ghislain Durif, Céline Vallot, Olivier Gandrillon, Sandrine Giraud, Bertrand Michel, Franck Picard</li>
<li>for: 这种方法用于比较单个细胞中分子特征的分布, 例如基因表达和epigenomic修饰。</li>
<li>methods: 该方法基于kernel embedding的非线性比较框架, 可以进行单元细胞特征之间的 feature-wise 分析以及全面的 transcriptome 或 epigenome 比较, 考虑到它们的复杂依赖关系。</li>
<li>results: 该方法可以检测单元细胞中的不同类型 células, 并且可以成功地识别在分化过程中的细胞在转化过程中的阶段。 此外，通过分析单元细胞 ChIP-Seq 数据, 该方法可以找到不受治疗的乳腺癌细胞中的 persistenter 细胞 под类型。<details>
<summary>Abstract</summary>
Single-cell technologies have provided valuable insights into the distribution of molecular features, such as gene expression and epigenomic modifications. However, comparing these complex distributions in a controlled and powerful manner poses methodological challenges. Here we propose to benefit from the kernel-testing framework to compare the complex cell-wise distributions of molecular features in a non-linear manner based on their kernel embedding. Our framework not only allows for feature-wise analyses but also enables global comparisons of transcriptomes or epigenomes, considering their intricate dependencies. By using a classifier to discriminate cells based on the variability of their embedding, our method uncovers heterogeneities in cell populations that would otherwise go undetected. We show that kernel testing overcomes the limitations of differential analysis methods dedicated to single-cell. Kernel testing is applied to investigate the reversion process of differentiating cells, successfully identifying cells in transition between reversion and differentiation stages. Additionally, we analyze single-cell ChIP-Seq data and identify a subpopulation of untreated breast cancer cells that exhibit an epigenomic profile similar to persister cells.
</details>
<details>
<summary>摘要</summary>
单元技术提供了价值的内在分布的分析，如基因表达和聚合酶修饰。然而，对这些复杂的分布进行控制和有力的比较具有挑战性。我们提议利用kernel-测试框架来比较单元细胞水平的分布，以非线性方式基于其内存映射。我们的框架不仅允许特征值分析，而且允许全球比较单元胞营养或者聚合酶修饰，考虑其复杂的依赖关系。通过使用一个分类器来根据单元细胞的变化程度来识别单元细胞，我们的方法揭示了单元细胞群体中的异质性，这些异质性否则可能会被忽略。我们在研究单元细胞的还原过程中成功地使用kernel测试方法，并成功地识别在还原和分化过程中的单元细胞。此外，我们分析单元细胞ChIP-Seq数据，并发现一个未经治疗的乳腺癌细胞subpopulation，其聚合酶修饰profile与持续细胞类似。
</details></li>
</ul>
<hr>
<h2 id="Efficient-and-Accurate-Optimal-Transport-with-Mirror-Descent-and-Conjugate-Gradients"><a href="#Efficient-and-Accurate-Optimal-Transport-with-Mirror-Descent-and-Conjugate-Gradients" class="headerlink" title="Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients"></a>Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08507">http://arxiv.org/abs/2307.08507</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adaptive-agents-lab/mdot-pncg">https://github.com/adaptive-agents-lab/mdot-pncg</a></li>
<li>paper_authors: Mete Kemertas, Allan D. Jepson, Amir-massoud Farahmand</li>
<li>for: 本文提出了一种新的优化运输算法，基于优化运输、投影下降和 conjugate gradients 等方法。</li>
<li>methods: 该算法可以计算优化运输成本，无论精度如何，而不会遇到数值稳定性问题。它利用 GPU 进行高效实现，并在许多情况下比传统算法 such as Sinkhorn’s Algorithm 更快 converges。</li>
<li>results: 我们对 marginal 分布 entropy 进行了特别关注，并证明高 entropy marginals 会导致更难的优化运输问题，而我们的算法适合这类问题。我们还进行了精心的减少分析，并对算法和问题参数进行了精心的调整。我们的结果表明，我们的算法可以为优化运输问题提供一个有用的工具。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/adaptive-agents-lab/MDOT-PNCG">https://github.com/adaptive-agents-lab/MDOT-PNCG</a> 上获取。<details>
<summary>Abstract</summary>
We design a novel algorithm for optimal transport by drawing from the entropic optimal transport, mirror descent and conjugate gradients literatures. Our algorithm is able to compute optimal transport costs with arbitrary accuracy without running into numerical stability issues. The algorithm is implemented efficiently on GPUs and is shown empirically to converge more quickly than traditional algorithms such as Sinkhorn's Algorithm both in terms of number of iterations and wall-clock time in many cases. We pay particular attention to the entropy of marginal distributions and show that high entropy marginals make for harder optimal transport problems, for which our algorithm is a good fit. We provide a careful ablation analysis with respect to algorithm and problem parameters, and present benchmarking over the MNIST dataset. The results suggest that our algorithm can be a useful addition to the practitioner's optimal transport toolkit. Our code is open-sourced at https://github.com/adaptive-agents-lab/MDOT-PNCG .
</details>
<details>
<summary>摘要</summary>
我们设计了一种新的优化交通算法，基于优化交通、镜像下降和 conjugate gradients 文献。我们的算法可以计算优化交通成本，无论精度如何，而不会遇到数值稳定性问题。我们的算法可以高效地运行在 GPU 上，并在许多情况下被证明可以更快 converge than 传统的算法，如 sinkhorn 算法，以数 iteration 和墙 clock 时间来说。我们特别关注到 marginal 分布的 entropy，并证明高 entropy marginal 会导致更难的优化交通问题，而我们的算法适合这种情况。我们进行了仔细的减少分析，并对算法和问题参数进行了精心的调整。我们在 MNIST 数据集上进行了 benchmarking，结果表明，我们的算法可以成为优化交通工具箱中的有用工具。我们的代码可以在 https://github.com/adaptive-agents-lab/MDOT-PNCG 上获取。
</details></li>
</ul>
<hr>
<h2 id="Does-Visual-Pretraining-Help-End-to-End-Reasoning"><a href="#Does-Visual-Pretraining-Help-End-to-End-Reasoning" class="headerlink" title="Does Visual Pretraining Help End-to-End Reasoning?"></a>Does Visual Pretraining Help End-to-End Reasoning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08506">http://arxiv.org/abs/2307.08506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Sun, Calvin Luo, Xingyi Zhou, Anurag Arnab, Cordelia Schmid</li>
<li>for:  investigate whether end-to-end learning of visual reasoning can be achieved with general-purpose neural networks, and confirm the feasibility of a neural network “generalist” to solve visual recognition and reasoning tasks.</li>
<li>methods: use a simple and general self-supervised framework which “compresses” each video frame into a small set of tokens with a transformer network, and reconstructs the remaining frames based on the compressed temporal context.</li>
<li>results: observe that pretraining is essential to achieve compositional generalization for end-to-end visual reasoning, and our proposed framework outperforms traditional supervised pretraining, including image classification and explicit object detection, by large margins.<details>
<summary>Abstract</summary>
We aim to investigate whether end-to-end learning of visual reasoning can be achieved with general-purpose neural networks, with the help of visual pretraining. A positive result would refute the common belief that explicit visual abstraction (e.g. object detection) is essential for compositional generalization on visual reasoning, and confirm the feasibility of a neural network "generalist" to solve visual recognition and reasoning tasks. We propose a simple and general self-supervised framework which "compresses" each video frame into a small set of tokens with a transformer network, and reconstructs the remaining frames based on the compressed temporal context. To minimize the reconstruction loss, the network must learn a compact representation for each image, as well as capture temporal dynamics and object permanence from temporal context. We perform evaluation on two visual reasoning benchmarks, CATER and ACRE. We observe that pretraining is essential to achieve compositional generalization for end-to-end visual reasoning. Our proposed framework outperforms traditional supervised pretraining, including image classification and explicit object detection, by large margins.
</details>
<details>
<summary>摘要</summary>
我们的目标是 investigate  Whether end-to-end 视觉逻辑学习可以通过通用神经网络实现，帮助了由 visual pretraining 。一个正面的结果会证明 Explicit 视觉抽象（例如对象检测）不是必要的 для compositional generalization 的视觉逻辑任务，并证明神经网络 "通用" 可以解决视识别和逻辑任务。我们提出了一个简单和通用的自我超vised framework，将每帧视频图片"压缩" 成一个小集合 of tokens 使用 transformer 网络，并使用压缩的时间上下文来重建剩下的帧。为了降低重建损失，网络必须学习每幅图片的紧凑表示，以及从时间上下文中捕捉时间动态和对象的持续性。我们在 CATER 和 ACRE 两个视觉逻辑标准benchmark上进行评估，发现预训练是必要的，以实现 compositional generalization 的 end-to-end 视觉逻辑学习。我们提出的方法在图像分类和显式对象检测的传统预训练下，都有大幅度的优势。
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-Evaluation-of-Topic-Models-and-Dimensionality-Reduction-Methods-for-2D-Text-Spatialization"><a href="#Large-Scale-Evaluation-of-Topic-Models-and-Dimensionality-Reduction-Methods-for-2D-Text-Spatialization" class="headerlink" title="Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization"></a>Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11770">http://arxiv.org/abs/2307.11770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cgshpi/topic-models-and-dimensionality-reduction-benchmark">https://github.com/cgshpi/topic-models-and-dimensionality-reduction-benchmark</a></li>
<li>paper_authors: Daniel Atzberger, Tim Cech, Willy Scheibel, Matthias Trapp, Rico Richter, Jürgen Döllner, Tobias Schreck</li>
<li>for: 这个论文的目的是 investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots.</li>
<li>methods: 该论文使用了多种主题模型和维度减少算法，并对它们的组合进行了大规模的计算评估。</li>
<li>results: 根据计算结果， interpretable topic models 能够很好地捕捉文本 Corpora 的结构，而 t-SNE 作为维度减少算法也有良好的效果。<details>
<summary>Abstract</summary>
Topic models are a class of unsupervised learning algorithms for detecting the semantic structure within a text corpus. Together with a subsequent dimensionality reduction algorithm, topic models can be used for deriving spatializations for text corpora as two-dimensional scatter plots, reflecting semantic similarity between the documents and supporting corpus analysis. Although the choice of the topic model, the dimensionality reduction, and their underlying hyperparameters significantly impact the resulting layout, it is unknown which particular combinations result in high-quality layouts with respect to accuracy and perception metrics. To investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots (or basis for landscape-type visualizations), we present a large-scale, benchmark-based computational evaluation. Our evaluation consists of (1) a set of corpora, (2) a set of layout algorithms that are combinations of topic models and dimensionality reductions, and (3) quality metrics for quantifying the resulting layout. The corpora are given as document-term matrices, and each document is assigned to a thematic class. The chosen metrics quantify the preservation of local and global properties and the perceptual effectiveness of the two-dimensional scatter plots. By evaluating the benchmark on a computing cluster, we derived a multivariate dataset with over 45 000 individual layouts and corresponding quality metrics. Based on the results, we propose guidelines for the effective design of text spatializations that are based on topic models and dimensionality reductions. As a main result, we show that interpretable topic models are beneficial for capturing the structure of text corpora. We furthermore recommend the use of t-SNE as a subsequent dimensionality reduction.
</details>
<details>
<summary>摘要</summary>
To investigate the effectiveness of topic models and dimensionality reduction methods for spatializing text corpora as two-dimensional scatter plots, we conducted a large-scale, benchmark-based computational evaluation. Our evaluation consisted of three parts:1. A set of corpora, given as document-term matrices, with each document assigned to a thematic class.2. A set of layout algorithms that combined topic models and dimensionality reductions.3. Quality metrics to quantify the resulting layout, including the preservation of local and global properties and the perceptual effectiveness of the two-dimensional scatter plots.We evaluated the benchmark on a computing cluster and derived a multivariate dataset with over 45,000 individual layouts and corresponding quality metrics. Our results show that interpretable topic models are beneficial for capturing the structure of text corpora, and we recommend the use of t-SNE as a subsequent dimensionality reduction. Based on our findings, we propose guidelines for the effective design of text spatializations that are based on topic models and dimensionality reductions.
</details></li>
</ul>
<hr>
<h2 id="Can-We-Trust-Race-Prediction"><a href="#Can-We-Trust-Race-Prediction" class="headerlink" title="Can We Trust Race Prediction?"></a>Can We Trust Race Prediction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08496">http://arxiv.org/abs/2307.08496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cangyuanli/pyethnicity">https://github.com/cangyuanli/pyethnicity</a></li>
<li>paper_authors: Cangyuan Li</li>
<li>for: 这个论文是为了提高美国选民登记数据中的人口统计和地理编码的准确性而写的。</li>
<li>methods: 作者使用了irectional Long Short-Term Memory (BiLSTM) 模型和一个ensemble模型，并使用了一个新的选民登记数据集来提高模型的性能。</li>
<li>results: 作者通过创建了一个全面的美国人名和姓氏分布数据库，并提供了一个高品质的比较数据集，以提高 bayesian improved surname geocoding (BISG) 和 bayesian improved firstname surname geocoding (BIFSG) 的准确性。<details>
<summary>Abstract</summary>
In the absence of sensitive race and ethnicity data, researchers, regulators, and firms alike turn to proxies. In this paper, I train a Bidirectional Long Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data from all 50 US states and create an ensemble that achieves up to 36.8% higher out of sample (OOS) F1 scores than the best performing machine learning models in the literature. Additionally, I construct the most comprehensive database of first and surname distributions in the US in order to improve the coverage and accuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved Firstname Surname Geocoding (BIFSG). Finally, I provide the first high-quality benchmark dataset in order to fairly compare existing models and aid future model developers.
</details>
<details>
<summary>摘要</summary>
在没有敏感的种族和族谱数据的情况下，研究人员、监管机构和公司都会寻找代理。在这篇论文中，我将一个 bidirectional Long Short-Term Memory（BiLSTM）模型训练在所有50个美国州的选民登记数据上，创建了一个ensemble，其在验证样本外的F1分数高达36.8%，较文献中最高performing机器学习模型高。此外，我还建立了美国最完整的姓名和名字分布数据库，以提高Bayesian Improved Surname Geocoding（BISG）和Bayesian Improved Firstname Surname Geocoding（BIFSG）的覆盖和精度。最后，我提供了第一个高品质的比较基准集，以便比较现有模型和未来的模型开发者。
</details></li>
</ul>
<hr>
<h2 id="Fairness-in-KI-Systemen"><a href="#Fairness-in-KI-Systemen" class="headerlink" title="Fairness in KI-Systemen"></a>Fairness in KI-Systemen</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08486">http://arxiv.org/abs/2307.08486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Janine Strotherm, Alissa Müller, Barbara Hammer, Benjamin Paaßen</li>
<li>for: 本文提供了关于机器学习中的公平研究的导入，包括主要的公平定义和实现公平的策略。</li>
<li>methods: 本文使用了可见的示例和图像来解释公平定义和策略，适用于多学科读者。</li>
<li>results: 本文提供了一个欧洲Context中的公平研究的导入，包括主要的公平定义和实现公平的策略。<details>
<summary>Abstract</summary>
The more AI-assisted decisions affect people's lives, the more important the fairness of such decisions becomes. In this chapter, we provide an introduction to research on fairness in machine learning. We explain the main fairness definitions and strategies for achieving fairness using concrete examples and place fairness research in the European context. Our contribution is aimed at an interdisciplinary audience and therefore avoids mathematical formulation but emphasizes visualizations and examples.   --   Je mehr KI-gest\"utzte Entscheidungen das Leben von Menschen betreffen, desto wichtiger ist die Fairness solcher Entscheidungen. In diesem Kapitel geben wir eine Einf\"uhrung in die Forschung zu Fairness im maschinellen Lernen. Wir erkl\"aren die wesentlichen Fairness-Definitionen und Strategien zur Erreichung von Fairness anhand konkreter Beispiele und ordnen die Fairness-Forschung in den europ\"aischen Kontext ein. Unser Beitrag richtet sich dabei an ein interdisziplin\"ares Publikum und verzichtet daher auf die mathematische Formulierung sondern betont Visualisierungen und Beispiele.
</details>
<details>
<summary>摘要</summary>
更多的AI助け的决策对人们的生活产生了影响，因此决策的公正性变得越来越重要。在这章中，我们提供了对决策公正性的研究Introduction，并解释了主要的公正性定义和在实际示例中实现公正性的策略。我们的贡献是向多学科读者群体进行了定向，因此弃用了数学表述，而是强调视觉化和示例。In this chapter, we provide an introduction to research on fairness in machine learning. We explain the main fairness definitions and strategies for achieving fairness using concrete examples and place fairness research in the European context. Our contribution is aimed at an interdisciplinary audience and therefore avoids mathematical formulation but emphasizes visualizations and examples. As AI-assisted decisions increasingly affect people's lives, the fairness of such decisions becomes more important.
</details></li>
</ul>
<hr>
<h2 id="Cross-Feature-Selection-to-Eliminate-Spurious-Interactions-and-Single-Feature-Dominance-Explainable-Boosting-Machines"><a href="#Cross-Feature-Selection-to-Eliminate-Spurious-Interactions-and-Single-Feature-Dominance-Explainable-Boosting-Machines" class="headerlink" title="Cross Feature Selection to Eliminate Spurious Interactions and Single Feature Dominance Explainable Boosting Machines"></a>Cross Feature Selection to Eliminate Spurious Interactions and Single Feature Dominance Explainable Boosting Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08485">http://arxiv.org/abs/2307.08485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shree Charran R, Sandipan Das Mahapatra</li>
<li>for: 本研究旨在提高EBM模型的解释性和可靠性，并应用于各种预测任务中。</li>
<li>methods: 本研究使用了 alternate 横向选择、集成特征和模型配置变更等技术来解决EBM模型中的干扰和单个特征主导问题。</li>
<li>results: 对三个 benchmark 数据集进行了评估，结果表明 alternate 技术可以超越原始 EBM 方法，提供更好的解释性和特征选择稳定性，并提高模型的预测性能。<details>
<summary>Abstract</summary>
Interpretability is a crucial aspect of machine learning models that enables humans to understand and trust the decision-making process of these models. In many real-world applications, the interpretability of models is essential for legal, ethical, and practical reasons. For instance, in the banking domain, interpretability is critical for lenders and borrowers to understand the reasoning behind the acceptance or rejection of loan applications as per fair lending laws. However, achieving interpretability in machine learning models is challenging, especially for complex high-performance models. Hence Explainable Boosting Machines (EBMs) have been gaining popularity due to their interpretable and high-performance nature in various prediction tasks. However, these models can suffer from issues such as spurious interactions with redundant features and single-feature dominance across all interactions, which can affect the interpretability and reliability of the model's predictions. In this paper, we explore novel approaches to address these issues by utilizing alternate Cross-feature selection, ensemble features and model configuration alteration techniques. Our approach involves a multi-step feature selection procedure that selects a set of candidate features, ensemble features and then benchmark the same using the EBM model. We evaluate our method on three benchmark datasets and show that the alternate techniques outperform vanilla EBM methods, while providing better interpretability and feature selection stability, and improving the model's predictive performance. Moreover, we show that our approach can identify meaningful interactions and reduce the dominance of single features in the model's predictions, leading to more reliable and interpretable models.   Index Terms- Interpretability, EBM's, ensemble, feature selection.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>机器学习模型的可解释性是一个关键的特点，它使得人们可以更好地理解和信任机器学习模型的决策过程。在实际应用中，机器学习模型的可解释性是非常重要的，特别是在银行领域。在这个领域中，可解释性是对借款申请的 Accept 或 Reject 决策进行法律、伦理和实用上的要求。然而，实现机器学习模型的可解释性是一个挑战，特别是在复杂高性能模型中。因此，可解释性增强的机器学习模型（EBM）在各种预测任务中得到了广泛的应用。然而，这些模型可能会面临一些问题，如 redundancy 特征之间的干扰和单个特征在所有交互中的占主导地位，这些问题可能会影响模型预测的可靠性和解释性。在这篇论文中，我们探讨了一些新的方法来解决这些问题，包括使用另一种 Cross-feature 选择、ensemble 特征和模型配置变化技术。我们的方法包括一个多步特征选择过程，选择一组候选特征、ensemble特征，然后使用 EBM 模型来评估。我们在三个 benchmark 数据集上评估了我们的方法，并显示了它们在比vanilla EBM方法更高的可解释性和特征选择稳定性，以及提高模型预测性能。此外，我们还发现了我们的方法可以找到有意义的交互和减少单个特征在模型预测中的主导地位，从而提高模型的可靠性和解释性。Index Terms- 可解释性, EBM, ensemble, 特征选择.
</details></li>
</ul>
<hr>
<h2 id="A-Fast-Task-Offloading-Optimization-Framework-for-IRS-Assisted-Multi-Access-Edge-Computing-System"><a href="#A-Fast-Task-Offloading-Optimization-Framework-for-IRS-Assisted-Multi-Access-Edge-Computing-System" class="headerlink" title="A Fast Task Offloading Optimization Framework for IRS-Assisted Multi-Access Edge Computing System"></a>A Fast Task Offloading Optimization Framework for IRS-Assisted Multi-Access Edge Computing System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08474">http://arxiv.org/abs/2307.08474</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uic-jq/iopo">https://github.com/uic-jq/iopo</a></li>
<li>paper_authors: Jianqiu Wu, Zhongyi Yu, Jianxiong Guo, Zhiqing Tang, Tian Wang, Weijia Jia</li>
<li>for: 这个论文旨在提高无线网络，尤其是基于飞行器多访问边缘计算系统。</li>
<li>methods: 该论文提出了一种基于深度学习的优化框架，称为迭代保持顺序决策（IOPO），用于生成高质量的任务卸载分配。</li>
<li>results: 实验结果表明，该提议的框架可以在很短的时间内生成高效的任务卸载决策，超过其他标准方法。<details>
<summary>Abstract</summary>
Terahertz communication networks and intelligent reflecting surfaces exhibit significant potential in advancing wireless networks, particularly within the domain of aerial-based multi-access edge computing systems. These technologies enable efficient offloading of computational tasks from user electronic devices to Unmanned Aerial Vehicles or local execution. For the generation of high-quality task-offloading allocations, conventional numerical optimization methods often struggle to solve challenging combinatorial optimization problems within the limited channel coherence time, thereby failing to respond quickly to dynamic changes in system conditions. To address this challenge, we propose a deep learning-based optimization framework called Iterative Order-Preserving policy Optimization (IOPO), which enables the generation of energy-efficient task-offloading decisions within milliseconds. Unlike exhaustive search methods, IOPO provides continuous updates to the offloading decisions without resorting to exhaustive search, resulting in accelerated convergence and reduced computational complexity, particularly when dealing with complex problems characterized by extensive solution spaces. Experimental results demonstrate that the proposed framework can generate energy-efficient task-offloading decisions within a very short time period, outperforming other benchmark methods.
</details>
<details>
<summary>摘要</summary>
“tera兆通信网络和智能反射表示技术在提高无线网络方面具有 significannot potential，特别是在基于飞行器多访问边缘计算系统中。这些技术可以有效地减轻用户电子设备中的计算任务到无人飞行机或本地执行。为生成高质量的任务卸载分配，常见的数字优化方法经常陷入在限制性减震时间内的复杂 combinatorial 优化问题中，从而无法快速应对系统条件的动态变化。为解决这个挑战，我们提出了一种基于深度学习的优化框架，即迭代保持顺序分配优化（IOPO）。与极限搜索方法不同，IOPO可以在毫秒级时间内生成能效的任务卸载决策，而无需进行极限搜索，从而降低计算复杂性，特别是在面临复杂问题时。实验结果表明，提议的框架可以在很短的时间内生成能效的任务卸载决策，超过了其他 Referenced 方法。”
</details></li>
</ul>
<hr>
<h2 id="Hidden-Markov-Models-with-Random-Restarts-vs-Boosting-for-Malware-Detection"><a href="#Hidden-Markov-Models-with-Random-Restarts-vs-Boosting-for-Malware-Detection" class="headerlink" title="Hidden Markov Models with Random Restarts vs Boosting for Malware Detection"></a>Hidden Markov Models with Random Restarts vs Boosting for Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10256">http://arxiv.org/abs/2307.10256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Raghavan, Fabio Di Troia, Mark Stamp</li>
<li>for: 这个研究是为了提高适当的黑客病毒检测方法。</li>
<li>methods: 这个研究使用了隐藏Markovchain模型（HMM）和AdaBoost数据集训练方法。</li>
<li>results: 研究发现，Random Restarts方法在训练数据匮乏时表现出来 surprisingly well，而boosting则仅在最困难的“冰结”案例（训练数据匮乏）中能够提供足够的改善，以正fi运算阶段的computational cost。<details>
<summary>Abstract</summary>
Effective and efficient malware detection is at the forefront of research into building secure digital systems. As with many other fields, malware detection research has seen a dramatic increase in the application of machine learning algorithms. One machine learning technique that has been used widely in the field of pattern matching in general-and malware detection in particular-is hidden Markov models (HMMs). HMM training is based on a hill climb, and hence we can often improve a model by training multiple times with different initial values. In this research, we compare boosted HMMs (using AdaBoost) to HMMs trained with multiple random restarts, in the context of malware detection. These techniques are applied to a variety of challenging malware datasets. We find that random restarts perform surprisingly well in comparison to boosting. Only in the most difficult "cold start" cases (where training data is severely limited) does boosting appear to offer sufficient improvement to justify its higher computational cost in the scoring phase.
</details>
<details>
<summary>摘要</summary>
“有效和高效的恶意软件检测是当前安全数字系统研究的核心。与其他领域一样，恶意软件检测研究在使用机器学习算法方面表现了快速增长。一种广泛应用于模式匹配领域（包括恶意软件检测）的机器学习技术是隐藏Markov模型（HMM）。HMM训练基于山峰搜索，因此可以通过不同的初始值进行多次训练以改进模型。在这项研究中，我们比较了使用AdaBoost加强HMM和多个随机重启来进行恶意软件检测。这些技术在许多复杂的恶意软件数据集中进行应用。我们发现，随机重启 surprisingly well 在对抗“冰结”（训练数据 severely limited）情况下表现出色，而boosting 只在这些情况下能够提供足够的改进，以 justify its higher computational cost in the scoring phase。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Classification-of-UHF-Partial-Discharge-Signals-in-Gas-Insulated-HVDC-Systems-Using-Neural-Networks"><a href="#Generalizable-Classification-of-UHF-Partial-Discharge-Signals-in-Gas-Insulated-HVDC-Systems-Using-Neural-Networks" class="headerlink" title="Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks"></a>Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08466">http://arxiv.org/abs/2307.08466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steffen Seitz, Thomas Götz, Christopher Lindenberg, Ronald Tetzlaff, Stephan Schlegel</li>
<li>for: 本研究旨在为高压直流绝缘系统（HVDC GIS）中检测未探测到的部分磁振（PD）提供一种基于神经网络的分类方法，无需基于振荡序列分析特征。</li>
<li>methods: 本研究使用神经网络分类方法，并对时域和频域输入信号进行比较，以及不同 нормализа schemes的影响。</li>
<li>results: 研究发现，使用神经网络分类方法可以区分由金属凸起和导电粒子引起的PD信号，并且可以在不同的运行电压多пли中进行泛化。<details>
<summary>Abstract</summary>
Undetected partial discharges (PDs) are a safety critical issue in high voltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under AC voltage is well-established, the analysis of PDs under DC voltage remains an active research field. A key focus of these investigations is the classification of different PD sources to enable subsequent sophisticated analysis.   In this paper, we propose and analyze a neural network-based approach for classifying PD signals caused by metallic protrusions and conductive particles on the insulator of HVDC GIS, without relying on pulse sequence analysis features. In contrast to previous approaches, our proposed model can discriminate the studied PD signals obtained at negative and positive potentials, while also generalizing to unseen operating voltage multiples. Additionally, we compare the performance of time- and frequency-domain input signals and explore the impact of different normalization schemes to mitigate the influence of free-space path loss between the sensor and defect location.
</details>
<details>
<summary>摘要</summary>
未探测的偏置负荷（PD）是高压直流瓦尔瑙系统（GIS）中的安全关键问题。 Although the diagnosis of PDs under AC voltage is well established, the analysis of PDs under DC voltage remains an active research field. A key focus of these investigations is the classification of different PD sources to enable subsequent sophisticated analysis.在本文中，我们提出了一种基于神经网络的方法，用于分类HVDC GIS中的卷积物和导电粒子引起的PD信号，不需要基于激射序列分析特征。 与前一些方法不同，我们的提议的模型可以在负和正潜能下分辨出 studied PD signals，同时还能泛化到未经见过的操作电压倍数。 此外，我们还比较了时域和频域输入信号的性能，并探讨了不同的normalization schemes来减少卷积物和导电粒子之间的自由空间通路损失的影响。
</details></li>
</ul>
<hr>
<h2 id="A-benchmark-of-categorical-encoders-for-binary-classification"><a href="#A-benchmark-of-categorical-encoders-for-binary-classification" class="headerlink" title="A benchmark of categorical encoders for binary classification"></a>A benchmark of categorical encoders for binary classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09191">http://arxiv.org/abs/2307.09191</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/drcohomology/encoderbenchmarking">https://github.com/drcohomology/encoderbenchmarking</a></li>
<li>paper_authors: Federico Matteucci, Vadim Arzamasov, Klemens Boehm</li>
<li>for: 本研究是机器学习领域中 categorical 编码器的最全面的比较研究，涵盖了多种编码器家族的32种配置，以及36种实验因素的组合，在50个数据集上进行了广泛的评估。</li>
<li>methods: 本研究使用了多种编码器家族的32种配置，以及36种实验因素的组合，在50个数据集上进行了广泛的评估。</li>
<li>results: 研究发现，选择数据集、实验因素和综合策略会对比较结论产生深远的影响，这些因素在先前的编码器比较中未得到考虑。<details>
<summary>Abstract</summary>
Categorical encoders transform categorical features into numerical representations that are indispensable for a wide range of machine learning models. Existing encoder benchmark studies lack generalizability because of their limited choice of (1) encoders, (2) experimental factors, and (3) datasets. Additionally, inconsistencies arise from the adoption of varying aggregation strategies. This paper is the most comprehensive benchmark of categorical encoders to date, including an extensive evaluation of 32 configurations of encoders from diverse families, with 36 combinations of experimental factors, and on 50 datasets. The study shows the profound influence of dataset selection, experimental factors, and aggregation strategies on the benchmark's conclusions -- aspects disregarded in previous encoder benchmarks.
</details>
<details>
<summary>摘要</summary>
categorical 编码器将 categorical 特征转换为数字表示形式，这些表示形式是机器学习模型的不可或缺的一部分。现有的编码器比较研究受到限制因为它们选择的（1）编码器、（2）实验因素和（3）数据集的选择有限。此外，由于不同的汇集策略的采用，导致了不一致性。这篇论文是目前最全面的 categorical 编码器比较研究，包括了32种编码器家族中的广泛评估，以及36种实验因素的组合，和50个数据集的评估。研究显示数据集选择、实验因素和汇集策略对比较的结论产生了深远的影响，这些因素在前一次编码器比较中被忽略了。
</details></li>
</ul>
<hr>
<h2 id="SBMLtoODEjax-efficient-simulation-and-optimization-of-ODE-SBML-models-in-JAX"><a href="#SBMLtoODEjax-efficient-simulation-and-optimization-of-ODE-SBML-models-in-JAX" class="headerlink" title="SBMLtoODEjax: efficient simulation and optimization of ODE SBML models in JAX"></a>SBMLtoODEjax: efficient simulation and optimization of ODE SBML models in JAX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08452">http://arxiv.org/abs/2307.08452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flowersteam/sbmltoodejax">https://github.com/flowersteam/sbmltoodejax</a></li>
<li>paper_authors: Mayalen Etcheverry, Michael Levin, Clément Moulin-Frier, Pierre-Yves Oudeyer</li>
<li>for: 这篇论文是为了提供一个可以自动将系统生物学标记语言（SBML）模型转换成Python代码的轻量级库。</li>
<li>methods: 该库使用JAX高性能数学计算库自动推导 capabilities来实现高效的数字化模拟和优化。</li>
<li>results: 该库可以帮助研究人员快速将SBML模型integrated into ихPython项目和机器学习管道，只需几行代码即可实现高性能的数字化模拟和优化。<details>
<summary>Abstract</summary>
Developing methods to explore, predict and control the dynamic behavior of biological systems, from protein pathways to complex cellular processes, is an essential frontier of research for bioengineering and biomedicine. Thus, significant effort has gone in computational inference and mathematical modeling of biological systems. This effort has resulted in the development of large collections of publicly-available models, typically stored and exchanged on online platforms (such as the BioModels Database) using the Systems Biology Markup Language (SBML), a standard format for representing mathematical models of biological systems. SBMLtoODEjax is a lightweight library that allows to automatically parse and convert SBML models into python models written end-to-end in JAX, a high-performance numerical computing library with automatic differentiation capabilities. SBMLtoODEjax is targeted at researchers that aim to incorporate SBML-specified ordinary differential equation (ODE) models into their python projects and machine learning pipelines, in order to perform efficient numerical simulation and optimization with only a few lines of code. SBMLtoODEjax is available at https://github.com/flowersteam/sbmltoodejax.
</details>
<details>
<summary>摘要</summary>
开发方法来探索、预测和控制生物系统的动态行为，从蛋白道路到复杂的细胞过程，是生物工程和生物医学研究的关键前沿。因此，在计算推理和数学模型化方面的努力很大，以致已经形成了大量的公共可用模型，通常通过在线平台（如生物模型数据库）存储和交换，使用系统生物学标记语言（SBML），这是表示生物系统数学模型的标准格式。SBMLtoODEjax 是一个轻量级库，它可以自动解析和将 SBML 模型转换为 Python 模型，并将其写入终端到终端在 JAX 中，JAX 是一个高性能的数值计算库，具有自动导数能力。SBMLtoODEjax 是为研究者们提供，他们想将 SBML 规定的常微分方程（ODE）模型 integrate 到他们的 Python 项目和机器学习管道中，以实现高效的数值优化和优化，只需几行代码。SBMLtoODEjax 可以在 GitHub 上找到：https://github.com/flowersteam/sbmltoodejax。
</details></li>
</ul>
<hr>
<h2 id="From-random-walks-to-graph-sprints-a-low-latency-node-embedding-framework-on-continuous-time-dynamic-graphs"><a href="#From-random-walks-to-graph-sprints-a-low-latency-node-embedding-framework-on-continuous-time-dynamic-graphs" class="headerlink" title="From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs"></a>From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08433">http://arxiv.org/abs/2307.08433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Naser Eddin, Jacopo Bono, David Aparício, Hugo Ferreira, João Ascensão, Pedro Ribeiro, Pedro Bizarro</li>
<li>for: 这篇研究是为了提出一个能够实现低延迟、高效的动态图像学习框架，并且能够处理真实世界的动态图像资料。</li>
<li>methods: 这篇研究使用了流动测量的方法，即时间感知的点 cloud 来捕捉多阶资讯，并且使用了单一阶资讯来计算时间感知的点 cloud。</li>
<li>results: 研究结果显示，使用 graph-sprints 的方法可以实现与现有的高延迟模型相同或更好的性能，并且可以实现低延迟的推断运算。<details>
<summary>Abstract</summary>
Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop operations on the incoming edges. We evaluate our proposed approach on three open-source datasets and two in-house datasets, and compare with three state-of-the-art algorithms (TGN-attn, TGN-ID, Jodie). We demonstrate that our graph-sprints features, combined with a machine learning classifier, achieve competitive performance (outperforming all baselines for the node classification tasks in five datasets). Simultaneously, graph-sprints significantly reduce inference latencies, achieving close to an order of magnitude speed-up in our experimental setting.
</details>
<details>
<summary>摘要</summary>
muchos datos del mundo real tienen una estructura de grafo subyacente dinámica, donde las entidades y sus interacciones evolucionan con el tiempo. Los modelos de aprendizaje automático deben considerar estas dinámicas para aprovechar su potencial completo en tareas downstream. Los enfoques anteriores para aprender representaciones de grafos han centrado en la muestra de neighbborhoods k-hop, similar a búsqueda en profundidad, o caminatas aleatorias, similar a búsqueda en anchura. Sin embargo, estos métodos son costosos en términos de computación y no son adecuados para inferencia en tiempo real y baja latencia en grafos dinámicos. Para superar estos límites, propusimos graph-sprints, un marco general de extracción de características para grafos continuos en tiempo dinámico (CTDGs) que tiene baja latencia y es competitivo con modelos de estado del arte de mayor latencia. Para lograr esto, se propone una aproximación de bajo latencia y streaming a las características basadas en caminatas aleatorias. En nuestro marco, las embeddings de nodos conscientes del tiempo resumen la información de múltiples hop utilizando solo operaciones de un hop en las aristas entrantes. Evaluamos nuestro enfoque propuesto en tres conjuntos de datos abiertos y dos conjuntos de datos internos, y lo comparamos con tres algoritmos estado del arte (TGN-attn, TGN-ID, Jodie). Demostramos que nuestras características de graph-sprints, combinadas con una clase de aprendizaje automático, logran rendimientos competitivos (superando a todos los baselines para las tareas de clasificación de nodos en cinco conjuntos de datos). Al mismo tiempo, graph-sprints reduce significativamente las latencias de inferencia, logrando una reducción de cerca de un orden de magnitud en nuestra configuración experimental.
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-for-Science-in-Quantum-Atomistic-and-Continuum-Systems"><a href="#Artificial-Intelligence-for-Science-in-Quantum-Atomistic-and-Continuum-Systems" class="headerlink" title="Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems"></a>Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08423">http://arxiv.org/abs/2307.08423</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/divelab/AIRS">https://github.com/divelab/AIRS</a></li>
<li>paper_authors: Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Haiyang Yu, YuQing Xie, Xiang Fu, Alex Strasser, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes Stärk, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar Azizzadenesheli, Ada Fang, Alán Aspuru-Guzik, Erik Bekkers, Michael Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Liò, Rose Yu, Stephan Günnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess Smidt, Shuiwang Ji</li>
<li>for: 这篇论文主要针对的是利用人工智能（AI）进行自然科学研究（AI4Science）的新 paradigm。</li>
<li>methods: 论文使用的方法包括深度学习方法，以捕捉自然系统中的物理原理，特别是对称变换的equivariance。</li>
<li>results: 论文提供了一种Foundational and unified treatment of AI for quantum, atomistic, and continuum systems，并提出了一些解释性、过度分布采样和不确定性评估等技术挑战。<details>
<summary>Abstract</summary>
Advances in artificial intelligence (AI) are fueling a new paradigm of discoveries in natural sciences. Today, AI has started to advance natural sciences by improving, accelerating, and enabling our understanding of natural phenomena at a wide range of spatial and temporal scales, giving rise to a new area of research known as AI for science (AI4Science). Being an emerging research paradigm, AI4Science is unique in that it is an enormous and highly interdisciplinary area. Thus, a unified and technical treatment of this field is needed yet challenging. This paper aims to provide a technically thorough account of a subarea of AI4Science; namely, AI for quantum, atomistic, and continuum systems. These areas aim at understanding the physical world from the subatomic (wavefunctions and electron density), atomic (molecules, proteins, materials, and interactions), to macro (fluids, climate, and subsurface) scales and form an important subarea of AI4Science. A unique advantage of focusing on these areas is that they largely share a common set of challenges, thereby allowing a unified and foundational treatment. A key common challenge is how to capture physics first principles, especially symmetries, in natural systems by deep learning methods. We provide an in-depth yet intuitive account of techniques to achieve equivariance to symmetry transformations. We also discuss other common technical challenges, including explainability, out-of-distribution generalization, knowledge transfer with foundation and large language models, and uncertainty quantification. To facilitate learning and education, we provide categorized lists of resources that we found to be useful. We strive to be thorough and unified and hope this initial effort may trigger more community interests and efforts to further advance AI4Science.
</details>
<details>
<summary>摘要</summary>
人工智能技术的发展（AI）正在推动一种新的发现 paradigm in 自然科学。今天，AI已经开始为自然科学的研究提供了改进、加速和实现自然现象的理解，从而创造了一个新的研究领域——AI for science（AI4Science）。作为一个emerging research paradigm，AI4Science具有巨大的多学科性和挑战性，因此需要一种统一和技术性的处理。本文的目的是提供AI4Science中的一个子领域的技术深入报告，即用AI研究量子、原子istic和连续体系。这些领域旨在理解自然世界从子原子（振荡函数和电子密度）、原子（分子、蛋白质、材料和交互）到宏观（液体、气候和地壳）级别的物理世界，并形成AI4Science中一个重要的子领域。这些领域之间共享许多挑战，因此可以实现一种统一和基础的处理。一个关键的共同挑战是如何通过深度学习方法捕捉自然系统中的物理基本原理，特别是对称性。我们提供了深入 yet 易于理解的对于实现对称变换的方法的详细讲解。我们还讨论了其他一些常见的技术挑战，包括可解释性、out-of-distribution扩展、基础和大语言模型知识传递、和不确定性评估。为便于学习和教育，我们提供了分类列表，我们认为是有用的资源。我们努力保持统一和完整，希望这个初步努力可以触发更多的社区兴趣和努力，以进一步推动AI4Science的发展。
</details></li>
</ul>
<hr>
<h2 id="Neurosymbolic-AI-for-Reasoning-on-Biomedical-Knowledge-Graphs"><a href="#Neurosymbolic-AI-for-Reasoning-on-Biomedical-Knowledge-Graphs" class="headerlink" title="Neurosymbolic AI for Reasoning on Biomedical Knowledge Graphs"></a>Neurosymbolic AI for Reasoning on Biomedical Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08411">http://arxiv.org/abs/2307.08411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauren Nicole DeLong, Ramon Fernández Mir, Zonglin Ji, Fiona Niamh Coulter Smith, Jacques D. Fleuriot</li>
<li>for: 这篇论文旨在介绍基于神经符号智能的 hybrid 方法，以及其在生物医学领域中的应用和优势。</li>
<li>methods: 这篇论文主要介绍了基于 embedding 和符号 logic 的 hybrid 方法，以及它们在生物医学领域中的应用。</li>
<li>results: 论文总结了 hybrid 方法的优势和可能性，并指出了它们在生物医学领域中的应用可能性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Biomedical datasets are often modeled as knowledge graphs (KGs) because they capture the multi-relational, heterogeneous, and dynamic natures of biomedical systems. KG completion (KGC), can, therefore, help researchers make predictions to inform tasks like drug repositioning. While previous approaches for KGC were either rule-based or embedding-based, hybrid approaches based on neurosymbolic artificial intelligence are becoming more popular. Many of these methods possess unique characteristics which make them even better suited toward biomedical challenges. Here, we survey such approaches with an emphasis on their utilities and prospective benefits for biomedicine.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Vocoder-drift-compensation-by-x-vector-alignment-in-speaker-anonymisation"><a href="#Vocoder-drift-compensation-by-x-vector-alignment-in-speaker-anonymisation" class="headerlink" title="Vocoder drift compensation by x-vector alignment in speaker anonymisation"></a>Vocoder drift compensation by x-vector alignment in speaker anonymisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08403">http://arxiv.org/abs/2307.08403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michele Panariello, Massimiliano Todisco, Nicholas Evans</li>
<li>for: 本研究旨在探讨xvector基于的听者隐私保护方法中， vocoding而不是核心隐私函数对听者隐私的保护产生了主要的影响。</li>
<li>methods: 本研究使用了一种新的方法来衡量 vocoder drift，并提出了一种新的隐私函数来减少 vocoder drift。</li>
<li>results: 研究发现，使用新的隐私函数可以有效地减少 vocoder drift，并提供了更好的控制 sobre xvector 空间。<details>
<summary>Abstract</summary>
For the most popular x-vector-based approaches to speaker anonymisation, the bulk of the anonymisation can stem from vocoding rather than from the core anonymisation function which is used to substitute an original speaker x-vector with that of a fictitious pseudo-speaker. This phenomenon can impede the design of better anonymisation systems since there is a lack of fine-grained control over the x-vector space. The work reported in this paper explores the origin of so-called vocoder drift and shows that it is due to the mismatch between the substituted x-vector and the original representations of the linguistic content, intonation and prosody. Also reported is an original approach to vocoder drift compensation. While anonymisation performance degrades as expected, compensation reduces vocoder drift substantially, offers improved control over the x-vector space and lays a foundation for the design of better anonymisation functions in the future.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "x-vector" is translated as "语音特征向量" (yùn zhòng yǐn xiàng wù)* "vocoding" is translated as "语音编码" (yùn zhòng biān mǎ)* "anonymization" is translated as "匿名化" (mìng mìng huà)* "core anonymization function" is translated as "核心匿名函数" (zhū xīn mìng mìng fù xiàng)* "fictitious pseudo-speaker" is translated as "虚拟的假发音者" (xū yì de jiǎ fā yīn zhě)* "linguistic content, intonation, and prosody" are translated as "语言内容、听调和语调" (yǔ yán nèi xìng, tīng diào hé yǔ diào)* "vocoder drift compensation" is translated as "语音编码落差补做" (yùn zhòng biān mǎ lù chē bǔ zuò)
</details></li>
</ul>
<hr>
<h2 id="On-the-application-of-Large-Language-Models-for-language-teaching-and-assessment-technology"><a href="#On-the-application-of-Large-Language-Models-for-language-teaching-and-assessment-technology" class="headerlink" title="On the application of Large Language Models for language teaching and assessment technology"></a>On the application of Large Language Models for language teaching and assessment technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08393">http://arxiv.org/abs/2307.08393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Caines, Luca Benedetto, Shiva Taslimipoor, Christopher Davis, Yuan Gao, Oeistein Andersen, Zheng Yuan, Mark Elliott, Russell Moore, Christopher Bryant, Marek Rei, Helen Yannakoudakis, Andrew Mullooly, Diane Nicholls, Paula Buttery</li>
<li>for: 这篇论文探讨了用大型自然语言处理模型（PaLM和GPT-4）在语言教学和评估系统中的潜在应用。</li>
<li>methods: 论文考虑了几个研究领域，并讨论了在教育技术中使用生成AI的风险和伦理问题。</li>
<li>results: 研究发现大型语言模型在文本生成方面有所改进，但是在自动评分和语法错误检查方面，大型语言模型独立使用不能超越现有的状态艺术metric。<details>
<summary>Abstract</summary>
The recent release of very large language models such as PaLM and GPT-4 has made an unprecedented impact in the popular media and public consciousness, giving rise to a mixture of excitement and fear as to their capabilities and potential uses, and shining a light on natural language processing research which had not previously received so much attention. The developments offer great promise for education technology, and in this paper we look specifically at the potential for incorporating large language models in AI-driven language teaching and assessment systems. We consider several research areas and also discuss the risks and ethical considerations surrounding generative AI in education technology for language learners. Overall we find that larger language models offer improvements over previous models in text generation, opening up routes toward content generation which had not previously been plausible. For text generation they must be prompted carefully and their outputs may need to be reshaped before they are ready for use. For automated grading and grammatical error correction, tasks whose progress is checked on well-known benchmarks, early investigations indicate that large language models on their own do not improve on state-of-the-art results according to standard evaluation metrics. For grading it appears that linguistic features established in the literature should still be used for best performance, and for error correction it may be that the models can offer alternative feedback styles which are not measured sensitively with existing methods. In all cases, there is work to be done to experiment with the inclusion of large language models in education technology for language learners, in order to properly understand and report on their capacities and limitations, and to ensure that foreseeable risks such as misinformation and harmful bias are mitigated.
</details>
<details>
<summary>摘要</summary>
最近发布的非常大的自然语言处理模型，如PaLM和GPT-4，在流行媒体和公众意识中产生了无前例的影响，引发了诸多人的兴奋和担忧，对其能力和应用领域的潜在影响。这些发展在教育技术方面具有巨大潜力，在这篇论文中，我们专门关注在AI驱动的语言教学和评估系统中可能的应用。我们考虑了多个研究领域，并讨论了生成AI在教育技术中的风险和伦理考虑。总之，更大的语言模型在文本生成方面提供了改进，打开了新的内容生成途径，但是需要谨慎地提供提示，并且输出可能需要重新处理。在自动评分和 grammatical error correction 方面，初步调查表明，大语言模型独立使用不会超越现有的标准评价指标。在评分方面，使用现有的语言特征仍然是最佳选择，而在 error correction 方面，模型可能提供不同的反馈样式，不同于现有方法的敏感度评价。总之，需要进行实验来探索将大语言模型包含在教育技术中的可能性和局限性，以确保预期的风险，如误导和不良偏见，得到控制。
</details></li>
</ul>
<hr>
<h2 id="Correlation-aware-Spatial-Temporal-Graph-Learning-for-Multivariate-Time-series-Anomaly-Detection"><a href="#Correlation-aware-Spatial-Temporal-Graph-Learning-for-Multivariate-Time-series-Anomaly-Detection" class="headerlink" title="Correlation-aware Spatial-Temporal Graph Learning for Multivariate Time-series Anomaly Detection"></a>Correlation-aware Spatial-Temporal Graph Learning for Multivariate Time-series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08390">http://arxiv.org/abs/2307.08390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/astha-chem/mvts-ano-eval">https://github.com/astha-chem/mvts-ano-eval</a></li>
<li>paper_authors: Yu Zheng, Huan Yee Koh, Ming Jin, Lianhua Chi, Khoa T. Phan, Shirui Pan, Yi-Ping Phoebe Chen, Wei Xiang</li>
<li>for: 本研究旨在提出一种新的多变量时间序列异常检测方法，以解决现有方法中的非线性关系捕捉和时间序列异常检测问题。</li>
<li>methods: 本方法基于多变量时间序列相关学习模块，并采用空间时间图学习网络（STGNN）来编码复杂的变量间相关性。另外，通过借鉴一元和多元邻居信息，我们的STGNN组件可以吸收复杂的空间信息。同时，我们还提出了一种新的异常分数组件，可以在无监督情况下估计异常程度。</li>
<li>results: 实验结果表明，CST-GL方法可以在一般情况下有效地检测异常，并且可以在不同的时间延迟下进行早期检测。<details>
<summary>Abstract</summary>
Multivariate time-series anomaly detection is critically important in many applications, including retail, transportation, power grid, and water treatment plants. Existing approaches for this problem mostly employ either statistical models which cannot capture the non-linear relations well or conventional deep learning models (e.g., CNN and LSTM) that do not explicitly learn the pairwise correlations among variables. To overcome these limitations, we propose a novel method, correlation-aware spatial-temporal graph learning (termed CST-GL), for time series anomaly detection. CST-GL explicitly captures the pairwise correlations via a multivariate time series correlation learning module based on which a spatial-temporal graph neural network (STGNN) can be developed. Then, by employing a graph convolution network that exploits one- and multi-hop neighbor information, our STGNN component can encode rich spatial information from complex pairwise dependencies between variables. With a temporal module that consists of dilated convolutional functions, the STGNN can further capture long-range dependence over time. A novel anomaly scoring component is further integrated into CST-GL to estimate the degree of an anomaly in a purely unsupervised manner. Experimental results demonstrate that CST-GL can detect anomalies effectively in general settings as well as enable early detection across different time delays.
</details>
<details>
<summary>摘要</summary>
多变量时间序列异常检测在许多应用程序中非常重要，包括零售、交通、电力网络和水处理厂。现有的方法通常使用统计模型，这些模型不能很好地捕捉非线性关系，或者使用传统的深度学习模型（如CNN和LSTM），这些模型不直接学习时间序列变量之间的对比关系。为了解决这些限制，我们提出了一种新的方法，即相关意识空间时间图学习（CST-GL），用于时间序列异常检测。CST-GL使用多变量时间序列相关学习模块，该模块可以识别时间序列变量之间的对比关系。然后，通过基于这些相关关系的空间时间图 neural network（STGNN）的开发，我们可以融合复杂的对比关系信息，以获得rich的空间信息。此外，我们还使用一个包含扩展延迟 convolutional functions的时间模块，以捕捉长距离时间关系。最后，我们还添加了一个异常分数组件，以无监督的方式估算异常的程度。实验结果表明，CST-GL可以有效地检测异常情况，并且可以在不同的时间延迟下进行早期检测。
</details></li>
</ul>
<hr>
<h2 id="Tabular-Machine-Learning-Methods-for-Predicting-Gas-Turbine-Emissions"><a href="#Tabular-Machine-Learning-Methods-for-Predicting-Gas-Turbine-Emissions" class="headerlink" title="Tabular Machine Learning Methods for Predicting Gas Turbine Emissions"></a>Tabular Machine Learning Methods for Predicting Gas Turbine Emissions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08386">http://arxiv.org/abs/2307.08386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca Potts, Rick Hackney, Georgios Leontidis</li>
<li>for: 这个研究旨在评估机器学习模型在预测液压机发动机排放气体中的性能。</li>
<li>methods: 我们比较了一个现有的预测排放模型（化学动力学模型）和我们基于SAINT和XGBoost的两种机器学习模型，以示机器学习技术可以提供更好的预测性能。</li>
<li>results: 我们发现，使用机器学习技术可以提高氮氧化物（NOx）和碳 моно氧化物（CO）的预测性能。<details>
<summary>Abstract</summary>
Predicting emissions for gas turbines is critical for monitoring harmful pollutants being released into the atmosphere. In this study, we evaluate the performance of machine learning models for predicting emissions for gas turbines. We compare an existing predictive emissions model, a first principles-based Chemical Kinetics model, against two machine learning models we developed based on SAINT and XGBoost, to demonstrate improved predictive performance of nitrogen oxides (NOx) and carbon monoxide (CO) using machine learning techniques. Our analysis utilises a Siemens Energy gas turbine test bed tabular dataset to train and validate the machine learning models. Additionally, we explore the trade-off between incorporating more features to enhance the model complexity, and the resulting presence of increased missing values in the dataset.
</details>
<details>
<summary>摘要</summary>
预测液压机排放是监测污染物排入大气中的关键。本研究对机器学习模型的表现进行评估，以证明使用机器学习技术可以改善液压机排放氮氧化物和碳 моно氧化物的预测性能。我们比较了现有的预测排放模型、基于化学动力学的首要原理模型，与我们基于SAINT和XGBoost开发的两种机器学习模型，以示出机器学习技术的改善性。我们的分析使用了Siemens Energy液压机测试床数据集来训练和验证机器学习模型。此外，我们还探讨了增加特征以增强模型复杂性所带来的数据缺失问题。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Battery-Lifetime-Under-Varying-Usage-Conditions-from-Early-Aging-Data"><a href="#Predicting-Battery-Lifetime-Under-Varying-Usage-Conditions-from-Early-Aging-Data" class="headerlink" title="Predicting Battery Lifetime Under Varying Usage Conditions from Early Aging Data"></a>Predicting Battery Lifetime Under Varying Usage Conditions from Early Aging Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08382">http://arxiv.org/abs/2307.08382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingkai Li, Zihao Zhou, Adam Thelen, David Howey, Chao Hu</li>
<li>For: 预测 Lithium-ion 电池寿命，以便预防维护、赔偿和改进电池设计和生产。* Methods: 利用 early-life 数据（例如容量-电压数据） derivate 新的特征，以预测 cells 在不同充放速度、放电速度和充放深度下的寿命。* Results: 使用新生成的数据集（来自 225 个 nickel-manganese-cobalt&#x2F;graphite Li-ion 电池），实现了准确预测 in-distribution cells 的寿命（15.1% 的 mean absolute percentage error），并且使用 hierarchical Bayesian regression model 可以更好地预测 extrapolation 情况（21.8% 的 mean absolute percentage error）。<details>
<summary>Abstract</summary>
Accurate battery lifetime prediction is important for preventative maintenance, warranties, and improved cell design and manufacturing. However, manufacturing variability and usage-dependent degradation make life prediction challenging. Here, we investigate new features derived from capacity-voltage data in early life to predict the lifetime of cells cycled under widely varying charge rates, discharge rates, and depths of discharge. Features were extracted from regularly scheduled reference performance tests (i.e., low rate full cycles) during cycling. The early-life features capture a cell's state of health and the rate of change of component-level degradation modes, some of which correlate strongly with cell lifetime. Using a newly generated dataset from 225 nickel-manganese-cobalt/graphite Li-ion cells aged under a wide range of conditions, we demonstrate a lifetime prediction of in-distribution cells with 15.1% mean absolute percentage error using no more than the first 15% of data, for most cells. Further testing using a hierarchical Bayesian regression model shows improved performance on extrapolation, achieving 21.8% mean absolute percentage error for out-of-distribution cells. Our approach highlights the importance of using domain knowledge of lithium-ion battery degradation modes to inform feature engineering. Further, we provide the community with a new publicly available battery aging dataset with cells cycled beyond 80% of their rated capacity.
</details>
<details>
<summary>摘要</summary>
importante battery lifetime prediction è importante per la manutenzione preventiva, le garanzie e l'improvviso design e produzione di celle. However, la variabilità di produzione e la degradazione dipendenti dall'utilizzo rendono la predizione della vita difficile. Ecco, investigiamo nuove feature derivate dai dati di capacità e tensione in primis dell' vita per predir la durata delle celle ciclate sotto caricate e discariche widely varying. Le feature sono estratte dai test di riferimento regolarmente programmati (ad esempio, cicli full low rate) durante il ciclo. Le feature early-life capture lo stato di salute della cella e il tasso di cambiamento dei modi di degradazione dei componenti, alcuni dei quali correlano strettamente con la durata della cella. Utilizzando un nuovo dataset generato da 225 celle nickel-manganese-cobalt/graphite Li-ion aged under a wide range of conditions, dimostriamo una predizione di vita di cellule in-distribution con un errore assoluto del 15,1% using no more than the first 15% of data, per la maggior parte delle celle. Further testing using a hierarchical Bayesian regression model shows improved performance on extrapolation, achieving 21,8% errore assoluto percentage per le cellule out-of-distribution. Our approach highlights the importance of using domain knowledge of lithium-ion battery degradation modes to inform feature engineering. Further, we provide the community with a new publicly available battery aging dataset with cells cycled beyond 80% of their rated capacity.
</details></li>
</ul>
<hr>
<h2 id="Q-D-O-ES-Population-based-Quality-Diversity-Optimisation-for-Post-Hoc-Ensemble-Selection-in-AutoML"><a href="#Q-D-O-ES-Population-based-Quality-Diversity-Optimisation-for-Post-Hoc-Ensemble-Selection-in-AutoML" class="headerlink" title="Q(D)O-ES: Population-based Quality (Diversity) Optimisation for Post Hoc Ensemble Selection in AutoML"></a>Q(D)O-ES: Population-based Quality (Diversity) Optimisation for Post Hoc Ensemble Selection in AutoML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08364">http://arxiv.org/abs/2307.08364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LennartPurucker/PopulationBasedQDO-PostHocEnsembleSelectionAutoML">https://github.com/LennartPurucker/PopulationBasedQDO-PostHocEnsembleSelectionAutoML</a></li>
<li>paper_authors: Lennart Purucker, Lennart Schneider, Marie Anastacio, Joeran Beel, Bernd Bischl, Holger Hoos</li>
<li>for: 提高预测性能（Post hoc ensemble learning）</li>
<li>methods: 引入两种新的人口基于 ensemble selection方法（QO-ES和QDO-ES），比较GES</li>
<li>results: 在71个分类数据集上测试，QO-ES和QDO-ES常常超过GES，但只有在验证数据上 statistically significant，并且发现多样性可以优化后期ensemble，但也增加预测风险。<details>
<summary>Abstract</summary>
Automated machine learning (AutoML) systems commonly ensemble models post hoc to improve predictive performance, typically via greedy ensemble selection (GES). However, we believe that GES may not always be optimal, as it performs a simple deterministic greedy search. In this work, we introduce two novel population-based ensemble selection methods, QO-ES and QDO-ES, and compare them to GES. While QO-ES optimises solely for predictive performance, QDO-ES also considers the diversity of ensembles within the population, maintaining a diverse set of well-performing ensembles during optimisation based on ideas of quality diversity optimisation. The methods are evaluated using 71 classification datasets from the AutoML benchmark, demonstrating that QO-ES and QDO-ES often outrank GES, albeit only statistically significant on validation data. Our results further suggest that diversity can be beneficial for post hoc ensembling but also increases the risk of overfitting.
</details>
<details>
<summary>摘要</summary>
自动机器学习（AutoML）系统通常会 ensemble 模型后增进预测性能，通常透过单调式排序（GES）。但我们认为 GES 可能不是最佳，因为它只是一个简单决策的排序。在这个工作中，我们引入了两种新的人口基于的ensemble选择方法，QO-ES 和 QDO-ES，并与 GES 进行比较。而 QO-ES 则优化仅对预测性能，而 QDO-ES 则考虑ensemble population 中的多样性，保持一个多样的集合 ensemble 的多样性在依据质量多样化优化。这些方法在 AutoML benchmark 中的 71 个分类 datasets 上进行评估，结果显示 QO-ES 和 QDO-ES 通常比 GES 高，但仅在验证数据上 statistically significant。我们的结果还表明了多样性可以帮助后续的ensemble，但也增加了过滤的风险。
</details></li>
</ul>
<hr>
<h2 id="Universal-Online-Learning-with-Gradual-Variations-A-Multi-layer-Online-Ensemble-Approach"><a href="#Universal-Online-Learning-with-Gradual-Variations-A-Multi-layer-Online-Ensemble-Approach" class="headerlink" title="Universal Online Learning with Gradual Variations: A Multi-layer Online Ensemble Approach"></a>Universal Online Learning with Gradual Variations: A Multi-layer Online Ensemble Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08360">http://arxiv.org/abs/2307.08360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Hu Yan, Peng Zhao, Zhi-Hua Zhou</li>
<li>for: 这个论文是为了提出一种在线 convex optimization 方法，该方法可以在不同级别上适应不同类型和凹度的损失函数。</li>
<li>methods: 该方法基于一种多层在线 ensemble，包括新的优势函数和层次误差 correction。</li>
<li>results: 该方法可以获得 $\mathcal{O}(\ln V_T)$, $\mathcal{O}(d \ln V_T)$ 和 $\hat{\mathcal{O}(\sqrt{V_T})$ 的 regret bounds，其中 $d$ 是维度、$V_T$ 是问题依赖于的梯度变化。此外，该方法还有广泛的应用和意义，包括保证最坏情况下的 guarantees，直接从分析中提取小损 bounds，以及与对抗&#x2F;随机 convex optimization 和游戏理论的深刻连接。<details>
<summary>Abstract</summary>
In this paper, we propose an online convex optimization method with two different levels of adaptivity. On a higher level, our method is agnostic to the specific type and curvature of the loss functions, while at a lower level, it can exploit the niceness of the environments and attain problem-dependent guarantees. To be specific, we obtain $\mathcal{O}(\ln V_T)$, $\mathcal{O}(d \ln V_T)$ and $\hat{\mathcal{O}(\sqrt{V_T})$ regret bounds for strongly convex, exp-concave and convex loss functions, respectively, where $d$ is the dimension, $V_T$ denotes problem-dependent gradient variations and $\hat{\mathcal{O}(\cdot)$-notation omits logarithmic factors on $V_T$. Our result finds broad implications and applications. It not only safeguards the worst-case guarantees, but also implies the small-loss bounds in analysis directly. Besides, it draws deep connections with adversarial/stochastic convex optimization and game theory, further validating its practical potential. Our method is based on a multi-layer online ensemble incorporating novel ingredients, including carefully-designed optimism for unifying diverse function types and cascaded corrections for algorithmic stability. Remarkably, despite its multi-layer structure, our algorithm necessitates only one gradient query per round, making it favorable when the gradient evaluation is time-consuming. This is facilitated by a novel regret decomposition equipped with customized surrogate losses.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种在线凸优化方法，具有两个不同的水平的适应性。在更高的水平上，我们的方法是对具体类型和曲率的损失函数不偏袋，而在更低的水平上，它可以利用环境的温柔性并实现问题依赖的保证。具体来说，我们获得了$\mathcal{O}(\ln V_T)$, $\mathcal{O}(d \ln V_T)$和$\hat{\mathcal{O}(\sqrt{V_T})$的 regret bound，其中$d$是维度，$V_T$表示问题依赖的梯度变化，$\hat{\mathcal{O}(\cdot)$-notation忽略了$V_T$的对数因子。我们的结果具有广泛的应用和意义。它不仅保证了最坏情况的保证，而且直接从分析中获得了小损失的 bound。此外，它还与反对抗/随机凸优化和游戏理论之间存在深刻的连接，进一步证明其实用性。我们的方法基于一种多层在线ensemble，包括新的优势因子，例如精心设计的乐观性以及随机逻辑的级联 corrections。备注意的是，即使具有多层结构，我们的算法只需要每个回合一次获取梯度，因此在梯度评估是时间耗费的情况下，它是有利的。这是由一种新的 regret decomposition和自定义损失函数帮助实现的。
</details></li>
</ul>
<hr>
<h2 id="Zero-th-Order-Algorithm-for-Softmax-Attention-Optimization"><a href="#Zero-th-Order-Algorithm-for-Softmax-Attention-Optimization" class="headerlink" title="Zero-th Order Algorithm for Softmax Attention Optimization"></a>Zero-th Order Algorithm for Softmax Attention Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08352">http://arxiv.org/abs/2307.08352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichuan Deng, Zhihang Li, Sridhar Mahadevan, Zhao Song</li>
<li>for: 本研究旨在提高大型自然语言模型（LLM）的优化技术，特别是在计算梯度时的效率。</li>
<li>methods: 本研究提出了一种针对软max单元的零次顺序算法，通过只进行前向传输来approximately计算梯度。</li>
<li>results: 我们的算法能够高效地计算大规模LLM的梯度，并且可以在不同的语言模型中实现效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have brought about significant transformations in human society. Among the crucial computations in LLMs, the softmax unit holds great importance. Its helps the model generating a probability distribution on potential subsequent words or phrases, considering a series of input words. By utilizing this distribution, the model selects the most probable next word or phrase, based on the assigned probabilities. The softmax unit assumes a vital function in LLM training as it facilitates learning from data through the adjustment of neural network weights and biases.   With the development of the size of LLMs, computing the gradient becomes expensive. However, Zero-th Order method can approximately compute the gradient with only forward passes. In this paper, we present a Zero-th Order algorithm specifically tailored for Softmax optimization. We demonstrate the convergence of our algorithm, highlighting its effectiveness in efficiently computing gradients for large-scale LLMs. By leveraging the Zeroth-Order method, our work contributes to the advancement of optimization techniques in the context of complex language models.
</details>
<details>
<summary>摘要</summary>
With the development of the size of LLMs, computing the gradient becomes expensive. However, the Zero-th Order method can approximately compute the gradient with only forward passes. In this paper, we present a Zero-th Order algorithm specifically tailored for Softmax optimization. We demonstrate the convergence of our algorithm, highlighting its effectiveness in efficiently computing gradients for large-scale LLMs. By leveraging the Zeroth-Order method, our work contributes to the advancement of optimization techniques in the context of complex language models.(Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China. The translation may differ slightly from the traditional Chinese writing system used in Hong Kong and Taiwan.)
</details></li>
</ul>
<hr>
<h2 id="M-FLAG-Medical-Vision-Language-Pre-training-with-Frozen-Language-Models-and-Latent-Space-Geometry-Optimization"><a href="#M-FLAG-Medical-Vision-Language-Pre-training-with-Frozen-Language-Models-and-Latent-Space-Geometry-Optimization" class="headerlink" title="M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization"></a>M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08347">http://arxiv.org/abs/2307.08347</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cheliu-computation/m-flag-miccai2023">https://github.com/cheliu-computation/m-flag-miccai2023</a></li>
<li>paper_authors: Che Liu, Sibo Cheng, Chen Chen, Mengyun Qiao, Weitong Zhang, Anand Shah, Wenjia Bai, Rossella Arcucci</li>
<li>for: 这篇研究旨在提出一种新的医疗视力语言模型预训练方法，以提高医疗视力语言模型的训练稳定性和效率。</li>
<li>methods: 提案方法名为M-FLAG，它利用固定的语言模型进行预训练，并引入一个新的正交对映损失来调和视力语言模型的内存空间几何。</li>
<li>results: 实验结果显示，M-FLAG方法可以对医疗视力语言模型进行有效的预训练，并在三个下游任务中表现出色：医疗图像分类、分割和物体检测。尤其是在分割任务中，M-FLAG方法只使用了RSNA数据集的1%，却可以超越已经精心适应的ImageNet预训练模型。<details>
<summary>Abstract</summary>
Medical vision-language models enable co-learning and integrating features from medical imaging and clinical text. However, these models are not easy to train and the latent representation space can be complex. Here we propose a novel way for pre-training and regularising medical vision-language models. The proposed method, named Medical vision-language pre-training with Frozen language models and Latent spAce Geometry optimization (M-FLAG), leverages a frozen language model for training stability and efficiency and introduces a novel orthogonality loss to harmonize the latent space geometry. We demonstrate the potential of the pre-trained model on three downstream tasks: medical image classification, segmentation, and object detection. Extensive experiments across five public datasets demonstrate that M-FLAG significantly outperforms existing medical vision-language pre-training approaches and reduces the number of parameters by 78\%. Notably, M-FLAG achieves outstanding performance on the segmentation task while using only 1\% of the RSNA dataset, even outperforming ImageNet pre-trained models that have been fine-tuned using 100\% of the data.
</details>
<details>
<summary>摘要</summary>
医疗视语模型可以同时学习医疗影像和临床文本特征。然而，这些模型不易于训练，其潜在表示空间可能很复杂。在这里，我们提出了一种新的医疗视语预训练方法，名为医疗视语预训练with Frozen language models和Latent spAce Geometry optimization（M-FLAG）。我们利用一个冻结的语言模型来保持训练稳定和高效，并引入了一种新的正交准则来融和潜在空间准则。我们在三个下游任务中展示了预训练模型的潜力：医疗影像分类、 segmentation 和对象检测。我们在五个公共数据集进行了广泛的实验，并证明了M-FLAG在现有的医疗视语预训练方法中显著超越，并将参数数量减少了78%。特别是，M-FLAG在分割任务上表现出色，只使用了RSNA数据集的1%，甚至超过了ImageNet预训练模型，这些模型在100%的数据上进行了精细调节。
</details></li>
</ul>
<hr>
<h2 id="Efficient-selective-attention-LSTM-for-well-log-curve-synthesis"><a href="#Efficient-selective-attention-LSTM-for-well-log-curve-synthesis" class="headerlink" title="Efficient selective attention LSTM for well log curve synthesis"></a>Efficient selective attention LSTM for well log curve synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10253">http://arxiv.org/abs/2307.10253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuankai Zhou, Huanyu Li, Hu liu</li>
<li>for: 这 paper 是为了提出一种机器学习方法，用于预测缺失的井 logging 曲线。</li>
<li>methods: 该方法基于传统的 Long Short-Term Memory (LSTM) 神经网络，并添加了一个自注意机制来分析数据的空间相关性。</li>
<li>results: 实验结果表明，该方法可以高效地预测缺失的井 logging 曲线，并且比传统的 Fully Connected Neural Networks (FCNN) 和 LSTM 方法更高精度。<details>
<summary>Abstract</summary>
Non-core drilling has gradually become the primary exploration method in geological engineering, and well logging curves have increasingly gained importance as the main carriers of geological information. However, factors such as geological environment, logging equipment, borehole quality, and unexpected events can all impact the quality of well logging curves. Previous methods of re-logging or manual corrections have been associated with high costs and low efficiency. This paper proposes a machine learning method that utilizes existing data to predict missing well logging curves, and its effectiveness and feasibility have been validated through experiments. The proposed method builds upon the traditional Long Short-Term Memory (LSTM) neural network by incorporating a self-attention mechanism to analyze the spatial dependencies of the data. It selectively includes the dominant computational results in the LSTM, reducing the computational complexity from O(n^2) to O(nlogn) and improving model efficiency. Experimental results demonstrate that the proposed method achieves higher accuracy compared to traditional curve synthesis methods based on Fully Connected Neural Networks (FCNN) and LSTM. This accurate, efficient, and cost-effective prediction method holds practical value in engineering applications.
</details>
<details>
<summary>摘要</summary>
非核心钻探逐渐成为地质工程的主要探测方法，而井 logging 曲线也逐渐成为主要的地质信息传递者。然而，地质环境、钻探设备、井井质量和意外事件等因素都会影响井 logging 曲线的质量。过去的重新采样或手动修正方法均具有高成本和低效率。这篇论文提出了一种使用现有数据预测缺失井 logging 曲线的机器学习方法，并通过实验证明其效果和可行性。该方法基于传统的 Long Short-Term Memory (LSTM) 神经网络，并在该网络中添加了自注意机制来分析数据的空间相关性。它选择性地包含 LSTM 中的主导计算结果，从而将计算复杂性从 O(n^2) 降低到 O(nlogn)，提高模型效率。实验结果表明，提议的方法与基于 Fully Connected Neural Networks (FCNN) 和 LSTM 的传统曲线合成方法相比，具有更高的准确率。这种准确、有效、Cost-effective 的预测方法在工程应用中具有实际价值。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-processes-for-Bayesian-inverse-problems-associated-with-linear-partial-differential-equations"><a href="#Gaussian-processes-for-Bayesian-inverse-problems-associated-with-linear-partial-differential-equations" class="headerlink" title="Gaussian processes for Bayesian inverse problems associated with linear partial differential equations"></a>Gaussian processes for Bayesian inverse problems associated with linear partial differential equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08343">http://arxiv.org/abs/2307.08343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianming Bai, Aretha L. Teckentrup, Konstantinos C. Zygalakis</li>
<li>for: 该论文关注使用 Gaussian 替身模型解决 bayesian 反问题，特别是只有小量训练数据的情况下。</li>
<li>methods:  authors extend Raissi et al. (2017) 的框架，使用 PDE-informed Gaussian 假设来构建不同的approximate posteriors。</li>
<li>results:  numerical experiments 表明，使用 PDE-informed Gaussian 假设可以提高模型的性能，比传统假设更好。Here’s the same information in English:</li>
<li>for: The paper focuses on using Gaussian surrogate models for Bayesian inverse problems associated with linear partial differential equations, particularly in the regime where only a small amount of training data is available.</li>
<li>methods: The authors extend the framework of Raissi et al. (2017) to construct PDE-informed Gaussian priors, which are used to construct different approximate posteriors.</li>
<li>results: Numerical experiments demonstrate the superiority of the PDE-informed Gaussian priors over more traditional priors.<details>
<summary>Abstract</summary>
This work is concerned with the use of Gaussian surrogate models for Bayesian inverse problems associated with linear partial differential equations. A particular focus is on the regime where only a small amount of training data is available. In this regime the type of Gaussian prior used is of critical importance with respect to how well the surrogate model will perform in terms of Bayesian inversion. We extend the framework of Raissi et. al. (2017) to construct PDE-informed Gaussian priors that we then use to construct different approximate posteriors. A number of different numerical experiments illustrate the superiority of the PDE-informed Gaussian priors over more traditional priors.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RAYEN-Imposition-of-Hard-Convex-Constraints-on-Neural-Networks"><a href="#RAYEN-Imposition-of-Hard-Convex-Constraints-on-Neural-Networks" class="headerlink" title="RAYEN: Imposition of Hard Convex Constraints on Neural Networks"></a>RAYEN: Imposition of Hard Convex Constraints on Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08336">http://arxiv.org/abs/2307.08336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leggedrobotics/rayen">https://github.com/leggedrobotics/rayen</a></li>
<li>paper_authors: Jesus Tordesillas, Jonathan P. How, Marco Hutter</li>
<li>for: 这个论文是用来实现神经网络中的硬 convex 约束的框架，保证任何输入或者神经网络的参数，约束都会被满足。</li>
<li>methods: 这个框架使用了一些新的技术，例如不需要计算量占用的正交投影步骤、不需要软约束（不能保证约束在测试时都会被满足）、不需要保守的近似约束集和不需要慢速的内部梯度下降来保证约束。</li>
<li>results: 使用这个框架，可以很快地（比如1Kquadratic约束在1000维变量上的 overhead低于8ms，300x300稠密矩阵LMI约束在10000维变量上的 overhead低于12ms）进行约束优化问题的解决，而且可以保证约束的满足，计算时间比状态艺术算法快，计算结果几乎与最优解一致。<details>
<summary>Abstract</summary>
This paper presents RAYEN, a framework to impose hard convex constraints on the output or latent variable of a neural network. RAYEN guarantees that, for any input or any weights of the network, the constraints are satisfied at all times. Compared to other approaches, RAYEN does not perform a computationally-expensive orthogonal projection step onto the feasible set, does not rely on soft constraints (which do not guarantee the satisfaction of the constraints at test time), does not use conservative approximations of the feasible set, and does not perform a potentially slow inner gradient descent correction to enforce the constraints. RAYEN supports any combination of linear, convex quadratic, second-order cone (SOC), and linear matrix inequality (LMI) constraints, achieving a very small computational overhead compared to unconstrained networks. For example, it is able to impose 1K quadratic constraints on a 1K-dimensional variable with an overhead of less than 8 ms, and an LMI constraint with 300x300 dense matrices on a 10K-dimensional variable in less than 12 ms. When used in neural networks that approximate the solution of constrained optimization problems, RAYEN achieves computation times between 20 and 7468 times faster than state-of-the-art algorithms, while guaranteeing the satisfaction of the constraints at all times and obtaining a cost very close to the optimal one.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Machine-Learning-based-Empirical-Evaluation-of-Cyber-Threat-Actors-High-Level-Attack-Patterns-over-Low-level-Attack-Patterns-in-Attributing-Attacks"><a href="#A-Machine-Learning-based-Empirical-Evaluation-of-Cyber-Threat-Actors-High-Level-Attack-Patterns-over-Low-level-Attack-Patterns-in-Attributing-Attacks" class="headerlink" title="A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks"></a>A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10252">http://arxiv.org/abs/2307.10252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Umara Noor, Sawera Shahid, Rimsha Kanwal, Zahid Rashid<br>for:这个论文旨在探讨了Cyber threat attribution的问题，即在网络空间中识别攻击者的过程。methods:这篇论文使用了手动分析攻击 patrerns，包括骗财机制、侵入检测系统、防火墙和trace-back过程，以及高级指标（IOC）和低级指标（IOC）的比较。results:实验结果显示，高级指标（IOC）训练模型可以有95%的准确率地归类攻击，而低级指标（IOC）训练模型的准确率只有40%。<details>
<summary>Abstract</summary>
Cyber threat attribution is the process of identifying the actor of an attack incident in cyberspace. An accurate and timely threat attribution plays an important role in deterring future attacks by applying appropriate and timely defense mechanisms. Manual analysis of attack patterns gathered by honeypot deployments, intrusion detection systems, firewalls, and via trace-back procedures is still the preferred method of security analysts for cyber threat attribution. Such attack patterns are low-level Indicators of Compromise (IOC). They represent Tactics, Techniques, Procedures (TTP), and software tools used by the adversaries in their campaigns. The adversaries rarely re-use them. They can also be manipulated, resulting in false and unfair attribution. To empirically evaluate and compare the effectiveness of both kinds of IOC, there are two problems that need to be addressed. The first problem is that in recent research works, the ineffectiveness of low-level IOC for cyber threat attribution has been discussed intuitively. An empirical evaluation for the measure of the effectiveness of low-level IOC based on a real-world dataset is missing. The second problem is that the available dataset for high-level IOC has a single instance for each predictive class label that cannot be used directly for training machine learning models. To address these problems in this research work, we empirically evaluate the effectiveness of low-level IOC based on a real-world dataset that is specifically built for comparative analysis with high-level IOC. The experimental results show that the high-level IOC trained models effectively attribute cyberattacks with an accuracy of 95% as compared to the low-level IOC trained models where accuracy is 40%.
</details>
<details>
<summary>摘要</summary>
“网络威胁识别是指在网络空间中识别攻击事件的具体执行者。正确和及时的威胁识别对于防止未来攻击提供了重要的防御机制。现今，安全分析师仍然采用手动分析攻击模式，包括骗子部署、入侵检测系统、防火墙等，以及跟踪返回过程，进行威胁识别。这些攻击模式被称为低级别征识（Indicators of Compromise，IOC）。它们表示敌对者在其攻击活动中使用的策略、技术、程序（Tactics, Techniques, Procedures，TTP）和软件工具。敌对者很少重复使用这些攻击模式，它们也可以被修改，导致假的和不公正的威胁识别。为了empirically评估和比较低级别征识和高级别征识的效果，这些问题需要被解决。第一个问题是，在当前的研究中，低级别征识的效果不够有效性已经被直观提出。empirical评估基于实际数据集的低级别征识效果缺失。第二个问题是，可用的高级别征识数据集中每个预测类别的单个实例不能直接用于机器学习模型训练。为解决这些问题，我们在这项研究中Empirically评估了低级别征识的效果，并使用特定 для比较分析的实际数据集。实验结果显示，高级别征识训练模型可以准确地归类攻击事件，准确率达95%，而低级别征识训练模型的准确率只有40%。”
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Impact-of-Adversarial-Examples-on-Explainable-Machine-Learning"><a href="#Analyzing-the-Impact-of-Adversarial-Examples-on-Explainable-Machine-Learning" class="headerlink" title="Analyzing the Impact of Adversarial Examples on Explainable Machine Learning"></a>Analyzing the Impact of Adversarial Examples on Explainable Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08327">http://arxiv.org/abs/2307.08327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prathyusha Devabhakthini, Sasmita Parida, Raj Mani Shukla, Suvendu Chandan Nayak</li>
<li>for: 这个论文探讨了深度学习模型对针对性攻击的抵触性。</li>
<li>methods: 作者采用了一种ML基于的文本分类模型，然后引入了针对性攻击来影响模型的分类性能。</li>
<li>results: 研究发现，针对性攻击可以轻松地使模型错误地预测文本。此外，作者还分析了模型的解释性前后针对性攻击，以了解模型在攻击后的性能。<details>
<summary>Abstract</summary>
Adversarial attacks are a type of attack on machine learning models where an attacker deliberately modifies the inputs to cause the model to make incorrect predictions. Adversarial attacks can have serious consequences, particularly in applications such as autonomous vehicles, medical diagnosis, and security systems. Work on the vulnerability of deep learning models to adversarial attacks has shown that it is very easy to make samples that make a model predict things that it doesn't want to. In this work, we analyze the impact of model interpretability due to adversarial attacks on text classification problems. We develop an ML-based classification model for text data. Then, we introduce the adversarial perturbations on the text data to understand the classification performance after the attack. Subsequently, we analyze and interpret the model's explainability before and after the attack
</details>
<details>
<summary>摘要</summary>
adversarial 攻击是一种针对机器学习模型的攻击，攻击者故意修改输入，以让模型作出错误预测。 adversarial 攻击可能会有严重的后果，特别是在自动驾驶、医疗诊断和安全系统等应用中。我们在深度学习模型对 adversarial 攻击的抵触性上进行了研究。我们开发了一个基于 ML 的文本数据分类模型，然后引入了对文本数据的 adversarial 偏移，以了解攻击后分类性能。接着，我们分析和解释模型在攻击后的解释性。
</details></li>
</ul>
<hr>
<h2 id="A-Secure-Aggregation-for-Federated-Learning-on-Long-Tailed-Data"><a href="#A-Secure-Aggregation-for-Federated-Learning-on-Long-Tailed-Data" class="headerlink" title="A Secure Aggregation for Federated Learning on Long-Tailed Data"></a>A Secure Aggregation for Federated Learning on Long-Tailed Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08324">http://arxiv.org/abs/2307.08324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanna Jiang, Baihe Ma, Xu Wang, Guangsheng Yu, Caijun Sun, Wei Ni, Ren Ping Liu</li>
<li>for: 本研究针对 Federated Learning (FL) 面临的两大挑战：数据分布不均和模型攻击。</li>
<li>methods: 提出了一种新的两层聚合方法，可以拒绝恶意模型和选择值得模型，并具有较好的抗辐射性。</li>
<li>results: 实验表明，思想团（think tank）可以有效地选择模型进行全局聚合。<details>
<summary>Abstract</summary>
As a distributed learning, Federated Learning (FL) faces two challenges: the unbalanced distribution of training data among participants, and the model attack by Byzantine nodes. In this paper, we consider the long-tailed distribution with the presence of Byzantine nodes in the FL scenario. A novel two-layer aggregation method is proposed for the rejection of malicious models and the advisable selection of valuable models containing tail class data information. We introduce the concept of think tank to leverage the wisdom of all participants. Preliminary experiments validate that the think tank can make effective model selections for global aggregation.
</details>
<details>
<summary>摘要</summary>
作为分布式学习的一种形式， federated learning (FL) 面临两大挑战：训练数据在参与者中的不均匀分布，以及由拜占庭节点引起的模型攻击。在这篇论文中，我们考虑了在 FL 场景中存在长板分布和拜占庭节点的情况下的长板分布。我们提出了一种新的两层聚合方法，用于拒绝恶意模型和选择值得采用的模型，并具有拥有尾类数据信息。我们引入了“思想库”的概念，以利用所有参与者的智慧。初步实验表明，思想库可以做到有效地选择模型进行全球聚合。
</details></li>
</ul>
<hr>
<h2 id="Airway-Label-Prediction-in-Video-Bronchoscopy-Capturing-Temporal-Dependencies-Utilizing-Anatomical-Knowledge"><a href="#Airway-Label-Prediction-in-Video-Bronchoscopy-Capturing-Temporal-Dependencies-Utilizing-Anatomical-Knowledge" class="headerlink" title="Airway Label Prediction in Video Bronchoscopy: Capturing Temporal Dependencies Utilizing Anatomical Knowledge"></a>Airway Label Prediction in Video Bronchoscopy: Capturing Temporal Dependencies Utilizing Anatomical Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08318">http://arxiv.org/abs/2307.08318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ron Keuth, Mattias Heinrich, Martin Eichenlaub, Marian Himstedt</li>
<li>For: This paper provides a novel approach for navigation guidance during bronchoscopy interventions without the need for electromagnetic tracking or patient-specific CT scans.* Methods: The proposed approach uses topological bronchoscope localization and incorporates sequences of CNN-based airway likelihoods into a Hidden Markov Model, leveraging anatomical constraints and temporal context for improved accuracy.* Results: The approach is evaluated in a lung phantom model and achieves an accuracy of up to 0.98 compared to 0.81 for a classification based on individual frames, demonstrating the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
Purpose: Navigation guidance is a key requirement for a multitude of lung interventions using video bronchoscopy. State-of-the-art solutions focus on lung biopsies using electromagnetic tracking and intraoperative image registration w.r.t. preoperative CT scans for guidance. The requirement of patient-specific CT scans hampers the utilisation of navigation guidance for other applications such as intensive care units.   Methods: This paper addresses navigation guidance solely incorporating bronchosopy video data. In contrast to state-of-the-art approaches we entirely omit the use of electromagnetic tracking and patient-specific CT scans. Guidance is enabled by means of topological bronchoscope localization w.r.t. an interpatient airway model. Particularly, we take maximally advantage of anatomical constraints of airway trees being sequentially traversed. This is realized by incorporating sequences of CNN-based airway likelihoods into a Hidden Markov Model.   Results: Our approach is evaluated based on multiple experiments inside a lung phantom model. With the consideration of temporal context and use of anatomical knowledge for regularization, we are able to improve the accuracy up to to 0.98 compared to 0.81 (weighted F1: 0.98 compared to 0.81) for a classification based on individual frames.   Conclusion: We combine CNN-based single image classification of airway segments with anatomical constraints and temporal HMM-based inference for the first time. Our approach renders vision-only guidance for bronchoscopy interventions in the absence of electromagnetic tracking and patient-specific CT scans possible.
</details>
<details>
<summary>摘要</summary>
目的：用视频镜头导航是肺间化学疗法中不可或缺的一种重要需求。现代解决方案主要关注于基于电磁场追踪和在手术过程中对先前的CT扫描图进行图像匹配的医学器械导航。但是，需要患者特定的CT扫描图的使用限制了导航导航的使用范围只能用于血液急救室等其他应用。方法：本文提出一种具有视频镜头导航功能的新方法，与现有方法不同之处在于完全没有使用电磁场追踪和患者特定的CT扫描图。导航是基于气管内部空间模型和视频镜头数据进行的，具有较高的准确率和可靠性。结果：我们在肺脏模型中进行了多次实验，结果表明，通过利用空间和时间上的约束和图像分类的权重补做，我们可以提高准确率至0.98（weighted F1 score: 0.98），比对ividual帧的分类结果（0.81）高出了17.6%。结论：我们在空间和时间上具有约束的HMM模型中结合了CNN基于单帧图像分类和空间约束，实现了没有电磁场追踪和患者特定CT扫描图的视频镜头导航。这种方法可以在肺间化学疗法中提供更加可靠和高效的导航导航。
</details></li>
</ul>
<hr>
<h2 id="Soft-Prompt-Tuning-for-Augmenting-Dense-Retrieval-with-Large-Language-Models"><a href="#Soft-Prompt-Tuning-for-Augmenting-Dense-Retrieval-with-Large-Language-Models" class="headerlink" title="Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models"></a>Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08303">http://arxiv.org/abs/2307.08303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhiyuanpeng/sptar">https://github.com/zhiyuanpeng/sptar</a></li>
<li>paper_authors: Zhiyuan Peng, Xuyang Wu, Yi Fang</li>
<li>for: 提高 dense retrieval 模型的性能，尤其是在lacking domain-specific training data的情况下。</li>
<li>methods: 使用 soft prompt tuning 方法，通过优化任务特定的软提示来提高 LLMs 生成的弱查询语句质量，然后使用这些弱查询语句来训练任务特定的 dense retriever。</li>
<li>results: SPTAR 方法在不supervised baselines BM25 和 LLMs-based augmentation method 的基础上具有更高的性能，可以提高 dense retrieval 模型的搜索效果。<details>
<summary>Abstract</summary>
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific dense retrievers. We design a filter to select high-quality example document-query pairs in the prompt to further improve the quality of weak tagged queries. To the best of our knowledge, there is no prior work utilizing soft prompt tuning to augment DR models. The experiments demonstrate that SPTAR outperforms the unsupervised baselines BM25 and the recently proposed LLMs-based augmentation method for DR.
</details>
<details>
<summary>摘要</summary>
dense retrieval (DR) 将查询和文档转换为紧凑表示并在 vector 空间中度量查询和文档之间的相似性。DR 的一个挑战是缺乏域pecific 训练数据。虽然 DR 模型可以通过转移学习从大规模公共数据集如 MS MARCO 学习，但证据表明不 все DR 模型和领域可以受益于转移学习相同。 reciently，一些研究人员已经使用大语言模型 (LLMs) 来提高零ocket 和几ocket DR 模型。然而，使用的 hard prompts 或人工写的 prompts 无法保证生成的弱 queries 的好质量。为了解决这个问题，我们提出了软提示调整 для增强 DR (SPTAR)：对每个任务，我们利用软提示调整来优化任务特定的软提示，然后使用 LLMs 将标注无标注文档，生成足够的弱文档-查询对以训练任务特定的紧凑检索器。我们设计了一个筛选器来选择高质量的示例文档-查询对，以进一步提高弱标注查询的质量。到目前为止，没有什么先进的工作利用软提示调整来增强 DR 模型。实验表明，SPTAR 超过了无监督基准和最近提出的基于 LLMs 的增强方法。
</details></li>
</ul>
<hr>
<h2 id="GBT-Two-stage-transformer-framework-for-non-stationary-time-series-forecasting"><a href="#GBT-Two-stage-transformer-framework-for-non-stationary-time-series-forecasting" class="headerlink" title="GBT: Two-stage transformer framework for non-stationary time series forecasting"></a>GBT: Two-stage transformer framework for non-stationary time series forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08302">http://arxiv.org/abs/2307.08302</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/origamisl/gbt">https://github.com/origamisl/gbt</a></li>
<li>paper_authors: Li Shen, Yuning Wei, Yangzhu Wang</li>
<li>for: 本研究旨在解决时间序列预测变换器（TSFT）的严重过拟合问题，尤其是在处理非站ARY时间序列时。</li>
<li>methods: 我们提出了一种新的两阶段变换器框架，称为Good Beginning Transformer（GBT），它将TSFT的预测过程分解成两个阶段：自动回归阶段和自我回归阶段。在自动回归阶段，预测结果作为一个更好的初始化方法，并在自我回归阶段进行进一步的预测。</li>
<li>results: 我们在七个基准数据集上进行了广泛的实验，结果显示GBT在预测能力方面超过了现有的TSFT和其他预测模型（SCINet、N-HiTS等），并且具有较低的时间和空间复杂度。GBT还可以与这些模型结合使用，以增强其预测能力。<details>
<summary>Abstract</summary>
This paper shows that time series forecasting Transformer (TSFT) suffers from severe over-fitting problem caused by improper initialization method of unknown decoder inputs, esp. when handling non-stationary time series. Based on this observation, we propose GBT, a novel two-stage Transformer framework with Good Beginning. It decouples the prediction process of TSFT into two stages, including Auto-Regression stage and Self-Regression stage to tackle the problem of different statistical properties between input and prediction sequences.Prediction results of Auto-Regression stage serve as a Good Beginning, i.e., a better initialization for inputs of Self-Regression stage. We also propose Error Score Modification module to further enhance the forecasting capability of the Self-Regression stage in GBT. Extensive experiments on seven benchmark datasets demonstrate that GBT outperforms SOTA TSFTs (FEDformer, Pyraformer, ETSformer, etc.) and many other forecasting models (SCINet, N-HiTS, etc.) with only canonical attention and convolution while owning less time and space complexity. It is also general enough to couple with these models to strengthen their forecasting capability. The source code is available at: https://github.com/OrigamiSL/GBT
</details>
<details>
<summary>摘要</summary>
To further enhance the forecasting capability of GBT, we propose an Error Score Modification module. This module adjusts the error scores of the Self-Regression stage to better handle the difference in statistical properties between the input and prediction sequences.Our extensive experiments on seven benchmark datasets show that GBT outperforms state-of-the-art TSFTs (FEDformer, Pyraformer, ETSformer, etc.) and other forecasting models (SCINet, N-HiTS, etc.) with only canonical attention and convolution, while requiring less time and space complexity. Additionally, GBT is general enough to be combined with these models to strengthen their forecasting capability. The source code is available at: <https://github.com/OrigamiSL/GBT>.
</details></li>
</ul>
<hr>
<h2 id="Systematic-Testing-of-the-Data-Poisoning-Robustness-of-KNN"><a href="#Systematic-Testing-of-the-Data-Poisoning-Robustness-of-KNN" class="headerlink" title="Systematic Testing of the Data-Poisoning Robustness of KNN"></a>Systematic Testing of the Data-Poisoning Robustness of KNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08288">http://arxiv.org/abs/2307.08288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannan Li, Jingbo Wang, Chao Wang</li>
<li>for: 这篇论文目的是提高机器学习基于训练集的软件组件的数据欺走抵触性。</li>
<li>methods: 该论文提出了一种系统测试基于方法，可以证明和证伪数据欺走 robustness。</li>
<li>results: 该方法比基eline枚举方法快速和准确，可以快速缩小搜索空间，并通过系统测试在具体空间找到实际的违反。测试结果表明，该方法可以有效地判断k- nearest neighbors（KNN）预测结果的数据欺走Robustness。<details>
<summary>Abstract</summary>
Data poisoning aims to compromise a machine learning based software component by contaminating its training set to change its prediction results for test inputs. Existing methods for deciding data-poisoning robustness have either poor accuracy or long running time and, more importantly, they can only certify some of the truly-robust cases, but remain inconclusive when certification fails. In other words, they cannot falsify the truly-non-robust cases. To overcome this limitation, we propose a systematic testing based method, which can falsify as well as certify data-poisoning robustness for a widely used supervised-learning technique named k-nearest neighbors (KNN). Our method is faster and more accurate than the baseline enumeration method, due to a novel over-approximate analysis in the abstract domain, to quickly narrow down the search space, and systematic testing in the concrete domain, to find the actual violations. We have evaluated our method on a set of supervised-learning datasets. Our results show that the method significantly outperforms state-of-the-art techniques, and can decide data-poisoning robustness of KNN prediction results for most of the test inputs.
</details>
<details>
<summary>摘要</summary>
“数据毒化”是一种攻击机器学习基础的软件元件，通过污染它的训练集，让它的预测结果对测试输入进行变化。现有的方法可以评估数据毒化Robustness，但是它们的精度受限，或者执行时间很长，而且它们只能认证一些真正可靠的情况，但是无法确定不可靠的情况。为了解决这个限制，我们提出了一个系统性的测试方法，可以确定以及否定数据毒化Robustness，这个方法在一个广泛使用的超过近边法（KNN）上进行了评估。我们的方法比基准枚举方法更快和更精度，是因为我们使用了一种新的抽象领域中的误差分析，快速地缩小搜索空间，并且在实际领域中进行系统性的测试，实际找到了违背的情况。我们在一些超过近边法的数据上进行了评估，结果显示，我们的方法可以在大多数的测试输入上实现数据毒化Robustness的决定。”
</details></li>
</ul>
<hr>
<h2 id="Going-Beyond-Linear-Mode-Connectivity-The-Layerwise-Linear-Feature-Connectivity"><a href="#Going-Beyond-Linear-Mode-Connectivity-The-Layerwise-Linear-Feature-Connectivity" class="headerlink" title="Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity"></a>Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08286">http://arxiv.org/abs/2307.08286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanpeng Zhou, Yongyi Yang, Xiaojiang Yang, Junchi Yan, Wei Hu</li>
<li>for: 本研究探讨了神经网络训练过程中的一些有趣实验现象，包括Linear Mode Connectivity（LMC）等。</li>
<li>methods: 本研究使用了多种方法来探讨LMC和Layerwise Linear Feature Connectivity（LLFC）的现象，包括随机排序和生成新的网络等。</li>
<li>results: 研究发现，当两个训练过的网络满足LMC时，它们通常也满足LLFC在大多数层次。此外，研究还探讨了LLFC的下面因素，提供了新的思路和技术来理解LMC和LLFC。<details>
<summary>Abstract</summary>
Recent work has revealed many intriguing empirical phenomena in neural network training, despite the poorly understood and highly complex loss landscapes and training dynamics. One of these phenomena, Linear Mode Connectivity (LMC), has gained considerable attention due to the intriguing observation that different solutions can be connected by a linear path in the parameter space while maintaining near-constant training and test losses. In this work, we introduce a stronger notion of linear connectivity, Layerwise Linear Feature Connectivity (LLFC), which says that the feature maps of every layer in different trained networks are also linearly connected. We provide comprehensive empirical evidence for LLFC across a wide range of settings, demonstrating that whenever two trained networks satisfy LMC (via either spawning or permutation methods), they also satisfy LLFC in nearly all the layers. Furthermore, we delve deeper into the underlying factors contributing to LLFC, which reveal new insights into the spawning and permutation approaches. The study of LLFC transcends and advances our understanding of LMC by adopting a feature-learning perspective.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Complexity-Matters-Rethinking-the-Latent-Space-for-Generative-Modeling"><a href="#Complexity-Matters-Rethinking-the-Latent-Space-for-Generative-Modeling" class="headerlink" title="Complexity Matters: Rethinking the Latent Space for Generative Modeling"></a>Complexity Matters: Rethinking the Latent Space for Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08283">http://arxiv.org/abs/2307.08283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyang Hu, Fei Chen, Haonan Wang, Jiawei Li, Wenjia Wang, Jiacheng Sun, Zhenguo Li</li>
<li>for: 本研究旨在探讨generative模型中latent space的选择，尤其是如何选择最佳的latent space，以提高generative性能。</li>
<li>methods: 我们提出了一种基于模型复杂度的latent space选择方法，并提出了一种两阶段训练策略called Decoupled Autoencoder (DAE)，可以改善latent distribution并提高生成性能。</li>
<li>results: 我们的理论分析和实验结果表明，DAE可以提高sample质量，同时降低模型的复杂度。<details>
<summary>Abstract</summary>
In generative modeling, numerous successful approaches leverage a low-dimensional latent space, e.g., Stable Diffusion models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel "distance" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN and Diffusion Transformer, where our modifications yield significant improvements in sample quality with decreased model complexity.
</details>
<details>
<summary>摘要</summary>
在生成模型中，许多成功的方法利用低维度的隐藏空间，例如稳定扩散模型，通过一个匹配的解码器生成图像。although the selection of the latent space is crucial, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel "distance" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN and Diffusion Transformer, where our modifications yield significant improvements in sample quality with decreased model complexity.Here's the translation in Traditional Chinese:在生成模型中，许多成功的方法利用低维度的隐藏空间，例如稳定扩散模型，通过一个匹配的解码器生成图像。although the selection of the latent space is crucial, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel "distance" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN and Diffusion Transformer, where our modifications yield significant improvements in sample quality with decreased model complexity.
</details></li>
</ul>
<hr>
<h2 id="Certifying-the-Fairness-of-KNN-in-the-Presence-of-Dataset-Bias"><a href="#Certifying-the-Fairness-of-KNN-in-the-Presence-of-Dataset-Bias" class="headerlink" title="Certifying the Fairness of KNN in the Presence of Dataset Bias"></a>Certifying the Fairness of KNN in the Presence of Dataset Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08722">http://arxiv.org/abs/2307.08722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannan Li, Jingbo Wang, Chao Wang</li>
<li>for: The paper is written for certifying the fairness of the classification result of the k-nearest neighbors (KNN) algorithm under the assumption of historical bias in the training data.</li>
<li>methods: The paper proposes a method for certifying fairness based on three variants of fairness definitions: individual fairness, $\epsilon$-fairness, and label-flipping fairness. The method uses sound approximations of the complex arithmetic computations used in the state-of-the-art KNN algorithm to reduce computational cost.</li>
<li>results: The paper shows the effectiveness of the proposed method through experimental evaluation on six widely used datasets in the fairness research literature. The method is able to obtain fairness certifications for a large number of test inputs despite the presence of historical bias in the datasets.<details>
<summary>Abstract</summary>
We propose a method for certifying the fairness of the classification result of a widely used supervised learning algorithm, the k-nearest neighbors (KNN), under the assumption that the training data may have historical bias caused by systematic mislabeling of samples from a protected minority group. To the best of our knowledge, this is the first certification method for KNN based on three variants of the fairness definition: individual fairness, $\epsilon$-fairness, and label-flipping fairness. We first define the fairness certification problem for KNN and then propose sound approximations of the complex arithmetic computations used in the state-of-the-art KNN algorithm. This is meant to lift the computation results from the concrete domain to an abstract domain, to reduce the computational cost. We show effectiveness of this abstract interpretation based technique through experimental evaluation on six datasets widely used in the fairness research literature. We also show that the method is accurate enough to obtain fairness certifications for a large number of test inputs, despite the presence of historical bias in the datasets.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于证明某种广泛使用的直接学习算法（k-最近邻）的分类结果是否公平，假设训练数据可能受到历史偏见的影响，特别是保护少数群体样本的系统化错误标注。根据我们所知，这是第一种基于三种公平定义（个体公平、ε-公平和标签抓取公平）的公平证明方法。我们首先定义了公平证明问题，然后提出了使用现代KNN算法中的复杂数学计算的准确估计方法，以减少计算成本。我们通过实验评估六个广泛用于公平研究文献中的数据集，并证明了这种抽象计算方法的有效性。我们还证明了方法可以快速获得大量测试输入的公平证明，即使训练数据中存在历史偏见。
</details></li>
</ul>
<hr>
<h2 id="Automated-Action-Model-Acquisition-from-Narrative-Texts"><a href="#Automated-Action-Model-Acquisition-from-Narrative-Texts" class="headerlink" title="Automated Action Model Acquisition from Narrative Texts"></a>Automated Action Model Acquisition from Narrative Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10247">http://arxiv.org/abs/2307.10247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiqi Li, Leyang Cui, Songtuan Lin, Patrik Haslum</li>
<li>for: 本研究旨在提高AI代理人的规划技术应用，通过自动从叙述文本中提取струк成事件和生成 планинг语言风格的动作模型。</li>
<li>methods: 本研究使用了自动提取叙述文本中的结构事件，并通过预测通用常识事件关系、文本矛盾和相似性来生成 planning-language-style 动作模型。</li>
<li>results: 实验结果表明，NaRuto可以在经典叙述规划领域生成高质量的动作模型，与现有的完全自动方法相当，甚至与半自动方法相当。<details>
<summary>Abstract</summary>
Action models, which take the form of precondition/effect axioms, facilitate causal and motivational connections between actions for AI agents. Action model acquisition has been identified as a bottleneck in the application of planning technology, especially within narrative planning. Acquiring action models from narrative texts in an automated way is essential, but challenging because of the inherent complexities of such texts. We present NaRuto, a system that extracts structured events from narrative text and subsequently generates planning-language-style action models based on predictions of commonsense event relations, as well as textual contradictions and similarities, in an unsupervised manner. Experimental results in classical narrative planning domains show that NaRuto can generate action models of significantly better quality than existing fully automated methods, and even on par with those of semi-automated methods.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文。<</SYS>>行动模型，即前提/效果axioms，为AI代理人提供了 causal 和 motivational 连接。行动模型获取被识别为规划技术应用的瓶颈，尤其在叙述规划领域。自动从叙述文本中获取行动模型是重要，但具有内在复杂性。我们提出了NaRuto系统，该系统通过预测常识事件关系以及文本矛盾和相似性来自动生成 планинг语言风格的行动模型，并在经验领域中达到了现有完全自动方法的水平。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-on-Traffic-Sign-Recognition-A-Survey"><a href="#Adversarial-Attacks-on-Traffic-Sign-Recognition-A-Survey" class="headerlink" title="Adversarial Attacks on Traffic Sign Recognition: A Survey"></a>Adversarial Attacks on Traffic Sign Recognition: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08278">http://arxiv.org/abs/2307.08278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Svetlana Pavlitska, Nico Lambing, J. Marius Zöllner</li>
<li>for: 本研究旨在探讨攻击 autonomous driving 系统的可能性，尤其是针对交通标识模型的攻击。</li>
<li>methods: 本研究准确地描述了现有的攻击方法，包括数字和实际攻击。</li>
<li>results: 研究发现，现有的攻击方法可以轻松地破坏交通标识模型的正常工作，需要进一步的研究以减少这些攻击的风险。<details>
<summary>Abstract</summary>
Traffic sign recognition is an essential component of perception in autonomous vehicles, which is currently performed almost exclusively with deep neural networks (DNNs). However, DNNs are known to be vulnerable to adversarial attacks. Several previous works have demonstrated the feasibility of adversarial attacks on traffic sign recognition models. Traffic signs are particularly promising for adversarial attack research due to the ease of performing real-world attacks using printed signs or stickers. In this work, we survey existing works performing either digital or real-world attacks on traffic sign detection and classification models. We provide an overview of the latest advancements and highlight the existing research areas that require further investigation.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆的辨识功能中，交通标志识别是一个关键组件，目前大多使用深度神经网络（DNN）来实现。但是，DNN受到恶意攻击的可能性很高。先前的研究已经证明了对交通标志识别模型的攻击的可能性。由于交通标志的易攻击性，使得实际攻击更加容易。在这种情况下，我们对现有的数字和实际攻击研究进行了抽象和概述，并高亮了需要进一步研究的领域。
</details></li>
</ul>
<hr>
<h2 id="Deep-Neural-Networks-and-Brain-Alignment-Brain-Encoding-and-Decoding-Survey"><a href="#Deep-Neural-Networks-and-Brain-Alignment-Brain-Encoding-and-Decoding-Survey" class="headerlink" title="Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)"></a>Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10246">http://arxiv.org/abs/2307.10246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subba Reddy Oota, Manish Gupta, Raju S. Bapi, Gael Jobard, Frederic Alexandre, Xavier Hinaut</li>
<li>for: 研究大脑如何表示不同的信息模式，以及设计一个系统可以自动理解用户的思维？</li>
<li>methods: 使用 функциональ磁共振成像（fMRI）记录大脑活动，并提出了多种基于深度学习的编码和解码模型。</li>
<li>results: 这些模型可以用于评估和诊断神经科学问题，以及设计大脑机器或计算机界面。<details>
<summary>Abstract</summary>
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for designing brain-machine or brain-computer interfaces. Inspired by the effectiveness of deep learning models for natural language processing, computer vision, and speech, recently several neural encoding and decoding models have been proposed. In this survey, we will first discuss popular representations of language, vision and speech stimuli, and present a summary of neuroscience datasets. Further, we will review popular deep learning based encoding and decoding architectures and note their benefits and limitations. Finally, we will conclude with a brief summary and discussion about future trends. Given the large amount of recently published work in the `computational cognitive neuroscience' community, we believe that this survey nicely organizes the plethora of work and presents it as a coherent story.
</details>
<details>
<summary>摘要</summary>
如何让脑子表示不同的信息？我们可以通过研究脑电图像（fMRI）来回答这些问题。脑科学社区已经提供了许多大量的认知神经科学数据集，这些数据集关于静止阅读/听取/观看概念词、故事、图片和电影。使用这些数据集，以前已经提出了编码和解码模型。这些模型可以用于基础研究认知科学和神经科学。编码模型可以自动生成脑电图像，它们有许多实际应用，如诊断和治疗神经系统疾病。解码模型可以 reconstruction 脑电图像，它们有用于设计脑机或脑计算机界面。鼓励于深度学习模型在自然语言处理、计算机视觉和语音处理等领域的效果，最近几年有很多 neural encoding 和 decoding 模型被提出。在这篇评论中，我们将首先讲讲语言、视觉和听说 stimuli 的受欢迎表示，并提供脑科学数据集的摘要。然后，我们将回顾深度学习基于编码和解码架构的一些模型，并注意它们的优点和局限性。最后，我们将结束于简要的总结和讨论，并讨论未来的趋势。由于最近出版的大量工作在 'computational cognitive neuroscience' 社区，我们认为这篇评论 nicely 组织了这些工作，并将它们表现为一个coherent 的故事。
</details></li>
</ul>
<hr>
<h2 id="Transferable-Graph-Neural-Fingerprint-Models-for-Quick-Response-to-Future-Bio-Threats"><a href="#Transferable-Graph-Neural-Fingerprint-Models-for-Quick-Response-to-Future-Bio-Threats" class="headerlink" title="Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats"></a>Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01921">http://arxiv.org/abs/2308.01921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Chen, Yihui Ren, Ai Kagawa, Matthew R. Carbone, Samuel Yen-Chi Chen, Xiaohui Qu, Shinjae Yoo, Austin Clyde, Arvind Ramanathan, Rick L. Stevens, Hubertus J. J. van Dam, Deyu Liu</li>
<li>for: 这个论文的目的是为了快速屏测药物分子，以便在药物发现过程中快速搜索出有效的药物候选者。</li>
<li>methods: 这个论文使用的方法是基于蛋白质绑定亲和力的图 neural fingerprint方法，这种方法可以在高速和高准确性之间进行药物 docking 模拟。</li>
<li>results: 这个论文的结果表明，使用图 neural fingerprint方法可以对 COVID-19 药物 docking 问题进行高效的虚拟屏测，并且其预测精度比传统的圆形指纹方法更高， сред平方误差小于 $0.21$ kcal&#x2F;mol。此外， authors 还提出了一种可以适用于未知目标的转移性图 neural fingerprint方法，该方法可以在多个目标上进行训练，并且与特定目标的图 neural fingerprint模型具有相似的准确性。<details>
<summary>Abstract</summary>
Fast screening of drug molecules based on the ligand binding affinity is an important step in the drug discovery pipeline. Graph neural fingerprint is a promising method for developing molecular docking surrogates with high throughput and great fidelity. In this study, we built a COVID-19 drug docking dataset of about 300,000 drug candidates on 23 coronavirus protein targets. With this dataset, we trained graph neural fingerprint docking models for high-throughput virtual COVID-19 drug screening. The graph neural fingerprint models yield high prediction accuracy on docking scores with the mean squared error lower than $0.21$ kcal/mol for most of the docking targets, showing significant improvement over conventional circular fingerprint methods. To make the neural fingerprints transferable for unknown targets, we also propose a transferable graph neural fingerprint method trained on multiple targets. With comparable accuracy to target-specific graph neural fingerprint models, the transferable model exhibits superb training and data efficiency. We highlight that the impact of this study extends beyond COVID-19 dataset, as our approach for fast virtual ligand screening can be easily adapted and integrated into a general machine learning-accelerated pipeline to battle future bio-threats.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:快速药物探测基于药物结合亲和力是药物发现管道中一个重要步骤。图像神经指纹是一种有前途的方法，可以快速、高精度地实现药物对抗体结合。在这项研究中，我们建立了一个COVID-19药物探测集合，包含约300,000个药物候选者，对23种新型冠状病毒蛋白目标进行了探测。使用这些数据集，我们训练了图像神经指纹对高通量虚拟COVID-19药物探测进行了训练。图像神经指纹模型在多数探测目标上显示了高精度预测吸附分数，与传统径向指纹方法相比，显示了明显的改善。为使 neural fingerprint 可以应用于未知目标，我们还提出了多目标图像神经指纹方法。与特定目标图像神经指纹模型相比，多目标模型在训练和数据效率方面表现出色。我们强调，这项研究的影响不仅限于COVID-19数据集，我们的方法可以轻松地适应和整合到一个通用的机器学习加速管道中，以应对未来的生物威胁。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-and-Enhancing-Robustness-of-Deep-Recommendation-Systems-Against-Hardware-Errors"><a href="#Evaluating-and-Enhancing-Robustness-of-Deep-Recommendation-Systems-Against-Hardware-Errors" class="headerlink" title="Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors"></a>Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10244">http://arxiv.org/abs/2307.10244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vu-detail/pytei">https://github.com/vu-detail/pytei</a></li>
<li>paper_authors: Dongning Ma, Xun Jiao, Fred Lin, Mengshi Zhang, Alban Desmaison, Thomas Sellinger, Daniel Moore, Sriram Sankar</li>
<li>for: 这 paper 是关于深度推荐系统（DRS）的可靠性研究，以寻找在大规模队列系统中发现的硬件错误对 DRS 的影响。</li>
<li>methods: 这 paper 使用了 PyTorch 构建了一个简单、高效、可扩展的错误插入框架（Terrorch），以测试 DRS 的可靠性。</li>
<li>results: 研究发现，DRS 对硬件错误的抵抗力受到多种因素的影响，包括模型参数和输入特征。研究还发现，使用活动clipping可以提高 AUC-ROC 分数，达到30%的恢复率。<details>
<summary>Abstract</summary>
Deep recommendation systems (DRS) heavily depend on specialized HPC hardware and accelerators to optimize energy, efficiency, and recommendation quality. Despite the growing number of hardware errors observed in large-scale fleet systems where DRS are deployed, the robustness of DRS has been largely overlooked. This paper presents the first systematic study of DRS robustness against hardware errors. We develop Terrorch, a user-friendly, efficient and flexible error injection framework on top of the widely-used PyTorch. We evaluate a wide range of models and datasets and observe that the DRS robustness against hardware errors is influenced by various factors from model parameters to input characteristics. We also explore 3 error mitigation methods including algorithm based fault tolerance (ABFT), activation clipping and selective bit protection (SBP). We find that applying activation clipping can recover up to 30% of the degraded AUC-ROC score, making it a promising mitigation method.
</details>
<details>
<summary>摘要</summary>
深度推荐系统（DRS）强依赖特殊的高性能计算硬件和加速器来优化能效和推荐质量。尽管大规模队列系统中DRS的可靠性受到许多硬件错误的影响，但DRS的可靠性问题还尚未得到足够的关注。本文提出了DRS可靠性对硬件错误的首次系统性研究。我们开发了一个简单、高效和灵活的错误插入框架——Terrorch，并在PyTorch上实现。我们对各种模型和数据集进行了广泛的测试，发现DRS对硬件错误的可靠性受到多种因素的影响，从模型参数到输入特征。我们还探讨了3种错误缓解方法，包括算法基于缺陷tolerance（ABFT）、活动截断和选择性位保护（SBP）。我们发现通过实施活动截断可以恢复30%的降低的AUC-ROC分数，这表明这是一种有前途的缓解方法。
</details></li>
</ul>
<hr>
<h2 id="Convex-Bi-Level-Optimization-Problems-with-Non-smooth-Outer-Objective-Function"><a href="#Convex-Bi-Level-Optimization-Problems-with-Non-smooth-Outer-Objective-Function" class="headerlink" title="Convex Bi-Level Optimization Problems with Non-smooth Outer Objective Function"></a>Convex Bi-Level Optimization Problems with Non-smooth Outer Objective Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08245">http://arxiv.org/abs/2307.08245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roey Merchav, Shoham Sabach</li>
<li>for: 解决 convex bi-level 优化问题</li>
<li>methods: 提出 Bi-Sub-Gradient (Bi-SG) 方法，基于 classical sub-gradient 方法的一种泛化</li>
<li>results:  Bi-SG 方法可以在 convex bi-level 优化问题中实现 sub-线性速率，并且如果外部目标函数具有强度 convexity，可以提高外部速率至线性速率。此外，我们证明 Bi-SG 方法生成的序列与 bi-level 优化问题的优化解的距离 converges to zero.<details>
<summary>Abstract</summary>
In this paper, we propose the Bi-Sub-Gradient (Bi-SG) method, which is a generalization of the classical sub-gradient method to the setting of convex bi-level optimization problems. This is a first-order method that is very easy to implement in the sense that it requires only a computation of the associated proximal mapping or a sub-gradient of the outer non-smooth objective function, in addition to a proximal gradient step on the inner optimization problem. We show, under very mild assumptions, that Bi-SG tackles bi-level optimization problems and achieves sub-linear rates both in terms of the inner and outer objective functions. Moreover, if the outer objective function is additionally strongly convex (still could be non-smooth), the outer rate can be improved to a linear rate. Last, we prove that the distance of the generated sequence to the set of optimal solutions of the bi-level problem converges to zero.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了Bi-Sub-Gradient（Bi-SG）方法，这是对凸二级优化问题的一种普适化。这是一种一阶方法，只需计算相关的贸易映射或外层非凸目标函数的子gradient，以及内部优化问题的质量步骤。我们证明，在非常轻松的假设下，Bi-SG可以解决二级优化问题，并在内部和外部目标函数上实现下行速率。此外，如果外层目标函数另外是强Converter (仍然可能是非凸)，我们可以提高外层速率到线性速率。最后，我们证明生成的序列与二级优化问题的最佳解集的距离 converge to zero。
</details></li>
</ul>
<hr>
<h2 id="A-Look-into-Causal-Effects-under-Entangled-Treatment-in-Graphs-Investigating-the-Impact-of-Contact-on-MRSA-Infection"><a href="#A-Look-into-Causal-Effects-under-Entangled-Treatment-in-Graphs-Investigating-the-Impact-of-Contact-on-MRSA-Infection" class="headerlink" title="A Look into Causal Effects under Entangled Treatment in Graphs: Investigating the Impact of Contact on MRSA Infection"></a>A Look into Causal Effects under Entangled Treatment in Graphs: Investigating the Impact of Contact on MRSA Infection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08237">http://arxiv.org/abs/2307.08237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Ma, Chen Chen, Anil Vullikanti, Ritwick Mishra, Gregory Madden, Daniel Borrajo, Jundong Li</li>
<li>for: The paper is written to study the problem of causal effect estimation with treatment entangled in a graph, and to propose a novel method (NEAT) to tackle this challenge.</li>
<li>methods: The proposed method NEAT explicitly leverages the graph structure to model the treatment assignment mechanism, and mitigates confounding biases based on the treatment assignment modeling.</li>
<li>results: The proposed method is validated through experiments on both synthetic datasets and a real-world MRSA dataset, and provides effective results in estimating causal effects with entangled treatments.<details>
<summary>Abstract</summary>
Methicillin-resistant Staphylococcus aureus (MRSA) is a type of bacteria resistant to certain antibiotics, making it difficult to prevent MRSA infections. Among decades of efforts to conquer infectious diseases caused by MRSA, many studies have been proposed to estimate the causal effects of close contact (treatment) on MRSA infection (outcome) from observational data. In this problem, the treatment assignment mechanism plays a key role as it determines the patterns of missing counterfactuals -- the fundamental challenge of causal effect estimation. Most existing observational studies for causal effect learning assume that the treatment is assigned individually for each unit. However, on many occasions, the treatments are pairwisely assigned for units that are connected in graphs, i.e., the treatments of different units are entangled. Neglecting the entangled treatments can impede the causal effect estimation. In this paper, we study the problem of causal effect estimation with treatment entangled in a graph. Despite a few explorations for entangled treatments, this problem still remains challenging due to the following challenges: (1) the entanglement brings difficulties in modeling and leveraging the unknown treatment assignment mechanism; (2) there may exist hidden confounders which lead to confounding biases in causal effect estimation; (3) the observational data is often time-varying. To tackle these challenges, we propose a novel method NEAT, which explicitly leverages the graph structure to model the treatment assignment mechanism, and mitigates confounding biases based on the treatment assignment modeling. We also extend our method into a dynamic setting to handle time-varying observational data. Experiments on both synthetic datasets and a real-world MRSA dataset validate the effectiveness of the proposed method, and provide insights for future applications.
</details>
<details>
<summary>摘要</summary>
MRSA（多剂肠炎杆菌）是一种抗药菌，它的感染难以预防。在抗生素耗用多年的尝试下，许多研究被提出来估计MRSA感染的 causal effect，从观察数据中获得。在这个问题中，治疗分配机制扮演着关键的角色，它确定了潜在的缺失对照数据的模式——基本挑战 causal effect 估计。大多数现有的观察数据研究假设每个单元都 individually 接受了治疗。然而，在许多情况下，治疗是在图表中连接的单元之间分配的，即不同单元的治疗是 entangled 的。忽略这些杂合的治疗可能会妨碍 causal effect 估计。在这篇文章中，我们研究了图表中的 causal effect 估计问题。虽然有一些对 entangled 治疗的探索，但这个问题仍然具有挑战，因为：（1）杂合带来了对 treatment assignment mechanism 的模型和利用的困难;（2）可能存在隐藏的假设因素，导致 causal effect 估计受到抵消的影响;（3）观察数据通常是时间变化的。为了解决这些挑战，我们提出了一种新方法 NEAT，它明确利用图表结构来模型治疗分配机制，并根据治疗分配模型来减少假设因素的影响。我们还将方法推广到动态设定，以处理时间变化的观察数据。在 synthetic 数据和一个实际MRSA数据上进行了实验，并证明了我们的方法的有效性，并提供了未来应用的参考。
</details></li>
</ul>
<hr>
<h2 id="HeroLT-Benchmarking-Heterogeneous-Long-Tailed-Learning"><a href="#HeroLT-Benchmarking-Heterogeneous-Long-Tailed-Learning" class="headerlink" title="HeroLT: Benchmarking Heterogeneous Long-Tailed Learning"></a>HeroLT: Benchmarking Heterogeneous Long-Tailed Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08235">http://arxiv.org/abs/2307.08235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ssskj/herolt">https://github.com/ssskj/herolt</a></li>
<li>paper_authors: Haohui Wang, Weijie Guan, Jianpeng Chen, Zi Wang, Dawei Zhou</li>
<li>for: 本研究旨在提供一个系统性的长尾学习视角，涵盖数据长尾性、域困难度和新任务多样性等三个纬度。</li>
<li>methods: 本研究开发了包括13种现状之最先进算法和6种评价指标的最全面的长尾学习 benchmark 名为 HeroLT，并在14个真实 benchmark 数据集上进行了264项实验。</li>
<li>results: 研究人员通过对 HeroLT  benchmark 进行了全面的实验和分析，并提出了一些有 Promise 的未来方向。<details>
<summary>Abstract</summary>
Long-tailed data distributions are prevalent in a variety of domains, including finance, e-commerce, biomedical science, and cyber security. In such scenarios, the performance of machine learning models is often dominated by the head categories, while the learning of tail categories is significantly inadequate. Given abundant studies conducted to alleviate the issue, this work aims to provide a systematic view of long-tailed learning with regard to three pivotal angles: (A1) the characterization of data long-tailedness, (A2) the data complexity of various domains, and (A3) the heterogeneity of emerging tasks. To achieve this, we develop the most comprehensive (to the best of our knowledge) long-tailed learning benchmark named HeroLT, which integrates 13 state-of-the-art algorithms and 6 evaluation metrics on 14 real-world benchmark datasets across 4 tasks from 3 domains. HeroLT with novel angles and extensive experiments (264 in total) enables researchers and practitioners to effectively and fairly evaluate newly proposed methods compared with existing baselines on varying types of datasets. Finally, we conclude by highlighting the significant applications of long-tailed learning and identifying several promising future directions. For accessibility and reproducibility, we open-source our benchmark HeroLT and corresponding results at https://github.com/SSSKJ/HeroLT.
</details>
<details>
<summary>摘要</summary>
长尾数据分布广泛存在多个领域，如金融、电商、生物医学和网络安全。在这些场景下，机器学习模型的性能frequently受到主要类别的影响，而tail categories的学习则是不足的。鉴于丰富的相关研究，本工作想要提供长尾学习的系统视图，涉及以下三个重要角度：（A1）数据长尾性的特征，（A2）各领域的数据复杂性，以及（A3）emerging task的多样性。为实现这一目标，我们开发了最 complet（到我们所知）的长尾学习 benchmarck named HeroLT，该benchmark integrate 13种state-of-the-art算法和6种评价指标在14个真实世界 benchmark数据集上。 HeroLT通过新的角度和广泛的实验（共264个），帮助研究者和实践者对新提出的方法进行有效和公平的评估，并与现有基准值进行比较。最后，我们 conclude by highlighting long-tailed learning的重要应用和未来发展的一些可能性。为便捷性和可重复性，我们在 GitHub 上公开了我们的 benchmark HeroLT 和相应的结果。
</details></li>
</ul>
<hr>
<h2 id="Learning-for-Counterfactual-Fairness-from-Observational-Data"><a href="#Learning-for-Counterfactual-Fairness-from-Observational-Data" class="headerlink" title="Learning for Counterfactual Fairness from Observational Data"></a>Learning for Counterfactual Fairness from Observational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08232">http://arxiv.org/abs/2307.08232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Ma, Ruocheng Guo, Aidong Zhang, Jundong Li</li>
<li>for: 避免机器学习模型具有对某些子群体的偏见（如种族、性别、年龄等），实现对所有 subgroup 的公正预测。</li>
<li>methods: Counterfactual fairness 是一种从 causal 角度定义的公正性观，通过比较每个个体在原始世界和在对敏感特征值进行修改后的世界中的预测，来衡量模型的公正性。但在实际应用中，通常无法获得准确的 causal 模型，因此直接使用这些模型可能会带来偏见。本文提出了一种新的框架 CLAIRE，通过对数据进行 counterfactual 数据扩展和一种对称约束来减轻敏感特征的偏见。</li>
<li>results:  experiments 表明，CLAIRE 在对实际数据进行预测时比其他方法更好，同时也能够保证对所有 subgroup 的公正预测。<details>
<summary>Abstract</summary>
Fairness-aware machine learning has attracted a surge of attention in many domains, such as online advertising, personalized recommendation, and social media analysis in web applications. Fairness-aware machine learning aims to eliminate biases of learning models against certain subgroups described by certain protected (sensitive) attributes such as race, gender, and age. Among many existing fairness notions, counterfactual fairness is a popular notion defined from a causal perspective. It measures the fairness of a predictor by comparing the prediction of each individual in the original world and that in the counterfactual worlds in which the value of the sensitive attribute is modified. A prerequisite for existing methods to achieve counterfactual fairness is the prior human knowledge of the causal model for the data. However, in real-world scenarios, the underlying causal model is often unknown, and acquiring such human knowledge could be very difficult. In these scenarios, it is risky to directly trust the causal models obtained from information sources with unknown reliability and even causal discovery methods, as incorrect causal models can consequently bring biases to the predictor and lead to unfair predictions. In this work, we address the problem of counterfactually fair prediction from observational data without given causal models by proposing a novel framework CLAIRE. Specifically, under certain general assumptions, CLAIRE effectively mitigates the biases from the sensitive attribute with a representation learning framework based on counterfactual data augmentation and an invariant penalty. Experiments conducted on both synthetic and real-world datasets validate the superiority of CLAIRE in both counterfactual fairness and prediction performance.
</details>
<details>
<summary>摘要</summary>
“对待公平机器学习在多个领域中引起了广泛关注，例如在网络广告、个人化推荐和社交媒体分析中的网络应用程序。对待公平机器学习的目标是删除机器学习模型对某些子群体（敏感特征）的偏袋，例如性别、年龄和种族。许多现有的公平定义中，Counterfactual fairness是一种受欢迎的定义，它从 causal 的角度定义了公平的定义。Counterfactual fairness 的定义是根据每个个体在原始世界中的预测和在替代世界中的预测来衡量模型的公平。现有的方法以前需要人类对敏感特征的 causal 模型有充分的知识。但在实际情况下，背景 causal 模型通常是未知的，获取这种人类知识可能是很困难的。在这些情况下，直接对这些信息来源不确定的 causal 模型进行信任可能是很危险的。在这个工作中，我们解决了从观察数据中进行 counterfactually 公平预测的问题，不需要人类对敏感特征的 causal 模型的知识。我们提出了一个名为 CLAIRE 的新框架，它在满足一些一般假设下，可以对敏感特征进行优化，并且使用 counterfactual 数据增强和不变 penalty 来减少偏袋。实验结果显示，CLAIRE 在 counterfactual 公平和预测性能方面具有优越性。”
</details></li>
</ul>
<hr>
<h2 id="Can-Euclidean-Symmetry-be-Leveraged-in-Reinforcement-Learning-and-Planning"><a href="#Can-Euclidean-Symmetry-be-Leveraged-in-Reinforcement-Learning-and-Planning" class="headerlink" title="Can Euclidean Symmetry be Leveraged in Reinforcement Learning and Planning?"></a>Can Euclidean Symmetry be Leveraged in Reinforcement Learning and Planning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08226">http://arxiv.org/abs/2307.08226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linfeng Zhao, Owen Howell, Jung Yeon Park, Xupeng Zhu, Robin Walters, Lawson L. S. Wong</li>
<li>for: 这个论文的目的是设计改进的学习算法，用于控制和规划任务，具有欧几何群同质性。</li>
<li>methods: 论文使用了一种统一优化算法，可以应用于离散和连续的 symmetry 问题，包括优化算法和样本生成算法。</li>
<li>results: 实验证明，通过具有欧几何群同质性的算法，可以更好地解决自然的控制问题。<details>
<summary>Abstract</summary>
In robotic tasks, changes in reference frames typically do not influence the underlying physical properties of the system, which has been known as invariance of physical laws.These changes, which preserve distance, encompass isometric transformations such as translations, rotations, and reflections, collectively known as the Euclidean group. In this work, we delve into the design of improved learning algorithms for reinforcement learning and planning tasks that possess Euclidean group symmetry. We put forth a theory on that unify prior work on discrete and continuous symmetry in reinforcement learning, planning, and optimal control. Algorithm side, we further extend the 2D path planning with value-based planning to continuous MDPs and propose a pipeline for constructing equivariant sampling-based planning algorithms. Our work is substantiated with empirical evidence and illustrated through examples that explain the benefits of equivariance to Euclidean symmetry in tackling natural control problems.
</details>
<details>
<summary>摘要</summary>
在机器人任务中，参照系统的变化通常不会影响系统的物理性质，这被称为不变性法律。这些变化包括同构射影、旋转和反射，合称为欧几何群。在这个工作中，我们深入探讨改进学习算法的设计，以便在奖励学习和规划任务中具有欧几何群的对称性。我们提出了对往年的绝对同构和连续同构在奖励学习、规划和最优控制中的统一理论。算法方面，我们进一步扩展了二维路径规划，并提出了一个管道的构建同构抽样计划算法。我们的工作得到了实验证明，并通过例子解释了在自然控制问题中如何通过对维持欧几何群的同构性来获得利益。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Framework-for-High-Quality-Code-Generation"><a href="#A-Lightweight-Framework-for-High-Quality-Code-Generation" class="headerlink" title="A Lightweight Framework for High-Quality Code Generation"></a>A Lightweight Framework for High-Quality Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08220">http://arxiv.org/abs/2307.08220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Latif Siddiq, Beatrice Casey, Joanna C. S. Santos</li>
<li>for: This paper aims to improve the quality and security of automatically generated source codes using transformer-based code generation models.</li>
<li>methods: The proposed framework, FRANC, includes a static filter and a quality-aware ranker to sort code snippets based on compilability and quality scores. Prompt engineering is also used to fix persistent quality issues.</li>
<li>results: FRANC improves the compilability of Java and Python code suggestions by 9% to 46% and 10% to 43%, respectively. The average improvement in NDCG@10 score is 0.0763, and the repairing techniques repair the highest 80% of prompts. The framework takes approximately 1.98 seconds for Java and 0.08 seconds for Python.<details>
<summary>Abstract</summary>
In recent years, the use of automated source code generation utilizing transformer-based generative models has expanded, and these models can generate functional code according to the requirements of the developers. However, recent research revealed that these automatically generated source codes can contain vulnerabilities and other quality issues. Despite researchers' and practitioners' attempts to enhance code generation models, retraining and fine-tuning large language models is time-consuming and resource-intensive. Thus, we describe FRANC, a lightweight framework for recommending more secure and high-quality source code derived from transformer-based code generation models. FRANC includes a static filter to make the generated code compilable with heuristics and a quality-aware ranker to sort the code snippets based on a quality score. Moreover, the framework uses prompt engineering to fix persistent quality issues. We evaluated the framework with five Python and Java code generation models and six prompt datasets, including a newly created one in this work (SOEval). The static filter improves 9% to 46% Java suggestions and 10% to 43% Python suggestions regarding compilability. The average improvement over the NDCG@10 score for the ranking system is 0.0763, and the repairing techniques repair the highest 80% of prompts. FRANC takes, on average, 1.98 seconds for Java; for Python, it takes 0.08 seconds.
</details>
<details>
<summary>摘要</summary>
近年来，使用自动生成源代码的使用者模型（transformer-based generative models）的使用已扩展。这些模型可以根据开发者的需求生成功能代码。然而，最新的研究发现，这些自动生成的代码可能含有漏洞和质量问题。尽管研究人员和实践者尝试了增强代码生成模型，但是重新训练和精度调整大型自然语言模型是时间consuming和资源占用。因此，我们描述了FRANC框架，它是一个轻量级的框架，可以为基于 transformer 的代码生成模型提供更安全和更高质量的源代码。FRANC 包括一个静态筛选器，使得生成的代码可以遵循规范和质量评分器，以根据代码片段的质量进行排序。此外，框架还使用 prompt 工程来修复持续存在的质量问题。我们对五种 Python 和 Java 代码生成模型，以及六个提示集进行评估。静态筛选器可以提高 Java 建议的可 compiling 率由 9% 到 46%，Python 建议的可 compiling 率由 10% 到 43%。具有 NDCG@10 指标的平均提升为 0.0763，并且修复技术可以修复最高 80% 的提示。FRANC 平均需要 1.98 秒钟 для Java，占用 0.08 秒钟 для Python。
</details></li>
</ul>
<hr>
<h2 id="Forward-Laplacian-A-New-Computational-Framework-for-Neural-Network-based-Variational-Monte-Carlo"><a href="#Forward-Laplacian-A-New-Computational-Framework-for-Neural-Network-based-Variational-Monte-Carlo" class="headerlink" title="Forward Laplacian: A New Computational Framework for Neural Network-based Variational Monte Carlo"></a>Forward Laplacian: A New Computational Framework for Neural Network-based Variational Monte Carlo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08214">http://arxiv.org/abs/2307.08214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruichen Li, Haotian Ye, Du Jiang, Xuelan Wen, Chuwei Wang, Zhe Li, Xiang Li, Di He, Ji Chen, Weiluo Ren, Liwei Wang</li>
<li>for: 能够扩展NN-VMC的应用范围到更大的系统，包括更多的原子、分子和化学反应。</li>
<li>methods: 使用了一种新的计算框架 named Forward Laplacian，通过高效的前进传播过程计算了神经网络中的 Laplacian，从而大幅提高了NN-VMC的计算效率。</li>
<li>results: 对于一系列的原子、分子和化学反应，NN-VMC通过Empirical数据示出了可以解决通用量子力学问题的潜力。<details>
<summary>Abstract</summary>
Neural network-based variational Monte Carlo (NN-VMC) has emerged as a promising cutting-edge technique of ab initio quantum chemistry. However, the high computational cost of existing approaches hinders their applications in realistic chemistry problems. Here, we report the development of a new NN-VMC method that achieves a remarkable speed-up by more than one order of magnitude, thereby greatly extending the applicability of NN-VMC to larger systems. Our key design is a novel computational framework named Forward Laplacian, which computes the Laplacian associated with neural networks, the bottleneck of NN-VMC, through an efficient forward propagation process. We then demonstrate that Forward Laplacian is not only versatile but also facilitates more developments of acceleration methods across various aspects, including optimization for sparse derivative matrix and efficient neural network design. Empirically, our approach enables NN-VMC to investigate a broader range of atoms, molecules and chemical reactions for the first time, providing valuable references to other ab initio methods. The results demonstrate a great potential in applying deep learning methods to solve general quantum mechanical problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Stealthy-Backdoor-Attacks-against-Speech-Recognition-via-Elements-of-Sound"><a href="#Towards-Stealthy-Backdoor-Attacks-against-Speech-Recognition-via-Elements-of-Sound" class="headerlink" title="Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound"></a>Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08208">http://arxiv.org/abs/2307.08208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hanbocai/badspeech_soe">https://github.com/hanbocai/badspeech_soe</a></li>
<li>paper_authors: Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Stefanos Koffas, Yiming Li</li>
<li>for: 这个论文的目的是研究潜在攻击者可以通过恶意投入到语音识别模型的训练过程中，使模型具有恶意预测行为的问题。</li>
<li>methods: 这篇论文使用了一些新的攻击方法，包括使用高频谱的尖声作为触发器，并将其与其他音频 clip 混合以实现更隐蔽的攻击。它们还使用了timbre特征来实现隐蔽的攻击。</li>
<li>results: 实验结果表明，这些攻击方法可以在不同的设定下（例如，all-to-one、all-to-all、干净标签、物理和多个攻击点设定）下实现高效的攻击。这些攻击方法也比较隐蔽，可以逃脱检测。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have been widely and successfully adopted and deployed in various applications of speech recognition. Recently, a few works revealed that these models are vulnerable to backdoor attacks, where the adversaries can implant malicious prediction behaviors into victim models by poisoning their training process. In this paper, we revisit poison-only backdoor attacks against speech recognition. We reveal that existing methods are not stealthy since their trigger patterns are perceptible to humans or machine detection. This limitation is mostly because their trigger patterns are simple noises or separable and distinctive clips. Motivated by these findings, we propose to exploit elements of sound ($e.g.$, pitch and timbre) to design more stealthy yet effective poison-only backdoor attacks. Specifically, we insert a short-duration high-pitched signal as the trigger and increase the pitch of remaining audio clips to `mask' it for designing stealthy pitch-based triggers. We manipulate timbre features of victim audios to design the stealthy timbre-based attack and design a voiceprint selection module to facilitate the multi-backdoor attack. Our attacks can generate more `natural' poisoned samples and therefore are more stealthy. Extensive experiments are conducted on benchmark datasets, which verify the effectiveness of our attacks under different settings ($e.g.$, all-to-one, all-to-all, clean-label, physical, and multi-backdoor settings) and their stealthiness. The code for reproducing main experiments are available at \url{https://github.com/HanboCai/BadSpeech_SoE}.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在语音识别应用中广泛采用和部署。近期一些研究表明，这些模型容易受到后门攻击，敌人可以通过恶意污染训练过程中植入Malicious prediction behaviors。在这篇文章中，我们再次研究了对语音识别的poison-only后门攻击。我们发现现有方法不够隐蔽，因为启发模式是人类或机器检测的。这是因为启发模式通常是简单的噪声或分离的和特征的音频clip。我们被这些发现 motivated，我们提议利用音频元素（如抑声和 timbre）设计更隐蔽又有效的poison-only后门攻击。我们插入短暂的高频声讯作为启发，并增加剩下的音频clip的抑声来mask它。我们操纵受害者音频的timbre特征来设计隐蔽的timbre-based攻击，并设计一个voiceprint选择模块来促进多个后门攻击。我们的攻击可以生成更自然的杂 poisoned samples，因此更隐蔽。我们在标准数据集上进行了广泛的实验，以验证我们的攻击在不同的设置（例如all-to-one、all-to-all、spot、physical和多个后门设置）下的效果和隐蔽性。代码可以在\url{https://github.com/HanboCai/BadSpeech_SoE}中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Quantum-Convolutional-Neural-Network-Approach-for-Object-Detection-and-Classification"><a href="#A-Quantum-Convolutional-Neural-Network-Approach-for-Object-Detection-and-Classification" class="headerlink" title="A Quantum Convolutional Neural Network Approach for Object Detection and Classification"></a>A Quantum Convolutional Neural Network Approach for Object Detection and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08204">http://arxiv.org/abs/2307.08204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gowri Namratha Meedinti, Kandukuri Sai Srirekha, Radhakrishnan Delhibabu</li>
<li>for: 这篇论文主要评估量子卷积神经网络（QCNN）的潜在能力，与经典卷积神经网络（CNN）和人工神经网络（ANN）模型进行比较。</li>
<li>methods: 本论文使用了量子计算方法，将数据存储在量子环境中，并应用了CNN结构来处理这些数据。</li>
<li>results: 分析结果表明，QCNNs在某些应用场景下可以超越经典CNN和ANN模型，both in terms of accuracy and efficiency。此外，QCNNs还可以处理更大的复杂性水平。<details>
<summary>Abstract</summary>
This paper presents a comprehensive evaluation of the potential of Quantum Convolutional Neural Networks (QCNNs) in comparison to classical Convolutional Neural Networks (CNNs) and Artificial / Classical Neural Network (ANN) models. With the increasing amount of data, utilizing computing methods like CNN in real-time has become challenging. QCNNs overcome this challenge by utilizing qubits to represent data in a quantum environment and applying CNN structures to quantum computers. The time and accuracy of QCNNs are compared with classical CNNs and ANN models under different conditions such as batch size and input size. The maximum complexity level that QCNNs can handle in terms of these parameters is also investigated. The analysis shows that QCNNs have the potential to outperform both classical CNNs and ANN models in terms of accuracy and efficiency for certain applications, demonstrating their promise as a powerful tool in the field of machine learning.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Here's the translation in Simplified Chinese:这篇论文对量子卷积神经网络（QCNN）与经典卷积神经网络（CNN）以及人工神经网络（ANN）模型进行了全面的评估。随着数据量不断增加，使用计算方法如CNN在实时中变得越来越困难。QCNNs利用量子粒子来表示数据，并在量子计算机上应用卷积结构，从而超越了经典模型的限制。在不同的批处理大小和输入大小条件下，QCNNs的时间和准确率与经典CNNs和ANN模型进行了比较。此外，QCNNs的最大复杂度水平也进行了调查。分析结果表明，QCNNs在某些应用中可以在准确率和效率方面超越经典模型，表明它们在机器学习领域是一种有力的工具。
</details></li>
</ul>
<hr>
<h2 id="Noise-removal-methods-on-ambulatory-EEG-A-Survey"><a href="#Noise-removal-methods-on-ambulatory-EEG-A-Survey" class="headerlink" title="Noise removal methods on ambulatory EEG: A Survey"></a>Noise removal methods on ambulatory EEG: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02437">http://arxiv.org/abs/2308.02437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarthak Johari, Gowri Namratha Meedinti, Radhakrishnan Delhibabu, Deepak Joshi</li>
<li>for: 本研究旨在实时处理患者短访EEG数据，以提高医疗干预的精确性和效率。</li>
<li>methods: 本研究使用了许多检测和移除噪声的技术，包括模式识别、机器学习、和信号处理等。</li>
<li>results: 本研究发现，不同条件下的EEG数据可以使用不同的检测和移除噪声技术，以提高医疗干预的精确性和效率。<details>
<summary>Abstract</summary>
Over many decades, research is being attempted for the removal of noise in the ambulatory EEG. In this respect, an enormous number of research papers is published for identification of noise removal, It is difficult to present a detailed review of all these literature. Therefore, in this paper, an attempt has been made to review the detection and removal of an noise. More than 100 research papers have been discussed to discern the techniques for detecting and removal the ambulatory EEG. Further, the literature survey shows that the pattern recognition required to detect ambulatory method, eye open and close, varies with different conditions of EEG datasets. This is mainly due to the fact that EEG detected under different conditions has different characteristics. This is, in turn, necessitates the identification of pattern recognition technique to effectively distinguish EEG noise data from a various condition of EEG data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HOPE-High-order-Polynomial-Expansion-of-Black-box-Neural-Networks"><a href="#HOPE-High-order-Polynomial-Expansion-of-Black-box-Neural-Networks" class="headerlink" title="HOPE: High-order Polynomial Expansion of Black-box Neural Networks"></a>HOPE: High-order Polynomial Expansion of Black-box Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08192">http://arxiv.org/abs/2307.08192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harrypotterxtx/hope">https://github.com/harrypotterxtx/hope</a></li>
<li>paper_authors: Tingxiong Xiao, Weihang Zhang, Yuxiao Cheng, Jinli Suo</li>
<li>for: 这篇论文旨在提供一种方法，使深度神经网络变得更加可解，以便在需要作出有理的决策的领域中应用。</li>
<li>methods: 这篇论文使用了高阶多项式扩展（High-order Polynomial Expansion，HOPE）方法，将神经网络拓展成高阶多项式的参考输入。特别是，authors derive了高阶DERIVATIVE规则 для复杂函数，并将其扩展到神经网络，以快速和准确地计算神经网络的高阶DERIVATIVE。</li>
<li>results: 数值分析表明，提案的方法具有高精度、低计算复杂度和良好的收敛性。此外，authors还用HOPE方法实现了深度学习中的功能发现、快速推理和特征选择等广泛应用。<details>
<summary>Abstract</summary>
Despite their remarkable performance, deep neural networks remain mostly ``black boxes'', suggesting inexplicability and hindering their wide applications in fields requiring making rational decisions. Here we introduce HOPE (High-order Polynomial Expansion), a method for expanding a network into a high-order Taylor polynomial on a reference input. Specifically, we derive the high-order derivative rule for composite functions and extend the rule to neural networks to obtain their high-order derivatives quickly and accurately. From these derivatives, we can then derive the Taylor polynomial of the neural network, which provides an explicit expression of the network's local interpretations. Numerical analysis confirms the high accuracy, low computational complexity, and good convergence of the proposed method. Moreover, we demonstrate HOPE's wide applications built on deep learning, including function discovery, fast inference, and feature selection. The code is available at https://github.com/HarryPotterXTX/HOPE.git.
</details>
<details>
<summary>摘要</summary>
尽管它们的表现很出色，深度神经网络仍然具有大量的“黑盒子”特性，这限制了它们在需要做合理决策的领域应用。我们在这里介绍HOPE（高阶多项式扩展）方法，它可以将神经网络扩展成参考输入的高阶多项式。我们 derivated高阶DERIVATIVE规则 для复杂函数，并将这个规则扩展到神经网络，从而快速和高精度地计算神经网络的高阶DERIVATIVE。基于这些DERIVATIVE，我们可以计算神经网络的泰勒多项式，从而获得神经网络的本地解释。数值分析表明HOPE的精度高、计算复杂度低，并且 converge 很好。此外，我们还证明HOPE在深度学习建立的各种应用中具有广泛的应用前景，包括函数发现、快速推理和特征选择。代码可以在https://github.com/HarryPotterXTX/HOPE.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Mini-Giants-“Small”-Language-Models-and-Open-Source-Win-Win"><a href="#Mini-Giants-“Small”-Language-Models-and-Open-Source-Win-Win" class="headerlink" title="Mini-Giants: “Small” Language Models and Open Source Win-Win"></a>Mini-Giants: “Small” Language Models and Open Source Win-Win</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08189">http://arxiv.org/abs/2307.08189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengping Zhou, Lezhi Li, Xinxi Chen, Andy Li</li>
<li>for: 这篇论文主要是为了讨论小语言模型的发展和应用。</li>
<li>methods: 论文使用了开源社区和小语言模型来实现技术、伦理和社会上的赢利。</li>
<li>results: 论文提出了小语言模型在实际应用场景中的需求和潜力，并进行了对小语言模型的比较研究和评估方法。<details>
<summary>Abstract</summary>
ChatGPT is phenomenal. However, it is prohibitively expensive to train and refine such giant models. Fortunately, small language models are flourishing and becoming more and more competent. We call them "mini-giants". We argue that open source community like Kaggle and mini-giants will win-win in many ways, technically, ethically and socially. In this article, we present a brief yet rich background, discuss how to attain small language models, present a comparative study of small language models and a brief discussion of evaluation methods, discuss the application scenarios where small language models are most needed in the real world, and conclude with discussion and outlook.
</details>
<details>
<summary>摘要</summary>
chatgpt是非常出色的，但是它的训练和精细化过程却非常昂贵。幸好，小语言模型在繁殖和成熔的过程中逐渐强大起来。我们称之为“小巨人”。我们认为开源社区如Kaggle和小巨人在技术、道德和社会各个方面都将取得胜利。在这篇文章中，我们会提供简短 yet rich的背景介绍，讲述如何获得小语言模型，进行小语言模型的比较研究， brief discussion of evaluation methods，介绍实际世界中小语言模型的应用场景，并结束 WITH discussion and outlook。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Investigation-of-Pre-trained-Model-Selection-for-Out-of-Distribution-Generalization-and-Calibration"><a href="#An-Empirical-Investigation-of-Pre-trained-Model-Selection-for-Out-of-Distribution-Generalization-and-Calibration" class="headerlink" title="An Empirical Investigation of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration"></a>An Empirical Investigation of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08187">http://arxiv.org/abs/2307.08187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiroki Naganuma, Ryuichiro Hataya</li>
<li>for: 提高out-of-distribution泛化性能和推理不确定性</li>
<li>methods:  investigate pre-trained model selection的影响，并比较不同的数据集和模型参数对性能指标的影响</li>
<li>results: 发现预训练模型选择对out-of-distribution泛化性能有显著影响，大型模型表现较好，但需要进一步研究memorization和真正的泛化之间的平衡。<details>
<summary>Abstract</summary>
In the realm of out-of-distribution generalization tasks, finetuning has risen as a key strategy. While the most focus has been on optimizing learning algorithms, our research highlights the influence of pre-trained model selection in finetuning on out-of-distribution performance and inference uncertainty. Balancing model size constraints of a single GPU, we examined the impact of varying pre-trained datasets and model parameters on performance metrics like accuracy and expected calibration error. Our findings underscore the significant influence of pre-trained model selection, showing marked performance improvements over algorithm choice. Larger models outperformed others, though the balance between memorization and true generalization merits further investigation. Ultimately, our research emphasizes the importance of pre-trained model selection for enhancing out-of-distribution generalization.
</details>
<details>
<summary>摘要</summary>
在异常分布泛化任务中， fine-tuning 已成为一项关键策略。而我们的研究表明，预训练模型选择在 fine-tuning 中对异常分布性能和推理不确定性产生了重要影响。我们在单个 GPU 的模型大小限制下对不同的预训练数据集和模型参数进行了研究，发现预训练模型选择对性能指标如准确率和预期抽象误差产生了显著的影响。大型模型表现更好，但是要找到Memorization 和真正的泛化之间的平衡仍然需要进一步的调查。最终，我们的研究强调了预训练模型选择对异常分布泛化的重要性。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Faithfulness-in-Chain-of-Thought-Reasoning"><a href="#Measuring-Faithfulness-in-Chain-of-Thought-Reasoning" class="headerlink" title="Measuring Faithfulness in Chain-of-Thought Reasoning"></a>Measuring Faithfulness in Chain-of-Thought Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13702">http://arxiv.org/abs/2307.13702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, Ethan Perez</li>
<li>for:  investigate how Chain-of-Thought (CoT) reasoning may be unfaithful in large language models (LLMs)</li>
<li>methods: examine how model predictions change when intervening on the CoT (e.g., adding mistakes or paraphrasing)</li>
<li>results: CoT’s performance boost is not due to added test-time compute or information encoded in the CoT’s phrasing, and models produce less faithful reasoning as they become larger and more capable.Here’s the full abstract in Simplified Chinese:for:  investigate how Chain-of-Thought (CoT) reasoning may be unfaithful in large language models (LLMs)methods: examine how model predictions change when intervening on the CoT (e.g., adding mistakes or paraphrasing)results: CoT’s performance boost is not due to added test-time compute or information encoded in the CoT’s phrasing, and models produce less faithful reasoning as they become larger and more capable.<details>
<summary>Abstract</summary>
Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)大型语言模型（LLM）在回答问题时，可以通过“链接思维”（CoT）的步骤式 reasoning 表现更好，但是未知模型的实际运算是否忠于 CoT 的解释。我们 investigate 假设 CoT 的不实际运算，通过对 CoT 进行干扰（例如添加错误或重新诠释）来探索。我们发现模型在不同任务上对 CoT 的conditioning 程度有很大的变化，有时将重点放在 CoT 上，有时则几乎忽略它。CoT 的性能提升不似乎来自 CoT 的额外测试计算或由特定表述 CoT 中的信息。当模型变得更大和更强大时，它们在大多数任务上显示出不忠的运算。总之，我们的结果表明，CoT 可以忠实，只要选择适当的模型大小和任务。
</details></li>
</ul>
<hr>
<h2 id="Question-Decomposition-Improves-the-Faithfulness-of-Model-Generated-Reasoning"><a href="#Question-Decomposition-Improves-the-Faithfulness-of-Model-Generated-Reasoning" class="headerlink" title="Question Decomposition Improves the Faithfulness of Model-Generated Reasoning"></a>Question Decomposition Improves the Faithfulness of Model-Generated Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11768">http://arxiv.org/abs/2307.11768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anthropics/decompositionfaithfulnesspaper">https://github.com/anthropics/decompositionfaithfulnesspaper</a></li>
<li>paper_authors: Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Sam McCandlish, Sheer El Showk, Tamera Lanham, Tim Maxwell, Venkatesa Chandrasekaran, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, Ethan Perez</li>
<li>for: 帮助验证大型自然语言模型（LLM）的正确性和安全性。</li>
<li>methods: 使用 decomposition-based methods，即将问题拆分成多个子问题，让模型在不同的上下文中答swers simpler subquestions，以提高模型生成的 reasoning 的准确性。</li>
<li>results: 研究表明，通过使用 decomposition-based methods，可以提高模型生成的 reasoning 的准确性，而不会 sacrifice  too much的性能。这些方法可以帮助我们更好地验证 LLM 的正确性和安全性。<details>
<summary>Abstract</summary>
As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.
</details>
<details>
<summary>摘要</summary>
To improve the faithfulness of CoT reasoning, we have developed methods that decompose questions into subquestions. This approach achieves strong performance on question-answering tasks and sometimes approaches the performance of CoT while improving the faithfulness of the model's stated reasoning on several recently proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we significantly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT.Our results show that it is possible to improve the faithfulness of model-generated reasoning. Continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Prediction-of-Peptide-Self-assembly-through-Sequential-and-Graphical-Encoding"><a href="#Efficient-Prediction-of-Peptide-Self-assembly-through-Sequential-and-Graphical-Encoding" class="headerlink" title="Efficient Prediction of Peptide Self-assembly through Sequential and Graphical Encoding"></a>Efficient Prediction of Peptide Self-assembly through Sequential and Graphical Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09169">http://arxiv.org/abs/2307.09169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Liu, Jiaqi Wang, Yun Luo, Shuang Zhao, Wenbin Li, Stan Z. Li</li>
<li>for: 这篇论文旨在探讨深度学习如何应用于蛋白质自组装预测，以提高预测精度。</li>
<li>methods: 本研究使用了现代深度学习模型，包括RNN、LSTM、Transformer和GCN、GAT、GraphSAGE等，进行了系统性的检查，以探讨蛋白质编码的影响。</li>
<li>results: 研究发现，Transformer模型是最有力的序列编码基于深度学习模型，可以预测蛋白质自组装的精度。此外，研究还发现了不同的蛋白质编码方法对预测精度的影响。<details>
<summary>Abstract</summary>
In recent years, there has been an explosion of research on the application of deep learning to the prediction of various peptide properties, due to the significant development and market potential of peptides. Molecular dynamics has enabled the efficient collection of large peptide datasets, providing reliable training data for deep learning. However, the lack of systematic analysis of the peptide encoding, which is essential for AI-assisted peptide-related tasks, makes it an urgent problem to be solved for the improvement of prediction accuracy. To address this issue, we first collect a high-quality, colossal simulation dataset of peptide self-assembly containing over 62,000 samples generated by coarse-grained molecular dynamics (CGMD). Then, we systematically investigate the effect of peptide encoding of amino acids into sequences and molecular graphs using state-of-the-art sequential (i.e., RNN, LSTM, and Transformer) and structural deep learning models (i.e., GCN, GAT, and GraphSAGE), on the accuracy of peptide self-assembly prediction, an essential physiochemical process prior to any peptide-related applications. Extensive benchmarking studies have proven Transformer to be the most powerful sequence-encoding-based deep learning model, pushing the limit of peptide self-assembly prediction to decapeptides. In summary, this work provides a comprehensive benchmark analysis of peptide encoding with advanced deep learning models, serving as a guide for a wide range of peptide-related predictions such as isoelectric points, hydration free energy, etc.
</details>
<details>
<summary>摘要</summary>
在最近几年，因为蛋白质的发展和市场潜力而增加了对深度学习应用于蛋白质性质预测的研究，特别是蛋白质自组合的预测。分子动力学技术为蛋白质自组合预测提供了可靠的训练数据，但是蛋白质编码问题的缺乏系统性分析，使得预测精度的提高成为了紧迫的问题。为解决这问题，我们首先收集了高质量的大型蛋白质自组合 simulated annealing 数据集，包含62,000个样本，并使用现状最佳的序列（如 RNN、LSTM 和 Transformer）和结构深度学习模型（如 GCN、GAT 和 GraphSAGE）进行系统性分析，以 investigate the effect of peptide encoding of amino acids into sequences and molecular graphs on the accuracy of peptide self-assembly prediction. 经过广泛的比较研究，我们发现Transformer是最强的序列编码基于深度学习模型，可以为蛋白质自组合预测提供最高的精度，并且可以推动蛋白质自组合预测至十个氨基酸。总之，这项研究提供了深度学习模型的全面性分析，可以作为蛋白质相关预测，如离子点、� hydration free energy 等的指南。
</details></li>
</ul>
<hr>
<h2 id="Multi-Objective-Optimization-of-Performance-and-Interpretability-of-Tabular-Supervised-Machine-Learning-Models"><a href="#Multi-Objective-Optimization-of-Performance-and-Interpretability-of-Tabular-Supervised-Machine-Learning-Models" class="headerlink" title="Multi-Objective Optimization of Performance and Interpretability of Tabular Supervised Machine Learning Models"></a>Multi-Objective Optimization of Performance and Interpretability of Tabular Supervised Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08175">http://arxiv.org/abs/2307.08175</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slds-lmu/paper_2023_eagga">https://github.com/slds-lmu/paper_2023_eagga</a></li>
<li>paper_authors: Lennart Schneider, Bernd Bischl, Janek Thomas</li>
<li>for: 提高超参数优化和解释性之间的协同优化，以提高表格数据上的预测性能和解释性。</li>
<li>methods: 利用多目标优化问题的方法，将超参数优化和解释性之间的质量考虑为一个单一的优化问题，并通过增加特征选择、交互和 monotonicity 约束来扩展学习算法的搜索空间。</li>
<li>results: 在 benchmark 实验中，提出了一种新的进化算法，可以高效地在扩展的搜索空间上进行优化，并在表格数据上提高了性能和解释性的模型。<details>
<summary>Abstract</summary>
We present a model-agnostic framework for jointly optimizing the predictive performance and interpretability of supervised machine learning models for tabular data. Interpretability is quantified via three measures: feature sparsity, interaction sparsity of features, and sparsity of non-monotone feature effects. By treating hyperparameter optimization of a machine learning algorithm as a multi-objective optimization problem, our framework allows for generating diverse models that trade off high performance and ease of interpretability in a single optimization run. Efficient optimization is achieved via augmentation of the search space of the learning algorithm by incorporating feature selection, interaction and monotonicity constraints into the hyperparameter search space. We demonstrate that the optimization problem effectively translates to finding the Pareto optimal set of groups of selected features that are allowed to interact in a model, along with finding their optimal monotonicity constraints and optimal hyperparameters of the learning algorithm itself. We then introduce a novel evolutionary algorithm that can operate efficiently on this augmented search space. In benchmark experiments, we show that our framework is capable of finding diverse models that are highly competitive or outperform state-of-the-art XGBoost or Explainable Boosting Machine models, both with respect to performance and interpretability.
</details>
<details>
<summary>摘要</summary>
我们提出了一个模型不偏向的框架，用于同时优化supervised机器学习模型的预测性能和可解性。可解性是通过三个度量来衡量：特征稀缺、特征之间的互动稀缺和非升序特征效应的稀缺。我们将hyperparameter优化问题定义为多目标优化问题，以便在单一优化运行中生成兼顾高性能和易于理解的模型。我们通过将特征选择、互动和升序约束添加到学习算法的搜索空间中来实现高效的优化。我们示出，优化问题实际上是找到允许互动的分组选择的Pareto优化集，以及这些分组中每个特征的最佳升序约束和学习算法的优化参数。然后，我们引入了一种新的进化算法，可以高效地在这个扩展的搜索空间上运行。在测试中，我们发现我们的框架能够找到高竞争力或超越当前XGBoost或Explainable Boosting Machine模型， both with respect to performance and interpretability。
</details></li>
</ul>
<hr>
<h2 id="Discovering-User-Types-Mapping-User-Traits-by-Task-Specific-Behaviors-in-Reinforcement-Learning"><a href="#Discovering-User-Types-Mapping-User-Traits-by-Task-Specific-Behaviors-in-Reinforcement-Learning" class="headerlink" title="Discovering User Types: Mapping User Traits by Task-Specific Behaviors in Reinforcement Learning"></a>Discovering User Types: Mapping User Traits by Task-Specific Behaviors in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08169">http://arxiv.org/abs/2307.08169</a></li>
<li>repo_url: None</li>
<li>paper_authors: L. L. Ankile, B. S. Ham, K. Mao, E. Shin, S. Swaroop, F. Doshi-Velez, W. Pan</li>
<li>for: 用于描述用户在帮助人类学习（RL）中的行为，并研究用户特征来减少 intervención设计中的时间。</li>
<li>methods: 使用RL代理表示用户，研究用户行为与特征之间的关系，并提出一种用于研究用户类型崩溃的直观工具。</li>
<li>results: 确认了不同实际环境中的用户类型崩溃是一样的，并将这一观察形式化为环境之间的等式关系。通过在同一等式类中转移 intervención设计，可以快速个性化 intervención。<details>
<summary>Abstract</summary>
When assisting human users in reinforcement learning (RL), we can represent users as RL agents and study key parameters, called \emph{user traits}, to inform intervention design. We study the relationship between user behaviors (policy classes) and user traits. Given an environment, we introduce an intuitive tool for studying the breakdown of "user types": broad sets of traits that result in the same behavior. We show that seemingly different real-world environments admit the same set of user types and formalize this observation as an equivalence relation defined on environments. By transferring intervention design between environments within the same equivalence class, we can help rapidly personalize interventions.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在帮助人类用户进行学习增强（RL）时，我们可以将用户表示为RL agent，并研究关键参数，即“用户特征”，以便设计干预。我们研究用户行为（策略类型）和用户特征之间的关系。给定环境，我们引入一种直观的工具来研究“用户类型”的分解：广泛的特征集合，导致同样的行为。我们发现不同的实际环境都可以拥有同一组用户类型，并将这一观察形式化为环境之间的等式关系。通过在同一等式类中传递干预设计，我们可以帮助快速个化干预。
</details></li>
</ul>
<hr>
<h2 id="Integer-Factorisation-Fermat-Machine-Learning-on-a-Classical-Computer"><a href="#Integer-Factorisation-Fermat-Machine-Learning-on-a-Classical-Computer" class="headerlink" title="Integer Factorisation, Fermat &amp; Machine Learning on a Classical Computer"></a>Integer Factorisation, Fermat &amp; Machine Learning on a Classical Computer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12290">http://arxiv.org/abs/2308.12290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Blake</li>
<li>for: 该论文提出了一种基于深度学习的整数分解算法。</li>
<li>methods: 该算法使用欧拉扩展的法拉第分解算法将整数分解问题转化为二分类问题，并使用大量的synthetic数据进行训练。</li>
<li>results: 该论文介绍了算法的实现和一些实验结果，并分析了实验的缺陷。它还呼吁其他研究人员复现、验证和改进这种方法，以确定其可scalability和实用性。<details>
<summary>Abstract</summary>
In this paper we describe a deep learning--based probabilistic algorithm for integer factorisation. We use Lawrence's extension of Fermat's factorisation algorithm to reduce the integer factorisation problem to a binary classification problem. To address the classification problem, based on the ease of generating large pseudo--random primes, a corpus of training data, as large as needed, is synthetically generated. We will introduce the algorithm, summarise some experiments, analyse where these experiments fall short, and finally put out a call to others to reproduce, verify and see if this approach can be improved to a point where it becomes a practical, scalable factorisation algorithm.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了一种深度学习基于概率算法的整数分解方法。我们使用劳伦斯扩展的费马分解算法将整数分解问题转化为二分类问题。为解决这个分类问题，我们使用大量生成的假随机 prime 数据集进行训练。我们将算法介绍、summarize一些实验结果、分析实验的缺陷，最后呼吁其他人重现、验证以及提高这种方法，以使其成为实用、可扩展的分解算法。
</details></li>
</ul>
<hr>
<h2 id="Feedback-is-All-You-Need-Real-World-Reinforcement-Learning-with-Approximate-Physics-Based-Models"><a href="#Feedback-is-All-You-Need-Real-World-Reinforcement-Learning-with-Approximate-Physics-Based-Models" class="headerlink" title="Feedback is All You Need: Real-World Reinforcement Learning with Approximate Physics-Based Models"></a>Feedback is All You Need: Real-World Reinforcement Learning with Approximate Physics-Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08168">http://arxiv.org/abs/2307.08168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Westenbroek, Jacob Levy, David Fridovich-Keil</li>
<li>for: 本研究旨在开发高效可靠的政策优化策略，用于机器学习实际数据上的 робоット学习。</li>
<li>methods: 本研究使用政策梯度方法，并系统地利用一个可能很简单的第一原理模型，以生成有限量的实际数据上的精确控制策略。</li>
<li>results: 本研究通过理论分析和硬件实验，证明了这种方法可以在几分钟的实际数据上学习精确控制策略，并且可以重新使用。<details>
<summary>Abstract</summary>
We focus on developing efficient and reliable policy optimization strategies for robot learning with real-world data. In recent years, policy gradient methods have emerged as a promising paradigm for training control policies in simulation. However, these approaches often remain too data inefficient or unreliable to train on real robotic hardware. In this paper we introduce a novel policy gradient-based policy optimization framework which systematically leverages a (possibly highly simplified) first-principles model and enables learning precise control policies with limited amounts of real-world data. Our approach $1)$ uses the derivatives of the model to produce sample-efficient estimates of the policy gradient and $2)$ uses the model to design a low-level tracking controller, which is embedded in the policy class. Theoretical analysis provides insight into how the presence of this feedback controller addresses overcomes key limitations of stand-alone policy gradient methods, while hardware experiments with a small car and quadruped demonstrate that our approach can learn precise control strategies reliably and with only minutes of real-world data.
</details>
<details>
<summary>摘要</summary>
我们主要专注于开发高效可靠的政策优化策略，以对现实世界数据进行机器学习。在最近几年中，政策势方法emerged as a promising paradigm for training control policies in simulation. However, these approaches often remain too data inefficient or unreliable to train on real robotic hardware. In this paper, we introduce a novel policy gradient-based policy optimization framework which systematically leverages a (possibly highly simplified) first-principles model and enables learning precise control policies with limited amounts of real-world data.Our approach $1)$ uses the derivatives of the model to produce sample-efficient estimates of the policy gradient and $2)$ uses the model to design a low-level tracking controller, which is embedded in the policy class. Theoretical analysis provides insight into how the presence of this feedback controller addresses overcomes key limitations of stand-alone policy gradient methods, while hardware experiments with a small car and quadruped demonstrate that our approach can learn precise control strategies reliably and with only minutes of real-world data.Here's the translation in Traditional Chinese:我们主要专注于开发高效可靠的政策优化策略，以对现实世界数据进行机器学习。在最近几年中，政策势方法emerged as a promising paradigm for training control policies in simulation. However, these approaches often remain too data inefficient or unreliable to train on real robotic hardware. In this paper, we introduce a novel policy gradient-based policy optimization framework which systematically leverages a (possibly highly simplified) first-principles model and enables learning precise control policies with limited amounts of real-world data.Our approach $1)$ uses the derivatives of the model to produce sample-efficient estimates of the policy gradient and $2)$ uses the model to design a low-level tracking controller, which is embedded in the policy class. Theoretical analysis provides insight into how the presence of this feedback controller addresses overcomes key limitations of stand-alone policy gradient methods, while hardware experiments with a small car and quadruped demonstrate that our approach can learn precise control strategies reliably and with only minutes of real-world data.
</details></li>
</ul>
<hr>
<h2 id="Computing-the-gradients-with-respect-to-all-parameters-of-a-quantum-neural-network-using-a-single-circuit"><a href="#Computing-the-gradients-with-respect-to-all-parameters-of-a-quantum-neural-network-using-a-single-circuit" class="headerlink" title="Computing the gradients with respect to all parameters of a quantum neural network using a single circuit"></a>Computing the gradients with respect to all parameters of a quantum neural network using a single circuit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08167">http://arxiv.org/abs/2307.08167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gphehub/grad2210">https://github.com/gphehub/grad2210</a></li>
<li>paper_authors: Guang Ping He</li>
<li>for: 计算量子神经网络的梯度时，需要计算两次Cost函数，一次是为了计算单个可调参数的梯度。当总参数数量很高时，量子电路需要调整和运行多次。</li>
<li>methods: 我们提出了一种方法，可以使用单个Circuit来计算所有梯度，减少Circuit的深度和классическийRegister的数量。</li>
<li>results: 我们实验表明，我们的方法可以在真实的量子硬件和模拟器上实现速度减少， compiling circuit takes significantly less time than conventional approach, resulting in a total runtime speedup.<details>
<summary>Abstract</summary>
When computing the gradients of a quantum neural network using the parameter-shift rule, the cost function needs to be calculated twice for the gradient with respect to a single adjustable parameter of the network. When the total number of parameters is high, the quantum circuit for the computation has to be adjusted and run for many times. Here we propose an approach to compute all the gradients using a single circuit only, with a much reduced circuit depth and less classical registers. We also demonstrate experimentally, on both real quantum hardware and simulator, that our approach has the advantages that the circuit takes a significantly shorter time to compile than the conventional approach, resulting in a speedup on the total runtime.
</details>
<details>
<summary>摘要</summary>
当计算量子神经网络中参数的梯度使用参数变化规则时，需要计算两次函数值以计算单个可变参数的梯度。当总参数数量较高时，量子电路需要调整并运行多次。我们提出了一种方法，可以通过单个电路计算所有梯度，减少电路深度和классический寄存器数量。我们还在实际中进行了实验，在真实的量子硬件和模拟器上证明了我们的方法可以减少电路编译时间，从而提高总时间的速度。Note: The word "quantum" is translated as "量子" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Neural-Stream-Functions"><a href="#Neural-Stream-Functions" class="headerlink" title="Neural Stream Functions"></a>Neural Stream Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08142">http://arxiv.org/abs/2307.08142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skywolf829/neuralstreamfunction">https://github.com/skywolf829/neuralstreamfunction</a></li>
<li>paper_authors: Skylar Wolfgang Wurster, Hanqi Guo, Tom Peterka, Han-Wei Shen</li>
<li>for: 这个论文是为了计算流函数的 neural network 方法，流函数是一种可以描述流体动态的scalar函数，其梯度与给定的vector field正交。</li>
<li>methods: 该论文使用了一种 implicit neural network 方法，将输入vector field作为输入，并使用内积loss函数来学习一个流函数。该网络可以将输入坐标映射到流函数值上，并且可以通过梯度内积来保证梯度的正交性。</li>
<li>results: 该论文的结果表明，使用这种方法可以生成高质量的流函数解，并且可以根据不同的regularizing loss函数来生成流函数解的不同版本。另外，论文还提供了一些关于如何正确visualize和提取artefact-free的流函数解的建议。<details>
<summary>Abstract</summary>
We present a neural network approach to compute stream functions, which are scalar functions with gradients orthogonal to a given vector field. As a result, isosurfaces of the stream function extract stream surfaces, which can be visualized to analyze flow features. Our approach takes a vector field as input and trains an implicit neural representation to learn a stream function for that vector field. The network learns to map input coordinates to a stream function value by minimizing the inner product of the gradient of the neural network's output and the vector field. Since stream function solutions may not be unique, we give optional constraints for the network to learn particular stream functions of interest. Specifically, we introduce regularizing loss functions that can optionally be used to generate stream function solutions whose stream surfaces follow the flow field's curvature, or that can learn a stream function that includes a stream surface passing through a seeding rake. We also discuss considerations for properly visualizing the trained implicit network and extracting artifact-free surfaces. We compare our results with other implicit solutions and present qualitative and quantitative results for several synthetic and simulated vector fields.
</details>
<details>
<summary>摘要</summary>
我们提出了一种神经网络方法来计算流函数，这些函数的梯度与给定的vector场垂直。因此，iso面流函数提取流面，可以用于分析流体特征。我们的方法通过输入vector场来训练一个隐式神经表示，以学习一个流函数。神经网络将输入坐标映射到流函数值上，通过神经网络输出的梯度和vector场的内积来进行折叠。由于流函数解可能不唯一，我们可以选择ally加入regularizing loss函数，以学习特定的流函数解。例如，我们可以添加一个束制约损失函数，使流函数解的流面与流体场的弯曲性相符，或者学习一个流函数解，其中流面通过种子托铁 passing through。我们还讨论了对训练完成后的隐式神经表示进行正确的visual化和提取 artifact-free的流面。我们与其他隐式解相比较，并对一些synthetic和simulated vector fields进行了质量和量化的结果。
</details></li>
</ul>
<hr>
<h2 id="DynamicFL-Balancing-Communication-Dynamics-and-Client-Manipulation-for-Federated-Learning"><a href="#DynamicFL-Balancing-Communication-Dynamics-and-Client-Manipulation-for-Federated-Learning" class="headerlink" title="DynamicFL: Balancing Communication Dynamics and Client Manipulation for Federated Learning"></a>DynamicFL: Balancing Communication Dynamics and Client Manipulation for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06267">http://arxiv.org/abs/2308.06267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bocheng Chen, Nikolay Ivanov, Guangjing Wang, Qiben Yan</li>
<li>for: 这篇论文的目的是提出一个新的联合学习（Federated Learning，FL）框架，以解决联合学习中的高系统多样性问题。</li>
<li>methods: 这篇论文使用了一个特殊的客户端操作策略，将客户端选择基于其网络预测和训练数据质量。此外，它还使用了一个长期追攻策略来解决网络动态环境中的性能下降问题。</li>
<li>results: 相比于现有的客户端选择方案，这篇论文的方法可以实现更好的模型准确性，并且只需要18.9%-84.0%的墙时钟时间。此外，论文还进行了分成和敏感性研究，以证明其在实际应用中的稳定性和可靠性。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning (ML) paradigm, aiming to train a global model by exploiting the decentralized data across millions of edge devices. Compared with centralized learning, FL preserves the clients' privacy by refraining from explicitly downloading their data. However, given the geo-distributed edge devices (e.g., mobile, car, train, or subway) with highly dynamic networks in the wild, aggregating all the model updates from those participating devices will result in inevitable long-tail delays in FL. This will significantly degrade the efficiency of the training process. To resolve the high system heterogeneity in time-sensitive FL scenarios, we propose a novel FL framework, DynamicFL, by considering the communication dynamics and data quality across massive edge devices with a specially designed client manipulation strategy. \ours actively selects clients for model updating based on the network prediction from its dynamic network conditions and the quality of its training data. Additionally, our long-term greedy strategy in client selection tackles the problem of system performance degradation caused by short-term scheduling in a dynamic network. Lastly, to balance the trade-off between client performance evaluation and client manipulation granularity, we dynamically adjust the length of the observation window in the training process to optimize the long-term system efficiency. Compared with the state-of-the-art client selection scheme in FL, \ours can achieve a better model accuracy while consuming only 18.9\% -- 84.0\% of the wall-clock time. Our component-wise and sensitivity studies further demonstrate the robustness of \ours under various real-life scenarios.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）是一种分布式机器学习（ML）模式，旨在透过分散在数百万副本设备（例如移动设备、车辆、火车、或地铁）上的分散数据，训练全球模型。相比中央学习，FL 保持客户端隐私，不直接下载客户端数据。然而，在野外的分散式边缘设备上，因为高度动态的网络环境，聚合所有模型更新从参与设备会带来不可预测的长尾延迟。这将严重损害训练过程的效率。为解决高度系统多样性的时间敏感FL情况下，我们提出了一个新的FL框架，即动态FL，通过考虑网络预测和训练数据质量，对于大量边缘设备进行特殊设计的客户端操作策略。我们在选择客户端进行模型更新时，会根据其网络条件预测和训练数据质量进行选择。此外，我们还使用了长期追击策略，以解决因为短期调度而导致的系统性能下降。最后，为寻求训练过程中的平衡，我们在训练过程中动态调整观察窗口的长度，以便最佳化系统效率。相比之前的客户端选择方案，我们的方案可以获得更好的模型精度，并且只需消耗18.9%-84.0%的壁网时间。我们的组件实验和敏感性研究显示了我们的方案在实际情况下的可持续性。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-graphs-model-spatial-relationships-between-biological-entities-for-breast-cancer-diagnosis"><a href="#Heterogeneous-graphs-model-spatial-relationships-between-biological-entities-for-breast-cancer-diagnosis" class="headerlink" title="Heterogeneous graphs model spatial relationships between biological entities for breast cancer diagnosis"></a>Heterogeneous graphs model spatial relationships between biological entities for breast cancer diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08132">http://arxiv.org/abs/2307.08132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akhila Krishna K, Ravi Kant Gupta, Nikhil Cherian Kurian, Pranav Jeevan, Amit Sethi</li>
<li>for: 这篇论文旨在提高乳癌早期检测、诊断和治疗选择的准确性，挑战乳癌病例的多样性。</li>
<li>methods: 这篇论文使用graph neural network（GNN）来捕捉乳癌病例中细胞和组织之间的空间关系，并将细胞和组织转换为网络结构，以提高对乳癌病例的检测和诊断。</li>
<li>results: 这篇论文的模型在三个公开可用的乳癌数据集上（BRIGHT、BreakHis和BACH）上显示出超过比较器-基于的现有方法的高准确性，并且显示出较低的参数数量和训练时间。<details>
<summary>Abstract</summary>
The heterogeneity of breast cancer presents considerable challenges for its early detection, prognosis, and treatment selection. Convolutional neural networks often neglect the spatial relationships within histopathological images, which can limit their accuracy. Graph neural networks (GNNs) offer a promising solution by coding the spatial relationships within images. Prior studies have investigated the modeling of histopathological images as cell and tissue graphs, but they have not fully tapped into the potential of extracting interrelationships between these biological entities. In this paper, we present a novel approach using a heterogeneous GNN that captures the spatial and hierarchical relations between cell and tissue graphs to enhance the extraction of useful information from histopathological images. We also compare the performance of a cross-attention-based network and a transformer architecture for modeling the intricate relationships within tissue and cell graphs. Our model demonstrates superior efficiency in terms of parameter count and achieves higher accuracy compared to the transformer-based state-of-the-art approach on three publicly available breast cancer datasets -- BRIGHT, BreakHis, and BACH.
</details>
<details>
<summary>摘要</summary>
breast cancer 的多样性呈现出较大的检测早期、诊断和治疗选择的挑战。 convolutional neural networks 经常忽略图像中的空间关系，这可能会限制其精度。 graph neural networks （GNNs）提供了一个有前途的解决方案，通过编码图像中的空间关系。先前的研究已经研究了模型 histopathological 图像为细胞和组织图像，但它们没有充分利用了提取这些生物体系间的关系的潜在。在本文中，我们提出了一种新的方法，使用多样性 GNN 捕捉图像中的空间和层次关系，以提高对 histopathological 图像的EXTRACT 有用信息。我们还对 cross-attention 网络和 transformer 架构进行比较，以模型图像中的复杂关系。我们的模型在三个公共可用的 breast cancer 数据集（BRIGHT、BreakHis 和 BACH）上达到了更高的准确率，并且在参数计数方面表现出了更高的效率，比较 transformer 基于 state-of-the-art 方法。
</details></li>
</ul>
<hr>
<h2 id="INFLECT-DGNN-Influencer-Prediction-with-Dynamic-Graph-Neural-Networks"><a href="#INFLECT-DGNN-Influencer-Prediction-with-Dynamic-Graph-Neural-Networks" class="headerlink" title="INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks"></a>INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08131">http://arxiv.org/abs/2307.08131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/banking-analytics-lab/inflect">https://github.com/banking-analytics-lab/inflect</a></li>
<li>paper_authors: Elena Tiukhova, Emiliano Penaloza, María Óskarsdóttir, Bart Baesens, Monique Snoeck, Cristián Bravo</li>
<li>for: 本研究旨在透过 integrate 动态图 neural network 和 recurrent neural network 等技术，提高 influencer 预测的准确性。</li>
<li>methods: 本研究提出了一种新的 INFLECT-DGNN 框架， combining 图 neural network 和 recurrent neural network ，并使用 weighted loss functions、Synthetic Minority Oversampling TEchnique (SMOTE) 和 rolling-window strategy。</li>
<li>results: 研究结果表明，使用 RNN 编码时间特征并与 GNN 结合使用，能够显著提高 influencer 预测的准确性。 compare various models， demonstrate  capture 图表示、时间依赖和使用财务驱动方法评价的重要性。<details>
<summary>Abstract</summary>
Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships. To elaborate this idea, we introduce INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural Networks (RNN) with weighted loss functions, the Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted rolling-window strategy. To evaluate predictive performance, we utilize a unique corporate data set with networks of three cities and derive a profit-driven evaluation methodology for influencer prediction. Our results show how using RNN to encode temporal attributes alongside GNNs significantly improves predictive performance. We compare the results of various models to demonstrate the importance of capturing graph representation, temporal dependencies, and using a profit-driven methodology for evaluation.
</details>
<details>
<summary>摘要</summary>
利用网络信息进行预测模型已经在多个领域广泛应用。在推荐和目标营销领域中，Influencer detection  stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships。为了开发这个想法，我们介绍了 INFLECT-DGNN，一个新的框架 для INFLuencer prEdiCTion with Dynamic Graph Neural Networks，该框架结合图 neural network (GNN) 和回归神经网络 (RNN)，并使用负权重函数、Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data，以及一种精心制定的滚动窗口策略。为了评估预测性能，我们使用了一个独特的企业数据集，并 derivated a profit-driven evaluation methodology for influencer prediction。我们的结果表明，使用 RNN 来编码时间特征 alongside GNNs 可以显著提高预测性能。我们对各种模型进行比较，以示出捕捉图表示、时间依赖和使用财务驱动的评估方法的重要性。
</details></li>
</ul>
<hr>
<h2 id="Tangent-Transformers-for-Composition-Privacy-and-Removal"><a href="#Tangent-Transformers-for-Composition-Privacy-and-Removal" class="headerlink" title="Tangent Transformers for Composition, Privacy and Removal"></a>Tangent Transformers for Composition, Privacy and Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08122">http://arxiv.org/abs/2307.08122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Yu Liu, Aditya Golatkar, Stefano Soatto</li>
<li>for: 这篇论文是为了提出一种名为 Tangent Attention Fine-Tuning (TAFT) 的方法，用于精度调整 linearized transformers。</li>
<li>methods: 这种方法使用计算 First-order Taylor Expansion 的方式来计算 Jacobian-Vector Product，从而在单个前进 pass 中计算训练和推理成本，与原始非线性网络相同的数目参数。</li>
<li>results: 当应用于不同的视觉分类任务时，使用 TAFT 精度调整 Tangent Transformer 可以与原始非线性网络精度调整相比肩，而且在同样的参数数量下。此外，TAFT 具有许多优点，例如模型组合、并行训练、机器忘却和差分隐私等。<details>
<summary>Abstract</summary>
We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy.
</details>
<details>
<summary>摘要</summary>
我们介绍 Tangent Attention Fine-Tuning（TAFT），一种精简 Linearized Transformers 的方法，通过计算首项泰利扩展来初始化预训练。我们证明了 Jacobian-Vector Product 的计算可以在单一前进中进行高效地进行，因此训练和测试成本与原始非线性网络相同的阶层，同时使用相同的参数数量。此外，我们显示了在不同的下游视觉分类任务中，使用 TAFT 精简 Tangent Transformer 的 fine-tuning 可以与原始非线性网络的 fine-tuning 相比。因为 Tangent Transformers 是对新的参数集线性的，并且 fine-tuning 的损失函数是凸函数，我们显示了 TAFT 在模型结构、平行训练、机器学习推广和数据隐私方面具有多个优点。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalisation-with-Bidirectional-Encoder-Representations-from-Vision-Transformers"><a href="#Domain-Generalisation-with-Bidirectional-Encoder-Representations-from-Vision-Transformers" class="headerlink" title="Domain Generalisation with Bidirectional Encoder Representations from Vision Transformers"></a>Domain Generalisation with Bidirectional Encoder Representations from Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08117">http://arxiv.org/abs/2307.08117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sw-packages/d23c4b6afa05094a23071333bd230aceceec08117355003f5c0ea958e60c9c98">https://github.com/sw-packages/d23c4b6afa05094a23071333bd230aceceec08117355003f5c0ea958e60c9c98</a></li>
<li>paper_authors: Hamza Riaz, Alan F. Smeaton</li>
<li>for: 这篇论文主要针对domain generalization问题，即将知识从源领域传递到未见领域，以实现深度学习模型的通用化。</li>
<li>methods: 本论文使用了vision transformer（ViT）、LeViT、DeiT和BEIT四种架构进行领域通用化，并在out-of-distribution（OOD）数据上进行初步评估。最终选择了BEIT架构进行进一步的实验。</li>
<li>results: 本论文的结果显示，使用BEIT架构进行领域通用化可以获得显著的提升，具体来说是在PACS、Home-Office和DomainNet三个benchmark上有着优秀的验证和测试准确率表现。此外，本论文的实现也能够填补在 Within-distribution和OOD数据之间的差距。<details>
<summary>Abstract</summary>
Domain generalisation involves pooling knowledge from source domain(s) into a single model that can generalise to unseen target domain(s). Recent research in domain generalisation has faced challenges when using deep learning models as they interact with data distributions which differ from those they are trained on. Here we perform domain generalisation on out-of-distribution (OOD) vision benchmarks using vision transformers. Initially we examine four vision transformer architectures namely ViT, LeViT, DeiT, and BEIT on out-of-distribution data. As the bidirectional encoder representation from image transformers (BEIT) architecture performs best, we use it in further experiments on three benchmarks PACS, Home-Office and DomainNet. Our results show significant improvements in validation and test accuracy and our implementation significantly overcomes gaps between within-distribution and OOD data.
</details>
<details>
<summary>摘要</summary>
域名总结是将多个源域的知识汇集到一个可以总结到未看到的目标域的模型中。近期在域名总结中使用深度学习模型时，面临了与训练数据分布不同的数据分布相互作用的挑战。我们在out-of-distribution（OOD）视觉审核中进行域名总结，初步分析了四种视觉变换器架构，即ViT、LeViT、DeiT和BEIT。其中， bidirectional encoder representation from image transformers（BEIT）架构表现最佳，因此我们在三个审核标准 benchmark（PACS、Home-Office和DomainNet）上进行了进一步的实验。我们的结果表明，使用BEIT架构可以在验证和测试精度上实现显著改进，并且我们的实现可以弥补在 dentro-distribution和OOD数据之间的差距。
</details></li>
</ul>
<hr>
<h2 id="Tangent-Model-Composition-for-Ensembling-and-Continual-Fine-tuning"><a href="#Tangent-Model-Composition-for-Ensembling-and-Continual-Fine-tuning" class="headerlink" title="Tangent Model Composition for Ensembling and Continual Fine-tuning"></a>Tangent Model Composition for Ensembling and Continual Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08114">http://arxiv.org/abs/2307.08114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Yu Liu, Stefano Soatto</li>
<li>for: 这种方法用于组合独立地练习过的模型，以实现逐步学习、集成、或忘记学习。</li>
<li>methods: 这种方法使用 Tangent Model Composition (TMC) 方法，该方法可以在推理时将组件模型相加、缩放或减去，以支持逐步学习、集成、或忘记学习。</li>
<li>results: TMC 方法可以提高精度，比采用非线性练习模型的集成方法高出4.2%，并且在推理成本的2.5倍至10倍的减少下，推理成本与单个模型相同。此外，TMC 方法可以免除额外成本，并且不会留下任何残留效应。<details>
<summary>Abstract</summary>
Tangent Model Composition (TMC) is a method to combine component models independently fine-tuned around a pre-trained point. Component models are tangent vectors to the pre-trained model that can be added, scaled, or subtracted to support incremental learning, ensembling, or unlearning. Component models are composed at inference time via scalar combination, reducing the cost of ensembling to that of a single model. TMC improves accuracy by 4.2% compared to ensembling non-linearly fine-tuned models at a 2.5x to 10x reduction of inference cost, growing linearly with the number of component models. Each component model can be forgotten at zero cost, with no residual effect on the resulting inference. When used for continual fine-tuning, TMC is not constrained by sequential bias and can be executed in parallel on federated data. TMC outperforms recently published continual fine-tuning methods almost uniformly on each setting -- task-incremental, class-incremental, and data-incremental -- on a total of 13 experiments across 3 benchmark datasets, despite not using any replay buffer. TMC is designed for composing models that are local to a pre-trained embedding, but could be extended to more general settings.
</details>
<details>
<summary>摘要</summary>
tangent模型组合（TMC）是一种方法，可以独立地微调component模型，然后在预训练点上组合。component模型是预训练模型的 tangent вектор，可以加、乘、减以支持逐步学习、集成或忘记学习。在推理时，component模型通过scalar组合来实现，因此推理成本只是一个模型的成本。TMC提高了精度4.2%，相比 Ensemble非线性微调模型，并且在推理成本的2.5倍至10倍之间减少了1.5倍。每个component模型可以忘记于零成本，无残留效果。在用于 continual fine-tuning 时，TMC不受顺序偏见的限制，可以在 federated data 上并行执行。TMC在任务逐步、类逐步和数据逐步的13个实验中，对 reciprocal fine-tuning 方法 almost uniformly 的性能优于其他方法，即使不使用 replay buffer。TMC是针对本地预训练 embedding 的模型组合方法，可以扩展到更广泛的设置。
</details></li>
</ul>
<hr>
<h2 id="Discovering-a-reaction-diffusion-model-for-Alzheimer’s-disease-by-combining-PINNs-with-symbolic-regression"><a href="#Discovering-a-reaction-diffusion-model-for-Alzheimer’s-disease-by-combining-PINNs-with-symbolic-regression" class="headerlink" title="Discovering a reaction-diffusion model for Alzheimer’s disease by combining PINNs with symbolic regression"></a>Discovering a reaction-diffusion model for Alzheimer’s disease by combining PINNs with symbolic regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08107">http://arxiv.org/abs/2307.08107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Zhang, Zongren Zou, Ellen Kuhl, George Em Karniadakis</li>
<li>for: 这些研究旨在描述阿尔ツ海默病的发展和病理过程中，蛋白质tau的折叠错误的角色。</li>
<li>methods: 这些研究使用深度学习和人工智能技术，以发现阿尔ツ海默病的数学模型。具体来说，他们使用物理学 Informed Neural Networks (PINNs) 和符号回归来发现tau蛋白质折叠错误的征化方程。</li>
<li>results: 这些研究发现，在46名可能发展阿尔ツ海默病的个体和30名健康控制群体的tau蛋白质扫描数据上，使用PINNs和符号回归可以发现不同的折叠模型，而且阿尔ツ海默病群体的折叠模型比健康控制群体快。这些结果表明，PINNs 和符号回归可以用于发现阿尔ツ海默病中tau蛋白质折叠错误的数学模型。<details>
<summary>Abstract</summary>
Misfolded tau proteins play a critical role in the progression and pathology of Alzheimer's disease. Recent studies suggest that the spatio-temporal pattern of misfolded tau follows a reaction-diffusion type equation. However, the precise mathematical model and parameters that characterize the progression of misfolded protein across the brain remain incompletely understood. Here, we use deep learning and artificial intelligence to discover a mathematical model for the progression of Alzheimer's disease using longitudinal tau positron emission tomography from the Alzheimer's Disease Neuroimaging Initiative database. Specifically, we integrate physics informed neural networks (PINNs) and symbolic regression to discover a reaction-diffusion type partial differential equation for tau protein misfolding and spreading. First, we demonstrate the potential of our model and parameter discovery on synthetic data. Then, we apply our method to discover the best model and parameters to explain tau imaging data from 46 individuals who are likely to develop Alzheimer's disease and 30 healthy controls. Our symbolic regression discovers different misfolding models $f(c)$ for two groups, with a faster misfolding for the Alzheimer's group, $f(c) = 0.23c^3 - 1.34c^2 + 1.11c$, than for the healthy control group, $f(c) = -c^3 +0.62c^2 + 0.39c$. Our results suggest that PINNs, supplemented by symbolic regression, can discover a reaction-diffusion type model to explain misfolded tau protein concentrations in Alzheimer's disease. We expect our study to be the starting point for a more holistic analysis to provide image-based technologies for early diagnosis, and ideally early treatment of neurodegeneration in Alzheimer's disease and possibly other misfolding-protein based neurodegenerative disorders.
</details>
<details>
<summary>摘要</summary>
互助蛋白质在阿尔茨海默病的发展和病理中扮演了关键角色。最新的研究表明，蛋白质的折叠发生 follows a reaction-diffusion type equation的特征。然而，正确的数学模型和参数，用于描述蛋白质的发展过程，仍然未得到完全理解。在这里，我们使用深度学习和人工智能，以发现阿尔茨海默病的数学模型。特别是，我们将物理学习神经网络（PINNs）和符号回归相结合，以找到蛋白质折叠的液态方程。首先，我们在 sintetic data 上验证了我们的模型和参数的潜力。然后，我们将我们的方法应用于46名可能发展阿尔茨海默病的个体和30名健康控制组的tau imaging数据中，以发现最佳的模型和参数，以解释蛋白质的折叠。我们的符号回归发现了两个组的不同的折叠模型，即 $f(c) = 0.23c^3 - 1.34c^2 + 1.11c$ 和 $f(c) = -c^3 + 0.62c^2 + 0.39c$。我们的结果表明，PINNs，补充符号回归，可以发现阿尔茨海默病中蛋白质折叠的液态方程。我们预计我们的研究将成为蛋白质折叠技术的开端，以提供早期诊断和治疗阿尔茨海默病和其他折叠蛋白质基因性神经退化疾病的技术。
</details></li>
</ul>
<hr>
<h2 id="Using-Decision-Trees-for-Interpretable-Supervised-Clustering"><a href="#Using-Decision-Trees-for-Interpretable-Supervised-Clustering" class="headerlink" title="Using Decision Trees for Interpretable Supervised Clustering"></a>Using Decision Trees for Interpretable Supervised Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08104">http://arxiv.org/abs/2307.08104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Natallia Kokash, Leonid Makhnist</li>
<li>for: 本研究探讨了对标注数据集中的分类数据进行可解释的分群问题，即interpretable supervised clustering。</li>
<li>methods: 本文提出了一种迭代方法，使用基于决策树的分类器作为最直观的学习方法，并讨论了节点选择方法以提高分群质量。</li>
<li>results: 本文获得了高密度分群，并通过描述分群的规则集来描述分群。<details>
<summary>Abstract</summary>
In this paper, we address an issue of finding explainable clusters of class-uniform data in labelled datasets. The issue falls into the domain of interpretable supervised clustering. Unlike traditional clustering, supervised clustering aims at forming clusters of labelled data with high probability densities. We are particularly interested in finding clusters of data of a given class and describing the clusters with the set of comprehensive rules. We propose an iterative method to extract high-density clusters with the help of decisiontree-based classifiers as the most intuitive learning method, and discuss the method of node selection to maximize quality of identified groups.
</details>
<details>
<summary>摘要</summary>
在本文中，我们讨论了一个标签数据集中找到可解释的封闭集的问题。这个问题属于可解释supervised clustering的领域。不同于传统封闭，supervised clustering寻求高概率密度的封闭，以便更好地描述数据。我们特别关注找到某个类型的数据的封闭，并使用设计树基于分类器来描述封闭。我们提出了一种迭代方法，使用决策树基于分类器来提取高密度封闭，并讨论了选择节点以提高寻索到的集的质量。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-max-affine-spline-approximation-of-neural-networks-using-the-Legendre-transform-of-a-convex-concave-representation"><a href="#A-max-affine-spline-approximation-of-neural-networks-using-the-Legendre-transform-of-a-convex-concave-representation" class="headerlink" title="A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation"></a>A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09602">http://arxiv.org/abs/2307.09602</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adamgoodtime/legendre_net">https://github.com/adamgoodtime/legendre_net</a></li>
<li>paper_authors: Adam Perrett, Danny Wood, Gavin Brown</li>
<li>for: 该 paper 的目的是提出一种将神经网络转换为spline表示的算法。</li>
<li>methods: 该算法不需要 convex 和 piecewise-affine 网络操作符，而是关注函数的 bounded-ness 和 second derivative 的定义性。</li>
<li>results: 该算法可以在整个网络中进行，而不仅仅是在每层独立进行。这种方法可以 bridge 神经网络和近似理论之间，同时允许抽象网络特征图。实验证明了该算法的正确性和效果。<details>
<summary>Abstract</summary>
This work presents a novel algorithm for transforming a neural network into a spline representation. Unlike previous work that required convex and piecewise-affine network operators to create a max-affine spline alternate form, this work relaxes this constraint. The only constraint is that the function be bounded and possess a well-define second derivative, although this was shown experimentally to not be strictly necessary. It can also be performed over the whole network rather than on each layer independently. As in previous work, this bridges the gap between neural networks and approximation theory but also enables the visualisation of network feature maps. Mathematical proof and experimental investigation of the technique is performed with approximation error and feature maps being extracted from a range of architectures, including convolutional neural networks.
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种新的算法，用于将神经网络转换成spline表示形式。与前一些研究不同，这个算法不需要几何和分割的网络运算符来创建一个最大 afine spline alternate form。它只需要函数是有界的，且具有定义的二阶导数，尽管实验表明这并不是必要的。此外，这个算法还可以在整个网络上进行，而不仅是每层独立进行。与以前的研究相似，这种技术将神经网络与近似理论相连接，同时允许网络特征地图的可视化。这个研究包括数学证明和实验调查，并从多种架构，包括卷积神经网络中提取了近似误差和特征地图。
</details></li>
</ul>
<hr>
<h2 id="EasyTPP-Towards-Open-Benchmarking-the-Temporal-Point-Processes"><a href="#EasyTPP-Towards-Open-Benchmarking-the-Temporal-Point-Processes" class="headerlink" title="EasyTPP: Towards Open Benchmarking the Temporal Point Processes"></a>EasyTPP: Towards Open Benchmarking the Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08097">http://arxiv.org/abs/2307.08097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ant-research/easytemporalpointprocess">https://github.com/ant-research/easytemporalpointprocess</a></li>
<li>paper_authors: Siqiao Xue, Xiaoming Shi, Zhixuan Chu, Yan Wang, Fan Zhou, Hongyan Hao, Caigao Jiang, Chen Pan, Yi Xu, James Y. Zhang, Qingsong Wen, Jun Zhou, Hongyuan Mei</li>
<li>for: This paper is written to establish a central benchmark for evaluating temporal point processes (TPPs) in order to promote reproducible research and accelerate progress in the field.</li>
<li>methods: The paper uses eight highly cited neural TPPs and integrates commonly used evaluation metrics and datasets into a standardized benchmarking pipeline. The benchmark is implemented in a universal framework that supports multiple machine learning libraries and custom implementations.</li>
<li>results: The paper presents a comprehensive implementation of TPPs and a standardized benchmarking pipeline for comparing different methods on different datasets, which can help promote reproducible research and accelerate progress in the field. The benchmark is open-sourced and available at a Github repository.<details>
<summary>Abstract</summary>
Continuous-time event sequences play a vital role in real-world domains such as healthcare, finance, online shopping, social networks, and so on. To model such data, temporal point processes (TPPs) have emerged as the most advanced generative models, making a significant impact in both academic and application communities. Despite the emergence of many powerful models in recent years, there is still no comprehensive benchmark to evaluate them. This lack of standardization impedes researchers and practitioners from comparing methods and reproducing results, potentially slowing down progress in this field. In this paper, we present EasyTPP, which aims to establish a central benchmark for evaluating TPPs. Compared to previous work that also contributed datasets, our EasyTPP has three unique contributions to the community: (i) a comprehensive implementation of eight highly cited neural TPPs with the integration of commonly used evaluation metrics and datasets; (ii) a standardized benchmarking pipeline for a transparent and thorough comparison of different methods on different datasets; (iii) a universal framework supporting multiple ML libraries (e.g., PyTorch and TensorFlow) as well as custom implementations. Our benchmark is open-sourced: all the data and implementation can be found at this \href{https://github.com/ant-research/EasyTemporalPointProcess}{\textcolor{blue}{Github repository}\footnote{\url{https://github.com/ant-research/EasyTemporalPointProcess}.}. We will actively maintain this benchmark and welcome contributions from other researchers and practitioners. Our benchmark will help promote reproducible research in this field, thus accelerating research progress as well as making more significant real-world impacts.
</details>
<details>
<summary>摘要</summary>
continuous-time event sequences在真实世界中的应用领域，如医疗、金融、在线购物、社交网络等，扮演着重要的角色。为模型这种数据，时间点过程（TPP）已经成为最先进的生成模型，在学术和应用社区中产生了深见的影响。尽管最近几年出现了许多强大的模型，但是还没有一个通用的标准准则来评估它们。这种标准化的缺失使得研究人员和实践者无法比较方法和重现结果，可能会抑制这个领域的进步。在这篇论文中，我们提出了EasyTPP，它的目标是建立TPP的中心评估标准。与之前的工作相比，EasyTPP有三个独特的贡献：（i）对八种最具影响力的神经网络TPP进行了完整的实现，并集成了通用的评估指标和数据集；（ii）提供了一个标准化的评估管道，使得不同的方法在不同的数据集上进行了公平的比较；（iii）支持多种Machine Learning库（如PyTorch和TensorFlow）以及自定义实现。我们的标准是开源的：所有的数据和实现可以在这个 \href{https://github.com/ant-research/EasyTemporalPointProcess}{\textcolor{blue}{Github repository} 中找到。我们将积极维护这个标准，并欢迎其他研究人员和实践者的贡献。我们的标准将助推可重复性的研究进步，从而加速这个领域的研究进步，并在真实世界中产生更 significative的影响。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-of-Forgetting-in-Deep-Learning-Beyond-Continual-Learning"><a href="#A-Comprehensive-Survey-of-Forgetting-in-Deep-Learning-Beyond-Continual-Learning" class="headerlink" title="A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning"></a>A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09218">http://arxiv.org/abs/2307.09218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ennengyang/awesome-forgetting-in-deep-learning">https://github.com/ennengyang/awesome-forgetting-in-deep-learning</a></li>
<li>paper_authors: Zhenyi Wang, Enneng Yang, Li Shen, Heng Huang</li>
<li>for: This paper is written to provide a comprehensive survey of forgetting in deep learning, beyond its conventional boundaries, and to explore the potential advantages of forgetting in certain cases, such as privacy-preserving scenarios.</li>
<li>methods: The paper uses a broad range of methods to examine forgetting in various research domains within deep learning, including generative models and federated learning. It also draws upon ideas and approaches from other fields that have dealt with forgetting.</li>
<li>results: The paper presents a nuanced understanding of forgetting as a double-edged sword, highlighting its potential advantages in certain cases, and provides a comprehensive list of papers about forgetting in various research fields. It encourages the development of novel strategies for mitigating, harnessing, or even embracing forgetting in real applications.Here’s the Chinese translation of the three key points:</li>
<li>for: 这篇论文是为了提供深度学习中忘记的全面检讨，超出传统的 bound，并 explore忘记在特定情况下的优点，如隐私保护场景。</li>
<li>methods: 论文使用了多种方法来检查深度学习中忘记的不同领域，包括生成器模型和联合学习。它还借鉴了其他领域对忘记的经验和方法。</li>
<li>results: 论文提供了忘记作为双刃剑的综合理解，并 highlight了忘记在特定情况下的优点。它还提供了关于忘记的广泛列表，并鼓励开发新的方法来 mitigate、利用或甚至欢迎忘记在实际应用中。<details>
<summary>Abstract</summary>
Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context, we aim to present a more nuanced understanding of this phenomenon and highlight its potential advantages. Through this comprehensive survey, we aspire to uncover potential solutions by drawing upon ideas and approaches from various fields that have dealt with forgetting. By examining forgetting beyond its conventional boundaries, in future work, we hope to encourage the development of novel strategies for mitigating, harnessing, or even embracing forgetting in real applications. A comprehensive list of papers about forgetting in various research fields is available at \url{https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning}.
</details>
<details>
<summary>摘要</summary>
忘卷（Forgetting）指的是在学习过程中失去或衰退已经获得的信息或知识。 existing surveys on forgetting 主要集中在持续学习领域，但是忘卷在深度学习中的研究领域中也是非常普遍的现象。忘卷在生成模型中的生成器变化和 federated learning 中的客户端数据分布不同而导致的现象。 Addressing 忘卷涉及到保持过去任务知识的 equilibrio 和快速学习新任务的挑战，以及处理任务干扰和矛盾目标的挑战。此外，现有的持续学习survey  implicit  assumes that forgetting is always harmful。相反，我们的survey  argue that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios。通过探讨忘卷在更广泛的上下文中，我们希望呈现一种更加细腻的理解，并高亮其潜在的优点。通过这种全面的survey，我们希望探讨可以从不同领域中的想法和方法中练习解决忘卷。在未来的工作中，我们希望通过探讨忘卷的不同方面，激发开发 novel strategies for mitigating, harnessing, or even embracing forgetting in real applications。一个完整的关于忘卷的paper的列表可以在 \url{https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning} 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/17/cs.LG_2023_07_17/" data-id="cloimipaa00l9s488b03k9mk7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/17/eess.IV_2023_07_17/" class="article-date">
  <time datetime="2023-07-17T09:00:00.000Z" itemprop="datePublished">2023-07-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/17/eess.IV_2023_07_17/">eess.IV - 2023-07-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Reconstructed-Convolution-Module-Based-Look-Up-Tables-for-Efficient-Image-Super-Resolution"><a href="#Reconstructed-Convolution-Module-Based-Look-Up-Tables-for-Efficient-Image-Super-Resolution" class="headerlink" title="Reconstructed Convolution Module Based Look-Up Tables for Efficient Image Super-Resolution"></a>Reconstructed Convolution Module Based Look-Up Tables for Efficient Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08544">http://arxiv.org/abs/2307.08544</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuguandu/rc-lut">https://github.com/liuguandu/rc-lut</a></li>
<li>paper_authors: Guandu Liu, Yukang Ding, Mading Li, Ming Sun, Xing Wen, Bin Wang</li>
<li>for: 提高单个图像超分解（SR）任务的效果</li>
<li>methods: 使用新型的重构卷积（RC）模块，它将通道和空间计算解耦，从而降低LUT大小并保持$n\times n$的辐射场</li>
<li>results: 与state-of-the-art LUT基elineSR方法相比，提出的RCLUT方法可以提高辐射场大小9倍，并在5个流行的benchmark数据集上达到优秀表现，同时可以作为LUT基elineSR方法的替换部件进行改进。<details>
<summary>Abstract</summary>
Look-up table(LUT)-based methods have shown the great efficacy in single image super-resolution (SR) task. However, previous methods ignore the essential reason of restricted receptive field (RF) size in LUT, which is caused by the interaction of space and channel features in vanilla convolution. They can only increase the RF at the cost of linearly increasing LUT size. To enlarge RF with contained LUT sizes, we propose a novel Reconstructed Convolution(RC) module, which decouples channel-wise and spatial calculation. It can be formulated as $n^2$ 1D LUTs to maintain $n\times n$ receptive field, which is obviously smaller than $n\times n$D LUT formulated before. The LUT generated by our RC module reaches less than 1/10000 storage compared with SR-LUT baseline. The proposed Reconstructed Convolution module based LUT method, termed as RCLUT, can enlarge the RF size by 9 times than the state-of-the-art LUT-based SR method and achieve superior performance on five popular benchmark dataset. Moreover, the efficient and robust RC module can be used as a plugin to improve other LUT-based SR methods. The code is available at https://github.com/liuguandu/RC-LUT.
</details>
<details>
<summary>摘要</summary>
look-up 表(LUT)-based 方法在单个图像超解像(SR) 任务中表现出色。然而，先前的方法忽视了 Look-up 表中 restricted 收发Field(RF) 的关键原因，这是因为混合空间和通道特征在 vanilla 核函数中所引起的。他们只能通过线性增加 LUT 大小来增加 RF。为了使 RF 增加而不是 LUT 大小线性增加，我们提议一种新的 Reconstructed Convolution(RC) 模块。这可以表示为 $n^2$ 1D LUT，以维护 $n\times n$ 收发Field。与之前的 $n\times n$D LUT 不同，这明显更小。我们的 RC 模块生成的 LUT 存储量低于 1/10000 比 SR-LUT 基eline。我们提议的 Reconstructed Convolution 模块基于 LUT 方法，称为 RCLUT，可以将 RF 尺寸提高至先前的 9 倍，并在五个流行的 benchmark 数据集上实现出色的性能。此外，我们的有效和可靠 RC 模块可以作为 LUT-based SR 方法的插件来改进其性能。代码可以在 https://github.com/liuguandu/RC-LUT 上获取。
</details></li>
</ul>
<hr>
<h2 id="Study-of-Vision-Transformers-for-Covid-19-Detection-from-Chest-X-rays"><a href="#Study-of-Vision-Transformers-for-Covid-19-Detection-from-Chest-X-rays" class="headerlink" title="Study of Vision Transformers for Covid-19 Detection from Chest X-rays"></a>Study of Vision Transformers for Covid-19 Detection from Chest X-rays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09402">http://arxiv.org/abs/2307.09402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandeep Angara, Sharath Thirunagaru</li>
<li>for: 这个研究旨在检测 COVID-19 病毒，使用视觉转换器进行检测，以提高检测效率和准确率。</li>
<li>methods: 本研究使用了许多现代的视觉转换器模型，包括 Vision Transformer (ViT)、Swin-transformer、Max vision transformer (MViT) 和 Pyramid Vision transformer (PVT)，通过转移学习IMAGENET 权重来实现高度的检测精度。</li>
<li>results: 实验结果显示，视觉转换器模型在 COVID-19 检测中达到了状态对的性能，即 98.75% 到 99.5% 的准确率，超过了传统方法和卷积神经网络（CNNs）的性能， highlighting the potential of Vision Transformers as a powerful tool for COVID-19 detection.<details>
<summary>Abstract</summary>
The COVID-19 pandemic has led to a global health crisis, highlighting the need for rapid and accurate virus detection. This research paper examines transfer learning with vision transformers for COVID-19 detection, known for its excellent performance in image recognition tasks. We leverage the capability of Vision Transformers to capture global context and learn complex patterns from chest X-ray images. In this work, we explored the recent state-of-art transformer models to detect Covid-19 using CXR images such as vision transformer (ViT), Swin-transformer, Max vision transformer (MViT), and Pyramid Vision transformer (PVT). Through the utilization of transfer learning with IMAGENET weights, the models achieved an impressive accuracy range of 98.75% to 99.5%. Our experiments demonstrate that Vision Transformers achieve state-of-the-art performance in COVID-19 detection, outperforming traditional methods and even Convolutional Neural Networks (CNNs). The results highlight the potential of Vision Transformers as a powerful tool for COVID-19 detection, with implications for improving the efficiency and accuracy of screening and diagnosis in clinical settings.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行导致全球卫生危机，高亮了快速和准确病毒检测的需求。这篇研究论文研究了通过视力变换器进行 COVID-19 检测，这种技术在图像识别任务中表现出色。我们利用视力变换器捕捉全局上下文和学习复杂的图像模式。在这项工作中，我们探索了最新的转换器模型，包括视力变换器（ViT）、Swin-transformer、Max视力变换器（MViT）和Pyramid视力变换器（PVT），以进行 COVID-19 检测使用 CXR 图像。通过使用转换学习IMAGENET 重量，模型实现了各种准确率范围为 98.75% 到 99.5%。我们的实验表明，视力变换器在 COVID-19 检测中实现了状态艺术表现，超越传统方法和卷积神经网络（CNNs）。结果表明，视力变换器是一种有力的工具，可以提高检测和诊断的效率和准确率。
</details></li>
</ul>
<hr>
<h2 id="EGE-UNet-an-Efficient-Group-Enhanced-UNet-for-skin-lesion-segmentation"><a href="#EGE-UNet-an-Efficient-Group-Enhanced-UNet-for-skin-lesion-segmentation" class="headerlink" title="EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation"></a>EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08473">http://arxiv.org/abs/2307.08473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jcruan519/ege-unet">https://github.com/jcruan519/ege-unet</a></li>
<li>paper_authors: Jiacheng Ruan, Mingye Xie, Jingsheng Gao, Ting Liu, Yuzhuo Fu</li>
<li>for: 这篇论文的目的是提出一个更有效率的医疗图像分类方法，以便应用于移动健康应用程序中。</li>
<li>methods: 这篇论文使用了一种名为Efficient Group Enhanced UNet（EGE-UNet）的方法，它将一种名为Group multi-axis Hadamard Product Attention（GHPA）和一种名为Group Aggregation Bridge（GAB）模组组合在一起，以提高分类精度和减少计算负载。</li>
<li>results: 根据实验结果，EGE-UNet比较 existed 的方法有着更好的分类性能，并且降低了参数和计算负载的比例，具体是494倍和160倍。此外，这是第一个参数数量只有50KB的模型。<details>
<summary>Abstract</summary>
Transformer and its variants have been widely used for medical image segmentation. However, the large number of parameter and computational load of these models make them unsuitable for mobile health applications. To address this issue, we propose a more efficient approach, the Efficient Group Enhanced UNet (EGE-UNet). We incorporate a Group multi-axis Hadamard Product Attention module (GHPA) and a Group Aggregation Bridge module (GAB) in a lightweight manner. The GHPA groups input features and performs Hadamard Product Attention mechanism (HPA) on different axes to extract pathological information from diverse perspectives. The GAB effectively fuses multi-scale information by grouping low-level features, high-level features, and a mask generated by the decoder at each stage. Comprehensive experiments on the ISIC2017 and ISIC2018 datasets demonstrate that EGE-UNet outperforms existing state-of-the-art methods. In short, compared to the TransFuse, our model achieves superior segmentation performance while reducing parameter and computation costs by 494x and 160x, respectively. Moreover, to our best knowledge, this is the first model with a parameter count limited to just 50KB. Our code is available at https://github.com/JCruan519/EGE-UNet.
</details>
<details>
<summary>摘要</summary>
“transformer和其 variants 在医疗影像 segmentation 方面广泛应用。然而，这些模型的参数数量和计算负担使得它们不适合移动医疗应用。为解决这个问题，我们提出了一种更高效的方法，efficient Group Enhanced UNet (EGE-UNet)。我们在轻量级的情况下嵌入了Group multi-axis Hadamard Product Attention module (GHPA)和Group Aggregation Bridge module (GAB)。GHPA 将输入特征分组并在不同轴上执行 Hadamard Product Attention 机制 (HPA)，以提取多个视角下的疾病信息。GAB 有效地将多尺度信息 fusion，通过分组低级特征、高级特征和解码器在每个阶段生成的掩码。经过了 ISIC2017 和 ISIC2018 数据集的广泛实验，我们的 EGE-UNet 超越了现有的状态态-of-the-art 方法。总之，相比于 TransFuse，我们的模型实现了更高效的 segmentation 性能，同时减少参数数量和计算成本，减少了 494 倍和 160 倍。此外，我们知道的是，这是第一个参数数量限制在 50KB 的模型。我们的代码可以在 <https://github.com/JCruan519/EGE-UNet> 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-using-Silver-Standard-Masks-for-Lateral-Ventricle-Segmentation-in-FLAIR-MRI"><a href="#Domain-Adaptation-using-Silver-Standard-Masks-for-Lateral-Ventricle-Segmentation-in-FLAIR-MRI" class="headerlink" title="Domain Adaptation using Silver Standard Masks for Lateral Ventricle Segmentation in FLAIR MRI"></a>Domain Adaptation using Silver Standard Masks for Lateral Ventricle Segmentation in FLAIR MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08456">http://arxiv.org/abs/2307.08456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Owen Crystal, Pejman J. Maralani, Sandra Black, Alan R. Moody, April Khademi</li>
<li>for: 这个研究旨在提出一种基于转移学习的左 Lateral ventricular volume (LVV) 分割方法，用于 Fluid-attenuated inversion recovery (FLAIR) MRI 图像。</li>
<li>methods: 这种方法使用了域 adaptation 技术，以便在目标领域中优化性能，并使用了一种新的传统图像处理 algorithm 生成了Silver standard (SS) mask。</li>
<li>results: 测试结果表明，使用 SS+GS 模型（在目标 SS Mask 和源 GS Mask 上进行练习和微调）在三个目标领域上得到了最好的和最稳定的性能（mean DSC &#x3D; 0.89，CoV &#x3D; 0.05），并与源 GS 模型在三个目标领域上显著 differently (p &lt; 0.05)。这些结果表明，在目标领域中生成的噪声标签可以帮助模型适应到 dataset-specific 特征，并提供了一个 robust 的参数初始化。<details>
<summary>Abstract</summary>
Lateral ventricular volume (LVV) is an important biomarker for clinical investigation. We present the first transfer learning-based LVV segmentation method for fluid-attenuated inversion recovery (FLAIR) MRI. To mitigate covariate shifts between source and target domains, this work proposes an domain adaptation method that optimizes performance on three target datasets. Silver standard (SS) masks were generated from the target domain using a novel conventional image processing ventricular segmentation algorithm and used to supplement the gold standard (GS) data from the source domain, Canadian Atherosclerosis Imaging Network (CAIN). Four models were tested on held-out test sets from four datasets: 1) SS+GS: trained on target SS masks and fine-tuned on source GS masks, 2) GS+SS: trained on source GS masks and fine-tuned on target SS masks, 3) trained on source GS (GS CAIN Only) and 4) trained on target SS masks (SS Only). The SS+GS model had the best and most consistent performance (mean DSC = 0.89, CoV = 0.05) and showed significantly (p < 0.05) higher DSC compared to the GS-only model on three target domains. Results suggest pre-training with noisy labels from the target domain allows the model to adapt to the dataset-specific characteristics and provides robust parameter initialization while fine-tuning with GS masks allows the model to learn detailed features. This method has wide application to other medical imaging problems where labeled data is scarce, and can be used as a per-dataset calibration method to accelerate wide-scale adoption.
</details>
<details>
<summary>摘要</summary>
lateral ventricular volume (LVV) 是一个重要的临床探索指标。本研究提出了首个将 Transfer Learning 应用于 fluid-attenuated inversion recovery (FLAIR) MRI 的 LVV 分割方法。为了减少对于源和目标领域之间的差异，本研究提出了一种领域适应方法，将目标领域中的 silver standard (SS) 标签转换为 gold standard (GS) 标签，并将其用于补充来自源领域的 GS 标签。本研究测试了四种模型，包括：1) SS+GS：使用目标领域中的 SS 标签进行 fine-tuning，并使用源领域中的 GS 标签进行训练；2) GS+SS：使用源领域中的 GS 标签进行 fine-tuning，并使用目标领域中的 SS 标签进行训练；3) 使用源领域中的 GS 标签进行训练（GS CAIN Only）；4) 使用目标领域中的 SS 标签进行训练（SS Only）。结果显示 SS+GS 模型的表现最佳和最稳定（平均 DSC = 0.89，CoV = 0.05），并与三个目标领域中的 GS-only 模型相比有 statistically significant 的差异（p < 0.05）。结果显示在目标领域中使用不精确的标签进行预训练可以让模型适应到dataset-specific的特性，并提供了Robust的初始化，而在 fine-tuning 过程中使用 GS 标签可以让模型学习到详细的特征。这种方法可以延伸到其他医学影像问题， где 标签是稀缺的，并且可以作为一种可靠的准备方法，帮助快速普遍推广。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Snake-Convolution-based-on-Topological-Geometric-Constraints-for-Tubular-Structure-Segmentation"><a href="#Dynamic-Snake-Convolution-based-on-Topological-Geometric-Constraints-for-Tubular-Structure-Segmentation" class="headerlink" title="Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation"></a>Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08388">http://arxiv.org/abs/2307.08388</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaoleiqi/dscnet">https://github.com/yaoleiqi/dscnet</a></li>
<li>paper_authors: Yaolei Qi, Yuting He, Xiaoming Qi, Yuan Zhang, Guanyu Yang</li>
<li>for: 提高 tubular 结构分割的准确性和效率，在各种领域 Ensure accuracy and efficiency in various fields.</li>
<li>methods: 使用动态蛇 convolution 精确捕捉 tubular 结构特征，并在多视图Feature fusion中补充注意力。 Propose a multi-view feature fusion strategy to complement attention to features from multiple perspectives during feature fusion.</li>
<li>results: 对 2D 和 3D 数据集进行实验，与其他方法比较，DSCNet 在 tubular 结构分割任务中提供更高的准确性和连续性。 Our experiments on 2D and 3D datasets show that our DSCNet provides better accuracy and continuity on the tubular structure segmentation task compared with several methods.<details>
<summary>Abstract</summary>
Accurate segmentation of topological tubular structures, such as blood vessels and roads, is crucial in various fields, ensuring accuracy and efficiency in downstream tasks. However, many factors complicate the task, including thin local structures and variable global morphologies. In this work, we note the specificity of tubular structures and use this knowledge to guide our DSCNet to simultaneously enhance perception in three stages: feature extraction, feature fusion, and loss constraint. First, we propose a dynamic snake convolution to accurately capture the features of tubular structures by adaptively focusing on slender and tortuous local structures. Subsequently, we propose a multi-view feature fusion strategy to complement the attention to features from multiple perspectives during feature fusion, ensuring the retention of important information from different global morphologies. Finally, a continuity constraint loss function, based on persistent homology, is proposed to constrain the topological continuity of the segmentation better. Experiments on 2D and 3D datasets show that our DSCNet provides better accuracy and continuity on the tubular structure segmentation task compared with several methods. Our codes will be publicly available.
</details>
<details>
<summary>摘要</summary>
精准分割 topological tubular 结构，如血管和道路，在各个领域是关键，以确保准确性和效率。然而，许多因素使得这个任务变得复杂，包括细小的地方结构和变化的全球形态。在这种情况下，我们注意到 tubular 结构的特殊性，并使用这些知识来引导我们的 DSCNet 在三个阶段中同时提高听见：特征提取、特征融合和损失约束。首先，我们提出了动态蛇 convolution，以准确地捕捉 tubular 结构的特征，并在细小和折衔的地方结构中进行适应性地调整。然后，我们提出了多视图特征融合策略，在特征融合时，从多个角度来补充对特征的注意力，以确保保留不同全球形态中的重要信息。最后，我们提出了基于 persistente homology 的连续性约束损失函数，以更好地限制分割结果的topological连续性。在 2D 和 3D 数据集上进行了实验，发现我们的 DSCNet 在 tubular 结构分割任务上提供了更高的准确性和连续性，比较多种方法。我们的代码将在公共可用。
</details></li>
</ul>
<hr>
<h2 id="Component-wise-Power-Estimation-of-Electrical-Devices-Using-Thermal-Imaging"><a href="#Component-wise-Power-Estimation-of-Electrical-Devices-Using-Thermal-Imaging" class="headerlink" title="Component-wise Power Estimation of Electrical Devices Using Thermal Imaging"></a>Component-wise Power Estimation of Electrical Devices Using Thermal Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08354">http://arxiv.org/abs/2307.08354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, Simon Grosche, Akarsh Bharadwaj, André Kaup</li>
<li>for: 这 paper 用于估计电子板子上不同活动组件的功率消耗。</li>
<li>methods: 该方法使用热成像技术，不需要特殊的高反射层。可以通过手动标注、物体检测方法或利用布局信息获得热图分割。</li>
<li>results: 评估结果显示，使用低分辨率消耗功率大于300mW的consumer infrared镜头，可以达到mean estimation error为10%。<details>
<summary>Abstract</summary>
This paper presents a novel method to estimate the power consumption of distinct active components on an electronic carrier board by using thermal imaging. The components and the board can be made of heterogeneous material such as plastic, coated microchips, and metal bonds or wires, where a special coating for high emissivity is not required. The thermal images are recorded when the components on the board are dissipating power. In order to enable reliable estimates, a segmentation of the thermal image must be available that can be obtained by manual labeling, object detection methods, or exploiting layout information. Evaluations show that with low-resolution consumer infrared cameras and dissipated powers larger than 300mW, mean estimation errors of 10% can be achieved.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的方法，用温存像来估算电子承载板上不同活动部件的能量消耗。这些部件和板可以由不同材料组成，如塑料、覆监微型逻辑器和金属带或电缆，而不需要特殊的高温透明层。温存像记录在部件上散热时，并进行了可靠的分割，可以通过手动标注、物体检测方法或利用布局信息来获得。评估结果显示，使用低分辨率消耗电频相机和大于300mW的散热功率，可以实现平均估算误差10%。
</details></li>
</ul>
<hr>
<h2 id="Neural-Modulation-Fields-for-Conditional-Cone-Beam-Neural-Tomography"><a href="#Neural-Modulation-Fields-for-Conditional-Cone-Beam-Neural-Tomography" class="headerlink" title="Neural Modulation Fields for Conditional Cone Beam Neural Tomography"></a>Neural Modulation Fields for Conditional Cone Beam Neural Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08351">http://arxiv.org/abs/2307.08351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/samuelepapa/cond-cbnt">https://github.com/samuelepapa/cond-cbnt</a></li>
<li>paper_authors: Samuele Papa, David M. Knigge, Riccardo Valperga, Nikita Moriakov, Miltos Kofinas, Jan-Jakob Sonke, Efstratios Gavves</li>
<li>for: 提高深度学习方法在 cone beam geometry computed tomography (CBCT) 重建中的精度和效率，使其能够在更复杂的CBCT重建中提供更好的结果。</li>
<li>methods: 基于神经场 (NF) 的方法，通过在输入空间中学习一个连续的神经网络来近似重建的密度。新提议的方法是通过使用每个扫描中的本地修饰来Conditioning Cone Beam Neural Tomography (CondCBNT)，使其能够更好地适应不同扫描数据中的变化。</li>
<li>results: CondCBNT 在不同数量的可用投射下对噪声数据和清晰数据都显示了改进的性能，比如使用单个CondCBNT模型可以在低投射数下达到高精度水平。<details>
<summary>Abstract</summary>
Conventional Computed Tomography (CT) methods require large numbers of noise-free projections for accurate density reconstructions, limiting their applicability to the more complex class of Cone Beam Geometry CT (CBCT) reconstruction. Recently, deep learning methods have been proposed to overcome these limitations, with methods based on neural fields (NF) showing strong performance, by approximating the reconstructed density through a continuous-in-space coordinate based neural network. Our focus is on improving such methods, however, unlike previous work, which requires training an NF from scratch for each new set of projections, we instead propose to leverage anatomical consistencies over different scans by training a single conditional NF on a dataset of projections. We propose a novel conditioning method where local modulations are modeled per patient as a field over the input domain through a Neural Modulation Field (NMF). The resulting Conditional Cone Beam Neural Tomography (CondCBNT) shows improved performance for both high and low numbers of available projections on noise-free and noisy data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficient-coding-of-360°-videos-exploiting-inactive-regions-in-projection-formats"><a href="#Efficient-coding-of-360°-videos-exploiting-inactive-regions-in-projection-formats" class="headerlink" title="Efficient coding of 360° videos exploiting inactive regions in projection formats"></a>Efficient coding of 360° videos exploiting inactive regions in projection formats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08344">http://arxiv.org/abs/2307.08344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, Mohammadreza Jamali, Stéphane Coulombe, Carlos Vazquez, Ahmad Vakili</li>
<li>for: 提高360度视频编码效率，即使在无法观看的区域忽略 pixels 值。</li>
<li>methods: 利用无效区域忽略 pixels 值，在重建Equirectangular格式或视口中减少损失。</li>
<li>results: 可以达到10%的比特率减少。<details>
<summary>Abstract</summary>
This paper presents an efficient method for encoding common projection formats in 360$^\circ$ video coding, in which we exploit inactive regions. These regions are ignored in the reconstruction of the equirectangular format or the viewport in virtual reality applications. As the content of these pixels is irrelevant, we neglect the corresponding pixel values in ratedistortion optimization, residual transformation, as well as inloop filtering and achieve bitrate savings of up to 10%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Power-Modeling-for-Virtual-Reality-Video-Playback-Applications"><a href="#Power-Modeling-for-Virtual-Reality-Video-Playback-Applications" class="headerlink" title="Power Modeling for Virtual Reality Video Playback Applications"></a>Power Modeling for Virtual Reality Video Playback Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08338">http://arxiv.org/abs/2307.08338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, Stéphane Coulombe, Ahmad Vakili, André Kaup</li>
<li>for: 评估和模型现代虚拟现实播放和流式应用程序在智能手机上的能 consumption。</li>
<li>methods: 通过进行功率测量，进一步构建一个用于估算真实能 consumption的模型，并且可以在关键电池水平下保存能源。</li>
<li>results: 结果显示，降低输入视频分辨率可以减少能 consumption。<details>
<summary>Abstract</summary>
This paper proposes a method to evaluate and model the power consumption of modern virtual reality playback and streaming applications on smartphones. Due to the high computational complexity of the virtual reality processing toolchain, the corresponding power consumption is very high, which reduces operating times of battery-powered devices. To tackle this problem, we analyze the power consumption in detail by performing power measurements. Furthermore, we construct a model to estimate the true power consumption with a mean error of less than 3.5%. The model can be used to save power at critical battery levels by changing the streaming video parameters. Particularly, the results show that the power consumption is significantly reduced by decreasing the input video resolution.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了一种方法来评估和模拟现代虚拟现实播放和流媒体应用程序在智能手机上的电力消耗。由于虚拟现实处理排序链的计算复杂性很高，相应的电力消耗很大，导致耗电器上的运行时间受限。为解决这个问题，我们进行了电力测量，并构建了一个估算真实电力消耗的模型，模型的误差低于3.5%。这个模型可以在关键的电池水平下保存能量，通过修改流媒体参数来降低输入视频分辨率。结果表明，降低输入视频分辨率可以减少电力消耗。
</details></li>
</ul>
<hr>
<h2 id="Power-Efficient-Video-Streaming-on-Mobile-Devices-Using-Optimal-Spatial-Scaling"><a href="#Power-Efficient-Video-Streaming-on-Mobile-Devices-Using-Optimal-Spatial-Scaling" class="headerlink" title="Power-Efficient Video Streaming on Mobile Devices Using Optimal Spatial Scaling"></a>Power-Efficient Video Streaming on Mobile Devices Using Optimal Spatial Scaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08337">http://arxiv.org/abs/2307.08337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, André Kaup, Stéphane Coulombe, Ahmad Vakili</li>
<li>for: 这个论文是为了实现功能强大的无线视频流媒体，以提高移动设备上的视频播放效率和能效性。</li>
<li>methods: 该论文使用了一种基于文献中的电源模型和主观质量评估指标，来 derive最佳的空间缩放和比特率控制参数。</li>
<li>results: 研究发现，可以通过调整输入视频的分辨率，以优化质量-能效性的交易。对于高清序列，可以保持10%的电源储备，而无损质量损失，或者保持15%的电源储备，而tolerable distortion。测试结果表明，该方法在Wi-Fi和移动网络中具有普遍适用性。<details>
<summary>Abstract</summary>
This paper derives optimal spatial scaling and rate control parameters for power-efficient wireless video streaming on portable devices. A video streaming application is studied, which receives a high-resolution and high-quality video stream from a remote server and displays the content to the end-user.We show that the resolution of the input video can be adjusted such that the quality-power trade-off is optimized. Making use of a power model from the literature and subjective quality evaluation using a perceptual metric, we derive optimal combinations of the scaling factor and the rate-control parameter for encoding. For HD sequences, up to 10% of power can be saved at negligible quality losses and up to 15% of power can be saved at tolerable distortions. To show general validity, the method was tested for Wi-Fi and a mobile network as well as for two different smartphones.
</details>
<details>
<summary>摘要</summary>
这篇论文研究了对移动设备进行功能强化的无线视频流式传输中的空间缩放和速率控制参数优化。一个视频流应用程序被研究，它从远程服务器接收高分辨率和高质量视频流，并将内容显示给终端用户。我们表明，可以根据输入视频的分辨率进行调整，以优化质量-功耗交易。使用文献中提供的电力模型和主观质量评价使用一种感知指标，我们得出了最佳的缩放因子和编码参数的组合。对高清序列，可以在不影响质量的情况下将电力减少10%，或者在可接受的损害下减少15%。为证明普适性，方法在Wi-Fi和移动网络以及两种不同的智能手机上进行了测试。
</details></li>
</ul>
<hr>
<h2 id="Combiner-and-HyperCombiner-Networks-Rules-to-Combine-Multimodality-MR-Images-for-Prostate-Cancer-Localisation"><a href="#Combiner-and-HyperCombiner-Networks-Rules-to-Combine-Multimodality-MR-Images-for-Prostate-Cancer-Localisation" class="headerlink" title="Combiner and HyperCombiner Networks: Rules to Combine Multimodality MR Images for Prostate Cancer Localisation"></a>Combiner and HyperCombiner Networks: Rules to Combine Multimodality MR Images for Prostate Cancer Localisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08279">http://arxiv.org/abs/2307.08279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Yan, Bernard Chiu, Ziyi Shen, Qianye Yang, Tom Syer, Zhe Min, Shonit Punwani, Mark Emberton, David Atkinson, Dean C. Barratt, Yipeng Hu</li>
<li>For: This paper aims to demonstrate the feasibility of using low-dimensional parametric models to model decision rules for radiologists’ reading of multiparametric prostate MR scans, and to improve the efficiency of automated radiologist labeling.* Methods: The proposed Combiner networks use a linear mixture model or a nonlinear stacking model to model PI-RADS decision rules, and train a single image segmentation network that can be conditioned on these hyperparameters during inference.* Results: Experimental results based on data from 850 patients show that the proposed combiner networks outperform other commonly-adopted end-to-end networks, and provide added advantages in obtaining and interpreting the modality combining rules. The paper also presents three clinical applications for prostate cancer segmentation, including modality availability assessment, importance quantification, and rule discovery.<details>
<summary>Abstract</summary>
One of the distinct characteristics in radiologists' reading of multiparametric prostate MR scans, using reporting systems such as PI-RADS v2.1, is to score individual types of MR modalities, T2-weighted, diffusion-weighted, and dynamic contrast-enhanced, and then combine these image-modality-specific scores using standardised decision rules to predict the likelihood of clinically significant cancer. This work aims to demonstrate that it is feasible for low-dimensional parametric models to model such decision rules in the proposed Combiner networks, without compromising the accuracy of predicting radiologic labels: First, it is shown that either a linear mixture model or a nonlinear stacking model is sufficient to model PI-RADS decision rules for localising prostate cancer. Second, parameters of these (generalised) linear models are proposed as hyperparameters, to weigh multiple networks that independently represent individual image modalities in the Combiner network training, as opposed to end-to-end modality ensemble. A HyperCombiner network is developed to train a single image segmentation network that can be conditioned on these hyperparameters during inference, for much improved efficiency. Experimental results based on data from 850 patients, for the application of automating radiologist labelling multi-parametric MR, compare the proposed combiner networks with other commonly-adopted end-to-end networks. Using the added advantages of obtaining and interpreting the modality combining rules, in terms of the linear weights or odds-ratios on individual image modalities, three clinical applications are presented for prostate cancer segmentation, including modality availability assessment, importance quantification and rule discovery.
</details>
<details>
<summary>摘要</summary>
一个 radiologists 在多 Parametric prostate MR 图像读取中的特征是将不同类型的 MR 模式分别评分，使用 PI-RADS v2.1 报告系统，并将这些图像模式特定的分数相互结合使用标准化的决策规则预测肉眼标签的可能性。本研究旨在证明可以使用低维度parametric模型来模型这些决策规则，无需妥协精度预测肉眼标签。首先，研究表明，线性混合模型或非线性堆叠模型都可以模型PI-RADS决策规则，用于本地化肉眼悬液肿瘤。其次，通过将这些（通用）线性模型的参数作为 гипер参数，可以将多个独立表示不同图像模式的网络在Combiner网络训练中进行权重调整，而不是END-TO-END模式ensemble。在这个HyperCombiner网络中，可以在推理时通过conditioning来控制这些参数，以提高效率。实验结果基于850名患者的数据，对于自动化肉眼标注多参量MR的应用，与其他常见的END-TO-END网络进行比较。通过获得和解释这些组合规则，即图像模式特定的线性权重或抽象比率，可以对肉眼标注进行多种优化和应用。例如，可以根据图像模式的可用性进行评估，或者根据图像模式的重要性进行量化，还可以通过发现新的规则来进行肉眼标注。
</details></li>
</ul>
<hr>
<h2 id="Liver-Tumor-Screening-and-Diagnosis-in-CT-with-Pixel-Lesion-Patient-Network"><a href="#Liver-Tumor-Screening-and-Diagnosis-in-CT-with-Pixel-Lesion-Patient-Network" class="headerlink" title="Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network"></a>Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08268">http://arxiv.org/abs/2307.08268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Yan, Xiaoli Yin, Yingda Xia, Fakai Wang, Shu Wang, Yuan Gao, Jiawen Yao, Chunli Li, Xiaoyu Bai, Jingren Zhou, Ling Zhang, Le Lu, Yu Shi</li>
<li>for: liver tumor segmentation and classification</li>
<li>methods: 使用mask transformer进行同时分割和类别 each lesion,以及image-wise classifier来Integrate global信息</li>
<li>results: 在非对照CT预处理任务中，PLAN achieved 95%和96%的患者级敏感性和特异性; 在对照CT任务中，我们的肿体分割精度、回卷率和类别精度分别达92%, 89%和86%,超过了广泛使用的CNN和transformers для肿体分割; 我们还对250个例进行了读者研究, PLAN的结果与一名高级人类放射学家一样，表明我们的结果具有临床意义。<details>
<summary>Abstract</summary>
Liver tumor segmentation and classification are important tasks in computer aided diagnosis. We aim to address three problems: liver tumor screening and preliminary diagnosis in non-contrast computed tomography (CT), and differential diagnosis in dynamic contrast-enhanced CT. A novel framework named Pixel-Lesion-pAtient Network (PLAN) is proposed. It uses a mask transformer to jointly segment and classify each lesion with improved anchor queries and a foreground-enhanced sampling loss. It also has an image-wise classifier to effectively aggregate global information and predict patient-level diagnosis. A large-scale multi-phase dataset is collected containing 939 tumor patients and 810 normal subjects. 4010 tumor instances of eight types are extensively annotated. On the non-contrast tumor screening task, PLAN achieves 95% and 96% in patient-level sensitivity and specificity. On contrast-enhanced CT, our lesion-level detection precision, recall, and classification accuracy are 92%, 89%, and 86%, outperforming widely used CNN and transformers for lesion segmentation. We also conduct a reader study on a holdout set of 250 cases. PLAN is on par with a senior human radiologist, showing the clinical significance of our results.
</details>
<details>
<summary>摘要</summary>
liver tumor分割和分类是计算机辅助诊断中的重要任务。我们想要解决三个问题：liver tumor在非对照计算机 Tomography（CT）中的检测和初步诊断，以及在动态对照CT中的分化诊断。我们提出了一个名为Pixel-Lesion-pAtient Network（PLAN）的框架。它使用一个面Mask transformer来同时分割和分类每个肿瘤，并使用改进的锚点查询和前景增强抽象损失来提高分割精度。它还有一个图像级别分类器，以有效地汇集全像信息并预测patient级诊断。我们收集了一个大规模多阶段数据集，包括939个肿瘤病人和810个正常Subject。4010个肿瘤实例中有八种类型进行了广泛的注释。在非对照肿瘤检测任务上，PLAN达到了95%和96%的patient级敏感性和特异性。在对照CT任务上，我们的肿瘤分割精度、检测精度和分类精度分别为92%, 89%和86%，超过了广泛使用的CNN和transformers для肿瘤分割。我们还进行了一个读者研究，其中PLAN与一名 senior human radiologist相当，表明了我们的结果的临床意义。
</details></li>
</ul>
<hr>
<h2 id="Extreme-Image-Compression-using-Fine-tuned-VQGAN-Models"><a href="#Extreme-Image-Compression-using-Fine-tuned-VQGAN-Models" class="headerlink" title="Extreme Image Compression using Fine-tuned VQGAN Models"></a>Extreme Image Compression using Fine-tuned VQGAN Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08265">http://arxiv.org/abs/2307.08265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Mao, Tinghan Yang, Yinuo Zhang, Shuyin Pan, Meng Wang, Shiqi Wang, Siwei Ma</li>
<li>for: 提高压缩数据的感知质量，特别是在低比特率下。</li>
<li>methods: 引入生成器模型（VQGAN），使用vector quantization（VQ）将图像表示为矢量编码。</li>
<li>results: 在EXTREMELY低比特率下（&lt;0.1 bpp），提高图像压缩后的感知质量，并且超过了现有的代码库。<details>
<summary>Abstract</summary>
Recent advances in generative compression methods have demonstrated remarkable progress in enhancing the perceptual quality of compressed data, especially in scenarios with low bitrates. Nevertheless, their efficacy and applicability in achieving extreme compression ratios ($<0.1$ bpp) still remain constrained. In this work, we propose a simple yet effective coding framework by introducing vector quantization (VQ)-based generative models into the image compression domain. The main insight is that the codebook learned by the VQGAN model yields strong expressive capacity, facilitating efficient compression of continuous information in the latent space while maintaining reconstruction quality. Specifically, an image can be represented as VQ-indices by finding the nearest codeword, which can be encoded using lossless compression methods into bitstreams. We then propose clustering a pre-trained large-scale codebook into smaller codebooks using the K-means algorithm. This enables images to be represented as diverse ranges of VQ-indices maps, resulting in variable bitrates and different levels of reconstruction quality. Extensive qualitative and quantitative experiments on various datasets demonstrate that the proposed framework outperforms the state-of-the-art codecs in terms of perceptual quality-oriented metrics and human perception under extremely low bitrates.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "generative compression methods" is translated as "生成压缩方法" (shēngchǎn zhùsuā fāngyì)* "perceptual quality" is translated as "感知质量" (gǎnzhì zhìliàng)* "codebook" is translated as "代码本" (dàimódian)* "VQGAN model" is translated as "VQGAN模型" (VQGAN módeli)* "K-means algorithm" is translated as "K-means算法" (K-means suānfǎ)* "variable bitrates" is translated as "变量比特率" (biànlvèng bǐtiéshù)* "different levels of reconstruction quality" is translated as "不同的重建质量" (bùdōng de zhòngjiàn zhìliàng)
</details></li>
</ul>
<hr>
<h2 id="Adaptively-Placed-Multi-Grid-Scene-Representation-Networks-for-Large-Scale-Data-Visualization"><a href="#Adaptively-Placed-Multi-Grid-Scene-Representation-Networks-for-Large-Scale-Data-Visualization" class="headerlink" title="Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization"></a>Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02494">http://arxiv.org/abs/2308.02494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skywolf829/apmgsrn">https://github.com/skywolf829/apmgsrn</a></li>
<li>paper_authors: Skylar Wolfgang Wurster, Tianyu Xiong, Han-Wei Shen, Hanqi Guo, Tom Peterka</li>
<li>for: 这 paper 是为了提高 scientific data 的压缩和可视化而提出的Scene Representation Networks (SRNs)。</li>
<li>methods: 该 paper 使用了适应放置的多重网格 SRN (APMGSRN) 和域 decomposit 训练和推理技术来加速多个 GPU 系统上的训练。</li>
<li>results: 该 paper 表明，使用 APMGSRN 可以提高 SRNs 的重建精度，而无需耗费贵重的 octree 细化、截割和搜索。 它还提供了一个开源的 neural volume rendering 应用程序，可以轻松地在任何 PyTorch-based SRN 上进行渲染。<details>
<summary>Abstract</summary>
Scene representation networks (SRNs) have been recently proposed for compression and visualization of scientific data. However, state-of-the-art SRNs do not adapt the allocation of available network parameters to the complex features found in scientific data, leading to a loss in reconstruction quality. We address this shortcoming with an adaptively placed multi-grid SRN (APMGSRN) and propose a domain decomposition training and inference technique for accelerated parallel training on multi-GPU systems. We also release an open-source neural volume rendering application that allows plug-and-play rendering with any PyTorch-based SRN. Our proposed APMGSRN architecture uses multiple spatially adaptive feature grids that learn where to be placed within the domain to dynamically allocate more neural network resources where error is high in the volume, improving state-of-the-art reconstruction accuracy of SRNs for scientific data without requiring expensive octree refining, pruning, and traversal like previous adaptive models. In our domain decomposition approach for representing large-scale data, we train an set of APMGSRNs in parallel on separate bricks of the volume to reduce training time while avoiding overhead necessary for an out-of-core solution for volumes too large to fit in GPU memory. After training, the lightweight SRNs are used for realtime neural volume rendering in our open-source renderer, where arbitrary view angles and transfer functions can be explored. A copy of this paper, all code, all models used in our experiments, and all supplemental materials and videos are available at https://github.com/skywolf829/APMGSRN.
</details>
<details>
<summary>摘要</summary>
Scene representation networks (SRNs) 有最近提出用于数据压缩和可视化的新方法。然而，当前的SRNs不会根据科学数据中复杂的特征进行分配可用的网络参数，导致重建质量下降。我们解决这个缺陷，通过适应地在多个网格上分布多个特性网络（APMGSRN），并提出基于多个GPU系统的域分解训练和执行技术。我们还发布了基于PyTorch的开源神经量化渲染应用，可以方便地在任意的PyTorch-based SRN上进行渲染。我们的APMGSRN架构使用多个空间自适应特征网格，以学习在域中的位置，以动态分配更多的神经网络资源，以提高SRNs的重建精度。在我们的域分解方法中，我们在不同的GPU系统上并行训练多个APMGSRN，以降低训练时间，而不需要昂贵的octree优化、剪辑和搜索。之后，我们使用轻量级的SRN进行实时神经量化渲染。一个包含这篇论文、所有代码、所有在我们实验中用到的模型、以及所有补充材料和视频的报告可以在https://github.com/skywolf829/APMGSRN中找到。
</details></li>
</ul>
<hr>
<h2 id="GastroVision-A-Multi-class-Endoscopy-Image-Dataset-for-Computer-Aided-Gastrointestinal-Disease-Detection"><a href="#GastroVision-A-Multi-class-Endoscopy-Image-Dataset-for-Computer-Aided-Gastrointestinal-Disease-Detection" class="headerlink" title="GastroVision: A Multi-class Endoscopy Image Dataset for Computer Aided Gastrointestinal Disease Detection"></a>GastroVision: A Multi-class Endoscopy Image Dataset for Computer Aided Gastrointestinal Disease Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08140">http://arxiv.org/abs/2307.08140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/debeshjha/gastrovision">https://github.com/debeshjha/gastrovision</a></li>
<li>paper_authors: Debesh Jha, Vanshali Sharma, Neethi Dasu, Nikhil Kumar Tomar, Steven Hicks, M. K. Bhuyan, Pradip K. Das, Michael A. Riegler, Pål Halvorsen, Ulas Bagci, Thomas de Lange<br>for:这个研究是为了解决融合实时人工智能（AI）系统在临床实践中的挑战，包括扩展和acceptability。methods:这个研究使用了多中心开放存取的胃肠综合镜像数据集（GastroVision），包括不同的解剖特征、病理异常、肿瘤移除 caso和正常找到（总共27个类别）的胃肠道。数据集包括来自挪威巴鲁姆医院和瑞典卡罗琳斯卡大学医院的8,000幅照片，并由经验轻肠综合医生进行标注和验证。results:我们 validate了我们的数据集的重要性，使用了广泛的benchmarking，基于受欢迎的深度学习基础模型。我们相信我们的数据集可以促进AI基于算法的胃肠疾病检测和分类的发展。我们的数据集可以在 \url{<a target="_blank" rel="noopener" href="https://osf.io/84e7f/%7D">https://osf.io/84e7f/}</a> 上获取。<details>
<summary>Abstract</summary>
Integrating real-time artificial intelligence (AI) systems in clinical practices faces challenges such as scalability and acceptance. These challenges include data availability, biased outcomes, data quality, lack of transparency, and underperformance on unseen datasets from different distributions. The scarcity of large-scale, precisely labeled, and diverse datasets are the major challenge for clinical integration. This scarcity is also due to the legal restrictions and extensive manual efforts required for accurate annotations from clinicians. To address these challenges, we present \textit{GastroVision}, a multi-center open-access gastrointestinal (GI) endoscopy dataset that includes different anatomical landmarks, pathological abnormalities, polyp removal cases and normal findings (a total of 27 classes) from the GI tract. The dataset comprises 8,000 images acquired from B{\ae}rum Hospital in Norway and Karolinska University Hospital in Sweden and was annotated and verified by experienced GI endoscopists. Furthermore, we validate the significance of our dataset with extensive benchmarking based on the popular deep learning based baseline models. We believe our dataset can facilitate the development of AI-based algorithms for GI disease detection and classification. Our dataset is available at \url{https://osf.io/84e7f/}.
</details>
<details>
<summary>摘要</summary>
临床应用人工智能（AI）系统整合面临挑战，包括可扩展性和接受性。这些挑战包括数据可用性、结果偏见、数据质量、透明度不足和不同分布下的性能下降。医疗数据的罕见性是临床整合的主要挑战之一，这也是由于法律限制和精度的手动准备所致。为解决这些挑战，我们介绍了《胃视》，一个多中心开放访问胃肠细胞图像数据集，包括胃肠脏器的不同解剖特征、疾病畸形、肿瘤除除例和正常发现（总共27个类）。该数据集包括8,000张从挪威布莱姆医院和瑞典卡罗琳斯卡大学医院所获取的图像，由经验丰富的胃肠镜头医生进行了标注和验证。此外，我们还验证了我们的数据集的重要性，通过基于深度学习的标准模型的比较。我们认为，我们的数据集可以促进基于AI的胃肠疾病检测和分类算法的发展。我们的数据集可以在 <https://osf.io/84e7f/> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Neural-Orientation-Distribution-Fields-for-Estimation-and-Uncertainty-Quantification-in-Diffusion-MRI"><a href="#Neural-Orientation-Distribution-Fields-for-Estimation-and-Uncertainty-Quantification-in-Diffusion-MRI" class="headerlink" title="Neural Orientation Distribution Fields for Estimation and Uncertainty Quantification in Diffusion MRI"></a>Neural Orientation Distribution Fields for Estimation and Uncertainty Quantification in Diffusion MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08138">http://arxiv.org/abs/2307.08138</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Consagra, Lipeng Ning, Yogesh Rathi</li>
<li>for: 这篇论文主要是用于描述一种新的深度学习方法，用于精确地估算 diffusion MRI（dMRI）信号中的方向分布函数（ODF）。</li>
<li>methods: 该方法使用神经网络（NF）来 parameterize一种随机列表表示的秘密 ODF 场，并通过显式地模型数据中的空间相关性结构，以提高精度和效率。</li>
<li>results: 对于 synthetic 和实际的 in-vivo diffusion数据，该方法与现有方法相比，具有更高的精度和更低的不确定性。<details>
<summary>Abstract</summary>
Inferring brain connectivity and structure \textit{in-vivo} requires accurate estimation of the orientation distribution function (ODF), which encodes key local tissue properties. However, estimating the ODF from diffusion MRI (dMRI) signals is a challenging inverse problem due to obstacles such as significant noise, high-dimensional parameter spaces, and sparse angular measurements. In this paper, we address these challenges by proposing a novel deep-learning based methodology for continuous estimation and uncertainty quantification of the spatially varying ODF field. We use a neural field (NF) to parameterize a random series representation of the latent ODFs, implicitly modeling the often ignored but valuable spatial correlation structures in the data, and thereby improving efficiency in sparse and noisy regimes. An analytic approximation to the posterior predictive distribution is derived which can be used to quantify the uncertainty in the ODF estimate at any spatial location, avoiding the need for expensive resampling-based approaches that are typically employed for this purpose. We present empirical evaluations on both synthetic and real in-vivo diffusion data, demonstrating the advantages of our method over existing approaches.
</details>
<details>
<summary>摘要</summary>
推断脑内连接和结构需要准确地估计Diffusion MRI（dMRI）信号中的方向分布函数（ODF），该函数包含脑组织重要的地方性特性。然而，从dMRI信号中估计ODF是一个困难的反向问题，因为存在干扰、高维度参数空间和缺乏方向测量的问题。在本文中，我们解决这些挑战，提出了一种基于深度学习的方法，用于连续地估计和评估空间变化的ODF场。我们使用神经场（NF）来参数化 latent ODFs 的随机列表表示，间接地模拟了通常被忽略的但有价值的空间相关结构，从而在稀缺和干扰的情况下提高效率。我们 Derive 一个analytic approximation to the posterior predictive distribution，可以用来评估 ODF 估计中任何空间位置的不确定性，避免使用常见的重新采样基本方法。我们在 synthetic 和实际的 in vivo  diffusion 数据上进行了实验，并证明了我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Untrained-neural-network-embedded-Fourier-phase-retrieval-from-few-measurements"><a href="#Untrained-neural-network-embedded-Fourier-phase-retrieval-from-few-measurements" class="headerlink" title="Untrained neural network embedded Fourier phase retrieval from few measurements"></a>Untrained neural network embedded Fourier phase retrieval from few measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08717">http://arxiv.org/abs/2307.08717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyuan-2000/trad">https://github.com/liyuan-2000/trad</a></li>
<li>paper_authors: Liyuan Ma, Hongxia Wang, Ningyi Leng, Ziyang Yuan</li>
<li>for: 这篇论文旨在解决快速执行 Fourier 频分析 (FPR) 问题，以减少时间和硬件成本。</li>
<li>methods: 该论文提出了一种基于 alternating direction method of multipliers (ADMM) 框架的无经验神经网络 (NN) 嵌入算法，用于解决 FPR 问题。</li>
<li>results: 实验结果表明，该算法在计算资源少的情况下表现更好，甚至可以与经过训练的神经网络 (NN) 算法竞争。<details>
<summary>Abstract</summary>
Fourier phase retrieval (FPR) is a challenging task widely used in various applications. It involves recovering an unknown signal from its Fourier phaseless measurements. FPR with few measurements is important for reducing time and hardware costs, but it suffers from serious ill-posedness. Recently, untrained neural networks have offered new approaches by introducing learned priors to alleviate the ill-posedness without requiring any external data. However, they may not be ideal for reconstructing fine details in images and can be computationally expensive. This paper proposes an untrained neural network (NN) embedded algorithm based on the alternating direction method of multipliers (ADMM) framework to solve FPR with few measurements. Specifically, we use a generative network to represent the image to be recovered, which confines the image to the space defined by the network structure. To improve the ability to represent high-frequency information, total variation (TV) regularization is imposed to facilitate the recovery of local structures in the image. Furthermore, to reduce the computational cost mainly caused by the parameter updates of the untrained NN, we develop an accelerated algorithm that adaptively trades off between explicit and implicit regularization. Experimental results indicate that the proposed algorithm outperforms existing untrained NN-based algorithms with fewer computational resources and even performs competitively against trained NN-based algorithms.
</details>
<details>
<summary>摘要</summary>
法ouveau频段恢复（FPR）是一项广泛应用的复杂任务，涉及于从傅里叶频域无法量测数据中恢复未知信号。FPR WITH few measurements是一项重要的应用，可以降低时间和硬件成本，但它受到严重的不定性困难。最近，无经过训练的神经网络（NN）已经提供了新的方法，通过引入学习的约束来缓解不定性，不需要任何外部数据。然而，它们可能无法完美地复制图像中的细节，并且可能具有高计算成本。这篇论文提出了一种无经过训练NN嵌入算法，基于 alternating direction method of multipliers（ADMM）框架来解决FPR WITH few measurements。特别是，我们使用一个生成网络来表示要恢复的图像，这使得图像受到生成网络的结构所限制。为了提高图像中高频信息的恢复，我们添加了总变量（TV）正则化，以便促进图像中的本地结构的恢复。此外，为了降低主要由无经过训练NN的参数更新所导致的计算成本，我们开发了一种可适应的加速算法，可以自适应地让拥有更多计算资源的计算机进行更多的计算。实验结果表明，我们的算法比现有的无经过训练NN基于算法更高效，甚至可以与经过训练NN基于算法竞争。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/17/eess.IV_2023_07_17/" data-id="cloimipgm0120s4881rkggln2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/16/cs.SD_2023_07_16/" class="article-date">
  <time datetime="2023-07-16T15:00:00.000Z" itemprop="datePublished">2023-07-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/16/cs.SD_2023_07_16/">cs.SD - 2023-07-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NoiseBandNet-Controllable-Time-Varying-Neural-Synthesis-of-Sound-Effects-Using-Filterbanks"><a href="#NoiseBandNet-Controllable-Time-Varying-Neural-Synthesis-of-Sound-Effects-Using-Filterbanks" class="headerlink" title="NoiseBandNet: Controllable Time-Varying Neural Synthesis of Sound Effects Using Filterbanks"></a>NoiseBandNet: Controllable Time-Varying Neural Synthesis of Sound Effects Using Filterbanks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08007">http://arxiv.org/abs/2307.08007</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adrianbarahona/noisebandnet">https://github.com/adrianbarahona/noisebandnet</a></li>
<li>paper_authors: Adrián Barahona-Ríos, Tom Collins</li>
<li>for: 本研究旨在提出一种可控制的神经音频合成方法，以便生成具有时间和频率分辨率的各种听起来不同的声音效果。</li>
<li>methods: 该方法使用滤波器链来过滤白噪，从而实现声音效果的生成和控制。</li>
<li>results: 对于十种声音效果的测试，NoiseBandNet得分高于四种变体的DDSP滤波器synthesizer，在九个评价类别中得分更高，表明NoiseBandNet可以生成具有时间和频率分辨率的各种听起来不同的声音效果。<details>
<summary>Abstract</summary>
Controllable neural audio synthesis of sound effects is a challenging task due to the potential scarcity and spectro-temporal variance of the data. Differentiable digital signal processing (DDSP) synthesisers have been successfully employed to model and control musical and harmonic signals using relatively limited data and computational resources. Here we propose NoiseBandNet, an architecture capable of synthesising and controlling sound effects by filtering white noise through a filterbank, thus going further than previous systems that make assumptions about the harmonic nature of sounds. We evaluate our approach via a series of experiments, modelling footsteps, thunderstorm, pottery, knocking, and metal sound effects. Comparing NoiseBandNet audio reconstruction capabilities to four variants of the DDSP-filtered noise synthesiser, NoiseBandNet scores higher in nine out of ten evaluation categories, establishing a flexible DDSP method for generating time-varying, inharmonic sound effects of arbitrary length with both good time and frequency resolution. Finally, we introduce some potential creative uses of NoiseBandNet, by generating variations, performing loudness transfer, and by training it on user-defined control curves.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified Chinese� controllable neural audio synthesis of sound effects is a challenging task due to the potential scarcity and spectro-temporal variance of the data. Differentiable digital signal processing (DDSP) synthesisers have been successfully employed to model and control musical and harmonic signals using relatively limited data and computational resources. Here we propose NoiseBandNet, an architecture capable of synthesising and controlling sound effects by filtering white noise through a filterbank, thus going further than previous systems that make assumptions about the harmonic nature of sounds. We evaluate our approach via a series of experiments, modelling footsteps, thunderstorm, pottery, knocking, and metal sound effects. Comparing NoiseBandNet audio reconstruction capabilities to four variants of the DDSP-filtered noise synthesiser, NoiseBandNet scores higher in nine out of ten evaluation categories, establishing a flexible DDSP method for generating time-varying, inharmonic sound effects of arbitrary length with both good time and frequency resolution. Finally, we introduce some potential creative uses of NoiseBandNet, by generating variations, performing loudness transfer, and by training it on user-defined control curves.Translation:控制可能的神经音频合成声效是一个挑战性的任务，因为声效数据的可能性和spectro-temporal variance很大。 diferenciable digital signal processing（DDSP）Synthesisers have been successfully employed to model and control musical and harmonic signals using relatively limited data and computational resources. 我们提议NoiseBandNet，一种可以通过filterbank filtering white noise来实现和控制声效的架构。这超过了之前的系统，它们假设声效的和谐性。我们通过一系列实验，模拟了踏步、雨天、陶艺、打击和金属声效。 Comparing NoiseBandNet的声音重建能力与四种DDSP滤波器处理的噪声合成器，NoiseBandNet在十个评价类别中得分高于其他四个， Establishing a flexible DDSP method for generating time-varying, inharmonic sound effects of arbitrary length with both good time and frequency resolution。最后，我们介绍了一些可能的创造性使用NoiseBandNet，如生成变化、卷积传递和用户定义的控制曲线。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/16/cs.SD_2023_07_16/" data-id="cloimipd400rws488cp3v3nqh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/16/eess.AS_2023_07_16/" class="article-date">
  <time datetime="2023-07-16T14:00:00.000Z" itemprop="datePublished">2023-07-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/16/eess.AS_2023_07_16/">eess.AS - 2023-07-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Model-Adaptation-for-ASR-in-low-resource-Indian-Languages"><a href="#Model-Adaptation-for-ASR-in-low-resource-Indian-Languages" class="headerlink" title="Model Adaptation for ASR in low-resource Indian Languages"></a>Model Adaptation for ASR in low-resource Indian Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07948">http://arxiv.org/abs/2307.07948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhayjeet Singh, Arjun Singh Mehta, Ashish Khuraishi K S, Deekshitha G, Gauri Date, Jai Nanavati, Jesuraja Bandekar, Karnalius Basumatary, Karthika P, Sandhya Badiger, Sathvik Udupa, Saurabh Kumar, Savitha, Prasanta Kumar Ghosh, Prashanthi V, Priyanka Pai, Raoul Nanavati, Rohan Saxena, Sai Praneeth Reddy Mora, Srinivasa Raghavan</li>
<li>for: The paper aims to improve automatic speech recognition (ASR) performance for low-resource languages, specifically Indian languages like Bengali and Bhojpuri.</li>
<li>methods: The paper uses self-supervised learning (SSL) based acoustic models like wav2vec2 and large-scale multi-lingual training like Whisper, and explores the use of adaptation and fine-tuning techniques to overcome the low-resource nature of the data.</li>
<li>results: The paper aims to understand the importance of each modality (acoustics and text) in building a reliable ASR system for low-resource languages, and to explore the applicability of these approaches to various languages spoken around the world.<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) performance has improved drastically in recent years, mainly enabled by self-supervised learning (SSL) based acoustic models such as wav2vec2 and large-scale multi-lingual training like Whisper. A huge challenge still exists for low-resource languages where the availability of both audio and text is limited. This is further complicated by the presence of multiple dialects like in Indian languages. However, many Indian languages can be grouped into the same families and share the same script and grammatical structure. This is where a lot of adaptation and fine-tuning techniques can be applied to overcome the low-resource nature of the data by utilising well-resourced similar languages.   In such scenarios, it is important to understand the extent to which each modality, like acoustics and text, is important in building a reliable ASR. It could be the case that an abundance of acoustic data in a language reduces the need for large text-only corpora. Or, due to the availability of various pretrained acoustic models, the vice-versa could also be true. In this proposed special session, we encourage the community to explore these ideas with the data in two low-resource Indian languages of Bengali and Bhojpuri. These approaches are not limited to Indian languages, the solutions are potentially applicable to various languages spoken around the world.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）性能在最近几年内有了惊人的提升，主要归功于基于自我超级学习（SSL）的声音模型，如wave2vec2以及大规模多语言训练如Whisper。然而，低资源语言仍然存在巨大的挑战，主要是因为语音和文本数据的可用性受限。这更加复杂，因为印度语言有多种方言。然而，许多印度语言可以分组，并且共享同一个字母和 grammatical structure。这使得可以应用大量的适应和精度调整技术来缓解低资源数据的问题，使用已有的资源更加有利。在这个特别会议中，我们邀请社区探讨以下想法：使用声音和文本Modalities 之间的关系来构建可靠的 ASR。可能是，一个语言有充足的声音数据，可以减少文本 corpora 的需求。或者，由于各种预训练声音模型的可用性，可以相反的情况。我们鼓励社区在孟买利语和帕雷语两种低资源印度语言中进行研究。这些方法不仅适用于印度语言，而且可能适用于世界各地的语言。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/16/eess.AS_2023_07_16/" data-id="cloimipfb00y7s4889v8gdrkz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/16/cs.CV_2023_07_16/" class="article-date">
  <time datetime="2023-07-16T13:00:00.000Z" itemprop="datePublished">2023-07-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/16/cs.CV_2023_07_16/">cs.CV - 2023-07-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Diffusion-to-Confusion-Naturalistic-Adversarial-Patch-Generation-Based-on-Diffusion-Model-for-Object-Detector"><a href="#Diffusion-to-Confusion-Naturalistic-Adversarial-Patch-Generation-Based-on-Diffusion-Model-for-Object-Detector" class="headerlink" title="Diffusion to Confusion: Naturalistic Adversarial Patch Generation Based on Diffusion Model for Object Detector"></a>Diffusion to Confusion: Naturalistic Adversarial Patch Generation Based on Diffusion Model for Object Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08076">http://arxiv.org/abs/2307.08076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo-Yen Lin, Ernie Chu, Che-Hsien Lin, Jun-Cheng Chen, Jia-Ching Wang</li>
<li>For: The paper aims to address the issue of poor-quality physical adversarial patches for protecting personal privacy from malicious monitoring using object detectors.* Methods: The proposed method uses diffusion models (DM) to generate naturalistic adversarial patches that are high-quality and stable, without suffering from mode collapse problems.* Results: The proposed approach achieves better-quality and more naturalistic adversarial patches than other state-of-the-art patch generation methods, with acceptable attack performance and various generation trade-offs under different conditions.Here are the three points in Simplified Chinese text:* For: 本文目标是解决对象检测器的恶意监测中的个人隐私泄露问题，通过生成高质量的物理攻击质 patches。* Methods: 提议方法基于扩散模型（DM），通过采样预训练于自然图像的DM模型，可以稳定地生成高质量和自然的物理攻击质 patches。* Results: 对比其他状态公共的攻击质 patch生成方法，提议方法可以实现更好的攻击性和质量，同时提供不同条件下的生成质量负担。<details>
<summary>Abstract</summary>
Many physical adversarial patch generation methods are widely proposed to protect personal privacy from malicious monitoring using object detectors. However, they usually fail to generate satisfactory patch images in terms of both stealthiness and attack performance without making huge efforts on careful hyperparameter tuning. To address this issue, we propose a novel naturalistic adversarial patch generation method based on the diffusion models (DM). Through sampling the optimal image from the DM model pretrained upon natural images, it allows us to stably craft high-quality and naturalistic physical adversarial patches to humans without suffering from serious mode collapse problems as other deep generative models. To the best of our knowledge, we are the first to propose DM-based naturalistic adversarial patch generation for object detectors. With extensive quantitative, qualitative, and subjective experiments, the results demonstrate the effectiveness of the proposed approach to generate better-quality and more naturalistic adversarial patches while achieving acceptable attack performance than other state-of-the-art patch generation methods. We also show various generation trade-offs under different conditions.
</details>
<details>
<summary>摘要</summary>
多种物理抗击方法已经广泛提出来保护个人隐私免受恶势力监测器的攻击。然而，这些方法通常无法生成满意的质量的抗击图像，而且需要大量的优化参数调整，以达到良好的攻击性和隐蔽性。为解决这个问题，我们提出了一种新的自然化抗击方法，基于扩散模型（DM）。通过从DM模型预训练于自然图像中采样最佳图像，我们可以稳定地生成高质量和自然化的物理抗击图像，而不会受到深度生成模型的严重混合问题的影响。我们认为我们是第一个提出DM模型基于的自然化抗击图像生成方法。通过了广泛的量化、质量和主观实验，我们的方法可以生成更高质量和更自然化的抗击图像，同时保持可接受的攻击性。我们还展示了不同条件下的生成质量的交易。
</details></li>
</ul>
<hr>
<h2 id="Dense-Multitask-Learning-to-Reconfigure-Comics"><a href="#Dense-Multitask-Learning-to-Reconfigure-Comics" class="headerlink" title="Dense Multitask Learning to Reconfigure Comics"></a>Dense Multitask Learning to Reconfigure Comics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08071">http://arxiv.org/abs/2307.08071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deblina Bhattacharjee, Sabine Süsstrunk, Mathieu Salzmann</li>
<li>for: 这paper的目的是开发一种多任务学习（MTL）模型，以实现漫画幕anel的精度预测，以便将漫画从一个发布频道传输到另一个。</li>
<li>methods: 我们使用了一种常见的策略，即无监督图像到图像翻译，以利用大量的真实世界约束。我们还利用了这些翻译结果，开发了一种基于视力变换器底层和域转移注意模块的多任务方法。</li>
<li>results: 我们的MTL方法可以成功地标识漫画幕anel中的Semantic单元以及嵌入的3D notion。这是一个非常具有挑战性的问题，因为漫画包含了不同的艺术风格、插图、布局和 объек scale，这些因素取决于作者的创作过程。<details>
<summary>Abstract</summary>
In this paper, we develop a MultiTask Learning (MTL) model to achieve dense predictions for comics panels to, in turn, facilitate the transfer of comics from one publication channel to another by assisting authors in the task of reconfiguring their narratives. Our MTL method can successfully identify the semantic units as well as the embedded notion of 3D in comic panels. This is a significantly challenging problem because comics comprise disparate artistic styles, illustrations, layouts, and object scales that depend on the authors creative process. Typically, dense image-based prediction techniques require a large corpus of data. Finding an automated solution for dense prediction in the comics domain, therefore, becomes more difficult with the lack of ground-truth dense annotations for the comics images. To address these challenges, we develop the following solutions: 1) we leverage a commonly-used strategy known as unsupervised image-to-image translation, which allows us to utilize a large corpus of real-world annotations; 2) we utilize the results of the translations to develop our multitasking approach that is based on a vision transformer backbone and a domain transferable attention module; 3) we study the feasibility of integrating our MTL dense-prediction method with an existing retargeting method, thereby reconfiguring comics.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们开发了一种多任务学习（MTL）模型，以实现漫画panel的密集预测，以便将漫画从一个发布渠道传输到另一个渠道，并 помочь作者重新配置他们的故事。我们的MTL方法可以成功地标识漫画中的语义单元以及嵌入的3D notion。这是一个非常具有挑战性的问题，因为漫画包含了不同的艺术风格、插图、布局和对象比例，这些因素取决于作者的创作过程。通常，密集图像基于预测技术需要很大的数据库。因此，在漫画领域找到自动化密集预测的解决方案变得更加困难，因为漫画图像的密集批注缺乏。为 Addressing these challenges, we develop the following solutions:1. 我们利用一种通常使用的策略，即无监督图像到图像翻译，以使用大量的实际世界约束。2. 我们利用翻译结果来开发我们的多任务方法，该方法基于视Transformer底层和域传递注意模块。3. 我们研究将我们的MTL密集预测方法与现有的重定向方法集成，以重新配置漫画。
</details></li>
</ul>
<hr>
<h2 id="MaGNAS-A-Mapping-Aware-Graph-Neural-Architecture-Search-Framework-for-Heterogeneous-MPSoC-Deployment"><a href="#MaGNAS-A-Mapping-Aware-Graph-Neural-Architecture-Search-Framework-for-Heterogeneous-MPSoC-Deployment" class="headerlink" title="MaGNAS: A Mapping-Aware Graph Neural Architecture Search Framework for Heterogeneous MPSoC Deployment"></a>MaGNAS: A Mapping-Aware Graph Neural Architecture Search Framework for Heterogeneous MPSoC Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08065">http://arxiv.org/abs/2307.08065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohanad Odema, Halima Bouzidi, Hamza Ouarnoughi, Smail Niar, Mohammad Abdullah Al Faruque</li>
<li>For: The paper is written for vision-based applications that require efficient processing on heterogeneous MPSoC platforms.* Methods: The paper proposes a novel unified design-mapping approach for efficient processing of vision GNN workloads on heterogeneous MPSoC platforms, including a mapping-aware Graph Neural Architecture Search (MaGNAS) framework.* Results: The proposed MaGNAS framework achieves 1.57x latency speedup and is 3.38x more energy-efficient for several vision datasets executed on the Xavier MPSoC platform compared to the GPU-only deployment, while sustaining an average 0.11% accuracy reduction from the baseline.Here’s the simplified Chinese text for the three information points:* For: 这篇论文是为视觉应用程序设计的，需要高效地运行在多核心处理器系统（MPSoC）上。* Methods: 论文提出了一种新的统一设计映射方法，用于高效地处理视觉Graph Neural Networks（GNN）任务在多核心MPSoC平台上。* Results: 提出的MaGNAS框架在Xavier MPSoC上实现了1.57倍的延迟速度提升和3.38倍的能效率提升，而且与基线相比减少了0.11%的准确率。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are becoming increasingly popular for vision-based applications due to their intrinsic capacity in modeling structural and contextual relations between various parts of an image frame. On another front, the rising popularity of deep vision-based applications at the edge has been facilitated by the recent advancements in heterogeneous multi-processor Systems on Chips (MPSoCs) that enable inference under real-time, stringent execution requirements. By extension, GNNs employed for vision-based applications must adhere to the same execution requirements. Yet contrary to typical deep neural networks, the irregular flow of graph learning operations poses a challenge to running GNNs on such heterogeneous MPSoC platforms. In this paper, we propose a novel unified design-mapping approach for efficient processing of vision GNN workloads on heterogeneous MPSoC platforms. Particularly, we develop MaGNAS, a mapping-aware Graph Neural Architecture Search framework. MaGNAS proposes a GNN architectural design space coupled with prospective mapping options on a heterogeneous SoC to identify model architectures that maximize on-device resource efficiency. To achieve this, MaGNAS employs a two-tier evolutionary search to identify optimal GNNs and mapping pairings that yield the best performance trade-offs. Through designing a supernet derived from the recent Vision GNN (ViG) architecture, we conducted experiments on four (04) state-of-the-art vision datasets using both (i) a real hardware SoC platform (NVIDIA Xavier AGX) and (ii) a performance/cost model simulator for DNN accelerators. Our experimental results demonstrate that MaGNAS is able to provide 1.57x latency speedup and is 3.38x more energy-efficient for several vision datasets executed on the Xavier MPSoC vs. the GPU-only deployment while sustaining an average 0.11% accuracy reduction from the baseline.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在视觉应用中变得越来越受欢迎，这是因为它们内置了图结构和上下文关系 между图像帧中不同部分的能力。同时，由于近期增加的深度视觉应用在边缘进行执行，因此GNN也必须遵循同样的执行要求。然而，与 Typical deep neural networks不同，图学习操作的不规则流动使得在多核心处理器系统（MPSoC）平台上运行GNN变得更加挑战。在这篇论文中，我们提出了一种新的统一设计映射方法，以便高效地处理视觉GNN工作负荷在多核心处理器平台上。具体来说，我们开发了 MaGNAS，一个具有映射意识的图 neural architecture search框架。MaGNAS提出了一个图 neural 架构设计空间，并与多核心 SoC 上的可能的映射选择相结合，以便确定最佳的GNN模型，以最大化设备资源利用。为达到这一目标，MaGNAS使用了两层演化搜索，以确定最佳的GNN和映射对。通过基于最近的视觉 GNN（ViG）架构的超网，我们进行了在四个（04） state-of-the-art 视觉 dataset 上的实验，使用了 both (i) 真实硬件 SoC 平台（NVIDIA Xavier AGX）和 (ii) 性能/成本模型适用于 DNN 加速器的表现/成本模拟器。我们的实验结果表明，MaGNAS 能够提供 1.57 倍的延迟速度提升和 3.38 倍的能效性提升，而在 Xavier MPSoC 上执行多个视觉dataset 时与 GPU-only 部署相比，保持了平均 0.11% 的准确性下降。
</details></li>
</ul>
<hr>
<h2 id="LafitE-Latent-Diffusion-Model-with-Feature-Editing-for-Unsupervised-Multi-class-Anomaly-Detection"><a href="#LafitE-Latent-Diffusion-Model-with-Feature-Editing-for-Unsupervised-Multi-class-Anomaly-Detection" class="headerlink" title="LafitE: Latent Diffusion Model with Feature Editing for Unsupervised Multi-class Anomaly Detection"></a>LafitE: Latent Diffusion Model with Feature Editing for Unsupervised Multi-class Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08059">http://arxiv.org/abs/2307.08059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haonan Yin, Guanlong Jiao, Qianhui Wu, Borje F. Karlsson, Biqing Huang, Chin Yew Lin</li>
<li>for: 本研究旨在为flexible manufacturing systems提供一种不需要监督的多类异常检测方法，能够在只有正常数据可用时检测到对象属于多个类型的异常。</li>
<li>methods: 本研究使用生成器基于方法，包括潜在扩散模型 для重建，以解决泛化难点’’问题，以及特征编辑策略来进一步缓解’’标识短ircuit’’问题。</li>
<li>results: 对MVTec-AD和MPDD数据集进行了广泛的实验，显示提出的LafitE方法在average AUROC指标上与现有方法相比，具有显著的优势。同时，通过我们提出的pseudo验证集来选择适合实际测试集的超参数。<details>
<summary>Abstract</summary>
In the context of flexible manufacturing systems that are required to produce different types and quantities of products with minimal reconfiguration, this paper addresses the problem of unsupervised multi-class anomaly detection: develop a unified model to detect anomalies from objects belonging to multiple classes when only normal data is accessible. We first explore the generative-based approach and investigate latent diffusion models for reconstruction to mitigate the notorious ``identity shortcut'' issue in auto-encoder based methods. We then introduce a feature editing strategy that modifies the input feature space of the diffusion model to further alleviate ``identity shortcuts'' and meanwhile improve the reconstruction quality of normal regions, leading to fewer false positive predictions. Moreover, we are the first who pose the problem of hyperparameter selection in unsupervised anomaly detection, and propose a solution of synthesizing anomaly data for a pseudo validation set to address this problem. Extensive experiments on benchmark datasets MVTec-AD and MPDD show that the proposed LafitE, \ie, Latent Diffusion Model with Feature Editing, outperforms state-of-art methods by a significant margin in terms of average AUROC. The hyperparamters selected via our pseudo validation set are well-matched to the real test set.
</details>
<details>
<summary>摘要</summary>
在需要生产不同类型和量的产品时，这篇论文解决了无监督多类异常检测的问题：开发一个综合模型，可以从多个类型的对象中检测异常。我们首先探讨了生成器基本的方法，并调查了抽象扩散模型来解决拥有短circuit''问题，这是抽象扩散模型基于方法的一个常见问题。然后，我们引入了特征编辑策略，将输入特征空间中的特征进行修改，以更好地降低异常点的检测难度，同时提高正常区域的重建质量，从而减少假阳性预测。此外，我们是第一个提出了无监督异常检测中参数选择的问题，并提出了一种使用生成异常数据来 Pseudo 验证集来解决这个问题。我们的LafitE（即潜在扩散模型与特征编辑）在 MPDD 和 MVTec-AD 测试集上进行了广泛的实验，结果表明，它在 average AUROC 方面与状态机器人在前方的方法相比，具有显著的优势。而我们选择的参数via Pseudo 验证集和实际测试集之间的匹配性也很高。
</details></li>
</ul>
<hr>
<h2 id="TransNuSeg-A-Lightweight-Multi-Task-Transformer-for-Nuclei-Segmentation"><a href="#TransNuSeg-A-Lightweight-Multi-Task-Transformer-for-Nuclei-Segmentation" class="headerlink" title="TransNuSeg: A Lightweight Multi-Task Transformer for Nuclei Segmentation"></a>TransNuSeg: A Lightweight Multi-Task Transformer for Nuclei Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08051">http://arxiv.org/abs/2307.08051</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenqi-he/transnuseg">https://github.com/zhenqi-he/transnuseg</a></li>
<li>paper_authors: Zhenqi He, Mathias Unberath, Jing Ke, Yiqing Shen<br>for:这篇论文的目的是提出一种基于Transformer的自动核体分割方法，以提高核体分割的准确性和效率。methods:这篇论文使用了一种新的多任务学习策略，将核体分割任务分解成三个子任务：核体实例分割、核体边沿分割和分割边缘集成。此外，它还使用了一种新的自适应共享机制，以便在不同分支之间共享自适应 heads。results:这篇论文的实验结果表明，使用这种方法可以在两个不同的数据集上，与CA2.5-Net等其他状态对抗方法相比，提高核体分割的精度。此外，这种方法还可以降低模型的参数量，从而提高计算效率。<details>
<summary>Abstract</summary>
Nuclei appear small in size, yet, in real clinical practice, the global spatial information and correlation of the color or brightness contrast between nuclei and background, have been considered a crucial component for accurate nuclei segmentation. However, the field of automatic nuclei segmentation is dominated by Convolutional Neural Networks (CNNs), meanwhile, the potential of the recently prevalent Transformers has not been fully explored, which is powerful in capturing local-global correlations. To this end, we make the first attempt at a pure Transformer framework for nuclei segmentation, called TransNuSeg. Different from prior work, we decouple the challenging nuclei segmentation task into an intrinsic multi-task learning task, where a tri-decoder structure is employed for nuclei instance, nuclei edge, and clustered edge segmentation respectively. To eliminate the divergent predictions from different branches in previous work, a novel self distillation loss is introduced to explicitly impose consistency regulation between branches. Moreover, to formulate the high correlation between branches and also reduce the number of parameters, an efficient attention sharing scheme is proposed by partially sharing the self-attention heads amongst the tri-decoders. Finally, a token MLP bottleneck replaces the over-parameterized Transformer bottleneck for a further reduction in model complexity. Experiments on two datasets of different modalities, including MoNuSeg have shown that our methods can outperform state-of-the-art counterparts such as CA2.5-Net by 2-3% Dice with 30% fewer parameters. In conclusion, TransNuSeg confirms the strength of Transformer in the context of nuclei segmentation, which thus can serve as an efficient solution for real clinical practice. Code is available at https://github.com/zhenqi-he/transnuseg.
</details>
<details>
<summary>摘要</summary>
核体在实际临床应用中显示为小型，但是在实际临床实践中，全局空间信息和背景颜色或亮度差异的 corrélation，被视为精确的核体分割的关键组成部分。然而，自动核体分割领域被 CNN 所主导，而 transformer 的潜在力量尚未得到完全利用，这是拥有地方-全局 corrélation 的强大能力。为此，我们首次提出了一个纯 transformer 框架 для核体分割，称为 TransNuSeg。与先前的工作不同，我们将核体分割任务分解为内生多任务学习任务，其中使用 tri-decoder 结构进行核体实例、核体边和分割的聚合edge 分割。为了消除不同分支的不同预测，我们引入了一种新的自我抽象损失，以直接强制不同分支之间的一致性规则。此外，我们还提出了一种高效的注意力共享方案，通过共享部分自动注意力头来降低模型参数数量。最后，我们将各自MLP 瓶颈替换为更加简单的 токен MLP 瓶颈，以进一步降低模型复杂性。在两个不同模式的数据集上进行了实验，包括 MoNuSeg，我们的方法可以与状态态-of-the-art 对手 CA2.5-Net 相比，提高 Dice 指标2-3%，同时减少参数数量30%。结论：TransNuSeg 证明了 transformer 在核体分割领域的力量，可以作为实际临床应用的高效解决方案。代码可以在 <https://github.com/zhenqi-he/transnuseg> 上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-SLCA-UNet-Architecture-for-Automatic-MRI-Brain-Tumor-Segmentation"><a href="#A-Novel-SLCA-UNet-Architecture-for-Automatic-MRI-Brain-Tumor-Segmentation" class="headerlink" title="A Novel SLCA-UNet Architecture for Automatic MRI Brain Tumor Segmentation"></a>A Novel SLCA-UNet Architecture for Automatic MRI Brain Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08048">http://arxiv.org/abs/2307.08048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tejashwini P S, Thriveni J, Venugopal K R<br>for: 这个论文主要针对的是如何通过深度学习来自动化脑肿图像分类和识别。methods: 该论文提出了一种基于UNet架构的修改方法，即SLCA UNet方法，该方法包括循环感知层、Channel Attention层和堆叠卷积层等模块，能够有效地捕捉脑肿图像中的细节和概念信息。results: 该论文在使用BraTS数据集进行测试时，实现了良好的性能，其中Dice指标、敏感度、特异度和 Hausdorff95指标分别为0.845、0.845、0.999和8.1。<details>
<summary>Abstract</summary>
Brain tumor is deliberated as one of the severe health complications which lead to decrease in life expectancy of the individuals and is also considered as a prominent cause of mortality worldwide. Therefore, timely detection and prediction of brain tumors can be helpful to prevent death rates due to brain tumors. Biomedical image analysis is a widely known solution to diagnose brain tumor. Although MRI is the current standard method for imaging tumors, its clinical usefulness is constrained by the requirement of manual segmentation which is time-consuming. Deep learning-based approaches have emerged as a promising solution to develop automated biomedical image exploration tools and the UNet architecture is commonly used for segmentation. However, the traditional UNet has limitations in terms of complexity, training, accuracy, and contextual information processing. As a result, the modified UNet architecture, which incorporates residual dense blocks, layered attention, and channel attention modules, in addition to stacked convolution, can effectively capture both coarse and fine feature information. The proposed SLCA UNet approach achieves good performance on the freely accessible Brain Tumor Segmentation (BraTS) dataset, with an average performance of 0.845, 0.845, 0.999, and 8.1 in terms of Dice, Sensitivity, Specificity, and Hausdorff95 for BraTS 2020 dataset, respectively.
</details>
<details>
<summary>摘要</summary>
脑肿是一种严重的健康问题，可能导致个体寿命减少，并被视为全球致死原因之一。因此，在时间上掌握和预测脑肿的技术是非常重要的。生物医学影像分析是一种广泛应用的解决方案，用于诊断脑肿。虽然MRI是当前标准的肿体影像方法，但其临床实用性受到手动分 segmentation 的限制，这是时间consuming 的。基于深度学习的方法在诊断方面出现了一种可能的解决方案，其中 UNet 架构是最常用的。然而，传统的 UNet 有许多局限性，包括复杂度、训练、准确率和上下文信息处理等方面。因此，基于 SLCA UNet 架构，通过添加径 residual dense blocks、层 attention 和通道 attention 模块，可以更好地捕捉肿体中粗细特征信息。提出的 SLCA UNet 方法在可以获得 BraTS 2020 数据集的自由访问Brain Tumor Segmentation（BraTS）数据集上的良好性能，其中的平均性能为0.845、0.845、0.999和8.1。
</details></li>
</ul>
<hr>
<h2 id="Planting-a-SEED-of-Vision-in-Large-Language-Model"><a href="#Planting-a-SEED-of-Vision-in-Large-Language-Model" class="headerlink" title="Planting a SEED of Vision in Large Language Model"></a>Planting a SEED of Vision in Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08041">http://arxiv.org/abs/2307.08041</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ailab-cvc/seed">https://github.com/ailab-cvc/seed</a></li>
<li>paper_authors: Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, Ying Shan</li>
<li>for: 这个论文是为了提供一种能让语言模型同时看图和画图的图像tokenizer。</li>
<li>methods: 这个论文使用了一种新的图像tokenizer architecture，它使用了一个1D causal dependency来生成图像token，并且通过在tokenizer训练阶段进行优化来使图像token capture高级别 semantics。</li>
<li>results: 通过使用这种新的图像tokenizer，LLM可以通过简单的LoRA tuning来实现图像-文本和文本-图像生成。这个论文在5.7天内使用64个V100 GPU和500万个公共可用的图像-文本对对其进行训练。<details>
<summary>Abstract</summary>
We present SEED, an elaborate image tokenizer that empowers Large Language Models (LLMs) with the emergent ability to SEE and Draw at the same time. Research on image tokenizers has previously reached an impasse, as frameworks employing quantized visual tokens have lost prominence due to subpar performance and convergence in multimodal comprehension (compared to BLIP-2, etc.) or generation (compared to Stable Diffusion, etc.). Despite the limitations, we remain confident in its natural capacity to unify visual and textual representations, facilitating scalable multimodal training with LLM's original recipe. In this study, we identify two crucial principles for the architecture and training of SEED that effectively ease subsequent alignment with LLMs. (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. As a result, the off-the-shelf LLM is able to perform both image-to-text and text-to-image generation by incorporating our SEED through efficient LoRA tuning. Comprehensive multimodal pretraining and instruction tuning, which may yield improved results, are reserved for future investigation. This version of SEED was trained in 5.7 days using only 64 V100 GPUs and 5M publicly available image-text pairs. Our preliminary study emphasizes the great potential of discrete visual tokens in versatile multimodal LLMs and the importance of proper image tokenizers in broader research.
</details>
<details>
<summary>摘要</summary>
我们介绍SEED，一个复杂的图像tokenizer，允许大型语言模型（LLM）同时“看”和“绘”。过去的研究图像tokenizer已经到了僵对，因为使用量化的视觉token导致了与BLIP-2等模型的比较不利，以及生成模型的比较不利（比如Stable Diffusion等）。尽管有限制，我们仍然信任它的自然能力，将vision和textual表现结合起来，实现标准多模式训练， LLM 的原始配方。在这个研究中，我们确定了两个重要的建筑和训练SEED的原则，以确保其与LLM的配合。1. 图像token应该与2D物理 patch 位置无关，而是通过1D causal dependency 生成，这样的自然依赖性与 LLM 的左往右预测机制相关。2. 图像token应该捕捉高度抽象的 semantics，与 слова的Semantic abstraction 相关，并在 tokenizer 训练阶段进行优化。因此，通过我们的SEED，标准 LLM 可以进行图像-文本和文本-图像生成，只需要通过LoRA调整。未来的多模式预训和指令调整可能会带来更好的结果。我们在5.7天内使用64个V100 GPU和500万个公开可用的图像-文本对给SEED进行训练。我们的初步研究显示，可以使用精确的图像tokenizer来实现多模式LLM的实用性和多元性。
</details></li>
</ul>
<hr>
<h2 id="Multi-Object-Discovery-by-Low-Dimensional-Object-Motion"><a href="#Multi-Object-Discovery-by-Low-Dimensional-Object-Motion" class="headerlink" title="Multi-Object Discovery by Low-Dimensional Object Motion"></a>Multi-Object Discovery by Low-Dimensional Object Motion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08027">http://arxiv.org/abs/2307.08027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sadrasafa/multi-object-segmentation">https://github.com/sadrasafa/multi-object-segmentation</a></li>
<li>paper_authors: Sadra Safadoust, Fatma Güney</li>
<li>for: 该研究旨在提高单图像中的动态 reconstruction，即使无法获取下一帧图像。</li>
<li>methods: 该研究使用了像素级几何和物体运动来解除单图像中的流动ambiguity。</li>
<li>results: 该研究在 sintetic和实际 datasets上达到了state-of-the-art的多物体分 segmentation结果，并且对预测深度图表现了可靠的性能。<details>
<summary>Abstract</summary>
Recent work in unsupervised multi-object segmentation shows impressive results by predicting motion from a single image despite the inherent ambiguity in predicting motion without the next image. On the other hand, the set of possible motions for an image can be constrained to a low-dimensional space by considering the scene structure and moving objects in it. We propose to model pixel-wise geometry and object motion to remove ambiguity in reconstructing flow from a single image. Specifically, we divide the image into coherently moving regions and use depth to construct flow bases that best explain the observed flow in each region. We achieve state-of-the-art results in unsupervised multi-object segmentation on synthetic and real-world datasets by modeling the scene structure and object motion. Our evaluation of the predicted depth maps shows reliable performance in monocular depth estimation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Analysing-Gender-Bias-in-Text-to-Image-Models-using-Object-Detection"><a href="#Analysing-Gender-Bias-in-Text-to-Image-Models-using-Object-Detection" class="headerlink" title="Analysing Gender Bias in Text-to-Image Models using Object Detection"></a>Analysing Gender Bias in Text-to-Image Models using Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08025">http://arxiv.org/abs/2307.08025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harveymannering/text-to-image-bias">https://github.com/harveymannering/text-to-image-bias</a></li>
<li>paper_authors: Harvey Mannering</li>
<li>for: 该研究目的是测试文本到图像模型中的偏见。</li>
<li>methods: 该研究使用了对应的提示，例如“一个男人&#x2F;女人持有一个物品”，以检查某些物品是否与certain gender相关。</li>
<li>results: 分析结果显示， masculine prompts 更 frequently generate了如锦标、剑、车、棒棒球和自行车等物品，而 feminine prompts 更 frequently generate了如手提包、雨伞、碗、瓶子和杯子等物品。<details>
<summary>Abstract</summary>
This work presents a novel strategy to measure bias in text-to-image models. Using paired prompts that specify gender and vaguely reference an object (e.g. "a man/woman holding an item") we can examine whether certain objects are associated with a certain gender. In analysing results from Stable Diffusion, we observed that male prompts generated objects such as ties, knives, trucks, baseball bats, and bicycles more frequently. On the other hand, female prompts were more likely to generate objects such as handbags, umbrellas, bowls, bottles, and cups. We hope that the method outlined here will be a useful tool for examining bias in text-to-image models.
</details>
<details>
<summary>摘要</summary>
Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Boosting-3-DoF-Ground-to-Satellite-Camera-Localization-Accuracy-via-Geometry-Guided-Cross-View-Transformer"><a href="#Boosting-3-DoF-Ground-to-Satellite-Camera-Localization-Accuracy-via-Geometry-Guided-Cross-View-Transformer" class="headerlink" title="Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View Transformer"></a>Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08015">http://arxiv.org/abs/2307.08015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujiao Shi, Fei Wu, Akhil Perincherry, Ankit Vora, Hongdong Li</li>
<li>for: 提高Camera pose estimation的精度，尤其是在具有有限样本密度的卫星图像库中。</li>
<li>methods: 我们提出了一种方法，通过估算地面图像和卫星图像之间的相对旋转和翻译来提高地面摄像机的位置和方向估计的准确性。我们的方法包括：(1) 使用geometry-guided cross-view transformer来将地面视图转换为飞行视图；(2) 使用神经网络pose optimizer来估计卫星图像和地面图像之间的相对旋转；(3) 使用uncertainty-guided spatial correlation来生成可能性图中的车辆位置。</li>
<li>results: 我们的方法在cross-view KITTI dataset上实验表明，与state-of-the-art方法相比，具有显著的改进。特别是，限制车辆 lateral pose 在1m内的概率从35.54%提高到76.44%，限制车辆 orientation 在1°内的概率从19.64%提高到99.10%。<details>
<summary>Abstract</summary>
Image retrieval-based cross-view localization methods often lead to very coarse camera pose estimation, due to the limited sampling density of the database satellite images. In this paper, we propose a method to increase the accuracy of a ground camera's location and orientation by estimating the relative rotation and translation between the ground-level image and its matched/retrieved satellite image. Our approach designs a geometry-guided cross-view transformer that combines the benefits of conventional geometry and learnable cross-view transformers to map the ground-view observations to an overhead view. Given the synthesized overhead view and observed satellite feature maps, we construct a neural pose optimizer with strong global information embedding ability to estimate the relative rotation between them. After aligning their rotations, we develop an uncertainty-guided spatial correlation to generate a probability map of the vehicle locations, from which the relative translation can be determined. Experimental results demonstrate that our method significantly outperforms the state-of-the-art. Notably, the likelihood of restricting the vehicle lateral pose to be within 1m of its Ground Truth (GT) value on the cross-view KITTI dataset has been improved from $35.54\%$ to $76.44\%$, and the likelihood of restricting the vehicle orientation to be within $1^{\circ}$ of its GT value has been improved from $19.64\%$ to $99.10\%$.
</details>
<details>
<summary>摘要</summary>
Image Retrieval-based Cross-view Localization Methods Often Lead to Very Coarse Camera Pose Estimation, Due to the Limited Sampling Density of the Database Satellite Images. In This Paper, We Propose a Method to Increase the Accuracy of a Ground Camera's Location and Orientation by Estimating the Relative Rotation and Translation Between the Ground-level Image and Its Matched/retrieved Satellite Image. Our Approach Designs a Geometry-guided Cross-view Transformer That Combines the Benefits of Conventional Geometry and Learnable Cross-view Transformers to Map the Ground-view Observations to an Overhead View. Given the Synthesized Overhead View and Observed Satellite Feature Maps, We Construct a Neural Pose Optimizer with Strong Global Information Embedding Ability to Estimate the Relative Rotation Between Them. After Aligning Their Rotations, We Develop an Uncertainty-guided Spatial Correlation to Generate a Probability Map of the Vehicle Locations, from Which the Relative Translation Can Be Determined. Experimental Results Demonstrate That Our Method Significantly Outperforms the State-of-the-art. Notably, the Likelihood of Restricting the Vehicle Lateral Pose to Be Within 1m of Its Ground Truth (GT) Value on the Cross-view KITTI Dataset Has Been Improved from $35.54\%$ to $76.44\%$, and the Likelihood of Restricting the Vehicle Orientation to Be Within $1^{\circ}$ of Its GT Value Has Been Improved from $19.64\%$ to $99.10\%$.
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Implicit-Models-Sparsity-Trade-offs-Capability-in-Weight-tied-Model-for-Vision-Tasks"><a href="#Revisiting-Implicit-Models-Sparsity-Trade-offs-Capability-in-Weight-tied-Model-for-Vision-Tasks" class="headerlink" title="Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks"></a>Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08013">http://arxiv.org/abs/2307.08013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haobo Song, Soumajit Majumder, Tao Lin</li>
<li>for: This paper aims to revisit the line of implicit models, specifically weight-tied models, and evaluate their effectiveness, stability, and efficiency on vision tasks.</li>
<li>methods: The paper uses weight-tied models as the basis for its study, and proposes the use of distinct sparse masks to improve the model capacity.</li>
<li>results: The paper finds that weight-tied models are more effective, stable, and efficient on vision tasks compared to DEQ variants, and provides design guidelines for practitioners regarding the selection of depth, width, and sparsity.<details>
<summary>Abstract</summary>
Implicit models such as Deep Equilibrium Models (DEQs) have garnered significant attention in the community for their ability to train infinite layer models with elegant solution-finding procedures and constant memory footprint. However, despite several attempts, these methods are heavily constrained by model inefficiency and optimization instability. Furthermore, fair benchmarking across relevant methods for vision tasks is missing. In this work, we revisit the line of implicit models and trace them back to the original weight-tied models. Surprisingly, we observe that weight-tied models are more effective, stable, as well as efficient on vision tasks, compared to the DEQ variants. Through the lens of these simple-yet-clean weight-tied models, we further study the fundamental limits in the model capacity of such models and propose the use of distinct sparse masks to improve the model capacity. Finally, for practitioners, we offer design guidelines regarding the depth, width, and sparsity selection for weight-tied models, and demonstrate the generalizability of our insights to other learning paradigms.
</details>
<details>
<summary>摘要</summary>
匿型模型（如深度均衡模型）在社区中受到了广泛关注，因为它们可以训练无穷层模型，并且有着简洁的解决方案和常量内存占用。然而，虽然有几次尝试，但这些方法受到了模型不充分利用和优化不稳定的限制。此外，相关的视觉任务中的公平比较缺失。在这种情况下，我们回到了权重相关模型的起源，并发现了权重相关模型在视觉任务上的效果更高，稳定性更好，并且更高效。通过这些简单 yet clean的权重相关模型，我们进一步研究了这些模型的基本限制，并提出了使用特定的稀疏面积来提高模型容量的方法。最后，我们向实践者提供了深度、宽度和稀疏性选择的设计指南，并证明了我们的理解在其他学习模式上的普适性。
</details></li>
</ul>
<hr>
<h2 id="Householder-Projector-for-Unsupervised-Latent-Semantics-Discovery"><a href="#Householder-Projector-for-Unsupervised-Latent-Semantics-Discovery" class="headerlink" title="Householder Projector for Unsupervised Latent Semantics Discovery"></a>Householder Projector for Unsupervised Latent Semantics Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08012">http://arxiv.org/abs/2307.08012</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kingjamessong/householdergan">https://github.com/kingjamessong/householdergan</a></li>
<li>paper_authors: Yue Song, Jichao Zhang, Nicu Sebe, Wei Wang</li>
<li>for: 这个论文主要是为了探索 generator adversarial networks (GANs) 的内在结构，以便更好地理解和控制图像生成过程。</li>
<li>methods: 作者提出了一种基于 Householder 变换的低级别正交矩阵表示方法（Householder Projector），用于参数化 projection matrix，从而实现对 latent code 的 traverse 以便发现更加精细的 semantic attributes。</li>
<li>results: 作者在 StyleGAN2&#x2F;StyleGAN3 模型中集成了 Householder Projector，并在多个 benchmark 上评估了模型的表现。结果显示，只需要在原始训练步骤的 1% 上进行微调，Householder Projector 可以帮助 StyleGANs 发现更加精细和准确的 semantic attributes，而不需要牺牲图像的准确性。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs), especially the recent style-based generators (StyleGANs), have versatile semantics in the structured latent space. Latent semantics discovery methods emerge to move around the latent code such that only one factor varies during the traversal. Recently, an unsupervised method proposed a promising direction to directly use the eigenvectors of the projection matrix that maps latent codes to features as the interpretable directions. However, one overlooked fact is that the projection matrix is non-orthogonal and the number of eigenvectors is too large. The non-orthogonality would entangle semantic attributes in the top few eigenvectors, and the large dimensionality might result in meaningless variations among the directions even if the matrix is orthogonal. To avoid these issues, we propose Householder Projector, a flexible and general low-rank orthogonal matrix representation based on Householder transformations, to parameterize the projection matrix. The orthogonality guarantees that the eigenvectors correspond to disentangled interpretable semantics, while the low-rank property encourages that each identified direction has meaningful variations. We integrate our projector into pre-trained StyleGAN2/StyleGAN3 and evaluate the models on several benchmarks. Within only $1\%$ of the original training steps for fine-tuning, our projector helps StyleGANs to discover more disentangled and precise semantic attributes without sacrificing image fidelity.
</details>
<details>
<summary>摘要</summary>
“生成问题网络”（GANs），特别是最近的样式基因生成器（StyleGANs），在结构化的底层空间中有多元 semantics。内在semantics发现方法产生了可以在底层代码中移动的方法，以便只有一个因素在旅游中变化。最近，一种无监督的方法提出了一个可能的方向， directly使用对应码到特征的投影矩阵的特征向量作为可解释的方向。然而，一个被遗忘的事实是，投影矩阵不对称，数量过多的特征向量会导致意义的变化，即使投影矩阵是对称的。为了解决这些问题，我们提出了“Householder Projector”，一种通用且统一的低维度对称矩阵表示，基于Householder变换。对称性 garantuees that the feature vectors correspond to disentangled interpretable semantics, while the low-rank property encourages that each identified direction has meaningful variations. We integrate our projector into pre-trained StyleGAN2/StyleGAN3 and evaluate the models on several benchmarks. Within only 1% of the original training steps for fine-tuning, our projector helps StyleGANs to discover more disentangled and precise semantic attributes without sacrificing image fidelity.
</details></li>
</ul>
<hr>
<h2 id="LUCYD-A-Feature-Driven-Richardson-Lucy-Deconvolution-Network"><a href="#LUCYD-A-Feature-Driven-Richardson-Lucy-Deconvolution-Network" class="headerlink" title="LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network"></a>LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07998">http://arxiv.org/abs/2307.07998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ctom2/lucyd-deconvolution">https://github.com/ctom2/lucyd-deconvolution</a></li>
<li>paper_authors: Tomáš Chobola, Gesine Müller, Veit Dausmann, Anton Theileis, Jan Taucher, Jan Huisken, Tingying Peng</li>
<li>for: 提高微scopic图像质量和可读性</li>
<li>methods: 结合Richardson-Lucy抽象方程和深度学习网络特征，提出了LUCYD方法，以增强图像质量并降低计算成本。</li>
<li>results: LUCYD方法在synthetic和实际微scopic图像中表现出色，超过了现有方法的性能，并能够处理不同的微scopic模式和捕捉条件。<details>
<summary>Abstract</summary>
The process of acquiring microscopic images in life sciences often results in image degradation and corruption, characterised by the presence of noise and blur, which poses significant challenges in accurately analysing and interpreting the obtained data. This paper proposes LUCYD, a novel method for the restoration of volumetric microscopy images that combines the Richardson-Lucy deconvolution formula and the fusion of deep features obtained by a fully convolutional network. By integrating the image formation process into a feature-driven restoration model, the proposed approach aims to enhance the quality of the restored images whilst reducing computational costs and maintaining a high degree of interpretability. Our results demonstrate that LUCYD outperforms the state-of-the-art methods in both synthetic and real microscopy images, achieving superior performance in terms of image quality and generalisability. We show that the model can handle various microscopy modalities and different imaging conditions by evaluating it on two different microscopy datasets, including volumetric widefield and light-sheet microscopy. Our experiments indicate that LUCYD can significantly improve resolution, contrast, and overall quality of microscopy images. Therefore, it can be a valuable tool for microscopy image restoration and can facilitate further research in various microscopy applications. We made the source code for the model accessible under https://github.com/ctom2/lucyd-deconvolution.
</details>
<details>
<summary>摘要</summary>
生物科学中获取微scopic图像过程经常会导致图像异常和损害，表现为噪声和模糊，这会对数据分析和解释带来重大挑战。这篇论文提出了LUCYD方法，该方法combines Richardson-Lucy整形方程和基于完全 convolutional neural network 的深度特征融合，以提高图像 restore 的质量，降低计算成本，保持高度可解释性。我们的结果表明，LUCYD方法在对比state-of-the-art方法时表现出色，在 sintetic 和实际 microscopy 图像中都达到了更高的图像质量和普适性。我们的实验表明，LUCYD方法可以处理不同的 microscopy 模式和拍摄条件，并且可以在两个不同的 microscopy 数据集上进行评估，包括volumetric widefield 和 light-sheet microscopy。我们的实验结果表明，LUCYD方法可以大幅提高微scopic图像的分辨率、对比度和总质量。因此，它可以成为微scopic图像 Restoration 的有价值工具，并且可以推动各种 microscopy 应用的进一步研究。我们将模型的源代码公开于 GitHub 上，可以通过 <https://github.com/ctom2/lucyd-deconvolution> 访问。
</details></li>
</ul>
<hr>
<h2 id="Enforcing-Topological-Interaction-between-Implicit-Surfaces-via-Uniform-Sampling"><a href="#Enforcing-Topological-Interaction-between-Implicit-Surfaces-via-Uniform-Sampling" class="headerlink" title="Enforcing Topological Interaction between Implicit Surfaces via Uniform Sampling"></a>Enforcing Topological Interaction between Implicit Surfaces via Uniform Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08716">http://arxiv.org/abs/2307.08716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hieu Le, Nicolas Talabot, Jiancheng Yang, Pascal Fua</li>
<li>for: 本文旨在提出一种新的方法，用于精确地模型3D物体表面，以保证它们之间的topological交互。</li>
<li>methods: 该方法基于随机点的统计方法，通过选择一组点作为参照点，来修正3D物体表面。</li>
<li>results: 实验表明，该方法可以准确地重建人体心脏，保证组件之间的topological连接。此外，该方法还可以用来模拟手与任意物体之间的各种交互方式。<details>
<summary>Abstract</summary>
Objects interact with each other in various ways, including containment, contact, or maintaining fixed distances. Ensuring these topological interactions is crucial for accurate modeling in many scenarios. In this paper, we propose a novel method to refine 3D object representations, ensuring that their surfaces adhere to a topological prior. Our key observation is that the object interaction can be observed via a stochastic approximation method: the statistic of signed distances between a large number of random points to the object surfaces reflect the interaction between them. Thus, the object interaction can be indirectly manipulated by using choosing a set of points as anchors to refine the object surfaces. In particular, we show that our method can be used to enforce two objects to have a specific contact ratio while having no surface intersection. The conducted experiments show that our proposed method enables accurate 3D reconstruction of human hearts, ensuring proper topological connectivity between components. Further, we show that our proposed method can be used to simulate various ways a hand can interact with an arbitrary object.
</details>
<details>
<summary>摘要</summary>
objects 与其他物体之间存在多种互动方式，包括含容、触摸或维持固定距离。保证这些拓扑互动是对很多场景中模型的精确预测非常重要。在这篇论文中，我们提出了一种新的方法来精细调整3D物体表示，使其表面遵循拓扑优先。我们的关键观察是物体互动可以通过一种随机点方法的统计来观察：对一大量Random点的积分可以反映物体之间的互动。因此，我们可以通过选择一组点作为安全来修改物体表面，以间接地控制物体互动。具体来说，我们表明了我们的方法可以用来保证两个物体之间有specific contact比例，而不会出现表面交叉。实验结果表明，我们的提议方法可以准确地重建人类心脏，并保证组件之间的拓扑连接性。此外，我们还表明了我们的方法可以用来模拟手部与任意物体之间的各种互动方式。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Human-Parsing-and-Pose-Network-for-Human-Action-Recognition"><a href="#Integrating-Human-Parsing-and-Pose-Network-for-Human-Action-Recognition" class="headerlink" title="Integrating Human Parsing and Pose Network for Human Action Recognition"></a>Integrating Human Parsing and Pose Network for Human Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07977">http://arxiv.org/abs/2307.07977</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liujf69/ipp-net-parsing">https://github.com/liujf69/ipp-net-parsing</a></li>
<li>paper_authors: Runwei Ding, Yuhang Wen, Jinfu Liu, Nan Dai, Fanyang Meng, Mengyuan Liu</li>
<li>for: 本研究旨在提高人体动作识别的精度，使用人体解剖特征图和人体分剖特征图作为输入模式。</li>
<li>methods: 本研究提出了一种Integrating Human Parsing and Pose Network（IPP-Net），利用人体解剖特征图和人体分剖特征图进行双路结合，以提高人体动作识别的精度。人体pose分支使用图型卷积网络来模型pose特征，而人体分剖分支使用人体探测和分剖器来提取多帧人体部分特征，然后使用卷积嵌入学习来学习人体分剖特征。</li>
<li>results: 对于NTU RGB+D和NTU RGB+D 120测试集，IPP-Net的实验结果表明，IPP-Net可以在人体动作识别任务中获得更高的准确率，比如exist的方法更高。代码可以在<a target="_blank" rel="noopener" href="https://github.com/liujf69/IPP-Net-Parsing%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/liujf69/IPP-Net-Parsing上获取。</a><details>
<summary>Abstract</summary>
Human skeletons and RGB sequences are both widely-adopted input modalities for human action recognition. However, skeletons lack appearance features and color data suffer large amount of irrelevant depiction. To address this, we introduce human parsing feature map as a novel modality, since it can selectively retain spatiotemporal features of the body parts, while filtering out noises regarding outfits, backgrounds, etc. We propose an Integrating Human Parsing and Pose Network (IPP-Net) for action recognition, which is the first to leverage both skeletons and human parsing feature maps in dual-branch approach. The human pose branch feeds compact skeletal representations of different modalities in graph convolutional network to model pose features. In human parsing branch, multi-frame body-part parsing features are extracted with human detector and parser, which is later learnt using a convolutional backbone. A late ensemble of two branches is adopted to get final predictions, considering both robust keypoints and rich semantic body-part features. Extensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistently verify the effectiveness of the proposed IPP-Net, which outperforms the existing action recognition methods. Our code is publicly available at https://github.com/liujf69/IPP-Net-Parsing .
</details>
<details>
<summary>摘要</summary>
人体骨架和RGB序列都是人类动作识别中广泛使用的输入模式。然而，骨架缺乏外表特征，RGB数据受到大量不相关的描述所受损害。为了解决这个问题，我们引入人体解析特征图作为一种新的输入模式，因为它可以选择性地保留身体部位的空间特征，并过滤背景和服装等不相关的噪音。我们提议一种 integrate human parsing and pose network（IPP-Net），它是第一个同时利用骨架和人体解析特征图进行双树结构的方法。人体姿势分支将不同模式的短暂骨架表示feed到图像卷积网络中，以模型姿势特征。人体解析分支使用人体检测和解析器来提取多帧身体部位解析特征，并使用卷积核心学习。最后，我们采用了两支分支的晚期ensemble来得到最终预测结果，考虑到了稳定的关键点和丰富的语义身体部位特征。我们的代码可以在https://github.com/liujf69/IPP-Net-Parsing上找到。extensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistently verify the effectiveness of the proposed IPP-Net, which outperforms the existing action recognition methods.
</details></li>
</ul>
<hr>
<h2 id="HRHD-HK-A-benchmark-dataset-of-high-rise-and-high-density-urban-scenes-for-3D-semantic-segmentation-of-photogrammetric-point-clouds"><a href="#HRHD-HK-A-benchmark-dataset-of-high-rise-and-high-density-urban-scenes-for-3D-semantic-segmentation-of-photogrammetric-point-clouds" class="headerlink" title="HRHD-HK: A benchmark dataset of high-rise and high-density urban scenes for 3D semantic segmentation of photogrammetric point clouds"></a>HRHD-HK: A benchmark dataset of high-rise and high-density urban scenes for 3D semantic segmentation of photogrammetric point clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07976">http://arxiv.org/abs/2307.07976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luzaijiaoxial/hrhd-hk">https://github.com/luzaijiaoxial/hrhd-hk</a></li>
<li>paper_authors: Maosu Li, Yijie Wu, Anthony G. O. Yeh, Fan Xue</li>
<li>for: 这篇论文旨在评估现有的3Dsemantic segmentation方法，以及它们在多样化的城市场景中的性能。</li>
<li>methods: 这篇论文使用了8种流行的semantic segmentation方法，并对其进行了全面的评估。</li>
<li>results: 实验结果表明，现有的3D semantic segmentation方法在处理高层、高密度城市区域时仍有很大的改进空间，特别是对于城市 объек的小体积物体。<details>
<summary>Abstract</summary>
Many existing 3D semantic segmentation methods, deep learning in computer vision notably, claimed to achieve desired results on urban point clouds, in which the city objects are too many and diverse for people to judge qualitatively. Thus, it is significant to assess these methods quantitatively in diversified real-world urban scenes, encompassing high-rise, low-rise, high-density, and low-density urban areas. However, existing public benchmark datasets primarily represent low-rise scenes from European cities and cannot assess the methods comprehensively. This paper presents a benchmark dataset of high-rise urban point clouds, namely High-Rise, High-Density urban scenes of Hong Kong (HRHD-HK), which has been vacant for a long time. HRHD-HK arranged in 150 tiles contains 273 million colorful photogrammetric 3D points from diverse urban settings. The semantic labels of HRHD-HK include building, vegetation, road, waterbody, facility, terrain, and vehicle. To the best of our knowledge, HRHD-HK is the first photogrammetric dataset that focuses on HRHD urban areas. This paper also comprehensively evaluates eight popular semantic segmentation methods on the HRHD-HK dataset. Experimental results confirmed plenty of room for enhancing the current 3D semantic segmentation of point clouds, especially for city objects with small volumes. Our dataset is publicly available at: https://github.com/LuZaiJiaoXiaL/HRHD-HK.
</details>
<details>
<summary>摘要</summary>
许多现有的3Dsemantic segmentation方法，特别是深度学习在计算机视觉领域，宣称达到了所需的结果在城市点云中，其中城市 объекts 太多和多样，使人无法评估其质量。因此，需要对这些方法进行量化的评估，以适应多样化的城市场景。然而，现有的公共benchmark数据集主要表示欧洲城市的低层建筑，无法全面评估这些方法。本文提出了一个高层、高密度城市点云数据集，即高层高密度城市区域的香港（HRHD-HK）数据集，该数据集已经空缺了很长时间。HRHD-HK包括150个块，每个块包含273万个颜色化光学3D点云，来自不同的城市环境。 semantic label 包括建筑、植被、路面、水域、设施、地形和车辆。根据我们所知，HRHD-HK是首个专注于高层高密度城市区域的光学数据集。本文还进行了8种流行的semantic segmentation方法的全面评估。实验结果表明，目前的3Dsemantic segmentation技术在城市点云中仍有很多的提升空间，特别是对城市对象的小体积。我们的数据集可以在：https://github.com/LuZaiJiaoXiaL/HRHD-HK中下载。
</details></li>
</ul>
<hr>
<h2 id="Towards-Viewpoint-Invariant-Visual-Recognition-via-Adversarial-Training"><a href="#Towards-Viewpoint-Invariant-Visual-Recognition-via-Adversarial-Training" class="headerlink" title="Towards Viewpoint-Invariant Visual Recognition via Adversarial Training"></a>Towards Viewpoint-Invariant Visual Recognition via Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10235">http://arxiv.org/abs/2307.10235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shouwei Ruan, Yinpeng Dong, Hang Su, Jianteng Peng, Ning Chen, Xingxing Wei</li>
<li>for: 提高图像分类器的视角不变性，使其能够在不同的视角下仍然准确地预测图像。</li>
<li>methods: 提出了一种基于对抗训练的方法，称为视点不变 adversarial training（VIAT），通过将视点变换视为攻击， формули了一个最小化损失函数，以实现视点不变的图像分类器。</li>
<li>results: 经验表明，VIAT 可以有效地提高多种图像分类器的视角不变性，并且可以通过将对抗视点分布传递给不同的图像，提高对象的泛化性能。<details>
<summary>Abstract</summary>
Visual recognition models are not invariant to viewpoint changes in the 3D world, as different viewing directions can dramatically affect the predictions given the same object. Although many efforts have been devoted to making neural networks invariant to 2D image translations and rotations, viewpoint invariance is rarely investigated. As most models process images in the perspective view, it is challenging to impose invariance to 3D viewpoint changes based only on 2D inputs. Motivated by the success of adversarial training in promoting model robustness, we propose Viewpoint-Invariant Adversarial Training (VIAT) to improve viewpoint robustness of common image classifiers. By regarding viewpoint transformation as an attack, VIAT is formulated as a minimax optimization problem, where the inner maximization characterizes diverse adversarial viewpoints by learning a Gaussian mixture distribution based on a new attack GMVFool, while the outer minimization trains a viewpoint-invariant classifier by minimizing the expected loss over the worst-case adversarial viewpoint distributions. To further improve the generalization performance, a distribution sharing strategy is introduced leveraging the transferability of adversarial viewpoints across objects. Experiments validate the effectiveness of VIAT in improving the viewpoint robustness of various image classifiers based on the diversity of adversarial viewpoints generated by GMVFool.
</details>
<details>
<summary>摘要</summary>
“视觉识别模型不具备对3D世界视角变化的不变性，因为不同的观察方向可能会对同一物体的预测产生很大的影响。虽然许多努力已经投入到了使用神经网络对2D图像的翻译和旋转进行不变性处理，但视点不变性 rarely investigated。因为大多数模型在 perspective view 中处理图像，因此基于2D输入 alone 提高3D视角不变性的问题是挑战。鼓动了对模型 robustness 的成功，我们提出了 Viewpoint-Invariant Adversarial Training（VIAT），以提高常见图像分类器的视角不变性。VIAT 是一种 minimax 优化问题，其中内部最大化部分表示多样化的敌方攻击视点，通过学习一个基于新的攻击 GMVFool 的 Gaussian mixture distribution，而外部最小化部分则是在最坏情况下的敌方视点分布上进行视点不变性的训练。为了进一步提高通用性表现，我们还提出了基于对敌方视点的传输性的分布分享策略。实验证明，VIAT 可以提高多种图像分类器的视角不变性，基于 GMVFool 生成的多样化敌方视点。”
</details></li>
</ul>
<hr>
<h2 id="Dual-level-Interaction-for-Domain-Adaptive-Semantic-Segmentation"><a href="#Dual-level-Interaction-for-Domain-Adaptive-Semantic-Segmentation" class="headerlink" title="Dual-level Interaction for Domain Adaptive Semantic Segmentation"></a>Dual-level Interaction for Domain Adaptive Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07972">http://arxiv.org/abs/2307.07972</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rainjamesy/dida">https://github.com/rainjamesy/dida</a></li>
<li>paper_authors: Dongyu Yao, Boheng Li</li>
<li>for: 这篇论文主要针对域 adaptation 上的 semantic segmentation 问题，提出了一种基于 dual-level interaction 的方法（DIDA），以增强模型的鲁棒性和精度。</li>
<li>methods: 该方法在 semantic segmentation 中使用了域 adaptation 技术，并在不同的域上进行了 augmented 视图的交互，以便增强模型的鲁棒性和精度。此外，该方法还使用了一种动态更新策略来保持一个有用的实例银行，以便更好地捕捉实例的特征。</li>
<li>results: 根据实验结果，该方法在 confusing 和 long-tailed 类上表现出了明显的优势，特别是在 semantic segmentation 中。与现有方法相比，该方法可以增强模型的精度和鲁棒性，并且可以更好地处理域 adaptation 问题。<details>
<summary>Abstract</summary>
Self-training approach recently secures its position in domain adaptive semantic segmentation, where a model is trained with target domain pseudo-labels. Current advances have mitigated noisy pseudo-labels resulting from the domain gap. However, they still struggle with erroneous pseudo-labels near the boundaries of the semantic classifier. In this paper, we tackle this issue by proposing a dual-level interaction for domain adaptation (DIDA) in semantic segmentation. Explicitly, we encourage the different augmented views of the same pixel to have not only similar class prediction (semantic-level) but also akin similarity relationship with respect to other pixels (instance-level). As it's impossible to keep features of all pixel instances for a dataset, we, therefore, maintain a labeled instance bank with dynamic updating strategies to selectively store the informative features of instances. Further, DIDA performs cross-level interaction with scattering and gathering techniques to regenerate more reliable pseudo-labels. Our method outperforms the state-of-the-art by a notable margin, especially on confusing and long-tailed classes. Code is available at \href{https://github.com/RainJamesY/DIDA}
</details>
<details>
<summary>摘要</summary>
自适应方法最近在域 adapted semantic segmentation 中脱颖而出，其中一个模型通过目标域 Pseudo-标签 进行训练。现有技术已经消除了域之间的噪声 Pseudo-标签，但仍然在边缘类划分器中遇到了错误 Pseudo-标签。在这篇论文中，我们解决了这个问题，我们提议一种双级互动 для域适应（DIDA）在semantic segmentation中。具体来说，我们要求不同的扩展视图（augmented views）中的同一个像素有不仅相似的类预测（semantic-level），还有类似的相似性关系与其他像素（instance-level）。由于不可能保持一个 dataset 中所有像素的特征，我们因此保持一个标注的实例银行，并使用动态更新策略来选择ively存储实例中的有用特征。此外，DIDA通过散布和聚集技术来进行交互，以重新生成更可靠的 Pseudo-标签。我们的方法与当前状态的较好，尤其是在混淆和长尾类上。代码可以在 \href{https://github.com/RainJamesY/DIDA} 上找到。
</details></li>
</ul>
<hr>
<h2 id="EmoSet-A-Large-scale-Visual-Emotion-Dataset-with-Rich-Attributes"><a href="#EmoSet-A-Large-scale-Visual-Emotion-Dataset-with-Rich-Attributes" class="headerlink" title="EmoSet: A Large-scale Visual Emotion Dataset with Rich Attributes"></a>EmoSet: A Large-scale Visual Emotion Dataset with Rich Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07961">http://arxiv.org/abs/2307.07961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyuan Yang, Qirui Huang, Tingting Ding, Dani Lischinski, Daniel Cohen-Or, Hui Huang<br>for:This paper aims to introduce a large-scale visual emotion dataset (EmoSet) with rich annotations to support research in visual emotion analysis and understanding.methods:The dataset is constructed by collecting images from social networks and artistic sources, and is annotated with 118,102 images and 14 emotion attributes, including brightness, colorfulness, scene type, object class, facial expression, and human action.results:EmoSet is five times larger than the largest existing dataset and is well balanced between different emotion categories, providing a valuable resource for researchers in the field of affective computing.<details>
<summary>Abstract</summary>
Visual Emotion Analysis (VEA) aims at predicting people's emotional responses to visual stimuli. This is a promising, yet challenging, task in affective computing, which has drawn increasing attention in recent years. Most of the existing work in this area focuses on feature design, while little attention has been paid to dataset construction. In this work, we introduce EmoSet, the first large-scale visual emotion dataset annotated with rich attributes, which is superior to existing datasets in four aspects: scale, annotation richness, diversity, and data balance. EmoSet comprises 3.3 million images in total, with 118,102 of these images carefully labeled by human annotators, making it five times larger than the largest existing dataset. EmoSet includes images from social networks, as well as artistic images, and it is well balanced between different emotion categories. Motivated by psychological studies, in addition to emotion category, each image is also annotated with a set of describable emotion attributes: brightness, colorfulness, scene type, object class, facial expression, and human action, which can help understand visual emotions in a precise and interpretable way. The relevance of these emotion attributes is validated by analyzing the correlations between them and visual emotion, as well as by designing an attribute module to help visual emotion recognition. We believe EmoSet will bring some key insights and encourage further research in visual emotion analysis and understanding. Project page: https://vcc.tech/EmoSet.
</details>
<details>
<summary>摘要</summary>
Visual Emotion Analysis (VEA) 目标是预测人们对视觉刺激的情感反应。这是一项有前途的、具有挑战性的任务，在情感计算领域内，在最近几年内受到了越来越多的关注。大多数现有的工作在这个领域都是特征设计方向，而忽略了数据建构。在这项工作中，我们介绍了Emoset，第一个大规模的视觉情感数据集，其中包含330万个图像，其中118,102个图像被人类标注员仔细标注，比现有最大的数据集大五倍。Emoset包含社交媒体图像以及艺术图像，并且具有良好的各种情感类别的均衡。受精神学研究 inspirited，每个图像还被标注了一组可见的情感特征：明亮度、颜色彩强、场景类型、物体类型、表情和人类动作，这些特征可以帮助理解视觉情感的精确和可读性。这些情感特征的相关性被证明通过对它们与视觉情感之间的相关性分析，以及设计了一个特征模块来帮助视觉情感认知。我们认为Emoset将带来一些关键的发现，并促进视觉情感分析和理解的进一步研究。项目页面：https://vcc.tech/EmoSet。
</details></li>
</ul>
<hr>
<h2 id="Accurate-3D-Prediction-of-Missing-Teeth-in-Diverse-Patterns-for-Precise-Dental-Implant-Planning"><a href="#Accurate-3D-Prediction-of-Missing-Teeth-in-Diverse-Patterns-for-Precise-Dental-Implant-Planning" class="headerlink" title="Accurate 3D Prediction of Missing Teeth in Diverse Patterns for Precise Dental Implant Planning"></a>Accurate 3D Prediction of Missing Teeth in Diverse Patterns for Precise Dental Implant Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07953">http://arxiv.org/abs/2307.07953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Ma, Peng Xue, Yuning Gu, Yue Zhao, Min Zhu, Zhongxiang Ding, Dinggang Shen</li>
<li>For: 这个研究旨在提供一种准确预测缺失牙齿的框架，以便为牙齿嵌入式计划提供更好的规划和置入。* Methods: 该研究使用了一种基于CBCT图像的数据集来估算牙齿模型之间的点对点匹配关系，并将每种牙齿类型的位置和形状信息编码到牙齿词典中。然后，使用 sparse 表示法来学习缺失牙齿的邻近牙齿的位置和形状信息，并将这些信息应用到缺失牙齿的词典中来生成准确的预测结果。* Results: 研究结果表明，该提出的框架可以准确预测缺失牙齿的位置和形状，其预测误差为1.04mm和1.33mm分别对于单个缺失牙齿和14个缺失牙齿的预测。这表明该框架可以准确预测缺失牙齿在不同的模式下。<details>
<summary>Abstract</summary>
In recent years, the demand for dental implants has surged, driven by their high success rates and esthetic advantages. However, accurate prediction of missing teeth for precise digital implant planning remains a challenge due to the intricate nature of dental structures and the variability in tooth loss patterns. This study presents a novel framework for accurate prediction of missing teeth in different patterns, facilitating digital implant planning. The proposed framework begins by estimating point-to-point correspondence among a dataset of dental mesh models reconstructed from CBCT images of healthy subjects. Subsequently, tooth dictionaries are constructed for each tooth type, encoding their position and shape information based on the established point-to-point correspondence. To predict missing teeth in a given dental mesh model, sparse coefficients are learned by sparsely representing adjacent teeth of the missing teeth using the corresponding tooth dictionaries. These coefficients are then applied to the dictionaries of the missing teeth to generate accurate predictions of their positions and shapes. The evaluation results on real subjects shows that our proposed framework achieves an average prediction error of 1.04mm for predictions of single missing tooth and an average prediction error of 1.33mm for the prediction of 14 missing teeth, which demonstrates its capability of accurately predicting missing teeth in various patterns. By accurately predicting missing teeth, dental professionals can improve the planning and placement of dental implants, leading to better esthetic and functional outcomes for patients undergoing dental implant procedures.
</details>
<details>
<summary>摘要</summary>
Recently, the demand for dental implants has increased significantly due to their high success rates and aesthetic advantages. However, accurately predicting missing teeth for precise digital implant planning remains a challenge due to the complexity of dental structures and the variability of tooth loss patterns. This study proposes a novel framework for accurately predicting missing teeth in different patterns, facilitating digital implant planning.The proposed framework begins by estimating point-to-point correspondence among a dataset of dental mesh models reconstructed from CBCT images of healthy subjects. Next, tooth dictionaries are constructed for each tooth type, encoding their position and shape information based on the established point-to-point correspondence. To predict missing teeth in a given dental mesh model, sparse coefficients are learned by sparsely representing adjacent teeth of the missing teeth using the corresponding tooth dictionaries. These coefficients are then applied to the dictionaries of the missing teeth to generate accurate predictions of their positions and shapes.The evaluation results on real subjects show that our proposed framework achieves an average prediction error of 1.04mm for predictions of single missing teeth and an average prediction error of 1.33mm for the prediction of 14 missing teeth, which demonstrates its ability to accurately predict missing teeth in various patterns. By accurately predicting missing teeth, dental professionals can improve the planning and placement of dental implants, leading to better esthetic and functional outcomes for patients undergoing dental implant procedures.
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Distributed-ML-Training-via-Selective-Synchronization"><a href="#Accelerating-Distributed-ML-Training-via-Selective-Synchronization" class="headerlink" title="Accelerating Distributed ML Training via Selective Synchronization"></a>Accelerating Distributed ML Training via Selective Synchronization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07950">http://arxiv.org/abs/2307.07950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahil Tyagi, Martin Swany</li>
<li>for: 这篇论文的目的是提出一种实用、低开销的深度神经网络（DNNs）训练方法，以提高分布式训练中的效率。</li>
<li>methods: 这篇论文使用的方法包括：	+ 精确地决定在每步的统计聚合是否有必要，以避免高交互成本的统计聚合所带来的过度负担。	+ 在不同的训练案例中，适当地调整统计聚合的频率，以便在训练时间中实现最佳的对照。	+ 提出了多种优化方法，以提高在半同步训练中的融合。</li>
<li>results: 这篇论文的结果显示，使用\texttt{SelSync}方法可以实现与BSP训练相同或更好的精度，而且可以降低训练时间，对应的缩减比例为14倍。<details>
<summary>Abstract</summary>
In distributed training, deep neural networks (DNNs) are launched over multiple workers concurrently and aggregate their local updates on each step in bulk-synchronous parallel (BSP) training. However, BSP does not linearly scale-out due to high communication cost of aggregation. To mitigate this overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous Parallel (SSP) either reduce synchronization frequency or eliminate it altogether, usually at the cost of lower final accuracy. In this paper, we present \texttt{SelSync}, a practical, low-overhead method for DNN training that dynamically chooses to incur or avoid communication at each step either by calling the aggregation op or applying local updates based on their significance. We propose various optimizations as part of \texttt{SelSync} to improve convergence in the context of \textit{semi-synchronous} training. Our system converges to the same or better accuracy than BSP while reducing training time by up to 14$\times$.
</details>
<details>
<summary>摘要</summary>
在分布式训练中，深度神经网络（DNN）被多个工作者同时启动，并在每次步骤中在大规模同步（BSP）训练中进行集中更新。然而，BSP不会线性扩展，因为聚合成本过高。为了缓解这个开销，有些alternatives如联邦平均（FedAvg）和停顿同步并行（SSP）可以减少同步频率，或者完全消除同步，通常是在牺牲最终准确性的代价。在这篇论文中，我们提出了\texttt{SelSync}，一种实用、低开销的DNN训练方法，可以在每次步骤中动态决定是否进行聚合或者应用本地更新，根据它们的重要性。我们还提出了多种优化，以提高在半同步训练中的整合。我们的系统可以与BSP相比，提高训练效率，同时保持最终准确性。
</details></li>
</ul>
<hr>
<h2 id="Language-Conditioned-Traffic-Generation"><a href="#Language-Conditioned-Traffic-Generation" class="headerlink" title="Language Conditioned Traffic Generation"></a>Language Conditioned Traffic Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07947">http://arxiv.org/abs/2307.07947</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, Philipp Kraehenbuehl</li>
<li>for: 这篇论文主要用于解决现代自动驾驶开发中的模拟问题，即创建真实、可扩展、具有吸引力的交通场景。</li>
<li>methods: 该论文使用语言作为交通场景生成的超visory工具， combines a large language model with a transformer-based decoder architecture，从数据集中选择可能的地图位置，并生成初始的交通分布和车辆的动态。</li>
<li>results:  compared to prior work，LCTGen模型在无条件和条件交通场景生成中显示出了更高的实际性和准确性。<details>
<summary>Abstract</summary>
Simulation forms the backbone of modern self-driving development. Simulators help develop, test, and improve driving systems without putting humans, vehicles, or their environment at risk. However, simulators face a major challenge: They rely on realistic, scalable, yet interesting content. While recent advances in rendering and scene reconstruction make great strides in creating static scene assets, modeling their layout, dynamics, and behaviors remains challenging. In this work, we turn to language as a source of supervision for dynamic traffic scene generation. Our model, LCTGen, combines a large language model with a transformer-based decoder architecture that selects likely map locations from a dataset of maps, and produces an initial traffic distribution, as well as the dynamics of each vehicle. LCTGen outperforms prior work in both unconditional and conditional traffic scene generation in terms of realism and fidelity. Code and video will be available at https://ariostgx.github.io/lctgen.
</details>
<details>
<summary>摘要</summary>
现代自动驾驶发展的核心是模拟。模拟器帮助开发、测试和改进驾驶系统，而不会对人类、车辆或环境造成危险。然而，模拟器遇到一个主要挑战：它们需要真实、可扩展、又有趣的内容。而最近的渲染和场景重建技术已经做出了很大的进步，但是模拟场景的布局、动态和行为仍然是一个挑战。在这项工作中，我们寻求语言作为模拟场景生成的超级视图。我们的模型LCTGen结合了一个大型语言模型和一个基于转换器的解码器架构，从数据集中选择可能的地图位置，并生成初始的交通分布以及每辆车辆的动力学。LCTGen在无条件和条件交通场景生成方面比前一代的工作更高效和更真实。代码和视频将在https://ariostgx.github.io/lctgen上提供。
</details></li>
</ul>
<hr>
<h2 id="Surface-Geometry-Processing-An-Efficient-Normal-based-Detail-Representation"><a href="#Surface-Geometry-Processing-An-Efficient-Normal-based-Detail-Representation" class="headerlink" title="Surface Geometry Processing: An Efficient Normal-based Detail Representation"></a>Surface Geometry Processing: An Efficient Normal-based Detail Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07945">http://arxiv.org/abs/2307.07945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wuyuan Xie, Miaohui Wang, Di Lin, Boxin Shi, Jianmin Jiang</li>
<li>for: 本文提出了一种高效的Surface detail处理框架，用于解决高分辨率3D视觉应用中的传统方法具有较大的内存和计算时间成本。</li>
<li>methods: 本文提出了一种基于2D正常域的新的Surface detail处理方法，通过抽取新的正常特征表示来表示微geometry结构。文中both theoretically和empirically阐述了该表示的三个重要性能属性：细节分离、细节传输和细节幂等。</li>
<li>results: 对比现有状态的艺技，我们证明并示出了提议的正常基于表示的效果和多样性。在最新的benchmark dataset上，我们实现了理论分析和实验结果，证明了我们的正常基于表示的效果和多样性。在相同输入Surface vertices上，我们的方法只需6.5%的内存成本和14.0%的运行时间，相比现有竞争算法。<details>
<summary>Abstract</summary>
With the rapid development of high-resolution 3D vision applications, the traditional way of manipulating surface detail requires considerable memory and computing time. To address these problems, we introduce an efficient surface detail processing framework in 2D normal domain, which extracts new normal feature representations as the carrier of micro geometry structures that are illustrated both theoretically and empirically in this article. Compared with the existing state of the arts, we verify and demonstrate that the proposed normal-based representation has three important properties, including detail separability, detail transferability and detail idempotence. Finally, three new schemes are further designed for geometric surface detail processing applications, including geometric texture synthesis, geometry detail transfer, and 3D surface super-resolution. Theoretical analysis and experimental results on the latest benchmark dataset verify the effectiveness and versatility of our normal-based representation, which accepts 30 times of the input surface vertices but at the same time only takes 6.5% memory cost and 14.0% running time in comparison with existing competing algorithms.
</details>
<details>
<summary>摘要</summary>
Traditional high-resolution 3D vision applications 的面精度处理方式很快发展，但这些方法需要大量的内存和计算时间。为解决这些问题，我们介绍了一种高效的面精度处理框架，该框架在2D正常域中提取了新的正常特征表示，这些表示包括微geometry结构的示例，我们在这篇文章中对其进行了理论和实验 validate。与现有的状态艺术相比，我们的正常基于表示具有三个重要特性，包括细节分离、细节传输和细节幂等。最后，我们针对几种几种 геометри�结构细节处理应用程序设计了三种新方案，包括几何�xture生成、细节传输和3D surface超分辨率。我们的正常基于表示在对最新的benchmark数据进行了理论分析和实验验证，其可以接受30倍的输入面Vertex，但同时只需6.5%的内存成本和14.0%的计算时间，与现有的竞争算法相比。
</details></li>
</ul>
<hr>
<h2 id="CVSformer-Cross-View-Synthesis-Transformer-for-Semantic-Scene-Completion"><a href="#CVSformer-Cross-View-Synthesis-Transformer-for-Semantic-Scene-Completion" class="headerlink" title="CVSformer: Cross-View Synthesis Transformer for Semantic Scene Completion"></a>CVSformer: Cross-View Synthesis Transformer for Semantic Scene Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07938">http://arxiv.org/abs/2307.07938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Dong, Enhui Ma, Lubo Wang, Miaohui Wang, Wuyuan Xie, Qing Guo, Ping Li, Lingyu Liang, Kairui Yang, Di Lin<br>for:CVSformer is proposed to improve semantic scene completion by learning cross-view object relationships.methods:CVSformer consists of Multi-View Feature Synthesis and Cross-View Transformer to learn cross-view object relationships.results:CVSformer achieves state-of-the-art results on public datasets.<details>
<summary>Abstract</summary>
Semantic scene completion (SSC) requires an accurate understanding of the geometric and semantic relationships between the objects in the 3D scene for reasoning the occluded objects. The popular SSC methods voxelize the 3D objects, allowing the deep 3D convolutional network (3D CNN) to learn the object relationships from the complex scenes. However, the current networks lack the controllable kernels to model the object relationship across multiple views, where appropriate views provide the relevant information for suggesting the existence of the occluded objects. In this paper, we propose Cross-View Synthesis Transformer (CVSformer), which consists of Multi-View Feature Synthesis and Cross-View Transformer for learning cross-view object relationships. In the multi-view feature synthesis, we use a set of 3D convolutional kernels rotated differently to compute the multi-view features for each voxel. In the cross-view transformer, we employ the cross-view fusion to comprehensively learn the cross-view relationships, which form useful information for enhancing the features of individual views. We use the enhanced features to predict the geometric occupancies and semantic labels of all voxels. We evaluate CVSformer on public datasets, where CVSformer yields state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose Cross-View Synthesis Transformer (CVSformer), which consists of Multi-View Feature Synthesis and Cross-View Transformer for learning cross-view object relationships. In Multi-View Feature Synthesis, we use a set of 3D convolutional kernels rotated differently to compute multi-view features for each voxel. In Cross-View Transformer, we employ cross-view fusion to comprehensively learn cross-view relationships, which form useful information for enhancing the features of individual views. We use the enhanced features to predict the geometric occupancies and semantic labels of all voxels.We evaluate CVSformer on public datasets, where CVSformer yields state-of-the-art results.
</details></li>
</ul>
<hr>
<h2 id="S2R-ViT-for-Multi-Agent-Cooperative-Perception-Bridging-the-Gap-from-Simulation-to-Reality"><a href="#S2R-ViT-for-Multi-Agent-Cooperative-Perception-Bridging-the-Gap-from-Simulation-to-Reality" class="headerlink" title="S2R-ViT for Multi-Agent Cooperative Perception: Bridging the Gap from Simulation to Reality"></a>S2R-ViT for Multi-Agent Cooperative Perception: Bridging the Gap from Simulation to Reality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07935">http://arxiv.org/abs/2307.07935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinlong Li, Runsheng Xu, Xinyu Liu, Baolu Li, Qin Zou, Jiaqi Ma, Hongkai Yu</li>
<li>for: 本研究旨在解决现有多智能体协同感知算法在实际世界中表现不佳的问题，即在实际数据上下文中，由于 simulate 数据和实际数据之间的域差异，导致模型在实际世界中的感知性能下降。</li>
<li>methods: 本研究提出了一种首次在多智能体协同感知中实现了 sim2real 转移学习框架，通过一种新的视力变换器（ViT），并考虑了实施差（Implementation Gap）和特征差（Feature Gap）两种域差。为了有效地 relief 实施差，我们提出了一种不确定性感知器，并通过在egosensor和inter-sensor之间进行特征适应而减少特征差。</li>
<li>results: 我们在公共多智能体协同感知数据集OPV2V和V2V4Real上进行了广泛的实验，结果表明，提出的S2R-ViT可以有效地跨越实际和模拟之间的域差，并在点云基于3D物体检测方面表现出色，至于其他方法。<details>
<summary>Abstract</summary>
Due to the lack of real multi-agent data and time-consuming of labeling, existing multi-agent cooperative perception algorithms usually select the simulated sensor data for training and validating. However, the perception performance is degraded when these simulation-trained models are deployed to the real world, due to the significant domain gap between the simulated and real data. In this paper, we propose the first Simulation-to-Reality transfer learning framework for multi-agent cooperative perception using a novel Vision Transformer, named as S2R-ViT, which considers both the Implementation Gap and Feature Gap between simulated and real data. We investigate the effects of these two types of domain gaps and propose a novel uncertainty-aware vision transformer to effectively relief the Implementation Gap and an agent-based feature adaptation module with inter-agent and ego-agent discriminators to reduce the Feature Gap. Our intensive experiments on the public multi-agent cooperative perception datasets OPV2V and V2V4Real demonstrate that the proposed S2R-ViT can effectively bridge the gap from simulation to reality and outperform other methods significantly for point cloud-based 3D object detection.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "multi-agent" is translated as "多智能" (duō zhìnéng)* "cooperative perception" is translated as "合作感知" (hézuò gǎnperce)* "simulation-trained models" is translated as "模拟训练模型" (móxī xùxíng módelì)* "real world" is translated as "实际世界" (shíjiè shìjiè)* "domain gap" is translated as "领域差距" (lánxìng jìnjù)* "Implementation Gap" is translated as "实现差距" (shíxiàn jìnjù)* "Feature Gap" is translated as "特征差距" (tèxí jìnjù)* "uncertainty-aware vision transformer" is translated as "不确定性意识感知变换器" (bù qièdìngxìng yìshì gǎnperce bianhuàng)* "agent-based feature adaptation module" is translated as "智能 Agent 基于特征修改模块" (zhìnéng agent jīyú yìxìng xiūgòu módèl)Note: The translation is based on Simplified Chinese, which is used in mainland China. If you need Traditional Chinese translation, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Multi-Task-Dense-Prediction"><a href="#Contrastive-Multi-Task-Dense-Prediction" class="headerlink" title="Contrastive Multi-Task Dense Prediction"></a>Contrastive Multi-Task Dense Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07934">http://arxiv.org/abs/2307.07934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/USTCPCS/CVPR2018_attention">https://github.com/USTCPCS/CVPR2018_attention</a></li>
<li>paper_authors: Siwei Yang, Hanrong Ye, Dan Xu</li>
<li>for: This paper addresses the problem of multi-task dense prediction, aiming to achieve simultaneous learning and inference on multiple dense prediction tasks in a single framework.</li>
<li>methods: The paper introduces feature-wise contrastive consistency to model cross-task interactions, which effectively boosts representation learning for different sub-tasks without extra expensive distillation modules.</li>
<li>results: The proposed multi-task contrastive learning approach achieves superior performance on two challenging datasets (NYUD-v2 and Pascal-Context), establishing new state-of-the-art results for dense predictions.<details>
<summary>Abstract</summary>
This paper targets the problem of multi-task dense prediction which aims to achieve simultaneous learning and inference on a bunch of multiple dense prediction tasks in a single framework. A core objective in design is how to effectively model cross-task interactions to achieve a comprehensive improvement on different tasks based on their inherent complementarity and consistency. Existing works typically design extra expensive distillation modules to perform explicit interaction computations among different task-specific features in both training and inference, bringing difficulty in adaptation for different task sets, and reducing efficiency due to clearly increased size of multi-task models. In contrast, we introduce feature-wise contrastive consistency into modeling the cross-task interactions for multi-task dense prediction. We propose a novel multi-task contrastive regularization method based on the consistency to effectively boost the representation learning of the different sub-tasks, which can also be easily generalized to different multi-task dense prediction frameworks, and costs no additional computation in the inference. Extensive experiments on two challenging datasets (i.e. NYUD-v2 and Pascal-Context) clearly demonstrate the superiority of the proposed multi-task contrastive learning approach for dense predictions, establishing new state-of-the-art performances.
</details>
<details>
<summary>摘要</summary>
In contrast, we introduce feature-wise contrastive consistency to model cross-task interactions for multi-task dense prediction. We propose a novel multi-task contrastive regularization method based on consistency to effectively boost representation learning of different sub-tasks, which can be easily generalized to different multi-task dense prediction frameworks and does not require additional computation in inference. Extensive experiments on two challenging datasets (NYUD-v2 and Pascal-Context) demonstrate the superiority of the proposed multi-task contrastive learning approach for dense predictions, establishing new state-of-the-art performances.
</details></li>
</ul>
<hr>
<h2 id="Holistic-Prototype-Attention-Network-for-Few-Shot-VOS"><a href="#Holistic-Prototype-Attention-Network-for-Few-Shot-VOS" class="headerlink" title="Holistic Prototype Attention Network for Few-Shot VOS"></a>Holistic Prototype Attention Network for Few-Shot VOS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07933">http://arxiv.org/abs/2307.07933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nust-machine-intelligence-laboratory/hpan">https://github.com/nust-machine-intelligence-laboratory/hpan</a></li>
<li>paper_authors: Yin Tang, Tao Chen, Xiruo Jiang, Yazhou Yao, Guo-Sen Xie, Heng-Tao Shen</li>
<li>for: 提高ew-shot video对象分割（FSVOS）中的动态对象分割精度，通过小量支持图像进行权重学习。</li>
<li>methods: 我们提出了一种整体prototype注意网络（HPAN），它包括 prototype graph注意模块（PGAM）和对称prototype注意模块（BPAM），通过将有用知识传递到未经见类别上来提高分割性能。</li>
<li>results: 我们在YouTube-FSVOS上进行了广泛的实验，并证明了我们提出的HPAN方法的有效性和优越性。<details>
<summary>Abstract</summary>
Few-shot video object segmentation (FSVOS) aims to segment dynamic objects of unseen classes by resorting to a small set of support images that contain pixel-level object annotations. Existing methods have demonstrated that the domain agent-based attention mechanism is effective in FSVOS by learning the correlation between support images and query frames. However, the agent frame contains redundant pixel information and background noise, resulting in inferior segmentation performance. Moreover, existing methods tend to ignore inter-frame correlations in query videos. To alleviate the above dilemma, we propose a holistic prototype attention network (HPAN) for advancing FSVOS. Specifically, HPAN introduces a prototype graph attention module (PGAM) and a bidirectional prototype attention module (BPAM), transferring informative knowledge from seen to unseen classes. PGAM generates local prototypes from all foreground features and then utilizes their internal correlations to enhance the representation of the holistic prototypes. BPAM exploits the holistic information from support images and video frames by fusing co-attention and self-attention to achieve support-query semantic consistency and inner-frame temporal consistency. Extensive experiments on YouTube-FSVOS have been provided to demonstrate the effectiveness and superiority of our proposed HPAN method.
</details>
<details>
<summary>摘要</summary>
“几帧影像物类分割（FSVOS）目的是将无法见的类别中的动态物类分割，通过一小集支持影像，其中包含像素级别物类标注。现有方法已经证明，对FSVOS使用域间代理机制可以将支持影像和询问帧之间建立相互关联。然而，代理帧中含有重复的像素信息和背景噪音，导致分割性能不佳。此外，现有方法往往忽略了询问影像之间的相互关联。为解决以上问题，我们提出了整体原型注意网络（HPAN），以提高FSVOS的性能。具体来说，HPAN包括一个原型图像注意模组（PGAM）和一个双向原型注意模组（BPAM），将有用的知识传递自见到未见的类别。PGAM从所有前景特征中生成本地区prototype，然后利用这些内部相关性来强化整体prototype的表现。BPAM利用支持影像和询问影像之间的共同关联和自我关联，实现支持询问semantic一致和内部时间一致。我们在YouTube-FSVOS上进行了广泛的实验，以证明我们提出的HPAN方法的有效性和superiority。”
</details></li>
</ul>
<hr>
<h2 id="DocTr-Document-Transformer-for-Structured-Information-Extraction-in-Documents"><a href="#DocTr-Document-Transformer-for-Structured-Information-Extraction-in-Documents" class="headerlink" title="DocTr: Document Transformer for Structured Information Extraction in Documents"></a>DocTr: Document Transformer for Structured Information Extraction in Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07929">http://arxiv.org/abs/2307.07929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haofu Liao, Aruni RoyChowdhury, Weijian Li, Ankan Bansal, Yuting Zhang, Zhuowen Tu, Ravi Kumar Satzoda, R. Manmatha, Vijay Mahadevan</li>
<li>for: 提出了一种新的结构化信息抽取（SIE）方法，用于从视觉 ric 文档中提取结构化信息。</li>
<li>methods: 使用了 anchor-based 对象检测器的想法，将实体表示为 anchor word 和 bounding box，并表示实体关联为 anchor word 的关联。</li>
<li>results: 评估在三个 SIE  benchmark 上，提出的方法显示效果很好，并且在语言上进行 предваритель训练 后，能够学习实体检测。<details>
<summary>Abstract</summary>
We present a new formulation for structured information extraction (SIE) from visually rich documents. It aims to address the limitations of existing IOB tagging or graph-based formulations, which are either overly reliant on the correct ordering of input text or struggle with decoding a complex graph. Instead, motivated by anchor-based object detectors in vision, we represent an entity as an anchor word and a bounding box, and represent entity linking as the association between anchor words. This is more robust to text ordering, and maintains a compact graph for entity linking. The formulation motivates us to introduce 1) a DOCument TRansformer (DocTr) that aims at detecting and associating entity bounding boxes in visually rich documents, and 2) a simple pre-training strategy that helps learn entity detection in the context of language. Evaluations on three SIE benchmarks show the effectiveness of the proposed formulation, and the overall approach outperforms existing solutions.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的结构信息提取（SIE）方式，用于从视觉丰富文档中提取结构信息。该方式希望解决现有的IOB标注或图像基于的形ulation的限制，这些限制是 Either too reliant on the correct ordering of input text or struggle with decoding a complex graph. 而是，我们将实体表示为一个锚字和一个矩形框，并将实体连接视为锚字之间的关系。这种方式更加鲁棒地对text的顺序，并保持了compact的图像 для实体连接。这种方式的提出使我们引入了以下两个方法：1. DOCument TRansformer (DocTr)，用于在视觉丰富文档中检测和关联实体矩形框。2. 一种简单的预训练策略，用于在语言上学习实体检测。我们对三个SIE benchmark进行了评估，结果显示了提posed方式的效果，并且总的approach exceeds existing solutions。
</details></li>
</ul>
<hr>
<h2 id="Reinforced-Disentanglement-for-Face-Swapping-without-Skip-Connection"><a href="#Reinforced-Disentanglement-for-Face-Swapping-without-Skip-Connection" class="headerlink" title="Reinforced Disentanglement for Face Swapping without Skip Connection"></a>Reinforced Disentanglement for Face Swapping without Skip Connection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07928">http://arxiv.org/abs/2307.07928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alaist/RD-FS">https://github.com/alaist/RD-FS</a></li>
<li>paper_authors: Xiaohang Ren, Xingyu Chen, Pengfei Yao, Heung-Yeung Shum, Baoyuan Wang</li>
<li>for: 解决SOTA face swap模型中的人脸特征泄露和非人脸特征失去Problem</li>
<li>methods: 引入新的face swap框架’WSC-swap’, eliminating skip connections and using two target encoders to respectively capture pixel-level non-facial region attributes and semantic non-identity attributes in the face region. Plus, employing both identity removal loss via adversarial training and non-identity preservation loss via prior 3DMM models.</li>
<li>results: 对FaceForensics++和CelebA-HQ进行了广泛的实验，比较表现出色，包括一个新的指标 для测试人脸一致性，这个指标之前完全被忽略了。<details>
<summary>Abstract</summary>
The SOTA face swap models still suffer the problem of either target identity (i.e., shape) being leaked or the target non-identity attributes (i.e., background, hair) failing to be fully preserved in the final results. We show that this insufficient disentanglement is caused by two flawed designs that were commonly adopted in prior models: (1) counting on only one compressed encoder to represent both the semantic-level non-identity facial attributes(i.e., pose) and the pixel-level non-facial region details, which is contradictory to satisfy at the same time; (2) highly relying on long skip-connections between the encoder and the final generator, leaking a certain amount of target face identity into the result. To fix them, we introduce a new face swap framework called 'WSC-swap' that gets rid of skip connections and uses two target encoders to respectively capture the pixel-level non-facial region attributes and the semantic non-identity attributes in the face region. To further reinforce the disentanglement learning for the target encoder, we employ both identity removal loss via adversarial training (i.e., GAN) and the non-identity preservation loss via prior 3DMM models like [11]. Extensive experiments on both FaceForensics++ and CelebA-HQ show that our results significantly outperform previous works on a rich set of metrics, including one novel metric for measuring identity consistency that was completely neglected before.
</details>
<details>
<summary>摘要</summary>
现状的SOTA面 swap模型仍然受到两种问题的困扰：一是目标人脸特征（即形状）泄露，二是目标非人脸特征（如背景和 волосы）在最终结果中未能完全保留。我们显示出这种不足的分离是由两种不当的设计引起的：（1）通过单一压缩编码器来表示面部非人脸特征（即姿势）和像素级非人脸地方特征，这是不可能同时满足的；（2）高度依赖长跳转连接来传递编码器到最终生成器，这会带来一定程度的目标人脸标识泄露。为了解决这些问题，我们提出了一个新的面 swap框架 called 'WSC-swap'，它 eliminates skip connections and uses two target encoders to respectively capture the pixel-level non-facial region attributes and the semantic non-identity attributes in the face region. 为了进一步加强目标编码器的分离学习，我们采用了both identity removal loss via adversarial training（i.e., GAN）和非人脸保持损失 via prior 3DMM models like [11]. Our extensive experiments on both FaceForensics++ and CelebA-HQ show that our results significantly outperform previous works on a rich set of metrics, including one novel metric for measuring identity consistency that was completely neglected before.
</details></li>
</ul>
<hr>
<h2 id="RayMVSNet-Learning-Ray-based-1D-Implicit-Fields-for-Accurate-Multi-View-Stereo"><a href="#RayMVSNet-Learning-Ray-based-1D-Implicit-Fields-for-Accurate-Multi-View-Stereo" class="headerlink" title="RayMVSNet++: Learning Ray-based 1D Implicit Fields for Accurate Multi-View Stereo"></a>RayMVSNet++: Learning Ray-based 1D Implicit Fields for Accurate Multi-View Stereo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10233">http://arxiv.org/abs/2307.10233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Shi, Junhua Xi, Dewen Hu, Zhiping Cai, Kai Xu</li>
<li>for: 这个论文的目的是提出一种基于学习的多视图静止（MVS）方法，以提高多视图静止的精度和效率。</li>
<li>methods: 这个方法使用了直接优化每个摄像头光束上的深度值，模拟激光激光仪的距离测量。具体来说，它使用了序列预测方法，通过转换器特征来学习Sequential Modeling，实际上是多视图静止中的epipolar线搜索。</li>
<li>results: 这个方法在DTU和Tanks &amp; Temples数据集上达到了所有前一代学习基于方法的总重建得分0.33mm和F-score59.48%。它能够在复杂的景象中提供高质量的深度估计和点云重建。此外，提出了RayMVSNet++以增强每个光束的上下文特征聚合，通过设计了一个注意力阀unit来选择在本地折射镜附近的相关的邻居光束。RayMVSNet++在ScanNet数据集上达到了状态艺术性能。<details>
<summary>Abstract</summary>
Learning-based multi-view stereo (MVS) has by far centered around 3D convolution on cost volumes. Due to the high computation and memory consumption of 3D CNN, the resolution of output depth is often considerably limited. Different from most existing works dedicated to adaptive refinement of cost volumes, we opt to directly optimize the depth value along each camera ray, mimicking the range finding of a laser scanner. This reduces the MVS problem to ray-based depth optimization which is much more light-weight than full cost volume optimization. In particular, we propose RayMVSNet which learns sequential prediction of a 1D implicit field along each camera ray with the zero-crossing point indicating scene depth. This sequential modeling, conducted based on transformer features, essentially learns the epipolar line search in traditional multi-view stereo. We devise a multi-task learning for better optimization convergence and depth accuracy. We found the monotonicity property of the SDFs along each ray greatly benefits the depth estimation. Our method ranks top on both the DTU and the Tanks & Temples datasets over all previous learning-based methods, achieving an overall reconstruction score of 0.33mm on DTU and an F-score of 59.48% on Tanks & Temples. It is able to produce high-quality depth estimation and point cloud reconstruction in challenging scenarios such as objects/scenes with non-textured surface, severe occlusion, and highly varying depth range. Further, we propose RayMVSNet++ to enhance contextual feature aggregation for each ray through designing an attentional gating unit to select semantically relevant neighboring rays within the local frustum around that ray. RayMVSNet++ achieves state-of-the-art performance on the ScanNet dataset. In particular, it attains an AbsRel of 0.058m and produces accurate results on the two subsets of textureless regions and large depth variation.
</details>
<details>
<summary>摘要</summary>
学习基于多视图涂抹（MVS）的方法主要集中在3D convolution中的成本量和存储占用。由于3D CNN的计算和存储占用非常高，输出深度的分辨率经常受到限制。与大多数现有的成本量优化方法不同，我们直接优化摄像头方向上的深度值，模拟激光雷达扫描器的范围找寻。这将MVS问题降低到折线基于深度优化，与全成本量优化相比许多轻量级。特别是，我们提出了RayMVSNet，它通过学习每个摄像头方向上的1D隐函数来预测折线上的深度值，并在扫描器范围内查找零交叉点。这种顺序模型化，基于变换器特征，实际上学习了传统多视图涂抹中的epipolar线搜索。我们设计了多任务学习来改进优化的吞吐量和深度准确率。我们发现折线上SDF的 monotonicity 性帮助深度估计。我们的方法在DTU和Tanks & Temples数据集上至今为止的所有学习基于方法中排名第一，实现了总重建分数为0.33mm（DTU）和59.48%（Tanks & Temples）。它能够在物体/场景中的非杂表面、严重遮挡和高度变化的深度范围中生成高质量的深度估计和点云重建。此外，我们提出了RayMVSNet++，它通过设计了注意力闭合单元来选择当地frustum中的相互 relevante的折线，从而增强每个折线的上下文特征汇集。RayMVSNet++在ScanNet数据集上达到了状态的最佳性能，其中包括Textureless Regions和大深度变化两个子集。
</details></li>
</ul>
<hr>
<h2 id="On-the-Robustness-of-Split-Learning-against-Adversarial-Attacks"><a href="#On-the-Robustness-of-Split-Learning-against-Adversarial-Attacks" class="headerlink" title="On the Robustness of Split Learning against Adversarial Attacks"></a>On the Robustness of Split Learning against Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07916">http://arxiv.org/abs/2307.07916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmy266/SplitADV">https://github.com/fmy266/SplitADV</a></li>
<li>paper_authors: Mingyuan Fan, Cen Chen, Chengyu Wang, Wenmeng Zhou, Jun Huang</li>
<li>for: 证明分学可以增强模型安全性，尤其是在敏感数据不能直接分享的情况下。</li>
<li>methods: 通过分享部分模型和计算结果，而不是直接分享敏感数据和模型细节，来实现模型训练和共享。</li>
<li>results: 研究发现，对于敏感数据，分学可以减少对模型的攻击，但是在中间层级上的攻击仍然存在，并且可以通过新的攻击方法（SPADV）来证明这一点。<details>
<summary>Abstract</summary>
Split learning enables collaborative deep learning model training while preserving data privacy and model security by avoiding direct sharing of raw data and model details (i.e., sever and clients only hold partial sub-networks and exchange intermediate computations). However, existing research has mainly focused on examining its reliability for privacy protection, with little investigation into model security. Specifically, by exploring full models, attackers can launch adversarial attacks, and split learning can mitigate this severe threat by only disclosing part of models to untrusted servers.This paper aims to evaluate the robustness of split learning against adversarial attacks, particularly in the most challenging setting where untrusted servers only have access to the intermediate layers of the model.Existing adversarial attacks mostly focus on the centralized setting instead of the collaborative setting, thus, to better evaluate the robustness of split learning, we develop a tailored attack called SPADV, which comprises two stages: 1) shadow model training that addresses the issue of lacking part of the model and 2) local adversarial attack that produces adversarial examples to evaluate.The first stage only requires a few unlabeled non-IID data, and, in the second stage, SPADV perturbs the intermediate output of natural samples to craft the adversarial ones. The overall cost of the proposed attack process is relatively low, yet the empirical attack effectiveness is significantly high, demonstrating the surprising vulnerability of split learning to adversarial attacks.
</details>
<details>
<summary>摘要</summary>
分学习可以实现合作深度学习模型训练，同时保护数据隐私和模型安全，因为服务器和客户端只保持部分子网和交换中间计算。然而，现有研究主要关注隐私保护的可靠性，很少研究模型安全。Specifically, by exploring全模型，攻击者可以发起对抗性攻击，并且分学习可以 Mitigate this severe threat by only disclosing部分模型 to untrusted servers。本文的目的是评估分学习对抗性攻击的Robustness，特别是在最具挑战性的设定下，即无法信任服务器仅有访问模型中间层。现有的对抗性攻击主要集中在中央设定中，而不是合作设定，因此，为更好地评估分学习的Robustness，我们开发了一种适应攻击，称为SPADV。SPADV包括两个阶段：1）阴影模型训练，解决了因为缺少部分模型而产生的问题，2）本地对抗性攻击，生成对抗性样本来评估。在第一阶段，只需几个非相关的非ID数据，而在第二阶段，SPADV在自然样本中 Output的中间部分进行了预处理，以生成对抗性样本。整个攻击过程的总成本相对较低， yet the empirical attack effectiveness is significantly high，表明了分学习对抗性攻击的Surprising vulnerability。
</details></li>
</ul>
<hr>
<h2 id="Predicting-mechanical-properties-of-Carbon-Nanotube-CNT-images-Using-Multi-Layer-Synthetic-Finite-Element-Model-Simulations"><a href="#Predicting-mechanical-properties-of-Carbon-Nanotube-CNT-images-Using-Multi-Layer-Synthetic-Finite-Element-Model-Simulations" class="headerlink" title="Predicting mechanical properties of Carbon Nanotube (CNT) images Using Multi-Layer Synthetic Finite Element Model Simulations"></a>Predicting mechanical properties of Carbon Nanotube (CNT) images Using Multi-Layer Synthetic Finite Element Model Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07912">http://arxiv.org/abs/2307.07912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaveh Safavigerdini, Koundinya Nouduri, Ramakrishna Surya, Andrew Reinhard, Zach Quinlan, Filiz Bunyak, Matthew R. Maschmann, Kannappan Palaniappan</li>
<li>For: The paper is written for predicting mechanical properties of vertically-oriented carbon nanotube (CNT) forest images using a deep learning model for artificial intelligence (AI)-based materials discovery.* Methods: The paper uses an innovative data augmentation technique that involves the use of multi-layer synthetic (MLS) or quasi-2.5D images, which are generated by blending 2D synthetic images. The paper also uses a physics-based model to estimate mechanical properties such as stiffness and buckling load for the MLS images. The proposed deep learning architecture, CNTNeXt, builds upon the previous CNTNet neural network, using a ResNeXt feature representation followed by random forest regression estimator.* Results: The paper expects the proposed machine learning approach to outperform single synthetic image-based learning when it comes to predicting mechanical properties of real scanning electron microscopy images. This has the potential to accelerate understanding and control of CNT forest self-assembly for diverse applications.Here are the three points in Simplified Chinese text:* For: 这篇论文是为了预测垂直方向碳纳米管（CNT）森林图像的机械性能使用深度学习模型进行人工智能（AI）基于材料发现。* Methods: 论文使用了一种创新的数据增强技术，利用多层合成（MLS）或 quasi-2.5D 图像，这些图像是通过拼接2D synthetic图像来生成。 MLs 图像更像真实的扫描电子显微镜（SEM）图像，但不需要进行expensive的3D simulations或实验。机械性能如强度和折倒荷重等被使用物理基本模型来估算。* Results: 论文预计该提出的机器学习方法会在真实SEM图像上预测机械性能时比单个synthetic图像基本学习更高效。这有可能加速CNT森林自组装的理解和控制，以推动多种应用。<details>
<summary>Abstract</summary>
We present a pipeline for predicting mechanical properties of vertically-oriented carbon nanotube (CNT) forest images using a deep learning model for artificial intelligence (AI)-based materials discovery. Our approach incorporates an innovative data augmentation technique that involves the use of multi-layer synthetic (MLS) or quasi-2.5D images which are generated by blending 2D synthetic images. The MLS images more closely resemble 3D synthetic and real scanning electron microscopy (SEM) images of CNTs but without the computational cost of performing expensive 3D simulations or experiments. Mechanical properties such as stiffness and buckling load for the MLS images are estimated using a physics-based model. The proposed deep learning architecture, CNTNeXt, builds upon our previous CNTNet neural network, using a ResNeXt feature representation followed by random forest regression estimator. Our machine learning approach for predicting CNT physical properties by utilizing a blended set of synthetic images is expected to outperform single synthetic image-based learning when it comes to predicting mechanical properties of real scanning electron microscopy images. This has the potential to accelerate understanding and control of CNT forest self-assembly for diverse applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一个管道，用于预测纵向碳纳米管（CNT）森林图像中的机械性能，使用深度学习模型，以实现人工智能（AI）基于材料发现。我们的方法包括一种创新的数据增强技术，使用多层合成（MLS）或 quasi-2.5D 图像，这些图像由混合2D 合成图像来生成。MLS 图像更加closely resemble 3D 合成和实验室扫描电子镜像（SEM）图像，但没有 computationally expensive 3D  simulations 或实验的成本。机械性能，如刚性和塌笔荷，对 MLS 图像进行估算，使用物理基础模型。我们提出的深度学习架构，CNTNeXt，基于我们之前的 CNTNet 神经网络，使用 ResNeXt 特征表示，然后使用随机森林回归估计器。我们的机器学习方法，通过使用混合的合成图像来预测 CNT 物理性能，对于实验室扫描电子镜像图像的机械性能预测，具有更高的性能，相比单个合成图像基于学习。这有助于加速理解和控制 CNT 森林自组装的应用。
</details></li>
</ul>
<hr>
<h2 id="Multitemporal-SAR-images-change-detection-and-visualization-using-RABASAR-and-simplified-GLR"><a href="#Multitemporal-SAR-images-change-detection-and-visualization-using-RABASAR-and-simplified-GLR" class="headerlink" title="Multitemporal SAR images change detection and visualization using RABASAR and simplified GLR"></a>Multitemporal SAR images change detection and visualization using RABASAR and simplified GLR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07892">http://arxiv.org/abs/2307.07892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiying Zhao, Charles-Alban Deledalle, Loïc Denis, Henri Maître, Jean-Marie Nicolas, Florence Tupin</li>
<li>for: 本研究旨在检测不同类型的地表变化，以便对地面监测进行更好的准备。</li>
<li>methods: 本研究提出了一种简化了通量比（SGLR）方法，假设同时间像素具有相同的等效数目（ENL）。此外，还开发了一种改进的光谱划分法以确定变化类型。</li>
<li>results: 研究人员通过处理模拟和SAR图像，并与传统方法进行比较，证明了提案方法的效果性。特别是，数值实验表明，该方法可以有效地检测农田地区变化、建筑地区变化、港区变化和洪涝变化。<details>
<summary>Abstract</summary>
Understanding the state of changed areas requires that precise information be given about the changes. Thus, detecting different kinds of changes is important for land surface monitoring. SAR sensors are ideal to fulfil this task, because of their all-time and all-weather capabilities, with good accuracy of the acquisition geometry and without effects of atmospheric constituents for amplitude data. In this study, we propose a simplified generalized likelihood ratio ($S_{GLR}$) method assuming that corresponding temporal pixels have the same equivalent number of looks (ENL). Thanks to the denoised data provided by a ratio-based multitemporal SAR image denoising method (RABASAR), we successfully applied this similarity test approach to compute the change areas. A new change magnitude index method and an improved spectral clustering-based change classification method are also developed. In addition, we apply the simplified generalized likelihood ratio to detect the maximum change magnitude time, and the change starting and ending times. Then, we propose to use an adaptation of the REACTIV method to visualize the detection results vividly. The effectiveness of the proposed methods is demonstrated through the processing of simulated and SAR images, and the comparison with classical techniques. In particular, numerical experiments proved that the developed method has good performances in detecting farmland area changes, building area changes, harbour area changes and flooding area changes.
</details>
<details>
<summary>摘要</summary>
理解改变区域的状态需要提供精确的改变信息。因此，检测不同类型的改变是重要的 для地面监测。SAR探测器是理想的选择，因为它们在任何时间和天气条件下都有good accuracy的获取geometry和无 atmospheric constituents的影响。在这项研究中，我们提出了一种简化了通用类比比率（SGLR）方法，假设同一时间的批量像素具有相同的等效数量looks（ENL）。经过了 ratio-based multitemporal SAR图像减噪方法（RABASAR）提供的净化数据，我们成功地应用了这种相似测试方法来计算改变区域。此外，我们还开发了一种改进的光谱分类法来分类改变，以及一种最大变化强度时间、改变开始和结束时间的检测方法。最后，我们使用了一种基于REACTIV方法的修改来可见化检测结果。我们通过处理 simulated和SAR图像，以及与传统技术进行比较，证明了我们提出的方法的有效性。具体来说，数值实验表明，我们的方法在检测农田改变、建筑改变、港口改变和洪涝改变方面具有良好的表现。
</details></li>
</ul>
<hr>
<h2 id="Why-Does-Little-Robustness-Help-Understanding-Adversarial-Transferability-From-Surrogate-Training"><a href="#Why-Does-Little-Robustness-Help-Understanding-Adversarial-Transferability-From-Surrogate-Training" class="headerlink" title="Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training"></a>Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07873">http://arxiv.org/abs/2307.07873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yechao Zhang, Shengshan Hu, Leo Yu Zhang, Junyu Shi, Minghui Li, Xiaogeng Liu, Wei Wan, Hai Jin</li>
<li>for: 本研究的目的是解释对深度神经网络（DNNs）的抗击例（Adversarial Examples，AE）的传播性。</li>
<li>methods: 本研究使用了一系列的理论和实验分析，以了解在对DNNs进行对抗性训练时，模型的平滑性和梯度相似性之间的负相关性。</li>
<li>results: 研究发现，在对DNNs进行对抗性训练时，数据分布shift和梯度相似性之间存在负相关性。此外，研究还发现，通过控制数据分布shift和梯度regularization，可以构建更好的代理模型，以提高对抗性训练的传播性。<details>
<summary>Abstract</summary>
Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in adversarial training explains the degradation of gradient similarity. Building on these insights, we explore the impacts of data augmentation and gradient regularization on transferability and identify that the trade-off generally exists in the various training mechanisms, thus building a comprehensive blueprint for the regulation mechanism behind transferability. Finally, we provide a general route for constructing better surrogates to boost transferability which optimizes both model smoothness and gradient similarity simultaneously, e.g., the combination of input gradient regularization and sharpness-aware minimization (SAM), validated by extensive experiments. In summary, we call for attention to the united impacts of these two factors for launching effective transfer attacks, rather than optimizing one while ignoring the other, and emphasize the crucial role of manipulating surrogate models.
</details>
<details>
<summary>摘要</summary>
adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in adversarial training explains the degradation of gradient similarity. Building on these insights, we explore the impacts of data augmentation and gradient regularization on transferability and identify that the trade-off generally exists in the various training mechanisms, thus building a comprehensive blueprint for the regulation mechanism behind transferability. Finally, we provide a general route for constructing better surrogates to boost transferability which optimizes both model smoothness and gradient similarity simultaneously, e.g., the combination of input gradient regularization and sharpness-aware minimization (SAM), validated by extensive experiments. In summary, we call for attention to the united impacts of these two factors for launching effective transfer attacks, rather than optimizing one while ignoring the other, and emphasize the crucial role of manipulating surrogate models.
</details></li>
</ul>
<hr>
<h2 id="Unified-Adversarial-Patch-for-Cross-modal-Attacks-in-the-Physical-World"><a href="#Unified-Adversarial-Patch-for-Cross-modal-Attacks-in-the-Physical-World" class="headerlink" title="Unified Adversarial Patch for Cross-modal Attacks in the Physical World"></a>Unified Adversarial Patch for Cross-modal Attacks in the Physical World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07859">http://arxiv.org/abs/2307.07859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingxing Wei, Yao Huang, Yitong Sun, Jie Yu<br>for:  This paper aims to demonstrate the potential risks of physical adversarial attacks on object detectors that use both visible and infrared sensors.methods:  The authors propose a unified adversarial patch that can fool both visible and infrared object detectors simultaneously, using a single patch. They design a novel boundary-limited shape optimization method to achieve compact and smooth shapes, and propose a score-aware iterative evaluation to balance the fooling degree between the two sensors.results:  The authors achieve an Attack Success Rate (ASR) of 73.33% and 69.17% against one-stage and two-stage object detectors, respectively. They also verify the effective attacks in the physical world under various settings, such as different angles, distances, postures, and scenes.<details>
<summary>Abstract</summary>
Recently, physical adversarial attacks have been presented to evade DNNs-based object detectors. To ensure the security, many scenarios are simultaneously deployed with visible sensors and infrared sensors, leading to the failures of these single-modal physical attacks. To show the potential risks under such scenes, we propose a unified adversarial patch to perform cross-modal physical attacks, i.e., fooling visible and infrared object detectors at the same time via a single patch. Considering different imaging mechanisms of visible and infrared sensors, our work focuses on modeling the shapes of adversarial patches, which can be captured in different modalities when they change. To this end, we design a novel boundary-limited shape optimization to achieve the compact and smooth shapes, and thus they can be easily implemented in the physical world. In addition, to balance the fooling degree between visible detector and infrared detector during the optimization process, we propose a score-aware iterative evaluation, which can guide the adversarial patch to iteratively reduce the predicted scores of the multi-modal sensors. We finally test our method against the one-stage detector: YOLOv3 and the two-stage detector: Faster RCNN. Results show that our unified patch achieves an Attack Success Rate (ASR) of 73.33% and 69.17%, respectively. More importantly, we verify the effective attacks in the physical world when visible and infrared sensors shoot the objects under various settings like different angles, distances, postures, and scenes.
</details>
<details>
<summary>摘要</summary>
最近，物理攻击被提出以逃脱基于DNN的物体检测器。为确保安全，许多场景同时使用可见感知器和红外感知器，导致单模态物理攻击失败。为了表明这些场景下的风险，我们提议一种横跨模态物理攻击，即通过单个贴图 Fooled 可见和红外检测器。considering 不同的感知机制，我们的工作专注于模型适应器形状的设计，这些形状可以在不同的感知器下被捕捉。为此，我们设计了一种 novel 边界限定的形状优化方法，以实现紧凑和平滑的形状，这些形状可以轻松地在物理世界中实现。此外，为保证可见检测器和红外检测器在优化过程中的攻击度差异，我们提议一种分数感知迭代评估，可以导引攻击贴图在迭代过程中逐渐减少多模检测器的预测分数。最后，我们对 YOLOv3 和 Faster RCNN 进行测试，结果显示我们的横跨模态贴图可以在不同的角度、距离、姿态和场景下实现73.33% 和 69.17% 的攻击成功率。更重要的是，我们在物理世界中验证了这些攻击的有效性，当可见和红外感知器在不同的设置下拍摄对象时。
</details></li>
</ul>
<hr>
<h2 id="Neural-Video-Recovery-for-Cloud-Gaming"><a href="#Neural-Video-Recovery-for-Cloud-Gaming" class="headerlink" title="Neural Video Recovery for Cloud Gaming"></a>Neural Video Recovery for Cloud Gaming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07847">http://arxiv.org/abs/2307.07847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Zhaoyuan He, Yifan Yang, Shuozhe Li, Diyuan Dai, Lili Qiu</li>
<li>for: 提高云游戏的视频恢复精度和效率</li>
<li>methods: 使用游戏状态提高视频恢复精度，使用部分解码视频快速恢复失去的视频帧</li>
<li>results: 在iPhone 12和笔记机实现中，证明了使用游戏状态提高视频恢复精度，并且实现了高效的视频恢复Here is the translation in Simplified Chinese:</li>
<li>for: 提高云游戏的视频恢复精度和效率</li>
<li>methods: 使用游戏状态提高视频恢复精度，使用部分解码视频快速恢复失去的视频帧</li>
<li>results: 在iPhone 12和笔记机实现中，证明了使用游戏状态提高视频恢复精度，并且实现了高效的视频恢复<details>
<summary>Abstract</summary>
Cloud gaming is a multi-billion dollar industry. A client in cloud gaming sends its movement to the game server on the Internet, which renders and transmits the resulting video back. In order to provide a good gaming experience, a latency below 80 ms is required. This means that video rendering, encoding, transmission, decoding, and display have to finish within that time frame, which is especially challenging to achieve due to server overload, network congestion, and losses. In this paper, we propose a new method for recovering lost or corrupted video frames in cloud gaming. Unlike traditional video frame recovery, our approach uses game states to significantly enhance recovery accuracy and utilizes partially decoded frames to recover lost portions. We develop a holistic system that consists of (i) efficiently extracting game states, (ii) modifying H.264 video decoder to generate a mask to indicate which portions of video frames need recovery, and (iii) designing a novel neural network to recover either complete or partial video frames. Our approach is extensively evaluated using iPhone 12 and laptop implementations, and we demonstrate the utility of game states in the game video recovery and the effectiveness of our overall design.
</details>
<details>
<summary>摘要</summary>
云台游戏是一个多百亿美元的业态。客户端在云台游戏中将其运动发送到游戏服务器上，服务器在互联网上渲染和传输结果，并将视频传输回客户端。为提供良好的游戏体验，云台游戏中的延迟必须低于80ms。这意味着视频渲染、编码、传输、解码和显示必须在这个时间段内完成，这是特别困难的因为服务器过载、网络压力和损失。在这篇论文中，我们提出了一种新的视频帧恢复方法，与传统视频帧恢复方法不同，我们的方法使用游戏状态进行明显提高恢复精度，并使用部分解码的帧来恢复丢失的部分。我们设计了一个整体系统，包括(i)高效地提取游戏状态，(ii)修改H.264视频解码器生成一个抑制器，用于指示需要恢复的视频帧部分，以及(iii)设计一种新的神经网络来恢复完整或部分的视频帧。我们的方法在iPhone 12和笔记机实现中进行了广泛的评估，并证明游戏状态在游戏视频恢复中的重要性以及我们的总体设计的有效性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/16/cs.CV_2023_07_16/" data-id="cloimip8900ees4889cd691ym" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/16/cs.AI_2023_07_16/" class="article-date">
  <time datetime="2023-07-16T12:00:00.000Z" itemprop="datePublished">2023-07-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/16/cs.AI_2023_07_16/">cs.AI - 2023-07-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Recursive-Bateson-Inspired-Model-for-the-Generation-of-Semantic-Formal-Concepts-from-Spatial-Sensory-Data"><a href="#A-Recursive-Bateson-Inspired-Model-for-the-Generation-of-Semantic-Formal-Concepts-from-Spatial-Sensory-Data" class="headerlink" title="A Recursive Bateson-Inspired Model for the Generation of Semantic Formal Concepts from Spatial Sensory Data"></a>A Recursive Bateson-Inspired Model for the Generation of Semantic Formal Concepts from Spatial Sensory Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08087">http://arxiv.org/abs/2307.08087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaime de Miguel-Rodriguez, Fernando Sancho-Caparrini</li>
<li>for: 这 paper 的目的是提出一种新的符号化方法，用于从复杂的空间感知数据中生成层次结构。</li>
<li>methods: 这 paper 使用了 Bateson 的不同理论来提取原始数据中的原子特征，然后通过 recursive 过程将这些特征组织成更高级别的构造。</li>
<li>results: 这 paper 的结果表明，使用这种符号化方法可以生成具有很好的可读性和可组合性的概念表示，而无需训练。此外，这些概念表示还具有高度的组合性、形式逻辑推理能力和对不同数据集的泛化和 OUT-OF-distribution 学习能力。<details>
<summary>Abstract</summary>
Neural-symbolic approaches to machine learning incorporate the advantages from both connectionist and symbolic methods. Typically, these models employ a first module based on a neural architecture to extract features from complex data. Then, these features are processed as symbols by a symbolic engine that provides reasoning, concept structures, composability, better generalization and out-of-distribution learning among other possibilities. However, neural approaches to the grounding of symbols in sensory data, albeit powerful, still require heavy training and tedious labeling for the most part. This paper presents a new symbolic-only method for the generation of hierarchical concept structures from complex spatial sensory data. The approach is based on Bateson's notion of difference as the key to the genesis of an idea or a concept. Following his suggestion, the model extracts atomic features from raw data by computing elemental sequential comparisons in a stream of multivariate numerical values. Higher-level constructs are built from these features by subjecting them to further comparisons in a recursive process. At any stage in the recursion, a concept structure may be obtained from these constructs and features by means of Formal Concept Analysis. Results show that the model is able to produce fairly rich yet human-readable conceptual representations without training. Additionally, the concept structures obtained through the model (i) present high composability, which potentially enables the generation of 'unseen' concepts, (ii) allow formal reasoning, and (iii) have inherent abilities for generalization and out-of-distribution learning. Consequently, this method may offer an interesting angle to current neural-symbolic research. Future work is required to develop a training methodology so that the model can be tested against a larger dataset.
</details>
<details>
<summary>摘要</summary>
神经 символиック方法可以结合神经网络和符号学方法的优点。通常，这些模型使用神经网络架构来提取复杂数据中的特征，然后将这些特征处理成符号学引擎中的符号，从而提供了逻辑、结构、可组合性、更好的泛化和外部数据学习等可能性。然而，神经方法对于将符号固定到感知数据中的降解，尚需很多的训练和繁琐的标注。这篇论文提出了一种新的符号学只的方法，用于从复杂空间感知数据中生成层次结构。该方法基于贝蒂逊的不同理论，即对象的想法或概念的起源是由差异所致。根据这个想法，模型从原始数据中提取原子特征，通过计算元素sequential比较来生成流行多变量数值中的特征。在这个过程中，更高级别的构造物可以通过进一步的比较来生成，并且可以通过正式概念分析来获得概念结构。在任何阶段，模型可以通过对构造物和特征进行比较来生成概念结构。 results表明，该模型可以生成比较 ric yet human-readable的概念表示，而无需训练。此外，模型生成的概念结构具有高可组合性，可能允许生成“未看到”的概念，具有逻辑逻辑和外部数据学习的能力。因此，这种方法可能会对当前神经 символиック研究提供一个有趣的角度。未来的工作是开发一种训练方法，以便将模型测试于更大的数据集。
</details></li>
</ul>
<hr>
<h2 id="Dataset-Distillation-Meets-Provable-Subset-Selection"><a href="#Dataset-Distillation-Meets-Provable-Subset-Selection" class="headerlink" title="Dataset Distillation Meets Provable Subset Selection"></a>Dataset Distillation Meets Provable Subset Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08086">http://arxiv.org/abs/2307.08086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Murad Tukan, Alaa Maalouf, Margarita Osadchy</li>
<li>for: 提高 dataset distillation 的效果，使其可以更好地压缩大量数据并保持模型的性能。</li>
<li>methods: 我们提出了两个方法来改进 dataset distillation：首先，我们使用抽象样本来初始化压缩后的数据集，并且在训练过程中使用重要样本进行更新。其次，我们将数据subset selection 与 dataset distillation 结合起来，在训练过程中使用重要的样本进行更新。</li>
<li>results: 我们的方法可以帮助 dataset distillation 更好地压缩数据并保持模型的性能。我们通过实验表明，我们的方法可以与现有的 dataset distillation 技术结合使用，并且能够提高其性能。<details>
<summary>Abstract</summary>
Deep learning has grown tremendously over recent years, yielding state-of-the-art results in various fields. However, training such models requires huge amounts of data, increasing the computational time and cost. To address this, dataset distillation was proposed to compress a large training dataset into a smaller synthetic one that retains its performance -- this is usually done by (1) uniformly initializing a synthetic set and (2) iteratively updating/learning this set according to a predefined loss by uniformly sampling instances from the full data. In this paper, we improve both phases of dataset distillation: (1) we present a provable, sampling-based approach for initializing the distilled set by identifying important and removing redundant points in the data, and (2) we further merge the idea of data subset selection with dataset distillation, by training the distilled set on ``important'' sampled points during the training procedure instead of randomly sampling the next batch. To do so, we define the notion of importance based on the relative contribution of instances with respect to two different loss functions, i.e., one for the initialization phase (a kernel fitting function for kernel ridge regression and $K$-means based loss function for any other distillation method), and the relative cross-entropy loss (or any other predefined loss) function for the training phase. Finally, we provide experimental results showing how our method can latch on to existing dataset distillation techniques and improve their performance.
</details>
<details>
<summary>摘要</summary>
深度学习在最近几年内有很大的发展，取得了多个领域的状态机器人模型。然而，训练这些模型需要巨大的数据量和计算资源。为解决这个问题，数据集减少技术被提出，即将大规模的训练数据集压缩成一个更小的人工数据集，保持其性能。在这篇论文中，我们提高了两个数据集减少阶段的技术：1. 我们提出了一种可证明的抽象样本方法，通过标识重要和减少数据中重复的点来初始化减少后的数据集。2. 我们将数据子集选择和数据集减少技术融合在一起，在训练过程中将减少后的数据集训练在“重要”的抽取样本上。为此，我们定义了对于两个不同损失函数的相对贡献度，即一个是初始化阶段的kernel适应函数和$K$-means基于损失函数，另一个是训练阶段的相对杂志损失函数。最后，我们提供了实验结果，表明我们的方法可以与现有的数据集减少技术相结合，提高其性能。
</details></li>
</ul>
<hr>
<h2 id="POMDP-inference-and-robust-solution-via-deep-reinforcement-learning-An-application-to-railway-optimal-maintenance"><a href="#POMDP-inference-and-robust-solution-via-deep-reinforcement-learning-An-application-to-railway-optimal-maintenance" class="headerlink" title="POMDP inference and robust solution via deep reinforcement learning: An application to railway optimal maintenance"></a>POMDP inference and robust solution via deep reinforcement learning: An application to railway optimal maintenance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08082">http://arxiv.org/abs/2307.08082</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/giarcieri/robust-optimal-maintenance-planning-through-reinforcement-learning-and-rllib">https://github.com/giarcieri/robust-optimal-maintenance-planning-through-reinforcement-learning-and-rllib</a></li>
<li>paper_authors: Giacomo Arcieri, Cyprien Hoelzl, Oliver Schwery, Daniel Straub, Konstantinos G. Papakonstantinou, Eleni Chatzi</li>
<li>for: 这篇论文是用于解决复杂的顺序决策问题的 POMDP 模型，以及使用深度学习解决 POMDP 的不确定性问题。</li>
<li>methods: 该论文提出了一种结合推理和 robust 解决 POMDP 的方法，包括使用 Markov Chain Monte Carlo 抽样来恢复转移和观察模型参数，然后使用深度学习技术来解决 POMDP 问题，并将解决方案与模型参数相结合，以提高解决方案的稳定性。</li>
<li>results: 该论文应用了这种方法于实际问题，即铁路资产保养规划问题，并进行了对 transformers 和 long short-term memory networks 的比较，以及模型基于&#x2F;模型自由混合的方法的比较。<details>
<summary>Abstract</summary>
Partially Observable Markov Decision Processes (POMDPs) can model complex sequential decision-making problems under stochastic and uncertain environments. A main reason hindering their broad adoption in real-world applications is the lack of availability of a suitable POMDP model or a simulator thereof. Available solution algorithms, such as Reinforcement Learning (RL), require the knowledge of the transition dynamics and the observation generating process, which are often unknown and non-trivial to infer. In this work, we propose a combined framework for inference and robust solution of POMDPs via deep RL. First, all transition and observation model parameters are jointly inferred via Markov Chain Monte Carlo sampling of a hidden Markov model, which is conditioned on actions, in order to recover full posterior distributions from the available data. The POMDP with uncertain parameters is then solved via deep RL techniques with the parameter distributions incorporated into the solution via domain randomization, in order to develop solutions that are robust to model uncertainty. As a further contribution, we compare the use of transformers and long short-term memory networks, which constitute model-free RL solutions, with a model-based/model-free hybrid approach. We apply these methods to the real-world problem of optimal maintenance planning for railway assets.
</details>
<details>
<summary>摘要</summary>
partially observable Markov decision processes (POMDPs) 可以模型复杂的顺序决策问题在Random and uncertain environment中. 一个主要阻碍POMDPs的广泛应用在实际场景中是lack of a suitable POMDP model or simulator thereof. 可用的解决方法，如Reinforcement Learning (RL), 需要知道过渡动力和观察生成过程，这些常常 unknown 和 non-trivial to infer. 在这种工作中，我们提出了一种 jointly inferring 和 Robust solution of POMDPs via deep RL. 首先，所有过渡和观察模型参数都是通过Markov Chain Monte Carlo sampling of a hidden Markov model, which is conditioned on actions, in order to recover full posterior distributions from the available data. 然后，POMDP with uncertain parameters is solved via deep RL techniques with the parameter distributions incorporated into the solution via domain randomization, in order to develop solutions that are robust to model uncertainty. 此外，我们还进行了transformers 和 long short-term memory networks (LSTM) 的比较，这些都是model-free RL solution, 以及model-based/model-free hybrid approach. 我们应用这些方法到了实际问题——铁路资产optimal maintenance planning.
</details></li>
</ul>
<hr>
<h2 id="Disco-Bench-A-Discourse-Aware-Evaluation-Benchmark-for-Language-Modelling"><a href="#Disco-Bench-A-Discourse-Aware-Evaluation-Benchmark-for-Language-Modelling" class="headerlink" title="Disco-Bench: A Discourse-Aware Evaluation Benchmark for Language Modelling"></a>Disco-Bench: A Discourse-Aware Evaluation Benchmark for Language Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08074">http://arxiv.org/abs/2307.08074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longyue Wang, Zefeng Du, Donghuai Liu, Deng Cai, Dian Yu, Haiyun Jiang, Yan Wang, Leyang Cui, Shuming Shi, Zhaopeng Tu</li>
<li>for: 这个论文的目的是提出一个新的评价标准，以评估自然语言处理（NLP）模型对文本中的语言现象进行模型化。</li>
<li>methods: 这个论文使用了一个新的评价标准，叫做Disco-Bench，这个标准可以评估NLP模型对文本中的语言现象进行模型化。</li>
<li>results: 这个论文的结果表明，使用文学文档级别的训练数据进行细化预训练可以提高NLP模型对文本中的语言现象的模型化。<details>
<summary>Abstract</summary>
Modeling discourse -- the linguistic phenomena that go beyond individual sentences, is a fundamental yet challenging aspect of natural language processing (NLP). However, existing evaluation benchmarks primarily focus on the evaluation of inter-sentence properties and overlook critical discourse phenomena that cross sentences. To bridge the gap, we propose Disco-Bench, a benchmark that can evaluate intra-sentence discourse properties across a diverse set of NLP tasks, covering understanding, translation, and generation. Disco-Bench consists of 9 document-level testsets in the literature domain, which contain rich discourse phenomena (e.g. cohesion and coherence) in Chinese and/or English. For linguistic analysis, we also design a diagnostic test suite that can examine whether the target models learn discourse knowledge. We totally evaluate 20 general-, in-domain and commercial models based on Transformer, advanced pretraining architectures and large language models (LLMs). Our results show (1) the challenge and necessity of our evaluation benchmark; (2) fine-grained pretraining based on literary document-level training data consistently improves the modeling of discourse information. We will release the datasets, pretrained models, and leaderboard, which we hope can significantly facilitate research in this field: https://github.com/longyuewangdcu/Disco-Bench.
</details>
<details>
<summary>摘要</summary>
模拟语言流行 -- 超出个别句子的语言现象，是自然语言处理（NLP）的基本 yet 挑战性方面。然而，现有的评估标准主要是评估间句性质，忽视了重要的演讲现象。为了bridging this gap，我们提议了Disco-Bench，一个可以评估语句级别的演讲属性的评估标准，覆盖了理解、翻译和生成多个NLP任务。Disco-Bench包括9个文档级测试集，其中包含了中文和英文的丰富的演讲现象（例如，凝聚和一致）。为了语言分析，我们还设计了一个 диагностические测试组，可以检查目标模型是否学习到了演讲知识。我们总共评估了20个通用-, 域内-和商业模型，基于Transformer、先进的预训练架构和大语言模型（LLMs）。我们的结果显示了以下两点：1. 我们的评估标准的挑战和必要性。2. 基于文学文档级别的预训练数据进行细化预训练，可以一直提高模型对演讲信息的模型。我们将发布数据集、预训练模型和排名，希望可以帮助研究人员在这一领域进行更多的研究：https://github.com/longyuewangdcu/Disco-Bench。
</details></li>
</ul>
<hr>
<h2 id="Do-Emergent-Abilities-Exist-in-Quantized-Large-Language-Models-An-Empirical-Study"><a href="#Do-Emergent-Abilities-Exist-in-Quantized-Large-Language-Models-An-Empirical-Study" class="headerlink" title="Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study"></a>Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08072">http://arxiv.org/abs/2307.08072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rucaibox/quantizedempirical">https://github.com/rucaibox/quantizedempirical</a></li>
<li>paper_authors: Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, Ji-Rong Wen</li>
<li>for:  investigate the impact of quantization on emergent abilities of large language models</li>
<li>methods: empirical experiments and fine-grained impact analysis</li>
<li>results: 4-bit quantization models still maintain emergent abilities, while 2-bit models show severe performance degradation; fine-tuning can improve performance<details>
<summary>Abstract</summary>
Despite the superior performance, Large Language Models~(LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increasing the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on \emph{emergent abilities}, which are important characteristics that distinguish LLMs from small language models. Specially, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on the test of these abilities. To improve the performance of low-bit models, we conduct two special experiments: (1) fine-gained impact analysis that studies which components (or substructures) are more sensitive to quantization, and (2) performance compensation through model fine-tuning. Our work derives a series of important findings to understand the impact of quantization on emergent abilities, and sheds lights on the possibilities of extremely low-bit quantization for LLMs.
</details>
<details>
<summary>摘要</summary>
尽管大语言模型（LLM）在性能方面表现出色，但它们在部署和使用时需要很大的计算资源。为了解决这个问题，量化方法在LLM中广泛应用，以减少它们的内存占用量并提高推理速度。然而，低位数量化方法通常会导致性能下降。关键在于理解量化对LLM的容量有何影响。与之前关注总性能的研究不同，本研究探讨量化对 Language Model 的 emergent ability 的影响，即语言模型的特点，例如语言模型在语言上下文中学习、链式思维和遵从指令的能力。我们的实验表明，4位量化模型仍可保留这些能力，而2位量化模型在测试这些能力时受到了严重的性能下降。为了提高低位模型的性能，我们进行了两个特殊实验：（1）细读影响分析，研究量化对不同组件（或子结构）的影响，以及（2）通过模型细调进行性能补偿。我们的工作得出了一系列重要的发现，可以理解量化对 emergent ability 的影响，并照明了EXTREMELY 低位量化是否可以实现的可能性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Flexible-Time-to-event-Modeling-Optimizing-Neural-Networks-via-Rank-Regression"><a href="#Towards-Flexible-Time-to-event-Modeling-Optimizing-Neural-Networks-via-Rank-Regression" class="headerlink" title="Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression"></a>Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08044">http://arxiv.org/abs/2307.08044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teboozas/dart_ecai23">https://github.com/teboozas/dart_ecai23</a></li>
<li>paper_authors: Hyunjun Lee, Junhyun Lee, Taehwa Choi, Jaewoo Kang, Sangbum Choi</li>
<li>for: 这个研究旨在开发一个基于深度学习的时间至事件预测模型，以提高预测性能和减少假设。</li>
<li>methods: 这个模型使用了一个基于Gehan的排名统计的目标函数，并且不需要设定基eline事件时间分布，因此可以保留直接预测事件时间的优点。</li>
<li>results: 经过量化分析various benchmark datasets，这个模型在处理高通量的停止时间至事件数据时表现出了优异的潜力。<details>
<summary>Abstract</summary>
Time-to-event analysis, also known as survival analysis, aims to predict the time of occurrence of an event, given a set of features. One of the major challenges in this area is dealing with censored data, which can make learning algorithms more complex. Traditional methods such as Cox's proportional hazards model and the accelerated failure time (AFT) model have been popular in this field, but they often require assumptions such as proportional hazards and linearity. In particular, the AFT models often require pre-specified parametric distributional assumptions. To improve predictive performance and alleviate strict assumptions, there have been many deep learning approaches for hazard-based models in recent years. However, representation learning for AFT has not been widely explored in the neural network literature, despite its simplicity and interpretability in comparison to hazard-focused methods. In this work, we introduce the Deep AFT Rank-regression model for Time-to-event prediction (DART). This model uses an objective function based on Gehan's rank statistic, which is efficient and reliable for representation learning. On top of eliminating the requirement to establish a baseline event time distribution, DART retains the advantages of directly predicting event time in standard AFT models. The proposed method is a semiparametric approach to AFT modeling that does not impose any distributional assumptions on the survival time distribution. This also eliminates the need for additional hyperparameters or complex model architectures, unlike existing neural network-based AFT models. Through quantitative analysis on various benchmark datasets, we have shown that DART has significant potential for modeling high-throughput censored time-to-event data.
</details>
<details>
<summary>摘要</summary>
时间到事分析（也称为存生分析）的目标是预测事件发生的时间， givens 一组特征。 一个主要挑战在这个领域是处理 censored 数据，这可能使学习算法变得更加复杂。传统方法如科克斯的相对准确度模型和加速失败时间（AFT）模型在这个领域非常流行，但它们经常假设 proportional hazards 和线性性。特别是 AF 模型经常需要预先指定 Parametric distributional assumptions。为了改进预测性能和减少严格假设，过去几年有很多深度学习方法在存生模型领域得到应用。然而， representation learning 在 neural network 文献中对 AFT 模型的应用还是不够广泛，尽管它在相对 simplicity 和 interpretability 方面与 hazard-focused 方法相比较有优势。在这个工作中，我们介绍了 Deep AFT Rank-regression model for Time-to-event prediction（DART）。这个模型使用基于 Gehan 排名统计的目标函数，这是efficient 和可靠的 representation learning 方法。在 eliminating the requirement to establish a baseline event time distribution 的同时，DART 保留了标准 AFT 模型中的优点，直接预测事件时间。我们的方法是一种 semi-parametric approach to AFT modeling，不需要任何分布 assumption 于存生时间分布。这也意味着我们不需要额外的 Hyperparameter 或复杂的模型结构，与现有的 neural network-based AFT 模型不同。通过对各种 benchmark 数据进行量化分析，我们表明了 DART 在高通过率 censored time-to-event 数据模型中的显著潜力。
</details></li>
</ul>
<hr>
<h2 id="A-Neural-Symbolic-Approach-Towards-Identifying-Grammatically-Correct-Sentences"><a href="#A-Neural-Symbolic-Approach-Towards-Identifying-Grammatically-Correct-Sentences" class="headerlink" title="A Neural-Symbolic Approach Towards Identifying Grammatically Correct Sentences"></a>A Neural-Symbolic Approach Towards Identifying Grammatically Correct Sentences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08036">http://arxiv.org/abs/2307.08036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicos Isaak</li>
<li>for:  validate English sentences</li>
<li>methods:  neural-symbolic approach, blending of grammatical and syntactical rules with language models</li>
<li>results:  effective tackling of Corpus of Linguistic Acceptability (COLA) task, improvement of symbolic systems’ accuracy results through blending with non-symbolic systems<details>
<summary>Abstract</summary>
Textual content around us is growing on a daily basis. Numerous articles are being written as we speak on online newspapers, blogs, or social media. Similarly, recent advances in the AI field, like language models or traditional classic AI approaches, are utilizing all the above to improve their learned representation to tackle NLP challenges with human-like accuracy. It is commonly accepted that it is crucial to have access to well-written text from valid sources to tackle challenges like text summarization, question-answering, machine translation, or even pronoun resolution. For instance, to summarize well, one needs to select the most important sentences in order to concatenate them to form the summary. However, what happens if we do not have access to well-formed English sentences or even non-valid sentences? Despite the importance of having access to well-written sentences, figuring out ways to validate them is still an open area of research. To address this problem, we present a simplified way to validate English sentences through a novel neural-symbolic approach. Lately, neural-symbolic approaches have triggered an increasing interest towards tackling various NLP challenges, as they are demonstrating their effectiveness as a central component in various AI systems. Through combining Classic with Modern AI, which involves the blending of grammatical and syntactical rules with language models, we effectively tackle the Corpus of Linguistic Acceptability (COLA), a task that shows whether or not a sequence of words is an English grammatical sentence. Among others, undertaken experiments effectively show that blending symbolic and non-symbolic systems helps the former provide insights about the latter's accuracy results.
</details>
<details>
<summary>摘要</summary>
Text around us 每天都在增长。许多文章在线报纸、博客或社交媒体上写作，而现代人工智能技术，如语言模型或传统的класси学习法，则在利用这些文本来提高其学习表示。通常认为，要解决文本处理挑战，需要访问有效的文本来。例如，要 SUMMARIZE 文本，需要选择最重要的句子，并将它们 concatenate 成 SUMMARY。但是，如果我们没有访问有效的英语句子或非有效的句子？尽管有效的文本访问是一个开放的研究领域。为解决这个问题，我们提出了一种简化的英语句子验证方法，基于一种新的神经符号学方法。在最近几年，神经符号学方法在解决各种文本处理挑战方面得到了越来越多的关注，因为它们在不同的 AI 系统中表现出了非常的效果。通过将传统的 grammatical 和 sintactical 规则与语言模型结合在一起，我们可以有效地解决 COLA 任务，这个任务是判断一个序列是否为英语 grammatical 句子。在其他实验中，我们发现了将符号学系统与非符号学系统混合，可以帮助符号学系统提供有关非符号学系统的准确性结果的深入理解。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-inference-for-data-efficient-explainable-and-safe-robotic-motion-planning-A-review"><a href="#Bayesian-inference-for-data-efficient-explainable-and-safe-robotic-motion-planning-A-review" class="headerlink" title="Bayesian inference for data-efficient, explainable, and safe robotic motion planning: A review"></a>Bayesian inference for data-efficient, explainable, and safe robotic motion planning: A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08024">http://arxiv.org/abs/2307.08024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengmin Zhou, Chao Wang, Haseeb Hassan, Himat Shah, Bingding Huang, Pasi Fränti</li>
<li>for: 这个论文旨在探讨bayesian inference在机器人运动规划中的应用，包括不确定性量化、安全性和优化 garanties、数据效率、模拟与实际差异和混合 bayesian inference和学习等方面。</li>
<li>methods: 论文使用bayesian inference的多种方法，包括 bayesian estimation、模型基于 bayesian RL 和无模型基于 bayesian RL、 inverse RL 和 hybridization of bayesian inference and RL等。</li>
<li>results: 论文提供了bayesian inference在机器人运动规划中的应用和研究进展，包括对复杂情况的 bayesian inference、数据效率、模拟与实际差异和混合 bayesian inference和学习等方面的分析和评估。<details>
<summary>Abstract</summary>
Bayesian inference has many advantages in robotic motion planning over four perspectives: The uncertainty quantification of the policy, safety (risk-aware) and optimum guarantees of robot motions, data-efficiency in training of reinforcement learning, and reducing the sim2real gap when the robot is applied to real-world tasks. However, the application of Bayesian inference in robotic motion planning is lagging behind the comprehensive theory of Bayesian inference. Further, there are no comprehensive reviews to summarize the progress of Bayesian inference to give researchers a systematic understanding in robotic motion planning. This paper first provides the probabilistic theories of Bayesian inference which are the preliminary of Bayesian inference for complex cases. Second, the Bayesian estimation is given to estimate the posterior of policies or unknown functions which are used to compute the policy. Third, the classical model-based Bayesian RL and model-free Bayesian RL algorithms for robotic motion planning are summarized, while these algorithms in complex cases are also analyzed. Fourth, the analysis of Bayesian inference in inverse RL is given to infer the reward functions in a data-efficient manner. Fifth, we systematically present the hybridization of Bayesian inference and RL which is a promising direction to improve the convergence of RL for better motion planning. Sixth, given the Bayesian inference, we present the interpretable and safe robotic motion plannings which are the hot research topic recently. Finally, all algorithms reviewed in this paper are summarized analytically as the knowledge graphs, and the future of Bayesian inference for robotic motion planning is also discussed, to pave the way for data-efficient, explainable, and safe robotic motion planning strategies for practical applications.
</details>
<details>
<summary>摘要</summary>
推断学有很多优势在机器人运动规划中，包括政策不确定性评估、安全性（风险意识）和优化机器人运动的 garantías, 数据效率在循环学习中训练, 和实际任务中的模拟与实际差异减少。然而，机器人运动规划中的推断学应用还处于完整理论的后方。此外，没有系统性的文章总结了推断学在机器人运动规划中的进步，以便给研究人员提供系统性的认识。这篇论文首先提供了推断学的概率理论，这些理论是复杂情况下的前提。其次，bayesian估计是用来估计政策或未知函数的 posterior，以计算政策。第三，本文总结了基于模型的推断学RL和无模型的推断学RL算法，这些算法在复杂情况下也进行了分析。第四，本文对 bayesian推断在反向RL中的分析，用数据效率的方式推断出奖励函数。第五，本文系统地介绍了推断学和RL的гибриди化，这是一个有前途的方向，以提高RL的收敛性。最后，本文总结了所有论文中的算法，并讨论了未来推断学在机器人运动规划中的发展，以便为数据效率、可读性和安全的机器人运动规划策略提供道路。
</details></li>
</ul>
<hr>
<h2 id="Breaking-Down-the-Task-A-Unit-Grained-Hybrid-Training-Framework-for-Vision-and-Language-Decision-Making"><a href="#Breaking-Down-the-Task-A-Unit-Grained-Hybrid-Training-Framework-for-Vision-and-Language-Decision-Making" class="headerlink" title="Breaking Down the Task: A Unit-Grained Hybrid Training Framework for Vision and Language Decision Making"></a>Breaking Down the Task: A Unit-Grained Hybrid Training Framework for Vision and Language Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08016">http://arxiv.org/abs/2307.08016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruipu Luo, Jiwen Zhang, Zhongyu Wei</li>
<li>for: 本研究旨在解决视觉语言决策任务中的长动作序列问题，提出了一个单位化训练框架，以实现环境内的活动探索和减少传递偏见。</li>
<li>methods: 本研究提出了一个单位转换器（UT），具有内置的循环状态，以保持单位缩寸跨模式内存。</li>
<li>results: 经过广泛的TEACH标准库实验，我们的提案方法在评估指标上优于现有的状况顶尖方法。<details>
<summary>Abstract</summary>
Vision language decision making (VLDM) is a challenging multimodal task. The agent have to understand complex human instructions and complete compositional tasks involving environment navigation and object manipulation. However, the long action sequences involved in VLDM make the task difficult to learn. From an environment perspective, we find that task episodes can be divided into fine-grained \textit{units}, each containing a navigation phase and an interaction phase. Since the environment within a unit stays unchanged, we propose a novel hybrid-training framework that enables active exploration in the environment and reduces the exposure bias. Such framework leverages the unit-grained configurations and is model-agnostic. Specifically, we design a Unit-Transformer (UT) with an intrinsic recurrent state that maintains a unit-scale cross-modal memory. Through extensive experiments on the TEACH benchmark, we demonstrate that our proposed framework outperforms existing state-of-the-art methods in terms of all evaluation metrics. Overall, our work introduces a novel approach to tackling the VLDM task by breaking it down into smaller, manageable units and utilizing a hybrid-training framework. By doing so, we provide a more flexible and effective solution for multimodal decision making.
</details>
<details>
<summary>摘要</summary>
视觉语言决策（VLDM）是一项复杂的多模态任务。智能体需要理解复杂的人类指令，完成环境导航和物体操作的compositional任务。然而，长的行动序列使得这个任务困难学习。从环境角度来看，我们发现任务集可以分为细化的单位，每个单位包括导航阶段和交互阶段。由于环境内单位保持不变，我们提议一种新的混合训练框架，允许活动探索环境，并减少曝光偏见。这种框架利用单位粒度的配置，是模型无关的。我们设计了 Unit-Transformer（UT），具有内置的自回归状态，保持单位级别的交互记忆。经过广泛的 TEACH  bencmark 实验，我们证明了我们提议的框架比现有状态的方法在所有评价指标上表现更好。总的来说，我们的工作介绍了一种新的方法来解决 VLDM 任务，通过将其分解成更小、可控的单位，并利用混合训练框架。这提供了更 flexible 和有效的多模态决策解决方案。
</details></li>
</ul>
<hr>
<h2 id="SHAMSUL-Simultaneous-Heatmap-Analysis-to-investigate-Medical-Significance-Utilizing-Local-interpretability-methods"><a href="#SHAMSUL-Simultaneous-Heatmap-Analysis-to-investigate-Medical-Significance-Utilizing-Local-interpretability-methods" class="headerlink" title="SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical Significance Utilizing Local interpretability methods"></a>SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical Significance Utilizing Local interpretability methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08003">http://arxiv.org/abs/2307.08003</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anondo1969/shamsul">https://github.com/anondo1969/shamsul</a></li>
<li>paper_authors: Mahbub Ul Alam, Jaakko Hollmén, Jón Rúnar Baldvinsson, Rahim Rahmani</li>
<li>For: The paper aims to evaluate the performance of four well-established interpretability methods (LIME, SHAP, Grad-CAM, and LRP) in interpreting deep neural network predictions for chest radiography images.* Methods: The study uses a transfer learning approach with a multi-label-multi-class chest radiography dataset, and evaluates the interpretability methods on both single-label and multi-label predictions. The analysis includes quantitative and qualitative investigations, and compares the results against human expert annotation.* Results: The study finds that Grad-CAM demonstrates the most favorable performance in quantitative evaluation, while the LIME heatmap segmentation visualization exhibits the highest level of medical significance. The research highlights the strengths and limitations of these interpretability methods and suggests that a multimodal-based approach could offer additional insights for enhancing interpretability in the medical domain.Here is the same information in Simplified Chinese text:* For: 这个研究旨在评估深度神经网络预测图像胸部X射线成像的 interpretability 方法 (LIME, SHAP, Grad-CAM, LRP) 的性能。* Methods: 该研究使用了传输学习方法，使用多个标签多个类的胸部X射线成像集合，并对这些方法进行了单标签和多标签预测的评估。研究包括量化和质量的调查，并与人工专家标注进行比较。* Results: 研究发现，Grad-CAM在量化评估中表现最佳，而 LIME 热图分 segmentation 可视化具有最高的医学意义。研究描述了这些 interpretability 方法的优缺点，并建议使用多Modal 基于的方法，可以在医疗领域提供更多的意义。<details>
<summary>Abstract</summary>
The interpretability of deep neural networks has become a subject of great interest within the medical and healthcare domain. This attention stems from concerns regarding transparency, legal and ethical considerations, and the medical significance of predictions generated by these deep neural networks in clinical decision support systems. To address this matter, our study delves into the application of four well-established interpretability methods: Local Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations (SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise Relevance Propagation (LRP). Leveraging the approach of transfer learning with a multi-label-multi-class chest radiography dataset, we aim to interpret predictions pertaining to specific pathology classes. Our analysis encompasses both single-label and multi-label predictions, providing a comprehensive and unbiased assessment through quantitative and qualitative investigations, which are compared against human expert annotation. Notably, Grad-CAM demonstrates the most favorable performance in quantitative evaluation, while the LIME heatmap segmentation visualization exhibits the highest level of medical significance. Our research highlights the strengths and limitations of these interpretability methods and suggests that a multimodal-based approach, incorporating diverse sources of information beyond chest radiography images, could offer additional insights for enhancing interpretability in the medical domain.
</details>
<details>
<summary>摘要</summary>
深度神经网络的可解释性在医疗领域已经引起了广泛的关注，这种关注的起点是因为透明度、法律和伦理考虑以及深度神经网络在临床决策支持系统中的医学意义。为解决这个问题，我们的研究探讨了四种已知的可解释方法：本地可解释性模型无关性解释（LIME）、Shapley添加itive exPlanations（SHAP）、梯度权重分类活动映射（Grad-CAM）和层次 relevance propagation（LRP）。通过转移学习的方式，我们使用了一个多标签多类肺高照图像数据集，以解释具体疾病类型的预测结果。我们的分析包括单标签和多标签预测，以提供全面和无偏见的评估，并与人类专家标注进行比较。考据表明，Grad-CAM在量化评估中表现最佳，而 LIME 热点Segmentation 视觉化可以达到最高的医学意义。我们的研究探讨了这些可解释方法的优缺点，并建议将多模式基于的方法应用于医疗领域，以获取更多的增强可解释性的信息。
</details></li>
</ul>
<hr>
<h2 id="MargCTGAN-A-“Marginally’’-Better-CTGAN-for-the-Low-Sample-Regime"><a href="#MargCTGAN-A-“Marginally’’-Better-CTGAN-for-the-Low-Sample-Regime" class="headerlink" title="MargCTGAN: A “Marginally’’ Better CTGAN for the Low Sample Regime"></a>MargCTGAN: A “Marginally’’ Better CTGAN for the Low Sample Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07997">http://arxiv.org/abs/2307.07997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tejuafonja/margctgan">https://github.com/tejuafonja/margctgan</a></li>
<li>paper_authors: Tejumade Afonja, Dingfan Chen, Mario Fritz</li>
<li>for: 评估现代 synthetic 表格数据生成器的能力，特别是在低样本情况下。</li>
<li>methods: 使用 CTGAN 模型和特征匹配技术来改进 synthetic 数据的统计性和下游任务用途性。</li>
<li>results: 提出了 MargCTGAN 模型，可以在高到低样本情况下保持同样的下游任务用途性和统计性。<details>
<summary>Abstract</summary>
The potential of realistic and useful synthetic data is significant. However, current evaluation methods for synthetic tabular data generation predominantly focus on downstream task usefulness, often neglecting the importance of statistical properties. This oversight becomes particularly prominent in low sample scenarios, accompanied by a swift deterioration of these statistical measures. In this paper, we address this issue by conducting an evaluation of three state-of-the-art synthetic tabular data generators based on their marginal distribution, column-pair correlation, joint distribution and downstream task utility performance across high to low sample regimes. The popular CTGAN model shows strong utility, but underperforms in low sample settings in terms of utility. To overcome this limitation, we propose MargCTGAN that adds feature matching of de-correlated marginals, which results in a consistent improvement in downstream utility as well as statistical properties of the synthetic data.
</details>
<details>
<summary>摘要</summary>
现有的Synthetic数据生成方法具有实用的潜力，但是现有评估方法倾向于专注在下游任务的有用性上，常常忽略了这些统计特性的重要性。这个问题在低数据情况下特别突出，伴随着这些统计指标的快速衰退。在这篇文章中，我们解决这个问题，通过评估三种现有的Synthetic数据生成器的边缘分布、列 pairs相互相关、共同分布和下游任务的有用性，以及在高至低数据情况下的表现。CTGAN模型在下游任务方面表现强，但在低数据情况下表现不佳，特别是在统计特性方面。为了解决这个问题，我们提出了MargCTGAN，它通过匹配特征的束缩边缘分布，实现了在下游任务和统计特性方面的一致性提升。
</details></li>
</ul>
<hr>
<h2 id="CoNAN-Conditional-Neural-Aggregation-Network-For-Unconstrained-Face-Feature-Fusion"><a href="#CoNAN-Conditional-Neural-Aggregation-Network-For-Unconstrained-Face-Feature-Fusion" class="headerlink" title="CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion"></a>CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10237">http://arxiv.org/abs/2307.10237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bhavin Jawade, Deen Dayal Mohan, Dennis Fedorishin, Srirangaraj Setlur, Venu Govindaraju</li>
<li>for: 这个论文旨在解决受到无法控制的环境影响的长距离识别 faces 的问题，例如：距离、分辨率、角度、照明、姿势和大气状况等。</li>
<li>methods: 本文提出了一种基于分布情况的特征聚合方法，即CoNAN，以提高长距离识别 faces 的精度。这个方法通过学习一个受到分布信息 conditional 的上下文 вектор，将特征按照其估计的有用程度进行权重。</li>
<li>results: 本文在长距离无条件 face 识别 dataset 上获得了state-of-the-art 的结果，例如：BTS 和 DroneSURF，这说明了这种聚合策略的优点。<details>
<summary>Abstract</summary>
Face recognition from image sets acquired under unregulated and uncontrolled settings, such as at large distances, low resolutions, varying viewpoints, illumination, pose, and atmospheric conditions, is challenging. Face feature aggregation, which involves aggregating a set of N feature representations present in a template into a single global representation, plays a pivotal role in such recognition systems. Existing works in traditional face feature aggregation either utilize metadata or high-dimensional intermediate feature representations to estimate feature quality for aggregation. However, generating high-quality metadata or style information is not feasible for extremely low-resolution faces captured in long-range and high altitude settings. To overcome these limitations, we propose a feature distribution conditioning approach called CoNAN for template aggregation. Specifically, our method aims to learn a context vector conditioned over the distribution information of the incoming feature set, which is utilized to weigh the features based on their estimated informativeness. The proposed method produces state-of-the-art results on long-range unconstrained face recognition datasets such as BTS, and DroneSURF, validating the advantages of such an aggregation strategy.
</details>
<details>
<summary>摘要</summary>
face recognition from image sets acquired under unregulated and uncontrolled settings, such as at large distances, low resolutions, varying viewpoints, illumination, pose, and atmospheric conditions, is challenging. Face feature aggregation, which involves aggregating a set of N feature representations present in a template into a single global representation, plays a pivotal role in such recognition systems. Existing works in traditional face feature aggregation either utilize metadata or high-dimensional intermediate feature representations to estimate feature quality for aggregation. However, generating high-quality metadata or style information is not feasible for extremely low-resolution faces captured in long-range and high altitude settings. To overcome these limitations, we propose a feature distribution conditioning approach called CoNAN for template aggregation. Specifically, our method aims to learn a context vector conditioned over the distribution information of the incoming feature set, which is utilized to weigh the features based on their estimated informativeness. The proposed method produces state-of-the-art results on long-range unconstrained face recognition datasets such as BTS, and DroneSURF, validating the advantages of such an aggregation strategy.Here's the translation in Traditional Chinese:面部识别从不受管制的图像集中，例如在大Distance、低分辨率、不同的角度、照明、姿势和大气情况下捕捉的面部图像，是一个问题。在这些识别系统中，面部特征聚合协助整合一个模板中的多个特征表示，是一个关键的步骤。现有的传统面部特征聚合方法通常是利用元metadata或高维度的中途特征表示来估算特征质量，但是在EXTREMELY低分辨率的面部图像中，生成高质量的元metadata或饰件信息是不可能的。为了突破这些限制，我们提出了一个特征分布条件对应方法called CoNAN。 Specifically, our method aims to learn a context vector conditioned over the distribution information of the incoming feature set, which is utilized to weigh the features based on their estimated informativeness. The proposed method produces state-of-the-art results on long-range unconstrained face recognition datasets such as BTS, and DroneSURF, validating the advantages of such an aggregation strategy.
</details></li>
</ul>
<hr>
<h2 id="Look-Before-You-Leap-An-Exploratory-Study-of-Uncertainty-Measurement-for-Large-Language-Models"><a href="#Look-Before-You-Leap-An-Exploratory-Study-of-Uncertainty-Measurement-for-Large-Language-Models" class="headerlink" title="Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models"></a>Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10236">http://arxiv.org/abs/2307.10236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuheng Huang, Jiayang Song, Zhijie Wang, Huaming Chen, Lei Ma</li>
<li>for: 这篇论文旨在探讨大语言模型（LLMs）的信任性问题，尤其是在安全性、安全性和可靠性等方面。</li>
<li>methods: 本研究使用了十二种不确定性估计方法，并在四个大语言模型（LLMs）上进行了四个常见的自然语言处理（NLP）任务的实验，以探索 LLMS 的预测风险。</li>
<li>results: 研究发现，不确定性估计方法可以帮助揭示 LLMS 的不确定&#x2F;非事实预测，并且在代码生成 зада务中发现了一些buggy程式的存在。<details>
<summary>Abstract</summary>
The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of LLMs. Our findings validate the effectiveness of uncertainty estimation for revealing LLMs' uncertain/non-factual predictions. In addition to general NLP tasks, we extensively conduct experiments with four LLMs for code generation on two datasets. We find that uncertainty estimation can potentially uncover buggy programs generated by LLMs. Insights from our study shed light on future design and development for reliable LLMs, facilitating further research toward enhancing the trustworthiness of LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Automated-Polynomial-Filter-Learning-for-Graph-Neural-Networks"><a href="#Automated-Polynomial-Filter-Learning-for-Graph-Neural-Networks" class="headerlink" title="Automated Polynomial Filter Learning for Graph Neural Networks"></a>Automated Polynomial Filter Learning for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07956">http://arxiv.org/abs/2307.07956</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Wendi Yu, Zhichao Hou, Xiaorui Liu</li>
<li>for: 本研究旨在探讨波利omial图像筛选器学习方法的潜力和局限性，以及提出一种通用的自动化波利omial图像筛选器学习框架，以提高波利omial图像筛选器的效果。</li>
<li>methods: 本研究使用了波利omial图像筛选器的适应学习方法，并提出了一种新的自动化波利omial图像筛选器学习框架，它可以有效地适应不同类型的图像信号。</li>
<li>results: 实验和减少研究表明，使用自动化波利omial图像筛选器学习框架可以提高波利omial图像筛选器的性能，并且在不同的学习设置下具有显著的鲁棒性和稳定性。<details>
<summary>Abstract</summary>
Polynomial graph filters have been widely used as guiding principles in the design of Graph Neural Networks (GNNs). Recently, the adaptive learning of the polynomial graph filters has demonstrated promising performance for modeling graph signals on both homophilic and heterophilic graphs, owning to their flexibility and expressiveness. In this work, we conduct a novel preliminary study to explore the potential and limitations of polynomial graph filter learning approaches, revealing a severe overfitting issue. To improve the effectiveness of polynomial graph filters, we propose Auto-Polynomial, a novel and general automated polynomial graph filter learning framework that efficiently learns better filters capable of adapting to various complex graph signals. Comprehensive experiments and ablation studies demonstrate significant and consistent performance improvements on both homophilic and heterophilic graphs across multiple learning settings considering various labeling ratios, which unleashes the potential of polynomial filter learning.
</details>
<details>
<summary>摘要</summary>
幂数图 filters 已经广泛应用于图神经网络（GNNs）的设计中，最近，可变学习幂数图 filters 的表现很出色，可以模型具有同性和不同性图号的图信号。在这项工作中，我们进行了一项首次的初步研究，探讨幂数图 filters 学习方法的潜力和限制，发现了严重的过拟合问题。为了改善幂数图 filters 的效果，我们提议了 Auto-Polynomial，一种新的和通用的自动幂数图 filters 学习框架，可以高效地学习更好的 filters，以适应不同的复杂图号。经过了广泛的实验和简要研究，我们发现在不同的学习设置下，Auto-Polynomial 能够在同性和不同性图号上提供显著和稳定的性能提升。
</details></li>
</ul>
<hr>
<h2 id="MinT-Boosting-Generalization-in-Mathematical-Reasoning-via-Multi-View-Fine-Tuning"><a href="#MinT-Boosting-Generalization-in-Mathematical-Reasoning-via-Multi-View-Fine-Tuning" class="headerlink" title="MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning"></a>MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07951">http://arxiv.org/abs/2307.07951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenwen Liang, Dian Yu, Xiaoman Pan, Wenlin Yao, Qingkai Zeng, Xiangliang Zhang, Dong Yu</li>
<li>For: The paper aims to improve mathematical reasoning in relatively small language models (LMs) by introducing a multi-view fine-tuning method that leverages diverse annotation styles in existing mathematical problem datasets.* Methods: The proposed method postpones distinct instructions to input questions and trains the model to generate solutions in diverse formats in a flexible manner, utilizing the various annotation formats as different “views”.* Results: The approach outperforms prior knowledge distillation-based methods and carefully established baselines, with the model demonstrating promising generalization ability across various views and datasets, as well as the capability to learn from inaccurate or incomplete noisy data.<details>
<summary>Abstract</summary>
Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on knowledge distillation from powerful but inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different "views" and leverages them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables a LLaMA-7B model to outperform prior approaches that utilize knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promising generalization ability across various views and datasets, and the capability to learn from inaccurate or incomplete noisy data. We hope our multi-view training paradigm could inspire future studies in other machine reasoning domains.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT推理在数学领域仍然是小语言模型（LM）面临的主要挑战。许多当前方法是特化LM来进行数学推理，并且依赖于强大 yet inefficient 大语言模型（LLM）的知识填充。在这项工作中，我们explore一个新的方向，避免过于依赖LLM教师，并 introducing a multi-view fine-tuning方法，高效地利用现有的数学问题Dataset中的多种注释样式。我们的方法会考虑不同的注释格式为不同的"视图"，并在训练模型时进行权重调整。通过在输入问题上附加特定的指令，我们的模型可以学习生成多种格式的解决方案，并且在灵活的方式下进行解决。实验结果显示，我们的策略可以让LLaMA-7B模型超越先前使用知识填充的方法，以及特制的基eline。此外，我们的方法还授予模型在不同的视图和数据集上具有扩展性和可学习性，并且能够从不准确或 incomplete 的噪音数据中学习。我们希望我们的多视图训练方法能够激发未来的机器推理领域的研究。
</details></li>
</ul>
<hr>
<h2 id="SentimentGPT-Exploiting-GPT-for-Advanced-Sentiment-Analysis-and-its-Departure-from-Current-Machine-Learning"><a href="#SentimentGPT-Exploiting-GPT-for-Advanced-Sentiment-Analysis-and-its-Departure-from-Current-Machine-Learning" class="headerlink" title="SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning"></a>SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10234">http://arxiv.org/abs/2307.10234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kiana Kheiri, Hamid Karimi</li>
<li>for: 这个研究的目的是对多种基于Transformer的生成推荐方法（GPT）在情感分析任务中进行了系统性的评估，特别是在SemEval 2017数据集上进行的任务4。</li>
<li>methods: 这个研究使用了三种主要策略：1）使用高级GPT-3.5 Turbo进行提示工程，2）精度地调整GPT模型，3）一种创新的嵌入分类方法。</li>
<li>results: 研究发现GPT方法在情感分析任务中的预测性能明显高于当前最佳性能，准确率高于22%；此外，研究还发现GPT模型在处理复杂情感任务时的能力更强，例如理解上下文和检测嘲讽。<details>
<summary>Abstract</summary>
This study presents a thorough examination of various Generative Pretrained Transformer (GPT) methodologies in sentiment analysis, specifically in the context of Task 4 on the SemEval 2017 dataset. Three primary strategies are employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2) fine-tuning GPT models, and 3) an inventive approach to embedding classification. The research yields detailed comparative insights among these strategies and individual GPT models, revealing their unique strengths and potential limitations. Additionally, the study compares these GPT-based methodologies with other current, high-performing models previously used with the same dataset. The results illustrate the significant superiority of the GPT approaches in terms of predictive performance, more than 22\% in F1-score compared to the state-of-the-art. Further, the paper sheds light on common challenges in sentiment analysis tasks, such as understanding context and detecting sarcasm. It underscores the enhanced capabilities of the GPT models to effectively handle these complexities. Taken together, these findings highlight the promising potential of GPT models in sentiment analysis, setting the stage for future research in this field. The code can be found at https://github.com/DSAatUSU/SentimentGPT
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Revisiting-Domain-Adaptive-3D-Object-Detection-by-Reliable-Diverse-and-Class-balanced-Pseudo-Labeling"><a href="#Revisiting-Domain-Adaptive-3D-Object-Detection-by-Reliable-Diverse-and-Class-balanced-Pseudo-Labeling" class="headerlink" title="Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling"></a>Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07944">http://arxiv.org/abs/2307.07944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhuoxiao-chen/redb-da-3ddet">https://github.com/zhuoxiao-chen/redb-da-3ddet</a></li>
<li>paper_authors: Zhuoxiao Chen, Yadan Luo, Zheng Wang, Mahsa Baktashmotlagh, Zi Huang<br>for: 本文提出了一种基于pseudo标签技术的不监督领域适应（DA）方法，用于解决多类训练场景下DA方法的性能下降问题。methods: 我们提出了一种名为ReDB的框架，用于同时学习所有类。我们使用了可靠的、多样化的和平衡的pseudo 3D框来引导自我训练。为了解决环境差异引起的干扰，我们提出了一种跨频道评估（CDE）方法，用于评估pseudo标签的正确性。此外，我们还设计了一种重叠框数（OBC）度量，用于降低计算负担和减少对象偏移。results: 我们的ReDB方法在三个标准 benchmarkdataset上进行了实验，使用了voxel-based（i.e., SECOND）和点 cloud-based 3D检测器（i.e., PointRCNN）。结果显示，我们的方法比现有的3D DA方法提高了23.15%的mAP值，在nuScenes $\rightarrow$ KITTI任务上。<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g., scales and point densities), we design an overlapped boxes counting (OBC) metric that allows to uniformly downsample pseudo-labeled objects across different geometric characteristics. To confront the issue of inter-class imbalance, we progressively augment the target point clouds with a class-balanced set of pseudo-labeled target instances and source objects, which boosts recognition accuracies on both frequently appearing and rare classes. Experimental results on three benchmark datasets using both voxel-based (i.e., SECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that our proposed ReDB approach outperforms existing 3D domain adaptation methods by a large margin, improving 23.15% mAP on the nuScenes $\rightarrow$ KITTI task. The code is available at https://github.com/zhuoxiao-chen/ReDB-DA-3Ddet.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified Chinese<</SYS>>无监督领域适应（DA）技术帮助了3D物体检测预测。虽然有效，但现有DA方法在多类训练场景下表现不佳，因为低质量pseudo标签和类别不均问题存在。在这篇文章中，我们解决这个挑战，提出了一种基于学习检测所有类别的 novel ReDB框架。我们的方法生成了可靠、多样和类别均衡的pseudo 3D框，用于反馈自我训练 distributionally different 目标频谱上。为了消除环境差异（例如，扩散）所引起的干扰，我们提出了跨频谱评估（CDE），用于复制目标实例到源环境中，并测量预测一致性。为了减少计算负担和 Mitigate 对象变换（例如，比例和点密度），我们设计了重叠的框数（OBC）度量，允许用于均一下 pseudo-标签对象的下采样。为了解决类别不均问题，我们逐渐增加了类别均衡的pseudo-标签目标实例和源对象，这有助于提高了识别率，包括常见的类和罕见的类。实验结果表明，我们的提出的 ReDB 方法在三个benchmarkdataset上（包括 voxel-based 和点 clouds 3D检测器）表现出色，相比现有的3D领域适应方法，提高了23.15%的MAP值。代码可以在 <https://github.com/zhuoxiao-chen/ReDB-DA-3Ddet> 上获取。
</details></li>
</ul>
<hr>
<h2 id="KECOR-Kernel-Coding-Rate-Maximization-for-Active-3D-Object-Detection"><a href="#KECOR-Kernel-Coding-Rate-Maximization-for-Active-3D-Object-Detection" class="headerlink" title="KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection"></a>KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07942">http://arxiv.org/abs/2307.07942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Luoyadan/KECOR-active-3Ddet">https://github.com/Luoyadan/KECOR-active-3Ddet</a></li>
<li>paper_authors: Yadan Luo, Zhuoxiao Chen, Zhen Fang, Zheng Zhang, Zi Huang, Mahsa Baktashmotlagh</li>
<li>for: 提高自动驾驶 LiDAR 对象检测器的可靠性，减少标注量。</li>
<li>methods: 使用新的 kernel coding rate maximization (KECOR) 策略，通过信息理论来选择最有用的点云进行标注。</li>
<li>results: 比对现有方法减少了约44% 的标注成本和26% 的计算时间，无需妥协检测性能。<details>
<summary>Abstract</summary>
Achieving a reliable LiDAR-based object detector in autonomous driving is paramount, but its success hinges on obtaining large amounts of precise 3D annotations. Active learning (AL) seeks to mitigate the annotation burden through algorithms that use fewer labels and can attain performance comparable to fully supervised learning. Although AL has shown promise, current approaches prioritize the selection of unlabeled point clouds with high uncertainty and/or diversity, leading to the selection of more instances for labeling and reduced computational efficiency. In this paper, we resort to a novel kernel coding rate maximization (KECOR) strategy which aims to identify the most informative point clouds to acquire labels through the lens of information theory. Greedy search is applied to seek desired point clouds that can maximize the minimal number of bits required to encode the latent features. To determine the uniqueness and informativeness of the selected samples from the model perspective, we construct a proxy network of the 3D detector head and compute the outer product of Jacobians from all proxy layers to form the empirical neural tangent kernel (NTK) matrix. To accommodate both one-stage (i.e., SECOND) and two-stage detectors (i.e., PVRCNN), we further incorporate the classification entropy maximization and well trade-off between detection performance and the total number of bounding boxes selected for annotation. Extensive experiments conducted on two 3D benchmarks and a 2D detection dataset evidence the superiority and versatility of the proposed approach. Our results show that approximately 44% box-level annotation costs and 26% computational time are reduced compared to the state-of-the-art AL method, without compromising detection performance.
</details>
<details>
<summary>摘要</summary>
要实现自适应驾驶中的可靠 LiDAR 基于对象探测器，是必备的，但其成功取决于获得大量精确的3D注释。活动学习（AL）可以减轻注释负担，但目前的方法通常选择低确定性和多样性的无标点云，导致更多的实例需要标注，并降低计算效率。在这篇论文中，我们采用了一种新的核心编码率最大化（KECOR）策略，以便通过信息理论的视角来标识最有用的点云。我们使用贪婪搜索算法来寻找需要标注的点云，以达到最小化数据量的编码。为了评估选择的样本唯一性和有用性，我们构建了一个3D探测器头的卫星网络，并计算所有卫星层的外产Jacobian的outer乘积，以建立empirical神经积分（NTK）矩阵。为了满足一stage（i.e., SECOND）和两stage（i.e., PVRCNN）探测器，我们还 incorporate分类Entropy最大化和检测性能和总绘制框数之间的融合。我们在两个3D benchmark和一个2D探测数据集上进行了广泛的实验，结果表明我们的方法在 box-level 注释成本和计算时间上具有明显的优越性和多样性。我们的结果表明，相比领先方法，我们的方法可以提高约44%的盒级注释成本和26%的计算时间，而无需牺牲检测性能。
</details></li>
</ul>
<hr>
<h2 id="GeoGPT-Understanding-and-Processing-Geospatial-Tasks-through-An-Autonomous-GPT"><a href="#GeoGPT-Understanding-and-Processing-Geospatial-Tasks-through-An-Autonomous-GPT" class="headerlink" title="GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT"></a>GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07930">http://arxiv.org/abs/2307.07930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhang, Cheng Wei, Shangyou Wu, Zhengting He, Wenhao Yu<br>for:The paper is written to explore a new framework called GeoGPT that integrates the semantic understanding ability of large language models (LLMs) with mature tools within the GIS community, aiming to lower the threshold of non-professional users to solve geospatial tasks.methods:The paper utilizes Generative Pre-trained Transformer (e.g., ChatGPT) and AutoGPT to enable the framework to automatically reason and call externally defined tools, and the framework is validated through several cases including geospatial data crawling, spatial query, facility siting, and mapping.results:The results show that GeoGPT can conduct geospatial data collection, processing, and analysis in an autonomous manner with the instruction of only natural language, and the framework is effective in solving various geospatial tasks. The paper also suggests that the “foundational plus professional” paradigm implied in GeoGPT provides an effective way to develop next-generation GIS in this era of large foundation models.<details>
<summary>Abstract</summary>
Decision-makers in GIS need to combine a series of spatial algorithms and operations to solve geospatial tasks. For example, in the task of facility siting, the Buffer tool is usually first used to locate areas close or away from some specific entities; then, the Intersect or Erase tool is used to select candidate areas satisfied multiple requirements. Though professionals can easily understand and solve these geospatial tasks by sequentially utilizing relevant tools, it is difficult for non-professionals to handle these problems. Recently, Generative Pre-trained Transformer (e.g., ChatGPT) presents strong performance in semantic understanding and reasoning. Especially, AutoGPT can further extend the capabilities of large language models (LLMs) by automatically reasoning and calling externally defined tools. Inspired by these studies, we attempt to lower the threshold of non-professional users to solve geospatial tasks by integrating the semantic understanding ability inherent in LLMs with mature tools within the GIS community. Specifically, we develop a new framework called GeoGPT that can conduct geospatial data collection, processing, and analysis in an autonomous manner with the instruction of only natural language. In other words, GeoGPT is used to understand the demands of non-professional users merely based on input natural language descriptions, and then think, plan, and execute defined GIS tools to output final effective results. Several cases including geospatial data crawling, spatial query, facility siting, and mapping validate the effectiveness of our framework. Though limited cases are presented in this paper, GeoGPT can be further extended to various tasks by equipping with more GIS tools, and we think the paradigm of "foundational plus professional" implied in GeoGPT provides an effective way to develop next-generation GIS in this era of large foundation models.
</details>
<details>
<summary>摘要</summary>
决策者在地理信息系统（GIS）中需要结合一系列的空间算法和操作来解决地理任务。例如，在设施位置选址任务中，首先使用 Buffer 工具来确定靠近特定实体的区域，然后使用 Intersect 或 Erase 工具来选择满足多个需求的候选区域。虽然专业人士可以轻松地理解和解决这些地理任务，但非专业人士则有困难。现在，生成预训练的 transformer（例如 ChatGPT）表现出了强大的语义理解和逻辑能力。特别是 AutoGPT 可以进一步扩展大语言模型（LLM）的能力，通过自动理解和调用外部定义的工具。受这些研究启发，我们尝试将非专业用户解决地理任务的门槛降低，通过将语义理解能力内置在 LLM 中与 GIS 社区熟悉的工具相结合。具体来说，我们开发了一个新框架 called GeoGPT，可以在自主 manner 中进行地理数据收集、处理和分析，只需要自然语言输入。即使非专业用户只提供了自然语言描述，GeoGPT 仍可以理解用户的需求，计划和执行定义的 GIS 工具，以获得最终有效的结果。我们在这篇论文中提出了一些案例，包括地理数据抓取、空间查询、设施位置选址和地图生成，证明了 GeoGPT 的有效性。虽然我们只是在这篇论文中提出了有限的案例，但我们认为 GeoGPT 可以在不同的任务中进行扩展，只需要附加更多的 GIS 工具即可。我们认为 GeoGPT 的“基础 plus 专业”的思想提供了下一代 GIS 的有效发展方式。
</details></li>
</ul>
<hr>
<h2 id="Neural-Architecture-Retrieval"><a href="#Neural-Architecture-Retrieval" class="headerlink" title="Neural Architecture Retrieval"></a>Neural Architecture Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07919">http://arxiv.org/abs/2307.07919</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Xiaohuan Pei, Yanxi Li, Minjing Dong, Chang Xu</li>
<li>for: 本研究旨在提高新的神经网络架构设计和现有的神经网络架构之间的相似性检索，以便帮助研究人员更好地将自己的贡献与现有的架构相比较，并确定它们之间的联系。</li>
<li>methods: 本研究使用分解图为模块进行重建大图，并 introduce multi-level contrastive learning来实现准确的图表示学习。</li>
<li>results: 对人工设计和生成的神经网络架构进行了广泛的评估，并证明了我们的算法的优越性。此外，还建立了一个包含12k个真实世界网络架构、其嵌入的数据集，用于神经网络架构检索。<details>
<summary>Abstract</summary>
With the increasing number of new neural architecture designs and substantial existing neural architectures, it becomes difficult for the researchers to situate their contributions compared with existing neural architectures or establish the connections between their designs and other relevant ones. To discover similar neural architectures in an efficient and automatic manner, we define a new problem Neural Architecture Retrieval which retrieves a set of existing neural architectures which have similar designs to the query neural architecture. Existing graph pre-training strategies cannot address the computational graph in neural architectures due to the graph size and motifs. To fulfill this potential, we propose to divide the graph into motifs which are used to rebuild the macro graph to tackle these issues, and introduce multi-level contrastive learning to achieve accurate graph representation learning. Extensive evaluations on both human-designed and synthesized neural architectures demonstrate the superiority of our algorithm. Such a dataset which contains 12k real-world network architectures, as well as their embedding, is built for neural architecture retrieval.
</details>
<details>
<summary>摘要</summary>
随着新的神经网络架构设计的数量不断增加，以及现有的神经网络架构的数量，对研究人员来说已经变得困难以对现有的神经网络架构进行比较，或者确定自己的设计与其他相关的神经网络架构之间的连接。为了在高效自动化的方式下发现相似的神经网络架构，我们定义了神经网络架构检索问题，该问题的目的是检索与查询神经网络架构相似的现有神经网络架构。现有的图预训练策略无法处理神经网络架构中的计算图因为图的大小和动作。为了解决这些问题，我们提议将图分解为模块，使用这些模块来重建宏图，并引入多级对比学习来实现准确的图表示学习。我们对人工设计的和生成的神经网络架构进行了广泛的评估，结果表明我们的算法具有优势。我们还构建了一个包含12000个实际世界网络架构，以及它们的嵌入的数据集，用于神经网络架构检索。
</details></li>
</ul>
<hr>
<h2 id="Recognition-of-Mental-Adjectives-in-An-Efficient-and-Automatic-Style"><a href="#Recognition-of-Mental-Adjectives-in-An-Efficient-and-Automatic-Style" class="headerlink" title="Recognition of Mental Adjectives in An Efficient and Automatic Style"></a>Recognition of Mental Adjectives in An Efficient and Automatic Style</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11767">http://arxiv.org/abs/2307.11767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Yang</li>
<li>for: 本研究旨在提出一种新的常识逻辑任务，即心理和物理分类（MPC），以便处理常识逻辑在逻辑图中。</li>
<li>methods: 本研究使用了一个BERT模型，并采用了活动学习算法来减少需要的标注资源。</li>
<li>results: 使用ENTROPY策略，模型达到了满意的准确率，仅需约300个标注词。此外，我们还与SentiWordNet进行比较，以检验MPC与情感分类任务的差异。<details>
<summary>Abstract</summary>
In recent years, commonsense reasoning has received more and more attention from academic community. We propose a new lexical inference task, Mental and Physical Classification (MPC), to handle commonsense reasoning in a reasoning graph. Mental words relate to mental activities, which fall into six categories: Emotion, Need, Perceiving, Reasoning, Planning and Personality. Physical words describe physical attributes of an object, like color, hardness, speed and malleability. A BERT model is fine-tuned for this task and active learning algorithm is adopted in the training framework to reduce the required annotation resources. The model using ENTROPY strategy achieves satisfactory accuracy and requires only about 300 labeled words. We also compare our result with SentiWordNet to check the difference between MPC and subjectivity classification task in sentiment analysis.
</details>
<details>
<summary>摘要</summary>
近年来，常识理解在学术社区中得到了越来越多的关注。我们提出了一个新的lexical inference任务，即心理和物理分类（MPC），以处理常识理解在推理图中。心理词表示心理活动，分为六类：情感、需求、感知、理解、规划和人格。物理词描述物体的物理属性，如颜色、硬度、速度和柔性。我们使用BERT模型进行了 fine-tuning，并采用了活动学习算法来降低需要的标注资源。使用ENTROPY策略的模型具有了满意的准确率，并只需约300个标注词。我们还与SentiWordNet进行了比较，以检查MPC任务与情感分类任务在情感分析中的差异。
</details></li>
</ul>
<hr>
<h2 id="Is-Imitation-All-You-Need-Generalized-Decision-Making-with-Dual-Phase-Training"><a href="#Is-Imitation-All-You-Need-Generalized-Decision-Making-with-Dual-Phase-Training" class="headerlink" title="Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training"></a>Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07909">http://arxiv.org/abs/2307.07909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yunyikristy/dualmind">https://github.com/yunyikristy/dualmind</a></li>
<li>paper_authors: Yao Wei, Yanchao Sun, Ruijie Zheng, Sai Vemprala, Rogerio Bonatti, Shuhang Chen, Ratnesh Madaan, Zhongjie Ba, Ashish Kapoor, Shuang Ma</li>
<li>for: 这篇论文旨在解决当前方法面临的挑战，如过拟合行为和任务特定的微调。</li>
<li>methods: 该方法使用了一种新的”双相”训练策略，模仿人类如何在世界上学习行为。首先通过一个自然语言控制任务的自我超vised目标学习基础共知，然后通过模仿行为根据给定的提示进行决策。</li>
<li>results: 对MetaWorld和Habitat进行了广泛的实验评估，比前一代通用代理agent高效性70%和50%。在MetaWorld上完成45个任务的成功率高于90%。<details>
<summary>Abstract</summary>
We introduce DualMind, a generalist agent designed to tackle various decision-making tasks that addresses challenges posed by current methods, such as overfitting behaviors and dependence on task-specific fine-tuning. DualMind uses a novel "Dual-phase" training strategy that emulates how humans learn to act in the world. The model first learns fundamental common knowledge through a self-supervised objective tailored for control tasks and then learns how to make decisions based on different contexts through imitating behaviors conditioned on given prompts. DualMind can handle tasks across domains, scenes, and embodiments using just a single set of model weights and can execute zero-shot prompting without requiring task-specific fine-tuning. We evaluate DualMind on MetaWorld and Habitat through extensive experiments and demonstrate its superior generalizability compared to previous techniques, outperforming other generalist agents by over 50$\%$ and 70$\%$ on Habitat and MetaWorld, respectively. On the 45 tasks in MetaWorld, DualMind achieves over 30 tasks at a 90$\%$ success rate.
</details>
<details>
<summary>摘要</summary>
我们介绍了DUALMIND，一种通用的智能代理，用于解决当前方法面临的挑战，如适应性和任务特定的精度调整。DUALMIND使用一种新的“双阶段”培训策略，模拟人类学习行为的方式。首先，模型通过一种自我监督目标，学习基本的共同知识，然后通过模仿行为来学习不同上下文的决策。DUALMIND可以在不同领域、场景和实现中处理任务，只需一个模型权重集，并可以在零基础下进行指令。我们通过对METAWORLD和HABITAT进行了广泛的实验，证明DUALMIND在总体化能力方面表现出优于前一代技术，在HABITAT和METAWORLD上的表现比其他通用代理高出50%和70%。在METAWORLD上的45个任务中，DUALMIND达到了90%的成功率。
</details></li>
</ul>
<hr>
<h2 id="A-Dialogue-System-for-Assessing-Activities-of-Daily-Living-Improving-Consistency-with-Grounded-Knowledge"><a href="#A-Dialogue-System-for-Assessing-Activities-of-Daily-Living-Improving-Consistency-with-Grounded-Knowledge" class="headerlink" title="A Dialogue System for Assessing Activities of Daily Living: Improving Consistency with Grounded Knowledge"></a>A Dialogue System for Assessing Activities of Daily Living: Improving Consistency with Grounded Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07544">http://arxiv.org/abs/2307.07544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhecheng Sheng, Raymond Finzel, Michael Lucke, Sheena Dufresne, Maria Gini, Serguei Pakhomov</li>
<li>for: 这种对话系统用于帮助评估人员更好地评估参与者的功能能力，尤其是新手评估人员。</li>
<li>methods: 这种对话系统使用自然语言理解（NLU）和自然语言生成（NLG）两个模块，通过模拟实际对话来评估参与者的功能能力。</li>
<li>results: 通过使用最新的InstructGPT-like模型，对话系统可以根据参与者的生活背景信息和查询来生成相应的回答，以保证对话系统的回答与知识库之间的一致性。<details>
<summary>Abstract</summary>
In healthcare, the ability to care for oneself is reflected in the "Activities of Daily Living (ADL)," which serve as a measure of functional ability (functioning). A lack of functioning may lead to poor living conditions requiring personal care and assistance. To accurately identify those in need of support, assistance programs continuously evaluate participants' functioning across various domains. However, the assessment process may encounter consistency issues when multiple assessors with varying levels of expertise are involved. Novice assessors, in particular, may lack the necessary preparation for real-world interactions with participants. To address this issue, we developed a dialogue system that simulates interactions between assessors and individuals of varying functioning in a natural and reproducible way. The dialogue system consists of two major modules, one for natural language understanding (NLU) and one for natural language generation (NLG), respectively. In order to generate responses consistent with the underlying knowledge base, the dialogue system requires both an understanding of the user's query and of biographical details of an individual being simulated. To fulfill this requirement, we experimented with query classification and generated responses based on those biographical details using some recently released InstructGPT-like models.
</details>
<details>
<summary>摘要</summary>
在医疗领域，自我照顾能力反映在日常生活活动（ADL）中， serves as a measure of functional ability（功能）。lack of functioning may lead to poor living conditions requiring personal care and assistance. To accurately identify those in need of support, assistance programs continuously evaluate participants' functioning across various domains. However, the assessment process may encounter consistency issues when multiple assessors with varying levels of expertise are involved. Novice assessors, in particular, may lack the necessary preparation for real-world interactions with participants. To address this issue, we developed a dialogue system that simulates interactions between assessors and individuals of varying functioning in a natural and reproducible way. The dialogue system consists of two major modules, one for natural language understanding (NLU) and one for natural language generation (NLG), respectively. In order to generate responses consistent with the underlying knowledge base, the dialogue system requires both an understanding of the user's query and of biographical details of an individual being simulated. To fulfill this requirement, we experimented with query classification and generated responses based on those biographical details using some recently released InstructGPT-like models.Note: The text has been translated using Google Translate, and some minor adjustments may be necessary to ensure accuracy and fluency.
</details></li>
</ul>
<hr>
<h2 id="Anomaly-Detection-in-Automated-Fibre-Placement-Learning-with-Data-Limitations"><a href="#Anomaly-Detection-in-Automated-Fibre-Placement-Learning-with-Data-Limitations" class="headerlink" title="Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations"></a>Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07893">http://arxiv.org/abs/2307.07893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Assef Ghamisi, Todd Charter, Li Ji, Maxime Rivard, Gil Lund, Homayoun Najjaran</li>
<li>for: 实时检测 Automated Fibre Placement (AFP) 中的瑕疵，以免让产品质量受到影响。</li>
<li>methods: 融合深度学习和传统computer vision算法，不需要大量的 Labelled defective samples 进行训练。使用对称性优化的抽取方法，从 AFP 的 fibre layup 表面中提取地方样本，并训练 autoencoder 来检测瑕疵。</li>
<li>results: 这种方法可以实时检测 AFP 中的瑕疵，并对产品质量进行严格的检测，而不需要大量的 Labelled defective samples。实验结果显示，这种方法可以实现高精度的瑕疵检测，并且可以检测到所有类型的瑕疵。<details>
<summary>Abstract</summary>
Conventional defect detection systems in Automated Fibre Placement (AFP) typically rely on end-to-end supervised learning, necessitating a substantial number of labelled defective samples for effective training. However, the scarcity of such labelled data poses a challenge. To overcome this limitation, we present a comprehensive framework for defect detection and localization in Automated Fibre Placement. Our approach combines unsupervised deep learning and classical computer vision algorithms, eliminating the need for labelled data or manufacturing defect samples. It efficiently detects various surface issues while requiring fewer images of composite parts for training. Our framework employs an innovative sample extraction method leveraging AFP's inherent symmetry to expand the dataset. By inputting a depth map of the fibre layup surface, we extract local samples aligned with each composite strip (tow). These samples are processed through an autoencoder, trained on normal samples for precise reconstructions, highlighting anomalies through reconstruction errors. Aggregated values form an anomaly map for insightful visualization. The framework employs blob detection on this map to locate manufacturing defects. The experimental findings reveal that despite training the autoencoder with a limited number of images, our proposed method exhibits satisfactory detection accuracy and accurately identifies defect locations. Our framework demonstrates comparable performance to existing methods, while also offering the advantage of detecting all types of anomalies without relying on an extensive labelled dataset of defects.
</details>
<details>
<summary>摘要</summary>
传统的自动纤维布置（AFP）系统通常采用端到端的supervised learning，需要一大量的标注的异常样本来进行有效的训练。然而，获取这些标注的样本是一个挑战。为了解决这个限制，我们提出了一个全面的异常检测和地图化方法 для自动纤维布置。我们的方法结合了深度学习和经典的计算机视觉算法，不需要标注的样本或制造异常样本。它能够高效地检测多种表面问题，只需要训练 fewer 的复合部件图像。我们的框架使用了一种创新的样本EXTRACTION方法，利用 AFP 的自然的对称性来扩大数据集。通过输入纤维布置表面的深度地图，我们EXTRACT了本地的样本，这些样本与每个复合带（tow）align。这些样本被处理 durch一个自动编码器，该自动编码器在正常样本上进行了精准的重建，通过重建错误来高亮异常。相对值形成了异常地图，用于有用的可视化。我们的框架使用了球体检测来确定制造异常。实验结果表明，即使使用有限的图像训练，我们提议的方法可以具有满意的检测精度，并准确地确定异常的位置。我们的框架与现有的方法相比，同时可以检测所有类型的异常，不需要大量的标注的异常样本。
</details></li>
</ul>
<hr>
<h2 id="Handwritten-and-Printed-Text-Segmentation-A-Signature-Case-Study"><a href="#Handwritten-and-Printed-Text-Segmentation-A-Signature-Case-Study" class="headerlink" title="Handwritten and Printed Text Segmentation: A Signature Case Study"></a>Handwritten and Printed Text Segmentation: A Signature Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07887">http://arxiv.org/abs/2307.07887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sina Gholamian, Ali Vahdat</li>
<li>for: 这个论文的目的是解决扫描文档中手写文本与印刷文本的重叠问题，以提高文档的Optical Character Recognition（OCR）和数字化过程。</li>
<li>methods: 该论文提出了新的方法来解决手写和印刷文本的分类问题，包括引入新的数据集SignaTR6K和一种新的模型建立方式。</li>
<li>results: 该论文的最佳配置在两个不同的数据集上比 Priors 的工作提高了17.9%和7.3%的Intersection over Union（IoU）分数。<details>
<summary>Abstract</summary>
While analyzing scanned documents, handwritten text can overlap with printed text. This overlap causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses solely on the binary classification of handwritten text or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This approach results in the assignment of overlapping handwritten and printed pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches to address the challenges of handwritten and printed text segmentation. Our objective is to recover text from different classes in their entirety, especially enhancing the segmentation performance on overlapping sections. To support this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as a new model architecture for the handwritten and printed text segmentation task. Our best configuration outperforms prior work on two different datasets by 17.9% and 7.3% on IoU scores. The SignaTR6K dataset is accessible for download via the following link: https://forms.office.com/r/2a5RDg7cAY.
</details>
<details>
<summary>摘要</summary>
While analyzing scanned documents, handwritten text can overlap with printed text, causing difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurting downstream NLP tasks. Prior research either focuses solely on the binary classification of handwritten text or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This approach results in the assignment of overlapping handwritten and printed pixels to only one of the classes, and thus, they are not accounted for in the other class. Therefore, in this research, we develop novel approaches to address the challenges of handwritten and printed text segmentation. Our objective is to recover text from different classes in their entirety, especially enhancing the segmentation performance on overlapping sections. To support this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as a new model architecture for the handwritten and printed text segmentation task. Our best configuration outperforms prior work on two different datasets by 17.9% and 7.3% on IoU scores. The SignaTR6K dataset is accessible for download via the following link: <https://forms.office.com/r/2a5RDg7cAY>.
</details></li>
</ul>
<hr>
<h2 id="Online-Goal-Recognition-in-Discrete-and-Continuous-Domains-Using-a-Vectorial-Representation"><a href="#Online-Goal-Recognition-in-Discrete-and-Continuous-Domains-Using-a-Vectorial-Representation" class="headerlink" title="Online Goal Recognition in Discrete and Continuous Domains Using a Vectorial Representation"></a>Online Goal Recognition in Discrete and Continuous Domains Using a Vectorial Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07876">http://arxiv.org/abs/2307.07876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Douglas Tesch, Leonardo Rosa Amado, Felipe Meneguzzi</li>
<li>for: 本研究的目的是提出一种高效的在线目标识别方法，可以在精细空间和连续空间两个Domain中进行目标识别。</li>
<li>methods: 本方法使用了一种单一调用 плаanner的方法，以实现在精细空间和连续空间两个Domain中的目标识别。在精细空间中，方法使用了一种简化的动作模型，从而减少了计算负担。</li>
<li>results: 本研究的结果表明，该方法可以在精细空间和连续空间两个Domain中进行高效的在线目标识别，并且可以在几乎实时的速度下完成目标识别。与现有技术相比，该方法的计算量减少了数个数量级，使其成为了首个可以实用于机器人应用的在线目标识别方法。<details>
<summary>Abstract</summary>
While recent work on online goal recognition efficiently infers goals under low observability, comparatively less work focuses on online goal recognition that works in both discrete and continuous domains. Online goal recognition approaches often rely on repeated calls to the planner at each new observation, incurring high computational costs. Recognizing goals online in continuous space quickly and reliably is critical for any trajectory planning problem since the real physical world is fast-moving, e.g. robot applications. We develop an efficient method for goal recognition that relies either on a single call to the planner for each possible goal in discrete domains or a simplified motion model that reduces the computational burden in continuous ones. The resulting approach performs the online component of recognition orders of magnitude faster than the current state of the art, making it the first online method effectively usable for robotics applications that require sub-second recognition.
</details>
<details>
<summary>摘要</summary>
现有研究对在低观察性下高效地识别目标，但相比之下，更少的研究集中在网上目标识别，包括网上目标识别在网上和连续域中。网上目标识别方法通常需要重复访问 плаanner，这会导致高度 computation costs。在实际的物理世界中，例如机器人应用程序，识别目标在连续空间中快速和可靠是非常重要。我们开发了一种高效的目标识别方法，这种方法可以在网上和连续域中快速识别目标，并且仅需对每个可能的目标进行单一的访问。这种方法与现有的状况对照，效率高得多，可以实现在机器人应用程序中的低秒识别。
</details></li>
</ul>
<hr>
<h2 id="Does-Double-Descent-Occur-in-Self-Supervised-Learning"><a href="#Does-Double-Descent-Occur-in-Self-Supervised-Learning" class="headerlink" title="Does Double Descent Occur in Self-Supervised Learning?"></a>Does Double Descent Occur in Self-Supervised Learning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07872">http://arxiv.org/abs/2307.07872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yonatangideoni/double_descent_tiny_paper">https://github.com/yonatangideoni/double_descent_tiny_paper</a></li>
<li>paper_authors: Alisia Lupidi, Yonatan Gideoni, Dulhan Jayalath</li>
<li>for: 研究双重下降现象的大多数研究是基于supervised模型，而自适应 Setting的 исследования却发现surprisingly absent。</li>
<li>methods: 我们使用标准和线性autoencoder两种未研究过的设定进行实验。</li>
<li>results: 我们发现测试损失函数 either exhibits a classical U-shape or monotonically decreases, rather than displaying a double-descent curve.Note: “双重下降” (double descent) in Chinese is “双重下降” (shuāngzhòng jiàoxiàng).<details>
<summary>Abstract</summary>
Most investigations into double descent have focused on supervised models while the few works studying self-supervised settings find a surprising lack of the phenomenon. These results imply that double descent may not exist in self-supervised models. We show this empirically using a standard and linear autoencoder, two previously unstudied settings. The test loss is found to have either a classical U-shape or to monotonically decrease instead of exhibiting a double-descent curve. We hope that further work on this will help elucidate the theoretical underpinnings of this phenomenon.
</details>
<details>
<summary>摘要</summary>
大多数研究双峰现象都集中在指导学习模型上，而自动学习设置中的研究却很少，这些结果表明双峰现象可能不存在于自动学习模型中。我们通过标准的自动encoder和线性的自动encoder两种未经研究的设置来证明这点。测试损失的曲线是 Either exhibiting a classical U-shape or monotonically decreasing, rather than a double-descent curve. 我们希望进一步的研究能够推动这个现象的理论基础的探索。
</details></li>
</ul>
<hr>
<h2 id="The-SocialAI-School-Insights-from-Developmental-Psychology-Towards-Artificial-Socio-Cultural-Agents"><a href="#The-SocialAI-School-Insights-from-Developmental-Psychology-Towards-Artificial-Socio-Cultural-Agents" class="headerlink" title="The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents"></a>The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07871">http://arxiv.org/abs/2307.07871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grgur Kovač, Rémy Portelas, Peter Ford Dominey, Pierre-Yves Oudeyer</li>
<li>for: 这个论文的目的是将发展心理学知识应用到人工智能领域，以便创造更智能的社交交互式代理人。</li>
<li>methods: 这篇论文使用了 Michael Tomasello 和 Jerome Bruner 等心理学家的理论，并提出了一种可 parameterized 的 эксперименталь设计，以便研究社交智能的发展。</li>
<li>results: 这篇论文提出了一种名为 SocialAI 的学术学习平台，可以帮助研究人员在社交智能方面进行实验研究，并提供了一些示例实验。<details>
<summary>Abstract</summary>
Developmental psychologists have long-established the importance of socio-cognitive abilities in human intelligence. These abilities enable us to enter, participate and benefit from human culture. AI research on social interactive agents mostly concerns the emergence of culture in a multi-agent setting (often without a strong grounding in developmental psychology). We argue that AI research should be informed by psychology and study socio-cognitive abilities enabling to enter a culture too. We discuss the theories of Michael Tomasello and Jerome Bruner to introduce some of their concepts to AI and outline key concepts and socio-cognitive abilities. We present The SocialAI school - a tool including a customizable parameterized uite of procedurally generated environments, which simplifies conducting experiments regarding those concepts. We show examples of such experiments with RL agents and Large Language Models. The main motivation of this work is to engage the AI community around the problem of social intelligence informed by developmental psychology, and to provide a tool to simplify first steps in this direction. Refer to the project website for code and additional information: https://sites.google.com/view/socialai-school.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose the SocialAI school, a tool that includes a customizable parameterized suite of procedurally generated environments to simplify experiments related to socio-cognitive abilities. Our goal is to engage the AI community in exploring the problem of social intelligence informed by developmental psychology and provide a tool to facilitate initial explorations in this area.The SocialAI school is based on the theories of Michael Tomasello and Jerome Bruner, who have contributed significantly to the understanding of socio-cognitive abilities. We introduce key concepts and abilities, such as joint attention, intentional understanding, and cultural learning, and demonstrate their application in experiments using reinforcement learning (RL) agents and large language models.Our main motivation is to encourage the AI community to explore the importance of social intelligence informed by developmental psychology. We believe that by integrating insights from psychology and AI, we can create more sophisticated and human-like intelligent systems. For more information and to access the tool, please refer to the project website at <https://sites.google.com/view/socialai-school>.
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-as-Superpositions-of-Cultural-Perspectives"><a href="#Large-Language-Models-as-Superpositions-of-Cultural-Perspectives" class="headerlink" title="Large Language Models as Superpositions of Cultural Perspectives"></a>Large Language Models as Superpositions of Cultural Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07870">http://arxiv.org/abs/2307.07870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grgur Kovač, Masataka Sawayama, Rémy Portelas, Cédric Colas, Peter Ford Dominey, Pierre-Yves Oudeyer</li>
<li>for: 这 paper  investigate 大型自然语言模型 (LLM) 是否可以看作一个 superposition 的 perspectives 和 values.</li>
<li>methods: 作者使用了问卷测试 (PVQ, VSM, IPIP) 来研究 LLM 在不同 context 下表现出的 values 和 personality traits 是如何变化的.</li>
<li>results: 研究发现 LLM 在不同 prompt 中表现出的 values 和 personality traits 是 Context-dependent 的，并且可以通过不同的方法来控制这些 values 和 personality traits.I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrating their context-dependent nature). We then conduct quantitative experiments to study the controllability of different models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the effectiveness of various methods for inducing perspectives, and the smoothness of the models' drivability. We conclude by examining the broader implications of our work and outline a variety of associated scientific questions. The project website is available at https://sites.google.com/view/llm-superpositions .
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM） oftentimes 被误解为具有人格或一组价值观。我们认为 LLM 可以被看作是一个组合多种看法的超position，每个看法都具有不同的价值观和人格特质。LLM 在不同的上下文中会表现出不同的价值观和人格特质，而人类则 tend to have more coherent 的价值观和人格特质 across contexts。我们引入了“ perspective controllability”的概念，它指的是一个模型在不同的上下文中能够采取不同的看法。我们通过问卷调查（PVQ、VSM、IPIP）研究 LLM 在不同上下文中表现出的价值观和人格特质是如何改变的。我们还通过实验证明 LLM 在不同上下文中表现出不同的价值观，并且这些价值观甚至在不明显的上下文中仍然会改变。我们然后进行了量化实验，研究不同模型（GPT-4、GPT-3.5、OpenAssistant、StableVicuna、StableLM）的可控性、对不同上下文的适应度以及模型的运作流略。我们最后结论是，我们的工作具有更广泛的科学影响和多种相关的科学问题。详情可以参考我们的项目网站：https://sites.google.com/view/llm-superpositions。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-the-Effectiveness-of-Classification-Algorithms-and-SVM-Kernels-for-Dry-Beans"><a href="#Benchmarking-the-Effectiveness-of-Classification-Algorithms-and-SVM-Kernels-for-Dry-Beans" class="headerlink" title="Benchmarking the Effectiveness of Classification Algorithms and SVM Kernels for Dry Beans"></a>Benchmarking the Effectiveness of Classification Algorithms and SVM Kernels for Dry Beans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07863">http://arxiv.org/abs/2307.07863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anant Mehta, Prajit Sengupta, Divisha Garg, Harpreet Singh, Yosi Shacham Diamand</li>
<li>for: 该研究旨在帮助植物育种人和农业研究人员提高作物产量，通过分析杯装豇产品数据集来发现感兴趣的特征、病虫害抵抗性和营养含量。</li>
<li>methods: 本研究使用了不同的支持向量机(SVM)分类算法，包括线性、多项式和卷积函数(RBF)，以及其他流行的分类算法，对杯装豇产品数据集进行分类。在进行分类之前，使用了主成分分析(PCA)来降维度。</li>
<li>results: 根据准确率为93.34%、准确率为92.61%、准确率为92.35%和F1得分为91.40%来评估算法的性能，RBF SVM kernel算法 achieve最高的准确率。此外，通过适当的视觉化和实验分析，本研究为复杂和非线性结构数据集的分类提供了有价值的指导。<details>
<summary>Abstract</summary>
Plant breeders and agricultural researchers can increase crop productivity by identifying desirable features, disease resistance, and nutritional content by analysing the Dry Bean dataset. This study analyses and compares different Support Vector Machine (SVM) classification algorithms, namely linear, polynomial, and radial basis function (RBF), along with other popular classification algorithms. The analysis is performed on the Dry Bean Dataset, with PCA (Principal Component Analysis) conducted as a preprocessing step for dimensionality reduction. The primary evaluation metric used is accuracy, and the RBF SVM kernel algorithm achieves the highest Accuracy of 93.34%, Precision of 92.61%, Recall of 92.35% and F1 Score as 91.40%. Along with adept visualization and empirical analysis, this study offers valuable guidance by emphasizing the importance of considering different SVM algorithms for complex and non-linear structured datasets.
</details>
<details>
<summary>摘要</summary>
植物育种者和农业研究人员可以提高作物产量，通过识别有利特征、疾病抵抗和营养含量，通过分析扁豆数据集。本研究分析和比较不同支持向量机(SVM)分类算法，包括直线、多项式和径向基函数(RBF)，以及其他流行的分类算法。经预处理（主成分分析），对扁豆数据集进行分类。主要评价指标为准确率，RBF SVM 算法实现最高准确率为 93.34%、精度为 92.61%、回归率为 92.35% 和 F1 分数为 91.40%。此外，通过明确的视觉分析和实际分析，本研究提供了有价值的指导，强调考虑不同 SVM 算法来处理复杂和非线性结构的数据集。
</details></li>
</ul>
<hr>
<h2 id="Automated-Knowledge-Modeling-for-Cancer-Clinical-Practice-Guidelines"><a href="#Automated-Knowledge-Modeling-for-Cancer-Clinical-Practice-Guidelines" class="headerlink" title="Automated Knowledge Modeling for Cancer Clinical Practice Guidelines"></a>Automated Knowledge Modeling for Cancer Clinical Practice Guidelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10231">http://arxiv.org/abs/2307.10231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pralaypati Ta, Bhumika Gupta, Arihant Jain, Sneha Sree C, Arunima Sarkar, Keerthi Ram, Mohanasankar Sivaprakasam</li>
<li>for: 本研究旨在提取和生成临床实践指南（CPGs）中的知识，以便实现这些指南的程序式交互。</li>
<li>methods: 本研究使用自动化方法提取国家全面癌病网络（NCCN）CPGs中的知识，并生成一个结构化模型。三种激发策略，包括肿瘤分期信息、Unified Medical Language System（UMLS）METAthesaurus &amp; National Cancer Institute thesaurus（NCIt）术语、和节点分类，也被提出以增强模型，以便实现癌病护理指南的程序式浏览和查询。</li>
<li>results: 节点分类使用支持向量机（SVM）模型，实现分类精度为0.81，通过十fold交叉验证。<details>
<summary>Abstract</summary>
Clinical Practice Guidelines (CPGs) for cancer diseases evolve rapidly due to new evidence generated by active research. Currently, CPGs are primarily published in a document format that is ill-suited for managing this developing knowledge. A knowledge model of the guidelines document suitable for programmatic interaction is required. This work proposes an automated method for extraction of knowledge from National Comprehensive Cancer Network (NCCN) CPGs in Oncology and generating a structured model containing the retrieved knowledge. The proposed method was tested using two versions of NCCN Non-Small Cell Lung Cancer (NSCLC) CPG to demonstrate the effectiveness in faithful extraction and modeling of knowledge. Three enrichment strategies using Cancer staging information, Unified Medical Language System (UMLS) Metathesaurus & National Cancer Institute thesaurus (NCIt) concepts, and Node classification are also presented to enhance the model towards enabling programmatic traversal and querying of cancer care guidelines. The Node classification was performed using a Support Vector Machine (SVM) model, achieving a classification accuracy of 0.81 with 10-fold cross-validation.
</details>
<details>
<summary>摘要</summary>
临床实践指南 (CPGs)  для癌症疾病在新证据的激发下逐渐发展。目前，CPGs 主要以文档格式发布，这种格式不适合管理这些发展中的知识。本工作提出了一种自动提取临床指南文档中的知识并生成结构化模型的方法。该方法在使用两个版本的国家癌症网络 (NCCN) Non-Small Cell Lung Cancer (NSCLC) 临床指南进行测试，以证明该方法的效果是忠实地提取和模型知识。此外，本文还描述了三种扩充策略，包括肿瘤分期信息、Unified Medical Language System (UMLS) Metathesaurus & National Cancer Institute thesaurus (NCIt) 概念和节点分类，以增强模型，使其可以进行程序化的浏览和查询癌症护理指南。节点分类使用支持向量机 (SVM) 模型，实现分类精度为 0.81 的十fold十字验证。
</details></li>
</ul>
<hr>
<h2 id="A-Multi-Heuristic-Search-based-Motion-Planning-for-Automated-Parking"><a href="#A-Multi-Heuristic-Search-based-Motion-Planning-for-Automated-Parking" class="headerlink" title="A Multi-Heuristic Search-based Motion Planning for Automated Parking"></a>A Multi-Heuristic Search-based Motion Planning for Automated Parking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07857">http://arxiv.org/abs/2307.07857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bhargav Adabala, Zlatan Ajanović<br>for:这篇论文是关于在无结构环境中，如停车场或建筑现场，由于搜索空间的巨大性和车辆的动态约束，实时规划是一项挑战。methods:本文采用多хеURISTIC搜索方法，通过使用多个хеURISTIC函数，捕捉不同的搜索空间复杂性，并且可以充分发挥每个хеURISTIC函数的优势。results:与Hybrid A<em>算法进行比较，Multi-Heuristic A</em>算法在计算效率和动作计划质量两个方面占据了优势。<details>
<summary>Abstract</summary>
In unstructured environments like parking lots or construction sites, due to the large search-space and kinodynamic constraints of the vehicle, it is challenging to achieve real-time planning. Several state-of-the-art planners utilize heuristic search-based algorithms. However, they heavily rely on the quality of the single heuristic function, used to guide the search. Therefore, they are not capable to achieve reasonable computational performance, resulting in unnecessary delays in the response of the vehicle. In this work, we are adopting a Multi-Heuristic Search approach, that enables the use of multiple heuristic functions and their individual advantages to capture different complexities of a given search space. Based on our knowledge, this approach was not used previously for this problem. For this purpose, multiple admissible and non-admissible heuristic functions are defined, the original Multi-Heuristic A* Search was extended for bidirectional use and dealing with hybrid continuous-discrete search space, and a mechanism for adapting scale of motion primitives is introduced. To demonstrate the advantage, the Multi-Heuristic A* algorithm is benchmarked against a very popular heuristic search-based algorithm, Hybrid A*. The Multi-Heuristic A* algorithm outperformed baseline in both terms, computation efficiency and motion plan (path) quality.
</details>
<details>
<summary>摘要</summary>
在无结构环境中，如停车场或建筑现场，因车辆的搜寻空间和运动约束导致实时规划成为挑战。许多现代的规划器使用了对搜寻的搜寻函数来导航搜寻。但是，这些规划器对单一搜寻函数的质量依赖甚高，因此在实际应用中会导致无必要的延迟。在这个工作中，我们运用了多个搜寻函数的多搜寻方法，以利用不同的搜寻函数优点，捕捉不同的搜寻空间复杂性。根据我们的知识，这种方法在这个问题上没有被使用过。因此，我们定义了多个可行和非可行的搜寻函数，扩展了原始的多搜寻A*搜寻算法，以便对两向和混合点几何搜寻空间进行搜寻，并导入了动态减少运动元素的机制。为了证明优势，我们将多搜寻A*算法与非常受欢迎的搜寻函数基本算法（Hybrid A*）进行比较。结果显示，Multi-Heuristic A*算法在计算效率和运动规划（路径）质量两个方面都高于基eline。
</details></li>
</ul>
<hr>
<h2 id="AspectCSE-Sentence-Embeddings-for-Aspect-based-Semantic-Textual-Similarity-using-Contrastive-Learning-and-Structured-Knowledge"><a href="#AspectCSE-Sentence-Embeddings-for-Aspect-based-Semantic-Textual-Similarity-using-Contrastive-Learning-and-Structured-Knowledge" class="headerlink" title="AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge"></a>AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07851">http://arxiv.org/abs/2307.07851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Schopf, Emanuel Gerber, Malte Ostendorff, Florian Matthes</li>
<li>for: This paper is written for improving the accuracy of information retrieval tasks by using aspect-based contrastive learning of sentence embeddings.</li>
<li>methods: The paper proposes a new approach called AspectCSE, which uses aspect-based contrastive learning to train sentence embeddings that can capture specific aspects of textual similarity.</li>
<li>results: The paper reports an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. Additionally, the paper shows that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks.<details>
<summary>Abstract</summary>
Generic sentence embeddings provide a coarse-grained approximation of semantic textual similarity but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose using Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks. Finally, we examine the aspect-based sentence embedding space and demonstrate that embeddings of semantically similar aspect labels are often close, even without explicit similarity training between different aspect labels.
</details>
<details>
<summary>摘要</summary>
中文翻译：通用句子嵌入提供了一级划算的 semantic 类似性表示，但它们忽略了特定方面的文本类似性。相反，方面基于句子嵌入提供了基于特定方面的类似性 predictions。这使得类似性预测更加专注于特定要求，并更易于解释。在这篇论文中，我们提出了 AspectCSE，一种方面基于的句子嵌入对比学习方法。结果显示，AspectCSE 在多个方面的信息检索任务上平均提高了3.97%，相比前一个最佳结果。我们还提出了使用 Wikidata 知识图Properties 来训练多个方面的句子嵌入模型，其中多个特定方面同时被考虑在类似性预测中。我们示出了多个方面嵌入在特定方面信息检索任务上的表现优于单个方面嵌入。最后，我们检查了方面基于句子嵌入空间，并证明了不同方面标签的嵌入在semantic上相似时，它们通常处于近距离。
</details></li>
</ul>
<hr>
<h2 id="AIOptimizer-–-A-reinforcement-learning-based-software-performance-optimisation-prototype-for-cost-minimisation"><a href="#AIOptimizer-–-A-reinforcement-learning-based-software-performance-optimisation-prototype-for-cost-minimisation" class="headerlink" title="AIOptimizer – A reinforcement learning-based software performance optimisation prototype for cost minimisation"></a>AIOptimizer – A reinforcement learning-based software performance optimisation prototype for cost minimisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07846">http://arxiv.org/abs/2307.07846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noopur Zambare</li>
<li>for: 本研究论文介绍了一个基于成本reduction的软件性能优化工具AIOptimizer的 проtotype。AIOptimizer使用一个基于强化学习的推荐系统，以提高软件系统的效率和可持续性。</li>
<li>methods: 本研究使用了一个模块化设计、数据收集技术、连续学习和可靠的集成，以提供有效和用户中心的性能优化解决方案。</li>
<li>results: 本研究发现AIOptimizer可以实现精确性、灵活性、可扩展性和用户友善性等设计因素，并且可以实现成本优化、缺陷识别、效率预测和合作等功能。<details>
<summary>Abstract</summary>
This research article introduces AIOptimizer, a prototype for a software performance optimisation tool based on cost reduction. AIOptimizer uses a recommendation system driven by reinforcement learning to improve software system efficiency and affordability. The paper highlights AIOptimizer's design factors, such as accuracy, adaptability, scalability, and user-friendliness. To provide effective and user-centric performance optimisation solutions, it emphasises the use of a modular design, data gathering techniques, continuous learning, and resilient integration. The article also investigates AIOptimizer features such as fault identification, cost optimisation recommendations, efficiency prediction, and cooperation. Furthermore, it explores several software development life cycle models and introduces AIOptimizer uses a reinforcement learning-based recommendation engine for cost optimisation. The purpose of this research study is to highlight AIOptimizer as a prototype that uses advanced optimisation techniques and smart recommendation systems to continually enhance software performance and save expenses. The research focuses on various software development life cycle models, such as the Waterfall model, Iterative model, Spiral model, V-Model, Big Bang model and Agile Model. Each model has advantages and disadvantages, and their usefulness is determined by the project's specifications and characteristics. The AIOptimizer tool is a theoretical prototype for such software performance optimizers.
</details>
<details>
<summary>摘要</summary>
本研究文章介绍了一种基于成本减少的软件性能优化工具prototype，称为AIOptimizer。AIOptimizer使用一种基于强化学习的推荐系统来提高软件系统的效率和可affordability。文章强调了AIOptimizer的设计因素，如准确率、适应性、扩展性和用户友好性。为提供有效和用户中心的性能优化解决方案，它强调了模块化设计、数据收集技术、连续学习和可靠的集成。文章还探讨了AIOptimizer的特性，如错误识别、成本优化建议、效率预测和合作。此外，它还介绍了软件开发生命周期模型，如水fall模型、迭代模型、散点模型、V模型、大 Bang模型和互动模型。每种模型具有优缺点，其用于特定项目的评估和适用性决定。AIOptimizer工具是一种理论上的软件性能优化器。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="RegExplainer-Generating-Explanations-for-Graph-Neural-Networks-in-Regression-Task"><a href="#RegExplainer-Generating-Explanations-for-Graph-Neural-Networks-in-Regression-Task" class="headerlink" title="RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task"></a>RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07840">http://arxiv.org/abs/2307.07840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxing Zhang, Zhuomin Chen, Hao Mei, Dongsheng Luo, Hua Wei</li>
<li>for: 本文旨在解释Graph Regression模型（XAIG-R）的具体行为，以便更好地理解它在 regression 任务中的工作原理。</li>
<li>methods: 本文提出了一种基于信息瓶颈理论的新目标函数，以及一种可以支持多种 GNN 的混合框架。此外，本文还提出了一种对比学习策略，用于处理继承顺序的标签问题。</li>
<li>results: 经验证明，提出的方法能够有效地解释 GNN 模型在 regression 任务中的行为。在三个 benchmark 数据集和一个实际数据集上进行了广泛的实验，结果表明提出的方法能够准确地捕捉 GNN 模型的行为特点。<details>
<summary>Abstract</summary>
Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation. Extensive experiments show the effectiveness of the proposed method in interpreting GNN models in regression tasks.
</details>
<details>
<summary>摘要</summary>
Graph 回归是一种基本任务，在各种图学习任务中受到了越来越多的关注。然而，推理过程经常不可解释。现有的解释技术主要是用于理解 GNN 的归类任务中的行为。在这项工作中，我们寻求一种用于解释 graph 回归模型（XAIG-R）的解释。我们发现现有的方法忽略了分布转移和连续顺序决策边界，这使得它们无法应用于回归任务中。为解决这些挑战，我们提出了一个基于信息瓶颈理论的新目标函数，并引入了一种新的混合框架，可以在不同的 GNN 上进行模型无关的应用。此外，我们还提出了一种对比学习策略，用于处理连续顺序的标签。为验证提出的方法的效果，我们引入了三个标准 benchmark 数据集和一个真实生成的数据集进行评估。广泛的实验表明，提出的方法可以有效地解释 GNN 模型在回归任务中。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/16/cs.AI_2023_07_16/" data-id="cloimip43000rs488gzo6erkx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/16/cs.CL_2023_07_16/" class="article-date">
  <time datetime="2023-07-16T11:00:00.000Z" itemprop="datePublished">2023-07-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/16/cs.CL_2023_07_16/">cs.CL - 2023-07-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Automatic-Identification-of-Alzheimer’s-Disease-using-Lexical-Features-extracted-from-Language-Samples"><a href="#Automatic-Identification-of-Alzheimer’s-Disease-using-Lexical-Features-extracted-from-Language-Samples" class="headerlink" title="Automatic Identification of Alzheimer’s Disease using Lexical Features extracted from Language Samples"></a>Automatic Identification of Alzheimer’s Disease using Lexical Features extracted from Language Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08070">http://arxiv.org/abs/2307.08070</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Zakaria Kurdi</li>
<li>for: 本研究的目的是提高阿尔茨海默病（AD）对不同方面的语言处理的影响的理解，以及使用这些方面的语言特征作为机器学习分类器的特征来实现状态对精度的自动识别语言样本。</li>
<li>methods: 本研究使用了ADDreSS挑战数据集，该数据集来自于DementiaBank集成。研究使用的数据集包括54名参与者在训练部分中提供的Cookie Theft图像描述语音样本，以及24名参与者在测试部分中提供的样本。总的来说，训练集和测试集中的语音样本数为108和48。首先，研究通过分析99个选择的语言特征对AD的影响进行了研究，并在训练和测试部分中进行了语音样本的分类。其次，研究通过不同的语言复杂度区域进行了一些机器学习实验，以确定可以实现优化性能的特征子组合。最后，研究还对语音样本的大小对分类的影响进行了研究。</li>
<li>results: 使用语言特征 seule，可以实现状态对精度高于91%的自动识别语言样本Produced by individuals with AD from those produced by healthy control subjects.这表明AD会对语言处理产生重要的影响。<details>
<summary>Abstract</summary>
Objective: this study has a twofold goal. First, it aims to improve the understanding of the impact of Dementia of type Alzheimer's Disease (AD) on different aspects of the lexicon. Second, it aims to demonstrate that such aspects of the lexicon, when used as features of a machine learning classifier, can help achieve state-of-the-art performance in automatically identifying language samples produced by patients with AD. Methods: data is derived from the ADDreSS challenge, which is a part of the DementiaBank corpus. The used dataset consists of transcripts of Cookie Theft picture descriptions, produced by 54 subjects in the training part and 24 subjects in the test part. The number of narrative samples is 108 in the training set and 48 in the test set. First, the impact of AD on 99 selected lexical features is studied using both the training and testing parts of the dataset. Then some machine learning experiments were conducted on the task of classifying transcribed speech samples with text samples that were produced by people with AD from those produced by normal subjects. Several experiments were conducted to compare the different areas of lexical complexity, identify the subset of features that help achieve optimal performance, and study the impact of the size of the input on the classification. To evaluate the generalization of the models built on narrative speech, two generalization tests were conducted using written data from two British authors, Iris Murdoch and Agatha Christie, and the transcription of some speeches by former President Ronald Reagan. Results: using lexical features only, state-of-the-art classification, F1 and accuracies, of over 91% were achieved in categorizing language samples produced by individuals with AD from the ones produced by healthy control subjects. This confirms the substantial impact of AD on lexicon processing.
</details>
<details>
<summary>摘要</summary>
Methods: The study uses data from the ADDreSS challenge, part of the DementiaBank corpus. The dataset consists of transcripts of Cookie Theft picture descriptions produced by 54 subjects in the training set and 24 subjects in the test set, with 108 narrative samples in the training set and 48 in the test set. The study first examines the impact of AD on 99 selected lexical features using both the training and testing parts of the dataset. Then, machine learning experiments are conducted to classify transcribed speech samples produced by people with AD from those produced by normal subjects. The experiments compare different areas of lexical complexity, identify the subset of features that achieve optimal performance, and study the impact of input size on classification. To evaluate the generalization of the models built on narrative speech, the study conducts two generalization tests using written data from Iris Murdoch and Agatha Christie, and the transcription of some speeches by former President Ronald Reagan.Results: The study achieves state-of-the-art classification, F1, and accuracies of over 91% in categorizing language samples produced by individuals with AD from those produced by healthy control subjects. This confirms the substantial impact of AD on lexicon processing.
</details></li>
</ul>
<hr>
<h2 id="Facilitating-Multi-turn-Emotional-Support-Conversation-with-Positive-Emotion-Elicitation-A-Reinforcement-Learning-Approach"><a href="#Facilitating-Multi-turn-Emotional-Support-Conversation-with-Positive-Emotion-Elicitation-A-Reinforcement-Learning-Approach" class="headerlink" title="Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach"></a>Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07994">http://arxiv.org/abs/2307.07994</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jfzhouyoo/supporter">https://github.com/jfzhouyoo/supporter</a></li>
<li>paper_authors: Jinfeng Zhou, Zhuang Chen, Bo Wang, Minlie Huang</li>
<li>for: 提供情感支持（ES），提高心理状态。</li>
<li>methods: 使用混合专家模型和资料学习，评估对话的凝合性和情感诱发效果。</li>
<li>results: 在回答过程中，Supporter模型可以诱发正面情感，同时维持对话的凝合性。<details>
<summary>Abstract</summary>
Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one's mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy's learning for responding. Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence.
</details>
<details>
<summary>摘要</summary>
emotional support conversation (ESC) 目的是提供情感支持 (ES) 以改善心理状态。现有的工作停留在适应地响应（例如问题），而忽视 ES 的影响和没有明确的目标导航情感积极转移。为此，我们引入了一种新的 парадиг，将多回合 ES 视为积极情感诱发的过程。在这个任务中，需要精准地调整情感诱发强度，以便在对话进行时维持对话目标，包括凝合性。在这篇论文中，我们提出了支持者，一种权重学习模型，并设计了 ES 和对话凝合性奖励来引导策略的学习。实验证明了支持者在回答时积极诱发情感的同时保持对话目标，包括凝合性。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Techniques-for-Optimizing-Transformer-Inference"><a href="#A-Survey-of-Techniques-for-Optimizing-Transformer-Inference" class="headerlink" title="A Survey of Techniques for Optimizing Transformer Inference"></a>A Survey of Techniques for Optimizing Transformer Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07982">http://arxiv.org/abs/2307.07982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krishna Teja Chitty-Venkata, Sparsh Mittal, Murali Emani, Venkatram Vishwanath, Arun K. Somani</li>
<li>for: 本文旨在概述transformer网络在推理阶段的优化技术，包括知识储存、剪枝、量化、 neural architecture search 和轻量级网络设计等方法。</li>
<li>methods: 本文 Survey了一系列的算法级别优化技术，包括知识储存、剪枝、量化、 neural architecture search 和轻量级网络设计等方法。</li>
<li>results: 本文 Summarized了一些模型和技术的量化结果，以及它们的准确率和计算复杂度之间的贸易OFF。 In English, the three main points of the paper are:</li>
<li>for: The paper aims to survey optimization techniques for the inference phase of transformer networks, including knowledge distillation, pruning, quantization, neural architecture search, and lightweight network design.</li>
<li>methods: The paper surveys a series of algorithm-level optimization techniques, including knowledge distillation, pruning, quantization, neural architecture search, and lightweight network design.</li>
<li>results: The paper summarizes the quantitative results of several models and techniques, including the tradeoff between accuracy and computational complexity.<details>
<summary>Abstract</summary>
Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review hardware-level optimization techniques and the design of novel hardware accelerators for transformers. We summarize the quantitative results on the number of parameters/FLOPs and accuracy of several models/techniques to showcase the tradeoff exercised by them. We also outline future directions in this rapidly evolving field of research. We believe that this survey will educate both novice and seasoned researchers and also spark a plethora of research efforts in this field.
</details>
<details>
<summary>摘要</summary>
This paper provides a comprehensive survey of techniques for optimizing transformer inference, including knowledge distillation, pruning, quantization, neural architecture search, and lightweight network design. We also review hardware-level optimization techniques and the design of novel hardware accelerators for transformers. We summarize the quantitative results of several models and techniques to show the tradeoffs they exercise.The survey aims to educate both novice and seasoned researchers in this rapidly evolving field and spark a plethora of research efforts. We believe that this survey will be a valuable resource for researchers and practitioners who are interested in optimizing transformer inference for a wide range of applications.Here is the Simplified Chinese translation of the text:最近几年，变换器神经网络的性能和应用已经惊人地增长。变换器家族，包括bidirectional Encoder Representations from Transformer（BERT）、Generative Pretrained Transformer（GPT）和Vision Transformer（ViT），在自然语言处理（NLP）和计算机视觉（CV）领域都有显著的效果。然而，为了追求高预测性能，变换器的内存和计算核心占用量已经呈指数增长。研究人员已经提出了多种优化变换器推理的技术，包括知识储存、剪辑、量化、神经网络搜索和轻量级网络设计。这篇论文提供了优化变换器推理的全面survey，包括知识储存、剪辑、量化、神经网络搜索和轻量级网络设计。我们还对硬件优化技术进行了评论，以及设计了新的硬件加速器 для变换器。我们SUMMARIZE了几种模型和技术的量化结果，以示其质量和计算量的交易。我们还对未来的发展方向进行了讨论。我们认为，这篇论文将成为 transformer 优化推理的全面资源，对研究人员和实践者都会是非常有价值的。我们期望，这篇论文将能够教育 novice 和经验丰富的研究人员，并促进这个领域的研究努力。
</details></li>
</ul>
<hr>
<h2 id="Model-Adaptation-for-ASR-in-low-resource-Indian-Languages"><a href="#Model-Adaptation-for-ASR-in-low-resource-Indian-Languages" class="headerlink" title="Model Adaptation for ASR in low-resource Indian Languages"></a>Model Adaptation for ASR in low-resource Indian Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07948">http://arxiv.org/abs/2307.07948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhayjeet Singh, Arjun Singh Mehta, Ashish Khuraishi K S, Deekshitha G, Gauri Date, Jai Nanavati, Jesuraja Bandekar, Karnalius Basumatary, Karthika P, Sandhya Badiger, Sathvik Udupa, Saurabh Kumar, Savitha, Prasanta Kumar Ghosh, Prashanthi V, Priyanka Pai, Raoul Nanavati, Rohan Saxena, Sai Praneeth Reddy Mora, Srinivasa Raghavan</li>
<li>for: 本研究旨在探讨如何使用自适应学习和大规模多语言训练提高语音识别性能，特别是对具有限制的语音和文本数据的低资源语言进行应用。</li>
<li>methods: 本研究使用的方法包括使用wav2vec2自适应学习模型和大规模多语言训练，以及对文本和语音数据进行适应和细化。</li>
<li>results: 研究发现，通过对相似语言进行适应和细化，可以在低资源语言中提高语音识别性能，同时也可以利用大量的文本数据来提高语音识别性能。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) performance has improved drastically in recent years, mainly enabled by self-supervised learning (SSL) based acoustic models such as wav2vec2 and large-scale multi-lingual training like Whisper. A huge challenge still exists for low-resource languages where the availability of both audio and text is limited. This is further complicated by the presence of multiple dialects like in Indian languages. However, many Indian languages can be grouped into the same families and share the same script and grammatical structure. This is where a lot of adaptation and fine-tuning techniques can be applied to overcome the low-resource nature of the data by utilising well-resourced similar languages.   In such scenarios, it is important to understand the extent to which each modality, like acoustics and text, is important in building a reliable ASR. It could be the case that an abundance of acoustic data in a language reduces the need for large text-only corpora. Or, due to the availability of various pretrained acoustic models, the vice-versa could also be true. In this proposed special session, we encourage the community to explore these ideas with the data in two low-resource Indian languages of Bengali and Bhojpuri. These approaches are not limited to Indian languages, the solutions are potentially applicable to various languages spoken around the world.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）性能在最近几年内有大幅度改善，主要归功于自我超级学习（SSL）基于音频模型如wav2vec2和大规模多语言训练如Whisper。然而，低资源语言仍然存在巨大挑战，特别是印度语言。这是因为印度语言有多种方言，使得训练ASR模型变得更加困难。然而，许多印度语言可以被分组为同一家族，并且共享同一个字母和语法结构。这使得可以应用大量相似语言的适应和细化技术来抵消低资源数据的问题。在这个特别 sessio中，我们邀请社区探讨以下问题：在印度语言中，音频和文本Modalities在建立可靠ASR模型中的重要性。可能是因为一种语言有充足的音频数据，因此减少了大量文本训练 corpora的需求。或者，由于可用的多种预训练音频模型，因此可能是文本训练 corpora的需求减少了。我们鼓励社区通过使用两种低资源印度语言：孟加拉语和季风语进行研究。这些方法不仅适用于印度语言，也适用于世界各地的其他语言。
</details></li>
</ul>
<hr>
<h2 id="Unifying-Token-and-Span-Level-Supervisions-for-Few-Shot-Sequence-Labeling"><a href="#Unifying-Token-and-Span-Level-Supervisions-for-Few-Shot-Sequence-Labeling" class="headerlink" title="Unifying Token and Span Level Supervisions for Few-Shot Sequence Labeling"></a>Unifying Token and Span Level Supervisions for Few-Shot Sequence Labeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07946">http://arxiv.org/abs/2307.07946</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zifengcheng/cdap">https://github.com/zifengcheng/cdap</a></li>
<li>paper_authors: Zifeng Cheng, Qingyu Zhou, Zhiwei Jiang, Xuemin Zhao, Yunbo Cao, Qing Gu</li>
<li>for: 这篇论文的目的是提出一个能够在几少标签样本的情况下识别新的类别的方法。</li>
<li>methods: 这篇论文提出了一个叫做 Consistent Dual Adaptive Prototypical (CDAP) 网络，这个网络包含了token level和span level的网络，并将它们在不同的粒度上进行联合训练。另外，这篇论文还提出了一个对于两个网络的一致损失函数，以便它们可以从对方学习。</li>
<li>results: 在实验阶段，这篇论文获得了三个benchmark dataset上的新的州际之最的结果。<details>
<summary>Abstract</summary>
Few-shot sequence labeling aims to identify novel classes based on only a few labeled samples. Existing methods solve the data scarcity problem mainly by designing token-level or span-level labeling models based on metric learning. However, these methods are only trained at a single granularity (i.e., either token level or span level) and have some weaknesses of the corresponding granularity. In this paper, we first unify token and span level supervisions and propose a Consistent Dual Adaptive Prototypical (CDAP) network for few-shot sequence labeling. CDAP contains the token-level and span-level networks, jointly trained at different granularities. To align the outputs of two networks, we further propose a consistent loss to enable them to learn from each other. During the inference phase, we propose a consistent greedy inference algorithm that first adjusts the predicted probability and then greedily selects non-overlapping spans with maximum probability. Extensive experiments show that our model achieves new state-of-the-art results on three benchmark datasets.
</details>
<details>
<summary>摘要</summary>
《几个样本级标注 для序列标注》的目标是基于只有几个标注样本来识别新的类别。现有方法主要通过设计token级或Span级标注模型来解决数据缺乏问题，但这些方法只是在单一级别（ тоeken级或Span级）进行训练，它们有相应的缺陷。在这篇论文中，我们首先统一了token级和Span级监督，并提出了一个Consistent Dual Adaptive Prototypical（CDAP）网络 для几个样本级标注。CDAP网络包括token级和Span级网络，在不同的级别进行联合训练。为了将两个网络的输出保持一致，我们还提出了一种一致损失函数，使其可以从彼此学习。在推断阶段，我们提出了一种一致推断算法，首先调整预测概率，然后选择非重叠的Span WITH maximum概率。广泛的实验表明，我们的模型在三个标准 benchmark dataset上达到了新的状态级 результа。
</details></li>
</ul>
<hr>
<h2 id="Deduplicating-and-Ranking-Solution-Programs-for-Suggesting-Reference-Solutions"><a href="#Deduplicating-and-Ranking-Solution-Programs-for-Suggesting-Reference-Solutions" class="headerlink" title="Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions"></a>Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07940">http://arxiv.org/abs/2307.07940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atsushi Shirafuji, Yutaka Watanobe</li>
<li>for: 提高在线评测系统中学生参照多种解决方案的能力</li>
<li>methods: 使用去重和排名常见解决方案来减少参照program的数量</li>
<li>results: 实验结果表明，去重后的program数量减少60.20%，比基准值减少29.59%，meaning users only need to refer to39.80% of programs on average，top-10 ranked programs cover 29.95% of programs on average.<details>
<summary>Abstract</summary>
Referring to the solution programs written by the other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the program can be a reference. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate the learners to refer to various solutions to learn the better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Based on the hypothesis that the more duplicated programs adopt a more common approach and can be a reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a real-world online judge system demonstrate that the number of programs is reduced by 60.20%, whereas the baseline only reduces by 29.59% after the deduplication, meaning that the users only need to refer to 39.80% of programs on average. Furthermore, our analysis shows that top-10 ranked programs cover 29.95% of programs on average, indicating that the users can grasp 29.95% of solution approaches by referring to only 10 programs. The proposed approach shows the potential of reducing the learners' burden of referring to too many solutions and motivating them to learn a variety of better approaches.
</details>
<details>
<summary>摘要</summary>
依据我们的提案，在编程教育中参考其他用户提交的解决方案程序是有帮助的。然而，当前在线评测系统只是列出所有由用户提交的解决方案程序作为参考，并根据提交日期和时间、执行时间或用户评分排序，而忽略了解决方案的多样性。此外，用户很难参考多种解决方法，因为有太多重复和相似的程序。为了鼓励学生参考多种解决方法，并学习更好的解决方法，我们提出了一种方法，可以在每个编程问题中去除重复的解决方案程序，并将唯一的程序排名基于重复计数。我们的实验表明，在一个真实的在线评测系统中，可以将解决方案程序的数量减少了60.20%，而基eline只减少了29.59%，这意味着用户只需要参考39.80%的程序的平均数量。此外，我们的分析表明，排名前10的程序覆盖了29.95%的程序的平均数量，这表明用户可以通过参考只有10个程序来掌握29.95%的解决方法。我们的方法表明可以减轻学生参考太多解决方案程序的负担，并鼓励他们学习更多的更好的解决方法。
</details></li>
</ul>
<hr>
<h2 id="Communicative-Agents-for-Software-Development"><a href="#Communicative-Agents-for-Software-Development" class="headerlink" title="Communicative Agents for Software Development"></a>Communicative Agents for Software Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07924">http://arxiv.org/abs/2307.07924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun</li>
<li>for: 这篇论文旨在探讨一种基于大语言模型（LLM）的软件开发方法，它可以在不同的软件开发阶段中使用自然语言交流来替代专门的模型。</li>
<li>methods: 这篇论文提出了一种基于虚拟对话的软件开发方法，其中每个阶段都有一个团队参与，包括程序员、代码审查人员和测试工程师。在每个阶段中，团队成员可以通过对话来协作、分享想法和解决问题。</li>
<li>results: 研究表明，使用这种方法可以在 less than 7 分钟内完成整个软件开发过程，并且成本低于 1 美元。此外，这种方法还可以快速察觉和修复潜在的漏洞和幻觉，保证了软件的可靠性和效率。<details>
<summary>Abstract</summary>
Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborative dialogue and facilitating a seamless workflow. The chat chain acts as a facilitator, breaking down each stage into atomic subtasks. This enables dual roles, allowing for proposing and validating solutions through context-aware communication, leading to efficient resolution of specific subtasks. The instrumental analysis of ChatDev highlights its remarkable efficacy in software generation, enabling the completion of the entire software development process in under seven minutes at a cost of less than one dollar. It not only identifies and alleviates potential vulnerabilities but also rectifies potential hallucinations while maintaining commendable efficiency and cost-effectiveness. The potential of ChatDev unveils fresh possibilities for integrating LLMs into the realm of software development.
</details>
<details>
<summary>摘要</summary>
软件工程是一个具有复杂决策过程的领域，常常依赖于细腻的直觉和咨询。在最近的深度学习技术的推动下，软件工程做法正在不断发展和改进。在这篇论文中，我们提出了一种创新的思路，利用大语言模型（LLMs）在软件开发过程中扮演重要角色，通过自然语言交流，解决特定阶段的问题，从而消除特殊模型的需求。在我们的思路中，核心是一家虚拟对话驱动的软件开发公司——ChatDev，它类似于传统的水平模型，将软件开发过程分成四个阶段：设计、编程、测试和文档。每个阶段都有一群代表不同职业的代理人，如程序员、代码审查员和测试工程师，通过对话协作，实现了无缝的工作流程。对话链 acts as a facilitator，将每个阶段分解成原子任务，使代理人能够通过上下文感知的沟通，提出和验证解决方案，从而提高效率。我们的研究表明，ChatDev在软件生成过程中具有惊人的效果，可以在七分钟之内完成整个软件开发过程，成本低于一元。它不仅可以找到和消除潜在的漏洞，还可以修正潜在的幻觉，保持了卓越的效率和成本效果。ChatDev的潜在力量探讨出了许多新的软件开发领域的可能性，它可以帮助我们更好地利用深度学习技术，提高软件开发效率和质量。
</details></li>
</ul>
<hr>
<h2 id="Cross-Lingual-NER-for-Financial-Transaction-Data-in-Low-Resource-Languages"><a href="#Cross-Lingual-NER-for-Financial-Transaction-Data-in-Low-Resource-Languages" class="headerlink" title="Cross-Lingual NER for Financial Transaction Data in Low-Resource Languages"></a>Cross-Lingual NER for Financial Transaction Data in Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08714">http://arxiv.org/abs/2307.08714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunisth Kumar, Davide Liu, Alexandre Boulenger</li>
<li>for: 这项研究的目的是提出一种高效的多语言命名实体识别框架，以便在不同语言的 semi-structured 文本数据中进行命名实体识别。</li>
<li>methods: 这种模型设计基于知识储存和一致训练，利用一个大型语言模型（XLMRoBERTa）预训练后的知识，并通过学生教师关系进行知识传递。学生模型还在低资源目标语言上进行不supervised一致训练（使用 KL 异谱损失）。</li>
<li>results: 我们使用了两个独立的英文和阿拉伯文短信数据集，每个数据集包含 semi-structured 银行交易信息，以证明知识传递的可行性。只需训练30个标注样本，我们的模型可以将英文中的商户、金额等场景转移到阿拉伯文中。我们的模型设计方式在比较状态艺术方法（如 DistilBERT 预训练目标语言或直接在目标语言上supervised 训练）时表现出色，并且在总体来说表现最佳。<details>
<summary>Abstract</summary>
We propose an efficient modeling framework for cross-lingual named entity recognition in semi-structured text data. Our approach relies on both knowledge distillation and consistency training. The modeling framework leverages knowledge from a large language model (XLMRoBERTa) pre-trained on the source language, with a student-teacher relationship (knowledge distillation). The student model incorporates unsupervised consistency training (with KL divergence loss) on the low-resource target language.   We employ two independent datasets of SMSs in English and Arabic, each carrying semi-structured banking transaction information, and focus on exhibiting the transfer of knowledge from English to Arabic. With access to only 30 labeled samples, our model can generalize the recognition of merchants, amounts, and other fields from English to Arabic. We show that our modeling approach, while efficient, performs best overall when compared to state-of-the-art approaches like DistilBERT pre-trained on the target language or a supervised model directly trained on labeled data in the target language.   Our experiments show that it is enough to learn to recognize entities in English to reach reasonable performance in a low-resource language in the presence of a few labeled samples of semi-structured data. The proposed framework has implications for developing multi-lingual applications, especially in geographies where digital endeavors rely on both English and one or more low-resource language(s), sometimes mixed with English or employed singly.
</details>
<details>
<summary>摘要</summary>
我们提出了一种高效的模型框架，用于跨语言名实Recognition semi-structured text数据。我们的方法基于知识储存和一致性训练。我们的模型框架利用源语言中已经预训练的大语言模型（XLMRoBERTa），并通过学生与师之间的关系（知识储存）。学生模型包括低资源目标语言中无监督一致性训练（KL异同损失）。我们使用了英文和阿拉伯语两个独立的短信数据集，每个数据集包含英文和阿拉伯语的 semi-structured 银行交易信息。我们主要关注在英文到阿拉伯语的知识传递中。只有访问30个标注样本后，我们的模型可以将英文中的商户、金额和其他字段识别到阿拉伯语中。我们的模型方法，尽管高效，与状态机器翻译的approaches如DistilBERT预训练目标语言或直接在目标语言上监督训练模型相比，表现最佳。我们的实验表明，只要学习英文中的实体，就可以在低资源语言中 reaches reasonable performance，即使只有几个标注样本的 semi-structured 数据。我们的模型框架在发展多语言应用程序方面具有重要意义，特别是在某些地区的数字努力依赖于英文和一些低资源语言（或混合英文）。
</details></li>
</ul>
<hr>
<h2 id="LLM-Comparative-Assessment-Zero-shot-NLG-Evaluation-through-Pairwise-Comparisons-using-Large-Language-Models"><a href="#LLM-Comparative-Assessment-Zero-shot-NLG-Evaluation-through-Pairwise-Comparisons-using-Large-Language-Models" class="headerlink" title="LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models"></a>LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07889">http://arxiv.org/abs/2307.07889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adian Liusie, Potsawee Manakul, Mark J. F. Gales</li>
<li>for: 本研究探讨了如何使用大语言模型（LLM）来自动评估自然语言生成（NLG）的能力，以及 Comparative assessment 的可能性和优势。</li>
<li>methods: 本研究使用了 LLM 的 emergent 能力进行 zero-shot NLG 评估，包括绝对分数预测和比较评估。比较评估使用了对候选作品之间的相对比较，而不是独立地评估每一个候选作品。</li>
<li>results: 研究发现，使用 LLM 进行 Comparative assessment 可以达到 moderate-sized open-source LLMs 的比较好的性能，并且可以超越prompt scoring。此外，研究还发现了 LLB 的位置偏见问题，并提出了一些debiasing方法来进一步改进性能。<details>
<summary>Abstract</summary>
Current developments in large language models (LLMs) have enabled impressive zero-shot capabilities across various natural language tasks. An interesting application of these systems is in the automated assessment of natural language generation (NLG), a highly challenging area with great practical benefit. In this paper, we explore two options for exploiting the emergent abilities of LLMs for zero-shot NLG assessment: absolute score prediction, and comparative assessment which uses relative comparisons between pairs of candidates. Though comparative assessment has not been extensively studied in NLG assessment, we note that humans often find it more intuitive to compare two options rather than scoring each one independently. This work examines comparative assessment from multiple perspectives: performance compared to absolute grading; positional biases in the prompt; and efficient ranking in terms of the number of comparisons. We illustrate that LLM comparative assessment is a simple, general and effective approach for NLG assessment. For moderate-sized open-source LLMs, such as FlanT5 and Llama2-chat, comparative assessment is superior to prompt scoring, and in many cases can achieve performance competitive with state-of-the-art methods. Additionally, we demonstrate that LLMs often exhibit strong positional biases when making pairwise comparisons, and we propose debiasing methods that can further improve performance.
</details>
<details>
<summary>摘要</summary>
现有大型语言模型（LLM）的发展，使得zero-shot能力在不同的自然语言任务中展现出卓越的表现。在这篇论文中，我们探索了两种利用LLM的发现能力进行零shot语言生成评估的方法：绝对分数预测和相对比较方法。相对比较方法，尽管尚未广泛研究在语言生成评估中，但人类往往觉得比较两个选项更直觉。这篇论文从多种角度探讨了相对比较方法：相对于绝对分数评估；提示中的位置偏见；以及有效的排名方法。我们发现了LLM相对比较评估是一种简单、通用且有效的语言生成评估方法。对于中等规模的开源LLM，如FlanT5和Llama2-chat，相对比较评估比提示分数评估更好，且在许多情况下可以与现有方法竞争。此外，我们发现了LLM在比较对比中时常表现出强烈的位置偏见，我们提出了修复方法，可以进一步提高表现。
</details></li>
</ul>
<hr>
<h2 id="Is-Prompt-Based-Finetuning-Always-Better-than-Vanilla-Finetuning-Insights-from-Cross-Lingual-Language-Understanding"><a href="#Is-Prompt-Based-Finetuning-Always-Better-than-Vanilla-Finetuning-Insights-from-Cross-Lingual-Language-Understanding" class="headerlink" title="Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights from Cross-Lingual Language Understanding"></a>Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights from Cross-Lingual Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07880">http://arxiv.org/abs/2307.07880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boleima/profit">https://github.com/boleima/profit</a></li>
<li>paper_authors: Bolei Ma, Ercong Nie, Helmut Schmid, Hinrich Schütze</li>
<li>for: 这个研究旨在研究在多语言预训练语义模型（MPLMs）中的提示基于finetuning的跨语言能力。</li>
<li>methods: 该研究使用了提示基于finetuning的方法，并进行了对多种目标语言的评估。</li>
<li>results: 研究发现，提示基于finetuning在跨语言语理理解任务中表现出了有效性和多样性，并且在不同的几shot和全数据情况下表现出了不同的表现特征。<details>
<summary>Abstract</summary>
Multilingual pretrained language models (MPLMs) have demonstrated substantial performance improvements in zero-shot cross-lingual transfer across various natural language understanding tasks by finetuning MPLMs on task-specific labelled data of a source language (e.g. English) and evaluating on a wide range of target languages. Recent studies show that prompt-based finetuning surpasses regular finetuning in few-shot scenarios. However, the exploration of prompt-based learning in multilingual tasks remains limited. In this study, we propose the ProFiT pipeline to investigate the cross-lingual capabilities of Prompt-based Finetuning. We conduct comprehensive experiments on diverse cross-lingual language understanding tasks (sentiment classification, paraphrase identification, and natural language inference) and empirically analyze the variation trends of prompt-based finetuning performance in cross-lingual transfer across different few-shot and full-data settings. Our results reveal the effectiveness and versatility of prompt-based finetuning in cross-lingual language understanding. Our findings indicate that prompt-based finetuning outperforms vanilla finetuning in full-data scenarios and exhibits greater advantages in few-shot scenarios, with different performance patterns dependent on task types. Additionally, we analyze underlying factors such as language similarity and pretraining data size that impact the cross-lingual performance of prompt-based finetuning. Overall, our work provides valuable insights into the cross-lingual prowess of prompt-based finetuning.
</details>
<details>
<summary>摘要</summary>
多语言预训言语模型（MPLM）已经在零shot横跨不同自然语言理解任务中显示出了很大的性能提升，通过在源语言（如英语）的任务特定标注数据上精度MPLM，并在多种目标语言进行评估。据最新的研究显示，在几个shot场景中，提问基本的训练超越常训练。然而，跨语言提问基本学习的探索仍然受限。在这项研究中，我们提出了ProFiT管道，以探索跨语言理解的提问基本学习能力。我们在多种跨语言语理解任务（情感分类、重叠识别和自然语言推理）上进行了广泛的实验，并详细分析了提问基本训练在跨语言传递中的变化趋势。我们的结果表明提问基本训练在跨语言语理解中具有效果和多样性。我们发现，提问基本训练在全数据场景中比常训练高效，并在几个shot场景中表现出更大的优势。此外，我们分析了跨语言性和预训练数据大小等因素对提问基本训练的跨语言性能的影响。总之，我们的工作提供了关于提问基本训练的跨语言才能的有价值的视角。
</details></li>
</ul>
<hr>
<h2 id="CIDER-Context-sensitive-sentiment-analysis-for-short-form-text"><a href="#CIDER-Context-sensitive-sentiment-analysis-for-short-form-text" class="headerlink" title="CIDER: Context sensitive sentiment analysis for short-form text"></a>CIDER: Context sensitive sentiment analysis for short-form text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07864">http://arxiv.org/abs/2307.07864</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jcy204/ciderpolarity">https://github.com/jcy204/ciderpolarity</a></li>
<li>paper_authors: James C. Young, Rudy Arthur, Hywel T. P. Williams</li>
<li>for: This paper is written for researchers who are interested in sentiment analysis and natural language processing.</li>
<li>methods: The paper presents a new approach called CIDER (Context Informed Dictionary and sEntiment Reasoner), which performs context-sensitive sentiment analysis by inferring the valence of sentiment-laden terms from the whole corpus before scoring individual texts.</li>
<li>results: The paper demonstrates that CIDER outperforms state-of-the-art generalist sentiment analysis on a large collection of tweets about the weather.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文是为研究者们编写的，他们关注情感分析和自然语言处理领域。</li>
<li>methods: 论文介绍了一种新方法，即Context Informed Dictionary and sEntiment Reasoner（CIDER），它通过从整个 corpus 中推理情感含义权值来对个体文本进行评分。</li>
<li>results: 论文表明，CIDER 在一大量天气关注的推文上超过了状态对比的通用情感分析。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Researchers commonly perform sentiment analysis on large collections of short texts like tweets, Reddit posts or newspaper headlines that are all focused on a specific topic, theme or event. Usually, general purpose sentiment analysis methods are used which perform well on average but miss the variation in meaning that happens across different contexts, for example, the word "active" has a very different intention and valence in the phrase "active lifestyle" versus "active volcano". This work presents a new approach, CIDER (Context Informed Dictionary and sEntiment Reasoner), which performs context sensitive sentiment analysis, where the valence of sentiment laden terms is inferred from the whole corpus before being used to score the individual texts. In this paper we detail the CIDER algorithm and demonstrate that it outperforms state-of-the-art generalist sentiment analysis on a large collection of tweets about the weather. We have made our implementation of CIDER available as a python package: https://pypi.org/project/ciderpolarity/.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Transformers-are-Universal-Predictors"><a href="#Transformers-are-Universal-Predictors" class="headerlink" title="Transformers are Universal Predictors"></a>Transformers are Universal Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07843">http://arxiv.org/abs/2307.07843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danderfer/Comp_Sci_Sem_2">https://github.com/danderfer/Comp_Sci_Sem_2</a></li>
<li>paper_authors: Sourya Basu, Moulik Choraria, Lav R. Varshney</li>
<li>for: 这个论文探讨了Transformer架构在自然语言处理中的限制，以及其在信息理论上的通用预测性。</li>
<li>methods: 该论文使用了信息理论来分析Transformer架构的非尺度数据 режиmes，并对不同的组成部分进行分析，以了解它们在数据效率训练中的作用。</li>
<li>results: 实验结果 validate了论文的理论分析，并在Synthetic和实际数据集上显示了Transformer架构的通用预测性。<details>
<summary>Abstract</summary>
We find limits to the Transformer architecture for language modeling and show it has a universal prediction property in an information-theoretic sense. We further analyze performance in non-asymptotic data regimes to understand the role of various components of the Transformer architecture, especially in the context of data-efficient training. We validate our theoretical analysis with experiments on both synthetic and real datasets.
</details>
<details>
<summary>摘要</summary>
我们发现 transformer 架构在语言模型预测中有限制，并证明它具有一般预测性质在信息理论上。我们进一步分析transformer架构的性能在非 asymptotic 数据 режиме，以了解不同组件的作用，特别是在数据效率训练中。我们对both synthetic和实际数据进行实验来验证我们的理论分析。Note: "Transformer" is a specific type of neural network architecture, and "language modeling" refers to the task of predicting the next word or character in a sequence of text given the context of the previous words.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/16/cs.CL_2023_07_16/" data-id="cloimip64007hs48885t9hcwj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/72/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/71/">71</a><a class="page-number" href="/page/72/">72</a><span class="page-number current">73</span><a class="page-number" href="/page/74/">74</a><a class="page-number" href="/page/75/">75</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/74/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
