
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/55/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.SD_2023_08_14/" class="article-date">
  <time datetime="2023-08-14T15:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/cs.SD_2023_08_14/">cs.SD - 2023-08-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Integrating-Emotion-Recognition-with-Speech-Recognition-and-Speaker-Diarisation-for-Conversations"><a href="#Integrating-Emotion-Recognition-with-Speech-Recognition-and-Speaker-Diarisation-for-Conversations" class="headerlink" title="Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations"></a>Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07145">http://arxiv.org/abs/2308.07145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/w-wu/steer">https://github.com/w-wu/steer</a></li>
<li>paper_authors: Wen Wu, Chao Zhang, Philip C. Woodland</li>
<li>for: 这个论文的目的是提出一种结合自动情感识别（AER）、自动语音识别（ASR）和 speaker 分类（SD）的共同训练系统，以便在对话系统中使用。</li>
<li>methods: 这个论文使用了一种共同encoder来构建不同的输出层，用于四个子任务：AER、ASR、语音活动检测和 speaker 分类。</li>
<li>results: 在IEMOCAP数据集上进行测试，提议的系统在AER、ASR和SD三个任务上均超越了两个基准系统，并且在时间权重的情感识别和 speaker 分类错误上提供了两个metric来评估AER性能。<details>
<summary>Abstract</summary>
Although automatic emotion recognition (AER) has recently drawn significant research interest, most current AER studies use manually segmented utterances, which are usually unavailable for dialogue systems. This paper proposes integrating AER with automatic speech recognition (ASR) and speaker diarisation (SD) in a jointly-trained system. Distinct output layers are built for four sub-tasks including AER, ASR, voice activity detection and speaker classification based on a shared encoder. Taking the audio of a conversation as input, the integrated system finds all speech segments and transcribes the corresponding emotion classes, word sequences, and speaker identities. Two metrics are proposed to evaluate AER performance with automatic segmentation based on time-weighted emotion and speaker classification errors. Results on the IEMOCAP dataset show that the proposed system consistently outperforms two baselines with separately trained single-task systems on AER, ASR and SD.
</details>
<details>
<summary>摘要</summary>
尽管自动情感识别（AER）最近吸引了广泛的研究兴趣，现有大多数AER研究使用手动分割的话语，这些话语通常不可用于对话系统。这篇论文提议将AER、自动语音识别（ASR）和speaker分类（SD）集成为一个联合训练系统。根据共享encoder构建了四个不同的输出层，用于四个子任务，包括AER、ASR、语音活动检测和speaker类型分类。将对话 audio作为输入，该集成系统可以找到所有的语音段落，并将对应的情感类别、单词序列和 speaker 标识符转录出来。为评估AER性能，提出了两种指标，一是基于时间Weighted Emotion Errors，另一是基于speaker Classification Errors。对于IEMOCAP dataset，提出的系统一直在AER、ASR和SD三个基础系统之上升级，并且在AER性能和自动分割性能两个方面均有优异表现。
</details></li>
</ul>
<hr>
<h2 id="VoxBlink-X-Large-Speaker-Verification-Dataset-on-Camera"><a href="#VoxBlink-X-Large-Speaker-Verification-Dataset-on-Camera" class="headerlink" title="VoxBlink: X-Large Speaker Verification Dataset on Camera"></a>VoxBlink: X-Large Speaker Verification Dataset on Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07056">http://arxiv.org/abs/2308.07056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuke Lin, Xiaoyi Qin, Ming Cheng, Ning Jiang, Guoqing Zhao, Ming Li</li>
<li>for: 这个论文主要用于提供一个大型的说话人识别数据集，以便进行说话人识别模型的训练和研究。</li>
<li>methods: 论文使用了自动化和可扩展的数据采集管道，从 YouTube 上下载了大量短视频，并从中提取了相关的speech和视频段落。</li>
<li>results: 论文的实验结果表明，通过在不同的基础结构上训练，可以获得13%-30%的性能提升，这些基础结构包括VoxCeleb2和VoxBlink-Clean。<details>
<summary>Abstract</summary>
In this paper, we contribute a novel and extensive dataset for speaker verification, which contains noisy 38k identities/1.45M utterances (VoxBlink) and relatively cleaned 18k identities/1.02M (VoxBlink-Clean) utterances for training. Firstly, we accumulate a 60K+ users' list with their avatars and download their short videos on YouTube. We then established an automatic and scalable pipeline to extract relevant speech and video segments from these videos. To our knowledge, the VoxBlink dataset is one of the largest speaker recognition datasets available. Secondly, we conduct a series of experiments based on different backbones trained on a mix of the VoxCeleb2 and the VoxBlink-Clean. Our findings highlight a notable performance improvement, ranging from 13% to 30%, across different backbone architectures upon integrating our dataset for training. The dataset will be made publicly available shortly.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们贡献了一个新的和广泛的演说识别数据集，包括噪音38k个人/1.45万个语音（VoxBlink）和相对干净的18k个人/1.02万个语音（VoxBlink-Clean） для训练。首先，我们积累了60,000名用户的名单和他们的小视频在YouTube上，然后我们建立了一个自动和可扩展的管道来提取这些视频中的相关语音和视频段落。根据我们所知，VoxBlink数据集是目前最大的演说识别数据集之一。其次，我们进行了不同的核心结构在VoxCeleb2和VoxBlink-Clean上进行训练的一系列实验。我们的发现表明，在将我们的数据集integrated into training中，不同的核心结构在13%到30%之间具有显著的性能提升。这个数据集即将公开。
</details></li>
</ul>
<hr>
<h2 id="Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder"><a href="#Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder" class="headerlink" title="Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder"></a>Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08488">http://arxiv.org/abs/2308.08488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mispchallenge/misp-icme-avsr">https://github.com/mispchallenge/misp-icme-avsr</a></li>
<li>paper_authors: Yusheng Dai, Hang Chen, Jun Du, Xiaofei Ding, Ning Ding, Feijun Jiang, Chin-Hui Lee</li>
<li>for: 提高 audio-visual speech recognition（AVSR）系统的性能</li>
<li>methods: 提出两种新技术：利用叙述 lip shape 和 syllable-level subword unit 的相关性来设定准确的帧级句子界限，并提出一种 audio-guided cross-modal fusion encoder（CMFE）神经网络来全面利用多modal complementarity。</li>
<li>results: 在 MISP2021-AVSR 数据集上进行实验，证明了两种提posed技术的效果性。使用只有相对较少的训练数据，最终系统可以达到比现有系统更高的性能水平。<details>
<summary>Abstract</summary>
In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the MISP2021-AVSR data set show the effectiveness of the two proposed techniques. Together, using only a relatively small amount of training data, the final system achieves better performances than state-of-the-art systems with more complex front-ends and back-ends.
</details>
<details>
<summary>摘要</summary>
近期研究发现，自动语音识别系统到audio-visual语音识别系统在端到端框架下的轻微性能提升。这可能是因为modalities之间的不匹配收敛率和特殊输入表示所致。在这篇论文中，我们提出了两种新的技巧来提高audio-visual语音识别（AVSR）在预训练和精度调整训练框架下。首先，我们研究了拼音和lip shape之间的相关性，以建立良好的帧级单词界限。这使得视频和音频流在视觉模型预训练和交叉模态融合时进行准确的同步。其次，我们提出了一种听音指导的交叉模态融合encoder（CMFE）神经网络，以利用主要训练参数进行多个交叉模态注意层，以便充分利用多模态的补做性。在MISP2021-AVSR数据集上进行的实验表明，两种提出的技巧具有效果。总之，使用只有相对较少的训练数据，最终系统可以超越state-of-the-art系统，具有更复杂的前端和后端。
</details></li>
</ul>
<hr>
<h2 id="The-Sound-Demixing-Challenge-2023-unicode-x2013-Cinematic-Demixing-Track"><a href="#The-Sound-Demixing-Challenge-2023-unicode-x2013-Cinematic-Demixing-Track" class="headerlink" title="The Sound Demixing Challenge 2023 $\unicode{x2013}$ Cinematic Demixing Track"></a>The Sound Demixing Challenge 2023 $\unicode{x2013}$ Cinematic Demixing Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06981">http://arxiv.org/abs/2308.06981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Uhlich, Giorgio Fabbro, Masato Hirano, Shusuke Takahashi, Gordon Wichern, Jonathan Le Roux, Dipam Chakraborty, Sharada Mohanty, Kai Li, Yi Luo, Jianwei Yu, Rongzhi Gu, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Mikhail Sukhovei, Yuki Mitsufuji</li>
<li>for: 这篇论文总结了2023年音频分离挑战（SDX’23）的电影分离（CDX）轨迹。</li>
<li>methods: 论文详细介绍了比赛的结构和使用的数据集，尤其是新构建的CDXDB23隐藏数据集，用于评测参赛者的提交。</li>
<li>results: 论文提供了参与者采用最successful的方法的 Insights，相比cocktail-fork基线，专门在 simulated Divide and Remaster（DnR）数据集上训练的系统在SDR中提高1.8dB，而开放排名中任何数据可以用 для训练的系统在SDR中提高5.7dB。<details>
<summary>Abstract</summary>
This paper summarizes the cinematic demixing (CDX) track of the Sound Demixing Challenge 2023 (SDX'23). We provide a comprehensive summary of the challenge setup, detailing the structure of the competition and the datasets used. Especially, we detail CDXDB23, a new hidden dataset constructed from real movies that was used to rank the submissions. The paper also offers insights into the most successful approaches employed by participants. Compared to the cocktail-fork baseline, the best-performing system trained exclusively on the simulated Divide and Remaster (DnR) dataset achieved an improvement of 1.8dB in SDR whereas the top performing system on the open leaderboard, where any data could be used for training, saw a significant improvement of 5.7dB.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "cinematic demixing" (CDX) was translated as "电影分离" (diàn yǐng fēn tiē)* "Sound Demixing Challenge" (SDX) was translated as "声音分离挑战" (shēng yīn fēn tiē bàzhàng)* "CDXDB23" was translated as "CDXDB2023" (CDXDB2023)* "simulated Divide and Remaster" (DnR) dataset was translated as "模拟分离和重新制作" (mó xiǎo fēn tiē hé zhòng xiān zhì zuò) dataset* "open leaderboard" was translated as "开放排行榜" (kāifàng pǔhàng bǎng)
</details></li>
</ul>
<hr>
<h2 id="The-Sound-Demixing-Challenge-2023-unicode-x2013-Music-Demixing-Track"><a href="#The-Sound-Demixing-Challenge-2023-unicode-x2013-Music-Demixing-Track" class="headerlink" title="The Sound Demixing Challenge 2023 $\unicode{x2013}$ Music Demixing Track"></a>The Sound Demixing Challenge 2023 $\unicode{x2013}$ Music Demixing Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06979">http://arxiv.org/abs/2308.06979</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zfturbo/mvsep-mdx23-music-separation-model">https://github.com/zfturbo/mvsep-mdx23-music-separation-model</a></li>
<li>paper_authors: Giorgio Fabbro, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Martínez-Ramírez, Weihsiang Liao, Igor Gadelha, Geraldo Ramos, Eddie Hsu, Hugo Rodrigues, Fabian-Robert Stöter, Alexandre Défossez, Yi Luo, Jianwei Yu, Dipam Chakraborty, Sharada Mohanty, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Nabarun Goswami, Tatsuya Harada, Minseok Kim, Jun Hyung Lee, Yuanliang Dong, Xinran Zhang, Jiafeng Liu, Yuki Mitsufuji</li>
<li>for: 音乐分离挑战(SDX’23)的音乐分离(MSS)轨迹</li>
<li>methods: 使用了新的数据集SDXDB23_LabelNoise和SDXDB23_Bleeding1进行训练，并介绍了一种新的训练数据集错误ormalization</li>
<li>results: 在competition中，提出了最高分的方法，并与之前的音乐分离挑战(SDX’21)进行了比较，得到了1.6dB的提高 Signal-to-distortion ratio，并通过 listening test 得到了聆听评价。<details>
<summary>Abstract</summary>
This paper summarizes the music demixing (MDX) track of the Sound Demixing Challenge (SDX'23). We provide a summary of the challenge setup and introduce the task of robust music source separation (MSS), i.e., training MSS models in the presence of errors in the training data. We propose a formalization of the errors that can occur in the design of a training dataset for MSS systems and introduce two new datasets that simulate such errors: SDXDB23_LabelNoise and SDXDB23_Bleeding1. We describe the methods that achieved the highest scores in the competition. Moreover, we present a direct comparison with the previous edition of the challenge (the Music Demixing Challenge 2021): the best performing system under the standard MSS formulation achieved an improvement of over 1.6dB in signal-to-distortion ratio over the winner of the previous competition, when evaluated on MDXDB21. Besides relying on the signal-to-distortion ratio as objective metric, we also performed a listening test with renowned producers/musicians to study the perceptual quality of the systems and report here the results. Finally, we provide our insights into the organization of the competition and our prospects for future editions.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文总结了Sound Demixing Challenge（SDX'23）中的音乐分离（MDX）track。我们介绍了挑战的设置和音乐来源分离（MSS）任务，即在训练数据中存在错误时训练MSS系统。我们提出了错误的形式化，并介绍了两个新的数据集，即SDXDB23_LabelNoise和SDXDB23_Bleeding1，这些数据集模拟了这些错误。我们 Described the methods that achieved the highest scores in the competition. In addition, we present a direct comparison with the previous edition of the challenge (the Music Demixing Challenge 2021): the best performing system under the standard MSS formulation achieved an improvement of over 1.6dB in signal-to-distortion ratio over the winner of the previous competition, when evaluated on MDXDB21. Besides relying on the signal-to-distortion ratio as objective metric, we also performed a listening test with renowned producers/musicians to study the perceptual quality of the systems and report here the results. Finally, we provide our insights into the organization of the competition and our prospects for future editions.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.SD_2023_08_14/" data-id="clohum9bk00topj88f6bg80wl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.CV_2023_08_14/" class="article-date">
  <time datetime="2023-08-14T13:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/cs.CV_2023_08_14/">cs.CV - 2023-08-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DS-Depth-Dynamic-and-Static-Depth-Estimation-via-a-Fusion-Cost-Volume"><a href="#DS-Depth-Dynamic-and-Static-Depth-Estimation-via-a-Fusion-Cost-Volume" class="headerlink" title="DS-Depth: Dynamic and Static Depth Estimation via a Fusion Cost Volume"></a>DS-Depth: Dynamic and Static Depth Estimation via a Fusion Cost Volume</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07225">http://arxiv.org/abs/2308.07225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xingy038/ds-depth">https://github.com/xingy038/ds-depth</a></li>
<li>paper_authors: Xingyu Miao, Yang Bai, Haoran Duan, Yawen Huang, Fan Wan, Xinxing Xu, Yang Long, Yefeng Zheng</li>
<li>for: 提高自适应单目深度估计的精度，解决动态对象景象中的错觉和干扰问题</li>
<li>methods: 使用差异投影错误来捕捉静止环境中的几何关系，并通过增强遗憾投影来改进错觉和干扰问题</li>
<li>results: 对比前一些基eline，模型在KITTI和Cityscapes datasets上表现出较高的精度和稳定性，并且能够更好地处理动态对象景象中的错觉和干扰问题<details>
<summary>Abstract</summary>
Self-supervised monocular depth estimation methods typically rely on the reprojection error to capture geometric relationships between successive frames in static environments. However, this assumption does not hold in dynamic objects in scenarios, leading to errors during the view synthesis stage, such as feature mismatch and occlusion, which can significantly reduce the accuracy of the generated depth maps. To address this problem, we propose a novel dynamic cost volume that exploits residual optical flow to describe moving objects, improving incorrectly occluded regions in static cost volumes used in previous work. Nevertheless, the dynamic cost volume inevitably generates extra occlusions and noise, thus we alleviate this by designing a fusion module that makes static and dynamic cost volumes compensate for each other. In other words, occlusion from the static volume is refined by the dynamic volume, and incorrect information from the dynamic volume is eliminated by the static volume. Furthermore, we propose a pyramid distillation loss to reduce photometric error inaccuracy at low resolutions and an adaptive photometric error loss to alleviate the flow direction of the large gradient in the occlusion regions. We conducted extensive experiments on the KITTI and Cityscapes datasets, and the results demonstrate that our model outperforms previously published baselines for self-supervised monocular depth estimation.
</details>
<details>
<summary>摘要</summary>
自我监睹的单目深度估计方法通常基于 reprojection 错误来捕捉静止环境中的几何关系。然而，这个假设不适用于动态对象场景中，导致视觉合成阶段的错误，如特征匹配和遮挡，可以 significatively 降低生成的深度图的准确性。为解决这个问题，我们提出了一种新的动态成本Volume，利用剩余运动流来描述移动对象，提高静止成本Volume中的错误区域。然而，动态成本Volume会生成额外的遮挡和噪声，因此我们采用一种融合模块，使静止和动态成本Volume相互补做。即静止成本Volume中的遮挡被动态成本Volume修正，而动态成本Volume中的错误信息被静止成本Volume消除。此外，我们提出了一种 pyramid 润照损失来降低低分辨率下的光метри错误偏差，以及一种适应性 photometric error 损失来减轻遮挡区域中的流向大Gradient。我们在 KITTI 和 Cityscapes 数据集上进行了广泛的实验，结果表明我们的模型在自我监睹单目深度估计方法中升级 previously 发表的基elines。
</details></li>
</ul>
<hr>
<h2 id="Distance-Matters-For-Improving-Performance-Estimation-Under-Covariate-Shift"><a href="#Distance-Matters-For-Improving-Performance-Estimation-Under-Covariate-Shift" class="headerlink" title="Distance Matters For Improving Performance Estimation Under Covariate Shift"></a>Distance Matters For Improving Performance Estimation Under Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07223">http://arxiv.org/abs/2308.07223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/melanibe/distance_matters_performance_estimation">https://github.com/melanibe/distance_matters_performance_estimation</a></li>
<li>paper_authors: Mélanie Roschewitz, Ben Glocker</li>
<li>for: 这篇论文的目的是提出一种新的性能估计方法，以便在 covariate shift 的情况下安全部署 AI 模型，特别是在敏感的应用场景中。</li>
<li>methods: 本文使用了许多已知的方法，包括模型预测和 softmax 信任度来 derive accuracy estimates。然而，在 dataset shift 的情况下，信任度可能会变得不准确，因为测试样本可能远离训练分布。本文提出了一种 “distance-check” 来检查测试样本是否位于预期的训练分布中，以避免将不可靠的模型输出用于精度估计。</li>
<li>results: 本文在 13 个图像分类任务上进行了实验，范围包括自然和 sintetic distribution shift，以及多种模型。结果显示，distance-check 方法可以对性能估计进行有效的改善，具体来说是使用 median relative MAE improvement 来衡量改善的程度。在所有任务上，distance-check 方法可以获得 SOTA 性能，并且在 10 个任务上获得了最佳 baseline。相关的代码可以在 GitHub 上找到。<details>
<summary>Abstract</summary>
Performance estimation under covariate shift is a crucial component of safe AI model deployment, especially for sensitive use-cases. Recently, several solutions were proposed to tackle this problem, most leveraging model predictions or softmax confidence to derive accuracy estimates. However, under dataset shifts, confidence scores may become ill-calibrated if samples are too far from the training distribution. In this work, we show that taking into account distances of test samples to their expected training distribution can significantly improve performance estimation under covariate shift. Precisely, we introduce a "distance-check" to flag samples that lie too far from the expected distribution, to avoid relying on their untrustworthy model outputs in the accuracy estimation step. We demonstrate the effectiveness of this method on 13 image classification tasks, across a wide-range of natural and synthetic distribution shifts and hundreds of models, with a median relative MAE improvement of 27% over the best baseline across all tasks, and SOTA performance on 10 out of 13 tasks. Our code is publicly available at https://github.com/melanibe/distance_matters_performance_estimation.
</details>
<details>
<summary>摘要</summary>
“性能估计下covariate shift是AI模型部署中的一个重要组成部分，特别是在敏感用途中。近年，一些解决方案被提出来解决这个问题，大多数是基于模型预测或softmax信任度来 derive accuracy estimates。但是，在数据shift中，信任度可能会变得不准确，如果测试样本太过远 FROM the training distribution。在这个工作中，我们显示了将测试样本与预期的training distribution之间的距离考虑到可以对性能估计下covariate shift做出重要改进。具体来说，我们引入了一个“distance-check”来检查测试样本是否位于预期的training distribution中，以避免依靠它们的不可靠的模型输出在准确性估计阶段中。我们在13个图像分类任务上证明了这个方法的有效性，包括各种自然和 sintetic distribution shift，以及数百个模型， WITH a median relative MAE improvement of 27% over the best baseline across all tasks, AND SOTA performance on 10 out of 13 tasks。我们的代码可以在https://github.com/melanibe/distance_matters_performance_estimation中找到。”
</details></li>
</ul>
<hr>
<h2 id="Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data"><a href="#Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data" class="headerlink" title="Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data"></a>Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07214">http://arxiv.org/abs/2308.07214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiranjeewee Prasad Koirala, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</li>
<li>for: 这 paper 探讨了对多modalità MRI 数据的深度学习应用，以提高脑肿瘤 segmentation 精度在南部非洲患者人口中。</li>
<li>methods: 这 paper 介绍了一种 ensemble 方法，包括 eleven 种不同的变体，基于三种核心架构：UNet3D、ONet3D 和 SphereNet3D，以及修改的损失函数。</li>
<li>results: 研究发现， ensemble 方法，结合不同的架构，可以提高评估指标。特别是，结果表明 ensemble 方法可以达到 Dice 分数为 0.82、0.82 和 0.87 的优秀表现，用于提高脑肿瘤的 segmentation。<details>
<summary>Abstract</summary>
Brain tumors, particularly glioblastoma, continue to challenge medical diagnostics and treatments globally. This paper explores the application of deep learning to multi-modality magnetic resonance imaging (MRI) data for enhanced brain tumor segmentation precision in the Sub-Saharan Africa patient population. We introduce an ensemble method that comprises eleven unique variations based on three core architectures: UNet3D, ONet3D, SphereNet3D and modified loss functions. The study emphasizes the need for both age- and population-based segmentation models, to fully account for the complexities in the brain. Our findings reveal that the ensemble approach, combining different architectures, outperforms single models, leading to improved evaluation metrics. Specifically, the results exhibit Dice scores of 0.82, 0.82, and 0.87 for enhancing tumor, tumor core, and whole tumor labels respectively. These results underline the potential of tailored deep learning techniques in precisely segmenting brain tumors and lay groundwork for future work to fine-tune models and assess performance across different brain regions.
</details>
<details>
<summary>摘要</summary>
脑肿，特别是 glioblastoma，仍然在全球医疗领域存在挑战。这篇论文探讨了对多Modal magnetic resonance imaging（MRI）数据的深度学习应用，以提高脑肿分 segmentation精度在非洲南部地区患者人群中。我们提出了一种ensemble方法，包括eleven个uniqu variation，基于三种核心体系：UNet3D、ONet3D和SphereNet3D，以及修改的损失函数。这篇研究强调了需要基于年龄和人口的分 segmentation模型，以全面考虑脑肿的复杂性。我们的发现表明， ensemble方法，组合不同的体系，在评估指标上超越单个模型，导致改进的评估结果。具体来说，结果显示 dice分数为0.82、0.82和0.87，用于提高肿体、肿核和全肿标签。这些结果表明了深度学习技术的可能性，在精度地分 segmentation脑肿，并为未来细化模型和评估不同脑区的表现奠定基础。
</details></li>
</ul>
<hr>
<h2 id="Automated-Ensemble-Based-Segmentation-of-Pediatric-Brain-Tumors-A-Novel-Approach-Using-the-CBTN-CONNECT-ASNR-MICCAI-BraTS-PEDs-2023-Challenge-Data"><a href="#Automated-Ensemble-Based-Segmentation-of-Pediatric-Brain-Tumors-A-Novel-Approach-Using-the-CBTN-CONNECT-ASNR-MICCAI-BraTS-PEDs-2023-Challenge-Data" class="headerlink" title="Automated Ensemble-Based Segmentation of Pediatric Brain Tumors: A Novel Approach Using the CBTN-CONNECT-ASNR-MICCAI BraTS-PEDs 2023 Challenge Data"></a>Automated Ensemble-Based Segmentation of Pediatric Brain Tumors: A Novel Approach Using the CBTN-CONNECT-ASNR-MICCAI BraTS-PEDs 2023 Challenge Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07212">http://arxiv.org/abs/2308.07212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashidhar Reddy Javaji, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</li>
<li>for: 这个研究旨在提高脑肿瘤诊断技术和治疗方法，特别是为pediatric patients提供年龄特定的分 segmentation模型。</li>
<li>methods: 这个研究使用深度学习技术，使用MRI模式进行数据采集和分 segmentation。研究提出了一种新的ensemble方法，组合ONet和修改后的UNet模型，并使用创新的损失函数。</li>
<li>results: 研究实现了精度的分 segmentation模型， lesion_wise dice scores为0.52、0.72和0.78，表明ensemble方法在不同的扫描协议下的Robustness和准确性。视觉比较也证明了 ensemble方法在脑肿瘤区域覆盖方面的superiority。<details>
<summary>Abstract</summary>
Brain tumors remain a critical global health challenge, necessitating advancements in diagnostic techniques and treatment methodologies. In response to the growing need for age-specific segmentation models, particularly for pediatric patients, this study explores the deployment of deep learning techniques using magnetic resonance imaging (MRI) modalities. By introducing a novel ensemble approach using ONet and modified versions of UNet, coupled with innovative loss functions, this study achieves a precise segmentation model for the BraTS-PEDs 2023 Challenge. Data augmentation, including both single and composite transformations, ensures model robustness and accuracy across different scanning protocols. The ensemble strategy, integrating the ONet and UNet models, shows greater effectiveness in capturing specific features and modeling diverse aspects of the MRI images which result in lesion_wise dice scores of 0.52, 0.72 and 0.78 for enhancing tumor, tumor core and whole tumor labels respectively. Visual comparisons further confirm the superiority of the ensemble method in accurate tumor region coverage. The results indicate that this advanced ensemble approach, building upon the unique strengths of individual models, offers promising prospects for enhanced diagnostic accuracy and effective treatment planning for brain tumors in pediatric brains.
</details>
<details>
<summary>摘要</summary>
脑肿症仍然是全球医疗挑战，需要不断发展诊断技术和治疗方法。为了应对儿童患者的年龄特定分 segmentation模型的增长需求，这种研究使用深度学习技术，利用 магни resonance imaging（MRI）Modalities进行分 segmentation。通过引入一种新的集成方法，使用 ONet 和修改后的 UNet，并采用创新的损失函数，这种研究实现了高精度的分 segmentation模型，用于 BraTS-PEDs 2023 挑战。数据增强，包括单个和复合变换，确保模型的稳定性和准确性在不同的扫描协议下。集成策略，将 ONet 和 UNet 模型集成起来，显示更高的效iveness，可以吸收特定的特征和多样化的 MRI 图像特征，导致 lesion_wise dice 分数为 0.52、0.72 和 0.78，用于增强肿瘤、肿瘤核和整个肿瘤标签。视觉比较也证明了集成策略的超越性，在准确地覆盖肿瘤区域方面。结果表明，这种高级集成策略，利用个体模型的独特优势，为脑肿症的诊断精度和有效的治疗规划提供了有希望的前景。
</details></li>
</ul>
<hr>
<h2 id="Unified-Data-Free-Compression-Pruning-and-Quantization-without-Fine-Tuning"><a href="#Unified-Data-Free-Compression-Pruning-and-Quantization-without-Fine-Tuning" class="headerlink" title="Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning"></a>Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07209">http://arxiv.org/abs/2308.07209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shipeng Bai, Jun Chen, Xintian Shen, Yixuan Qian, Yong Liu</li>
<li>for: 提高神经网络的推理时间和内存占用，并且能够处理敏感或商业秘密的数据。</li>
<li>methods: 提出了一种新的框架，即统一数据 свобод压缩（UDFC），它不需要原始训练集进行 fine-tuning，可以同时进行压缩和量化，从而提高神经网络的压缩率和量化精度。</li>
<li>results: 在大规模图像分类任务中，UDFC 实现了 significan 的提高，比如在 ImageNet 数据集上，与 SOTA 方法相比，UDFC 实现了20.54% 的准确率提高，并且可以在不同的网络架构和压缩方法上实现显著的改进。<details>
<summary>Abstract</summary>
Structured pruning and quantization are promising approaches for reducing the inference time and memory footprint of neural networks. However, most existing methods require the original training dataset to fine-tune the model. This not only brings heavy resource consumption but also is not possible for applications with sensitive or proprietary data due to privacy and security concerns. Therefore, a few data-free methods are proposed to address this problem, but they perform data-free pruning and quantization separately, which does not explore the complementarity of pruning and quantization. In this paper, we propose a novel framework named Unified Data-Free Compression(UDFC), which performs pruning and quantization simultaneously without any data and fine-tuning process. Specifically, UDFC starts with the assumption that the partial information of a damaged(e.g., pruned or quantized) channel can be preserved by a linear combination of other channels, and then derives the reconstruction form from the assumption to restore the information loss due to compression. Finally, we formulate the reconstruction error between the original network and its compressed network, and theoretically deduce the closed-form solution. We evaluate the UDFC on the large-scale image classification task and obtain significant improvements over various network architectures and compression methods. For example, we achieve a 20.54% accuracy improvement on ImageNet dataset compared to SOTA method with 30% pruning ratio and 6-bit quantization on ResNet-34.
</details>
<details>
<summary>摘要</summary>
Structured pruning和量化是降低神经网络执行时间和内存占用的有望方法。然而，大多数现有方法需要原始训练集来细化模型，这不仅带来重量的资源占用，还不可能在敏感或商业秘密数据的应用中进行，因为隐私和安全问题。因此，一些不需要数据的方法被提议，但它们分别进行无数据的采样和量化，而不是探索归一化的逻辑。在这篇论文中，我们提出了一种名为统一无数据压缩（UDFC）的新框架，它在无数据和不需要细化过程下同时进行压缩和量化。具体来说，UDFC从假设部分损坏的通道可以通过其他通道的线性组合来保留一些信息来，然后 derive 从假设来恢复因压缩而产生的信息损失。最后，我们将原始网络和压缩后的网络之间的重建误差计算出来，并理论上得出关闭式解决方案。我们在大规模的图像分类任务上评估了UDFC，并得到了显著的改进。例如，在ImageNet数据集上，我们在ResNet-34架构上实现了30%的采样和6位数字化后的20.54%的准确率提升，比SOTA方法更高。
</details></li>
</ul>
<hr>
<h2 id="FOLT-Fast-Multiple-Object-Tracking-from-UAV-captured-Videos-Based-on-Optical-Flow"><a href="#FOLT-Fast-Multiple-Object-Tracking-from-UAV-captured-Videos-Based-on-Optical-Flow" class="headerlink" title="FOLT: Fast Multiple Object Tracking from UAV-captured Videos Based on Optical Flow"></a>FOLT: Fast Multiple Object Tracking from UAV-captured Videos Based on Optical Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07207">http://arxiv.org/abs/2308.07207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mufeng Yao, Jiaqi Wang, Jinlong Peng, Mingmin Chi, Chao Liu</li>
<li>for: 本研究旨在解决小型物体跟踪在无人机视频中的挑战，即小物体size、模糊的物体表现和无人机平台上的大小和不规则运动。</li>
<li>methods: 我们提出了FOLT方法，它采用现代检测器和轻量级光流提取器来提取物体检测特征和运动特征，并通过流动导向的特征增强和运动预测来提高小物体检测和跟踪性能。</li>
<li>results: 实验结果表明，我们的提出的模型在Visdrone和UAVDT数据集上可以成功跟踪小物体，并在无人机-MOT任务中超越现有状态的方法。<details>
<summary>Abstract</summary>
Multiple object tracking (MOT) has been successfully investigated in computer vision.   However, MOT for the videos captured by unmanned aerial vehicles (UAV) is still challenging due to small object size, blurred object appearance, and very large and/or irregular motion in both ground objects and UAV platforms.   In this paper, we propose FOLT to mitigate these problems and reach fast and accurate MOT in UAV view.   Aiming at speed-accuracy trade-off, FOLT adopts a modern detector and light-weight optical flow extractor to extract object detection features and motion features at a minimum cost.   Given the extracted flow, the flow-guided feature augmentation is designed to augment the object detection feature based on its optical flow, which improves the detection of small objects.   Then the flow-guided motion prediction is also proposed to predict the object's position in the next frame, which improves the tracking performance of objects with very large displacements between adjacent frames.   Finally, the tracker matches the detected objects and predicted objects using a spatially matching scheme to generate tracks for every object.   Experiments on Visdrone and UAVDT datasets show that our proposed model can successfully track small objects with large and irregular motion and outperform existing state-of-the-art methods in UAV-MOT tasks.
</details>
<details>
<summary>摘要</summary>
多bject tracking (MOT) 在计算机视觉领域已经得到了成功的探索。然而，UAV拍摄视频中的 MOT 仍然是一个挑战，因为物体的小size、模糊的表现和 UAV 平台上的大型和/或不规则运动。在这篇论文中，我们提出了FOLT来解决这些问题，以达到快速精度的 MOT 在 UAV 视野中。以速度精度负担为目标，FOLT 采用了现代探测器和轻量级光流提取器来提取物体检测特征和运动特征，以最小化成本。根据提取的流动，我们提出了流动导向的特征增强技术，以提高小物体的检测。然后，我们还提出了流动导向的运动预测技术，以预测物体在下一帧的位置，提高对大幅运动的物体的跟踪性。最后，我们使用空间匹配算法匹配检测到的物体和预测的物体，以生成每个物体的轨迹。实验结果表明，我们提出的模型可以成功跟踪 UAV 拍摄视频中的小物体，并且在 UAV-MOT 任务中超过现有状态的方法。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-Real-Time-Scene-Text-Detection-From-Semantic-to-Instance-Representation-Learning"><a href="#Towards-Robust-Real-Time-Scene-Text-Detection-From-Semantic-to-Instance-Representation-Learning" class="headerlink" title="Towards Robust Real-Time Scene Text Detection: From Semantic to Instance Representation Learning"></a>Towards Robust Real-Time Scene Text Detection: From Semantic to Instance Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07202">http://arxiv.org/abs/2308.07202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xugong Qin, Pengyuan Lyu, Chengquan Zhang, Yu Zhou, Kun Yao, Peng Zhang, Hailun Lin, Weiping Wang</li>
<li>for: 本研究旨在提高现场文本检测的准确率和速度，提出了一种基于表示学习的底层 segmentation-based 方法。</li>
<li>methods: 方法包括global-dense semantic contrast (GDSC)和top-down modeling (TDM)，它们帮助encoder网络学习更强的表示，而不需要在推理过程中添加参数和计算。</li>
<li>results: 实验结果表明，提出的方法可以在四个公共数据集上达到或超过现状之准确率和速度，具体来说是在Total-Text 上获得87.2% F-度量值，并在MSRA-TD500上获得89.6% F-度量值，这些结果都是在单个 GeForce RTX 2080 Ti GPU 上实现的。<details>
<summary>Abstract</summary>
Due to the flexible representation of arbitrary-shaped scene text and simple pipeline, bottom-up segmentation-based methods begin to be mainstream in real-time scene text detection. Despite great progress, these methods show deficiencies in robustness and still suffer from false positives and instance adhesion. Different from existing methods which integrate multiple-granularity features or multiple outputs, we resort to the perspective of representation learning in which auxiliary tasks are utilized to enable the encoder to jointly learn robust features with the main task of per-pixel classification during optimization. For semantic representation learning, we propose global-dense semantic contrast (GDSC), in which a vector is extracted for global semantic representation, then used to perform element-wise contrast with the dense grid features. To learn instance-aware representation, we propose to combine top-down modeling (TDM) with the bottom-up framework to provide implicit instance-level clues for the encoder. With the proposed GDSC and TDM, the encoder network learns stronger representation without introducing any parameters and computations during inference. Equipped with a very light decoder, the detector can achieve more robust real-time scene text detection. Experimental results on four public datasets show that the proposed method can outperform or be comparable to the state-of-the-art on both accuracy and speed. Specifically, the proposed method achieves 87.2% F-measure with 48.2 FPS on Total-Text and 89.6% F-measure with 36.9 FPS on MSRA-TD500 on a single GeForce RTX 2080 Ti GPU.
</details>
<details>
<summary>摘要</summary>
For semantic representation learning, we propose global-dense semantic contrast (GDSC), which extracts a vector for global semantic representation and then performs element-wise contrast with the dense grid features. To learn instance-aware representation, we propose combining top-down modeling (TDM) with the bottom-up framework to provide implicit instance-level clues for the encoder. With the proposed GDSC and TDM, the encoder network learns stronger representation without introducing any additional parameters and computations during inference.Equipped with a very light decoder, the detector can achieve more robust real-time scene text detection. Experimental results on four public datasets show that the proposed method can outperform or be comparable to the state-of-the-art on both accuracy and speed. Specifically, the proposed method achieves 87.2% F-measure with 48.2 FPS on Total-Text and 89.6% F-measure with 36.9 FPS on MSRA-TD500 on a single GeForce RTX 2080 Ti GPU.
</details></li>
</ul>
<hr>
<h2 id="SEMI-CenterNet-A-Machine-Learning-Facilitated-Approach-for-Semiconductor-Defect-Inspection"><a href="#SEMI-CenterNet-A-Machine-Learning-Facilitated-Approach-for-Semiconductor-Defect-Inspection" class="headerlink" title="SEMI-CenterNet: A Machine Learning Facilitated Approach for Semiconductor Defect Inspection"></a>SEMI-CenterNet: A Machine Learning Facilitated Approach for Semiconductor Defect Inspection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07180">http://arxiv.org/abs/2308.07180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vic De Ridder, Bappaditya Dey, Enrique Dehaerne, Sandip Halder, Stefan De Gendt, Bartel Van Waeyenberge</li>
<li>for: 本研究旨在提出一种自动化的深度学习（深度学习）基本实现方法，以提高半导体缺陷检测中的精度和效率。</li>
<li>methods: 我们提出了一种自定义的中心点网络（CN）架构，并在半导体缺陷图像中训练该架构。该方法只预测潜在缺陷中心点，从而提高计算效率。</li>
<li>results: 我们在两个数据集上训练了两个ResNet背景模型，并对其进行了比较。结果显示，使用我们的SEMI-CN方法可以大幅提高检测速度，并且在训练时间和精度之间取得了良好的平衡。<details>
<summary>Abstract</summary>
Continual shrinking of pattern dimensions in the semiconductor domain is making it increasingly difficult to inspect defects due to factors such as the presence of stochastic noise and the dynamic behavior of defect patterns and types. Conventional rule-based methods and non-parametric supervised machine learning algorithms like KNN mostly fail at the requirements of semiconductor defect inspection at these advanced nodes. Deep Learning (DL)-based methods have gained popularity in the semiconductor defect inspection domain because they have been proven robust towards these challenging scenarios. In this research work, we have presented an automated DL-based approach for efficient localization and classification of defects in SEM images. We have proposed SEMI-CenterNet (SEMI-CN), a customized CN architecture trained on SEM images of semiconductor wafer defects. The use of the proposed CN approach allows improved computational efficiency compared to previously studied DL models. SEMI-CN gets trained to output the center, class, size, and offset of a defect instance. This is different from the approach of most object detection models that use anchors for bounding box prediction. Previous methods predict redundant bounding boxes, most of which are discarded in postprocessing. CN mitigates this by only predicting boxes for likely defect center points. We train SEMI-CN on two datasets and benchmark two ResNet backbones for the framework. Initially, ResNet models pretrained on the COCO dataset undergo training using two datasets separately. Primarily, SEMI-CN shows significant improvement in inference time against previous research works. Finally, transfer learning (using weights of custom SEM dataset) is applied from ADI dataset to AEI dataset and vice-versa, which reduces the required training time for both backbones to reach the best mAP against conventional training method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified ChineseSemiconductor 领域中元件缩小的趋势使得缺陷检测变得越来越困难，主要因为存在Stochastic noise和缺陷模式和类型的动态行为。传统的规则基本方法和非 Parametric 超vised 机器学习算法如 KNN 等方法在这些高级节点上很难以满足半导体缺陷检测的要求。深度学习（DL）基本方法在半导体缺陷检测领域中具有耐用性，因此在这种研究中，我们提出了一种自动化的 DL 基本方法，用于高效地local化和类型化半导体缺陷图像中的缺陷。我们提出了一种自定义的 CN 架构，用于训练在半导体晶圆缺陷图像上。与传统的 DL 模型不同，我们的 CN 方法可以更好地提高计算效率。SEMI-CN 通过输出缺陷实例的中心、类型、大小和偏移量来进行定位和分类。与大多数对象检测模型不同，我们不使用锚点来预测缺陷 bounding box。之前的方法通常会预测多个 redundancy 的缺陷 bounding box，大多数这些 bounding box 在后处理中被抛弃。CN 可以避免这种情况，只预测可能的缺陷中心点。我们在两个 dataset 上训练 SEMI-CN，并对两个 ResNet 背景进行了对比。首先，ResNet 模型在 COCO  dataset 上进行了预训练，然后在两个 dataset 上进行了分别训练。在初始化时，SEMI-CN 显示了明显的计算效率提高，相比之前的研究成果。最后，我们将 ADI dataset 和 AEI  dataset 中的权重进行了转移学习，从而减少了训练时间以达到最佳 mAP。
</details></li>
</ul>
<hr>
<h2 id="HyperSparse-Neural-Networks-Shifting-Exploration-to-Exploitation-through-Adaptive-Regularization"><a href="#HyperSparse-Neural-Networks-Shifting-Exploration-to-Exploitation-through-Adaptive-Regularization" class="headerlink" title="HyperSparse Neural Networks: Shifting Exploration to Exploitation through Adaptive Regularization"></a>HyperSparse Neural Networks: Shifting Exploration to Exploitation through Adaptive Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07163">http://arxiv.org/abs/2308.07163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/greenautoml4fas/hypersparse">https://github.com/greenautoml4fas/hypersparse</a></li>
<li>paper_authors: Patrick Glandorf, Timo Kaiser, Bodo Rosenhahn</li>
<li>for: 提出了一种新的强大的稀疏学习方法，即适应规范训练（ART），用于压缩稠密的网络。</li>
<li>methods: 相比于通常使用二进制面板进行训练来减少模型权重数量，我们在迭代 manner中增加权重规范，使权重逐渐逼近零。我们的方法将预训练模型知识压缩到最高权重中。</li>
<li>results: 我们的方法在CIFAR和TinyImageNet上进行了广泛的实验，并显示了与其他缩短方法相比，特别是在极高缩短度（99.8%）下表现出了显著的性能提升。此外，我们还对权重中高度强度的编码 Pattern进行了新的调查。<details>
<summary>Abstract</summary>
Sparse neural networks are a key factor in developing resource-efficient machine learning applications. We propose the novel and powerful sparse learning method Adaptive Regularized Training (ART) to compress dense into sparse networks. Instead of the commonly used binary mask during training to reduce the number of model weights, we inherently shrink weights close to zero in an iterative manner with increasing weight regularization. Our method compresses the pre-trained model knowledge into the weights of highest magnitude. Therefore, we introduce a novel regularization loss named HyperSparse that exploits the highest weights while conserving the ability of weight exploration. Extensive experiments on CIFAR and TinyImageNet show that our method leads to notable performance gains compared to other sparsification methods, especially in extremely high sparsity regimes up to 99.8 percent model sparsity. Additional investigations provide new insights into the patterns that are encoded in weights with high magnitudes.
</details>
<details>
<summary>摘要</summary>
稀疏神经网络是开发资源有效的机器学习应用的关键因素。我们提出了新的有力的稀疏学习方法 Adaptive Regularized Training（ART），用于压缩稀疏网络。而不是通常使用训练时的 binary mask 来减少模型权重的数量，我们在迭代方式下增加权重规化，使权重逐渐接近零。我们的方法将预训练知识压缩到最高权重中。因此，我们引入了一种新的规化损失函数名为 HyperSparse，利用最高权重而忽略权重探索的能力。我们在 CIFAR 和 TinyImageNet 上进行了广泛的实验，发现我们的方法在极高稀疏度范围内（达到 99.8% 模型稀疏度）表现出了显著的性能提升，特别是与其他稀疏化方法相比。此外，我们还进行了新的探索，发现权重高度具有潜在的编码特征。
</details></li>
</ul>
<hr>
<h2 id="SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation"><a href="#SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation" class="headerlink" title="SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation"></a>SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07156">http://arxiv.org/abs/2308.07156</a></li>
<li>repo_url: None</li>
<li>paper_authors: An Wang, Mobarakol Islam, Mengya Xu, Yang Zhang, Hongliang Ren<br>for:* 这个论文主要研究的是Semantic Segmentation of Robotic Surgery Instruments，即使用Segment Anything Model（SAM）在外科手术中的应用。methods:* 这个论文使用了SAM模型，并对其进行了多种场景和环境的测试和评估，包括Prompted和Unprompted的情况，以及不同的损害和扰动等级。results:* SAM在Boundary Box Prompt的情况下显示出了 Zero-shot 通用性，但在Point-based Prompt和Unprompted情况下，其 segmentation效果不佳，尤其是在复杂的外科手术场景中，如血液、反射、模糊和阴影等情况下。此外，SAM在不同级别的数据损害下也不具备 suficient 的Robustness。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) serves as a fundamental model for semantic segmentation and demonstrates remarkable generalization capabilities across a wide range of downstream scenarios. In this empirical study, we examine SAM's robustness and zero-shot generalizability in the field of robotic surgery. We comprehensively explore different scenarios, including prompted and unprompted situations, bounding box and points-based prompt approaches, as well as the ability to generalize under corruptions and perturbations at five severity levels. Additionally, we compare the performance of SAM with state-of-the-art supervised models. We conduct all the experiments with two well-known robotic instrument segmentation datasets from MICCAI EndoVis 2017 and 2018 challenges. Our extensive evaluation results reveal that although SAM shows remarkable zero-shot generalization ability with bounding box prompts, it struggles to segment the whole instrument with point-based prompts and unprompted settings. Furthermore, our qualitative figures demonstrate that the model either failed to predict certain parts of the instrument mask (e.g., jaws, wrist) or predicted parts of the instrument as wrong classes in the scenario of overlapping instruments within the same bounding box or with the point-based prompt. In fact, SAM struggles to identify instruments in complex surgical scenarios characterized by the presence of blood, reflection, blur, and shade. Additionally, SAM is insufficiently robust to maintain high performance when subjected to various forms of data corruption. We also attempt to fine-tune SAM using Low-rank Adaptation (LoRA) and propose SurgicalSAM, which shows the capability in class-wise mask prediction without prompt. Therefore, we can argue that, without further domain-specific fine-tuning, SAM is not ready for downstream surgical tasks.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 是一个基本模型 для semantic segmentation，它在多种下游enario中示出了惊人的总体化能力。在这个实验研究中，我们检查了SAM的Robustness和零Instance化能力在 робоcot surgery 领域。我们完整地探索了不同的enario，包括提示和无提示的情况，以及 bounding box 和点based prompt 的应用。此外，我们还评估了SAM与state-of-the-art 监督模型的比较。我们在两个well-known robotic instrument segmentation dataset 上进行了所有实验，这些dataset 来自 MICCAI EndoVis 2017 和 2018 挑战。我们的广泛评估结果显示，although SAM 在 bounding box 提示下示出了惊人的零Instance化能力，但在 point-based prompt 和无提示情况下，SAM 对于全部工具的分类还是困难。此外，我们的质量图示表明，SAM 在复杂的手术场景中，例如血液、镜面、模糊和阴影的存在下，还是很困难实现高性能。此外，SAM 在不同的数据承载变化下也不足以保持高性能。为了解决这个问题，我们尝试使用 Low-rank Adaptation (LoRA) 进行 fine-tuning，并提出了 SurgicalSAM，它可以在无提示情况下进行分类mask prediction。因此，我们可以 argument � SAM 在下游手术任务中不够充分适用，需要进一步的领域特定 fine-tuning。
</details></li>
</ul>
<hr>
<h2 id="DELO-Deep-Evidential-LiDAR-Odometry-using-Partial-Optimal-Transport"><a href="#DELO-Deep-Evidential-LiDAR-Odometry-using-Partial-Optimal-Transport" class="headerlink" title="DELO: Deep Evidential LiDAR Odometry using Partial Optimal Transport"></a>DELO: Deep Evidential LiDAR Odometry using Partial Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07153">http://arxiv.org/abs/2308.07153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sk Aziz Ali, Djamila Aouada, Gerd Reis, Didier Stricker</li>
<li>for: 这个论文是为了提供一个精确、可靠、实时的 LiDAR 基于 odometry（LO）方法，用于机器人导航、全球一致的 3D 场景重建或安全的动作规划等应用。</li>
<li>methods: 这个方法使用了深度学习的方法，将精确的几何变换组合成一个实时（约 35-40ms 每帧）的 LO 方法，并且同时学习出精确的几何变换和预测不确定性（PU）作为证据，以确保 LO 预测的正确性。</li>
<li>results: 这个方法在 KITTI 数据集上进行评估，与最近的州际顶对比方法相比， exhibits 竞争性的性能，甚至超越了其他方法的一般化能力。<details>
<summary>Abstract</summary>
Accurate, robust, and real-time LiDAR-based odometry (LO) is imperative for many applications like robot navigation, globally consistent 3D scene map reconstruction, or safe motion-planning. Though LiDAR sensor is known for its precise range measurement, the non-uniform and uncertain point sampling density induce structural inconsistencies. Hence, existing supervised and unsupervised point set registration methods fail to establish one-to-one matching correspondences between LiDAR frames. We introduce a novel deep learning-based real-time (approx. 35-40ms per frame) LO method that jointly learns accurate frame-to-frame correspondences and model's predictive uncertainty (PU) as evidence to safe-guard LO predictions. In this work, we propose (i) partial optimal transportation of LiDAR feature descriptor for robust LO estimation, (ii) joint learning of predictive uncertainty while learning odometry over driving sequences, and (iii) demonstrate how PU can serve as evidence for necessary pose-graph optimization when LO network is either under or over confident. We evaluate our method on KITTI dataset and show competitive performance, even superior generalization ability over recent state-of-the-art approaches. Source codes are available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>精准、可靠、实时的LiDAR基于滤波器（LO）是许多应用程序的关键，如机器人导航、全球一致的3D场景重建、安全的运动规划。虽然LiDAR传感器知道的精准范围测量，但非均匀和不确定的点抽样密度引起结构不一致。因此，现有的超级vised和无级视点注册方法无法建立一对一匹配关系 между LiDAR帧。我们介绍了一种新的深度学习基于实时（约35-40ms每帧）LO方法，该方法同时学习准确的帧到帧匹配和预测uncertainty（PU）作为证据，以保障LO预测。在这种工作中，我们提出了（i）LiDAR特征描述符的 partial optimal transportation 以实现Robust LO估计，（ii）在驾驶序列上同时学习预测uncertainty和LO，以及（iii）示出PU可以作为证据来优化pose-graph估计，当LO网络是 either under 或 over confident 时。我们在KITTI数据集上评估了我们的方法，并表现出competitive performance，甚至超越了最近的状态 искусственный智能方法。源代码可以获得。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Based-Augmentation-for-Captioning-and-Retrieval-in-Cultural-Heritage"><a href="#Diffusion-Based-Augmentation-for-Captioning-and-Retrieval-in-Cultural-Heritage" class="headerlink" title="Diffusion Based Augmentation for Captioning and Retrieval in Cultural Heritage"></a>Diffusion Based Augmentation for Captioning and Retrieval in Cultural Heritage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07151">http://arxiv.org/abs/2308.07151</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ciodar/cultural-heritage-diffaug">https://github.com/ciodar/cultural-heritage-diffaug</a></li>
<li>paper_authors: Dario Cioni, Lorenzo Berlincioni, Federico Becattini, Alberto del Bimbo</li>
<li>for: This paper aims to address the challenges of limited annotated data and domain shifts in the cultural heritage domain by leveraging generative vision-language models to augment art datasets.</li>
<li>methods: The proposed approach uses generative vision-language models to generate diverse variations of artworks conditioned on their captions, enhancing dataset diversity and improving the alignment of visual cues with knowledge from general-purpose datasets.</li>
<li>results: The generated variations assist in training vision and language models with a deeper understanding of artistic characteristics, allowing for better caption generation with appropriate jargon.<details>
<summary>Abstract</summary>
Cultural heritage applications and advanced machine learning models are creating a fruitful synergy to provide effective and accessible ways of interacting with artworks. Smart audio-guides, personalized art-related content and gamification approaches are just a few examples of how technology can be exploited to provide additional value to artists or exhibitions. Nonetheless, from a machine learning point of view, the amount of available artistic data is often not enough to train effective models. Off-the-shelf computer vision modules can still be exploited to some extent, yet a severe domain shift is present between art images and standard natural image datasets used to train such models. As a result, this can lead to degraded performance. This paper introduces a novel approach to address the challenges of limited annotated data and domain shifts in the cultural heritage domain. By leveraging generative vision-language models, we augment art datasets by generating diverse variations of artworks conditioned on their captions. This augmentation strategy enhances dataset diversity, bridging the gap between natural images and artworks, and improving the alignment of visual cues with knowledge from general-purpose datasets. The generated variations assist in training vision and language models with a deeper understanding of artistic characteristics and that are able to generate better captions with appropriate jargon.
</details>
<details>
<summary>摘要</summary>
文化遗产应用和高级机器学习模型之间存在辉煌的共同作用，以提供有效和可accessible的艺术作品交互方式。智能音频导览、个性化艺术内容和游戏化方法等是技术的应用场景之一。然而，从机器学习的角度来看，可用的艺术数据量往往不够用于训练有效的模型。使用市场上的计算机视觉模块仍然可以获得一定的利用优势，但是领域转移问题仍然存在，这可能导致模型的性能下降。这篇论文提出了一种新的方法，用于解决文化遗产领域中缺乏注释数据和领域转移问题。通过利用生成视力语言模型，我们可以对艺术作品集添加多样化的变化，使得这些变化覆盖了自然图像和艺术作品之间的差异。这种增强策略可以增加数据集的多样性，使得视觉和语言模型能够更好地理解艺术特征，并且可以生成更好的描述文本。
</details></li>
</ul>
<hr>
<h2 id="A-Time-aware-tensor-decomposition-for-tracking-evolving-patterns"><a href="#A-Time-aware-tensor-decomposition-for-tracking-evolving-patterns" class="headerlink" title="A Time-aware tensor decomposition for tracking evolving patterns"></a>A Time-aware tensor decomposition for tracking evolving patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07126">http://arxiv.org/abs/2308.07126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Chatzis, Max Pfeffer, Pedro Lind, Evrim Acar</li>
<li>for: 用于提取时间序列数据中逐渐发展的下游模式</li>
<li>methods: 使用PARAFAC2基于tensor分解方法，增加时间正则化来捕捉时间序列数据中的下游模式</li>
<li>results: 在Synthetic数据上进行了广泛的实验，表明tPARAFAC2可以更加准确地捕捉时间序列数据中的下游模式，比PARAFAC2和时间稳定正则化 coupled matrix factorization perfom better.<details>
<summary>Abstract</summary>
Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regularization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将数据集视为高阶张量，其中一个方向是时间方向，可以使用张量分解方法捕捉下面数据集中的底层模式。然而，已有的方法通常忽略了时间方面，allowing for the reordering of time points。在latest studies, temporal regularizers are incorporated in the time mode to tackle this issue.However, existing approaches still do not allow underlying patterns to change in time（例如，在脑中的空间变化，话题中的上下文变化）。本文提出了temporal PARAFAC2（tPARAFAC2）：基于PARAFAC2的张量分解方法，带有时间正则化来提取时间数据中的慢慢发展模式。通过对synthetic数据进行了广泛的实验，我们表明了tPARAFAC2可以准确地捕捉下面数据集中的下面模式，并且perform better than PARAFAC2和coupled matrix factorization with temporal smoothness regularization。Note that "高阶张量" (gāo xià zhāng liàng) in the text refers to a higher-order tensor, and "张量分解" (zhāng liàng fāng jiě) refers to tensor factorization.
</details></li>
</ul>
<hr>
<h2 id="An-Outlook-into-the-Future-of-Egocentric-Vision"><a href="#An-Outlook-into-the-Future-of-Egocentric-Vision" class="headerlink" title="An Outlook into the Future of Egocentric Vision"></a>An Outlook into the Future of Egocentric Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07123">http://arxiv.org/abs/2308.07123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiara Plizzari, Gabriele Goletto, Antonino Furnari, Siddhant Bansal, Francesco Ragusa, Giovanni Maria Farinella, Dima Damen, Tatiana Tommasi</li>
<li>for: 本文探讨了现代 egocentric vision 研究与未来的 gap，即将穿戴式计算机、外向摄像头和数字覆盖层 integrate 到我们日常生活中。</li>
<li>methods: 本文首先通过人物故事来幻想未来，并通过示例表明当前技术的限制。然后，文章提供了将未来与已定义的研究任务映射的方法，并对每个任务进行了论述，包括前沿技术、现状的方法和数据集。</li>
<li>results: 本文结束于对未来研究的建议，以解锁我们的Path to the future的 always-on、个性化和生活改善 egocentric vision。<details>
<summary>Abstract</summary>
What will the future be? We wonder! In this survey, we explore the gap between current research in egocentric vision and the ever-anticipated future, where wearable computing, with outward facing cameras and digital overlays, is expected to be integrated in our every day lives. To understand this gap, the article starts by envisaging the future through character-based stories, showcasing through examples the limitations of current technology. We then provide a mapping between this future and previously defined research tasks. For each task, we survey its seminal works, current state-of-the-art methodologies and available datasets, then reflect on shortcomings that limit its applicability to future research. Note that this survey focuses on software models for egocentric vision, independent of any specific hardware. The paper concludes with recommendations for areas of immediate explorations so as to unlock our path to the future always-on, personalised and life-enhancing egocentric vision.
</details>
<details>
<summary>摘要</summary>
未来是什么样的？我们感到不知道！在这份调查中，我们探索了现有的研究和未来预期的区别，其中包括与我们日常生活结合的携带式计算，以及对我们每日生活的数位覆写。为了理解这个差距，这篇文章首先透过人物故事来预见未来，示例了现有技术的限制。然后，我们提供了对这些未来任务的映射，并评估了每个任务的先驱性研究、目前的技术方法和可用数据集。我们反思了这些任务的缺陷，限制了它们的应用性。注意，这份调查专注于视觉辨识软件模型，不受任何特定硬件的限制。文章结束时，我们提出了对未来研究的探索方向，以解锁我们的未来总是“在”、“个人化”和“生活改善”的视觉辨识。
</details></li>
</ul>
<hr>
<h2 id="On-the-Importance-of-Spatial-Relations-for-Few-shot-Action-Recognition"><a href="#On-the-Importance-of-Spatial-Relations-for-Few-shot-Action-Recognition" class="headerlink" title="On the Importance of Spatial Relations for Few-shot Action Recognition"></a>On the Importance of Spatial Relations for Few-shot Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07119">http://arxiv.org/abs/2308.07119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilun Zhang, Yuqian Fu, Xingjun Ma, Lizhe Qi, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang</li>
<li>for: 这个论文主要targets at improving few-shot action recognition in videos by leveraging both spatial and temporal information.</li>
<li>methods: The proposed method, called Spatial Alignment Cross Transformer (SA-CT), incorporates a novel spatial alignment mechanism to re-adjust the spatial relations between objects in videos, and integrates temporal information through a Temporal Mixer module.</li>
<li>results: The proposed method achieves comparable performance to temporal-based methods on 3&#x2F;4 benchmarks, and outperforms the state-of-the-art few-shot action recognition methods on 2 benchmarks. Additionally, the authors exploit large-scale pretrained models for few-shot action recognition and provide useful insights for this research direction.<details>
<summary>Abstract</summary>
Deep learning has achieved great success in video recognition, yet still struggles to recognize novel actions when faced with only a few examples. To tackle this challenge, few-shot action recognition methods have been proposed to transfer knowledge from a source dataset to a novel target dataset with only one or a few labeled videos. However, existing methods mainly focus on modeling the temporal relations between the query and support videos while ignoring the spatial relations. In this paper, we find that the spatial misalignment between objects also occurs in videos, notably more common than the temporal inconsistency. We are thus motivated to investigate the importance of spatial relations and propose a more accurate few-shot action recognition method that leverages both spatial and temporal information. Particularly, a novel Spatial Alignment Cross Transformer (SA-CT) which learns to re-adjust the spatial relations and incorporates the temporal information is contributed. Experiments reveal that, even without using any temporal information, the performance of SA-CT is comparable to temporal based methods on 3/4 benchmarks. To further incorporate the temporal information, we propose a simple yet effective Temporal Mixer module. The Temporal Mixer enhances the video representation and improves the performance of the full SA-CT model, achieving very competitive results. In this work, we also exploit large-scale pretrained models for few-shot action recognition, providing useful insights for this research direction.
</details>
<details>
<summary>摘要</summary>
深度学习在视频识别中取得了很大的成功，然而仍然在面临只有一些示例时难以识别新的动作。为解决这个挑战，一些基于几个示例的动作识别方法已经被提出来，这些方法主要是模型着视频中的时间关系。然而，现有的方法很多都忽略了视频中的空间关系。在这篇论文中，我们发现了视频中的空间不一致现象，特别是在视频中的 объек 之间存在很多的空间不一致。这使我们被激励去研究空间关系的重要性，并提出了一种更准确的几个示例动作识别方法。特别是，我们提出了一种新的空间对准交叉传播（SA-CT）模型，它可以学习重新调整视频中的空间关系，并同时 incorporate 时间信息。实验表明，即使不使用任何时间信息，SA-CT 模型的性能与基于时间信息的方法相当，在 3/4  benchmark 上。为了进一步 incorporate 时间信息，我们还提出了一种简单 yet 有效的时间混合模块。时间混合模块可以提高视频表示，并使全SA-CT模型的性能非常竞争力。此外，我们还利用了大规模预训练模型来进行几个示例动作识别，提供了有用的研究方向。
</details></li>
</ul>
<hr>
<h2 id="SCSC-Spatial-Cross-scale-Convolution-Module-to-Strengthen-both-CNNs-and-Transformers"><a href="#SCSC-Spatial-Cross-scale-Convolution-Module-to-Strengthen-both-CNNs-and-Transformers" class="headerlink" title="SCSC: Spatial Cross-scale Convolution Module to Strengthen both CNNs and Transformers"></a>SCSC: Spatial Cross-scale Convolution Module to Strengthen both CNNs and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07110">http://arxiv.org/abs/2308.07110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xijun Wang, Xiaojie Chu, Chunrui Han, Xiangyu Zhang<br>for:This paper presents a module called Spatial Cross-scale Convolution (SCSC) that improves the performance of both Convolutional Neural Networks (CNNs) and Transformers.methods:The SCSC module uses an efficient spatial cross-scale encoder and spatial embed module to capture a variety of features in one layer, addressing the issues of large dense kernels and self-attention in existing architectures.results:The SCSC module is shown to improve the performance of various base models on the face recognition task and ImageNet classification task, with 2.7% and 5.3% improvement in accuracy, respectively, while reducing the number of parameters and FLOPs by 79% and 22%, respectively. Additionally, a traditional network embedded with SCSC can match the performance of Swin Transformer.<details>
<summary>Abstract</summary>
This paper presents a module, Spatial Cross-scale Convolution (SCSC), which is verified to be effective in improving both CNNs and Transformers. Nowadays, CNNs and Transformers have been successful in a variety of tasks. Especially for Transformers, increasing works achieve state-of-the-art performance in the computer vision community. Therefore, researchers start to explore the mechanism of those architectures. Large receptive fields, sparse connections, weight sharing, and dynamic weight have been considered keys to designing effective base models. However, there are still some issues to be addressed: large dense kernels and self-attention are inefficient, and large receptive fields make it hard to capture local features. Inspired by the above analyses and to solve the mentioned problems, in this paper, we design a general module taking in these design keys to enhance both CNNs and Transformers. SCSC introduces an efficient spatial cross-scale encoder and spatial embed module to capture assorted features in one layer. On the face recognition task, FaceResNet with SCSC can improve 2.7% with 68% fewer FLOPs and 79% fewer parameters. On the ImageNet classification task, Swin Transformer with SCSC can achieve even better performance with 22% fewer FLOPs, and ResNet with CSCS can improve 5.3% with similar complexity. Furthermore, a traditional network (e.g., ResNet) embedded with SCSC can match Swin Transformer's performance.
</details>
<details>
<summary>摘要</summary>
Inspired by these analyses and to solve these problems, the authors design a general module that incorporates these design elements to enhance both CNNs and Transformers. The SCSC module introduces an efficient spatial cross-scale encoder and spatial embed module to capture diverse features in one layer.On the face recognition task, the FaceResNet model with SCSC improves performance by 2.7% with 68% fewer floating-point operations (FLOPs) and 79% fewer parameters. On the ImageNet classification task, the Swin Transformer model with SCSC achieves better performance with 22% fewer FLOPs, and the ResNet model with CSCS improves performance by 5.3% with similar complexity. Additionally, a traditional network (e.g., ResNet) embedded with SCSC can match the performance of Swin Transformer.
</details></li>
</ul>
<hr>
<h2 id="Checklist-to-Transparently-Define-Test-Oracles-for-TP-FP-and-FN-Objects-in-Automated-Driving"><a href="#Checklist-to-Transparently-Define-Test-Oracles-for-TP-FP-and-FN-Objects-in-Automated-Driving" class="headerlink" title="Checklist to Transparently Define Test Oracles for TP, FP, and FN Objects in Automated Driving"></a>Checklist to Transparently Define Test Oracles for TP, FP, and FN Objects in Automated Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07106">http://arxiv.org/abs/2308.07106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michael-hoss/paper-oracle-definition">https://github.com/michael-hoss/paper-oracle-definition</a></li>
<li>paper_authors: Michael Hoss</li>
<li>for: 这篇论文是为了提供一份关于汽车自动驾驶系统的感知子系统测试 oracle 的检查列表。</li>
<li>methods: 该论文使用了一系列的函数方面和实现细节来描述测试 oracle 的行为。</li>
<li>results: 该论文提供了一份可以帮助实践者提高测试 oracle 的透明度，从而使对象感知的声明更加可靠和比较可靠。<details>
<summary>Abstract</summary>
Popular test oracles for the perception subsystem of driving automation systems identify true-positive (TP), false-positive (FP), and false-negative (FN) objects. Oracle transparency is needed for comparing test results and for safety cases. To date, there exists a common notion of TPs, FPs, and FNs in the field, but apparently no published way to comprehensively define their oracles. Therefore, this paper provides a checklist of functional aspects and implementation details that affect the oracle behavior. Besides labeling policies of the test set, we cover fields of view, occlusion handling, safety-relevant areas, matching criteria, temporal and probabilistic issues, and further aspects. Even though our checklist can hardly be formalized, it can help practitioners maximize the transparency of their oracles, which, in turn, makes statements on object perception more reliable and comparable.
</details>
<details>
<summary>摘要</summary>
Popular test oracles for the perception subsystem of autonomous driving systems identify true-positive (TP), false-positive (FP), and false-negative (FN) objects. Oracle transparency is needed for comparing test results and for safety cases. To date, there exists a common notion of TPs, FPs, and FNs in the field, but apparently no published way to comprehensively define their oracles. Therefore, this paper provides a checklist of functional aspects and implementation details that affect the oracle behavior. Besides labeling policies of the test set, we cover fields of view, occlusion handling, safety-relevant areas, matching criteria, temporal and probabilistic issues, and further aspects. Even though our checklist can hardly be formalized, it can help practitioners maximize the transparency of their oracles, which, in turn, makes statements on object perception more reliable and comparable.Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving"><a href="#FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving" class="headerlink" title="FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving"></a>FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07104">http://arxiv.org/abs/2308.07104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhonghuayi/focusflow_official">https://github.com/zhonghuayi/focusflow_official</a></li>
<li>paper_authors: Zhonghua Yi, Hao Shi, Kailun Yang, Qi Jiang, Yaozu Ye, Ze Wang, Kaiwei Wang</li>
<li>for: 提高数据驱动 optical flow 估计方法的精度和稳定性，特别是在关键点方面。</li>
<li>methods: 引入点 cloud 模型化方法，并使用权重控制机制来让模型更加注意点云。基于这种模型化方法，提出了一个混合损失函数和特别的 Conditional Point Control Loss (CPCL) 函数来进行多个点的监督。</li>
<li>results: 与基于原始模型的方法相比，FocusFlow 显示出了 +44.5% 的精度提高，并且具有出色的扩展性和灵活性。此外，FocusFlow 在不同的关键点方面也有着优秀的表现，如 ORB、SIFT 和学习基于 SiLK 的关键点。<details>
<summary>Abstract</summary>
Key-point-based scene understanding is fundamental for autonomous driving applications. At the same time, optical flow plays an important role in many vision tasks. However, due to the implicit bias of equal attention on all points, classic data-driven optical flow estimation methods yield less satisfactory performance on key points, limiting their implementations in key-point-critical safety-relevant scenarios. To address these issues, we introduce a points-based modeling method that requires the model to learn key-point-related priors explicitly. Based on the modeling method, we present FocusFlow, a framework consisting of 1) a mix loss function combined with a classic photometric loss function and our proposed Conditional Point Control Loss (CPCL) function for diverse point-wise supervision; 2) a conditioned controlling model which substitutes the conventional feature encoder by our proposed Condition Control Encoder (CCE). CCE incorporates a Frame Feature Encoder (FFE) that extracts features from frames, a Condition Feature Encoder (CFE) that learns to control the feature extraction behavior of FFE from input masks containing information of key points, and fusion modules that transfer the controlling information between FFE and CFE. Our FocusFlow framework shows outstanding performance with up to +44.5% precision improvement on various key points such as ORB, SIFT, and even learning-based SiLK, along with exceptional scalability for most existing data-driven optical flow methods like PWC-Net, RAFT, and FlowFormer. Notably, FocusFlow yields competitive or superior performances rivaling the original models on the whole frame. The source code will be available at https://github.com/ZhonghuaYi/FocusFlow_official.
</details>
<details>
<summary>摘要</summary>
KEY-POINT-BASED SCENE UNDERSTANDING IS FUNDAMENTAL FOR AUTONOMOUS DRIVING APPLICATIONS. AT THE SAME TIME, OPTICAL FLOW PLAYS AN IMPORTANT ROLE IN MANY VISION TASKS. HOWEVER, DUE TO THE IMPLICIT BIAS OF EQUAL ATTENTION ON ALL POINTS, CLASSIC DATA-DRIVEN OPTICAL FLOW ESTIMATION METHODS YIELD LESS SATISFACTORY PERFORMANCE ON KEY POINTS, LIMITING THEIR IMPLEMENTATIONS IN KEY-POINT-CRITICAL SAFETY-RELEVANT SCENARIOS. TO ADDRESS THESE ISSUES, WE INTRODUCE A POINTS-BASED MODELING METHOD THAT REQUIRES THE MODEL TO LEARN KEY-POINT-RELATED PRIORS EXPLICITLY. BASED ON THE MODELING METHOD, WE PRESENT FOCUSFLOW, A FRAMEWORK CONSISTING OF 1) A MIX LOSS FUNCTION COMBINED WITH A CLASSIC PHOTOMETRIC LOSS FUNCTION AND OUR PROPOSED CONDITIONAL POINT CONTROL LOSS (CPCL) FUNCTION FOR DIVERSE POINT-WISE SUPERVISION; 2) A CONDITIONED CONTROLLING MODEL WHICH SUBSTITUTES THE CONVENTIONAL FEATURE ENCODER BY OUR PROPOSED CONDITION CONTROL ENCODER (CCE). CCE INCORPORATES A FRAME FEATURE ENCODER (FFE) THAT EXTRACTS FEATURES FROM FRAMES, A CONDITION FEATURE ENCODER (CFE) THAT LEARNS TO CONTROL THE FEATURE EXTRACTION BEHAVIOR OF FFE FROM INPUT MASKS CONTAINING INFORMATION OF KEY POINTS, AND FUSION MODULES THAT TRANSFER THE CONTROLLING INFORMATION BETWEEN FFE AND CFE. OUR FOCUSFLOW FRAMEWORK SHOWS OUTSTANDING PERFORMANCE WITH UP TO +44.5% PRECISION IMPROVEMENT ON VARIOUS KEY POINTS SUCH AS ORB, SIFT, AND EVEN LEARNING-BASED SiLK, ALONG WITH EXCEPTIONAL SCALABILITY FOR MOST EXISTING DATA-DRIVEN OPTICAL FLOW METHODS LIKE PWC-NET, RAFT, AND FLOWFORMER. NOTABLY, FOCUSFLOW YIELDS COMPETITIVE OR SUPERIOR PERFORMANCES RIVALING THE ORIGINAL MODELS ON THE WHOLE FRAME. THE SOURCE CODE WILL BE AVAILABLE AT <https://GITHUB.COM/ZHONGHUAYI/FOCUSFLOW_OFFICIAL>.
</details></li>
</ul>
<hr>
<h2 id="Masked-Motion-Predictors-are-Strong-3D-Action-Representation-Learners"><a href="#Masked-Motion-Predictors-are-Strong-3D-Action-Representation-Learners" class="headerlink" title="Masked Motion Predictors are Strong 3D Action Representation Learners"></a>Masked Motion Predictors are Strong 3D Action Representation Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07092">http://arxiv.org/abs/2308.07092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunyao Mao, Jiajun Deng, Wengang Zhou, Yao Fang, Wanli Ouyang, Houqiang Li</li>
<li>for: 本研究旨在提出一种有效的自助学习预训练方法，以提高3D人体动作识别模型的性能。</li>
<li>methods: 本研究使用的方法是Masked Motion Prediction（MAMP）框架，具体来说是对带有掩蔽的空间时间骨架序列进行预测，预测的是掩蔽的人体关节的 temporal 运动。在预测过程中，研究者们还利用了高度重复的时间序列的特点，通过将动作信息作为empirical semantic richness prior，引导掩蔽过程，从而提高了对semantically rich的时间区域的注意力。</li>
<li>results: 对于NTU-60、NTU-120和PKU-MMD等 datasets，MAMP预训练后的vanilla transformer得到了state-of-the-art的结果，不需要额外的技术和工具。研究者们还提供了源代码，可以在<a target="_blank" rel="noopener" href="https://github.com/maoyunyao/MAMP%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/maoyunyao/MAMP上下载。</a><details>
<summary>Abstract</summary>
In 3D human action recognition, limited supervised data makes it challenging to fully tap into the modeling potential of powerful networks such as transformers. As a result, researchers have been actively investigating effective self-supervised pre-training strategies. In this work, we show that instead of following the prevalent pretext task to perform masked self-component reconstruction in human joints, explicit contextual motion modeling is key to the success of learning effective feature representation for 3D action recognition. Formally, we propose the Masked Motion Prediction (MAMP) framework. To be specific, the proposed MAMP takes as input the masked spatio-temporal skeleton sequence and predicts the corresponding temporal motion of the masked human joints. Considering the high temporal redundancy of the skeleton sequence, in our MAMP, the motion information also acts as an empirical semantic richness prior that guide the masking process, promoting better attention to semantically rich temporal regions. Extensive experiments on NTU-60, NTU-120, and PKU-MMD datasets show that the proposed MAMP pre-training substantially improves the performance of the adopted vanilla transformer, achieving state-of-the-art results without bells and whistles. The source code of our MAMP is available at https://github.com/maoyunyao/MAMP.
</details>
<details>
<summary>摘要</summary>
在3D人体动作识别领域，有限的指导数据使得模型充分发挥其力量是挑战。因此，研究人员 актив地寻找有效的自监学习前置策略。在这项工作中，我们表明，而不是通过常见的预文务masked自组件重建来实现自监学习，而是明确的上下文动作模型化才是3D动作识别中的锚点。我们提出了Masked Motion Prediction（MAMP）框架。具体来说，我们的MAMP框架接受带有mask的空间时间骨架序列作为输入，并预测带mask的人体关节的时间动作。由于骨架序列的高时间重复率，我们在MAMP中使用动作信息作为empirical semantic richness prior，以便更好地引导masking过程，使得更好地注意到semantically rich的时间区域。我们在NTU-60、NTU-120和PKU-MMD datasets上进行了广泛的实验，结果显示，我们的MAMP预训练方法可以在不使用额外技巧的情况下，以状态 искусственный的方式提高采纳的Transformer模型的性能，达到当前最佳的结果。MAMP的源代码可以在https://github.com/maoyunyao/MAMP上下载。
</details></li>
</ul>
<hr>
<h2 id="ICPC-Instance-Conditioned-Prompting-with-Contrastive-Learning-for-Semantic-Segmentation"><a href="#ICPC-Instance-Conditioned-Prompting-with-Contrastive-Learning-for-Semantic-Segmentation" class="headerlink" title="ICPC: Instance-Conditioned Prompting with Contrastive Learning for Semantic Segmentation"></a>ICPC: Instance-Conditioned Prompting with Contrastive Learning for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07078">http://arxiv.org/abs/2308.07078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaohui Yu, Qiang Zhou, Zhibin Wang, Fan Wang</li>
<li>for: 这篇论文主要针对的是提高 Semantic Segmentation 中的 multimodal alignment，以提高 CLIP 知识的传递性能。</li>
<li>methods: 本文提出了两个方法来提高 multimodal alignment：一是使用动态提问来更好地利用文本编码器，二是提出了一种对比学习引导的 alignment 损失函数。</li>
<li>results: 对三个大规模数据集（ADE20K、COCO-Stuff10k 和 ADE20K-Full）进行了广泛的实验，结果显示，ICPC 在不同的底层模型上都能够得到了稳定的改进，比如使用 ResNet-50 为例，ICPC 在三个数据集上的 mIoU 分别提高了1.71%、1.05% 和 1.41%。<details>
<summary>Abstract</summary>
Modern supervised semantic segmentation methods are usually finetuned based on the supervised or self-supervised models pre-trained on ImageNet. Recent work shows that transferring the knowledge from CLIP to semantic segmentation via prompt learning can achieve promising performance. The performance boost comes from the feature enhancement with multimodal alignment, i.e., the dot product between vision and text embeddings. However, how to improve the multimodal alignment for better transfer performance in dense tasks remains underexplored. In this work, we focus on improving the quality of vision-text alignment from two aspects of prompting design and loss function, and present an instance-conditioned prompting with contrastive learning (ICPC) framework. First, compared with the static prompt designs, we reveal that dynamic prompting conditioned on image content can more efficiently utilize the text encoder for complex dense tasks. Second, we propose an align-guided contrastive loss to refine the alignment of vision and text embeddings. We further propose lightweight multi-scale alignment for better performance. Extensive experiments on three large-scale datasets (ADE20K, COCO-Stuff10k, and ADE20K-Full) demonstrate that ICPC brings consistent improvements across diverse backbones. Taking ResNet-50 as an example, ICPC outperforms the state-of-the-art counterpart by 1.71%, 1.05%, and 1.41% mIoU on the three datasets, respectively.
</details>
<details>
<summary>摘要</summary>
现代超级vised semantic segmentation方法通常是基于ImageNet预训练的超级vised或自适应模型的finetuning。 latest work shows that transferring the knowledge from CLIP to semantic segmentation via prompt learning can achieve promising performance. The performance boost comes from the feature enhancement with multimodal alignment, i.e., the dot product between vision and text embeddings. However, how to improve the multimodal alignment for better transfer performance in dense tasks remains underexplored. In this work, we focus on improving the quality of vision-text alignment from two aspects of prompting design and loss function, and present an instance-conditioned prompting with contrastive learning (ICPC) framework. First, compared with the static prompt designs, we reveal that dynamic prompting conditioned on image content can more efficiently utilize the text encoder for complex dense tasks. Second, we propose an align-guided contrastive loss to refine the alignment of vision and text embeddings. We further propose lightweight multi-scale alignment for better performance. Extensive experiments on three large-scale datasets (ADE20K, COCO-Stuff10k, and ADE20K-Full) demonstrate that ICPC brings consistent improvements across diverse backbones. Taking ResNet-50 as an example, ICPC outperforms the state-of-the-art counterpart by 1.71%, 1.05%, and 1.41% mIoU on the three datasets, respectively.
</details></li>
</ul>
<hr>
<h2 id="Teeth-And-Root-Canals-Segmentation-Using-ZXYFormer-With-Uncertainty-Guidance-And-Weight-Transfer"><a href="#Teeth-And-Root-Canals-Segmentation-Using-ZXYFormer-With-Uncertainty-Guidance-And-Weight-Transfer" class="headerlink" title="Teeth And Root Canals Segmentation Using ZXYFormer With Uncertainty Guidance And Weight Transfer"></a>Teeth And Root Canals Segmentation Using ZXYFormer With Uncertainty Guidance And Weight Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07072">http://arxiv.org/abs/2308.07072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shangxuan Li, Yu Du, Li Ye, Chichi Li, Yanshu Fang, Cheng Wang, Wu Zhou</li>
<li>for: 这个研究旨在从CBCT图像中同时分类牙齿和根 канал，但是这个过程存在许多挑战。</li>
<li>methods: 我们提出了一种从粗细到细的分类方法，使用倒推特征融合变换和不确定性估计来解决这些挑战。</li>
<li>results: 经过157份高级CBCT数据的合作分类实验，发现我们的方法比现有的牙齿或根 канал分类方法更好。<details>
<summary>Abstract</summary>
This study attempts to segment teeth and root-canals simultaneously from CBCT images, but there are very challenging problems in this process. First, the clinical CBCT image data is very large (e.g., 672 *688 * 688), and the use of downsampling operation will lose useful information about teeth and root canals. Second, teeth and root canals are very different in morphology, and it is difficult for a simple network to identify them precisely. In addition, there are weak edges at the tooth, between tooth and root canal, which makes it very difficult to segment such weak edges. To this end, we propose a coarse-to-fine segmentation method based on inverse feature fusion transformer and uncertainty estimation to address above challenging problems. First, we use the downscaled volume data (e.g., 128 * 128 * 128) to conduct coarse segmentation and map it to the original volume to obtain the area of teeth and root canals. Then, we design a transformer with reverse feature fusion, which can bring better segmentation effect of different morphological objects by transferring deeper features to shallow features. Finally, we design an auxiliary branch to calculate and refine the difficult areas in order to improve the weak edge segmentation performance of teeth and root canals. Through the combined tooth and root canal segmentation experiment of 157 clinical high-resolution CBCT data, it is verified that the proposed method is superior to the existing tooth or root canal segmentation methods.
</details>
<details>
<summary>摘要</summary>
First, we use the downscaled volume data (e.g., 128 x 128 x 128) to conduct coarse segmentation and map it to the original volume to obtain the area of teeth and root canals. Then, we design a transformer with reverse feature fusion, which can bring better segmentation effects of different morphological objects by transferring deeper features to shallow features. Finally, we design an auxiliary branch to calculate and refine the difficult areas in order to improve the weak edge segmentation performance of teeth and root canals.Through the combined tooth and root canal segmentation experiment of 157 clinical high-resolution CBCT data, it is verified that the proposed method is superior to the existing tooth or root canal segmentation methods.
</details></li>
</ul>
<hr>
<h2 id="A-Local-Iterative-Approach-for-the-Extraction-of-2D-Manifolds-from-Strongly-Curved-and-Folded-Thin-Layer-Structures"><a href="#A-Local-Iterative-Approach-for-the-Extraction-of-2D-Manifolds-from-Strongly-Curved-and-Folded-Thin-Layer-Structures" class="headerlink" title="A Local Iterative Approach for the Extraction of 2D Manifolds from Strongly Curved and Folded Thin-Layer Structures"></a>A Local Iterative Approach for the Extraction of 2D Manifolds from Strongly Curved and Folded Thin-Layer Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07070">http://arxiv.org/abs/2308.07070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Klenert, Verena Lepper, Daniel Baum</li>
<li>for: 本文旨在分析 ancient rolled and folded 纸质结构，如纸、羊皮纸和银箔等，通过分析微型计算Tomography（Micro-CT）图像数据。</li>
<li>methods: 本文提出了一种新的方法，基于本地快走算法和分区区域的方法，可以提取2D manifold。</li>
<li>results: 本文通过使用 искусственный数据和实际数据进行示例，证明了该方法的可靠性和灵活性。<details>
<summary>Abstract</summary>
Ridge surfaces represent important features for the analysis of 3-dimensional (3D) datasets in diverse applications and are often derived from varying underlying data including flow fields, geological fault data, and point data, but they can also be present in the original scalar images acquired using a plethora of imaging techniques. Our work is motivated by the analysis of image data acquired using micro-computed tomography (Micro-CT) of ancient, rolled and folded thin-layer structures such as papyrus, parchment, and paper as well as silver and lead sheets. From these documents we know that they are 2-dimensional (2D) in nature. Hence, we are particularly interested in reconstructing 2D manifolds that approximate the document's structure. The image data from which we want to reconstruct the 2D manifolds are often very noisy and represent folded, densely-layered structures with many artifacts, such as ruptures or layer splitting and merging. Previous ridge-surface extraction methods fail to extract the desired 2D manifold for such challenging data. We have therefore developed a novel method to extract 2D manifolds. The proposed method uses a local fast marching scheme in combination with a separation of the region covered by fast marching into two sub-regions. The 2D manifold of interest is then extracted as the surface separating the two sub-regions. The local scheme can be applied for both automatic propagation as well as interactive analysis. We demonstrate the applicability and robustness of our method on both artificial data as well as real-world data including folded silver and papyrus sheets.
</details>
<details>
<summary>摘要</summary>
三维数据集中的ridge表面对于多种应用场景是重要的特征，它们可以从不同的基础数据中 deriv，包括流体场数据、地质断层数据和点数据。但是，它们也可以在原始的scalar图像中存在。我们的工作受到ancient、rolled和folded薄层结构中的纸、羊皮纸和银屑Sheet的微计算 Tomography（Micro-CT）图像数据的分析启发。这些文档都是二维的（2D）性质。因此，我们特别关心从图像数据中提取2D manifold。图像数据经常具有噪音和缺失数据，表现为折叠、厚层结构和多种遗产物，如裂隙或层合并。现有的ridge-surface提取方法无法提取desired 2D manifold。我们因此开发了一种新的方法，使用本地快速推进方案和分区区域的分解。我们提取的2D manifold是这两个分区域之间的表面。本地方案可以用于自动推进以及交互分析。我们在人工数据和实际数据，包括折叠的银和纸Sheet中证明了我们的方法的可行性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Survey-on-video-anomaly-detection-in-dynamic-scenes-with-moving-cameras"><a href="#Survey-on-video-anomaly-detection-in-dynamic-scenes-with-moving-cameras" class="headerlink" title="Survey on video anomaly detection in dynamic scenes with moving cameras"></a>Survey on video anomaly detection in dynamic scenes with moving cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07050">http://arxiv.org/abs/2308.07050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runyu Jiao, Yi Wan, Fabio Poiesi, Yiming Wang</li>
<li>for: 这篇论文旨在为摄像头动态场景中检测异常现象提供一个全面的综述。</li>
<li>methods: 这篇论文评估了不同的检测方法，包括深度学习、异常点检测、异常流行分析等。</li>
<li>results: 这篇论文通过对多个应用领域和数据集的分析，发现了现有的检测方法具有一定的局限性和挑战，并提出了未来研究的方向和新贡献。<details>
<summary>Abstract</summary>
The increasing popularity of compact and inexpensive cameras, e.g.~dash cameras, body cameras, and cameras equipped on robots, has sparked a growing interest in detecting anomalies within dynamic scenes recorded by moving cameras. However, existing reviews primarily concentrate on Video Anomaly Detection (VAD) methods assuming static cameras. The VAD literature with moving cameras remains fragmented, lacking comprehensive reviews to date. To address this gap, we endeavor to present the first comprehensive survey on Moving Camera Video Anomaly Detection (MC-VAD). We delve into the research papers related to MC-VAD, critically assessing their limitations and highlighting associated challenges. Our exploration encompasses three application domains: security, urban transportation, and marine environments, which in turn cover six specific tasks. We compile an extensive list of 25 publicly-available datasets spanning four distinct environments: underwater, water surface, ground, and aerial. We summarize the types of anomalies these datasets correspond to or contain, and present five main categories of approaches for detecting such anomalies. Lastly, we identify future research directions and discuss novel contributions that could advance the field of MC-VAD. With this survey, we aim to offer a valuable reference for researchers and practitioners striving to develop and advance state-of-the-art MC-VAD methods.
</details>
<details>
<summary>摘要</summary>
随着小型便宜的摄像机的普及，如推车摄像机、身体摄像机和机器人装备的摄像机，对动态场景中的异常检测已经引起了越来越多的关注。然而，现有的评论主要集中在静止摄像机上的视频异常检测（VAD）方法上，而移动摄像机上的VAD方法的研究仍然是 Fragmented，无法提供全面的综述。为了bridging这个差距，我们努力为您提供首个全面的移动摄像机视频异常检测（MC-VAD）综述。我们对MC-VAD相关的研究论文进行了严格的评估，并指出了相关挑战和局限性。我们的探索包括安全、城市交通和海洋环境等三个应用领域，这些领域内包括六个特定任务。我们编辑了25个公共可用的数据集，这些数据集覆盖了四个不同的环境：水下、水面、地面和空中。我们总结了这些数据集中的异常类型和含义，并提出了五种主要的异常检测方法。最后，我们标识了未来研究的方向和提出了新贡献，以便进一步推动MC-VAD领域的发展。通过这份综述，我们希望为研究人员和实践者提供一份有价值的参考，以帮助他们开发和提高MC-VAD方法的状态泰。
</details></li>
</ul>
<hr>
<h2 id="An-Inherent-Trade-Off-in-Noisy-Neural-Communication-with-Rank-Order-Coding"><a href="#An-Inherent-Trade-Off-in-Noisy-Neural-Communication-with-Rank-Order-Coding" class="headerlink" title="An Inherent Trade-Off in Noisy Neural Communication with Rank-Order Coding"></a>An Inherent Trade-Off in Noisy Neural Communication with Rank-Order Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07034">http://arxiv.org/abs/2308.07034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibrahim Alsolami, Tomoki Fukai</li>
<li>for: 研究哺乳动物大脑快速能力的新方法——排序编码法。</li>
<li>methods: 使用排序编码法研究哺乳动物大脑快速能力，并对噪声的影响进行研究。</li>
<li>results: 发现在某种噪声范围内，排序编码法可以实现更高的信息传输率，但也存在一类特殊的错误，这些错误会随着噪声增加。<details>
<summary>Abstract</summary>
Rank-order coding, a form of temporal coding, has emerged as a promising scheme to explain the rapid ability of the mammalian brain. Owing to its speed as well as efficiency, rank-order coding is increasingly gaining interest in diverse research areas beyond neuroscience. However, much uncertainty still exists about the performance of rank-order coding under noise. Herein we show what information rates are fundamentally possible and what trade-offs are at stake. An unexpected finding in this paper is the emergence of a special class of errors that, in a regime, increase with less noise.
</details>
<details>
<summary>摘要</summary>
层次编码（rank-order coding），一种时间编码方式，已成为人类大脑快速能力的解释方案。由于其速度和效率，层次编码在不同研究领域 beyond neuroscience 中日益受到关注。然而，对层次编码下噪声的性能仍存在很多不确定性。在这篇文章中，我们展示了可以实现的信息速率和协议的权衡。这篇文章的意外发现是，在某个 режиме下，噪声越少，这种特殊的错误会增加。Here's the word-for-word translation of the text into Simplified Chinese:人类大脑快速能力的解释方案，层次编码（rank-order coding）已经广泛受到关注，由于其速度和效率。然而，对层次编码下噪声的性能仍存在很多不确定性。本文章展示了可以实现的信息速率和协议的权衡，并发现了在某个REGIME下，噪声越少，特殊的错误会增加。
</details></li>
</ul>
<hr>
<h2 id="S3IM-Stochastic-Structural-SIMilarity-and-Its-Unreasonable-Effectiveness-for-Neural-Fields"><a href="#S3IM-Stochastic-Structural-SIMilarity-and-Its-Unreasonable-Effectiveness-for-Neural-Fields" class="headerlink" title="S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields"></a>S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07032">http://arxiv.org/abs/2308.07032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/madaoer/s3im_nerf">https://github.com/madaoer/s3im_nerf</a></li>
<li>paper_authors: Zeke Xie, Xindi Yang, Yujie Yang, Qi Sun, Yixiang Jiang, Haoran Wang, Yunfeng Cai, Mingming Sun</li>
<li>for: 本研究旨在提高NeRF和相关神经场方法（如神经表面表示）的性能，使其能够更好地Synthesize novel-view images。</li>
<li>methods: 本研究提出了一种非本地多重训练 paradigm，通过一种新的Stochastic Structural SIMilarity（S3IM）损失函数，将多个数据点处理为一个整体，而不是独立处理多个输入。</li>
<li>results: 我们的实验表明，S3IM可以减少TensoRF和DVGO的测试MSE损失率超过90%，并提高NeuS的F-score得分198%和Chamfer $L_{1}$距离减少64%。此外，S3IM也能够在缺乏输入、损坏图像和动态场景下保持稳定性。<details>
<summary>Abstract</summary>
Recently, Neural Radiance Field (NeRF) has shown great success in rendering novel-view images of a given scene by learning an implicit representation with only posed RGB images. NeRF and relevant neural field methods (e.g., neural surface representation) typically optimize a point-wise loss and make point-wise predictions, where one data point corresponds to one pixel. Unfortunately, this line of research failed to use the collective supervision of distant pixels, although it is known that pixels in an image or scene can provide rich structural information. To the best of our knowledge, we are the first to design a nonlocal multiplex training paradigm for NeRF and relevant neural field methods via a novel Stochastic Structural SIMilarity (S3IM) loss that processes multiple data points as a whole set instead of process multiple inputs independently. Our extensive experiments demonstrate the unreasonable effectiveness of S3IM in improving NeRF and neural surface representation for nearly free. The improvements of quality metrics can be particularly significant for those relatively difficult tasks: e.g., the test MSE loss unexpectedly drops by more than 90% for TensoRF and DVGO over eight novel view synthesis tasks; a 198% F-score gain and a 64% Chamfer $L_{1}$ distance reduction for NeuS over eight surface reconstruction tasks. Moreover, S3IM is consistently robust even with sparse inputs, corrupted images, and dynamic scenes.
</details>
<details>
<summary>摘要</summary>
最近，神经辐射场（NeRF）已经取得了大成功，通过只使用拍摄的RGB图像学习一个场景中的隐式表示，并且可以生成新视图图像。NeRF和相关的神经场方法（例如神经表面表示）通常是通过点位损失和点位预测来优化，其中一个数据点对应一个像素。然而，这一研究没有使用场景中像素之间的共同监督，尽管已知图像和场景中的像素可以提供丰富的结构信息。据我们所知，我们是首次设计了一种非本地多重训练方法，通过一种新的随机 Structural SIMilarity（S3IM）损失来处理多个数据点，而不是独立处理多个输入。我们的广泛实验表明，S3IM可以减少NeRF和神经表面表示的测试MSE损失，并且可以提高图像质量指标。特别是在比较困难的任务中，例如TensoRF和DVGO上的八个新视图合成任务，测试MSE损失异常下降了 más de 90%，而NeuS上的八个表面重建任务中，F-score提升了198%，Chamfer $L_{1}$距离减少了64%。此外，S3IM具有高度的稳定性，可以在稀缺输入、损坏图像和动态场景下表现出色。
</details></li>
</ul>
<hr>
<h2 id="AdvCLIP-Downstream-agnostic-Adversarial-Examples-in-Multimodal-Contrastive-Learning"><a href="#AdvCLIP-Downstream-agnostic-Adversarial-Examples-in-Multimodal-Contrastive-Learning" class="headerlink" title="AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning"></a>AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07026">http://arxiv.org/abs/2308.07026</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cgcl-codes/advclip">https://github.com/cgcl-codes/advclip</a></li>
<li>paper_authors: Ziqi Zhou, Shengshan Hu, Minghui Li, Hangtao Zhang, Yechao Zhang, Hai Jin</li>
<li>for: 这个论文目的是为了开发一个可以在各种复杂下渠任务上表现出色的通用特征提取器，例如CLIP，并在大量未经标记的图像文本数据上进行训练。</li>
<li>methods: 这个论文使用了交叉模态的预训练Encoder，并通过构建一个图像图文 topological graph структуры和一种基于这种结构的生成对抗网络来生成一个通用的对抗示例。</li>
<li>results: 该论文的结果表明，通过添加这个对抗示例到图像中，可以使图像的嵌入空间Sim（类似性）与不同模态之间的相似度减少，并在特征空间中扰乱样本分布，从而实现通用的非目标攻击。<details>
<summary>Abstract</summary>
Multimodal contrastive learning aims to train a general-purpose feature extractor, such as CLIP, on vast amounts of raw, unlabeled paired image-text data. This can greatly benefit various complex downstream tasks, including cross-modal image-text retrieval and image classification. Despite its promising prospect, the security issue of cross-modal pre-trained encoder has not been fully explored yet, especially when the pre-trained encoder is publicly available for commercial use.   In this work, we propose AdvCLIP, the first attack framework for generating downstream-agnostic adversarial examples based on cross-modal pre-trained encoders. AdvCLIP aims to construct a universal adversarial patch for a set of natural images that can fool all the downstream tasks inheriting the victim cross-modal pre-trained encoder. To address the challenges of heterogeneity between different modalities and unknown downstream tasks, we first build a topological graph structure to capture the relevant positions between target samples and their neighbors. Then, we design a topology-deviation based generative adversarial network to generate a universal adversarial patch. By adding the patch to images, we minimize their embeddings similarity to different modality and perturb the sample distribution in the feature space, achieving unviersal non-targeted attacks. Our results demonstrate the excellent attack performance of AdvCLIP on two types of downstream tasks across eight datasets. We also tailor three popular defenses to mitigate AdvCLIP, highlighting the need for new defense mechanisms to defend cross-modal pre-trained encoders.
</details>
<details>
<summary>摘要</summary>
多模态对照学习目标是训练一个通用特征提取器，如CLIP，在大量未标注的图像文本数据上。这可以对多种复杂的下游任务产生很大的改进，包括跨模态图像文本检索和图像分类。尽管其承诺的前景很亮色，但跨模态预训练编码器的安全问题尚未得到完全探索，特别是当预训练编码器是商业用途上公开可用时。在这项工作中，我们提出了AdvCLIP，跨模态预训练编码器的首个攻击框架。AdvCLIP目标是生成基于跨模态预训练编码器的下游不受限制的攻击示例。我们希望通过构建一个图像和文本之间的 topological graph 结构，捕捉目标样本和其相关样本之间的相互关系。然后，我们设计了一种基于 topological deviation 的生成 adversarial network，生成一个通用的攻击质量。通过将质量添加到图像中，我们使得图像与不同模式之间的嵌入度相互不同，并在特征空间中扰乱样本分布，实现了不受限制的非目标攻击。我们的结果显示AdvCLIP在八个数据集上对两种下游任务进行了出色的攻击性能。我们还适应了三种流行的防御机制，强调了防御跨模态预训练编码器的需要。
</details></li>
</ul>
<hr>
<h2 id="PGT-Net-Progressive-Guided-Multi-task-Neural-Network-for-Small-area-Wet-Fingerprint-Denoising-and-Recognition"><a href="#PGT-Net-Progressive-Guided-Multi-task-Neural-Network-for-Small-area-Wet-Fingerprint-Denoising-and-Recognition" class="headerlink" title="PGT-Net: Progressive Guided Multi-task Neural Network for Small-area Wet Fingerprint Denoising and Recognition"></a>PGT-Net: Progressive Guided Multi-task Neural Network for Small-area Wet Fingerprint Denoising and Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07024">http://arxiv.org/abs/2308.07024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Ting Li, Ching-Te Chiu, An-Ting Hsieh, Mao-Hsiu Hsu, Long Wenyong, Jui-Min Hsu</li>
<li>for: 提高手势识别精度（Fingerprint Recognition）</li>
<li>methods: 提出了一种END-TO-END TRAINABLE PROGRESSIVE GUIDED MULTI-TASK NEURAL NETWORK（PGT-Net），包括共享阶段和特定多任务阶段，使网络可以顺序训练binary和非binary手势图像。</li>
<li>results: 实验结果表明，PGT-Net在湿式手势图像干涂除和手势识别精度提高方面具有优秀表现，并在FT-lightnoised和FW9395数据集上降低了手势识别错误率（FRR）。在FT-lightnoised数据集上，FRR从17.75%降低到4.47%；在FW9395数据集上，FRR从9.45%降低到1.09%。<details>
<summary>Abstract</summary>
Fingerprint recognition on mobile devices is an important method for identity verification. However, real fingerprints usually contain sweat and moisture which leads to poor recognition performance. In addition, for rolling out slimmer and thinner phones, technology companies reduce the size of recognition sensors by embedding them with the power button. Therefore, the limited size of fingerprint data also increases the difficulty of recognition. Denoising the small-area wet fingerprint images to clean ones becomes crucial to improve recognition performance. In this paper, we propose an end-to-end trainable progressive guided multi-task neural network (PGT-Net). The PGT-Net includes a shared stage and specific multi-task stages, enabling the network to train binary and non-binary fingerprints sequentially. The binary information is regarded as guidance for output enhancement which is enriched with the ridge and valley details. Moreover, a novel residual scaling mechanism is introduced to stabilize the training process. Experiment results on the FW9395 and FT-lightnoised dataset provided by FocalTech shows that PGT-Net has promising performance on the wet-fingerprint denoising and significantly improves the fingerprint recognition rate (FRR). On the FT-lightnoised dataset, the FRR of fingerprint recognition can be declined from 17.75% to 4.47%. On the FW9395 dataset, the FRR of fingerprint recognition can be declined from 9.45% to 1.09%.
</details>
<details>
<summary>摘要</summary>
Mobile device上的指纹识别是重要的身份验证方法。然而，真正的指纹通常含有汗水和湿度，导致识别性能差。此外，为了推出更薄和更细的手机，技术公司通常减小识别感知器的大小，并将其嵌入在电源按钮中。因此，限制的指纹数据大小也增加了识别的困难。减去小区域湿指纹图像，以便提高识别性能。在这篇论文中，我们提出了一个端到端训练可进程指纹网络（PGT-Net）。PGT-Net包括共享阶段和特定多任务阶段，使得网络可以顺序地训练二进制和非二进制指纹。二进制信息被视为识别输出增强的指导，并且通过ridge和谷峰细节来增强输出。此外，我们还提出了一种新的径规约稳定化机制，以确保训练过程的稳定。实验结果表明，PGT-Net在湿指纹减去和指纹识别率（FRR）上具有良好的表现，在FT-lightnoised数据集上，FRR可以从17.75%降至4.47%。在FW9395数据集上，FRR可以从9.45%降至1.09%。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Bi-Projector-for-Unsupervised-Domain-Adaption"><a href="#Contrastive-Bi-Projector-for-Unsupervised-Domain-Adaption" class="headerlink" title="Contrastive Bi-Projector for Unsupervised Domain Adaption"></a>Contrastive Bi-Projector for Unsupervised Domain Adaption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07017">http://arxiv.org/abs/2308.07017</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tom99763/Contrastive-Bi-Projector-for-Unsupervised-Domain-Adaption">https://github.com/tom99763/Contrastive-Bi-Projector-for-Unsupervised-Domain-Adaption</a></li>
<li>paper_authors: Lin-Chieh Huang, Hung-Hsu Tsai</li>
<li>For: The paper proposes a novel unsupervised domain adaptation (UDA) method called CBPUDA, which improves existing UDA methods by reducing the generation of ambiguous features for classification and domain adaptation.* Methods: The CBPUDA method uses contrastive bi-projectors (CBP) to train feature extractors (FEs) adversarially, obtaining more refined decision boundaries and powerful classification performance. The proposed loss function, contrastive discrepancy (CD) loss, is analyzed for its properties, including an upper bound of joint prediction entropy and a gradient scaling (GS) scheme to overcome instability.* Results: The paper shows that the CBPUDA method is superior to conventional UDA methods for UDA and fine-grained UDA tasks, achieving better performance in classification and domain adaptation.Here is the simplified Chinese text for the three main points:* 用途：本文提出了一种基于对比 би项目（CBP）的新型无监督领域适应（UDA）方法，可以提高现有UDA方法的性能。* 方法：CBPUDA方法使用对比 би项目来训练特征提取器（FEs），通过对抗学习来提取更精细的决策边界，从而获得更高的分类性能。提出了一种对比抽象（CD）损失函数，并分析了其两个性能。* 结果：本文表明，CBPUDA方法比现有的UDA方法在UDA和细化UDA任务中表现更好，达到了更高的分类和领域适应性能。<details>
<summary>Abstract</summary>
This paper proposes a novel unsupervised domain adaption (UDA) method based on contrastive bi-projector (CBP), which can improve the existing UDA methods. It is called CBPUDA here, which effectively promotes the feature extractors (FEs) to reduce the generation of ambiguous features for classification and domain adaption. The CBP differs from traditional bi-classifier-based methods at that these two classifiers are replaced with two projectors of performing a mapping from the input feature to two distinct features. These two projectors and the FEs in the CBPUDA can be trained adversarially to obtain more refined decision boundaries so that it can possess powerful classification performance. Two properties of the proposed loss function are analyzed here. The first property is to derive an upper bound of joint prediction entropy, which is used to form the proposed loss function, contrastive discrepancy (CD) loss. The CD loss takes the advantages of the contrastive learning and the bi-classifier. The second property is to analyze the gradient of the CD loss and then overcome the drawback of the CD loss. The result of the second property is utilized in the development of the gradient scaling (GS) scheme in this paper. The GS scheme can be exploited to tackle the unstable problem of the CD loss because training the CBPUDA requires using contrastive learning and adversarial learning at the same time. Therefore, using the CD loss with the GS scheme overcomes the problem mentioned above to make features more compact for intra-class and distinguishable for inter-class. Experimental results express that the CBPUDA is superior to conventional UDA methods under consideration in this paper for UDA and fine-grained UDA tasks.
</details>
<details>
<summary>摘要</summary>
Two properties of the proposed loss function, contrastive discrepancy (CD) loss, are analyzed:1. The CD loss has an upper bound on joint prediction entropy, which is used to form the loss function.2. The gradient of the CD loss is analyzed, and a gradient scaling (GS) scheme is developed to overcome the drawbacks of the CD loss.The GS scheme is used to tackle the unstable problem of the CD loss, which arises when using contrastive learning and adversarial learning simultaneously. By using the CD loss with the GS scheme, features are made more compact for intra-class and distinguishable for inter-class.Experimental results show that the CBPUDA outperforms conventional UDA methods for UDA and fine-grained UDA tasks.
</details></li>
</ul>
<hr>
<h2 id="HPFormer-Hyperspectral-image-prompt-object-tracking"><a href="#HPFormer-Hyperspectral-image-prompt-object-tracking" class="headerlink" title="HPFormer: Hyperspectral image prompt object tracking"></a>HPFormer: Hyperspectral image prompt object tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07016">http://arxiv.org/abs/2308.07016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuedong Tan</li>
<li>for: 提高视觉跟踪性能</li>
<li>methods: 使用Transformers架构，具有强大表示学习能力，并提出了一种新的卷积束注意力模块（Hyperspectral Hybrid Attention，HHA）和一种选择性地汇集空间细节和谱спектраль特征的变换带模块（Transform Band Module，TBM）。</li>
<li>results: 在NIR和VIS跟踪数据集上实现了状态之最性表现，提供了新的途径来利用变换器和卷积束注意力来提高对象跟踪。<details>
<summary>Abstract</summary>
Hyperspectral imagery contains abundant spectral information beyond the visible RGB bands, providing rich discriminative details about objects in a scene. Leveraging such data has the potential to enhance visual tracking performance. While prior hyperspectral trackers employ CNN or hybrid CNN-Transformer architectures, we propose a novel approach HPFormer on Transformers to capitalize on their powerful representation learning capabilities. The core of HPFormer is a Hyperspectral Hybrid Attention (HHA) module which unifies feature extraction and fusion within one component through token interactions. Additionally, a Transform Band Module (TBM) is introduced to selectively aggregate spatial details and spectral signatures from the full hyperspectral input for injecting informative target representations. Extensive experiments demonstrate state-of-the-art performance of HPFormer on benchmark NIR and VIS tracking datasets. Our work provides new insights into harnessing the strengths of transformers and hyperspectral fusion to advance robust object tracking.
</details>
<details>
<summary>摘要</summary>
《卷积神经网络在多 Spectral 图像中的应用》 introduce a novel approach called HPFormer, which leverages the powerful representation learning capabilities of transformers to enhance visual tracking performance. The core of HPFormer is a Hyperspectral Hybrid Attention (HHA) module, which unifies feature extraction and fusion within one component through token interactions. Additionally, a Transform Band Module (TBM) is introduced to selectively aggregate spatial details and spectral signatures from the full hyperspectral input for injecting informative target representations. Extensive experiments demonstrate state-of-the-art performance of HPFormer on benchmark NIR and VIS tracking datasets. Our work provides new insights into harnessing the strengths of transformers and hyperspectral fusion to advance robust object tracking.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you prefer Traditional Chinese, please let me know and I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="ACTIVE-Towards-Highly-Transferable-3D-Physical-Camouflage-for-Universal-and-Robust-Vehicle-Evasion"><a href="#ACTIVE-Towards-Highly-Transferable-3D-Physical-Camouflage-for-Universal-and-Robust-Vehicle-Evasion" class="headerlink" title="ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion"></a>ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07009">http://arxiv.org/abs/2308.07009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naufal Suryanto, Yongsu Kim, Harashta Tatimma Larasati, Hyoeun Kang, Thi-Thu-Huong Le, Yoonyoung Hong, Hunmin Yang, Se-Yoon Oh, Howon Kim</li>
<li>for: 这个论文旨在攻击物探器，将任何3D车辆覆盖在探器面前。</li>
<li>methods: 这个方法使用了创新的文本渲染技术，可以将通用的文本应用到不同的车辆上，不受特定的 texture map 限制。它还使用了一个新的隐身损失函数，使车辆完全隐藏不见，以及一个缓和遮瑕损失函数，以增强伪装的自然性。</li>
<li>results: 在15种不同的模型上进行了广泛的实验，结果显示了ACTIVE在不同的公共探器上（包括最新的YOLOv7）都能够优于现有的作品。尤其是在其他车辆类别、任务（分类模型）和实际世界中的可转移性测试中，ACTIVE表现了良好的传递性。<details>
<summary>Abstract</summary>
Adversarial camouflage has garnered attention for its ability to attack object detectors from any viewpoint by covering the entire object's surface. However, universality and robustness in existing methods often fall short as the transferability aspect is often overlooked, thus restricting their application only to a specific target with limited performance. To address these challenges, we present Adversarial Camouflage for Transferable and Intensive Vehicle Evasion (ACTIVE), a state-of-the-art physical camouflage attack framework designed to generate universal and robust adversarial camouflage capable of concealing any 3D vehicle from detectors. Our framework incorporates innovative techniques to enhance universality and robustness, including a refined texture rendering that enables common texture application to different vehicles without being constrained to a specific texture map, a novel stealth loss that renders the vehicle undetectable, and a smooth and camouflage loss to enhance the naturalness of the adversarial camouflage. Our extensive experiments on 15 different models show that ACTIVE consistently outperforms existing works on various public detectors, including the latest YOLOv7. Notably, our universality evaluations reveal promising transferability to other vehicle classes, tasks (segmentation models), and the real world, not just other vehicles.
</details>
<details>
<summary>摘要</summary>
adversarial camouflage 引起了关注，因为它可以从任何视角攻击物体探测器，覆盖整个物体表面。然而，现有的方法中的 universality 和 robustness frequently 缺乏，因为它们通常忽略了传输性问题，因此只能应用于特定目标，并且表现有限。为解决这些挑战，我们提出了 Adversarial Camouflage for Transferable and Intensive Vehicle Evasion (ACTIVE)，一个状态 искусственный智能Physical camouflage attack 框架，可以生成universal 和 robust adversarial camouflage，用于隐藏任何 3D 汽车。我们的框架包括创新的技术来提高 universality 和 robustness，包括改进的文本渲染，使得不同汽车可以共享同一个文本映射，以及一种新的隐身损失，使汽车无法探测，以及一种平滑和 camouflage 损失，以提高隐藏的自然性。我们对 15 个不同的模型进行了广泛的实验，显示 ACTIVE  consistently 超越了现有的工作在各种公共探测器上，包括最新的 YOLOv7。尤其是，我们的 universality 评估表明了适用于其他汽车类、任务（分割模型）和实际世界的良好传输性。
</details></li>
</ul>
<hr>
<h2 id="Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks"><a href="#Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks" class="headerlink" title="Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks"></a>Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07003">http://arxiv.org/abs/2308.07003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Fisch, Stefan Zumdick, Carlotta Barkhau, Daniel Emden, Jan Ernsting, Ramona Leenings, Kelvin Sarink, Nils R. Winter, Benjamin Risse, Udo Dannlowski, Tim Hahn</li>
<li>for: 本研究旨在开发一个高精度、高速的brain extractedction工具，用于多种нейро成像预处理管道中的分割步骤。</li>
<li>methods: 本研究使用了一个独特的数据集，包括568个T1-weighted(T1w) MR图像，并使用了当今最先进的深度学习方法来建立一个两阶段预测过程，以提高分割性能。</li>
<li>results:  compared to当前状态的艺术模型（DSC &#x3D; 97.8%和DSC &#x3D; 97.9%），深入的brain extractedction模型在权重平衡分割中实现了新的状态对照性（DSC &#x3D; 99.0%），并在所有样本中保持Dice分数 &gt; 96.9%。此外，该模型可以加速brain extractedction的速度，比现有方法快约10倍，可以在低级别硬件上处理一个图像只需2秒钟。<details>
<summary>Abstract</summary>
Brain extraction in magnetic resonance imaging (MRI) data is an important segmentation step in many neuroimaging preprocessing pipelines. Image segmentation is one of the research fields in which deep learning had the biggest impact in recent years enabling high precision segmentation with minimal compute. Consequently, traditional brain extraction methods are now being replaced by deep learning-based methods. Here, we used a unique dataset comprising 568 T1-weighted (T1w) MR images from 191 different studies in combination with cutting edge deep learning methods to build a fast, high-precision brain extraction tool called deepbet. deepbet uses LinkNet, a modern UNet architecture, in a two stage prediction process. This increases its segmentation performance, setting a novel state-of-the-art performance during cross-validation with a median Dice score (DSC) of 99.0% on unseen datasets, outperforming current state of the art models (DSC = 97.8% and DSC = 97.9%). While current methods are more sensitive to outliers, resulting in Dice scores as low as 76.5%, deepbet manages to achieve a Dice score of > 96.9% for all samples. Finally, our model accelerates brain extraction by a factor of ~10 compared to current methods, enabling the processing of one image in ~2 seconds on low level hardware.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging (MRI) 数据中的脑部提取是许多神经成像预处理管道中的重要分 Segmentation step。图像分 segmentation 是深度学习在过去几年中对神经成像领域的研究中所带来的最大影响，使得传统的脑部提取方法被取代了。我们使用了568个T1-weighted (T1w) MR 图像和最前沿的深度学习方法，建立了一个高速、高精度的脑部提取工具 called deepbet。deepbet 使用了LinkNet，一种现代的UNet架构，在两个阶段的预测过程中。这使得它的 segmentation 性能得到了提高，在批处理中 median Dice 分数 (DSC) 为99.0%，超越了当前的状态码模型 (DSC = 97.8%和DSC = 97.9%)。而当前的方法更感应外围数据，导致 Dice 分数只有76.5%，而 deepbet 则可以达到 > 96.9% 的 Dice 分数 для所有样本。最后，我们的模型将脑部提取加速了约10倍，可以在低级别硬件上处理一个图像只需2秒钟。
</details></li>
</ul>
<hr>
<h2 id="Mutual-Information-driven-Triple-Interaction-Network-for-Efficient-Image-Dehazing"><a href="#Mutual-Information-driven-Triple-Interaction-Network-for-Efficient-Image-Dehazing" class="headerlink" title="Mutual Information-driven Triple Interaction Network for Efficient Image Dehazing"></a>Mutual Information-driven Triple Interaction Network for Efficient Image Dehazing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06998">http://arxiv.org/abs/2308.06998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/it-hao/mitnet">https://github.com/it-hao/mitnet</a></li>
<li>paper_authors: Hao Shen, Zhong-Qiu Zhao, Yulun Zhang, Zhao Zhang</li>
<li>for: 这个论文主要针对图像降雨问题进行解决，通过分解为多个更加 tractable 子任务，逐步估计降雨后的图像。</li>
<li>methods: 该论文提出了一种基于空间频域双域信息和两Stage Architecture的新方法，即MITNet，它利用了振荡 Spectrum 的恢复、phas spectrum 的学习和 Adaptive Triple Interaction Module (ATIM) 来提高图像降雨的性能。</li>
<li>results: 对多个公共数据集进行了广泛的实验，表明MITNet可以在低于同类模型的复杂性下达到更高的性能水平。<details>
<summary>Abstract</summary>
Multi-stage architectures have exhibited efficacy in image dehazing, which usually decomposes a challenging task into multiple more tractable sub-tasks and progressively estimates latent hazy-free images. Despite the remarkable progress, existing methods still suffer from the following shortcomings: (1) limited exploration of frequency domain information; (2) insufficient information interaction; (3) severe feature redundancy. To remedy these issues, we propose a novel Mutual Information-driven Triple interaction Network (MITNet) based on spatial-frequency dual domain information and two-stage architecture. To be specific, the first stage, named amplitude-guided haze removal, aims to recover the amplitude spectrum of the hazy images for haze removal. And the second stage, named phase-guided structure refined, devotes to learning the transformation and refinement of the phase spectrum. To facilitate the information exchange between two stages, an Adaptive Triple Interaction Module (ATIM) is developed to simultaneously aggregate cross-domain, cross-scale, and cross-stage features, where the fused features are further used to generate content-adaptive dynamic filters so that applying them to enhance global context representation. In addition, we impose the mutual information minimization constraint on paired scale encoder and decoder features from both stages. Such an operation can effectively reduce information redundancy and enhance cross-stage feature complementarity. Extensive experiments on multiple public datasets exhibit that our MITNet performs superior performance with lower model complexity.The code and models are available at https://github.com/it-hao/MITNet.
</details>
<details>
<summary>摘要</summary>
多Stage网络在图像抑霜方面表现了效果，通常将复杂的任务分解成多个更容易处理的子任务，并逐步估计灰度图像的干扰。 despite the remarkable progress, existing methods still have the following shortcomings: (1) limited exploration of frequency domain information; (2) insufficient information interaction; (3) severe feature redundancy. To address these issues, we propose a novel Mutual Information-driven Triple interaction Network (MITNet) based on spatial-frequency dual domain information and two-stage architecture. Specifically, the first stage, named amplitude-guided haze removal, aims to recover the amplitude spectrum of hazy images for haze removal. And the second stage, named phase-guided structure refined, devotes to learning the transformation and refinement of the phase spectrum. To facilitate the information exchange between two stages, an Adaptive Triple Interaction Module (ATIM) is developed to simultaneously aggregate cross-domain, cross-scale, and cross-stage features, where the fused features are further used to generate content-adaptive dynamic filters for enhancing global context representation. In addition, we impose the mutual information minimization constraint on paired scale encoder and decoder features from both stages. This operation can effectively reduce information redundancy and enhance cross-stage feature complementarity. Extensive experiments on multiple public datasets show that our MITNet achieves superior performance with lower model complexity. The code and models are available at https://github.com/it-hao/MITNet.
</details></li>
</ul>
<hr>
<h2 id="PatchContrast-Self-Supervised-Pre-training-for-3D-Object-Detection"><a href="#PatchContrast-Self-Supervised-Pre-training-for-3D-Object-Detection" class="headerlink" title="PatchContrast: Self-Supervised Pre-training for 3D Object Detection"></a>PatchContrast: Self-Supervised Pre-training for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06985">http://arxiv.org/abs/2308.06985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oren Shrout, Ori Nitzan, Yizhak Ben-Shabat, Ayellet Tal</li>
<li>for: 自动驾驶车辆环境中物体检测的准确检测是一项关键挑战。然而，获得用于检测的标注数据是昂贵的和耗时的。我们介绍了PatchContrast，一种新的自我超vised点云预训练框架，用于提高3D物体检测的性能。</li>
<li>methods: 我们提出了两级含义来学习不supervised数据中的抽象表示：提案级别和 patch级别。提案级别寻找物体在它所处的环境中的坐标，而 patch级别添加了物体组件之间的内部连接信息，因此可以通过物体的个体组件来分辨不同的物体。我们示示了如何将这两级含义集成到不同的backbone中进行自我超vised预训练，以提高下游3D检测任务的性能。</li>
<li>results: 我们的方法比现有的状态先进模型在三个常用的3D检测数据集上表现出色，提高了3D检测任务的性能。<details>
<summary>Abstract</summary>
Accurately detecting objects in the environment is a key challenge for autonomous vehicles. However, obtaining annotated data for detection is expensive and time-consuming. We introduce PatchContrast, a novel self-supervised point cloud pre-training framework for 3D object detection. We propose to utilize two levels of abstraction to learn discriminative representation from unlabeled data: proposal-level and patch-level. The proposal-level aims at localizing objects in relation to their surroundings, whereas the patch-level adds information about the internal connections between the object's components, hence distinguishing between different objects based on their individual components. We demonstrate how these levels can be integrated into self-supervised pre-training for various backbones to enhance the downstream 3D detection task. We show that our method outperforms existing state-of-the-art models on three commonly-used 3D detection datasets.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆环境中物体检测是一项关键挑战。然而，获取标注数据 для检测却是成本高且时间费时的。我们介绍了PatchContrast，一种新的自我超vised点云预训练框架 для3D物体检测。我们提议利用两级层次抽象来学习不同数据集中的抽象表示：提案级别和 patch级别。提案级别将物体本身和其周围环境进行Localizaion，而patch级别则提供了对物体组件之间的内部连接信息，从而通过不同物体的组件来分辨不同物体。我们示出了如何将这两级层次级别integreinto自我超vised预训练中，以提高下游3D检测任务的性能。我们证明了我们的方法可以与现有状态的最佳模型在三个常用的3D检测数据集上进行比较，并且表现出较好的性能。
</details></li>
</ul>
<hr>
<h2 id="A-One-Stop-3D-Target-Reconstruction-and-multilevel-Segmentation-Method"><a href="#A-One-Stop-3D-Target-Reconstruction-and-multilevel-Segmentation-Method" class="headerlink" title="A One Stop 3D Target Reconstruction and multilevel Segmentation Method"></a>A One Stop 3D Target Reconstruction and multilevel Segmentation Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06974">http://arxiv.org/abs/2308.06974</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ganlab/ostra">https://github.com/ganlab/ostra</a></li>
<li>paper_authors: Jiexiong Xu, Weikun Zhao, Zhiyan Tang, Xiangchao Gan</li>
<li>for: 本研究旨在提出一个开源的三维目标重建和多层分类框架（OSTRA），用于实现图像序列中多个实体的分类、追踪和三维重建。</li>
<li>methods: OSTRA使用多视角ステレオ（MVS）或RGBD基于的三维重建方法进行三维物体重建，并将二维图像中的分类扩展至三维空间，以支持连续的分类标签。</li>
<li>results: OSTRA在多个三维数据集上实现高性能的Semantic Segmentation、Instance Segmentation和Part Segmentation，甚至超越人工分类在复杂的场景和遮掩中。<details>
<summary>Abstract</summary>
3D object reconstruction and multilevel segmentation are fundamental to computer vision research. Existing algorithms usually perform 3D scene reconstruction and target objects segmentation independently, and the performance is not fully guaranteed due to the challenge of the 3D segmentation. Here we propose an open-source one stop 3D target reconstruction and multilevel segmentation framework (OSTRA), which performs segmentation on 2D images, tracks multiple instances with segmentation labels in the image sequence, and then reconstructs labelled 3D objects or multiple parts with Multi-View Stereo (MVS) or RGBD-based 3D reconstruction methods. We extend object tracking and 3D reconstruction algorithms to support continuous segmentation labels to leverage the advances in the 2D image segmentation, especially the Segment-Anything Model (SAM) which uses the pretrained neural network without additional training for new scenes, for 3D object segmentation. OSTRA supports most popular 3D object models including point cloud, mesh and voxel, and achieves high performance for semantic segmentation, instance segmentation and part segmentation on several 3D datasets. It even surpasses the manual segmentation in scenes with complex structures and occlusions. Our method opens up a new avenue for reconstructing 3D targets embedded with rich multi-scale segmentation information in complex scenes. OSTRA is available from https://github.com/ganlab/OSTRA.
</details>
<details>
<summary>摘要</summary>
三Dimensional对象重建和多级分割是计算机视觉研究的基础。现有算法通常在独立地进行三Dimensional场景重建和目标对象分割，并且性能不能够保证由三Dimensional分割的挑战。我们提出了一个开源的一站式三Dimensional目标重建和多级分割框架（OSTRA），它在图像序列中进行分割，跟踪图像序列中的多个实例，并使用多视图镜像（MVS）或RGBD基于的三Dimensional重建方法来重建标签的三Dimensional对象或多部分。我们扩展了对象跟踪和三Dimensional重建算法，以支持连续的分割标签，以便利用二Dimensional图像分割的进步，特别是Segment-Anything Model（SAM），它使用预训练的神经网络，不需要额外训练，对于新场景进行三Dimensional对象分割。OSTRA支持大多数三Dimensional对象模型，包括点云、网格和体ixel，并在多个三Dimensional数据集上实现高性能的semantic分割、实例分割和部分分割。甚至超过了人工分割在复杂结构和遮挡的场景中。我们的方法开启了一新的途径，将三Dimensional目标嵌入了rich multi-scale分割信息的复杂场景中进行重建。OSTRA可以从https://github.com/ganlab/OSTRA获取。
</details></li>
</ul>
<hr>
<h2 id="How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation"><a href="#How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation" class="headerlink" title="How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation"></a>How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06964">http://arxiv.org/abs/2308.06964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parinaz Roshanzamir, Hassan Rivaz, Joshua Ahn, Hamza Mirza, Neda Naghdi, Meagan Anstruther, Michele C. Battié, Maryse Fortin, Yiming Xiao</li>
<li>for: 这篇论文旨在探讨深度学习（DL）技术在医疗影像分类任务中的表现，特别是最新的Transformer模型和其变体。</li>
<li>methods: 本文使用test-time augmentation（TTA）、test-time dropout（TTD）和深度结构来量化 aleatoric和epistemic uncertainty，并评估它们与多注解者间的不确定性之间的关系。此外，本文比较了UNet和TransUNet，以研究Transformers对模型不确定性的影响，并评估了两种标签融合策略。</li>
<li>results: 本研究发现了多注解者间的不确定性和模型不确定性之间的交互关系，受到标签融合策略和DL模型的选择的影响。<details>
<summary>Abstract</summary>
Recent developments in deep learning (DL) techniques have led to great performance improvement in medical image segmentation tasks, especially with the latest Transformer model and its variants. While labels from fusing multi-rater manual segmentations are often employed as ideal ground truths in DL model training, inter-rater variability due to factors such as training bias, image noise, and extreme anatomical variability can still affect the performance and uncertainty of the resulting algorithms. Knowledge regarding how inter-rater variability affects the reliability of the resulting DL algorithms, a key element in clinical deployment, can help inform better training data construction and DL models, but has not been explored extensively. In this paper, we measure aleatoric and epistemic uncertainties using test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to explore their relationship with inter-rater variability. Furthermore, we compare UNet and TransUNet to study the impacts of Transformers on model uncertainty with two label fusion strategies. We conduct a case study using multi-class paraspinal muscle segmentation from T2w MRIs. Our study reveals the interplay between inter-rater variability and uncertainties, affected by choices of label fusion strategies and DL models.
</details>
<details>
<summary>摘要</summary>
In this paper, we use test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to measure aleatoric and epistemic uncertainties and explore their relationship with inter-rater variability. Additionally, we compare UNet and TransUNet to study the impact of Transformers on model uncertainty with two label fusion strategies. We conduct a case study using multi-class paraspinal muscle segmentation from T2w MRIs. Our study reveals the interplay between inter-rater variability and uncertainties, which is affected by choices of label fusion strategies and DL models.
</details></li>
</ul>
<hr>
<h2 id="Color-NeuS-Reconstructing-Neural-Implicit-Surfaces-with-Color"><a href="#Color-NeuS-Reconstructing-Neural-Implicit-Surfaces-with-Color" class="headerlink" title="Color-NeuS: Reconstructing Neural Implicit Surfaces with Color"></a>Color-NeuS: Reconstructing Neural Implicit Surfaces with Color</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06962">http://arxiv.org/abs/2308.06962</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Colmar-zlicheng/Color-NeuS">https://github.com/Colmar-zlicheng/Color-NeuS</a></li>
<li>paper_authors: Licheng Zhong, Lixin Yang, Kailin Li, Haoyu Zhen, Mei Han, Cewu Lu</li>
<li>for: 本研究旨在重新定义物体表面从多视图图像或单视频图像中，并且同时恢复颜色。</li>
<li>methods: 我们使用了一个颜色推导网络来除掉视角依赖的颜色，并且透过一个重新推导网络来保持颜色推导性能。 mesh 则是从 signed distance function（SDF）网络中提取出来的。</li>
<li>results: 我们在一个实际的手持物体扫描任务中评估了我们的方法，结果比任何可以同时恢复 mesh 和颜色的方法更好。 更进一步地，我们将方法评估在公共数据集上，包括 DTU、BlendedMVS 和 OmniObject3D 等数据集，结果显示我们的方法在这些数据集上表现良好。<details>
<summary>Abstract</summary>
The reconstruction of object surfaces from multi-view images or monocular video is a fundamental issue in computer vision. However, much of the recent research concentrates on reconstructing geometry through implicit or explicit methods. In this paper, we shift our focus towards reconstructing mesh in conjunction with color. We remove the view-dependent color from neural volume rendering while retaining volume rendering performance through a relighting network. Mesh is extracted from the signed distance function (SDF) network for the surface, and color for each surface vertex is drawn from the global color network. To evaluate our approach, we conceived a in hand object scanning task featuring numerous occlusions and dramatic shifts in lighting conditions. We've gathered several videos for this task, and the results surpass those of any existing methods capable of reconstructing mesh alongside color. Additionally, our method's performance was assessed using public datasets, including DTU, BlendedMVS, and OmniObject3D. The results indicated that our method performs well across all these datasets. Project page: https://colmar-zlicheng.github.io/color_neus.
</details>
<details>
<summary>摘要</summary>
“重建物体表面从多视图图像或单视图视频是计算机视觉中的基本问题。然而，现有的大部分研究强调 геометрическое重建方法，而我们在这篇论文中强调重建网格，并同时保留了颜色渲染性能。我们在神经网络中除掉了视角依赖的颜色，并将网格提取自 signed distance function（SDF）网络。每个表面Vertex的颜色从全局颜色网络中绘制。为评估我们的方法，我们设计了一个手持物体扫描任务，该任务具有各种遮挡和极大的照明变化。我们收集了许多视频数据，并结果超过任何可以同时重建网格和颜色的方法。此外，我们的方法在公共数据集上进行评估，包括 DTU、BlendedMVS 和 OmniObject3D，结果表明我们的方法在这些数据集上表现良好。项目页面：https://colmar-zlicheng.github.io/color_neus。”Note that Simplified Chinese is used here, as it is the more widely used standard for Chinese writing. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="CEmb-SAM-Segment-Anything-Model-with-Condition-Embedding-for-Joint-Learning-from-Heterogeneous-Datasets"><a href="#CEmb-SAM-Segment-Anything-Model-with-Condition-Embedding-for-Joint-Learning-from-Heterogeneous-Datasets" class="headerlink" title="CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets"></a>CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06957">http://arxiv.org/abs/2308.06957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongik Shin, Beomsuk Kim, Seungjun Baek</li>
<li>for:  assist medical experts with diagnostic and therapeutic procedures</li>
<li>methods:  jointly learning from heterogeneous datasets, using Segment Anything model (SAM) with Condition Embedding block (CEmb-SAM)</li>
<li>results:  outperforms baseline methods on ultrasound image segmentation for peripheral nerves and breast cancer<details>
<summary>Abstract</summary>
Automated segmentation of ultrasound images can assist medical experts with diagnostic and therapeutic procedures. Although using the common modality of ultrasound, one typically needs separate datasets in order to segment, for example, different anatomical structures or lesions with different levels of malignancy. In this paper, we consider the problem of jointly learning from heterogeneous datasets so that the model can improve generalization abilities by leveraging the inherent variability among datasets. We merge the heterogeneous datasets into one dataset and refer to each component dataset as a subgroup. We propose to train a single segmentation model so that the model can adapt to each sub-group. For robust segmentation, we leverage recently proposed Segment Anything model (SAM) in order to incorporate sub-group information into the model. We propose SAM with Condition Embedding block (CEmb-SAM) which encodes sub-group conditions and combines them with image embeddings from SAM. The conditional embedding block effectively adapts SAM to each image sub-group by incorporating dataset properties through learnable parameters for normalization. Experiments show that CEmb-SAM outperforms the baseline methods on ultrasound image segmentation for peripheral nerves and breast cancer. The experiments highlight the effectiveness of Cemb-SAM in learning from heterogeneous datasets in medical image segmentation tasks.
</details>
<details>
<summary>摘要</summary>
自动分割超声图像可以帮助医疗专业人员进行诊断和治疗过程。尽管使用常见的探测器模式，但一般需要分立的数据集以便分别对各种解剖结构或恶性肿瘤进行分割。在这篇论文中，我们考虑了将异构数据集合并学习的问题，以便模型可以通过利用异构数据集的自然变化提高泛化能力。我们将异构数据集合并成一个数据集，并将每个组件数据集称为子组。我们提议在单个分割模型中培训多个分割模型，以便模型可以适应每个子组。为了robust分割，我们利用最近提出的Segment Anything模型（SAM），并在SAM中添加条件嵌入块（CEmb），以编码子组条件并与SAM中的图像嵌入相结合。 conditional embedding block可以有效地适应SAM每个图像子组的变化，通过学习参数进行Normalization。实验表明CEmb-SAM在超声图像分割任务中表现出色，超过了基eline方法。实验还表明，CEmb-SAM在医疗图像分割任务中学习异构数据集的能力非常有用。
</details></li>
</ul>
<hr>
<h2 id="Global-Features-are-All-You-Need-for-Image-Retrieval-and-Reranking"><a href="#Global-Features-are-All-You-Need-for-Image-Retrieval-and-Reranking" class="headerlink" title="Global Features are All You Need for Image Retrieval and Reranking"></a>Global Features are All You Need for Image Retrieval and Reranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06954">http://arxiv.org/abs/2308.06954</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shihaoshao-gh/superglobal">https://github.com/shihaoshao-gh/superglobal</a></li>
<li>paper_authors: Shihao Shao, Kaifeng Chen, Arjun Karpur, Qinghua Cui, Andre Araujo, Bingyi Cao</li>
<li>for: 本研究旨在提高图像检索系统的效率和准确率，并提供一个可扩展的高性能图像检索系统。</li>
<li>methods: 本paper使用全球特征来进行初始化和重新排序，并提出了一些新的模块来改进全球特征提取和重新排序过程。</li>
<li>results: 对比于现有的图像检索系统，本paper的实验结果表明，SuperGlobal可以提供substantial improvement，特别是在Revisited Oxford+1M Hard数据集上，单阶段结果提高7.1%，两阶段结果提高3.7%，并且具有64,865x的速度提升。<details>
<summary>Abstract</summary>
Image retrieval systems conventionally use a two-stage paradigm, leveraging global features for initial retrieval and local features for reranking. However, the scalability of this method is often limited due to the significant storage and computation cost incurred by local feature matching in the reranking stage. In this paper, we present SuperGlobal, a novel approach that exclusively employs global features for both stages, improving efficiency without sacrificing accuracy. SuperGlobal introduces key enhancements to the retrieval system, specifically focusing on the global feature extraction and reranking processes. For extraction, we identify sub-optimal performance when the widely-used ArcFace loss and Generalized Mean (GeM) pooling methods are combined and propose several new modules to improve GeM pooling. In the reranking stage, we introduce a novel method to update the global features of the query and top-ranked images by only considering feature refinement with a small set of images, thus being very compute and memory efficient. Our experiments demonstrate substantial improvements compared to the state of the art in standard benchmarks. Notably, on the Revisited Oxford+1M Hard dataset, our single-stage results improve by 7.1%, while our two-stage gain reaches 3.7% with a strong 64,865x speedup. Our two-stage system surpasses the current single-stage state-of-the-art by 16.3%, offering a scalable, accurate alternative for high-performing image retrieval systems with minimal time overhead. Code: https://github.com/ShihaoShao-GH/SuperGlobal.
</details>
<details>
<summary>摘要</summary>
传统的图像检索系统采用两个阶段方法，首先使用全球特征进行初始检索，然后使用本地特征进行重新排序。然而，这种方法的扩展性往往受到本地特征匹配的存储和计算成本的限制。在这篇论文中，我们提出了SuperGlobal方法，它凭借全球特征进行两个阶段的检索，提高效率而无需牺牲准确性。SuperGlobal方法在检索系统中进行了重要改进，具体包括全球特征提取和重新排序过程中的增强。在提取阶段，我们发现在广泛使用的ArcFace损失和Generalized Mean（GeM）混合方法时存在下限性，并提出了一些新的模块来改进GeM混合方法。在重新排序阶段，我们引入了一种新的方法，只考虑一小 subsets of images来更新全球特征，从而具有很高的计算和存储效率。我们的实验表明，SuperGlobal方法在标准的测试集上具有显著的改进，特别是在Revisited Oxford+1M Hard dataset上，我们的单阶段结果提高了7.1%，而我们的两阶段结果提高了3.7%，同时具有64,865倍的速度提升。我们的两阶段系统超过了当前单阶段状态之差16.3%，提供了可扩展、准确的图像检索系统选择。代码可在<https://github.com/ShihaoShao-GH/SuperGlobal>中找到。
</details></li>
</ul>
<hr>
<h2 id="Channel-Wise-Contrastive-Learning-for-Learning-with-Noisy-Labels"><a href="#Channel-Wise-Contrastive-Learning-for-Learning-with-Noisy-Labels" class="headerlink" title="Channel-Wise Contrastive Learning for Learning with Noisy Labels"></a>Channel-Wise Contrastive Learning for Learning with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06952">http://arxiv.org/abs/2308.06952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Kang, Sheng Liu, Huaxi Huang, Tongliang Liu</li>
<li>for: 本研究旨在Addressing the challenge of learning with noisy labels (LNL) by introducing channel-wise contrastive learning (CWCL) to distinguish authentic label information from noise.</li>
<li>methods: 该方法通过在多个通道进行对比学习，以提取准确标签信息，并逐步细化使用这些样本进行进一步精度调整。</li>
<li>results: 对多个 benchmark 数据集进行评估，显示该方法与现有方法相比，具有更高的精度和更好的鲁棒性。<details>
<summary>Abstract</summary>
In real-world datasets, noisy labels are pervasive. The challenge of learning with noisy labels (LNL) is to train a classifier that discerns the actual classes from given instances. For this, the model must identify features indicative of the authentic labels. While research indicates that genuine label information is embedded in the learned features of even inaccurately labeled data, it's often intertwined with noise, complicating its direct application. Addressing this, we introduce channel-wise contrastive learning (CWCL). This method distinguishes authentic label information from noise by undertaking contrastive learning across diverse channels. Unlike conventional instance-wise contrastive learning (IWCL), CWCL tends to yield more nuanced and resilient features aligned with the authentic labels. Our strategy is twofold: firstly, using CWCL to extract pertinent features to identify cleanly labeled samples, and secondly, progressively fine-tuning using these samples. Evaluations on several benchmark datasets validate our method's superiority over existing approaches.
</details>
<details>
<summary>摘要</summary>
实际数据集中，噪声标签是普遍存在的。学习噪声标签（LNL）的挑战是训练一个能够识别实际类别的分类器。为此，模型必须识别实际标签中的特征。虽然研究表明，正确的标签信息在噪声涂抹后仍然包含在学习的数据中，但它通常与噪声混合在一起，使其直接应用更加复杂。为解决这个问题，我们介绍了通道 wise contrastive learning（CWCL）。这种方法通过对多个通道进行对比来分离真实标签信息和噪声。与传统的实例 wise contrastive learning（IWCL）不同，CWCL更有可能产生更加细腻和抗噪声的特征，与真实标签更好地align。我们的策略是两重的：首先，使用 CWCL 提取有关实际标签的重要特征，并在这些样本上进行逐渐细化。其次，通过这些样本进行进一步细化。我们在多个 benchmark 数据集上进行了评估，并证明了我们的方法在现有方法之上具有superiority。
</details></li>
</ul>
<hr>
<h2 id="MixBCT-Towards-Self-Adapting-Backward-Compatible-Training"><a href="#MixBCT-Towards-Self-Adapting-Backward-Compatible-Training" class="headerlink" title="MixBCT: Towards Self-Adapting Backward-Compatible Training"></a>MixBCT: Towards Self-Adapting Backward-Compatible Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06948">http://arxiv.org/abs/2308.06948</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuleung/mixbct">https://github.com/yuleung/mixbct</a></li>
<li>paper_authors: Yu Liang, Shiliang Zhang, Yaowei Wang, Sheng Xiao, Kenli Li, Xiaoyu Wang</li>
<li>for: 提高图像检索系统的效果，适用于具有不同质量的老模型。</li>
<li>methods: 提出了一种简单 yet高效的倒向兼容训练方法（MixBCT），通过约束新特征的分布来保证兼容性。</li>
<li>results: 在大规模的人脸识别数据集MS1Mv3和IJB-C上，与前一代方法相比，实验结果显示MixBCT具有明显的优势。<details>
<summary>Abstract</summary>
The exponential growth of data, alongside advancements in model structures and loss functions, has necessitated the enhancement of image retrieval systems through the utilization of new models with superior feature embeddings. However, the expensive process of updating the old retrieval database by replacing embeddings poses a challenge. As a solution, backward-compatible training can be employed to avoid the necessity of updating old retrieval datasets. While previous methods achieved backward compatibility by aligning prototypes of the old model, they often overlooked the distribution of the old features, thus limiting their effectiveness when the old model's low quality leads to a weakly discriminative feature distribution. On the other hand, instance-based methods like L2 regression take into account the distribution of old features but impose strong constraints on the performance of the new model itself. In this paper, we propose MixBCT, a simple yet highly effective backward-compatible training method that serves as a unified framework for old models of varying qualities. Specifically, we summarize four constraints that are essential for ensuring backward compatibility in an ideal scenario, and we construct a single loss function to facilitate backward-compatible training. Our approach adaptively adjusts the constraint domain for new features based on the distribution of the old embeddings. We conducted extensive experiments on the large-scale face recognition datasets MS1Mv3 and IJB-C to verify the effectiveness of our method. The experimental results clearly demonstrate its superiority over previous methods. Code is available at https://github.com/yuleung/MixBCT
</details>
<details>
<summary>摘要</summary>
“数据的激素增长以及模型结构和损失函数的进步，使得图像检索系统需要通过使用新的模型来提高特征嵌入。然而，更新老检索数据库的过程是昂贵的，这pose了一个挑战。为解决这个问题，我们可以使用回溯相容的训练方法，以避免更新老检索数据库。在过去的方法中，通常通过对老模型的批处理来实现回溯相容性，但这经常忽视了老特征分布，从而限制了它们的效果。而instance-based方法，如L2回归，则考虑了老特征分布，但是它们对新模型的性能带来很强的限制。在这篇论文中，我们提出了一种简单 yet高效的回溯相容训练方法，即MixBCT。具体来说，我们总结了回溯相容训练中的四个关键约束，并构建了一个单一的损失函数来促进回溯相容训练。我们的方法可以适应不同质量的老模型，并可以自动调整新特征的约束领域，以适应老特征分布。我们在大规模的人脸识别数据集MS1Mv3和IJB-C上进行了广泛的实验，结果明显超过了先前的方法。代码可以在https://github.com/yuleung/MixBCT中找到。”
</details></li>
</ul>
<hr>
<h2 id="Knowing-Where-to-Focus-Event-aware-Transformer-for-Video-Grounding"><a href="#Knowing-Where-to-Focus-Event-aware-Transformer-for-Video-Grounding" class="headerlink" title="Knowing Where to Focus: Event-aware Transformer for Video Grounding"></a>Knowing Where to Focus: Event-aware Transformer for Video Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06947">http://arxiv.org/abs/2308.06947</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhyunj/eatr">https://github.com/jinhyunj/eatr</a></li>
<li>paper_authors: Jinhyun Jang, Jungin Park, Jin Kim, Hyeongjun Kwon, Kwanghoon Sohn</li>
<li>for: 本 paper 的目的是提出一种基于 DETR 的视频基础设施模型，以便在视频中提取事件和时间信息。</li>
<li>methods: 本 paper 使用了一种名为Event-Aware Dynamic Moment Query的方法，该方法通过构成具有具体事件单元的构成的视频的具体事件单元，并通过将这些事件单元与视频中的句子表示相互作用，以便更好地预测视频中的时间信息。</li>
<li>results: 在多个视频基础设施测试benchmark上，Event-Aware Dynamic Moment Query 方法表现出色，比前一代方法更高效和精度地预测视频中的时间信息。<details>
<summary>Abstract</summary>
Recent DETR-based video grounding models have made the model directly predict moment timestamps without any hand-crafted components, such as a pre-defined proposal or non-maximum suppression, by learning moment queries. However, their input-agnostic moment queries inevitably overlook an intrinsic temporal structure of a video, providing limited positional information. In this paper, we formulate an event-aware dynamic moment query to enable the model to take the input-specific content and positional information of the video into account. To this end, we present two levels of reasoning: 1) Event reasoning that captures distinctive event units constituting a given video using a slot attention mechanism; and 2) moment reasoning that fuses the moment queries with a given sentence through a gated fusion transformer layer and learns interactions between the moment queries and video-sentence representations to predict moment timestamps. Extensive experiments demonstrate the effectiveness and efficiency of the event-aware dynamic moment queries, outperforming state-of-the-art approaches on several video grounding benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Event reasoning that captures distinctive event units constituting a given video using a slot attention mechanism;2. Moment reasoning that fuses the moment queries with a given sentence through a gated fusion transformer layer and learns interactions between the moment queries and video-sentence representations to predict moment timestamps.Extensive experiments demonstrate the effectiveness and efficiency of the event-aware dynamic moment queries, outperforming state-of-the-art approaches on several video grounding benchmarks.</details></li>
</ol>
<hr>
<h2 id="Semantic-aware-Network-for-Aerial-to-Ground-Image-Synthesis"><a href="#Semantic-aware-Network-for-Aerial-to-Ground-Image-Synthesis" class="headerlink" title="Semantic-aware Network for Aerial-to-Ground Image Synthesis"></a>Semantic-aware Network for Aerial-to-Ground Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06945">http://arxiv.org/abs/2308.06945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhyunj/sanet">https://github.com/jinhyunj/sanet</a></li>
<li>paper_authors: Jinhyun Jang, Taeyong Song, Kwanghoon Sohn</li>
<li>for: 这篇论文旨在解决从空中图像到地面图像的合成问题，这是一个复杂且挑战性较高的问题。</li>
<li>methods: 该论文提出了一种新的框架，通过强化结构对采取和semantic意识来解决这个问题。具体来说，该框架包括一个新的semantic-attentive特征转换模块，该模块可以将空中特征对应到地面布局中的复杂结构。此外，该论文还提出了semantic意识损失函数，通过利用预训练的分割网络来强制网络Synthesize实际的物体 across多个类型，并对不同类型的物体进行分别计算损失并均衡。</li>
<li>results: 对比之前的方法和简化学习，该论文的方法实现了较高的效果， both qualitatively and quantitatively。<details>
<summary>Abstract</summary>
Aerial-to-ground image synthesis is an emerging and challenging problem that aims to synthesize a ground image from an aerial image. Due to the highly different layout and object representation between the aerial and ground images, existing approaches usually fail to transfer the components of the aerial scene into the ground scene. In this paper, we propose a novel framework to explore the challenges by imposing enhanced structural alignment and semantic awareness. We introduce a novel semantic-attentive feature transformation module that allows to reconstruct the complex geographic structures by aligning the aerial feature to the ground layout. Furthermore, we propose semantic-aware loss functions by leveraging a pre-trained segmentation network. The network is enforced to synthesize realistic objects across various classes by separately calculating losses for different classes and balancing them. Extensive experiments including comparisons with previous methods and ablation studies show the effectiveness of the proposed framework both qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary>
“空中到地面图像合成是一个emerging和挑战性的问题，旨在将地面图像从空中图像合成。由于空中和地面图像之间的元素布局和对象表示异常不同，现有的方法通常无法将空中场景中的元素传输到地面场景中。在这篇论文中，我们提出了一个新的框架，以便探讨这些挑战。我们引入了一个新的semantic-attentive特征转换模块，该模块可以对空中特征进行对地面布局的对接，以重建复杂的地理结构。此外，我们提出了semantic-aware的损失函数，通过利用预训练的分割网络，使网络在不同类型的对象中Synthesize realistic object。我们进行了广泛的实验，包括与之前的方法进行比较和简要的拆分学习，以证明我们的框架的效果。”Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="One-shot-lip-based-biometric-authentication-extending-behavioral-features-with-authentication-phrase-information"><a href="#One-shot-lip-based-biometric-authentication-extending-behavioral-features-with-authentication-phrase-information" class="headerlink" title="One-shot lip-based biometric authentication: extending behavioral features with authentication phrase information"></a>One-shot lip-based biometric authentication: extending behavioral features with authentication phrase information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06944">http://arxiv.org/abs/2308.06944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brando Koch, Ratko Grbić</li>
<li>For: 这篇论文旨在提出一种基于语音动作的生物特征认证方法，以便在视频数据中提取物理和行为特征，并通过一架深度图像网络和循环神经网络来训练。* Methods: 该方法使用一架深度图像网络，包括3D卷积和循环神经网络层，以提取物理和行为特征。另外，一种自定义的 triplet 损失函数也是提出来的，以进行批处理硬negative mining。* Results: 根据在自定义GRID数据集上进行的测试，该方法可以实现3.2% FAR和3.8% FRR的性能。此外，研究还进行了针对不同特征的分析，以评估语音动作认证方法中的物理和行为特征的影响和权重。<details>
<summary>Abstract</summary>
Lip-based biometric authentication (LBBA) is an authentication method based on a person's lip movements during speech in the form of video data captured by a camera sensor. LBBA can utilize both physical and behavioral characteristics of lip movements without requiring any additional sensory equipment apart from an RGB camera. State-of-the-art (SOTA) approaches use one-shot learning to train deep siamese neural networks which produce an embedding vector out of these features. Embeddings are further used to compute the similarity between an enrolled user and a user being authenticated. A flaw of these approaches is that they model behavioral features as style-of-speech without relation to what is being said. This makes the system vulnerable to video replay attacks of the client speaking any phrase. To solve this problem we propose a one-shot approach which models behavioral features to discriminate against what is being said in addition to style-of-speech. We achieve this by customizing the GRID dataset to obtain required triplets and training a siamese neural network based on 3D convolutions and recurrent neural network layers. A custom triplet loss for batch-wise hard-negative mining is proposed. Obtained results using an open-set protocol are 3.2% FAR and 3.8% FRR on the test set of the customized GRID dataset. Additional analysis of the results was done to quantify the influence and discriminatory power of behavioral and physical features for LBBA.
</details>
<details>
<summary>摘要</summary>
口形基于身份验证（LBBA）是一种基于人脸运动的身份验证方法，通过摄像头捕捉的视频数据获取人脸运动特征。LBBA可以利用人脸运动的物理和行为特征，无需额外的感知设备。现有最佳实践（SOTA）方法使用一shot学习训练深度同声网络，生成特征向量并计算用户认证和承诺者之间的相似性。但这些方法存在一个缺陷，即模型行为特征为无关于发言内容的样式。这使得系统容易受到客户端发送任何话语的视频重播攻击。为解决这个问题，我们提出了一种一shot方法，模型行为特征以确定发言内容。我们通过自定义GRID数据集来获取必要的三重ts和训练基于3D卷积和循环神经网络层的siameseneural网络。我们还提出了自定义 triplet损失来进行批处理硬negative挑战。实验结果表明，在自定义GRID数据集上，得到的FRR和FAR分别为3.2%和3.8%。此外，我们还进行了更多的分析，以量化LBBA中物理和行为特征的影响和权威程度。
</details></li>
</ul>
<hr>
<h2 id="Radiomics-Informed-Deep-Learning-for-Classification-of-Atrial-Fibrillation-Sub-Types-from-Left-Atrium-CT-Volumes"><a href="#Radiomics-Informed-Deep-Learning-for-Classification-of-Atrial-Fibrillation-Sub-Types-from-Left-Atrium-CT-Volumes" class="headerlink" title="Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes"></a>Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06933">http://arxiv.org/abs/2308.06933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/ridl">https://github.com/xmed-lab/ridl</a></li>
<li>paper_authors: Weihang Dai, Xiaomeng Li, Taihui Yu, Di Zhao, Jun Shen, Kwang-Ting Cheng</li>
<li>for: 这个研究旨在提高心脏病电位矫正诊断，特别是Automatic Atrial Fibrillation (AF) 分类。</li>
<li>methods: 该方法结合深度学习和生物 markers的优点，使用 радиом�кс指南了深度学习模型，并通过特有的特征减少深度学习模型的过拟合问题。</li>
<li>results: 该方法在 AF 分类任务上达到了 86.9% AUC，超过了现有的 радиом�кс、深度学习和混合方法。<details>
<summary>Abstract</summary>
Atrial Fibrillation (AF) is characterized by rapid, irregular heartbeats, and can lead to fatal complications such as heart failure. The disease is divided into two sub-types based on severity, which can be automatically classified through CT volumes for disease screening of severe cases. However, existing classification approaches rely on generic radiomic features that may not be optimal for the task, whilst deep learning methods tend to over-fit to the high-dimensional volume inputs. In this work, we propose a novel radiomics-informed deep-learning method, RIDL, that combines the advantages of deep learning and radiomic approaches to improve AF sub-type classification. Unlike existing hybrid techniques that mostly rely on na\"ive feature concatenation, we observe that radiomic feature selection methods can serve as an information prior, and propose supplementing low-level deep neural network (DNN) features with locally computed radiomic features. This reduces DNN over-fitting and allows local variations between radiomic features to be better captured. Furthermore, we ensure complementary information is learned by deep and radiomic features by designing a novel feature de-correlation loss. Combined, our method addresses the limitations of deep learning and radiomic approaches and outperforms state-of-the-art radiomic, deep learning, and hybrid approaches, achieving 86.9% AUC for the AF sub-type classification task. Code is available at https://github.com/xmed-lab/RIDL.
</details>
<details>
<summary>摘要</summary>
心律不正（AF）特征是快速、不规则的心跳，可能导致致命的合并症状，如心力衰竭。根据严重程度分为两种亚型，可通过CT图像进行疾病检测。现有的分类方法多数基于通用的 радиом来特征，这些特征可能不适合任务，而深度学习方法容易过拟合高维度的volume输入。在这种情况下，我们提出了一种新的 радиом扩展深度学习方法（RIDL），该方法结合深度学习和 радиом特征的优点，以提高AF亚型分类的准确率。 unlike existing hybrid techniques that mostly rely on naive feature concatenation, we observe that radiomic feature selection methods can serve as an information prior, and propose supplementing low-level deep neural network (DNN) features with locally computed radiomic features。这将 reduces DNN over-fitting and allows local variations between radiomic features to be better captured。另外，我们设计了一种新的特征分解损失函数，以确保深度和 радиом特征之间学习的信息是комplementary。结果显示，我们的方法可以在AF亚型分类任务中升级现有的 радиом、深度学习和混合方法，实现86.9%的AUC分类精度。代码可以在https://github.com/xmed-lab/RIDL上获取。
</details></li>
</ul>
<hr>
<h2 id="OpenGCD-Assisting-Open-World-Recognition-with-Generalized-Category-Discovery"><a href="#OpenGCD-Assisting-Open-World-Recognition-with-Generalized-Category-Discovery" class="headerlink" title="OpenGCD: Assisting Open World Recognition with Generalized Category Discovery"></a>OpenGCD: Assisting Open World Recognition with Generalized Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06926">http://arxiv.org/abs/2308.06926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fulin-gao/opengcd">https://github.com/fulin-gao/opengcd</a></li>
<li>paper_authors: Fulin Gao, Weimin Zhong, Zhixing Cao, Xin Peng, Zhi Li</li>
<li>for: 该论文的目的是提出一种可靠的开放世界认知（OWR）系统，可以在线进行开放集 recognition（OSR）、分类 unknown 数据和进行逐步学习（IL）。</li>
<li>methods: 该论文提出了三个关键想法来解决上述问题，即（1）根据分类器预测结果的不确定性对实例的起源进行评分；（2）首次在 OWR 中应用总分类发现技术（GCD）来帮助人们对无标签数据进行分类；（3）为保持 IL 和 GCD 的平滑执行，保留每个类别的等量示例，并且目标是保持类别的多样性。</li>
<li>results: 实验结果表明，OpenGCD 不仅具有优秀的兼容性，还可以明显超越其他基elines。 Code: <a target="_blank" rel="noopener" href="https://github.com/Fulin-Gao/OpenGCD">https://github.com/Fulin-Gao/OpenGCD</a>.<details>
<summary>Abstract</summary>
A desirable open world recognition (OWR) system requires performing three tasks: (1) Open set recognition (OSR), i.e., classifying the known (classes seen during training) and rejecting the unknown (unseen$/$novel classes) online; (2) Grouping and labeling these unknown as novel known classes; (3) Incremental learning (IL), i.e., continual learning these novel classes and retaining the memory of old classes. Ideally, all of these steps should be automated. However, existing methods mostly assume that the second task is completely done manually. To bridge this gap, we propose OpenGCD that combines three key ideas to solve the above problems sequentially: (a) We score the origin of instances (unknown or specifically known) based on the uncertainty of the classifier's prediction; (b) For the first time, we introduce generalized category discovery (GCD) techniques in OWR to assist humans in grouping unlabeled data; (c) For the smooth execution of IL and GCD, we retain an equal number of informative exemplars for each class with diversity as the goal. Moreover, we present a new performance evaluation metric for GCD called harmonic clustering accuracy. Experiments on two standard classification benchmarks and a challenging dataset demonstrate that OpenGCD not only offers excellent compatibility but also substantially outperforms other baselines. Code: https://github.com/Fulin-Gao/OpenGCD.
</details>
<details>
<summary>摘要</summary>
一个愿景的开放世界认知（OWR）系统需要完成三个任务：（1）开放集 recognition（OSR），即在线上分类已知（训练中看到的类）并拒绝未知（未看到的类）;（2）对未知类进行分类和标注为新知类;（3）Continual learning（IL），即不断学习这些新类并保持古老类的记忆。理想情况下，所有这些步骤都应该是自动化的。然而，现有方法大多数假设第二个任务是完全手动完成的。为了bridging这个差距，我们提出了OpenGCD，它结合了三个关键想法来解决上述问题：（a）根据分类器预测结果的不确定性来评分实例的起源（未知或特定知的）;（b）在OWR中首次引入通用类发现（GCD）技术，以帮助人类分类未标注数据;（c）为IL和GCD的畅通执行，我们保留每个类型的等数量的有用示例，并且目的是寻求多样性。此外，我们还提出了一个新的性能评价指标 дляGCD，即和谐分 clustering准确率。实验结果表明，OpenGCD不仅具有极好的兼容性，而且也substantially Outperform其他基准。代码：https://github.com/Fulin-Gao/OpenGCD。
</details></li>
</ul>
<hr>
<h2 id="CBA-Improving-Online-Continual-Learning-via-Continual-Bias-Adaptor"><a href="#CBA-Improving-Online-Continual-Learning-via-Continual-Bias-Adaptor" class="headerlink" title="CBA: Improving Online Continual Learning via Continual Bias Adaptor"></a>CBA: Improving Online Continual Learning via Continual Bias Adaptor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06925">http://arxiv.org/abs/2308.06925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wqza/cba-online-cl">https://github.com/wqza/cba-online-cl</a></li>
<li>paper_authors: Quanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, Deyu Meng</li>
<li>for: 提高在非站ARY数据流中学习新知识和稳定把握先前学习的知识。</li>
<li>methods: 提出了一种Continual Bias Adaptor（CBA）模块，用于在训练过程中增强分类器网络，以适应变化的训练环境，使分类器网络能够稳定地把握先前学习的任务。</li>
<li>results: 经过了广泛的实验，证明了CBA模块能够有效地缓解Catastrophic Distribution Shift问题，并且在测试阶段可以移除CBA模块，不增加计算成本和内存开销。<details>
<summary>Abstract</summary>
Online continual learning (CL) aims to learn new knowledge and consolidate previously learned knowledge from non-stationary data streams. Due to the time-varying training setting, the model learned from a changing distribution easily forgets the previously learned knowledge and biases toward the newly received task. To address this problem, we propose a Continual Bias Adaptor (CBA) module to augment the classifier network to adapt to catastrophic distribution change during training, such that the classifier network is able to learn a stable consolidation of previously learned tasks. In the testing stage, CBA can be removed which introduces no additional computation cost and memory overhead. We theoretically reveal the reason why the proposed method can effectively alleviate catastrophic distribution shifts, and empirically demonstrate its effectiveness through extensive experiments based on four rehearsal-based baselines and three public continual learning benchmarks.
</details>
<details>
<summary>摘要</summary>
在线 continual learning (CL) 目标是从非站ARY的数据流中学习新知识并巩固先前学习的知识。由于训练环境的时间变化，模型从变化的分布中学习的知识容易忘记先前学习的知识，偏向最新接收的任务。为解决这个问题，我们提议一种Continual Bias Adaptor (CBA)模块，用于补充分类网络，以适应训练中的慢性分布变化，使分类网络能够稳定地 консоли达先前学习的任务。在测试阶段，CBA可以被移除，不添加计算成本和内存负担。我们理论上解释了我们提议的方法可以有效缓解慢性分布变化的问题，并通过了广泛的实验，包括四个基础elines和三个公共的 continual learning benchmark。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Lightweight-Hierarchical-Vision-Transformers-for-Efficient-Visual-Tracking"><a href="#Exploring-Lightweight-Hierarchical-Vision-Transformers-for-Efficient-Visual-Tracking" class="headerlink" title="Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking"></a>Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06904">http://arxiv.org/abs/2308.06904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kangben258/hit">https://github.com/kangben258/hit</a></li>
<li>paper_authors: Ben Kang, Xin Chen, Dong Wang, Houwen Peng, Huchuan Lu</li>
<li>for: 提高视觉跟踪器的运行速度，以便在具有限制计算能力的设备上实现高性能。</li>
<li>methods: 提出了一种新的高效跟踪模型家族，即HiT，它通过bridge模块将现代轻量级 transformer 和跟踪框架相连接，从而提高跟踪性能。同时，提出了一种新的双重图像位编码技术，以同时编码搜索区域和模板图像的位置信息。</li>
<li>results: HiT 模型在 Nvidia Jetson AGX 边缘设备上运行于 61 帧每秒 (fps)，并在 LaSOT benchmark 上达到 64.6% AUC，超过了所有之前的高效跟踪器。<details>
<summary>Abstract</summary>
Transformer-based visual trackers have demonstrated significant progress owing to their superior modeling capabilities. However, existing trackers are hampered by low speed, limiting their applicability on devices with limited computational power. To alleviate this problem, we propose HiT, a new family of efficient tracking models that can run at high speed on different devices while retaining high performance. The central idea of HiT is the Bridge Module, which bridges the gap between modern lightweight transformers and the tracking framework. The Bridge Module incorporates the high-level information of deep features into the shallow large-resolution features. In this way, it produces better features for the tracking head. We also propose a novel dual-image position encoding technique that simultaneously encodes the position information of both the search region and template images. The HiT model achieves promising speed with competitive performance. For instance, it runs at 61 frames per second (fps) on the Nvidia Jetson AGX edge device. Furthermore, HiT attains 64.6% AUC on the LaSOT benchmark, surpassing all previous efficient trackers.
</details>
<details>
<summary>摘要</summary>
“具有增强能力的变换器基于视觉跟踪器在过去几年中已经取得了显著进步，但现有的跟踪器受到计算能力限制，导致它们在设备上运行速度低下。为解决这个问题，我们提出了HiT，一种新的高效跟踪模型，可以在不同的设备上快速运行，同时保持高性能。HiT的中心思想是桥模块，它将现代轻量级变换器和跟踪框架相连接，将深度特征中的高级信息与浅大分辨率特征相结合。这种方法可以为跟踪头产生更好的特征。我们还提出了一种新的双图像位编码技术，同时编码搜索区域和模板图像的位置信息。HiT模型在Nvidia Jetson AGX边缘设备上运行速度达61帧每秒（fps），并在LaSOT测试benchmark上取得了64.6%的AUC，超过了所有的高效跟踪器。”
</details></li>
</ul>
<hr>
<h2 id="Orthogonal-Temporal-Interpolation-for-Zero-Shot-Video-Recognition"><a href="#Orthogonal-Temporal-Interpolation-for-Zero-Shot-Video-Recognition" class="headerlink" title="Orthogonal Temporal Interpolation for Zero-Shot Video Recognition"></a>Orthogonal Temporal Interpolation for Zero-Shot Video Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06897">http://arxiv.org/abs/2308.06897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Zhu, Junbao Zhuo, Bin Ma, Jiajia Geng, Xiaoming Wei, Xiaolin Wei, Shuhui Wang</li>
<li>for:  zero-shot video recognition (ZSVR)</li>
<li>methods:  vision-language models (VLMs) with an additional temporal learning module and orthogonal temporal interpolation, as well as a matching loss</li>
<li>results:  the proposed OTI model outperforms previous state-of-the-art methods on popular video datasets (Kinetics-600, UCF101, and HMDB51) with clear margins.<details>
<summary>Abstract</summary>
Zero-shot video recognition (ZSVR) is a task that aims to recognize video categories that have not been seen during the model training process. Recently, vision-language models (VLMs) pre-trained on large-scale image-text pairs have demonstrated impressive transferability for ZSVR. To make VLMs applicable to the video domain, existing methods often use an additional temporal learning module after the image-level encoder to learn the temporal relationships among video frames. Unfortunately, for video from unseen categories, we observe an abnormal phenomenon where the model that uses spatial-temporal feature performs much worse than the model that removes temporal learning module and uses only spatial feature. We conjecture that improper temporal modeling on video disrupts the spatial feature of the video. To verify our hypothesis, we propose Feature Factorization to retain the orthogonal temporal feature of the video and use interpolation to construct refined spatial-temporal feature. The model using appropriately refined spatial-temporal feature performs better than the one using only spatial feature, which verifies the effectiveness of the orthogonal temporal feature for the ZSVR task. Therefore, an Orthogonal Temporal Interpolation module is designed to learn a better refined spatial-temporal video feature during training. Additionally, a Matching Loss is introduced to improve the quality of the orthogonal temporal feature. We propose a model called OTI for ZSVR by employing orthogonal temporal interpolation and the matching loss based on VLMs. The ZSVR accuracies on popular video datasets (i.e., Kinetics-600, UCF101 and HMDB51) show that OTI outperforms the previous state-of-the-art method by a clear margin.
</details>
<details>
<summary>摘要</summary>
“零目标影片识别（ZSVR）是一个目标，旨在识别过去训练过程中未看过的影片类别。现在，视力语模型（VLM）在大规模的图像文本对中进行预训后，对ZSVR预示出了优秀的转移性。为了让VLM在影片领域中可以应用，现有的方法通常使用额外的时间学习模块，以学习影片帧之间的时间关系。但是，对于未见类别的影片，我们观察到一个异常现象，即使用时间学习模块的模型在识别影片类别时表现很差，而不使用时间学习模块的模型则表现较好。我们推测，在影片中不恰当的时间模型可能会干扰影片的空间特征。为了证明我们的假设，我们提出了特征分解，以保留影片的正交时间特征，并使用插值来建构更精确的空间-时间特征。模型使用适当的特征分解和插值，对于ZSVR任务的表现来说明了其效iveness。因此，我们提出了一个名为OTI的模型，该模型通过使用正交时间插值和基于VLM的匹配损失来学习更好的空间-时间影片特征。在实验中，我们发现OTI在 популяр的影片 dataset（即Kinetics-600、UCF101和HMDB51）上的ZSVR准确率都高于前一代方法。”
</details></li>
</ul>
<hr>
<h2 id="Robustness-Stress-Testing-in-Medical-Image-Classification"><a href="#Robustness-Stress-Testing-in-Medical-Image-Classification" class="headerlink" title="Robustness Stress Testing in Medical Image Classification"></a>Robustness Stress Testing in Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06889">http://arxiv.org/abs/2308.06889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mobarakol/robustness_stress_testing">https://github.com/mobarakol/robustness_stress_testing</a></li>
<li>paper_authors: Mobarakol Islam, Zeju Li, Ben Glocker</li>
<li>for: 这个论文旨在评估图像基于疾病检测模型的可靠性和抗耗性。</li>
<li>methods: 该论文使用了进攻性测试来评估模型的抗耗性和 subgroup 性能差异。进攻性测试使用了五种bidirectional和unidirectional图像抖动，six个不同的严重程度。</li>
<li>results: 实验结果表明，不同的模型在不同的抖动和严重程度下的性能差异很大。此外，预训练特征对下游的可靠性有重要的影响。研究结果表明，进攻性测试是一种重要的工具，应成为图像基于疾病检测模型的严格验证标准之一。<details>
<summary>Abstract</summary>
Deep neural networks have shown impressive performance for image-based disease detection. Performance is commonly evaluated through clinical validation on independent test sets to demonstrate clinically acceptable accuracy. Reporting good performance metrics on test sets, however, is not always a sufficient indication of the generalizability and robustness of an algorithm. In particular, when the test data is drawn from the same distribution as the training data, the iid test set performance can be an unreliable estimate of the accuracy on new data. In this paper, we employ stress testing to assess model robustness and subgroup performance disparities in disease detection models. We design progressive stress testing using five different bidirectional and unidirectional image perturbations with six different severity levels. As a use case, we apply stress tests to measure the robustness of disease detection models for chest X-ray and skin lesion images, and demonstrate the importance of studying class and domain-specific model behaviour. Our experiments indicate that some models may yield more robust and equitable performance than others. We also find that pretraining characteristics play an important role in downstream robustness. We conclude that progressive stress testing is a viable and important tool and should become standard practice in the clinical validation of image-based disease detection models.
</details>
<details>
<summary>摘要</summary>
深度神经网络在图像基于疾病检测方面表现出色。表现的评估通常通过临床验证在独立的测试集上进行，以证明临床可接受的准确率。然而，只是在测试数据来自同一个分布 alsothe training data时，iid测试集表现可能不是一个可靠的准确率估计。在这篇论文中，我们使用压力测试来评估模型的可靠性和 subgroup 性能差异。我们设计了五种逆向和单向图像干扰，并将其分为六个不同的严重程度。作为一个使用例子，我们将压力测试应用于肺X射线和皮肤斑点图像的疾病检测模型中，并证明了研究类和领域特有的模型行为的重要性。我们的实验表明，一些模型可能比其他模型更加可靠和公平。我们还发现，预训练特征对下游的可靠性产生了重要的影响。我们 conclude 进展性的压力测试是一种可靠和重要的工具，应成为临床验证图像基于疾病检测模型的标准实践。
</details></li>
</ul>
<hr>
<h2 id="Towards-Open-Set-Test-Time-Adaptation-Utilizing-the-Wisdom-of-Crowds-in-Entropy-Minimization"><a href="#Towards-Open-Set-Test-Time-Adaptation-Utilizing-the-Wisdom-of-Crowds-in-Entropy-Minimization" class="headerlink" title="Towards Open-Set Test-Time Adaptation Utilizing the Wisdom of Crowds in Entropy Minimization"></a>Towards Open-Set Test-Time Adaptation Utilizing the Wisdom of Crowds in Entropy Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06879">http://arxiv.org/abs/2308.06879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jungsoo Lee, Debasmit Das, Jaegul Choo, Sungha Choi</li>
<li>for: 这篇论文的目的是提出一种简单 yet effective的测验时适应（TTA）方法，以解决现有TTA方法中的问题，包括开放集类TTA。</li>
<li>methods: 这篇论文使用了一种简单的样本选择方法，它是基于以下关键的实践观察：当使用Entropy minimization时， incorrect或open-set predictions对于模型的适应会带来噪音信号。这篇论文发现，这些噪音信号通常会导致模型的预测数据具有较低的信任值。因此，这篇论文提出了一种筛选样本的方法，以除除这些噪音信号。</li>
<li>results: 这篇论文的实验结果显示，这种筛选样本方法可以帮助TTA方法在图像分类（例如，TENT的错误率下降49.4%）和 semanticsegmentation（例如，TENT的mIoU提高11.7%）中提高长期适应性。<details>
<summary>Abstract</summary>
Test-time adaptation (TTA) methods, which generally rely on the model's predictions (e.g., entropy minimization) to adapt the source pretrained model to the unlabeled target domain, suffer from noisy signals originating from 1) incorrect or 2) open-set predictions. Long-term stable adaptation is hampered by such noisy signals, so training models without such error accumulation is crucial for practical TTA. To address these issues, including open-set TTA, we propose a simple yet effective sample selection method inspired by the following crucial empirical finding. While entropy minimization compels the model to increase the probability of its predicted label (i.e., confidence values), we found that noisy samples rather show decreased confidence values. To be more specific, entropy minimization attempts to raise the confidence values of an individual sample's prediction, but individual confidence values may rise or fall due to the influence of signals from numerous other predictions (i.e., wisdom of crowds). Due to this fact, noisy signals misaligned with such 'wisdom of crowds', generally found in the correct signals, fail to raise the individual confidence values of wrong samples, despite attempts to increase them. Based on such findings, we filter out the samples whose confidence values are lower in the adapted model than in the original model, as they are likely to be noisy. Our method is widely applicable to existing TTA methods and improves their long-term adaptation performance in both image classification (e.g., 49.4% reduced error rates with TENT) and semantic segmentation (e.g., 11.7% gain in mIoU with TENT).
</details>
<details>
<summary>摘要</summary>
测试时适应（TTA）方法通常基于模型预测（例如 entropy 最小化）来适应无标目标频道的源预训练模型。然而，由于各种各样的预测错误和开放集预测，TTA 方法容易受到各种各样的噪声影响，从而长期稳定的适应变得困难。为了解决这些问题，包括开放集 TTA，我们提出了一种简单 yet 有效的样本选择方法，基于以下关键的实际观察：在 entropy 最小化过程中，模型会增加预测标签的可信度（即 confidence value），但噪声样本却会显示下降的可信度。实际上， entropy 最小化尝试通过提高个体样本预测的可信度来增加模型的预测可信度，但个体可信度可能由数据量级的其他预测信号所影响（即 "智慧的群体"）。由于这种情况，噪声样本不会因为受到其他预测信号的影响而增加个体可信度，即使 entropy 最小化尝试提高它们。基于这些观察，我们将各种各样的样本滤除，其中 confidence value 在适应模型中下降的比原始模型更低。我们的方法可以与现有的 TTA 方法结合使用，并在图像分类（例如 TENT 的 49.4% 降低错误率）和 semantic segmentation（例如 TENT 的 11.7% 提高 mIoU）中提高长期适应性能。
</details></li>
</ul>
<hr>
<h2 id="Shape-Graph-Matching-Network-SGM-net-Registration-for-Statistical-Shape-Analysis"><a href="#Shape-Graph-Matching-Network-SGM-net-Registration-for-Statistical-Shape-Analysis" class="headerlink" title="Shape-Graph Matching Network (SGM-net): Registration for Statistical Shape Analysis"></a>Shape-Graph Matching Network (SGM-net): Registration for Statistical Shape Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06869">http://arxiv.org/abs/2308.06869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shenyuan Liang, Mauricio Pamplona Segundo, Sathyanarayanan N. Aakur, Sudeep Sarkar, Anuj Srivastava</li>
<li>for: 本研究针对数据对象形状的统计分析，具体来说是一种称为形状图的数据集，其中节点之间连接由自由形态曲线相连接。</li>
<li>methods: 本研究使用一种新的神经网络架构来解决对点（节点到节点、边到边）的受限注册问题，这个问题由于对节点（数量、位置）和边（形状、位置、大小）的不同而变得更加挑战。</li>
<li>results: 本研究使用一种新的神经网络架构和一个不supervised损失函数基于扭形度量来解决对点的受限注册问题，这得到了（1）状态机器的匹配性和（2）与基eline方法相比，计算成本减少一个数量级。作者通过实验和实际数据 demonstrate了该方法的有效性。<details>
<summary>Abstract</summary>
This paper focuses on the statistical analysis of shapes of data objects called shape graphs, a set of nodes connected by articulated curves with arbitrary shapes. A critical need here is a constrained registration of points (nodes to nodes, edges to edges) across objects. This, in turn, requires optimization over the permutation group, made challenging by differences in nodes (in terms of numbers, locations) and edges (in terms of shapes, placements, and sizes) across objects. This paper tackles this registration problem using a novel neural-network architecture and involves an unsupervised loss function developed using the elastic shape metric for curves. This architecture results in (1) state-of-the-art matching performance and (2) an order of magnitude reduction in the computational cost relative to baseline approaches. We demonstrate the effectiveness of the proposed approach using both simulated data and real-world 2D and 3D shape graphs. Code and data will be made publicly available after review to foster research.
</details>
<details>
<summary>摘要</summary>
To tackle this registration problem, the paper proposes a novel neural network architecture and an unsupervised loss function based on the elastic shape metric for curves. The proposed approach achieves state-of-the-art matching performance and reduces the computational cost by an order of magnitude compared to baseline methods.The paper demonstrates the effectiveness of the proposed approach using both simulated data and real-world 2D and 3D shape graphs. To facilitate further research, the authors will make the code and data publicly available after review.
</details></li>
</ul>
<hr>
<h2 id="Camera-Based-mmWave-Beam-Prediction-Towards-Multi-Candidate-Real-World-Scenarios"><a href="#Camera-Based-mmWave-Beam-Prediction-Towards-Multi-Candidate-Real-World-Scenarios" class="headerlink" title="Camera Based mmWave Beam Prediction: Towards Multi-Candidate Real-World Scenarios"></a>Camera Based mmWave Beam Prediction: Towards Multi-Candidate Real-World Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06868">http://arxiv.org/abs/2308.06868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gouranga Charan, Muhammad Alrabeiah, Tawfik Osman, Ahmed Alkhateeb<br>for:The paper is written for the purpose of exploring the use of sensory information to aid the beam selection process in millimeter-wave (mmWave) and sub-terahertz (sub-THz) communication systems.methods:The paper proposes a machine learning-based framework that utilizes visual and positional data to predict the optimal beam indices, as an alternative to conventional beam sweeping approaches.results:The proposed solutions achieve close to 100% top-5 beam prediction accuracy for single-user scenarios and close to 95% top-5 beam prediction accuracy for multi-candidate scenarios. Additionally, the approach can identify the probable transmitting candidate with over 93% accuracy across different scenarios, highlighting a promising approach for nearly eliminating the beam training overhead in mmWave&#x2F;THz communication systems.Here is the text in Simplified Chinese:for:本文是为了探讨使用感知信息来帮助mmWave&#x2F;THz通信系统的扫描过程中的 beam 选择问题。methods:本文提出了基于机器学习的框架，利用视觉和位置数据预测最佳扫描指标，作为传统扫描方法的替代方案。results:提议的解决方案在单用户场景中几乎达到100%的前5个扫描指标预测精度，在多候选场景中几乎达到95%的前5个扫描指标预测精度。此外，该方法还可以在不同场景中准确地识别发射器 кандидат的概率，达到了93%以上的准确率。这 highlights 一种可行的方法，可以减少mmWave&#x2F;THz通信系统中的扫描训练开销。<details>
<summary>Abstract</summary>
Leveraging sensory information to aid the millimeter-wave (mmWave) and sub-terahertz (sub-THz) beam selection process is attracting increasing interest. This sensory data, captured for example by cameras at the basestations, has the potential of significantly reducing the beam sweeping overhead and enabling highly-mobile applications. The solutions developed so far, however, have mainly considered single-candidate scenarios, i.e., scenarios with a single candidate user in the visual scene, and were evaluated using synthetic datasets. To address these limitations, this paper extensively investigates the sensing-aided beam prediction problem in a real-world multi-object vehicle-to-infrastructure (V2I) scenario and presents a comprehensive machine learning-based framework. In particular, this paper proposes to utilize visual and positional data to predict the optimal beam indices as an alternative to the conventional beam sweeping approaches. For this, a novel user (transmitter) identification solution has been developed, a key step in realizing sensing-aided multi-candidate and multi-user beam prediction solutions. The proposed solutions are evaluated on the large-scale real-world DeepSense $6$G dataset. Experimental results in realistic V2I communication scenarios indicate that the proposed solutions achieve close to $100\%$ top-5 beam prediction accuracy for the scenarios with single-user and close to $95\%$ top-5 beam prediction accuracy for multi-candidate scenarios. Furthermore, the proposed approach can identify the probable transmitting candidate with more than $93\%$ accuracy across the different scenarios. This highlights a promising approach for nearly eliminating the beam training overhead in mmWave/THz communication systems.
</details>
<details>
<summary>摘要</summary>
使用感知信息来帮助 millimeter-wave（mmWave）和Sub-teraHertz（sub-THz）的扫描过程选择 beam 是一项吸引越来越多的关注。这些感知数据，例如通过基站的摄像头捕获的视频数据，有可能减少扫描过程的负担和实现高移动端应用。现有的解决方案主要是基于单个候选者场景（即场景中只有一个用户），并使用合成数据进行评估。为了解决这些局限性，本文广泛 investigate了感知帮助 beam 预测问题在实际的多对象 Vehicle-to-Infrastructure（V2I）场景中，并提出了一个完整的机器学习基础框架。特别是，本文提议使用视觉和位置数据预测optimal beam 指标，作为传统扫描方法的替代方案。为此，我们开发了一种新的用户（发送器）标识解决方案，是实现感知帮助多候选者和多用户 beam 预测解决方案的关键步骤。提出的解决方案在 DeepSense $6$G 大规模实际场景中进行了实验，实际 results 表明，在单用户场景中，提出的解决方案可达 $100\%$ top-5 beam 预测精度，而在多候选场景中，可达 $95\%$ top-5 beam 预测精度。此外，我们的方法可以在不同场景下识别发送者的概率高于 $93\%$。这表明，我们的方法可以减少 mmWave/THz 通信系统中的扫描训练过程负担。
</details></li>
</ul>
<hr>
<h2 id="Improving-Face-Recognition-from-Caption-Supervision-with-Multi-Granular-Contextual-Feature-Aggregation"><a href="#Improving-Face-Recognition-from-Caption-Supervision-with-Multi-Granular-Contextual-Feature-Aggregation" class="headerlink" title="Improving Face Recognition from Caption Supervision with Multi-Granular Contextual Feature Aggregation"></a>Improving Face Recognition from Caption Supervision with Multi-Granular Contextual Feature Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06866">http://arxiv.org/abs/2308.06866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Mahedi Hasan, Nasser Nasrabadi<br>for: 本文提出了一种新的框架，即caption-guided face recognition（CGFR），以提高现成的面Recognition（FR）系统的性能。methods: 本文使用了face examiners提供的facial descriptions作为auxiliary information，并提出了一种Contextual feature aggregation module（CFAM）和textual feature refinement module（TFRM）来有效地 fusion image和textual features。results: 对于Multi-Modal CelebA-HQ数据集，我们的CGFR框架对ArcFace模型进行了改进，并在1:1验证和1:N识别协议中显著提高了性能。<details>
<summary>Abstract</summary>
We introduce caption-guided face recognition (CGFR) as a new framework to improve the performance of commercial-off-the-shelf (COTS) face recognition (FR) systems. In contrast to combining soft biometrics (eg., facial marks, gender, and age) with face images, in this work, we use facial descriptions provided by face examiners as a piece of auxiliary information. However, due to the heterogeneity of the modalities, improving the performance by directly fusing the textual and facial features is very challenging, as both lie in different embedding spaces. In this paper, we propose a contextual feature aggregation module (CFAM) that addresses this issue by effectively exploiting the fine-grained word-region interaction and global image-caption association. Specifically, CFAM adopts a self-attention and a cross-attention scheme for improving the intra-modality and inter-modality relationship between the image and textual features, respectively. Additionally, we design a textual feature refinement module (TFRM) that refines the textual features of the pre-trained BERT encoder by updating the contextual embeddings. This module enhances the discriminative power of textual features with a cross-modal projection loss and realigns the word and caption embeddings with visual features by incorporating a visual-semantic alignment loss. We implemented the proposed CGFR framework on two face recognition models (ArcFace and AdaFace) and evaluated its performance on the Multi-Modal CelebA-HQ dataset. Our framework significantly improves the performance of ArcFace in both 1:1 verification and 1:N identification protocol.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个新的框架，即caption-guided face recognition（CGFR），以提高现成通商（COTS）面Recognition（FR）系统的性能。相比于结合软生物мет里（例如面上的标识、性别和年龄），在这个工作中，我们使用面对面的描述提供的 auxiliary information。然而，由于modalities的不同，直接融合文本和面像特征是非常困难的，因为它们都处于不同的嵌入空间。在这篇文章中，我们提出了一个具有上下文特征聚合模块（CFAM），以解决这个问题。CFAM使用了自注意和跨注意的方式，从而提高了内模组和跨模组的关系。此外，我们设计了一个文本特征修正模块（TFRM），将预训BERTencoder的文本特征修正为更加精确。这个模块通过跨模式投射损失和与Visual特征Alignment损失，提高了文本特征的推测力。我们实现了CGFR框架在ArcFace和AdaFace两个面识别模型上，并在Multi-Modal CelebA-HQ dataset上进行了评估。我们的框架在1:1验证和1:N识别协议中具有明显的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Manifold-DivideMix-A-Semi-Supervised-Contrastive-Learning-Framework-for-Severe-Label-Noise"><a href="#Manifold-DivideMix-A-Semi-Supervised-Contrastive-Learning-Framework-for-Severe-Label-Noise" class="headerlink" title="Manifold DivideMix: A Semi-Supervised Contrastive Learning Framework for Severe Label Noise"></a>Manifold DivideMix: A Semi-Supervised Contrastive Learning Framework for Severe Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06861">http://arxiv.org/abs/2308.06861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fahim-f/manifolddividemix">https://github.com/fahim-f/manifolddividemix</a></li>
<li>paper_authors: Fahimeh Fooladgar, Minh Nguyen Nhat To, Parvin Mousavi, Purang Abolmaesumi</li>
<li>for: 提高深度神经网络模型在含有噪声标签数据时的性能，并针对实际世界数据集中存在噪声标签样本的问题。</li>
<li>methods: 提出一种基于自我监督学习的方法，通过EXTRACTING meaningful和普适的嵌入空间，并使用简单 yet effective K-nearest neighbor方法来移除部分噪声样本，然后通过一种迭代的”Manifold DivideMix”算法来找到干净的样本和噪声样本，并在半超vised的方式下训练模型。此外，提出一种新的”MixEMatch”算法，通过在输入和最终隐藏表示空间中进行mixup扩展，以EXTRACT更好的表示。</li>
<li>results: 在多个Synthetic-noise图像标准 benchmark和实际世界web-crawled数据集上进行了广泛的实验，并证明了我们提出的框架的有效性。<details>
<summary>Abstract</summary>
Deep neural networks have proven to be highly effective when large amounts of data with clean labels are available. However, their performance degrades when training data contains noisy labels, leading to poor generalization on the test set. Real-world datasets contain noisy label samples that either have similar visual semantics to other classes (in-distribution) or have no semantic relevance to any class (out-of-distribution) in the dataset. Most state-of-the-art methods leverage ID labeled noisy samples as unlabeled data for semi-supervised learning, but OOD labeled noisy samples cannot be used in this way because they do not belong to any class within the dataset. Hence, in this paper, we propose incorporating the information from all the training data by leveraging the benefits of self-supervised training. Our method aims to extract a meaningful and generalizable embedding space for each sample regardless of its label. Then, we employ a simple yet effective K-nearest neighbor method to remove portions of out-of-distribution samples. By discarding these samples, we propose an iterative "Manifold DivideMix" algorithm to find clean and noisy samples, and train our model in a semi-supervised way. In addition, we propose "MixEMatch", a new algorithm for the semi-supervised step that involves mixup augmentation at the input and final hidden representations of the model. This will extract better representations by interpolating both in the input and manifold spaces. Extensive experiments on multiple synthetic-noise image benchmarks and real-world web-crawled datasets demonstrate the effectiveness of our proposed framework. Code is available at https://github.com/Fahim-F/ManifoldDivideMix.
</details>
<details>
<summary>摘要</summary>
深度神经网络在有大量高质量标签数据时表现非常出色。然而，当训练数据包含噪声标签时，其性能会下降，导致测试集上的泛化性不佳。现实中的数据集中存在噪声标签样本，其中一些样本与其他类具有相似的视觉 semantics（在 Distribution 中），而其他样本则完全没有任何类别相关性（out-of-distribution）。大多数当前的方法利用 ID 标注的噪声样本作为无标签数据进行半超vised学习，但 OOD 标注的噪声样本不能这样使用，因为它们不属于数据集中的任何类别。因此，在这篇论文中，我们提出了利用自我标注的优势，抽象出每个样本的含义和普适的嵌入空间。然后，我们使用简单而有效的 K-最近邻查找方法，从out-of-distribution样本中排除一部分样本。通过这种方式，我们提出了一种迭代的“替换Mix”算法，用于从含有噪声的样本中提取净化后的样本。此外，我们还提出了一种新的“混合匹配”算法，用于半超vised步骤，通过在输入和最终隐藏层中进行混合增强，从而提取更好的表示。我们的方法在多个人造噪音图像标准检验和实际web-抓取的数据集上进行了广泛的实验，并得到了非常出色的效果。代码可以在https://github.com/Fahim-F/ManifoldDivideMix中找到。
</details></li>
</ul>
<hr>
<h2 id="UGC-Quality-Assessment-Exploring-the-Impact-of-Saliency-in-Deep-Feature-Based-Quality-Assessment"><a href="#UGC-Quality-Assessment-Exploring-the-Impact-of-Saliency-in-Deep-Feature-Based-Quality-Assessment" class="headerlink" title="UGC Quality Assessment: Exploring the Impact of Saliency in Deep Feature-Based Quality Assessment"></a>UGC Quality Assessment: Exploring the Impact of Saliency in Deep Feature-Based Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06853">http://arxiv.org/abs/2308.06853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Wang, Angeliki Katsenou, David Bull</li>
<li>for: 本研究旨在提高用户生成内容（UGC）质量的评估方法。</li>
<li>methods: 本研究使用了现代metric，抽取自然场景统计和深度神经网络特征，并使用了焦点地图提高可见性。</li>
<li>results: 初步结果显示，只使用深度特征可以 дости得高相关性，而添加焦点不总是提高性能。结果和代码将公开发布，以便作为研究社区的参考。<details>
<summary>Abstract</summary>
The volume of User Generated Content (UGC) has increased in recent years. The challenge with this type of content is assessing its quality. So far, the state-of-the-art metrics are not exhibiting a very high correlation with perceptual quality. In this paper, we explore state-of-the-art metrics that extract/combine natural scene statistics and deep neural network features. We experiment with these by introducing saliency maps to improve perceptibility. We train and test our models using public datasets, namely, YouTube-UGC and KoNViD-1k. Preliminary results indicate that high correlations are achieved by using only deep features while adding saliency is not always boosting the performance. Our results and code will be made publicly available to serve as a benchmark for the research community and can be found on our project page: https://github.com/xinyiW915/SPIE-2023-Supplementary.
</details>
<details>
<summary>摘要</summary>
“用户生成内容”（UGC）的量在最近几年增加了。但是评估这种内容质量的挑战很大。目前最先进的度量不 exhibit 高度相关性。在这篇文章中，我们探索了使用自然场景统计和深度神经网络特征的度量。我们尝试使用 saliency map 提高可视性。我们使用 YouTube-UGC 和 KoNViD-1k 公共数据集进行训练和测试。初步结果显示，仅使用深度特征可以取得高相关性，而添加 saliency 并不总是提高表现。我们的结果和代码将会在我们的项目页面上公开，供研究社区使用，可以在 GitHub 上找到：https://github.com/xinyiW915/SPIE-2023-Supplementary。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Brain-Tumor-Classification-A-Comprehensive-Study-on-Transfer-Learning-and-Imbalance-Handling-in-Deep-Learning-Models"><a href="#Optimizing-Brain-Tumor-Classification-A-Comprehensive-Study-on-Transfer-Learning-and-Imbalance-Handling-in-Deep-Learning-Models" class="headerlink" title="Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models"></a>Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06821">http://arxiv.org/abs/2308.06821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/razaimam45/ai701-project-transfer-learning-approach-for-imbalance-classification-of-brain-tumor-mri-">https://github.com/razaimam45/ai701-project-transfer-learning-approach-for-imbalance-classification-of-brain-tumor-mri-</a></li>
<li>paper_authors: Raza Imam, Mohammed Talha Alam</li>
<li>for: 这个研究的目的是为了开发一个基于深度学习的数据不均衡类别数据的方法，以提高脑癌MRI图像的类别精度。</li>
<li>methods: 这个研究使用了转移学习的方法，将公开available的模型的预测能力转移到CNN中，以提高类别精度。实验使用了不同的损失函数和补充方法，包括 focal loss 和 SMOTE&#x2F;ADASYN，来解决数据不均衡问题。</li>
<li>results: 实验结果显示，提案的方法可以实现96%的准确率，比其他方法高得多。<details>
<summary>Abstract</summary>
Deep learning has emerged as a prominent field in recent literature, showcasing the introduction of models that utilize transfer learning to achieve remarkable accuracies in the classification of brain tumor MRI images. However, the majority of these proposals primarily focus on balanced datasets, neglecting the inherent data imbalance present in real-world scenarios. Consequently, there is a pressing need for approaches that not only address the data imbalance but also prioritize precise classification of brain cancer. In this work, we present a novel deep learning-based approach, called Transfer Learning-CNN, for brain tumor classification using MRI data. The proposed model leverages the predictive capabilities of existing publicly available models by utilizing their pre-trained weights and transferring those weights to the CNN. By leveraging a publicly available Brain MRI dataset, the experiment evaluated various transfer learning models for classifying different tumor types, including meningioma, glioma, and pituitary tumors. We investigate the impact of different loss functions, including focal loss, and oversampling methods, such as SMOTE and ADASYN, in addressing the data imbalance issue. Notably, the proposed strategy, which combines VGG-16 and CNN, achieved an impressive accuracy rate of 96%, surpassing alternative approaches significantly.
</details>
<details>
<summary>摘要</summary>
深度学习已经成为当前文献中的一个突出的领域，展示了使用传输学习来实现在大脑肿瘤MRI图像分类中的惊人准确率。然而，大多数这些建议都主要关注均衡数据集，忽视了实际世界中的数据不均衡问题。因此，有一项急需的是解决数据不均衡问题并且强调大脑癌精准分类的方法。在这项工作中，我们提出了一种基于深度学习的新方法，即传输学习-CNN，用于大脑肿瘤分类。该方法利用了现有公共可用的模型的预测能力，将其预训练的参数传递到CNN中。通过使用公共可用的大脑MRI数据集，我们对不同类型的肿瘤进行了不同的传输学习模型的评估，包括脑膜肿瘤、 glioma 和肾脏肿瘤。我们也 investigate了不同的损失函数，包括关注损失和 oversampling 方法，例如 SMOTE 和 ADASYN，以解决数据不均衡问题。值得一提的是，我们提出的策略，即将 VGG-16 和 CNN 结合使用，实现了96%的准确率，与其他方法相比明显超越。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.CV_2023_08_14/" data-id="clohum97500g1pj8812fca1bx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.AI_2023_08_14/" class="article-date">
  <time datetime="2023-08-14T12:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/cs.AI_2023_08_14/">cs.AI - 2023-08-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MM-GEF-Multi-modal-representation-meet-collaborative-filtering"><a href="#MM-GEF-Multi-modal-representation-meet-collaborative-filtering" class="headerlink" title="MM-GEF: Multi-modal representation meet collaborative filtering"></a>MM-GEF: Multi-modal representation meet collaborative filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07222">http://arxiv.org/abs/2308.07222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wu, Alejandro Ariza-Casabona, Bartłomiej Twardowski, Tri Kurniawan Wijaya</li>
<li>for: 这篇论文的目的是提出一种基于图的项目结构优化方法，以提高多Modal的推荐系统的性能。</li>
<li>methods: 该方法使用图形 Early-Fusion 技术，将多Modal的内容特征综合到一起，以获取更加精准的项目表示。</li>
<li>results: 经过广泛的实验 validate，该方法在四个公开数据集上实现了比基于单Modal的方法更高的推荐性能。<details>
<summary>Abstract</summary>
In modern e-commerce, item content features in various modalities offer accurate yet comprehensive information to recommender systems. The majority of previous work either focuses on learning effective item representation during modelling user-item interactions, or exploring item-item relationships by analysing multi-modal features. Those methods, however, fail to incorporate the collaborative item-user-item relationships into the multi-modal feature-based item structure. In this work, we propose a graph-based item structure enhancement method MM-GEF: Multi-Modal recommendation with Graph Early-Fusion, which effectively combines the latent item structure underlying multi-modal contents with the collaborative signals. Instead of processing the content feature in different modalities separately, we show that the early-fusion of multi-modal features provides significant improvement. MM-GEF learns refined item representations by injecting structural information obtained from both multi-modal and collaborative signals. Through extensive experiments on four publicly available datasets, we demonstrate systematical improvements of our method over state-of-the-art multi-modal recommendation methods.
</details>
<details>
<summary>摘要</summary>
现代电子商务中，物品内容特征在不同的Modalities上提供了准确且全面的信息，以便推荐系统进行推荐。大多数前一次的工作都集中在学习用户-物品交互中有效的物品表示，或者分析多Modalities的特征来探索物品之间的关系。然而，这些方法却忽略了用户-物品-物品的相互关系在多Modalities的特征基础上的协同作用。在这项工作中，我们提出了一种图structured enhancement方法MM-GEF：多Modalities推荐with Graph Early-Fusion，该方法能够有效地结合多Modalities的特征基础和用户-物品-物品的协同信号。而不是分别处理不同Modalities的内容特征，我们表明在早期融合多Modalities的特征提供了显著的改善。MM-GEF通过对多Modalities特征基础和协同信号获得的结构信息进行投入，学习精细的物品表示。经过对四个公共数据集的广泛实验，我们证明了我们的方法在多Modalities推荐方法中的系统性改进。
</details></li>
</ul>
<hr>
<h2 id="Generating-Individual-Trajectories-Using-GPT-2-Trained-from-Scratch-on-Encoded-Spatiotemporal-Data"><a href="#Generating-Individual-Trajectories-Using-GPT-2-Trained-from-Scratch-on-Encoded-Spatiotemporal-Data" class="headerlink" title="Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data"></a>Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07940">http://arxiv.org/abs/2308.07940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taizo Horikomi, Shouji Fujimoto, Atushi Ishikawa, Takayuki Mizuno</li>
<li>for: 这个论文主要是为了构建一个基于深度学习的人体行为预测模型，以便预测人们的日常行为。</li>
<li>methods: 这个论文使用了GPT-2架构来训练一个序列生成模型，该模型可以从零开始训练，并且可以基于具有不同空间缩放的地理坐标和时间间隔 tokens 来生成人们的日常行动路径。</li>
<li>results: 这个论文的结果表明，通过使用GPT-2架构和特殊符号来表示环境因素和个人特征，可以生成基于具有多种空间缩放的人们的日常行动路径，并且可以预测人们在不同时间和空间上的行为。<details>
<summary>Abstract</summary>
Following Mizuno, Fujimoto, and Ishikawa's research (Front. Phys. 2022), we transpose geographical coordinates expressed in latitude and longitude into distinctive location tokens that embody positions across varied spatial scales. We encapsulate an individual daily trajectory as a sequence of tokens by adding unique time interval tokens to the location tokens. Using the architecture of an autoregressive language model, GPT-2, this sequence of tokens is trained from scratch, allowing us to construct a deep learning model that sequentially generates an individual daily trajectory. Environmental factors such as meteorological conditions and individual attributes such as gender and age are symbolized by unique special tokens, and by training these tokens and trajectories on the GPT-2 architecture, we can generate trajectories that are influenced by both environmental factors and individual attributes.
</details>
<details>
<summary>摘要</summary>
根据米泽野、藤本、石川等人的研究（Front. Phys. 2022），我们将地理坐标表示为纬度和经度转化为特征强调的位置标记，这些标记表示位置在不同的空间尺度上的不同位置。我们将每天的行走路径视为一个序列的位置标记，通过将唯一的时间间隔标记添加到位置标记中来实现。使用GPT-2架构的自然语言模型，我们从头来训练这些标记和路径，以建立一个可以顺序生成每天行走路径的深度学习模型。环境因素，如天气条件和个人特征，例如性别和年龄，被表示为特殊的特征标记，通过训练这些标记和路径，我们可以生成受环境因素和个人特征影响的行走路径。
</details></li>
</ul>
<hr>
<h2 id="Algorithms-for-the-Training-of-Neural-Support-Vector-Machines"><a href="#Algorithms-for-the-Training-of-Neural-Support-Vector-Machines" class="headerlink" title="Algorithms for the Training of Neural Support Vector Machines"></a>Algorithms for the Training of Neural Support Vector Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07204">http://arxiv.org/abs/2308.07204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Lars Simon, Manuel Radons</li>
<li>for: 这篇论文是为了探讨逻辑支持向量机器学习（NSVMs）如何利用领域知识在模型架构设计中。</li>
<li>methods: 这篇论文使用了 Pegasos 算法，并提供了一些训练算法来实现 NSVMs。</li>
<li>results: 这篇论文通过解决一些标准机器学习任务来证明 NSVMs 的效果。<details>
<summary>Abstract</summary>
Neural support vector machines (NSVMs) allow for the incorporation of domain knowledge in the design of the model architecture. In this article we introduce a set of training algorithms for NSVMs that leverage the Pegasos algorithm and provide a proof of concept by solving a set of standard machine learning tasks.
</details>
<details>
<summary>摘要</summary>
神经支持向量机 (NSVM) 允许在模型建立的架构中吸收域知识。在这篇文章中，我们介绍了一组使用 Pegasos 算法的 NSVM 训练算法，并通过解决一组标准机器学习任务来提供证明。Here's a word-for-word translation of the text:神经支持向量机（NSVM）允许在模型建立的架构中吸收域知识。在这篇文章中，我们介绍了一组使用 Pegasos 算法的 NSVM 训练算法，并通过解决一组标准机器学习任务来提供证明。
</details></li>
</ul>
<hr>
<h2 id="Neural-Categorical-Priors-for-Physics-Based-Character-Control"><a href="#Neural-Categorical-Priors-for-Physics-Based-Character-Control" class="headerlink" title="Neural Categorical Priors for Physics-Based Character Control"></a>Neural Categorical Priors for Physics-Based Character Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07200">http://arxiv.org/abs/2308.07200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tencent-RoboticsX/NCP">https://github.com/Tencent-RoboticsX/NCP</a></li>
<li>paper_authors: Qingxu Zhu, He Zhang, Mengting Lan, Lei Han</li>
<li>for: 本研究旨在提出一种新的学习框架，用于控制基于物理的人形角色，以获得更高质量和多样性的运动。</li>
<li>methods: 该方法使用奖励学习（RL）来跟踪和模仿生物样的运动，并使用矩阵量化自适应器（VQ-VAE）来压缩运动clip中的信息。</li>
<li>results: 该方法可以生成高质量的生物样运动，并且可以帮助上层政策学习下游任务。我们在人形角色上进行了广泛的实验，并获得了considerably高质量的运动。<details>
<summary>Abstract</summary>
Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervision of the encoder's output, it follows the original motion clip distribution in the dataset and could lead to imbalanced behaviors in our setting. To address the issue, we further propose a technique named prior shifting to adjust the prior distribution using curiosity-driven RL. The outcome distribution is demonstrated to offer sufficient behavioral diversity and significantly facilitates upper-level policy learning for downstream tasks. We conduct comprehensive experiments using humanoid characters on two challenging downstream tasks, sword-shield striking and two-player boxing game. Our results demonstrate that the proposed framework is capable of controlling the character to perform considerably high-quality movements in terms of behavioral strategies, diversity, and realism. Videos, codes, and data are available at https://tencent-roboticsx.github.io/NCP/.
</details>
<details>
<summary>摘要</summary>
近期研究生成可重用运动偏好的进步已经证明它们可以生成自然的行为。在这篇论文中，我们提出了一种新的学习框架，用于控制基于物理的角色，并且可以提高运动质量和多样性。我们使用奖励学习（RL）来初始化并模仿生命体的自然运动，使用不结构化运动片断中的离散信息瓶颈，这种结构可以压缩运动片断中最重要的信息到一个 компакт又有用的积分空间中。通过从训练过的分类假设分布中采样代码，可以生成高质量的生命体运动。虽然这个假设分布可以通过编码器的输出进行训练，但它会跟踪原始运动片断的分布，这可能会导致行为偏执。为了解决这个问题，我们进一步提出了一种名为“偏shift”的技术，通过吸引力驱动RL来调整假设分布。经过这种技术的调整，结果分布能够提供足够的行为多样性，并且可以大大提高下游策略学习的效果。我们在humanoid角色上进行了严格的实验，使用两个具有挑战性的下游任务：剑盾擦擦和两个玩家拳击游戏。我们的结果表明，我们的框架可以控制角色进行较高质量的运动，包括行为策略、多样性和现实感。视频、代码和数据可以在https://tencent-roboticsx.github.io/NCP/中获得。
</details></li>
</ul>
<hr>
<h2 id="Explaining-Black-Box-Models-through-Counterfactuals"><a href="#Explaining-Black-Box-Models-through-Counterfactuals" class="headerlink" title="Explaining Black-Box Models through Counterfactuals"></a>Explaining Black-Box Models through Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07198">http://arxiv.org/abs/2308.07198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juliatrustworthyai/counterfactualexplanations.jl">https://github.com/juliatrustworthyai/counterfactualexplanations.jl</a></li>
<li>paper_authors: Patrick Altmeyer, Arie van Deursen, Cynthia C. S. Liem</li>
<li>for: 这篇论文旨在提供一个用于生成对比事实解释（Counterfactual Explanations，CE）和算法救济（Algorithmic Recourse，AR）的Julia包，用于解释黑盒模型的输出。</li>
<li>methods: 这篇论文使用了Julia语言开发了一个包，包含了一系列的对比事实生成器，用于生成可行、可信度高的对比事实解释。</li>
<li>results: 论文通过使用这些对比事实生成器，可以提供实用、可行的对比事实解释，并且可以用于提供算法救济，帮助改善不满的结果。<details>
<summary>Abstract</summary>
We present CounterfactualExplanations.jl: a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box models in Julia. CE explain how inputs into a model need to change to yield specific model predictions. Explanations that involve realistic and actionable changes can be used to provide AR: a set of proposed actions for individuals to change an undesirable outcome for the better. In this article, we discuss the usefulness of CE for Explainable Artificial Intelligence and demonstrate the functionality of our package. The package is straightforward to use and designed with a focus on customization and extensibility. We envision it to one day be the go-to place for explaining arbitrary predictive models in Julia through a diverse suite of counterfactual generators.
</details>
<details>
<summary>摘要</summary>
我们介绍CounterfactualExplanations.jl：一个用于生成反事件解释（CE）和算法救援（AR）的 julia 套件。CE 可以解释模型对特定预测所需的输入如何改变，以提供实用和可行的改善方案。我们认为 CE 对于可解释人工智能（Explainable AI）是非常有用，并在这篇文章中详细介绍了我们的套件功能。我们的套件易于使用，采用了自定义和扩展的设计，我们希望这将成为一个用于解释 julia 中的任意预测模型的首选场所，通过多种反事件生成器。
</details></li>
</ul>
<hr>
<h2 id="Task-Offloading-for-Smart-Glasses-in-Healthcare-Enhancing-Detection-of-Elevated-Body-Temperature"><a href="#Task-Offloading-for-Smart-Glasses-in-Healthcare-Enhancing-Detection-of-Elevated-Body-Temperature" class="headerlink" title="Task Offloading for Smart Glasses in Healthcare: Enhancing Detection of Elevated Body Temperature"></a>Task Offloading for Smart Glasses in Healthcare: Enhancing Detection of Elevated Body Temperature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07193">http://arxiv.org/abs/2308.07193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdenacer Naouri, Nabil Abdelkader Nouri, Attia Qammar, Feifei Shi, Huansheng Ning, Sahraoui Dhelim</li>
<li>for: 这个研究的目的是分析在智能眼镜上执行医疗监测应用时的任务卸载场景，以确定最佳的卸载条件。</li>
<li>methods: 该研究使用了实际情况下的性能指标，包括任务完成时间、计算能力和能源消耗，来评估卸载的有效性。</li>
<li>results: 研究发现，在一个室内环境中，如机场，使用智能眼镜检测高体温可以减轻医疗人员的工作负担，提高医疗服务质量。这些发现表明在医疗设置中，任务卸载可以为智能眼镜提供实用性和 relevance。<details>
<summary>Abstract</summary>
Wearable devices like smart glasses have gained popularity across various applications. However, their limited computational capabilities pose challenges for tasks that require extensive processing, such as image and video processing, leading to drained device batteries. To address this, offloading such tasks to nearby powerful remote devices, such as mobile devices or remote servers, has emerged as a promising solution. This paper focuses on analyzing task-offloading scenarios for a healthcare monitoring application performed on smart wearable glasses, aiming to identify the optimal conditions for offloading. The study evaluates performance metrics including task completion time, computing capabilities, and energy consumption under realistic conditions. A specific use case is explored within an indoor area like an airport, where security agents wearing smart glasses to detect elevated body temperature in individuals, potentially indicating COVID-19. The findings highlight the potential benefits of task offloading for wearable devices in healthcare settings, demonstrating its practicality and relevance.
</details>
<details>
<summary>摘要</summary>
智能眼镜和其他智能穿戴设备在不同应用领域中得到了普及。然而，它们的计算能力有限，导致需要大量处理的任务，如图像和视频处理，会使设备电池耗尽。为解决这问题，将这些任务外卸到附近的强大Remote设备，如移动设备或远程服务器，已成为一种有前途的解决方案。本文针对智能眼镜在医疗监测应用中进行任务外卸场景分析，以评估最佳外卸条件。研究评估了任务完成时间、计算能力和能源消耗的性能指标，在实际情况下进行测试。一个具体的应用场景是在机场内，安全人员通过穿戴智能眼镜检测身体发热，可能indi COVID-19。发现任务外卸可以为智能穿戴设备在医疗设置中提供优化的性能。
</details></li>
</ul>
<hr>
<h2 id="Context-Aware-Service-Recommendation-System-for-the-Social-Internet-of-Things"><a href="#Context-Aware-Service-Recommendation-System-for-the-Social-Internet-of-Things" class="headerlink" title="Context-Aware Service Recommendation System for the Social Internet of Things"></a>Context-Aware Service Recommendation System for the Social Internet of Things</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08499">http://arxiv.org/abs/2308.08499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amar Khelloufi, Huansheng Ning, Abdelkarim Ben Sada, Abdenacer Naouri, Sahraoui Dhelim</li>
<li>for: This paper aims to improve the accuracy and relevance of personalized service recommendations in the Social Internet of Things (SIoT) context by exploring the contextual representation of each device-service pair.</li>
<li>methods: The proposed framework uses a latent features combination technique to capture latent feature interactions and Factorization Machines to model higher-order feature interactions specific to each SIoT device-service pair.</li>
<li>results: The experimental evaluation demonstrates the framework’s effectiveness in improving service recommendation accuracy and relevance.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是在社交互联设备（SIoT）上提高个性化服务推荐的准确率和相关性，通过研究每个设备-服务对的Contextual表示。</li>
<li>methods: 提议的框架使用Latent features combination技术捕捉设备之间的Latent特征交互，并使用Factorization Machines模型每个SIoT设备-服务对的高阶特征交互。</li>
<li>results: 实验证明框架可以提高服务推荐准确率和相关性。<details>
<summary>Abstract</summary>
The Social Internet of Things (SIoT) enables interconnected smart devices to share data and services, opening up opportunities for personalized service recommendations. However, existing research often overlooks crucial aspects that can enhance the accuracy and relevance of recommendations in the SIoT context. Specifically, existing techniques tend to consider the extraction of social relationships between devices and neglect the contextual presentation of service reviews. This study aims to address these gaps by exploring the contextual representation of each device-service pair. Firstly, we propose a latent features combination technique that can capture latent feature interactions, by aggregating the device-device relationships within the SIoT. Then, we leverage Factorization Machines to model higher-order feature interactions specific to each SIoT device-service pair to accomplish accurate rating prediction. Finally, we propose a service recommendation framework for SIoT based on review aggregation and feature learning processes. The experimental evaluation demonstrates the framework's effectiveness in improving service recommendation accuracy and relevance.
</details>
<details>
<summary>摘要</summary>
社交互联网关系（SIoT）可以让智能设备之间进行数据和服务之间的共享，从而开启了个性化服务推荐的可能性。然而，现有的研究往往忽视了在SIoT上的重要因素，这些因素可以提高推荐的准确性和相关性。具体来说，现有的技术通常会忽视设备之间的社交关系和服务评价的上下文显示。本研究旨在解决这些缺陷，通过对每个设备-服务对的上下文表示进行描述。首先，我们提出一种秘密特征组合技术，可以捕捉设备之间的秘密特征互动，通过在SIoT中对设备之间的关系进行聚合。然后，我们利用因子分解机制来模型每个SIoT设备-服务对的高阶特征互动，以实现准确的评级预测。最后，我们提出了基于评价聚合和特征学习的服务推荐框架 дляSIoT。实验评估表明该框架可以提高服务推荐的准确性和相关性。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Predictions-Enhanced-Expert-guided-Meshing-with-Graph-Neural-Networks"><a href="#Conformal-Predictions-Enhanced-Expert-guided-Meshing-with-Graph-Neural-Networks" class="headerlink" title="Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks"></a>Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07358">http://arxiv.org/abs/2308.07358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahnobari/autosurf">https://github.com/ahnobari/autosurf</a></li>
<li>paper_authors: Amin Heyrani Nobari, Justin Rey, Suhas Kodali, Matthew Jones, Faez Ahmed</li>
<li>for: 这个论文是为了自动生成CFD模型的网格而设计的。</li>
<li>methods: 这个论文使用图 Nueral Networks（GNN）和专家指导来自动生成CFD模型的网格。它还提出了一种新的3D分割算法，可以更高效地分类表面。</li>
<li>results: 论文通过一个实际案例研究表明，自动生成的网格与专家生成的网格相当，并且可以使得计算机 fluid dynamics 方法 converge 并生成准确的结果。此外，论文还比较了自动生成网格和适应重新网格两种方法的效率，发现自动生成网格比适应重新网格5倍 faster。代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/ahnobari/AutoSurf%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/ahnobari/AutoSurf上下载。</a><details>
<summary>Abstract</summary>
Computational Fluid Dynamics (CFD) is widely used in different engineering fields, but accurate simulations are dependent upon proper meshing of the simulation domain. While highly refined meshes may ensure precision, they come with high computational costs. Similarly, adaptive remeshing techniques require multiple simulations and come at a great computational cost. This means that the meshing process is reliant upon expert knowledge and years of experience. Automating mesh generation can save significant time and effort and lead to a faster and more efficient design process. This paper presents a machine learning-based scheme that utilizes Graph Neural Networks (GNN) and expert guidance to automatically generate CFD meshes for aircraft models. In this work, we introduce a new 3D segmentation algorithm that outperforms two state-of-the-art models, PointNet++ and PointMLP, for surface classification. We also present a novel approach to project predictions from 3D mesh segmentation models to CAD surfaces using the conformal predictions method, which provides marginal statistical guarantees and robust uncertainty quantification and handling. We demonstrate that the addition of conformal predictions effectively enables the model to avoid under-refinement, hence failure, in CFD meshing even for weak and less accurate models. Finally, we demonstrate the efficacy of our approach through a real-world case study that demonstrates that our automatically generated mesh is comparable in quality to expert-generated meshes and enables the solver to converge and produce accurate results. Furthermore, we compare our approach to the alternative of adaptive remeshing in the same case study and find that our method is 5 times faster in the overall process of simulation. The code and data for this project are made publicly available at https://github.com/ahnobari/AutoSurf.
</details>
<details>
<summary>摘要</summary>
computational fluid dynamics (CFD) 在不同的工程领域广泛使用，但准确的 simulate 需要正确的刻分 simulation 领域。高度细化的 mesh 可能确保准确性，但会带来高计算成本。同时，自适应刻分技术需要多次 simulate 和高计算成本。这意味着刻分过程依赖于专业知识和年资。自动生成 mesh 可以 savesignificant time and effort, leading to a faster and more efficient design process.这篇文章提出了一种基于机器学习的方案，使用 Graph Neural Networks (GNN) 和专家指导来自动生成 CFD 模型的 mesh  для飞机模型。在这种工作中，我们提出了一种新的3D 分割算法，超过了 PointNet++ 和 PointMLP 两种surface classification 模型。我们还提出了一种将预测从 3D  mesh 分割模型 projet 到 CAD 表面的方法，使用 conformal predictions 方法，该方法提供了边缘统计保证和稳定的uncertainty quantification和处理。我们证明了，通过添加conformal predictions，模型可以避免under-refinement，因此失败，在 CFD 刻分中。最后，我们通过一个真实的案例研究，证明我们自动生成的 mesh 与专家生成的 mesh 相比较，并且能够使解ilder converge 并生成准确的结果。此外，我们与 adaptive remeshing 的相对比较，发现我们的方法比 adaptive remeshing 5 倍快。我们的代码和数据在 https://github.com/ahnobari/AutoSurf 上公开 disponibles。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Prompt-tuning-for-Sequential-Recommendation"><a href="#Knowledge-Prompt-tuning-for-Sequential-Recommendation" class="headerlink" title="Knowledge Prompt-tuning for Sequential Recommendation"></a>Knowledge Prompt-tuning for Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08459">http://arxiv.org/abs/2308.08459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhaijianyang/kp4sr">https://github.com/zhaijianyang/kp4sr</a></li>
<li>paper_authors: Jianyang Zhai, Xiawu Zheng, Chang-Dong Wang, Hui Li, Yonghong Tian</li>
<li>for: 本研究的目的是提出一种以知识库为基础的sequential recommendation（SR）方法，以解决现有SR方法缺乏域知识和细化用户喜好的问题。</li>
<li>methods: 本研究提出了一种名为Knowledge Prompt-tuning for Sequential Recommendation（KP4SR）的方法，它利用了外部知识库和知识提示来解决 semantic gap 问题。而知识提示的执行方式包括构建关系模板和知识树，以及在知识树上应用知识树面罩来缓解噪声问题。</li>
<li>results: 实验结果显示，KP4SR方法在三个真实世界数据集上的评价指标上比现有的PLM-based方法（基于语言模型）表现出色，特别是在NDCG@5和HR@5指标上有40.65%、36.42%和22.17%的提升。<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) have demonstrated strong performance in sequential recommendation (SR), which are utilized to extract general knowledge. However, existing methods still lack domain knowledge and struggle to capture users' fine-grained preferences. Meanwhile, many traditional SR methods improve this issue by integrating side information while suffering from information loss. To summarize, we believe that a good recommendation system should utilize both general and domain knowledge simultaneously. Therefore, we introduce an external knowledge base and propose Knowledge Prompt-tuning for Sequential Recommendation (\textbf{KP4SR}). Specifically, we construct a set of relationship templates and transform a structured knowledge graph (KG) into knowledge prompts to solve the problem of the semantic gap. However, knowledge prompts disrupt the original data structure and introduce a significant amount of noise. We further construct a knowledge tree and propose a knowledge tree mask, which restores the data structure in a mask matrix form, thus mitigating the noise problem. We evaluate KP4SR on three real-world datasets, and experimental results show that our approach outperforms state-of-the-art methods on multiple evaluation metrics. Specifically, compared with PLM-based methods, our method improves NDCG@5 and HR@5 by \textcolor{red}{40.65\%} and \textcolor{red}{36.42\%} on the books dataset, \textcolor{red}{11.17\%} and \textcolor{red}{11.47\%} on the music dataset, and \textcolor{red}{22.17\%} and \textcolor{red}{19.14\%} on the movies dataset, respectively. Our code is publicly available at the link: \href{https://github.com/zhaijianyang/KP4SR}{\textcolor{blue}{https://github.com/zhaijianyang/KP4SR}.}
</details>
<details>
<summary>摘要</summary>
预训语言模型（PLM）在sequential recommendation（SR）中表现出了强大的能力，但现有方法仍缺乏域知识和用户细致的偏好。而 tradicional SR 方法通常通过integrating side information来解决这个问题，但这会导致信息损失。因此，我们认为一个好的推荐系统应该同时利用通用知识和域知识。因此，我们引入了外部知识库和建议 Knowledge Prompt-tuning for Sequential Recommendation（KP4SR）。我们构建了一组关系模板，将结构化知识图（KG）转换为知识提示，以解决 semantic gap 问题。然而，知识提示会破坏原始数据结构并引入很多噪声。我们进一步构建了知识树和知识树面罩，使得数据结构在面罩矩阵形式中得到修复，因此 Mitigate the noise problem。我们在三个真实世界数据集上测试了我们的方法，结果显示，我们的方法在多个评价指标上超越了现有方法。具体来说，与 PLM 基于方法相比，我们的方法在书籍数据集上提高 NDCG@5 和 HR@5 的值为 \red{40.65\%} 和 \red{36.42\%}，在音乐数据集上提高 \red{11.17\%} 和 \red{11.47\%}，在电影数据集上提高 \red{22.17\%} 和 \red{19.14\%}，分别。我们的代码公开available于以下链接：\href{https://github.com/zhaijianyang/KP4SR}{\textcolor{blue}{https://github.com/zhaijianyang/KP4SR}.}
</details></li>
</ul>
<hr>
<h2 id="Demonstration-of-CORNET-A-System-For-Learning-Spreadsheet-Formatting-Rules-By-Example"><a href="#Demonstration-of-CORNET-A-System-For-Learning-Spreadsheet-Formatting-Rules-By-Example" class="headerlink" title="Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example"></a>Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07357">http://arxiv.org/abs/2308.07357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mukul Singh, Jose Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen</li>
<li>for: 用于自动学习条件 formatting 规则，并将其应用到 Microsoft Excel 中。</li>
<li>methods: 使用 симвоlic rule enumeration、 semi-supervised clustering 和 iterative decision tree learning，以及 neural ranker 来生成条件 formatting 规则。</li>
<li>results: 可以将用户提供的一两个格式化 cell 作为示范，然后生成 formatting rule 建议供用户应用到 spreadsheet 中。<details>
<summary>Abstract</summary>
Data management and analysis tasks are often carried out using spreadsheet software. A popular feature in most spreadsheet platforms is the ability to define data-dependent formatting rules. These rules can express actions such as "color red all entries in a column that are negative" or "bold all rows not containing error or failure." Unfortunately, users who want to exercise this functionality need to manually write these conditional formatting (CF) rules. We introduce CORNET, a system that automatically learns such conditional formatting rules from user examples. CORNET takes inspiration from inductive program synthesis and combines symbolic rule enumeration, based on semi-supervised clustering and iterative decision tree learning, with a neural ranker to produce accurate conditional formatting rules. In this demonstration, we show CORNET in action as a simple add-in to Microsoft Excel. After the user provides one or two formatted cells as examples, CORNET generates formatting rule suggestions for the user to apply to the spreadsheet.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese数据管理和分析任务经常使用表格软件进行。许多表格平台具有定义数据依赖的格式化规则的功能。这些规则可以表达如"将列中的负数颜色为红"或"不包含错误或失败的行加粗"。然而，用户们想要实现这些Conditional Formatting（CF）规则需要手动写出这些规则。我们介绍了CORNET，一个系统可以自动从用户示例中学习Conditional Formatting规则。CORNET吸取了 inductive 程序生成的灵感，结合半supervised clustering和迭代决策树学习，以生成准确的Conditional Formatting规则。在这次演示中，我们将CORNET作为Microsoft Excel中的一个简单插件展示。在用户提供一个或二个格式化的单元格示例后，CORNET会生成格式化规则建议，让用户应用于表格。
</details></li>
</ul>
<hr>
<h2 id="SPEGTI-Structured-Prediction-for-Efficient-Generative-Text-to-Image-Models"><a href="#SPEGTI-Structured-Prediction-for-Efficient-Generative-Text-to-Image-Models" class="headerlink" title="SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models"></a>SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10997">http://arxiv.org/abs/2308.10997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadeep Jayasumana, Daniel Glasner, Srikumar Ramalingam, Andreas Veit, Ayan Chakrabarti, Sanjiv Kumar</li>
<li>for: 提高文本描述生成图像质量和效率</li>
<li>methods: 使用Markov Random Field（MRF）模型提高图像兼容性和减少Muse预测步骤</li>
<li>results: 提高Muse模型的运算效率，无损输出图像质量Here’s a brief explanation of each point:* “for”: The paper is aimed at improving the quality and efficiency of text-to-image generation models.* “methods”: The paper proposes using a Markov Random Field (MRF) model to improve the compatibility between different regions of an image, which in turn speeds up the Muse model.* “results”: The proposed method, called SPEGTI, achieves a 1.5X speedup in Muse inference with no loss in output image quality.<details>
<summary>Abstract</summary>
Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MRF inference as a differentiable neural-network layer. Our full model, SPEGTI, uses this proposed MRF model to speed up Muse by 1.5X with no loss in output image quality.
</details>
<details>
<summary>摘要</summary>
现代文本到图像生成模型可以生成高质量的图像，这些图像不仅具有摄影真实性，还具有文本提示的准确性。然而，这种质量需要支付高计算成本：大多数这些模型都是迭代的，需要在大型模型上进行多次推理。这种迭代过程是为确保图像各个区域不仅与文本提示相Alignment，还与其他区域兼容。在这项工作中，我们提出了一种轻量级的方法来实现图像各个区域之间的兼容性，使用Markov随机场（MRF）模型。这种方法可以与最近提出的Muse模型一起使用，并且可以减少Muse预测步骤的数量，从而大幅降低计算成本。我们的全模型，SPEGTI，使用这种提出的MRF模型，可以在Muse模型中快速预测图像，并且不会失去输出图像质量。
</details></li>
</ul>
<hr>
<h2 id="HyperBandit-Contextual-Bandit-with-Hypernewtork-for-Time-Varying-User-Preferences-in-Streaming-Recommendation"><a href="#HyperBandit-Contextual-Bandit-with-Hypernewtork-for-Time-Varying-User-Preferences-in-Streaming-Recommendation" class="headerlink" title="HyperBandit: Contextual Bandit with Hypernewtork for Time-Varying User Preferences in Streaming Recommendation"></a>HyperBandit: Contextual Bandit with Hypernewtork for Time-Varying User Preferences in Streaming Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08497">http://arxiv.org/abs/2308.08497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenglei Shen, Xiao Zhang, Wei Wei, Jun Xu</li>
<li>for: 本研究旨在提出一种能够快速适应用户时间变化的流媒体推荐模型，以满足实际流媒体推荐场景中用户偏好的动态变化。</li>
<li>methods: 本研究提出了一种Contextual Bandit方法，使用了hypernetwork来模型时间变化的用户偏好，并采用了bandit策略来在线进行推荐。为了满足实时要求，研究者们在训练过程中利用了低级结构的low-rank factorization。</li>
<li>results: 对实际 dataset进行了广泛的实验，并证明了 HyperBandit 能够在流媒体推荐场景中具有优于现有基eline的表现，并且可以快速适应用户时间变化。<details>
<summary>Abstract</summary>
In real-world streaming recommender systems, user preferences often dynamically change over time (e.g., a user may have different preferences during weekdays and weekends). Existing bandit-based streaming recommendation models only consider time as a timestamp, without explicitly modeling the relationship between time variables and time-varying user preferences. This leads to recommendation models that cannot quickly adapt to dynamic scenarios. To address this issue, we propose a contextual bandit approach using hypernetwork, called HyperBandit, which takes time features as input and dynamically adjusts the recommendation model for time-varying user preferences. Specifically, HyperBandit maintains a neural network capable of generating the parameters for estimating time-varying rewards, taking into account the correlation between time features and user preferences. Using the estimated time-varying rewards, a bandit policy is employed to make online recommendations by learning the latent item contexts. To meet the real-time requirements in streaming recommendation scenarios, we have verified the existence of a low-rank structure in the parameter matrix and utilize low-rank factorization for efficient training. Theoretically, we demonstrate a sublinear regret upper bound against the best policy. Extensive experiments on real-world datasets show that the proposed HyperBandit consistently outperforms the state-of-the-art baselines in terms of accumulated rewards.
</details>
<details>
<summary>摘要</summary>
实际流媒体推荐系统中，用户偏好经常在时间上变化（例如，用户在工作日和周末有不同的偏好）。现有的铲剑基于推荐模型仅考虑时间为毫科学上的时间戳，没有明确模型时间变量和用户偏好之间的关系。这会导致推荐模型无法快速适应动态场景。为解决这个问题，我们提议一种 Contextual Bandit 方法，使用嵌入式神经网络（HyperNetwork），以时间特征为输入，动态调整用户偏好变化的推荐模型。具体来说，HyperBandit 保持一个可以生成时间变量相关的参数来估计用户偏好变化的神经网络，同时考虑用户偏好和时间特征之间的相关性。使用估计的时间变量奖励，采用铲剑策略进行在线推荐，学习隐藏的项目上下文。为满足流媒体推荐场景的实时需求，我们已经验证了低纬度结构的存在，并利用低纬度因子化进行高效的训练。理论上，我们证明了对最佳策略的下界 regret Upper Bound。实际实验表明，提议的 HyperBandit 在实际 dataset 上持续超过状态艺术基elines。
</details></li>
</ul>
<hr>
<h2 id="AIGC-In-China-Current-Developments-And-Future-Outlook"><a href="#AIGC-In-China-Current-Developments-And-Future-Outlook" class="headerlink" title="AIGC In China: Current Developments And Future Outlook"></a>AIGC In China: Current Developments And Future Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08451">http://arxiv.org/abs/2308.08451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Li, Yuqing Fan, Shenghui Cheng</li>
<li>for: 本研究旨在分析中国AI生成内容（AIGC）领域的当前状况，包括技术基础和应用领域。</li>
<li>methods: 本研究使用关键词搜索方法来 Identify relevant academic papers and analyze the market status, policy landscape, and development trajectory of AIGC in China.</li>
<li>results: 研究发现，中国AIGC领域正在快速发展，但也面临着一些挑战和风险。本研究提供了一个全面的AIGC产品和相关生态系统的分析，以及对AIGC产业未来发展的前瞻性分析。<details>
<summary>Abstract</summary>
The increasing attention given to AI Generated Content (AIGC) has brought a profound impact on various aspects of daily life, industrial manufacturing, and the academic sector. Recognizing the global trends and competitiveness in AIGC development, this study aims to analyze China's current status in the field. The investigation begins with an overview of the foundational technologies and current applications of AIGC. Subsequently, the study delves into the market status, policy landscape, and development trajectory of AIGC in China, utilizing keyword searches to identify relevant scholarly papers. Furthermore, the paper provides a comprehensive examination of AIGC products and their corresponding ecosystem, emphasizing the ecological construction of AIGC. Finally, this paper discusses the challenges and risks faced by the AIGC industry while presenting a forward-looking perspective on the industry's future based on competitive insights in AIGC.
</details>
<details>
<summary>摘要</summary>
随着人工智能生成内容（AIGC）的注意力增加，它对日常生活、工业生产和学术领域产生了深远的影响。在认识全球趋势和竞争力的基础上，本研究目的是分析中国AIGC领域的当前状况。研究从基础技术和当前应用领域入手，然后探讨中国AIGC市场情况、政策风景和发展轨迹，通过关键词搜索获取相关学术论文。此外，本文还进行了全面的AIGC产品和相关生态系统的检视，强调AIGC生态建设。最后，本文讨论了AIGC行业所面临的挑战和风险，并提供了基于竞争情况的未来展望。
</details></li>
</ul>
<hr>
<h2 id="OctoPack-Instruction-Tuning-Code-Large-Language-Models"><a href="#OctoPack-Instruction-Tuning-Code-Large-Language-Models" class="headerlink" title="OctoPack: Instruction Tuning Code Large Language Models"></a>OctoPack: Instruction Tuning Code Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07124">http://arxiv.org/abs/2308.07124</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bigcode-project/octopack">https://github.com/bigcode-project/octopack</a></li>
<li>paper_authors: Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, Shayne Longpre</li>
<li>for: 这篇论文的目的是精致地训练大型自然语言模型（LLMs），以提高自然语言任务的性能。</li>
<li>methods: 这篇论文使用了代码的自然结构，即Git提交，并将其转换为人类的指令。 authors 创建了 CommitPack，一个包含了350种程式语言的4 terabytes Git提交。</li>
<li>results: 在使用 CommitPack 训练 StarCoder 模型（16B参数）时，在 HumanEval Python 测试 benchmark 上 achieved state-of-the-art 性能（46.2% pass@1），并在 HumanEvalPack 测试 benchmark 上显示了最好的性能。<details>
<summary>Abstract</summary>
Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的微调使得自然语言任务中的表现得到了很大的改善。我们使用代码的自然结构，通过 Git 提交来进行 instruction tuning，并compile了 4 兆Byte的 Git 提交 across 350 种程式语言。我们将其与其他自然和合成代码指令（xP3x、Self-Instruct、OASST）进行比较，使用 16B 参数的 StarCoder 模型，在 HumanEval Python 套件中获得了最佳性能（46.2% pass@1）。我们还引入了 HumanEvalPack，扩展了 HumanEval 套件，包括 3 个程式码任务（Code Repair、Code Explanation、Code Synthesis） across 6 种程式语言（Python、JavaScript、Java、Go、C++、Rust）。我们的模型 OctoCoder 和 OctoGeeX 在 HumanEvalPack 中获得了最佳性能，证明 CommitPack 对于更多的语言和自然程式码任务具有普遍性。代码、模型和数据可以免费下载于 https://github.com/bigcode-project/octopack。
</details></li>
</ul>
<hr>
<h2 id="CTP-Towards-Vision-Language-Continual-Pretraining-via-Compatible-Momentum-Contrast-and-Topology-Preservation"><a href="#CTP-Towards-Vision-Language-Continual-Pretraining-via-Compatible-Momentum-Contrast-and-Topology-Preservation" class="headerlink" title="CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation"></a>CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07146">http://arxiv.org/abs/2308.07146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kevinlight831/ctp">https://github.com/kevinlight831/ctp</a></li>
<li>paper_authors: Hongguang Zhu, Yunchao Wei, Xiaodan Liang, Chunjie Zhang, Yao Zhao</li>
<li>for: 这个论文是为了研究视觉语言不间断预训练（Vision-Language Continual Pretraining，VLCP）而写的。</li>
<li>methods: 这个论文使用了一种新的算法，即兼容势量对比法（Compatible Momentum Contrast），以及一种概率转移方法（Topology Preservation）来实现VLCP。</li>
<li>results: 实验结果表明，这个算法不仅可以在多个基线上达到更高的性能，而且也不需要付出贵重的训练成本。<details>
<summary>Abstract</summary>
Vision-Language Pretraining (VLP) has shown impressive results on diverse downstream tasks by offline training on large-scale datasets. Regarding the growing nature of real-world data, such an offline training paradigm on ever-expanding data is unsustainable, because models lack the continual learning ability to accumulate knowledge constantly. However, most continual learning studies are limited to uni-modal classification and existing multi-modal datasets cannot simulate continual non-stationary data stream scenarios. To support the study of Vision-Language Continual Pretraining (VLCP), we first contribute a comprehensive and unified benchmark dataset P9D which contains over one million product image-text pairs from 9 industries. The data from each industry as an independent task supports continual learning and conforms to the real-world long-tail nature to simulate pretraining on web data. We comprehensively study the characteristics and challenges of VLCP, and propose a new algorithm: Compatible momentum contrast with Topology Preservation, dubbed CTP. The compatible momentum model absorbs the knowledge of the current and previous-task models to flexibly update the modal feature. Moreover, Topology Preservation transfers the knowledge of embedding across tasks while preserving the flexibility of feature adjustment. The experimental results demonstrate our method not only achieves superior performance compared with other baselines but also does not bring an expensive training burden. Dataset and codes are available at https://github.com/KevinLight831/CTP.
</details>
<details>
<summary>摘要</summary>
vision-language预训练（VLP）在多种下游任务上表现出色，但由于实际数据的不断增长，这种离线训练方式在不断学习的能力不足以满足需求。然而，大多数不断学习研究仅限于单modal类别，现有的多modal数据集不能模拟不断非站ARY数据流场景。为支持视图语言不断预训练（VLCP）的研究，我们首先提供了一个完整的、统一的benchmark数据集P9D，包含超过一百万个产品图像-文本对from 9个行业。每个行业的数据作为独立任务支持不断学习，并与实际世界的长尾分布相符，以模拟在网络数据上的预训练。我们系统地研究了VLCP的特点和挑战，并提出了一种新的算法：Compatible Momentum Contrast with Topology Preservation（CTP）。Compatible Momentum模型吸收当前和前一任务模型的知识，以flexibly更新Modal特征。此外，Topology Preservation将知识传递到下一任务模型，保持特征调整的灵活性。实验结果表明，我们的方法不仅在其他基eline上达到了superior表现，而且不会带来昂贵的训练负担。数据集和代码可以在https://github.com/KevinLight831/CTP获取。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-is-All-a-Graph-Needs"><a href="#Natural-Language-is-All-a-Graph-Needs" class="headerlink" title="Natural Language is All a Graph Needs"></a>Natural Language is All a Graph Needs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07134">http://arxiv.org/abs/2308.07134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang</li>
<li>for: 本研究的目的是探讨 Whether large language models (LLMs) can replace graph neural networks (GNNs) as the foundation model for graphs.</li>
<li>methods: 我们提出了 InstructGLM (Instruction-finetuned Graph Language Model)，使用自然语言指令系统atically design高级别可扩展的提示，并使用自然语言描述图像的几何结构和节点特征。</li>
<li>results: 我们的方法在ogbn-arxiv、Cora和PubMed dataset上都超过了所有竞争性GNN基线值，这表明了我们的方法的有效性，并且探讨了大语言模型作为图像机器学习的基础模型的可能性。<details>
<summary>Abstract</summary>
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.
</details>
<details>
<summary>摘要</summary>
大规模预训练语言模型，如ChatGPT，在人工智能多个研究领域中引发革命。基于Transformers的大语言模型（LLM）逐渐取代了CNNs和RNNs，将计算机视觉和自然语言处理等领域联系起来。与独立存在的数据类型如图像、视频或文本不同，图表是一种包含丰富结构和关系信息的数据类型。同时，自然语言作为最具表达力的媒介，能够描述复杂结构。然而，将图学学习问题 incorporated into the generative language modeling framework 的现有工作很有限。随着大语言模型的重要性不断增长，我们认为可以explore whether LLMs can also replace GNNs as the foundation model for graphs。在这篇论文中，我们提出了InstructGLM（基于natural language instruction的图语言模型），系统地设计了可扩展的提示，并使用自然语言来描述图表的几何结构和节点特征。通过对LLM进行学习和推理，我们实现了对图表的生成式学习和推理。我们的方法在ogbn-arxiv、Cora和PubMed dataset上超过了所有相关GNN基elines，这 demonstartes了我们的方法的效果，并且推翻了大语言模型作为图机器学习基础模型的可能性。
</details></li>
</ul>
<hr>
<h2 id="Implementation-of-The-Future-of-Drug-Discovery-QuantumBased-Machine-Learning-Simulation-QMLS"><a href="#Implementation-of-The-Future-of-Drug-Discovery-QuantumBased-Machine-Learning-Simulation-QMLS" class="headerlink" title="Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS)"></a>Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08561">http://arxiv.org/abs/2308.08561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yew Kee Wong, Yifan Zhou, Yan Shing Liang, Haichuan Qiu, Yu Xi Wu, Bin He</li>
<li>for: 这个研究旨在缩短药物开发过程的R&amp;D阶段，从三到六个月，并降低成本至五万到八万美元。</li>
<li>methods: 这个概念使用机器学习和量子模拟来发现可能的灵数，并将其筛选以根据反应和与目标蛋白质的缩合效果进行筛选。</li>
<li>results: 这个概念可以将R&amp;D阶段缩短至三到六个月，并降低成本至五万到八万美元，并生成多达数十个适用于临床试验的药物。<details>
<summary>Abstract</summary>
The Research & Development (R&D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction effectiveness and safety, creating a few dozen pre-clinical-trail-ready drugs. This paper is based on our first paper, where we pitched the concept of machine learning combined with quantum simulations. In this paper we will go over the detailed design and framework of QMLS, including MLMG, MLMV, and QS.
</details>
<details>
<summary>摘要</summary>
研发（R&D）阶段是药品开发的长期和昂贵的过程。为了革新这个过程，我们提出了新的概念——快速药品开发（QMLS），可以缩短整个R&D阶段到3-6个月，并降低成本至50-80万美元。在找到可能的杀手（Hit）方面，机器学习分子生成（MLMG）根据目标蛋白质的分子结构生成可能的杀手，而量子模拟（QS）从原始试验中筛选出符合目标蛋白质的反应和结合效果的分子。在吸引化学物质阶段，得到的分子被QS筛选后，通过机器学习分子变化（MLMV）生成多种分子变体，而其他分子只生成一些变体。最后，所有优化的分子都会经过多次QS筛选，以确保它们具有高效性和安全性。通过这种方式，我们可以在几个月内生成几十个前期临床药品。这篇文章是我们的第一篇论文，我们在那里提出了机器学习与量子模拟的概念。在这篇文章中，我们将详细介绍QMLS的设计和框架，包括MLMG、MLMV和QS。
</details></li>
</ul>
<hr>
<h2 id="Ada-QPacknet-–-adaptive-pruning-with-bit-width-reduction-as-an-efficient-continual-learning-method-without-forgetting"><a href="#Ada-QPacknet-–-adaptive-pruning-with-bit-width-reduction-as-an-efficient-continual-learning-method-without-forgetting" class="headerlink" title="Ada-QPacknet – adaptive pruning with bit width reduction as an efficient continual learning method without forgetting"></a>Ada-QPacknet – adaptive pruning with bit width reduction as an efficient continual learning method without forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07939">http://arxiv.org/abs/2308.07939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcin Pietroń, Dominik Żurek, Kamil Faber, Roberto Corizzo</li>
<li>for: This paper aims to improve the efficiency of Continual Learning (CL) algorithms in dynamic and complex environments.</li>
<li>methods: The proposed approach, called Ada-QPacknet, incorporates both pruning and quantization techniques to reduce the size of the model and improve its performance in CL scenarios.</li>
<li>results: The presented results show that the proposed approach achieves similar accuracy as floating-point sub-networks in well-known CL scenarios, and outperforms most other CL strategies in task and class incremental scenarios.Here’s the full text in Simplified Chinese:</li>
<li>for: 本文目的是提高深度学习模型在动态复杂环境下的效率，通过Continual Learning（CL）算法。</li>
<li>methods: 提议的方法是Ada-QPacknet，它将杜绝和量化技术相结合，以减少模型的大小并提高CL场景中的性能。</li>
<li>results:  presente results表明，提议的方法在知名CL场景中与浮点子网络相当的准确率，并在任务和类增量场景中超过大多数CL策略。<details>
<summary>Abstract</summary>
Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combinations and compared with most popular algorithms. Results show that proposed approach outperforms most of the CL strategies in task and class incremental scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="InsTag-Instruction-Tagging-for-Analyzing-Supervised-Fine-tuning-of-Large-Language-Models"><a href="#InsTag-Instruction-Tagging-for-Analyzing-Supervised-Fine-tuning-of-Large-Language-Models" class="headerlink" title="#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models"></a>#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07074">http://arxiv.org/abs/2308.07074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ofa-sys/instag">https://github.com/ofa-sys/instag</a></li>
<li>paper_authors: Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, Jingren Zhou</li>
<li>for: 这篇论文主要用于探讨基于监督精细调教（SFT）的语言模型是如何获得 instrucion-following 能力的？</li>
<li>methods: 该论文提出了一种名为 InsTag 的开放集成精细标注器，用于标注 SFT 数据集中的样本，并定义了 instrucion 多样性和复杂性的量化分析。</li>
<li>results: 研究发现，通过使用 InsTag 选择的 6K 多样性和复杂性的样本进行精细调教，可以使模型的能力得到显著提升，并在 MT-Bench 中与大量 SFT 数据进行比较。<details>
<summary>Abstract</summary>
Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and complexity. We open-source InsTag in https://github.com/OFA-Sys/InsTag.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Machine-Unlearning-Solutions-and-Challenges"><a href="#Machine-Unlearning-Solutions-and-Challenges" class="headerlink" title="Machine Unlearning: Solutions and Challenges"></a>Machine Unlearning: Solutions and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07061">http://arxiv.org/abs/2308.07061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Xu, Zihan Wu, Cong Wang, Xiaohua Jia</li>
<li>for: 本研究旨在提供一个全面的机器学习忘记研究taxonomy，并对现有研究进行分析和评价。</li>
<li>methods: 本研究使用了精确的忘记算法和近似的忘记方法，并对这些方法进行了分析和评价。</li>
<li>results: 本研究提出了未来机器学习忘记研究的发展方向，并鼓励研究人员通过解决实际问题来提供影响ful的贡献。<details>
<summary>Abstract</summary>
Machine learning models may inadvertently memorize sensitive, unauthorized, or malicious data, posing risks of privacy violations, security breaches, and performance deterioration. To address these issues, machine unlearning has emerged as a critical technique to selectively remove specific training data points' influence on trained models. This paper provides a comprehensive taxonomy and analysis of machine unlearning research. We categorize existing research into exact unlearning that algorithmically removes data influence entirely and approximate unlearning that efficiently minimizes influence through limited parameter updates. By reviewing the state-of-the-art solutions, we critically discuss their advantages and limitations. Furthermore, we propose future directions to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning. This paper provides researchers with a roadmap of open problems, encouraging impactful contributions to address real-world needs for selective data removal.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:机器学习模型可能不慎地记忆敏感、未经授权或黑客数据，导致隐私侵犯、安全泄露和性能下降。为解决这些问题，机器忘记技术已经出现，可以选择性地从训练模型中除去特定数据点的影响。这篇论文提供了机器忘记的全面分类和分析，将现有研究分为精确忘记和近似忘记两类。我们 kritisch 评估了现状的解决方案，并提出了未来的发展方向，以便在可靠和适应性Machine learning中确立机器忘记的能力。这篇论文为研究人员提供了一份未解决的问题路线图，鼓励他们对实际需求进行有力的贡献，以解决实际中的选择性数据 removals。
</details></li>
</ul>
<hr>
<h2 id="Distinguishing-Risk-Preferences-using-Repeated-Gambles"><a href="#Distinguishing-Risk-Preferences-using-Repeated-Gambles" class="headerlink" title="Distinguishing Risk Preferences using Repeated Gambles"></a>Distinguishing Risk Preferences using Repeated Gambles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07054">http://arxiv.org/abs/2308.07054</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Price, Colm Connaughton</li>
<li>For: The paper explores the practical challenges of inferring risk preferences from the observed choices of artificial agents in sequences of repeated gambles.* Methods: The paper uses the Yeo-Johnson transformation to construct a family of gambles that interpolates smoothly between the additive and multiplicative cases, and analyzes the optimal strategy for this family both analytically and numerically.* Results: The paper finds that it becomes increasingly difficult to distinguish the risk preferences of agents as their wealth increases, because agents with different risk preferences eventually make the same decisions for sufficiently high wealth.<details>
<summary>Abstract</summary>
Sequences of repeated gambles provide an experimental tool to characterize the risk preferences of humans or artificial decision-making agents. The difficulty of this inference depends on factors including the details of the gambles offered and the number of iterations of the game played. In this paper we explore in detail the practical challenges of inferring risk preferences from the observed choices of artificial agents who are presented with finite sequences of repeated gambles. We are motivated by the fact that the strategy to maximize long-run wealth for sequences of repeated additive gambles (where gains and losses are independent of current wealth) is different to the strategy for repeated multiplicative gambles (where gains and losses are proportional to current wealth.) Accurate measurement of risk preferences would be needed to tell whether an agent is employing the optimal strategy or not. To generalize the types of gambles our agents face we use the Yeo-Johnson transformation, a tool borrowed from feature engineering for time series analysis, to construct a family of gambles that interpolates smoothly between the additive and multiplicative cases. We then analyze the optimal strategy for this family, both analytically and numerically. We find that it becomes increasingly difficult to distinguish the risk preferences of agents as their wealth increases. This is because agents with different risk preferences eventually make the same decisions for sufficiently high wealth. We believe that these findings are informative for the effective design of experiments to measure risk preferences in humans.
</details>
<details>
<summary>摘要</summary>
sequences of repeated gambles 提供了一种实验工具来描述人类或人工决策代理的风险偏好。这种推断的困难程度取决于因素，包括对投注的细节和游戏的数量。在这篇论文中，我们详细探讨了人工代理在重复的加权投注中的实际挑战。我们被激励于因为在重复的加权投注中，最佳长期财富战略和加权投注战略不同。准确测量风险偏好是必要的，以确定代理是否使用最佳策略。为推广代理面临的投注类型，我们使用 Yeo-Johnson 变换，一种从特征工程中借鉴的时间序列分析工具，构建了一家 interpolation between additive and multiplicative cases 的投注家族。然后，我们分析了这家族中的最佳策略，包括分析和数值方法。我们发现，随着代理的财富增加，代理的风险偏好难以分辨。这是因为代理不同的风险偏好在财富增加到足够高的时候会做出同样的决策。我们认为这些发现对人类风险偏好测量的设计有用。
</details></li>
</ul>
<hr>
<h2 id="Diagnosis-of-Scalp-Disorders-using-Machine-Learning-and-Deep-Learning-Approach-–-A-Review"><a href="#Diagnosis-of-Scalp-Disorders-using-Machine-Learning-and-Deep-Learning-Approach-–-A-Review" class="headerlink" title="Diagnosis of Scalp Disorders using Machine Learning and Deep Learning Approach – A Review"></a>Diagnosis of Scalp Disorders using Machine Learning and Deep Learning Approach – A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07052">http://arxiv.org/abs/2308.07052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishabh Tiwari, Jatin Moolchandani, Shamla Mantri</li>
<li>for: 该研究旨在提高scalp疾病的诊断精度和效率。</li>
<li>methods: 该研究使用了深度学习模型，包括CNN和FCN，以及一个扩展的APP，以实现精准的scalp疾病诊断。</li>
<li>results: 研究表明，使用深度学习模型可以实现scalp疾病诊断的高精度和高效率，其中一些研究达到了97.41%-99.09%的准确率，而其他研究则达到了82.9%和91.4%的准确率。<details>
<summary>Abstract</summary>
The morbidity of scalp diseases is minuscule compared to other diseases, but the impact on the patient's life is enormous. It is common for people to experience scalp problems that include Dandruff, Psoriasis, Tinea-Capitis, Alopecia and Atopic-Dermatitis. In accordance with WHO research, approximately 70% of adults have problems with their scalp. It has been demonstrated in descriptive research that hair quality is impaired by impaired scalp, but these impacts are reversible with early diagnosis and treatment. Deep Learning advances have demonstrated the effectiveness of CNN paired with FCN in diagnosing scalp and skin disorders. In one proposed Deep-Learning-based scalp inspection and diagnosis system, an imaging microscope and a trained model are combined with an app that classifies scalp disorders accurately with an average precision of 97.41%- 99.09%. Another research dealt with classifying the Psoriasis using the CNN with an accuracy of 82.9%. As part of another study, an ML based algorithm was also employed. It accurately classified the healthy scalp and alopecia areata with 91.4% and 88.9% accuracy with SVM and KNN algorithms. Using deep learning models to diagnose scalp related diseases has improved due to advancements i computation capabilities and computer vision, but there remains a wide horizon for further improvements.
</details>
<details>
<summary>摘要</summary>
Scalp 疾病的患病率相对其他疾病较低，但对病人生活的影响很大。人们常常会经历Scalp 问题，包括斑点病、 Psoriasis、Tinea-Capitis、 Alopecia 和 Atopic-Dermatitis。根据Who研究，成人约70%有Scalp 问题。研究表明，发现早期Scalp 问题可以有效地改善毛发质量，但这些影响可以逆转。在 Deep Learning 技术的推动下，CNN 与 FCN 的结合已经在诊断Scalp 和皮肤疾病方面达到了高度的准确率。一种提议的 Deep-Learning-based scalp 检查和诊断系统使用了一个快速的 imaging 镜和一个训练好的模型，并与一个APP结合，可以准确地分类Scalp 疾病，其准确率为97.41%-99.09%。另一项研究则是使用CNN来分类 Psoriasis，其准确率为82.9%。在另一项研究中，一种ML 基于的算法也被使用，可以准确地分类健康的Scalp 和 Alopecia areata，其准确率为91.4%和88.9%。使用 Deep learning 模型诊断Scalp 相关疾病的精度已经得到了进一步提高，但还有很大的可exploration空间。
</details></li>
</ul>
<hr>
<h2 id="The-minimal-computational-substrate-of-fluid-intelligence"><a href="#The-minimal-computational-substrate-of-fluid-intelligence" class="headerlink" title="The minimal computational substrate of fluid intelligence"></a>The minimal computational substrate of fluid intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07039">http://arxiv.org/abs/2308.07039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amy PK Nelson, Joe Mole, Guilherme Pombo, Robert J Gray, James K Ruffle, Edgar Chan, Geraint E Rees, Lisa Cipolotti, Parashkev Nachev</li>
<li>for: 这个研究是用来评估一个改进后的Raven进攻性智能测试（RAPM），以验证人类水平的智商测试能否通过自我指导的人工神经网络（LaMa）完成。</li>
<li>methods: 这个研究使用了LaMa自我指导的人工神经网络，该网络只在完成部分遮盖的自然环境场景图像上进行了自我学习。</li>
<li>results: 研究发现，LaMa在完成RAPM测试时达到了人类水平的成绩，而且与健康参与者和focus lesion参与者的表现类似，并且在损害右前额叶功能时出现了类似的错误。<details>
<summary>Abstract</summary>
The quantification of cognitive powers rests on identifying a behavioural task that depends on them. Such dependence cannot be assured, for the powers a task invokes cannot be experimentally controlled or constrained a priori, resulting in unknown vulnerability to failure of specificity and generalisability. Evaluating a compact version of Raven's Advanced Progressive Matrices (RAPM), a widely used clinical test of fluid intelligence, we show that LaMa, a self-supervised artificial neural network trained solely on the completion of partially masked images of natural environmental scenes, achieves human-level test scores a prima vista, without any task-specific inductive bias or training. Compared with cohorts of healthy and focally lesioned participants, LaMa exhibits human-like variation with item difficulty, and produces errors characteristic of right frontal lobe damage under degradation of its ability to integrate global spatial patterns. LaMa's narrow training and limited capacity -- comparable to the nervous system of the fruit fly -- suggest RAPM may be open to computationally simple solutions that need not necessarily invoke abstract reasoning.
</details>
<details>
<summary>摘要</summary>
评估认知能力的量化基于确定一个行为任务取决于它们。但是，这种依赖性不可控制，因为任务所调用的能力无法在实验上预先控制或受限制，导致不知之处的失败和不一致。我们评估了一个简化版的鸟智慧进步性测验（RAPM），一种广泛用于诊断智商的临床测试，我们发现LaMa，一个自我指导的人工神经网络，在完全不受任务指导的情况下，直接完成部分遮盖的自然环境场景图像的完成任务，可以达到人类水平的测试得分。与健康参与者和损伤参与者的群组相比，LaMa表现出人类化的变化，并且在Item难度上出现了人类类似的错误。LaMa的窄训练和有限容量（与蜂蜜蜂 nervous system相当）表明，RAPM可能是一个计算简单的解决方案，不需要涉及抽象的理性。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Flow-Networks"><a href="#Bayesian-Flow-Networks" class="headerlink" title="Bayesian Flow Networks"></a>Bayesian Flow Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07037">http://arxiv.org/abs/2308.07037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stefanradev93/BayesFlow">https://github.com/stefanradev93/BayesFlow</a></li>
<li>paper_authors: Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, Faustino Gomez</li>
<li>for: 这篇论文探讨了一种新的生成模型，即权重投影网络（BFN），它利用 bayesian 推理来修改一组独立的分布参数，然后将这些参数作为神经网络的输入，输出一个第二个、相互依赖的分布。</li>
<li>methods: 该模型使用了 bayesian 推理来修改参数，然后通过神经网络输出一个生成分布。在实验中，使用了不同的损失函数，包括离散和连续时间损失函数，以及采样生成过程。</li>
<li>results: 实验表明，BFNs 可以与其他权重投影模型相比，在静止化 MNIST 和 CIFAR-10 图像模型任务上实现竞争性的 log-likelihood，并在文本8 字符级语言模型任务上超越所有已知的离散扩散模型。<details>
<summary>Abstract</summary>
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no restrictions on the network architecture. In our experiments BFNs achieve competitive log-likelihoods for image modelling on dynamically binarized MNIST and CIFAR-10, and outperform all known discrete diffusion models on the text8 character-level language modelling task.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文介绍了概率流网络（BFN），一种新的生成模型，其中概率流网络中参数的修改使用权化推断，然后通过神经网络输出第二个、相互dependent的分布。这个过程类似于扩散模型的反向过程，但是更简单，不需要前向过程。模型可以处理整数、连续和整数化数据，并且神经网络输入的整数数据位于概率 simpliciter 上，因此可以使用导数下降的技术进行批处理和几步生成。损失函数直接优化数据压缩，不受网络结构限制。在实验中，BFNs在动态 binary MNIST 和 CIFAR-10 上实现了图像模型的竞争性Log-likelihood，并在 text8 字符级语言模型任务上超过了所有已知整数扩散模型。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Physics-Informed-Neural-Network-for-the-Forward-and-Inverse-Simulation-of-Engineered-Nano-particles-Mobility-in-a-Contaminated-Aquifer"><a href="#Bayesian-Physics-Informed-Neural-Network-for-the-Forward-and-Inverse-Simulation-of-Engineered-Nano-particles-Mobility-in-a-Contaminated-Aquifer" class="headerlink" title="Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer"></a>Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07352">http://arxiv.org/abs/2308.07352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Nilabh, Fidel Grandia</li>
<li>for: 这研究旨在开发一种可预测的工程尺度材料浸泡环境中的气体传输模型，以便为地下水污染 Site 的整体环境和生态系统进行有效的恢复。</li>
<li>methods: 该研究使用了一种基于 Bayesian Physics-Informed Neural Network（B-PINN）的方法，模拟了气体在aquifer中的传输行为。</li>
<li>results: 研究结果表明，B-PINN 方法可以高度准确地预测气体的传输行为，并且可以量化不确定性。此外，研究还发现了aquifer中气体传输的主要控制因素。这些结果表明，该工具可以为开发有效的地下水污染 Site 整治策略提供预测性的 Insights。<details>
<summary>Abstract</summary>
Globally, there are many polluted groundwater sites that need an active remediation plan for the restoration of local ecosystem and environment. Engineered nanoparticles (ENPs) have proven to be an effective reactive agent for the in-situ degradation of pollutants in groundwater. While the performance of these ENPs has been highly promising on the laboratory scale, their application in real field case conditions is still limited. The complex transport and retention mechanisms of ENPs hinder the development of an efficient remediation strategy. Therefore, a predictive tool to comprehend the transport and retention behavior of ENPs is highly required. The existing tools in the literature are dominated with numerical simulators, which have limited flexibility and accuracy in the presence of sparse datasets and the aquifer heterogeneity. This work uses a Bayesian Physics-Informed Neural Network (B-PINN) framework to model the nano-particles mobility within an aquifer. The result from the forward model demonstrates the effective capability of B-PINN in accurately predicting the ENPs mobility and quantifying the uncertainty. The inverse model output is then used to predict the governing parameters for the ENPs mobility in a small-scale aquifer. The research demonstrates the capability of the tool to provide predictive insights for developing an efficient groundwater remediation strategy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="IOB-Integrating-Optimization-Transfer-and-Behavior-Transfer-for-Multi-Policy-Reuse"><a href="#IOB-Integrating-Optimization-Transfer-and-Behavior-Transfer-for-Multi-Policy-Reuse" class="headerlink" title="IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse"></a>IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07351">http://arxiv.org/abs/2308.07351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Li, Hao Li, Jin Zhang, Zhen Wang, Peng Liu, Chongjie Zhang</li>
<li>for: 本研究旨在解决选择适用于目标任务的源策略的挑战，提出了一种新的转移学习RL方法。</li>
<li>methods: 该方法利用actor-critic框架中的Q函数导引策略选择，选择目标策略的最大一步改进的源策略。它还 combining optimization transfer和behavior transfer（IOB），通过准则学习的策略来模仿导引策略，以提高转移效果。</li>
<li>results: 对于基准任务，该方法超过了现有的转移RL基线，并在持续学习场景中提高了最终性和知识传递性。此外，我们证明了该优化传输技术可以提高目标策略学习。<details>
<summary>Abstract</summary>
Humans have the ability to reuse previously learned policies to solve new tasks quickly, and reinforcement learning (RL) agents can do the same by transferring knowledge from source policies to a related target task. Transfer RL methods can reshape the policy optimization objective (optimization transfer) or influence the behavior policy (behavior transfer) using source policies. However, selecting the appropriate source policy with limited samples to guide target policy learning has been a challenge. Previous methods introduce additional components, such as hierarchical policies or estimations of source policies' value functions, which can lead to non-stationary policy optimization or heavy sampling costs, diminishing transfer effectiveness. To address this challenge, we propose a novel transfer RL method that selects the source policy without training extra components. Our method utilizes the Q function in the actor-critic framework to guide policy selection, choosing the source policy with the largest one-step improvement over the current target policy. We integrate optimization transfer and behavior transfer (IOB) by regularizing the learned policy to mimic the guidance policy and combining them as the behavior policy. This integration significantly enhances transfer effectiveness, surpasses state-of-the-art transfer RL baselines in benchmark tasks, and improves final performance and knowledge transferability in continual learning scenarios. Additionally, we show that our optimization transfer technique is guaranteed to improve target policy learning.
</details>
<details>
<summary>摘要</summary>
人类具有重用已经学习的策略来快速解决新任务的能力，而强化学习（RL）代理也可以通过将来源策略传播到相关的目标任务中来传递知识。传递RL方法可以改变策略优化目标（优化传递）或影响行为策略（行为传递）使用源策略。然而，在有限样本情况下选择合适的源策略是一个挑战。先前的方法可能会添加额外的组件，如层次政策或估计源策略的价值函数，这可能会导致非站点策略优化或重大的采样成本， thereby reducing transfer effectiveness.为了解决这个挑战，我们提出了一种新的传递RL方法，不需要训练额外的组件。我们利用actor-critic框架中的Q函数来导引策选择，选择目标策略中最大化一步改进的源策略。我们将优化传递和行为传递（IOB）相结合，通过对学习的策略进行正则化，使其模仿指导策略，并将其与行为策略相结合。这种结合显著提高了传递效果，超过了状态静态的传递RL基准值，并提高了最终性和知识传递性在不断学习场景中。此外，我们证明了我们的优化传递技术可以确保目标策略学习的改进。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Neural-PDE-Solvers-using-Quantization-Aware-Training"><a href="#Efficient-Neural-PDE-Solvers-using-Quantization-Aware-Training" class="headerlink" title="Efficient Neural PDE-Solvers using Quantization Aware Training"></a>Efficient Neural PDE-Solvers using Quantization Aware Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07350">http://arxiv.org/abs/2308.07350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Winfried van den Dool, Tijmen Blankevoort, Max Welling, Yuki M. Asano</li>
<li>for: 解决Partial Differential Equations（PDE）领域中计算成本的问题，通过使用神经网络作为传统数学方法的替代方案。</li>
<li>methods: 使用现有的量化方法来降低神经网络的计算成本，而不需要限制PDE的空间分辨率。</li>
<li>results: 对四个标准PDE数据集和三种网络架构进行了实验，并证明了在不同的设置下，量化意识训练可以成功降低计算成本，同时保持性能。此外，我们还证明了将计算成本与性能之间的 pareto优化是通过量化来实现的。<details>
<summary>Abstract</summary>
In the past years, the application of neural networks as an alternative to classical numerical methods to solve Partial Differential Equations has emerged as a potential paradigm shift in this century-old mathematical field. However, in terms of practical applicability, computational cost remains a substantial bottleneck. Classical approaches try to mitigate this challenge by limiting the spatial resolution on which the PDEs are defined. For neural PDE solvers, we can do better: Here, we investigate the potential of state-of-the-art quantization methods on reducing computational costs. We show that quantizing the network weights and activations can successfully lower the computational cost of inference while maintaining performance. Our results on four standard PDE datasets and three network architectures show that quantization-aware training works across settings and three orders of FLOPs magnitudes. Finally, we empirically demonstrate that Pareto-optimality of computational cost vs performance is almost always achieved only by incorporating quantization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Aggregating-Intrinsic-Information-to-Enhance-BCI-Performance-through-Federated-Learning"><a href="#Aggregating-Intrinsic-Information-to-Enhance-BCI-Performance-through-Federated-Learning" class="headerlink" title="Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning"></a>Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11636">http://arxiv.org/abs/2308.11636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Liu, Yuanyuan Chen, Anran Li, Yi Ding, Han Yu, Cuntai Guan</li>
<li>for: 提高Brain-Computer Interface（BCI）高性能深度学习模型的建立</li>
<li>methods: 提出了一种 Hierarchical Personalized Federated Learning EEG Decoding（FLEEG）框架，通过协同学习多个数据集，提高模型的泛化能力和稳定性</li>
<li>results: 在 Motor Imagery（MI）分类任务中，与9个不同设备收集的EEG数据集进行了合作训练，可以提高分类性能达16.7%，特别是对小数据集的提升更大。<details>
<summary>Abstract</summary>
Insufficient data is a long-standing challenge for Brain-Computer Interface (BCI) to build a high-performance deep learning model. Though numerous research groups and institutes collect a multitude of EEG datasets for the same BCI task, sharing EEG data from multiple sites is still challenging due to the heterogeneity of devices. The significance of this challenge cannot be overstated, given the critical role of data diversity in fostering model robustness. However, existing works rarely discuss this issue, predominantly centering their attention on model training within a single dataset, often in the context of inter-subject or inter-session settings. In this work, we propose a hierarchical personalized Federated Learning EEG decoding (FLEEG) framework to surmount this challenge. This innovative framework heralds a new learning paradigm for BCI, enabling datasets with disparate data formats to collaborate in the model training process. Each client is assigned a specific dataset and trains a hierarchical personalized model to manage diverse data formats and facilitate information exchange. Meanwhile, the server coordinates the training procedure to harness knowledge gleaned from all datasets, thus elevating overall performance. The framework has been evaluated in Motor Imagery (MI) classification with nine EEG datasets collected by different devices but implementing the same MI task. Results demonstrate that the proposed frame can boost classification performance up to 16.7% by enabling knowledge sharing between multiple datasets, especially for smaller datasets. Visualization results also indicate that the proposed framework can empower the local models to put a stable focus on task-related areas, yielding better performance. To the best of our knowledge, this is the first end-to-end solution to address this important challenge.
</details>
<details>
<summary>摘要</summary>
BCIs 长期面临不充分数据的挑战，建立高性能深度学习模型。虽然许多研究机构和机构收集了多个 EEG 数据集，但是共享 EEG 数据从多个地点仍然困难，主要因为设备的不同。这个挑战的重要性不可遗憾，因为数据多样性对模型的稳定性具有关键作用。然而，现有的研究很少讨论这一问题，通常是在单个数据集的训练方法上围绕着间subject或间 session 设置中做出主要听讲。在这种情况下，我们提出了一种层次个性化联合学习 EEG 解码（FLEEG）框架，以超越这一挑战。这种创新的框架标识了一种新的学习 paradigma  для BCIs，使得不同数据格式的数据集可以在模型训练过程中合作。每个客户端被分配特定的数据集，并训练一个层次个性化模型来管理多种数据格式和促进信息交换。同时，服务器协调训练过程，以利用所有数据集中获得的知识，从而提高整体性能。我们在 Motor Imagery （MI） 分类任务上使用了九个 EEG 数据集，每个数据集都是由不同的设备收集的，但是实现了同一个 MI 任务。结果表明，我们的框架可以提高分类性能达到 16.7%，尤其是 для小型数据集。视觉结果还表明，我们的框架可以让本地模型固定焦点在任务相关的区域，从而提高表现。到目前为止，我们知道这是第一个综合解决这一重要挑战的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder"><a href="#Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder" class="headerlink" title="Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder"></a>Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08488">http://arxiv.org/abs/2308.08488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mispchallenge/misp-icme-avsr">https://github.com/mispchallenge/misp-icme-avsr</a></li>
<li>paper_authors: Yusheng Dai, Hang Chen, Jun Du, Xiaofei Ding, Ning Ding, Feijun Jiang, Chin-Hui Lee</li>
<li>for: 提高音频视频演示系统（AVSR）的表现，特别是在低质量视频下。</li>
<li>methods: 提出两种新技术：一是基于 Mandarin 语音形态的 lip shape 和 syllable-level subword unit 的关系研究，以获得准确的视频和音频流重叠。二是一种 audio-guided cross-modal fusion encoder（CMFE）神经网络，使用多个交叠模式来完全利用多modal complementarity。</li>
<li>results: 在 MISP2021-AVSR 数据集上进行实验，证明了两种提posed技术的有效性。使用只需相对较少的训练数据，最终系统的表现优于一些更复杂的前端和后端的现有系统。<details>
<summary>Abstract</summary>
In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the MISP2021-AVSR data set show the effectiveness of the two proposed techniques. Together, using only a relatively small amount of training data, the final system achieves better performances than state-of-the-art systems with more complex front-ends and back-ends.
</details>
<details>
<summary>摘要</summary>
现在的研究显示，自动音频识别系统在端到端框架中对低质量视频进行自动化识别时会有轻微的性能提升。这是因为音频和视频模式之间的匹配速率和特殊输入表示方式存在差异。在这篇论文中，我们提出了两种新的技术来改进音频视频识别（AVSR），并在预训练和精度调整训练框架下进行评估。首先，我们研究了拟合舌形和句子级别的子音单元之间的相关性，以确定准确的帧级句子界限。这使得视频和音频流之间的同步Alignment能够更加精准。然后，我们提出了一种听音导向的交叉模态融合编程（CMFE）神经网络，以利用主要训练参数在多个交叉模态注意层中进行全面的融合。实验表明，使用这两种提posed技术后，只需使用相对较小的训练数据，最终系统可以在与更复杂的前端和后端的状态-of-the-art系统相比，达到更高的性能。
</details></li>
</ul>
<hr>
<h2 id="pNNCLR-Stochastic-Pseudo-Neighborhoods-for-Contrastive-Learning-based-Unsupervised-Representation-Learning-Problems"><a href="#pNNCLR-Stochastic-Pseudo-Neighborhoods-for-Contrastive-Learning-based-Unsupervised-Representation-Learning-Problems" class="headerlink" title="pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems"></a>pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06983">http://arxiv.org/abs/2308.06983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Momojit Biswas, Himanshu Buckchash, Dilip K. Prasad</li>
<li>for: 本研究旨在提高 nearest neighbor 基于自助学习（SSL）的图像识别问题中的 semantic variation。</li>
<li>methods: 本研究使用 nearest neighbor  sampling 提供更多的 semantic variation，并引入 pseudo nearest neighbors（pNN）控制支持集质量，以提高表现。 </li>
<li>results: 对多个公共图像识别和医学图像识别数据集进行评估，本方法与基eline nearest neighbor 方法相比，性能提高约8%，与其他先前提出的 SSL 方法相比几乎相当。<details>
<summary>Abstract</summary>
Nearest neighbor (NN) sampling provides more semantic variations than pre-defined transformations for self-supervised learning (SSL) based image recognition problems. However, its performance is restricted by the quality of the support set, which holds positive samples for the contrastive loss. In this work, we show that the quality of the support set plays a crucial role in any nearest neighbor based method for SSL. We then provide a refined baseline (pNNCLR) to the nearest neighbor based SSL approach (NNCLR). To this end, we introduce pseudo nearest neighbors (pNN) to control the quality of the support set, wherein, rather than sampling the nearest neighbors, we sample in the vicinity of hard nearest neighbors by varying the magnitude of the resultant vector and employing a stochastic sampling strategy to improve the performance. Additionally, to stabilize the effects of uncertainty in NN-based learning, we employ a smooth-weight-update approach for training the proposed network. Evaluation of the proposed method on multiple public image recognition and medical image recognition datasets shows that it performs up to 8 percent better than the baseline nearest neighbor method, and is comparable to other previously proposed SSL methods.
</details>
<details>
<summary>摘要</summary>
近邻采样（NN）提供更多semantic变化 than pre-defined transformations for self-supervised learning（SSL）based image recognition problems. However, its performance is restricted by the quality of the support set, which holds positive samples for the contrastive loss. In this work, we show that the quality of the support set plays a crucial role in any nearest neighbor based method for SSL. We then provide a refined baseline（pNNCLR）to the nearest neighbor based SSL approach（NNCLR）. To this end, we introduce pseudo nearest neighbors（pNN）to control the quality of the support set, wherein, rather than sampling the nearest neighbors, we sample in the vicinity of hard nearest neighbors by varying the magnitude of the resultant vector and employing a stochastic sampling strategy to improve the performance. Additionally, to stabilize the effects of uncertainty in NN-based learning, we employ a smooth-weight-update approach for training the proposed network. Evaluation of the proposed method on multiple public image recognition and medical image recognition datasets shows that it performs up to 8 percent better than the baseline nearest neighbor method, and is comparable to other previously proposed SSL methods.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese languages used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Routing-Recovery-for-UAV-Networks-with-Deliberate-Attacks-A-Reinforcement-Learning-based-Approach"><a href="#Routing-Recovery-for-UAV-Networks-with-Deliberate-Attacks-A-Reinforcement-Learning-based-Approach" class="headerlink" title="Routing Recovery for UAV Networks with Deliberate Attacks: A Reinforcement Learning based Approach"></a>Routing Recovery for UAV Networks with Deliberate Attacks: A Reinforcement Learning based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06973">http://arxiv.org/abs/2308.06973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijie He, Ziye Jia, Chao Dong, Wei Wang, Yilu Cao, Yang Yang, Qihui Wu</li>
<li>for: 本研究针对无人机网络中的攻击问题，提出了一种路由计划和恢复策略。</li>
<li>methods: 本文提出了一种基于重要性分析的节点重要性排名机制，并使用了人工智能技术来恢复路由路径。</li>
<li>results: 实验结果表明，提出的方法比其他已知方法更高效，能够有效地恢复路由路径并提高无人机网络的可靠性。<details>
<summary>Abstract</summary>
The unmanned aerial vehicle (UAV) network is popular these years due to its various applications. In the UAV network, routing is significantly affected by the distributed network topology, leading to the issue that UAVs are vulnerable to deliberate damage. Hence, this paper focuses on the routing plan and recovery for UAV networks with attacks. In detail, a deliberate attack model based on the importance of nodes is designed to represent enemy attacks. Then, a node importance ranking mechanism is presented, considering the degree of nodes and link importance. However, it is intractable to handle the routing problem by traditional methods for UAV networks, since link connections change with the UAV availability. Hence, an intelligent algorithm based on reinforcement learning is proposed to recover the routing path when UAVs are attacked. Simulations are conducted and numerical results verify the proposed mechanism performs better than other referred methods.
</details>
<details>
<summary>摘要</summary>
自这些年来，无人飞行器（UAV）网络已经非常受欢迎，因为它们在各种应用方面表现出了优异的能力。在UAV网络中，路由受到分布式网络架构的影响，导致UAV受到意外损害。因此，这篇论文关注于UAV网络中的路由规划和恢复，以适应攻击。具体来说，我们设计了一种基于节点重要性的敌对攻击模型，并提出了一种考虑节点度和链接重要性的节点重要性排名机制。但是，由于UAV网络中的链接连接不断变化，因此传统的路由方法无法应对UAV网络中的攻击。因此，我们提出了一种基于强化学习的智能算法，以恢复UAV网络中的路由路径当UAV受到攻击时。我们进行了 simulations 和数值分析，并证明了我们提出的机制在UAV网络中恢复路由路径时比其他已知方法更好。
</details></li>
</ul>
<hr>
<h2 id="BIRP-Bitcoin-Information-Retrieval-Prediction-Model-Based-on-Multimodal-Pattern-Matching"><a href="#BIRP-Bitcoin-Information-Retrieval-Prediction-Model-Based-on-Multimodal-Pattern-Matching" class="headerlink" title="BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching"></a>BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08558">http://arxiv.org/abs/2308.08558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsuk Kim, Byungchul Kim, Junyeong Yong, Jeongwoo Park, Gyeongmin Kim</li>
<li>for: 该论文旨在提出一种基于PC模式匹配算法的方向预测模型，以提高对财务时间序列的预测能力。</li>
<li>methods: 该论文使用了多模式匹配算法来检测财务市场中的征兆，并将PC模式匹配结果作为额外特征来提高方向预测模型的准确性。</li>
<li>results: 研究人员通过应用该方法在比特币市场中进行了方向预测，并发现该方法可以提高方向预测的准确性。<details>
<summary>Abstract</summary>
Financial time series have historically been assumed to be a martingale process under the Random Walk hypothesis. Instead of making investment decisions using the raw prices alone, various multimodal pattern matching algorithms have been developed to help detect subtly hidden repeatable patterns within the financial market. Many of the chart-based pattern matching tools only retrieve similar past chart (PC) patterns given the current chart (CC) pattern, and leaves the entire interpretive and predictive analysis, thus ultimately the final investment decision, to the investors. In this paper, we propose an approach of ranking similar PC movements given the CC information and show that exploiting this as additional features improves the directional prediction capacity of our model. We apply our ranking and directional prediction modeling methodologies on Bitcoin due to its highly volatile prices that make it challenging to predict its future movements.
</details>
<details>
<summary>摘要</summary>
金融时间序列历史上通常被视为一个martingale过程，而不是使用 raw 价格做投资决策。多种多模式匹配算法已经被开发出来帮助检测金融市场中的潜在征性重复模式。许多图表基本 pattern matching 工具只是根据当前图表（CC）提供类似过去图表（PC）模式，留下整个解释和预测分析，最终决策，给投资者。在这篇论文中，我们提出一种基于 CC 信息对 PC 运动进行排名的方法，并证明在我们的模型中利用这些特征可以提高方向预测能力。我们在使用我们的排名和方向预测模型方法时选择比特币，因为它的价格波动性较高，使其预测未来运动更加挑战。
</details></li>
</ul>
<hr>
<h2 id="Graph-Structural-Residuals-A-Learning-Approach-to-Diagnosis"><a href="#Graph-Structural-Residuals-A-Learning-Approach-to-Diagnosis" class="headerlink" title="Graph Structural Residuals: A Learning Approach to Diagnosis"></a>Graph Structural Residuals: A Learning Approach to Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06961">http://arxiv.org/abs/2308.06961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Lukas Augustin, Oliver Niggemann</li>
<li>for: 提出了一种基于深度图结构学习的数据驱动的系统诊断方法，它可以轻松地集成图结构学习和模型基于诊断。</li>
<li>methods: 使用两个不同的自适应图结构学习模型架构，通过自动学习系统的下游结构来提取系统的动态观察数据，并将系统的构造、观察和缺陷重新定义。</li>
<li>results: 通过对振荡器系统的实验， demonstate了该方法的可行性和效果，并证明了该方法可以提供更加准确和有效的系统诊断。<details>
<summary>Abstract</summary>
Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
</details>
<details>
<summary>摘要</summary>
传统的模型基于诊断方法是通过构建明确的系统模型来进行，这可能是劳动密集且需要专业知识的。在这篇论文中，我们提出了一种新的框架，它将模型基于诊断与深度图结构学习结合起来。这种数据驱动的方法利用数据来学习系统的下面结构，并提供动态观察结果，表示为两个不同的图邻接矩阵。我们的工作使得图结构学习与模型基于诊断的集成变得自然和简单，我们做出了三个主要贡献：（一）重新定义系统表示、观察和故障的构造（二）引入两种不同的自我超级vised图结构学习模型架构（三）通过对振荡器系统进行实验，证明我们的数据驱动诊断方法的潜力。
</details></li>
</ul>
<hr>
<h2 id="Search-to-Fine-tune-Pre-trained-Graph-Neural-Networks-for-Graph-level-Tasks"><a href="#Search-to-Fine-tune-Pre-trained-Graph-Neural-Networks-for-Graph-level-Tasks" class="headerlink" title="Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks"></a>Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06960">http://arxiv.org/abs/2308.06960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Wang, Shimin Di, Lei Chen, Xiaofang Zhou</li>
<li>for: This paper aims to design a better fine-tuning strategy for pre-trained graph neural networks (GNNs) to improve their performance on downstream graph-level tasks.</li>
<li>methods: The proposed method, called S2PGNN, searches for an appropriate fine-tuning framework for the given labeled data on the downstream task, adaptively designing a suitable strategy for each task.</li>
<li>results: The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Additionally, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area.<details>
<summary>Abstract</summary>
Recently, graph neural networks (GNNs) have shown its unprecedented success in many graph-related tasks. However, GNNs face the label scarcity issue as other neural networks do. Thus, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs to improve the model performance in this paper. Given a pre-trained GNN, we propose to search to fine-tune pre-trained graph neural networks for graph-level tasks (S2PGNN), which adaptively design a suitable fine-tuning framework for the given labeled data on the downstream task. To ensure the improvement brought by searching fine-tuning strategy, we carefully summarize a proper search space of fine-tuning framework that is suitable for GNNs. The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Besides, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area. Our code is publicly available at \url{https://anonymous.4open.science/r/code_icde2024-A9CB/}.
</details>
<details>
<summary>摘要</summary>
近些年来，图节点网络（GNNs）在许多图关联任务上显示出无前例的成功。然而，GNNs还面临着标签稀缺问题，与其他神经网络一样。因此，current efforts是在大规模无标记图上预训练GNNs，然后将知识从无标记图 adapts到目标下游任务。适应通常是通过精度调整预训练GNNs中的一部分参数来实现。 despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs to improve the model performance in this paper. Given a pre-trained GNN, we propose to search for fine-tune pre-trained graph neural networks for graph-level tasks (S2PGNN), which adaptively designs a suitable fine-tuning framework for the given labeled data on the downstream task. To ensure the improvement brought by searching fine-tuning strategy, we carefully summarize a proper search space of fine-tuning framework that is suitable for GNNs. The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Besides, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area. Our code is publicly available at \url{https://anonymous.4open.science/r/code_icde2024-A9CB/}.
</details></li>
</ul>
<hr>
<h2 id="Approximating-Human-Like-Few-shot-Learning-with-GPT-based-Compression"><a href="#Approximating-Human-Like-Few-shot-Learning-with-GPT-based-Compression" class="headerlink" title="Approximating Human-Like Few-shot Learning with GPT-based Compression"></a>Approximating Human-Like Few-shot Learning with GPT-based Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06942">http://arxiv.org/abs/2308.06942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cynthia Huang, Yuqing Xie, Zhiying Jiang, Jimmy Lin, Ming Li</li>
<li>For: 本研究旨在塑造生成模型具备人类学习能力，以便在推理过程中进行数据压缩。* Methods: 本文提出了一种使用生成预训练模型（GPT）来估计kolmogorov复杂度，以便在几何学习中进行数据压缩。* Results: 实验结果表明，使用GPT模型作为压缩约束可以实现15.5倍的压缩率，并且对于困难的NLG任务（包括语义相似性、零和一极少文本分类和零极少文本排名）具有优秀的性能。<details>
<summary>Abstract</summary>
In this work, we conceptualize the learning process as information compression. We seek to equip generative pre-trained models with human-like learning capabilities that enable data compression during inference. We present a novel approach that utilizes the Generative Pre-trained Transformer (GPT) to approximate Kolmogorov complexity, with the aim of estimating the optimal Information Distance for few-shot learning. We first propose using GPT as a prior for lossless text compression, achieving a noteworthy compression ratio. Experiment with LLAMA2-7B backbone achieves a compression ratio of 15.5 on enwik9. We justify the pre-training objective of GPT models by demonstrating its equivalence to the compression length, and, consequently, its ability to approximate the information distance for texts. Leveraging the approximated information distance, our method allows the direct application of GPT models in quantitative text similarity measurements. Experiment results show that our method overall achieves superior performance compared to embedding and prompt baselines on challenging NLP tasks, including semantic similarity, zero and one-shot text classification, and zero-shot text ranking.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们概念化学习过程为信息压缩。我们希望为生成预训练模型增加人类学习能力，以便在推理过程中进行数据压缩。我们提出了一种新的方法，利用生成预训练变换器（GPT）来近似kolmogorov复杂度，以便估算少量学习中的最佳信息距离。我们首先提出使用GPT作为损失less文本压缩的先验，实现了一个很好的压缩比例。在LLAMA2-7B底层上进行实验，实现了enwik9上的压缩比例为15.5。我们证明了GPT模型的预训练目标的正确性，并且因此能够近似信息距离的估算，从而使得GPT模型可以直接应用于文本相似度量度中。通过利用近似信息距离，我们的方法可以在挑战性的NLP任务中实现超越预测和描述基elines的性能。
</details></li>
</ul>
<hr>
<h2 id="FusionPlanner-A-Multi-task-Motion-Planner-for-Mining-Trucks-using-Multi-sensor-Fusion-Method"><a href="#FusionPlanner-A-Multi-task-Motion-Planner-for-Mining-Trucks-using-Multi-sensor-Fusion-Method" class="headerlink" title="FusionPlanner: A Multi-task Motion Planner for Mining Trucks using Multi-sensor Fusion Method"></a>FusionPlanner: A Multi-task Motion Planner for Mining Trucks using Multi-sensor Fusion Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06931">http://arxiv.org/abs/2308.06931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Teng, Luxi Li, Yuchen Li, Xuemin Hu, Lingxi Li, Yunfeng Ai, Long Chen<br>for:This paper proposes a comprehensive paradigm for unmanned transportation in open-pit mines, including a simulation platform, a testing benchmark, and a trustworthy and robust motion planner.methods:The paper proposes a multi-task motion planning algorithm called FusionPlanner, which uses a multi-sensor fusion method to adapt both lateral and longitudinal control tasks for unmanned transportation.results:The performance of FusionPlanner is tested by MiningNav in PMS, and the empirical results demonstrate a significant reduction in the number of collisions and takeovers of their planner.<details>
<summary>Abstract</summary>
In recent years, significant achievements have been made in motion planning for intelligent vehicles. However, as a typical unstructured environment, open-pit mining attracts limited attention due to its complex operational conditions and adverse environmental factors. A comprehensive paradigm for unmanned transportation in open-pit mines is proposed in this research, including a simulation platform, a testing benchmark, and a trustworthy and robust motion planner. \textcolor{red}{Firstly, we propose a multi-task motion planning algorithm, called FusionPlanner, for autonomous mining trucks by the Multi-sensor fusion method to adapt both lateral and longitudinal control tasks for unmanned transportation. Then, we develop a novel benchmark called MiningNav, which offers three validation approaches to evaluate the trustworthiness and robustness of well-trained algorithms in transportation roads of open-pit mines. Finally, we introduce the Parallel Mining Simulator (PMS), a new high-fidelity simulator specifically designed for open-pit mining scenarios. PMS enables the users to manage and control open-pit mine transportation from both the single-truck control and multi-truck scheduling perspectives.} \textcolor{red}{The performance of FusionPlanner is tested by MiningNav in PMS, and the empirical results demonstrate a significant reduction in the number of collisions and takeovers of our planner. We anticipate our unmanned transportation paradigm will bring mining trucks one step closer to trustworthiness and robustness in continuous round-the-clock unmanned transportation.
</details>
<details>
<summary>摘要</summary>
近年来，在智能汽车运动规划方面有了 significative achievements。然而，由于开采矿场的复杂操作条件和不利环境因素，这种场景吸引了有限的关注。本研究提出了一种涵盖全面的无人运输解决方案，包括仿真平台、测试标准和可靠性和稳定性较高的运动规划算法。首先，我们提出了一种多任务运动规划算法，称为FusionPlanner，用于自动采矿车辆的多感器融合方法，以适应无人运输中的 lateral和longitudinal控制任务。然后，我们开发了一个新的测试标准，称为MiningNav，它提供了三种验证方法来评估训练过的算法在交通路上的可靠性和稳定性。最后，我们介绍了一个新的高级仿真平台，称为Parallel Mining Simulator (PMS)，它专门针对开采矿场 scenarios。PMS允许用户在交通路上控制开采矿车辆的单车控制和多车调度两种视角。FusionPlanner的性能被MiningNav在PMS中测试，实际结果表明我们的 плаanner的数量紧急和takeover的减少了显著。我们预计我们的无人运输方案将使采矿车辆一步 closer to trustworthiness和稳定性在无人不断运输中。
</details></li>
</ul>
<hr>
<h2 id="FedEdge-AI-TC-A-Semi-supervised-Traffic-Classification-Method-based-on-Trusted-Federated-Deep-Learning-for-Mobile-Edge-Computing"><a href="#FedEdge-AI-TC-A-Semi-supervised-Traffic-Classification-Method-based-on-Trusted-Federated-Deep-Learning-for-Mobile-Edge-Computing" class="headerlink" title="FedEdge AI-TC: A Semi-supervised Traffic Classification Method based on Trusted Federated Deep Learning for Mobile Edge Computing"></a>FedEdge AI-TC: A Semi-supervised Traffic Classification Method based on Trusted Federated Deep Learning for Mobile Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06924">http://arxiv.org/abs/2308.06924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pan Wang, Zeyi Li, Mengyi Fu, Zixuan Wang, Ze Zhang, MinYao Liu<br>for: 本文旨在提出一种基于联合学习（Federated Learning，FL）的可靠网络流量分类（TC）框架，以提高5G客户端设备（CPE）中的TC性能。methods: 本文使用了机器学习（Machine Learning，ML）和深度学习（Deep Learning，DL）技术来提高TC性能，并提出了一种基于自变量自编码器（Variational Auto-Encoder，VAE）和卷积神经网络（Convolutional Neural Network，CNN）的半监督TC算法，以减少数据依赖性。此外，本文还提出了一种名为XAI-Pruning的AI模型压缩方法，以减少模型大小并保持模型解释性。results: 实验评估表明，基于FedEdge AI-TC框架的TC模型在精度和效率两个方面具有明显的优势，而且可以保护用户隐私和模型准确性。这种框架可以提高服务质量和安全性，因此具有广泛的应用前景。<details>
<summary>Abstract</summary>
As a typical entity of MEC (Mobile Edge Computing), 5G CPE (Customer Premise Equipment)/HGU (Home Gateway Unit) has proven to be a promising alternative to traditional Smart Home Gateway. Network TC (Traffic Classification) is a vital service quality assurance and security management method for communication networks, which has become a crucial functional entity in 5G CPE/HGU. In recent years, many researchers have applied Machine Learning or Deep Learning (DL) to TC, namely AI-TC, to improve its performance. However, AI-TC faces challenges, including data dependency, resource-intensive traffic labeling, and user privacy concerns. The limited computing resources of 5G CPE further complicate efficient classification. Moreover, the "black box" nature of AI-TC models raises transparency and credibility issues. The paper proposes the FedEdge AI-TC framework, leveraging Federated Learning (FL) for reliable Network TC in 5G CPE. FL ensures privacy by employing local training, model parameter iteration, and centralized training. A semi-supervised TC algorithm based on Variational Auto-Encoder (VAE) and convolutional neural network (CNN) reduces data dependency while maintaining accuracy. To optimize model light-weight deployment, the paper introduces XAI-Pruning, an AI model compression method combined with DL model interpretability. Experimental evaluation demonstrates FedEdge AI-TC's superiority over benchmarks in terms of accuracy and efficient TC performance. The framework enhances user privacy and model credibility, offering a comprehensive solution for dependable and transparent Network TC in 5G CPE, thus enhancing service quality and security.
</details>
<details>
<summary>摘要</summary>
为了提高5G CPE（客户端设备）/HGU（家庭网关）的服务质量和安全性，MEC（移动边缘计算）中的网络TC（流量分类）已成为一种有前途的替代方案。然而，使用人工智能（AI）进行TC（TC使用AI）存在一些挑战，包括数据依赖、负担重的流量标注和用户隐私问题。另外，TC模型的“黑盒”性也会导致透明度和信任问题。为了解决这些问题，本文提出了FedEdge AI-TC框架，利用联邦学习（FL）来确保5G CPE中的可靠网络TC。FL确保了隐私，通过本地训练、模型参数迭代和中心训练。使用变量自动编码器（VAE）和卷积神经网络（CNN）的半监督TC算法可以减少数据依赖而保持准确性。为了优化模型轻量级部署，本文介绍了XAI-Pruning，一种将AI模型压缩与DL模型解释相结合的模型压缩方法。实验评估表明FedEdge AI-TC在精度和效率方面与标准准点。这种框架提高了用户隐私和模型信任度，为5G CPE中可靠和透明的网络TC提供了全面的解决方案，从而提高服务质量和安全性。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-contingent-planning-based-on-HTN-for-high-quality-plans"><a href="#Probabilistic-contingent-planning-based-on-HTN-for-high-quality-plans" class="headerlink" title="Probabilistic contingent planning based on HTN for high-quality plans"></a>Probabilistic contingent planning based on HTN for high-quality plans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06922">http://arxiv.org/abs/2308.06922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Zhao</li>
<li>For: The paper is written for planning in partially observable environments, where traditional deterministic planning methods are not practical.* Methods: The paper proposes a probabilistic contingent Hierarchical Task Network (HTN) planner called High-Quality Contingent Planner (HQCP) to generate high-quality plans in partially observable environments. The planner extends HTN planning formalisms to partial observability and evaluates plans based on cost.* Results: The paper explores a novel heuristic for high-quality plans and develops an integrated planning algorithm. An empirical study verifies the effectiveness and efficiency of the planner in probabilistic contingent planning and obtaining high-quality plans.<details>
<summary>Abstract</summary>
Deterministic planning assumes that the planning evolves along a fully predictable path, and therefore it loses the practical value in most real projections. A more realistic view is that planning ought to take into consideration partial observability beforehand and aim for a more flexible and robust solution. What is more significant, it is inevitable that the quality of plan varies dramatically in the partially observable environment. In this paper we propose a probabilistic contingent Hierarchical Task Network (HTN) planner, named High-Quality Contingent Planner (HQCP), to generate high-quality plans in the partially observable environment. The formalisms in HTN planning are extended into partial observability and are evaluated regarding the cost. Next, we explore a novel heuristic for high-quality plans and develop the integrated planning algorithm. Finally, an empirical study verifies the effectiveness and efficiency of the planner both in probabilistic contingent planning and for obtaining high-quality plans.
</details>
<details>
<summary>摘要</summary>
<<SYS>>transliteration: zhèng zhì yì yù xiǎng zhèng zhì yì yù, yīn zhèng zhì yì yù de zhèng yì yù zhèng zhì yì yù. translated text:  deterministic planning assumes that the planning evolves along a fully predictable path, and therefore it loses the practical value in most real projections. a more realistic view is that planning ought to take into consideration partial observability beforehand and aim for a more flexible and robust solution. what is more significant, it is inevitable that the quality of plan varies dramatically in the partially observable environment. in this paper we propose a probabilistic contingent hierarchical task network (htn) planner, named high-quality contingent planner (hqcp), to generate high-quality plans in the partially observable environment. the formalisms in htn planning are extended into partial observability and are evaluated regarding the cost. next, we explore a novel heuristic for high-quality plans and develop the integrated planning algorithm. finally, an empirical study verifies the effectiveness and efficiency of the planner both in probabilistic contingent planning and for obtaining high-quality plans.Note: The transliteration is based on the Hanyu Pinyin system, which is a standardized system for romanizing Chinese characters. The translated text is in Simplified Chinese, which is the standardized form of Chinese used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Chatbots-in-Drug-Discovery-A-Case-Study-on-Anti-Cocaine-Addiction-Drug-Development-with-ChatGPT"><a href="#Chatbots-in-Drug-Discovery-A-Case-Study-on-Anti-Cocaine-Addiction-Drug-Development-with-ChatGPT" class="headerlink" title="Chatbots in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with ChatGPT"></a>Chatbots in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06920">http://arxiv.org/abs/2308.06920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Wang, Hongsong Feng, Guo-Wei Wei</li>
<li>for: 这个论文的目的是发展抗吸毒药物，使用GPT-4作为虚拟导航员，为研究人员提供策略和方法指导，以开发更有价值的药物候选体。</li>
<li>methods: 这个研究使用GPT-4语言模型chatbot，作为虚拟导航员，为研究人员提供策略和方法指导，以开发更有价值的药物候选体。</li>
<li>results: 这个研究发现，通过使用GPT-4语言模型chatbot，可以帮助研究人员更好地开发更有价值的药物候选体，并且可以提高药物开发的效率和优化性。<details>
<summary>Abstract</summary>
The birth of ChatGPT, a cutting-edge language model chatbot developed by OpenAI, ushered in a new era in AI, and this paper vividly showcases its innovative application within the field of drug discovery. Focused specifically on developing anti-cocaine addiction drugs, the study employs GPT-4 as a virtual guide, offering strategic and methodological insights to researchers working on generative models for drug candidates. The primary objective is to generate optimal drug-like molecules with desired properties. By leveraging the capabilities of ChatGPT, the study introduces a novel approach to the drug discovery process. This symbiotic partnership between AI and researchers transforms how drug development is approached. Chatbots become facilitators, steering researchers towards innovative methodologies and productive paths for creating effective drug candidates. This research sheds light on the collaborative synergy between human expertise and AI assistance, wherein ChatGPT's cognitive abilities enhance the design and development of potential pharmaceutical solutions. This paper not only explores the integration of advanced AI in drug discovery but also reimagines the landscape by advocating for AI-powered chatbots as trailblazers in revolutionizing therapeutic innovation.
</details>
<details>
<summary>摘要</summary>
开启AI时代的掌门人---ChatGPT，一个前所未有的语言模型对话机器人，由OpenAI开发，带来了新的时代。这篇研究专注于开发抗科塞鸽药物，使用GPT-4作为虚拟导师，为研究人员工作于生成模型的药物候选者提供策略和方法学见解。研究的主要目标是生成符合需求的药物分子。通过利用ChatGPT的能力，这篇研究引入了一个新的药物发现过程。在人类专家和AI助手的协力下，虚拟导师成为了药物发现的帮手，导引研究人员朝着创新的方法和有效的药物候选者进行探索。这篇研究不仅探讨了AI在药物发现中的应用，而且重新定义了领域的概念，宣扬AI助手作为药物发现的先驱者，推动药物创新的发展。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Ehanced-Move-Recognition-Algorithm-Based-on-Pre-trained-Models-with-Positional-Embeddings"><a href="#A-Novel-Ehanced-Move-Recognition-Algorithm-Based-on-Pre-trained-Models-with-Positional-Embeddings" class="headerlink" title="A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings"></a>A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10822">http://arxiv.org/abs/2308.10822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wen, Jie Wang, Xiaodong Qiao</li>
<li>for: 这篇论文主要targets at improving the recognition of abstracts in Chinese scientific and technological papers.</li>
<li>methods: 该论文提出了一种基于改进预训练模型和闭包网络听写机制的新的增强 Move recognition算法，用于处理中文科技论文的摘要。</li>
<li>results: 实验结果显示，该算法相比原始数据集，在分割数据集上提高了13.37%的准确率，并在基础比较模型上提高了7.55%的准确率。<details>
<summary>Abstract</summary>
The recognition of abstracts is crucial for effectively locating the content and clarifying the article. Existing move recognition algorithms lack the ability to learn word position information to obtain contextual semantics. This paper proposes a novel enhanced move recognition algorithm with an improved pre-trained model and a gated network with attention mechanism for unstructured abstracts of Chinese scientific and technological papers. The proposed algorithm first performs summary data segmentation and vocabulary training. The EP-ERNIE$\_$AT-GRU framework is leveraged to incorporate word positional information, facilitating deep semantic learning and targeted feature extraction. Experimental results demonstrate that the proposed algorithm achieves 13.37$\%$ higher accuracy on the split dataset than on the original dataset and a 7.55$\%$ improvement in accuracy over the basic comparison model.
</details>
<details>
<summary>摘要</summary>
摘要理解是科技文献检索和解释的关键，但现有的 Move 认识算法无法学习单词位置信息以获取语义上的上下文。这篇论文提出了一种新的增强 Move 认识算法，使用改进的预训练模型和闭合网络听力机制来处理中文科技文献摘要。该算法首先执行摘要数据分 segmentation 和词汇训练。使用 EP-ERNIE $\_$ AT-GRU 框架，以获取单词位置信息，进一步深入学习语义和特定特征提取。实验结果表明，提出的算法在分数据集上比原始数据集高出 13.37%，与基本对比模型相比高出 7.55%。
</details></li>
</ul>
<hr>
<h2 id="Hierarchy-Flow-For-High-Fidelity-Image-to-Image-Translation"><a href="#Hierarchy-Flow-For-High-Fidelity-Image-to-Image-Translation" class="headerlink" title="Hierarchy Flow For High-Fidelity Image-to-Image Translation"></a>Hierarchy Flow For High-Fidelity Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06909">http://arxiv.org/abs/2308.06909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weichenfan/hierarchyflow">https://github.com/weichenfan/hierarchyflow</a></li>
<li>paper_authors: Weichen Fan, Jinghuan Chen, Ziwei Liu</li>
<li>for: 本研究旨在提高图像到图像翻译中的内容保持性。</li>
<li>methods: 我们提出了一种新的流基模型，即层次流（Hierarchy Flow），以提高翻译过程中的内容保持性。</li>
<li>results: 我们的方法在各种图像到图像翻译benchmark上实现了状态的表现，特别是在强级和正常级翻译任务中表现出了明显的优势。<details>
<summary>Abstract</summary>
Image-to-image (I2I) translation comprises a wide spectrum of tasks. Here we divide this problem into three levels: strong-fidelity translation, normal-fidelity translation, and weak-fidelity translation, indicating the extent to which the content of the original image is preserved. Although existing methods achieve good performance in weak-fidelity translation, they fail to fully preserve the content in both strong- and normal-fidelity tasks, e.g. sim2real, style transfer and low-level vision. In this work, we propose Hierarchy Flow, a novel flow-based model to achieve better content preservation during translation. Specifically, 1) we first unveil the drawbacks of standard flow-based models when applied to I2I translation. 2) Next, we propose a new design, namely hierarchical coupling for reversible feature transformation and multi-scale modeling, to constitute Hierarchy Flow. 3) Finally, we present a dedicated aligned-style loss for a better trade-off between content preservation and stylization during translation. Extensive experiments on a wide range of I2I translation benchmarks demonstrate that our approach achieves state-of-the-art performance, with convincing advantages in both strong- and normal-fidelity tasks. Code and models will be at https://github.com/WeichenFan/HierarchyFlow.
</details>
<details>
<summary>摘要</summary>
Image-to-image（I2I）翻译包括多种任务。我们在这里将这个问题分为三级：强度精度翻译、常规精度翻译和弱度精度翻译，这些级别指的是原始图像内容的保留程度。虽然现有方法在弱度翻译任务上达到了好的性能，但是它们在强度翻译和常规翻译任务中却失败了，例如sim2real、style transfer和低级视觉。在这种情况下，我们提出了一种新的方法——层次流模型，以提高翻译过程中内容的保留。具体来说，我们首先揭示了标准流模型在I2I翻译中的缺陷。然后，我们提出了一种新的设计——层次协调对应的反转特征转换和多尺度模型，以构成层次流模型。最后，我们提出了一种专门设计的对齐风格损失，以实现在翻译过程中更好的内容保留和风格化融合。我们在多种I2I翻译 benchmark 上进行了广泛的实验，并证明了我们的方法可以达到状态之最的性能，在强度翻译和常规翻译任务中都有证明性的优势。代码和模型将在 GitHub 上提供。
</details></li>
</ul>
<hr>
<h2 id="Generative-Interpretation"><a href="#Generative-Interpretation" class="headerlink" title="Generative Interpretation"></a>Generative Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06907">http://arxiv.org/abs/2308.06907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yonathanarbel/generativeinterpretation">https://github.com/yonathanarbel/generativeinterpretation</a></li>
<li>paper_authors: Yonathan A. Arbel, David Hoffman</li>
<li>for: 这篇论文目的是提出一种新的合同解释方法，使用大语言模型来估算合同意义。</li>
<li>methods: 这篇论文使用了大语言模型来实现这个目的，并通过实践案例来示例其能力。</li>
<li>results: 论文显示了这些模型可以帮助法官确定合同的真实意义，衡量不确定性，并填充党之间的协议缺陷。它还示出了这些模型可以评估外部证据的证据价值。<details>
<summary>Abstract</summary>
We introduce generative interpretation, a new approach to estimating contractual meaning using large language models. As AI triumphalism is the order of the day, we proceed by way of grounded case studies, each illustrating the capabilities of these novel tools in distinct ways. Taking well-known contracts opinions, and sourcing the actual agreements that they adjudicated, we show that AI models can help factfinders ascertain ordinary meaning in context, quantify ambiguity, and fill gaps in parties' agreements. We also illustrate how models can calculate the probative value of individual pieces of extrinsic evidence. After offering best practices for the use of these models given their limitations, we consider their implications for judicial practice and contract theory. Using LLMs permits courts to estimate what the parties intended cheaply and accurately, and as such generative interpretation unsettles the current interpretative stalemate. Their use responds to efficiency-minded textualists and justice-oriented contextualists, who argue about whether parties will prefer cost and certainty or accuracy and fairness. Parties--and courts--would prefer a middle path, in which adjudicators strive to predict what the contract really meant, admitting just enough context to approximate reality while avoiding unguided and biased assimilation of evidence. As generative interpretation offers this possibility, we argue it can become the new workhorse of contractual interpretation.
</details>
<details>
<summary>摘要</summary>
我们介绍生成解释，一种新的方法来估算合同意义使用大语言模型。在人工智能胜利的时代，我们通过实际案例来证明这些新工具在不同方面的能力。使用知名合同意见和实际协议，我们表明AI模型可以帮助判决者了解具体的意思，衡量不确定性，并填充党们的协议中的缺陷。我们还示出了模型可以计算个别外部证据的证据价值。在使用这些模型的限制后，我们考虑了它们对法律实践和合同理论的影响。使用LLMs permit courts to estimate what the parties intended at a low cost and high accuracy, and thus generative interpretation breaks the current interpretive deadlock. Its use responds to efficiency-minded textualists and justice-oriented contextualists, who argue about whether parties will prefer cost and certainty or accuracy and fairness. Parties and courts would prefer a middle path, in which adjudicators strive to predict what the contract really meant, admitting just enough context to approximate reality while avoiding unguided and biased assimilation of evidence. As generative interpretation offers this possibility, we argue it can become the new workhorse of contractual interpretation.
</details></li>
</ul>
<hr>
<h2 id="The-Michigan-Robotics-Undergraduate-Curriculum-Defining-the-Discipline-of-Robotics-for-Equity-and-Excellence"><a href="#The-Michigan-Robotics-Undergraduate-Curriculum-Defining-the-Discipline-of-Robotics-for-Equity-and-Excellence" class="headerlink" title="The Michigan Robotics Undergraduate Curriculum: Defining the Discipline of Robotics for Equity and Excellence"></a>The Michigan Robotics Undergraduate Curriculum: Defining the Discipline of Robotics for Equity and Excellence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06905">http://arxiv.org/abs/2308.06905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Odest Chadwicke Jenkins, Jessy Grizzle, Ella Atkins, Leia Stirling, Elliott Rouse, Mark Guzdial, Damen Provost, Kimberly Mann, Joanna Millunchick<br>for:The paper is written to propose and establish a new undergraduate program in Robotics at the University of Michigan, with a focus on equity and excellence.methods:The program is designed with an adaptable curriculum that is accessible through a diversity of student pathways, and includes partnerships with Historically Black Colleges and Universities.results:The program has been highly successful in its first academic year, with over 100 students declaring Robotics as their major, completion of the Robotics major by the first two graduates, and soaring enrollments in Robotics classes.<details>
<summary>Abstract</summary>
The Robotics Major at the University of Michigan was successfully launched in the 2022-23 academic year as an innovative step forward to better serve students, our communities, and our society. Building on our guiding principle of "Robotics with Respect" and our larger Robotics Pathways model, the Michigan Robotics Major was designed to define robotics as a true academic discipline with both equity and excellence as our highest priorities. Understanding that talent is equally distributed but opportunity is not, the Michigan Robotics Major has embraced an adaptable curriculum that is accessible through a diversity of student pathways and enables successful and sustained career-long participation in robotics, AI, and automation professions. The results after our planning efforts (2019-22) and first academic year (2022-23) have been highly encouraging: more than 100 students declared Robotics as their major, completion of the Robotics major by our first two graduates, soaring enrollments in our Robotics classes, thriving partnerships with Historically Black Colleges and Universities. This document provides our original curricular proposal for the Robotics Undergraduate Program at the University of Michigan, submitted to the Michigan Association of State Universities in April 2022 and approved in June 2022. The dissemination of our program design is in the spirit of continued growth for higher education towards realizing equity and excellence.   The most recent version of this document is also available on Google Docs through this link: https://ocj.me/robotics_major
</details>
<details>
<summary>摘要</summary>
美国密歇根大学机器人学专业在2022-23学年 successfully 发起了一个创新的步骤，以更好地服务学生、社区和社会。 基于我们的指导原则“机器人学 avec 尊重”和我们更大的机器人路径模型，密歇根大学机器人学专业是为了定义机器人学为真正的学术领域，并将 equity 和 excellence 作为我们最高的优先级。 理解才华平等分布，但机会不平等分布，密歇根大学机器人学专业采用了适应性课程，通过多种学生路径访问，帮助学生在机器人、人工智能和自动化领域成功地职业发展。 我们的规划努力（2019-22）和首学年（2022-23）的结果非常鼓舞人：More than 100 学生声明机器人学为他们的主修，首两名毕业生完成机器人学专业，课程报名人数在趋升，与 Historically Black Colleges and Universities 的合作也在逐渐增长。这份文件包含我们原始的课程设计提案，在2022年6月由密歇根州大学协会批准。 我们希望通过分享我们的program design ，促进高等教育的增长，实现 equity 和 excellence。最新版本的这份文件可以在 Google Docs 上通过以下链接获取：https://ocj.me/robotics_major
</details></li>
</ul>
<hr>
<h2 id="Robustified-ANNs-Reveal-Wormholes-Between-Human-Category-Percepts"><a href="#Robustified-ANNs-Reveal-Wormholes-Between-Human-Category-Percepts" class="headerlink" title="Robustified ANNs Reveal Wormholes Between Human Category Percepts"></a>Robustified ANNs Reveal Wormholes Between Human Category Percepts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06887">http://arxiv.org/abs/2308.06887</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ggaziv/wormholes">https://github.com/ggaziv/wormholes</a></li>
<li>paper_authors: Guy Gaziv, Michael J. Lee, James J. DiCarlo</li>
<li>for: 这篇论文旨在探讨人工神经网络（ANNs）对图像分类的敏感性，以及人类视觉处理模型的不足。</li>
<li>methods: 研究人员使用了标准的ANN模型和roboustified ANN模型，生成了小norm图像扰动，并评估了人类视觉对这些扰动的稳定性。</li>
<li>results: 研究发现，人类视觉对小norm图像扰动是高度稳定的，而roboustified ANN模型可以可靠地找到低norm图像扰动，使人类视觉受到强烈扰动。此外，研究还发现了“孔雀门”现象，即在图像空间中的任意起点都存在一些“蛇口”，可以带领人类视觉从当前分类状态转移到另一个semantically very different的状态。<details>
<summary>Abstract</summary>
The visual object category reports of artificial neural networks (ANNs) are notoriously sensitive to tiny, adversarial image perturbations. Because human category reports (aka human percepts) are thought to be insensitive to those same small-norm perturbations -- and locally stable in general -- this argues that ANNs are incomplete scientific models of human visual perception. Consistent with this, we show that when small-norm image perturbations are generated by standard ANN models, human object category percepts are indeed highly stable. However, in this very same "human-presumed-stable" regime, we find that robustified ANNs reliably discover low-norm image perturbations that strongly disrupt human percepts. These previously undetectable human perceptual disruptions are massive in amplitude, approaching the same level of sensitivity seen in robustified ANNs. Further, we show that robustified ANNs support precise perceptual state interventions: they guide the construction of low-norm image perturbations that strongly alter human category percepts toward specific prescribed percepts. These observations suggest that for arbitrary starting points in image space, there exists a set of nearby "wormholes", each leading the subject from their current category perceptual state into a semantically very different state. Moreover, contemporary ANN models of biological visual processing are now accurate enough to consistently guide us to those portals.
</details>
<details>
<summary>摘要</summary>
人工神经网络（ANNs）的视觉物体类别报告具有极其敏感于微小、敌意的图像扰动的特点。因为人类的视觉报告（也称为人类感知）被认为不受这些小范围扰动的影响，而且在总体上是稳定的，这意味着ANNs是人类视觉认知的不完整科学模型。我们的研究表明，当使用标准ANN模型生成小范围扰动时，人类对象类别报告是非常稳定的。然而，在这个“人类假设稳定”的 régime中，我们发现了使用强化ANN模型时发现的低范围扰动，这些扰动会强烈地打乱人类报告。这些扰动的振荡强度非常大，相当于强化ANNs的敏感度。此外，我们发现了使用强化ANN模型支持精确的感知状态改变：它们可以生成低范围扰动，使人类category报告强烈地改变。这些观察表明，对于任意的图像初始状态，存在一组“蠕虫洞”，每个蠕虫洞都可以将主体从其当前的报告状态转移到具有不同semantics的状态。此外，当代的生物视觉处理ANN模型已经够精度，可以一直引导我们到这些门户。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Offensive-Gameplan-in-the-National-Basketball-Association-with-Machine-Learning"><a href="#Optimizing-Offensive-Gameplan-in-the-National-Basketball-Association-with-Machine-Learning" class="headerlink" title="Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning"></a>Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06851">http://arxiv.org/abs/2308.06851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eamon Mukhopadhyay</li>
<li>for: This paper aims to verify the effectiveness of the Offensive Rating (ORTG) metric in predicting different NBA playtypes.</li>
<li>methods: The authors use both linear regression and neural network regression models to evaluate the correlation between ORTG and different playtypes.</li>
<li>results: The authors find that both models have a strong correlation with the playtypes, but the neural network model performs slightly better than the linear regression model. They also use the accuracy of the models to optimize the output of the model with test examples, demonstrating the combination of features that can achieve a highly functioning offense.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文目的是验证 Offensive Rating (ORTG) metric 是否有效地预测不同的 NBA 战术类型。</li>
<li>methods: 作者使用了线性回归和神经网络回归模型来评估 ORTG 和不同战术类型之间的相关性。</li>
<li>results: 作者发现两种模型都有强相关性，但神经网络模型在准确性上略微高于线性回归模型。他们还使用模型准确性来优化输出模型的测试例子，以示出可以实现高效的攻击防御。<details>
<summary>Abstract</summary>
Throughout the analytical revolution that has occurred in the NBA, the development of specific metrics and formulas has given teams, coaches, and players a new way to see the game. However - the question arises - how can we verify any metrics? One method would simply be eyeball approximation (trying out many different gameplans) and/or trial and error - an estimation-based and costly approach. Another approach is to try to model already existing metrics with a unique set of features using machine learning techniques. The key to this approach is that with these features that are selected, we can try to gauge the effectiveness of these features combined, rather than using individual analysis in simple metric evaluation. If we have an accurate model, it can particularly help us determine the specifics of gameplan execution. In this paper, the statistic ORTG (Offensive Rating, developed by Dean Oliver) was found to have a correlation with different NBA playtypes using both a linear regression model and a neural network regression model, although ultimately, a neural network worked slightly better than linear regression. Using the accuracy of the models as a justification, the next step was to optimize the output of the model with test examples, which would demonstrate the combination of features to best achieve a highly functioning offense.
</details>
<details>
<summary>摘要</summary>
在NBA analytics革命中，发展特定的指标和公式为球队、教练和球员提供了一种新的视角。然而，问题 arise - 如何验证这些指标呢？一种方法是通过观察多个不同的战斗策略来估算（trying out many different gameplans），以及或者试错法则 - 一种估计基于的成本高的方法。另一种方法是使用机器学习技术来模型已有的指标，并选择一组独特的特征。这种方法的关键在于，我们可以通过这些选择的特征来评估这些特征的组合效果，而不是单独评估指标。如果我们有一个准确的模型，那么它可以尤其帮助我们确定游戏计划的执行细节。在这篇论文中，发展了ORTG（Offensive Rating，由Dean Oliver创造）指标，与不同的NBA战斗类型之间存在相关性，使用线性回归模型和神经网络回归模型进行相关性分析，最终发现神经网络模型的准确性略高于线性回归模型。使用模型准确性作为正当化，下一步是优化模型输出的测试例子，以示出最佳执行游戏计划所需的组合特征。
</details></li>
</ul>
<hr>
<h2 id="A-Parallel-Ensemble-of-Metaheuristic-Solvers-for-the-Traveling-Salesman-Problem"><a href="#A-Parallel-Ensemble-of-Metaheuristic-Solvers-for-the-Traveling-Salesman-Problem" class="headerlink" title="A Parallel Ensemble of Metaheuristic Solvers for the Traveling Salesman Problem"></a>A Parallel Ensemble of Metaheuristic Solvers for the Traveling Salesman Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07347">http://arxiv.org/abs/2308.07347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swetha Varadarajan, Darrell Whitley</li>
<li>for: 解决购物人巡游问题 (TSP)，一个广泛研究的NP困难问题。</li>
<li>methods: 使用Lin-Kernighan-Helsgaun（LKH）规则和Edge Assembly crossover（EAX）算法，以及其hybrid版本和Mixing Genetic Algorithm（MGA）。</li>
<li>results:  ensemble setup中 combine these solvers，可以超越单个 solver的性能，特别是在大于10,000个城市的问题上。<details>
<summary>Abstract</summary>
The travelling salesman problem (TSP) is one of the well-studied NP-hard problems in the literature. The state-of-the art inexact TSP solvers are the Lin-Kernighan-Helsgaun (LKH) heuristic and Edge Assembly crossover (EAX). A recent study suggests that EAX with restart mechanisms perform well on a wide range of TSP instances. However, this study is limited to 2,000 city problems. We study for problems ranging from 2,000 to 85,900. We see that the performance of the solver varies with the type of the problem. However, combining these solvers in an ensemble setup, we are able to outperform the individual solver's performance. We see the ensemble setup as an efficient way to make use of the abundance of compute resources. In addition to EAX and LKH, we use several versions of the hybrid of EAX and Mixing Genetic Algorithm (MGA). A hybrid of MGA and EAX is known to solve some hard problems. We see that the ensemble of the hybrid version outperforms the state-of-the-art solvers on problems larger than 10,000 cities.
</details>
<details>
<summary>摘要</summary>
“旅游销售问题”（TSP）是一个已有广泛研究的NP困难问题。现今的State-of-the-art对于TSP问题的不精确解决方案是林-肯尼根-赫尔斯堡（LKH）规律和边组聚合交叉（EAX）。一 recent study 显示，在广泛的TSP问题上，EAX加上重新启动机制perform well。但是，这个研究仅对2,000个城市问题进行了研究。我们在2,000至85,900个城市之间进行了研究，发现解释器的表现随问题的类型而异。不过，将这些解释器集成为一个组合设置，我们能够超过个体解释器的表现。我们视这个组合设置为一种有效的使用计算资源的方式。此外，我们还使用了一些版本的EAX和混合遗传算法（MGA）的复合版本。一个混合版本已经能够解决一些困难的问题。我们发现，这个组合设置在10,000个城市以上的问题上表现更好。
</details></li>
</ul>
<hr>
<h2 id="Diagnostic-Reasoning-Prompts-Reveal-the-Potential-for-Large-Language-Model-Interpretability-in-Medicine"><a href="#Diagnostic-Reasoning-Prompts-Reveal-the-Potential-for-Large-Language-Model-Interpretability-in-Medicine" class="headerlink" title="Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model Interpretability in Medicine"></a>Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model Interpretability in Medicine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06834">http://arxiv.org/abs/2308.06834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, Jonathan H Chen</li>
<li>for: 本研究旨在探讨语言模型在医学领域中是否可以准确地诊断疾病，并使用什么方法来实现这一目标。</li>
<li>methods: 本研究使用了GPT4语言模型，并开发了一些新的诊断逻辑提问来评估语言模型的诊断能力。</li>
<li>results: 研究发现，通过使用新的诊断逻辑提问，GPT4可以模仿医生的常见诊断过程，同时保持诊断准确性。这表明，可以使用价值的诊断逻辑提问来让语言模型进行可读性的诊断。<details>
<summary>Abstract</summary>
One of the major barriers to using large language models (LLMs) in medicine is the perception they use uninterpretable methods to make clinical decisions that are inherently different from the cognitive processes of clinicians. In this manuscript we develop novel diagnostic reasoning prompts to study whether LLMs can perform clinical reasoning to accurately form a diagnosis. We find that GPT4 can be prompted to mimic the common clinical reasoning processes of clinicians without sacrificing diagnostic accuracy. This is significant because an LLM that can use clinical reasoning to provide an interpretable rationale offers physicians a means to evaluate whether LLMs can be trusted for patient care. Novel prompting methods have the potential to expose the black box of LLMs, bringing them one step closer to safe and effective use in medicine.
</details>
<details>
<summary>摘要</summary>
一个主要阻碍大语言模型（LLMs）在医学中使用的问题是人们认为它们使用不可解释的方法来做诊断决策，这与医生的认知过程有很大差异。在这篇论文中，我们开发了新的诊断思维提要，以研究LLMs是否可以准确地进行诊断。我们发现GPT4可以通过模仿常见的临床思维过程来提供一个可解释的诊断原因，而不会产生诊断准确性的损失。这是重要的，因为一个可以使用临床思维来提供可解释的诊断原因的 LLM 可以让医生评估 LLMS 是否可以用于患者护理。这种新的提要方法有助于暴露 LLMS 的黑盒子，使其更接近安全和有效的使用。
</details></li>
</ul>
<hr>
<h2 id="InTune-Reinforcement-Learning-based-Data-Pipeline-Optimization-for-Deep-Recommendation-Models"><a href="#InTune-Reinforcement-Learning-based-Data-Pipeline-Optimization-for-Deep-Recommendation-Models" class="headerlink" title="InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models"></a>InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08500">http://arxiv.org/abs/2308.08500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kabir Nagrecha, Lingyi Liu, Pablo Delgado, Prasanna Padmanabhan</li>
<li>for: 这篇论文旨在探讨深度学习推荐模型（DLRM）训练中的数据接入问题，以及相关的管道瓶颈和挑战。</li>
<li>methods: 本论文使用了人工智能学习（RL）代理来学习训练机器的CPU资源分配方式，以更好地平行化数据加载和提高通过put。</li>
<li>results:  experiments表明，使用InTune可以在几分钟内构建优化数据管道配置，并可以轻松地与现有训练工作流 integration。 InTune可以提高在线数据接入速率，从而减少模型执行时间的浪费和提高效率。在实际场景中，InTune可以提高数据接入吞吐量 by up to 2.29倍，并同时提高CPU和GPU资源利用率。<details>
<summary>Abstract</summary>
Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- and time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning training jobs are dominated by model execution, the most important factor in DLRM training performance is often online data ingestion.   In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into DLRM training pipeline bottlenecks and challenges. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and to identify shortfalls in existing pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization to adopt. Our studies lead us to design and build a new solution for data pipeline optimization, InTune.   InTune employs a reinforcement learning (RL) agent to learn how to distribute the CPU resources of a trainer machine across a DLRM data pipeline to more effectively parallelize data loading and improve throughput. Our experiments show that InTune can build an optimized data pipeline configuration within only a few minutes, and can easily be integrated into existing training workflows. By exploiting the responsiveness and adaptability of RL, InTune achieves higher online data ingestion rates than existing optimizers, thus reducing idle times in model execution and increasing efficiency. We apply InTune to our real-world cluster, and find that it increases data ingestion throughput by as much as 2.29X versus state-of-the-art data pipeline optimizers while also improving both CPU & GPU utilization.
</details>
<details>
<summary>摘要</summary>
深度学习基本的推荐模型（DLRM）已经成为现代推荐系统中的重要组件。许多公司现在为DLRM训练建立大型计算集群，导致新的成本和时间OPTIMIZATION需求。这些系统的挑战是独特的； Typical deep learning training Jobs是模型执行所dominated，但DLRM训练性能中最重要的因素通常是在线数据取入。 在这篇论文中，我们探索DLRM数据取入问题的独特特性并提供了推荐管道瓶颈和挑战。我们研究了Netflix的compute集群中的实际DLRM数据处理管道，并观察了在线取入的性能影响以及现有管道优化工具的缺点。我们发现现有工具可能会导致下 optimize performance，或者频繁崩溃，或者需要重新组织集群以采用。我们的研究导致我们设计并建立了一个新的数据管道优化解决方案，即InTune。InTune使用了强化学习（RL）代理来分配训练机器的CPU资源在DLRM数据管道中更有效地并行数据加载和提高吞吐量。我们的实验表明，InTune可以在只需几分钟之内构建优化的数据管道配置，并可以轻松地与现有训练工作流 integrate。通过强化学习的响应和适应性，InTune可以在现有优化器的基础上提高在线数据取入率，从而降低模型执行时的空闲时间和提高效率。我们在实际集群上应用InTune，发现它可以提高数据取入吞吐量达2.29倍，而且同时提高CPU和GPU资源利用率。
</details></li>
</ul>
<hr>
<h2 id="An-Ensemble-Approach-to-Question-Classification-Integrating-Electra-Transformer-GloVe-and-LSTM"><a href="#An-Ensemble-Approach-to-Question-Classification-Integrating-Electra-Transformer-GloVe-and-LSTM" class="headerlink" title="An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM"></a>An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06828">http://arxiv.org/abs/2308.06828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanad Aburass, Osama Dorgham</li>
<li>for: 本研究は问题分类 задачіに对するnovelensemble方法を提出します。</li>
<li>methods: 提案された模型はElectra、GloVe、LSTMの三种state-of-the-art模型を组み合わせた Ensemble方法です。</li>
<li>results: 对TREC数据集进行训练和评估，结果显示提案的模型在所有评估指标上都超过了BERT、RoBERTa、DistilBERT等其他 cutting-edge模型，测试集准确率达0.8。<details>
<summary>Abstract</summary>
This paper introduces a novel ensemble approach for question classification using state-of-the-art models -- Electra, GloVe, and LSTM. The proposed model is trained and evaluated on the TREC dataset, a well-established benchmark for question classification tasks. The ensemble model combines the strengths of Electra, a transformer-based model for language understanding, GloVe, a global vectors for word representation, and LSTM, a recurrent neural network variant, providing a robust and efficient solution for question classification. Extensive experiments were carried out to compare the performance of the proposed ensemble approach with other cutting-edge models, such as BERT, RoBERTa, and DistilBERT. Our results demonstrate that the ensemble model outperforms these models across all evaluation metrics, achieving an accuracy of 0.8 on the test set. These findings underscore the effectiveness of the ensemble approach in enhancing the performance of question classification tasks, and invite further exploration of ensemble methods in natural language processing.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的集成方法用于问题分类，使用当前的模型---Electra、GloVe和LSTM。该提议的模型在TREC数据集上进行训练和评估，这是一个已知的问题分类任务 benchmark。集成模型 combinesthe strengths of Electra、GloVe和LSTM，提供一种强大和高效的问题分类解决方案。我们进行了广泛的实验，比较了该集成模型与其他当前最佳模型，如BERT、RoBERTa和DistilBERT的性能。我们的结果表明，集成模型在所有评估指标上都超过了这些模型，实现了测试集上的准确率0.8。这些发现证明了集成方法在问题分类任务中的效iveness，并邀请进一步探索集成方法在自然语言处理领域的应用。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Graph-Clustering-with-Unknown-Cluster-Number"><a href="#Reinforcement-Graph-Clustering-with-Unknown-Cluster-Number" class="headerlink" title="Reinforcement Graph Clustering with Unknown Cluster Number"></a>Reinforcement Graph Clustering with Unknown Cluster Number</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06827">http://arxiv.org/abs/2308.06827</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yueliu1999/awesome-deep-graph-clustering">https://github.com/yueliu1999/awesome-deep-graph-clustering</a></li>
<li>paper_authors: Yue Liu, Ke Liang, Jun Xia, Xihong Yang, Sihang Zhou, Meng Liu, Xinwang Liu, Stan Z. Li</li>
<li>for: 这种方法的目的是为了不需要先知道cluster数量的情况下，使用神经网络进行深度图 clustering。</li>
<li>methods: 该方法使用了对冲预测任务来学习节点表示，然后使用了 reinforcement learning 机制来决定节点分布。</li>
<li>results: 实验表明，该方法可以具有高效率和高准确率，并且可以在不知道cluster数量的情况下进行深度图 clustering。<details>
<summary>Abstract</summary>
Deep graph clustering, which aims to group nodes into disjoint clusters by neural networks in an unsupervised manner, has attracted great attention in recent years. Although the performance has been largely improved, the excellent performance of the existing methods heavily relies on an accurately predefined cluster number, which is not always available in the real-world scenario. To enable the deep graph clustering algorithms to work without the guidance of the predefined cluster number, we propose a new deep graph clustering method termed Reinforcement Graph Clustering (RGC). In our proposed method, cluster number determination and unsupervised representation learning are unified into a uniform framework by the reinforcement learning mechanism. Concretely, the discriminative node representations are first learned with the contrastive pretext task. Then, to capture the clustering state accurately with both local and global information in the graph, both node and cluster states are considered. Subsequently, at each state, the qualities of different cluster numbers are evaluated by the quality network, and the greedy action is executed to determine the cluster number. In order to conduct feedback actions, the clustering-oriented reward function is proposed to enhance the cohesion of the same clusters and separate the different clusters. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method. The source code of RGC is shared at https://github.com/yueliu1999/RGC and a collection (papers, codes and, datasets) of deep graph clustering is shared at https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering on Github.
</details>
<details>
<summary>摘要</summary>
深度图 clustering，目标是通过神经网络在无监督的情况下将节点分组成不同的分支，在过去几年内吸引了广泛的关注。 although the performance has been largely improved, the excellent performance of the existing methods heavily relies on accurately predefined cluster number, which is not always available in the real-world scenario. To enable the deep graph clustering algorithms to work without the guidance of the predefined cluster number, we propose a new deep graph clustering method termed Reinforcement Graph Clustering (RGC). In our proposed method, cluster number determination and unsupervised representation learning are unified into a uniform framework by the reinforcement learning mechanism. Concretely, the discriminative node representations are first learned with the contrastive pretext task. Then, to capture the clustering state accurately with both local and global information in the graph, both node and cluster states are considered. Subsequently, at each state, the qualities of different cluster numbers are evaluated by the quality network, and the greedy action is executed to determine the cluster number. In order to conduct feedback actions, the clustering-oriented reward function is proposed to enhance the cohesion of the same clusters and separate the different clusters. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method. The source code of RGC is shared at <https://github.com/yueliu1999/RGC> and a collection (papers, codes and, datasets) of deep graph clustering is shared at <https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering> on Github.
</details></li>
</ul>
<hr>
<h2 id="Approximate-and-Weighted-Data-Reconstruction-Attack-in-Federated-Learning"><a href="#Approximate-and-Weighted-Data-Reconstruction-Attack-in-Federated-Learning" class="headerlink" title="Approximate and Weighted Data Reconstruction Attack in Federated Learning"></a>Approximate and Weighted Data Reconstruction Attack in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06822">http://arxiv.org/abs/2308.06822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Wang, Yongcun Song, Enrique Zuazua</li>
<li>for: 这个论文的目的是提出一种可以攻击 Federated Learning（FL）中最常使用的水平 Federated Averaging（FedAvg）scenario的方法，并且提高攻击效率和预测品质。</li>
<li>methods: 本文使用了一种 interpolating-based approximation method，将客户端的本地训练过程中的模型更新转换为可以攻击的形式，然后设计了一个层别加权损失函数来提高攻击的数据质量。</li>
<li>results: 实验结果显示，提出的 aproximate和weighted攻击（AWA）方法比其他现有的方法有更好的攻击效率和预测品质，特别是在静止图像数据重建 task 上。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed learning paradigm that enables multiple clients to collaborate on building a machine learning model without sharing their private data. Although FL is considered privacy-preserved by design, recent data reconstruction attacks demonstrate that an attacker can recover clients' training data based on the parameters shared in FL. However, most existing methods fail to attack the most widely used horizontal Federated Averaging (FedAvg) scenario, where clients share model parameters after multiple local training steps. To tackle this issue, we propose an interpolation-based approximation method, which makes attacking FedAvg scenarios feasible by generating the intermediate model updates of the clients' local training processes. Then, we design a layer-wise weighted loss function to improve the data quality of reconstruction. We assign different weights to model updates in different layers concerning the neural network structure, with the weights tuned by Bayesian optimization. Finally, experimental results validate the superiority of our proposed approximate and weighted attack (AWA) method over the other state-of-the-art methods, as demonstrated by the substantial improvement in different evaluation metrics for image data reconstructions.
</details>
<details>
<summary>摘要</summary>
受欢迎的分布式学习（FL）是一种分布式学习模式，允许多个客户端共同建立一个机器学习模型，无需分享他们的私人数据。虽然FL被视为隐私保护的设计，但最近的数据重建攻击表明，攻击者可以根据在FL中共享的参数来恢复客户端的训练数据。然而，大多数现有的方法无法攻击最常用的水平 Federated Averaging（FedAvg）场景，在这种场景下，客户端在多个本地训练步骤后共享模型参数。为解决这个问题，我们提出了一种 interpolate-based approximation 方法，可以让攻击者在客户端的本地训练过程中生成 intercept 的模型更新。然后，我们设计了层weise 权重损失函数，以提高数据重建的质量。我们对模型更新在不同层之间分配不同的权重，并通过 Bayesian 优化调整这些权重。最后，我们进行了实验，并证明了我们提出的approximate和权重 attacked（AWA）方法在不同评价指标中具有显著的优势，特别是对于图像数据重建 task。
</details></li>
</ul>
<hr>
<h2 id="Ground-Manipulator-Primitive-Tasks-to-Executable-Actions-using-Large-Language-Models"><a href="#Ground-Manipulator-Primitive-Tasks-to-Executable-Actions-using-Large-Language-Models" class="headerlink" title="Ground Manipulator Primitive Tasks to Executable Actions using Large Language Models"></a>Ground Manipulator Primitive Tasks to Executable Actions using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06810">http://arxiv.org/abs/2308.06810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Cao, C. S. George Lee</li>
<li>for: 这 paper 的目的是解决 robot 系统中高级任务与低级动作之间的转换问题。</li>
<li>methods: 该 paper 使用大型自然语言模型 (LLM) 将 manipulate primitive task 转换为机器人低级动作。</li>
<li>results: 该 paper 提供了一种基于任务框架的程序式似的提问，使得 LLM 可以生成位置&#x2F;力集点，以便实现混合控制。<details>
<summary>Abstract</summary>
Layered architectures have been widely used in robot systems. The majority of them implement planning and execution functions in separate layers. However, there still lacks a straightforward way to transit high-level tasks in the planning layer to the low-level motor commands in the execution layer. In order to tackle this challenge, we propose a novel approach to ground the manipulator primitive tasks to robot low-level actions using large language models (LLMs). We designed a program-like prompt based on the task frame formalism. In this way, we enable LLMs to generate position/force set-points for hybrid control. Evaluations over several state-of-the-art LLMs are provided.
</details>
<details>
<summary>摘要</summary>
层次架构在机器人系统中广泛应用。大多数其中实现规划和执行功能在不同的层次。然而，从高级任务到低级机器指令的转换仍然存在一定的挑战。为了解决这个问题，我们提出了一种新的方法，通过大型自然语言模型（LLM）将抓取器基础任务地标准化到机器人低级动作上。我们设计了基于任务框架 formalism的程序式样本。这样，我们允许 LLM 生成位/力设点，以便在混合控制中生成位置/力信号。我们对多个现代 LLM 进行了评估。
</details></li>
</ul>
<hr>
<h2 id="Neural-Networks-for-Programming-Quantum-Annealers"><a href="#Neural-Networks-for-Programming-Quantum-Annealers" class="headerlink" title="Neural Networks for Programming Quantum Annealers"></a>Neural Networks for Programming Quantum Annealers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06807">http://arxiv.org/abs/2308.06807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boschsamuel/nnforprogrammingquantumannealers">https://github.com/boschsamuel/nnforprogrammingquantumannealers</a></li>
<li>paper_authors: Samuel Bosch, Bobak Kiani, Rui Yang, Adrian Lupascu, Seth Lloyd</li>
<li>for: 这个论文旨在探讨量子机器学习是否可以提高人工智能的发展，特别是解决类别问题。</li>
<li>methods: 本论文使用了一种组合量子逻辑和类传播神经网络的方法，通过将量子逻辑连接到神经网络中来实现分类任务。</li>
<li>results: 研究发现，在使用量子逻辑的情况下，神经网络的性能不变化很大，并且不如使用常规非线性神经网络来解决分类问题。<details>
<summary>Abstract</summary>
Quantum machine learning has the potential to enable advances in artificial intelligence, such as solving problems intractable on classical computers. Some fundamental ideas behind quantum machine learning are similar to kernel methods in classical machine learning. Both process information by mapping it into high-dimensional vector spaces without explicitly calculating their numerical values. We explore a setup for performing classification on labeled classical datasets, consisting of a classical neural network connected to a quantum annealer. The neural network programs the quantum annealer's controls and thereby maps the annealer's initial states into new states in the Hilbert space. The neural network's parameters are optimized to maximize the distance of states corresponding to inputs from different classes and minimize the distance between quantum states corresponding to the same class. Recent literature showed that at least some of the "learning" is due to the quantum annealer, connecting a small linear network to a quantum annealer and using it to learn small and linearly inseparable datasets. In this study, we consider a similar but not quite the same case, where a classical fully-fledged neural network is connected with a small quantum annealer. In such a setting, the fully-fledged classical neural-network already has built-in nonlinearity and learning power, and can already handle the classification problem alone, we want to see whether an additional quantum layer could boost its performance. We simulate this system to learn several common datasets, including those for image and sound recognition. We conclude that adding a small quantum annealer does not provide a significant benefit over just using a regular (nonlinear) classical neural network.
</details>
<details>
<summary>摘要</summary>
量子机器学习有潜力推动人工智能的发展，如解决классические计算机无法解决的问题。一些量子机器学习的基本想法与经典机器学习的核心思想类似。两者都将信息映射到高维向量空间中，不必显式计算其数值。我们研究一种将标注的古典数据集用类icial neural network和小规模量子热退器连接起来进行分类。 neural network控制量子热退器的初态，并将其映射到彪维空间中的新状态。 neural network的参数被优化，以最大化输入 différences between states corresponding to different classes and minimize the difference between states corresponding to the same class.在文献中显示，至少一些"学习"是由量子热退器提供的，将小线性网络连接到量子热退器，并用其学习小和线性不可分离的数据集。在这种情况下，我们考虑一个类似的情况，其中一个完整的古典神经网络与一个小量子热退器连接在一起。在这种设置下，古典神经网络已经具有内置的非线性和学习能力，它可以独立处理分类问题。我们通过模拟这个系统，学习了一些常见的数据集，包括图像和声音识别。我们结论是，添加小量子热退器并不提供显著的改善，相比于使用非线性的古典神经网络。
</details></li>
</ul>
<hr>
<h2 id="Py-Tetrad-and-RPy-Tetrad-A-New-Python-Interface-with-R-Support-for-Tetrad-Causal-Search"><a href="#Py-Tetrad-and-RPy-Tetrad-A-New-Python-Interface-with-R-Support-for-Tetrad-Causal-Search" class="headerlink" title="Py-Tetrad and RPy-Tetrad: A New Python Interface with R Support for Tetrad Causal Search"></a>Py-Tetrad and RPy-Tetrad: A New Python Interface with R Support for Tetrad Causal Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07346">http://arxiv.org/abs/2308.07346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph D. Ramsey, Bryan Andrews</li>
<li>for: 这个论文是为了提供一个新的Python和R接口来访问Tetrad项目的 causal 模型计算、搜索和估计。</li>
<li>methods: 这个论文使用了JPype和Reticulate两种新的接口方法来实现Python和R与Tetrad的交互，这些方法是直接解决现有的问题。</li>
<li>results: 该论文提供了一些简单的工具和一些工作示例，使用JPype和Reticulate来接口Python和R与Tetrad是直观的和易懂的。<details>
<summary>Abstract</summary>
We give novel Python and R interfaces for the (Java) Tetrad project for causal modeling, search, and estimation. The Tetrad project is a mainstay in the literature, having been under consistent development for over 30 years. Some of its algorithms are now classics, like PC and FCI; others are recent developments. It is increasingly the case, however, that researchers need to access the underlying Java code from Python or R. Existing methods for doing this are inadequate. We provide new, up-to-date methods using the JPype Python-Java interface and the Reticulate Python-R interface, directly solving these issues. With the addition of some simple tools and the provision of working examples for both Python and R, using JPype and Reticulate to interface Python and R with Tetrad is straightforward and intuitive.
</details>
<details>
<summary>摘要</summary>
我们提供了一个新的Python和R接口 дляJava Tetrad项目，用于 causal模型搜索和估计。Tetrad项目已经在文献中被不断开发了超过30年，其中一些算法已经成为了经典，如PC和FCI；其他则是最近的发展。然而，研究人员在使用Python或R访问下面的Java代码变得越来越重要。现有的方法无法满足这些需求。我们提供了新的、当前的方法，使用JPype Python-Java接口和Reticulate Python-R接口，直接解决这些问题。另外，我们还提供了一些简单的工具和工作示例，使用JPype和Reticulate来接口Python和R与Tetrad是直观的。
</details></li>
</ul>
<hr>
<h2 id="SAILOR-Structural-Augmentation-Based-Tail-Node-Representation-Learning"><a href="#SAILOR-Structural-Augmentation-Based-Tail-Node-Representation-Learning" class="headerlink" title="SAILOR: Structural Augmentation Based Tail Node Representation Learning"></a>SAILOR: Structural Augmentation Based Tail Node Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06801">http://arxiv.org/abs/2308.06801</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jie-re/sailor">https://github.com/jie-re/sailor</a></li>
<li>paper_authors: Jie Liao, Jintang Li, Liang Chen, Bingzhe Wu, Yatao Bian, Zibin Zheng</li>
<li>for: 提高Graph Neural Networks（GNNs）对尾节点的表示学习效果，增强GNNs在真实世界图像中的表示能力。</li>
<li>methods: 提出了一种通用的Structural Augmentation based taIL nOde Representation learning框架，名为SAILOR，可以同时增强图структуры和提取更有用的尾节点表示。</li>
<li>results: 对公共 benchmark 数据集进行了广泛的实验，显示SAILOR可以显著提高尾节点表示，并超越当前的基线。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved state-of-the-art performance in representation learning for graphs recently. However, the effectiveness of GNNs, which capitalize on the key operation of message propagation, highly depends on the quality of the topology structure. Most of the graphs in real-world scenarios follow a long-tailed distribution on their node degrees, that is, a vast majority of the nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes since they lack structural information. In the pursuit of promoting the expressiveness of GNNs for tail nodes, we explore how the deficiency of structural information deteriorates the performance of tail nodes and propose a general Structural Augmentation based taIL nOde Representation learning framework, dubbed as SAILOR, which can jointly learn to augment the graph structure and extract more informative representations for tail nodes. Extensive experiments on public benchmark datasets demonstrate that SAILOR can significantly improve the tail node representations and outperform the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
GRAPHNeural Networks (GNNs) 最近已经达到了图表示学习中的状态艺术水平。然而，GNNs的效果，它们利用消息传递操作的关键，具体取决于图的结构质量。大多数实际场景中的图都follows a long-tailed distribution on node degrees, that is, most nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes because they lack structural information. In order to improve the expressiveness of GNNs for tail nodes, we explore how the lack of structural information degrades the performance of tail nodes and propose a general Structural Augmentation based tail Node Representation learning framework, called SAILOR, which can jointly learn to augment the graph structure and extract more informative representations for tail nodes. Extensive experiments on public benchmark datasets show that SAILOR can significantly improve the tail node representations and outperform the state-of-the-art baselines.Here's the word-for-word translation of the text into Simplified Chinese:GRAPHNeural Networks (GNNs) 最近已经达到了图表示学习中的状态艺术水平。然而，GNNs的效果，它们利用消息传递操作的关键，具体取决于图的结构质量。大多数实际场景中的图都follows a long-tailed distribution on node degrees, that is, most nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes because they lack structural information. In order to improve the expressiveness of GNNs for tail nodes, we explore how the lack of structural information degrades the performance of tail nodes and propose a general Structural Augmentation based tail Node Representation learning framework, called SAILOR, which can jointly learn to augment the graph structure and extract more informative representations for tail nodes. Extensive experiments on public benchmark datasets show that SAILOR can significantly improve the tail node representations and outperform the state-of-the-art baselines.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.AI_2023_08_14/" data-id="clohum90f002dpj88483a86mh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.CL_2023_08_14/" class="article-date">
  <time datetime="2023-08-14T11:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/cs.CL_2023_08_14/">cs.CL - 2023-08-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Human-centered-NLP-Fact-checking-Co-Designing-with-Fact-checkers-using-Matchmaking-for-AI"><a href="#Human-centered-NLP-Fact-checking-Co-Designing-with-Fact-checkers-using-Matchmaking-for-AI" class="headerlink" title="Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI"></a>Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07213">http://arxiv.org/abs/2308.07213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Houjiang Liu, Anubrata Das, Alexander Boltz, Didi Zhou, Daisy Pinaroc, Matthew Lease, Min Kyung Lee</li>
<li>for: 本研究旨在帮助职业Fact-checking增加效率和可扩展性，通过与Fact-checker合作设计AI工具，以满足Fact-checker的需求和价值观。</li>
<li>methods: 本研究使用了Matchmaking for AI方法，通过合作设计，让Fact-checker、设计师和NLP研究人员共同探索Fact-checker需要如何被技术支持，并从Fact-checker的角度设计AI工具。</li>
<li>results: 本研究在22名职业Fact-checker的合作设计中提出了11个新的设计想法，帮助Fact-checker更有效率地进行信息搜索、处理和写作任务，以及预防未来的谣言和减少自己的可能的偏见。<details>
<summary>Abstract</summary>
A key challenge in professional fact-checking is its limited scalability in relation to the magnitude of false information. While many Natural Language Processing (NLP) tools have been proposed to enhance fact-checking efficiency and scalability, both academic research and fact-checking organizations report limited adoption of such tooling due to insufficient alignment with fact-checker practices, values, and needs. To address this gap, we investigate a co-design method, Matchmaking for AI, which facilitates fact-checkers, designers, and NLP researchers to collaboratively discover what fact-checker needs should be addressed by technology and how. Our co-design sessions with 22 professional fact-checkers yielded a set of 11 novel design ideas. They assist in information searching, processing, and writing tasks for efficient and personalized fact-checking; help fact-checkers proactively prepare for future misinformation; monitor their potential biases; and support internal organization collaboration. Our work offers implications for human-centered fact-checking research and practice and AI co-design research.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在职业事实核查是它的限定可扩展性，与假信息的规模相对。虽然许多自然语言处理（NLP）工具已经被提议用于增强事实核查效率和可扩展性，但是学术研究和事实核查组织都报告了这些工具的采用率很低，主要是因为技术不符合事实核查员的做法、价值和需求。为解决这个差距，我们调查了一种合作设计方法，即Matchmaking for AI，该方法使得事实核查员、设计师和NLP研究人员共同探索事实核查员需要技术解决的问题和如何解决。我们与22名职业事实核查员进行了合作设计会议，得到了11项新的设计想法。这些设计想法可以帮助事实核查员更有效地搜索、处理和撰写信息，以及帮助他们预先准备对未来的假信息进行核查；监测他们的潜在偏见；以及支持内部组织合作。我们的工作对人类中心的事实核查研究和实践以及AI合作研究具有启示性。
</details></li>
</ul>
<hr>
<h2 id="ChatEval-Towards-Better-LLM-based-Evaluators-through-Multi-Agent-Debate"><a href="#ChatEval-Towards-Better-LLM-based-Evaluators-through-Multi-Agent-Debate" class="headerlink" title="ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate"></a>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07201">http://arxiv.org/abs/2308.07201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chanchimin/chateval">https://github.com/chanchimin/chateval</a></li>
<li>paper_authors: Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu</li>
<li>for: 这个论文的目的是提出一种多智能体评估方法，以取代人工评估，并使用多种语言模型合作来提高评估效果。</li>
<li>methods: 这个论文使用了多种语言模型，包括Transformer和Recurrent Neural Network，并通过多智能体评估方法来提高评估效果。</li>
<li>results: 这个论文的实验结果表明，使用多智能体评估方法可以提高评估效果，并且可以与人工评估相比。<details>
<summary>Abstract</summary>
Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.
</details>
<details>
<summary>摘要</summary>
文本评估历史上总是存在巨大的困难和成本高昂的问题。随着大语言模型（LLM）的出现，研究人员开始探索LLM的可能性作为人类评估的替代方案。although these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.因此，我们认为最佳的人类评估过程经常含有多个人类标注者在评估中合作，我们采用多代理人讨论框架，超越单代理人提示策略。这种多代理人基本思路可以让一群LLM协同作用，利用它们的不同能力和专业知识来提高处理复杂任务的效率和效果。在这篇论文中，我们构建了一个多代理人评审组called ChatEval，以自动讨论和评估不同模型生成的响应质量。我们的分析表明，ChatEval不仅可以对文本进行评分，还可以提供人类模仿的评估过程，为可靠的评估提供可靠的评估。我们的代码可以在https://github.com/chanchimin/ChatEval中获取。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Annotator-Uncertainty-into-Representations-of-Discourse-Relations"><a href="#Incorporating-Annotator-Uncertainty-into-Representations-of-Discourse-Relations" class="headerlink" title="Incorporating Annotator Uncertainty into Representations of Discourse Relations"></a>Incorporating Annotator Uncertainty into Representations of Discourse Relations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07179">http://arxiv.org/abs/2308.07179</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Magalí López Cortez, Cassandra L. Jacobs</li>
<li>for: 本研究探讨了不熟悉分类者对对话数据的谏话关系标注中的uncertainty。</li>
<li>methods: 研究使用对话上的单词、对话内speaker之间的对话、对话之间的对话 контекст来预测分类者的信任度。基于这些统计学特征，计算出谏话关系的分布式表示，并使用 hierarchical clustering 分析。</li>
<li>results: 研究发现，将分类者对谏话关系标注的uncertainty incorporated into distributed representations of discourse relations，可以准确地模型分类者的信任度。<details>
<summary>Abstract</summary>
Annotation of discourse relations is a known difficult task, especially for non-expert annotators. In this paper, we investigate novice annotators' uncertainty on the annotation of discourse relations on spoken conversational data. We find that dialogue context (single turn, pair of turns within speaker, and pair of turns across speakers) is a significant predictor of confidence scores. We compute distributed representations of discourse relations from co-occurrence statistics that incorporate information about confidence scores and dialogue context. We perform a hierarchical clustering analysis using these representations and show that weighting discourse relation representations with information about confidence and dialogue context coherently models our annotators' uncertainty about discourse relation labels.
</details>
<details>
<summary>摘要</summary>
描述关系标注是一项知名的困难任务，尤其是 для非专家标注员。本文研究非专家标注员对对话语音数据中描述关系的标注不确定性。我们发现对话上下文（单个转折、对话中 speaker 的对话对）是标注不确定性的重要预测因素。我们从共occurrence统计中计算出描述关系的分布式表示，并使用这些表示进行层次划分分析。我们发现，将描述关系表示与信度和对话上下文信息一起权重计算可以准确地模型我们的标注不确定性。
</details></li>
</ul>
<hr>
<h2 id="Mind-your-Language-Model-Fact-Checking-LLMs-and-their-Role-in-NLP-Research-and-Practice"><a href="#Mind-your-Language-Model-Fact-Checking-LLMs-and-their-Role-in-NLP-Research-and-Practice" class="headerlink" title="Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice"></a>Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07120">http://arxiv.org/abs/2308.07120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandra Sasha Luccioni, Anna Rogers</li>
<li>for: 本论文提出了大语言模型（LLMs）的定义，探讨了其功能和潜在应用，以及现有证据和反证据。</li>
<li>methods: 本论文使用了定义和探讨现有证据和反证据来探讨 LLMS 的功能和潜在应用。</li>
<li>results: 本论文提出了一个定义 LLMS，并探讨了现有证据和反证据，以及未来研究的可能性和方向。<details>
<summary>Abstract</summary>
Much of the recent discourse within the NLP research community has been centered around Large Language Models (LLMs), their functionality and potential -- yet not only do we not have a working definition of LLMs, but much of this discourse relies on claims and assumptions that are worth re-examining. This position paper contributes a definition of LLMs, explicates some of the assumptions made regarding their functionality, and outlines the existing evidence for and against them. We conclude with suggestions for research directions and their framing in future work.
</details>
<details>
<summary>摘要</summary>
很多最近的NLP研究社区的讨论都集中在大语言模型（LLMs）上，其功能和潜力，然而我们没有一个正式的定义，而且大多数这些讨论都基于可能需要重新评估的假设和宣称。这篇Position paper提供了LLMs的定义，探讨了关于它们的功能假设，并对现有证据进行了总结。我们结束于未来研究方向的建议和Future工作的框架。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Information-Retrieval-A-Survey"><a href="#Large-Language-Models-for-Information-Retrieval-A-Survey" class="headerlink" title="Large Language Models for Information Retrieval: A Survey"></a>Large Language Models for Information Retrieval: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07107">http://arxiv.org/abs/2308.07107</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ruc-nlpir/llm4ir-survey">https://github.com/ruc-nlpir/llm4ir-survey</a></li>
<li>paper_authors: Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, Ji-Rong Wen</li>
<li>For: This paper is focused on the integration of large language models (LLMs) with information retrieval (IR) systems to improve the accuracy and efficiency of IR systems.* Methods: The paper discusses various methods for combining LLMs with IR systems, including query rewriters, retrievers, rerankers, and readers. These methods aim to leverage the language understanding and generation capabilities of LLMs to improve the performance of IR systems.* Results: The paper provides a comprehensive overview of the current state of research in this field and highlights the promising directions for future research. It also discusses the challenges and limitations of integrating LLMs with IR systems, such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses.<details>
<summary>Abstract</summary>
As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions within this expanding field.
</details>
<details>
<summary>摘要</summary>
primary means of information acquisition，information retrieval（IR）系统，如搜索引擎，已经成为我们日常生活的重要组成部分。这些系统还作为对话，问答和推荐系统的组件。IR的发展轨迹从起源的单词方法发展到与高级神经网络模型结合。尽管神经网络模型能够捕捉复杂的上下文信号和语义差异，但仍面临数据缺乏，可读性和生成上下文可能准确但又不正确的响应的挑战。这种发展需要结合传统方法（如简单的term-based sparse retrieval方法）和现代神经网络 architecture（如语言模型）。 Meanwhile，大型语言模型（LLMs），如ChatGPT和GPT-4，对自然语言处理产生了革命，因为它们具有出色的语言理解、生成、总结和逻辑能力。因此，最近的研究尝试利用LLMs提高IR系统。给出 rapidevolving的研究轨迹，我们需要结合现有的方法和提供细化的洞察。在这个survey中，我们探讨了LLMs和IR系统的结合，包括关键的query rewriter，retriever，reranker和reader。此外，我们还探讨了这个扩展的领域中的可能的方向。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Sentence-Grounding-in-Streaming-Videos"><a href="#Temporal-Sentence-Grounding-in-Streaming-Videos" class="headerlink" title="Temporal Sentence Grounding in Streaming Videos"></a>Temporal Sentence Grounding in Streaming Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07102">http://arxiv.org/abs/2308.07102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sczwangxiao/tsgvs-mm2023">https://github.com/sczwangxiao/tsgvs-mm2023</a></li>
<li>paper_authors: Tian Gan, Xiao Wang, Yan Sun, Jianlong Wu, Qingpei Guo, Liqiang Nie</li>
<li>for: 本研究目标是解决一个新的任务：流动视频中的时间句子根据（TSGSV）。</li>
<li>methods: 我们提出了两种新方法：一种是一种双网络结构，帮助模型学习到来自未来帧的事件信息；另一种是一种语言引导的视觉压缩器，从涉及到查询的视觉帧中消除无关的帧并强化相关的帧。</li>
<li>results: 我们在ActivityNet Captions、TACoS和MAD datasets上进行了广泛的实验，结果表明我们提出的方法具有优势。一种系统的减少研究也证明了它们的有效性。<details>
<summary>Abstract</summary>
This paper aims to tackle a novel task - Temporal Sentence Grounding in Streaming Videos (TSGSV). The goal of TSGSV is to evaluate the relevance between a video stream and a given sentence query. Unlike regular videos, streaming videos are acquired continuously from a particular source, and are always desired to be processed on-the-fly in many applications such as surveillance and live-stream analysis. Thus, TSGSV is challenging since it requires the model to infer without future frames and process long historical frames effectively, which is untouched in the early methods. To specifically address the above challenges, we propose two novel methods: (1) a TwinNet structure that enables the model to learn about upcoming events; and (2) a language-guided feature compressor that eliminates redundant visual frames and reinforces the frames that are relevant to the query. We conduct extensive experiments using ActivityNet Captions, TACoS, and MAD datasets. The results demonstrate the superiority of our proposed methods. A systematic ablation study also confirms their effectiveness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>A TwinNet structure that enables the model to learn about upcoming events.2. A language-guided feature compressor that eliminates redundant visual frames and reinforces the frames that are relevant to the query.We conduct extensive experiments using ActivityNet Captions, TACoS, and MAD datasets. The results demonstrate the superiority of our proposed methods. A systematic ablation study also confirms their effectiveness.</details></li>
</ol>
<hr>
<h2 id="Aesthetics-of-Sanskrit-Poetry-from-the-Perspective-of-Computational-Linguistics-A-Case-Study-Analysis-on-Siksastaka"><a href="#Aesthetics-of-Sanskrit-Poetry-from-the-Perspective-of-Computational-Linguistics-A-Case-Study-Analysis-on-Siksastaka" class="headerlink" title="Aesthetics of Sanskrit Poetry from the Perspective of Computational Linguistics: A Case Study Analysis on Siksastaka"></a>Aesthetics of Sanskrit Poetry from the Perspective of Computational Linguistics: A Case Study Analysis on Siksastaka</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07081">http://arxiv.org/abs/2308.07081</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sanskritshala/shikshastakam">https://github.com/sanskritshala/shikshastakam</a></li>
<li>paper_authors: Jivnesh Sandhan, Amruta Barbadikar, Malay Maity, Pavankumar Satuluri, Tushar Sandhan, Ravi M. Gupta, Pawan Goyal, Laxmidhar Behera</li>
<li>for: 这篇论文旨在探讨 sanskrit 诗歌的计算语言学分析和分类方法，具体来说是通过人工智能和专家的协作来挖掘古代 sanskrit 诗歌中的隐藏美丽之处。</li>
<li>methods: 该论文提出了一个可解Framework，该框架包括机器学习和人工智能的混合使用，以及一个人类在循环中的协作方式，用于分析和分类诗歌的质量和特征。</li>
<li>results: 通过对 sanskrit 诗歌 “Siksastaka” 的分析和注释，该论文提供了一个深入的分析和评价，并提供了一个在线应用程序，以便未来的研究人员可以继续进行相关研究。<details>
<summary>Abstract</summary>
Sanskrit poetry has played a significant role in shaping the literary and cultural landscape of the Indian subcontinent for centuries. However, not much attention has been devoted to uncovering the hidden beauty of Sanskrit poetry in computational linguistics. This article explores the intersection of Sanskrit poetry and computational linguistics by proposing a roadmap of an interpretable framework to analyze and classify the qualities and characteristics of fine Sanskrit poetry. We discuss the rich tradition of Sanskrit poetry and the significance of computational linguistics in automatically identifying the characteristics of fine poetry. The proposed framework involves a human-in-the-loop approach that combines deterministic aspects delegated to machines and deep semantics left to human experts. We provide a deep analysis of Siksastaka, a Sanskrit poem, from the perspective of 6 prominent kavyashastra schools, to illustrate the proposed framework. Additionally, we provide compound, dependency, anvaya (prose order linearised form), meter, rasa (mood), alankar (figure of speech), and riti (writing style) annotations for Siksastaka and a web application to illustrate the poem's analysis and annotations. Our key contributions include the proposed framework, the analysis of Siksastaka, the annotations and the web application for future research. Link for interactive analysis: https://sanskritshala.github.io/shikshastakam/
</details>
<details>
<summary>摘要</summary>
sanskrit 诗歌在印度次大陆的文学和文化领域中扮演了重要的角色，但在计算语言学方面却没有受到充分的关注。本文探讨了 sanskrit 诗歌和计算语言学之间的交叉点，并提出了一个可解释的框架，用于分析和分类精美的 sanskrit 诗歌的特点和特色。我们讨论了 sanskrit 诗歌的丰富传统和计算语言学在自动识别精美诗歌的重要性。我们提出的框架采用了人类在 loop 的方法，将 deterministic 的方面委托给机器，而 deep semantics 则委托给人类专家。我们对 sanskrit 诗歌 "Siksastaka" 进行了6种 prominent kavyashastra 学派的深入分析，以 illustrate 我们的框架。此外，我们还提供了 Siksastaka 的 compound、dependency、anvaya（排序 linearized form）、米eter、rasa（情感）、alankar（ figura of speech）和 riti（书写风格）注释，以及一个网站，以便未来的研究。关于交互分析的链接：https://sanskritshala.github.io/shikshastakam/Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Can-Knowledge-Graphs-Simplify-Text"><a href="#Can-Knowledge-Graphs-Simplify-Text" class="headerlink" title="Can Knowledge Graphs Simplify Text?"></a>Can Knowledge Graphs Simplify Text?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06975">http://arxiv.org/abs/2308.06975</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/subhasmalik/Microsoft-azure-cognitive-services">https://github.com/subhasmalik/Microsoft-azure-cognitive-services</a></li>
<li>paper_authors: Anthony Colas, Haodi Ma, Xuanli He, Yang Bai, Daisy Zhe Wang</li>
<li>for: 这篇论文是关于无监督文本简化的研究，旨在使用知识图(KG)技术来生成简洁的文本，保持原始文本的意义。</li>
<li>methods: 该论文提出了一种名为KGSimple的新方法，它通过迭代和采样KG-first的方式，利用知识图生成的技术来生成简洁的文本，并保持原始文本的意义。</li>
<li>results: 该论文在使用现有的KG-to-text dataset进行评估，并示出了KGSimple模型的效果比起无监督文本简化模型更好。 Code available on GitHub.<details>
<summary>Abstract</summary>
Knowledge Graph (KG)-to-Text Generation has seen recent improvements in generating fluent and informative sentences which describe a given KG. As KGs are widespread across multiple domains and contain important entity-relation information, and as text simplification aims to reduce the complexity of a text while preserving the meaning of the original text, we propose KGSimple, a novel approach to unsupervised text simplification which infuses KG-established techniques in order to construct a simplified KG path and generate a concise text which preserves the original input's meaning. Through an iterative and sampling KG-first approach, our model is capable of simplifying text when starting from a KG by learning to keep important information while harnessing KG-to-text generation to output fluent and descriptive sentences. We evaluate various settings of the KGSimple model on currently-available KG-to-text datasets, demonstrating its effectiveness compared to unsupervised text simplification models which start with a given complex text. Our code is available on GitHub.
</details>
<details>
<summary>摘要</summary>
知识图（KG）-to-文本生成技术在最近得到了改进，能够生成流畅、有信息的句子，描述给定的KG。由于KG广泛存在多个领域，含有重要的实体关系信息，而文本简化的目标是将文本简化到最小化复杂度，保持原始文本的意思，因此我们提议KGSimple，一种新的无监督文本简化方法，利用KG确立的技术来构建简化KG路径，生成简洁的文本，保持原始输入的意思。我们采用迭代和采样KG-first方法，使我们的模型能够从KG开始简化文本，学习保留重要信息，同时利用KG-to-文本生成技术输出流畅、描述性的句子。我们在当前可用的KG-to-文本 datasets上评估了不同的KGSimple模型设置，并证明其比无监督文本简化模型，从给定的复杂文本开始简化文本更有效。我们的代码可以在GitHub上找到。
</details></li>
</ul>
<hr>
<h2 id="EcomGPT-Instruction-tuning-Large-Language-Model-with-Chain-of-Task-Tasks-for-E-commerce"><a href="#EcomGPT-Instruction-tuning-Large-Language-Model-with-Chain-of-Task-Tasks-for-E-commerce" class="headerlink" title="EcomGPT: Instruction-tuning Large Language Model with Chain-of-Task Tasks for E-commerce"></a>EcomGPT: Instruction-tuning Large Language Model with Chain-of-Task Tasks for E-commerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06966">http://arxiv.org/abs/2308.06966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangning Li, Shirong Ma, Xiaobin Wang, Shen Huang, Chengyue Jiang, Hai-Tao Zheng, Pengjun Xie, Fei Huang, Yong Jiang</li>
<li>For: The paper aims to address the challenge of using general language models for e-commerce tasks, and proposes a new dataset and a tailored model (EcomGPT) to improve the model’s performance on these tasks.* Methods: The paper introduces a new dataset called EcomInstruct, which consists of 2.5 million instruction data and is designed to scale up the data size and task diversity for e-commerce tasks. The model EcomGPT is trained on this dataset using the backbone model BLOOMZ, and the authors use a chain-of-task approach to improve the model’s generalization capabilities.* Results: The paper reports that EcomGPT outperforms ChatGPT in terms of cross-dataset&#x2F;task generalization on e-commerce tasks, as demonstrated through extensive experiments and human evaluations. The authors also show that EcomGPT acquires fundamental semantic understanding capabilities through the chain-of-task approach, which improves its performance on these tasks.Here are the three points in Simplified Chinese text:* For: 本研究旨在解决通用语言模型在电商任务上的挑战，并提出了一个新的数据集和一种适应型（EcomGPT），以提高这些任务的表现。* Methods: 本研究引入了一个新的数据集 called EcomInstruct，该数据集包含250万个指令数据，旨在扩大电商任务的数据大小和任务多样性。模型EcomGPT通过使用BLOOMZ的后托模型，在EcomInstruct上进行训练，并使用链式任务方法来提高模型的总体化能力。* Results: 本研究表明，EcomGPT比ChatGPT在电商任务上的跨数据集&#x2F;任务总体化性能更高，经过广泛的实验和人工评估。 authors也表明，EcomGPT通过链式任务方法获得了基本的Semantic理解能力，从而提高了它在这些任务上的表现。<details>
<summary>Abstract</summary>
Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first e-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT exhibits excellent zero-shot generalization capabilities. Extensive experiments and human evaluations demonstrate that EcomGPT outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "Recently" is translated as "最近" (most recent)* "Large Language Models" is translated as "大型自然语言模型" (large natural language models)* "E-commerce" is translated as "电商" (e-commerce)* "instruction" is translated as "指南" (instructions)* "dataset" is translated as "数据集" (dataset)* "task" is translated as "任务" (task)* "ChatGPT" is translated as "ChatGPT" (ChatGPT)* "EcomInstruct" is translated as "EcomInstruct" (E-commerce instruction dataset)* "atomic tasks" is translated as "原子任务" (atomic tasks)* "Chain-of-Task tasks" is translated as "链接任务" (chain-of-task tasks)* "backbone model" is translated as "基本模型" (backbone model)* "BLOOMZ" is translated as "BLOOMZ" (BLOOMZ)* "zero-shot generalization" is translated as "零批学习泛化" (zero-shot generalization)* "extensive experiments" is translated as "广泛的实验" (extensive experiments)* "human evaluations" is translated as "人类评估" (human evaluations)
</details></li>
</ul>
<hr>
<h2 id="Thresh-A-Unified-Customizable-and-Deployable-Platform-for-Fine-Grained-Text-Evaluation"><a href="#Thresh-A-Unified-Customizable-and-Deployable-Platform-for-Fine-Grained-Text-Evaluation" class="headerlink" title="Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation"></a>Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06953">http://arxiv.org/abs/2308.06953</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Heineman, Yao Dou, Wei Xu</li>
<li>for: 本研究是为了提供一个可靠、可重复的方式来评估文本生成任务，如摘要、简化、机器翻译和新闻生成。</li>
<li>methods: 本研究使用了一个名为Thresh的 plataform，该平台可以快速创建和测试任务特定的评估界面，并且可以在一个网页浏览器窗口中完成所有步骤。</li>
<li>results: 本研究通过Thresh平台可以快速创建和测试多种NLP任务的评估界面，并且可以提供多种批处理和大规模评估的选项。<details>
<summary>Abstract</summary>
Fine-grained, span-level human evaluation has emerged as a reliable and robust method for evaluating text generation tasks such as summarization, simplification, machine translation and news generation, and the derived annotations have been useful for training automatic metrics and improving language models. However, existing annotation tools implemented for these evaluation frameworks lack the adaptability to be extended to different domains or languages, or modify annotation settings according to user needs. And the absence of a unified annotated data format inhibits the research in multi-task learning. In this paper, we introduce Thresh, a unified, customizable and deployable platform for fine-grained evaluation. By simply creating a YAML configuration file, users can build and test an annotation interface for any framework within minutes -- all in one web browser window. To facilitate collaboration and sharing, Thresh provides a community hub that hosts a collection of fine-grained frameworks and corresponding annotations made and collected by the community, covering a wide range of NLP tasks. For deployment, Thresh offers multiple options for any scale of annotation projects from small manual inspections to large crowdsourcing ones. Additionally, we introduce a Python library to streamline the entire process from typology design and deployment to annotation processing. Thresh is publicly accessible at https://thresh.tools.
</details>
<details>
<summary>摘要</summary>
最细化的、span级人工评估已成为文本生成任务 such as 概要、简化、翻译和新闻生成的可靠和可靠的评估方法。但现有的评估工具不具备扩展到不同领域或语言的能力，也无法根据用户需求修改评估设置。此外，缺乏一个统一的注释数据格式，限制了多任务学习的研究。本文提出了 Thresh，一个统一、可定制和可部署的精细评估平台。通过创建一个 YAML 配置文件，用户可以在浏览器窗口内快速建立和测试任何框架的注释界面，并且可以在社区平台上共享和协作。为投入大规模注释项目，Thresh 提供多种部署选项。此外，我们还提供了一个 Python 库，以便从类型设计、部署到注释处理的整个过程。Thresh 公共访问地址为 <https://thresh.tools>。
</details></li>
</ul>
<hr>
<h2 id="Automated-Testing-and-Improvement-of-Named-Entity-Recognition-Systems"><a href="#Automated-Testing-and-Improvement-of-Named-Entity-Recognition-Systems" class="headerlink" title="Automated Testing and Improvement of Named Entity Recognition Systems"></a>Automated Testing and Improvement of Named Entity Recognition Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07937">http://arxiv.org/abs/2308.07937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boxi Yu, Yiyan Hu, Qiuyang Mang, Wenhan Hu, Pinjia He</li>
<li>for: 提高Named Entity Recognition（NER）系统的可靠性和精度，使其在不同的自然语言处理应用中更可靠。</li>
<li>methods: 提出了一种新的、广泛适用的方法，可以自动测试和修复不同的NER系统。</li>
<li>results: 通过测试两个state-of-the-art（SOTA）NER模型和两个商业NER API，发现自动测试和修复可以高效地提高NER系统的精度和可靠性。<details>
<summary>Abstract</summary>
Named entity recognition (NER) systems have seen rapid progress in recent years due to the development of deep neural networks. These systems are widely used in various natural language processing applications, such as information extraction, question answering, and sentiment analysis. However, the complexity and intractability of deep neural networks can make NER systems unreliable in certain circumstances, resulting in incorrect predictions. For example, NER systems may misidentify female names as chemicals or fail to recognize the names of minority groups, leading to user dissatisfaction. To tackle this problem, we introduce TIN, a novel, widely applicable approach for automatically testing and repairing various NER systems. The key idea for automated testing is that the NER predictions of the same named entities under similar contexts should be identical. The core idea for automated repairing is that similar named entities should have the same NER prediction under the same context. We use TIN to test two SOTA NER models and two commercial NER APIs, i.e., Azure NER and AWS NER. We manually verify 784 of the suspicious issues reported by TIN and find that 702 are erroneous issues, leading to high precision (85.0%-93.4%) across four categories of NER errors: omission, over-labeling, incorrect category, and range error. For automated repairing, TIN achieves a high error reduction rate (26.8%-50.6%) over the four systems under test, which successfully repairs 1,056 out of the 1,877 reported NER errors.
</details>
<details>
<summary>摘要</summary>
TIN的关键想法是在相似的上下文中，NER预测的相同名称应该是相同的。TIN的核心想法是在相似的上下文中，相似的名称应该有相同的NER预测。我们使用 TIN 测试了两个 SOTA NER 模型和两个商业 NER API，即 Azure NER 和 AWS NER。我们手动验证了 TIN 发现的 784 个可疑问题中的 702 个是错误的问题，得到了高精度（85.0%-93.4%）在四个NER错误类型中。对于自动修复，TIN 在四个系统上得到了高错误率（26.8%-50.6%），成功修复了 1,056 个reported NER 错误。
</details></li>
</ul>
<hr>
<h2 id="CausalLM-is-not-optimal-for-in-context-learning"><a href="#CausalLM-is-not-optimal-for-in-context-learning" class="headerlink" title="CausalLM is not optimal for in-context learning"></a>CausalLM is not optimal for in-context learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06912">http://arxiv.org/abs/2308.06912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, Radu Soricut</li>
<li>for: 本研究旨在理解在 context 学习中使用 prefix 语言模型 (prefixLM) 和 causal 语言模型 (causalLM) 的性能差异。</li>
<li>methods: 本研究采用了理论分析方法，对 prefixLM 和 causalLM 的参数构造进行分析，并通过synthetic 和实际任务的实验 verify 其理论结论。</li>
<li>results: 研究结果显示， prefixLM 在 linear 回归问题中 converges to 优质解，而 causalLM 的 convergence 动态类似于在线梯度下降算法，不能 garantate 优质性，即使样本数量无限大。 Empirical 实验表明， causalLM 在所有设置下consistently underperforms prefixLM。<details>
<summary>Abstract</summary>
Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.
</details>
<details>
<summary>摘要</summary>
近期实验证据表明，基于转换器的内容学习perform better使用预refix语言模型（prefixLM），因为所有的内容样本都可以互相attend，而不是 causalLM，它们使用自动征回应，禁止内容样本attend到未来的样本。 Although this result is intuitive, it is not understood from a theoretical perspective. 在这篇论文中，我们从理论角度分析了prefixLM和causalLM的整合行为。我们的分析显示，两种LM类型都会在某些参数构造下 converge to their stationary points at a linear rate，但是prefixLM会 converges to the optimal solution of linear regression，而causalLM的整合动态则类似于在线梯度下降算法，这并不是确保优化的，即使样本数量 infinitely grows。 我们在实验中补充了我们的理论声明，并通过使用不同的转换器和实验任务进行了empirical experiments。我们的实验结果表明，causalLM在所有设置中一直下perform prefixLM。
</details></li>
</ul>
<hr>
<h2 id="GIT-Mol-A-Multi-modal-Large-Language-Model-for-Molecular-Science-with-Graph-Image-and-Text"><a href="#GIT-Mol-A-Multi-modal-Large-Language-Model-for-Molecular-Science-with-Graph-Image-and-Text" class="headerlink" title="GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text"></a>GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06911">http://arxiv.org/abs/2308.06911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Liu, Yiming Ren, Zhixiang Ren</li>
<li>for: 这篇论文旨在开发一种多Modal语言模型，以捕捉分子数据的充分和复杂信息。</li>
<li>methods: 该论文使用了一种新的GIT-Former架构，可以将所有modalities映射到一个统一的 latent space中。</li>
<li>results: 该论文实现了一种创新的任意语言分子翻译策略，在分子captioning中提高了10%-15%的精度，在分子性预测中提高了5%-10%的准确率，并在分子生成中提高了20%的有效性。<details>
<summary>Abstract</summary>
Large language models have made significant strides in natural language processing, paving the way for innovative applications including molecular representation and generation. However, most existing single-modality approaches cannot capture the abundant and complex information in molecular data. Here, we introduce GIT-Mol, a multi-modal large language model that integrates the structure Graph, Image, and Text information, including the Simplified Molecular Input Line Entry System (SMILES) and molecular captions. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture capable of mapping all modalities into a unified latent space. Our study develops an innovative any-to-language molecular translation strategy and achieves a 10%-15% improvement in molecular captioning, a 5%-10% accuracy increase in property prediction, and a 20% boost in molecule generation validity compared to baseline or single-modality models.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型已经取得了重要进展，开创了新的应用领域，包括分子表示和生成。然而，现有的单模态方法通常无法捕捉分子数据中的丰富和复杂信息。我们介绍了 GIT-Mol，一个多模态大语言模型，该模型结合结构图、图像和文本信息，包括简化分子输入语言系统（SMILES）和分子描述。为了实现多modal分子数据的集成，我们提出了 GIT-Former 架构，可以将所有模式映射到一个统一的隐藏空间。我们的研究开发了一种创新的任意语言分子翻译策略，在分子描述中提高了10%-15%，在物理预测中提高了5%-10%，在分子生成VALIDIDAD中提高了20% compared to 基eline或单模态模型。
</details></li>
</ul>
<hr>
<h2 id="SpeechX-Neural-Codec-Language-Model-as-a-Versatile-Speech-Transformer"><a href="#SpeechX-Neural-Codec-Language-Model-as-a-Versatile-Speech-Transformer" class="headerlink" title="SpeechX: Neural Codec Language Model as a Versatile Speech Transformer"></a>SpeechX: Neural Codec Language Model as a Versatile Speech Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06873">http://arxiv.org/abs/2308.06873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, Takuya Yoshioka</li>
<li>for: 这篇论文旨在探讨高质量零参数文本识别模型，以及如何使其能够处理多种声音转换任务，包括噪音抑制、目标说话人提取、声音编辑等。</li>
<li>methods: 这篇论文提出了一种名为SpeechX的多任务学习模型，通过将语音编码语言模型和多任务学习相结合，实现了对声音转换任务的统一和可扩展的模型化。</li>
<li>results: 实验结果表明，SpeechX模型在不同任务中表现出色，包括零参数文本识别、噪音抑制、目标说话人提取、声音编辑等，与专门设计的模型相比，其性能均或超过专门的模型。<details>
<summary>Abstract</summary>
Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.
</details>
<details>
<summary>摘要</summary>
现代化的生成式speech模型，基于音频-文本提示，已经实现了高质量的零shot文本到语音。然而，现有的模型仍然面临着许多不同的音频-文本语音生成任务，包括音频 capture在不利的噪声条件下进行处理。这篇论文介绍了SpeechX，一种多功能的语音生成模型，可以实现零shot TTS和多种语音转换任务，并处理干净和噪声信号。SpeechX结合了神经编码语言模型和多任务学习，使用任务dependent的提示，实现了一个统一的和可扩展的模型，并提供了一种通用的文本输入方式来进行语音增强和转换任务。实验结果表明SpeechX在不同任务中具有优秀的性能，包括零shot TTS、噪声抑制、目标 speaker 提取、语音除去和语音编辑等，与专门的模型在任务中表现相当或更高。请参考 https://aka.ms/speechx 获取示例样本。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.CL_2023_08_14/" data-id="clohum94y0096pj8810jbflph" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.LG_2023_08_14/" class="article-date">
  <time datetime="2023-08-14T10:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/cs.LG_2023_08_14/">cs.LG - 2023-08-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Distance-Matters-For-Improving-Performance-Estimation-Under-Covariate-Shift"><a href="#Distance-Matters-For-Improving-Performance-Estimation-Under-Covariate-Shift" class="headerlink" title="Distance Matters For Improving Performance Estimation Under Covariate Shift"></a>Distance Matters For Improving Performance Estimation Under Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07223">http://arxiv.org/abs/2308.07223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/melanibe/distance_matters_performance_estimation">https://github.com/melanibe/distance_matters_performance_estimation</a></li>
<li>paper_authors: Mélanie Roschewitz, Ben Glocker</li>
<li>for: 本文旨在提出一种基于距离测试样本预期的训练分布的性能估计方法，以便在数据变换时进行安全的 AI 模型部署。</li>
<li>methods: 本文使用了一种基于距离测试样本预期的训练分布的方法，通过检查样本与预期的训练分布之间的距离，以避免在数据变换时取得不可靠的模型输出。</li>
<li>results:  experiments 表明，该方法可以在13种图像分类任务上提供 statistically 显著的性能估计改进（相对于最佳基准），并在10种任务上达到最佳性能。 code 可以在 <a target="_blank" rel="noopener" href="https://github.com/melanibe/distance_matters_performance_estimation">https://github.com/melanibe/distance_matters_performance_estimation</a> 中找到。<details>
<summary>Abstract</summary>
Performance estimation under covariate shift is a crucial component of safe AI model deployment, especially for sensitive use-cases. Recently, several solutions were proposed to tackle this problem, most leveraging model predictions or softmax confidence to derive accuracy estimates. However, under dataset shifts, confidence scores may become ill-calibrated if samples are too far from the training distribution. In this work, we show that taking into account distances of test samples to their expected training distribution can significantly improve performance estimation under covariate shift. Precisely, we introduce a "distance-check" to flag samples that lie too far from the expected distribution, to avoid relying on their untrustworthy model outputs in the accuracy estimation step. We demonstrate the effectiveness of this method on 13 image classification tasks, across a wide-range of natural and synthetic distribution shifts and hundreds of models, with a median relative MAE improvement of 27% over the best baseline across all tasks, and SOTA performance on 10 out of 13 tasks. Our code is publicly available at https://github.com/melanibe/distance_matters_performance_estimation.
</details>
<details>
<summary>摘要</summary>
In this work, we show that taking into account the distances of test samples to their expected training distribution can significantly improve performance estimation under covariate shift. Specifically, we introduce a "distance-check" to flag samples that lie too far from the expected distribution, in order to avoid relying on their untrustworthy model outputs in the accuracy estimation step.We demonstrate the effectiveness of this method on 13 image classification tasks, across a wide range of natural and synthetic distribution shifts and hundreds of models. Our results show a median relative MAE improvement of 27% over the best baseline across all tasks, and SOTA performance on 10 out of 13 tasks. Our code is publicly available at <https://github.com/melanibe/distance_matters_performance_estimation>.
</details></li>
</ul>
<hr>
<h2 id="AudioFormer-Audio-Transformer-learns-audio-feature-representations-from-discrete-acoustic-codes"><a href="#AudioFormer-Audio-Transformer-learns-audio-feature-representations-from-discrete-acoustic-codes" class="headerlink" title="AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes"></a>AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07221">http://arxiv.org/abs/2308.07221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LZH-0225/AudioFormer">https://github.com/LZH-0225/AudioFormer</a></li>
<li>paper_authors: Zhaohui Li, Haitao Wang, Xinghua Jiang</li>
<li>for: 本研究旨在提出一种名为AudioFormer的方法，该方法通过获得隐藏的音频编码并在其基础之上进行微调，以便为音频分类任务进行特征表示。</li>
<li>methods: 我们首先提出一种新的思路，即将音频分类任务看作自然语言理解（NLU）的一种形式。然后，我们利用现有的神经网络音频编码器模型，生成隐藏的音频编码，并将其用于训练一个假名句语言模型（MLM），从而获得音频特征表示。此外，我们还提出了一种多 positivesample contrastive（MPC）学习方法，该方法可以学习多个隐藏的音频编码之间的共同表示。</li>
<li>results: 在我们的实验中，我们将隐藏的音频编码视为文本数据，并使用cloze-like方法训练一个假名句语言模型，最终获得高质量的音频表示。特别是，MPC学习技术可以有效地捕捉多个正样本之间的协同表示。我们的研究结果表明，AudioFormer在多个数据集上达到了 significatively improved的性能，甚至超过了一些音视频多模态分类模型。具体的表现为：在AudioSet（2M,20K）、FSD50K等数据集上，AudioFormer的性能分别为53.9、45.1和65.6。我们已经公开分享了代码和模型：<a target="_blank" rel="noopener" href="https://github.com/LZH-0225/AudioFormer.git%E3%80%82">https://github.com/LZH-0225/AudioFormer.git。</a><details>
<summary>Abstract</summary>
We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures collaborative representations among distinct positive samples. Our research outcomes demonstrate that AudioFormer attains significantly improved performance compared to prevailing monomodal audio classification models across multiple datasets,and even outperforms audio-visual multimodal classification models on select datasets. Specifically,our approach achieves remarkable results on datasets including AudioSet (2M,20K),and FSD50K,with performance scores of 53.9,45.1,and 65.6,respectively. We have openly shared both the code and models: https://github.com/LZH-0225/AudioFormer.git.
</details>
<details>
<summary>摘要</summary>
我们提出一种方法 named AudioFormer，它通过获取批量音频编码并进行精细调整，以便为音频分类任务学习音频特征表示。我们首先提出一种新的视角，即视音频分类任务为自然语言理解（NLU）的一种形式。利用现有的神经网络音频编码器模型，我们生成了批量音频编码，并使用它们训练一个隐藏状态语言模型（MLM），从而获得音频特征表示。此外，我们开拓了多个正样本对比（MPC）学习方法的 интеграción。这种方法使得在同一个音频输入中学习多个批量音频编码的联合表示。在我们的实验中，我们将批量音频编码视为文本数据，并使用cloze-like方法训练一个隐藏状态语言模型，最终得到高质量的音频表示。各种实验结果表明，AudioFormer在多个数据集上达到了 significativamente提高的性能，并在一些数据集上 even outperform 音频-视觉多模态分类模型。具体来说，我们的方法在AudioSet（2M,20K）、FSD50K 等数据集上达到了性能分数为 53.9、45.1 和 65.6 等。我们已经在 GitHub 上公开分享了代码和模型：https://github.com/LZH-0225/AudioFormer.git。
</details></li>
</ul>
<hr>
<h2 id="Generating-Individual-Trajectories-Using-GPT-2-Trained-from-Scratch-on-Encoded-Spatiotemporal-Data"><a href="#Generating-Individual-Trajectories-Using-GPT-2-Trained-from-Scratch-on-Encoded-Spatiotemporal-Data" class="headerlink" title="Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data"></a>Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07940">http://arxiv.org/abs/2308.07940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taizo Horikomi, Shouji Fujimoto, Atushi Ishikawa, Takayuki Mizuno</li>
<li>for: 本研究使用GPT-2语言模型来生成个人日常路径序列，以考虑环境因素和个人特征的影响。</li>
<li>methods: 研究人员使用了坐标转换技术将地理坐标表示为特定的位置符号，并将每天的路径序列表示为一系列这些位置符号。特定的时间间隔符号和环境因素符号也被添加到序列中，以便在GPT-2架构上进行训练。</li>
<li>results: 通过训练这些位置符号和时间间隔符号，研究人员可以生成受环境因素和个人特征影响的个人日常路径序列。<details>
<summary>Abstract</summary>
Following Mizuno, Fujimoto, and Ishikawa's research (Front. Phys. 2022), we transpose geographical coordinates expressed in latitude and longitude into distinctive location tokens that embody positions across varied spatial scales. We encapsulate an individual daily trajectory as a sequence of tokens by adding unique time interval tokens to the location tokens. Using the architecture of an autoregressive language model, GPT-2, this sequence of tokens is trained from scratch, allowing us to construct a deep learning model that sequentially generates an individual daily trajectory. Environmental factors such as meteorological conditions and individual attributes such as gender and age are symbolized by unique special tokens, and by training these tokens and trajectories on the GPT-2 architecture, we can generate trajectories that are influenced by both environmental factors and individual attributes.
</details>
<details>
<summary>摘要</summary>
根据米泽野、藤本和石川等人的研究（Front. Phys. 2022），我们将地理坐标表示为纬度和经度转换为特征化的位置标记，这些标记表示在不同的空间尺度上的位置。我们将每天的行走路径序列为一系列标记，并将具有特定时间间隔的唯一标记添加到位置标记中。使用GPT-2架构的自然语言模型，我们从头开始训练这些标记和路径，以生成基于环境因素和个人特征的各天行走路径。特殊的环境因素和个人特征被象化为唯一的特殊标记，通过训练这些标记和路径，我们可以生成受环境因素和个人特征影响的行走路径。
</details></li>
</ul>
<hr>
<h2 id="Automated-Ensemble-Based-Segmentation-of-Pediatric-Brain-Tumors-A-Novel-Approach-Using-the-CBTN-CONNECT-ASNR-MICCAI-BraTS-PEDs-2023-Challenge-Data"><a href="#Automated-Ensemble-Based-Segmentation-of-Pediatric-Brain-Tumors-A-Novel-Approach-Using-the-CBTN-CONNECT-ASNR-MICCAI-BraTS-PEDs-2023-Challenge-Data" class="headerlink" title="Automated Ensemble-Based Segmentation of Pediatric Brain Tumors: A Novel Approach Using the CBTN-CONNECT-ASNR-MICCAI BraTS-PEDs 2023 Challenge Data"></a>Automated Ensemble-Based Segmentation of Pediatric Brain Tumors: A Novel Approach Using the CBTN-CONNECT-ASNR-MICCAI BraTS-PEDs 2023 Challenge Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07212">http://arxiv.org/abs/2308.07212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashidhar Reddy Javaji, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</li>
<li>for: 这个研究旨在发展deep learning技术，对于脑癌诊断和治疗方法进行改进。</li>
<li>methods: 这个研究使用了深度学习技术，包括ONet和modified UNet，以及新的损失函数。</li>
<li>results:  ensemble方法可以实现更高的精度和更好的特征捕捉，实现lesion_wise dice scores的0.52、0.72和0.78，并且可以更好地覆盖肿瘤区域。<details>
<summary>Abstract</summary>
Brain tumors remain a critical global health challenge, necessitating advancements in diagnostic techniques and treatment methodologies. In response to the growing need for age-specific segmentation models, particularly for pediatric patients, this study explores the deployment of deep learning techniques using magnetic resonance imaging (MRI) modalities. By introducing a novel ensemble approach using ONet and modified versions of UNet, coupled with innovative loss functions, this study achieves a precise segmentation model for the BraTS-PEDs 2023 Challenge. Data augmentation, including both single and composite transformations, ensures model robustness and accuracy across different scanning protocols. The ensemble strategy, integrating the ONet and UNet models, shows greater effectiveness in capturing specific features and modeling diverse aspects of the MRI images which result in lesion_wise dice scores of 0.52, 0.72 and 0.78 for enhancing tumor, tumor core and whole tumor labels respectively. Visual comparisons further confirm the superiority of the ensemble method in accurate tumor region coverage. The results indicate that this advanced ensemble approach, building upon the unique strengths of individual models, offers promising prospects for enhanced diagnostic accuracy and effective treatment planning for brain tumors in pediatric brains.
</details>
<details>
<summary>摘要</summary>
�� funcionado global health challenge, requiring advancements in diagnostic techniques and treatment methodologies. In response to the growing need for age-specific segmentation models, particularly for pediatric patients, this study explores the deployment of deep learning techniques using magnetic resonance imaging (MRI) modalities. By introducing a novel ensemble approach using ONet and modified versions of UNet, coupled with innovative loss functions, this study achieves a precise segmentation model for the BraTS-PEDs 2023 Challenge. Data augmentation, including both single and composite transformations, ensures model robustness and accuracy across different scanning protocols. The ensemble strategy, integrating the ONet and UNet models, shows greater effectiveness in capturing specific features and modeling diverse aspects of the MRI images which result in lesion_wise dice scores of 0.52, 0.72 and 0.78 for enhancing tumor, tumor core and whole tumor labels respectively. Visual comparisons further confirm the superiority of the ensemble method in accurate tumor region coverage. The results indicate that this advanced ensemble approach, building upon the unique strengths of individual models, offers promising prospects for enhanced diagnostic accuracy and effective treatment planning for brain tumors in pediatric brains.Here's the word-for-word translation:�Git tumors remain a critical global health challenge, necessitating advancements in diagnostic techniques and treatment methodologies. In response to the growing need for age-specific segmentation models, particularly for pediatric patients, this study explores the deployment of deep learning techniques using magnetic resonance imaging (MRI) modalities. By introducing a novel ensemble approach using ONet and modified versions of UNet, coupled with innovative loss functions, this study achieves a precise segmentation model for the BraTS-PEDs 2023 Challenge. Data augmentation, including both single and composite transformations, ensures model robustness and accuracy across different scanning protocols. The ensemble strategy, integrating the ONet and UNet models, shows greater effectiveness in capturing specific features and modeling diverse aspects of the MRI images which result in lesion_wise dice scores of 0.52, 0.72 and 0.78 for enhancing tumor, tumor core and whole tumor labels respectively. Visual comparisons further confirm the superiority of the ensemble method in accurate tumor region coverage. The results indicate that this advanced ensemble approach, building upon the unique strengths of individual models, offers promising prospects for enhanced diagnostic accuracy and effective treatment planning for brain tumors in pediatric brains.
</details></li>
</ul>
<hr>
<h2 id="Unified-Data-Free-Compression-Pruning-and-Quantization-without-Fine-Tuning"><a href="#Unified-Data-Free-Compression-Pruning-and-Quantization-without-Fine-Tuning" class="headerlink" title="Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning"></a>Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07209">http://arxiv.org/abs/2308.07209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shipeng Bai, Jun Chen, Xintian Shen, Yixuan Qian, Yong Liu</li>
<li>for: 提高神经网络的推理时间和内存占用的压缩和量化方法，但大多数现有方法需要原始训练集来微调模型，带来负担重大并不适用于敏感或商业化数据的应用。</li>
<li>methods: 提出了一些数据自由方法，但它们分别进行数据自由压缩和量化，而不是同时进行压缩和量化。</li>
<li>results: 在大规模图像分类任务中，我们的方法（Unified Data-Free Compression，UDFC）可以在不需要数据和微调过程的情况下，同时进行压缩和量化，并实现了与现有方法相当的性能提升。例如，在ImageNet dataset上，我们对ResNet-34网络进行30%压缩和6比特量化后，与最佳方法相比，我们的方法可以达到20.54%的精度提升。<details>
<summary>Abstract</summary>
Structured pruning and quantization are promising approaches for reducing the inference time and memory footprint of neural networks. However, most existing methods require the original training dataset to fine-tune the model. This not only brings heavy resource consumption but also is not possible for applications with sensitive or proprietary data due to privacy and security concerns. Therefore, a few data-free methods are proposed to address this problem, but they perform data-free pruning and quantization separately, which does not explore the complementarity of pruning and quantization. In this paper, we propose a novel framework named Unified Data-Free Compression(UDFC), which performs pruning and quantization simultaneously without any data and fine-tuning process. Specifically, UDFC starts with the assumption that the partial information of a damaged(e.g., pruned or quantized) channel can be preserved by a linear combination of other channels, and then derives the reconstruction form from the assumption to restore the information loss due to compression. Finally, we formulate the reconstruction error between the original network and its compressed network, and theoretically deduce the closed-form solution. We evaluate the UDFC on the large-scale image classification task and obtain significant improvements over various network architectures and compression methods. For example, we achieve a 20.54% accuracy improvement on ImageNet dataset compared to SOTA method with 30% pruning ratio and 6-bit quantization on ResNet-34.
</details>
<details>
<summary>摘要</summary>
《结构化剪辑和量化是减少神经网络推理时间和内存占用的有效方法。然而，大多数现有方法需要原始训练集来精度调整模型，这不仅带来重要资源占用，还不可能 для涉及隐私或商业机密的应用程序 due to privacy and security concerns。因此，一些无数据方法被提议，但它们分别进行无数据剪辑和量化，而不是探索剪辑和量化的共同优势。在这篇论文中，我们提出了一个名为统一无数据压缩（UDFC）的新框架，它在无数据情况下同时进行剪辑和量化。具体来说，UDFC从假设部分频道（例如剪辑或量化）的信息可以通过其他频道的线性组合来保留一些信息，然后 derive 恢复形式来恢复因压缩而产生的信息损失。最后，我们将重建误差 между 原始网络和压缩后的网络，并 theoretically 递归解决。我们在大规模图像分类任务上评估了UDFC，并实现了对不同网络架构和压缩方法的显著改进。例如，我们在 ImageNet 数据集上实现了与 SOTA 方法相比的 20.54% 的准确率提升，其中 ResNet-34 网络的 30% 剪辑率和 6 位量化。
</details></li>
</ul>
<hr>
<h2 id="Algorithms-for-the-Training-of-Neural-Support-Vector-Machines"><a href="#Algorithms-for-the-Training-of-Neural-Support-Vector-Machines" class="headerlink" title="Algorithms for the Training of Neural Support Vector Machines"></a>Algorithms for the Training of Neural Support Vector Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07204">http://arxiv.org/abs/2308.07204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Lars Simon, Manuel Radons</li>
<li>for: 本文旨在探讨基于域知识的神经支持向量机（NSVM）模型的设计，以及一些基于Pegasos算法的训练算法。</li>
<li>methods: 本文使用Pegasos算法和一些基于神经网络的训练算法来训练NSVM模型。</li>
<li>results: 本文通过解决一些标准机器学习任务来证明NSVM模型的可行性。<details>
<summary>Abstract</summary>
Neural support vector machines (NSVMs) allow for the incorporation of domain knowledge in the design of the model architecture. In this article we introduce a set of training algorithms for NSVMs that leverage the Pegasos algorithm and provide a proof of concept by solving a set of standard machine learning tasks.
</details>
<details>
<summary>摘要</summary>
神经支持向量机器 (NSVM) 允许在模型建立之处 incorporate 领域知识。在这篇文章中，我们介绍了一组用 Pegasos 算法进行训练 NSVM 的算法，并通过解决一组标准机器学习任务来提供证明。Note that "神经支持向量机器" (NSVM) is the Simplified Chinese term for "neural support vector machine".
</details></li>
</ul>
<hr>
<h2 id="Neural-Categorical-Priors-for-Physics-Based-Character-Control"><a href="#Neural-Categorical-Priors-for-Physics-Based-Character-Control" class="headerlink" title="Neural Categorical Priors for Physics-Based Character Control"></a>Neural Categorical Priors for Physics-Based Character Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07200">http://arxiv.org/abs/2308.07200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tencent-RoboticsX/NCP">https://github.com/Tencent-RoboticsX/NCP</a></li>
<li>paper_authors: Qingxu Zhu, He Zhang, Mengting Lan, Lei Han</li>
<li>for: 本研究目的是提出一种新的学习框架，用于控制基于物理学的人工智能角色，以实现更高质量和多样性的运动。</li>
<li>methods: 本研究使用了强化学习（RL）来跟踪和模仿生命力运动的精准信息，并使用了量化自适应变换器（VQ-VAE）来压缩运动clip中的最重要信息。</li>
<li>results: 研究结果表明，提出的方法可以控制人工智能角色进行高质量、多样化的运动，并且可以在两个复杂的下游任务中表现出色，包括剑盾攻击和两个玩家拳击游戏。<details>
<summary>Abstract</summary>
Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervision of the encoder's output, it follows the original motion clip distribution in the dataset and could lead to imbalanced behaviors in our setting. To address the issue, we further propose a technique named prior shifting to adjust the prior distribution using curiosity-driven RL. The outcome distribution is demonstrated to offer sufficient behavioral diversity and significantly facilitates upper-level policy learning for downstream tasks. We conduct comprehensive experiments using humanoid characters on two challenging downstream tasks, sword-shield striking and two-player boxing game. Our results demonstrate that the proposed framework is capable of controlling the character to perform considerably high-quality movements in terms of behavioral strategies, diversity, and realism. Videos, codes, and data are available at https://tencent-roboticsx.github.io/NCP/.
</details>
<details>
<summary>摘要</summary>
近期研究生成可重用运动先验的进步已经证明了它们在生成自然化行为方面的效果。在这篇论文中，我们提出了一种新的学习框架，用于控制基于物理学的角色，并提高了现有状态艺术方法的运动质量和多样性。我们的方法使用了奖励学习（RL）来初始化并模仿生命体运动，使用不结构化运动片段中的精炼信息，并使用Vector Quantized Variational AutoEncoder（VQ-VAE）结构压缩运动片段中的最重要信息。这种结构将运动片段中的信息压缩成一个紧凑而有用的秘密空间中，通过从已经训练的分类先验分布中采样代码，可以生成高质量的生命体运动。虽然这种先验分布可以通过Encoder的输出进行超vision训练，但它遵循原始运动片段分布，可能会导致行为偏好。为解决这个问题，我们进一步提出了一种名为“先验偏移”的技术，通过吸引力驱动RL来调整先验分布。结果显示，我们的框架可以控制角色进行高质量的运动，包括行为策略、多样性和真实性。我们在人iform机器人上进行了广泛的实验，并在剑盾战和两个玩家盒子游戏中进行了两个下游任务。我们的结果表明，我们的框架可以控制角色进行较高质量的运动，并且可以提高下游任务的性能。视频、代码和数据可以在https://tencent-roboticsx.github.io/NCP/上获取。
</details></li>
</ul>
<hr>
<h2 id="Explaining-Black-Box-Models-through-Counterfactuals"><a href="#Explaining-Black-Box-Models-through-Counterfactuals" class="headerlink" title="Explaining Black-Box Models through Counterfactuals"></a>Explaining Black-Box Models through Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07198">http://arxiv.org/abs/2308.07198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juliatrustworthyai/counterfactualexplanations.jl">https://github.com/juliatrustworthyai/counterfactualexplanations.jl</a></li>
<li>paper_authors: Patrick Altmeyer, Arie van Deursen, Cynthia C. S. Liem</li>
<li>for: 这篇论文是用于解释人工智能的Explainable Artificial Intelligence（ExplaI）。</li>
<li>methods: 这篇论文使用Counterfactual Explanations（CE）和Algorithmic Recourse（AR）来解释黑盒模型的预测结果。</li>
<li>results: 这篇论文提供了一个用于Julia语言的CounterfactualExplanations.jl包，可以生成Counterfactual Explanations和Algorithmic Recourse，并且可以用于解释任何黑盒模型的预测结果。<details>
<summary>Abstract</summary>
We present CounterfactualExplanations.jl: a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box models in Julia. CE explain how inputs into a model need to change to yield specific model predictions. Explanations that involve realistic and actionable changes can be used to provide AR: a set of proposed actions for individuals to change an undesirable outcome for the better. In this article, we discuss the usefulness of CE for Explainable Artificial Intelligence and demonstrate the functionality of our package. The package is straightforward to use and designed with a focus on customization and extensibility. We envision it to one day be the go-to place for explaining arbitrary predictive models in Julia through a diverse suite of counterfactual generators.
</details>
<details>
<summary>摘要</summary>
我们介绍CounterfactualExplanations.jl：一个用于生成Counterfactual Explanations（CE）和Algorithmic Recourse（AR）的套件，用于黑盒模型中的Julia。CE解释了如何让模型的输入变化以获得具体预测。这些解释可以提供AR：一组建议行动，以改善不愉快的结果。在这篇文章中，我们讨论了Counterfactual Explanations在可解释人工智能中的用途，并详细介绍了我们的套件。套件易于使用，设计了一个重点在自定义和扩展。我们将这个套件作为Julia中解释任意预测模型的首选场所。
</details></li>
</ul>
<hr>
<h2 id="gSASRec-Reducing-Overconfidence-in-Sequential-Recommendation-Trained-with-Negative-Sampling"><a href="#gSASRec-Reducing-Overconfidence-in-Sequential-Recommendation-Trained-with-Negative-Sampling" class="headerlink" title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling"></a>gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07192">http://arxiv.org/abs/2308.07192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asash/gsasrec">https://github.com/asash/gsasrec</a></li>
<li>paper_authors: Aleksandr Petrov, Craig Macdonald</li>
<li>for: 这篇论文旨在解释为何SASRec模型在比较BERT4Rec模型时表现不佳，并提出一种新的总体二进制十字Entropy损失函数（gBCE）以及改进后的gSASRec模型，以 Mitigate overconfidence问题。</li>
<li>methods: 这篇论文使用了SASRec模型和BERT4Rec模型，并对它们进行了比较。它还提出了一种新的总体二进制十字Entropy损失函数（gBCE），并证明了它可以降低overconfidence问题。</li>
<li>results: 这篇论文通过了详细的实验表明，gSASRec模型可以在三个 datasets上不受overconfidence问题的影响，并且可以超越BERT4Rec模型（例如，MovieLens-1M数据集上的NDCG提高了9.47%），同时需要更少的训练时间（例如，MovieLens-1M数据集上的训练时间减少了73%）。<details>
<summary>Abstract</summary>
A large catalogue size is one of the central challenges in training recommendation models: a large number of items makes them memory and computationally inefficient to compute scores for all items during training, forcing these models to deploy negative sampling. However, negative sampling increases the proportion of positive interactions in the training data, and therefore models trained with negative sampling tend to overestimate the probabilities of positive interactions a phenomenon we call overconfidence. While the absolute values of the predicted scores or probabilities are not important for the ranking of retrieved recommendations, overconfident models may fail to estimate nuanced differences in the top-ranked items, resulting in degraded performance. In this paper, we show that overconfidence explains why the popular SASRec model underperforms when compared to BERT4Rec. This is contrary to the BERT4Rec authors explanation that the difference in performance is due to the bi-directional attention mechanism. To mitigate overconfidence, we propose a novel Generalised Binary Cross-Entropy Loss function (gBCE) and theoretically prove that it can mitigate overconfidence. We further propose the gSASRec model, an improvement over SASRec that deploys an increased number of negatives and the gBCE loss. We show through detailed experiments on three datasets that gSASRec does not exhibit the overconfidence problem. As a result, gSASRec can outperform BERT4Rec (e.g. +9.47% NDCG on the MovieLens-1M dataset), while requiring less training time (e.g. -73% training time on MovieLens-1M). Moreover, in contrast to BERT4Rec, gSASRec is suitable for large datasets that contain more than 1 million items.
</details>
<details>
<summary>摘要</summary>
大型目录大小是训练推荐模型的中心挑战之一：大量的项目使得计算和存储成本增加，使得这些模型在训练期间计算所有项目的得分变得不可能。然而，使用负样本增加了正交互动的比例在训练数据中，因此模型受负样本部署后会过度估计正交互动的现象，我们称之为过信任。虽然绝对值的预测分数或概率不重要于推荐结果的排序，但过信任的模型可能无法估计顶层推荐的细微差异，导致性能下降。在这篇论文中，我们表明了过信任问题导致SASRec模型在比较BERT4Rec时表现不佳。这与BERT4Rec作者的解释不同，即Bi-directional attention机制导致的差异。为了消除过信任，我们提出一种通用二进制十进制损失函数（gBCE），并论证它可以消除过信任。此外，我们还提出了gSASRec模型，它在SASRec模型的基础上增加了更多的负样本和gBCE损失函数。我们通过三个数据集的详细实验表明，gSASRec模型不会出现过信任问题。因此，gSASRec可以超过BERT4Rec（例如，MovieLens-1M数据集上的NDCG提高9.47%），同时需要较少的训练时间（例如，MovieLens-1M数据集上的训练时间减少73%）。此外，gSASRec模型适用于包含更多 чем100万个项目的大型数据集。
</details></li>
</ul>
<hr>
<h2 id="Improving-ICD-based-semantic-similarity-by-accounting-for-varying-degrees-of-comorbidity"><a href="#Improving-ICD-based-semantic-similarity-by-accounting-for-varying-degrees-of-comorbidity" class="headerlink" title="Improving ICD-based semantic similarity by accounting for varying degrees of comorbidity"></a>Improving ICD-based semantic similarity by accounting for varying degrees of comorbidity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07359">http://arxiv.org/abs/2308.07359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Janosch Schneider, Marius Adler, Christoph Ammer-Herrmenau, Alexander Otto König, Ulrich Sax, Jonas Hügel<br>for: 这篇论文的目的是什么？* 这篇论文的目的是找出类似的病人，以便评估治疗结果和促进临床决策。methods: 这篇论文使用了哪些方法？* 这篇论文使用了 semantic similarity algorithms，包括 level-based information content、Leacock &amp; Chodorow concept similarity 和 bipartite graph matching。results: 这篇论文的结果是什么？* 这篇论文的结果表明， Accounting for comorbidity variance can significantly improve the performance of semantic similarity algorithms。最佳结果为 level-based information content、Leacock &amp; Chodorow concept similarity 和 bipartite graph matching的 комbination，与专家评验的真实值相符。<details>
<summary>Abstract</summary>
Finding similar patients is a common objective in precision medicine, facilitating treatment outcome assessment and clinical decision support. Choosing widely-available patient features and appropriate mathematical methods for similarity calculations is crucial. International Statistical Classification of Diseases and Related Health Problems (ICD) codes are used worldwide to encode diseases and are available for nearly all patients. Aggregated as sets consisting of primary and secondary diagnoses they can display a degree of comorbidity and reveal comorbidity patterns. It is possible to compute the similarity of patients based on their ICD codes by using semantic similarity algorithms. These algorithms have been traditionally evaluated using a single-term expert rated data set.   However, real-word patient data often display varying degrees of documented comorbidities that might impair algorithm performance. To account for this, we present a scale term that considers documented comorbidity-variance. In this work, we compared the performance of 80 combinations of established algorithms in terms of semantic similarity based on ICD-code sets. The sets have been extracted from patients with a C25.X (pancreatic cancer) primary diagnosis and provide a variety of different combinations of ICD-codes. Using our scale term we yielded the best results with a combination of level-based information content, Leacock & Chodorow concept similarity and bipartite graph matching for the set similarities reaching a correlation of 0.75 with our expert's ground truth. Our results highlight the importance of accounting for comorbidity variance while demonstrating how well current semantic similarity algorithms perform.
</details>
<details>
<summary>摘要</summary>
寻找类似病人是精准医学中常见的目标，可以促进治疗结果评估和临床决策支持。选择广泛可用的病人特征和适当的数学方法进行相似性计算是关键。国际疾病分类和相关医学问题（ICD）代码是全球通用的疾病编码，可以为大多数病人提供。将这些代码集成为主要和次要诊断的集合，可以显示疾病复杂性和潜在的疾病模式。可以使用语义相似算法计算病人之间的相似性。这些算法traditionally被评估使用专家评分的单个数据集。但是，实际的病人数据经常具有不同程度的记录的相关疾病，这可能会影响算法性能。为了考虑这一点，我们提出了一个权重因子，以考虑记录的相关疾病差异。在这种情况下，我们比较了80组已知算法的语义相似性，基于ICD代码集。这些代码集来自悉尼癌病（C25.X）主诊断的病人，并提供了不同的ICD代码组合。通过我们的权重因子，我们得到了最佳的结果，其中包括水平基本信息内容、Leacock & Chodorow概念相似和 биipartite图匹配算法，达到了专家的参考真实值的0.75相似度。我们的结果 highlights the importance of accounting for comorbidity variance while demonstrating the current state-of-the-art semantic similarity algorithms perform well.
</details></li>
</ul>
<hr>
<h2 id="Conformal-Predictions-Enhanced-Expert-guided-Meshing-with-Graph-Neural-Networks"><a href="#Conformal-Predictions-Enhanced-Expert-guided-Meshing-with-Graph-Neural-Networks" class="headerlink" title="Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks"></a>Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07358">http://arxiv.org/abs/2308.07358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahnobari/autosurf">https://github.com/ahnobari/autosurf</a></li>
<li>paper_authors: Amin Heyrani Nobari, Justin Rey, Suhas Kodali, Matthew Jones, Faez Ahmed<br>for:This paper aims to develop a machine learning-based scheme for automatically generating high-quality meshes for computational fluid dynamics (CFD) simulations, with a focus on aircraft models.methods:The proposed method utilizes graph neural networks (GNN) and expert guidance to generate CFD meshes. A new 3D segmentation algorithm is introduced, which outperforms two state-of-the-art models, PointNet++ and PointMLP, for surface classification. The conformal predictions method is used to project predictions from 3D mesh segmentation models to CAD surfaces, providing marginal statistical guarantees and robust uncertainty quantification and handling.results:The proposed approach is demonstrated through a real-world case study, showing that the automatically generated mesh is comparable in quality to expert-generated meshes and enables the solver to converge and produce accurate results. Additionally, the approach is found to be 5 times faster than adaptive remeshing in the overall process of simulation. The code and data for this project are made publicly available at <a target="_blank" rel="noopener" href="https://github.com/ahnobari/AutoSurf">https://github.com/ahnobari/AutoSurf</a>.<details>
<summary>Abstract</summary>
Computational Fluid Dynamics (CFD) is widely used in different engineering fields, but accurate simulations are dependent upon proper meshing of the simulation domain. While highly refined meshes may ensure precision, they come with high computational costs. Similarly, adaptive remeshing techniques require multiple simulations and come at a great computational cost. This means that the meshing process is reliant upon expert knowledge and years of experience. Automating mesh generation can save significant time and effort and lead to a faster and more efficient design process. This paper presents a machine learning-based scheme that utilizes Graph Neural Networks (GNN) and expert guidance to automatically generate CFD meshes for aircraft models. In this work, we introduce a new 3D segmentation algorithm that outperforms two state-of-the-art models, PointNet++ and PointMLP, for surface classification. We also present a novel approach to project predictions from 3D mesh segmentation models to CAD surfaces using the conformal predictions method, which provides marginal statistical guarantees and robust uncertainty quantification and handling. We demonstrate that the addition of conformal predictions effectively enables the model to avoid under-refinement, hence failure, in CFD meshing even for weak and less accurate models. Finally, we demonstrate the efficacy of our approach through a real-world case study that demonstrates that our automatically generated mesh is comparable in quality to expert-generated meshes and enables the solver to converge and produce accurate results. Furthermore, we compare our approach to the alternative of adaptive remeshing in the same case study and find that our method is 5 times faster in the overall process of simulation. The code and data for this project are made publicly available at https://github.com/ahnobari/AutoSurf.
</details>
<details>
<summary>摘要</summary>
computational fluid dynamics (CFD) 广泛应用于不同的工程领域，但准确的 simulations 受到 mesh 的限制。高精度的 mesh 可以确保精度，但是来自计算成本的代价很高。 adaptive remeshing 技术也需要多次 simulations 和大量计算成本。这意味着 meshing 过程依赖于专家知识和多年的经验。自动生成 mesh 可以保存很多时间和努力，并且导致更快的设计过程。本文提出了一种基于 machine learning 的 scheme，使用 graph neural networks (GNN) 和专家指导生成 CFD mesh  для飞机模型。在这种工作中，我们提出了一种新的 3D 分割算法，其在 surface classification 方面超过了两个 state-of-the-art 模型：PointNet++ 和 PointMLP。我们还提出了一种将 predictions 从 3D mesh 分割模型项project 到 CAD 表面的新方法，使用 conformal predictions 方法，该方法提供了边缘统计保证和稳定的 uncertainty quantification 和处理。我们示示了添加 conformal predictions 可以使模型避免 under-refinement 和失败，即CFD  meshing中的负面刻。 finally，我们通过一个实际的案例研究证明了我们自动生成的 mesh 与专家生成的 mesh 相当，并且使得解除器能够 converges 并生成准确的结果。此外，我们与 adaptive remeshing 的相对比较发现，我们的方法在整个 simulations 过程中速度比 adaptive remeshing 5 倍。代码和数据可以在 https://github.com/ahnobari/AutoSurf 上公开获取。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Learning-of-Quantum-States-Prepared-With-Few-Non-Clifford-Gates-II-Single-Copy-Measurements"><a href="#Efficient-Learning-of-Quantum-States-Prepared-With-Few-Non-Clifford-Gates-II-Single-Copy-Measurements" class="headerlink" title="Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates II: Single-Copy Measurements"></a>Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates II: Single-Copy Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07175">http://arxiv.org/abs/2308.07175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabee Grewal, Vishnu Iyer, William Kretschmer, Daniel Liang</li>
<li>for: 学习 $n$-qubit 量子状态，输出由最多 $t$ 单位 qubit 非截归 gate 生成的 circuits，可以使用 $\mathsf{poly}(n,2^t,1&#x2F;\epsilon)$ 时间和样本来达到 trace distance $\epsilon$。</li>
<li>methods: 使用单复本测量来学习该类状态，而不需要双复本测量。</li>
<li>results: 实现了同样高效的学习算法，但使用单复本测量而不需要双复本测量。<details>
<summary>Abstract</summary>
Recent work has shown that $n$-qubit quantum states output by circuits with at most $t$ single-qubit non-Clifford gates can be learned to trace distance $\epsilon$ using $\mathsf{poly}(n,2^t,1/\epsilon)$ time and samples. All prior algorithms achieving this runtime use entangled measurements across two copies of the input state. In this work, we give a similarly efficient algorithm that learns the same class of states using only single-copy measurements.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PitchNet-A-Fully-Convolutional-Neural-Network-for-Pitch-Estimation"><a href="#PitchNet-A-Fully-Convolutional-Neural-Network-for-Pitch-Estimation" class="headerlink" title="PitchNet: A Fully Convolutional Neural Network for Pitch Estimation"></a>PitchNet: A Fully Convolutional Neural Network for Pitch Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07170">http://arxiv.org/abs/2308.07170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy Cochoy</li>
<li>for: 用于提高音乐和声音处理中的抽取音高精度</li>
<li>methods: 使用卷积神经网络和自相关函数优化抽取音高精度</li>
<li>results: 在各种数据集上（包括合唱、歌剧录音和时间压缩的元音）进行评估，达到了更高的抽取音高精度<details>
<summary>Abstract</summary>
In the domain of music and sound processing, pitch extraction plays a pivotal role. This research introduces "PitchNet", a convolutional neural network tailored for pitch extraction from the human singing voice, including acapella performances. Integrating autocorrelation with deep learning techniques, PitchNet aims to optimize the accuracy of pitch detection. Evaluation across datasets comprising synthetic sounds, opera recordings, and time-stretched vowels demonstrates its efficacy. This work paves the way for enhanced pitch extraction in both music and voice settings.
</details>
<details>
<summary>摘要</summary>
在音乐和声音处理领域中，抓取高度扮演着关键性的角色。这项研究介绍了“抓取网络”（PitchNet），一种针对人声 singing voice 的卷积神经网络，包括 acapella 表演。通过与深度学习技术结合自相关性，PitchNet 目标优化抓取精度。对于各种数据集，包括 sintetic 声音、歌剧录音和时间压缩的元音，评估表明 PitchNet 的可行性。这项工作将为音乐和声音设置中的抓取提供新的 возможности。
</details></li>
</ul>
<hr>
<h2 id="SPEGTI-Structured-Prediction-for-Efficient-Generative-Text-to-Image-Models"><a href="#SPEGTI-Structured-Prediction-for-Efficient-Generative-Text-to-Image-Models" class="headerlink" title="SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models"></a>SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10997">http://arxiv.org/abs/2308.10997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadeep Jayasumana, Daniel Glasner, Srikumar Ramalingam, Andreas Veit, Ayan Chakrabarti, Sanjiv Kumar</li>
<li>for: 提高文本图像生成模型的计算效率，以便在不输出质量下降的情况下提高图像生成速度。</li>
<li>methods: 使用MarkovRandomField（MRF）模型来编码图像各个位置的图像元素之间的兼容性，并使用这个MRF模型与之前提出的Muse模型结合使用，以便减少Muse预测步骤数量，从而提高图像生成速度。</li>
<li>results: 通过使用MRF模型，可以在不输出质量下降的情况下，提高文本图像生成模型的计算效率，并且可以在不需要多次预测的情况下，提高图像生成速度。<details>
<summary>Abstract</summary>
Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MRF inference as a differentiable neural-network layer. Our full model, SPEGTI, uses this proposed MRF model to speed up Muse by 1.5X with no loss in output image quality.
</details>
<details>
<summary>摘要</summary>
现代文本到图像生成模型可以生成高质量的图像，这些图像不仅具有高度的真实性，还具有与文本提示符的准确性。然而，这些高质量图像的生成需要大量的计算成本：大多数这些模型都是迭代的，需要多次运行推理。这种迭代过程是为了确保图像中的不同区域不仅与文本提示符吻合，而且也与其他区域吻合。在这种工作中，我们提出了一种轻量级的方法来实现图像中不同区域之间的吻合，使用Markov随机场（MRF）模型。这种方法可以与最近提出的Muse模型结合使用，并且可以在推理过程中减少Muse预测步骤的数量。MRF模型可以编码图像元素之间的空间位置的兼容性，从而使得推理过程中的计算成本得到了显著减少。我们的全模型SPEGTI使用这种提议的MRF模型，可以在推理过程中加速Muse的执行，而无需减少输出图像质量。
</details></li>
</ul>
<hr>
<h2 id="Pairing-interacting-protein-sequences-using-masked-language-modeling"><a href="#Pairing-interacting-protein-sequences-using-masked-language-modeling" class="headerlink" title="Pairing interacting protein sequences using masked language modeling"></a>Pairing interacting protein sequences using masked language modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07136">http://arxiv.org/abs/2308.07136</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bitbol-lab/diffpalm">https://github.com/bitbol-lab/diffpalm</a></li>
<li>paper_authors: Umberto Lupo, Damiano Sgarbossa, Anne-Florence Bitbol</li>
<li>for: The paper aims to predict which proteins interact together from their amino-acid sequences, which is an important task in protein structure prediction and function prediction.</li>
<li>methods: The paper develops a method called DiffPALM that leverages protein language models trained on multiple sequence alignments to pair interacting protein sequences. The method uses MSA Transformer and the EvoFormer module of AlphaFold to fill in masked amino acids in multiple sequence alignments and capture inter-chain coevolution.</li>
<li>results: The paper shows that DiffPALM outperforms existing coevolution-based pairing methods on difficult benchmarks of shallow multiple sequence alignments and achieves competitive performance with using orthology-based pairing. Additionally, DiffPALM improves the structure prediction of some eukaryotic protein complexes by AlphaFold-Multimer without significantly deteriorating any of those tested.Here is the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文目标是从蛋白质序列中预测哪些蛋白质相互作用，这是蛋白质结构预测和功能预测中非常重要的任务。</li>
<li>methods: 论文提出了一种名为DiffPALM的方法，它利用蛋白质语言模型在多个序列对上进行训练，以对相互作用的蛋白质序列进行对应。该方法使用MSA Transformer和AlphaFold中的EvoFormer模块来填充多个序列对中的遮盖氨基酸，并capture氨基酸之间的跨链共演化。</li>
<li>results: 论文表明，DiffPALM比现有的相互作用基于共演化的对应方法在困难的多个序列对上表现出色，并达到与使用同源蛋白质对应的竞争性表现。此外，DiffPALM还可以提高一些细胞蛋白质复合物的结构预测，无需进行较major fine-tuning。<details>
<summary>Abstract</summary>
Predicting which proteins interact together from amino-acid sequences is an important task. We develop a method to pair interacting protein sequences which leverages the power of protein language models trained on multiple sequence alignments, such as MSA Transformer and the EvoFormer module of AlphaFold. We formulate the problem of pairing interacting partners among the paralogs of two protein families in a differentiable way. We introduce a method called DiffPALM that solves it by exploiting the ability of MSA Transformer to fill in masked amino acids in multiple sequence alignments using the surrounding context. MSA Transformer encodes coevolution between functionally or structurally coupled amino acids. We show that it captures inter-chain coevolution, while it was trained on single-chain data, which means that it can be used out-of-distribution. Relying on MSA Transformer without fine-tuning, DiffPALM outperforms existing coevolution-based pairing methods on difficult benchmarks of shallow multiple sequence alignments extracted from ubiquitous prokaryotic protein datasets. It also outperforms an alternative method based on a state-of-the-art protein language model trained on single sequences. Paired alignments of interacting protein sequences are a crucial ingredient of supervised deep learning methods to predict the three-dimensional structure of protein complexes. DiffPALM substantially improves the structure prediction of some eukaryotic protein complexes by AlphaFold-Multimer, without significantly deteriorating any of those we tested. It also achieves competitive performance with using orthology-based pairing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>预测蛋白质之间的互作是一项重要任务。我们开发了一种方法来对蛋白质序列进行互作对应，利用蛋白质语言模型在多个序列对alignment中学习的力量，如MSA transformer和AlphaFold中的EvoFormer模块。我们将对蛋白质家族中的参数进行分配的问题进行形式化。我们提出了一种名为DiffPALM的方法，它利用MSA transformer填充多个序列对alignment中的masked amino酸的能力，以获得更好的互作对应。MSA transformer编码了功能或结构相关的氨基酸之间的共演化，我们表明它可以在单链数据上进行填充，并且在多个序列对alignment中提取深层次的数据时表现出色。与现有的共演化基于方法相比，DiffPALM在具有深度多个序列对alignment的困难benchmark上表现出色，并且在使用不需要微调的情况下，也能够与一种基于state-of-the-art蛋白质语言模型的方法相比。对于一些细菌蛋白质复合物的三维结构预测，DiffPALM提供了显著改善，而不是显著下降任何已测试的结构。它还可以与基于orthology的对应方法相比。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-is-All-a-Graph-Needs"><a href="#Natural-Language-is-All-a-Graph-Needs" class="headerlink" title="Natural Language is All a Graph Needs"></a>Natural Language is All a Graph Needs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07134">http://arxiv.org/abs/2308.07134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang</li>
<li>for: 本研究旨在探讨whether large language models (LLMs) can replace graph neural networks (GNNs) as the foundation model for graphs.</li>
<li>methods: 本研究提出了InstructGLM（Instruction-finetuned Graph Language Model），通过自然语言指令设计了高度扩展的提示，并使用自然语言描述图像的几何结构和节点特征。</li>
<li>results: 研究结果表明，InstructGLM在ogbn-arxiv、Cora和PubMed datasets上都超过了所有竞争GNN基elines，这证明了我们的方法的有效性，同时也释放了大语言模型作为图机器学习基础模型的潜在性。<details>
<summary>Abstract</summary>
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.
</details>
<details>
<summary>摘要</summary>
大型预训语言模型，如ChatGPT，的出现对人工智能多个研究领域产生了革命性的影响。基于Transformers的大型语言模型（LLMs）逐渐取代了CNNs和RNNs，统一了计算机视觉和自然语言处理的领域。相比于独立存在的数据，如图像、视频或文本，图表是一种包含丰富结构和关系信息的数据类型。同时，自然语言作为最有表达力的媒体，在描述复杂结构方面表现出色。然而，将图学问题 incorporated into the generative language modeling framework 的现有工作很有限。随着大语言模型的重要性不断增长，我们需要探索是否可以将LLMs作为图像学习的基础模型。在这篇论文中，我们提出了InstructGLM（基于自然语言指令的图语言模型），系统地设计了可扩展的自然语言指令，并使用自然语言来描述图形结构和节点特征。通过这种方式，我们使用大语言模型进行图像学习和推理，并达到了在ogbn-arxiv、Cora和PubMed数据集上的所有竞争GNN基elines的超越。这说明了我们的方法的有效性，并且推照到了大语言模型作为图像学习的基础模型。
</details></li>
</ul>
<hr>
<h2 id="Implementation-of-The-Future-of-Drug-Discovery-QuantumBased-Machine-Learning-Simulation-QMLS"><a href="#Implementation-of-The-Future-of-Drug-Discovery-QuantumBased-Machine-Learning-Simulation-QMLS" class="headerlink" title="Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS)"></a>Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08561">http://arxiv.org/abs/2308.08561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yew Kee Wong, Yifan Zhou, Yan Shing Liang, Haichuan Qiu, Yu Xi Wu, Bin He</li>
<li>For: 这篇论文主要目的是提出一种新的药物开发研究与发展（R&amp;D）阶段缩短方法，使其从原来的几年和十万美元降低到只需三到六个月和五万到八千美元。* Methods: 这篇论文使用的方法包括机器学习分子生成（MLMG）和量子 simulate（QS）。 MLMG 根据目标蛋白质的分子结构生成可能的吸引者，而 QS 根据反应和绑定效果从原始试剂中筛选出符合条件的分子。* Results: 这篇论文的结果是提出了一种基于机器学习和量子 simulate 的药物开发研究方法，可以在三到六个月和五万到八千美元的范围内缩短 R&amp;D 阶段，并且可以生成多达几十个前期临床试验准备的药物。<details>
<summary>Abstract</summary>
The Research & Development (R&D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction effectiveness and safety, creating a few dozen pre-clinical-trail-ready drugs. This paper is based on our first paper, where we pitched the concept of machine learning combined with quantum simulations. In this paper we will go over the detailed design and framework of QMLS, including MLMG, MLMV, and QS.
</details>
<details>
<summary>摘要</summary>
研发（R&D）阶段是药品开发的 longest 和最昂贵的阶段。为了革新这个过程，我们介绍了一新的概念——QMLS，它可以缩短整个R&D阶段的时间至3-6个月，并将成本降至50-80万美元。在hit生成阶段，机器学习分子生成（MLMG）根据目标蛋白质分子结构生成可能的hit，而量子 simulations（QS）则从首轮试验中筛选出符合反应和结合效果的分子。在Lead优化阶段，由MLMG和QS生成的结果分子进行比较，并生成几十个分子变化through machine learning分子变化（MLMV），而其他分子则只生成几个变化。最后，所有优化的分子都会经过多轮QS筛选，以确保反应效果和安全性。通过这种方式，我们可以在几个月内生成数十个前期临床药物。这篇文章是我们之前的第一篇论文中提出的概念的详细设计和框架，包括MLMG、MLMV和QS。
</details></li>
</ul>
<hr>
<h2 id="A-Time-aware-tensor-decomposition-for-tracking-evolving-patterns"><a href="#A-Time-aware-tensor-decomposition-for-tracking-evolving-patterns" class="headerlink" title="A Time-aware tensor decomposition for tracking evolving patterns"></a>A Time-aware tensor decomposition for tracking evolving patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07126">http://arxiv.org/abs/2308.07126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Chatzis, Max Pfeffer, Pedro Lind, Evrim Acar</li>
<li>for: 这篇论文主要旨在提出一种基于PARAFAC2的时间 regularization方法，用于从时间数据中提取慢慢发展的模式。</li>
<li>methods: 该方法使用时间 regularization来防止时间点的重新排序，并使用PARAFAC2进行tensor factorization来捕捉时间数据中的下降模式。</li>
<li>results: 经过广泛的实验表明，tPARAFAC2能够准确地捕捉时间数据中的下降模式，并在表现上超过PARAFAC2和 coupling matrix factorization with temporal smoothness regularization。<details>
<summary>Abstract</summary>
Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regularization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化字符串。<</SYS>>时间演化数据集经常可以被视为高阶张量，其中一个模式是时间模式。而张量分解技术已经成功地捕捉了高阶数据集中的下面纹理，但是忽略了时间方面，允许时间点的重新排序。在最近的研究中，人们尝试将时间正则化添加到时间模式中，以解决这个问题。然而，现有的方法仍然不允许下面纹理在时间上发生变化（例如，脑中的空间变化，话题中的上下文变化）。在这篇论文中，我们提出了时间PARAFAC2（tPARAFAC2）：一种基于PARAFAC2的张量分解方法，带有时间正则化来提取时间演化的慢慢发展模式。通过对synthetic数据进行了广泛的实验，我们示出了tPARAFAC2可以准确地捕捉到时间演化中的下面纹理，并且比PARAFAC2和联合矩阵因子化 WITH 时间平滑正则化更好。
</details></li>
</ul>
<hr>
<h2 id="Active-Bird2Vec-Towards-End-to-End-Bird-Sound-Monitoring-with-Transformers"><a href="#Active-Bird2Vec-Towards-End-to-End-Bird-Sound-Monitoring-with-Transformers" class="headerlink" title="Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with Transformers"></a>Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07121">http://arxiv.org/abs/2308.07121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Rauch, Raphael Schwinger, Moritz Wirth, Bernhard Sick, Sven Tomforde, Christoph Scholz</li>
<li>for: 鸟叫声监测的终端学习shift，结合自动学习(SSL)和深度活动学习(DAL)。</li>
<li>methods: 利用 transformer 模型，直接处理原始音频数据，不需要传统的spectrogram转换。</li>
<li>results: 通过 SSL 生成高质量鸟叫声表示，可能加速环境变化评估和风力 farm 决策过程。同时，通过 DAL 利用鸟类 vocals 的多样性，减少人工标注数据的依赖，提高生物听音研究的可比性和可重现性。<details>
<summary>Abstract</summary>
We propose a shift towards end-to-end learning in bird sound monitoring by combining self-supervised (SSL) and deep active learning (DAL). Leveraging transformer models, we aim to bypass traditional spectrogram conversions, enabling direct raw audio processing. ActiveBird2Vec is set to generate high-quality bird sound representations through SSL, potentially accelerating the assessment of environmental changes and decision-making processes for wind farms. Additionally, we seek to utilize the wide variety of bird vocalizations through DAL, reducing the reliance on extensively labeled datasets by human experts. We plan to curate a comprehensive set of tasks through Huggingface Datasets, enhancing future comparability and reproducibility of bioacoustic research. A comparative analysis between various transformer models will be conducted to evaluate their proficiency in bird sound recognition tasks. We aim to accelerate the progression of avian bioacoustic research and contribute to more effective conservation strategies.
</details>
<details>
<summary>摘要</summary>
我们提议将学习方法转向终端学习（End-to-End Learning），将自我超级vised学习（Self-Supervised Learning）和深度活动学习（Deep Active Learning）相结合。通过使用转换器模型，我们希望直接处理原始音频数据，并不需要传统的spectrogram转换。活动鸟2Vec可以通过SSL生成高质量鸟叫表示，可能加速环境变化评估和风轮农场决策过程。此外，我们计划利用鸟类 vocals 的多样性，减少人工标注数据的依赖性。我们将使用Huggingface Datasets框架，实现未来比较性和可重复性的生物声学研究。我们计划对不同的转换器模型进行比较分析，以评估它们在鸟叫识别任务中的效果。我们希望通过加速鸟类生物声学研究，为更有效的保护策略做出贡献。
</details></li>
</ul>
<hr>
<h2 id="Neural-radiance-fields-in-the-industrial-and-robotics-domain-applications-research-opportunities-and-use-cases"><a href="#Neural-radiance-fields-in-the-industrial-and-robotics-domain-applications-research-opportunities-and-use-cases" class="headerlink" title="Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases"></a>Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07118">http://arxiv.org/abs/2308.07118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maftej/iisnerf">https://github.com/maftej/iisnerf</a></li>
<li>paper_authors: Eugen Šlapak, Enric Pardo, Matúš Dopiriak, Taras Maksymyuk, Juraj Gazda</li>
<li>for: 本研究旨在探讨基于提供训练图像的神经辐射场（NeRF）在各种工业领域的应用潜力，并提供未来研究方向。</li>
<li>methods: 本研究使用NeRF来实现3D场景表示，并在视频压缩和3D动作估计等领域进行证明。</li>
<li>results: 研究显示，使用NeRF进行视频压缩可以达到48%和74%的压缩率提升，而在3D动作估计中，使用D-NeRF实现的 disparity map PSNR值达到23 dB，SSIM值为0.97。<details>
<summary>Abstract</summary>
The proliferation of technologies, such as extended reality (XR), has increased the demand for high-quality three-dimensional (3D) graphical representations. Industrial 3D applications encompass computer-aided design (CAD), finite element analysis (FEA), scanning, and robotics. However, current methods employed for industrial 3D representations suffer from high implementation costs and reliance on manual human input for accurate 3D modeling. To address these challenges, neural radiance fields (NeRFs) have emerged as a promising approach for learning 3D scene representations based on provided training 2D images. Despite a growing interest in NeRFs, their potential applications in various industrial subdomains are still unexplored. In this paper, we deliver a comprehensive examination of NeRF industrial applications while also providing direction for future research endeavors. We also present a series of proof-of-concept experiments that demonstrate the potential of NeRFs in the industrial domain. These experiments include NeRF-based video compression techniques and using NeRFs for 3D motion estimation in the context of collision avoidance. In the video compression experiment, our results show compression savings up to 48\% and 74\% for resolutions of 1920x1080 and 300x168, respectively. The motion estimation experiment used a 3D animation of a robotic arm to train Dynamic-NeRF (D-NeRF) and achieved an average peak signal-to-noise ratio (PSNR) of disparity map with the value of 23 dB and an structural similarity index measure (SSIM) 0.97.
</details>
<details>
<summary>摘要</summary>
“技术的普及，如扩展现实（XR），已经提高了高品质三维图形的需求。工业三维应用包括计算机支持设计（CAD）、finite element分析（FEA）、扫描和机器人。然而，现有的工业三维表示方法受到高实施成本和人工输入的假设，以获得准确的三维模型。为了解决这些挑战，神经辐射场（NeRF）已经出现为了学习基于提供训练图像的三维场景表示。尽管NeRF在不同领域产生了增长的兴趣，但它们在不同的工业子领域的潜在应用还未得到了足够的探索。在这篇论文中，我们提供了对NeRF工业应用的全面评估，并为未来研究提供方向。我们还进行了一系列的证明性实验，以示NeRF在工业领域的潜在应用。这些实验包括基于NeRF的视频压缩技术和使用NeRF进行3D运动估计，以避免碰撞。在视频压缩实验中，我们得到了1920x1080和300x168的分辨率下的压缩率为48%和74%。在运动估计实验中，我们使用了一个3D动画的机械臂进行训练，并获得了23 dB的平均峰值信号噪声比（PSNR）和0.97的结构相似度指标（SSIM）。”
</details></li>
</ul>
<hr>
<h2 id="iSTFTNet2-Faster-and-More-Lightweight-iSTFT-Based-Neural-Vocoder-Using-1D-2D-CNN"><a href="#iSTFTNet2-Faster-and-More-Lightweight-iSTFT-Based-Neural-Vocoder-Using-1D-2D-CNN" class="headerlink" title="iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN"></a>iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07117">http://arxiv.org/abs/2308.07117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, Shogo Seki</li>
<li>for: 高速、轻量级、高精度的语音合成</li>
<li>methods: 使用快速、轻量级的1D CNN作为基础网络，并将一些神经过程替换为iSTFT，以提高速度和精度。</li>
<li>results: iSTFTNet2比iSTFTNet更快、更轻量级，且音质相对保持不变。<details>
<summary>Abstract</summary>
The inverse short-time Fourier transform network (iSTFTNet) has garnered attention owing to its fast, lightweight, and high-fidelity speech synthesis. It obtains these characteristics using a fast and lightweight 1D CNN as the backbone and replacing some neural processes with iSTFT. Owing to the difficulty of a 1D CNN to model high-dimensional spectrograms, the frequency dimension is reduced via temporal upsampling. However, this strategy compromises the potential to enhance the speed. Therefore, we propose iSTFTNet2, an improved variant of iSTFTNet with a 1D-2D CNN that employs 1D and 2D CNNs to model temporal and spectrogram structures, respectively. We designed a 2D CNN that performs frequency upsampling after conversion in a few-frequency space. This design facilitates the modeling of high-dimensional spectrograms without compromising the speed. The results demonstrated that iSTFTNet2 made iSTFTNet faster and more lightweight with comparable speech quality. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/.
</details>
<details>
<summary>摘要</summary>
它的快速、轻量级和高精度语音合成使得倒时傅立卷网络（iSTFTNet）受到了关注。它使用了快速和轻量级的1D CNN作为核心，并将一些神经过程替换为iSTFT。由于1D CNNDifficulty modeling高维spectrograms，因此在频率维度上做了时间upsampling。然而，这种策略会减少速度的潜在提高。因此，我们提出了iSTFTNet2，它是iSTFTNet的改进版本，使用1D-2D CNN来模型时间和spectrogram结构。我们设计了一个2D CNN，它在几个频率空间中进行频率upsampling。这种设计可以模型高维spectrograms，而不会减少速度。结果表明，iSTFTNet2使得iSTFTNet更快速和轻量级，并且与相同的语音质量相对。音频样本可以在https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/上获取。
</details></li>
</ul>
<hr>
<h2 id="Ada-QPacknet-–-adaptive-pruning-with-bit-width-reduction-as-an-efficient-continual-learning-method-without-forgetting"><a href="#Ada-QPacknet-–-adaptive-pruning-with-bit-width-reduction-as-an-efficient-continual-learning-method-without-forgetting" class="headerlink" title="Ada-QPacknet – adaptive pruning with bit width reduction as an efficient continual learning method without forgetting"></a>Ada-QPacknet – adaptive pruning with bit width reduction as an efficient continual learning method without forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07939">http://arxiv.org/abs/2308.07939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcin Pietroń, Dominik Żurek, Kamil Faber, Roberto Corizzo</li>
<li>for: 这篇论文主要针对各种各样的动态和复杂环境下的Continual Learning（CL）问题。</li>
<li>methods: 该论文提出了一种基于架构的CL方法，称为Ada-QPacknet，它通过减少模型大小来实现CL。该方法使用有效的线性和非线性归一化方法来减少模型的权重的位数据类型。</li>
<li>results: 根据实验结果，hybrid 8和4位归一化的混合归一化方法可以达到类似于浮点子网络的准确率，而且在任务和类增量场景中比大多数CL策略表现更好。<details>
<summary>Abstract</summary>
Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combinations and compared with most popular algorithms. Results show that proposed approach outperforms most of the CL strategies in task and class incremental scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Age-Stratified-Differences-in-Morphological-Connectivity-Patterns-in-ASD-An-sMRI-and-Machine-Learning-Approach"><a href="#Age-Stratified-Differences-in-Morphological-Connectivity-Patterns-in-ASD-An-sMRI-and-Machine-Learning-Approach" class="headerlink" title="Age-Stratified Differences in Morphological Connectivity Patterns in ASD: An sMRI and Machine Learning Approach"></a>Age-Stratified Differences in Morphological Connectivity Patterns in ASD: An sMRI and Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07356">http://arxiv.org/abs/2308.07356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokul Manoj, Sandeep Singh Sengar, Jac Fredo Agastinose Ronickom</li>
<li>for: 本研究的目的是用 morphological features (MF) 和 morphological connectivity features (MCF) 来分类 autism spectrum disorder (ASD)，并比较不同年龄组的分类效果。</li>
<li>methods: 研究使用了 two publicly available databases, ABIDE-I 和 ABIDE-II, 获取了 structural magnetic resonance imaging (sMRI) 数据，并对数据进行了标准化处理。然后，将数据分割成 148 个不同区域，根据 Destrieux  Atlases，并从每个区域提取了面积、厚度、体积和平均弯曲信息。使用了统计学 t-test (p&lt;0.05) 来选择特征，然后使用 random forest (RF) 分类器进行训练。</li>
<li>results: 研究结果表明，6-11 岁的年龄组的表现最高，然后是 6-18 岁和 11-18 岁的年龄组。总的来说，MCF 与 RF 在 6-11 岁的年龄组中表现最好，其中的准确率、 F1 分数、回归率和精度分别为 75.8%、83.1%、86% 和 80.4%。结论：本研究因此表明，使用 morphological connectivity 和年龄相关的诊断模型可以有效地分类 ASD。<details>
<summary>Abstract</summary>
Purpose: Age biases have been identified as an essential factor in the diagnosis of ASD. The objective of this study was to compare the effect of different age groups in classifying ASD using morphological features (MF) and morphological connectivity features (MCF). Methods: The structural magnetic resonance imaging (sMRI) data for the study was obtained from the two publicly available databases, ABIDE-I and ABIDE-II. We considered three age groups, 6 to 11, 11 to 18, and 6 to 18, for our analysis. The sMRI data was pre-processed using a standard pipeline and was then parcellated into 148 different regions according to the Destrieux atlas. The area, thickness, volume, and mean curvature information was then extracted for each region which was used to create a total of 592 MF and 10,878 MCF for each subject. Significant features were identified using a statistical t-test (p<0.05) which was then used to train a random forest (RF) classifier. Results: The results of our study suggested that the performance of the 6 to 11 age group was the highest, followed by the 6 to 18 and 11 to 18 ages in both MF and MCF. Overall, the MCF with RF in the 6 to 11 age group performed better in the classification than the other groups and produced an accuracy, F1 score, recall, and precision of 75.8%, 83.1%, 86%, and 80.4%, respectively. Conclusion: Our study thus demonstrates that morphological connectivity and age-related diagnostic model could be an effective approach to discriminating ASD.
</details>
<details>
<summary>摘要</summary>
目的：识别自适应发育障碍（ASD）的年龄因素有所重要。本研究的目标是比较不同年龄组的ASD诊断使用形态特征（MF）和形态连接特征（MCF）的效果。方法：我们使用ABIDE-I和ABIDE-II两个公共数据库获取了structural magnetic resonance imaging（sMRI）数据。我们分为三个年龄组：6-11岁、11-18岁和6-18岁进行分析。sMRI数据经过标准化处理后，使用Destrieux Atlas将数据分割成148个区域。然后提取每个区域的面积、厚度、体积和平均曲率信息，共计592个MF和10878个MCF。使用统计t检测test（p<0.05）标识特征，然后使用随机森林（RF）分类器进行训练。结果：我们的研究发现，6-11岁年龄组的表现最高，其次是6-18岁和11-18岁年龄组。总的来说，MCF与RF在6-11岁年龄组中表现较好，其准确率、F1分数、报告率和准确率分别为75.8%、83.1%、86%和80.4%。结论：因此，我们的研究表明，形态连接和年龄相关的诊断模型可以有效地识别ASD。
</details></li>
</ul>
<hr>
<h2 id="InsTag-Instruction-Tagging-for-Analyzing-Supervised-Fine-tuning-of-Large-Language-Models"><a href="#InsTag-Instruction-Tagging-for-Analyzing-Supervised-Fine-tuning-of-Large-Language-Models" class="headerlink" title="#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models"></a>#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07074">http://arxiv.org/abs/2308.07074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ofa-sys/instag">https://github.com/ofa-sys/instag</a></li>
<li>paper_authors: Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, Jingren Zhou</li>
<li>for: 这个论文的目的是提高基础模型的命令遵从能力，并通过量化分析定义命令多样性和复杂性。</li>
<li>methods: 本文使用了一种名为InsTag的开放集 Fine-grained tagger，通过Semantics和Intention来标签SFT数据集中的样本，并定义命令多样性和复杂性。</li>
<li>results: 根据MT-Bench的评价，使用InsTag选择的6K多样性和复杂性的样本进行微调，可以使基础模型的命令遵从能力得到显著提高。<details>
<summary>Abstract</summary>
Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and complexity. We open-source InsTag in https://github.com/OFA-Sys/InsTag.
</details>
<details>
<summary>摘要</summary>
基于监督精度（SFT）的基础语言模型获得了指令遵循能力，但是关键因素如多样性和复杂性的定义尚未得到准确的量化分析。在这项工作中，我们提出了InsTag，一个开放集成细词标注器，用于在SFT数据集中标注样本基于含义和目标，并定义指令多样性和复杂性的标签。我们获得了6.6K个标签来描述用户查询的全面性。然后我们分析了一些常用的开源SFT数据集，发现模型能力随着数据集的多样性和复杂性增加而增长。基于这一观察，我们提出了基于InsTag的数据选择器，用于从开源数据集中选择6K个多样性和复杂性最高的样本，并在InsTag-选择的数据上精度 fine-tune 模型。得到的模型TagLM在MT-Bench上评估得到了较大的SFT数据集的较好的性能，证明了查询多样性和复杂性的重要性。我们将InsTag开源在https://github.com/OFA-Sys/InsTag。
</details></li>
</ul>
<hr>
<h2 id="Machine-Unlearning-Solutions-and-Challenges"><a href="#Machine-Unlearning-Solutions-and-Challenges" class="headerlink" title="Machine Unlearning: Solutions and Challenges"></a>Machine Unlearning: Solutions and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07061">http://arxiv.org/abs/2308.07061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Xu, Zihan Wu, Cong Wang, Xiaohua Jia</li>
<li>for: 本研究旨在Addressing privacy and security concerns in machine learning by selectively removing specific training data points’ influence on trained models.</li>
<li>methods: 本研究 categorizes existing machine unlearning research into two types: exact unlearning和approximate unlearning, and reviews state-of-the-art solutions with their advantages and limitations.</li>
<li>results: 本研究提出了未来研究方向，并鼓励研究人员通过Addressing open problems to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning.<details>
<summary>Abstract</summary>
Machine learning models may inadvertently memorize sensitive, unauthorized, or malicious data, posing risks of privacy violations, security breaches, and performance deterioration. To address these issues, machine unlearning has emerged as a critical technique to selectively remove specific training data points' influence on trained models. This paper provides a comprehensive taxonomy and analysis of machine unlearning research. We categorize existing research into exact unlearning that algorithmically removes data influence entirely and approximate unlearning that efficiently minimizes influence through limited parameter updates. By reviewing the state-of-the-art solutions, we critically discuss their advantages and limitations. Furthermore, we propose future directions to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning. This paper provides researchers with a roadmap of open problems, encouraging impactful contributions to address real-world needs for selective data removal.
</details>
<details>
<summary>摘要</summary>
Translation notes:* " Machine learning models" is translated as "机器学习模型" (jī zhī xué xí mó delè)* "inadvertently" is translated as "无意" (wú yì)* "sensitive, unauthorized, or malicious" is translated as "敏感、未授权或黑客" (mǐn gǎn, wèi shèng qián, hēi kè)* "privacy violations" is translated as "隐私侵犯" (yǐn wèi qiāng fāng)* "security breaches" is translated as "安全泄露" (ān què lù)* "performance deterioration" is translated as "性能下降" (xìng néng xià gōng)* "machine unlearning" is translated as "机器忘记" (jī zhī wàng jī)* "selectively remove" is translated as "选择性地移除" (选择性地移除)* "specific training data points" is translated as "特定训练数据点" (特定训练数据点)* "influence" is translated as "影响" (yìng xiǎng)* "entirely" is translated as "完全" (quán zhèng)* "efficiently" is translated as "高效" (gāo yù)* "minimizes" is translated as "最小化" (zuì xiǎo hóu)* "limited parameter updates" is translated as "有限参数更新" (yǒu xiàn paramètres jīn gòu)* "state-of-the-art solutions" is translated as "现状的解决方案" (xiàn zhèng de jiě jīng fāng àn)* "advantages and limitations" is translated as "优点和缺点" (yòu dòng hé qiòng diǎn)* "future directions" is translated as "未来方向" (wèi lāi fāng dìng)* "trustworthy and adaptive machine learning" is translated as "可靠性和适应性机器学习" (kě zuò xìng yì jī zhī xué xí)* "open problems" is translated as "开放问题" (kāi fàng wèn tí)* "impactful contributions" is translated as "有影响的贡献" (yǒu yìng xiǎng de gōng jìn)
</details></li>
</ul>
<hr>
<h2 id="Diagnosis-of-Scalp-Disorders-using-Machine-Learning-and-Deep-Learning-Approach-–-A-Review"><a href="#Diagnosis-of-Scalp-Disorders-using-Machine-Learning-and-Deep-Learning-Approach-–-A-Review" class="headerlink" title="Diagnosis of Scalp Disorders using Machine Learning and Deep Learning Approach – A Review"></a>Diagnosis of Scalp Disorders using Machine Learning and Deep Learning Approach – A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07052">http://arxiv.org/abs/2308.07052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishabh Tiwari, Jatin Moolchandani, Shamla Mantri</li>
<li>for: 这个研究是为了提高皮肤病诊断的准确率和效率。</li>
<li>methods: 这个研究使用了深度学习模型，包括CNN和FCN，以及一个APP来识别皮肤和scalp疾病。</li>
<li>results: 研究结果表明，使用深度学习模型可以准确地识别皮肤和scalp疾病，其中最高准确率达97.41%-99.09%。<details>
<summary>Abstract</summary>
The morbidity of scalp diseases is minuscule compared to other diseases, but the impact on the patient's life is enormous. It is common for people to experience scalp problems that include Dandruff, Psoriasis, Tinea-Capitis, Alopecia and Atopic-Dermatitis. In accordance with WHO research, approximately 70% of adults have problems with their scalp. It has been demonstrated in descriptive research that hair quality is impaired by impaired scalp, but these impacts are reversible with early diagnosis and treatment. Deep Learning advances have demonstrated the effectiveness of CNN paired with FCN in diagnosing scalp and skin disorders. In one proposed Deep-Learning-based scalp inspection and diagnosis system, an imaging microscope and a trained model are combined with an app that classifies scalp disorders accurately with an average precision of 97.41%- 99.09%. Another research dealt with classifying the Psoriasis using the CNN with an accuracy of 82.9%. As part of another study, an ML based algorithm was also employed. It accurately classified the healthy scalp and alopecia areata with 91.4% and 88.9% accuracy with SVM and KNN algorithms. Using deep learning models to diagnose scalp related diseases has improved due to advancements i computation capabilities and computer vision, but there remains a wide horizon for further improvements.
</details>
<details>
<summary>摘要</summary>
scalp病的感染率相对其他疾病较低，但对病人生活的影响却很大。人们常常会经历头皮问题，包括痤疮、 Psoriasis、脚抄螯、脱发和过敏性皮肤炎。根据Who的研究，大约70%的成年人都有头皮问题。研究表明，损害的头皮质量可以通过早期诊断和治疗来改善，但这些影响可以逆转。深度学习技术的发展使得 CNN 和 FCN 的结合可以准确地诊断头皮和皮肤疾病。一种提议的深度学习基于的头皮检查和诊断系统使用了一个升级的探针和训练模型，并与一个APP结合，可以准确地分类头皮疾病，其精度为97.41%-99.09%。另一项研究用到 CNN 分类痤疮，精度为82.9%。另外一项研究使用 ML 算法，可以准确地分类健康的头皮和脱发症，精度分别为91.4%和88.9%。使用深度学习模型诊断头皮相关疾病，因计算能力和计算视觉的进步而得到改善，但还有很大的发展空间。
</details></li>
</ul>
<hr>
<h2 id="Fourier-neural-operator-for-learning-solutions-to-macroscopic-traffic-flow-models-Application-to-the-forward-and-inverse-problems"><a href="#Fourier-neural-operator-for-learning-solutions-to-macroscopic-traffic-flow-models-Application-to-the-forward-and-inverse-problems" class="headerlink" title="Fourier neural operator for learning solutions to macroscopic traffic flow models: Application to the forward and inverse problems"></a>Fourier neural operator for learning solutions to macroscopic traffic flow models: Application to the forward and inverse problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07051">http://arxiv.org/abs/2308.07051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bilal Thonnam Thodi, Sai Venkata Ramana Ambadipudi, Saif Eddin Jabari</li>
<li>for: 本研究使用深度学习方法解决非线性散射方程的问题，具体是用神经网络扩散算法来学习宏观交通流模型中的全部交通状态。</li>
<li>methods: 本研究使用的是一种名为 физи学 Informed Fourier Neural Operator（π-FNO）的神经网络算法，该算法在训练过程中添加了物理损失函数来补做冲击预测，以提高冲击预测的准确性。</li>
<li>results: 实验结果表明，使用本研究的神经网络算法可以高度准确地预测环路交通网络和城市信号灯控制下的density dynamics，并且可以适应不同的车辆队列分布和多个交通信号周期。此外，研究还发现，使用physics regularizer可以帮助学习长期交通状态的预测，特别是在periodic boundary data的情况下。<details>
<summary>Abstract</summary>
Deep learning methods are emerging as popular computational tools for solving forward and inverse problems in traffic flow. In this paper, we study a neural operator framework for learning solutions to nonlinear hyperbolic partial differential equations with applications in macroscopic traffic flow models. In this framework, an operator is trained to map heterogeneous and sparse traffic input data to the complete macroscopic traffic state in a supervised learning setting. We chose a physics-informed Fourier neural operator ($\pi$-FNO) as the operator, where an additional physics loss based on a discrete conservation law regularizes the problem during training to improve the shock predictions. We also propose to use training data generated from random piecewise constant input data to systematically capture the shock and rarefied solutions. From experiments using the LWR traffic flow model, we found superior accuracy in predicting the density dynamics of a ring-road network and urban signalized road. We also found that the operator can be trained using simple traffic density dynamics, e.g., consisting of $2-3$ vehicle queues and $1-2$ traffic signal cycles, and it can predict density dynamics for heterogeneous vehicle queue distributions and multiple traffic signal cycles $(\geq 2)$ with an acceptable error. The extrapolation error grew sub-linearly with input complexity for a proper choice of the model architecture and training data. Adding a physics regularizer aided in learning long-term traffic density dynamics, especially for problems with periodic boundary data.
</details>
<details>
<summary>摘要</summary>
深度学习方法在交通流动中应用得更加广泛，用于解决前向和反向问题。在这篇论文中，我们研究了一种神经运算框架，用于学习解决非线性偏微分方程的解。在这个框架中，一个运算被训练来将各种不同和稀缺的交通输入数据映射到完整的宏观交通状态中。我们选择了一种physics-informed Fourier neural operator（$\pi$-FNO）作为运算，其中添加了物理损失，以便在训练过程中进行辐射预测。我们还提议使用来自随机划分输入数据的训练数据，以系统地捕捉冲击和稀缺解。在使用LWR交通流模型的实验中，我们发现了在密度动力学中的高精度预测，特别是在环路网络和城市控制措施下。我们还发现，运算可以通过简单的交通密度动力学，例如由2-3辆汽车队列和1-2个交通信号循环组成的，来预测密度动力学。并且可以在多个交通信号循环和不同车辆队列分布下进行预测，并且误差在输入复杂性增加时呈线性增长。添加物理正则化有助于学习长期交通密度动力学，特别是在 Periodic boundary data 下。
</details></li>
</ul>
<hr>
<h2 id="UIPC-MF-User-Item-Prototype-Connection-Matrix-Factorization-for-Explainable-Collaborative-Filtering"><a href="#UIPC-MF-User-Item-Prototype-Connection-Matrix-Factorization-for-Explainable-Collaborative-Filtering" class="headerlink" title="UIPC-MF: User-Item Prototype Connection Matrix Factorization for Explainable Collaborative Filtering"></a>UIPC-MF: User-Item Prototype Connection Matrix Factorization for Explainable Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07048">http://arxiv.org/abs/2308.07048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Pan, Von-Wun Soo</li>
<li>for: 提供可解释的用户行为推荐（Recommending items to potentially interested users with explainable user behavior）</li>
<li>methods: 使用prototype-based matrix factorization方法（UIPC-MF），用户和Item分别与一组prototype相关联，以提高推荐的可解释性。</li>
<li>results: 在三个 dataset 上比基eline方法高效，并且提供更好的透明度（Hit Ratio和Normalized Discounted Cumulative Gain）。<details>
<summary>Abstract</summary>
Recommending items to potentially interested users has been an important commercial task that faces two main challenges: accuracy and explainability. While most collaborative filtering models rely on statistical computations on a large scale of interaction data between users and items and can achieve high performance, they often lack clear explanatory power. We propose UIPC-MF, a prototype-based matrix factorization method for explainable collaborative filtering recommendations. In UIPC-MF, both users and items are associated with sets of prototypes, capturing general collaborative attributes. To enhance explainability, UIPC-MF learns connection weights that reflect the associative relations between user and item prototypes for recommendations. UIPC-MF outperforms other prototype-based baseline methods in terms of Hit Ratio and Normalized Discounted Cumulative Gain on three datasets, while also providing better transparency.
</details>
<details>
<summary>摘要</summary>
推荐预测已成为商业中的一个重要任务，面临两大挑战：准确率和可解释性。大多数共同推荐模型基于大规模的用户和项目互动数据进行统计计算，可以达到高性能，但往往缺乏明确的解释力。我们提出了UIPC-MF，一种基于Matrix Factorization的原型基于方法，用于可解释的共同推荐。在UIPC-MF中，用户和项目都关联有一组概念prototype，捕捉用户和项目之间的共同特征。为了增强可解释性，UIPC-MF学习用户和项目概念之间的关联关系，以便为推荐提供更好的透明性。UIPC-MF在三个数据集上相比其他原型基本方法而言，有较高的 Hit Ratio 和 Normalized Discounted Cumulative Gain，同时也提供更好的透明性。
</details></li>
</ul>
<hr>
<h2 id="No-Regularization-is-Needed-An-Efficient-and-Effective-Model-for-Incomplete-Label-Distribution-Learning"><a href="#No-Regularization-is-Needed-An-Efficient-and-Effective-Model-for-Incomplete-Label-Distribution-Learning" class="headerlink" title="No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning"></a>No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07047">http://arxiv.org/abs/2308.07047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Songcan Chen</li>
<li>for: This paper focuses on addressing the problem of Incomplete Label Distribution Learning (InLDL), where the labels are incomplete or unobserved for some samples.</li>
<li>methods: The authors propose a new method that uses the prior of label distribution to solve the InLDL problem without any explicit regularization. They define a weighted empirical risk and derive upper bounds to reveal the implicit regularization role of weighting.</li>
<li>results: The proposed method has four advantages: 1) it is model selection free, 2) it has a closed form solution and is easy to implement, 3) it has linear computational complexity, and 4) it is competitive with state-of-the-art methods even without any explicit regularization.<details>
<summary>Abstract</summary>
Label Distribution Learning (LDL) assigns soft labels, a.k.a. degrees, to a sample. In reality, it is always laborious to obtain complete degrees, giving birth to the Incomplete LDL (InLDL). However, InLDL often suffers from performance degeneration. To remedy it, existing methods need one or more explicit regularizations, leading to burdensome parameter tuning and extra computation. We argue that label distribution itself may provide useful prior, when used appropriately, the InLDL problem can be solved without any explicit regularization. In this paper, we offer a rational alternative to use such a prior. Our intuition is that large degrees are likely to get more concern, the small ones are easily overlooked, whereas the missing degrees are completely neglected in InLDL. To learn an accurate label distribution, it is crucial not to ignore the small observed degrees but to give them properly large weights, while gradually increasing the weights of the missing degrees. To this end, we first define a weighted empirical risk and derive upper bounds between the expected risk and the weighted empirical risk, which reveals in principle that weighting plays an implicit regularization role. Then, by using the prior of degrees, we design a weighted scheme and verify its effectiveness. To sum up, our model has four advantages, it is 1) model selection free, as no explicit regularization is imposed; 2) with closed form solution (sub-problem) and easy-to-implement (a few lines of codes); 3) with linear computational complexity in the number of samples, thus scalable to large datasets; 4) competitive with state-of-the-arts even without any explicit regularization.
</details>
<details>
<summary>摘要</summary>
Label Distribution Learning (LDL)  assigns 软标签，即学习度，到一个样本上。在实际应用中，通常难以获得完整的学习度，从而产生了不完整的LDL（InLDL）问题。然而，InLDL经常会导致性能下降。为了解决这个问题，现有方法通常需要一或多个显式正则化，从而增加参数调整的复杂性和计算量。我们认为标签分布本身可以提供有用的先验知识，当用于适当的情况时，InLDL问题可以解决无需显式正则化。在这篇论文中，我们提出了一种有理的方法，使用这种先验知识来解决InLDL问题。我们的假设是，大的学习度更有可能得到更多的注意力，小的学习度容易被忽略，而缺失的学习度完全被InLDL忽略。为了学习准确的标签分布，非常重要不要忽略小 observed 的学习度，而是给它们分配正确的大小，同时逐渐增加缺失的学习度的权重。我们首先定义一个权重 empirical risk，并 deriv 上下文中的预期风险和权重 empirical risk 之间的Upper bound，这表明了权重在本质上扮演了隐式正则化的角色。然后，我们使用学习度的先验知识来设计一种权重方案，并证明其效果。总之，我们的模型具有以下四个优点：1) 无需显式正则化，因为不需要在数据上添加任何正则化项；2) 具有关闭式解决方案和易于实现（只需一些代码）；3) 计算复杂度为数据集的线性时间，因此可扩展到大型数据集；4) 可与当前的状态艺技相比，甚至没有任何显式正则化。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Flow-Networks"><a href="#Bayesian-Flow-Networks" class="headerlink" title="Bayesian Flow Networks"></a>Bayesian Flow Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07037">http://arxiv.org/abs/2308.07037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stefanradev93/BayesFlow">https://github.com/stefanradev93/BayesFlow</a></li>
<li>paper_authors: Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, Faustino Gomez</li>
<li>for: 本研究旨在提出一种新的生成模型—权当流网络（BFN），它通过bayesian推理在噪声数据样本的指导下修改参数集中的独立分布，然后将这些分布作为输入传递给神经网络，从而生成第二个相互关联的分布。</li>
<li>methods: 本研究使用了权当流网络（BFN），它们的生成过程类似于反射模型的逆过程，但是更加简单，不需要前向过程。研究者还 derive了离散和连续时间的损失函数，以及批量生成过程。</li>
<li>results: 实验表明，BFNs可以在 dynamical binarized MNIST 和 CIFAR-10 图像模型任务上 achieve 竞争力的 log-likelihood，并且在 text8 字符级语言模型任务上超越了所有已知的杂分 diffusion 模型。<details>
<summary>Abstract</summary>
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no restrictions on the network architecture. In our experiments BFNs achieve competitive log-likelihoods for image modelling on dynamically binarized MNIST and CIFAR-10, and outperform all known discrete diffusion models on the text8 character-level language modelling task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="S3IM-Stochastic-Structural-SIMilarity-and-Its-Unreasonable-Effectiveness-for-Neural-Fields"><a href="#S3IM-Stochastic-Structural-SIMilarity-and-Its-Unreasonable-Effectiveness-for-Neural-Fields" class="headerlink" title="S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields"></a>S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07032">http://arxiv.org/abs/2308.07032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/madaoer/s3im_nerf">https://github.com/madaoer/s3im_nerf</a></li>
<li>paper_authors: Zeke Xie, Xindi Yang, Yujie Yang, Qi Sun, Yixiang Jiang, Haoran Wang, Yunfeng Cai, Mingming Sun</li>
<li>For: The paper aims to improve the quality of Neural Radiance Field (NeRF) and related neural field methods for novel-view image synthesis and surface reconstruction tasks.* Methods: The paper introduces a nonlocal multiplex training paradigm for NeRF and related neural field methods, using a novel Stochastic Structural SIMilarity (S3IM) loss that processes multiple data points as a whole set instead of processing multiple inputs independently.* Results: The paper shows that the proposed S3IM loss leads to significant improvements in quality metrics for NeRF and neural surface representation, particularly for difficult tasks such as novel view synthesis and surface reconstruction. The improvements are robust even with sparse inputs, corrupted images, and dynamic scenes.<details>
<summary>Abstract</summary>
Recently, Neural Radiance Field (NeRF) has shown great success in rendering novel-view images of a given scene by learning an implicit representation with only posed RGB images. NeRF and relevant neural field methods (e.g., neural surface representation) typically optimize a point-wise loss and make point-wise predictions, where one data point corresponds to one pixel. Unfortunately, this line of research failed to use the collective supervision of distant pixels, although it is known that pixels in an image or scene can provide rich structural information. To the best of our knowledge, we are the first to design a nonlocal multiplex training paradigm for NeRF and relevant neural field methods via a novel Stochastic Structural SIMilarity (S3IM) loss that processes multiple data points as a whole set instead of process multiple inputs independently. Our extensive experiments demonstrate the unreasonable effectiveness of S3IM in improving NeRF and neural surface representation for nearly free. The improvements of quality metrics can be particularly significant for those relatively difficult tasks: e.g., the test MSE loss unexpectedly drops by more than 90% for TensoRF and DVGO over eight novel view synthesis tasks; a 198% F-score gain and a 64% Chamfer $L_{1}$ distance reduction for NeuS over eight surface reconstruction tasks. Moreover, S3IM is consistently robust even with sparse inputs, corrupted images, and dynamic scenes.
</details>
<details>
<summary>摘要</summary>
最近，神经辐射场（NeRF）已经取得了大成功，通过学习含义表示的唯一RGB图像来生成新视图图像。NeRF和相关的神经场方法（例如神经表面表示）通常通过点级损失来优化和预测点级数据，而这些数据点与每个像素相对应。然而，这一线索的研究忽略了远程像素的共同监督，尽管知道图像或场景中的像素可以提供丰富的结构信息。据我们所知，我们是第一个设计非本地多重训练方法via一种新的随机结构相似性（S3IM）损失，该损失处理多个数据点作为整体而不是独立处理多个输入。我们的广泛实验表明S3IM在改进NeRF和神经表面表示方法方面具有不可思议的效果，并且这些改进的质量指标可以特别显著，例如在八个新视图合成任务中，测试MSE损失意外下降了More than 90%  дляTensoRF和DVGO; NeuS在八个表面重建任务中获得了198%的F-score提升和64%的L1距离减少。此外，S3IM具有对于稀缺输入、损坏图像和动态场景的一致性。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Physics-Informed-Neural-Network-for-the-Forward-and-Inverse-Simulation-of-Engineered-Nano-particles-Mobility-in-a-Contaminated-Aquifer"><a href="#Bayesian-Physics-Informed-Neural-Network-for-the-Forward-and-Inverse-Simulation-of-Engineered-Nano-particles-Mobility-in-a-Contaminated-Aquifer" class="headerlink" title="Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer"></a>Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07352">http://arxiv.org/abs/2308.07352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Nilabh, Fidel Grandia</li>
<li>for: 这项研究的目的是为了开发一种能够在地下水域中有效地预测粒子的移动和停留行为，以便开发一种有效的地下水恢复策略。</li>
<li>methods: 这项研究使用了一种bayesian physics-informed neural network（B-PINN）框架，通过对模拟粒子在aquifer中的移动进行前向模型，并通过对模型输出进行逆向模型，来量化粒子的移动和停留行为。</li>
<li>results: 研究表明，B-PINN框架可以准确地预测粒子的移动和停留行为，并且可以量化这些行为的不确定性。此外，研究还发现了一些关键参数，可以用于控制粒子的移动和停留。这些结果表明，B-PINN框架可以提供有用的预测情况，以便开发有效的地下水恢复策略。<details>
<summary>Abstract</summary>
Globally, there are many polluted groundwater sites that need an active remediation plan for the restoration of local ecosystem and environment. Engineered nanoparticles (ENPs) have proven to be an effective reactive agent for the in-situ degradation of pollutants in groundwater. While the performance of these ENPs has been highly promising on the laboratory scale, their application in real field case conditions is still limited. The complex transport and retention mechanisms of ENPs hinder the development of an efficient remediation strategy. Therefore, a predictive tool to comprehend the transport and retention behavior of ENPs is highly required. The existing tools in the literature are dominated with numerical simulators, which have limited flexibility and accuracy in the presence of sparse datasets and the aquifer heterogeneity. This work uses a Bayesian Physics-Informed Neural Network (B-PINN) framework to model the nano-particles mobility within an aquifer. The result from the forward model demonstrates the effective capability of B-PINN in accurately predicting the ENPs mobility and quantifying the uncertainty. The inverse model output is then used to predict the governing parameters for the ENPs mobility in a small-scale aquifer. The research demonstrates the capability of the tool to provide predictive insights for developing an efficient groundwater remediation strategy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="IOB-Integrating-Optimization-Transfer-and-Behavior-Transfer-for-Multi-Policy-Reuse"><a href="#IOB-Integrating-Optimization-Transfer-and-Behavior-Transfer-for-Multi-Policy-Reuse" class="headerlink" title="IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse"></a>IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07351">http://arxiv.org/abs/2308.07351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Li, Hao Li, Jin Zhang, Zhen Wang, Peng Liu, Chongjie Zhang</li>
<li>for: 本研究旨在解决选择适当的源策略以促进目标策略学习的挑战，提出了一种新的转移学习RL方法。</li>
<li>methods: 该方法利用actor-critic框架中的Q函数引导策略选择，选择源策略可以提供最大一步改进。另外，该方法还结合了优化转移和行为转移（IOB），通过规范学习的策略来模仿指导策略，并将其与行为策略相结合。</li>
<li>results: 该方法在标准任务中超过了状态艺术RL基线，并在连续学习场景中提高了最终性和知识传递性。此外，该方法的优化转移技术保证了目标策略学习的提高。<details>
<summary>Abstract</summary>
Humans have the ability to reuse previously learned policies to solve new tasks quickly, and reinforcement learning (RL) agents can do the same by transferring knowledge from source policies to a related target task. Transfer RL methods can reshape the policy optimization objective (optimization transfer) or influence the behavior policy (behavior transfer) using source policies. However, selecting the appropriate source policy with limited samples to guide target policy learning has been a challenge. Previous methods introduce additional components, such as hierarchical policies or estimations of source policies' value functions, which can lead to non-stationary policy optimization or heavy sampling costs, diminishing transfer effectiveness. To address this challenge, we propose a novel transfer RL method that selects the source policy without training extra components. Our method utilizes the Q function in the actor-critic framework to guide policy selection, choosing the source policy with the largest one-step improvement over the current target policy. We integrate optimization transfer and behavior transfer (IOB) by regularizing the learned policy to mimic the guidance policy and combining them as the behavior policy. This integration significantly enhances transfer effectiveness, surpasses state-of-the-art transfer RL baselines in benchmark tasks, and improves final performance and knowledge transferability in continual learning scenarios. Additionally, we show that our optimization transfer technique is guaranteed to improve target policy learning.
</details>
<details>
<summary>摘要</summary>
人类有能力快速解决新任务使用已经学习过的策略，而强化学习（RL）代理也可以通过将来源策略中的知识传递到相关的目标任务中来实现此目的。传输RL方法可以修改策略优化目标（优化传递）或影响行为策略（行为传递）使用源策略。然而，选择适当的源策略可以受有限样本数的限制，从而影响目标策略的学习。先前的方法通过引入层次政策或估计源策略的价值函数等附加组件，可能导致非站点策略优化或重大的样本成本，减弱传输效果。为解决这个挑战，我们提出了一种新的传输RL方法，不需要训练附加组件。我们利用actor-critic框架中的Q函数来导引策选择，选择目标策略中最大化一步改进的源策略。我们将优化传递和行为传递（IOB）相结合，通过规范学习的策略来模仿指导策略，并将其与之相结合。这种结合显著提高了传输效果，超越了基准测试RL方法，并在持续学习场景中提高了最终性和知识传递性。此外，我们证明我们的优化传递技术是 garantizado 提高目标策略学习。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Neural-PDE-Solvers-using-Quantization-Aware-Training"><a href="#Efficient-Neural-PDE-Solvers-using-Quantization-Aware-Training" class="headerlink" title="Efficient Neural PDE-Solvers using Quantization Aware Training"></a>Efficient Neural PDE-Solvers using Quantization Aware Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07350">http://arxiv.org/abs/2308.07350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Winfried van den Dool, Tijmen Blankevoort, Max Welling, Yuki M. Asano</li>
<li>for: 解决Partial Differential Equations（PDE）中的计算成本问题，以减少计算成本并维持性能。</li>
<li>methods: 使用现有的量化方法来减少计算成本，包括量化网络参数和活动。</li>
<li>results: 对四个标准PDE数据集和三种网络架构进行了训练，并证明了量化意识训练可以降低计算成本，同时维持性能。最终，我们实际示出，只有通过量化来实现Pareto优化计算成本与性能的平衡。<details>
<summary>Abstract</summary>
In the past years, the application of neural networks as an alternative to classical numerical methods to solve Partial Differential Equations has emerged as a potential paradigm shift in this century-old mathematical field. However, in terms of practical applicability, computational cost remains a substantial bottleneck. Classical approaches try to mitigate this challenge by limiting the spatial resolution on which the PDEs are defined. For neural PDE solvers, we can do better: Here, we investigate the potential of state-of-the-art quantization methods on reducing computational costs. We show that quantizing the network weights and activations can successfully lower the computational cost of inference while maintaining performance. Our results on four standard PDE datasets and three network architectures show that quantization-aware training works across settings and three orders of FLOPs magnitudes. Finally, we empirically demonstrate that Pareto-optimality of computational cost vs performance is almost always achieved only by incorporating quantization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Optimize-LSM-trees-Towards-A-Reinforcement-Learning-based-Key-Value-Store-for-Dynamic-Workloads"><a href="#Learning-to-Optimize-LSM-trees-Towards-A-Reinforcement-Learning-based-Key-Value-Store-for-Dynamic-Workloads" class="headerlink" title="Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads"></a>Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07013">http://arxiv.org/abs/2308.07013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dingheng Mo, Fanchao Chen, Siqiang Luo, Caihua Shan</li>
<li>for: 提高静态工作负荷下的系统性能优化。</li>
<li>methods: 使用Reinforcement Learning（RL）导向LSM树变换，并提出新的LSM树设计——FLSM树，以便在不同的压缩策略之间进行高效的过渡。</li>
<li>results: 在多种工作负荷下，RusKey可以达到4倍的终端性能优化，比RocksDB系统更强。<details>
<summary>Abstract</summary>
LSM-trees are widely adopted as the storage backend of key-value stores. However, optimizing the system performance under dynamic workloads has not been sufficiently studied or evaluated in previous work. To fill the gap, we present RusKey, a key-value store with the following new features: (1) RusKey is a first attempt to orchestrate LSM-tree structures online to enable robust performance under the context of dynamic workloads; (2) RusKey is the first study to use Reinforcement Learning (RL) to guide LSM-tree transformations; (3) RusKey includes a new LSM-tree design, named FLSM-tree, for an efficient transition between different compaction policies -- the bottleneck of dynamic key-value stores. We justify the superiority of the new design with theoretical analysis; (4) RusKey requires no prior workload knowledge for system adjustment, in contrast to state-of-the-art techniques. Experiments show that RusKey exhibits strong performance robustness in diverse workloads, achieving up to 4x better end-to-end performance than the RocksDB system under various settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>RusKey是首次在线上预约LSM-树结构，以确保在动态负荷下的稳定性表现。2. RusKey是首次使用强化学习（RL）引导LSM-树变换的研究。3. RusKey包含一种新的LSM-树设计，称为FLSM-树，可以有效地在不同的压缩策略之间进行过渡。4. RusKey不需要先知系统负荷特性，与现有技术不同。我们通过理论分析证明了新设计的优越性。实验结果表明，RusKey在多种工作负荷下表现出了强大的性能稳定性，与RocksDB系统在不同设置下实现了最高的终端性能，达到4倍之多。</details></li>
</ol>
<hr>
<h2 id="Greedy-online-change-point-detection"><a href="#Greedy-online-change-point-detection" class="headerlink" title="Greedy online change point detection"></a>Greedy online change point detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07012">http://arxiv.org/abs/2308.07012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jou-Hui Ho, Felipe Tobar</li>
<li>for: 提高 online Change Point Detection（CPD）方法的精度和准确性。</li>
<li>methods: 使用 Greedy Online Change Point Detection（GOCPD）方法，通过最大化数据来自两个独立模型（temporal）的概率，以找到时间序列中的变化点。</li>
<li>results: 在单个变化点的情况下，使用ternary搜索，逻辑复杂度为对数。在synthetic数据和实际世界 univariate和multivariate设置中，证明GOCPD的有效性。<details>
<summary>Abstract</summary>
Standard online change point detection (CPD) methods tend to have large false discovery rates as their detections are sensitive to outliers. To overcome this drawback, we propose Greedy Online Change Point Detection (GOCPD), a computationally appealing method which finds change points by maximizing the probability of the data coming from the (temporal) concatenation of two independent models. We show that, for time series with a single change point, this objective is unimodal and thus CPD can be accelerated via ternary search with logarithmic complexity. We demonstrate the effectiveness of GOCPD on synthetic data and validate our findings on real-world univariate and multivariate settings.
</details>
<details>
<summary>摘要</summary>
标准在线变点检测（CPD）方法通常会有较大的假阳性率，因为它们对异常值敏感。为了解决这个缺点，我们提议了Greedy Online Change Point Detection（GOCPD），一种计算效率高的方法，它通过最大化数据来自（时间）拼接两个独立模型的概率来检测变点。我们显示，对于具有单个变点的时间序列，这个目标函数是单峰性的，因此可以通过ternary search进行加速，其复杂度为对数型。我们在 synthetic 数据上证明了 GOCPD 的效果，并在实际世界的单variate 和多variate 设置中验证了我们的结论。
</details></li>
</ul>
<hr>
<h2 id="Aggregating-Intrinsic-Information-to-Enhance-BCI-Performance-through-Federated-Learning"><a href="#Aggregating-Intrinsic-Information-to-Enhance-BCI-Performance-through-Federated-Learning" class="headerlink" title="Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning"></a>Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11636">http://arxiv.org/abs/2308.11636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Liu, Yuanyuan Chen, Anran Li, Yi Ding, Han Yu, Cuntai Guan</li>
<li>for: 这个研究旨在解决脑computer接口（BCI）建立高性能深度学习模型所面临的长期挑战，即脑电图（EEG）数据的共享。</li>
<li>methods: 本研究提出了一个层次化个性化联合学习（FLEEG）框架，以解决EEG数据之间的不同格式问题。每个客户端都被指派特定的数据集，并训练层次化个性化模型，以管理不同数据格式并促进信息交换。服务器则处理训练过程，将来自所有数据集的知识融合，以提高总表现。</li>
<li>results: 研究将在脑意念（MI）类别任务上进行了评估，使用了9个由不同设备收集的EEG数据集。结果显示，提出的框架可以提高类别性能达16.7%，尤其是 для较小的数据集。可视化结果也显示出该框架可以让本地模型对任务相关区域进行稳定的注意力集中，从而提高表现。<details>
<summary>Abstract</summary>
Insufficient data is a long-standing challenge for Brain-Computer Interface (BCI) to build a high-performance deep learning model. Though numerous research groups and institutes collect a multitude of EEG datasets for the same BCI task, sharing EEG data from multiple sites is still challenging due to the heterogeneity of devices. The significance of this challenge cannot be overstated, given the critical role of data diversity in fostering model robustness. However, existing works rarely discuss this issue, predominantly centering their attention on model training within a single dataset, often in the context of inter-subject or inter-session settings. In this work, we propose a hierarchical personalized Federated Learning EEG decoding (FLEEG) framework to surmount this challenge. This innovative framework heralds a new learning paradigm for BCI, enabling datasets with disparate data formats to collaborate in the model training process. Each client is assigned a specific dataset and trains a hierarchical personalized model to manage diverse data formats and facilitate information exchange. Meanwhile, the server coordinates the training procedure to harness knowledge gleaned from all datasets, thus elevating overall performance. The framework has been evaluated in Motor Imagery (MI) classification with nine EEG datasets collected by different devices but implementing the same MI task. Results demonstrate that the proposed frame can boost classification performance up to 16.7% by enabling knowledge sharing between multiple datasets, especially for smaller datasets. Visualization results also indicate that the proposed framework can empower the local models to put a stable focus on task-related areas, yielding better performance. To the best of our knowledge, this is the first end-to-end solution to address this important challenge.
</details>
<details>
<summary>摘要</summary>
BCIs 长期面临缺乏数据的挑战，建立高性能的深度学习模型。虽然多个研究组织和机构收集了大量的 EEG 数据，但是在不同设备上分享 EEG 数据仍然具有挑战性，这是因为设备之间存在差异。这种挑战的重要性无法被低估，因为数据多样性对模型的稳定性具有关键作用。然而，现有的研究很少讨论这个问题，通常在单一数据集上进行模型训练，通常在 между Subject 或 Session 上进行。在这种情况下，我们提出了一种层次个性化 Federated Learning EEG 解码（FLEEG）框架，以超越这个挑战。这种创新的框架标识了一种新的学习模式 для BCIs，使得不同数据格式的数据可以在模型训练过程中合作。每个客户端都被分配了特定的数据集，并训练了一个层次个性化模型来管理多样的数据格式并促进信息交换。同时，服务器协调训练过程，以利用所有数据集中所获得的知识，从而提高总性能。我们在 Motor Imagery （MI） 分类任务中使用九个 EEG 数据集，每个数据集都是由不同的设备收集的，但是实现了同一个 MI 任务。结果表明，我们的框架可以提高分类性能达到 16.7%，尤其是对小数据集的提高。视觉结果还表明，我们的框架可以让本地模型固定焦点于任务相关的区域，从而提高表现。到目前为止，这是我们知道的首个综合解决这个重要挑战的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Deep-convolutional-neural-networks-for-cyclic-sensor-data"><a href="#Deep-convolutional-neural-networks-for-cyclic-sensor-data" class="headerlink" title="Deep convolutional neural networks for cyclic sensor data"></a>Deep convolutional neural networks for cyclic sensor data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06987">http://arxiv.org/abs/2308.06987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Payman Goodarzi, Yannick Robin, Andreas Schütze, Tizian Schneider</li>
<li>for: 本研究旨在探讨基于感知器的维保维护，并使用深度学习技术对一个液压系统测试平台数据进行应用。</li>
<li>methods: 本研究使用了三个模型：基线模型使用传统方法、单个CNN模型使用早期感知融合、以及两个CNN模型（2L-CNN）使用晚期感知融合。</li>
<li>results: 基线模型使用晚期感知融合实现了低于1%的测试错误率，而CNN模型由于感知器之间的多样性而遇到挑战，导致错误率高达20.5%。在进一步调查这个问题时，我们发现了每个感知器都需要独立进行特征提取的问题。此外，我们还评估了2L-CNN模型，并发现它可以将最佳和最差的感知器组合起来，以减少错误率33%。这种研究认真地面对了多感知器系统中的复杂性。<details>
<summary>Abstract</summary>
Predictive maintenance plays a critical role in ensuring the uninterrupted operation of industrial systems and mitigating the potential risks associated with system failures. This study focuses on sensor-based condition monitoring and explores the application of deep learning techniques using a hydraulic system testbed dataset. Our investigation involves comparing the performance of three models: a baseline model employing conventional methods, a single CNN model with early sensor fusion, and a two-lane CNN model (2L-CNN) with late sensor fusion. The baseline model achieves an impressive test error rate of 1% by employing late sensor fusion, where feature extraction is performed individually for each sensor. However, the CNN model encounters challenges due to the diverse sensor characteristics, resulting in an error rate of 20.5%. To further investigate this issue, we conduct separate training for each sensor and observe variations in accuracy. Additionally, we evaluate the performance of the 2L-CNN model, which demonstrates significant improvement by reducing the error rate by 33% when considering the combination of the least and most optimal sensors. This study underscores the importance of effectively addressing the complexities posed by multi-sensor systems in sensor-based condition monitoring.
</details>
<details>
<summary>摘要</summary>
预测维护在工业系统不间断运行和降低系统故障的风险方面扮演着关键角色。本研究利用液压系统测试平台数据进行了深度学习技术的应用，并对三种模型进行比较：基线模型使用传统方法、单个CNN模型使用早期感知融合，以及两个CNN模型（2L-CNN）使用晚期感知融合。基线模型通过使用晚期感知融合实现了测试错误率为1%，但CNN模型由于感知器的多样性而遇到问题，导致错误率为20.5%。为了更深入了解这个问题，我们对每个感知器进行了分别的训练，并观察到了减少精度的变化。此外，我们还评估了2L-CNN模型的性能，其能够在考虑最佳和最差感知器的组合下降低错误率33%。这个研究重申了对多感知器系统的预测维护存在多样性和复杂性的挑战。
</details></li>
</ul>
<hr>
<h2 id="pNNCLR-Stochastic-Pseudo-Neighborhoods-for-Contrastive-Learning-based-Unsupervised-Representation-Learning-Problems"><a href="#pNNCLR-Stochastic-Pseudo-Neighborhoods-for-Contrastive-Learning-based-Unsupervised-Representation-Learning-Problems" class="headerlink" title="pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems"></a>pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06983">http://arxiv.org/abs/2308.06983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Momojit Biswas, Himanshu Buckchash, Dilip K. Prasad</li>
<li>for: 本研究的目的是提高 nearest neighbor 基于自助学习（SSL）的图像识别问题中的 semantic variation。</li>
<li>methods: 本研究使用 nearest neighbor  sampling 方法，并引入 pseudo nearest neighbors（pNN）来控制支持集质量。此外，通过随机抽样和平滑重量更新方法来稳定 nearest neighbor 基于学习的不确定性。</li>
<li>results: 对多个公共图像识别和医学图像识别数据集进行评估，本研究的提案方法可以与基准 nearest neighbor 方法相比，并与其他先前提出的 SSL 方法相当。<details>
<summary>Abstract</summary>
Nearest neighbor (NN) sampling provides more semantic variations than pre-defined transformations for self-supervised learning (SSL) based image recognition problems. However, its performance is restricted by the quality of the support set, which holds positive samples for the contrastive loss. In this work, we show that the quality of the support set plays a crucial role in any nearest neighbor based method for SSL. We then provide a refined baseline (pNNCLR) to the nearest neighbor based SSL approach (NNCLR). To this end, we introduce pseudo nearest neighbors (pNN) to control the quality of the support set, wherein, rather than sampling the nearest neighbors, we sample in the vicinity of hard nearest neighbors by varying the magnitude of the resultant vector and employing a stochastic sampling strategy to improve the performance. Additionally, to stabilize the effects of uncertainty in NN-based learning, we employ a smooth-weight-update approach for training the proposed network. Evaluation of the proposed method on multiple public image recognition and medical image recognition datasets shows that it performs up to 8 percent better than the baseline nearest neighbor method, and is comparable to other previously proposed SSL methods.
</details>
<details>
<summary>摘要</summary>
近邻采样（NN）提供更多语义变化，对自助学习（SSL）基于图像识别问题的性能有较好的影响。然而，其性能受支持集质量的限制。在这种情况下，我们表明支持集质量对任何近邻基于SSL方法的性能具有关键作用。我们然后提供一种精度的基线（pNNCLR），用于改进近邻基于SSL方法（NNCLR）。为此，我们引入 pseudo 近邻（pNN），以控制支持集质量。具体来说，而不是直接采样最近邻，我们采样邻近硬邻邻的附近，通过变化结果向量的大小和使用随机采样策略来提高性能。此外，为了稳定NN基于学习中的uncertainty的效果，我们使用了平滑Weight更新方法进行网络训练。多个公共图像识别和医疗图像识别数据集上的评估表明，我们提出的方法与基eline最近邻方法相比，性能提高达8%，与其他之前提出的SSL方法相当。
</details></li>
</ul>
<hr>
<h2 id="Routing-Recovery-for-UAV-Networks-with-Deliberate-Attacks-A-Reinforcement-Learning-based-Approach"><a href="#Routing-Recovery-for-UAV-Networks-with-Deliberate-Attacks-A-Reinforcement-Learning-based-Approach" class="headerlink" title="Routing Recovery for UAV Networks with Deliberate Attacks: A Reinforcement Learning based Approach"></a>Routing Recovery for UAV Networks with Deliberate Attacks: A Reinforcement Learning based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06973">http://arxiv.org/abs/2308.06973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijie He, Ziye Jia, Chao Dong, Wei Wang, Yilu Cao, Yang Yang, Qihui Wu</li>
<li>for: 本研究强调路由计划和恢复方法，以适应无人机网络受到攻击的情况。</li>
<li>methods: 该研究提出了一种基于节点重要性的攻击模型，并实现了节点重要性排名机制。此外，基于强化学习算法的智能路由方法也被提出，以恢复路由路径在无人机网络受到攻击时。</li>
<li>results: 数据示，提出的方法比其他相关方法更为有效。<details>
<summary>Abstract</summary>
The unmanned aerial vehicle (UAV) network is popular these years due to its various applications. In the UAV network, routing is significantly affected by the distributed network topology, leading to the issue that UAVs are vulnerable to deliberate damage. Hence, this paper focuses on the routing plan and recovery for UAV networks with attacks. In detail, a deliberate attack model based on the importance of nodes is designed to represent enemy attacks. Then, a node importance ranking mechanism is presented, considering the degree of nodes and link importance. However, it is intractable to handle the routing problem by traditional methods for UAV networks, since link connections change with the UAV availability. Hence, an intelligent algorithm based on reinforcement learning is proposed to recover the routing path when UAVs are attacked. Simulations are conducted and numerical results verify the proposed mechanism performs better than other referred methods.
</details>
<details>
<summary>摘要</summary>
“无人航空器（UAV）网络在这些年变得非常流行，它在各种应用方面表现出了优异的表现。然而，UAV网络中的路由却受到分布式网络架构的影响，导致UAV易受到意外攻击。因此，本文关注UAV网络中的路由计划和恢复，以适应攻击。具体来说，我们设计了一种基于节点重要性的攻击模型，并提出了一种考虑节点和链接重要性的节点重要性排名机制。然而，由于UAV网络中的链接连接随着UAV可用性的变化，传统的路由方法无法处理UAV网络的路由问题。因此，我们提出了基于强化学习算法的智能路由恢复方法，以便在UAV被攻击时恢复路由路径。我们对此进行了仿真和数值分析，结果表明我们的提案在恢复路由路径方面表现出了更好的性能。”Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="AutoAssign-Automatic-Shared-Embedding-Assignment-in-Streaming-Recommendation"><a href="#AutoAssign-Automatic-Shared-Embedding-Assignment-in-Streaming-Recommendation" class="headerlink" title="AutoAssign+: Automatic Shared Embedding Assignment in Streaming Recommendation"></a>AutoAssign+: Automatic Shared Embedding Assignment in Streaming Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06965">http://arxiv.org/abs/2308.06965</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/AutoAssign-Plus">https://github.com/Applied-Machine-Learning-Lab/AutoAssign-Plus</a></li>
<li>paper_authors: Ziru Liu, Kecheng Chen, Fengyi Song, Bo Chen, Xiangyu Zhao, Huifeng Guo, Ruiming Tang</li>
<li>For: The paper aims to address the challenges of assigning initial ID embeddings randomly in streaming recommender systems, which can result in suboptimal prediction performance for items or users with limited interactive data, and lead to unnecessary memory consumption.* Methods: The paper proposes a reinforcement learning-driven framework called AutoAssign+, which utilizes an Identity Agent to represent low-frequency IDs field-wise with a small set of shared embeddings, and dynamically determine which ID features should be retained or eliminated in the embedding table.* Results: The paper demonstrates that AutoAssign+ is capable of significantly enhancing recommendation performance by mitigating the cold-start problem, and yields a reduction in memory usage of approximately 20-30%, verifying its practical effectiveness and efficiency for streaming recommender systems.Here’s the simplified Chinese text for the three key points:* For: 这篇论文目标是解决流动推荐系统中 randomly 分配初始 ID 嵌入的问题，这可能导致有限交互数据的用户或物品预测性能下降，并且需要不断扩展嵌入表，从而导致过度的内存消耗。* Methods: 论文提出一种基于强化学习的框架，即 AutoAssign+，该框架利用一个 Identity Agent 作为actor网络，该网络在两个角色下运行：一是用一小组共享嵌入来代表低频 ID，以提高嵌入初始化；二是在嵌入表中决定应保留或消除哪些 ID 特征。批评网络对策优化。* Results: 实验结果表明，AutoAssign+ 能够显著提高推荐性能，减轻冷启 проблеme，并且减少内存使用量约 20-30%，证明其在流动推荐系统中的实用性和效率。<details>
<summary>Abstract</summary>
In the domain of streaming recommender systems, conventional methods for addressing new user IDs or item IDs typically involve assigning initial ID embeddings randomly. However, this practice results in two practical challenges: (i) Items or users with limited interactive data may yield suboptimal prediction performance. (ii) Embedding new IDs or low-frequency IDs necessitates consistently expanding the embedding table, leading to unnecessary memory consumption. In light of these concerns, we introduce a reinforcement learning-driven framework, namely AutoAssign+, that facilitates Automatic Shared Embedding Assignment Plus. To be specific, AutoAssign+ utilizes an Identity Agent as an actor network, which plays a dual role: (i) Representing low-frequency IDs field-wise with a small set of shared embeddings to enhance the embedding initialization, and (ii) Dynamically determining which ID features should be retained or eliminated in the embedding table. The policy of the agent is optimized with the guidance of a critic network. To evaluate the effectiveness of our approach, we perform extensive experiments on three commonly used benchmark datasets. Our experiment results demonstrate that AutoAssign+ is capable of significantly enhancing recommendation performance by mitigating the cold-start problem. Furthermore, our framework yields a reduction in memory usage of approximately 20-30%, verifying its practical effectiveness and efficiency for streaming recommender systems.
</details>
<details>
<summary>摘要</summary>
在流动推荐系统领域，传统方法通常是随机分配初始ID embedding。然而，这种做法会导致两个实际挑戦：（i）有限交互数据的物品或用户可能会得到低效预测性能。（ii）添加新ID或低频ID需要不断扩大 embedding 表，从而导致不必要的内存浪费。为了解决这些问题，我们介绍了一个基于强化学习的框架，即AutoAssign+，它实现了自动共享 embedding 分配加 plus。具体来说，AutoAssign+ 使用一个 Identity Agent 作为actor网络，该网络在两个角色中进行表达：（i）在 embeddings 中场景化低频 ID 使用一小组共享 embedding 进行增强初始化。（ii）在 embedding 表中决定保留或 eliminating ID 特征。Identity Agent 的策略通过批评网络的指导优化。为了评估我们的方法的有效性，我们在三个常用的标准数据集上进行了广泛的实验。实验结果表明，AutoAssign+ 能够有效地缓解冷启点问题，并且它的内存使用率比传统方法减少了约20-30%，证明了它在流动推荐系统中的实际效果和效率。
</details></li>
</ul>
<hr>
<h2 id="Graph-Structural-Residuals-A-Learning-Approach-to-Diagnosis"><a href="#Graph-Structural-Residuals-A-Learning-Approach-to-Diagnosis" class="headerlink" title="Graph Structural Residuals: A Learning Approach to Diagnosis"></a>Graph Structural Residuals: A Learning Approach to Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06961">http://arxiv.org/abs/2308.06961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Lukas Augustin, Oliver Niggemann</li>
<li>for: This paper proposes a novel framework for model-based diagnosis that combines concepts of model-based diagnosis with deep graph structure learning, aiming to facilitate a seamless integration of graph structure learning with model-based diagnosis.</li>
<li>methods: The proposed framework uses two distinct graph adjacency matrices to represent the system’s underlying structure and provide dynamic observations. Additionally, the paper introduces two versions of a self-supervised graph structure learning model architecture.</li>
<li>results: The authors demonstrate the potential of their data-driven diagnostic method through experiments on a system of coupled oscillators.<details>
<summary>Abstract</summary>
Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
</details>
<details>
<summary>摘要</summary>
传统的模型基于诊断方法是通过构建明确的系统模型来进行，这可能是一项劳动密集且需要专家知识的过程。在这篇论文中，我们提出了一种新的框架，它将模型基于诊断与深度图结构学习结合起来。这种数据驱动的方法利用数据来学习系统的下面结构，并提供动态观察结果，表示为两个不同的图邻接矩阵。我们的工作使得图结构学习与模型基于诊断的集成变得自然和简单，我们的主要贡献包括：1. 重新定义系统表示、观察和缺陷的构造2. 提出两种自动学习图结构模型建立方法3. 通过对振荡器系统的实验，证明我们的数据驱动诊断方法的潜力。
</details></li>
</ul>
<hr>
<h2 id="Search-to-Fine-tune-Pre-trained-Graph-Neural-Networks-for-Graph-level-Tasks"><a href="#Search-to-Fine-tune-Pre-trained-Graph-Neural-Networks-for-Graph-level-Tasks" class="headerlink" title="Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks"></a>Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06960">http://arxiv.org/abs/2308.06960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Wang, Shimin Di, Lei Chen, Xiaofang Zhou<br>for: 这paper是为了提出一种更好的微调策略来改进预训练的graph neural network (GNN)的性能，以便在下游任务上提高模型性能。methods: 这paper使用了针对大规模未标注图数据进行预训练，并通过限制数据量的微调来适应目标下游任务。具体来说，它们提出了一种名为S2PGNN的搜索式微调策略，可以在各种下游任务上实现更好的性能。results: 这paper的实验结果表明，S2PGNN可以在10种著名的预训练GNN上实现性能提升，并且在预训练GNN的内部和外部比较其他微调策略的情况下都有更好的性能。codes可以在\url{<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/code_icde2024-A9CB/%7D%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/code_icde2024-A9CB/}上获取。</a><details>
<summary>Abstract</summary>
Recently, graph neural networks (GNNs) have shown its unprecedented success in many graph-related tasks. However, GNNs face the label scarcity issue as other neural networks do. Thus, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs to improve the model performance in this paper. Given a pre-trained GNN, we propose to search to fine-tune pre-trained graph neural networks for graph-level tasks (S2PGNN), which adaptively design a suitable fine-tuning framework for the given labeled data on the downstream task. To ensure the improvement brought by searching fine-tuning strategy, we carefully summarize a proper search space of fine-tuning framework that is suitable for GNNs. The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Besides, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area. Our code is publicly available at \url{https://anonymous.4open.science/r/code_icde2024-A9CB/}.
</details>
<details>
<summary>摘要</summary>
近期，图 нейрон网络（GNNs）在许多图关联任务中显示了无前例的成功。然而，GNNs面临标签缺乏问题，与其他神经网络一样。因此，当前努力通过大规模无标签图进行Pre-training GNNs，并将知识从无标签图传递到目标下游任务。适应通常通过精度调整Pre-trained GNNs中的一部分参数来实现。 despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only a few works have started to investigate a better fine-tuning strategy for pre-trained GNNs, but their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs to improve the model performance in this paper. Given a pre-trained GNN, we propose to search for a fine-tuning framework that adaptively designs a suitable fine-tuning strategy for the given labeled data on the downstream task. To ensure the improvement brought by searching fine-tuning strategy, we carefully summarize a proper search space of fine-tuning framework that is suitable for GNNs. The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Besides, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area. Our code is publicly available at \url{https://anonymous.4open.science/r/code_icde2024-A9CB/}.
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Allocation-of-Preventive-Care-With-Application-to-Diabetes-Mellitus-Type-II"><a href="#Data-Driven-Allocation-of-Preventive-Care-With-Application-to-Diabetes-Mellitus-Type-II" class="headerlink" title="Data-Driven Allocation of Preventive Care With Application to Diabetes Mellitus Type II"></a>Data-Driven Allocation of Preventive Care With Application to Diabetes Mellitus Type II</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06959">http://arxiv.org/abs/2308.06959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias Kraus, Stefan Feuerriegel, Maytal Saar-Tsechansky</li>
<li>for: 预防疾病的效果性评估和决策支持</li>
<li>methods: 结合Counterfactual推理、机器学习和优化技术，建立可扩展的数据驱动决策模型，可以利用现代电子医疗记录中的高维医疗数据</li>
<li>results: 对89,191名 prediabetic 患者的电子医疗记录进行评估，与现有医疗实践相比，我们的数据驱动决策模型可以每年节省11亿美元。并且在不同预算水平下进行成本效果分析。<details>
<summary>Abstract</summary>
Problem Definition. Increasing costs of healthcare highlight the importance of effective disease prevention. However, decision models for allocating preventive care are lacking.   Methodology/Results. In this paper, we develop a data-driven decision model for determining a cost-effective allocation of preventive treatments to patients at risk. Specifically, we combine counterfactual inference, machine learning, and optimization techniques to build a scalable decision model that can exploit high-dimensional medical data, such as the data found in modern electronic health records. Our decision model is evaluated based on electronic health records from 89,191 prediabetic patients. We compare the allocation of preventive treatments (metformin) prescribed by our data-driven decision model with that of current practice. We find that if our approach is applied to the U.S. population, it can yield annual savings of $1.1 billion. Finally, we analyze the cost-effectiveness under varying budget levels.   Managerial Implications. Our work supports decision-making in health management, with the goal of achieving effective disease prevention at lower costs. Importantly, our decision model is generic and can thus be used for effective allocation of preventive care for other preventable diseases.
</details>
<details>
<summary>摘要</summary>
问题定义：医疗成本的增长强调了疾病预防的重要性。然而，决策模型用于分配预防治疗的缺失。方法ология/结果：在这篇论文中，我们开发了一种基于数据的决策模型，用于确定有效分配预防治疗给患有风险的病人。具体来说，我们结合Counterfactual推理、机器学习和优化技术，构建了可扩展的决策模型，可以利用现代电子医疗记录中的高维医疗数据。我们的决策模型在89191名 prediabetic 患者的电子医疗记录上进行评估。我们将比较我们的数据驱动的决策模型与现有做法分配预防治疗（metformin）的分配方式。我们发现，如果我们的方法应用于美国人口，可以每年节省11亿美元。最后，我们分析了不同预算水平下的成本效果。管理意义：我们的工作支持医疗管理决策，以实现更有效的疾病预防，并降低成本。重要的是，我们的决策模型是通用的，可以用于有效地分配预防治疗其他预防性疾病。
</details></li>
</ul>
<hr>
<h2 id="CEmb-SAM-Segment-Anything-Model-with-Condition-Embedding-for-Joint-Learning-from-Heterogeneous-Datasets"><a href="#CEmb-SAM-Segment-Anything-Model-with-Condition-Embedding-for-Joint-Learning-from-Heterogeneous-Datasets" class="headerlink" title="CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets"></a>CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06957">http://arxiv.org/abs/2308.06957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongik Shin, Beomsuk Kim, Seungjun Baek</li>
<li>for: 助 медицин专家进行诊断和治疗过程中的自动图像分割。</li>
<li>methods: 使用多modal ultrasound图像，并将不同的 анатомиче结构或癌变分为不同的子集，以便使用单一模型进行学习和泛化。</li>
<li>results: 在实验中，使用 Condition Embedding block (CEmb-SAM) 可以有效地适应不同的子集，并且在 peripheral nerves 和 breast cancer 图像分割任务中表现出色，比基eline方法有更好的效果。<details>
<summary>Abstract</summary>
Automated segmentation of ultrasound images can assist medical experts with diagnostic and therapeutic procedures. Although using the common modality of ultrasound, one typically needs separate datasets in order to segment, for example, different anatomical structures or lesions with different levels of malignancy. In this paper, we consider the problem of jointly learning from heterogeneous datasets so that the model can improve generalization abilities by leveraging the inherent variability among datasets. We merge the heterogeneous datasets into one dataset and refer to each component dataset as a subgroup. We propose to train a single segmentation model so that the model can adapt to each sub-group. For robust segmentation, we leverage recently proposed Segment Anything model (SAM) in order to incorporate sub-group information into the model. We propose SAM with Condition Embedding block (CEmb-SAM) which encodes sub-group conditions and combines them with image embeddings from SAM. The conditional embedding block effectively adapts SAM to each image sub-group by incorporating dataset properties through learnable parameters for normalization. Experiments show that CEmb-SAM outperforms the baseline methods on ultrasound image segmentation for peripheral nerves and breast cancer. The experiments highlight the effectiveness of Cemb-SAM in learning from heterogeneous datasets in medical image segmentation tasks.
</details>
<details>
<summary>摘要</summary>
自动 segmentation of ultrasound images 可以帮助医疗专家进行诊断和治疗过程。 although using the common modality of ultrasound, one typically needs separate datasets in order to segment, for example, different anatomical structures or lesions with different levels of malignancy. 在这篇论文中，我们考虑了将异类数据集合在一起，以便模型可以利用数据集之间的自然变化来提高泛化能力。 we merge the heterogeneous datasets into one dataset and refer to each component dataset as a subgroup. we propose to train a single segmentation model so that the model can adapt to each sub-group. for robust segmentation, we leverage recently proposed Segment Anything model (SAM) in order to incorporate sub-group information into the model. we propose SAM with Condition Embedding block (CEmb-SAM) which encodes sub-group conditions and combines them with image embeddings from SAM. the conditional embedding block effectively adapts SAM to each image sub-group by incorporating dataset properties through learnable parameters for normalization. experiments show that CEmb-SAM outperforms the baseline methods on ultrasound image segmentation for peripheral nerves and breast cancer. the experiments highlight the effectiveness of Cemb-SAM in learning from heterogeneous datasets in medical image segmentation tasks.
</details></li>
</ul>
<hr>
<h2 id="Channel-Wise-Contrastive-Learning-for-Learning-with-Noisy-Labels"><a href="#Channel-Wise-Contrastive-Learning-for-Learning-with-Noisy-Labels" class="headerlink" title="Channel-Wise Contrastive Learning for Learning with Noisy Labels"></a>Channel-Wise Contrastive Learning for Learning with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06952">http://arxiv.org/abs/2308.06952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Kang, Sheng Liu, Huaxi Huang, Tongliang Liu</li>
<li>for: 本研究旨在Addressing the challenge of learning with noisy labels (LNL), 即训练一个能够从给定的实例中分辨真实的类别信息的分类器。</li>
<li>methods: 本研究提出了一种频道 wise contrastive learning (CWCL) 方法，通过在多个频道上进行对比学习，以分离真实的标签信息和噪声。</li>
<li>results: 对多个 benchmark 数据集进行评估，研究发现 CWCL 方法比既有的方法更高效，能够提取更加细腻和鲜明的特征，以便更好地分辨真实的标签信息。<details>
<summary>Abstract</summary>
In real-world datasets, noisy labels are pervasive. The challenge of learning with noisy labels (LNL) is to train a classifier that discerns the actual classes from given instances. For this, the model must identify features indicative of the authentic labels. While research indicates that genuine label information is embedded in the learned features of even inaccurately labeled data, it's often intertwined with noise, complicating its direct application. Addressing this, we introduce channel-wise contrastive learning (CWCL). This method distinguishes authentic label information from noise by undertaking contrastive learning across diverse channels. Unlike conventional instance-wise contrastive learning (IWCL), CWCL tends to yield more nuanced and resilient features aligned with the authentic labels. Our strategy is twofold: firstly, using CWCL to extract pertinent features to identify cleanly labeled samples, and secondly, progressively fine-tuning using these samples. Evaluations on several benchmark datasets validate our method's superiority over existing approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Knowing-Where-to-Focus-Event-aware-Transformer-for-Video-Grounding"><a href="#Knowing-Where-to-Focus-Event-aware-Transformer-for-Video-Grounding" class="headerlink" title="Knowing Where to Focus: Event-aware Transformer for Video Grounding"></a>Knowing Where to Focus: Event-aware Transformer for Video Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06947">http://arxiv.org/abs/2308.06947</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhyunj/eatr">https://github.com/jinhyunj/eatr</a></li>
<li>paper_authors: Jinhyun Jang, Jungin Park, Jin Kim, Hyeongjun Kwon, Kwanghoon Sohn</li>
<li>for: This paper aims to improve video grounding models by incorporating event-aware dynamic moment queries to better capture the temporal structure of videos and provide more accurate moment timestamps.</li>
<li>methods: The proposed method uses a slot attention mechanism for event reasoning and a gated fusion transformer layer for moment reasoning, which fuses the moment queries with the video-sentence representations to predict moment timestamps.</li>
<li>results: The proposed approach outperforms state-of-the-art video grounding models on several benchmarks, demonstrating its effectiveness and efficiency.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文目的是提高视频落实模型，通过包含事件相关的动态时刻查询来更好地捕捉视频的时间结构，并提供更准确的时刻查询。</li>
<li>methods: 该方法使用槽注意机制进行事件理解，并使用阀门融合变换层与视频句子表示之间的交互来预测时刻查询。</li>
<li>results: 该方法在多个benchmark上表现出色，超越了现有的视频落实模型，证明其效果和效率。<details>
<summary>Abstract</summary>
Recent DETR-based video grounding models have made the model directly predict moment timestamps without any hand-crafted components, such as a pre-defined proposal or non-maximum suppression, by learning moment queries. However, their input-agnostic moment queries inevitably overlook an intrinsic temporal structure of a video, providing limited positional information. In this paper, we formulate an event-aware dynamic moment query to enable the model to take the input-specific content and positional information of the video into account. To this end, we present two levels of reasoning: 1) Event reasoning that captures distinctive event units constituting a given video using a slot attention mechanism; and 2) moment reasoning that fuses the moment queries with a given sentence through a gated fusion transformer layer and learns interactions between the moment queries and video-sentence representations to predict moment timestamps. Extensive experiments demonstrate the effectiveness and efficiency of the event-aware dynamic moment queries, outperforming state-of-the-art approaches on several video grounding benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Event reasoning that captures distinctive event units constituting a given video using a slot attention mechanism.2. Moment reasoning that fuses the moment queries with a given sentence through a gated fusion transformer layer and learns interactions between the moment queries and video-sentence representations to predict moment timestamps.Extensive experiments demonstrate the effectiveness and efficiency of the event-aware dynamic moment queries, outperforming state-of-the-art approaches on several video grounding benchmarks.Translation notes:* DETR-based: 基于DETR的 (DETR是一种引入了 transformer 的 Object Detection 模型)* input-agnostic: 无关输入的 (ignore the input)* event-aware: 事件意识的 (aware of events)* dynamic moment queries: 动态时刻查询 (query the moment of an event)* gated fusion transformer layer: 阻塞融合变换层 (a type of transformer layer that combines multiple inputs)* video-sentence representations: 视频句子表示 (representations of video and sentence)* moment timestamps: 时刻查询 (query the moment of an event)</details></li>
</ol>
<hr>
<h2 id="Semantic-aware-Network-for-Aerial-to-Ground-Image-Synthesis"><a href="#Semantic-aware-Network-for-Aerial-to-Ground-Image-Synthesis" class="headerlink" title="Semantic-aware Network for Aerial-to-Ground Image Synthesis"></a>Semantic-aware Network for Aerial-to-Ground Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06945">http://arxiv.org/abs/2308.06945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhyunj/sanet">https://github.com/jinhyunj/sanet</a></li>
<li>paper_authors: Jinhyun Jang, Taeyong Song, Kwanghoon Sohn</li>
<li>for: 本文 targets  Aerial-to-ground image synthesis, an emerging and challenging problem that aims to synthesize a ground image from an aerial image.</li>
<li>methods: 本文提出了一个 novel framework，通过强化结构对运算和 semantic awareness 来解决这个问题。具体来说，本文引入了一个新的 semantic-attentive feature transformation module，可以将 aerial 特征转换为 ground 的 complex geographic structures。此外，本文还提出了 semantic-aware loss functions，通过利用预训练的 segmentation network，让网络 Synthesize  realistic objects across various classes，并对不同类别进行分别计算损失和均衡。</li>
<li>results: 实验结果显示，提出的 framework 能够实现高品质的 Aerial-to-ground image synthesis，并与先前的方法进行比较和范例研究。<details>
<summary>Abstract</summary>
Aerial-to-ground image synthesis is an emerging and challenging problem that aims to synthesize a ground image from an aerial image. Due to the highly different layout and object representation between the aerial and ground images, existing approaches usually fail to transfer the components of the aerial scene into the ground scene. In this paper, we propose a novel framework to explore the challenges by imposing enhanced structural alignment and semantic awareness. We introduce a novel semantic-attentive feature transformation module that allows to reconstruct the complex geographic structures by aligning the aerial feature to the ground layout. Furthermore, we propose semantic-aware loss functions by leveraging a pre-trained segmentation network. The network is enforced to synthesize realistic objects across various classes by separately calculating losses for different classes and balancing them. Extensive experiments including comparisons with previous methods and ablation studies show the effectiveness of the proposed framework both qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary>
空中图像与地面图像合成是一个emerging和挑战性的问题，目标是将空中图像转换为地面图像。由于空中和地面图像之间的 Layout和对象表示差异极大，现有的方法通常无法将空中场景中的组件迁移到地面场景中。在这篇论文中，我们提出了一个新的框架，以探讨这些挑战。我们引入了一个新的semantic-attentive特征变换模块，该模块可以将空中特征与地面布局相互对应，并且我们提出了Semantic-aware的损失函数，该函数通过使用预训练的分割网络来适应不同类别的物体，以实现Synthesize realistic的对象。我们进行了广泛的实验，包括与之前的方法进行比较和简要的ablation study，以证明我们的框架的效果。
</details></li>
</ul>
<hr>
<h2 id="Insurance-pricing-on-price-comparison-websites-via-reinforcement-learning"><a href="#Insurance-pricing-on-price-comparison-websites-via-reinforcement-learning" class="headerlink" title="Insurance pricing on price comparison websites via reinforcement learning"></a>Insurance pricing on price comparison websites via reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06935">http://arxiv.org/abs/2308.06935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanut Treetanthiploet, Yufei Zhang, Lukasz Szpruch, Isaac Bowers-Barnard, Henrietta Ridley, James Hickey, Chris Pearce</li>
<li>for: This paper aims to address the challenges of formulating effective pricing strategies for insurers on price comparison websites (PCWs) by introducing a reinforcement learning (RL) framework that integrates model-based and model-free methods.</li>
<li>methods: The proposed methodology uses a model-based component to train agents in an offline setting, and model-free algorithms in a contextual bandit (CB) manner to dynamically update the pricing policy and maximize expected revenue.</li>
<li>results: The paper demonstrates the superiority of the proposed methodology over existing off-the-shelf RL&#x2F;CB approaches using synthetic data, and shows that the hybrid agent outperforms benchmarks in terms of sample efficiency and cumulative reward.<details>
<summary>Abstract</summary>
The emergence of price comparison websites (PCWs) has presented insurers with unique challenges in formulating effective pricing strategies. Operating on PCWs requires insurers to strike a delicate balance between competitive premiums and profitability, amidst obstacles such as low historical conversion rates, limited visibility of competitors' actions, and a dynamic market environment. In addition to this, the capital intensive nature of the business means pricing below the risk levels of customers can result in solvency issues for the insurer. To address these challenges, this paper introduces reinforcement learning (RL) framework that learns the optimal pricing policy by integrating model-based and model-free methods. The model-based component is used to train agents in an offline setting, avoiding cold-start issues, while model-free algorithms are then employed in a contextual bandit (CB) manner to dynamically update the pricing policy to maximise the expected revenue. This facilitates quick adaptation to evolving market dynamics and enhances algorithm efficiency and decision interpretability. The paper also highlights the importance of evaluating pricing policies using an offline dataset in a consistent fashion and demonstrates the superiority of the proposed methodology over existing off-the-shelf RL/CB approaches. We validate our methodology using synthetic data, generated to reflect private commercially available data within real-world insurers, and compare against 6 other benchmark approaches. Our hybrid agent outperforms these benchmarks in terms of sample efficiency and cumulative reward with the exception of an agent that has access to perfect market information which would not be available in a real-world set-up.
</details>
<details>
<summary>摘要</summary>
随着价格比较网站（PCW）的出现，保险公司面临着独特的价格策略形成挑战。在PCW上运营需要保险公司坚持细致的平衡，同时综合考虑竞争价格、利润和市场环境的变化。此外，保险业务具有资本密集的特点，如果价格低于客户风险水平，可能会导致保险公司的资本危机。为 Addressing these challenges, this paper proposes a reinforcement learning (RL) framework that learns the optimal pricing policy by integrating model-based and model-free methods. The model-based component is used to train agents in an offline setting, avoiding cold-start issues, while model-free algorithms are then employed in a contextual bandit (CB) manner to dynamically update the pricing policy to maximize the expected revenue. This facilitates quick adaptation to evolving market dynamics and enhances algorithm efficiency and decision interpretability. The paper also highlights the importance of evaluating pricing policies using an offline dataset in a consistent fashion and demonstrates the superiority of the proposed methodology over existing off-the-shelf RL/CB approaches. We validate our methodology using synthetic data, generated to reflect private commercially available data within real-world insurers, and compare against 6 other benchmark approaches. Our hybrid agent outperforms these benchmarks in terms of sample efficiency and cumulative reward, with the exception of an agent that has access to perfect market information, which is not available in a real-world setting.
</details></li>
</ul>
<hr>
<h2 id="Predicting-Listing-Prices-In-Dynamic-Short-Term-Rental-Markets-Using-Machine-Learning-Models"><a href="#Predicting-Listing-Prices-In-Dynamic-Short-Term-Rental-Markets-Using-Machine-Learning-Models" class="headerlink" title="Predicting Listing Prices In Dynamic Short Term Rental Markets Using Machine Learning Models"></a>Predicting Listing Prices In Dynamic Short Term Rental Markets Using Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06929">http://arxiv.org/abs/2308.06929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Chapman, Seifey Mohammad, Kimberly Villegas</li>
<li>For: The paper aims to predict the prices of Airbnb rentals in Austin, Texas using a machine learning modeling approach, with the primary objective of constructing an accurate model and the secondary objective of identifying the key factors that drive rental prices.* Methods: The paper uses a machine learning approach and incorporates sentiment analysis into the feature engineering to gain a deeper understanding of periodic changes in Airbnb rental prices.* Results: The paper aims to provide accurate predictions of Airbnb rental prices in Austin, Texas and identify the key factors that drive these prices, with a focus on understanding how these factors vary across different locations and property types.Here is the same information in Simplified Chinese:* For: 这篇论文目标是预测美国得克萨斯州奥斯汀的空bnb租赁价格，使用机器学习模型方法，主要目标是构建准确的模型，并且次要目标是确定租赁价格的关键因素。* Methods: 论文使用机器学习方法，并将情感分析integrated into feature engineering，以更深入理解 periodic change in Airbnb租赁价格。* Results: 论文期望提供准确的空bnb租赁价格预测，并确定租赁价格关键因素，特别是在不同的地点和房型上。<details>
<summary>Abstract</summary>
Our research group wanted to take on the difficult task of predicting prices in a dynamic market. And short term rentals such as Airbnb listings seemed to be the perfect proving ground to do such a thing. Airbnb has revolutionized the travel industry by providing a platform for homeowners to rent out their properties to travelers. The pricing of Airbnb rentals is prone to high fluctuations, with prices changing frequently based on demand, seasonality, and other factors. Accurate prediction of Airbnb rental prices is crucial for hosts to optimize their revenue and for travelers to make informed booking decisions. In this project, we aim to predict the prices of Airbnb rentals using a machine learning modeling approach.   Our project expands on earlier research in the area of analyzing Airbnb rental prices by taking a methodical machine learning approach as well as incorporating sentiment analysis into our feature engineering. We intend to gain a deeper understanding on periodic changes of Airbnb rental prices. The primary objective of this study is to construct an accurate machine learning model for predicting Airbnb rental prices specifically in Austin, Texas. Our project's secondary objective is to identify the key factors that drive Airbnb rental prices and to investigate how these factors vary across different locations and property types.
</details>
<details>
<summary>摘要</summary>
我们的研究小组想要解决动态市场中价格预测的复杂任务。短期租赁如 Airbnb 列表似乎是完美的证明场地。 Airbnb 为旅行者提供了一个平台，让房东租出他们的房屋给旅行者。 Airbnb 租赁价格受到高涨的影响，价格频繁变化，与需求、季节和其他因素有关。正确预测 Airbnb 租赁价格是hosts 优化收益和旅行者做出 Informed 预订决策的关键。在这个项目中，我们使用机器学习模型方法来预测 Airbnb 租赁价格。我们的项目在分析 Airbnb 租赁价格方面进一步发展了之前的研究。我们采用了系统的机器学习方法，同时还包括了情感分析在feature工程中。我们想要更深入了解 periodic 变化 Airbnb 租赁价格。我们项目的主要目标是在奥斯汀、得克萨斯建立准确的机器学习模型，预测 Airbnb 租赁价格。我们项目的次要目标是确定 Airbnb 租赁价格的关键因素，以及这些因素在不同的地点和房型上如何变化。
</details></li>
</ul>
<hr>
<h2 id="CBA-Improving-Online-Continual-Learning-via-Continual-Bias-Adaptor"><a href="#CBA-Improving-Online-Continual-Learning-via-Continual-Bias-Adaptor" class="headerlink" title="CBA: Improving Online Continual Learning via Continual Bias Adaptor"></a>CBA: Improving Online Continual Learning via Continual Bias Adaptor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06925">http://arxiv.org/abs/2308.06925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wqza/cba-online-cl">https://github.com/wqza/cba-online-cl</a></li>
<li>paper_authors: Quanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, Deyu Meng</li>
<li>for: 提高在非站ARY数据流中进行在线 continual learning（CL）的能力，以抵御数据流中的 Distribution shift 问题。</li>
<li>methods: 提出了一种 Continual Bias Adaptor（CBA）模块，用于在训练过程中增强分类器网络，以适应不断变化的数据分布，从而保持 previously learned tasks 的稳定整合。</li>
<li>results: 通过理论分析和实验测试，证明了 CBA 模块的有效性，并与四种基eline和三个公共的 continual learning  benchmark 进行了广泛的比较。<details>
<summary>Abstract</summary>
Online continual learning (CL) aims to learn new knowledge and consolidate previously learned knowledge from non-stationary data streams. Due to the time-varying training setting, the model learned from a changing distribution easily forgets the previously learned knowledge and biases toward the newly received task. To address this problem, we propose a Continual Bias Adaptor (CBA) module to augment the classifier network to adapt to catastrophic distribution change during training, such that the classifier network is able to learn a stable consolidation of previously learned tasks. In the testing stage, CBA can be removed which introduces no additional computation cost and memory overhead. We theoretically reveal the reason why the proposed method can effectively alleviate catastrophic distribution shifts, and empirically demonstrate its effectiveness through extensive experiments based on four rehearsal-based baselines and three public continual learning benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Novel-Ehanced-Move-Recognition-Algorithm-Based-on-Pre-trained-Models-with-Positional-Embeddings"><a href="#A-Novel-Ehanced-Move-Recognition-Algorithm-Based-on-Pre-trained-Models-with-Positional-Embeddings" class="headerlink" title="A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings"></a>A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10822">http://arxiv.org/abs/2308.10822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wen, Jie Wang, Xiaodong Qiao</li>
<li>for: 本研究旨在提高中文科技论文摘要中的 Move 识别精度。</li>
<li>methods: 该算法使用了改进的预训练模型和笛卡尔网络听力机制，以获取摘要中的字符位信息，进而提高深度 semantics 学习和targeted 特征提取。</li>
<li>results: 实验结果表明，提案的算法相比原始数据集，在分割数据集上达到了13.37% 高的准确率，并与基础对比模型相比，提高了7.55%。<details>
<summary>Abstract</summary>
The recognition of abstracts is crucial for effectively locating the content and clarifying the article. Existing move recognition algorithms lack the ability to learn word position information to obtain contextual semantics. This paper proposes a novel enhanced move recognition algorithm with an improved pre-trained model and a gated network with attention mechanism for unstructured abstracts of Chinese scientific and technological papers. The proposed algorithm first performs summary data segmentation and vocabulary training. The EP-ERNIE$\_$AT-GRU framework is leveraged to incorporate word positional information, facilitating deep semantic learning and targeted feature extraction. Experimental results demonstrate that the proposed algorithm achieves 13.37$\%$ higher accuracy on the split dataset than on the original dataset and a 7.55$\%$ improvement in accuracy over the basic comparison model.
</details>
<details>
<summary>摘要</summary>
“摘要识别是对中文科技论文内容的效果搜索和解释的关键。现有的移动识别算法缺乏 Contextual semantics 的学习能力。本文提出了一种新的增强移动识别算法，包括改进的预训练模型和闭合网络带有注意力机制，以提高中文科技论文的摘要中的字位信息。提议的算法首先执行摘要数据分 segmentation 和词汇训练。EP-ERNIE $\_$ AT-GRU 框架被利用，以包括字位信息，促进深层 semantic learning 和targeted feature extraction。实验结果表明，提议的算法在分 Split 集上比原始集上的准确率高出 13.37%，与基本比较模型的准确率高出 7.55%。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="CausalLM-is-not-optimal-for-in-context-learning"><a href="#CausalLM-is-not-optimal-for-in-context-learning" class="headerlink" title="CausalLM is not optimal for in-context learning"></a>CausalLM is not optimal for in-context learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06912">http://arxiv.org/abs/2308.06912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, Radu Soricut</li>
<li>for: 本研究旨在理解 prefixLM 和 causalLM 在理论上的异同，以及它们在不同任务上的表现。</li>
<li>methods: 本研究使用了一种特定的参数构造来分析 prefixLM 和 causalLM 的收敛行为。</li>
<li>results: 我们的分析显示，prefixLM 在Linear Regression的优点点上收敛，而 causalLM 的收敛 dynamics 类似于在线 gradient descent 算法，不是 garantied 为优化的，即使样本数量在无穷大。我们的实验结果也表明， causalLM 在所有任务上一直下perform prefixLM。<details>
<summary>Abstract</summary>
Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.
</details>
<details>
<summary>摘要</summary>
近期实验证据表明，基于转换器的受Context学习（prefixLM）在比 causalLM（causalLM）的情况下表现更好，这两种语言模型的区别在于，前一种使用预测语言模型，可以让所有的受Context样本都可以互相注意，而后一种使用自动递归注意力，这会禁止受Context样本注意到未来的样本。虽然这个结果很直观，但是从理论角度来看还不够了解。在这篇论文中，我们采用理论方法，分析了 prefixLM 和 causalLM 两种语言模型在某些参数构造下的收敛行为。我们的分析表明，两种LM类型在线性收敛，但是 prefixLM  converge 到线性回归的优秀解，而 causalLM 的收敛动态类似于在线上梯度下降算法，这并不是 garantate 为INF 多个样本收敛到优秀解。我们在实验中验证了这些理论声明，通过在 sintetic 和实际任务上进行了多种 transformer 的实验，并发现 causalLM 在所有设置下 consistently underperform prefixLM。
</details></li>
</ul>
<hr>
<h2 id="GIT-Mol-A-Multi-modal-Large-Language-Model-for-Molecular-Science-with-Graph-Image-and-Text"><a href="#GIT-Mol-A-Multi-modal-Large-Language-Model-for-Molecular-Science-with-Graph-Image-and-Text" class="headerlink" title="GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text"></a>GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06911">http://arxiv.org/abs/2308.06911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Liu, Yiming Ren, Zhixiang Ren</li>
<li>for: 本研究旨在开发一种多模态大语言模型，以捕捉分子数据中的丰富和复杂信息。</li>
<li>methods: 本研究使用GIT-Mol模型，该模型结合结构图、图像和文本信息，包括简化分子输入线Entry系统（SMILES）和分子caption。为了实现多Modal数据的集成，我们提出了GIT-Former模型，可以将所有模式映射到一个统一的幽默空间。</li>
<li>results: 我们开发了一种创新的任意语言分子翻译策略，比基线或单模态模型提高了10%-15%的分子描述率，提高了5%-10%的物理预测精度，并提高了20%的分子生成有效性。<details>
<summary>Abstract</summary>
Large language models have made significant strides in natural language processing, paving the way for innovative applications including molecular representation and generation. However, most existing single-modality approaches cannot capture the abundant and complex information in molecular data. Here, we introduce GIT-Mol, a multi-modal large language model that integrates the structure Graph, Image, and Text information, including the Simplified Molecular Input Line Entry System (SMILES) and molecular captions. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture capable of mapping all modalities into a unified latent space. Our study develops an innovative any-to-language molecular translation strategy and achieves a 10%-15% improvement in molecular captioning, a 5%-10% accuracy increase in property prediction, and a 20% boost in molecule generation validity compared to baseline or single-modality models.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经做出了很大的进步，导致了创新的应用，如分子表示和生成。但是，现有的单一模式方法通常无法捕捉分子数据中的实际和复杂的信息。在这里，我们介绍GIT-Mol，一个多模式大语言模型，它统合了分子结构graph、图像和文本信息，包括简润分子输入语系 (SMILES) 和分子描述。为了促进多模式分子数据的集成，我们提出GIT-Former，一个新的架构，可以将所有模式转换到一个统一的隐藏空间中。我们的研究开发了一种创新的任意语言分子翻译策略，并在分子描述、性能预测和分子生成效果上实现了10%-15%的改善、5%-10%的精度提高和20%的效果提高，相比基准或单一模式模型。
</details></li>
</ul>
<hr>
<h2 id="Generative-Interpretation"><a href="#Generative-Interpretation" class="headerlink" title="Generative Interpretation"></a>Generative Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06907">http://arxiv.org/abs/2308.06907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yonathanarbel/generativeinterpretation">https://github.com/yonathanarbel/generativeinterpretation</a></li>
<li>paper_authors: Yonathan A. Arbel, David Hoffman</li>
<li>for:  This paper aims to introduce a new approach to estimating contractual meaning using large language models.</li>
<li>methods: The paper uses grounded case studies to illustrate the capabilities of these novel tools in distinct ways, such as ascertaining ordinary meaning in context, quantifying ambiguity, filling gaps in parties’ agreements, and calculating the probative value of individual pieces of extrinsic evidence.</li>
<li>results: The paper shows that AI models can help factfinders accurately estimate what the parties intended, and that generative interpretation can unsettle the current interpretative stalemate between efficiency-minded textualists and justice-oriented contextualists.<details>
<summary>Abstract</summary>
We introduce generative interpretation, a new approach to estimating contractual meaning using large language models. As AI triumphalism is the order of the day, we proceed by way of grounded case studies, each illustrating the capabilities of these novel tools in distinct ways. Taking well-known contracts opinions, and sourcing the actual agreements that they adjudicated, we show that AI models can help factfinders ascertain ordinary meaning in context, quantify ambiguity, and fill gaps in parties' agreements. We also illustrate how models can calculate the probative value of individual pieces of extrinsic evidence. After offering best practices for the use of these models given their limitations, we consider their implications for judicial practice and contract theory. Using LLMs permits courts to estimate what the parties intended cheaply and accurately, and as such generative interpretation unsettles the current interpretative stalemate. Their use responds to efficiency-minded textualists and justice-oriented contextualists, who argue about whether parties will prefer cost and certainty or accuracy and fairness. Parties--and courts--would prefer a middle path, in which adjudicators strive to predict what the contract really meant, admitting just enough context to approximate reality while avoiding unguided and biased assimilation of evidence. As generative interpretation offers this possibility, we argue it can become the new workhorse of contractual interpretation.
</details>
<details>
<summary>摘要</summary>
我们介绍生成解释，一种新的合约解释方法使用大型自然语言模型。随着人工智能豪语气势日益增长，我们透过实际案例进行grounded的应用，每个案例都展示了这些新工具在不同方面的能力。从知名合约案例和实际契约中获取了诉讼官员可能需要了解的资讯，我们示出AI模型可以帮助诉讼官员了解合约的内容意义，衡量内容的模糊性，填充点数和契约中的漏洞。我们还示出了AI模型可以评估个别外部证据的证据价值。在提供了最佳实践方法后，我们考虑了这些模型的局限性和影响，并探讨它们对法律实践和合约理论的影响。使用LLMs可以让法院估算合约的意思便宜且精确，因此生成解释会使现有的解释僵局破产。这种方法可以满足效率主义者和正义主义者的需求，他们认为党籍人将偏好成本和可靠性或精确和公正。党籍人和法院都偏好一条中路，在这条路上，仲裁官员将努力预测合约的真正意思，接受足够的文本背景，以避免无方向和偏袋的证据融合。因此，我们认为生成解释将成为未来合约解释的主要工具。
</details></li>
</ul>
<hr>
<h2 id="Federated-Classification-in-Hyperbolic-Spaces-via-Secure-Aggregation-of-Convex-Hulls"><a href="#Federated-Classification-in-Hyperbolic-Spaces-via-Secure-Aggregation-of-Convex-Hulls" class="headerlink" title="Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls"></a>Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06895">http://arxiv.org/abs/2308.06895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurav Prakash, Jin Sima, Chao Pan, Eli Chien, Olgica Milenkovic</li>
<li>for: 这个论文旨在解决在分布式和隐私化的设置下进行 Federated Learning 的问题，特别是在 hyperbolic spaces 中进行分类。</li>
<li>methods: 本文提出了一种首次在 hyperbolic spaces 中进行 Federated Classification 的方法，包括分布式版本的 convex SVM 分类器，integer $B_h$ 序列来解决标签替换问题，以及基于 Poincaré 盘的量化方法来限制数据泄露。</li>
<li>results: 本文通过测试多种多样化的数据集，包括具有层次结构的单元细胞 RNA-seq 数据，demonstrated 该方法可以提高分类精度，比其欧几何空间中的对应方法更好。<details>
<summary>Abstract</summary>
Hierarchical and tree-like data sets arise in many applications, including language processing, graph data mining, phylogeny and genomics. It is known that tree-like data cannot be embedded into Euclidean spaces of finite dimension with small distortion. This problem can be mitigated through the use of hyperbolic spaces. When such data also has to be processed in a distributed and privatized setting, it becomes necessary to work with new federated learning methods tailored to hyperbolic spaces. As an initial step towards the development of the field of federated learning in hyperbolic spaces, we propose the first known approach to federated classification in hyperbolic spaces. Our contributions are as follows. First, we develop distributed versions of convex SVM classifiers for Poincar\'e discs. In this setting, the information conveyed from clients to the global classifier are convex hulls of clusters present in individual client data. Second, to avoid label switching issues, we introduce a number-theoretic approach for label recovery based on the so-called integer $B_h$ sequences. Third, we compute the complexity of the convex hulls in hyperbolic spaces to assess the extent of data leakage; at the same time, in order to limit the communication cost for the hulls, we propose a new quantization method for the Poincar\'e disc coupled with Reed-Solomon-like encoding. Fourth, at server level, we introduce a new approach for aggregating convex hulls of the clients based on balanced graph partitioning. We test our method on a collection of diverse data sets, including hierarchical single-cell RNA-seq data from different patients distributed across different repositories that have stringent privacy constraints. The classification accuracy of our method is up to $\sim 11\%$ better than its Euclidean counterpart, demonstrating the importance of privacy-preserving learning in hyperbolic spaces.
</details>
<details>
<summary>摘要</summary>
随着应用领域的发展，树状和树状数据在语言处理、图数据挖掘、phylogeny和 genomics 等领域中变得越来越普遍。然而，树状数据无法在有限维度的欧式空间中嵌入，这会导致数据泄露问题。为解决这问题，我们可以使用拥有不同维度的几何空间。在分布式和隐私化的设置下，我们需要采用特有的联邦学习方法，以适应几何空间中的树状数据。为开拓联邦学习在几何空间中的领域，我们提出了首个已知的联邦分类方法。我们的贡献如下：1. 我们开发了分布式版本的 convex SVM 分类器，适用于Poincaré盘中的数据。在这个设置下，客户端上的信息都是各个客户端数据中的凸集。2. 为避免标签交换问题，我们引入了一种数学基础的方法，基于 so-called 整数 $B_h$ 序列。3. 我们计算了几何空间中凸集的复杂度，以评估数据泄露的程度。同时，我们提出了一种新的量化方法，用于压缩 Poincaré 盘。4. 在服务器端，我们引入了一种新的方法，用于将客户端上的凸集聚合到 Balanced Graph Partitioning 中。我们对一些多样化的数据集进行测试，包括不同病人的单元细胞 RNA-seq 数据，分布在不同的存储库中，具有严格的隐私限制。我们的方法的分类精度比其欧式对应方法高出至多 11%，这说明了隐私保护在几何空间中的重要性。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Offline-Online-Evaluation-with-a-Time-dependent-and-Popularity-Bias-free-Offline-Metric-for-Recommenders"><a href="#Bridging-Offline-Online-Evaluation-with-a-Time-dependent-and-Popularity-Bias-free-Offline-Metric-for-Recommenders" class="headerlink" title="Bridging Offline-Online Evaluation with a Time-dependent and Popularity Bias-free Offline Metric for Recommenders"></a>Bridging Offline-Online Evaluation with a Time-dependent and Popularity Bias-free Offline Metric for Recommenders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06885">http://arxiv.org/abs/2308.06885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Petr Kasalický, Rodrigo Alves, Pavel Kordík</li>
<li>for: 评估推荐系统的效果是一项复杂的任务。在线和离线评估指标对推荐系统的真正目标存在偏误。大多数最近发表的论文使用不精准的离线评估方法来评估其方法的性能，从而削弱了学术研究对实际应用的影响。</li>
<li>methods: 我们研究了和评估推荐系统在线性能相关的离线评估指标。我们发现，惩罚受欢迎的Item和考虑交易时间在评估中可以提高我们选择最佳推荐模型的能力。</li>
<li>results: 我们对五个大规模的实际应用数据进行了测试，并发现，使用我们提出的离线评估指标可以更好地反映推荐系统在线性能。这些结果可以帮助学术界更好地理解离线评估和优化的标准，以便更好地应用推荐系统。<details>
<summary>Abstract</summary>
The evaluation of recommendation systems is a complex task. The offline and online evaluation metrics for recommender systems are ambiguous in their true objectives. The majority of recently published papers benchmark their methods using ill-posed offline evaluation methodology that often fails to predict true online performance. Because of this, the impact that academic research has on the industry is reduced. The aim of our research is to investigate and compare the online performance of offline evaluation metrics. We show that penalizing popular items and considering the time of transactions during the evaluation significantly improves our ability to choose the best recommendation model for a live recommender system. Our results, averaged over five large-size real-world live data procured from recommenders, aim to help the academic community to understand better offline evaluation and optimization criteria that are more relevant for real applications of recommender systems.
</details>
<details>
<summary>摘要</summary>
评估推荐系统的复杂性使得评估方法存在各种问题。在线和离线评估指标对于推荐系统来说是不确定的。大多数最近发表的论文使用不精准的离线评估方法来评估自己的方法，这会导致实际在线性能与评估结果存在差异。这种情况使得学术研究对于实际应用的影响减少。我们的研究目标是研究和比较在线评估指标的表现。我们发现，对 популяр item 进行 penalty 和在评估过程中考虑交易时间可以显著改善我们选择最佳推荐模型的能力。我们的结果，基于五个大型实际生产环境中的真实数据， hopes to help学术界更好地理解推荐系统的离线评估和优化标准，以便更好地应用于实际推荐系统中。
</details></li>
</ul>
<hr>
<h2 id="Multi-Receiver-Task-Oriented-Communications-via-Multi-Task-Deep-Learning"><a href="#Multi-Receiver-Task-Oriented-Communications-via-Multi-Task-Deep-Learning" class="headerlink" title="Multi-Receiver Task-Oriented Communications via Multi-Task Deep Learning"></a>Multi-Receiver Task-Oriented Communications via Multi-Task Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06884">http://arxiv.org/abs/2308.06884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus</li>
<li>for: 本研究探讨了任务导向的通信系统，在 transmitter 与多个接收器之间进行交互，每个接收器都需要完成自己的任务，例如图像分类等，并在 transmitter 上训练共享encoder和每个接收器上的专门decoder。</li>
<li>methods: 该方法使用多任务深度学习来实现多任务的共同优化和多接收器之间的通信，并通过在边缘的6G网络中进行有效的资源分配，以适应不同的通信频道条件，并最小化传输过程中的过载。</li>
<li>results: 实验结果表明，相比单任务导向的通信系统，多任务导向的通信系统可以更好地适应不同的任务和通信环境，并且可以提高图像分类精度和资源利用率。<details>
<summary>Abstract</summary>
This paper studies task-oriented, otherwise known as goal-oriented, communications, in a setting where a transmitter communicates with multiple receivers, each with its own task to complete on a dataset, e.g., images, available at the transmitter. A multi-task deep learning approach that involves training a common encoder at the transmitter and individual decoders at the receivers is presented for joint optimization of completing multiple tasks and communicating with multiple receivers. By providing efficient resource allocation at the edge of 6G networks, the proposed approach allows the communications system to adapt to varying channel conditions and achieves task-specific objectives while minimizing transmission overhead. Joint training of the encoder and decoders using multi-task learning captures shared information across tasks and optimizes the communication process accordingly. By leveraging the broadcast nature of wireless communications, multi-receiver task-oriented communications (MTOC) reduces the number of transmissions required to complete tasks at different receivers. Performance evaluation conducted on the MNIST, Fashion MNIST, and CIFAR-10 datasets (with image classification considered for different tasks) demonstrates the effectiveness of MTOC in terms of classification accuracy and resource utilization compared to single-task-oriented communication systems.
</details>
<details>
<summary>摘要</summary>
Performance evaluation on the MNIST, Fashion MNIST, and CIFAR-10 datasets (with image classification considered for different tasks) demonstrates the effectiveness of task-oriented communication in terms of classification accuracy and resource utilization.
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Outlierness-of-Funds-from-their-Categories-using-Supervised-Similarity"><a href="#Quantifying-Outlierness-of-Funds-from-their-Categories-using-Supervised-Similarity" class="headerlink" title="Quantifying Outlierness of Funds from their Categories using Supervised Similarity"></a>Quantifying Outlierness of Funds from their Categories using Supervised Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06882">http://arxiv.org/abs/2308.06882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Desai, Ashmita Dhiman, Tushar Sharma, Deepika Sharma, Dhagash Mehta, Stefano Pasquali</li>
<li>For: 本研究旨在量化基金分类错误的影响，并提出一种基于机器学习的方法来检测和识别基金分类错误。* Methods: 本研究使用Random Forest方法进行距离度量学习，计算每个数据点的类别异常指标，以识别基金分类错误。* Results: 研究发现基金分类错误与未来回报之间存在强相关关系，并讨论了这些结果的意义。<details>
<summary>Abstract</summary>
Mutual fund categorization has become a standard tool for the investment management industry and is extensively used by allocators for portfolio construction and manager selection, as well as by fund managers for peer analysis and competitive positioning. As a result, a (unintended) miscategorization or lack of precision can significantly impact allocation decisions and investment fund managers. Here, we aim to quantify the effect of miscategorization of funds utilizing a machine learning based approach. We formulate the problem of miscategorization of funds as a distance-based outlier detection problem, where the outliers are the data-points that are far from the rest of the data-points in the given feature space. We implement and employ a Random Forest (RF) based method of distance metric learning, and compute the so-called class-wise outlier measures for each data-point to identify outliers in the data. We test our implementation on various publicly available data sets, and then apply it to mutual fund data. We show that there is a strong relationship between the outlier measures of the funds and their future returns and discuss the implications of our findings.
</details>
<details>
<summary>摘要</summary>
资金基金分类已成为投资管理industry的标准工具，广泛用于分配和资金经理选择，以及资金管理人员用于对比分析和竞争位置。因此，任意错误分类或缺乏精度可能对分配决策和投资基金产生深远的影响。在这里，我们想要量化基金分类错误的影响，使用机器学习基于的方法。我们将基金分类问题定义为一个距离度量学习问题，其中异常数据点是与其他数据点在给定特征空间的距离最远的数据点。我们实现了Random Forest（RF）基于的距离度量学习方法，并计算每个数据点的类别异常度量来标识异常数据点。我们在各种公开可用的数据集上测试了我们的实现，然后应用于基金数据。我们发现，基金异常度量和未来回报之间存在强相关关系，并讨论了我们的发现的意义。
</details></li>
</ul>
<hr>
<h2 id="AutoSeqRec-Autoencoder-for-Efficient-Sequential-Recommendation"><a href="#AutoSeqRec-Autoencoder-for-Efficient-Sequential-Recommendation" class="headerlink" title="AutoSeqRec: Autoencoder for Efficient Sequential Recommendation"></a>AutoSeqRec: Autoencoder for Efficient Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06878">http://arxiv.org/abs/2308.06878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sliu675/autoseqrec">https://github.com/sliu675/autoseqrec</a></li>
<li>paper_authors: Sijia Liu, Jiahao Liu, Hansu Gu, Dongsheng Li, Tun Lu, Peng Zhang, Ning Gu</li>
<li>for: 这篇论文主要针对继承推荐任务，旨在提供一种高效可靠的继承推荐方法。</li>
<li>methods: 该方法基于自适应器，包括一个编码器和三个解码器，它们考虑了用户-项交互矩阵和项过渡矩阵的行列。</li>
<li>results: 对比其他方法，AutoSeqRec显示了更高的准确率，同时具有更好的可靠性和效率。<details>
<summary>Abstract</summary>
Sequential recommendation demonstrates the capability to recommend items by modeling the sequential behavior of users. Traditional methods typically treat users as sequences of items, overlooking the collaborative relationships among them. Graph-based methods incorporate collaborative information by utilizing the user-item interaction graph. However, these methods sometimes face challenges in terms of time complexity and computational efficiency. To address these limitations, this paper presents AutoSeqRec, an incremental recommendation model specifically designed for sequential recommendation tasks. AutoSeqRec is based on autoencoders and consists of an encoder and three decoders within the autoencoder architecture. These components consider both the user-item interaction matrix and the rows and columns of the item transition matrix. The reconstruction of the user-item interaction matrix captures user long-term preferences through collaborative filtering. In addition, the rows and columns of the item transition matrix represent the item out-degree and in-degree hopping behavior, which allows for modeling the user's short-term interests. When making incremental recommendations, only the input matrices need to be updated, without the need to update parameters, which makes AutoSeqRec very efficient. Comprehensive evaluations demonstrate that AutoSeqRec outperforms existing methods in terms of accuracy, while showcasing its robustness and efficiency.
</details>
<details>
<summary>摘要</summary>
带有顺序推荐的模型可以根据用户的顺序行为来推荐项目。传统方法通常将用户看作为序列中的项目，忽略了用户之间的协同关系。基于图的方法可以包含协同信息，但是它们有时会面临时间复杂度和计算效率的限制。为了解决这些限制，这篇论文提出了 AutoSeqRec，一种特点是适用于顺序推荐任务的增量推荐模型。AutoSeqRec基于自适应器，包括一个Encoder和三个解码器。这些组件考虑了用户-项目交互矩阵和行列式的项目过渡矩阵。重建用户-项目交互矩阵可以捕捉用户长期的偏好，通过协同筛选。此外，行列式的项目过渡矩阵表示用户短期的兴趣，允许模型化用户的短期偏好。在进行增量推荐时，只需更新输入矩阵，无需更新参数，这使得AutoSeqRec非常高效。 comprehensive评估表明，AutoSeqRec在准确性方面超过了现有方法，并展示了其稳定性和高效性。
</details></li>
</ul>
<hr>
<h2 id="SpeechX-Neural-Codec-Language-Model-as-a-Versatile-Speech-Transformer"><a href="#SpeechX-Neural-Codec-Language-Model-as-a-Versatile-Speech-Transformer" class="headerlink" title="SpeechX: Neural Codec Language Model as a Versatile Speech Transformer"></a>SpeechX: Neural Codec Language Model as a Versatile Speech Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06873">http://arxiv.org/abs/2308.06873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, Takuya Yoshioka</li>
<li>for: 高质量零批text-to-speech创新和多种语音转换任务</li>
<li>methods:  neural codec语言模型+多任务学习+任务dependent prompting</li>
<li>results: 在多种任务中表现优秀，包括零批TTS、噪声抑制、目标 speaker提取、speech removing和speech editing等，与专门模型相当或更高的性能<details>
<summary>Abstract</summary>
Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.
</details>
<details>
<summary>摘要</summary>
近期，基于音频文本提示的生成术语模型得到了很大的进步，可以实现高质量的零shot文本至语音转化。然而，现有模型仍然面临着处理多样化音频文本术语生成任务的限制，包括转化输入语音和处理附带噪声的audio捕获条件。本文介绍SpeechX，一种通用的术语生成模型，可以实现零shot TTS和多种术语转换任务，并处理干净和噪声信号。SpeechX通过神经编码语言模型和多任务学习，使用任务特定的提示，实现了一个统一和可扩展的模型，可以通过文本输入来进行语音提高和转换任务。实验结果表明SpeechX在多种任务中具有优秀的表现，包括零shot TTS、噪声减少、目标说话人EXTRACTION、语音删除和语音编辑等，与专门的模型相比，在任务中表现相当或更高。请参考https://aka.ms/speechx查看示例样本。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Dual-Stream-Self-Attentive-Adversarial-Graph-Contrastive-Learning-for-Cross-Subject-EEG-based-Emotion-Recognition"><a href="#Semi-Supervised-Dual-Stream-Self-Attentive-Adversarial-Graph-Contrastive-Learning-for-Cross-Subject-EEG-based-Emotion-Recognition" class="headerlink" title="Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition"></a>Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11635">http://arxiv.org/abs/2308.11635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weishan Ye, Zhiguo Zhang, Min Zhang, Fei Teng, Li Zhang, Linling Li, Gan Huang, Jianhong Wang, Dong Ni, Zhen Liang<br>for: 本研究旨在解决脑波Emotion recognition中数据标注率的限制问题，提高脑波Emotion recognition的精度和可靠性。methods: 本研究提出了一种基于Self-Attentive Adversarial Graph Contrastive learning的半supervised Dual-stream架构（简称DS-AGC），包括两个平行流处理非结构和结构脑波特征。非结构流使用半supervised多域适应方法来减轻来源频率域和目标频率域之间的分布差异。结构流开发了图像异构学习方法来提取多个EEG渠道之间的有效特征表示。此外，一种自注意 fusion模块也被提出，用于特征融合、样本选择和情绪识别，其中更加注重EEG特征与数据样本在标签源频率域中更加相关的部分。results: 经过对两个 benchmark数据库（SEED和SEED-IV）进行了大规模的实验，结果表明，提出的模型在不同的受测 incomplete label 条件下（包括标签率不同的情况）表现出了5.83%和6.99%的提高在SEED和SEED-IV数据库上，证明了该模型在跨主体EEG Emotion recognition中有效地解决了标签稀缺问题。<details>
<summary>Abstract</summary>
Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-Attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentive fusion module is developed for feature fusion, sample selection, and emotion recognition, which highlights EEG features more relevant to emotions and data samples in the labeled source domain that are closer to the target domain. Extensive experiments conducted on two benchmark databases (SEED and SEED-IV) using a semi-supervised cross-subject leave-one-subject-out cross-validation evaluation scheme show that the proposed model outperforms existing methods under different incomplete label conditions (with an average improvement of 5.83% on SEED and 6.99% on SEED-IV), demonstrating its effectiveness in addressing the label scarcity problem in cross-subject EEG-based emotion recognition.
</details>
<details>
<summary>摘要</summary>
电子脑电图像 (EEG) 是一种客观工具 для情感识别，具有广泛的应用前景。然而，有限的标签数据仍然是这一领域的主要挑战，限制了 EEG 基于情感识别的普及使用。在这篇论文中，一种半supervised dual-stream自我注意力对抗图像学习框架（简称 DS-AGC）被提出，以解决跨主体 EEG 基于情感识别的标签稀缺问题。DS-AGC 框架包括两个并行的流动，用于提取非结构化和结构化 EEG 特征。非结构化流动利用半supervised多域适应方法，以减轻不同标签领域之间的分布差异。结构化流动开发了图像学习方法，以提取多个 EEG 通道之间的有效图像特征表示。此外，一个自我注意力融合模块被开发，用于特征融合、样本选择和情感识别，并强调 EEG 特征更加 relevante 于情感和数据样本在标签领域中更加接近的目标领域。广泛的实验在两个标准数据库（SEED 和 SEED-IV）上进行，使用半supervised cross-subject leave-one-subject-out 跨领域验证方式，显示提出的模型在不同的标签缺失情况下（SEED 上的平均提升率为 5.83%，SEED-IV 上的平均提升率为 6.99%），表明其能够有效地解决跨主体 EEG 基于情感识别的标签稀缺问题。
</details></li>
</ul>
<hr>
<h2 id="Effect-of-Choosing-Loss-Function-when-Using-T-batching-for-Representation-Learning-on-Dynamic-Networks"><a href="#Effect-of-Choosing-Loss-Function-when-Using-T-batching-for-Representation-Learning-on-Dynamic-Networks" class="headerlink" title="Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks"></a>Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06862">http://arxiv.org/abs/2308.06862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/erfanloghmani/effect-of-loss-function-tbatching">https://github.com/erfanloghmani/effect-of-loss-function-tbatching</a></li>
<li>paper_authors: Erfan Loghmani, MohammadAmin Fazli</li>
<li>for: 这篇论文旨在探讨静态网络模型的动态学习方法，以提高网络模型的训练效率和准确性。</li>
<li>methods: 本论文提出了两种替代训练损失函数，并通过数学分析显示了这些损失函数可以解决训练损失函数中的问题，提高训练性能。</li>
<li>results: 实验结果显示，将这两种替代损失函数应用于训练静态网络模型可以提高模型的训练效率和准确性，特别是在实际世界的动态网络上。<details>
<summary>Abstract</summary>
Representation learning methods have revolutionized machine learning on networks by converting discrete network structures into continuous domains. However, dynamic networks that evolve over time pose new challenges. To address this, dynamic representation learning methods have gained attention, offering benefits like reduced learning time and improved accuracy by utilizing temporal information.   T-batching is a valuable technique for training dynamic network models that reduces training time while preserving vital conditions for accurate modeling. However, we have identified a limitation in the training loss function used with t-batching. Through mathematical analysis, we propose two alternative loss functions that overcome these issues, resulting in enhanced training performance.   We extensively evaluate the proposed loss functions on synthetic and real-world dynamic networks. The results consistently demonstrate superior performance compared to the original loss function. Notably, in a real-world network characterized by diverse user interaction histories, the proposed loss functions achieved more than 26.9% enhancement in Mean Reciprocal Rank (MRR) and more than 11.8% improvement in Recall@10. These findings underscore the efficacy of the proposed loss functions in dynamic network modeling.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>机器学习在网络上进行了革命，通过将分类网络结构转换为连续域。然而，时间演化的网络却提出了新的挑战。为此，动态表示学习方法在抓取到关注。这些方法可以减少学习时间，并使用时间信息提高准确性。 T-批处理是训练动态网络模型的有价值技术，可以降低训练时间，保持模型准确的条件。然而，我们发现在使用 t-批处理时的训练损失函数中存在一定的限制。通过数学分析，我们提出了两种替代的损失函数，可以解决这些问题，从而提高训练性能。我们对提出的损失函数进行了广泛的评估，在 sintetic 和实际的动态网络上进行了测试。结果表明，提出的损失函数在动态网络模型训练中具有显著优势，与原始损失函数相比，可以提高 Mean Reciprocal Rank（MRR）的表现至少26.9%，并提高 Recall@10 的表现至少11.8%。这些发现证明了我们提出的损失函数在动态网络模型中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Offensive-Gameplan-in-the-National-Basketball-Association-with-Machine-Learning"><a href="#Optimizing-Offensive-Gameplan-in-the-National-Basketball-Association-with-Machine-Learning" class="headerlink" title="Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning"></a>Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06851">http://arxiv.org/abs/2308.06851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eamon Mukhopadhyay</li>
<li>for: 本研究的目的是确认Stats的有效性，以及将Stats与NBA比赛类型之间建立关联性。</li>
<li>methods: 本研究使用了机器学习技术，选择了一组特定的特征，以评估Stats的有效性。 linear regression 模型和对应性网络模型都被用来检验Stats 的跟踪能力。</li>
<li>results: 研究发现，使用ORTG 的 linear regression 模型和对应性网络模型都能够与不同的NBA比赛类型建立关联性。然而，使用对应性网络模型的精度较高。通过对模型的调整，研究人员发现了一组特定的特征，可以帮助建立一个高效的进攻策略。<details>
<summary>Abstract</summary>
Throughout the analytical revolution that has occurred in the NBA, the development of specific metrics and formulas has given teams, coaches, and players a new way to see the game. However - the question arises - how can we verify any metrics? One method would simply be eyeball approximation (trying out many different gameplans) and/or trial and error - an estimation-based and costly approach. Another approach is to try to model already existing metrics with a unique set of features using machine learning techniques. The key to this approach is that with these features that are selected, we can try to gauge the effectiveness of these features combined, rather than using individual analysis in simple metric evaluation. If we have an accurate model, it can particularly help us determine the specifics of gameplan execution. In this paper, the statistic ORTG (Offensive Rating, developed by Dean Oliver) was found to have a correlation with different NBA playtypes using both a linear regression model and a neural network regression model, although ultimately, a neural network worked slightly better than linear regression. Using the accuracy of the models as a justification, the next step was to optimize the output of the model with test examples, which would demonstrate the combination of features to best achieve a highly functioning offense.
</details>
<details>
<summary>摘要</summary>
在NBA的分析革命中，开发特定的指标和公式为球队、教练和球员提供了一种新的视角。然而，问题出现：如何证明这些指标？一种方法是通过观察和尝试多种战斗策略来估算，这是一种估算性的和昂贵的方法。另一种方法是使用机器学习技术来模型现有的指标，并使用这些特定的特征来评估这些指标的效果。如果我们有一个准确的模型，那么它可以帮助我们确定游戏计划的具体执行方式。根据这篇论文，由Dean Oliver开发的ORTG指标（进攻评估指标）与不同的NBA战斗类型之间存在正相关关系，使用线性回归模型和神经网络回归模型进行评估，最终神经网络模型的性能略高于线性回归模型。使用模型的准确性为正当化，接下来的步骤是使用测试例子来优化模型的输出，以达到高效的攻击战斗。
</details></li>
</ul>
<hr>
<h2 id="When-Monte-Carlo-Dropout-Meets-Multi-Exit-Optimizing-Bayesian-Neural-Networks-on-FPGA"><a href="#When-Monte-Carlo-Dropout-Meets-Multi-Exit-Optimizing-Bayesian-Neural-Networks-on-FPGA" class="headerlink" title="When Monte-Carlo Dropout Meets Multi-Exit: Optimizing Bayesian Neural Networks on FPGA"></a>When Monte-Carlo Dropout Meets Multi-Exit: Optimizing Bayesian Neural Networks on FPGA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06849">http://arxiv.org/abs/2308.06849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/os-hxfan/bayesnn_fpga">https://github.com/os-hxfan/bayesnn_fpga</a></li>
<li>paper_authors: Hongxiang Fan, Hao Chen, Liam Castelli, Zhiqiang Que, He Li, Kenneth Long, Wayne Luk</li>
<li>for: 提高安全应用中的投机率预测，如医学影像和自动驾驶。</li>
<li>methods: 提出了一种基于多出口Monte Carlo Dropout（MCD）的 bayesian neural network，实现了准确预测，同时降低了算法复杂性。</li>
<li>results: 对比CPU、GPU和其他当前硬件实现，自动生成的加速器实现了更高的能效率。<details>
<summary>Abstract</summary>
Bayesian Neural Networks (BayesNNs) have demonstrated their capability of providing calibrated prediction for safety-critical applications such as medical imaging and autonomous driving. However, the high algorithmic complexity and the poor hardware performance of BayesNNs hinder their deployment in real-life applications. To bridge this gap, this paper proposes a novel multi-exit Monte-Carlo Dropout (MCD)-based BayesNN that achieves well-calibrated predictions with low algorithmic complexity. To further reduce the barrier to adopting BayesNNs, we propose a transformation framework that can generate FPGA-based accelerators for multi-exit MCD-based BayesNNs. Several novel optimization techniques are introduced to improve hardware performance. Our experiments demonstrate that our auto-generated accelerator achieves higher energy efficiency than CPU, GPU, and other state-of-the-art hardware implementations.
</details>
<details>
<summary>摘要</summary>
bayesian neural networks (bayesNNs) 有显示出在安全关键应用，如医疗成像和自动驾驶中提供了调整后预测的能力。然而，高算法复杂性和 BayesNNs 的硬件性能问题使得它们在实际应用中困难得 deployment。为bridge这个差距，本文提出了一种基于多出口 Monte Carlo Dropout (MCD) 的 BayesNN ，可以实现低算法复杂性下的准确预测。此外，我们还提出了一种转换框架，可以生成 FPGA 基于的加速器，以便快速采用 BayesNNs。我们还引入了一些新的优化技术，以提高硬件性能。我们的实验表明，我们自动生成的加速器在能耗效率方面高于 CPU、GPU 和其他现有硬件实现。Note: "BayesNNs" is a abbreviation of "Bayesian Neural Networks" in English, and "bayesNNs" is the pinyin Romanization of "拜耳 нейрон网络" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Topological-Graph-Neural-Networks-with-Paths"><a href="#Generalizing-Topological-Graph-Neural-Networks-with-Paths" class="headerlink" title="Generalizing Topological Graph Neural Networks with Paths"></a>Generalizing Topological Graph Neural Networks with Paths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06838">http://arxiv.org/abs/2308.06838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang Truong, Peter Chin</li>
<li>for: 本文主要研究Graph Neural Networks (GNNs)的限制和提高。</li>
<li>methods: 本文提出了一种以路径为中心的方法，该方法可以在不假设图structure的情况下提高GNNs的性能。</li>
<li>results: 本文的方法在多个 benchmark 上达到了状态之artefact的表现。<details>
<summary>Abstract</summary>
While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.
</details>
<details>
<summary>摘要</summary>
GNNS （图 neural network）在多种领域取得了重要进步，但它们受到一种理论限制，称为Weisfeiler-Lehmann测试。 latest advancements in higher-order GNNs 可以突破这个限制，但它们通常围绕 graf 组件如 clique 或 cycle 进行中心。 然而，我们的研究采取了不同的方向。 我们强调 path，这些是所有 graf 的内在特征。 我们可以构建一个更通用的 topological 视角，并与其他已确立的 topological 领域之间建立桥梁。 很有趣的是，不需要任何关于 graf 子结构的假设，我们的方法可以在这个领域中超越之前的技术，在多个 benchmark 上达到状态的表现。
</details></li>
</ul>
<hr>
<h2 id="InTune-Reinforcement-Learning-based-Data-Pipeline-Optimization-for-Deep-Recommendation-Models"><a href="#InTune-Reinforcement-Learning-based-Data-Pipeline-Optimization-for-Deep-Recommendation-Models" class="headerlink" title="InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models"></a>InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08500">http://arxiv.org/abs/2308.08500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kabir Nagrecha, Lingyi Liu, Pablo Delgado, Prasanna Padmanabhan</li>
<li>for: 这个论文的目的是探讨深度学习推荐模型（DLRM）的训练过程中的数据接收问题，以及这个问题在现实世界中的瓶颈和挑战。</li>
<li>methods: 这篇论文使用了人工智能的强化学习（RL）技术来解决数据接收问题，RL机器学习agent可以学习如何在DLRM数据管道中分配CPU资源，以更好地并行数据加载和提高throughput。</li>
<li>results: 实验表明，使用InTune可以在只需几分钟之内构建优化数据管道配置，并且可以轻松地与现有训练工作流 integrate。InTune可以提高在线数据接收率，从而减少模型执行时间的浪费和提高效率。在实际世界中应用InTune后，发现它可以提高数据接收 durchput 比现有数据管道优化器高出2.29倍，同时也提高CPU和GPU资源的利用率。<details>
<summary>Abstract</summary>
Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- and time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning training jobs are dominated by model execution, the most important factor in DLRM training performance is often online data ingestion.   In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into DLRM training pipeline bottlenecks and challenges. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and to identify shortfalls in existing pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization to adopt. Our studies lead us to design and build a new solution for data pipeline optimization, InTune.   InTune employs a reinforcement learning (RL) agent to learn how to distribute the CPU resources of a trainer machine across a DLRM data pipeline to more effectively parallelize data loading and improve throughput. Our experiments show that InTune can build an optimized data pipeline configuration within only a few minutes, and can easily be integrated into existing training workflows. By exploiting the responsiveness and adaptability of RL, InTune achieves higher online data ingestion rates than existing optimizers, thus reducing idle times in model execution and increasing efficiency. We apply InTune to our real-world cluster, and find that it increases data ingestion throughput by as much as 2.29X versus state-of-the-art data pipeline optimizers while also improving both CPU & GPU utilization.
</details>
<details>
<summary>摘要</summary>
现代推荐系统中的深度学习基于模型（DLRM）已成为关键组件。许多公司正在建立专门用于DLRM训练的大型计算集群，导致新的成本和时间OPTIMIZATION的兴趣。这种系统中的挑战是唯一的；通常的深度学习训练任务由模型执行所主导，而DLRM训练中最重要的因素则是在线数据接收。在这篇论文中，我们探讨DLRM数据接收问题的独特特点，并对DLRM训练管道中的瓶颈和挑战提供了新的视角。我们研究了Netflix的计算集群中的真实DLRM数据处理管道，以观察在线接收的性能影响和标准化管道优化器的缺陷。我们发现现有的工具可以提供低效率、频繁崩溃或者需要重新组织集群的优化。为了解决这些问题，我们设计了一种新的数据管道优化解决方案——InTune。InTune使用了强化学习（RL）代理来学习如何在DLRM数据管道中分配训练机器的CPU资源，以更好地并行数据加载并提高吞吐量。我们的实验表明，InTune可以在只需几分钟之内构建优化后的数据管道配置，并可以轻松地与现有训练工作流 integrate。通过强化学习的响应和适应性，InTune可以在现有优化器的基础上提高在线数据接收速率，从而降低模型执行时间的浪费和提高效率。我们在实际 cluster 中应用InTune，发现它可以提高数据接收吞吐量，最高可达2.29倍于当前状态艺术数据管道优化器，同时也提高CPU和GPU资源的利用率。
</details></li>
</ul>
<hr>
<h2 id="An-Ensemble-Approach-to-Question-Classification-Integrating-Electra-Transformer-GloVe-and-LSTM"><a href="#An-Ensemble-Approach-to-Question-Classification-Integrating-Electra-Transformer-GloVe-and-LSTM" class="headerlink" title="An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM"></a>An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06828">http://arxiv.org/abs/2308.06828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanad Aburass, Osama Dorgham</li>
<li>for: 本研究提出了一种新的集成方法，用于问题分类任务。</li>
<li>methods: 该模型使用了现代化的 Electra、GloVe 和 LSTM 模型，并将其集成起来，以提高问题分类的精度和效率。</li>
<li>results: 对于 TREC 数据集上的问题分类任务，我们的模型实现了以下成果： accuracy 0.8 。这些结果表明，集成方法在问题分类任务中具有显著的优势，并且鼓励进一步探索 ensemble 方法在自然语言处理中的应用。<details>
<summary>Abstract</summary>
This paper introduces a novel ensemble approach for question classification using state-of-the-art models -- Electra, GloVe, and LSTM. The proposed model is trained and evaluated on the TREC dataset, a well-established benchmark for question classification tasks. The ensemble model combines the strengths of Electra, a transformer-based model for language understanding, GloVe, a global vectors for word representation, and LSTM, a recurrent neural network variant, providing a robust and efficient solution for question classification. Extensive experiments were carried out to compare the performance of the proposed ensemble approach with other cutting-edge models, such as BERT, RoBERTa, and DistilBERT. Our results demonstrate that the ensemble model outperforms these models across all evaluation metrics, achieving an accuracy of 0.8 on the test set. These findings underscore the effectiveness of the ensemble approach in enhancing the performance of question classification tasks, and invite further exploration of ensemble methods in natural language processing.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的ensemble方法 для问题分类，使用当今最佳模型——Electra、GloVe和LSTM。提议的模型在TREC数据集上进行训练和评估，TREC数据集是问题分类任务的可靠的标准 benchmark。 ensemble模型结合了Electra、GloVe和LSTM的优势，提供了一种强大和高效的问题分类解决方案。我们进行了广泛的实验，比较了提议的ensemble方法与其他最新的模型，如BERT、RoBERTa和DistilBERT的性能。我们的结果表明， ensemble模型在所有评价指标上都超过了这些模型，在测试集上达到了0.8的准确率。这些发现证明了 ensemble方法在问题分类任务中的效果，并邀请了进一步的ensemble方法在自然语言处理领域的探索。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Graph-Clustering-with-Unknown-Cluster-Number"><a href="#Reinforcement-Graph-Clustering-with-Unknown-Cluster-Number" class="headerlink" title="Reinforcement Graph Clustering with Unknown Cluster Number"></a>Reinforcement Graph Clustering with Unknown Cluster Number</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06827">http://arxiv.org/abs/2308.06827</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yueliu1999/awesome-deep-graph-clustering">https://github.com/yueliu1999/awesome-deep-graph-clustering</a></li>
<li>paper_authors: Yue Liu, Ke Liang, Jun Xia, Xihong Yang, Sihang Zhou, Meng Liu, Xinwang Liu, Stan Z. Li</li>
<li>for: 这个论文的目标是提出一种无监督的深度图 clustering 方法，以便在实际场景中不需要预先定义群集数量。</li>
<li>methods: 该方法使用了强化学习机制，将集群数量决定和无监督表示学习集成到一个统一框架中。首先学习出了强化表示，然后考虑了节点和集群状态，并使用了质量网络评估不同群集数量的质量。最后，通过强化学习机制来确定最佳的群集数量。</li>
<li>results: 实验表明，提出的方法可以有效地进行深度图 clustering，并且比既有方法更加高效。code 和数据集可以在 GitHub 上找到。<details>
<summary>Abstract</summary>
Deep graph clustering, which aims to group nodes into disjoint clusters by neural networks in an unsupervised manner, has attracted great attention in recent years. Although the performance has been largely improved, the excellent performance of the existing methods heavily relies on an accurately predefined cluster number, which is not always available in the real-world scenario. To enable the deep graph clustering algorithms to work without the guidance of the predefined cluster number, we propose a new deep graph clustering method termed Reinforcement Graph Clustering (RGC). In our proposed method, cluster number determination and unsupervised representation learning are unified into a uniform framework by the reinforcement learning mechanism. Concretely, the discriminative node representations are first learned with the contrastive pretext task. Then, to capture the clustering state accurately with both local and global information in the graph, both node and cluster states are considered. Subsequently, at each state, the qualities of different cluster numbers are evaluated by the quality network, and the greedy action is executed to determine the cluster number. In order to conduct feedback actions, the clustering-oriented reward function is proposed to enhance the cohesion of the same clusters and separate the different clusters. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method. The source code of RGC is shared at https://github.com/yueliu1999/RGC and a collection (papers, codes and, datasets) of deep graph clustering is shared at https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering on Github.
</details>
<details>
<summary>摘要</summary>
深度图 clustering，目标是通过神经网络在无监督情况下将节点分组成不重叠的分组，在过去几年内吸引了很大的关注。虽然现有的方法已经提高了性能，但是它们的出色表现受到准确预定的分组数量的限制，这在实际应用场景中并不总是可用。为了让深度图 clustering 算法不受预定分组数量的限制，我们提出了一种新的深度图 clustering 方法，称为奖励图 clustering（RGC）。在我们的提议方法中，帧定分组数量和无监督表示学习被统一到一个奖励学习机制中。具体来说，首先通过对比预文本任务来学习描述性节点表示。然后，为了准确地捕捉图中节点和分组的相互关系，在每个状态下考虑节点和分组状态。接着，在每个状态下，通过质量网络评估不同分组数量的质量，并执行贪婪的动作来确定分组数量。为了进行反馈动作，我们提出了一种集成吸引函数，以提高同分组内节点之间的凝聚力和不同分组内节点之间的分离力。我们的实验表明，RGC 方法可以具有高效率和高效性。RGC 的源代码可以在 GitHub 上获取（https://github.com/yueliu1999/RGC），并且我们在 GitHub 上分享了一个包含深度图 clustering 相关论文、代码和数据集的集成（https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering）。
</details></li>
</ul>
<hr>
<h2 id="Approximate-and-Weighted-Data-Reconstruction-Attack-in-Federated-Learning"><a href="#Approximate-and-Weighted-Data-Reconstruction-Attack-in-Federated-Learning" class="headerlink" title="Approximate and Weighted Data Reconstruction Attack in Federated Learning"></a>Approximate and Weighted Data Reconstruction Attack in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06822">http://arxiv.org/abs/2308.06822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Wang, Yongcun Song, Enrique Zuazua</li>
<li>for: 该研究旨在攻击基于 horizontal Federated Averaging（FedAvg）的分布式学习（Federated Learning，FL）场景，以便在无需分享客户端数据的情况下，攻击者可以recover客户端的训练数据。</li>
<li>methods: 该研究提出了一种 interpolation-based approximation 方法，通过生成客户端的本地训练过程中的中间模型更新，使得攻击 FedAvg 场景变得可行。此外，该研究还提出了一种层wise weighted loss function，用于提高数据重建的质量。</li>
<li>results: 实验结果表明，该研究的提出的 Approximate and Weighted Attack（AWA）方法在不同的评价指标中具有显著的改善，特别是在图像数据重建中。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed learning paradigm that enables multiple clients to collaborate on building a machine learning model without sharing their private data. Although FL is considered privacy-preserved by design, recent data reconstruction attacks demonstrate that an attacker can recover clients' training data based on the parameters shared in FL. However, most existing methods fail to attack the most widely used horizontal Federated Averaging (FedAvg) scenario, where clients share model parameters after multiple local training steps. To tackle this issue, we propose an interpolation-based approximation method, which makes attacking FedAvg scenarios feasible by generating the intermediate model updates of the clients' local training processes. Then, we design a layer-wise weighted loss function to improve the data quality of reconstruction. We assign different weights to model updates in different layers concerning the neural network structure, with the weights tuned by Bayesian optimization. Finally, experimental results validate the superiority of our proposed approximate and weighted attack (AWA) method over the other state-of-the-art methods, as demonstrated by the substantial improvement in different evaluation metrics for image data reconstructions.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）是一种分布式学习模式，允许多个客户端共同构建一个机器学习模型，而无需分享他们的私人数据。虽然FL被视为隐私保护的设计，但是最近的数据重建攻击表明，攻击者可以根据在FL中共享的参数恢复客户端的训练数据。然而，现有的方法无法攻击最常用的水平联合平均（FedAvg）场景，在这里，客户端在多个本地训练步骤后共享模型参数。为解决这个问题，我们提议一种 interpolation-based 近似方法，使得在客户端的本地训练过程中生成Intermediate模型更新。然后，我们设计了层weise weighted 损失函数，以提高数据重建的质量。我们对模型更新在不同层中分配不同的权重，并通过抽样优化得到最佳权重。最后，我们的提出的近似和权重攻击（AWA）方法在不同评价指标中具有显著的提高，与其他当前state-of-the-art方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="SoK-Realistic-Adversarial-Attacks-and-Defenses-for-Intelligent-Network-Intrusion-Detection"><a href="#SoK-Realistic-Adversarial-Attacks-and-Defenses-for-Intelligent-Network-Intrusion-Detection" class="headerlink" title="SoK: Realistic Adversarial Attacks and Defenses for Intelligent Network Intrusion Detection"></a>SoK: Realistic Adversarial Attacks and Defenses for Intelligent Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06819">http://arxiv.org/abs/2308.06819</a></li>
<li>repo_url: None</li>
<li>paper_authors: João Vitorino, Isabel Praça, Eva Maia</li>
<li>for: 本研究旨在汇总当前领域中 adversarial 学习的应用情况，以及对 realistic 的攻击示例的生成方法。</li>
<li>methods: 本研究使用了多种 adversarial 攻击方法，包括黑盒测试、灰盒测试、探测隐蔽攻击等。</li>
<li>results: 本研究通过对多种 adversarial 攻击方法进行分析和评估，提出了一些 open challenges 和 future research directions，以及一些实际应用场景的推荐。<details>
<summary>Abstract</summary>
Machine Learning (ML) can be incredibly valuable to automate anomaly detection and cyber-attack classification, improving the way that Network Intrusion Detection (NID) is performed. However, despite the benefits of ML models, they are highly susceptible to adversarial cyber-attack examples specifically crafted to exploit them. A wide range of adversarial attacks have been created and researchers have worked on various defense strategies to safeguard ML models, but most were not intended for the specific constraints of a communication network and its communication protocols, so they may lead to unrealistic examples in the NID domain. This Systematization of Knowledge (SoK) consolidates and summarizes the state-of-the-art adversarial learning approaches that can generate realistic examples and could be used in real ML development and deployment scenarios with real network traffic flows. This SoK also describes the open challenges regarding the use of adversarial ML in the NID domain, defines the fundamental properties that are required for an adversarial example to be realistic, and provides guidelines for researchers to ensure that their future experiments are adequate for a real communication network.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:机器学习（ML）可以极其有价值地自动检测异常和识别攻击，提高网络入侵检测（NID）的方式。然而，尽管ML模型具有各种优点，但它们又受到特制的攻击示例的威胁。有许多攻击方法被创造出来，研究人员也为了保护ML模型而努力了很多，但大多数这些方法不适用于通信网络和其通信协议的特定限制，因此可能导致NID领域中的不真实的示例。这个系统化知识（SoK）总结了当前领域中最佳的抗击学习方法，这些方法可以生成真实的示例，并可以在实际的ML开发和部署场景中使用实际的网络流量。这个SoK还描述了使用抗击学习在NID领域的开放挑战，定义了真实示例所需的基本属性，并提供了指导方针，以便未来研究人员可以在真正的通信网络中进行合适的实验。
</details></li>
</ul>
<hr>
<h2 id="SAILOR-Structural-Augmentation-Based-Tail-Node-Representation-Learning"><a href="#SAILOR-Structural-Augmentation-Based-Tail-Node-Representation-Learning" class="headerlink" title="SAILOR: Structural Augmentation Based Tail Node Representation Learning"></a>SAILOR: Structural Augmentation Based Tail Node Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06801">http://arxiv.org/abs/2308.06801</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jie-re/sailor">https://github.com/jie-re/sailor</a></li>
<li>paper_authors: Jie Liao, Jintang Li, Liang Chen, Bingzhe Wu, Yatao Bian, Zibin Zheng<br>for: 本文是为了提高图像中的尾节点表示性而提出的一种框架，即SAILOR，该框架可以同时学习图像的结构增强和尾节点表示提取更多的信息。methods: 本文使用的方法包括message propagation和structural augmentation，这两种方法可以帮助提高尾节点的表示性。results: 实验结果表明，SAILOR可以显著提高尾节点的表示性，并超越现有的基elines。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved state-of-the-art performance in representation learning for graphs recently. However, the effectiveness of GNNs, which capitalize on the key operation of message propagation, highly depends on the quality of the topology structure. Most of the graphs in real-world scenarios follow a long-tailed distribution on their node degrees, that is, a vast majority of the nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes since they lack structural information. In the pursuit of promoting the expressiveness of GNNs for tail nodes, we explore how the deficiency of structural information deteriorates the performance of tail nodes and propose a general Structural Augmentation based taIL nOde Representation learning framework, dubbed as SAILOR, which can jointly learn to augment the graph structure and extract more informative representations for tail nodes. Extensive experiments on public benchmark datasets demonstrate that SAILOR can significantly improve the tail node representations and outperform the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）在近期 representation learning 中取得了状态理想的表现。然而，GNNs 的效果，它们基于消息传递操作，具体取决于图结构质量。大多数实际场景中的图都遵循一个长尾分布，即图中的大多数节点是tail节点，只有几个连接的边。GNNs 对tail节点进行表示不够，因为它们缺乏结构信息。为了提高 GNNs 对tail节点的表示能力，我们研究了tail节点表示力下降的原因，并提出了一种通用的结构扩充基于 taIL nOde Representation 学习框架，名为 SAILOR，可以同时学习扩充图结构并提取更有用的表示信息。我们对公共 benchmark 数据集进行了广泛的实验，结果显示，SAILOR 可以明显提高tail节点表示能力，并超过当前的基elines。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.LG_2023_08_14/" data-id="clohum99800mvpj882qh26s3r" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/eess.IV_2023_08_14/" class="article-date">
  <time datetime="2023-08-14T09:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/eess.IV_2023_08_14/">eess.IV - 2023-08-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data"><a href="#Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data" class="headerlink" title="Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data"></a>Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07214">http://arxiv.org/abs/2308.07214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiranjeewee Prasad Koirala, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</li>
<li>for: 这篇论文旨在探讨使用深度学习技术来提高脑肿瘤分割精度，尤其是在 SUB-SAHARAN AFRICA 地区患者群体中。</li>
<li>methods: 这篇论文使用了多种多Modal MRI 数据，并提出了一种ensemble方法，包括eleven个不同的变体，基于三种核心架构：UNet3D、ONet3D 和 SphereNet3D，以及修改的损失函数。</li>
<li>results: 研究发现， ensemble方法可以在不同的核心架构和修改后的损失函数下提高评估指标，例如 Dice 分数为0.82、0.82和0.87分别用于提高肿瘤、肿瘤核心和全肿瘤标签。<details>
<summary>Abstract</summary>
Brain tumors, particularly glioblastoma, continue to challenge medical diagnostics and treatments globally. This paper explores the application of deep learning to multi-modality magnetic resonance imaging (MRI) data for enhanced brain tumor segmentation precision in the Sub-Saharan Africa patient population. We introduce an ensemble method that comprises eleven unique variations based on three core architectures: UNet3D, ONet3D, SphereNet3D and modified loss functions. The study emphasizes the need for both age- and population-based segmentation models, to fully account for the complexities in the brain. Our findings reveal that the ensemble approach, combining different architectures, outperforms single models, leading to improved evaluation metrics. Specifically, the results exhibit Dice scores of 0.82, 0.82, and 0.87 for enhancing tumor, tumor core, and whole tumor labels respectively. These results underline the potential of tailored deep learning techniques in precisely segmenting brain tumors and lay groundwork for future work to fine-tune models and assess performance across different brain regions.
</details>
<details>
<summary>摘要</summary>
脑肿、特别是 glioblastoma，在全球医学诊断和治疗中仍然存在挑战。这篇论文探讨了深度学习在多modal磁共振成像（MRI）数据上的应用，以提高脑肿分 segmentation精度在非洲南部患者人口中。我们介绍了一个集成方法，包括eleven个独特变种，基于三种核心架构：UNet3D、ONet3D 和 SphereNet3D，以及修改的损失函数。研究强调了需要Age和Population基于的分 segmentation模型，以全面考虑脑部的复杂性。我们的发现表明，ensemble方法，将不同架构相结合，可以提高评价指标。具体来说，结果表明，整合ensemble方法可以提高评价指标，达到0.82、0.82和0.87的Dice分数。这些结果证明了深度学习技术在精确地分 segmentation脑肿中的潜力，并为未来细化模型和评估不同脑部区域的性能奠定基础。
</details></li>
</ul>
<hr>
<h2 id="SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation"><a href="#SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation" class="headerlink" title="SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation"></a>SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07156">http://arxiv.org/abs/2308.07156</a></li>
<li>repo_url: None</li>
<li>paper_authors: An Wang, Mobarakol Islam, Mengya Xu, Yang Zhang, Hongliang Ren</li>
<li>for: 这个论文主要研究的是semantic segmentation模型Segment Anything Model（SAM）在Robotic Surgery领域的Robustness和零shot泛化能力。</li>
<li>methods: 这个论文使用了SAM模型，以及不同的提示方法，包括bounding box和点提示。</li>
<li>results: 研究发现，SAM在bounding box提示下表现出remarkable的零shot泛化能力，但在点提示和无提示情况下表现不佳，特别是在复杂的外科手术场景下。此外，SAM也存在对数据损害的敏感性和难以在不同情况下维持高性能的问题。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) serves as a fundamental model for semantic segmentation and demonstrates remarkable generalization capabilities across a wide range of downstream scenarios. In this empirical study, we examine SAM's robustness and zero-shot generalizability in the field of robotic surgery. We comprehensively explore different scenarios, including prompted and unprompted situations, bounding box and points-based prompt approaches, as well as the ability to generalize under corruptions and perturbations at five severity levels. Additionally, we compare the performance of SAM with state-of-the-art supervised models. We conduct all the experiments with two well-known robotic instrument segmentation datasets from MICCAI EndoVis 2017 and 2018 challenges. Our extensive evaluation results reveal that although SAM shows remarkable zero-shot generalization ability with bounding box prompts, it struggles to segment the whole instrument with point-based prompts and unprompted settings. Furthermore, our qualitative figures demonstrate that the model either failed to predict certain parts of the instrument mask (e.g., jaws, wrist) or predicted parts of the instrument as wrong classes in the scenario of overlapping instruments within the same bounding box or with the point-based prompt. In fact, SAM struggles to identify instruments in complex surgical scenarios characterized by the presence of blood, reflection, blur, and shade. Additionally, SAM is insufficiently robust to maintain high performance when subjected to various forms of data corruption. We also attempt to fine-tune SAM using Low-rank Adaptation (LoRA) and propose SurgicalSAM, which shows the capability in class-wise mask prediction without prompt. Therefore, we can argue that, without further domain-specific fine-tuning, SAM is not ready for downstream surgical tasks.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM)  acted as a fundamental model for semantic segmentation and demonstrated remarkable generalization capabilities across a wide range of downstream scenarios. In this empirical study, we examined SAM's robustness and zero-shot generalizability in the field of robotic surgery. We comprehensively explored different scenarios, including prompted and unprompted situations, bounding box and points-based prompt approaches, as well as the ability to generalize under corruptions and perturbations at five severity levels. Additionally, we compared the performance of SAM with state-of-the-art supervised models. We conducted all the experiments with two well-known robotic instrument segmentation datasets from MICCAI EndoVis 2017 and 2018 challenges. Our extensive evaluation results showed that although SAM showed remarkable zero-shot generalization ability with bounding box prompts, it struggled to segment the whole instrument with point-based prompts and unprompted settings. Furthermore, our qualitative figures demonstrated that the model either failed to predict certain parts of the instrument mask (e.g., jaws, wrist) or predicted parts of the instrument as wrong classes in the scenario of overlapping instruments within the same bounding box or with the point-based prompt. In fact, SAM struggled to identify instruments in complex surgical scenarios characterized by the presence of blood, reflection, blur, and shade. Additionally, SAM was insufficiently robust to maintain high performance when subjected to various forms of data corruption. We also attempted to fine-tune SAM using Low-rank Adaptation (LoRA) and proposed SurgicalSAM, which showed the capability in class-wise mask prediction without prompt. Therefore, we can argue that, without further domain-specific fine-tuning, SAM is not ready for downstream surgical tasks.
</details></li>
</ul>
<hr>
<h2 id="FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving"><a href="#FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving" class="headerlink" title="FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving"></a>FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07104">http://arxiv.org/abs/2308.07104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhonghuayi/focusflow_official">https://github.com/zhonghuayi/focusflow_official</a></li>
<li>paper_authors: Zhonghua Yi, Hao Shi, Kailun Yang, Qi Jiang, Yaozu Ye, Ze Wang, Kaiwei Wang<br>for:The paper is focused on improving the performance of optical flow estimation in key-point-critical scenarios for autonomous driving applications.methods:The proposed method, called FocusFlow, uses a points-based modeling approach that explicitly learns key-point-related priors. It also introduces a new loss function called Conditional Point Control Loss (CPCL) and a Condition Control Encoder (CCE) to improve the performance of the model.results:The proposed FocusFlow framework shows outstanding performance with up to +44.5% precision improvement on various key points such as ORB, SIFT, and even learning-based SiLK, and exceptional scalability for most existing data-driven optical flow methods. It also yields competitive or superior performances rivaling the original models on the whole frame.<details>
<summary>Abstract</summary>
Key-point-based scene understanding is fundamental for autonomous driving applications. At the same time, optical flow plays an important role in many vision tasks. However, due to the implicit bias of equal attention on all points, classic data-driven optical flow estimation methods yield less satisfactory performance on key points, limiting their implementations in key-point-critical safety-relevant scenarios. To address these issues, we introduce a points-based modeling method that requires the model to learn key-point-related priors explicitly. Based on the modeling method, we present FocusFlow, a framework consisting of 1) a mix loss function combined with a classic photometric loss function and our proposed Conditional Point Control Loss (CPCL) function for diverse point-wise supervision; 2) a conditioned controlling model which substitutes the conventional feature encoder by our proposed Condition Control Encoder (CCE). CCE incorporates a Frame Feature Encoder (FFE) that extracts features from frames, a Condition Feature Encoder (CFE) that learns to control the feature extraction behavior of FFE from input masks containing information of key points, and fusion modules that transfer the controlling information between FFE and CFE. Our FocusFlow framework shows outstanding performance with up to +44.5% precision improvement on various key points such as ORB, SIFT, and even learning-based SiLK, along with exceptional scalability for most existing data-driven optical flow methods like PWC-Net, RAFT, and FlowFormer. Notably, FocusFlow yields competitive or superior performances rivaling the original models on the whole frame. The source code will be available at https://github.com/ZhonghuaYi/FocusFlow_official.
</details>
<details>
<summary>摘要</summary>
基点 centered scene understanding 是自动驾驶应用的基础。同时，光流扮演了许多视觉任务中重要的角色。然而，由于约定性偏袋所有点都具有相同的注意力， класси的数据驱动光流估计方法在关键点上的表现不如预期，这限制了它们在关键点关键的安全相关场景中的实现。为解决这些问题，我们介绍了一种点 cloud 模型化方法，要求模型直接学习关键点相关的前置知识。基于该方法，我们提出了 FOCUSFLOW 框架，包括以下两个部分：1. 一种 mix 损失函数，与 класси的光学损失函数和我们所提出的 Conditional Point Control Loss (CPCL) 函数进行多样化点wise 超vision;2. 一种受控制的模型，替换了传统的特征编码器，我们提出的 Condition Control Encoder (CCE)。CCE 包括 Frame Feature Encoder (FFE)、Condition Feature Encoder (CFE) 和 fusion module，CFE 通过输入Mask 中关键点信息来学习控制 FFE 提取特征的行为，并将控制信息传递给 FFE。我们的 FOCUSFLOW 框架在多个关键点上（包括 ORB、SIFT 和学习基于 SiLK 的）表现出色，同时具有出色的扩展性，可以与大多数现有的数据驱动光流估计方法（如 PWC-Net、RAFT 和 FlowFormer）进行比较。特别是，FOCUSFLOW 在整个帧上的表现与原始模型相当或更好。源代码将在 GitHub 上发布，详情请参考 <https://github.com/ZhonghuaYi/FocusFlow_official>。
</details></li>
</ul>
<hr>
<h2 id="When-Deep-Learning-Meets-Multi-Task-Learning-in-SAR-ATR-Simultaneous-Target-Recognition-and-Segmentation"><a href="#When-Deep-Learning-Meets-Multi-Task-Learning-in-SAR-ATR-Simultaneous-Target-Recognition-and-Segmentation" class="headerlink" title="When Deep Learning Meets Multi-Task Learning in SAR ATR: Simultaneous Target Recognition and Segmentation"></a>When Deep Learning Meets Multi-Task Learning in SAR ATR: Simultaneous Target Recognition and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07093">http://arxiv.org/abs/2308.07093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Zhiyong Wang, Yulin Huang, Junjie Wu, Haiguang Yang, Jianyu Yang</li>
<li>for: 本文旨在提出一种基于多任务学习的Synthetic Aperture Radar（SAR）自动目标识别（ATR）方法，以便同时提取多种目标特征。</li>
<li>methods: 本文提出了一种基于深度学习理论的多任务学习框架，包括两个主要结构：编码器和解码器。编码器用于提取不同尺度的图像特征，而解码器则是一种任务特有的结构，它使用这些提取的特征进行适应性和优化性地进行识别和分割。</li>
<li>results: 根据Moving and Stationary Target Acquisition and Recognition（MSTAR） dataset的实验结果表明，提出的方法在识别和分割方面具有优秀的性能。<details>
<summary>Abstract</summary>
With the recent advances of deep learning, automatic target recognition (ATR) of synthetic aperture radar (SAR) has achieved superior performance. By not being limited to the target category, the SAR ATR system could benefit from the simultaneous extraction of multifarious target attributes. In this paper, we propose a new multi-task learning approach for SAR ATR, which could obtain the accurate category and precise shape of the targets simultaneously. By introducing deep learning theory into multi-task learning, we first propose a novel multi-task deep learning framework with two main structures: encoder and decoder. The encoder is constructed to extract sufficient image features in different scales for the decoder, while the decoder is a tasks-specific structure which employs these extracted features adaptively and optimally to meet the different feature demands of the recognition and segmentation. Therefore, the proposed framework has the ability to achieve superior recognition and segmentation performance. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, experimental results show the superiority of the proposed framework in terms of recognition and segmentation.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)随着深度学习的进步，激光探测器自动目标识别（ATR）已经达到了出色的性能。由于不受目标类别的限制，SAR ATR系统可以从同时提取多种目标属性中受益。在这篇论文中，我们提出了一种新的多任务学习方法 для SAR ATR，可以同时获得目标的准确分类和精确的形状信息。通过将深度学习理论引入多任务学习中，我们首先提出了一种新的多任务深度学习框架，包括编码器和解码器两部分。编码器用于抽取不同缩放级别的图像特征，以便解码器使用这些抽取的特征进行适应性和优化的处理。因此，我们提出的框架具有提高认知和分割性能的能力。基于MSTAR数据集，我们的实验结果表明，我们的方法在认知和分割方面具有优越性。
</details></li>
</ul>
<hr>
<h2 id="Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks"><a href="#Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks" class="headerlink" title="Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks"></a>Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07003">http://arxiv.org/abs/2308.07003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Fisch, Stefan Zumdick, Carlotta Barkhau, Daniel Emden, Jan Ernsting, Ramona Leenings, Kelvin Sarink, Nils R. Winter, Benjamin Risse, Udo Dannlowski, Tim Hahn</li>
<li>for: 这个论文的目的是提出一种高精度、快速的脑部EXTRACTION方法，用于多种神经成像预处理管道中。</li>
<li>methods: 该方法使用了现代的深度学习方法，包括LinkNet网络，在两个预测过程中进行两个阶段预测，从而提高了 segmentation 性能。</li>
<li>results: 该方法在跨验证中得到了 novel state-of-the-art 性能， median Dice 分数 (DSC) 为 99.0%，超过当前状态的艺术模型 (DSC &#x3D; 97.8% 和 DSC &#x3D; 97.9%)。此外，该方法能够更好地抗抑异常值，Dice 分数大于 96.9%  для所有样本。最后，该模型可以加速脑部EXTRACTION的速度，比现有方法快约 10 倍，可以在低级别硬件上处理一个图像只需 ~2 秒。<details>
<summary>Abstract</summary>
Brain extraction in magnetic resonance imaging (MRI) data is an important segmentation step in many neuroimaging preprocessing pipelines. Image segmentation is one of the research fields in which deep learning had the biggest impact in recent years enabling high precision segmentation with minimal compute. Consequently, traditional brain extraction methods are now being replaced by deep learning-based methods. Here, we used a unique dataset comprising 568 T1-weighted (T1w) MR images from 191 different studies in combination with cutting edge deep learning methods to build a fast, high-precision brain extraction tool called deepbet. deepbet uses LinkNet, a modern UNet architecture, in a two stage prediction process. This increases its segmentation performance, setting a novel state-of-the-art performance during cross-validation with a median Dice score (DSC) of 99.0% on unseen datasets, outperforming current state of the art models (DSC = 97.8% and DSC = 97.9%). While current methods are more sensitive to outliers, resulting in Dice scores as low as 76.5%, deepbet manages to achieve a Dice score of > 96.9% for all samples. Finally, our model accelerates brain extraction by a factor of ~10 compared to current methods, enabling the processing of one image in ~2 seconds on low level hardware.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging（MRI）数据中的脑部提取是许多神经成像预处理管道中重要的分 segmentation步骤。图像分 segmentation是深度学习过去几年内最大的影响领域之一，使得传统的脑部提取方法被取代了深度学习基于方法。我们使用了568张T1-weighted（T1w）MR图像从191个不同的研究中的独特数据集，并使用最新的深度学习方法来构建一个高速、高精度的脑部提取工具called deepbet。deepbet使用了LinkNet，一种现代的UNet架构，在两个预测过程中进行两个阶段预测。这使得它的分 segmentation性能得到了提升，在批处理中 median Dice score（DSC）达到了99.0%，超越当前状态的艺术模型（DSC = 97.8%和DSC = 97.9%）。而当前方法更敏感于异常值，导致Dice score只有76.5%，而deepbet则可以达到> 96.9%的Dice score。最后，我们的模型可以将脑部提取加速了约10倍，使得一张图像在低级别硬件上只需2秒钟左右。
</details></li>
</ul>
<hr>
<h2 id="How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation"><a href="#How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation" class="headerlink" title="How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation"></a>How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06964">http://arxiv.org/abs/2308.06964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parinaz Roshanzamir, Hassan Rivaz, Joshua Ahn, Hamza Mirza, Neda Naghdi, Meagan Anstruther, Michele C. Battié, Maryse Fortin, Yiming Xiao<br>for: This paper aims to explore the relationship between inter-rater variability and uncertainty in deep learning models for medical image segmentation, and to compare the performance of different DL models and label fusion strategies.methods: The authors use test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to measure aleatoric and epistemic uncertainties, and compare the performance of UNet and TransUNet with two label fusion strategies.results: The study reveals the interplay between inter-rater variability and uncertainties, and shows that the choice of label fusion strategies and DL models can affect the performance and uncertainty of the resulting algorithms.<details>
<summary>Abstract</summary>
Recent developments in deep learning (DL) techniques have led to great performance improvement in medical image segmentation tasks, especially with the latest Transformer model and its variants. While labels from fusing multi-rater manual segmentations are often employed as ideal ground truths in DL model training, inter-rater variability due to factors such as training bias, image noise, and extreme anatomical variability can still affect the performance and uncertainty of the resulting algorithms. Knowledge regarding how inter-rater variability affects the reliability of the resulting DL algorithms, a key element in clinical deployment, can help inform better training data construction and DL models, but has not been explored extensively. In this paper, we measure aleatoric and epistemic uncertainties using test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to explore their relationship with inter-rater variability. Furthermore, we compare UNet and TransUNet to study the impacts of Transformers on model uncertainty with two label fusion strategies. We conduct a case study using multi-class paraspinal muscle segmentation from T2w MRIs. Our study reveals the interplay between inter-rater variability and uncertainties, affected by choices of label fusion strategies and DL models.
</details>
<details>
<summary>摘要</summary>
In this paper, we investigate the relationship between inter-rater variability and the reliability of DL algorithms by measuring aleatoric and epistemic uncertainties using test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble. We also compare UNet and TransUNet to study the impact of Transformers on model uncertainty with two label fusion strategies. Our case study focuses on multi-class paraspinal muscle segmentation from T2w MRIs.Our findings reveal an interplay between inter-rater variability and uncertainties, which are influenced by the choice of label fusion strategies and DL models. By understanding these relationships, we can better construct training data and develop more reliable DL models for clinical applications.
</details></li>
</ul>
<hr>
<h2 id="Robustness-Stress-Testing-in-Medical-Image-Classification"><a href="#Robustness-Stress-Testing-in-Medical-Image-Classification" class="headerlink" title="Robustness Stress Testing in Medical Image Classification"></a>Robustness Stress Testing in Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06889">http://arxiv.org/abs/2308.06889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mobarakol/robustness_stress_testing">https://github.com/mobarakol/robustness_stress_testing</a></li>
<li>paper_authors: Mobarakol Islam, Zeju Li, Ben Glocker<br>for:  This paper aims to assess the robustness and equity of disease detection models using progressive stress testing.methods: The authors use five different bidirectional and unidirectional image perturbations with six different severity levels to test the models’ robustness.results: The authors find that some models may yield more robust and equitable performance than others, and pretraining characteristics play an important role in downstream robustness.<details>
<summary>Abstract</summary>
Deep neural networks have shown impressive performance for image-based disease detection. Performance is commonly evaluated through clinical validation on independent test sets to demonstrate clinically acceptable accuracy. Reporting good performance metrics on test sets, however, is not always a sufficient indication of the generalizability and robustness of an algorithm. In particular, when the test data is drawn from the same distribution as the training data, the iid test set performance can be an unreliable estimate of the accuracy on new data. In this paper, we employ stress testing to assess model robustness and subgroup performance disparities in disease detection models. We design progressive stress testing using five different bidirectional and unidirectional image perturbations with six different severity levels. As a use case, we apply stress tests to measure the robustness of disease detection models for chest X-ray and skin lesion images, and demonstrate the importance of studying class and domain-specific model behaviour. Our experiments indicate that some models may yield more robust and equitable performance than others. We also find that pretraining characteristics play an important role in downstream robustness. We conclude that progressive stress testing is a viable and important tool and should become standard practice in the clinical validation of image-based disease detection models.
</details>
<details>
<summary>摘要</summary>
深度神经网络在图像基于疾病检测方面表现出色。性能通常通过临床验证方法进行评估，以证明在临床上达到可接受的准确率。然而，只是在测试数据集上报告好的性能指标不一定是一个可靠的指示器，特别是当测试数据集来自同一个分布为训练数据集时，iid测试集性能可能是一个不可靠的准确率估计。在这篇论文中，我们使用压力测试来评估模型的可靠性和 subgroup 性能差异。我们设计了进行逆向和直向的图像扰动，并使用六个不同的严重程度。作为一个使用场景，我们将压力测试应用于肺X射线和皮肤损伤图像中的疾病检测模型，并证明了研究类和领域特定的模型行为的重要性。我们的实验表示某些模型可能具有更高的可靠性和公平性。我们还发现预训练特征对下游可靠性具有重要作用。我们结论，进行进程式压力测试是一种可靠的和重要的工具，应成为临床验证图像基于疾病检测模型的标准实践。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/eess.IV_2023_08_14/" data-id="clohum9fa013kpj88caf79jd2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/cs.CV_2023_08_13/" class="article-date">
  <time datetime="2023-08-13T13:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/13/cs.CV_2023_08_13/">cs.CV - 2023-08-13</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Modified-Topological-Image-Preprocessing-for-Skin-Lesion-Classifications"><a href="#Modified-Topological-Image-Preprocessing-for-Skin-Lesion-Classifications" class="headerlink" title="Modified Topological Image Preprocessing for Skin Lesion Classifications"></a>Modified Topological Image Preprocessing for Skin Lesion Classifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06796">http://arxiv.org/abs/2308.06796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Cheng, Rebekah Leamons, Ahmad Al Shami</li>
<li>for: 这个论文是为了提出一种修改了拓扑数据分析模型，用于精确地处理皮肤图像的预处理和优化。</li>
<li>methods: 该模型使用了修改后的拓扑数据分析方法，并在使用深度卷积神经网络和视transformer模型进行训练。</li>
<li>results: 实验结果表明，使用修改后的拓扑数据分析方法可以在皮肤图像预处理中提高性能。<details>
<summary>Abstract</summary>
This paper proposes a modified Topological Data Analysis model for skin images preprocessing and enhancements. The skin lesion dataset HAM10000 used with the intention of identifying the important objects in relevant regions of the images. In order to evaluate both the original dataset and the preprocessed dataset, Deep Convolutional Neural Network and Vision Transformer models were utilized to train both models. After training, the experimental results demonstrate that the images preprocessed using the Modified Topological Data Analysis consistently perform better.
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种修改后的拓扑数据分析模型，用于皮肤图像的预处理和改进。使用了悬峰10000个皮肤病变数据集，以便在相关区域中标识重要对象。为了评估原始数据集和预处理后的数据集，使用了深度卷积神经网络和视 traducción transformer 模型进行训练。经训练后，实验结果表明，使用修改后的拓扑数据分析后的图像预处理 consistently perform better。Note: "拓扑数据分析" (topological data analysis) is a bit of a mouthful in Chinese, so I've shortened it to "拓扑分析" (topological analysis) in the translation.
</details></li>
</ul>
<hr>
<h2 id="PV-SSD-A-Projection-and-Voxel-based-Double-Branch-Single-Stage-3D-Object-Detector"><a href="#PV-SSD-A-Projection-and-Voxel-based-Double-Branch-Single-Stage-3D-Object-Detector" class="headerlink" title="PV-SSD: A Projection and Voxel-based Double Branch Single-Stage 3D Object Detector"></a>PV-SSD: A Projection and Voxel-based Double Branch Single-Stage 3D Object Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06791">http://arxiv.org/abs/2308.06791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongxin Shao, Aihong Tan, Zhetao Sun, Enhui Zheng, Tianhong Yan</li>
<li>for: 提高LIDAR数据的3D对象检测和分类精度，以便自动驾驶。</li>
<li>methods: 提出基于精度抽象和投影叠加的double branch特征提取方法（PV-SSD），以减少投影过程中的信息损失。</li>
<li>results: 与之前的工作相比，本方法实现了良好的性能，并且提出了多个贡献，包括：1）基于精度抽象的粒子特征提取方法；2）基于重要性抽象的特征点抽取方法；3）基于SSFA模块的MSSFA模块。<details>
<summary>Abstract</summary>
LIDAR-based 3D object detection and classification is crucial for autonomous driving. However, inference in real-time from extremely sparse 3D data poses a formidable challenge. To address this issue, a common approach is to project point clouds onto a bird's-eye or perspective view, effectively converting them into an image-like data format. However, this excessive compression of point cloud data often leads to the loss of information. This paper proposes a 3D object detector based on voxel and projection double branch feature extraction (PV-SSD) to address the problem of information loss. We add voxel features input containing rich local semantic information, which is fully fused with the projected features in the feature extraction stage to reduce the local information loss caused by projection. A good performance is achieved compared to the previous work. In addition, this paper makes the following contributions: 1) a voxel feature extraction method with variable receptive fields is proposed; 2) a feature point sampling method by weight sampling is used to filter out the feature points that are more conducive to the detection task; 3) the MSSFA module is proposed based on the SSFA module. To verify the effectiveness of our method, we designed comparison experiments.
</details>
<details>
<summary>摘要</summary>
LIDAR-based 3D对象检测和分类是自动驾驶中关键。然而，在实时推理从极其稀疏3D数据中却成为一大挑战。为解决这个问题，一种常见的方法是将点云 proyect onto a bird's-eye or perspective view，实际上将其转换成图像类数据格式。然而，这种压缩点云数据的方法经常会导致信息损失。这篇论文提出了基于voxel和投影双分支特征提取（PV-SSD）的3D对象检测器，以解决信息损失问题。我们添加了包含丰富本地语义信息的voxel特征输入，并将其完全与投影特征在特征提取阶段进行了完全融合，以降低由投影所导致的本地信息损失。我们实现了与之前的工作相比的良好性能。此外，本文还做出了以下贡献：1）基于voxel特征提取方法中的可变感知场被提出；2）通过重点抽样来筛选更适合检测任务的特征点；3）基于SSFA模块的MSSFA模块被提出。为证明我们的方法的有效性，我们设计了对比实验。
</details></li>
</ul>
<hr>
<h2 id="RMP-Loss-Regularizing-Membrane-Potential-Distribution-for-Spiking-Neural-Networks"><a href="#RMP-Loss-Regularizing-Membrane-Potential-Distribution-for-Spiking-Neural-Networks" class="headerlink" title="RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks"></a>RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06787">http://arxiv.org/abs/2308.06787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Guo, Xiaode Liu, Yuanpei Chen, Liwen Zhang, Weihang Peng, Yuhan Zhang, Xuhui Huang, Zhe Ma</li>
<li>for: 这篇论文是为了解决神经网络中的数字化错误问题，并提出一个简单且直观的训练方法来减少这种错误的影响。</li>
<li>methods: 本论文使用的方法是一种叫做Regularizing membrane potential loss (RMP-Loss)的调整项，可以将数字化错误的影响降到最小化。这个方法实现非常简单，并且可以轻松地训练神经网络。</li>
<li>results: 本论文的实验结果显示，使用RMP-Loss训练神经网络可以对数字化错误问题做出有效的降低，并且可以与其他已知的方法相比，在不同的网络架构和数据集上表现更好。<details>
<summary>Abstract</summary>
Spiking Neural Networks (SNNs) as one of the biology-inspired models have received much attention recently. It can significantly reduce energy consumption since they quantize the real-valued membrane potentials to 0/1 spikes to transmit information thus the multiplications of activations and weights can be replaced by additions when implemented on hardware. However, this quantization mechanism will inevitably introduce quantization error, thus causing catastrophic information loss. To address the quantization error problem, we propose a regularizing membrane potential loss (RMP-Loss) to adjust the distribution which is directly related to quantization error to a range close to the spikes. Our method is extremely simple to implement and straightforward to train an SNN. Furthermore, it is shown to consistently outperform previous state-of-the-art methods over different network architectures and datasets.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）作为生物体系静脉模型，最近受到了非常多的关注。它可以减少能耗，因为它将实际值膜电压转换为0/1气压来传输信息，因此硬件实现中的多Multiplications of activations and weights可以被替换为加法运算。然而，这种归一化机制会不可避免地导致归一化错误，从而导致重大的信息损失。为解决这个问题，我们提议一种调整膜电压损失（RMP-Loss）来调整直接与归一化错误相关的分布，使其落在近距离气压范围内。我们的方法非常简单易于实现，并且可以 straightforwardly 训练一个 SNN。此外，我们的方法在不同的网络架构和数据集上都能够 consistently outperform 前一代的方法。
</details></li>
</ul>
<hr>
<h2 id="Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature"><a href="#Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature" class="headerlink" title="Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature"></a>Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06781">http://arxiv.org/abs/2308.06781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Deo, Haoran Dou, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</li>
<li>for: 该研究旨在提高对脑血管疾病的研究和临床 intervención中对脑血管的理解，通过生成真实的3D脑血管分割图像，包括较少见的脑血管变化。</li>
<li>methods: 该研究使用了一种新的生成模型，基于 conditional latent diffusion model，具有形态和解剖指导，以生成真实的3D脑血管分割图像，包括不同的脑血管变化。</li>
<li>results: 研究结果显示，该模型生成的脑血管分割图像比较真实，与其他生成模型，如 conditional GAN和 conditional VAE，具有更高的视觉准确性，FID分数比best-performing GAN-based model高53%。<details>
<summary>Abstract</summary>
The Circle of Willis (CoW) is the part of cerebral vasculature responsible for delivering blood to the brain. Understanding the diverse anatomical variations and configurations of the CoW is paramount to advance research on cerebrovascular diseases and refine clinical interventions. However, comprehensive investigation of less prevalent CoW variations remains challenging because of the dominance of a few commonly occurring configurations. We propose a novel generative approach utilising a conditional latent diffusion model with shape and anatomical guidance to generate realistic 3D CoW segmentations, including different phenotypical variations. Our conditional latent diffusion model incorporates shape guidance to better preserve vessel continuity and demonstrates superior performance when compared to alternative generative models, including conditional variants of 3D GAN and 3D VAE. We observed that our model generated CoW variants that are more realistic and demonstrate higher visual fidelity than competing approaches with an FID score 53\% better than the best-performing GAN-based model.
</details>
<details>
<summary>摘要</summary>
圆形 Wille （CoW）是脑血管系统中听取血液到脑部的部分。 了解不同的 anatomical 变化和配置的 CoW 对于进展研究脑血管疾病和细化临床 intervención 至关重要。 然而，对于 menos prevalence CoW 变化的全面调查仍然具有挑战，因为一些常见的配置占据了主导地位。 我们提出了一种新的生成方法，使用 conditioned 潜在扩散模型，包括形态和解剖指导来生成真实的 3D CoW 分割，包括不同的现象变化。 我们的 conditioned 潜在扩散模型 包括形态指导，以更好地保持血管连续性，并在与其他生成模型进行比较时，表现出更高的性能。 我们观察到，我们的模型生成的 CoW 变化更加真实，与竞争方法相比，FID 分数高达 53% 更高。
</details></li>
</ul>
<hr>
<h2 id="Neural-Networks-at-a-Fraction-with-Pruned-Quaternions"><a href="#Neural-Networks-at-a-Fraction-with-Pruned-Quaternions" class="headerlink" title="Neural Networks at a Fraction with Pruned Quaternions"></a>Neural Networks at a Fraction with Pruned Quaternions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06780">http://arxiv.org/abs/2308.06780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smlab-niser/quartLT22">https://github.com/smlab-niser/quartLT22</a></li>
<li>paper_authors: Sahel Mohammad Iqbal, Subhankar Mishra</li>
<li>for: 这个研究目的是实现具有限制的计算能力的装置上进行深度学习模型的部署。</li>
<li>methods: 这个研究使用删减技术来删除不必要的参数，以减少训练和推导的资源需求。另外，使用高维度数据嵌入，如复数或四元数，可以降低模型的参数数量，保持精度。</li>
<li>results: 研究发现，在某些架构和任务上，这些复数模型在高给定率下具有更高的准确性，比如在CIFAR-10上的图像分类任务中，使用Conv-4架构，删减后的复数模型比同架构的实际模型提高了超过10%。实验结果显示，在极限的资源环境下，一个简短的复数网络可能比同架构的实际简短模型更适合进行部署。<details>
<summary>Abstract</summary>
Contemporary state-of-the-art neural networks have increasingly large numbers of parameters, which prevents their deployment on devices with limited computational power. Pruning is one technique to remove unnecessary weights and reduce resource requirements for training and inference. In addition, for ML tasks where the input data is multi-dimensional, using higher-dimensional data embeddings such as complex numbers or quaternions has been shown to reduce the parameter count while maintaining accuracy. In this work, we conduct pruning on real and quaternion-valued implementations of different architectures on classification tasks. We find that for some architectures, at very high sparsity levels, quaternion models provide higher accuracies than their real counterparts. For example, at the task of image classification on CIFAR-10 using Conv-4, at $3\%$ of the number of parameters as the original model, the pruned quaternion version outperforms the pruned real by more than $10\%$. Experiments on various network architectures and datasets show that for deployment in extremely resource-constrained environments, a sparse quaternion network might be a better candidate than a real sparse model of similar architecture.
</details>
<details>
<summary>摘要</summary>
现代神经网络在训练和推理过程中的参数数量逐渐增加，这限制了它们在计算机能力有限的设备上进行部署。折射是一种技术来移除不必要的权重，以降低训练和推理所需的资源。此外，在多维输入数据的机器学习任务中，使用高维数域嵌入，如复数或四元数，可以降低参数数量而保持准确性。在这项工作中，我们对实际值和四元数值实现的不同架构进行了折射。我们发现，在某些架构上，在非常高的稀疏程度下，四元数模型可以提供更高的准确性。例如，在使用Conv-4架构进行图像分类任务时，在原始模型的3%的参数数量下，折射后的四元数模型高于原始实际模型的准确性超过10%。在不同的网络架构和数据集上进行了多种实验，我们发现在极限的资源环境下，一个稀疏的四元数网络可能比同类架构的实际稀疏模型更适合进行部署。
</details></li>
</ul>
<hr>
<h2 id="Shrinking-Class-Space-for-Enhanced-Certainty-in-Semi-Supervised-Learning"><a href="#Shrinking-Class-Space-for-Enhanced-Certainty-in-Semi-Supervised-Learning" class="headerlink" title="Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning"></a>Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06777">http://arxiv.org/abs/2308.06777</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LiheYoung/ShrinkMatch">https://github.com/LiheYoung/ShrinkMatch</a></li>
<li>paper_authors: Lihe Yang, Zhen Zhao, Lei Qi, Yu Qiao, Yinghuan Shi, Hengshuang Zhao</li>
<li>for: 本文针对semi-supervised learning中的问题提出了一个新的方法，即ShrinkMatch，以解决过去的 Pseudo label 可能不准确问题。</li>
<li>methods: 本文使用了一个新的方法，即ShrinkMatch，它可以将uncertain samples转换为certain samples，并且运用了一个consistency regularization来实现更好的表现。</li>
<li>results: 本文的实验结果显示了ShrinkMatch方法在各个 benchmark 上的出色表现，并且与其他state-of-the-art方法相比，它的表现更好。<details>
<summary>Abstract</summary>
Semi-supervised learning is attracting blooming attention, due to its success in combining unlabeled data. To mitigate potentially incorrect pseudo labels, recent frameworks mostly set a fixed confidence threshold to discard uncertain samples. This practice ensures high-quality pseudo labels, but incurs a relatively low utilization of the whole unlabeled set. In this work, our key insight is that these uncertain samples can be turned into certain ones, as long as the confusion classes for the top-1 class are detected and removed. Invoked by this, we propose a novel method dubbed ShrinkMatch to learn uncertain samples. For each uncertain sample, it adaptively seeks a shrunk class space, which merely contains the original top-1 class, as well as remaining less likely classes. Since the confusion ones are removed in this space, the re-calculated top-1 confidence can satisfy the pre-defined threshold. We then impose a consistency regularization between a pair of strongly and weakly augmented samples in the shrunk space to strive for discriminative representations. Furthermore, considering the varied reliability among uncertain samples and the gradually improved model during training, we correspondingly design two reweighting principles for our uncertain loss. Our method exhibits impressive performance on widely adopted benchmarks. Code is available at https://github.com/LiheYoung/ShrinkMatch.
</details>
<details>
<summary>摘要</summary>
semi-supervised learning 已经吸引了大量的注意力，因为它可以将无标签数据与标签数据结合起来。为了避免 potential incorrect pseudo labels，现有的框架通常设置固定的信任度reshold来抛弃不确定的样本。这种做法可以保证高质量的 pseudo labels，但是会导致未利用整个无标签集的资源。在这项工作中，我们的关键发现是，这些不确定的样本可以被转化为确定的样本，只要检测并移除混淆类。驱使这个想法，我们提出了一种名为 ShrinkMatch 的新方法。对于每个不确定的样本，它可以适应地寻找一个缩小的类空间，这个类空间只包含原始的 top-1 类，以及剩下的 less likely 类。由于混淆类被移除在这个空间中，重新计算的 top-1 信任度可以满足预定的阈值。然后，我们对一对强制和弱制的扩展样本之间的一致性regularization进行强制。此外，考虑到不确定样本之间的不同可靠性和在训练过程中逐渐提高的模型，我们采用了两种不同的 uncertain loss 重量原则。我们的方法在广泛采用的 benchmark 上表现出色。代码可以在 <https://github.com/LiheYoung/ShrinkMatch> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches"><a href="#Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches" class="headerlink" title="Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches"></a>Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06776">http://arxiv.org/abs/2308.06776</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linxin0/scpgabnet">https://github.com/linxin0/scpgabnet</a></li>
<li>paper_authors: Xin Lin, Chao Ren, Xiao Liu, Jie Huang, Yinjie Lei</li>
<li>for: 提高无标注数据集上的图像去噪性能，不需要大量的训练数据。</li>
<li>methods: 基于生成对抗网络的无supervised方法，通过在filter-guided噪音提取模块中逐次更新denoiser来提高噪音提取性能。</li>
<li>results: 与state-of-the-art无supervised方法相比，本方法实现了更高的噪音提取性能，而且不需要增加训练数据量或计算复杂度。<details>
<summary>Abstract</summary>
Deep learning methods have shown remarkable performance in image denoising, particularly when trained on large-scale paired datasets. However, acquiring such paired datasets for real-world scenarios poses a significant challenge. Although unsupervised approaches based on generative adversarial networks offer a promising solution for denoising without paired datasets, they are difficult in surpassing the performance limitations of conventional GAN-based unsupervised frameworks without significantly modifying existing structures or increasing the computational complexity of denoisers. To address this problem, we propose a SC strategy for multiple denoisers. This strategy can achieve significant performance improvement without increasing the inference complexity of the GAN-based denoising framework. Its basic idea is to iteratively replace the previous less powerful denoiser in the filter-guided noise extraction module with the current powerful denoiser. This process generates better synthetic clean-noisy image pairs, leading to a more powerful denoiser for the next iteration. This baseline ensures the stability and effectiveness of the training network. The experimental results demonstrate the superiority of our method over state-of-the-art unsupervised methods.
</details>
<details>
<summary>摘要</summary>
深度学习方法在图像去噪中表现出了惊人的表现，特别是在大规模对应数据集上训练。然而，在实际场景中获得这些对应数据集是一项巨大的挑战。although generative adversarial networks（GAN）基于的无监督方法可以提供去噪无需对应数据集的解决方案，但是它们在不改变现有结构或提高去噪器的计算复杂度下难以超越传统GAN基于无监督框架的性能限制。为解决这个问题，我们提出了SC策略。这种策略可以在GAN基于的去噪框架中实现显著性能提升，无需提高去噪器的计算复杂度。它的基本思想是在滤波器指导噪音EXTRACTION模块中，逐次替换以前较弱的去噪器，使得当前更强的去噪器生成更好的干涉clean-noisy图像对。这个基准确保了训练网络的稳定性和效果。实验结果表明，我们的方法在无监督去噪方法中表现出了明显的优势。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations"><a href="#A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations" class="headerlink" title="A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations"></a>A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06767">http://arxiv.org/abs/2308.06767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrcheng1066/awesome-pruning">https://github.com/hrcheng1066/awesome-pruning</a></li>
<li>paper_authors: Hongrong Cheng, Miao Zhang, Javen Qinfeng Shi</li>
<li>for: 本研究寻求解决现代深度神经网络的大型模型需要大量计算和存储资源的问题，以便在有限的资源环境下部署和加速推理。</li>
<li>methods: 本文对现有的深度神经网络剪辑技术进行了一个权威的综述，包括1) 通用&#x2F;特定加速、2) 何时剪辑、3) 如何剪辑、4) 剪辑与其他压缩技术的融合。</li>
<li>results: 本文对7对不同的剪辑设置进行了比较分析，并探讨了一些新的话题，如后处理剪辑、不同水平的监督剪辑和应用于不同领域（如对抗攻击），以便更好地了解现有方法的共同点和不同点，并为未来的研究提供基础。<details>
<summary>Abstract</summary>
Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of seven pairs of contrast settings for pruning (e.g., unstructured/structured) and explore emerging topics, including post-training pruning, different levels of supervision for pruning, and broader applications (e.g., adversarial robustness) to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. To facilitate future research, we build a curated collection of datasets, networks, and evaluations on different applications. Finally, we provide some valuable recommendations on selecting pruning methods and prospect promising research directions. We build a repository at https://github.com/hrcheng1066/awesome-pruning.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络，特别是最近的大语言模型，具有庞大的模型大小，需要显著的计算和存储资源。为了在有限资源环境中部署现代模型和加速推理时间，研究人员逐渐探索剪枝技术作为神经网络压缩的流行研究方向。然而，有很多相关研究的报告是不够全面的。为了解决这个问题，在这个调查中，我们提供了一份完整的剪枝技术评论，包括1) 通用/特定速度，2) 何时剪枝，3) 如何剪枝，和4) 剪枝与其他压缩技术的融合。然后，我们进行了7对7的对比分析，探讨不同的设定（例如，无结构/结构），并探索了新的主题，如后期剪枝、不同水平的监督、以及更广泛的应用（例如，对抗攻击），以便更好地了解现有方法的共同点和差异，并为未来的研究提供基础。为便于未来的研究，我们创建了一个汇总的数据集、网络和评估的库，并提供了一些有价值的建议，以及一些前景探索的可能性。
</details></li>
</ul>
<hr>
<h2 id="Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes"><a href="#Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes" class="headerlink" title="Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes"></a>Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06762">http://arxiv.org/abs/2308.06762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Huang, Xukun Zhang, Zhiming Cui, He Zhang, Geng Chen, Dinggang Shen</li>
<li>for: 这个论文的目的是为了提高胎儿大脑磁共振成像（MR）扫描中的精准组织分割。</li>
<li>methods: 这篇论文使用了域适应技术，将高质量的ISO体磁共振图像（和其相应的注解）作为指导，以提高胎儿大脑磁共振扫描中的精准组织分割。</li>
<li>results: 这篇论文的实验结果表明，使用C2DA-Net可以在胎儿大脑磁共振扫描中提高精准组织分割的性能，并且比前Edge的方法更好。<details>
<summary>Abstract</summary>
Accurate tissue segmentation of thick-slice fetal brain magnetic resonance (MR) scans is crucial for both reconstruction of isotropic brain MR volumes and the quantification of fetal brain development. However, this task is challenging due to the use of thick-slice scans in clinically-acquired fetal brain data. To address this issue, we propose to leverage high-quality isotropic fetal brain MR volumes (and also their corresponding annotations) as guidance for segmentation of thick-slice scans. Due to existence of significant domain gap between high-quality isotropic volume (i.e., source data) and thick-slice scans (i.e., target data), we employ a domain adaptation technique to achieve the associated knowledge transfer (from high-quality <source> volumes to thick-slice <target> scans). Specifically, we first register the available high-quality isotropic fetal brain MR volumes across different gestational weeks to construct longitudinally-complete source data. To capture domain-invariant information, we then perform Fourier decomposition to extract image content and style codes. Finally, we propose a novel Cycle-Consistent Domain Adaptation Network (C2DA-Net) to efficiently transfer the knowledge learned from high-quality isotropic volumes for accurate tissue segmentation of thick-slice scans. Our C2DA-Net can fully utilize a small set of annotated isotropic volumes to guide tissue segmentation on unannotated thick-slice scans. Extensive experiments on a large-scale dataset of 372 clinically acquired thick-slice MR scans demonstrate that our C2DA-Net achieves much better performance than cutting-edge methods quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
准确的脏部分 segmentation thick-slice 胎 Mind Magnetic Resonance（MR）扫描是关键的，以重建是otropic 胎 Mind MR 体积以及胎 Mind 发展评估。然而，这项任务受到thick-slice 扫描的使用带来挑战，因为这些扫描通常具有低分辨率。为了解决这个问题，我们提议利用高质量的 isotropic 胎 Mind MR 体积（以及其相应的注释）作为指导，以提高 thick-slice 扫描的 segmentation 精度。由于源数据和目标数据之间存在显著的领域差异，我们采用领域适应技术来实现相关的知识传递。具体来说，我们首先将可用的高质量 isotropic 胎 Mind MR 体积进行注册，以构建不同 Gestational Week 的 longitudinally-complete 源数据。然后，我们使用 Fourier 分解来提取图像内容和样式代码。最后，我们提议一种新的 Cycle-Consistent Domain Adaptation Network（C2DA-Net），以高效地将高质量 isotropic 体积中学到的知识传递到 thick-slice 扫描中。我们的 C2DA-Net 可以充分利用一小组注释的 isotropic 体积来导引脏部分 segmentation on unannotated thick-slice scans。我们在一个大规模的数据集上进行了广泛的实验，并证明了我们的 C2DA-Net 在量和质量上都有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Influence-Function-Based-Second-Order-Channel-Pruning-Evaluating-True-Loss-Changes-For-Pruning-Is-Possible-Without-Retraining"><a href="#Influence-Function-Based-Second-Order-Channel-Pruning-Evaluating-True-Loss-Changes-For-Pruning-Is-Possible-Without-Retraining" class="headerlink" title="Influence Function Based Second-Order Channel Pruning-Evaluating True Loss Changes For Pruning Is Possible Without Retraining"></a>Influence Function Based Second-Order Channel Pruning-Evaluating True Loss Changes For Pruning Is Possible Without Retraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06755">http://arxiv.org/abs/2308.06755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrcheng1066/ifso">https://github.com/hrcheng1066/ifso</a></li>
<li>paper_authors: Hongrong Cheng, Miao Zhang, Javen Qinfeng Shi</li>
<li>for: 这篇论文旨在提出一种新的通道缩减方法，以更有效地选择需要缩减的通道。</li>
<li>methods: 该方法使用了Influence Function（影响函数）来评估通道的真实损失变化，而不需要重新训练权重。</li>
<li>results: 实验表明，该方法可以更加准确地选择需要缩减的通道，并且比exististing方法更快速。此外，该方法还开拓出了一些新的可能性，例如可以不需要重新训练权重来评估true损失变化。<details>
<summary>Abstract</summary>
A challenge of channel pruning is designing efficient and effective criteria to select channels to prune. A widely used criterion is minimal performance degeneration. To accurately evaluate the truth performance degeneration requires retraining the survived weights to convergence, which is prohibitively slow. Hence existing pruning methods use previous weights (without retraining) to evaluate the performance degeneration. However, we observe the loss changes differ significantly with and without retraining. It motivates us to develop a technique to evaluate true loss changes without retraining, with which channels to prune can be selected more reliably and confidently. We first derive a closed-form estimator of the true loss change per pruning mask change, using influence functions without retraining. Influence function which is from robust statistics reveals the impacts of a training sample on the model's prediction and is repurposed by us to assess impacts on true loss changes. We then show how to assess the importance of all channels simultaneously and develop a novel global channel pruning algorithm accordingly. We conduct extensive experiments to verify the effectiveness of the proposed algorithm. To the best of our knowledge, we are the first that shows evaluating true loss changes for pruning without retraining is possible. This finding will open up opportunities for a series of new paradigms to emerge that differ from existing pruning methods. The code is available at https://github.com/hrcheng1066/IFSO.
</details>
<details>
<summary>摘要</summary>
一个频道剔除挑战是设计高效、有效的选择频道的 критеририи。广泛使用的标准是最小性能倒退。然而，要准确评估真正的性能倒退，需要重新训练存活的权重，这是非常慢的。因此，现有的剔除方法使用前一个 weights（无需重新训练）来评估性能倒退。但我们发现，无需重新训练时的损失变化很大。这种发现使我们开发一种评估真正的损失变化的技术，以更加可靠和自信地选择剔除频道。我们首先 deriv 一个关闭式估计器，用于评估每个剔除面积变化后的真正损失变化。我们使用 robust 统计中的影响函数，无需重新训练，可以准确地评估频道对模型预测的影响。然后，我们可以同时评估所有频道的重要性，并开发了一种全局频道剔除算法。我们进行了广泛的实验，证明了我们的提案的有效性。根据我们所知，我们是第一个证明可以无需重新训练评估真正的损失变化的人。这一发现将开启一系列的新思想，与现有的剔除方法不同。我们的代码可以在 <https://github.com/hrcheng1066/IFSO> 上找到。
</details></li>
</ul>
<hr>
<h2 id="FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table"><a href="#FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table" class="headerlink" title="FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table"></a>FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06749">http://arxiv.org/abs/2308.06749</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenhao-li-777/fastllve">https://github.com/wenhao-li-777/fastllve</a></li>
<li>paper_authors: Wenhao Li, Guangyang Wu, Wenyi Wang, Peiran Ren, Xiaohong Liu</li>
<li>for: 提高低光照视频质量</li>
<li>methods: 使用Look-Up-Table（LUT）技术维护间帧亮度一致性，并设计了学习型Intensity-Aware LUT（IA-LUT）模块进行自适应增强。</li>
<li>results: 实验结果表明，我们的方法在质量和间帧亮度一致性两个方面均达到了领先水平，并且可以在1080p视频上实现50+帧&#x2F;秒的处理速度，比SOTA CNN基于方法更快。<details>
<summary>Abstract</summary>
Low-Light Video Enhancement (LLVE) has received considerable attention in recent years. One of the critical requirements of LLVE is inter-frame brightness consistency, which is essential for maintaining the temporal coherence of the enhanced video. However, most existing single-image-based methods fail to address this issue, resulting in flickering effect that degrades the overall quality after enhancement. Moreover, 3D Convolution Neural Network (CNN)-based methods, which are designed for video to maintain inter-frame consistency, are computationally expensive, making them impractical for real-time applications. To address these issues, we propose an efficient pipeline named FastLLVE that leverages the Look-Up-Table (LUT) technique to maintain inter-frame brightness consistency effectively. Specifically, we design a learnable Intensity-Aware LUT (IA-LUT) module for adaptive enhancement, which addresses the low-dynamic problem in low-light scenarios. This enables FastLLVE to perform low-latency and low-complexity enhancement operations while maintaining high-quality results. Experimental results on benchmark datasets demonstrate that our method achieves the State-Of-The-Art (SOTA) performance in terms of both image quality and inter-frame brightness consistency. More importantly, our FastLLVE can process 1,080p videos at $\mathit{50+}$ Frames Per Second (FPS), which is $\mathit{2 \times}$ faster than SOTA CNN-based methods in inference time, making it a promising solution for real-time applications. The code is available at https://github.com/Wenhao-Li-777/FastLLVE.
</details>
<details>
<summary>摘要</summary>
低光照视频提升（LLVE）在最近几年内获得了广泛关注。一个关键的需求是 между帧亮度一致性，这是维护提升后视频的时间一致性的关键。然而，大多数现有的单图像基方法无法解决这个问题，导致提升后的视频呈现出抖抖的效果，从而降低总质量。此外，基于视频的3D卷积神经网络（CNN）方法，尽管可以维护间帧一致性，但是计算成本高昂，使其不适合实时应用。为解决这些问题，我们提出了一个高效的排序名为快速LLVE，它利用了Look-Up-Table（LUT）技术来保证间帧亮度一致性。我们特别设计了一个可学习的Intensity-Aware LUT（IA-LUT）模块，用于自适应增强，解决低动态问题在低光照场景下。这使得快速LLVE可以在低延迟和低复杂度下进行增强操作，同时保持高质量结果。实验结果表明，我们的方法在标准测试集上达到了领先的性能水平， both image quality和间帧亮度一致性。此外，我们的快速LLVE可以处理1080p视频，并在50+帧每秒进行加速，这比SOTA CNN基于方法的推理时间快速2倍。代码可以在https://github.com/Wenhao-Li-777/FastLLVE中找到。
</details></li>
</ul>
<hr>
<h2 id="Target-before-Shooting-Accurate-Anomaly-Detection-and-Localization-under-One-Millisecond-via-Cascade-Patch-Retrieval"><a href="#Target-before-Shooting-Accurate-Anomaly-Detection-and-Localization-under-One-Millisecond-via-Cascade-Patch-Retrieval" class="headerlink" title="Target before Shooting: Accurate Anomaly Detection and Localization under One Millisecond via Cascade Patch Retrieval"></a>Target before Shooting: Accurate Anomaly Detection and Localization under One Millisecond via Cascade Patch Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06748">http://arxiv.org/abs/2308.06748</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flyinghu123/cpr">https://github.com/flyinghu123/cpr</a></li>
<li>paper_authors: Hanxi Li, Jianfei Hu, Bo Li, Hao Chen, Yongbin Zheng, Chunhua Shen</li>
<li>for: 提出了一种新的异常检测框架，实现了同时保证异常检测精度和运行速度的两个目标。</li>
<li>methods: 该框架通过粗细匹配方法选择测试图像各个小块的最佳对比图像，然后使用地区匹配方法在这些地区找到最佳的地方匹配。最后，计算每个测试图像块的异常分数基于地方匹配距离和非背景概率。</li>
<li>results: 在MVTec AD、BTAD和MVTec-3D AD等三个评测 dataset 上，提出的方法与所有参照方法进行比较，具有显著的优势，测试结果表明，该方法在不同的异常检测任务中具有较高的精度和较低的时间复杂度。<details>
<summary>Abstract</summary>
In this work, by re-examining the "matching" nature of Anomaly Detection (AD), we propose a new AD framework that simultaneously enjoys new records of AD accuracy and dramatically high running speed. In this framework, the anomaly detection problem is solved via a cascade patch retrieval procedure that retrieves the nearest neighbors for each test image patch in a coarse-to-fine fashion. Given a test sample, the top-K most similar training images are first selected based on a robust histogram matching process. Secondly, the nearest neighbor of each test patch is retrieved over the similar geometrical locations on those "global nearest neighbors", by using a carefully trained local metric. Finally, the anomaly score of each test image patch is calculated based on the distance to its "local nearest neighbor" and the "non-background" probability. The proposed method is termed "Cascade Patch Retrieval" (CPR) in this work. Different from the conventional patch-matching-based AD algorithms, CPR selects proper "targets" (reference images and locations) before "shooting" (patch-matching). On the well-acknowledged MVTec AD, BTAD and MVTec-3D AD datasets, the proposed algorithm consistently outperforms all the comparing SOTA methods by remarkable margins, measured by various AD metrics. Furthermore, CPR is extremely efficient. It runs at the speed of 113 FPS with the standard setting while its simplified version only requires less than 1 ms to process an image at the cost of a trivial accuracy drop. The code of CPR is available at https://github.com/flyinghu123/CPR.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们重新审视了异常检测（AD）的“匹配”性质，并提出了一种新的AD框架，该框架同时具有新纪录级AD准确率和极高的运行速度。在该框架中，异常检测问题通过一种层次补丁检索过程来解决，首先选择测试样本中最相似的训练图像集，然后在这些“全球最似图像”上进行精心训练的本地度量来检索测试补丁的最近邻居。最后，测试图像补丁的异常分数根据补丁与“本地最似图像”以及“非背景”概率来计算。我们称这种方法为“层次补丁检索”（CPR）。与传统的补丁匹配基于AD算法不同，CPR在选择“目标”（参考图像和位置）之前已经选择了合适的“目标”。在广泛承认的MVTec AD、BTAD和MVTec-3D AD数据集上，我们的提案方法与所有比较参考方法的较大胜利差度相比，按照不同的AD指标进行评价。此外，CPR非常高效，它在标准设置下运行速度达113帧/秒，而其简化版本只需0.1毫秒来处理一幅图像，而且只有一rivial的准确率下降。CPR的代码可以在GitHub上找到：https://github.com/flyinghu123/CPR。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising"><a href="#Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising" class="headerlink" title="Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising"></a>Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06746">http://arxiv.org/abs/2308.06746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyuan01/self-supervised-noise2noise-for-ldct">https://github.com/xyuan01/self-supervised-noise2noise-for-ldct</a></li>
<li>paper_authors: Yuting Zhu, Qiang He, Yudong Yao, Yueyang Teng</li>
<li>for: 这篇论文旨在提出一种基于单簇 Computed Tomography (CT) 影像的自动降噪方法，不需要配对的陌生资料。</li>
<li>methods: 这篇论文使用了一种组合方法，包括自我指导的噪声2噪声模型和陌生噪声策略。首先，我们将 LDCT 影像重复地添加了一种相似的噪声。然后，我们使用只有次要损坏的影像进行训练。我们选择了一个模组化 U-Net 结构来进行任务，这样可以增加讯号场的视野而无需增加参数数。</li>
<li>results: 实验结果显示，提案的方法比过去的深度学习方法更有效率，在 Mayo LDCT 数据集上得到了好的效果。<details>
<summary>Abstract</summary>
Deep learning is a very promising technique for low-dose computed tomography (LDCT) image denoising. However, traditional deep learning methods require paired noisy and clean datasets, which are often difficult to obtain. This paper proposes a new method for performing LDCT image denoising with only LDCT data, which means that normal-dose CT (NDCT) is not needed. We adopt a combination including the self-supervised noise2noise model and the noisy-as-clean strategy. First, we add a second yet similar type of noise to LDCT images multiple times. Note that we use LDCT images based on the noisy-as-clean strategy for corruption instead of NDCT images. Then, the noise2noise model is executed with only the secondary corrupted images for training. We select a modular U-Net structure from several candidates with shared parameters to perform the task, which increases the receptive field without increasing the parameter size. The experimental results obtained on the Mayo LDCT dataset show the effectiveness of the proposed method compared with that of state-of-the-art deep learning methods. The developed code is available at https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT.
</details>
<details>
<summary>摘要</summary>
深度学习是LDCT图像锈除的非常有前途的技术。然而，传统的深度学习方法需要配备附近的噪声和清洁数据集，这经常很难以获得。这篇论文提出了一种使用仅LDCT数据进行LDCT图像锈除的新方法。我们采用了混合自我supervised随机噪声模型和噪声作为清洁策略。首先，我们将LDCT图像添加了多个相似的噪声。注意，我们使用LDCT图像作为噪声Strategy instead ofNDCT图像。然后，我们执行了噪声2噪声模型，只使用次要损害的图像进行训练。我们选择了一种模块化U-Net结构从多个候选结构中，以增加感知场而不是增加参数大小。实验结果在Mayo LDCT数据集上表明了提议的方法的有效性，比对现有的深度学习方法更好。开发代码可以在https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT中下载。
</details></li>
</ul>
<hr>
<h2 id="TextDiff-Mask-Guided-Residual-Diffusion-Models-for-Scene-Text-Image-Super-Resolution"><a href="#TextDiff-Mask-Guided-Residual-Diffusion-Models-for-Scene-Text-Image-Super-Resolution" class="headerlink" title="TextDiff: Mask-Guided Residual Diffusion Models for Scene Text Image Super-Resolution"></a>TextDiff: Mask-Guided Residual Diffusion Models for Scene Text Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06743">http://arxiv.org/abs/2308.06743</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lenubolim/textdiff">https://github.com/lenubolim/textdiff</a></li>
<li>paper_authors: Baolin Liu, Zongyuan Yang, Pengfei Wang, Junjie Zhou, Ziqi Liu, Ziyi Song, Yan Liu, Yongping Xiong</li>
<li>For: The paper aims to improve the readability and recognizability of scene text images by proposing a diffusion-based framework for scene text image super-resolution.* Methods: The proposed method, called TextDiff, consists of two modules: the Text Enhancement Module (TEM) and the Mask-Guided Residual Diffusion Module (MRD). The TEM generates an initial deblurred text image and a mask that encodes the spatial location of the text, while the MRD effectively sharpenes the text edge by modeling the residuals between the ground-truth images and the initial deblurred images.* Results: The proposed TextDiff achieves state-of-the-art (SOTA) performance on public benchmark datasets and can improve the readability of scene text images. Additionally, the MRD module is plug-and-play and can effectively sharpens the text edges produced by SOTA methods without requiring any additional joint training.<details>
<summary>Abstract</summary>
The goal of scene text image super-resolution is to reconstruct high-resolution text-line images from unrecognizable low-resolution inputs. The existing methods relying on the optimization of pixel-level loss tend to yield text edges that exhibit a notable degree of blurring, thereby exerting a substantial impact on both the readability and recognizability of the text. To address these issues, we propose TextDiff, the first diffusion-based framework tailored for scene text image super-resolution. It contains two modules: the Text Enhancement Module (TEM) and the Mask-Guided Residual Diffusion Module (MRD). The TEM generates an initial deblurred text image and a mask that encodes the spatial location of the text. The MRD is responsible for effectively sharpening the text edge by modeling the residuals between the ground-truth images and the initial deblurred images. Extensive experiments demonstrate that our TextDiff achieves state-of-the-art (SOTA) performance on public benchmark datasets and can improve the readability of scene text images. Moreover, our proposed MRD module is plug-and-play that effectively sharpens the text edges produced by SOTA methods. This enhancement not only improves the readability and recognizability of the results generated by SOTA methods but also does not require any additional joint training. Available Codes:https://github.com/Lenubolim/TextDiff.
</details>
<details>
<summary>摘要</summary>
目标是帮助您将低分辨率的场景文本图像转换成高分辨率文本线图像。现有的方法通常通过像素级损失优化来实现文本边缘的增强，但这会导致文本边缘变得模糊，从而影响文本的可读性和识别性。为了解决这些问题，我们提出了 TextDiff，首个适用于场景文本图像超分辨率的扩散框架。它包括两个模块：文本增强模块（TEM）和帮助器导向残差扩散模块（MRD）。TEM 生成了初始的去噪文本图像和一个描述文本的空间位置的面罩。MRD 负责通过模拟实际图像和初始去噪图像之间的差异来有效地尖锐文本边缘。我们进行了广泛的实验，结果表明 TextDiff 在公共测试集上达到了领先的表现水平（SOTA），并可以提高场景文本图像的可读性。此外，我们提出的 MRD 模块可以很好地增强 SOTA 方法生成的文本边缘，无需额外的联合训练。可以在 GitHub 上下载代码：https://github.com/Lenubolim/TextDiff。
</details></li>
</ul>
<hr>
<h2 id="Free-ATM-Exploring-Unsupervised-Learning-on-Diffusion-Generated-Images-with-Free-Attention-Masks"><a href="#Free-ATM-Exploring-Unsupervised-Learning-on-Diffusion-Generated-Images-with-Free-Attention-Masks" class="headerlink" title="Free-ATM: Exploring Unsupervised Learning on Diffusion-Generated Images with Free Attention Masks"></a>Free-ATM: Exploring Unsupervised Learning on Diffusion-Generated Images with Free Attention Masks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06739">http://arxiv.org/abs/2308.06739</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Junhao Zhang, Mutian Xu, Chuhui Xue, Wenqing Zhang, Xiaoguang Han, Song Bai, Mike Zheng Shou</li>
<li>for: 本研究旨在解决无监督学习在视觉表示中的快速进步，尽管需要训练大规模数据集，但这会导致数据采集成本高昂，并且存在数据隐私问题。</li>
<li>methods: 我们开始通过探索 diffusion models 的 cross-attention层内置的annotation-free注意力掩模来解决这一问题。我们还investigate了三种常见的无监督学习技术（即对比学习、遮盖模型和视觉语言预训练），并提出了专门采用这些自由注意力掩模的解决方案。</li>
<li>results: 我们通过了广泛的实验，证明了我们的方法可以在不同的下游任务中提高基eline模型的性能，包括图像分类、检测、分割和图像文本检索。通过使用我们的方法，可以将无监督预训练在synthetic数据上的性能与实际场景中的性能趋同。<details>
<summary>Abstract</summary>
Despite the rapid advancement of unsupervised learning in visual representation, it requires training on large-scale datasets that demand costly data collection, and pose additional challenges due to concerns regarding data privacy. Recently, synthetic images generated by text-to-image diffusion models, have shown great potential for benefiting image recognition. Although promising, there has been inadequate exploration dedicated to unsupervised learning on diffusion-generated images. To address this, we start by uncovering that diffusion models' cross-attention layers inherently provide annotation-free attention masks aligned with corresponding text inputs on generated images. We then investigate the problems of three prevalent unsupervised learning techniques ( i.e., contrastive learning, masked modeling, and vision-language pretraining) and introduce customized solutions by fully exploiting the aforementioned free attention masks. Our approach is validated through extensive experiments that show consistent improvements in baseline models across various downstream tasks, including image classification, detection, segmentation, and image-text retrieval. By utilizing our method, it is possible to close the performance gap between unsupervised pretraining on synthetic data and real-world scenarios.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:尽管Unsupervised learning在视觉表示方面进步 Rapidly, but it still requires expensive data collection and raises additional concerns about data privacy. Recently, text-to-image diffusion models generated synthetic images have shown great potential for image recognition. Although promising, there has been inadequate exploration of unsupervised learning on diffusion-generated images. To address this, we start by discovering that diffusion models' cross-attention layers inherently provide annotation-free attention masks aligned with corresponding text inputs on generated images. We then investigate the problems of three prevalent unsupervised learning techniques (i.e., contrastive learning, masked modeling, and vision-language pretraining) and introduce customized solutions by fully exploiting the aforementioned free attention masks. Our approach is validated through extensive experiments that show consistent improvements in baseline models across various downstream tasks, including image classification, detection, segmentation, and image-text retrieval. By utilizing our method, it is possible to close the performance gap between unsupervised pretraining on synthetic data and real-world scenarios.Translated into Traditional Chinese:尽管Unsupervised learning在视觉表示方面进步 Rapidly, but it still requires expensive data collection and raises additional concerns about data privacy. Recently, text-to-image diffusion models generated synthetic images have shown great potential for image recognition. Although promising, there has been inadequate exploration of unsupervised learning on diffusion-generated images. To address this, we start by discovering that diffusion models' cross-attention layers inherently provide annotation-free attention masks aligned with corresponding text inputs on generated images. We then investigate the problems of three prevalent unsupervised learning techniques (i.e., contrastive learning, masked modeling, and vision-language pretraining) and introduce customized solutions by fully exploiting the aforementioned free attention masks. Our approach is validated through extensive experiments that show consistent improvements in baseline models across various downstream tasks, including image classification, detection, segmentation, and image-text retrieval. By utilizing our method, it is possible to close the performance gap between unsupervised pretraining on synthetic data and real-world scenarios.
</details></li>
</ul>
<hr>
<h2 id="3D-Scene-Graph-Prediction-on-Point-Clouds-Using-Knowledge-Graphs"><a href="#3D-Scene-Graph-Prediction-on-Point-Clouds-Using-Knowledge-Graphs" class="headerlink" title="3D Scene Graph Prediction on Point Clouds Using Knowledge Graphs"></a>3D Scene Graph Prediction on Point Clouds Using Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06719">http://arxiv.org/abs/2308.06719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiding Qiu, Henrik I. Christensen</li>
<li>for:  scene graph prediction in 3D environments</li>
<li>methods: message-passing method with commonsense knowledge graphs</li>
<li>results: 15.0% improvement in scene graph prediction accuracy with external knowledge, 7.96% improvement with internal knowledge compared to state-of-the-art algorithms, and real-world testing with 10 frames per second for scene graph generation.Here’s the full text in Simplified Chinese:</li>
<li>for: scene graph prediction在3D环境中</li>
<li>methods: message-passing方法与常识知识图</li>
<li>results: 外部知识Integration leads to 15.0% improvement in scene graph prediction accuracy, 7.96% improvement with internal knowledge compared to state-of-the-art algorithms, and real-world testing with 10 frames per second for scene graph generation.<details>
<summary>Abstract</summary>
3D scene graph prediction is a task that aims to concurrently predict object classes and their relationships within a 3D environment. As these environments are primarily designed by and for humans, incorporating commonsense knowledge regarding objects and their relationships can significantly constrain and enhance the prediction of the scene graph. In this paper, we investigate the application of commonsense knowledge graphs for 3D scene graph prediction on point clouds of indoor scenes. Through experiments conducted on a real-world indoor dataset, we demonstrate that integrating external commonsense knowledge via the message-passing method leads to a 15.0 % improvement in scene graph prediction accuracy with external knowledge and $7.96\%$ with internal knowledge when compared to state-of-the-art algorithms. We also tested in the real world with 10 frames per second for scene graph generation to show the usage of the model in a more realistic robotics setting.
</details>
<details>
<summary>摘要</summary>
三维场景图预测是一项任务，旨在同时预测场景中对象的类别和其之间的关系。由于这些环境主要由人类设计和使用，因此包含常识知识对场景图预测具有明显的约束和优化作用。在这篇论文中，我们调查了在点云indoor场景中使用commonsense知识图进行三维场景图预测的应用。通过对实际indoor数据集进行实验，我们表明了将外部常识知识integrated到消息传递方法中可以提高场景图预测精度，比对 estado-of-the-art算法提高15.0%。此外，我们还在真实的 robotics 环境中测试了Scene Graph生成，以示模型的应用。
</details></li>
</ul>
<hr>
<h2 id="StairNetV3-Depth-aware-Stair-Modeling-using-Deep-Learning"><a href="#StairNetV3-Depth-aware-Stair-Modeling-using-Deep-Learning" class="headerlink" title="StairNetV3: Depth-aware Stair Modeling using Deep Learning"></a>StairNetV3: Depth-aware Stair Modeling using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06715">http://arxiv.org/abs/2308.06715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Wang, Zhongcai Pei, Shuang Qiu, Yachun Wang, Zhiyong Tang</li>
<li>for: 这 paper 的目的是提出一种基于视觉的自主移动 робоット climb 楼梯的技术，尤其是在不熟悉的环境中。</li>
<li>methods: 该 paper 使用了一种基于 convolutional neural network (CNN) 的 depth-aware stair modeling 方法，包括提取楼梯几何特征和预测深度图像为联合任务，并使用设计的信息传播架构以实现有效的超视觉学习。</li>
<li>results: 实验表明，该方法与之前最佳的单目视觉方法相比，有一个显著的提升（IOU 提升3.4%），并且Lightweight 版本具有快速检测速度，可满足大多数实时应用的需求。<details>
<summary>Abstract</summary>
Vision-based stair perception can help autonomous mobile robots deal with the challenge of climbing stairs, especially in unfamiliar environments. To address the problem that current monocular vision methods are difficult to model stairs accurately without depth information, this paper proposes a depth-aware stair modeling method for monocular vision. Specifically, we take the extraction of stair geometric features and the prediction of depth images as joint tasks in a convolutional neural network (CNN), with the designed information propagation architecture, we can achieve effective supervision for stair geometric feature learning by depth information. In addition, to complete the stair modeling, we take the convex lines, concave lines, tread surfaces and riser surfaces as stair geometric features and apply Gaussian kernels to enable the network to predict contextual information within the stair lines. Combined with the depth information obtained by depth sensors, we propose a stair point cloud reconstruction method that can quickly get point clouds belonging to the stair step surfaces. Experiments on our dataset show that our method has a significant improvement over the previous best monocular vision method, with an intersection over union (IOU) increase of 3.4 %, and the lightweight version has a fast detection speed and can meet the requirements of most real-time applications. Our dataset is available at https://data.mendeley.com/datasets/6kffmjt7g2/1.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用视觉技术，自动移动Robot可以更好地处理楼梯，特别是在未知环境中。为了解决目前的单目视觉方法难以准确地模型楼梯 without depth information，这篇论文提出了一种基于深度信息的楼梯模型方法。具体来说，我们将提取楼梯的 geometric 特征和预测深度图作为一个 convolutional neural network (CNN) 中的联合任务，通过我们设计的信息传递架构，可以实现有效的监督楼梯 geometric 特征学习。此外，为了完成楼梯模型，我们将楼梯的 convex 线、拱线、踏板面和踏梯面作为楼梯的 geometric 特征，并应用 Gaussian kernels，使网络可以预测楼梯内部的信息。与depth sensor获取的深度信息结合，我们提出了一种可以快速获取楼梯步骤表面的点云重建方法。实验结果表明，我们的方法与前一个最佳单目视觉方法相比，IOU 提高了 3.4%，轻量版本具有快速检测速度，可满足大多数实时应用的需求。我们的数据集可以在 <https://data.mendeley.com/datasets/6kffmjt7g2/1> 中下载。
</details></li>
</ul>
<hr>
<h2 id="LAW-Diffusion-Complex-Scene-Generation-by-Diffusion-with-Layouts"><a href="#LAW-Diffusion-Complex-Scene-Generation-by-Diffusion-with-Layouts" class="headerlink" title="LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts"></a>LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06713">http://arxiv.org/abs/2308.06713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binbin Yang, Yi Luo, Ziliang Chen, Guangrun Wang, Xiaodan Liang, Liang Lin</li>
<li>for: 这篇研究是为了实现高品质的复杂场景生成，以优化现有的散射模型。</li>
<li>methods: 这篇研究提出了一个具有 semantic control 的 Layout-Aware 散射模型（LAW-Diffusion），通过内置的空间依赖解析和位置意识的跨物体注意力模组，实现了具有属地对应性和空间相互关联的场景生成。</li>
<li>results:  compared to previous Layout-to-Image（L2I）方法，LAW-Diffusion 可以更好地生成具有内在逻辑和空间相互关联的场景，并且可以实现实际中的实例重新构成。<details>
<summary>Abstract</summary>
Thanks to the rapid development of diffusion models, unprecedented progress has been witnessed in image synthesis. Prior works mostly rely on pre-trained linguistic models, but a text is often too abstract to properly specify all the spatial properties of an image, e.g., the layout configuration of a scene, leading to the sub-optimal results of complex scene generation. In this paper, we achieve accurate complex scene generation by proposing a semantically controllable Layout-AWare diffusion model, termed LAW-Diffusion. Distinct from the previous Layout-to-Image generation (L2I) methods that only explore category-aware relationships, LAW-Diffusion introduces a spatial dependency parser to encode the location-aware semantic coherence across objects as a layout embedding and produces a scene with perceptually harmonious object styles and contextual relations. To be specific, we delicately instantiate each object's regional semantics as an object region map and leverage a location-aware cross-object attention module to capture the spatial dependencies among those disentangled representations. We further propose an adaptive guidance schedule for our layout guidance to mitigate the trade-off between the regional semantic alignment and the texture fidelity of generated objects. Moreover, LAW-Diffusion allows for instance reconfiguration while maintaining the other regions in a synthesized image by introducing a layout-aware latent grafting mechanism to recompose its local regional semantics. To better verify the plausibility of generated scenes, we propose a new evaluation metric for the L2I task, dubbed Scene Relation Score (SRS) to measure how the images preserve the rational and harmonious relations among contextual objects. Comprehensive experiments demonstrate that our LAW-Diffusion yields the state-of-the-art generative performance, especially with coherent object relations.
</details>
<details>
<summary>摘要</summary>
due to the rapid development of diffusion models, there have been unprecedented advances in image synthesis. previous works mainly rely on pre-trained linguistic models, but a text is often too abstract to properly specify all the spatial properties of an image, such as the layout configuration of a scene, leading to sub-optimal results of complex scene generation. in this paper, we achieve accurate complex scene generation by proposing a semantically controllable Layout-AWare diffusion model, termed LAW-Diffusion. unlike previous Layout-to-Image (L2I) methods that only explore category-aware relationships, LAW-Diffusion introduces a spatial dependency parser to encode the location-aware semantic coherence across objects as a layout embedding and produces a scene with perceptually harmonious object styles and contextual relations. specifically, we delicately instantiate each object's regional semantics as an object region map and leverage a location-aware cross-object attention module to capture the spatial dependencies among those disentangled representations. we also propose an adaptive guidance schedule for our layout guidance to mitigate the trade-off between the regional semantic alignment and the texture fidelity of generated objects. furthermore, LAW-Diffusion allows for instance reconfiguration while maintaining the other regions in a synthesized image by introducing a layout-aware latent grafting mechanism to recompose its local regional semantics. to better verify the plausibility of generated scenes, we propose a new evaluation metric for the L2I task, dubbed Scene Relation Score (SRS) to measure how the images preserve the rational and harmonious relations among contextual objects. comprehensive experiments demonstrate that our LAW-Diffusion yields the state-of-the-art generative performance, especially with coherent object relations.
</details></li>
</ul>
<hr>
<h2 id="Compositional-Feature-Augmentation-for-Unbiased-Scene-Graph-Generation"><a href="#Compositional-Feature-Augmentation-for-Unbiased-Scene-Graph-Generation" class="headerlink" title="Compositional Feature Augmentation for Unbiased Scene Graph Generation"></a>Compositional Feature Augmentation for Unbiased Scene Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06712">http://arxiv.org/abs/2308.06712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Li, Guikun Chen, Jun Xiao, Yi Yang, Chunping Wang, Long Chen</li>
<li>for: 本研究旨在探讨如何更好地探测图像中的视觉关系 triplets &lt;sub, pred, obj&gt;，以提高Scene Graph Generation (SGG) 的性能。</li>
<li>methods: 本文提出了一种新的Compositional Feature Augmentation (CFA)策略，该策略可以增加每个 predicate 的关系 triplet 特征的多样性，从而提高 SGG 的鲁棒性。CFA 包括将每个关系 triplet 特征分解成两部分：内在特征和外在特征，然后通过将这些特征与其他样本的特征进行替换或混合来增加 triplet 特征的多样性。</li>
<li>results: 对比于现有的重新权衡策略，CFA 可以更好地增加每个 predicate 的关系 triplet 特征的多样性，从而提高 SGG 的性能。经过广泛的ablation研究，我们发现CFA 可以在不同的 metrics 之间取得新的状态公共表现。<details>
<summary>Abstract</summary>
Scene Graph Generation (SGG) aims to detect all the visual relation triplets <sub, pred, obj> in a given image. With the emergence of various advanced techniques for better utilizing both the intrinsic and extrinsic information in each relation triplet, SGG has achieved great progress over the recent years. However, due to the ubiquitous long-tailed predicate distributions, today's SGG models are still easily biased to the head predicates. Currently, the most prevalent debiasing solutions for SGG are re-balancing methods, e.g., changing the distributions of original training samples. In this paper, we argue that all existing re-balancing strategies fail to increase the diversity of the relation triplet features of each predicate, which is critical for robust SGG. To this end, we propose a novel Compositional Feature Augmentation (CFA) strategy, which is the first unbiased SGG work to mitigate the bias issue from the perspective of increasing the diversity of triplet features. Specifically, we first decompose each relation triplet feature into two components: intrinsic feature and extrinsic feature, which correspond to the intrinsic characteristics and extrinsic contexts of a relation triplet, respectively. Then, we design two different feature augmentation modules to enrich the feature diversity of original relation triplets by replacing or mixing up either their intrinsic or extrinsic features from other samples. Due to its model-agnostic nature, CFA can be seamlessly incorporated into various SGG frameworks. Extensive ablations have shown that CFA achieves a new state-of-the-art performance on the trade-off between different metrics.
</details>
<details>
<summary>摘要</summary>
Specifically, we first decompose each relation triplet feature into two components: intrinsic feature and extrinsic feature, which correspond to the intrinsic characteristics and extrinsic contexts of a relation triplet, respectively. Then, we design two different feature augmentation modules to enrich the feature diversity of original relation triplets by replacing or mixing up either their intrinsic or extrinsic features from other samples. Due to its model-agnostic nature, CFA can be seamlessly incorporated into various SGG frameworks. Extensive ablations have shown that CFA achieves a new state-of-the-art performance on the trade-off between different metrics.
</details></li>
</ul>
<hr>
<h2 id="Condition-Adaptive-Graph-Convolution-Learning-for-Skeleton-Based-Gait-Recognition"><a href="#Condition-Adaptive-Graph-Convolution-Learning-for-Skeleton-Based-Gait-Recognition" class="headerlink" title="Condition-Adaptive Graph Convolution Learning for Skeleton-Based Gait Recognition"></a>Condition-Adaptive Graph Convolution Learning for Skeleton-Based Gait Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06707">http://arxiv.org/abs/2308.06707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oliverhxh/cag">https://github.com/oliverhxh/cag</a></li>
<li>paper_authors: Xiaohu Huang, Xinggang Wang, Zhidianqiu Jin, Bo Yang, Botao He, Bin Feng, Wenyu Liu</li>
<li>for: 本研究旨在提高skeleton-based gait认知 task中的个人识别率，使用graph convolutional networks (GCNs)来提取多视角下不同人体姿势的特征。</li>
<li>methods: 我们提出了一种condition-adaptive graph (CAG) convolution network，具有自适应特征和视角的能力。CAG网络包括joint-specific filter learning (JSFL)模块和view-adaptive topology learning (VATL)模块。JSFL模块生成每个关节独特的滤波器， capture细腻的姿势特征；VATL模块生成适应视角的图学结构，对关节进行相应的相关处理。</li>
<li>results: 实验结果表明，CAG网络在CASIA-B和OU-MVLP两个最常用的数据集上都超过了所有之前的skeleton-based方法。此外，通过与视觉基本方法相结合，CAG网络可以提供有用的补充信息，提高了识别率。<details>
<summary>Abstract</summary>
Graph convolutional networks have been widely applied in skeleton-based gait recognition. A key challenge in this task is to distinguish the individual walking styles of different subjects across various views. Existing state-of-the-art methods employ uniform convolutions to extract features from diverse sequences and ignore the effects of viewpoint changes. To overcome these limitations, we propose a condition-adaptive graph (CAG) convolution network that can dynamically adapt to the specific attributes of each skeleton sequence and the corresponding view angle. In contrast to using fixed weights for all joints and sequences, we introduce a joint-specific filter learning (JSFL) module in the CAG method, which produces sequence-adaptive filters at the joint level. The adaptive filters capture fine-grained patterns that are unique to each joint, enabling the extraction of diverse spatial-temporal information about body parts. Additionally, we design a view-adaptive topology learning (VATL) module that generates adaptive graph topologies. These graph topologies are used to correlate the joints adaptively according to the specific view conditions. Thus, CAG can simultaneously adjust to various walking styles and viewpoints. Experiments on the two most widely used datasets (i.e., CASIA-B and OU-MVLP) show that CAG surpasses all previous skeleton-based methods. Moreover, the recognition performance can be enhanced by simply combining CAG with appearance-based methods, demonstrating the ability of CAG to provide useful complementary information.The source code will be available at https://github.com/OliverHxh/CAG.
</details>
<details>
<summary>摘要</summary>
“几何卷积网络在人体骨架基于步行识别中广泛应用。一个关键挑战在这个任务中是在不同的视角下分辨别人的步行风格。现有的状态艺术方法使用固定的权重来抽取不同序列中的特征，并忽略视角变化的影响。为了解决这些限制，我们提议一种可适应条件的几何卷积网络（CAG），可以动态适应每个骨架序列和相应的视角。而不是使用所有关节和序列中的固定权重，我们引入了关节特定的缓冲学（JSFL）模块，该模块生成序列特有的缓冲。这些缓冲能够捕捉每个关节细腻的特征，并提取不同的空间-时间信息。此外，我们设计了视角适应图学（VATL）模块，该模块生成适应视角的图学结构。这些图学结构用于相互相关关节，以适应特定的视角条件。因此，CAG可以同时适应不同的步行风格和视角。实验结果表明，CAG超过了所有之前的骨架基于方法，并且可以通过简单地将CAG与外观基于方法相结合，进一步提高识别性能。代码将在 GitHub 上发布，请参考 <https://github.com/OliverHxh/CAG>。”
</details></li>
</ul>
<hr>
<h2 id="Isomer-Isomerous-Transformer-for-Zero-shot-Video-Object-Segmentation"><a href="#Isomer-Isomerous-Transformer-for-Zero-shot-Video-Object-Segmentation" class="headerlink" title="Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation"></a>Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06693">http://arxiv.org/abs/2308.06693</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dlut-yyc/isomer">https://github.com/dlut-yyc/isomer</a></li>
<li>paper_authors: Yichen Yuan, Yifan Wang, Lijun Wang, Xiaoqi Zhao, Huchuan Lu, Yu Wang, Weibo Su, Lei Zhang</li>
<li>for: 这个论文主要针对 Zero-Shot Video Object Segmentation (ZVOS) 任务，即在不使用任何 annotated video data 的情况下，将视频中的 объекты segmentation 到准确的位置和类别。</li>
<li>methods: 该论文提出了两种基于 Transformer 的方法，分别是 Context-Sharing Transformer (CST) 和 Semantic Gathering-Scattering Transformer (SGST)，以提高 ZVOS 的性能和计算效率。</li>
<li>results: 与基eline相比，该论文的方法在 ZVOS 任务中具有新的 state-of-the-art 性能，同时提高了计算效率，相比基eline的 13 倍。 Code 可以在 <a target="_blank" rel="noopener" href="https://github.com/DLUT-yyc/Isomer">https://github.com/DLUT-yyc/Isomer</a> 上下载。<details>
<summary>Abstract</summary>
Recent leading zero-shot video object segmentation (ZVOS) works devote to integrating appearance and motion information by elaborately designing feature fusion modules and identically applying them in multiple feature stages. Our preliminary experiments show that with the strong long-range dependency modeling capacity of Transformer, simply concatenating the two modality features and feeding them to vanilla Transformers for feature fusion can distinctly benefit the performance but at a cost of heavy computation. Through further empirical analysis, we find that attention dependencies learned in Transformer in different stages exhibit completely different properties: global query-independent dependency in the low-level stages and semantic-specific dependency in the high-level stages. Motivated by the observations, we propose two Transformer variants: i) Context-Sharing Transformer (CST) that learns the global-shared contextual information within image frames with a lightweight computation. ii) Semantic Gathering-Scattering Transformer (SGST) that models the semantic correlation separately for the foreground and background and reduces the computation cost with a soft token merging mechanism. We apply CST and SGST for low-level and high-level feature fusions, respectively, formulating a level-isomerous Transformer framework for ZVOS task. Compared with the baseline that uses vanilla Transformers for multi-stage fusion, ours significantly increase the speed by 13 times and achieves new state-of-the-art ZVOS performance. Code is available at https://github.com/DLUT-yyc/Isomer.
</details>
<details>
<summary>摘要</summary>
现代领先的零shot视频对象分割（ZVOS）方法强调 интеграción appeared和动作信息，通过设计优化的特征融合模块来实现。我们的初步实验表明，使用强大的长距离依赖模型Transformer可以明显提高性能，但是需要高计算成本。通过进一步的实验分析，我们发现Transformer中不同阶段的注意力关系都有不同性质：低阶段的全局缺省关系和高阶段的Semantic特定关系。这些发现驱动我们提出两种Transformer变体：i) 共享上下文Transformer（CST），通过轻量级计算学习图像帧中的全局共享上下文信息。ii)  semantic聚合散发Transformer（SGST），通过软token合并机制模型对eground和background的semantic相关性，减少计算成本。我们在不同阶段使用CST和SGST进行特征融合，组成了级别异谱Transformer框架，与基eline相比，我们的方法可以提高13倍的速度，并达到新的ZVOS性能记录。代码可以在https://github.com/DLUT-yyc/Isomer上下载。
</details></li>
</ul>
<hr>
<h2 id="SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency"><a href="#SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency" class="headerlink" title="SimMatchV2: Semi-Supervised Learning with Graph Consistency"></a>SimMatchV2: Semi-Supervised Learning with Graph Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06692">http://arxiv.org/abs/2308.06692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mingkai-zheng/simmatchv2">https://github.com/mingkai-zheng/simmatchv2</a></li>
<li>paper_authors: Mingkai Zheng, Shan You, Lang Huang, Chen Luo, Fei Wang, Chen Qian, Chang Xu</li>
<li>for: 这个论文目的是提出一种新的半监督学习算法，以解决计算机视觉领域中的半监督图像分类问题。</li>
<li>methods: 该算法基于图 teoría的消息传递和节点分类，并提出了四种一致性，包括节点-节点一致性、节点-边一致性、边-边一致性和边-节点一致性。</li>
<li>results: 该算法在多个半监督学习benchmark上进行验证，与ResNet-50作为背景网络和300个训练 epoch，SimMatchV2实现了71.9%和76.2%的Top-1准确率，分别使用1%和10%的标注样本。这些成果在之前的方法中显著超越，达到了状态作准的性能。<details>
<summary>Abstract</summary>
Semi-Supervised image classification is one of the most fundamental problem in computer vision, which significantly reduces the need for human labor. In this paper, we introduce a new semi-supervised learning algorithm - SimMatchV2, which formulates various consistency regularizations between labeled and unlabeled data from the graph perspective. In SimMatchV2, we regard the augmented view of a sample as a node, which consists of a label and its corresponding representation. Different nodes are connected with the edges, which are measured by the similarity of the node representations. Inspired by the message passing and node classification in graph theory, we propose four types of consistencies, namely 1) node-node consistency, 2) node-edge consistency, 3) edge-edge consistency, and 4) edge-node consistency. We also uncover that a simple feature normalization can reduce the gaps of the feature norm between different augmented views, significantly improving the performance of SimMatchV2. Our SimMatchV2 has been validated on multiple semi-supervised learning benchmarks. Notably, with ResNet-50 as our backbone and 300 epochs of training, SimMatchV2 achieves 71.9\% and 76.2\% Top-1 Accuracy with 1\% and 10\% labeled examples on ImageNet, which significantly outperforms the previous methods and achieves state-of-the-art performance. Code and pre-trained models are available at \href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2}.
</details>
<details>
<summary>摘要</summary>
semi-supervised图像分类是计算机视觉中最基本的问题之一，可以减少人工劳动。在这篇论文中，我们介绍了一种新的semi-supervised学习算法——SimMatchV2，它在图像视角下对各个样本进行了不同的拓展视图，并在图表视角下定义了多种一致性规范。在SimMatchV2中，我们将每个样本的拓展视图看作一个节点，这些节点之间通过 Edge 连接， Edge 的 Similarity 度量节点表示的一致性。我们提出了四种一致性类型：1）节点-节点一致性，2）节点-边一致性，3）边-边一致性，4）边-节点一致性。我们还发现，一个简单的特征Normalization可以降低不同拓展视图特征的差异，从而提高SimMatchV2的性能。我们的SimMatchV2在多个 semi-supervised 学习 benchmark 上进行了验证，与 ResNet-50 作为背景网络和 300  epoch 训练，SimMatchV2 在 ImageNet 上 achieve 71.9% 和 76.2% Top-1 Accuracy  WITH 1% 和 10% 标注样本，显著超过先前的方法，实现了状态的最佳性能。代码和预训练模型可以在 \href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Estimator-Meets-Equilibrium-Perspective-A-Rectified-Straight-Through-Estimator-for-Binary-Neural-Networks-Training"><a href="#Estimator-Meets-Equilibrium-Perspective-A-Rectified-Straight-Through-Estimator-for-Binary-Neural-Networks-Training" class="headerlink" title="Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training"></a>Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06689">http://arxiv.org/abs/2308.06689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dravenalg/reste">https://github.com/dravenalg/reste</a></li>
<li>paper_authors: Xiao-Ming Wu, Dian Zheng, Zuhao Liu, Wei-Shi Zheng</li>
<li>for: 这个论文的目的是提出一种能够充分考虑条件对应网络的训练稳定性的条件对应网络训练方法。</li>
<li>methods: 这个论文使用了一种名为Rectified Straight Through Estimator（ReSTE）的新的条件对应网络训练方法，它可以充分考虑条件对应网络的训练稳定性。</li>
<li>results: 实验结果显示，ReSTE可以在CIFAR-10和ImageNet datasets上 achieve excellent performance，并且比其他方法（不含任何辅助模组或损失）还要好。<details>
<summary>Abstract</summary>
Binarization of neural networks is a dominant paradigm in neural networks compression. The pioneering work BinaryConnect uses Straight Through Estimator (STE) to mimic the gradients of the sign function, but it also causes the crucial inconsistency problem. Most of the previous methods design different estimators instead of STE to mitigate it. However, they ignore the fact that when reducing the estimating error, the gradient stability will decrease concomitantly. These highly divergent gradients will harm the model training and increase the risk of gradient vanishing and gradient exploding. To fully take the gradient stability into consideration, we present a new perspective to the BNNs training, regarding it as the equilibrium between the estimating error and the gradient stability. In this view, we firstly design two indicators to quantitatively demonstrate the equilibrium phenomenon. In addition, in order to balance the estimating error and the gradient stability well, we revise the original straight through estimator and propose a power function based estimator, Rectified Straight Through Estimator (ReSTE for short). Comparing to other estimators, ReSTE is rational and capable of flexibly balancing the estimating error with the gradient stability. Extensive experiments on CIFAR-10 and ImageNet datasets show that ReSTE has excellent performance and surpasses the state-of-the-art methods without any auxiliary modules or losses.
</details>
<details>
<summary>摘要</summary>
neural networks 的归纳化是现代神经网络压缩的主导方法。 BinaryConnect 开创性的工作使用 Straight Through Estimator (STE) 模仿签名函数的梯度，但也会导致重要的不一致问题。 前一些方法设计不同的估计器来缓解这个问题，但它们忽略了当减少估计错误时，模型的梯度稳定性会降低。这些高度不同梯度会危害模型的训练和梯度涨落和爆炸。为了充分考虑梯度稳定性，我们提出了一新的审视方法，将 BNNs 训练视为梯度稳定性和估计错误之间的平衡。在这种视角下，我们首先设计了两个指标来量化平衡现象。此外，为了平衡估计错误和梯度稳定性，我们修改了原始的直通估计器，并提出了一个功能基于 rectified straight through estimator (ReSTE)。与其他估计器相比，ReSTE 是理性的，可以很好地平衡估计错误和梯度稳定性。我们对 CIFAR-10 和 ImageNet  dataset 进行了广泛的实验，结果表明 ReSTE 表现出色，超过了当前的状态艺术方法，不需要任何辅助模块或损失。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges"><a href="#Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges" class="headerlink" title="Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges"></a>Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06668">http://arxiv.org/abs/2308.06668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiajiali04/agriculture-foundation-models">https://github.com/jiajiali04/agriculture-foundation-models</a></li>
<li>paper_authors: Jiajia Li, Mingle Xu, Lirong Xiang, Dong Chen, Weichao Zhuang, Xunyuan Yin, Zhaojian Li</li>
<li>for: 本研究旨在探讨基础模型（Foundation Model，FM）在智能农业领域的潜力。</li>
<li>methods: 本研究首先对最新的FM进行了 обзор，并将其分为四类：语言FM、视觉FM、多模态FM和强化学习FM。然后，我们详细介绍了在农业领域开发农业FM的过程，以及其在智能农业中的潜在应用。</li>
<li>results: 本研究通过对FM的探讨，提供了一个新的AI在农业领域的发展方向，即基于FM的智能农业系统。这种系统可以减少大量标注数据的依赖，提高效率和通用性。同时，我们还描述了在开发农业FM时的独特挑战，包括模型训练、验证和部署。<details>
<summary>Abstract</summary>
The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field. To this end, we first review recent FMs in the general computer science domain and categorize them into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Subsequently, we outline the process of developing agriculture FMs and discuss their potential applications in smart agriculture. We also discuss the unique challenges associated with developing AFMs, including model training, validation, and deployment. Through this study, we contribute to the advancement of AI in agriculture by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems.
</details>
<details>
<summary>摘要</summary>
过去一代，机器学习（ML）和深度学习（DL）方法在农业系统中得到了迅速发展，在各种农业应用中显示出了很大成功。然而，传统的ML/DL模型有一些局限性：它们需要大量、成本高的标注数据进行训练，需要专业知识进行开发和维护，而且主要针对特定任务，缺乏总体化性。在最近的几年，基础模型（FM）在语言和视觉任务中获得了很大成功。这些模型在多个领域和模式上训练了庞大数据。一旦训练完成，它们可以通过微调和微量标注数据完成多种任务。despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field. To this end, we first review recent FMs in the general computer science domain and categorize them into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Subsequently, we outline the process of developing agriculture FMs and discuss their potential applications in smart agriculture. We also discuss the unique challenges associated with developing AFMs, including model training, validation, and deployment. Through this study, we contribute to the advancement of AI in agriculture by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems.
</details></li>
</ul>
<hr>
<h2 id="Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks"><a href="#Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks" class="headerlink" title="Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks"></a>Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06654">http://arxiv.org/abs/2308.06654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</li>
<li>for: 预测步行人的轨迹是自动驾驶车辆安全导航中的关键能力，特别是在与步行人共享空间时。步行人运动在共享空间中受到汽车和其他步行人的影响，因此可以更好地模型步行人-汽车和步行人之间的交互，从而提高步行人轨迹预测模型的准确性。</li>
<li>methods: 我们提出了一种基于启发的方法，通过计算碰撞风险来选择交互对象。我们将关注与目标步行人之间可能碰撞的两个代理的时间到碰撞和方向角来编码交互效果。我们还 introduce了一种新的方向角坐标系，以便更好地表示交互对象之间的位势。</li>
<li>results: 我们的结果显示，与基eline方法（用作比较）相比，我们的方法在HBS数据集上预测的轨迹更加准确。<details>
<summary>Abstract</summary>
Predicting pedestrians' trajectories is a crucial capability for autonomous vehicles' safe navigation, especially in spaces shared with pedestrians. Pedestrian motion in shared spaces is influenced by both the presence of vehicles and other pedestrians. Therefore, effectively modelling both pedestrian-pedestrian and pedestrian-vehicle interactions can increase the accuracy of the pedestrian trajectory prediction models. Despite the huge literature on ways to encode the effect of interacting agents on a pedestrian's predicted trajectory using deep-learning models, limited effort has been put into the effective selection of interacting agents. In the majority of cases, the interaction features used are mainly based on relative distances while paying less attention to the effect of the velocity and approaching direction in the interaction formulation. In this paper, we propose a heuristic-based process of selecting the interacting agents based on collision risk calculation. Focusing on interactions of potentially colliding agents with a target pedestrian, we propose the use of time-to-collision and the approach direction angle of two agents for encoding the interaction effect. This is done by introducing a novel polar collision grid map. Our results have shown predicted trajectories closer to the ground truth compared to existing methods (used as a baseline) on the HBS dataset.
</details>
<details>
<summary>摘要</summary>
预测行人轨迹是自动驾驶车辆安全导航中的关键能力，特别是在与行人共享空间时。行人运动在共享空间中受到车辆和其他行人的影响。因此，可以准确地模拟行人与车辆和其他行人之间的交互，可以提高行人轨迹预测模型的准确性。虽然有很大的文献研究了使用深度学习模型来编码交互代理的影响，但是对选择交互代理的有效选择尚未得到足够的关注。大多数情况下，交互特征 mainly based on relative distances，而忽略了交互形式中 velocities和接近方向的影响。在这篇论文中，我们提出了一种基于冲突风险计算的交互代理选择规则。关注可能发生冲突的两个代理之间的时间差距和接近方向角，以编码交互效果。我们通过引入一种新的圆形冲突网格地图来实现这一点。我们的结果显示，与基eline方法（作为参照）相比，我们的方法在HBS数据集上预测轨迹更加准确。
</details></li>
</ul>
<hr>
<h2 id="Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition"><a href="#Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition" class="headerlink" title="Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition"></a>Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11633">http://arxiv.org/abs/2308.11633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Sheffield, Frank E. Bobe III, Bradley Marchand, Matthew S. Emigh</li>
<li>for: 提高水下探测技术的精度和效率</li>
<li>methods: 使用自主学习方法（SSL）处理SAS数据，进行分类和特征识别</li>
<li>results: 实验结果表明，MoCo-SAS在F1分数方面表现 significanly better than传统的指导学习方法，这表明SSL在SAS数据处理中具有潜在的应用前景和可能性。<details>
<summary>Abstract</summary>
Synthetic Aperture Sonar (SAS) imaging has become a crucial technology for underwater exploration because of its unique ability to maintain resolution at increasing ranges, a characteristic absent in conventional sonar techniques. However, the effective application of deep learning to SAS data processing is often limited due to the scarcity of labeled data. To address this challenge, this paper proposes MoCo-SAS that leverages self-supervised learning (SSL) for SAS data processing, classification, and pattern recognition. The experimental results demonstrate that MoCo-SAS significantly outperforms traditional supervised learning methods, as evidenced by significant improvements observed in terms of the F1-score. These findings highlight the potential of SSL in advancing the state-of-the-art in SAS data processing, offering promising avenues for enhanced underwater object detection and classification.
</details>
<details>
<summary>摘要</summary>
美式 Synthetic Aperture Sonar（SAS）成像技术在水下探索中变得非常重要，因为它可以保持分辨率随距离增长，这是传统sonar技术缺乏的特点。然而，通常的深度学习应用于SAS数据处理中频繁受限因为标注数据的罕见。为解决这个挑战，这篇论文提议了MoCo-SAS，它利用自动标注学习（SSL）进行SAS数据处理、分类和模式识别。实验结果表明，MoCo-SAS在F1分数方面有显著提高，比传统监督学习方法要好。这些发现表明SSL在SAS数据处理中具有潜在的潜力，提供了更好的水下对象检测和分类技术。
</details></li>
</ul>
<hr>
<h2 id="3DMOTFormer-Graph-Transformer-for-Online-3D-Multi-Object-Tracking"><a href="#3DMOTFormer-Graph-Transformer-for-Online-3D-Multi-Object-Tracking" class="headerlink" title="3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking"></a>3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06635">http://arxiv.org/abs/2308.06635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsx0511/3dmotformer">https://github.com/dsx0511/3dmotformer</a></li>
<li>paper_authors: Shuxiao Ding, Eike Rehder, Lukas Schneider, Marius Cordts, Juergen Gall</li>
<li>for: 这篇论文的目的是提出一种学习基于 transformer 架构的三维物体跟踪（3DMOT）方法，以提高自动驾驶 vehicle 的精度和可靠性。</li>
<li>methods: 本文使用 Edge-Augmented Graph Transformer 来在帧帧基础上进行 track-detection 图грамreasoning，并通过边类划分进行数据归一化。在线上训练中，我们提出了一种novel的自适应训练策略，包括循环和回归的前进 pass，以及顺序批量优化。</li>
<li>results: 使用 CenterPoint 检测结果，本文的方法实现了 71.2% 和 68.2% AMOTA 在 nuScenes 验证和测试分别，并且一个训练好的 3DMOTFormer 模型可以在不同的物体检测器上进行泛化。<details>
<summary>Abstract</summary>
Tracking 3D objects accurately and consistently is crucial for autonomous vehicles, enabling more reliable downstream tasks such as trajectory prediction and motion planning. Based on the substantial progress in object detection in recent years, the tracking-by-detection paradigm has become a popular choice due to its simplicity and efficiency. State-of-the-art 3D multi-object tracking (MOT) approaches typically rely on non-learned model-based algorithms such as Kalman Filter but require many manually tuned parameters. On the other hand, learning-based approaches face the problem of adapting the training to the online setting, leading to inevitable distribution mismatch between training and inference as well as suboptimal performance. In this work, we propose 3DMOTFormer, a learned geometry-based 3D MOT framework building upon the transformer architecture. We use an Edge-Augmented Graph Transformer to reason on the track-detection bipartite graph frame-by-frame and conduct data association via edge classification. To reduce the distribution mismatch between training and inference, we propose a novel online training strategy with an autoregressive and recurrent forward pass as well as sequential batch optimization. Using CenterPoint detections, our approach achieves 71.2% and 68.2% AMOTA on the nuScenes validation and test split, respectively. In addition, a trained 3DMOTFormer model generalizes well across different object detectors. Code is available at: https://github.com/dsx0511/3DMOTFormer.
</details>
<details>
<summary>摘要</summary>
Tracking 3D 物体 precisely 和 consistently 是自动驾驶车辆中关键的，允许更可靠的下游任务，如轨迹预测和运动规划。基于近年来对 объек detection 的重要进步，跟踪-by-detection 方法在现场中变得越来越受欢迎，因为它的简单性和效率。当前的 3D 多对象跟踪（MOT）方法通常采用非学习基于模型的方法，如卡尔曼筛滤器，但是需要许多手动调整的参数。在另一方面，学习基于approaches 在线设定中遇到了适应训练的问题，导致在执行和训练之间的分布差异，以及不佳的性能。在这种情况下，我们提出了 3DMOTFormer，一种基于 transformer 架构的学习geometry-based 3D MOT 框架。我们使用 Edge-Augmented Graph Transformer 来在每帧上对跟踪-检测二分图进行理解，并通过边类别进行数据关联。为了减少训练和执行之间的分布差异，我们提出了一种新的在线训练策略，包括自适应循环和回归前进 pass，以及顺序批量优化。使用 CenterPoint 检测，我们的方法实现了 71.2% 和 68.2% AMOTA 在 nuScenes 验证和测试分割中，并且一个训练过的 3DMOTFormer 模型具有良好的泛化性。代码可以在 GitHub 上找到：https://github.com/dsx0511/3DMOTFormer。
</details></li>
</ul>
<hr>
<h2 id="Fusion-GRU-A-Deep-Learning-Model-for-Future-Bounding-Box-Prediction-of-Traffic-Agents-in-Risky-Driving-Videos"><a href="#Fusion-GRU-A-Deep-Learning-Model-for-Future-Bounding-Box-Prediction-of-Traffic-Agents-in-Risky-Driving-Videos" class="headerlink" title="Fusion-GRU: A Deep Learning Model for Future Bounding Box Prediction of Traffic Agents in Risky Driving Videos"></a>Fusion-GRU: A Deep Learning Model for Future Bounding Box Prediction of Traffic Agents in Risky Driving Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06628">http://arxiv.org/abs/2308.06628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Monjurul Karim, Ruwen Qin, Yinhai Wang</li>
<li>for: 预测周围交通代理人员未来矩形 bounding box 以确保自动驾驶车辆和高级驾驶协助系统在复杂交通场景中安全和高效 navigate.</li>
<li>methods: 本文提出了一种 novel encoder-decoder 架构 called Fusion-GRU, which accounts for the mutual and complex interactions among input features, and uses an intermediary estimator coupled with a self-attention aggregation layer to learn sequential dependencies for long-range prediction.</li>
<li>results: 实验结果表明 Fusion-GRU 能够有效地预测交通代理人员未来矩形 bounding box, 并且在 ROL 和 HEV-I 两个公共数据集上达到了出色的表现。<details>
<summary>Abstract</summary>
To ensure the safe and efficient navigation of autonomous vehicles and advanced driving assistance systems in complex traffic scenarios, predicting the future bounding boxes of surrounding traffic agents is crucial. However, simultaneously predicting the future location and scale of target traffic agents from the egocentric view poses challenges due to the vehicle's egomotion causing considerable field-of-view changes. Moreover, in anomalous or risky situations, tracking loss or abrupt motion changes limit the available observation time, requiring learning of cues within a short time window. Existing methods typically use a simple concatenation operation to combine different cues, overlooking their dynamics over time. To address this, this paper introduces the Fusion-Gated Recurrent Unit (Fusion-GRU) network, a novel encoder-decoder architecture for future bounding box localization. Unlike traditional GRUs, Fusion-GRU accounts for mutual and complex interactions among input features. Moreover, an intermediary estimator coupled with a self-attention aggregation layer is also introduced to learn sequential dependencies for long range prediction. Finally, a GRU decoder is employed to predict the future bounding boxes. The proposed method is evaluated on two publicly available datasets, ROL and HEV-I. The experimental results showcase the promising performance of the Fusion-GRU, demonstrating its effectiveness in predicting future bounding boxes of traffic agents.
</details>
<details>
<summary>摘要</summary>
要确保自动驾驶车和高级驾驶帮助系统在复杂交通场景中安全和高效地导航，预测周围交通代理的未来矩形框是关键。然而，同时预测目标交通代理的未来位置和Scale从 egocentric 视图出现困难，由于车辆的 egomotion 导致了 considrable 视场变化。此外，在异常或危险情况下，跟踪损失或突然运动变化限制了可用观察时间，需要学习在短时间窗口内的信号。现有方法通常使用简单的 concatenation 操作将不同的信号组合起来，忽略他们在时间上的动态变化。为解决这个问题，本文提出了 Fusion-Gated Recurrent Unit (Fusion-GRU) 网络，一种新的编码器-解码器架构 для 未来矩形框 Localization。与传统 GRU 不同，Fusion-GRU 考虑了输入特征之间的相互和复杂交互。此外，一个中间估计器和一个自我注意汇聚层也是引入，以学习长距离预测的时间序列关系。最后，一个 GRU 解码器用于预测未来矩形框。提案的方法在 ROL 和 HEV-I 两个公共可用数据集上进行了测试，实验结果表明 Fusion-GRU 的批处理能力很出色，证明其在预测交通代理未来矩形框方面的效果是非常有 Promise。
</details></li>
</ul>
<hr>
<h2 id="ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss"><a href="#ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss" class="headerlink" title="ADRMX: Additive Disentanglement of Domain Features with Remix Loss"></a>ADRMX: Additive Disentanglement of Domain Features with Remix Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06624">http://arxiv.org/abs/2308.06624</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berkerdemirel/ADRMX">https://github.com/berkerdemirel/ADRMX</a></li>
<li>paper_authors: Berker Demirel, Erchan Aptoula, Huseyin Ozkan</li>
<li>for: 这个研究目的是为了实现多域领域对应，即将模型从多个来源领域中撷取具有通用性的特征，以减少域别的分布变化对模型的影响。</li>
<li>methods: 本研究使用了一种名为“添加式分离”的新架构，将域别特征与域共通特征整合在一起，以实现域variant特征的捕捉。此外，还引入了一种新的数据增强技术，将不同域的样本混合在维度空间中，以进一步支持模型的通用能力。</li>
<li>results: 经过广泛的DomainBed experiments，ADRMX模型在竞争性的情况下实现了州际状态的表现，并且超过了现有的模型。代码将会在GitHub上公开。<details>
<summary>Abstract</summary>
The common assumption that train and test sets follow similar distributions is often violated in deployment settings. Given multiple source domains, domain generalization aims to create robust models capable of generalizing to new unseen domains. To this end, most of existing studies focus on extracting domain invariant features across the available source domains in order to mitigate the effects of inter-domain distributional changes. However, this approach may limit the model's generalization capacity by relying solely on finding common features among the source domains. It overlooks the potential presence of domain-specific characteristics that could be prevalent in a subset of domains, potentially containing valuable information. In this work, a novel architecture named Additive Disentanglement of Domain Features with Remix Loss (ADRMX) is presented, which addresses this limitation by incorporating domain variant features together with the domain invariant ones using an original additive disentanglement strategy. Moreover, a new data augmentation technique is introduced to further support the generalization capacity of ADRMX, where samples from different domains are mixed within the latent space. Through extensive experiments conducted on DomainBed under fair conditions, ADRMX is shown to achieve state-of-the-art performance. Code will be made available at GitHub after the revision process.
</details>
<details>
<summary>摘要</summary>
通常假设训练集和测试集都遵循相似的分布是在部署场景下常被违反的。面对多个源领域，领域泛化目标是创建可以泛化到新未经见过的领域的Robust模型。为此，大多数现有的研究都是EXTRACTING DOMAIN INVARIANT FEATURES ACROSS AVAILABLE SOURCE DOMAINS，以减少INTER-DOMAIN分布变化的影响。然而，这种方法可能会限制模型的泛化能力，因为它只是在多个源领域中找到共同特征。这会忽略可能在一些领域中具有价值信息的领域特有特征。在这种工作中，一种新的架构被提出，即Additive Disentanglement of Domain Features with Remix Loss（ADRMX），它解决了这个限制。ADRMX通过将领域特征与领域 invariant 特征相加来实现这一点。此外，一种新的数据增强技术也被引入，其中来自不同领域的样本被混合在离散空间中。经过了EXTENSIVE EXPERIMENTS CONDUCTED ON DOMAINBED UNDER FAIR CONDITIONS，ADRMX得到了状态机器的表现。代码将在GitHub上公布后进行修订。
</details></li>
</ul>
<hr>
<h2 id="Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation"><a href="#Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation" class="headerlink" title="Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?"></a>Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06623">http://arxiv.org/abs/2308.06623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RisabBiswas/Polyp-SAM-PlusPlus">https://github.com/RisabBiswas/Polyp-SAM-PlusPlus</a></li>
<li>paper_authors: Risab Biswas</li>
<li>for: The paper is written for the task of polyp segmentation in medical images, with the goal of improving the accuracy and robustness of the segmentation process.</li>
<li>methods: The paper uses the Segment Anything Model (SAM) as the base model for polyp segmentation, and incorporates text prompting to guide the segmentation process.</li>
<li>results: The paper evaluates the performance of the text-guided SAM on benchmark datasets and compares the results with unprompted SAM. The results show that the text-guided SAM achieves better segmentation accuracy and robustness than unprompted SAM.Here are the three points in Simplified Chinese text:</li>
<li>for: 本文是为医疗图像中的肿吸分 segmentation任务而写的，目标是提高分 segmentation的准确性和稳定性。</li>
<li>methods: 本文使用 Segment Anything Model (SAM) 作为基本模型，并通过文本提示来导引分 segmentation 过程。</li>
<li>results: 本文对 benchmark 数据集进行评估，并比较文本提示 SAM 和无提示 SAM 的结果。结果显示，文本提示 SAM 在分 segmentation 任务上的性能更高、更稳定。<details>
<summary>Abstract</summary>
Meta recently released SAM (Segment Anything Model) which is a general-purpose segmentation model. SAM has shown promising results in a wide variety of segmentation tasks including medical image segmentation. In the field of medical image segmentation, polyp segmentation holds a position of high importance, thus creating a model which is robust and precise is quite challenging. Polyp segmentation is a fundamental task to ensure better diagnosis and cure of colorectal cancer. As such in this study, we will see how Polyp-SAM++, a text prompt-aided SAM, can better utilize a SAM using text prompting for robust and more precise polyp segmentation. We will evaluate the performance of a text-guided SAM on the polyp segmentation task on benchmark datasets. We will also compare the results of text-guided SAM vs unprompted SAM. With this study, we hope to advance the field of polyp segmentation and inspire more, intriguing research. The code and other details will be made publically available soon at https://github.com/RisabBiswas/Polyp-SAM++.
</details>
<details>
<summary>摘要</summary>
meta 最近发布了 SAM（ Segment Anything Model），这是一个通用分割模型。 SAM 在多种分割任务中表现出色，包括医疗图像分割。在医疗图像分割领域，肿瘤分割具有非常高的重要性，因此创建一个稳定和精准的模型非常具有挑战性。肿瘤分割是检测和治疗潜肿瘤的基础任务。在这项研究中，我们将研究如何使用文本提示来更好地使用 SAM 进行肿瘤分割。我们将对文本引导 SAM 在标准数据集上进行评估，并与不引导 SAM 进行比较。我们希望通过这项研究，推动肿瘤分割领域的发展，并鼓励更多的激动人心的研究。代码和其他细节将在 https://github.com/RisabBiswas/Polyp-SAM++ 上公开。
</details></li>
</ul>
<hr>
<h2 id="DFM-X-Augmentation-by-Leveraging-Prior-Knowledge-of-Shortcut-Learning"><a href="#DFM-X-Augmentation-by-Leveraging-Prior-Knowledge-of-Shortcut-Learning" class="headerlink" title="DFM-X: Augmentation by Leveraging Prior Knowledge of Shortcut Learning"></a>DFM-X: Augmentation by Leveraging Prior Knowledge of Shortcut Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06622">http://arxiv.org/abs/2308.06622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nis-research/dfmx-augmentation">https://github.com/nis-research/dfmx-augmentation</a></li>
<li>paper_authors: Shunxin Wang, Christoph Brune, Raymond Veldhuis, Nicola Strisciuglio</li>
<li>for: 提高模型的普适性和鲁棒性，防止神经网络学习 superficiale 的统计学特征，从而提高模型的泛化能力和鲁棒性。</li>
<li>methods: 提出了一种数据增强策略，称为DFM-X，该策略利用了预测模型中的主导频率图（DFM）来避免神经网络学习快捷解决方案。</li>
<li>results: 实验结果表明，DFM-X 可以提高模型对常见损害和攻击的Robustness，并且可以轻松地与其他增强技术结合使用，以进一步提高模型的泛化能力和鲁棒性。<details>
<summary>Abstract</summary>
Neural networks are prone to learn easy solutions from superficial statistics in the data, namely shortcut learning, which impairs generalization and robustness of models. We propose a data augmentation strategy, named DFM-X, that leverages knowledge about frequency shortcuts, encoded in Dominant Frequencies Maps computed for image classification models. We randomly select X% training images of certain classes for augmentation, and process them by retaining the frequencies included in the DFMs of other classes. This strategy compels the models to leverage a broader range of frequencies for classification, rather than relying on specific frequency sets. Thus, the models learn more deep and task-related semantics compared to their counterpart trained with standard setups. Unlike other commonly used augmentation techniques which focus on increasing the visual variations of training data, our method targets exploiting the original data efficiently, by distilling prior knowledge about destructive learning behavior of models from data. Our experimental results demonstrate that DFM-X improves robustness against common corruptions and adversarial attacks. It can be seamlessly integrated with other augmentation techniques to further enhance the robustness of models.
</details>
<details>
<summary>摘要</summary>
We use Dominant Frequencies Maps (DFMs) to identify the frequency shortcuts that image classification models are prone to learning. We then select a percentage of training images from certain classes and process them by retaining the frequencies included in the DFMs of other classes. This forces the models to use a broader range of frequencies for classification, rather than relying on specific frequency sets.Unlike other augmentation techniques that focus on increasing visual variations in the training data, DFM-X targets the efficient use of the original data by leveraging prior knowledge about the destructive learning behavior of models. Our experimental results show that DFM-X improves the robustness of models against common corruptions and adversarial attacks. It can be easily integrated with other augmentation techniques to further enhance the robustness of models.
</details></li>
</ul>
<hr>
<h2 id="LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net"><a href="#LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net" class="headerlink" title="LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net"></a>LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06603">http://arxiv.org/abs/2308.06603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ach-1914/ladlenet">https://github.com/ach-1914/ladlenet</a></li>
<li>paper_authors: Tonghui Zou</li>
<li>for: 这 paper 的目的是将thermal infrared (TIR) 图像转换成可见光 (VI) 图像，并且可以应用于多个领域，如TIR-VI 图像registratin 和融合。</li>
<li>methods: 这 paper 使用了一种基于 U-Net 架构的算法，称为 LadleNet，其包括 ‘Handle’ 模块和 ‘Bowl’ 模块。 Handle 模块constructs an abstract semantic space，而 Bowl 模块 decode这 semantic space来生成 mapped VI 图像。 Handle 模块可以通过使用semantic segmentation networks来扩展其网络架构，从而提高模型性能。</li>
<li>results:  comparative experiments 表明， compared to existing methodologies, our approach achieves state-of-the-art performance in terms of image clarity and perceptual quality。<details>
<summary>Abstract</summary>
The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibility by allowing the substitution of its network architecture with semantic segmentation networks, thereby establishing more abstract semantic spaces to bolster model performance. Consequently, we propose LadleNet+, which replaces LadleNet's Handle module with the pre-trained DeepLabv3+ network, thereby endowing the model with enhanced semantic space construction capabilities. The proposed method is evaluated and tested on the KAIST dataset, accompanied by quantitative and qualitative analyses. Compared to existing methodologies, our approach achieves state-of-the-art performance in terms of image clarity and perceptual quality. The source code will be made available at https://github.com/Ach-1914/LadleNet/tree/main/.
</details>
<details>
<summary>摘要</summary>
通过将热成像（TIR）图像转换成可见光（VI）图像，提供了一些应用领域的挑战，如TIR-VI图像匹配和融合。利用TIR图像转换生成的补充信息可以significantly enhance模型性能和泛化性。然而，现有的问题包括低效图像准确性和有限的模型扩展性。在这篇文章中，我们提出了一种算法，即LadleNet，基于U-Net架构。LadleNet使用了两个阶段的U-Net堆叠结构，并添加了跳过连接和精细特征聚合技术，从而实现了显著提高模型性能。LadleNet由“ Handle”和“Bowl”模块组成，其中“ Handle”模块建立了一个抽象的 semantic space，而“Bowl”模块将这个semantic space解码成生成的VI图像。“ Handle”模块具有扩展性，可以通过更改其网络架构来使用semantic segmentation网络，从而建立更加抽象的semantic spaces，以提高模型性能。因此，我们提出了LadleNet+，其替换了LadleNet的“ Handle”模块为预训练的DeepLabv3+网络，从而使模型具有更高的semantic space建立能力。我们的方法在KAIST数据集上进行了评估和测试，并进行了量化和质量分析。相比现有的方法，我们的方法在图像清晰度和感知质量方面达到了状态态的性能。模型源代码将在https://github.com/Ach-1914/LadleNet/tree/main/下提供。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/cs.CV_2023_08_13/" data-id="clohum97400fxpj88ac4y3v84" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/cs.AI_2023_08_13/" class="article-date">
  <time datetime="2023-08-13T12:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/13/cs.AI_2023_08_13/">cs.AI - 2023-08-13</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dual-Meta-Learning-with-Longitudinally-Generalized-Regularization-for-One-Shot-Brain-Tissue-Segmentation-Across-the-Human-Lifespan"><a href="#Dual-Meta-Learning-with-Longitudinally-Generalized-Regularization-for-One-Shot-Brain-Tissue-Segmentation-Across-the-Human-Lifespan" class="headerlink" title="Dual Meta-Learning with Longitudinally Generalized Regularization for One-Shot Brain Tissue Segmentation Across the Human Lifespan"></a>Dual Meta-Learning with Longitudinally Generalized Regularization for One-Shot Brain Tissue Segmentation Across the Human Lifespan</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06774">http://arxiv.org/abs/2308.06774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongheng Sun, Fan Wang, Jun Shu, Haifeng Wang, Li Wang. Deyu Meng, Chunfeng Lian</li>
<li>for: 这个论文旨在提出一种用于批处理数据的脑细胞分割方法，以便于 neuroscience 和临床研究。</li>
<li>methods: 该方法使用 dual meta-learning 模型，包括一个 plug-and-play 特征提取器和一个 initializer 任务头，以学习 longitudinally 一致的表示。此外，两种类 aware 正则化也是提出来鼓励 longitudinal 一致性。</li>
<li>results: 实验结果表明，该方法在 iSeg2019 和 ADNI 数据集上具有效果。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/ladderlab-xjtu/DuMeta">https://github.com/ladderlab-xjtu/DuMeta</a> 上下载。<details>
<summary>Abstract</summary>
Brain tissue segmentation is essential for neuroscience and clinical studies. However, segmentation on longitudinal data is challenging due to dynamic brain changes across the lifespan. Previous researches mainly focus on self-supervision with regularizations and will lose longitudinal generalization when fine-tuning on a specific age group. In this paper, we propose a dual meta-learning paradigm to learn longitudinally consistent representations and persist when fine-tuning. Specifically, we learn a plug-and-play feature extractor to extract longitudinal-consistent anatomical representations by meta-feature learning and a well-initialized task head for fine-tuning by meta-initialization learning. Besides, two class-aware regularizations are proposed to encourage longitudinal consistency. Experimental results on the iSeg2019 and ADNI datasets demonstrate the effectiveness of our method. Our code is available at https://github.com/ladderlab-xjtu/DuMeta.
</details>
<details>
<summary>摘要</summary>
��rett�Brain tissue segmentation是 neuroscience 和 clinical studies 中必备的一环。然而，对 longitudinal 数据进行 segmentation 是一个挑战，因为大脑在生长过程中会发生 dynamically 的变化。先前的研究主要集中在自我超vised 的 regularization 上，这会导致 fine-tuning 时失去 longitudinal 一致性。在这篇论文中，我们提出了 dual meta-learning  парадигма，以学习 longitudinally 一致的表示和 fine-tuning  persist。具体来说，我们学习了一个可插入的 feature extractor，用于抽取 longitudinally 一致的 anatomical 表示，以及一个 Well-Initialized 任务头，用于 fine-tuning。此外，我们还提出了两种类型 aware 的 regularization，以促进 longitudinal 一致性。我们的实验结果在 iSeg2019 和 ADNI 数据集上表明了我们的方法的有效性。我们的代码可以在 https://github.com/ladderlab-xjtu/DuMeta 上获取。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Class-incremental-Learning-A-Survey"><a href="#Few-shot-Class-incremental-Learning-A-Survey" class="headerlink" title="Few-shot Class-incremental Learning: A Survey"></a>Few-shot Class-incremental Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06764">http://arxiv.org/abs/2308.06764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinghua Zhang, Li Liu, Olli Silven, Matti Pietikäinen, Dewen Hu</li>
<li>for: 这篇论文旨在提供关于几拟学习（Few-shot Class-Incremental Learning，FSCIL）的系统性和深入的评论。</li>
<li>methods: 本论文涵盖了FSCIL中的多种方法，包括数据基于、结构基于和优化基于的分类方法以及锚点基于和锚点自由的对象检测方法。</li>
<li>results: 本论文提供了一个彻底的检查和评估的 benchmark 数据集和评价指标，以及一些在FSCIL中的推荐的研究方向。<details>
<summary>Abstract</summary>
Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge in machine learning, as it necessitates the continuous learning of new classes from sparse labeled training samples without forgetting previous knowledge. While this field has seen recent progress, it remains an active area of exploration. This paper aims to provide a comprehensive and systematic review of FSCIL. In our in-depth examination, we delve into various facets of FSCIL, encompassing the problem definition, the discussion of primary challenges of unreliable empirical risk minimization and the stability-plasticity dilemma, general schemes, and relevant problems of incremental learning and few-shot learning. Besides, we offer an overview of benchmark datasets and evaluation metrics. Furthermore, we introduce the classification methods in FSCIL from data-based, structure-based, and optimization-based approaches and the object detection methods in FSCIL from anchor-free and anchor-based approaches. Beyond these, we illuminate several promising research directions within FSCIL that merit further investigation.
</details>
<details>
<summary>摘要</summary>
《几个示例学习（Few-shot Class-Incremental Learning，FSCIL）》是机器学习领域的一个独特挑战，它需要在缺乏标注训练样本的情况下，不断学习新的类型，而不会忘记之前的知识。虽然这一领域已经有了相应的进步，但仍然是一个活跃的探索领域。本文的目的是提供关于FSCIL的全面和系统性的综述，我们在这里进行了深入的检查，涵盖了FSCIL的问题定义、主要挑战、基本方案、相关的增量学习和几个示例学习方法等方面。此外，我们还介绍了FSCIL中的标准数据集和评价指标。在FSCIL中，我们分析了数据基于、结构基于和优化基于的分类方法，以及anchor-free和anchor-based的对应检测方法。此外，我们还透出了FSCIL中一些有前途的研究方向，以便进一步探索和发展这一领域。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-anticipated-outcomes-of-MRI-seizure-image-from-open-source-tool-Prototype-approach"><a href="#Evaluating-the-anticipated-outcomes-of-MRI-seizure-image-from-open-source-tool-Prototype-approach" class="headerlink" title="Evaluating the anticipated outcomes of MRI seizure image from open-source tool- Prototype approach"></a>Evaluating the anticipated outcomes of MRI seizure image from open-source tool- Prototype approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07762">http://arxiv.org/abs/2308.07762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayanthi Vajiram, Aishwarya Senthil, Utkarsh Maurya</li>
<li>for: 这个论文主要用于描述世界各地约70亿人口中 epilepsy 病人的脑部功能障碍和分析。</li>
<li>methods: 本文使用了多种开源神经成像工具，包括 MATLAB、Slicer 3D、Brain Suite21a、SPM 和 MedCalc，以进行脑部功能障碍的检查和分析。大约 60% 的研究人员使用了 MATLAB 进行图像处理，而其他 30% 使用了其他开源软件工具。</li>
<li>results: 根据本文的报告，大约 70% 的研究人员使用了 MATLAB 进行图像处理，而其他 30% 使用了其他开源软件工具。<details>
<summary>Abstract</summary>
Epileptic Seizure is an abnormal neuronal exertion in the brain, affecting nearly 70 million of the world's population (Ngugi et al., 2010). So many open-source neuroimaging tools are used for metabolism checkups and analysis purposes. The scope of open-source tools like MATLAB, Slicer 3D, Brain Suite21a, SPM, and MedCalc are explained in this paper. MATLAB was used by 60% of the researchers for their image processing and 10% of them use their proprietary software. More than 30% of the researchers use other open-source software tools with their processing techniques for the study of magnetic resonance seizure images
</details>
<details>
<summary>摘要</summary>
эпилептический приступ是脑内部不正常的神经舒张，影响全球约70亿人口（ Ngugi et al., 2010）。这么多个开源神经成像工具用于 метаболизма检查和分析。这篇论文中 explain了开源工具如MATLAB、Slicer 3D、Brain Suite21a、SPM 和 MedCalc 的范围。MATLAB 被60%的研究人员用于图像处理，而10%的他们使用自己的专有软件。 более30%的研究人员使用其他开源软件工具进行频率逐点图像的研究
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization"><a href="#Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization" class="headerlink" title="Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization"></a>Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06741">http://arxiv.org/abs/2308.06741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mehdi Nasiri, Mansoor Rezghi</li>
<li>for: solving cooperative Multi-Agent Reinforcement Learning (MARL) problems with varying agent abilities and individual policies</li>
<li>methods: Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO) algorithm, which utilizes the multi-agent advantage decomposition lemma for efficient policy updates and guarantees stability and performance improvements</li>
<li>results: superiority over state-of-the-art algorithms such as HATRPO and HAPPO, demonstrated through experiments on Multi-Agent MuJoCo and StarCraftII tasks<details>
<summary>Abstract</summary>
This paper presents an extension of the Mirror Descent method to overcome challenges in cooperative Multi-Agent Reinforcement Learning (MARL) settings, where agents have varying abilities and individual policies. The proposed Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO) algorithm utilizes the multi-agent advantage decomposition lemma to enable efficient policy updates for each agent while ensuring overall performance improvements. By iteratively updating agent policies through an approximate solution of the trust-region problem, HAMDPO guarantees stability and improves performance. Moreover, the HAMDPO algorithm is capable of handling both continuous and discrete action spaces for heterogeneous agents in various MARL problems. We evaluate HAMDPO on Multi-Agent MuJoCo and StarCraftII tasks, demonstrating its superiority over state-of-the-art algorithms such as HATRPO and HAPPO. These results suggest that HAMDPO is a promising approach for solving cooperative MARL problems and could potentially be extended to address other challenging problems in the field of MARL.
</details>
<details>
<summary>摘要</summary>
HAMDPO uses the multi-agent advantage decomposition lemma to efficiently update agent policies while ensuring improved overall performance. The algorithm iteratively updates agent policies using an approximate solution of the trust-region problem, which guarantees stability and improves performance.HAMDPO is capable of handling both continuous and discrete action spaces for heterogeneous agents in various MARL problems. The authors evaluate HAMDPO on Multi-Agent MuJoCo and StarCraftII tasks and show that it outperforms state-of-the-art algorithms such as HATRPO and HAPPO. These results suggest that HAMDPO is a promising approach for solving cooperative MARL problems and could potentially be extended to address other challenging problems in the field of MARL.Here is the Simplified Chinese translation of the text:这篇论文提出了一种新的方法 called Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO)，用于解决多体决策学习（MARL）中的合作问题。该方法是为了处理每个代理机器人有不同能力和个性政策的情况。HAMDPO 使用多体优势分解证明来有效地更新代理机器人的策略，同时保证总体性能提高。该算法通过迭代更新代理机器人的策略，使用approxSolution 的信任区问题的解决方案，保证稳定性和性能提高。HAMDPO 可以处理不同的连续和离散动作空间，并在多种 MARL 问题上进行应用。作者们通过在 Multi-Agent MuJoCo 和 StarCraftII 任务上评估 HAMDPO，发现它比state-of-the-art 算法如 HATRPO 和 HAPPO 表现出色。这些结果表明，HAMDPO 是一种有前途的方法，可以解决多体决策学习中的合作问题，并可能扩展到其他难题。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data"><a href="#Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data" class="headerlink" title="Probabilistic Imputation for Time-series Classification with Missing Data"></a>Probabilistic Imputation for Time-series Classification with Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06738">http://arxiv.org/abs/2308.06738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout">https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout</a></li>
<li>paper_authors: SeungHyun Kim, Hyunsu Kim, EungGu Yun, Hwangrae Lee, Jaehun Lee, Juho Lee</li>
<li>For: This paper proposes a novel probabilistic framework for classification with multivariate time series data that contains missing values.* Methods: The proposed method consists of two parts: a deep generative model for missing value imputation and a classifier. The generative model is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation. The classifier takes the time series data along with the imputed missing values and classifies signals, and is trained to capture the predictive uncertainty due to the multiple possibilities of imputations.* Results: The proposed method is demonstrated to be effective through extensive experiments on real-world time series data with missing values.<details>
<summary>Abstract</summary>
Multivariate time series data for real-world applications typically contain a significant amount of missing values. The dominant approach for classification with such missing values is to impute them heuristically with specific values (zero, mean, values of adjacent time-steps) or learnable parameters. However, these simple strategies do not take the data generative process into account, and more importantly, do not effectively capture the uncertainty in prediction due to the multiple possibilities for the missing values. In this paper, we propose a novel probabilistic framework for classification with multivariate time series data with missing values. Our model consists of two parts; a deep generative model for missing value imputation and a classifier. Extending the existing deep generative models to better capture structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation. The classifier part takes the time series data along with the imputed missing values and classifies signals, and is trained to capture the predictive uncertainty due to the multiple possibilities of imputations. Importantly, we show that na\"ively combining the generative model and the classifier could result in trivial solutions where the generative model does not produce meaningful imputations. To resolve this, we present a novel regularization technique that can promote the model to produce useful imputation values that help classification. Through extensive experiments on real-world time series data with missing values, we demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
多变量时间序列数据在实际应用中通常含有大量缺失值。现有的主流方法为这些缺失值是采用各种各样的归纳法（如零、平均值、邻近时间步的值）或学习参数。然而，这些简单策略并不考虑数据生成过程，更重要的是，它们不能有效地捕捉预测中的不确定性。在这篇论文中，我们提出了一种新的概率 frameworks  для类型化多变量时间序列数据中的缺失值。我们的模型包括两部分：深度生成模型和分类器。我们在深度生成模型部分使用了多种可能性的填充方法，以模型缺失值的不确定性。分类器部分接受了时间序列数据以及填充后的缺失值，并将信号分类，并且在不同的填充情况下预测不确定性。我们发现，将生成模型和分类器直接组合可能会导致生成模型不生成有意义的填充值，从而影响分类的准确性。为解决这个问题，我们提出了一种新的规范技术，可以促进模型生成有用的填充值，以便于分类。我们通过对实际时间序列数据中缺失值的广泛实验，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="AerialVLN-Vision-and-Language-Navigation-for-UAVs"><a href="#AerialVLN-Vision-and-Language-Navigation-for-UAVs" class="headerlink" title="AerialVLN: Vision-and-Language Navigation for UAVs"></a>AerialVLN: Vision-and-Language Navigation for UAVs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06735">http://arxiv.org/abs/2308.06735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubo Liu, Hongsheng Zhang, Yuankai Qi, Peng Wang, Yaning Zhang, Qi Wu</li>
<li>for: 这个论文的目的是为了提出一个新的机器人任务，即空中视语Navigation（AerialVLN），用于研究机器人在开放空间中 navigation 的问题。</li>
<li>methods: 这篇论文使用了一种基于 cross-modal-alignment（CMA）方法的扩展基线模型，并使用了一个3D simulator，其中包括25个城市级别的enario，以支持连续导航、环境扩展和配置。</li>
<li>results: 论文发现，基于CMA方法的扩展基线模型与人类表现之间仍然存在一定的差距，这表明空中视语Navigation（AerialVLN）是一个新的挑战性任务。<details>
<summary>Abstract</summary>
Recently emerged Vision-and-Language Navigation (VLN) tasks have drawn significant attention in both computer vision and natural language processing communities. Existing VLN tasks are built for agents that navigate on the ground, either indoors or outdoors. However, many tasks require intelligent agents to carry out in the sky, such as UAV-based goods delivery, traffic/security patrol, and scenery tour, to name a few. Navigating in the sky is more complicated than on the ground because agents need to consider the flying height and more complex spatial relationship reasoning. To fill this gap and facilitate research in this field, we propose a new task named AerialVLN, which is UAV-based and towards outdoor environments. We develop a 3D simulator rendered by near-realistic pictures of 25 city-level scenarios. Our simulator supports continuous navigation, environment extension and configuration. We also proposed an extended baseline model based on the widely-used cross-modal-alignment (CMA) navigation methods. We find that there is still a significant gap between the baseline model and human performance, which suggests AerialVLN is a new challenging task. Dataset and code is available at https://github.com/AirVLN/AirVLN.
</details>
<details>
<summary>摘要</summary>
现在刚刚出现的视力语言导航（VLN）任务已经吸引了计算机视觉和自然语言处理领域的广泛关注。现有的VLN任务都是为地面上的agent进行定位 navigating，但是许多任务需要在天空中进行，如用UAV进行物资交通/安全巡查和景色游览等。在天空中导航比在地面上更加复杂，因为代理需要考虑飞行高度和更复杂的空间关系理解。为了填补这个空白和促进这一领域的研究，我们提出了一个新任务名为空中VLN，它是基于UAV的和向外部环境。我们开发了25个城市级的enario的3D模拟器，模拟器支持连续导航、环境扩展和配置。我们还提出了一种基于协调多modal（CMA）导航方法的扩展基线模型。我们发现，与人类性能相比，基线模型还有一定的差距，这表明空中VLN是一个新的挑战性任务。数据集和代码可以在https://github.com/AirVLN/AirVLN上获取。
</details></li>
</ul>
<hr>
<h2 id="Precipitation-nowcasting-with-generative-diffusion-models"><a href="#Precipitation-nowcasting-with-generative-diffusion-models" class="headerlink" title="Precipitation nowcasting with generative diffusion models"></a>Precipitation nowcasting with generative diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06733">http://arxiv.org/abs/2308.06733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models">https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models</a></li>
<li>paper_authors: Andrea Asperti, Fabio Merizzi, Alberto Paparella, Giorgio Pedrazzi, Matteo Angelinelli, Stefano Colamonaco</li>
<li>for: 该研究旨在检验气候预测中 diffusion models 的可行性，特别是在降水预测（precipitation nowcasting）方面。</li>
<li>methods: 该研究使用了一种生成ensemble diffusion（GED）模型，通过生成多个可能的天气enario，然后使用post-processing网络将其融合成可能性最高的预测。</li>
<li>results: 相比之前的深度学习模型，GED模型在总性能方面显著提高了。<details>
<summary>Abstract</summary>
In recent years traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Numerous historical datasets used for short and medium-range weather forecasts are typically organized into a regular spatial grid structure. This arrangement closely resembles images: each weather variable can be visualized as a map or, when considering the temporal axis, as a video. Several classes of generative models, comprising Generative Adversarial Networks, Variational Autoencoders, or the recent Denoising Diffusion Models have largely proved their applicability to the next-frame prediction problem, and is thus natural to test their performance on the weather prediction benchmarks. Diffusion models are particularly appealing in this context, due to the intrinsically probabilistic nature of weather forecasting: what we are really interested to model is the probability distribution of weather indicators, whose expected value is the most likely prediction.   In our study, we focus on a specific subset of the ERA-5 dataset, which includes hourly data pertaining to Central Europe from the years 2016 to 2021. Within this context, we examine the efficacy of diffusion models in handling the task of precipitation nowcasting. Our work is conducted in comparison to the performance of well-established U-Net models, as documented in the existing literature. Our proposed approach of Generative Ensemble Diffusion (GED) utilizes a diffusion model to generate a set of possible weather scenarios which are then amalgamated into a probable prediction via the use of a post-processing network. This approach, in comparison to recent deep learning models, substantially outperformed them in terms of overall performance.
</details>
<details>
<summary>摘要</summary>
在最近几年，传统的数学方法 для准确的天气预测逐渐面临深度学习方法的挑战。历史数据集用于短距离和中距离天气预测通常有规则的空间格局结构，这种设置与图像非常相似，每种天气变量都可以被视为地图或者在考虑时间轴的情况下为视频。多种生成模型，包括生成对抗网络、变量自动编码器和最近的干扰扩散模型，在下一帧预测问题上有广泛的应用，因此自然地测试它们的性能在天气预测中。干扰扩散模型在这种情况下特别吸引人，因为天气预测的本质是probabilistic的：我们实际上是希望模型可以描述天气指标的概率分布，其期望值是最有可能的预测。在我们的研究中，我们专注于ERA-5数据集的一个子集，包括2016-2021年中欧每小时的数据。在这个上下文中，我们研究了干扰扩散模型在降水预测方面的能力。我们的方法与文献中已有的U-Net模型相比，并使用生成ensemble扩散（GED）模型。这种方法使用扩散模型生成一组可能的天气情况，然后使用Post处理网络将这些情况融合成一个可能的预测。与最近的深度学习模型相比，我们的方法在总性能方面表现得更好。
</details></li>
</ul>
<hr>
<h2 id="Transforming-Sentiment-Analysis-in-the-Financial-Domain-with-ChatGPT"><a href="#Transforming-Sentiment-Analysis-in-the-Financial-Domain-with-ChatGPT" class="headerlink" title="Transforming Sentiment Analysis in the Financial Domain with ChatGPT"></a>Transforming Sentiment Analysis in the Financial Domain with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07935">http://arxiv.org/abs/2308.07935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgios Fatouros, John Soldatos, Kalliopi Kouroumali, Georgios Makridis, Dimosthenis Kyriazis</li>
<li>for: 本研究旨在探讨大语言模型ChatGPT 3.5在金融情感分析中的潜力, 特别是在外汇市场（forex）中。</li>
<li>methods: 本研究使用了零shot提示方法，对手动抽取的forex相关新闻标题进行了多个ChatGPT提问的测试，并使用了精度、准确率、f1score和 Mean Absolute Error（MAE）来衡量情感分类的性能。此外，还进行了对预测的情感和股票市场回报的相关性的评估。</li>
<li>results: ChatGPT比FinBERT更高级的情感分类性能提升约35%，并且与股票市场回报之间的相关性提高约36%。这些结果表明，提示工程在零shot上是非常重要的，并且指出了ChatGPT在金融应用中的潜力。<details>
<summary>Abstract</summary>
Financial sentiment analysis plays a crucial role in decoding market trends and guiding strategic trading decisions. Despite the deployment of advanced deep learning techniques and language models to refine sentiment analysis in finance, this study breaks new ground by investigating the potential of large language models, particularly ChatGPT 3.5, in financial sentiment analysis, with a strong emphasis on the foreign exchange market (forex). Employing a zero-shot prompting approach, we examine multiple ChatGPT prompts on a meticulously curated dataset of forex-related news headlines, measuring performance using metrics such as precision, recall, f1-score, and Mean Absolute Error (MAE) of the sentiment class. Additionally, we probe the correlation between predicted sentiment and market returns as an additional evaluation approach. ChatGPT, compared to FinBERT, a well-established sentiment analysis model for financial texts, exhibited approximately 35\% enhanced performance in sentiment classification and a 36\% higher correlation with market returns. By underlining the significance of prompt engineering, particularly in zero-shot contexts, this study spotlights ChatGPT's potential to substantially boost sentiment analysis in financial applications. By sharing the utilized dataset, our intention is to stimulate further research and advancements in the field of financial services.
</details>
<details>
<summary>摘要</summary>
Note:* "Financial sentiment analysis" Financial sentiment analysis (FSAs) is the process of analyzing text data to determine the sentiment of investors, analysts, and other market participants towards a particular financial asset or market event.* "Zero-shot prompting" Zero-shot prompting refers to the use of language models to perform tasks or generate text based on prompts that are not present in the training data.* "ChatGPT" ChatGPT is a type of large language model that uses a transformer architecture and is trained on a large corpus of text data to generate human-like text.* "FinBERT" FinBERT is a pre-trained language model that is specifically designed for financial text analysis.
</details></li>
</ul>
<hr>
<h2 id="CLE-Diffusion-Controllable-Light-Enhancement-Diffusion-Model"><a href="#CLE-Diffusion-Controllable-Light-Enhancement-Diffusion-Model" class="headerlink" title="CLE Diffusion: Controllable Light Enhancement Diffusion Model"></a>CLE Diffusion: Controllable Light Enhancement Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06725">http://arxiv.org/abs/2308.06725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyang Yin, Dejia Xu, Chuangchuang Tan, Ping Liu, Yao Zhao, Yunchao Wei</li>
<li>For: 提高低光照图像质量，提供用户rich的控制能力* Methods: 使用 conditional diffusion model 和 Segment-Anything Model (SAM) 实现用户定制的光照增强* Results: 在量值、质量和可控性三个方面达到竞争性表现，并且提供了rich的用户控制能力<details>
<summary>Abstract</summary>
Low light enhancement has gained increasing importance with the rapid development of visual creation and editing. However, most existing enhancement algorithms are designed to homogeneously increase the brightness of images to a pre-defined extent, limiting the user experience. To address this issue, we propose Controllable Light Enhancement Diffusion Model, dubbed CLE Diffusion, a novel diffusion framework to provide users with rich controllability. Built with a conditional diffusion model, we introduce an illumination embedding to let users control their desired brightness level. Additionally, we incorporate the Segment-Anything Model (SAM) to enable user-friendly region controllability, where users can click on objects to specify the regions they wish to enhance. Extensive experiments demonstrate that CLE Diffusion achieves competitive performance regarding quantitative metrics, qualitative results, and versatile controllability. Project page: \url{https://yuyangyin.github.io/CLEDiffusion/}
</details>
<details>
<summary>摘要</summary>
低光照增强在视觉创作和编辑领域的快速发展中得到了越来越重要的注目。然而，现有的增强算法大多采用一致性提高图像亮度，限制用户体验。为解决这个问题，我们提出了可控光增强扩散模型（CLE Diffusion），这是一种新的扩散框架，提供了用户 Rich 控制性。通过加入条件扩散模型，我们引入了照明嵌入，让用户控制自己的需要的亮度水平。此外，我们还 incorporate了 Segment-Anything Model（SAM），使得用户可以通过点击对象来指定需要增强的区域。广泛的实验表明，CLE Diffusion 在量化指标、质量效果和多样性控制方面具有竞争力。项目页面：<https://yuyangyin.github.io/CLEDiffusion/>
</details></li>
</ul>
<hr>
<h2 id="IP-Adapter-Text-Compatible-Image-Prompt-Adapter-for-Text-to-Image-Diffusion-Models"><a href="#IP-Adapter-Text-Compatible-Image-Prompt-Adapter-for-Text-to-Image-Diffusion-Models" class="headerlink" title="IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models"></a>IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06721">http://arxiv.org/abs/2308.06721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, Wei Yang</li>
<li>for: 这篇论文旨在提出一种可靠且轻量级的适配器，使得预训练的文本到图像扩散模型可以使用图像提示来生成图像。</li>
<li>methods: 该适配器使用了解coupled crossed attention机制，将文本特征和图像特征分开处理，以提高生成图像的精度和效率。</li>
<li>results:  experiments show that IP-Adapter可以与预训练的图像提示模型相比，在生成图像方面达到相当或更好的性能，并且可以与文本提示结合使用，实现多模态图像生成。<details>
<summary>Abstract</summary>
Recent years have witnessed the strong power of large text-to-image diffusion models for the impressive generative capability to create high-fidelity images. However, it is very tricky to generate desired images using only text prompt as it often involves complex prompt engineering. An alternative to text prompt is image prompt, as the saying goes: "an image is worth a thousand words". Although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter is decoupled cross-attention mechanism that separates cross-attention layers for text features and image features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fully fine-tuned image prompt model. As we freeze the pretrained diffusion model, the proposed IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. With the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to achieve multimodal image generation. The project page is available at \url{https://ip-adapter.github.io}.
</details>
<details>
<summary>摘要</summary>
近年来，大型文本到图像扩散模型的强大能力吸引了广泛的关注，这些模型可以生成高效的图像。然而，使用仅文本提示来生成愿景图像是非常困难，因为这通常需要复杂的提示工程。一种alternative是使用图像提示，以示人们所说的“一个图像值得一千个话”。 although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. 在这篇论文中，我们提出了IP-Adapter，一种高效且轻量级的适配器，可以使得预训练的文本到图像扩散模型具备图像提示能力。我们的键要设计是分离的交叉关注机制，它将文本特征和图像特征的交叉关注层分离开来。尽管我们的方法简单，一个只有22M参数的IP-Adapter仍可以达到与完全 fine-tune 的图像提示模型相当或更好的性能。由于我们冻结了预训练的扩散模型，我们的IP-Adapter可以被普遍化到其他自定义模型，以及使用现有的可控生成工具进行可控生成。此外，使用分离的交叉关注策略，图像提示还可以与文本提示共同工作，以实现多modal图像生成。相关项目页面可以查看于 \url{https://ip-adapter.github.io}。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables"><a href="#Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables" class="headerlink" title="Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables"></a>Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06718">http://arxiv.org/abs/2308.06718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Xie, Biwei Huang, Zhengming Chen, Ruichu Cai, Clark Glymour, Zhi Geng, Kun Zhang</li>
<li>For: The paper is focused on learning the causal structure of a system with latent variables, including identifying the number of latent variables and their relationships with observed variables.* Methods: The authors propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models with latent variables, which is used to identify the causal relationships between the observed and latent variables. They also develop a search procedure to efficiently estimate the underlying causal structure.* Results: The authors show that the proposed approach can identify the causal structure of a system with latent variables, and demonstrate its effectiveness through experimental results. Additionally, they find that the independent noise condition can be seen as a special case of the GIN condition, which provides a connection between the two concepts.<details>
<summary>Abstract</summary>
We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous set $\mathcal{S}$ relative to the parent set of $\mathbf{Y}$ (w.r.t. the causal ordering), such that $\mathcal{S}$ d-separates $\mathbf{Y}$ from $\mathbf{Z}$. Interestingly, we find that the independent noise condition (i.e., if there is no confounder, causes are independent of the residual derived from regressing the effect on the causes) can be seen as a special case of GIN. With such a connection between GIN and latent causal structures, we further leverage the proposed GIN condition, together with a well-designed search procedure, to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. We show that the underlying causal structure of a LiNGLaH is identifiable in light of GIN conditions under mild assumptions. Experimental results show the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
我们研究一个复杂任务：在含有隐变量的情况下学习 causal 结构。其中包括找到隐变量的位置和它们的数量，以及确定隐变量和观测变量之间的 causal 关系。为此，我们提出一个通用独立噪声（GIN）条件，该条件用于线性非常 Gaussian 隐含 causal 模型中的隐变量，并且可以独立地测试隐变量的存在和数量。 Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^\intercal \mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous set $\mathcal{S}$ relative to the parent set of $\mathbf{Y}$ (w.r.t. the causal ordering), such that $\mathcal{S}$ d-separates $\mathbf{Y}$ from $\mathbf{Z}$. Interestingly, we find that the independent noise condition (i.e., if there is no confounder, causes are independent of the residual derived from regressing the effect on the causes) can be seen as a special case of GIN. With such a connection between GIN and latent causal structures, we further leverage the proposed GIN condition, together with a well-designed search procedure, to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. We show that the underlying causal structure of a LiNGLaH is identifiable in light of GIN conditions under mild assumptions. Experimental results show the effectiveness of the proposed approach.
</details></li>
</ul>
<hr>
<h2 id="Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards"><a href="#Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards" class="headerlink" title="Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards"></a>Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06717">http://arxiv.org/abs/2308.06717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilgin Dogan, Zuo-Jun Max Shen, Anil Aswani</li>
<li>for: 本研究探讨了一种复杂的主-代理人游戏，在该游戏中，代理人不能直接观察代理人的奖励实现情况，这与许多主-代理人模型不同。这种信息不均衡使得代理人困难地估计代理人的未知奖励，这种问题在各种实际场景中都有广泛的应用，如绿色能源存储合同和个性化奖励等。</li>
<li>methods: 本研究使用了多臂抽象（MAB）问题和学习代理人的方法来解决这种复杂的主-代理人游戏。在这个框架中，代理人通过学习来决定选择，而代理人则通过训练并采用了一种并行的算法来估计代理人的未知奖励。</li>
<li>results: 本研究证明了在非 Parametric 模型下，可以使用历史记录来估计代理人的未知奖励，并且可以通过一种数据驱动的奖励策略来实现这一目标。此外，我们还证明了代理人的 regret bound，即代理人在选择奖励策略时的误差 bound。最后，我们通过 simulations 来证明我们的框架在绿色能源集成合同中的实用性。<details>
<summary>Abstract</summary>
In practice, incentive providers (i.e., principals) often cannot observe the reward realizations of incentivized agents, which is in contrast to many principal-agent models that have been previously studied. This information asymmetry challenges the principal to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, which becomes even more challenging when the agent has to learn its own rewards. This complex setting is observed in various real-life scenarios ranging from renewable energy storage contracts to personalized healthcare incentives. Hence, it offers not only interesting theoretical questions but also wide practical relevance. This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal. The agent tackles a multi-armed bandit (MAB) problem to maximize their expected reward plus incentive. On top of the agent's learning, the principal trains a parallel algorithm and faces a trade-off between consistently estimating the agent's unknown rewards and maximizing their own utility by offering adaptive incentives to lead the agent. For a non-parametric model, we introduce an estimator whose only input is the history of principal's incentives and agent's choices. We unite this estimator with a proposed data-driven incentive policy within a MAB framework. Without restricting the type of the agent's algorithm, we prove finite-sample consistency of the estimator and a rigorous regret bound for the principal by considering the sequential externality imposed by the agent. Lastly, our theoretical results are reinforced by simulations justifying applicability of our framework to green energy aggregator contracts.
</details>
<details>
<summary>摘要</summary>
在实践中，奖励提供者（即主体）经常无法观察奖励的实现，这与许多主体-代理模型不同。这种信息不均衡使得主体Difficult to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, especially when the agent needs to learn its own rewards. This complex setting is observed in various real-life scenarios, such as renewable energy storage contracts and personalized healthcare incentives. Therefore, it not only raises interesting theoretical questions but also has wide practical relevance.本文研究了一个反复的反选择游戏，在这个游戏中，一个自利益的学习代理和一个学习的主体之间进行交互。代理面临着一个多重武器问题（MAB），以最大化其预期奖励加上奖励。同时，主体也在学习，面临着一种奖励的适应性和自己的利用率之间的负担。为一个非 Parametric 模型，我们引入了一个仅基于主体的奖励历史和代理的选择的估计器。我们将这个估计器与一种基于 MAB 的数据驱动奖励策略结合。不Restricting the type of the agent's algorithm, we prove the finite-sample consistency of the estimator and a rigorous regret bound for the principal by considering the sequential externality imposed by the agent.最后，我们的理论结果被实践中的 simulations 证明了我们的框架在绿色能源聚合合同中的应用可行性。
</details></li>
</ul>
<hr>
<h2 id="Learning-on-Graphs-with-Out-of-Distribution-Nodes"><a href="#Learning-on-Graphs-with-Out-of-Distribution-Nodes" class="headerlink" title="Learning on Graphs with Out-of-Distribution Nodes"></a>Learning on Graphs with Out-of-Distribution Nodes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06714">http://arxiv.org/abs/2308.06714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songyyyy/kdd22-oodgat">https://github.com/songyyyy/kdd22-oodgat</a></li>
<li>paper_authors: Yu Song, Donglin Wang</li>
<li>for: 本研究旨在 Addressing the problem of graph learning with out-of-distribution nodes, aiming to detect outliers and classify remaining nodes to known classes.</li>
<li>methods: 提出了一种新的 Graph Attention Network (GAT) 模型，称为 Out-of-Distribution Graph Attention Network (OODGAT)，该模型通过Explicitly modeling the interaction between different kinds of nodes and separating inliers from outliers during feature propagation,以便检测异常节点和分类正常节点。</li>
<li>results: 对多个实验 dataset 进行了extensive experiments，结果表明 OODGAT 比现有的异常检测方法具有更大的优势，同时与正常分类 Task 的性能相当。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are state-of-the-art models for performing prediction tasks on graphs. While existing GNNs have shown great performance on various tasks related to graphs, little attention has been paid to the scenario where out-of-distribution (OOD) nodes exist in the graph during training and inference. Borrowing the concept from CV and NLP, we define OOD nodes as nodes with labels unseen from the training set. Since a lot of networks are automatically constructed by programs, real-world graphs are often noisy and may contain nodes from unknown distributions. In this work, we define the problem of graph learning with out-of-distribution nodes. Specifically, we aim to accomplish two tasks: 1) detect nodes which do not belong to the known distribution and 2) classify the remaining nodes to be one of the known classes. We demonstrate that the connection patterns in graphs are informative for outlier detection, and propose Out-of-Distribution Graph Attention Network (OODGAT), a novel GNN model which explicitly models the interaction between different kinds of nodes and separate inliers from outliers during feature propagation. Extensive experiments show that OODGAT outperforms existing outlier detection methods by a large margin, while being better or comparable in terms of in-distribution classification.
</details>
<details>
<summary>摘要</summary>
GRAPH NEURAL NETWORKS (GNNs) 是当今最先进的图数据预测模型。而现有的 GNN 模型在各种图数据任务上已经表现出色，但是对于图数据中存在 OUT-OF-DISTRIBUTION（OOD）节点的情况却得到了相对的少的关注。从 Computer Vision 和自然语言处理中借鉴的概念，我们定义 OOD 节点为训练集中未出现过的标签。实际上，由于许多网络是通过程序自动构建的，真实世界中的图数据经常具有噪音和未知分布的特点，因此在这种情况下，我们定义了图学习 WITH OUT-OF-DISTRIBUTION NODES 的问题。特别是，我们希望完成两个任务：1）检测图中不属于已知分布的节点，2）分类剩下的节点为已知类别中的一个。我们表明了图中连接 patrern 可以为 OUTLIER 检测提供信息，并提出了一种新的 GNN 模型，即 Out-of-Distribution Graph Attention Network (OODGAT)，该模型在传播特征时显式地处理不同类型的节点之间的交互，以分离异常节点和常见节点。我们进行了广泛的实验，并证明了 OODGAT 在 OUTLIER 检测方面比现有的方法有大幅度的提升，而且在可见分类方面也是或等于于比较好的。
</details></li>
</ul>
<hr>
<h2 id="Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection"><a href="#Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection" class="headerlink" title="Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection"></a>Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06701">http://arxiv.org/abs/2308.06701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haichao Zhang, Can Qin, Yu Yin, Yun Fu</li>
<li>for: 提高掩蔽物检测的深度学习模型性能</li>
<li>methods: 使用生成模型生成真实的掩蔽图像，并将其用于训练现有的对象检测模型</li>
<li>results: 在三个数据集（COD10k、CAMO和CHAMELEON）上超越当前状态的方法， demonstarting 其在掩蔽物检测中的有效性<details>
<summary>Abstract</summary>
Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-and-play data generation and augmentation module for existing camouflaged object detection tasks and provides a novel way to introduce more diversity and distributions into current camouflage datasets.
</details>
<details>
<summary>摘要</summary>
伪装物体在自然场景中混合是深度学习模型检测和synthesize的重要挑战。伪装物体检测是计算机视觉中重要的应用领域之一，但这一研究领域受到有限的数据可用性的限制。我们提出了一种框架，用于增强自然场景中的伪装物体检测。我们的方法使用生成模型生成真实的伪装图像，这些图像可以用来训练现有的对象检测模型。具体来说，我们使用一个伪装环境生成器，它是根据伪装分布分类器进行监督的。我们的框架在三个数据集（COD10k、CAMO和CHAMELEON）上表现出了比前一个状态的方法更好的性能，这说明了我们的方法的有效性。这种方法可以作为现有伪装物体检测任务的数据生成和增强模块，并提供了一种新的多样性和分布的引入方式，以便为当前的伪装数据集增加更多的多样性。
</details></li>
</ul>
<hr>
<h2 id="MACO-A-Modality-Adversarial-and-Contrastive-Framework-for-Modality-missing-Multi-modal-Knowledge-Graph-Completion"><a href="#MACO-A-Modality-Adversarial-and-Contrastive-Framework-for-Modality-missing-Multi-modal-Knowledge-Graph-Completion" class="headerlink" title="MACO: A Modality Adversarial and Contrastive Framework for Modality-missing Multi-modal Knowledge Graph Completion"></a>MACO: A Modality Adversarial and Contrastive Framework for Modality-missing Multi-modal Knowledge Graph Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06696">http://arxiv.org/abs/2308.06696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjukg/maco">https://github.com/zjukg/maco</a></li>
<li>paper_authors: Yichi Zhang, Zhuo Chen, Wen Zhang</li>
<li>for: 提高大规模知识图（KG）中缺失模态信息的问题，以便更好地完成知识图完成（KGC）任务。</li>
<li>methods: 提出了一种模态对抗和对比框架（MACO），通过对Generator和Discriminator进行对抗训练，生成缺失模态特征，以便在MMKGC模型中使用。同时，我们设计了交叉模态对比损失，以提高生成器的性能。</li>
<li>results: 在公共benchmark上进行了实验，并进行了进一步的探索，结果显示MACO可以达到状态机器人的 результаats，并且可以用于强化多种MMKGC模型。我们的代码和benchmark数据可以在<a target="_blank" rel="noopener" href="https://github.com/zjukg/MACO%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zjukg/MACO上获取。</a><details>
<summary>Abstract</summary>
Recent years have seen significant advancements in multi-modal knowledge graph completion (MMKGC). MMKGC enhances knowledge graph completion (KGC) by integrating multi-modal entity information, thereby facilitating the discovery of unobserved triples in the large-scale knowledge graphs (KGs). Nevertheless, existing methods emphasize the design of elegant KGC models to facilitate modality interaction, neglecting the real-life problem of missing modalities in KGs. The missing modality information impedes modal interaction, consequently undermining the model's performance. In this paper, we propose a modality adversarial and contrastive framework (MACO) to solve the modality-missing problem in MMKGC. MACO trains a generator and discriminator adversarially to generate missing modality features that can be incorporated into the MMKGC model. Meanwhile, we design a cross-modal contrastive loss to improve the performance of the generator. Experiments on public benchmarks with further explorations demonstrate that MACO could achieve state-of-the-art results and serve as a versatile framework to bolster various MMKGC models. Our code and benchmark data are available at https://github.com/zjukg/MACO.
</details>
<details>
<summary>摘要</summary>
近年来，多模式知识图完成（MMKGC）技术得到了 significiant 进步。 MMKGC 可以将多种模式实体信息集成到知识图中，从而促进发现大规模知识图中的未观察 triple。然而，现有的方法强调设计美观的 KGC 模型，以便模态之间的互动，忽略了实际生活中的缺失模态问题。缺失模态信息会阻碍模态之间的互动，从而降低模型的性能。在这篇论文中，我们提出了一种模态对抗和对比框架（MACO），用于解决 MMKGC 中缺失模态问题。MACO 通过对generator和discriminator进行对抗训练，生成缺失模态特征，以便在 MMKGC 模型中添加。同时，我们设计了对比架构来提高生成器的性能。实验结果表明，MACO 可以达到领先的Result和服务为多种 MMKGC 模型的可靠框架。我们的代码和 benchmark 数据可以在 https://github.com/zjukg/MACO 中下载。
</details></li>
</ul>
<hr>
<h2 id="Video-Captioning-with-Aggregated-Features-Based-on-Dual-Graphs-and-Gated-Fusion"><a href="#Video-Captioning-with-Aggregated-Features-Based-on-Dual-Graphs-and-Gated-Fusion" class="headerlink" title="Video Captioning with Aggregated Features Based on Dual Graphs and Gated Fusion"></a>Video Captioning with Aggregated Features Based on Dual Graphs and Gated Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06685">http://arxiv.org/abs/2308.06685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yutao Jin, Bin Liu, Jing Wang</li>
<li>for: 本研究旨在提高视频captioning模型的准确性和完整性，通过使用 dual graphs和gated fusion来生成多维度特征表示。</li>
<li>methods: 我们提出了基于 dual graphs和gated fusion的视频captioning模型，包括 dual-graphs reasoning和gated fusion两部分。 dual-graphs reasoning通过两种图来生成视频内容的多个方面特征，而gated fusion则是将多个特征表示之间的信息聚合以提高视频内容的全面理解。</li>
<li>results: 我们在MSVD和MSR-VTT两个常用 dataset上进行了实验，并取得了当前领域最佳表现。<details>
<summary>Abstract</summary>
The application of video captioning models aims at translating the content of videos by using accurate natural language. Due to the complex nature inbetween object interaction in the video, the comprehensive understanding of spatio-temporal relations of objects remains a challenging task. Existing methods often fail in generating sufficient feature representations of video content. In this paper, we propose a video captioning model based on dual graphs and gated fusion: we adapt two types of graphs to generate feature representations of video content and utilize gated fusion to further understand these different levels of information. Using a dual-graphs model to generate appearance features and motion features respectively can utilize the content correlation in frames to generate various features from multiple perspectives. Among them, dual-graphs reasoning can enhance the content correlation in frame sequences to generate advanced semantic features; The gated fusion, on the other hand, aggregates the information in multiple feature representations for comprehensive video content understanding. The experiments conducted on worldly used datasets MSVD and MSR-VTT demonstrate state-of-the-art performance of our proposed approach.
</details>
<details>
<summary>摘要</summary>
视频captioning模型的应用目标是将视频内容翻译成准确的自然语言。由于视频中对象之间的复杂交互，理解视频内容的空间时间关系是一项具有挑战性的任务。现有方法通常无法生成足够的视频内容特征表示。在这篇论文中，我们提出了基于双图和闭合融合的视频captioning模型：我们采用两种类型的图来生成视频内容的特征表示，并使用闭合融合来进一步理解这些不同水平的信息。使用双图模型生成出现特征和运动特征分别可以利用帧序列中的内容相关性来生成多种特征从多个角度。其中，双图理解可以增强帧序列中的内容相关性，生成更高级别的 semantic 特征；而闭合融合则可以将多个特征表示的信息聚合在一起，实现视频内容全面理解。我们在MSVD和MSR-VTT等世界 commonly 使用的数据集上进行了实验，并达到了当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent"><a href="#Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent" class="headerlink" title="Law of Balance and Stationary Distribution of Stochastic Gradient Descent"></a>Law of Balance and Stationary Distribution of Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06671">http://arxiv.org/abs/2308.06671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Ziyin, Hongchao Li, Masahito Ueda<br>for: 这个论文的目的是解释如何使用权重学习来训练神经网络，以及神经网络训练过程中SGD算法的工作机制。methods: 这篇论文使用了权重学习和SGD算法来训练神经网络，并通过分批训练来证明SGD算法可以带来对神经网络的稳定化。results: 这篇论文的结果表明，当损失函数具有对称性时，SGD算法会带来对神经网络的稳定化，并且可以带来复杂的非线性现象，如阶梯过程中的相态转变、缺失ерgodicity和振荡倒转。这些现象只存在于深度很大的神经网络中，这表明了深度神经网络和浅度神经网络之间的根本区别。<details>
<summary>Abstract</summary>
The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.
</details>
<details>
<summary>摘要</summary>
Stochastic gradient descent（SGD）算法是我们用来训练神经网络的算法。但是，SGD在神经网络的高度非线性和潜在的异常点的搜索方面仍然不甚了解。在这项工作中，我们证明了SGD中的小批处理噪声规范化神经网络的解决方案，当损失函数包含对称性时。因为在对称性存在时，SGD的动态和普通液体流动的差异最大，我们的理论意味着损失函数的对称性是SGD工作的重要检验。我们then使用这个结果来Derive stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. Stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.Note that Simplified Chinese is a simplified version of Chinese that uses shorter words and sentences, and is often used in informal writing and online communication. Traditional Chinese is a more formal version of Chinese that is used in formal writing and in most printed materials.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Adaptation-of-Polyp-Segmentation-Models-via-Coarse-to-Fine-Self-Supervision"><a href="#Unsupervised-Adaptation-of-Polyp-Segmentation-Models-via-Coarse-to-Fine-Self-Supervision" class="headerlink" title="Unsupervised Adaptation of Polyp Segmentation Models via Coarse-to-Fine Self-Supervision"></a>Unsupervised Adaptation of Polyp Segmentation Models via Coarse-to-Fine Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06665">http://arxiv.org/abs/2308.06665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiexiang Wang, Chaoqi Chen</li>
<li>for: 本研究实验为了解决受隐私和安全问题限制的对应领域自适应<del>(UDA) 问题，专注于不需要源数据的对应领域自适应</del>(SFDA) 方法。</li>
<li>methods: 本研究提出了一个名为 Region-to-Pixel Adaptation Network~(RPANet) 的新 SFDA 框架，通过均衡粗细自我监督学习，从粗细到细节层次掌握区域和像素层次的标识表现。RPANet 包括两个模组：Foreground-aware Contrastive Learning (FCL) 和 Confidence-Calibrated Pseudo-Labeling (CCPL)，它们分别解决了“如何区别”和“如何调整”的关键挑战。</li>
<li>results: 实验结果显示，RPANet 在三个跨领域肿瘤分类任务上具有优秀的表现，较以前SFDA和UDA方法无法达到的水准，显示了SFDA在医疗应用中的潜力。<details>
<summary>Abstract</summary>
Unsupervised Domain Adaptation~(UDA) has attracted a surge of interest over the past decade but is difficult to be used in real-world applications. Considering the privacy-preservation issues and security concerns, in this work, we study a practical problem of Source-Free Domain Adaptation (SFDA), which eliminates the reliance on annotated source data. Current SFDA methods focus on extracting domain knowledge from the source-trained model but neglects the intrinsic structure of the target domain. Moreover, they typically utilize pseudo labels for self-training in the target domain, but suffer from the notorious error accumulation problem. To address these issues, we propose a new SFDA framework, called Region-to-Pixel Adaptation Network~(RPANet), which learns the region-level and pixel-level discriminative representations through coarse-to-fine self-supervision. The proposed RPANet consists of two modules, Foreground-aware Contrastive Learning (FCL) and Confidence-Calibrated Pseudo-Labeling (CCPL), which explicitly address the key challenges of ``how to distinguish'' and ``how to refine''. To be specific, FCL introduces a supervised contrastive learning paradigm in the region level to contrast different region centroids across different target images, which efficiently involves all pseudo labels while robust to noisy samples. CCPL designs a novel fusion strategy to reduce the overconfidence problem of pseudo labels by fusing two different target predictions without introducing any additional network modules. Extensive experiments on three cross-domain polyp segmentation tasks reveal that RPANet significantly outperforms state-of-the-art SFDA and UDA methods without access to source data, revealing the potential of SFDA in medical applications.
</details>
<details>
<summary>摘要</summary>
自然语言处理中的Unsupervised Domain Adaptation~(UDA)在过去的一个十年里引起了广泛的关注，但在实际应用中却难以使用。 Considering the privacy-preservation issues和安全问题，在这种工作中，我们研究了一个实用的源无需采用Domain Adaptation~(SFDA)问题，该问题消除了源数据的注释。Current SFDA方法通过提取源模型中的领域知识来解决问题，但忽略了目标频谱的内在结构。另外，它们通常通过pseudo标签进行自我训练在目标频谱中，但受到误差积累问题的困扰。为了解决这些问题，我们提出了一个新的SFDA框架，calledRegion-to-Pixel Adaptation Network~(RPANet)，该框架通过粗细自动supervised contrastive learning和Confidence-Calibrated Pseudo-Labeling（CCPL）模块来学习区域水平和像素级别的抑制表示。RPANet包括两个模块：Foreground-aware Contrastive Learning（FCL）和Confidence-Calibrated Pseudo-Labeling（CCPL），这两个模块直接解决了“如何 отличи出”和“如何精细化”的关键问题。具体来说，FCL引入了一种监督对照学习 парадигмы，在不同的目标图像中对不同的区域中心进行对比，以fficiently利用所有pseudo标签，并且对噪声样本具有鲁棒性。CCPL设计了一种新的融合策略，通过将两个不同的目标预测融合而不引入任何额外网络模块，以减少pseudo标签过度自信的问题。经验表明，RPANet在三个跨频谱肠segmentation任务上显著超过了state-of-the-art SFDA和UDA方法，无需访问源数据，探明SFDA在医疗应用中的潜力。
</details></li>
</ul>
<hr>
<h2 id="ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN"><a href="#ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN" class="headerlink" title="ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN"></a>ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06663">http://arxiv.org/abs/2308.06663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abul Bashar, Richi Nayak</li>
<li>for: 这篇论文旨在应用生成对抗网络（GAN）方法来探测时间序列资料中的异常点。</li>
<li>methods: 这篇论文提出了一个新的GAN模型，名为调整LSTM GAN（ALGAN），它可以在无supervision的情况下提高时间序列资料中的异常点探测精度。</li>
<li>results: 论文的实验结果显示，ALGAN在46个真实世界单变时间序列数据集和多个领域的大量多变时间序列数据集上的异常点探测精度高于传统、神经网络基于的和其他GAN基于的方法。<details>
<summary>Abstract</summary>
Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
</details>
<details>
<summary>摘要</summary>
“时间序列资料中的偏差探测，以探测不同于常规行为的点，是不同领域中的一个常见问题，例如生产、医疗影像和 cybersecurity。在最近的研究中，生成对抗网络（GANs）已经被证明能够优化时间序列资料中的偏差探测精度。这个神经网络架构（i.e. 生成器和识别器）可以在无监督下提高偏差探测精度。在这篇论文中，我们提出了一个新的 GAN 模型，名为 Adjusted-LSTM GAN（ALGAN），它可以在无监督下对时间序列资料进行优化的偏差探测。我们将这个模型评估在 46 个真实的时间序列资料集和一个大的多重时间序列资料集中，结果显示 ALGAN 可以在无监督下优化时间序列资料中的偏差探测精度，并且比较传统的神经网络、神经网络基于的方法和其他 GAN 基于的方法更高。”
</details></li>
</ul>
<hr>
<h2 id="Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features"><a href="#Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features" class="headerlink" title="Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features"></a>Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08482">http://arxiv.org/abs/2308.08482</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiiizhang/shortcutDebiasing">https://github.com/yiiizhang/shortcutDebiasing</a></li>
<li>paper_authors: Yi Zhang, Jitao Sang, Junyang Wang, Dongmei Jiang, Yaowei Wang</li>
<li>for: 这篇论文旨在解决机器学习模型对敏感社会特征（如性别和种族）的预测问题，以确保在社会应用中保持公平性。</li>
<li>methods: 这篇论文提出了一种称为“Shortcut Debiasing”的方法，将偏见特征（如性别）转换为快捷特征（ Shortcut Features），然后使用 causal intervention 方法删除这些快捷特征 durante 推断过程。</li>
<li>results: 这篇论文在多个 benchmark 数据集上实现了与现有debiasing方法相比的重要改善， both 精度和公平性方面。<details>
<summary>Abstract</summary>
Machine learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Existing work tackles this issue by minimizing the employed information about social attributes in models for debiasing. However, the high correlation between target task and these social attributes makes learning on the target task incompatible with debiasing. Given that model bias arises due to the learning of bias features (\emph{i.e}., gender) that help target task optimization, we explore the following research question: \emph{Can we leverage shortcut features to replace the role of bias feature in target task optimization for debiasing?} To this end, we propose \emph{Shortcut Debiasing}, to first transfer the target task's learning of bias attributes from bias features to shortcut features, and then employ causal intervention to eliminate shortcut features during inference. The key idea of \emph{Shortcut Debiasing} is to design controllable shortcut features to on one hand replace bias features in contributing to the target task during the training stage, and on the other hand be easily removed by intervention during the inference stage. This guarantees the learning of the target task does not hinder the elimination of bias features. We apply \emph{Shortcut Debiasing} to several benchmark datasets, and achieve significant improvements over the state-of-the-art debiasing methods in both accuracy and fairness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Smart-Knowledge-Transfer-using-Google-like-Search"><a href="#Smart-Knowledge-Transfer-using-Google-like-Search" class="headerlink" title="Smart Knowledge Transfer using Google-like Search"></a>Smart Knowledge Transfer using Google-like Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06653">http://arxiv.org/abs/2308.06653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srijoni Majumdar, Partha Pratim Das</li>
<li>for: Addressing the issue of rising software maintenance cost due to program comprehension challenges.</li>
<li>methods: Proposes SMARTKT (Smart Knowledge Transfer), a search framework that extracts and integrates knowledge related to various aspects of an application in the form of a semantic graph, supporting syntax and semantic queries and converting the process of program comprehension into a “google-like” search problem.</li>
<li>results: Not specified in the abstract, but the paper likely presents the effectiveness of SMARTKT in improving program comprehension and reducing software maintenance costs.<details>
<summary>Abstract</summary>
To address the issue of rising software maintenance cost due to program comprehension challenges, we propose SMARTKT (Smart Knowledge Transfer), a search framework, which extracts and integrates knowledge related to various aspects of an application in form of a semantic graph. This graph supports syntax and semantic queries and converts the process of program comprehension into a {\em google-like} search problem.
</details>
<details>
<summary>摘要</summary>
Here's the breakdown of the translation:* "rising software maintenance cost" is translated as "软件维护成本的增加"* "due to program comprehension challenges" is translated as "由程序理解困难引起"* "we propose SMARTKT" is translated as "我们提出智能知识传输"* "a search framework" is translated as "一个搜索框架"* "which extracts and integrates knowledge related to various aspects of an application" is translated as "可以提取和集成各个方面的应用程序知识"* "in the form of a semantic graph" is translated as "以semantic graph的形式"* "This graph supports syntax and semantic queries" is translated as "这个图表支持语法和 semantics 查询"* "and converts the process of program comprehension into a 'Google-like' search problem" is translated as "并将程序理解过程转化为一个"Google-like"搜索问题"Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Stationary-Algorithmic-Balancing-For-Dynamic-Email-Re-Ranking-Problem"><a href="#Stationary-Algorithmic-Balancing-For-Dynamic-Email-Re-Ranking-Problem" class="headerlink" title="Stationary Algorithmic Balancing For Dynamic Email Re-Ranking Problem"></a>Stationary Algorithmic Balancing For Dynamic Email Re-Ranking Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08460">http://arxiv.org/abs/2308.08460</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jylevangeline/mosr">https://github.com/jylevangeline/mosr</a></li>
<li>paper_authors: Jiayi Liu, Jennifer Neville</li>
<li>for: 这个研究旨在提出一个基于多重目标的电子邮件推荐系统，以满足用户在不同时间的偏好变化。</li>
<li>methods: 这个研究使用了一个适应控制模型来动态均衡多重目标，包括 sender和topic的相关性、时间新鲜度和信息简洁度。</li>
<li>results: 研究结果显示，MOSR在非站ARY preferences下表现更好，特别是在用户偏好变化时。另外，MOSR在不同样本中的稳定性也得到了证明。<details>
<summary>Abstract</summary>
Email platforms need to generate personalized rankings of emails that satisfy user preferences, which may vary over time. We approach this as a recommendation problem based on three criteria: closeness (how relevant the sender and topic are to the user), timeliness (how recent the email is), and conciseness (how brief the email is). We propose MOSR (Multi-Objective Stationary Recommender), a novel online algorithm that uses an adaptive control model to dynamically balance these criteria and adapt to preference changes. We evaluate MOSR on the Enron Email Dataset, a large collection of real emails, and compare it with other baselines. The results show that MOSR achieves better performance, especially under non-stationary preferences, where users value different criteria more or less over time. We also test MOSR's robustness on a smaller down-sampled dataset that exhibits high variance in email characteristics, and show that it maintains stable rankings across different samples. Our work offers novel insights into how to design email re-ranking systems that account for multiple objectives impacting user satisfaction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation"><a href="#Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation" class="headerlink" title="Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation"></a>Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06644">http://arxiv.org/abs/2308.06644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation">https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation</a></li>
<li>paper_authors: Junwei Huang, Zhiqing Sun, Yiming Yang</li>
<li>for: 提高NP-完全 combinatorial优化（CO）问题的解决质量和搜索效率</li>
<li>methods: 使用进步浸泡法加速推理，通过在推理过程中采取 fewer steps 来减少推理时间</li>
<li>results: 实验结果显示，使用进步浸泡法可以提高推理速度 16 倍，只带来 0.019% 的性能下降在 TSP-50 数据集上<details>
<summary>Abstract</summary>
Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
</details>
<details>
<summary>摘要</summary>
几何基于的扩散模型已经在解决NP完备（NPC） combinatorial优化（CO）问题中显示出了可观的成果，但是这些模型往往在推断中效率低下，因为推断过程是迭代评估的。本文提议使用进步养分来加速推断，在推断过程中只需要几步（例如在单步中预测两步）。我们的实验结果显示，使用进步养分的模型可以在TSP-50 dataset上实现16倍的推断速度，仅带来0.019%的性能下降。
</details></li>
</ul>
<hr>
<h2 id="Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks"><a href="#Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks" class="headerlink" title="Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?"></a>Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06619">http://arxiv.org/abs/2308.06619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Liao, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione</li>
<li>for: 降低深度神经网络大小而保持性能</li>
<li>methods: 使用Entropy Guided Pruning算法（EGP），主要是根据层次 entropy 来决定 Connection 的缩减和完全删除</li>
<li>results: 对 популяр的模型 ResNet-18 和 Swin-T 进行了广泛的实验，发现 EGP 能够有效地压缩深度神经网络，同时保持竞争性能水平。<details>
<summary>Abstract</summary>
Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relationship between entropy, pruning techniques, and deep learning performance. The EGP algorithm and its insights hold great promise for advancing the field of network compression and optimization. The source code for EGP is released open-source.
</details>
<details>
<summary>摘要</summary>
剪辑是一种广泛使用的技术，用于减少深度神经网络的大小，保持其性能。然而，这种技术，即使能够压缩深度模型，几乎无法完全移除层（即使是结构化的）：是这个任务可行吗？在这项研究中，我们介绍了EGP算法，一种创新的熵导向剪辑算法，用于减少深度神经网络的大小，保持其性能。EGP的关键焦点在于优先剪辑层中的低熵连接，最终导致它们的完全移除。经过对流行的模型如ResNet-18和Swin-T进行了广泛的实验，我们的发现表明EGP有效地压缩深度神经网络，保持竞争力的性能水平。我们的结果不仅解释了剪辑技术的优势，还开创了进一步研究熵、剪辑技术和深度学习性能之间的复杂关系。EGP算法和其发现将对神经网络压缩和优化领域的发展带来巨大的潜力。EGP算法的源代码已经开源发布。
</details></li>
</ul>
<hr>
<h2 id="On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness"><a href="#On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness" class="headerlink" title="On the Interplay of Convolutional Padding and Adversarial Robustness"></a>On the Interplay of Convolutional Padding and Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06612">http://arxiv.org/abs/2308.06612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Gavrikov, Janis Keuper</li>
<li>for: 本研究探讨了 padding 对 adversarial attack 的影响，并 analyzed 了不同的 padding 模式对 adversarial robustness 的影响。</li>
<li>methods: 本研究使用了 Convolutional Neural Networks (CNN)，并对 input 进行 padding 以 preserve 特征图像的分辨率。</li>
<li>results: 研究发现， adversarial attacks 通常会在图像边缘产生偏差异常，这些偏差异常与 padding 使用的边界有关。<details>
<summary>Abstract</summary>
It is common practice to apply padding prior to convolution operations to preserve the resolution of feature-maps in Convolutional Neural Networks (CNN). While many alternatives exist, this is often achieved by adding a border of zeros around the inputs. In this work, we show that adversarial attacks often result in perturbation anomalies at the image boundaries, which are the areas where padding is used. Consequently, we aim to provide an analysis of the interplay between padding and adversarial attacks and seek an answer to the question of how different padding modes (or their absence) affect adversarial robustness in various scenarios.
</details>
<details>
<summary>摘要</summary>
通常来说，在卷积神经网络（CNN）中，会在特征地图前加padding以保持特征地图的分辨率。虽然有很多选择，通常是通过添加边框中 zeros 来实现。在这项工作中，我们发现了敌意攻击通常会在图像边界上产生异常的扰动，这 precisamente 是 padding 使用的地方。因此，我们想进行 padding 和敌意攻击之间的分析，并查找不同 padding 模式（或其缺失）对敌意Robustness 在不同情况下的影响。
</details></li>
</ul>
<hr>
<h2 id="Bio-SIEVE-Exploring-Instruction-Tuning-Large-Language-Models-for-Systematic-Review-Automation"><a href="#Bio-SIEVE-Exploring-Instruction-Tuning-Large-Language-Models-for-Systematic-Review-Automation" class="headerlink" title="Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation"></a>Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06610">http://arxiv.org/abs/2308.06610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ambroser53/bio-sieve">https://github.com/ambroser53/bio-sieve</a></li>
<li>paper_authors: Ambrose Robinson, William Thorne, Ben P. Wu, Abdullah Pandor, Munira Essat, Mark Stevenson, Xingyi Song</li>
<li>for: 这个研究旨在探讨如何使用大自然语言模型（LLMs）支持和训练，以便在提供明确的选择标准下进行文献屏选。</li>
<li>methods: 研究使用了 instruction tuning 方法来训练 LLaMA 和 Guanaco 模型，以进行摘要屏选。</li>
<li>results: 研究发现，使用 Bio-SIEVE 模型可以超越 ChatGPT 和经过训练的传统方法，并在医学领域中更好地泛化。然而，在安全性优先的场景下，模型仍然需要进一步适应。此外，研究还探讨了多任务训练 Bio-SIEVE-Multi 模型，包括 PICO 提取和排除逻辑等任务，但发现它无法与单任务 Bio-SIEVE 的性能相比。<details>
<summary>Abstract</summary>
Medical systematic reviews can be very costly and resource intensive. We explore how Large Language Models (LLMs) can support and be trained to perform literature screening when provided with a detailed set of selection criteria. Specifically, we instruction tune LLaMA and Guanaco models to perform abstract screening for medical systematic reviews. Our best model, Bio-SIEVE, outperforms both ChatGPT and trained traditional approaches, and generalises better across medical domains. However, there remains the challenge of adapting the model to safety-first scenarios. We also explore the impact of multi-task training with Bio-SIEVE-Multi, including tasks such as PICO extraction and exclusion reasoning, but find that it is unable to match single-task Bio-SIEVE's performance. We see Bio-SIEVE as an important step towards specialising LLMs for the biomedical systematic review process and explore its future developmental opportunities. We release our models, code and a list of DOIs to reconstruct our dataset for reproducibility.
</details>
<details>
<summary>摘要</summary>
医学系统atic review可以非常昂贵和资源占用。我们探讨如何使用大型自然语言模型（LLM）支持和训练来执行文献屏选。特别是我们 instrucion 调整 LLaMA 和 Guanaco 模型来执行医学系统atic review的摘要屏选。我们的最佳模型，生物-SIEVE，超过了 ChatGPT 和训练过的传统方法，并在医学领域中更好地泛化。然而，还有一个适应模型到安全第一的挑战。我们还探讨 Bio-SIEVE 多任务训练的影响，包括 PICO 提取和排除逻辑任务，但发现它无法与单任务 Bio-SIEVE 的性能匹配。我们认为 Bio-SIEVE 是特циализиing LLMs  для医学系统atic review过程中的重要一步，并探讨其未来发展机遇。我们发布我们的模型、代码和 DOIs 以便重现我们的数据集。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/cs.AI_2023_08_13/" data-id="clohum90c0025pj884unh4zed" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/cs.CL_2023_08_13/" class="article-date">
  <time datetime="2023-08-13T11:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/13/cs.CL_2023_08_13/">cs.CL - 2023-08-13</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP"><a href="#Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP" class="headerlink" title="Faithful to Whom? Questioning Interpretability Measures in NLP"></a>Faithful to Whom? Questioning Interpretability Measures in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06795">http://arxiv.org/abs/2308.06795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evan Crothers, Herna Viktor, Nathalie Japkowicz</li>
<li>for: 这 paper 的目的是探讨现有的 faithfulness metrics 是否适用于比较不同的神经网络文本分类器的解释性。</li>
<li>methods: 作者使用 iterative masking 方法测试 faithfulness metrics，并发现这些度量在不同的模型之间存在很大的变化。</li>
<li>results: 作者发现 masked samples frequently 外部训练数据分布，并且 iterative masking 可能导致 faithfulness scores 的巨大变化。 另外，作者还研究了对 faithfulness scores 的影响，包括 adversarial attacks 和 adversarial training。<details>
<summary>Abstract</summary>
A common approach to quantifying model interpretability is to calculate faithfulness metrics based on iteratively masking input tokens and measuring how much the predicted label changes as a result. However, we show that such metrics are generally not suitable for comparing the interpretability of different neural text classifiers as the response to masked inputs is highly model-specific. We demonstrate that iterative masking can produce large variation in faithfulness scores between comparable models, and show that masked samples are frequently outside the distribution seen during training. We further investigate the impact of adversarial attacks and adversarial training on faithfulness scores, and demonstrate the relevance of faithfulness measures for analyzing feature salience in text adversarial attacks. Our findings provide new insights into the limitations of current faithfulness metrics and key considerations to utilize them appropriately.
</details>
<details>
<summary>摘要</summary>
一种常见的方法量化模型解释性是通过 iteratively masking input token 并测量预测标签变化的方式来计算 faithfulness 度量。然而，我们显示这些度量不适合比较不同的神经网络文本分类器的解释性，因为模型具有很高的特定性。我们示出了 iterative 遮盖可能会导致大量的 faithfulness 分数变化，并且遮盖样本通常不在训练时间段内。我们进一步研究了对 faithfulness 度量的影响和对文本对抗攻击的分析，并证明了 faithfulness 度量的重要性。我们的发现提供了新的理解现有 faithfulness 度量的限制和使其正确使用的关键考虑因素。
</details></li>
</ul>
<hr>
<h2 id="Modeling-the-Dashboard-Provenance"><a href="#Modeling-the-Dashboard-Provenance" class="headerlink" title="Modeling the Dashboard Provenance"></a>Modeling the Dashboard Provenance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06788">http://arxiv.org/abs/2308.06788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johne Jarske, Jorge Rady, Lucia V. L. Filgueiras, Leandro M. Velloso, Tania L. Santos</li>
<li>For: The paper aims to provide a provenance representation model for dashboards and its visual and data components, which can help organizations evaluate the quality, consistency, and reliability of the information presented on dashboards.* Methods: The proposed model will offer a comprehensive set of essential provenance metadata that enables users to evaluate the context in which a specific dashboard was developed, including information about people, organizations, entities, and activities involved in the production, influence, or delivery of the data or object.* Results: The paper aims to provide a standardized and visualized representation of provenance metadata for dashboards, which can help users make better decisions based on the quality and reliability of the information presented.<details>
<summary>Abstract</summary>
Organizations of all kinds, whether public or private, profit-driven or non-profit, and across various industries and sectors, rely on dashboards for effective data visualization. However, the reliability and efficacy of these dashboards rely on the quality of the visual and data they present. Studies show that less than a quarter of dashboards provide information about their sources, which is just one of the expected metadata when provenance is seriously considered. Provenance is a record that describes people, organizations, entities, and activities that had a role in the production, influence, or delivery of a piece of data or an object. This paper aims to provide a provenance representation model, that entitles standardization, modeling, generation, capture, and visualization, specifically designed for dashboards and its visual and data components. The proposed model will offer a comprehensive set of essential provenance metadata that enables users to evaluate the quality, consistency, and reliability of the information presented on dashboards. This will allow a clear and precise understanding of the context in which a specific dashboard was developed, ultimately leading to better decision-making.
</details>
<details>
<summary>摘要</summary>
Provenance is a record that describes people, organizations, entities, and activities that had a role in the production, influence, or delivery of a piece of data or an object. This paper aims to provide a provenance representation model, that entitles standardization, modeling, generation, capture, and visualization, specifically designed for dashboards and its visual and data components. The proposed model will offer a comprehensive set of essential provenance metadata that enables users to evaluate the quality, consistency, and reliability of the information presented on dashboards. This will allow a clear and precise understanding of the context in which a specific dashboard was developed, ultimately leading to better decision-making.
</details></li>
</ul>
<hr>
<h2 id="Token-Scaled-Logit-Distillation-for-Ternary-Weight-Generative-Language-Models"><a href="#Token-Scaled-Logit-Distillation-for-Ternary-Weight-Generative-Language-Models" class="headerlink" title="Token-Scaled Logit Distillation for Ternary Weight Generative Language Models"></a>Token-Scaled Logit Distillation for Ternary Weight Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06744">http://arxiv.org/abs/2308.06744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wonyong Sung, Jungwook Choi</li>
<li>for: 这个研究是为了解决生成模型在实际应用中的大型模型问题。</li>
<li>methods: 这个研究使用了量化测试敏感训练（QAT）方法，并提出了一个专门适用于生成模型的知识传递法。</li>
<li>results: 这个研究获得了较少于1.0倍的衰落和无损失的推理任务结果，表明了这个方法的成功。<details>
<summary>Abstract</summary>
Generative Language Models (GLMs) have shown impressive performance in tasks such as text generation, understanding, and reasoning. However, the large model size poses challenges for practical deployment. To solve this problem, Quantization-Aware Training (QAT) has become increasingly popular. However, current QAT methods for generative models have resulted in a noticeable loss of accuracy. To counteract this issue, we propose a novel knowledge distillation method specifically designed for GLMs. Our method, called token-scaled logit distillation, prevents overfitting and provides superior learning from the teacher model and ground truth. This research marks the first evaluation of ternary weight quantization-aware training of large-scale GLMs with less than 1.0 degradation in perplexity and no loss of accuracy in a reasoning task.
</details>
<details>
<summary>摘要</summary>
生成语言模型（GLM）在文本生成、理解和推理等任务中表现出色，但模型大小带来实际部署的挑战。为解决这个问题，量化意识训练（QAT）在生成模型中变得越来越流行。然而，现有的QAT方法对生成模型带来明显的精度损失。为此，我们提出了一种特有的知识储存方法，称为Token扩展LOGIT储存。该方法防止过拟合，并从教师模型和真实数据中提取优质知识。这项研究标志着大规模GLM的三进制重量量化意识训练的首次评估，并达到了低于1.0的质量下降和无损失的理解任务准确率。
</details></li>
</ul>
<hr>
<h2 id="Emergent-communication-for-AR"><a href="#Emergent-communication-for-AR" class="headerlink" title="Emergent communication for AR"></a>Emergent communication for AR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07342">http://arxiv.org/abs/2308.07342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruxiao Chen, Shuaishuai Guo</li>
<li>for: 这篇论文旨在提出一种用于Mobile Augmented Reality（MAR）的 emergent semantic communication 框架，以便在 MAR 中提高通信效率。</li>
<li>methods: 作者使用了两个代理人通过修改了 Lewis 信号游戏进行训练，以便自动生成一种简短的通信协议。</li>
<li>results: 实验表明，提出的方案在不可见对象上具有更好的泛化性，并且可以通过使用小型消息来提高通信效率。<details>
<summary>Abstract</summary>
Mobile augmented reality (MAR) is widely acknowledged as one of the ubiquitous interfaces to the digital twin and Metaverse, demanding unparalleled levels of latency, computational power, and energy efficiency. The existing solutions for realizing MAR combine multiple technologies like edge, cloud computing, and fifth-generation (5G) networks. However, the inherent communication latency of visual data imposes apparent limitations on the quality of experience (QoE). To address the challenge, we propose an emergent semantic communication framework to learn the communication protocols in MAR. Specifically, we train two agents through a modified Lewis signaling game to emerge a discrete communication protocol spontaneously. Based on this protocol, two agents can communicate about the abstract idea of visual data through messages with extremely small data sizes in a noisy channel, which leads to message errors. To better simulate real-world scenarios, we incorporate channel uncertainty into our training process. Experiments have shown that the proposed scheme has better generalization on unseen objects than traditional object recognition used in MAR and can effectively enhance communication efficiency through the utilization of small-size messages.
</details>
<details>
<summary>摘要</summary>
移动增强现实（MAR）被广泛承认为数字双胞迷和Metaverse的一种普遍的界面，需要无 précédent 的延迟、计算能力和能效率。现有的 MAR 实现方案 combining 多种技术，如边缘计算、云计算和 fifth-generation（5G）网络。然而，视觉数据的自然通信延迟带来明显的用户体验质量（QoE）限制。为 Addressing 这个挑战，我们提出了一种emergent semantic communication框架，用于在 MAR 中学习通信协议。具体来说，我们通过 modify 了 Lewis 信号游戏来训练两个代理人，从而自然地生成一个精简的通信协议。根据这个协议，两个代理人可以通过 messages  WITH  extremely small data sizes 在噪音频道中交换信息，这会导致消息错误。为更好地模拟实际情况，我们将频率uncertainty  incorporated 到我们的训练过程中。实验结果表明，我们的方案在未看到对象时比传统 MAR 中使用的对象识别更好地 generalization ，并可以通过利用小型消息来提高通信效率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/cs.CL_2023_08_13/" data-id="clohum94x0092pj88eurq0o3r" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/cs.LG_2023_08_13/" class="article-date">
  <time datetime="2023-08-13T10:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/13/cs.LG_2023_08_13/">cs.LG - 2023-08-13</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP"><a href="#Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP" class="headerlink" title="Faithful to Whom? Questioning Interpretability Measures in NLP"></a>Faithful to Whom? Questioning Interpretability Measures in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06795">http://arxiv.org/abs/2308.06795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evan Crothers, Herna Viktor, Nathalie Japkowicz</li>
<li>for: 这 paper 是为了评估不同神经网络文本分类器的解释性而写的。</li>
<li>methods: 这 paper 使用了基于层次遮盖的 faithfulness 度量来评估模型的解释性，并证明了这些度量不适合比较不同模型的解释性。</li>
<li>results: 研究发现，基于层次遮盖的 faithfulness 度量在不同模型之间可能会带来很大的差异，而且遮盖样本经常处于训练期间所未见 Distribution 之外。<details>
<summary>Abstract</summary>
A common approach to quantifying model interpretability is to calculate faithfulness metrics based on iteratively masking input tokens and measuring how much the predicted label changes as a result. However, we show that such metrics are generally not suitable for comparing the interpretability of different neural text classifiers as the response to masked inputs is highly model-specific. We demonstrate that iterative masking can produce large variation in faithfulness scores between comparable models, and show that masked samples are frequently outside the distribution seen during training. We further investigate the impact of adversarial attacks and adversarial training on faithfulness scores, and demonstrate the relevance of faithfulness measures for analyzing feature salience in text adversarial attacks. Our findings provide new insights into the limitations of current faithfulness metrics and key considerations to utilize them appropriately.
</details>
<details>
<summary>摘要</summary>
一种常见的方法量化模型解释性是通过逐渐遮盖输入符号来计算输出标签变化的程度。然而，我们显示这些度量不适合比较不同的神经网络文本分类器的解释性，因为遮盖输入的响应是高度模型特定的。我们示出了遮盖样本会导致大量的 faithfulness 分数变化，并且显示遮盖样本 frequently 外部训练数据分布。我们进一步调查了对抗攻击和对抗训练对 faithfulness 度量的影响，并证明了 faithfulness 度量对文本对抗攻击中的特征突出性进行分析具有重要意义。我们的发现为现有的 faithfulness 度量带来新的理解和使用其应用中的关键考虑因素。
</details></li>
</ul>
<hr>
<h2 id="Neural-Networks-at-a-Fraction-with-Pruned-Quaternions"><a href="#Neural-Networks-at-a-Fraction-with-Pruned-Quaternions" class="headerlink" title="Neural Networks at a Fraction with Pruned Quaternions"></a>Neural Networks at a Fraction with Pruned Quaternions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06780">http://arxiv.org/abs/2308.06780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smlab-niser/quartLT22">https://github.com/smlab-niser/quartLT22</a></li>
<li>paper_authors: Sahel Mohammad Iqbal, Subhankar Mishra</li>
<li>for: 这个研究旨在测试在极端资源受限的环境中，使用简单的神经网络来进行预测。</li>
<li>methods: 研究使用删减来简化神经网络中的参数数量，并使用高维数据嵌入来维持预测精度。</li>
<li>results: 研究发现，在某些架构和 dataset 上，删减后的数值网络可以超过相同架构的实际网络。例如，在 CIFAR-10 上使用 Conv-4 架构时，删减后的数值网络在 $3%$ 的参数数量下，可以比实际网络高于 $10%$。<details>
<summary>Abstract</summary>
Contemporary state-of-the-art neural networks have increasingly large numbers of parameters, which prevents their deployment on devices with limited computational power. Pruning is one technique to remove unnecessary weights and reduce resource requirements for training and inference. In addition, for ML tasks where the input data is multi-dimensional, using higher-dimensional data embeddings such as complex numbers or quaternions has been shown to reduce the parameter count while maintaining accuracy. In this work, we conduct pruning on real and quaternion-valued implementations of different architectures on classification tasks. We find that for some architectures, at very high sparsity levels, quaternion models provide higher accuracies than their real counterparts. For example, at the task of image classification on CIFAR-10 using Conv-4, at $3\%$ of the number of parameters as the original model, the pruned quaternion version outperforms the pruned real by more than $10\%$. Experiments on various network architectures and datasets show that for deployment in extremely resource-constrained environments, a sparse quaternion network might be a better candidate than a real sparse model of similar architecture.
</details>
<details>
<summary>摘要</summary>
当代最先进的神经网络具有越来越多的参数，这限制了它们在计算能力有限的设备上进行训练和推理的可行性。剪枝是一种技术，可以从神经网络中移除不必要的权重，以降低训练和推理的资源需求。此外，在多维输入数据的机器学习任务中，使用高维数域嵌入，如复数或四元数，可以降低参数数量而保持准确性。在这项工作中，我们对实际值和四元数值实现的不同架构进行剪枝处理，并发现在某些架构上，难以架构的四元数模型在高度精简率下提供更高的准确性。例如，在使用Conv-4架构进行图像分类任务时，采用了$3\%$的参数数量的剪枝后，四元数模型的准确性高于实际值模型的准确性超过$10\%$。经过了不同的网络架构和数据集的实验，我们发现在极其有限的资源环境中，一个稀疏的四元数网络可能比同类架构的实际稀疏模型更适合进行部署。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations"><a href="#A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations" class="headerlink" title="A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations"></a>A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06767">http://arxiv.org/abs/2308.06767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrcheng1066/awesome-pruning">https://github.com/hrcheng1066/awesome-pruning</a></li>
<li>paper_authors: Hongrong Cheng, Miao Zhang, Javen Qinfeng Shi</li>
<li>for: 本文提供了现代深度神经网络压缩的综述，尤其是最新的大型语言模型，以及压缩方法的批判和评价。</li>
<li>methods: 本文分类了现有的压缩研究工作，包括一般&#x2F;特定加速、压缩时机、压缩方法和压缩与其他压缩技术的融合。</li>
<li>results: 本文提供了七对对比设定的深度神经网络压缩的比较分析，并探讨了emerging topics such as post-training pruning, different levels of supervision for pruning, and broader applications。<details>
<summary>Abstract</summary>
Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of seven pairs of contrast settings for pruning (e.g., unstructured/structured) and explore emerging topics, including post-training pruning, different levels of supervision for pruning, and broader applications (e.g., adversarial robustness) to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. To facilitate future research, we build a curated collection of datasets, networks, and evaluations on different applications. Finally, we provide some valuable recommendations on selecting pruning methods and prospect promising research directions. We build a repository at https://github.com/hrcheng1066/awesome-pruning.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络，特别是最近的大型语言模型，具有庞大的计算和存储资源需求。为实现资源约束环境中部署现代模型和加速推理时间，研究人员开始了压缩神经网络的研究，成为现代神经网络压缩的流行研究方向。然而，当前存在相对落后的压缩研究报告。为解决这问题，在这篇评论中，我们提供了一个包括1)通用/特定速度、2)何时压缩、3)如何压缩和4)压缩与其他压缩技术融合的taxonomy的全面评论。然后，我们对7对对比设定进行了综合分析（例如，无结构/结构），并探讨了emerging topics（例如，后处理压缩、不同级别的监督压缩和更广泛的应用，例如对抗攻击），以抛光现有方法的相似性和差异，并为未来的研究提供了基础。为便于未来的研究，我们创建了一个 curaated 的数据集、网络和评估集。最后，我们提供了一些有价值的建议，包括选择压缩方法和前景探索的可能性，以及未来研究的可能性。我们在 <https://github.com/hrcheng1066/awesome-pruning> 上建立了一个存储库。
</details></li>
</ul>
<hr>
<h2 id="Conic-Descent-Redux-for-Memory-Efficient-Optimization"><a href="#Conic-Descent-Redux-for-Memory-Efficient-Optimization" class="headerlink" title="Conic Descent Redux for Memory-Efficient Optimization"></a>Conic Descent Redux for Memory-Efficient Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07343">http://arxiv.org/abs/2308.07343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingcong Li, Georgios B. Giannakis</li>
<li>for: 本研究探讨了一种最近发展的首项凹降（CD）解决方案，并在三个方面进行了改进：intuition、理论和算法实现。</li>
<li>methods: 本研究发现CD可以提供一种直观的几何 derivation，来自对准题的 dual 问题。这开启了新的算法设计的门户，其中一个是旋转 variant of CD（MOCO）的示例。透过分析 CD 和 MOCO 的双重行为，发现：i) 可以分析性地确定停止标准；ii) 可以设计预conditioners 以加速双方的准确。</li>
<li>results: 最后，本研究开发了一种内存效率高的 MOCO 变体，用于扩展 SDP 特别是低级解。numerical validation 表明，这种变体可以快速和精准地解决 SDP 问题。<details>
<summary>Abstract</summary>
Conic programming has well-documented merits in a gamut of signal processing and machine learning tasks. This contribution revisits a recently developed first-order conic descent (CD) solver, and advances it in three aspects: intuition, theory, and algorithmic implementation. It is found that CD can afford an intuitive geometric derivation that originates from the dual problem. This opens the door to novel algorithmic designs, with a momentum variant of CD, momentum conic descent (MOCO) exemplified. Diving deeper into the dual behavior CD and MOCO reveals: i) an analytically justified stopping criterion; and, ii) the potential to design preconditioners to speed up dual convergence. Lastly, to scale semidefinite programming (SDP) especially for low-rank solutions, a memory efficient MOCO variant is developed and numerically validated.
</details>
<details>
<summary>摘要</summary>
带形编程在信号处理和机器学习任务中有良好的记录。这篇论文探讨了最近开发的首项对数算法（CD）解决方案，并在三个方面提高：直观、理论和算法实现。发现CD可以提供直观的几何 derivation，这开启了新的算法设计的门户，例如帕摩散度降低（MOCO）。透过对CD和MOCO的分析，发现：一、分析正确的停止标准；二、设计加速对偶速度的预处理器。最后，为了扩大低级解的SDP，我们开发了内存有效的MOCO变体，并在数值上验证了其正确性。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Class-incremental-Learning-A-Survey"><a href="#Few-shot-Class-incremental-Learning-A-Survey" class="headerlink" title="Few-shot Class-incremental Learning: A Survey"></a>Few-shot Class-incremental Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06764">http://arxiv.org/abs/2308.06764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinghua Zhang, Li Liu, Olli Silven, Matti Pietikäinen, Dewen Hu</li>
<li>for: 本文提供了一个系统性的和深入的简要评论，涵盖了多类增量学习（Few-shot Class-Incremental Learning，FSCIL）领域的各种方面，包括问题定义、基本挑战、一般方案、相关逻辑和评价指标等。</li>
<li>methods: 本文总结了FSCIL中的一些常见方法，包括基于数据、基于结构和优化基的方法，以及对象检测方法的各种改进方法，如 anchor-free 和 anchor-based 方法。</li>
<li>results: 本文提供了一些在FSCIL领域的研究方向，包括数据-based、结构-based 和优化-based 方法，以及一些需要进一步探索的研究方向。<details>
<summary>Abstract</summary>
Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge in machine learning, as it necessitates the continuous learning of new classes from sparse labeled training samples without forgetting previous knowledge. While this field has seen recent progress, it remains an active area of exploration. This paper aims to provide a comprehensive and systematic review of FSCIL. In our in-depth examination, we delve into various facets of FSCIL, encompassing the problem definition, the discussion of primary challenges of unreliable empirical risk minimization and the stability-plasticity dilemma, general schemes, and relevant problems of incremental learning and few-shot learning. Besides, we offer an overview of benchmark datasets and evaluation metrics. Furthermore, we introduce the classification methods in FSCIL from data-based, structure-based, and optimization-based approaches and the object detection methods in FSCIL from anchor-free and anchor-based approaches. Beyond these, we illuminate several promising research directions within FSCIL that merit further investigation.
</details>
<details>
<summary>摘要</summary>
《几个示例学习（Few-shot Class-Incremental Learning，FSCIL）》是机器学习领域中的一个独特挑战，它需要在缺乏标注训练样本的情况下，不断学习新的类型，而不会忘记之前的知识。尽管这一领域在最近几年内已经取得了一些进展，但仍然是一个活跃的探索领域。本文的目标是提供一个全面和系统的FSCIL评审，包括问题定义、主要挑战的不可靠的实际风险最小化和稳定性-柔软性之间的矛盾、通用方案和相关的增量学习和几个示例学习的问题。此外，我们还介绍了评价指标和标准测试集。进而，我们介绍了FSCIL中的分类方法，包括数据基于、结构基于和优化基于的方法，以及对象检测方法，包括无锚和锚基的方法。此外，我们还逐光了一些在FSCIL中的有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Discovering-the-Symptom-Patterns-of-COVID-19-from-Recovered-and-Deceased-Patients-Using-Apriori-Association-Rule-Mining"><a href="#Discovering-the-Symptom-Patterns-of-COVID-19-from-Recovered-and-Deceased-Patients-Using-Apriori-Association-Rule-Mining" class="headerlink" title="Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining"></a>Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06763">http://arxiv.org/abs/2308.06763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Dehghani, Zahra Yazdanparast, Mobin Mohammadi</li>
<li>for: 该研究用于挖掘COVID-19患者的症状模式，以帮助临床医生更好地诊断和治疗疾病。</li>
<li>methods: 该研究使用了Apriori算法进行协会规则挖掘，从COVID-19患者的临床数据中挖掘出最常见的症状。</li>
<li>results: 研究结果显示，COVID-19患者最常见的症状包括呼吸停止（72%）、咳嗽（64%）、发热（59%）、衰弱（18%）、肌肉疼痛（14.5%）和喉咙痛（12%）。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has a devastating impact globally, claiming millions of lives and causing significant social and economic disruptions. In order to optimize decision-making and allocate limited resources, it is essential to identify COVID-19 symptoms and determine the severity of each case. Machine learning algorithms offer a potent tool in the medical field, particularly in mining clinical datasets for useful information and guiding scientific decisions. Association rule mining is a machine learning technique for extracting hidden patterns from data. This paper presents an application of association rule mining based Apriori algorithm to discover symptom patterns from COVID-19 patients. The study, using 2875 records of patient, identified the most common symptoms as apnea (72%), cough (64%), fever (59%), weakness (18%), myalgia (14.5%), and sore throat (12%). The proposed method provides clinicians with valuable insight into disease that can assist them in managing and treating it effectively.
</details>
<details>
<summary>摘要</summary>
COVID-19 流行病在全球产生了毁灭性的影响，让数百万人丧生，引起了重大的社会和经济干扰。为了优化决策和分配有限的资源，必须识别 COVID-19 症状并评估每个病例的严重程度。机器学习算法在医疗领域中提供了一个强大的工具，特别是在挖掘医疗数据中找到有用信息和导引科学决策。在这篇文章中，我们使用 Apriori 算法进行协会规则挖掘，以找到 COVID-19 患者的症状模式。研究使用了 2875 份病例数据，发现最常见的症状包括呼吸抑制（72%）、咳嗽（64%）、高烧（59%）、衰弱（18%）、肌痛（14.5%）和喉咙痛（12%）。我们的方法可以帮助医生更好地理解这种疾病，从而更有效地诊治和治疗。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization"><a href="#Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization" class="headerlink" title="Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization"></a>Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06741">http://arxiv.org/abs/2308.06741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mehdi Nasiri, Mansoor Rezghi</li>
<li>for: 这个研究旨在解决多智能机器人学习（Multi-Agent Reinforcement Learning，MARL）中参与者的不同能力和个人策略问题。</li>
<li>methods: 提案的Heterogeneous-Agent Mirror Descent Policy Optimization（HAMDPO）算法利用多智能机器人优势分解定理来实现每个代理策略的有效更新，并确保总性表现提高。HAMDPO通过迭代更新代理策略的近似解决信任区域问题，以确保稳定性和表现改善。</li>
<li>results: 在Multi-Agent MuJoCo和StarCraftII任务中，HAMDPO比state-of-the-art算法HATRPO和HAPPO表现出色，实现了稳定性和表现提高。这些结果显示HAMDPO是解决合作MARL问题的有望方法，可能会扩展到其他MARL领域中的挑战性问题。<details>
<summary>Abstract</summary>
This paper presents an extension of the Mirror Descent method to overcome challenges in cooperative Multi-Agent Reinforcement Learning (MARL) settings, where agents have varying abilities and individual policies. The proposed Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO) algorithm utilizes the multi-agent advantage decomposition lemma to enable efficient policy updates for each agent while ensuring overall performance improvements. By iteratively updating agent policies through an approximate solution of the trust-region problem, HAMDPO guarantees stability and improves performance. Moreover, the HAMDPO algorithm is capable of handling both continuous and discrete action spaces for heterogeneous agents in various MARL problems. We evaluate HAMDPO on Multi-Agent MuJoCo and StarCraftII tasks, demonstrating its superiority over state-of-the-art algorithms such as HATRPO and HAPPO. These results suggest that HAMDPO is a promising approach for solving cooperative MARL problems and could potentially be extended to address other challenging problems in the field of MARL.
</details>
<details>
<summary>摘要</summary>
The HAMDPO algorithm uses the multi-agent advantage decomposition lemma to efficiently update agent policies while ensuring overall performance improvements. The algorithm iteratively updates agent policies through an approximate solution of the trust-region problem, which guarantees stability and improves performance.HAMDPO is capable of handling both continuous and discrete action spaces for heterogeneous agents in various MARL problems. The authors evaluate the algorithm on Multi-Agent MuJoCo and StarCraftII tasks and show that it outperforms state-of-the-art algorithms such as HATRPO and HAPPO. These results suggest that HAMDPO is a promising approach for solving cooperative MARL problems and could potentially be extended to address other challenging problems in the field of MARL.
</details></li>
</ul>
<hr>
<h2 id="Weighted-Sparse-Partial-Least-Squares-for-Joint-Sample-and-Feature-Selection"><a href="#Weighted-Sparse-Partial-Least-Squares-for-Joint-Sample-and-Feature-Selection" class="headerlink" title="Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection"></a>Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06740">http://arxiv.org/abs/2308.06740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenwenmin/wspls">https://github.com/wenwenmin/wspls</a></li>
<li>paper_authors: Wenwen Min, Taosheng Xu, Chris Ding</li>
<li>for: 这种研究旨在扩展sPLS的应用范围，通过特定subset of samples和减少异常值来检测稀疏的数据集。</li>
<li>methods: 该研究提出了一种$\ell_\infty&#x2F;\ell_0$-norm压缩权重稀疏PLS（wsPLS）方法，通过$\ell_\infty&#x2F;\ell_0$-norm压缩来选择一个subset of samples，并使用多视图数据可以处理多个数据集。</li>
<li>results: 研究人员通过数值和生物医学数据实验表明，提出的方法可以减少数据维度，提高数据融合的稳定性和准确性。<details>
<summary>Abstract</summary>
Sparse Partial Least Squares (sPLS) is a common dimensionality reduction technique for data fusion, which projects data samples from two views by seeking linear combinations with a small number of variables with the maximum variance. However, sPLS extracts the combinations between two data sets with all data samples so that it cannot detect latent subsets of samples. To extend the application of sPLS by identifying a specific subset of samples and remove outliers, we propose an $\ell_\infty/\ell_0$-norm constrained weighted sparse PLS ($\ell_\infty/\ell_0$-wsPLS) method for joint sample and feature selection, where the $\ell_\infty/\ell_0$-norm constrains are used to select a subset of samples. We prove that the $\ell_\infty/\ell_0$-norm constrains have the Kurdyka-\L{ojasiewicz}~property so that a globally convergent algorithm is developed to solve it. Moreover, multi-view data with a same set of samples can be available in various real problems. To this end, we extend the $\ell_\infty/\ell_0$-wsPLS model and propose two multi-view wsPLS models for multi-view data fusion. We develop an efficient iterative algorithm for each multi-view wsPLS model and show its convergence property. As well as numerical and biomedical data experiments demonstrate the efficiency of the proposed methods.
</details>
<details>
<summary>摘要</summary>
“罕缺部分最小方差（sPLS）是一种常见的维度减少技术，用于数据融合，它通过寻找两个视图中数据样本的线性组合，以实现最大差异。然而，sPLS不能检测隐藏的样本集。为了扩展sPLS的应用，我们提出了一种$\ell_\infty/\ell_0$-norm受限的重量 sparse PLS（$\ell_\infty/\ell_0$-wsPLS）方法，用于联合样本和特征选择。我们证明了$\ell_\infty/\ell_0$-norm受限有 Kurdyka-\L{ojasiewicz} 性质，因此可以开发一个全球收敛的算法来解决它。此外，多视图数据中的样本可能是同一个集合的。为此，我们扩展了$\ell_\infty/\ell_0$-wsPLS模型，并提出了两种多视图wsPLS模型 для多视图数据融合。我们开发了一个高效的迭代算法，并证明其收敛性。数值和生物医学数据实验 demonstrate了我们提出的方法的效率。”Note: Simplified Chinese is a written form of Chinese that uses simpler characters and grammar than Traditional Chinese. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data"><a href="#Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data" class="headerlink" title="Probabilistic Imputation for Time-series Classification with Missing Data"></a>Probabilistic Imputation for Time-series Classification with Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06738">http://arxiv.org/abs/2308.06738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout">https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout</a></li>
<li>paper_authors: SeungHyun Kim, Hyunsu Kim, EungGu Yun, Hwangrae Lee, Jaehun Lee, Juho Lee</li>
<li>for: 这个论文主要是为了解决多重时间序列资料中的缺失价值问题。</li>
<li>methods: 我们提出了一个新的机会统计学 frameworks，它包括两个部分：一个深度生成模型来填写缺失价值，以及一个分类器。我们将深度生成模型扩展到更好地捕捉时间序列资料的结构，并将分类器训练为将时间序列资料与填写的缺失价值分类。</li>
<li>results: 我们通过实际实验表明，我们的方法可以有效地解决多重时间序列资料中的缺失价值问题，并且可以提供更好的预测结果。<details>
<summary>Abstract</summary>
Multivariate time series data for real-world applications typically contain a significant amount of missing values. The dominant approach for classification with such missing values is to impute them heuristically with specific values (zero, mean, values of adjacent time-steps) or learnable parameters. However, these simple strategies do not take the data generative process into account, and more importantly, do not effectively capture the uncertainty in prediction due to the multiple possibilities for the missing values. In this paper, we propose a novel probabilistic framework for classification with multivariate time series data with missing values. Our model consists of two parts; a deep generative model for missing value imputation and a classifier. Extending the existing deep generative models to better capture structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation. The classifier part takes the time series data along with the imputed missing values and classifies signals, and is trained to capture the predictive uncertainty due to the multiple possibilities of imputations. Importantly, we show that na\"ively combining the generative model and the classifier could result in trivial solutions where the generative model does not produce meaningful imputations. To resolve this, we present a novel regularization technique that can promote the model to produce useful imputation values that help classification. Through extensive experiments on real-world time series data with missing values, we demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
多变量时间序列数据在实际应用中通常含有大量缺失值。现有的主流方法为这种缺失值是轮廓性地填充它们（零、平均值、邻近时间步颗度）或学习参数。然而，这些简单策略并不考虑数据生成过程，更重要的是，它们不能有效捕捉预测中的不确定性，因为缺失值的多种可能性。在这篇论文中，我们提出了一种新的概率 Framework for classification with multivariate time series data containing missing values.我们的模型包括两部分：深度生成模型和分类器。我们对深度生成模型进行了扩展，以更好地捕捉时间序列数据的结构，并训练它们以生成多种可能的缺失值，以模拟缺失值的uncertainty。分类器部分接受了时间序列数据以及填充后的缺失值，并分类信号，并训练它们以捕捉多种缺失值的预测不确定性。然而，我们发现，直接组合生成模型和分类器可能会导致轻微的解决方案，其中生成模型不会生成有用的填充值。为解决这个问题，我们提出了一种新的规范技术，可以促进模型生成有用的填充值，以便分类。通过对实际时间序列数据进行了广泛的实验，我们证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Precipitation-nowcasting-with-generative-diffusion-models"><a href="#Precipitation-nowcasting-with-generative-diffusion-models" class="headerlink" title="Precipitation nowcasting with generative diffusion models"></a>Precipitation nowcasting with generative diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06733">http://arxiv.org/abs/2308.06733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models">https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models</a></li>
<li>paper_authors: Andrea Asperti, Fabio Merizzi, Alberto Paparella, Giorgio Pedrazzi, Matteo Angelinelli, Stefano Colamonaco</li>
<li>For: 这个研究是用来测试深度学习方法在气象预报中的精度。* Methods: 这个研究使用了数种深度学习模型，包括生成模型、Variational Autoencoders和抑制算法。* Results: 研究发现，使用生成ensemble扩展（GED）模型可以对于降水预报提供更高的精度，比起现有的深度学习模型。<details>
<summary>Abstract</summary>
In recent years traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Numerous historical datasets used for short and medium-range weather forecasts are typically organized into a regular spatial grid structure. This arrangement closely resembles images: each weather variable can be visualized as a map or, when considering the temporal axis, as a video. Several classes of generative models, comprising Generative Adversarial Networks, Variational Autoencoders, or the recent Denoising Diffusion Models have largely proved their applicability to the next-frame prediction problem, and is thus natural to test their performance on the weather prediction benchmarks. Diffusion models are particularly appealing in this context, due to the intrinsically probabilistic nature of weather forecasting: what we are really interested to model is the probability distribution of weather indicators, whose expected value is the most likely prediction.   In our study, we focus on a specific subset of the ERA-5 dataset, which includes hourly data pertaining to Central Europe from the years 2016 to 2021. Within this context, we examine the efficacy of diffusion models in handling the task of precipitation nowcasting. Our work is conducted in comparison to the performance of well-established U-Net models, as documented in the existing literature. Our proposed approach of Generative Ensemble Diffusion (GED) utilizes a diffusion model to generate a set of possible weather scenarios which are then amalgamated into a probable prediction via the use of a post-processing network. This approach, in comparison to recent deep learning models, substantially outperformed them in terms of overall performance.
</details>
<details>
<summary>摘要</summary>
Recently, traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Historical weather data used for short and medium-range forecasts are typically organized into a regular spatial grid structure, resembling images or videos when considering the temporal axis. Generative models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Denoising Diffusion Models (DDMs) have shown great potential in predicting the next frame of weather patterns. Diffusion models are particularly appealing in this context, as weather forecasting is inherently probabilistic and what we are really interested in modeling is the probability distribution of weather indicators.In our study, we focus on a specific subset of the ERA-5 dataset, which includes hourly data for Central Europe from 2016 to 2021. We examine the efficacy of diffusion models in handling the task of precipitation nowcasting and compare their performance to well-established U-Net models. Our proposed approach, Generative Ensemble Diffusion (GED), utilizes a diffusion model to generate a set of possible weather scenarios, which are then combined into a probable prediction using a post-processing network. This approach outperforms recent deep learning models in terms of overall performance.
</details></li>
</ul>
<hr>
<h2 id="Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables"><a href="#Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables" class="headerlink" title="Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables"></a>Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06718">http://arxiv.org/abs/2308.06718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Xie, Biwei Huang, Zhengming Chen, Ruichu Cai, Clark Glymour, Zhi Geng, Kun Zhang</li>
<li>for: The paper is written for learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables.</li>
<li>methods: The paper proposes a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. The paper also provides necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models.</li>
<li>results: The paper shows that the proposed GIN condition, together with a well-designed search procedure, can be used to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. The paper also demonstrates the effectiveness of the proposed approach through experimental results.<details>
<summary>Abstract</summary>
We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous set $\mathcal{S}$ relative to the parent set of $\mathbf{Y}$ (w.r.t. the causal ordering), such that $\mathcal{S}$ d-separates $\mathbf{Y}$ from $\mathbf{Z}$. Interestingly, we find that the independent noise condition (i.e., if there is no confounder, causes are independent of the residual derived from regressing the effect on the causes) can be seen as a special case of GIN. With such a connection between GIN and latent causal structures, we further leverage the proposed GIN condition, together with a well-designed search procedure, to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. We show that the underlying causal structure of a LiNGLaH is identifiable in light of GIN conditions under mild assumptions. Experimental results show the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:我们研究一个复杂的任务，即在存在隐变量的情况下学习 causal 结构，包括找到隐变量的位置和量，以及确定隐变量和观测变量之间的 causal 关系。为此，我们提出了一种 Generalized Independent Noise (GIN) 条件，用于 linear non-Gaussian 隐变量模型，该条件 garanties that a linear combination of certain observed variables and some other observed variables are independent. Specifically, for two observed random vectors $\mathbf{Y}$ and $\mathbf{Z}$, GIN holds if and only if $\omega^\top \mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then provide necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous set $\mathcal{S}$ relative to the parent set of $\mathbf{Y}$ (w.r.t. the causal ordering), such that $\mathcal{S}$ d-separates $\mathbf{Y}$ from $\mathbf{Z}$. Interestingly, we find that the independent noise condition (i.e., if there is no confounder, causes are independent of the residual derived from regressing the effect on the causes) can be seen as a special case of GIN. With such a connection between GIN and latent causal structures, we further leverage the proposed GIN condition, together with a well-designed search procedure, to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. We show that the underlying causal structure of a LiNGLaH is identifiable in light of GIN conditions under mild assumptions. Experimental results show the effectiveness of the proposed approach.
</details></li>
</ul>
<hr>
<h2 id="Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards"><a href="#Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards" class="headerlink" title="Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards"></a>Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06717">http://arxiv.org/abs/2308.06717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilgin Dogan, Zuo-Jun Max Shen, Anil Aswani<br>for: This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal in a setting where the principal cannot observe the agent’s reward realizations.methods: The paper uses a multi-armed bandit (MAB) problem to model the agent’s learning and a parallel algorithm for the principal to consistently estimate the agent’s unknown rewards while maximizing their own utility.results: The paper proves finite-sample consistency of an estimator and a rigorous regret bound for the principal by considering the sequential externality imposed by the agent, and simulations justify the applicability of the framework to green energy aggregator contracts.<details>
<summary>Abstract</summary>
In practice, incentive providers (i.e., principals) often cannot observe the reward realizations of incentivized agents, which is in contrast to many principal-agent models that have been previously studied. This information asymmetry challenges the principal to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, which becomes even more challenging when the agent has to learn its own rewards. This complex setting is observed in various real-life scenarios ranging from renewable energy storage contracts to personalized healthcare incentives. Hence, it offers not only interesting theoretical questions but also wide practical relevance. This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal. The agent tackles a multi-armed bandit (MAB) problem to maximize their expected reward plus incentive. On top of the agent's learning, the principal trains a parallel algorithm and faces a trade-off between consistently estimating the agent's unknown rewards and maximizing their own utility by offering adaptive incentives to lead the agent. For a non-parametric model, we introduce an estimator whose only input is the history of principal's incentives and agent's choices. We unite this estimator with a proposed data-driven incentive policy within a MAB framework. Without restricting the type of the agent's algorithm, we prove finite-sample consistency of the estimator and a rigorous regret bound for the principal by considering the sequential externality imposed by the agent. Lastly, our theoretical results are reinforced by simulations justifying applicability of our framework to green energy aggregator contracts.
</details>
<details>
<summary>摘要</summary>
在实践中，奖励提供者（即主体）经常无法观察奖励的实现情况，这与许多主体-代理模型不同，这种信息不均衡会让主体难以透过决策来估计代理人的未知奖励，这变得更加复杂，当代理人需要学习自己的奖励时。这种复杂的设定在各种实际场景中出现，包括可再生能源存储合同和个性化医疗奖励。因此，它不仅存在许多理论问题，还有广泛的实际应用。本文研究了一个反复的对抗选择游戏，其中一个自利主义学习代理人与一个学习主体之间进行交互。代理人面临多支枪战（MAB）问题，以最大化他们的预期奖励加上奖励。除了代理人的学习之外，主体还需要训练一个平行算法，并面临一种奖励优化和代理人奖励的负担。为了不假设代理人的算法类型，我们提出了一种无参数的估计器，其唯一的输入是主体的奖励历史和代理人的选择。我们将这种估计器与一种基于MAB框架的数据驱动奖励策略联系起来。我们证明了这种估计器的finite-sample consistent性和对主体的正确做出约束。最后，我们通过实验证明了我们的框架在绿色能源总包合同中的应用可行性。
</details></li>
</ul>
<hr>
<h2 id="CDR-Conservative-Doubly-Robust-Learning-for-Debiased-Recommendation"><a href="#CDR-Conservative-Doubly-Robust-Learning-for-Debiased-Recommendation" class="headerlink" title="CDR: Conservative Doubly Robust Learning for Debiased Recommendation"></a>CDR: Conservative Doubly Robust Learning for Debiased Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08461">http://arxiv.org/abs/2308.08461</a></li>
<li>repo_url: None</li>
<li>paper_authors: ZiJie Song, JiaWei Chen, Sheng Zhou, QiHao Shi, Yan Feng, Chun Chen, Can Wang</li>
<li>for: 提高推荐系统中偏见的稳定性和性能</li>
<li>methods: 使用 Conservative Doubly Robust 策略（CDR），包括对填充值进行筛选和分析，以减少偏见的影响</li>
<li>results: 比较 experiments 表明，CDR 可以提高推荐系统的性能，同时减少偏见的频率<details>
<summary>Abstract</summary>
In recommendation systems (RS), user behavior data is observational rather than experimental, resulting in widespread bias in the data. Consequently, tackling bias has emerged as a major challenge in the field of recommendation systems. Recently, Doubly Robust Learning (DR) has gained significant attention due to its remarkable performance and robust properties. However, our experimental findings indicate that existing DR methods are severely impacted by the presence of so-called Poisonous Imputation, where the imputation significantly deviates from the truth and becomes counterproductive.   To address this issue, this work proposes Conservative Doubly Robust strategy (CDR) which filters imputations by scrutinizing their mean and variance. Theoretical analyses show that CDR offers reduced variance and improved tail bounds.In addition, our experimental investigations illustrate that CDR significantly enhances performance and can indeed reduce the frequency of poisonous imputation.
</details>
<details>
<summary>摘要</summary>
在推荐系统（RS）中，用户行为数据是观察性的而不是实验性的，导致数据中存在普遍的偏见。因此，解决偏见问题已成为推荐系统领域的主要挑战。近些年来，双重稳健学习（DR）已经受到了广泛关注，因为它的表现良好和稳健性。然而，我们的实验结果表明，现有的DR方法受到 socalled "poisonous imputation" 的影响，其中的填充数据显著不符合事实，甚至变得counterproductive。为解决这个问题，本工作提出了 Conservative Doubly Robust 策略（CDR），该策略通过评估填充数据的mean和variance来筛选填充。理论分析表明，CDR可以降低方差和提高尾 bounds。此外，我们的实验研究表明，CDR可以显著提高性能，并可以减少poisonous imputation的频率。
</details></li>
</ul>
<hr>
<h2 id="Learning-on-Graphs-with-Out-of-Distribution-Nodes"><a href="#Learning-on-Graphs-with-Out-of-Distribution-Nodes" class="headerlink" title="Learning on Graphs with Out-of-Distribution Nodes"></a>Learning on Graphs with Out-of-Distribution Nodes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06714">http://arxiv.org/abs/2308.06714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songyyyy/kdd22-oodgat">https://github.com/songyyyy/kdd22-oodgat</a></li>
<li>paper_authors: Yu Song, Donglin Wang</li>
<li>for: 本文旨在Addressing the problem of graph learning with out-of-distribution nodes, including detecting nodes that do not belong to the known distribution and classifying the remaining nodes to be one of the known classes.</li>
<li>methods: 本文提出了一种新的Graph Attention Network（GAT）模型，即Out-of-Distribution Graph Attention Network（OODGAT），该模型可以Explicitly model the interaction between different kinds of nodes and separate inliers from outliers during feature propagation.</li>
<li>results: 实验表明，OODGAT比现有的异常检测方法表现出较大的优势，同时与现有的分类方法相比，OODGAT的分类性能也是比较良好的。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are state-of-the-art models for performing prediction tasks on graphs. While existing GNNs have shown great performance on various tasks related to graphs, little attention has been paid to the scenario where out-of-distribution (OOD) nodes exist in the graph during training and inference. Borrowing the concept from CV and NLP, we define OOD nodes as nodes with labels unseen from the training set. Since a lot of networks are automatically constructed by programs, real-world graphs are often noisy and may contain nodes from unknown distributions. In this work, we define the problem of graph learning with out-of-distribution nodes. Specifically, we aim to accomplish two tasks: 1) detect nodes which do not belong to the known distribution and 2) classify the remaining nodes to be one of the known classes. We demonstrate that the connection patterns in graphs are informative for outlier detection, and propose Out-of-Distribution Graph Attention Network (OODGAT), a novel GNN model which explicitly models the interaction between different kinds of nodes and separate inliers from outliers during feature propagation. Extensive experiments show that OODGAT outperforms existing outlier detection methods by a large margin, while being better or comparable in terms of in-distribution classification.
</details>
<details>
<summary>摘要</summary>
图ael Neural Networks (GNNs) 是当前最佳模型 для图ael任务中的预测模型。 Although existing GNNs have shown great performance on various graph-related tasks, little attention has been paid to the scenario where out-of-distribution (OOD) nodes exist in the graph during training and inference. Based on the concept from CV and NLP, we define OOD nodes as nodes with labels not seen in the training set. Since many networks are automatically constructed by programs, real-world graphs are often noisy and may contain nodes from unknown distributions. In this work, we define the problem of graph learning with out-of-distribution nodes. Specifically, we aim to accomplish two tasks: 1) detect nodes that do not belong to the known distribution and 2) classify the remaining nodes as one of the known classes. We demonstrate that the connection patterns in graphs are informative for outlier detection, and propose Out-of-Distribution Graph Attention Network (OODGAT), a novel GNN model that explicitly models the interaction between different types of nodes and separates inliers from outliers during feature propagation. Extensive experiments show that OODGAT outperforms existing outlier detection methods by a large margin, while being better or comparable in terms of in-distribution classification.
</details></li>
</ul>
<hr>
<h2 id="The-Hard-Constraint-PINNs-for-Interface-Optimal-Control-Problems"><a href="#The-Hard-Constraint-PINNs-for-Interface-Optimal-Control-Problems" class="headerlink" title="The Hard-Constraint PINNs for Interface Optimal Control Problems"></a>The Hard-Constraint PINNs for Interface Optimal Control Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06709">http://arxiv.org/abs/2308.06709</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tianyouzeng/pinns-interface-optimal-control">https://github.com/tianyouzeng/pinns-interface-optimal-control</a></li>
<li>paper_authors: Ming-Chih Lai, Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng</li>
<li>for:  solves optimal control problems subject to partial differential equations (PDEs) with interfaces and some control constraints.</li>
<li>methods:  combines physics-informed neural networks (PINNs) with recently developed discontinuity capturing neural networks to solve the problems.</li>
<li>results:  guarantees that both the boundary and interface conditions can be satisfied exactly, and is efficient for elliptic and parabolic interface optimal control problems.Here’s the full summary in Simplified Chinese:</li>
<li>for:  solves optimal control problems subject to PDEs with interfaces and control constraints.</li>
<li>methods:  combines PINNs with discontinuity capturing neural networks.</li>
<li>results:  guarantees exact satisfaction of boundary and interface conditions, and is efficient for elliptic and parabolic interface optimal control problems.<details>
<summary>Abstract</summary>
We show that the physics-informed neural networks (PINNs), in combination with some recently developed discontinuity capturing neural networks, can be applied to solve optimal control problems subject to partial differential equations (PDEs) with interfaces and some control constraints. The resulting algorithm is mesh-free and scalable to different PDEs, and it ensures the control constraints rigorously. Since the boundary and interface conditions, as well as the PDEs, are all treated as soft constraints by lumping them into a weighted loss function, it is necessary to learn them simultaneously and there is no guarantee that the boundary and interface conditions can be satisfied exactly. This immediately causes difficulties in tuning the weights in the corresponding loss function and training the neural networks. To tackle these difficulties and guarantee the numerical accuracy, we propose to impose the boundary and interface conditions as hard constraints in PINNs by developing a novel neural network architecture. The resulting hard-constraint PINNs approach guarantees that both the boundary and interface conditions can be satisfied exactly and they are decoupled from the learning of the PDEs. Its efficiency is promisingly validated by some elliptic and parabolic interface optimal control problems.
</details>
<details>
<summary>摘要</summary>
我们显示了物理学 Informed Neural Networks (PINNs) 可以与最近发展的破碎点捕捉神经网络 (DCNNs) 结合，以解决具有界面和一些控制约束的最佳控制问题。这个算法是无网格的和可扩展的，并且保证控制约束的严格性。由于边界和界面条件，以及PDEs，都是软的约束，因此需要同时学习它们，并且没有保证边界和界面条件可以精确地满足。这会导致调整约束的预测条件和神经网络训练中的困难。为了解决这些困难并保证数值精度，我们提出了将边界和界面条件作为硬的约束在PINNs中，通过开发一种新的神经网络架构。这种硬约束PINNs方法可以保证边界和界面条件可以精确地满足，并且与PDEs的学习分离开来。我们在一些椭圆和带形interface最佳控制问题中调查了这种方法的效率，并证明了其可靠性。
</details></li>
</ul>
<hr>
<h2 id="Generating-observation-guided-ensembles-for-data-assimilation-with-denoising-diffusion-probabilistic-model"><a href="#Generating-observation-guided-ensembles-for-data-assimilation-with-denoising-diffusion-probabilistic-model" class="headerlink" title="Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model"></a>Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06708">http://arxiv.org/abs/2308.06708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yasahi-hpc/generative-enkf">https://github.com/yasahi-hpc/generative-enkf</a></li>
<li>paper_authors: Yuuichi Asahi, Yuta Hasegawa, Naoyuki Onodera, Takashi Shimokawabe, Hayato Shiba, Yasuhiro Idomura</li>
<li>for: 这 paper 用于 ensemble data assimilation，使用 pseudo ensemble 生成的 denoising diffusion  probabilistic model。</li>
<li>methods: 该方法使用模型对含杂和罕见观测数据进行训练，生成多个不同的 Ensemble，并利用这些 Ensemble 的差异来进行数据融合。</li>
<li>results: 比较 conventional ensemble data assimilation 方法，这种方法在模型不完善时显示出更好的性能。<details>
<summary>Abstract</summary>
This paper presents an ensemble data assimilation method using the pseudo ensembles generated by denoising diffusion probabilistic model. Since the model is trained against noisy and sparse observation data, this model can produce divergent ensembles close to observations. Thanks to the variance in generated ensembles, our proposed method displays better performance than the well-established ensemble data assimilation method when the simulation model is imperfect.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种ensemble数据融合方法，使用pseudo ensemble生成于噪声扩散概率模型。由于模型对噪声和稀缺观测数据进行训练，因此这个模型可以生成与观测数据相近的多个分布。由于这些生成的分布之间的差异，我们提议的方法在模型不完美时表现更好 than traditional ensemble数据融合方法。Note: "pseudo ensemble" in Chinese is "假集合" (fǎ jiéhù), and "denoising diffusion probabilistic model" in Chinese is "噪声扩散概率模型" (zāi shēng kuò shiān yù jì mó delè).
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-robustness-difference-between-stochastic-gradient-descent-and-adaptive-gradient-methods"><a href="#Understanding-the-robustness-difference-between-stochastic-gradient-descent-and-adaptive-gradient-methods" class="headerlink" title="Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods"></a>Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06703">http://arxiv.org/abs/2308.06703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avery Ma, Yangchen Pan, Amir-massoud Farahmand</li>
<li>for: 论述了使用权重更新法（SGD）和自适应梯度方法（Adam、RMSProp）训练深度神经网络的研究。</li>
<li>methods: 使用SGD和自适应梯度方法训练深度神经网络。</li>
<li>results: 对于自然数据集，SGD训练的模型对输入扰动 exhibit 较好的Robustness，而使用自适应梯度方法训练的模型则对于这些扰动 exhibit 较差的Robustness。这种差异可以通过学习动态研究和synthetic dataset的实验来解释。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam and RMSProp, have been widely used in training deep neural networks. We empirically show that while the difference between the standard generalization performance of models trained using these methods is small, those trained using SGD exhibit far greater robustness under input perturbations. Notably, our investigation demonstrates the presence of irrelevant frequencies in natural datasets, where alterations do not affect models' generalization performance. However, models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations. To better understand this difference, we study the learning dynamics of gradient descent (GD) and sign gradient descent (signGD) on a synthetic dataset that mirrors natural signals. With a three-dimensional input space, the models optimized with GD and signGD have standard risks close to zero but vary in their adversarial risks. Our result shows that linear models' robustness to $\ell_2$-norm bounded changes is inversely proportional to the model parameters' weight norm: a smaller weight norm implies better robustness. In the context of deep learning, our experiments show that SGD-trained neural networks show smaller Lipschitz constants, explaining the better robustness to input perturbations than those trained with adaptive gradient methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection"><a href="#Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection" class="headerlink" title="Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection"></a>Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06701">http://arxiv.org/abs/2308.06701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haichao Zhang, Can Qin, Yu Yin, Yun Fu</li>
<li>for: 提高深度学习模型对涂抹式对象检测的能力</li>
<li>methods: 使用生成模型生成涂抹式图像，以增强现有对象检测模型的识别能力</li>
<li>results: 比现有方法高效，在COD10k、CAMO和CHAMELEON三个数据集上达到了更高的检测精度<details>
<summary>Abstract</summary>
Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-and-play data generation and augmentation module for existing camouflaged object detection tasks and provides a novel way to introduce more diversity and distributions into current camouflage datasets.
</details>
<details>
<summary>摘要</summary>
伪装物体在自然场景中混合很困难对深度学习模型进行检测和生成。隐身物体检测是计算机视觉中重要的任务，它在各种实际应用中具有广泛的意义。然而，这一研究领域受到有限的数据可用性的限制。我们提出了一种框架，用于增强自然场景中隐身物体的检测。我们的方法使用生成模型生成真实的伪装图像，这些图像可以用来训练现有的物体检测模型。具体来说，我们使用一个伪装环境生成器，该生成器被监督于伪装分布分类器，以生成伪装图像。这些图像然后被我们的生成器扩展，以增加数据集。我们的框架在COD10k、CAMO和CHAMELEON三个数据集上表现出色，超越当前状态的方法，证明了我们的方法的有效性。这种方法可以作为现有隐身物体检测任务的数据生成和增强模块，并提供一种新的多样性和分布引入现有的伪装数据集的方法。
</details></li>
</ul>
<hr>
<h2 id="SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency"><a href="#SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency" class="headerlink" title="SimMatchV2: Semi-Supervised Learning with Graph Consistency"></a>SimMatchV2: Semi-Supervised Learning with Graph Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06692">http://arxiv.org/abs/2308.06692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mingkai-zheng/simmatchv2">https://github.com/mingkai-zheng/simmatchv2</a></li>
<li>paper_authors: Mingkai Zheng, Shan You, Lang Huang, Chen Luo, Fei Wang, Chen Qian, Chang Xu</li>
<li>for: 这个研究目的是为了提出一个新的半supervised learning算法，以减少人工劳动。</li>
<li>methods: 这个算法叫做SimMatchV2，它利用图论的观点来设计了多种一致规律，以确保labeled和unlabeled数据之间的一致性。</li>
<li>results: 这个算法在多个半supervised learningbenchmark上进行验证，以300次训练和ResNet-50底层，SimMatchV2在ImageNet上得到71.9%和76.2%的Top-1准确率，优于之前的方法，并达到了现有的最佳性能。<details>
<summary>Abstract</summary>
Semi-Supervised image classification is one of the most fundamental problem in computer vision, which significantly reduces the need for human labor. In this paper, we introduce a new semi-supervised learning algorithm - SimMatchV2, which formulates various consistency regularizations between labeled and unlabeled data from the graph perspective. In SimMatchV2, we regard the augmented view of a sample as a node, which consists of a label and its corresponding representation. Different nodes are connected with the edges, which are measured by the similarity of the node representations. Inspired by the message passing and node classification in graph theory, we propose four types of consistencies, namely 1) node-node consistency, 2) node-edge consistency, 3) edge-edge consistency, and 4) edge-node consistency. We also uncover that a simple feature normalization can reduce the gaps of the feature norm between different augmented views, significantly improving the performance of SimMatchV2. Our SimMatchV2 has been validated on multiple semi-supervised learning benchmarks. Notably, with ResNet-50 as our backbone and 300 epochs of training, SimMatchV2 achieves 71.9\% and 76.2\% Top-1 Accuracy with 1\% and 10\% labeled examples on ImageNet, which significantly outperforms the previous methods and achieves state-of-the-art performance. Code and pre-trained models are available at \href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2}.
</details>
<details>
<summary>摘要</summary>
《半指导Image Classification》是计算机视觉中的一个基本问题，它可以减少人工劳动量。在这篇论文中，我们介绍了一种新的半指导学习算法——SimMatchV2，它在图像视角下划定了不同类别的样本。在SimMatchV2中，我们将每个样本视为一个节点，每个节点有一个标签和对应的表示。不同的节点之间连接了边，边的 Similarity 度量节点表示之间的相似性。我们还提出了四种一致性，即1）节点-节点一致性，2）节点-边一致性，3）边-边一致性，4）边-节点一致性。我们还发现了一种简单的特征归一化可以减少不同扩展视图之间的特征范围差异，从而显著提高SimMatchV2的性能。我们的SimMatchV2在多个半指导学习标准benchmark上进行验证，与ResNet-50作为背景和300个训练周期，SimMatchV2在ImageNet上 achieve 71.9%和76.2%的Top-1准确率，与先前的方法相比显著超越，实现了状态的最佳性能。代码和预训练模型可以在 <https://github.com/mingkai-zheng/SimMatchV2> 中获取。
</details></li>
</ul>
<hr>
<h2 id="MDB-Interactively-Querying-Datasets-and-Models"><a href="#MDB-Interactively-Querying-Datasets-and-Models" class="headerlink" title="MDB: Interactively Querying Datasets and Models"></a>MDB: Interactively Querying Datasets and Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06686">http://arxiv.org/abs/2308.06686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaditya Naik, Adam Stein, Yinjun Wu, Eric Wong, Mayur Naik</li>
<li>for: 这篇论文是为了提供一个Debugging框架，帮助开发者在机器学习管道中系统地调试错误。</li>
<li>methods: 这篇论文使用了函数编程和关系代数来构建表达式查询数据集和模型预测。查询可重用和轻松修改，帮助调试员快速缩小查询错误和模型行为。</li>
<li>results:  experiments show that MDB可以提供更快（10倍）和更短（40%）的查询，并且在用户研究中，开发者可以成功构建复杂的查询来描述机器学习模型的错误。<details>
<summary>Abstract</summary>
As models are trained and deployed, developers need to be able to systematically debug errors that emerge in the machine learning pipeline. We present MDB, a debugging framework for interactively querying datasets and models. MDB integrates functional programming with relational algebra to build expressive queries over a database of datasets and model predictions. Queries are reusable and easily modified, enabling debuggers to rapidly iterate and refine queries to discover and characterize errors and model behaviors. We evaluate MDB on object detection, bias discovery, image classification, and data imputation tasks across self-driving videos, large language models, and medical records. Our experiments show that MDB enables up to 10x faster and 40\% shorter queries than other baselines. In a user study, we find developers can successfully construct complex queries that describe errors of machine learning models.
</details>
<details>
<summary>摘要</summary>
models 是在训练和部署过程中，开发人员需要系统地调试出现在机器学习管道中的错误。我们提出了 MDB，一个用于交互查询数据集和模型的调试框架。MDB将函数编程与关系代数结合，以构建表达式查询数据库中的数据集和模型预测。查询可重复使用，易于修改，让调试者可以快速灵活地 iteratively 修改查询，以描述和揭示错误和模型行为。我们在对自动驾驶视频、大语言模型和医疗记录进行对象检测、偏见发现、图像分类和数据补充任务上进行了实验，发现 MDB 可以提高查询速度和查询长度，相比于其他基eline。在用户研究中，我们发现开发人员可以成功地构建复杂的查询，以描述机器学习模型的错误。
</details></li>
</ul>
<hr>
<h2 id="Separable-Gaussian-Neural-Networks-Structure-Analysis-and-Function-Approximations"><a href="#Separable-Gaussian-Neural-Networks-Structure-Analysis-and-Function-Approximations" class="headerlink" title="Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations"></a>Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06679">http://arxiv.org/abs/2308.06679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Xing, Jianqiao Sun</li>
<li>For: 这个论文想要解决高维输入数据的快速 interpolate和分类问题，提出了一种新的前向网络模型 - 分解 Gaussian 神经网络（SGNN）。* Methods: SGNN 利用 Gaussian 函数的分解性，将输入数据分割成多列，然后在并行层中进行批量处理，从而将计算量从 GRBFNN 的 O(N^d) 减少到 O(dN)，速度增长 linear 地。* Results: 实验表明，SGNN 可以与 GRBFNN 相比，在 tri-variate 函数近似中实现 100 倍的速度提升，并且保持 GRBFNN 的级别准确性。 SGNN 还比 DNNs  WITH RuLU 和 Sigmoid 函数更易于训练和调整。 在approximating 函数 WITH complex geometry 时，SGNN 可以达到三个数量级更高的准确性。<details>
<summary>Abstract</summary>
The Gaussian-radial-basis function neural network (GRBFNN) has been a popular choice for interpolation and classification. However, it is computationally intensive when the dimension of the input vector is high. To address this issue, we propose a new feedforward network - Separable Gaussian Neural Network (SGNN) by taking advantage of the separable property of Gaussian functions, which splits input data into multiple columns and sequentially feeds them into parallel layers formed by uni-variate Gaussian functions. This structure reduces the number of neurons from O(N^d) of GRBFNN to O(dN), which exponentially improves the computational speed of SGNN and makes it scale linearly as the input dimension increases. In addition, SGNN can preserve the dominant subspace of the Hessian matrix of GRBFNN in gradient descent training, leading to a similar level of accuracy to GRBFNN. It is experimentally demonstrated that SGNN can achieve 100 times speedup with a similar level of accuracy over GRBFNN on tri-variate function approximations. The SGNN also has better trainability and is more tuning-friendly than DNNs with RuLU and Sigmoid functions. For approximating functions with complex geometry, SGNN can lead to three orders of magnitude more accurate results than a RuLU-DNN with twice the number of layers and the number of neurons per layer.
</details>
<details>
<summary>摘要</summary>
Gaussian-radial-basis函数神经网络（GRBFNN）已经是选择 interpolation和分类的受欢迎选择。然而，当输入向量维度高时，它会占用大量计算资源。为解决这个问题，我们提出了一个新的前向网络——分解 Gaussian 神经网络（SGNN），利用 Gaussian 函数的分解性，将输入数据分解成多列，然后将它们顺序输入到由单variate Gaussian 函数组成的并行层中。这种结构将 GRBFNN 中的 neuron 数由 O(N^d) 降低到 O(dN)，从而 exponential 提高 SGNN 的计算速度，使其与输入维度增加时呈线性增长。此外，SGNN 还可以保留 GRBFNN 的主要子空间，从而在梯度下降训练中达到类似精度水平。实验表明，SGNN 可以在 tri-variate 函数拟合中实现 100 倍的速度提升，同时保持精度水平。此外，SGNN 还比 DNNs  WITH RuLU 和 sigmoid 函数更易于训练和调整。对于拟合复杂几何函数的情况，SGNN 可以 achieve 三个排名的精度提升。
</details></li>
</ul>
<hr>
<h2 id="A-deep-learning-framework-for-multi-scale-models-based-on-physics-informed-neural-networks"><a href="#A-deep-learning-framework-for-multi-scale-models-based-on-physics-informed-neural-networks" class="headerlink" title="A deep learning framework for multi-scale models based on physics-informed neural networks"></a>A deep learning framework for multi-scale models based on physics-informed neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06672">http://arxiv.org/abs/2308.06672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Wang, Yanzhong Yao, Jiawei Guo, Zhiming Gao</li>
<li>for: 解决多级别问题（multi-scale problems）</li>
<li>methods: 基于深度神经网络（deep neural networks）和解决partial differential equations（PDEs）的physics-informed neural networks（PINN）方法</li>
<li>results: 提出了一种新的框架，可以同时优化多级别的损失项，并且可以处理不同子域的问题变化。<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINN) combine deep neural networks with the solution of partial differential equations (PDEs), creating a new and promising research area for numerically solving PDEs. Faced with a class of multi-scale problems that include loss terms of different orders of magnitude in the loss function, it is challenging for standard PINN methods to obtain an available prediction. In this paper, we propose a new framework for solving multi-scale problems by reconstructing the loss function. The framework is based on the standard PINN method, and it modifies the loss function of the standard PINN method by applying different numbers of power operations to the loss terms of different magnitudes, so that the individual loss terms composing the loss function have approximately the same order of magnitude among themselves. In addition, we give a grouping regularization strategy, and this strategy can deal well with the problem which varies significantly in different subdomains. The proposed method enables loss terms with different magnitudes to be optimized simultaneously, and it advances the application of PINN for multi-scale problems.
</details>
<details>
<summary>摘要</summary>
物理学 Informed neural networks (PINN) combine deep neural networks with partial differential equations (PDEs) 的解决方法，创造了一个新的研究领域，用于数值解决 PDEs。面临多个层次问题，其中loss函数中的损失项有不同的级别，标准的PINN方法难以获得可用的预测。在这篇论文中，我们提出了一种新的多层次问题解决框架。这种框架基于标准的PINN方法，对loss函数中的各个损失项应用不同的数量的power操作，使得各个损失项的级别相对较同。此外，我们提出了一种分组常见化策略，该策略可以处理不同子领域中变化很大的问题。提出的方法可以同时优化不同级别的损失项，并提高PINN在多层次问题上的应用。
</details></li>
</ul>
<hr>
<h2 id="Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent"><a href="#Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent" class="headerlink" title="Law of Balance and Stationary Distribution of Stochastic Gradient Descent"></a>Law of Balance and Stationary Distribution of Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06671">http://arxiv.org/abs/2308.06671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Ziyin, Hongchao Li, Masahito Ueda</li>
<li>for: This paper aims to understand how the stochastic gradient descent (SGD) algorithm navigates the highly nonlinear and degenerate loss landscape of a neural network.</li>
<li>methods: The paper uses theoretical analysis to prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry.</li>
<li>results: The paper derives the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width, and shows that the stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion, which are unique to deep networks.Here is the answer in Simplified Chinese text:</li>
<li>for: 这篇论文目标是理解权重梯度下降（SGD）算法在神经网络的高非线性和平衡梯度图像中的探索。</li>
<li>methods: 论文使用理论分析，证明SGD中批处理噪声对于包含扩缩尺度Symmetry的损失函数的解决方法。</li>
<li>results: 论文Derive diagonally linear network with arbitrary depth and width的stationary distribution of stochastic gradient flow，并显示其站立分布具有复杂非线性现象，如相转变、破碎Ergodicity和振荡反转，这些现象只存在于深度很大的网络中。<details>
<summary>Abstract</summary>
The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.
</details>
<details>
<summary>摘要</summary>
SGD算法是我们用来训练神经网络的算法，但是它在神经网络的高度非线性和缺乏稳定性的损失函数空间中 navigation 仍然不够了解。在这个工作中，我们证明了SGD中的小批量噪声规范化解决方案，当损失函数具有扩展对称性时。由于噪声和SGD动力学的差异最大化在对称性存在时，我们的理论 imply 损失函数对称性是SGD工作的重要探测器。我们然后使用这结果来 derive 神经网络的站点分布，并证明了深度神经网络存在复杂非线性现象，如相对稳定性、破坏性和异常倒振。这些现象仅存在深度神经网络中，表明深度和浅度模型之间存在根本的差异。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges"><a href="#Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges" class="headerlink" title="Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges"></a>Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06668">http://arxiv.org/abs/2308.06668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiajiali04/agriculture-foundation-models">https://github.com/jiajiali04/agriculture-foundation-models</a></li>
<li>paper_authors: Jiajia Li, Mingle Xu, Lirong Xiang, Dong Chen, Weichao Zhuang, Xunyuan Yin, Zhaojian Li</li>
<li>for: 本研究旨在探讨基于machine learning和deep learning的智能农业领域中的应用Foundation Model（FM）。</li>
<li>methods: 本研究首先对现代计算机科学领域的FM进行了综述，并将其分为四类：语言FM、视觉FM、多模态FM和奖励学习FM。然后，我们详细介绍了在农业领域开发农业FM的过程，并讨论了其在智能农业中的潜在应用。</li>
<li>results: 本研究通过引入基于FM的应用方法，可以减少农业AI系统的依赖于大量标注数据，提高效率、有效性和通用性。此外，本研究还提出了开发农业FM的一些挑战，包括模型训练、验证和部署。<details>
<summary>Abstract</summary>
The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field. To this end, we first review recent FMs in the general computer science domain and categorize them into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Subsequently, we outline the process of developing agriculture FMs and discuss their potential applications in smart agriculture. We also discuss the unique challenges associated with developing AFMs, including model training, validation, and deployment. Through this study, we contribute to the advancement of AI in agriculture by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems.
</details>
<details>
<summary>摘要</summary>
过去一代，机器学习（ML）和深度学习（DL）方法在农业系统中得到了迅速发展，在多种农业应用中显示出了很大成功。然而，这些传统的ML/DL模型具有一些限制：它们需要大量、昂贵的标签数据进行训练，需要专门的专业知识进行开发和维护，而且主要是为特定任务设计，因此缺乏普适性。在最近的几年里，基础模型（FM）在语言和视觉任务中获得了惊人的成功。这些模型通过大量的数据来自多个领域和模式进行训练，一旦训练完成，就可以完成多种任务，只需要微小的调整和微小的任务特定的标签数据。尽管它们的可效性和潜在的潜力很大，但在农业领域中还没有多少探索基础模型的应用。因此，本研究旨在探讨基础模型在智能农业领域的潜力。具体来说，我们首先将最近的FM在通用计算机科学领域中进行了综述，并将其分为四类：语言FM、视觉FM、多模式FM和奖励学习FM。然后，我们详细介绍了在农业领域开发农业FM的过程，并讨论了它们在智能农业中的潜在应用。我们还讨论了开发AFM的独特挑战，包括模型训练、验证和部署。通过本研究，我们为农业AI的发展做出了贡献，将基础模型作为一种可能的解决方案，可以减少农业AI系统的依赖于大量标签数据，提高效率、有效性和普适性。
</details></li>
</ul>
<hr>
<h2 id="ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN"><a href="#ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN" class="headerlink" title="ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN"></a>ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06663">http://arxiv.org/abs/2308.06663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abul Bashar, Richi Nayak</li>
<li>For: Anomaly detection in time series data, specifically in univariate and multivariate datasets in an unsupervised setting.* Methods: Proposes a new GAN model called Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection accuracy.* Results: Outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data, as demonstrated through experiments on 46 real-world univariate time series datasets and a large multivariate dataset.<details>
<summary>Abstract</summary>
Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
</details>
<details>
<summary>摘要</summary>
<<SYS>>时间序列数据中异常检测，以识别不同于常规行为的点，是多个领域中的一个常见问题，包括制造、医疗影像和网络安全等。最近，生成对抗网络（GANs）在时间序列数据中的异常检测中表现出色。GANs的神经网络架构（即生成器和识别器）可以显著提高异常检测精度。在本文中，我们提出了一种新的GAN模型，名为调整LSTM GAN（ALGAN），该模型可以在无监督的情况下，对单变量和多变量时间序列数据进行改进的异常检测。我们对46个真实的单变量时间序列数据集和多个领域的大量多变量数据集进行了试验，结果表明，ALGAN比传统的神经网络基于的方法、神经网络GAN方法和其他GAN方法在时间序列数据中的异常检测方面表现出色。Note: "LSTM" stands for Long Short-Term Memory, which is a type of Recurrent Neural Network (RNN) designed to handle time series data.
</details></li>
</ul>
<hr>
<h2 id="Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features"><a href="#Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features" class="headerlink" title="Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features"></a>Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08482">http://arxiv.org/abs/2308.08482</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiiizhang/shortcutDebiasing">https://github.com/yiiizhang/shortcutDebiasing</a></li>
<li>paper_authors: Yi Zhang, Jitao Sang, Junyang Wang, Dongmei Jiang, Yaowei Wang</li>
<li>for: 降低机器学习模型中的偏见风险，特别是在社会应用中，如雇用、银行和刑事司法等。</li>
<li>methods: 我们提出了一种简洁处理方法，称为“快捷偏见处理”（Shortcut Debiasing），它首先将偏见特征 transferred to快捷特征，然后使用 causal intervention 把快捷特征 eliminated during inference。</li>
<li>results: 我们将此方法应用到多个 benchmark 数据集上，并与现有的偏见处理方法进行比较，获得了显著的改善。<details>
<summary>Abstract</summary>
Machine learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Existing work tackles this issue by minimizing the employed information about social attributes in models for debiasing. However, the high correlation between target task and these social attributes makes learning on the target task incompatible with debiasing. Given that model bias arises due to the learning of bias features (\emph{i.e}., gender) that help target task optimization, we explore the following research question: \emph{Can we leverage shortcut features to replace the role of bias feature in target task optimization for debiasing?} To this end, we propose \emph{Shortcut Debiasing}, to first transfer the target task's learning of bias attributes from bias features to shortcut features, and then employ causal intervention to eliminate shortcut features during inference. The key idea of \emph{Shortcut Debiasing} is to design controllable shortcut features to on one hand replace bias features in contributing to the target task during the training stage, and on the other hand be easily removed by intervention during the inference stage. This guarantees the learning of the target task does not hinder the elimination of bias features. We apply \emph{Shortcut Debiasing} to several benchmark datasets, and achieve significant improvements over the state-of-the-art debiasing methods in both accuracy and fairness.
</details>
<details>
<summary>摘要</summary>
机器学习模型经常学习依赖敏感社会特征如性别和种族的预测，这会带来公平风险，特别是在社会应用中，如招聘、银行和刑事司法。现有的工作解决这个问题，是通过减少模型使用的社会特征来减少模型的偏见。然而，目标任务和社会特征之间的高相关性使得学习目标任务与减少偏见不兼容。基于模型偏见来自偏见特征（例如性别）的学习，我们提出了以下研究问题：“可以通过剪辑特征来替代偏见特征的角色来优化目标任务吗？”为此，我们提出了短Circuit Debiasing，即在训练阶段通过将目标任务学习的偏见特征转移到剪辑特征上，然后通过 causal intervention 在推理阶段消除剪辑特征。短Circuit Debiasing 的关键思想是设计可控的剪辑特征，以便在训练阶段替代偏见特征，并在推理阶段通过 intervention 轻松消除。这 garantizes 学习目标任务不会阻碍减少偏见。我们在多个标准数据集上应用短Circuit Debiasing，并在准确率和公平性两个方面获得了 significan 的改进。
</details></li>
</ul>
<hr>
<h2 id="Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks"><a href="#Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks" class="headerlink" title="Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks"></a>Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06654">http://arxiv.org/abs/2308.06654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</li>
<li>for: 预测行人轨迹是自动驾驶车辆安全导航中的关键能力，特别是在与行人共享空间时。行人运动在共享空间中受到车辆和其他行人的影响，因此可以更好地模型行人-车辆和行人-行人交互，从而提高行人轨迹预测模型的准确性。</li>
<li>methods: 我们提出了一种基于启发的交互代理选择过程，利用碰撞风险计算来选择交互代理。我们关注与可能碰撞的代理之间的时间到碰撞和接近方向的影响，并通过引入一种新的极地增量增量Grid Map来编码交互效果。</li>
<li>results: 我们的结果表明，使用我们提出的方法可以比基eline方法（作为参考）在HBS数据集上预测轨迹更加准确。<details>
<summary>Abstract</summary>
Predicting pedestrians' trajectories is a crucial capability for autonomous vehicles' safe navigation, especially in spaces shared with pedestrians. Pedestrian motion in shared spaces is influenced by both the presence of vehicles and other pedestrians. Therefore, effectively modelling both pedestrian-pedestrian and pedestrian-vehicle interactions can increase the accuracy of the pedestrian trajectory prediction models. Despite the huge literature on ways to encode the effect of interacting agents on a pedestrian's predicted trajectory using deep-learning models, limited effort has been put into the effective selection of interacting agents. In the majority of cases, the interaction features used are mainly based on relative distances while paying less attention to the effect of the velocity and approaching direction in the interaction formulation. In this paper, we propose a heuristic-based process of selecting the interacting agents based on collision risk calculation. Focusing on interactions of potentially colliding agents with a target pedestrian, we propose the use of time-to-collision and the approach direction angle of two agents for encoding the interaction effect. This is done by introducing a novel polar collision grid map. Our results have shown predicted trajectories closer to the ground truth compared to existing methods (used as a baseline) on the HBS dataset.
</details>
<details>
<summary>摘要</summary>
预测行人轨迹是自动驾驶车辆安全导航中的关键能力，特别是在与行人共享空间时。行人运动在共享空间中受到车辆和其他行人的影响。因此，可以准确地模拟行人与车辆和其他行人之间的互动，可以提高行人轨迹预测模型的准确性。Despite the extensive literature on using deep-learning models to encode the effect of interacting agents on a pedestrian's predicted trajectory, there has been limited effort put into selecting the interacting agents effectively. Most existing methods use relative distance as the main factor in the interaction formulation, while ignoring the effect of velocity and approaching direction.在这篇论文中，我们提出了一种基于启发的互动代理选择过程，通过计算碰撞风险来选择互动代理。我们将注意力集中在可能碰撞的代理与目标行人之间的互动效应上，并通过引入一种新的极地碰撞格图来编码这种互动效应。我们的结果表明，与基eline方法相比，我们的方法可以在HBS数据集上预测轨迹更加准确。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation"><a href="#Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation" class="headerlink" title="Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation"></a>Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06644">http://arxiv.org/abs/2308.06644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation">https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation</a></li>
<li>paper_authors: Junwei Huang, Zhiqing Sun, Yiming Yang</li>
<li>for: 提高 NP-完全 combinatorial 优化问题的解决速度</li>
<li>methods: 使用进步干涤法加速推理，在杂化过程中采取 fewer steps，如在单步内预测两步</li>
<li>results: 实验结果显示，使用进步干涤模型可以将推理速度提高 16 倍，而性能下降仅 0.019%，在 TSP-50 数据集上。<details>
<summary>Abstract</summary>
Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
</details>
<details>
<summary>摘要</summary>
GRaph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.Here's the translation in Traditional Chinese: GRaph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
</details></li>
</ul>
<hr>
<h2 id="Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition"><a href="#Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition" class="headerlink" title="Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition"></a>Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11633">http://arxiv.org/abs/2308.11633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Sheffield, Frank E. Bobe III, Bradley Marchand, Matthew S. Emigh</li>
<li>for: 本研究旨在提高水下探索中SAS数据处理、分类和 Pattern recognition的效果，通过使用自助学习（SSL）技术。</li>
<li>methods: 本研究提出了MoCo-SAS，一种基于SSL的SAS数据处理方法，包括数据预处理、特征提取、模型训练和测试。</li>
<li>results: 实验结果表明，MoCo-SAS与传统的指导学习方法相比，在F1分数上有显著提高，表明SSL可以在SAS数据处理中提高效果，并且具有潜在的应用前景。<details>
<summary>Abstract</summary>
Synthetic Aperture Sonar (SAS) imaging has become a crucial technology for underwater exploration because of its unique ability to maintain resolution at increasing ranges, a characteristic absent in conventional sonar techniques. However, the effective application of deep learning to SAS data processing is often limited due to the scarcity of labeled data. To address this challenge, this paper proposes MoCo-SAS that leverages self-supervised learning (SSL) for SAS data processing, classification, and pattern recognition. The experimental results demonstrate that MoCo-SAS significantly outperforms traditional supervised learning methods, as evidenced by significant improvements observed in terms of the F1-score. These findings highlight the potential of SSL in advancing the state-of-the-art in SAS data processing, offering promising avenues for enhanced underwater object detection and classification.
</details>
<details>
<summary>摘要</summary>
射频成像技术（SAS）已成为水下探测中不可或缺的一种重要技术，因其可以维持分辨率随距离增长，这是传统声纳技术缺乏的特点。然而，各种深度学习在SAS数据处理中的有效应用却受到标注数据的罕见性的限制。为解决这个挑战，本文提出了MoCo-SAS，利用自动编程学习（SSL）进行SAS数据处理、分类和模式识别。实验结果表明，MoCo-SAS在F1分数方面显著超越传统监督学习方法，这表明SSL在SAS数据处理中具有潜在的潜在优势。这些发现表明SSL在SAS数据处理中可能提供新的突破口，用于提高水下对象检测和分类的精度。
</details></li>
</ul>
<hr>
<h2 id="ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss"><a href="#ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss" class="headerlink" title="ADRMX: Additive Disentanglement of Domain Features with Remix Loss"></a>ADRMX: Additive Disentanglement of Domain Features with Remix Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06624">http://arxiv.org/abs/2308.06624</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berkerdemirel/ADRMX">https://github.com/berkerdemirel/ADRMX</a></li>
<li>paper_authors: Berker Demirel, Erchan Aptoula, Huseyin Ozkan</li>
<li>for: 这个研究旨在创建能够在新不同预设范围中具有普遍化能力的模型，以减少因为不同预设范围之间的分布变化对模型的影响。</li>
<li>methods: 这个研究使用了一种名为“Additive Disentanglement of Domain Features with Remix Loss”的新架构，并 introduce了一种新的数据增强技术，将不同预设范围中的数据混合在潜在空间中。</li>
<li>results: 这个研究透过对DomainBed进行了EXTENSIVE的实验，展示了ADRMX可以实现现场的表现，并且比以前的研究得到更好的结果。<details>
<summary>Abstract</summary>
The common assumption that train and test sets follow similar distributions is often violated in deployment settings. Given multiple source domains, domain generalization aims to create robust models capable of generalizing to new unseen domains. To this end, most of existing studies focus on extracting domain invariant features across the available source domains in order to mitigate the effects of inter-domain distributional changes. However, this approach may limit the model's generalization capacity by relying solely on finding common features among the source domains. It overlooks the potential presence of domain-specific characteristics that could be prevalent in a subset of domains, potentially containing valuable information. In this work, a novel architecture named Additive Disentanglement of Domain Features with Remix Loss (ADRMX) is presented, which addresses this limitation by incorporating domain variant features together with the domain invariant ones using an original additive disentanglement strategy. Moreover, a new data augmentation technique is introduced to further support the generalization capacity of ADRMX, where samples from different domains are mixed within the latent space. Through extensive experiments conducted on DomainBed under fair conditions, ADRMX is shown to achieve state-of-the-art performance. Code will be made available at GitHub after the revision process.
</details>
<details>
<summary>摘要</summary>
通常的假设是训练集和测试集都follow相似的分布是在部署设置中常常被违反。给定多个源领域，领域泛化目标是创建抗衰假设模型，以便在新未经见过的领域中泛化。为此，大多数现有的研究都是EXTRACTING DOMAIN INVARIANT FEATURES ACROSS AVAILABLE SOURCE DOMAINS，以mitigate INTER-DOMAIN distributional changes的影响。然而，这种方法可能会限制模型的泛化能力，因为它只是在 source domains 中找到共同特征。它忽略了可能存在一些领域特有的特征，这些特征可能在一些领域中具有价值信息。在这项工作中，一种新的架构名为 Additive Disentanglement of Domain Features with Remix Loss (ADRMX) 被提出，它解决了这种限制，通过将领域特征和领域 invariants 相加拼接在一起。此外，一种新的数据增强技术也被引入，用于进一步支持 ADRMX 的泛化能力，其中不同领域的样本在离散空间中混合。通过对 DomainBed 进行了广泛的实验，ADRMX 在 fair 的条件下显示出了状态的表现。代码将在 GitHub 上提供。
</details></li>
</ul>
<hr>
<h2 id="Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks"><a href="#Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks" class="headerlink" title="Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?"></a>Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06619">http://arxiv.org/abs/2308.06619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Liao, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione</li>
<li>for: 降低深度神经网络大小 while maintaining performance</li>
<li>methods: 基于Entropy Guided Pruning算法，优先遍历层次 entropy 低的连接，进行完全移除</li>
<li>results: 成功地压缩深度神经网络，保持竞争力水平，并提供了关于不结构压缩的机制和深度学习性能之间的新的视角。<details>
<summary>Abstract</summary>
Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relationship between entropy, pruning techniques, and deep learning performance. The EGP algorithm and its insights hold great promise for advancing the field of network compression and optimization. The source code for EGP is released open-source.
</details>
<details>
<summary>摘要</summary>
剪辑是一种广泛使用的技术，用于降低深度神经网络的大小，保持性能。然而，这种技术，即使可以压缩深度模型，几乎不能完全移除层（即使是结构化的）：是这个任务可行吗？在这项研究中，我们介绍了EGP算法，一种创新的熵导向剪辑算法，用于减少深度神经网络的大小，保持性能。EGP的关键点在于优先剪辑层中的熵低的连接，以便完全移除它们。我们在popular模型如ResNet-18和Swin-T等模型上进行了广泛的实验，发现EGP有效地减少深度神经网络的大小，保持竞争力水平。我们的研究不仅解释了不结构化剪辑的优势，还为深度学习性能和剪辑技术之间的复杂关系开辟了新的可能性。EGP算法和其洞见拥有很大的潜力，可以推动深度神经网络压缩和优化领域的进步。EGP算法的源代码已经开源。
</details></li>
</ul>
<hr>
<h2 id="On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness"><a href="#On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness" class="headerlink" title="On the Interplay of Convolutional Padding and Adversarial Robustness"></a>On the Interplay of Convolutional Padding and Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06612">http://arxiv.org/abs/2308.06612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Gavrikov, Janis Keuper</li>
<li>for: 本文旨在研究padding和敌意攻击之间的交互关系，以及不同padding模式对敌意Robustness的影响。</li>
<li>methods: 本文使用Convolutional Neural Networks (CNN)进行研究，并对不同padding模式进行比较。</li>
<li>results: 本文发现，敌意攻击通常会导致图像边界上的异常，这些异常与padding有关。此外，本文还发现不同padding模式对敌意Robustness的影响不同。<details>
<summary>Abstract</summary>
It is common practice to apply padding prior to convolution operations to preserve the resolution of feature-maps in Convolutional Neural Networks (CNN). While many alternatives exist, this is often achieved by adding a border of zeros around the inputs. In this work, we show that adversarial attacks often result in perturbation anomalies at the image boundaries, which are the areas where padding is used. Consequently, we aim to provide an analysis of the interplay between padding and adversarial attacks and seek an answer to the question of how different padding modes (or their absence) affect adversarial robustness in various scenarios.
</details>
<details>
<summary>摘要</summary>
通常来说，在卷积神经网络（CNN）中， pading 被用来保持特征地图的分辨率。虽然有很多方法可供选择，但通常是通过在输入添加一个边界的零值来实现。在这项工作中，我们发现了一个现象：攻击者经常在图像边界处引起异常的杂变，这些区域 precisly 是在 padding 中使用的地方。因此，我们想进行 padding 和攻击者之间的分析，并问到不同的 padding 模式（或其缺失）对于不同的场景中的鲁棒性有什么影响。
</details></li>
</ul>
<hr>
<h2 id="LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net"><a href="#LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net" class="headerlink" title="LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net"></a>LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06603">http://arxiv.org/abs/2308.06603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ach-1914/ladlenet">https://github.com/ach-1914/ladlenet</a></li>
<li>paper_authors: Tonghui Zou</li>
<li>for: 该 paper 的目的是提出一种基于 U-Net 架构的算法，用于将thermal infrared（TIR）图像转换为可见光（VI）图像，以满足不同领域的应用需求。</li>
<li>methods: 该算法使用了两个阶段的 U-Net  concatenation结构，以及缺省连接和精细特征聚合技术，从而提高模型性能。该算法包括 ‘Handle’ 模块和 ‘Bowl’ 模块，其中 ‘Handle’ 模块建立了一个抽象的 semantic space，而 ‘Bowl’ 模块将该 semantic space 转换为封装的 VI 图像。</li>
<li>results:  comparing to existing methodologies, 该方法在 KAIST 数据集上测试得到了最佳性能，包括图像清晰度和感知质量。<details>
<summary>Abstract</summary>
The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibility by allowing the substitution of its network architecture with semantic segmentation networks, thereby establishing more abstract semantic spaces to bolster model performance. Consequently, we propose LadleNet+, which replaces LadleNet's Handle module with the pre-trained DeepLabv3+ network, thereby endowing the model with enhanced semantic space construction capabilities. The proposed method is evaluated and tested on the KAIST dataset, accompanied by quantitative and qualitative analyses. Compared to existing methodologies, our approach achieves state-of-the-art performance in terms of image clarity and perceptual quality. The source code will be made available at https://github.com/Ach-1914/LadleNet/tree/main/.
</details>
<details>
<summary>摘要</summary>
文本翻译：thermal infrared（TIR）图像到可见光（VI）图像的翻译问题具有广泛的应用领域，如TIR-VI图像匹配和融合。利用TIR图像的补充信息可以大幅提高模型性能和泛化性。然而，现有的问题包括图像质量不佳和模型缺乏扩展性。本文介绍一种算法，叫做LadleNet，基于U-Net架构。LadleNet使用了两个阶段的U-Net concatenation结构，加上了跳过连接和细化特征聚合技术，从而实现了显著提高模型性能。LadleNet包括“ Handle”和“Bowl”模块，其中“ Handle”模块建立了一个抽象的语义空间，而“Bowl”模块将这个语义空间转换成VI图像。“ Handle”模块具有扩展性，可以将其网络架构替换为语义分割网络，从而建立更加抽象的语义空间，提高模型性能。因此，我们提出了LadleNet+，其替换了LadleNet的“ Handle”模块，使用了预训练的DeepLabv3+网络，从而为模型增加了更多的语义空间建构能力。我们的方法在KAIST数据集上进行了评估和测试，并进行了量化和质量分析。与现有方法相比，我们的方法在图像清晰度和感知质量方面达到了国际前ier的性能。代码将在https://github.com/Ach-1914/LadleNet/tree/main/中提供。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/cs.LG_2023_08_13/" data-id="clohum99600mppj881b3r2ltr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/54/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/53/">53</a><a class="page-number" href="/page/54/">54</a><span class="page-number current">55</span><a class="page-number" href="/page/56/">56</a><a class="page-number" href="/page/57/">57</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/56/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
