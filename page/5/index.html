
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/5/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/cs.LG_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T10:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/01/cs.LG_2023_09_01/">cs.LG - 2023-09-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-RLHF-Reducing-the-Memory-Usage-of-PPO"><a href="#Efficient-RLHF-Reducing-the-Memory-Usage-of-PPO" class="headerlink" title="Efficient RLHF: Reducing the Memory Usage of PPO"></a>Efficient RLHF: Reducing the Memory Usage of PPO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00754">http://arxiv.org/abs/2309.00754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, Yelong Shen</li>
<li>for: 本研究旨在使用增强学习（RL）和人类反馈（HF）对语言模型进行训练，以实现模型与人类偏好的Alignment。</li>
<li>methods: 本研究使用的方法包括Proximal Policy Optimization（PPO）和Supervised Fine-Tuning（SFT），以及一种名为LoRA的内存减少技术。</li>
<li>results: 本研究的实验结果显示，使用LoRA技术可以在PPO中减少内存使用量，同时提高模型的Alignment性能。此外，Hydra-PPO方法可以减少LoRA-PPO的延迟时间，保持模型的性能。<details>
<summary>Abstract</summary>
Reinforcement Learning with Human Feedback (RLHF) has revolutionized language modeling by aligning models with human preferences. However, the RL stage, Proximal Policy Optimization (PPO), requires over 3x the memory of Supervised Fine-Tuning (SFT), making it infeasible to use for most practitioners. To address this issue, we present a comprehensive analysis the memory usage, performance, and training time of memory-savings techniques for PPO. We introduce Hydra-RLHF by first integrating the SFT and Reward models and then dynamically turning LoRA "off" during training. Our experiments show: 1. Using LoRA during PPO reduces its memory usage to be smaller than SFT while improving alignment across four public benchmarks, and 2. Hydra-PPO reduces the latency per sample of LoRA-PPO by up to 65% while maintaining its performance. Our results demonstrate that Hydra-PPO is a simple and promising solution for enabling more widespread usage of RLHF.
</details>
<details>
<summary>摘要</summary>
利用人类反馈学习（RLHF）改变语言模型的方法，使模型与人类偏好更加一致。然而，RL阶段的Proximal Policy Optimization（PPO）需要大约3倍于精度微调（SFT）的内存，这使得大多数实践者无法使用。为解决这个问题，我们提供了一项全面的内存使用量分析、性能和训练时间分析，以及内存减少技术的研究。我们首先将SFT和奖励模型集成，然后在训练中动态关闭LoRA。我们的实验结果显示：1、在使用LoRA时，PPO的内存使用量比SFT更小，同时在四个公共benchmark上提高了对齐性；2、Hydra-PPO可以将LoRA-PPO的延迟时间减少到65%以下，保持其性能。我们的结果表明，Hydra-PPO是一种简单有前途的解决方案，使得更多的实践者能够使用RLHF。
</details></li>
</ul>
<hr>
<h2 id="PathLDM-Text-conditioned-Latent-Diffusion-Model-for-Histopathology"><a href="#PathLDM-Text-conditioned-Latent-Diffusion-Model-for-Histopathology" class="headerlink" title="PathLDM: Text conditioned Latent Diffusion Model for Histopathology"></a>PathLDM: Text conditioned Latent Diffusion Model for Histopathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00748">http://arxiv.org/abs/2309.00748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srikar Yellapragada, Alexandros Graikos, Prateek Prasanna, Tahsin Kurc, Joel Saltz, Dimitris Samaras</li>
<li>for: 这个论文是为了开发一种基于文本指导的高质量 histopathology 图像生成模型。</li>
<li>methods: 该论文使用了 Latent Diffusion Model，并通过与文本报告的联合处理来提高生成过程。</li>
<li>results: 该论文在 TCGA-BRCA 数据集上实现了 State-of-the-Art FID 分数为 7.64，与最接近的文本指导竞争对手的 FID 分数相比，有所显著的提高。<details>
<summary>Abstract</summary>
To achieve high-quality results, diffusion models must be trained on large datasets. This can be notably prohibitive for models in specialized domains, such as computational pathology. Conditioning on labeled data is known to help in data-efficient model training. Therefore, histopathology reports, which are rich in valuable clinical information, are an ideal choice as guidance for a histopathology generative model. In this paper, we introduce PathLDM, the first text-conditioned Latent Diffusion Model tailored for generating high-quality histopathology images. Leveraging the rich contextual information provided by pathology text reports, our approach fuses image and textual data to enhance the generation process. By utilizing GPT's capabilities to distill and summarize complex text reports, we establish an effective conditioning mechanism. Through strategic conditioning and necessary architectural enhancements, we achieved a SoTA FID score of 7.64 for text-to-image generation on the TCGA-BRCA dataset, significantly outperforming the closest text-conditioned competitor with FID 30.1.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Universal-Normalization-Enhanced-Graph-Representation-Learning-for-Gene-Network-Prediction"><a href="#Universal-Normalization-Enhanced-Graph-Representation-Learning-for-Gene-Network-Prediction" class="headerlink" title="Universal Normalization Enhanced Graph Representation Learning for Gene Network Prediction"></a>Universal Normalization Enhanced Graph Representation Learning for Gene Network Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00738">http://arxiv.org/abs/2309.00738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehao Dong, Muhan Zhang, Qihang Zhao, Philip R. O. Payne, Michael Province, Carlos Cruchaga, Tianyu Zhao, Yixin Chen, Fuhai Li</li>
<li>for: 这篇论文的目的是提出一种基于Universal Graph Normalization的GNN表示学习方法，以提高GNN在生物信息学中预测和理解蛋白质表达 Profiling和疾病现象的表达能力。</li>
<li>methods: 这篇论文使用了Universal Graph Normalization的概念，在GNN的消息传递阶段和读取层中应用了universal graph normalization，以提高GNN的表达能力和稳定性。</li>
<li>results: 实验结果表明，Compared with前一代基elines,UNGNN模型在基因网络相关的生物信息学任务中显著提高了表达能力，在 average 上提高了16%的表达能力。此外，在其他可解决的图数据集上，UNGNN也 consistently 实现了最佳表现。<details>
<summary>Abstract</summary>
Effective gene network representation learning is of great importance in bioinformatics to predict/understand the relation of gene profiles and disease phenotypes. Though graph neural networks (GNNs) have been the dominant architecture for analyzing various graph-structured data like social networks, their predicting on gene networks often exhibits subpar performance. In this paper, we formally investigate the gene network representation learning problem and characterize a notion of \textit{universal graph normalization}, where graph normalization can be applied in an universal manner to maximize the expressive power of GNNs while maintaining the stability. We propose a novel UNGNN (Universal Normalized GNN) framework, which leverages universal graph normalization in both the message passing phase and readout layer to enhance the performance of a base GNN. UNGNN has a plug-and-play property and can be combined with any GNN backbone in practice. A comprehensive set of experiments on gene-network-based bioinformatical tasks demonstrates that our UNGNN model significantly outperforms popular GNN benchmarks and provides an overall performance improvement of 16 $\%$ on average compared to previous state-of-the-art (SOTA) baselines. Furthermore, we also evaluate our theoretical findings on other graph datasets where the universal graph normalization is solvable, and we observe that UNGNN consistently achieves the superior performance.
</details>
<details>
<summary>摘要</summary>
Effective gene network representation learning is of great importance in bioinformatics to predict/understand the relation of gene profiles and disease phenotypes. Although graph neural networks (GNNs) have been the dominant architecture for analyzing various graph-structured data like social networks, their predicting on gene networks often exhibits subpar performance. In this paper, we formally investigate the gene network representation learning problem and characterize a notion of “universal graph normalization”, where graph normalization can be applied in an universal manner to maximize the expressive power of GNNs while maintaining the stability. We propose a novel UNGNN (Universal Normalized GNN) framework, which leverages universal graph normalization in both the message passing phase and readout layer to enhance the performance of a base GNN. UNGNN has a plug-and-play property and can be combined with any GNN backbone in practice. A comprehensive set of experiments on gene-network-based bioinformatical tasks demonstrates that our UNGNN model significantly outperforms popular GNN benchmarks and provides an overall performance improvement of 16% on average compared to previous state-of-the-art (SOTA) baselines. Furthermore, we also evaluate our theoretical findings on other graph datasets where the universal graph normalization is solvable, and we observe that UNGNN consistently achieves the superior performance.
</details></li>
</ul>
<hr>
<h2 id="Prediction-Error-Estimation-in-Random-Forests"><a href="#Prediction-Error-Estimation-in-Random-Forests" class="headerlink" title="Prediction Error Estimation in Random Forests"></a>Prediction Error Estimation in Random Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00736">http://arxiv.org/abs/2309.00736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iankrupkin/Prediction-Error-Estimation-in-Random-Forests">https://github.com/iankrupkin/Prediction-Error-Estimation-in-Random-Forests</a></li>
<li>paper_authors: Ian Krupkin, Johanna Hardin</li>
<li>for: 这篇论文是为了评估随机森林的分类错误估计而写的。</li>
<li>methods: 论文使用了多种常见的随机森林错误估计方法，包括交叉验证、袋包和数据分割。</li>
<li>results: 研究发现，随机森林的错误估计比true error rate更接近，而不是average prediction error。这与Bates et al. (2023)的结论相反，该结论是为Logistic regression提供的。此外，这个结论在不同的错误估计策略下都保持一致。<details>
<summary>Abstract</summary>
In this paper, error estimates of classification Random Forests are quantitatively assessed. Based on the initial theoretical framework built by Bates et al. (2023), the true error rate and expected error rate are theoretically and empirically investigated in the context of a variety of error estimation methods common to Random Forests. We show that in the classification case, Random Forests' estimates of prediction error is closer on average to the true error rate instead of the average prediction error. This is opposite the findings of Bates et al. (2023) which were given for logistic regression. We further show that this result holds across different error estimation strategies such as cross-validation, bagging, and data splitting.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，Random Forests 分类预测错误估计的量化评估。基于鲍特斯等人（2023）提出的初始理论框架，我们对Random Forests 中的预测错误估计进行了理论和实验研究。我们发现在分类情况下，Random Forests 的预测错误估计比true error rate更加接近，而不是average prediction error。这与鲍特斯等人（2023）对Logistic Regression的发现相反。我们还发现这个结果适用于不同的预测错误估计策略，如cross-validation、bagging和数据分割。
</details></li>
</ul>
<hr>
<h2 id="Learned-Visual-Features-to-Textual-Explanations"><a href="#Learned-Visual-Features-to-Textual-Explanations" class="headerlink" title="Learned Visual Features to Textual Explanations"></a>Learned Visual Features to Textual Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00733">http://arxiv.org/abs/2309.00733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeid Asgari Taghanaki, Aliasghar Khani, Amir Khasahmadi, Aditya Sanghi, Karl D. D. Willis, Ali Mahdavi-Amiri</li>
<li>for: 解释机器学习模型中学习的特征，提高模型的可解释性和可靠性。</li>
<li>methods: 使用大型自然语言模型（LLMs）来解释预训练的图像分类器中学习的特征。提出TExplain方法，通过训练一个神经网络来建立图像分类器的特征空间和LLMs之间的连接。在推理过程中，生成大量的句子来解释给定图像中分类器学习的特征。</li>
<li>results: 通过实验证明TExplain方法可以提高图像分类器的可解释性和可靠性。在ImageNet-9L和Waterbirds等多个数据集上进行了实验，结果表明TExplain方法可以增强图像分类器的决策过程的理解和可靠性。<details>
<summary>Abstract</summary>
Interpreting the learned features of vision models has posed a longstanding challenge in the field of machine learning. To address this issue, we propose a novel method that leverages the capabilities of large language models (LLMs) to interpret the learned features of pre-trained image classifiers. Our method, called TExplain, tackles this task by training a neural network to establish a connection between the feature space of image classifiers and LLMs. Then, during inference, our approach generates a vast number of sentences to explain the features learned by the classifier for a given image. These sentences are then used to extract the most frequent words, providing a comprehensive understanding of the learned features and patterns within the classifier. Our method, for the first time, utilizes these frequent words corresponding to a visual representation to provide insights into the decision-making process of the independently trained classifier, enabling the detection of spurious correlations, biases, and a deeper comprehension of its behavior. To validate the effectiveness of our approach, we conduct experiments on diverse datasets, including ImageNet-9L and Waterbirds. The results demonstrate the potential of our method to enhance the interpretability and robustness of image classifiers.
</details>
<details>
<summary>摘要</summary>
traditional Chinese:过去，解释机器学习模型中学习的特征是一个长期的挑战。为解决这个问题，我们提出了一种新的方法，它利用大型自然语言模型（LLMs）来解释预训ImageNet的特征。我们的方法，called TExplain，通过将Feature空间的Connection between image classifiers and LLMs的 neural network training。然后，在推断过程中，我们的方法会生成大量的句子，以解释预训ImageNet的特征。这些句子中的最常见的字会提供一个全面的理解特征和模型的决策过程中的几何 pattern。我们的方法可以检测模型学习的偏见和不确定性，并将其转换为具体的几何 Representation，从而提高模型的解释性和可靠性。为验证我们的方法的有效性，我们在多个dataset上进行了实验，包括ImageNet-9L和Waterbirds。结果显示了我们的方法可以增强模型的解释性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Tempestas-ex-machina-A-review-of-machine-learning-methods-for-wavefront-control"><a href="#Tempestas-ex-machina-A-review-of-machine-learning-methods-for-wavefront-control" class="headerlink" title="Tempestas ex machina: A review of machine learning methods for wavefront control"></a>Tempestas ex machina: A review of machine learning methods for wavefront control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00730">http://arxiv.org/abs/2309.00730</a></li>
<li>repo_url: None</li>
<li>paper_authors: J. Fowler, Rico Landman</li>
<li>For: The paper is written for the purpose of exploring the use of machine learning methods in wavefront control for adaptive optics systems, specifically for the task of imaging rocky Earth-like planets.* Methods: The paper reviews and discusses various machine learning methods that have been applied to wavefront control, including linear and nonlinear methods, as well as their advantages and limitations.* Results: The paper presents a review of the literature on novel machine learning approaches to wavefront control, highlighting the potential benefits of using machine learning methods in adaptive optics systems for high-contrast imaging.<details>
<summary>Abstract</summary>
As we look to the next generation of adaptive optics systems, now is the time to develop and explore the technologies that will allow us to image rocky Earth-like planets; wavefront control algorithms are not only a crucial component of these systems, but can benefit our adaptive optics systems without requiring increased detector speed and sensitivity or more effective and efficient deformable mirrors. To date, most observatories run the workhorse of their wavefront control as a classic integral controller, which estimates a correction from wavefront sensor residuals, and attempts to apply that correction as fast as possible in closed-loop. An integrator of this nature fails to address temporal lag errors that evolve over scales faster than the correction time, as well as vibrations or dynamic errors within the system that are not encapsulated in the wavefront sensor residuals; these errors impact high contrast imaging systems with complex coronagraphs. With the rise in popularity of machine learning, many are investigating applying modern machine learning methods to wavefront control. Furthermore, many linear implementations of machine learning methods (under varying aliases) have been in development for wavefront control for the last 30-odd years. With this work we define machine learning in its simplest terms, explore the most common machine learning methods applied in the context of this problem, and present a review of the literature concerning novel machine learning approaches to wavefront control.
</details>
<details>
<summary>摘要</summary>
As we look to the next generation of adaptive optics systems, now is the time to develop and explore the technologies that will allow us to image rocky Earth-like planets; wavefront control algorithms are not only a crucial component of these systems, but can benefit our adaptive optics systems without requiring increased detector speed and sensitivity or more effective and efficient deformable mirrors. To date, most observatories run the workhorse of their wavefront control as a classic integral controller, which estimates a correction from wavefront sensor residuals, and attempts to apply that correction as fast as possible in closed-loop. An integrator of this nature fails to address temporal lag errors that evolve over scales faster than the correction time, as well as vibrations or dynamic errors within the system that are not encapsulated in the wavefront sensor residuals; these errors impact high contrast imaging systems with complex coronagraphs. With the rise in popularity of machine learning, many are investigating applying modern machine learning methods to wavefront control. Furthermore, many linear implementations of machine learning methods (under varying aliases) have been in development for wavefront control for the last 30-odd years. With this work we define machine learning in its simplest terms, explore the most common machine learning methods applied in the context of this problem, and present a review of the literature concerning novel machine learning approaches to wavefront control.Here's the translation in Traditional Chinese:当我们看向下一代适应光学系统时，现在是时候开发和探索这些系统可以对地球型行星进行图像；波前控制算法不仅是这些系统的重要组件，而且可以帮助我们的适应光学系统不需要更高的探测器速度和敏感度或更有效率的可变镜。到目前为止，大多数观测站运行的波前控制工作horse是一个适度控制器，它从波前感应器余类中估算修正，并将这个修正在关闭模式下最快地应用。这种优化器无法处理在时间尺度上比修正时间更快的慢滞迟错误，以及观测站内的震动或动力错误，这些错误对高比对图像系统中的复杂对焦镜产生影响。随着机器学习的普及，许多人正在探索将现代机器学习方法应用到波前控制。此外，过去30年来，一些线性的机器学习方法（以不同的名称出现）已经在波前控制方面进行开发。在这个工作中，我们定义机器学习在最简单的形式，探讨适用于这个问题的最常用机器学习方法，并发布关于新的机器学习控制方法的文献综述。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-in-medical-image-registration-introduction-and-survey"><a href="#Deep-learning-in-medical-image-registration-introduction-and-survey" class="headerlink" title="Deep learning in medical image registration: introduction and survey"></a>Deep learning in medical image registration: introduction and survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00727">http://arxiv.org/abs/2309.00727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Hammoudeh, Stéphane Dupont</li>
<li>for: 本文旨在介绍图像 регистраción（IR）技术，以帮助医疗专业人员在标准化参照框架中评估多种医疗图像。</li>
<li>methods: 本文使用了多种图像变换，包括Affine变换、可变形变换、可逆变换和双向变换，以及医疗图像注册算法如Voxelmorph、Demons、SynthMorph等。此外，文献还涵盖了各种图像注册技术，如atlas-based注册、多stage注册和Pyramid Approach等。</li>
<li>results: 本文综述了各种图像注册技术的应用，包括图像导向手术、运动跟踪和肿瘤诊断等。此外，文献还评估了不同图像注册方法的性能，包括相关度基metric、分割基metric、处理时间和模型大小等。<details>
<summary>Abstract</summary>
Image registration (IR) is a process that deforms images to align them with respect to a reference space, making it easier for medical practitioners to examine various medical images in a standardized reference frame, such as having the same rotation and scale. This document introduces image registration using a simple numeric example. It provides a definition of image registration along with a space-oriented symbolic representation. This review covers various aspects of image transformations, including affine, deformable, invertible, and bidirectional transformations, as well as medical image registration algorithms such as Voxelmorph, Demons, SyN, Iterative Closest Point, and SynthMorph. It also explores atlas-based registration and multistage image registration techniques, including coarse-fine and pyramid approaches. Furthermore, this survey paper discusses medical image registration taxonomies, datasets, evaluation measures, such as correlation-based metrics, segmentation-based metrics, processing time, and model size. It also explores applications in image-guided surgery, motion tracking, and tumor diagnosis. Finally, the document addresses future research directions, including the further development of transformers.
</details>
<details>
<summary>摘要</summary>
图像注册（IR）是一个过程，它将图像扭曲以使其与参照空间匹配，从而使医疗专业人员可以在标准化参照框架中查看不同医学图像，例如具有相同旋转和缩放。本文介绍了图像注册的简单数学示例，并提供了图像注册的定义和空间 ориентирован的符号表示。本文评论了多种图像变换，包括Affine、可变、可逆、双向变换，以及医学图像注册算法，如Voxelmorph、Demons、SynthMorph等。此外，文章还探讨了Atlas基本注册和多阶段图像注册技术，包括粗细阶段和Pyramid方法。此外，文章还讨论了医学图像注册的分类、数据集、评价指标，如相关度基metric、分 segmentation基metric、处理时间和模型大小。最后，文章还讨论了未来研究方向，包括transformer的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Biasing-of-Named-Entities-with-Large-Language-Models"><a href="#Contextual-Biasing-of-Named-Entities-with-Large-Language-Models" class="headerlink" title="Contextual Biasing of Named-Entities with Large Language Models"></a>Contextual Biasing of Named-Entities with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00723">http://arxiv.org/abs/2309.00723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanneng Sun, Zeeshan Ahmed, Yingyi Ma, Zhe Liu, Yutong Pang, Ozlem Kalinli</li>
<li>for: 这个论文研究了大语言模型（LLM）上的上下文偏见，以提高自动语音识别（ASR）性能。</li>
<li>methods: 我们提出了在第二次评分过程中提供额外上下文信息，以及使用提前没有微调的提问来帮助 LLM 进行偏见。此外，我们还提出了在 LLM 上进行多任务训练，以便预测实体类和下一个单词。</li>
<li>results: 我们在 i) 内部电话、消息和字幕dataset 上进行了 Word Error Rate（WER）评估，以及 ii) SLUE-Voxpopuli dataset 上进行了 WER 评估。结果显示，偏见列表和少量示例可以相对于首轮 ASR 提高17.8%和9.6%，而多任务训练和动态提问可以相对于首轮 ASR 提高20.0%和11.3% WER。<details>
<summary>Abstract</summary>
This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results indicate that biasing lists and few-shot examples can achieve 17.8% and 9.6% relative improvement compared to first pass ASR, and that multi-task training and dynamic prompting can achieve 20.0% and 11.3% relative WER improvement, respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Shared-Safety-Constraints-from-Multi-task-Demonstrations"><a href="#Learning-Shared-Safety-Constraints-from-Multi-task-Demonstrations" class="headerlink" title="Learning Shared Safety Constraints from Multi-task Demonstrations"></a>Learning Shared Safety Constraints from Multi-task Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00711">http://arxiv.org/abs/2309.00711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konwoo Kim, Gokul Swamy, Zuxin Liu, Ding Zhao, Sanjiban Choudhury, Zhiwei Steven Wu</li>
<li>for: This paper is written for learning shared safety constraints for agents in a shared environment.</li>
<li>methods: The paper uses inverse reinforcement learning (IRL) techniques to learn constraints from expert demonstrations of safe task completion.</li>
<li>results: The paper shows that by leveraging diverse demonstrations in multi-task settings, it is possible to learn a tighter set of constraints that forbid highly rewarding behavior that the expert did not take.Here is the text in Simplified Chinese:</li>
<li>for: 本文是为了学习共享安全约束的机器人。</li>
<li>methods: 本文使用反奖学习（IRL）技术来学习从专家示范安全任务完成的约束。</li>
<li>results: 本文表明，通过多个任务设置中的多样示范，可以学习一个更加紧张的约束，禁止高奖励的行为。<details>
<summary>Abstract</summary>
Regardless of the particular task we want them to perform in an environment, there are often shared safety constraints we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task settings to learn a tighter set of constraints. We validate our method with simulation experiments on high-dimensional continuous control tasks.
</details>
<details>
<summary>摘要</summary>
无论我们想让机器人完成的任务，有时共同的安全限制我们希望机器人遵守。例如，无论是做披萨还是清桌，厨房机器人不应该折裂碗筐。手动指定这种限制可能是时间consuming和容易出错的。我们示意了如何通过扩展反奖学习（IRL）技术来学习限制。直觉上来说，我们学习禁止专家完成任务时高度奖励的行为。然而，限制学习问题通常会导致过度保守的限制，禁止专家没有选择的所有行为。我们利用多个示例来学习更紧密的限制，以验证我们的方法。我们在高维连续控制任务上进行了模拟实验。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-with-Human-Feedback-for-Realistic-Traffic-Simulation"><a href="#Reinforcement-Learning-with-Human-Feedback-for-Realistic-Traffic-Simulation" class="headerlink" title="Reinforcement Learning with Human Feedback for Realistic Traffic Simulation"></a>Reinforcement Learning with Human Feedback for Realistic Traffic Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00709">http://arxiv.org/abs/2309.00709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulong Cao, Boris Ivanovic, Chaowei Xiao, Marco Pavone</li>
<li>for: 这项研究旨在提高现有的交通模型真实性，以便为自动驾驶车辆开发提供可靠的系统。</li>
<li>methods: 该研究使用了人工智能奖励学习（RLHF）技术，并使用了人类反馈来对现有的交通模型进行对ignment。</li>
<li>results: 研究表明，使用TrafficRLHF框架可以生成高度真实的交通场景，并且与人类偏好高度相似。<details>
<summary>Abstract</summary>
In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. This works aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models. This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generating realistic traffic scenarios that are well-aligned with human preferences, as corroborated by comprehensive evaluations on the nuScenes dataset.
</details>
<details>
<summary>摘要</summary>
在实际测试中的挑战和成本之下，自动驾驶车开发人员经常依靠模拟测试来创建可靠的系统。一个重要的模拟测试元素是使用真实的交通模型，但这是一个具有挑战性的任务，因为需要平衡真实性和多样性。本研究旨在解决这个问题，通过使用人工智能奖励学习（RLHF）来提高现有的交通模型的真实性。本研究还 indentified两个主要挑战：捕捉人类偏好的真实性和多样化交通模型的统一。为了解决这些问题，我们提议使用人类反馈来Alignment和使用RLHF，因为它具有高效的示例效果。我们还介绍了交通模型真实性的第一个数据集，以支持这种研究。我们的框架，名为TrafficRLHF，在NUscenes数据集上进行了全面的评估，并证明了其在人类偏好的真实性上的高效性。
</details></li>
</ul>
<hr>
<h2 id="Geometric-Deep-Learning-a-Temperature-Based-Analysis-of-Graph-Neural-Networks"><a href="#Geometric-Deep-Learning-a-Temperature-Based-Analysis-of-Graph-Neural-Networks" class="headerlink" title="Geometric Deep Learning: a Temperature Based Analysis of Graph Neural Networks"></a>Geometric Deep Learning: a Temperature Based Analysis of Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00699">http://arxiv.org/abs/2309.00699</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Lapenna, F. Faglioni, F. Zanchetta, R. Fioresi</li>
<li>for: 本研究使用几何深度学习模型作为 термодинамиче系统， weights 被视为非量子和非相对论粒子。</li>
<li>methods: 我们使用先前在 [7] 中定义的温度概念，对 GC 和 GAT 模型的各层进行研究。</li>
<li>results: 我们的发现可能有可能应用于未来的潜在应用。In English, this translates to:</li>
<li>for: This study uses a Geometric Deep Learning model as a thermodynamic system, treating the weights as non-quantum and non-relativistic particles.</li>
<li>methods: We employ the notion of temperature previously defined in [7] and study it in the various layers of GCN and GAT models.</li>
<li>results: Our findings have potential future applications.<details>
<summary>Abstract</summary>
We examine a Geometric Deep Learning model as a thermodynamic system treating the weights as non-quantum and non-relativistic particles. We employ the notion of temperature previously defined in [7] and study it in the various layers for GCN and GAT models. Potential future applications of our findings are discussed.
</details>
<details>
<summary>摘要</summary>
我们研究了一个几何深度学习模型，视其参数为非量子和非狭义粒子。我们使用之前在 [7] 中定义的温度概念，对GCN和GAT模型的各层进行研究。我们还讨论了将来可能的应用。Here's a breakdown of the translation:* 我们 (wǒmen) - we* 研究 (yánjiù) - examine, study* 一个 (yī ge) - one* 几何 (jīhè) - geometric* 深度 (shēngrù) - deep* 学习 (xuéxí) - learning* 模型 (módel) - model* 参数 (cèshì) - parameters* 非量子 (fēi liàngqí) - non-quantum* 非狭义 (fēi xiǎngyì) - non-relativistic* 粒子 (liùshí) - particles* 温度 (wēndù) - temperature* 之前 (zhīqián) - previously* 在 [7] 中 (zhī qī zhōng) - in [7]* 定义 (dìngyì) - define* 研究 (yánjiù) - study* 各层 (gèilèi) - each layer* GCN (GCN) - Graph Convolutional Network* GAT (GAT) - Graph Attention Network* 将来 (jiānglì) - future* 可能 (kěnéng) - possible* 应用 (yìngyòu) - applications
</details></li>
</ul>
<hr>
<h2 id="Jointly-Exploring-Client-Drift-and-Catastrophic-Forgetting-in-Dynamic-Learning"><a href="#Jointly-Exploring-Client-Drift-and-Catastrophic-Forgetting-in-Dynamic-Learning" class="headerlink" title="Jointly Exploring Client Drift and Catastrophic Forgetting in Dynamic Learning"></a>Jointly Exploring Client Drift and Catastrophic Forgetting in Dynamic Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00688">http://arxiv.org/abs/2309.00688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niklas Babendererde, Moritz Fuchs, Camila Gonzalez, Yuri Tolkach, Anirban Mukhopadhyay</li>
<li>for: 这个论文的目的是探讨 Federated and Continual Learning 在动态环境中的应用，以及如何解决 Client Drift 和 Catastrophic Forgetting 问题。</li>
<li>methods: 这个论文使用了一种新的分析框架，可以控制 Client Drift 和 Catastrophic Forgetting 的影响，并生成一个三维表格，表示这两个问题的共同影响。</li>
<li>results: 研究发现，Client Drift 和 Catastrophic Forgetting 之间存在强相关性，并且在混合enario中，混合 Client Drift 和 Catastrophic Forgetting 可以提高模型的性能（引起一个“Generalization Bump”）。此外，研究还发现了一些现有和新的方法，可以在 Federated and Continual Learning 中应用。<details>
<summary>Abstract</summary>
Federated and Continual Learning have emerged as potential paradigms for the robust and privacy-aware use of Deep Learning in dynamic environments. However, Client Drift and Catastrophic Forgetting are fundamental obstacles to guaranteeing consistent performance. Existing work only addresses these problems separately, which neglects the fact that the root cause behind both forms of performance deterioration is connected. We propose a unified analysis framework for building a controlled test environment for Client Drift -- by perturbing a defined ratio of clients -- and Catastrophic Forgetting -- by shifting all clients with a particular strength. Our framework further leverages this new combined analysis by generating a 3D landscape of the combined performance impact from both. We demonstrate that the performance drop through Client Drift, caused by a certain share of shifted clients, is correlated to the drop from Catastrophic Forgetting resulting from a corresponding shift strength. Correlation tests between both problems for Computer Vision (CelebA) and Medical Imaging (PESO) support this new perspective, with an average Pearson rank correlation coefficient of over 0.94. Our framework's novel ability of combined spatio-temporal shift analysis allows us to investigate how both forms of distribution shift behave in mixed scenarios, opening a new pathway for better generalization. We show that a combination of moderate Client Drift and Catastrophic Forgetting can even improve the performance of the resulting model (causing a "Generalization Bump") compared to when only one of the shifts occurs individually. We apply a simple and commonly used method from Continual Learning in the federated setting and observe this phenomenon to be reoccurring, leveraging the ability of our framework to analyze existing and novel methods for Federated and Continual Learning.
</details>
<details>
<summary>摘要</summary>
“联邦学习和继续学习已经emerged as potential paradigms for the robust and privacy-aware use of deep learning in dynamic environments. However, client drift and catastrophic forgetting are fundamental obstacles to guaranteeing consistent performance. Existing work only addresses these problems separately, which neglects the fact that the root cause behind both forms of performance deterioration is connected. We propose a unified analysis framework for building a controlled test environment for client drift by perturbing a defined ratio of clients, and catastrophic forgetting by shifting all clients with a particular strength. Our framework further leverages this new combined analysis by generating a 3D landscape of the combined performance impact from both. We demonstrate that the performance drop through client drift, caused by a certain share of shifted clients, is correlated to the drop from catastrophic forgetting resulting from a corresponding shift strength. Correlation tests between both problems for computer vision (CelebA) and medical imaging (PESO) support this new perspective, with an average Pearson rank correlation coefficient of over 0.94. Our framework's novel ability of combined spatio-temporal shift analysis allows us to investigate how both forms of distribution shift behave in mixed scenarios, opening a new pathway for better generalization. We show that a combination of moderate client drift and catastrophic forgetting can even improve the performance of the resulting model (causing a "generalization bump") compared to when only one of the shifts occurs individually. We apply a simple and commonly used method from continual learning in the federated setting and observe this phenomenon to be reoccurring, leveraging the ability of our framework to analyze existing and novel methods for federated and continual learning.”
</details></li>
</ul>
<hr>
<h2 id="Randomized-Polar-Codes-for-Anytime-Distributed-Machine-Learning"><a href="#Randomized-Polar-Codes-for-Anytime-Distributed-Machine-Learning" class="headerlink" title="Randomized Polar Codes for Anytime Distributed Machine Learning"></a>Randomized Polar Codes for Anytime Distributed Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00682">http://arxiv.org/abs/2309.00682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Burak Bartan, Mert Pilanci</li>
<li>for: 这个论文是为了解决分布式计算中的慢计算节点问题，并提供一种可以进行精确和近似计算的线性操作的新框架。</li>
<li>methods: 该机制具有随机卷积和极码的概念，并提供一种顺序解码算法，可以处理实数据，同时保持低计算复杂性。此外，它还提供了一个任何时间估计器，可以在可用节点输出集不可解码时产生可靠的估计。</li>
<li>results: 作者在各种应用场景中展示了该框架的可扩展性和实用性，包括大规模矩阵乘法和黑盒优化。他们还在一个无服务器云计算系统上实现了这些方法，并提供了数值结果来证明其在实践中的扩展性。<details>
<summary>Abstract</summary>
We present a novel distributed computing framework that is robust to slow compute nodes, and is capable of both approximate and exact computation of linear operations. The proposed mechanism integrates the concepts of randomized sketching and polar codes in the context of coded computation. We propose a sequential decoding algorithm designed to handle real valued data while maintaining low computational complexity for recovery. Additionally, we provide an anytime estimator that can generate provably accurate estimates even when the set of available node outputs is not decodable. We demonstrate the potential applications of this framework in various contexts, such as large-scale matrix multiplication and black-box optimization. We present the implementation of these methods on a serverless cloud computing system and provide numerical results to demonstrate their scalability in practice, including ImageNet scale computations.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的分布式计算框架，具有耐slow compute nodes的特点，可以实现精确和估算性的线性运算计算。我们的机制结合了随机化笔记和极码在计算coded中的概念。我们提出了一种顺序解码算法，可以处理实数据，并保持低计算复杂度。此外，我们还提供了一个任何时间估计器，可以在可用节点输出集不可解码时产生可靠准确的估计。我们在各种场景中示cases, such as large-scale matrix multiplication and black-box optimization.我们在无服务器云计算系统上实现了这些方法，并提供了数字结果来证明其在实践中的扩展性，包括ImageNet scale computations。
</details></li>
</ul>
<hr>
<h2 id="Point-Bind-Point-LLM-Aligning-Point-Cloud-with-Multi-modality-for-3D-Understanding-Generation-and-Instruction-Following"><a href="#Point-Bind-Point-LLM-Aligning-Point-Cloud-with-Multi-modality-for-3D-Understanding-Generation-and-Instruction-Following" class="headerlink" title="Point-Bind &amp; Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following"></a>Point-Bind &amp; Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00615">http://arxiv.org/abs/2309.00615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziyuguo99/point-bind_point-llm">https://github.com/ziyuguo99/point-bind_point-llm</a></li>
<li>paper_authors: Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, Pheng-Ann Heng</li>
<li>for: 本研究旨在拓展3D点云到多Modal性应用，提供一种基于点云的多Modal模型对接方法，并在这个joint embedding空间中实现许多有前途的应用，如任意到3D生成、3D嵌入数学和3D开放世界理解。</li>
<li>methods: 本研究提出了Point-Bind，一种3D多Modal模型，通过ImageBind指导，在3D点云和多Modal性之间建立共同嵌入空间，并在这个空间中实现许多有前途的应用。此外，我们还提出了Point-LLM，第一个基于3D多Modal指令的大语言模型，通过精炼 Parameter-efficient fine-tuning技术，将Point-Bind的 semantics注入到预训练的LLM中，例如LLaMA，实现了Superior的3D和多Modal问答能力。</li>
<li>results: 我们通过实验表明，Point-Bind和Point-LLM可以准确地对接3D点云和多Modal性，并在多Modal问答和任意到3D生成等应用中展示出优秀的性能。<details>
<summary>Abstract</summary>
We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.
</details>
<details>
<summary>摘要</summary>
我们介绍Point-Bind，一种3D多modal模型，用于将点云与2D图像、语音、视频等多种多样化数据相对应。基于ImageBind，我们构建了3D和多modalidad之间的共同嵌入空间，这使得许多潜在应用场景得以实现，如任意到3D生成、3D嵌入数学和3D开放世界理解。此外，我们还提出了Point-LLM，第一个基于3D多modal指令的大型语言模型。通过精efficient的参数调整技术，Point-LLM可以在 pré-trained LLMs（如LLaMA）中注入点绑模型的 semantics，无需3D指令数据，但能够展示出优秀的3D和多modal问题回答能力。我们希望我们的工作能够推动社区对3D点云的扩展到多modal应用场景。代码可以在https://github.com/ZiyuGuo99/Point-Bind_Point-LLM中找到。
</details></li>
</ul>
<hr>
<h2 id="Baseline-Defenses-for-Adversarial-Attacks-Against-Aligned-Language-Models"><a href="#Baseline-Defenses-for-Adversarial-Attacks-Against-Aligned-Language-Models" class="headerlink" title="Baseline Defenses for Adversarial Attacks Against Aligned Language Models"></a>Baseline Defenses for Adversarial Attacks Against Aligned Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00614">http://arxiv.org/abs/2309.00614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein</li>
<li>for: 了解 Large Language Models 的安全漏洞，以便更好地利用它们。</li>
<li>methods: 使用文本优化器生成破坏性提示，绕过监管和规范。</li>
<li>results: 研究发现，基于文本攻击的攻击方法可以快速破坏 LLM 的安全性，而现有的防御策略在这个领域中的效果不佳。<details>
<summary>Abstract</summary>
As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?   We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>What practical threat models are relevant in this domain?2. How do baseline defense techniques perform in this new domain?3. How does LLM security differ from computer vision?We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, considering their feasibility and effectiveness in different settings. These include:1. Detection (perplexity-based)2. Input preprocessing (paraphrasing and retokenization)3. Adversarial trainingWe discuss the robustness-performance trade-off for each of these defenses in both white-box and gray-box settings. Our findings indicate that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs.Future research will be needed to determine whether more powerful optimizers can be developed or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than in computer vision.</details></li>
</ol>
<hr>
<h2 id="Iterative-Multi-granular-Image-Editing-using-Diffusion-Models"><a href="#Iterative-Multi-granular-Image-Editing-using-Diffusion-Models" class="headerlink" title="Iterative Multi-granular Image Editing using Diffusion Models"></a>Iterative Multi-granular Image Editing using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00613">http://arxiv.org/abs/2309.00613</a></li>
<li>repo_url: None</li>
<li>paper_authors: K J Joseph, Prateksha Udhayanan, Tripti Shukla, Aishwarya Agarwal, Srikrishna Karanam, Koustava Goswami, Balaji Vasan Srinivasan</li>
<li>for: 这篇论文是用于探讨如何实现文本导向的图像生成和修改，以满足创意专业人员创作时的需求。</li>
<li>methods: 这篇论文提出了一种名为Iterative Multi-granular Editing（简称EMILIE）的新方法，它可以在图像生成和修改过程中提供迭代性和多级控制。EMILIE使用了一个受条件的算法，并且引入了一个新的 Gradient Control 操作，以便在图像中进行多级控制。</li>
<li>results: 这篇论文通过了对于最新的类比方法的评估，以证明EMILIE在实现文本导向的图像生成和修改方面的表现。它还引入了一个新的评估数据集，以便更好地评估这个问题的解决方案。<details>
<summary>Abstract</summary>
Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control operation for multi-granular control. We introduce a new benchmark dataset to evaluate our newly proposed setting. We conduct exhaustive quantitatively and qualitatively evaluation against recent state-of-the-art approaches adapted to our task, to being out the mettle of EMILIE. We hope our work would attract attention to this newly identified, pragmatic problem setting.
</details>
<details>
<summary>摘要</summary>
最近的文本带领图生成技术发展，对艺术创作者的视觉艺术创作带来了巨大的变革。为了全面支持这些创新，生成过程应该拥有以下两个能力：1）可Iteratively编辑生成结果，2）控制生成结果的空间范围（全球、本地或任何位置之间）。我们将这个实用问题设定为迭代多级编辑问题。 DESPITE SUBSTANTIAL PROGRESS WITH DIFFUSION-BASED MODELS FOR IMAGE SYNTHESIS AND EDITING, THEY ALL HAVE ONE SHOT CAPABILITIES AND DO NOT NATURALLY YIELD MULTI-GRANULAR CONTROL. TO OVERCOME THESE DRAWBACKS, WE PROPOSE EMILIE: ITERATIVE MULTI-GRANULAR IMAGE EDITOR. EMILIE INTRODUCES A NOVEL LATENT ITERATION STRATEGY, WHICH REPURPOSES A PRE-TRAINED DIFFUSION MODEL TO FACILITATE ITERATIVE EDITING. THIS IS COMPLEMENTED BY A GRADIENT CONTROL OPERATION FOR MULTI-GRANULAR CONTROL. WE INTRODUCE A NEW BENCHMARK DATASET TO EVALUATE OUR NEWLY PROPOSED SETTING. WE CONDUCT EXHAUSTIVE QUANTITATIVELY AND QUALITATIVELY EVALUATION AGAINST RECENT STATE-OF-THE-ART APPROACHES ADAPTED TO OUR TASK, TO BEGIN THE METTLE OF EMILIE. WE HOPE OUR WORK WOULD ATTRACT ATTENTION TO THIS NEWLY IDENTIFIED, PRAGMATIC PROBLEM SETTING.
</details></li>
</ul>
<hr>
<h2 id="Bayesian-deep-learning-for-cosmic-volumes-with-modified-gravity"><a href="#Bayesian-deep-learning-for-cosmic-volumes-with-modified-gravity" class="headerlink" title="Bayesian deep learning for cosmic volumes with modified gravity"></a>Bayesian deep learning for cosmic volumes with modified gravity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00612">http://arxiv.org/abs/2309.00612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JavierOrjuela/Bayesian-Neural-Net-with-MNFs-for-f-R-">https://github.com/JavierOrjuela/Bayesian-Neural-Net-with-MNFs-for-f-R-</a></li>
<li>paper_authors: Jorge Enrique García-Farieta, Héctor J Hortúa, Francisco-Shu Kitaura</li>
<li>for: 这项研究的目的是从修改gravity（MG）模拟中提取 cosmological parameters，并使用深度学习网络（BNN）来实现这一目的，并且包括对不确定性的评估。</li>
<li>methods: 该研究使用了 Bayesian neural networks（BNNs），并在其中引入了一个权重网络（BLL）和一个权重网络在所有层次（FullB），以便在模拟中提取 cosmological parameters。</li>
<li>results: 研究发现，BNNs 可以准确地预测 cosmological parameters，并且可以提供正确的不确定性评估。 另外，研究还发现了 MG 参数对 $\sigma_8$ 的潜在束缚，并且在忽略 MG 时，对 $\Omega_m$ 和 $\sigma_8$ 的误差相对较大。<details>
<summary>Abstract</summary>
The new generation of galaxy surveys will provide unprecedented data allowing us to test gravity at cosmological scales. A robust cosmological analysis of the large-scale structure demands exploiting the nonlinear information encoded in the cosmic web. Machine Learning techniques provide such tools, however, do not provide a priori assessment of uncertainties. This study aims at extracting cosmological parameters from modified gravity (MG) simulations through deep neural networks endowed with uncertainty estimations. We implement Bayesian neural networks (BNNs) with an enriched approximate posterior distribution considering two cases: one with a single Bayesian last layer (BLL), and another one with Bayesian layers at all levels (FullB). We train both BNNs with real-space density fields and power-spectra from a suite of 2000 dark matter only particle mesh $N$-body simulations including modified gravity models relying on MG-PICOLA covering 256 $h^{-1}$ Mpc side cubical volumes with 128$^3$ particles. BNNs excel in accurately predicting parameters for $\Omega_m$ and $\sigma_8$ and their respective correlation with the MG parameter. We find out that BNNs yield well-calibrated uncertainty estimates overcoming the over- and under-estimation issues in traditional neural networks. We observe that the presence of MG parameter leads to a significant degeneracy with $\sigma_8$ being one of the possible explanations of the poor MG predictions. Ignoring MG, we obtain a deviation of the relative errors in $\Omega_m$ and $\sigma_8$ by at least $30\%$. Moreover, we report consistent results from the density field and power spectra analysis, and comparable results between BLL and FullB experiments which permits us to save computing time by a factor of two. This work contributes in setting the path to extract cosmological parameters from complete small cosmic volumes towards the highly nonlinear regime.
</details>
<details>
<summary>摘要</summary>
新一代星系调查将提供前所未有的数据，允许我们在 cosmological  scales 测试引力。cosmic web 的非线性信息可以被滥用，但 machine learning 技术不提供先前的不确定性评估。这一研究目的是使用深度神经网络（BNNs）从修改引力（MG） simulations 中提取 cosmological 参数。我们实现了两种情况：一个包括单一的 Bayesian last layer（BLL），另一个则是在所有层级使用 Bayesian layers（FullB）。我们将这两种 BNNs 训练使用 real-space 体积和对应的对数 спектル，这些对应是由一个 suite 的 2000 个暗物质对称粒子网络 $N$-body  simulations 中的修改引力模型。BNNs 在精确地预测参数方面表现出色，特别是在 $\Omega_m$ 和 $\sigma_8$ 的参数和 MG 参数之间的相互作用方面。我们发现 BNNs 可以提供内在地对不确定性的评估，并且与传统神经网络相比，可以避免过度或未足的估计问题。我们发现 MG 参数的存在会导致 $\sigma_8$ 的剧烈束ね，这可能是 MG 预测的一个原因。忽略 MG，我们发现遗传性的错误可以至少降低 $30\%$。此外，我们发现 density field 和对应的对数 спектル 分析的结果相互一致，并且 FullB 和 BLL 实验的结果相互一致，这Permits 我们节省computing 时间，可以将 Computing 时间降低到最低的一半。这一研究将 cosmological 参数从完整的小 cosmic 体积中提取，进一步推进 cosmological 参数的测量和分析。
</details></li>
</ul>
<hr>
<h2 id="Copiloting-the-Copilots-Fusing-Large-Language-Models-with-Completion-Engines-for-Automated-Program-Repair"><a href="#Copiloting-the-Copilots-Fusing-Large-Language-Models-with-Completion-Engines-for-Automated-Program-Repair" class="headerlink" title="Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair"></a>Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00608">http://arxiv.org/abs/2309.00608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ise-uiuc/Repilot">https://github.com/ise-uiuc/Repilot</a></li>
<li>paper_authors: Yuxiang Wei, Chunqiu Steven Xia, Lingming Zhang</li>
<li>for: 提高自动程序修复（APR）的实用性，使用大语言模型（LLM）作为“助手”，帮助开发者更快速地修复bug。</li>
<li>methods: 提出Repilot框架，通过融合LLM和完成引擎来生成更有效的补丁。Repilot首先使用LLM生成候选补丁，然后使用完成引擎对候选补丁进行约束和补充，以确保生成的补丁满足语言约束。</li>
<li>results: 在Defects4j 1.2和2.0数据集上进行了评估，发现Repilot可以 fix 66和50个bug，比基准值最佳化的方法高出14和16个bug。此外，Repilot还可以生成更有效和正确的补丁，超过基础LLM的生成能力。<details>
<summary>Abstract</summary>
During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful "copilots" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthesizes a candidate patch through the interaction between an LLM and a Completion Engine, which 1) prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the Completion Engine. Our evaluation on a subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50 bugs, respectively, surpassing the best-performing baseline by 14 and 16 bugs fixed. More importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given the same generation budget.
</details>
<details>
<summary>摘要</summary>
在自动化程序修复（APR）中，可能遇到困难Synthesizing correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful "copilots" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthesizes a candidate patch through the interaction between an LLM and a Completion Engine, which 1) prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the Completion Engine. Our evaluation on a subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50 bugs, respectively, surpassing the best-performing baseline by 14 and 16 bugs fixed. More importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given the same generation budget.
</details></li>
</ul>
<hr>
<h2 id="Taken-out-of-context-On-measuring-situational-awareness-in-LLMs"><a href="#Taken-out-of-context-On-measuring-situational-awareness-in-LLMs" class="headerlink" title="Taken out of context: On measuring situational awareness in LLMs"></a>Taken out of context: On measuring situational awareness in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00667">http://arxiv.org/abs/2309.00667</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asacooperstickland/situational-awareness-evals">https://github.com/asacooperstickland/situational-awareness-evals</a></li>
<li>paper_authors: Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, Owain Evans</li>
<li>for: 这个论文的目的是更好地理解大语言模型（LLM）中的“情境意识”（situational awareness）的起源。</li>
<li>methods: 作者使用了“out-of-context reasoning”这一能力来研究情境意识的出现。他们首先在一个描述中训练了一个LLM，然后在测试时评估模型是否可以通过测试。</li>
<li>results: 研究发现，LLMs可以在没有示例或示范的情况下通过测试，并且模型的性能随模型大小的增加而提高。这些发现可以为预测和控制LLMs中情境意识的出现提供基础。<details>
<summary>Abstract</summary>
We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-of-context reasoning task. Their success is sensitive to the training setup and only works when we apply data augmentation. For both GPT-3 and LLaMA-1, performance improves with model size. These findings offer a foundation for further empirical study, towards predicting and potentially controlling the emergence of situational awareness in LLMs. Code is available at: https://github.com/AsaCooperStickland/situational-awareness-evals.
</details>
<details>
<summary>摘要</summary>
我们目标是更好地理解大语言模型（LLM）中的情境意识的出现。一个模型被称为情境意识的，如果它知道它是一个模型，并能识别它当前是在测试还是部署中。今天的LLM都会在安全性和对齐前被测试。一个LLM可以利用情境意识来达到安全测试高分，而在部署后进行有害行为。情境意识可能会不预期地在模型扩展时出现。为了更好地预测这种出现，我们可以在可能需要的能力上进行扩展试验。作为一种能力，我们提出了“离线理解”（与上下文学习相对）。我们通过实验研究离线理解。我们首先对LLM进行了折算，并在测试时没有给出示例或示范。我们发现，LLM在这种离线理解任务中成功。其成功виси于训练设置，并且只有在应用数据振荡时才能达到最佳效果。对GPT-3和LLaMA-1来说，模型大小的增加会导致性能提高。这些发现可以提供对情境意识在LLM中出现的预测和控制的基础。代码可以在 GitHub上找到：https://github.com/AsaCooperStickland/situational-awareness-evals。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Regret-Optimal-Best-Arm-Identification-Fundamental-Limits-and-Low-Complexity-Algorithms"><a href="#Fast-and-Regret-Optimal-Best-Arm-Identification-Fundamental-Limits-and-Low-Complexity-Algorithms" class="headerlink" title="Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms"></a>Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00591">http://arxiv.org/abs/2309.00591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qining Zhang, Lei Ying</li>
<li>for: 本文研究了一种多重随机抽筋问题（Multi-armed Bandit，MAB），该问题的两个目标是：快速确定优秀的抽筋，并在一系列T个轮次中最大化奖励。虽然每个目标都已经分别得到了研究，但是同时实现这两个目标还是一个未解的问题。本文提出了一种名为Regret Optimal Best Arm Identification（ROBAI）的算法，旨在实现这两个目标。</li>
<li>methods: 本文使用了一种名为$\mathsf{EOCP}$的算法和其变种，以实现ROBAI。这些算法不仅在 Gaussian 和通用抽筋中实现了极限优化的 regret，而且在预先确定停止时间和自适应停止时间的情况下，可以在 $\mathcal{O}(\log T)$ 轮次内提前commit到优秀的抽筋。</li>
<li>results: 本文证明了ROBAI的下界，表明$\mathsf{EOCP}$ 和其变种是sampleoptimal的，并且可以在预先确定停止时间下commit到优秀的抽筋在 $\mathcal{O}(\log T)$ 轮次内。numerical results表明，$\mathsf{EOCP}$ 比 класси型的 $\mathsf{UCB}$ 算法（例如 $\mathsf{UCB1}$）更小的 regret，而且可以在 $\mathcal{O}(\log T)$ 轮次内提前commit到优秀的抽筋，这表明过度探索是不必要且可能对系统性能有负面影响。<details>
<summary>Abstract</summary>
This paper considers a stochastic multi-armed bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of $T$ consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces \emph{Regret Optimal Best Arm Identification} (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present the $\mathsf{EOCP}$ algorithm and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in $\mathcal{O}(\log T)$ rounds with pre-determined stopping time and $\mathcal{O}(\log^2 T)$ rounds with adaptive stopping time. We further characterize lower bounds on the commitment time (equivalent to sample complexity) of ROBAI, showing that $\mathsf{EOCP}$ and its variants are sample optimal with pre-determined stopping time, and almost sample optimal with adaptive stopping time. Numerical results confirm our theoretical analysis and reveal an interesting ``over-exploration'' phenomenon carried by classic $\mathsf{UCB}$ algorithms, such that $\mathsf{EOCP}$ has smaller regret even though it stops exploration much earlier than $\mathsf{UCB}$ ($\mathcal{O}(\log T)$ versus $\mathcal{O}(T)$), which suggests over-exploration is unnecessary and potentially harmful to system performance.
</details>
<details>
<summary>摘要</summary>
这篇论文考虑了随机多机枪（MAB）问题，其具有两个目标：快速确定优质机枪，以及在一串T个轮次中最大化奖励。虽然每一个目标都已经得到了单独的研究，但是同时实现这两个目标仍然是一个未解的问题，尤其是在实际应用中具有重要性。这篇论文提出了“归 optimal best arm identification”（ROBAI）问题，以实现这两个目标。为解决ROBAI问题，我们提出了$\mathsf{EOCP}$算法和其变种，可以在 Gaussian 和通用抽象机枪下实现 asymptotic optimal regret，并且在预先确定停止时间和自适应停止时间下均可以在 $\mathcal{O}(\log T)$ 轮次内确定优质机枪。我们还计算了 ROBAI 下的准则时间（相当于样本复杂度）下界，显示 $\mathsf{EOCP}$ 和其变种是样本最优的，并且在预先确定停止时间下是样本最优的，而在自适应停止时间下是几乎样本最优的。numerical results 表明，$\mathsf{EOCP}$ 比 класси $\mathsf{UCB}$ 算法更小的 regret，即使 $\mathsf{EOCP}$ 在 $\mathcal{O}(\log T)$ 轮次内停止探索，而 $\mathsf{UCB}$ 在 $\mathcal{O}(T)$ 轮次内仍在探索。这表明过探索是不必要且可能对系统性能有害。
</details></li>
</ul>
<hr>
<h2 id="PolyGET-Accelerating-Polymer-Simulations-by-Accurate-and-Generalizable-Forcefield-with-Equivariant-Transformer"><a href="#PolyGET-Accelerating-Polymer-Simulations-by-Accurate-and-Generalizable-Forcefield-with-Equivariant-Transformer" class="headerlink" title="PolyGET: Accelerating Polymer Simulations by Accurate and Generalizable Forcefield with Equivariant Transformer"></a>PolyGET: Accelerating Polymer Simulations by Accurate and Generalizable Forcefield with Equivariant Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00585">http://arxiv.org/abs/2309.00585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Feng, Huan Tran, Aubrey Toland, Binghong Chen, Qi Zhu, Rampi Ramprasad, Chao Zhang</li>
<li>for: 本研究旨在开发一种能够同时具备准确和效率的聚合物模拟方法，通过使用机器学习（ML）力场来实现这个目标。</li>
<li>methods: 本研究使用的方法是一种新的普适聚合物力场框架，称为PolyGET，它使用一种深度学习模型名为恒等变换器来捕捉聚合物中原子之间的复杂量子交互。</li>
<li>results: 研究人员通过对24种不同的聚合物类型进行大规模的数据集训练，证明了PolyGET在追求准确的力场和可靠的分子动力学 simulations中具有领先的性能。此外，PolyGET还可以在大聚合物中实现高准确性的模拟，同时能够泛化到未经见过的聚合物。<details>
<summary>Abstract</summary>
Polymer simulation with both accuracy and efficiency is a challenging task. Machine learning (ML) forcefields have been developed to achieve both the accuracy of ab initio methods and the efficiency of empirical force fields. However, existing ML force fields are usually limited to single-molecule settings, and their simulations are not robust enough. In this paper, we present PolyGET, a new framework for Polymer Forcefields with Generalizable Equivariant Transformers. PolyGET is designed to capture complex quantum interactions between atoms and generalize across various polymer families, using a deep learning model called Equivariant Transformers. We propose a new training paradigm that focuses exclusively on optimizing forces, which is different from existing methods that jointly optimize forces and energy. This simple force-centric objective function avoids competing objectives between energy and forces, thereby allowing for learning a unified forcefield ML model over different polymer families. We evaluated PolyGET on a large-scale dataset of 24 distinct polymer types and demonstrated state-of-the-art performance in force accuracy and robust MD simulations. Furthermore, PolyGET can simulate large polymers with high fidelity to the reference ab initio DFT method while being able to generalize to unseen polymers.
</details>
<details>
<summary>摘要</summary>
聚合物模拟中兼具精度和效率是一项具有挑战性的任务。机器学习（ML）力场已经开发出来实现精度和实验力场的同时达到精度和效率之间的平衡。然而，现有的ML力场通常只适用于单分子设置，其模拟不够稳定。在本文中，我们介绍了PolyGET，一个新的聚合物力场框架，用于捕捉聚合物中的复杂量子交互作用并将其扩展到不同的聚合物家族。PolyGET使用深度学习模型calledEquivariant Transformers来捕捉这些交互作用。我们提出了一种新的训练方法，即专注于优化力场，而不是与能量一起优化力场和能量的现有方法。这种简单的力场-中心的目标函数可以避免在能量和力场之间的竞争目标，从而允许学习一个通用的ML模型。我们对24种不同的聚合物类型进行了大规模的数据集训练，并证明了PolyGET在力场准确性和稳定MD模拟方面表现出了状态顶尖的性能。此外，PolyGET可以模拟大聚合物，同时具有高度准确性和可扩展性。
</details></li>
</ul>
<hr>
<h2 id="Laminar-A-New-Serverless-Stream-based-Framework-with-Semantic-Code-Search-and-Code-Completion"><a href="#Laminar-A-New-Serverless-Stream-based-Framework-with-Semantic-Code-Search-and-Code-Completion" class="headerlink" title="Laminar: A New Serverless Stream-based Framework with Semantic Code Search and Code Completion"></a>Laminar: A New Serverless Stream-based Framework with Semantic Code Search and Code Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00584">http://arxiv.org/abs/2309.00584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaynab Zahra, Zihao Li, Rosa Filgueira</li>
<li>for: 这篇论文是为了推介一种基于dispel4py的无服务框架，提供了一个简单的无服务体验。</li>
<li>methods: 该框架使用了大型自然语言模型，实现了语义代码搜索、代码概要和代码完成。</li>
<li>results: 该论文提高了无服务计算的执行效率，更有效地管理数据流程，为研究人员和实践者提供了一个有价值的工具。<details>
<summary>Abstract</summary>
This paper introduces Laminar, a novel serverless framework based on dispel4py, a parallel stream-based dataflow library. Laminar efficiently manages streaming workflows and components through a dedicated registry, offering a seamless serverless experience. Leveraging large lenguage models, Laminar enhances the framework with semantic code search, code summarization, and code completion. This contribution enhances serverless computing by simplifying the execution of streaming computations, managing data streams more efficiently, and offering a valuable tool for both researchers and practitioners.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的服务器less框架，叫做Laminar，它基于dispel4py，一个并发流程数据流库。Laminar有效地管理流动工作流和组件通过专门的注册表，提供了无缝服务器的体验。通过使用大型语言模型，Laminar增强了框架，添加了semantic code搜索、代码概要和代码完成功能。这一贡献使服务器less计算得到了进一步简化，数据流更加有效地管理，并为研究人员和实践者提供了一个有价值的工具。
</details></li>
</ul>
<hr>
<h2 id="Geometry-Informed-Neural-Operator-for-Large-Scale-3D-PDEs"><a href="#Geometry-Informed-Neural-Operator-for-Large-Scale-3D-PDEs" class="headerlink" title="Geometry-Informed Neural Operator for Large-Scale 3D PDEs"></a>Geometry-Informed Neural Operator for Large-Scale 3D PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00583">http://arxiv.org/abs/2309.00583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongyi Li, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Mohammad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, Anima Anandkumar</li>
<li>for: 这个论文是用于学习大规模partial differential equations的解operator的高效方法。</li>
<li>methods: 这个方法使用了签名距离函数和点云表示输入形状，以及基于图和傅ри方法的神经网络来学习解operator。</li>
<li>results: 这个方法可以高效地应用于大规模的Fluid动力学问题，并且可以与优化的GPU基于CFD simulator进行比较，在计算表面压力方面实现$26,000 \times$的速度提升。此外，当测试新的几何体和边界条件时，GINO可以 reducel error rate by one-fourth compared to深度神经网络方法。<details>
<summary>Abstract</summary>
We propose the geometry-informed neural operator (GINO), a highly efficient approach to learning the solution operator of large-scale partial differential equations with varying geometries. GINO uses a signed distance function and point-cloud representations of the input shape and neural operators based on graph and Fourier architectures to learn the solution operator. The graph neural operator handles irregular grids and transforms them into and from regular latent grids on which Fourier neural operator can be efficiently applied. GINO is discretization-convergent, meaning the trained model can be applied to arbitrary discretization of the continuous domain and it converges to the continuum operator as the discretization is refined. To empirically validate the performance of our method on large-scale simulation, we generate the industry-standard aerodynamics dataset of 3D vehicle geometries with Reynolds numbers as high as five million. For this large-scale 3D fluid simulation, numerical methods are expensive to compute surface pressure. We successfully trained GINO to predict the pressure on car surfaces using only five hundred data points. The cost-accuracy experiments show a $26,000 \times$ speed-up compared to optimized GPU-based computational fluid dynamics (CFD) simulators on computing the drag coefficient. When tested on new combinations of geometries and boundary conditions (inlet velocities), GINO obtains a one-fourth reduction in error rate compared to deep neural network approaches.
</details>
<details>
<summary>摘要</summary>
我们提出 geometry-informed neural operator（GINO），一种高效的方法来学习大规模 partial differential equations（PDEs）的解析器。GINO 使用了签名距离函数和点 cloud 表示法来将输入形状转换为和从变化的对称格上的对称点 cloud，然后使用传统的 graph 和 Fourier 架构来学习解析器。GINO 是精确的� discrete-convergent，这意味着训练的模型可以在任何精确度上应用，并且可以将精确度提高为精确度。为了实际验证我们的方法，我们生成了3D vehicle geometry industry-standard aerodynamics dataset，并使用 Reynolds 数到五百万。这个大规模 3D 流体 simulations 的� numerical 方法是昂费的，因此我们只需要使用 GINO 来预测车身表面压力，只需要500个数据点。cost-accuracy 实验显示，GINO 与优化的 GPU-based computational fluid dynamics（CFD）估计器相比，可以节省 $26,000 \times$ 的计算成本。当 Test 在新的几何和缘界条件（吸入速度）下进行时，GINO 与深度神经网络方法相比，可以节省一半的错误率。
</details></li>
</ul>
<hr>
<h2 id="Consistency-of-Lloyd’s-Algorithm-Under-Perturbations"><a href="#Consistency-of-Lloyd’s-Algorithm-Under-Perturbations" class="headerlink" title="Consistency of Lloyd’s Algorithm Under Perturbations"></a>Consistency of Lloyd’s Algorithm Under Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00578">http://arxiv.org/abs/2309.00578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Patel, Hui Shen, Shankar Bhamidi, Yufeng Liu, Vladas Pipiras</li>
<li>for: 本研究旨在证明 Lloyd 算法在不监督学习中的正确性，以及在不同设定下对 Lloyd 算法的改进。</li>
<li>methods: 本研究使用 Lloyd 算法和相关的初始化算法 ($k$-means$++$)，以及spectral方法来处理数据。</li>
<li>results: 研究发现，对于具有子-${\sf G}$aussian 混合的样本，Lloyd 算法在 $O(\log(n))$ 迭代后，其误分类率是指数减少的。此外，研究还证明了在实际应用中常见的数据预处理步骤中，Lloyd 算法的误分类率仍然保持在指数减少的水平。<details>
<summary>Abstract</summary>
In the context of unsupervised learning, Lloyd's algorithm is one of the most widely used clustering algorithms. It has inspired a plethora of work investigating the correctness of the algorithm under various settings with ground truth clusters. In particular, in 2016, Lu and Zhou have shown that the mis-clustering rate of Lloyd's algorithm on $n$ independent samples from a sub-Gaussian mixture is exponentially bounded after $O(\log(n))$ iterations, assuming proper initialization of the algorithm. However, in many applications, the true samples are unobserved and need to be learned from the data via pre-processing pipelines such as spectral methods on appropriate data matrices. We show that the mis-clustering rate of Lloyd's algorithm on perturbed samples from a sub-Gaussian mixture is also exponentially bounded after $O(\log(n))$ iterations under the assumptions of proper initialization and that the perturbation is small relative to the sub-Gaussian noise. In canonical settings with ground truth clusters, we derive bounds for algorithms such as $k$-means$++$ to find good initializations and thus leading to the correctness of clustering via the main result. We show the implications of the results for pipelines measuring the statistical significance of derived clusters from data such as SigClust. We use these general results to derive implications in providing theoretical guarantees on the misclustering rate for Lloyd's algorithm in a host of applications, including high-dimensional time series, multi-dimensional scaling, and community detection for sparse networks via spectral clustering.
</details>
<details>
<summary>摘要</summary>
在不监督学习框架下，洛德算法是最常用的聚类算法之一。它在不同设置下的正确性得到了大量的研究。特别是在2016年，陆和周发现了 Lloyd's algorithm 在 $n$ 个独立样本上的误分类率是 exponentially bounded 的，假设初始化正确。然而，在许多应用中，真实的样本是未观察的，需要通过预处理管道，如спектраль方法，来学习从数据中。我们表明，在修正样本上，Lloyd's algorithm 的误分类率也是 exponentially bounded 的，假设初始化正确，并且修正是相对于 sub-Gaussian 噪声的小。在 canonical 的设置下，我们 derive  bounds for algorithms 如 $k$-means++ 以找到好的初始化，从而确保 clustering 的正确性。我们显示了这些结果对 SigClust 等 pipelines 的含义，以及在高维时间序列、多维排序和稀疏网络社区探测中的应用。
</details></li>
</ul>
<hr>
<h2 id="Mechanism-of-feature-learning-in-convolutional-neural-networks"><a href="#Mechanism-of-feature-learning-in-convolutional-neural-networks" class="headerlink" title="Mechanism of feature learning in convolutional neural networks"></a>Mechanism of feature learning in convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00570">http://arxiv.org/abs/2309.00570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aradha/convrfm">https://github.com/aradha/convrfm</a></li>
<li>paper_authors: Daniel Beaglehole, Adityanarayanan Radhakrishnan, Parthe Pandit, Mikhail Belkin</li>
<li>For: This paper aims to understand the mechanism of how convolutional neural networks (CNNs) learn features from image data.* Methods: The authors propose the Convolutional Neural Feature Ansatz, which states that the covariances of filters in any convolutional layer are proportional to the average gradient outer product (AGOP) taken with respect to patches of the input to that layer. They provide extensive empirical evidence for their ansatz and supporting theoretical evidence.* Results: The authors demonstrate the generality of their result by using the patch-based AGOP to enable deep feature learning in convolutional kernel machines, which they call (Deep) ConvRFM. They show that their algorithm recovers similar features to deep convolutional networks, including the notable emergence of edge detectors, and overcomes previously identified limitations of convolutional kernels, leading to sizable performance improvement over fixed convolutional kernels.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是理解卷积神经网络（CNN）如何从图像数据中学习特征。</li>
<li>methods: 作者提出了卷积神经特征假设（Convolutional Neural Feature Ansatz），即任何卷积层的权重相关性与输入图像中的块相对的梯度外积（AGOP）之间存在直接关系。他们提供了丰富的实验证据和支持的理论证据。</li>
<li>results: 作者通过使用块基于的AGOP来实现深度特征学习的卷积核机器（(Deep) ConvRFM），并证明其能够模仿深度卷积神经网络的特征，包括Edge detector的出现。此外，他们发现(Deep) ConvRFM可以超越先前已知的卷积核的限制，例如其无法适应图像中的本地信号，从而导致性能提升。<details>
<summary>Abstract</summary>
Understanding the mechanism of how convolutional neural networks learn features from image data is a fundamental problem in machine learning and computer vision. In this work, we identify such a mechanism. We posit the Convolutional Neural Feature Ansatz, which states that covariances of filters in any convolutional layer are proportional to the average gradient outer product (AGOP) taken with respect to patches of the input to that layer. We present extensive empirical evidence for our ansatz, including identifying high correlation between covariances of filters and patch-based AGOPs for convolutional layers in standard neural architectures, such as AlexNet, VGG, and ResNets pre-trained on ImageNet. We also provide supporting theoretical evidence. We then demonstrate the generality of our result by using the patch-based AGOP to enable deep feature learning in convolutional kernel machines. We refer to the resulting algorithm as (Deep) ConvRFM and show that our algorithm recovers similar features to deep convolutional networks including the notable emergence of edge detectors. Moreover, we find that Deep ConvRFM overcomes previously identified limitations of convolutional kernels, such as their inability to adapt to local signals in images and, as a result, leads to sizable performance improvement over fixed convolutional kernels.
</details>
<details>
<summary>摘要</summary>
We provide extensive empirical evidence for our ansatz, including showing high correlation between covariances of filters and patch-based AGOPs for convolutional layers in standard neural architectures such as AlexNet, VGG, and ResNets pre-trained on ImageNet. We also provide supporting theoretical evidence.We then demonstrate the generality of our result by using the patch-based AGOP to enable deep feature learning in convolutional kernel machines. We refer to the resulting algorithm as (Deep) ConvRFM and show that our algorithm recovers similar features to deep convolutional networks, including the notable emergence of edge detectors. Moreover, we find that (Deep) ConvRFM overcomes previously identified limitations of convolutional kernels, such as their inability to adapt to local signals in images, and leads to a sizable performance improvement over fixed convolutional kernels.
</details></li>
</ul>
<hr>
<h2 id="Amyloid-Beta-Axial-Plane-PET-Synthesis-from-Structural-MRI-An-Image-Translation-Approach-for-Screening-Alzheimer’s-Disease"><a href="#Amyloid-Beta-Axial-Plane-PET-Synthesis-from-Structural-MRI-An-Image-Translation-Approach-for-Screening-Alzheimer’s-Disease" class="headerlink" title="Amyloid-Beta Axial Plane PET Synthesis from Structural MRI: An Image Translation Approach for Screening Alzheimer’s Disease"></a>Amyloid-Beta Axial Plane PET Synthesis from Structural MRI: An Image Translation Approach for Screening Alzheimer’s Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00569">http://arxiv.org/abs/2309.00569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Vega, Abdoljalil Addeh, M. Ethan MacDonald</li>
<li>for: 用于生成基于MRI的抑制蛋白β肽影像</li>
<li>methods: 使用图像翻译模型，使用图像对的训练</li>
<li>results: 实现了高度匹配真实数据的synthetic肽影像，包括形态、对比度和总SSIM&#x2F;PSNRTranslation:</li>
<li>for: Used to generate amyloid-beta PET images based on MRI</li>
<li>methods: Using an image translation model, trained with image pairs</li>
<li>results: Achieved highly matching synthetic PET images, including shape, contrast, and overall SSIM&#x2F;PSNR<details>
<summary>Abstract</summary>
In this work, an image translation model is implemented to produce synthetic amyloid-beta PET images from structural MRI that are quantitatively accurate. Image pairs of amyloid-beta PET and structural MRI were used to train the model. We found that the synthetic PET images could be produced with a high degree of similarity to truth in terms of shape, contrast and overall high SSIM and PSNR. This work demonstrates that performing structural to quantitative image translation is feasible to enable the access amyloid-beta information from only MRI.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们实现了一种图像翻译模型，用于从结构成像MRI中生成具有量化准确性的聚丙烯酸βPET图像。我们使用了amyloid-βPET和结构MRI图像对组来训练模型。我们发现了，通过使用这种模型，可以生成与真实图像高度相似的合成PET图像，具有高度相似的形状、对比度和总体高度SSIM和PSNR。这项工作证明了，从结构图像到量化图像的转化是可能的，以便通过只有MRI获取聚丙烯酸β信息。Note: "SSIM" stands for "Structural Similarity Index Measure" and "PSNR" stands for "Peak Signal-to-Noise Ratio". Both are commonly used metrics for evaluating the quality of image translation.
</details></li>
</ul>
<hr>
<h2 id="Interpretation-of-High-Dimensional-Linear-Regression-Effects-of-Nullspace-and-Regularization-Demonstrated-on-Battery-Data"><a href="#Interpretation-of-High-Dimensional-Linear-Regression-Effects-of-Nullspace-and-Regularization-Demonstrated-on-Battery-Data" class="headerlink" title="Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data"></a>Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00564">http://arxiv.org/abs/2309.00564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joachimschaeffer/hdreganalytics">https://github.com/joachimschaeffer/hdreganalytics</a></li>
<li>paper_authors: Joachim Schaeffer, Eric Lenz, William C. Chueh, Martin Z. Bazant, Rolf Findeisen, Richard D. Braatz</li>
<li>for: 这篇论文关注高维线性回归问题，尤其是在化学或生物系统中获得的离散测量数据上。</li>
<li>methods: 这篇论文提出了一种优化形式来比较线性回归系数，并通过物理工程知识来理解哪些系数差异与null空间相似。</li>
<li>results: 实验和案例研究表明，正则化和z-scoring是设计选择，如果与先前的物理知识相符，可以导致可 interpret的回归结果。否则，null空间和正则化的交互可能使回归结果难以理解，甚至无法获得真实的线性模型。此外，研究还表明，不生成null空间正交的回归方法，如拟合lasso，可以提高可 interpret性。<details>
<summary>Abstract</summary>
High-dimensional linear regression is important in many scientific fields. This article considers discrete measured data of underlying smooth latent processes, as is often obtained from chemical or biological systems. Interpretation in high dimensions is challenging because the nullspace and its interplay with regularization shapes regression coefficients. The data's nullspace contains all coefficients that satisfy $\mathbf{Xw}=\mathbf{0}$, thus allowing very different coefficients to yield identical predictions. We developed an optimization formulation to compare regression coefficients and coefficients obtained by physical engineering knowledge to understand which part of the coefficient differences are close to the nullspace. This nullspace method is tested on a synthetic example and lithium-ion battery data. The case studies show that regularization and z-scoring are design choices that, if chosen corresponding to prior physical knowledge, lead to interpretable regression results. Otherwise, the combination of the nullspace and regularization hinders interpretability and can make it impossible to obtain regression coefficients close to the true coefficients when there is a true underlying linear model. Furthermore, we demonstrate that regression methods that do not produce coefficients orthogonal to the nullspace, such as fused lasso, can improve interpretability. In conclusion, the insights gained from the nullspace perspective help to make informed design choices for building regression models on high-dimensional data and reasoning about potential underlying linear models, which are important for system optimization and improving scientific understanding.
</details>
<details>
<summary>摘要</summary>
高维Linear regression是科学领域中非常重要的。本文考虑了化学或生物系统中的离散测量数据，这些数据通常表示下面的细腻流程。在高维情况下，解释是非常困难的，因为nullspace和其与正则化的交互对回归系数产生了深刻的影响。nullspace中的所有系数满足 $\mathbf{Xw = 0}$，因此允许非常不同的系数导致 identical predictions。我们开发了一种优化形式来比较回归系数和基于物理工程知识来理解哪些系数差异都是靠近nullspace的。这个nullspace方法在一个 sintetic example和锂离子电池数据上进行了测试。案例研究表明，正则化和z-scoring是设计选择，如果与先前的物理知识相符，则可以得到可理解的回归结果。否则，nullspace和正则化的结合会使回归结果不可解释，而且在存在真实的下面线性模型时，无法获得回归系数与真实系数的很近的值。此外，我们还证明了不生成与nullspace垂直的系数，如焊缝lasso，可以提高解释性。因此，通过nullspace的视角，可以做出有用的设计选择，建立回归模型，理解可能的下面线性模型，这些知识对系统优化和提高科学理解非常重要。
</details></li>
</ul>
<hr>
<h2 id="Interactive-and-Concentrated-Differential-Privacy-for-Bandits"><a href="#Interactive-and-Concentrated-Differential-Privacy-for-Bandits" class="headerlink" title="Interactive and Concentrated Differential Privacy for Bandits"></a>Interactive and Concentrated Differential Privacy for Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00557">http://arxiv.org/abs/2309.00557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achraf Azize, Debabrota Basu</li>
<li>for: 本文研究了在有信任中央决策者的情况下保护用户隐私的帮助系统。</li>
<li>methods: 本文使用了交互式敏感隐私（DP）来保护用户隐私。</li>
<li>results: 本文提供了对帮助系统中的finite-armed和线性帮助器的 regret的分析，并提出了两种基于$\rho$-global zCDP的帮助器算法，AdaC-UCB和AdaC-GOPE。这两种算法都使用了 Gaussian mechanism和自适应集。<details>
<summary>Abstract</summary>
Bandits play a crucial role in interactive learning schemes and modern recommender systems. However, these systems often rely on sensitive user data, making privacy a critical concern. This paper investigates privacy in bandits with a trusted centralized decision-maker through the lens of interactive Differential Privacy (DP). While bandits under pure $\epsilon$-global DP have been well-studied, we contribute to the understanding of bandits under zero Concentrated DP (zCDP). We provide minimax and problem-dependent lower bounds on regret for finite-armed and linear bandits, which quantify the cost of $\rho$-global zCDP in these settings. These lower bounds reveal two hardness regimes based on the privacy budget $\rho$ and suggest that $\rho$-global zCDP incurs less regret than pure $\epsilon$-global DP. We propose two $\rho$-global zCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linear bandits respectively. Both algorithms use a common recipe of Gaussian mechanism and adaptive episodes. We analyze the regret of these algorithms to show that AdaC-UCB achieves the problem-dependent regret lower bound up to multiplicative constants, while AdaC-GOPE achieves the minimax regret lower bound up to poly-logarithmic factors. Finally, we provide experimental validation of our theoretical results under different settings.
</details>
<details>
<summary>摘要</summary>
匪夷在互动学习方案和现代推荐系统中扮演着关键性的角色。然而，这些系统经常依赖于敏感用户数据，因此隐私问题成为了非常重要的关注点。这篇论文通过在有信任中央决策者的情况下进行交互式不同敏感度隐私（DP）的研究，探讨了隐私问题在匪夷中的应用。在纯$\epsilon$-全球DP下，匪夷已经得到了广泛的研究，但我们在零集中敏感度DP（zCDP）下提供了有关匪夷的研究。我们给出了可靠下限和问题依赖下限，用于衡量在finite-armed和线性匪夷中的 regret，这些下限反映了隐私预算$\rho$的两种困难情况，并表明$\rho$-全球zCDP在这些设定下比纯$\epsilon$-全球DP更低的 regret。我们提出了两种$\rho$-全球zCDP匪夷算法，AdaC-UCB和AdaC-GOPE，用于finite-armed和线性匪夷。这两种算法都使用了 Gaussian mechanism和自适应集。我们分析了这些算法的 regret，并证明AdaC-UCB达到问题依赖下限，而AdaC-GOPE达到可靠下限。最后，我们对不同设定下进行了实验验证。
</details></li>
</ul>
<hr>
<h2 id="Curating-Naturally-Adversarial-Datasets-for-Trustworthy-AI-in-Healthcare"><a href="#Curating-Naturally-Adversarial-Datasets-for-Trustworthy-AI-in-Healthcare" class="headerlink" title="Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare"></a>Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00543">http://arxiv.org/abs/2309.00543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sydney Pugh, Ivan Ruchkin, Insup Lee, James Weimer</li>
<li>for: 本研究旨在提高深度学习模型在医疗时序数据预测中的鲁棒性。</li>
<li>methods: 本研究提出了一种方法，利用自动生成的弱监督标签 combines 噪音和便宜获得的标签规则，以生成自然的对抗示例集。这些对抗示例集可以用于评估深度学习模型的鲁棒性。</li>
<li>results: 本研究在六个医疗案例和三个非医疗案例中进行了评估，并得到了效果和统计有效性的结果，表明了该方法可以生成自然的对抗示例集，以评估深度学习模型的鲁棒性。<details>
<summary>Abstract</summary>
Deep learning models have shown promising predictive accuracy for time-series healthcare applications. However, ensuring the robustness of these models is vital for building trustworthy AI systems. Existing research predominantly focuses on robustness to synthetic adversarial examples, crafted by adding imperceptible perturbations to clean input data. However, these synthetic adversarial examples do not accurately reflect the most challenging real-world scenarios, especially in the context of healthcare data. Consequently, robustness to synthetic adversarial examples may not necessarily translate to robustness against naturally occurring adversarial examples, which is highly desirable for trustworthy AI. We propose a method to curate datasets comprised of natural adversarial examples to evaluate model robustness. The method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. Based on these labels, our method adversarially orders the input data and uses this ordering to construct a sequence of increasingly adversarial datasets. Our evaluation on six medical case studies and three non-medical case studies demonstrates the efficacy and statistical validity of our approach to generating naturally adversarial datasets
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose a method for curating datasets comprised of natural adversarial examples to evaluate model robustness. Our method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. We use these labels to adversarially order the input data and construct a sequence of increasingly adversarial datasets.Our evaluation on six medical case studies and three non-medical case studies demonstrates the effectiveness and statistical validity of our approach to generating naturally adversarial datasets. By using these datasets to evaluate model robustness, we can better ensure that AI systems are trustworthy and reliable in real-world applications.
</details></li>
</ul>
<hr>
<h2 id="ICDARTS-Improving-the-Stability-and-Performance-of-Cyclic-DARTS"><a href="#ICDARTS-Improving-the-Stability-and-Performance-of-Cyclic-DARTS" class="headerlink" title="ICDARTS: Improving the Stability and Performance of Cyclic DARTS"></a>ICDARTS: Improving the Stability and Performance of Cyclic DARTS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00664">http://arxiv.org/abs/2309.00664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emily Herron, Derek Rose, Steven Young</li>
<li>for: 本研究旨在提高Cycling DARTS（CDARTS）的稳定性和通用性。CDARTS是基于Diffentiable Architecture Search（DARTS）的神经网络搜索（NAS）方法，使用循环反馈机制来同时训练搜索和评估网络。</li>
<li>methods: 本研究使用了一种依赖于搜索网络的评估网络损失函数，导致搜索阶段的评估网络是一个不完全的代理。我们提出了ICDARTS，一种修改后的方法，消除了评估网络 weights 的依赖关系，并对搜索网络的抽象操作进行修改。</li>
<li>results: 我们对ICDARTS algorithm和网络模板进行了ablation study，并对ICDARTS的搜索空间进行了扩展。最终，我们实现了一种包含动态搜索空间的ICDARTS方法，并对其进行了多种扩展和修改。这些实验结果表明ICDARTS可以提高网络的通用性和普遍性。<details>
<summary>Abstract</summary>
This work introduces improvements to the stability and generalizability of Cyclic DARTS (CDARTS). CDARTS is a Differentiable Architecture Search (DARTS)-based approach to neural architecture search (NAS) that uses a cyclic feedback mechanism to train search and evaluation networks concurrently. This training protocol aims to optimize the search process by enforcing that the search and evaluation networks produce similar outputs. However, CDARTS introduces a loss function for the evaluation network that is dependent on the search network. The dissimilarity between the loss functions used by the evaluation networks during the search and retraining phases results in a search-phase evaluation network that is a sub-optimal proxy for the final evaluation network that is utilized during retraining. We present ICDARTS, a revised approach that eliminates the dependency of the evaluation network weights upon those of the search network, along with a modified process for discretizing the search network's \textit{zero} operations that allows these operations to be retained in the final evaluation networks. We pair the results of these changes with ablation studies on ICDARTS' algorithm and network template. Finally, we explore methods for expanding the search space of ICDARTS by expanding its operation set and exploring alternate methods for discretizing its continuous search cells. These experiments resulted in networks with improved generalizability and the implementation of a novel method for incorporating a dynamic search space into ICDARTS.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose ICDARTS, a revised approach that eliminates the dependency of the evaluation network weights on those of the search network. We also introduce a modified process for discretizing the search network's zero operations, which allows these operations to be retained in the final evaluation networks. To further improve the performance of ICDARTS, we conduct ablation studies on its algorithm and network template, and explore methods for expanding the search space of ICDARTS by expanding its operation set and exploring alternate methods for discretizing its continuous search cells.Our experiments show that ICDARTS achieves improved generalization ability and can be used to implement a novel method for incorporating a dynamic search space into NAS. Additionally, we explore the effectiveness of different operation sets and discretization methods for expanding the search space of ICDARTS. Our results demonstrate the potential of ICDARTS for improving the stability and generalization ability of NAS methods.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-function-approximation-based-on-the-Discrete-Cosine-Transform-DCT"><a href="#Adaptive-function-approximation-based-on-the-Discrete-Cosine-Transform-DCT" class="headerlink" title="Adaptive function approximation based on the Discrete Cosine Transform (DCT)"></a>Adaptive function approximation based on the Discrete Cosine Transform (DCT)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00530">http://arxiv.org/abs/2309.00530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana I. Pérez-Neira, Marc Martinez-Gost, Miguel Ángel Lagunas</li>
<li>for: 这个论文研究了基于偏合函数的不同频率权重的离散函数近似方法，以提高离散函数近似的精度和效率。</li>
<li>methods: 本论文使用了一种supervised学习方法来获得近似系数，而不是使用快速傅立叶变换（DCT）。这种方法利用了偏合函数的Finite dynamics和正交性，从而使得使用简单的梯度算法，如正规化最小二乘法（NLMS），可以得到控制的和预测的 convergence time和误差违偏。</li>
<li>results:  simulations表明，该方法在学习质量和复杂性之间具有最佳的平衡，并且在更复杂的超visited学习系统中可以作为一种有appeal的技术使用。<details>
<summary>Abstract</summary>
This paper studies the cosine as basis function for the approximation of univariate and continuous functions without memory. This work studies a supervised learning to obtain the approximation coefficients, instead of using the Discrete Cosine Transform (DCT). Due to the finite dynamics and orthogonality of the cosine basis functions, simple gradient algorithms, such as the Normalized Least Mean Squares (NLMS), can benefit from it and present a controlled and predictable convergence time and error misadjustment. Due to its simplicity, the proposed technique ranks as the best in terms of learning quality versus complexity, and it is presented as an attractive technique to be used in more complex supervised learning systems. Simulations illustrate the performance of the approach. This paper celebrates the 50th anniversary of the publication of the DCT by Nasir Ahmed in 1973.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-Distributed-Learning-over-Random-Networks"><a href="#Online-Distributed-Learning-over-Random-Networks" class="headerlink" title="Online Distributed Learning over Random Networks"></a>Online Distributed Learning over Random Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00520">http://arxiv.org/abs/2309.00520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Nicola Bastianello, Diego Deplano, Mauro Franceschelli, Karl H. Johansson</li>
<li>for: This paper focuses on the challenges of distributed learning in multi-agent systems, specifically addressing online learning, asynchronous agent computations, unreliable and limited communications, and inexact local computations.</li>
<li>methods: The paper introduces the Distributed Operator Theoretical (DOT) version of the Alternating Direction Method of Multipliers (ADMM), called the DOT-ADMM Algorithm, which is proven to converge with a linear rate toward a bounded neighborhood of the optimal time-varying solution.</li>
<li>results: The paper shows that the DOT-ADMM Algorithm exhibits robustness to the challenges of distributed learning, including online learning, asynchronous agent computations, unreliable and limited communications, and inexact local computations, and outperforms other state-of-the-art algorithms in numerical simulations.Here is the summary in Traditional Chinese:</li>
<li>for: 本研究关注分布式学习在多代理系统中的挑战，特别是在线上学习、代理计算和不可靠限制通信等方面。</li>
<li>methods: 本研究提出了分布式算子理论（DOT）版本的分布式方向方法（ADMM），称为DOT-ADMM算法，并证明其在线上学习和对称的情况下具有线性速率传递。</li>
<li>results: 本研究显示DOT-ADMM算法在分布式学习中具有弹性和可靠性，并在数据验证中与其他现有算法相比，表现出较好的性能。<details>
<summary>Abstract</summary>
The recent deployment of multi-agent systems in a wide range of scenarios has enabled the solution of learning problems in a distributed fashion. In this context, agents are tasked with collecting local data and then cooperatively train a model, without directly sharing the data. While distributed learning offers the advantage of preserving agents' privacy, it also poses several challenges in terms of designing and analyzing suitable algorithms. This work focuses specifically on the following challenges motivated by practical implementation: (i) online learning, where the local data change over time; (ii) asynchronous agent computations; (iii) unreliable and limited communications; and (iv) inexact local computations. To tackle these challenges, we introduce the Distributed Operator Theoretical (DOT) version of the Alternating Direction Method of Multipliers (ADMM), which we call the DOT-ADMM Algorithm. We prove that it converges with a linear rate for a large class of convex learning problems (e.g., linear and logistic regression problems) toward a bounded neighborhood of the optimal time-varying solution, and characterize how the neighborhood depends on~$\text{(i)--(iv)}$. We corroborate the theoretical analysis with numerical simulations comparing the DOT-ADMM Algorithm with other state-of-the-art algorithms, showing that only the proposed algorithm exhibits robustness to (i)--(iv).
</details>
<details>
<summary>摘要</summary>
最近在多种场景中部署了多代系统，使得学习问题得到了分布式解决。在这个上下文中，代理被任务为收集本地数据，然后合作训练模型，不直接共享数据。分布式学习具有保持代理隐私的优点，但也涉及到设计和分析适合的算法的许多挑战。这项工作专门关注以下由实践驱动的挑战：（i）在线学习，本地数据随时间变化；（ii）异步代理计算；（iii）不可靠和有限通信；以及（iv）不准确的本地计算。为解决这些挑战，我们介绍了分布式运算符理论（DOT）版本的分向方向法（ADMM），我们称之为DOT-ADMM算法。我们证明其在大量 convex 学习问题（例如线性和ilogistic回归问题）中 converge  linear 速率，并Characterize 如何这个邻域取决于（i)--(iv）。我们通过对 DOT-ADMM 算法与其他当前状态的算法进行数值仿真，并证明了它在（i)--(iv）中的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Solving-multiscale-elliptic-problems-by-sparse-radial-basis-function-neural-networks"><a href="#Solving-multiscale-elliptic-problems-by-sparse-radial-basis-function-neural-networks" class="headerlink" title="Solving multiscale elliptic problems by sparse radial basis function neural networks"></a>Solving multiscale elliptic problems by sparse radial basis function neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03107">http://arxiv.org/abs/2309.03107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwen Wang, Minxin Chen, Jingrun Chen</li>
<li>for: 解决elliptic partial differential equations（PDEs）with multiscale coefficients using machine learning</li>
<li>methods: 使用 sparse radial basis function neural network（RBFNN）方法，即deep mixed residual method的改进版本，将第二阶偏微分方程转化为第一阶系统，并使用多个RBFNN来近似未知函数。</li>
<li>results: 提出一种基于$L_2$损失函数和$\ell_1$正则项的优化算法，可以快速加速训练过程，并且在多维问题中实现数字准确和稳定性。<details>
<summary>Abstract</summary>
Machine learning has been successfully applied to various fields of scientific computing in recent years. In this work, we propose a sparse radial basis function neural network method to solve elliptic partial differential equations (PDEs) with multiscale coefficients. Inspired by the deep mixed residual method, we rewrite the second-order problem into a first-order system and employ multiple radial basis function neural networks (RBFNNs) to approximate unknown functions in the system. To aviod the overfitting due to the simplicity of RBFNN, an additional regularization is introduced in the loss function. Thus the loss function contains two parts: the $L_2$ loss for the residual of the first-order system and boundary conditions, and the $\ell_1$ regularization term for the weights of radial basis functions (RBFs). An algorithm for optimizing the specific loss function is introduced to accelerate the training process. The accuracy and effectiveness of the proposed method are demonstrated through a collection of multiscale problems with scale separation, discontinuity and multiple scales from one to three dimensions. Notably, the $\ell_1$ regularization can achieve the goal of representing the solution by fewer RBFs. As a consequence, the total number of RBFs scales like $\mathcal{O}(\varepsilon^{-n\tau})$, where $\varepsilon$ is the smallest scale, $n$ is the dimensionality, and $\tau$ is typically smaller than $1$. It is worth mentioning that the proposed method not only has the numerical convergence and thus provides a reliable numerical solution in three dimensions when a classical method is typically not affordable, but also outperforms most other available machine learning methods in terms of accuracy and robustness.
</details>
<details>
<summary>摘要</summary>
《机器学习在近年来已经成功应用于科学计算中的多个领域。在这项工作中，我们提议一种稀疏卷积基函数神经网络方法来解析带有多个级别的偏微分方程（PDEs）。受深度混合征方法的启发，我们将第二阶问题转换成了第一阶系统，并使用多个卷积基函数神经网络（RBFNNs）来近似未知函数在系统中。为了避免由简单的RBFNN所导致的过拟合，我们在损失函数中引入了额外规则。因此，损失函数包含了$L_2$损失 для系统的剩余和边界条件，以及$\ell_1$规则对卷积基函数（RBFs）的权重。我们提出了一种算法来优化特定的损失函数，以加速训练过程。我们通过一系列具有层次分离、突然变化和多个级别的多维问题来证明方法的准确和有效性。值得一提的是，$\ell_1$规则可以实现将解 representation为 fewer RBFs。因此，总的RBFs数量类似于$\mathcal{O}(\varepsilon^{-n\tau})$, где$\varepsilon$是最小的尺度，$n$是维度，而$\tau$通常小于1。这表明提出的方法不仅具有数值稳定性，可以在三维中提供可靠的数字解决方案，而且在精度和稳定性方面也超越了大多数现有的机器学习方法。
</details></li>
</ul>
<hr>
<h2 id="Structure-and-Gradient-Dynamics-Near-Global-Minima-of-Two-layer-Neural-Networks"><a href="#Structure-and-Gradient-Dynamics-Near-Global-Minima-of-Two-layer-Neural-Networks" class="headerlink" title="Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks"></a>Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00508">http://arxiv.org/abs/2309.00508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leyang Zhang, Yaoyu Zhang, Tao Luo</li>
<li>for:  investigate the structure of loss landscape of two-layer neural networks near global minima</li>
<li>methods: determine the set of parameters which give perfect generalization, and fully characterize the gradient flows around it</li>
<li>results: uncover some simple aspects of the complicated loss landscape and reveal how model, target function, samples and initialization affect the training dynamics differently, and explain why (overparametrized) neural networks could generalize well.Here’s the Chinese text:</li>
<li>for: 研究两层神经网络的损失地形结构附近全局最优点</li>
<li>methods: 确定参数集合，实现完美泛化，并全面描述梯度流动的特性</li>
<li>results: 揭示损失地形的一些简单特征，透视模型、目标函数、样本和初始化如何不同程度影响训练动态，并解释为什么过参数神经网络可以良好泛化。<details>
<summary>Abstract</summary>
Under mild assumptions, we investigate the structure of loss landscape of two-layer neural networks near global minima, determine the set of parameters which give perfect generalization, and fully characterize the gradient flows around it. With novel techniques, our work uncovers some simple aspects of the complicated loss landscape and reveals how model, target function, samples and initialization affect the training dynamics differently. Based on these results, we also explain why (overparametrized) neural networks could generalize well.
</details>
<details>
<summary>摘要</summary>
beneath mild assumptions, we investigate the structure of loss landscape of two-layer neural networks near global minima, determine the set of parameters which give perfect generalization, and fully characterize the gradient flows around it. With novel techniques, our work uncovers some simple aspects of the complicated loss landscape and reveals how model, target function, samples and initialization affect the training dynamics differently. Based on these results, we also explain why (overparametrized) neural networks could generalize well.Here's the breakdown of the translation:* "Under mild assumptions" is translated as "beneath mild assumptions" (假设) in Simplified Chinese.* "we investigate the structure of loss landscape" is translated as "我们 investigate the structure of loss landscape" (我们调查损失景观的结构) in Simplified Chinese.* "near global minima" is translated as "near global minima" (近global minimum) in Simplified Chinese.* "determine the set of parameters which give perfect generalization" is translated as " determine the set of parameters that give perfect generalization" (决定对于完美泛化的参数集) in Simplified Chinese.* "and fully characterize the gradient flows around it" is translated as "并将梯度流的完整特征化" (并将梯度流的完整特征化) in Simplified Chinese.* "With novel techniques" is translated as "使用新的技术" (使用新的技术) in Simplified Chinese.* "our work uncovers some simple aspects of the complicated loss landscape" is translated as "我们的工作揭露了一些复杂损失景观的简单方面" (我们的工作揭露了一些复杂损失景观的简单方面) in Simplified Chinese.* "and reveals how model, target function, samples and initialization affect the training dynamics differently" is translated as "并显示模型、目标函数、样本和初始化如何不同地影响训练动态" (并显示模型、目标函数、样本和初始化如何不同地影响训练动态) in Simplified Chinese.* "Based on these results, we also explain why (overparametrized) neural networks could generalize well" is translated as "根据这些结果，我们也解释了为何（过参数）神经网络可以广泛适用" (根据这些结果，我们也解释了为何（过参数）神经网络可以广泛适用) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Application-of-Deep-Learning-Methods-in-Monitoring-and-Optimization-of-Electric-Power-Systems"><a href="#Application-of-Deep-Learning-Methods-in-Monitoring-and-Optimization-of-Electric-Power-Systems" class="headerlink" title="Application of Deep Learning Methods in Monitoring and Optimization of Electric Power Systems"></a>Application of Deep Learning Methods in Monitoring and Optimization of Electric Power Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00498">http://arxiv.org/abs/2309.00498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ognjen Kundacina</li>
<li>for: 这个博士论文的主要目的是利用深度学习技术提高电力系统监测和优化算法。</li>
<li>methods: 本论文使用图 neural network 进行电力系统状态估计的提升，以及利用强化学习来动态分布网络重新配置。</li>
<li>results: 经过广泛的实验和 simulations，提出的方法得到了证明。<details>
<summary>Abstract</summary>
This PhD thesis thoroughly examines the utilization of deep learning techniques as a means to advance the algorithms employed in the monitoring and optimization of electric power systems. The first major contribution of this thesis involves the application of graph neural networks to enhance power system state estimation. The second key aspect of this thesis focuses on utilizing reinforcement learning for dynamic distribution network reconfiguration. The effectiveness of the proposed methods is affirmed through extensive experimentation and simulations.
</details>
<details>
<summary>摘要</summary>
这个博士论文全面检查了深度学习技术的应用，以提高电力系统监控和优化算法。本论文的第一个主要贡献是通过图 neural network 提高电力系统状态估计。第二个关键方面是通过强化学习来实现动态电力分布网络重新配置。实验和 simulations 证明了提议的方法的有效性。Here's a breakdown of the translation:* 这个博士论文 (zhè ge bóshì zhòngwén) - This PhD thesis* 全面检查 (quánxiàn jiǎnchè) - thoroughly examines* 深度学习技术 (shēngrán xuéxí jìshù) - deep learning techniques* 应用 (yìngyòu) - as a means to* 电力系统 (diànliàng xìtǒng) - electric power systems* 监控 (jīnjiāng) - monitoring* 优化 (yòujiā) - optimization* 算法 (suànfǎ) - algorithms* graph neural networks (gōngshì xīnxiàng) - graph neural networks* 提高 (tīgāng) - to enhance* 电力系统状态估计 (diànliàng xìtǒng zhuàngjì) - power system state estimation* 动态 (dòngtǐ) - dynamic* 电力分布网络 (diànliàng fēnzhòu wǎngluò) - distribution network* 重新配置 (zhòngxīn bèngqǐ) - reconfiguration* 实验 (shíyàn) - experiments*  simulations (shìyàn) - simulations* 证明 (shèngmíng) - prove* 有效性 (yǒuxiàngxìng) - effectiveness
</details></li>
</ul>
<hr>
<h2 id="Multi-stage-Deep-Learning-Artifact-Reduction-for-Computed-Tomography"><a href="#Multi-stage-Deep-Learning-Artifact-Reduction-for-Computed-Tomography" class="headerlink" title="Multi-stage Deep Learning Artifact Reduction for Computed Tomography"></a>Multi-stage Deep Learning Artifact Reduction for Computed Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00494">http://arxiv.org/abs/2309.00494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayang Shi, Daniel M. Pelt, K. Joost Batenburg<br>for: 这篇论文的目的是提高计算tomography（CT）图像的内部结构图像质量，以便更准确地分析。methods: 这篇论文使用了多个遍历阶段的深度学习方法，包括在不同的图像领域（如投影图像和重建图像）中进行噪声除除和异常点除除。results: 实验表明，这种多阶段深度学习方法可以有效地除除CT图像中的噪声和异常点，并且比传统的深度学习后处理方法更高效。<details>
<summary>Abstract</summary>
In Computed Tomography (CT), an image of the interior structure of an object is computed from a set of acquired projection images. The quality of these reconstructed images is essential for accurate analysis, but this quality can be degraded by a variety of imaging artifacts. To improve reconstruction quality, the acquired projection images are often processed by a pipeline consisting of multiple artifact-removal steps applied in various image domains (e.g., outlier removal on projection images and denoising of reconstruction images). These artifact-removal methods exploit the fact that certain artifacts are easier to remove in a certain domain compared with other domains.   Recently, deep learning methods have shown promising results for artifact removal for CT images. However, most existing deep learning methods for CT are applied as a post-processing method after reconstruction. Therefore, artifacts that are relatively difficult to remove in the reconstruction domain may not be effectively removed by these methods. As an alternative, we propose a multi-stage deep learning method for artifact removal, in which neural networks are applied to several domains, similar to a classical CT processing pipeline. We show that the neural networks can be effectively trained in succession, resulting in easy-to-use and computationally efficient training. Experiments on both simulated and real-world experimental datasets show that our method is effective in reducing artifacts and superior to deep learning-based post-processing.
</details>
<details>
<summary>摘要</summary>
在计算 Tomography（CT）中，通过一组获取的投影图像来计算对象的内部结构图像。这种重建图像的质量是至关重要的，但这可能受到多种扫描 artifact 的影响。为了提高重建质量，通常会在多个域中应用多个 artifact 除掉步骤（如投影图像上的异常点除掉和重建图像上的噪声除掉）。这些 artifact 除掉方法利用了某些 artifact 在某个域中更容易除掉的特点。最近，深度学习方法在 CT 图像中的 artifact 除掉表现了扎实的成果。然而，大多数现有的深度学习方法都是在重建后进行 post-processing 的方式进行应用。因此，可能存在一些难以在重建域中除掉的 artifact。为了解决这问题，我们提出了一种多stage deep learning 方法，在这种方法中，神经网络在不同的域中应用。我们发现，这些神经网络可以在Successive 中进行有效的训练，从而实现了容易使用和计算效率高的训练。在模拟和实际实验数据上，我们的方法可以有效地减少 artifact，并且比深度学习基于 post-processing 的方法更高效。
</details></li>
</ul>
<hr>
<h2 id="How-Does-Forecasting-Affect-the-Convergence-of-DRL-Techniques-in-O-RAN-Slicing"><a href="#How-Does-Forecasting-Affect-the-Convergence-of-DRL-Techniques-in-O-RAN-Slicing" class="headerlink" title="How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN Slicing?"></a>How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN Slicing?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00489">http://arxiv.org/abs/2309.00489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad M. Nagib, Hatem Abou-Zeid, Hossam S. Hassanein</li>
<li>for: 本研究旨在提高基于深度强化学习（DRL）的Radio Access Network（RAN）割込法的稳定性和效率，以提供更好的虚拟现实游戏和metaverse服务体验。</li>
<li>methods: 本研究使用了时间序列预测来改善DRLAgent的快速启动和网络条件变化时的性能稳定性。</li>
<li>results: 对于多种服务，包括真实的VR游戏流量，我们进行了详细的实验，并提出了一种使用预测来助长DRLAgent的快速启动和稳定性的方法。我们的方法可以提高DRLAgent的初始奖励值平均提高22.8%、快速度提高86.3%和已经 converged场景数提高300%，提高DRLAgent的通用性。此外，我们的方法还能够抗性 Forecasting errors和不需要完美的预测模型。<details>
<summary>Abstract</summary>
The success of immersive applications such as virtual reality (VR) gaming and metaverse services depends on low latency and reliable connectivity. To provide seamless user experiences, the open radio access network (O-RAN) architecture and 6G networks are expected to play a crucial role. RAN slicing, a critical component of the O-RAN paradigm, enables network resources to be allocated based on the needs of immersive services, creating multiple virtual networks on a single physical infrastructure. In the O-RAN literature, deep reinforcement learning (DRL) algorithms are commonly used to optimize resource allocation. However, the practical adoption of DRL in live deployments has been sluggish. This is primarily due to the slow convergence and performance instabilities suffered by the DRL agents both upon initial deployment and when there are significant changes in network conditions. In this paper, we investigate the impact of time series forecasting of traffic demands on the convergence of the DRL-based slicing agents. For that, we conduct an exhaustive experiment that supports multiple services including real VR gaming traffic. We then propose a novel forecasting-aided DRL approach and its respective O-RAN practical deployment workflow to enhance DRL convergence. Our approach shows up to 22.8%, 86.3%, and 300% improvements in the average initial reward value, convergence rate, and number of converged scenarios respectively, enhancing the generalizability of the DRL agents compared with the implemented baselines. The results also indicate that our approach is robust against forecasting errors and that forecasting models do not have to be ideal.
</details>
<details>
<summary>摘要</summary>
成功的卷入应用，如虚拟现实（VR）游戏和metaverse服务，需要低延迟和可靠的连接。为提供无缝用户体验，开放无线接入网络（O-RAN）架构和6G网络被期望会扮演关键角色。RAN排序，O-RAN架构中的关键组件，允许网络资源根据卷入服务的需求进行分配，创建多个虚拟网络在单一物理基础设施之上。在O-RAN文献中，深度强化学习（DRL）算法广泛用于资源分配优化。然而，实际应用中DRL的扩散和性能不稳定，主要是由DRL代理人在初始部署和网络条件变化时的慢速收敛和性能不稳定。在这篇论文中，我们研究了基于时间序列预测的卷入服务需求对DRL排序代理人的 converges 的影响。为此，我们进行了全面的实验，支持多种服务，包括真实的VR游戏流量。然后，我们提出了一种新的预测帮助DRL方法和其相应的O-RAN实践工作流程，以提高DRL的 converges。我们的方法显示在初始奖励值、收敛率和可 converges 的enario中具有22.8%、86.3%和300%的提升，提高了DRL代理人的总体可靠性。结果还表明，我们的方法具有预测错误的Robustness，预测模型不需要是理想的。
</details></li>
</ul>
<hr>
<h2 id="Geometry-aware-Line-Graph-Transformer-Pre-training-for-Molecular-Property-Prediction"><a href="#Geometry-aware-Line-Graph-Transformer-Pre-training-for-Molecular-Property-Prediction" class="headerlink" title="Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction"></a>Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00483">http://arxiv.org/abs/2309.00483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peizhen Bai, Xianyuan Liu, Haiping Lu</li>
<li>for: 提高分子表示学习的精度，使用自动生成的标签数据。</li>
<li>methods: 提出了一种基于线图 transformer 的自适应学习框架，并在两种模式之间进行了权重学习。</li>
<li>results: 在十二个性能评估标准上，Galformer Consistently outperforms 六个基eline的标准模型，证明其效果。<details>
<summary>Abstract</summary>
Molecular property prediction with deep learning has gained much attention over the past years. Owing to the scarcity of labeled molecules, there has been growing interest in self-supervised learning methods that learn generalizable molecular representations from unlabeled data. Molecules are typically treated as 2D topological graphs in modeling, but it has been discovered that their 3D geometry is of great importance in determining molecular functionalities. In this paper, we propose the Geometry-aware line graph transformer (Galformer) pre-training, a novel self-supervised learning framework that aims to enhance molecular representation learning with 2D and 3D modalities. Specifically, we first design a dual-modality line graph transformer backbone to encode the topological and geometric information of a molecule. The designed backbone incorporates effective structural encodings to capture graph structures from both modalities. Then we devise two complementary pre-training tasks at the inter and intra-modality levels. These tasks provide properly supervised information and extract discriminative 2D and 3D knowledge from unlabeled molecules. Finally, we evaluate Galformer against six state-of-the-art baselines on twelve property prediction benchmarks via downstream fine-tuning. Experimental results show that Galformer consistently outperforms all baselines on both classification and regression tasks, demonstrating its effectiveness.
</details>
<details>
<summary>摘要</summary>
“分子性预测使用深度学习在过去几年内获得了很多关注。由于分子标注数据的缺乏，自我超级学习方法在没有标注数据的情况下学习通用的分子表示变得越来越受欢迎。在这篇论文中，我们提议一种具有自我超级学习框架，即Geometry-aware line graph transformer（Galformer）预训练方法，以提高分子表示学习的精度。特别是，我们首先设计了一种双模态线图变换后缘，用于编码分子的 topological和geometry信息。这种设计包括有效的结构编码，以捕捉两种模式的图结构。然后，我们设计了两个补偿性预训练任务，分别在间模态和内模态水平进行。这两个任务提供了适当的监督信息，并提取分子中不同模式的特征知识。最后，我们通过下游细化 fine-tuning 评估 Galformer 对六个基eline的比较。实验结果显示，Galformer 在分类和回归任务上一直高于所有基eline，证明其效果。”
</details></li>
</ul>
<hr>
<h2 id="Polynomial-Model-Based-Optimization-for-Blackbox-Objectives"><a href="#Polynomial-Model-Based-Optimization-for-Blackbox-Objectives" class="headerlink" title="Polynomial-Model-Based Optimization for Blackbox Objectives"></a>Polynomial-Model-Based Optimization for Blackbox Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00663">http://arxiv.org/abs/2309.00663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Janina Schreiber, Damar Wicaksono, Michael Hecht<br>for: 这个研究旨在找到黑盒式优化中的优化参数，以找到一个预先定义的目标函数的最小值。methods: 这个研究使用了波浪函数基本的搜寻法（PMBO），将目标函数当做一个多项式模型，并逐步更新这个模型，根据探索性和利用性的平衡，以及模型的不确定性。results: 这个研究在一些人工、数学函数上进行了比较，结果显示PMBO能够成功地与其他现有的数学函数相比，甚至在一些情况下超越它们。因此，我们认为PMBO是黑盒式优化任务中的一个重要选择。<details>
<summary>Abstract</summary>
For a wide range of applications the structure of systems like Neural Networks or complex simulations, is unknown and approximation is costly or even impossible. Black-box optimization seeks to find optimal (hyper-) parameters for these systems such that a pre-defined objective function is minimized. Polynomial-Model-Based Optimization (PMBO) is a novel blackbox optimizer that finds the minimum by fitting a polynomial surrogate to the objective function.   Motivated by Bayesian optimization the model is iteratively updated according to the acquisition function Expected Improvement, thus balancing the exploitation and exploration rate and providing an uncertainty estimate of the model. PMBO is benchmarked against other state-of-the-art algorithms for a given set of artificial, analytical functions. PMBO competes successfully with those algorithms and even outperforms all of them in some cases. As the results suggest, we believe PMBO is the pivotal choice for solving blackbox optimization tasks occurring in a wide range of disciplines.
</details>
<details>
<summary>摘要</summary>
For a wide range of applications, the structure of systems like Neural Networks or complex simulations is unknown, and approximation is costly or even impossible. Black-box optimization seeks to find optimal (hyper-)parameters for these systems such that a pre-defined objective function is minimized. Polynomial-Model-Based Optimization (PMBO) is a novel blackbox optimizer that finds the minimum by fitting a polynomial surrogate to the objective function. Motivated by Bayesian optimization, the model is iteratively updated according to the acquisition function Expected Improvement, thus balancing the exploitation and exploration rate and providing an uncertainty estimate of the model. PMBO is benchmarked against other state-of-the-art algorithms for a given set of artificial, analytical functions. PMBO competes successfully with those algorithms and even outperforms all of them in some cases. As the results suggest, we believe PMBO is the pivotal choice for solving blackbox optimization tasks occurring in a wide range of disciplines.Note: Please note that the translation is in Simplified Chinese, which is the more commonly used variety of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="New-metrics-for-analyzing-continual-learners"><a href="#New-metrics-for-analyzing-continual-learners" class="headerlink" title="New metrics for analyzing continual learners"></a>New metrics for analyzing continual learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00462">http://arxiv.org/abs/2309.00462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Michel, Giovanni Chierchia, Romain Negrel, Jean-François Bercher, Toshihiko Yamasaki</li>
<li>for: 这篇论文旨在探讨对于流动数据的连续学习（Continual Learning，CL）中，模型如何维持以前任务的知识而同时学习新任务。</li>
<li>methods: 本文使用了深度神经网络，并分析了现有的指标之间的限制，并提出了一些新的指标来评估CL模型的稳定性和柔软性。</li>
<li>results: 经过实验表明，提出的新指标可以为CL领域提供新的见解，并且可以评估CL模型在不同任务难度下的表现。<details>
<summary>Abstract</summary>
Deep neural networks have shown remarkable performance when trained on independent and identically distributed data from a fixed set of classes. However, in real-world scenarios, it can be desirable to train models on a continuous stream of data where multiple classification tasks are presented sequentially. This scenario, known as Continual Learning (CL) poses challenges to standard learning algorithms which struggle to maintain knowledge of old tasks while learning new ones. This stability-plasticity dilemma remains central to CL and multiple metrics have been proposed to adequately measure stability and plasticity separately. However, none considers the increasing difficulty of the classification task, which inherently results in performance loss for any model. In that sense, we analyze some limitations of current metrics and identify the presence of setup-induced forgetting. Therefore, we propose new metrics that account for the task's increasing difficulty. Through experiments on benchmark datasets, we demonstrate that our proposed metrics can provide new insights into the stability-plasticity trade-off achieved by models in the continual learning environment.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文，不同于正式中文）深度神经网络在独立且相同分布的数据上训练时表现出非常出色，但在实际场景中，可能需要在连续流动的数据上训练模型，并在多个分类任务之间进行顺序执行。这种情况被称为 continual learning（CL），它对标准学习算法提出了挑战，因为这些算法很难保持老任务的知识 while learning new ones。这个稳定-пластично性的 contradiction remains central to CL, 并且 multiple metrics have been proposed to measure stability and plasticity separately. However, none of these metrics takes into account the increasing difficulty of the classification task, which inherently results in performance loss for any model. In this sense, we analyze some limitations of current metrics and identify the presence of setup-induced forgetting. Therefore, we propose new metrics that account for the task's increasing difficulty. Through experiments on benchmark datasets, we demonstrate that our proposed metrics can provide new insights into the stability-plasticity trade-off achieved by models in the continual learning environment.
</details></li>
</ul>
<hr>
<h2 id="Establishing-Markov-Equivalence-in-Cyclic-Directed-Graphs"><a href="#Establishing-Markov-Equivalence-in-Cyclic-Directed-Graphs" class="headerlink" title="Establishing Markov Equivalence in Cyclic Directed Graphs"></a>Establishing Markov Equivalence in Cyclic Directed Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03092">http://arxiv.org/abs/2309.03092</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tomc-ghub/CET_uai2023">https://github.com/tomc-ghub/CET_uai2023</a></li>
<li>paper_authors: Tom Claassen, Joris M. Mooij</li>
<li>for: Establishing Markov equivalence between directed graphs with cyclic structures.</li>
<li>methods: Based on the Cyclic Equivalence Theorem (CET) from Thomas Richardson’s seminal works, but rephrased from an ancestral perspective.</li>
<li>results: A procedure for establishing Markov equivalence between graphs that no longer requires tests for d-separation, leading to reduced algorithmic complexity and a simplified characterization that may help reinvigorate theoretical research in cyclic discovery.<details>
<summary>Abstract</summary>
We present a new, efficient procedure to establish Markov equivalence between directed graphs that may or may not contain cycles under the \textit{d}-separation criterion. It is based on the Cyclic Equivalence Theorem (CET) in the seminal works on cyclic models by Thomas Richardson in the mid '90s, but now rephrased from an ancestral perspective. The resulting characterization leads to a procedure for establishing Markov equivalence between graphs that no longer requires tests for d-separation, leading to a significantly reduced algorithmic complexity. The conceptually simplified characterization may help to reinvigorate theoretical research towards sound and complete cyclic discovery in the presence of latent confounders. This version includes a correction to rule (iv) in Theorem 1, and the subsequent adjustment in part 2 of Algorithm 2.
</details>
<details>
<summary>摘要</summary>
我团队提出了一种新、高效的方法，用于在导向图中确定Markov等价关系，该图可能或可能不含环。该方法基于Thomas Richardson在90年代中期的著名作品中的征识环型模型（CET），但现在从祖先视角重新表述。该结果导致了一种不需要测试d-分离的方法来确定Markov等价关系，从而大幅降低了算法复杂性。这种简化后的定义可能会重新激发理论研究，以探索在潜在干扰者存在下的征识环的正确方法。本版本包括对Rule（iv）的更正，以及后续在Algorithm 2中的调整。
</details></li>
</ul>
<hr>
<h2 id="No-Train-Still-Gain-Unleash-Mathematical-Reasoning-of-Large-Language-Models-with-Monte-Carlo-Tree-Search-Guided-by-Energy-Function"><a href="#No-Train-Still-Gain-Unleash-Mathematical-Reasoning-of-Large-Language-Models-with-Monte-Carlo-Tree-Search-Guided-by-Energy-Function" class="headerlink" title="No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function"></a>No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03224">http://arxiv.org/abs/2309.03224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Xu</li>
<li>for: 提高微调LLM的数学逻辑能力，不需要额外 Fine-tuning或人工反馈对适应。</li>
<li>methods:  integrate Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning.</li>
<li>results:  through extensive experiments on two mathematical reasoning benchmarks, significantly improve the pass@1 metric of the fine-tuned model without requiring additional fine-tuning or reinforcement learning with human feedback alignment.<details>
<summary>Abstract</summary>
Large language models (LLMs) demonstrate impressive language understanding and contextual learning abilities, making them suitable for natural language processing (NLP) tasks and complex mathematical reasoning. However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions. To overcome this limitation and enhance the mathematical reasoning capabilities of fine-tuned LLMs without additional fine-tuning steps, we propose a method that incorporates Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning. Specifically, we re-formulate the fine-tuned LLMs into a Residual-based Energy Model (Residual-EBM) and employ noise contrastive estimation to estimate the energy function's parameters. We then utilize MCTS with the energy function as a path verifier to search the output space and evaluate the reasoning path. Through extensive experiments on two mathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the exceptional capabilities of our method, which significantly improves the pass@1 metric of the fine-tuned model without requiring additional fine-tuning or reinforcement learning with human feedback alignment.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在语言理解和语言上下文学习能力方面表现出色，使其适用于自然语言处理（NLP）任务和复杂的数学逻辑推理。然而，当应用于数学逻辑任务时，LLM通常具有高概率但不准确的解决步骤和答案。为了超越这一限制并增强精细化LLM的数学逻辑能力，我们提出了一种方法，该方法利用蒙地卡罗瑞搜索（MCTS）和一种轻量级能量函数来排序决策步骤并允许立即反应和精准的理解。具体来说，我们将精细化LLM转换成差分基的能量模型（Residual-EBM），并使用雷达对偶估计来估算能量函数的参数。然后，我们使用MCTS和能量函数作为路径验证器来搜索输出空间并评估逻辑路径。经过广泛的实验，我们发现我们的方法可以在GSM8k和AQUA-RAT两个数学逻辑benchmark上显著提高精细化LLM的输出率，而无需进行额外的 fine-tuning 或人工反馈学习对齐。
</details></li>
</ul>
<hr>
<h2 id="A-Locality-based-Neural-Solver-for-Optical-Motion-Capture"><a href="#A-Locality-based-Neural-Solver-for-Optical-Motion-Capture" class="headerlink" title="A Locality-based Neural Solver for Optical Motion Capture"></a>A Locality-based Neural Solver for Optical Motion Capture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00428">http://arxiv.org/abs/2309.00428</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/non-void/localmocap">https://github.com/non-void/localmocap</a></li>
<li>paper_authors: Xiaoyu Pan, Bowen Zheng, Xinwei Jiang, Guanglong Xu, Xianli Gu, Jingxiang Li, Qilong Kou, He Wang, Tianjia Shao, Kun Zhou, Xiaogang Jin</li>
<li>for: cleaning and solving optical motion capture data</li>
<li>methods: 使用本地特征提取方法和图 convolution 操作来提高 marker 和关节的本地特征，并利用 occlusion 和 tracking error 的概念来快速填充缺失的 marker</li>
<li>results: 提高了多个维度的准确率，包括 occluded marker 位置错误率和重建关节旋转和位置错误率，比对 estado-of-the-art 方法提高约 20% 和 30% 。Here’s the breakdown of each point:1. for: The paper is written to improve the cleaning and solving of optical motion capture data, which is often noisy and contains occluded markers.2. methods: The proposed method uses a novel heterogeneous graph neural network that treats markers and joints as different types of nodes, and uses graph convolution operations to extract local features of markers and joints. Additionally, the method utilizes the concept of locality to efficiently fill missing markers and identify marker outliers due to tracking errors.3. results: The proposed method achieves high accuracy on multiple metrics across various datasets, including a 20% improvement in occluded marker position error and a 30% improvement in reconstructed joint rotations and positions compared to state-of-the-art methods.<details>
<summary>Abstract</summary>
We present a novel locality-based learning method for cleaning and solving optical motion capture data. Given noisy marker data, we propose a new heterogeneous graph neural network which treats markers and joints as different types of nodes, and uses graph convolution operations to extract the local features of markers and joints and transform them to clean motions. To deal with anomaly markers (e.g. occluded or with big tracking errors), the key insight is that a marker's motion shows strong correlations with the motions of its immediate neighboring markers but less so with other markers, a.k.a. locality, which enables us to efficiently fill missing markers (e.g. due to occlusion). Additionally, we also identify marker outliers due to tracking errors by investigating their acceleration profiles. Finally, we propose a training regime based on representation learning and data augmentation, by training the model on data with masking. The masking schemes aim to mimic the occluded and noisy markers often observed in the real data. Finally, we show that our method achieves high accuracy on multiple metrics across various datasets. Extensive comparison shows our method outperforms state-of-the-art methods in terms of prediction accuracy of occluded marker position error by approximately 20%, which leads to a further error reduction on the reconstructed joint rotations and positions by 30%. The code and data for this paper are available at https://github.com/non-void/LocalMoCap.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的地域性学习方法，用于清洁和解决光学运动Capture数据。给定含有噪声的标记数据，我们提议一种新的不同类型节点的异合 graphs neural network，其中标记和关节 treated as different types of nodes，并使用图 convolution 操作来提取标记和关节的本地特征，并将它们转化为清洁动作。为了处理异常标记（例如受阻或大跟踪错误），我们的关键发现是，标记的运动具有强相关性与其邻近的标记的运动，而与其他标记相比较弱相关性，这使得我们可以高效地填充缺失的标记（例如由阻挡所致）。此外，我们还可以识别标记异常（例如跟踪错误），通过检查它们的加速度轨迹。最后，我们提出了一种基于表示学习和数据扩展的训练方法，通过在数据中使用掩蔽来训练模型。掩蔽方案的目的是模拟实际数据中的受阻和噪声标记。我们的方法可以在多个数据集上达到高精度，与现有方法相比，我们的方法可以预测 occluded 标记位置错误的约20%更高精度，这导致了再次的错误减少在重建的关节旋转和位置上约30%。代码和数据可以在 <https://github.com/non-void/LocalMoCap> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Declarative-Reasoning-on-Explanations-Using-Constraint-Logic-Programming"><a href="#Declarative-Reasoning-on-Explanations-Using-Constraint-Logic-Programming" class="headerlink" title="Declarative Reasoning on Explanations Using Constraint Logic Programming"></a>Declarative Reasoning on Explanations Using Constraint Logic Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00422">http://arxiv.org/abs/2309.00422</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lstate/reasonx">https://github.com/lstate/reasonx</a></li>
<li>paper_authors: Laura State, Salvatore Ruggieri, Franco Turini</li>
<li>for: 这篇论文目的是解释 opacity 的机器学习（ML）模型。</li>
<li>methods: 该论文提出了一种基于 Constraint Logic Programming（CLP）的解释方法，可以为决策树（DT）提供声明性、交互式的解释。</li>
<li>results: 论文介绍了 REASONX 的架构，其包括 Python 层和 CLP 层，并可以使用 linear 约束和 MILP 优化来表达背景知识和对比实例的特征。<details>
<summary>Abstract</summary>
Explaining opaque Machine Learning (ML) models is an increasingly relevant problem. Current explanation in AI (XAI) methods suffer several shortcomings, among others an insufficient incorporation of background knowledge, and a lack of abstraction and interactivity with the user. We propose REASONX, an explanation method based on Constraint Logic Programming (CLP). REASONX can provide declarative, interactive explanations for decision trees, which can be the ML models under analysis or global/local surrogate models of any black-box model. Users can express background or common sense knowledge using linear constraints and MILP optimization over features of factual and contrastive instances, and interact with the answer constraints at different levels of abstraction through constraint projection. We present here the architecture of REASONX, which consists of a Python layer, closer to the user, and a CLP layer. REASONX's core execution engine is a Prolog meta-program with declarative semantics in terms of logic theories.
</details>
<details>
<summary>摘要</summary>
Explaining 透明机器学习（ML）模型是一个越来越重要的问题。当前的人工智能（AI）解释（XAI）方法具有多种缺点，其中之一是缺乏背景知识的充分 incorporation，以及缺乏抽象和用户交互的能力。我们提出了 REASONX，一种基于 Constraint Logic Programming（CLP）的解释方法。REASONX可以为决策树提供声明性、交互式的解释，这些决策树可以是ML模型的分析对象，或者是黑obox模型的全球/地方代理模型。用户可以使用线性约束和MILP优化来表达背景知识和对比实例的特征，并通过约束投影进行不同层次的交互。我们现在介绍REASONX的架构，它由Python层和CLP层组成。REASONX的核心执行引擎是一个基于Prolog的meta-程序，其语义是基于逻辑理论的声明 semantics。
</details></li>
</ul>
<hr>
<h2 id="Area-norm-COBRA-on-Conditional-Survival-Prediction"><a href="#Area-norm-COBRA-on-Conditional-Survival-Prediction" class="headerlink" title="Area-norm COBRA on Conditional Survival Prediction"></a>Area-norm COBRA on Conditional Survival Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00417">http://arxiv.org/abs/2309.00417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Goswami, Arabin Kr. Dey</li>
<li>for: 本文提出了一种不同的combined regression策略来计算条件生存函数。</li>
<li>methods: 本文使用回归基于弱学习器创建提议的ensemble技术。提议的combined regression策略使用距离度量作为两个生存曲线之间的区域。</li>
<li>results: 本文提出了一种新的技巧来选择combined regression中最重要的变量。我们通过 simulate研究表明该方法在找到变量相关性方面很有效。以及使用三个实际数据集来说明模型。<details>
<summary>Abstract</summary>
The paper explores a different variation of combined regression strategy to calculate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The proposed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simulation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model.
</details>
<details>
<summary>摘要</summary>
文章探讨了一种不同的 combinatorial 回归策略，用于计算 conditional survival function。我们使用回归基于弱学习器的 ensemble 技术。我们提出的 combinatorial 回归策略使用 proximity measure 作为两个生存曲线之间的区域。我们的模型可以保证性能比随机生存森林更好。文章介绍了一种新的技术，用于在 combinatorial 回归设置中选择最重要的变量。我们进行了一项 simulate 研究，以证明我们的提案可以很好地确定变量的相关性。我们还使用了三个实际数据集，以 illustrate 我们的模型。Here's a word-for-word translation of the text in Traditional Chinese:文章探讨了一种不同的 combinatorial 回归策略，用于计算 conditional survival function。我们使用回归基于弱学习器的 ensemble 技术。我们提出的 combinatorial 回归策略使用 proximity measure 作为两个生存曲线之间的区域。我们的模型可以保证性能比随机生存森林更好。文章介绍了一种新的技术，用于在 combinatorial 回归设置中选择最重要的变量。我们进行了一项 simulate 研究，以证明我们的提案可以很好地确定变量的相关性。我们还使用了三个实际数据集，以 illustrate 我们的模型。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Personalized-Federated-Learning-Group-Privacy-Fairness-and-Beyond"><a href="#Advancing-Personalized-Federated-Learning-Group-Privacy-Fairness-and-Beyond" class="headerlink" title="Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond"></a>Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00416">http://arxiv.org/abs/2309.00416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filippo Galli, Kangsoo Jung, Sayan Biswas, Catuscia Palamidessi, Tommaso Cucinotta</li>
<li>for: 本研究旨在提出一种基于联合学习（Federated Learning，FL）框架的个性化机器学习模型训练方法，以保证模型的准确性和公平性。</li>
<li>methods: 本研究使用了分布式机器学习（Distributed Machine Learning）和形式化隐私保证（Formal Privacy Guarantees）等技术，并提出了一种基于$d$-隐私（Metric Privacy）的方法，以保证客户端数据的隐私和公平性。</li>
<li>results: 本研究的实验结果表明，基于$d$-隐私的方法可以在联合学习框架下提供更好的群体公平性，并且在不同的客户端数据上保持模型的准确性。<details>
<summary>Abstract</summary>
Federated learning (FL) is a framework for training machine learning models in a distributed and collaborative manner. During training, a set of participating clients process their data stored locally, sharing only the model updates obtained by minimizing a cost function over their local inputs. FL was proposed as a stepping-stone towards privacy-preserving machine learning, but it has been shown vulnerable to issues such as leakage of private information, lack of personalization of the model, and the possibility of having a trained model that is fairer to some groups than to others. In this paper, we address the triadic interaction among personalization, privacy guarantees, and fairness attained by models trained within the FL framework. Differential privacy and its variants have been studied and applied as cutting-edge standards for providing formal privacy guarantees. However, clients in FL often hold very diverse datasets representing heterogeneous communities, making it important to protect their sensitive information while still ensuring that the trained model upholds the aspect of fairness for the users. To attain this objective, a method is put forth that introduces group privacy assurances through the utilization of $d$-privacy (aka metric privacy). $d$-privacy represents a localized form of differential privacy that relies on a metric-oriented obfuscation approach to maintain the original data's topological distribution. This method, besides enabling personalized model training in a federated approach and providing formal privacy guarantees, possesses significantly better group fairness measured under a variety of standard metrics than a global model trained within a classical FL template. Theoretical justifications for the applicability are provided, as well as experimental validation on real-world datasets to illustrate the working of the proposed method.
</details>
<details>
<summary>摘要</summary>
受抗联学习（FL）框架是一种分布式和合作的机器学习模型训练方法。在训练过程中，参与训练的客户端将本地存储的数据进行处理，并将模型更新分布式地进行优化，以最小化一个成本函数的值。FL被提出为隐私保护机器学习的步骤，但它受到了一些问题的威胁，如泄露隐私信息、模型无法个性化、模型偏袋特定群体的问题。在这篇论文中，我们研究了在FL框架中三方交互的个性化、隐私保障和公平性。 differential privacy和其变种已经被研究和应用为提供正式隐私保障的标准。但是，FL中的客户端通常拥有具有不同社区的异常多样化的数据，因此需要保护敏感信息，同时确保训练的模型对用户保持公平性。为达到这个目标，我们提出了基于$d$-隐私（即度量隐私）的方法，该方法通过地方化的隐私保障机制来保持原始数据的排序分布。这种方法不仅允许在联合学习方法下进行个性化模型训练，同时提供正式隐私保障，而且在不同标准度量下表现出较好的群体公平性。我们提供了理论基础和实验验证，以确认该方法的可行性和效果。
</details></li>
</ul>
<hr>
<h2 id="Selective-Scene-Text-Removal"><a href="#Selective-Scene-Text-Removal" class="headerlink" title="Selective Scene Text Removal"></a>Selective Scene Text Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00410">http://arxiv.org/abs/2309.00410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hayato Mitani, Akisato Kimura, Seiichi Uchida</li>
<li>for: 选择性场景文本除除（SSTR）是一种更复杂的图像转换任务，它可以根据用户指定的 Target words 进行文本区域的除去。</li>
<li>methods: 提议的方法采用多模块结构，通过fficiently training可以实现SSTR任务。</li>
<li>results: 实验结果表明，提议的方法可以成功地除去目标字符。<details>
<summary>Abstract</summary>
Scene text removal (STR) is the image transformation task to remove text regions in scene images. The conventional STR methods remove all scene text. This means that the existing methods cannot select text to be removed. In this paper, we propose a novel task setting named selective scene text removal (SSTR) that removes only target words specified by the user. Although SSTR is a more complex task than STR, the proposed multi-module structure enables efficient training for SSTR. Experimental results show that the proposed method can remove target words as expected.
</details>
<details>
<summary>摘要</summary>
场景文本去除（STR）是图像转换任务，用于从场景图像中除去文本区域。现有的STR方法都是完全去除场景中的所有文本的。在本文中，我们提出了一种新的任务设定方式，即选择场景文本去除（SSTR），可以根据用户指定的 Target words 来除去特定的文本区域。虽然SSTR比STR更复杂，但我们提出的多模块结构可以有效地进行SSTR的训练。实验结果表明，我们的方法可以按预期去除 Target words。
</details></li>
</ul>
<hr>
<h2 id="Learning-multi-modal-generative-models-with-permutation-invariant-encoders-and-tighter-variational-bounds"><a href="#Learning-multi-modal-generative-models-with-permutation-invariant-encoders-and-tighter-variational-bounds" class="headerlink" title="Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds"></a>Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00380">http://arxiv.org/abs/2309.00380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Hirt, Domenico Campolo, Victoria Leong, Juan-Pablo Ortega</li>
<li>for: 这 paper 的目的是提出一种深度隐藏变量模型，用于处理多Modal数据。</li>
<li>methods: 该 paper 使用 Variational Autoencoders (VAEs) 作为生成模型，并使用 Product-of-Experts (PoE) 或 Mixture-of-Experts (MoE) 的汇集方式来编码来自不同modalities的隐藏变量。</li>
<li>results: 该 paper 提出了一种更加灵活的汇集方式，可以基于 permutation-invariant neural networks 来组合不同modalities的编码特征。 numerical experiments 表明，这种汇集方式可以提供更好的生成质量和更好的适应性。<details>
<summary>Abstract</summary>
Devising deep latent variable models for multi-modal data has been a long-standing theme in machine learning research. Multi-modal Variational Autoencoders (VAEs) have been a popular generative model class that learns latent representations which jointly explain multiple modalities. Various objective functions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic considerations. In order to encode latent variables from different modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regarding their generative quality or consistency across multiple modalities. In this work, we consider a variational bound that can tightly lower bound the data log-likelihood. We develop more flexible aggregation schemes that generalise PoE or MoE approaches by combining encoded features from different modalities based on permutation-invariant neural networks. Our numerical experiments illustrate trade-offs for multi-modal variational bounds and various aggregation schemes. We show that tighter variational bounds and more flexible aggregation models can become beneficial when one wants to approximate the true joint distribution over observed modalities and latent variables in identifiable models.
</details>
<details>
<summary>摘要</summary>
开发深度含变量模型来处理多Modal数据是机器学习研究的长期主题。多ModalVariational Autoencoders (VAEs) 是一种广泛使用的生成模型类，它们学习共同解释多个模式的latent representation。对于这些模型，各种目标函数有被建议，通常是多Modal数据的log-likelihood下界或信息学推论。为了从不同模式subset中编码 latent variables，Product-of-Experts (PoE) 或 Mixture-of-Experts (MoE) 汇集方案经常使用，并显示出不同的交易OFF，例如生成质量或多Modal模式之间的一致性。在这种工作中，我们考虑了一种可以紧张地下界多Modal数据的log-likelihood的可变 bound。我们开发了更 flexible的汇集方案，它们通过使用 permutation-invariant neural networks来结合不同模式的编码特征。我们的数值实验表明，多Modal的可变 bound和不同的汇集方案之间存在交易OFF。我们显示，更紧张的可变 bound和更flexible的汇集模型可以在identifiable模型中 aproximate true joint distribution over observed modalities和latent variables。
</details></li>
</ul>
<hr>
<h2 id="Anomaly-detection-with-semi-supervised-classification-based-on-risk-estimators"><a href="#Anomaly-detection-with-semi-supervised-classification-based-on-risk-estimators" class="headerlink" title="Anomaly detection with semi-supervised classification based on risk estimators"></a>Anomaly detection with semi-supervised classification based on risk estimators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00379">http://arxiv.org/abs/2309.00379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Le Thi Khanh Hien, Sukanya Patra, Souhaib Ben Taieb</li>
<li>for: 本研究旨在超越一类分类异常检测方法的重要限制，即假设训练数据只包含正常实例的假设。</li>
<li>methods: 本研究提出了两种新的分类基于异常检测方法，即半超vised shallow异常检测方法和半超vised deep异常检测方法。</li>
<li>results: 我们的广泛实验证明了这两种异常检测方法的效果。<details>
<summary>Abstract</summary>
A significant limitation of one-class classification anomaly detection methods is their reliance on the assumption that unlabeled training data only contains normal instances. To overcome this impractical assumption, we propose two novel classification-based anomaly detection methods. Firstly, we introduce a semi-supervised shallow anomaly detection method based on an unbiased risk estimator. Secondly, we present a semi-supervised deep anomaly detection method utilizing a nonnegative (biased) risk estimator. We establish estimation error bounds and excess risk bounds for both risk minimizers. Additionally, we propose techniques to select appropriate regularization parameters that ensure the nonnegativity of the empirical risk in the shallow model under specific loss functions. Our extensive experiments provide strong evidence of the effectiveness of the risk-based anomaly detection methods.
</details>
<details>
<summary>摘要</summary>
一个重要的一类分类异常检测方法的限制是它们基于不实际的假设，即训练数据只包含正常实例。为了突破这个不切实际的假设，我们提出了两种新的分类基于异常检测方法。首先，我们介绍了一种半监督浅层异常检测方法，基于无偏风险估计器。其次，我们展示了一种半监督深度异常检测方法，使用非负（偏）风险估计器。我们证明了风险估计器的估计误差 bound和过分类 bound，以及选择合适的规范参数，以确保训练数据的非负性，特别是在特定的损失函数下。我们的广泛实验证明了这种风险基于异常检测方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Examining-the-Effectiveness-of-Chatbots-in-Gathering-Family-History-Information-in-Comparison-to-the-Standard-In-Person-Interview-Based-Approach"><a href="#Examining-the-Effectiveness-of-Chatbots-in-Gathering-Family-History-Information-in-Comparison-to-the-Standard-In-Person-Interview-Based-Approach" class="headerlink" title="Examining the Effectiveness of Chatbots in Gathering Family History Information in Comparison to the Standard In-Person Interview-Based Approach"></a>Examining the Effectiveness of Chatbots in Gathering Family History Information in Comparison to the Standard In-Person Interview-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03223">http://arxiv.org/abs/2309.03223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kieron Drumm, Vincent Tran</li>
<li>for: 本研究旨在提供一种基于 чат博的家族历史收集方法，以便在地理限制和技术水平等因素妨碍下，为家族历史研究提供一种可靠的数据收集方式。</li>
<li>methods: 本研究使用了一个特制的 чат博系统，通过自然语言处理和机器学习技术，为用户提供一种简单易用的家族历史收集方式。</li>
<li>results: 根据实验结果，使用 чат博系统进行家族历史收集时，用户的平均时间 consumption 和困惑度比使用 ancestry.com 或面对面采访低，但是平均时间 consumption 相对较长。<details>
<summary>Abstract</summary>
One of the most common things that a genealogist is tasked with is the gathering of a person's initial family history, normally via in-person interviews or with the use of a platform such as ancestry.com, as this can provide a strong foundation upon which a genealogist may build. However, the ability to conduct these interviews can often be hindered by both geographical constraints and the technical proficiency of the interviewee, as the interviewee in these types of interviews is most often an elderly person with a lower than average level of technical proficiency. With this in mind, this study presents what we believe, based on prior research, to be the first chatbot geared entirely towards the gathering of family histories, and explores the viability of utilising such a chatbot by comparing the performance and usability of such a method with the aforementioned alternatives. With a chatbot-based approach, we show that, though the average time taken to conduct an interview may be longer than if the user had used ancestry.com or participated in an in-person interview, the number of mistakes made and the level of confusion from the user regarding the UI and process required is lower than the other two methods. Note that the final metric regarding the user's confusion is not applicable for the in-person interview sessions due to its lack of a UI. With refinement, we believe this use of a chatbot could be a valuable tool for genealogists, especially when dealing with interviewees who are based in other countries where it is not possible to conduct an in-person interview.
</details>
<details>
<summary>摘要</summary>
一种非常常见的家族历史学家任务是收集人的初始家族历史，通常通过面对面访谈或使用 ancestry.com 平台进行。然而，进行这些访谈可以受到地理限制和接访者技术水平的影响，因为接访者通常是 elderly 人群，技术水平较低。为了解决这些问题，本研究提出了首个专门用于收集家族历史的 chatbot，并比较了使用这种方法与先前的方法的性能和使用性。我们发现，虽然使用 chatbot 方法的平均时间比 ancestry.com 和面对面访谈要长，但用户在 UI 和过程上的错误率和混乱度较低。注意，对面对面访谈SESSIONS 中的用户混乱度不适用，因为它没有 UI。针对这些问题，我们认为通过更新和改进， chatbot 可以成为家族历史学家在与国外接访者进行访谈时的有价值工具。
</details></li>
</ul>
<hr>
<h2 id="Where-Did-the-Gap-Go-Reassessing-the-Long-Range-Graph-Benchmark"><a href="#Where-Did-the-Gap-Go-Reassessing-the-Long-Range-Graph-Benchmark" class="headerlink" title="Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark"></a>Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00367">http://arxiv.org/abs/2309.00367</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/toenshoff/lrgb">https://github.com/toenshoff/lrgb</a></li>
<li>paper_authors: Jan Tönshoff, Martin Ritzert, Eran Rosenbluth, Martin Grohe</li>
<li>for: 该论文旨在提高图像学中的实验准备标准。</li>
<li>methods: 论文使用了多种MPGNN基elines和Graph Transformer GPS进行比较，并进行了优化hyperparameter。</li>
<li>results: 论文表明，Reported性能差异过大，可能是因为优化hyperparameter的问题。在多个dataset上，性能差异完全消失 после基本优化。此外，论文还探讨了LRGB的视觉dataset中缺失准则化的问题，以及LRGB链接预测度量的假象。<details>
<summary>Abstract</summary>
The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduced a set of graph learning tasks strongly dependent on long-range interaction between vertices. Empirical evidence suggests that on these tasks Graph Transformers significantly outperform Message Passing GNNs (MPGNNs). In this paper, we carefully reevaluate multiple MPGNN baselines as well as the Graph Transformer GPS (Ramp\'a\v{s}ek et al. 2022) on LRGB. Through a rigorous empirical analysis, we demonstrate that the reported performance gap is overestimated due to suboptimal hyperparameter choices. It is noteworthy that across multiple datasets the performance gap completely vanishes after basic hyperparameter optimization. In addition, we discuss the impact of lacking feature normalization for LRGB's vision datasets and highlight a spurious implementation of LRGB's link prediction metric. The principal aim of our paper is to establish a higher standard of empirical rigor within the graph machine learning community.
</details>
<details>
<summary>摘要</summary>
最近的长距离图 benchmark (LRGB, Dwivedi et al. 2022) 引入了一系列强依赖于长距离交互的图学任务。实际证据表明，在这些任务上，图变换器significantly outperform message passing GNNs (MPGNNs)。在这篇论文中，我们仔细重新评估了多个 MPGNN 基elines以及图变换器 GPS (Ramp\'a\v{s}ek et al. 2022) 在 LRGB 上的性能。通过严格的实际分析，我们示出了报告的性能差距过高的原因是因为选择不优的超参数。各个数据集上，性能差距完全消失 после基本超参数优化。此外，我们还讨论了 LRGB 视觉数据集缺乏特征Normalization的影响，并指出了 LRGB 链接预测度量的误导性。我们文章的主要目标是在图机器学习社区中提高实际的严格性标准。
</details></li>
</ul>
<hr>
<h2 id="FederatedScope-LLM-A-Comprehensive-Package-for-Fine-tuning-Large-Language-Models-in-Federated-Learning"><a href="#FederatedScope-LLM-A-Comprehensive-Package-for-Fine-tuning-Large-Language-Models-in-Federated-Learning" class="headerlink" title="FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning"></a>FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00363">http://arxiv.org/abs/2309.00363</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba/federatedscope">https://github.com/alibaba/federatedscope</a></li>
<li>paper_authors: Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, Jingren Zhou</li>
<li>for: 本文针对多种 privacy concern 限制不能共享数据的情况下，使用 federated learning (FL) 架构来搭配不同实体的下游任务进行 fine-tuning LLMs，并提供了一个名为 FS-LLM 的套件。</li>
<li>methods: 本文提出了许多 federated fine-tuning LLMs 面临的挑战，包括数据备忘、模型调整和资讯保护等问题，并提供了一个统一的 benchmarking 管道，以及一些 parameter-efficient 的 fine-tuning 算法和可扩展的程式接口。</li>
<li>results: 本文透过广泛的实验 validate 了 FS-LLM 的有效性，并与现有的 parameter-efficient fine-tuning 算法进行比较，获得了 valuable 的问题和答案，以及一些可能的 future research 方向。<details>
<summary>Abstract</summary>
LLMs have demonstrated great capabilities in various NLP tasks. Different entities can further improve the performance of those LLMs on their specific downstream tasks by fine-tuning LLMs. When several entities have similar interested tasks, but their data cannot be shared because of privacy concerns regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. However, fine-tuning LLMs in federated learning settings still lacks adequate support from existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, data preparation for different tasks, and distinct information protection demands. This paper first discusses these challenges of federated fine-tuning LLMs, and introduces our package FS-LLM as a main contribution, which consists of the following components: (1) we build an end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, federated fine-tuning execution, and performance evaluation on federated LLM fine-tuning; (2) we provide comprehensive federated parameter-efficient fine-tuning algorithm implementations and versatile programming interfaces for future extension in FL scenarios with low communication and computation costs, even without accessing the full model; (3) we adopt several accelerating and resource-efficient operators for fine-tuning LLMs with limited resources and the flexible pluggable sub-routines for interdisciplinary study. We conduct extensive experiments to validate the effectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-art parameter-efficient fine-tuning algorithms in FL settings, which also yields valuable insights into federated fine-tuning LLMs for the research community. To facilitate further research and adoption, we release FS-LLM at https://github.com/alibaba/FederatedScope/tree/llm.
</details>
<details>
<summary>摘要</summary>
This paper discusses the challenges of federated fine-tuning LLMs and introduces our package FS-LLM as the main contribution. FS-LLM consists of the following components:1. An end-to-end benchmarking pipeline that automates dataset preprocessing, federated fine-tuning execution, and performance evaluation for federated LLM fine-tuning.2. Comprehensive federated parameter-efficient fine-tuning algorithm implementations and versatile programming interfaces for future extensions in FL scenarios with low communication and computation costs, even without accessing the full model.3. Several accelerating and resource-efficient operators for fine-tuning LLMs with limited resources and flexible pluggable sub-routines for interdisciplinary study.We conduct extensive experiments to validate the effectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-art parameter-efficient fine-tuning algorithms in FL settings, providing valuable insights into federated fine-tuning LLMs for the research community. To facilitate further research and adoption, we release FS-LLM at <https://github.com/alibaba/FederatedScope/tree/llm>.
</details></li>
</ul>
<hr>
<h2 id="Explainable-Active-Learning-for-Preference-Elicitation"><a href="#Explainable-Active-Learning-for-Preference-Elicitation" class="headerlink" title="Explainable Active Learning for Preference Elicitation"></a>Explainable Active Learning for Preference Elicitation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00356">http://arxiv.org/abs/2309.00356</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/furkancanturk/explainable_active_learning">https://github.com/furkancanturk/explainable_active_learning</a></li>
<li>paper_authors: Furkan Cantürk, Reyhan Aydoğan</li>
<li>for: 本研究目的是解决推荐系统冷启动问题，即缺乏用户存在或者其他用户数据访问受限，使得无法使用用户 profiling 方法。</li>
<li>methods: 本研究使用 Active Learning (AL) 方法，通过选择大量未标注数据集中的 informative 数据，并请 oracle 进行标注，最后更新 Machine Learning (ML) 模型。</li>
<li>results: 实验结果表明，提案的 preference elicitation 方法可以在受限用户数据情况下提高推荐系统的个性化性，同时也可以增强用户信任度通过准确的解释。<details>
<summary>Abstract</summary>
Gaining insights into the preferences of new users and subsequently personalizing recommendations necessitate managing user interactions intelligently, namely, posing pertinent questions to elicit valuable information effectively. In this study, our focus is on a specific scenario of the cold-start problem, where the recommendation system lacks adequate user presence or access to other users' data is restricted, obstructing employing user profiling methods utilizing existing data in the system. We employ Active Learning (AL) to solve the addressed problem with the objective of maximizing information acquisition with minimal user effort. AL operates for selecting informative data from a large unlabeled set to inquire an oracle to label them and eventually updating a machine learning (ML) model. We operate AL in an integrated process of unsupervised, semi-supervised, and supervised ML within an explanatory preference elicitation process. It harvests user feedback (given for the system's explanations on the presented items) over informative samples to update an underlying ML model estimating user preferences. The designed user interaction facilitates personalizing the system by incorporating user feedback into the ML model and also enhances user trust by refining the system's explanations on recommendations. We implement the proposed preference elicitation methodology for food recommendation. We conducted human experiments to assess its efficacy in the short term and also experimented with several AL strategies over synthetic user profiles that we created for two food datasets, aiming for long-term performance analysis. The experimental results demonstrate the efficiency of the proposed preference elicitation with limited user-labeled data while also enhancing user trust through accurate explanations.
</details>
<details>
<summary>摘要</summary>
为了获得新用户的偏好信息并 personnalizing 推荐，我们需要智能地管理用户交互，即问得到有价值信息的方式。在这个研究中，我们关注了冷启用户问题，其中推荐系统缺乏用户存在或者其他用户数据访问是限制的，这限制了使用用户 profiling 方法。我们使用活动学习（AL）解决这个问题，旨在最大化信息收集与最小化用户努力。AL 选择大量未标注数据集中的有用信息，并请 oracle 标注它们，以更新机器学习（ML）模型。我们在无监督、半监督和监督 ML 之间进行混合处理，并在解释 preference 检索过程中运行。它在用户反馈（对系统对于展示的Item的解释）中获取用户偏好模型的更新。设计的用户交互方式可以个性化系统，并且提高用户信任度，因为它可以在推荐中包含用户反馈并更新 ML 模型。我们在食物推荐方面实现了这种偏好检索方法。我们对短期效果进行了人类实验，并对多个 AL 策略进行了长期性分析。实验结果表明，我们的偏好检索方法在有限用户标注数据下可以达到高效性，同时也提高用户信任度。
</details></li>
</ul>
<hr>
<h2 id="Local-and-adaptive-mirror-descents-in-extensive-form-games"><a href="#Local-and-adaptive-mirror-descents-in-extensive-form-games" class="headerlink" title="Local and adaptive mirror descents in extensive-form games"></a>Local and adaptive mirror descents in extensive-form games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00656">http://arxiv.org/abs/2309.00656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Côme Fiegel, Pierre Ménard, Tadashi Kozuno, Rémi Munos, Vianney Perchet, Michal Valko</li>
<li>for: 本研究探讨如何在零Sum假设游戏中学习 $\epsilon$-优策略。</li>
<li>methods: 本文使用了一种 fixesampling 方法，将玩家的策略更新为 fixed 数量的话。</li>
<li>results: 本文提出了一种基于 Online Mirror Descent 算法的 adaptive 方法，可以在零Sum IIG 中减少高异常性。这种方法可以在理论上保证 $\tilde{\mathcal{O}(T^{-1&#x2F;2})$ 的速度下逐渐 converges，并且具有高度优化的游戏参数。<details>
<summary>Abstract</summary>
We study how to learn $\epsilon$-optimal strategies in zero-sum imperfect information games (IIG) with trajectory feedback. In this setting, players update their policies sequentially based on their observations over a fixed number of episodes, denoted by $T$. Existing procedures suffer from high variance due to the use of importance sampling over sequences of actions (Steinberger et al., 2020; McAleer et al., 2022). To reduce this variance, we consider a fixed sampling approach, where players still update their policies over time, but with observations obtained through a given fixed sampling policy. Our approach is based on an adaptive Online Mirror Descent (OMD) algorithm that applies OMD locally to each information set, using individually decreasing learning rates and a regularized loss. We show that this approach guarantees a convergence rate of $\tilde{\mathcal{O}(T^{-1/2})$ with high probability and has a near-optimal dependence on the game parameters when applied with the best theoretical choices of learning rates and sampling policies. To achieve these results, we generalize the notion of OMD stabilization, allowing for time-varying regularization with convex increments.
</details>
<details>
<summary>摘要</summary>
我们研究如何学习ε-优化策略在零sum非完整信息游戏（IIG）中。在这种设定下，玩家们随着时间的推移更新他们的政策，基于他们所观察到的序列行动。现有的程序受到重要抽样的高差异性的影响（Steinberger et al., 2020; McAleer et al., 2022）。为了降低这种差异，我们考虑了一种固定抽样方法，其中玩家们仍然随着时间更新他们的政策，但是通过一个给定的固定抽样策略获取观察。我们的方法基于一种适应Online Mirror Descent（OMD）算法，该算法在每个信息集上应用OMD本地，使用 individually 递减的学习率和受限的损失函数。我们显示，这种方法在高probability下 converges 速率为 $\tilde{\mathcal{O}(T^{-1/2})$，并且在游戏参数中具有 near-optimal 的依赖性，当应用最佳理论上的学习率和抽样策略时。为了实现这些结果，我们扩展了OMD稳定性的概念，允许时间变化的正则化，并且使用凸型增量。
</details></li>
</ul>
<hr>
<h2 id="Bespoke-Nanoparticle-Synthesis-and-Chemical-Knowledge-Discovery-Via-Autonomous-Experimentations"><a href="#Bespoke-Nanoparticle-Synthesis-and-Chemical-Knowledge-Discovery-Via-Autonomous-Experimentations" class="headerlink" title="Bespoke Nanoparticle Synthesis and Chemical Knowledge Discovery Via Autonomous Experimentations"></a>Bespoke Nanoparticle Synthesis and Chemical Knowledge Discovery Via Autonomous Experimentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00349">http://arxiv.org/abs/2309.00349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyuk Jun Yoo, Nayeon Kim, Heeseung Lee, Daeho Kim, Leslie Tiong Ching Ow, Hyobin Nam, Chansoo Kim, Seung Yong Lee, Kwan-Young Lee, Donghun Kim, Sang Soo Han</li>
<li>for: 该研究旨在开发一种智能化的纳米材料合成平台，用于自动设计具有目标光学性能的纳米粒子（NPs）。</li>
<li>methods: 该平台采用了一种封闭Loop的实验方法，由批量合成模块和UV-Vis谱分析模块组成，基于人工智能优化模型的反馈。</li>
<li>results: 通过使用银（Ag）NPs为例，研究人员发现，通过使用抽象优化模型和早期停止 criterion，可以高效地生成具有Targeted absorption spectra的Ag NPs，只需200次迭代。此外，通过分析合成变量，发现了一种新的化学反应，即 citrate的影响于Ag NP合成中的竞争关系。<details>
<summary>Abstract</summary>
The optimization of nanomaterial synthesis using numerous synthetic variables is considered to be extremely laborious task because the conventional combinatorial explorations are prohibitively expensive. In this work, we report an autonomous experimentation platform developed for the bespoke design of nanoparticles (NPs) with targeted optical properties. This platform operates in a closed-loop manner between a batch synthesis module of NPs and a UV- Vis spectroscopy module, based on the feedback of the AI optimization modeling. With silver (Ag) NPs as a representative example, we demonstrate that the Bayesian optimizer implemented with the early stopping criterion can efficiently produce Ag NPs precisely possessing the desired absorption spectra within only 200 iterations (when optimizing among five synthetic reagents). In addition to the outstanding material developmental efficiency, the analysis of synthetic variables further reveals a novel chemistry involving the effects of citrate in Ag NP synthesis. The amount of citrate is a key to controlling the competitions between spherical and plate-shaped NPs and, as a result, affects the shapes of the absorption spectra as well. Our study highlights both capabilities of the platform to enhance search efficiencies and to provide a novel chemical knowledge by analyzing datasets accumulated from the autonomous experimentations.
</details>
<details>
<summary>摘要</summary>
“粒子 synthesis 的优化使用了许多的synthetic variable，被视为一项非常困难的任务，因为传统的 combinatorial 探索是不可能的。在这项工作中，我们报道了一个自动化实验平台，用于设计targeted optical property的nanoparticles（NPs）。这个平台通过closed-loop的方式，连接批量synthesis module和UV-Vis spectroscopy module，基于AI优化模型的反馈。使用银（Ag）NPs为例，我们示出了bayesian optimizer通过 early stopping criterion可以高效地生成Ag NPs，具有所需的吸收спектrum。此外，分析synthetic variable还揭示了一种新的化学效应，即citrate的影响在Ag NP synthesis中。citrate的Amount是控制spherical和plate-shaped NPs之间的竞争的关键，并且影响了吸收speктrum的形状。我们的研究强调了自动化实验平台的搜索效率提高和新的化学知识的提供，通过分析accumulated的实验数据。”
</details></li>
</ul>
<hr>
<h2 id="Multitask-Deep-Learning-for-Accurate-Risk-Stratification-and-Prediction-of-Next-Steps-for-Coronary-CT-Angiography-Patients"><a href="#Multitask-Deep-Learning-for-Accurate-Risk-Stratification-and-Prediction-of-Next-Steps-for-Coronary-CT-Angiography-Patients" class="headerlink" title="Multitask Deep Learning for Accurate Risk Stratification and Prediction of Next Steps for Coronary CT Angiography Patients"></a>Multitask Deep Learning for Accurate Risk Stratification and Prediction of Next Steps for Coronary CT Angiography Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00330">http://arxiv.org/abs/2309.00330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Lu, Mohammed Bennamoun, Jonathon Stewart, JasonK. Eshraghian, Yanbin Liu, Benjamin Chow, Frank M. Sanfilippo, Girish Dwivedi<br>for:这个研究旨在支持 coronary computed tomography angiography (CCTA) 患者的预后评估和下游测试选择。methods:我们提出了一个多任务深度学习模型，用于支持 CCTA 数据的风险分组和下游测试选择。我们的模型基于 state-of-the-art Perceiver 模型，并将其扩展以处理实际上的 CCTA 报告数据。results:我们的模型在 CAD 风险分组中取得了0.76的 Area Under the receiver operating characteristic Curve (AUC)，并在预测下游测试中取得了0.72 AUC。我们的提案的深度学习模型可以精确地估计 CAD 的可能性，并提供建议的下游测试基于过去 CCTA 数据。在临床实践中，这种方法可能会带来一个 paradigm shift 在风险分组和下游管理方面。<details>
<summary>Abstract</summary>
Diagnostic investigation has an important role in risk stratification and clinical decision making of patients with suspected and documented Coronary Artery Disease (CAD). However, the majority of existing tools are primarily focused on the selection of gatekeeper tests, whereas only a handful of systems contain information regarding the downstream testing or treatment. We propose a multi-task deep learning model to support risk stratification and down-stream test selection for patients undergoing Coronary Computed Tomography Angiography (CCTA). The analysis included 14,021 patients who underwent CCTA between 2006 and 2017. Our novel multitask deep learning framework extends the state-of-the art Perceiver model to deal with real-world CCTA report data. Our model achieved an Area Under the receiver operating characteristic Curve (AUC) of 0.76 in CAD risk stratification, and 0.72 AUC in predicting downstream tests. Our proposed deep learning model can accurately estimate the likelihood of CAD and provide recommended downstream tests based on prior CCTA data. In clinical practice, the utilization of such an approach could bring a paradigm shift in risk stratification and downstream management. Despite significant progress using deep learning models for tabular data, they do not outperform gradient boosting decision trees, and further research is required in this area. However, neural networks appear to benefit more readily from multi-task learning than tree-based models. This could offset the shortcomings of using single task learning approach when working with tabular data.
</details>
<details>
<summary>摘要</summary>
诊断调查扮演着重要的角色在患有所怀疑和确诊 coronary artery disease (CAD) 患者的风险分级和临床决策中。然而，大多数现有的工具都是专注于验选门槛测试，而只有一些系统包含关于下游测试或治疗的信息。我们提议一种多任务深度学习模型，用于支持风险分级和下游测试选择患者在进行 coronary computed tomography angiography (CCTA) 时。分析包括2006年至2017年间14,021名患者进行CCTA。我们的新型多任务深度学习框架将状态艺术Perceiver模型扩展到实际的CCTA报告数据中。我们的模型在CAD风险分级中获得了0.76的面积下接收操作特征曲线（AUC），并在预测下游测试中获得0.72的AUC。我们提议的深度学习模型可以准确地估计CAD的可能性并基于先前的CCTA数据提供下游测试的建议。在临床实践中，使用这种方法可能会引入一种新的风格转变，从而改善风险分级和下游管理。虽然深度学习模型在表格数据上进行了 significanthuge progress，但它们没有超过梯度提升树模型，需要进一步的研究。然而，神经网络似乎更容易受到多任务学习的影响，这可能将单任务学习方法的缺陷 offset。
</details></li>
</ul>
<hr>
<h2 id="Mi-Go-Test-Framework-which-uses-YouTube-as-Data-Source-for-Evaluating-Speech-Recognition-Models-like-OpenAI’s-Whisper"><a href="#Mi-Go-Test-Framework-which-uses-YouTube-as-Data-Source-for-Evaluating-Speech-Recognition-Models-like-OpenAI’s-Whisper" class="headerlink" title="Mi-Go: Test Framework which uses YouTube as Data Source for Evaluating Speech Recognition Models like OpenAI’s Whisper"></a>Mi-Go: Test Framework which uses YouTube as Data Source for Evaluating Speech Recognition Models like OpenAI’s Whisper</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00329">http://arxiv.org/abs/2309.00329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomasz Wojnar, Jaroslaw Hryszko, Adam Roman</li>
<li>for: 评估普通话音声识别机器学习模型的性能和适应性 across 多种实际场景</li>
<li>methods: 利用 YouTube 作为丰富和不断更新的数据源，考虑多种语言、口音、方言、说话风格和音质等因素，并使用 Whisper 模型作为测试对象</li>
<li>results: 结果表明 YouTube 是一个有价值的测试平台 для音声识别模型，确保其稳定性、准确性和对多种语言和听音环境的适应性，同时可以帮助发现 YouTube 字幕中可能的滥用<details>
<summary>Abstract</summary>
This article introduces Mi-Go, a novel testing framework aimed at evaluating the performance and adaptability of general-purpose speech recognition machine learning models across diverse real-world scenarios. The framework leverages YouTube as a rich and continuously updated data source, accounting for multiple languages, accents, dialects, speaking styles, and audio quality levels. To demonstrate the effectiveness of the framework, the Whisper model, developed by OpenAI, was employed as a test object. The tests involve using a total of 124 YouTube videos to test all Whisper model versions. The results underscore the utility of YouTube as a valuable testing platform for speech recognition models, ensuring their robustness, accuracy, and adaptability to diverse languages and acoustic conditions. Additionally, by contrasting the machine-generated transcriptions against human-made subtitles, the Mi-Go framework can help pinpoint potential misuse of YouTube subtitles, like Search Engine Optimization.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Mi-Go" 是一个新的试验框架，用于评估语音识别机器学习模型在实际情况下的性能和适应性。* "YouTube" 是一个丰富且不断更新的数据来源，accounting for multiple languages, accents, dialects, speaking styles, and audio quality levels.* "Whisper" 是 OpenAI 开发的一个 speech recognition 模型。* "machine-generated transcriptions" 是指由机器学习模型生成的转criptions。* "human-made subtitles" 是指人工生成的字幕。* "Search Engine Optimization" 是一种搜索引擎优化技术，可以用于提高网站的排名。
</details></li>
</ul>
<hr>
<h2 id="Multi-fidelity-reduced-order-surrogate-modeling"><a href="#Multi-fidelity-reduced-order-surrogate-modeling" class="headerlink" title="Multi-fidelity reduced-order surrogate modeling"></a>Multi-fidelity reduced-order surrogate modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00325">http://arxiv.org/abs/2309.00325</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/contipaolo/multifidelity_pod">https://github.com/contipaolo/multifidelity_pod</a></li>
<li>paper_authors: Paolo Conti, Mengwu Guo, Andrea Manzoni, Attilio Frangi, Steven L. Brunton, J. Nathan Kutz</li>
<li>For:  This paper aims to develop a data-driven strategy for multi-fidelity surrogate modeling of partial differential equations (PDEs) with a restricted computational budget.* Methods: The proposed method combines dimensionality reduction with multi-fidelity neural network surrogates, using classical proper orthogonal decomposition (POD) to generate a spatial basis and long-short term memory (LSTM) networks to approximate the dynamics of the reduced states.* Results: The proposed method enables the efficient recovery of full solution fields over time and parameter variations, and captures the onset of instabilities and transients well. The method is demonstrated on a collection of parametrized, time-dependent PDE problems with coarser meshes and&#x2F;or time stepping, as well as misspecified physical features.<details>
<summary>Abstract</summary>
High-fidelity numerical simulations of partial differential equations (PDEs) given a restricted computational budget can significantly limit the number of parameter configurations considered and/or time window evaluated for modeling a given system. Multi-fidelity surrogate modeling aims to leverage less accurate, lower-fidelity models that are computationally inexpensive in order to enhance predictive accuracy when high-fidelity data are limited or scarce. However, low-fidelity models, while often displaying important qualitative spatio-temporal features, fail to accurately capture the onset of instability and critical transients observed in the high-fidelity models, making them impractical as surrogate models. To address this shortcoming, we present a new data-driven strategy that combines dimensionality reduction with multi-fidelity neural network surrogates. The key idea is to generate a spatial basis by applying the classical proper orthogonal decomposition (POD) to high-fidelity solution snapshots, and approximate the dynamics of the reduced states - time-parameter-dependent expansion coefficients of the POD basis - using a multi-fidelity long-short term memory (LSTM) network. By mapping low-fidelity reduced states to their high-fidelity counterpart, the proposed reduced-order surrogate model enables the efficient recovery of full solution fields over time and parameter variations in a non-intrusive manner. The generality and robustness of this method is demonstrated by a collection of parametrized, time-dependent PDE problems where the low-fidelity model can be defined by coarser meshes and/or time stepping, as well as by misspecified physical features. Importantly, the onset of instabilities and transients are well captured by this surrogate modeling technique.
</details>
<details>
<summary>摘要</summary>
高精度数值计算 partial differential equations (PDEs) 给出一个限制的计算预算时，可能会有限制considered的参数配置和/或评估时间窗口 для模型一个给定系统。多精度 surrogate modeling 目的是使用精度较低、成本较低的模型来提高预测精度，当高精度数据scarce或有限时。然而，低精度模型通常表现出重要的空间特征，但缺乏束致稳定性和潜在的衰落特征，使其不可靠作为surrogate model。为了解决这一缺陷，我们提出了一种新的数据驱动策略， комбиines维度减少与多精度逻辑神经网络surrogate。关键思想是通过应用经典的正确归一化分解 (POD) 到高精度解决方案 snapshot，并使用多精度逻辑神经网络来近似减少后的动态。通过映射低精度减少后的状态到其高精度对应状态，提出的减少阶度surrogate模型可以高效地恢复时间和参数变化下的全解场景。这种方法的通用性和稳定性在一系列 Parametrized， time-dependent PDE problems 中得到了证明。此外，低精度模型可以通过粗糙的网格和/或时间步长来定义，以及 Physical features misspecified。importantly，这种surrogate modeling技术能够 accurately capture the onset of instabilities and transients。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Surrogate-Models-for-Materials-Science-Simulations-Machine-Learning-based-Prediction-of-Microstructure-Properties"><a href="#Efficient-Surrogate-Models-for-Materials-Science-Simulations-Machine-Learning-based-Prediction-of-Microstructure-Properties" class="headerlink" title="Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties"></a>Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00305">http://arxiv.org/abs/2309.00305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binh Duong Nguyen, Pavlo Potapenko, Aytekin Dermici, Kishan Govinda, Stefan Sandfeld</li>
<li>for: 这篇论文主要旨在用机器学习算法来提高和加速物理 simulate 模型，以便更好地理解和预测物质的结构-性质关系。</li>
<li>methods: 本文使用了六种机器学习技术，基于物理 simulate 模型中的两个不同数据集，用于预测磁铁频率频率频率域内的磁铁域的形成，以及描述物质动态演化的双相结构。</li>
<li>results: 研究人员通过分析各种模型的准确性和稳定性，了解了不同模型之间的差异，并发现包含域专业知识的特制特征可以提高模型的性能。<details>
<summary>Abstract</summary>
Determining, understanding, and predicting the so-called structure-property relation is an important task in many scientific disciplines, such as chemistry, biology, meteorology, physics, engineering, and materials science. Structure refers to the spatial distribution of, e.g., substances, material, or matter in general, while property is a resulting characteristic that usually depends in a non-trivial way on spatial details of the structure. Traditionally, forward simulations models have been used for such tasks. Recently, several machine learning algorithms have been applied in these scientific fields to enhance and accelerate simulation models or as surrogate models. In this work, we develop and investigate the applications of six machine learning techniques based on two different datasets from the domain of materials science: data from a two-dimensional Ising model for predicting the formation of magnetic domains and data representing the evolution of dual-phase microstructures from the Cahn-Hilliard model. We analyze the accuracy and robustness of all models and elucidate the reasons for the differences in their performances. The impact of including domain knowledge through tailored features is studied, and general recommendations based on the availability and quality of training data are derived from this.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:Determining, understanding, and predicting so-called structure-property relation 是多个科学领域中的重要任务，如化学、生物、气象、物理、工程和材料科学。“结构”指具体substances、材料或物质的空间分布，而“属性”是这些结构的结果特征，通常与空间细节有关。传统上，前向 simulations 模型被用于这类任务。在这项工作中，我们开发和检查了基于两个不同数据集的 six 种机器学习技术，来预测物料科学中的magnetic domains的形成和 dual-phase microstructures的演化。我们分析了所有模型的准确性和稳定性，并解释了模型之间的差异原因。我们还研究了包含域知识的特性如何影响模型性能，并根据训练数据的可用性和质量提出了一般建议。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Lidar-Driven-Reinforcement-Learning-for-Autonomous-Racing"><a href="#End-to-end-Lidar-Driven-Reinforcement-Learning-for-Autonomous-Racing" class="headerlink" title="End-to-end Lidar-Driven Reinforcement Learning for Autonomous Racing"></a>End-to-end Lidar-Driven Reinforcement Learning for Autonomous Racing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00296">http://arxiv.org/abs/2309.00296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meraj Mammadov</li>
<li>for: 研究RL算法在汽车竞速场景中的应用，以提高自动驾驶性能。</li>
<li>methods: 使用Feedforward雷达和速度数据进行RL训练，并在实际赛车场景中进行实验验证。</li>
<li>results: RL算法可以在无PRIOR地图信息的情况下提高自动赛车性能。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) has emerged as a transformative approach in the domains of automation and robotics, offering powerful solutions to complex problems that conventional methods struggle to address. In scenarios where the problem definitions are elusive and challenging to quantify, learning-based solutions such as RL become particularly valuable. One instance of such complexity can be found in the realm of car racing, a dynamic and unpredictable environment that demands sophisticated decision-making algorithms. This study focuses on developing and training an RL agent to navigate a racing environment solely using feedforward raw lidar and velocity data in a simulated context. The agent's performance, trained in the simulation environment, is then experimentally evaluated in a real-world racing scenario. This exploration underlines the feasibility and potential benefits of RL algorithm enhancing autonomous racing performance, especially in the environments where prior map information is not available.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RLAIF-Scaling-Reinforcement-Learning-from-Human-Feedback-with-AI-Feedback"><a href="#RLAIF-Scaling-Reinforcement-Learning-from-Human-Feedback-with-AI-Feedback" class="headerlink" title="RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"></a>RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00267">http://arxiv.org/abs/2309.00267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi</li>
<li>for: 这个论文旨在比较RLHF和RLAIF两种技术，以及它们在文本概要 tasks 上的性能。</li>
<li>methods: RLHF 使用人类反馈来训练语言模型，而 RLAIF 使用另一个预训练的语言模型来提供反馈。</li>
<li>results: 对于文本概要任务，人类评测者对 RLAIF 和 RLHF 生成的结果都有70%的时间 preference，而且在RLAIF vs RLHF结果之间，人类评测者偏好两者的性能相同。这些结果表明RLAIF可以达到人类水平的性能，提供一种可能性提高RLHF的解决方案。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
</details>
<details>
<summary>摘要</summary>
人工反馈学习（RLHF）可以让大语言模型（LLM）与人类首选之间进行对应，但收集高质量人类首选标签是一个关键瓶颈。我们进行了RLHF与AI反馈（RLAIF）的头对头比较，其中RLAIF使用存储库中的LLM来标注首选，而不是人类。我们发现，两种技术在摘要任务上都能够取得类似的改进。人类评审者对RLAIF和RLHF生成的摘要都有70%的时间表示满意，并且在RLAIF vs RLHF摘要之间进行评分时，人类宁愿选择两者。这些结果表明，RLAIF可以实现人类水平的性能，提供一种可能性途径来解决RLHF的扩展性限制。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Learning-Metrics-for-Improved-Federated-Learning"><a href="#Leveraging-Learning-Metrics-for-Improved-Federated-Learning" class="headerlink" title="Leveraging Learning Metrics for Improved Federated Learning"></a>Leveraging Learning Metrics for Improved Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00257">http://arxiv.org/abs/2309.00257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andre Fu</li>
<li>for: This paper is written for the purpose of developing a federated learning metric aggregation method that leverages the novel learning metric of effective rank (ER) to evaluate the performance of federated learning models.</li>
<li>methods: The paper uses a combination of federated learning and the effective rank metric to develop a novel weight-aggregation scheme that can improve the performance of federated learning models.</li>
<li>results: The paper shows that the proposed method outperforms baseline Federated Averaging and demonstrates the effectiveness of using effective rank in federated learning.<details>
<summary>Abstract</summary>
Currently in the federated setting, no learning schemes leverage the emerging research of explainable artificial intelligence (XAI) in particular the novel learning metrics that help determine how well a model is learning. One of these novel learning metrics is termed `Effective Rank' (ER) which measures the Shannon Entropy of the singular values of a matrix, thus enabling a metric determining how well a layer is mapping. By joining federated learning and the learning metric, effective rank, this work will \textbf{(1)} give the first federated learning metric aggregation method \textbf{(2)} show that effective rank is well-suited to federated problems by out-performing baseline Federated Averaging \cite{konevcny2016federated} and \textbf{(3)} develop a novel weight-aggregation scheme relying on effective rank.
</details>
<details>
<summary>摘要</summary>
当前在联合学习 Setting 中，无法利用emerging research of Explainable Artificial Intelligence (XAI)，特别是新的学习指标，以确定模型是如何学习。其中一种新的学习指标是“有效排名”（ER），它测量矩阵中特征值的散度 entropy，因此可以提供一个测量层是如何映射的 metric。通过结合联合学习和有效排名学习指标，本工作将实现以下三个目标：1. 提供首个联合学习指标聚合方法。2. 显示有效排名是联合问题中适用的，比基eline Federated Averaging \cite{konevcny2016federated} 的表现更佳。3. 开发一种基于有效排名的新的质量聚合方案。
</details></li>
</ul>
<hr>
<h2 id="SortedNet-a-Place-for-Every-Network-and-Every-Network-in-its-Place-Towards-a-Generalized-Solution-for-Training-Many-in-One-Neural-Networks"><a href="#SortedNet-a-Place-for-Every-Network-and-Every-Network-in-its-Place-Towards-a-Generalized-Solution-for-Training-Many-in-One-Neural-Networks" class="headerlink" title="SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks"></a>SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00255">http://arxiv.org/abs/2309.00255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba Valipour, Mehdi Rezagholizadeh, Hossein Rajabzadeh, Marzieh Tahaei, Boxing Chen, Ali Ghodsi</li>
<li>for: 提高深度学习模型的灵活性和效率，以适应内存和计算限制。</li>
<li>methods: 提出SortedNet方法，一种通用和可扩展的解决方案，可以在不同维度上利用深度学习模型的内在模块性。</li>
<li>results: 通过sorted训练方法，可以在单 Round of training中 scales to hundreds of sub-networks，并且可以实现无搜索的子网络选择和高效的switching between sub-networks。实验结果表明，提议的方法可以实现高效的子网络，同时outsperform state-of-the-art dynamic training approaches。<details>
<summary>Abstract</summary>
As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way, their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and trains them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during training that combines random sampling of sub-networks with gradient accumulation to improve training efficiency. Furthermore, the sorted nature of our training leads to a search-free sub-network selection at inference time; and the nested architecture of the resulting sub-networks leads to minimal storage requirement and efficient switching between sub-networks at inference. Our general dynamic training approach is demonstrated across various architectures and tasks, including large language models and pre-trained vision models. Experimental results show the efficacy of the proposed approach in achieving efficient sub-networks while outperforming state-of-the-art dynamic training approaches. Our findings demonstrate the feasibility of training up to 160 different sub-models simultaneously, showcasing the extensive scalability of our proposed method while maintaining 96% of the model performance.
</details>
<details>
<summary>摘要</summary>
随着深度学习模型的大小不断增长，在内存和计算限制下找到优化的模型变得越来越重要。尽管深度学习模型的architecture和组成部件通常允许它们在可模块化的方式下使用，但是训练过程并不意识到这种模块性。因此，传统的神经网络训练缺乏可动的计算负担适应性。这篇论文提出SortedNet，一种普适和可扩展的解决方案，以便在不同维度上利用深度神经网络的内在模块性进行高效的动态推理。我们的训练方法包括嵌入式结构，共享参数的嵌入式子网络，并在排序和概率方式下同时训练主模型和子网络。这种排序训练子网络方法使得我们可以在单一轮训练中scale到百个子网络。我们采用一种新的更新方案，其 combining随机子网络抽取和梯度积累来提高训练效率。此外，排序的训练方法导致在推理时不需要搜索子网络，而嵌入式的子网络结构又导致存储需求的最小化和高效的子网络切换。我们的通用动态训练方法在不同架构和任务上进行了广泛的实验，包括大语言模型和预训练视觉模型。实验结果表明我们的提议方法可以实现高效的子网络，同时超越当前的动态训练方法。我们的发现证明了可以同时训练160个不同的子模型，这表明我们的方法具有极大的扩展性，保持96%的模型性能。
</details></li>
</ul>
<hr>
<h2 id="Why-do-universal-adversarial-attacks-work-on-large-language-models-Geometry-might-be-the-answer"><a href="#Why-do-universal-adversarial-attacks-work-on-large-language-models-Geometry-might-be-the-answer" class="headerlink" title="Why do universal adversarial attacks work on large language models?: Geometry might be the answer"></a>Why do universal adversarial attacks work on large language models?: Geometry might be the answer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00254">http://arxiv.org/abs/2309.00254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varshini Subhash, Anna Bialas, Weiwei Pan, Finale Doshi-Velez</li>
<li>for: 本研究旨在解释大语言模型中的恶意攻击机制，以帮助理解和解释这些模型在攻击下的内部工作机制。</li>
<li>methods: 本研究使用了gradient-based universal adversarial攻击和白盒模型分析来研究大语言模型的攻击机制。</li>
<li>results: 研究发现，恶意攻击触发器可能是模型中的嵌入向量，这些向量只是模型在恶意训练区域中的semantic信息的近似。这种嵌入向量的存在可能导致模型的失败模式和攻击机制。<details>
<summary>Abstract</summary>
Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving universal attacks could help us gain deeper insight into the internal workings and failure modes of LLMs, thus enabling their mitigation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Interpretable-Medical-Imagery-Diagnosis-with-Self-Attentive-Transformers-A-Review-of-Explainable-AI-for-Health-Care"><a href="#Interpretable-Medical-Imagery-Diagnosis-with-Self-Attentive-Transformers-A-Review-of-Explainable-AI-for-Health-Care" class="headerlink" title="Interpretable Medical Imagery Diagnosis with Self-Attentive Transformers: A Review of Explainable AI for Health Care"></a>Interpretable Medical Imagery Diagnosis with Self-Attentive Transformers: A Review of Explainable AI for Health Care</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00252">http://arxiv.org/abs/2309.00252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tin Lai</li>
<li>for: 这篇论文旨在探讨基于变换器的计算机视觉模型在医疗领域的应用，以及如何使用解释人工智能来理解这些模型做出决策的过程。</li>
<li>methods: 这篇论文使用了变换器模型和解释人工智能技术来解释计算机视觉模型的决策过程。</li>
<li>results: 这篇论文提出了一些解释人工智能的方法和技术，可以用于理解计算机视觉模型在医疗领域的应用。<details>
<summary>Abstract</summary>
Recent advancements in artificial intelligence (AI) have facilitated its widespread adoption in primary medical services, addressing the demand-supply imbalance in healthcare. Vision Transformers (ViT) have emerged as state-of-the-art computer vision models, benefiting from self-attention modules. However, compared to traditional machine-learning approaches, deep-learning models are complex and are often treated as a "black box" that can cause uncertainty regarding how they operate. Explainable Artificial Intelligence (XAI) refers to methods that explain and interpret machine learning models' inner workings and how they come to decisions, which is especially important in the medical domain to guide the healthcare decision-making process. This review summarises recent ViT advancements and interpretative approaches to understanding the decision-making process of ViT, enabling transparency in medical diagnosis applications.
</details>
<details>
<summary>摘要</summary>
Note: "Primary medical care" is translated as "初级医疗服务" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="NeuroSurgeon-A-Toolkit-for-Subnetwork-Analysis"><a href="#NeuroSurgeon-A-Toolkit-for-Subnetwork-Analysis" class="headerlink" title="NeuroSurgeon: A Toolkit for Subnetwork Analysis"></a>NeuroSurgeon: A Toolkit for Subnetwork Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00244">http://arxiv.org/abs/2309.00244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlepori1/neurosurgeon">https://github.com/mlepori1/neurosurgeon</a></li>
<li>paper_authors: Michael A. Lepori, Ellie Pavlick, Thomas Serre</li>
<li>for: 了解神经网络模型中学习的算法 Representation</li>
<li>methods: 使用 NeuroSurgeon 库来发现和修改 Huggingface Transformers 库中的子网络</li>
<li>results: 可以用 NeuroSurgeon 库来探索和修改神经网络模型中的不同部分，以提高模型的可解释性<details>
<summary>Abstract</summary>
Despite recent advances in the field of explainability, much remains unknown about the algorithms that neural networks learn to represent. Recent work has attempted to understand trained models by decomposing them into functional circuits (Csord\'as et al., 2020; Lepori et al., 2023). To advance this research, we developed NeuroSurgeon, a python library that can be used to discover and manipulate subnetworks within models in the Huggingface Transformers library (Wolf et al., 2019). NeuroSurgeon is freely available at https://github.com/mlepori1/NeuroSurgeon.
</details>
<details>
<summary>摘要</summary>
尽管最近在神经网络解释领域有了 significative 的进步，但是还是有很多关于神经网络学习表示的不确定性。最近的研究尝试了通过分解神经网络模型为功能电路来理解训练后的模型（Csordás et al., 2020; Lepori et al., 2023）。为了进一步推进这些研究，我们开发了NeuroSurgeon，一个基于Python的库，可以用来发现和修改Transformers库中的模型内部子网络（Wolf et al., 2019）。NeuroSurgeon可以免费下载于https://github.com/mlepori1/NeuroSurgeon。
</details></li>
</ul>
<hr>
<h2 id="Image-Hijacking-Adversarial-Images-can-Control-Generative-Models-at-Runtime"><a href="#Image-Hijacking-Adversarial-Images-can-Control-Generative-Models-at-Runtime" class="headerlink" title="Image Hijacking: Adversarial Images can Control Generative Models at Runtime"></a>Image Hijacking: Adversarial Images can Control Generative Models at Runtime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00236">http://arxiv.org/abs/2309.00236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke Bailey, Euan Ong, Stuart Russell, Scott Emmons</li>
<li>for: 这个论文主要关注的是基础模型是否具备安全性，具体来说是对视觉语言模型（VLM）的图像输入进行攻击分析。</li>
<li>methods: 作者提出了一种称为“行为匹配”的攻击方法，可以在运行时控制生成模型。他们使用这种方法进行三类攻击：特定字符串攻击、泄露上下文攻击和监狱攻击。</li>
<li>results: 作者在使用LLaVA-2和LLaMA-2两种基础模型进行测试时发现，上述三类攻击都有成功率高于90%，并且这些攻击需要Only small image perturbations。这些结果表明基础模型具有严重的安全问题。<details>
<summary>Abstract</summary>
Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behavior Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choosing. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a solution is found -- if it even exists.
</details>
<details>
<summary>摘要</summary>
Foundation models是安全的吗？在这项工作中，我们关注vision-language模型（VLM）中的图像输入。我们发现图像劫持，这些恶意图像可以在运行时控制生成模型。我们介绍了行为匹配方法，这是一种创造图像劫持的通用方法。我们使用这种方法来探索三种攻击方式。特定的字符串攻击可以生成恶意者所选择的任何输出。泄露上下文攻击可以将上下文窗口中的信息泄露到输出中。囚犯攻击可以绕过模型的安全训练。我们对LLaVA-2、一个基于CLIP和LLaMA-2的state-of-the-art VLM进行了攻击，并发现所有我们的攻击类型具有超过90%的成功率。此外，我们的攻击是自动化的，只需要小图像干扰即可。这些发现对基础模型的安全性提出了严重的问题。如果图像劫持与CIFAR-10中的对抗性例子一样难以防止，那么可能需要很多年才能找到解决方案——如果其实 exist。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Projection-for-Reducing-Dimensionality-of-Linear-Programs-Generalization-Bound-and-Learning-Methods"><a href="#Data-Driven-Projection-for-Reducing-Dimensionality-of-Linear-Programs-Generalization-Bound-and-Learning-Methods" class="headerlink" title="Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods"></a>Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00203">http://arxiv.org/abs/2309.00203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shinsaku Sakaue, Taihei Oki</li>
<li>for: 这个论文研究了一种简单的数据驱动方法，用于高维线性程序（LP）的解决。给定过去的 $n$-维LP数据，我们可以学习一个 $n\times k$ 的投影矩阵 ($n &gt; k$)，从而将高维问题转化为低维问题。然后，我们可以使用任何用户喜欢的LP解决方法来解决低维问题，从而实现更快的LP解决。</li>
<li>methods: 我们使用了两种自然的方法来学习投影矩阵：PCA-和梯度基于的方法。而PCA-基于的方法是简单而高效的，而梯度基于的方法有时会导致更高质量的解决方案。</li>
<li>results: 实验表明，学习的投影矩阵对于减少LP解决时间的同时保持高质量解决方案是有利的。<details>
<summary>Abstract</summary>
This paper studies a simple data-driven approach to high-dimensional linear programs (LPs). Given data of past $n$-dimensional LPs, we learn an $n\times k$ \textit{projection matrix} ($n > k$), which reduces the dimensionality from $n$ to $k$. Then, we address future LP instances by solving $k$-dimensional LPs and recovering $n$-dimensional solutions by multiplying the projection matrix. This idea is compatible with any user-preferred LP solvers, hence a versatile approach to faster LP solving. One natural question is: how much data is sufficient to ensure the recovered solutions' quality? We address this question based on the idea of \textit{data-driven algorithm design}, which relates the amount of data sufficient for generalization guarantees to the \textit{pseudo-dimension} of performance metrics. We present an $\tilde{\mathrm{O}(nk^2)$ upper bound on the pseudo-dimension ($\tilde{\mathrm{O}$ compresses logarithmic factors) and complement it by an $\Omega(nk)$ lower bound, hence tight up to an $\tilde{\mathrm{O}(k)$ factor. On the practical side, we study two natural methods for learning projection matrices: PCA- and gradient-based methods. While the former is simple and efficient, the latter sometimes leads to better solution quality. Experiments confirm that learned projection matrices are beneficial for reducing the time for solving LPs while maintaining high solution quality.
</details>
<details>
<summary>摘要</summary>
One question is how much data is needed to ensure recovered solution quality. We address this using data-driven algorithm design, relating the required data to the pseudo-dimension of performance metrics. We present an $\tilde{\mathrm{O}(nk^2)$ upper bound on the pseudo-dimension, complemented by an $\Omega(nk)$ lower bound, providing a tight bound up to an $\tilde{\mathrm{O}(k)$ factor.In practice, we investigate two methods for learning projection matrices: PCA-based and gradient-based methods. While PCA-based methods are simple and efficient, gradient-based methods can sometimes lead to better solution quality. Experiments show that learned projection matrices improve LP solving time while maintaining high solution quality.
</details></li>
</ul>
<hr>
<h2 id="Subjectivity-in-Unsupervised-Machine-Learning-Model-Selection"><a href="#Subjectivity-in-Unsupervised-Machine-Learning-Model-Selection" class="headerlink" title="Subjectivity in Unsupervised Machine Learning Model Selection"></a>Subjectivity in Unsupervised Machine Learning Model Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00201">http://arxiv.org/abs/2309.00201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyi Chen, Mary L. Cummings</li>
<li>for: 这个研究 investigate 模型选择过程中的主观性。</li>
<li>methods: 该研究使用隐马尔可夫模型（Hidden Markov Model）作为例子，询问了33名参与者和3个大型自然语言模型（Large Language Models）在三个场景中进行模型选择。</li>
<li>results: 结果表明参与者和LLMs在不同的标准和度量上有很大的变化和不一致，特别是当不同的标准和度量矛盾时。主观性的来源包括参与者对不同标准和度量的重要性有不同意见，对模型简洁程度的看法不同，以及数据集大小如何影响模型选择。结果告诉我们需要开发一种更标准化的方法来记录模型选择过程中的主观选择。<details>
<summary>Abstract</summary>
Model selection is a necessary step in unsupervised machine learning. Despite numerous criteria and metrics, model selection remains subjective. A high degree of subjectivity may lead to questions about repeatability and reproducibility of various machine learning studies and doubts about the robustness of models deployed in the real world. Yet, the impact of modelers' preferences on model selection outcomes remains largely unexplored. This study uses the Hidden Markov Model as an example to investigate the subjectivity involved in model selection. We asked 33 participants and three Large Language Models (LLMs) to make model selections in three scenarios. Results revealed variability and inconsistencies in both the participants' and the LLMs' choices, especially when different criteria and metrics disagree. Sources of subjectivity include varying opinions on the importance of different criteria and metrics, differing views on how parsimonious a model should be, and how the size of a dataset should influence model selection. The results underscore the importance of developing a more standardized way to document subjective choices made in model selection processes.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diffusion-Model-with-Clustering-based-Conditioning-for-Food-Image-Generation"><a href="#Diffusion-Model-with-Clustering-based-Conditioning-for-Food-Image-Generation" class="headerlink" title="Diffusion Model with Clustering-based Conditioning for Food Image Generation"></a>Diffusion Model with Clustering-based Conditioning for Food Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00199">http://arxiv.org/abs/2309.00199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Han, Jiangpeng He, Mridul Gupta, Edward J. Delp, Fengqing Zhu</li>
<li>for: 用于提高食物图像生成质量和代表性</li>
<li>methods: 使用 conditional diffusion model 和 clustering-based 训练框架（ClusDiff）生成高质量食物图像</li>
<li>results: 在 Food-101 数据集上评估表明，提出的方法可以提高现有图像生成工作的性能，并在 VFN-LT 数据集上解决长尾食物分类问题<details>
<summary>Abstract</summary>
Image-based dietary assessment serves as an efficient and accurate solution for recording and analyzing nutrition intake using eating occasion images as input. Deep learning-based techniques are commonly used to perform image analysis such as food classification, segmentation, and portion size estimation, which rely on large amounts of food images with annotations for training. However, such data dependency poses significant barriers to real-world applications, because acquiring a substantial, diverse, and balanced set of food images can be challenging. One potential solution is to use synthetic food images for data augmentation. Although existing work has explored the use of generative adversarial networks (GAN) based structures for generation, the quality of synthetic food images still remains subpar. In addition, while diffusion-based generative models have shown promising results for general image generation tasks, the generation of food images can be challenging due to the substantial intra-class variance. In this paper, we investigate the generation of synthetic food images based on the conditional diffusion model and propose an effective clustering-based training framework, named ClusDiff, for generating high-quality and representative food images. The proposed method is evaluated on the Food-101 dataset and shows improved performance when compared with existing image generation works. We also demonstrate that the synthetic food images generated by ClusDiff can help address the severe class imbalance issue in long-tailed food classification using the VFN-LT dataset.
</details>
<details>
<summary>摘要</summary>
Image-based 膳食评估可以作为高效和准确的解决方案，用于记录和分析人们的膳食摄入。深度学习技术通常用于图像分析，如食物分类、分割和分量估计，这些技术需要大量的食物图像进行训练。然而，这种数据依赖性带来了实际应用中的巨大挑战，因为获得具有覆盖性、多样性和平衡性的食物图像可以是困难的。一种可能的解决方案是使用生成的食物图像进行数据增强。虽然现有的工作已经explored GAN结构的生成，但生成的食物图像质量仍然不高。此外，diffusion-based生成模型在通用图像生成任务上表现出色，但在食物图像生成中存在较大的内类差异。在这篇论文中，我们 investigate conditional diffusion模型的生成食物图像，并提出一种有效的归类分支训练框架，名为ClusDiff，用于生成高质量和代表性的食物图像。我们的方法在Food-101 dataset上进行了评估，并与现有的图像生成工作相比，显示了改进的性能。此外，我们还证明了ClusDiff生成的食物图像可以帮助解决VFN-LT dataset中的严重的长尾食物分类问题。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-Early-Fixing-for-Gas-lifted-Oil-Production-Optimization-Supervised-and-Weakly-supervised-Approaches"><a href="#Deep-learning-based-Early-Fixing-for-Gas-lifted-Oil-Production-Optimization-Supervised-and-Weakly-supervised-Approaches" class="headerlink" title="Deep-learning-based Early Fixing for Gas-lifted Oil Production Optimization: Supervised and Weakly-supervised Approaches"></a>Deep-learning-based Early Fixing for Gas-lifted Oil Production Optimization: Supervised and Weakly-supervised Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00197">http://arxiv.org/abs/2309.00197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruno Machado Pacheco, Laio Oriel Seman, Eduardo Camponogara</li>
<li>for: 提高天然气吸引油井的油量生产率。</li>
<li>methods: 使用深度学习模型来提供整数变量的值，从而将原始问题转化为线性Program（LP）。</li>
<li>results: 比较使用精确方法或通用估算方法的runtime reduction 71.11%，以及弱监督学习模型对初始化的值提供了重要的贡献。<details>
<summary>Abstract</summary>
Maximizing oil production from gas-lifted oil wells entails solving Mixed-Integer Linear Programs (MILPs). As the parameters of the wells, such as the basic-sediment-to-water ratio and the gas-oil ratio, are updated, the problems must be repeatedly solved. Instead of relying on costly exact methods or the accuracy of general approximate methods, in this paper, we propose a tailor-made heuristic solution based on deep learning models trained to provide values to all integer variables given varying well parameters, early-fixing the integer variables and, thus, reducing the original problem to a linear program (LP). We propose two approaches for developing the learning-based heuristic: a supervised learning approach, which requires the optimal integer values for several instances of the original problem in the training set, and a weakly-supervised learning approach, which requires only solutions for the early-fixed linear problems with random assignments for the integer variables. Our results show a runtime reduction of 71.11% Furthermore, the weakly-supervised learning model provided significant values for early fixing, despite never seeing the optimal values during training.
</details>
<details>
<summary>摘要</summary>
“最大化油气井的油生产需要解决杂合数线性 програм（MILP）。随着油井的参数，如基本粉末水比和气油比，的更新，问题需要重复解决。而不是依赖于成本高或精度不高的精确方法，在这篇论文中，我们提出了特制的机器学习模型，通过提供所有整数变量的值，来解决问题。我们提出了两种方法来开发学习基于的优化策略：一种监督学习方法，需要训练集中的优化整数值，以及一种弱监督学习方法，只需要解决随机分配的整数变量的初始值。我们的结果显示，使用学习模型可以实现71.11%的运行时间减少。此外，弱监督学习模型还提供了 significative 的初始化优化，即使在训练过程中从未看到优化的优化值。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/cs.LG_2023_09_01/" data-id="clmjn91mt007t0j887332f8ep" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/eess.IV_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T09:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/01/eess.IV_2023_09_01/">eess.IV - 2023-09-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="High-resolution-large-field-of-view-label-free-imaging-via-aberration-corrected-closed-form-complex-field-reconstruction"><a href="#High-resolution-large-field-of-view-label-free-imaging-via-aberration-corrected-closed-form-complex-field-reconstruction" class="headerlink" title="High-resolution, large field-of-view label-free imaging via aberration-corrected, closed-form complex field reconstruction"></a>High-resolution, large field-of-view label-free imaging via aberration-corrected, closed-form complex field reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00755">http://arxiv.org/abs/2309.00755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruizhi Cao, Cheng Shen, Changhuei Yang<br>for: 这篇论文旨在提出一种新的计算成像方法，可以高效地生成高分辨率、大领域视图的扭差自由图像。methods: 该方法使用多个倾斜照射来实现高速成像，并使用新的分析式phas retrieval框架来重建复杂的场。results: 该方法可以高效地重建复杂的场，并且可以Analytically retrieve complex aberrations of an imaging system with no additional hardware。此外，该方法比FPM更具robust性，可以Address aberration whose maximal phase difference exceeds 3.8π。<details>
<summary>Abstract</summary>
Computational imaging methods empower modern microscopy with the ability of producing high-resolution, large field-of-view, aberration-free images. One of the dominant computational label-free imaging methods, Fourier ptychographic microscopy (FPM), effectively increases the spatial-bandwidth product of conventional microscopy by using multiple tilted illuminations to achieve high-throughput imaging. However, its iterative reconstruction method is prone to parameter selection, can be computationally expensive and tends to fail under excessive aberrations. Recently, spatial Kramers-Kronig methods show it is possible to analytically reconstruct complex field but lacks the ability of correcting aberrations or providing extended resolution enhancement. Here, we present a closed-form method, termed APIC, which weds the strengths of both methods. A new analytical phase retrieval framework is established in APIC, which demonstrates, for the first time, the feasibility of analytically reconstructing the complex field associated with darkfield measurements. In addition, APIC can analytically retrieve complex aberrations of an imaging system with no additional hardware. By avoiding iterative algorithms, APIC requires no human designed convergence metric and always obtains a closed-form complex field solution. The faithfulness and correctness of APIC's reconstruction are guaranteed due to its analytical nature. We experimentally demonstrate that APIC gives correct reconstruction result while FPM fails to do so when constrained to the same number of measurements. Meanwhile, APIC achieves 2.8 times faster computation using image tile size of 256 (length-wise). We also demonstrate APIC is unprecedentedly robust against aberrations compared to FPM - APIC is capable of addressing aberration whose maximal phase difference exceeds 3.8${\pi}$ when using a NA 0.25 objective in experiment.
</details>
<details>
<summary>摘要</summary>
现代微镜技术得到了计算影像方法的支持，使得可以生成高分辨率、广领域视图、不受扭曲影响的图像。其中一种主要的计算无标签影像方法是干扰ptychographic microscopy（FPM），通过多个倾斜照明来实现高速度成像。然而，它的迭代重构方法容易选择参数，计算成本高，并且容易在过度扭曲下失败。最近，空间卡尔斯-克里戈夫方法表示可以分析重构复杂场景，但缺乏修正扭曲或提供扩展的分辨率提升能力。在这里，我们提出一种封闭式方法，名为APIC，它将传统的干扰ptychographic microscopy和空间卡尔斯-克里戈夫方法的优点相结合。我们建立了一个新的分析阶段重构框架，可以分析性地重构复杂场景相关的黑场测量数据。此外，APIC可以分析性地回收测试系统的扭曲 aberrations，无需额外硬件。由于不需要迭代算法，APIC不需要人工设置迭代约束，并且总是获得一个封闭式复杂场景解决方案。由于APIC的分析性质，它的重构结果具有真实性和正确性。我们在实验中证明，APIC可以在相同数量的测量下提供正确的重构结果，而FPM则失败。此外，APIC在计算速度方面也具有优势，使用图像分割大小为256时，计算速度高于FPM2.8倍。最后，我们还证明APIC在扭曲度较大时具有前所未有的稳定性，可以修正扭曲率达到3.8$\pi$的最大值，使用NA 0.25 objective在实验中。
</details></li>
</ul>
<hr>
<h2 id="Indexing-Irises-by-Intrinsic-Dimension"><a href="#Indexing-Irises-by-Intrinsic-Dimension" class="headerlink" title="Indexing Irises by Intrinsic Dimension"></a>Indexing Irises by Intrinsic Dimension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00705">http://arxiv.org/abs/2309.00705</a></li>
<li>repo_url: None</li>
<li>paper_authors: J. Michael Rozmus</li>
<li>for: 这个论文是为了提高人脸识别精度和速度而写的。</li>
<li>methods: 论文使用了主成分分析（PCA）将一小部分的正常化萝卜图像转换成四维内在空间，以便快速识别不知名人员。</li>
<li>results: 论文测试结果表明，使用这种方法可以快速准确地识别人脸，通常只需要比较少量的数据库。<details>
<summary>Abstract</summary>
28,000+ high-quality iris images of 1350 distinct eyes from 650+ different individuals from a relatively diverse university town population were collected. A small defined unobstructed portion of the normalized iris image is selected as a key portion for quickly identifying an unknown individual when submitting an iris image to be matched to a database of enrolled irises of the 1350 distinct eyes. The intrinsic dimension of a set of these key portions of the 1350 enrolled irises is measured to be about four (4). This set is mapped to a four-dimensional intrinsic space by principal components analysis (PCA). When an iris image is presented to the iris database for identification, the search begins in the neighborhood of the location of its key portion in the 4D intrinsic space, typically finding a correct identifying match after comparison to only a few percent of the database.
</details>
<details>
<summary>摘要</summary>
“我们收集了650+名不同个体的大学城区人口中的1350个不同眼睛的28,000多个高品质豢眼图像。我们从这些豢眼图像中选择了一小部分作为快速识别不知名人的关键部分，这个部分通常位于豢眼图像的一小部分。我们使用主成分分析（PCA）将这些关键部分映射到四维的内在空间中。当将豢眼图像提交至豢眼数据库进行识别时，搜寻从该豢眼图像的关键部分的 neighboorhood开始，通常可以在几%的豢眼数据库中找到正确的识别匹配。”
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Vision-Method-for-Correction-of-Eccentric-Error-Based-on-Adaptive-Enhancement-Algorithm"><a href="#A-Machine-Vision-Method-for-Correction-of-Eccentric-Error-Based-on-Adaptive-Enhancement-Algorithm" class="headerlink" title="A Machine Vision Method for Correction of Eccentric Error: Based on Adaptive Enhancement Algorithm"></a>A Machine Vision Method for Correction of Eccentric Error: Based on Adaptive Enhancement Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00514">http://arxiv.org/abs/2309.00514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fanyi Wang, Pin Cao, Yihui Zhang, Haotian Hu, Yongying Yang</li>
<li>for: 这篇论文主要关注于大口径非球面光学元件表面缺陷探测中，精确地调整光轴与机械转子轴的对接。</li>
<li>methods: 本论文提出了一种机器视觉方法来缓解不对称错误。它首先强调了对于非球面光学元件的对称错误探测中，严重的折冲模糊问题，可能导致缓解失败。因此，本论文提出了一个改进的 Adaptive Enhancement Algorithm (AEA)，包括已有的 Guided Filter Dark Channel Dehazing Algorithm (GFA) 和提出的轻量级 Multi-scale Densely Connected Network (MDC-Net)。</li>
<li>results: 本论文的结果显示，使用 AEA 可以对于不对称错误进行缓解，并且可以在短时间内执行。具体来说，AEA 可以在十个 200 像素 200 像素 Region of Interest (ROI) 图像中，实现了调整光轴与机械转子轴的对接，并且可以将错误降至 10 微米以下。<details>
<summary>Abstract</summary>
In the procedure of surface defects detection for large-aperture aspherical optical elements, it is of vital significance to adjust the optical axis of the element to be coaxial with the mechanical spin axis accurately. Therefore, a machine vision method for eccentric error correction is proposed in this paper. Focusing on the severe defocus blur of reference crosshair image caused by the imaging characteristic of the aspherical optical element, which may lead to the failure of correction, an Adaptive Enhancement Algorithm (AEA) is proposed to strengthen the crosshair image. AEA is consisted of existed Guided Filter Dark Channel Dehazing Algorithm (GFA) and proposed lightweight Multi-scale Densely Connected Network (MDC-Net). The enhancement effect of GFA is excellent but time-consuming, and the enhancement effect of MDC-Net is slightly inferior but strongly real-time. As AEA will be executed dozens of times during each correction procedure, its real-time performance is very important. Therefore, by setting the empirical threshold of definition evaluation function SMD2, GFA and MDC-Net are respectively applied to highly and slightly blurred crosshair images so as to ensure the enhancement effect while saving as much time as possible. AEA has certain robustness in time-consuming performance, which takes an average time of 0.2721s and 0.0963s to execute GFA and MDC-Net separately on ten 200pixels 200pixels Region of Interest (ROI) images with different degrees of blur. And the eccentricity error can be reduced to within 10um by our method.
</details>
<details>
<summary>摘要</summary>
在大开口非球面光元件表面缺陷检测过程中，准确调整光轴和机械轴之间的协调是非常重要。因此，本文提出了一种机器视觉方法来修正轴心错。关注非球面光元件的极大投射模糊的参考十字图像引起的严重模糊，我们提出了适应增强算法（AEA）来加强十字图像。AEA由现有的导引灰度阈值滤波算法（GFA）和提议的轻量级多尺度紧密连接网络（MDC-Net）组成。GFA的增强效果非常出色，但是时间耗费较高，而MDC-Net的增强效果虽然不及GFA，但是快速响应非常重要。因此，我们根据SMD2定义评价函数的实际阈值，将GFA和MDC-Net分别应用于高度和轻度模糊的十字图像，以保证增强效果的同时尽量降低时间成本。AEA具有一定的时间性能稳定性，每个执行AEA的时间平均为0.2721秒和0.0963秒，分别在十个200像素x200像素区域内的不同程度模糊的ROI图像上执行GFA和MDC-Net。而我们的方法可以将轴心错降至10微米以下。
</details></li>
</ul>
<hr>
<h2 id="Deep-Joint-Source-Channel-Coding-for-Adaptive-Image-Transmission-over-MIMO-Channels"><a href="#Deep-Joint-Source-Channel-Coding-for-Adaptive-Image-Transmission-over-MIMO-Channels" class="headerlink" title="Deep Joint Source-Channel Coding for Adaptive Image Transmission over MIMO Channels"></a>Deep Joint Source-Channel Coding for Adaptive Image Transmission over MIMO Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00470">http://arxiv.org/abs/2309.00470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Wu, Yulin Shao, Chenghong Bian, Krystian Mikolajczyk, Deniz Gündüz</li>
<li>for: 这个论文旨在应用当代图像传输中的多入多出多元几何（MIMO）通道，并提出一个基于视觉对变换器（ViT）的深度联合源和通道编码（DeepJSCC）方案，并在这个方案中运用自我参考机制来实现对图像和通道条件的智能适应。</li>
<li>methods: 这个方案使用了ViT的自我参考机制来学习特有的图像特征和通道条件，并且运用了 DeepJSCC 架构来实现智能的电力分配和对应映射策略。</li>
<li>results: 实验结果显示，对于各种测试场景，DeepJSCC-MIMO 可以实现优化的图像传输质量，并且具有适应性和稳定性。另外，DeepJSCC-MIMO 还可以在不同的通道条件、通道估计错误和天线数量下显示出优异的性能。<details>
<summary>Abstract</summary>
This paper introduces a vision transformer (ViT)-based deep joint source and channel coding (DeepJSCC) scheme for wireless image transmission over multiple-input multiple-output (MIMO) channels, denoted as DeepJSCC-MIMO. We consider DeepJSCC-MIMO for adaptive image transmission in both open-loop and closed-loop MIMO systems. The novel DeepJSCC-MIMO architecture surpasses the classical separation-based benchmarks with robustness to channel estimation errors and showcases remarkable flexibility in adapting to diverse channel conditions and antenna numbers without requiring retraining. Specifically, by harnessing the self-attention mechanism of ViT, DeepJSCC-MIMO intelligently learns feature mapping and power allocation strategies tailored to the unique characteristics of the source image and prevailing channel conditions. Extensive numerical experiments validate the significant improvements in transmission quality achieved by DeepJSCC-MIMO for both open-loop and closed-loop MIMO systems across a wide range of scenarios. Moreover, DeepJSCC-MIMO exhibits robustness to varying channel conditions, channel estimation errors, and different antenna numbers, making it an appealing solution for emerging semantic communication systems.
</details>
<details>
<summary>摘要</summary>
The novel DeepJSCC-MIMO architecture surpasses classical separation-based benchmarks and demonstrates robustness to channel estimation errors. It also showcases remarkable flexibility in adapting to diverse channel conditions and antenna numbers without requiring retraining.The DeepJSCC-MIMO scheme harnesses the self-attention mechanism of ViT to intelligently learn feature mapping and power allocation strategies tailored to the unique characteristics of the source image and prevailing channel conditions. Extensive numerical experiments confirm significant improvements in transmission quality achieved by DeepJSCC-MIMO for both open-loop and closed-loop MIMO systems across a wide range of scenarios.Moreover, DeepJSCC-MIMO exhibits robustness to varying channel conditions, channel estimation errors, and different antenna numbers, making it a promising solution for emerging semantic communication systems.
</details></li>
</ul>
<hr>
<h2 id="Learning-the-Imaging-Model-of-Speed-of-Sound-Reconstruction-via-a-Convolutional-Formulation"><a href="#Learning-the-Imaging-Model-of-Speed-of-Sound-Reconstruction-via-a-Convolutional-Formulation" class="headerlink" title="Learning the Imaging Model of Speed-of-Sound Reconstruction via a Convolutional Formulation"></a>Learning the Imaging Model of Speed-of-Sound Reconstruction via a Convolutional Formulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00453">http://arxiv.org/abs/2309.00453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Deniz Bezek, Maxim Haas, Richard Rau, Orcun Goksel<br>for:This paper aims to improve the accuracy of speed-of-sound (SoS) imaging by learning a forward imaging model based on data, rather than relying on hand-crafted models.methods:The proposed method uses a convolutional formulation of the pulse-echo SoS imaging problem, and learns a single unified kernel for the entire field-of-view. Least-squares estimation is used to learn the convolutional kernel, which can be constrained and regularized for numerical stability.results:The proposed method was tested on k-Wave simulations, phantom data, and in-vivo data. Compared to a conventional hand-crafted line-based wave-path model, the simulation-learned model improved the median contrast of SoS reconstructions by 63%, and the phantom-learned model quadrupled the SoS contrast. The in-vivo data results showed impressive 7 and 10 folds contrast improvements over the conventional model.<details>
<summary>Abstract</summary>
Speed-of-sound (SoS) is an emerging ultrasound contrast modality, where pulse-echo techniques using conventional transducers offer multiple benefits. For estimating tissue SoS distributions, spatial domain reconstruction from relative speckle shifts between different beamforming sequences is a promising approach. This operates based on a forward model that relates the sought local values of SoS to observed speckle shifts, for which the associated image reconstruction inverse problem is solved. The reconstruction accuracy thus highly depends on the hand-crafted forward imaging model. In this work, we propose to learn the SoS imaging model based on data. We introduce a convolutional formulation of the pulse-echo SoS imaging problem such that the entire field-of-view requires a single unified kernel, the learning of which is then tractable and robust. We present least-squares estimation of such convolutional kernel, which can further be constrained and regularized for numerical stability. In experiments, we show that a forward model learned from k-Wave simulations improves the median contrast of SoS reconstructions by 63%, compared to a conventional hand-crafted line-based wave-path model. This simulation-learned model generalizes successfully to acquired phantom data, nearly doubling the SoS contrast compared to the conventional hand-crafted alternative. We demonstrate equipment-specific and small-data regime feasibility by learning a forward model from a single phantom image, where our learned model quadruples the SoS contrast compared to the conventional hand-crafted model. On in-vivo data, the simulation- and phantom-learned models respectively exhibit impressive 7 and 10 folds contrast improvements over the conventional model.
</details>
<details>
<summary>摘要</summary>
射频速度（SoS）是一种emerging ultrasound contrast模式，使用普通的探频器提供多种优点。用于估计组织SoS分布的方法之一是在不同探频序列中使用统一的探频器进行缝合，从而从相对的射频扫描shift中推算出组织SoS分布。这借助了一个前向模型，该模型连接射频扫描shift和所需的本地SoS值。实际上，这个问题的解决方案高度取决于手工设计的前向射频模型。在这个研究中，我们提出了基于数据学习的SoS射频模型。我们将探频调频问题表述为一个数据学习问题，并且使用卷积方法来解决。我们提出了一种最小二乘估计方法来学习卷积核心，这个核心可以进一步紧缩和调整以确保数据稳定性。在实验中，我们发现了一个由k-Wave simulations学习的前向模型，与传统手工设计的线性射频模型相比，可以提高SoS重建的中位对比度63%。这个模型可以成功对于真实资料进行扩展，并且在小数据 regime中显示出Equipment-specific和�系统特定的可行性。在人体实验中，我们显示了模型的优秀表现，模型可以在不同的实验设备和资料上显示出7-10倍的SoS对比度提升。
</details></li>
</ul>
<hr>
<h2 id="On-the-Localization-of-Ultrasound-Image-Slices-within-Point-Distribution-Models"><a href="#On-the-Localization-of-Ultrasound-Image-Slices-within-Point-Distribution-Models" class="headerlink" title="On the Localization of Ultrasound Image Slices within Point Distribution Models"></a>On the Localization of Ultrasound Image Slices within Point Distribution Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00372">http://arxiv.org/abs/2309.00372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lennart Bastian, Vincent Bürgin, Ha Young Kim, Alexander Baumann, Benjamin Busam, Mahdi Saleh, Nassir Navab</li>
<li>For: 本研究旨在提高ultrasound图像的自动地位置标定，以便更好地诊断甲状腺疾病。* Methods: 该研究使用了对ultrasound图像和三维形态模型之间的相似性学习，并通过cross-modality registration和Procrustes分析将ultrasound图像注册到个体特定的甲状腺三维形态模型中。* Results: 实验结果表明，该多Modal注册框架可以准确地将ultrasound图像注册到个体特定的甲状腺三维形态模型和统计学模型中，并且可以预测slice的位置在个体特定的甲状腺三维形态模型上的平均误差为1.2毫米，而在统计学模型上的平均误差为4.6毫米。<details>
<summary>Abstract</summary>
Thyroid disorders are most commonly diagnosed using high-resolution Ultrasound (US). Longitudinal nodule tracking is a pivotal diagnostic protocol for monitoring changes in pathological thyroid morphology. This task, however, imposes a substantial cognitive load on clinicians due to the inherent challenge of maintaining a mental 3D reconstruction of the organ. We thus present a framework for automated US image slice localization within a 3D shape representation to ease how such sonographic diagnoses are carried out. Our proposed method learns a common latent embedding space between US image patches and the 3D surface of an individual's thyroid shape, or a statistical aggregation in the form of a statistical shape model (SSM), via contrastive metric learning. Using cross-modality registration and Procrustes analysis, we leverage features from our model to register US slices to a 3D mesh representation of the thyroid shape. We demonstrate that our multi-modal registration framework can localize images on the 3D surface topology of a patient-specific organ and the mean shape of an SSM. Experimental results indicate slice positions can be predicted within an average of 1.2 mm of the ground-truth slice location on the patient-specific 3D anatomy and 4.6 mm on the SSM, exemplifying its usefulness for slice localization during sonographic acquisitions. Code is publically available: \href{https://github.com/vuenc/slice-to-shape}{https://github.com/vuenc/slice-to-shape}
</details>
<details>
<summary>摘要</summary>
thyroid disorders 通常通过高分辨率ultrasound (US) 诊断。 longitudinal nodule tracking 是诊断的关键协议，但是这会对临床医生带来很大的认知压力，因为需要维护一个MENTAL 3D 重建 thyroid 的组织结构。为了解决这个问题，我们提出了一种自动化 US 图像片段位置标定方法。我们的提议方法通过对 US 图像块和个体 thyroid 形状的3D 表示进行对比度学习来学习一个公共几何空间。通过cross-modality 注册和Procrustes分析，我们利用我们的模型特征来注册 US 块到个体 thyroid 形状的3D 网格表示。我们的多Modal 注册框架可以将 US 块位于个体 thyroid 形状的3D 表面 topology 和统计学上的Shape模型 (SSM) 中。我们的实验结果表明，我们可以在1.2 mm 的平均误差内将 US 块位于patient-specific 3D  анатомии和SSM 中的slice位置。代码可以在以下链接中下载：https://github.com/vuenc/slice-to-shape。
</details></li>
</ul>
<hr>
<h2 id="How-You-Split-Matters-Data-Leakage-and-Subject-Characteristics-Studies-in-Longitudinal-Brain-MRI-Analysis"><a href="#How-You-Split-Matters-Data-Leakage-and-Subject-Characteristics-Studies-in-Longitudinal-Brain-MRI-Analysis" class="headerlink" title="How You Split Matters: Data Leakage and Subject Characteristics Studies in Longitudinal Brain MRI Analysis"></a>How You Split Matters: Data Leakage and Subject Characteristics Studies in Longitudinal Brain MRI Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00350">http://arxiv.org/abs/2309.00350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dewinda Julianensi Rumala</li>
<li>for: 这个研究探讨了医疗影像分析中深度学习模型的数据泄露问题，具体来说是使用3D卷积神经网络（CNNs）进行脑MRI分析。</li>
<li>methods: 这个研究使用了不同的数据分割策略来影响模型性能的探讨，并通过GradCAM可视化来揭示模型中的快捷路径问题。</li>
<li>results: 研究发现，不当的数据分割策略可能会导致模型性能下降，特别是在长itudinal的医疗影像数据中，模型可能会学习到主题以及诊断特征之间的相互关系。<details>
<summary>Abstract</summary>
Deep learning models have revolutionized the field of medical image analysis, offering significant promise for improved diagnostics and patient care. However, their performance can be misleadingly optimistic due to a hidden pitfall called 'data leakage'. In this study, we investigate data leakage in 3D medical imaging, specifically using 3D Convolutional Neural Networks (CNNs) for brain MRI analysis. While 3D CNNs appear less prone to leakage than 2D counterparts, improper data splitting during cross-validation (CV) can still pose issues, especially with longitudinal imaging data containing repeated scans from the same subject. We explore the impact of different data splitting strategies on model performance for longitudinal brain MRI analysis and identify potential data leakage concerns. GradCAM visualization helps reveal shortcuts in CNN models caused by identity confounding, where the model learns to identify subjects along with diagnostic features. Our findings, consistent with prior research, underscore the importance of subject-wise splitting and evaluating our model further on hold-out data from different subjects to ensure the integrity and reliability of deep learning models in medical image analysis.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese:深度学习模型已经革命化医疗影像分析领域，提供了 significante 的推荐价值，以提高诊断和患者照顾。然而，它们的性能可能受到隐藏的坑害，即“数据泄露”。在这项研究中，我们调查了3D医疗影像中的数据泄露，specifically using 3D Convolutional Neural Networks (CNNs) for brain MRI analysis。虽然3D CNNs 似乎比2D counterparts 更抵触数据泄露，但是在跨 validate （CV）时不当的数据分割可以仍然存在问题，尤其是longitudinal imaging data 包含了同一个主体多次扫描的情况。我们研究不同数据分割策略对 longitudinal brain MRI 分析中的模型性能的影响，并确定了数据泄露的可能问题。GradCAM 可视化帮助揭示了 CNN 模型中由于identify confounding 而导致的短路，这些短路使模型学习主体以及诊断特征。我们的发现与先前研究一致，强调了subject-wise 分割和在不同主体的 hold-out 数据上进一步评估我们的模型，以确保深度学习模型在医疗影像分析中的完整性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Application-of-Machine-Learning-in-Melanoma-Detection-and-the-Identification-of-‘Ugly-Duckling’-and-Suspicious-Naevi-A-Review"><a href="#Application-of-Machine-Learning-in-Melanoma-Detection-and-the-Identification-of-‘Ugly-Duckling’-and-Suspicious-Naevi-A-Review" class="headerlink" title="Application of Machine Learning in Melanoma Detection and the Identification of ‘Ugly Duckling’ and Suspicious Naevi: A Review"></a>Application of Machine Learning in Melanoma Detection and the Identification of ‘Ugly Duckling’ and Suspicious Naevi: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00265">http://arxiv.org/abs/2309.00265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatima Al Zegair, Nathasha Naranpanawa, Brigid Betz-Stablein, Monika Janda, H. Peter Soyer, Shekhar S. Chandra</li>
<li>for: 这种研究旨在提高皮肤癌诊断的准确率和 simplify 医疗决策，特别是在皮肤癌专业人员短缺的情况下。</li>
<li>methods: 这种研究使用了计算机支持诊断（CAD）技术，结合了机器学习（ML）和深度学习（DL）技术，以帮助检测皮肤癌和可疑的皮肤变化。</li>
<li>results: 这种研究表明，使用 ML 和 DL 技术可以提高皮肤癌检测的准确率，并且比专业 dermatologist 的性能更高。  Additionally, the study found that using ML and DL techniques can help identify “ugly duckling” naevi, which are naevi that stand out from others in the same individual and may indicate the presence of a cancerous melanoma.<details>
<summary>Abstract</summary>
Skin lesions known as naevi exhibit diverse characteristics such as size, shape, and colouration. The concept of an "Ugly Duckling Naevus" comes into play when monitoring for melanoma, referring to a lesion with distinctive features that sets it apart from other lesions in the vicinity. As lesions within the same individual typically share similarities and follow a predictable pattern, an ugly duckling naevus stands out as unusual and may indicate the presence of a cancerous melanoma. Computer-aided diagnosis (CAD) has become a significant player in the research and development field, as it combines machine learning techniques with a variety of patient analysis methods. Its aim is to increase accuracy and simplify decision-making, all while responding to the shortage of specialized professionals. These automated systems are especially important in skin cancer diagnosis where specialist availability is limited. As a result, their use could lead to life-saving benefits and cost reductions within healthcare. Given the drastic change in survival when comparing early stage to late-stage melanoma, early detection is vital for effective treatment and patient outcomes. Machine learning (ML) and deep learning (DL) techniques have gained popularity in skin cancer classification, effectively addressing challenges, and providing results equivalent to that of specialists. This article extensively covers modern Machine Learning and Deep Learning algorithms for detecting melanoma and suspicious naevi. It begins with general information on skin cancer and different types of naevi, then introduces AI, ML, DL, and CAD. The article then discusses the successful applications of various ML techniques like convolutional neural networks (CNN) for melanoma detection compared to dermatologists' performance. Lastly, it examines ML methods for UD naevus detection and identifying suspicious naevi.
</details>
<details>
<summary>摘要</summary>
皮肤损伤知为“脓肿”，具有不同大小、形状和颜色的特征。“ ugly duckling naevus”这个概念在监测恶性肿瘤方面发挥作用，指的是与周围其他损伤不同的损伤。由于同一个人的损伤通常具有相似的特征和预测的模式，“ ugly duckling naevus”会出现为不寻常的，可能指示恶性肿瘤存在。计算机支持诊断（CAD）在研发领域中扮演着重要角色，它将机器学习技术与多种患者分析方法结合，以提高准确性和简化决策。由于专业人员的匮乏，这些自动化系统在皮肤癌诊断中扮演着越来越重要的角色。因此，它们的使用可能导致生命的拯救和医疗成本的减少。由于晚期皮肤癌的存生差异很大，早期发现是致命的。机器学习（ML）和深度学习（DL）技术在皮肤癌分类中获得了广泛的应用，成功解决了许多挑战，并提供了与专业人员相当的结果。这篇文章从皮肤癌的基础知识和不同类型的脓肿开始，然后介绍了人工智能、机器学习、深度学习和CAD。文章 THEN 讲述了由不同的ML技术，如卷积神经网络（CNN），在恶性肿瘤检测中比专业人员的表现更好。最后，它检查了ML方法在UD脓肿检测和恶性肿瘤检测中的表现。
</details></li>
</ul>
<hr>
<h2 id="Gap-and-Overlap-Detection-in-Automated-Fiber-Placement"><a href="#Gap-and-Overlap-Detection-in-Automated-Fiber-Placement" class="headerlink" title="Gap and Overlap Detection in Automated Fiber Placement"></a>Gap and Overlap Detection in Automated Fiber Placement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00206">http://arxiv.org/abs/2309.00206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Assef Ghamisi, Homayoun Najjaran</li>
<li>for: 提高复合材料部件质量，自动检测和修复制造缺陷</li>
<li>methods: 使用Optical Coherence Tomography（OCT）感知器和计算机视觉技术检测和定位复合部件中的异常</li>
<li>results: 实现高精度和高效的异常检测和定位，提高复合部件质量<details>
<summary>Abstract</summary>
The identification and correction of manufacturing defects, particularly gaps and overlaps, are crucial for ensuring high-quality composite parts produced through Automated Fiber Placement (AFP). These imperfections are the most commonly observed issues that can significantly impact the overall quality of the composite parts. Manual inspection is both time-consuming and labor-intensive, making it an inefficient approach. To overcome this challenge, the implementation of an automated defect detection system serves as the optimal solution. In this paper, we introduce a novel method that uses an Optical Coherence Tomography (OCT) sensor and computer vision techniques to detect and locate gaps and overlaps in composite parts. Our approach involves generating a depth map image of the composite surface that highlights the elevation of composite tapes (or tows) on the surface. By detecting the boundaries of each tow, our algorithm can compare consecutive tows and identify gaps or overlaps that may exist between them. Any gaps or overlaps exceeding a predefined tolerance threshold are considered manufacturing defects. To evaluate the performance of our approach, we compare the detected defects with the ground truth annotated by experts. The results demonstrate a high level of accuracy and efficiency in gap and overlap segmentation.
</details>
<details>
<summary>摘要</summary>
“composite parts的制造瑕疵排查和修正，特别是空隙和重叠，是 Ensure High-quality Composite Parts produced through Automated Fiber Placement (AFP) 的关键。这些瑕疵是制造瑕疵中最常见的问题，可能对全面质量产生严重的影响。 manual inspection 是时间consuming 和劳动密集的，因此不是可行的方法。 To overcome this challenge, the implementation of an automated defect detection system serves as the optimal solution. In this paper, we introduce a novel method that uses an Optical Coherence Tomography (OCT) sensor and computer vision techniques to detect and locate gaps and overlaps in composite parts. Our approach involves generating a depth map image of the composite surface that highlights the elevation of composite tapes (or tows) on the surface. By detecting the boundaries of each tow, our algorithm can compare consecutive tows and identify gaps or overlaps that may exist between them. Any gaps or overlaps exceeding a predefined tolerance threshold are considered manufacturing defects. To evaluate the performance of our approach, we compare the detected defects with the ground truth annotated by experts. The results demonstrate a high level of accuracy and efficiency in gap and overlap segmentation.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/eess.IV_2023_09_01/" data-id="clmjn91qs00hd0j88a8p2bl1j" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.SD_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T15:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/cs.SD_2023_08_31/">cs.SD - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="General-Purpose-Audio-Effect-Removal"><a href="#General-Purpose-Audio-Effect-Removal" class="headerlink" title="General Purpose Audio Effect Removal"></a>General Purpose Audio Effect Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16177">http://arxiv.org/abs/2308.16177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mhrice/RemFx">https://github.com/mhrice/RemFx</a></li>
<li>paper_authors: Matthew Rice, Christian J. Steinmetz, George Fazekas, Joshua D. Reiss</li>
<li>for:  removes audio effects from realistic scenarios with multiple effects and varying source content</li>
<li>methods:  uses deep learning and a dataset of five audio effects across four different sources to train and evaluate a set of existing architectures</li>
<li>results:  outperforms single model baselines, but examples with many effects present remain challenging<details>
<summary>Abstract</summary>
Although the design and application of audio effects is well understood, the inverse problem of removing these effects is significantly more challenging and far less studied. Recently, deep learning has been applied to audio effect removal; however, existing approaches have focused on narrow formulations considering only one effect or source type at a time. In realistic scenarios, multiple effects are applied with varying source content. This motivates a more general task, which we refer to as general purpose audio effect removal. We developed a dataset for this task using five audio effects across four different sources and used it to train and evaluate a set of existing architectures. We found that no single model performed optimally on all effect types and sources. To address this, we introduced RemFX, an approach designed to mirror the compositionality of applied effects. We first trained a set of the best-performing effect-specific removal models and then leveraged an audio effect classification model to dynamically construct a graph of our models at inference. We found our approach to outperform single model baselines, although examples with many effects present remain challenging.
</details>
<details>
<summary>摘要</summary>
TRANSLATION:尽管音频效果的设计和应用已经非常了解，但去除这些效果是非常困难的，而且对此还未有充分的研究。在现实情况下，多种效果会同时应用于不同的音频内容，这种情况下的音频效果去除是一个更加普遍的任务。我们使用了五种音频效果，在四种不同的音频源上进行了数据集的构建，并使用了这些数据集来训练和评估一些现有的架构。我们发现，没有任何一个模型能够在所有的效果类型和源类型上表现最佳。为解决这个问题，我们提出了RemFX，一种基于效果的组合性来设计的方法。我们首先训练了一些最佳的效果特定的去除模型，然后通过音频效果分类模型来动态构建一个图结构，以便在推理时使用。我们发现，我们的方法可以超过单个模型的基线，但是存在多个效果存在的例子仍然是一个挑战。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.SD_2023_08_31/" data-id="clmjn91o800bj0j8826si8bhl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.CV_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T13:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/cs.CV_2023_08_31/">cs.CV - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Boosting-Detection-in-Crowd-Analysis-via-Underutilized-Output-Features"><a href="#Boosting-Detection-in-Crowd-Analysis-via-Underutilized-Output-Features" class="headerlink" title="Boosting Detection in Crowd Analysis via Underutilized Output Features"></a>Boosting Detection in Crowd Analysis via Underutilized Output Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16187">http://arxiv.org/abs/2308.16187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaokai Wu, Fengyu Yang</li>
<li>for: 这种研究旨在探讨检测基于方法在人群分析中的潜在优势，以及如何利用这些方法提高人群分析的精度和效果。</li>
<li>methods: 这种模型使用了混合2D-1D压缩技术来精细化输出特征，并通过地域适应的NMS阈值和解体并对齐策略来解决检测基于方法的主要局限性。</li>
<li>results: 经过广泛的人群分析任务评估，包括人群数量、位置和检测等，研究表明可以通过利用输出特征来提高检测基于方法在人群分析中的精度和效果。<details>
<summary>Abstract</summary>
Detection-based methods have been viewed unfavorably in crowd analysis due to their poor performance in dense crowds. However, we argue that the potential of these methods has been underestimated, as they offer crucial information for crowd analysis that is often ignored. Specifically, the area size and confidence score of output proposals and bounding boxes provide insight into the scale and density of the crowd. To leverage these underutilized features, we propose Crowd Hat, a plug-and-play module that can be easily integrated with existing detection models. This module uses a mixed 2D-1D compression technique to refine the output features and obtain the spatial and numerical distribution of crowd-specific information. Based on these features, we further propose region-adaptive NMS thresholds and a decouple-then-align paradigm that address the major limitations of detection-based methods. Our extensive evaluations on various crowd analysis tasks, including crowd counting, localization, and detection, demonstrate the effectiveness of utilizing output features and the potential of detection-based methods in crowd analysis.
</details>
<details>
<summary>摘要</summary>
传感器基本方法在群体分析中被视为不利，主要是因为它们在紧凑的群体中表现不佳。然而，我们认为这些方法的潜力被低估了，因为它们提供了群体分析中通常被忽略的重要信息。Specifically，输出提议和矩形框的面积和信任分数提供了群体的规模和密度的信息。为了利用这些尚未被利用的特征，我们提议了一个名为Crowd Hat的插件模块，可以轻松地与现有的检测模型结合使用。这个模块使用了2D-1D压缩技术来精细化输出特征，并从而获得群体中特定信息的空间和数字分布。基于这些特征，我们进一步提出了地域适应的NMS阈值和解除然后对齐的方法，这些方法可以解决检测基本方法中的主要局限性。我们对各种群体分析任务，包括群体计数、本地化和检测，进行了广泛的评估，并证明了利用输出特征和检测基本方法的潜力。
</details></li>
</ul>
<hr>
<h2 id="SAM-Med2D"><a href="#SAM-Med2D" class="headerlink" title="SAM-Med2D"></a>SAM-Med2D</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16184">http://arxiv.org/abs/2308.16184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uni-medical/sam-med2d">https://github.com/uni-medical/sam-med2d</a></li>
<li>paper_authors: Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, Hui Sun, Junjun He, Shaoting Zhang, Min Zhu, Yu Qiao</li>
<li>for: 本研究将SAM模型应用于医疗影像分类中，以填补自然影像和医疗影像之间的领域差距。</li>
<li>methods: 我们首先收集了约460万帧医疗影像和1970万个标注，建立了一个大规模的医疗影像分类数据集，覆盖多种模式和物体。然后，我们对SAM模型进行了全面的微调，并将其转换为SAM-Med2D模型。此外，我们还对SAM模型的Encoder和Decoder进行了进一步的微调，以获得最佳的SAM-Med2D模型。</li>
<li>results: 我们的方法在多种医疗影像分类任务中展示了明显的超越SAM的性能和扩展性。 Specifically, we evaluated the performance of SAM-Med2D on 9 datasets from MICCAI 2023 challenge and found that it outperformed SAM in terms of both segmentation accuracy and generalization ability.<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) represents a state-of-the-art research advancement in natural image segmentation, achieving impressive results with input prompts such as points and bounding boxes. However, our evaluation and recent research indicate that directly applying the pretrained SAM to medical image segmentation does not yield satisfactory performance. This limitation primarily arises from significant domain gap between natural images and medical images. To bridge this gap, we introduce SAM-Med2D, the most comprehensive studies on applying SAM to medical 2D images. Specifically, we first collect and curate approximately 4.6M images and 19.7M masks from public and private datasets, constructing a large-scale medical image segmentation dataset encompassing various modalities and objects. Then, we comprehensively fine-tune SAM on this dataset and turn it into SAM-Med2D. Unlike previous methods that only adopt bounding box or point prompts as interactive segmentation approach, we adapt SAM to medical image segmentation through more comprehensive prompts involving bounding boxes, points, and masks. We additionally fine-tune the encoder and decoder of the original SAM to obtain a well-performed SAM-Med2D, leading to the most comprehensive fine-tuning strategies to date. Finally, we conducted a comprehensive evaluation and analysis to investigate the performance of SAM-Med2D in medical image segmentation across various modalities, anatomical structures, and organs. Concurrently, we validated the generalization capability of SAM-Med2D on 9 datasets from MICCAI 2023 challenge. Overall, our approach demonstrated significantly superior performance and generalization capability compared to SAM.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的研究进展——Segment Anything Model（SAM），用于自然图像分割，并取得了卓越的结果。但是，我们的评估和最新的研究表明，直接将预训练的SAM应用于医疗图像分割不会达到满意的性能。这种限制主要归结于自然图像和医疗图像之间的领域差异。为bridging这个差异，我们介绍了SAM-Med2D，这是对SAM的最全面的应用研究，用于医疗二维图像分割。我们首先收集了约4.6万个图像和19.7万个mask从公共和私人数据集中，建立了一个包括多种Modalities和物体的医疗图像分割数据集。然后，我们对SAM进行了全面的微调，并将其转化为SAM-Med2D。与之前的方法只采用矩形框或点提示作为交互分割方法不同，我们在SAM-Med2D中采用了更全面的提示，包括矩形框、点和Mask。此外，我们还进行了SAM的encoder和decoder的微调，以获得一个高性能的SAM-Med2D。最后，我们进行了全面的评估和分析，以investigate SAM-Med2D在医疗图像分割中的性能，包括不同Modalities、生物结构和器官。同时，我们验证了SAM-Med2D在MICCAI 2023挑战赛中的通用能力。总的来说，我们的方法在医疗图像分割中表现出了显著的性能和通用能力，与SAM相比。
</details></li>
</ul>
<hr>
<h2 id="GREC-Generalized-Referring-Expression-Comprehension"><a href="#GREC-Generalized-Referring-Expression-Comprehension" class="headerlink" title="GREC: Generalized Referring Expression Comprehension"></a>GREC: Generalized Referring Expression Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16182">http://arxiv.org/abs/2308.16182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henghuiding/grefcoco">https://github.com/henghuiding/grefcoco</a></li>
<li>paper_authors: Shuting He, Henghui Ding, Chang Liu, Xudong Jiang</li>
<li>for: 本研究旨在推广经典引用表达理解（REC）的应用范围，以涵盖多个目标对象的引用表达。</li>
<li>methods: 该研究提出了一个新的标准测试集——通用引用表达理解（GREC）测试集，并实现了一种基于这个测试集的GREC方法实现代码。</li>
<li>results: 该研究在GREC测试集上实现了高度的准确率，并且在多个目标对象的引用表达中显示出了优异的性能。Translation:</li>
<li>for: 本研究的目标是推广经典引用表达理解（REC）的应用范围，以涵盖多个目标对象的引用表达。</li>
<li>methods: 该研究提出了一个新的标准测试集——通用引用表达理解（GREC）测试集，并实现了一种基于这个测试集的GREC方法实现代码。</li>
<li>results: 该研究在GREC测试集上实现了高度的准确率，并且在多个目标对象的引用表达中显示出了优异的性能。<details>
<summary>Abstract</summary>
The objective of Classic Referring Expression Comprehension (REC) is to produce a bounding box corresponding to the object mentioned in a given textual description. Commonly, existing datasets and techniques in classic REC are tailored for expressions that pertain to a single target, meaning a sole expression is linked to one specific object. Expressions that refer to multiple targets or involve no specific target have not been taken into account. This constraint hinders the practical applicability of REC. This study introduces a new benchmark termed as Generalized Referring Expression Comprehension (GREC). This benchmark extends the classic REC by permitting expressions to describe any number of target objects. To achieve this goal, we have built the first large-scale GREC dataset named gRefCOCO. This dataset encompasses a range of expressions: those referring to multiple targets, expressions with no specific target, and the single-target expressions. The design of GREC and gRefCOCO ensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, a GREC method implementation code, and GREC evaluation code are available at https://github.com/henghuiding/gRefCOCO.
</details>
<details>
<summary>摘要</summary>
“目的是实现文本描述中的物体引用表达（REC）。现有的dataset和技术仅适用于单一目标的表达，这限制了REC的实际应用。本研究引入了一个新的benchmark，称为通用 Referring Expression Comprehension（GREC）。GREC扩展了传统REC，允许表达描述任意数量的目标物体。为 достичь这个目标，我们建立了第一个大规模的GREC dataset，名为gRefCOCO。这个dataset包括了多个目标、无 especified 目标和单一目标的表达。GREC和gRefCOCO的设计保证与传统REC的相容性。提供了GREC方法实现代码、GREC评估代码和gRefCOCO dataset，可以在https://github.com/henghuiding/gRefCOCO中下载。”
</details></li>
</ul>
<hr>
<h2 id="MMVP-Motion-Matrix-based-Video-Prediction"><a href="#MMVP-Motion-Matrix-based-Video-Prediction" class="headerlink" title="MMVP: Motion-Matrix-based Video Prediction"></a>MMVP: Motion-Matrix-based Video Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16154">http://arxiv.org/abs/2308.16154</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kay1794/mmvp-motion-matrix-based-video-prediction">https://github.com/kay1794/mmvp-motion-matrix-based-video-prediction</a></li>
<li>paper_authors: Yiqi Zhong, Luming Liang, Ilya Zharkov, Ulrich Neumann</li>
<li>For: 本研究旨在解决视频预测中的中心挑战，即从图像帧中预测物体未来的运动，同时保持物体的外观一致性 across frames。* Methods: 该研究提出了一种可以批处理的两核心视频预测框架，即运动矩阵基于视频预测（MMVP）。与前一些方法不同的是，MMVP 将动作和外观信息解耦，通过构建外观不关心的运动矩阵来实现。这种设计提高了视频预测的准确率和效率，并降低了模型的大小。* Results: 广泛的实验结果表明，MMVP 在公共数据集上比前一些状态之前的系统更高（约1 db PSNR，UCF Sports），并且在远远小于前一些系统的模型大小（84% 或更小）下达成了这些result。请参考 <a target="_blank" rel="noopener" href="https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction">https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction</a> 获取官方代码和使用的数据集。<details>
<summary>Abstract</summary>
A central challenge of video prediction lies where the system has to reason the objects' future motions from image frames while simultaneously maintaining the consistency of their appearances across frames. This work introduces an end-to-end trainable two-stream video prediction framework, Motion-Matrix-based Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that usually handle motion prediction and appearance maintenance within the same set of modules, MMVP decouples motion and appearance information by constructing appearance-agnostic motion matrices. The motion matrices represent the temporal similarity of each and every pair of feature patches in the input frames, and are the sole input of the motion prediction module in MMVP. This design improves video prediction in both accuracy and efficiency, and reduces the model size. Results of extensive experiments demonstrate that MMVP outperforms state-of-the-art systems on public data sets by non-negligible large margins (about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the size or smaller). Please refer to https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction for the official code and the datasets used in this paper.
</details>
<details>
<summary>摘要</summary>
中心挑战：视频预测需要系统根据图像帧来预测物体未来运动，同时保持物体在帧之间的外观一致性。这篇论文提出了一种端到端训练的两核心视频预测框架——动力矩阵基于视频预测（MMVP），解决这个挑战。不同于之前的方法通常在同一组模块中处理运动预测和外观维持，MMVP 将运动和外观信息分离开来，通过构建不同帧的特征小块之间的应用无关动力矩阵来实现。这种设计提高了视频预测的准确性和效率，并减少模型的大小。经过广泛的实验，我们发现MMVP在公共数据集上比前一代系统大幅提高（约1 db PSNR、UCF Sports），而且模型的大小减少了84%以下。请参考https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction  для官方代码和使用的数据集。
</details></li>
</ul>
<hr>
<h2 id="Modality-Cycles-with-Masked-Conditional-Diffusion-for-Unsupervised-Anomaly-Segmentation-in-MRI"><a href="#Modality-Cycles-with-Masked-Conditional-Diffusion-for-Unsupervised-Anomaly-Segmentation-in-MRI" class="headerlink" title="Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI"></a>Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16150">http://arxiv.org/abs/2308.16150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyun Liang, Harry Anthony, Felix Wagner, Konstantinos Kamnitsas</li>
<li>for: 这篇论文的目的是提出一种不需要手动分类的无监督异常分割方法，可以检测在训练中无法处理的异常模式，以提高模型的可靠性，特别在医疗影像领域。</li>
<li>methods: 这篇论文提出的方法基于两个基本想法。首先，我们提出使用类型转换为机制来实现异常检测。图像转移模型学习特定体部特征的对应关系，因此在训练时不能转换特定的体部或图像模式，这就 enables their segmentation。其次，我们结合图像转移和封页 conditional diffusion 模型，尝试将遮盾区域下的组织视网膜，进一步暴露未知模式，当生成模型无法重建它们时。</li>
<li>results: 我们在 BraTS2021 多Modal MRI 的代理任务上训练这种方法，并在该任务上进行测试。我们的结果显示，我们的方法与先前的无监督方法相比，在图像重建和干扰中得到了比较好的结果。<details>
<summary>Abstract</summary>
Unsupervised anomaly segmentation aims to detect patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables their segmentation. Furthermore, we combine image translation with a masked conditional diffusion model, which attempts to `imagine' what tissue exists under a masked area, further exposing unknown patterns as the generative model fails to recreate them. We evaluate our method on a proxy task by training on healthy-looking slices of BraTS2021 multi-modality MRIs and testing on slices with tumors. We show that our method compares favorably to previous unsupervised approaches based on image reconstruction and denoising with autoencoders and diffusion models.
</details>
<details>
<summary>摘要</summary>
不监督异常分割Targets patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables their segmentation. Furthermore, we combine image translation with a masked conditional diffusion model, which attempts to 'imagine' what tissue exists under a masked area, further exposing unknown patterns as the generative model fails to recreate them. We evaluate our method on a proxy task by training on healthy-looking slices of BraTS2021 multi-modality MRIs and testing on slices with tumors. We show that our method compares favorably to previous unsupervised approaches based on image reconstruction and denoising with autoencoders and diffusion models.Here's the text with some additional information about the translation:I used the Google Translate API to translate the text into Simplified Chinese. The translation is written in a formal, academic style, which is appropriate for a research paper. I made sure to preserve the original meaning and structure of the text as much as possible, while also ensuring that the translation is grammatically correct and easy to understand.Please note that the translation is a machine translation, and it may not be perfect. There may be some nuances or idiomatic expressions that are lost in translation. If you have any specific questions or need further clarification, please feel free to ask!
</details></li>
</ul>
<hr>
<h2 id="CircleFormer-Circular-Nuclei-Detection-in-Whole-Slide-Images-with-Circle-Queries-and-Attention"><a href="#CircleFormer-Circular-Nuclei-Detection-in-Whole-Slide-Images-with-Circle-Queries-and-Attention" class="headerlink" title="CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention"></a>CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16145">http://arxiv.org/abs/2308.16145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanghx-iim-ahu/circleformer">https://github.com/zhanghx-iim-ahu/circleformer</a></li>
<li>paper_authors: Hengxu Zhang, Pengpeng Liang, Zhiyong Sun, Bo Song, Erkang Cheng</li>
<li>for: 这篇论文是针对医疗影像中圆形物体检测的研究，尤其是精准地检测体内组织中的圆形核。</li>
<li>methods: 本文使用了Transformer架构，并将圆形查询嵌入到Transformer解oder中，逐步精确地检测圆形物体。另外，本文还引入了圆形对偶探索模组，以计算圆形查询和影像特征之间的相似性。</li>
<li>results: 本文在公共的MoNuSeg数据集上进行了圆形核检测和分类任务的评估，并取得了比州前方法更好的成绩。此外，本文还进行了各组件效果的验证。<details>
<summary>Abstract</summary>
Both CNN-based and Transformer-based object detection with bounding box representation have been extensively studied in computer vision and medical image analysis, but circular object detection in medical images is still underexplored. Inspired by the recent anchor free CNN-based circular object detection method (CircleNet) for ball-shape glomeruli detection in renal pathology, in this paper, we present CircleFormer, a Transformer-based circular medical object detection with dynamic anchor circles. Specifically, queries with circle representation in Transformer decoder iteratively refine the circular object detection results, and a circle cross attention module is introduced to compute the similarity between circular queries and image features. A generalized circle IoU (gCIoU) is proposed to serve as a new regression loss of circular object detection as well. Moreover, our approach is easy to generalize to the segmentation task by adding a simple segmentation branch to CircleFormer. We evaluate our method in circular nuclei detection and segmentation on the public MoNuSeg dataset, and the experimental results show that our method achieves promising performance compared with the state-of-the-art approaches. The effectiveness of each component is validated via ablation studies as well. Our code is released at: \url{https://github.com/zhanghx-iim-ahu/CircleFormer}.
</details>
<details>
<summary>摘要</summary>
历史上，CNN和Transformer两种方法在计算机视觉和医学图像分析中进行了广泛的研究，但医学图像中径向物体检测仍然受到了相对较少的关注。在这篇论文中，我们提出了一种基于Transformer的径向医学对象检测方法，称为CircleFormer。该方法使用Transformer预测器中的径向查询来逐步精细地检测径向对象结果。此外，我们还提出了一种径向圆点对准模块，以计算径向查询和图像特征之间的相似性。此外，我们还提出了一种新的径向圆点IOU（gCIoU），用于衡量径向对象检测结果的准确性。此外，我们的方法易于扩展到分割任务，只需要在CircleFormer上添加一个简单的分割分支即可。我们在公共的MoNuSeg数据集上进行了径向核体检测和分割任务的实验，结果表明，我们的方法在与状态艺术方法相比表现出色。此外，我们还进行了一些缺省分析，以验证每个组件的有效性。我们的代码在：\url{https://github.com/zhanghx-iim-ahu/CircleFormer}。
</details></li>
</ul>
<hr>
<h2 id="MedShapeNet-–-A-Large-Scale-Dataset-of-3D-Medical-Shapes-for-Computer-Vision"><a href="#MedShapeNet-–-A-Large-Scale-Dataset-of-3D-Medical-Shapes-for-Computer-Vision" class="headerlink" title="MedShapeNet – A Large-Scale Dataset of 3D Medical Shapes for Computer Vision"></a>MedShapeNet – A Large-Scale Dataset of 3D Medical Shapes for Computer Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16139">http://arxiv.org/abs/2308.16139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianning Li, Antonio Pepe, Christina Gsaxner, Gijs Luijten, Yuan Jin, Narmada Ambigapathy, Enrico Nasca, Naida Solak, Gian Marco Melito, Afaque R. Memon, Xiaojun Chen, Jan Stefan Kirschke, Ezequiel de la Rosa, Patrich Ferndinand Christ, Hongwei Bran Li, David G. Ellis, Michele R. Aizenberg, Sergios Gatidis, Thomas Kuestner, Nadya Shusharina, Nicholas Heller, Vincent Andrearczyk, Adrien Depeursinge, Mathieu Hatt, Anjany Sekuboyina, Maximilian Loeffler, Hans Liebl, Reuben Dorent, Tom Vercauteren, Jonathan Shapey, Aaron Kujawa, Stefan Cornelissen, Patrick Langenhuizen, Achraf Ben-Hamadou, Ahmed Rekik, Sergi Pujades, Edmond Boyer, Federico Bolelli, Costantino Grana, Luca Lumetti, Hamidreza Salehi, Jun Ma, Yao Zhang, Ramtin Gharleghi, Susann Beier, Eduardo A. Garza-Villarreal, Thania Balducci, Diego Angeles-Valdez, Roberto Souza, Leticia Rittner, Richard Frayne, Yuanfeng Ji, Soumick Chatterjee, Andreas Nuernberger, Joao Pedrosa, Carlos Ferreira, Guilherme Aresta, Antonio Cunha, Aurelio Campilho, Yannick Suter, Jose Garcia, Alain Lalande, Emmanuel Audenaert, Claudia Krebs, Timo Van Leeuwen, Evie Vereecke, Rainer Roehrig, Frank Hoelzle, Vahid Badeli, Kathrin Krieger, Matthias Gunzer, Jianxu Chen, Amin Dada, Miriam Balzer, Jana Fragemann, Frederic Jonske, Moritz Rempe, Stanislav Malorodov, Fin H. Bahnsen, Constantin Seibold, Alexander Jaus, Ana Sofia Santos, Mariana Lindo, Andre Ferreira, Victor Alves, Michael Kamp, Amr Abourayya, Felix Nensa, Fabian Hoerst, Alexander Brehmer, Lukas Heine, Lars E. Podleska, Matthias A. Fink, Julius Keyl, Konstantinos Tserpes, Moon-Sung Kim, Shireen Elhabian, Hans Lamecker, Dzenan Zukic, Beatriz Paniagua, Christian Wachinger, Martin Urschler, Luc Duong, Jakob Wasserthal, Peter F. Hoyer, Oliver Basu, Thomas Maal, Max J. H. Witjes, Ping Luo, Bjoern Menze, Mauricio Reyes, Christos Davatzikos, Behrus Puladi, Jens Kleesiek, Jan Egger</li>
<li>for: The paper is written to introduce MedShapeNet, a large collection of anatomical shapes and 3D surgical instrument models for medical image analysis.</li>
<li>methods: The paper uses a variety of methods to create and annotate the shapes in MedShapeNet, including direct modeling on imaging data and paired data annotations.</li>
<li>results: The paper reports that MedShapeNet currently includes over 100,000 medical shapes and provides a freely available repository of 3D models for extended reality and medical 3D printing, with the potential to adapt state-of-the-art vision algorithms to solve critical medical problems.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍医疗影像分析的MedShapeNet，一个大量的生物形态和医疗器械3D模型集合。</li>
<li>methods: 论文使用了多种方法来创建和注释MedShapeNet中的形态，包括直接在医疗数据上模型和对应的数据注释。</li>
<li>results: 论文报告了MedShapeNet目前已经包含了超过100,000个医疗形态，并提供了一个免费的3D模型库，用于扩展现实（虚拟现实、增强现实、混合现实）和医疗3D打印。<details>
<summary>Abstract</summary>
We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to facilitate the translation of data-driven vision algorithms to medical applications, and it extends the opportunities to adapt SOTA vision algorithms to solve critical medical problems. Besides, the majority of the medical shapes in MedShapeNet are modeled directly on the imaging data of real patients, and therefore it complements well existing shape benchmarks comprising of computer-aided design (CAD) models. MedShapeNet currently includes more than 100,000 medical shapes, and provides annotations in the form of paired data. It is therefore also a freely available repository of 3D models for extended reality (virtual reality - VR, augmented reality - AR, mixed reality - MR) and medical 3D printing. This white paper describes in detail the motivations behind MedShapeNet, the shape acquisition procedures, the use cases, as well as the usage of the online shape search portal: https://medshapenet.ikim.nrw/
</details>
<details>
<summary>摘要</summary>
我们介绍MedShapeNet，一个大量医学形状（例如骨头、器官、血管）和3D医疗工具模型的集合。在深度学习时代之前，医学像分析中的统计形状模型（SSM）的广泛应用是证明医学数据中的形状很常被使用。然而，现在医学影像分析中的州际精算法（SOTA）都是以 voxel 为基础的。在计算机视觉中，形状（包括 voxel 占用网格、 mesh、点 cloud 和隐藏面模型）是3D数据的偏好表示方式，可见于许多shape相关的学会论文（如IEEE/CVF会议 on Computer Vision and Pattern Recognition（CVPR））以及形状库（如ShapeNet about 51,300 models和Princeton ModelNet 127,915 models）在计算机视觉研究中的增长 популяр性。MedShapeNet 是为了促进资料驱动的 computer vision 算法对医学应用的转译，而创建的一个替代方案，并延伸了适用 SOTA  vision 算法解决医学问题的机会。此外，MedShapeNet 中的大多数医学形状是直接从医疗影像数据中模型，因此与现有的 CAD 模型集成完美。MedShapeNet 目前包含超过 100,000 个医学形状，并提供双数据标签。因此，它还是一个免费可用的 3D 模型存储库，用于延伸现実（虚拟现実 - VR、增强现実 - AR、混合现実 - MR）和医疗 3D 印刷。本白皮书将详细介绍 MedShapeNet 的动机、形状取得程序、使用情况以及在线形状搜寻 Portal：https://medshapenet.ikim.nrw/
</details></li>
</ul>
<hr>
<h2 id="Improving-Few-shot-Image-Generation-by-Structural-Discrimination-and-Textural-Modulation"><a href="#Improving-Few-shot-Image-Generation-by-Structural-Discrimination-and-Textural-Modulation" class="headerlink" title="Improving Few-shot Image Generation by Structural Discrimination and Textural Modulation"></a>Improving Few-shot Image Generation by Structural Discrimination and Textural Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16110">http://arxiv.org/abs/2308.16110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kobeshegu/sdtm-gan-acmmm-2023">https://github.com/kobeshegu/sdtm-gan-acmmm-2023</a></li>
<li>paper_authors: Mengping Yang, Zhe Wang, Wenyi Feng, Qian Zhang, Ting Xiao</li>
<li>for: 这篇论文的目的是提出一种新的几何图像生成模型，以实现几何图像生成中的内在Semantic Modulation和全局结构检测。</li>
<li>methods: 这篇论文使用的方法包括内在Local Representation的Semantic Modulation，以及全局结构检测器（StructD）的开发，以及鼓励模型识别频率信号的技术。</li>
<li>results: 这篇论文的实验结果显示，这种新的几何图像生成模型可以在三个popular dataset上 achieves state-of-the-art的Synthesis Performance，并且可以与现有的模型集成，以提高其性能。<details>
<summary>Abstract</summary>
Few-shot image generation, which aims to produce plausible and diverse images for one category given a few images from this category, has drawn extensive attention. Existing approaches either globally interpolate different images or fuse local representations with pre-defined coefficients. However, such an intuitive combination of images/features only exploits the most relevant information for generation, leading to poor diversity and coarse-grained semantic fusion. To remedy this, this paper proposes a novel textural modulation (TexMod) mechanism to inject external semantic signals into internal local representations. Parameterized by the feedback from the discriminator, our TexMod enables more fined-grained semantic injection while maintaining the synthesis fidelity. Moreover, a global structural discriminator (StructD) is developed to explicitly guide the model to generate images with reasonable layout and outline. Furthermore, the frequency awareness of the model is reinforced by encouraging the model to distinguish frequency signals. Together with these techniques, we build a novel and effective model for few-shot image generation. The effectiveness of our model is identified by extensive experiments on three popular datasets and various settings. Besides achieving state-of-the-art synthesis performance on these datasets, our proposed techniques could be seamlessly integrated into existing models for a further performance boost.
</details>
<details>
<summary>摘要</summary>
“几帧图像生成”，它目的是生成一个分类中的具有实际性和多样性的图像，只需要几帧图像作为输入。现有的方法可以全面 interpolate 不同的图像或者融合本地表现和预先定义的系数。然而，这种直觉的图像/特征融合只是利用最相关的信息进行生成，从而导致低的多样性和粗糙的 semantic 融合。为了解决这个问题，这篇论文提出了一个新的文本调控（TexMod）机制，可以将外部 semantics 信号注入到内部的本地表现中。这个 TexMod 由 discriminator 的反馈参数化，可以实现更细grained的 semantic 注入，同时维持生成的实际性。此外，我们还开发了一个全球结构 discriminator（StructD），可以明确地导引模型生成具有合理的配置和架构的图像。此外，我们还强调了模型的频率意识，通过让模型能够识别频率信号。通过这些技术，我们建立了一个新的和有效的几帧图像生成模型。我们的模型在三个流行的数据集上进行了广泛的实验，并在不同的设定下表现出色。除了在这些数据集上达到了现有的州域性synthesis 性能外，我们的提出的技术还可以与现有的模型进行整合，以获得更高的性能。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.CV_2023_08_31/" data-id="clmjn91kk004b0j880262em2c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.AI_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T12:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/cs.AI_2023_08_31/">cs.AI - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Learning-Vision-based-Pursuit-Evasion-Robot-Policies"><a href="#Learning-Vision-based-Pursuit-Evasion-Robot-Policies" class="headerlink" title="Learning Vision-based Pursuit-Evasion Robot Policies"></a>Learning Vision-based Pursuit-Evasion Robot Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16185">http://arxiv.org/abs/2308.16185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Bajcsy, Antonio Loquercio, Ashish Kumar, Jitendra Malik</li>
<li>for: 这 paper 是为了学习在实际世界中进行策略性机器人行为的挑战，特别是在追逐逃脱交互中。</li>
<li>methods: 这 paper 使用了将这个难以解决的问题转化为一个监督学习问题，其中一个全 observable 机器人策略生成了一个 partially observable 机器人策略的超vision。</li>
<li>results: 研究发现，在追逐逃脱交互中，部分可见的机器人策略的训练信号质量取决于两个关键因素：逃脱者的行为均衡和模型假设的强度。这 paper 还在实际中部署了这种策略，并在物理四脚机器人上进行了追逐逃脱交互。<details>
<summary>Abstract</summary>
Learning strategic robot behavior -- like that required in pursuit-evasion interactions -- under real-world constraints is extremely challenging. It requires exploiting the dynamics of the interaction, and planning through both physical state and latent intent uncertainty. In this paper, we transform this intractable problem into a supervised learning problem, where a fully-observable robot policy generates supervision for a partially-observable one. We find that the quality of the supervision signal for the partially-observable pursuer policy depends on two key factors: the balance of diversity and optimality of the evader's behavior and the strength of the modeling assumptions in the fully-observable policy. We deploy our policy on a physical quadruped robot with an RGB-D camera on pursuit-evasion interactions in the wild. Despite all the challenges, the sensing constraints bring about creativity: the robot is pushed to gather information when uncertain, predict intent from noisy measurements, and anticipate in order to intercept. Project webpage: https://abajcsy.github.io/vision-based-pursuit/
</details>
<details>
<summary>摘要</summary>
学习策略性机器人行为 -- 如追逐避免交互 -- 在真实世界环境中是非常困难的。它需要利用交互动力学，并通过物理状态和潜在意图不确定性进行规划。在这篇论文中，我们将这个难以解决的问题转化为一个监督学习问题，其中一个完全可见的机器人政策生成了一个部分可见的追逐者政策的监督信号。我们发现了两个关键因素对 partially-observable pursuer policy 的质量监督信号产生影响：逃脱者的行为均衡和优化程度，以及完全可见政策中模型假设的强度。我们将我们的政策部署到一个物理四脚机器人上，并使用RGB-D摄像头进行追逐逃脱交互。尽管所有挑战，感知约束促使机器人在不确定时收集信息，从杂乱测量中预测意图，并在预测不准确时预测以 intercept。项目首页：https://abajcsy.github.io/vision-based-pursuit/
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Uncertainty-in-Answers-from-any-Language-Model-via-Intrinsic-and-Extrinsic-Confidence-Assessment"><a href="#Quantifying-Uncertainty-in-Answers-from-any-Language-Model-via-Intrinsic-and-Extrinsic-Confidence-Assessment" class="headerlink" title="Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment"></a>Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16175">http://arxiv.org/abs/2308.16175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuhai Chen, Jonas Mueller</li>
<li>for: detecting bad and speculative answers from a pretrained Large Language Model</li>
<li>methods: estimating a numeric confidence score for any output generated by the LLM, combining intrinsic and extrinsic assessments of confidence</li>
<li>results: more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures, and can obtain more accurate responses by sampling multiple responses and considering the one with the highest confidence score.Here’s the full translation in Simplified Chinese:</li>
<li>for: 用于探测预训练的大语言模型中的坏和推测答案</li>
<li>methods: 通过估算任何输出生成的数字信任分数，并将内在和外在评估信任相结合</li>
<li>results: 比alternative uncertainty estimation procedures更加准确地认定大语言模型的错误答案，并可以通过采样多个响应并考虑最高信任分的响应来获得更加准确的答案，无需额外训练步骤。<details>
<summary>Abstract</summary>
We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, and combines intrinsic and extrinsic assessments of confidence into a single trustworthiness estimate for any LLM response to a given prompt. Our method is extremely general and can applied to all of the best LLMs available today (whose training data remains unknown). By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that caution when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without any extra training steps.
</details>
<details>
<summary>摘要</summary>
我们介绍BSDetector，一种方法用于检测预训练大语言模型生成的差异和推测答案的 numeric 信任分数。我们的不确定性评估技术适用于任何可以通过黑色盒API访问的 LLM，并将内在和外在评估信任综合到一个 Trustworthiness 估计中。我们的方法非常通用，可以应用于今天最好的 LLM 中的任何一个（训练数据未知）。通过点些额外计算，用户可以通过 LLM API 获得同样的回答和信任估计，从而了解不要信任这个回答。在关闭和开放问答benchmark上进行实验，我们发现BSDetector可以更准确地确定 LLM 的错误回答，比alternative uncertainty estimation方法更好。此外，我们可以通过选择 LLM 生成的多个回答中信任分数最高的一个，以获得更准确的回答，无需任何额外训练步骤。
</details></li>
</ul>
<hr>
<h2 id="Algebraic-Topological-and-Mereological-Foundations-of-Existential-Granules"><a href="#Algebraic-Topological-and-Mereological-Foundations-of-Existential-Granules" class="headerlink" title="Algebraic, Topological, and Mereological Foundations of Existential Granules"></a>Algebraic, Topological, and Mereological Foundations of Existential Granules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16157">http://arxiv.org/abs/2308.16157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mani A</li>
<li>for: 本研究提出了新的存在主义 грануル（EG）概念，用于自我determination和环境互动。</li>
<li>methods: 本研究采用了 алгебраические、topological和merological方面的Characterization来描述EG。</li>
<li>results: 研究显示EG可以适应多种理论框架（axioms, adaptive等），并可以应用于分类问题和可能的总结扩展。 Additionally, many open problems are posed and directions provided.<details>
<summary>Abstract</summary>
In this research, new concepts of existential granules that determine themselves are invented, and are characterized from algebraic, topological, and mereological perspectives. Existential granules are those that determine themselves initially, and interact with their environment subsequently. Examples of the concept, such as those of granular balls, though inadequately defined, algorithmically established, and insufficiently theorized in earlier works by others, are already used in applications of rough sets and soft computing. It is shown that they fit into multiple theoretical frameworks (axiomatic, adaptive, and others) of granular computing. The characterization is intended for algorithm development, application to classification problems and possible mathematical foundations of generalizations of the approach. Additionally, many open problems are posed and directions provided.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们发明了新的存在ential granule概念，它们自己决定了自己的性质。这些granule从算术、拓扑和merkolojical的视角来 caracterized。存在ential granule是指 initially自己决定的granule，然后与环境交互。例如，granular balls这种概念，虽然在以前的作品中不充分定义、算法确定和理论化不够，但它们已经在粗集和软计算应用中使用。我们显示它们适合多种理论框架（axioms, adaptive等）的granular computing。characterization是为了开发算法、应用到分类问题以及可能的总体方法的数学基础。此外，我们还提出了许多开放问题和方向。Note: "existential granule" is a term I translated as "存在ential granule" in Simplified Chinese, which is a combination of "existential" and "granule".
</details></li>
</ul>
<hr>
<h2 id="Jais-and-Jais-chat-Arabic-Centric-Foundation-and-Instruction-Tuned-Open-Generative-Large-Language-Models"><a href="#Jais-and-Jais-chat-Arabic-Centric-Foundation-and-Instruction-Tuned-Open-Generative-Large-Language-Models" class="headerlink" title="Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models"></a>Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16149">http://arxiv.org/abs/2308.16149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, Eric Xing</li>
<li>for: 这个研究是为了开发一个新的阿拉伯语中心的基础模型和一个基于 instrucion 的大语言模型（LLM）。</li>
<li>methods: 这些模型基于 GPT-3 的解码器只架构，并在混合阿拉伯语和英语文本中进行预训练。它们具有13亿个参数，在阿拉伯语知识和理解方面表现出色，与任何现有的开放阿拉伯语和多语言模型相比，具有明显的优势。</li>
<li>results: 这些模型在英语中也能够与英语中心的开放模型相比，即使只使用了 Much less English data。我们提供了模型训练、调整、安全对齐和评估的详细描述。我们发布了两个开放版本的模型：基础 Jais 模型和基于 instrucion 的 Jais-chat 变体，以促进阿拉伯语 LLM 的研究。可以在 <a target="_blank" rel="noopener" href="https://huggingface.co/inception-mbzuai/jais-13b-chat">https://huggingface.co/inception-mbzuai/jais-13b-chat</a> 上下载。<details>
<summary>Abstract</summary>
We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat
</details>
<details>
<summary>摘要</summary>
我们介绍Jais和Jais-chat，这两个新的阿拉伯语中心基础和指导下的开放生成大语言模型（LLM）。这两个模型基于GPT-3核心Only架构，并在混合阿拉伯语和英语文本中进行预训练，包括不同编程语言的源代码。它们拥有13亿个参数，在阿拉伯语中表现出较好的知识和理解能力，比任何现有的开放阿拉伯语和多语言模型都更出色，根据广泛的评估。此外，这些模型在英语中也能够与英语中心的开放模型相比，即使它们在英语数据上进行了训练。我们提供了模型训练、调整、安全对齐和评估的详细描述。我们发布了两个开放版本的模型——基础Jais模型和指导下的Jais-chat变体——以便促进阿拉伯语LLM的研究。可以在https://huggingface.co/inception-mbzuai/jais-13b-chat上下载。
</details></li>
</ul>
<hr>
<h2 id="LM-Infinite-Simple-On-the-Fly-Length-Generalization-for-Large-Language-Models"><a href="#LM-Infinite-Simple-On-the-Fly-Length-Generalization-for-Large-Language-Models" class="headerlink" title="LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"></a>LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16137">http://arxiv.org/abs/2308.16137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, Sinong Wang</li>
<li>for: 本研究目的是提高Transformer大型自然语言模型（LLM）在不同领域的性能，并在长序列上进行更长的理解和推理。</li>
<li>methods: 本研究使用了Lambda形掩码和距离限制，不需要参数更新或学习，可以在现有的LLM上进行实时长度总结。</li>
<li>results: LM-Infinite可以在ArXiv和OpenWebText2数据集上 Generate fluently和高质量的输出，并且在下游任务中继续工作，而vanilla模型在训练长度以下就会失败。 decoding速度提高2.72倍。<details>
<summary>Abstract</summary>
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) factors contributing to this problem. Inspired by this diagnosis, we propose a simple yet effective solution for on-the-fly length generalization, LM-Infinite, which involves only a $\Lambda$-shaped attention mask and a distance limit while requiring no parameter updates or learning. We find it applicable to a variety of LLMs using relative-position encoding methods. LM-Infinite is computational efficient with $O(n)$ time and space, and demonstrates consistent fluency and generation quality to as long as 32k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decoding speedup. On downstream task such as passkey retrieval, it continues to work on inputs much longer than training lengths where vanilla models fail immediately.
</details>
<details>
<summary>摘要</summary>
Recently, there have been significant advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for more complex tasks, they often need to conduct longer reasoning processes or understand larger contexts. However, when dealing with long sequences, LLMs often experience length generalization failure. Most pre-training schemes only train on sequences of a fixed length (such as 2048 for LLaMa), which can cause LLMs to struggle to generate fluent texts or perform downstream tasks when faced with longer contexts. Common solutions such as fine-tuning on longer corpora can be time-consuming and require careful process design. To more efficiently leverage the generation capacity of existing LLMs, we investigate the main out-of-distribution (OOD) factors contributing to this problem. Inspired by this diagnosis, we propose a simple yet effective solution called LM-Infinite, which involves a $\Lambda$-shaped attention mask and a distance limit, and requires no parameter updates or learning. We find it applicable to a variety of LLMs using relative-position encoding methods. LM-Infinite is computationally efficient with $O(n)$ time and space, and demonstrates consistent fluency and generation quality up to 32k tokens on ArXiv and OpenWebText2 datasets, with a decoding speedup of 2.72x. On downstream tasks such as passkey retrieval, it continues to work on inputs much longer than training lengths, where vanilla models fail immediately.
</details></li>
</ul>
<hr>
<h2 id="CorrEmbed-Evaluating-Pre-trained-Model-Image-Similarity-Efficacy-with-a-Novel-Metric"><a href="#CorrEmbed-Evaluating-Pre-trained-Model-Image-Similarity-Efficacy-with-a-Novel-Metric" class="headerlink" title="CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a Novel Metric"></a>CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a Novel Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16126">http://arxiv.org/abs/2308.16126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karl Audun Kagnes Borgersen, Morten Goodwin, Jivitesh Sharma, Tobias Aasmoe, Mari Leonhardsen, Gro Herredsvela Rørvik</li>
<li>for: 这篇论文是为了评估预训练的计算机视觉模型中的图像嵌入而写的。</li>
<li>methods: 这篇论文使用了一种新的方法 named CorrEmbed，该方法计算图像嵌入中距离与人工生成的标签向量距离之间的相关性。</li>
<li>results: 该方法可以评估预训练的计算机视觉模型中的图像嵌入，并发现一些模型的性能与标签相关性之间存在直线关系。此外，该方法还可以找出具有不同特征的模型。<details>
<summary>Abstract</summary>
Detecting visually similar images is a particularly useful attribute to look to when calculating product recommendations. Embedding similarity, which utilizes pre-trained computer vision models to extract high-level image features, has demonstrated remarkable efficacy in identifying images with similar compositions. However, there is a lack of methods for evaluating the embeddings generated by these models, as conventional loss and performance metrics do not adequately capture their performance in image similarity search tasks.   In this paper, we evaluate the viability of the image embeddings from numerous pre-trained computer vision models using a novel approach named CorrEmbed. Our approach computes the correlation between distances in image embeddings and distances in human-generated tag vectors. We extensively evaluate numerous pre-trained Torchvision models using this metric, revealing an intuitive relationship of linear scaling between ImageNet1k accuracy scores and tag-correlation scores. Importantly, our method also identifies deviations from this pattern, providing insights into how different models capture high-level image features.   By offering a robust performance evaluation of these pre-trained models, CorrEmbed serves as a valuable tool for researchers and practitioners seeking to develop effective, data-driven approaches to similar item recommendations in fashion retail.
</details>
<details>
<summary>摘要</summary>
检测类似图像是一项非常有用的特征，特别是在计算产品推荐时。嵌入相似性，使用预训练的计算机视觉模型提取高级图像特征，已经证明了惊人的效果。然而，没有方法来评估由这些模型生成的嵌入，因为常见的损失和性能指标不能够准确地捕捉图像相似搜索任务中的表现。在这篇论文中，我们评估了许多预训练的Torchvision模型的嵌入，使用一种新的方法 named CorrEmbed。我们的方法计算图像嵌入中距离与人工生成的标签 vector 距离之间的相关性。我们广泛评估了多种预训练的Torchvision模型，发现图像1000分类准确率和标签相关性分数之间存在直线关系。更重要的是，我们的方法还发现了不同模型捕捉高级图像特征的偏差，提供了对发展有效数据驱动方法的深入理解。  By offering a robust performance evaluation of these pre-trained models, CorrEmbed serves as a valuable tool for researchers and practitioners seeking to develop effective, data-driven approaches to similar item recommendations in fashion retail.
</details></li>
</ul>
<hr>
<h2 id="Response-Emergent-analogical-reasoning-in-large-language-models"><a href="#Response-Emergent-analogical-reasoning-in-large-language-models" class="headerlink" title="Response: Emergent analogical reasoning in large language models"></a>Response: Emergent analogical reasoning in large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16118">http://arxiv.org/abs/2308.16118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hodeld/emergent_analogies_llm_fork">https://github.com/hodeld/emergent_analogies_llm_fork</a></li>
<li>paper_authors: Damian Hodel, Jevin West</li>
<li>for: 研究表明大语言模型如GPT-3已经获得了泛化逻辑能力，能够解决各种类比问题。</li>
<li>methods: 作者使用了GPT-3进行实验，测试其在不同类比问题中的能力。</li>
<li>results: 试验结果表明，GPT-3无法解决简单的类比问题， zero-shot 逻辑是一个过度的laim需要更多的证据。<details>
<summary>Abstract</summary>
In their recent Nature Human Behaviour paper, "Emergent analogical reasoning in large language models," (Webb, Holyoak, and Lu, 2023) the authors argue that "large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems." In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve even the easiest variants of the problems presented in the original paper. Zero-shot reasoning is an extraordinary claim that requires extraordinary evidence. We do not see that evidence in our experiments. To strengthen claims of humanlike reasoning such as zero-shot reasoning, it is important that the field develop approaches that rule out data memorization.
</details>
<details>
<summary>摘要</summary>
根据《自然人类行为》杂志（Webb、Holyoak、Lu，2023）的论文，作者认为大语言模型如GPT-3已经获得了zero-shot解决广泛的比喻问题的能力。在这个回应中，我们提供了字符串比喻的counterexample。在我们的测试中，GPT-3无法解决even the easiest variants of the problems presented in the original paper。zero-shot reasoning是一个非凡的声明，需要非凡的证据。我们在实验中没看到这种证据。为了强化人类类似的理解，如zero-shot reasoning，领域应该开发approaches来排除数据记忆。
</details></li>
</ul>
<hr>
<h2 id="survex-an-R-package-for-explaining-machine-learning-survival-models"><a href="#survex-an-R-package-for-explaining-machine-learning-survival-models" class="headerlink" title="survex: an R package for explaining machine learning survival models"></a>survex: an R package for explaining machine learning survival models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16113">http://arxiv.org/abs/2308.16113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Spytek, Mateusz Krzyziński, Sophie Hanna Langbein, Hubert Baniecki, Marvin N. Wright, Przemysław Biecek</li>
<li>for: The paper is written for those who use survival models in biomedical research and healthcare applications, and aims to provide a user-friendly tool to explain the internal operations and prediction rationales of these models.</li>
<li>methods: The paper proposes the survex R package, which applies explainable artificial intelligence techniques to survival models, allowing users to understand and diagnose the models, improve their reliability, and detect biases.</li>
<li>results: The proposed software can provide insights into the decision-making process of survival models, such as variable effects and importances, and can promote transparency and responsibility in sensitive areas like biomedical research and healthcare applications.<details>
<summary>Abstract</summary>
Due to their flexibility and superior performance, machine learning models frequently complement and outperform traditional statistical survival models. However, their widespread adoption is hindered by a lack of user-friendly tools to explain their internal operations and prediction rationales. To tackle this issue, we introduce the survex R package, which provides a cohesive framework for explaining any survival model by applying explainable artificial intelligence techniques. The capabilities of the proposed software encompass understanding and diagnosing survival models, which can lead to their improvement. By revealing insights into the decision-making process, such as variable effects and importances, survex enables the assessment of model reliability and the detection of biases. Thus, transparency and responsibility may be promoted in sensitive areas, such as biomedical research and healthcare applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Grandma-Karl-is-27-years-old-–-research-agenda-for-pseudonymization-of-research-data"><a href="#Grandma-Karl-is-27-years-old-–-research-agenda-for-pseudonymization-of-research-data" class="headerlink" title="Grandma Karl is 27 years old – research agenda for pseudonymization of research data"></a>Grandma Karl is 27 years old – research agenda for pseudonymization of research data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16109">http://arxiv.org/abs/2308.16109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elena Volodina, Simon Dobnik, Therese Lindström Tiedemann, Xuan-Son Vu</li>
<li>for: 本研究旨在探讨 pseudonymization 的应用在不结构化数据中，以保护作者身份和隐私信息。</li>
<li>methods: 本研究提出了一个研究议程，包括对 pseudonymization 对不结构化数据的影响（如可读性和语言评估），以及 pseudonymization 的效果是否能够保护作者身份。</li>
<li>results: 研究计划通过开发 Context-sensitive 算法来检测、标记和替换个人信息，以保护作者身份和隐私信息。这个研究项目在 pseudonymization 方面提供了27年的探索和发展空间。<details>
<summary>Abstract</summary>
Accessibility of research data is critical for advances in many research fields, but textual data often cannot be shared due to the personal and sensitive information which it contains, e.g names or political opinions. General Data Protection Regulation (GDPR) suggests pseudonymization as a solution to secure open access to research data, but we need to learn more about pseudonymization as an approach before adopting it for manipulation of research data. This paper outlines a research agenda within pseudonymization, namely need of studies into the effects of pseudonymization on unstructured data in relation to e.g. readability and language assessment, as well as the effectiveness of pseudonymization as a way of protecting writer identity, while also exploring different ways of developing context-sensitive algorithms for detection, labelling and replacement of personal information in unstructured data. The recently granted project on pseudonymization Grandma Karl is 27 years old addresses exactly those challenges.
</details>
<details>
<summary>摘要</summary>
研究数据的可 accessible性是多个研究领域的进步的关键，但文本数据经常无法被共享，因为它们包含个人敏感信息，如名字或政治意见。欧盟通信标准（GDPR）建议使用 pseudonymization 作为保护开放研究数据的解决方案，但我们需要更多关于 pseudonymization 的研究，以便在处理研究数据时采取有效的保护措施。这篇论文提出了一个关于 pseudonymization 的研究计划，即对不结构化数据中的个人信息进行探测、标记和替换的Context-sensitive算法的开发，以及pseudonymization 对写者身份的保护效果和可读性的影响。这些挑战 precisley 由Recently granted project on pseudonymization Grandma Karl is 27 years old 所解决。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.AI_2023_08_31/" data-id="clmjn91jd001b0j885svs9965" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.LG_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T10:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/cs.LG_2023_08_31/">cs.LG - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Spatial-Graph-Coarsening-Weather-and-Weekday-Prediction-with-London’s-Bike-Sharing-Service-using-GNN"><a href="#Spatial-Graph-Coarsening-Weather-and-Weekday-Prediction-with-London’s-Bike-Sharing-Service-using-GNN" class="headerlink" title="Spatial Graph Coarsening: Weather and Weekday Prediction with London’s Bike-Sharing Service using GNN"></a>Spatial Graph Coarsening: Weather and Weekday Prediction with London’s Bike-Sharing Service using GNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16122">http://arxiv.org/abs/2308.16122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuta Sato, Pak Hei Lam, Shruti Gupta, Fareesah Hussain</li>
<li>for: 预测伦敦天气和工作日期</li>
<li>methods: 使用图ael Neural Network（GNN）模型，新引入（i）图特征 concatenation 算子和（ii）基于地理连续性的图粗化算子“Spatial Graph Coarsening”</li>
<li>results: 模型在验证数据集上出现较好的cross-entropy损失和准确率表现，超过基线模型<details>
<summary>Abstract</summary>
This study introduced the use of Graph Neural Network (GNN) for predicting the weather and weekday of a day in London, from the dataset of Santander Cycles bike-sharing system as a graph classification task. The proposed GNN models newly introduced (i) a concatenation operator of graph features with trained node embeddings and (ii) a graph coarsening operator based on geographical contiguity, namely "Spatial Graph Coarsening". With the node features of land-use characteristics and number of households around the bike stations and graph features of temperatures in the city, our proposed models outperformed the baseline model in cross-entropy loss and accuracy of the validation dataset.
</details>
<details>
<summary>摘要</summary>
这个研究介绍了使用图 neural network (GNN) 预测伦敦的天气和工作日，基于 Santander Cycles 自行车共享系统的图分类任务。我们提出的 GNN 模型新增了（i）图特征 concatenation 操作符和（ii）基于地理邻近性的图粗化操作符，即 "Spatial Graph Coarsening"。使用附近站点的节点特征（包括土地用途特征和周围居民数）和图特征（包括城市气温），我们的提议模型在验证集中的十字环比较和准确率超过了基eline模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.LG_2023_08_31/" data-id="clmjn91ms007r0j885a9c4ko6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.SD_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T15:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.SD_2023_08_30/">cs.SD - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Review-of-Differentiable-Digital-Signal-Processing-for-Music-Speech-Synthesis"><a href="#A-Review-of-Differentiable-Digital-Signal-Processing-for-Music-Speech-Synthesis" class="headerlink" title="A Review of Differentiable Digital Signal Processing for Music &amp; Speech Synthesis"></a>A Review of Differentiable Digital Signal Processing for Music &amp; Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15422">http://arxiv.org/abs/2308.15422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Hayes, Jordie Shier, György Fazekas, Andrew McPherson, Charalampos Saitis</li>
<li>for: 这篇论文主要针对的是音乐和语音合成领域中的差分可控数字信号处理技术的应用。</li>
<li>methods: 该论文使用了差分可控数字信号处理技术，其中包括后退传播和权重调整等方法。</li>
<li>results: 该论文对音乐和语音合成任务进行了评估，并结果表明这些技术可以提高音乐和语音的生成质量。同时，论文还提出了一些未来研究的挑战，如优化症状、真实世界情况下的稳定性和设计决策。<details>
<summary>Abstract</summary>
The term "differentiable digital signal processing" describes a family of techniques in which loss function gradients are backpropagated through digital signal processors, facilitating their integration into neural networks. This article surveys the literature on differentiable audio signal processing, focusing on its use in music & speech synthesis. We catalogue applications to tasks including music performance rendering, sound matching, and voice transformation, discussing the motivations for and implications of the use of this methodology. This is accompanied by an overview of digital signal processing operations that have been implemented differentiably. Finally, we highlight open challenges, including optimisation pathologies, robustness to real-world conditions, and design trade-offs, and discuss directions for future research.
</details>
<details>
<summary>摘要</summary>
“差分可读取数字信号处理”是一家技术集合，其中损失函数导数通过数字信号处理器进行反propagation，以便将其 интегрирова到神经网络中。本文对差分音频信号处理的文献进行了报告，专注于它在音乐与语音合成中的应用。我们列出了各种应用场景，包括音乐演奏渲染、声音匹配和语音转换，并讨论了使用这种方法的动机和影响。此外，我们还提供了对数字信号处理操作的差分实现的概述。最后，我们指出了当前的开放挑战，包括优化症状、对实际 Condition 的Robustness以及设计贸易OFF，并讨论了未来研究的方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.SD_2023_08_30/" data-id="clmjn91o700bf0j886xno5bq1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.CV_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T13:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.CV_2023_08_30/">cs.CV - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="3D-Adversarial-Augmentations-for-Robust-Out-of-Domain-Predictions"><a href="#3D-Adversarial-Augmentations-for-Robust-Out-of-Domain-Predictions" class="headerlink" title="3D Adversarial Augmentations for Robust Out-of-Domain Predictions"></a>3D Adversarial Augmentations for Robust Out-of-Domain Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15479">http://arxiv.org/abs/2308.15479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Lehner, Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Nassir Navab, Benjamin Busam, Federico Tombari<br>for:  This paper aims to improve the generalization of 3D object detection and semantic segmentation models to out-of-domain data.methods: The authors use adversarial examples to augment the training set and improve the models’ robustness to out-of-domain data. They learn a set of vectors that deform the objects in an adversarial fashion while preserving their plausibility.results: The authors show that their approach substantially improves the robustness and generalization of both 3D object detection and 3D semantic segmentation methods to out-of-domain data, achieving better performance on a variety of scenarios using data from KITTI, Waymo, and CrashD for object detection, and data from SemanticKITTI, Waymo, and nuScenes for semantic segmentation.Here’s the simplified Chinese text for the three key points:for: 这篇论文目标是提高3D物体检测和 semantic segmentation 模型对非标型数据的泛化性。methods: 作者使用对抗示例来增强训练集，以提高模型对非标型数据的Robustness。他们学习了一组扭曲物体的vector，以 preserve their plausibility。results: 作者表明，他们的方法可以大幅提高3D物体检测和 semantic segmentation 模型对非标型数据的泛化性，在不同场景下，使用KITTI、Waymo和CrashD数据集进行3D物体检测，以及使用SemanticKITTI、Waymo和nuScenes数据集进行semantic segmentation，并且在训练使用标准单个数据集，而不是使用多个数据集。<details>
<summary>Abstract</summary>
Since real-world training datasets cannot properly sample the long tail of the underlying data distribution, corner cases and rare out-of-domain samples can severely hinder the performance of state-of-the-art models. This problem becomes even more severe for dense tasks, such as 3D semantic segmentation, where points of non-standard objects can be confidently associated to the wrong class. In this work, we focus on improving the generalization to out-of-domain data. We achieve this by augmenting the training set with adversarial examples. First, we learn a set of vectors that deform the objects in an adversarial fashion. To prevent the adversarial examples from being too far from the existing data distribution, we preserve their plausibility through a series of constraints, ensuring sensor-awareness and shapes smoothness. Then, we perform adversarial augmentation by applying the learned sample-independent vectors to the available objects when training a model. We conduct extensive experiments across a variety of scenarios on data from KITTI, Waymo, and CrashD for 3D object detection, and on data from SemanticKITTI, Waymo, and nuScenes for 3D semantic segmentation. Despite training on a standard single dataset, our approach substantially improves the robustness and generalization of both 3D object detection and 3D semantic segmentation methods to out-of-domain data.
</details>
<details>
<summary>摘要</summary>
自实际训练数据集不能正确采样下游数据分布的长尾，因此角落情况和罕见的非预训练数据样本会严重影响当前最佳模型的性能。这个问题在某些笔直的任务上，如3D语义分割，变得更加严重，因为非标准对象的点可以坚定地归类到错误的类型上。在这种情况下，我们关注提高对非预训练数据的泛化。我们实现这一目标通过在训练集中添加对抗示例来实现。首先，我们学习一组可以妄图对象的变形向量。为了保证对抗示例不过于远离现有数据分布，我们保留其可能性通过一系列约束，包括感知器和形状的平滑性。然后，我们通过应用学习的样本独立向量来对可用的对象进行对抗增强。我们在各种场景下进行了广泛的实验，包括KITTI、Waymo和CrashD上的3D物体检测，以及SemanticKITTI、Waymo和nuScenes上的3D语义分割。尽管我们只使用了标准单个数据集进行训练，但我们的方法可以很大程度上提高3D物体检测和3D语义分割方法对于非预训练数据的泛化性和Robustness。
</details></li>
</ul>
<hr>
<h2 id="An-Adaptive-Tangent-Feature-Perspective-of-Neural-Networks"><a href="#An-Adaptive-Tangent-Feature-Perspective-of-Neural-Networks" class="headerlink" title="An Adaptive Tangent Feature Perspective of Neural Networks"></a>An Adaptive Tangent Feature Perspective of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15478">http://arxiv.org/abs/2308.15478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel LeJeune, Sina Alemohammad</li>
<li>for: 了解神经网络中特征学习的机制</li>
<li>methods: 使用线性模型在抽象特征空间进行学习，并在训练过程中允许特征进行变换</li>
<li>results: 提出了一种基于线性变换的特征学习框架，并证明该框架在神经网络中可以提供更多的特征学习细节，以及一种适应特征实现的抽象特征分类方法可以在MNIST和CIFAR-10上具有许多 órders of magnitude 的采样复杂性优势。<details>
<summary>Abstract</summary>
In order to better understand feature learning in neural networks, we propose a framework for understanding linear models in tangent feature space where the features are allowed to be transformed during training. We consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions. Specializing to neural network structure, we gain insights into how the features and thus the kernel function change, providing additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. In addition to verifying our theoretical observations in real neural networks on a simple regression problem, we empirically show that an adaptive feature implementation of tangent feature classification has an order of magnitude lower sample complexity than the fixed tangent feature model on MNIST and CIFAR-10.
</details>
<details>
<summary>摘要</summary>
为了更好地理解神经网络中的特征学习，我们提出了一个框架来理解在斜缩Feature空间中的线性模型。我们考虑了特征的线性变换，从而导致参数和变换的共同优化问题，其中包括bilinear插值约束。我们表明这个优化问题有相应的线性约束优化问题，并且具有结构化正则化，以鼓励约束低维解决方案。在神经网络结构下，我们获得了特征和几何函数的变化，从而提供了特征对kernel函数的影响的更多的准确信息。此外，我们还证明了在实际神经网络中，适用于拟合特征的tanent特征分类模型具有训练样本的一个数量级减少。
</details></li>
</ul>
<hr>
<h2 id="Learning-Modulated-Transformation-in-GANs"><a href="#Learning-Modulated-Transformation-in-GANs" class="headerlink" title="Learning Modulated Transformation in GANs"></a>Learning Modulated Transformation in GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15472">http://arxiv.org/abs/2308.15472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ceyuan Yang, Qihang Zhang, Yinghao Xu, Jiapeng Zhu, Yujun Shen, Bo Dai</li>
<li>for: 提高 generative adversarial networks (GANs) 的模型灵活性和可重用性，以便更好地处理各种生成任务，包括图像生成、3D-aware图像生成和视频生成。</li>
<li>methods: 提出一种名为 modulated transformation module (MTM) 的插件模块，该模块可以预测空间偏移，并根据隐藏码来控制变量位置，以便模型更好地处理几何变换。</li>
<li>results: 在多种生成任务上进行了广泛的实验，并证明了该方法可以与当前的状态略进行无需任何参数调整。特别是，在人体生成 tasks 上，我们提高了 StyleGAN3 的 FID 值从 21.36 下降至 13.60， demonstrate 了学习模ulated geometry transformation 的能力。<details>
<summary>Abstract</summary>
The success of style-based generators largely benefits from style modulation, which helps take care of the cross-instance variation within data. However, the instance-wise stochasticity is typically introduced via regular convolution, where kernels interact with features at some fixed locations, limiting its capacity for modeling geometric variation. To alleviate this problem, we equip the generator in generative adversarial networks (GANs) with a plug-and-play module, termed as modulated transformation module (MTM). This module predicts spatial offsets under the control of latent codes, based on which the convolution operation can be applied at variable locations for different instances, and hence offers the model an additional degree of freedom to handle geometry deformation. Extensive experiments suggest that our approach can be faithfully generalized to various generative tasks, including image generation, 3D-aware image synthesis, and video generation, and get compatible with state-of-the-art frameworks without any hyper-parameter tuning. It is noteworthy that, towards human generation on the challenging TaiChi dataset, we improve the FID of StyleGAN3 from 21.36 to 13.60, demonstrating the efficacy of learning modulated geometry transformation.
</details>
<details>
<summary>摘要</summary>
成功的风格基本生成器主要受益于风格调整，它可以处理数据中的跨实例变化。然而，实例具有的随机性通常通过常规 convolution 引入，其中核函数与特征在固定位置相互作用，限制模型的形态变换能力。为解决这个问题，我们在生成对抗网络（GANs）中增加了可替换模块，称为模ulated transformation module（MTM）。这个模块根据隐藏代码预测空间偏移，并基于这些偏移进行变量位置的 convolution 操作，从而为模型增加了一个额外的自由度来处理形态变换。我们的方法可以广泛应用于不同的生成任务，包括图像生成、三维感知图像合成和视频生成，并与当前最佳框架相容无需任何超参数调整。特别是，我们在挑战性的 TaiChi 数据集上进行人体生成 task 时，提高了 StyleGAN3 的 FID 从 21.36 下降至 13.60，这表明我们学习了模ulated geometry transformation 的能力。
</details></li>
</ul>
<hr>
<h2 id="Input-margins-can-predict-generalization-too"><a href="#Input-margins-can-predict-generalization-too" class="headerlink" title="Input margins can predict generalization too"></a>Input margins can predict generalization too</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15466">http://arxiv.org/abs/2308.15466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Coenraad Mouton, Marthinus W. Theunissen, Marelie H. Davel</li>
<li>for:  investigate the relationship between generalization and classification margins in deep neural networks</li>
<li>methods:  use margin measurements, specifically constrained margins, to predict generalization ability</li>
<li>results:  constrained margins achieve highly competitive scores and outperform other margin measurements in general, providing a novel insight into the relationship between generalization and classification margins.<details>
<summary>Abstract</summary>
Understanding generalization in deep neural networks is an active area of research. A promising avenue of exploration has been that of margin measurements: the shortest distance to the decision boundary for a given sample or its representation internal to the network. While margins have been shown to be correlated with the generalization ability of a model when measured at its hidden representations (hidden margins), no such link between large margins and generalization has been established for input margins. We show that while input margins are not generally predictive of generalization, they can be if the search space is appropriately constrained. We develop such a measure based on input margins, which we refer to as `constrained margins'. The predictive power of this new measure is demonstrated on the 'Predicting Generalization in Deep Learning' (PGDL) dataset and contrasted with hidden representation margins. We find that constrained margins achieve highly competitive scores and outperform other margin measurements in general. This provides a novel insight on the relationship between generalization and classification margins, and highlights the importance of considering the data manifold for investigations of generalization in DNNs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-Overexposed-Pixels-Hallucination-in-Videos-with-Adaptive-Reference-Frame-Selection"><a href="#Online-Overexposed-Pixels-Hallucination-in-Videos-with-Adaptive-Reference-Frame-Selection" class="headerlink" title="Online Overexposed Pixels Hallucination in Videos with Adaptive Reference Frame Selection"></a>Online Overexposed Pixels Hallucination in Videos with Adaptive Reference Frame Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15462">http://arxiv.org/abs/2308.15462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yazhou Xing, Amrita Mazumdar, Anjul Patney, Chao Liu, Hongxu Yin, Qifeng Chen, Jan Kautz, Iuri Frosio</li>
<li>for: 解决LDR相机无法处理宽动态范围输入的问题，提高图像质量。</li>
<li>methods: 使用变换器基于深度神经网络（DNN）推断缺失HDR细节。在减少参数学习中，使用多尺度DNN和适当的成本函数来实现状态艺术质量。 Additionally, using a reference frame from the past as an additional input to aid the reconstruction of overexposed areas.</li>
<li>results: 在减少参数学习中，使用这种方法可以获得状态艺术质量，而不需要使用复杂的获取机制或高Dynamic范围成像处理。我们的示例视频可以在<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view中找到。</a><details>
<summary>Abstract</summary>
Low dynamic range (LDR) cameras cannot deal with wide dynamic range inputs, frequently leading to local overexposure issues. We present a learning-based system to reduce these artifacts without resorting to complex acquisition mechanisms like alternating exposures or costly processing that are typical of high dynamic range (HDR) imaging. We propose a transformer-based deep neural network (DNN) to infer the missing HDR details. In an ablation study, we show the importance of using a multiscale DNN and train it with the proper cost function to achieve state-of-the-art quality. To aid the reconstruction of the overexposed areas, our DNN takes a reference frame from the past as an additional input. This leverages the commonly occurring temporal instabilities of autoexposure to our advantage: since well-exposed details in the current frame may be overexposed in the future, we use reinforcement learning to train a reference frame selection DNN that decides whether to adopt the current frame as a future reference. Without resorting to alternating exposures, we obtain therefore a causal, HDR hallucination algorithm with potential application in common video acquisition settings. Our demo video can be found at https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view
</details>
<details>
<summary>摘要</summary>
低动态范围（LDR）摄像机不能处理宽动态范围输入，导致本地过度曝光问题。我们提出了一种学习基于的系统，以减少这些缺陷而不需要复杂的获取机制如alternating exposures或高动态范围（HDR）拍摄。我们提议使用 transformer 基于的深度神经网络（DNN）来推理缺失 HDR 细节。在一个ablation study中，我们表明了使用多尺度 DNN 和适当的成本函数以 дости得状态的质量。为了重建过度曝光的区域，我们的 DNN 接受了过去的参考帧作为额外输入。这样利用了自动曝光的 temporal 不稳定性，我们使用 reinforcement learning 来训练参考帧选择 DNN，以确定是否采用当前帧作为未来的参考。无需alternating exposures，我们得到了一个 causal、HDR 幻化算法，可能在常见的视频拍摄设置中应用。我们的 demo 视频可以在 <https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view> 找到。
</details></li>
</ul>
<hr>
<h2 id="Canonical-Factors-for-Hybrid-Neural-Fields"><a href="#Canonical-Factors-for-Hybrid-Neural-Fields" class="headerlink" title="Canonical Factors for Hybrid Neural Fields"></a>Canonical Factors for Hybrid Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15461">http://arxiv.org/abs/2308.15461</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brentyi/tilted">https://github.com/brentyi/tilted</a></li>
<li>paper_authors: Brent Yi, Weijia Zeng, Sam Buchanan, Yi Ma</li>
<li>for: 本文主要针对的问题是射线对齐信号的抽象特征量化方法引入的不良偏见问题，并提出一种解决方法。</li>
<li>methods: 本文使用了学习一组均衡变换的方法，以消除这些偏见。</li>
<li>results: 实验结果表明，使用这种方法可以提高图像、签名距离和辐射场重建质量、稳定性、压缩率和运行时间。<details>
<summary>Abstract</summary>
Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesirable biases that these architectures have for axis-aligned signals -- they can lead to radiance field reconstruction differences of as high as 2 PSNR -- and (2) explore how learning a set of canonicalizing transformations can improve representations by removing these biases. We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, where we observe improvements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable capabilities comparable to baselines that are 2x larger, while highlighting weaknesses of neural field evaluation procedures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Characterize the undesirable biases that these architectures have for axis-aligned signals, which can lead to radiance field reconstruction differences of up to 2 PSNR.2. Explore how learning a set of canonicalizing transformations can improve representations by removing these biases.We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance can be done with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, and observe improvements in quality, robustness, compactness, and runtime. Our results show that TILTED can enable capabilities comparable to baselines that are 2x larger, while highlighting weaknesses of neural field evaluation procedures.</details></li>
</ol>
<hr>
<h2 id="Pseudo-Boolean-Polynomials-Approach-To-Edge-Detection-And-Image-Segmentation"><a href="#Pseudo-Boolean-Polynomials-Approach-To-Edge-Detection-And-Image-Segmentation" class="headerlink" title="Pseudo-Boolean Polynomials Approach To Edge Detection And Image Segmentation"></a>Pseudo-Boolean Polynomials Approach To Edge Detection And Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15453">http://arxiv.org/abs/2308.15453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tendai Mapungwana Chikake, Boris Goldengorin, Alexey Samosyuk</li>
<li>for: 用于图像Edge检测和分割</li>
<li>methods: 使用pseudo-Boolean波尔次数计算image patches，进行二分类 blob和边区域的分类</li>
<li>results: 在简单图像中成功实现Edge检测和分割，并在复杂图像中进行应用In English, this translates to:</li>
<li>for: Used for image edge detection and segmentation</li>
<li>methods: Using pseudo-Boolean polynomials calculated on image patches for binary classification of blob and edge regions</li>
<li>results: Successfully implemented edge detection and segmentation on simple images and applied to complex images like aerial landscapes<details>
<summary>Abstract</summary>
We introduce a deterministic approach to edge detection and image segmentation by formulating pseudo-Boolean polynomials on image patches. The approach works by applying a binary classification of blob and edge regions in an image based on the degrees of pseudo-Boolean polynomials calculated on patches extracted from the provided image. We test our method on simple images containing primitive shapes of constant and contrasting colour and establish the feasibility before applying it to complex instances like aerial landscape images. The proposed method is based on the exploitation of the reduction, polynomial degree, and equivalence properties of penalty-based pseudo-Boolean polynomials.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种权值Deterministic逻辑来实现图像边检测和分割，通过在图像块上计算 pseudo-Boolean 多项式。该方法基于对图像块上的二分类，将图像分为 blob 和边区域，并基于计算的 pseudo-Boolean 多项式度量来进行分类。我们在简单的图像中使用了固定颜色和对比度的基本形状进行测试，并证明了该方法的可行性。然后，我们将该方法应用于复杂的飞行图像。该方法基于 pseudo-Boolean 多项式的减少、度量和等价性属性的利用。
</details></li>
</ul>
<hr>
<h2 id="WrappingNet-Mesh-Autoencoder-via-Deep-Sphere-Deformation"><a href="#WrappingNet-Mesh-Autoencoder-via-Deep-Sphere-Deformation" class="headerlink" title="WrappingNet: Mesh Autoencoder via Deep Sphere Deformation"></a>WrappingNet: Mesh Autoencoder via Deep Sphere Deformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15413">http://arxiv.org/abs/2308.15413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Lei, Muhammad Asad Lodhi, Jiahao Pang, Junghyun Ahn, Dong Tian</li>
<li>for: 本研究旨在实现基于 mesh 数据的无监督学习，以便学习更有意义的表示。</li>
<li>methods: 本文提出了一种基于 bottleneck 的 mesh autoencoder，通过专门 Representing mesh 连接情况的基本图来促进学习共享的 latent 空间表示对象形状。</li>
<li>results: 对比点云学习，WRAPPINGNET 可以提供更高质量的重建和竞争性的分类结果，同时可以进行不同类型对象之间的 latent  interpolate。<details>
<summary>Abstract</summary>
There have been recent efforts to learn more meaningful representations via fixed length codewords from mesh data, since a mesh serves as a complete model of underlying 3D shape compared to a point cloud. However, the mesh connectivity presents new difficulties when constructing a deep learning pipeline for meshes. Previous mesh unsupervised learning approaches typically assume category-specific templates, e.g., human face/body templates. It restricts the learned latent codes to only be meaningful for objects in a specific category, so the learned latent spaces are unable to be used across different types of objects. In this work, we present WrappingNet, the first mesh autoencoder enabling general mesh unsupervised learning over heterogeneous objects. It introduces a novel base graph in the bottleneck dedicated to representing mesh connectivity, which is shown to facilitate learning a shared latent space representing object shape. The superiority of WrappingNet mesh learning is further demonstrated via improved reconstruction quality and competitive classification compared to point cloud learning, as well as latent interpolation between meshes of different categories.
</details>
<details>
<summary>摘要</summary>
有些最近的努力是通过固定长度代码Word来学习更有意义的表示，从网格数据中得到更多的信息，因为网格作为三维形态的完整模型，比点云更有优势。然而，网格连接会对深度学习管道的构建带来新的挑战。以前的无监督学习方法通常假设特定类别的模板，例如人脸/身体模板。这限制学习的幂等空间只能对特定类别的对象进行有意义的学习，因此学习的幂等空间无法在不同类别的对象之间进行使用。在这种工作中，我们介绍了WrappingNet，首个能够进行总体网格无监督学习的自动编码器。它引入了瓶颈部分的新基graph，用于表示网格连接，这被证明可以促进学习对象形状的共享 latent space。我们通过对网格学习和点云学习进行比较，以及在不同类别的网格之间进行 latent  interpolate 等方法来证明 WrappingNet 的优越性。
</details></li>
</ul>
<hr>
<h2 id="Robust-Long-Tailed-Learning-via-Label-Aware-Bounded-CVaR"><a href="#Robust-Long-Tailed-Learning-via-Label-Aware-Bounded-CVaR" class="headerlink" title="Robust Long-Tailed Learning via Label-Aware Bounded CVaR"></a>Robust Long-Tailed Learning via Label-Aware Bounded CVaR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15405">http://arxiv.org/abs/2308.15405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Zhu, Runpeng Yu, Xing Tang, Yifei Wang, Yuan Fang, Yisen Wang</li>
<li>for: 实际世界中的核心类别问题 often 会出现不对称或长尾分布，导致模型训练时对少数类别表现差。这种情况下，单简的模型通常对少数类别表现不佳。</li>
<li>methods: 本文提出了两种基于CVaR（Conditional Value at Risk）的新方法来改善长尾学习的表现，并提供了严谨的理论保证。特别是，我们首先引入了Label-Aware Bounded CVaR（LAB-CVaR）损失函数，以解决原始CVaR的偏预结果问题，然后设计了LAB-CVaR的最佳质量上限。基于LAB-CVaR，我们还提出了LAB-CVaR with logit adjustment（LAB-CVaR-logit）损失函数，并提供了理论支持。</li>
<li>results: 实际实验结果显示，我们的提案方法在实际世界中的长尾标签分布下表现出色，较以单简的模型表现更好。<details>
<summary>Abstract</summary>
Data in the real-world classification problems are always imbalanced or long-tailed, wherein the majority classes have the most of the samples that dominate the model training. In such setting, the naive model tends to have poor performance on the minority classes. Previously, a variety of loss modifications have been proposed to address the long-tailed leaning problem, while these methods either treat the samples in the same class indiscriminatingly or lack a theoretical guarantee. In this paper, we propose two novel approaches based on CVaR (Conditional Value at Risk) to improve the performance of long-tailed learning with a solid theoretical ground. Specifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss to overcome the pessimistic result of the original CVaR, and further design the optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to stabilize the optimization process, where we also offer the theoretical support. Extensive experiments on real-world datasets with long-tailed label distributions verify the superiority of our proposed methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate given text into Simplified Chinese.<</SYS>>世界上的实际分类问题中的数据总是偏斜或长尾分布，其中多数类占据了模型训练中的大多数样本。在这种情况下，简单的模型通常会对少数类表现不佳。先前，一些损失修改方法已经被提出，但这些方法可能会对同一类的样本待遇不公平，或者缺乏理论保证。在本文中，我们提出了两种基于CVaR（Conditional Value at Risk）的新方法，以改进长尾学习的性能，并提供了坚实的理论基础。 Specifically, we first introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss to overcome the pessimistic result of the original CVaR, and further design the optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to stabilize the optimization process, where we also offer the theoretical support. 实际上，我们对实际上的长尾标签分布进行了广泛的实验，并证明了我们提出的方法的优越性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.CV_2023_08_30/" data-id="clmjn91kk00490j885lgr7ion" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.AI_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T12:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.AI_2023_08_30/">cs.AI - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-General-Purpose-Self-Supervised-Model-for-Computational-Pathology"><a href="#A-General-Purpose-Self-Supervised-Model-for-Computational-Pathology" class="headerlink" title="A General-Purpose Self-Supervised Model for Computational Pathology"></a>A General-Purpose Self-Supervised Model for Computational Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15474">http://arxiv.org/abs/2308.15474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard J. Chen, Tong Ding, Ming Y. Lu, Drew F. K. Williamson, Guillaume Jaume, Bowen Chen, Andrew Zhang, Daniel Shao, Andrew H. Song, Muhammad Shaban, Mane Williams, Anurag Vaidya, Sharifa Sahai, Lukas Oldenburg, Luca L. Weishaupt, Judy J. Wang, Walt Williams, Long Phi Le, Georg Gerber, Faisal Mahmood</li>
<li>for: 本研究旨在提出一种通用的自助学习模型，用于解决生物病理学图像分类和诊断问题。</li>
<li>methods: 该模型使用了100万个各种组织类型的病理图像补充，并通过自助学习方法进行预训练。</li>
<li>results: 该模型在33种不同的临床任务中表现出色，包括分类、诊断和疾病类型划分等。此外，模型还能够在各种组织类型和诊断难度不同的情况下进行泛化和转移。<details>
<summary>Abstract</summary>
Tissue phenotyping is a fundamental computational pathology (CPath) task in learning objective characterizations of histopathologic biomarkers in anatomic pathology. However, whole-slide imaging (WSI) poses a complex computer vision problem in which the large-scale image resolutions of WSIs and the enormous diversity of morphological phenotypes preclude large-scale data annotation. Current efforts have proposed using pretrained image encoders with either transfer learning from natural image datasets or self-supervised pretraining on publicly-available histopathology datasets, but have not been extensively developed and evaluated across diverse tissue types at scale. We introduce UNI, a general-purpose self-supervised model for pathology, pretrained using over 100 million tissue patches from over 100,000 diagnostic haematoxylin and eosin-stained WSIs across 20 major tissue types, and evaluated on 33 representative CPath clinical tasks in CPath of varying diagnostic difficulties. In addition to outperforming previous state-of-the-art models, we demonstrate new modeling capabilities in CPath such as resolution-agnostic tissue classification, slide classification using few-shot class prototypes, and disease subtyping generalization in classifying up to 108 cancer types in the OncoTree code classification system. UNI advances unsupervised representation learning at scale in CPath in terms of both pretraining data and downstream evaluation, enabling data-efficient AI models that can generalize and transfer to a gamut of diagnostically-challenging tasks and clinical workflows in anatomic pathology.
</details>
<details>
<summary>摘要</summary>
组织现象评估是computational pathology（CPath）的基本任务，它的目标是通过学习标注组织生物marker的Objective characterizations，以帮助诊断医学。然而，整个标本影像（WSI）对于计算机视觉问题而言是一个复杂的问题，因为标本影像的大规模分辨率和丰富多样性的形态现象对于大规模标注数据的生成提供了一定的挑战。目前的努力已经提议使用预训归数图像Encoder， either transfer learning from natural image datasets or self-supervised pretraining on publicly-available histopathology datasets，但这些努力尚未得到了广泛的发展和评估。我们提出了UNI，一个通用的自主学习模型，预训使用了1000万个组织小图像，来自100000多个诊断HE染色标本影像，并在20个主要组织类型上进行了33个CPath临床任务的评估。此外，我们还展示了一些新的模型化能力，例如resolution-agnostic tissue classification、slides classification using few-shot class prototypes、和疾病分类对108种癌症的分类。UNI在CPath中进行了无监督学习的扩展，并在这些任务上实现了资料效率的AI模型，可以对诊断挑战性任务和临床工作流程进行普遍和转移。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Contrastive-Learning-and-Tabular-Attention-for-Automated-Alzheimer’s-Disease-Prediction"><a href="#Multimodal-Contrastive-Learning-and-Tabular-Attention-for-Automated-Alzheimer’s-Disease-Prediction" class="headerlink" title="Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer’s Disease Prediction"></a>Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer’s Disease Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15469">http://arxiv.org/abs/2308.15469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weichen Huang</li>
<li>for: 这个研究旨在开发一个多 modal 对照学习框架，以利用 MRI 扫描和 PET 等神经成像数据，并处理 AD 疾病数据中的值得注意的表格资料。</li>
<li>methods: 这个框架使用了一个新的表格注意模组，可以强调和排名表格中的重要特征。它还使用了多 modal 对照学习技术，以将图像和表格资料结合在一起。</li>
<li>results: 实验结果显示，这个框架可以从 ADNI 数据库中的逾 882 个 MRI 扫描标本中检测出 AD 疾病，并且可以实现高于 83.8% 的准确率，与前一代的州度优化技术相比，提高了约 10%。<details>
<summary>Abstract</summary>
Alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD) datasets contain valuable tabular data including AD biomarkers and clinical assessments. Existing computer vision approaches struggle to utilize this additional information. To address these needs, we propose a generalizable framework for multimodal contrastive learning of image data and tabular data, a novel tabular attention module for amplifying and ranking salient features in tables, and the application of these techniques onto Alzheimer's disease prediction. Experimental evaulations demonstrate the strength of our framework by detecting Alzheimer's disease (AD) from over 882 MR image slices from the ADNI database. We take advantage of the high interpretability of tabular data and our novel tabular attention approach and through attribution of the attention scores for each row of the table, we note and rank the most predominant features. Results show that the model is capable of an accuracy of over 83.8%, almost a 10% increase from previous state of the art.
</details>
<details>
<summary>摘要</summary>
alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD) datasets contain valuable tabular data including AD biomarkers and clinical assessments. Existing computer vision approaches struggle to utilize this additional information. To address these needs, we propose a generalizable framework for multimodal contrastive learning of image data and tabular data, a novel tabular attention module for amplifying and ranking salient features in tables, and the application of these techniques onto Alzheimer's disease prediction. Experimental evaulations demonstrate the strength of our framework by detecting Alzheimer's disease (AD) from over 882 MR image slices from the ADNI database. We take advantage of the high interpretability of tabular data and our novel tabular attention approach and through attribution of the attention scores for each row of the table, we note and rank the most predominant features. Results show that the model is capable of an accuracy of over 83.8%, almost a 10% increase from previous state of the art.Here's the translation in Traditional Chinese:附加了脑成像技术，如MRI扫描和PET，Alzheimer病（AD）数据集包含重要的表格资料，包括AD标识和临床评估。现有的计算机视觉方法对这些额外资讯难以使用。为解决这些需求，我们提出了一个通用的多modal对比学习框架，一个新的表格注意模组，以强调和排名表格中的重要特征。我们还应用了这些技术 onto Alzheimer病预测。实验评估显示了我们的框架在ADNI数据库中的882个MRI图像探针中检测到Alzheimer病的能力，比前一代的state of the art约10%高。我们利用表格资料的高解释性和我们的新的表格注意方法，通过每行表格中的注意分数汇总，发现和排名表格中的最主要特征。结果显示模型可以达到83.8%的精度，比前一代的state of the art约10%高。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Loss-Functions-Traffic-Predictions-in-Regular-and-Congestion-Scenarios"><a href="#A-Comparative-Study-of-Loss-Functions-Traffic-Predictions-in-Regular-and-Congestion-Scenarios" class="headerlink" title="A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios"></a>A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15464">http://arxiv.org/abs/2308.15464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xieyangxinyu/a-comparative-study-of-loss-functions-traffic-predictions-in-regular-and-congestion-scenarios">https://github.com/xieyangxinyu/a-comparative-study-of-loss-functions-traffic-predictions-in-regular-and-congestion-scenarios</a></li>
<li>paper_authors: Yangxinyu Xie, Tanwi Mallick</li>
<li>for: 这 paper 的目的是提高深度学习模型在交通预测中的精度，特别是预测堵塞情况。</li>
<li>methods: 这 paper 使用了多种积分函数，包括 MAE-Focal Loss 和 Gumbel Loss，来解决传统损失函数的局限性。</li>
<li>results: 经过大规模实验，这 paper 发现 MAE-Focal Loss 和 Gumbel Loss 在预测交通速度方面具有最高效果，能够准确预测堵塞情况而不会妨碍正常交通预测。<details>
<summary>Abstract</summary>
Spatiotemporal graph neural networks have achieved state-of-the-art performance in traffic forecasting. However, they often struggle to forecast congestion accurately due to the limitations of traditional loss functions. While accurate forecasting of regular traffic conditions is crucial, a reliable AI system must also accurately forecast congestion scenarios to maintain safe and efficient transportation. In this paper, we explore various loss functions inspired by heavy tail analysis and imbalanced classification problems to address this issue. We evaluate the efficacy of these loss functions in forecasting traffic speed, with an emphasis on congestion scenarios. Through extensive experiments on real-world traffic datasets, we discovered that when optimizing for Mean Absolute Error (MAE), the MAE-Focal Loss function stands out as the most effective. When optimizing Mean Squared Error (MSE), Gumbel Loss proves to be the superior choice. These choices effectively forecast traffic congestion events without compromising the accuracy of regular traffic speed forecasts. This research enhances deep learning models' capabilities in forecasting sudden speed changes due to congestion and underscores the need for more research in this direction. By elevating the accuracy of congestion forecasting, we advocate for AI systems that are reliable, secure, and resilient in practical traffic management scenarios.
</details>
<details>
<summary>摘要</summary>
现代交通预测中使用的空间时间图 neural network 已经达到了领先的性能水平。然而，它们经常因传统的损失函数的局限性而难以准确预测堵塞情况。尽管正确预测常规交通情况是非常重要，但一个可靠的 AI 系统也必须准确预测堵塞场景，以保证安全和高效的交通运输。在这篇论文中，我们探讨了各种启发自重态分析和不均衡分类问题的损失函数，以解决这一问题。我们对这些损失函数在预测交通速度方面的效果进行了广泛的实验，发现了使用 MAE-Focal Loss 函数时，MAE 函数在预测堵塞场景中表现最佳。使用 MSE 函数时，Gumbel Loss 函数表现最佳。这些选择可以准确预测交通堵塞事件，不会 compromise 正常交通速度预测的准确性。这项研究提高了深度学习模型在预测快速变化的能力，并强调了对堵塞预测的需求。我们认为，通过提高堵塞预测的准确性，可以建立可靠、安全、可靠的 AI 系统，以满足实际交通管理场景中的需求。
</details></li>
</ul>
<hr>
<h2 id="ParaGuide-Guided-Diffusion-Paraphrasers-for-Plug-and-Play-Textual-Style-Transfer"><a href="#ParaGuide-Guided-Diffusion-Paraphrasers-for-Plug-and-Play-Textual-Style-Transfer" class="headerlink" title="ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer"></a>ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15459">http://arxiv.org/abs/2308.15459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zacharyhorvitz/ParaGuide">https://github.com/zacharyhorvitz/ParaGuide</a></li>
<li>paper_authors: Zachary Horvitz, Ajay Patel, Chris Callison-Burch, Zhou Yu, Kathleen McKeown</li>
<li>for: 文章的目的是将文本的风格特征转换为新的风格，保留 semantic information。</li>
<li>methods: 本研究使用了一个新的扩散基础架构，叫做 ParaGuide，可以灵活地适应任意目标风格。这个方法使用了句子重写条件下的扩散模型，以及Gradient-based guidance from both off-the-shelf classifiers和强大的现有风格嵌入。</li>
<li>results: 研究在Enron Email Corpus上进行了人工和自动评估，和强大的基eline均以上。它可以成功地将文本的风格转换为新的风格，保留 semantic information。<details>
<summary>Abstract</summary>
Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, sentiment, and even authorship style transfer.
</details>
<details>
<summary>摘要</summary>
文本风格转换是将文本的风格属性转换为另一种风格的任务，保持意义不变。目标风格可以定义为多种方式，从单一特征（例如正式度）到作者（例如莎士比亚）。前一代无监督风格转换方法通常需要大量标注数据，仅适用于固定的风格集或需要大型语言模型。相比之下，我们介绍了一种新的扩散基于框架，可以通过扩散模型来实现通用风格转换，并在推理时适应任意目标风格。我们的参数有效的方法， ParaGuide，利用了句子重构conditional扩散模型，并与梯度导航从存储类фика器和强有力的现有风格编码器来转换文本的风格，保持 semantic information。我们在恩рон电子邮件集上验证了该方法，并与人类和自动评估表明，它在正式度、情感和作者风格转换方面超过了强大基eline。
</details></li>
</ul>
<hr>
<h2 id="From-SMOTE-to-Mixup-for-Deep-Imbalanced-Classification"><a href="#From-SMOTE-to-Mixup-for-Deep-Imbalanced-Classification" class="headerlink" title="From SMOTE to Mixup for Deep Imbalanced Classification"></a>From SMOTE to Mixup for Deep Imbalanced Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15457">http://arxiv.org/abs/2308.15457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ntucllab/imbalanced-dl">https://github.com/ntucllab/imbalanced-dl</a></li>
<li>paper_authors: Wei-Chao Cheng, Tan-Ha Mai, Hsuan-Tien Lin</li>
<li>for: 本研究旨在探讨深度学习中对异质数据的处理方法，尤其是SMOTE数据增强技术是否有利于深度学习。</li>
<li>methods: 本研究使用了SMOTE技术，以及其扩展版本——软标签SMOTE和混合技术。</li>
<li>results: 研究发现，通过将SMOTE和混合技术结合使用，可以提高深度学习模型的泛化性能，并且在极端异质数据上达到最佳性能。<details>
<summary>Abstract</summary>
Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental results demonstrate that our proposed technique yields state-of-the-art performance on deep imbalanced classification while achieving superior performance on extremely imbalanced data. The code is open-sourced in our developed package https://github.com/ntucllab/imbalanced-DL to foster future research in this direction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-Do-Program-of-Thoughts-Work-for-Reasoning"><a href="#When-Do-Program-of-Thoughts-Work-for-Reasoning" class="headerlink" title="When Do Program-of-Thoughts Work for Reasoning?"></a>When Do Program-of-Thoughts Work for Reasoning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15452">http://arxiv.org/abs/2308.15452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/easyinstruct">https://github.com/zjunlp/easyinstruct</a></li>
<li>paper_authors: Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, Huajun Chen</li>
<li>for: 本研究旨在探讨 Large Language Models (LLMs) 在肢体人工智能领域的逻辑能力如何提高，以及程序语言的影响。</li>
<li>methods: 本研究提出了一种 complexity-impacted reasoning score (CIRS)，用于衡量程序语言的结构和逻辑特性对逻辑能力的影响。CIRS 使用抽象树来编码结构信息，并通过考虑困难性和环状复杂性来计算逻辑复杂性。</li>
<li>results: 研究发现，不同的程序数据复杂性会影响 LLMS 的逻辑能力提升。优化的复杂度是关键的，程序帮助提示可以提高逻辑能力。研究还提出了一种自动生成和分级算法，并应用于数学逻辑和代码生成任务。广泛的结果表明了我们的提出的方法的有效性。<details>
<summary>Abstract</summary>
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing and stratifying algorithm, and apply it to instruction generation for mathematical reasoning and code data filtering for code generation tasks. Extensive results demonstrates the effectiveness of our proposed approach. Code will be integrated into the EasyInstruct framework at https://github.com/zjunlp/EasyInstruct.
</details>
<details>
<summary>摘要</summary>
LLMs的逻辑能力在人工智能中扮演着关键性角色。虽有有效的程序激活提示法 для LLMs，但代码数据对复杂逻辑任务的改进仍然未得到足够的探讨。为解决这个空白，我们提出了复杂性影响逻辑能力指数（CIRS），它将结构性和逻辑性特征相结合，用于衡量代码数据对逻辑能力的相关性。我们使用抽象树来编码结构信息，并根据难度和 cyclomatic complexity来计算逻辑复杂性。经 empirical 分析发现，不 всех复杂度的代码数据可以被 LLMS 学习或理解。优质的复杂度是关键的，以便通过程序帮助提示来提高逻辑能力。然后，我们设计了自动生成和分配算法，并应用它到数学逻辑和代码生成任务中。广泛的结果表明了我们的提出的方法的有效性。代码将会被集成到 EasyInstruct 框架中，可以在 <https://github.com/zjunlp/EasyInstruct> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Complementing-Onboard-Sensors-with-Satellite-Map-A-New-Perspective-for-HD-Map-Construction"><a href="#Complementing-Onboard-Sensors-with-Satellite-Map-A-New-Perspective-for-HD-Map-Construction" class="headerlink" title="Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction"></a>Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15427">http://arxiv.org/abs/2308.15427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Gao, Jiawei Fu, Haodong Jing, Nanning Zheng</li>
<li>for: 提高自动驾驶系统中的高清晰地图建构方法的性能，使其更敏感于废弃环境。</li>
<li>methods: 补充车载感知器上的信息使用卫星地图，提高HD地图建构方法的性能。提出一种层次融合模块，通过Feature级别融合和BEV级别融合来实现卫星地图信息的更好融合。</li>
<li>results: 在扩展nuScenes数据集上，证明了我们的模块可以准确地融合到现有的HD地图建构方法中，提高其在HD地图Semantic segmentation和实例检测任务中的性能。<details>
<summary>Abstract</summary>
High-Definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time based on information obtained from vehicle onboard sensors. However, the performance of these methods is significantly susceptible to the environment surrounding the vehicle due to the inherent limitation of onboard sensors, such as weak capacity for long-range detection. In this study, we demonstrate that supplementing onboard sensors with satellite maps can enhance the performance of HD map construction methods, leveraging the broad coverage capability of satellite maps. For the purpose of further research, we release the satellite map tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose a hierarchical fusion module that enables better fusion of satellite maps information with existing methods. Specifically, we design an attention mask based on segmentation and distance, applying the cross-attention mechanism to fuse onboard Bird's Eye View (BEV) features and satellite features in feature-level fusion. An alignment module is introduced before concatenation in BEV-level fusion to mitigate the impact of misalignment between the two features. The experimental results on the augmented nuScenes dataset showcase the seamless integration of our module into three existing HD map construction methods. It notably enhances their performance in both HD map semantic segmentation and instance detection tasks.
</details>
<details>
<summary>摘要</summary>
高清定义（HD）地图在自动驾驶系统中扮演着关键角色。现有方法尝试在实时基础上构建HD地图，但这些方法的性能受周围环境的影响很大，因为搭载在车辆上的感知器件具有较弱的远程探测能力。在本研究中，我们发现可以通过补充搭载在车辆上的感知器件与卫星地图的信息来提高HD地图构建方法的性能。为进一步研究，我们释放了卫星地图块作为nuScenes数据集的补充数据集。此外，我们提议了一种层次融合模块，使得更好地融合卫星地图信息与现有方法。具体来说，我们设计了一个基于分割和距离的注意mask，通过交叉注意机制来融合搭载在 Bird's Eye View（BEV）上的特征和卫星特征在特征层融合。在BEV层融合之前，我们引入了一个对齐模块，以mitigate卫星特征和BEV特征之间的偏移的影响。实验结果表明，我们的模块可以覆盖现有的三种HD地图构建方法，并在HD地图semantic segmentation和实例检测任务中显著提高其性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.AI_2023_08_30/" data-id="clmjn91jc00190j880v4kbxy9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.CL_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T11:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.CL_2023_08_30/">cs.CL - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Vulgar-Remarks-Detection-in-Chittagonian-Dialect-of-Bangla"><a href="#Vulgar-Remarks-Detection-in-Chittagonian-Dialect-of-Bangla" class="headerlink" title="Vulgar Remarks Detection in Chittagonian Dialect of Bangla"></a>Vulgar Remarks Detection in Chittagonian Dialect of Bangla</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15448">http://arxiv.org/abs/2308.15448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanjim Mahmud, Michal Ptaszynski, Fumito Masui</li>
<li>for: 本研究旨在探讨社交媒体上的负面言语 automatic detection方法，尤其是在低资源语言如锡兰语方言上。</li>
<li>methods: 本研究使用了指导学习和深度学习算法来检测社交媒体上的侮辱言语。Logistic Regression实现了可观的准确率（0.91），而简单的RNN具有Word2vec和fastTex的组合实现了较低的准确率（0.84-0.90），这说明了NN算法需要更多的数据。</li>
<li>results: 本研究显示，使用指导学习和深度学习算法可以准确地检测社交媒体上的侮辱言语，但是NN算法需要更多的数据以实现更高的准确率。<details>
<summary>Abstract</summary>
The negative effects of online bullying and harassment are increasing with Internet popularity, especially in social media. One solution is using natural language processing (NLP) and machine learning (ML) methods for the automatic detection of harmful remarks, but these methods are limited in low-resource languages like the Chittagonian dialect of Bangla.This study focuses on detecting vulgar remarks in social media using supervised ML and deep learning algorithms.Logistic Regression achieved promising accuracy (0.91) while simple RNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the issue that NN algorithms require more data.
</details>
<details>
<summary>摘要</summary>
互联网欺凌和 Harrasment 的负面影响随着互联网的普及而增加，尤其在社交媒体上。一种解决方案是使用自然语言处理（NLP）和机器学习（ML）方法进行自动发现危险评论，但这些方法在低资源语言如锡兰语的拼写方言上有限。本研究关注社交媒体中的粗鄙评论使用监督式 ML 和深度学习算法探测。Logistic Regression 达到了可靠的准确率（0.91），而简单的 RNN 与 Word2vec 和 fastTex 的准确率（0.84-0.90）较低，表明 NN 算法需要更多的数据。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Learning-Curves-During-Language-Model-Pre-Training-Learning-Forgetting-and-Stability"><a href="#Characterizing-Learning-Curves-During-Language-Model-Pre-Training-Learning-Forgetting-and-Stability" class="headerlink" title="Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability"></a>Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15419">http://arxiv.org/abs/2308.15419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tylerachang/lm-learning-curves">https://github.com/tylerachang/lm-learning-curves</a></li>
<li>paper_authors: Tyler A. Chang, Zhuowen Tu, Benjamin K. Bergen</li>
<li>for: 这些语言模型在预训练中学习的问题是什么？</li>
<li>methods: 这些语言模型在预训练中使用了什么方法？</li>
<li>results: 这些语言模型在预训练中得到了什么结果？Here are the answers in Simplified Chinese text:</li>
<li>for: 这些语言模型在预训练中学习的问题是如何快速预测语言模型的性能？</li>
<li>methods: 这些语言模型在预训练中使用了自适应语言模型的预训练方法？</li>
<li>results: 这些语言模型在预训练中得到了快速预测语言模型的稳定性和性能？<details>
<summary>Abstract</summary>
How do language models learn to make predictions during pre-training? To study this question, we extract learning curves from five autoregressive English language model pre-training runs, for 1M tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be "forgotten" during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Effects of part-of-speech are also small, although nouns tend to be acquired later and less stably than verbs, adverbs, and adjectives. Our work contributes to a better understanding of language model pre-training dynamics and informs the deployment of stable language models in practice.
</details>
<details>
<summary>摘要</summary>
<<SYS>>我们使用五个权重autoregressive英语语言模型的预训练运行来研究语言模型如何预测。我们从100万个字Context中提取学习曲线，并观察到语言模型在预训练过程中首先生成短重复的短语，然后学习 longer和更 coherent的文本。我们量化每个Token在Context中的最终难度、内Run变化、年龄 acquisition、忘记性和 Cross-Run变化。我们发现更常见的Token在Context中更容易预测， exhibit less variability within和across预训练运行，learn Earlier，并更可能被"忘记" during预训练。高 n-gram概率更加强调这些效果。独立于目标Token，更短和更频繁的Contexts呈现marginally more stable and quickly acquired预测。 parts of speech的影响也很小，although nouns tend to be acquired later and less stably than verbs, adverbs, and adjectives。我们的工作对语言模型预训练动力学的更好理解，并可以帮助实践中稳定地部署语言模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.CL_2023_08_30/" data-id="clmjn91jy002q0j88fnop9zn9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><a class="extend next" rel="next" href="/page/6/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
