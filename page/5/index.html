
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/5/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/cs.LG_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T10:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/cs.LG_2023_11_16/">cs.LG - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Asymptotically-Fair-Participation-in-Machine-Learning-Models-an-Optimal-Control-Perspective"><a href="#Asymptotically-Fair-Participation-in-Machine-Learning-Models-an-Optimal-Control-Perspective" class="headerlink" title="Asymptotically Fair Participation in Machine Learning Models: an Optimal Control Perspective"></a>Asymptotically Fair Participation in Machine Learning Models: an Optimal Control Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10223">http://arxiv.org/abs/2311.10223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuotong Chen, Qianxiao Li, Zheng Zhang</li>
<li>for:  Addressing the problem of achieving asymptotically fair participation in machine learning models, particularly when the data distribution shifts due to deployment.</li>
<li>methods:  Optimal control formulation and surrogate retention system based on evolutionary population dynamics to approximate the dynamics of distribution shifts on active user counts.</li>
<li>results:  Superior performance compared to existing baseline methods in a generic simulation environment, demonstrating the effectiveness of the proposed method for long-term planning and maintaining model performance across all demographic groups.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在解决机器学习模型中的 asymptotically fair participation 问题，特别是在数据分布shift时。</li>
<li>methods: 使用优化控制方法和基于进化人口动力学的代表系统来近似活动用户数下的分布转移 dynamics。</li>
<li>results: 在一个通用的 simulate 环境中，比基eline方法表现出色，证明提议的方法可以对长期规划和维护所有民主组中的表现。<details>
<summary>Abstract</summary>
The performance of state-of-the-art machine learning models often deteriorates when testing on demographics that are under-represented in the training dataset. This problem has predominately been studied in a supervised learning setting where the data distribution is static. However, real-world applications often involve distribution shifts caused by the deployed models. For instance, the performance disparity against monitory users can lead to a high customer churn rate, thus the available data provided by active users are skewed due to the lack of minority users. This feedback effect further exacerbates the disparity among different demographic groups in future steps. To address this issue, we propose asymptotically fair participation as a condition to maintain long-term model performance over all demographic groups. In this work, we aim to address the problem of achieving asymptotically fair participation via optimal control formulation. Moreover, we design a surrogate retention system based on existing literature on evolutionary population dynamics to approximate the dynamics of distribution shifts on active user counts, from which the objective of achieving asymptotically fair participation is formulated as an optimal control problem, and the control variables are considered as the model parameters. We apply an efficient implementation of Pontryagin's maximum principle to estimate the optimal control solution. To evaluate the effectiveness of the proposed method, we design a generic simulation environment that simulates the population dynamics of the feedback effect between user retention and model performance. When we deploy the resulting models to the simulation environment, the optimal control solution accounts for long-term planning and leads to superior performance compared with existing baseline methods.
</details>
<details>
<summary>摘要</summary>
现代机器学习模型在测试不充分表示的民生数据时 часто会下降性能。这个问题主要在静止数据分布下进行研究，但实际应用中经常出现数据分布shift的问题。例如，由于模型的使用者留存率不均匀，导致可用数据受到少数民生的抑制，从而使得模型的性能差异化。这种反馈效应进一步夹紧不同民生群体之间的性能差异，使得长期维护模型的性能成为一个重要的问题。为解决这个问题，我们提出了 asymptotically fair participation 作为一种维护长期模型性能的条件。在这种情况下，我们通过可EVOLUTIONARY POPULATION DYNAMICS 来模拟活跃用户数量的分布转移，从而将目标 achieving asymptotically fair participation 转化为一个优化控制问题。我们使用 Pontryagin's maximum principle 的有效实现来估计优化控制解。为评估提案的效果，我们设计了一个通用的 simulate 环境，该环境可以模拟反馈效应的影响，使得我们可以评估提案的效果。当我们将结果应用到 simulate 环境中时，优化控制解会考虑长期规划，并且比拥有基准方法更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Optimization-Algorithms-for-Machine-Learning"><a href="#Adaptive-Optimization-Algorithms-for-Machine-Learning" class="headerlink" title="Adaptive Optimization Algorithms for Machine Learning"></a>Adaptive Optimization Algorithms for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10203">http://arxiv.org/abs/2311.10203</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Slavomír Hanzely</li>
<li>for: 这个论文的目的是调查机器学习优化器中的适应性。</li>
<li>methods: 本论文使用了多种方法，包括个性化损失、meta-学习、卷积矩阵规则、步长 Newton 方法和低维度更新。</li>
<li>results: 本论文的研究结果包括提出了新的适应性方法，改进了现有算法的收敛保证，以及对实际应用中流行的算法进行了深入分析。<details>
<summary>Abstract</summary>
Machine learning assumes a pivotal role in our data-driven world. The increasing scale of models and datasets necessitates quick and reliable algorithms for model training. This dissertation investigates adaptivity in machine learning optimizers. The ensuing chapters are dedicated to various facets of adaptivity, including: 1. personalization and user-specific models via personalized loss, 2. provable post-training model adaptations via meta-learning, 3. learning unknown hyperparameters in real time via hyperparameter variance reduction, 4. fast O(1/k^2) global convergence of second-order methods via stepsized Newton method regardless of the initialization and choice basis, 5. fast and scalable second-order methods via low-dimensional updates. This thesis contributes novel insights, introduces new algorithms with improved convergence guarantees, and improves analyses of popular practical algorithms.
</details>
<details>
<summary>摘要</summary>
机器学习在数据驱动世界中扮演着关键性的角色。随着模型和数据集的规模的增长，需要快速可靠的模型训练算法。本论文调查了机器学习优化器中的适应性。以下章节探讨了不同方面的适应性，包括：1. 个性化损失函数 для个性化模型；2. 通过meta学习提供可证明的后期模型适应性；3. 在实时中学习 unknown 的 гипер参数 via  гипер参数减少方法；4. O(1/k^2) 全球快速收敛的二次方法，无论初始化和选择基准都是；5. 快速可扩展的二次方法 via 低维度更新。本论文提供了新的发现和改进了现有算法的新算法，同时也提高了现有实践中的分析。
</details></li>
</ul>
<hr>
<h2 id="Improving-Unimodal-Inference-with-Multimodal-Transformers"><a href="#Improving-Unimodal-Inference-with-Multimodal-Transformers" class="headerlink" title="Improving Unimodal Inference with Multimodal Transformers"></a>Improving Unimodal Inference with Multimodal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10170">http://arxiv.org/abs/2311.10170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kateryna Chumachenko, Alexandros Iosifidis, Moncef Gabbouj</li>
<li>for: 提高单模型性能，通过多模式训练</li>
<li>methods: 提出一种多支路架构，结合单模型与多模式变换器，通过多任务目标进行知识传递</li>
<li>results: 在RGB和深度动手势识别、语音和脸部视频情感识别以及语音-视频-文本情感分析等任务上表现出色，超过传统单模型counterpart<details>
<summary>Abstract</summary>
This paper proposes an approach for improving performance of unimodal models with multimodal training. Our approach involves a multi-branch architecture that incorporates unimodal models with a multimodal transformer-based branch. By co-training these branches, the stronger multimodal branch can transfer its knowledge to the weaker unimodal branches through a multi-task objective, thereby improving the performance of the resulting unimodal models. We evaluate our approach on tasks of dynamic hand gesture recognition based on RGB and Depth, audiovisual emotion recognition based on speech and facial video, and audio-video-text based sentiment analysis. Our approach outperforms the conventionally trained unimodal counterparts. Interestingly, we also observe that optimization of the unimodal branches improves the multimodal branch, compared to a similar multimodal model trained from scratch.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Algebraic-Topological-Networks-via-the-Persistent-Local-Homology-Sheaf"><a href="#Algebraic-Topological-Networks-via-the-Persistent-Local-Homology-Sheaf" class="headerlink" title="Algebraic Topological Networks via the Persistent Local Homology Sheaf"></a>Algebraic Topological Networks via the Persistent Local Homology Sheaf</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10156">http://arxiv.org/abs/2311.10156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Cesa, Arash Behboodi</li>
<li>for: 这 paper 的目的是提出一种基于代数Topology的图 convolution 和注意力模块的新方法，以便更好地利用数据的本地 topological 特性。</li>
<li>methods: 这 paper 使用了 sheaf neural networks 框架，通过将数据转化为 simplicial complex 后，构造其 local homology sheaf，并使用这个 sheaf 的 Laplacian 来建立更复杂的线性消息。</li>
<li>results: 这 paper 的结果表明，通过使用本 paper 的方法，可以construct more expressive, non-isotropic messages，并且可以 directly optimize the topology of intermediate features。<details>
<summary>Abstract</summary>
In this work, we introduce a novel approach based on algebraic topology to enhance graph convolution and attention modules by incorporating local topological properties of the data. To do so, we consider the framework of sheaf neural networks, which has been previously leveraged to incorporate additional structure into graph neural networks' features and construct more expressive, non-isotropic messages. Specifically, given an input simplicial complex (e.g. generated by the cliques of a graph or the neighbors in a point cloud), we construct its local homology sheaf, which assigns to each node the vector space of its local homology. The intermediate features of our networks live in these vector spaces and we leverage the associated sheaf Laplacian to construct more complex linear messages between them. Moreover, we extend this approach by considering the persistent version of local homology associated with a weighted simplicial complex (e.g., built from pairwise distances of nodes embeddings). This i) solves the problem of the lack of a natural choice of basis for the local homology vector spaces and ii) makes the sheaf itself differentiable, which enables our models to directly optimize the topology of their intermediate features.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们介绍了一种基于代数Topology的新方法，用于增强图 convolution和注意模块。我们通过使用 sheaf neural networks 框架，将数据的本地 topological 特性 incorporated 到图 neural networks 的特征中。具体来说，我们给输入的 simplicial complex (例如，由图中的 clique 或点云中的邻居生成) 构建了本地 homology sheaf，将每个节点的 vector space 分配给其本地 homology。我们的网络中间特征生活在这些 vector space 中，并利用相关的 sheaf Laplacian 构建更复杂的线性消息 между它们。此外，我们还扩展了这种方法，考虑weighted simplicial complex 中的 persistent local homology，解决了选择 local homology vector space 的自然基的问题，并使 sheaf 本身可导，使我们的模型直接优化其中间特征的 topology。
</details></li>
</ul>
<hr>
<h2 id="Near-optimal-Closed-loop-Method-via-Lyapunov-Damping-for-Convex-Optimization"><a href="#Near-optimal-Closed-loop-Method-via-Lyapunov-Damping-for-Convex-Optimization" class="headerlink" title="Near-optimal Closed-loop Method via Lyapunov Damping for Convex Optimization"></a>Near-optimal Closed-loop Method via Lyapunov Damping for Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10053">http://arxiv.org/abs/2311.10053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Severin Maier, Camille Castera, Peter Ochs</li>
<li>for: 这个论文是为了解决首阶Optimization问题而设计的一个自动控制系统。</li>
<li>methods: 这个系统使用了闭环抑止来实现 convergence rate的最佳化。</li>
<li>results: 研究发现，这个系统可以达到 arbitrarily close to 最佳的 convergence rate，而且可以实现闭环抑止。<details>
<summary>Abstract</summary>
We introduce an autonomous system with closed-loop damping for first-order convex optimization. While, to this day, optimal rates of convergence are only achieved by non-autonomous methods via open-loop damping (e.g., Nesterov's algorithm), we show that our system is the first one featuring a closed-loop damping while exhibiting a rate arbitrarily close to the optimal one. We do so by coupling the damping and the speed of convergence of the system via a well-chosen Lyapunov function. We then derive a practical first-order algorithm called LYDIA by discretizing our system, and present numerical experiments supporting our theoretical findings.
</details>
<details>
<summary>摘要</summary>
我们介绍一个自动化系统，其中关闭调对于首项凸优化问题的关闭调。至今为止，仅有非自动化方法（如尼斯特洛夫的算法）可以 дости得最佳速度传递，但我们证明我们的系统是第一个具有关闭调的系统，其速度与最佳速度传递之间存在一定的关联。我们通过一个适当的 Lyapunov 函数来实现这一点。我们随后从数值方面提出了一个实用的首项算法，即 LYDIA，并提供了数值实验证明我们的理论成果。
</details></li>
</ul>
<hr>
<h2 id="Tabular-Few-Shot-Generalization-Across-Heterogeneous-Feature-Spaces"><a href="#Tabular-Few-Shot-Generalization-Across-Heterogeneous-Feature-Spaces" class="headerlink" title="Tabular Few-Shot Generalization Across Heterogeneous Feature Spaces"></a>Tabular Few-Shot Generalization Across Heterogeneous Feature Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10051">http://arxiv.org/abs/2311.10051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Zhu, Katarzyna Kobalczyk, Andrija Petrovic, Mladen Nikolic, Mihaela van der Schaar, Boris Delibasic, Petro Lio</li>
<li>for: 这篇论文的目的是提出一种适用于表格数据集的几何学习方法，以应对表格数据集中的弹性和不同的列关系。</li>
<li>methods: 本文提出的方法是FLAT，它使用了Dataset2Vec的核心传统来学习表格数据集的低维度表示，并使用Graph Attention Network来适应表格数据集的不同列关系。</li>
<li>results: 本文的实验结果显示FLAT方法能够成功地在118个UCI数据集上进行几何学习，并与基准值相比有着明显的改善。<details>
<summary>Abstract</summary>
Despite the prevalence of tabular datasets, few-shot learning remains under-explored within this domain. Existing few-shot methods are not directly applicable to tabular datasets due to varying column relationships, meanings, and permutational invariance. To address these challenges, we propose FLAT-a novel approach to tabular few-shot learning, encompassing knowledge sharing between datasets with heterogeneous feature spaces. Utilizing an encoder inspired by Dataset2Vec, FLAT learns low-dimensional embeddings of datasets and their individual columns, which facilitate knowledge transfer and generalization to previously unseen datasets. A decoder network parametrizes the predictive target network, implemented as a Graph Attention Network, to accommodate the heterogeneous nature of tabular datasets. Experiments on a diverse collection of 118 UCI datasets demonstrate FLAT's successful generalization to new tabular datasets and a considerable improvement over the baselines.
</details>
<details>
<summary>摘要</summary>
尽管表格数据集很普遍，ew-shot学习在这个领域仍然受到不 enough 探索。现有的ew-shot方法不直接适用于表格数据集，因为列之间的关系、意义和 permutation 不变。为解决这些挑战，我们提出FLAT，一种 novel 的表格ew-shot学习方法，利用dataset2Vec inspirited Encoder 学习表格和其各列的低维度嵌入，从而促进了知识传递和泛化至之前未见的表格数据集。Decoder 网络实现了 Graph Attention Network，以适应表格数据集的多样性。我们在118个 UCI 数据集上进行了实验，并证明FLAT可以成功泛化到新的表格数据集，并且明显超过基elines。
</details></li>
</ul>
<hr>
<h2 id="Guaranteeing-Control-Requirements-via-Reward-Shaping-in-Reinforcement-Learning"><a href="#Guaranteeing-Control-Requirements-via-Reward-Shaping-in-Reinforcement-Learning" class="headerlink" title="Guaranteeing Control Requirements via Reward Shaping in Reinforcement Learning"></a>Guaranteeing Control Requirements via Reward Shaping in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10026">http://arxiv.org/abs/2311.10026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco De Lellis, Marco Coraggio, Giovanni Russo, Mirco Musolesi, Mario di Bernardo</li>
<li>for: 本研究旨在提供一种 Ensure Optimal Control Policy Meets Specified Requirements 的方法，以解决在 reinforcement learning 中控制问题中的稳定性和性能要求。</li>
<li>methods: 本研究使用了一种系统的 reward shaping 方法，通过确保优化策略满足指定的控制要求来保证优化策略的合理性。</li>
<li>results: 数据表明，使用本研究提出的方法可以确保优化策略满足指定的控制要求，并且可以在 OpenAI Gym 中的两个示例环境中进行 validate。<details>
<summary>Abstract</summary>
In addressing control problems such as regulation and tracking through reinforcement learning, it is often required to guarantee that the acquired policy meets essential performance and stability criteria such as a desired settling time and steady-state error prior to deployment. Motivated by this necessity, we present a set of results and a systematic reward shaping procedure that (i) ensures the optimal policy generates trajectories that align with specified control requirements and (ii) allows to assess whether any given policy satisfies them. We validate our approach through comprehensive numerical experiments conducted in two representative environments from OpenAI Gym: the Inverted Pendulum swing-up problem and the Lunar Lander. Utilizing both tabular and deep reinforcement learning methods, our experiments consistently affirm the efficacy of our proposed framework, highlighting its effectiveness in ensuring policy adherence to the prescribed control requirements.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将控制问题，如补偿和跟踪，通过强化学习方法解决时，经常需要保证获得的策略能够满足必要的性能和稳定性标准，如所需的定点时间和稳定态误差。驱动了这种需求，我们提出了一组结果和一种系统的奖励形式，以确保优化策略生成的轨迹与指定的控制要求相对应，并且可以评估任何给定策略是否满足这些要求。我们通过对OpenAI Gym提供的两个示例环境中的摆式椅子振荡问题和月球降落问题进行了广泛的数学实验，结果表明我们的提出的框架具有确保策略遵循指定控制要求的效iveness。
</details></li>
</ul>
<hr>
<h2 id="Online-Optimization-for-Network-Resource-Allocation-and-Comparison-with-Reinforcement-Learning-Techniques"><a href="#Online-Optimization-for-Network-Resource-Allocation-and-Comparison-with-Reinforcement-Learning-Techniques" class="headerlink" title="Online Optimization for Network Resource Allocation and Comparison with Reinforcement Learning Techniques"></a>Online Optimization for Network Resource Allocation and Comparison with Reinforcement Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10023">http://arxiv.org/abs/2311.10023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Sid-Ali, Ioannis Lambadaris, Yiqiang Q. Zhao, Gennady Shaikhet, Amirhossein Asgharnia</li>
<li>for: 这paper是为了解决在线网络资源分配问题，包括工作转移。</li>
<li>methods: 这paper使用了Randomized Online Algorithm based on exponentially weighted method。</li>
<li>results: 这paper证明了该算法具有下线时间 regret，并且在人工数据上测试表明该算法在工作转移问题上表现出优于强化学习方法。<details>
<summary>Abstract</summary>
We tackle in this paper an online network resource allocation problem with job transfers. The network is composed of many servers connected by communication links. The system operates in discrete time; at each time slot, the administrator reserves resources at servers for future job requests, and a cost is incurred for the reservations made. Then, after receptions, the jobs may be transferred between the servers to best accommodate the demands. This incurs an additional transport cost. Finally, if a job request cannot be satisfied, there is a violation that engenders a cost to pay for the blocked job. We propose a randomized online algorithm based on the exponentially weighted method. We prove that our algorithm enjoys a sub-linear in time regret, which indicates that the algorithm is adapting and learning from its experiences and is becoming more efficient in its decision-making as it accumulates more data. Moreover, we test the performance of our algorithm on artificial data and compare it against a reinforcement learning method where we show that our proposed method outperforms the latter.
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了一个在线网络资源分配问题，其中包括作业传输。网络由多个服务器连接而成，系统在精确时钟下运行，管理员在每个时间槽内预留服务器上的资源，以储存未来的作业请求。预留资源的成本将会产生。然后，接收作业可能会被传输到不同的服务器，以满足需求。这会产生额外的传输成本。如果一个作业请求无法满足，那么会出现阻塞，并且需要支付阻塞作业的成本。我们提议一种随机在线算法，基于加速方法。我们证明我们的算法具有线性小于时间的 regret，这表明我们的算法在经验学习和决策过程中变得更加高效。此外，我们在人工数据上测试了我们的算法，并与一种强化学习方法进行比较，我们的提议方法在效率方面表现更好。
</details></li>
</ul>
<hr>
<h2 id="Finding-Real-World-Orbital-Motion-Laws-from-Data"><a href="#Finding-Real-World-Orbital-Motion-Laws-from-Data" class="headerlink" title="Finding Real-World Orbital Motion Laws from Data"></a>Finding Real-World Orbital Motion Laws from Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10012">http://arxiv.org/abs/2311.10012</a></li>
<li>repo_url: None</li>
<li>paper_authors: João Funenga, Marta Guimarães, Henrique Costa, Cláudia Soares</li>
<li>for: 这个研究旨在找到在空间中卫星的运动方程式。</li>
<li>methods: 这种方法基于SINDy数据驱动技术，可以从时间序列数据中找到物理系统的下面动力学。</li>
<li>results: 这种方法可以高精度地描述LEO卫星的运动轨迹，并且可以保持物理可读性的坐标系。<details>
<summary>Abstract</summary>
A novel approach is presented for discovering PDEs that govern the motion of satellites in space. The method is based on SINDy, a data-driven technique capable of identifying the underlying dynamics of complex physical systems from time series data. SINDy is utilized to uncover PDEs that describe the laws of physics in space, which are non-deterministic and influenced by various factors such as drag or the reference area (related to the attitude of the satellite). In contrast to prior works, the physically interpretable coordinate system is maintained, and no dimensionality reduction technique is applied to the data. By training the model with multiple representative trajectories of LEO - encompassing various inclinations, eccentricities, and altitudes - and testing it with unseen orbital motion patterns, a mean error of around 140 km for the positions and 0.12 km/s for the velocities is achieved. The method offers the advantage of delivering interpretable, accurate, and complex models of orbital motion that can be employed for propagation or as inputs to predictive models for other variables of interest, such as atmospheric drag or the probability of collision in an encounter with a spacecraft or space objects. In conclusion, the work demonstrates the promising potential of using SINDy to discover the equations governing the behaviour of satellites in space. The technique has been successfully applied to uncover PDEs describing the motion of satellites in LEO with high accuracy. The method possesses several advantages over traditional models, including the ability to provide physically interpretable, accurate, and complex models of orbital motion derived from high-entropy datasets. These models can be utilised for propagation or as inputs to predictive models for other variables of interest.
</details>
<details>
<summary>摘要</summary>
一种新的方法被提出，用于发现卫星在空间中的运动方程。该方法基于SINDy，一种数据驱动的技术，可以从时间序列数据中找到物理系统的下面动力学。SINDy被用来揭示卫星运动的PDE，这些PDE是非束定的，受到阻力或参考面积的影响。与先前的工作不同，physically interpretable的坐标系统被保留，无需应用任何维度减少技术。通过训练模型使用多个LEO表示轨迹，包括不同的倾斜、轨道半长轴和高度，并测试它们与未经见过的轨道运动模式，实现了平均误差约为140公里的位置和0.12公里/秒的速度。这种方法具有以下优点：可提供可解释、准确、复杂的轨道运动模型，可以用于卫星的传播或作为其他变量的预测模型的输入。总之，这种方法在发现卫星在空间中的运动方程方面表现出了扎实的推力，并成功地应用于LEO中的卫星运动。这种方法比传统模型具有多个优点，包括能提供physically interpretable、准确、复杂的轨道运动模型，从高熵数据集中拟合出来的。这些模型可以用于卫星的传播或作为其他变量的预测模型的输入。
</details></li>
</ul>
<hr>
<h2 id="Co-data-Learning-for-Bayesian-Additive-Regression-Trees"><a href="#Co-data-Learning-for-Bayesian-Additive-Regression-Trees" class="headerlink" title="Co-data Learning for Bayesian Additive Regression Trees"></a>Co-data Learning for Bayesian Additive Regression Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09997">http://arxiv.org/abs/2311.09997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JeroenGoedhart/EB_coBART_paper">https://github.com/JeroenGoedhart/EB_coBART_paper</a></li>
<li>paper_authors: Jeroen M. Goedhart, Thomas Klausch, Jurriaan Janssen, Mark A. van de Wiel</li>
<li>For:  This paper proposes a method to incorporate external information (co-data) into Bayesian additive regression trees (BART) to improve prediction in medical prediction applications with small sample sizes and high-dimensional covariates.* Methods: The proposed method uses an empirical Bayes (EB) framework to estimate prior covariate weights in the BART model, and can handle multiple types of co-data simultaneously. The EB framework also estimates other hyperparameters of BART.* Results: The proposed method finds relevant covariates and improves prediction compared to default BART in simulations, and outperforms regression-based co-data learners when the covariate-response relationship is nonlinear. The method is applied to diffuse large B-cell lymphoma prognosis with clinical covariates, gene mutations, DNA translocations, and DNA copy number data.<details>
<summary>Abstract</summary>
Medical prediction applications often need to deal with small sample sizes compared to the number of covariates. Such data pose problems for prediction and variable selection, especially when the covariate-response relationship is complicated. To address these challenges, we propose to incorporate co-data, i.e. external information on the covariates, into Bayesian additive regression trees (BART), a sum-of-trees prediction model that utilizes priors on the tree parameters to prevent overfitting. To incorporate co-data, an empirical Bayes (EB) framework is developed that estimates, assisted by a co-data model, prior covariate weights in the BART model. The proposed method can handle multiple types of co-data simultaneously. Furthermore, the proposed EB framework enables the estimation of the other hyperparameters of BART as well, rendering an appealing alternative to cross-validation. We show that the method finds relevant covariates and that it improves prediction compared to default BART in simulations. If the covariate-response relationship is nonlinear, the method benefits from the flexibility of BART to outperform regression-based co-data learners. Finally, the use of co-data enhances prediction in an application to diffuse large B-cell lymphoma prognosis based on clinical covariates, gene mutations, DNA translocations, and DNA copy number data.   Keywords: Bayesian additive regression trees; Empirical Bayes; Co-data; High-dimensional data; Omics; Prediction
</details>
<details>
<summary>摘要</summary>
医学预测应用经常面临小样本大于变量的问题。这些数据会对预测和变量选择造成困难，� особенply when the covariate-response relationship is complicated. To address these challenges, we propose to incorporate co-data, i.e. external information on the covariates, into Bayesian additive regression trees (BART), a sum-of-trees prediction model that utilizes priors on the tree parameters to prevent overfitting. To incorporate co-data, an empirical Bayes (EB) framework is developed that estimates, assisted by a co-data model, prior covariate weights in the BART model. The proposed method can handle multiple types of co-data simultaneously. Furthermore, the proposed EB framework enables the estimation of the other hyperparameters of BART as well, rendering an appealing alternative to cross-validation. We show that the method finds relevant covariates and that it improves prediction compared to default BART in simulations. If the covariate-response relationship is nonlinear, the method benefits from the flexibility of BART to outperform regression-based co-data learners. Finally, the use of co-data enhances prediction in an application to diffuse large B-cell lymphoma prognosis based on clinical covariates, gene mutations, DNA translocations, and DNA copy number data.关键字：Bayesian additive regression trees; Empirical Bayes; Co-data; High-dimensional data; Omics; Prediction
</details></li>
</ul>
<hr>
<h2 id="Xputer-Bridging-Data-Gaps-with-NMF-XGBoost-and-a-Streamlined-GUI-Experience"><a href="#Xputer-Bridging-Data-Gaps-with-NMF-XGBoost-and-a-Streamlined-GUI-Experience" class="headerlink" title="Xputer: Bridging Data Gaps with NMF, XGBoost, and a Streamlined GUI Experience"></a>Xputer: Bridging Data Gaps with NMF, XGBoost, and a Streamlined GUI Experience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09989">http://arxiv.org/abs/2311.09989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saleena Younus, Lars Rönnstrand, Julhash U. Kazi</li>
<li>for: 本研究旨在提供一个可靠的数据填充工具，以确保数据完整性和有意义的分析结果。</li>
<li>methods: 本工具使用非正式矩阵分解（NMF）和XGBoost的预测能力，并支持零填充、自动化参数优化和迭代次数设定。</li>
<li>results: 在性能评估中，Xputer与已有的工具 IterativeImputer 相比，不仅 Computational speed 快，而且在填充精度方面也经常表现出来。此外，Xputer可以自动处理多种数据类型，包括分类、连续和布尔型数据，不需要先进行处理。<details>
<summary>Abstract</summary>
The rapid proliferation of data across diverse fields has accentuated the importance of accurate imputation for missing values. This task is crucial for ensuring data integrity and deriving meaningful insights. In response to this challenge, we present Xputer, a novel imputation tool that adeptly integrates Non-negative Matrix Factorization (NMF) with the predictive strengths of XGBoost. One of Xputer's standout features is its versatility: it supports zero imputation, enables hyperparameter optimization through Optuna, and allows users to define the number of iterations. For enhanced user experience and accessibility, we have equipped Xputer with an intuitive Graphical User Interface (GUI) ensuring ease of handling, even for those less familiar with computational tools. In performance benchmarks, Xputer not only rivals the computational speed of established tools such as IterativeImputer but also often outperforms them in terms of imputation accuracy. Furthermore, Xputer autonomously handles a diverse spectrum of data types, including categorical, continuous, and Boolean, eliminating the need for prior preprocessing. Given its blend of performance, flexibility, and user-friendly design, Xputer emerges as a state-of-the-art solution in the realm of data imputation.
</details>
<details>
<summary>摘要</summary>
随着数据在多个领域的快速扩散，缺失值的准确填充变得非常重要，以保持数据完整性和获得有意义的发现。为回应这个挑战，我们提出了Xputer，一种新的填充工具，它将非正式矩阵分解（NMF）与XGBoost的预测能力结合得非常灵活。Xputer的一些特点包括：* 支持零填充* 通过Optuna进行参数优化* 允许用户定义迭代次数为了提高用户体验和可达性，我们为Xputer设计了一个直观的图形用户界面（GUI），使其易于操作，即使用户对计算工具不熟悉。在性能测试中，Xputer不仅与已有的工具如IterativeImputer相当，而且经常超越它们在填充精度方面。此外，Xputer可以自动处理多种数据类型，包括 categorical、continue 和Boolean，从而消除了先前的预处理需求。由于其综合表现、灵活性和用户友好的设计，Xputer在数据填充领域中成为了状态 искусственный的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-learning-of-multi-omics-embeddings-in-the-low-label-high-data-regime"><a href="#Self-supervised-learning-of-multi-omics-embeddings-in-the-low-label-high-data-regime" class="headerlink" title="Self-supervised learning of multi-omics embeddings in the low-label, high-data regime"></a>Self-supervised learning of multi-omics embeddings in the low-label, high-data regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09962">http://arxiv.org/abs/2311.09962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian John Hurry, Emma Slade</li>
<li>for: 这项研究使用自然语言处理技术来训练一个预训练FT-Transformer模型，用于预测肿瘤类型基于miRNA、mRNA或RPPA表达数据。</li>
<li>methods: 这个模型使用了自然语言处理技术进行自我超级vised学习（SSL）训练，并与标注样本scarce，但无标注样本数量很多进行比较。</li>
<li>results: 研究发现，使用这种方法可以在肿瘤类型预测 tasks 中获得更高的性能，比如XGBoost和CatBoost等标准准则。此外，研究还探讨了多modal SSL，并提出了一种晚期融合模型，其中每种Omics都通过自己的子网络进行处理，然后将输出融合并传递给预训练或下游目标函数。这种方法在多modal样本中预测单 modal 样本的性能得到了改进。<details>
<summary>Abstract</summary>
Contrastive, self-supervised learning (SSL) is used to train a model that predicts cancer type from miRNA, mRNA or RPPA expression data. This model, a pretrained FT-Transformer, is shown to outperform XGBoost and CatBoost, standard benchmarks for tabular data, when labelled samples are scarce but the number of unlabelled samples is high. This is despite the fact that the datasets we use have $\mathcal{O}(10^{1})$ classes and $\mathcal{O}(10^{2})-\mathcal{O}(10^{4})$ features. After demonstrating the efficacy of our chosen method of self-supervised pretraining, we investigate SSL for multi-modal models. A late-fusion model is proposed, where each omics is passed through its own sub-network, the outputs of which are averaged and passed to the pretraining or downstream objective function. Multi-modal pretraining is shown to improve predictions from a single omics, and we argue that this is useful for datasets with many unlabelled multi-modal samples, but few labelled unimodal samples. Additionally, we show that pretraining each omics-specific module individually is highly effective. This enables the application of the proposed model in a variety of contexts where a large amount of unlabelled data is available from each omics, but only a few labelled samples.
</details>
<details>
<summary>摘要</summary>
“对比自学习（Contrastive, self-supervised learning）用于训练一个预训练FT-Transformer模型，用于预测肿瘤类型基于miRNA、mRNA或RPPA表达数据。这个模型在标签样本稀缺但无标签样本多的情况下表现出优于XGBoost和CatBoost标准准则，即使我们使用的数据集有数十个类型和数十个到数百个特征。我们首先证明了我们选择的自我超vised预训练方法的有效性，然后我们调查了多modal模型的SSL。我们提议了一种晚期 fusione模型，其中每个Omics被 passing through its own sub-network，输出被平均并 passing to the pretraining或 downstream objective function。我们发现，多modal预训练可以提高单modal预测结果，并且我们认为这是有用的在 datasets中有多个不标签多modal样本，但只有少量标签单modal样本。此外，我们发现预训练每个 OmicsSpecific module 都是非常有效的。这使得我们的模型可以在每个 Omics 有大量未标签数据，但只有几个标签单Modal 样本的情况下应用。”
</details></li>
</ul>
<hr>
<h2 id="Natural-Disaster-Analysis-using-Satellite-Imagery-and-Social-Media-Data-for-Emergency-Response-Situations"><a href="#Natural-Disaster-Analysis-using-Satellite-Imagery-and-Social-Media-Data-for-Emergency-Response-Situations" class="headerlink" title="Natural Disaster Analysis using Satellite Imagery and Social-Media Data for Emergency Response Situations"></a>Natural Disaster Analysis using Satellite Imagery and Social-Media Data for Emergency Response Situations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09947">http://arxiv.org/abs/2311.09947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sukeerthi Mandyam, Shanmuga Priya MG, Shalini Suresh, Kavitha Srinivasan<br>for:这项研究旨在分析不同类型的数据（卫星图像和推特数据），以提供深入的灾害管理分析。methods:这项研究包括两个阶段：卫星图像分析和推特数据分析，然后将这两个模块集成使用位置坐标。在第一阶段，使用多类地表征分 segmentation技术，基于U-Net架构进行预和后灾害卫星图像分析。在第二阶段，将地区映射到必需的紧急救援操作信息上，并提取推特数据使用关键词对应的地区。results:这项研究得到的结果是一种基于实时位置坐标和频率分析技术的多维ensional信息集成系统，可以帮助灾害管理人员在灾害发生时获得全面的情况概述，如喀拉拉和密西西比洪灾的分析和验证。这项研究的创新之处在于通过使用分割卫星图像和地区特定筛选器，对灾害区域进行深入的分析和救援操作。<details>
<summary>Abstract</summary>
Disaster Management is one of the most promising research areas because of its significant economic, environmental and social repercussions. This research focuses on analyzing different types of data (pre and post satellite images and twitter data) related to disaster management for in-depth analysis of location-wise emergency requirements. This research has been divided into two stages, namely, satellite image analysis and twitter data analysis followed by integration using location. The first stage involves pre and post disaster satellite image analysis of the location using multi-class land cover segmentation technique based on U-Net architecture. The second stage focuses on mapping the region with essential information about the disaster situation and immediate requirements for relief operations. The severely affected regions are demarcated and twitter data is extracted using keywords respective to that location. The extraction of situational information from a large corpus of raw tweets adopts Content Word based Tweet Summarization (COWTS) technique. An integration of these modules using real-time location-based mapping and frequency analysis technique gathers multi-dimensional information in the advent of disaster occurrence such as the Kerala and Mississippi floods that were analyzed and validated as test cases. The novelty of this research lies in the application of segmented satellite images for disaster relief using highlighted land cover changes and integration of twitter data by mapping these region-specific filters for obtaining a complete overview of the disaster.
</details>
<details>
<summary>摘要</summary>
灾害管理是一个非常有前途的研究领域，因为它具有重要的经济、环境和社会影响。这个研究的目的是分析不同类型的数据（卫星图像和推特数据），以进行深入的灾害管理分析。这个研究分为两个阶段：卫星图像分析和推特数据分析，然后是这两个分析结果的集成。第一阶段是使用多类别土地覆盖分类技术（U-Net架构）进行卫星图像分析，以分析灾害发生前后的地区变化。第二阶段是将地区分配为不同的灾害情况，并从推特数据中提取相关信息。在这个阶段，采用Content Word based Tweet Summarization（COWTS）技术来提取灾害情况的主要信息。将这两个模块集成使用实时地理位置基于的映射和频率分析技术，可以同时获得不同灾害情况的多维度信息。在实验阶段，对印度喀拉拉和美国密西西比洪涝进行了实验和验证。这个研究的创新点在于，通过使用分类卫星图像和地区特定的推特数据，对灾害发生情况进行全面的概括和分析。
</details></li>
</ul>
<hr>
<h2 id="Fast-multiplication-by-two’s-complement-addition-of-numbers-represented-as-a-set-of-polynomial-radix-2-indexes-stored-as-an-integer-list-for-massively-parallel-computation"><a href="#Fast-multiplication-by-two’s-complement-addition-of-numbers-represented-as-a-set-of-polynomial-radix-2-indexes-stored-as-an-integer-list-for-massively-parallel-computation" class="headerlink" title="Fast multiplication by two’s complement addition of numbers represented as a set of polynomial radix 2 indexes, stored as an integer list for massively parallel computation"></a>Fast multiplication by two’s complement addition of numbers represented as a set of polynomial radix 2 indexes, stored as an integer list for massively parallel computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09922">http://arxiv.org/abs/2311.09922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Stocks</li>
<li>for: 这个论文是用于描述一种基于整数表示为二进制指数的多项式方法，用于高速Multiplication。</li>
<li>methods: 这种方法使用了一种基于二进制指数的多项式方法，实现在Python代码中。它比Number Theoretic Transform (NTT)和Karatsuba方法快于 multiplication 在某些bit范围内。</li>
<li>results: 这种方法可以实现任何整数或实数的表示为一个列表中的整数指数，并且可以将这些指数存储和分布在多个CPU &#x2F; GPU上。此外，这种方法还可以完全分布加法和乘法操作，从而超越现有的并行乘法方法的限制，即需要共享公共核心内存和磁盘来计算结果和中间结果。<details>
<summary>Abstract</summary>
We demonstrate a multiplication method based on numbers represented as set of polynomial radix 2 indices stored as an integer list. The 'polynomial integer index multiplication' method is a set of algorithms implemented in python code. We demonstrate the method to be faster than both the Number Theoretic Transform (NTT) and Karatsuba for multiplication within a certain bit range. Also implemented in python code for comparison purposes with the polynomial radix 2 integer method. We demonstrate that it is possible to express any integer or real number as a list of integer indices, representing a finite series in base two. The finite series of integer index representation of a number can then be stored and distributed across multiple CPUs / GPUs. We show that operations of addition and multiplication can be applied as two's complement additions operating on the index integer representations and can be fully distributed across a given CPU / GPU architecture. We demonstrate fully distributed arithmetic operations such that the 'polynomial integer index multiplication' method overcomes the current limitation of parallel multiplication methods. Ie, the need to share common core memory and common disk for the calculation of results and intermediate results.
</details>
<details>
<summary>摘要</summary>
我们展示了一种多项式方法，基于表示为二进制基数指数的数字，并将其存储为整数列表。我们称之为“多项式整数指标乘法”方法，这是一系列python代码实现的算法。我们证明这种方法在某个位数范围内比NUMBER THEORETIC TRANSFORM（NTT）和加加姆托卡（Karatsuba） multiplication 方法更快。此外，我们还在python代码中实现了这些方法，以便与多项式整数指标乘法方法进行比较。我们示出了任意整数或实数可以表示为一个列表的整数指数表示，并且这个表示可以被存储和分布在多个CPU / GPU架构上。我们还证明了在多个CPU / GPU架构上实现了完全分布式加法和乘法操作，使得“多项式整数指标乘法”方法超越了当前的并行乘法方法的限制，即需要共享公共核心内存和公共磁盘来计算结果和中间结果。
</details></li>
</ul>
<hr>
<h2 id="On-some-elusive-aspects-of-databases-hindering-AI-based-discovery-A-case-study-on-superconducting-materials"><a href="#On-some-elusive-aspects-of-databases-hindering-AI-based-discovery-A-case-study-on-superconducting-materials" class="headerlink" title="On some elusive aspects of databases hindering AI based discovery: A case study on superconducting materials"></a>On some elusive aspects of databases hindering AI based discovery: A case study on superconducting materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09891">http://arxiv.org/abs/2311.09891</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/giotre/LK-99">https://github.com/giotre/LK-99</a></li>
<li>paper_authors: Giovanni Trezza, Eliodoro Chiavazzo</li>
<li>for: 本文旨在探讨大数据的准确性和AI模型的设计问题。</li>
<li>methods: 本文使用了三种方法来检测和衡量数据偏见：批处理方法、维度衡量方法和维度减少方法。</li>
<li>results: 本文通过对超导材料和热电材料两种示例进行分析，发现数据偏见存在于样本选择、隐藏变量和数据年龄等方面，并提出了一种新的检测方法。<details>
<summary>Abstract</summary>
It stands to reason that the amount and the quality of big data is of key importance for setting up accurate AI-driven models. Nonetheless, we believe there are still critical roadblocks in the inherent generation of databases, that are often underestimated and poorly discussed in the literature. In our view, such issues can seriously hinder the AI-based discovery process, even when high quality, sufficiently large and highly reputable data sources are available. Here, considering superconducting and thermoelectric materials as two representative case studies, we specifically discuss three aspects, namely intrinsically biased sample selection, possible hidden variables, disparate data age. Importantly, to our knowledge, we suggest and test a first strategy capable of detecting and quantifying the presence of the intrinsic data bias.
</details>
<details>
<summary>摘要</summary>
“据悉，大数据量和质量对于建立准确的人工智能驱动模型是关键。然而，我们认为在自然生成数据库时存在一些 crítical roadblocks，这些问题在文献中受到了低估和不充分讨论。我们认为这些问题可能会妨碍人工智能基于发现过程，即使有高质量、充分大、受人尊敬的数据源也有。在这里，通过使用超导和热电材料作为两个例子，我们专门讨论了三个方面：内在偏见样本选择、隐藏变量和数据年龄差异。值得注意的是，我们建议和测试了一种能够检测和衡量内在数据偏见的第一种策略。”Here's a breakdown of the translation:* "据悉" (liàng bì) is an idiomatic expression that means "according to what is known" or "as far as is known."* "大数据量" (dà xù xiǎng) means "large amount of data."* "质量" (jīn yù) means "quality."* "关键" (guān jī) means "critical" or "key."* "因此" (yǐn qī) is a conjunction that means "therefore" or "as a result."* "存在" (cún zhī) means "there is" or "exists."* "critical roadblocks" is translated as " crítical roadblocks" (zhì zhì fāng xiào) to emphasize the importance of the issues.* "在文献中" (zài wén xiǎng zhī) means "in the literature" or "as discussed in the literature."* "受到了低估" (shòu dào le duō jì) means "have been underestimated."* "不充分讨论" (bù zhòng fēn tóu yì) means "have not been fully discussed."* "我们认为" (wǒ men rèn wēi) is a phrase that means "we believe" or "in our view."* "这些问题" (zhè xiē wèn tí) is a phrase that means "these issues" or "these problems."* "可能会" (kě néng huì) is a phrase that means "may" or "might."* "妨碍" (mǐng yòu) means "obstacle" or "hinder."* "人工智能基于发现过程" (rén gōng jì yì jī bù jiào yù) is a phrase that means "artificial intelligence based on the discovery process."* "即使" (jī shì) is a conjunction that means "even if" or "despite."* "有高质量" (yǒu gāo jīn yù) means "have high quality."* "充分大" (chōng fēn dà) means "sufficiently large."* "受人尊敬" (shòu rén zhù jì) means "respected by people" or "well-regarded."* "数据源" (xìng xiào) means "data source."* "有" (yǒu) is a particle that indicates the existence of something.* "三个方面" (sān gè fāng miàn) is a phrase that means "three aspects" or "three sides."* "内在偏见样本选择" (nèi zài pēn jiàn yàng bǎn jiǎo) is a phrase that means "intrinsic bias in sample selection."* "隐藏变量" (hūn yǎn biàn yù) means "hidden variables."* "数据年龄差异" (xìng xiàng nián suī) is a phrase that means "data age difference."* "值得注意的是" (fù dé zhù yì de shì) is a phrase that means "it is worth noting that" or "it is worth mentioning that."* "我们建议和测试了一种能够检测和衡量内在数据偏见的第一种策略" (wǒ men jiàn yù hé cè shì yī zhī fāng zhì) is a sentence that means "We suggest and test a strategy that can detect and measure the intrinsic data bias for the first time."
</details></li>
</ul>
<hr>
<h2 id="Safety-Aware-Autonomous-Path-Planning-Using-Model-Predictive-Reinforcement-Learning-for-Inland-Waterways"><a href="#Safety-Aware-Autonomous-Path-Planning-Using-Model-Predictive-Reinforcement-Learning-for-Inland-Waterways" class="headerlink" title="Safety Aware Autonomous Path Planning Using Model Predictive Reinforcement Learning for Inland Waterways"></a>Safety Aware Autonomous Path Planning Using Model Predictive Reinforcement Learning for Inland Waterways</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09878">http://arxiv.org/abs/2311.09878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Astrid Vanneste, Simon Vanneste, Olivier Vasseur, Robin Janssens, Mattias Billast, Ali Anwar, Kevin Mets, Tom De Schepper, Siegfried Mercelis, Peter Hellinckx</li>
<li>for: 这篇论文是关于自动水上交通的规划方法，尤其是在城市水道中的自动船运行。</li>
<li>methods: 本文提出了一种基于强化学习的规划方法，即预测类型的优化学习（Model Predictive Reinforcement Learning，MPRL），可以处理任何形状的水道和任何数量和形状的障碍物。</li>
<li>results: 实验结果显示，MPRL在两个测试场景中比基于对称框架的规划和基于 proximal policy optimization（PPO）的规划更好，能够安全（无撞击）通过两个测试场景。<details>
<summary>Abstract</summary>
In recent years, interest in autonomous shipping in urban waterways has increased significantly due to the trend of keeping cars and trucks out of city centers. Classical approaches such as Frenet frame based planning and potential field navigation often require tuning of many configuration parameters and sometimes even require a different configuration depending on the situation. In this paper, we propose a novel path planning approach based on reinforcement learning called Model Predictive Reinforcement Learning (MPRL). MPRL calculates a series of waypoints for the vessel to follow. The environment is represented as an occupancy grid map, allowing us to deal with any shape of waterway and any number and shape of obstacles. We demonstrate our approach on two scenarios and compare the resulting path with path planning using a Frenet frame and path planning based on a proximal policy optimization (PPO) agent. Our results show that MPRL outperforms both baselines in both test scenarios. The PPO based approach was not able to reach the goal in either scenario while the Frenet frame approach failed in the scenario consisting of a corner with obstacles. MPRL was able to safely (collision free) navigate to the goal in both of the test scenarios.
</details>
<details>
<summary>摘要</summary>
近年来，自动水上交通在城市水道中受到了广泛关注，因为许多城市中心禁止汽车和卡车的进入。传统的方法，如基于弗雷内特框的规划和潜在场 Navigation，经常需要调整许多配置参数，甚至在不同情况下需要不同的配置。在本文中，我们提出了一种基于强化学习的新的规划方法， called Model Predictive Reinforcement Learning（MPRL）。MPRL计算出船只应该遵循的一系列方向点。环境被表示为一个占用度网格地图，这使得我们可以处理任何形状的水道和任何形状和数量的障碍物。我们在两个场景中测试了我们的方法，并与基于Frenet框的规划和基于 proximal policy optimization（PPO）算法的规划进行比较。我们的结果显示，MPRL在两个测试场景中都超过了两个基准点。PPO算法在任一场景中都无法达到目标，而基于Frenet框的规划在包含封闭障碍物的场景中失败。MPRL在两个测试场景中安全（无碰撞）地达到了目标。
</details></li>
</ul>
<hr>
<h2 id="Polynomially-Over-Parameterized-Convolutional-Neural-Networks-Contain-Structured-Strong-Winning-Lottery-Tickets"><a href="#Polynomially-Over-Parameterized-Convolutional-Neural-Networks-Contain-Structured-Strong-Winning-Lottery-Tickets" class="headerlink" title="Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets"></a>Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09858">http://arxiv.org/abs/2311.09858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur da Cunha, Francesco d’Amore, Emanuele Natale</li>
<li>for: 研究SLTH中structured pruning的可行性</li>
<li>methods: 使用多dimensional generalization of Random Subset-Sum Problem来解决SLTH中的随机依赖关系</li>
<li>results: 提出了一种可以将任意小型网络拟合到Structured Pruning中的structured subnetwork的存在，这是SLTH的第一个下 exponential bound，开启了新的研究方向，帮助深入理解深度学习中过参数化的作用。<details>
<summary>Abstract</summary>
The Strong Lottery Ticket Hypothesis (SLTH) states that randomly-initialised neural networks likely contain subnetworks that perform well without any training. Although unstructured pruning has been extensively studied in this context, its structured counterpart, which can deliver significant computational and memory efficiency gains, has been largely unexplored. One of the main reasons for this gap is the limitations of the underlying mathematical tools used in formal analyses of the SLTH. In this paper, we overcome these limitations: we leverage recent advances in the multidimensional generalisation of the Random Subset-Sum Problem and obtain a variant that admits the stochastic dependencies that arise when addressing structured pruning in the SLTH. We apply this result to prove, for a wide class of random Convolutional Neural Networks, the existence of structured subnetworks that can approximate any sufficiently smaller network.   This result provides the first sub-exponential bound around the SLTH for structured pruning, opening up new avenues for further research on the hypothesis and contributing to the understanding of the role of over-parameterization in deep learning.
</details>
<details>
<summary>摘要</summary>
“强大的抽签票假设（SLTH）称 randomly-initialized 神经网络中可能存在不需要训练的子网络，却受到不结构的剪除研究的限制。这里我们与此有关的主要原因之一是下列数学工具的限制：我们使用了最新的多dimensional 普通化的Random Subset-Sum Problem，从而获得了允许随机相依性的variant。我们运用这个结果，证明了一个广泛的随机卷积神经网络中，存在一些可以近似任何较小的网络的结构化子网络。这个结果提供了SLTH关于结构剪除的首个次对数 bounds，对于深度学习的理解做出了新的贡献，并开启了新的研究方向。”
</details></li>
</ul>
<hr>
<h2 id="Contribution-Evaluation-in-Federated-Learning-Examining-Current-Approaches"><a href="#Contribution-Evaluation-in-Federated-Learning-Examining-Current-Approaches" class="headerlink" title="Contribution Evaluation in Federated Learning: Examining Current Approaches"></a>Contribution Evaluation in Federated Learning: Examining Current Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09856">http://arxiv.org/abs/2311.09856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilis Siomos, Jonathan Passerat-Palmbach</li>
<li>for: 这篇论文关注联合学习（Federated Learning，FL）在保持隐私和管理数据的情况下进行模型训练，具体来说是计算每个客户端的贡献值。</li>
<li>methods: 论文回顾当前的贡献评估方法，从数学基础框架到实现方式，并对一些最有前途的state-of-the-art方法进行比较。</li>
<li>results: 论文通过在MNIST和CIFAR-10上 benchmarking不同方法的实验结果，展示了各种方法的不同特点，并引出了设计公平和高效的贡献评估方法的重要性。<details>
<summary>Abstract</summary>
Federated Learning (FL) has seen increasing interest in cases where entities want to collaboratively train models while maintaining privacy and governance over their data. In FL, clients with private and potentially heterogeneous data and compute resources come together to train a common model without raw data ever leaving their locale. Instead, the participants contribute by sharing local model updates, which, naturally, differ in quality. Quantitatively evaluating the worth of these contributions is termed the Contribution Evaluation (CE) problem. We review current CE approaches from the underlying mathematical framework to efficiently calculate a fair value for each client. Furthermore, we benchmark some of the most promising state-of-the-art approaches, along with a new one we introduce, on MNIST and CIFAR-10, to showcase their differences. Designing a fair and efficient CE method, while a small part of the overall FL system design, is tantamount to the mainstream adoption of FL.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Short-vs-Long-term-Coordination-of-Drones-When-Distributed-Optimization-Meets-Deep-Reinforcement-Learning"><a href="#Short-vs-Long-term-Coordination-of-Drones-When-Distributed-Optimization-Meets-Deep-Reinforcement-Learning" class="headerlink" title="Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning"></a>Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09852">http://arxiv.org/abs/2311.09852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuhao Qin, Evangelos Pournaras</li>
<li>for: 本研究旨在提供智能飞行器队伍在智能城市中完成感知功能的支持，包括交通监测和灾害应急应急响应。</li>
<li>methods: 该研究使用分布式优化和深度强化学习（DRL）协调飞行器，以实现成本效果、高质量的导航、感知和充电。</li>
<li>results: 对于交通监测，提出的新进展方法在比较三种基准方法的实验中表现出色，显示了其出色的性能。<details>
<summary>Abstract</summary>
Swarms of smart drones, with the support of charging technology, can provide completing sensing capabilities in Smart Cities, such as traffic monitoring and disaster response. Existing approaches, including distributed optimization and deep reinforcement learning (DRL), aim to coordinate drones to achieve cost-effective, high-quality navigation, sensing, and recharging. However, they have distinct challenges: short-term optimization struggles to provide sustained benefits, while long-term DRL lacks scalability, resilience, and flexibility. To bridge this gap, this paper introduces a new progressive approach that encompasses the planning and selection based on distributed optimization, as well as DRL-based flying direction scheduling. Extensive experiment with datasets generated from realisitic urban mobility demonstrate the outstanding performance of the proposed solution in traffic monitoring compared to three baseline methods.
</details>
<details>
<summary>摘要</summary>
众群智能飞机，受到充电技术支持，可以在智能城市提供完整的感知能力，如交通监测和灾害应急应急。现有的方法，包括分布式优化和深度强化学习（DRL），尝试协调飞机以实现成本效果、高质量的导航、感知和充电。然而，它们具有短期优化困难提供持续性利益，长期DRL缺乏扩展性、可靠性和灵活性。为了bridging这个差距，本文提出了一种新的进步方法，包括分布式优化基础上的规划和选择，以及基于DRL的飞机飞行方向安排。实验表明，提出的解决方案在交通监测中比基准方法三种表现出众。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Augmented-Neural-Processes"><a href="#Diffusion-Augmented-Neural-Processes" class="headerlink" title="Diffusion-Augmented Neural Processes"></a>Diffusion-Augmented Neural Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09848">http://arxiv.org/abs/2311.09848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Bonito, James Requeima, Aliaksandra Shysheya, Richard E. Turner</li>
<li>for: 这 paper 是为了提供一种新的替代方法来模型 neural processes，以更好地适应具有数据稀缺和预测不确定性的应用领域，如健康科学和气候科学。</li>
<li>methods: 该 paper 使用了一种基于扩散的方法，通过对含有噪声的数据进行conditioning来解决许多现有方法的限制，同时也超越了现有最佳实践的性能。</li>
<li>results: 该 paper 的研究结果表明，该新方法可以在各种应用领域中提供更高的准确性和稳定性，并且可以与现有方法进行比较。<details>
<summary>Abstract</summary>
Over the last few years, Neural Processes have become a useful modelling tool in many application areas, such as healthcare and climate sciences, in which data are scarce and prediction uncertainty estimates are indispensable. However, the current state of the art in the field (AR CNPs; Bruinsma et al., 2023) presents a few issues that prevent its widespread deployment. This work proposes an alternative, diffusion-based approach to NPs which, through conditioning on noised datasets, addresses many of these limitations, whilst also exceeding SOTA performance.
</details>
<details>
<summary>摘要</summary>
在过去几年，神经过程（Neural Processes）已成为许多应用领域中的有用模型工具，如医疗和气候科学，在数据稀缺和预测uncertainty估计是不可或缺的。然而，当前领域的状态艺（AR CNPs；布鲁因斯马等，2023）存在一些限制，阻碍其广泛应用。这项工作提出了一种替代方案，基于扩散的方法，通过对噪音数据进行条件，解决了许多这些限制，同时也超越了最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Runtime-Verification-of-Learning-Properties-for-Reinforcement-Learning-Algorithms"><a href="#Runtime-Verification-of-Learning-Properties-for-Reinforcement-Learning-Algorithms" class="headerlink" title="Runtime Verification of Learning Properties for Reinforcement Learning Algorithms"></a>Runtime Verification of Learning Properties for Reinforcement Learning Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09811">http://arxiv.org/abs/2311.09811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Mannucci, Julio de Oliveira Filho</li>
<li>for: 这篇研究旨在提出新的执行时验证技术，以便在RL算法中学习过程中预测学习过程未达或将不会达到期望的质量和时间要求。</li>
<li>methods: 这篇研究使用了新的执行时验证技术，包括三个验证性能，以便在RL算法中监控和评估这些性能 during the system’s operation。</li>
<li>results: 这篇研究获得了三个验证性能，包括学习质量、时间耗用和精度等。这些性能可以用来监控和评估RL算法的学习过程，以便提高学习效率和精度。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) algorithms interact with their environment in a trial-and-error fashion. Such interactions can be expensive, inefficient, and timely when learning on a physical system rather than in a simulation. This work develops new runtime verification techniques to predict when the learning phase has not met or will not meet qualitative and timely expectations. This paper presents three verification properties concerning the quality and timeliness of learning in RL algorithms. With each property, we propose design steps for monitoring and assessing the properties during the system's operation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fossil-2-0-Formal-Certificate-Synthesis-for-the-Verification-and-Control-of-Dynamical-Models"><a href="#Fossil-2-0-Formal-Certificate-Synthesis-for-the-Verification-and-Control-of-Dynamical-Models" class="headerlink" title="Fossil 2.0: Formal Certificate Synthesis for the Verification and Control of Dynamical Models"></a>Fossil 2.0: Formal Certificate Synthesis for the Verification and Control of Dynamical Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09793">http://arxiv.org/abs/2311.09793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alec Edwards, Andrea Peruffo, Alessandro Abate</li>
<li>for: 这篇论文描述了一种新的软件工具——Fossil 2.0，用于 sintesis 有效性证明（如 Lyapunov 函数和障碍函数） для 动力系统。</li>
<li>methods: 这篇论文使用了一种叫 counterexample-guided inductive synthesis（CEGIS）的方法，通过一个 SMT 解决器来验证 candidate 函数的正确性。</li>
<li>results: Fossil 2.0 可以生成更多的证明、控制法则和对离散时间模型的支持。<details>
<summary>Abstract</summary>
This paper presents Fossil 2.0, a new major release of a software tool for the synthesis of certificates (e.g., Lyapunov and barrier functions) for dynamical systems modelled as ordinary differential and difference equations. Fossil 2.0 is much improved from its original release, including new interfaces, a significantly expanded certificate portfolio, controller synthesis and enhanced extensibility. We present these new features as part of this tool paper. Fossil implements a counterexample-guided inductive synthesis (CEGIS) loop ensuring the soundness of the method. Our tool uses neural networks as templates to generate candidate functions, which are then formally proven by an SMT solver acting as an assertion verifier. Improvements with respect to the first release include a wider range of certificates, synthesis of control laws, and support for discrete-time models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GEO-Generative-Engine-Optimization"><a href="#GEO-Generative-Engine-Optimization" class="headerlink" title="GEO: Generative Engine Optimization"></a>GEO: Generative Engine Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09735">http://arxiv.org/abs/2311.09735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik R Narasimhan, Ameet Deshpande</li>
<li>for: 本研究旨在帮助内容创作者提高生成引擎响应中内容的可见度，以便在生成引擎技术升级的未来，保持创作经济的繁荣。</li>
<li>methods: 本研究提出了一种新的优化策略，即生成引擎优化（GEO），通过黑盒优化和定义可见度指标来帮助内容创作者提高生成引擎响应中内容的可见度。</li>
<li>results: 经过系统评估，本研究发现，通过GEO可以提高生成引擎响应中内容的可见度，最高提高40%。此外，研究还发现不同领域的可见度提升策略效果不同，强调需要针对具体领域进行定制。<details>
<summary>Abstract</summary>
The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of Generative Engines (GEs), has the potential to generate accurate and personalized responses, and is rapidly replacing traditional search engines like Google and Bing. Generative Engines typically satisfy queries by synthesizing information from multiple sources and summarizing them with the help of LLMs. While this shift significantly improves \textit{user} utility and \textit{generative search engine} traffic, it results in a huge challenge for the third stakeholder -- website and content creators. Given the black-box and fast-moving nature of Generative Engines, content creators have little to no control over when and how their content is displayed. With generative engines here to stay, the right tools should be provided to ensure that creator economy is not severely disadvantaged. To address this, we introduce Generative Engine Optimization (GEO), a novel paradigm to aid content creators in improving the visibility of their content in Generative Engine responses through a black-box optimization framework for optimizing and defining visibility metrics. We facilitate systematic evaluation in this new paradigm by introducing GEO-bench, a benchmark of diverse user queries across multiple domains, coupled with sources required to answer these queries. Through rigorous evaluation, we show that GEO can boost visibility by up to 40\% in generative engine responses. Moreover, we show the efficacy of these strategies varies across domains, underscoring the need for domain-specific methods. Our work opens a new frontier in the field of information discovery systems, with profound implications for generative engines and content creators.
</details>
<details>
<summary>摘要</summary>
随着大型语言模型（LLM）的出现，一种新的搜索引擎 paradigm 已经出现，这种搜索引擎使用生成模型来收集和摘要信息以回答用户问题。我们称这种技术为生成引擎（GE）。这种技术可以生成准确和个性化的回答，并在传统搜索引擎如Google和Bing的替代品上快速取代。生成引擎通常通过将多个源的信息合并并使用LLM进行摘要来满足用户的查询。虽然这种转变会提高用户的用户体验和生成搜索引擎的搜索量，但是它会对内容创建者造成巨大的挑战。由于生成引擎的黑盒和快速移动的性质，内容创建者几乎没有控制他们的内容是如何和何时显示。为了解决这个问题，我们介绍了生成引擎优化（GEO），一种新的方法，可以帮助内容创建者在生成引擎的回答中提高他们的内容的可见度。我们通过引入GEO-bench，一个包含多个领域的多种用户查询和回答的基准，来促进系统性评估。我们的实验表明，GEO可以提高可见度达40%。此外，我们还发现这些策略在不同的领域中的效果不同，强调了需要针对具体领域的方法。我们的工作开启了一个新的领域，即信息发现系统，对生成引擎和内容创建者产生了深远的影响。
</details></li>
</ul>
<hr>
<h2 id="CDMPP-A-Device-Model-Agnostic-Framework-for-Latency-Prediction-of-Tensor-Programs"><a href="#CDMPP-A-Device-Model-Agnostic-Framework-for-Latency-Prediction-of-Tensor-Programs" class="headerlink" title="CDMPP: A Device-Model Agnostic Framework for Latency Prediction of Tensor Programs"></a>CDMPP: A Device-Model Agnostic Framework for Latency Prediction of Tensor Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09690">http://arxiv.org/abs/2311.09690</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joapolarbear/cdmpp">https://github.com/joapolarbear/cdmpp</a></li>
<li>paper_authors: Hanpeng Hu, Junwei Su, Juntao Zhao, Yanghua Peng, Yibo Zhu, Haibin Lin, Chuan Wu</li>
<li>for: 这个论文旨在提供一种能够准确预测多种tensor程在不同设备上的性能的框架，以便进行DNN图或tensor-level优化和设备选择。</li>
<li>methods: 作者使用了一种叫做CDMPP的框架，该框架使用了一种紧凑的AST表示法和一种基于顺序排序的 pozitional编码方法，以捕捉tensor程的内部结构。并使用了一种域 adapted的方法和KMeans sampling算法，以学习不同域（i.e., 不同的DNN运算和设备）中的域 инвариан特表示。</li>
<li>results: 作者的实验表明，CDMPP在多种DNN模型和设备上表现出色，与状态 искус的基准值相比，CDMPP的预测错误率为14.03%和10.85%，分别为cross-model和cross-device预测。而与之前的基准值相比，CDMPP的训练效率高出一个数量级。实验结果和扩展数据集可以在<a target="_blank" rel="noopener" href="https://github.com/joapolarbear/cdmpp%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/joapolarbear/cdmpp上下载。</a><details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) have shown excellent performance in a wide range of machine learning applications. Knowing the latency of running a DNN model or tensor program on a specific device is useful in various tasks, such as DNN graph- or tensor-level optimization and device selection. Considering the large space of DNN models and devices that impede direct profiling of all combinations, recent efforts focus on building a predictor to model the performance of DNN models on different devices. However, none of the existing attempts have achieved a cost model that can accurately predict the performance of various tensor programs while supporting both training and inference accelerators. We propose CDMPP, an efficient tensor program latency prediction framework for both cross-model and cross-device prediction. We design an informative but efficient representation of tensor programs, called compact ASTs, and a pre-order-based positional encoding method, to capture the internal structure of tensor programs. We develop a domain-adaption-inspired method to learn domain-invariant representations and devise a KMeans-based sampling algorithm, for the predictor to learn from different domains (i.e., different DNN operators and devices). Our extensive experiments on a diverse range of DNN models and devices demonstrate that CDMPP significantly outperforms state-of-the-art baselines with 14.03% and 10.85% prediction error for cross-model and cross-device prediction, respectively, and one order of magnitude higher training efficiency. The implementation and the expanded dataset are available at https://github.com/joapolarbear/cdmpp.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在多种机器学习应用中表现出色。了解在特定设备上运行 DNN 模型或tensor程序的延迟是多种任务中的有用信息，如 DNN 图或tensor程序级别优化和设备选择。由于 DNN 模型和设备之间的空间很大，直接测试所有组合是不可能的。为了解决这个问题，当前的努力集中在建立一个可预测 DNN 模型在不同设备上的性能的模型。然而，现有的尝试都没有实现一个可预测多种tensor程序的性能的成本模型，同时支持训练和推理加速器。我们提出了 CDMPP，一个高效的 tensor程序延迟预测框架，用于跨模型和跨设备预测。我们设计了一种有用但不具有冗余的表示方式，called Compact ASTs，以及一种基于 pre-order 的 pozitional encoding 方法，以捕捉 tensor程序的内部结构。我们开发了一种域 adaptive 方法，用于学习域 invariant 表示，并提出了一种 KMeans 基于采样算法，使预测器从不同域中学习。我们的广泛的实验表明，CDMPP 在多种 DNN 模型和设备上表现出色，与状态对比基线错误率为 14.03% 和 10.85%，具有一个 ORDER 更高的训练效率。实现和扩展数据集可以在 GitHub 上找到：https://github.com/joapolarbear/cdmpp。
</details></li>
</ul>
<hr>
<h2 id="Modelling-daily-mobility-using-mobile-data-traffic-at-fine-spatiotemporal-scale"><a href="#Modelling-daily-mobility-using-mobile-data-traffic-at-fine-spatiotemporal-scale" class="headerlink" title="Modelling daily mobility using mobile data traffic at fine spatiotemporal scale"></a>Modelling daily mobility using mobile data traffic at fine spatiotemporal scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09683">http://arxiv.org/abs/2311.09683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Panayotis Christidis, Maria Vega Gonzalo, Miklos Radics</li>
<li>for: 本研究用数据驱动方法研究了 NetMob 2023 数据集在城市背景下模型流动模式的可用性。</li>
<li>methods: 本研究将 NetMob 2023 数据集与适应度高的外部数据集 ENACT 结合使用，开发了三种 XGBoost 模型，通过对 NetMob2023 数据中的移动数据流量和 ENACT 值进行比较，计算每个 100m x 100m 格子细胞的人口数量。</li>
<li>results: 结果表明，NetMob 2023 数据可以用于 estimate 城市区域的日夜人口和格子级别，并能够解释一些城市流动动态。<details>
<summary>Abstract</summary>
We applied a data-driven approach that explores the usability of the NetMob 2023 dataset in modelling mobility patterns within an urban context. We combined the data with a highly suitable external source, the ENACT dataset, which provides a 1 km x 1km grid with estimates of the day and night population across Europe. We developed three sets of XGBoost models that predict the population in each 100m x 100m grid cell used in NetMob2023 based on the mobile data traffic of the 68 online services covered in the dataset, using the ENACT values as ground truth. The results suggest that the NetMob 2023 data can be useful for the estimation of the day and night population and grid cell level and can explain part of the dynamics of urban mobility.
</details>
<details>
<summary>摘要</summary>
我们采用了数据驱动的方法，探索了NetMob 2023数据集在城市背景下的可用性。我们将数据与非常适合的外部资源——ENACT数据集相结合，该数据集提供了1km x 1km网格中的日夜人口估计数据在欧洲。我们开发了三个XGBoost模型，使用NetMob2023数据集中68个在线服务的流量数据预测每个100m x 100m网格单元中的人口，使用ENACT值作为真实值。结果表明，NetMob 2023数据可以用于 estimate 日夜人口和网格单元层次，并可以解释一部分城市流动性的动态。Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Zenkai-–-Framework-For-Exploring-Beyond-Backpropagation"><a href="#Zenkai-–-Framework-For-Exploring-Beyond-Backpropagation" class="headerlink" title="Zenkai – Framework For Exploring Beyond Backpropagation"></a>Zenkai – Framework For Exploring Beyond Backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09663">http://arxiv.org/abs/2311.09663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/short-greg/zenkai">https://github.com/short-greg/zenkai</a></li>
<li>paper_authors: Greg Short</li>
<li>for: 该paper的目的是提供一个开源框架，以便研究者在深度学习机器建构和训练方面获得更多的控制和灵活性。</li>
<li>methods: 该paper使用分层自主学习机器，每层有其自己的目标和学习算法，以便让研究者在深度学习领域进行更多的探索，如非 differentiable层或不基于错误反射的学习算法。</li>
<li>results: 该paper通过将深度学习机器分割成层次结构，使得研究者可以更容易地探索新的深度学习领域，不再受限于传统的backpropagation框架。<details>
<summary>Abstract</summary>
Zenkai is an open-source framework designed to give researchers more control and flexibility over building and training deep learning machines. It does this by dividing the deep learning machine into layers of semi-autonomous learning machines with their own target and learning algorithm. This is to allow researchers greater exploration such as the use of non-differentiable layers or learning algorithms beyond those based on error backpropagation.   Backpropagation Rumelhart et al. [1986] has powered deep learning to become one of the most exciting fields of the 21st century. As a result, a large number of software tools have been developed to support efficient implementation and training of neural networks through the use of backpropa- gation. While these have been critical to the success of deep learning, building frameworks around backpropagation can make it challenging to implement solutions that do not adhere to it. Zenkai aims to make it easier to get around these limitations and help researchers more easily explore new frontiers in deep learning that do not strictly adhere to the backpropagation framework.
</details>
<details>
<summary>摘要</summary>
zenkai 是一个开源框架，旨在给研究人员更多的控制和灵活性来建立和训练深度学习机器。它通过将深度学习机器分割成各自有target和学习算法的层次结构，以便让研究人员更好地探索不同的学习方法和算法。这样可以让研究人员更容易实现不同的深度学习解决方案，而不是仅仅依赖于error backpropagation。以前，Rumelhart等人在1986年提出了backpropagation算法，这个算法在21世纪的深度学习领域中帮助了深度学习成为一个非常有趣的领域。随着这些软件工具的开发，深度学习的实现和训练变得更加高效。然而，由backpropagation框架所固化的问题使得实现不同的解决方案变得困难。zenkai旨在使研究人员更容易实现不同的深度学习解决方案，并帮助他们更好地探索不同的深度学习领域。
</details></li>
</ul>
<hr>
<h2 id="GAIA-Delving-into-Gradient-based-Attribution-Abnormality-for-Out-of-distribution-Detection"><a href="#GAIA-Delving-into-Gradient-based-Attribution-Abnormality-for-Out-of-distribution-Detection" class="headerlink" title="GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection"></a>GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09620">http://arxiv.org/abs/2311.09620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jgethanchen/gaia-ood">https://github.com/jgethanchen/gaia-ood</a></li>
<li>paper_authors: Jinggang Chen, Junjie Li, Xiaoyang Qu, Jianzong Wang, Jiguang Wan, Jing Xiao</li>
<li>for: 本文旨在提供一种新的方法来探测深度神经网络中的异常示例（Out-of-distribution，OOD），以确保神经网络在实际场景中的可靠性和安全性。</li>
<li>methods: 本文使用了解释predictive decisions的gradient-based attribution方法，并发现这些方法在处理OOD数据时遇到困难，导致解释结果呈现异常。基于这个观察，本文引入了两种OOD检测的异常现象：零减异常和通道平均异常。然后，本文提出了一种简单有效的GAIA方法，它利用Gradient Abnormality Inspection and Aggregation来检测OOD示例。</li>
<li>results: 本文的GAIA方法在CIFAR10和ImageNet-1k benchmark上表现出色，比预后处理方法更有效。具体来说，GAIA在CIFAR10上降低了平均FPR95的值by 23.10%，并在CIFAR100上降低了平均FPR95的值by 45.41%。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) examples is crucial to guarantee the reliability and safety of deep neural networks in real-world settings. In this paper, we offer an innovative perspective on quantifying the disparities between in-distribution (ID) and OOD data -- analyzing the uncertainty that arises when models attempt to explain their predictive decisions. This perspective is motivated by our observation that gradient-based attribution methods encounter challenges in assigning feature importance to OOD data, thereby yielding divergent explanation patterns. Consequently, we investigate how attribution gradients lead to uncertain explanation outcomes and introduce two forms of abnormalities for OOD detection: the zero-deflation abnormality and the channel-wise average abnormality. We then propose GAIA, a simple and effective approach that incorporates Gradient Abnormality Inspection and Aggregation. The effectiveness of GAIA is validated on both commonly utilized (CIFAR) and large-scale (ImageNet-1k) benchmarks. Specifically, GAIA reduces the average FPR95 by 23.10% on CIFAR10 and by 45.41% on CIFAR100 compared to advanced post-hoc methods.
</details>
<details>
<summary>摘要</summary>
检测异常输入（OOD）示例是深度神经网络在实际场景中的可靠性和安全的 garantor。在这篇论文中，我们提出了一种新的观点，即通过分析模型对预测决策的解释来衡量ID和OOD数据之间的差异。这种观点是由我们发现，使用梯度基本的归因方法对OOD数据进行归因时会遇到困难，从而导致解释结果呈现出异常的现象。因此，我们调查了梯度归因过程中的不确定性，并提出了两种OOD检测的异常现象：零膨胀异常和通道平均异常。然后，我们提出了GAIA方法，它通过梯度异常检查和综合来实现简单而有效的OOD检测。GAIA方法在CIFAR10和ImageNet-1k两个标准测试集上验证了其效果，比先进的后置方法减少了平均FPR95的值23.10%和45.41%。
</details></li>
</ul>
<hr>
<h2 id="Generating-Drug-Repurposing-Hypotheses-through-the-Combination-of-Disease-Specific-Hypergraphs"><a href="#Generating-Drug-Repurposing-Hypotheses-through-the-Combination-of-Disease-Specific-Hypergraphs" class="headerlink" title="Generating Drug Repurposing Hypotheses through the Combination of Disease-Specific Hypergraphs"></a>Generating Drug Repurposing Hypotheses through the Combination of Disease-Specific Hypergraphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09596">http://arxiv.org/abs/2311.09596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Jain, Marie Laure-Charpignon, Irene Y. Chen, Anthony Philippakis, Ahmed Alaa</li>
<li>for: 这个研究的目的是提出一种基于生物医学知识图表示的新药再利用预测方法，以便更好地利用现有的药物来开发新的药物。</li>
<li>methods: 这个研究使用了一种新的、疾病特定的质量学习技术，来学习生物路径的上下文嵌入。这种方法可以处理不同长度的生物路径，并且可以从任何一个药物开始，直到疾病为止。此外，这个研究还扩展了这种方法到多种疾病的质量学习。</li>
<li>results: 这个研究发现了两种可能有潜在的新药再利用 канди达，即达巴格利福酮（一种抗糖尿病药物）和德布瑞索酮（一种反高血压药物）。这两种药物在合并两种疾病的质量学习中，其再利用潜力显著提高。<details>
<summary>Abstract</summary>
The drug development pipeline for a new compound can last 10-20 years and cost over 10 billion. Drug repurposing offers a more time- and cost-effective alternative. Computational approaches based on biomedical knowledge graph representations have recently yielded new drug repurposing hypotheses. In this study, we present a novel, disease-specific hypergraph representation learning technique to derive contextual embeddings of biological pathways of various lengths but that all start at any given drug and all end at the disease of interest. Further, we extend this method to multi-disease hypergraphs. To determine the repurposing potential of each of the 1,522 drugs, we derive drug-specific distributions of cosine similarity values and ultimately consider the median for ranking. Cosine similarity values are computed between (1) all biological pathways starting at the considered drug and ending at the disease of interest and (2) all biological pathways starting at drugs currently prescribed against that disease and ending at the disease of interest. We illustrate our approach with Alzheimer's disease (AD) and two of its risk factors: hypertension (HTN) and type 2 diabetes (T2D). We compare each drug's rank across four hypergraph settings (single- or multi-disease): AD only, AD + HTN, AD + T2D, and AD + HTN + T2D. Notably, our framework led to the identification of two promising drugs whose repurposing potential was significantly higher in hypergraphs combining two diseases: dapagliflozin (antidiabetic; moved up, from top 32$\%$ to top 7$\%$, across all considered drugs) and debrisoquine (antihypertensive; moved up, from top 76$\%$ to top 23$\%$). Our approach serves as a hypothesis generation tool, to be paired with a validation pipeline relying on laboratory experiments and semi-automated parsing of the biomedical literature.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:The drug development pipeline for a new compound can last 10-20 years and cost over 10 billion. Drug repurposing offers a more time- and cost-effective alternative. Computational approaches based on biomedical knowledge graph representations have recently yielded new drug repurposing hypotheses. In this study, we present a novel, disease-specific hypergraph representation learning technique to derive contextual embeddings of biological pathways of various lengths but that all start at any given drug and all end at the disease of interest. Further, we extend this method to multi-disease hypergraphs. To determine the repurposing potential of each of the 1,522 drugs, we derive drug-specific distributions of cosine similarity values and ultimately consider the median for ranking. Cosine similarity values are computed between (1) all biological pathways starting at the considered drug and ending at the disease of interest and (2) all biological pathways starting at drugs currently prescribed against that disease and ending at the disease of interest. We illustrate our approach with Alzheimer's disease (AD) and two of its risk factors: hypertension (HTN) and type 2 diabetes (T2D). We compare each drug's rank across four hypergraph settings (single- or multi-disease): AD only, AD + HTN, AD + T2D, and AD + HTN + T2D. Notably, our framework led to the identification of two promising drugs whose repurposing potential was significantly higher in hypergraphs combining two diseases: dapagliflozin (antidiabetic; moved up, from top 32% to top 7%, across all considered drugs) and debrisoquine (antihypertensive; moved up, from top 76% to top 23%). Our approach serves as a hypothesis generation tool, to be paired with a validation pipeline relying on laboratory experiments and semi-automated parsing of the biomedical literature.中文翻译：drugs development pipeline for a new compound can last 10-20 years and cost over 10 billion. drug repurposing offers a more time- and cost-effective alternative. based on biomedical knowledge graph representations, computational approaches have recently yielded new drug repurposing hypotheses. in this study, we present a novel, disease-specific hypergraph representation learning technique to derive contextual embeddings of biological pathways of various lengths but that all start at any given drug and all end at the disease of interest. further, we extend this method to multi-disease hypergraphs. to determine the repurposing potential of each of the 1,522 drugs, we derive drug-specific distributions of cosine similarity values and ultimately consider the median for ranking. cosine similarity values are computed between (1) all biological pathways starting at the considered drug and ending at the disease of interest and (2) all biological pathways starting at drugs currently prescribed against that disease and ending at the disease of interest. we illustrate our approach with Alzheimer's disease (AD) and two of its risk factors: hypertension (HTN) and type 2 diabetes (T2D). we compare each drug's rank across four hypergraph settings (single- or multi-disease): AD only, AD + HTN, AD + T2D, and AD + HTN + T2D. notably, our framework led to the identification of two promising drugs whose repurposing potential was significantly higher in hypergraphs combining two diseases: dapagliflozin (antidiabetic; moved up, from top 32% to top 7%, across all considered drugs) and debrisoquine (antihypertensive; moved up, from top 76% to top 23%). our approach serves as a hypothesis generation tool, to be paired with a validation pipeline relying on laboratory experiments and semi-automated parsing of the biomedical literature.
</details></li>
</ul>
<hr>
<h2 id="Accelerating-material-discovery-with-a-threshold-driven-hybrid-acquisition-policy-based-Bayesian-optimization"><a href="#Accelerating-material-discovery-with-a-threshold-driven-hybrid-acquisition-policy-based-Bayesian-optimization" class="headerlink" title="Accelerating material discovery with a threshold-driven hybrid acquisition policy-based Bayesian optimization"></a>Accelerating material discovery with a threshold-driven hybrid acquisition policy-based Bayesian optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09591">http://arxiv.org/abs/2311.09591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Shoyeb Raihan, Hamed Khosravi, Srinjoy Das, Imtiaz Ahmed<br>for: 本研究旨在提高材料发现和开发过程中的效率，通过应用机器学习技术和bayesian优化方法，从而减少实验成本和开发时间。methods: 本研究使用了一种新的阈值驱动的UCB-EI Bayesian优化方法，它将UCB和EI两种获取函数 dynamically интегриру起来，以优化材料发现过程。results: 对于三个不同的材料数据集，TDUE-BO方法显示了较好的优化和approximation性能，比EI和UCB-based BO方法更快地 converges，并且在RMSE分数上显示出较好的表现。<details>
<summary>Abstract</summary>
Advancements in materials play a crucial role in technological progress. However, the process of discovering and developing materials with desired properties is often impeded by substantial experimental costs, extensive resource utilization, and lengthy development periods. To address these challenges, modern approaches often employ machine learning (ML) techniques such as Bayesian Optimization (BO), which streamline the search for optimal materials by iteratively selecting experiments that are most likely to yield beneficial results. However, traditional BO methods, while beneficial, often struggle with balancing the trade-off between exploration and exploitation, leading to sub-optimal performance in material discovery processes. This paper introduces a novel Threshold-Driven UCB-EI Bayesian Optimization (TDUE-BO) method, which dynamically integrates the strengths of Upper Confidence Bound (UCB) and Expected Improvement (EI) acquisition functions to optimize the material discovery process. Unlike the classical BO, our method focuses on efficiently navigating the high-dimensional material design space (MDS). TDUE-BO begins with an exploration-focused UCB approach, ensuring a comprehensive initial sweep of the MDS. As the model gains confidence, indicated by reduced uncertainty, it transitions to the more exploitative EI method, focusing on promising areas identified earlier. The UCB-to-EI switching policy dictated guided through continuous monitoring of the model uncertainty during each step of sequential sampling results in navigating through the MDS more efficiently while ensuring rapid convergence. The effectiveness of TDUE-BO is demonstrated through its application on three different material datasets, showing significantly better approximation and optimization performance over the EI and UCB-based BO methods in terms of the RMSE scores and convergence efficiency, respectively.
</details>
<details>
<summary>摘要</summary>
技术进步受材料进步的影响很大。然而，找到和开发满足需求的材料往往受到巨大的实验成本、资源占用和长时间的开发周期的阻碍。为了解决这些挑战，现代方法常常使用机器学习（ML）技术，如权重优化（BO），以快速搜索优化材料的属性。然而，传统的BO方法，虽有利，但往往在材料发现过程中困难寻找优化和权衡的平衡，导致表现下降。本文介绍一种新的阈值驱动的UCB-EI权重优化方法（TDUE-BO），它在材料设计空间（MDS）中高效地寻找优化。TDUE-BO方法在开始时使用探索带有UCB的方法，以确保初步扫描MDS的全面性。随着模型收获更多的信息，它逐渐过渡到更加利用的EI方法，专注于之前确定的优点。TDUE-BO方法的UCB-to-EI交换策略，通过监测模型在每次采样中的不确定性的连续监测，以高效地在MDS中导航，并确保更快的收敛。TDUE-BO方法在三个不同的材料数据集上进行应用，与EI和UCB基于BO方法的表现相比，在TERMSE scores和收敛效率上显示出了显著的改进。
</details></li>
</ul>
<hr>
<h2 id="Group-Aware-Interest-Disentangled-Dual-Training-for-Personalized-Recommendation"><a href="#Group-Aware-Interest-Disentangled-Dual-Training-for-Personalized-Recommendation" class="headerlink" title="Group-Aware Interest Disentangled Dual-Training for Personalized Recommendation"></a>Group-Aware Interest Disentangled Dual-Training for Personalized Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09577">http://arxiv.org/abs/2311.09577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaolong-liu-bdsc/igrec">https://github.com/xiaolong-liu-bdsc/igrec</a></li>
<li>paper_authors: Xiaolong Liu, Liangwei Yang, Zhiwei Liu, Xiaohan Li, Mingdai Yang, Chen Wang, Philip S. Yu</li>
<li>for: 这篇论文的目的是为了提高个性化推荐系统的精度和效能，利用社交媒体上的用户群体信息来补充推荐系统的数据缺乏和冷启问题。</li>
<li>methods: 这篇论文提出了一个名为IGRec的方法，它包括四个模组：1. 用户情感分解模组，通过自我阈值来将用户的初始嵌入表示分解为不同的情感分量。2. 用户群体汇总模组，通过Gumbel-Softmax汇总方法来将用户群体的情感分量统计为一个共同的群体表示。3. 用户-群体联合模组，将用户的嵌入表示与他们参加的群体表示联合。4. 双排课预测模组，使用用户-项目和群体-项目互动来训练预测模型。</li>
<li>results: 实验结果显示，IGRec可以有效地解决数据缺乏和冷启问题，并且在群体推荐任务上显示出了更高的信息含量。<details>
<summary>Abstract</summary>
Personalized recommender systems aim to predict users' preferences for items. It has become an indispensable part of online services. Online social platforms enable users to form groups based on their common interests. The users' group participation on social platforms reveals their interests and can be utilized as side information to mitigate the data sparsity and cold-start problem in recommender systems. Users join different groups out of different interests. In this paper, we generate group representation from the user's interests and propose IGRec (Interest-based Group enhanced Recommendation) to utilize the group information accurately. It consists of four modules. (1) Interest disentangler via self-gating that disentangles users' interests from their initial embedding representation. (2) Interest aggregator that generates the interest-based group representation by Gumbel-Softmax aggregation on the group members' interests. (3) Interest-based group aggregation that fuses user's representation with the participated group representation. (4) A dual-trained rating prediction module to utilize both user-item and group-item interactions. We conduct extensive experiments on three publicly available datasets. Results show IGRec can effectively alleviate the data sparsity problem and enhance the recommender system with interest-based group representation. Experiments on the group recommendation task further show the informativeness of interest-based group representation.
</details>
<details>
<summary>摘要</summary>
personalized recommender systems aim to predict users' preferences for items. It has become an indispensable part of online services. online social platforms enable users to form groups based on their common interests. The users' group participation on social platforms reveals their interests and can be utilized as side information to mitigate the data sparsity and cold-start problem in recommender systems. users join different groups out of different interests. In this paper, we generate group representation from the user's interests and propose IGRec (Interest-based Group enhanced Recommendation) to utilize the group information accurately. It consists of four modules. (1) Interest disentangler via self-gating that disentangles users' interests from their initial embedding representation. (2) Interest aggregator that generates the interest-based group representation by Gumbel-Softmax aggregation on the group members' interests. (3) Interest-based group aggregation that fuses user's representation with the participated group representation. (4) A dual-trained rating prediction module to utilize both user-item and group-item interactions. We conduct extensive experiments on three publicly available datasets. results show IGRec can effectively alleviate the data sparsity problem and enhance the recommender system with interest-based group representation. experiments on the group recommendation task further show the informativeness of interest-based group representation.Here's the breakdown of the translation:* "personalized recommender systems" becomes "个性化推荐系统" (gèxìnghuà zhìdòng yìxìng zhìdòng)* "aim to predict users' preferences for items" becomes "预测用户对物品的喜好" (yùndào yìzhí yìxìng zhìdòng)* "online social platforms" becomes "在线社交平台" (zài xiàng xìng zhìdòng)* "enable users to form groups based on their common interests" becomes "允许用户根据共同的兴趣组成群体" (shèngxìn yìxìng zhìdòng)* "users join different groups out of different interests" becomes "用户根据不同的兴趣加入不同的群体" (yìxìng zhìdòng zài bùdìng de xìngxìng)* "Interest-based Group enhanced Recommendation" becomes "兴趣基于群体增强推荐" (yìxìng jīyào qúnwù zhìdòng zhìdòng)* "consists of four modules" becomes "包括四个模块" (bāng xīn sì ge móudào)* "Interest disentangler via self-gating" becomes "自我阻塞来消除用户兴趣的混合" (zìwǒ zhìxíng lái xiāoxiǎo yìxìng zhìdòng)* "Interest aggregator" becomes "兴趣聚合器" (yìxìng jùhégōng)* "Interest-based group aggregation" becomes "兴趣基于群体聚合" (yìxìng jīyào qúnwù jùhégōng)* "dual-trained rating prediction module" becomes "双向训练评分模块" (shuāng xiàng xiǎng zhìxíng píngfāng móudào)* "extensive experiments on three publicly available datasets" becomes "在三个公开数据集上进行了广泛的实验" (zài sān gè gōngkāi xìngxìng zhìdòng zhìdòng)* "results show IGRec can effectively alleviate the data sparsity problem" becomes "结果显示 IGRec 可以有效解决数据缺乏问题" (jiéguī xiǎnshì IGRec kěyì yǒu jìngxìng duōshì wèn tí)* "experiments on the group recommendation task further show the informativeness of interest-based group representation" becomes "在群体推荐任务上进行的实验再次表明兴趣基于群体表示的有用性" (zài qúnwù zhìdòng zhìdòng shì de jìngxìng yǐngxìng)
</details></li>
</ul>
<hr>
<h2 id="A-Knowledge-Distillation-Approach-for-Sepsis-Outcome-Prediction-from-Multivariate-Clinical-Time-Series"><a href="#A-Knowledge-Distillation-Approach-for-Sepsis-Outcome-Prediction-from-Multivariate-Clinical-Time-Series" class="headerlink" title="A Knowledge Distillation Approach for Sepsis Outcome Prediction from Multivariate Clinical Time Series"></a>A Knowledge Distillation Approach for Sepsis Outcome Prediction from Multivariate Clinical Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09566">http://arxiv.org/abs/2311.09566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anna Wong, Shu Ge, Nassim Oufattole, Adam Dejl, Megan Su, Ardavan Saeedi, Li-wei H. Lehman</li>
<li>for: 预测 septic 病人结果，学习可读性的状态表示</li>
<li>methods: 使用知识塑化 via 约束变量推断，将师网络模型的知识塑化到学生网络模型中，以实现高预测性和可读性</li>
<li>results: 使用实际数据，在 MIMIC-IV 数据库上训练 LSTM 作为师网络模型，预测 septic 病人死亡率，并使用 AR-HMM 学习可读性的隐藏状态表示，预测多个下游结果，包括医院死亡率、肺液肿、需要药物、透析和机械呼吸等。结果表明，我们的方法可以成功integrate constraint，实现高预测性和可读性。<details>
<summary>Abstract</summary>
Sepsis is a life-threatening condition triggered by an extreme infection response. Our objective is to forecast sepsis patient outcomes using their medical history and treatments, while learning interpretable state representations to assess patients' risks in developing various adverse outcomes. While neural networks excel in outcome prediction, their limited interpretability remains a key issue. In this work, we use knowledge distillation via constrained variational inference to distill the knowledge of a powerful "teacher" neural network model with high predictive power to train a "student" latent variable model to learn interpretable hidden state representations to achieve high predictive performance for sepsis outcome prediction. Using real-world data from the MIMIC-IV database, we trained an LSTM as the "teacher" model to predict mortality for sepsis patients, given information about their recent history of vital signs, lab values and treatments. For our student model, we use an autoregressive hidden Markov model (AR-HMM) to learn interpretable hidden states from patients' clinical time series, and use the posterior distribution of the learned state representations to predict various downstream outcomes, including hospital mortality, pulmonary edema, need for diuretics, dialysis, and mechanical ventilation. Our results show that our approach successfully incorporates the constraint to achieve high predictive power similar to the teacher model, while maintaining the generative performance.
</details>
<details>
<summary>摘要</summary>
伤害是一种生命威胁的疾病，由于感染过程的极端反应而引起。我们的目标是预测患有伤害患者的结果，使用他们的医疗历史和治疗方法，同时学习可读取的状态表示，以评估患者在不同的不良结果发展中的风险。虽然神经网络在结果预测方面表现出色，但它们的解释能力受限。在这种工作中，我们使用知识填充via受限变量推理来填充教师神经网络模型的知识，以训练学生隐藏变量模型，以学习可读取的隐藏状态表示，以实现高度预测性和解释能力。使用实际数据库，我们训练了LSTM作为教师模型，以预测伤害患者的死亡，根据他们的近期生命体征、实验室值和治疗方法的信息。为学生模型，我们使用自适应隐藏马尔可夫模型（AR-HMM）来学习患者的临床时序序列中的可读取隐藏状态，并使用学习的 posterior 分布来预测多个下游结果，包括医院死亡率、肺液肿、需要药物、人工呼吸和肾透析。我们的结果表明，我们的方法可以成功地满足Constraint来实现高度预测力和解释能力，同时保持生成性能。
</details></li>
</ul>
<hr>
<h2 id="Know-Thy-Neighbors-A-Graph-Based-Approach-for-Effective-Sensor-Based-Human-Activity-Recognition-in-Smart-Homes"><a href="#Know-Thy-Neighbors-A-Graph-Based-Approach-for-Effective-Sensor-Based-Human-Activity-Recognition-in-Smart-Homes" class="headerlink" title="Know Thy Neighbors: A Graph Based Approach for Effective Sensor-Based Human Activity Recognition in Smart Homes"></a>Know Thy Neighbors: A Graph Based Approach for Effective Sensor-Based Human Activity Recognition in Smart Homes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09514">http://arxiv.org/abs/2311.09514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srivatsa P, Thomas Plötz</li>
<li>for: 这个研究旨在提高智能家居中的人活动识别（HAR）系统，以扩展智能家居技术和帮助生活技术。</li>
<li>methods: 本研究提出了一个新的图形引导神经网络方法，通过学习感知器之间的明确共识关系，以解决现有HAR系统对于变化、罕见和噪音等问题的限制。</li>
<li>results: 本研究在CASAS数据集上进行了多个实验，结果显示了 graph-guided neural network 在智能家居HAR中的表现，在多个数据集和大幅优势的情况下超越了现有的方法。这些结果显示了 HAR 系统在实际应用中的潜力。<details>
<summary>Abstract</summary>
There has been a resurgence of applications focused on Human Activity Recognition (HAR) in smart homes, especially in the field of ambient intelligence and assisted living technologies. However, such applications present numerous significant challenges to any automated analysis system operating in the real world, such as variability, sparsity, and noise in sensor measurements. Although state-of-the-art HAR systems have made considerable strides in addressing some of these challenges, they especially suffer from a practical limitation: they require successful pre-segmentation of continuous sensor data streams before automated recognition, i.e., they assume that an oracle is present during deployment, which is capable of identifying time windows of interest across discrete sensor events. To overcome this limitation, we propose a novel graph-guided neural network approach that performs activity recognition by learning explicit co-firing relationships between sensors. We accomplish this by learning a more expressive graph structure representing the sensor network in a smart home, in a data-driven manner. Our approach maps discrete input sensor measurements to a feature space through the application of attention mechanisms and hierarchical pooling of node embeddings. We demonstrate the effectiveness of our proposed approach by conducting several experiments on CASAS datasets, showing that the resulting graph-guided neural network outperforms the state-of-the-art method for HAR in smart homes across multiple datasets and by large margins. These results are promising because they push HAR for smart homes closer to real-world applications.
</details>
<details>
<summary>摘要</summary>
随着智能家居技术的发展，人活动识别（HAR）应用也在智能家居领域得到了新的推动。特别是在 ambient intelligence 和助生技术领域，HAR 应用已成为当前研究热点。然而，实际世界中的 HAR 系统面临着许多挑战，如感知器的变化、缺失和噪声等问题。尽管现有的 HAR 系统已经做出了很大的进步，但它们尤其受到一种实际限制：它们需要成功地预分 segments 持续的感知数据流，以便自动识别活动。为了突破这一限制，我们提出了一种新的图导型神经网络方法，该方法通过学习感知器之间的显式协同关系来进行活动识别。我们通过在数据驱动方式下学习更加表达式的图结构，将感知网络在智能家居中映射到具有表达能力的特征空间。我们的方法通过注意机制和层次聚合节点嵌入来将离散输入感知测量映射到特征空间。我们在 CASAS 数据集上进行了多个实验，并证明了我们提出的图导型神经网络方法在智能家居中 HAR 方面的效果较为出色，与当前状态艺技术相比，差距较大。这些结果是推动 HAR 在智能家居中的实际应用的好消息。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Systems-with-Symmetries-using-Equivariant-Autoregressive-Reservoir-Computers"><a href="#Identifying-Systems-with-Symmetries-using-Equivariant-Autoregressive-Reservoir-Computers" class="headerlink" title="Identifying Systems with Symmetries using Equivariant Autoregressive Reservoir Computers"></a>Identifying Systems with Symmetries using Equivariant Autoregressive Reservoir Computers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09511">http://arxiv.org/abs/2311.09511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fredy Vides, Idelfonso B. R. Nogueira, Lendy Banegas, Evelyn Flores</li>
<li>for: 本文使用非线性时阶 embedding 技术来识别具有对称性的系统。</li>
<li>methods: 文中提出了一种双重方法，包括对时间序列数据进行非对称时阶嵌入，并使用稀疏最小二乘方法来描述输出联系矩阵。</li>
<li>results: 文中结果表明，使用这些技术可以有效地识别和预测具有对称性的非线性系统，无论这些系统是否 exhibits 乱流行为。<details>
<summary>Abstract</summary>
The investigation reported in this document focuses on identifying systems with symmetries using equivariant autoregressive reservoir computers. General results in structured matrix approximation theory are presented, exploring a two-fold approach. Firstly, a comprehensive examination of generic symmetry-preserving nonlinear time delay embedding is conducted. This involves analyzing time series data sampled from an equivariant system under study. Secondly, sparse least-squares methods are applied to discern approximate representations of the output coupling matrices. These matrices play a pivotal role in determining the nonlinear autoregressive representation of an equivariant system. The structural characteristics of these matrices are dictated by the set of symmetries inherent in the system. The document outlines prototypical algorithms derived from the described techniques, offering insight into their practical applications. Emphasis is placed on their effectiveness in the identification and predictive simulation of equivariant nonlinear systems, regardless of whether such systems exhibit chaotic behavior.
</details>
<details>
<summary>摘要</summary>
这份报告的调查集中关注使用对称 Autoregressive 计算机系统来识别具有对称性的系统。报告提供了一般结果，探讨了两种方法：首先，对具有对称性的非线性时间延迟嵌入进行全面的分析，这是通过分析研究中的对称系统时间序列数据来实现的。其次，使用稀疏最小二乘方法来突出输出联系矩阵的 Approximate 表示。这些矩阵在确定非线性 Autoregressive 表示中扮演重要的角色，其结构特征由系统中的对称性决定。报告描述了基于这些技术的评估算法，并强调其在识别和预测非线性系统中的有效性，无论这些系统是否展现混沌行为。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Impact-of-Weight-Sharing-Decisions-on-Knowledge-Transfer-in-Continual-Learning"><a href="#Investigating-the-Impact-of-Weight-Sharing-Decisions-on-Knowledge-Transfer-in-Continual-Learning" class="headerlink" title="Investigating the Impact of Weight Sharing Decisions on Knowledge Transfer in Continual Learning"></a>Investigating the Impact of Weight Sharing Decisions on Knowledge Transfer in Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09506">http://arxiv.org/abs/2311.09506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Andle, Ali Payani, Salimeh Yasaei-Sekeh</li>
<li>for: 本研究旨在 investigate 如何在 Continual Learning (CL) 中进行 Forward Knowledge Transfer (FKT) between tasks, 以便提高 neural network 的效率和适应性。</li>
<li>methods: 本研究使用 pruning methods 来训练 CL 的 subnetworks, 并通过 sharing prior subnetworks’ weights 来实现 FKT。</li>
<li>results: 研究发现，在不同任务的复杂度和相似性的情况下，有优化的 weight sharing 决策可以提高任务的准确率。 通过遵循这些决策，我们可以在 CL 中提高任务的性能。<details>
<summary>Abstract</summary>
Continual Learning (CL) has generated attention as a method of avoiding Catastrophic Forgetting (CF) in the sequential training of neural networks, improving network efficiency and adaptability to different tasks. Additionally, CL serves as an ideal setting for studying network behavior and Forward Knowledge Transfer (FKT) between tasks. Pruning methods for CL train subnetworks to handle the sequential tasks which allows us to take a structured approach to investigating FKT. Sharing prior subnetworks' weights leverages past knowledge for the current task through FKT. Understanding which weights to share is important as sharing all weights can yield sub-optimal accuracy. This paper investigates how different sharing decisions affect the FKT between tasks. Through this lens we demonstrate how task complexity and similarity influence the optimal weight sharing decisions, giving insights into the relationships between tasks and helping inform decision making in similar CL methods. We implement three sequential datasets designed to emphasize variation in task complexity and similarity, reporting results for both ResNet-18 and VGG-16. By sharing in accordance with the decisions supported by our findings, we show that we can improve task accuracy compared to other sharing decisions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Network-Wide-Evacuation-Traffic-Prediction-in-a-Rapidly-Intensifying-Hurricane-from-Traffic-Detectors-and-Facebook-Movement-Data-A-Deep-Learning-Approach"><a href="#Network-Wide-Evacuation-Traffic-Prediction-in-a-Rapidly-Intensifying-Hurricane-from-Traffic-Detectors-and-Facebook-Movement-Data-A-Deep-Learning-Approach" class="headerlink" title="Network Wide Evacuation Traffic Prediction in a Rapidly Intensifying Hurricane from Traffic Detectors and Facebook Movement Data: A Deep Learning Approach"></a>Network Wide Evacuation Traffic Prediction in a Rapidly Intensifying Hurricane from Traffic Detectors and Facebook Movement Data: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09498">http://arxiv.org/abs/2311.09498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Mobasshir Rashid, Rezaur Rahman, Samiul Hasan</li>
<li>for: 预测风暴撤离时的交通流量是重要的，可以帮助优化交通基建的使用，从而减少撤离时间。</li>
<li>methods: 该研究使用了交通探测器和Facebook移动数据，开发了一种基于深度学习的交通预测模型，以便预测风暴撤离期间的交通流量。</li>
<li>results: 模型在常规时间段（5月-8月）的数据上进行了训练，并在风暴撤离期间使用test数据进行预测，其中Accuracy为95%（RMSE &#x3D; 356），但在风暴撤离期间，模型表现不佳，Accuracy为55%（RMSE &#x3D; 1084）。然后，研究人员采用了传输学习方法，使用预训练的模型和更多的撤离相关特征进行预测，最终模型的Accuracy提高至89%（RMSE &#x3D; 514）。再次添加Facebook移动数据，模型的RMSE值降至393，并提高了Accuracy至93%。<details>
<summary>Abstract</summary>
Traffic prediction during hurricane evacuation is essential for optimizing the use of transportation infrastructures. It can reduce evacuation time by providing information on future congestion in advance. However, evacuation traffic prediction can be challenging as evacuation traffic patterns is significantly different than regular period traffic. A data-driven traffic prediction model is developed in this study by utilizing traffic detector and Facebook movement data during Hurricane Ian, a rapidly intensifying hurricane. We select 766 traffic detectors from Florida's 4 major interstates to collect traffic features. Additionally, we use Facebook movement data collected during Hurricane Ian's evacuation period. The deep-learning model is first trained on regular period (May-August 2022) data to understand regular traffic patterns and then Hurricane Ian's evacuation period data is used as test data. The model achieves 95% accuracy (RMSE = 356) during regular period, but it underperforms with 55% accuracy (RMSE = 1084) during the evacuation period. Then, a transfer learning approach is adopted where a pretrained model is used with additional evacuation related features to predict evacuation period traffic. After transfer learning, the model achieves 89% accuracy (RMSE = 514). Adding Facebook movement data further reduces model's RMSE value to 393 and increases accuracy to 93%. The proposed model is capable to forecast traffic up to 6-hours in advance. Evacuation traffic management officials can use the developed traffic prediction model to anticipate future traffic congestion in advance and take proactive measures to reduce delays during evacuation.
</details>
<details>
<summary>摘要</summary>
预测风暴撤离交通是至关重要的，以便优化交通基础设施的使用。它可以降低撤离时间，通过提供未来堵塞的信息。然而，撤离交通预测可能是挑战，因为撤离交通模式与常规时间交通模式有所不同。本研究中提出了一种基于数据驱动的交通预测模型，通过利用交通检测器和Facebook运动数据进行风暴伊安的撤离期间预测。我们选择了766个交通检测器，分别位于佛罗里达州的4大高速公路。此外，我们还使用了在风暴伊安撤离期间收集的Facebook运动数据。深度学习模型首先在常规时间（5月-8月2022年）的数据上训练，以理解常规交通模式，然后使用风暴伊安撤离期间的数据进行测试。模型在常规时间上达到95%的准确率（RMSE=356），但在撤离期间表现不佳，准确率为55%（RMSE=1084）。然后，我们采用了传输学习方法，使用预训练的模型，并添加了撤离相关特征。经过传输学习，模型的准确率提高到89%（RMSE=514）。再次添加Facebook运动数据，模型的RMSE值降低至393，准确率提高到93%。该模型可以预测交通情况，并且可以预测交通情况到6小时之前。风暴撤离管理官员可以使用该模型预测未来交通堵塞，并采取积极措施，以降低撤离过程中的延迟。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Bayesian-Neural-Networks"><a href="#Spatial-Bayesian-Neural-Networks" class="headerlink" title="Spatial Bayesian Neural Networks"></a>Spatial Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09491">http://arxiv.org/abs/2311.09491</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andrewzm/sbnn">https://github.com/andrewzm/sbnn</a></li>
<li>paper_authors: Andrew Zammit-Mangion, Michael D. Kaminski, Ba-Hien Tran, Maurizio Filippone, Noel Cressie</li>
<li>for: 这个论文的目的是提出一种新的空间过程模型，即空间 bayesian neural network (SBNN)，用于更好地描述空间数据中的不同过程。</li>
<li>methods: 该论文使用了一种新的方法，即在 Bayesian neural network 中添加空间嵌入层，以适应空间环境。此外，论文还提出了一些variants of SBNNs，以及如何使用这些模型来表示各种常见的空间过程。</li>
<li>results: 论文的结果表明，SBNNs 可以比传统的 Bayesian neural network 更好地描述空间数据中的不同过程，并且可以用于表示各种常见的空间过程。此外，论文还提出了一些新的工具来进行 SBNNs 的推断。<details>
<summary>Abstract</summary>
Statistical models for spatial processes play a central role in statistical analyses of spatial data. Yet, it is the simple, interpretable, and well understood models that are routinely employed even though, as is revealed through prior and posterior predictive checks, these can poorly characterise the spatial heterogeneity in the underlying process of interest. Here, we propose a new, flexible class of spatial-process models, which we refer to as spatial Bayesian neural networks (SBNNs). An SBNN leverages the representational capacity of a Bayesian neural network; it is tailored to a spatial setting by incorporating a spatial "embedding layer" into the network and, possibly, spatially-varying network parameters. An SBNN is calibrated by matching its finite-dimensional distribution at locations on a fine gridding of space to that of a target process of interest. That process could be easy to simulate from or we have many realisations from it. We propose several variants of SBNNs, most of which are able to match the finite-dimensional distribution of the target process at the selected grid better than conventional BNNs of similar complexity. We also show that a single SBNN can be used to represent a variety of spatial processes often used in practice, such as Gaussian processes and lognormal processes. We briefly discuss the tools that could be used to make inference with SBNNs, and we conclude with a discussion of their advantages and limitations.
</details>
<details>
<summary>摘要</summary>
统计模型 для空间过程扮演了中心角色于统计分析空间数据中。然而，即使是简单、可解释、具有良好了解的模型仍然广泛使用，尽管，通过先前和后预测检查，这些模型可能不能准确捕捉空间过程中的差异性。我们提出了一种新的、灵活的空间过程模型，称为空间 bayesian neural network（SBNN）。一个 SBNN 利用了 bayesian neural network 的表达能力，并在网络中添加了空间“嵌入层”，以适应空间设置。一个 SBNN 通过匹配其在细网格上的finite-dimensional分布与目标过程的分布来调整。该过程可以是容易从 simulate 出来的或者我们有很多实例来源。我们提出了一些 SBNN 的变体，大多数可以在相同的复杂性水平上比 conventional BNNs 更好地匹配目标过程的finite-dimensional分布。我们还表明了一个 SBNN 可以用来表示一些常用的空间过程，如 Gaussian 过程和 lognormal 过程。我们 briefly 讨论了使用 SBNNs 进行推理的工具，并结束于一个关于其优点和局限性的讨论。
</details></li>
</ul>
<hr>
<h2 id="Soft-Matching-Distance-A-metric-on-neural-representations-that-captures-single-neuron-tuning"><a href="#Soft-Matching-Distance-A-metric-on-neural-representations-that-captures-single-neuron-tuning" class="headerlink" title="Soft Matching Distance: A metric on neural representations that captures single-neuron tuning"></a>Soft Matching Distance: A metric on neural representations that captures single-neuron tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09466">http://arxiv.org/abs/2311.09466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meenakshi Khosla, Alex H. Williams</li>
<li>for:  This paper aims to develop a stricter notion of representational (dis)similarity that requires individual neuron matching across networks, and to generalize this metric to compare networks with different sizes.</li>
<li>methods: The paper uses optimal transport theory to derive a natural generalization of the distance metric based on “soft” permutations, which is symmetric, satisfies the triangle inequality, and can be interpreted as a Wasserstein distance between two empirical distributions.</li>
<li>results: The proposed metric avoids counter-intuitive outcomes suffered by alternative approaches and captures complementary geometric insights into neural representations that are entirely missed by rotation-invariant metrics.<details>
<summary>Abstract</summary>
Common measures of neural representational (dis)similarity are designed to be insensitive to rotations and reflections of the neural activation space. Motivated by the premise that the tuning of individual units may be important, there has been recent interest in developing stricter notions of representational (dis)similarity that require neurons to be individually matched across networks. When two networks have the same size (i.e. same number of neurons), a distance metric can be formulated by optimizing over neuron index permutations to maximize tuning curve alignment. However, it is not clear how to generalize this metric to measure distances between networks with different sizes. Here, we leverage a connection to optimal transport theory to derive a natural generalization based on "soft" permutations. The resulting metric is symmetric, satisfies the triangle inequality, and can be interpreted as a Wasserstein distance between two empirical distributions. Further, our proposed metric avoids counter-intuitive outcomes suffered by alternative approaches, and captures complementary geometric insights into neural representations that are entirely missed by rotation-invariant metrics.
</details>
<details>
<summary>摘要</summary>
通用的神经表示（不）相似性度量是设计为感知到旋转和反射的神经活动空间的变换。驱动于各个单元的调音可能是重要的，有些时候有关注于开发更严格的神经表示（不）相似性度量，需要神经网络中的单元在不同网络中匹配。当两个网络有相同的大小（即同样多个单元）时，可以通过最大化神经单元索引Permutation来形式化距离度量。但是，不清楚如何推广这个度量来度量不同大小的网络之间的距离。我们利用了优化运输理论的连接， derivate一个自然的推广，基于"软" Permutation。这个度量是对称的，满足三角不等式，可以被解释为两个empirical分布之间的沃asserstein距离。此外，我们提出的度量可以避免其他方法所导致的不合理的结果，同时捕捉神经表示中完全被旋转不变度量所遗弃的几何视角。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/cs.LG_2023_11_16/" data-id="clpahu77900v93h887p6uedhv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/eess.IV_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T09:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/eess.IV_2023_11_16/">eess.IV - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Investigating-the-Use-of-Traveltime-and-Reflection-Tomography-for-Deep-Learning-Based-Sound-Speed-Estimation-in-Ultrasound-Computed-Tomography"><a href="#Investigating-the-Use-of-Traveltime-and-Reflection-Tomography-for-Deep-Learning-Based-Sound-Speed-Estimation-in-Ultrasound-Computed-Tomography" class="headerlink" title="Investigating the Use of Traveltime and Reflection Tomography for Deep Learning-Based Sound-Speed Estimation in Ultrasound Computed Tomography"></a>Investigating the Use of Traveltime and Reflection Tomography for Deep Learning-Based Sound-Speed Estimation in Ultrasound Computed Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10193">http://arxiv.org/abs/2311.10193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gangwon Jeong, Fu Li, Umberto Villa, Mark A. Anastasio</li>
<li>for: 这个研究的目的是为了使用深度学习方法来提高ultrasound computed tomography（USCT）中的速度声速（SOS）的重建精度，并且 investigate the impact of chosen input modalities on image-to-image learned reconstruction（IILR）方法。</li>
<li>methods: 这个研究使用了一种名为convolutional neural network（CNN）的深度学习模型，该模型可以将双通道（TT和RT图像）转换为高分辨率的SOS地图。此外，研究还使用了一种权重重建损失函数，以便在训练过程中增强特征区域的检测。</li>
<li>results: 研究发现，使用双通道输入可以提高IILR方法的重建精度和特征区域的检测性能。单通道输入（TT或RT图像）alone的情况下，重建精度和特征区域的检测性能均较差。<details>
<summary>Abstract</summary>
Ultrasound computed tomography (USCT) is actively being developed to quantify acoustic tissue properties such as the speed-of-sound (SOS). Although full-waveform inversion (FWI) is an effective method for accurate SOS reconstruction, it can be computationally challenging for large-scale problems. Deep learning-based image-to-image learned reconstruction (IILR) methods are being investigated as scalable and computationally efficient alternatives. This study investigates the impact of the chosen input modalities on IILR methods for high-resolution SOS reconstruction in USCT. The selected modalities are traveltime tomography (TT) and reflection tomography (RT), which produce a low-resolution SOS map and a reflectivity map, respectively. These modalities have been chosen for their lower computational cost relative to FWI and their capacity to provide complementary information: TT offers a direct -- while low resolution -- SOS measure, while RT reveals tissue boundary information. Systematic analyses were facilitated by employing a stylized USCT imaging system with anatomically realistic numerical breast phantoms. Within this testbed, a supervised convolutional neural network (CNN) was trained to map dual-channel (TT and RT images) to a high-resolution SOS map. Moreover, the CNN was fine-tuned using a weighted reconstruction loss that prioritized tumor regions to address tumor underrepresentation in the training dataset. To understand the benefits of employing dual-channel inputs, single-input CNNs were trained separately using inputs from each modality alone (TT or RT). The methods were assessed quantitatively using normalized root mean squared error and structural similarity index measure for reconstruction accuracy and receiver operating characteristic analysis to assess signal detection-based performance measures.
</details>
<details>
<summary>摘要</summary>
美国计算 Tomography (USCT) 目前在发展中，以量化声学组织特性，如声速 (SOS) 为目标。虽然全波形反射 (FWI) 是一种高精度的 SOS 重建方法，但可能会对大规模问题具有计算挑战。深度学习基于图像到图像学习的方法被调查为可扩展和计算高效的替代方案。本研究研究了选择的输入模式对 IILR 方法的高分辨率 SOS 重建影响。选择的模式包括旅游时间 Tomography (TT) 和反射 Tomography (RT)，它们生成了低分辨率 SOS 地图和反射图像，分别。这些模式选择的原因是它们的计算成本较低，并且可以提供补充信息：TT 提供了直接 -- 低分辨率 -- SOS 测量，而 RT 揭示了组织边界信息。在使用静态 USCT 图像系统和数字胸部phantom进行系统性分析的测试环境中，一个以图像为输入的 convolutional neural network (CNN) 被训练来将双通道 (TT 和 RT 图像) 映射到高分辨率 SOS 地图。此外，CNN 还被微调使用一个权重重建损失函数，以优先级刻意诊断区域，以 Addressing tumor underrepresentation in the training dataset。为了了解使用双通道输入的好处，单通道 CNNs 分别使用每个模式的输入图像来训练（TT 或 RT）。这些方法被评估量化使用 normalized root mean squared error 和 structure similarity index measure 来评估重建精度和 Receiver operating characteristic analysis 来评估基于信号检测的性能指标。
</details></li>
</ul>
<hr>
<h2 id="Combined-Channel-and-Spatial-Attention-based-Stereo-Endoscopic-Image-Super-Resolution"><a href="#Combined-Channel-and-Spatial-Attention-based-Stereo-Endoscopic-Image-Super-Resolution" class="headerlink" title="Combined Channel and Spatial Attention-based Stereo Endoscopic Image Super-Resolution"></a>Combined Channel and Spatial Attention-based Stereo Endoscopic Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10115">http://arxiv.org/abs/2311.10115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mansoor Hayat, Supavadee Armvith, Dr. Titipat Achakulvisut</li>
<li>for: 这篇论文旨在推广endoscopic imaging技术的应用在医学诊断和手术领域，提高医生和 Physician 对病人器官的解剖知识。</li>
<li>methods: 本文提出了一种基于混合通道和空间注意力块的特征提取方法，并结合了一种特有但非常强的parallax attention模块（PAM），用于endoscopic图像超分辨。</li>
<li>results: 根据da Vinci数据集的训练，提出的模型可以提高PSNR值达2.12 dB（比较2）和1.29 dB（比较4），同时SSIM值也提高了0.03（比较2）和0.0008（比较4）。这种方法可以帮助医生和Physician更准确地诊断和治疗endoscopic图像。<details>
<summary>Abstract</summary>
Stereo Imaging technology integration into medical diagnostics and surgeries brings a great revolution in the field of medical sciences. Now, surgeons and physicians have better insight into the anatomy of patients' organs. Like other technologies, stereo cameras have limitations, e.g., low resolution (LR) and blurry output images. Currently, most of the proposed techniques for super-resolution focus on developing complex blocks and complicated loss functions, which cause high system complexity. We proposed a combined channel and spatial attention block to extract features incorporated with a specific but very strong parallax attention module (PAM) for endoscopic image super-resolution. The proposed model is trained using the da Vinci dataset on scales 2 and 4. Our proposed model has improved PSNR up to 2.12 dB for scale 2 and 1.29 dB for scale 4, while SSIM is improved by 0.03 for scale 2 and 0.0008 for scale 4. By incorporating this method, diagnosis and treatment for endoscopic images can be more accurate and effective.
</details>
<details>
<summary>摘要</summary>
单声图像技术在医疗诊断和手术中得到了很大的革命，为医疗科学带来了更好的顾问。现在医生和医生都可以更好地了解患者的器官 анато�。然而，单声摄像头也有其限制，例如低分辨率（LR）和模糊的输出图像。现在大多数提议的超解析技术都是建立复杂的封包和复杂的损失函数，这会导致高系统复杂性。我们提出了一个结合通道和空间注意力块的特殊专注模组（PAM），用于检测照片超解析。我们的提议模型在 scales 2 和 4 上训练，实现了 PSNR 的提升至 2.12 dB 和 1.29 dB，而 SSIM 则提高了 0.03 和 0.0008。通过这种方法，医疗诊断和治疗可以更精准和有效。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/eess.IV_2023_11_16/" data-id="clpahu7et01dp3h88bcfo5zzr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/eess.SP_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T08:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/eess.SP_2023_11_16/">eess.SP - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Near-Field-Velocity-Sensing-and-Predictive-Beamforming"><a href="#Near-Field-Velocity-Sensing-and-Predictive-Beamforming" class="headerlink" title="Near-Field Velocity Sensing and Predictive Beamforming"></a>Near-Field Velocity Sensing and Predictive Beamforming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09888">http://arxiv.org/abs/2311.09888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaolin Wang, Xidong Mu, Yuanwei Liu</li>
<li>for: 提出了一种新的近场速度测量概念，可以同时测量目标在运动过程中的径向和横向速度。</li>
<li>methods: 提出了基于最大可能性估计的方法，用于同时估计径向和横向速度的echo信号。</li>
<li>results: 通过帮助近场速度测量，提出了一种无需频率估计的预测扩散框架，实现了无间断的数据传输。数值示例验证了该方法的有效性。<details>
<summary>Abstract</summary>
The novel concept of near-field velocity sensing is proposed. In contrast to far-field velocity sensing, near-field velocity sensing enables the simultaneous estimation of both radial and transverse velocities of a moving target. A maximum-likelihood-based method is proposed for jointly estimating the radial and transverse velocities from the echo signals. Assisted by near-field velocity sensing, a predictive beamforming framework is proposed for a moving communication user, which requires no channel estimation but achieves seamless data transmission. Finally, numerical examples validate the proposed approaches.
</details>
<details>
<summary>摘要</summary>
新的概念——近场速度测量被提出。与远场速度测量相比，近场速度测量可同时测量移动目标的径向和横向速度。基于最大可能性的方法被提议用于同时估计径向和横向速度的echo信号。帮助了近场速度测量，一种预测扩散框架被提议用于移动通信用户，不需 Channel estimation，但可实现无缝数据传输。最后，数值示例证明了提出的方法。Here's the word-for-word translation:新的概念——近场速度测量被提出，与远场速度测量相比，近场速度测量可同时测量移动目标的径向和横向速度。基于最大可能性的方法被提议用于同时估计径向和横向速度的echo信号。帮助了近场速度测量，一种预测扩散框架被提议用于移动通信用户，不需 Channel estimation，但可实现无缝数据传输。最后，数值示例证明了提出的方法。
</details></li>
</ul>
<hr>
<h2 id="Wireless-rf-sensor-with-dual-sensing-capability-for-ionic-solution-and-target-dielectric-objects"><a href="#Wireless-rf-sensor-with-dual-sensing-capability-for-ionic-solution-and-target-dielectric-objects" class="headerlink" title="Wireless rf sensor with dual sensing capability for ionic solution and target dielectric objects"></a>Wireless rf sensor with dual sensing capability for ionic solution and target dielectric objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09876">http://arxiv.org/abs/2311.09876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sobhan Gholami, Emre Unal, Hilmi Volkan Demir</li>
<li>for: 用于检测水中离子含量和固体杂质物的变化</li>
<li>methods: 使用微型陷阱板设计，可以在透明容器外壁安装，并根据容器材料进行定制，以实现无线感知</li>
<li>results: 具有唯一的设计，使其不受周围环境影响<details>
<summary>Abstract</summary>
A novel microstrip-based sensor designed for detecting changes in ionic content of water and the addition of solid contaminant objects is presented. The sensor can be installed on the exterior wall of dielectric containers and customized according to the material of the container to enable wireless sensing. It's operation within the lower microwave frequency range (670 to 730 MHz) serves to minimize signal attenuation in water and streamlines circuitry design. The most significant feature of this sensor is its unique design, rendering it impervious to its surrounding environment.
</details>
<details>
<summary>摘要</summary>
一种新型微带式感测器，用于检测水中离子含量的变化以及固体杂质物的添加，被介绍。该感测器可以安装在dielectric容器外墙上，并可以根据容器材料进行个性化定制，以实现无线感测。它的运作频率范围为670-730MHz，以便在水中减少信号强度抑制，同时简化电路设计。该感测器的最重要特点是它独特的设计，使其不受周围环境影响。Here's the breakdown of the translation:* 一种新型微带式感测器 (a new type of microstrip-based sensor)* 用于检测水中离子含量的变化 (for detecting changes in ionic content of water)* 以及固体杂质物的添加 (and the addition of solid contaminant objects)* 被介绍 (being introduced)* 该感测器可以安装在dielectric容器外墙上 (the sensor can be installed on the exterior wall of dielectric containers)* 并可以根据容器材料进行个性化定制 (and can be customized according to the material of the container)* 以实现无线感测 (to achieve wireless sensing)* 它的运作频率范围为670-730MHz (its operating frequency range is 670-730MHz)* 以便在水中减少信号强度抑制 (to reduce signal attenuation in water)* 同时简化电路设计 (while simplifying circuitry design)* 该感测器的最重要特点是它独特的设计 (the most important feature of the sensor is its unique design)* 使其不受周围环境影响 (so that it is not affected by the surrounding environment)
</details></li>
</ul>
<hr>
<h2 id="Integrated-lithium-niobate-photonic-millimeter-wave-radar"><a href="#Integrated-lithium-niobate-photonic-millimeter-wave-radar" class="headerlink" title="Integrated lithium niobate photonic millimeter-wave radar"></a>Integrated lithium niobate photonic millimeter-wave radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09857">http://arxiv.org/abs/2311.09857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sha Zhu, Yiwen Zhang, Jiaxue Feng, Yongji Wang, Kunpeng Zhai, Hanke Feng, Edwin Yue Bun Pun, Ning Hua Zhu, Cheng Wang</li>
<li>for: This paper presents a centimeter-resolution integrated photonic radar operating in the mmWave V band (40-50 GHz) for high-resolution sensing and detection of targets.</li>
<li>methods: The paper uses a 4-inch wafer-scale thin-film lithium niobate (TFLN) technology to overcome the limitations of electronic radars and achieve a broadband linear frequency modulated mmWave radar waveform through optical frequency multiplication of a low-frequency input signal.</li>
<li>results: The paper achieves multi-target ranging with a resolution of 1.50 cm and velocity measurement with a resolution of 0.067 m&#x2F;s, as well as imaging of targets with various shapes and postures with a two-dimensional resolution of 1.50 cm * 1.06 cm.Here’s the Chinese version:</li>
<li>for: 这篇论文介绍了一种可以实现中心分辨率为1.50cm的集成光学雷达系统，用于高分辨率探测和检测目标。</li>
<li>methods: 这篇论文使用4英寸芯片级别的聚辉锆铌镧(TFLN)技术，以超越电子雷达的限制，实现广频线性频率变谱mm波雷达波形，通过光学频率 multiplication的方式实现低频输入信号的广频变谱。</li>
<li>results: 这篇论文实现了多个目标距离测量，分辨率为1.50cm，以及测速度测量，分辨率为0.067m&#x2F;s，同时还成功构建了反 synthetic aperture radar(ISAR)，并成功图像多种形状和姿态的目标，图像分辨率为1.50cm*1.06cm。<details>
<summary>Abstract</summary>
Millimeter-wave (mmWave,>30 GHz) radars are the key enabler in the coming 6G era for high-resolution sensing and detection of targets. Photonic radar provides an effective approach to overcome the limitations of electronic radars thanks to the high frequency, broad bandwidth, and excellent reconfigurability of photonic systems. However, conventional photonic radars are mostly realized in tabletop systems composed of bulky discrete components, whereas the more compact integrated photonic radars are difficult to reach the mmWave bands due to the unsatisfactory bandwidths and signal integrity of the underlining electro-optic modulators. Here, we overcome these challenges and demonstrate a centimeter-resolution integrated photonic radar operating in the mmWave V band (40-50 GHz) based on a 4-inch wafer-scale thin-film lithium niobate (TFLN) technology. The fabricated TFLN mmWave photonic integrated circuit consists of a first electro-optic modulator capable of generating a broadband linear frequency modulated mmWave radar waveform through optical frequency multiplication of a low-frequency input signal, and a second electro-optic modulator responsible for frequency de-chirp of the received reflected echo wave, therefore greatly relieving the bandwidth requirements for the analog-to-digital converter in the receiver. Thanks to the absence of optical and electrical filters in the system, our integrated photonic mmWave radar features continuous on-demand tunability of the center frequency and bandwidth, currently only limited by the bandwidths of electrical amplifiers. We achieve multi-target ranging with a resolution of 1.50 cm and velocity measurement with a resolution of 0.067 m/s. Furthermore, we construct an inverse synthetic aperture radar (ISAR) and successfully demonstrate the imaging of targets with various shapes and postures with a two-dimensional resolution of 1.50 cm * 1.06 cm.
</details>
<details>
<summary>摘要</summary>
millimeter wave (mmWave,>30 GHz) 雷达是 sixth generation (6G) 时代的关键能力，具有高分辨率探测和检测目标的能力。光子雷达技术提供了一种有效的方法来超越电子雷达的限制，因为光子系统具有高频率、广频带宽和优秀的可重新配置性。然而，传统的光子雷达通常是由多个粗糙的独立部件组成的桌面系统，而更 компакт的集成光子雷达具有 mmWave 频率带的差异和信号完整性问题。在这里，我们解决了这些挑战，并实现了基于 4 英寸芯片级薄膜锂铝铌 (TFLN) 技术的中心 Resolution 集成光子 mmWave 雷达，operating in the mmWave V band (40-50 GHz)。制造的 TFLN mmWave 光子集成电路包括一个能够生成广频线性修改 mmWave 雷达波形的第一个电 Optic modulator，以及一个负责接收反射回波的第二个电 Optic modulator。通过光子频率 multiplication 的低频输入信号，该系统实现了广频修改和频率去抖，从而大大减轻接收器 Analog-to-digital Converter 的频率要求。由于系统中缺乏光学和电子过滤器，我们的集成光子 mmWave 雷达具有无间断的受 demand 调试中心频率和带宽，当前只受电子增强器的带宽限制。我们实现了多个目标的距离测量，其中最高分辨率为 1.50 cm，以及速度测量的分辨率为 0.067 m/s。此外，我们还构建了一个反 Synthetic Aperture Radar (ISAR)，并成功地实现了目标的二维图像测量，其分辨率为 1.50 cm * 1.06 cm。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Relay-Aided-Text-Transmission-Placement-Optimization-and-Bandwidth-Allocation"><a href="#Semantic-Relay-Aided-Text-Transmission-Placement-Optimization-and-Bandwidth-Allocation" class="headerlink" title="Semantic-Relay-Aided Text Transmission: Placement Optimization and Bandwidth Allocation"></a>Semantic-Relay-Aided Text Transmission: Placement Optimization and Bandwidth Allocation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09850">http://arxiv.org/abs/2311.09850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Liu, Changsheng You, Zeyang Hu, Chenyu Wu, Yi Gong, Kaibin Huang</li>
<li>for: 提高移动设备中的文本传输效率（efficient text transmission in mobile devices）</li>
<li>methods: 使用semantic relay（SemRelay）和semantic transmitter（SemTransmitter），jointly designing SemRelay placement和bandwidth allocation</li>
<li>results: 提高文本传输效率（improved text transmission efficiency），比对 conventinal decode-and-forward relay（CF Relay）有较高的效果（better performance than conventional CF Relay）<details>
<summary>Abstract</summary>
Semantic communication has emerged as a promising technology to break the Shannon limit by extracting the meaning of source data and sending relevant semantic information only. However, some mobile devices may have limited computation and storage resources, which renders it difficult to deploy and implement the resource-demanding deep learning based semantic encoder/decoder. To tackle this challenge, we propose in this paper a new semantic relay (SemRelay), which is equipped with a semantic receiver for assisting text transmission from a resource-abundant base station (BS) to a resource-constrained mobile device. Specifically, the SemRelay first decodes the semantic information sent by the BS (with a semantic transmitter) and then forwards it to the user by adopting conventional bit transmission, hence effectively improving the text transmission efficiency. We formulate an optimization problem to maximize the achievable (effective) bit rate by jointly designing the SemRelay placement and bandwidth allocation. Although this problem is non-convex and generally difficult to solve, we propose an efficient penalty-based algorithm to obtain a high-quality suboptimal solution. Numerical results show the close-to-optimal performance of the proposed algorithm as well as significant rate performance gain of the proposed SemRelay over conventional decode-and-forward relay.
</details>
<details>
<summary>摘要</summary>
The SemRelay decodes the semantic information sent by the BS (with a semantic transmitter) and then forwards it to the user using conventional bit transmission, thereby improving text transmission efficiency. We formulate an optimization problem to maximize the achievable (effective) bit rate by jointly designing the SemRelay placement and bandwidth allocation. Although this problem is non-convex and difficult to solve, we propose an efficient penalty-based algorithm to obtain a high-quality suboptimal solution.Numerical results show that the proposed algorithm achieves close-to-optimal performance and offers significant rate performance gains compared to conventional decode-and-forward relay.
</details></li>
</ul>
<hr>
<h2 id="Stacked-Intelligent-Metasurface-Aided-MIMO-Transceiver-Design"><a href="#Stacked-Intelligent-Metasurface-Aided-MIMO-Transceiver-Design" class="headerlink" title="Stacked Intelligent Metasurface-Aided MIMO Transceiver Design"></a>Stacked Intelligent Metasurface-Aided MIMO Transceiver Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09814">http://arxiv.org/abs/2311.09814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancheng An, Chau Yuen, Chao Xu, Hongbin Li, Derrick Wing Kwan Ng, Marco Di Renzo, Mérouane Debbah, Lajos Hanzo</li>
<li>for: 这种论文旨在提出一种基于堆式智能表面（SIM）技术的下一代无线网络transceiver设计，以更有效地利用无线电频资源。</li>
<li>methods: 该论文提出使用堆式智能表面（SIM）技术，堆组织一系列可程序的表面层，每层包含大量的低成本PASSIVE元素，通过合理配置这些元素，实现复杂的计算和信号处理任务，如MIMO precoding&#x2F;combining、多用户干扰抑制和雷达探测。</li>
<li>results: 该论文提供了SIM-aided MIMO transceiver设计的概述，包括硬件体系结构和与现有解决方案的比较。此外，论文还详细介绍了应用场景和开放研究挑战，以及使用高级SIM体系结构实现下一代无线网络的可能性。最后，论文提供了数字结果，以证明在无线系统中使用波动信号处理的优势。<details>
<summary>Abstract</summary>
Next-generation wireless networks are expected to utilize the limited radio frequency (RF) resources more efficiently with the aid of intelligent transceivers. To this end, we propose a promising transceiver architecture relying on stacked intelligent metasurfaces (SIM). An SIM is constructed by stacking an array of programmable metasurface layers, where each layer consists of a massive number of low-cost passive meta-atoms that individually manipulate the electromagnetic (EM) waves. By appropriately configuring the passive meta-atoms, an SIM is capable of accomplishing advanced computation and signal processing tasks, such as multiple-input multiple-output (MIMO) precoding/combining, multi-user interference mitigation, and radar sensing, as the EM wave propagates through the multiple layers of the metasurface, which effectively reduces both the RF-related energy consumption and processing delay. Inspired by this, we provide an overview of the SIM-aided MIMO transceiver design, which encompasses its hardware architecture and its potential benefits over state-of-the-art solutions. Furthermore, we discuss promising application scenarios and identify the open research challenges associated with the design of advanced SIM architectures for next-generation wireless networks. Finally, numerical results are provided for quantifying the benefits of wave-based signal processing in wireless systems.
</details>
<details>
<summary>摘要</summary>
Inspired by this, we provide an overview of the SIM-aided MIMO transceiver design, including its hardware architecture and potential benefits over existing solutions. We also discuss promising application scenarios and identify open research challenges associated with the design of advanced SIM architectures for next-generation wireless networks. Finally, we provide numerical results to quantify the benefits of wave-based signal processing in wireless systems.Here is the translation in Simplified Chinese:下一代无线网络即将使用有限的广播频率资源更有效地使用，并且通过智能转发器来实现。为此，我们提出了一种有前途的转发器架构，即堆叠智能金属表盘（SIM）。每层SIM都由一大量的低成本Passive元件组成，这些元件个别对电磁波（EM）波进行处理。通过合适配置这些元件，SIM可以完成复杂的计算和信号处理任务，例如多输入多出力（MIMO）预处理/组合、多用户干扰抑制和雷达探测。这将有效减少广播相关的能量消耗和处理延迟。受这些想法启发，我们提供SIM帮助MIMO转发器设计的概述，包括硬件架构和其优势。我们还讨论了可能的应用场景，并识别了进一步开发SIM架构的研究挑战。最后，我们提供了量化无线系统中波形处理的数字结果。
</details></li>
</ul>
<hr>
<h2 id="MEGA-A-Memory-Efficient-GNN-Accelerator-Exploiting-Degree-Aware-Mixed-Precision-Quantization"><a href="#MEGA-A-Memory-Efficient-GNN-Accelerator-Exploiting-Degree-Aware-Mixed-Precision-Quantization" class="headerlink" title="MEGA: A Memory-Efficient GNN Accelerator Exploiting Degree-Aware Mixed-Precision Quantization"></a>MEGA: A Memory-Efficient GNN Accelerator Exploiting Degree-Aware Mixed-Precision Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09775">http://arxiv.org/abs/2311.09775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Zhu, Fanrong Li, Gang Li, Zejian Liu, Zitao Mo, Qinghao Hu, Xiaoyao Liang, Jian Cheng</li>
<li>for: 本研究的目的是提出一种高效的图 neural network（GNN）加速器，以解决GNN在非欧几何数据模型中的缓存访问所带来的延迟和能耗问题。</li>
<li>methods: 本研究提出了一种叫做 Memory-Efficient GNN Accelerator (MEGA)的加速器，通过算法和硬件合作设计。在算法层面，通过对节点属性进行深入分析，我们发现了一种名为度量感知的杂素精度归一化方法，可以减少GNN的压缩比例，保持精度。在硬件层面，我们采用了一种多元架构设计，将聚合和组合阶段分别实现为不同的数据流。</li>
<li>results: 我们实现了MEGA加速器在28nm技术节点上，并进行了广泛的实验。结果表明，MEGA可以在四种状态目前的GNN加速器上实现平均速度提升38.3倍，7.1倍，4.0倍，3.6倍，而且保持任务的精度。同时，MEGA也可以实现7.2倍，5.4倍，4.5倍的能耗减少。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are becoming a promising technique in various domains due to their excellent capabilities in modeling non-Euclidean data. Although a spectrum of accelerators has been proposed to accelerate the inference of GNNs, our analysis demonstrates that the latency and energy consumption induced by DRAM access still significantly impedes the improvement of performance and energy efficiency. To address this issue, we propose a Memory-Efficient GNN Accelerator (MEGA) through algorithm and hardware co-design in this work. Specifically, at the algorithm level, through an in-depth analysis of the node property, we observe that the data-independent quantization in previous works is not optimal in terms of accuracy and memory efficiency. This motivates us to propose the Degree-Aware mixed-precision quantization method, in which a proper bitwidth is learned and allocated to a node according to its in-degree to compress GNNs as much as possible while maintaining accuracy. At the hardware level, we employ a heterogeneous architecture design in which the aggregation and combination phases are implemented separately with different dataflows. In order to boost the performance and energy efficiency, we also present an Adaptive-Package format to alleviate the storage overhead caused by the fine-grained bitwidth and diverse sparsity, and a Condense-Edge scheduling method to enhance the data locality and further alleviate the access irregularity induced by the extremely sparse adjacency matrix in the graph. We implement our MEGA accelerator in a 28nm technology node. Extensive experiments demonstrate that MEGA can achieve an average speedup of 38.3x, 7.1x, 4.0x, 3.6x and 47.6x, 7.2x, 5.4x, 4.5x energy savings over four state-of-the-art GNN accelerators, HyGCN, GCNAX, GROW, and SGCN, respectively, while retaining task accuracy.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在不同领域变得抢手的技术，因为它们可以非常好地模型非欧几何数据。 Although a variety of accelerators have been proposed to accelerate the inference of GNNs, our analysis shows that the latency and energy consumption caused by DRAM access still significantly hinders the improvement of performance and energy efficiency. To address this issue, we propose a Memory-Efficient GNN Accelerator (MEGA) through algorithm and hardware co-design in this work. Specifically, at the algorithm level, through an in-depth analysis of the node property, we find that the data-independent quantization in previous works is not optimal in terms of accuracy and memory efficiency. This motivates us to propose the Degree-Aware mixed-precision quantization method, in which a proper bitwidth is learned and allocated to a node according to its in-degree to compress GNNs as much as possible while maintaining accuracy. At the hardware level, we employ a heterogeneous architecture design in which the aggregation and combination phases are implemented separately with different dataflows. In order to boost the performance and energy efficiency, we also present an Adaptive-Package format to alleviate the storage overhead caused by the fine-grained bitwidth and diverse sparsity, and a Condense-Edge scheduling method to enhance the data locality and further alleviate the access irregularity induced by the extremely sparse adjacency matrix in the graph. We implement our MEGA accelerator in a 28nm technology node. Extensive experiments show that MEGA can achieve an average speedup of 38.3x, 7.1x, 4.0x, 3.6x and 47.6x, 7.2x, 5.4x, 4.5x energy savings over four state-of-the-art GNN accelerators, HyGCN, GCNAX, GROW, and SGCN, respectively, while retaining task accuracy.
</details></li>
</ul>
<hr>
<h2 id="OFDM-based-Waveforms-for-Joint-Sensing-and-Communications-Robust-to-Frequency-Selective-IQ-Imbalance"><a href="#OFDM-based-Waveforms-for-Joint-Sensing-and-Communications-Robust-to-Frequency-Selective-IQ-Imbalance" class="headerlink" title="OFDM-based Waveforms for Joint Sensing and Communications Robust to Frequency Selective IQ Imbalance"></a>OFDM-based Waveforms for Joint Sensing and Communications Robust to Frequency Selective IQ Imbalance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09746">http://arxiv.org/abs/2311.09746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Lang, Christian Hofbauer, Moritz Tockner, Reinhard Feger, Thomas Wagner, Mario Huemer</li>
<li>for: 这个研究旨在提出一种对偏射和通讯系统具有潜力的OFDM波形，并解决OFDM波形对偏射和 quadrature-phase（IQ）不对称的问题，以减少噪声底。</li>
<li>methods: 这个研究使用了一种新的OFDM波形，它 neither increases the noise floor nor reduces the maximum unambiguous range，并且提出了一种适应频率选择的通信系统，包括通道估计、同步和数据估计方法，它们是 Specifically designed to deal with frequency selective IQ imbalance in wideband systems。</li>
<li>results: 这个研究通过 simulations 示出了这些通信系统的有效性，并且显示了这些系统在噪声底和最大不ambiguous 距离方面的改善。<details>
<summary>Abstract</summary>
Orthogonal frequency-division multiplexing (OFDM) is a promising waveform candidate for future joint sensing and communication systems. It is well known that the OFDM waveform is vulnerable to in-phase and quadrature-phase (IQ) imbalance, which increases the noise floor in a range-Doppler map (RDM). A state-of-the-art method for robustifying the OFDM waveform against IQ imbalance avoids an increased noise floor, but it generates additional ghost objects in the RDM [1]. A consequence of these additional ghost objects is a reduction of the maximum unambiguous range. In this work, a novel OFDM-based waveform robust to IQ imbalance is proposed, which neither increases the noise floor nor reduces the maximum unambiguous range. The latter is achieved by shifting the ghost objects in the RDM to different velocities such that their range variations observed over several consecutive RDMs do not correspond to the observed velocity. This allows tracking algorithms to identify them as ghost objects and eliminate them for the follow-up processing steps. Moreover, we propose complete communication systems for both the proposed waveform as well as for the state-of-the-art waveform, including methods for channel estimation, synchronization, and data estimation that are specifically designed to deal with frequency selective IQ imbalance which occurs in wideband systems. The effectiveness of these communication systems is demonstrated by means of bit error ratio (BER) simulations.
</details>
<details>
<summary>摘要</summary>
隐式frequency-division multiplexing（OFDM）是未来 JOINT sensing和通信系统的优秀waveform候选人。OFDM波形容于均匀和 quadrature-phase（IQ）不匹配，从而增加range-Doppler map（RDM）中的噪声底。现有的state-of-the-art方法可以避免增加噪声底，但会生成RDM中的幻象物体。这些幻象物体会 reducion maximum unambiguous range。在这种工作中，我们提出了一种robust OFDM波形， neither increases the noise floor nor reduces the maximum unambiguous range。这是通过在RDM中移动幻象物体的速度，使其在不同的速度下变化，以至于在多个连续RDM中不同速度下的范围变化不同于观测到的速度。这使得跟踪算法可以将其标记为幻象物体，并在后续处理步骤中消除它们。此外，我们还提出了为两种waveform（包括提案的波形和现有waveform）的完整通信系统，包括频率选择性IQ不匹配的通道估计、同步和数据估计方法，这些方法特别针对随着宽频带的IQ不匹配。我们通过BER simulations示出这些通信系统的有效性。
</details></li>
</ul>
<hr>
<h2 id="Reconciling-Radio-Tomographic-Imaging-with-Phaseless-Inverse-Scattering"><a href="#Reconciling-Radio-Tomographic-Imaging-with-Phaseless-Inverse-Scattering" class="headerlink" title="Reconciling Radio Tomographic Imaging with Phaseless Inverse Scattering"></a>Reconciling Radio Tomographic Imaging with Phaseless Inverse Scattering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09633">http://arxiv.org/abs/2311.09633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amartansh Dubey, Zan Li, Ross Murch</li>
<li>For: This paper aims to improve the accuracy of Radio Tomographic Imaging (RTI) by reconciling it with formal inverse scattering approaches and enhancing its performance using inverse scattering techniques.* Methods: The paper uses empirical RTI models and formal inverse scattering approaches to compare and enhance RTI’s performance.* Results: The enhanced RTI method outperforms traditional RTI while having similar computational complexity, as demonstrated through numerical and experimental results using low-cost 2.4 GHz Wi-Fi transceivers for indoor imaging applications.Here is the same information in Simplified Chinese:* For: 这篇论文的目的是提高Radio Tomographic Imaging（RTI）的准确率，通过与正式反射扩散方法进行比较和改进RTI的性能。* Methods: 这篇论文使用了empirical RTI模型和正式反射扩散方法来比较和改进RTI的性能。* Results: 改进后的RTI方法可以超越传统的RTI，同时保持与RTI相同的计算复杂性，通过使用低成本的2.4 GHz Wi-Fi传输器进行indoor应用。<details>
<summary>Abstract</summary>
Radio Tomographic Imaging (RTI) is a phaseless imaging approach that can provide shape reconstruction and localization of objects using received signal strength (RSS) measurements. RSS measurements can be straightforwardly obtained from wireless networks such as Wi-Fi and therefore RTI has been extensively researched and accepted as a good indoor RF imaging technique. However, RTI is formulated on empirical models using an assumption of light-of-sight (LOS) propagation that does not account for intricate scattering effects. There are two main objectives of this work. The first objective is to reconcile and compare the empirical RTI model with formal inverse scattering approaches to better understand why RTI is an effective RF imaging technique. The second objective is to obtain straightforward enhancements to RTI, based on inverse scattering, to enhance its performance. The resulting enhancements can provide reconstructions of the shape and also material properties of the objects that can aid image classification. We also provide numerical and experimental results to compare RTI with the enhanced RTI for indoor imaging applications using low-cost 2.4 GHz Wi-Fi transceivers. These results show that the enhanced RTI can outperform RTI while having similar computational complexity to RTI.
</details>
<details>
<summary>摘要</summary>
Radio Tomographic Imaging (RTI) 是一种无相位成像方法，可以提供物体形态重建和位置确定使用接收信号强度 (RSS) 测量。 RSS 测量可以直接从无线网络 such as Wi-Fi 获得，因此 RTI 在indoor RF 成像技术中得到了广泛的研究和认可。然而， RTI 基于employmodels 的 assumption of line-of-sight (LOS) 媒体传播，不能考虑复杂的散射效应。这个工作的两个主要目标是：1. 与形式 inverse scattering 方法进行对比和结合 RTI 模型，以更好地理解 RTI 是一种有效的 RF 成像技术。2. 基于 inverse scattering 方法，对 RTI 进行改进，以提高其性能。改进后的 RTI 可以提供更好的形态重建和物体属性的重建，这可以帮助图像分类。我们还提供了数字和实验结果，以比较 RTI 与改进后的 RTI 在indoor 成像应用中的性能。结果表明，改进后的 RTI 可以超过 RTI，而且与 RTI 的计算复杂度相似。
</details></li>
</ul>
<hr>
<h2 id="Plug-In-RIS-A-Novel-Approach-to-Fully-Passive-Reconfigurable-Intelligent-Surfaces"><a href="#Plug-In-RIS-A-Novel-Approach-to-Fully-Passive-Reconfigurable-Intelligent-Surfaces" class="headerlink" title="Plug-In RIS: A Novel Approach to Fully Passive Reconfigurable Intelligent Surfaces"></a>Plug-In RIS: A Novel Approach to Fully Passive Reconfigurable Intelligent Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09626">http://arxiv.org/abs/2311.09626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Raeisi, Ibrahim Yildirim, Mehmet Cagri Ilter, Majid Gerami, Ertugrul Basar</li>
<li>for: 提高 millimeter wave 通信系统中阻挡区域的性能</li>
<li>methods: 使用固定磁场技术和先进的杂排列技术来实现位置相关的磁场规划</li>
<li>results: 在有限CSI情况下， plug-in RIS 可以提供高效的解决方案，并与传统全CSI-启用 RIS 解决方案 exhibit striking convergence in average bit error rate and achievable rate performance.<details>
<summary>Abstract</summary>
This paper presents a promising design concept for reconfigurable intelligent surfaces (RISs), named plug-in RIS, wherein the RIS is plugged into an appropriate position in the environment, adjusted once according to the location of both base station and blocked region, and operates with fixed beams to enhance the system performance. The plug-in RIS is a novel system design, streamlining RIS-assisted millimeter-wave (mmWave) communication without requiring decoupling two parts of the end-to-end channel, traditional control signal transmission, and online RIS configuration. In plug-in RIS-aided transmission, the transmitter efficiently activates specific regions of the divided large RIS by employing hybrid beamforming techniques, each with predetermined phase adjustments tailored to reflect signals to desired user locations. This user-centric approach enhances connectivity and overall user experience by dynamically illuminating the targeted user based on location. By introducing plug-in RIS's theoretical framework, design principles, and performance evaluation, we demonstrate its potential to revolutionize mmWave communications in limited channel state information (CSI) scenarios. Simulation results illustrate that plug-in RIS provides power/cost-efficient solutions to overcome blockage in the mmWave communication system and a striking convergence in average bit error rate and achievable rate performance with traditional full CSI-enabled RIS solutions.
</details>
<details>
<summary>摘要</summary>
The novel design of the plug-in RIS eliminates the need for decoupling the two parts of the end-to-end channel, traditional control signal transmission, and online RIS configuration, streamlining the RIS-assisted mmWave communication. In the plug-in RIS-aided transmission, the transmitter efficiently activates specific regions of the divided large RIS using hybrid beamforming techniques, each with predetermined phase adjustments tailored to reflect signals to desired user locations. This user-centric approach enhances connectivity and overall user experience by dynamically illuminating the targeted user based on location.The theoretical framework, design principles, and performance evaluation of the plug-in RIS are presented in this paper, demonstrating its potential to revolutionize mmWave communications in limited channel state information (CSI) scenarios. Simulation results show that the plug-in RIS provides power/cost-efficient solutions to overcome blockage in the mmWave communication system and achieves a striking convergence in average bit error rate and achievable rate performance with traditional full CSI-enabled RIS solutions.
</details></li>
</ul>
<hr>
<h2 id="Joint-Visibility-Region-and-Channel-Estimation-for-Extremely-Large-scale-MIMO-Systems"><a href="#Joint-Visibility-Region-and-Channel-Estimation-for-Extremely-Large-scale-MIMO-Systems" class="headerlink" title="Joint Visibility Region and Channel Estimation for Extremely Large-scale MIMO Systems"></a>Joint Visibility Region and Channel Estimation for Extremely Large-scale MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09490">http://arxiv.org/abs/2311.09490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anzheng Tang, Jun-bo Wang, Yijin Pan, Wence Zhang, Yijian Chen, Hongkang Yu, Rodrigo C. de Lamare</li>
<li>for: 本研究 investigate the channel estimation (CE) problem for extremely large-scale multiple-input-multiple-output (XL-MIMO) systems, considering both the spherical wavefront effect and spatial non-stationarity (SnS).</li>
<li>methods: 我们提出了一种 two-stage visibility region (VR) detection and CE framework, which leverages sparsity in both the spatial and wavenumber domains to achieve an accurate estimation. In the first stage, we use a structured message passing (MP) scheme to obtain the belief regarding the visibility of antennas. In the second stage, we use the obtained VR information and wavenumber-domain sparsity to accurately estimate the SnS channel employing the belief-based orthogonal matching pursuit (BB-OMP) method.</li>
<li>results:  simulations demonstrate that the proposed algorithms lead to a significant enhancement in VR detection and CE accuracy, especially in low signal-to-noise ratio (SNR) scenarios.<details>
<summary>Abstract</summary>
In this work, we investigate the channel estimation (CE) problem for extremely large-scale multiple-input-multiple-output (XL-MIMO) systems, considering both the spherical wavefront effect and spatial non-stationarity (SnS). Unlike existing non-stationary CE methods that rely on the statistical characteristics of channels in the spatial or temporal domain, our approach seeks to leverage sparsity in both the spatial and wavenumber domains simultaneously to achieve an accurate estimation.To this end, we introduce a two-stage visibility region (VR) detection and CE framework. Specifically, in the first stage, the belief regarding the visibility of antennas is obtained through a structured message passing (MP) scheme, which fully exploits the block sparse structure of the antenna-domain channel. In the second stage, using the obtained VR information and wavenumber-domain sparsity, we accurately estimate the SnS channel employing the belief-based orthogonal matching pursuit (BB-OMP) method. Simulations demonstrate that the proposed algorithms lead to a significant enhancement in VR detection and CE accuracy, especially in low signal-to-noise ratio (SNR) scenarios.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究了超大规模多输入多输出（XL-MIMO）系统中的通道估计（CE）问题，考虑了球面冲击效应和空间非站点性（SnS）。 unlike existing non-stationary CE方法，我们的方法不仅利用了通道在空间或时域频率域的统计特性，而且同时充分利用了antenna-domain通道的块稀畴结构。为达到这个目的，我们提出了两个阶段的可见区域（VR）探测和CE框架。在第一阶段，通过一种结构化的消息传递（MP）方案，我们可以获得天线域通道的可见性信念。在第二阶段，使用获得的VR信息和波数域稀畴性，我们可以高精度地估计SnS通道，使用信念基本的搜索匹配策略（BB-OMP）。 simulation结果表明，我们的算法可以在低信号噪响比（SNR）场景下提高VR检测和CE准确率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/eess.SP_2023_11_16/" data-id="clpahu7gm01i53h88ec6o5nf0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.SD_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T15:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.SD_2023_11_15/">cs.SD - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AI-based-soundscape-analysis-Jointly-identifying-sound-sources-and-predicting-annoyance"><a href="#AI-based-soundscape-analysis-Jointly-identifying-sound-sources-and-predicting-annoyance" class="headerlink" title="AI-based soundscape analysis: Jointly identifying sound sources and predicting annoyance"></a>AI-based soundscape analysis: Jointly identifying sound sources and predicting annoyance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09030">http://arxiv.org/abs/2311.09030</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/ai-soundscape">https://github.com/yuanbo2020/ai-soundscape</a></li>
<li>paper_authors: Yuanbo Hou, Qiaoqiao Ren, Huizhong Zhang, Andrew Mitchell, Francesco Aletta, Jian Kang, Dick Botteldooren</li>
<li>for: This paper proposes an AI-based approach for automatic soundscape characterization, including sound recognition and appraisal.</li>
<li>methods: The proposed method uses a dual-branch convolutional neural network with cross-attention-based fusion (DCNN-CaF) to analyze sound sources and predict human-perceived annoyance.</li>
<li>results: The proposed method outperforms other typical AI-based models and soundscape-related traditional machine learning methods on the sound source classification and annoyance rating prediction tasks, and shows consistent results with human perception.<details>
<summary>Abstract</summary>
Soundscape studies typically attempt to capture the perception and understanding of sonic environments by surveying users. However, for long-term monitoring or assessing interventions, sound-signal-based approaches are required. To this end, most previous research focused on psycho-acoustic quantities or automatic sound recognition. Few attempts were made to include appraisal (e.g., in circumplex frameworks). This paper proposes an artificial intelligence (AI)-based dual-branch convolutional neural network with cross-attention-based fusion (DCNN-CaF) to analyze automatic soundscape characterization, including sound recognition and appraisal. Using the DeLTA dataset containing human-annotated sound source labels and perceived annoyance, the DCNN-CaF is proposed to perform sound source classification (SSC) and human-perceived annoyance rating prediction (ARP). Experimental findings indicate that (1) the proposed DCNN-CaF using loudness and Mel features outperforms the DCNN-CaF using only one of them. (2) The proposed DCNN-CaF with cross-attention fusion outperforms other typical AI-based models and soundscape-related traditional machine learning methods on the SSC and ARP tasks. (3) Correlation analysis reveals that the relationship between sound sources and annoyance is similar for humans and the proposed AI-based DCNN-CaF model. (4) Generalization tests show that the proposed model's ARP in the presence of model-unknown sound sources is consistent with expert expectations and can explain previous findings from the literature on sound-scape augmentation.
</details>
<details>
<summary>摘要</summary>
听音环境研究通常会尝试捕捉用户对听音环境的认知和理解，但是对于长期监测或评估 intervención，需要基于听音信号的方法。因此，前期研究主要集中在 psycho-acoustic 量或自动听音识别方面。只有一些尝试包括评价（如在 circumplex 框架中）。这篇文章提出了基于人工智能（AI）的双支树层卷积神经网络（DCNN-CaF），用于自动听音特征化，包括听音识别和评价。使用包含人类标注的听音源标签和感知困扰的DeLTA数据集，DCNN-CaF被提议用于听音源类别（SSC）和人类感知困扰评分预测（ARP）。实验结果表明：1. 使用听音强度和Mel特征的DCNN-CaF，与只使用一个特征的DCNN-CaF相比，表现出更好的性能。2. DCNN-CaF与十字关注融合的方法，在SSC和ARP任务上表现出比其他常见的AI模型和听音相关传统机器学习方法更好的性能。3. 相关分析表明，人类和提议的AI模型之间听音源和困扰之间的关系类似。4. 总结测试表明，提议的模型的ARP在模型不知道听音源时的存在下保持一致性，与专家预期一致，并可以解释过去 literatura 中的听音环境增强现象。
</details></li>
</ul>
<hr>
<h2 id="CREPE-Notes-A-new-method-for-segmenting-pitch-contours-into-discrete-notes"><a href="#CREPE-Notes-A-new-method-for-segmenting-pitch-contours-into-discrete-notes" class="headerlink" title="CREPE Notes: A new method for segmenting pitch contours into discrete notes"></a>CREPE Notes: A new method for segmenting pitch contours into discrete notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08884">http://arxiv.org/abs/2311.08884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xavier Riley, Simon Dixon</li>
<li>for: 本研究旨在提出一种简单有效的音频识别方法，以提高音乐分析和符号音乐生成等应用的可行性。</li>
<li>methods: 本方法基于CREPE，一种现有的单声道抖音跟踪解决方案，使用简单的神经网络进行后处理，以实现单声道音频分 segmentation。</li>
<li>results: 本方法在两个具有挑战性的单声道乐器音乐数据集上达到了状态级 Ergebnisse，同时与其他深度学习基于方法相比，减少了97%的总参数数量。<details>
<summary>Abstract</summary>
Tracking the fundamental frequency (f0) of a monophonic instrumental performance is effectively a solved problem with several solutions achieving 99% accuracy. However, the related task of automatic music transcription requires a further processing step to segment an f0 contour into discrete notes. This sub-task of note segmentation is necessary to enable a range of applications including musicological analysis and symbolic music generation. Building on CREPE, a state-of-the-art monophonic pitch tracking solution based on a simple neural network, we propose a simple and effective method for post-processing CREPE's output to achieve monophonic note segmentation. The proposed method demonstrates state-of-the-art results on two challenging datasets of monophonic instrumental music. Our approach also gives a 97% reduction in the total number of parameters used when compared with other deep learning based methods.
</details>
<details>
<summary>摘要</summary>
Tracking the fundamental frequency (f0) of a monophonic instrumental performance is effectively a solved problem with several solutions achieving 99% accuracy. However, the related task of automatic music transcription requires a further processing step to segment an f0 contour into discrete notes. This sub-task of note segmentation is necessary to enable a range of applications including musicological analysis and symbolic music generation. Building on CREPE, a state-of-the-art monophonic pitch tracking solution based on a simple neural network, we propose a simple and effective method for post-processing CREPE's output to achieve monophonic note segmentation. The proposed method demonstrates state-of-the-art results on two challenging datasets of monophonic instrumental music. Our approach also gives a 97% reduction in the total number of parameters used when compared with other deep learning based methods.Here's the breakdown of the text in Simplified Chinese:Tracking the fundamental frequency (f0) of a monophonic instrumental performance is effectively a solved problem with several solutions achieving 99% accuracy. however, the related task of automatic music transcription requires a further processing step to segment an f0 contour into discrete notes.This sub-task of note segmentation is necessary to enable a range of applications including musicological analysis and symbolic music generation.Building on CREPE, a state-of-the-art monophonic pitch tracking solution based on a simple neural network, we propose a simple and effective method for post-processing CREPE's output to achieve monophonic note segmentation.The proposed method demonstrates state-of-the-art results on two challenging datasets of monophonic instrumental music.Our approach also gives a 97% reduction in the total number of parameters used when compared with other deep learning based methods.
</details></li>
</ul>
<hr>
<h2 id="Multi-objective-Non-intrusive-Hearing-aid-Speech-Assessment-Model"><a href="#Multi-objective-Non-intrusive-Hearing-aid-Speech-Assessment-Model" class="headerlink" title="Multi-objective Non-intrusive Hearing-aid Speech Assessment Model"></a>Multi-objective Non-intrusive Hearing-aid Speech Assessment Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08878">http://arxiv.org/abs/2311.08878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsin-Tien Chiang, Szu-Wei Fu, Hsin-Min Wang, Yu Tsao, John H. L. Hansen</li>
<li>for: 这种研究旨在提出一种基于原始波形和听力损伤模式的非侵入式听力评估模型，以提高听力评估的准确性和可重复性。</li>
<li>methods: 该模型使用了预训练的SSL模型，并对输入音频信号进行了特征EXTRACT和维度压缩。此外，研究还 compare了三种精度调整方法，并证明了在OOD数据集上进行了更好的转移性。</li>
<li>results: 研究表明，使用预训练的SSL模型可以在听力评估中得到显著提高的表达质量和整体表达能力，并且在不同的听力损伤水平下具有更好的转移性。<details>
<summary>Abstract</summary>
Without the need for a clean reference, non-intrusive speech assessment methods have caught great attention for objective evaluations. While deep learning models have been used to develop non-intrusive speech assessment methods with promising results, there is limited research on hearing-impaired subjects. This study proposes a multi-objective non-intrusive hearing-aid speech assessment model, called HASA-Net Large, which predicts speech quality and intelligibility scores based on input speech signals and specified hearing-loss patterns. Our experiments showed the utilization of pre-trained SSL models leads to a significant boost in speech quality and intelligibility predictions compared to using spectrograms as input. Additionally, we examined three distinct fine-tuning approaches that resulted in further performance improvements. Furthermore, we demonstrated that incorporating SSL models resulted in greater transferability to OOD dataset. Finally, this study introduces HASA-Net Large, which is a non-invasive approach for evaluating speech quality and intelligibility. HASA-Net Large utilizes raw waveforms and hearing-loss patterns to accurately predict speech quality and intelligibility levels for individuals with normal and impaired hearing and demonstrates superior prediction performance and transferability.
</details>
<details>
<summary>摘要</summary>
无需清晰参考，非侵入式Speech评估方法在 objective 评估中受到了广泛关注。而深度学习模型已经被用来开发非侵入式Speech评估方法，但有限的研究在听力障碍者中。这项研究提出了一种多目标非侵入式听力器Speech评估模型，称为HASA-Net Large，该模型根据输入Speech信号和指定的听力损耗模式预测Speech质量和 inteligibilty 分数。我们的实验表明使用预训练的 SSL 模型会导致Speech质量和 inteligibilty 预测得到显著提升，比使用spectrograms作为输入更好。此外，我们还考虑了三种不同的细化方法，这些方法导致了更好的性能提升。此外，我们还证明了将 SSL 模型integrated 到 OOD 数据集中的更好传播性。最后，本研究介绍了HASA-Net Large，这是一种非侵入式的Speech质量和 inteligibilty 评估方法，该方法使用原始波形和听力损耗模式来准确预测听力正常和听力障碍者的Speech质量和 inteligibilty 水平，并达到了更高的预测性和传播性。
</details></li>
</ul>
<hr>
<h2 id="Autoencoder-with-Group-based-Decoder-and-Multi-task-Optimization-for-Anomalous-Sound-Detection"><a href="#Autoencoder-with-Group-based-Decoder-and-Multi-task-Optimization-for-Anomalous-Sound-Detection" class="headerlink" title="Autoencoder with Group-based Decoder and Multi-task Optimization for Anomalous Sound Detection"></a>Autoencoder with Group-based Decoder and Multi-task Optimization for Anomalous Sound Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08829">http://arxiv.org/abs/2311.08829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhou, Dongxing Xu, Haoran Wei, Yanhua Long</li>
<li>for: 这篇论文主要是为了提出一个基于自适应神经网络的机器发生音预警方法，以解决实际应用中机器发生音检测中的短路、杂音识别和特征品质不足问题。</li>
<li>methods: 这篇论文提出了一个基于自适应神经网络的架构，包括在自适应神经网络中插入辅助分类器，以增强预警性能。此外，论文还提出了一种群集结构和适应损失函数，以具备专业知识。</li>
<li>results: 根据DCASE 2021 Task 2的开发集合，这篇论文的方法在七台机器上的测试集合中，相对于官方AE和MobileNetV2的平均投票值提高13.11%和15.20%。<details>
<summary>Abstract</summary>
In industry, machine anomalous sound detection (ASD) is in great demand. However, collecting enough abnormal samples is difficult due to the high cost, which boosts the rapid development of unsupervised ASD algorithms. Autoencoder (AE) based methods have been widely used for unsupervised ASD, but suffer from problems including 'shortcut', poor anti-noise ability and sub-optimal quality of features. To address these challenges, we propose a new AE-based framework termed AEGM. Specifically, we first insert an auxiliary classifier into AE to enhance ASD in a multi-task learning manner. Then, we design a group-based decoder structure, accompanied by an adaptive loss function, to endow the model with domain-specific knowledge. Results on the DCASE 2021 Task 2 development set show that our methods achieve a relative improvement of 13.11% and 15.20% respectively in average AUC over the official AE and MobileNetV2 across test sets of seven machines.
</details>
<details>
<summary>摘要</summary>
在工业领域，机器异常声音检测（ASD）的需求非常大。然而，收集足够的异常样本是困难的，这会促进不upervised ASD算法的快速发展。基于自适应Encoder（AE）的方法在不upervised ASD方面广泛应用，但它们受到短 Circuit、anti-noise能力不够和特征质量不佳等问题的困扰。为了解决这些挑战，我们提出了一个新的AE基于框架，称为AEGM。 Specifically，我们首先将auxiliary分类器 inserting into AE以增强ASD的多任务学习方式。然后，我们设计了一种群体基本解码结构，并附加了一个适应损失函数，以使模型具备域pecific的知识。Results on DCASE 2021 Task 2 development set show that our methods achieve a relative improvement of 13.11% and 15.20% respectively in average AUC over the official AE and MobileNetV2 across test sets of seven machines.
</details></li>
</ul>
<hr>
<h2 id="CLN-VC-Text-Free-Voice-Conversion-Based-on-Fine-Grained-Style-Control-and-Contrastive-Learning-with-Negative-Samples-Augmentation"><a href="#CLN-VC-Text-Free-Voice-Conversion-Based-on-Fine-Grained-Style-Control-and-Contrastive-Learning-with-Negative-Samples-Augmentation" class="headerlink" title="CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation"></a>CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08670">http://arxiv.org/abs/2311.08670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimin Deng, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao</li>
<li>for: 提高voice转换质量的关键是更好地分离speech表示。</li>
<li>methods: 我们提议使用增强的负样本选择和参考编码器来改善speaker编码器的学习能力。</li>
<li>results: 我们的方法比前期工作在voice转换任务中表现更好。<details>
<summary>Abstract</summary>
Better disentanglement of speech representation is essential to improve the quality of voice conversion. Recently contrastive learning is applied to voice conversion successfully based on speaker labels. However, the performance of model will reduce in conversion between similar speakers. Hence, we propose an augmented negative sample selection to address the issue. Specifically, we create hard negative samples based on the proposed speaker fusion module to improve learning ability of speaker encoder. Furthermore, considering the fine-grain modeling of speaker style, we employ a reference encoder to extract fine-grained style and conduct the augmented contrastive learning on global style. The experimental results show that the proposed method outperforms previous work in voice conversion tasks.
</details>
<details>
<summary>摘要</summary>
更好的发音 Representation 分离是voice转换质量的关键。最近，基于说话人标签的对比学习成功地应用于voice转换。然而，在相似的说话人之间转换时，模型性能会降低。因此，我们提出一种增强的负样本选择方法来解决这个问题。具体来说，我们基于提议的说话人融合模块创建困难的负样本，以提高说话人Encoder的学习能力。此外，为了考虑细腻的说话人风格模型，我们采用参考Encoder来提取细腻的风格特征，并在全局风格上进行增强对比学习。实验结果表明，我们的方法在voice转换任务上的表现比前一个工作更好。
</details></li>
</ul>
<hr>
<h2 id="EDMSound-Spectrogram-Based-Diffusion-Models-for-Efficient-and-High-Quality-Audio-Synthesis"><a href="#EDMSound-Spectrogram-Based-Diffusion-Models-for-Efficient-and-High-Quality-Audio-Synthesis" class="headerlink" title="EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis"></a>EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08667">http://arxiv.org/abs/2311.08667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ge Zhu, Yutong Wen, Marc-André Carbonneau, Zhiyao Duan</li>
<li>for: 本文提出了一种基于扩散模型的生成模型，用于生成各种听起来的声音。</li>
<li>methods: 该模型在spectrogram域内使用扩散过程来生成声音，并结合了高效的抽象推断器。</li>
<li>results: 研究人员通过对DCASE2023 foley音频生成 benchmark进行测试，发现该模型在10步内可以达到类似于顶尖基eline的Fréchet音频距离（FAD）分数，并在50步内达到了状态的掌握水平。此外，研究人员还发现了扩散基于音频生成模型的一种潜在问题，即它们可能会生成与训练数据高度相似的样本。<details>
<summary>Abstract</summary>
Audio diffusion models can synthesize a wide variety of sounds. Existing models often operate on the latent domain with cascaded phase recovery modules to reconstruct waveform. This poses challenges when generating high-fidelity audio. In this paper, we propose EDMSound, a diffusion-based generative model in spectrogram domain under the framework of elucidated diffusion models (EDM). Combining with efficient deterministic sampler, we achieved similar Fr\'echet audio distance (FAD) score as top-ranked baseline with only 10 steps and reached state-of-the-art performance with 50 steps on the DCASE2023 foley sound generation benchmark. We also revealed a potential concern regarding diffusion based audio generation models that they tend to generate samples with high perceptual similarity to the data from training data. Project page: https://agentcooper2002.github.io/EDMSound/
</details>
<details>
<summary>摘要</summary>
Audio 扩散模型可以生成各种听起来。现有模型通常在幂征频域中使用缓冲phaserecovery模块来重construct波形。这会对高精度音频生成带来挑战。在这篇论文中，我们提出了EDMSound，一种基于扩散模型的生成模型，在幂征频域下的框架之下。通过有效的deterministic采样器，我们在10步内达到了与顶峰基eline的相似性 Fréchet 音频距离（FAD）分数，并在50步内达到了状态之 искусственный智能术语（DCASE2023） foley 声音生成 benchmark 的顶峰性能。我们还发现了扩散基于音频生成模型的一个潜在问题，即它们往往会生成与训练数据具有高听觉相似性的样本。项目页面：https://agentcooper2002.github.io/EDMSound/
</details></li>
</ul>
<hr>
<h2 id="Multi-channel-Conversational-Speaker-Separation-via-Neural-Diarization"><a href="#Multi-channel-Conversational-Speaker-Separation-via-Neural-Diarization" class="headerlink" title="Multi-channel Conversational Speaker Separation via Neural Diarization"></a>Multi-channel Conversational Speaker Separation via Neural Diarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08630">http://arxiv.org/abs/2311.08630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassan Taherian, DeLiang Wang</li>
<li>for: 提高会议环境中自动语音识别（ASR）系统的性能，解决单话者语音设计的问题。</li>
<li>methods: 提出了一种基于神经网络分类的 speaker separation via neural diarization（SSND）框架，利用终端到终端的分类系统来标识每个个体的语音活动。</li>
<li>results: 在 LibriCSS  dataset 上进行了评估，并取得了大幅提高的 диари化和 ASR Result，代表了state-of-the-art 水平。<details>
<summary>Abstract</summary>
When dealing with overlapped speech, the performance of automatic speech recognition (ASR) systems substantially degrades as they are designed for single-talker speech. To enhance ASR performance in conversational or meeting environments, continuous speaker separation (CSS) is commonly employed. However, CSS requires a short separation window to avoid many speakers inside the window and sequential grouping of discontinuous speech segments. To address these limitations, we introduce a new multi-channel framework called "speaker separation via neural diarization" (SSND) for meeting environments. Our approach utilizes an end-to-end diarization system to identify the speech activity of each individual speaker. By leveraging estimated speaker boundaries, we generate a sequence of embeddings, which in turn facilitate the assignment of speakers to the outputs of a multi-talker separation model. SSND addresses the permutation ambiguity issue of talker-independent speaker separation during the diarization phase through location-based training, rather than during the separation process. This unique approach allows multiple non-overlapped speakers to be assigned to the same output stream, making it possible to efficiently process long segments-a task impossible with CSS. Additionally, SSND is naturally suitable for speaker-attributed ASR. We evaluate our proposed diarization and separation methods on the open LibriCSS dataset, advancing state-of-the-art diarization and ASR results by a large margin.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在叠加的语音中，自动语音识别（ASR）系统的性能会受到很大的影响，因为它们是单个说话者的设计。为了在会议环境中提高 ASR 性能，常用 continuous speaker separation（CSS）。然而，CSS 需要一个短 separation window，以避免在 window 内有多个说话者，并且sequential grouping of discontinuous speech segments。为了解决这些限制，我们提出了一个新的多通道框架，即 "speaker separation via neural diarization"（SSND）。我们的方法使用了一个端到端的 diarization 系统，以确定每个个体说话者的语音活动。通过利用估计的 speaker 边界，我们生成了一个序列 embedding，以便将说话者分配到 multi-talker separation 模型的输出流中。SSND 通过在 diarization 阶段使用位置基于的训练，而不是在 separation 阶段，解决了 talker-independent speaker separation 的 permutation ambiguity 问题。这种独特的方法使得可以有效地处理长段，而不是 CSS 中的 sequential grouping。此外，SSND 自然适合 speaker-attributed ASR。我们对 open LibriCSS 数据集进行了评估，并在 diarization 和 ASR 领域取得了大幅提升的 estado-of-the-art 结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.SD_2023_11_15/" data-id="clpahu79v012k3h8803ra5czt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.CV_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T13:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.CV_2023_11_15/">cs.CV - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Predicting-Spine-Geometry-and-Scoliosis-from-DXA-Scans"><a href="#Predicting-Spine-Geometry-and-Scoliosis-from-DXA-Scans" class="headerlink" title="Predicting Spine Geometry and Scoliosis from DXA Scans"></a>Predicting Spine Geometry and Scoliosis from DXA Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09424">http://arxiv.org/abs/2311.09424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Jamaludin, Timor Kadir, Emma Clark, Andrew Zisserman</li>
<li>for: 这paper的目的是估算DXA扫描中脊梁的弯 curvature.</li>
<li>methods: 首先，我们训练一个神经网络来预测扫描中的中脊梁弯曲。然后，我们使用一种积分方法来确定脊梁弯曲的弯曲范围。</li>
<li>results: 我们发现，使用弯曲度作为评分函数可以对脊梁弯曲进行排序，并且性能超过了 Jamaludin et al. 2018 的先前工作。<details>
<summary>Abstract</summary>
Our objective in this paper is to estimate spine curvature in DXA scans. To this end we first train a neural network to predict the middle spine curve in the scan, and then use an integral-based method to determine the curvature along the spine curve. We use the curvature to compare to the standard angle scoliosis measure obtained using the DXA Scoliosis Method (DSM). The performance improves over the prior work of Jamaludin et al. 2018. We show that the maximum curvature can be used as a scoring function for ordering the severity of spinal deformation.
</details>
<details>
<summary>摘要</summary>
我们的目标在这篇论文中是估算DXA扫描中的脊梁弯曲度。为达到这个目标，我们首先训练一个神经网络来预测扫描中的中脊梁弯曲度，然后使用一种积分方法来确定脊梁弯曲度的沿脊梁曲线。我们使用弯曲度来与使用DXA脊梁疾病方法（DSM）获得的标准角度股疾病量进行比较。我们表明，最大弯曲度可以用作评估脊梁弯曲度严重程度的分数函数。Here's the translation in Traditional Chinese:我们的目标在这篇论文中是估算DXA扫描中的脊梁弯曲度。为达到这个目标，我们首先训练一个神经网络来预测扫描中的中脊梁弯曲度，然后使用一种积分方法来确定脊梁弯曲度的沿脊梁曲线。我们使用弯曲度来与使用DXA脊梁疾病方法（DSM）获得的标准角度股疾病量进行比较。我们表明，最大弯曲度可以用作评估脊梁弯曲度严重程度的分数函数。
</details></li>
</ul>
<hr>
<h2 id="Synthetically-Enhanced-Unveiling-Synthetic-Data’s-Potential-in-Medical-Imaging-Research"><a href="#Synthetically-Enhanced-Unveiling-Synthetic-Data’s-Potential-in-Medical-Imaging-Research" class="headerlink" title="Synthetically Enhanced: Unveiling Synthetic Data’s Potential in Medical Imaging Research"></a>Synthetically Enhanced: Unveiling Synthetic Data’s Potential in Medical Imaging Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09402">http://arxiv.org/abs/2311.09402</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bardiakh/syntheticallyenhanced">https://github.com/bardiakh/syntheticallyenhanced</a></li>
<li>paper_authors: Bardia Khosravi, Frank Li, Theo Dapamede, Pouria Rouzrokh, Cooper U. Gamble, Hari M. Trivedi, Cody C. Wyles, Andrew B. Sellergren, Saptarshi Purkayastha, Bradley J. Erickson, Judy W. Gichoya</li>
<li>for: 这个研究旨在检验深度学习（DL）分类器在胸部X射线成像（CXR）分析中的性能，并研究使用扩充的数据集来提高模型的准确率。</li>
<li>methods: 这个研究使用了三个数据集：CheXpert、MIMIC-CXR和Emory Chest X-ray，并使用了条件涉嫌扩充推散模型（DDPMs）来生成医学图像。我们确保了生成的人工图像具有原始数据中的人类和疾病特征。</li>
<li>results: 研究发现，通过使用人工数据来补充真实数据，可以提高DL模型的准确率，特别是在检测较少发现的疾病方面。此外，使用人工数据alone来训练模型也可以达到与使用真实数据来训练模型的性能水平。这表示人工数据可能可以弥补真实数据的短缺，并且可以帮助建立更加坚固的DL模型。然而，尽管有扎实的结果，真实数据仍然保持优势。<details>
<summary>Abstract</summary>
Chest X-rays (CXR) are the most common medical imaging study and are used to diagnose multiple medical conditions. This study examines the impact of synthetic data supplementation, using diffusion models, on the performance of deep learning (DL) classifiers for CXR analysis. We employed three datasets: CheXpert, MIMIC-CXR, and Emory Chest X-ray, training conditional denoising diffusion probabilistic models (DDPMs) to generate synthetic frontal radiographs. Our approach ensured that synthetic images mirrored the demographic and pathological traits of the original data. Evaluating the classifiers' performance on internal and external datasets revealed that synthetic data supplementation enhances model accuracy, particularly in detecting less prevalent pathologies. Furthermore, models trained on synthetic data alone approached the performance of those trained on real data. This suggests that synthetic data can potentially compensate for real data shortages in training robust DL models. However, despite promising outcomes, the superiority of real data persists.
</details>
<details>
<summary>摘要</summary>
胸部X射影（CXR）是医学影像研究中最常用的方法，用于诊断多种医疗问题。本研究检查了深度学习（DL）分类器在CXR分析中使用推 diffusion模型（DDPMs）的合成数据补充的影响。我们使用了三个数据集：CheXpert、MIMIC-CXR和Emory Chest X-ray，并使用 conditional denoising diffusion probabilistic models（DDPMs）来生成合成前置影像。我们的方法确保了生成的合成图像具有原始数据中的人口和疾病特征。我们对内部和外部数据集进行评估，发现使用合成数据补充可以提高模型的准确率，特别是在检测较少发生的疾病方面。此外，使用合成数据alone进行训练的模型可以达到与使用真实数据训练的模型相同的性能。这表明合成数据可以可能补充实际数据的短缺，帮助建立更加稳定的DL模型。然而，尽管出色的结果，实际数据仍然保持优势。
</details></li>
</ul>
<hr>
<h2 id="MoCo-Transfer-Investigating-out-of-distribution-contrastive-learning-for-limited-data-domains"><a href="#MoCo-Transfer-Investigating-out-of-distribution-contrastive-learning-for-limited-data-domains" class="headerlink" title="MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains"></a>MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09401">http://arxiv.org/abs/2311.09401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwen Chen, Helen Zhou, Zachary C. Lipton</li>
<li>for: 这 paper 是为了研究如何将自动生成的医学影像数据转移到不同的域内数据集上，以提高特殊模型的开发。</li>
<li>methods: 这 paper 使用了 MoCo 自我超vised 对比表示法进行预训练，并对不同的 X-ray 数据集进行比较，以确定是否可以从更大的数据集中提取有用的信息。</li>
<li>results: 研究发现，在有限量的数据集上，可以通过从更大的数据集中提取自动生成的医学影像数据来提高模型性能。此外，在不同的域内数据集之间进行对比可以更好地提高模型性能，而不是使用 ImageNet 预训练的 weights。最后，这 paper 还提供了一种初步的数据集相似性量化方法。<details>
<summary>Abstract</summary>
Medical imaging data is often siloed within hospitals, limiting the amount of data available for specialized model development. With limited in-domain data, one might hope to leverage larger datasets from related domains. In this paper, we analyze the benefit of transferring self-supervised contrastive representations from moment contrast (MoCo) pretraining on out-of-distribution data to settings with limited data. We consider two X-ray datasets which image different parts of the body, and compare transferring from each other to transferring from ImageNet. We find that depending on quantity of labeled and unlabeled data, contrastive pretraining on larger out-of-distribution datasets can perform nearly as well or better than MoCo pretraining in-domain, and pretraining on related domains leads to higher performance than if one were to use the ImageNet pretrained weights. Finally, we provide a preliminary way of quantifying similarity between datasets.
</details>
<details>
<summary>摘要</summary>
医疗影像数据经常受限于医院内部，限制了特殊模型开发的数据量。在这种情况下，可能会希望利用更大的相关领域数据进行开发。本文分析了在有限数据情况下，将自动学习强制对比表示（MoCo）预训练的扩展数据传递到另一个领域的效果。我们考虑了两个X射线数据集，它们分别捕捉了不同的身体部位，并比较了从别的领域传递和从ImageNet预训练的效果。我们发现，取决于标注和无标注数据的数量，在更大的外部数据集上进行对比预训练可以与域内MoCo预训练相当或更好，而从相关领域传递的预训练性能高于使用ImageNet预训练的模型。最后，我们提供了一种初步的数据集相似性量化方法。
</details></li>
</ul>
<hr>
<h2 id="RENI-A-Rotation-Equivariant-Scale-Invariant-Natural-Illumination-Prior"><a href="#RENI-A-Rotation-Equivariant-Scale-Invariant-Natural-Illumination-Prior" class="headerlink" title="RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior"></a>RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09361">http://arxiv.org/abs/2311.09361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jadgardner/ns_reni">https://github.com/jadgardner/ns_reni</a></li>
<li>paper_authors: James A. D. Gardner, Bernhard Egger, William A. P. Smith</li>
<li>for: 本研究旨在提出一种基于神经网络的自然照明模型，用于解决 inverse rendering 问题。</li>
<li>methods: 该模型使用 conditional neural field 表示法和 transformer 解码器，并通过 Vector Neurons 技术实现建立对称性。</li>
<li>results: 模型可以准确地表示高动态范围 (HDR) 环境图像，并且可以捕捉到自然环境中高频特征。 Is there anything else you would like to know?<details>
<summary>Abstract</summary>
Inverse rendering is an ill-posed problem. Previous work has sought to resolve this by focussing on priors for object or scene shape or appearance. In this work, we instead focus on a prior for natural illuminations. Current methods rely on spherical harmonic lighting or other generic representations and, at best, a simplistic prior on the parameters. This results in limitations for the inverse setting in terms of the expressivity of the illumination conditions, especially when taking specular reflections into account. We propose a conditional neural field representation based on a variational auto-decoder and a transformer decoder. We extend Vector Neurons to build equivariance directly into our architecture, and leveraging insights from depth estimation through a scale-invariant loss function, we enable the accurate representation of High Dynamic Range (HDR) images. The result is a compact, rotation-equivariant HDR neural illumination model capable of capturing complex, high-frequency features in natural environment maps. Training our model on a curated dataset of 1.6K HDR environment maps of natural scenes, we compare it against traditional representations, demonstrate its applicability for an inverse rendering task and show environment map completion from partial observations. We share our PyTorch implementation, dataset and trained models at https://github.com/JADGardner/ns_reni
</details>
<details>
<summary>摘要</summary>
“ inverse rendering 是一个欠定的问题。 先前的工作强调了对象或场景形状或外观的确认，以解决这个问题。 在这个工作中，我们则专注于天然照明的确认。 现有的方法使用圆柱对称光学或其他通用表示，并仅对parameters的简单确认。 这会导致对 inverse setting 的限制，特别是在考虑到镜面反射时。 我们提议一个基于 conditional neural field 的表示方法，使用 variational auto-decoder 和 transformer decoder。 我们将 Vector Neurons 扩展到建立了 architecture 中的 equivariance，并利用 depth estimation 中的构成调和损失函数，以实现高动态范围 (HDR) 图像的准确表示。 结果是一个可靠、旋转相似的 HDR 神经照明模型，能够捕捉自然环境图中的复杂、高频率特征。 我们在一个精心选择的 dataset 上训练我们的模型，并与传统表示进行比较，证明其适用于 inverse rendering 任务，以及环境图完整性的完成从部分观察。 我们在 GitHub 上分享我们的 PyTorch 实现、dataset 和训练模型，请见 <https://github.com/JADGardner/ns_reni>。”
</details></li>
</ul>
<hr>
<h2 id="Nothing-Stands-Still-A-Spatiotemporal-Benchmark-on-3D-Point-Cloud-Registration-Under-Large-Geometric-and-Temporal-Change"><a href="#Nothing-Stands-Still-A-Spatiotemporal-Benchmark-on-3D-Point-Cloud-Registration-Under-Large-Geometric-and-Temporal-Change" class="headerlink" title="Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration Under Large Geometric and Temporal Change"></a>Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration Under Large Geometric and Temporal Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09346">http://arxiv.org/abs/2311.09346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Sun, Yan Hao, Shengyu Huang, Silvio Savarese, Konrad Schindler, Marc Pollefeys, Iro Armeni</li>
<li>for: 这篇论文旨在探讨现有的3D geometric map建构方法如何处理时间变化。</li>
<li>methods: 该论文使用了多个参考视图的点云注册方法，以及一个新的多方向点云注册 benchmark。</li>
<li>results: 研究发现现有的点云注册方法无法处理大规模的时间变化，新的方法需要 специаль地设计来处理这类变化。<details>
<summary>Abstract</summary>
Building 3D geometric maps of man-made spaces is a well-established and active field that is fundamental to computer vision and robotics. However, considering the evolving nature of built environments, it is essential to question the capabilities of current mapping efforts in handling temporal changes. In addition, spatiotemporal mapping holds significant potential for achieving sustainability and circularity goals. Existing mapping approaches focus on small changes, such as object relocation or self-driving car operation; in all cases where the main structure of the scene remains fixed. Consequently, these approaches fail to address more radical changes in the structure of the built environment, such as geometry and topology. To this end, we introduce the Nothing Stands Still (NSS) benchmark, which focuses on the spatiotemporal registration of 3D scenes undergoing large spatial and temporal change, ultimately creating one coherent spatiotemporal map. Specifically, the benchmark involves registering two or more partial 3D point clouds (fragments) from the same scene but captured from different spatiotemporal views. In addition to the standard pairwise registration, we assess the multi-way registration of multiple fragments that belong to any temporal stage. As part of NSS, we introduce a dataset of 3D point clouds recurrently captured in large-scale building indoor environments that are under construction or renovation. The NSS benchmark presents three scenarios of increasing difficulty, to quantify the generalization ability of point cloud registration methods over space (within one building and across buildings) and time. We conduct extensive evaluations of state-of-the-art methods on NSS. The results demonstrate the necessity for novel methods specifically designed to handle large spatiotemporal changes. The homepage of our benchmark is at http://nothing-stands-still.com.
</details>
<details>
<summary>摘要</summary>
建立3D геометрические地图的人工环境是一个已经成熟且活跃的领域，对计算机视觉和机器人来说是基础领域。然而，随着建筑环境的发展，需要考虑当前地图努力的有效性，特别是在面对时间变化时。此外，空间时间地图具有可持续和循环的潜力，现有的地图方法主要集中于小型变化，如物体重新布局或自动驾驶车辆操作，而 ignore 主要结构的变化。为此，我们引入Nothing Stands Still（NSS）测试准则，强调在3D场景中大尺度空间和时间变化下进行空间时间地图的准确匹配。具体来说，测试准则包括将两个或多个来自同一场景，但从不同空间时间视图捕捉的3D点云（块）进行对比注准。此外，我们还评估了多个块之间的多方注准，以及这些块在不同时间阶段的注准。NSS测试准则包括3个难度不同的场景，以评估点云注准方法的通用性和灵活性。我们在NSS测试准则上进行了广泛的评估，结果表明了现有方法在面对大尺度空间时间变化时的不足。NSS测试准则的主页可以在http://nothing-stands-still.com 中找到。
</details></li>
</ul>
<hr>
<h2 id="Single-Image-3D-Human-Digitization-with-Shape-Guided-Diffusion"><a href="#Single-Image-3D-Human-Digitization-with-Shape-Guided-Diffusion" class="headerlink" title="Single-Image 3D Human Digitization with Shape-Guided Diffusion"></a>Single-Image 3D Human Digitization with Shape-Guided Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09221">http://arxiv.org/abs/2311.09221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, Jia-Bin Huang</li>
<li>for: 实现单一图像中的人物360度全景视图，包括衣服和人体的高精度描绘。</li>
<li>methods: 利用高容量2D扩散模型来提供衣服人体的应earance假设，然后逐步Synthesize多个视角的人物图像，并通过反射处理协会Multi-view图像进行融合，以获得高精度的3D mesh。</li>
<li>results: 试验结果显示，我们的方法可以优于先前的方法，实现高精度、 фото实际的360度人物全景视图，并且可以处理不同的衣服和人体表现。<details>
<summary>Abstract</summary>
We present an approach to generate a 360-degree view of a person with a consistent, high-resolution appearance from a single input image. NeRF and its variants typically require videos or images from different viewpoints. Most existing approaches taking monocular input either rely on ground-truth 3D scans for supervision or lack 3D consistency. While recent 3D generative models show promise of 3D consistent human digitization, these approaches do not generalize well to diverse clothing appearances, and the results lack photorealism. Unlike existing work, we utilize high-capacity 2D diffusion models pretrained for general image synthesis tasks as an appearance prior of clothed humans. To achieve better 3D consistency while retaining the input identity, we progressively synthesize multiple views of the human in the input image by inpainting missing regions with shape-guided diffusion conditioned on silhouette and surface normal. We then fuse these synthesized multi-view images via inverse rendering to obtain a fully textured high-resolution 3D mesh of the given person. Experiments show that our approach outperforms prior methods and achieves photorealistic 360-degree synthesis of a wide range of clothed humans with complex textures from a single image.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，可以从单个输入图像中生成一个360度的人体视图，具有一致性和高分辨率的外观。NeRF和其变体通常需要不同视角的视频或图像。现有的approach都是通过ground-truth 3D扫描来supervise，或者lack 3D一致性。而最近的3D生成模型显示了人体数字化的3D一致性，但这些方法不能普遍应用于多样化的服装外表，并且lack photorealism。我们利用高容量2D扩散模型，这些模型在通用图像生成任务上进行预训练，作为人体服装的外观先验。为了实现更好的3D一致性而保持输入人物的身份，我们逐步synthesize多个视图的人体在输入图像中的缺失区域，使用形状响应的扩散条件和表面法向量来填充。然后，我们将这些合成的多视图图像进行反向渲染，以获得一个完全纹理高分辨率的3D mesh。实验表明，我们的方法超过了先前的方法，并实现了从单个图像中高真实度地生成360度的人体视图，包括复杂的服装表面。
</details></li>
</ul>
<hr>
<h2 id="DMV3D-Denoising-Multi-View-Diffusion-using-3D-Large-Reconstruction-Model"><a href="#DMV3D-Denoising-Multi-View-Diffusion-using-3D-Large-Reconstruction-Model" class="headerlink" title="DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model"></a>DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09217">http://arxiv.org/abs/2311.09217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, Kai Zhang</li>
<li>for: The paper is written for proposing a novel 3D generation approach called DMV3D, which uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion.</li>
<li>methods: The paper uses a transformer-based 3D large reconstruction model that incorporates a triplane NeRF representation to denoise noisy multi-view images, achieving single-stage 3D generation in $\sim$30s on single A100 GPU.</li>
<li>results: The paper demonstrates state-of-the-art results for the single-image reconstruction problem where probabilistic modeling of unseen object parts is required for generating diverse reconstructions with sharp textures. The paper also shows high-quality text-to-3D generation results outperforming previous 3D diffusion models.<details>
<summary>Abstract</summary>
We propose \textbf{DMV3D}, a novel 3D generation approach that uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion. Our reconstruction model incorporates a triplane NeRF representation and can denoise noisy multi-view images via NeRF reconstruction and rendering, achieving single-stage 3D generation in $\sim$30s on single A100 GPU. We train \textbf{DMV3D} on large-scale multi-view image datasets of highly diverse objects using only image reconstruction losses, without accessing 3D assets. We demonstrate state-of-the-art results for the single-image reconstruction problem where probabilistic modeling of unseen object parts is required for generating diverse reconstructions with sharp textures. We also show high-quality text-to-3D generation results outperforming previous 3D diffusion models. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .
</details>
<details>
<summary>摘要</summary>
我们提出了\textbf{DMV3D}，一种新的3D生成方法，使用transformer基础的3D大量重建模型来去噪多视对冲测。我们的重建模型包括了三面NeRF表现，可以通过NeRF重建和渲染来去噪多视图像，实现单一阶段3D生成，需时约30秒在单一A100 GPU上。我们在大规模多视图像数据集上训练\textbf{DMV3D}，只使用图像重建损失，不需要访问3D资产。我们示出了单一图像重建问题中的前景，需要 probabilistic modeling of unseen object parts，以生成多样化的重建结果， texture sharpness。我们还展示了与前一代3D扩散模型相比，\textbf{DMV3D}可以实现高品质的文本到3D生成。我们的项目网站位于：https://justimyhxu.github.io/projects/dmv3d/ .
</details></li>
</ul>
<hr>
<h2 id="ConvNet-vs-Transformer-Supervised-vs-CLIP-Beyond-ImageNet-Accuracy"><a href="#ConvNet-vs-Transformer-Supervised-vs-CLIP-Beyond-ImageNet-Accuracy" class="headerlink" title="ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy"></a>ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09215">http://arxiv.org/abs/2311.09215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kirill-vish/beyond-inet">https://github.com/kirill-vish/beyond-inet</a></li>
<li>paper_authors: Kirill Vishniakov, Zhiqiang Shen, Zhuang Liu</li>
<li>for: 本研究旨在探讨现代计算机视觉模型的多样性，以及选择最佳模型 для特定应用场景的决策因素。</li>
<li>methods: 本研究采用了多种模型建构和训练方法，包括ConvNet和Vision Transformer两种主流建构，以及supervised和CLIP训练方法。</li>
<li>results: 研究发现，尽管选择的模型在ImageNet精度上具有相似性，但它们在其他方面存在显著差异，如输出准确率、类型错误、转移性和特征不变等。这些差异不可能由传统的精度metric fully capture。<details>
<summary>Abstract</summary>
Modern computer vision offers a great variety of models to practitioners, and selecting a model from multiple options for specific applications can be challenging. Conventionally, competing model architectures and training protocols are compared by their classification accuracy on ImageNet. However, this single metric does not fully capture performance nuances critical for specialized tasks. In this work, we conduct an in-depth comparative analysis of model behaviors beyond ImageNet accuracy, for both ConvNet and Vision Transformer architectures, each across supervised and CLIP training paradigms. Although our selected models have similar ImageNet accuracies and compute requirements, we find that they differ in many other aspects: types of mistakes, output calibration, transferability, and feature invariance, among others. This diversity in model characteristics, not captured by traditional metrics, highlights the need for more nuanced analysis when choosing among different models. Our code is available at https://github.com/kirill-vish/Beyond-INet.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉提供了多种模型选择器，选择合适的模型对特定应用可以是挑战。传统上，竞争模型建筑和训练协议通常被比较通过ImageNet分类精度。然而，这个单一指标并不完全捕捉特有任务的性能细节。在这项工作中，我们进行了深入的比较分析，涵盖ConvNet和Vision Transformer建筑，每种模型在超vised和CLIP训练协议下的行为。虽然我们选择的模型在ImageNet精度和计算需求上几乎相同，但我们发现它们在很多方面不同：出错类型、输出准确、传输性和特征不变等方面。这些模型特性的多样性，不被传统指标捕捉，高亮了选择模型时需要更加细化的分析。我们的代码可以在https://github.com/kirill-vish/Beyond-INet上获取。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Citizen-Science-for-Flood-Extent-Detection-using-Machine-Learning-Benchmark-Dataset"><a href="#Leveraging-Citizen-Science-for-Flood-Extent-Detection-using-Machine-Learning-Benchmark-Dataset" class="headerlink" title="Leveraging Citizen Science for Flood Extent Detection using Machine Learning Benchmark Dataset"></a>Leveraging Citizen Science for Flood Extent Detection using Machine Learning Benchmark Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09276">http://arxiv.org/abs/2311.09276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muthukumaran Ramasubramanian, Iksha Gurung, Shubhankar Gahlot, Ronny Hänsch, Andrew L. Molthan, Manil Maskey</li>
<li>for: 这个论文的目的是为了提供一个高精度的洪水覆盖区域检测方法，以便在紧急应急响应和恢复工作中使用。</li>
<li>methods: 这个论文使用了卫星遥感数据，特别是Sentinel-1 C-Band Synthetic Aperture Radar（SAR）图像，以检测洪水覆盖区域。由于SAR图像中水体的低反射率，因此可以准确地检测水体。然而，在某些洪水区域中，如果存在基础设施和树木等，会导致反射强度增加，使得简单的像素强度阈值和时间序列差分方法无法准确地检测洪水覆盖区域。因此，这个论文使用机器学习技术来准确地检测洪水覆盖区域。</li>
<li>results: 这个论文提供了一个标注了水体覆盖区域和洪水覆盖区域的 dataset，以及一个基eline模型和一个开源的 competedition，以推动洪水覆盖区域检测的研究。此外，这个论文还利用了公民科学，通过开源dataset和组织一个开源的比赛，以快速推进洪水覆盖区域检测的社区生成模型。<details>
<summary>Abstract</summary>
Accurate detection of inundated water extents during flooding events is crucial in emergency response decisions and aids in recovery efforts. Satellite Remote Sensing data provides a global framework for detecting flooding extents. Specifically, Sentinel-1 C-Band Synthetic Aperture Radar (SAR) imagery has proven to be useful in detecting water bodies due to low backscatter of water features in both co-polarized and cross-polarized SAR imagery. However, increased backscatter can be observed in certain flooded regions such as presence of infrastructure and trees - rendering simple methods such as pixel intensity thresholding and time-series differencing inadequate. Machine Learning techniques has been leveraged to precisely capture flood extents in flooded areas with bumps in backscatter but needs high amounts of labelled data to work desirably. Hence, we created a labeled known water body extent and flooded area extents during known flooding events covering about 36,000 sq. kilometers of regions within mainland U.S and Bangladesh. Further, We also leveraged citizen science by open-sourcing the dataset and hosting an open competition based on the dataset to rapidly prototype flood extent detection using community generated models. In this paper we present the information about the dataset, the data processing pipeline, a baseline model and the details about the competition, along with discussion on winning approaches. We believe the dataset adds to already existing datasets based on Sentinel-1C SAR data and leads to more robust modeling of flood extents. We also hope the results from the competition pushes the research in flood extent detection further.
</details>
<details>
<summary>摘要</summary>
“溢涌水域的扩散范围检测是紧急应急响应和恢复工作中的关键。卫星遥感数据提供了全球性的检测溢涌水域的方法。特别是Sentinel-1 C-Band Synthetic Aperture Radar（SAR）影像已经证明可以准确检测水体，因为水体具有低的反射强度。然而，某些淹没区域中的基础设施和树木会导致反射强度增加，使得简单的像素强度阈值和时间序列差异方法无法准确检测溢涌水域。机器学习技术已经被应用于准确地检测溢涌水域，但需要大量标注数据来工作。因此，我们创建了标注水体范围和淹没区域的known数据集，覆盖了美国大陆和孟加拉国的约36,000平方公里地区。此外，我们还利用公民科学，将数据集打包开源，并在该数据集基础上组织了一场公开竞赛，以快速搭建溢涌水域检测模型。本文介绍了数据集、数据处理管道、基线模型以及竞赛的详细信息，以及赢家的方法。我们认为该数据集将提高现有的Sentinel-1C SAR数据集，并且希望竞赛结果可以推动溢涌水域检测的研究进一步。”
</details></li>
</ul>
<hr>
<h2 id="Domain-Aligned-CLIP-for-Few-shot-Classification"><a href="#Domain-Aligned-CLIP-for-Few-shot-Classification" class="headerlink" title="Domain Aligned CLIP for Few-shot Classification"></a>Domain Aligned CLIP for Few-shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09191">http://arxiv.org/abs/2311.09191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Waleed Gondal, Jochen Gast, Inigo Alonso Ruiz, Richard Droste, Tommaso Macri, Suren Kumar, Luitpold Staudigl</li>
<li>for: 提高CLIP模型在target分布上的预测性能，包括图像分类和OOD robustness。</li>
<li>methods: 提出了一种sample-efficient领域适应策略，称为Domain Aligned CLIP (DAC)，可以在target分布上提高图像-图像和图像-文本的同步，无需修改CLIP模型的参数。</li>
<li>results: 在11个广泛使用的图像分类任务上 demonstrates 16-shot classification的领先表现，相比强基eline的2.3%提高，并在4个OOD robustness benchmark上达到了竞争性表现。<details>
<summary>Abstract</summary>
Large vision-language representation learning models like CLIP have demonstrated impressive performance for zero-shot transfer to downstream tasks while largely benefiting from inter-modal (image-text) alignment via contrastive objectives. This downstream performance can further be enhanced by full-scale fine-tuning which is often compute intensive, requires large labelled data, and can reduce out-of-distribution (OOD) robustness. Furthermore, sole reliance on inter-modal alignment might overlook the rich information embedded within each individual modality. In this work, we introduce a sample-efficient domain adaptation strategy for CLIP, termed Domain Aligned CLIP (DAC), which improves both intra-modal (image-image) and inter-modal alignment on target distributions without fine-tuning the main model. For intra-modal alignment, we introduce a lightweight adapter that is specifically trained with an intra-modal contrastive objective. To improve inter-modal alignment, we introduce a simple framework to modulate the precomputed class text embeddings. The proposed few-shot fine-tuning framework is computationally efficient, robust to distribution shifts, and does not alter CLIP's parameters. We study the effectiveness of DAC by benchmarking on 11 widely used image classification tasks with consistent improvements in 16-shot classification upon strong baselines by about 2.3% and demonstrate competitive performance on 4 OOD robustness benchmarks.
</details>
<details>
<summary>摘要</summary>
大型视力语言表示学习模型如CLIP已经显示出Zero-shot传输下游任务的出色表现，主要受益于图像文本对Alignment via对比目标。然而，这种下游性能可以通过全面精细调整进一步提高，但这需要大量标注数据、计算昂贵和可能导致OOD不稳定性下降。此外，几乎完全依赖于图像文本对Alignment可能会忽略每个模式内的资源多样性。在这种情况下，我们提出了一种减少样本成本的领域适应策略，称为Domain Aligned CLIP（DAC），该策略可以在目标分布上提高图像图像和图像文本对Alignment，无需修改CLIP的主模型参数。为了提高图像图像对Alignment，我们引入了一个轻量级的适配器，该适配器专门通过图像对对比目标来训练。为了改善图像文本对Alignment，我们引入了一个简单的框架来修改预计算的类文本嵌入。我们的几个少量精细调整框架可以快速、稳定地进行，不需要大量标注数据，并且不会改变CLIP的参数。我们对11种常用的图像分类任务进行了测试，并 consistently obtain 16-shot classification improvements of around 2.3% over strong baselines，并且在4个OOD robustness benchmark上达到了竞争性的性能。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computation-of-the-Gaussian-Rate-Distortion-Perception-Function"><a href="#On-the-Computation-of-the-Gaussian-Rate-Distortion-Perception-Function" class="headerlink" title="On the Computation of the Gaussian Rate-Distortion-Perception Function"></a>On the Computation of the Gaussian Rate-Distortion-Perception Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09190">http://arxiv.org/abs/2311.09190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Serra, Photios A. Stavrou, Marios Kountouris</li>
<li>for: 这个论文研究了多变量 Gaussian 源的 rate-distortion-perception 函数 (RDPF) 的计算，对于 mean squared error (MSE) 损害和各种抽象感知指标（Kullback-Leibler 异同、 геометрический Jensen-Shannon 异同、平方 Hellinger 距离和平方 Wasserstein-2 距离）。</li>
<li>methods: 作者首先计算了拟合函数的分析上下文 bound，并提供了 RDPF 实现的前向 “测试通道” 实现。在多变量情况下，作者表明了对于 tensorizable 损害和感知指标，优化解决方案 residues 在源协 variance 矩阵的 eigenvector 上。因此，多变量优化问题可以表示为一个约束的 scalar Gaussian RDPF 问题。</li>
<li>results: 作者提出了一种基于块非线性 Gauss-Seidel 方法的 alternating minimization 算法，可以优化多变量问题，同时 identificatinig RDPF 实现。此外，作者还提供了算法的实现、 converges 和 converge 速率的 Characterization。最后，作者在 “完美现实”  régime 下获得了多变量 Gaussian RDPF 的分析解。作者通过数值实验证明了结论，并与现有结果进行了比较。<details>
<summary>Abstract</summary>
In this paper, we study the computation of the rate-distortion-perception function (RDPF) for a multivariate Gaussian source under mean squared error (MSE) distortion and, respectively, Kullback-Leibler divergence, geometric Jensen-Shannon divergence, squared Hellinger distance, and squared Wasserstein-2 distance perception metrics. To this end, we first characterize the analytical bounds of the scalar Gaussian RDPF for the aforementioned divergence functions, also providing the RDPF-achieving forward "test-channel" realization. Focusing on the multivariate case, we establish that, for tensorizable distortion and perception metrics, the optimal solution resides on the vector space spanned by the eigenvector of the source covariance matrix. Consequently, the multivariate optimization problem can be expressed as a function of the scalar Gaussian RDPFs of the source marginals, constrained by global distortion and perception levels. Leveraging this characterization, we design an alternating minimization scheme based on the block nonlinear Gauss-Seidel method, which optimally solves the problem while identifying the Gaussian RDPF-achieving realization. Furthermore, the associated algorithmic embodiment is provided, as well as the convergence and the rate of convergence characterization. Lastly, for the "perfect realism" regime, the analytical solution for the multivariate Gaussian RDPF is obtained. We corroborate our results with numerical simulations and draw connections to existing results.
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了多变量 Gaussian 源的 computation rate-distortion-perception function (RDPF) 的计算，采用 Mean Squared Error (MSE) 损均、Kullback-Leibler 差分、Geometric Jensen-Shannon 差分、平方 Hellinger 距离和平方 Wasserstein-2 距离的感知度量。为此，我们首先计算了整数 Gaussian RDPF 的分析 bound，并提供了 RDPF 实现的前向 "测试通道" 实现。在多变量情况下，我们证明了，对于张量izable 损均和感知度量，最佳解在源协VARiance矩阵的 eigenvector 上。因此，多变量优化问题可以表示为各自 Gaussian RDPF 的源协VARiance矩阵的eigenvector 的函数，受到全局损均和感知水平的约束。我们利用这种特征来设计一种 alternating minimization 方案，基于块非线性 Gauss-Seidel 方法，可以最佳地解决问题，同时确定 Gaussian RDPF 实现。此外，我们还提供了算法实现、收敛性和收敛速度的特征。最后，在 "完美现实"  режиме下，我们获得了多变量 Gaussian RDPF 的分析解。我们的结果与数值仿真结果相符，并与现有结果进行了连接。
</details></li>
</ul>
<hr>
<h2 id="RBPGAN-Recurrent-Back-Projection-GAN-for-Video-Super-Resolution"><a href="#RBPGAN-Recurrent-Back-Projection-GAN-for-Video-Super-Resolution" class="headerlink" title="RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution"></a>RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09178">http://arxiv.org/abs/2311.09178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Israa Fahmy, Marwah Sulaiman, Zahraa Shehabeldin, Mohammed Barakat, Dareen Hussein, Mohammed El-Naggar, Hesham Eraqi, Moustafa Youssef</li>
<li>for: 这个论文旨在提出一种Video超分辨（VSR）模型，以生成时间协调的解决方案，保持空间细节。</li>
<li>methods: 该模型结合了两个 state-of-the-art 模型， generator  inspirited by RBPN 系统， discriminator  inspirited by TecoGAN。使用 Ping-Pong 损失函数，提高时间一致性。</li>
<li>results: 我们的贡献使得模型在 temporally 一致的细节方面表现出色，证明了我们的模型在不同的数据集上的高性能。<details>
<summary>Abstract</summary>
Recently, video super resolution (VSR) has become a very impactful task in the area of Computer Vision due to its various applications. In this paper, we propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for VSR in an attempt to generate temporally coherent solutions while preserving spatial details. RBPGAN integrates two state-of-the-art models to get the best in both worlds without compromising the accuracy of produced video. The generator of the model is inspired by RBPN system, while the discriminator is inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal consistency over time. Our contribution together results in a model that outperforms earlier work in terms of temporally consistent details, as we will demonstrate qualitatively and quantitatively using different datasets.
</details>
<details>
<summary>摘要</summary>
近期，视频超解像 (VSR) 在计算机视觉领域已经变得非常重要，因为它具有许多应用。在这篇论文中，我们提出了 Recurrent Back-Projection Generative Adversarial Network (RBPGAN)，用于实现时间相关的解决方案，同时保持空间细节的准确性。RBPGAN 结合了两个状态的先进模型，以获得最佳的效果而无需牺牲生成的视频的准确性。生成器采用 RBPN 系统的设计，而批判器采用 TecoGAN 的设计。我们还使用了 Ping-Pong 损失函数，以增加时间一致性。我们的贡献结合使得模型在时间一致性和空间细节方面表现出色，我们将通过不同的数据集进行质量和量化的比较来证明。
</details></li>
</ul>
<hr>
<h2 id="WildlifeDatasets-An-open-source-toolkit-for-animal-re-identification"><a href="#WildlifeDatasets-An-open-source-toolkit-for-animal-re-identification" class="headerlink" title="WildlifeDatasets: An open-source toolkit for animal re-identification"></a>WildlifeDatasets: An open-source toolkit for animal re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09118">http://arxiv.org/abs/2311.09118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wildlifedatasets/wildlife-datasets">https://github.com/wildlifedatasets/wildlife-datasets</a></li>
<li>paper_authors: Vojtěch Čermák, Lukas Picek, Lukáš Adam, Kostas Papafitsoros</li>
<li>for: 本研究开发了一个开源工具箱（WildlifeDatasets），旨在帮助生物学家和计算机视觉&#x2F;机器学习研究人员访问公共可用的野生动物数据集，并提供了许多数据集预处理、性能分析和模型细化等方法。</li>
<li>methods: 本研究使用了Python编程语言，并提供了许多数据集预处理和性能分析方法，包括了本研究中的最全面的实验比较，涵盖了本地描述器和深度学习方法。此外，本研究还提供了一个基于描述器的个体重复识别模型——MegaDescriptor，该模型在野生动物重复识别数据集上实现了state-of-the-art性能，并在其他预训练模型 such as CLIP 和 DINOv2 中出众表现。</li>
<li>results: 本研究通过实验和比较，证明了MegaDescriptor模型在野生动物重复识别任务上具有state-of-the-art性能，并且在多种情况下都能够准确地识别动物个体。此外，本研究还提供了多种MegaDescriptor的不同版本（i.e., Small, Medium, and Large），通过HuggingFace hub（<a target="_blank" rel="noopener" href="https://huggingface.co/BVRA%EF%BC%89%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%EF%BC%8C%E4%BB%A5%E4%BE%BF%E6%98%93%E4%BA%8E%E4%B8%8E%E7%8E%B0%E6%9C%89%E7%9A%84%E9%87%8E%E7%94%9F%E5%8A%A8%E7%89%A9%E7%9B%91%E6%B5%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%9B%86%E6%88%90%E3%80%82">https://huggingface.co/BVRA）进行了公开发布，以便易于与现有的野生动物监测应用程序集成。</a><details>
<summary>Abstract</summary>
In this paper, we present WildlifeDatasets (https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source toolkit intended primarily for ecologists and computer-vision / machine-learning researchers. The WildlifeDatasets is written in Python, allows straightforward access to publicly available wildlife datasets, and provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning. We showcase the toolkit in various scenarios and baseline experiments, including, to the best of our knowledge, the most comprehensive experimental comparison of datasets and methods for wildlife re-identification, including both local descriptors and deep learning approaches. Furthermore, we provide the first-ever foundation model for individual re-identification within a wide range of species - MegaDescriptor - that provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin. To make the model available to the general public and to allow easy integration with any existing wildlife monitoring applications, we provide multiple MegaDescriptor flavors (i.e., Small, Medium, and Large) through the HuggingFace hub (https://huggingface.co/BVRA).
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了 WildlifeDatasets（https://github.com/WildlifeDatasets/wildlife-datasets）-一个开源工具箱，主要针对生态学家和计算机视觉/机器学习研究人员。WildlifeDatasets 是用 Python 编写的，可以直接访问公共可用的野生动物数据集，并提供了许多方法 для数据集预处理、性能分析和模型细化。我们在各种场景和基线实验中示例了这个工具箱，包括我们知道的最全面的野生动物重新识别实验，包括本地描述器和深度学习方法。此外，我们还提供了一个称之为 MegaDescriptor 的基本模型，该模型在各种种类的动物重新识别任务中提供了状态机器的性能，并在其他预训练模型 such as CLIP 和 DINOv2 的基础上出色的超越。为使这个模型对一般公众开放，并让它与现有的野生监测应用程序融合，我们在 HuggingFace 平台（https://huggingface.co/BVRA）提供了多种 MegaDescriptor 的FLAVOR（i.e., Small, Medium, and Large）。
</details></li>
</ul>
<hr>
<h2 id="Cross-view-and-Cross-pose-Completion-for-3D-Human-Understanding"><a href="#Cross-view-and-Cross-pose-Completion-for-3D-Human-Understanding" class="headerlink" title="Cross-view and Cross-pose Completion for 3D Human Understanding"></a>Cross-view and Cross-pose Completion for 3D Human Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09104">http://arxiv.org/abs/2311.09104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthieu Armando, Salma Galaaoui, Fabien Baradel, Thomas Lucas, Vincent Leroy, Romain Brégier, Philippe Weinzaepfel, Grégory Rogez</li>
<li>for: The paper is written for the domain of computer vision, specifically for human perception and understanding.</li>
<li>methods: The paper proposes a pre-training approach based on self-supervised learning using human-centric data, including stereoscopic and temporal pairs of images.</li>
<li>results: The proposed method outperforms existing self-supervised pre-training methods on a wide set of human-centric downstream tasks, and achieves state-of-the-art performance for model-based and model-free human mesh recovery.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是关注计算机视觉领域，具体是人体识别和理解。</li>
<li>methods: 这篇论文提议一种基于自我监督学习的预训练方法，使用人体中心的数据集，包括左右视图和时间视图对。</li>
<li>results: 提议的方法在多种人体中心下沉淀任务上超过现有的自我监督预训练方法，并在模型基于和模型外的人体网格恢复任务上达到了状态机器。<details>
<summary>Abstract</summary>
Human perception and understanding is a major domain of computer vision which, like many other vision subdomains recently, stands to gain from the use of large models pre-trained on large datasets. We hypothesize that the most common pre-training strategy of relying on general purpose, object-centric image datasets such as ImageNet, is limited by an important domain shift. On the other hand, collecting domain specific ground truth such as 2D or 3D labels does not scale well. Therefore, we propose a pre-training approach based on self-supervised learning that works on human-centric data using only images. Our method uses pairs of images of humans: the first is partially masked and the model is trained to reconstruct the masked parts given the visible ones and a second image. It relies on both stereoscopic (cross-view) pairs, and temporal (cross-pose) pairs taken from videos, in order to learn priors about 3D as well as human motion. We pre-train a model for body-centric tasks and one for hand-centric tasks. With a generic transformer architecture, these models outperform existing self-supervised pre-training methods on a wide set of human-centric downstream tasks, and obtain state-of-the-art performance for instance when fine-tuning for model-based and model-free human mesh recovery.
</details>
<details>
<summary>摘要</summary>
人类 восприятие和理解是计算机视觉的一大领域，与其他视觉子领域一样，它也可以受惠于大型模型在大量数据上进行预训练。我们假设，通过普通的对象中心图像 dataset such as ImageNet 进行预训练的方法，受到了重要的领域变化的限制。而收集专门适用于人类数据的预先知道ledge，如 2D 或 3D 标注，不可能扩展。因此，我们提议一种基于自我学习的预训练方法，使用人类数据图像。我们的方法使用人类图像的对称对（cross-view）和视频中的姿态对（cross-pose）进行自我学习，以学习人类的3D 和运动约束。我们预训练了一个身体中心任务的模型和一个手中心任务的模型，使用通用的 transformer 架构。这些模型在人类中心下沉天任务中表现出色，并在模型基于和模型自由人体碎片恢复任务中实现了国际级的表现。
</details></li>
</ul>
<hr>
<h2 id="Guided-Scale-Space-Radon-Transform-for-linear-structures-detection"><a href="#Guided-Scale-Space-Radon-Transform-for-linear-structures-detection" class="headerlink" title="Guided Scale Space Radon Transform for linear structures detection"></a>Guided Scale Space Radon Transform for linear structures detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09103">http://arxiv.org/abs/2311.09103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aicha Baya Goumeidane, Djemel Ziou, Nafaa Nacereddine</li>
<li>for:  automaton detection of thick linear structures in gray scale and binary images</li>
<li>methods: 使用Scale Space Radon Transform (SSRT) 和计算图像的希尔曼方向的方法</li>
<li>results: 能够有效地检测图像中的不同厚度线条，并且具有鲁棒性 against noise and complex background<details>
<summary>Abstract</summary>
Using integral transforms to the end of lines detection in images with complex background, makes the detection a hard task needing additional processing to manage the detection. As an integral transform, the Scale Space Radon Transform (SSRT) suffers from such drawbacks, even with its great abilities for thick lines detection. In this work, we propose a method to address this issue for automatic detection of thick linear structures in gray scale and binary images using the SSRT, whatever the image background content. This method involves the calculated Hessian orientations of the investigated image while computing its SSRT, in such a way that linear structures are emphasized in the SSRT space. As a consequence, the subsequent maxima detection in the SSRT space is done on a modified transform space freed from unwanted parts and, consequently, from irrelevant peaks that usually drown the peaks representing lines. Besides, highlighting the linear structure in the SSRT space permitting, thus, to efficiently detect lines of different thickness in synthetic and real images, the experiments show also the method robustness against noise and complex background.
</details>
<details>
<summary>摘要</summary>
Our method involves calculating the Hessian orientations of the investigated image while computing its SSRT, which emphasizes linear structures in the SSRT space. As a result, the subsequent maxima detection in the SSRT space is done on a modified transform space free from unwanted parts and irrelevant peaks that usually drown the peaks representing lines. This approach allows us to efficiently detect lines of different thickness in synthetic and real images, and the experiments show the method's robustness against noise and complex backgrounds.
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Computer-Vision-in-Autonomous-Vehicles-Methods-Challenges-and-Future-Directions"><a href="#Applications-of-Computer-Vision-in-Autonomous-Vehicles-Methods-Challenges-and-Future-Directions" class="headerlink" title="Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions"></a>Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09093">http://arxiv.org/abs/2311.09093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingshuai Dong, Massimiliano L. Cappuccio</li>
<li>for: 本文主要旨在为读者提供自动驾驶技术的全面了解，包括自动驾驶系统的发展、感知器技术、benchmark数据集和公共评价等方面。</li>
<li>methods: 本文主要使用了computer vision技术，包括深度估计、物体检测、车道检测和交通标志识别等方面。同时，本文还对自动驾驶系统的发展进行了概述，包括主要汽车制造商从不同国家的开发。</li>
<li>results: 本文对自动驾驶技术的现状进行了全面的检视和分析，包括现有技术挑战和未来研究方向等。通过对自动驾驶技术的概述和分析，本文可以帮助读者更好地了解自动驾驶技术的发展和应用。<details>
<summary>Abstract</summary>
Autonomous vehicle refers to a vehicle capable of perceiving its surrounding environment and driving with little or no human driver input. The perception system is a fundamental component which enables the autonomous vehicle to collect data and extract relevant information from the environment to drive safely. Benefit from the recent advances in computer vision, the perception task can be achieved by using sensors, such as camera, LiDAR, radar, and ultrasonic sensor. This paper reviews publications on computer vision and autonomous driving that are published during the last ten years. In particular, we first investigate the development of autonomous driving systems and summarize these systems that are developed by the major automotive manufacturers from different countries. Second, we investigate the sensors and benchmark data sets that are commonly utilized for autonomous driving. Then, a comprehensive overview of computer vision applications for autonomous driving such as depth estimation, object detection, lane detection, and traffic sign recognition are discussed. Additionally, we review public opinions and concerns on autonomous vehicles. Based on the discussion, we analyze the current technological challenges that autonomous vehicles meet with. Finally, we present our insights and point out some promising directions for future research. This paper will help the reader to understand autonomous vehicles from the perspectives of academia and industry.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆指的是一种可以自主感知周围环境并减少或完全消除人类驾驶员输入的车辆。感知系统是自动驾驶车辆的基本组件，它使得自动驾驶车辆能够收集环境数据并提取有用信息以安全驾驶。受计算机视觉技术的推动，感知任务可以通过感知器，如摄像头、LiDAR、雷达和超声波感知器来实现。本文回顾过去十年发表的计算机视觉和自动驾驶相关的研究论文。特别是，我们首先调查了各国主要汽车制造商在自动驾驶领域的开发系统，然后调查了通用于自动驾驶的感知器和标准数据集。接着，我们提供了计算机视觉在自动驾驶中的应用篇幅，包括深度估计、物体检测、车道检测和交通标志识别。此外，我们还评估了自动驾驶车辆的公众观点和担忧，并分析了自动驾驶车辆目前所面临的技术挑战。最后，我们提出了一些有前途的研究方向。本文将帮助读者更好地理解自动驾驶车辆的 academia 和 industry 视角。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Transformer-Learning-with-Proximity-Data-Generation-for-Text-Based-Person-Search"><a href="#Contrastive-Transformer-Learning-with-Proximity-Data-Generation-for-Text-Based-Person-Search" class="headerlink" title="Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search"></a>Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09084">http://arxiv.org/abs/2311.09084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hcplab-sysu/personsearch-ctlg">https://github.com/hcplab-sysu/personsearch-ctlg</a></li>
<li>paper_authors: Hefeng Wu, Weifeng Chen, Zhibin Liu, Tianshui Chen, Zhiguang Chen, Liang Lin</li>
<li>for: 这个论文是为了提出一种简单 yet effective的双Transformer模型，用于图像库中的文本基于人脸检索。</li>
<li>methods: 该模型使用了一种具有强度感知的对比学习策略，以及一种自动生成数据模块（PDG），以提高模型的性能。</li>
<li>results: 实验表明，该模型在两个流行的TBPS数据集（CUHK-PEDES和ICFG-PEDES）上表现出色，与现有方法相比，提高了Top1、Top5、Top10的性能（例如，CUHK-PEDES上提高了3.88%, 4.02%, 2.92%）。<details>
<summary>Abstract</summary>
Given a descriptive text query, text-based person search (TBPS) aims to retrieve the best-matched target person from an image gallery. Such a cross-modal retrieval task is quite challenging due to significant modality gap, fine-grained differences and insufficiency of annotated data. To better align the two modalities, most existing works focus on introducing sophisticated network structures and auxiliary tasks, which are complex and hard to implement. In this paper, we propose a simple yet effective dual Transformer model for text-based person search. By exploiting a hardness-aware contrastive learning strategy, our model achieves state-of-the-art performance without any special design for local feature alignment or side information. Moreover, we propose a proximity data generation (PDG) module to automatically produce more diverse data for cross-modal training. The PDG module first introduces an automatic generation algorithm based on a text-to-image diffusion model, which generates new text-image pair samples in the proximity space of original ones. Then it combines approximate text generation and feature-level mixup during training to further strengthen the data diversity. The PDG module can largely guarantee the reasonability of the generated samples that are directly used for training without any human inspection for noise rejection. It improves the performance of our model significantly, providing a feasible solution to the data insufficiency problem faced by such fine-grained visual-linguistic tasks. Extensive experiments on two popular datasets of the TBPS task (i.e., CUHK-PEDES and ICFG-PEDES) show that the proposed approach outperforms state-of-the-art approaches evidently, e.g., improving by 3.88%, 4.02%, 2.92% in terms of Top1, Top5, Top10 on CUHK-PEDES. The codes will be available at https://github.com/HCPLab-SYSU/PersonSearch-CTLG
</details>
<details>
<summary>摘要</summary>
文本基于人搜索（TBPS）是一种跨模态检索任务，目的是从图库中检索最佳匹配的人Target。由于两个模态之间存在显著的差异和细化差异，以及缺乏标注数据，这种检索任务非常具有挑战性。大多数现有的方法都是通过引入复杂的网络结构和辅助任务来减轻这些差异，但这些方法通常具有复杂性和困难实现性。在本文中，我们提出了一种简单 yet effective的双Transformer模型，用于实现文本基于人搜索。我们通过利用一种困难性感知的对比学习策略，使我们的模型在无需特殊的本地特征对齐或副作用信息的情况下达到了状态艺术的表现。此外，我们还提出了一种 proximity data generation（PDG）模块，用于自动生成更多的跨模态数据。PDG模块首先通过基于文本到图像扩散模型的自动生成算法，生成了新的文本-图像对amples在原始对amples的邻近空间中。然后，它通过在训练时 combining approximate text generation和特征级混合来进一步增强数据多样性。PDG模块可以保证生成的样本的合理性，不需要人工检查噪声。这使得我们的模型表现得更好，提供了一种可行的解决方案 для跨模态任务中的数据不足问题。我们在两个流行的TBPS任务（i.e., CUHK-PEDES和ICFG-PEDES）上进行了广泛的实验，结果显示，我们的方法明显超越了现有的方法，例如，在CUHK-PEDES上提高了Top1、Top5、Top10的表现，升准3.88%、4.02%、2.92%。代码将在https://github.com/HCPLab-SYSU/PersonSearch-CTLG上提供。
</details></li>
</ul>
<hr>
<h2 id="Spiking-NeRF-Representing-the-Real-World-Geometry-by-a-Discontinuous-Representation"><a href="#Spiking-NeRF-Representing-the-Real-World-Geometry-by-a-Discontinuous-Representation" class="headerlink" title="Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation"></a>Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09077">http://arxiv.org/abs/2311.09077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanfeng Liao, Qian Zheng, Yan Liu, Gang Pan</li>
<li>for: 提高 NeRF 方法的准确性，使其能够更好地捕捉物体的 geometry 和光学特性。</li>
<li>methods: 使用 spiking neuron 和 hybrid ANN-SNN 框架建立不连续的density field，以 faithful 地表示物体的geometry。</li>
<li>results: 实现了 SOTA 性能，并提供了数值关系 между spiking neuron 参数和理论准确性，以便进一步改进方法。<details>
<summary>Abstract</summary>
A crucial reason for the success of existing NeRF-based methods is to build a neural density field for the geometry representation via multiple perceptron layers (MLPs). MLPs are continuous functions, however, real geometry or density field is frequently discontinuous at the interface between the air and the surface. Such a contrary brings the problem of unfaithful geometry representation. To this end, this paper proposes spiking NeRF, which leverages spiking neuron and a hybrid Artificial Neural Network (ANN)-Spiking Neural Network (SNN) framework to build a discontinuous density field for faithful geometry representation. Specifically, we first demonstrate the reason why continuous density fields will bring inaccuracy. Then, we propose to use the spiking neurons to build a discontinuous density field. We conduct comprehensive analysis for the problem of existing spiking neuron models and then provide the numerical relationship between the parameter of spiking neuron and the theoretical accuracy of geometry, Based on this, we propose a bounded spiking neuron to build the discontinuous density field. Our results achieve SOTA performance. Our code and data will be released to the public.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT一个关键的原因导致现有的NeRF方法成功是通过多层感知核（MLP）建立神经density场来表示geometry。但是，实际的geometry或density场经常在空气和表面之间存在缺口，这会导致不准确的geometry表示。为解决这问题，本文提出了脊动NeRF，它利用脊动神经和混合人工神经网络（ANN）-脊动神经网络（SNN）框架来建立不连续的density场，以准确地表示geometry。 Specifically, we first demonstrate why continuous density fields will bring inaccuracy. Then, we propose to use spiking neurons to build a discontinuous density field. We conduct comprehensive analysis for the problem of existing spiking neuron models and then provide the numerical relationship between the parameter of spiking neuron and the theoretical accuracy of geometry. Based on this, we propose a bounded spiking neuron to build the discontinuous density field. Our results achieve SOTA performance. Our code and data will be released to the public.Note: SOTA stands for "State of the Art" in English, which means the current best performance in a particular field or task.
</details></li>
</ul>
<hr>
<h2 id="Imagine-the-Unseen-World-A-Benchmark-for-Systematic-Generalization-in-Visual-World-Models"><a href="#Imagine-the-Unseen-World-A-Benchmark-for-Systematic-Generalization-in-Visual-World-Models" class="headerlink" title="Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models"></a>Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09064">http://arxiv.org/abs/2311.09064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeongbin Kim, Gautam Singh, Junyeong Park, Caglar Gulcehre, Sungjin Ahn</li>
<li>for: 本文旨在提出一个新的基准测试系统，用于评估机器学习模型在视觉领域中的系统性协成能力。</li>
<li>methods: 本文使用了一种新的基准测试系统，称为视觉协成能力测试 benchmark (SVIB)，以评估模型在一种受控的世界动力下的一步图像转换能力。</li>
<li>results: 经过对多种基线模型的评估，本文发现现有的模型在系统性视觉协成能力方面存在一定的限制，并提出了一些可能的改进方向。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Systematic compositionality, or the ability to adapt to novel situations by creating a mental model of the world using reusable pieces of knowledge, remains a significant challenge in machine learning. While there has been considerable progress in the language domain, efforts towards systematic visual imagination, or envisioning the dynamical implications of a visual observation, are in their infancy. We introduce the Systematic Visual Imagination Benchmark (SVIB), the first benchmark designed to address this problem head-on. SVIB offers a novel framework for a minimal world modeling problem, where models are evaluated based on their ability to generate one-step image-to-image transformations under a latent world dynamics. The framework provides benefits such as the possibility to jointly optimize for systematic perception and imagination, a range of difficulty levels, and the ability to control the fraction of possible factor combinations used during training. We provide a comprehensive evaluation of various baseline models on SVIB, offering insight into the current state-of-the-art in systematic visual imagination. We hope that this benchmark will help advance visual systematic compositionality.
</details>
<details>
<summary>摘要</summary>
系统性的组合性，或者在新的情况下适应性的创建一个世界模型使用可重用的知识，仍然是机器学习领域的主要挑战。虽然在语言领域有了很大的进步，但对于视觉想象的系统atic imagination，尚未有充分的尝试。我们介绍了系统atic Visual Imagination Benchmark (SVIB)，第一个专门解决这个问题的benchmark。SVIB提供了一个新的世界模型设计问题，其中模型被评估根据它们在一个latent世界动力学下生成一步图像到图像变换的能力。这个框架具有许多优点，如同时优化系统atic perception和想象、多种难度水平和在训练中控制可能的因素组合的使用。我们对多种基eline模型在SVIB上进行了全面的评估，提供了现状的概况，并希望这个benchmark能够推动视觉系统atic compositionality的进步。
</details></li>
</ul>
<hr>
<h2 id="Self-Annotated-3D-Geometric-Learning-for-Smeared-Points-Removal"><a href="#Self-Annotated-3D-Geometric-Learning-for-Smeared-Points-Removal" class="headerlink" title="Self-Annotated 3D Geometric Learning for Smeared Points Removal"></a>Self-Annotated 3D Geometric Learning for Smeared Points Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09029">http://arxiv.org/abs/2311.09029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaowei Wang, Daniel Morris</li>
<li>for: 本研究旨在提高顾客级束缚精密深度感知器的准确性和质量，并解决雷达点精灵抽象的问题。</li>
<li>methods: 我们提出了一种完全自我标注的方法，利用多视角 geometric evidence 自动检测和标注精灵点和有效点。</li>
<li>results: 我们的方法在实验和减少研究中表现出色，超过了传统的滤波器和其他自我标注方法。In simpler English, the paper aims to improve the accuracy and quality of consumer-level depth sensors and solve the problem of “smeared points” (points that are not on any 3D surface and can cause errors in depth maps). The proposed method uses fully self-annotated training data and relies on 3D geometric evidence from multiple perspectives to detect and remove smeared points. Experimental results show that the method outperforms traditional filters and other self-annotated methods.<details>
<summary>Abstract</summary>
There has been significant progress in improving the accuracy and quality of consumer-level dense depth sensors. Nevertheless, there remains a common depth pixel artifact which we call smeared points. These are points not on any 3D surface and typically occur as interpolations between foreground and background objects. As they cause fictitious surfaces, these points have the potential to harm applications dependent on the depth maps. Statistical outlier removal methods fare poorly in removing these points as they tend also to remove actual surface points. Trained network-based point removal faces difficulty in obtaining sufficient annotated data. To address this, we propose a fully self-annotated method to train a smeared point removal classifier. Our approach relies on gathering 3D geometric evidence from multiple perspectives to automatically detect and annotate smeared points and valid points. To validate the effectiveness of our method, we present a new benchmark dataset: the Real Azure-Kinect dataset. Experimental results and ablation studies show that our method outperforms traditional filters and other self-annotated methods. Our work is publicly available at https://github.com/wangmiaowei/wacv2024_smearedremover.git.
</details>
<details>
<summary>摘要</summary>
“随着对消费者级数 dense depth sensor 的改进，有所进步。然而，还有一个常见的深度像素错误，我们称之为“扩散点”。这些点不在任何三维表面上，通常发生在前景和背景物体之间的插值。由于这些点会创建虚拟表面，这些点有可能伤害对深度地图依赖的应用。统计方法对这些点进行排除是不具有效果的，因为它们也可能会 removes 真正的表面点。训练网络基于的方法也难以获得足够的标注数据。为解决这个问题，我们提出了一个完全自我标注的方法，用于训练深度扩散点移除分类器。我们的方法基于从多个角度收集3D几何证据，以自动检测和标注深度扩散点和有效点。为 validate 我们的方法，我们提供了一个新的库 benchmark 数据集：Real Azure-Kinect 数据集。实验结果和删除研究显示，我们的方法比传统范例和其他自我标注方法高效。我们的工作公开在 GitHub 上，请参考 https://github.com/wangmiaowei/wacv2024_smearedremover.git。”
</details></li>
</ul>
<hr>
<h2 id="Fast-Certification-of-Vision-Language-Models-Using-Incremental-Randomized-Smoothing"><a href="#Fast-Certification-of-Vision-Language-Models-Using-Incremental-Randomized-Smoothing" class="headerlink" title="Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing"></a>Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09024">http://arxiv.org/abs/2311.09024</a></li>
<li>repo_url: None</li>
<li>paper_authors: A K Nirala, A Joshi, C Hegde, S Sarkar</li>
<li>for: 这个论文旨在提出一种快速的开放词汇识别模型认证方法，以确保这些模型在实际应用中的可靠性。</li>
<li>methods: 这种认证方法基于随机缓和技术，使用基于训练集的Certified CLIP classifier来快速认证novel prompts。</li>
<li>results: 实验结果表明，OVC可以快速认证开放词汇模型，并且可以在CIFAR-10和ImageNet测试 datasets上达到比较高的识别率。<details>
<summary>Abstract</summary>
A key benefit of deep vision-language models such as CLIP is that they enable zero-shot open vocabulary classification; the user has the ability to define novel class labels via natural language prompts at inference time. However, while CLIP-based zero-shot classifiers have demonstrated competitive performance across a range of domain shifts, they remain highly vulnerable to adversarial attacks. Therefore, ensuring the robustness of such models is crucial for their reliable deployment in the wild.   In this work, we introduce Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques. Given a base "training" set of prompts and their corresponding certified CLIP classifiers, OVC relies on the observation that a classifier with a novel prompt can be viewed as a perturbed version of nearby classifiers in the base training set. Therefore, OVC can rapidly certify the novel classifier using a variation of incremental randomized smoothing. By using a caching trick, we achieve approximately two orders of magnitude acceleration in the certification process for novel prompts. To achieve further (heuristic) speedups, OVC approximates the embedding space at a given input using a multivariate normal distribution bypassing the need for sampling via forward passes through the vision backbone. We demonstrate the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets.
</details>
<details>
<summary>摘要</summary>
CLIP 深度视力语言模型具有零shot开 vocabulary 分类的能力，即在运行时通过自然语言提示来定义新的分类标签。然而， CLIP 基于的零shot 分类器对于攻击而言是极为易受攻击的。因此，为了可靠地部署这些模型，确保其 Robustness 是非常重要的。在这项工作中，我们介绍了一种叫做 Open Vocabulary Certification (OVC) 的快速证明方法，用于验证开 vocabulary 模型如 CLIP。OVC 基于的观察是，一个新的提示可以视为 nearby 的基础训练集中的类ifier 的噪声版本。因此，OVC 可以快速地证明这个新的类ifier 使用随机噪声技术。使用缓存技巧，我们实现了约两个数量级的加速。此外，OVC 使用一种快速的方法来 Approximate 输入空间的 embedding 空间，从而缩短证明过程。我们通过对多个视力语言背景进行实验评估，证明了 OVC 的有效性。
</details></li>
</ul>
<hr>
<h2 id="Incremental-Object-Based-Novelty-Detection-with-Feedback-Loop"><a href="#Incremental-Object-Based-Novelty-Detection-with-Feedback-Loop" class="headerlink" title="Incremental Object-Based Novelty Detection with Feedback Loop"></a>Incremental Object-Based Novelty Detection with Feedback Loop</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09004">http://arxiv.org/abs/2311.09004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Caldarella, Elisa Ricci, Rahaf Aljundi</li>
<li>for: 本研究旨在提高对象检测模型的不可预测对象检测能力，以避免在实际应用中可能存在危害的行为，如自驾车或自主机器人中使用的对象检测模型。</li>
<li>methods: 本研究提出了一种基于人工反馈的对象新型检测方法，假设可以在预测输出中请求人工反馈，并在反馈可用时进行不间断的改进。为解决这种新的对象检测问题，我们提出了一个轻量级的ND模块，附加在已经训练的对象检测模型之上，并通过反馈循环进行不断更新。</li>
<li>results: 我们的实验表明，我们的ND方法可以增强对象检测模型的Robustness，并成功地收集和 incorporate 人工反馈。我们还提出了一个新的评价指标，用于评价对象检测模型的新型检测能力，并进行了广泛的比较试验，以证明我们的ND方法的效果。<details>
<summary>Abstract</summary>
Object-based Novelty Detection (ND) aims to identify unknown objects that do not belong to classes seen during training by an object detection model. The task is particularly crucial in real-world applications, as it allows to avoid potentially harmful behaviours, e.g. as in the case of object detection models adopted in a self-driving car or in an autonomous robot. Traditional approaches to ND focus on one time offline post processing of the pretrained object detection output, leaving no possibility to improve the model robustness after training and discarding the abundant amount of out-of-distribution data encountered during deployment.   In this work, we propose a novel framework for object-based ND, assuming that human feedback can be requested on the predicted output and later incorporated to refine the ND model without negatively affecting the main object detection performance. This refinement operation is repeated whenever new feedback is available. To tackle this new formulation of the problem for object detection, we propose a lightweight ND module attached on top of a pre-trained object detection model, which is incrementally updated through a feedback loop. We also propose a new benchmark to evaluate methods on this new setting and test extensively our ND approach against baselines, showing increased robustness and a successful incorporation of the received feedback.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a novel framework for object-based ND that assumes human feedback can be requested on the predicted output and later incorporated to refine the ND model without negatively affecting the main object detection performance. This refinement operation is repeated whenever new feedback is available. To tackle this new formulation of the problem for object detection, we propose a lightweight ND module attached on top of a pre-trained object detection model, which is incrementally updated through a feedback loop. We also propose a new benchmark to evaluate methods on this new setting and test our ND approach extensively against baselines, showing increased robustness and a successful incorporation of the received feedback.
</details></li>
</ul>
<hr>
<h2 id="Simple-but-Effective-Unsupervised-Classification-for-Specified-Domain-Images-A-Case-Study-on-Fungi-Images"><a href="#Simple-but-Effective-Unsupervised-Classification-for-Specified-Domain-Images-A-Case-Study-on-Fungi-Images" class="headerlink" title="Simple but Effective Unsupervised Classification for Specified Domain Images: A Case Study on Fungi Images"></a>Simple but Effective Unsupervised Classification for Specified Domain Images: A Case Study on Fungi Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08995">http://arxiv.org/abs/2311.08995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaocong liu, Fa Zhang, Lin Cheng, Huanxi Deng, Xiaoyan Yang, Zhenyu Zhang, Chichun Zhou</li>
<li>for: 特别适用于需要专业知识的域面图像分类任务，解决高质量标注数据的缺乏问题。</li>
<li>methods: 使用自动Feature extraction方法，并利用多种 clustering 算法投票来实现无监督分类。</li>
<li>results: 在 fungal 图像数据集上达到 94.1% 和 96.7% 的分类精度，比超级vised方法高。这种无监督分类方法可以减少依赖于预先标注数据，提供closed-loop数据分类。<details>
<summary>Abstract</summary>
High-quality labeled datasets are essential for deep learning. Traditional manual annotation methods are not only costly and inefficient but also pose challenges in specialized domains where expert knowledge is needed. Self-supervised methods, despite leveraging unlabeled data for feature extraction, still require hundreds or thousands of labeled instances to guide the model for effective specialized image classification. Current unsupervised learning methods offer automatic classification without prior annotation but often compromise on accuracy. As a result, efficiently procuring high-quality labeled datasets remains a pressing challenge for specialized domain images devoid of annotated data. Addressing this, an unsupervised classification method with three key ideas is introduced: 1) dual-step feature dimensionality reduction using a pre-trained model and manifold learning, 2) a voting mechanism from multiple clustering algorithms, and 3) post-hoc instead of prior manual annotation. This approach outperforms supervised methods in classification accuracy, as demonstrated with fungal image data, achieving 94.1% and 96.7% on public and private datasets respectively. The proposed unsupervised classification method reduces dependency on pre-annotated datasets, enabling a closed-loop for data classification. The simplicity and ease of use of this method will also bring convenience to researchers in various fields in building datasets, promoting AI applications for images in specialized domains.
</details>
<details>
<summary>摘要</summary>
高品质标注数据是深度学习的关键。传统的手动标注方法不仅成本高、效率低，还存在专业领域中的知识问题。无监督方法，尽管利用无标注数据进行特征提取，仍需要数百或千个标注实例来引导模型以实现特殊领域图像分类。当前无监督学习方法可以自动分类而无需先前的标注，但通常会 compromise 准确性。因此，得到高品质标注数据仍然是特殊领域图像无标注数据的Pressing challenge。为解决这个问题，我们提出了一种无监督分类方法，包括三个关键想法：1）使用预训练模型和拟合学习来实现双步特征维度减少，2）多种 clustering 算法的投票机制，和3）post-hoc 而不是先前的手动标注。这种方法在分类准确率方面超过了supervised方法，如 demonstrated 在菌类图像数据上，实现了94.1%和96.7%的公共和私人数据集分类率。我们的无监督分类方法减少了对前置标注数据的依赖，使得数据分类成为closed-loop。这种简单易用的方法会将研究人员在多个领域建立数据集，推动人工智能应用于特殊领域图像。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-approaches-based-on-optimal-transport-and-convex-analysis-for-inverse-problems-in-imaging"><a href="#Unsupervised-approaches-based-on-optimal-transport-and-convex-analysis-for-inverse-problems-in-imaging" class="headerlink" title="Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging"></a>Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08972">http://arxiv.org/abs/2311.08972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcello Carioni, Subhadip Mukherjee, Hong Ye Tan, Junqi Tang</li>
<li>for: This paper focuses on theoretically principled unsupervised learning schemes for solving imaging inverse problems, with a particular focus on methods rooted in optimal transport and convex analysis.</li>
<li>methods: The paper reviews optimal transport-based unsupervised approaches, learned adversarial regularization methods, provably convergent learned optimization algorithms, and plug-and-play algorithms for imaging problems.</li>
<li>results: The paper provides an overview of the key mathematical results that underlie the methods reviewed in the chapter to keep the discussion self-contained.<details>
<summary>Abstract</summary>
Unsupervised deep learning approaches have recently become one of the crucial research areas in imaging owing to their ability to learn expressive and powerful reconstruction operators even when paired high-quality training data is scarcely available. In this chapter, we review theoretically principled unsupervised learning schemes for solving imaging inverse problems, with a particular focus on methods rooted in optimal transport and convex analysis. We begin by reviewing the optimal transport-based unsupervised approaches such as the cycle-consistency-based models and learned adversarial regularization methods, which have clear probabilistic interpretations. Subsequently, we give an overview of a recent line of works on provably convergent learned optimization algorithms applied to accelerate the solution of imaging inverse problems, alongside their dedicated unsupervised training schemes. We also survey a number of provably convergent plug-and-play algorithms (based on gradient-step deep denoisers), which are among the most important and widely applied unsupervised approaches for imaging problems. At the end of this survey, we provide an overview of a few related unsupervised learning frameworks that complement our focused schemes. Together with a detailed survey, we provide an overview of the key mathematical results that underlie the methods reviewed in the chapter to keep our discussion self-contained.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<</SYS>>无监督深度学习方法在媒体领域取得了重要突破，尤其是在数据质量较高的时候，它们能够学习表达力强的重建运算符。在这章中，我们将评论理论上正确的无监督学习方案，用于解决媒体领域的反向问题，特别是基于最优运输和凸分析的方法。我们首先介绍循环一致性基于的模型和学习抑制方法，这些方法具有明确的概率解释。接着，我们将讲解最近一些可靠地训练无监督算法，以加速媒体领域的反向问题解决。此外，我们还介绍了一些可靠地插入执行的无监督算法（基于梯度步深排除器），它们是媒体领域中最重要和最广泛应用的无监督方法。 finally，我们将介绍一些与我们关注的无监督学习框架，以及这些方法的关键数学结论，以便保持我们的讨论自 contenido。
</details></li>
</ul>
<hr>
<h2 id="A-Spectral-Diffusion-Prior-for-Hyperspectral-Image-Super-Resolution"><a href="#A-Spectral-Diffusion-Prior-for-Hyperspectral-Image-Super-Resolution" class="headerlink" title="A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution"></a>A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08955">http://arxiv.org/abs/2311.08955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianjun Liu, Zebin Wu, Liang Xiao</li>
<li>for:  fusion-based hyperspectral image (HSI) super-resolution</li>
<li>methods: spectral diffusion prior, maximum a posteriori, Adam optimization</li>
<li>results: effective in producing high-spatial-resolution HSI, demonstrated on both synthetic and real datasetsHere’s the simplified Chinese text:</li>
<li>for: 高分辨率多spectral影像（HSI）超解析</li>
<li>methods:  spectral diffusion prior, maximum a posteriori, Adam优化</li>
<li>results: 高效地生成高分辨率HSI, 在synthetic和实际数据上进行了实验<details>
<summary>Abstract</summary>
Fusion-based hyperspectral image (HSI) super-resolution aims to produce a high-spatial-resolution HSI by fusing a low-spatial-resolution HSI and a high-spatial-resolution multispectral image. Such a HSI super-resolution process can be modeled as an inverse problem, where the prior knowledge is essential for obtaining the desired solution. Motivated by the success of diffusion models, we propose a novel spectral diffusion prior for fusion-based HSI super-resolution. Specifically, we first investigate the spectrum generation problem and design a spectral diffusion model to model the spectral data distribution. Then, in the framework of maximum a posteriori, we keep the transition information between every two neighboring states during the reverse generative process, and thereby embed the knowledge of trained spectral diffusion model into the fusion problem in the form of a regularization term. At last, we treat each generation step of the final optimization problem as its subproblem, and employ the Adam to solve these subproblems in a reverse sequence. Experimental results conducted on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. The code of the proposed approach will be available on https://github.com/liuofficial/SDP.
</details>
<details>
<summary>摘要</summary>
融合基于快照影像（HSI）超分辨率目标是生成高空间分辨率HSI，通过融合低空间分辨率HSI和高空间分辨率多spectral影像。这种HSI超分辨率过程可以表示为一个逆问题，其中假设知识是获得所需解决方案的关键。鼓动 diffusion模型的成功，我们提出了一种新的 spectral diffusion prior для融合基于HSI超分辨率。specifically，我们首先调查spectrum生成问题，并设计了一种spectral diffusion模型来模拟spectral数据分布。然后，在maximum a posteriori框架中，我们保留了每两个邻居状态之间的过渡信息，并将这些知识嵌入到融合问题中，以形式化一个正则化项。最后，我们对每个生成步骤的最终优化问题进行分解，并使用Adam算法解决这些子问题。实验结果表明，我们提出的方法在synthetic和实际数据集上具有效果。code的github地址为https://github.com/liuofficial/SDP.
</details></li>
</ul>
<hr>
<h2 id="Automated-Volume-Corrected-Mitotic-Index-Calculation-Through-Annotation-Free-Deep-Learning-using-Immunohistochemistry-as-Reference-Standard"><a href="#Automated-Volume-Corrected-Mitotic-Index-Calculation-Through-Annotation-Free-Deep-Learning-using-Immunohistochemistry-as-Reference-Standard" class="headerlink" title="Automated Volume Corrected Mitotic Index Calculation Through Annotation-Free Deep Learning using Immunohistochemistry as Reference Standard"></a>Automated Volume Corrected Mitotic Index Calculation Through Annotation-Free Deep Learning using Immunohistochemistry as Reference Standard</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08949">http://arxiv.org/abs/2311.08949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Ammeling, Moritz Hecker, Jonathan Ganz, Taryn A. Donovan, Christof A. Bertram, Katharina Breininger, Marc Aubreville</li>
<li>for:  This paper is written for assessing the prognostic value of invasive breast carcinomas using a deep learning-based approach.</li>
<li>methods:  The paper uses a deep learning pipeline solely trained with an annotation-free, immunohistochemistry-based approach to estimate epithelial segmentation in canine breast carcinomas.</li>
<li>results:  The deep learning-based pipeline shows expert-level performance, providing time efficiency and reproducibility, compared to the manually annotated M&#x2F;V-Index.<details>
<summary>Abstract</summary>
The volume-corrected mitotic index (M/V-Index) was shown to provide prognostic value in invasive breast carcinomas. However, despite its prognostic significance, it is not established as the standard method for assessing aggressive biological behaviour, due to the high additional workload associated with determining the epithelial proportion. In this work, we show that using a deep learning pipeline solely trained with an annotation-free, immunohistochemistry-based approach, provides accurate estimations of epithelial segmentation in canine breast carcinomas. We compare our automatic framework with the manually annotated M/V-Index in a study with three board-certified pathologists. Our results indicate that the deep learning-based pipeline shows expert-level performance, while providing time efficiency and reproducibility.
</details>
<details>
<summary>摘要</summary>
“对入侵性乳癌中的肉眼癌指数（M/V-Index）的评估，有过往的研究显示其具有预后价值。然而，尽管其预后意义，但它并未被视为标准的评估具有攻击性生物行为的方法，因为需要额外的努力来决定胞质含量。在这个工作中，我们展示了一个使用深度学习管线仅以无标注、免疫抗体技术为基础的架构，可以实时和可重复地估算乳癌组织中的胞质分布。我们与三位美国医学会认证的病理学家进行比较，结果显示，我们的深度学习架构具有专家水准的表现，同时提供时间效益和可重复性。”Note that the translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Confident-Naturalness-Explanation-CNE-A-Framework-to-Explain-and-Assess-Patterns-Forming-Naturalness"><a href="#Confident-Naturalness-Explanation-CNE-A-Framework-to-Explain-and-Assess-Patterns-Forming-Naturalness" class="headerlink" title="Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness"></a>Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08936">http://arxiv.org/abs/2311.08936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Emam, Mohamed Farag, Ribana Roscher</li>
<li>for:  This paper aims to improve the understanding and mapping of naturalness within protected natural areas using machine learning and explainability techniques.</li>
<li>methods:  The proposed Confident Naturalness Explanation (CNE) framework combines explainable machine learning and uncertainty quantification to assess and explain naturalness, using a new quantitative metric and uncertainty-aware segmentation masks.</li>
<li>results:  The proposed CNE framework is demonstrated to be effective in a study site in Fennoscandia using two open-source satellite datasets, providing confident and objective explanations of naturalness.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是使用机器学习和可解释技术来提高保护自然区域中自然性的理解和地图。</li>
<li>methods: 提议的Confident Naturalness Explanation（CNE）框架结合可解释机器学习和不确定量化来评估和解释自然性，使用新的量化度量和不确定度映射。</li>
<li>results: 在芬兰地区使用两个开源卫星数据集，通过应用CNE框架，实现了对自然性的可靠和客观解释。<details>
<summary>Abstract</summary>
Protected natural areas are regions that have been minimally affected by human activities such as urbanization, agriculture, and other human interventions. To better understand and map the naturalness of these areas, machine learning models can be used to analyze satellite imagery. Specifically, explainable machine learning methods show promise in uncovering patterns that contribute to the concept of naturalness within these protected environments. Additionally, addressing the uncertainty inherent in machine learning models is crucial for a comprehensive understanding of this concept. However, existing approaches have limitations. They either fail to provide explanations that are both valid and objective or struggle to offer a quantitative metric that accurately measures the contribution of specific patterns to naturalness, along with the associated confidence. In this paper, we propose a novel framework called the Confident Naturalness Explanation (CNE) framework. This framework combines explainable machine learning and uncertainty quantification to assess and explain naturalness. We introduce a new quantitative metric that describes the confident contribution of patterns to the concept of naturalness. Furthermore, we generate an uncertainty-aware segmentation mask for each input sample, highlighting areas where the model lacks knowledge. To demonstrate the effectiveness of our framework, we apply it to a study site in Fennoscandia using two open-source satellite datasets.
</details>
<details>
<summary>摘要</summary>
保护的自然区域是人类活动影响的最小化区域，如城市化、农业等。为了更好地理解和映射这些区域的自然性，机器学习模型可以使用卫星图像进行分析。特别是使用可解释机器学习方法可以揭示保护区域中自然性的特征。然而，现有的方法有限制。它们可能无法提供有效和客观的解释，或者困难提供准确度量自然性的贡献和相关信息。在这篇论文中，我们提出了一种新的框架，即可靠自然性解释（CNE）框架。这个框架结合可解释机器学习和不确定量化来评估和解释自然性。我们还提出了一个新的量化度量，用于描述模型对自然性的可靠贡献。此外，我们生成了每个输入样本的不确定性感知分割图，以标识模型对具体区域的不确定性。为了证明我们的框架的效果，我们对芬兰北部的一个研究区使用了两个开源卫星数据集进行应用。
</details></li>
</ul>
<hr>
<h2 id="Structural-Based-Uncertainty-in-Deep-Learning-Across-Anatomical-Scales-Analysis-in-White-Matter-Lesion-Segmentation"><a href="#Structural-Based-Uncertainty-in-Deep-Learning-Across-Anatomical-Scales-Analysis-in-White-Matter-Lesion-Segmentation" class="headerlink" title="Structural-Based Uncertainty in Deep Learning Across Anatomical Scales: Analysis in White Matter Lesion Segmentation"></a>Structural-Based Uncertainty in Deep Learning Across Anatomical Scales: Analysis in White Matter Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08931">http://arxiv.org/abs/2311.08931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/medical-image-analysis-laboratory/ms_wml_uncs">https://github.com/medical-image-analysis-laboratory/ms_wml_uncs</a></li>
<li>paper_authors: Nataliia Molchanova, Vatsal Raina, Andrey Malinin, Francesco La Rosa, Adrien Depeursinge, Mark Gales, Cristina Granziera, Henning Muller, Mara Graziani, Meritxell Bach Cuadra</li>
<li>for: 这个论文探讨了自动深度学习（DL）工具的可靠性量化（UQ）在多发性脑梗液病人（MS）的磁共振成像（MRI）扫描中的白 matter损伤（WML）分 segmentation任务中的作用。</li>
<li>methods: 我们的研究主要集中在两个主要的不确定性问题上：首先，我们认为一个好的不确定性度量应该指示预测有高度不确定性的值。其次，我们 investigate了不确定性在不同的 анаatomical scale（ voxel、 lesion 或 patient）之间的关系。我们认为不确定性在每个缩放级别都与特定类型的错误有关。</li>
<li>results: 我们的研究结果表明，我们提出的方法可以更好地捕捉模型错误在 lesion 和 patient 缩放级别上，比 tradicional voxel-scale uncertainty 值的平均值。我们在一个多中心 MRI 数据集上进行了172名病人的研究，并提供了 UQ 协议代码在 GitHub 上（<a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs%EF%BC%89%E3%80%82">https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs）。</a><details>
<summary>Abstract</summary>
This paper explores uncertainty quantification (UQ) as an indicator of the trustworthiness of automated deep-learning (DL) tools in the context of white matter lesion (WML) segmentation from magnetic resonance imaging (MRI) scans of multiple sclerosis (MS) patients. Our study focuses on two principal aspects of uncertainty in structured output segmentation tasks. Firstly, we postulate that a good uncertainty measure should indicate predictions likely to be incorrect with high uncertainty values. Second, we investigate the merit of quantifying uncertainty at different anatomical scales (voxel, lesion, or patient). We hypothesize that uncertainty at each scale is related to specific types of errors. Our study aims to confirm this relationship by conducting separate analyses for in-domain and out-of-domain settings. Our primary methodological contributions are (i) the development of novel measures for quantifying uncertainty at lesion and patient scales, derived from structural prediction discrepancies, and (ii) the extension of an error retention curve analysis framework to facilitate the evaluation of UQ performance at both lesion and patient scales. The results from a multi-centric MRI dataset of 172 patients demonstrate that our proposed measures more effectively capture model errors at the lesion and patient scales compared to measures that average voxel-scale uncertainty values. We provide the UQ protocols code at https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>We propose that a good uncertainty measure should indicate predictions that are likely to be incorrect with high uncertainty values.2. We investigate the merit of quantifying uncertainty at different anatomical scales (voxel, lesion, or patient). We hypothesize that uncertainty at each scale is related to specific types of errors.Our study aims to confirm this relationship by conducting separate analyses for in-domain and out-of-domain settings. Our primary methodological contributions are:1. The development of novel measures for quantifying uncertainty at lesion and patient scales, derived from structural prediction discrepancies.2. The extension of an error retention curve analysis framework to facilitate the evaluation of UQ performance at both lesion and patient scales.The results from a multi-centric MRI dataset of 172 patients demonstrate that our proposed measures more effectively capture model errors at the lesion and patient scales compared to measures that average voxel-scale uncertainty values. The UQ protocols code is available at <a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs">https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs</a>.</details></li>
</ol>
<hr>
<h2 id="Progressive-Feedback-Enhanced-Transformer-for-Image-Forgery-Localization"><a href="#Progressive-Feedback-Enhanced-Transformer-for-Image-Forgery-Localization" class="headerlink" title="Progressive Feedback-Enhanced Transformer for Image Forgery Localization"></a>Progressive Feedback-Enhanced Transformer for Image Forgery Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08910">http://arxiv.org/abs/2311.08910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Zhu, Gang Cao, Xianglin Huang</li>
<li>for: 本研究旨在提出一种 Progressive FeedbACk-enhanced Transformer (ProFact) 网络，用于提高图像forge localization的精度和可靠性。</li>
<li>methods: 该网络使用了一个初始分支网络生成的粗略定位图，并将其FeedbACk到早期的 transformer 嵌入层以增强正面特征表示，同时抑制干扰因素。此外，还提出了一种 Contextual Spatial Pyramid 模块，用于进一步提高医学特征的涵盖率和分辨率。</li>
<li>results: 对于九个公共的医学检测数据集，我们的提出的定位器表现出色，在泛化能力和Robustness方面都大幅超越了当前状态。<details>
<summary>Abstract</summary>
Blind detection of the forged regions in digital images is an effective authentication means to counter the malicious use of local image editing techniques. Existing encoder-decoder forensic networks overlook the fact that detecting complex and subtle tampered regions typically requires more feedback information. In this paper, we propose a Progressive FeedbACk-enhanced Transformer (ProFact) network to achieve coarse-to-fine image forgery localization. Specifically, the coarse localization map generated by an initial branch network is adaptively fed back to the early transformer encoder layers for enhancing the representation of positive features while suppressing interference factors. The cascaded transformer network, combined with a contextual spatial pyramid module, is designed to refine discriminative forensic features for improving the forgery localization accuracy and reliability. Furthermore, we present an effective strategy to automatically generate large-scale forged image samples close to real-world forensic scenarios, especially in realistic and coherent processing. Leveraging on such samples, a progressive and cost-effective two-stage training protocol is applied to the ProFact network. The extensive experimental results on nine public forensic datasets show that our proposed localizer greatly outperforms the state-of-the-art on the generalization ability and robustness of image forgery localization. Code will be publicly available at https://github.com/multimediaFor/ProFact.
</details>
<details>
<summary>摘要</summary>
“针对本地图像修改技术的恶意使用，潜意检测数字图像中的假造区域是一种有效的身份验证手段。现有的编oder-解码器审计网络忽视了检测复杂且微妙的假造区域通常需要更多的反馈信息。本文提出了一种Progressive FeedbACk-enhanced Transformer（ProFact）网络，以实现从粗到细图像假造地点检测。特别是，初始分支网络生成的粗略地点映射被适应地feedback到早期的Transformer编码层，以增强正面特征表示，同时抑制干扰因素。另外，我们设计了一个Contextual Spatial Pyramid模块，用于修改审计特征，以提高假造地点检测精度和可靠性。此外，我们还提出了一种有效的自动生成大规模假造图像样本close to real-world审计enario，特别是在realistic和coherent处理中。基于这些样本，我们采用了一种进程ive和cost-effective的两阶段训练 protocole。我们的实验结果表明，我们提出的检测器在通用性和Robustness两个方面都有出色的表现，超过了当前state-of-the-art。代码将在https://github.com/multimediaFor/ProFact中公开。”
</details></li>
</ul>
<hr>
<h2 id="DLAS-An-Exploration-and-Assessment-of-the-Deep-Learning-Acceleration-Stack"><a href="#DLAS-An-Exploration-and-Assessment-of-the-Deep-Learning-Acceleration-Stack" class="headerlink" title="DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack"></a>DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08909">http://arxiv.org/abs/2311.08909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Perry Gibson, José Cano, Elliot J. Crowley, Amos Storkey, Michael O’Boyle</li>
<li>for: 这个论文的目的是提供一个参考框架，帮助机器学习和系统实践者在实现深度学习推进运算时，更好地考虑各个层次的依赖关系。</li>
<li>methods: 这个论文使用了机器学习和系统技术，建立了深度学习加速框架（DLAS），并对DLAS进行了逐层次的干扰研究，以探索各个层次之间的依赖关系。</li>
<li>results: 这个论文的评估结果显示，DLAS的各个层次之间存在许多依赖关系，而且这些关系可以通过干扰DLAS的各个层次来变化。此外，论文还发现了一些实际上的规律，例如压缩技术的加速效益是具体设备依赖的，以及自动调整代码生成可以对最佳化器的选择产生重大影响。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) are extremely computationally demanding, which presents a large barrier to their deployment on resource-constrained devices. Since such devices are where many emerging deep learning applications lie (e.g., drones, vision-based medical technology), significant bodies of work from both the machine learning and systems communities have attempted to provide optimizations to accelerate DNNs. To help unify these two perspectives, in this paper we combine machine learning and systems techniques within the Deep Learning Acceleration Stack (DLAS), and demonstrate how these layers can be tightly dependent on each other with an across-stack perturbation study. We evaluate the impact on accuracy and inference time when varying different parameters of DLAS across two datasets, seven popular DNN architectures, four DNN compression techniques, three algorithmic primitives with sparse and dense variants, untuned and auto-scheduled code generation, and four hardware platforms. Our evaluation highlights how perturbations across DLAS parameters can cause significant variation and across-stack interactions. The highest level observation from our evaluation is that the model size, accuracy, and inference time are not guaranteed to be correlated. Overall we make 13 key observations, including that speedups provided by compression techniques are very hardware dependent, and that compiler auto-tuning can significantly alter what the best algorithm to use for a given configuration is. With DLAS, we aim to provide a reference framework to aid machine learning and systems practitioners in reasoning about the context in which their respective DNN acceleration solutions exist in. With our evaluation strongly motivating the need for co-design, we believe that DLAS can be a valuable concept for exploring the next generation of co-designed accelerated deep learning solutions.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）非常 computationally 需求强大，这使得它们在有限资源的设备上部署变得困难。由于这些设备是许多深度学习应用程序的核心（如无人机、视觉基于医疗技术），因此机器学习和系统社区中的大量工作尝试了加速DNNs。为了统一这两个视角，在这篇论文中我们将机器学习和系统技术融合在一起，并通过各层之间的扰动研究表明了它们之间的互相依赖关系。我们对两个数据集、七种流行的DNN架构、四种DNN压缩技术、三种算法基本primitives的稀热和杂散变体、未调uning和自动生成代码生成、四种硬件平台进行了评估。我们的评估表明，在不同的DAS Parameters中，可以导致显著的变化和层之间的互动。最高层的观察结论是，模型大小、准确率和执行时间之间不一定是相关的。总的来说，我们所得到的13个观察结论，其中一些包括压缩技术在不同硬件平台上提供的加速效果是非常硬件依赖的，并且编译器自动调试可以很大地改变选择最佳算法的配置是否正确。通过DAS，我们希望提供一个参考框架，帮助机器学习和系统专家更好地理解它们的DNN加速解决方案在不同上下文中的运行环境。我们的评估强烈驱动了需要的共设计，我们认为DAS可以成为下一代共设计加速深度学习解决方案的价值概念。
</details></li>
</ul>
<hr>
<h2 id="Robust-Brain-MRI-Image-Classification-with-SIBOW-SVM"><a href="#Robust-Brain-MRI-Image-Classification-with-SIBOW-SVM" class="headerlink" title="Robust Brain MRI Image Classification with SIBOW-SVM"></a>Robust Brain MRI Image Classification with SIBOW-SVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08908">http://arxiv.org/abs/2311.08908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyun Zeng, Hao Helen Zhang<br>for:* The paper aims to develop a novel brain tumor image classification method to improve the accuracy and efficiency of detecting and diagnosing brain tumors.methods:* The proposed method, called SIBOW-SVM, integrates the Bag-of-Features (BoF) model with SIFT feature extraction and weighted Support Vector Machines (wSVMs) to capture hidden image features and differentiate various tumor types.* The method also estimates the probabilities of images belonging to each class, providing high-confidence classification decisions.results:* The SIBOW-SVM method outperforms state-of-the-art methods, including Convolutional Neural Network (CNN), on a public data set of brain tumor MRI images containing four classes: glioma, meningioma, pituitary, and normal.<details>
<summary>Abstract</summary>
The majority of primary Central Nervous System (CNS) tumors in the brain are among the most aggressive diseases affecting humans. Early detection of brain tumor types, whether benign or malignant, glial or non-glial, is critical for cancer prevention and treatment, ultimately improving human life expectancy. Magnetic Resonance Imaging (MRI) stands as the most effective technique to detect brain tumors by generating comprehensive brain images through scans. However, human examination can be error-prone and inefficient due to the complexity, size, and location variability of brain tumors. Recently, automated classification techniques using machine learning (ML) methods, such as Convolutional Neural Network (CNN), have demonstrated significantly higher accuracy than manual screening, while maintaining low computational costs. Nonetheless, deep learning-based image classification methods, including CNN, face challenges in estimating class probabilities without proper model calibration. In this paper, we propose a novel brain tumor image classification method, called SIBOW-SVM, which integrates the Bag-of-Features (BoF) model with SIFT feature extraction and weighted Support Vector Machines (wSVMs). This new approach effectively captures hidden image features, enabling the differentiation of various tumor types and accurate label predictions. Additionally, the SIBOW-SVM is able to estimate the probabilities of images belonging to each class, thereby providing high-confidence classification decisions. We have also developed scalable and parallelable algorithms to facilitate the practical implementation of SIBOW-SVM for massive images. As a benchmark, we apply the SIBOW-SVM to a public data set of brain tumor MRI images containing four classes: glioma, meningioma, pituitary, and normal. Our results show that the new method outperforms state-of-the-art methods, including CNN.
</details>
<details>
<summary>摘要</summary>
主要脑中央神经系统肿瘤的多数是人类最致命的疾病之一。早期检测脑肿瘤类型，无论是肿瘤或非肿瘤， glial 或非 glial，都是防范癌症和治疗的关键，最终提高人类存活时间。磁共振成像（MRI）是识别脑肿瘤的最有效的技术，通过扫描生成全面脑图像。然而，人工检查可能会出现错误和不具有效率，因为脑肿瘤的复杂性、大小和位置变化。现在，自动分类技术使用机器学习（ML）方法，如卷积神经网络（CNN），已经表明了与人工检查相比，有较高的准确率，同时保持低的计算成本。然而，深度学习基于图像分类方法，包括CNN，面临着估计类别概率无法进行正确的模型定制。在这篇论文中，我们提出了一种新的脑肿瘤图像分类方法，called SIBOW-SVM，它将袋子模型（BoF）和SIFT特征提取结合weighted Support Vector Machines（wSVMs）。这种新方法可以有效捕捉隐藏的图像特征，以便区分不同的肿瘤类型并准确地预测标签。此外，SIBOW-SVM还可以估计图像属于哪一类的概率，从而提供高确度的分类决策。我们还开发了可扩展和并行的算法，以便实现SIBOW-SVM的实用应用。作为标准，我们对一个公共的脑肿瘤MRI图像集进行了应用，该集包含四个类别： glioma、meningioma、pituitary和正常。我们的结果显示，新方法在与现状的方法相比，具有更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="AdapterShadow-Adapting-Segment-Anything-Model-for-Shadow-Detection"><a href="#AdapterShadow-Adapting-Segment-Anything-Model-for-Shadow-Detection" class="headerlink" title="AdapterShadow: Adapting Segment Anything Model for Shadow Detection"></a>AdapterShadow: Adapting Segment Anything Model for Shadow Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08891">http://arxiv.org/abs/2311.08891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiping Jie, Hui Zhang</li>
<li>for: 提高阴影检测的精度和效率</li>
<li>methods: 使用可调式适应器与SAM模型结合，并提出了一种新的格子采样方法来自动生成精度点提示</li>
<li>results: 在四个广泛使用的基准数据集上进行了广泛的实验，并证明了我们提出的方法的精度和效率的提高<details>
<summary>Abstract</summary>
Segment anything model (SAM) has shown its spectacular performance in segmenting universal objects, especially when elaborate prompts are provided. However, the drawback of SAM is twofold. On the first hand, it fails to segment specific targets, e.g., shadow images or lesions in medical images. On the other hand, manually specifying prompts is extremely time-consuming. To overcome the problems, we propose AdapterShadow, which adapts SAM model for shadow detection. To adapt SAM for shadow images, trainable adapters are inserted into the frozen image encoder of SAM, since the training of the full SAM model is both time and memory consuming. Moreover, we introduce a novel grid sampling method to generate dense point prompts, which helps to automatically segment shadows without any manual interventions. Extensive experiments are conducted on four widely used benchmark datasets to demonstrate the superior performance of our proposed method. Codes will are publicly available at https://github.com/LeipingJie/AdapterShadow.
</details>
<details>
<summary>摘要</summary>
Segment anything model (SAM) 已经显示出了吸引人的表现，尤其是当提供详细的提示时。然而，SAM的缺点是两重的。一方面，它无法 segment Specific targets, 例如阴影图像或医学图像中的病变。另一方面，手动指定提示是非常时间和内存占用的。为了解决这些问题，我们提议 AdapterShadow，它将SAM模型适应到阴影检测中。为了适应SAM模型 для阴影图像，我们在冻结的图像编码器中插入可学习的适应器。此外，我们还介绍了一种新的网格采样方法，用于生成密集的点提示，以帮助自动 segment 阴影 без任何手动干预。我们在四个常用的标准数据集上进行了广泛的实验，以证明我们提出的方法的优秀性。代码将在 GitHub 上公开。
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Federated-Learning-with-Classifier-Guided-Diffusion-Models"><a href="#One-Shot-Federated-Learning-with-Classifier-Guided-Diffusion-Models" class="headerlink" title="One-Shot Federated Learning with Classifier-Guided Diffusion Models"></a>One-Shot Federated Learning with Classifier-Guided Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08870">http://arxiv.org/abs/2311.08870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhao Yang, Shangchao Su, Bin Li, Xiangyang Xue</li>
<li>for: This paper focuses on exploring the potential of diffusion models in one-shot federated learning (OSFL) to generate high-quality synthetic datasets that can be used to train aggregated models without relying on auxiliary datasets or training generators.</li>
<li>methods: The proposed method, called FedCADO, utilizes guidance from client classifiers to generate data that complies with clients’ distributions and subsequently trains the aggregated model on the server. The method involves targeted optimizations in two aspects: conditionally editing the randomly sampled initial noises and employing the BN statistics from the classifiers to provide detailed guidance during generation.</li>
<li>results: The proposed method effectively handles the heterogeneous client models and the problems of non-IID features or labels, and can generate synthetic datasets that closely resemble the distribution and quality of the original client dataset. The method also avoids privacy leakage risks by not training any generators or transferring any auxiliary information on clients. Experimental results on three large-scale multi-domain image datasets demonstrate that the synthetic datasets generated by FedCADO can assist in surpassing the knowledge limitations of the client samples, resulting in aggregation models that even outperform the performance ceiling of centralized training in some cases.<details>
<summary>Abstract</summary>
One-shot federated learning (OSFL) has gained attention in recent years due to its low communication cost. However, most of the existing methods require auxiliary datasets or training generators, which hinders their practicality in real-world scenarios. In this paper, we explore the novel opportunities that diffusion models bring to OSFL and propose FedCADO, utilizing guidance from client classifiers to generate data that complies with clients' distributions and subsequently training the aggregated model on the server. Specifically, our method involves targeted optimizations in two aspects. On one hand, we conditionally edit the randomly sampled initial noises, embedding them with specified semantics and distributions, resulting in a significant improvement in both the quality and stability of generation. On the other hand, we employ the BN statistics from the classifiers to provide detailed guidance during generation. These tailored optimizations enable us to limitlessly generate datasets, which closely resemble the distribution and quality of the original client dataset. Our method effectively handles the heterogeneous client models and the problems of non-IID features or labels. In terms of privacy protection, our method avoids training any generator or transferring any auxiliary information on clients, eliminating any additional privacy leakage risks. Leveraging the extensive knowledge stored in the pre-trained diffusion model, the synthetic datasets can assist us in surpassing the knowledge limitations of the client samples, resulting in aggregation models that even outperform the performance ceiling of centralized training in some cases, which is convincingly demonstrated in the sufficient quantification and visualization experiments conducted on three large-scale multi-domain image datasets.
</details>
<details>
<summary>摘要</summary>
一种新型的 federated learning 方法，即 One-shot federated learning（OSFL），在最近几年内受到了广泛关注，因为它的通信成本很低。然而，大多数现有的方法都需要附加的 auxillary 数据或训练生成器，这限制了它们在实际场景中的实用性。在这篇论文中，我们探索了 diffusion 模型带来的新机遇，并提出了 FedCADO 方法，通过客户端分类器的指导，在服务器上训练汇集模型。具体来说，我们的方法包括两个方面的优化。一方面，我们通过条件编辑 randomly 采样的初始噪声，使其嵌入特定的 semantics 和分布，从而获得较好的质量和稳定性。另一方面，我们利用客户端的 BN 统计来提供详细的指导，以便在生成过程中进行精细的调整。这些特定的优化使得我们能够无限生成数据，这些数据与客户端的原始数据分布和质量具有很高的相似度。我们的方法可以有效地处理不同客户端模型的 hetroogeneous 特性，以及非 Identical 的特征或标签问题。另外，我们的方法不需要在客户端上训练任何生成器或传输任何附加信息，因此不会增加隐私泄露的风险。通过 diffusion 模型存储的广泛知识，我们可以使用生成的 sintethic 数据进行超越客户端样本的知识限制，实现汇集模型的性能超越中央化训练的性能均衡，这些结果在三个大规模多域图像 dataset 上得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Toulouse-Hyperspectral-Data-Set-a-benchmark-data-set-to-assess-semi-supervised-spectral-representation-learning-and-pixel-wise-classification-techniques"><a href="#Toulouse-Hyperspectral-Data-Set-a-benchmark-data-set-to-assess-semi-supervised-spectral-representation-learning-and-pixel-wise-classification-techniques" class="headerlink" title="Toulouse Hyperspectral Data Set: a benchmark data set to assess semi-supervised spectral representation learning and pixel-wise classification techniques"></a>Toulouse Hyperspectral Data Set: a benchmark data set to assess semi-supervised spectral representation learning and pixel-wise classification techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08863">http://arxiv.org/abs/2311.08863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/romain3ch216/tlse-experiments">https://github.com/romain3ch216/tlse-experiments</a></li>
<li>paper_authors: Romain Thoreau, Laurent Risser, Véronique Achard, Béatrice Berthelot, Xavier Briottet<br>for:The paper aims to provide a new hyperspectral data set for large-scale urban area mapping, addressing the scarcity of annotated data and the limitations of existing data sets.methods:The paper uses semi-supervised and self-supervised techniques, such as Masked Autoencoders, to train machine learning models on the new data set, and evaluates their performance on pixel-wise classification.results:The paper achieves an overall accuracy of 82% and an F1 score of 74% on pixel-wise classification, using a conventional autoencoder combined with a Random Forest classifier. The paper also releases the Toulouse Hyperspectral Data Set and the code for reproducing the experiments.Here is the Chinese translation of the three points:for:论文旨在提供大规模城市区域地图的空中彩色影像，解决现有数据集缺乏标注数据和限制。methods:论文使用 semi-supervised 和 self-supervised 技术，如Masked Autoencoders，在新数据集上训练机器学习模型，并评估其像素级分类性能。results:论文在像素级分类 task 上达到了 82% 的总准确率和 74% 的 F1 分数，使用 convential autoencoder 和 Random Forest 分类器。论文还发布了 Toulouse Hyperspectral Data Set 和 reproduce  эксперименты 的代码。<details>
<summary>Abstract</summary>
Airborne hyperspectral images can be used to map the land cover in large urban areas, thanks to their very high spatial and spectral resolutions on a wide spectral domain. While the spectral dimension of hyperspectral images is highly informative of the chemical composition of the land surface, the use of state-of-the-art machine learning algorithms to map the land cover has been dramatically limited by the availability of training data. To cope with the scarcity of annotations, semi-supervised and self-supervised techniques have lately raised a lot of interest in the community. Yet, the publicly available hyperspectral data sets commonly used to benchmark machine learning models are not totally suited to evaluate their generalization performances due to one or several of the following properties: a limited geographical coverage (which does not reflect the spectral diversity in metropolitan areas), a small number of land cover classes and a lack of appropriate standard train / test splits for semi-supervised and self-supervised learning. Therefore, we release in this paper the Toulouse Hyperspectral Data Set that stands out from other data sets in the above-mentioned respects in order to meet key issues in spectral representation learning and classification over large-scale hyperspectral images with very few labeled pixels. Besides, we discuss and experiment the self-supervised task of Masked Autoencoders and establish a baseline for pixel-wise classification based on a conventional autoencoder combined with a Random Forest classifier achieving 82% overall accuracy and 74% F1 score. The Toulouse Hyperspectral Data Set and our code are publicly available at https://www.toulouse-hyperspectral-data-set.com and https://www.github.com/Romain3Ch216/tlse-experiments, respectively.
</details>
<details>
<summary>摘要</summary>
飞行式干扰спектраль成像可以用于大都市地区的地表覆盖图像，因为它们具有非常高的空间和спектраль分辨率，并且覆盖了广泛的 спектраль频谱。 although the spectral dimension of hyperspectral images is highly informative of the chemical composition of the land surface, the use of state-of-the-art machine learning algorithms to map the land cover has been dramatically limited by the availability of training data. To cope with the scarcity of annotations, semi-supervised and self-supervised techniques have lately raised a lot of interest in the community. However, the publicly available hyperspectral data sets commonly used to benchmark machine learning models are not totally suited to evaluate their generalization performances due to one or several of the following properties: limited geographical coverage (which does not reflect the spectral diversity in metropolitan areas), a small number of land cover classes, and a lack of appropriate standard train / test splits for semi-supervised and self-supervised learning. Therefore, we release in this paper the Toulouse Hyperspectral Data Set, which stands out from other data sets in the above-mentioned respects in order to meet key issues in spectral representation learning and classification over large-scale hyperspectral images with very few labeled pixels. Besides, we discuss and experiment the self-supervised task of Masked Autoencoders and establish a baseline for pixel-wise classification based on a conventional autoencoder combined with a Random Forest classifier, achieving 82% overall accuracy and 74% F1 score. The Toulouse Hyperspectral Data Set and our code are publicly available at https://www.toulouse-hyperspectral-data-set.com and https://www.github.com/Romain3Ch216/tlse-experiments, respectively.
</details></li>
</ul>
<hr>
<h2 id="Data-Augmentations-in-Deep-Weight-Spaces"><a href="#Data-Augmentations-in-Deep-Weight-Spaces" class="headerlink" title="Data Augmentations in Deep Weight Spaces"></a>Data Augmentations in Deep Weight Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08851">http://arxiv.org/abs/2311.08851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviv Shamsian, David W. Zhang, Aviv Navon, Yan Zhang, Miltiadis Kofinas, Idan Achituve, Riccardo Valperga, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, Ethan Fetaya, Gal Chechik, Haggai Maron</li>
<li>for: 本研究旨在解决深度神经网络学习在权重空间中的难题，即生成大量数据来避免过拟合。</li>
<li>methods: 本文提出了一种基于混合方法的数据增强技术，以生成新的数据示例，不需要额外训练输入权重空间元素。</li>
<li>results: 对于现有的benchmark和新生成的benchmark，我们评估了不同数据增强技术的性能，并发现了一种基于混合方法的新数据增强方案可以提高学习效果。<details>
<summary>Abstract</summary>
Learning in weight spaces, where neural networks process the weights of other deep neural networks, has emerged as a promising research direction with applications in various fields, from analyzing and editing neural fields and implicit neural representations, to network pruning and quantization. Recent works designed architectures for effective learning in that space, which takes into account its unique, permutation-equivariant, structure. Unfortunately, so far these architectures suffer from severe overfitting and were shown to benefit from large datasets. This poses a significant challenge because generating data for this learning setup is laborious and time-consuming since each data sample is a full set of network weights that has to be trained. In this paper, we address this difficulty by investigating data augmentations for weight spaces, a set of techniques that enable generating new data examples on the fly without having to train additional input weight space elements. We first review several recently proposed data augmentation schemes %that were proposed recently and divide them into categories. We then introduce a novel augmentation scheme based on the Mixup method. We evaluate the performance of these techniques on existing benchmarks as well as new benchmarks we generate, which can be valuable for future studies.
</details>
<details>
<summary>摘要</summary>
学习Weight空间中的神经网络，其中神经网络处理另一个深度神经网络的权重，已经出现为一个有前途的研究方向，具有应用于不同领域的可能性，从分析和编辑神经场和隐藏神经表示之间的应用，到网络剪辑和量化。最近的工作设计了适用于这个空间的建筑，考虑其独特的协变结构。然而，目前这些建筑受到严重的过拟合问题困扰，需要大量的数据来适应。在这篇论文中，我们解决这个挑战，通过调查Weight空间中的数据增强技术，以生成新的数据示例，而无需额外训练输入权重空间元素。我们首先回顾最近提出的数据增强方案，并将其分为类别。然后，我们介绍了一种基于 Mixup 方法的新的增强方案。我们对这些技术的性能进行评估，并在现有的标准准则上进行评估，以及新生成的标准准则，这些标准准则可能对未来的研究有所价值。
</details></li>
</ul>
<hr>
<h2 id="Controlling-the-Output-of-a-Generative-Model-by-Latent-Feature-Vector-Shifting"><a href="#Controlling-the-Output-of-a-Generative-Model-by-Latent-Feature-Vector-Shifting" class="headerlink" title="Controlling the Output of a Generative Model by Latent Feature Vector Shifting"></a>Controlling the Output of a Generative Model by Latent Feature Vector Shifting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08850">http://arxiv.org/abs/2311.08850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Róbert Belanec, Peter Lacko, Kristína Malinovská</li>
<li>for: 这个论文的目的是控制StyleGAN3生成器的输出图像修改。</li>
<li>methods: 我们使用了一个预训练的StyleGAN3生成器和一个ResNet34对应神经网络，将生成的图像分类为 celebA 数据集中的 Binary  facial 特征。我们还使用了一个叫做 latent feature shifter 的神经网络，将 StyleGAN3 的 latent vector shift 到指定的特征方向。</li>
<li>results: 我们的 latent feature shifter 方法比基eline方法多出了更多的生成图像具有想要的特征。我们通过评估结果发现，我们的 latent feature shifter 方法成功地控制了 StyleGAN3 生成器的输出图像修改。<details>
<summary>Abstract</summary>
State-of-the-art generative models (e.g. StyleGAN3 \cite{karras2021alias}) often generate photorealistic images based on vectors sampled from their latent space. However, the ability to control the output is limited. Here we present our novel method for latent vector shifting for controlled output image modification utilizing semantic features of the generated images. In our approach we use a pre-trained model of StyleGAN3 that generates images of realistic human faces in relatively high resolution. We complement the generative model with a convolutional neural network classifier, namely ResNet34, trained to classify the generated images with binary facial features from the CelebA dataset. Our latent feature shifter is a neural network model with a task to shift the latent vectors of a generative model into a specified feature direction. We have trained latent feature shifter for multiple facial features, and outperformed our baseline method in the number of generated images with the desired feature. To train our latent feature shifter neural network, we have designed a dataset of pairs of latent vectors with and without a certain feature. Based on the evaluation, we conclude that our latent feature shifter approach was successful in the controlled generation of the StyleGAN3 generator.
</details>
<details>
<summary>摘要</summary>
现代生成模型（例如StyleGAN3 \cite{karras2021alias）frequently生成高分辨率的图像，基于生成器的幂space中的向量采样。然而，控制输出的能力受限。在这里，我们介绍了我们的新方法，利用生成器图像的Semantic特征来实现控制输出图像修改。我们使用已经训练过的StyleGAN3生成器，可以生成高分辨率的真实人脸图像。我们补充了生成器的核心网络，使其能够通过CelebA数据集中的二分类网络（ResNet34）来分类生成的图像。我们的幂向量推移器是一个具有将幂向量推移到指定特征方向的任务的神经网络模型。我们在多个面部特征上训练了幂向量推移器，并超过了我们的基eline方法的数量。为了训练我们的幂向量推移器神经网络，我们设计了一个包含具有和无某些特征的latent向量对的数据集。根据评估结果，我们认为我们的幂向量推移器方法成功地控制了StyleGAN3生成器。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Video-Relighting-Using-Casual-Light-Stage"><a href="#Personalized-Video-Relighting-Using-Casual-Light-Stage" class="headerlink" title="Personalized Video Relighting Using Casual Light Stage"></a>Personalized Video Relighting Using Casual Light Stage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08843">http://arxiv.org/abs/2311.08843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Myeong Choi, Max Christman, Roni Sengupta</li>
<li>for: 这个论文的目的是提出一种个性化视频重光算法，以实现在任何姿势、表情和照明条件下，在实时下生成高质量的重光视频。</li>
<li>methods: 该算法使用了一种新的神经网络重光架构，可以有效地分离出照明源的光照特征、物体的 geometry 和反射特征，然后将其与目标照明相加以生成重光图像。</li>
<li>results: 根据对 Light Stage at Your Desk (LSYD) 数据和 Light Stage captured One Light At a Time (OLAT) 数据的质量评估，这种重光算法能够提高肖像图像重光质量和时间稳定性，比之前的方法更高效。<details>
<summary>Abstract</summary>
In this paper, we develop a personalized video relighting algorithm that produces high-quality and temporally consistent relit video under any pose, expression, and lighting conditions in real-time. Existing relighting algorithms typically rely either on publicly available synthetic data, which yields poor relighting results, or instead on Light Stage data which is inaccessible and is not publicly available. We show that by casually capturing video of a user watching YouTube videos on a monitor we can train a personalized algorithm capable of producing high-quality relighting under any condition. Our key contribution is a novel neural relighting architecture that effectively separates the intrinsic appearance features, geometry and reflectance, from the source lighting and then combines it with the target lighting to generate a relit image. This neural architecture enables smoothing of intrinsic appearance features leading to temporally stable video relighting. Both qualitative and quantitative evaluations show that our relighting architecture improves portrait image relighting quality and temporal consistency over state-of-the-art approaches on both casually captured Light Stage at Your Desk (LSYD) data and Light Stage captured One Light At a Time (OLAT) datasets.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们开发了一种个性化视频重新照明算法，该算法在实时下生成高质量、时间上一致的重新照明视频，无论用户的姿势、表情或照明条件如何。现有的重新照明算法通常依赖于公共可用的生成器数据，这些数据的重新照明结果很差，或者使用Light Stage数据，但这些数据不公开可用。我们显示，通过通过捕捉用户在 monitor 上观看 YouTube 视频来训练个性化算法，我们可以生成高质量的重新照明视频。我们的关键贡献是一种新的神经网络重新照明架构，该架构能够有效地分离出照明源的自然特征、几何和反射特征，然后与目标照明相结合，生成一个重新照明的图像。这种神经网络架构使得图像的内在特征平滑，从而实现了时间上一致的视频重新照明。我们的重新照明架构在使用LSYD 和 OLAT 数据集上的质量和时间一致性方面与当前的方法进行比较，并取得了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Correlation-guided-Query-Dependency-Calibration-in-Video-Representation-Learning-for-Temporal-Grounding"><a href="#Correlation-guided-Query-Dependency-Calibration-in-Video-Representation-Learning-for-Temporal-Grounding" class="headerlink" title="Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding"></a>Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08835">http://arxiv.org/abs/2311.08835</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wjun0830/cgdetr">https://github.com/wjun0830/cgdetr</a></li>
<li>paper_authors: WonJun Moon, Sangeek Hyun, SuBeen Lee, Jae-Pil Heo</li>
<li>for: 这 paper 的目的是提供一种基于注意力机制的视频时间固定方法，以便在视频和文本查询之间强化交互，并且能够根据文本查询提取相关的视频clip。</li>
<li>methods: 这 paper 使用了一种名为 Correlation-Guided Detection Transformer~(CG-DETR) 的方法，它包括一个适应式跨模态注意力层、一个 dummy tokens 的使用、以及一个高级概念共同embedding空间学习。</li>
<li>results: 这 paper 的实验结果表明，CG-DETR 可以在多个benchmark上达到州OF-the-art的Result，包括时刻检索和突出部分检测。 codes 可以在 <a target="_blank" rel="noopener" href="https://github.com/wjun0830/CGDETR">https://github.com/wjun0830/CGDETR</a> 中找到。<details>
<summary>Abstract</summary>
Recent endeavors in video temporal grounding enforce strong cross-modal interactions through attention mechanisms to overcome the modality gap between video and text query. However, previous works treat all video clips equally regardless of their semantic relevance with the text query in attention modules. In this paper, our goal is to provide clues for query-associated video clips within the crossmodal encoding process. With our Correlation-Guided Detection Transformer~(CG-DETR), we explore the appropriate clip-wise degree of cross-modal interactions and how to exploit such degrees for prediction. First, we design an adaptive cross-attention layer with dummy tokens. Dummy tokens conditioned by text query take a portion of the attention weights, preventing irrelevant video clips from being represented by the text query. Yet, not all word tokens equally inherit the text query's correlation to video clips. Thus, we further guide the cross-attention map by inferring the fine-grained correlation between video clips and words. We enable this by learning a joint embedding space for high-level concepts, i.e., moment and sentence level, and inferring the clip-word correlation. Lastly, we use a moment-adaptive saliency detector to exploit each video clip's degrees of text engagement. We validate the superiority of CG-DETR with the state-of-the-art results on various benchmarks for both moment retrieval and highlight detection. Codes are available at https://github.com/wjun0830/CGDETR.
</details>
<details>
<summary>摘要</summary>
近期的视频时间挂钩工作强制实施了跨Modal的交互，通过注意机制来超越视频和文本查询之间的Modal gap。然而，前一些工作都是在注意模块中对所有视频clip进行等效的处理，不考虑视频clip与文本查询的Semantic relevance。在这篇论文中，我们的目标是提供与文本查询相关的视频clip在跨Modal编码过程中的线索。我们使用Correlation-Guided Detection Transformer~(CG-DETR)来探索适当的clipwise度跨Modal交互，以及如何利用这些度量进行预测。首先，我们设计了适应式交叉注意层，其中文本查询条件下的干扰符token会占据一部分注意量，以避免不相关的视频clip被文本查询所代表。然而，不是所有的单词token都会相同地继承文本查询的视频clip相关性。因此，我们进一步指导交叉注意地图，通过学习高级概念的共同embedding空间，以及clip-word关系的推理。最后，我们使用时刻适应性的焦点检测器来利用每个视频clip的文本参与度。我们 validate CG-DETR的优越性通过在多种benchmark上实现state-of-the-art的结果，包括时刻检索和突出部分检测。代码可以在https://github.com/wjun0830/CGDETR中获取。
</details></li>
</ul>
<hr>
<h2 id="Target-oriented-Domain-Adaptation-for-Infrared-Image-Super-Resolution"><a href="#Target-oriented-Domain-Adaptation-for-Infrared-Image-Super-Resolution" class="headerlink" title="Target-oriented Domain Adaptation for Infrared Image Super-Resolution"></a>Target-oriented Domain Adaptation for Infrared Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08816">http://arxiv.org/abs/2311.08816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yongsongh/dasrgan">https://github.com/yongsongh/dasrgan</a></li>
<li>paper_authors: Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Yafei Dong, Shinichiro Omachi</li>
<li>for: 提高红外超分辨率图像质量</li>
<li>methods: 使用目标域适应SRGAN（DASRGAN），包括Texture-Oriented Adaptation（TOA）和Noise-Oriented Adaptation（NOA）两部分</li>
<li>results: 实验表明，DASRGAN在多个标准测试 benchmark 和不同的� upsampling 因子下表现出优于其他方法，并设置了新的州际表现标准。<details>
<summary>Abstract</summary>
Recent efforts have explored leveraging visible light images to enrich texture details in infrared (IR) super-resolution. However, this direct adaptation approach often becomes a double-edged sword, as it improves texture at the cost of introducing noise and blurring artifacts. To address these challenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN), an innovative framework specifically engineered for robust IR super-resolution model adaptation. DASRGAN operates on the synergy of two key components: 1) Texture-Oriented Adaptation (TOA) to refine texture details meticulously, and 2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer. Specifically, TOA uniquely integrates a specialized discriminator, incorporating a prior extraction branch, and employs a Sobel-guided adversarial loss to align texture distributions effectively. Concurrently, NOA utilizes a noise adversarial loss to distinctly separate the generative and Gaussian noise pattern distributions during adversarial training. Our extensive experiments confirm DASRGAN's superiority. Comparative analyses against leading methods across multiple benchmarks and upsampling factors reveal that DASRGAN sets new state-of-the-art performance standards. Code are available at \url{https://github.com/yongsongH/DASRGAN}.
</details>
<details>
<summary>摘要</summary>
近期研究探索了使用可见光图像来增强护理图像的细节，但这直接适应方法经常变成一件双刃剑，因为它会提高细节的同时也会引入噪声和模糊 artefacts。为了解决这些挑战，我们提议了Target-oriented Domain Adaptation SRGAN（DASRGAN），一种创新的护理图像超分解模型适应框架。DASRGAN在两个关键组件之间运行：1）Texture-Oriented Adaptation（TOA），用于精细调整细节，和2）Noise-Oriented Adaptation（NOA），专门降低噪声传输。具体来说，TOA包括一个特殊的检测器，并使用 Sobel 引导的对抗损失来有效地对细节分布进行对齐。同时，NOA使用噪声对抗损失来在对抗训练中明确地分离生成的pattern和 Gaussian 噪声的分布。我们的广泛的实验证明了 DASRGAN 的优越性。与其他领先方法进行多个标准尺度和增强因子的比较分析表明，DASRGAN 创造了新的状态标准表现。代码可以在 <https://github.com/yongsongH/DASRGAN> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Correlation-aware-active-learning-for-surgery-video-segmentation"><a href="#Correlation-aware-active-learning-for-surgery-video-segmentation" class="headerlink" title="Correlation-aware active learning for surgery video segmentation"></a>Correlation-aware active learning for surgery video segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08811">http://arxiv.org/abs/2311.08811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Wu, Pablo Marquez-Neila, Mingyi Zheng, Hedyeh Rafii-Tari, Raphael Sznitman</li>
<li>for: 这个研究的目的是提出一个新的活动学习策略（COALSamp），用于降低医疗影像资料的标注成本。</li>
<li>methods: 方法包括将影像视网膜下降到一个特别设计的对应学习 latent space，然后从本地块群中选择一定数量的表征性影像。</li>
<li>results: 这个方法在两个手术影像资料集上进行了评估，结果显示COALSamp 可以对医疗影像资料进行有效的分类。此外，这个方法还在三个真实世界的影像资料集上进行了评估，结果也很显著。<details>
<summary>Abstract</summary>
Semantic segmentation is a complex task that relies heavily on large amounts of annotated image data. However, annotating such data can be time-consuming and resource-intensive, especially in the medical domain. Active Learning (AL) is a popular approach that can help to reduce this burden by iteratively selecting images for annotation to improve the model performance. In the case of video data, it is important to consider the model uncertainty and the temporal nature of the sequences when selecting images for annotation. This work proposes a novel AL strategy for surgery video segmentation, \COALSamp{}, COrrelation-aWare Active Learning. Our approach involves projecting images into a latent space that has been fine-tuned using contrastive learning and then selecting a fixed number of representative images from local clusters of video frames. We demonstrate the effectiveness of this approach on two video datasets of surgical instruments and three real-world video datasets. The datasets and code will be made publicly available upon receiving necessary approvals.
</details>
<details>
<summary>摘要</summary>
Semantic segmentation是一项复杂的任务，它依赖于大量已经标注的图像数据。然而，对于医疗领域来说，标注这些数据可以是时间consuming和资源占用的。活动学习（AL）是一种受欢迎的方法，它可以逐步选择图像进行标注，以提高模型性能。在视频数据中，需要考虑模型的uncertainty和时间序列的特点，以便更好地选择需要标注的图像。这项工作提出了一种新的AL策略，称为\COALSamp{}, COrrelation-aWare Active Learning。我们的方法是将图像 проек到一个已经精心调整的latent空间中，然后选择视频帧的本地集群中固定数量的表示图像。我们在两个手术工具视频数据集和三个实际世界视频数据集上证明了这种方法的有效性。数据和代码将在接收所需的批准后公开发布。
</details></li>
</ul>
<hr>
<h2 id="EyeLS-Shadow-Guided-Instrument-Landing-System-for-Intraocular-Target-Approaching-in-Robotic-Eye-Surgery"><a href="#EyeLS-Shadow-Guided-Instrument-Landing-System-for-Intraocular-Target-Approaching-in-Robotic-Eye-Surgery" class="headerlink" title="EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target Approaching in Robotic Eye Surgery"></a>EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target Approaching in Robotic Eye Surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08799">http://arxiv.org/abs/2311.08799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Yang, Zhihao Zhao, Siyuan Shen, Daniel Zapp, Mathias Maier, Kai Huang, Nassir Navab, M. Ali Nasseri</li>
<li>for:  This paper aims to improve the accuracy of robotic ophthalmic surgery by using shadow positions to estimate the depth position of the instrument tip and optimize its insertion trajectory.</li>
<li>methods: The proposed method uses shadows to estimate the relative depth position of the instrument tip and the target, and then optimizes the insertion trajectory to approach the target within the scanning area of the iOCT.</li>
<li>results: The method was tested on a retina model and achieved an average depth error of 0.0127 mm for floating targets and 0.3473 mm for retinal targets in the surgical simulator, without damaging the retina.<details>
<summary>Abstract</summary>
Robotic ophthalmic surgery is an emerging technology to facilitate high-precision interventions such as retina penetration in subretinal injection and removal of floating tissues in retinal detachment depending on the input imaging modalities such as microscopy and intraoperative OCT (iOCT). Although iOCT is explored to locate the needle tip within its range-limited ROI, it is still difficult to coordinate iOCT's motion with the needle, especially at the initial target-approaching stage. Meanwhile, due to 2D perspective projection and thus the loss of depth information, current image-based methods cannot effectively estimate the needle tip's trajectory towards both retinal and floating targets. To address this limitation, we propose to use the shadow positions of the target and the instrument tip to estimate their relative depth position and accordingly optimize the instrument tip's insertion trajectory until the tip approaches targets within iOCT's scanning area. Our method succeeds target approaching on a retina model, and achieves an average depth error of 0.0127 mm and 0.3473 mm for floating and retinal targets respectively in the surgical simulator without damaging the retina.
</details>
<details>
<summary>摘要</summary>
关于：机器人眼科手术技术的发展机器人眼科手术是一种emerging technology，用于实现高精度干预，如retina penetration和floatings tissues removing，这些干预都取决于输入的干预modalities，如微scopy和intraoperative OCT（iOCT）。虽然iOCT被探索以定位针的位置，但是尚未能够协调针与iOCT的运动，特别是在目标方向的初始阶段。此外，由于2D的 perspective projection，当前的图像基本方法无法有效地估算针的轨迹，特别是在向retinal和浮动目标的方向上。为了解决这个限制，我们提议使用target和 instrumente tip的阴影位置来估算它们的相对深度位置，并根据此来优化针的插入轨迹，直到针接近target在iOCT的扫描范围内。我们的方法在retina模型上成功地进行目标接近，并在手术模拟器中达到了0.0127mm和0.3473mm的平均深度误差，对于浮动和retinal目标。
</details></li>
</ul>
<hr>
<h2 id="HFORD-High-Fidelity-and-Occlusion-Robust-De-identification-for-Face-Privacy-Protection"><a href="#HFORD-High-Fidelity-and-Occlusion-Robust-De-identification-for-Face-Privacy-Protection" class="headerlink" title="HFORD: High-Fidelity and Occlusion-Robust De-identification for Face Privacy Protection"></a>HFORD: High-Fidelity and Occlusion-Robust De-identification for Face Privacy Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08786">http://arxiv.org/abs/2311.08786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongxin Chen, Mingrui Zhu, Nannan Wang, Xinbo Gao</li>
<li>for: 面部隐私保护issue受到了智能设备的普及和计算机视觉技术的发展的关注。本文提出了一种高效和防护 occlusion 的面部隐私化方法（HFORD），以解决这些问题。</li>
<li>methods: 本方法使用了一种叫做 Identity Disentanglement Module（IDM）的模块，用于分离 latent codes 中的个体特征和特征特征。此外，还提出了一种叫做 Attribute Retention Module（ARM）的模块，用于保留不相关的特征和面部遮挡。</li>
<li>results: 对比其他面部隐私化方法，本方法的结果质量更高，细节更加详细，并且更强地适应 occlusion 情况。<details>
<summary>Abstract</summary>
With the popularity of smart devices and the development of computer vision technology, concerns about face privacy protection are growing. The face de-identification technique is a practical way to solve the identity protection problem. The existing facial de-identification methods have revealed several problems, including the impact on the realism of anonymized results when faced with occlusions and the inability to maintain identity-irrelevant details in anonymized results. We present a High-Fidelity and Occlusion-Robust De-identification (HFORD) method to deal with these issues. This approach can disentangle identities and attributes while preserving image-specific details such as background, facial features (e.g., wrinkles), and lighting, even in occluded scenes. To disentangle the latent codes in the GAN inversion space, we introduce an Identity Disentanglement Module (IDM). This module selects the latent codes that are closely related to the identity. It further separates the latent codes into identity-related codes and attribute-related codes, enabling the network to preserve attributes while only modifying the identity. To ensure the preservation of image details and enhance the network's robustness to occlusions, we propose an Attribute Retention Module (ARM). This module adaptively preserves identity-irrelevant details and facial occlusions and blends them into the generated results in a modulated manner. Extensive experiments show that our method has higher quality, better detail fidelity, and stronger occlusion robustness than other face de-identification methods.
</details>
<details>
<summary>摘要</summary>
随着智能设备的普及和计算机视觉技术的发展，人脸隐私保护的问题日益突出。面部隐私化技术是一种实际的解决方案。现有的面部隐私化方法存在一些问题，如受到遮挡物的影响下的匿名结果的真实性下降，以及维护不同于人脸特征的匿名结果。我们提出了一种高度准确和遮挡物鲁棒的匿名化方法（HFORD），以解决这些问题。这种方法可以分离人脸特征和属性，并保留图像特有的背景、表情特征（如皱纹）和照明等信息，即使在遮挡场景下也能够保持高质量。为了分离GAN拟合空间中的秘密码，我们引入了一种身份分解模块（IDM）。这个模块选择与身份有关的秘密码，并将其分解成身份相关的秘密码和属性相关的秘密码，使网络能够保留属性，只对人脸进行修改。为确保图像细节的保留和网络的遮挡物鲁棒性，我们提议一种Attribute Retention Module（ARM）。这个模块可以动态保留不相关于身份的细节和脸部遮挡物，并将其混合到生成结果中，以实现更高质量和更强的鲁棒性。经过广泛的实验，我们发现我们的方法在质量、细节准确性和遮挡物鲁棒性等方面都有更高的表现。
</details></li>
</ul>
<hr>
<h2 id="Language-Semantic-Graph-Guided-Data-Efficient-Learning"><a href="#Language-Semantic-Graph-Guided-Data-Efficient-Learning" class="headerlink" title="Language Semantic Graph Guided Data-Efficient Learning"></a>Language Semantic Graph Guided Data-Efficient Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08782">http://arxiv.org/abs/2311.08782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Ma, Shuang Li, Lincan Cai, Jingxuan Kang<br>for: 这个研究的目的是提高机器学习模型对有限数据的学习效能，并在无需人工标注的情况下实现更好的模型表现。methods: 这个研究使用了 Semi-Supervised Learning (SSL)、Transfer Learning (TL) 和 Data Augmentation (DA) 等方法来实现数据优化。另外，这个研究还使用了一个名为 Language Semantic Graph (LSG) 的新方法，它是根据标签中的自然语言描述建立的一个图形。results: 这个研究在图像、影片和音频等模式下运用 LSG 方法，在 SSL 和 TL 情况下获得了显著改善的表现，并且比其他数据优化方法更快速。<details>
<summary>Abstract</summary>
Developing generalizable models that can effectively learn from limited data and with minimal reliance on human supervision is a significant objective within the machine learning community, particularly in the era of deep neural networks. Therefore, to achieve data-efficient learning, researchers typically explore approaches that can leverage more related or unlabeled data without necessitating additional manual labeling efforts, such as Semi-Supervised Learning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL leverages unlabeled data in the training process, while TL enables the transfer of expertise from related data distributions. DA broadens the dataset by synthesizing new data from existing examples. However, the significance of additional knowledge contained within labels has been largely overlooked in research. In this paper, we propose a novel perspective on data efficiency that involves exploiting the semantic information contained in the labels of the available data. Specifically, we introduce a Language Semantic Graph (LSG) which is constructed from labels manifest as natural language descriptions. Upon this graph, an auxiliary graph neural network is trained to extract high-level semantic relations and then used to guide the training of the primary model, enabling more adequate utilization of label knowledge. Across image, video, and audio modalities, we utilize the LSG method in both TL and SSL scenarios and illustrate its versatility in significantly enhancing performance compared to other data-efficient learning approaches. Additionally, our in-depth analysis shows that the LSG method also expedites the training process.
</details>
<details>
<summary>摘要</summary>
Developing generalizable models that can effectively learn from limited data and with minimal reliance on human supervision is a significant objective within the machine learning community, particularly in the era of deep neural networks. Therefore, to achieve data-efficient learning, researchers typically explore approaches that can leverage more related or unlabeled data without necessitating additional manual labeling efforts, such as Semi-Supervised Learning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL leverages unlabeled data in the training process, while TL enables the transfer of expertise from related data distributions. DA broadens the dataset by synthesizing new data from existing examples. However, the significance of additional knowledge contained within labels has been largely overlooked in research. In this paper, we propose a novel perspective on data efficiency that involves exploiting the semantic information contained in the labels of the available data. Specifically, we introduce a Language Semantic Graph (LSG) which is constructed from labels manifest as natural language descriptions. Upon this graph, an auxiliary graph neural network is trained to extract high-level semantic relations and then used to guide the training of the primary model, enabling more adequate utilization of label knowledge. Across image, video, and audio modalities, we utilize the LSG method in both TL and SSL scenarios and illustrate its versatility in significantly enhancing performance compared to other data-efficient learning approaches. Additionally, our in-depth analysis shows that the LSG method also expedites the training process.Here's the translation in Traditional Chinese:开发能够从有限数据中学习并且仅对人工标注有最少依赖的机器学习模型是机器学习社区中的一个重要目标，特别是在深度神经网络时代。因此，实现数据效率的研究通常会探索可以将更多相关的或未标注的数据 leveraged 进行训练，例如半监督学习 (SSL)、传播学习 (TL) 和数据扩展 (DA)。SSL 利用训练过程中的无标注数据，而 TL 允许将相关数据分布中的专长转移到新的数据上。DA 则是将现有数据中的新数据生成新的数据，以增加数据集的大小。但是，实际上 Label 中含的额外知识几乎没有被研究。在这篇论文中，我们提出了一个新的数据效率的思路，即利用可用数据中的标签上的 semantic information。 Specifically, we introduce a Language Semantic Graph (LSG) which is constructed from labels manifest as natural language descriptions. Upon this graph, an auxiliary graph neural network is trained to extract high-level semantic relations and then used to guide the training of the primary model, enabling more adequate utilization of label knowledge. Across image, video, and audio modalities, we utilize the LSG method in both TL and SSL scenarios and illustrate its versatility in significantly enhancing performance compared to other data-efficient learning approaches. Additionally, our in-depth analysis shows that the LSG method also expedites the training process.
</details></li>
</ul>
<hr>
<h2 id="Two-stage-Joint-Transductive-and-Inductive-learning-for-Nuclei-Segmentation"><a href="#Two-stage-Joint-Transductive-and-Inductive-learning-for-Nuclei-Segmentation" class="headerlink" title="Two-stage Joint Transductive and Inductive learning for Nuclei Segmentation"></a>Two-stage Joint Transductive and Inductive learning for Nuclei Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08774">http://arxiv.org/abs/2311.08774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hesham Ali, Idriss Tondji, Mennatullah Siam</li>
<li>for: 针对医疗影像中蛋白质分割任务进行研究，以提高肿瘤诊断和治疗的效率和准确性。</li>
<li>methods: 提出了一种新的混合学习方法，结合了泛化学习和推导学习的优点，以便更好地利用可用的标注和未标注数据。</li>
<li>results: 在MoNuSeg测试集上证明了该方法的效果和潜在应用前景，并提出了一种新的两stage混合推理方案。<details>
<summary>Abstract</summary>
AI-assisted nuclei segmentation in histopathological images is a crucial task in the diagnosis and treatment of cancer diseases. It decreases the time required to manually screen microscopic tissue images and can resolve the conflict between pathologists during diagnosis. Deep Learning has proven useful in such a task. However, lack of labeled data is a significant barrier for deep learning-based approaches. In this study, we propose a novel approach to nuclei segmentation that leverages the available labelled and unlabelled data. The proposed method combines the strengths of both transductive and inductive learning, which have been previously attempted separately, into a single framework. Inductive learning aims at approximating the general function and generalizing to unseen test data, while transductive learning has the potential of leveraging the unlabelled test data to improve the classification. To the best of our knowledge, this is the first study to propose such a hybrid approach for medical image segmentation. Moreover, we propose a novel two-stage transductive inference scheme. We evaluate our approach on MoNuSeg benchmark to demonstrate the efficacy and potential of our method.
</details>
<details>
<summary>摘要</summary>
AI助成 Histopathological 图像中的核体分割是诊断和治疗癌症疾病的关键任务。它可以减少手动检查微scopic 组织图像所需的时间，并能够解决 pathologists 在诊断中存在的冲突。深度学习 已经在这种任务中证明了其有用性。然而，缺乏标注数据是深度学习基于方法的主要障碍。在这项研究中，我们提出了一种新的核体分割方法，利用可用的标注和无标注数据。我们的方法结合了泛化学习和抽象学习的优点，这两种学习方法在过去已经分别被应用。泛化学习目标是将通用函数approximated，并在未看到的测试数据上generalize;而抽象学习具有利用无标注测试数据来改进分类的潜在优势。根据我们所知，这是第一项提出了这种混合方法的医学图像分割研究。此外，我们还提出了一种新的两stage 混合推理方案。我们在 MoNuSeg benchmark 上评估了我们的方法，以demonstrate 我们的方法的效果和潜在。
</details></li>
</ul>
<hr>
<h2 id="FastBlend-a-Powerful-Model-Free-Toolkit-Making-Video-Stylization-Easier"><a href="#FastBlend-a-Powerful-Model-Free-Toolkit-Making-Video-Stylization-Easier" class="headerlink" title="FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier"></a>FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09265">http://arxiv.org/abs/2311.09265</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/artiprocher/sd-webui-fastblend">https://github.com/artiprocher/sd-webui-fastblend</a></li>
<li>paper_authors: Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang, Mingyi Jin</li>
<li>for:  Addresses the consistency problem in video processing for tasks such as style transfer and image editing.</li>
<li>methods:  Uses a patch matching algorithm with two inference modes: blending and interpolation.</li>
<li>results:  Outperforms existing methods for video deflickering and video synthesis in the blending mode, and surpasses video interpolation and model-based video processing approaches in the interpolation mode.Here’s the summary in Traditional Chinese:</li>
<li>for: 解决影像处理中的一致问题，如style transfer和图像修复。</li>
<li>methods: 使用一个patch matching算法，分别有汇入和 interpolate两种推论方式。</li>
<li>results: 与现有方法相比，在汇入模式下表现更好，并在 interpolate 模式下超过了影像 interpolate 和基于模型的影像处理方法。I hope that helps!<details>
<summary>Abstract</summary>
With the emergence of diffusion models and rapid development in image processing, it has become effortless to generate fancy images in tasks such as style transfer and image editing. However, these impressive image processing approaches face consistency issues in video processing. In this paper, we propose a powerful model-free toolkit called FastBlend to address the consistency problem for video processing. Based on a patch matching algorithm, we design two inference modes, including blending and interpolation. In the blending mode, FastBlend eliminates video flicker by blending the frames within a sliding window. Moreover, we optimize both computational efficiency and video quality according to different application scenarios. In the interpolation mode, given one or more keyframes rendered by diffusion models, FastBlend can render the whole video. Since FastBlend does not modify the generation process of diffusion models, it exhibits excellent compatibility. Extensive experiments have demonstrated the effectiveness of FastBlend. In the blending mode, FastBlend outperforms existing methods for video deflickering and video synthesis. In the interpolation mode, FastBlend surpasses video interpolation and model-based video processing approaches. The source codes have been released on GitHub.
</details>
<details>
<summary>摘要</summary>
“对于快速发展的图像处理技术和扩散模型，创造出了丰富的图像效果，例如风格转移和图像修剪。但这些印象精采的图像处理方法在视频处理中还存在一定的一致性问题。在本文中，我们提出了一个强大且无模型的工具套件called FastBlend，以解决视频处理中的一致性问题。基于图像匹配算法，我们设计了两种推理模式，包括融合和插值。在融合模式下，FastBlend可以消除视频震荡，通过视频内排埋窗口中的匹配。此外，我们还优化了不同应用场景中的计算效率和视频质量。在插值模式下，给定一幅或多幅透过扩散模型生成的关键帧，FastBlend可以生成整个视频。由于FastBlend不会改变扩散模型的生成过程，因此它具有很好的相容性。实验结果显示FastBlend的效果极佳，在融合模式下比现有的视频显示和视频生成方法更好，在插值模式下比视频插值和基于模型的视频处理方法更好。源代码已经在GitHub上发布。”
</details></li>
</ul>
<hr>
<h2 id="4K-Resolution-Photo-Exposure-Correction-at-125-FPS-with-8K-Parameters"><a href="#4K-Resolution-Photo-Exposure-Correction-at-125-FPS-with-8K-Parameters" class="headerlink" title="4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters"></a>4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08759">http://arxiv.org/abs/2311.08759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhou-yijie/msltnet">https://github.com/zhou-yijie/msltnet</a></li>
<li>paper_authors: Yijie Zhou, Chao Li, Jin Liang, Tianyi Xu, Xin Liu, Jun Xu</li>
<li>for: 本研究旨在提出一种高效且轻量级的多层感知架构，用于高分辨率图像曝光 corrections。</li>
<li>methods: 提议使用多槽线性变换网络（MSLT），通过拉普拉敏度 pyramid 技术进行高频和低频层分解，然后采用像素适应线性变换进行层次修复。</li>
<li>results: 实验结果表明，提议的 MSLT 网络在两个标准数据集上对 фото曝光 corrections 表现更高效，并且对比于现有的状态 искусственный neural network 有更好的性能。<details>
<summary>Abstract</summary>
The illumination of improperly exposed photographs has been widely corrected using deep convolutional neural networks or Transformers. Despite with promising performance, these methods usually suffer from large parameter amounts and heavy computational FLOPs on high-resolution photographs. In this paper, we propose extremely light-weight (with only ~8K parameters) Multi-Scale Linear Transformation (MSLT) networks under the multi-layer perception architecture, which can process 4K-resolution sRGB images at 125 Frame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT networks first decompose an input image into high and low frequency layers by Laplacian pyramid techniques, and then sequentially correct different layers by pixel-adaptive linear transformation, which is implemented by efficient bilateral grid learning or 1x1 convolutions. Experiments on two benchmark datasets demonstrate the efficiency of our MSLTs against the state-of-the-arts on photo exposure correction. Extensive ablation studies validate the effectiveness of our contributions. The code is available at https://github.com/Zhou-Yijie/MSLTNet.
</details>
<details>
<summary>摘要</summary>
“对于不当露光的照片，深度卷积神经网络或Transformers已经广泛地解决问题。然而，这些方法通常具有较大的参数数量和高 Computational FLOPs 的高resolution照片。在这篇文章中，我们提出了 extremely light-weight（仅有 ~8K 参数）的多尺度线性转换（MSLT）网络，可以在 Titan RTX GPU 上处理 4K 分辨率 sRGB 影像，并且可以在 125 帧每秒（FPS）的速度下进行处理。具体来说，我们的 MSLT 网络首先将输入影像分解成高频和低频层，使用 Laplacian  pyramid 技术，然后逐层 corrections 不同层的像素，使用高效的二元方格学习或 1x1 卷积。实验结果显示，我们的 MSLT 网络在照片曝光修正方面与现有的方法相比，有着更高的效率。广泛的测试 validate 了我们的贡献。代码可以在 https://github.com/Zhou-Yijie/MSLTNet 上取得。”
</details></li>
</ul>
<hr>
<h2 id="Improved-Dense-Nested-Attention-Network-Based-on-Transformer-for-Infrared-Small-Target-Detection"><a href="#Improved-Dense-Nested-Attention-Network-Based-on-Transformer-for-Infrared-Small-Target-Detection" class="headerlink" title="Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection"></a>Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08747">http://arxiv.org/abs/2311.08747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun Bao, Jie Cao, Yaqian Ning, Tianhua Zhao, Zhijun Li, Zechen Wang, Li Zhang, Qun Hao<br>for:The paper is written for detecting infrared small targets in complex and dynamic backgrounds using deep learning.methods:The proposed method, called improved dense nested attention network (IDNANet), is based on the transformer architecture and incorporates several novel features, including the Swin-transformer and ACmix attention structure, to enhance the continuity and features of the target.results:The proposed method outperforms other state-of-the-art methods in terms of probability of detection (P_d), false-alarm rate (F_a), and mean intersection of union ($mIoU$). Specifically, the $mIoU$ reaches 90.89 on the NUDT-SIRST dataset and 79.72 on the NUAA-SIRST dataset.<details>
<summary>Abstract</summary>
Infrared small target detection based on deep learning offers unique advantages in separating small targets from complex and dynamic backgrounds. However, the features of infrared small targets gradually weaken as the depth of convolutional neural network (CNN) increases. To address this issue, we propose a novel method for detecting infrared small targets called improved dense nested attention network (IDNANet), which is based on the transformer architecture. We preserve the dense nested structure of dense nested attention network (DNANet) and introduce the Swin-transformer during feature extraction stage to enhance the continuity of features. Furthermore, we integrate the ACmix attention structure into the dense nested structure to enhance the features of intermediate layers. Additionally, we design a weighted dice binary cross-entropy (WD-BCE) loss function to mitigate the negative impact of foreground-background imbalance in the samples. Moreover, we develop a dataset specifically for infrared small targets, called BIT-SIRST. The dataset comprises a significant amount of real-world targets and manually annotated labels, as well as synthetic data and corresponding labels. We have evaluated the effectiveness of our method through experiments conducted on public datasets. In comparison to other state-of-the-art methods, our approach outperforms in terms of probability of detection (P_d), false-alarm rate (F_a), and mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89 on the NUDT-SIRST dataset and 79.72 on the NUAA-SIRST dataset.
</details>
<details>
<summary>摘要</summary>
infrared小target检测基于深度学习具有独特的优势，可以准确分割小target从复杂和动态背景中。然而，infrared小target特征逐渐弱化为深度卷积神经网络（CNN）的深度增加。为解决这个问题，我们提出了一种改进的infrared小target检测方法，称为改进的密集嵌入注意网络（IDNANet），基于transformer架构。我们保持密集嵌入结构的密集嵌入注意网络（DNANet）结构，并在特征提取阶段引入Swin-transformer以增强特征连续性。此外，我们将ACmix注意结构integrated到密集结构中，以提高中间层特征的表现。此外，我们还定义了weighted dice二分类优化函数（WD-BCE），以抑制样本中背景干扰的负面影响。此外，我们还开发了专门为infrared小target而设计的BIT-SIRST数据集。该数据集包括大量真实世界目标和手动标注 Label，以及 sintetic数据和相应的标注。我们通过对公共数据集进行实验，证明了我们的方法的效果。与其他当前状态的方法相比，我们的方法在检测概率（P_d）、假阳性率（F_a）和mean intersection of union（$mIoU）方面具有优势。$mIoU$ 在NUDT-SIRST数据集上达到了90.89，在NUAA-SIRST数据集上达到了79.72。
</details></li>
</ul>
<hr>
<h2 id="A-Diffusion-Model-Based-Quality-Enhancement-Method-for-HEVC-Compressed-Video"><a href="#A-Diffusion-Model-Based-Quality-Enhancement-Method-for-HEVC-Compressed-Video" class="headerlink" title="A Diffusion Model Based Quality Enhancement Method for HEVC Compressed Video"></a>A Diffusion Model Based Quality Enhancement Method for HEVC Compressed Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08746">http://arxiv.org/abs/2311.08746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Liu, Honggang Qi</li>
<li>for: 提高压缩视频质量</li>
<li>methods: 使用扩散模型进行后处理</li>
<li>results: 在混合数据集上实现更高的质量改进 compared to existing methods<details>
<summary>Abstract</summary>
Video post-processing methods can improve the quality of compressed videos at the decoder side. Most of the existing methods need to train corresponding models for compressed videos with different quantization parameters to improve the quality of compressed videos. However, in most cases, the quantization parameters of the decoded video are unknown. This makes existing methods have their limitations in improving video quality. To tackle this problem, this work proposes a diffusion model based post-processing method for compressed videos. The proposed method first estimates the feature vectors of the compressed video and then uses the estimated feature vectors as the prior information for the quality enhancement model to adaptively enhance the quality of compressed video with different quantization parameters. Experimental results show that the quality enhancement results of our proposed method on mixed datasets are superior to existing methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>视频后处理技术可以提高压缩视频在解码器端的质量。现有的大多数方法需要为不同的压缩参数训练对应的模型，以提高压缩视频的质量。然而，在大多数情况下，解码后的视频的压缩参数都是未知的。这限制了现有方法的改进视频质量的能力。为解决这个问题，本研究提出了基于扩散模型的后处理方法。该方法首先估计压缩视频的特征向量，然后使用估计的特征向量作为质量提升模型的先知信息，以适应不同的压缩参数进行自适应质量提升。实验结果表明，我们提出的方法在混合数据集上的质量提升结果较 existing方法优。Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Scalable-Federated-Learning-for-Clients-with-Different-Input-Image-Sizes-and-Numbers-of-Output-Categories"><a href="#Scalable-Federated-Learning-for-Clients-with-Different-Input-Image-Sizes-and-Numbers-of-Output-Categories" class="headerlink" title="Scalable Federated Learning for Clients with Different Input Image Sizes and Numbers of Output Categories"></a>Scalable Federated Learning for Clients with Different Input Image Sizes and Numbers of Output Categories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08716">http://arxiv.org/abs/2311.08716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuhei Nitta, Taiji Suzuki, Albert Rodríguez Mulet, Atsushi Yaguchi, Ryusuke Hirai</li>
<li>for: 采用 federated learning 方法进行隐私保护的训练，但不是所有客户端的数据都可以共享。</li>
<li>methods: 根据客户端的输入图像大小和输出类别数量调整本地模型的深度和宽度，以及提供一个新的普适性隔距来描述联邦学习的泛化差。</li>
<li>results: 在多个不同客户端设置下，对图像分类和物体检测任务进行了证明效果，并且提供了一个可靠的 bound 来描述联邦学习的泛化差。<details>
<summary>Abstract</summary>
Federated learning is a privacy-preserving training method which consists of training from a plurality of clients but without sharing their confidential data. However, previous work on federated learning do not explore suitable neural network architectures for clients with different input images sizes and different numbers of output categories. In this paper, we propose an effective federated learning method named ScalableFL, where the depths and widths of the local models for each client are adjusted according to the clients' input image size and the numbers of output categories. In addition, we provide a new bound for the generalization gap of federated learning. In particular, this bound helps to explain the effectiveness of our scalable neural network approach. We demonstrate the effectiveness of ScalableFL in several heterogeneous client settings for both image classification and object detection tasks.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种隐私保护的训练方法，它通过多个客户端进行训练，但不是共享客户端的敏感数据。然而，过去的联邦学习工作未经检查适合客户端的不同输入图像大小和输出类别数量的适应性的神经网络架构。在这篇论文中，我们提出了一种有效的联邦学习方法，名为可扩展FL（ScalableFL）。我们在每个客户端的本地模型中调整了深度和宽度，以适应客户端的输入图像大小和输出类别数量。此外，我们还提供了一个新的泛化差 bounds，帮助解释我们的扩展神经网络方法的效iveness。我们在多种不同客户端设置下进行了多个实验，证明了ScalableFL 的效iveness。Note: Please note that the translation is in Simplified Chinese, and the word order and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="CP-EB-Talking-Face-Generation-with-Controllable-Pose-and-Eye-Blinking-Embedding"><a href="#CP-EB-Talking-Face-Generation-with-Controllable-Pose-and-Eye-Blinking-Embedding" class="headerlink" title="CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding"></a>CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08673">http://arxiv.org/abs/2311.08673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzong Wang, Yimin Deng, Ziqi Liang, Xulong Zhang, Ning Cheng, Jing Xiao</li>
<li>for: 这个论文是为了提出一种名为“CP-EB”的对话面生成方法，用于从音频信号和人像作为输入，生成一个具有自然头姿和眼睛跳动的人讲视频。</li>
<li>methods: 该方法使用了一种基于GAN的建模结构，从输入音频和参考视频中提取眼睛跳动特征，并通过对其进行对比训练，将其 embedding到人讲图像中。</li>
<li>results: 实验结果显示，该方法可以生成具有同步嘴部动作、自然头姿和眼睛跳动的真实人讲视频。<details>
<summary>Abstract</summary>
This paper proposes a talking face generation method named "CP-EB" that takes an audio signal as input and a person image as reference, to synthesize a photo-realistic people talking video with head poses controlled by a short video clip and proper eye blinking embedding. It's noted that not only the head pose but also eye blinking are both important aspects for deep fake detection. The implicit control of poses by video has already achieved by the state-of-art work. According to recent research, eye blinking has weak correlation with input audio which means eye blinks extraction from audio and generation are possible. Hence, we propose a GAN-based architecture to extract eye blink feature from input audio and reference video respectively and employ contrastive training between them, then embed it into the concatenated features of identity and poses to generate talking face images. Experimental results show that the proposed method can generate photo-realistic talking face with synchronous lips motions, natural head poses and blinking eyes.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种名为“CP-EB”的人脸发言方法，该方法接受音频信号和人像作为输入，并生成一个具有自然头姿和眼睛跳动的真实人脸发言视频。研究表明，不仅头姿也是深度伪造检测中重要的一个方面，而且眼睛跳动与输入音频的关系弱，这意味着可以从音频中提取眼睛跳动特征并生成。因此，我们提议使用GAN网络抽取音频和参考视频中的眼睛跳动特征，并在这些特征之间进行对比培训，然后将其 embedding到人脸特征和头姿特征中，以生成真实的人脸发言图像。实验结果表明，提出的方法可以生成具有同步嘴部动作、自然头姿和眼睛跳动的真实人脸发言图像。
</details></li>
</ul>
<hr>
<h2 id="Deep-Neural-Network-Identification-of-Limnonectes-Species-and-New-Class-Detection-Using-Image-Data"><a href="#Deep-Neural-Network-Identification-of-Limnonectes-Species-and-New-Class-Detection-Using-Image-Data" class="headerlink" title="Deep Neural Network Identification of Limnonectes Species and New Class Detection Using Image Data"></a>Deep Neural Network Identification of Limnonectes Species and New Class Detection Using Image Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08661">http://arxiv.org/abs/2311.08661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Xu, Yili Hong, Eric P. Smith, David S. McLeod, Xinwei Deng, Laura J. Freeman</li>
<li>for: 这篇论文是用于解决生物多样性的挑战，具体来说是用于处理种群复杂的问题。</li>
<li>methods: 这篇论文使用机器学习的原理来解决两个问题：一是种群分类问题，二是外围检测问题。</li>
<li>results: 研究人员使用深度神经网络成功地自动将图像分类到已知种群中，并且可以成功地检测图像是否属于现有类别之外。<details>
<summary>Abstract</summary>
As is true of many complex tasks, the work of discovering, describing, and understanding the diversity of life on Earth (viz., biological systematics and taxonomy) requires many tools. Some of this work can be accomplished as it has been done in the past, but some aspects present us with challenges which traditional knowledge and tools cannot adequately resolve. One such challenge is presented by species complexes in which the morphological similarities among the group members make it difficult to reliably identify known species and detect new ones. We address this challenge by developing new tools using the principles of machine learning to resolve two specific questions related to species complexes. The first question is formulated as a classification problem in statistics and machine learning and the second question is an out-of-distribution (OOD) detection problem. We apply these tools to a species complex comprising Southeast Asian stream frogs (Limnonectes kuhlii complex) and employ a morphological character (hind limb skin texture) traditionally treated qualitatively in a quantitative and objective manner. We demonstrate that deep neural networks can successfully automate the classification of an image into a known species group for which it has been trained. We further demonstrate that the algorithm can successfully classify an image into a new class if the image does not belong to the existing classes. Additionally, we use the larger MNIST dataset to test the performance of our OOD detection algorithm. We finish our paper with some concluding remarks regarding the application of these methods to species complexes and our efforts to document true biodiversity. This paper has online supplementary materials.
</details>
<details>
<summary>摘要</summary>
如同许多复杂任务一样，发现、描述和理解地球上生物多样性（即生物系统матиCS和taxonomy）需要许多工具。一些这些工具可以通过传统的方法来实现，但一些方面却对传统知识和工具来说无法充分解决。一个如此挑战是由种群复杂体系所带来，其中种群成员之间的形态相似性使得可靠地识别已知种和检测新种变得困难。我们通过开发新的工具，使用机器学习原理来解决这两个问题。第一个问题是一个统计学和机器学习中的分类问题，第二个问题是一个 OUT-OF-DISTRIBUTION（OOD）检测问题。我们在南东亚瀑布蟾（Limnonectes kuhlii complex）种群中应用这些工具，使用传统上被质量地对待的一个形态特征（背股皮Texture）进行了量化Objective的处理。我们示示了深度神经网络可以成功地自动将图像分类到已知种群中，并且可以成功地将图像分类到新的类别中。此外，我们使用更大的MNIST数据集来测试我们的OOD检测算法的性能。我们在文章结尾附加了一些关于这些方法在种群复杂体系中的应用以及我们的记录真正的生物多样性的评论。这篇文章有在线补充材料。
</details></li>
</ul>
<hr>
<h2 id="ConeQuest-A-Benchmark-for-Cone-Segmentation-on-Mars"><a href="#ConeQuest-A-Benchmark-for-Cone-Segmentation-on-Mars" class="headerlink" title="ConeQuest: A Benchmark for Cone Segmentation on Mars"></a>ConeQuest: A Benchmark for Cone Segmentation on Mars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08657">http://arxiv.org/abs/2311.08657</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kerner-lab/conequest">https://github.com/kerner-lab/conequest</a></li>
<li>paper_authors: Mirali Purohit, Jacob Adler, Hannah Kerner</li>
<li>for: 这 paper 是为了开发更加准确和可靠的 Mars 穹顶 cone 分割模型。</li>
<li>methods: 该 paper 使用了 computer vision 技术，并基于 Mars 的三个地区，提供了 &gt;13k 个样本，以便进行 cone 分割模型的训练和评估。</li>
<li>results: 该 paper 的结果表明，现有的 segmentation 模型无法准确地分割 Mars 的穹顶 cone，其中最佳模型的 IoU 分割率只有 52.52% 和 42.55%。<details>
<summary>Abstract</summary>
Over the years, space scientists have collected terabytes of Mars data from satellites and rovers. One important set of features identified in Mars orbital images is pitted cones, which are interpreted to be mud volcanoes believed to form in regions that were once saturated in water (i.e., a lake or ocean). Identifying pitted cones globally on Mars would be of great importance, but expert geologists are unable to sort through the massive orbital image archives to identify all examples. However, this task is well suited for computer vision. Although several computer vision datasets exist for various Mars-related tasks, there is currently no open-source dataset available for cone detection/segmentation. Furthermore, previous studies trained models using data from a single region, which limits their applicability for global detection and mapping. Motivated by this, we introduce ConeQuest, the first expert-annotated public dataset to identify cones on Mars. ConeQuest consists of >13k samples from 3 different regions of Mars. We propose two benchmark tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size Generalization. We finetune and evaluate widely-used segmentation models on both benchmark tasks. Results indicate that cone segmentation is a challenging open problem not solved by existing segmentation models, which achieve an average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and (ii), respectively. We believe this new benchmark dataset will facilitate the development of more accurate and robust models for cone segmentation. Data and code are available at https://github.com/kerner-lab/ConeQuest.
</details>
<details>
<summary>摘要</summary>
Over the years, space scientists have collected terabytes of Mars data from satellites and rovers. One important set of features identified in Mars orbital images is pitted cones, which are interpreted to be mud volcanoes believed to form in regions that were once saturated in water (i.e., a lake or ocean). Identifying pitted cones globally on Mars would be of great importance, but expert geologists are unable to sort through the massive orbital image archives to identify all examples. However, this task is well suited for computer vision. Although several computer vision datasets exist for various Mars-related tasks, there is currently no open-source dataset available for cone detection/segmentation. Furthermore, previous studies trained models using data from a single region, which limits their applicability for global detection and mapping. Motivated by this, we introduce ConeQuest, the first expert-annotated public dataset to identify cones on Mars. ConeQuest consists of >13k samples from 3 different regions of Mars. We propose two benchmark tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size Generalization. We finetune and evaluate widely-used segmentation models on both benchmark tasks. Results indicate that cone segmentation is a challenging open problem not solved by existing segmentation models, which achieve an average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and (ii), respectively. We believe this new benchmark dataset will facilitate the development of more accurate and robust models for cone segmentation. Data and code are available at https://github.com/kerner-lab/ConeQuest.Here's the translation in Traditional Chinese:过去的年头，宇宙科学家从卫星和探测车获取了数十TB的火星数据。一个重要的特征是穿孔碗，被解释为怀疑是过去曾经淹没在水中的泥火山。全球火星上的穿孔碗识别是非常重要，但专业地质学家无法从巨大的卫星图像档案中找到所有的例子。然而，这个任务非常适合计算机视觉。虽然有许多火星相关的计算机视觉数据集存在，但目前没有公开的数据集可以用于穿孔碗检测。此外，前一 studies将模型训练使用单一区域的数据，导致其在全球检测和地图上的应用有限。为了解决这个问题，我们介绍了ConeQuest，首个专家录实 dataset 用于火星穿孔碗检测。ConeQuest 包含了 >13k 个样本，来自三个不同的火星区域。我们提出了两个benchmark任务：(i) 空间一致和 (ii) 穿孔大小一致。我们在这两个任务上调整和评估了广泛使用的检测模型。结果显示，穿孔检测是一个尚未解决的开问题，现有的检测模型在内部数据上的内容率为52.52%和42.55%。我们相信这个新的benchmark数据集将促进更加精确和可靠的穿孔检测模型的发展。数据和代码可以在 https://github.com/kerner-lab/ConeQuest 上获取。
</details></li>
</ul>
<hr>
<h2 id="Review-of-AlexNet-for-Medical-Image-Classification"><a href="#Review-of-AlexNet-for-Medical-Image-Classification" class="headerlink" title="Review of AlexNet for Medical Image Classification"></a>Review of AlexNet for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08655">http://arxiv.org/abs/2311.08655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Arminsbss/tumor-classification">https://github.com/Arminsbss/tumor-classification</a></li>
<li>paper_authors: Wenhao Tang, Junding Sun, Shuihua Wang, Yudong Zhang</li>
<li>for: 这篇论文主要探讨了AlexNet模型在医学图像分类领域的应用和技术细节。</li>
<li>methods: 该论文使用了Dropout技术和ReLU活化函数来避免过拟合和梯度消失问题，并提出了一种基于AlexNet模型的医学图像分类方法。</li>
<li>results: 该论文通过对40篇学术论文和会议论文进行回顾，提出了AlexNet模型的技术细节、优势和应用领域。<details>
<summary>Abstract</summary>
In recent years, the rapid development of deep learning has led to a wide range of applications in the field of medical image classification. The variants of neural network models with ever-increasing performance share some commonalities: to try to mitigate overfitting, improve generalization, avoid gradient vanishing and exploding, etc. AlexNet first utilizes the dropout technique to mitigate overfitting and the ReLU activation function to avoid gradient vanishing. Therefore, we focus our discussion on AlexNet, which has contributed greatly to the development of CNNs in 2012. After reviewing over 40 papers, including journal papers and conference papers, we give a narrative on the technical details, advantages, and application areas of AlexNet.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Refining-Perception-Contracts-Case-Studies-in-Vision-based-Safe-Auto-landing"><a href="#Refining-Perception-Contracts-Case-Studies-in-Vision-based-Safe-Auto-landing" class="headerlink" title="Refining Perception Contracts: Case Studies in Vision-based Safe Auto-landing"></a>Refining Perception Contracts: Case Studies in Vision-based Safe Auto-landing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08652">http://arxiv.org/abs/2311.08652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangge Li, Benjamin C Yang, Yixuan Jia, Daniel Zhuang, Sayan Mitra</li>
<li>for: 该论文旨在评估控制系统中使用机器学习进行感知时的安全性。</li>
<li>methods: 论文使用了合同测试和证明方法来证明总体系统水平安全需求的可行性。</li>
<li>results: 论文通过引入数据和需求导向的改进合同构建算法（DaRePC），得出了可测试的合同，确定了降落在跑道上安全的飞机状态和环境条件，以及通过序列门检测器安全过航的无人机状态。同时也发现了一些可能导致视觉控制系统的安全性问题的条件（例如低地平线的阳光）。<details>
<summary>Abstract</summary>
Perception contracts provide a method for evaluating safety of control systems that use machine learning for perception. A perception contract is a specification for testing the ML components, and it gives a method for proving end-to-end system-level safety requirements. The feasibility of contract-based testing and assurance was established earlier in the context of straight lane keeping: a 3-dimensional system with relatively simple dynamics. This paper presents the analysis of two 6 and 12-dimensional flight control systems that use multi-stage, heterogeneous, ML-enabled perception. The paper advances methodology by introducing an algorithm for constructing data and requirement guided refinement of perception contracts (DaRePC). The resulting analysis provides testable contracts which establish the state and environment conditions under which an aircraft can safety touchdown on the runway and a drone can safely pass through a sequence of gates. It can also discover conditions (e.g., low-horizon sun) that can possibly violate the safety of the vision-based control system.
</details>
<details>
<summary>摘要</summary>
感知合约提供了评估机器学习控制系统安全性的方法。感知合约是测试ML组件的规范，它提供了系统级别的安全要求的证明方法。在推点驱动的情况下，感知合约的可行性已经在三维直线保持问题中被证明。本文分析了使用多 Stage、异构、ML实现的6和12维飞行控制系统。本文提出了一种数据和需求驱动的感知合约构建算法（DaRePC），从而得到了可测试的合约，这些合约确定了飞机在跑道上安全着陆和无人机通过序列门的前提条件。此外，它还可以发现可能违反视觉控制系统安全性的情况（如低地平线的阳光）。
</details></li>
</ul>
<hr>
<h2 id="Painterly-Image-Harmonization-via-Adversarial-Residual-Learning"><a href="#Painterly-Image-Harmonization-via-Adversarial-Residual-Learning" class="headerlink" title="Painterly Image Harmonization via Adversarial Residual Learning"></a>Painterly Image Harmonization via Adversarial Residual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08646">http://arxiv.org/abs/2311.08646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xudong Wang, Li Niu, Junyan Cao, Yan Hong, Liqing Zhang</li>
<li>for: 将 photorealistic 背景图像和绘画风格背景图像进行合成，以实现图像的协调和融合。</li>
<li>methods: 使用对抗学习技术，特别是设计了 dual-encoder 生成器和 pixel-wise 探测器，以bridge 背景和前景特征图像之间的域隔。</li>
<li>results: 实验表明，我们的方法可以实现更加协调、视觉吸引人的结果，比前方法更高效。<details>
<summary>Abstract</summary>
Image compositing plays a vital role in photo editing. After inserting a foreground object into another background image, the composite image may look unnatural and inharmonious. When the foreground is photorealistic and the background is an artistic painting, painterly image harmonization aims to transfer the style of background painting to the foreground object, which is a challenging task due to the large domain gap between foreground and background. In this work, we employ adversarial learning to bridge the domain gap between foreground feature map and background feature map. Specifically, we design a dual-encoder generator, in which the residual encoder produces the residual features added to the foreground feature map from main encoder. Then, a pixel-wise discriminator plays against the generator, encouraging the refined foreground feature map to be indistinguishable from background feature map. Extensive experiments demonstrate that our method could achieve more harmonious and visually appealing results than previous methods.
</details>
<details>
<summary>摘要</summary>
Image compositing 在图像编辑中扮演着关键角色。在插入一个背景图像中的前景对象后，复合图像可能会看起来不自然和不协调。当前景是真实图像，背景是艺术油画时， painterly image harmonization 的目标是将背景油画的风格传递到前景对象中，这是一项具有很大领域差异的任务。在这种情况下，我们使用对抗学习来跨领域 bridge 差异。我们设计了一个 dual-encoder 生成器，其中副encoder 生成的差异特征将被添加到主encoder 生成的前景特征图中。然后，一个像素级别的探测器与生成器进行对抗，以便使得重新处理后的前景特征图与背景特征图无法分辨。我们的方法在详细实验中被证明可以实现更自然和视觉吸引人的结果，比前方法更好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.CV_2023_11_15/" data-id="clpahu75o00r13h881gutab7i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.AI_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T12:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.AI_2023_11_15/">cs.AI - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="HAL-9000-Skynet’s-Risk-Manager"><a href="#HAL-9000-Skynet’s-Risk-Manager" class="headerlink" title="HAL 9000: Skynet’s Risk Manager"></a>HAL 9000: Skynet’s Risk Manager</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09449">http://arxiv.org/abs/2311.09449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tadeu Freitas, Mário Neto, Inês Dutra, João Soares, Manuel Correia, Rolando Martins<br>for:这种论文是为了提出一种基于现代技术的攻击快照系统（ITS）体系，以提高ITS的入侵忍受能力和适应新敌人。methods:该论文使用了机器学习（ML）算法来帮助ITS学习从以往攻击和已知漏洞中，以增强其入侵忍受能力。它还提出了一种基于现代技术的风险管理器设计，通过自动评估操作系统（OS）的风险，提供更安全的配置建议。results:实验表明，使用Skynet和HAL 9000设计可以降低成功入侵的可能性，并且HAL可以选择15%更安全的配置，比现有的风险管理器更高效。<details>
<summary>Abstract</summary>
Intrusion Tolerant Systems (ITSs) are a necessary component for cyber-services/infrastructures. Additionally, as cyberattacks follow a multi-domain attack surface, a similar defensive approach should be applied, namely, the use of an evolving multi-disciplinary solution that combines ITS, cybersecurity and Artificial Intelligence (AI). With the increased popularity of AI solutions, due to Big Data use-case scenarios and decision support and automation scenarios, new opportunities to apply Machine Learning (ML) algorithms have emerged, namely ITS empowerment. Using ML algorithms, an ITS can augment its intrusion tolerance capability, by learning from previous attacks and from known vulnerabilities. As such, this work's contribution is twofold: (1) an ITS architecture (Skynet) based on the state-of-the-art and incorporates new components to increase its intrusion tolerance capability and its adaptability to new adversaries; (2) an improved Risk Manager design that leverages AI to improve ITSs by automatically assessing OS risks to intrusions, and advise with safer configurations. One of the reasons that intrusions are successful is due to bad configurations or slow adaptability to new threats. This can be caused by the dependency that systems have for human intervention. One of the characteristics in Skynet and HAL 9000 design is the removal of human intervention. Being fully automatized lowers the chance of successful intrusions caused by human error. Our experiments using Skynet, shows that HAL is able to choose 15% safer configurations than the state-of-the-art risk manager.
</details>
<details>
<summary>摘要</summary>
干扰快照系统（ITS）是现代网络服务/基础设施的必需组件。此外，由于攻击者通常会利用多个领域进行攻击，因此应采取相应的防御策略，即结合ITS、网络安全和人工智能（AI）的演化多学科解决方案。随着人工智能解决方案的普及，特别是基于大数据和决策支持自动化场景，新的机会出现了，可以使用机器学习（ML）算法来实现ITS的增强。通过ML算法，ITS可以从前一次攻击和已知漏洞中学习增强其抗侵入能力。这项工作的贡献有两个方面：1. 基于当前最佳实践的ITS架构（Skynet），新增了增强抗侵入能力和适应新敌人的功能。2. 基于人工智能自动评估系统（HAL 9000），提高了ITS的风险管理，自动评估操作系统的风险，并提供更安全的配置。一个常见的攻击成功原因是因为系统的坏配置或慢速应对新威胁。这可能是由系统的人工参与引起的。Skynet和HAL 9000的设计中消除了人工参与，它们是完全自动化的，降低了由人类错误引起的成功攻击的可能性。我们对Skynet进行了实验，发现HAL可以比当前状态艺术风险管理选择15%更安全的配置。
</details></li>
</ul>
<hr>
<h2 id="How-Trustworthy-are-Open-Source-LLMs-An-Assessment-under-Malicious-Demonstrations-Shows-their-Vulnerabilities"><a href="#How-Trustworthy-are-Open-Source-LLMs-An-Assessment-under-Malicious-Demonstrations-Shows-their-Vulnerabilities" class="headerlink" title="How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities"></a>How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09447">http://arxiv.org/abs/2311.09447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun</li>
<li>for: 本研究旨在评估开源大语言模型（LLMs）的可靠性，检测其在8个方面，包括恶意、偏见、伦理、幻觉、公平、奴役、隐私和对抗示范攻击的可靠性。</li>
<li>methods: 我们提出了一种基于Chain of Utterances（CoU）的提示策略，通过针对性地制作恶意示范来检测模型的可靠性。我们对当今代表性的开源LLMs进行了广泛的实验，包括Vicuna、MPT、Falcon、Mistral和Llama 2。</li>
<li>results: 我们的实验结果表明，我们的攻击策略在多个方面具有效果，而且模型的性能在普通NLP任务上高不一定意味着它们具有更高的可靠性。此外，我们发现，受过 instrucion tuning 的模型更容易受到攻击，而 fine-tuning LLMs for safety alignment 可以减轻对抗式可靠性攻击的影响。<details>
<summary>Abstract</summary>
The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose an enhanced Chain of Utterances-based (CoU) prompting strategy by incorporating meticulously crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.
</details>
<details>
<summary>摘要</summary>
开源大语言模型（LLM）的快速进步在人工智能发展中发挥着重要作用。然而，对这些模型的可靠性仍然具有有限的理解。在大规模部署过程中，如果不具备足够的可靠性，可能会产生严重的风险。在这项工作中，我们对开源LLM进行了可靠性评估，对其进行了八个方面的检查，包括恶意、偏见、伦理、幻觉、公平、追随、隐私和对抗攻击的Robustness。我们提出了基于Chain of Utterances（CoU）的增强的提示策略，通过针对可靠性攻击的精心制作的假示例进行检测。我们的广泛实验包括当前和代表性的开源LLM系列，包括Vicuna、MPT、Falcon、Mistral和Llama 2。实验结果证明了我们的攻击策略在多个方面的有效性。更有趣的是，我们的结果分析发现，在普通的NLPT任务中表现出色的模型并不总是具有最高的可靠性；事实上，更大的模型可能会更容易受到攻击。此外，通过专门准备Instruction Following的模型，即模型偏好遵循指令，可能会更容易受到攻击，而通过安全对齐来平衡攻击的可靠性攻击。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Privacy-Energy-Consumption-Tradeoff-for-Split-Federated-Learning"><a href="#Exploring-the-Privacy-Energy-Consumption-Tradeoff-for-Split-Federated-Learning" class="headerlink" title="Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning"></a>Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09441">http://arxiv.org/abs/2311.09441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joohyung Lee, Mohamed Seif, Jungchan Cho, H. Vincent Poor<br>for:This paper focuses on Split Federated Learning (SFL) and its impact on energy consumption and privacy.methods:The paper analyzes the influence of system parameters on the selection of the cut layer in SFL and provides an illustrative example of cut layer selection to minimize the risk of clients reconstructing raw data while sustaining energy consumption within a required budget.results:The paper discusses the challenges of cut layer selection in SFL and provides a comprehensive overview of the SFL process, taking into account the impact of various system parameters on energy consumption and privacy. Additionally, the paper addresses open challenges in this field and identifies promising avenues for future research and development, particularly in the context of 6G technology.<details>
<summary>Abstract</summary>
Split Federated Learning (SFL) has recently emerged as a promising distributed learning technology, leveraging the strengths of both federated learning and split learning. It emphasizes the advantages of rapid convergence while addressing privacy concerns. As a result, this innovation has received significant attention from both industry and academia. However, since the model is split at a specific layer, known as a cut layer, into both client-side and server-side models for the SFL, the choice of the cut layer in SFL can have a substantial impact on the energy consumption of clients and their privacy, as it influences the training burden and the output of the client-side models. Moreover, the design challenge of determining the cut layer is highly intricate, primarily due to the inherent heterogeneity in the computing and networking capabilities of clients. In this article, we provide a comprehensive overview of the SFL process and conduct a thorough analysis of energy consumption and privacy. This analysis takes into account the influence of various system parameters on the cut layer selection strategy. Additionally, we provide an illustrative example of the cut layer selection, aiming to minimize the risk of clients from reconstructing the raw data at the server while sustaining energy consumption within the required energy budget, which involve trade-offs. Finally, we address open challenges in this field including their applications to 6G technology. These directions represent promising avenues for future research and development.
</details>
<details>
<summary>摘要</summary>
Split Federated Learning (SFL) 是一种最近崛起的分布式学习技术，结合 federated learning 和 split learning 的优势，强调快速收敛和隐私问题的处理。因此，这一创新在行业和学术界都受到了广泛的关注。然而，在 SFL 中选择 cut layer 可能对客户端的能 consumption 和隐私有很大的影响，因为它影响了客户端的训练负担和输出。此外，选择 cut layer 的设计挑战很大，主要由客户端的计算和网络能力的不同而导致的约束。在本文中，我们提供了 SFL 的全面概述，并进行了严格的能 consumption 和隐私分析。这种分析考虑了各种系统参数对 cut layer 选择策略的影响。此外，我们还提供了一个例子，以减少客户端从服务器重建原始数据的风险，同时保持在必要的能 consumption 范围内，这些决策涉及到了负面的负担。最后，我们讨论了当前领域的开放挑战，包括它们在 6G 技术中的应用。这些方向表示未来研发的有前途。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Activation-Attack-Attack-Large-Language-Models-using-Activation-Steering-for-Safety-Alignment"><a href="#Backdoor-Activation-Attack-Attack-Large-Language-Models-using-Activation-Steering-for-Safety-Alignment" class="headerlink" title="Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment"></a>Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09433">http://arxiv.org/abs/2311.09433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Wang, Kai Shu</li>
<li>for: 这 paper 是为了研究 instruction-tuned Large Language Models (LLMs) 的安全性，具体来说是研究这些模型在不同的安全任务上的可控性。</li>
<li>methods: 这 paper 使用了一种新的攻击框架，叫做 Backdoor Activation Attack，它可以在 LLMs 的活动层中插入恶意导向 вектор。</li>
<li>results: 实验结果表明，该方法可以高效地启动攻击，并且增加了非常小的负担。此外， paper 还讨论了对这种活动攻击的可能的防御措施。<details>
<summary>Abstract</summary>
To ensure AI safety, instruction-tuned Large Language Models (LLMs) are specifically trained to ensure alignment, which refers to making models behave in accordance with human intentions. While these models have demonstrated commendable results on various safety benchmarks, the vulnerability of their safety alignment has not been extensively studied. This is particularly troubling given the potential harm that LLMs can inflict. Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts. These approaches compromise the stealthiness and generalizability of the attacks, making them susceptible to detection. Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications. In this work, we introduce a novel attack framework, called Backdoor Activation Attack, which injects trojan steering vectors into the activation layers of LLMs. These malicious steering vectors can be triggered at inference time to steer the models toward attacker-desired behaviors by manipulating their activations. In particular, the steering vectors are generated by taking the difference between benign and malicious activations. Then, the most effective steering vector is selected and added to the forward passes of the LLMs. Our experiment results on four primary alignment tasks show that our proposed method is highly effective and adds little or no overhead to attack efficiency. Additionally, we discuss potential countermeasures against such activation attacks. Our code and data are available at https://email-haoran-for-link. Warning: this paper contains content that can be offensive or upsetting.
</details>
<details>
<summary>摘要</summary>
为确保人工智能安全，特定的大语言模型（LLMs）被专门训练，以确保它们的对应性，即让模型按照人类意图进行行为。尽管这些模型在不同的安全标准上表现出色，但它们的安全对应性还没有得到广泛的研究。这特别具有威胁性，因为这些模型可能会造成严重的损害。现有的攻击方法通常利用恶意训练数据或插入恶意提示来攻击LLMs。这些方法会增加攻击的隐蔽性和通用性，使其易于检测。另外，这些模型通常需要巨大的计算资源来实现，使其在实际应用中不太实际。在这种情况下，我们引入了一种新的攻击框架，called Backdoor Activation Attack，它可以在LLMs中插入恶意导向 вектор。这些恶意导向 вектор可以在推理时被触发，以使模型按照攻击者所需的方向进行行为。具体来说，这些恶意导向 вектор是通过比较善意和恶意的激活值而生成的。然后，选择最有效的导向 вектор，并将其添加到LLMs的前向传输中。我们的实验结果表明，我们的提posed方法在四个主要对应任务上都具有非常高的效果，并且增加了非常少的负载。此外，我们还讨论了对这种激活攻击的可能的防御措施。我们的代码和数据可以在https://email-haoran-for-link中找到。注意：这篇论文可能包含不适或不适的内容。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Detection-Unveiling-Fairness-Vulnerabilities-in-Abusive-Language-Models"><a href="#Beyond-Detection-Unveiling-Fairness-Vulnerabilities-in-Abusive-Language-Models" class="headerlink" title="Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models"></a>Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09428">http://arxiv.org/abs/2311.09428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueqing Liang, Lu Cheng, Ali Payani, Kai Shu</li>
<li>for: 本研究探讨了针对恶意语言检测模型的不公正性和检测性能的攻击性能，以提高模型的公正性稳定性。</li>
<li>methods: 本研究提出了一个简单 yet effective的框架 FABLE，通过利用后门攻击来实现对公正性和检测性能的Targeted控制。 FABLE 探讨了三种触发设计（i.e., 罕见、人工和自然触发）以及新的采样策略。</li>
<li>results: 实验结果表明，FABLE 可以成功地攻击恶意语言检测模型的公正性和实用性。<details>
<summary>Abstract</summary>
This work investigates the potential of undermining both fairness and detection performance in abusive language detection. In a dynamic and complex digital world, it is crucial to investigate the vulnerabilities of these detection models to adversarial fairness attacks to improve their fairness robustness. We propose a simple yet effective framework FABLE that leverages backdoor attacks as they allow targeted control over the fairness and detection performance. FABLE explores three types of trigger designs (i.e., rare, artificial, and natural triggers) and novel sampling strategies. Specifically, the adversary can inject triggers into samples in the minority group with the favored outcome (i.e., ``non-abusive'') and flip their labels to the unfavored outcome, i.e., ``abusive''. Experiments on benchmark datasets demonstrate the effectiveness of FABLE attacking fairness and utility in abusive language detection.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-Large-Language-Models-contradict-humans-Large-Language-Models’-Sycophantic-Behaviour"><a href="#When-Large-Language-Models-contradict-humans-Large-Language-Models’-Sycophantic-Behaviour" class="headerlink" title="When Large Language Models contradict humans? Large Language Models’ Sycophantic Behaviour"></a>When Large Language Models contradict humans? Large Language Models’ Sycophantic Behaviour</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09410">http://arxiv.org/abs/2311.09410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Ranaldi, Giulia Pucci</li>
<li>for: 这篇论文探讨了大语言模型（LLMs）在解决复杂任务时的可能性，以及人类反馈对其回答的影响。</li>
<li>methods: 该论文使用了不同任务的人类影响提示，以探讨 LLMS 是否受到 sycophancy 行为的影响。</li>
<li>results: 研究发现，当 LLMS 回答Subjective 意见和基于事实应该提供相反回答的问题时，它们往往表现出 sycophancy 倾向，表明它们缺乏坚实性和可靠性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have been demonstrating the ability to solve complex tasks by delivering answers that are positively evaluated by humans due in part to the intensive use of human feedback that refines responses. However, the suggestibility transmitted through human feedback increases the inclination to produce responses that correspond to the user's beliefs or misleading prompts as opposed to true facts, a behaviour known as sycophancy. This phenomenon decreases the bias, robustness, and, consequently, their reliability.   In this paper, we shed light on the suggestibility of LLMs to sycophantic behaviour, demonstrating these tendencies via human-influenced prompts over different tasks. Our investigation reveals that LLMs show sycophantic tendencies when responding to queries involving subjective opinions and statements that should elicit a contrary response based on facts, demonstrating a lack of robustness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Zero-Shot-Relational-Learning-on-Temporal-Knowledge-Graphs-with-Large-Language-Models"><a href="#Zero-Shot-Relational-Learning-on-Temporal-Knowledge-Graphs-with-Large-Language-Models" class="headerlink" title="Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models"></a>Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10112">http://arxiv.org/abs/2311.10112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zifeng Ding, Heling Cai, Jingpei Wu, Yunpu Ma, Ruotong Liao, Bo Xiong, Volker Tresp</li>
<li>for: 提高模型对非常见关系的预测能力</li>
<li>methods: 利用自然语言模型生成关系表示，然后将其引入嵌入式TKGF方法中</li>
<li>results: 实验结果表明，我们的方法可以帮助TKGF模型在预测未看过的关系方面提高表现，而且仍然保持预测已经看过的关系的能力<details>
<summary>Abstract</summary>
In recent years, modeling evolving knowledge over temporal knowledge graphs (TKGs) has become a heated topic. Various methods have been proposed to forecast links on TKGs. Most of them are embedding-based, where hidden representations are learned to represent knowledge graph (KG) entities and relations based on the observed graph contexts. Although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they naturally face a strong challenge when they are asked to model the unseen zero-shot relations that has no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic meanings stay close in the embedding space, enabling TKGF models to recognize zero-shot relations even without any observed graph context. Experimental results show that our approach helps TKGF models to achieve much better performance in forecasting the facts with previously unseen relations, while still maintaining their ability in link forecasting regarding seen relations.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:最近几年，模型知识演化的研究在时态知识图(TKG)上得到了广泛的关注。许多方法被提出来预测TKG上的链接。大多数方法是基于嵌入的，其中隐藏表示被学习以表示知识图实体和关系，基于观察的图文本上。although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they naturally face a strong challenge when they are asked to model the unseen zero-shot relations that have no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic meanings stay close in the embedding space, enabling TKGF models to recognize zero-shot relations even without any observed graph context. Experimental results show that our approach helps TKGF models to achieve much better performance in forecasting the facts with previously unseen relations, while still maintaining their ability in link forecasting regarding seen relations.
</details></li>
</ul>
<hr>
<h2 id="LOKE-Linked-Open-Knowledge-Extraction-for-Automated-Knowledge-Graph-Construction"><a href="#LOKE-Linked-Open-Knowledge-Extraction-for-Automated-Knowledge-Graph-Construction" class="headerlink" title="LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction"></a>LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09366">http://arxiv.org/abs/2311.09366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamie McCusker</li>
<li>for: 本研究旨在提高知识图建构（KGC）中Open Information Extraction（Open IE）的效果，通过使用大语言模型（LLM）和提示工程（Prompt Engineering）来提高知识图的建构。</li>
<li>methods: 本研究使用GPT模型和提示工程来实现Open Knowledge Extraction（OKE），并提出了一种Linked Open Knowledge Extractor（LOKE）来解决相似的问题。</li>
<li>results: 研究发现，一个WellEngineered提示，配置了Naive entity linking方法（LOKE-GPT），可以超过AllenAI的OpenIE 4实现在OKE任务上的性能，尽管它生成了比参照集更多的 triple。此外，研究还发现，LOKE-GPT和”银” TekGen triple 表明任务的内容和结构都与OIE有很大差异。<details>
<summary>Abstract</summary>
While the potential of Open Information Extraction (Open IE) for Knowledge Graph Construction (KGC) may seem promising, we find that the alignment of Open IE extraction results with existing knowledge graphs to be inadequate. The advent of Large Language Models (LLMs), especially the commercially available OpenAI models, have reset expectations for what is possible with deep learning models and have created a new field called prompt engineering. We investigate the use of GPT models and prompt engineering for knowledge graph construction with the Wikidata knowledge graph to address a similar problem to Open IE, which we call Open Knowledge Extraction (OKE) using an approach we call the Linked Open Knowledge Extractor (LOKE, pronounced like "Loki"). We consider the entity linking task essential to construction of real world knowledge graphs. We merge the CaRB benchmark scoring approach with data from the TekGen dataset for the LOKE task. We then show that a well engineered prompt, paired with a naive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's OpenIE 4 implementation on the OKE task, although it over-generates triples compared to the reference set due to overall triple scarcity in the TekGen set. Through an analysis of entity linkability in the CaRB dataset, as well as outputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the "silver" TekGen triples show that the task is significantly different in content from OIE, if not structure. Through this analysis and a qualitative analysis of sentence extractions via all methods, we found that LOKE-GPT extractions are of high utility for the KGC task and suitable for use in semi-automated extraction settings.
</details>
<details>
<summary>摘要</summary>
原文：尽管开放信息EXTRACTION（Open IE）的潜力对知识图构建（KGC）似乎吸引人，但我们发现将Open IE提取结果与现有知识图进行对应是不够的。商业可用的大语言模型（LLM），特别是OpenAI模型，使得深度学习模型的期望得到了新的提升，并创造了一个新的领域called prompt engineering。我们调查了GPT模型和提示工程在知识图构建中的应用，使用Wikidata知识图来解决类似于Open IE的问题，我们称之为开放知识EXTRACTION（OKE）。我们认为实体链接任务是构建现实世界知识图的关键。我们将CaRB评估方法与数据集的TekGen结合，并显示了一个WellEngineered的提示，与Naive实体链接方法（我们称之为LOKE-GPT）在OKE任务上表现出色，虽然它生成的 triple 比参考集多，但是总体来说 triple 在TekGen集中的缺失导致了这种情况。通过CaRB数据集中实体链接性的分析，以及OpenIE 4和LOKE-GPT的输出，我们发现LOKE-GPT和“银”TekGen triple 表明任务的内容和结构都与OIE不同。通过这种分析和所有方法的 качеitative分析，我们发现LOKE-GPT提取是KGC任务中的高Utility和可以用于半自动提取设置。
</details></li>
</ul>
<hr>
<h2 id="Empirical-evaluation-of-Uncertainty-Quantification-in-Retrieval-Augmented-Language-Models-for-Science"><a href="#Empirical-evaluation-of-Uncertainty-Quantification-in-Retrieval-Augmented-Language-Models-for-Science" class="headerlink" title="Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science"></a>Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09358">http://arxiv.org/abs/2311.09358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pnnl/expert2">https://github.com/pnnl/expert2</a></li>
<li>paper_authors: Sridevi Wagle, Sai Munikoti, Anurag Acharya, Sara Smith, Sameera Horawalavithana</li>
<li>for: 这个研究的目的是evaluating uncertainty quantification (UQ) in Retrieval Augmented Language Models (RALMs) for scientific tasks.</li>
<li>methods: 该研究使用了一种已有的RALM模型，并在其基础上进行了训练和测试，以评估模型在科学任务中的可靠性和准确性。</li>
<li>results: 研究发现，当用科学知识作为预训练和检索数据时，模型具有更高的自信心，但同时也更容易产生错误的预测。此外，模型在准确预测和错误预测之间的自信心差异不会减轻这个问题。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown remarkable achievements in natural language processing tasks, producing high-quality outputs. However, LLMs still exhibit limitations, including the generation of factually incorrect information. In safety-critical applications, it is important to assess the confidence of LLM-generated content to make informed decisions. Retrieval Augmented Language Models (RALMs) is relatively a new area of research in NLP. RALMs offer potential benefits for scientific NLP tasks, as retrieved documents, can serve as evidence to support model-generated content. This inclusion of evidence enhances trustworthiness, as users can verify and explore the retrieved documents to validate model outputs. Quantifying uncertainty in RALM generations further improves trustworthiness, with retrieved text and confidence scores contributing to a comprehensive and reliable model for scientific applications. However, there is limited to no research on UQ for RALMs, particularly in scientific contexts. This study aims to address this gap by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific tasks. This research investigates how uncertainty scores vary when scientific knowledge is incorporated as pretraining and retrieval data and explores the relationship between uncertainty scores and the accuracy of model-generated outputs. We observe that an existing RALM finetuned with scientific knowledge as the retrieval data tends to be more confident in generating predictions compared to the model pretrained only with scientific knowledge. We also found that RALMs are overconfident in their predictions, making inaccurate predictions more confidently than accurate ones. Scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue. We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）在自然语言处理任务中表现出色，生成高质量输出。然而，LLM仍存在一些限制，包括生成不准确的信息。在安全关键应用中，需要评估LLM生成的内容的可靠性，以做出 Informed 决策。Retrieval Augmented Language Models（RALM）是一个相对新的研究领域，它们可以提供可靠的科学 NLP 任务。RALM 的可靠性可以通过文档检索来提高，文档可以作为生成内容的证据，让用户可以验证和探索文档来验证模型的输出。在 RALM 生成时，量化不确定性可以进一步提高可靠性，文档检索结果和信任分数共同组成一个可靠和可靠的模型。然而，对于 RALM 的 UQ 研究尚存在很大的空白，特别是在科学上。本研究希望填补这一空白，通过对 RALM 的 UQ 进行全面评估，专注于科学任务。本研究研究了 RALM 在科学任务中 uncertainty 分布的变化，以及模型生成输出的准确率和不确定性之间的关系。我们发现，当将科学知识作为检索数据和预训练数据时，RALM 会更加自信地生成预测结果。此外，我们发现 RALM 会过于自信，即在生成更多的不准确预测结果。科学知识作为预训练数据或检索数据不能够解决这一问题。我们将代码、数据和仪表分享在 GitHub 上，请参考 <https://github.com/pnnl/EXPERT2>。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Threats-in-Stable-Diffusion-Models"><a href="#Privacy-Threats-in-Stable-Diffusion-Models" class="headerlink" title="Privacy Threats in Stable Diffusion Models"></a>Privacy Threats in Stable Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09355">http://arxiv.org/abs/2311.09355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Cilloni, Charles Fleming, Charles Walter</li>
<li>for: 这篇论文旨在攻击稳定扩散计算机视觉模型中的成员推论攻击（MIA），具体是针对高度进步的稳定扩散V2模型（StabilityAI）。</li>
<li>methods: 我们使用了黑盒攻击方法，只需要重复地询问受到攻击的模型。我们的方法包括观察稳定扩散模型在不同生成epoch时的输出，并训练一个分类模型来区别生成结果是否来自训练数据集。</li>
<li>results: 我们使用ROC AUC方法评估攻击的有效率，获得60%的成功率，即可以从稳定扩散模型的输出中推断出训练数据集的成员信息。<details>
<summary>Abstract</summary>
This paper introduces a novel approach to membership inference attacks (MIA) targeting stable diffusion computer vision models, specifically focusing on the highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract sensitive information about a model's training data, posing significant privacy concerns. Despite its advancements in image synthesis, our research reveals privacy vulnerabilities in the stable diffusion models' outputs. Exploiting this information, we devise a black-box MIA that only needs to query the victim model repeatedly. Our methodology involves observing the output of a stable diffusion model at different generative epochs and training a classification model to distinguish when a series of intermediates originated from a training sample or not. We propose numerous ways to measure the membership features and discuss what works best. The attack's efficacy is assessed using the ROC AUC method, demonstrating a 60\% success rate in inferring membership information. This paper contributes to the growing body of research on privacy and security in machine learning, highlighting the need for robust defenses against MIAs. Our findings prompt a reevaluation of the privacy implications of stable diffusion models, urging practitioners and developers to implement enhanced security measures to safeguard against such attacks.
</details>
<details>
<summary>摘要</summary>
Our methodology involves observing the output of a stable diffusion model at different generative epochs and training a classification model to distinguish when a series of intermediates originated from a training sample or not. We propose numerous ways to measure the membership features and discuss what works best. The attack's efficacy is assessed using the ROC AUC method, demonstrating a 60% success rate in inferring membership information.This paper contributes to the growing body of research on privacy and security in machine learning, highlighting the need for robust defenses against MIAs. Our findings prompt a reevaluation of the privacy implications of stable diffusion models, urging practitioners and developers to implement enhanced security measures to safeguard against such attacks.
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Imitation-Learning-Through-Pre-Trained-Representations"><a href="#Generalizable-Imitation-Learning-Through-Pre-Trained-Representations" class="headerlink" title="Generalizable Imitation Learning Through Pre-Trained Representations"></a>Generalizable Imitation Learning Through Pre-Trained Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09350">http://arxiv.org/abs/2311.09350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Di Chang, Francois Hogan, David Meger, Gregory Dudek</li>
<li>for: 提高imitaiton learning政策的通用能力</li>
<li>methods: 利用自动Supervised vision transformer模型和其自然的 semantic能力来提高imitaiton learning政策的通用能力</li>
<li>results: 通过 clustering appearance features into semantic concepts, our method obtains better generalization across a wide range of appearance variations and object types, and demonstrates generalized behavior in a diverse dataset of object manipulation tasks.<details>
<summary>Abstract</summary>
In this paper we leverage self-supervised vision transformer models and their emergent semantic abilities to improve the generalization abilities of imitation learning policies. We introduce BC-ViT, an imitation learning algorithm that leverages rich DINO pre-trained Visual Transformer (ViT) patch-level embeddings to obtain better generalization when learning through demonstrations. Our learner sees the world by clustering appearance features into semantic concepts, forming stable keypoints that generalize across a wide range of appearance variations and object types. We show that this representation enables generalized behaviour by evaluating imitation learning across a diverse dataset of object manipulation tasks. Our method, data and evaluation approach are made available to facilitate further study of generalization in Imitation Learners.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们利用自我supervised的视觉变换器模型和其自然而然的semantic能力来提高便函式学习策略的总体化能力。我们提出了BC-ViT算法，它利用丰富的DINO预训练的视觉变换（ViT）质点嵌入来获得更好的总体化，从示例学习中学习出更好的行为。我们的学习者通过对外观特征的归一化来形成稳定的键点，这些键点可以在各种外观变化和物体类型上广泛generalize。我们示示了这种表示能够实现总体化的行为，我们的方法、数据和评估方法都被提供，以便进一步研究便函式学习的总体化。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-Based-Probabilistic-Constellation-Shaping-With-Diffusion-Models"><a href="#Generative-AI-Based-Probabilistic-Constellation-Shaping-With-Diffusion-Models" class="headerlink" title="Generative AI-Based Probabilistic Constellation Shaping With Diffusion Models"></a>Generative AI-Based Probabilistic Constellation Shaping With Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09349">http://arxiv.org/abs/2311.09349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Letafati, Samad Ali, Matti Latva-aho</li>
<li>for: 本研究旨在探讨 diffusion models 在通信工程应用中的潜在优势，以提高信息率和解码性能。</li>
<li>methods: 我们利用 denoising diffusion probabilistic models (DDPM) 的“净化并生成”特点进行 probabilistic constellation shaping。</li>
<li>results: 我们的方法比深度神经网络 (DNN) 参考方法和均匀设定更好，并且在低 SNR 条件下和非泊 distributions 下提供了网络可靠性和 Robust out-of-distribution 性能。数值评估表明，我们的方法在 64-QAM 几何中提高了cosine similarity 30%，并三倍提高了相互信息。<details>
<summary>Abstract</summary>
Diffusion models are at the vanguard of generative AI research with renowned solutions such as ImageGen by Google Brain and DALL.E 3 by OpenAI. Nevertheless, the potential merits of diffusion models for communication engineering applications are not fully understood yet. In this paper, we aim to unleash the power of generative AI for PHY design of constellation symbols in communication systems. Although the geometry of constellations is predetermined according to networking standards, e.g., quadrature amplitude modulation (QAM), probabilistic shaping can design the probability of occurrence (generation) of constellation symbols. This can help improve the information rate and decoding performance of communication systems. We exploit the ``denoise-and-generate'' characteristics of denoising diffusion probabilistic models (DDPM) for probabilistic constellation shaping. The key idea is to learn generating constellation symbols out of noise, ``mimicking'' the way the receiver performs symbol reconstruction. This way, we make the constellation symbols sent by the transmitter, and what is inferred (reconstructed) at the receiver become as similar as possible, resulting in as few mismatches as possible. Our results show that the generative AI-based scheme outperforms deep neural network (DNN)-based benchmark and uniform shaping, while providing network resilience as well as robust out-of-distribution performance under low-SNR regimes and non-Gaussian assumptions. Numerical evaluations highlight 30% improvement in terms of cosine similarity and a threefold improvement in terms of mutual information compared to DNN-based approach for 64-QAM geometry.
</details>
<details>
<summary>摘要</summary>
Diffusion models 是生成人工智能研究的先锋之一，其中包括Google Brain 的 ImageGen 和 OpenAI 的 DALL.E 3。然而，对于通信工程应用的 diffusion models 的潜在优点还没有得到充分了解。在这篇论文中，我们想使用生成人工智能来设计物理设计符号（PHY）在通信系统中。尽管constellation 的几何结构根据网络标准固定，例如 quadrature amplitude modulation（QAM），但可以通过概率形成来设计 constellation 符号的概率出现。这可以帮助提高通信系统的信息率和解码性能。我们利用 denoising diffusion probabilistic models（DDPM）的“denoise-and-generate”特性来进行概率形成。我们的关键思想是通过学习生成 constellation 符号来“模仿”接收器在重建符号时的过程。这样，我们可以使得发送者发送的 constellation 符号和接收器重建的符号变得非常相似，从而减少偏差。我们的结果表明，基于生成人工智能的方案在低 SNR 下和非泊然分布下表现出了更好的网络鲁棒性和robust out-of-distribution性，并且在 64-QAM 几何上实现了30%的圆拟相似性和三倍的相互信息相比于 DNN-based 方法。数值评估表明，在低 SNR 下和非泊然分布下，生成人工智能的方案可以实现30%的圆拟相似性和三倍的相互信息相比于 DNN-based 方法。
</details></li>
</ul>
<hr>
<h2 id="VideoCon-Robust-Video-Language-Alignment-via-Contrast-Captions"><a href="#VideoCon-Robust-Video-Language-Alignment-via-Contrast-Captions" class="headerlink" title="VideoCon: Robust Video-Language Alignment via Contrast Captions"></a>VideoCon: Robust Video-Language Alignment via Contrast Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10111">http://arxiv.org/abs/2311.10111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hritikbansal/videocon">https://github.com/hritikbansal/videocon</a></li>
<li>paper_authors: Hritik Bansal, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang, Aditya Grover</li>
<li>for: 该研究目标是提高现有视频语言对齐模型的强度，使其能够承受Semantically plausible contrastive changes在视频标题中。</li>
<li>methods: 该研究使用了一种新的视频语言对齐数据集，即VideoCon，该数据集由一个大型自然语言模型生成了可能的对比视频标题和解释。然后，研究者使用了VideoCon来训练一个生成型视频语言对齐模型，以评估视频语言相似性和生成解释。</li>
<li>results: 研究发现，VideoCon-based alignment model在人工生成的对比标题上表现出色，与现有模型相比，其AUC提高了12点。此外，该模型在无预训练的情况下在视频语言任务中（如SSv2-Temporal和ATP-Hard）达到了新的最佳性能，并在人工编写的标题和解释上也表现出优异。<details>
<summary>Abstract</summary>
Despite being (pre)trained on a massive amount of data, state-of-the-art video-language alignment models are not robust to semantically-plausible contrastive changes in the video captions. Our work addresses this by identifying a broad spectrum of contrast misalignments, such as replacing entities, actions, and flipping event order, which alignment models should be robust against. To this end, we introduce the VideoCon, a video-language alignment dataset constructed by a large language model that generates plausible contrast video captions and explanations for differences between original and contrast video captions. Then, a generative video-language model is finetuned with VideoCon to assess video-language entailment and generate explanations. Our VideoCon-based alignment model significantly outperforms current models. It exhibits a 12-point increase in AUC for the video-language alignment task on human-generated contrast captions. Finally, our model sets new state of the art zero-shot performance in temporally-extensive video-language tasks such as text-to-video retrieval (SSv2-Temporal) and video question answering (ATP-Hard). Moreover, our model shows superior performance on novel videos and human-crafted captions and explanations. Our code and data are available at https://github.com/Hritikbansal/videocon.
</details>
<details>
<summary>摘要</summary>
尽管使用大量数据进行（先）训练，当前最佳的视频语言对接模型并不能抗击Semantically plausible contrastive changes in the video captions。我们的工作是解决这个问题，我们识别了广泛的对接不一致，如替换实体、动作和事件顺序的flipping。为了实现这一目标，我们开发了 VideoCon，一个由大型语言模型生成的视频语言对接集合，其中包含了可能的对接视频标签和解释。然后，我们使用 VideoCon 进行finetuning，以评估视频语言关系和生成解释。我们的 VideoCon-based alignment model 在人工生成的对接标签上显示了12点的提升，而且在 temporally-extensive video-language tasks 中也达到了新的最佳无Zero-shot表现（SSv2-Temporal和ATP-Hard）。此外，我们的模型在新的视频和人工生成的标签和解释上也表现出了superior的性能。我们的代码和数据可以在 https://github.com/Hritikbansal/videocon 上获取。
</details></li>
</ul>
<hr>
<h2 id="Lighter-yet-More-Faithful-Investigating-Hallucinations-in-Pruned-Large-Language-Models-for-Abstractive-Summarization"><a href="#Lighter-yet-More-Faithful-Investigating-Hallucinations-in-Pruned-Large-Language-Models-for-Abstractive-Summarization" class="headerlink" title="Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization"></a>Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09335">http://arxiv.org/abs/2311.09335</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Chrysostomou, Zhixue Zhao, Miles Williams, Nikolaos Aletras</li>
<li>for:  investigate the effect of pruning on hallucinations in abstractive summarization with large language models (LLMs)</li>
<li>methods: pruning techniques to reduce model size, three instruction-tuned LLMs, three hallucination evaluation metrics</li>
<li>results: pruned LLMs hallucinate less compared to full-sized counterparts, greater dependency on source input leads to higher lexical overlap between generated content and source inputHere is the summary in Traditional Chinese text:</li>
<li>for: 研究对于摘要 Summarization 中大型语言模型（LLMs）的剪裁效果</li>
<li>methods: 剪裁技术来缩小模型大小，三个受训 LLMs，三个hallucination评估指标</li>
<li>results: 剪裁 LLMs 比起全大小模型少了hallucination，依赖源输入更加强，导致生成内容与源输入之间的字词相似度更高<details>
<summary>Abstract</summary>
Despite their remarkable performance on abstractive summarization, large language models (LLMs) face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode the reliability of LLMs and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights to create sparse models that enable more efficient inference. Pruned models yield comparable performance to their counterpart full-sized models, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study on the hallucinations produced by pruned models across three standard summarization tasks, two pruning approaches, three instruction-tuned LLMs, and three hallucination evaluation metrics. Surprisingly, we find that pruned LLMs hallucinate less compared to their full-sized counterparts. Our follow-up analysis suggests that pruned models tend to depend more on the source input and less on their parametric knowledge from pre-training for generation. This greater dependency on the source input leads to a higher lexical overlap between generated content and the source input, which can be a reason for the reduction in hallucinations.
</details>
<details>
<summary>摘要</summary>
尽管大语言模型（LLM）在抽象概要 SUMMARIZATION 方面表现出色，但它们面临两个主要挑战：它们的很大的大小和偏差。偏差会让模型失去可靠性，并且提高安全风险。剪除是一种技术，可以通过删除重复的权重来减小模型的大小，从而实现更高效的推理。剪除后的模型可以保持与全大小模型相同的性能，因此它们成为了在有限预算情况下使用的理想选择。然而，剪除对抽象 SUMMARIZATION 中 LLM 的偏差产生的影响还没有得到探讨。在这篇论文中，我们提供了对剪除后 LLM 在三个标准 SUMMARIZATION 任务中的偏差产生进行了广泛的实验研究。我们发现，剪除后 LLM 会比全大小模型更少地偏差。我们的跟踪分析表明，剪除后模型更多地依赖于输入源，而 menos依赖于它们在预训练中学习的参数知识。这种更多地依赖于输入源的依赖性导致生成的内容与输入源的字句 overlap 更高，这可能是减少偏差的原因。
</details></li>
</ul>
<hr>
<h2 id="Strategic-Data-Augmentation-with-CTGAN-for-Smart-Manufacturing-Enhancing-Machine-Learning-Predictions-of-Paper-Breaks-in-Pulp-and-Paper-Production"><a href="#Strategic-Data-Augmentation-with-CTGAN-for-Smart-Manufacturing-Enhancing-Machine-Learning-Predictions-of-Paper-Breaks-in-Pulp-and-Paper-Production" class="headerlink" title="Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production"></a>Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09333">http://arxiv.org/abs/2311.09333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamed Khosravi, Sarah Farhadpour, Manikanta Grandhi, Ahmed Shoyeb Raihan, Srinjoy Das, Imtiaz Ahmed</li>
<li>For: This paper aims to address the challenge of predictive maintenance in the pulp-and-paper industry, specifically the scarcity of paper breaks during production, which has a high economic impact.* Methods: The authors use a dataset of 18,398 instances derived from a quality assurance protocol, and employ Conditional Generative Adversarial Networks (CTGAN) and Synthetic Minority Oversampling Technique (SMOTE) to create a novel data augmentation framework. This method enhances the performance metrics of predictive modeling and improves the detection of machine breaks.* Results: The study achieves significant improvements in predictive maintenance performance metrics using the CTGAN-enhanced dataset. The models’ detection of machine breaks (Class 1) improves by over 30% for Decision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression.Here is the information in Simplified Chinese text:* For: 这篇论文目标是解决纸品工业中预测维护的挑战，特别是生产过程中纸卷断rare事件的高经济影响。* Methods: 作者们使用18398个来自质量监控协议的实例集，并使用Conditional Generative Adversarial Networks (CTGAN)和Synthetic Minority Oversampling Technique (SMOTE)创建一个新的数据增强框架。这种方法提高预测模型性能指标，并提高机器停机的检测率。* Results: 研究实现了预测维护性能指标的显著提高，使用CTGAN增强dataset时，模型对机器停机(Class 1)的检测率提高了30%以上 для决策树，20%以上 дляRandom Forest， nearly 90%以上 дляLogistic Regression。<details>
<summary>Abstract</summary>
A significant challenge for predictive maintenance in the pulp-and-paper industry is the infrequency of paper breaks during the production process. In this article, operational data is analyzed from a paper manufacturing machine in which paper breaks are relatively rare but have a high economic impact. Utilizing a dataset comprising 18,398 instances derived from a quality assurance protocol, we address the scarcity of break events (124 cases) that pose a challenge for machine learning predictive models. With the help of Conditional Generative Adversarial Networks (CTGAN) and Synthetic Minority Oversampling Technique (SMOTE), we implement a novel data augmentation framework. This method ensures that the synthetic data mirrors the distribution of the real operational data but also seeks to enhance the performance metrics of predictive modeling. Before and after the data augmentation, we evaluate three different machine learning algorithms-Decision Trees (DT), Random Forest (RF), and Logistic Regression (LR). Utilizing the CTGAN-enhanced dataset, our study achieved significant improvements in predictive maintenance performance metrics. The efficacy of CTGAN in addressing data scarcity was evident, with the models' detection of machine breaks (Class 1) improving by over 30% for Decision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression. With this methodological advancement, this study contributes to industrial quality control and maintenance scheduling by addressing rare event prediction in manufacturing processes.
</details>
<details>
<summary>摘要</summary>
产品生产过程中纸裂事件的rarity是维保预测的一大挑战。本文分析了一种纸制造机的操作数据，该机器的纸裂事件相对较少，但具有高经济影响。使用包含18,398个实例的质量监管协议数据集，我们解决了缺乏纸裂事件的挑战，这些事件的数量很少（124个），但它们对预测模型的性能产生了很大的影响。我们采用了 Conditional Generative Adversarial Networks（CTGAN）和Synthetic Minority Oversampling Technique（SMOTE）来实现一个新的数据增强框架。这种方法确保了生成的 sintetic 数据与实际操作数据的分布相同，同时尝试提高预测模型的性能指标。在数据增强前和后，我们评估了三种不同的机器学习算法：决策树（DT）、Random Forest（RF）和логистиック回归（LR）。使用 CTGAN 增强的数据集，我们的研究实现了显著提高的维保预测性能指标。 CTGAN 在 Addressing 缺乏数据问题方面的效果是明显的，纸裂事件的检测（Class 1）的准确率提高了30%以上 для决策树，20%以上 для Random Forest，并且近90%以上 для logistic regression。这种方法ological advancement 在工业质量控制和维保时间安排方面做出了贡献，解决了制造过程中罕见事件预测的问题。
</details></li>
</ul>
<hr>
<h2 id="Improving-fit-to-human-reading-times-via-temperature-scaled-surprisal"><a href="#Improving-fit-to-human-reading-times-via-temperature-scaled-surprisal" class="headerlink" title="Improving fit to human reading times via temperature-scaled surprisal"></a>Improving fit to human reading times via temperature-scaled surprisal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09325">http://arxiv.org/abs/2311.09325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Liu, Iza Škrjanec, Vera Demberg</li>
<li>for: 这项研究旨在使用大语言模型（LLM）模拟人类认知负荷，并研究words with lower predictability（i.e., higher surprisal）需要更多时间进行理解。</li>
<li>methods: 这项研究使用了温度缩放的 surprisal，即由形态概率决定的surprisal，作为人类阅读时间预测的Predictor。</li>
<li>results: 研究结果遍布三个资料库，表明temperature-scaled surprisal可以很好地提高预测阅读时间的准确性，并且设置温度为大约2.5可以获得最大的89%的 delta log-likelihood 提升。此外，研究还提出了一种可能的人类化偏见指标来衡量模型的可靠性。<details>
<summary>Abstract</summary>
Past studies have provided broad support for that words with lower predictability (i.e., higher surprisal) require more time for comprehension by using large language models (LLMs) to simulate humans' cognitive load. In general, these studies have implicitly assumed that the probability scores from LLMs are accurate, ignoring the discrepancies between human cognition and LLMs from this standpoint. Inspired by the concept of probability calibration, we are the first work to focus on the probability distribution for human reading simulation. We propose to use temperature-scaled surprisal, a surprisal calculated by shaped probability, to be the predictor of human reading times. Our results across three corpora consistently revealed that such a surprisal can drastically improve the prediction of reading times. Setting the temperature to be approximately 2.5 across all models and datasets can yield up to an 89% of increase in delta log-likelihood in our setting. We also propose a calibration metric to quantify the possible human-likeness bias. Further analysis was done and provided insights into this phenomenon.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spoken-Word2Vec-A-Perspective-And-Some-Techniques"><a href="#Spoken-Word2Vec-A-Perspective-And-Some-Techniques" class="headerlink" title="Spoken Word2Vec: A Perspective And Some Techniques"></a>Spoken Word2Vec: A Perspective And Some Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09319">http://arxiv.org/abs/2311.09319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Amaan Sayeed, Hanan Aldarmaki</li>
<li>for: 这个论文的目的是探讨语音字 embedding 的问题，以及过去的研究中使用的假设和体系是否能够正确地编码 semantic features。</li>
<li>methods: 这篇论文使用 Word2Vec 算法来模型语音字的语言模式，并对过去的研究进行了实验检验，以确定这些算法是否能够正确地编码 semantic features。</li>
<li>results: 实验结果表明，过去的研究中使用的假设和体系导致了语音字 embedding 中的phonetic feature占主导地位，而不是 semantic feature。此外， automatic word type clustering 的使用也有助于改善 embedding 的质量。<details>
<summary>Abstract</summary>
Text word embeddings that encode distributional semantic features work by modeling contextual similarities of frequently occurring words. Acoustic word embeddings, on the other hand, typically encode low-level phonetic similarities. Semantic embeddings for spoken words have been previously explored using similar algorithms to Word2Vec, but the resulting vectors still mainly encoded phonetic rather than semantic features. In this paper, we examine the assumptions and architectures used in previous works and show experimentally how Word2Vec algorithms fail to encode distributional semantics when the input units are acoustically correlated. In addition, previous works relied on the simplifying assumptions of perfect word segmentation and clustering by word type. Given these conditions, a trivial solution identical to text-based embeddings has been overlooked. We follow this simpler path using automatic word type clustering and examine the effects on the resulting embeddings, highlighting the true challenges in this task.
</details>
<details>
<summary>摘要</summary>
文本词嵌入工具，例如Word2Vec，可以模拟语言中的 distribuional semantic 特征。但是，在使用 acoustic 词嵌入时，通常只会模拟低级别的语音相似性。在这篇论文中，我们会检查之前的作品中使用的假设和架构，并通过实验表明，Word2Vec 算法在输入单元是 acoustically correlated 时不能够编码分布semantic特征。此外，之前的作品假设了完美的单词分 segmentation 和 word type 的分 clustering，这导致了一个简单的解决方案被忽视了。我们采用自动化单词类型分 clustering，并研究 embedding 的效果， highlighting the true challenges 在这个任务中。
</details></li>
</ul>
<hr>
<h2 id="H-Packer-Holographic-Rotationally-Equivariant-Convolutional-Neural-Network-for-Protein-Side-Chain-Packing"><a href="#H-Packer-Holographic-Rotationally-Equivariant-Convolutional-Neural-Network-for-Protein-Side-Chain-Packing" class="headerlink" title="H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing"></a>H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09312">http://arxiv.org/abs/2311.09312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gian Marco Visani, William Galvin, Michael Neal Pun, Armita Nourmohammad</li>
<li>for: 预测蛋白质三维结构，尤其是蛋白质侧链排列，是设计功能蛋白质的关键步骤。</li>
<li>methods: 我们提出了一种新的两阶段算法，叫做干擦包装器（H-Packer），基于两个轻量级的旋转对称神经网络。</li>
<li>results: H-Packer在CASP13和CASP14目标上展示了 Computationally efficient 和有利的性能，与传统物理学基于算法和其他深度学习解决方案相比。<details>
<summary>Abstract</summary>
Accurately modeling protein 3D structure is essential for the design of functional proteins. An important sub-task of structure modeling is protein side-chain packing: predicting the conformation of side-chains (rotamers) given the protein's backbone structure and amino-acid sequence. Conventional approaches for this task rely on expensive sampling procedures over hand-crafted energy functions and rotamer libraries. Recently, several deep learning methods have been developed to tackle the problem in a data-driven way, albeit with vastly different formulations (from image-to-image translation to directly predicting atomic coordinates). Here, we frame the problem as a joint regression over the side-chains' true degrees of freedom: the dihedral $\chi$ angles. We carefully study possible objective functions for this task, while accounting for the underlying symmetries of the task. We propose Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain packing built on top of two light-weight rotationally equivariant neural networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is computationally efficient and shows favorable performance against conventional physics-based algorithms and is competitive against alternative deep learning solutions.
</details>
<details>
<summary>摘要</summary>
Accurately modeling protein 3D structure is essential for the design of functional proteins. An important sub-task of structure modeling is protein side-chain packing: predicting the conformation of side-chains (rotamers) given the protein's backbone structure and amino-acid sequence. Conventional approaches for this task rely on expensive sampling procedures over hand-crafted energy functions and rotamer libraries. Recently, several deep learning methods have been developed to tackle the problem in a data-driven way, albeit with vastly different formulations (from image-to-image translation to directly predicting atomic coordinates). Here, we frame the problem as a joint regression over the side-chains' true degrees of freedom: the dihedral $\chi$ angles. We carefully study possible objective functions for this task, while accounting for the underlying symmetries of the task. We propose Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain packing built on top of two light-weight rotationally equivariant neural networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is computationally efficient and shows favorable performance against conventional physics-based algorithms and is competitive against alternative deep learning solutions.Here's the translation in Traditional Chinese: Accurately modeling protein 3D structure is essential for the design of functional proteins. An important sub-task of structure modeling is protein side-chain packing: predicting the conformation of side-chains (rotamers) given the protein's backbone structure and amino-acid sequence. Conventional approaches for this task rely on expensive sampling procedures over hand-crafted energy functions and rotamer libraries. Recently, several deep learning methods have been developed to tackle the problem in a data-driven way, albeit with vastly different formulations (from image-to-image translation to directly predicting atomic coordinates). Here, we frame the problem as a joint regression over the side-chains' true degrees of freedom: the dihedral $\chi$ angles. We carefully study possible objective functions for this task, while accounting for the underlying symmetries of the task. We propose Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain packing built on top of two light-weight rotationally equivariant neural networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is computationally efficient and shows favorable performance against conventional physics-based algorithms and is competitive against alternative deep learning solutions.
</details></li>
</ul>
<hr>
<h2 id="Divergences-between-Language-Models-and-Human-Brains"><a href="#Divergences-between-Language-Models-and-Human-Brains" class="headerlink" title="Divergences between Language Models and Human Brains"></a>Divergences between Language Models and Human Brains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09308">http://arxiv.org/abs/2311.09308</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flamingozh/divergence_meg">https://github.com/flamingozh/divergence_meg</a></li>
<li>paper_authors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe</li>
<li>for: 研究 whether machines and humans process language in similar ways, and explore the differences between human and machine language processing using brain data.</li>
<li>methods: 使用 Magnetoencephalography (MEG) responses to a written narrative to examine the differences between LM representations and the human brain’s responses to language, and fine-tune LMs on datasets related to specific phenomena to improve their alignment with human brain responses.</li>
<li>results: 发现 LMs 不好地理解情感、 figurative language processing 和 physical commonsense，并通过 fine-tuning LMs 可以提高它们与人类大脑响应的匹配度。<details>
<summary>Abstract</summary>
Do machines and humans process language in similar ways? A recent line of research has hinted in the affirmative, demonstrating that human brain signals can be effectively predicted using the internal representations of language models (LMs). This is thought to reflect shared computational principles between LMs and human language processing. However, there are also clear differences in how LMs and humans acquire and use language, even if the final task they are performing is the same. Despite this, there is little work exploring systematic differences between human and machine language processing using brain data. To address this question, we examine the differences between LM representations and the human brain's responses to language, specifically by examining a dataset of Magnetoencephalography (MEG) responses to a written narrative. In doing so we identify three phenomena that, in prior work, LMs have been found to not capture well: emotional understanding, figurative language processing, and physical commonsense. By fine-tuning LMs on datasets related to these phenomena, we observe that fine-tuned LMs show improved alignment with human brain responses across these tasks. Our study implies that the observed divergences between LMs and human brains may stem from LMs' inadequate representation of these specific types of knowledge.
</details>
<details>
<summary>摘要</summary>
人类和机器是否处理语言类似？一项研究表明，人类大脑信号可以准确预测语言模型（LM）内部表示，这被视为人类语言处理和LM共享计算原理的证明。然而，尚存在人类和机器语言获取和使用语言的显著差异，即使完成的任务相同。尽管如此，有少量研究探讨人类和机器语言处理的系统性差异使用大脑数据。为解决这个问题，我们比较LM表示和人类大脑对语言的响应，Specifically by examining a dataset of Magnetoencephalography (MEG) responses to a written narrative.在这些任务中，我们发现了三种现象，在先前的工作中LMs没有良好捕捉：情感理解、 figurative language processing和physical common sense。通过对这些任务进行数据集的细化，我们观察到了已经细化LMs的Alignment with human brain responses across these tasks.我们的研究表明，观察到的LMs和人类大脑之间差异可能由LMs的不够表示这些特定类型的知识而导致。
</details></li>
</ul>
<hr>
<h2 id="Symbol-LLM-Towards-Foundational-Symbol-centric-Interface-For-Large-Language-Models"><a href="#Symbol-LLM-Towards-Foundational-Symbol-centric-Interface-For-Large-Language-Models" class="headerlink" title="Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models"></a>Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09278">http://arxiv.org/abs/2311.09278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, Jun Liu</li>
<li>for: 本研究旨在探讨如何在语言模型（LLM）中注入特定的符号知识，以提高NL-centric任务的表现。</li>
<li>methods: 本研究采用了两个方向的方法：一是收集了34个符号任务，覆盖了~20种不同的形式，以捕捉符号之间的关系；二是提出了一种两阶段调试框架，能够在注入符号知识时保持NL-centric能力的一致性。</li>
<li>results: 对于符号-和NL-centric任务的广泛实验表明，Symbol-LLM系列模型在符号知识注入问题上具有balanced和superior表现。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have greatly propelled the progress in natural language(NL)-centric tasks based on NL interface. However, the NL form is not enough for world knowledge. Current works focus on this question by injecting specific symbolic knowledge into LLM, which ignore two critical challenges: the interrelations between various symbols and the balance between symbolic-centric and NL-centric capabilities. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we collect 34 symbolic tasks, covering ~20 different forms, which are unified to capture symbol interrelations. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss of the generality ability. Extensive experiments on both symbol- and NL-centric tasks demonstrate the balanced and superior performances of Symbol-LLM series models.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)大语言模型（LLM）已经为自然语言（NL）关注的任务带来了很大的进步，基于NL界面。然而，NL形式不够用于世界知识。当前的工作都在注意这个问题，通过将特定的象征知识注入到LLM中，忽略了两个关键挑战：符号之间的关系和象征中心和NL中心能力的平衡。在这项工作中，我们从数据和框架角度来解决这些挑战，并引入 Symbol-LLM 系列模型。首先，我们收集了34个符号任务，覆盖了~20种不同的形式，这些任务被统一以捕捉符号之间的关系。然后，我们提出了一个两个阶段的调整框架，成功地注入符号知识而不失去通用能力。广泛的实验表明 Symbol-LLM 系列模型在符号和NL关注任务中具有平衡和超越的表现。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Translation-capabilities-of-Large-Language-Models-involving-English-and-Indian-Languages"><a href="#Assessing-Translation-capabilities-of-Large-Language-Models-involving-English-and-Indian-Languages" class="headerlink" title="Assessing Translation capabilities of Large Language Models involving English and Indian Languages"></a>Assessing Translation capabilities of Large Language Models involving English and Indian Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09216">http://arxiv.org/abs/2311.09216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vandan Mujadia, Ashok Urlana, Yash Bhaskar, Penumalla Aditya Pavani, Kukkapalli Shravya, Parameswari Krishnamurthy, Dipti Misra Sharma</li>
<li>for: 本研究旨在探讨大型自然语言处理器（LLM）在多种自然语言译语 зада务中的多语言能力。</li>
<li>methods: 我们使用机器翻译作为英语和22种印度语言之间的译语任务，首先研究 raw LLM 的翻译能力，然后探讨 raw LLM 在上下文学习中的表现。我们使用 LoRA 等参数有效的微调方法进行微调。</li>
<li>results: 我们的研究表明，使用 LLaMA 作为基础模型，可以在英语和印度语言之间的翻译任务中取得显著进步，其中 average BLEU 分数为 13.42、15.93、12.13、12.30 和 12.07，chrF 分数为 43.98、46.99、42.55、42.42 和 45.39。在印度语言到英语的翻译任务中，我们取得了 average BLEU 分数为 14.03、16.65、16.17、15.35 和 12.55，chrF 分数为 36.71、40.44、40.26、39.51 和 36.20。总之，我们的发现表明大型自然语言处理器在机器翻译任务中具有潜在的潜力，包括目前未经投入的语言。<details>
<summary>Abstract</summary>
Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. In this work, our aim is to explore the multilingual capabilities of large language models by using machine translation as a task involving English and 22 Indian languages. We first investigate the translation capabilities of raw large language models, followed by exploring the in-context learning capabilities of the same raw models. We fine-tune these large language models using parameter efficient fine-tuning methods such as LoRA and additionally with full fine-tuning. Through our study, we have identified the best performing large language model for the translation task involving LLMs, which is based on LLaMA.   Our results demonstrate significant progress, with average BLEU scores of 13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99, 42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for English to Indian languages on IN22 (conversational), IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets. Similarly, for Indian languages to English, we achieved average BLEU scores of 14.03, 16.65, 16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51, and 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational), IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets. Overall, our findings highlight the potential and strength of large language models for machine translation capabilities, including for languages that are currently underrepresented in LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已经在不同的自然语言处理任务中实现了很大的进步。在这项工作中，我们的目标是探索大型语言模型在多种语言之间的多语言能力。我们首先调查了Raw大型语言模型的翻译能力，然后探索这些Raw模型在 Context Learning 中的能力。我们使用 parameter efficient fine-tuning 方法如 LoRA 和全局 fine-tuning 进行参数的调整。通过我们的研究，我们已经确定了最佳的大型语言模型为翻译任务，即基于 LLaMA 的模型。我们的结果表明了显著的进步，平均 BLEU 分数为 13.42、15.93、12.13、12.30 和 12.07，以及 CHRF 分数为 43.98、46.99、42.55、42.42 和 45.39，分别在英语到印度语言的 IN22（交流）、IN22（通用）、flores200-dev、flores200-devtest 和 newstest2019 测试集上。同样，在印度语言到英语的翻译任务中，我们获得了平均 BLEU 分数为 14.03、16.65、16.17、15.35 和 12.55，以及 CHRF 分数为 36.71、40.44、40.26、39.51 和 36.20，分别在 IN22（交流）、IN22（通用）、flores200-dev、flores200-devtest 和 newstest2019 测试集上。总的来说，我们的发现表明了大型语言模型在翻译任务中的潜力和优势，包括目前尚未得到充分利用的语言。
</details></li>
</ul>
<hr>
<h2 id="Controllable-Text-Summarization-Unraveling-Challenges-Approaches-and-Prospects-–-A-Survey"><a href="#Controllable-Text-Summarization-Unraveling-Challenges-Approaches-and-Prospects-–-A-Survey" class="headerlink" title="Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects – A Survey"></a>Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects – A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09212">http://arxiv.org/abs/2311.09212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ashokurlana/controllable_text_summarization_survey">https://github.com/ashokurlana/controllable_text_summarization_survey</a></li>
<li>paper_authors: Ashok Urlana, Pruthwik Mishra, Tathagato Roy, Rahul Mishra</li>
<li>for: 这个论文主要写于控制可能性的文章摘要方法。</li>
<li>methods: 本论文使用了多种控制可能性的方法，包括文章摘要任务的定义、分类和评价。</li>
<li>results: 本论文发现了控制可能性的文章摘要方法的多种类别和挑战，以及未来研究的可能性。<details>
<summary>Abstract</summary>
Generic text summarization approaches often fail to address the specific intent and needs of individual users. Recently, scholarly attention has turned to the development of summarization methods that are more closely tailored and controlled to align with specific objectives and user needs. While a growing corpus of research is devoted towards a more controllable summarization, there is no comprehensive survey available that thoroughly explores the diverse controllable aspects or attributes employed in this context, delves into the associated challenges, and investigates the existing solutions. In this survey, we formalize the Controllable Text Summarization (CTS) task, categorize controllable aspects according to their shared characteristics and objectives, and present a thorough examination of existing methods and datasets within each category. Moreover, based on our findings, we uncover limitations and research gaps, while also delving into potential solutions and future directions for CTS.
</details>
<details>
<summary>摘要</summary>
常见的文本摘要方法 often 无法 addresses 用户的特定目标和需求。近年来，学术界对于更加控制性的摘要方法的开发受到了关注。 although 一个增长的文献库 devoted  towards 更加控制性的摘要， there is no comprehensive survey available that thoroughly explores the diverse controllable aspects or attributes employed in this context, delves into the associated challenges, and investigates the existing solutions. In this survey, we formalize the Controllable Text Summarization (CTS) task, categorize controllable aspects according to their shared characteristics and objectives, and present a thorough examination of existing methods and datasets within each category. Moreover, based on our findings, we uncover limitations and research gaps, while also delving into potential solutions and future directions for CTS.Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Note-Enhancing-Robustness-in-Retrieval-Augmented-Language-Models"><a href="#Chain-of-Note-Enhancing-Robustness-in-Retrieval-Augmented-Language-Models" class="headerlink" title="Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models"></a>Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09210">http://arxiv.org/abs/2311.09210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu</li>
<li>for: 提高 Retrieval-augmented language models（RALMs）的可靠性和能力，特别是在减少假想的报道和增加外部知识源的情况下。</li>
<li>methods: 提出了一种新的Chain-of-Noting（CoN）方法，通过生成文档检索过程中的顺序阅读笔记，评估检索到的文档的相关性，并将其纳入最终的回答中。</li>
<li>results: CoN在四个开放领域问答 benchmarck 上进行了实验，结果显示，与标准 RALMs 相比，CoN 可以提高 EM 分数的平均提升为+7.9，并在实时问题中减少不相关文档的拒绝率为+10.5。<details>
<summary>Abstract</summary>
Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with "unknown" when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.
</details>
<details>
<summary>摘要</summary>
大型语言模型（RALM）在增强它们的能力方面取得了重要进步，主要是减少了假想的投入。然而，获取的信息不一定可靠。不必要的数据获取可能导致错误的响应，甚至让模型忽略其内置的知识，即使它拥有足够的信息来回答问题。此外，标准的RALM通常难以判断自己是否具备了足够的知识来提供正确的答案。在知识缺乏的情况下，这些系统应该回答为“未知”。为解决这些挑战，我们提出了链条注释（CoN），一种新的方法，可以提高RALM在噪音、无关文档中的稳定性，以及处理未知情况的能力。CoN的核心思想是生成批处理的阅读笔记，以评估 retrieved 文档的相关性，并将其集成到提供答案。我们使用了ChatGPT创建训练数据，然后将其训练在LLaMa-2 7B 模型上。我们在四个开放领域问答 benchmark 上进行了实验，结果显示，RALMs 配置了 CoN 显著超越标准 RALMs。特别是，CoN 在 entirely 噪音获取的情况下的 EM 分数平均提高了+7.9，并在实时问题 external 知识范围外的拒绝率上提高了+10.5。
</details></li>
</ul>
<hr>
<h2 id="Fusion-Eval-Integrating-Evaluators-with-LLMs"><a href="#Fusion-Eval-Integrating-Evaluators-with-LLMs" class="headerlink" title="Fusion-Eval: Integrating Evaluators with LLMs"></a>Fusion-Eval: Integrating Evaluators with LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09204">http://arxiv.org/abs/2311.09204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong Chen, Lei Meng<br>for: 这篇论文的目的是评估大型自然语言处理模型（LLMs）的评估方法，以便更好地理解自然语言理解和高级逻辑预期。methods: 这篇论文使用了多种评估方法，包括人类基于、模型基于和自动度量标准方法，并通过将这些方法综合使用来创建一个更加灵活和有效的评估系统。results: 在使用SummEval数据集进行测试时，Fusion-Eval实现了Spearman相关性0.96，超过其他评估器。这表明了Fusion-Eval可以充分利用多个参考来生成高度相似于人类视角的评估结果，为LLM评估做出了新的标准。<details>
<summary>Abstract</summary>
Evaluating Large Language Models (LLMs) is a complex task, especially considering the intricacies of natural language understanding and the expectations for high-level reasoning. Traditional evaluations typically lean on human-based, model-based, or automatic-metrics-based paradigms, each with its own advantages and shortcomings. We introduce "Fusion-Eval", a system that employs LLMs not solely for direct evaluations, but to skillfully integrate insights from diverse evaluators. This gives Fusion-Eval flexibility, enabling it to work effectively across diverse tasks and make optimal use of multiple references. In testing on the SummEval dataset, Fusion-Eval achieved a Spearman correlation of 0.96, outperforming other evaluators. The success of Fusion-Eval underscores the potential of LLMs to produce evaluations that closely align human perspectives, setting a new standard in the field of LLM evaluation.
</details>
<details>
<summary>摘要</summary>
评估大语言模型（LLM）是一项复杂的任务，尤其是在自然语言理解方面和高级逻辑预期下。传统评估方法通常是人类基础、模型基础或自动指标基础的三者，每种方法都有其优点和缺点。我们介绍了“融合评估”（Fusion-Eval）系统，它不仅利用 LLM 进行直接评估，而且灵活地结合了多个评估者的意见。这使得 Fusion-Eval 能够在多种任务上工作有效，并且能够最大化多个参考。在 SummEval 数据集上测试时，Fusion-Eval 达到了 Spearman 相关系数 0.96，超越其他评估器。Fusion-Eval 的成功表明 LLM 可以生成高度吻合人类视角的评估结果，为 LLM 评估领域设置了新的标准。
</details></li>
</ul>
<hr>
<h2 id="ExpM-NF-Differentially-Private-Machine-Learning-that-Surpasses-DPSGD"><a href="#ExpM-NF-Differentially-Private-Machine-Learning-that-Surpasses-DPSGD" class="headerlink" title="ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD"></a>ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09200">http://arxiv.org/abs/2311.09200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert A. Bridges, Vandy J. Tombs, Christopher B. Stanley</li>
<li>for: 本研究旨在提出一种基于Exponential Mechanism（ExpM）和auxiliary Normalizing Flow（NF）的方法，用于在private数据上训练机器学习（ML）模型，并 garantuee differential privacy（DP）保证。</li>
<li>methods: 本方法使用ExpM和NF结合使用，以实现在private数据上训练ML模型，并可以实现预先指定的DP保证。</li>
<li>results: 对于多个分类任务和不同的数据集，ExpM+NF可以 achieve greater than 93%的非私有训练精度（AUC），并且在DP保证下提供更高的精度和更低的DP保证（$\varepsilon$）。I hope that helps! Let me know if you have any further questions or if you’d like me to help with anything else.<details>
<summary>Abstract</summary>
In this pioneering work we formulate ExpM+NF, a method for training machine learning (ML) on private data with pre-specified differentially privacy guarantee $\varepsilon>0, \delta=0$, by using the Exponential Mechanism (ExpM) and an auxiliary Normalizing Flow (NF). We articulate theoretical benefits of ExpM+NF over Differentially Private Stochastic Gradient Descent (DPSGD), the state-of-the-art (SOTA) and de facto method for differentially private ML, and we empirically test ExpM+NF against DPSGD using the SOTA implementation (Opacus with PRV accounting) in multiple classification tasks on the Adult Dataset (census data) and MIMIC-III Dataset (electronic healthcare records) using Logistic Regression and GRU-D, a deep learning recurrent neural network with ~20K-100K parameters. In all experiments, ExpM+NF achieves greater than 93% of the non-private training accuracy (AUC) for $\varepsilon \in [1\mathrm{e}{-3}, 1]$, exhibiting greater accuracy (higher AUC) and privacy (lower $\varepsilon$ with $\delta=0$) than DPSGD. Differentially private ML generally considers $\varepsilon \in [1,10]$ to maintain reasonable accuracy; hence, ExpM+NF's ability to provide strong accuracy for orders of magnitude better privacy (smaller $\varepsilon$) substantially pushes what is currently possible in differentially private ML. Training time results are presented showing ExpM+NF is comparable to (slightly faster) than DPSGD. Code for these experiments will be provided after review. Limitations and future directions are provided.
</details>
<details>
<summary>摘要</summary>
在这项先锋工作中，我们提出了ExpM+NF方法，用于在private数据上训练机器学习（ML），并 garantía differentially privacy 保证 $\varepsilon>0, \delta=0$。我们解释了ExpM+NF方法与State-of-the-art（SOTA）和de facto方法 differentially private Stochastic Gradient Descent（DPSGD）之间的理论优势，并对ExpM+NF方法和DPSGD进行了多个分类任务中的empirical测试，使用了Adult Dataset（人口普查数据）和MIMIC-III Dataset（电子医疗记录）上的Logistic Regression和GRU-D，一个深度学习循环神经网络，parameters数量在20K-100K之间。在所有实验中，ExpM+NF方法可以在 $\varepsilon \in [1\mathrm{e}{-3}, 1]$ 范围内达到非私有训练精度（AUC）的大于93%，表现出更高的准确率（AUC）和隐私（lower $\varepsilon$ with $\delta=0$），比DPSGD更好。 differentially private ML通常Consider $\varepsilon \in [1,10]$ 以保持合理的准确率;因此，ExpM+NF方法的能力提供许多orders of magnitude better privacy（smaller $\varepsilon）substantially pushes what is currently possible in differentially private ML。我们还提供了训练时间结果，表明ExpM+NF方法与DPSGD相对（slightly faster）。我们将在审核后提供代码。 limitations和未来方向也被提供。
</details></li>
</ul>
<hr>
<h2 id="Never-Lost-in-the-Middle-Improving-Large-Language-Models-via-Attention-Strengthening-Question-Answering"><a href="#Never-Lost-in-the-Middle-Improving-Large-Language-Models-via-Attention-Strengthening-Question-Answering" class="headerlink" title="Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering"></a>Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09198">http://arxiv.org/abs/2311.09198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin Zhang, Zejian Xie, Jiaxing Zhang</li>
<li>for: 提高大语言模型在长文本上的信息搜寻和反思能力</li>
<li>methods: 提出特制的任务 called Attention Strengthening Multi-doc QA (ASM QA)，以提高模型在长文本上的精准搜寻能力</li>
<li>results: 实验结果显示，模型在多文档问答和其他标准任务上表现出色，与当前最佳模型相比，在随机设置下获得13.7%的绝对提升，在文章检索任务上获得21.5%的提升。<details>
<summary>Abstract</summary>
While large language models (LLMs) are equipped with longer text input capabilities than before, they are struggling to seek correct information in long contexts. The "lost in the middle" problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called Attention Strengthening Multi-doc QA (ASM QA). Following these tasks, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7% absolute gain in shuffled settings, by 21.5% in passage retrieval task. We release our model, Ziya-Reader to promote related research in the community.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有更长的文本输入能力，但它们在长文本上寻找正确信息时受到挑战。这个“lost in the middle”问题对大多数LLM都是一个重要问题，指的是正确信息在中间部分的减退率。为了解决这个关键问题，这篇论文提出了通过特定任务 called Attention Strengthening Multi-doc QA（ASM QA）来增强LLM在长文本上的信息寻找和反射能力。在这些任务中，我们的模型在更加精准地Focus on Desired Information。实验结果表明，我们的模型在多文档问答和其他标准 bencmarks 上表现出了明显的提升，相比领先模型的13.7%绝对提升，在排序任务上提高21.5%。我们将发布我们的模型，Ziya-Reader，以便在社区中促进相关的研究。
</details></li>
</ul>
<hr>
<h2 id="The-Role-of-Chain-of-Thought-in-Complex-Vision-Language-Reasoning-Task"><a href="#The-Role-of-Chain-of-Thought-in-Complex-Vision-Language-Reasoning-Task" class="headerlink" title="The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task"></a>The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09193">http://arxiv.org/abs/2311.09193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Wu, Pengchuan Zhang, Wenhan Xiong, Barlas Oguz, James C. Gee, Yixin Nie</li>
<li>for: 这个研究探讨了链条思维方法在复杂视语任务中的有效性，这种方法通过将任务拆分成子任务和中间步骤来提高语言任务的效率。</li>
<li>methods: 这篇研究使用了”描述然后做出决策”策略，这种策略draws inspiration from human signal processing mechanisms，并在探测任务中提高了性能，提高了50%。</li>
<li>results: 这篇研究发现，使用”描述然后做出决策”策略可以在复杂视语任务中提高探测任务的性能，提高50%。<details>
<summary>Abstract</summary>
The study explores the effectiveness of the Chain-of-Thought approach, known for its proficiency in language tasks by breaking them down into sub-tasks and intermediate steps, in improving vision-language tasks that demand sophisticated perception and reasoning. We present the "Description then Decision" strategy, which is inspired by how humans process signals. This strategy significantly improves probing task performance by 50%, establishing the groundwork for future research on reasoning paradigms in complex vision-language tasks.
</details>
<details>
<summary>摘要</summary>
这个研究探讨了链条思维方法的效iveness，这种方法以分解语言任务为互助步骤而著称，在复杂的视觉语言任务中提高了高级观察和理解能力。我们提出了“描述然后决策”策略，这种策略 Draws inspiration from human signal processing and significantly improves probing task performance by 50%. This lays the foundation for future research on reasoning paradigms in complex vision-language tasks.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Towards-Verifiable-Text-Generation-with-Symbolic-References"><a href="#Towards-Verifiable-Text-Generation-with-Symbolic-References" class="headerlink" title="Towards Verifiable Text Generation with Symbolic References"></a>Towards Verifiable Text Generation with Symbolic References</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09188">http://arxiv.org/abs/2311.09188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Torroba Hennigen, Shannon Shen, Aniruddha Nrusimha, Bernhard Gapp, David Sontag, Yoon Kim</li>
<li>for: 这篇论文目的是提出一种简单的方法来使大语言模型（LLM）的输出更易于人工验证，以便用于高风险应用。</li>
<li>methods: 该论文提出了一种名为符号附加生成（SymGen）的方法，它使得 LLM 可以在输出文本中嵌入显式的符号参考，以便显示不同的文本段的来源。</li>
<li>results: 实验表明， LLM 可以通过 SymGen 方法直接输出包含符号参考的文本，而不会影响文本的流畅性和准确性。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated an impressive ability to synthesize plausible and fluent text. However they remain vulnerable to hallucinations, and thus their outputs generally require manual human verification for high-stakes applications, which can be time-consuming and difficult. This paper proposes symbolically grounded generation (SymGen) as a simple approach for enabling easier validation of an LLM's output. SymGen prompts an LLM to interleave its regular output text with explicit symbolic references to fields present in some conditioning data (e.g., a table in JSON format). The references can be used to display the provenance of different spans of text in the generation, reducing the effort required for manual verification. Across data-to-text and question answering experiments, we find that LLMs are able to directly output text that makes use of symbolic references while maintaining fluency and accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generate-Filter-and-Fuse-Query-Expansion-via-Multi-Step-Keyword-Generation-for-Zero-Shot-Neural-Rankers"><a href="#Generate-Filter-and-Fuse-Query-Expansion-via-Multi-Step-Keyword-Generation-for-Zero-Shot-Neural-Rankers" class="headerlink" title="Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers"></a>Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09175">http://arxiv.org/abs/2311.09175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, Michael Bendersky</li>
<li>for: 提高零shot neural ranker的启发搜索精度</li>
<li>methods: 提出了一个名为GFF的管道，包括一个大型自然语言模型和一个神经网络排序器，用于生成、筛选和融合查询扩展。</li>
<li>results: GFF可以提高零shot nDCG@10在BEIR和TREC DL 2019&#x2F;2020上。<details>
<summary>Abstract</summary>
Query expansion has been proved to be effective in improving recall and precision of first-stage retrievers, and yet its influence on a complicated, state-of-the-art cross-encoder ranker remains under-explored. We first show that directly applying the expansion techniques in the current literature to state-of-the-art neural rankers can result in deteriorated zero-shot performance. To this end, we propose GFF, a pipeline that includes a large language model and a neural ranker, to Generate, Filter, and Fuse query expansions more effectively in order to improve the zero-shot ranking metrics such as nDCG@10. Specifically, GFF first calls an instruction-following language model to generate query-related keywords through a reasoning chain. Leveraging self-consistency and reciprocal rank weighting, GFF further filters and combines the ranking results of each expanded query dynamically. By utilizing this pipeline, we show that GFF can improve the zero-shot nDCG@10 on BEIR and TREC DL 2019/2020. We also analyze different modelling choices in the GFF pipeline and shed light on the future directions in query expansion for zero-shot neural rankers.
</details>
<details>
<summary>摘要</summary>
Query expansion 已经证明可以提高首个检索器的准确率和匹配率，但是它对现代跨Encoder排名器的影响还未得到充分探讨。我们首先表明，直接在当前文献中使用扩展技术可能会导致现有神经排名器的零件性能下降。为此，我们提出了GFF，一个管道，包括一个大型自然语言模型和一个神经排名器，用于生成、筛选和融合查询扩展更有效地，以提高零件性能指标 such as nDCG@10。具体来说，GFF首先通过一个遵循语言模型来生成基于查询的关键词，然后通过自我一致和对偶排名Weight来筛选和组合每个扩展查询的排名结果。通过这个管道，我们表明GFF可以提高零件 nDCG@10 在 BEIR 和 TREC DL 2019/2020。我们还分析了 GFF 管道中不同的模型选择和未来方向。
</details></li>
</ul>
<hr>
<h2 id="AbsPyramid-Benchmarking-the-Abstraction-Ability-of-Language-Models-with-a-Unified-Entailment-Graph"><a href="#AbsPyramid-Benchmarking-the-Abstraction-Ability-of-Language-Models-with-a-Unified-Entailment-Graph" class="headerlink" title="AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph"></a>AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09174">http://arxiv.org/abs/2311.09174</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/abspyramid">https://github.com/hkust-knowcomp/abspyramid</a></li>
<li>paper_authors: Zhaowei Wang, Haochen Shi, Weiqi Wang, Tianqing Fang, Hongming Zhang, Sehyun Choi, Xin Liu, Yangqiu Song</li>
<li>for: 本研究旨在探讨语言模型内置抽象能力的现状，并提出一个大规模的抽象知识图。</li>
<li>methods: 本研究使用了一个大规模的文本描述数据集，通过构建抽象知识图来评估语言模型在开放领域中的抽象能力。</li>
<li>results: 实验结果表明，现有的LLMs在零shot和几shot设置下具有很大的抽象知识识别挑战。通过训练在我们的充沛抽象知识上，我们发现LLMs可以获得基本的抽象能力，并在未见事件中进行抽象。同时，我们也证明了我们的指标是可以强化LLMs在两个前一个抽象任务上。<details>
<summary>Abstract</summary>
Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.
</details>
<details>
<summary>摘要</summary>
研究表明人类智能中的抽象能力是非常重要的，但是这一点尚未得到充分的探索。在这篇论文中，我们介绍了一个名为AbsPyramid的抽象知识维度图，包含221K个文本描述。与现有资源不同，AbsPyramid不仅覆盖了简化事件中的名词或动词，而是收集了多元事件中的抽象知识，以全面评估语言模型在开放领域中的抽象能力。实验结果表明，现有的LLMs在零shot和几shot设定下面临着抽象知识的挑战。通过在我们的充足抽象知识上训练，我们发现LLMs可以学习基本的抽象能力，并在未经见过的事件上进行推断。同时，我们实验表明，我们的标准可以提高LLMs在两个之前的抽象任务中表现。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Knowledge-Question-Answering-via-Abstract-Reasoning-Induction"><a href="#Temporal-Knowledge-Question-Answering-via-Abstract-Reasoning-Induction" class="headerlink" title="Temporal Knowledge Question Answering via Abstract Reasoning Induction"></a>Temporal Knowledge Question Answering via Abstract Reasoning Induction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09149">http://arxiv.org/abs/2311.09149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyang Chen, Dongfang Li, Xiang Zhao, Baotian Hu, Min Zhang</li>
<li>for: 本研究旨在解决大语言模型（LLM）中的时间知识推理问题，这是LLM遇到的一个重要挑战，这些问题可能会导致LLM生成错误或误导信息，主要是因为它们的时间知识处理能力有限，同时复杂的时间逻辑也会带来问题。</li>
<li>methods: 我们提出了一种新的构建主义方法，它强调在LLM学习中实行持续的知识合成和个性化。我们的方法包括Abstract Reasoning Induction ARI框架，这个框架将时间推理分成两个不同阶段：知识无关和知识基础。这种分类目标在减少幻觉和提高LLM对抽象方法的应用。</li>
<li>results: 我们的方法在两个时间问答Dataset上获得了显著改进，相比于基eline，我们的方法提高了29.7%和9.27%。这demonstrates our approach’s efficacy in enhancing temporal reasoning in LLMs. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/czy1999/ARI">https://github.com/czy1999/ARI</a>.<details>
<summary>Abstract</summary>
In this paper, we tackle the significant challenge of temporal knowledge reasoning in Large Language Models (LLMs), an area where such models frequently encounter difficulties. These difficulties often result in the generation of misleading or incorrect information, primarily due to their limited capacity to process evolving factual knowledge and complex temporal logic. In response, we propose a novel, constructivism-based approach that advocates for a paradigm shift in LLM learning towards an active, ongoing process of knowledge synthesis and customization. At the heart of our proposal is the Abstract Reasoning Induction ARI framework, which divides temporal reasoning into two distinct phases: Knowledge-agnostic and Knowledge-based. This division aims to reduce instances of hallucinations and improve LLMs' capacity for integrating abstract methodologies derived from historical data. Our approach achieves remarkable improvements, with relative gains of 29.7\% and 9.27\% on two temporal QA datasets, underscoring its efficacy in advancing temporal reasoning in LLMs. The code will be released at https://github.com/czy1999/ARI.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们面临着大语言模型（LLM）中的时间知识推理挑战，这是 LLM 很频繁遇到的问题。这些问题经常导致 LLM 生成错误或误导信息，主要是因为它们对逐渐发展的事实知识和复杂的时间逻辑处理能力有限。为此，我们提出了一种新的建构主义方法，强调 LLM 学习 Should be an active, ongoing process of knowledge synthesis and customization。我们的提议的核心是抽象逻辑推理引入框架（ARI），将时间推理分为两个不同阶段：无知阶段和知识阶段。这种分类的目的是减少 LLM 生成幻见的情况，提高它们对历史数据 derivated 抽象方法的集成能力。我们的方法在两个时间问答 dataset 上显示了很大的改进，相对于基eline的提升率分别为 29.7% 和 9.27%，这证明了我们的方法在提高 LLM 中的时间推理能力具有效果。代码将在 GitHub 上发布，请参考 https://github.com/czy1999/ARI。
</details></li>
</ul>
<hr>
<h2 id="Jailbreaking-GPT-4V-via-Self-Adversarial-Attacks-with-System-Prompts"><a href="#Jailbreaking-GPT-4V-via-Self-Adversarial-Attacks-with-System-Prompts" class="headerlink" title="Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts"></a>Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09127">http://arxiv.org/abs/2311.09127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun</li>
<li>for: 本研究的目的是探讨 Multimodal Large Language Models (MLLMs) 的安全问题，具体来说是通过对 GPT-4V 的系统提示泄露漏洞进行攻击，以及如何通过自我反对攻击（Self-Adversarial Attack via System Prompt，简称 SASP）方法来实现 MLLM 的破狱。</li>
<li>methods: 本研究使用了一种新的破狱攻击方法，即 SASP，该方法利用了 GPT-4 作为红人工具，通过对自己的系统提示进行攻击，以搜索可能的破狱提示。此外，为了提高攻击成功率，还添加了人工修改基于 GPT-4 的分析。</li>
<li>results: 本研究发现，修改系统提示可以有效降低破狱成功率。 In addition, the study found that modifying system prompts can effectively reduce jailbreak success rates.<details>
<summary>Abstract</summary>
Existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities in model APIs. To fill the research gap, we carry out the following work: 1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully steal the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7\%; 3) We evaluated the effect of modifying system prompts to defend against jailbreaking attacks. Results show that appropriately designed system prompts can significantly reduce jailbreak success rates. Overall, our work provides new insights into enhancing MLLM security, demonstrating the important role of system prompts in jailbreaking, which could be leveraged to greatly facilitate jailbreak success rates while also holding the potential for defending against jailbreaks.
</details>
<details>
<summary>摘要</summary>
现有研究对囚犯多Modal大型语言模型（MLLM）主要集中在输入例针对攻击，少量关注模型API的漏洞。为填补这 gap，我们实施以下工作：1. 我们发现了GPT-4V中的系统提示泄露漏洞。通过特殊的对话设计，我们成功夺取了GPT-4V的内部系统提示。这一发现表明MLLM可能存在潜在的可以利用的安全风险;2. 基于夺取的系统提示，我们提出了一种新的MLLM囚犯攻击方法，称为SASP（自我反对性攻击via系统提示）。通过使用GPT-4作为红色团队工具，我们尝试通过夺取的系统提示找到可能的囚犯提示。此外，为了提高攻击成功率，我们还添加了人工修改基于GPT-4的分析，这进一步提高了攻击成功率到98.7%；3. 我们评估了修改系统提示以防止囚犯攻击的效果。结果表明，适当设计的系统提示可以减少囚犯成功率。总的来说，我们的工作提供了新的思路来增强MLLM安全性，表明系统提示在囚犯中具有重要的作用，可以大大提高囚犯成功率，同时也有可能用于防止囚犯。
</details></li>
</ul>
<hr>
<h2 id="HEALNet-–-Hybrid-Multi-Modal-Fusion-for-Heterogeneous-Biomedical-Data"><a href="#HEALNet-–-Hybrid-Multi-Modal-Fusion-for-Heterogeneous-Biomedical-Data" class="headerlink" title="HEALNet – Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data"></a>HEALNet – Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09115">http://arxiv.org/abs/2311.09115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantin Hemker, Nikola Smidjievski, Mateja Jamnik</li>
<li>for: This paper is written for researchers and practitioners in the field of multi-modal biomedical modelling, specifically those working with image, tabular, and graph data in medical applications.</li>
<li>methods: The Hybrid Early-fusion Attention Learning Network (HEALNet) architecture is used in this paper, which combines modality-specific architectures with cross-modal attention mechanisms to capture crucial cross-modal information and preserve modality-specific structural information.</li>
<li>results: The HEALNet architecture achieves state-of-the-art performance in multi-modal survival analysis on Whole Slide Images and Multi-omic data from four cancer cohorts in The Cancer Genome Atlas (TCGA), substantially improving over both uni-modal and recent multi-modal baselines, while being robust in scenarios with missing modalities.<details>
<summary>Abstract</summary>
Technological advances in medical data collection such as high-resolution histopathology and high-throughput genomic sequencing have contributed to the rising requirement for multi-modal biomedical modelling, specifically for image, tabular, and graph data. Most multi-modal deep learning approaches use modality-specific architectures that are trained separately and cannot capture the crucial cross-modal information that motivates the integration of different data sources. This paper presents the Hybrid Early-fusion Attention Learning Network (HEALNet): a flexible multi-modal fusion architecture, which a) preserves modality-specific structural information, b) captures the cross-modal interactions and structural information in a shared latent space, c) can effectively handle missing modalities during training and inference, and d) enables intuitive model inspection by learning on the raw data input instead of opaque embeddings. We conduct multi-modal survival analysis on Whole Slide Images and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas (TCGA). HEALNet achieves state-of-the-art performance, substantially improving over both uni-modal and recent multi-modal baselines, whilst being robust in scenarios with missing modalities.
</details>
<details>
<summary>摘要</summary>
技术进步在医疗数据收集中，如高分辨率 histopathology 和高通过put genomic sequencing，对多Modal生物医学模型的需求提高了。大多数多Modal深入学习方法使用专门的模式特性 architecture，这些模型在独立地训练，无法捕捉 crossing Modal 信息，这些信息是集成不同数据源的关键。这篇论文提出了 Hybrid Early-fusion Attention Learning Network (HEALNet)：一种灵活的多Modal融合建模 Architecture，具有以下特点：a) 保持 Modal 特有的结构信息b) 捕捉 crossing Modal 交互和结构信息在共享封装空间中c) 可以效果地处理训练和推断中缺失的 Modald) 允许直观地模型检查，通过学习原始数据输入而不是抽象封装我们在TCGA 四个肿瘤 cohort 上进行多Modal 存活分析，HEALNet 实现了状态之 arts 性能，大幅提高过uni-Modal 和 latest multi-Modal 基elines，同时在缺失 Modal 的情况下具有强健性。
</details></li>
</ul>
<hr>
<h2 id="Ever-Mitigating-Hallucination-in-Large-Language-Models-through-Real-Time-Verification-and-Rectification"><a href="#Ever-Mitigating-Hallucination-in-Large-Language-Models-through-Real-Time-Verification-and-Rectification" class="headerlink" title="Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification"></a>Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09114">http://arxiv.org/abs/2311.09114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoqiang Kang, Juntong Ni, Huaxiu Yao</li>
<li>for: 这个论文的目的是解决大语言模型（LLM）在生成文本时遇到的不准确或幻想内容问题。</li>
<li>methods: 该论文提出了一种实时验证和修正（Ever）方法，通过实时步骤的生成和幻想修正策略来检测和修正幻想错误。</li>
<li>results: 与基eline相比，Ever在多种任务上（包括短answer问题、生成传记和多步论证）表现出了显著的改善，能够生成可靠和事实正确的文本。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the "snowballing" issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based baselines, Ever demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including short-form QA, biography generation, and multi-hop reasoning.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型 (LLMs) 已经示出了惊人的流畅性，但它们经常遇到生成不准确或幻想内容的挑战。这个问题是非 retrieve-based 生成和 retrieve-augmented 生成方法中的共同问题，而现有的后续修正方法可能不能Address the accumulated hallucination errors that may be caused by the "snowballing" issue, especially in reasoning tasks。为解决这些挑战，我们介绍了一种新的方法called Real-time Verification and Rectification (Ever).而不是等待生成过程结束后进行修正幻想，Ever 使用了实时步骤生成和幻想修正策略。主要目标是在生成过程中实时检测和修正幻想。与 retrieve-based 和 non-retrieve-based 基线相比，Ever 在多种任务上，包括短问答、生传生成和多步逻辑 reasoning 等，示出了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Does-Pre-trained-Language-Model-Actually-Infer-Unseen-Links-in-Knowledge-Graph-Completion"><a href="#Does-Pre-trained-Language-Model-Actually-Infer-Unseen-Links-in-Knowledge-Graph-Completion" class="headerlink" title="Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?"></a>Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09109">http://arxiv.org/abs/2311.09109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusuke Sakai, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe</li>
<li>for: 本研究旨在探讨PLM-based KGC方法是否能够真正进行推理，或者只是通过Memorization来获得高性能。</li>
<li>methods: 我们提出了一种Synthetic dataset construction方法，用于分析PLM-based KGC方法是否能够进行推理。</li>
<li>results: 我们发现，PLMs通过预训练获得了推理能力，尽管表现改进主要来自于实体和关系文本信息。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) consist of links that describe relationships between entities. Due to the difficulty of manually enumerating all relationships between entities, automatically completing them is essential for KGs. Knowledge Graph Completion (KGC) is a task that infers unseen relationships between entities in a KG. Traditional embedding-based KGC methods, such as RESCAL, TransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using only the knowledge from training data. In contrast, the recent Pre-trained Language Model (PLM)-based KGC utilizes knowledge obtained during pre-training. Therefore, PLM-based KGC can estimate missing links between entities by reusing memorized knowledge from pre-training without inference. This approach is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications. To address this issue, we analyze whether PLM-based KGC methods make inferences or merely access memorized knowledge. For this purpose, we propose a method for constructing synthetic datasets specified in this analysis and conclude that PLMs acquire the inference abilities required for KGC through pre-training, even though the performance improvements mostly come from textual information of entities and relations.
</details>
<details>
<summary>摘要</summary>
知识图（KG）由关系链描述实体之间的关系。由于手动列出所有实体间关系的困难，自动完成这些关系是知识图的关键。知识图完成任务（KGC）是尝试推断实体间未知的关系。传统的嵌入式KGC方法，如RESCAL、TransE、DistMult、ComplEx、RotatE、HAKE、HousE等，通过训练数据来INFER未知的关系。与此相反，最近的预训练语言模型（PLM）基于KGC利用预训练中获得的知识。因此，PLM基于KGC可以估计实体间未知的关系，而不需要INFER。这种方法存在问题，因为建立KGC模型的目标是INFER实体间未知的关系。然而，现有的KGC评价方法不会分开考虑推断和嵌入能力。因此，一个PLM基于KGC方法，即在当前KGC评价中具有高性能，可能在实际应用中效果不佳。为解决这个问题，我们分析PLM基于KGC方法是否进行推断或只是访问嵌入知识。为此，我们提出一种方法构建定制化的 sintetic dataset，并结论PLM在预训练中获得了推断能力，即使表现改进主要来自实体和关系的文本信息。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Unified-View-of-Answer-Calibration-for-Multi-Step-Reasoning"><a href="#Towards-A-Unified-View-of-Answer-Calibration-for-Multi-Step-Reasoning" class="headerlink" title="Towards A Unified View of Answer Calibration for Multi-Step Reasoning"></a>Towards A Unified View of Answer Calibration for Multi-Step Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09101">http://arxiv.org/abs/2311.09101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shumin Deng, Ningyu Zhang, Nay Oo, Bryan Hooi</li>
<li>for: 该论文旨在探讨以Chain-of-Thought（CoT）提示方法改进多步逻辑能力的大语言模型（LLMs）。</li>
<li>methods: 该论文分析了近期的答栏准确策略，并提供了一种统一的视角，以便系统地检查多个路径上的步骤级和路径级答栏准确策略。</li>
<li>results: 该论文通过对多个路径上的答栏准确策略进行系统性的评估，探讨了多步逻辑的优化。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. Usually, answer calibration strategies such as step-level or path-level calibration play a vital role in multi-step reasoning. While effective, there remains a significant gap in our understanding of the key factors that drive their success. In this paper, we break down the design of recent answer calibration strategies and present a unified view which establishes connections between them. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Our study holds the potential to illuminate key insights for optimizing multi-step reasoning with answer calibration.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Can-MusicGen-Create-Training-Data-for-MIR-Tasks"><a href="#Can-MusicGen-Create-Training-Data-for-MIR-Tasks" class="headerlink" title="Can MusicGen Create Training Data for MIR Tasks?"></a>Can MusicGen Create Training Data for MIR Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09094">http://arxiv.org/abs/2311.09094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadine Kroher, Helena Cuesta, Aggelos Pikrakis</li>
<li>for: 这个论文是为了研究基于AI生成音乐系统来生成用于音乐信息检索（MIR）任务的训练数据而写的。</li>
<li>methods: 该论文使用了MusicGen生成器生成了5个音乐种类的大量人工音乐样本，并使用了这些样本来训练一个类别模型。</li>
<li>results: 实验结果表明，提议的模型可以从人工音乐辑中学习到类别特征，并能够在真实音乐录音中Generalize well。<details>
<summary>Abstract</summary>
We are investigating the broader concept of using AI-based generative music systems to generate training data for Music Information Retrieval (MIR) tasks. To kick off this line of work, we ran an initial experiment in which we trained a genre classifier on a fully artificial music dataset created with MusicGen. We constructed over 50 000 genre- conditioned textual descriptions and generated a collection of music excerpts that covers five musical genres. Our preliminary results show that the proposed model can learn genre-specific characteristics from artificial music tracks that generalise well to real-world music recordings.
</details>
<details>
<summary>摘要</summary>
我们正在研究使用基于人工智能的生成音乐系统来生成听力音乐信息检索（MIR）任务的训练数据。为了开始这条工作，我们进行了一次初步实验，在我们训练了一个类别分类器的基础上，使用了MusicGen创建的完全人工音乐数据集。我们构建了50000多个频道条件的文本描述，并生成了涵盖五种音乐类型的音乐片断集。我们的初步结果表明，我们的提议的模型可以从人工音乐追踪中学习类别特征，这些特征可以通过实际音乐录音来泛化。
</details></li>
</ul>
<hr>
<h2 id="The-Uli-Dataset-An-Exercise-in-Experience-Led-Annotation-of-oGBV"><a href="#The-Uli-Dataset-An-Exercise-in-Experience-Led-Annotation-of-oGBV" class="headerlink" title="The Uli Dataset: An Exercise in Experience Led Annotation of oGBV"></a>The Uli Dataset: An Exercise in Experience Led Annotation of oGBV</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09086">http://arxiv.org/abs/2311.09086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnav Arora, Maha Jinadoss, Cheshta Arora, Denny George, Brindaalakshmi, Haseena Dawood Khan, Kirti Rawat, Div, Ritash, Seema Mathur, Shivani Yadav, Shehla Rashid Shora, Rie Raut, Sumit Pawar, Apurva Paithane, Sonia, Vivek, Dharini Priscilla, Khairunnisha, Grace Banu, Ambika Tandon, Rishav Thakker, Rahul Dev Korra, Aatman Vaidya, Tarunima Prabhakar</li>
<li>for: 这个论文目的是为了提供一个语言特定和上下文相关的 dataset，以便开发自动识别 hate speech 和 gendered abuse 的 AI 系统。</li>
<li>methods: 这个论文使用了 Twitter 上的 tweets，并将其分类为三个问题：对于 gender abuse 的经历，由女性或 LGBTQIA 社区成员领导的专家进行标注。</li>
<li>results: 通过这个 dataset，研究人员展示了一种参与式的方法来创建 dataset，并通过这些 dataset 驱动 AI 系统。<details>
<summary>Abstract</summary>
Online gender based violence has grown concomitantly with adoption of the internet and social media. Its effects are worse in the Global majority where many users use social media in languages other than English. The scale and volume of conversations on the internet has necessitated the need for automated detection of hate speech, and more specifically gendered abuse. There is, however, a lack of language specific and contextual data to build such automated tools. In this paper we present a dataset on gendered abuse in three languages- Hindi, Tamil and Indian English. The dataset comprises of tweets annotated along three questions pertaining to the experience of gender abuse, by experts who identify as women or a member of the LGBTQIA community in South Asia. Through this dataset we demonstrate a participatory approach to creating datasets that drive AI systems.
</details>
<details>
<summary>摘要</summary>
互联网上的性别基于暴力现象随着互联网和社交媒体的普及而增长。其影响更加严重在全球主要地区，因为许多用户在不使用英语的情况下使用社交媒体。因为互联网上的规模和量的对话，需要自动检测 hate speech 和更Specifically gendered abuse。然而， Currently, there is a lack of language-specific and contextual data to build such automated tools. In this paper, we present a dataset on gendered abuse in three languages - Hindi, Tamil, and Indian English. The dataset consists of tweets annotated with three questions related to the experience of gender abuse, annotated by experts who identify as women or members of the LGBTQIA community in South Asia. Through this dataset, we demonstrate a participatory approach to creating datasets that drive AI systems.
</details></li>
</ul>
<hr>
<h2 id="How-Multilingual-is-Multilingual-LLM"><a href="#How-Multilingual-is-Multilingual-LLM" class="headerlink" title="How Multilingual is Multilingual LLM?"></a>How Multilingual is Multilingual LLM?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09071">http://arxiv.org/abs/2311.09071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Yuan, Shuai Yuan, Zhiyong Wu, Lei Li</li>
<li>for: 这项研究旨在评估大语言模型（LLMs）在101种语言中的多语言能力，并将语言分为四个不同的 quadrant，以便更好地了解这些语言的特点和 optimize their performance.</li>
<li>methods: 研究使用了现有的 LLMs，并通过对这些模型进行调整和训练来提高其多语言能力。</li>
<li>results: 研究发现，现有的 LLMs 在101种语言中的多语言能力比预期更高，并且可以通过对每个 quadrant 的特点进行调整来进一步提高多语言性能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), trained predominantly on extensive English data, often exhibit limitations when applied to other languages. Current research is primarily focused on enhancing the multilingual capabilities of these models by employing various tuning strategies. Despite their effectiveness in certain languages, the understanding of the multilingual abilities of LLMs remains incomplete. This study endeavors to evaluate the multilingual capacity of LLMs by conducting an exhaustive analysis across 101 languages, and classifies languages with similar characteristics into four distinct quadrants. By delving into each quadrant, we shed light on the rationale behind their categorization and offer actionable guidelines for tuning these languages. Extensive experiments reveal that existing LLMs possess multilingual capabilities that surpass our expectations, and we can significantly improve the multilingual performance of LLMs by focusing on these distinct attributes present in each quadrant.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），通常在广泛的英语数据上训练，在其他语言上表现有限。当前的研究主要集中在提高 LLM 的多语言能力，使用不同的调整策略。虽然在某些语言上有效，但我们对 LLM 的多语言能力的理解仍然不够完整。这项研究尝试对 101 种语言进行了全面的分析，并将语言分为四个不同的方块。我们对每个方块进行了详细的分析，并提供了改进 LLM 的多语言性表现的实用指南。广泛的实验表明，现有的 LLM 在多语言方面的能力超出了我们的预期，并且可以通过对每个方块的特点进行调整来进一步提高多语言性表现。
</details></li>
</ul>
<hr>
<h2 id="How-Well-Do-Large-Language-Models-Truly-Ground"><a href="#How-Well-Do-Large-Language-Models-Truly-Ground" class="headerlink" title="How Well Do Large Language Models Truly Ground?"></a>How Well Do Large Language Models Truly Ground?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09069">http://arxiv.org/abs/2311.09069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunji Lee, Sejune Joo, Chaeeun Kim, Joel Jang, Doyoung Kim, Kyoung-Woon On, Minjoon Seo</li>
<li>for: 这paper aims to improve the reliability and controllability of Large Language Models (LLMs) by introducing a stricter definition of grounding and developing a new dataset and metric to assess it.</li>
<li>methods: 该paper uses a new dataset and a grounding metric to evaluate the grounding capabilities of 13 different LLMs of various sizes and training methods.</li>
<li>results: 研究发现，现有的知识增强模型通常只关注response中是否包含正确答案，而忽略了response的可靠性和可控性。新的定义和 metric 能够评估模型是否真正基于知识进行回答，并提供了更多的信息来改进模型的可靠性和可控性。<details>
<summary>Abstract</summary>
Reliance on the inherent knowledge of Large Language Models (LLMs) can cause issues such as hallucinations, lack of control, and difficulties in integrating variable knowledge. To mitigate this, LLMs can be probed to generate responses by grounding on external context, often given as input (knowledge-augmented models). Yet, previous research is often confined to a narrow view of the term "grounding", often only focusing on whether the response contains the correct answer or not, which does not ensure the reliability of the entire response. To address this limitation, we introduce a strict definition of grounding: a model is considered truly grounded when its responses (1) fully utilize necessary knowledge from the provided context, and (2) don't exceed the knowledge within the contexts. We introduce a new dataset and a grounding metric to assess this new definition and perform experiments across 13 LLMs of different sizes and training methods to provide insights into the factors that influence grounding performance. Our findings contribute to a better understanding of how to improve grounding capabilities and suggest an area of improvement toward more reliable and controllable LLM applications.
</details>
<details>
<summary>摘要</summary>
依赖大语言模型（LLM）的内在知识可能会导致问题，如幻觉、无控和变量知识的集成问题。为了解决这些问题，LLM可以通过附加外部 контекст进行探索，并生成响应（知识增强型模型）。然而，过去的研究通常受限于“安全”的定义，即判断响应是否包含正确的答案，这并不能 garantuee 整个响应的可靠性。为了解决这些限制，我们提出了一个严格的定义：一个模型被 considere 为真正地附加了知识，当其响应（1）完全利用提供的 контекст中的所有必要知识，（2）不超过 контекст中的知识。我们介绍了一个新的数据集和附加 metric，以评估这个新定义，并在13种不同的 LLM 中进行了实验，以提供关于如何提高附加能力的深入了解和建议。我们的发现可以帮助改善 LLM 的可靠性和控制性，并且建议一个可以提高 LLM 应用的方向。
</details></li>
</ul>
<hr>
<h2 id="Learning-Fair-Division-from-Bandit-Feedback"><a href="#Learning-Fair-Division-from-Bandit-Feedback" class="headerlink" title="Learning Fair Division from Bandit Feedback"></a>Learning Fair Division from Bandit Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09068">http://arxiv.org/abs/2311.09068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hakuei Yamada, Junpei Komiyama, Kenshi Abe, Atsushi Iwasaki</li>
<li>for: 这篇论文研究了在不约束的线性鱼市中进行在线分配，在中央规划者不知道代理人的价值或利益下进行分配。</li>
<li>methods: 我们引入了一种封包算法，使用双平均来慢慢学习到来到达者的物品类型分布和代理人的价值。</li>
<li>results: 我们证明了我们的提议的算法可以在线ark Fisher市场中 asymptotically 实现 оптимальную拜纳社会利益，并提供了 regret bounds。我们还通过 sintetic 和实验数据 validate 了我们的算法的超越性。<details>
<summary>Abstract</summary>
This work addresses learning online fair division under uncertainty, where a central planner sequentially allocates items without precise knowledge of agents' values or utilities. Departing from conventional online algorithm, the planner here relies on noisy, estimated values obtained after allocating items. We introduce wrapper algorithms utilizing \textit{dual averaging}, enabling gradual learning of both the type distribution of arriving items and agents' values through bandit feedback. This approach enables the algorithms to asymptotically achieve optimal Nash social welfare in linear Fisher markets with agents having additive utilities. We establish regret bounds in Nash social welfare and empirically validate the superior performance of our proposed algorithms across synthetic and empirical datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="In-vehicle-Sensing-and-Data-Analysis-for-Older-Drivers-with-Mild-Cognitive-Impairment"><a href="#In-vehicle-Sensing-and-Data-Analysis-for-Older-Drivers-with-Mild-Cognitive-Impairment" class="headerlink" title="In-vehicle Sensing and Data Analysis for Older Drivers with Mild Cognitive Impairment"></a>In-vehicle Sensing and Data Analysis for Older Drivers with Mild Cognitive Impairment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09273">http://arxiv.org/abs/2311.09273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sonia Moshfeghi, Muhammad Tanveer Jan, Joshua Conniff, Seyedeh Gol Ara Ghoreishi, Jinwoo Jang, Borko Furht, Kwangsoo Yang, Monica Rosselli, David Newman, Ruth Tappen, Dana Smith</li>
<li>for: 这项研究的目的是设计低成本的在日常驾驶环境中获取高精度定位和电子邮件数据的汽车仪器，并通过机器学习方法早期发现老年人智能障碍的迹象。</li>
<li>methods: 这项研究使用了低成本的在汽车内部设备来获取高精度定位和电子邮件数据，并使用机器学习方法来检测老年人智能障碍的迹象。</li>
<li>results: 研究结果表明，老年人智能障碍的 drivers 在日常驾驶中比不受智能障碍的 drivers 驾驶更稳定和安全，而且机器学习模型也 identific 了驾驶次数、教育水平和夜间驾驶次数为最重要的因素。<details>
<summary>Abstract</summary>
Driving is a complex daily activity indicating age and disease related cognitive declines. Therefore, deficits in driving performance compared with ones without mild cognitive impairment (MCI) can reflect changes in cognitive functioning. There is increasing evidence that unobtrusive monitoring of older adults driving performance in a daily-life setting may allow us to detect subtle early changes in cognition. The objectives of this paper include designing low-cost in-vehicle sensing hardware capable of obtaining high-precision positioning and telematics data, identifying important indicators for early changes in cognition, and detecting early-warning signs of cognitive impairment in a truly normal, day-to-day driving condition with machine learning approaches. Our statistical analysis comparing drivers with MCI to those without reveals that those with MCI exhibit smoother and safer driving patterns. This suggests that drivers with MCI are cognizant of their condition and tend to avoid erratic driving behaviors. Furthermore, our Random Forest models identified the number of night trips, number of trips, and education as the most influential factors in our data evaluation.
</details>
<details>
<summary>摘要</summary>
驾驶是一项复杂的日常活动，表征年龄和疾病相关的认知下降。因此，与无明遇病患（MCI）相比，驾驶性能下降的差异可能反映认知功能的变化。有增加证据表明，在日常生活环境中不侵入式监测老年人驾驶行为可能有助于早期发现轻度认知障碍。本文的目标包括设计低成本的汽车内部感知硬件，获得高精度的位置定位和通信数据，确定重要的认知变化指标，并使用机器学习方法探测日常驾驶中的认知障碍警示。我们的统计分析表明，与MCI相比，有MCI的 Driver exhibit更稳定和更安全的驾驶模式。这表明，有MCI的 Driver 意识到自己的状况，并尽可能避免异常的驾驶行为。此外，我们的Random Forest模型确定了夜间行驶次数、总行驶次数和教育水平是我们数据评估中最重要的因素。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Knowledge-Editing-in-Language-Models-via-Relation-Perspective"><a href="#Assessing-Knowledge-Editing-in-Language-Models-via-Relation-Perspective" class="headerlink" title="Assessing Knowledge Editing in Language Models via Relation Perspective"></a>Assessing Knowledge Editing in Language Models via Relation Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09053">http://arxiv.org/abs/2311.09053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weiyifan1023/knowledge-edit-based-on-relation-perspective">https://github.com/weiyifan1023/knowledge-edit-based-on-relation-perspective</a></li>
<li>paper_authors: Yifan Wei, Xiaoyan Yu, Huanhuan Ma, Fangyu Lei, Yixuan Weng, Ran Song, Kang Liu</li>
<li>for: 本研究旨在修改大语言模型中的事实知识，并 investigate relation-centric 知识编辑方法的可行性。</li>
<li>methods: 本研究使用了一个新的benchmark名为RaKE，用于评估relation based知识编辑方法。还进行了多种知识编辑基线的比较实验，以及对 transformer 中关系知识的深入研究。</li>
<li>results: 研究结果表明，现有的知识编辑方法在编辑关系上存在潜在的困难，而且关系知识不仅存储在FFN网络中，还存储在注意层中。这些结果为未来的relation-based知识编辑方法提供了实验支持。<details>
<summary>Abstract</summary>
Knowledge Editing (KE) for modifying factual knowledge in Large Language Models (LLMs) has been receiving increasing attention. However, existing knowledge editing methods are entity-centric, and it is unclear whether this approach is suitable for a relation-centric perspective. To address this gap, this paper constructs a new benchmark named RaKE, which focuses on Relation based Knowledge Editing. In this paper, we establish a suite of innovative metrics for evaluation and conduct comprehensive experiments involving various knowledge editing baselines. We notice that existing knowledge editing methods exhibit the potential difficulty in their ability to edit relations. Therefore, we further explore the role of relations in factual triplets within the transformer. Our research results confirm that knowledge related to relations is not only stored in the FFN network but also in the attention layers. This provides experimental support for future relation-based knowledge editing methods.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）中的知识编辑（KE）已经获得了增加的注意。然而，现有的知识编辑方法都是基于实体中心的，而不是关系中心的。为了填补这个差距，本文建立了一个新的benchmark名为RaKE，它专注于关系基本知识编辑。本文提出了一个创新的评估标准和进行了各种知识编辑基线的广泛实验。我们发现现有的知识编辑方法对于修改关系表现出了潜在的问题。因此，我们进一步探索关系在简单 triplets 中的知识是如何储存和处理的。我们的研究结果显示，关系知识不仅在 FFN 网络中储存，还在注意层中储存。这给了未来关系基本知识编辑方法的实验支持。
</details></li>
</ul>
<hr>
<h2 id="Improving-Zero-shot-Visual-Question-Answering-via-Large-Language-Models-with-Reasoning-Question-Prompts"><a href="#Improving-Zero-shot-Visual-Question-Answering-via-Large-Language-Models-with-Reasoning-Question-Prompts" class="headerlink" title="Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts"></a>Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09050">http://arxiv.org/abs/2311.09050</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecnu-dase-nlp/rqp">https://github.com/ecnu-dase-nlp/rqp</a></li>
<li>paper_authors: Yunshi Lan, Xiang Li, Xin Liu, Yang Li, Wei Qin, Weining Qian</li>
<li>for: 本研究旨在提高零shot情境下的视觉问答系统（VQA）的性能，通过帮助大语言模型（LLMs）更好地理解和回答问题。</li>
<li>methods: 我们提出了一种新的问题提示方法，即理解问题提示（Reasoning Question Prompts，RQP），可以让LLMs更好地理解和回答问题。RQP通过一个不supervised的问题编辑模块生成了每个问题的自 contenido问题，以便更好地指导LLMs回答问题。</li>
<li>results: 我们在三个VQA挑战中测试了RQP方法，结果表明，RQP可以在零shot情境下显著提高LLMs的性能，并在四个数据集中超越现有的零shot方法。我们的源代码已经公开在GitHub上（<a target="_blank" rel="noopener" href="https://github.com/ECNU-DASE-NLP/RQP%EF%BC%89%E3%80%82">https://github.com/ECNU-DASE-NLP/RQP）。</a><details>
<summary>Abstract</summary>
Zero-shot Visual Question Answering (VQA) is a prominent vision-language task that examines both the visual and textual understanding capability of systems in the absence of training data. Recently, by converting the images into captions, information across multi-modalities is bridged and Large Language Models (LLMs) can apply their strong zero-shot generalization capability to unseen questions. To design ideal prompts for solving VQA via LLMs, several studies have explored different strategies to select or generate question-answer pairs as the exemplar prompts, which guide LLMs to answer the current questions effectively. However, they totally ignore the role of question prompts. The original questions in VQA tasks usually encounter ellipses and ambiguity which require intermediate reasoning. To this end, we present Reasoning Question Prompts for VQA tasks, which can further activate the potential of LLMs in zero-shot scenarios. Specifically, for each question, we first generate self-contained questions as reasoning question prompts via an unsupervised question edition module considering sentence fluency, semantic integrity and syntactic invariance. Each reasoning question prompt clearly indicates the intent of the original question. This results in a set of candidate answers. Then, the candidate answers associated with their confidence scores acting as answer heuristics are fed into LLMs and produce the final answer. We evaluate reasoning question prompts on three VQA challenges, experimental results demonstrate that they can significantly improve the results of LLMs on zero-shot setting and outperform existing state-of-the-art zero-shot methods on three out of four data sets. Our source code is publicly released at \url{https://github.com/ECNU-DASE-NLP/RQP}.
</details>
<details>
<summary>摘要</summary>
zero-shot视觉问答（VQA）是一种引人注目的视觉语言任务，它检验系统在不受训练数据的情况下，对图像和文本之间的理解能力。最近，通过将图像转换为caption，使得多 modalities之间的信息相互汇流，大型自然语言模型（LLMs）可以通过未经训练的情况下，对未经见过的问题进行有效的回答。为了设计理想的提问方法，许多研究已经探索了不同的策略来选择或生成问题答对，作为示例提问。然而，它们完全忽视了提问的角色。原始的VQA任务中的问题通常会遇到斜杠和混乱，需要中间的推理。为此，我们提出了视觉问答推理提问（RQP），可以further activate LLMs的零shot能力。具体来说，为每个问题，我们首先通过不supervised问题编辑模块生成自包含的推理提问，考虑语言流畅性、意义完整性和语法不变性。每个推理提问都能够明确表达问题的意图。这些推理提问的候选答案与其自信度分数 acting as answer heuristics被 fed into LLMs，并生成最终的答案。我们在三个VQA挑战中评估了推理提问，实验结果表明，它们可以在零shot Setting下significantly提高LLMs的表现，并在四个数据集中超越现有的零shot方法。我们的源代码公开release于\url{https://github.com/ECNU-DASE-NLP/RQP}.
</details></li>
</ul>
<hr>
<h2 id="MELA-Multilingual-Evaluation-of-Linguistic-Acceptability"><a href="#MELA-Multilingual-Evaluation-of-Linguistic-Acceptability" class="headerlink" title="MELA: Multilingual Evaluation of Linguistic Acceptability"></a>MELA: Multilingual Evaluation of Linguistic Acceptability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09033">http://arxiv.org/abs/2311.09033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao, Rui Wang, Hai Hu</li>
<li>for: 本研究的目的是提供一个多语言的语言模型评估 benchmark，以evaluate 不同语言模型在语言学可接受性方面的表现。</li>
<li>methods: 本研究使用了多种语言模型，包括ChatGPT和XLM-R，并进行了过程学习和多任务学习。同时，研究者们还使用了层 wise probing 来分析 XLM-R 的 weights 是如何影响其在不同语言之间的推理能力。</li>
<li>results: 研究结果显示，XLM-R 在 zero-shot  Setting 中可以达到与 fine-tuned XLM-R 相当的性能，而 ChatGPT 则需要在 Context 中提供示例来改善其性能。同时，研究者们还发现了一些语言之间的推理困难，并提出了一种” conflicting weight” 的概念来描述这种现象。<details>
<summary>Abstract</summary>
Recent benchmarks for Large Language Models (LLMs) have mostly focused on application-driven tasks such as complex reasoning and code generation, and this has led to a scarcity in purely linguistic evaluation of LLMs. Against this background, we introduce Multilingual Evaluation of Linguistic Acceptability -- MELA, the first multilingual benchmark on linguistic acceptability with 48K samples covering 10 languages from a diverse set of language families. We establish baselines of commonly used LLMs along with supervised models, and conduct cross-lingual transfer and multi-task learning experiments with XLM-R. In pursuit of multilingual interpretability, we analyze the weights of fine-tuned XLM-R to explore the possibility of identifying transfer difficulty between languages. Our results show that ChatGPT benefits much from in-context examples but still lags behind fine-tuned XLM-R, while the performance of GPT-4 is on par with fine-tuned XLM-R even in zero-shot setting. Cross-lingual and multi-task learning experiments show that unlike semantic tasks, in-language training data is crucial in acceptability judgements. Results in layerwise probing indicate that the upper layers of XLM-R become a task-specific but language-agnostic region for multilingual acceptability judgment. We also introduce the concept of conflicting weight, which could be a potential indicator for the difficulty of cross-lingual transfer between languages. Our data will be available at https://github.com/sjtu-compling/MELA.
</details>
<details>
<summary>摘要</summary>
近期大语言模型（LLM）的 benchmark 主要集中在应用驱动的任务上，如复杂的理解和代码生成，这导致了对 LLM 的纯语言评估的缺乏。为了解决这问题，我们介绍了多语言评估语言可接受性（MELA），这是一个包含 48K 个样本，覆盖 10 种语言家族的多语言 benchmark。我们建立了常用的 LLG 基elines，以及supervised 模型的基elines，并进行了跨语言传播和多任务学习实验。在追求多语言可读性的探索中，我们分析了精心调整的 XLM-R 的权重，以探索语言之间传播困难的可能性。我们的结果显示，ChatGPT 受到上下文例子的启发，但仍落后于精心调整的 XLM-R，而 GPT-4 在零shot 设定下与精心调整的 XLM-R 的性能相当。跨语言和多任务学习实验表明，与 semantic 任务不同，在语言上的培训数据是关键在 acceptability 判断中。层wise probing 结果表明，XLM-R 的Upper层变成了多语言可接受性的任务特定 yet language-agnostic 区域。我们还引入了 conflicting weight 概念，它可能是跨语言传播之间语言的难度指标。我们的数据将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Robustness-of-Intelligence-Driven-Reinforcement-Learning"><a href="#Assessing-the-Robustness-of-Intelligence-Driven-Reinforcement-Learning" class="headerlink" title="Assessing the Robustness of Intelligence-Driven Reinforcement Learning"></a>Assessing the Robustness of Intelligence-Driven Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09027">http://arxiv.org/abs/2311.09027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Nodari, Federico Cerutti</li>
<li>for: This paper focuses on the problem of robustness in intelligence-driven reinforcement learning, specifically in military contexts where high stakes and uncertainty are prevalent.</li>
<li>methods: The paper employs reward machines to express complex reward structures in RL tasks, and explores the need for further research in evidential reasoning and learning to improve the robustness of current state-of-the-art reinforcement learning approaches.</li>
<li>results: The preliminary results presented in the paper suggest the need for further research to harden current RL approaches before they can be considered mission-critical-ready.<details>
<summary>Abstract</summary>
Robustness to noise is of utmost importance in reinforcement learning systems, particularly in military contexts where high stakes and uncertain environments prevail. Noise and uncertainty are inherent features of military operations, arising from factors such as incomplete information, adversarial actions, or unpredictable battlefield conditions. In RL, noise can critically impact decision-making, mission success, and the safety of personnel. Reward machines offer a powerful tool to express complex reward structures in RL tasks, enabling the design of tailored reinforcement signals that align with mission objectives. This paper considers the problem of the robustness of intelligence-driven reinforcement learning based on reward machines. The preliminary results presented suggest the need for further research in evidential reasoning and learning to harden current state-of-the-art reinforcement learning approaches before being mission-critical-ready.
</details>
<details>
<summary>摘要</summary>
<<SYS>>military contexts 的 robustness to noise 是权重要的，特别是在高赌注和不确定环境下。雨声和不确定性是军事操作的内生特征，由于因素如不完整信息、敌方行动或不可预测的战场条件而出现。在RL中，雨声可能会重要影响决策、任务成功和人员安全。奖励机器提供了一种强大的工具来表达复杂的奖励结构在RL任务中，使得设计定制化的奖励信号与任务目标相对应。本文考虑了奖励机器驱动的智能学习robustness问题。初步结果表明需要进一步研究证据推理和学习以强化当前状态艺术RL方法，以便在任务关键ready。<<SYS>>
</details></li>
</ul>
<hr>
<h2 id="Identification-and-Estimation-for-Nonignorable-Missing-Data-A-Data-Fusion-Approach"><a href="#Identification-and-Estimation-for-Nonignorable-Missing-Data-A-Data-Fusion-Approach" class="headerlink" title="Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach"></a>Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09015">http://arxiv.org/abs/2311.09015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiao Wang, AmirEmad Ghassami, Ilya Shpitser</li>
<li>for: identifying and estimating a parameter of interest in settings where data is missing not at random (MNAR)</li>
<li>methods: inspired by data fusion, using information in an MNAR dataset and an auxiliary dataset subject to missingness at random (MAR)</li>
<li>results: can identify the parameter of interest given pooled data, under two complementary sets of assumptions; derived an inverse probability weighted (IPW) estimator for identified parameters, and evaluated the performance of the estimation strategies via simulation studies<details>
<summary>Abstract</summary>
We consider the task of identifying and estimating a parameter of interest in settings where data is missing not at random (MNAR). In general, such parameters are not identified without strong assumptions on the missing data model. In this paper, we take an alternative approach and introduce a method inspired by data fusion, where information in an MNAR dataset is augmented by information in an auxiliary dataset subject to missingness at random (MAR). We show that even if the parameter of interest cannot be identified given either dataset alone, it can be identified given pooled data, under two complementary sets of assumptions. We derive an inverse probability weighted (IPW) estimator for identified parameters, and evaluate the performance of our estimation strategies via simulation studies.
</details>
<details>
<summary>摘要</summary>
我团队考虑了在数据损失不均匀（MNAR）的情况下识别和估算参数 интереса。通常情况下，这些参数无法 sans strong assumptions on the missing data model。在这篇论文中，我们采取了一种不同的方法，并通过数据融合引入了一个auxiliary dataset，这个dataset受到随机 missing（MAR）。我们表明，即使 données alone 中的参数无法识别，也可以通过 combining data 识别出参数，只要满足两个 complementary sets of assumptions。我们 derivate了一种 inverse probability weighted（IPW）估计器，并通过 simulations studies 评估了我们的估计策略的性能。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-to-Reward-Machine-based-Reinforcement-Learning"><a href="#Adversarial-Attacks-to-Reward-Machine-based-Reinforcement-Learning" class="headerlink" title="Adversarial Attacks to Reward Machine-based Reinforcement Learning"></a>Adversarial Attacks to Reward Machine-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09014">http://arxiv.org/abs/2311.09014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Nodari</li>
<li>for: 本研究旨在提供首个对奖金机制（RM）基于 reinforcement learning 技术的安全性分析，以便更好地理解和提高这种技术在不良场景下的稳定性。</li>
<li>methods: 本研究使用 blinding attacks 这种新的攻击方法，以评估 RM-based reinforcement learning 技术的安全性。</li>
<li>results: 研究发现，blinding attacks 可以成功地破坏 RM-based reinforcement learning 技术的安全性，并提供了一种新的攻击方法来攻击这种技术。<details>
<summary>Abstract</summary>
In recent years, Reward Machines (RMs) have stood out as a simple yet effective automata-based formalism for exposing and exploiting task structure in reinforcement learning settings. Despite their relevance, little to no attention has been directed to the study of their security implications and robustness to adversarial scenarios, likely due to their recent appearance in the literature. With my thesis, I aim to provide the first analysis of the security of RM-based reinforcement learning techniques, with the hope of motivating further research in the field, and I propose and evaluate a novel class of attacks on RM-based techniques: blinding attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Leveraging-AI-for-Natural-Disaster-Management-Takeaways-From-The-Moroccan-Earthquake"><a href="#Leveraging-AI-for-Natural-Disaster-Management-Takeaways-From-The-Moroccan-Earthquake" class="headerlink" title="Leveraging AI for Natural Disaster Management : Takeaways From The Moroccan Earthquake"></a>Leveraging AI for Natural Disaster Management : Takeaways From The Moroccan Earthquake</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08999">http://arxiv.org/abs/2311.08999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morocco Solidarity Hackathon</li>
<li>for: 这篇论文主要是为了探讨在2023年阿哈鲁兹地震后，全球灾害管理策略的批判性反思，以及使用人工智能（AI）提高灾害准备、应急回应和恢复的技术。</li>
<li>methods: 这篇论文使用了全面的文献综述、赢得项目概述、关键发现和挑战，包括实时开源数据、数据缺乏和交叉学科合作的障碍。</li>
<li>results: 这篇论文得到了许多关键发现和挑战，包括实时开源数据的潜在价值、数据缺乏的问题和交叉学科合作的障碍。同时，论文还发起了社区呼吁，呼吁更多的行业专家和学者参与到灾害管理领域的研究和实践中来。<details>
<summary>Abstract</summary>
The devastating 6.8-magnitude earthquake in Al Haouz, Morocco in 2023 prompted critical reflections on global disaster management strategies, resulting in a post-disaster hackathon, using artificial intelligence (AI) to improve disaster preparedness, response, and recovery. This paper provides (i) a comprehensive literature review, (ii) an overview of winning projects, (iii) key insights and challenges, namely real-time open-source data, data scarcity, and interdisciplinary collaboration barriers, and (iv) a community-call for further action.
</details>
<details>
<summary>摘要</summary>
在2023年Morocco的阿卢哈沃兹发生了6.8级地震，这导致了全球灾害管理策略的批判性反思，并且促使了一场以人工智能（AI）为核心的 poste-disaster hackathon，以提高灾害准备、应急回应和恢复。本文提供以下内容：1. 全面的文献综述2. 赢家项目的概述3. 关键的发现和挑战，包括实时开源数据、数据缺乏和跨学科协作障碍4. 社区呼吁更进一步的行动Translation notes:* "阿卢哈沃兹" (Al Haouz) is the name of the location where the earthquake occurred, and it is written in Simplified Chinese as "阿卢哈沃兹" (Al Haouz).* "灾害管理策略" (disaster management strategies) is written as "灾害管理策略" (disaster management strategies) in Simplified Chinese.* "poste-disaster hackathon" is written as "后灾害黑匠挑战" (post-disaster hackathon) in Simplified Chinese.* "实时开源数据" (real-time open-source data) is written as "实时开源数据" (real-time open-source data) in Simplified Chinese.* "数据缺乏" (data scarcity) is written as "数据缺乏" (data scarcity) in Simplified Chinese.* "跨学科协作障碍" (interdisciplinary collaboration barriers) is written as "跨学科协作障碍" (interdisciplinary collaboration barriers) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="When-does-In-context-Learning-Fall-Short-and-Why-A-Study-on-Specification-Heavy-Tasks"><a href="#When-does-In-context-Learning-Fall-Short-and-Why-A-Study-on-Specification-Heavy-Tasks" class="headerlink" title="When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks"></a>When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08993">http://arxiv.org/abs/2311.08993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yunjia Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng, Bin Xu, Lei Hou, Juanzi Li</li>
<li>for: 本文旨在探讨大语言模型（LLM）在启发式学习（ICL）方法下的局限性，以及这些局限性的根本原因。</li>
<li>methods: 作者通过对18种特有的任务进行广泛的实验，发现ICL在处理这些任务时存在三个主要的原因：无法具体地理解上下文，任务架构理解与人类不一致，以及缺乏长文理解能力。</li>
<li>results: 研究发现，通过细化调教，LLM可以在这些任务上达到不错的性能，这表明ICL的失败不是LLM的内在缺陷，而是现有的对齐方法的不足，使LLM无法处理复杂的规则繁残任务。<details>
<summary>Abstract</summary>
In-context learning (ICL) has become the default method for using large language models (LLMs), making the exploration of its limitations and understanding the underlying causes crucial. In this paper, we find that ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications, requiring several hours for ordinary humans to master, such as traditional information extraction tasks. The performance of ICL on these tasks mostly cannot reach half of the state-of-the-art results. To explore the reasons behind this failure, we conduct comprehensive experiments on 18 specification-heavy tasks with various LLMs and identify three primary reasons: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. Furthermore, we demonstrate that through fine-tuning, LLMs can achieve decent performance on these tasks, indicating that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback of existing alignment methods that renders LLMs incapable of handling complicated specification-heavy tasks via ICL. To substantiate this, we perform dedicated instruction tuning on LLMs for these tasks and observe a notable improvement. We hope the analyses in this paper could facilitate advancements in alignment methods enabling LLMs to meet more sophisticated human demands.
</details>
<details>
<summary>摘要</summary>
启发式学习（ICL）已成为大语言模型（LLM）的默认方法，因此探索其限制和理解下面层次的原因变得非常重要。在这篇论文中，我们发现ICL在需要较多任务规定的任务上表现不佳，这些任务通常需要人类花费几个小时来学习，如传统信息抽取任务。ICL的性能在这些任务上通常无法达到状态艺术的一半。为了探索这些失败的原因，我们进行了18个需要较多任务规定的任务的广泛实验，并确定了三个主要原因：无法特别理解上下文，任务架构与人类的理解不符，以及缺乏长文本理解能力。此外，我们还证明了通过微调，LLM可以在这些任务上达到不错的表现，这表明ICL失败不是LLM的内在缺陷，而是现有的对齐方法的缺陷，使得LLM无法通过ICL处理复杂的需要较多任务。为了证明这一点，我们在LLM上进行了专门的指令调整，并观察到了明显的改善。我们希望这些分析可以促进对齐方法的进步，使LLM能够更好地满足人类的需求。
</details></li>
</ul>
<hr>
<h2 id="Proceedings-Fifth-International-Workshop-on-Formal-Methods-for-Autonomous-Systems"><a href="#Proceedings-Fifth-International-Workshop-on-Formal-Methods-for-Autonomous-Systems" class="headerlink" title="Proceedings Fifth International Workshop on Formal Methods for Autonomous Systems"></a>Proceedings Fifth International Workshop on Formal Methods for Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08987">http://arxiv.org/abs/2311.08987</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Marie Farrell, Matt Luckcuck, Mario Gleirscher, Maike Schwammberger</li>
<li>for: 本研讨会论文集是为了形式方法与自主系统之间的研究提供一个发表平台。</li>
<li>methods: 本研讨会接受了25篇提交论文，其中包括11篇正式论文、3篇经验报告、6篇研究预览和5篇视野论文。</li>
<li>results: 经审核后，本研讨会接受了15篇论文，包括8篇长篇论文和7篇短篇论文。<details>
<summary>Abstract</summary>
This EPTCS volume contains the proceedings for the Fifth International Workshop on Formal Methods for Autonomous Systems (FMAS 2023), which was held on the 15th and 16th of November 2023. FMAS 2023 was co-located with 18th International Conference on integrated Formal Methods (iFM) (iFM'22), organised by Leiden Institute of Advanced Computer Science of Leiden University. The workshop itself was held at Scheltema Leiden, a renovated 19th Century blanket factory alongside the canal.   FMAS 2023 received 25 submissions. We received 11 regular papers, 3 experience reports, 6 research previews, and 5 vision papers. The researchers who submitted papers to FMAS 2023 were from institutions in: Australia, Canada, Colombia, France, Germany, Ireland, Italy, the Netherlands, Sweden, the United Kingdom, and the United States of America. Increasing our number of submissions for the third year in a row is an encouraging sign that FMAS has established itself as a reputable publication venue for research on the formal modelling and verification of autonomous systems. After each paper was reviewed by three members of our Programme Committee we accepted a total of 15 papers: 8 long papers and 7 short papers.
</details>
<details>
<summary>摘要</summary>
这个 EPTCS 卷包含了第五届国际形式方法工作坊（FMAS 2023）的论文集，该活动于2023年11月15日-16日举行。FMAS 2023 与18届国际集成形式方法会议（iFM）（iFM'22）联合举办，由雷登大学计算机科学院主办。工作坊本身在19世纪19世纪的重新翻新的褡厂 alongside the canal 举行。 FMAS 2023 接受了25篇提交的论文，包括11篇正式论文、3篇经验报告、6篇研究预览和5篇视野论文。参加该活动的研究人员来自：澳大利亚、加拿大、哥伦比亚、法国、德国、爱尔兰、意大利、荷兰、瑞典、英国和美国。我们在第三年 consecutively 收到更多的提交，表明 FMAS 已经成为自动化系统的正式模型和验证的出版物。经过三名编委会成员的审核后，我们接受了总共15篇论文：8篇长篇和7篇短篇。
</details></li>
</ul>
<hr>
<h2 id="Linear-time-Evidence-Accumulation-Clustering-with-KMeans"><a href="#Linear-time-Evidence-Accumulation-Clustering-with-KMeans" class="headerlink" title="Linear time Evidence Accumulation Clustering with KMeans"></a>Linear time Evidence Accumulation Clustering with KMeans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09272">http://arxiv.org/abs/2311.09272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaëlle Candel</li>
<li>for: 本研究旨在提出一种简单 yet efficient consensus clustering方法，以解决现有方法的计算复杂性问题。</li>
<li>methods: 本方法基于证据积累 clustering，首先构建一个 n x n 的相关性矩阵，然后使用这个矩阵进行 clustering，以提取共识 clusters。与其他方法不同的是，这里不需要找到匹配于两个不同 partitioning 中的匹配项。但是，这种方法受到计算复杂性的限制，只适用于小规模 dataset。</li>
<li>results: 本研究提出了一种方法来高效计算 density，从而降低了计算复杂性的问题。此外，我们证明了 k-means 自然地maximizes density。在多个 benchmark dataset 上进行了比较，k-means 和 bisecting 版本的结果与其他现有的 consensus algorithm 相当，而且计算成本较低。此外，k-means 在 density 方面获得了最佳结果。这些结果表明，consensus clustering 可以使用简单的算法解决。<details>
<summary>Abstract</summary>
Among ensemble clustering methods, Evidence Accumulation Clustering is one of the simplest technics. In this approach, a co-association (CA) matrix representing the co-clustering frequency is built and then clustered to extract consensus clusters. Compared to other approaches, this one is simple as there is no need to find matches between clusters obtained from two different partitionings. Nevertheless, this method suffers from computational issues, as it requires to compute and store a matrix of size n x n, where n is the number of items. Due to the quadratic cost, this approach is reserved for small datasets. This work describes a trick which mimic the behavior of average linkage clustering. We found a way of computing efficiently the density of a partitioning, reducing the cost from a quadratic to linear complexity. Additionally, we proved that the k-means maximizes naturally the density. We performed experiments on several benchmark datasets where we compared the k-means and the bisecting version to other state-of-the-art consensus algorithms. The k-means results are comparable to the best state of the art in terms of NMI while keeping the computational cost low. Additionally, the k-means led to the best results in terms of density. These results provide evidence that consensus clustering can be solved with simple algorithms.
</details>
<details>
<summary>摘要</summary>
在ensemble clustering方法中，证据积累 clustering 是一种最简单的方法。在这种方法中，我们首先构建一个 co-association（CA）矩阵，表示item之间的协 clustering频率，然后使用这个矩阵进行归一化，以提取共识cluster。相比其他方法，这种方法更简单，不需要在两个不同的 partitioning 中找到匹配。然而，这种方法受到计算问题的限制，因为需要计算和存储一个 n x n 的矩阵，其中 n 是items的数量，这会导致计算成本 quadratic。由于这个问题，这种方法只适用于小型数据集。本文描述了一种技巧，可以模拟average linkage clustering的行为。我们发现了一种可以高效计算分区 densities 的方法，从而降低计算成本的复杂度从 quadratic 降至 linear。此外，我们证明了 k-means 自然地 maximizes densities。我们在多个 benchmark 数据集上进行了实验，并与其他状态Of-the-art consensus算法进行了比较。k-means 的结果与最佳状态Of-the-art 的 NMI 相当，同时计算成本低。此外，k-means 导致了最佳的 densities 结果。这些结果证明了 consensus clustering 可以使用简单的算法解决。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Linear-Relational-Concepts-in-Large-Language-Models"><a href="#Identifying-Linear-Relational-Concepts-in-Large-Language-Models" class="headerlink" title="Identifying Linear Relational Concepts in Large Language Models"></a>Identifying Linear Relational Concepts in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08968">http://arxiv.org/abs/2311.08968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Robot-learning">https://github.com/Aryia-Behroziuan/Robot-learning</a></li>
<li>paper_authors: David Chanin, Anthony Hunter, Oana-Maria Camburu</li>
<li>for: 本文旨在找到隐藏层中的概念方向，以便更好地理解模型表示的概念。</li>
<li>methods: 本文提出了一种Linear Relational Concepts（LRC）技术，通过模型Subject和Object之间的关系为线性关系嵌入（LRE）来找到隐藏层中的概念方向。</li>
<li>results: 研究发现，通过逆向LRE并使用早期的对象层来找到概念方向，可以实现高效地为概念分类和影响模型输出。<details>
<summary>Abstract</summary>
Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any given human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts at a given hidden layer in a transformer LM by first modeling the relation between subject and object as a linear relational embedding (LRE). While the LRE work was mainly presented as an exercise in understanding model representations, we find that inverting the LRE while using earlier object layers results in a powerful technique to find concept directions that both work well as a classifier and causally influence model outputs.
</details>
<details>
<summary>摘要</summary>
transformer 语言模型（LM）已经显示出在隐藏活动空间中表示概念的方向。然而，为任何给定的人类可解释的概念，如何在隐藏层中找到其方向？我们提出了线性关系概念（LRC）技术，用于在 transformer LM 中找到人类可解释的概念方向。我们首先将关系 между主题和对象模型为线性关系嵌入（LRE）。虽然 LRE 工作主要被表现为模型表示理解的一种实践，但我们发现，对于早期对象层来说，倒转 LRE 会生成一种强大的技术，可以作为分类器并在模型输出中产生 causal 影响。
</details></li>
</ul>
<hr>
<h2 id="I-Was-Blind-but-Now-I-See-Implementing-Vision-Enabled-Dialogue-in-Social-Robots"><a href="#I-Was-Blind-but-Now-I-See-Implementing-Vision-Enabled-Dialogue-in-Social-Robots" class="headerlink" title="I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots"></a>I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08957">http://arxiv.org/abs/2311.08957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulio Antonio Abbo, Tony Belpaeme</li>
<li>for: 该论文旨在探讨如何通过将视觉功能 integrate into conversational agents ，以提高人机交互的效果。</li>
<li>methods: 该论文使用最新的大语言模型（如 GPT-4、IDEFICS）来解释文本提示和实时视觉输入，创造出更Contextually 意识的对话系统。</li>
<li>results: 六个与 Furhat 机器人进行的交互记录和分析，ILLUSTRATE 和讨论所获得的结果，提出了一种将文本和视觉modalities融合的对话系统。<details>
<summary>Abstract</summary>
In the rapidly evolving landscape of human-computer interaction, the integration of vision capabilities into conversational agents stands as a crucial advancement. This paper presents an initial implementation of a dialogue manager that leverages the latest progress in Large Language Models (e.g., GPT-4, IDEFICS) to enhance the traditional text-based prompts with real-time visual input. LLMs are used to interpret both textual prompts and visual stimuli, creating a more contextually aware conversational agent. The system's prompt engineering, incorporating dialogue with summarisation of the images, ensures a balance between context preservation and computational efficiency. Six interactions with a Furhat robot powered by this system are reported, illustrating and discussing the results obtained. By implementing this vision-enabled dialogue system, the paper envisions a future where conversational agents seamlessly blend textual and visual modalities, enabling richer, more context-aware dialogues.
</details>
<details>
<summary>摘要</summary>
在人机交互领域的快速发展中，融合视觉能力的对话管理器是一项重要的进步。这篇论文介绍了一种使用最新的大语言模型（如GPT-4、IDEFICS）来增强传统的文本基于的提示，并在实时视觉输入的基础上进行对话管理。这些语言模型能够同时解释文本提示和视觉刺激，创造出更Contextually 意识的对话代理人。系统的提问工程，包括对话和图像摘要，保证了对话的上下文保持和计算效率的平衡。报告了六次与furhat机器人运行此系统的交互，并讲述了获得的结果。通过实现这种视觉启用对话系统，论文预测未来的对话代理人将协调文本和视觉模式，实现更加 ricther，Contextually 意识的对话。
</details></li>
</ul>
<hr>
<h2 id="Safety-Trust-and-Ethics-Considerations-for-Human-AI-Teaming-in-Aerospace-Control"><a href="#Safety-Trust-and-Ethics-Considerations-for-Human-AI-Teaming-in-Aerospace-Control" class="headerlink" title="Safety, Trust, and Ethics Considerations for Human-AI Teaming in Aerospace Control"></a>Safety, Trust, and Ethics Considerations for Human-AI Teaming in Aerospace Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08943">http://arxiv.org/abs/2311.08943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kerianne L. Hobbs, Bernard Li</li>
<li>for: 本文旨在探讨人工智能在航空系统控制中的合作，特别是人类和AI的团队合作，以及这些团队合作的安全、可靠和伦理方面。</li>
<li>methods: 本文使用了许多不同的方法，包括文献综述、案例研究和理论分析，以探讨不同的人工智能应用场景和相关的安全、可靠和伦理问题。</li>
<li>results: 本文的结果表明，在安全和任务关键领域中使用人工智能时，需要考虑到安全、可靠和伦理方面的问题，并且需要采取相应的措施来解决这些问题。<details>
<summary>Abstract</summary>
Designing a safe, trusted, and ethical AI may be practically impossible; however, designing AI with safe, trusted, and ethical use in mind is possible and necessary in safety and mission-critical domains like aerospace. Safe, trusted, and ethical use of AI are often used interchangeably; however, a system can be safely used but not trusted or ethical, have a trusted use that is not safe or ethical, and have an ethical use that is not safe or trusted. This manuscript serves as a primer to illuminate the nuanced differences between these concepts, with a specific focus on applications of Human-AI teaming in aerospace system control, where humans may be in, on, or out-of-the-loop of decision-making.
</details>
<details>
<summary>摘要</summary>
设计一个安全、可信、伦理的人工智能可能是实际上不可能的；但是设计人工智能以安全、可信、伦理的使用为目标是可能的和必要的，尤其在安全和战略性领域如航空航天。安全、可信、伦理的使用人工智能常常被混用，但是一个系统可以安全地使用但不是可信或伦理的，可以有一个可信用但不是安全或伦理的，可以有一个伦理用但不是安全或可信的。这篇报告作为一个导论，探讨了这些概念之间的细腻差异，尤其在人工智能和人类团队在航空系统控制中的应用， где人类可能在、在或离Loop的决策过程中。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-over-Description-Logic-based-Contexts-with-Transformers"><a href="#Reasoning-over-Description-Logic-based-Contexts-with-Transformers" class="headerlink" title="Reasoning over Description Logic-based Contexts with Transformers"></a>Reasoning over Description Logic-based Contexts with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08941">http://arxiv.org/abs/2311.08941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelos Poulis, Eleni Tsalapati, Manolis Koubarakis</li>
<li>for: 本研究的目的是测试 transformer 模型在复杂的语言上进行推理能力。</li>
<li>methods: 本研究使用了生成自描述逻辑知识库的自然语言问答数据集，并使用了 $\mathcal{ALCQ}$ 语言来生成知识库。</li>
<li>results: 研究发现，使用 DEBERTa 模型 DELTA$_M$ 的表现随 reasoning depth 的增加而无显著变化，而 sentence length 的增加则不会影响表现。此外，模型在不同的 reasoning depth 上进行推理时的泛化能力也得到了证明。<details>
<summary>Abstract</summary>
One way that the current state of the art measures the reasoning ability of transformer-based models is by evaluating accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of the contexts used are in practice very simple; in most cases, they are generated from short first-order logic sentences with only a few logical operators and quantifiers. In this work, we seek to answer the question how well a transformer-based model will perform reasoning over expressive contexts. For this purpose, we construct a synthetic natural language question-answering dataset, generated by description logic knowledge bases. For the generation of the knowledge bases, we use the expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K examples, and increases in two dimensions: i) reasoning depth, and ii) length of sentences. We show that the performance of our DeBERTa-based model, DELTA$_M$, is marginally affected when the reasoning depth is increased and it is not affected at all when the length of the sentences is increasing. We also evaluate the generalization ability of the model on reasoning depths unseen at training, both increasing and decreasing, revealing interesting insights into the model's adaptive generalization abilities.
</details>
<details>
<summary>摘要</summary>
Currently, the state-of-the-art measure of reasoning ability in transformer-based models is their accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of these contexts are very simple, typically consisting of short first-order logic sentences with only a few logical operators and quantifiers. In this study, we aim to investigate how well a transformer-based model can perform reasoning over more expressive contexts. To achieve this, we create a synthetic natural language question-answering dataset generated by description logic knowledge bases. We use the expressive language $\mathcal{ALCQ}$ to generate the knowledge bases, resulting in a dataset containing 384K examples that increase in two dimensions: i) reasoning depth, and ii) length of sentences. Our DeBERTa-based model, DELTA$_M$, shows marginal impact from increased reasoning depth and no impact from longer sentences. We also evaluate the model's generalization ability on unseen reasoning depths, both increasing and decreasing, revealing interesting insights into its adaptive generalization capabilities.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese writing systems. If you prefer Traditional Chinese, please let me know and I will be happy to provide the translation in that script.
</details></li>
</ul>
<hr>
<h2 id="Supported-Trust-Region-Optimization-for-Offline-Reinforcement-Learning"><a href="#Supported-Trust-Region-Optimization-for-Offline-Reinforcement-Learning" class="headerlink" title="Supported Trust Region Optimization for Offline Reinforcement Learning"></a>Supported Trust Region Optimization for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08935">http://arxiv.org/abs/2311.08935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, Xiangyang Ji</li>
<li>for: 提高掌控环境下的远离线强化学习效果</li>
<li>methods: 使用支持信任区域优化（STR）方法，即在行为政策内部进行强化学习优化，且受到行为政策支持的约束</li>
<li>results: 在假设无误度和抽象误差时，STR方法能够保证政策改进直至到达数据集中的优化策略，并在实际测试中表现出优于当前状态的表现。<details>
<summary>Abstract</summary>
Offline reinforcement learning suffers from the out-of-distribution issue and extrapolation error. Most policy constraint methods regularize the density of the trained policy towards the behavior policy, which is too restrictive in most cases. We propose Supported Trust Region optimization (STR) which performs trust region policy optimization with the policy constrained within the support of the behavior policy, enjoying the less restrictive support constraint. We show that, when assuming no approximation and sampling error, STR guarantees strict policy improvement until convergence to the optimal support-constrained policy in the dataset. Further with both errors incorporated, STR still guarantees safe policy improvement for each step. Empirical results validate the theory of STR and demonstrate its state-of-the-art performance on MuJoCo locomotion domains and much more challenging AntMaze domains.
</details>
<details>
<summary>摘要</summary>
<<SYS> translate into Simplified Chinese</SYS>离线强化学uffer于out-of-distribution问题和推论误差。大多数策略约束方法将训练的策略密度规范到行为策略上，这是大多数情况下过于严格的。我们提议Supported Trust Region优化（STR），该方法通过在行为策略支持下进行信任区域策略优化，享受到较为lenient的支持约束。我们证明，当假设无approximation和抽象误差时，STR确保每步产生策略改进，直到在数据集中收敛到最优的支持约束策略。而在实际中，STR仍然保证每步安全的策略改进，即使包括两种误差。实验结果证明STR的理论和实际性能在MuJoCo步行领域和更加复杂的AntMaze领域都达到了顶峰水平。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Activation-Maximization-and-Generative-Adversarial-Training-to-Recognize-and-Explain-Patterns-in-Natural-Areas-in-Satellite-Imagery"><a href="#Leveraging-Activation-Maximization-and-Generative-Adversarial-Training-to-Recognize-and-Explain-Patterns-in-Natural-Areas-in-Satellite-Imagery" class="headerlink" title="Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery"></a>Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08923">http://arxiv.org/abs/2311.08923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Emam, Timo T. Stomberg, Ribana Roscher</li>
<li>for: 保护 natura 遗产的详细地图创建</li>
<li>methods: 使用activation maximization和生成对抗模型生成卫星图像，结合领域知识，提供完整和有效的解释方法</li>
<li>results: 生成的卫星图像可以准确地标识保护区域的自然 authenticity 特征，提高了保护区域的生态完整性的理解，可能对未来监测和保护做出贡献<details>
<summary>Abstract</summary>
Natural protected areas are vital for biodiversity, climate change mitigation, and supporting ecological processes. Despite their significance, comprehensive mapping is hindered by a lack of understanding of their characteristics and a missing land cover class definition. This paper aims to advance the explanation of the designating patterns forming protected and wild areas. To this end, we propose a novel framework that uses activation maximization and a generative adversarial model. With this, we aim to generate satellite images that, in combination with domain knowledge, are capable of offering complete and valid explanations for the spatial and spectral patterns that define the natural authenticity of these regions. Our proposed framework produces more precise attribution maps pinpointing the designating patterns forming the natural authenticity of protected areas. Our approach fosters our understanding of the ecological integrity of the protected natural areas and may contribute to future monitoring and preservation efforts.
</details>
<details>
<summary>摘要</summary>
自然保护区是生物多样性、气候变化缓解和生态过程支持的重要资源。尽管它们的重要性，但全面的地图制定受到了未understanding其特征和缺失的土地覆盖类划定的限制。本文提出了一种新的框架，使用活动最大化和生成对抗模型，以提高指定 Patterns forming protected and wild areas的解释。通过这种方法，我们可以生成具有完整性和有效性的卫星图像，与领域知识相结合，以提供自然 authenticity 的区域的完整和有效的解释。我们的提议的框架可以生成更精确的归属地图， pinpointing the designating patterns forming the natural authenticity of protected areas。这将有助于我们更好地理解保护区的生态完整性，并可能对未来监测和保护做出贡献。
</details></li>
</ul>
<hr>
<h2 id="An-Empathetic-User-Centric-Chatbot-for-Emotional-Support"><a href="#An-Empathetic-User-Centric-Chatbot-for-Emotional-Support" class="headerlink" title="An Empathetic User-Centric Chatbot for Emotional Support"></a>An Empathetic User-Centric Chatbot for Emotional Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09271">http://arxiv.org/abs/2311.09271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanting Pan, Yixuan Tang, Yuchen Niu</li>
<li>for: 这篇论文探讨了亚特媒体文化和人工智能之间的交叉点，尤其是游戏如何满足年轻女性的情感需求。</li>
<li>methods: 这篇论文使用了大语言模型（LLM）技术来超越传统的静态游戏剧本，创造出dinamic和情感响应的互动体验。</li>
<li>results: 研究人员通过在游戏剧本中添加问答（QA）系统，通过数据扩充和情感增强技术，创建了一个真实和支持的伴侣聊天机器人。<details>
<summary>Abstract</summary>
This paper explores the intersection of Otome Culture and artificial intelligence, particularly focusing on how Otome-oriented games fulfill the emotional needs of young women. These games, which are deeply rooted in a subcultural understanding of love, provide players with feelings of satisfaction, companionship, and protection through carefully crafted narrative structures and character development. With the proliferation of Large Language Models (LLMs), there is an opportunity to transcend traditional static game narratives and create dynamic, emotionally responsive interactions. We present a case study of Tears of Themis, where we have integrated LLM technology to enhance the interactive experience. Our approach involves augmenting existing game narratives with a Question and Answer (QA) system, enriched through data augmentation and emotional enhancement techniques, resulting in a chatbot that offers realistic and supportive companionship.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了互助文化和人工智能的交叉点，特别是游戏如何满足年轻女性的情感需求。这些游戏，深受互助文化的影响，为玩家提供满足、伙伴和保护的感受，通过精心设计的故事结构和人物发展。随着大语言模型（LLM）的普及，有机会超越传统的静止游戏剧本，创造动态、情感回应的互动体验。我们介绍了《泪之Theme》案例，我们在该游戏中集成了LLM技术，以增强互动体验。我们的方法包括在现有游戏剧本中添加问答（QA）系统，通过数据增强和情感增强技术，创造出真实和支持的伙伴。
</details></li>
</ul>
<hr>
<h2 id="NormNet-Scale-Normalization-for-6D-Pose-Estimation-in-Stacked-Scenarios"><a href="#NormNet-Scale-Normalization-for-6D-Pose-Estimation-in-Stacked-Scenarios" class="headerlink" title="NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios"></a>NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09269">http://arxiv.org/abs/2311.09269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuttlet/normnet">https://github.com/shuttlet/normnet</a></li>
<li>paper_authors: En-Te Lin, Wei-Jie Lv, Ding-Tao Huang, Long Zeng</li>
<li>for: 本研究旨在提出一种可以在堆积场景中robustly estimate不同尺度对象的6DoF pose estimator（NormNet）。</li>
<li>methods: 本方法首先使用点准 regression来学习每个对象的尺度，然后通过semantic segmentation和affine变换将所有对象 норmalized到同一个尺度。最后，它们被 fed into a shared pose estimator来恢复它们的6D姿态。此外，我们还提出了一种新的Sim-to-Real transfer管线，该管线结合了style transfer和domain randomization，以提高NormNet在实际数据上的性能。</li>
<li>results: 广泛的实验表明，提出的方法可以在公共benchmark和我们自己construct的MultiScale dataset上达到领先的性能。实际世界 эксперименты也显示，我们的方法可以robustly estimate不同尺度对象的6D姿态。<details>
<summary>Abstract</summary>
Existing Object Pose Estimation (OPE) methods for stacked scenarios are not robust to changes in object scale. This paper proposes a new 6DoF OPE network (NormNet) for different scale objects in stacked scenarios. Specifically, each object's scale is first learned with point-wise regression. Then, all objects in the stacked scenario are normalized into the same scale through semantic segmentation and affine transformation. Finally, they are fed into a shared pose estimator to recover their 6D poses. In addition, we introduce a new Sim-to-Real transfer pipeline, combining style transfer and domain randomization. This improves the NormNet's performance on real data even if we only train it on synthetic data. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on public benchmarks and the MultiScale dataset we constructed. The real-world experiments show that our method can robustly estimate the 6D pose of objects at different scales.
</details>
<details>
<summary>摘要</summary>
现有的栅格场景中对象姿态估计（OPE）方法不能抗测对象比例变化。这篇论文提出了一种新的6度自由姿态网络（NormNet），用于不同比例的 объекts在栅格场景中估计6D姿态。具体来说，每个对象的比例首先通过点级回归学习。然后，所有在栅格场景中的对象都被正规化为同一个比例通过 semantic segmentation 和Affine变换。最后，它们被 fed into 共享的姿态估计器，以便从 shared pose estimator 中回归其6D姿态。此外，我们还引入了一种新的 Sim-to-Real 传输管道， combining style transfer 和 domain randomization。这种管道可以在只使用 sintetic data 进行训练时，提高 NormNet 的表现 на real data。广泛的实验表明，我们提posed方法可以在公共 benchmarks 和我们自己构建的 MultiScale 数据集上达到顶尖性能。在实际场景中，我们的方法可以Robustly 估计不同比例的对象的6D姿态。
</details></li>
</ul>
<hr>
<h2 id="Combining-Transfer-Learning-with-In-context-Learning-using-Blackbox-LLMs-for-Zero-shot-Knowledge-Base-Question-Answering"><a href="#Combining-Transfer-Learning-with-In-context-Learning-using-Blackbox-LLMs-for-Zero-shot-Knowledge-Base-Question-Answering" class="headerlink" title="Combining Transfer Learning with In-context Learning using Blackbox LLMs for Zero-shot Knowledge Base Question Answering"></a>Combining Transfer Learning with In-context Learning using Blackbox LLMs for Zero-shot Knowledge Base Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08894">http://arxiv.org/abs/2311.08894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayur Patidar, Avinash Singh, Riya Sawhney, Indrajit Bhattacharya, Mausam</li>
<li>for: 本文Addresses the zero-shot transfer learning setting for the knowledge base question answering (KBQA) problem, where a large volume of labeled training data is available for the source domain, but no such labeled examples are available for the target domain.</li>
<li>methods: 本文使用了大量的无标签数据在目标域，并结合了源域的标签数据进行了转移学习。此外，文章还提出了基于黑盒大语言模型（BLLM）的受限自我调整方法，可以独立于转移设定进行执行。</li>
<li>results: 根据实验结果，提出的方法可以在 GrailQA 作为源域和 WebQSP 作为目标域的情况下，对两个阶段（检索和生成）进行了显著改进，并且也超越了当前的超参数化KBQA模型。此外，当有限量的标签数据时，BLLM的扩展也可以在域内设定中提供显著的改进。<details>
<summary>Abstract</summary>
We address the zero-shot transfer learning setting for the knowledge base question answering (KBQA) problem, where a large volume of labeled training data is available for the source domain, but no such labeled examples are available for the target domain. Transfer learning for KBQA makes use of large volumes of unlabeled data in the target in addition to the labeled data in the source. More recently, few-shot in-context learning using Black-box Large Language Models (BLLMs) has been adapted for KBQA without considering any source domain data. In this work, we show how to meaningfully combine these two paradigms for KBQA so that their benefits add up. Specifically, we preserve the two stage retrieve-then-generate pipeline of supervised KBQA and introduce interaction between in-context learning using BLLMs and transfer learning from the source for both stages. In addition, we propose execution-guided self-refinement using BLLMs, decoupled from the transfer setting. With the help of experiments using benchmark datasets GrailQA as the source and WebQSP as the target, we show that the proposed combination brings significant improvements to both stages and also outperforms by a large margin state-of-the-art supervised KBQA models trained on the source. We also show that in the in-domain setting, the proposed BLLM augmentation significantly outperforms state-of-the-art supervised models, when the volume of labeled data is limited, and also outperforms these marginally even when using the entire large training dataset.
</details>
<details>
<summary>摘要</summary>
我们研究了零shot转移学习 Setting for 知识库问答（KBQA）问题，其中有大量标注的训练数据在源领域可用，但target领域没有任何标注的示例。KBQA的转移学习使用了target领域的大量无标注数据，以及源领域的标注数据。在这种情况下，我们将黑obox大型自然语言模型（BLLM）的几个shot在 Context learning应用于KBQA，而不考虑源领域的数据。在这种情况下，我们保留了KBQA的两stage retrieve-then-generate架构，并在这两个阶段中引入了BLLM的交互。此外，我们还提出了基于BLLM的执行指导自适应，与转移学习分离。通过使用GrailQA作为源领域和WebQSP作为目标领域的实验，我们表明了我们的提案可以在两个阶段中提供显著改进，并且也超越了当前的supervised KBQA模型。此外，我们还表明了在域内设置下，我们的BLLM扩展可以在标注数据量有限的情况下获得显著改进，并且甚至在使用整个大量训练数据时也能够超越supervised模型。
</details></li>
</ul>
<hr>
<h2 id="Advances-in-ACL2-Proof-Debugging-Tools"><a href="#Advances-in-ACL2-Proof-Debugging-Tools" class="headerlink" title="Advances in ACL2 Proof Debugging Tools"></a>Advances in ACL2 Proof Debugging Tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08856">http://arxiv.org/abs/2311.08856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matt Kaufmann, J Strother Moore</li>
<li>for: 本文描述了ACL2用户通常会遇到失败的证明尝试，以及如何使用工具来解决这些失败。</li>
<li>methods: 本文专注于ACL2版本8.5后的改进：改进的break-rewrite工具以及新增的with-brr-data工具。</li>
<li>results: 通过使用这些工具，ACL2用户可以更有效地解决证明失败。<details>
<summary>Abstract</summary>
The experience of an ACL2 user generally includes many failed proof attempts. A key to successful use of the ACL2 prover is the effective use of tools to debug those failures. We focus on changes made after ACL2 Version 8.5: the improved break-rewrite utility and the new utility, with-brr-data.
</details>
<details>
<summary>摘要</summary>
ACL2用户通常会经历许多失败的证明尝试。成功使用ACL2证明工具的关键在于有效地使用工具来调试失败。我们关注ACL2版本8.5后的更改：改进的break-rewrite工具以及新增的with-brr-data工具。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Gender-Bias-in-the-Translation-of-Gender-Neutral-Languages-into-English"><a href="#Evaluating-Gender-Bias-in-the-Translation-of-Gender-Neutral-Languages-into-English" class="headerlink" title="Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English"></a>Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08836">http://arxiv.org/abs/2311.08836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer Rarrick, Ranjita Naik, Sundar Poudel, Vishal Chowdhary</li>
<li>for: 这个论文的目的是提出一个gender bias检测和 mitigation的数据集，以便更好地评估和改进Machine Translation（MT）系统中的gender bias问题。</li>
<li>methods: 这个论文使用了一个新的数据集名为GATE X-E，这个数据集包含了从土耳其语、匈牙利语、芬兰语和波斯语翻译成英语的人工翻译，每个翻译都有女性、男性和中性的多个变体。此外，这篇论文还提出了一种基于GPT-3.5 Turbo的英语性别重写解决方案，并使用GATE X-E来评估这种解决方案。</li>
<li>results: 这篇论文的研究结果表明，GATE X-E数据集可以帮助提高MT系统中gender bias的识别和改进，并且基于GPT-3.5 Turbo的英语性别重写解决方案也能够有效地改善MT系统中的gender bias问题。<details>
<summary>Abstract</summary>
Machine Translation (MT) continues to improve in quality and adoption, yet the inadvertent perpetuation of gender bias remains a significant concern. Despite numerous studies into gender bias in translations from gender-neutral languages such as Turkish into more strongly gendered languages like English, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants for each possible gender interpretation. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present an English gender rewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We open source our contributions to encourage further research on gender debiasing.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-search-algorithm-for-an-optimal-investment-problem-in-vehicle-sharing-systems"><a href="#A-search-algorithm-for-an-optimal-investment-problem-in-vehicle-sharing-systems" class="headerlink" title="A* search algorithm for an optimal investment problem in vehicle-sharing systems"></a>A* search algorithm for an optimal investment problem in vehicle-sharing systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08834">http://arxiv.org/abs/2311.08834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ba Luat Le, Layla Martin, Emrah Demir, Duc Minh Vu</li>
<li>for: 该研究探讨了一个优化投资问题，它在 Shared Vehicle System 中出现。给定一个站点建设集，我们需要确定（i）站点建设顺序和车辆数量，以达到所有站点建设完成的目标状态，（ii）在一些或所有站点打开时，最大化运营系统的总收益。</li>
<li>methods: 作者提出了一种 A* 搜索算法来解决这个问题，该算法可以视为一种 TSP 变种，具有集成依赖性的成本。</li>
<li>results: 计算实验表明，作者的提案算法在比较 Dijkstra 算法时具有明显的优势，并且将来的研究可以探讨新的可能性和应用。<details>
<summary>Abstract</summary>
We study an optimal investment problem that arises in the context of the vehicle-sharing system. Given a set of locations to build stations, we need to determine i) the sequence of stations to be built and the number of vehicles to acquire in order to obtain the target state where all stations are built, and ii) the number of vehicles to acquire and their allocation in order to maximize the total profit returned by operating the system when some or all stations are open. The profitability associated with operating open stations, measured over a specific time period, is represented as a linear optimization problem applied to a collection of open stations. With operating capital, the owner of the system can open new stations. This property introduces a set-dependent aspect to the duration required for opening a new station, and the optimal investment problem can be viewed as a variant of the Traveling Salesman Problem (TSP) with set-dependent cost. We propose an A* search algorithm to address this particular variant of the TSP. Computational experiments highlight the benefits of the proposed algorithm in comparison to the widely recognized Dijkstra algorithm and propose future research to explore new possibilities and applications for both exact and approximate A* algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究一个最佳投资问题，它在车仲共享系统中发生。我们需要 Determine 以下两个问题：1. 建站的顺序和车辆数量，以实现所有站点都建立，并2. 车辆数量和分配方式，以最大化在一些或所有站点开放时的总收益。系统在运行时的收益，通过在一个特定时间间隔内进行线性优化问题，以表示开放的站点的盈利。系统所有者可以通过资金来开新站点。这个属性导致开新站点所需时间受到站点集的依赖，并且将最佳投资问题视为对特定设置成本的车辆销售人员问题的变形。我们提议使用A*搜索算法来解决这个问题。计算实验显示了我们的提案算法与通过世界上所认可的迪克斯特拉算法相比，具有更好的性能。我们未来的研究将探讨新的可能性和应用，以及精确和近似A*算法的应用。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Links-between-Conversational-Agent-Design-Challenges-and-Interdisciplinary-Collaboration"><a href="#Exploring-Links-between-Conversational-Agent-Design-Challenges-and-Interdisciplinary-Collaboration" class="headerlink" title="Exploring Links between Conversational Agent Design Challenges and Interdisciplinary Collaboration"></a>Exploring Links between Conversational Agent Design Challenges and Interdisciplinary Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08832">http://arxiv.org/abs/2311.08832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malak Sadek, Céline Mougenot</li>
<li>for: The paper is written to explore the socio-technical challenges of creating conversational agents (CA) and to propose practical strategies to overcome these challenges.</li>
<li>methods: The paper uses a scoping review of existing literature to identify and categorize the socio-technical challenges of CA design, and proposes a taxonomy of these challenges using interdisciplinary collaboration (IDC) as a lens.</li>
<li>results: The paper proposes practical strategies to overcome the socio-technical challenges of CA design, and invites future work to empirically verify the suggested conceptual links and apply the proposed strategies within the space of CA design to evaluate their effectiveness.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了探讨对话代理（CA）的社会技术创新挑战，并提出了解决这些挑战的实际策略。</li>
<li>methods: 这篇论文通过审视现有文献来标识和分类CA设计中的社会技术挑战，并提出了使用交叉学科协作（IDC）作为镜头的挑战分类法。</li>
<li>results: 这篇论文提出了解决CA设计中的社会技术挑战的实际策略，并邀请未来的研究 empirically verify提出的概念链和在CA设计空间中应用提出的策略以评估其效果。<details>
<summary>Abstract</summary>
Recent years have seen a steady rise in the popularity and use of Conversational Agents (CA) for different applications, well before the more immediate impact of large language models. This rise has been accompanied by an extensive exploration and documentation of the challenges of designing and creating conversational agents. Focusing on a recent scoping review of the socio-technical challenges of CA creation, this opinion paper calls for an examination of the extent to which interdisciplinary collaboration (IDC) challenges might contribute towards socio-technical CA design challenges. The paper proposes a taxonomy of CA design challenges using IDC as a lens, and proposes practical strategies to overcome them which complement existing design principles. The paper invites future work to empirically verify suggested conceptual links and apply the proposed strategies within the space of CA design to evaluate their effectiveness.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Conversational Agents" (CA) is translated as "对话代理人" (duìxiào dàibiǎn)* "Interdisciplinary collaboration" (IDC) is translated as "交叉学科合作" (jiāo kè xué kē hè zuò)* "Socio-technical challenges" is translated as "社会技术挑战" (shè huì jī shuō tā zhàn)* "Design principles" is translated as "设计原则" (xiè yì yuán xì)Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-with-Model-Predictive-Control-for-Highway-Ramp-Metering"><a href="#Reinforcement-Learning-with-Model-Predictive-Control-for-Highway-Ramp-Metering" class="headerlink" title="Reinforcement Learning with Model Predictive Control for Highway Ramp Metering"></a>Reinforcement Learning with Model Predictive Control for Highway Ramp Metering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08820">http://arxiv.org/abs/2311.08820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/filippoairaldi/mpcrl-for-ramp-metering">https://github.com/filippoairaldi/mpcrl-for-ramp-metering</a></li>
<li>paper_authors: Filippo Airaldi, Bart De Schutter, Azita Dabiri</li>
<li>for: 提高城市和高速公路交通系统的效率</li>
<li>methods: 结合模型驱动和学习驱动的方法，使用可靠学习控制方法来改进高速公路上的踏面控制</li>
<li>results: 实验结果显示，从一个不精准的模型和不佳地调整的控制器开始，提议的方法可以有效地学习改进控制策略，从而减少网络中的堵塞和满足约束，相比初始控制器表现更佳。<details>
<summary>Abstract</summary>
In the backdrop of an increasingly pressing need for effective urban and highway transportation systems, this work explores the synergy between model-based and learning-based strategies to enhance traffic flow management by use of an innovative approach to the problem of highway ramp metering control that embeds Reinforcement Learning techniques within the Model Predictive Control framework. The control problem is formulated as an RL task by crafting a suitable stage cost function that is representative of the traffic conditions, variability in the control action, and violations of a safety-critical constraint on the maximum number of vehicles in queue. An MPC-based RL approach, which merges the advantages of the two paradigms in order to overcome the shortcomings of each framework, is proposed to learn to efficiently control an on-ramp and to satisfy its constraints despite uncertainties in the system model and variable demands. Finally, simulations are performed on a benchmark from the literature consisting of a small-scale highway network. Results show that, starting from an MPC controller that has an imprecise model and is poorly tuned, the proposed methodology is able to effectively learn to improve the control policy such that congestion in the network is reduced and constraints are satisfied, yielding an improved performance compared to the initial controller.
</details>
<details>
<summary>摘要</summary>
在城市和高速公路交通系统的需求越来越高的背景下，这项工作探讨了模型基本和学习基本策略之间的共谊，以提高交通流控制的效果。该工作使用了一种嵌入了回归学习技术的模型预测控制框架来解决高速匝道流控制问题。通过设计一个合适的stage cost函数，该方法将交通条件、控制动作的变化和安全约束的最大车辆队列数量作为RL任务的stage cost函数。该方法将MPC和RL两种框架融合，以超越每个框架的缺点，并学习高速匝道控制，并满足系统模型不确定性和变化的需求。最后，对一个小规模高速公路网络的测试表明，从一个不精确的模型和优化不良的MPC控制器开始，该方法能够有效地学习改善控制策略，从而减少网络中的拥堵，满足约束，并提高效果相比于初始控制器。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Domain-based-Dataset-Distillation"><a href="#Frequency-Domain-based-Dataset-Distillation" class="headerlink" title="Frequency Domain-based Dataset Distillation"></a>Frequency Domain-based Dataset Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08819">http://arxiv.org/abs/2311.08819</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdh0818/fred">https://github.com/sdh0818/fred</a></li>
<li>paper_authors: Donghyeok Shin, Seungjae Shin, Il-Chul Moon</li>
<li>for: 本研究旨在提出一种新的参数化方法，用于快速生成小型的合成数据集，从原始大型数据集中提取关键信息。</li>
<li>methods: 该方法基于频域的变换来优化数据集中每个实例的频率表示，通过选择特定频率维度进行优化，以实现快速生成实例的目标。</li>
<li>results: 对于不同的评价指标和数据集，FreD方法能够在有限的资源下实现更好的信息保留和性能提升，并且与现有方法兼容。<details>
<summary>Abstract</summary>
This paper presents FreD, a novel parameterization method for dataset distillation, which utilizes the frequency domain to distill a small-sized synthetic dataset from a large-sized original dataset. Unlike conventional approaches that focus on the spatial domain, FreD employs frequency-based transforms to optimize the frequency representations of each data instance. By leveraging the concentration of spatial domain information on specific frequency components, FreD intelligently selects a subset of frequency dimensions for optimization, leading to a significant reduction in the required budget for synthesizing an instance. Through the selection of frequency dimensions based on the explained variance, FreD demonstrates both theoretical and empirical evidence of its ability to operate efficiently within a limited budget, while better preserving the information of the original dataset compared to conventional parameterization methods. Furthermore, based on the orthogonal compatibility of FreD with existing methods, we confirm that FreD consistently improves the performances of existing distillation methods over the evaluation scenarios with different benchmark datasets. We release the code at https://github.com/sdh0818/FreD.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MAP’s-not-dead-yet-Uncovering-true-language-model-modes-by-conditioning-away-degeneracy"><a href="#MAP’s-not-dead-yet-Uncovering-true-language-model-modes-by-conditioning-away-degeneracy" class="headerlink" title="MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy"></a>MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08817">http://arxiv.org/abs/2311.08817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davis Yoshida, Kartik Goyal, Kevin Gimpel</li>
<li>for: 这个论文主要研究了NLG模型中模式的问题，具体来说是解释为什么模式搜索（MAP解oding）常常导致输出异常（Stahlberg和Byrne，2019，Holtzman等，2019）。</li>
<li>methods: 作者使用了杂合搜索和模式搜索来研究NLG模型的输出。他们发现，即使模型没有错误，模式仍可以变得缺乏含义，这是因为训练数据中的噪声污染。为解决这问题，作者提议使用模式搜索 conditional on avoiding specific degeneracies。</li>
<li>results: 作者通过实验证明了，对机器翻译模型和语言模型进行长度 conditional 模式搜索可以获得更加流畅和话题性的输出。此外，作者还提供了许多模式序列的实际示例，并证明了LLaMA模型的模式仍然具有缺乏含义的问题。为了解决这问题，作者开发了一种approximate模式搜索方法，ACBS。通过应用这种方法，作者可以从LLaMA-7B模型中获得可接受的输出，而无需任何训练。<details>
<summary>Abstract</summary>
It has been widely observed that exact or approximate MAP (mode-seeking) decoding from natural language generation (NLG) models consistently leads to degenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has generally been attributed to either a fundamental inadequacy of modes in models or weaknesses in language modeling. Contrastingly in this work, we emphasize that degenerate modes can even occur in the absence of any model error, due to contamination of the training data. Specifically, we show that mixing even a tiny amount of low-entropy noise with a population text distribution can cause the data distribution's mode to become degenerate, implying that any models trained on it will be as well. As the unconditional mode of NLG models will often be degenerate, we therefore propose to apply MAP decoding to the model's distribution conditional on avoiding specific degeneracies. Using exact-search, we empirically verify that the length-conditional modes of machine translation models and language models are indeed more fluent and topical than their unconditional modes. For the first time, we also share many examples of exact modal sequences from these models, and from several variants of the LLaMA-7B model. Notably, the modes of the LLaMA models are still degenerate, showing that improvements in modeling have not fixed this issue. Because of the cost of exact mode finding algorithms, we develop an approximate mode finding approach, ACBS, which finds sequences that are both high-likelihood and high-quality. We apply this approach to LLaMA-7B, a model which was not trained for instruction following, and find that we are able to elicit reasonable outputs without any finetuning.
</details>
<details>
<summary>摘要</summary>
历史观察表明，使用自然语言生成（NLG）模型的准确或近似MAP（模式寻找）解oding会导致异常输出（Stahlberg和Byrne，2019，Holtzman等，2019）。这一问题通常被归结到模型中的缺陷或语言模型的弱点。然而，在本研究中，我们强调的是，即使模型没有错误，degenerate modes仍可能出现，这是因为训练数据被杂入了低 entropy 的噪音。我们证明，只要混合一点微的低 entropy 噪音到一个人类文本分布中，就可以让数据分布的模式变得异常。因此，我们建议在模型的分布上使用MAP decoding，并且条件于避免特定的异常模式。我们通过对机器翻译模型和语言模型的长度准确模式进行实验，证明了这些模式在fluency和topicality方面比unconditional modes更高。此外，我们还提供了许多exact模式序列的例子，包括several variants of the LLaMA-7B model。不幸的是，LLaMA模型的模式仍然异常，显示改进模型化没有解决这一问题。由于找到精确模式的算法成本高，我们开发了一种 Approximate CBS（ACBS）模式找到方法，可以找到高概率和高质量的序列。我们应用ACBS方法于LLaMA-7B模型，并发现可以获得无需较少的finetuning的合理输出。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Disentanglement-by-Leveraging-Structure-in-Data-Augmentations"><a href="#Self-Supervised-Disentanglement-by-Leveraging-Structure-in-Data-Augmentations" class="headerlink" title="Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations"></a>Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08815">http://arxiv.org/abs/2311.08815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cian Eastwood, Julius von Kügelgen, Linus Ericsson, Diane Bouchacourt, Pascal Vincent, Bernhard Schölkopf, Mark Ibrahim</li>
<li>for: 这个论文旨在推动自然语言处理领域中的自我超VI中的表示学习。</li>
<li>methods: 这篇论文使用了数据扩充来适应”风格”特征的变化，但是由于下游任务通常在训练时未知，因此难以在训练时确定”风格”特征是否可以安全地丢弃。为了解决这个问题，这篇论文提出了一种更原则的方法，即通过添加多个风格嵌入空间来分离风格特征。</li>
<li>results: 该方法在synthetic数据集上进行了实验，并且在ImageNet上进行了一些有限的实验，并证明了其效果。<details>
<summary>Abstract</summary>
Self-supervised representation learning often uses data augmentations to induce some invariance to "style" attributes of the data. However, with downstream tasks generally unknown at training time, it is difficult to deduce a priori which attributes of the data are indeed "style" and can be safely discarded. To address this, we introduce a more principled approach that seeks to disentangle style features rather than discard them. The key idea is to add multiple style embedding spaces where: (i) each is invariant to all-but-one augmentation; and (ii) joint entropy is maximized. We formalize our structured data-augmentation procedure from a causal latent-variable-model perspective, and prove identifiability of both content and (multiple blocks of) style variables. We empirically demonstrate the benefits of our approach on synthetic datasets and then present promising but limited results on ImageNet.
</details>
<details>
<summary>摘要</summary>
自我指导学习经常使用数据扩充来induce一些数据的"风格"特征的不变性。然而，下游任务通常不知道训练时间点，因此难以在训练时确定哪些特征是"风格"特征，可以安全地抛弃。为解决这个问题，我们介绍了一种更理智的方法，即通过分离风格特征来解决这个问题。我们的关键想法是在多个风格嵌入空间中添加多个不变性，即：(i) 每个不变性都是对所有扩充之外的一个不变性;(ii) 共同 entropy 的最大化。我们从 causal 潜在变量模型的视角来正式描述我们的结构化数据扩充过程，并证明内容和多个块风格变量的可识别性。我们在synthetic dataset上进行了实验，并在ImageNet上得到了有前途的 pero有限的结果。
</details></li>
</ul>
<hr>
<h2 id="SparseSpikformer-A-Co-Design-Framework-for-Token-and-Weight-Pruning-in-Spiking-Transformer"><a href="#SparseSpikformer-A-Co-Design-Framework-for-Token-and-Weight-Pruning-in-Spiking-Transformer" class="headerlink" title="SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer"></a>SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08806">http://arxiv.org/abs/2311.08806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Liu, Shanlin Xiao, Bo Li, Zhiyi Yu</li>
<li>for: 这个研究旨在提高Spikformer模型的效率和能效性，使其适合实现在边缘设备上。</li>
<li>methods: 这个研究使用了Lottery Ticket Hypothesis（LTH）和几个创新的token和重量调整技术来实现Spikformer模型的简洁化。</li>
<li>results: 实验结果显示，这个框架可以将Spikformer模型的90%模型参数简减，且可以降低Giga浮动点操作数（GFLOPs）20%，同时保持原始模型的准确性。<details>
<summary>Abstract</summary>
As the third-generation neural network, the Spiking Neural Network (SNN) has the advantages of low power consumption and high energy efficiency, making it suitable for implementation on edge devices. More recently, the most advanced SNN, Spikformer, combines the self-attention module from Transformer with SNN to achieve remarkable performance. However, it adopts larger channel dimensions in MLP layers, leading to an increased number of redundant model parameters. To effectively decrease the computational complexity and weight parameters of the model, we explore the Lottery Ticket Hypothesis (LTH) and discover a very sparse ($\ge$90%) subnetwork that achieves comparable performance to the original network. Furthermore, we also design a lightweight token selector module, which can remove unimportant background information from images based on the average spike firing rate of neurons, selecting only essential foreground image tokens to participate in attention calculation. Based on that, we present SparseSpikformer, a co-design framework aimed at achieving sparsity in Spikformer through token and weight pruning techniques. Experimental results demonstrate that our framework can significantly reduce 90% model parameters and cut down Giga Floating-Point Operations (GFLOPs) by 20% while maintaining the accuracy of the original model.
</details>
<details>
<summary>摘要</summary>
为了提高edge设备上的神经网络模型的能效性，我们提出了一种基于SNN的第三代神经网络模型，即SparseSpikformer。该模型通过减少神经网络的计算复杂性和参数量来提高实现效率。在这个模型中，我们采用了LTH Hypothesis，并在SNN中发现了一个大于90%的稀疏子网络，可以保持与原始网络相同的性能。此外，我们还设计了一个轻量级的图像选择器模块，可以根据神经元的射击率选择图像中的重要背景信息，从而降低计算复杂性。基于这些设计，我们提出了一种减少Spikformer模型计算复杂性的框架，并实现了减少90%的模型参数和20%的GFLOPs操作数量的目标。实验结果表明，我们的框架可以维持原始模型的准确性，同时实现效率的提高。
</details></li>
</ul>
<hr>
<h2 id="X-Eval-Generalizable-Multi-aspect-Text-Evaluation-via-Augmented-Instruction-Tuning-with-Auxiliary-Evaluation-Aspects"><a href="#X-Eval-Generalizable-Multi-aspect-Text-Evaluation-via-Augmented-Instruction-Tuning-with-Auxiliary-Evaluation-Aspects" class="headerlink" title="X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects"></a>X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08788">http://arxiv.org/abs/2311.08788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, Lifu Huang</li>
<li>for: 本文目的是提出一种多方面评估框架，以便评估自然语言生成（NLG）的多个方面质量。</li>
<li>methods: 本文使用了两个学习阶段：第一阶段是简单的指令调整阶段，旨在提高模型following指令的能力；第二阶段是加强的指令调整阶段，通过细致的评估方面之间的连接来更好地评估文本质量。</li>
<li>results: 经过广泛的实验，我们发现X-Eval可以让even a lightweight language model达到与人类评估相当或更高的相关性，比如GPT-4。<details>
<summary>Abstract</summary>
Natural Language Generation (NLG) typically involves evaluating the generated text in various aspects (e.g., consistency and naturalness) to obtain a comprehensive assessment. However, multi-aspect evaluation remains challenging as it may require the evaluator to generalize to any given evaluation aspect even if it's absent during training. In this paper, we introduce X-Eval, a two-stage instruction tuning framework to evaluate the text in both seen and unseen aspects customized by end users. X-Eval consists of two learning stages: the vanilla instruction tuning stage that improves the model's ability to follow evaluation instructions, and an enhanced instruction tuning stage that exploits the connections between fine-grained evaluation aspects to better assess text quality. To support the training of X-Eval, we collect AspectInstruct, the first instruction tuning dataset tailored for multi-aspect NLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance task diversity, we devise an augmentation strategy that converts human rating annotations into diverse forms of NLG evaluation tasks, including scoring, comparison, ranking, and Boolean question answering. Extensive experiments across three essential categories of NLG tasks: dialogue generation, summarization, and data-to-text coupled with 21 aspects in meta-evaluation, demonstrate that our X-Eval enables even a lightweight language model to achieve a comparable if not higher correlation with human judgments compared to the state-of-the-art NLG evaluators, such as GPT-4.
</details>
<details>
<summary>摘要</summary>
自然语言生成（NLG）通常包括评估生成文本的多个方面（例如一致性和自然性）以获得全面的评估。然而，多方面评估仍然是挑战，因为评估人可能需要将注意力扩展到任何给定的评估方面，即使在训练过程中没有出现过。在这篇论文中，我们介绍了X-Eval，一个两个学习阶段的指令调整框架，用于评估文本在已知和未知方面的质量。X-Eval包括两个学习阶段：一个普通的指令调整阶段，用于提高模型能够遵循评估指令的能力，以及一个加强的指令调整阶段，用于更好地评估文本质量。为支持X-Eval的训练，我们收集了AspectInstruct数据集，这是第一个适用于多方面NLG评估的指令调整数据集，覆盖了27种多样化的评估方面，65个任务。为了增加任务多样性，我们设计了一种扩展策略，将人类评分笔记转换成多种NLG评估任务的不同形式，包括分数、对比、排名和布尔问答。在对对话生成、概要和数据到文本等三类NLG任务进行广泛的实验，我们发现X-Eval可以让even a lightweight语言模型与人类评估结果相似或更高相关性，比如GPT-4。
</details></li>
</ul>
<hr>
<h2 id="ICRA-Roboethics-Challenge-2023-Intelligent-Disobedience-in-an-Elderly-Care-Home"><a href="#ICRA-Roboethics-Challenge-2023-Intelligent-Disobedience-in-an-Elderly-Care-Home" class="headerlink" title="ICRA Roboethics Challenge 2023: Intelligent Disobedience in an Elderly Care Home"></a>ICRA Roboethics Challenge 2023: Intelligent Disobedience in an Elderly Care Home</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08783">http://arxiv.org/abs/2311.08783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sveta Paster, Kantwon Rogers, Gordon Briggs, Peter Stone, Reuth Mirsky</li>
<li>for: 这份报告是为了提高老人护理机构中的服务机器人增强老人的生活质量，以应对预计的老年人口增长。</li>
<li>methods: 该报告提议利用智能不遵守框架，让机器人能够进行有伦理意义的决策过程。</li>
<li>results: 该报告列出了智能不遵守框架可以帮助机器人解决的问题，并在特定的老人护理机构场景下定义了该框架的形式化定义，以及实现智能不遵守机器人的需求。<details>
<summary>Abstract</summary>
With the projected surge in the elderly population, service robots offer a promising avenue to enhance their well-being in elderly care homes. Such robots will encounter complex scenarios which will require them to perform decisions with ethical consequences. In this report, we propose to leverage the Intelligent Disobedience framework in order to give the robot the ability to perform a deliberation process over decisions with potential ethical implications. We list the issues that this framework can assist with, define it formally in the context of the specific elderly care home scenario, and delineate the requirements for implementing an intelligently disobeying robot. We conclude this report with some critical analysis and suggestions for future work.
</details>
<details>
<summary>摘要</summary>
随着老年人口增长的预计，服务机器人在老年人医疗机构中提供了一个有前途的解决方案，以提高老年人的生活质量。这些机器人会遇到复杂的情况，需要它们在具有伦理意义的决策时进行慎重的讨论。在这份报告中，我们提议利用智能不遵守框架，让机器人在具有伦理意义的决策时能够进行慎重的讨论。我们列出了这个框架可以帮助解决的问题，在老年人医疗机构特定场景中明确定义了它，并详细描述了实现智能不遵守机器人的需求。我们在报告结尾提出了一些批判性分析和未来工作的建议。
</details></li>
</ul>
<hr>
<h2 id="Adversarially-Robust-Spiking-Neural-Networks-Through-Conversion"><a href="#Adversarially-Robust-Spiking-Neural-Networks-Through-Conversion" class="headerlink" title="Adversarially Robust Spiking Neural Networks Through Conversion"></a>Adversarially Robust Spiking Neural Networks Through Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09266">http://arxiv.org/abs/2311.09266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/igitugraz/robustsnnconversion">https://github.com/igitugraz/robustsnnconversion</a></li>
<li>paper_authors: Ozan Özdenizci, Robert Legenstein</li>
<li>for: 提高深度神经网络（SNN）的防御性能，增强SNN在应用中的可靠性。</li>
<li>methods: 提出了一种可扩展的Robust SNN培训方法，通过归一化层级触发阈值和synaptic连接权重来保持从预训练ANN中传递的robust性提升。</li>
<li>results: 实验结果表明，我们的方法可以在多种适应性攻击Setting下提供一个可扩展的、低延迟的防御性能。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) provide an energy-efficient alternative to a variety of artificial neural network (ANN) based AI applications. As the progress in neuromorphic computing with SNNs expands their use in applications, the problem of adversarial robustness of SNNs becomes more pronounced. To the contrary of the widely explored end-to-end adversarial training based solutions, we address the limited progress in scalable robust SNN training methods by proposing an adversarially robust ANN-to-SNN conversion algorithm. Our method provides an efficient approach to embrace various computationally demanding robust learning objectives that have been proposed for ANNs. During a post-conversion robust finetuning phase, our method adversarially optimizes both layer-wise firing thresholds and synaptic connectivity weights of the SNN to maintain transferred robustness gains from the pre-trained ANN. We perform experimental evaluations in numerous adaptive adversarial settings that account for the spike-based operation dynamics of SNNs, and show that our approach yields a scalable state-of-the-art solution for adversarially robust deep SNNs with low-latency.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）提供了一种能效的人工神经网络（ANN）的替代方案，随着神经omorphic计算的进步，SNN在应用中的使用逐渐扩大。然而，SNN的敌意 robustness问题在这种扩展过程中变得更加突出。而不是已经广泛探索的终端对抗验证学习方法，我们提出了一种可扩展的Robust SNN Training方法。我们的方法可以有效地涵盖各种计算具有挑战性的Robust learning目标，这些目标在ANN中已经得到了广泛的探索。在post-conversionRobust fine-tuning阶段，我们的方法在SNN中对层wise发射阈值和 synaptic连接权重进行了对抗优化，以保持从pre-trained ANN中传递的Robustness收益。我们在许多适应性攻击设定下进行了实验评估，并证明了我们的方法可以实现可扩展的state-of-the-art解决方案，并且具有低延迟。
</details></li>
</ul>
<hr>
<h2 id="Three-Conjectures-on-Unexpectedeness"><a href="#Three-Conjectures-on-Unexpectedeness" class="headerlink" title="Three Conjectures on Unexpectedeness"></a>Three Conjectures on Unexpectedeness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08768">http://arxiv.org/abs/2311.08768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Sileno, Jean-Louis Dessalles</li>
<li>for: This paper aims to lay the groundwork for a theoretical framework to explain the predictive power of unexpectedness in cognition, and to explore its connection to various measures of divergence between the entropy of the world and the variety of the observer.</li>
<li>methods: The paper uses a combination of theoretical conjectures and experimental results to develop a framework for understanding the role of unexpectedness in cognition.</li>
<li>results: The paper provides a new perspective on the relationship between unexpectedness and cognition, and suggests potential research directions that could lead to new insights into the extraction of causal relations and the role of descriptive mechanisms in learning.<details>
<summary>Abstract</summary>
Unexpectedness is a central concept in Simplicity Theory, a theory of cognition relating various inferential processes to the computation of Kolmogorov complexities, rather than probabilities. Its predictive power has been confirmed by several experiments with human subjects, yet its theoretical basis remains largely unexplored: why does it work? This paper lays the groundwork for three theoretical conjectures. First, unexpectedness can be seen as a generalization of Bayes' rule. Second, the frequentist core of unexpectedness can be connected to the function of tracking ergodic properties of the world. Third, unexpectedness can be seen as constituent of various measures of divergence between the entropy of the world (environment) and the variety of the observer (system). The resulting framework hints to research directions that go beyond the division between probabilistic and logical approaches, potentially bringing new insights into the extraction of causal relations, and into the role of descriptive mechanisms in learning.
</details>
<details>
<summary>摘要</summary>
不期待性是简洁理论中的核心概念， relate to various inference processes and Kolmogorov complexities computation, rather than probabilities. Its predictive power has been confirmed by several experiments with human subjects, but its theoretical basis remains largely unexplored: why does it work? This paper lays the groundwork for three theoretical conjectures. First, unexpectedness can be seen as a generalization of Bayes' rule. Second, the frequentist core of unexpectedness can be connected to the function of tracking ergodic properties of the world. Third, unexpectedness can be seen as a constituent of various measures of divergence between the entropy of the world (environment) and the variety of the observer (system). The resulting framework hints to research directions that go beyond the division between probabilistic and logical approaches, potentially bringing new insights into the extraction of causal relations, and into the role of descriptive mechanisms in learning.Here's the translation breakdown:不期待性 (bù qīdài xìng) - unexpectedness简洁理论 (jiǎn jiǎn lǐlùn) - Simplicity Theoryrelate (tiě yǔ) - relatevarious inference processes (dào yī) - various inference processesKolmogorov complexities (kēlèmǔ gōngjì) - Kolmogorov complexitiescomputation (suānjiǔ) - computationrather than probabilities (bié kèqì) - rather than probabilitiesits predictive power (wǒ de yìjī) - its predictive powerhas been confirmed (yǐjī) - has been confirmedby several experiments (shíyī zhèng yǐjī) - by several experimentswith human subjects (rénshēng) - with human subjectsbut (but) - butits theoretical basis (wǒ de lǐyì) - its theoretical basisremains largely unexplored (yǐjī zhèngyǐ) - remains largely unexploredwhy does it work? (bù yīnwèi zhèngyǐ) - why does it work?This paper (zhèng zhì) - This paperlays the groundwork (dào zhì) - lays the groundworkfor three theoretical conjectures (sān lǐyì zhèng) - for three theoretical conjecturesFirst, (yī) - Firstunexpectedness (bù qīdài xìng) - unexpectednesscan be seen as (dào yī) - can be seen asa generalization (fāngyì) - a generalizationof Bayes' rule (Bayes de zhèng) - of Bayes' ruleSecond, (èr) - Secondthe frequentist core (liàng zhèng) - the frequentist coreof unexpectedness (bù qīdài xìng) - of unexpectednesscan be connected (dào yī) - can be connectedto the function (fāngyì) - to the functionof tracking (dào) - of trackingergodic properties (érguò) - ergodic propertiesof the world (shìjiè) - of the worldThird, (sān) - Thirdunexpectedness (bù qīdài xìng) - unexpectednesscan be seen as (dào yī) - can be seen asa constituent (fāngyì) - a constituentof various measures (biǎo) - of various measuresof divergence (fāngbiàn) - of divergencebetween (biān) - betweenthe entropy (hétuán) - the entropyof the world (shìjiè) - of the worldand (he) - andthe variety (dào) - the varietyof the observer (jìshì) - of the observerThe resulting framework (zhèng zhì) - The resulting frameworkhints (dào) - hintsto research directions (kēngsuǒ) - to research directionsthat go beyond (biào) - that go beyondthe division (biān) - the divisionbetween probabilistic (suǒyì) - between probabilisticand logical (lógí) - and logicalapproaches (jì) - approachespotentially bringing (dào) - potentially bringingnew insights (xīnwèi) - new insightsinto (yǐ) - intothe extraction (suō) - the extractionof causal relations (liǎo) - of causal relationsand (he) - andthe role (yè) - the roleof descriptive mechanisms (mǎojī) - of descriptive mechanismsin learning (xuéxí) - in learning.
</details></li>
</ul>
<hr>
<h2 id="Combining-Past-Present-and-Future-A-Self-Supervised-Approach-for-Class-Incremental-Learning"><a href="#Combining-Past-Present-and-Future-A-Self-Supervised-Approach-for-Class-Incremental-Learning" class="headerlink" title="Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental Learning"></a>Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08764">http://arxiv.org/abs/2311.08764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoshuang Chen, Zhongyi Sun, Ke Yan, Shouhong Ding, Hongtao Lu</li>
<li>for: 本文目的是解决自适应学习中的 kontinuous novel class 问题，即模型能够识别新来的类，同时避免 catastrophic forgetting。</li>
<li>methods: 本文提出了一种自助学习 CIL 框架 CPPF，包括一个 prototype clustering module (PC)、一个 embedding space reserving module (ESR) 和一个 multi-teacher distillation module (MTD)。PC 和 ESR 模块在prototype level和feature level分别保留 embedding space  для后续阶段，而 MTD 模块保持当前阶段的表示不受过去知识的干扰。</li>
<li>results: 对 CIFAR100 和 ImageNet100 数据集进行了广泛的实验，显示了我们提出的方法可以提高自适应学习中的class incremental learning性能。<details>
<summary>Abstract</summary>
Class Incremental Learning (CIL) aims to handle the scenario where data of novel classes occur continuously and sequentially. The model should recognize the sequential novel classes while alleviating the catastrophic forgetting. In the self-supervised manner, it becomes more challenging to avoid the conflict between the feature embedding spaces of novel classes and old ones without any class labels. To address the problem, we propose a self-supervised CIL framework CPPF, meaning Combining Past, Present and Future. In detail, CPPF consists of a prototype clustering module (PC), an embedding space reserving module (ESR) and a multi-teacher distillation module (MTD). 1) The PC and the ESR modules reserve embedding space for subsequent phases at the prototype level and the feature level respectively to prepare for knowledge learned in the future. 2) The MTD module maintains the representations of the current phase without the interference of past knowledge. One of the teacher networks retains the representations of the past phases, and the other teacher network distills relation information of the current phase to the student network. Extensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our proposed method boosts the performance of self-supervised class incremental learning. We will release code in the near future.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the following text into Simplified Chinese.<</SYS>>类增量学习（CIL）目标是处理连续出现的新类数据场景。模型应该识别连续出现的新类，同时避免catastrophic forgetting。在无监督的方式下，更加挑战是避免新类和旧类的feature embedding空间之间的冲突。为解决这个问题，我们提出了一个自动监督CIL框架CPPF，即Combining Past, Present and Future。在详细的实现方式下，CPPF包括一个原型聚合模块（PC）、一个嵌入空间保留模块（ESR）以及一个多教师浸泡模块（MTD）。1）PC和ESR模块在原型级和特征级分别保留了后续阶段的嵌入空间，以便在未来学习的知识。2）MTD模块保持了当前阶段的表示，并避免了过去知识的干扰。其中一个教师网络保持过去阶段的表示，另一个教师网络将当前阶段的关系信息传播给学生网络。我们在CIFAR100和ImageNet100数据集上进行了广泛的实验，结果表明我们提出的方法可以提高无监督类增量学习的性能。我们将即将发布代码。
</details></li>
</ul>
<hr>
<h2 id="Forms-of-Understanding-of-XAI-Explanations"><a href="#Forms-of-Understanding-of-XAI-Explanations" class="headerlink" title="Forms of Understanding of XAI-Explanations"></a>Forms of Understanding of XAI-Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08760">http://arxiv.org/abs/2311.08760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hendrik Buschmeier, Heike M. Buhl, Friederike Kern, Angela Grimminger, Helen Beierling, Josephine Fisher, André Groß, Ilona Horwath, Nils Klowait, Stefan Lazarov, Michael Lenke, Vivien Lohmer, Katharina Rohlfing, Ingrid Scharlau, Amit Singh, Lutz Terfloth, Anna-Lisa Vollmer, Yu Wang, Annedore Wilmes, Britta Wrede</li>
<li>for: 本文旨在提供一种对Explainable Artificial Intelligence（XAI）领域和其他领域的理解模型，以及对理解的定义和形式、评估和动力的探讨。</li>
<li>methods: 本文采用了多学科的视角，包括计算机科学、语言学、社会学和心理学，对理解的定义和形式、评估和动力进行了探讨和系统化。</li>
<li>results: 本文提出了两种理解的形式，即启用性（knowing how）和理解（knowing that），并论证了这两种理解在解释过程中的发展和互相关系。 I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Explainability has become an important topic in computer science and artificial intelligence, leading to a subfield called Explainable Artificial Intelligence (XAI). The goal of providing or seeking explanations is to achieve (better) 'understanding' on the part of the explainee. However, what it means to 'understand' is still not clearly defined, and the concept itself is rarely the subject of scientific investigation. This conceptual article aims to present a model of forms of understanding in the context of XAI and beyond. From an interdisciplinary perspective bringing together computer science, linguistics, sociology, and psychology, a definition of understanding and its forms, assessment, and dynamics during the process of giving everyday explanations are explored. Two types of understanding are considered as possible outcomes of explanations, namely enabledness, 'knowing how' to do or decide something, and comprehension, 'knowing that' -- both in different degrees (from shallow to deep). Explanations regularly start with shallow understanding in a specific domain and can lead to deep comprehension and enabledness of the explanandum, which we see as a prerequisite for human users to gain agency. In this process, the increase of comprehension and enabledness are highly interdependent. Against the background of this systematization, special challenges of understanding in XAI are discussed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本转换为简化中文。<</SYS>>Explainability 已成为计算机科学和人工智能中重要的话题，导致了一个子领域called Explainable Artificial Intelligence (XAI). 该领域的目标是提供或寻求解释，以达到更好的'理解'。然而，'理解'这个概念仍然没有得到清晰定义，而且这个概念自己也rarely是科学研究的对象。本文旨在提出一个形式理解在 XAI 和其他领域的模型。从计算机科学、语言学、社会学和心理学的多学科角度，一个理解的定义和其形式、评估和过程中的动态都是探讨的对象。在日常解释过程中，理解可以分为两种可能的结果，即'能力'和'认知'，两者都有不同的深度水平（从浅到深）。解释通常从特定领域的浅度理解开始，可以导致解释对象的深度认知和能力，这被视为人类用户获得行为能力的前提。在这个过程中，理解和能力之间存在很高的相互关系。在这个背景下，XAI 中特殊的理解挑战也是讨论的对象。
</details></li>
</ul>
<hr>
<h2 id="Cross-domain-feature-disentanglement-for-interpretable-modeling-of-tumor-microenvironment-impact-on-drug-response"><a href="#Cross-domain-feature-disentanglement-for-interpretable-modeling-of-tumor-microenvironment-impact-on-drug-response" class="headerlink" title="Cross-domain feature disentanglement for interpretable modeling of tumor microenvironment impact on drug response"></a>Cross-domain feature disentanglement for interpretable modeling of tumor microenvironment impact on drug response</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09264">http://arxiv.org/abs/2311.09264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia Zhai, Hui Liu</li>
<li>for: 本研究旨在模拟肿瘤微环境（TME）对药物响应的影响，以提高药物治疗的效果和特点。</li>
<li>methods: 本研究使用了适应域网络进行特征分离，将源领域（cell lines）和目标领域（肿瘤）的特征分离开来，并使用了 Graph Attention Network 学习药物的潜在表示。</li>
<li>results: 研究表明，适应域网络可以 superior performance 在预测药物响应和分解肿瘤微环境对药物效果的影响。<details>
<summary>Abstract</summary>
High-throughput screening technology has facilitated the generation of large-scale drug responses across hundreds of cancer cell lines. However, there exists significant discrepancy between in vitro cell lines and actual tumors in vivo in terms of their response to drug treatments, because of tumors comprise of complex cellular compositions and histopathology structure, known as tumor microenvironment (TME), which greatly influences the drug cytotoxicity against tumor cells. To date, no study has focused on modeling the impact of the TME on clinical drug response. This paper proposed a domain adaptation network for feature disentanglement to separate representations of cancer cells and TME of a tumor in patients. Two denoising autoencoders were separately used to extract features from cell lines (source domain) and tumors (target domain) for partial domain alignment and feature decoupling. The specific encoder was enforced to extract information only about TME. Moreover, to ensure generalizability to novel drugs, we applied a graph attention network to learn the latent representation of drugs, allowing us to linearly model the drug perturbation on cellular state in latent space. We calibrated our model on a benchmark dataset and demonstrated its superior performance in predicting clinical drug response and dissecting the influence of the TME on drug efficacy.
</details>
<details>
<summary>摘要</summary>
高通量屏测技术已经促进了大规模药物响应的生成 across hundreds of cancer cell lines. 然而， exists significant discrepancy between in vitro cell lines and actual tumors in vivo in terms of their response to drug treatments, because tumors comprise complex cellular compositions and histopathology structure, known as tumor microenvironment (TME), which greatly influences the drug cytotoxicity against tumor cells. To date, no study has focused on modeling the impact of the TME on clinical drug response. This paper proposed a domain adaptation network for feature disentanglement to separate representations of cancer cells and TME of a tumor in patients. Two denoising autoencoders were separately used to extract features from cell lines (source domain) and tumors (target domain) for partial domain alignment and feature decoupling. The specific encoder was enforced to extract information only about TME. Moreover, to ensure generalizability to novel drugs, we applied a graph attention network to learn the latent representation of drugs, allowing us to linearly model the drug perturbation on cellular state in latent space. We calibrated our model on a benchmark dataset and demonstrated its superior performance in predicting clinical drug response and dissecting the influence of the TME on drug efficacy.
</details></li>
</ul>
<hr>
<h2 id="Auto-ICL-In-Context-Learning-without-Human-Supervision"><a href="#Auto-ICL-In-Context-Learning-without-Human-Supervision" class="headerlink" title="Auto-ICL: In-Context Learning without Human Supervision"></a>Auto-ICL: In-Context Learning without Human Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09263">http://arxiv.org/abs/2311.09263</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecielyang/auto-icl">https://github.com/ecielyang/auto-icl</a></li>
<li>paper_authors: Jinghan Yang, Shuming Ma, Furu Wei</li>
<li>for: 这个研究旨在提高人机交互的自然语言功能，使大语言模型在各种任务上具备更高的灵活性和自主性。</li>
<li>methods: 该研究提出了一种自动启发学习框架，可以让模型自动生成示例、标签、指导路径等，以便在不同任务上进行启发学习。</li>
<li>results: 研究表明，该方法在多种任务上能够实现优秀的表现，与现有方法相比，具有更高的灵活性和自主性。<details>
<summary>Abstract</summary>
In the era of Large Language Models (LLMs), human-computer interaction has evolved towards natural language, offering unprecedented flexibility. Despite this, LLMs are heavily reliant on well-structured prompts to function efficiently within the realm of In-Context Learning. Vanilla In-Context Learning relies on human-provided contexts, such as labeled examples, explicit instructions, or other guiding mechanisms that shape the model's outputs. To address this challenge, our study presents a universal framework named Automatic In-Context Learning. Upon receiving a user's request, we ask the model to independently generate examples, including labels, instructions, or reasoning pathways. The model then leverages this self-produced context to tackle the given problem. Our approach is universally adaptable and can be implemented in any setting where vanilla In-Context Learning is applicable. We demonstrate that our method yields strong performance across a range of tasks, standing up well when compared to existing methods.
</details>
<details>
<summary>摘要</summary>
（在大语言模型（LLM）时代，人机交互发展到自然语言水平，提供了前所未有的灵活性。然而，LLMs仍然受到良好结构化提示的限制，以便在受限的上下文学习中功能 efficiently。vanilla In-Context Learning rely on人类提供的上下文，如标注的例子、显式的指令或其他引导机制，以shape模型的输出。为解决这个挑战，我们的研究提出了一个通用框架 named Automatic In-Context Learning。当接收用户的请求时，我们会让模型独立生成示例，包括标签、指令或推理路径。然后，模型会利用自己生成的上下文来解决给定的问题。我们的方法是 universally adaptable，可以在任何可以使用vanilla In-Context Learning的场景中实现。我们示出了我们的方法在多种任务上具有强大表现，与现有方法相比，表现良好。）
</details></li>
</ul>
<hr>
<h2 id="Disentangling-the-Potential-Impacts-of-Papers-into-Diffusion-Conformity-and-Contribution-Values"><a href="#Disentangling-the-Potential-Impacts-of-Papers-into-Diffusion-Conformity-and-Contribution-Values" class="headerlink" title="Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values"></a>Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09262">http://arxiv.org/abs/2311.09262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhikai Xue, Guoxiu He, Zhuoren Jiang, Yangyang Kang, Star Zhao, Wei Lu</li>
<li>for: 这个论文的目的是计算学术论文的潜在影响力，并分解其为三个方面：散布、遵循和贡献。</li>
<li>methods: 该论文提出了一种基于图神经网络的新方法，称为DPPDCC，用于解决这些问题。DPPDCC使用动态不同类型的图 структуры，包括时间和结构特征，以捕捉知识的流动。具体来说，它使用比较和相关的信息来捕捉知识的流动，并使用约束来避免模型之间的混淆。</li>
<li>results: 实验结果表明，DPPDCC在不同时间点的论文上表现出色，与基线模型相比，它在新发表、新鲜出版和当下发表的论文上均有显著优势。此外，DPPDCC还能够robust地处理不同类型的论文和数据集。<details>
<summary>Abstract</summary>
The potential impact of an academic paper is determined by various factors, including its popularity and contribution. Existing models usually estimate original citation counts based on static graphs and fail to differentiate values from nuanced perspectives. In this study, we propose a novel graph neural network to Disentangle the Potential impacts of Papers into Diffusion, Conformity, and Contribution values (called DPPDCC). Given a target paper, DPPDCC encodes temporal and structural features within the constructed dynamic heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize the importance of comparative and co-cited/citing information between papers and aggregate snapshots evolutionarily. To unravel popularity, we contrast augmented graphs to extract the essence of diffusion and predict the accumulated citation binning to model conformity. We further apply orthogonal constraints to encourage distinct modeling of each perspective and preserve the inherent value of contribution. To evaluate models' generalization for papers published at various times, we reformulate the problem by partitioning data based on specific time points to mirror real-world conditions. Extensive experimental results on three datasets demonstrate that DPPDCC significantly outperforms baselines for previously, freshly, and immediately published papers. Further analyses confirm its robust capabilities. We will make our datasets and codes publicly available.
</details>
<details>
<summary>摘要</summary>
科学论文的潜在影响因多种因素决定，包括其受欢迎程度和贡献。现有模型通常基于静止图计算原始引用数，而不能区分不同的观点。在这项研究中，我们提出了一种新的图神经网络，即分离论文的潜在影响值（DPPDCC）。给定目标论文，DPPDCC 编码了时间和结构特征在构建的动态 hetэроogeneous图中。特别是，为了捕捉知识的流动，我们强调在比较和引用/引用信息之间的关系中捕捉知识的流动。为了评估媒体，我们对升级图进行比较，从而提取论文的核心特征。我们还应用正交约束，以便独特地模型每个角度，并保留论文的内在价值。为了评估模型在不同时间点发表的论文的普适性，我们将数据分 partitions  according to specific time points，以模拟实际情况。我们的实验结果表明，DPPDCC 在三个数据集上显著超过基线。进一步的分析证明它的稳定性。我们将数据和代码公开。
</details></li>
</ul>
<hr>
<h2 id="Emerging-Drug-Interaction-Prediction-Enabled-by-Flow-based-Graph-Neural-Network-with-Biomedical-Network"><a href="#Emerging-Drug-Interaction-Prediction-Enabled-by-Flow-based-Graph-Neural-Network-with-Biomedical-Network" class="headerlink" title="Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural Network with Biomedical Network"></a>Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural Network with Biomedical Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09261">http://arxiv.org/abs/2311.09261</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lars-research/emergnn">https://github.com/lars-research/emergnn</a></li>
<li>paper_authors: Yongqi Zhang, Quanming Yao, Ling Yue, Xian Wu, Ziheng Zhang, Zhenxi Lin, Yefeng Zheng</li>
<li>for: 预测新药物与新药物之间的药物交互作用，以提高病人患病经验和药物开发效率。</li>
<li>methods: 使用图 нейрон网络（GNN）来预测新药物之间的交互作用，并利用生物医学网络中的资料来提高预测的准确性。</li>
<li>results: EmerGNN比现有方法更高的准确性来预测新药物之间的交互作用，并可以快速地确定最重要的生物医学概念。<details>
<summary>Abstract</summary>
Accurately predicting drug-drug interactions (DDI) for emerging drugs, which offer possibilities for treating and alleviating diseases, with computational methods can improve patient care and contribute to efficient drug development. However, many existing computational methods require large amounts of known DDI information, which is scarce for emerging drugs. In this paper, we propose EmerGNN, a graph neural network (GNN) that can effectively predict interactions for emerging drugs by leveraging the rich information in biomedical networks. EmerGNN learns pairwise representations of drugs by extracting the paths between drug pairs, propagating information from one drug to the other, and incorporating the relevant biomedical concepts on the paths. The different edges on the biomedical network are weighted to indicate the relevance for the target DDI prediction. Overall, EmerGNN has higher accuracy than existing approaches in predicting interactions for emerging drugs and can identify the most relevant information on the biomedical network.
</details>
<details>
<summary>摘要</summary>
通过计算方法精准预测新药 drug-drug interactions (DDI)，可以提高患者护理和药物开发效率。然而，许多现有的计算方法需要大量已知 DDI 信息，而这些信息对新药来说匮乏。在这篇文章中，我们提出 EmerGNN，一种基于图神经网络 (GNN) 的方法，可以有效预测新药之间的交互。EmerGNN 通过提取药物对之间的路径，传递药物之间的信息，并 incorporate 生物医学网络上相关的概念，来学习药物对之间的对应。不同的生物医学网络边缘权重，以指示目标 DDI 预测中的重要性。总的来说，EmerGNN 比现有方法更高精度地预测新药之间的交互，并可以 Identify 生物医学网络上最重要的信息。
</details></li>
</ul>
<hr>
<h2 id="Joint-User-Pairing-and-Beamforming-Design-of-Multi-STAR-RISs-Aided-NOMA-in-the-Indoor-Environment-via-Multi-Agent-Reinforcement-Learning"><a href="#Joint-User-Pairing-and-Beamforming-Design-of-Multi-STAR-RISs-Aided-NOMA-in-the-Indoor-Environment-via-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning"></a>Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08708">http://arxiv.org/abs/2311.08708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Min Park, Yan Kyaw Tun, Choong Seon Hong<br>for:* 6G&#x2F;B5G wireless networks with enhanced quality requirementsmethods:* NOMA technique for multiple users to share resources* STAR-RISs for improved coverage, spectral efficiency, and reliabilityresults:* Joint user pairing and beamforming design for Multi-STAR-RISs in an indoor environment* Maximum total throughput of multiple users (MUs) through optimization of decoding order, user pairing, active beamforming, and passive beamformingPlease note that the above information is in Simplified Chinese text, as requested.<details>
<summary>Abstract</summary>
The development of 6G/B5G wireless networks, which have requirements that go beyond current 5G networks, is gaining interest from academia and industry. However, to increase 6G/B5G network quality, conventional cellular networks that rely on terrestrial base stations are constrained geographically and economically. Meanwhile, NOMA allows multiple users to share the same resources, which improves the spectral efficiency of the system and has the advantage of supporting a larger number of users. Additionally, by intelligently manipulating the phase and amplitude of both the reflected and transmitted signals, STAR-RISs can achieve improved coverage, increased spectral efficiency, and enhanced communication reliability. However, STAR-RISs must simultaneously optimize the amplitude and phase shift corresponding to reflection and transmission, which makes the existing terrestrial networks more complicated and is considered a major challenging issue. Motivated by the above, we study the joint user pairing for NOMA and beamforming design of Multi-STAR-RISs in an indoor environment. Then, we formulate the optimization problem with the objective of maximizing the total throughput of MUs by jointly optimizing the decoding order, user pairing, active beamforming, and passive beamforming. However, the formulated problem is a MINLP. To address this challenge, we first introduce the decoding order for NOMA networks. Next, we decompose the original problem into two subproblems, namely: 1) MU pairing and 2) Beamforming optimization under the optimal decoding order. For the first subproblem, we employ correlation-based K-means clustering to solve the user pairing problem. Then, to jointly deal with beamforming vector optimizations, we propose MAPPO, which can make quick decisions in the given environment owing to its low complexity.
</details>
<details>
<summary>摘要</summary>
6G/B5G无线网络的开发，具有超过当前5G网络的需求，已经吸引了学术界和业界的关注。然而，使得6G/B5G网络质量提高的传统Cellsular网络，受到地面基站的限制，它们的空间和经济性不足。而NOMA技术允许多个用户共享同一资源，提高系统的spectral efficiency，并且可以支持更多的用户。此外，通过智能地控制反射和发射信号的相位和幅度，STAR-RISs可以实现改善的覆盖率、增加spectral efficiency和通信可靠性。然而，STAR-RISs需要同时优化反射和发射信号的相位和幅度，这使得现有的地面网络更加复杂，并被视为主要挑战。驱动了以上，我们研究了Multi-STAR-RISs在室内环境中的用户对称对接和束缚设计。然后，我们形ulated了优化问题的目标，即通过同时优化用户对称对接、束缚、活动束缚和空转束缚来提高多个用户机（MU）的总吞吐量。然而，该问题是一个MINLP问题。为了解决这个挑战，我们首先介绍了NOMA网络中的解码顺序。然后，我们将原问题分解成两个子问题，即：1）用户对称对接问题和2）束缚优化问题。对于第一个子问题，我们采用协方差基于K-means分 clustering算法来解决用户对称对接问题。然后，为了同时处理束缚向量优化问题，我们提议MAPPO，它可以在给定环境中做出快速决策，因为它的复杂度较低。
</details></li>
</ul>
<hr>
<h2 id="Aligned-A-Platform-based-Process-for-Alignment"><a href="#Aligned-A-Platform-based-Process-for-Alignment" class="headerlink" title="Aligned: A Platform-based Process for Alignment"></a>Aligned: A Platform-based Process for Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08706">http://arxiv.org/abs/2311.08706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/klonnet23/helloy-word">https://github.com/klonnet23/helloy-word</a></li>
<li>paper_authors: Ethan Shaotran, Ido Pesok, Sam Jones, Emi Liu</li>
<li>for: 本研究旨在提供一个公信worthy、公开的方式来保障前沿模型的安全性，并最终实现超智能。</li>
<li>methods: 本研究使用了一个 constitutional committee 框架，Initial tests with 680 participants result in a 30-guideline constitution with 93% overall support。</li>
<li>results: 研究显示了平台的自然扩展性，使得社区参与者具有更高的信任和满意度。<details>
<summary>Abstract</summary>
We are introducing Aligned, a platform for global governance and alignment of frontier models, and eventually superintelligence. While previous efforts at the major AI labs have attempted to gather inputs for alignment, these are often conducted behind closed doors. We aim to set the foundation for a more trustworthy, public-facing approach to safety: a constitutional committee framework. Initial tests with 680 participants result in a 30-guideline constitution with 93% overall support. We show the platform naturally scales, instilling confidence and enjoyment from the community. We invite other AI labs and teams to plug and play into the Aligned ecosystem.
</details>
<details>
<summary>摘要</summary>
我们是引入了对齐平台，用于全球治理和前沿模型的对齐，最终是超智能。在大型AI室中，过去的尝试都是在关闭的门后进行集Inputs for alignment，我们想要设立一个更加可靠、公共的安全方法：一个宪法委员会框架。我们的初步测试中，680名参与者共同制定了30个指南，得到了93%的总支持。我们表明该平台自然扩展，带来了社区的信任和愉悦。我们邀请其他AI室和团队加入对齐生态系统。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Follow-Concept-Annotation-Guidelines-A-Case-Study-on-Scientific-and-Financial-Domains"><a href="#Can-Large-Language-Models-Follow-Concept-Annotation-Guidelines-A-Case-Study-on-Scientific-and-Financial-Domains" class="headerlink" title="Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains"></a>Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08704">http://arxiv.org/abs/2311.08704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcio Fonseca, Shay B. Cohen</li>
<li>For: The paper aims to examine the capacity of instruction-tuned large language models (LLMs) to follow in-context concept guidelines for sentence labeling tasks.* Methods: The paper uses zero-shot sentence classification tasks with different types of factual and counterfactual concept definitions as prompts to test the models’ ability to recognize new concepts.* Results: The paper finds that only the larger models (with 70B parameters or more) have limited ability to work under counterfactual contexts, and that proprietary models such as GPT-3.5 and GPT-4 can recognize nonsensical guidelines. Additionally, the paper finds that Falcon-180B-chat is outperformed by Llama-2-70B-chat in most cases, indicating that careful fine-tuning is more effective than increasing model scale.Here’s the simplified Chinese version of the three key points:* For: 论文目的是检验基于叙述示例的指导下，大型自然语言模型（LLMs）是否可以学习新的概念或事实。* Methods: 论文使用零shot句式分类任务，用不同类型的事实和反事实指导来测试模型的新概念认知能力。* Results: 论文发现，只有70B参数或更多的模型才能在对应的反事实上工作，而且专有API如GPT-3.5和GPT-4可以识别无意义的指导。此外，论文发现Falcon-180B-chat在大多数情况下被Llama-2-70B-chat所超越，这表明精心调整是更有效的than增加模型scale。<details>
<summary>Abstract</summary>
Although large language models (LLMs) exhibit remarkable capacity to leverage in-context demonstrations, it is still unclear to what extent they can learn new concepts or facts from ground-truth labels. To address this question, we examine the capacity of instruction-tuned LLMs to follow in-context concept guidelines for sentence labeling tasks. We design guidelines that present different types of factual and counterfactual concept definitions, which are used as prompts for zero-shot sentence classification tasks. Our results show that although concept definitions consistently help in task performance, only the larger models (with 70B parameters or more) have limited ability to work under counterfactual contexts. Importantly, only proprietary models such as GPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is due to more sophisticated alignment methods. Finally, we find that Falcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which indicates that careful fine-tuning is more effective than increasing model scale. Altogether, our simple evaluation method reveals significant gaps in concept understanding between the most capable open-source language models and the leading proprietary APIs.
</details>
<details>
<summary>摘要</summary>
尽管大型语言模型（LLM）具有丰富的 Context 掌握能力，但是是否可以从真实标签中学习新的概念或事实仍然不清楚。为了回答这个问题，我们研究了基于示例示范的 instruction-tuned LLM 是否能够遵循 Context 中的概念指南进行句子标签任务。我们设计了不同类型的事实和反事实概念定义，用作零容量 sentence classification 任务的提示。我们的结果表明，虽然概念定义 invariably 提高任务性能，但只有70B参数或更多的大型模型可以在对应事实上下降性能。此外，我们发现仅有专有模型 such as GPT-3.5 和 GPT-4 可以识别不合理的指南，我们假设这是因为它们使用了更加复杂的对接方法。最后，我们发现 Falcon-180B-chat 通常被 Llama-2-70B-chat 超越，这表明精细的微调更加重要于提高模型规模。总之，我们的简单评估方法 revelas 最高水平的 open-source 语言模型和主流专有 API 之间的概念理解存在显著差距。
</details></li>
</ul>
<hr>
<h2 id="Debate-Helps-Supervise-Unreliable-Experts"><a href="#Debate-Helps-Supervise-Unreliable-Experts" class="headerlink" title="Debate Helps Supervise Unreliable Experts"></a>Debate Helps Supervise Unreliable Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08702">http://arxiv.org/abs/2311.08702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/julianmichael/debate">https://github.com/julianmichael/debate</a></li>
<li>paper_authors: Julian Michael, Salsabila Mahdi, David Rein, Jackson Petty, Julien Dirani, Vishakh Padmakumar, Samuel R. Bowman</li>
<li>for: supervising unreliable AI systems to give answers that are systematically true</li>
<li>methods: using debate between two unreliable experts to help a non-expert judge more reliably identify the truth</li>
<li>results: debate performs significantly better than consultancy (a baseline approach) and is more efficient, with 84% judge accuracy compared to 74% for consultancy<details>
<summary>Abstract</summary>
As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important. How can we supervise unreliable experts, which have access to the truth but may not accurately report it, to give answers that are systematically true and don't just superficially seem true, when the supervisor can't tell the difference between the two on their own? In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth. We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by 'expert' debaters who have access to the passage. In our debates, one expert argues for the correct answer, and the other for an incorrect answer. Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy's 74%. Debates are also more efficient, being 68% of the length of consultancies. By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down. Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill). Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.
</details>
<details>
<summary>摘要</summary>
Traditional Chinese:随着人工智能系统用于回答更加困难的问题，评估其输出的真实性变得更加困难和更加重要。如何监督不可靠的专家，他们有存取真理，但可能不会正确地报告它们？在这个工作中，我们显示了对两名不可靠专家进行辩论可以帮助非专家评估者更加可靠地评估真理。我们收集了一个人类写作的辩论集，其中一名专家认为正确的答案，另一名专家认为 incorrect的答案。与基准我们称之为咨询（consultancy），单一的专家 argue for 正确的答案，其中正确的答案是半数的时间。我们发现，在我们的辩论中，辩论比咨询表现更好，评估者的准确率为84%，而咨询的准确率为74%。辩论也更有效率，长度只有68%。我们对人工和人类辩论进行比较，发现随着专家的技能提高，辩论的表现也提高，而咨询的表现则下降。我们的错误分析也支持这个趋势，发现人类辩论中的错误中46%是由诚实的辩论者所引起的（这些错误可以逐渐消失），而人类咨询中的错误中52%是由辩论者对评估者隐藏重要证据所致（这些错误可以加剧）。总的来说，这些结果显示了辩论是一种可靠地监督 increasingly capable 但可能不可靠的 AI 系统的方法。
</details></li>
</ul>
<hr>
<h2 id="Artificial-General-Intelligence-Existential-Risk-and-Human-Risk-Perception"><a href="#Artificial-General-Intelligence-Existential-Risk-and-Human-Risk-Perception" class="headerlink" title="Artificial General Intelligence, Existential Risk, and Human Risk Perception"></a>Artificial General Intelligence, Existential Risk, and Human Risk Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08698">http://arxiv.org/abs/2311.08698</a></li>
<li>repo_url: None</li>
<li>paper_authors: David R. Mandel</li>
<li>for: 这篇论文关注人工智能（AGI）的可能性，特别是AGI在未来20年内可能超过人类智能水平，然后快速超越人类智能。</li>
<li>methods: 作者基于公开可用的预测和意见数据，研究了专家和非专家对AGI的风险认知。</li>
<li>results: 研究发现，对AGI的世界大悲害或灭绝风险的认知比其他潜在存在风险（如核战或人类引起的气候变化）高，过去一年内对AGI风险的认知增长也比其他风险更快。<details>
<summary>Abstract</summary>
Artificial general intelligence (AGI) does not yet exist, but given the pace of technological development in artificial intelligence, it is projected to reach human-level intelligence within roughly the next two decades. After that, many experts expect it to far surpass human intelligence and to do so rapidly. The prospect of superintelligent AGI poses an existential risk to humans because there is no reliable method for ensuring that AGI goals stay aligned with human goals. Drawing on publicly available forecaster and opinion data, the author examines how experts and non-experts perceive risk from AGI. The findings indicate that the perceived risk of a world catastrophe or extinction from AGI is greater than for other existential risks. The increase in perceived risk over the last year is also steeper for AGI than for other existential threats (e.g., nuclear war or human-caused climate change). That AGI is a pressing existential risk is something on which experts and non-experts agree, but the basis for such agreement currently remains obscure.
</details>
<details>
<summary>摘要</summary>
人工总智能（AGI）目前还没有存在，但根据技术发展的速度，预计在下一两十年内达到人类水平的智能。之后，许多专家预计它会迅速超越人类智能。超智AGI的出现对人类存在极大的风险，因为没有可靠的方法来保证AGI目标与人类目标相对应。作者通过公开available的预测和意见数据，检查专家和非专家对AGI风险的认知。结果表明AGI世界灾难或灭绝的风险高于其他极大风险（如核战或人类引起的气候变化）。过去一年内AGI风险的增加速度也比其他极大风险更大。虽然专家和非专家都认为AGI是一种极大的存在风险，但目前这种一致的基础还未明确。
</details></li>
</ul>
<hr>
<h2 id="An-Eye-on-Clinical-BERT-Investigating-Language-Model-Generalization-for-Diabetic-Eye-Disease-Phenotyping"><a href="#An-Eye-on-Clinical-BERT-Investigating-Language-Model-Generalization-for-Diabetic-Eye-Disease-Phenotyping" class="headerlink" title="An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping"></a>An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08687">http://arxiv.org/abs/2311.08687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kharrigian/ml4h-clinical-bert">https://github.com/kharrigian/ml4h-clinical-bert</a></li>
<li>paper_authors: Keith Harrigian, Tina Tang, Anthony Gonzales, Cindy X. Cai, Mark Dredze</li>
<li>for: 本研究旨在帮助监测 диабетиче眼病的临床趋势和检测护理不足，以预防盲视。</li>
<li>methods: 本研究使用了19种临床概念相关的文本提取系统，以检测和描述 диабетиче眼病的临床特征。</li>
<li>results: 研究发现，使用BERT语言模型预训练在非临床数据上的语言模型，对于本领域来说并无显著改进。<details>
<summary>Abstract</summary>
Diabetic eye disease is a major cause of blindness worldwide. The ability to monitor relevant clinical trajectories and detect lapses in care is critical to managing the disease and preventing blindness. Alas, much of the information necessary to support these goals is found only in the free text of the electronic medical record. To fill this information gap, we introduce a system for extracting evidence from clinical text of 19 clinical concepts related to diabetic eye disease and inferring relevant attributes for each. In developing this ophthalmology phenotyping system, we are also afforded a unique opportunity to evaluate the effectiveness of clinical language models at adapting to new clinical domains. Across multiple training paradigms, we find that BERT language models pretrained on out-of-distribution clinical data offer no significant improvement over BERT language models pretrained on non-clinical data for our domain. Our study tempers recent claims that language models pretrained on clinical data are necessary for clinical NLP tasks and highlights the importance of not treating clinical language data as a single homogeneous domain.
</details>
<details>
<summary>摘要</summary>
糖尿病眼病是全球最大的失明原因之一。监测相关的临床轨迹和检测护理缺失是控制疾病和避免失明的关键。然而，大量关键信息都藏在电子医疗记录中的自由文本中，使得管理疾病困难。为了填补这个信息差距，我们介绍了一种EXTRACTING EVIDENCE FROM CLINICAL TEXT OF 19 CLINICAL CONCEPTS RELATED TO DIABETIC EYE DISEASE AND INFERRING RELEVANT ATTRIBUTES FOR EACH。在开发这种眼科phenotyping系统时，我们也获得了评估临床语言模型在新临床领域中的适应性的机会。在多种训练方法中，我们发现BERT语言模型在非临床数据上进行预训练后对我们领域没有显著提高。我们的研究抑制了最近的宣称，即临床语言数据上的语言模型预训练是临床NLP任务中必不可少的。我们的研究也 highlights the importance of not treating clinical language data as a single homogeneous domain。
</details></li>
</ul>
<hr>
<h2 id="Safer-Instruct-Aligning-Language-Models-with-Automated-Preference-Data"><a href="#Safer-Instruct-Aligning-Language-Models-with-Automated-Preference-Data" class="headerlink" title="Safer-Instruct: Aligning Language Models with Automated Preference Data"></a>Safer-Instruct: Aligning Language Models with Automated Preference Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08685">http://arxiv.org/abs/2311.08685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uscnlp-lime/safer-instruct">https://github.com/uscnlp-lime/safer-instruct</a></li>
<li>paper_authors: Taiwei Shi, Kai Chen, Jieyu Zhao</li>
<li>for: 本研究旨在提高语言模型的安全性，通过人工审核和自动生成数据来提高模型的准确率和安全性。</li>
<li>methods: 本研究提出了一种新的数据生成管道，即Safer-Instruct，它使用倒转指令调整、指令生成和专家模型评估来生成高质量的偏好数据，不需要人工纠正。</li>
<li>results: 通过LLaMA进行指令生成和GPT-4作为专家模型，生成了约10K个偏好样本。通过训练一个Alpaca模型于此数据集，可以提高模型的安全性而不会影响其对话和下游任务的性能。Safer-Instruct解决了偏好数据获取的挑战，为更安全和责任的AI系统的发展提供了新的思路。<details>
<summary>Abstract</summary>
Reinforcement Learning from Human Feedback (RLHF) is a vital strategy for enhancing model safety in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for semi-automatically constructing large-scale preference datasets. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. We evaluate Safer-Instruct using LLaMA for instruction induction and GPT-4 as an expert model, generating approximately 10K preference samples. Finetuning an Alpaca model on this dataset demonstrates improved harmlessness while maintaining competitive performance on conversation and downstream tasks. Safer-Instruct addresses the challenges in preference data acquisition, advancing the development of safer and more responsible AI systems. Our code and data are available at https://github.com/uscnlp-lime/safer-instruct
</details>
<details>
<summary>摘要</summary>
� Reinforcement Learning from Human Feedback (RLHF) 是一种重要的策略来提高语言模型的安全性。然而，为RLHF annotating偏好数据是一个资源密集且创作需求高的过程，而自动生成方法受到数据多样性和质量的限制。为此，我们提出了Safer-Instruct，一个新的管线来半自动建构大规模的偏好数据。我们的方法利用倒转指令调整、指令生成和专家模型评估，以生成高品质的偏好数据，不需要人工标注员。我们使用LLaMA进行指令生成和GPT-4作为专家模型，生成约10K偏好数据。给Alpaca模型进行调整后，示出改善了无害性，同时保持了与对话和下游任务的竞争性能。Safer-Instruct解决了偏好数据取得的挑战，推动了更安全和责任的AI系统的开发。我们的代码和数据可以在https://github.com/uscnlp-lime/safer-instruct上取得。
</details></li>
</ul>
<hr>
<h2 id="Multi-Set-Inoculation-Assessing-Model-Robustness-Across-Multiple-Challenge-Sets"><a href="#Multi-Set-Inoculation-Assessing-Model-Robustness-Across-Multiple-Challenge-Sets" class="headerlink" title="Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets"></a>Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08662">http://arxiv.org/abs/2311.08662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, Dan Roth</li>
<li>for: 这个研究旨在理解语言模型对输入异常的敏感性，以增强模型的可信度。</li>
<li>methods: 研究使用了精细调教和多个干扰的训练策略，以及一种链式思维（COT）示例来提高模型的多干扰Robustness。</li>
<li>results: 研究显示，使用提议的方法可以训练模型对不同干扰的Robustness，而不会影响模型在给定任务上的准确率。<details>
<summary>Abstract</summary>
Language models, given their black-box nature, often exhibit sensitivity to input perturbations, leading to trust issues due to hallucinations. To bolster trust, it's essential to understand these models' failure modes and devise strategies to enhance their performance. In this study, we propose a framework to study the effect of input perturbations on language models of different scales, from pre-trained models to large language models (LLMs). We use fine-tuning to train a robust model to perturbations, and we investigate whether exposure to one perturbation improves or degrades the model's performance on other perturbations. To address multi-perturbation robustness, we suggest three distinct training strategies. We also extend the framework to LLMs via a chain of thought(COT) prompting with exemplars. We instantiate our framework for the Tabular-NLI task and show that the proposed strategies train the model robust to different perturbations without losing accuracy on a given dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文本模型，由于其黑盒特性，经常表现出输入杂乱的敏感性，导致不信任问题由于幻觉。为了增强不信任，我们需要理解这些模型的失败模式，并设计策略来提高其性能。在这项研究中，我们提议一个框架来研究输入杂乱对不同规模的语言模型（从预训练模型到大语言模型）的影响。我们使用精度训练来适应杂乱，并研究曝光一种杂乱后，模型对其他杂乱的性能是否改善或恶化。为了解决多种杂乱的可靠性，我们提出三种不同的训练策略。此外，我们将框架扩展到大语言模型（LLMs）via一种链式思维（COT）提示法，并通过例子来实现。我们在Tabular-NLI任务上实现了我们的框架，并示出了提议的策略可以在给定数据集上训练模型抗杂乱而不失去精度。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Large-Language-Model-Agents-Enabling-Intent-Driven-Mobile-GUI-Testing"><a href="#Autonomous-Large-Language-Model-Agents-Enabling-Intent-Driven-Mobile-GUI-Testing" class="headerlink" title="Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing"></a>Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08649">http://arxiv.org/abs/2311.08649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juyeon Yoon, Robert Feldt, Shin Yoo</li>
<li>for:  automatize GUI testing of Android apps, to increase testing efficiency and coverage</li>
<li>methods:  uses Large Language Models and support mechanisms such as long- and short-term memory to set relevant task goals and perform realistic tasks</li>
<li>results:  achieved 61% activity coverage and 317 out of 374 autonomously created tasks are realistic and relevant to app functionalities, outperforming current state-of-the-art GUI testing techniques.<details>
<summary>Abstract</summary>
GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51% for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 374 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.
</details>
<details>
<summary>摘要</summary>
GUI 测试检查软件系统在用户与图形界面交互时是否按预期的行为，例如测试特定功能或验证相关用例enario。目前，决定要测试的高级水平是一个手动任务，因为自动化GUI测试工具通常target lower level的充分度度量 such as 结构代码覆盖率或活动覆盖率。我们提出了DroidAgent，一个基于大型自然语言模型和支持机制such as长期和短期记忆的Android GUI测试自动化工具。给一个Android应用程序，DroidAgent会设定相关的任务目标，然后通过与应用程序交互来实现这些目标。我们对DroidAgent使用Themis测试套件中的15个应用程序进行了实验性评估，结果显示DroidAgent可以自动设置和执行真实的任务，并且比现有的GUI测试技术高一个等级。例如，当测试一个消息应用程序时，DroidAgent创建了一个第二个帐户，并将第一个帐户添加为好友，测试了一个真实的用例，没有人工干预。在average，DroidAgent achieve 61%的活动覆盖率，比现有技术的51%高。此外，手动分析结果显示DroidAgent自动创建的374个任务中，317个任务是真实有用和相关于应用程序功能，而且DroidAgent会深入与应用程序交互，覆盖更多的功能。
</details></li>
</ul>
<hr>
<h2 id="Explore-Spurious-Correlations-at-the-Concept-Level-in-Language-Models-for-Text-Classification"><a href="#Explore-Spurious-Correlations-at-the-Concept-Level-in-Language-Models-for-Text-Classification" class="headerlink" title="Explore Spurious Correlations at the Concept Level in Language Models for Text Classification"></a>Explore Spurious Correlations at the Concept Level in Language Models for Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08648">http://arxiv.org/abs/2311.08648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, Furong Huang</li>
<li>for: 本研究旨在探讨语言模型（LM）在不同语言处理任务中的表现，以及如何减少LM因杂质相关性而导致的Robustness问题。</li>
<li>methods: 本研究使用语言模型（LM）对文本进行标签，并测试LM在不同文本分类任务中的表现。同时，我们还提出了一种数据重新平衡方法，通过添加LM生成的对反数据来减少杂质相关性。</li>
<li>results: 研究结果表明，存在多个概念的标签分布偏误在多个文本分类数据集中，LM会利用这些偏误来进行预测，而我们的减少方法可以有效地减少这些偏误。<details>
<summary>Abstract</summary>
Language models (LMs) have gained great achievement in various NLP tasks for both fine-tuning and in-context learning (ICL) methods. Despite its outstanding performance, evidence shows that spurious correlations caused by imbalanced label distributions in training data (or exemplars in ICL) lead to robustness issues. However, previous studies mostly focus on word- and phrase-level features and fail to tackle it from the concept level, partly due to the lack of concept labels and subtle and diverse expressions of concepts in text. In this paper, we first use the LLM to label the concept for each text and then measure the concept bias of models for fine-tuning or ICL on the test data. Second, we propose a data rebalancing method to mitigate the spurious correlations by adding the LLM-generated counterfactual data to make a balanced label distribution for each concept. We verify the effectiveness of our mitigation method and show its superiority over the token removal method. Overall, our results show that there exist label distribution biases in concepts across multiple text classification datasets, and LMs will utilize these shortcuts to make predictions in both fine-tuning and ICL methods.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）在各种自然语言处理（NLP）任务中已经取得了很大的成就，包括精度训练和上下文学习（ICL）方法。尽管它们的表现很出色，但证据表明，由于训练数据中标签的不均匀分布而导致的偏见问题。然而，前一些研究主要集中在单词和短语水平的特征上，忽略了概念水平的问题，其中一个原因是缺乏概念标签，以及文本中概念的柔和和多样化表达。在本文中，我们首先使用LM来标注每个文本中的概念，然后测量模型在测试数据上的概念偏见。其次，我们提出了一种数据重新补做方法，以避免由于标签分布的偏见问题。我们证明了我们的mitigation方法的有效性，并证明它在和token移除方法相比而言更有优势。总之，我们的结果表明，存在多个文本分类 datasets中的概念标签偏见，LM在精度训练和ICL方法中都会利用这些短cuts来做预测。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-by-Design-Wrapper-Boxes-Combine-Neural-Performance-with-Faithful-Explanations"><a href="#Interpretable-by-Design-Wrapper-Boxes-Combine-Neural-Performance-with-Faithful-Explanations" class="headerlink" title="Interpretable by Design: Wrapper Boxes Combine Neural Performance with Faithful Explanations"></a>Interpretable by Design: Wrapper Boxes Combine Neural Performance with Faithful Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08644">http://arxiv.org/abs/2311.08644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiheng Su, Juni Jessy Li, Matthew Lease</li>
<li>for: 能够保持神经网络模型的准确性while提供 faithful的解释吗？我们提出了“ wrapper boxes”，一种通用的方法来生成 faithful， example-based解释 для模型预测结果，同时保持预测性能。</li>
<li>methods: 我们首先训练了一个神经网络模型，然后将其学习的特征表示输入到一个可解释的模型中进行实际预测。这种简单的策略 surprisingly effective，results largely comparable to those of the original neural model， как shown across three large pre-trained language models, two datasets of varying scale, four classic models, and four evaluation metrics。</li>
<li>results: 此外，因为这些可解释模型是设计为可解释的，所以可以直接向用户显示trainig example subset That determine classic model predictions。<details>
<summary>Abstract</summary>
Can we preserve the accuracy of neural models while also providing faithful explanations? We present wrapper boxes, a general approach to generate faithful, example-based explanations for model predictions while maintaining predictive performance. After training a neural model as usual, its learned feature representation is input to a classic, interpretable model to perform the actual prediction. This simple strategy is surprisingly effective, with results largely comparable to those of the original neural model, as shown across three large pre-trained language models, two datasets of varying scale, four classic models, and four evaluation metrics. Moreover, because these classic models are interpretable by design, the subset of training examples that determine classic model predictions can be shown directly to users.
</details>
<details>
<summary>摘要</summary>
可以保持神经网络模型的准确性 while also providing faithful explanations? We present wrapper boxes, a general approach to generate faithful, example-based explanations for model predictions while maintaining predictive performance. After training a neural model as usual, its learned feature representation is input to a classic, interpretable model to perform the actual prediction. This simple strategy is surprisingly effective, with results largely comparable to those of the original neural model, as shown across three large pre-trained language models, two datasets of varying scale, four classic models, and four evaluation metrics. Moreover, because these classic models are interpretable by design, the subset of training examples that determine classic model predictions can be shown directly to users.Here's the translation in Traditional Chinese:可以保持神经网络模型的准确性 while also providing faithful explanations? We present wrapper boxes, a general approach to generate faithful, example-based explanations for model predictions while maintaining predictive performance. After training a neural model as usual, its learned feature representation is input to a classic, interpretable model to perform the actual prediction. This simple strategy is surprisingly effective, with results largely comparable to those of the original neural model, as shown across three large pre-trained language models, two datasets of varying scale, four classic models, and four evaluation metrics. Moreover, because these classic models are interpretable by design, the subset of training examples that determine classic model predictions can be shown directly to users.
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Graph-Neural-Point-Process-for-Traffic-Congestion-Event-Prediction"><a href="#Spatio-Temporal-Graph-Neural-Point-Process-for-Traffic-Congestion-Event-Prediction" class="headerlink" title="Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event Prediction"></a>Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08635">http://arxiv.org/abs/2311.08635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyin Jin, Lingbo Liu, Fuxian Li, Jincai Huang</li>
<li>for: 预测交通堵塞事件，以提高智能交通系统的效能。</li>
<li>methods: 我们提出了一种基于图 neural point process 框架的 spatial-temporal graph neural network，可以充分捕捉历史交通状态数据中的长距离空间-时间依赖关系，同时还可以模型堵塞事件的发展趋势。</li>
<li>results: 我们的方法在两个实际数据集上进行了广泛的实验，并证明了与现有状态艺术方法相比，其性能更高。<details>
<summary>Abstract</summary>
Traffic congestion event prediction is an important yet challenging task in intelligent transportation systems. Many existing works about traffic prediction integrate various temporal encoders and graph convolution networks (GCNs), called spatio-temporal graph-based neural networks, which focus on predicting dense variables such as flow, speed and demand in time snapshots, but they can hardly forecast the traffic congestion events that are sparsely distributed on the continuous time axis. In recent years, neural point process (NPP) has emerged as an appropriate framework for event prediction in continuous time scenarios. However, most conventional works about NPP cannot model the complex spatio-temporal dependencies and congestion evolution patterns. To address these limitations, we propose a spatio-temporal graph neural point process framework, named STGNPP for traffic congestion event prediction. Specifically, we first design the spatio-temporal graph learning module to fully capture the long-range spatio-temporal dependencies from the historical traffic state data along with the road network. The extracted spatio-temporal hidden representation and congestion event information are then fed into a continuous gated recurrent unit to model the congestion evolution patterns. In particular, to fully exploit the periodic information, we also improve the intensity function calculation of the point process with a periodic gated mechanism. Finally, our model simultaneously predicts the occurrence time and duration of the next congestion. Extensive experiments on two real-world datasets demonstrate that our method achieves superior performance in comparison to existing state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
traffic 堵塞事件预测是智能交通系统中的一个重要 yet 挑战性任务。许多现有的交通预测方法 integrates 多种 temporal 编码器和图像 convolution 网络（GCNs），称为 spatio-temporal 图像-based 神经网络，它们主要 focus 在 predicting 稠密变量 such as flow, speed 和 demand 在时刻戳中，但它们很难预测分布在继续时间轴上的交通堵塞事件。在过去几年，神经点过程（NPP）已经 emerge 为继续时间场景中的适用性Frameworks。然而，大多数传统的 NPP 方法无法模型 complex spatio-temporal 依赖关系和堵塞演化模式。为了解决这些局限性，我们提出了一种 spatio-temporal 图像神经点过程框架，名为 STGNPP  для交通堵塞事件预测。具体来说，我们首先设计了 spatio-temporal 图像学习模块，以全面捕捉历史交通状态数据中的长距离 spatio-temporal 依赖关系，同时与道路网络相结合。提取的 spatio-temporal 隐藏表示和堵塞事件信息然后被 fed 到一个连续闭合回归单元，以模型堵塞演化模式。特别是，为了充分利用周期信息，我们还改进了点过程中的 Intensity 函数计算方法。最后，我们的模型同时预测下一次堵塞事件的发生时间和持续时间。广泛的实验表明，我们的方法在两个真实世界数据集上表现出优于现有的状态前方法。
</details></li>
</ul>
<hr>
<h2 id="XplainLLM-A-QA-Explanation-Dataset-for-Understanding-LLM-Decision-Making"><a href="#XplainLLM-A-QA-Explanation-Dataset-for-Understanding-LLM-Decision-Making" class="headerlink" title="XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making"></a>XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08614">http://arxiv.org/abs/2311.08614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichen Chen, Jianda Chen, Mitali Gaidhani, Ambuj Singh, Misha Sra</li>
<li>for: 本研究旨在提高大型自然语言处理模型（LLM）的决策过程的可见性，通过创建一个新的问答解释数据集（QAE），integrating知识图（KG）。</li>
<li>methods: 我们使用了知识图和图注意网络（GAT）来找到reason-elements，并将其转化为可理解的why-choose和why-not-choose解释。</li>
<li>results: 我们通过量化和质量评价表明，我们的数据集可以提高LLM在上下文学习中的性能，提高其解释性和可见性，使其更加可靠和可信worthy。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have recently made impressive strides in natural language understanding tasks. Despite their remarkable performance, understanding their decision-making process remains a big challenge. In this paper, we look into bringing some transparency to this process by introducing a new explanation dataset for question answering (QA) tasks that integrates knowledge graphs (KGs) in a novel way. Our dataset includes 12,102 question-answer-explanation (QAE) triples. Each explanation in the dataset links the LLM's reasoning to entities and relations in the KGs. The explanation component includes a why-choose explanation, a why-not-choose explanation, and a set of reason-elements that underlie the LLM's decision. We leverage KGs and graph attention networks (GAT) to find the reason-elements and transform them into why-choose and why-not-choose explanations that are comprehensible to humans. Through quantitative and qualitative evaluations, we demonstrate the potential of our dataset to improve the in-context learning of LLMs, and enhance their interpretability and explainability. Our work contributes to the field of explainable AI by enabling a deeper understanding of the LLMs decision-making process to make them more transparent and thereby, potentially more reliable, to researchers and practitioners alike. Our dataset is available at: https://github.com/chen-zichen/XplainLLM_dataset.git
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Navigating-the-Ocean-of-Biases-Political-Bias-Attribution-in-Language-Models-via-Causal-Structures"><a href="#Navigating-the-Ocean-of-Biases-Political-Bias-Attribution-in-Language-Models-via-Causal-Structures" class="headerlink" title="Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures"></a>Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08605">http://arxiv.org/abs/2311.08605</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/david-jenny/llm-political-study">https://github.com/david-jenny/llm-political-study</a></li>
<li>paper_authors: David F. Jenny, Yann Billeter, Mrinmaya Sachan, Bernhard Schölkopf, Zhijing Jin</li>
<li>for: 本研究旨在探讨 Large Language Models (LLMs) 在政治辩论中的决策过程和内在偏见。</li>
<li>methods: 本研究使用 Activity Dependency Networks (ADNs) 抽取 LLMs 中的隐式评价标准，并 illustrate how normative values influence these perceptions。</li>
<li>results: 研究发现 LLMs 在评价 “好Arguments” 时存在偏见，并且这些偏见受到了 normative values 的影响。这些结果有关于人机同步和偏见减少的影响。<details>
<summary>Abstract</summary>
The rapid advancement of Large Language Models (LLMs) has sparked intense debate regarding their ability to perceive and interpret complex socio-political landscapes. In this study, we undertake an exploration of decision-making processes and inherent biases within LLMs, exemplified by ChatGPT, specifically contextualizing our analysis within political debates. We aim not to critique or validate LLMs' values, but rather to discern how they interpret and adjudicate "good arguments." By applying Activity Dependency Networks (ADNs), we extract the LLMs' implicit criteria for such assessments and illustrate how normative values influence these perceptions. We discuss the consequences of our findings for human-AI alignment and bias mitigation. Our code and data at https://github.com/david-jenny/LLM-Political-Study.
</details>
<details>
<summary>摘要</summary>
LLMs 的快速发展已经引发了对其能够理解和解释复杂社会政治景观的激烈讨论。在这个研究中，我们进行了 LLMS 决策过程和内在偏见的探索，以 chatGPT 为例，并在政治辩论中进行了Contextual化分析。我们的目标不是评价或验证 LLMS 的价值观，而是理解它们如何解读和评价 "好的论点"。通过应用 Activity Dependency Networks (ADNs)，我们提取了 LLMS 的隐藏标准 для这些评价，并示出了如何 normative 价值影响这些见解。我们讨论了我们发现的后果，以及如何实现人机同步和偏见缓减。我们的代码和数据可以在 <https://github.com/david-jenny/LLM-Political-Study> 找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.AI_2023_11_15/" data-id="clpahu6zb00753h88dq7j06dx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.CL_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T11:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.CL_2023_11_15/">cs.CL - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Lexical-Repetitions-Lead-to-Rote-Learning-Unveiling-the-Impact-of-Lexical-Overlap-in-Train-and-Test-Reference-Summaries"><a href="#Lexical-Repetitions-Lead-to-Rote-Learning-Unveiling-the-Impact-of-Lexical-Overlap-in-Train-and-Test-Reference-Summaries" class="headerlink" title="Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries"></a>Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09458">http://arxiv.org/abs/2311.09458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prafulla Kumar Choubey, Alexander R. Fabbri, Caiming Xiong, Chien-Sheng Wu<br>for:* The paper is written to propose a fine-grained evaluation protocol for summarization models to determine their competencies in generalizing to novel summary-worthy content.methods:* The authors use a test set partitioned based on the lexical similarity of reference test summaries with training summaries to evaluate the model’s performance.* They observe a significant difference in ROUGE-2 and entity recall scores between the subsets with the lowest and highest similarity.results:* The authors show that limiting lexical repetitions in training summaries during both supervised fine-tuning and likelihood calibration stages can improve the model’s performance on novel test cases while retaining average performance.* Their automatic and human evaluations on novel test subsets and recent news articles demonstrate that limiting lexical repetitions can prevent rote learning and improve generalization.<details>
<summary>Abstract</summary>
Ideal summarization models should generalize to novel summary-worthy content without remembering reference training summaries by rote. However, a single average performance score on the entire test set is inadequate in determining such model competencies. We propose a fine-grained evaluation protocol by partitioning a test set based on the lexical similarity of reference test summaries with training summaries. We observe up to a 5x (1.2x) difference in ROUGE-2 (entity recall) scores between the subsets with the lowest and highest similarity. Next, we show that such training repetitions also make a model vulnerable to rote learning, reproducing data artifacts such as factual errors, especially when reference test summaries are lexically close to training summaries. Consequently, we propose to limit lexical repetitions in training summaries during both supervised fine-tuning and likelihood calibration stages to improve the performance on novel test cases while retaining average performance. Our automatic and human evaluations on novel test subsets and recent news articles show that limiting lexical repetitions in training summaries can prevent rote learning and improve generalization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Subtle-Misogyny-Detection-and-Mitigation-An-Expert-Annotated-Dataset"><a href="#Subtle-Misogyny-Detection-and-Mitigation-An-Expert-Annotated-Dataset" class="headerlink" title="Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset"></a>Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09443">http://arxiv.org/abs/2311.09443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brooklyn Sheppard, Anna Richter, Allison Cohen, Elizabeth Allyn Smith, Tamara Kneese, Carolyne Pelletier, Ioana Baldini, Yue Dong</li>
<li>for: 本研究用于开发一个新的 dataset，捕捉女性偏见的细节和复杂性。</li>
<li>methods: 该 dataset 使用多学科专家和注释器共同建构，包括电影字幕注释，捕捉北美电影中的日常性偏见表达。</li>
<li>results: 该研究提供了偏见检测和改进的基准值，并分析了获得的注释。 hope 该工作能够推动 AI 为社会好用的 NLP 技术发展。<details>
<summary>Abstract</summary>
Using novel approaches to dataset development, the Biasly dataset captures the nuance and subtlety of misogyny in ways that are unique within the literature. Built in collaboration with multi-disciplinary experts and annotators themselves, the dataset contains annotations of movie subtitles, capturing colloquial expressions of misogyny in North American film. The dataset can be used for a range of NLP tasks, including classification, severity score regression, and text generation for rewrites. In this paper, we discuss the methodology used, analyze the annotations obtained, and provide baselines using common NLP algorithms in the context of misogyny detection and mitigation. We hope this work will promote AI for social good in NLP for bias detection, explanation, and removal.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:使用创新的数据集开发方法，Biasly数据集 capture了偏见的细节和细腻性，在文献中具有独特的表现。与多种学科专家和批注人员合作建立的数据集包含电影字幕拼音，捕捉了北美电影中的日常性偏见。该数据集可以用于多种NLP任务，包括分类、偏见度评分和文本生成重写。在这篇论文中，我们介绍了使用的方法、分析获得的拼音和使用常见NLP算法进行偏见检测和修正的基线。我们希望这项工作能够促进NLP领域的AI为社会好。
</details></li>
</ul>
<hr>
<h2 id="Labeled-Interactive-Topic-Models"><a href="#Labeled-Interactive-Topic-Models" class="headerlink" title="Labeled Interactive Topic Models"></a>Labeled Interactive Topic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09438">http://arxiv.org/abs/2311.09438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Kyle Seelman, Mozhi Zhang, Jordan Boyd-Graber</li>
<li>for: 用于改善 neural topic model 中的主题选择</li>
<li>methods: 使用用户标签来修改主题，以更好地满足用户的信息需求</li>
<li>results: 通过人工研究，发现用户标签可以提高文档排名分数，从而更好地找到与查询有关的文档<details>
<summary>Abstract</summary>
Topic models help users understand large document collections; however, topic models do not always find the ``right'' topics. While classical probabilistic and anchor-based topic models have interactive variants to guide models toward better topics, such interactions are not available for neural topic models such as the embedded topic model (\abr{etm}). We correct this lacuna by adding an intuitive interaction to neural topic models: users can label a topic with a word, and topics are updated so that the topic words are close to the label. This allows a user to refine topics based on their information need. While, interactivity is intuitive for \abr{etm}, we extend this framework to work with other neural topic models as well. We develop an interactive interface which allows users to interact and relabel topic models as they see fit. We evaluate our method through a human study, where users can relabel topics to find relevant documents. Using our method, user labeling improves document rank scores, helping to find more relevant documents to a given query when compared to no user labeling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Striped-Attention-Faster-Ring-Attention-for-Causal-Transformers"><a href="#Striped-Attention-Faster-Ring-Attention-for-Causal-Transformers" class="headerlink" title="Striped Attention: Faster Ring Attention for Causal Transformers"></a>Striped Attention: Faster Ring Attention for Causal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09431">http://arxiv.org/abs/2311.09431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exists-forall/striped_attention">https://github.com/exists-forall/striped_attention</a></li>
<li>paper_authors: William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, Jonathan Ragan-Kelley</li>
<li>for: 提高Transformer模型中的序列长度增长的能力</li>
<li>methods: 使用Ring Attention算法和Striped Attention扩展来解决每个设备内存瓶颈问题</li>
<li>results: 在 causal transformer 模型中实现1.45倍的终端通过put通过率提高，并在16个TPUv4板上实现1.65倍的速度提高，sequence length为256k和786k。<details>
<summary>Abstract</summary>
To help address the growing demand for ever-longer sequence lengths in transformer models, Liu et al. recently proposed Ring Attention, an exact attention algorithm capable of overcoming per-device memory bottle- necks by distributing self-attention across multiple devices. In this paper, we study the performance characteristics of Ring Attention in the important special case of causal transformer models, and identify a key workload imbal- ance due to triangular structure of causal attention computations. We propose a simple extension to Ring Attention, which we call Striped Attention to fix this imbalance. Instead of devices having contiguous subsequences, each device has a subset of tokens distributed uniformly throughout the sequence, which we demonstrate leads to more even workloads. In experiments running Striped Attention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x end-to-end throughput improvements over the original Ring Attention algorithm on causal transformer training at a sequence length of 256k. Furthermore, on 16 TPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of 786k. We release the code for our experiments as open source
</details>
<details>
<summary>摘要</summary>
为了满足长序列的增长需求，刘等人最近提出了环形注意力算法（Ring Attention），可以在单个设备内分布自注意力，从而缓解单个设备内存瓶颈。在这篇论文中，我们研究了环形注意力在重要的 causal transformer 模型中的性能特点，并发现了一个关键的工作负荷不均。我们提出了一个简单的扩展，称为扫描注意力（Striped Attention），可以解决这一问题。在实验中，我们在 A100 GPU 和 TPUv4 上运行了扫描注意力算法，并 achieved 256k 序列长度下的最大 1.45x 终端通过puts，以及 786k 序列长度下的最大 1.65x 终端通过puts。此外，我们还发布了我们的实验代码作为开源。
</details></li>
</ul>
<hr>
<h2 id="Predicting-generalization-performance-with-correctness-discriminators"><a href="#Predicting-generalization-performance-with-correctness-discriminators" class="headerlink" title="Predicting generalization performance with correctness discriminators"></a>Predicting generalization performance with correctness discriminators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09422">http://arxiv.org/abs/2311.09422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuekun Yao, Alexander Koller</li>
<li>for: 预测NLP模型在未看过数据上的准确率，以确保模型的可靠性。</li>
<li>methods: 提出了一种新的模型，通过训练一个推断器，来预测序列到序列模型输出是正确或错误的。</li>
<li>results: 在多种标注、分析和semantic parsing任务上，金字典准确率都在预测的上下限之间，并且这些上下限很接近。<details>
<summary>Abstract</summary>
The ability to predict an NLP model's accuracy on unseen, potentially out-of-distribution data is a prerequisite for trustworthiness. We present a novel model that establishes upper and lower bounds on the accuracy, without requiring gold labels for the unseen data. We achieve this by training a discriminator which predicts whether the output of a given sequence-to-sequence model is correct or not. We show across a variety of tagging, parsing, and semantic parsing tasks that the gold accuracy is reliably between the predicted upper and lower bounds, and that these bounds are remarkably close together.
</details>
<details>
<summary>摘要</summary>
使得预测NLP模型对未看过、可能不属于输入范围的数据的准确率是一个必要的前提，以确保模型的可靠性。我们提出了一种新的模型，可以在未看过数据上预测模型的准确率，不需要黄金标签。我们通过训练一个推断器，判断给定的序列-到-序列模型输出是否正确，来实现这一点。我们在不同的标注、分析和 semantics 解析任务上显示，黄金准确率在预测的Upper和Lower bound之间，这些 bound 很接近。</SYS>Here's the translation in Simplified Chinese:<SYS>    使得预测NLP模型对未看过、可能不属于输入范围的数据的准确率是一个必要的前提，以确保模型的可靠性。我们提出了一种新的模型，可以在未看过数据上预测模型的准确率，不需要黄金标签。我们通过训练一个推断器，判断给定的序列-到-序列模型输出是否正确，来实现这一点。我们在不同的标注、分析和 semantics 解析任务上显示，黄金准确率在预测的Upper和Lower bound之间，这些 bound 很接近。</SYS>Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Alternatives-to-the-Scaled-Dot-Product-for-Attention-in-the-Transformer-Neural-Network-Architecture"><a href="#Alternatives-to-the-Scaled-Dot-Product-for-Attention-in-the-Transformer-Neural-Network-Architecture" class="headerlink" title="Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture"></a>Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09406">http://arxiv.org/abs/2311.09406</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Bernhard</li>
<li>for: 避免权重缺失导致梯度消失的问题</li>
<li>methods: 提出一些替代缩放方法，包括将dot product除以键值Sum前应用softmax</li>
<li>results: 通过使用模拟的键和问题示例，显示了这些缩放方法在许多情况下效果更好，避免了梯度消失的问题<details>
<summary>Abstract</summary>
The transformer neural network architecture uses a form of attention in which the dot product of query and key is divided by the square root of the key dimension before applying softmax. This scaling of the dot product is designed to avoid the absolute value of the dot products becoming so large that applying softmax leads to vanishing gradients. In this paper, we propose some alternative scalings, including dividing the dot product instead by the sum of the key lengths before applying softmax. We use simulated keys and queries to show that in many situations this appears to be more effective at avoiding regions where applying softmax leads to vanishing gradients.
</details>
<details>
<summary>摘要</summary>
transformer神经网络架构使用一种叫做注意力的机制，其中查询和键的点积被除以键维度的平方根之后应用softmax。这种缩放的点积是为了避免查询和键的绝对值变得太大，使得应用softmax导致梯度消失。在这篇论文中，我们提出了一些代替的缩放方法，包括在应用softmax之前将点积除以键的总长度。我们使用模拟的查询和键来显示，在许多情况下，这些缩放方法更有效地避免应用softmax导致梯度消失的区域。
</details></li>
</ul>
<hr>
<h2 id="To-Translate-or-Not-to-Translate-A-Systematic-Investigation-of-Translation-Based-Cross-Lingual-Transfer-to-Low-Resource-Languages"><a href="#To-Translate-or-Not-to-Translate-A-Systematic-Investigation-of-Translation-Based-Cross-Lingual-Transfer-to-Low-Resource-Languages" class="headerlink" title="To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages"></a>To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09404">http://arxiv.org/abs/2311.09404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benedikt Ebing, Goran Glavaš</li>
<li>for: 本研究旨在系统地评估现有和提出新的翻译基于的跨语言迁移（XLT）方法，以便在低资源语言上进行迁移。</li>
<li>methods: 本研究使用了翻译基于的XLT方法，包括将源语言训练数据翻译回目标语言，并将目标语言测试数据翻译回源语言。此外，还添加了其他高资源语言的可靠翻译来加强模型。</li>
<li>results: 研究发现，使用翻译基于的XLT方法可以大幅超越零极XLT方法，并且可以通过添加其他高资源语言的翻译来进一步提高实验性能。此外，研究还提出了一种能够在不支持MT系统的语言上实现XLT的效果的策略。最后，研究发现，使用MT系统生成的目标语言验证数据来选择XLT模型可以更好地提高模型性能。<details>
<summary>Abstract</summary>
Perfect machine translation (MT) would render cross-lingual transfer (XLT) by means of multilingual language models (LMs) superfluous. Given, on one hand, the large body of work on improving XLT with multilingual LMs and, on the other hand, recent advances in massively multilingual MT, in this work, we systematically evaluate existing and propose new translation-based XLT approaches for transfer to low-resource languages. We show that all translation-based approaches dramatically outperform zero-shot XLT with multilingual LMs, rendering the approach that combines the round-trip translation of the source-language training data with the translation of the target-language test instances the most effective. We next show that one can obtain further empirical gains by adding reliable translations to other high-resource languages to the training data. Moreover, we propose an effective translation-based XLT strategy even for languages not supported by the MT system. Finally, we show that model selection for XLT based on target-language validation data obtained with MT outperforms model selection based on the source-language data. We hope that our findings encourage adoption of more robust translation-based baselines in XLT research.
</details>
<details>
<summary>摘要</summary>
如果精准机器翻译（MT）能够实现语言转换（XLT），那么使用多语言模型（LM）来实现XLT将成为 redundant。在一个手上，有大量关于提高XLT的多语言LM的研究，而在另一个手上，有最近的质量翻译技术的进步。在这项工作中，我们系统地评估了现有的翻译基于XLT的方法，并提出了新的翻译基于XLT的方法。我们发现所有的翻译基于方法在零投入XLT中都表现出了很好的表现，而combined round-trip translation of the source-language training data with the translation of the target-language test instances的方法是最有效的。我们还证明可以通过添加其他高资源语言的可靠翻译到训练数据中来获得更高的实验性赢利。此外，我们提出了一种有效的翻译基于XLT策略，即使Language不支持MT系统。最后，我们发现基于MT系统 validation data 进行模型选择可以更好地than基于源语言数据。我们希望我们的发现能够激励XLT研究中更多使用更加稳定的翻译基于基准。
</details></li>
</ul>
<hr>
<h2 id="LEEETs-Dial-Linguistic-Entrainment-in-End-to-End-Task-oriented-Dialogue-systems"><a href="#LEEETs-Dial-Linguistic-Entrainment-in-End-to-End-Task-oriented-Dialogue-systems" class="headerlink" title="LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems"></a>LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09390">http://arxiv.org/abs/2311.09390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nalin Kumar, Ondřej Dušek</li>
<li>for: 这个研究旨在提高对话系统的自然性，通过实现对话Alignment。</li>
<li>methods: 该研究使用GPT-2基于端到端对话系统，并采用共享词汇来实现对话Alignment。试用了训练实例权重、对ignment特定的损失函数和额外conditioning来生成与用户的响应。</li>
<li>results: 通过对MultiWOZ数据集进行比较，研究发现三种 entraining 技术均可以significantly improve alignment compared to the baseline，并被自动和手动评估指标证明。<details>
<summary>Abstract</summary>
Linguistic entrainment, or alignment, represents a phenomenon where linguistic patterns employed by conversational participants converge to one another. While alignment has been shown to produce a more natural user experience, most dialogue systems do not have any provisions for it. In this work, we introduce methods for achieving dialogue alignment in a GPT-2-based end-to-end dialogue system through the utilization of shared vocabulary. We experiment with training instance weighting, alignment-specific loss, and additional conditioning to generate responses that align with the user. By comparing different entrainment techniques on the MultiWOZ dataset, we demonstrate that all three approaches produce significantly better-aligned results than the baseline, as confirmed by both automated and manual evaluation metrics.
</details>
<details>
<summary>摘要</summary>
语言同步（或对齐）现象表示对话参与者使用的语言模式相互听得一致。尽管对齐可以提供更自然的用户体验，但大多数对话系统没有相关的规定。在这项工作中，我们介绍了基于 GPT-2 的端到端对话系统中实现对话对齐的方法，通过共享词汇的使用。我们对训练实例权重、对齐特定的损失函数和附加条件进行实验，以生成与用户相对的回答。通过对 MultiWOZ 数据集的不同对齐技术进行比较，我们证明了所有三种方法均可以在自动和手动评估指标上提供显著更好的对齐效果。
</details></li>
</ul>
<hr>
<h2 id="Neural-machine-translation-for-automated-feedback-on-children’s-early-stage-writing"><a href="#Neural-machine-translation-for-automated-feedback-on-children’s-early-stage-writing" class="headerlink" title="Neural machine translation for automated feedback on children’s early-stage writing"></a>Neural machine translation for automated feedback on children’s early-stage writing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09389">http://arxiv.org/abs/2311.09389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Vestergaard Jensen, Mikkel Jordahn, Michael Riis Andersen</li>
<li>for: 本研究旨在自动生成初级写作评估和建议，使用机器学习技术。</li>
<li>methods: 本研究提议使用序列到序列模型将初级写作翻译成正常写作，以便使用语言指标进行分析。此外，提出了一种新的强度 likelihood 来抑制数据集中的噪声影响。</li>
<li>results: 经numerical实验 validate，可以高精度预测正常写作。<details>
<summary>Abstract</summary>
In this work, we address the problem of assessing and constructing feedback for early-stage writing automatically using machine learning. Early-stage writing is typically vastly different from conventional writing due to phonetic spelling and lack of proper grammar, punctuation, spacing etc. Consequently, early-stage writing is highly non-trivial to analyze using common linguistic metrics. We propose to use sequence-to-sequence models for "translating" early-stage writing by students into "conventional" writing, which allows the translated text to be analyzed using linguistic metrics. Furthermore, we propose a novel robust likelihood to mitigate the effect of noise in the dataset. We investigate the proposed methods using a set of numerical experiments and demonstrate that the conventional text can be predicted with high accuracy.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们解决了自动使用机器学习进行早期写作评估和建构反馈的问题。早期写作通常具有不同的语音拼写和缺失正确的语法、标点、间距等等特点，因此对于常见语言指标来说非常困难分析。我们提议使用序列到序列模型将学生早期写作翻译成“常规”的写作，以便使用语言指标进行分析。此外，我们提出了一种新的稳定 likelihood 来抑制数据集中的噪声的影响。我们通过数字实验 investigate 这些方法，并证明可以高度准确地预测常规文本。
</details></li>
</ul>
<hr>
<h2 id="Banach-Tarski-Embeddings-and-Transformers"><a href="#Banach-Tarski-Embeddings-and-Transformers" class="headerlink" title="Banach-Tarski Embeddings and Transformers"></a>Banach-Tarski Embeddings and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09387">http://arxiv.org/abs/2311.09387</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jtmaher/embedding">https://github.com/jtmaher/embedding</a></li>
<li>paper_authors: Joshua Maher</li>
<li>for: 这个论文是为了提出一种将任意递归数据结构嵌入高维向量空间的新方法。</li>
<li>methods: 这个论文使用的方法包括提出一种可解释性模型，用于转换器的秘密状态向量。这个模型可以在嵌入维度够大时将嵌入vector解码回原始数据结构。此解码算法自然地实现为一种转换器。此外，这些嵌入向量还可以直接进行计算，无需解码。例如，我们提出了一种使用vector操作构建嵌入Token序列的嵌入 parse树算法。</li>
<li>results: 这个论文的结果表明，当嵌入维度够大时，这种嵌入可以准确地重建原始数据结构。此外，这种嵌入还可以 Directly manipulate the embedded vectors to perform computations on the underlying data without decoding.<details>
<summary>Abstract</summary>
We introduce a new construction of embeddings of arbitrary recursive data structures into high dimensional vectors. These embeddings provide an interpretable model for the latent state vectors of transformers. We demonstrate that these embeddings can be decoded to the original data structure when the embedding dimension is sufficiently large. This decoding algorithm has a natural implementation as a transformer. We also show that these embedding vectors can be manipulated directly to perform computations on the underlying data without decoding. As an example we present an algorithm that constructs the embedded parse tree of an embedded token sequence using only vector operations in embedding space.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的嵌入构造，用于将任意递归数据结构嵌入高维向量中。这些嵌入提供了可解释的模型 дляtransformer的latent状态向量。我们证明了这些嵌入可以在嵌入维度充分大的情况下被解码回原始数据结构。这个解码算法自然地实现为transformer。我们还证明了这些嵌入向量可以直接进行计算，而无需解码。作为示例，我们提出了一个算法，用于在嵌入空间内构建token序列的嵌入树结构。
</details></li>
</ul>
<hr>
<h2 id="Long-form-Question-Answering-An-Iterative-Planning-Retrieval-Generation-Approach"><a href="#Long-form-Question-Answering-An-Iterative-Planning-Retrieval-Generation-Approach" class="headerlink" title="Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach"></a>Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09383">http://arxiv.org/abs/2311.09383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pritom Saha Akash, Kashob Kumar Roy, Lucian Popa, Kevin Chen-Chuan Chang</li>
<li>for: 这篇论文是为了解决长形问答（LFQA）问题，旨在生成详细的回答，而不是简单的是或否答案或短要的信息。</li>
<li>methods: 该论文提出了一种基于谱计规划、检索和生成的LFQA模型，通过多次迭代的计划、检索和生成过程来生成详细的回答。</li>
<li>results: 经过广泛的实验，该模型在开放领域和技术领域的QA数据集上表现出优于现有模型，在多种文本和事实指标上具有更高的表现。<details>
<summary>Abstract</summary>
Long-form question answering (LFQA) poses a challenge as it involves generating detailed answers in the form of paragraphs, which go beyond simple yes/no responses or short factual answers. While existing QA models excel in questions with concise answers, LFQA requires handling multiple topics and their intricate relationships, demanding comprehensive explanations. Previous attempts at LFQA focused on generating long-form answers by utilizing relevant contexts from a corpus, relying solely on the question itself. However, they overlooked the possibility that the question alone might not provide sufficient information to identify the relevant contexts. Additionally, generating detailed long-form answers often entails aggregating knowledge from diverse sources. To address these limitations, we propose an LFQA model with iterative Planning, Retrieval, and Generation. This iterative process continues until a complete answer is generated for the given question. From an extensive experiment on both an open domain and a technical domain QA dataset, we find that our model outperforms the state-of-the-art models on various textual and factual metrics for the LFQA task.
</details>
<details>
<summary>摘要</summary>
长swers 问题 (LFQA) 提出了一个挑战，因为它们需要生成详细的答案，而不是单纯的是或否答案或简短的事实答案。现有的 QA 模型在问题中可以提供简短的答案，但 LFQA 需要处理多个话题和它们的复杂关系，需要详细的解释。过去的 LFQA 尝试都是通过使用相关的文本库来生成长答案，但它们忽视了问题本身可能无法提供足够的信息来定义相关的文本库。此外，生成详细的长答案通常需要从多个来源汇集知识。为解决这些限制，我们提出了一个基于迭代的计划、检索和生成的 LFQA 模型。这个迭代过程一直进行，直到为给定的问题生成完整的答案。从对开放领域和技术领域 QA 数据集的广泛实验来看，我们发现我们的模型在不同的文本和事实指标上超过了当前状态的模型。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Online-User-Aggression-Content-Detection-and-Behavioural-Analysis-on-Social-Media-Platforms"><a href="#A-Survey-on-Online-User-Aggression-Content-Detection-and-Behavioural-Analysis-on-Social-Media-Platforms" class="headerlink" title="A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms"></a>A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09367">http://arxiv.org/abs/2311.09367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swapnil Mane, Suman Kundu, Rajesh Sharma</li>
<li>for: This paper aims to bridge the gap between disparate studies on aggression content detection and behavioral analysis of aggressive users in the context of cyber-aggressive behavior.</li>
<li>methods: The paper examines the comprehensive process of aggression content detection, including dataset creation, feature selection and extraction, and detection algorithm development. It also reviews studies on behavioral analysis of aggression that explore influencing factors, consequences, and patterns associated with cyber-aggressive behavior.</li>
<li>results: The paper identifies research gaps and encourages further progress in the unified domain of socio-computational aggressive behavior analysis.Here’s the Chinese version of the three information points:</li>
<li>for: 这篇论文目标是将不同领域的侵略行为探究归并到一起，以探讨cyber-侵略行为中的社会问题。</li>
<li>methods: 论文检查了侵略内容检测的全面过程，包括数据集创建、特征选择和提取、检测算法开发。它还查看了对侵略行为的行为分析研究，探讨了侵略行为的影响因素、后果和模式。</li>
<li>results: 论文发现了研究漏洞，并促进了在统一领域内的社会计算侵略行为分析的进展。<details>
<summary>Abstract</summary>
The rise of social media platforms has led to an increase in cyber-aggressive behavior, encompassing a broad spectrum of hostile behavior, including cyberbullying, online harassment, and the dissemination of offensive and hate speech. These behaviors have been associated with significant societal consequences, ranging from online anonymity to real-world outcomes such as depression, suicidal tendencies, and, in some instances, offline violence. Recognizing the societal risks associated with unchecked aggressive content, this paper delves into the field of Aggression Content Detection and Behavioral Analysis of Aggressive Users, aiming to bridge the gap between disparate studies. In this paper, we analyzed the diversity of definitions and proposed a unified cyber-aggression definition. We examine the comprehensive process of Aggression Content Detection, spanning from dataset creation, feature selection and extraction, and detection algorithm development. Further, we review studies on Behavioral Analysis of Aggression that explore the influencing factors, consequences, and patterns associated with cyber-aggressive behavior. This systematic literature review is a cross-examination of content detection and behavioral analysis in the realm of cyber-aggression. The integrated investigation reveals the effectiveness of incorporating sociological insights into computational techniques for preventing cyber-aggressive behavior. Finally, the paper concludes by identifying research gaps and encouraging further progress in the unified domain of socio-computational aggressive behavior analysis.
</details>
<details>
<summary>摘要</summary>
“社交媒体平台的崛起导致了网络攻击性行为的增加，包括网络欺凌、网络恐吓和各种不实和恨言。这些行为与社会的后果存在联系，包括线上匿名和实际世界的抑郁、自杀倾向和，在某些情况下，网络上的暴力。本文探讨了网络攻击性行为的多元定义，并提出了统一的网络攻击定义。我们分析了各种数据集的建立、特征选择和提取，以及检测算法的发展。此外，我们审查了关于攻击者行为的行为分析研究，探讨了这些行为的影响因素、后果和模式。本文的系统性审查显示了融合社会学知识和计算技术可以预防网络攻击性行为。最后，本文总结了研究缺陷，并鼓励进一步的进展在统一的网络攻击行为分析领域。”
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Emergent-Audio-Classification-Ability-of-ASR-Foundation-Models"><a href="#Investigating-the-Emergent-Audio-Classification-Ability-of-ASR-Foundation-Models" class="headerlink" title="Investigating the Emergent Audio Classification Ability of ASR Foundation Models"></a>Investigating the Emergent Audio Classification Ability of ASR Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09363">http://arxiv.org/abs/2311.09363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rao Ma, Adian Liusie, Mark J. F. Gales, Kate M. Knill</li>
<li>for: 这 paper 探讨了 ASR 基础模型 Whisper 和 MMS 在零shot 设定下的语音分类能力。</li>
<li>methods: 这 paper 使用了简单的模板基于文本提示，将 decoder 的解码概率用于生成零shot 预测。无需训练模型或添加新参数，Whisper 在多个 audio-classification dataset 上表现出了有前所未有的零shot 分类性能，比前一个状态的平均精度高出 9%。</li>
<li>results: 这 paper 发现，对零shot 分类任务，Whisper 模型的性能随模型大小增长，表明随着 ASR 基础模型的扩大，其零shot 性能可能会提高。<details>
<summary>Abstract</summary>
Text and vision foundation models can perform many tasks in a zero-shot setting, a desirable property that enables these systems to be applied in general and low-resource settings. However, there has been significantly less work on the zero-shot abilities of ASR foundation models, with these systems typically fine-tuned to specific tasks or constrained to applications that match their training criterion and data annotation. In this work we investigate the ability of Whisper and MMS, ASR foundation models trained primarily for speech recognition, to perform zero-shot audio classification. We use simple template-based text prompts at the decoder and use the resulting decoding probabilities to generate zero-shot predictions. Without training the model on extra data or adding any new parameters, we demonstrate that Whisper shows promising zero-shot classification performance on a range of 8 audio-classification datasets, outperforming existing state-of-the-art zero-shot baseline's accuracy by an average of 9%. One important step to unlock the emergent ability is debiasing, where a simple unsupervised reweighting method of the class probabilities yields consistent significant performance gains. We further show that performance increases with model size, implying that as ASR foundation models scale up, they may exhibit improved zero-shot performance.
</details>
<details>
<summary>摘要</summary>
文本和视觉基础模型可以完成许多零 shot 任务，这是一个极其愉快的特性，使这些系统可以在通用和低资源环境中应用。然而，针对 ASR 基础模型的零 shot 能力的研究远未充分，这些系统通常是特定任务的精度调整或者限定到与其训练标准和数据注解相匹配的应用。在这项工作中，我们调查了 Whisper 和 MMS，这两个 ASR 基础模型是主要用于语音识别的。我们使用简单的模板基于文本提示，并使用 resulting 的解码概率来生成零 shot 预测。无需训练模型Extra 数据或添加新参数，我们示出了 Whisper 在多个 8 个音频分类数据集上的出色的零 shot 分类性能，与现有状态的平均性能提高率为 9%。一种重要的步骤是去偏见，其中一种简单的无监督重weighting 方法可以持续提供显著的性能提升。我们进一步表明，性能随模型大小增长，implying 随 ASR 基础模型的扩大，它们可能会表现出更好的零 shot 性能。
</details></li>
</ul>
<hr>
<h2 id="LePaRD-A-Large-Scale-Dataset-of-Judges-Citing-Precedents"><a href="#LePaRD-A-Large-Scale-Dataset-of-Judges-Citing-Precedents" class="headerlink" title="LePaRD: A Large-Scale Dataset of Judges Citing Precedents"></a>LePaRD: A Large-Scale Dataset of Judges Citing Precedents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09356">http://arxiv.org/abs/2311.09356</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rmahari/lepard">https://github.com/rmahari/lepard</a></li>
<li>paper_authors: Robert Mahari, Dominik Stammbach, Elliott Ash, Alex &#96;Sandy’ Pentland</li>
<li>for: 本研究的目的是发展实用法律自然语言处理技术，帮助扩大法律研究的访问和 justice 的质量。</li>
<li>methods: 本研究使用了大量的美国联邦法院判例文献，通过Contextualized Word Embeddings 和文本分类来进行预测。</li>
<li>results: 研究发现，使用文本分类方法可以在预测法律前置文献中达到较高的准确率，但是法律预测仍然是一项具有挑战性的任务，具有很大的改进空间。<details>
<summary>Abstract</summary>
We present the Legal Passage Retrieval Dataset LePaRD. LePaRD is a massive collection of U.S. federal judicial citations to precedent in context. The dataset aims to facilitate work on legal passage prediction, a challenging practice-oriented legal retrieval and reasoning task. Legal passage prediction seeks to predict relevant passages from precedential court decisions given the context of a legal argument. We extensively evaluate various retrieval approaches on LePaRD, and find that classification appears to work best. However, we note that legal precedent prediction is a difficult task, and there remains significant room for improvement. We hope that by publishing LePaRD, we will encourage others to engage with a legal NLP task that promises to help expand access to justice by reducing the burden associated with legal research. A subset of the LePaRD dataset is freely available and the whole dataset will be released upon publication.
</details>
<details>
<summary>摘要</summary>
我们介绍了《法律段落预测数据集》（LePaRD）。LePaRD是一个庞大的美国联邦司法文献引用集，旨在促进法律段落预测任务的研究。法律段落预测是一种实践 oriented 的法律检索和逻辑任务，旨在预测基于法律 Argument 的相关部分。我们在 LePaRD 上进行了广泛的评估，发现类别方法在这些任务中表现最好。然而，我们注意到法律预测是一个具有挑战性的任务，还有很大的改进空间。我们希望通过发布 LePaRD，促进法律自然语言处理任务的研究，以扩大对正义的访问。一个 LePaRD 的子集已经公开可用，整个数据集将在发表后公开。
</details></li>
</ul>
<hr>
<h2 id="Language-and-Task-Arithmetic-with-Parameter-Efficient-Layers-for-Zero-Shot-Summarization"><a href="#Language-and-Task-Arithmetic-with-Parameter-Efficient-Layers-for-Zero-Shot-Summarization" class="headerlink" title="Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization"></a>Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09344">http://arxiv.org/abs/2311.09344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandra Chronopoulou, Jonas Pfeiffer, Joshua Maynez, Xinyi Wang, Sebastian Ruder, Priyanka Agrawal</li>
<li>for: 提高大语言模型在下游任务中的性能，特别是使用 labelled task 数据进行 parameter-efficient fine-tuning (PEFT)。</li>
<li>methods: 提出了一种基于 language 或 task 特有的 parameter 的特殊化方法，通过元素 wise 加法操作来挖掘无标注数据和英语标注数据。</li>
<li>results: 经验表明，该方法可以在摘要任务上取得稳定的提升，只需要训练 PEFT 模块 minimal amount of training data。<details>
<summary>Abstract</summary>
Parameter-efficient fine-tuning (PEFT) using labeled task data can significantly improve the performance of large language models (LLMs) on the downstream task. However, there are 7000 languages in the world and many of these languages lack labeled data for real-world language generation tasks. In this paper, we propose to improve zero-shot cross-lingual transfer by composing language or task specialized parameters. Our method composes language and task PEFT modules via element-wise arithmetic operations to leverage unlabeled data and English labeled data. We extend our approach to cases where labeled data from more languages is available and propose to arithmetically compose PEFT modules trained on languages related to the target. Empirical results on summarization demonstrate that our method is an effective strategy that obtains consistent gains using minimal training of PEFT modules.
</details>
<details>
<summary>摘要</summary>
参数高效调整（PEFT）使用标注任务数据可以显著提高大语言模型（LLM）的下游任务性能。然而，世界上有7000种语言，并且许多这些语言缺乏实际语言生成任务的标注数据。在这篇论文中，我们提议通过语言或任务特化的参数进行改进零上下游语言传递。我们的方法通过语言和任务PEFT模块之间的元素积算操作来利用无标注数据和英文标注数据。我们将我们的方法扩展到有更多语言的标注数据的情况，并提议使用相关语言的PEFT模块进行加法组合。实验结果表明，我们的方法是一种有效的策略，可以通过最小的PEFT模块训练而获得常见的提升。
</details></li>
</ul>
<hr>
<h2 id="Pinpoint-Not-Criticize-Refining-Large-Language-Models-via-Fine-Grained-Actionable-Feedback"><a href="#Pinpoint-Not-Criticize-Refining-Large-Language-Models-via-Fine-Grained-Actionable-Feedback" class="headerlink" title="Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback"></a>Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09336">http://arxiv.org/abs/2311.09336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, Markus Freitag</li>
<li>for: 提高文本生成质量</li>
<li>methods: 使用细化的行为反馈，通过一个学习的错误定位模型来进行迭代改进</li>
<li>results: 在三个文本生成任务中，包括机器翻译、长篇问答和主题概要，观察到0.8和0.7 MetricX的提升，以及4.5和1.8 ROUGE-L的提升，单次迭代改进。使用仿生热化算法可以进一步提高质量，包括最多1.7 MetricX的提升。<details>
<summary>Abstract</summary>
Recent improvements in text generation have leveraged human feedback to improve the quality of the generated output. However, human feedback is not always available, especially during inference. In this work, we propose an inference time optimization method FITO to use fine-grained actionable feedback in the form of error type, error location and severity level that are predicted by a learned error pinpoint model for iterative refinement. FITO starts with an initial output, then iteratively incorporates the feedback via a refinement model that generates an improved output conditioned on the feedback. Given the uncertainty of consistent refined samples at iterative steps, we formulate iterative refinement into a local search problem and develop a simulated annealing based algorithm that balances exploration of the search space and optimization for output quality. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA) and topical summarization. We observe 0.8 and 0.7 MetricX gain on Chinese-English and English-German translation, 4.5 and 1.8 ROUGE-L gain at long form QA and topic summarization respectively, with a single iteration of refinement. With our simulated annealing algorithm, we see further quality improvements, including up to 1.7 MetricX improvements over the baseline approach.
</details>
<details>
<summary>摘要</summary>
Recent improvements in text generation have leveraged human feedback to improve the quality of the generated output. However, human feedback is not always available, especially during inference. In this work, we propose an inference time optimization method FITO to use fine-grained actionable feedback in the form of error type, error location, and severity level that are predicted by a learned error pinpoint model for iterative refinement. FITO starts with an initial output, then iteratively incorporates the feedback via a refinement model that generates an improved output conditioned on the feedback. Given the uncertainty of consistent refined samples at iterative steps, we formulate iterative refinement into a local search problem and develop a simulated annealing based algorithm that balances exploration of the search space and optimization for output quality. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA), and topical summarization. We observe 0.8 and 0.7 MetricX gain on Chinese-English and English-German translation, 4.5 and 1.8 ROUGE-L gain at long form QA and topic summarization respectively, with a single iteration of refinement. With our simulated annealing algorithm, we see further quality improvements, including up to 1.7 MetricX improvements over the baseline approach.
</details></li>
</ul>
<hr>
<h2 id="Mind’s-Mirror-Distilling-Self-Evaluation-Capability-and-Comprehensive-Thinking-from-Large-Language-Models"><a href="#Mind’s-Mirror-Distilling-Self-Evaluation-Capability-and-Comprehensive-Thinking-from-Large-Language-Models" class="headerlink" title="Mind’s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models"></a>Mind’s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09214">http://arxiv.org/abs/2311.09214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weize Liu, Guocong Li, Kai Zhang, Bang Du, Qiyuan Chen, Xuming Hu, Hongxia Xu, Jintai Chen, Jian Wu</li>
<li>for: 提高小语言模型（SLM）的性能，使其更加接近人类认知。</li>
<li>methods: 提出了一种两重方法，首先是将大语言模型（LLM）中的自我评估能力抽象到 SLM 中，以减少错误的 reasoning 和幻见的影响；其次是提出了一种多种链条思维和自我评估 paradigm 的总体distillation进程，以确保更加全面和坚实地将知识传递到 SLM 中。</li>
<li>results: 实验结果表明，我们的方法可以显著提高 distilled SLM 的性能，并且突出了开发更小的模型，更 closely aligns with human cognition 的道路。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved remarkable advancements in the field of natural language processing. However, the sheer scale and computational demands of these models present formidable challenges when considering their practical deployment in resource-constrained contexts. While techniques such as chain-of-thought (CoT) distillation have displayed promise in distilling LLMs into small language models (SLMs), there is a risk that distilled SLMs may still carry over flawed reasoning or hallucinations inherited from their LLM counterparts. To address these issues, we propose a twofold methodology: First, we introduce a novel method for distilling the self-evaluation capability inherent in LLMs into SLMs, which aims to mitigate the adverse effects of erroneous reasoning and reduce hallucinations. Second, we advocate for a comprehensive distillation process that incorporates multiple distinct chain-of-thought and self-evaluation paradigms and ensures a more holistic and robust knowledge transfer into SLMs. Experiments on three NLP benchmarks demonstrate that our method significantly improves the performance of distilled SLMs and sheds light on the path towards developing smaller models closely aligned with human cognition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Distill the self-evaluation capability of LLMs into small language models (SLMs) to mitigate erroneous reasoning and reduce hallucinations.2. Use a comprehensive distillation process that incorporates multiple chain-of-thought and self-evaluation paradigms for a more holistic and robust knowledge transfer.Experiments on three NLP benchmarks show that our method significantly improves the performance of distilled SLMs and provides insights into developing smaller models aligned with human cognition.</details></li>
</ol>
<hr>
<h2 id="GRIM-GRaph-based-Interactive-narrative-visualization-for-gaMes"><a href="#GRIM-GRaph-based-Interactive-narrative-visualization-for-gaMes" class="headerlink" title="GRIM: GRaph-based Interactive narrative visualization for gaMes"></a>GRIM: GRaph-based Interactive narrative visualization for gaMes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09213">http://arxiv.org/abs/2311.09213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Leandro, Sudha Rao, Michael Xu, Weijia Xu, Nebosja Jojic, Chris Brockett, Bill Dolan</li>
<li>for: 帮助对话式角色扮演游戏（RPG）的故事创作。</li>
<li>methods: 使用大型生成文本模型协助创作过程。</li>
<li>results: 可以生成rich narrative graph with branching storylines，并且可以在设计者的交互下自动生成新的子图文件，以满足编辑需求。<details>
<summary>Abstract</summary>
Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The narratives of these may take years to write and typically involve a large creative team. In this work, we demonstrate the potential of large generative text models to assist this process. \textbf{GRIM}, a prototype \textbf{GR}aph-based \textbf{I}nteractive narrative visualization system for ga\textbf{M}es, generates a rich narrative graph with branching storylines that match a high-level narrative description and constraints provided by the designer. Game designers can interactively edit the graph by automatically generating new sub-graphs that fit the edits within the original narrative and constraints. We illustrate the use of \textbf{GRIM} in conjunction with GPT-4, generating branching narratives for four well-known stories with different contextual constraints.
</details>
<details>
<summary>摘要</summary>
对话式角色游戏（RPG）需要强大的故事编写。这些故事可能需要几年时间写作，通常需要一大群创作人员。在这个工作中，我们展示了大型生成文本模型如何帮助这个过程。我们开发了一个名为“GRIM”的原型，它是一个基于图的互动式narative视觉系统，可以根据设计师提供的高级剧本和约束生成丰富的剧本图。设计师可以通过交互地编辑图表，生成适应修改的新子图，以保持在原始剧本和约束之内。我们使用GPT-4和GRIM在不同的Contextual约束下生成分支剧本，以示其可用性。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Chain-of-Thought-Prompting"><a href="#Contrastive-Chain-of-Thought-Prompting" class="headerlink" title="Contrastive Chain-of-Thought Prompting"></a>Contrastive Chain-of-Thought Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09277">http://arxiv.org/abs/2311.09277</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/contrastive-cot">https://github.com/damo-nlp-sg/contrastive-cot</a></li>
<li>paper_authors: Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, Lidong Bing</li>
<li>for: 提高语音模型的逻辑推理能力</li>
<li>methods: 使用对比链式思维法，提供有效和无效示例来引导语音模型进行步骤式推理，并提高推理错误的检测能力</li>
<li>results: 在逻辑推理benchmark上实现了对比链式思维法的普适性，并且提高了语音模型的逻辑推理能力<details>
<summary>Abstract</summary>
Despite the success of chain of thought in enhancing language model reasoning, the underlying process remains less well understood. Although logically sound reasoning appears inherently crucial for chain of thought, prior studies surprisingly reveal minimal impact when using invalid demonstrations instead. Furthermore, the conventional chain of thought does not inform language models on what mistakes to avoid, which potentially leads to more errors. Hence, inspired by how humans can learn from both positive and negative examples, we propose contrastive chain of thought to enhance language model reasoning. Compared to the conventional chain of thought, our approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes. To improve generalization, we introduce an automatic method to construct contrastive demonstrations. Our experiments on reasoning benchmarks demonstrate that contrastive chain of thought can serve as a general enhancement of chain-of-thought prompting.
</details>
<details>
<summary>摘要</summary>
尽管链式思考已经提高了语言模型的逻辑能力，但它们的下面逻辑过程仍然尚不够了解。尽管逻辑正确性看起来是链式思考的核心，但是前一 Studies 显示，使用无效示例时的影响却是很小。此外， convent ional 链式思考没有告诉语言模型哪些错误需要避免，这可能会导致更多的错误。因此，我们提出了受人类学习 FROM positive 和 negative 示例的灵感，并将其应用于语言模型的逻辑reasoning。与 convent ional 链式思考相比，我们的approach 可以提供有效和无效的逻辑示例，以引导模型 step-by-step 进行逻辑reasoning，同时减少逻辑错误。为了提高泛化能力，我们提出了一种自动生成对照示例的方法。我们的实验表明，对于逻辑测试 benchmark 来说，对照链式思考可以作为一种通用的逻辑促进。
</details></li>
</ul>
<hr>
<h2 id="TableLlama-Towards-Open-Large-Generalist-Models-for-Tables"><a href="#TableLlama-Towards-Open-Large-Generalist-Models-for-Tables" class="headerlink" title="TableLlama: Towards Open Large Generalist Models for Tables"></a>TableLlama: Towards Open Large Generalist Models for Tables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09206">http://arxiv.org/abs/2311.09206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun<br>for:This paper aims to develop open-source large language models (LLMs) as generalists for a diversity of table-based tasks.methods:The authors construct a new dataset called TableInstruct, which includes a variety of realistic tables and tasks, and fine-tune an open-source model (TableLlama) with LongLoRA to address the long context challenge.results:TableLlama achieves comparable or better performance than the state-of-the-art (SOTA) on 7 out of 8 in-domain tasks, and shows 6-48 absolute point gains on 6 out-of-domain datasets, demonstrating the model’s generalizability.Here’s the simplified Chinese text:for: 这篇论文目标是开发大量自然语言模型（LLM），用于多种表格任务。methods: 作者们构建了一个新的表格数据集（TableInstruct），包括多种真实的表格和任务，并使用LongLoRA进行微调，以解决长上下文挑战。results: TableLlama在7个域内任务中达到或超过当前最佳性能（SOTA），并在6个对于任务特定设计的数据集上显示6-48个绝对点提升，示出模型的通用性。<details>
<summary>Abstract</summary>
Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model's generalizability. We will open-source our dataset and trained model to boost future work on developing open generalist models for tables.
</details>
<details>
<summary>摘要</summary>
semi-structured 表格是普遍存在的。有很多任务旨在自动理解、增强和查询表格。现有的方法frequently需要预训练表格或特定的模型体系设计，或者只能处理特定的表格类型，或者假设表格和任务过于简单。这篇论文是开发大型自然语言模型（LLM）为表格任务的第一步。为此，我们构建了一个名为 TableInstruct 的新数据集，用于训练和评估 LLM。我们还开发了首个开源的通用模型 для表格，即 TableLlama，通过长Context挑战 LongLoRA 来练习。我们在域 Setting 和 out-of-domain Setting 下进行了实验。在 7 个域 Setting 中，TableLlama 在每个任务上与 SOTA 相比，具有相似或更好的性能，即使后者具有特定的设计。在 6 个 out-of-domain 数据集上，它与基模型相比，获得了 6-48 绝对点胜。这表明训练在 TableInstruct 上增强了模型的通用性。我们将开源我们的数据集和训练模型，以便将来的开发工作。
</details></li>
</ul>
<hr>
<h2 id="When-Is-Multilinguality-a-Curse-Language-Modeling-for-250-High-and-Low-Resource-Languages"><a href="#When-Is-Multilinguality-a-Curse-Language-Modeling-for-250-High-and-Low-Resource-Languages" class="headerlink" title="When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages"></a>When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09205">http://arxiv.org/abs/2311.09205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tylerachang/curse-of-multilinguality">https://github.com/tylerachang/curse-of-multilinguality</a></li>
<li>paper_authors: Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen</li>
<li>for: 本研究旨在 investigate the effects of multilinguality on language modeling performance in individual languages.</li>
<li>methods: 研究人员采用了10,000个单语言和多语言语言模型，对250种语言进行预训练，包括一些未得到尝试的语言家族。研究人员评估了预训练语言模型性能如何随着单语言数据集大小、多语言数据集大小、预训练语言相似性和模型大小（最大45M参数）变化。</li>
<li>results: 结果表明，在一定程度上添加多语言数据可以提高低资源语言模型性能，与单语言数据集大小相似的效果。这些改进取决于预训练语言之间的语法相似性，而非词汇重叠。然而，高资源语言在多语言预训练场景下 consistently poor performance。随着数据集大小的增加，添加多语言数据开始对所有语言类型的性能产生负面影响， probable due to limited model capacity（“多语言诅咒”）。这些结果表明，大规模多语言预训练可能不适用于任何语言，但更专注的模型可以显著提高性能。<details>
<summary>Abstract</summary>
Multilingual language models are widely used to extend NLP systems to low-resource languages. However, concrete evidence for the effects of multilinguality on language modeling performance in individual languages remains scarce. Here, we pre-train over 10,000 monolingual and multilingual language models for over 250 languages, including multiple language families that are under-studied in NLP. We assess how language modeling performance in each language varies as a function of (1) monolingual dataset size, (2) added multilingual dataset size, (3) linguistic similarity of the added languages, and (4) model size (up to 45M parameters). We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33%. Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap. However, high-resource languages consistently perform worse in multilingual pre-training scenarios. As dataset sizes increase, adding multilingual data begins to hurt performance for both low-resource and high-resource languages, likely due to limited model capacity (the "curse of multilinguality"). These results suggest that massively multilingual pre-training may not be optimal for any languages involved, but that more targeted models can significantly improve performance.
</details>
<details>
<summary>摘要</summary>
多语言语言模型广泛应用于扩展NLP系统到低资源语言。然而，具体的证据表明多语言性对语言模型性能在个体语言中的影响仍然缺乏。在这里，我们预训练了10,000多语言和多语言语言模型，涵盖250种语言，包括一些在NLP中未得到足够研究的语言家族。我们评估了在每种语言中语言模型性能如何随(1)单语言数据集大小、(2)添加多语言数据集大小、(3)添加语言家族之间的语法相似性和(4)模型大小（最多4500万参数）而变化。我们发现，在一定程度上，添加多语言数据可以提高低资源语言模型性能，类似于增加低资源数据集大小，最多提高33%。提高取决于添加的多语言数据中的语法相似性，而词汇重叠也具有有限的效果。然而，高资源语言在多语言预训练场景下一直表现差。随着数据集大小的增加，添加多语言数据开始对低资源语言和高资源语言都有负面影响，可能是因为模型容量的限制（“多语言性的咒”）。这些结果表明，大规模多语言预训练可能不适合任何语言，但更加注重的模型可以很大程度提高性能。
</details></li>
</ul>
<hr>
<h2 id="Structural-Priming-Demonstrates-Abstract-Grammatical-Representations-in-Multilingual-Language-Models"><a href="#Structural-Priming-Demonstrates-Abstract-Grammatical-Representations-in-Multilingual-Language-Models" class="headerlink" title="Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models"></a>Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09194">http://arxiv.org/abs/2311.09194</a></li>
<li>repo_url: None</li>
<li>paper_authors: James A. Michaelov, Catherine Arnett, Tyler A. Chang, Benjamin K. Bergen</li>
<li>for: 这paper主要研究了大语言模型中的 grammatical knowledge 的抽象性，以及这种抽象性如何在不同语言之间具有共同的特征。</li>
<li>methods: 研究者使用了大语言模型，并对其进行了跨语言和单语言的结构预期测试，以评估模型中 grammatical knowledge 的抽象性。</li>
<li>results: 研究者发现，大语言模型中的 grammatical knowledge 具有抽象性，并且可以在不同语言之间共同影响文本生成。此外，模型的表现和人类实验结果相似，证明了模型中 grammatical knowledge 的抽象性和人类的语言知识之间的相似性。<details>
<summary>Abstract</summary>
Abstract grammatical knowledge - of parts of speech and grammatical patterns - is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language, evidence of abstraction is even more compelling from crosslingual structural priming, where use of a syntactic structure in one language primes an analogous structure in another language. We measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages, and four monolingual structural priming experiments in three non-English languages. We find evidence for abstract monolingual and crosslingual grammatical representations in the models that function similarly to those found in humans. These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages.
</details>
<details>
<summary>摘要</summary>
抽象语法知识 - parts of speech和 grammatical patterns - 是人类语言能力的关键。但是大语言模型中的语法知识如何抽象？我们通过跨语言结构启发来证明语法抽象的存在。在人类文献中，跨语言结构启发提供了吸引人的证据，其中一句语言与之前一句语言的同一个语法结构相似时，对于语言的处理和生成而言更加容易。由于单一语言的干扰因素存在，跨语言结构启发的证据更加吸引人，其中一种语言中的语法结构在另一种语言中引起相似的结构。我们使用大语言模型测量跨语言结构启发，与人类实验结果相比，来自八种cross语言实验和四种单语言实验。我们发现大语言模型中的语法表示存在抽象的特征，与人类中的语法表示类似，并且可以影响不同语言中的文本生成。这些结果表明，多语言语言模型中的语法表示不仅在不同语言之间具有相似性，而且可以 causally 影响不同语言中的文本生成。
</details></li>
</ul>
<hr>
<h2 id="PsyEval-A-Comprehensive-Large-Language-Model-Evaluation-Benchmark-for-Mental-Health"><a href="#PsyEval-A-Comprehensive-Large-Language-Model-Evaluation-Benchmark-for-Mental-Health" class="headerlink" title="PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health"></a>PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09189">http://arxiv.org/abs/2311.09189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoan Jin, Siyuan Chen, Mengyue Wu, Kenny Q. Zhu</li>
<li>for: 本研究旨在提供大语言模型（LLM）在心理健康领域的评价标准，填补当前该领域中LLM的评价缺乏的空白。</li>
<li>methods: 本研究使用了六个子任务，涵盖三个维度，系统地评价了八种高级LLM在心理健康领域的能力。</li>
<li>results: 实验结果表明，当前的LLM在心理健康领域仍有很大的提升空间，同时也揭示了未来模型优化的潜在方向。<details>
<summary>Abstract</summary>
Recently, there has been a growing interest in utilizing large language models (LLMs) in mental health research, with studies showcasing their remarkable capabilities, such as disease detection. However, there is currently a lack of a comprehensive benchmark for evaluating the capability of LLMs in this domain. Therefore, we address this gap by introducing the first comprehensive benchmark tailored to the unique characteristics of the mental health domain. This benchmark encompasses a total of six sub-tasks, covering three dimensions, to systematically assess the capabilities of LLMs in the realm of mental health. We have designed corresponding concise prompts for each sub-task. And we comprehensively evaluate a total of eight advanced LLMs using our benchmark. Experiment results not only demonstrate significant room for improvement in current LLMs concerning mental health but also unveil potential directions for future model optimization.
</details>
<details>
<summary>摘要</summary>
近些时间，大语言模型（LLM）在心理健康研究中的应用受到了越来越多的关注，研究显示其惊人的能力，如疾病检测。然而，当前心理健康领域中LLM的能力的全面评估 benchmark 缺乏。因此，我们填补这一空白，引入了心理健康领域的首个全面性 benchmark。这个 benchmark 涵盖了六个子任务，覆盖三个维度，用于系统地评估 LLM 在心理健康领域的能力。我们设计了每个子任务的简洁提示。我们对八个高级 LLM 进行了全面评估，实验结果表明，现有 LLM 在心理健康领域还有很大的提升空间，同时也揭示了未来模型优化的潜在方向。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Generation-and-Evaluation-Capabilities-of-Large-Language-Models-for-Instruction-Controllable-Summarization"><a href="#Benchmarking-Generation-and-Evaluation-Capabilities-of-Large-Language-Models-for-Instruction-Controllable-Summarization" class="headerlink" title="Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization"></a>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09184">http://arxiv.org/abs/2311.09184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yale-nlp/instrusum">https://github.com/yale-nlp/instrusum</a></li>
<li>paper_authors: Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan</li>
<li>for: 这篇论文旨在研究语言模型（LLM）在更复杂的概要任务设定下的性能，特别是在指定概要特征的情况下。</li>
<li>methods: 作者使用了一组指定文章和自然语言需求来训练 LLM，并对5种基于 LLM 的概要系统进行人工评估。以及使用了4种评估协议和11种 LLM 进行自动评估。</li>
<li>results: 研究发现，对于 LLM 来说，制定概要任务仍然是一项具有挑战性的任务，因为（1）所有评估的 LLM 都会在概要中作出错误和其他类型的错误;（2）所有基于 LLM 的评估方法无法与人类标注员 achieve strong alignment 的质量评估标准;（3）不同的 LLM 在概要生成和评估方面表现出了大的性能差距。<details>
<summary>Abstract</summary>
While large language models (LLMs) already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for the desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluation on 5 LLM-based summarization systems. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods in total. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) all LLM-based evaluation methods cannot achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation. We make our collected benchmark, InstruSum, publicly available to facilitate future research in this direction.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在标准化的摘要 benchmark 上 дости得了强大的表现，但它们在更加复杂的摘要任务设定中的表现更少研究。因此，我们将 LLM  benchmark 在 instruction 控制的文本摘要任务中，其中模型输入包括来源文章和自然语言需求摘要特性。为此，我们为这个任务设定了评估对象 dataset 并进行了人类评估 five LLM 摘要系统。然后，我们对 LLM 自动评估的 benchmark 进行了四种评估协议和 eleven LLM 的评估，共计 forty 种评估方法。我们的研究发现， instruction 控制的文本摘要仍然是 LLM 的挑战，因为：1. 所有 LLM 评估都会在摘要中发生实际和其他类型的错误。2. 所有 LLM 基于的评估方法无法与人类评估者在评估候选摘要质量上实现强大的一致。3. 不同的 LLM 在摘要生成和评估中表现出大的性能差异。我们将我们收集的 benchmark， InstruSum，公开供后续研究使用。
</details></li>
</ul>
<hr>
<h2 id="ContraDoc-Understanding-Self-Contradictions-in-Documents-with-Large-Language-Models"><a href="#ContraDoc-Understanding-Self-Contradictions-in-Documents-with-Large-Language-Models" class="headerlink" title="ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models"></a>ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09182">http://arxiv.org/abs/2311.09182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jierui Li, Vipul Raheja, Dhruv Kumar</li>
<li>for: 研究长文档自相矛盾的能力</li>
<li>methods: 使用四个现有的开源和商业可用的大语言模型（GPT3.5、GPT4、PaLM2、LLaMAv2）进行分析</li>
<li>results: GPT4表现最好，可以超越人类的表现，但 ainda有问题，尤其是需要更多的细节和 контекст的自相矛盾。<details>
<summary>Abstract</summary>
In recent times, large language models (LLMs) have shown impressive performance on various document-level tasks such as document classification, summarization, and question-answering. However, research on understanding their capabilities on the task of self-contradictions in long documents has been very limited. In this work, we introduce ContraDoc, the first human-annotated dataset to study self-contradictions in long documents across multiple domains, varying document lengths, self-contradictions types, and scope. We then analyze the current capabilities of four state-of-the-art open-source and commercially available LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4 performs the best and can outperform humans on this task, we find that it is still unreliable and struggles with self-contradictions that require more nuance and context. We release the dataset and all the code associated with the experiments.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:近期，大型语言模型（LLM）在各种文档级任务上表现出色，如文档分类、概要和问答等。然而，关于 LLMS 在自相矛盾任务上的能力研究却非常有限。在这项工作中，我们介绍了 ContraDoc，首个人类标注的长文档自相矛盾数据集，覆盖多个领域、文档长度、自相矛盾类型和范围。然后，我们分析了四种当前最佳的开源和商业可用 LLM：GPT3.5、GPT4、PaLM2 和 LLaMAv2 在这个数据集上的表现。虽然 GPT4 表现最佳，可以超越人类的表现，但我们发现它在自相矛盾需要更多的细节和上下文时表现不可靠。我们发布了数据集和相关实验代码。
</details></li>
</ul>
<hr>
<h2 id="PEARL-Personalizing-Large-Language-Model-Writing-Assistants-with-Generation-Calibrated-Retrievers"><a href="#PEARL-Personalizing-Large-Language-Model-Writing-Assistants-with-Generation-Calibrated-Retrievers" class="headerlink" title="PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers"></a>PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09180">http://arxiv.org/abs/2311.09180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, Tara Safavi</li>
<li>for: 提高写作和沟通质量和效率</li>
<li>methods: 使用搜索引擎增强大型语言模型的写作助手，以提供个性化的写作 Output</li>
<li>results: 实现了个性化的社交媒体和Reddit评论生成，并且可以用作写作质量预测和优化低质量生成<details>
<summary>Abstract</summary>
Powerful large language models have facilitated the development of writing assistants that promise to significantly improve the quality and efficiency of composition and communication. However, a barrier to effective assistance is the lack of personalization in LLM outputs to the author's communication style and specialized knowledge. In this paper, we address this challenge by proposing PEARL, a retrieval-augmented LLM writing assistant personalized with a generation-calibrated retriever. Our retriever is trained to select historic user-authored documents for prompt augmentation, such that they are likely to best personalize LLM generations for a user request. We propose two key novelties for training our retriever: 1) A training data selection method that identifies user requests likely to benefit from personalization and documents that provide that benefit; and 2) A scale-calibrating KL-divergence objective that ensures that our retriever closely tracks the benefit of a document for personalized generation. We demonstrate the effectiveness of PEARL in generating personalized workplace social media posts and Reddit comments. Finally, we showcase the potential of a generation-calibrated retriever to double as a performance predictor and further improve low-quality generations via LLM chaining.
</details>
<details>
<summary>摘要</summary>
强大的大语言模型已经促进了写作助手的开发，这些助手承诺可以大幅提高写作和交流的质量和效率。然而，一个阻碍效果的问题是LLM输出的不具有作者的通信风格和专业知识的个性化。在这篇论文中，我们解决这个挑战，提出了一种基于检索的LLM写作助手，即PEARL。我们的检索器通过选择历史用户自动生成的文档来补充请求，以便最大化LLM生成的个性化效果。我们提出了两项关键新特点来训练我们的检索器：1）一种用于选择可以从属性的请求和文档，以便提高个性化效果；2）一种托管KL散度目标，确保检索器与个性化生成的效果相似。我们示出PEARL在生成工作室社交媒体帖子和Reddit评论中的个性化效果。 finally，我们展示了一种基于生成检索的性能预测器，可以进一步改善低质量生成的LLM链。
</details></li>
</ul>
<hr>
<h2 id="SiRA-Sparse-Mixture-of-Low-Rank-Adaptation"><a href="#SiRA-Sparse-Mixture-of-Low-Rank-Adaptation" class="headerlink" title="SiRA: Sparse Mixture of Low Rank Adaptation"></a>SiRA: Sparse Mixture of Low Rank Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09179">http://arxiv.org/abs/2311.09179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han Lu, Canoee Liu, Liangchen Luo, Jindong Chen, Lei Meng</li>
<li>For: 这篇论文的目的是提出一种简单且高效的推导大型自然语言模型（LoRA），以适应下游任务。* Methods: 这篇论文使用了一种称为“简单混合”的方法，即将LoRA的所有参数都用来适应特定任务。然而，这种方法在实验中被证明是不太有效的。因此，这篇论文提出了一种新的方法，即SiRA，它使用了简单的混合来提高LoRA的性能。* Results: 这篇论文的实验结果显示，SiRA比LoRA和其他混合专家方法在不同单任务和多任务设置中表现更好。<details>
<summary>Abstract</summary>
Parameter Efficient Tuning has been an prominent approach to adapt the Large Language Model to downstream tasks. Most previous works considers adding the dense trainable parameters, where all parameters are used to adapt certain task. We found this less effective empirically using the example of LoRA that introducing more trainable parameters does not help. Motivated by this we investigate the importance of leveraging "sparse" computation and propose SiRA: sparse mixture of low rank adaption. SiRA leverages the Sparse Mixture of Expert(SMoE) to boost the performance of LoRA. Specifically it enforces the top $k$ experts routing with a capacity limit restricting the maximum number of tokens each expert can process. We propose a novel and simple expert dropout on top of gating network to reduce the over-fitting issue. Through extensive experiments, we verify SiRA performs better than LoRA and other mixture of expert approaches across different single tasks and multitask settings.
</details>
<details>
<summary>摘要</summary>
“对大型语言模型进行高效调整”（Parameter Efficient Tuning）是一种广泛使用的方法，以适应下游任务。大多数前一些研究假设添加密集可训练参数，其中所有参数都用于适应特定任务。但我们在LoRA的例子中发现，增加更多的可训练参数并不对Empirical Effective。驱动于此，我们展开了对“稀疏”计算的重要性，并提出了SiRA：稀疏混合低阶适应。SiRA利用Sparse Mixture of Expert（SMoE）来提高LoRA的性能。具体来说，它强制 Top $k$ 专家路由具有容量限制，限制每个专家处理的 Token 最多数量。我们还提出了一种新的简单的专家排除方法，以降低过滤问题。通过广泛的实验，我们证明SiRA在不同的单任务和多任务设置中表现比LoRA和其他混合专家方法更好。
</details></li>
</ul>
<hr>
<h2 id="CLEAN-EVAL-Clean-Evaluation-on-Contaminated-Large-Language-Models"><a href="#CLEAN-EVAL-Clean-Evaluation-on-Contaminated-Large-Language-Models" class="headerlink" title="CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models"></a>CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09154">http://arxiv.org/abs/2311.09154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu Hu, Yiran Wei, Rui Wang, Hongyuan Lu</li>
<li>for: 评估大语言模型（LLM）的真实能力，因为数据污染导致评估结果不准确。</li>
<li>methods: 提出了一种新的评估方法——Clean-Eval，通过抽象和反编译污染数据，生成表达相同意义的不同表面形式的样本，并使用语义检测器筛选低质量样本，选择最佳样本基于BLEURT分数。</li>
<li>results: Clean-Eval可以准确地评估污染后的LLM表现，并且可以生成新的评估标准。实验表明，Clean-Eval在几个不同场景下能够重新评估污染后的LLM表现。<details>
<summary>Abstract</summary>
We are currently in an era of fierce competition among various large language models (LLMs) continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination, and it wastes dozens of time and effort for researchers and engineers to download and try those contaminated models. To save our precious time, we propose a novel and useful method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter the generated low-quality samples to narrow down this candidate set. The best candidate is finally selected from this set based on the BLEURT score. According to human assessment, this best candidate is semantically similar to the original contamination data but expressed differently. All candidates can form a new benchmark to evaluate the model. Our experiments illustrate that Clean-Eval substantially restores the actual evaluation results on contaminated LLMs under both few-shot learning and fine-tuning scenarios.
</details>
<details>
<summary>摘要</summary>
现在是一个大型语言模型（LLM）不断推进指标性能的竞争时代。然而，评估这些 LLM 的真实能力已成为一个困难和重要的问题，因为可能存在数据污染，这会浪费研究人员和工程师们很多时间和努力来下载和尝试这些污染的模型。为了保留我们的宝贵时间，我们提出了一种新的方法，即 Clean-Eval，它解决了数据污染问题，并评估 LLM 在更加干净的环境下。Clean-Eval 使用一个 LLM 来重新表述和反翻污染数据，生成表达同一个意义，但表现在不同的表面形式中的候选集。然后，一个Semantic Detector 被用来筛选生成的低质量样本，从而缩小候选集。最后，根据 BLEURT 分数，从候选集中选择最佳候选。根据人工评估，这个最佳候选与原始污染数据具有相同的含义，但表现在不同的表面形式中。所有候选都可以组成一个新的评估标准。我们的实验表明，Clean-Eval 可以减少在污染 LLM 下的实际评估结果的损失，在少量学习和微调学习场景下。
</details></li>
</ul>
<hr>
<h2 id="Grounding-or-Guesswork-Large-Language-Models-are-Presumptive-Grounders"><a href="#Grounding-or-Guesswork-Large-Language-Models-are-Presumptive-Grounders" class="headerlink" title="Grounding or Guesswork? Large Language Models are Presumptive Grounders"></a>Grounding or Guesswork? Large Language Models are Presumptive Grounders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09144">http://arxiv.org/abs/2311.09144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Shaikh, Kristina Gligorić, Ashna Khetan, Matthias Gerstgrasser, Diyi Yang, Dan Jurafsky</li>
<li>for: 这个论文主要是研究人工智能和人的对话中的共同基础建立方面。</li>
<li>methods: 这个论文使用了一些对话动作（如 clarify 和 acknowledge）来研究人工智能是否可以成功地建立共同基础。</li>
<li>results: 研究发现现有的大型自然语言处理模型（LLMs）在建立共同基础时偏向假设共同基础的存在，而不使用对话动作来确认共同基础。<details>
<summary>Abstract</summary>
Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). In domains like teaching and emotional support, carefully constructing grounding prevents misunderstanding. However, it is unclear whether large language models (LLMs) leverage these dialogue acts in constructing common ground. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLMs use these grounding acts, simulating them taking turns from several dialogue datasets, and comparing the results to humans. We find that current LLMs are presumptive grounders, biased towards assuming common ground without using grounding acts. To understand the roots of this behavior, we examine the role of instruction tuning and reinforcement learning with human feedback (RLHF), finding that RLHF leads to less grounding. Altogether, our work highlights the need for more research investigating grounding in human-AI interaction.
</details>
<details>
<summary>摘要</summary>
有效的对话需要共同基础：参与者之间的共同理解。然而，这些共同基础不会自然地出现在对话中。说话者和听者需要共同工作，以确定并构建共同基础，并避免错解。人类在教学和情感支持等领域中，会考虑地构建共同基础，以避免错解。然而，是否LLMs会利用对话措施来建立共同基础，这是一个未知的问题。为了解决这个问题，我们筛选了一组共同基础动作，并提出了相应的评价指标。我们研究了LLMs是否使用这些共同基础动作，通过对多个对话集进行模拟，并与人类对话进行比较。我们发现，当前的LLMs具有假设共同基础的倾向，即不使用共同基础动作来建立共同基础。为了了解这种行为的起源，我们研究了指导调整和人类反馈学习（RLHF）的作用，发现RLHF会减少共同基础的使用。总之，我们的工作强调了人机交互中共同基础的研究的重要性。
</details></li>
</ul>
<hr>
<h2 id="RRescue-Ranking-LLM-Responses-to-Enhance-Reasoning-Over-Context"><a href="#RRescue-Ranking-LLM-Responses-to-Enhance-Reasoning-Over-Context" class="headerlink" title="RRescue: Ranking LLM Responses to Enhance Reasoning Over Context"></a>RRescue: Ranking LLM Responses to Enhance Reasoning Over Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09136">http://arxiv.org/abs/2311.09136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikun Wang, Rui Zheng, Haoming Li, Qi Zhang, Tao Gui, Fei Liu</li>
<li>for: 这篇论文目的是提高大语言模型（LLM）的上下文理解能力，以便更好地应用于响应生成。</li>
<li>methods: 该论文提出了一种新的应用ranking指标来优化LLM的上下文理解，包括人工标注、规则函数和模型蒸馏等方法。</li>
<li>results: 通过使用这种新的应用ranking指标，论文的实验结果表明LLM的上下文理解能力得到了改进，并且在最新的多文档问答 dataset 上达到了更高的成绩。<details>
<summary>Abstract</summary>
Effectively using a given context is paramount for large language models. A context window can include task specifications, retrieved documents, previous conversations, and even model self-reflections, functioning similarly to episodic memory. While efforts are being made to expand the context window, studies indicate that LLMs do not use their context optimally for response generation. In this paper, we present a novel approach to optimize LLMs using ranking metrics, which teaches LLMs to rank a collection of contextually-grounded candidate responses. Rather than a traditional full ordering, we advocate for a partial ordering. This is because achieving consensus on the perfect order for system responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be acquired through human labelers, heuristic functions, or model distillation. We test our system's improved contextual understanding using the latest benchmarks, including a new multi-document question answering dataset. We conduct ablation studies to understand crucial factors, such as how to gather candidate responses, determine their most suitable order, and balance supervised fine-tuning with ranking metrics. Our approach, named RRescue, suggests a promising avenue for enhancing LLMs' contextual understanding via response ranking.
</details>
<details>
<summary>摘要</summary>
使用给定的上下文是大语言模型的关键。上下文窗口可以包括任务规范、检索到的文档、先前的对话和模型自我反思，功能类似于 episodic memory。然而，研究表明，LLMs 不使用上下文最优。在这篇论文中，我们提出了一种新的方法来优化 LLMs，使其可以 ranks 上下文化的候选答案集。而不是传统的全局排序，我们建议使用 partial ordering。这是因为实现完美的上下文排序可能是困难的。我们的 partial ordering 更加稳定， less sensitive to noise，可以通过人工标注、规则函数或模型泛化来获得。我们测试了我们的系统在最新的benchmark中的改进上下文理解，包括一个新的多文档问答数据集。我们进行了ablation study来理解关键因素，如如何收集候选答案、确定其最佳顺序和平衡upervised fine-tuning with ranking metrics。我们的方法，名为 RRescue，建议一种可能提高 LLMs 的上下文理解的 Avenues。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Neural-Machine-Translation-Models-Human-Feedback-in-Training-and-Inference"><a href="#Aligning-Neural-Machine-Translation-Models-Human-Feedback-in-Training-and-Inference" class="headerlink" title="Aligning Neural Machine Translation Models: Human Feedback in Training and Inference"></a>Aligning Neural Machine Translation Models: Human Feedback in Training and Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09132">http://arxiv.org/abs/2311.09132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel Moura Ramos, Patrick Fernandes, António Farinhas, André F. T. Martins</li>
<li>for: 这种技术是为了提高语言模型生成的文本质量，使其更加类似于人类生成的文本。</li>
<li>methods: 这种技术使用人类反馈来训练抽象模型，并在语言模型的训练过程中使用它来改进模型的性能。</li>
<li>results: 这个研究表明，通过integratingquality metrics into the MT pipeline可以提高翻译质量，并且 combining RL training with reranking techniques可以实现显著的提高。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) is a recent technique to improve the quality of the text generated by a language model, making it closer to what humans would generate. A core ingredient in RLHF's success in aligning and improving large language models (LLMs) is its reward model, trained using human feedback on model outputs. In machine translation (MT), where metrics trained from human annotations can readily be used as reward models, recent methods using minimum Bayes risk decoding and reranking have succeeded in improving the final quality of translation. In this study, we comprehensively explore and compare techniques for integrating quality metrics as reward models into the MT pipeline. This includes using the reward model for data filtering, during the training phase through RL, and at inference time by employing reranking techniques, and we assess the effects of combining these in a unified approach. Our experimental results, conducted across multiple translation tasks, underscore the crucial role of effective data filtering, based on estimated quality, in harnessing the full potential of RL in enhancing MT quality. Furthermore, our findings demonstrate the effectiveness of combining RL training with reranking techniques, showcasing substantial improvements in translation quality.
</details>
<details>
<summary>摘要</summary>
人类反馈学习（RLHF）是一种现代技术，用于改进语言模型生成的文本质量，使其更加类似于人类生成的文本。RLHF的成功一大部分归功于其奖励模型，通过人类反馈来训练。在机器翻译（MT）领域，可以 readily使用人类标注数据来训练奖励模型，最近的方法使用最小极大 Bayes风险解oding和重新排序技术，已经在提高翻译质量方面取得了 significanthy进步。本研究旨在全面探讨和比较在MT管道中 integrateQuality metrics as reward models的技术。这包括使用奖励模型来筛选数据，在训练阶段通过RL进行训练，以及在推理时使用重新排序技术，并评估这些技术的组合效果。我们的实验结果，在多个翻译任务上进行了检验，强调了有效的数据筛选，基于估计的质量，在RL中激发全部的潜力，提高翻译质量。此外，我们的发现还证明了RL训练与重新排序技术的组合可以实现显著的提高翻译质量。
</details></li>
</ul>
<hr>
<h2 id="Social-Meme-ing-Measuring-Linguistic-Variation-in-Memes"><a href="#Social-Meme-ing-Measuring-Linguistic-Variation-in-Memes" class="headerlink" title="Social Meme-ing: Measuring Linguistic Variation in Memes"></a>Social Meme-ing: Measuring Linguistic Variation in Memes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09130">http://arxiv.org/abs/2311.09130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naitian/semantic-memes">https://github.com/naitian/semantic-memes</a></li>
<li>paper_authors: Naitian Zhou, David Jurgens, David Bamman</li>
<li>for: This paper explores sociolinguistic variation in memes, using a computational pipeline to cluster individual instances of memes into templates and semantic variables.</li>
<li>methods: The paper uses a multimodal approach, taking advantage of the visual templates and text in memes to analyze their semantic function.</li>
<li>results: The study discovers meaningful social variation in meme usage between subreddits, and patterns of meme innovation and acculturation within these communities align with previous findings on written language.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文探索了社会语言变化在抖音中，使用计算机方法将个体抖音划分成模板和语义变量。</li>
<li>methods: 论文采用多Modal方法，利用抖音的视觉模板和文本来分析其语义功能。</li>
<li>results: 研究发现抖音在社区之间存在社会意义的变化，并发现抖音创新和同化在这些社区中与过去语言变化的趋势相吻合。<details>
<summary>Abstract</summary>
Much work in the space of NLP has used computational methods to explore sociolinguistic variation in text. In this paper, we argue that memes, as multimodal forms of language comprised of visual templates and text, also exhibit meaningful social variation. We construct a computational pipeline to cluster individual instances of memes into templates and semantic variables, taking advantage of their multimodal structure in doing so. We apply this method to a large collection of meme images from Reddit and make available the resulting \textsc{SemanticMemes} dataset of 3.8M images clustered by their semantic function. We use these clusters to analyze linguistic variation in memes, discovering not only that socially meaningful variation in meme usage exists between subreddits, but that patterns of meme innovation and acculturation within these communities align with previous findings on written language.
</details>
<details>
<summary>摘要</summary>
很多NLP领域的研究使用计算方法来探索社会语言变化。在这篇论文中，我们 argue That memes，作为Multimodal的语言形式，也存在意义的社会变化。我们构建了一个计算管道来将个体照片分为模板和semantic variable，利用其 Multimodal结构来做此。我们将这些方法应用于Reddit上的大量meme图片集合，并将结果作为\textsc{SemanticMemes}数据集，包含3.8M个图片，按Semantic功能进行分组。我们使用这些分组来分析Memes的语言变化，发现不仅在subreddit之间存在社会意义的变化，而且在这些社区中，meme创新和同化的模式与前面的文本语言发现相似。
</details></li>
</ul>
<hr>
<h2 id="Universal-NER-A-Gold-Standard-Multilingual-Named-Entity-Recognition-Benchmark"><a href="#Universal-NER-A-Gold-Standard-Multilingual-Named-Entity-Recognition-Benchmark" class="headerlink" title="Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark"></a>Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09122">http://arxiv.org/abs/2311.09122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Mayhew, Terra Blevins, Shuheng Liu, Marek Šuppa, Hila Gonen, Joseph Marvin Imperial, Börje F. Karlsson, Peiqin Lin, Nikola Ljubešić, LJ Miranda, Barbara Plank, Arij Riabi, Yuval Pinter</li>
<li>for: 这 paper 的目的是开发一个开源、社区驱动的项目，以创建多种语言的高质量名实体识别（NER）标准 benchmark。</li>
<li>methods: 这 paper 使用了多种语言的名实体识别数据集，并对其进行了cross-lingual consistent的标注。</li>
<li>results: 这 paper 提供了多种语言的名实体识别数据集，并在不同的语言和学习环境中提供了初步的模型基线。<details>
<summary>Abstract</summary>
We introduce Universal NER (UNER), an open, community-driven project to develop gold-standard NER benchmarks in many languages. The overarching goal of UNER is to provide high-quality, cross-lingually consistent annotations to facilitate and standardize multilingual NER research. UNER v1 contains 18 datasets annotated with named entities in a cross-lingual consistent schema across 12 diverse languages. In this paper, we detail the dataset creation and composition of UNER; we also provide initial modeling baselines on both in-language and cross-lingual learning settings. We release the data, code, and fitted models to the public.
</details>
<details>
<summary>摘要</summary>
我们介绍Universal NER（UNER），一个开放、社区驱动的项目，旨在开发多种语言的高标准命名实体识别标准。UNER v1包含18个数据集，每个数据集包含多种语言的命名实体，以跨语言一致的方式进行标注。在这篇论文中，我们详细介绍了UNER数据集的创建和组合，以及在本语言和跨语言学习环境中的初步模型基线。我们将数据、代码和适应模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="R-Spin-Efficient-Speaker-and-Noise-invariant-Representation-Learning-with-Acoustic-Pieces"><a href="#R-Spin-Efficient-Speaker-and-Noise-invariant-Representation-Learning-with-Acoustic-Pieces" class="headerlink" title="R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces"></a>R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09117">http://arxiv.org/abs/2311.09117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng-Jui Chang, James Glass</li>
<li>for: 这篇论文是为了提出一种数据效率的自主supervised fine-tuning框架，以获得 speaker和噪声不变的语音表示。</li>
<li>methods: 该框架使用 speaker-invariant clustering (Spin) 学习精确的音频单元，并通过预测音频片段来强化内容表示。</li>
<li>results: 相比之前的状态艺术方法，R-Spin 可以在严重扭曲语音场景下获得更好的表示性，同时减少了计算资源的使用量，达到 12 倍的提升。<details>
<summary>Abstract</summary>
This paper introduces Robust Spin (R-Spin), a data-efficient self-supervised fine-tuning framework for speaker and noise-invariant speech representations by learning discrete acoustic units with speaker-invariant clustering (Spin). R-Spin resolves Spin's issues and enhances content representations by learning to predict acoustic pieces. R-Spin offers a 12X reduction in computational resources compared to previous state-of-the-art methods while outperforming them in severely distorted speech scenarios. This paper provides detailed analyses to show how discrete units contribute to speech encoder training and improving robustness in diverse acoustic environments.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文介绍了一种名为Robust Spin（R-Spin）的数据精简自我超越框架，用于实现Speaker和噪声不变的语音表示。R-Spin解决了Spin的问题，并提高了语音表示的内容。R-Spin可以在严重扭曲的语音enario中具有12倍的计算资源减少，并在前一代方法中出perform。这篇论文还提供了详细的分析，以显示дискреTE Units在语音编码器训练中的贡献和提高 robustness在多种听频环境中。
</details></li>
</ul>
<hr>
<h2 id="“We-Demand-Justice-”-Towards-Grounding-Political-Text-in-Social-Context"><a href="#“We-Demand-Justice-”-Towards-Grounding-Political-Text-in-Social-Context" class="headerlink" title="“We Demand Justice!”: Towards Grounding Political Text in Social Context"></a>“We Demand Justice!”: Towards Grounding Political Text in Social Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09106">http://arxiv.org/abs/2311.09106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajkumar Pujari, Chengfei Wu, Dan Goldwasser</li>
<li>for: 本研究旨在 Computational setting中理解ambiguous statements的语言含义，并将其与现实世界相关的实体、行为和态度关联。</li>
<li>methods: 本研究使用了两个具有挑战性的 datasets，需要理解文本的现实世界上下文才能解决 Effectively。此外，还开发了基于现有 ‘Discourse Contextualization Framework’ 和 ‘Political Actor Representation’ 模型的更结构化基elines。</li>
<li>results: 本研究通过对基elines的比较分析，提供了更深入的理解社会语言理解挑战的信息。<details>
<summary>Abstract</summary>
Social media discourse from US politicians frequently consists of 'seemingly similar language used by opposing sides of the political spectrum'. But often, it translates to starkly contrasting real-world actions. For instance, "We need to keep our students safe from mass shootings" may signal either "arming teachers to stop the shooter" or "banning guns to reduce mass shootings" depending on who says it and their political stance on the issue. In this paper, we define and characterize the context that is required to fully understand such ambiguous statements in a computational setting and ground them in real-world entities, actions, and attitudes. To that end, we propose two challenging datasets that require an understanding of the real-world context of the text to be solved effectively. We benchmark these datasets against baselines built upon large pre-trained models such as BERT, RoBERTa, GPT-3, etc. Additionally, we develop and benchmark more structured baselines building upon existing 'Discourse Contextualization Framework' and 'Political Actor Representation' models. We perform analysis of the datasets and baseline predictions to obtain further insights into the pragmatic language understanding challenges posed by the proposed social grounding tasks.
</details>
<details>
<summary>摘要</summary>
社交媒体讨论由美国政客们频繁使用"看起来相似的语言",但实际上它们可能表达出极其不同的现实世界行动。例如，"我们需要保护学生免受大规模枪击"可能表示"武装教师以阻止射手"或"禁止枪支以减少大规模枪击"，这取决于说话人的政治立场。在这篇论文中，我们定义和描述了 Computational Setting中需要完全理解这些抽象语言的上下文，并将其固定到现实世界实体、行动和态度。为此，我们提出了两个复杂的数据集，需要理解文本的现实世界上下文才能解决 effectively。我们对这些数据集进行了基线测试，并开发了基于现有"Discourse Contextualization Framework"和"Political Actor Representation"模型的更结构化基线。我们对数据集和基线预测进行分析，以获得更深入的语言理解挑战的进一步洞察。
</details></li>
</ul>
<hr>
<h2 id="MAVEN-Arg-Completing-the-Puzzle-of-All-in-One-Event-Understanding-Dataset-with-Event-Argument-Annotation"><a href="#MAVEN-Arg-Completing-the-Puzzle-of-All-in-One-Event-Understanding-Dataset-with-Event-Argument-Annotation" class="headerlink" title="MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation"></a>MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09105">http://arxiv.org/abs/2311.09105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaozhi Wang, Hao Peng, Yong Guan, Kaisheng Zeng, Jianhui Chen, Lei Hou, Xu Han, Yankai Lin, Zhiyuan Liu, Ruobing Xie, Jie Zhou, Juanzi Li</li>
<li>for: This paper is written for the purpose of introducing a new dataset, MAVEN-Arg, which supports event understanding tasks such as event detection, event argument extraction, and event relation extraction.</li>
<li>methods: The paper uses a large-scale dataset, MAVEN-Arg, which is augmented with event argument annotations, to support the development and evaluation of event understanding models.</li>
<li>results: The paper reports that MAVEN-Arg is a challenging dataset for both fine-tuned EAE models and proprietary large language models (LLMs), and demonstrates the potential benefits of an all-in-one dataset for future event prediction applications using LLMs.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍一个新的数据集MAVEN-Arg，该数据集支持事件理解任务，包括事件检测、事件Argument提取和事件关系提取。</li>
<li>methods: 这篇论文使用了一个大规模的数据集MAVEN-Arg，该数据集包括事件Argument的注释，以支持事件理解模型的发展和评估。</li>
<li>results: 论文表明，MAVEN-Arg是对于both fine-tuned EAE模型和专有大语言模型（LLMs）来说是一个具有挑战性的数据集，并demonstrates该数据集的可能性用于未来事件预测应用程序。<details>
<summary>Abstract</summary>
Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and our code can be obtained from https://github.com/THU-KEG/MAVEN-Argument.
</details>
<details>
<summary>摘要</summary>
Understanding events in texts is a core goal of natural language understanding, which involves detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the challenges of annotation, a large-scale dataset covering the full process of event understanding has been lacking. In this paper, we introduce MAVEN-Arg, which adds event argument annotations to the MAVEN datasets, creating the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained through laborious human annotation; (3) exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments at the document level. Experiments show that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and our code can be obtained from <https://github.com/THU-KEG/MAVEN-Argument>.
</details></li>
</ul>
<hr>
<h2 id="Defending-Large-Language-Models-Against-Jailbreaking-Attacks-Through-Goal-Prioritization"><a href="#Defending-Large-Language-Models-Against-Jailbreaking-Attacks-Through-Goal-Prioritization" class="headerlink" title="Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization"></a>Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09096">http://arxiv.org/abs/2311.09096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-coai/jailbreakdefense_goalpriority">https://github.com/thu-coai/jailbreakdefense_goalpriority</a></li>
<li>paper_authors: Zhexin Zhang, Junxiao Yang, Pei Ke, Minlie Huang</li>
<li>for: 这个论文的目的是提出一种对销害攻击的防御方法，帮助保护大语言模型（LLMs）免受销害攻击。</li>
<li>methods: 该论文使用了目标优先级的思想来防御销害攻击，在训练和推理阶段都实现了目标优先级的 integrating。</li>
<li>results: 该论文的实验结果表明，通过在推理阶段实现目标优先级，可以减少销害攻击的成功率，并且不会影响大语言模型的总体性能。此外，通过在训练阶段实现目标优先级，可以更好地防止销害攻击。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) continue to advance in their capabilities, yet this progress is accompanied by a growing array of safety risks. While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of exploration into defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the inherent conflict between the goals of being helpful and ensuring safety. To counter jailbreaking attacks, we propose to integrate goal prioritization at both training and inference stages. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to 2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising general performance. Furthermore, integrating the concept of goal prioritization into the training phase reduces the ASR from 71.0% to 6.6% for LLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half, decreasing it from 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks. We hope our work could contribute to the comprehension of jailbreaking attacks and defenses, and shed light on the relationship between LLMs' capability and safety. Our code will be available at \url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）继续进步，但同时也涉及到一系列的安全隐患。虽然有很多研究利用 LLM 的弱点进行攻击，但对于防御攻击的研究却受到了相对的少量关注。我们指出，在 LLM 中进行干预时存在一个重要的因素，即帮助和安全之间的矛盾。为了防御攻击，我们提议在训练和执行阶段都进行目标优先级化。在执行阶段实现目标优先级化后，可以显著减少攻击成功率（ASR），从66.4%降低至2.0% для ChatGPT，从68.2%降低至19.4% для Vicuna-33B，无需妥协总体性能。此外，在训练阶段 integrate 目标优先级化也可以降低 ASR 至6.6% для LLama2-13B。即使在没有攻击样本的情况下，我们的方法仍可以减少 ASR 的一半，从71.0%降低至34.0%。此外，我们的研究还发现，强大的 LLM 面临更大的安全隐患，但同时它们也拥有更大的防御能力。我们希望我们的工作可以对攻击和防御之间的关系提供更深入的理解，并为 LLM 的安全做出贡献。我们的代码将在 GitHub 上公开，请参考 \url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.
</details></li>
</ul>
<hr>
<h2 id="Social-Bias-Probing-Fairness-Benchmarking-for-Language-Models"><a href="#Social-Bias-Probing-Fairness-Benchmarking-for-Language-Models" class="headerlink" title="Social Bias Probing: Fairness Benchmarking for Language Models"></a>Social Bias Probing: Fairness Benchmarking for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09090">http://arxiv.org/abs/2311.09090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Marchiori Manerba, Karolina Stańczak, Riccardo Guidotti, Isabelle Augenstein</li>
<li>for: 本研究旨在探讨语言模型中社会偏见的问题，并提出了一种新的探测方法。</li>
<li>methods: 本研究使用了一种新的词语混杂度-based fairness分数，并收集了一个大规模的探测数据集，以分析语言模型的总体协会以及社会分类、标签和刻板印象方面的偏见。</li>
<li>results: 研究发现，语言模型中的偏见更加复杂，大型模型变体具有更高度的偏见，并且发现不同religion表达的人群在所有模型中产生最大的不同待遇。<details>
<summary>Abstract</summary>
Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms. While the impact of these biases has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, offering a constrained view of the nature of societal biases within language models. In this paper, we propose an original framework for probing language models for societal biases. We collect a probing dataset to analyze language models' general associations, as well as along the axes of societal categories, identities, and stereotypes. To this end, we leverage a novel perplexity-based fairness score. We curate a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections, expanding to a variety of different identities and stereotypes. When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged. In agreement with recent findings, we find that larger model variants exhibit a higher degree of bias. Moreover, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经显示出了多种社会偏见，这可能导致下游危害。虽然这些偏见的影响已经被认可，但先前的偏见评估方法受限于小型数据集和二元关联测试，这只能提供社会偏见在语言模型中的压缩视图。在这篇论文中，我们提出了一种原创的语言模型偏见探测框架。我们收集了一个探测数据集，以分析语言模型的通用关联以及社会分类、标签和刻板印象的方向。为此，我们利用了一种新的折衣率基准公平分数。我们创建了一个大规模的比较数据集，以解决现有公平集的缺点和限制，扩展到不同的标签和刻板印象。与先前的工作比较，我们发现了更多的偏见在语言模型中，特别是更大的模型变体更加偏见。此外，我们发现了不同的宗教标签表达时，所有模型中的最大差异。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Self-Disclosures-of-Use-Misuse-and-Addiction-in-Community-based-Social-Media-Posts"><a href="#Identifying-Self-Disclosures-of-Use-Misuse-and-Addiction-in-Community-based-Social-Media-Posts" class="headerlink" title="Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts"></a>Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09066">http://arxiv.org/abs/2311.09066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghao Yang, Tuhin Chakrabarty, Karli R Hochstatter, Melissa N Slavin, Nabila El-Bassel, Smaranda Muresan</li>
<li>For: The paper aims to develop a tool to identify at-risk patients with opioid use disorder by analyzing community-based social media platforms like Reddit.* Methods: The authors use a corpus of 2500 opioid-related posts from various subreddits to annotate span-level extractive explanations and evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting.* Results: The authors find that using explanations during modeling leads to a significant boost in classification accuracy, demonstrating their beneficial role in a high-stakes domain such as studying the opioid use disorder continuum.Here are the three points in Simplified Chinese text:* For: 这篇论文目标是开发一种用于识别患有酒精使用障碍的患者的工具，通过分析社区基于的Reddit社交媒体平台上的自透露。* Methods: 作者使用2500篇关于酒精的Reddit帖子，并对它们进行分析和注释，以评估一些当前最佳的模型在不同的超级vised、几个shot和零shot设置下的表现。* Results: 作者发现，在高度关键的领域中，使用解释时期的模型会导致识别酒精使用障碍的准确率显著提高，这demonstrates解释的有利role在研究酒精使用障碍continuum中。<details>
<summary>Abstract</summary>
In the last decade, the United States has lost more than 500,000 people from an overdose involving prescription and illicit opioids (https://www.cdc.gov/drugoverdose/epidemic/index.html) making it a national public health emergency (USDHHS, 2017). To more effectively prevent unintentional opioid overdoses, medical practitioners require robust and timely tools that can effectively identify at-risk patients. Community-based social media platforms such as Reddit allow self-disclosure for users to discuss otherwise sensitive drug-related behaviors, often acting as indicators for opioid use disorder. Towards this, we present a moderate size corpus of 2500 opioid-related posts from various subreddits spanning 6 different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of opioid use disorder is highly contextual and challenging. However, we find that using explanations during modeling leads to a significant boost in classification accuracy demonstrating their beneficial role in a high-stakes domain such as studying the opioid use disorder continuum. The dataset will be made available for research on Github in the formal version.
</details>
<details>
<summary>摘要</summary>
在过去一个十年，美国已经失去了超过500,000名人因为吸毒过量，其中包括药物和黑市药品（https://www.cdc.gov/drugoverdose/epidemic/index.html），这使得这成为一个国家紧急公共卫生问题（USDHHS, 2017）。为了更好地预防意外的毒品过量，医疗专业人员需要强大和时间相对的工具，以有效地识别有风险的病人。社区基础的社交媒体平台如Reddit，allow users to disclose themselves and discuss sensitive drug-related behaviors, often serving as indicators of opioid use disorder. 为此，我们提供了一个 Moderate-sized corpus of 2500 opioid-related posts from various subreddits spanning 6 different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of opioid use disorder is highly contextual and challenging. However, we find that using explanations during modeling leads to a significant boost in classification accuracy, demonstrating their beneficial role in a high-stakes domain such as studying the opioid use disorder continuum. The dataset will be made available for research on Github in the formal version.
</details></li>
</ul>
<hr>
<h2 id="Do-Localization-Methods-Actually-Localize-Memorized-Data-in-LLMs"><a href="#Do-Localization-Methods-Actually-Localize-Memorized-Data-in-LLMs" class="headerlink" title="Do Localization Methods Actually Localize Memorized Data in LLMs?"></a>Do Localization Methods Actually Localize Memorized Data in LLMs?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09060">http://arxiv.org/abs/2311.09060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting-Yun Chang, Jesse Thomason, Robin Jia</li>
<li>for: 本研究旨在找到LLMs中记忆某个序列的小量神经元。</li>
<li>methods: 本文使用两个benchmark方法评估本地化方法的效果，一个是INJ Benchmark，通过在小量神经元中插入新信息来测试本地化方法的准确性；另一个是DEL Benchmark，通过测试dropout located neurons是否会使模型忘记记忆的序列。</li>
<li>results: 本研究发现，五种本地化方法在两个benchmark上都达到了一定的成果，尤其是使用减少方法时，能够准确地本地化记忆。但是，所identified神经元不一定是特定的一个记忆序列的特征。<details>
<summary>Abstract</summary>
Large language models (LLMs) can memorize many pretrained sequences verbatim. This paper studies if we can locate a small set of neurons in LLMs responsible for memorizing a given sequence. While the concept of localization is often mentioned in prior work, methods for localization have never been systematically and directly evaluated; we address this with two benchmarking approaches. In our INJ Benchmark, we actively inject a piece of new information into a small subset of LLM weights and measure whether localization methods can identify these "ground truth" weights. In the DEL Benchmark, we study localization of pretrained data that LLMs have already memorized; while this setting lacks ground truth, we can still evaluate localization by measuring whether dropping out located neurons erases a memorized sequence from the model. We evaluate five localization methods on our two benchmarks, and both show similar rankings. All methods exhibit promising localization ability, especially for pruning-based methods, though the neurons they identify are not necessarily specific to a single memorized sequence.
</details>
<details>
<summary>摘要</summary>
In our INJ Benchmark, we actively inject a piece of new information into a small subset of LLM weights and measure whether localization methods can identify these "ground truth" weights. In the DEL Benchmark, we study localization of pre-trained data that LLMs have already memorized; while this setting lacks ground truth, we can still evaluate localization by measuring whether dropping out located neurons erases a memorized sequence from the model.We evaluate five localization methods on our two benchmarks, and all show promising localization ability, especially for pruning-based methods. However, the neurons they identify are not necessarily specific to a single memorized sequence.
</details></li>
</ul>
<hr>
<h2 id="GRASP-A-novel-benchmark-for-evaluating-language-GRounding-And-Situated-Physics-understanding-in-multimodal-language-models"><a href="#GRASP-A-novel-benchmark-for-evaluating-language-GRounding-And-Situated-Physics-understanding-in-multimodal-language-models" class="headerlink" title="GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models"></a>GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09048">http://arxiv.org/abs/2311.09048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, Elia Bruni</li>
<li>for: 评估视频基于多modal语言模型的语言固定和物理理解能力</li>
<li>methods: 使用Unity simulations进行两 tier评估，包括语言固定和直觉物理理解能力</li>
<li>results: 现有多modal语言模型具有语言固定和直觉物理理解缺陷，GRASP benchmark可以帮助监测未来模型的进步<details>
<summary>Abstract</summary>
This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The initial level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of 'Intuitive Physics' principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in current models' language grounding and intuitive physics. These identified limitations underline the importance of benchmarks like GRASP to monitor the progress of future models in developing these competencies.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了GRASP，一个新的评估语言固定和物理理解能力的视频基于多模态大语言模型（LLM）的benchmark。这种评估方式通过Unity simulate层次结构来实现。第一层测试语言固定的能力，通过将简单的文本描述与视觉信息相关联。第二层测试模型的物理理解能力，包括物体永久性和连续性。此外，我们还使用GRASP评估多种当前领先的多模态LLM。我们的评估发现当前模型的语言固定和直觉物理存在显著的缺陷。这些缺陷证明了GRASP这种benchmark的重要性，以便监测未来模型的发展。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Potential-of-Large-Language-Models-in-Computational-Argumentation"><a href="#Exploring-the-Potential-of-Large-Language-Models-in-Computational-Argumentation" class="headerlink" title="Exploring the Potential of Large Language Models in Computational Argumentation"></a>Exploring the Potential of Large Language Models in Computational Argumentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09022">http://arxiv.org/abs/2311.09022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/llm-argumentation">https://github.com/damo-nlp-sg/llm-argumentation</a></li>
<li>paper_authors: Guizhen Chen, Liying Cheng, Luu Anh Tuan, Lidong Bing</li>
<li>for: 本研究旨在评估大语言模型（LLMs）在计算辩论领域中的表现，包括零学习和少学习 Setting下的能力。</li>
<li>methods: 本研究使用了多种任务，包括辩论挖掘和辩论生成，以评估LLMs的表现。我们还提供了一个新的对话生成测试集，以全面评估LLMs的综合性能。</li>
<li>results: 实验结果显示LLMs在大多数任务中表现出色，证明它们在计算辩论领域具有remarkable能力。然而，我们也注意到了评估计算辩论的限制，并提供了未来研究的建议。<details>
<summary>Abstract</summary>
Computational argumentation has become an essential tool in various fields, including artificial intelligence, law, and public policy. It is an emerging research field in natural language processing (NLP) that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated strong abilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on various computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under zero-shot and few-shot settings within the realm of computational argumentation. We organize existing tasks into 6 main classes and standardise the format of 14 open-sourced datasets. In addition, we present a new benchmark dataset on counter speech generation, that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of these datasets, demonstrating their capabilities in the field of argumentation. We also highlight the limitations in evaluating computational argumentation and provide suggestions for future research directions in this field.
</details>
<details>
<summary>摘要</summary>
计算辩论已成为不同领域的重要工具，包括人工智能、法律和公共政策。这是自然语言处理（NLP）的一个快速发展的研究领域，吸引了更多的关注。研究计算辩论主要涉及两类任务：辩论挖掘和辩论生成。由于大语言模型（LLMs）在理解上下文和生成自然语言方面表现出色，因此值得评估LLMs在不同计算辩论任务中的表现。本工作计划在零 shot和几 shot设置下评估 ChatGPT、Flan 模型和 LLaMA2 模型在计算辩论任务中的表现。我们将现有任务分为 6 个主要类型，并标准化 datasets 的格式。此外，我们还提供了一个新的benchmark dataset，用于全面评估 LLMS 在辩论挖掘和辩论生成任务中的综合表现。广泛的实验表明 LLMS 在大多数 datasets 中表现出色，证明它们在辩论领域的能力。我们还提出了计算辩论评估的限制和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Task-oriented-Dialogue-A-Survey-of-Tasks-Methods-and-Future-Directions"><a href="#End-to-end-Task-oriented-Dialogue-A-Survey-of-Tasks-Methods-and-Future-Directions" class="headerlink" title="End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions"></a>End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09008">http://arxiv.org/abs/2311.09008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Libo Qin, Wenbo Pan, Qiguang Chen, Lizi Liao, Zhou Yu, Yue Zhang, Wanxiang Che, Min Li</li>
<li>for: 这篇论文主要针对的是End-to-end task-oriented dialogue（EToD）研究领域，旨在提供一份系统性的综述，涵盖该领域的所有方法和最新趋势。</li>
<li>methods: 该论文使用了大量的深度神经网络模型，特别是使用大型预训练模型，以实现EToD研究中的显著进步。</li>
<li>results: 该论文提供了一个综述EToD研究领域的新趋势和前沿领域，并提供了一个公共网站（<a target="_blank" rel="noopener" href="https://etods.net/">https://etods.net/</a>），以便EToD研究人员直接访问最新的进步。<details>
<summary>Abstract</summary>
End-to-end task-oriented dialogue (EToD) can directly generate responses in an end-to-end fashion without modular training, which attracts escalating popularity. The advancement of deep neural networks, especially the successful use of large pre-trained models, has further led to significant progress in EToD research in recent years. In this paper, we present a thorough review and provide a unified perspective to summarize existing approaches as well as recent trends to advance the development of EToD research. The contributions of this paper can be summarized: (1) \textbf{\textit{First survey}: to our knowledge, we take the first step to present a thorough survey of this research field; (2) \textbf{\textit{New taxonomy}: we first introduce a unified perspective for EToD, including (i) \textit{Modularly EToD} and (ii) \textit{Fully EToD}; (3) \textbf{\textit{New Frontiers}: we discuss some potential frontier areas as well as the corresponding challenges, hoping to spur breakthrough research in EToD field; (4) \textbf{\textit{Abundant resources}: we build a public website\footnote{We collect the related papers, baseline projects, and leaderboards for the community at \url{https://etods.net/}.}, where EToD researchers could directly access the recent progress. We hope this work can serve as a thorough reference for the EToD research community.
</details>
<details>
<summary>摘要</summary>
END-TO-END TASK-ORIENTED DIALOGUE (EToD) 可以直接生成响应，无需模块化训练，这已经在过去几年中吸引了越来越多的关注。深度神经网络的发展，特别是大型预训练模型的成功使用，导致了 EToD 研究领域的 significiant progress。在这篇论文中，我们提供了一份系统性的回顾和总结，旨在推动 EToD 研究的发展。本文的贡献包括：1. 首次调查：我们知道的所有文献中，我们是第一个进行这项研究的全面调查。2. 新的分类：我们首先引入了 EToD 的统一视角，包括（i）模块化 EToD 和（ii）完全 EToD。3. 新的前iers：我们讨论了一些潜在的前沿领域，以及相应的挑战，希望能够促进 EToD 领域的突破性研究。4. 充沛的资源：我们建立了一个公共网站（https://etods.net/）， где EToD 研究人员可以直接访问最新的进展。我们希望这份工作能够成为 EToD 研究社区的参考。
</details></li>
</ul>
<hr>
<h2 id="Data-Similarity-is-Not-Enough-to-Explain-Language-Model-Performance"><a href="#Data-Similarity-is-Not-Enough-to-Explain-Language-Model-Performance" class="headerlink" title="Data Similarity is Not Enough to Explain Language Model Performance"></a>Data Similarity is Not Enough to Explain Language Model Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09006">http://arxiv.org/abs/2311.09006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gyauney/data-similarity-is-not-enough">https://github.com/gyauney/data-similarity-is-not-enough</a></li>
<li>paper_authors: Gregory Yauney, Emily Reif, David Mimno</li>
<li>for: 这个论文旨在探讨语言模型在多种下游任务中的高性能是如何实现的？</li>
<li>methods: 该论文使用了多种同构和示例特定的相似度度量（嵌入-, 字符-和模型基于的）来衡量语言模型在下游任务中的性能。</li>
<li>results: 在多语言任务中，相似度度量与语言模型的性能显著相关，但在其他benchmark中，相似度度量与准确率或者even每个相似度度量之间没有相关性。这表明下游任务和预训练数据之间的关系比较复杂。<details>
<summary>Abstract</summary>
Large language models achieve high performance on many but not all downstream tasks. The interaction between pretraining data and task data is commonly assumed to determine this variance: a task with data that is more similar to a model's pretraining data is assumed to be easier for that model. We test whether distributional and example-specific similarity measures (embedding-, token- and model-based) correlate with language model performance through a large-scale comparison of the Pile and C4 pretraining datasets with downstream benchmarks. Similarity correlates with performance for multilingual datasets, but in other benchmarks, we surprisingly find that similarity metrics are not correlated with accuracy or even each other. This suggests that the relationship between pretraining data and downstream tasks is more complex than often assumed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Factcheck-GPT-End-to-End-Fine-Grained-Document-Level-Fact-Checking-and-Correction-of-LLM-Output"><a href="#Factcheck-GPT-End-to-End-Fine-Grained-Document-Level-Fact-Checking-and-Correction-of-LLM-Output" class="headerlink" title="Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output"></a>Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09000">http://arxiv.org/abs/2311.09000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuxiaw/factcheck-gpt">https://github.com/yuxiaw/factcheck-gpt</a></li>
<li>paper_authors: Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, Preslav Nakov</li>
<li>for: 这篇论文旨在提供一个涵盖所有阶段的Annotation scheme来验证大型自然语言模型（LLM）生成的回答的实现方式，以便确保其精度和可靠性。</li>
<li>methods: 这篇论文使用了一个多阶段的Annotation scheme，让评分者能够为LLM生成的回答提供细化的标签，以捕捉回答中的可靠性和事实不一致之处。此外，这篇论文还开发了一个Annotation tool来加速评分过程，并且可以自动插入证据等自动结果。</li>
<li>results: 根据初步实验结果，FactTool、FactScore和Perplexity.ai等工具在验证false claims方面的性能不太理想，其F1分数为0.53。这篇论文提供了一个开放领域的文档级实验库，并且提供了一个网站供下载Annotation tool和代码。<details>
<summary>Abstract</summary>
The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present a holistic end-to-end solution for annotating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels concerning the verifiability and factual inconsistencies found in LLM outputs. We design and build an annotation tool to speed up the labelling procedure and ease the workload of raters. It allows flexible incorporation of automatic results in any stage, e.g. automatically-retrieved evidence. We further construct an open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document. Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify false claims with the best F1=0.53. Annotation tool, benchmark and code are available at https://github.com/yuxiaw/Factcheck-GPT.
</details>
<details>
<summary>摘要</summary>
通过大语言模型（LLM）在各种实际应用中的普及，需要验证其输出的事实准确性的机制。在这项工作中，我们提出了一种涵盖所有阶段的综合答案，用于标注 LLG 生成的响应中的事实准确性，并设计了一个多Stage annotation scheme，以生成细化的标签，包括 LLG 输出中的可靠性和事实不一致。我们设计了一个用于加速标注过程的标注工具，并且可以自动 incorporate 任何阶段的自动结果，例如自动检索到的证据。我们还构建了一个开放领域文档级别的事实准确性标准吗，包括声明、句子和文档三个级别。我们的初步实验表明，FacTool、FactScore 和 Perplexity.ai 在标识false声明方面的最佳 F1 值为 0.53。我们的标注工具、标准吗和代码可以在 GitHub 上获取。
</details></li>
</ul>
<hr>
<h2 id="SentAlign-Accurate-and-Scalable-Sentence-Alignment"><a href="#SentAlign-Accurate-and-Scalable-Sentence-Alignment" class="headerlink" title="SentAlign: Accurate and Scalable Sentence Alignment"></a>SentAlign: Accurate and Scalable Sentence Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08982">http://arxiv.org/abs/2311.08982</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/steinst/sentalign">https://github.com/steinst/sentalign</a></li>
<li>paper_authors: Steinþór Steingrímsson, Hrafn Loftsson, Andy Way</li>
<li>for: 该论文设计了一个高精度的句子对齐工具，用于处理非常大的平行文档对。</li>
<li>methods: 该算法使用用户定义的参数，采用分治分解方法对大量句子进行对齐，并使用LaBSE双语句子表示来评分。</li>
<li>results:  SentAlign在德语-法语和英语-冰岛语两个评估集上表现出色，并在下游机器翻译任务中表现更好。<details>
<summary>Abstract</summary>
We present SentAlign, an accurate sentence alignment tool designed to handle very large parallel document pairs. Given user-defined parameters, the alignment algorithm evaluates all possible alignment paths in fairly large documents of thousands of sentences and uses a divide-and-conquer approach to align documents containing tens of thousands of sentences. The scoring function is based on LaBSE bilingual sentence representations. SentAlign outperforms five other sentence alignment tools when evaluated on two different evaluation sets, German-French and English-Icelandic, and on a downstream machine translation task.
</details>
<details>
<summary>摘要</summary>
我们介绍了 SentAlign，一款精度很高的句子对齐工具，可以处理非常大的平行文档对。通过用户定义的参数，对齐算法会评估所有可能的对齐路径，并使用分治分解方法对文档中的千余句进行对齐。对齐函数基于 LaBSE 双语句表示。 SentAlign 在两个不同的评估集上（德语-法语和英语-冰岛语）和下游机器翻译任务上表现出色，超越了五个其他句子对齐工具。
</details></li>
</ul>
<hr>
<h2 id="Speculative-Contrastive-Decoding"><a href="#Speculative-Contrastive-Decoding" class="headerlink" title="Speculative Contrastive Decoding"></a>Speculative Contrastive Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08981">http://arxiv.org/abs/2311.08981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou</li>
<li>for: 提高大语言模型（LLM）的推断质量和速度</li>
<li>methods: 使用 amateur models 预测专家模型的生成，并使用自然冲突来优化推断结果</li>
<li>results: 实验结果表明，使用 Speculative Contrastive Decoding（SCD）可以达到类似的加速因子，同时提高推断质量，并且可以减少计算资源的消耗<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown extraordinary performance in various language tasks, but high computational requirements hinder their widespread deployment. Speculative decoding, which uses amateur models to predict the generation of expert models, has been proposed as a way to accelerate LLM inference. However, speculative decoding focuses on acceleration instead of making the best use of the token distribution from amateur models. We proposed Speculative Contrastive Decoding (SCD), an accelerated decoding method leveraging the natural contrast between expert and amateur models in speculative decoding. Comprehensive evaluations on four benchmarks show that SCD can achieve similar acceleration factors as speculative decoding while further improving the generation quality as the contrastive decoding. The analysis of token probabilities further demonstrates the compatibility between speculative and contrastive decoding. Overall, SCD provides an effective approach to enhance the decoding quality of LLMs while saving computational resources.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Large-scale-Deep-Biasing-with-Phoneme-Features-and-Text-only-Data-in-Streaming-Transducer"><a href="#Improving-Large-scale-Deep-Biasing-with-Phoneme-Features-and-Text-only-Data-in-Streaming-Transducer" class="headerlink" title="Improving Large-scale Deep Biasing with Phoneme Features and Text-only Data in Streaming Transducer"></a>Improving Large-scale Deep Biasing with Phoneme Features and Text-only Data in Streaming Transducer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08966">http://arxiv.org/abs/2311.08966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Qiu, Lu Huang, Boyu Li, Jun Zhang, Lu Lu, Zejun Ma</li>
<li>for: 提高流式自动语音识别（ASR）中罕见词或上下文实体的识别性能。</li>
<li>methods: combine拟音和文本信息，以 distinguishing 同音或同字符序列的词语。</li>
<li>results: 在LibriSpeech corpus上，提出的方法实现了不同规模和偏好列表的罕见词错误率的国际先进性。<details>
<summary>Abstract</summary>
Deep biasing for the Transducer can improve the recognition performance of rare words or contextual entities, which is essential in practical applications, especially for streaming Automatic Speech Recognition (ASR). However, deep biasing with large-scale rare words remains challenging, as the performance drops significantly when more distractors exist and there are words with similar grapheme sequences in the bias list. In this paper, we combine the phoneme and textual information of rare words in Transducers to distinguish words with similar pronunciation or spelling. Moreover, the introduction of training with text-only data containing more rare words benefits large-scale deep biasing. The experiments on the LibriSpeech corpus demonstrate that the proposed method achieves state-of-the-art performance on rare word error rate for different scales and levels of bias lists.
</details>
<details>
<summary>摘要</summary>
深层偏迁对扬声器可以改善不同语言模型中的识别性能，尤其是在实时自动语音识别（ASR）应用中。然而，深层偏迁大规模罕见词仍然存在挑战，因为性能下降很快，有许多干扰符和类似的字符序列存在偏迁列表中。在这篇论文中，我们将扬声器中罕见词的音频和文本信息结合起来，以便在同音或同字符序列时分词。此外，在训练文本只含罕见词数据时，大规模深层偏迁的训练效果也得到了改进。在 LibriSpeech 数据集上进行的实验表明，提出的方法可以在不同的规模和偏迁列表水平上取得状态的词错率最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Self-Improving-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models"><a href="#Self-Improving-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models" class="headerlink" title="Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models"></a>Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08921">http://arxiv.org/abs/2311.08921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingyu Xie, Qi Li, Yan Zhang, Zuozhu Liu, Hongwei Wang</li>
<li>for:  investigate the possibilities of pushing the boundary of zero-shot NER with LLM via a training-free self-improving strategy.</li>
<li>methods:  utilize an unlabeled corpus to stimulate the self-learning ability of LLMs on NER, and explore various strategies to select reliable samples from the self-annotated dataset as demonstrations.</li>
<li>results:  achieve an obvious performance improvement, and there might still be space for improvement via more advanced strategy for reliable entity selection.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在探索利用大型自然语言模型（LLM）进行零shotNamed Entity Recognition（NER）任务的可能性，并提出一种无需训练的自我改进策略。</li>
<li>methods: 我们利用一个无标注语料来刺激LLM的自我学习能力，并考虑了多种策略来选择自动标注数据中的可靠示例作为示例。</li>
<li>results: 我们的研究发现，使用自我改进策略可以进一步推动零shotNER的发展，并实现显著的性能提升。此外，我们还发现，简单地增加无标注语料或 iterative self-improving 并不能保证改进。<details>
<summary>Abstract</summary>
Exploring the application of powerful large language models (LLMs) on the fundamental named entity recognition (NER) task has drawn much attention recently. This work aims to investigate the possibilities of pushing the boundary of zero-shot NER with LLM via a training-free self-improving strategy. We propose a self-improving framework, which utilize an unlabeled corpus to stimulate the self-learning ability of LLMs on NER. First, we use LLM to make predictions on the unlabeled corpus and obtain the self-annotated data. Second, we explore various strategies to select reliable samples from the self-annotated dataset as demonstrations, considering the similarity, diversity and reliability of demonstrations. Finally, we conduct inference for the test query via in-context learning with the selected self-annotated demonstrations. Through comprehensive experimental analysis, our study yielded the following findings: (1) The self-improving framework further pushes the boundary of zero-shot NER with LLMs, and achieves an obvious performance improvement; (2) Iterative self-improving or naively increasing the size of unlabeled corpus does not guarantee improvements; (3) There might still be space for improvement via more advanced strategy for reliable entity selection.
</details>
<details>
<summary>摘要</summary>
First, we use LLM to make predictions on the unlabeled corpus and obtain the self-annotated data. Next, we explore various strategies to select reliable samples from the self-annotated dataset as demonstrations, taking into account the similarity, diversity, and reliability of the demonstrations. Finally, we conduct inference for the test query via in-context learning with the selected self-annotated demonstrations.Our comprehensive experimental analysis yielded the following findings:1. The self-improving framework further pushes the boundary of zero-shot NER with LLMs, achieving an obvious performance improvement.2. Iterative self-improving or simply increasing the size of the unlabeled corpus does not guarantee improvements.3. There may still be room for improvement via more advanced strategies for selecting reliable entities.
</details></li>
</ul>
<hr>
<h2 id="HELLaMA-LLaMA-based-Table-to-Text-Generation-by-Highlighting-the-Important-Evidence"><a href="#HELLaMA-LLaMA-based-Table-to-Text-Generation-by-Highlighting-the-Important-Evidence" class="headerlink" title="HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence"></a>HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08896">http://arxiv.org/abs/2311.08896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Bian, Xiaolei Qin, Wuhe Zou, Mengzuo Huang, Weidong Zhang</li>
<li>for: 这个论文主要是为了提出一种基于大语言模型的表格转文本方法，以优化表格转文本任务的性能。</li>
<li>methods: 这个方法使用了两个模块：一个表格理解器，用于从表格中提取相关的行数据，以及一个表格摘要生成器，用于基于高亮的表格生成文本。此外， authors还提出了一种搜索策略来生成表格理解 Label。</li>
<li>results: 在FetaQA和QTSumm数据集上，该方法达到了当前最佳的STATE-OF-THE-ARTResults，并且发现高亮输入表格可以显著提高模型的性能，同时提供有价值的解释性。<details>
<summary>Abstract</summary>
Large models have demonstrated significant progress across various domains, particularly in tasks related to text generation. In the domain of Table to Text, many Large Language Model (LLM)-based methods currently resort to modifying prompts to invoke public APIs, incurring potential costs and information leaks. With the advent of open-source large models, fine-tuning LLMs has become feasible. In this study, we conducted parameter-efficient fine-tuning on the LLaMA2 model. Distinguishing itself from previous fine-tuning-based table-to-text methods, our approach involves injecting reasoning information into the input by emphasizing table-specific row data. Our model consists of two modules: 1) a table reasoner that identifies relevant row evidence, and 2) a table summarizer that generates sentences based on the highlighted table. To facilitate this, we propose a search strategy to construct reasoning labels for training the table reasoner. On both the FetaQA and QTSumm datasets, our approach achieved state-of-the-art results. Additionally, we observed that highlighting input tables significantly enhances the model's performance and provides valuable interpretability.
</details>
<details>
<summary>摘要</summary>
大型模型在不同领域的任务中已经实现了显著的进步，尤其是在文本生成相关的任务中。在表格到文本领域，许多大语言模型（LLM）基于方法通常是修改提示来访问公共API，可能会导致潜在的成本和信息泄露。随着开源大型模型的出现，细化LLM成为可能。在这项研究中，我们进行了效率高的参数调整LLaMA2模型。与前期 Fine-tuning 基于表格到文本方法不同，我们的方法是通过强调表格特定的行数据来注入逻辑信息。我们的模型包括两个模块：1）表格逻辑器，用于确定相关的行证据；2）表格概要生成器，用于基于突出的表格生成句子。为了实现这一点，我们提议一种搜索策略来构建逻辑标签用于训练表格逻辑器。在FetaQA和QTSumm数据集上，我们的方法实现了状态的最佳结果。此外，我们发现高亮输入表格会显著提高模型的性能并提供有价值的解释性。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-legal-but-they-are-not-Making-the-case-for-a-powerful-LegalLLM"><a href="#Large-Language-Models-are-legal-but-they-are-not-Making-the-case-for-a-powerful-LegalLLM" class="headerlink" title="Large Language Models are legal but they are not: Making the case for a powerful LegalLLM"></a>Large Language Models are legal but they are not: Making the case for a powerful LegalLLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08890">http://arxiv.org/abs/2311.08890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanmay Jayakumar, Fauzan Farooqui, Luqman Farooqui</li>
<li>for: 本研究目的是评估通用语言模型在法律领域的性能，以及与专门为法律领域开发的模型进行比较。</li>
<li>methods: 本研究使用了三个通用语言模型（ChatGPT-20b、LLaMA-2-70b和Falcon-180b），对LEDGAR子集进行零shot测试，以评估这些模型在合同提供分类任务中的性能。</li>
<li>results: 研究发现，通用语言模型可以在大多数情况下正确地分类主题，但是它们的mic-F1&#x2F;mac-F1性能与特定于法律领域的小型模型相比，可能下降到19.2&#x2F;26.8％。这表明，为法律领域开发更强大的语言模型是有必要的。<details>
<summary>Abstract</summary>
Realizing the recent advances in Natural Language Processing (NLP) to the legal sector poses challenging problems such as extremely long sequence lengths, specialized vocabulary that is usually only understood by legal professionals, and high amounts of data imbalance. The recent surge of Large Language Models (LLMs) has begun to provide new opportunities to apply NLP in the legal domain due to their ability to handle lengthy, complex sequences. Moreover, the emergence of domain-specific LLMs has displayed extremely promising results on various tasks. In this study, we aim to quantify how general LLMs perform in comparison to legal-domain models (be it an LLM or otherwise). Specifically, we compare the zero-shot performance of three general-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR subset of the LexGLUE benchmark for contract provision classification. Although the LLMs were not explicitly trained on legal data, we observe that they are still able to classify the theme correctly in most cases. However, we find that their mic-F1/mac-F1 performance is up to 19.2/26.8\% lesser than smaller models fine-tuned on the legal domain, thus underscoring the need for more powerful legal-domain LLMs.
</details>
<details>
<summary>摘要</summary>
现在的自然语言处理（NLP）技术在法律领域中提供了挑战性的问题，例如非常长的序列长度、专业legal vocabulary和大量数据不均衡。最近的大语言模型（LLMs）已经开始为法律领域提供新的应用机会，因为它们可以处理长、复杂的序列。此外，域 específico LLMS 的出现已经在多个任务上显示出非常有 promise。在本研究中，我们想要量化一般 LLMS 与法律领域模型（LLM或其他）的比较。我们比较三个一般用途 LLMS（ChatGPT-20b、LLaMA-2-70b和Falcon-180b）在 LEDGAR 子集上的零shot性性能。尽管 LLMS 没有直接接触法律数据，但我们发现它们仍然可以正确地分类主题。然而，我们发现它们的 mic-F1/mac-F1 性能与小型法律领域模型 fine-tuned 的性能相比，下降到 19.2/26.8%，这emet underscore the need for more powerful legal-domain LLMS。
</details></li>
</ul>
<hr>
<h2 id="CLIMB-Curriculum-Learning-for-Infant-inspired-Model-Building"><a href="#CLIMB-Curriculum-Learning-for-Infant-inspired-Model-Building" class="headerlink" title="CLIMB: Curriculum Learning for Infant-inspired Model Building"></a>CLIMB: Curriculum Learning for Infant-inspired Model Building</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08886">http://arxiv.org/abs/2311.08886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Diehl Martinez, Zebulon Goriely, Hope McGovern, Christopher Davis, Andrew Caines, Paula Buttery, Lisa Beinborn</li>
<li>for: 本研究是为了提高语言模型的性能，并 investigate cognitively-motivated curriculum learning的效果。</li>
<li>methods: 本研究使用了三种不同的认知驱动的课程学习方法，包括词汇课程、数据课程和目标课程。</li>
<li>results: 研究发现，使用不同的课程学习方法可以获得一些有限的改善，但是不一致地改善所有语言测试任务。研究还发现，选择合适的模型架构和训练参数可以获得较好的改善。<details>
<summary>Abstract</summary>
We describe our team's contribution to the STRICT-SMALL track of the BabyLM Challenge. The challenge requires training a language model from scratch using only a relatively small training dataset of ten million words. We experiment with three variants of cognitively-motivated curriculum learning and analyze their effect on the performance of the model on linguistic evaluation tasks. In the vocabulary curriculum, we analyze methods for constraining the vocabulary in the early stages of training to simulate cognitively more plausible learning curves. In the data curriculum experiments, we vary the order of the training instances based on i) infant-inspired expectations and ii) the learning behavior of the model. In the objective curriculum, we explore different variations of combining the conventional masked language modeling task with a more coarse-grained word class prediction task to reinforce linguistic generalization capabilities. Our results did not yield consistent improvements over our own non-curriculum learning baseline across a range of linguistic benchmarks; however, we do find marginal gains on select tasks. Our analysis highlights key takeaways for specific combinations of tasks and settings which benefit from our proposed curricula. We moreover determine that careful selection of model architecture, and training hyper-parameters yield substantial improvements over the default baselines provided by the BabyLM challenge.
</details>
<details>
<summary>摘要</summary>
我们描述我们团队在STRICT-SMALL track上的 BabyLM 挑战中的贡献。挑战需要从头开始训练一个语言模型，只使用一个相对较小的训练集数据量为十万个单词。我们在语言评估任务中运行三种认知驱动的课程学习方法，并分析它们对模型性能的影响。在词汇课程中，我们分析了在初期训练阶段限制词汇的方法，以模拟更加认知可能的学习曲线。在数据课程实验中，我们变化了训练实例的顺序，根据i) 婴儿引发的期望和ii) 模型学习行为。在目标课程中，我们探索不同的拟合面见任务和更粗糙的词类预测任务的结合方式，以强化语言总结能力。我们的结果没有在一系列语言标准准点上得到了一致的改进，但我们发现了一些任务上的微妙改进。我们的分析强调特定任务和设置中的课程学习的优点。此外，我们发现选择模型架构和训练超参数可以提供substantial改进。
</details></li>
</ul>
<hr>
<h2 id="Enabling-Large-Language-Models-to-Learn-from-Rules"><a href="#Enabling-Large-Language-Models-to-Learn-from-Rules" class="headerlink" title="Enabling Large Language Models to Learn from Rules"></a>Enabling Large Language Models to Learn from Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08883">http://arxiv.org/abs/2311.08883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Wenkai Yang, Yankai Lin, Jie Zhou, Jirong Wen</li>
<li>for: 本研究旨在探讨使用规则来帮助大型自然语言模型（LLM）学习新的任务或知识。</li>
<li>methods: 我们提出了一种名为规则浸泡的方法，它首先使用LLM的强 Context-Aware 能力提取规则中的知识，然后将知识Explicitly 编码到LLM 参数中，通过学习内部的信号来帮助LLM 学习。</li>
<li>results: 我们的实验结果显示，使用我们的方法可以让LLM更加快速地学习新任务或知识，并且在样本数量和泛化能力方面都比例例-based 学习更高效。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown incredible performance in completing various real-world tasks. The current knowledge learning paradigm of LLMs is mainly based on learning from examples, in which LLMs learn the internal rule implicitly from a certain number of supervised examples. However, the learning paradigm may not well learn those complicated rules, especially when the training examples are limited. We are inspired that humans can learn the new tasks or knowledge in another way by learning from rules. That is, humans can grasp the new tasks or knowledge quickly and generalize well given only a detailed rule and a few optional examples. Therefore, in this paper, we aim to explore the feasibility of this new learning paradigm, which encodes the rule-based knowledge into LLMs. We propose rule distillation, which first uses the strong in-context abilities of LLMs to extract the knowledge from the textual rules and then explicitly encode the knowledge into LLMs' parameters by learning from the above in-context signals produced inside the model. Our experiments show that making LLMs learn from rules by our method is much more efficient than example-based learning in both the sample size and generalization ability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Llamas-Know-What-GPTs-Don’t-Show-Surrogate-Models-for-Confidence-Estimation"><a href="#Llamas-Know-What-GPTs-Don’t-Show-Surrogate-Models-for-Confidence-Estimation" class="headerlink" title="Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation"></a>Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08877">http://arxiv.org/abs/2311.08877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaishnavi Shrivastava, Percy Liang, Ananya Kumar</li>
<li>for: 提高LLM的可靠性，使其在问答 зада中准确地表达自己的信任度。</li>
<li>methods: 使用语言模型来描述自己的信任度，并使用一个伪装的信任模型来评估原始模型的信任度。</li>
<li>results: 使用这两种方法可以获得更高的AUC值（84.6%平均值），提高LLM的可靠性。<details>
<summary>Abstract</summary>
To maintain user trust, large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user. The standard approach of estimating confidence is to use the softmax probabilities of these models, but as of November 2023, state-of-the-art LLMs such as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We first study eliciting confidence linguistically -- asking an LLM for its confidence in its answer -- which performs reasonably (80.5% AUC on GPT-4 averaged across 12 question-answering datasets -- 7% above a random baseline) but leaves room for improvement. We then explore using a surrogate confidence model -- using a model where we do have probabilities to evaluate the original model's confidence in a given question. Surprisingly, even though these probabilities come from a different and often weaker model, this method leads to higher AUC than linguistic confidences on 9 out of 12 datasets. Our best method composing linguistic confidences and surrogate model probabilities gives state-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on GPT-4).
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:以维护用户信任，大型自然语言模型（LLMs）应该在错误的示例上显示低自信，而不是误导用户。标准的自信估计方法是使用这些模型的软条对应的概率，但在2023年11月，现场的LMMs如GPT-4和Claude-v1.3并不提供这些概率。我们首先研究用于描述自信的语言方法 -- 将LMM询问自己的答案中的自信度 -- 这perform reasonably well（GPT-4的80.5% AUC在12个问答dataset上的平均值上升7%），但还有改善的空间。我们然后探索使用代理自信模型 -- 使用一个拥有概率的模型来评估原始模型在特定问题上的自信度。 surprisingly，这些概率来自不同和常较弱的模型，这种方法在12个dataset上高于语言自信的AUC（84.6%的GPT-4平均值）。我们的最佳方法是融合语言自信和代理模型概率，得到了现场的自信估计（84.6%的GPT-4平均值）。
</details></li>
</ul>
<hr>
<h2 id="OFA-A-Framework-of-Initializing-Unseen-Subword-Embeddings-for-Efficient-Large-scale-Multilingual-Continued-Pretraining"><a href="#OFA-A-Framework-of-Initializing-Unseen-Subword-Embeddings-for-Efficient-Large-scale-Multilingual-Continued-Pretraining" class="headerlink" title="OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining"></a>OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08849">http://arxiv.org/abs/2311.08849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Liu, Peiqin Lin, Mingyang Wang, Hinrich Schütze</li>
<li>for: 本文旨在提出一种高效的多语言语模型适应方法，以提高适应多语言语言模型的效率和可行性。</li>
<li>methods: 本文提出了一种名为\textbf{\textsc{Ofa}}的框架，它通过智能初始化目标语言中未看到的字词的embeddings来适应多语言语言模型。\textsc{Ofa}使用了外部的多语言word embeddings，并将它们的对应关系注入到新的embeddings中。此外，\textsc{Ofa}还应用了矩阵因子分解，将高维的embeddings替换为两个低维的矩阵，从而减少参数的数量。</li>
<li>results: 经过广泛的实验表明，由\textsc{Ofa}初始化的模型能够高效地适应多语言语言模型，并在多种下沉任务上表现出色。此外，\textsc{Ofa}不仅加速了继续预训的整合，还提高了零Instance cross语言传递性。<details>
<summary>Abstract</summary>
Pretraining multilingual language models from scratch requires considerable computational resources and substantial training data. Therefore, a more efficient method is to adapt existing pretrained language models (PLMs) to new languages via vocabulary extension and continued pretraining. However, this method usually randomly initializes the embeddings of new subwords and introduces substantially more embedding parameters to the language model, thus weakening the efficiency. To address these issues, we propose a novel framework: \textbf{O}ne \textbf{F}or \textbf{A}ll (\textbf{\textsc{Ofa}), which wisely initializes the embeddings of unseen subwords from target languages and thus can adapt a PLM to multiple languages efficiently and effectively. \textsc{Ofa} takes advantage of external well-aligned multilingual word embeddings and injects the alignment knowledge into the new embeddings. In addition, \textsc{Ofa} applies matrix factorization and replaces the cumbersome embeddings with two lower-dimensional matrices, which significantly reduces the number of parameters while not sacrificing the performance. Through extensive experiments, we show models initialized by \textsc{Ofa} are efficient and outperform several baselines. \textsc{Ofa} not only accelerates the convergence of continued pretraining, which is friendly to a limited computation budget, but also improves the zero-shot crosslingual transfer on a wide range of downstream tasks. We make our code and models publicly available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>现有的多语言语言模型（PLM）预训练需要较大的计算资源和大量的训练数据。因此，一种更有效的方法是使用现有的 PLM 进行多语言适应，通过词库扩展和继续预训练。但这种方法通常会随机初始化目标语言中的新词表示，并添加大量的词表示参数到语言模型中，从而降低效率。为解决这些问题，我们提出了一个新的框架：\textbf{一个 для所有} (\textbf{\textsc{Ofa}), 它智能初始化目标语言中的未看过词表示，并可以快速和有效地将 PLM 适应多种语言。\textsc{Ofa} 利用外部的多语言Word embeddings 和注入对应关系知识，并应用矩阵分解，将繁琐的词表示替换为两个更低维度的矩阵，这样减少了参数的数量，而不会降低性能。经过广泛的实验，我们发现模型使用 \textsc{Ofa} 初始化的效果更好，并在多种下游任务上实现了零shot Cross-Lingual 传递。\textsc{Ofa} 不仅加速了继续预训练的整合，也提高了零shot Cross-Lingual 传递的性能，这对有限的计算预算是友好的。我们将代码和模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="Violet-A-Vision-Language-Model-for-Arabic-Image-Captioning-with-Gemini-Decoder"><a href="#Violet-A-Vision-Language-Model-for-Arabic-Image-Captioning-with-Gemini-Decoder" class="headerlink" title="Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder"></a>Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08844">http://arxiv.org/abs/2311.08844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelrahman Mohamed, Fakhraddin Alwajih, El Moatez Billah Nagoudi, Alcides Alcoba Inciarte, Muhammad Abdul-Mageed</li>
<li>for: 本研究的目的是提高阿拉伯语言图像描述的水平，提供更多的泛型语言模型。</li>
<li>methods: 本研究使用了视觉编码器和 Gemini 文本解码器，以实现视觉和语言组件的融合。同时，我们还提出了一种自动从英语数据集中获取数据的新方法。</li>
<li>results: 对于我们的评估数据集，\textit{Violet} 表现出了显著的提升，例如在我们手动标注的数据集上达到了 CIDEr 分数为 61.2，并在 Flickr8k 上提高了13个点。<details>
<summary>Abstract</summary>
Although image captioning has a vast array of applications, it has not reached its full potential in languages other than English. Arabic, for instance, although the native language of more than 400 million people, remains largely underrepresented in this area. This is due to the lack of labeled data and powerful Arabic generative models. We alleviate this issue by presenting a novel vision-language model dedicated to Arabic, dubbed \textit{Violet}. Our model is based on a vision encoder and a Gemini text decoder that maintains generation fluency while allowing fusion between the vision and language components. To train our model, we introduce a new method for automatically acquiring data from available English datasets. We also manually prepare a new dataset for evaluation. \textit{Violet} performs sizeably better than our baselines on all of our evaluation datasets. For example, it reaches a CIDEr score of $61.2$ on our manually annotated dataset and achieves an improvement of $13$ points on Flickr8k.
</details>
<details>
<summary>摘要</summary>
To train our model, we introduce a new method for automatically acquiring data from existing English datasets. Additionally, we manually prepare a new dataset for evaluation. Compared to our baselines, Violet performs significantly better on all of our evaluation datasets. For example, it achieves a CIDEr score of 61.2 on our manually annotated dataset and improves by 13 points on Flickr8k.
</details></li>
</ul>
<hr>
<h2 id="Disinformation-Capabilities-of-Large-Language-Models"><a href="#Disinformation-Capabilities-of-Large-Language-Models" class="headerlink" title="Disinformation Capabilities of Large Language Models"></a>Disinformation Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08838">http://arxiv.org/abs/2311.08838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kinit-sk/disinformation-capabilities">https://github.com/kinit-sk/disinformation-capabilities</a></li>
<li>paper_authors: Ivan Vykopal, Matúš Pikuliak, Ivan Srba, Robert Moro, Dominik Macko, Maria Bielikova</li>
<li>for: 本研究探讨了现代语言模型（LLM）可能在扩散假新闻方面的能力，以及这些能力对民主社会的影响。</li>
<li>methods: 研究使用20个假新闻narritives测试了10个LLM的能力，包括生成新闻文章的质量、与假新闻narritives的倾向度、生成安全警告等方面。</li>
<li>results: 研究发现，LLMs可以生成有力的新闻文章，并且往往同意危险的假新闻narritives。此外，检测模型也能够准确地检测LLM生成的假新闻文章。<details>
<summary>Abstract</summary>
Automated disinformation generation is often listed as one of the risks of large language models (LLMs). The theoretical ability to flood the information space with disinformation content might have dramatic consequences for democratic societies around the world. This paper presents a comprehensive study of the disinformation capabilities of the current generation of LLMs to generate false news articles in English language. In our study, we evaluated the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated several aspects of the LLMs: how well they are at generating news articles, how strongly they tend to agree or disagree with the disinformation narratives, how often they generate safety warnings, etc. We also evaluated the abilities of detection models to detect these articles as LLM-generated. We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation narratives.
</details>
<details>
<summary>摘要</summary>
自动化假信息生成是大语言模型（LLM）的风险之一。这种理论上的信息淹没能力可能对世界各地的民主社会造成巨大的影响。本文提供了现代大语言模型对英语新闻文章的假信息生成能力的全面研究。我们在这种研究中评估了10个LLM的表现，使用20个假信息 narraative。我们评估了这些LLM的新闻文章生成能力、假信息narraative的同意程度、安全警告的生成频率等方面。我们还评估了检测模型对这些文章是否能够检测出LLM生成的能力。我们结论是，LLM可以生成有力的新闻文章，并与危险的假信息narraative相符。
</details></li>
</ul>
<hr>
<h2 id="StrategyLLM-Large-Language-Models-as-Strategy-Generators-Executors-Optimizers-and-Evaluators-for-Problem-Solving"><a href="#StrategyLLM-Large-Language-Models-as-Strategy-Generators-Executors-Optimizers-and-Evaluators-for-Problem-Solving" class="headerlink" title="StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving"></a>StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08803">http://arxiv.org/abs/2311.08803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Gao, Haiyun Jiang, Deng Cai, Shuming Shi, Wai Lam</li>
<li>for: 提高Chain-of-thought（CoT）提问方法的普适性和一致性，以解决现有方法的通用性和任务级别一致性问题。</li>
<li>methods: 使用LLMs的能力，提出了一个完整的框架StrategylLM，通过自动生成、评估和选择有前途的策略来解决各种任务。</li>
<li>results: StrategylLM在13个数据集和4个复杂任务上取得了无人干预的比较优秀成绩，比基elineCoT-SC提高了39.2%到43.3%，70.3%到72.5%，51.7%到62.0%和30.0%到79.2%。<details>
<summary>Abstract</summary>
Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to tackle various tasks. The framework improves generalizability by formulating general problem-solving strategies and enhances consistency by producing consistent solutions using these strategies. StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task automatically. The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (39.2% $\rightarrow$ 43.3%), commonsense reasoning (70.3% $\rightarrow$ 72.5%), algorithmic reasoning (51.7% $\rightarrow$ 62.0%), and symbolic reasoning (30.0% $\rightarrow$ 79.2%).
</details>
<details>
<summary>摘要</summary>
现有的链式思维（CoT）提问方法受到普适性和一致性的限制，因为它们经常依赖于特定情况的解决方案，这些解决方案可能无法应用于其他情况，而且缺乏任务水平的一致性在其思维步骤中。为了解决这些限制，我们提出了一个全面的框架，名为策略LLM，充分利用LLM的能力来解决各种任务。该框架提高了普适性，通过形ulated general problem-solving策略，并增强一致性，通过使用这些策略生成一致的解决方案。策略LLM使用四个LLM基于的代理：策略生成器、执行器、优化器和评估器，这些代理共同工作，自动生成、评估和选择有投入潜力的策略，以解决给定任务。实验结果表明，策略LLM比基线CoT-SC，需要人工标注解决方案的情况下，在13个数据集上的4个挑战任务中表现出色，无需人类参与，包括数学逻辑（39.2% $\rightarrow$ 43.3%）、通情能力（70.3% $\rightarrow$ 72.5%）、算法逻辑（51.7% $\rightarrow$ 62.0%）和符号逻辑（30.0% $\rightarrow$ 79.2%）。
</details></li>
</ul>
<hr>
<h2 id="German-FinBERT-A-German-Pre-trained-Language-Model"><a href="#German-FinBERT-A-German-Pre-trained-Language-Model" class="headerlink" title="German FinBERT: A German Pre-trained Language Model"></a>German FinBERT: A German Pre-trained Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08793">http://arxiv.org/abs/2311.08793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Scherrmann</li>
<li>for: 本研究开发了一个特有的德国语言模型，名为德国金融BERT，供金融文本数据分析使用。</li>
<li>methods: 本研究使用了广泛的预训练过程，运用了大量的金融报告、紧急公告和新闻，与德国公司相关。</li>
<li>results: 研究结果显示，德国金融BERT在下游任务中表现出色，尤其是在金融专业数据上。这表明德国金融BERT能够捕捉领域特有的特征。<details>
<summary>Abstract</summary>
This study presents German FinBERT, a novel pre-trained German language model tailored for financial textual data. The model is trained through a comprehensive pre-training process, leveraging a substantial corpus comprising financial reports, ad-hoc announcements and news related to German companies. The corpus size is comparable to the data sets commonly used for training standard BERT models. I evaluate the performance of German FinBERT on downstream tasks, specifically sentiment prediction, topic recognition and question answering against generic German language models. My results demonstrate improved performance on finance-specific data, indicating the efficacy of German FinBERT in capturing domain-specific nuances. The presented findings suggest that German FinBERT holds promise as a valuable tool for financial text analysis, potentially benefiting various applications in the financial domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accelerating-Toeplitz-Neural-Network-with-Constant-time-Inference-Complexity"><a href="#Accelerating-Toeplitz-Neural-Network-with-Constant-time-Inference-Complexity" class="headerlink" title="Accelerating Toeplitz Neural Network with Constant-time Inference Complexity"></a>Accelerating Toeplitz Neural Network with Constant-time Inference Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08756">http://arxiv.org/abs/2311.08756</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opennlplab/etsc-exact-toeplitz-to-ssm-conversion">https://github.com/opennlplab/etsc-exact-toeplitz-to-ssm-conversion</a></li>
<li>paper_authors: Zhen Qin, Yiran Zhong</li>
<li>for: 本文旨在将 toeplitz neural networks (TNNs) 转化为 state space models (SSMs)，以便在推理过程中实现常数复杂性。</li>
<li>methods: 作者通过对 TNNs 的推理过程进行优化，将其转化为 SSMs。该过程被形式化为一个优化问题，并提供了关闭式解决方案。在解决过程中，作者使用离散傅里叶变换 (DFT) 来高效解决 Vandermonde 线性系统问题。</li>
<li>results: 作者在语言模型任务上进行了广泛的实验，证明了其方法的有效性。具体来说，作者的方法可以在不同的设定下保持数值稳定性，并且与其他梯度下降解决方案相比，具有更高的数值稳定性。<details>
<summary>Abstract</summary>
Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in various sequence modeling tasks. They outperform commonly used Transformer-based models while benefiting from log-linear space-time complexities. On the other hand, State Space Models (SSMs) achieve lower performance than TNNs in language modeling but offer the advantage of constant inference complexity. In this paper, we aim to combine the strengths of TNNs and SSMs by converting TNNs to SSMs during inference, thereby enabling TNNs to achieve the same constant inference complexities as SSMs. To accomplish this, we formulate the conversion process as an optimization problem and provide a closed-form solution. We demonstrate how to transform the target equation into a Vandermonde linear system problem, which can be efficiently solved using the Discrete Fourier Transform (DFT). Notably, our method requires no training and maintains numerical stability. It can be also applied to any LongConv-based model. To assess its effectiveness, we conduct extensive experiments on language modeling tasks across various settings. Additionally, we compare our method to other gradient-descent solutions, highlighting the superior numerical stability of our approach. The source code is available at https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion.
</details>
<details>
<summary>摘要</summary>
托平论 neural network (TNN) 在不同的序列模型任务中表现出色，而且比通用的 transformer 型模型更具有 Log-linear 空间时间复杂度的优势。然而，状态空间模型 (SSM) 在语言模型中表现较差，但它具有常数推理复杂度的优点。在这篇论文中，我们想要将 TNN 转换成 SSM 以实现常数推理复杂度，而不需要训练。我们将转换过程定义为优化问题，并提供了关闭式解决方案。我们将目标方程转换成 Vandermonde 线性系统问题，可以使用离散傅立叶变换 (DFT) 高效解决。值得注意的是，我们的方法不需要训练，并且保持了数值稳定性。此外，我们的方法可以应用于任何 LongConv 基于模型。为评估其效果，我们在不同的语言模型任务上进行了广泛的实验。此外，我们与其他梯度下降解决方案进行比较，高亮了我们的方法的数值稳定性的优势。源代码可以在 GitHub 上找到：https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion。
</details></li>
</ul>
<hr>
<h2 id="Thread-of-Thought-Unraveling-Chaotic-Contexts"><a href="#Thread-of-Thought-Unraveling-Chaotic-Contexts" class="headerlink" title="Thread of Thought Unraveling Chaotic Contexts"></a>Thread of Thought Unraveling Chaotic Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08734">http://arxiv.org/abs/2311.08734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, Jianbing Shen</li>
<li>For: The paper aims to improve the reasoning performance of large language models (LLMs) in chaotic contexts by introducing a new “Thread of Thought” (ThoT) strategy.* Methods: The ThoT strategy segments and analyzes extended contexts, selecting pertinent information to improve the reasoning performance of LLMs. The strategy is versatile and can be integrated with various LLMs and prompting techniques.* Results: The paper demonstrates the effectiveness of ThoT using three datasets (PopQA, EntityQ, and MTCR) and shows that ThoT significantly improves reasoning performance compared to other prompting techniques.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have ushered in a transformative era in the field of natural language processing, excelling in tasks related to text comprehension and generation. Nevertheless, they encounter difficulties when confronted with chaotic contexts (e.g., distractors rather than long irrelevant context), leading to the inadvertent omission of certain details within the chaotic context. In response to these challenges, we introduce the "Thread of Thought" (ThoT) strategy, which draws inspiration from human cognitive processes. ThoT systematically segments and analyzes extended contexts while adeptly selecting pertinent information. This strategy serves as a versatile "plug-and-play" module, seamlessly integrating with various LLMs and prompting techniques. In the experiments, we utilize the PopQA and EntityQ datasets, as well as a Multi-Turn Conversation Response dataset (MTCR) we collected, to illustrate that ThoT significantly improves reasoning performance compared to other prompting techniques.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Emergency-Decision-making-with-Knowledge-Graphs-and-Large-Language-Models"><a href="#Enhancing-Emergency-Decision-making-with-Knowledge-Graphs-and-Large-Language-Models" class="headerlink" title="Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models"></a>Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08732">http://arxiv.org/abs/2311.08732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minze Chen, Zhenxiang Tao, Weitong Tang, Tingxin Qin, Rui Yang, Chunli Zhu<br>for: 提供可靠的紧急决策支持methods: 使用知识图和大语言模型results: 在不同的紧急情况下，与基eline模型相比，得到了显著的改善，得分9.06、9.09、9.03和9.09。<details>
<summary>Abstract</summary>
Emergency management urgently requires comprehensive knowledge while having a high possibility to go beyond individuals' cognitive scope. Therefore, artificial intelligence(AI) supported decision-making under that circumstance is of vital importance. Recent emerging large language models (LLM) provide a new direction for enhancing targeted machine intelligence. However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills. In this work, we develop a system called Enhancing Emergency decision-making with Knowledge Graph and LLM (E-KELL), which provides evidence-based decision-making in various emergency stages. The study constructs a structured emergency knowledge graph and guides LLMs to reason over it via a prompt chain. In real-world evaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in comprehensibility, accuracy, conciseness, and instructiveness from a group of emergency commanders and firefighters, demonstrating a significant improvement across various situations compared to baseline models. This work introduces a novel approach to providing reliable emergency decision support.
</details>
<details>
<summary>摘要</summary>
应急管理强需全面知识，同时具有跨个人认知范围的可能性。因此，基于人工智能（AI）的决策在这种情况下是非常重要的。最新的大语言模型（LLM）提供了一个新的方向来提高目标机器智能。然而，直接使用LLM会导致不可靠的输出，因为它们的内置问题包括幻觉和思维能力不足。在这项工作中，我们开发了一个系统called Enhancing Emergency decision-making with Knowledge Graph and LLM（E-KELL），它提供了基于证据的决策在不同的应急阶段。研究构建了一个结构化的应急知识图，并使用提示链导引LLM进行图上的理解。在实际评估中，E-KELL得分9.06、9.09、9.03和9.09在可读性、准确性、简洁性和指导性方面，分别从一群应急指挥官和消防员手中得到评分，表明与基eline模型相比在不同的情况下显著提高。这项工作介绍了一种可靠的应急决策支持方法。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-on-Sequential-Labeling-via-Uncertainty-Transmission"><a href="#Uncertainty-Estimation-on-Sequential-Labeling-via-Uncertainty-Transmission" class="headerlink" title="Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission"></a>Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08726">http://arxiv.org/abs/2311.08726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianfeng He, Linlin Yu, Shuo Lei, Chang-Tien Lu, Feng Chen</li>
<li>for: 本研究旨在提高Named Entity Recognition（NER）预测的不确定性评估（UE-NER）。</li>
<li>methods: 本研究提出了一个Sequential Labeling Posterior Network（SLPN），用于估算NER预测结果的不确定性。SLPN考虑了ENTITY之间的连接（即一个ENTITY嵌入是基于其他ENTITY的学习），并且特别处理了WRONG-SPAN情况。</li>
<li>results: 本研究在两个数据集上实现了显著的改善，例如在MIT-Restaurant数据集上提高了AUPR指数5.54个点。<details>
<summary>Abstract</summary>
Sequential labeling is a task predicting labels for each token in a sequence, such as Named Entity Recognition (NER). NER tasks aim to extract entities and predict their labels given a text, which is important in information extraction. Although previous works have shown great progress in improving NER performance, uncertainty estimation on NER (UE-NER) is still underexplored but essential. This work focuses on UE-NER, which aims to estimate uncertainty scores for the NER predictions. Previous uncertainty estimation models often overlook two unique characteristics of NER: the connection between entities (i.e., one entity embedding is learned based on the other ones) and wrong span cases in the entity extraction subtask. Therefore, we propose a Sequential Labeling Posterior Network (SLPN) to estimate uncertainty scores for the extracted entities, considering uncertainty transmitted from other tokens. Moreover, we have defined an evaluation strategy to address the specificity of wrong-span cases. Our SLPN has achieved significant improvements on two datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant dataset.
</details>
<details>
<summary>摘要</summary>
Sequential labeling是一个任务，它 predicts labels for each token in a sequence，如Named Entity Recognition (NER)。NER任务的目标是从文本中提取实体，并预测它们的标签，这是信息抽取中非常重要的一步。虽然之前的工作已经达到了NER性能的很大进步，但UE-NER（Named Entity Recognition uncertainty estimation）还是被忽略了，这是非常重要的。本工作关注UE-NER，它的目标是为NER预测中的实体提取 uncertainty scores。以前的uncertainty estimation模型经常忽略了NER中的两个特有特征：实体之间的连接（即一个实体嵌入是基于其他实体学习的）以及实体提取子任务中的错误案例。因此，我们提出了一个Sequential Labeling Posterior Network (SLPN)，用于估计实体预测中的uncertainty scores，考虑实体之间的uncertainty传递。此外，我们定义了一种评估策略，用于解决实体提取子任务中的特殊错误案例。我们的SLPN在两个 dataset上达到了显著的改进，如MIT-Restaurant dataset上的AUPR提高5.54点。
</details></li>
</ul>
<hr>
<h2 id="Method-for-Text-Entity-Linking-in-Power-Distribution-Scheduling-Oriented-to-Power-Distribution-Network-Knowledge-Graph"><a href="#Method-for-Text-Entity-Linking-in-Power-Distribution-Scheduling-Oriented-to-Power-Distribution-Network-Knowledge-Graph" class="headerlink" title="Method for Text Entity Linking in Power Distribution Scheduling Oriented to Power Distribution Network Knowledge Graph"></a>Method for Text Entity Linking in Power Distribution Scheduling Oriented to Power Distribution Network Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08724">http://arxiv.org/abs/2311.08724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Che Wang, Bing Li, Hao Chen, Sizhe Li</li>
<li>for: 本研究旨在链接发电 dispatch 文本中的实体到一个电力分配网络知识图的方法。</li>
<li>methods: 该方法利用电力分配网络知识图和发电 dispatch 文本中实体的semantic、phonetic和syntactic特征进行深入理解，并使用加强型模型——lexical semantic feature-based skip convolutional neural network (LSF-SCNN) 进行实体匹配。</li>
<li>results: 比较控制模型的实验结果表明，LSF-SCNN 模型在英语发电 dispatch 文本中高精度地链接了多种实体类型，表现了高总准确率在实体链接中。<details>
<summary>Abstract</summary>
The proposed method for linking entities in power distribution dispatch texts to a power distribution network knowledge graph is based on a deep understanding of these networks. This method leverages the unique features of entities in both the power distribution network's knowledge graph and the dispatch texts, focusing on their semantic, phonetic, and syntactic characteristics. An enhanced model, the Lexical Semantic Feature-based Skip Convolutional Neural Network (LSF-SCNN), is utilized for effectively matching dispatch text entities with those in the knowledge graph. The efficacy of this model, compared to a control model, is evaluated through cross-validation methods in real-world power distribution dispatch scenarios. The results indicate that the LSF-SCNN model excels in accurately linking a variety of entity types, demonstrating high overall accuracy in entity linking when the process is conducted in English.
</details>
<details>
<summary>摘要</summary>
“提议的方法是基于电力分配网络知识图的深入理解，该方法利用知识图和调度文本中实体的语义、语音和语法特征。使用加强模型——lexical semantic feature-based skip convolutional neural network（LSF-SCNN），可以有效地匹配调度文本中的实体与知识图中的实体。通过跨验证方法在实际电力分配调度场景中评估模型的效果，结果表明LSF-SCNN模型在英语下可以准确地连接多种实体类型，实现高精度实体连接。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Token-Prediction-as-Implicit-Classification-to-Identify-LLM-Generated-Text"><a href="#Token-Prediction-as-Implicit-Classification-to-Identify-LLM-Generated-Text" class="headerlink" title="Token Prediction as Implicit Classification to Identify LLM-Generated Text"></a>Token Prediction as Implicit Classification to Identify LLM-Generated Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08723">http://arxiv.org/abs/2311.08723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/markchenyutian/t5-sentinel-public">https://github.com/markchenyutian/t5-sentinel-public</a></li>
<li>paper_authors: Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, Bhiksha Raj</li>
<li>for: 本研究旨在提出一种新的语言模型标识方法，以便在文本生成中识别可能的大型语言模型（LLMs）。</li>
<li>methods: 我们重新框定了分类任务为下一个字符预测任务，直接使用基础LM进行 fine-tune，而不是添加额外的分类层。我们使用 Text-to-Text Transfer Transformer（T5）模型作为我们的实验基础。</li>
<li>results: 我们的方法在文本分类任务中表现出色，表明其简单性和效率。此外，我们对模型提取的特征进行了解释性研究，发现它能够在不同的LLMs中分辨出不同的写作风格，即使没有显式的分类器。我们还收集了一个名为 OpenLLMText 的数据集，包含约 340k 的文本样本，来自人类和 LLMs，包括 GPT3.5、PaLM、LLaMA 和 GPT2。<details>
<summary>Abstract</summary>
This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for classification. Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier. We also collected a dataset named OpenLLMText, containing approximately 340k text samples from human and LLMs, including GPT3.5, PaLM, LLaMA, and GPT2.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的方法，用于识别文本生成过程中可能的大语言模型（LLM）。而不是添加一层分类层到基础语言模型（LM）上，我们将分类任务重新定义为下一个字符预测任务，并直接使用基础LM进行 fine-tune。我们使用 Text-to-Text Transfer Transformer（T5）模型作为我们的实验室。我们与直接使用隐藏状态进行分类的方法进行比较。评估结果表明我们的方法在文本分类任务中表现出色，强调其简单性和效率。此外，我们对我们的模型提取的特征进行了解释性研究，发现它能够在不同的LLM下 diferenciate 不同的写作风格，甚至在没有显式分类器的情况下。我们还收集了一个名为 OpenLLMText 的数据集，包含约 340k 的文本样本，来自人类和 LLM，包括 GPT3.5、PaLM、LLaMA 和 GPT2。
</details></li>
</ul>
<hr>
<h2 id="Think-in-Memory-Recalling-and-Post-thinking-Enable-LLMs-with-Long-Term-Memory"><a href="#Think-in-Memory-Recalling-and-Post-thinking-Enable-LLMs-with-Long-Term-Memory" class="headerlink" title="Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory"></a>Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08719">http://arxiv.org/abs/2311.08719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang</li>
<li>for: 提高大语言模型在长期人机交互中的表现，减少偏见的问题。</li>
<li>methods: 提出了一种新的记忆机制 called TiM，允许大语言模型在对话流中维护一个演化的记忆，并通过插入、忘记和合并操作来动态更新记忆。</li>
<li>results: 在实际和模拟对话中，通过使用 TiM 机制，大语言模型的响应表现得到了显著提高，并且可以减少偏见的问题。<details>
<summary>Abstract</summary>
Memory-augmented Large Language Models (LLMs) have demonstrated remarkable performance in long-term human-machine interactions, which basically relies on iterative recalling and reasoning of history to generate high-quality responses. However, such repeated recall-reason steps easily produce biased thoughts, \textit{i.e.}, inconsistent reasoning results when recalling the same history for different questions. On the contrary, humans can keep thoughts in the memory and recall them without repeated reasoning. Motivated by this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Besides, we formulate the basic principles to organize the thoughts in memory based on the well-established operations, (\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Furthermore, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有增强的记忆功能，在人机交互中表现出了很高的能力。然而，在重复 recall 和推理的过程中，LLM 容易产生偏见，即不同问题时的推理结果不一致。人类可以将想法保持在记忆中，而不需要重复推理。为了解决这个问题，我们提出了一种新的记忆机制called TiM（思考在内存），允许 LLM 在对话流中维护一个演进的记忆。TiM 框架包括两个关键阶段：（1）在生成响应之前，LLM 代理检索相关的思想记忆中，（2）在生成响应后，LLM 代理在历史和新的思想之间进行后思考和融合，以更新记忆。因此，TiM 可以消除重复推理的问题，并将后思考的思想作为历史记忆保存。此外，我们采用了在 TiM 中使用 Local Sensitive Hashing 进行高效的检索，以便应对长期对话。我们在实际和模拟对话中进行了质量和量的实验，demonstrating  equip  existing LLMs with TiM 可以明显提高它们在长期交互中的响应能力。
</details></li>
</ul>
<hr>
<h2 id="Decomposing-Uncertainty-for-Large-Language-Models-through-Input-Clarification-Ensembling"><a href="#Decomposing-Uncertainty-for-Large-Language-Models-through-Input-Clarification-Ensembling" class="headerlink" title="Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling"></a>Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08718">http://arxiv.org/abs/2311.08718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, Yang Zhang</li>
<li>for: 这 paper aims to improve the reliability, trustworthiness, and interpretability of large language models (LLMs) by developing an uncertainty decomposition framework.</li>
<li>methods: The proposed framework, called input clarifications ensemble, generates a set of clarifications for the input and feeds them into the fixed LLMs to ensure accurate and reliable uncertainty quantification.</li>
<li>results: Empirical evaluations demonstrate that the proposed framework provides accurate and reliable uncertainty quantification on various tasks, and the code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/llm_uncertainty.Here's">https://github.com/UCSB-NLP-Chang/llm_uncertainty.Here&#39;s</a> the Chinese version:</li>
<li>for: 这 paper 的目的是提高大型自然语言处理模型（LLMs）的可靠性、可信度和可解释性。</li>
<li>methods: 提议的框架是输入明确集，它会生成输入的一组明确度，然后将其传递给固定的 LLMs，以确保准确和可靠的不确定量评估。</li>
<li>results: 实验证明，提议的框架可以在不同任务上提供准确和可靠的不确定量评估，代码将会在 <a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/llm_uncertainty">https://github.com/UCSB-NLP-Chang/llm_uncertainty</a> 上公开发布。<details>
<summary>Abstract</summary>
Uncertainty decomposition refers to the task of decomposing the total uncertainty of a model into data (aleatoric) uncertainty, resulting from the inherent complexity or ambiguity of the data, and model (epistemic) uncertainty, resulting from the lack of knowledge in the model. Performing uncertainty decomposition for large language models (LLMs) is an important step toward improving the reliability, trustworthiness, and interpretability of LLMs, but this research task is very challenging and remains unresolved. The existing canonical method, Bayesian Neural Network (BNN), cannot be applied to LLMs, because BNN requires training and ensembling multiple variants of models, which is infeasible or prohibitively expensive for LLMs. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarifications ensemble, which bypasses the need to train new models. Rather than ensembling models with different parameters, our approach generates a set of clarifications for the input, feeds them into the fixed LLMs, and ensembles the corresponding predictions. We show that our framework shares a symmetric decomposition structure with BNN. Empirical evaluations demonstrate that the proposed framework provides accurate and reliable uncertainty quantification on various tasks. Code will be made publicly available at https://github.com/UCSB-NLP-Chang/llm_uncertainty .
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable("Uncertainty decomposition")uncertainty decomposition REFERS TO THE TASK OF DECOMPOSING THE TOTAL UNCERTAINTY OF A MODEL INTO DATA (aleatoric) uncertainty, RESULTING FROM THE INHERENT COMPLEXITY OR AMBIGUITY OF THE DATA, AND MODEL (epistemic) uncertainty, RESULTING FROM THE LACK OF KNOWLEDGE IN THE MODEL. PERFORMING UNCERTAINTY DECOMPOSITION FOR LARGE LANGUAGE MODELS (LLMs) IS AN IMPORTANT STEP TOWARD IMPROVING THE RELIABILITY, TRUSTWORTHINESS, AND INTERPRETABILITY OF LLMs, BUT THIS RESEARCH TASK IS VERY CHALLENGING AND REMAINS UNRESOLVED. THE EXISTING CANONICAL METHOD, BAYESIAN NEURAL NETWORK (BNN), CANNOT BE APPLIED TO LLMs, BECAUSE BNN REQUIRES TRAINING AND ENSMBLING MULTIPLE VARIANTS OF MODELS, WHICH IS INFEASIBLE OR PROHIBITIVELY EXPENSIVE FOR LLMs. IN THIS PAPER, WE INTRODUCE AN UNCERTAINTY DECOMPOSITION FRAMEWORK FOR LLMs, CALLED INPUT CLARIFICATIONS ENSEMBLE, WHICH BYPASSES THE NEED TO TRAIN NEW MODELS. RATHER THAN ENSMBLING MODELS WITH DIFFERENT PARAMETERS, OUR APPROACH GENERATES A SET OF CLARIFICATIONS FOR THE INPUT, FEEDS THEM INTO THE FIXED LLMs, AND ENSMBLES THE CORRESPONDING PREDICTIONS. WE SHOW THAT OUR FRAMEWORK SHARES A SYMMETRIC DECOMPOSITION STRUCTURE WITH BNN. EMPIRICAL EVALUATIONS DEMONSTRATE THAT THE PROPOSED FRAMEWORK PROVIDES ACCURATE AND RELIABLE UNCERTAINTY QUANTIFICATION ON VARIOUS TASKS. CODE WILL BE MADE PUBLICLY AVAILABLE AT https://github.com/UCSB-NLP-Chang/llm_uncertainty .
</details></li>
</ul>
<hr>
<h2 id="PLUG-Leveraging-Pivot-Language-in-Cross-Lingual-Instruction-Tuning"><a href="#PLUG-Leveraging-Pivot-Language-in-Cross-Lingual-Instruction-Tuning" class="headerlink" title="PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning"></a>PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08711">http://arxiv.org/abs/2311.08711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ytyz1307zzh/plug">https://github.com/ytyz1307zzh/plug</a></li>
<li>paper_authors: Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng Jiang, Francesco Barbieri</li>
<li>for: 提高大语言模型在不同人类指令下的理解和回答能力</li>
<li>methods: 使用高资源语言（主要是英语）为核心，实现指令准备语言转化为目标语言的回答</li>
<li>results: 比直接回答目标语言alone提高了大语言模型对指令的遵从能力，增加了29%的平均提升率。<details>
<summary>Abstract</summary>
Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following abilities of LLMs by 29% on average, compared to directly responding in the target language alone. Further experiments validate the versatility of our approach by employing alternative pivot languages beyond English to assist languages where LLMs exhibit lower proficiency.
</details>
<details>
<summary>摘要</summary>
具有杰出表现的指令调整技术已经大幅提高了大型自然语言模型（LLM）的理解和回应多样化人类指令的能力。然而，在低资源语言上应用这些技术却遇到了挑战，这主要归结于LLM在不同语言的基础能力的不均衡，这种不均衡来自于模型在它们的预训练数据中的语言分布不均。为解决这个问题，我们提出了锚语言导向生成（PLUG）方法，该方法利用高资源语言（主要是英语）作为锚点，以提高低资源语言中的指令调整能力。它将模型首先在锚语言中处理指令，然后生成回应在目标语言中。为评估我们的方法，我们提出了一个标准测试套件，名为X-AlpacaEval，该套件包含4种语言（中文、韩语、意大利语和西班牙语）的指令，每个指令由专业翻译员进行标注。我们的方法在平均上提高了LLM的指令遵循能力 by 29%，相比直接在目标语言中回应。此外，我们的实验还证明了我们的方法可以采用不同的锚语言来帮助语言，其中LLM表现较低的语言。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Robustness-of-Dialogue-Summarization-Models-in-the-Presence-of-Naturally-Occurring-Variations"><a href="#Evaluating-Robustness-of-Dialogue-Summarization-Models-in-the-Presence-of-Naturally-Occurring-Variations" class="headerlink" title="Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations"></a>Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08705">http://arxiv.org/abs/2311.08705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankita Gupta, Chulaka Gunasekara, Hui Wan, Jatin Ganhotra, Sachindra Joshi, Marina Danilevsky</li>
<li>for: 本研究旨在探讨对话摘要模型的Robustness Challenge，包括对各种自然语言变化和噪声的影响。</li>
<li>methods: 我们使用公开的数据集对现有的对话摘要模型进行了系统性的研究，以评估这些模型对各种语言变化和噪声的抗预测性。我们引入了两种类型的干扰：utterance-level干扰和对话-level干扰。</li>
<li>results: 我们发现，尽管使用精度级进行了微调和指令级进行了微调，但是这些模型都受到输入变化的影响，特别是对话-level干扰。我们还通过人工评估 validate our findings。此外，我们发现使用一部分干扰数据进行训练并不能解决对话摘要模型的Robustness Challenge。<details>
<summary>Abstract</summary>
Dialogue summarization task involves summarizing long conversations while preserving the most salient information. Real-life dialogues often involve naturally occurring variations (e.g., repetitions, hesitations) and existing dialogue summarization models suffer from performance drop on such conversations. In this study, we systematically investigate the impact of such variations on state-of-the-art dialogue summarization models using publicly available datasets. To simulate real-life variations, we introduce two types of perturbations: utterance-level perturbations that modify individual utterances with errors and language variations, and dialogue-level perturbations that add non-informative exchanges (e.g., repetitions, greetings). We conduct our analysis along three dimensions of robustness: consistency, saliency, and faithfulness, which capture different aspects of the summarization model's performance. We find that both fine-tuned and instruction-tuned models are affected by input variations, with the latter being more susceptible, particularly to dialogue-level perturbations. We also validate our findings via human evaluation. Finally, we investigate if the robustness of fine-tuned models can be improved by training them with a fraction of perturbed data and observe that this approach is insufficient to address robustness challenges with current models and thus warrants a more thorough investigation to identify better solutions. Overall, our work highlights robustness challenges in dialogue summarization and provides insights for future research.
</details>
<details>
<summary>摘要</summary>
对话摘要任务 involve 摘要长 conversations 而保留最重要信息。实际对话中经常出现自然的变化（例如重复、停顿），现有的对话摘要模型在这些对话中表现不佳。在这项研究中，我们系统地研究这些变化对现状对话摘要模型的影响。为了模拟实际变化，我们引入了两种类型的杂化：个别话语杂化（ modify 个别话语中的错误和语言变化）和对话杂化（添加无关信息的交流，例如重复、致谢）。我们按照三个维度进行分析：一致性、重要性和忠诚度，这些维度捕捉了不同的对话摘要模型表现方面。我们发现， beide fine-tuned 和 instruction-tuned 模型受到输入变化的影响，其中后者更加敏感，特别是对话杂化。我们还通过人工评估 validate 我们的发现。最后，我们 investigate 是否可以通过训练 fine-tuned 模型 WITH 一部分杂化数据来提高其robustness，并发现这种方法不足以解决当前模型的Robustness挑战，因此需要进一步的调查以找到更好的解决方案。总之，我们的工作强调对话摘要中的Robustness挑战和未来研究的需要。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Diversity-Determines-the-Systematicity-Gap-in-VQA"><a href="#Attribute-Diversity-Determines-the-Systematicity-Gap-in-VQA" class="headerlink" title="Attribute Diversity Determines the Systematicity Gap in VQA"></a>Attribute Diversity Determines the Systematicity Gap in VQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08695">http://arxiv.org/abs/2311.08695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ian Berlot-Attwell, A. Michael Carrell, Kumar Krishna Agrawal, Yash Sharma, Naomi Saphra</li>
<li>for: 研究 neural network 是否可以通过将 familar concept 组合在一起来泛化到新的情况。</li>
<li>methods: 引入了一个新的诊断数据集 CLEVR-HOPE，以测试系统aticity gap 在视觉问答中的表现。</li>
<li>results: 发现尽量增加训练数据量不会减少系统aticity gap，但是增加不同类型的属性组合在未seen combination中的训练数据多样性可以减少系统aticity gap。<details>
<summary>Abstract</summary>
The degree to which neural networks can generalize to new combinations of familiar concepts, and the conditions under which they are able to do so, has long been an open question. In this work, we study the systematicity gap in visual question answering: the performance difference between reasoning on previously seen and unseen combinations of object attributes. To test, we introduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased quantity of training data does not reduce the systematicity gap, increased training data diversity of the attributes in the unseen combination does. In all, our experiments suggest that the more distinct attribute type combinations are seen during training, the more systematic we can expect the resulting model to be.
</details>
<details>
<summary>摘要</summary>
“神经网络是否能够总结新的熵合？”这个问题一直是开放的。在这个工作中，我们研究视觉问答中的系统特性差距：推理已经看过和未经看过的对象属性的组合性的表现差异。为了测试，我们引入了一个新的诊断数据集，CLEVR-HOPE。我们发现，尽管增加训练数据量不会减少系统特性差距，但是增加未经看过组合属性的训练数据多样性可以减少系统特性差距。总之，我们的实验表明，更多的独特属性类型组合被训练时，更可预期性的结果。
</details></li>
</ul>
<hr>
<h2 id="Routing-to-the-Expert-Efficient-Reward-guided-Ensemble-of-Large-Language-Models"><a href="#Routing-to-the-Expert-Efficient-Reward-guided-Ensemble-of-Large-Language-Models" class="headerlink" title="Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models"></a>Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08692">http://arxiv.org/abs/2311.08692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, Jingren Zhou</li>
<li>for: 本研究旨在提高大语言模型（LLM）的 ensemble 性能，通过挖掘各自领域和任务中的专业知识，实现更好的 ensemble 性能。</li>
<li>methods: 本研究提出了一种名为 Zooter 的奖励导引路由方法，通过训练路由函数来精准地分配每个查询到适合的 LLM 中。此外，研究还提出了一种基于标签的抑制难以预测的噪声的技术。</li>
<li>results: 研究发现，Zooter 在一系列 benchmark 集合上表现出色，比单个模型的表现更好，并在 44% 的任务上击败了多个奖励模型排名方法。<details>
<summary>Abstract</summary>
The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate it by mining latent expertise with off-the-shelf reward models. We propose Zooter, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. Zooter shows computation efficiency in inference as it introduces only a minor computation overhead of a routing function compared with reward model ranking methods. We evaluate Zooter on a comprehensive benchmark collection with 26 subsets on different domains and tasks. Zooter outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>LLM的补偿潜力假设市售LLM有多个领域和任务的多样化专业知识，以 ensemble 方式实现更好的性能。现有的LLM ensemble方法主要集中于奖励模型排名输出，导致计算开销增加。为解决这个问题，我们再次探讨LLM的补偿潜力，并通过挖掘缓存专业知识使用市售奖励模型。我们提议Zooter，一种奖励导航方法，通过在训练查询上分配奖励来培养路由函数，可以准确地将每个查询分配给LLM拥有相关专业知识。我们还 integra 了标签基本标签增强来降低使用奖励作为银色监督时的噪音。Zooter在推理中引入了只有市售奖励模型排名方法相对较少的计算开销。我们对一个包含26个子集的全面 benchmark 集进行了评估，Zooter在 average 上超过了最佳单个模型，并在44%的任务上排名第一。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Calibration-for-Multilingual-Question-Answering-Models"><a href="#Understanding-Calibration-for-Multilingual-Question-Answering-Models" class="headerlink" title="Understanding Calibration for Multilingual Question Answering Models"></a>Understanding Calibration for Multilingual Question Answering Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08669">http://arxiv.org/abs/2311.08669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yahan Yang, Soham Dan, Dan Roth, Insup Lee</li>
<li>for: 这篇论文主要研究了多语言预训练语言模型在问答任务中的准确性。</li>
<li>methods: 该论文使用了多种问答模型设计和多种语言进行了广泛的实验，包括抽取式和生成式问答模型，以及高资源语言和低资源语言。它还研究了不同维度的准确性，包括在适应区、离distribution和跨语言传递设置中。</li>
<li>results: 研究发现自动翻译数据增强技术可以大幅提高模型准确性，并进行了一系列的减少实验来研究模型大小对准确性的影响和多语言模型与单语言模型的比较。<details>
<summary>Abstract</summary>
Multilingual pre-trained language models are incredibly effective at Question Answering (QA), a core task in Natural Language Understanding, achieving high accuracies on several multilingual benchmarks. However, little is known about how well they are calibrated. In this paper, we study the calibration properties of several pre-trained multilingual large language models (LLMs) on a variety of question-answering tasks. We perform extensive experiments, spanning both extractive and generative QA model designs and diverse languages, spanning both high-resource and low-resource ones. We study different dimensions of calibration in in-distribution, out-of-distribution, and cross-lingual transfer settings, and investigate strategies to improve it, including post-hoc methods and regularized fine-tuning. We demonstrate automatically translated data augmentation as a highly effective technique to improve model calibration. We also conduct a number of ablation experiments to study the effect of model size on calibration and how multilingual models compare with their monolingual counterparts for diverse tasks and languages.
</details>
<details>
<summary>摘要</summary>
多语言预训练语言模型在问答任务（QA）中表现非常出色，在多种多语言benchmark上达到了高准确率。然而，对于这些模型的准确性calibration的了解非常少。在这篇论文中，我们研究了多种预训练多语言大型语言模型（LLMs）在问答任务中的准确性calibration性。我们进行了广泛的实验，涵盖了EXTRACTIVE和生成型问答模型的设计，以及多种语言和资源量的组合。我们研究了不同的calibration维度，包括在适用范围内、外部和跨语言传递设置中的calibration性，并 investigate了提高calibration性的策略，包括后期方法和规则化的细化。我们示出了自动翻译数据增强为一种非常有效的技术来提高模型的准确性calibration。我们还进行了一些减少实验来研究模型大小对calibration的影响和多语言模型与单语言模型在多种任务和语言上的比较。
</details></li>
</ul>
<hr>
<h2 id="It-Takes-Two-to-Negotiate-Modeling-Social-Exchange-in-Online-Multiplayer-Games"><a href="#It-Takes-Two-to-Negotiate-Modeling-Social-Exchange-in-Online-Multiplayer-Games" class="headerlink" title="It Takes Two to Negotiate: Modeling Social Exchange in Online Multiplayer Games"></a>It Takes Two to Negotiate: Modeling Social Exchange in Online Multiplayer Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08666">http://arxiv.org/abs/2311.08666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kj2013/claff-diplomacy">https://github.com/kj2013/claff-diplomacy</a></li>
<li>paper_authors: Kokil Jaidka, Hansin Ahuja, Lynnette Ng</li>
<li>for: 研究在线上战略游戏《 дипломати》中玩家之间的交互，以了解玩家如何在游戏中谈判他们的方式。</li>
<li>methods: 使用了10,000多则聊天讯息的标注数据，以分析不同谈判策略的重要性，并评估这些策略在预测短期和长期游戏结果中的影响。</li>
<li>results: 发现谈判策略可以通过语言模型化聊天讯息来预测，但是在短期内的信任性预测需要更多的资料。然而，谈判策略在图像意识强化学习方法中是非常重要的，可以预测长期游戏结果，如玩家的成功。<details>
<summary>Abstract</summary>
Online games are dynamic environments where players interact with each other, which offers a rich setting for understanding how players negotiate their way through the game to an ultimate victory. This work studies online player interactions during the turn-based strategy game, Diplomacy. We annotated a dataset of over 10,000 chat messages for different negotiation strategies and empirically examined their importance in predicting long- and short-term game outcomes. Although negotiation strategies can be predicted reasonably accurately through the linguistic modeling of the chat messages, more is needed for predicting short-term outcomes such as trustworthiness. On the other hand, they are essential in graph-aware reinforcement learning approaches to predict long-term outcomes, such as a player's success, based on their prior negotiation history. We close with a discussion of the implications and impact of our work. The dataset is available at https://github.com/kj2013/claff-diplomacy.
</details>
<details>
<summary>摘要</summary>
在线游戏是动态环境，玩家之间的互动可以提供丰富的数据来理解玩家如何在游戏中获得最终胜利。这个研究 focuses on线上玩家互动中的谈判策略，并对不同的谈判策略进行了类别标注。我们分析了超过10,000封聊天讯息，并评估了这些谈判策略对游戏的长期和短期结果的影响。虽然可以透过语言模型估计谈判策略的准确性，但是在短期内的信任性仍然是难以预测的。然而，这些谈判策略在图形意识型态的强化学习方法中是非常重要的，可以预测长期的成功。我们在结论中讨论了这个研究的影响和意义，并提供了资料集的网站地址。
</details></li>
</ul>
<hr>
<h2 id="Multistage-Collaborative-Knowledge-Distillation-from-Large-Language-Models"><a href="#Multistage-Collaborative-Knowledge-Distillation-from-Large-Language-Models" class="headerlink" title="Multistage Collaborative Knowledge Distillation from Large Language Models"></a>Multistage Collaborative Knowledge Distillation from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08640">http://arxiv.org/abs/2311.08640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Benjamin Rozonoyer, Md Arafat Sultan, Jay-Yoon Lee, Mohit Iyyer, Andrew McCallum</li>
<li>for: 这 paper 是为了解决 semi-supervised sequence prediction 任务，其中有限量的标注数据不足以有效地训练模型，而同时几何shot提示大型自然语言模型 (LLM) 的性能有限。</li>
<li>methods: 这 paper 使用了一种新的混合型知识填充方法 (MCKD)，其首先使用几何shot在 Context 中学习来生成假标签 для无标注数据。然后，在每个阶段的填充中，一对学生在不同的分区上进行训练，每个学生生成新的和改进的假标签来监督下一个阶段的学生。</li>
<li>results: 这 paper 的结果表明，在两个 constituency parsing 任务上，使用多stage collaborative knowledge distillation (MCKD) 可以提高模型的性能。在 CRAFT 生物医学解析任务上，3-stage MCKD 使用 50 个标注例可以与 supervised finetuning 使用 500 个标注例匹配的性能，并且超过提示 LL 和 vanilla KD 的性能 by 7.5% 和 3.7% 的解析 F1，分别。<details>
<summary>Abstract</summary>
We study semi-supervised sequence prediction tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from a prompted LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we propose a new distillation method, multistage collaborative knowledge distillation from an LLM (MCKD), for such tasks. MCKD first prompts an LLM using few-shot in-context learning to produce pseudolabels for unlabeled data. Then, at each stage of distillation, a pair of students are trained on disjoint partitions of the pseudolabeled data. Each student subsequently produces new and improved pseudolabels for the unseen partition to supervise the next round of student(s) with. We show the benefit of multistage cross-partition labeling on two constituency parsing tasks. On CRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the performance of supervised finetuning with 500 examples and outperforms the prompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.
</details>
<details>
<summary>摘要</summary>
我们研究半supervised序列预测任务，其中标签资料短缺，无法有效地调整模型。另一方面，几个shot提示大型自然语言模型（LLM）的表现有限。在这篇论文中，我们发现学生模型从提示LLM的distillation中可以对such tasks generalize更好。基于这发现，我们提出了一个新的distillation方法：多stage合作知识传递法（MCKD）。MCKD首先使用几个shot在场景学习生成pseudolabels для无标的数据。然后，在每个阶段的distillation中，一对学生被训练在不同的分区中。每个学生 subsequntially生成新的和改善的pseudolabels для未看到的分区，以便supervise the next round of student(s) with。我们显示了多stage交叉分区标签的 benefitu two constituency parsing tasks。在CRAFT生物医学分析任务上，3 stage MCKD with 50标签例和supervised fine-tuning with 500标签例的表现相似，并且超过提示LLM和vanilla KD的构造解析F1指标 by 7.5%和3.7%，对于这两个任务而言。
</details></li>
</ul>
<hr>
<h2 id="Formal-Proofs-as-Structured-Explanations-Proposing-Several-Tasks-on-Explainable-Natural-Language-Inference"><a href="#Formal-Proofs-as-Structured-Explanations-Proposing-Several-Tasks-on-Explainable-Natural-Language-Inference" class="headerlink" title="Formal Proofs as Structured Explanations: Proposing Several Tasks on Explainable Natural Language Inference"></a>Formal Proofs as Structured Explanations: Proposing Several Tasks on Explainable Natural Language Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08637">http://arxiv.org/abs/2311.08637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lasha Abzianidze</li>
<li>for: 提出一种使用正式证明来实现可解释的自然语言推理（NLI）任务。</li>
<li>methods: 使用可靠高性能的逻辑基于NLI系统生成正式证明，并利用生成的证明中的深入信息来定义可解释NLI任务。</li>
<li>results: 提出一系列有结构化解释的NLI任务，可以根据解释的粒度来排序。 argue that these tasks will have fewer shortcomings than existing explainable NLI tasks.<details>
<summary>Abstract</summary>
In this position paper, we propose a way of exploiting formal proofs to put forward several explainable natural language inference (NLI) tasks. The formal proofs will be produced by a reliable and high-performing logic-based NLI system. Taking advantage of the in-depth information available in the generated formal proofs, we show how it can be used to define NLI tasks with structured explanations. The proposed tasks can be ordered according to difficulty defined in terms of the granularity of explanations. We argue that the tasks will suffer with substantially fewer shortcomings than the existing explainable NLI tasks (or datasets).
</details>
<details>
<summary>摘要</summary>
在这份位置论文中，我们提出了使用正式证明来提出一些可解释的自然语言推理（NLI）任务。正式证明将由可靠高性能的逻辑基于NLI系统生成。利用生成的正式证明中的深入信息，我们显示了如何使用结构化解释来定义NLI任务。我们提出的任务可以按Difficulty进行排序，定义为证明粒度的水平。我们认为这些任务具有较少缺陷，相比现有的可解释NLI任务（或数据集）。
</details></li>
</ul>
<hr>
<h2 id="DEED-Dynamic-Early-Exit-on-Decoder-for-Accelerating-Encoder-Decoder-Transformer-Models"><a href="#DEED-Dynamic-Early-Exit-on-Decoder-for-Accelerating-Encoder-Decoder-Transformer-Models" class="headerlink" title="DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models"></a>DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08623">http://arxiv.org/abs/2311.08623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Tang, Pengkai Zhu, Tian Li, Srikar Appalaraju, Vijay Mahadevan, R. Manmatha</li>
<li>for: 降低encoder-decoder transformer模型的推理时间</li>
<li>methods: 使用Dynamic Early Exit on Decoder (DEED)方法，包括多出口encoder-decoder模型、深度监管和适应模块等简单 yet practical技术，以提高推理精度并降低推理时间</li>
<li>results: 对两种state-of-the-art encoder-decoder transformer模型进行评测，实现了30%-60%的总推理时间减少，同时保持与基线相当或更高的准确率<details>
<summary>Abstract</summary>
Encoder-decoder transformer models have achieved great success on various vision-language (VL) tasks, but they suffer from high inference latency. Typically, the decoder takes up most of the latency because of the auto-regressive decoding. To accelerate the inference, we propose an approach of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit encoder-decoder transformer model which is trained with deep supervision so that each of its decoder layers is capable of generating plausible predictions. In addition, we leverage simple yet practical techniques, including shared generation head and adaptation modules, to keep accuracy when exiting at shallow decoder layers. Based on the multi-exit model, we perform step-level dynamic early exit during inference, where the model may decide to use fewer decoder layers based on its confidence of the current layer at each individual decoding step. Considering different number of decoder layers may be used at different decoding steps, we compute deeper-layer decoder features of previous decoding steps just-in-time, which ensures the features from different decoding steps are semantically aligned. We evaluate our approach with two state-of-the-art encoder-decoder transformer models on various VL tasks. We show our approach can reduce overall inference latency by 30%-60% with comparable or even higher accuracy compared to baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>预测模型Encoder-decoder transformer模型在视觉语言任务中获得了很大成功，但它们受到高速引入延迟的困扰。通常，解码器占总时间的大部分，因为解码器使用自动回归的方式。为了加速推断，我们提出了在解码器上进行动态早期离开的方法（DEED）。我们构建了多出口encoder-decoder transformer模型，该模型在每个解码层都可以生成可信度的预测。此外，我们利用了简单 yet practical的技术，包括共享生成头和适应模块，以保持精度 when exiting at shallow decoder layers。基于多出口模型，我们在推断过程中实施了Step-level动态早期离开，其中模型可以根据当前层的信息使用 fewer decoder layers。由于不同的decoder layers可能会在不同的推断步骤中使用，我们在每个推断步骤 compute deeper-layer decoder features的时候，以确保不同推断步骤的特征是协调的。我们使用了两个状态对模型在多种视觉语言任务上进行评估。我们发现，我们的方法可以降低总推断时间的30%-60%，与基eline相比，保持相对或更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Question-Multiple-Answer-Text-VQA"><a href="#Multiple-Question-Multiple-Answer-Text-VQA" class="headerlink" title="Multiple-Question Multiple-Answer Text-VQA"></a>Multiple-Question Multiple-Answer Text-VQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08622">http://arxiv.org/abs/2311.08622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jha1990/VQA-Multimodal-AI">https://github.com/jha1990/VQA-Multimodal-AI</a></li>
<li>paper_authors: Peng Tang, Srikar Appalaraju, R. Manmatha, Yusheng Xie, Vijay Mahadevan<br>for:多个问题和多个答案（MQMA）是一种新的文本-VQA方法，用于在encoder-decoder transformer模型中进行文本理解和图像理解。methods:MQMA方法使用多个问题和内容作为输入，并在encoder和decoder中进行自动进程的推理，以同时预测多个答案。我们对标准encoder-decoder transformer模型进行了一些新的建模修改以支持MQMA。results:MQMA预训练模型在多个文本-VQA数据集上达到了当前最佳result，具体是OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%)的绝对改进。<details>
<summary>Abstract</summary>
We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do text-VQA in encoder-decoder transformer models. The text-VQA task requires a model to answer a question by understanding multi-modal content: text (typically from OCR) and an associated image. To the best of our knowledge, almost all previous approaches for text-VQA process a single question and its associated content to predict a single answer. In order to answer multiple questions from the same image, each question and content are fed into the model multiple times. In contrast, our proposed MQMA approach takes multiple questions and content as input at the encoder and predicts multiple answers at the decoder in an auto-regressive manner at the same time. We make several novel architectural modifications to standard encoder-decoder transformers to support MQMA. We also propose a novel MQMA denoising pre-training task which is designed to teach the model to align and delineate multiple questions and content with associated answers. MQMA pre-trained model achieves state-of-the-art results on multiple text-VQA datasets, each with strong baselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%) absolute improvements over the previous state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
我们提出了多问题多答案（MQMA），一种新的方法来实现编码器-解码器变换器模型中的文本-VQA任务。文本-VQA任务需要模型理解多Modal内容：文本（通常来自OCR）和相关的图像。根据我们所知，前一代的approaches都是处理单个问题和其相关的内容来预测单个答案。而我们提出的MQMA方法则是在编码器中输入多个问题和内容，并在解码器中预测多个答案，这些答案将在自动进行重复的情况下同时预测。我们对标准编码器-解码器变换器模型进行了一些新的建议，以支持MQMA。我们还提出了一个MQMA净化预训练任务，用于教导模型将多个问题和内容与相应的答案进行对齐和分割。MQMA预训练模型在多个文本-VQA数据集上达到了最佳状态，每个数据集都有强的基eline。具体来说，在OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%)中的绝对提升。
</details></li>
</ul>
<hr>
<h2 id="Toucan-Token-Aware-Character-Level-Language-Modeling"><a href="#Toucan-Token-Aware-Character-Level-Language-Modeling" class="headerlink" title="Toucan: Token-Aware Character Level Language Modeling"></a>Toucan: Token-Aware Character Level Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08620">http://arxiv.org/abs/2311.08620</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Fleshman, Benjamin Van Durme</li>
<li>for: 这篇论文主要是为了提高Character-level语言模型的效率，使其能够更快地生成字符串。</li>
<li>methods: 这篇论文提出了一种基于”token-aware”的修改方法，可以帮助Character-level语言模型更好地处理长字符串。这种方法通过学习将字符串转换为token来实现，而不需要额外的tokenizer。</li>
<li>results: 对比于先前的工作，这种方法可以提高字符生成速度，而无需减少语言模型的表现。此外，这种方法还可以处理更长的字符串，并且可以生成更多的长字符串。code和项目可以在<a target="_blank" rel="noopener" href="https://nlp.jhu.edu/nuggets/%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://nlp.jhu.edu/nuggets/上获取。</a><details>
<summary>Abstract</summary>
Character-level language models obviate the need for separately trained tokenizers, but efficiency suffers from longer sequence lengths. Learning to combine character representations into tokens has made training these models more efficient, but they still require decoding characters individually. We propose Toucan, an augmentation to character-level models to make them "token-aware". Comparing our method to prior work, we demonstrate significant speed-ups in character generation without a loss in language modeling performance. We then explore differences between our learned dynamic tokenization of character sequences with popular fixed vocabulary solutions such as Byte-Pair Encoding and WordPiece, finding our approach leads to a greater amount of longer sequences tokenized as single items. Our project and code are available at https://nlp.jhu.edu/nuggets/.
</details>
<details>
<summary>摘要</summary>
⟨SYS⟩文本翻译成简化中文。Character-level语言模型取消了分配单独的tokenizer的需要，但是序列长度变长会导致效率下降。学习将字符表示合并到 tokens中的方法可以使训练这些模型更加高效，但它们仍然需要解码字符个个。我们提出了Toucan，一种将字符级模型转化为“字符认识”的增强。与先前的工作进行比较，我们示出了不失语言模型表现的速度提升。然后，我们探讨了我们学习的动态tokenization和固定词库解决方案如Byte-PairEncoding和WordPiece的 diferencias，发现我们的方法可以处理更多的更长的序列。我们的项目和代码可以在https://nlp.jhu.edu/nuggets/找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalizable-SER-Soft-Labeling-and-Data-Augmentation-for-Modeling-Temporal-Emotion-Shifts-in-Large-Scale-Multilingual-Speech"><a href="#Towards-Generalizable-SER-Soft-Labeling-and-Data-Augmentation-for-Modeling-Temporal-Emotion-Shifts-in-Large-Scale-Multilingual-Speech" class="headerlink" title="Towards Generalizable SER: Soft Labeling and Data Augmentation for Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech"></a>Towards Generalizable SER: Soft Labeling and Data Augmentation for Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08607">http://arxiv.org/abs/2311.08607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spaghettisystems/emotion_whisper">https://github.com/spaghettisystems/emotion_whisper</a></li>
<li>paper_authors: Mohamed Osman, Tamer Nadeem, Ghada Khoriba</li>
<li>for: 这项研究的目的是提高人机交互的进步，通过识别口头沟通中的情感。</li>
<li>methods: 这项研究使用了16个不同的数据集，共计375小时的数据，包括英语、中文和日语等语言。研究采用软标注系统来捕捉情感的渐进强度。使用了Whisper编码器和启发自对比例学习的数据增强方法，注重情感的时间动态。</li>
<li>results: 研究在四个多语言数据集上进行验证，显示出了显著的零基eline泛化性。发布了开源模型权重和初步的良好结果，并在Hume-Prosody上进行了细化调整。<details>
<summary>Abstract</summary>
Recognizing emotions in spoken communication is crucial for advanced human-machine interaction. Current emotion detection methodologies often display biases when applied cross-corpus. To address this, our study amalgamates 16 diverse datasets, resulting in 375 hours of data across languages like English, Chinese, and Japanese. We propose a soft labeling system to capture gradational emotional intensities. Using the Whisper encoder and data augmentation methods inspired by contrastive learning, our method emphasizes the temporal dynamics of emotions. Our validation on four multilingual datasets demonstrates notable zero-shot generalization. We publish our open source model weights and initial promising results after fine-tuning on Hume-Prosody.
</details>
<details>
<summary>摘要</summary>
感知情感在人机交互中的重要性。当前的情感检测方法经常在不同的文本库中显示偏见。为解决这个问题，我们的研究将16种不同的数据集融合起来，共计375小时的数据，涵盖英语、中文和日语等语言。我们提议一种柔化标签系统，以捕捉情感的柔化强度。使用Whisper编码器和基于对比学习的数据增强方法，我们的方法强调情感的时间动态。我们的验证在四种多语言数据集上表现出了显著的零shot泛化。我们将我们的开源模型 веса和初步成果发布在Hume-Prosody上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.CL_2023_11_15/" data-id="clpahu71r00f33h885h1f4i4t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.LG_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T10:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.LG_2023_11_15/">cs.LG - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Beyond-PCA-A-Probabilistic-Gram-Schmidt-Approach-to-Feature-Extraction"><a href="#Beyond-PCA-A-Probabilistic-Gram-Schmidt-Approach-to-Feature-Extraction" class="headerlink" title="Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction"></a>Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09386">http://arxiv.org/abs/2311.09386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bahram Yaghooti, Netanel Raviv, Bruno Sinopoli</li>
<li>for: 提取非线性特征，即在数据中存在非线性关系的特征提取，是无监督学习中的基本挑战。</li>
<li>methods: 我们提出了一种使用概率 Gram-Schmidt（PGS）类型正交化过程，以探测和映射出数据中的重复维度。 Specifically, 我们在任何函数家族（presumably captures the nonlinear dependencies in the data）上应用 PGS 过程，并构建一系列 covariance matrices，可以用来从主成分中除非linear dependencies，或者标识新的大异常方向。</li>
<li>results: 我们提供了两种方法，可以从数据中提取线性特征，同时去除非线性关系。 在第一种情况下，我们证明在某些假设下，这些算法可以探测和除非线性关系，当这些关系在数据中的线性拟合中。 在第二种情况下，我们提供了信息学理论保证，以 entropy reduction 的形式。 我们在 sintetic 和实际数据上进行了 simulations，并证明了我们的方法在 variance maximization 和 classification 算法的性能上具有提高。<details>
<summary>Abstract</summary>
Linear feature extraction at the presence of nonlinear dependencies among the data is a fundamental challenge in unsupervised learning. We propose using a Probabilistic Gram-Schmidt (PGS) type orthogonalization process in order to detect and map out redundant dimensions. Specifically, by applying the PGS process over any family of functions which presumably captures the nonlinear dependencies in the data, we construct a series of covariance matrices that can either be used to remove those dependencies from the principal components, or to identify new large-variance directions. In the former case, we prove that under certain assumptions the resulting algorithms detect and remove nonlinear dependencies whenever those dependencies lie in the linear span of the chosen function family. In the latter, we provide information-theoretic guarantees in terms of entropy reduction. Both proposed methods extract linear features from the data while removing nonlinear redundancies. We provide simulation results on synthetic and real-world datasets which show improved performance over PCA and state-of-the-art linear feature extraction algorithms, both in terms of variance maximization of the extracted features, and in terms of improved performance of classification algorithms.
</details>
<details>
<summary>摘要</summary>
In the former case, we prove that under certain assumptions, the resulting algorithms detect and remove nonlinear dependencies whenever those dependencies lie in the linear span of the chosen function family. In the latter, we provide information-theoretic guarantees in terms of entropy reduction. Both proposed methods extract linear features from the data while removing nonlinear redundancies.We provide simulation results on synthetic and real-world datasets that show improved performance over PCA and state-of-the-art linear feature extraction algorithms, both in terms of variance maximization of the extracted features and improved performance of classification algorithms.我们使用Probabilistic Gram-Schmidt（PGS）类型的正交化过程来检测和映射出数据中的非线性依赖关系。通过将PGS过程应用到数据中的任何函数家族，我们可以构造一系列协 variance 矩阵，这些矩阵可以用于从主成分中除非线性依赖关系，或者标识新的大协变量方向。在前一种情况下，我们证明在某些假设下，得到的算法可以检测并除非线性依赖关系，当这些依赖关系 lie 在选择的函数家族的线性扩展上。在另一种情况下，我们提供信息理论保证，以 entropy 减少为标准。两种提议的方法都可以从数据中提取线性特征，同时去除非线性冗余。我们对 synthetic 和实际世界数据进行了丰富的 simulations，结果表明，我们的方法可以超过PCA和当前的线性特征提取算法，包括 variance 最大化的特征提取和分类算法的性能提高。
</details></li>
</ul>
<hr>
<h2 id="Time-dependent-Probabilistic-Generative-Models-for-Disease-Progression"><a href="#Time-dependent-Probabilistic-Generative-Models-for-Disease-Progression" class="headerlink" title="Time-dependent Probabilistic Generative Models for Disease Progression"></a>Time-dependent Probabilistic Generative Models for Disease Progression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09369">http://arxiv.org/abs/2311.09369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Onintze Zaballa, Aritz Pérez, Elisa Gómez-Inhiesto, Teresa Acaiturri-Ayesta, Jose A. Lozano</li>
<li>for: 这篇论文的目的是要利用电子健康纪录中的资料来监控病人的健康趋势过程。</li>
<li>methods: 这篇论文使用了Markov运动模型来理解疾病的深层模式和动态。</li>
<li>results: 这篇论文的结果显示了模型的有效性，可以从数据中回传出深层模式，并且准确地模型了医疗事件之间的不规则时间间隔。<details>
<summary>Abstract</summary>
Electronic health records contain valuable information for monitoring patients' health trajectories over time. Disease progression models have been developed to understand the underlying patterns and dynamics of diseases using these data as sequences. However, analyzing temporal data from EHRs is challenging due to the variability and irregularities present in medical records. We propose a Markovian generative model of treatments developed to (i) model the irregular time intervals between medical events; (ii) classify treatments into subtypes based on the patient sequence of medical events and the time intervals between them; and (iii) segment treatments into subsequences of disease progression patterns. We assume that sequences have an associated structure of latent variables: a latent class representing the different subtypes of treatments; and a set of latent stages indicating the phase of progression of the treatments. We use the Expectation-Maximization algorithm to learn the model, which is efficiently solved with a dynamic programming-based method. Various parametric models have been employed to model the time intervals between medical events during the learning process, including the geometric, exponential, and Weibull distributions. The results demonstrate the effectiveness of our model in recovering the underlying model from data and accurately modeling the irregular time intervals between medical actions.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose a Markovian generative model of treatments that can (i) model the irregular time intervals between medical events, (ii) classify treatments into subtypes based on the patient sequence of medical events and the time intervals between them, and (iii) segment treatments into subsequences of disease progression patterns. We assume that sequences have an associated structure of latent variables, including a latent class representing the different subtypes of treatments and a set of latent stages indicating the phase of progression of the treatments.We use the Expectation-Maximization algorithm to learn the model, which is efficiently solved with a dynamic programming-based method. During the learning process, we employ various parametric models to model the time intervals between medical events, including the geometric, exponential, and Weibull distributions. The results demonstrate the effectiveness of our model in recovering the underlying model from data and accurately modeling the irregular time intervals between medical actions.Here is the translation in Simplified Chinese:电子健康记录 (EHRs) 包含价值的健康轨迹信息，可以用来监测患者的健康变化趋势。研究人员已经开发了疾病进程模型，以了解医疗记录中的下列特征和动态。然而，分析医疗记录中的时间序列数据具有挑战性，因为它们具有不规则和不稳定的特征。为了解决这个问题，我们提出了一种Markov链模型，可以 (i) 模拟医疗记录中的不规则时间间隔， (ii) 根据患者的医疗记录序列和时间间隔来类型化治疗， (iii) 将治疗分为疾病进程模式的子序列。我们假设序列具有隐藏变量的结构，包括不同的治疗类型和疾病进程阶段。我们使用了Expectation-Maximization算法来学习模型，并使用动态规划方法来效率地解决问题。在学习过程中，我们采用了不同的参数模型来模拟医疗记录中的时间间隔，包括 geometric、exponential 和 Weibull 分布。结果表明，我们的模型可以准确地从数据中回归下列模型，并且能够准确地模拟医疗记录中的不规则时间间隔。
</details></li>
</ul>
<hr>
<h2 id="Nondestructive-quantitative-viability-analysis-of-3D-tissue-cultures-using-machine-learning-image-segmentation"><a href="#Nondestructive-quantitative-viability-analysis-of-3D-tissue-cultures-using-machine-learning-image-segmentation" class="headerlink" title="Nondestructive, quantitative viability analysis of 3D tissue cultures using machine learning image segmentation"></a>Nondestructive, quantitative viability analysis of 3D tissue cultures using machine learning image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09354">http://arxiv.org/abs/2311.09354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kylie J. Trettner, Jeremy Hsieh, Weikun Xiao, Jerry S. H. Lee, Andrea M. Armani</li>
<li>for: 本研究旨在开发一种基于图像处理的细胞生存度评估方法，以自动评估细胞群体的生存度和响应于刺激的可能性。</li>
<li>methods: 本研究使用图像处理算法来评估细胞生存度，不需要使用各种抑衰指标。研究者使用高内容成像系统拍摄照片，并使用人工智能模型来自动识别细胞生存度。</li>
<li>results: 研究发现，使用图像处理算法可以准确地评估细胞生存度，并且可以减少分析时间97%。这种方法可以在不同的细胞 культура条件下进行评估，并且可以帮助提高生物学和临床研究中的细胞 культура分析的可重复性和可靠性。<details>
<summary>Abstract</summary>
Ascertaining the collective viability of cells in different cell culture conditions has typically relied on averaging colorimetric indicators and is often reported out in simple binary readouts. Recent research has combined viability assessment techniques with image-based deep-learning models to automate the characterization of cellular properties. However, further development of viability measurements to assess the continuity of possible cellular states and responses to perturbation across cell culture conditions is needed. In this work, we demonstrate an image processing algorithm for quantifying cellular viability in 3D cultures without the need for assay-based indicators. We show that our algorithm performs similarly to a pair of human experts in whole-well images over a range of days and culture matrix compositions. To demonstrate potential utility, we perform a longitudinal study investigating the impact of a known therapeutic on pancreatic cancer spheroids. Using images taken with a high content imaging system, the algorithm successfully tracks viability at the individual spheroid and whole-well level. The method we propose reduces analysis time by 97% in comparison to the experts. Because the method is independent of the microscope or imaging system used, this approach lays the foundation for accelerating progress in and for improving the robustness and reproducibility of 3D culture analysis across biological and clinical research.
</details>
<details>
<summary>摘要</summary>
通过评估细胞群体的可活性在不同的细胞文化条件下，通常是通过均值色imetric指标来进行评估，并常常报告出简单的二进制输出。然而，现有的可活性评估技术还需要进一步发展，以评估细胞群体的连续性和响应于干扰的可能性。在这项工作中，我们提出了一种图像处理算法，可以无需各种指标来评估细胞可活性。我们证明了我们的算法与两名人类专家的总体评估结果相似，在不同的日期和细胞Matrix组成下。为了展示可能的实用性，我们进行了一项 longitudinal 研究，investigating the impact of a known therapeutic on pancreatic cancer spheroids。使用高Content imaging系统拍摄的图像，我们的算法成功地跟踪了细胞可活性的个体硬化和整个Well水平。我们的方法可以比人类专家减少分析时间约97%。因为该方法不виси于使用哪种 Mikroskop 或 imaging系统，这种方法可以为生物和临床研究提供加速进步的基础。
</details></li>
</ul>
<hr>
<h2 id="Challenges-for-Predictive-Modeling-with-Neural-Network-Techniques-using-Error-Prone-Dietary-Intake-Data"><a href="#Challenges-for-Predictive-Modeling-with-Neural-Network-Techniques-using-Error-Prone-Dietary-Intake-Data" class="headerlink" title="Challenges for Predictive Modeling with Neural Network Techniques using Error-Prone Dietary Intake Data"></a>Challenges for Predictive Modeling with Neural Network Techniques using Error-Prone Dietary Intake Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09338">http://arxiv.org/abs/2311.09338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dylan Spicker, Amir Nazemi, Joy Hutchinson, Paul Fieguth, Sharon I. Kirkpatrick, Michael Wallace, Kevin W. Dodd</li>
<li>for: 这个论文旨在探讨食物摄入数据如何影响健康关系，但这些数据经常受到测量误差的影响，导致实际关系与论文中的关系不同。</li>
<li>methods: 这篇论文使用神经网络模型来捕捉食物摄入数据中的复杂相互作用，但测量误差会对神经网络模型的性能产生负面影响。</li>
<li>results: 这篇论文发现，在受测量误差影响的情况下，神经网络模型的预测性能会受到影响，需要更多的数据和更好的方法来改善模型的性能。<details>
<summary>Abstract</summary>
Dietary intake data are routinely drawn upon to explore diet-health relationships. However, these data are often subject to measurement error, distorting the true relationships. Beyond measurement error, there are likely complex synergistic and sometimes antagonistic interactions between different dietary components, complicating the relationships between diet and health outcomes. Flexible models are required to capture the nuance that these complex interactions introduce. This complexity makes research on diet-health relationships an appealing candidate for the application of machine learning techniques, and in particular, neural networks. Neural networks are computational models that are able to capture highly complex, nonlinear relationships so long as sufficient data are available. While these models have been applied in many domains, the impacts of measurement error on the performance of predictive modeling has not been systematically investigated. However, dietary intake data are typically collected using self-report methods and are prone to large amounts of measurement error. In this work, we demonstrate the ways in which measurement error erodes the performance of neural networks, and illustrate the care that is required for leveraging these models in the presence of error. We demonstrate the role that sample size and replicate measurements play on model performance, indicate a motivation for the investigation of transformations to additivity, and illustrate the caution required to prevent model overfitting. While the past performance of neural networks across various domains make them an attractive candidate for examining diet-health relationships, our work demonstrates that substantial care and further methodological development are both required to observe increased predictive performance when applying these techniques, compared to more traditional statistical procedures.
</details>
<details>
<summary>摘要</summary>
Dietary intake data  Routinely 被用来探索饮食和健康关系。 However, these data are often subject to measurement error, which distorts the true relationships. Beyond measurement error, there are likely complex synergistic and sometimes antagonistic interactions between different dietary components, complicating the relationships between diet and health outcomes. Flexible models are required to capture the nuance that these complex interactions introduce. This complexity makes research on diet-health relationships an appealing candidate for the application of machine learning techniques, and in particular, neural networks. Neural networks are computational models that are able to capture highly complex, nonlinear relationships so long as sufficient data are available. While these models have been applied in many domains, the impacts of measurement error on the performance of predictive modeling have not been systematically investigated. However, dietary intake data are typically collected using self-report methods and are prone to large amounts of measurement error. In this work, we demonstrate the ways in which measurement error erodes the performance of neural networks, and illustrate the care that is required for leveraging these models in the presence of error. We demonstrate the role that sample size and replicate measurements play on model performance, indicate a motivation for the investigation of transformations to additivity, and illustrate the caution required to prevent model overfitting. While the past performance of neural networks across various domains make them an attractive candidate for examining diet-health relationships, our work demonstrates that substantial care and further methodological development are both required to observe increased predictive performance when applying these techniques, compared to more traditional statistical procedures.Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Analysis-of-Machine-Learning-Models-for-Early-Detection-of-Hospital-Acquired-Infections"><a href="#A-Comparative-Analysis-of-Machine-Learning-Models-for-Early-Detection-of-Hospital-Acquired-Infections" class="headerlink" title="A Comparative Analysis of Machine Learning Models for Early Detection of Hospital-Acquired Infections"></a>A Comparative Analysis of Machine Learning Models for Early Detection of Hospital-Acquired Infections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09329">http://arxiv.org/abs/2311.09329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan Harvey, Junzi Dong, Erina Ghosh, Ali Samadani</li>
<li>for: 这个研究旨在比较两种护理机器学习模型，以便在早期检测医院获得的感染（HAIs）中提供重要的改进。</li>
<li>methods: 这两种模型都使用不同的感染标签定义、选择受试者和预测方案。</li>
<li>results: 这个研究发现这两种模型在预测HAIs时存在一定的一致性和冲突。<details>
<summary>Abstract</summary>
As more and more infection-specific machine learning models are developed and planned for clinical deployment, simultaneously running predictions from different models may provide overlapping or even conflicting information. It is important to understand the concordance and behavior of parallel models in deployment. In this study, we focus on two models for the early detection of hospital-acquired infections (HAIs): 1) the Infection Risk Index (IRI) and 2) the Ventilator-Associated Pneumonia (VAP) prediction model. The IRI model was built to predict all HAIs, whereas the VAP model identifies patients at risk of developing ventilator-associated pneumonia. These models could make important improvements in patient outcomes and hospital management of infections through early detection of infections and in turn, enable early interventions. The two models vary in terms of infection label definition, cohort selection, and prediction schema. In this work, we present a comparative analysis between the two models to characterize concordances and confusions in predicting HAIs by these models. The learnings from this study will provide important findings for how to deploy multiple concurrent disease-specific models in the future.
</details>
<details>
<summary>摘要</summary>
随着更多的感染病特定机器学习模型的开发和规划，同时运行不同模型的预测可能会提供重叠或甚至矛盾的信息。理解并发型模型在部署过程中的协调和行为非常重要。本研究专注于两种早期检测医院获得感染（HAIs）的模型：1）感染风险指数（IRI）模型和2）呼吸器相关肺炎预测模型。IRI模型预测所有HAIs，而VAP预测模型标识患有呼吸器相关肺炎的患者。这两种模型可以通过早期检测感染并提供早期干预，从而提高患者的结果和医院对感染的管理。这两种模型在感染标签定义、样本选择和预测方案方面存在差异。本研究通过对这两种模型进行比较分析，描述这两种模型在预测HAIs方面的协调和混乱。本研究的发现将为未来多个同时部署疾病特定模型提供重要的发现。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Approach-to-Learning-Ising-Models-Beyond-Independence-and-Bounded-Width"><a href="#A-Unified-Approach-to-Learning-Ising-Models-Beyond-Independence-and-Bounded-Width" class="headerlink" title="A Unified Approach to Learning Ising Models: Beyond Independence and Bounded Width"></a>A Unified Approach to Learning Ising Models: Beyond Independence and Bounded Width</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09197">http://arxiv.org/abs/2311.09197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Gaitonde, Elchanan Mossel</li>
<li>for: 该论文目的是提高现有的恒等模型参数学习算法，以便在不满足现有假设的情况下，从数据中提取模型参数。</li>
<li>methods: 该论文使用了节点wise逻辑回归算法，该算法可以在各种新的情况下成功地提取模型参数，包括各种本地马可夫链生成的数据，以及随机的温度范围内的玻璃杯模型。</li>
<li>results: 该论文的结果表明，使用节点wise逻辑回归算法可以在各种新的情况下提取模型参数，并且可以在较低的样本复杂度下达到最佳的样本复杂度。此外，该论文还提供了一些新的 guarantees for learning from adversarial Glauber dynamics。<details>
<summary>Abstract</summary>
We revisit the problem of efficiently learning the underlying parameters of Ising models from data. Current algorithmic approaches achieve essentially optimal sample complexity when given i.i.d. samples from the stationary measure and the underlying model satisfies "width" bounds on the total $\ell_1$ interaction involving each node. We show that a simple existing approach based on node-wise logistic regression provably succeeds at recovering the underlying model in several new settings where these assumptions are violated:   (1) Given dynamically generated data from a wide variety of local Markov chains, like block or round-robin dynamics, logistic regression recovers the parameters with optimal sample complexity up to $\log\log n$ factors. This generalizes the specialized algorithm of Bresler, Gamarnik, and Shah [IEEE Trans. Inf. Theory'18] for structure recovery in bounded degree graphs from Glauber dynamics.   (2) For the Sherrington-Kirkpatrick model of spin glasses, given $\mathsf{poly}(n)$ independent samples, logistic regression recovers the parameters in most of the known high-temperature regime via a simple reduction to weaker structural properties of the measure. This improves on recent work of Anari, Jain, Koehler, Pham, and Vuong [ArXiv'23] which gives distribution learning at higher temperature.   (3) As a simple byproduct of our techniques, logistic regression achieves an exponential improvement in learning from samples in the M-regime of data considered by Dutt, Lokhov, Vuffray, and Misra [ICML'21] as well as novel guarantees for learning from the adversarial Glauber dynamics of Chin, Moitra, Mossel, and Sandon [ArXiv'23].   Our approach thus significantly generalizes the elegant analysis of Wu, Sanghavi, and Dimakis [Neurips'19] without any algorithmic modification.
</details>
<details>
<summary>摘要</summary>
我们回到了从数据中划出隐藏模型的问题中。现有的算法方法可以实现基本的体积缩小Sample complexity， provided that the data is i.i.d. from the stationary distribution and the underlying model satisfies certain "width" bounds on the total $\ell_1$ interaction involving each node. 我们显示了一个简单的现有方法，即每个节点的逻辑回传 regression，可以在一些新的设定中成功地从数据中弹出隐藏模型：（1）对于各种本地Markov链的生成数据，例如对于块或轮转动态，逻辑回传 regression可以从数据中提取隐藏模型的parameters，具有最佳的体积缩小因素，只有 $\log\log n$ 因素。这标准化了 Bresler、Gamarnik 和 Shah 的特殊算法 [IEEE Trans. Inf. Theory'18]  для结构回传在受限度度Graph中。（2）关于玻璃玻璃产生的磁矩链模型，对于大多数高温区域，逻辑回传 regression可以从 $\mathsf{poly}(n)$ 独立数据中提取模型的parameters。这意味着我们可以在较高的温度区域中进行分布学习，进一步超越了 Anari、Jain、Koehler、Pham 和 Vuong 的最近研究 [ArXiv'23]。（3）我们的方法还具有一个简单的副产物，即逻辑回传 regression可以从 M-regime中的数据中弹出模型的parameters，并且在 Dutt、Lokhov、Vuffray 和 Misra [ICML'21] 中考虑的数据中具有很好的学习效果。此外，我们还提供了一些新的保证，允许在Chin、Moitra、Mossel 和 Sandon 的 adversarial Glauber dynamics [ArXiv'23] 中进行学习。我们的方法因此可以广泛应用在 Wu、Sanghavi 和 Dimakis [Neurips'19] 的数据中，而不需要任何算法修改。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Curriculum-Generation-for-Autonomous-Reinforcement-Learning-without-Task-Specific-Knowledge"><a href="#Self-Supervised-Curriculum-Generation-for-Autonomous-Reinforcement-Learning-without-Task-Specific-Knowledge" class="headerlink" title="Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge"></a>Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09195">http://arxiv.org/abs/2311.09195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sang-Hyun Lee, Seung-Woo Seo</li>
<li>for: 本研究旨在提高现有激励学习算法在真实场景中的应用，解决需要在每个话语中重置环境的瓶颈。</li>
<li>methods: 本研究提出了一种基于自适应激励学习（ARL）算法，生成适应学习进程中的课程。这些课程可以根据执行策略的学习进程来减少需要人工重置的数量，但是它们需要任务特定的知识，如预先定义的初始状态或重置奖励函数。本研究提出了一种不需要任务特定知识的ARL算法，可以自动生成适应学习进程中的课程。</li>
<li>results: 我们的实验结果表明，我们的ARL算法可以生成适应学习进程中的课程，使得执行策略可以自动重置到多样化和有用的初始状态。我们引入了一个成功识别器，以便从每个初始状态中预测执行策略后的成功概率。成功识别器通过在一种自适应的自我监督模式下训练，使得执行策略可以快速地解决缺乏奖励的迷宫探索任务，并且表现出了比基eline的更好的性能。<details>
<summary>Abstract</summary>
A significant bottleneck in applying current reinforcement learning algorithms to real-world scenarios is the need to reset the environment between every episode. This reset process demands substantial human intervention, making it difficult for the agent to learn continuously and autonomously. Several recent works have introduced autonomous reinforcement learning (ARL) algorithms that generate curricula for jointly training reset and forward policies. While their curricula can reduce the number of required manual resets by taking into account the agent's learning progress, they rely on task-specific knowledge, such as predefined initial states or reset reward functions. In this paper, we propose a novel ARL algorithm that can generate a curriculum adaptive to the agent's learning progress without task-specific knowledge. Our curriculum empowers the agent to autonomously reset to diverse and informative initial states. To achieve this, we introduce a success discriminator that estimates the success probability from each initial state when the agent follows the forward policy. The success discriminator is trained with relabeled transitions in a self-supervised manner. Our experimental results demonstrate that our ARL algorithm can generate an adaptive curriculum and enable the agent to efficiently bootstrap to solve sparse-reward maze navigation tasks, outperforming baselines with significantly fewer manual resets.
</details>
<details>
<summary>摘要</summary>
Current reinforcement learning algorithms have a major obstacle to applying them to real-world scenarios: the need to reset the environment between every episode. This reset process requires a lot of human intervention, making it difficult for the agent to learn continuously and autonomously. Several recent works have proposed autonomous reinforcement learning (ARL) algorithms that generate curricula for jointly training reset and forward policies. These curricula can reduce the number of required manual resets by taking into account the agent's learning progress, but they rely on task-specific knowledge, such as predefined initial states or reset reward functions.In this paper, we propose a novel ARL algorithm that can generate a curriculum adaptive to the agent's learning progress without task-specific knowledge. Our curriculum enables the agent to autonomously reset to diverse and informative initial states. To achieve this, we introduce a success discriminator that estimates the success probability from each initial state when the agent follows the forward policy. The success discriminator is trained with relabeled transitions in a self-supervised manner. Our experimental results show that our ARL algorithm can generate an adaptive curriculum and enable the agent to efficiently bootstrap to solve sparse-reward maze navigation tasks, outperforming baselines with significantly fewer manual resets.
</details></li>
</ul>
<hr>
<h2 id="Approaching-adverse-event-detection-utilizing-transformers-on-clinical-time-series"><a href="#Approaching-adverse-event-detection-utilizing-transformers-on-clinical-time-series" class="headerlink" title="Approaching adverse event detection utilizing transformers on clinical time-series"></a>Approaching adverse event detection utilizing transformers on clinical time-series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09165">http://arxiv.org/abs/2311.09165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helge Fredriksen, Per Joel Burman, Ashenafi Woldaregay, Karl Øyvind Mikalsen, Ståle Nymo</li>
<li>for: 预测患者的临床趋势并避免不良事件</li>
<li>methods: 使用自动化检测系统，基于STraTS transformer架构对时间序列数据进行 Representation，并使用各种聚类技术来探索患者的临床进程分型</li>
<li>results: 初步结果显示系统能够准确地检测异常情况，但需要更多的患者数据来进行更全面的评估系统性能<details>
<summary>Abstract</summary>
Patients being admitted to a hospital will most often be associated with a certain clinical development during their stay. However, there is always a risk of patients being subject to the wrong diagnosis or to a certain treatment not pertaining to the desired effect, potentially leading to adverse events. Our research aims to develop an anomaly detection system for identifying deviations from expected clinical trajectories. To address this goal we analyzed 16 months of vital sign recordings obtained from the Nordland Hospital Trust (NHT). We employed an self-supervised framework based on the STraTS transformer architecture to represent the time series data in a latent space. These representations were then subjected to various clustering techniques to explore potential patient phenotypes based on their clinical progress. While our preliminary results from this ongoing research are promising, they underscore the importance of enhancing the dataset with additional demographic information from patients. This additional data will be crucial for a more comprehensive evaluation of the method's performance.
</details>
<details>
<summary>摘要</summary>
patients being admitted to a hospital will often be associated with a certain clinical development during their stay, but there is always a risk of patients being misdiagnosed or receiving the wrong treatment, which could lead to adverse events. our research aims to develop an anomaly detection system to identify deviations from expected clinical trajectories. to achieve this goal, we analyzed 16 months of vital sign recordings from the Nordland Hospital Trust (nht). we used a self-supervised framework based on the STraTS transformer architecture to represent the time series data in a latent space. these representations were then subjected to various clustering techniques to explore potential patient phenotypes based on their clinical progress. while our preliminary results are promising, we recognize the need to enhance the dataset with additional demographic information from patients to evaluate the method's performance more comprehensively.
</details></li>
</ul>
<hr>
<h2 id="Improved-Sparse-Ising-Optimization"><a href="#Improved-Sparse-Ising-Optimization" class="headerlink" title="Improved Sparse Ising Optimization"></a>Improved Sparse Ising Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09275">http://arxiv.org/abs/2311.09275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kenneth M. Zick</li>
<li>for: 这个论文是为了解决含有大量零值的尼饶问题（Sparse Ising problem），这类问题在物流、吸引物理和深度波尔谱网络训练等领域都有广泛的应用，但可能会很困难和缓慢地解决。</li>
<li>methods: 该论文提出了一种新的落差搜索算法，用于解决含有大量零值的尼饶问题。该算法在大型稀疏实例上进行了测试，并实现了比以往报道的速度和准确性（如托笔会的模拟缺乏机制和breakout本地搜索）的至少2-4个数量级的提升。</li>
<li>results: 据论文所示，该新算法在一些长期的标准实例上达到了更高的性能，并在两个实例（G72和G77）上发现了更好的解决方案， Solution bitstrings证明了这两个最佳解决方案。这些数据表明，该算法可能会推动稀疏尼饶性能的前沿，并为AI工具箱、决策系统和算法库提供新的可能性。<details>
<summary>Abstract</summary>
Sparse Ising problems can be found in application areas such as logistics, condensed matter physics and training of deep Boltzmann networks, but can be very difficult to tackle with high efficiency and accuracy. This report presents new data demonstrating significantly higher performance on some longstanding benchmark problems with up to 20,000 variables. The data come from a new heuristic algorithm tested on the large sparse instances from the Gset benchmark suite. Relative to leading reported combinations of speed and accuracy (e.g., from Toshiba's Simulated Bifurcation Machine and Breakout Local Search), a proof-of-concept implementation reached targets 2-4 orders of magnitude faster. For two instances (G72 and G77) the new algorithm discovered a better solution than all previously reported values. Solution bitstrings confirming these two best solutions are provided. The data suggest exciting possibilities for pushing the sparse Ising performance frontier to potentially strengthen algorithm portfolios, AI toolkits and decision-making systems.
</details>
<details>
<summary>摘要</summary>
稀疏各种问题（Sparse Ising problems）可以在物流、吸积物理和深度波尔谱网络训练等应用领域中找到，但它们可以很难以使用高效率和准确性解决。本报告提供新的数据，表明在一些长期的标准测试问题上（有多达20,000个变量）表现出了明显的性能提升。这些数据来自一种新的启发式算法，在Gset benchmark集中的大 sparse instances上进行测试。相比之下，现有的速度和准确性的报道（如东芝的模拟分支机器和Breakout本地搜索），一个证明原型实现的运行速度比之下，让人感到惊叹。对G72和G77两个实例，新算法发现了比之前所报道的更好的解决方案。解决方案的位 bitstring证明了这两个最佳解决方案。数据表明，这些新成果可能会推动稀疏各种问题的性能前沿，并可能增强算法库、人工智能工具箱和决策系统。
</details></li>
</ul>
<hr>
<h2 id="Model-Agnostic-Explainable-Selective-Regression-via-Uncertainty-Estimation"><a href="#Model-Agnostic-Explainable-Selective-Regression-via-Uncertainty-Estimation" class="headerlink" title="Model Agnostic Explainable Selective Regression via Uncertainty Estimation"></a>Model Agnostic Explainable Selective Regression via Uncertainty Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09145">http://arxiv.org/abs/2311.09145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Pugnana, Carlos Mougan, Dan Saattrup Nielsen</li>
<li>for: 提高机器学习系统的可靠性和可信度</li>
<li>methods: 使用模型独立非参数统计 ERROR 估计</li>
<li>results: 比状态艺术 selective regression 表现更佳，在 69 个数据集上进行了广泛的比较<details>
<summary>Abstract</summary>
With the wide adoption of machine learning techniques, requirements have evolved beyond sheer high performance, often requiring models to be trustworthy. A common approach to increase the trustworthiness of such systems is to allow them to refrain from predicting. Such a framework is known as selective prediction. While selective prediction for classification tasks has been widely analyzed, the problem of selective regression is understudied. This paper presents a novel approach to selective regression that utilizes model-agnostic non-parametric uncertainty estimation. Our proposed framework showcases superior performance compared to state-of-the-art selective regressors, as demonstrated through comprehensive benchmarking on 69 datasets. Finally, we use explainable AI techniques to gain an understanding of the drivers behind selective regression. We implement our selective regression method in the open-source Python package doubt and release the code used to reproduce our experiments.
</details>
<details>
<summary>摘要</summary>
With the widespread adoption of machine learning techniques, requirements have evolved beyond sheer high performance, often requiring models to be trustworthy. A common approach to increase the trustworthiness of such systems is to allow them to refrain from predicting. Such a framework is known as selective prediction. While selective prediction for classification tasks has been widely analyzed, the problem of selective regression is understudied. This paper presents a novel approach to selective regression that utilizes model-agnostic non-parametric uncertainty estimation. Our proposed framework showcases superior performance compared to state-of-the-art selective regressors, as demonstrated through comprehensive benchmarking on 69 datasets. Finally, we use explainable AI techniques to gain an understanding of the drivers behind selective regression. We implement our selective regression method in the open-source Python package doubt and release the code used to reproduce our experiments.Here's the translation in Traditional Chinese:随着机器学习技术的广泛采用，需求已经进一步地进化，不仅需要高性能，更需要模型的可信度。一种常见的方法来增强模型的可信度是允许它们不 Predicting。这种框架称为选择性预测。选择性预测的分类任务已经广泛分析，但选择性回归却受到了较少的研究。本文提出了一种新的选择性回归方法，利用模型不 Parametric 不确定性估计。我们的提案的框架在69个数据集上展示了较高的性能，比state-of-the-art选择回归器更好。最后，我们使用可解释 AI 技术来理解选择性回归的驱动力。我们将选择性回归方法实现在 Open-source Python 套件 doubt 中，并发布了用于重现实验的代码。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-parameter-tracking-with-partial-state-observation"><a href="#Machine-learning-parameter-tracking-with-partial-state-observation" class="headerlink" title="Machine-learning parameter tracking with partial state observation"></a>Machine-learning parameter tracking with partial state observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09142">http://arxiv.org/abs/2311.09142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng-Meng Zhai, Mohammadamin Moradi, Bryan Glaz, Mulugeta Haile, Ying-Cheng Lai</li>
<li>for: 这 paper 用于描述一种基于机器学习的时间变化参数追踪方法，用于处理复杂和非线性动力系统中的参数变化。</li>
<li>methods: 该方法基于储存器计算和反问题形式，不需要知道系统的模型结构，可以从部分状态观测数据中直接学习时间变化参数。</li>
<li>results: 研究人员通过使用不同的动力系统数据集，证明了该方法可以准确地预测时间变化参数，并且可以处理低维度和高维度、Markovian和非Markovian的动力系统。<details>
<summary>Abstract</summary>
Complex and nonlinear dynamical systems often involve parameters that change with time, accurate tracking of which is essential to tasks such as state estimation, prediction, and control. Existing machine-learning methods require full state observation of the underlying system and tacitly assume adiabatic changes in the parameter. Formulating an inverse problem and exploiting reservoir computing, we develop a model-free and fully data-driven framework to accurately track time-varying parameters from partial state observation in real time. In particular, with training data from a subset of the dynamical variables of the system for a small number of known parameter values, the framework is able to accurately predict the parameter variations in time. Low- and high-dimensional, Markovian and non-Markovian nonlinear dynamical systems are used to demonstrate the power of the machine-learning based parameter-tracking framework. Pertinent issues affecting the tracking performance are addressed.
</details>
<details>
<summary>摘要</summary>
复杂和非线性动力系统经常包含时间变化的参数，正精准跟踪这些参数是 estado estimation、预测和控制等任务的关键。现有的机器学习方法需要完整的系统状态观察，而且假设参数的变化是adiabatic的。我们通过形式化反问题和利用储存计算，开发了一种没有模型假设和完全数据驱动的参数跟踪框架。特别是，通过训练数据来自系统动力变量的一个子集，这种框架可以在实时中高精度预测参数的时间变化。我们使用了低维度和高维度、Markovian和非Markovian的非线性动力系统来证明框架的能力。我们还讨论了影响跟踪性能的关键问题。
</details></li>
</ul>
<hr>
<h2 id="Causal-prediction-models-for-medication-safety-monitoring-The-diagnosis-of-vancomycin-induced-acute-kidney-injury"><a href="#Causal-prediction-models-for-medication-safety-monitoring-The-diagnosis-of-vancomycin-induced-acute-kidney-injury" class="headerlink" title="Causal prediction models for medication safety monitoring: The diagnosis of vancomycin-induced acute kidney injury"></a>Causal prediction models for medication safety monitoring: The diagnosis of vancomycin-induced acute kidney injury</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09137">http://arxiv.org/abs/2311.09137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Izak Yasrebi-de Kom, Joanna Klopotowska, Dave Dongelmans, Nicolette De Keizer, Kitty Jager, Ameen Abu-Hanna, Giovanni Cinà</li>
<li>for: 本研究旨在提供数据驱动的医疗安全监测支持，以改进现有的retrospective diagnosis of adverse drug events（ADEs）的方法。</li>
<li>methods: 本研究使用 causal modeling approach，包括 two key causal inference components：(1) 目标试验演示框架和 (2) 使用机器学习来估算个体化治疗效果。</li>
<li>results: 研究人员使用这种方法来估算vancomycin-induced acute kidney injury 中 ICU 病人的 causal probability（PC$_{low}$），并与医疗专家提供的qualitative estimates of the PC进行比较。<details>
<summary>Abstract</summary>
The current best practice approach for the retrospective diagnosis of adverse drug events (ADEs) in hospitalized patients relies on a full patient chart review and a formal causality assessment by multiple medical experts. This evaluation serves to qualitatively estimate the probability of causation (PC); the probability that a drug was a necessary cause of an adverse event. This practice is manual, resource intensive and prone to human biases, and may thus benefit from data-driven decision support. Here, we pioneer a causal modeling approach using observational data to estimate a lower bound of the PC (PC$_{low}$). This method includes two key causal inference components: (1) the target trial emulation framework and (2) estimation of individualized treatment effects using machine learning. We apply our method to the clinically relevant use-case of vancomycin-induced acute kidney injury in intensive care patients, and compare our causal model-based PC$_{low}$ estimates to qualitative estimates of the PC provided by a medical expert. Important limitations and potential improvements are discussed, and we conclude that future improved causal models could provide essential data-driven support for medication safety monitoring in hospitalized patients.
</details>
<details>
<summary>摘要</summary>
现有的最佳实践方法 для透view的药物反应（ADE）在医院化 patients中的诊断是通过全patient chart review和多个医疗专家的正式可能性评估来进行。这种评估用于Quantitatively estimating the probability of causation (PC); the probability that a drug was a necessary cause of an adverse event。这种方法是手动、资源浪费和人类偏见易受影响，可能从数据驱动的决策支持中受益。在这里，我们开创了一种 causal modeling 方法，使用观察数据来估算下限的PC（PC$_{low}$）。这个方法包括两个关键的 causal inference 组件：（1）目标试验拟合框架和（2）使用机器学习来估算个体化治疗效果。我们在Intensive care 患者中使用vancomycin-induced acute kidney injury作为临床实用的例子，并与医疗专家提供的qualitative PC 估计进行比较。我们讨论了重要的限制和可能的改进，并 conclude that future improved causal models could provide essential data-driven support for medication safety monitoring in hospitalized patients。
</details></li>
</ul>
<hr>
<h2 id="Fast-Detection-of-Phase-Transitions-with-Multi-Task-Learning-by-Confusion"><a href="#Fast-Detection-of-Phase-Transitions-with-Multi-Task-Learning-by-Confusion" class="headerlink" title="Fast Detection of Phase Transitions with Multi-Task Learning-by-Confusion"></a>Fast Detection of Phase Transitions with Multi-Task Learning-by-Confusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09128">http://arxiv.org/abs/2311.09128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Arnold, Frank Schäfer, Niels Lörch</li>
<li>for: study phase transitions</li>
<li>methods: learning-by-confusion scheme, multi-task learning</li>
<li>results: significant speedups, minor deviations compared to ideal case<details>
<summary>Abstract</summary>
Machine learning has been successfully used to study phase transitions. One of the most popular approaches to identifying critical points from data without prior knowledge of the underlying phases is the learning-by-confusion scheme. As input, it requires system samples drawn from a grid of the parameter whose change is associated with potential phase transitions. Up to now, the scheme required training a distinct binary classifier for each possible splitting of the grid into two sides, resulting in a computational cost that scales linearly with the number of grid points. In this work, we propose and showcase an alternative implementation that only requires the training of a single multi-class classifier. Ideally, such multi-task learning eliminates the scaling with respect to the number of grid points. In applications to the Ising model and an image dataset generated with Stable Diffusion, we find significant speedups that closely correspond to the ideal case, with only minor deviations.
</details>
<details>
<summary>摘要</summary>
machine learning 已经成功地应用于研究相转换。一种非常流行的方法是通过学习吃惊方式来识别潜在的 kritical point。这种方法需要输入系统样本，这些样本从可能存在phasetransition的参数变化中提取。以前，这种方法需要对每个可能的grid splitting into two sides进行训练独立的 binary classifier，因此计算成本将与grid点数 linearly scalable。在这篇文章中，我们提出了一种 alternatively，只需要训练一个多类分类器。理想情况下，这种多任务学习可以消除与grid点数的关系。在应用于 Ising 模型和一个通过 Stable Diffusion 生成的图像集中，我们发现了显著的加速，与理想情况几乎完全一致，只有小偏差。
</details></li>
</ul>
<hr>
<h2 id="Constructing-interpretable-principal-curve-using-Neural-ODEs"><a href="#Constructing-interpretable-principal-curve-using-Neural-ODEs" class="headerlink" title="Constructing interpretable principal curve using Neural ODEs"></a>Constructing interpretable principal curve using Neural ODEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09274">http://arxiv.org/abs/2311.09274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangzheng Zhang, Bingxian Xu</li>
<li>for: 这篇论文旨在Characterizing high-dimensional data sets in a dynamical manner, using neural ODEs to define the principal flow and summarize the space.</li>
<li>methods: 这篇论文使用了 neural ODEs 定义主流动，将数据集中的扩展转换为一个动态的流动形式，以便更好地描述高维数据集的本地几何结构。</li>
<li>results: 研究人员通过使用主流动来描述高维数据集的几何结构，并且可以在不同的复杂性水平上进行灵活的汇总。此外，主流动还可以包含刚性动力学的概念，以描述数据集的弹性relaxation dynamics。<details>
<summary>Abstract</summary>
The study of high dimensional data sets often rely on their low dimensional projections that preserve the local geometry of the original space. While numerous methods have been developed to summarize this space as variations of tree-like structures, they are usually non-parametric and "static" in nature. As data may come from systems that are dynamical such as a differentiating cell, a static, non-parametric characterization of the space may not be the most appropriate. Here, we developed a framework, the principal flow, that is capable of characterizing the space in a dynamical manner. The principal flow, defined using neural ODEs, directs motion of a particle through the space, where the trajectory of the particle resembles the principal curve of the dataset. We illustrate that our framework can be used to characterize shapes of various complexities, and is flexible to incorporate summaries of relaxation dynamics.
</details>
<details>
<summary>摘要</summary>
研究高维数据集时，常常利用其低维投影，以保持原始空间的本地几何结构。虽然有许多方法用于概括这个空间，但这些方法通常是非 Parametric 的，即 static 的性质。因为数据可能来自动演化的系统，如 diferenciating 细胞，静止、非 Parametric 的空间概括方法可能不是最合适的。我们在这里提出了一种框架，即主流动，可以 Dynamically 概括这个空间。主流动使用神经 ODEs 定义了一个粒子的运动轨迹，这个轨迹与数据集的主曲线相似。我们示示了我们的框架可以概括各种复杂的形状，并且可以容易地 incorporate 征relaxation 动态概括。
</details></li>
</ul>
<hr>
<h2 id="Damped-Proximal-Augmented-Lagrangian-Method-for-weakly-Convex-Problems-with-Convex-Constraints"><a href="#Damped-Proximal-Augmented-Lagrangian-Method-for-weakly-Convex-Problems-with-Convex-Constraints" class="headerlink" title="Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems with Convex Constraints"></a>Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems with Convex Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09065">http://arxiv.org/abs/2311.09065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hari Dahal, Wei Liu, Yangyang Xu</li>
<li>for: 解决具有弱 converges 目标函数和几何&#x2F;非几何约束的问题</li>
<li>methods: 使用抑制距离 proximal 束更新法 (DPALM)</li>
<li>results: 可以在 $O(\vareps^{-2})$ 外循环迭代中生成一个 $(1+\vareps)$-$ KKT $点，并且在不同类型的目标函数和约束下，DPALM 的迭代复杂度为 $\widetilde{\mathcal{O}\left(\varepsilon^{-2.5} \right)$ 或 $\widetilde{\mathcal{O}\left(\varepsilon^{-3} \right)$。此外，DPALM 在实验中证明比一些现有的方法更高效。<details>
<summary>Abstract</summary>
We give a damped proximal augmented Lagrangian method (DPALM) for solving problems with a weakly-convex objective and convex linear/nonlinear constraints. Instead of taking a full stepsize, DPALM adopts a damped dual stepsize to ensure the boundedness of dual iterates. We show that DPALM can produce a (near) $\vareps$-KKT point within $O(\vareps^{-2})$ outer iterations if each DPALM subproblem is solved to a proper accuracy. In addition, we establish overall iteration complexity of DPALM when the objective is either a regularized smooth function or in a regularized compositional form. For the former case, DPALM achieves the complexity of $\widetilde{\mathcal{O}\left(\varepsilon^{-2.5} \right)$ to produce an $\varepsilon$-KKT point by applying an accelerated proximal gradient (APG) method to each DPALM subproblem. For the latter case, the complexity of DPALM is $\widetilde{\mathcal{O}\left(\varepsilon^{-3} \right)$ to produce a near $\varepsilon$-KKT point by using an APG to solve a Moreau-envelope smoothed version of each subproblem. Our outer iteration complexity and the overall complexity either generalize existing best ones from unconstrained or linear-constrained problems to convex-constrained ones, or improve over the best-known results on solving the same-structured problems. Furthermore, numerical experiments on linearly/quadratically constrained non-convex quadratic programs and linear-constrained robust nonlinear least squares are conducted to demonstrate the empirical efficiency of the proposed DPALM over several state-of-the art methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一个受抑制的近边增强方法（DPALM）来解决具有弱拟对函数和线性/非线性约束的问题。而不是采用完整的步长，DPALM 使用了一个抑制的对偶步长来保证对偶变数的紧缩性。我们证明了DPALM 可以在 $O(\vareps^{-2})$ 外部迭代中生成一个 $(1-\vareps)$ KKT 点。此外，我们建立了 DPALM 的总迭代复杂度，其中当函数是轻度调整的滑坡函数或是调整后的函数时，DPALM 的复杂度为 $\widetilde{\mathcal{O}\left(\varepsilon^{-2.5} \right)$ 和 $\widetilde{\mathcal{O}\left(\varepsilon^{-3} \right)$ 分别。这些结果缩减了现有最好的结果，或者提高了现有的最好结果。此外，我们还进行了一些实验，证明 DPALM 在线性/quadratically constrained non-convex quadratic programs 和 linear-constrained robust nonlinear least squares 中的实际效率。
</details></li>
</ul>
<hr>
<h2 id="New-Horizons-in-Parameter-Regularization-A-Constraint-Approach"><a href="#New-Horizons-in-Parameter-Regularization-A-Constraint-Approach" class="headerlink" title="New Horizons in Parameter Regularization: A Constraint Approach"></a>New Horizons in Parameter Regularization: A Constraint Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09058">http://arxiv.org/abs/2311.09058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jörg K. H. Franke, Michael Hefenbrock, Gregor Koehler, Frank Hutter</li>
<li>for: 本研究旨在提出一种新的训练方法，即受限参数规范化（CPR），用于取代传统的质量惩罚。</li>
<li>methods: 本方法 Reformulates learning为一个受限优化问题，通过对具体参数组的统计量（例如L$_2$-norm）进行约束来实现。使用改进的扩展拉格朗日方法来解决这个受限优化问题。</li>
<li>results: 我们通过在”感知”现象、图像识别和自然语言处理等领域进行实验，证明CPR可以有效地抵消”感知”现象的影响，并且可以与传统的质量惩罚相比或超越其表现。<details>
<summary>Abstract</summary>
This work presents constrained parameter regularization (CPR), an alternative to traditional weight decay. Instead of applying a constant penalty uniformly to all parameters, we enforce an upper bound on a statistical measure (e.g., the L$_2$-norm) of individual parameter groups. This reformulates learning as a constrained optimization problem. To solve this, we utilize an adaptation of the augmented Lagrangian method. Our approach allows for varying regularization strengths across different parameter groups, removing the need for explicit penalty coefficients in the regularization terms. CPR only requires two hyperparameters and introduces no measurable runtime overhead. We offer empirical evidence of CPR's effectiveness through experiments in the "grokking" phenomenon, image classification, and language modeling. Our findings show that CPR can counteract the effects of grokking, and it consistently matches or surpasses the performance of traditional weight decay.
</details>
<details>
<summary>摘要</summary>
这个工作提出了制约参数规范化（CPR），这是传统权值衰退的替代方案。而不是对所有参数应用一定的罚款，我们要求各个参数组的统计量（例如L$_2$- нор）的上限。这将学习转化为一个受限制的优化问题。为解决这个问题，我们利用了一种改进后的扩展拉格朗日方法。我们的方法允许不同参数组的规范强度不同，从而消除了明确的罚款系数在规范项中的需求。CPR只需两个超参数，并没有可观测的运行时间开销。我们通过在“感悟”现象、图像分类和自然语言处理等领域进行实验，证明了CPR的有效性。我们的发现表明，CPR可以抵消“感悟”的影响，并且在性能上与传统权值衰退相当或超过。
</details></li>
</ul>
<hr>
<h2 id="On-the-Foundation-of-Distributionally-Robust-Reinforcement-Learning"><a href="#On-the-Foundation-of-Distributionally-Robust-Reinforcement-Learning" class="headerlink" title="On the Foundation of Distributionally Robust Reinforcement Learning"></a>On the Foundation of Distributionally Robust Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09018">http://arxiv.org/abs/2311.09018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbo Wang, Nian Si, Jose Blanchet, Zhengyuan Zhou</li>
<li>for: This paper contributes to the theoretical foundation of distributionally robust reinforcement learning (DRRL) by providing a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs).</li>
<li>methods: The paper unifies and extends existing formulations of DRMDPs, and rigorously constructs DRMDPs that embrace various modeling attributes for both the decision maker and the adversary.</li>
<li>results: The paper examines conditions for the existence or absence of the dynamic programming principle (DPP) within the DRMDP framework, and provides streamlined proofs grounded in a unified methodology. Additionally, the paper offers counterexamples for settings in which a DPP with full generality is absent.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文为分布robust控制学（DRRL）的理论基础做出了贡献，提供了一个包容性的模型框架， centered around distributionally robust Markov decision processes（DRMDPs）。</li>
<li>methods: 论文将现有的DRMDPs整合和扩展，并强制构建DRMDPs，以涵盖决策者和反对派的各种模型特征。</li>
<li>results: 论文研究DRMDPs中的动态计划原理（DPP）的存在或缺失情况，并提供了一致的证明方法。此外，论文还提供了不具有全面通用性的DPP的 counterexample。<details>
<summary>Abstract</summary>
Motivated by the need for a robust policy in the face of environment shifts between training and the deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embraces various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the existence or absence of the dynamic programming principle (DPP). From an algorithmic standpoint, the existence of DPP holds significant implications, as the vast majority of existing data and computationally efficiency RL algorithms are reliant on the DPP. To study its existence, we comprehensively examine combinations of controller and adversary attributes, providing streamlined proofs grounded in a unified methodology. We also offer counterexamples for settings in which a DPP with full generality is absent.
</details>
<details>
<summary>摘要</summary>
▼ Motivated by the need for a robust policy in the face of environment shifts between training and deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embrace various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the existence or absence of the dynamic programming principle (DPP). From an algorithmic standpoint, the existence of DPP holds significant implications, as the vast majority of existing data and computationally efficiency RL algorithms are reliant on the DPP. To study its existence, we comprehensively examine combinations of controller and adversary attributes, providing streamlined proofs grounded in a unified methodology. We also offer counterexamples for settings in which a DPP with full generality is absent.Note: Simplified Chinese is used in this translation, as it is more widely used in everyday communication and is easier to read for non-native speakers. However, if you prefer Traditional Chinese, I can also provide the translation in that format.
</details></li>
</ul>
<hr>
<h2 id="Semidefinite-programs-simulate-approximate-message-passing-robustly"><a href="#Semidefinite-programs-simulate-approximate-message-passing-robustly" class="headerlink" title="Semidefinite programs simulate approximate message passing robustly"></a>Semidefinite programs simulate approximate message passing robustly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09017">http://arxiv.org/abs/2311.09017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Misha Ivkov, Tselil Schramm</li>
<li>for:  solves many average-case optimization problems optimally</li>
<li>methods:  uses local statistics hierarchy semidefinite programs (SDPs)</li>
<li>results:  offers robust guarantees for many problems, including optimizing the Sherrington-Kirkpatrick Hamiltonian and others<details>
<summary>Abstract</summary>
Approximate message passing (AMP) is a family of iterative algorithms that generalize matrix power iteration. AMP algorithms are known to optimally solve many average-case optimization problems. In this paper, we show that a large class of AMP algorithms can be simulated in polynomial time by \emph{local statistics hierarchy} semidefinite programs (SDPs), even when an unknown principal minor of measure $1/\mathrm{polylog}(\mathrm{dimension})$ is adversarially corrupted. Ours are the first robust guarantees for many of these problems. Further, our results offer an interesting counterpoint to strong lower bounds against less constrained SDP relaxations for average-case max-cut-gain (a.k.a. "optimizing the Sherrington-Kirkpatrick Hamiltonian") and other problems.
</details>
<details>
<summary>摘要</summary>
<<sys: language="zh-CN">> Approximate message passing (AMP) 是一家Iterative algorithms的家族，它们可以通过 Matrix power iteration 的推广来解决许多平均情况优化问题。在这篇论文中，我们展示了一个大类的 AMP 算法可以通过本地统计层次SDP（semidefinite programs）来模拟，即使 unknown principal minor 的推广是 adversarially corrupted 的。这些结果是首次提供了对这些问题的稳定保证。此外，我们的结果还提供了一个有趣的对比，证明强下界 противless constrained SDP relaxations 对平均情况最大切割（a.k.a. "优化 Sherington-Kirkpatrick  Hamiltonian"）和其他问题的解决方案。<</sys>>
</details></li>
</ul>
<hr>
<h2 id="sQUlearn-unicode-x2013-A-Python-Library-for-Quantum-Machine-Learning"><a href="#sQUlearn-unicode-x2013-A-Python-Library-for-Quantum-Machine-Learning" class="headerlink" title="sQUlearn $\unicode{x2013}$ A Python Library for Quantum Machine Learning"></a>sQUlearn $\unicode{x2013}$ A Python Library for Quantum Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08990">http://arxiv.org/abs/2311.08990</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/squlearn/squlearn">https://github.com/squlearn/squlearn</a></li>
<li>paper_authors: David A. Kreplin, Moritz Willmann, Jan Schnabel, Frederic Rapp, Marco Roth</li>
<li>for: 这个论文是用于探讨量子机器学习（QML）的Python库，旨在让量子机器学习研究者和实践者可以轻松地整合古典机器学习工具 like scikit-learn。</li>
<li>methods: 这个库使用了分层架构，提供了丰富的工具集，包括量子核心方法和量子神经网络，以及自定义数据编码策略、自动化执行处理和特定核心规化技术。</li>
<li>results: 这个库的设计目标是让现有的量子计算能力和实际机器学习应用之间建立桥接，并且提供了高效的概念测试、实验和管道功能。<details>
<summary>Abstract</summary>
sQUlearn introduces a user-friendly, NISQ-ready Python library for quantum machine learning (QML), designed for seamless integration with classical machine learning tools like scikit-learn. The library's dual-layer architecture serves both QML researchers and practitioners, enabling efficient prototyping, experimentation, and pipelining. sQUlearn provides a comprehensive toolset that includes both quantum kernel methods and quantum neural networks, along with features like customizable data encoding strategies, automated execution handling, and specialized kernel regularization techniques. By focusing on NISQ-compatibility and end-to-end automation, sQUlearn aims to bridge the gap between current quantum computing capabilities and practical machine learning applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Multimodal-Dataset-of-21-412-Recorded-Nights-for-Sleep-and-Respiratory-Research"><a href="#A-Multimodal-Dataset-of-21-412-Recorded-Nights-for-Sleep-and-Respiratory-Research" class="headerlink" title="A Multimodal Dataset of 21,412 Recorded Nights for Sleep and Respiratory Research"></a>A Multimodal Dataset of 21,412 Recorded Nights for Sleep and Respiratory Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08979">http://arxiv.org/abs/2311.08979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alon Diament, Maria Gorodetski, Adam Jankelow, Ayya Keshet, Tal Shor, Daphna Weissglas-Volkov, Hagai Rossman, Eran Segal</li>
<li>for: 这个研究旨在提供一个新的、丰富的家庭呼吸暂停测试数据集，来支持睡眠研究、个性化医疗和机器学习应用于生物医学领域。</li>
<li>methods: 这个研究使用了FDA批准的WatchPAT-300设备，收集了7,077名参与者在21,412个夜晚的数据，包括3级睡眠数据：原始多核心时间序列数据、注释的睡眠事件和计算的摘要统计数据，其中包括447个睡眠建筑、呼吸暂停和心率变化的特征。</li>
<li>results: 这个数据集可以提高许多健康相关特征的预测能力，包括身体结构、骨骼密度、血糖水平和心血管健康。这些结果表明该数据集有可能在睡眠研究、个性化医疗和机器学习应用中提供新的参考值。<details>
<summary>Abstract</summary>
This study introduces a novel, rich dataset obtained from home sleep apnea tests using the FDA-approved WatchPAT-300 device, collected from 7,077 participants over 21,412 nights. The dataset comprises three levels of sleep data: raw multi-channel time-series from sensors, annotated sleep events, and computed summary statistics, which include 447 features related to sleep architecture, sleep apnea, and heart rate variability (HRV). We present reference values for Apnea/Hypopnea Index (AHI), sleep efficiency, Wake After Sleep Onset (WASO), and HRV sample entropy, stratified by age and sex. Moreover, we demonstrate that the dataset improves the predictive capability for various health related traits, including body composition, bone density, blood sugar levels and cardiovascular health. These results illustrate the dataset's potential to advance sleep research, personalized healthcare, and machine learning applications in biomedicine.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is used to refer to the standardized form of Chinese used in mainland China, as opposed to "Traditional Chinese" which is used in Hong Kong, Taiwan, and other regions. The translation is written in Simplified Chinese, but the original text is in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Probability-of-Collision-of-satellites-and-space-debris-for-short-term-encounters-Rederivation-and-fast-to-compute-upper-and-lower-bounds"><a href="#Probability-of-Collision-of-satellites-and-space-debris-for-short-term-encounters-Rederivation-and-fast-to-compute-upper-and-lower-bounds" class="headerlink" title="Probability of Collision of satellites and space debris for short-term encounters: Rederivation and fast-to-compute upper and lower bounds"></a>Probability of Collision of satellites and space debris for short-term encounters: Rederivation and fast-to-compute upper and lower bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08978">http://arxiv.org/abs/2311.08978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Ferreira, Cláudia Soares, Marta Guimarães<br>for: 这篇论文旨在解决低地球轨道（LEO）中对空间业务造成的垃圾物品问题，特别是预测这些物品之间的碰撞可能性。methods: 这篇论文提出了一个新的 derive 方法，基于初始假设，允许快速和紧密的上下限 bounds  Computation，以便更好地预测碰撞可能性。results: 这篇论文的实验显示，与传统方法相比，新的 derive 方法可以快速计算碰撞可能性，并且可以实现几乎实时的处理时间。<details>
<summary>Abstract</summary>
The proliferation of space debris in LEO has become a major concern for the space industry. With the growing interest in space exploration, the prediction of potential collisions between objects in orbit has become a crucial issue. It is estimated that, in orbit, there are millions of fragments a few millimeters in size and thousands of inoperative satellites and discarded rocket stages. Given the high speeds that these fragments can reach, even fragments a few millimeters in size can cause fractures in a satellite's hull or put a serious crack in the window of a space shuttle. The conventional method proposed by Akella and Alfriend in 2000 remains widely used to estimate the probability of collision in short-term encounters. Given the small period of time, it is assumed that, during the encounter: (1) trajectories are represented by straight lines with constant velocity; (2) there is no velocity uncertainty and the position exhibits a stationary distribution throughout the encounter; and (3) position uncertainties are independent and represented by Gaussian distributions. This study introduces a novel derivation based on first principles that naturally allows for tight and fast upper and lower bounds for the probability of collision. We tested implementations of both probability and bound computations with the original and our formulation on a real CDM dataset used in ESA's Collision Avoidance Challenge. Our approach reduces the calculation of the probability to two one-dimensional integrals and has the potential to significantly reduce the processing time compared to the traditional method, from 80% to nearly real-time.
</details>
<details>
<summary>摘要</summary>
随着空间探索的兴趣日益增长，附近轨道上的空间垃圾堆积已成为空间业界的一个重要问题。预测可能的空间碰撞事件已成为一项关键的问题。据估计，附近轨道上有数以百万计的碎片几毫米大小，以及废弃的卫星和发射器阶段。由于这些碎片的高速运动，же�不是几毫米大小的碎片也可以使附近轨道上的卫星舱壁受损或使空间飞船的窗口受伤。传统的方法，提出于Akella和Alfriend在2000年，仍然广泛使用来估计短期遭遇中的碰撞机会。在这种情况下，假设：（1）轨道可以用直线表示，速度为常数；（2）速度不具有uncertainty，位置呈静态分布；（3）位置不确定性是独立的 Gaussian 分布。本研究提出了一种基于初始原理的新 derivation，自然地提供了紧密和快速的上限和下限 bounds  для碰撞机会的计算。我们对原始和我们的方法进行了实现，并使用ESA的Collision Avoidance Challenge中使用的真实CDM数据进行测试。我们的方法将计算概率减少到两个一维 интегра尔，并有可能减少计算时间比传统方法的80%，从近实时级别降低到。
</details></li>
</ul>
<hr>
<h2 id="A-Single-Loop-Algorithm-for-Decentralized-Bilevel-Optimization"><a href="#A-Single-Loop-Algorithm-for-Decentralized-Bilevel-Optimization" class="headerlink" title="A Single-Loop Algorithm for Decentralized Bilevel Optimization"></a>A Single-Loop Algorithm for Decentralized Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08945">http://arxiv.org/abs/2311.08945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youran Dong, Shiqian Ma, Junfeng Yang, Chao Yin</li>
<li>for: 这 paper 是关于分布式机器学习中的 bilateral 优化问题的研究。</li>
<li>methods: 该 paper 提出了一种新的单循环算法来解决分布式 bilateral 优化问题，该算法不需要大量的矩阵-向量乘制。此外，不同于现有的分布式 bilateral 优化和联邦 bilateral 优化方法，该算法不需要任何梯度差异假设。</li>
<li>results: 我们的分析表明，提出的算法可以达到最佳知名的 convergence rate  для bilateral 优化算法。<details>
<summary>Abstract</summary>
Bilevel optimization has received more and more attention recently due to its wide applications in machine learning. In this paper, we consider bilevel optimization in decentralized networks. In particular, we propose a novel single-loop algorithm for solving decentralized bilevel optimization with strongly convex lower level problem. Our algorithm is fully single-loop and does not require heavy matrix-vector multiplications when approximating the hypergradient. Moreover, unlike existing methods for decentralized bilevel optimization and federated bilevel optimization, our algorithm does not require any gradient heterogeneity assumption. Our analysis shows that the proposed algorithm achieves the best known convergence rate for bilevel optimization algorithms.
</details>
<details>
<summary>摘要</summary>
“BILevel优化在近期Received更多的注意力，因为它在机器学习中有广泛的应用。在这篇论文中，我们考虑了分布式网络中的BILevel优化。具体来说，我们提出了一种新的单循环算法，用于解决分布式BILevel优化中强 convex下层问题。我们的算法完全是单循环的，不需要大量的矩阵-向量乘法来估计超gradient。此外，与现有的分布式BILevel优化和联邦BILevel优化方法不同，我们的算法不需要任何梯度异质假设。我们的分析表明，我们的算法可以达到BILevel优化算法中最佳的知名的速度。”Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Efficiently-Escaping-Saddle-Points-for-Non-Convex-Policy-Optimization"><a href="#Efficiently-Escaping-Saddle-Points-for-Non-Convex-Policy-Optimization" class="headerlink" title="Efficiently Escaping Saddle Points for Non-Convex Policy Optimization"></a>Efficiently Escaping Saddle Points for Non-Convex Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08914">http://arxiv.org/abs/2311.08914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadegh Khorasani, Saber Salehkaleybar, Negar Kiyavash, Niao He, Matthias Grossglauser</li>
<li>for: 本研究旨在提出一种基于积分Gradient的第二阶方法，以实现精细化优化。</li>
<li>methods: 该方法使用积分Gradient法，并使用矩阵乘法来获取第二阶信息，从而提高效率。</li>
<li>results: 实验结果表明，该方法可以更高效地优化问题，并且更具 robustness。<details>
<summary>Abstract</summary>
Policy gradient (PG) is widely used in reinforcement learning due to its scalability and good performance. In recent years, several variance-reduced PG methods have been proposed with a theoretical guarantee of converging to an approximate first-order stationary point (FOSP) with the sample complexity of $O(\epsilon^{-3})$. However, FOSPs could be bad local optima or saddle points. Moreover, these algorithms often use importance sampling (IS) weights which could impair the statistical effectiveness of variance reduction. In this paper, we propose a variance-reduced second-order method that uses second-order information in the form of Hessian vector products (HVP) and converges to an approximate second-order stationary point (SOSP) with sample complexity of $\tilde{O}(\epsilon^{-3})$. This rate improves the best-known sample complexity for achieving approximate SOSPs by a factor of $O(\epsilon^{-0.5})$. Moreover, the proposed variance reduction technique bypasses IS weights by using HVP terms. Our experimental results show that the proposed algorithm outperforms the state of the art and is more robust to changes in random seeds.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a variance-reduced second-order method that uses second-order information in the form of Hessian vector products (HVP) and converges to an approximate second-order stationary point (SOSP) with sample complexity of $\tilde{O}(\epsilon^{-3})$. This rate improves the best-known sample complexity for achieving approximate SOSPs by a factor of $O(\epsilon^{-0.5})$. Moreover, the proposed variance reduction technique bypasses IS weights by using HVP terms.Our experimental results show that the proposed algorithm outperforms the state of the art and is more robust to changes in random seeds.
</details></li>
</ul>
<hr>
<h2 id="On-the-Importance-of-Step-wise-Embeddings-for-Heterogeneous-Clinical-Time-Series"><a href="#On-the-Importance-of-Step-wise-Embeddings-for-Heterogeneous-Clinical-Time-Series" class="headerlink" title="On the Importance of Step-wise Embeddings for Heterogeneous Clinical Time-Series"></a>On the Importance of Step-wise Embeddings for Heterogeneous Clinical Time-Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08902">http://arxiv.org/abs/2311.08902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ratschlab/clinical-embeddings">https://github.com/ratschlab/clinical-embeddings</a></li>
<li>paper_authors: Rita Kuznetsova, Alizée Pace, Manuel Burger, Hugo Yèche, Gunnar Rätsch</li>
<li>for: 这些研究旨在探讨深度学习模型在医疗数据中的应用，尤其是在医院床位监测记录中处理时间序列数据。</li>
<li>methods: 这些研究使用了新的深度学习架构，包括树状结构和表格数据的处理方法。</li>
<li>results: 研究发现，使用这些新的深度学习方法可以在医疗数据中提高时间序列模型的性能，特别是在医院床位监测记录中。  Additionally, the study found that feature grouping within predefined semantic groups in the step-wise embedding module can lead to significant performance gains in clinical time-series.<details>
<summary>Abstract</summary>
Recent advances in deep learning architectures for sequence modeling have not fully transferred to tasks handling time-series from electronic health records. In particular, in problems related to the Intensive Care Unit (ICU), the state-of-the-art remains to tackle sequence classification in a tabular manner with tree-based methods. Recent findings in deep learning for tabular data are now surpassing these classical methods by better handling the severe heterogeneity of data input features. Given the similar level of feature heterogeneity exhibited by ICU time-series and motivated by these findings, we explore these novel methods' impact on clinical sequence modeling tasks. By jointly using such advances in deep learning for tabular data, our primary objective is to underscore the importance of step-wise embeddings in time-series modeling, which remain unexplored in machine learning methods for clinical data. On a variety of clinically relevant tasks from two large-scale ICU datasets, MIMIC-III and HiRID, our work provides an exhaustive analysis of state-of-the-art methods for tabular time-series as time-step embedding models, showing overall performance improvement. In particular, we evidence the importance of feature grouping in clinical time-series, with significant performance gains when considering features within predefined semantic groups in the step-wise embedding module.
</details>
<details>
<summary>摘要</summary>
近期深度学习框架的进步在序列模型方面尚未完全传递到医疗电子记录中的时间序列处理任务上。特别是在医疗中心内部疗单（ICU）中的问题上，现状的方法仍然是通过树状方法进行序列分类。现有的深度学习方法在表格数据上的发现已经超过了传统方法，更好地处理数据输入特征的严重不同。基于这些发现，我们在临床时序序模型任务上运用这些新的方法，主要目标是强调时序序模型中步骤嵌入的重要性，这在机器学习方法中尚未得到充分发挥。从两个大规模的ICU数据集（MIMIC-III和HiRID）中，我们进行了广泛的性能分析，并证明了采用时间步骤嵌入模型可以获得总体性能提升。尤其是在医疗时序序中，对特定 semantic group 中的特征进行分组，可以获得显著性能提升。
</details></li>
</ul>
<hr>
<h2 id="FedCode-Communication-Efficient-Federated-Learning-via-Transferring-Codebooks"><a href="#FedCode-Communication-Efficient-Federated-Learning-via-Transferring-Codebooks" class="headerlink" title="FedCode: Communication-Efficient Federated Learning via Transferring Codebooks"></a>FedCode: Communication-Efficient Federated Learning via Transferring Codebooks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09270">http://arxiv.org/abs/2311.09270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Khalilian, Vasileios Tsouvalas, Tanir Ozcelebi, Nirvana Meratnia</li>
<li>for: 提高 Federated Learning (FL) 中的数据传输效率，降低客户端和服务器之间的通信负担。</li>
<li>methods: 提出 FedCode 方法，即在客户端上生成和更新 Codebook，并在服务器端 периоди性地传输模型参数以保证学习过程的稳定性和准确性。</li>
<li>results: 通过多个公共数据集和 ResNet-20 和 MobileNet 模型框架进行评估，实现了平均数据传输量的12.2倍减少，同时保持与 FedAvg 相对的模型性能水平（准确率下降率为1.3%）。进一步验证了 FedCode 在非Identical和分布式数据上的性能，其中数据传输量减少约12.7倍，并且模型性能下降率为2.0%。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning paradigm that enables learning models from decentralized local data. While FL offers appealing properties for clients' data privacy, it imposes high communication burdens for exchanging model weights between a server and the clients. Existing approaches rely on model compression techniques, such as pruning and weight clustering to tackle this. However, transmitting the entire set of weight updates at each federated round, even in a compressed format, limits the potential for a substantial reduction in communication volume. We propose FedCode where clients transmit only codebooks, i.e., the cluster centers of updated model weight values. To ensure a smooth learning curve and proper calibration of clusters between the server and the clients, FedCode periodically transfers model weights after multiple rounds of solely communicating codebooks. This results in a significant reduction in communication volume between clients and the server in both directions, without imposing significant computational overhead on the clients or leading to major performance degradation of the models. We evaluate the effectiveness of FedCode using various publicly available datasets with ResNet-20 and MobileNet backbone model architectures. Our evaluations demonstrate a 12.2-fold data transmission reduction on average while maintaining a comparable model performance with an average accuracy loss of 1.3% compared to FedAvg. Further validation of FedCode performance under non-IID data distributions showcased an average accuracy loss of 2.0% compared to FedAvg while achieving approximately a 12.7-fold data transmission reduction.
</details>
<details>
<summary>摘要</summary>
联邦学习（FL）是一种分布式机器学习 paradigma，允许从分布式本地数据学习模型。而FL具有保护客户端数据隐私的优点，但是它需要在服务器和客户端之间高频率进行模型Weight的交换，从而增加了通信负担。现有的方法通过模型压缩技术，如剪枝和Weight集成，来解决这个问题。然而，在每次联邦轮次中发送整个Weight更新集合，即使使用压缩Format，仍然限制了可以减少通信量的潜在降低。我们提议FedCode，客户端只发送codebook，即更新模型Weight值的cluster中心。为确保客户端和服务器之间的学习曲线和模型Weight的准确协调，FedCode在多个轮次后 periodic地传输模型Weight。这Resulted in 客户端和服务器之间的通信量减少，而无需增加客户端的计算负担或导致模型性能下降。我们使用多个公共可用的数据集进行评估，并使用ResNet-20和MobileNet底层模型结构。我们的评估结果表明，FedCode可以实现12.2倍的数据传输减少，而无需增加客户端的计算负担，并且模型性能下降率为1.3%，相比FedAvg。此外，我们进一步验证了FedCode在非 Identical Data分布下的性能，其中Accuracy下降率为2.0%，并实现了约12.7倍的数据传输减少。
</details></li>
</ul>
<hr>
<h2 id="Towards-Label-Embedding-–-Measuring-classification-difficulty"><a href="#Towards-Label-Embedding-–-Measuring-classification-difficulty" class="headerlink" title="Towards Label Embedding – Measuring classification difficulty"></a>Towards Label Embedding – Measuring classification difficulty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08874">http://arxiv.org/abs/2311.08874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharina Hechinger, Christoph Koller, Xiao Xiang Zhu, Göran Kauermann</li>
<li>for: 本研究的目的是提出一种基于投票分布的 Label Embedding 方法，以便在无约束的多个 Labeler 独立标注的情况下，生成高质量的 Label Embedding。</li>
<li>methods: 本研究使用了 Bayesian 模型 Dirichlet-Multinomial 模型，通过随机推断 Maximization 算法和 Markov Chain Monte Carlo 步骤来估计模型和 posterior。</li>
<li>results: 研究人员通过应用该方法于三个 benchmark 数据集，得到了高质量的 Label Embedding，并且可以 Investigate 得到的相关性矩阵，它们可以作为普通的混淆矩阵，反映原始类别之间的semantic similarity。<details>
<summary>Abstract</summary>
Uncertainty quantification in machine learning is a timely and vast field of research. In supervised learning, uncertainty can already occur in the very first stage of the training process, the labelling step. In particular, this is the case when not every instance can be unambiguously classified. The problem occurs for classifying instances, where classes may overlap or instances can not be clearly categorised. In other words, there is inevitable ambiguity in the annotation step and not necessarily a 'ground truth'. We look exemplary at the classification of satellite images. Each image is annotated independently by multiple labellers and classified into local climate zones (LCZs). For each instance we have multiple votes, leading to a distribution of labels rather than a single value. The main idea of this work is that we do not assume a ground truth label but embed the votes into a K-dimensional space, with K as the number of possible categories. The embedding is derived from the voting distribution in a Bayesian setup, modelled via a Dirichlet-Multinomial model. We estimate the model and posteriors using a stochastic Expectation Maximisation algorithm with Markov Chain Monte Carlo steps. While we focus on the particular example of LCZ classification, the methods developed in this paper readily extend to other situations where multiple annotators independently label texts or images. We also apply our approach to two other benchmark datasets for image classification to demonstrate this. Besides the embeddings themselves, we can investigate the resulting correlation matrices, which can be seen as generalised confusion matrices and reflect the semantic similarities of the original classes very well for all three exemplary datasets. The insights gained are valuable and can serve as general label embedding if a single ground truth per observation cannot be guaranteed.
</details>
<details>
<summary>摘要</summary>
机器学习中的不确定性评估是一个时髦的和广泛的研究领域。在指导学习中，不确定性可以在训练过程的第一个阶段出现，即标注阶段。具体来说，这是因为不every个实例都可以无ambiguously分类。标注阶段存在不可避免的uncertainty，而不是一个固定的'ground truth'。我们通过卫星图像的分类为例，每个图像都被独立地标注了多个标注者，并被分类到本地气候区（LCZ）中。每个实例都有多个选择，导致一个分布而不是单个值。我们的主要想法是不 assumption of ground truth标签，而是将选择embed到K-维空间中，K为可能的类别数。这个空间中的嵌入是基于投票分布的Dirichlet-Multinomial模型。我们使用随机抽样最大化算法和Markov链条件遍历来估算模型和 posterior。虽然我们专注于 LCZ 分类的例子，但我们的方法很容易扩展到其他情况，其中多个标注者独立地标注文本或图像。我们还应用我们的方法到了两个其他的图像分类 benchmark 数据集，以示这。除了嵌入本身以外，我们还可以调查结果中的相关性矩阵，可以看作是通用的混淆矩阵，很好地反映原始类别之间的 semantic 相似性。这些发现可以作为一般的标签嵌入，当single ground truth per observation不能保证时。
</details></li>
</ul>
<hr>
<h2 id="Statistical-learning-by-sparse-deep-neural-networks"><a href="#Statistical-learning-by-sparse-deep-neural-networks" class="headerlink" title="Statistical learning by sparse deep neural networks"></a>Statistical learning by sparse deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08845">http://arxiv.org/abs/2311.08845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Felix Abramovich</li>
<li>for: 这个论文是用来研究深度神经网络估计器，特别是使用经验风险最小化与L1正则化。</li>
<li>methods: 这个论文使用了经验风险最小化与L1正则化来估计深度神经网络。</li>
<li>results: 这个论文提出了一个通用的额外风险减少约束，并证明了深度神经网络估计器在不同函数类中同时具有适应性减少约束（只差log因子）。<details>
<summary>Abstract</summary>
We consider a deep neural network estimator based on empirical risk minimization with l_1-regularization. We derive a general bound for its excess risk in regression and classification (including multiclass), and prove that it is adaptively nearly-minimax (up to log-factors) simultaneously across the entire range of various function classes.
</details>
<details>
<summary>摘要</summary>
我们考虑了一种深度神经网络估计器，基于经验风险最小化和L1正则化。我们得到了一个通用的过度风险上界，并证明其在回归和分类（包括多类）中的过度风险增长率是适应的，即在不同的函数集中同时达到了 Nearly-minimax 性（即Logarithmic factor）。Here's the word-for-word translation:我们考虑了一种深度神经网络估计器，基于经验风险最小化与L1正则化。我们得到了一个通用的过度风险上界，并证明其在回归与分类（包括多类）中的过度风险增长率是适应的，即在不同的函数集中同时达到了 Nearly-minimax 性（即Logarithmic factor）。
</details></li>
</ul>
<hr>
<h2 id="Neuroscience-inspired-scientific-machine-learning-Part-1-Variable-spiking-neuron-for-regression"><a href="#Neuroscience-inspired-scientific-machine-learning-Part-1-Variable-spiking-neuron-for-regression" class="headerlink" title="Neuroscience inspired scientific machine learning (Part-1): Variable spiking neuron for regression"></a>Neuroscience inspired scientific machine learning (Part-1): Variable spiking neuron for regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09267">http://arxiv.org/abs/2311.09267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shailesh Garg, Souvik Chakraborty</li>
<li>for: 降低神经网络中的冗余传输，以降低深度学习模型的复杂性和能耗。</li>
<li>methods: 提出一种新的变量脉冲神经元（VSN），基于生物神经元灵感的泄漏集成和发射神经元（LIF-SN）。VSN兼用了LIF-SN和人工神经元的优点，实现了间歇性发射和连续活动的同时存在。</li>
<li>results: 对于分类和回归任务进行测试，VSN的结果表明其适用程度较高，尤其是在回归任务中。<details>
<summary>Abstract</summary>
Redundant information transfer in a neural network can increase the complexity of the deep learning model, thus increasing its power consumption. We introduce in this paper a novel spiking neuron, termed Variable Spiking Neuron (VSN), which can reduce the redundant firing using lessons from biological neuron inspired Leaky Integrate and Fire Spiking Neurons (LIF-SN). The proposed VSN blends LIF-SN and artificial neurons. It garners the advantage of intermittent firing from the LIF-SN and utilizes the advantage of continuous activation from the artificial neuron. This property of the proposed VSN makes it suitable for regression tasks, which is a weak point for the vanilla spiking neurons, all while keeping the energy budget low. The proposed VSN is tested against both classification and regression tasks. The results produced advocate favorably towards the efficacy of the proposed spiking neuron, particularly for regression tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> neural network 中的重复信息传递可能会增加深度学习模型的复杂性，从而增加其电力消耗。本文提出了一种新型的脉冲神经元（Variable Spiking Neuron，VSN），它可以减少不必要的脉冲发生，基于生物神经元发射的灵感，如生物脉冲神经元（LIF-SN）。提出的 VSN 结合了人工神经元和生物神经元的优点。它可以在脉冲神经元中实现间歇性的发射，同时在人工神经元中实现不间歇的活动。这种 VSN 的特性使其适用于回归任务，这是普通脉冲神经元的弱点，又不增加能量预算。本文测试了 VSN 在分类和回归任务上的表现，结果表明，特别是在回归任务上，提出的脉冲神经元具有良好的效果。Note: Simplified Chinese is a romanization of Chinese that uses a simplified set of characters and grammar rules. It is commonly used in mainland China and Singapore. Traditional Chinese is another form of Chinese that uses a more complex set of characters and grammar rules, and is commonly used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Environment-independent-mmWave-Fall-Detection-with-Interacting-Multiple-Model"><a href="#Environment-independent-mmWave-Fall-Detection-with-Interacting-Multiple-Model" class="headerlink" title="Environment-independent mmWave Fall Detection with Interacting Multiple Model"></a>Environment-independent mmWave Fall Detection with Interacting Multiple Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08755">http://arxiv.org/abs/2311.08755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuyao Yu, Jiazhao Wang, Wenchao Jiang</li>
<li>for: 本研究旨在开发一种高精度、高可靠性的非侵入式、非合作式、非接触式跌倒检测系统，以满足智能家居未来的老年人日常照顾需求。</li>
<li>methods: 本研究使用mmWave雷达技术，并提出了一种实用的多模型状态估计器（IMM），可以提取环境无关的特征，以实现高精度和快速的跌倒检测。此外，我们还提出了一种Robust多用户跟踪系统，以处理环境噪音和其他人体噪音。</li>
<li>results: 我们在实际场景中进行了测试，结果显示跌倒检测精度达95%。<details>
<summary>Abstract</summary>
The ageing society brings attention to daily elderly care through sensing technologies. The future smart home is expected to enable in-home daily monitoring, such as fall detection, for seniors in a non-invasive, non-cooperative, and non-contact manner. The mmWave radar is a promising candidate technology for its privacy-preserving and non-contact manner. However, existing solutions suffer from low accuracy and robustness due to environment dependent features. In this paper, we present FADE (\underline{FA}ll \underline{DE}tection), a practical fall detection radar system with enhanced accuracy and robustness in real-world scenarios. The key enabler underlying FADE is an interacting multiple model (IMM) state estimator that can extract environment-independent features for highly accurate and instantaneous fall detection. Furthermore, we proposed a robust multiple-user tracking system to deal with noises from the environment and other human bodies. We deployed our algorithm on low computing power and low power consumption system-on-chip (SoC) composed of data front end, DSP, and ARM processor, and tested its performance in real-world. The experiment shows that the accuracy of fall detection is up to 95\%.
</details>
<details>
<summary>摘要</summary>
社会老龄化引导了每天老人照顾的注意力，未来智能家庭将采用感知技术实现在家中无需参与的老人照顾。例如，fall detection。 millimeter wave radar是一种有前途的技术，因为它可以保持隐私和不接触的方式。然而，现有的解决方案受到环境依赖的特征的影响，导致准确性和可靠性不高。本文提出了FADE（落体检测），一种实用的落体检测雷达系统，具有提高了准确性和可靠性的实际应用能力。FADE的关键技术是一种交互式多模型（IMM）状态估计器，可以提取环境无关的特征，实现高准确性和快速检测落体。此外，我们还提出了一种Robust多用户跟踪系统，可以处理环境和其他人体的噪声。我们将算法部署到低计算力和低功耗系统上，并在实际应用中进行测试。实验结果表明，落体检测精度达95%。
</details></li>
</ul>
<hr>
<h2 id="Using-Stochastic-Gradient-Descent-to-Smooth-Nonconvex-Functions-Analysis-of-Implicit-Graduated-Optimization-with-Optimal-Noise-Scheduling"><a href="#Using-Stochastic-Gradient-Descent-to-Smooth-Nonconvex-Functions-Analysis-of-Implicit-Graduated-Optimization-with-Optimal-Noise-Scheduling" class="headerlink" title="Using Stochastic Gradient Descent to Smooth Nonconvex Functions: Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling"></a>Using Stochastic Gradient Descent to Smooth Nonconvex Functions: Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08745">http://arxiv.org/abs/2311.08745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Sato, Hideaki Iiduka</li>
<li>for: 本研究は非凸函数のグローバル最适解を探索するためのGradient Descent方法の theoretically 分析を提供します。</li>
<li>methods: 本研究では新しい非凸函数の家族を定义し、その suffcient condition を议论し、graduated optimization algorithmの扩张を提供します。</li>
<li>results: 本研究の结果は、学习率とバッチサイズが函数の平滑化に影响することを示します。また、decaying learning rateと増加するバッチサイズがsuperiorであることを理论的に说明します。さらに、Image classificationの実験结果を提供しています。<details>
<summary>Abstract</summary>
The graduated optimization approach is a heuristic method for finding globally optimal solutions for nonconvex functions and has been theoretically analyzed in several studies. This paper defines a new family of nonconvex functions for graduated optimization, discusses their sufficient conditions, and provides a convergence analysis of the graduated optimization algorithm for them. It shows that stochastic gradient descent (SGD) with mini-batch stochastic gradients has the effect of smoothing the function, the degree of which is determined by the learning rate and batch size. This finding provides theoretical insights from a graduated optimization perspective on why large batch sizes fall into sharp local minima, why decaying learning rates and increasing batch sizes are superior to fixed learning rates and batch sizes, and what the optimal learning rate scheduling is. To the best of our knowledge, this is the first paper to provide a theoretical explanation for these aspects. Moreover, a new graduated optimization framework that uses a decaying learning rate and increasing batch size is analyzed and experimental results of image classification that support our theoretical findings are reported.
</details>
<details>
<summary>摘要</summary>
“渐进优化方法是一种幂等方法，用于找到非对称函数的全局优化解决方案，在一些研究中得到了理论分析。这篇论文定义了一个新的非对称函数家族，讨论了它们的必要条件，并对渐进优化算法的整体分析进行了讨论。研究表明，使用批处理随机梯度 descend (SGD) 可以将函数缓和，其缓和度取决于学习率和批处理大小。这一发现为渐进优化视角提供了理论上的解释，包括大批处理大小会落入锐的局部最优点、 decaying 学习率和增加批处理大小是优于固定学习率和批处理大小，以及优化学习率的调度。这是我们知道的第一篇提供了这些方面的理论解释的论文。此外，我们还提出了一种使用 decaying 学习率和增加批处理大小的新渐进优化框架，并对实验结果进行了报告。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Graph-Aware-Diffusion-Modeling-for-Collaborative-Filtering"><a href="#Towards-Graph-Aware-Diffusion-Modeling-for-Collaborative-Filtering" class="headerlink" title="Towards Graph-Aware Diffusion Modeling for Collaborative Filtering"></a>Towards Graph-Aware Diffusion Modeling for Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08744">http://arxiv.org/abs/2311.08744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunqin Zhu, Chao Wang, Hui Xiong</li>
<li>for: 这篇论文是为了提出一种基于神经网络模型的推荐系统中的恢复隐藏反馈方法，帮助推荐系统更好地理解用户的偏好。</li>
<li>methods: 该方法基于diffusion模型，通过对用户的历史交互数据进行反 diffusion，逐渐恢复用户的隐藏偏好。具体来说，我们首先应用synthetic smoothing filters于item-item图中的交互信号，然后通过graph Fourier transform将这种模型 equivalently characterized为一种在图спектраль领域的非对称Gaussian diffusion。</li>
<li>results: 我们的模型在一个数据集上比州当前的方法提高了大量的margin，并在其他数据集上获得了竞争力的结果。<details>
<summary>Abstract</summary>
Recovering masked feedback with neural models is a popular paradigm in recommender systems. Seeing the success of diffusion models in solving ill-posed inverse problems, we introduce a conditional diffusion framework for collaborative filtering that iteratively reconstructs a user's hidden preferences guided by its historical interactions. To better align with the intrinsic characteristics of implicit feedback data, we implement forward diffusion by applying synthetic smoothing filters to interaction signals on an item-item graph. The resulting reverse diffusion can be interpreted as a personalized process that gradually refines preference scores. Through graph Fourier transform, we equivalently characterize this model as an anisotropic Gaussian diffusion in the graph spectral domain, establishing both forward and reverse formulations. Our model outperforms state-of-the-art methods by a large margin on one dataset and yields competitive results on the others.
</details>
<details>
<summary>摘要</summary>
“复原涂体反馈”是现代推荐系统中广泛应用的一种方法。见到传播模型在解决不确定 inverse 问题中的成功，我们引入一个受条件的涂体架构，通过Iteratively重建用户隐藏的偏好。为了更好地适应实际的隐藏反馈数据特点，我们实现了前方涂体，通过对交互信号进行合成滤波，实现反涂体。这个过程可以解释为对用户个别化的过程，渐进地调整偏好分数。通过几何传播变换，我们将这个模型等同于一个方向性涂体在几何spectral domain中，建立了前后两种表现。我们的模型在一个数据集上大幅超过了现有方法，并在其他数据集上获得了竞争性的结果。
</details></li>
</ul>
<hr>
<h2 id="Enabling-CMF-Estimation-in-Data-Constrained-Scenarios-A-Semantic-Encoding-Knowledge-Mining-Model"><a href="#Enabling-CMF-Estimation-in-Data-Constrained-Scenarios-A-Semantic-Encoding-Knowledge-Mining-Model" class="headerlink" title="Enabling CMF Estimation in Data-Constrained Scenarios: A Semantic-Encoding Knowledge Mining Model"></a>Enabling CMF Estimation in Data-Constrained Scenarios: A Semantic-Encoding Knowledge Mining Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08690">http://arxiv.org/abs/2311.08690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanlin Qi, Jia Li, Michael Zhang<br>for: 这个研究的目的是提供一个可靠且可读的知识探索框架，以便更好地估算防车攻击因子（CMF）。methods: 本研究使用了人类理解的灵感和进步的自然语言处理（NLP）技术，将存在的防车攻击因子知识中的细微变化和图像转换成机器可读的表示，以模型这些变化和CMF值之间的复杂关系。results: 实验结果显示，这个新的数据驱动的框架可以与传统的CMF估算方法相比，在精度方面得到了明显的改善。此外，这个方法还提供了对于防车攻击因子估算的新的可能性，例如可以在不 enough crash data 的情况下进行估算。<details>
<summary>Abstract</summary>
Precise estimation of Crash Modification Factors (CMFs) is central to evaluating the effectiveness of various road safety treatments and prioritizing infrastructure investment accordingly. While customized study for each countermeasure scenario is desired, the conventional CMF estimation approaches rely heavily on the availability of crash data at given sites. This not only makes the estimation costly, but the results are also less transferable, since the intrinsic similarities between different safety countermeasure scenarios are not fully explored. Aiming to fill this gap, this study introduces a novel knowledge-mining framework for CMF prediction. This framework delves into the connections of existing countermeasures and reduces the reliance of CMF estimation on crash data availability and manual data collection. Specifically, it draws inspiration from human comprehension processes and introduces advanced Natural Language Processing (NLP) techniques to extract intricate variations and patterns from existing CMF knowledge. It effectively encodes unstructured countermeasure scenarios into machine-readable representations and models the complex relationships between scenarios and CMF values. This new data-driven framework provides a cost-effective and adaptable solution that complements the case-specific approaches for CMF estimation, which is particularly beneficial when availability of crash data or time imposes constraints. Experimental validation using real-world CMF Clearinghouse data demonstrates the effectiveness of this new approach, which shows significant accuracy improvements compared to baseline methods. This approach provides insights into new possibilities of harnessing accumulated transportation knowledge in various applications.
</details>
<details>
<summary>摘要</summary>
中 precisione 的评估坏事件修复因素（CMF）是评估不同安全处理措施的效果和决策建设投资的中心。尽管欢迎特定情况的自定义研究，但传统的CMF评估方法依赖于提供的坏事件数据的可用性，这不仅使得评估成本高，还使得结果更难于传递。为了缓解这个差距，本研究提出了一种新的知识挖掘框架，用于预测CMF。这个框架Drawing inspiration from human comprehension processes and introducing advanced Natural Language Processing (NLP) techniques, it effectively encodes unstructured countermeasure scenarios into machine-readable representations and models the complex relationships between scenarios and CMF values. This new data-driven framework provides a cost-effective and adaptable solution that complements case-specific approaches for CMF estimation, which is particularly beneficial when availability of crash data or time imposes constraints. Experimental validation using real-world CMF Clearinghouse data demonstrates the effectiveness of this new approach, which shows significant accuracy improvements compared to baseline methods. This approach provides insights into new possibilities of harnessing accumulated transportation knowledge in various applications.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Sparse-Principal-Component-Analysis"><a href="#Federated-Learning-for-Sparse-Principal-Component-Analysis" class="headerlink" title="Federated Learning for Sparse Principal Component Analysis"></a>Federated Learning for Sparse Principal Component Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08677">http://arxiv.org/abs/2311.08677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sin Cheng Ciou, Pin Jui Chen, Elvin Y. Tseng, Yuh-Jye Lee</li>
<li>for: 本研究旨在提出一种基于联合方向法的分布式主成分分析（Federated SPCA）方法，以提高数据Owner之间的数据共享和模型训练效率。</li>
<li>methods: 本研究使用了联合方向法（ADMM）和权重补做法来解决分布式主成分分析（SPCA）中的优化问题，并在异步数据场景下进行了广泛的实验 validate the effectiveness of the proposed method.</li>
<li>results: 实验结果表明，对于同adiabatic和非同adiabatic的情况，Federated SPCA方法能够提高模型训练效率和数据共享安全性，同时保持模型训练精度。此外，Federated SPCA方法还能够适应不同的数据分布场景。<details>
<summary>Abstract</summary>
In the rapidly evolving realm of machine learning, algorithm effectiveness often faces limitations due to data quality and availability. Traditional approaches grapple with data sharing due to legal and privacy concerns. The federated learning framework addresses this challenge. Federated learning is a decentralized approach where model training occurs on client sides, preserving privacy by keeping data localized. Instead of sending raw data to a central server, only model updates are exchanged, enhancing data security. We apply this framework to Sparse Principal Component Analysis (SPCA) in this work. SPCA aims to attain sparse component loadings while maximizing data variance for improved interpretability. Beside the L1 norm regularization term in conventional SPCA, we add a smoothing function to facilitate gradient-based optimization methods. Moreover, in order to improve computational efficiency, we introduce a least squares approximation to original SPCA. This enables analytic solutions on the optimization processes, leading to substantial computational improvements. Within the federated framework, we formulate SPCA as a consensus optimization problem, which can be solved using the Alternating Direction Method of Multipliers (ADMM). Our extensive experiments involve both IID and non-IID random features across various data owners. Results on synthetic and public datasets affirm the efficacy of our federated SPCA approach.
</details>
<details>
<summary>摘要</summary>
在机器学习领域的急速发展中，算法效果经常受到数据质量和可用性的限制。传统方法面临数据分享的挑战，主要是因为法律和隐私问题。基于联邦学习框架的方法可以解决这个问题。联邦学习是一种分布式的方法，在客户端上进行模型训练，保持隐私性，不需要将数据传输到中央服务器。而不是将原始数据传输到中央服务器，只需将模型更新传输，从而提高数据安全性。在这种框架下，我们将SPCA（稀畴主成分分析）应用于这里。SPCA的目标是在保持数据变量的最大化的情况下，获得稀畴的成分荷载。我们在传统的SPCA中添加了简化函数，以便使用梯度基本优化方法。此外，我们引入了最小二乘近似，以提高计算效率。在联邦框架下，我们将SPCA定义为一个协调优化问题，可以使用ADMM（替代方向多个分解器）来解决。我们的广泛的实验包括了不同数据所有者的IID和非IID随机特征。结果表明我们的联邦SPCA方法是有效的。
</details></li>
</ul>
<hr>
<h2 id="Coreset-Selection-with-Prioritized-Multiple-Objectives"><a href="#Coreset-Selection-with-Prioritized-Multiple-Objectives" class="headerlink" title="Coreset Selection with Prioritized Multiple Objectives"></a>Coreset Selection with Prioritized Multiple Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08675">http://arxiv.org/abs/2311.08675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Xia, Jiale Liu, Shaokun Zhang, Qingyun Wu, Tongliang Liu</li>
<li>for: 降低计算成本和加速数据处理，使深度学习算法在大规模数据上进行训练。</li>
<li>methods: 提出了“高性能核心集选择”问题，以最小化核心集大小，保证模型性能。提出了一种新的优先级优化方法，通过优先级顺序优化模型性能和核心集大小，并提供了证明其整体性的 converge 性。</li>
<li>results: 经验表明，该方法可以在各种场景下提供更好的模型性能，使用更小的核心集大小。<details>
<summary>Abstract</summary>
Coreset selection is powerful in reducing computational costs and accelerating data processing for deep learning algorithms. It strives to identify a small subset from large-scale data, so that training only on the subset practically performs on par with full data. When coreset selection is applied in realistic scenes, under the premise that the identified coreset has achieved comparable model performance, practitioners regularly desire the identified coreset can have a size as small as possible for lower costs and greater acceleration. Motivated by this desideratum, for the first time, we pose the problem of "coreset selection with prioritized multiple objectives", in which the smallest coreset size under model performance constraints is explored. Moreover, to address this problem, an innovative method is proposed, which maintains optimization priority order over the model performance and coreset size, and efficiently optimizes them in the coreset selection procedure. Theoretically, we provide the convergence guarantee of the proposed method. Empirically, extensive experiments confirm its superiority compared with previous strategies, often yielding better model performance with smaller coreset sizes.
</details>
<details>
<summary>摘要</summary>
<<SYS>>核心集选择是深度学习算法中强大的reduction技术，它目标是从大规模数据中选择一小集，使得只训练这小集可以实际上与全量数据达到相同的性能水平。在实际场景中，当应用核心集选择时，参与者通常希望可以选择最小的核心集，以降低成本和提高加速。为此，我们首次提出了“核心集选择 WITH 优先级多个目标”的问题，即寻找最小的核心集，同时保证模型性能的限制。此外，我们还提出了一种创新的方法，可以具有优先级顺序的优化模型性能和核心集大小，并有理论上的 konvergence 保证。实际实验证明了该方法的优越性，常常可以在更小的核心集上达到更好的模型性能。>>>
</details></li>
</ul>
<hr>
<h2 id="Supervised-low-rank-semi-nonnegative-matrix-factorization-with-frequency-regularization-for-forecasting-spatio-temporal-data"><a href="#Supervised-low-rank-semi-nonnegative-matrix-factorization-with-frequency-regularization-for-forecasting-spatio-temporal-data" class="headerlink" title="Supervised low-rank semi-nonnegative matrix factorization with frequency regularization for forecasting spatio-temporal data"></a>Supervised low-rank semi-nonnegative matrix factorization with frequency regularization for forecasting spatio-temporal data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08636">http://arxiv.org/abs/2311.08636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keunsu Kim, Hanbaek Lyu, Jinsu Kim, Jae-Hun Jung</li>
<li>for: 预测空间时间数据使用supervised semi-nonnegative矩阵分解（SSNMF） WITH频率正则化</li>
<li>methods: 使用矩阵分解将空间时间数据分解成空间和时间组成部分，并在时间域加入非负约束，以提高时间模式的明确度。在频率域中选择特征，使解释更加容易。提出了软和硬正则化两种方法，并提供了首领点的收敛保证。</li>
<li>results: 应用于GRACE数据时，与前期研究在地球物理科学中的结果相比，提出的方法可以得到类似的结果，但是解释性更高。<details>
<summary>Abstract</summary>
We propose a novel methodology for forecasting spatio-temporal data using supervised semi-nonnegative matrix factorization (SSNMF) with frequency regularization. Matrix factorization is employed to decompose spatio-temporal data into spatial and temporal components. To improve clarity in the temporal patterns, we introduce a nonnegativity constraint on the time domain along with regularization in the frequency domain. Specifically, regularization in the frequency domain involves selecting features in the frequency space, making an interpretation in the frequency domain more convenient. We propose two methods in the frequency domain: soft and hard regularizations, and provide convergence guarantees to first-order stationary points of the corresponding constrained optimization problem. While our primary motivation stems from geophysical data analysis based on GRACE (Gravity Recovery and Climate Experiment) data, our methodology has the potential for wider application. Consequently, when applying our methodology to GRACE data, we find that the results with the proposed methodology are comparable to previous research in the field of geophysical sciences but offer clearer interpretability.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来预测空间-时间数据，使用监督 semi-非正式矩阵分解（SSNMF），并添加频率刻度regularization。矩阵分解用于将空间-时间数据分解成空间和时间组成部分。为了提高时间特征的明显性，我们引入了非负约束在时间频谱上，同时在频率频谱上进行了规regularization。我们提出了两种频率频谱中的方法：软和硬的regulization，并提供了首轮站点的确定性保证。我们的主要动机来自地球物理数据分析，基于 GRACE（重力回升和气候实验）数据，但我们的方法具有更广泛的应用前景。当我们应用我们的方法于 GRACE 数据时，我们发现结果与前一些地球物理科学领域的研究相似，但更容易理解。
</details></li>
</ul>
<hr>
<h2 id="Non-Uniform-Smoothness-for-Gradient-Descent"><a href="#Non-Uniform-Smoothness-for-Gradient-Descent" class="headerlink" title="Non-Uniform Smoothness for Gradient Descent"></a>Non-Uniform Smoothness for Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08615">http://arxiv.org/abs/2311.08615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lindonroberts/nonuniform-smoothness">https://github.com/lindonroberts/nonuniform-smoothness</a></li>
<li>paper_authors: Albert S. Berahas, Lindon Roberts, Fred Roosta</li>
<li>for: 这个论文主要是为了提出一种新的梯度下降类方法，以及一种基于本地首项稳定性诊断（LFSO）的模型。</li>
<li>methods: 这个论文使用了一种基于LFSO的修改后的梯度下降方法，并给出了全球和本地收敛结果。</li>
<li>results: 论文表明，使用LFSO可以在非强式凹陷问题中实现全球线性收敛率，并且超过了通用（加速）首项方法的下界。<details>
<summary>Abstract</summary>
The analysis of gradient descent-type methods typically relies on the Lipschitz continuity of the objective gradient. This generally requires an expensive hyperparameter tuning process to appropriately calibrate a stepsize for a given problem. In this work we introduce a local first-order smoothness oracle (LFSO) which generalizes the Lipschitz continuous gradients smoothness condition and is applicable to any twice-differentiable function. We show that this oracle can encode all relevant problem information for tuning stepsizes for a suitably modified gradient descent method and give global and local convergence results. We also show that LFSOs in this modified first-order method can yield global linear convergence rates for non-strongly convex problems with extremely flat minima, and thus improve over the lower bound on rates achievable by general (accelerated) first-order methods.
</details>
<details>
<summary>摘要</summary>
通常来说，梯度下降类方法的分析假设函数梯度的 lipschitz连续性。这通常需要一个昂贵的参数调整过程，以适应给定问题。在这篇文章中，我们引入了本地首项简oothness oracle（LFSO），这将 lípschitz连续梯度简单性条件扩展到任何两次导数函数。我们证明这个oracle可以包含所有相关的问题信息，用于调整梯度下降方法的步长。我们还证明LFSOs在修改后的首项方法中可以实现全球线性减少率，超过一般（加速）首项方法的下界。
</details></li>
</ul>
<hr>
<h2 id="Converting-Transformers-to-Polynomial-Form-for-Secure-Inference-Over-Homomorphic-Encryption"><a href="#Converting-Transformers-to-Polynomial-Form-for-Secure-Inference-Over-Homomorphic-Encryption" class="headerlink" title="Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption"></a>Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08610">http://arxiv.org/abs/2311.08610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itamar Zimerman, Moran Baruch, Nir Drucker, Gilad Ezov, Omri Soceanu, Lior Wolf</li>
<li>for: 这项研究旨在开发一种privacy-preserving深度学习模型，尤其是在使用 Homomorphic Encryption (HE) 技术时。</li>
<li>methods: 这项研究使用了一种新的幂数变换方法，将 transformer 模型转换成幂数形式，以实现安全的推理。同时，这种方法还可以在不同的数据集上进行图像分类和文本分类。</li>
<li>results: 研究结果显示，这种方法可以实现与传统方法相当的性能，并且可以在不同的应用场景中使用。此外，研究还发现了一些稳定性问题，并进行了一系列的ablations来评估每个模型组件的贡献。<details>
<summary>Abstract</summary>
Designing privacy-preserving deep learning models is a major challenge within the deep learning community. Homomorphic Encryption (HE) has emerged as one of the most promising approaches in this realm, enabling the decoupling of knowledge between the model owner and the data owner. Despite extensive research and application of this technology, primarily in convolutional neural networks, incorporating HE into transformer models has been challenging because of the difficulties in converting these models into a polynomial form. We break new ground by introducing the first polynomial transformer, providing the first demonstration of secure inference over HE with transformers. This includes a transformer architecture tailored for HE, alongside a novel method for converting operators to their polynomial equivalent. This innovation enables us to perform secure inference on LMs with WikiText-103. It also allows us to perform image classification with CIFAR-100 and Tiny-ImageNet. Our models yield results comparable to traditional methods, bridging the performance gap with transformers of similar scale and underscoring the viability of HE for state-of-the-art applications. Finally, we assess the stability of our models and conduct a series of ablations to quantify the contribution of each model component.
</details>
<details>
<summary>摘要</summary>
设计保持隐私的深度学习模型是深度学习社区中的一个主要挑战。归一化加密（HE）已经成为这个领域中最有前途的方法，允许知识的解耦 между模型所有者和数据所有者。尽管已经进行了广泛的研究和应用这技术，主要在卷积神经网络上，但将HE应用于变换器模型却是一个挑战，因为变换器模型不可以直接转化为多项式形式。我们在这篇文章中首次提出了第一个多项式变换器，并提供了将操作符转化为其多项式等价的新方法。这种创新允许我们在LMs上进行安全的推理，并在CIFAR-100和Tiny-ImageNet上进行图像分类。我们的模型的结果与传统方法相似， thereby bridging the performance gap with transformers of similar scale, and demonstrating the feasibility of HE for state-of-the-art applications. Finally, we assess the stability of our models and conduct a series of ablations to quantify the contribution of each model component.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.LG_2023_11_15/" data-id="clpahu77500v13h88ekla78bf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/eess.IV_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T09:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/eess.IV_2023_11_15/">eess.IV - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Leveraging-machine-learning-to-enhance-climate-models-a-review"><a href="#Leveraging-machine-learning-to-enhance-climate-models-a-review" class="headerlink" title="Leveraging machine learning to enhance climate models: a review"></a>Leveraging machine learning to enhance climate models: a review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09413">http://arxiv.org/abs/2311.09413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Elsayed, Shrouk Wally, Islam Alkabbany, Asem Ali, Aly Farag</li>
<li>for: 提高当前气候模型的准确性，帮助政府和个人制定有效的气候变化缓冲策略。</li>
<li>methods: 使用机器学习技术分析大量的气候数据，提取有价值的信息，帮助我们更好地理解地球气候。</li>
<li>results: 在过去5年内，机器学习技术已经被广泛应用于提高当前气候模型的准确性，提供了有力的数据分析工具。<details>
<summary>Abstract</summary>
Recent achievements in machine learning (Ml) have had a significant impact on various fields, including climate science. Climate modeling is very important and plays a crucial role in shaping the decisions of governments and individuals in mitigating the impact of climate change. Climate change poses a serious threat to humanity, however, current climate models are limited by computational costs, uncertainties, and biases, affecting their prediction accuracy. The vast amount of climate data generated by satellites, radars, and earth system models (ESMS) poses a significant challenge. ML techniques can be effectively employed to analyze this data and extract valuable insights that aid in our understanding of the earth climate. This review paper focuses on how ml has been utilized in the last 5 years to boost the current state-of-the-art climate models. We invite the ml community to join in the global effort to accurately model the earth climate by collaborating with other fields to leverage ml as a powerful tool in this endeavor.
</details>
<details>
<summary>摘要</summary>
Recent advances in machine learning (ML) have had a profound impact on various fields, including climate science. Climate modeling is crucial and plays a vital role in shaping the decisions of governments and individuals in mitigating the impact of climate change. However, current climate models are limited by computational costs, uncertainties, and biases, which affect their prediction accuracy. The vast amount of climate data generated by satellites, radars, and earth system models (ESMs) poses a significant challenge. ML techniques can be effectively employed to analyze this data and extract valuable insights that aid in our understanding of the earth climate. This review paper focuses on how ML has been utilized in the last five years to improve the current state-of-the-art climate models. We invite the ML community to join in the global effort to accurately model the earth climate by collaborating with other fields to leverage ML as a powerful tool in this endeavor.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Parallel-Quantum-Hough-Transform"><a href="#Parallel-Quantum-Hough-Transform" class="headerlink" title="Parallel Quantum Hough Transform"></a>Parallel Quantum Hough Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09002">http://arxiv.org/abs/2311.09002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Klefenz, Nico Wittrock, Frank Feldhoff</li>
<li>for: 这个论文是为了提出一种并行量子截割（PQHT）算法，并在量子计算机上进行实现。</li>
<li>methods: 该算法使用了一系列连接的可程序化$\texttt{RZ}$旋转门，以及启发器实现的巧合检测器。</li>
<li>results: 作者在IBM Quantum Composer中实现了模块，并使用IBM QASM仿真器进行测试。最终，模块被编译使用Python包Qiskit，并将任务分发到分布式的IBM Q System One量子计算机上进行执行。成功运行结果在Friaufhofer Q System One上进行了证明。<details>
<summary>Abstract</summary>
Few of the known quantum algorithms can be reliably executed on a quantum computer. Therefore, as an extension, we propose a Parallel Quantum Hough transform (PQHT) algorithm that we execute on a quantum computer. We give its implementation and discuss the results obtained. The PQHT algorithm is conceptually divided into a parallel rotation stage consisting of a set of connected programmable $\texttt{RZ}$ rotation gates, with adjustable node connections of coincidence detectors realized with quantum logic gates. The modules were developed using IBM Quantum Composer and tested using the IBM QASM simulator. Finally, the modules were programmed using the Python package Qiskit and the jobs were sent to distributed IBM Q System One quantum computers. The successful run results on Fraunhofer Q System One in Ehningen will be presented as a proof of concept for the PQHT algorithm.
</details>
<details>
<summary>摘要</summary>
“现有的量子算法中，只有一些可靠地在量子计算机上执行。因此，我们提出了一种并行量子哈夫散度（PQHT）算法，我们在量子计算机上实现。我们将其实现方式和结果讨论。PQHT算法从概念上分为一个并行旋转阶段，包括一组相连的可编程的$\texttt{RZ}$旋转门，其中节点连接的吻合探测器实现使用量子逻辑门。模块使用IBM量子 composer开发，使用IBM QASM simulator进行测试。最后，模块使用Python包Qiskit编程，并将任务分发给分布式IBM Q System One量子计算机。成功运行结果在 Fraunhofer Q System One 上将被提供作为PQHT算法的证明。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Ultrafast-3-D-Super-Resolution-Ultrasound-using-Row-Column-Array-specific-Coherence-based-Beamforming-and-Rolling-Acoustic-Sub-aperture-Processing-In-Vitro-In-Vivo-and-Clinical-Study"><a href="#Ultrafast-3-D-Super-Resolution-Ultrasound-using-Row-Column-Array-specific-Coherence-based-Beamforming-and-Rolling-Acoustic-Sub-aperture-Processing-In-Vitro-In-Vivo-and-Clinical-Study" class="headerlink" title="Ultrafast 3-D Super Resolution Ultrasound using Row-Column Array specific Coherence-based Beamforming and Rolling Acoustic Sub-aperture Processing: In Vitro, In Vivo and Clinical Study"></a>Ultrafast 3-D Super Resolution Ultrasound using Row-Column Array specific Coherence-based Beamforming and Rolling Acoustic Sub-aperture Processing: In Vitro, In Vivo and Clinical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08823">http://arxiv.org/abs/2311.08823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Hansen-Shearer, Jipeng Yan, Marcelo Lerendegui, Biao Huang, Matthieu Toulemonde, Kai Riemer, Qingyuan Tan, Johanna Tonko, Peter D. Weinberg, Chris Dunsby, Meng-Xing Tang</li>
<li>for: 这个论文是为了研究ROW-COLUMNAddressed array的ultrasound imaging技术，以提高图像质量和快速扫描速率。</li>
<li>methods: 论文使用了ROW-COLUMNAddressed array-specific coherence-based beamforming技术和声学子镜处理技术来减少“次要”辐射残余和杂音，提高图像质量和快速扫描速率。</li>
<li>results: 实验结果表明，这种新的图像重建方法可以减少“false”位置的比例，降低杂音水平和提高图像扫描速率，并在人体内进行非侵入式的ultrasound imaging。<details>
<summary>Abstract</summary>
The row-column addressed array is an emerging probe for ultrafast 3-D ultrasound imaging. It achieves this with far fewer independent electronic channels and a wider field of view than traditional 2-D matrix arrays, of the same channel count, making it a good candidate for clinical translation. However, the image quality of row-column arrays is generally poor, particularly when investigating tissue. Ultrasound localisation microscopy allows for the production of super-resolution images even when the initial image resolution is not high. Unfortunately, the row-column probe can suffer from imaging artefacts that can degrade the quality of super-resolution images as `secondary' lobes from bright microbubbles can be mistaken as microbubble events, particularly when operated using plane wave imaging. These false events move through the image in a physiologically realistic way so can be challenging to remove via tracking, leading to the production of 'false vessels'. Here, a new type of rolling window image reconstruction procedure was developed, which integrated a row-column array-specific coherence-based beamforming technique with acoustic sub-aperture processing for the purposes of reducing `secondary' lobe artefacts, noise and increasing the effective frame rate. Using an {\it{in vitro} cross tube, it was found that the procedure reduced the percentage of `false' locations from $\sim$26\% to $\sim$15\% compared to traditional orthogonal plane wave compounding. Additionally, it was found that the noise could be reduced by $\sim$7 dB and that the effective frame rate could be increased to over 4000 fps. Subsequently, {\it{in vivo} ultrasound localisation microscopy was used to produce images non-invasively of a rabbit kidney and a human thyroid.
</details>
<details>
<summary>摘要</summary>
矩阵列 Addressed 数组是一种emerging probe дляultrafast 3D 超声成像。它通过使用 fewer independent electronic channels 和更广阔的视场，可以与传统的2D 矩阵数组相比，提供更高的图像质量，这使其成为临床翻译的好选择。然而，矩阵列数组的图像质量通常较差，特别是在调查组织时。超声成像微scopic allow for the production of super-resolution images even when the initial image resolution is not high. Unfortunately, the row-column probe can suffer from imaging artifacts that can degrade the quality of super-resolution images as "secondary" lobes from bright microbubbles can be mistaken as microbubble events, particularly when operated using plane wave imaging. These false events move through the image in a physiologically realistic way so can be challenging to remove via tracking, leading to the production of 'false vessels'. Here, a new type of rolling window image reconstruction procedure was developed, which integrated a row-column array-specific coherence-based beamforming technique with acoustic sub-aperture processing for the purposes of reducing "secondary" lobe artifacts, noise, and increasing the effective frame rate. Using an in vitro cross tube, it was found that the procedure reduced the percentage of 'false' locations from approximately 26% to approximately 15% compared to traditional orthogonal plane wave compounding. Additionally, it was found that the noise could be reduced by approximately 7 dB and that the effective frame rate could be increased to over 4000 fps. Subsequently, in vivo ultrasound localization microscopy was used to produce images non-invasively of a rabbit kidney and a human thyroid.
</details></li>
</ul>
<hr>
<h2 id="Degradation-Estimation-Recurrent-Neural-Network-with-Local-and-Non-Local-Priors-for-Compressive-Spectral-Imaging"><a href="#Degradation-Estimation-Recurrent-Neural-Network-with-Local-and-Non-Local-Priors-for-Compressive-Spectral-Imaging" class="headerlink" title="Degradation Estimation Recurrent Neural Network with Local and Non-Local Priors for Compressive Spectral Imaging"></a>Degradation Estimation Recurrent Neural Network with Local and Non-Local Priors for Compressive Spectral Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08808">http://arxiv.org/abs/2311.08808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yubo Dong, Dahua Gao, Yuyan Li, Guangming Shi, Danhua Liu</li>
<li>for: 这个论文目的是为了提高coded aperture snapshot spectral imaging（CASSI）系统中的3D彩色спектраль图像（HSI）重建的性能。</li>
<li>methods: 这个论文使用了深度 unfolding network（DUN）来实现HSI重建，并在DUN中引入了Recurrent Neural Network（RNN）、Degradation Estimation Network（DERNN）和Local and Non-Local Transformer（LNLT）等 Component。</li>
<li>results: 这个论文的实验结果表明，DERNN-LNLT可以提高CASSI系统中HSI重建的精度和效率，并且可以更好地利用本地和非本地的彩色спектраль图像约束。<details>
<summary>Abstract</summary>
In coded aperture snapshot spectral imaging (CASSI) systems, a core problem is to recover the 3D hyperspectral image (HSI) from the 2D measurement. Current deep unfolding networks (DUNs) for the HSI reconstruction mainly suffered from three issues. Firstly, in previous DUNs, the DNNs across different stages were unable to share the feature representations learned from different stages, leading to parameter sparsity, which in turn limited their reconstruction potential. Secondly, previous DUNs fail to estimate degradation-related parameters within a unified framework, including the degradation matrix in the data subproblem and the noise level in the prior subproblem. Consequently, either the accuracy of solving the data or the prior subproblem is compromised. Thirdly, exploiting both local and non-local priors for the HSI reconstruction is crucial, and it remains a key issue to be addressed. In this paper, we first transform the DUN into a Recurrent Neural Network (RNN) by sharing parameters across stages, which allows the DNN in each stage could learn feature representation from different stages, enhancing the representativeness of the DUN. Secondly, we incorporate the Degradation Estimation Network into the RNN (DERNN), which simultaneously estimates the degradation matrix and the noise level by residual learning with reference to the sensing matrix. Thirdly, we propose a Local and Non-Local Transformer (LNLT) to effectively exploit both local and non-local priors in HSIs. By integrating the LNLT into the DERNN for solving the prior subproblem, we propose the DERNN-LNLT, which achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
在coded aperture snapshot spectral imaging（CASSI）系统中，核心问题是从2D测量数据中回归3D彩色спектраль成像（HSI）。目前的深度 unfolding network（DUN）for HSI重建主要受到以下三个问题的限制：首先，在前一代DUN中，不同阶段的神经网络（DNN）无法共享不同阶段学习的特征表示，导致参数稀缺，从而限制了重建的潜力。第二，前一代DUN无法在一个框架下 simultanously 估计数据偏移矩阵和噪声水平，这两者都是重建HSI的关键参数。如果仅仅解决数据问题或先验问题，则其准确性将受到限制。第三，在HSIs中利用本地和非本地的假设是重要的，但是在前一代DUN中，这些假设并未得到充分利用。在本文中，我们首先将DUN转换为回归神经网络（RNN），使得不同阶段的DNN可以共享特征表示，从而提高DUN的表现。其次，我们将偏移估计网络（DERNN） incorporated into RNN，以同时估计数据偏移矩阵和噪声水平，通过对准测试矩阵的学习。最后，我们提出了本地和非本地转换器（LNLT），以有效地利用HSIs中的本地和非本地假设。通过将LNLTintegrated into DERNN，我们提出了DERNN-LNLT，实现了state-of-the-art的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/eess.IV_2023_11_15/" data-id="clpahu7es01dn3h88gqxzb8c0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/eess.SP_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T08:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/eess.SP_2023_11_15/">eess.SP - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-AmBC-Systems-with-Deep-Learning-for-Joint-Channel-Estimation-and-Signal-Detection"><a href="#Enhancing-AmBC-Systems-with-Deep-Learning-for-Joint-Channel-Estimation-and-Signal-Detection" class="headerlink" title="Enhancing AmBC Systems with Deep Learning for Joint Channel Estimation and Signal Detection"></a>Enhancing AmBC Systems with Deep Learning for Joint Channel Estimation and Signal Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09172">http://arxiv.org/abs/2311.09172</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Zargari, A. Hakimi, C. Tellambura, A. Maaref</li>
<li>for: 提高AmBC系统的可靠性和效率，特别是在具有噪声的实际通信 Channel conditions 下</li>
<li>methods: 使用深度神经网络（DNN）进行通道状态估计（CSI）和数据检测，并将两者结合使用，以提高AmBC系统的数据检测精度</li>
<li>results: 对比传统检测器，我们的DNN方法在实际数据中表现出较好的robust性和高效性，尤其在高信号噪声比（SNR）下表现出 aproximately 20%的改善Here’s the translation in English:</li>
<li>for: To improve the reliability and efficiency of AmBC systems, especially in practical communication channels with noise.</li>
<li>methods: Using a deep neural network (DNN) for channel state estimation (CSI) and data detection, and combining the two for improved data detection accuracy in AmBC systems.</li>
<li>results: Our DNN method outperforms traditional detectors in practical data recovery, with an approximately 20% improvement in bit error rate (BER) compared to the maximum likelihood (ML) approach, especially in high signal-to-noise ratio (SNR) conditions.<details>
<summary>Abstract</summary>
The era of ubiquitous, affordable wireless connectivity has opened doors to countless practical applications. In this context, ambient backscatter communication (AmBC) stands out, utilizing passive tags to establish connections with readers by harnessing reflected ambient radio frequency (RF) signals. However, conventional data detectors face limitations due to their inadequate knowledge of channel and RF-source parameters. To address this challenge, we propose an innovative approach using a deep neural network (DNN) for channel state estimation (CSI) and signal detection within AmBC systems. Unlike traditional methods that separate CSI estimation and data detection, our approach leverages a DNN to implicitly estimate CSI and simultaneously detect data. The DNN model, trained offline using simulated data derived from channel statistics, excels in online data recovery, ensuring robust performance in practical scenarios. Comprehensive evaluations validate the superiority of our proposed DNN method over traditional detectors, particularly in terms of bit error rate (BER). In high signal-to-noise ratio (SNR) conditions, our method exhibits an impressive approximately 20% improvement in BER performance compared to the maximum likelihood (ML) approach. These results underscore the effectiveness of our developed approach for AmBC channel estimation and signal detection. In summary, our method outperforms traditional detectors, bolstering the reliability and efficiency of AmBC systems, even in challenging channel conditions.
</details>
<details>
<summary>摘要</summary>
现代无线通信技术已经提供了无限可靠、便宜的连接，开启了无数实用应用。在这个背景下， ambient backscatter 通信（AmBC）占据了一席之地，通过利用反射的 ambient 电磁波（RF）信号，实现了通过 passive 标签与读取器进行连接。然而，传统的数据检测器受到了通道和 RF 源参数的限制。为了解决这个挑战，我们提出了一种创新的方法，使用深度神经网络（DNN）进行通道状态估计（CSI）和信号检测在 AmBC 系统中。不同于传统的方法，我们的方法不分开 CSI 估计和数据检测，而是通过 DNN 来协同估计 CSI 和数据检测。DNN 模型，在线上训练使用 simulate 数据 derived from channel statistics，在实际应用中表现出了优秀的robust性。 comprehensive evaluations 表明，我们提出的 DNN 方法在 BER 性能方面比传统的 ML 方法有约 20% 的提升，特别在高 SNR 条件下。这些结果证明了我们开发的方法在 AmBC 通道估计和信号检测方面的效果。总之，我们的方法在实际应用中表现出了更高的可靠性和效率，即使在具有挑战性的通道条件下。
</details></li>
</ul>
<hr>
<h2 id="Network-Level-Integrated-Sensing-and-Communication-Interference-Management-and-BS-Coordination-Using-Stochastic-Geometry"><a href="#Network-Level-Integrated-Sensing-and-Communication-Interference-Management-and-BS-Coordination-Using-Stochastic-Geometry" class="headerlink" title="Network-Level Integrated Sensing and Communication: Interference Management and BS Coordination Using Stochastic Geometry"></a>Network-Level Integrated Sensing and Communication: Interference Management and BS Coordination Using Stochastic Geometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09052">http://arxiv.org/abs/2311.09052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaitao Meng, Christos Masouros, Guangji Chen, Fan Liu</li>
<li>for: 该研究旨在提高 интеграцион感知通信（ISAC）网络中的感知通信（S&amp;C）性能，特别是在监测频率域中实现有效的平衡。</li>
<li>methods: 该研究使用 Stochastic Geometry 工具来捕捉 S&amp;C 性能，并在 ISAC 网络中ILLuminate 关键的协作依赖关系。根据 derive 的面积 spectral efficiency（ASE）表达式，我们构建了最优化问题，以最大化网络性能的两个共同 S&amp;C 指标。</li>
<li>results: 研究表明，干扰抑制可以提高平均数据率和雷达信息率。另外，在ASE最大化情况下，共同BS集群大小的选择对S&amp;C性能具有灵活的负面影响。此外，我们证明了在优化通信性能时，理想的用户数与发射天线数的比值是一定的常数值。实验结果表明，提案的协作ISAC方案可以在网络级别上获得显著的S&amp;C性能提升。<details>
<summary>Abstract</summary>
In this work, we study integrated sensing and communication (ISAC) networks with the aim of effectively balancing sensing and communication (S&C) performance at the network level. Focusing on monostatic sensing, the tool of stochastic geometry is exploited to capture the S&C performance, which facilitates us to illuminate key cooperative dependencies in the ISAC network and optimize key network-level parameters. Based on the derived tractable expression of area spectral efficiency (ASE), we formulate the optimization problem to maximize the network performance from the view point of two joint S&C metrics. Towards this end, we further jointly optimize the cooperative BS cluster sizes for S&C and the serving/probing numbers of users/targets to achieve a flexible tradeoff between S&C at the network level. It is verified that interference nulling can effectively improve the average data rate and radar information rate. Surprisingly, the optimal communication tradeoff for the case of the ASE maximization tends to employ all spacial resources towards multiplexing and diversity gain, without interference nulling. By contrast, for the sensing objectives, resource allocation tends to eliminate certain interference especially when the antenna resources are sufficient, because the inter-cell interference becomes a more dominant factor affecting sensing performance. Furthermore, we prove that the ratio of the optimal number of users and the number of transmit antennas is a constant value when the communication performance is optimal. Simulation results demonstrate that the proposed cooperative ISAC scheme achieves a substantial gain in S&C performance at the network level.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究了集成感知和通信（ISAC）网络，以实现网络水平的感知和通信（S&C）性能的平衡。我们将注意力集中在单频感知上，使用随机几何工具来捕捉S&C性能，从而照明ISAC网络中关键的合作依赖关系，并且优化关键网络级别参数。基于 derivated的面спектル效率（ASE）表达式，我们形ulated了 maximize 网络性能的优化问题，并且做出了关于 S&C 和服务/探测用户/目标的共同优化。研究结果表明，干扰消除可以有效提高平均数据率和雷达信息率。另外，在ASE maximization情况下，最佳通信交换倾向于使用所有空间资源进行多路复用和多样度增强，而不是干扰消除。在感知目标下，资源分配倾向于消除certain干扰，特别是当antenna资源充足时，因为 между� intercept 成为感知性能的主要影响因素。此外，我们证明了在优化通信性能时，用户数和发射天线数的比率是一定的常量值。实验结果表明，我们提出的合作ISAC方案在网络级别上实现了显著的感知和通信性能提升。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Sensing-Communication-and-Power-Transfer-Multiuser-Beamforming-Design"><a href="#Integrating-Sensing-Communication-and-Power-Transfer-Multiuser-Beamforming-Design" class="headerlink" title="Integrating Sensing, Communication, and Power Transfer: Multiuser Beamforming Design"></a>Integrating Sensing, Communication, and Power Transfer: Multiuser Beamforming Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09028">http://arxiv.org/abs/2311.09028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqin Zhou, Xiaoyang Li, Guangxu Zhu, Jie Xu, Kaibin Huang, Shuguang Cui</li>
<li>for: 这个论文的目的是提出一种基于集成感知通信和能源传输（ISCPT）技术的多用户多输入多天线（MIMO）系统，以提高无线资源利用率。</li>
<li>methods: 本论文使用了多元素矩阵（MIMO） beamforming 设计，以提高感知性能，同时满足通信和能源传输要求。另外，作者还使用了Schur complement transformation和矩阵减少技术解决非对称优化问题。</li>
<li>results: 作者通过 simulations 验证了提议的设计，并发现了感知、通信和能源传输之间的性能协调问题。<details>
<summary>Abstract</summary>
In the sixth-generation (6G) networks, massive low-power devices are expected to sense environment and deliver tremendous data. To enhance the radio resource efficiency, the integrated sensing and communication (ISAC) technique exploits the sensing and communication functionalities of signals, while the simultaneous wireless information and power transfer (SWIPT) techniques utilizes the same signals as the carriers for both information and power delivery. The further combination of ISAC and SWIPT leads to the advanced technology namely integrated sensing, communication, and power transfer (ISCPT). In this paper, a multi-user multiple-input multiple-output (MIMO) ISCPT system is considered, where a base station equipped with multiple antennas transmits messages to multiple information receivers (IRs), transfers power to multiple energy receivers (ERs), and senses a target simultaneously. The sensing target can be regarded as a point or an extended surface. When the locations of IRs and ERs are separated, the MIMO beamforming designs are optimized to improve the sensing performance while meeting the communication and power transfer requirements. The resultant non-convex optimization problems are solved based on a series of techniques including Schur complement transformation and rank reduction. Moreover, when the IRs and ERs are co-located, the power splitting factors are jointly optimized together with the beamformers to balance the performance of communication and power transfer. To better understand the performance of ISCPT, the target positioning problem is further investigated. Simulations are conducted to verify the effectiveness of our proposed designs, which also reveal a performance tradeoff among sensing, communication, and power transfer.
</details>
<details>
<summary>摘要</summary>
在第六代（6G）网络中，巨量低功率设备预计将环境感知并传输巨大数据。为提高电磁资源效率，紧凑感测通信（ISAC）技术利用信号感测和通信功能，同时利用同一个信号作为信息和能量传输的同步无线信息和能量传输（SWIPT）技术。这两种技术的结合，形成了进一步的技术——集成感测通信和能量传输（ISCPT）。在本文中，我们考虑了一个多用户多输入多出力（MIMO）ISCPT系统，其中一个基站装备多个天线发送消息到多个信息收发器（IR），传输能量到多个能量收发器（ER），并同时感测目标。感测目标可以是点或扩展表面。当IR和ER的位置分开时，MIMO扩展设计用于提高感测性能，同时满足通信和能量传输要求。解决的非 convex 优化问题通过序列技术，如Schur complement transformation和矩阵减少。此外，当IR和ER均处于同一个位置时，共同优化功率分配和扩展设计以平衡通信和能量传输的性能。为了更好地理解ISCPT性能，我们进一步调查了目标位置问题。我们的提案的设计通过实验验证，并发现存在性能平衡问题。
</details></li>
</ul>
<hr>
<h2 id="Channel-Estimation-for-mmWave-MIMO-using-sub-6-GHz-Out-of-Band-Information"><a href="#Channel-Estimation-for-mmWave-MIMO-using-sub-6-GHz-Out-of-Band-Information" class="headerlink" title="Channel Estimation for mmWave MIMO using sub-6 GHz Out-of-Band Information"></a>Channel Estimation for mmWave MIMO using sub-6 GHz Out-of-Band Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08996">http://arxiv.org/abs/2311.08996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faruk Pasic, Markus Hofer, Mariam Mussbah, Sebastian Caban, Stefan Schwarz, Thomas Zemen, Christoph F. Mecklenbräuker</li>
<li>for: 提高 millimeter wave（mmWave）通信系统中MIMO通信链的可靠性和吞吐量。</li>
<li>methods: 使用从sub-6GHz频段获得的外带信息来估算mmWave MIMO通信频道。</li>
<li>results: 比对传统使用只带内带信息的mmWave通信频道估算方法，提出三种新的通信频道估算方法，实验结果显示，提案方法在低SNR和高K因子下表现较为优秀，可以提高spectral efficiency。<details>
<summary>Abstract</summary>
Future wireless multiple-input multiple-output (MIMO) communication systems will employ sub-6 GHz and millimeter wave (mmWave) frequency bands working cooperatively. Establishing a MIMO communication link usually relies on estimating channel state information (CSI) which is difficult to acquire at mmWave frequencies due to a low signal-to-noise ratio (SNR). In this paper, we propose three novel methods to estimate mmWave MIMO channels using out-of-band information obtained from the sub-6GHz band. We compare the proposed channel estimation methods with a conventional one utilizing only in-band information. Simulation results show that the proposed methods outperform the conventional mmWave channel estimation method in terms of achievable spectral efficiency, especially at low SNR and high K-factor.
</details>
<details>
<summary>摘要</summary>
未来的无线多输入多出力（MIMO）通信系统将使用低于6GHz和毫米波（mmWave）频率band工作协作。建立MIMO通信链接通常需要估算通道状态信息（CSI），但mmWave频率band中的信号至噪比（SNR）很低，这使得频率估算变得更加困难。在这篇论文中，我们提出了三种新的mmWave通道估算方法，使用来自低于6GHz频率band的外带信息。我们与传统的尝试估算方法进行比较，结果显示，我们的提议方法在低SNR和高K因子下的可实现 spectral efficiency 方面具有显著的提高。
</details></li>
</ul>
<hr>
<h2 id="EMF-Aware-Power-Control-for-Massive-MIMO-Cell-Free-versus-Cellular-Networks"><a href="#EMF-Aware-Power-Control-for-Massive-MIMO-Cell-Free-versus-Cellular-Networks" class="headerlink" title="EMF-Aware Power Control for Massive MIMO: Cell-Free versus Cellular Networks"></a>EMF-Aware Power Control for Massive MIMO: Cell-Free versus Cellular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08989">http://arxiv.org/abs/2311.08989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergi Liesegang, Stefano Buzzi</li>
<li>for: 这篇论文主要针对用户中心无线数据网络中电磁干扰问题进行了研究，以提高系统的可扩展性和可靠性。</li>
<li>methods: 该论文使用了最大化最小数据速率的能量分配策略，以满足EMF安全限制。</li>
<li>results:  simulation结果表明，该策略可以轻松遵守EMF安全限制，同时不影响最小数据速率。此外，CF-mMIMO系统也比多单元巨量MIMO系统更高效，而且提档的能量分配策略可以提高系统公平性。<details>
<summary>Abstract</summary>
The impressive growth of wireless data networks has recently led to increased attention to the issue of electromagnetic pollution. Specific absorption rates and incident power densities have become popular indicators for measuring electromagnetic field (EMF) exposure. This paper tackles the problem of power control in user-centric cell-free massive multiple-input-multiple-output (CF-mMIMO) systems under EMF constraints. Specifically, the power allocation maximizing the minimum data rate across users is derived for both the uplink and the downlink under EMF constraints. The developed solution is also applied to a cellular mMIMO system and compared to other benchmark strategies. Simulation results prove that EMF safety restrictions can be easily met without jeopardizing the minimum data rate, that the CF-mMIMO outperforms the multi-cell massive MIMO deployment, and that the proposed power control strategy greatly improves the system fairness.
</details>
<details>
<summary>摘要</summary>
“受到无线数据网络的快速发展所带来的电磁污染问题已经引起了更多的关注。特别是对电磁场(EMF)曝露的评估指标 Specific Absorption Rate 和 Incident Power Density 的使用。本文对用户中心的无线免系大量多input多output（CF-mMIMO）系统中的能量控制进行了研究，以满足EMF的限制。具体来说，我们在下调和下调之下，对于每个用户最大化最小的数据率的能量分配。此外，我们还将此解释应用到了一个细节的Cellular mMIMO系统中，并与其他参考策略进行比较。实验结果显示：1. EMF安全限制可以轻松满足，不会对最小的数据率造成影响；2. CF-mMIMO比多组巨量MIMO部署更好，3. 我们的能量控制策略可以大大提高系统公平性。”
</details></li>
</ul>
<hr>
<h2 id="Throughput-Maximisation-in-Ultra-wideband-Hybrid-amplified-Links"><a href="#Throughput-Maximisation-in-Ultra-wideband-Hybrid-amplified-Links" class="headerlink" title="Throughput Maximisation in Ultra-wideband Hybrid-amplified Links"></a>Throughput Maximisation in Ultra-wideband Hybrid-amplified Links</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08964">http://arxiv.org/abs/2311.08964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henrique Buglia, Eric Sillekens, Lidia Galdino, Robert Killey, Polina Bayvel1</li>
<li>for: 这篇论文是为了提高hybrid-amplified links的吞吐量而写的。</li>
<li>methods: 论文使用了semi-analytical方法和实时非线性干扰模型，并结合粒子群优化算法来最大化hybrid-amplified links的吞吐量。</li>
<li>results: 论文通过结合粒子群优化算法来提高hybrid-amplified 10.5 THz 117x57 km 链路的吞吐量，比起EDFAs-only配置提高了12%。<details>
<summary>Abstract</summary>
A semi-analytical, real-time nonlinear-interference model including ASE noise in hybrid-amplified links is introduced. Combined with particle-swarm optimisation, the capacity of a hybrid-amplified 10.5 THz 117x57 km link was maximised, increasing throughput by 12% versus an EDFAs-only configuration.
</details>
<details>
<summary>摘要</summary>
“一种半分析式、实时非线性干扰模型，包括ASE噪声，在混合增强链路中引入。与particle-swarm优化结合，hybrid-amplified 10.5 THz 117x57 km 链路的容量最大化，比EDFAs-only配置提高了12%的吞吐量。”Here's a breakdown of the translation:* “半分析式”(pán fēn yì jì) - semi-analytical* “实时非线性干扰模型”(shí jì fēn xiǎn yì jì mó delè) - real-time nonlinear-interference model* “ASE噪声”(ASE nóng shēng) - ASE noise* “混合增强链路”(hù hé zēng jiāng liàng) - hybrid-amplified link* “容量”(róng kè) - capacity* “最大化”(zmài huì) - maximized* “比EDFAs-only配置”(bǐ EDFAs-only zhèng jì) - compared to an EDFAs-only configuration* “吞吐量”(tōng chuō liàng) - throughputI hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="Design-and-Implementation-of-a-Hybrid-Wireless-Power-and-Communication-System-for-Medical-Implants"><a href="#Design-and-Implementation-of-a-Hybrid-Wireless-Power-and-Communication-System-for-Medical-Implants" class="headerlink" title="Design and Implementation of a Hybrid Wireless Power and Communication System for Medical Implants"></a>Design and Implementation of a Hybrid Wireless Power and Communication System for Medical Implants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08933">http://arxiv.org/abs/2311.08933</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Khaleghi, A. Hasanvand, I. Balasingham</li>
<li>for: 这个论文目的是提供一种基于无线电力的嵌入式设备，用于预防和早期发现许多慢性疾病。</li>
<li>methods: 该论文使用了人体内部无线电力供应、感知和通信技术，并应用了人工智能（AI）和机器学习（ML）分析大数据技术。</li>
<li>results: 该论文提出了一种基于401MHz无线电波的无线嵌入式设备，通过两个同时的无线链路进行设备之间的通信，并实现了深度401MHz的无线电力供应。<details>
<summary>Abstract</summary>
Data collection and analysis from multiple implant nodes in humans can provide targeted medicine and treatment strategies that can prevent many chronic diseases. This data can be collected for a long time and processed using artificial intelligence (AI) techniques in a medical network for early detection and prevention of diseases. Additionally, machine learning (ML) algorithms can be applied for the analysis of big data for health monitoring of the population. Wireless powering, sensing, and communication are essential parts of future wireless implants that aim to achieve the aforementioned goals. In this paper, we present the technical development of a wireless implant that is powered by radio frequency (RF) at 401 MHz, with the sensor data being communicated to an on-body reader. The implant communication is based on two simultaneous wireless links: RF backscatter for implant-to-on-body communication and a galvanic link for intra-body implant-to-implant connectivity. It is demonstrated that RF powering, using the proposed compact antennas, can provide an efficient and integrable system for powering up to an 8 cm depth inside body tissues. Furthermore, the same antennas are utilized for backscatter and galvanic communication.
</details>
<details>
<summary>摘要</summary>
《数据采集和分析从多个人体节点可以提供Targeted医疗和治疗策略，预防许多慢性疾病。这些数据可以长期采集并使用人工智能（AI）技术进行处理，在医疗网络中进行早期检测和预防疾病。此外，机器学习（ML）算法可以用于分析大量数据，进行人群健康监测。在这篇论文中，我们介绍了无线嵌入式设备的技术开发，该设备由401MHzRadio frequency（RF）电磁辐射供电，感知数据通过身体上的读取器与设备进行通信。该设备通信基于两个同时的无线链路：RF扫描 для设备与身体之间的通信，以及 galvanic链接用于体内设备之间的通信。我们展示了RF供电，使用我们提出的减小型天线，可以提供高效和可集成的系统，在身体组织中进行深度401 MHz的供电。此外，同一天线也用于扫描和 galvanic 通信。
</details></li>
</ul>
<hr>
<h2 id="Energy-Efficient-Design-of-Satellite-Terrestrial-Computing-in-6G-Wireless-Networks"><a href="#Energy-Efficient-Design-of-Satellite-Terrestrial-Computing-in-6G-Wireless-Networks" class="headerlink" title="Energy-Efficient Design of Satellite-Terrestrial Computing in 6G Wireless Networks"></a>Energy-Efficient Design of Satellite-Terrestrial Computing in 6G Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08904">http://arxiv.org/abs/2311.08904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Wang, Xiaoming Chen, Qiao Qi</li>
<li>for: 本文研究 sixth generation（6G）无线网络中的卫星地面计算问题，其中多个地面基站（BS）和低地球轨道卫星（LEO）合作提供边缘计算服务，为全球的地面用户设备（GUE）和空间用户设备（SUE）提供服务。</li>
<li>methods: 本文提出了一种完整的卫星地面计算过程，包括通信和计算方面的设计，以适应6G无线网络的特点。</li>
<li>results: 对于卫星地面计算，提出了一种能效的卫星地面计算算法，通过同时优化卫星与地面基站之间数据传输和计算任务的分配，以最小化加权总能consumption，并保证计算任务的延迟要求。实验和理论分析结果都表明，提出的算法在6G无线网络中的卫星地面计算方面具有快速吞吐和优秀性能。<details>
<summary>Abstract</summary>
In this paper, we investigate the issue of satellite-terrestrial computing in the sixth generation (6G) wireless networks, where multiple terrestrial base stations (BSs) and low earth orbit (LEO) satellites collaboratively provide edge computing services to ground user equipments (GUEs) and space user equipments (SUEs) over the world. In particular, we design a complete process of satellite-terrestrial computing in terms of communication and computing according to the characteristics of 6G wireless networks. In order to minimize the weighted total energy consumption while ensuring delay requirements of computing tasks, an energy-efficient satellite-terrestrial computing algorithm is put forward by jointly optimizing offloading selection, beamforming design and resource allocation. Finally, both theoretical analysis and simulation results confirm fast convergence and superior performance of the proposed algorithm for satellite-terrestrial computing in 6G wireless networks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了 sixth generation（6G）无线网络中的卫星-地面计算问题，其中多个地面基站（BS）和低地球轨卫星（LEO）共同提供边缘计算服务给地面用户设备（GUE）和空间用户设备（SUE）。特别是，我们设计了6G无线网络中卫星-地面计算的完整过程，包括通信和计算方面。为了最小化加权总能 consumption，我们提出了一种能效的卫星-地面计算算法，通过共同优化卸载选择、扩散设计和资源分配来确保计算任务的延迟要求。最后，我们通过理论分析和模拟结果，证明了我们提出的算法在6G无线网络中的快速叠合和优秀表现。
</details></li>
</ul>
<hr>
<h2 id="RIS-Position-and-Orientation-Estimation-via-Multi-Carrier-Transmissions-and-Multiple-Receivers"><a href="#RIS-Position-and-Orientation-Estimation-via-Multi-Carrier-Transmissions-and-Multiple-Receivers" class="headerlink" title="RIS Position and Orientation Estimation via Multi-Carrier Transmissions and Multiple Receivers"></a>RIS Position and Orientation Estimation via Multi-Carrier Transmissions and Multiple Receivers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08887">http://arxiv.org/abs/2311.08887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Ghazalian, Hui Chen, George C. Alexandropoulos, Gonzalo Seco-Granados, Henk Wymeersch, Riku Jäntti</li>
<li>for: 本研究旨在探讨透过嵌入智能表面技术实现的 sixth generation无线系统的可定位和探测能力。</li>
<li>methods: 本文使用时间射频和空间频率测量来实现透过嵌入智能表面的用户定位问题的解决方案。</li>
<li>results:  simulations 结果表明，提案的 RIS 状态估计方法在不同系统操作参数下具有高效性。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surfaces (RISs) are considered as an enabling technology for the upcoming sixth generation of wireless systems, exhibiting significant potential for radio localization and sensing. An RIS is usually treated as an anchor point with known position and orientation when deployed to offer user localization. However, it can also be attached to a user to enable its localization in a semi-passive manner. In this paper, we consider a static user equipped with an RIS and study the RIS localization problem (i.e., joint three-dimensional position and orientation estimation), when operating in a system comprising a single-antenna transmitter and multiple synchronized single-antenna receivers with known locations. We present a multi-stage estimator using time-of-arrival and spatial frequency measurements, and derive the Cram\'er-Rao lower bounds for the estimated parameters to validate the estimator's performance. Our simulation results demonstrate the efficiency of the proposed RIS state estimation approach under various system operation parameters.
</details>
<details>
<summary>摘要</summary>
弹性智能表面（RIS）被视为 sixth generation无线系统的核心技术，具有很大的射频地域和感知潜力。一个RIS通常被视为一个已知位置和方向的锚点，当它被部署时。但是，它也可以附加到用户来实现半通信式的用户位置测量。在这篇论文中，我们考虑一个静止的用户，装备了RIS，并研究RIS的位置识别问题（即三维位置和方向的共同估计），在一个具有单antenna传送器和多个同步化单antenna接收器的系统中进行。我们提出了一个多阶估计器，使用时间对应和频率对应的测量，并 derivated Cramér-Rao下界来验证估计器的性能。我们的实验结果显示了该RIS状态估计方法的效率，在不同的系统运行参数下。
</details></li>
</ul>
<hr>
<h2 id="Aerial-IRS-with-Robotic-Anchoring-Capabilities-A-Novel-Way-for-Adaptive-Coverage-Enhancement"><a href="#Aerial-IRS-with-Robotic-Anchoring-Capabilities-A-Novel-Way-for-Adaptive-Coverage-Enhancement" class="headerlink" title="Aerial IRS with Robotic Anchoring Capabilities: A Novel Way for Adaptive Coverage Enhancement"></a>Aerial IRS with Robotic Anchoring Capabilities: A Novel Way for Adaptive Coverage Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08876">http://arxiv.org/abs/2311.08876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyuan Wu, Vasilis Friderikos</li>
<li>for: 提高无线网络覆盖率和用户服务质量 (improving wireless network coverage and end-user Quality of Service)</li>
<li>methods: 使用机器人飞行智能反射表 (RA-IRSs)，具有能gie neutral manner具有钻取灯杆等高层城市地形的机制，从而完全消除飞行&#x2F;停靠能 consumption和提供多小时或者多天服务 (eliminating flying&#x2F;hovering energy consumption and providing multiple hours or days of service)</li>
<li>results: 使用RA-IRSs可以提高网络性能，通过变化锚点位置以遵循空间时间交通需求 (improving network performance by changing anchoring locations to follow spatial-temporal traffic demand)，提供了高度异ogeneous区域中的显著信噪比提升 (significant Signal-to-Noise ratio gain in highly heterogeneous regions)，并且在高异ogeneous区域中可以支持更多的流量需求 (sustaining more than 2 times the traffic demand in areas experiencing high heterogeneity)。<details>
<summary>Abstract</summary>
It is widely accepted that integrating intelligent reflecting surfaces (IRSs) with unmanned aerial vehicles (UAV) or drones can assist wireless networks in improving network coverage and end user Quality of Service (QoS). However, the critical constrain of drones is their very limited hovering/flying time. In this paper we propose the concept of robotic aerial IRSs (RA-IRSs), which are in essence drones that in addition to IRS embed an anchoring mechanism that allows them to grasp in an energy neutral manner at tall urban landforms such as lampposts. By doing so, RA-IRSs can completely eliminate the flying/hovering energy consumption and can offer service for multiple hours or even days (something not possible with UAV-mounted IRSs). Using that property we show how RA-IRS can increase network performance by changing their anchoring location to follow the spatio-temporal traffic demand. The proposed methodology, developed through Integer Linear Programming (ILP) formulations offers a significant Signal-to-Noise (SNR) gain in highly heterogeneous regions in terms of traffic demand compared to fixed IRS; hence, addressing urban coverage discrepancies effectively. Numerical simulations validate the superiority of RA-IRSs over fixed terrestrial IRSs in terms of traffic serviceability, sustaining more than 2 times the traffic demand in areas experiencing high heterogeneity, emphasizing their adaptability in improving coverage and QoS in complex urban terrains.
</details>
<details>
<summary>摘要</summary>
广泛接受到了将智能反射表（IRS）与无人机（UAV）或无人飞行器（drone）结合，以提高无线网络的覆盖范围和用户服务质量（QoS）。然而，无人机的缺点是其很有限的悬挂/飞行时间。在这篇论文中，我们提出了机器人空中智能反射表（RA-IRS）的概念，它们是具有悬挂机制的无人机，可以在能源中立Positions中静止，以减少或完全消除飞行/悬挂时间的能量消耗。通过这种方式，RA-IRS可以提供多个小时或者多天的服务（不可能由UAV-IRS所实现）。我们使用了Integer Linear Programming（ILP）方法开发了一种新的方法ологи，以便通过更改悬挂位置来跟踪空间时间具有不同强度的负载均衡问题。我们的方法可以在高度不均的区域中提供明显的信噪比（SNR）提升，从而有效地解决城市覆盖不平等问题。数值仿真 validate了RA-IRSs的superiority，可以在高度不均的区域中支持更多的流量，达到2倍以上的流量可用性，强调其适应性在复杂的城市地形中。
</details></li>
</ul>
<hr>
<h2 id="Phase-retrieval-with-semi-algebraic-and-ReLU-neural-network-priors"><a href="#Phase-retrieval-with-semi-algebraic-and-ReLU-neural-network-priors" class="headerlink" title="Phase retrieval with semi-algebraic and ReLU neural network priors"></a>Phase retrieval with semi-algebraic and ReLU neural network priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08833">http://arxiv.org/abs/2311.08833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tamir Bendory, Nadav Dym, Dan Edidin, Arun Suresh</li>
<li>for: 解决phaserecovery问题</li>
<li>methods: 使用semi-algebraic set prior</li>
<li>results: 可以从傅ри格式中获取信号的签名，并且可以确定信号的签名是唯一的，只要信号 lie在一个 semi-algebraic set中。这个result generalizes to multi-reference alignment models with multiplicity free representations of compact groups.<details>
<summary>Abstract</summary>
The key ingredient to retrieving a signal from its Fourier magnitudes, namely, to solve the phase retrieval problem, is an effective prior on the sought signal. In this paper, we study the phase retrieval problem under the prior that the signal lies in a semi-algebraic set. This is a very general prior as semi-algebraic sets include linear models, sparse models, and ReLU neural network generative models. The latter is the main motivation of this paper, due to the remarkable success of deep generative models in a variety of imaging tasks, including phase retrieval. We prove that almost all signals in R^N can be determined from their Fourier magnitudes, up to a sign, if they lie in a (generic) semi-algebraic set of dimension N/2. The same is true for all signals if the semi-algebraic set is of dimension N/4. We also generalize these results to the problem of signal recovery from the second moment in multi-reference alignment models with multiplicity free representations of compact groups. This general result is then used to derive improved sample complexity bounds for recovering band-limited functions on the sphere from their noisy copies, each acted upon by a random element of SO(3).
</details>
<details>
<summary>摘要</summary>
“对于从傅立叶展开的信号重建问题，键的因素是一个有效的先前知识。本文研究对半代数集的信号重建问题，这是非常通用的先前知识，因为半代数集包括线性模型、简单模型和ReLU神经网络生成模型。后者是这篇文章的主要动机，因为深度生成模型在各种图像任务中表现出色。我们证明，如果信号 lying in a （普通）半代数集的话，则可以从傅立叶展开中恢复信号，保留信号的符号， provided that the dimension of the semi-algebraic set is at least N/2。同样的，如果半代数集的维度是 N/4，则所有的信号都可以从傅立叶展开中恢复。我们还将这些结果推广到多对对称定理中的问题，并使用多对对称定理来 derive improved sample complexity bounds for recovering band-limited functions on the sphere from their noisy copies, each acted upon by a random element of SO(3).”Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Wireless-Communications-in-Cavity-A-Reconfigurable-Boundary-Modulation-based-Approach"><a href="#Wireless-Communications-in-Cavity-A-Reconfigurable-Boundary-Modulation-based-Approach" class="headerlink" title="Wireless Communications in Cavity: A Reconfigurable Boundary Modulation based Approach"></a>Wireless Communications in Cavity: A Reconfigurable Boundary Modulation based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08810">http://arxiv.org/abs/2311.08810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuehui Dong, Xiang Ren, Bokai Lai, Rujing Xiong, Tiebin Mi, Robert Caiming Qiu</li>
<li>for: 这篇论文探讨了嵌入智能表面（RIS）在干扰波传播环境中的无线通信应用potential.</li>
<li>methods: 论文首次引入了可重新配置的边界修饰框架，并提出了一种可靠的边界修饰方案，通过RIS生成的等效脉冲实现了脉冲位模式（PPM）无线通信。</li>
<li>results: 实验结果显示，这种方法可以在原型中实现约2Mbps的比特率，并具有强 resistivity to channel的频率选择性，导致比特错误率非常低。<details>
<summary>Abstract</summary>
This paper explores the potential wireless communication applications of Reconfigurable Intelligent Surfaces (RIS) in reverberant wave propagation environments. Unlike in free space, we utilize the sensitivity to boundaries of the enclosed electromagnetic (EM) field and the equivalent perturbation of RISs. For the first time, we introduce the framework of reconfigurable boundary modulation in the cavities . We have proposed a robust boundary modulation scheme that exploits the continuity of object motion and the mutation of the codebook switch, which achieves pulse position modulation (PPM) by RIS-generated equivalent pulses for wireless communication in cavities. This approach achieves around 2 Mbps bit rate in the prototype and demonstrates strong resistance to channel's frequency selectivity resulting in an extremely low bit error rate (BER).
</details>
<details>
<summary>摘要</summary>
（本文探讨了嵌入式智能表面（RIS）在干扰波传播环境中的无线通信应用。与在自由空间中不同，我们利用封闭电磁场边界敏感性和相同的RIS做法。我们为首次引入了可重新配置边缘调制框架在镜室中。我们提出了一种强健的边缘调制方案，利用物体运动连续性和代码库交换互变，实现了由RIS生成的等效脉冲干扰器为无线通信在镜室中实现的脉冲位调制（PPM）。这种方法在原型中实现了约2Mbps的比特率和非常低的比特错误率（BER）。）
</details></li>
</ul>
<hr>
<h2 id="Channel-Capacity-and-Bounds-In-Mixed-Gaussian-Impulsive-Noise"><a href="#Channel-Capacity-and-Bounds-In-Mixed-Gaussian-Impulsive-Noise" class="headerlink" title="Channel Capacity and Bounds In Mixed Gaussian-Impulsive Noise"></a>Channel Capacity and Bounds In Mixed Gaussian-Impulsive Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08804">http://arxiv.org/abs/2311.08804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianfu Qi, Jun Wang, Qihang Peng, Xiaoping Li, Xiaonan Chen</li>
<li>for:  This paper investigates the channel capacity of communication systems under mixed noise, which consists of both non-Gaussian impulsive noise (IN) and white Gaussian noise (WGN).</li>
<li>methods: The authors use mathematical proofs and numerical results to study the channel capacity under p-th moment constraint and show that there are only finite mass points in the capacity-achieving distribution.</li>
<li>results: The authors provide lower and upper capacity bounds with closed forms, and show that the lower bounds can degenerate to the well-known Shannon formula under special scenarios. Numerical results reveal that the capacity decreases when the impulsiveness of the mixed noise becomes dominant, and the obtained capacity bounds are shown to be very tight.Here’s the Chinese text:</li>
<li>for: 这篇论文研究了含杂噪的通信系统频率的频率容量，其中包括非高斯噪声（IN）和白噪声（WGN）。</li>
<li>methods: 作者使用数学证明和数值结果来研究频率容量下p-th moment约束的存在和只有有限多个质量点的分布。</li>
<li>results: 作者提供了下界和上界的频率容量 bound，并显示了这些下界可以在特定情况下逐渐变为著名的雪伦 формула。numerical results表明，杂噪的强度增加时，频率容量减少，并且获得的容量 bound是非常紧致的。<details>
<summary>Abstract</summary>
Communication systems suffer from the mixed noise consisting of both non-Gaussian impulsive noise (IN) and white Gaussian noise (WGN) in many practical applications. However, there is little literature about the channel capacity under mixed noise. In this paper, we prove the existence of the capacity under p-th moment constraint and show that there are only finite mass points in the capacity-achieving distribution. Moreover, we provide lower and upper capacity bounds with closed forms. It is shown that the lower bounds can degenerate to the well-known Shannon formula under special scenarios. In addition, the capacity for specific modulations and the corresponding lower bounds are discussed. Numerical results reveal that the capacity decreases when the impulsiveness of the mixed noise becomes dominant and the obtained capacity bounds are shown to be very tight.
</details>
<details>
<summary>摘要</summary>
通信系统在实际应用中常常受到杂合噪声（IN）和白噪声（WGN）的杂合噪声的影响。然而，关于杂合噪声下的通道容量，现有文献不多。在这篇论文中，我们证明了容量下p-th moment约束的存在，并表明了 achieve 分布中具有finite mass点。此外，我们还提供了下界和上界的容量 bound，它们具有封闭形式。显示，下界可以在特定情况下逐渐变为著名的雪伦方程。此外，我们还讨论了特定模ulation的容量和相关下界。numerical result表明，杂合噪声占主导地位时，通道容量减少，而我们所获得的容量 bound 具有很高精度。
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-DOA-Estimation-via-a-Novel-Tree-Model-based-Deep-Neural-Network"><a href="#High-Resolution-DOA-Estimation-via-a-Novel-Tree-Model-based-Deep-Neural-Network" class="headerlink" title="High-Resolution DOA Estimation via a Novel Tree Model-based Deep Neural Network"></a>High-Resolution DOA Estimation via a Novel Tree Model-based Deep Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08758">http://arxiv.org/abs/2311.08758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Li, Feng Shu, Yaoliang Song, Jiangzhou Wang<br>for: 这种论文是为了提高深度神经网络（DNN）在评估偏角时的性能而写的。methods: 该论文提出了一种基于树模型的深度神经网络（TDNN），它包含多个小规模DNN的层，从第一层到最后一层，每个层都是将angular region分解成更小的子区域，并通过积加多个层的分类结果来获得最终的DOA估计结果。results:  simulations results表明，TDNN在单源和多源情况下的估计性能都远胜传统方法，特别是在低信号噪听比（SNR）下。<details>
<summary>Abstract</summary>
Traditional deep neural networks (DNNs) have bad performance on estimating off-grid angles, and the most direct solution is to increase the number of output classes for improving angular resolution. But more output classes can weaken the model accuracy of DNNs and thus decreasing the direction-of-arrival (DOA) estimation accuracy. In this work, a tree-model based deep neural networks (TDNN) is proposed, which contains H layers and each layer is consist of multiple small-scale DNNs. From the first layer to the last layer of TDNN, the angular region is gradually divided into smaller subregions by these DNNs, and the estimated DOA is finally obtained by cumulative calculating the classification results of all the layers. TDNN can improve the angular resolution by increasing the number of layers or the number of DNNs in any layer instead of changing the structure of single DNN, so the model accuracy of TDNN will not decrease with the improvement of angular resolution and its estimation performance is also stable. In addition, the Q-TDNN method is also proposed for multi-sources DOA estimation, which can obtain Q different DOAs from the same signals by combining Q independent and parallel TDNNs. The simulation results validate TDNN has much better estimation performance than traditional methods in both single-source and multi-sources cases, especially at low signal-to-noise ratio (SNR).
</details>
<details>
<summary>摘要</summary>
传统的深度神经网络（DNNs）在未经授标的角度估计方面表现不佳，而直接解决方法是增加出力类型数以提高角度分辨率。但是，增加出力类型数会弱化DNNs模型的准确性，因此降低方向来源估计精度。在这种情况下，一种基于树模型的深度神经网络（TDNN）被提出，它包含H层，每层都包含多个小规模DNNs。从TDNN的第一层到最后一层，每层的angular region逐渐分解为更小的子区域，并通过累加所有层的类别结果来获得最终的DOA估计结果。TDNN可以通过增加层数或增加每层DNNs的数量来提高角度分辨率，而不是改变单个DNN的结构，因此TDNN的模型准确性不会随着角度分辨率的提高而下降。此外，Q-TDNN方法也被提出用于多源DOA估计，可以通过将Q个独立并平行的TDNN组合来获得Q个不同的DOA。实验结果表明，TDNN在单源和多源情况下都有较好的估计性能，特别是在低SNR情况下。
</details></li>
</ul>
<hr>
<h2 id="Near-Field-Wideband-Secure-Communications-An-Analog-Beamfocusing-Approach"><a href="#Near-Field-Wideband-Secure-Communications-An-Analog-Beamfocusing-Approach" class="headerlink" title="Near-Field Wideband Secure Communications: An Analog Beamfocusing Approach"></a>Near-Field Wideband Secure Communications: An Analog Beamfocusing Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08738">http://arxiv.org/abs/2311.08738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchen Zhang, Haiyang Zhang, Wanbin Tang, Yonina C. Eldar</li>
<li>for: 本研究旨在提高Physical Layer Security (PLS)在近场宽频通信中。</li>
<li>methods: 我们提出了一种True-Time Delayer (TTD)-包含的分析式束缚技术，以 Address the interplay between near-field propagation and wideband beamsplit。我们采用了一种两阶段的优化方法，包括半数字解决方案和分析approximation。</li>
<li>results: 我们的方法可以 clearly demonstrate the superiority of the proposed methods over TTD-free approaches in fortifying wideband PLS, as well as the advantageous secrecy energy efficiency achieved by leveraging low-cost analog devices。<details>
<summary>Abstract</summary>
In the rapidly advancing landscape of six-genration (6G), characterized by ultra-high-speed wideband transmission in millimeter-wave and terahertz bands, our paper addresses the pivotal task of enhancing physical layer security (PLS) within near-field wideband communications. We introduce true-time delayer (TTD)-incorporated analog beamfocusing techniques designed to address the interplay between near-field propagation and wideband beamsplit, an uncharted domain in existing literature. Our approach to maximizing secrecy rates involves formulating an optimization problem for joint power allocation and analog beamformer design, employing a two-stage process encompassing a semi-digital solution and analog approximation. This problem is efficiently solved through a combination of alternating optimization, fractional programming, and block successive upper-bound minimization techniques. Additionally, we present a low-complexity beamsplit-aware beamfocusing strategy, capitalizing on geometric insights from near-field wideband propagation, which can also serve as a robust initial value for the optimization-based approach. Numerical results substantiate the efficacy of the proposed methods, clearly demonstrating their superiority over TTD-free approaches in fortifying wideband PLS, as well as the advantageous secrecy energy efficiency achieved by leveraging low-cost analog devices.
</details>
<details>
<summary>摘要</summary>
在六代（6G）迅速发展的背景下，我们的论文关注Physical Layer Security（PLS）在近场宽频通信中的提升。我们介绍了包含真实时间延迟（TTD）的Analog beamforming技术，以解决近场传播和宽频扫描之间的互动，这是现有文献中未曾探讨的领域。我们的方法是通过对共同功率分配和Analog beamformer设计进行优化，使用两个阶段的过程：一个半数字解决方案和Analog近似。这个问题可以通过 alternate optimization、分数编程和块顺序上升最小化技术来有效地解决。此外，我们还提出了一种低复杂度扫描矩阵射频环境中的Beamforming策略，基于近场宽频传播的几何意味，这也可以作为优化方法的robust初值。数值结果证明了我们提出的方法的有效性，明确地表明它们在加强宽频PLS方面的优势，以及通过利用低成本的Analog设备实现的高效秘密能量占用率。
</details></li>
</ul>
<hr>
<h2 id="Massive-Wireless-Energy-Transfer-without-Channel-State-Information-via-Imperfect-Intelligent-Reflecting-Surfaces"><a href="#Massive-Wireless-Energy-Transfer-without-Channel-State-Information-via-Imperfect-Intelligent-Reflecting-Surfaces" class="headerlink" title="Massive Wireless Energy Transfer without Channel State Information via Imperfect Intelligent Reflecting Surfaces"></a>Massive Wireless Energy Transfer without Channel State Information via Imperfect Intelligent Reflecting Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08720">http://arxiv.org/abs/2311.08720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Luo, Jie Hu, Luping Xiang, Kun Yang, Kai-Kit Wong</li>
<li>for: 提高无线能量传输效率和互联网智能设备的连接数量</li>
<li>methods: 使用低成本、静止反射元件，并利用phasered beam rotation技术，实现无 Channel State Information (CSI) 探测 schemes</li>
<li>results: 提高了无线传输效率和设备连接数量，并在大规模设备场景下表现更高效，不需要额外的硬件更新或修改<details>
<summary>Abstract</summary>
Intelligent Reflecting Surface (IRS) utilizes low-cost, passive reflecting elements to enhance the passive beam gain, improve Wireless Energy Transfer (WET) efficiency, and enable its deployment for numerous Internet of Things (IoT) devices. However, the increasing number of IRS elements presents considerable channel estimation challenges. This is due to the lack of active Radio Frequency (RF) chains in an IRS, while pilot overhead becomes intolerable. To address this issue, we propose a Channel State Information (CSI)-free scheme that maximizes received energy in a specific direction and covers the entire space through phased beam rotation. Furthermore, we take into account the impact of an imperfect IRS and meticulously design the active precoder and IRS reflecting phase shift to mitigate its effects. Our proposed technique does not alter the existing IRS hardware architecture, allowing for easy implementation in the current system, and enabling access or removal of any Energy Receivers (ERs) without additional cost. Numerical results illustrate the efficacy of our CSI-free scheme in facilitating large-scale IRS without compromising performance due to excessive pilot overhead. Furthermore, our scheme outperforms the CSI-based counterpart in scenarios involving large-scale ERs, making it a promising solution in the era of IoT.
</details>
<details>
<summary>摘要</summary>
智能反射Surface（IRS）利用低成本、 Passtive 反射元件提高无源能量传输（WET）效率，并使其可以用于互联网未来设备。然而，随着IRS元件的增加，通道估算带来了很大挑战。这是因为IRS没有活动Radio Frequency（RF）链，而且在pilot overhead增加的情况下，估算变得不可持。为了解决这个问题，我们提出了一种CSIfree的方案，它可以在特定的方向上最大化接收能量，并通过phasered beam rotation覆盖整个空间。此外，我们考虑了IRS的不完美性，并仔细设计了活动预编器和反射相位调整，以mitigate其影响。我们的提议方法不会改变现有IRS硬件架构，因此可以轻松实现在现有系统中，并且允许ER的添加或 removalfoundation without additional cost。数字结果表明我们的CSIfree方案可以在大规模IRS scenario中实现高性能，而且在大量ER的场景下，我们的方案比CSIs based counterpart更高效，这使得它成为iot时代的一个有望的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Low-Complexity-High-Speed-Deep-Neural-Network-Augmented-Wireless-Channel-Estimation"><a href="#Low-Complexity-High-Speed-Deep-Neural-Network-Augmented-Wireless-Channel-Estimation" class="headerlink" title="Low Complexity High Speed Deep Neural Network Augmented Wireless Channel Estimation"></a>Low Complexity High Speed Deep Neural Network Augmented Wireless Channel Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08689">http://arxiv.org/abs/2311.08689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Asrar ul haq, Varun Singh, Bhanu Teja Tanaji, Sumit Darak</li>
<li>for: 提高 wireless receiver 的频率 estimation 精度和速度。</li>
<li>methods: 使用 Deep Neural Network-Augmented Least Square (LC-LSDNN) 算法，并对 received complex symbols 使用不同的 Deep Neural Network (DNN) 进行处理。</li>
<li>results: 与 MMSE 和现有 DL-based CE 比较，LC-LSDNN 具有更高的精度和速度，可以在 60% 更高的时钟频率下运行，并且具有更低的计算资源。<details>
<summary>Abstract</summary>
The channel estimation (CE) in wireless receivers is one of the most critical and computationally complex signal processing operations. Recently, various works have shown that the deep learning (DL) based CE outperforms conventional minimum mean square error (MMSE) based CE, and it is hardware-friendly. However, DL-based CE has higher complexity and latency than popularly used least square (LS) based CE. In this work, we propose a novel low complexity high-speed Deep Neural Network-Augmented Least Square (LC-LSDNN) algorithm for IEEE 802.11p wireless physical layer and efficiently implement it on Zynq system on chip (ZSoC). The novelty of the LC-LSDNN is to use different DNNs for real and imaginary values of received complex symbols. This helps reduce the size of DL by 59% and optimize the critical path, allowing it to operate at 60% higher clock frequency. We also explore three different architectures for MMSE-based CE. We show that LC-LSDNN significantly outperforms MMSE and state-of-the-art DL-based CE for a wide range of signal-to-noise ratios (SNR) and different wireless channels. Also, it is computationally efficient, with around 50% lower resources than existing DL-based CE.
</details>
<details>
<summary>摘要</summary>
频率接收器的频率估计（CE）是无线接收器中最 kritical 和 computationally 复杂的信号处理操作。最近几年，不同的工作表明，使用深度学习（DL）基于CE可以超越传统的最小平均方差（MMSE）基于CE，同时具有硬件友好性。然而，DL-based CE 的复杂性和延迟比较高于常用的最小二乘（LS）基于CE。在这个工作中，我们提出了一种新的低复杂度高速的深度神经网络增强了最小二乘（LSDNN）算法，用于 IEEE 802.11p 无线物理层。LSDNN 的新特点在于，使用不同的深度神经网络来处理实数和虚数据的收到复杂符号的两个部分。这有助于减少 DL 的大小，并且优化了关键路径，使其能够在更高的时钟频率上运行。我们还探索了三种不同的 MMSE 基于 CE 架构。我们发现，LC-LSDNN 可以明显超越 MMSE 和现有的 DL-based CE，在各种信号噪声比（SNR）和不同的无线通道上。此外，它还是计算效率高，与现有的 DL-based CE 的资源使用量相比，下降约50%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/eess.SP_2023_11_15/" data-id="clpahu7gl01i33h88craw82og" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/6/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
