
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/5/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_11_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/03/cs.CL_2023_11_03/" class="article-date">
  <time datetime="2023-11-03T11:00:00.000Z" itemprop="datePublished">2023-11-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/03/cs.CL_2023_11_03/">cs.CL - 2023-11-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Grounded-Intuition-of-GPT-Vision’s-Abilities-with-Scientific-Images"><a href="#Grounded-Intuition-of-GPT-Vision’s-Abilities-with-Scientific-Images" class="headerlink" title="Grounded Intuition of GPT-Vision’s Abilities with Scientific Images"></a>Grounded Intuition of GPT-Vision’s Abilities with Scientific Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02069">http://arxiv.org/abs/2311.02069</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahwang16/grounded-intuition-gpt-vision">https://github.com/ahwang16/grounded-intuition-gpt-vision</a></li>
<li>paper_authors: Alyssa Hwang, Andrew Head, Chris Callison-Burch</li>
<li>for: 本研究旨在帮助研究者更好地理解新型模型GPT-Vision的能力和局限性。</li>
<li>methods: 本研究使用了grounded theory和主题分析，从社会科学和人机交互的角度来设置一个严格的质量评估框架，以便对自然语言处理领域的新模型进行评估。</li>
<li>results: 研究发现，GPT-Vision具有特殊的激励特性，它响应于提示、图像中的对话文本和相对空间关系。这种方法和分析可以帮助研究者更好地了解新模型的应用前景，同时探索如何使用GPT-Vision来减轻信息的访问难度。<details>
<summary>Abstract</summary>
GPT-Vision has impressed us on a range of vision-language tasks, but it comes with the familiar new challenge: we have little idea of its capabilities and limitations. In our study, we formalize a process that many have instinctively been trying already to develop "grounded intuition" of this new model. Inspired by the recent movement away from benchmarking in favor of example-driven qualitative evaluation, we draw upon grounded theory and thematic analysis in social science and human-computer interaction to establish a rigorous framework for qualitative evaluation in natural language processing. We use our technique to examine alt text generation for scientific figures, finding that GPT-Vision is particularly sensitive to prompting, counterfactual text in images, and relative spatial relationships. Our method and analysis aim to help researchers ramp up their own grounded intuitions of new models while exposing how GPT-Vision can be applied to make information more accessible.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Vicinal-Risk-Minimization-for-Few-Shot-Cross-lingual-Transfer-in-Abusive-Language-Detection"><a href="#Vicinal-Risk-Minimization-for-Few-Shot-Cross-lingual-Transfer-in-Abusive-Language-Detection" class="headerlink" title="Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection"></a>Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02025">http://arxiv.org/abs/2311.02025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gretel Liz De la Peña Sarracén, Paolo Rosso, Robert Litschko, Goran Glavaš, Simone Paolo Ponzetto</li>
<li>for: 本研究旨在提高跨语言恶意语言识别的性能，使用数据扩充和持续预训练进行领域适应。</li>
<li>methods: 本研究使用了两种现有的数据扩充技术，并提出了一种新的数据扩充方法（MIXAG），该方法根据实例表示的角度进行 interpolate 对照对。</li>
<li>results: 实验结果表明，数据扩充策略可以提高跨语言少量恶意语言识别的性能，特别是在多领域和多语言环境下。<details>
<summary>Abstract</summary>
Cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adaptation can favour the class of abusive texts (reducing false negatives), but at the same time, declines the precision of the abusive language detection model.
</details>
<details>
<summary>摘要</summary>
cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adaptation can favour the class of abusive texts (reducing false negatives), but at the same time, declines the precision of the abusive language detection model.Here's the translation in Traditional Chinese:cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adaptation can favour the class of abusive texts (reducing false negatives), but at the same time, declines the precision of the abusive language detection model.
</details></li>
</ul>
<hr>
<h2 id="ProSG-Using-Prompt-Synthetic-Gradients-to-Alleviate-Prompt-Forgetting-of-RNN-like-Language-Models"><a href="#ProSG-Using-Prompt-Synthetic-Gradients-to-Alleviate-Prompt-Forgetting-of-RNN-like-Language-Models" class="headerlink" title="ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models"></a>ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01981">http://arxiv.org/abs/2311.01981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Luo, Kunming Wu, Cheng Dai, Sixian Ding, Xinhao Chen</li>
<li>for: 解决语言模型在生成过程中忘记提示问题</li>
<li>methods: 使用 sintetic gradient 教导模型在生成过程中记忆提示</li>
<li>results: 实验结果表明，该方法能够解决语言模型在生成过程中忘记提示的问题<details>
<summary>Abstract</summary>
RNN-like language models are getting renewed attention from NLP researchers in recent years and several models have made significant progress, which demonstrates performance comparable to traditional transformers. However, due to the recurrent nature of RNNs, this kind of language model can only store information in a set of fixed-length state vectors. As a consequence, they still suffer from forgetfulness though after a lot of improvements and optimizations, when given complex instructions or prompts. As the prompted generation is the main and most concerned function of LMs, solving the problem of forgetting in the process of generation is no wonder of vital importance. In this paper, focusing on easing the prompt forgetting during generation, we proposed an architecture to teach the model memorizing prompt during generation by synthetic gradient. To force the model to memorize the prompt, we derive the states that encode the prompt, then transform it into model parameter modification using low-rank gradient approximation, which hard-codes the prompt into model parameters temporarily. We construct a dataset for experiments, and the results have demonstrated the effectiveness of our method in solving the problem of forgetfulness in the process of prompted generation. We will release all the code upon acceptance.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注在生成过程中缓解提示忘记的问题，我们提出了一种建议，通过合成梯度来教育模型在生成过程中记忆提示。我们首先提取了编码提示的状态，然后将其转换成模型参数修改使用低级导数预测，这会将提示短时间内写入模型参数中。我们构建了一个数据集，并进行了实验，结果表明我们的方法有效地解决了生成过程中的忘记问题。我们将代码发布于接受后。
</details></li>
</ul>
<hr>
<h2 id="Too-Much-Information-Keeping-Training-Simple-for-BabyLMs"><a href="#Too-Much-Information-Keeping-Training-Simple-for-BabyLMs" class="headerlink" title="Too Much Information: Keeping Training Simple for BabyLMs"></a>Too Much Information: Keeping Training Simple for BabyLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01955">http://arxiv.org/abs/2311.01955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Edman, Lisa Bylinina</li>
<li>for: 这篇论文描述了格罗宁根大学对 BabyLM 挑战的工作。</li>
<li>methods: 我们采用了如宝宝一样，将语言模型引入 simpler concept 先后理解更复杂的概念的想法。我们通过不同的角度（context size、词汇量和总语言复杂度）来检查这种策略的效果。</li>
<li>results: 我们发现只有context size truly beneficial to training a language model，但这simple change to context size 使我们在(Super)GLUE任务上的平均提高2点，在MSGS任务上的平均提高1点，在BLiMP任务上的平均提高12%。我们的context-limited模型比基线模型，在10 times更多的数据上进行训练。<details>
<summary>Abstract</summary>
This paper details the work of the University of Groningen for the BabyLM Challenge. We follow the idea that, like babies, language models should be introduced to simpler concepts first and build off of that knowledge to understand more complex concepts. We examine this strategy of simple-then-complex through a variety of lenses, namely context size, vocabulary, and overall linguistic complexity of the data. We find that only one, context size, is truly beneficial to training a language model. However this simple change to context size gives us improvements of 2 points on average on (Super)GLUE tasks, 1 point on MSGS tasks, and 12\% on average on BLiMP tasks. Our context-limited model outperforms the baseline that was trained on 10$\times$ the amount of data.
</details>
<details>
<summary>摘要</summary>
这份论文介绍了格隆根大学对宝宝LM挑战的工作。我们采用了婴儿式学习策略，即首先教育语言模型简单概念，然后逐步增加知识来理解更复杂的概念。我们通过不同的角度来检查这种简单然后复杂的策略，即上下文大小、词汇量和总语言复杂度。我们发现只有上下文大小真正有利于语言模型训练，但这种简单的改变使我们在（Super）GLUE任务上平均提高2点，MSGS任务上平均提高1点，BLiMP任务上平均提高12%。我们的上下文限定模型超过基eline模型，即使训练数据量为10倍。
</details></li>
</ul>
<hr>
<h2 id="Hint-enhanced-In-Context-Learning-wakes-Large-Language-Models-up-for-knowledge-intensive-tasks"><a href="#Hint-enhanced-In-Context-Learning-wakes-Large-Language-Models-up-for-knowledge-intensive-tasks" class="headerlink" title="Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks"></a>Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01949">http://arxiv.org/abs/2311.01949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Wang, Qingyan Guo, Xinzhe Ni, Chufan Shi, Lemao Liu, Haiyun Jiang, Yujiu Yang</li>
<li>for: 提高大语言模型（LLM）在知识密集任务中表现，特别是开放领域问答任务。</li>
<li>methods: 提出Hint-enhanced In-Context Learning（HICL）新 парадиг，利用LLM的解释能力从示例中提取问题相关的知识，然后将知识用作更Explicit的提示。同时，跟踪示例的来源以确定特定的示例，并引入Hint-related Example Retriever（HER）来选择有用的示例。</li>
<li>results: 对3个开放领域问答 benchmark进行评估，与标准设置相比，HICL加HER得到了平均性能提升2.89 EM score和2.52 F1 score在gpt-3.5-turbo上，7.62 EM score和7.27 F1 score在LLaMA-2-Chat-7B上。<details>
<summary>Abstract</summary>
In-context learning (ICL) ability has emerged with the increasing scale of large language models (LLMs), enabling them to learn input-label mappings from demonstrations and perform well on downstream tasks. However, under the standard ICL setting, LLMs may sometimes neglect query-related information in demonstrations, leading to incorrect predictions. To address this limitation, we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to explore the power of ICL in open-domain question answering, an important form in knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract query-related knowledge from demonstrations, then concatenates the knowledge to prompt LLMs in a more explicit way. Furthermore, we track the source of this knowledge to identify specific examples, and introduce a Hint-related Example Retriever (HER) to select informative examples for enhanced demonstrations. We evaluate HICL with HER on 3 open-domain QA benchmarks, and observe average performance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EM score and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.
</details>
<details>
<summary>摘要</summary>
受大语言模型（LLM）的规模增长的影响，宽 Context Learning（ICL）能力已经出现，使得 LLMS 可以从示例中学习输入标签映射，并在下游任务中表现良好。然而，在标准 ICLE 设置下， LLMS 可能会忽略示例中相关的查询信息，导致错误预测。为了解决这个限制，我们提出了一种新的思路calledHint-enhanced In-Context Learning（HICL），以探索 ICLE 在开放领域问答中的力量。HICL 利用 LLMS 的理解能力提取示例中相关的查询知识，然后将这些知识 concatenates 到提示 LLMS 以更加显式的方式。此外，我们跟踪这些知识的来源，并引入一个Hint-related Example Retriever（HER）来选择有用的示例，以提高示例的质量。我们在3个开放领域问答标准 benchmark上评估 HICL 和 HER，并观察了gpt-3.5-turbo 和 LLaMA-2-Chat-7B 上的平均性能提升2.89 EM 分数和2.52 F1 分数，升级7.62 EM 分数和7.27 F1 分数。
</details></li>
</ul>
<hr>
<h2 id="Constructing-Temporal-Dynamic-Knowledge-Graphs-from-Interactive-Text-based-Games"><a href="#Constructing-Temporal-Dynamic-Knowledge-Graphs-from-Interactive-Text-based-Games" class="headerlink" title="Constructing Temporal Dynamic Knowledge Graphs from Interactive Text-based Games"></a>Constructing Temporal Dynamic Knowledge Graphs from Interactive Text-based Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01928">http://arxiv.org/abs/2311.01928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yukw777/temporal-discrete-graph-updater">https://github.com/yukw777/temporal-discrete-graph-updater</a></li>
<li>paper_authors: Keunwoo Peter Yu</li>
<li>for: 这个论文的目的是提出一种新的图ppoydunker模型，以提高对文本游戏中的动态知识图的表示和学习。</li>
<li>methods: 该模型使用一种名为时间点基于图神经网络的方法，将动态知识图表示为一系列时间戳的图事件，以提高知识图的准确性和可解释性。</li>
<li>results: 通过对TextWorld数据集进行实验，研究发现TDGU模型比基elineDGU模型表现更好，并且通过缺省研究和对更复杂的环境的演示，证明TDGU模型具有更好的泛化能力。<details>
<summary>Abstract</summary>
In natural language processing, interactive text-based games serve as a test bed for interactive AI systems. Prior work has proposed to play text-based games by acting based on discrete knowledge graphs constructed by the Discrete Graph Updater (DGU) to represent the game state from the natural language description. While DGU has shown promising results with high interpretability, it suffers from lower knowledge graph accuracy due to its lack of temporality and limited generalizability to complex environments with objects with the same label. In order to address DGU's weaknesses while preserving its high interpretability, we propose the Temporal Discrete Graph Updater (TDGU), a novel neural network model that represents dynamic knowledge graphs as a sequence of timestamped graph events and models them using a temporal point based graph neural network. Through experiments on the dataset collected from a text-based game TextWorld, we show that TDGU outperforms the baseline DGU. We further show the importance of temporal information for TDGU's performance through an ablation study and demonstrate that TDGU has the ability to generalize to more complex environments with objects with the same label. All the relevant code can be found at \url{https://github.com/yukw777/temporal-discrete-graph-updater}.
</details>
<details>
<summary>摘要</summary>
在自然语言处理领域，文本基于游戏作为互动AI系统的测试床。先前的工作已经提议通过基于自然语言描述生成的Discrete Graph Updater（DGU）来控制文本基于游戏。 although DGU has shown promising results with high interpretability, it suffers from lower knowledge graph accuracy due to its lack of temporality and limited generalizability to complex environments with objects with the same label. 为了解决DGU的缺陷而保持高度可读性，我们提出了Temporal Discrete Graph Updater（TDGU），一种新的神经网络模型，它表示动态知识图为一个时间戳的图事件序列，并使用时间点基于图神经网络来模型。 通过TextWorld数据集上的实验，我们表明TDGU超过了基准DGU。我们还进行了剖析研究，证明了TDGU的时间信息的重要性，并示出TDGU可以在更复杂的环境中 generale化。所有相关的代码可以在 GitHub上找到，链接为 \url{https://github.com/yukw777/temporal-discrete-graph-updater}.
</details></li>
</ul>
<hr>
<h2 id="BoschAI-PLABA-2023-Leveraging-Edit-Operations-in-End-to-End-Neural-Sentence-Simplification"><a href="#BoschAI-PLABA-2023-Leveraging-Edit-Operations-in-End-to-End-Neural-Sentence-Simplification" class="headerlink" title="BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification"></a>BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01907">http://arxiv.org/abs/2311.01907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentin Knappich, Simon Razniewski, Annemarie Friedrich</li>
<li>for: 这个论文的目的是提出一种基于LLAMA2的自动简化系统，以便非专业人员更好地理解复杂的科学文献。</li>
<li>methods: 该系统使用语言模型将复杂语言翻译成简单语言。论文提出了使用句子级和字节级损失权重来减少模型的训练信号和保守性。</li>
<li>results: 经验证明，该方法可以生成更加接近人工标注者创造的简化文本 (+1.8% &#x2F; +3.5% SARI),使用更加简单的语言 (-1 &#x2F; -1.1 FKGL)和更多的修改（1.6x &#x2F; 1.8x编辑距离），相比同模型通过标准十字 entropy进行 fine-tuning。此外，论文还表明了控制编辑距离和简单性水平（FKGL）的Hyperparameter $\lambda$。<details>
<summary>Abstract</summary>
Automatic simplification can help laypeople to comprehend complex scientific text. Language models are frequently applied to this task by translating from complex to simple language. In this paper, we describe our system based on Llama 2, which ranked first in the PLABA shared task addressing the simplification of biomedical text. We find that the large portion of shared tokens between input and output leads to weak training signals and conservatively editing models. To mitigate these issues, we propose sentence-level and token-level loss weights. They give higher weight to modified tokens, indicated by edit distance and edit operations, respectively. We conduct an empirical evaluation on the PLABA dataset and find that both approaches lead to simplifications closer to those created by human annotators (+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x / 1.8x edit distance) compared to the same model fine-tuned with standard cross entropy. We furthermore show that the hyperparameter $\lambda$ in token-level loss weights can be used to control the edit distance and the simplicity level (FKGL).
</details>
<details>
<summary>摘要</summary>
自动简化可以帮助非专家理解复杂科学文本。语言模型经常用于这种任务，将复杂语言翻译成简单语言。在这篇论文中，我们描述了基于LLAMA 2的系统，该系统在PLABA共享任务中排名第一，用于简化生物医学文本。我们发现输入和输出共享的大量共同token会导致弱的训练信号和保守的编辑模型。为了解决这些问题，我们提议使用句子级和token级损失权重。它们将修改后的token得到更高的权重，根据编辑距离和编辑操作来进行评估。我们对PLABA数据集进行了实验评估，发现两种方法都能够生成更加简洁的简化文本（+1.8% / +3.5% SARI）， simpler language (-1 / -1.1 FKGL）和更多的编辑（1.6x / 1.8x编辑距离），比同样的模型通过标准十字Entropy训练更好。我们还发现了$\lambda$参数在token级损失权重中可以控制编辑距离和简洁水平（FKGL）。
</details></li>
</ul>
<hr>
<h2 id="Indicative-Summarization-of-Long-Discussions"><a href="#Indicative-Summarization-of-Long-Discussions" class="headerlink" title="Indicative Summarization of Long Discussions"></a>Indicative Summarization of Long Discussions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01882">http://arxiv.org/abs/2311.01882</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webis-de/emnlp-23">https://github.com/webis-de/emnlp-23</a></li>
<li>paper_authors: Shahbaz Syed, Dominik Schwabe, Khalid Al-Khatib, Martin Potthast</li>
<li>for: 提供一种novel的无监督方法，使用大型自然语言模型（LLM）生成长讨论的指示性摘要，以便方便用户快速浏览和理解长讨论。</li>
<li>methods: 方法首先对讨论中的argument sentence进行聚类，然后生成聚类标签作为摘要，最后将生成的摘要分类为口语框架。</li>
<li>results: 经过优化的提问工程approach，我们测试了19个LLM的生成聚类标签和口语框架分类能力，并进行了用户研究，结果表明，我们的提出的指示性摘要可以帮助用户快速浏览和理解长讨论。<details>
<summary>Abstract</summary>
Online forums encourage the exchange and discussion of different stances on many topics. Not only do they provide an opportunity to present one's own arguments, but may also gather a broad cross-section of others' arguments. However, the resulting long discussions are difficult to overview. This paper presents a novel unsupervised approach using large language models (LLMs) to generating indicative summaries for long discussions that basically serve as tables of contents. Our approach first clusters argument sentences, generates cluster labels as abstractive summaries, and classifies the generated cluster labels into argumentation frames resulting in a two-level summary. Based on an extensively optimized prompt engineering approach, we evaluate 19~LLMs for generative cluster labeling and frame classification. To evaluate the usefulness of our indicative summaries, we conduct a purpose-driven user study via a new visual interface called Discussion Explorer: It shows that our proposed indicative summaries serve as a convenient navigation tool to explore long discussions.
</details>
<details>
<summary>摘要</summary>
在线讨论区域鼓励不同观点的交流和讨论。不仅可以展示自己的Arguments，还可以收集各种不同的Arguments。然而，长时间的讨论可能很难概括。这篇论文提出了一种新的无监督方法，使用大型自然语言模型（LLMs）生成长讨论的指示性摘要。我们的方法首先对Argument sentence进行聚合，生成聚合Label作为摘要，然后将生成的聚合Label进行分类，生成两级摘要。通过大量优化的提示工程 Approach，我们评估了19种LLMs的生成聚合标签和框架分类。为了评估我们的指示性摘要的有用性，我们进行了一项目的用途驱动的用户研究，通过一种新的视觉界面 called Discussion Explorer：它表明了我们的提posed indicative summaries可以作为浏览长讨论的便捷导航工具。
</details></li>
</ul>
<hr>
<h2 id="Sentiment-Analysis-through-LLM-Negotiations"><a href="#Sentiment-Analysis-through-LLM-Negotiations" class="headerlink" title="Sentiment Analysis through LLM Negotiations"></a>Sentiment Analysis through LLM Negotiations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01876">http://arxiv.org/abs/2311.01876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaofei Sun, Xiaoya Li, Shengyu Zhang, Shuhe Wang, Fei Wu, Jiwei Li, Tianwei Zhang, Guoyin Wang</li>
<li>for: This paper aims to improve the accuracy of sentiment analysis by introducing a multi-LLM negotiation framework that leverages the complementary abilities of multiple language models to generate more accurate and well-reasoned decisions.</li>
<li>methods: The proposed framework consists of a reasoning-infused generator and an explanation-deriving discriminator, which iterate until a consensus is reached. The generator provides decisions along with rationale, while the discriminator evaluates the credibility of the generator’s decisions.</li>
<li>results: The proposed approach consistently outperforms the in-context learning (ICL) baseline across all benchmarks, and even achieves superior performances compared to supervised baselines on the Twitter and movie review datasets.<details>
<summary>Abstract</summary>
A standard paradigm for sentiment analysis is to rely on a singular LLM and makes the decision in a single round under the framework of in-context learning. This framework suffers the key disadvantage that the single-turn output generated by a single LLM might not deliver the perfect decision, just as humans sometimes need multiple attempts to get things right. This is especially true for the task of sentiment analysis where deep reasoning is required to address the complex linguistic phenomenon (e.g., clause composition, irony, etc) in the input.   To address this issue, this paper introduces a multi-LLM negotiation framework for sentiment analysis. The framework consists of a reasoning-infused generator to provide decision along with rationale, a explanation-deriving discriminator to evaluate the credibility of the generator. The generator and the discriminator iterate until a consensus is reached. The proposed framework naturally addressed the aforementioned challenge, as we are able to take the complementary abilities of two LLMs, have them use rationale to persuade each other for correction.   Experiments on a wide range of sentiment analysis benchmarks (SST-2, Movie Review, Twitter, yelp, amazon, IMDB) demonstrate the effectiveness of proposed approach: it consistently yields better performances than the ICL baseline across all benchmarks, and even superior performances to supervised baselines on the Twitter and movie review datasets.
</details>
<details>
<summary>摘要</summary>
一般来说，用一个单一的深度学习模型（LLM）进行情感分析是一种常见的方法。这种方法的缺点是，单个LLM的输出可能不会提供完美的决策，就像人类在做出决策时有时需要多次尝试。这是特别真的 для情感分析任务，因为这个任务需要深入理解复杂的语言现象（例如句子组成、讽刺等）。为解决这个问题，这篇论文提出了一种多个LLM谈判框架 для情感分析。该框架包括一个理由感染生成器，用于提供决策以及理由，以及一个解释评估器，用于评估生成器的合理性。生成器和解释评估器会进行谈判，直到达成一致。提议的框架自然地解决了以上挑战，因为我们可以利用两个LLM的补充能力，让它们使用理由来证明对方需要更正。实验结果表明，提议的方法在各种情感分析标准benchmark（SST-2、电影评论、Twitter、Yelp、Amazon、IMDB）上表现出色， consistently 超过ICL基线，甚至在Twitter和电影评论数据集上超越了经过监督的基线。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Black-Box-Adversarial-Attacks-on-Neural-Text-Detectors"><a href="#Efficient-Black-Box-Adversarial-Attacks-on-Neural-Text-Detectors" class="headerlink" title="Efficient Black-Box Adversarial Attacks on Neural Text Detectors"></a>Efficient Black-Box Adversarial Attacks on Neural Text Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01873">http://arxiv.org/abs/2311.01873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vitalii Fishchuk, Daniel Braun</li>
<li>for:  investigate the effectiveness of three simple and resource-efficient strategies to alter texts generated by GPT-3.5 to misclassify neural text detectors.</li>
<li>methods:  parameter tweaking, prompt engineering, and character-level mutations.</li>
<li>results:  especially parameter tweaking and character-level mutations are effective strategies.Here’s the summary in Traditional Chinese as well:</li>
<li>for: 研究使用三种简单且资源有效的策略，让GPT-3.5生成的文本被神经文本探测器误将为人工生成的文本。</li>
<li>methods: 参数调整、提示工程和字元水平的变化。</li>
<li>results: 特别是参数调整和字元水平的变化是有效的策略。<details>
<summary>Abstract</summary>
Neural text detectors are models trained to detect whether a given text was generated by a language model or written by a human. In this paper, we investigate three simple and resource-efficient strategies (parameter tweaking, prompt engineering, and character-level mutations) to alter texts generated by GPT-3.5 that are unsuspicious or unnoticeable for humans but cause misclassification by neural text detectors. The results show that especially parameter tweaking and character-level mutations are effective strategies.
</details>
<details>
<summary>摘要</summary>
neural text detectors 是模型，用于detect whether a given text was generated by a language model or written by a human。在这篇论文中，我们investigate three simple and resource-efficient strategies（parameter tweaking，prompt engineering，and character-level mutations）to alter texts generated by GPT-3.5 that are unsuspicious or unnoticeable for humans but cause misclassification by neural text detectors。result showsthat especially parameter tweaking and character-level mutations are effective strategies。
</details></li>
</ul>
<hr>
<h2 id="R-3-NL2GQL-A-Hybrid-Models-Approach-for-for-Accuracy-Enhancing-and-Hallucinations-Mitigation"><a href="#R-3-NL2GQL-A-Hybrid-Models-Approach-for-for-Accuracy-Enhancing-and-Hallucinations-Mitigation" class="headerlink" title="$R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation"></a>$R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01862">http://arxiv.org/abs/2311.01862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhiqix/nl2gql">https://github.com/zhiqix/nl2gql</a></li>
<li>paper_authors: Yuhang Zhou, He Yu, Siyu Tian, Dan Chen, Liuzhi Zhou, Xinlin Yu, Chuanjun Ji, Sen Liu, Guangnan Ye, Hongfeng Chai</li>
<li>for: 这篇论文主要应用于将自然语言转换为graph查询语言（NL2GQL）任务中，并解决了Foundation Models在NL2GQL任务中的挑战。</li>
<li>methods: 本论文使用了Foundation Models，并将其分为大小不同的模型，以进行不同的调整和组合。</li>
<li>results: 实验结果显示，大型Foundation Models在NL2GQL任务中展现出了优秀的横推数据能力，而小型Foundation Models则在细化和调整后，对于意思理解和 grammatical accuracy 有所进步。<details>
<summary>Abstract</summary>
While current NL2SQL tasks constructed using Foundation Models have achieved commendable results, their direct application to Natural Language to Graph Query Language (NL2GQL) tasks poses challenges due to the significant differences between GQL and SQL expressions, as well as the numerous types of GQL. Our extensive experiments reveal that in NL2GQL tasks, larger Foundation Models demonstrate superior cross-schema generalization abilities, while smaller Foundation Models struggle to improve their GQL generation capabilities through fine-tuning. However, after fine-tuning, smaller models exhibit better intent comprehension and higher grammatical accuracy. Diverging from rule-based and slot-filling techniques, we introduce R3-NL2GQL, which employs both smaller and larger Foundation Models as reranker, rewriter and refiner. The approach harnesses the comprehension ability of smaller models for information reranker and rewriter, and the exceptional generalization and generation capabilities of larger models to transform input natural language queries and code structure schema into any form of GQLs. Recognizing the lack of established datasets in this nascent domain, we have created a bilingual dataset derived from graph database documentation and some open-source Knowledge Graphs (KGs). We tested our approach on this dataset and the experimental results showed that delivers promising performance and robustness.Our code and dataset is available at https://github.com/zhiqix/NL2GQL
</details>
<details>
<summary>摘要</summary>
当前的NL2SQL任务使用基础模型构建得到了可嘉的结果，但直接应用于自然语言到图查询语言（NL2GQL）任务却存在挑战，主要是因为GQL和SQL表达之间存在显著差异，以及GQL的多种类型。我们的广泛实验表明，在NL2GQL任务中，更大的基础模型在跨 schema 泛化能力方面表现出色，而更小的基础模型通过细化不能提高其生成GQL能力。然而，经细化后，更小的模型具有更高的意图理解和语法正确率。不同于规则基于和槽填充技术，我们提出了R3-NL2GQL，它使用更小和更大的基础模型来重新排序、重写和精度。这种方法利用更小的模型对信息重新排序和重写的能力，以及更大的模型对输入自然语言查询和代码结构 schema 的转换能力。认识到这个领域的数据集还没有成熔，我们从图数据库文档和一些开源知识图（KG） derivated 一个双语数据集。我们对这个数据集进行了测试，实验结果表明了我们的方法具有扎实的表现和稳定性。代码和数据集可以在https://github.com/zhiqix/NL2GQL 上获取。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-to-the-Rescue-Reducing-the-Complexity-in-Scientific-Workflow-Development-Using-ChatGPT"><a href="#Large-Language-Models-to-the-Rescue-Reducing-the-Complexity-in-Scientific-Workflow-Development-Using-ChatGPT" class="headerlink" title="Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT"></a>Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01825">http://arxiv.org/abs/2311.01825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mario Sänger, Ninon De Mecquenem, Katarzyna Ewa Lewińska, Vasilis Bountris, Fabian Lehmann, Ulf Leser, Thomas Kosch</li>
<li>for: 这篇研究旨在测试大自然语言模型（LLM）在科学工作流程中的效率，以支持用户在实现工作流程时所遇到的挑战。</li>
<li>methods: 研究使用了ChatGPT作为LLM，并进行了三个使用者研究，以评估ChatGPT在理解、适应和扩展工作流程方面的效能。</li>
<li>results: 研究结果显示LLM对工作流程的解释有高效性，但在交换组件或目的性工作流程扩展方面表现较差。研究也描述了LLM在这些困难情况下的限制，并建议未来研究的方向。<details>
<summary>Abstract</summary>
Scientific workflow systems are increasingly popular for expressing and executing complex data analysis pipelines over large datasets, as they offer reproducibility, dependability, and scalability of analyses by automatic parallelization on large compute clusters. However, implementing workflows is difficult due to the involvement of many black-box tools and the deep infrastructure stack necessary for their execution. Simultaneously, user-supporting tools are rare, and the number of available examples is much lower than in classical programming languages. To address these challenges, we investigate the efficiency of Large Language Models (LLMs), specifically ChatGPT, to support users when dealing with scientific workflows. We performed three user studies in two scientific domains to evaluate ChatGPT for comprehending, adapting, and extending workflows. Our results indicate that LLMs efficiently interpret workflows but achieve lower performance for exchanging components or purposeful workflow extensions. We characterize their limitations in these challenging scenarios and suggest future research directions.
</details>
<details>
<summary>摘要</summary>
We conducted three user studies in two scientific domains to evaluate ChatGPT's ability to comprehend, adapt, and extend workflows. Our results show that LLMs can efficiently interpret workflows, but their performance is lower when it comes to exchanging components or creating purposeful workflow extensions. We have identified the limitations of these challenging scenarios and suggest future research directions.Translated into Simplified Chinese:科学工作流系统在表达和执行复杂数据分析管道上占据着越来越多的市场份额，因为它们提供了可重现性、可靠性和可扩展性，并可自动平行化在大型计算集群上。然而，实现工作流程的困难在于许多黑盒工具的参与以及执行所需的深层基础设施。同时，用户支持工具罕见，可用的示例数量也远低于经典编程语言。为了解决这些挑战，我们研究了大语言模型（LLM），具体来说是ChatGPT，在科学工作流程中支持用户。我们在两个科学领域中进行了三个用户研究，以评估ChatGPT在理解、适应和扩展工作流程方面的能力。我们的结果表明，LLM可以高效地理解工作流程，但在交换组件或创造有目的工作流程扩展方面表现较差。我们对这些挑战的限制进行了特点分析，并建议未来的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Minimalist-Grammar-Construction-without-Overgeneration"><a href="#Minimalist-Grammar-Construction-without-Overgeneration" class="headerlink" title="Minimalist Grammar: Construction without Overgeneration"></a>Minimalist Grammar: Construction without Overgeneration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01820">http://arxiv.org/abs/2311.01820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isidor Konrad Maier, Johannes Kuhn, Jesse Beisegel, Markus Huber-Liebl, Matthias Wolff</li>
<li>for: 这篇论文是如何编写 minimalist grammar (MG) 的指南。</li>
<li>methods: 使用 variant of context free grammars (CFG) 作为输入格式，并使用 licensors&#x2F;-ees 特殊的方式处理例外情况。</li>
<li>results: 构建的 MG 可以避免过度生成，并且使用 adapters 解决 exceptions 处理中的问题。<details>
<summary>Abstract</summary>
In this paper we give instructions on how to write a minimalist grammar (MG). In order to present the instructions as an algorithm, we use a variant of context free grammars (CFG) as an input format. We can exclude overgeneration, if the CFG has no recursion, i.e. no non-terminal can (indirectly) derive to a right-hand side containing itself. The constructed MGs utilize licensors/-ees as a special way of exception handling. A CFG format for a derivation $A\_eats\_B\mapsto^* peter\_eats\_apples$, where $A$ and $B$ generate noun phrases, normally leads to overgeneration, e.\,g., $i\_eats\_apples$. In order to avoid overgeneration, a CFG would need many non-terminal symbols and rules, that mainly produce the same word, just to handle exceptions. In our MGs however, we can summarize CFG rules that produce the same word in one item and handle exceptions by a proper distribution of licensees/-ors. The difficulty with this technique is that in most generations the majority of licensees/-ors is not needed, but still has to be triggered somehow. We solve this problem with $\epsilon$-items called \emph{adapters}.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了写 minimalist grammar（MG）的指导方针。为了表示这些指导方针为算法，我们使用 variant of context free grammars（CFG）作为输入格式。如果 CFG 没有回归，则可以排除过度生成。 constructed MGs 使用licensee/-or作为特殊的例外处理方式。CFG 格式 для一个 derivation $A\_eats\_B\mapsto^* peter\_eats\_apples$，where $A$ 和 $B$ 生成名词短语，通常会导致过度生成，例如 $i\_eats\_apples$。为了避免过度生成，一个 CFG 需要很多非树状符号和规则，主要生成同一个词的不同形式，只是为了处理例外。在我们的 MGs 中，我们可以汇总 CFG 规则生成同一个词的项目，并通过正确的分配licensee/-or来处理例外。这种技术的困难在于，在大多数生成中，主要的licensee/-or并不需要，但仍需要某种触发方式。我们解决这个问题使用 $\epsilon$-item called \emph{adapters}。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Framing-Bias-with-Polarity-Minimization-Loss"><a href="#Mitigating-Framing-Bias-with-Polarity-Minimization-Loss" class="headerlink" title="Mitigating Framing Bias with Polarity Minimization Loss"></a>Mitigating Framing Bias with Polarity Minimization Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01817">http://arxiv.org/abs/2311.01817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yejin Bang, Nayeon Lee, Pascale Fung</li>
<li>for: 防止新闻报道中的偏见倾向</li>
<li>methods: 提出一种新的损失函数，用于降低多个新闻报道中的偏见差异</li>
<li>results: 实验结果表明，通过在模型中添加该损失函数可以减少偏见倾向，其效果最大化在降低信息偏见倾向（即报道中选择的信息偏见）。<details>
<summary>Abstract</summary>
Framing bias plays a significant role in exacerbating political polarization by distorting the perception of actual events. Media outlets with divergent political stances often use polarized language in their reporting of the same event. We propose a new loss function that encourages the model to minimize the polarity difference between the polarized input articles to reduce framing bias. Specifically, our loss is designed to jointly optimize the model to map polarity ends bidirectionally. Our experimental results demonstrate that incorporating the proposed polarity minimization loss leads to a substantial reduction in framing bias when compared to a BART-based multi-document summarization model. Notably, we find that the effectiveness of this approach is most pronounced when the model is trained to minimize the polarity loss associated with informational framing bias (i.e., skewed selection of information to report).
</details>
<details>
<summary>摘要</summary>
帧偏调 plays a significant role in exacerbating political polarization by distorting the perception of actual events. Media outlets with divergent political stances often use polarized language in their reporting of the same event. We propose a new loss function that encourages the model to minimize the polarity difference between the polarized input articles to reduce framing bias. Specifically, our loss is designed to jointly optimize the model to map polarity ends bidirectionally. Our experimental results demonstrate that incorporating the proposed polarity minimization loss leads to a substantial reduction in framing bias when compared to a BART-based multi-document summarization model. Notably, we find that the effectiveness of this approach is most pronounced when the model is trained to minimize the polarity loss associated with informational framing bias (i.e., skewed selection of information to report).Here's the translation in Traditional Chinese:帧偏调对政治化分化具有重要作用，导致现实事件的观察被扭曲。媒体对同一事件的报导 often 使用偏 polarized 的语言，这会导致政治分化。我们提出了一个新的损失函数，这个损失函数鼓励模型将 polarity 的差异最小化，以减少帧偏调。具体来说，我们的损失函数设计来对 polarity 的端点进行bidirectional 的对映。我们的实验结果显示，将 proposed polarity 损失函数添加到模型中可以对帧偏调进行重大减少，相比之下，使用 BART 基于多篇文章摘要模型。当然，我们发现这种方法在对 informational framing bias 进行对映时表现最佳。
</details></li>
</ul>
<hr>
<h2 id="UP4LS-User-Profile-Constructed-by-Multiple-Attributes-for-Enhancing-Linguistic-Steganalysis"><a href="#UP4LS-User-Profile-Constructed-by-Multiple-Attributes-for-Enhancing-Linguistic-Steganalysis" class="headerlink" title="UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis"></a>UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01775">http://arxiv.org/abs/2311.01775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihao Wang, Ruiqi Song, Ru Zhang, Jianyi Liu</li>
<li>for: 提高语言隐藏分析（LS）任务的性能，特别是在社交媒体上。</li>
<li>methods: 利用用户 profiling 技术，挖掘用户的写作习惯、心理状态和关注点，然后与现有方法结合使用语言模型来提取特征。</li>
<li>results: 对现有方法进行改进，实现减少隐藏样本数量下的性能提升，具体提升约25%。<details>
<summary>Abstract</summary>
Linguistic steganalysis (LS) tasks aim to effectively detect stegos generated by linguistic steganography. Existing LS methods overlook the distinctive user characteristics, leading to weak performance in social networks. The limited occurrence of stegos further complicates detection. In this paper, we propose the UP4LS, a novel framework with the User Profile for enhancing LS performance. Specifically, by delving into post content, we explore user attributes like writing habits, psychological states, and focal areas, thereby building the user profile for LS. For each attribute, we design the identified feature extraction module. The extracted features are mapped to high-dimensional user features via deep-learning networks from existing methods. Then the language model is employed to extract content features. The user and content features are integrated to optimize feature representation. During the training phase, we prioritize the distribution of stegos. Experiments demonstrate that UP4LS can significantly enhance the performance of existing methods, and an overall accuracy improvement of nearly 25%. In particular, the improvement is especially pronounced with fewer stego samples. Additionally, UP4LS also sets the stage for studies on related tasks, encouraging extensive applications on LS tasks.
</details>
<details>
<summary>摘要</summary>
文本隐藏分析（LS）任务目的是有效检测基于语言隐藏技术生成的隐藏文本（stegos）。现有的LS方法忽略了用户特征，导致检测效果在社交网络中弱化。隐藏文本的有限发生频率更进一步复杂了检测。本文提出了UP4LS，一种新的框架，通过探索文章内容，捕捉用户特征，如写作习惯、心理状态和焦点领域，建立用户profile，并为每个特征设计特定的特征提取模块。这些特征被映射到现有方法中的深度学习网络中，然后使用语言模型提取内容特征。用户和内容特征被结合，以优化特征表示。在训练阶段，我们优先考虑隐藏文本的分布。实验表明，UP4LS可以显著提高现有方法的性能，具体提高约25%。尤其是在 fewer stego samples 的情况下，提高更加明显。此外，UP4LS还为相关任务提供了开门篇，激发了广泛的应用研究。
</details></li>
</ul>
<hr>
<h2 id="PPTC-Benchmark-Evaluating-Large-Language-Models-for-PowerPoint-Task-Completion"><a href="#PPTC-Benchmark-Evaluating-Large-Language-Models-for-PowerPoint-Task-Completion" class="headerlink" title="PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion"></a>PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01767">http://arxiv.org/abs/2311.01767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gydpku/pptc">https://github.com/gydpku/pptc</a></li>
<li>paper_authors: Yiduo Guo, Zekai Zhang, Yaobo Liang, Dongyan Zhao, Duan Nan</li>
<li>for: 这项研究旨在评估大自然语言模型（LLM）在完成多个交互、多modal操作的复杂多modal环境中的能力。</li>
<li>methods: 该研究使用了PowerPoint Task Completion（PPTC） benchmarch来评估LLM在创建和编辑PPT文件基于用户 instrucion的能力。</li>
<li>results: 研究发现GPT-4在单转对话测试中具有75.1%的准确率，但在完成整个会话中表现不佳，只有6%的会话准确率。研究发现三种主要错误原因：交互累积、长时间处理PPT模板和多modal识别。这些问题对未来LLM和代理系统 pose 极大挑战。<details>
<summary>Abstract</summary>
Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1\% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6\% session accuracy. We find three main error causes in our benchmark: error accumulation in the multi-turn session, long PPT template processing, and multi-modality perception. These pose great challenges for future LLM and agent systems. We release the data, code, and evaluation system of PPTC at \url{https://github.com/gydpku/PPTC}.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）评估中心在测试它们零shot/几shot能力来完成基本的自然语言任务以及将指令转化为工具API。然而，对于使用复杂工具完成多Turn多模态任务在复杂多模态环境中评估LLM的能力还没有被研究。为了解决这一漏洞，我们介绍了PowerPoint任务完成（PPTC）标准测试套件，用于评估LLM在基于用户指令创建和编辑PPT文件方面的能力。该套件包含279个多Turn会话，涵盖多个主题和百度 instrucciones 涉及多模态操作。我们还提出了PPTXMatch评估系统，它根据预测文件而不是标签API序列来评估LLM是否完成了指令。这种支持多种LLM生成的API序列。我们测试了三个关闭LLM和六个开源LLM。结果显示，GPT-4在单Turn对话测试中的准确率为75.1%，但在完成整个会话时表现不佳，只有6%的会话准确率。我们发现了三种主要的错误原因：在多Turn会话中的错误积累、长PPT模板处理和多模态感知。这些问题对未来LLM和代理系统带来了很大挑战。我们将数据、代码和评估系统发布到GitHub上，请参考 \url{https://github.com/gydpku/PPTC}.
</details></li>
</ul>
<hr>
<h2 id="Support-or-Refute-Analyzing-the-Stance-of-Evidence-to-Detect-Out-of-Context-Mis-and-Disinformation"><a href="#Support-or-Refute-Analyzing-the-Stance-of-Evidence-to-Detect-Out-of-Context-Mis-and-Disinformation" class="headerlink" title="Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation"></a>Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01766">http://arxiv.org/abs/2311.01766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yuan, Jie Guo, Weidong Qiu, Zheng Huang, Shujun Li</li>
<li>for: 防止在线谣言和false information的扩散</li>
<li>methods: 提出了一种基于多模态证据的偏见抽取网络（SEN），可以同时抽取不同证据的偏见，以提高识别结果的准确性</li>
<li>results: 对大规模公共数据集进行了广泛的实验，发现提出的方法比前期基elines的表现升高3.2%的精度。<details>
<summary>Abstract</summary>
Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale dataset demonstrated that our proposed method outperformed the state-of-the-art baselines, with the best model achieving a performance gain of 3.2% in accuracy.
</details>
<details>
<summary>摘要</summary>
互联网上的谬误和不准确信息已成为现代社会的重要问题，是多种不同类型的在线危害的主要来源。一种常见的谬误信息形式是Context Out-of-Context（OOC）信息，即不同的信息元素被谬误地联系起来，例如真实的图像与谬误的文字描述或歪曲的文本描述。 although some past studies have tried to defend against OOC misinformation through external evidence, they tend to ignore the role of different pieces of evidence with different stances. 驱动了寻求解决这个问题的直觉，我们提出了一种姿态提取网络（SEN），可以在一个统一的框架中提取不同类型的多Modal证据的姿态。此外，我们还引入了基于命名实体之间的共occurrence关系的支持驳回分数，来进一步提高文本SEN的准确性。经过了一系列的大规模公共数据集的实验，我们的提议方法在准确性方面超过了现有的基线，最佳模型在准确性方面提高了3.2%。
</details></li>
</ul>
<hr>
<h2 id="EmojiLM-Modeling-the-New-Emoji-Language"><a href="#EmojiLM-Modeling-the-New-Emoji-Language" class="headerlink" title="EmojiLM: Modeling the New Emoji Language"></a>EmojiLM: Modeling the New Emoji Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01751">http://arxiv.org/abs/2311.01751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/komeijiforce/emojilm">https://github.com/komeijiforce/emojilm</a></li>
<li>paper_authors: Letian Peng, Zilong Wang, Hang Liu, Zihan Wang, Jingbo Shang</li>
<li>for: 研究在线上社交媒体上的表情符号（emoji）的使用趋势和应用。</li>
<li>methods: 使用大型自然语言模型创建了大量文本-表情符号平行数据库（Text2Emoji），并基于这个平行数据库对文本-表情符号 bidirectional 翻译进行了几何分析。</li>
<li>results: 比较baseline模型和平行数据库，我们的提案模型在公共benchmark上和人工评估中均有出色的表现，并且显示了文本-表情符号bidirectional 翻译的应用价值。<details>
<summary>Abstract</summary>
With the rapid development of the internet, online social media welcomes people with different backgrounds through its diverse content. The increasing usage of emoji becomes a noticeable trend thanks to emoji's rich information beyond cultural or linguistic borders. However, the current study on emojis is limited to single emoji prediction and there are limited data resources available for further study of the interesting linguistic phenomenon. To this end, we synthesize a large text-emoji parallel corpus, Text2Emoji, from a large language model. Based on the parallel corpus, we distill a sequence-to-sequence model, EmojiLM, which is specialized in the text-emoji bidirectional translation. Extensive experiments on public benchmarks and human evaluation demonstrate that our proposed model outperforms strong baselines and the parallel corpus benefits emoji-related downstream tasks.
</details>
<details>
<summary>摘要</summary>
“因互联网的快速发展，在线社交媒体逐渐推广不同背景的人透过各种多元内容。增加使用表情符号的趋势也因为表情符号具有跨文化或语言边界的丰富信息，成为当前研究热点。然而，现有的研究仅专注于单一表情符号预测，有限的数据资源对进一步研究表情符号的兴趣语言现象提供了有限的支持。为此，我们合成了大量文本-表情符号平行数据库，Text2Emoji，基于大型语言模型。根据平行数据库，我们提炼了文本-表情符号双向翻译模型，EmojiLM，并进行了广泛的公共benchmark和人类评价。实验结果显示，我们提议的模型优于强基eline，并且平行数据库对表情符号相关下游任务具有助益。”
</details></li>
</ul>
<hr>
<h2 id="SAC-3-Reliable-Hallucination-Detection-in-Black-Box-Language-Models-via-Semantic-aware-Cross-check-Consistency"><a href="#SAC-3-Reliable-Hallucination-Detection-in-Black-Box-Language-Models-via-Semantic-aware-Cross-check-Consistency" class="headerlink" title="SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency"></a>SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01740">http://arxiv.org/abs/2311.01740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin, Sricharan Kumar</li>
<li>for: 检测语言模型中的幻觉是现代自然语言处理中的一个关键步骤，以确定语言模型的可靠性。</li>
<li>methods: 我们基于语言模型的自我一致性进行检测，并发现了问题水平和模型水平的两种幻觉，这些幻觉不能通过自我一致性检测察看到。我们提出了一种新的采样方法，即含义相关的检查三重方法（SAC$^3$），该方法基于自我一致性检测的原理，并具有更多的机制来检测问题水平和模型水平的幻觉。</li>
<li>results: 我们通过广泛和系统的实验分析，证明了SAC$^3$ 方法在多个问答和开放领域生成 benchmark 上的表现，可以准确地检测非事实和事实声明。<details>
<summary>Abstract</summary>
Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC$^3$) that expands on the principle of self-consistency checking. Our SAC$^3$ approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC$^3$ outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.
</details>
<details>
<summary>摘要</summary>
现代语言模型（LM）的可信worthiness问题是一个关键步骤。为了解决这个问题，我们重新审视了现有的检测方法，基于语言模型自我一致性。我们发现了两种类型的幻觉，即问题级幻觉和模型级幻觉，这些幻觉不可以通过自我一致性检查 alone 检测出来。基于这一发现，我们提出了一种新的采样基于方法，即含义相关的交叉检查一致性（SAC$^3$）。我们的SAC$^3$方法具有检测问题级和模型级幻觉的能力，通过利用包括semantically相同的问题抖动和跨模型响应一致性检查在内的进一步技术。我们通过了广泛和系统的实验分析，证明了SAC$^3$在检测多个问答和开放领域生成benchmark上的非事实和事实陈述性能比前者更高。
</details></li>
</ul>
<hr>
<h2 id="Proto-lm-A-Prototypical-Network-Based-Framework-for-Built-in-Interpretability-in-Large-Language-Models"><a href="#Proto-lm-A-Prototypical-Network-Based-Framework-for-Built-in-Interpretability-in-Large-Language-Models" class="headerlink" title="Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models"></a>Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01732">http://arxiv.org/abs/2311.01732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Xie, Soroush Vosoughi, Saeed Hassanpour</li>
<li>for: This paper aims to improve the interpretability of Large Language Models (LLMs) by developing a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance.</li>
<li>methods: The proposed method, called proto-lm, uses a prototypical network to learn interpretable embeddings that can be used to understand how the LLM is making predictions. The method is based on a white-box framework, which allows for transparency and interpretability of the model’s inner workings.</li>
<li>results: The authors demonstrate the applicability and interpretability of their method through experiments on a wide range of NLP tasks, and show that their approach can pave the way for more interpretable models without sacrificing performance. Specifically, their results indicate that the proposed method can learn interpretable embeddings that can be used to understand how the LLM is making predictions, and that the method maintains competitive performance on a variety of NLP tasks.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher level text units. In this work, we introduce proto-lm, a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance. Our method's applicability and interpretability are demonstrated through experiments on a wide range of NLP tasks, and our results indicate a new possibility of creating interpretable models without sacrificing performance. This novel approach to interpretability in LLMs can pave the way for more interpretable models without the need to sacrifice performance.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)大型语言模型（LLMs）已经帮助了自然语言处理（NLP）领域的发展，但它们的无法解释性带来了主要的担忧。现有的LLMs解释方法都是后期应用的，并且有些缺点，如专注于低级特征和文本单位高级解释性的缺失。在这项工作中，我们提出了 proto-lm，一种基于 прото型网络的白色盒框架，使得 LLMs 可以在练习阶段直接学习可解释的嵌入，而不会影响性能。我们的方法在多种 NLP 任务上进行了实验，并证明了其可应用性和解释性。 results 表明了一种可能性，即创建可解释的模型不需要牺牲性能。这种新的LLMs解释方法可能会开辟出一条新的解释性道路，无需牺牲性能。
</details></li>
</ul>
<hr>
<h2 id="A-New-Korean-Text-Classification-Benchmark-for-Recognizing-the-Political-Intents-in-Online-Newspapers"><a href="#A-New-Korean-Text-Classification-Benchmark-for-Recognizing-the-Political-Intents-in-Online-Newspapers" class="headerlink" title="A New Korean Text Classification Benchmark for Recognizing the Political Intents in Online Newspapers"></a>A New Korean Text Classification Benchmark for Recognizing the Political Intents in Online Newspapers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01712">http://arxiv.org/abs/2311.01712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kdavid2355/kopolitic-benchmark-dataset">https://github.com/kdavid2355/kopolitic-benchmark-dataset</a></li>
<li>paper_authors: Beomjune Kim, Eunsun Lee, Dongbin Na</li>
<li>for: 本文主要针对在南韩新闻媒体上发表的政治意图文章进行自动识别。</li>
<li>methods: 该文使用了深度学习基于变换器架构的语言模型，并在大规模的韩国新闻数据集上进行训练。</li>
<li>results: 训练后的模型显示了良好的文本分类性能，并且可以同时进行多任务分类。此外，该文还提供了大规模的韩国新闻数据集，可供Future研究使用。<details>
<summary>Abstract</summary>
Many users reading online articles in various magazines may suffer considerable difficulty in distinguishing the implicit intents in texts. In this work, we focus on automatically recognizing the political intents of a given online newspaper by understanding the context of the text. To solve this task, we present a novel Korean text classification dataset that contains various articles. We also provide deep-learning-based text classification baseline models trained on the proposed dataset. Our dataset contains 12,000 news articles that may contain political intentions, from the politics section of six of the most representative newspaper organizations in South Korea. All the text samples are labeled simultaneously in two aspects (1) the level of political orientation and (2) the level of pro-government. To the best of our knowledge, our paper is the most large-scale Korean news dataset that contains long text and addresses multi-task classification problems. We also train recent state-of-the-art (SOTA) language models that are based on transformer architectures and demonstrate that the trained models show decent text classification performance. All the codes, datasets, and trained models are available at https://github.com/Kdavid2355/KoPolitic-Benchmark-Dataset.
</details>
<details>
<summary>摘要</summary>
многие用户在阅读在线报纸时可能会遇到很大的区分隐含意图的困难。在这项工作中，我们关注自动识别在线报纸中的政治意图，通过理解文本的上下文来解决这个问题。为解决这个任务，我们提供了一个新的韩国文本分类数据集，该数据集包含了多种文章。我们还提供了基于深度学习的文本分类基线模型，该模型在我们提posed的数据集上训练。我们的数据集包含12,000篇报纸文章，这些文章可能包含政治意图，来自韩国六家最重要的报纸组织的政治部分。所有的文本样本都同时被标注了两个方面：（1）政治方向的水平和（2）政府支持度的水平。根据我们所知，我们的论文是最大规模的韩国新闻数据集，它包含了长文本，并解决了多任务分类问题。我们还训练了最新的状态zig对应的语言模型，该模型基于变换架构，并示出了训练后的模型在文本分类任务上的不错表现。所有的代码、数据集和训练模型都可以在https://github.com/Kdavid2355/KoPolitic-Benchmark-Dataset上获取。
</details></li>
</ul>
<hr>
<h2 id="CASE-Commonsense-Augmented-Score-with-an-Expanded-Answer-Space"><a href="#CASE-Commonsense-Augmented-Score-with-an-Expanded-Answer-Space" class="headerlink" title="CASE: Commonsense-Augmented Score with an Expanded Answer Space"></a>CASE: Commonsense-Augmented Score with an Expanded Answer Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01684">http://arxiv.org/abs/2311.01684</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wk-chen/commonsense-augmented-score-with-an-expanded-answer-space">https://github.com/wk-chen/commonsense-augmented-score-with-an-expanded-answer-space</a></li>
<li>paper_authors: Wenkai Chen, Sahithya Ravi, Vered Shwartz</li>
<li>for: 这个论文是为了提高 Language Model (LM) 在多项选择问答任务中的表现，特别是 Addressing the limitation of basic score 对所有单词的对待。</li>
<li>methods: 该论文提出了 Commonsense-Augmented Score with Expanded Answer Space (CASE)，即基于含义关系的单词重要性权重，以及生成多元答案的方法。</li>
<li>results: 对五个常识 benchmark 进行了测试，RESULTS 表明，在使用 smaller LMs 时，CASE 方法可以超越强基线，并且与答案空间扩展方法相结合时，效果更好。<details>
<summary>Abstract</summary>
LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks to the knowledge they acquired in their training. In multiple-choice QA tasks, the LM probabilities are used as an imperfect measure of the plausibility of each answer choice. One of the major limitations of the basic score is that it treats all words as equally important. We propose CASE, a Commonsense-Augmented Score with an Expanded Answer Space. CASE addresses this limitation by assigning importance weights for individual words based on their semantic relations to other words in the input. The dynamic weighting approach outperforms basic LM scores, not only because it reduces noise from unimportant words, but also because it informs the model of implicit commonsense knowledge that may be useful for answering the question. We then also follow prior work in expanding the answer space by generating lexically-divergent answers that are conceptually-similar to the choices. When combined with answer space expansion, our method outperforms strong baselines on 5 commonsense benchmarks. We further show these two approaches are complementary and may be especially beneficial when using smaller LMs.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China. The translation may not be perfect, and some nuances or idioms may not be fully conveyed.
</details></li>
</ul>
<hr>
<h2 id="Plot-Retrieval-as-an-Assessment-of-Abstract-Semantic-Association"><a href="#Plot-Retrieval-as-an-Assessment-of-Abstract-Semantic-Association" class="headerlink" title="Plot Retrieval as an Assessment of Abstract Semantic Association"></a>Plot Retrieval as an Assessment of Abstract Semantic Association</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01666">http://arxiv.org/abs/2311.01666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shicheng Xu, Liang Pang, Jiangnan Li, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, Jie Zhou</li>
<li>for: 提高阅读体验和效率，提取相关剧情图文</li>
<li>methods: 使用标注数据集Plot Retrieval进行训练和评估信息检索模型的抽象含义关系能力</li>
<li>results: 现有信息检索模型仍然在捕捉抽象含义关系方面做不够，需要进一步研究抽象含义模型化能力<details>
<summary>Abstract</summary>
Retrieving relevant plots from the book for a query is a critical task, which can improve the reading experience and efficiency of readers. Readers usually only give an abstract and vague description as the query based on their own understanding, summaries, or speculations of the plot, which requires the retrieval model to have a strong ability to estimate the abstract semantic associations between the query and candidate plots. However, existing information retrieval (IR) datasets cannot reflect this ability well. In this paper, we propose Plot Retrieval, a labeled dataset to train and evaluate the performance of IR models on the novel task Plot Retrieval. Text pairs in Plot Retrieval have less word overlap and more abstract semantic association, which can reflect the ability of the IR models to estimate the abstract semantic association, rather than just traditional lexical or semantic matching. Extensive experiments across various lexical retrieval, sparse retrieval, dense retrieval, and cross-encoder methods compared with human studies on Plot Retrieval show current IR models still struggle in capturing abstract semantic association between texts. Plot Retrieval can be the benchmark for further research on the semantic association modeling ability of IR models.
</details>
<details>
<summary>摘要</summary>
<<SYS>> Retrieving relevant plots from a book based on a query is a crucial task that can enhance the reading experience and efficiency of readers. However, existing information retrieval (IR) datasets do not reflect this ability well. In this paper, we propose Plot Retrieval, a labeled dataset to train and evaluate the performance of IR models on the novel task of Plot Retrieval. The text pairs in Plot Retrieval have less word overlap and more abstract semantic association, which can better reflect the ability of IR models to estimate the abstract semantic association rather than just traditional lexical or semantic matching. Extensive experiments comparing various lexical retrieval, sparse retrieval, dense retrieval, and cross-encoder methods with human studies on Plot Retrieval show that current IR models still struggle in capturing abstract semantic associations between texts. Plot Retrieval can serve as a benchmark for further research on the semantic association modeling ability of IR models.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/03/cs.CL_2023_11_03/" data-id="closbronj00dx0g88awy6gv7c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/03/cs.LG_2023_11_03/" class="article-date">
  <time datetime="2023-11-03T10:00:00.000Z" itemprop="datePublished">2023-11-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/03/cs.LG_2023_11_03/">cs.LG - 2023-11-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Universal-Sharpness-Dynamics-in-Neural-Network-Training-Fixed-Point-Analysis-Edge-of-Stability-and-Route-to-Chaos"><a href="#Universal-Sharpness-Dynamics-in-Neural-Network-Training-Fixed-Point-Analysis-Edge-of-Stability-and-Route-to-Chaos" class="headerlink" title="Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos"></a>Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02076">http://arxiv.org/abs/2311.02076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayal Singh Kalra, Tianyu He, Maissam Barkeshli</li>
<li>For: 研究Gradient Descent动态学习神经网络中的稳定性和学习速率问题。* Methods: 使用一个简单的2层线性网络（UV模型）在一个单个训练样本上进行训练，并分析函数空间的固定点结构和函数更新的向量场，以揭示学习过程中稳定性和学习速率的机理。* Results: 发现在训练过程中，稳定性可能会随时间的推移而减退（早期稳定性减退），然后转为进行进攻性的加剧（进攻性加剧），并且在学习率增加时可能会出现边缘稳定性边缘。通过分析固定点结构和函数更新向量场，我们揭示了这些稳定性趋势的机理，包括早期稳定性减退的机理、进攻性加剧的机理和学习率增加时边缘稳定性边缘的机理。<details>
<summary>Abstract</summary>
In gradient descent dynamics of neural networks, the top eigenvalue of the Hessian of the loss (sharpness) displays a variety of robust phenomena throughout training. This includes early time regimes where the sharpness may decrease during early periods of training (sharpness reduction), and later time behavior such as progressive sharpening and edge of stability. We demonstrate that a simple $2$-layer linear network (UV model) trained on a single training example exhibits all of the essential sharpness phenomenology observed in real-world scenarios. By analyzing the structure of dynamical fixed points in function space and the vector field of function updates, we uncover the underlying mechanisms behind these sharpness trends. Our analysis reveals (i) the mechanism behind early sharpness reduction and progressive sharpening, (ii) the required conditions for edge of stability, and (iii) a period-doubling route to chaos on the edge of stability manifold as learning rate is increased. Finally, we demonstrate that various predictions from this simplified model generalize to real-world scenarios and discuss its limitations.
</details>
<details>
<summary>摘要</summary>
在神经网络的梯度下降动力学中，损失函数的希尔比率（锐度）在训练过程中展现了多种 Robust 现象。包括在训练的早期阶段减少锐度（锐度减少），以及 later 阶段的进攻性锐度和稳定边缘。我们示出了一个简单的两层线性网络（UV 模型）在单个训练示例上进行训练时显示了所有真实场景中的锐度现象。通过分析函数空间的固定点结构和函数更新的向量场，我们揭示了这些锐度趋势的内在机制。我们的分析显示了（i）锐度减少和进攻性锐度的机制，（ii）学习率增加时稳定边缘的必要条件，以及（iii）学习率增加时period-doubling 到稳定边缘抽象的混沌路径。最后，我们证明了这个简化模型的预测在真实场景中有效，并讨论了它的局限性。
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-Based-Species-Range-Estimation"><a href="#Active-Learning-Based-Species-Range-Estimation" class="headerlink" title="Active Learning-Based Species Range Estimation"></a>Active Learning-Based Species Range Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02061">http://arxiv.org/abs/2311.02061</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chris-lange/sdm_active_sampling">https://github.com/chris-lange/sdm_active_sampling</a></li>
<li>paper_authors: Christian Lange, Elijah Cole, Grant Van Horn, Oisin Mac Aodha</li>
<li>for: 该论文旨在提出一种新的活动学习方法，用于有效地估计种群范围从有限的地面观察数据中。</li>
<li>methods: 该方法基于模型大量弱监睹社区收集的观察数据，并使用这些模型来生成候选种群范围集。然后，该方法采用一种新的活动询问方法，Sequentially选择最有优势的地理位置进行访问，以减少对未地图的种群范围的不确定性。</li>
<li>results: 作者对该方法进行了详细的评估，并与现有的活动学习方法和方法进行比较。结果表明，该方法高效地估计种群范围，并且与使用末端训练的模型准确率相似，即使只使用一部分数据。这显示了活动学习通过传输学习的空间表示来估计种群范围的utilty，以及利用emerging大规模的社区收集数据来活动发现种群。<details>
<summary>Abstract</summary>
We propose a new active learning approach for efficiently estimating the geographic range of a species from a limited number of on the ground observations. We model the range of an unmapped species of interest as the weighted combination of estimated ranges obtained from a set of different species. We show that it is possible to generate this candidate set of ranges by using models that have been trained on large weakly supervised community collected observation data. From this, we develop a new active querying approach that sequentially selects geographic locations to visit that best reduce our uncertainty over an unmapped species' range. We conduct a detailed evaluation of our approach and compare it to existing active learning methods using an evaluation dataset containing expert-derived ranges for one thousand species. Our results demonstrate that our method outperforms alternative active learning methods and approaches the performance of end-to-end trained models, even when only using a fraction of the data. This highlights the utility of active learning via transfer learned spatial representations for species range estimation. It also emphasizes the value of leveraging emerging large-scale crowdsourced datasets, not only for modeling a species' range, but also for actively discovering them.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的活动学习方法，用于效率地估算一种动物的地理范围从有限多个地面观察数据中。我们模型了这种未映射的物种的范围为多种不同种的估算范围的权重组合。我们表明可以通过使用已经训练过大规模、弱监督社区收集的观察数据来生成这些候选者。然后，我们开发了一种新的活动查询方法，该方法在不断选择不确定度最高的地理位置进行访问，以减少对未映射物种范围的不确定性。我们进行了详细的评估，并与现有的活动学习方法和方法进行比较，使用专家所 derivation 的范围数据集中的一千种物种的评估结果表明，我们的方法在使用只有一部分数据时仍可以超越其他活动学习方法，并且接近终端训练模型的性能。这种结果强调了通过活动学习 transferred 的空间表示来估算物种范围的有用性，以及利用emerging大规模的社区收集数据来模型物种范围的重要性。
</details></li>
</ul>
<hr>
<h2 id="Reproducible-Parameter-Inference-Using-Bagged-Posteriors"><a href="#Reproducible-Parameter-Inference-Using-Bagged-Posteriors" class="headerlink" title="Reproducible Parameter Inference Using Bagged Posteriors"></a>Reproducible Parameter Inference Using Bagged Posteriors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02019">http://arxiv.org/abs/2311.02019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan H. Huggins, Jeffrey W. Miller</li>
<li>for: 本研究旨在解决 bayesian  posterior 在模型误差下的不准确性问题，并提出一种可靠的 reproduceability  критерий。</li>
<li>methods: 本研究使用了 bagging 技术，即使用 posterior  Distribution  conditioned on bootstrapped datasets，以提高 reproduceability。</li>
<li>results: 研究发现，bayesbag  Typically  satisfies the overlap lower bound，并且有一个 Bernstein–Von Mises  theorem，确定它的 asymptotic normal distribution。 通过 simulated experiments 和犯罪率预测应用，研究证明 bayesbag 的优点。<details>
<summary>Abstract</summary>
Under model misspecification, it is known that Bayesian posteriors often do not properly quantify uncertainty about true or pseudo-true parameters. Even more fundamentally, misspecification leads to a lack of reproducibility in the sense that the same model will yield contradictory posteriors on independent data sets from the true distribution. To define a criterion for reproducible uncertainty quantification under misspecification, we consider the probability that two confidence sets constructed from independent data sets have nonempty overlap, and we establish a lower bound on this overlap probability that holds for any valid confidence sets. We prove that credible sets from the standard posterior can strongly violate this bound, particularly in high-dimensional settings (i.e., with dimension increasing with sample size), indicating that it is not internally coherent under misspecification. To improve reproducibility in an easy-to-use and widely applicable way, we propose to apply bagging to the Bayesian posterior ("BayesBag"'); that is, to use the average of posterior distributions conditioned on bootstrapped datasets. We motivate BayesBag from first principles based on Jeffrey conditionalization and show that the bagged posterior typically satisfies the overlap lower bound. Further, we prove a Bernstein--Von Mises theorem for the bagged posterior, establishing its asymptotic normal distribution. We demonstrate the benefits of BayesBag via simulation experiments and an application to crime rate prediction.
</details>
<details>
<summary>摘要</summary>
“在模型错误下， bayesian posterior 通常不能妥善量化 true 或 pseudo-true 参数的不确定性。 更重要的是，错误会导致模型的不可重复性，即使使用同一个模型，在独立的数据集上得到的 posterior 会具有矛盾的结果。 为了定义在错误下的可重复性量化标准，我们考虑了两个独立的数据集上constructed confidence set之间的非空 overlap概率，并证明了这个 overlap 概率下界，该下界适用于任何有效的confidence set。 我们证明了标准 posterior 的信任集可能会强烈违反这个下界，特别是在高维度 Setting（即采样大小增长）中，表明这些信任集不是内在coherent 的。 为了改善可重复性，我们提议使用 bagging 技术（即 conditioned on bootstrapped datasets 的 posterior distribution）。我们从 Jeffrey conditionalization 的基本原理出发，motivate BayesBag，并证明 BayesBag 通常满足 overlap 下界。 更重要的是，我们证明了 BayesBag 的 asymptotic normal distribution，以及其在不同 Setting 下的性能。 我们通过 simulations 和犯罪率预测应用 demonstrate 了 BayesBag 的好处。”
</details></li>
</ul>
<hr>
<h2 id="A-Variational-Perspective-on-High-Resolution-ODEs"><a href="#A-Variational-Perspective-on-High-Resolution-ODEs" class="headerlink" title="A Variational Perspective on High-Resolution ODEs"></a>A Variational Perspective on High-Resolution ODEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02002">http://arxiv.org/abs/2311.02002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoomaan Maskan, Konstantinos C. Zygalakis, Alp Yurtsever</li>
<li>for: 这个论文主要针对无约制最小化凸函数的问题。</li>
<li>methods: 这篇论文提出了一种新的变量观点，使得可以研究高分辨率ODE。通过这种观点，我们可以更快地 converge  gradient norm minimization 使用 Nesterov 加速器方法。</li>
<li>results: 我们的方法可以在噪声梯度下实现更好的性能，并且可以与现有的方法进行比较。在一些数学实验中，我们的方法与现有的方法进行了比较，并且得到了更好的结果。<details>
<summary>Abstract</summary>
We consider unconstrained minimization of smooth convex functions. We propose a novel variational perspective using forced Euler-Lagrange equation that allows for studying high-resolution ODEs. Through this, we obtain a faster convergence rate for gradient norm minimization using Nesterov's accelerated gradient method. Additionally, we show that Nesterov's method can be interpreted as a rate-matching discretization of an appropriately chosen high-resolution ODE. Finally, using the results from the new variational perspective, we propose a stochastic method for noisy gradients. Several numerical experiments compare and illustrate our stochastic algorithm with state of the art methods.
</details>
<details>
<summary>摘要</summary>
我们考虑不受限制的极小化的几何函数。我们提出了一种新的量子视角，使用强制的欧拉-拉格朗日方程，以研究高分辨率ODE。透过这个新的视角，我们获得了更快的梯度距离减少率，从尼斯特洛夫的加速器梯度方法中。此外，我们显示了尼斯特洛夫的方法可以被解释为一种调整对应的高分辨率ODE的率调整策略。最后，我们使用新的量子视角提出了一种随机方法 для杂质梯度。一些数学实验比较和IlлюSTRATE了我们的随机算法与现有的方法。
</details></li>
</ul>
<hr>
<h2 id="High-Probability-Convergence-of-Adam-Under-Unbounded-Gradients-and-Affine-Variance-Noise"><a href="#High-Probability-Convergence-of-Adam-Under-Unbounded-Gradients-and-Affine-Variance-Noise" class="headerlink" title="High Probability Convergence of Adam Under Unbounded Gradients and Affine Variance Noise"></a>High Probability Convergence of Adam Under Unbounded Gradients and Affine Variance Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02000">http://arxiv.org/abs/2311.02000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusu Hong, Junhong Lin</li>
<li>for: 本文研究了 Adam 算法在非对称非 convex 概率优化中的收敛性。尽管在机器学习领域广泛应用，但其理论性仍然有限。先前的研究主要从预期角度研究 Adam 的收敛，经常需要强制ASSUME 梯度是均匀的和问题依赖的先验知识。这限制了这些发现在实际世界应用中的可用性。</li>
<li>methods: 作者提供了深入分析，证明 Adam 可以在高 probabilit 下 converge 到站点点，其速率为 $\mathcal{O}\left(\frac{\rm poly(\log T)}{\sqrt{T}\right)$，不需要任何梯度假设和问题依赖的先验知识来调整超参数。此外，也发现 Adam 限制了梯度的大小在 $\mathcal{O}\left(\rm poly(\log T)\right)$ 之间。最后，作者还研究了一种简化版 Adam 算法，取消一个纠正项，并获得了适应噪音水平的收敛率。</li>
<li>results: 本文的结果表明，在高probabilit 下，Adam 算法可以 converge 到站点点，其速率为 $\mathcal{O}\left(\frac{\rm poly(\log T)}{\sqrt{T}\right)$，而不需要任何梯度假设和问题依赖的先验知识来调整超参数。此外，Adam 算法还限制了梯度的大小在 $\mathcal{O}\left(\rm poly(\log T)\right)$ 之间。<details>
<summary>Abstract</summary>
In this paper, we study the convergence of the Adaptive Moment Estimation (Adam) algorithm under unconstrained non-convex smooth stochastic optimizations. Despite the widespread usage in machine learning areas, its theoretical properties remain limited. Prior researches primarily investigated Adam's convergence from an expectation view, often necessitating strong assumptions like uniformly stochastic bounded gradients or problem-dependent knowledge in prior. As a result, the applicability of these findings in practical real-world scenarios has been constrained. To overcome these limitations, we provide a deep analysis and show that Adam could converge to the stationary point in high probability with a rate of $\mathcal{O}\left({\rm poly}(\log T)/\sqrt{T}\right)$ under coordinate-wise "affine" variance noise, not requiring any bounded gradient assumption and any problem-dependent knowledge in prior to tune hyper-parameters. Additionally, it is revealed that Adam confines its gradients' magnitudes within an order of $\mathcal{O}\left({\rm poly}(\log T)\right)$. Finally, we also investigate a simplified version of Adam without one of the corrective terms and obtain a convergence rate that is adaptive to the noise level.
</details>
<details>
<summary>摘要</summary>
“在这篇论文中，我们研究了Adaptive Moment Estimation（Adam）算法在不受约束的非凸泛环境中的收敛性。虽然在机器学习领域广泛使用，但其理论性Properties remain limited。先前的研究主要从预期的角度研究了Adam的收敛性，经常假设 gradients是均匀的和bounded，这限制了其在实际场景中的应用。为了突破这些限制，我们提供了深入的分析，并证明Adam可以在高probability下收敛到站点点，其速度为 $\mathcal{O}\left(\frac{\rm poly(\log T)}{\sqrt{T}\right)$，不需要任何 bounded gradient假设和任何问题依赖的优化参数。此外，我们还发现Adam将 gradients的大小限制在 $\mathcal{O}\left(\rm poly(\log T)\right)$ 的范围内。最后，我们还 investigate了Adam中一个简化版本，去掉一个修正项，并 obtain了一个适应噪声水平的收敛速度。”Note: "Simplified Chinese" refers to the written form of Chinese that uses simpler grammar and vocabulary, and is often used in informal writing and online communication. The translation is based on the standardized Simplified Chinese writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Conditions-on-Preference-Relations-that-Guarantee-the-Existence-of-Optimal-Policies"><a href="#Conditions-on-Preference-Relations-that-Guarantee-the-Existence-of-Optimal-Policies" class="headerlink" title="Conditions on Preference Relations that Guarantee the Existence of Optimal Policies"></a>Conditions on Preference Relations that Guarantee the Existence of Optimal Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01990">http://arxiv.org/abs/2311.01990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Colaco Carr, Prakash Panangaden, Doina Precup</li>
<li>For: The paper is written to address the gap between the theory and application of Learning from Preferential Feedback (LfPF) algorithms, specifically in partially-observable, non-Markovian environments.* Methods: The paper introduces the Direct Preference Process, a new framework for analyzing LfPF problems, and uses the von Neumann-Morgenstern Expected Utility Theorem to establish conditions for the existence of optimal policies.* Results: The paper shows that the Direct Preference Process generalizes the standard reinforcement learning problem and provides future practitioners with the tools necessary for a more principled design of LfPF agents, narrowing the gap between empirical success and theoretical understanding.Here is the information in Simplified Chinese text:</li>
<li>for: 本文是为了填补学习从偏好反馈（LfPF）算法的理论和实践之间的空白，特别是在部分可见、非马歇维尔环境中。</li>
<li>methods: 本文引入了直接偏好过程（Direct Preference Process），一种新的分析LfPF问题的框架，并使用 von Neumann-Morgenstern 期望风险函数来确定优质策略的存在条件。</li>
<li>results: 本文表明，直接偏好过程可以将标准循环学习问题推广到非马歇维尔环境中，为未来的实践者提供更原则性的LfPF代理设计的工具。<details>
<summary>Abstract</summary>
Learning from Preferential Feedback (LfPF) plays an essential role in training Large Language Models, as well as certain types of interactive learning agents. However, a substantial gap exists between the theory and application of LfPF algorithms. Current results guaranteeing the existence of optimal policies in LfPF problems assume that both the preferences and transition dynamics are determined by a Markov Decision Process. We introduce the Direct Preference Process, a new framework for analyzing LfPF problems in partially-observable, non-Markovian environments. Within this framework, we establish conditions that guarantee the existence of optimal policies by considering the ordinal structure of the preferences. Using the von Neumann-Morgenstern Expected Utility Theorem, we show that the Direct Preference Process generalizes the standard reinforcement learning problem. Our findings narrow the gap between the empirical success and theoretical understanding of LfPF algorithms and provide future practitioners with the tools necessary for a more principled design of LfPF agents.
</details>
<details>
<summary>摘要</summary>
学习偏好反馈（LfPF）在训练大语言模型和某些交互学习代理人中扮演了关键角色。然而，现有的理论和应用中的LfPF算法存在一定的知识差距。现有的结果只有在Markov决策过程下确保优化策略的存在。我们介绍了新的直接偏好过程框架，用于分析LfPF问题在部分可见、非马歇维环境中。在这个框架下，我们设置了 garantia优化策略的条件，通过考虑偏好的顺序结构。使用von Neumann-Morgenstern预期用途函数，我们表明了直接偏好过程对标准强化学习问题的总结。我们的发现将减少LfPF算法的实际成功和理论理解之间的差距，并为未来的实践者提供了更原则性的LfPF代理人设计的工具。
</details></li>
</ul>
<hr>
<h2 id="Latent-Diffusion-Model-for-Conditional-Reservoir-Facies-Generation"><a href="#Latent-Diffusion-Model-for-Conditional-Reservoir-Facies-Generation" class="headerlink" title="Latent Diffusion Model for Conditional Reservoir Facies Generation"></a>Latent Diffusion Model for Conditional Reservoir Facies Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01968">http://arxiv.org/abs/2311.01968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daesoo Lee, Oscar Ovanger, Jo Eidsvik, Erlend Aune, Jacob Skauvold, Ragnar Hauge</li>
<li>for: used to generate high-fidelity reservoir facies realizations that preserve conditioning data</li>
<li>methods: uses a novel Latent Diffusion Model that leverages the superiority of diffusion models over GANs</li>
<li>results: significantly outperforms a GAN-based alternative in generating realistic reservoir facies<details>
<summary>Abstract</summary>
Creating accurate and geologically realistic reservoir facies based on limited measurements is crucial for field development and reservoir management, especially in the oil and gas sector. Traditional two-point geostatistics, while foundational, often struggle to capture complex geological patterns. Multi-point statistics offers more flexibility, but comes with its own challenges. With the rise of Generative Adversarial Networks (GANs) and their success in various fields, there has been a shift towards using them for facies generation. However, recent advances in the computer vision domain have shown the superiority of diffusion models over GANs. Motivated by this, a novel Latent Diffusion Model is proposed, which is specifically designed for conditional generation of reservoir facies. The proposed model produces high-fidelity facies realizations that rigorously preserve conditioning data. It significantly outperforms a GAN-based alternative.
</details>
<details>
<summary>摘要</summary>
创建准确且地质学上实际的沉积 facies 基于有限的测量是钻井开发和沉积管理中的关键，特别是在油气领域。传统的两点地 statistcs 是基础知识，但它们经常难以捕捉复杂的地质模式。多点统计学提供更多的灵活性，但它们也有自己的挑战。随着生成 adversarial Networks（GANs）在不同领域的成功，有人开始使用它们 для facies 生成。然而，最近的计算机视觉领域的进步表明了扩散模型在 GANs 之上的超越。驱动于这一点，我们提出了一种新的潜在扩散模型，用于 conditional 生成沉积 facies。我们的模型可以生成高精度的 facies 实现，严格保持 conditioning 数据。与 GANs 相比，我们的模型在 conditioning 数据上的表现显著优于。
</details></li>
</ul>
<hr>
<h2 id="Hardness-of-Low-Rank-Approximation-of-Entrywise-Transformed-Matrix-Products"><a href="#Hardness-of-Low-Rank-Approximation-of-Entrywise-Transformed-Matrix-Products" class="headerlink" title="Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products"></a>Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01960">http://arxiv.org/abs/2311.01960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tamas Sarlos, Xingyou Song, David Woodruff, Qiuyi, Zhang</li>
<li>for: 本文研究了在entrywise transformed setting下的低级 Approximation问题，即想要找到一个好的rank $k$ Approximation来表示$f(U\cdot V)$, где$U, V^\top \in \mathbb{R}^{n \times r}$是 givens, $r &#x3D; O(\log(n))$, $f(x)$是一个通用的scalar函数。</li>
<li>methods: 我们使用了 previoius work in sublinear low rank approximation中的方法，并给出了首次的conditional time hardness result，证明了两个condition（1）和（2）是必要的，以获得better than $n^{2-o(1)}$ time的相对误差low rank Approximation。</li>
<li>results: 我们给出了一个novel reduction from Strong Exponential Time Hypothesis (SETH)，这个reduction rely on lower bounding the leverage scores of flat sparse vectors，并在$U \neq V^\top$情况下提供了runtime lower bounds of the form $\Omega(\min(n^{2-o(1)}, \Omega(2^p)))$. 最后，我们证明了我们的下界是紧的，给出了一个$O(n \cdot \text{poly}(k, 2^p, 1&#x2F;\epsilon))$ time relative error approximation algorithm和一个fast $O(n \cdot \text{poly}(k, p, 1&#x2F;\epsilon))$ additive error approximation。<details>
<summary>Abstract</summary>
Inspired by fast algorithms in natural language processing, we study low rank approximation in the entrywise transformed setting where we want to find a good rank $k$ approximation to $f(U \cdot V)$, where $U, V^\top \in \mathbb{R}^{n \times r}$ are given, $r = O(\log(n))$, and $f(x)$ is a general scalar function. Previous work in sublinear low rank approximation has shown that if both (1) $U = V^\top$ and (2) $f(x)$ is a PSD kernel function, then there is an $O(nk^{\omega-1})$ time constant relative error approximation algorithm, where $\omega \approx 2.376$ is the exponent of matrix multiplication. We give the first conditional time hardness results for this problem, demonstrating that both conditions (1) and (2) are in fact necessary for getting better than $n^{2-o(1)}$ time for a relative error low rank approximation for a wide class of functions. We give novel reductions from the Strong Exponential Time Hypothesis (SETH) that rely on lower bounding the leverage scores of flat sparse vectors and hold even when the rank of the transformed matrix $f(UV)$ and the target rank are $n^{o(1)}$, and when $U = V^\top$. Furthermore, even when $f(x) = x^p$ is a simple polynomial, we give runtime lower bounds in the case when $U \neq V^\top$ of the form $\Omega(\min(n^{2-o(1)}, \Omega(2^p)))$. Lastly, we demonstrate that our lower bounds are tight by giving an $O(n \cdot \text{poly}(k, 2^p, 1/\epsilon))$ time relative error approximation algorithm and a fast $O(n \cdot \text{poly}(k, p, 1/\epsilon))$ additive error approximation using fast tensor-based sketching. Additionally, since our low rank algorithms rely on matrix-vector product subroutines, our lower bounds extend to show that computing $f(UV)W$, for even a small matrix $W$, requires $\Omega(n^{2-o(1)})$ time.
</details>
<details>
<summary>摘要</summary>
受快速算法在自然语言处理中启发的启示，我们研究了在变换后的下rankapprox问题，即找到一个好的rank-$k$approximation于$f(U\cdot V)$, где$U, V^\top \in \mathbb{R}^{n \times r}$是给定的，$r = O(\log(n))$,和$f(x)$是一个通用的整数函数。先前的低线性下rankapprox问题的研究表明，如果 Both (1) $U = V^\top$和 (2) $f(x)$是一个PSDkernel函数，那么有一个$O(nk^{2.376-1})$时间常量相对误差approximation算法。我们给出了首次的条件时间困难结果，证明了这两个条件是必要的，以获得一个better than $n^{2-o(1)}$时间的相对误差low rankapproximation。我们还给出了novel的SETH降低，基于lower bounding the leverages cores of flat sparse vectors，这些降低可以在$f(UV)$的rank和目标rank是$n^{o(1)}$时仍然保持有效。几种情况下，我们给出了runtime lower bounds的形式，包括$f(x) = x^p$是一个简单的多项式时，当$U \neq V^\top$时，我们给出了$\Omega(\min(n^{2-o(1)}, \Omega(2^p)))$的时间下界。最后，我们证明了我们的下界是紧的，给出了一个$O(n \cdot \text{poly}(k, 2^p, 1/\epsilon))$时间相对误差approximation算法和一个快速的$O(n \cdot \text{poly}(k, p, 1/\epsilon))$添加itive error approximation。此外，由于我们的low rank算法 rely on matrix-vector product subroutines，我们的下界也适用于计算$f(UV)W$，其中$W$是一个小的矩阵。
</details></li>
</ul>
<hr>
<h2 id="Optimistic-Multi-Agent-Policy-Gradient-for-Cooperative-Tasks"><a href="#Optimistic-Multi-Agent-Policy-Gradient-for-Cooperative-Tasks" class="headerlink" title="Optimistic Multi-Agent Policy Gradient for Cooperative Tasks"></a>Optimistic Multi-Agent Policy Gradient for Cooperative Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01953">http://arxiv.org/abs/2311.01953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenshuai Zhao, Yi Zhao, Zhiyuan Li, Juho Kannala, Joni Pajarinen</li>
<li>for: 多智能体学习任务中的协同学习问题，特别是在使用函数拟合学习时遇到的相对过拟合问题。</li>
<li>methods: 我们提出了一种基于Leaky ReLU函数的简单框架，以减少多智能体学习过程中的相对过拟合问题。</li>
<li>results: 我们在多种多智能体任务上进行了广泛的测试，并证明了我们的方法可以在13个测试任务中超过强基eline，并在剩下的6个任务中与基eline匹配性。<details>
<summary>Abstract</summary>
\textit{Relative overgeneralization} (RO) occurs in cooperative multi-agent learning tasks when agents converge towards a suboptimal joint policy due to overfitting to suboptimal behavior of other agents. In early work, optimism has been shown to mitigate the \textit{RO} problem when using tabular Q-learning. However, with function approximation optimism can amplify overestimation and thus fail on complex tasks. On the other hand, recent deep multi-agent policy gradient (MAPG) methods have succeeded in many complex tasks but may fail with severe \textit{RO}. We propose a general, yet simple, framework to enable optimistic updates in MAPG methods and alleviate the RO problem. Specifically, we employ a \textit{Leaky ReLU} function where a single hyperparameter selects the degree of optimism to reshape the advantages when updating the policy. Intuitively, our method remains optimistic toward individual actions with lower returns which are potentially caused by other agents' sub-optimal behavior during learning. The optimism prevents the individual agents from quickly converging to a local optimum. We also provide a formal analysis from an operator view to understand the proposed advantage transformation. In extensive evaluations on diverse sets of tasks, including illustrative matrix games, complex \textit{Multi-agent MuJoCo} and \textit{Overcooked} benchmarks, the proposed method\footnote{Code can be found at \url{https://github.com/wenshuaizhao/optimappo}.} outperforms strong baselines on 13 out of 19 tested tasks and matches the performance on the rest.
</details>
<details>
<summary>摘要</summary>
\begin{blockquote}多代理人学习任务中的相对过拟合（RO）现象发生在多个代理人协作学习环境中，当代理人因其他代理人的不优秀行为而过拟合到低优秀的共同策略。在早期的工作中，使用表格式Q学习的Optimism已经被证明可以降低RO问题。然而，使用函数拟合的Optimism可能会增加过估计，因此在复杂任务上失败。在 contrary，最近的深度多代理人策略梯度法（MAPG）方法在许多复杂任务上取得了成功，但可能会在严重的RO问题下失败。我们提出了一个通用 yet simple 的框架，以便在 MAPG 方法中实现可信的更新和RO问题的缓解。具体来说，我们使用 \_ Leaky ReLU 函数，其中一个参数选择度量优化的度量来重塑优势。我们的方法保持对个体动作的低返回值的optimism，这些返回值可能是由其他代理人的不优秀行为所导致的。optimism 防止个体代理人快速 converges to 局部优点。我们还提供了一种基于运算员视角的正式分析，以便更好地理解我们提议的优势转换。在多种任务集中，包括简单的矩阵游戏、复杂的多代理人 MuJoCo 和 Overcooked  bencmarks，我们的方法（代码可以在 <https://github.com/wenshuaizhao/optimappo> 找到）在 13 个测试任务中超过强基线，并在剩下 6 个任务中匹配性能。\end{blockquote}Note that the translation is done using Google Translate, and may not be perfect. Please let me know if you have any further questions or need any corrections.
</details></li>
</ul>
<hr>
<h2 id="ForecastPFN-Synthetically-Trained-Zero-Shot-Forecasting"><a href="#ForecastPFN-Synthetically-Trained-Zero-Shot-Forecasting" class="headerlink" title="ForecastPFN: Synthetically-Trained Zero-Shot Forecasting"></a>ForecastPFN: Synthetically-Trained Zero-Shot Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01933">http://arxiv.org/abs/2311.01933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abacusai/forecastpfn">https://github.com/abacusai/forecastpfn</a></li>
<li>paper_authors: Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Siddartha Naidu, Colin White</li>
<li>for: 这篇论文是为了解决时间序列预测问题，特别是当初始资料够少时。</li>
<li>methods: 本文使用了一个名为 ForecastPFN 的预测模型，这是一个基于统计学的专案调整网络。这个模型可以在单一的前进传递中对新的时间序列资料进行预测。</li>
<li>results: 根据实验结果，ForecastPFN 的预测结果比 state-of-the-art 方法更精确和更快速，甚至当其他方法被允许使用百名以上的内部数据点时。<details>
<summary>Abstract</summary>
The vast majority of time-series forecasting approaches require a substantial training dataset. However, many real-life forecasting applications have very little initial observations, sometimes just 40 or fewer. Thus, the applicability of most forecasting methods is restricted in data-sparse commercial applications. While there is recent work in the setting of very limited initial data (so-called `zero-shot' forecasting), its performance is inconsistent depending on the data used for pretraining. In this work, we take a different approach and devise ForecastPFN, the first zero-shot forecasting model trained purely on a novel synthetic data distribution. ForecastPFN is a prior-data fitted network, trained to approximate Bayesian inference, which can make predictions on a new time series dataset in a single forward pass. Through extensive experiments, we show that zero-shot predictions made by ForecastPFN are more accurate and faster compared to state-of-the-art forecasting methods, even when the other methods are allowed to train on hundreds of additional in-distribution data points.
</details>
<details>
<summary>摘要</summary>
大多数时间序列预测方法需要很多训练数据。然而，在实际应用中，有很多情况只有40个或更少的初始观察值。因此，大多数预测方法在商业应用中的可用性受限。而在最近的研究中，有一些在非常有限的初始数据上进行预测（称为“零 shot”预测），但其性能因数据使用 для预测而异常。在这项工作中，我们采用了一种不同的方法，并提出了 ForecastPFN，第一个基于新的 sintetic 数据分布的零 shot 预测模型。ForecastPFN 是一种基于先验知识的 fitted 网络，通过单次前进 pass 来预测新的时间序列数据。经过广泛的实验，我们表明，由 ForecastPFN 进行预测的零 shot 预测结果比现有的预测方法更准确和更快，即使它们在 hundreds 个更多的在 Distribution 上进行训练。
</details></li>
</ul>
<hr>
<h2 id="Simplifying-Transformer-Blocks"><a href="#Simplifying-Transformer-Blocks" class="headerlink" title="Simplifying Transformer Blocks"></a>Simplifying Transformer Blocks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01906">http://arxiv.org/abs/2311.01906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bobby-he/simplified_transformers">https://github.com/bobby-he/simplified_transformers</a></li>
<li>paper_authors: Bobby He, Thomas Hofmann</li>
<li>For: The paper aims to simplify the standard transformer block to improve training speed and reduce the number of parameters.* Methods: The authors use signal propagation theory and empirical observations to motivate modifications to the standard transformer block, including removing skip connections, projection or value parameters, sequential sub-blocks, and normalization layers.* Results: The simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput and using 15% fewer parameters.<details>
<summary>Abstract</summary>
A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections & normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable.   In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput, and using 15% fewer parameters.
</details>
<details>
<summary>摘要</summary>
“一个简单的设计方程式 для深度Transformer是将相同的建筑块组合起来。但标准transformer块与聚合、对称层和normalization层的组合，使得模型变得非常复杂，几乎所有的变更都会导致模型训练速度下降或者无法训练。在这个研究中，我们询问这些标准transformer块可以被简化到多少 extent？通过信号传递理论和实验观察，我们提出了一些修改，让许多块件可以被移除无损训练速度，包括跳过 Connection、投影或值参数、Sequential sub-blocks 和normalization层。在采用了 both autoregressive decoder-only 和 BERT encoder-only 模型的实验中，我们的简化transformer模型可以与标准transformer模型相似的每个更新训练速度和性能，并且在使用15% fewer parameters的情况下，比标准transformer模型快15%。”
</details></li>
</ul>
<hr>
<h2 id="High-Precision-Causal-Model-Evaluation-with-Conditional-Randomization"><a href="#High-Precision-Causal-Model-Evaluation-with-Conditional-Randomization" class="headerlink" title="High Precision Causal Model Evaluation with Conditional Randomization"></a>High Precision Causal Model Evaluation with Conditional Randomization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01902">http://arxiv.org/abs/2311.01902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Ma, Cheng Zhang</li>
<li>for: 评估 causal 模型的标准方法是比较模型预测与真实的效应来自Randomized controlled trials (RCT)。但RCT不 always feasible或道德可靠。在这种情况下，使用conditionally randomized experiments based on inverse probability weighting (IPW) 可能会受到高估程度的问题。</li>
<li>methods: 我们引入了一种新的low-variance estimator for causal error，称为 pairs estimator。我们将同一个IPW estimator应用于模型和真实的实验效应上，从而使得其变iance due to IPW 消失，并 achieve smaller asymptotic variance。</li>
<li>results: 我们的方法可以在实验设置下提高 causal inference 模型的评估，并且在实验中 demonstrate 了我们的方法的优势，表明它可以达到 near-RCT 性能。这种简单 yet powerful 的方法可以在 conditional randomization 设置下评估 causal inference 模型，无需修改 IPW estimator 本身，从而为模型评估带来更加robust和可靠。<details>
<summary>Abstract</summary>
The gold standard for causal model evaluation involves comparing model predictions with true effects estimated from randomized controlled trials (RCT). However, RCTs are not always feasible or ethical to perform. In contrast, conditionally randomized experiments based on inverse probability weighting (IPW) offer a more realistic approach but may suffer from high estimation variance. To tackle this challenge and enhance causal model evaluation in real-world conditional randomization settings, we introduce a novel low-variance estimator for causal error, dubbed as the pairs estimator. By applying the same IPW estimator to both the model and true experimental effects, our estimator effectively cancels out the variance due to IPW and achieves a smaller asymptotic variance. Empirical studies demonstrate the improved of our estimator, highlighting its potential on achieving near-RCT performance. Our method offers a simple yet powerful solution to evaluate causal inference models in conditional randomization settings without complicated modification of the IPW estimator itself, paving the way for more robust and reliable model assessments.
</details>
<details>
<summary>摘要</summary>
“金Standard” для评估 causal模型 involves comparing model predictions with true effects estimated from randomized controlled trials（RCT）。However，RCTs are not always feasible or ethical to perform。In contrast，conditionally randomized experiments based on inverse probability weighting（IPW）offer a more realistic approach but may suffer from high estimation variance。To tackle this challenge and enhance causal model evaluation in real-world conditional randomization settings，we introduce a novel low-variance estimator for causal error，dubbed as the pairs estimator。By applying the same IPW estimator to both the model and true experimental effects，our estimator effectively cancels out the variance due to IPW and achieves a smaller asymptotic variance。Empirical studies demonstrate the improved of our estimator，highlighting its potential on achieving near-RCT performance。Our method offers a simple yet powerful solution to evaluate causal inference models in conditional randomization settings without complicated modification of the IPW estimator itself，paving the way for more robust and reliable model assessments。
</details></li>
</ul>
<hr>
<h2 id="Online-non-parametric-likelihood-ratio-estimation-by-Pearson-divergence-functional-minimization"><a href="#Online-non-parametric-likelihood-ratio-estimation-by-Pearson-divergence-functional-minimization" class="headerlink" title="Online non-parametric likelihood-ratio estimation by Pearson-divergence functional minimization"></a>Online non-parametric likelihood-ratio estimation by Pearson-divergence functional minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01900">http://arxiv.org/abs/2311.01900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alejandro de la Concha, Nicolas Vayatis, Argyris Kalogeratos</li>
<li>for: 本研究旨在提供一种在线非 Parametric 的 likelihood-ratio estimation（OLRE）方法，用于比较两个概率密度函数（p和q）的差异，并且在观察到 iid 样本 $(x_t \sim p, x’_t \sim q)$ 序列的时候进行。</li>
<li>methods: 我们的方法基于最近的 Kernel Methods 和函数最小化技术，可以有效地在线更新 estimator。我们的方法是非 Parametric，即不知道 $p$ 和 $q$ 的形式。</li>
<li>results: 我们提供了对 OLRE 方法的 theoretically  garantuee，并进行了synthetic experiment 的实验验证。<details>
<summary>Abstract</summary>
Quantifying the difference between two probability density functions, $p$ and $q$, using available data, is a fundamental problem in Statistics and Machine Learning. A usual approach for addressing this problem is the likelihood-ratio estimation (LRE) between $p$ and $q$, which -- to our best knowledge -- has been investigated mainly for the offline case. This paper contributes by introducing a new framework for online non-parametric LRE (OLRE) for the setting where pairs of iid observations $(x_t \sim p, x'_t \sim q)$ are observed over time. The non-parametric nature of our approach has the advantage of being agnostic to the forms of $p$ and $q$. Moreover, we capitalize on the recent advances in Kernel Methods and functional minimization to develop an estimator that can be efficiently updated online. We provide theoretical guarantees for the performance of the OLRE method along with empirical validation in synthetic experiments.
</details>
<details>
<summary>摘要</summary>
“统计和机器学习中衡量两个概率密度函数($p$和$q$)之间的差异使用数据是一个基本问题。一般来说，用likelihood-ratio estimation（LRE）方法来解决这个问题，尽管这个方法主要在单个观测点上进行研究。本文提供了一种新的在线非 Parametric LRE（OLRE）方法，用于在时间序列中观测到的独立identically distributed（iid）观测点 $(x_t \sim p, x'_t \sim q)$。我们的非 Parametric 方法具有不知道 $p$ 和 $q$ 的形式的优点，同时我们利用了最近的核函数方法和函数最小化来开发一个可以高效地在线更新的估计器。我们提供了对OLRE方法的理论保证以及实验 validate在 sintetic experiment中。”Note: "Simplified Chinese" is a translation of the text into Traditional Chinese, which is one of the two standard forms of Chinese writing. The other form is "Traditional Chinese".
</details></li>
</ul>
<hr>
<h2 id="Learning-Sparse-Codes-with-Entropy-Based-ELBOs"><a href="#Learning-Sparse-Codes-with-Entropy-Based-ELBOs" class="headerlink" title="Learning Sparse Codes with Entropy-Based ELBOs"></a>Learning Sparse Codes with Entropy-Based ELBOs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01888">http://arxiv.org/abs/2311.01888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dmytro Velychko, Simon Damm, Asja Fischer, Jörg Lücke</li>
<li>for: 这个论文的目的是提出一种基于信息论的条件随机 sparse coding 学习目标函数，用于非正态 posterior approximations。</li>
<li>methods: 该论文使用了一种基于信息论的条件随机 sparse coding 学习方法，包括非正态 posterior approximations 和 entropy-based 学习目标函数。</li>
<li>results: 该论文的实验结果表明，使用该学习方法可以有效地学习条件随机 sparse coding 模型，并且可以通过 entropy-based 学习目标函数来适应不同的 posterior approximations。<details>
<summary>Abstract</summary>
Standard probabilistic sparse coding assumes a Laplace prior, a linear mapping from latents to observables, and Gaussian observable distributions. We here derive a solely entropy-based learning objective for the parameters of standard sparse coding. The novel variational objective has the following features: (A) unlike MAP approximations, it uses non-trivial posterior approximations for probabilistic inference; (B) unlike for previous non-trivial approximations, the novel objective is fully analytical; and (C) the objective allows for a novel principled form of annealing. The objective is derived by first showing that the standard ELBO objective converges to a sum of entropies, which matches similar recent results for generative models with Gaussian priors. The conditions under which the ELBO becomes equal to entropies are then shown to have analytical solutions, which leads to the fully analytical objective. Numerical experiments are used to demonstrate the feasibility of learning with such entropy-based ELBOs. We investigate different posterior approximations including Gaussians with correlated latents and deep amortized approximations. Furthermore, we numerically investigate entropy-based annealing which results in improved learning. Our main contributions are theoretical, however, and they are twofold: (1) for non-trivial posterior approximations, we provide the (to the knowledge of the authors) first analytical ELBO objective for standard probabilistic sparse coding; and (2) we provide the first demonstration on how a recently shown convergence of the ELBO to entropy sums can be used for learning.
</details>
<details>
<summary>摘要</summary>
标准的数学潜在簇节架假设了拉普拉斯假设、线性对应从潜在到观察值，以及 Gaussian 观察值分布。我们在这里 derivates a solely entropy-based learning objective for the parameters of standard sparse coding。这个新的可变核心目标有以下特点：（A）与MAP估计不同，使用非贫则 posterior 估计 для条件arinferencing;（B）与前一些非贫则估计不同，这个新的目标是完全分析的;（C）这个目标允许一种新的均衡化原理。我们首先显示了标准的ELBO目标可以将转换为一个总 entropy 的和，这与最近的一些生成模型具有 Gaussian 假设的结果相符。然后，我们显示了这些条件下ELBO的解是分析的，这导致了一个完全分析的目标。我们使用了不同的 posterior 估计，包括相关的潜在对应和深度束质化估计。此外，我们还进行了实验，以证明可以使用这种 entropy-based ELBO 进行学习。我们的主要贡献是理论的，主要是：（1）为非贫则 posterior 估计提供了（到我们知识的作者）第一个分析的 ELBO 目标 для标准的数学潜在簇节架;（2）我们提供了最初将 ELBO 转换为 entropy 和的概念的证明。
</details></li>
</ul>
<hr>
<h2 id="Domain-Randomization-via-Entropy-Maximization"><a href="#Domain-Randomization-via-Entropy-Maximization" class="headerlink" title="Domain Randomization via Entropy Maximization"></a>Domain Randomization via Entropy Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01885">http://arxiv.org/abs/2311.01885</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabrieletiboni/doraemon">https://github.com/gabrieletiboni/doraemon</a></li>
<li>paper_authors: Gabriele Tiboni, Pascal Klink, Jan Peters, Tatiana Tommasi, Carlo D’Eramo, Georgia Chalvatzaki</li>
<li>for: 本研究旨在解决Domain Randomization（DR）中的现实差距问题，即RL中在不同的环境中的行为不同。</li>
<li>methods: 本研究提出了一种新的方法，即Domain Randomization via Entropy MaximizatiON（DORAEMON），它是一个受限的优化问题，通过直接最大化训练 distribuion 的熵来自动调整环境参数的分布。</li>
<li>results: 实验结果表明，DORAEMON 可以获得高度适应和普适的策略，即在不同的环境参数下能够解决任务。此外，DORAEMON 还可以在不知道实际世界参数的情况下进行零基础转移。<details>
<summary>Abstract</summary>
Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL). Nevertheless, DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to regularize the agent's behavior but notoriously leads to overly conservative policies when randomizing excessively. In this paper, we propose a novel approach to address sim-to-real transfer, which automatically shapes dynamics distributions during training in simulation without requiring real-world data. We introduce DOmain RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization problem that directly maximizes the entropy of the training distribution while retaining generalization capabilities. In achieving this, DORAEMON gradually increases the diversity of sampled dynamics parameters as long as the probability of success of the current policy is sufficiently high. We empirically validate the consistent benefits of DORAEMON in obtaining highly adaptive and generalizable policies, i.e. solving the task at hand across the widest range of dynamics parameters, as opposed to representative baselines from the DR literature. Notably, we also demonstrate the Sim2Real applicability of DORAEMON through its successful zero-shot transfer in a robotic manipulation setup under unknown real-world parameters.
</details>
<details>
<summary>摘要</summary>
varying 动力参数在模拟中是一种受欢迎的Domain Randomization（DR）方法，以减少RL中的现实差距。然而，DR强烈取决于模拟中的参数采样分布的选择，因为高度的变化是关键来减少代理人的行为，但同时不应该过度随机。在这篇文章中，我们提出了一种新的方法来解决模拟到实际的转移问题，即在训练中自动调整动力分布，无需真实世界数据。我们称之为Domain Randomization via Entropy Maximization（DORAEMON），它是一个受限制的优化问题，直接最大化训练 distribuion的熵，保持泛化能力。在实现这一点上，DORAEMON逐渐增加样本动力参数的多样性，只要当当前策略的成功概率充分高时。我们在许多DR文献中进行了比较，证明了DORAEMON可以获得高度适应和泛化的策略，即在不同的动力参数下能够成功解决任务。此外，我们还证明了DORAEMON在机器人 manipulate setup中的零化转移可行性。
</details></li>
</ul>
<hr>
<h2 id="Spectral-Clustering-of-Attributed-Multi-relational-Graphs"><a href="#Spectral-Clustering-of-Attributed-Multi-relational-Graphs" class="headerlink" title="Spectral Clustering of Attributed Multi-relational Graphs"></a>Spectral Clustering of Attributed Multi-relational Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01840">http://arxiv.org/abs/2311.01840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ylli Sadikaj, Yllka Velaj, Sahar Behzadi, Claudia Plant</li>
<li>for: 本研究旨在提出一种基于多种关系和特征属性的图 clustering 方法，以便更好地理解图结构和属性之间的相互关系。</li>
<li>methods: 本研究提出了 SpectralMix 方法，它是一种结合所有图信息的维度减少技术，可以将图结构、不同类型的关系和属性信息都减少到一个维度中，从而更好地理解图结构和属性之间的相互关系。</li>
<li>results: 实验结果表明，SpectralMix 方法可以更好地捕捉图结构和属性之间的相互关系，并且在多个实际数据集上显示出了更高的效果。<details>
<summary>Abstract</summary>
Graph clustering aims at discovering a natural grouping of the nodes such that similar nodes are assigned to a common cluster. Many different algorithms have been proposed in the literature: for simple graphs, for graphs with attributes associated to nodes, and for graphs where edges represent different types of relations among nodes. However, complex data in many domains can be represented as both attributed and multi-relational networks.   In this paper, we propose SpectralMix, a joint dimensionality reduction technique for multi-relational graphs with categorical node attributes. SpectralMix integrates all information available from the attributes, the different types of relations, and the graph structure to enable a sound interpretation of the clustering results. Moreover, it generalizes existing techniques: it reduces to spectral embedding and clustering when only applied to a single graph and to homogeneity analysis when applied to categorical data. Experiments conducted on several real-world datasets enable us to detect dependencies between graph structure and categorical attributes, moreover, they exhibit the superiority of SpectralMix over existing methods.
</details>
<details>
<summary>摘要</summary>
graph clustering aimed at discovering natural grouping of nodes, such that similar nodes are assigned to common cluster. many different algorithms have been proposed in literature: for simple graphs, for graphs with attributes associated to nodes, and for graphs where edges represent different types of relations among nodes. however, complex data in many domains can be represented as both attributed and multi-relational networks. in this paper, we propose spectralmix, a joint dimensionality reduction technique for multi-relational graphs with categorical node attributes. spectralmix integrates all information available from attributes, different types of relations, and graph structure to enable sound interpretation of clustering results. moreover, it generalizes existing techniques: it reduces to spectral embedding and clustering when only applied to single graph and to homogeneity analysis when applied to categorical data. experiments conducted on several real-world datasets enable us to detect dependencies between graph structure and categorical attributes, moreover, they exhibit superiority of spectralmix over existing methods.
</details></li>
</ul>
<hr>
<h2 id="Mix-ME-Quality-Diversity-for-Multi-Agent-Learning"><a href="#Mix-ME-Quality-Diversity-for-Multi-Agent-Learning" class="headerlink" title="Mix-ME: Quality-Diversity for Multi-Agent Learning"></a>Mix-ME: Quality-Diversity for Multi-Agent Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01829">http://arxiv.org/abs/2311.01829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Garðar Ingvarsson, Mikayel Samvelyan, Bryan Lim, Manon Flageat, Antoine Cully, Tim Rocktäschel</li>
<li>for: 本研究旨在探讨多智能体系中的质量多样性（Quality-Diversity，QD）方法，以实现在不同的情况和需求下发现高性能解决方案的多样性。</li>
<li>methods: 本研究提出了一种基于MAP-Elites算法的多智能体变体 Mix-ME，通过混合不同队伍中的智能体来生成新的解决方案。</li>
<li>results: 在多种部分可见控制任务上进行评估，研究发现，基于Mix-ME算法生成的多智能体变体不仅与单智能体基准相匹配，而且在多智能体情况下，在部分可见情况下也经常超越单智能体基准。<details>
<summary>Abstract</summary>
In many real-world systems, such as adaptive robotics, achieving a single, optimised solution may be insufficient. Instead, a diverse set of high-performing solutions is often required to adapt to varying contexts and requirements. This is the realm of Quality-Diversity (QD), which aims to discover a collection of high-performing solutions, each with their own unique characteristics. QD methods have recently seen success in many domains, including robotics, where they have been used to discover damage-adaptive locomotion controllers. However, most existing work has focused on single-agent settings, despite many tasks of interest being multi-agent. To this end, we introduce Mix-ME, a novel multi-agent variant of the popular MAP-Elites algorithm that forms new solutions using a crossover-like operator by mixing together agents from different teams. We evaluate the proposed methods on a variety of partially observable continuous control tasks. Our evaluation shows that these multi-agent variants obtained by Mix-ME not only compete with single-agent baselines but also often outperform them in multi-agent settings under partial observability.
</details>
<details>
<summary>摘要</summary>
在许多实际系统中，如适应机器人学习，单个优化解决方案可能不足。相反，需要一个多样化高性能解决方案来适应不同的上下文和需求。这是质量多样性（QD）的领域，旨在发现一组高性能解决方案，每个都具有独特的特点。QD方法在多个领域中获得成功，包括机器人学习，其用于发现损害适应行动控制器。然而，大多数现有工作都集中在单机器人设置下，尽管许多任务对象是多机器人。为此，我们介绍 Mix-ME，一种新的多机器人变体，使用混合操作将不同队伍中的机器人混合而成新的解决方案。我们对多种部分可见连续控制任务进行评估，结果显示，由 Mix-ME 获得的多机器人变体不仅与单机器人基elines竞争，而且在部分可见情况下frequently outperform 他们。
</details></li>
</ul>
<hr>
<h2 id="Sketching-for-Convex-and-Nonconvex-Regularized-Least-Squares-with-Sharp-Guarantees"><a href="#Sketching-for-Convex-and-Nonconvex-Regularized-Least-Squares-with-Sharp-Guarantees" class="headerlink" title="Sketching for Convex and Nonconvex Regularized Least Squares with Sharp Guarantees"></a>Sketching for Convex and Nonconvex Regularized Least Squares with Sharp Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01806">http://arxiv.org/abs/2311.01806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingzhen Yang, Ping Li</li>
<li>for:  solving large-scale optimization problems with regularization functions, such as least square problems with convex or nonconvex regularization.</li>
<li>methods:  proposes a fast sketching algorithm called Sketching for Regularized Optimization (SRO), which generates a sketch of the original data matrix and solves the sketched problem to obtain the optimization results.</li>
<li>results:  the proposed algorithm handles general Frechet subdifferentiable regularization functions in an unified framework, and provides general theoretical results for the approximation error between the original problem and the sketched problem for regularized least square problems. Additionally, minimax rates for sparse signal estimation by solving the sketched sparse convex or nonconvex learning problems are obtained under mild conditions.<details>
<summary>Abstract</summary>
Randomized algorithms are important for solving large-scale optimization problems. In this paper, we propose a fast sketching algorithm for least square problems regularized by convex or nonconvex regularization functions, Sketching for Regularized Optimization (SRO). Our SRO algorithm first generates a sketch of the original data matrix, then solves the sketched problem. Different from existing randomized algorithms, our algorithm handles general Frechet subdifferentiable regularization functions in an unified framework. We present general theoretical result for the approximation error between the optimization results of the original problem and the sketched problem for regularized least square problems which can be convex or nonconvex. For arbitrary convex regularizer, relative-error bound is proved for the approximation error. Importantly, minimax rates for sparse signal estimation by solving the sketched sparse convex or nonconvex learning problems are also obtained using our general theoretical result under mild conditions. To the best of our knowledge, our results are among the first to demonstrate minimax rates for convex or nonconvex sparse learning problem by sketching under a unified theoretical framework. We further propose an iterative sketching algorithm which reduces the approximation error exponentially by iteratively invoking the sketching algorithm. Experimental results demonstrate the effectiveness of the proposed SRO and Iterative SRO algorithms.
</details>
<details>
<summary>摘要</summary>
随机算法在解决大规模优化问题上具有重要的意义。在这篇论文中，我们提出了一种快速的笔记算法，即Sketching for Regularized Optimization（SRO）。我们的SRO算法首先生成了原始数据矩阵的笔记，然后解决笔记中的问题。与现有的随机算法不同，我们的算法可以处理通用的Fréchet次导函数。我们提供了对于各种正则化函数的通用理论结论，包括对于几何函数的 bounds。我们还证明了对于各种正则化函数的最小最大rate，并通过实验证明了我们的提案的有效性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Generalization-Properties-of-Diffusion-Models"><a href="#On-the-Generalization-Properties-of-Diffusion-Models" class="headerlink" title="On the Generalization Properties of Diffusion Models"></a>On the Generalization Properties of Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01797">http://arxiv.org/abs/2311.01797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lphleo/diffusion_generalization">https://github.com/lphleo/diffusion_generalization</a></li>
<li>paper_authors: Puheng Li, Zhong Li, Huishuai Zhang, Jiang Bian</li>
<li>for: 这个论文旨在理解扩散模型的泛化能力。</li>
<li>methods: 这篇论文使用了评估扩散模型的泛化误差，并提出了一种基于样本大小和模型容量的泛化误差估计。</li>
<li>results: 研究发现，扩散模型在训练过程中的泛化误差随着样本大小和模型容量的增长而减少，并且在数据集中的模式变化下保持可靠的泛化性。<details>
<summary>Abstract</summary>
Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$ and the model capacity $m$, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. Furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progressively increasing distances between modes. This precisely elucidates the adverse effect of "modes shift" in ground truths on the model generalization. Moreover, these estimates are not solely theoretical constructs but have also been confirmed through numerical simulations. Our findings contribute to the rigorous understanding of diffusion models' generalization properties and provide insights that may guide practical applications.
</details>
<details>
<summary>摘要</summary>
Diffusion models是一类生成模型，旨在建立一个随机运输map，将empirical observation中的未知目标分布和一个已知的先验分布相匹配。尽管它们在实际应用中表现出色，但对它们的总体泛化能力的理论理解仍然受到限制。这项工作开始了Diffusion models的总体泛化能力的理论探索。我们提出了Diffusion models的泛化差的理论估计，表明在训练Score-based diffusion models时，泛化错误的演变矩阵是$O(n^{-2/5}+m^{-4/5})$，其中$n$是样本大小，$m$是模型容量，不是数据维度的幂次增长，这意味着Diffusion models在训练时可以避免欠拟合症（curse of dimensionality）。此外，我们还扩展了我们的量化分析至数据依赖的场景，在这种场景下，目标分布是一系列的浓度，每个浓度之间的距离逐渐增长。这准确地阐述了模型泛化中"模式shift"的弊端，并且这些估计不仅是理论构造，还经过了数值 simulations 的验证。我们的发现对Diffusion models的泛化性能的理论理解做出了贡献，并提供了实践应用中的指导。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Augment-Distributions-for-Out-of-Distribution-Detection"><a href="#Learning-to-Augment-Distributions-for-Out-of-Distribution-Detection" class="headerlink" title="Learning to Augment Distributions for Out-of-Distribution Detection"></a>Learning to Augment Distributions for Out-of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01796">http://arxiv.org/abs/2311.01796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qizhou Wang, Zhen Fang, Yonggang Zhang, Feng Liu, Yixuan Li, Bo Han</li>
<li>for: 本文旨在解决在开放世界下，使用 auxiliary OOD 数据进行 OOD 探测时，存在异常分布的问题。</li>
<li>methods: 本文提出了 Distributional-Augmented OOD Learning (DAL) 方法，通过在 Wasserstein 球中心auxiliary OOD 分布上构造 OOD 分布集来减少 OOD 分布差异。</li>
<li>results: 对多种 OOD 探测设置进行了广泛的评估，并证明 DAL 在 auxiliary OOD 数据上进行训练的预测器可以提高开放世界下 OOD 探测性能。<details>
<summary>Abstract</summary>
Open-world classification systems should discern out-of-distribution (OOD) data whose labels deviate from those of in-distribution (ID) cases, motivating recent studies in OOD detection. Advanced works, despite their promising progress, may still fail in the open world, owing to the lack of knowledge about unseen OOD data in advance. Although one can access auxiliary OOD data (distinct from unseen ones) for model training, it remains to analyze how such auxiliary data will work in the open world. To this end, we delve into such a problem from a learning theory perspective, finding that the distribution discrepancy between the auxiliary and the unseen real OOD data is the key to affecting the open-world detection performance. Accordingly, we propose Distributional-Augmented OOD Learning (DAL), alleviating the OOD distribution discrepancy by crafting an OOD distribution set that contains all distributions in a Wasserstein ball centered on the auxiliary OOD distribution. We justify that the predictor trained over the worst OOD data in the ball can shrink the OOD distribution discrepancy, thus improving the open-world detection performance given only the auxiliary OOD data. We conduct extensive evaluations across representative OOD detection setups, demonstrating the superiority of our DAL over its advanced counterparts.
</details>
<details>
<summary>摘要</summary>
To address this discrepancy, we propose Distributional-Augmented OOD Learning (DAL), which involves crafting an OOD distribution set that contains all distributions within a Wasserstein ball centered on the auxiliary OOD distribution. We show that training a predictor over the worst OOD data in the ball can help shrink the OOD distribution discrepancy, thereby improving open-world detection performance given only the auxiliary OOD data.We conduct extensive evaluations across representative OOD detection setups and demonstrate the superiority of our DAL over its advanced counterparts.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Generalized-Low-Rank-Tensor-Contextual-Bandits"><a href="#Efficient-Generalized-Low-Rank-Tensor-Contextual-Bandits" class="headerlink" title="Efficient Generalized Low-Rank Tensor Contextual Bandits"></a>Efficient Generalized Low-Rank Tensor Contextual Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01771">http://arxiv.org/abs/2311.01771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianxin Yi, Yiyang Yang, Yao Wang, Shaojie Tang</li>
<li>for: This paper aims to provide high-usable and accountable decision-making services by building a novel bandits algorithm that fully harnesses the power of multi-dimensional data and non-linear reward functions.</li>
<li>methods: The paper introduces a generalized low-rank tensor contextual bandits model, which represents an action as a tensor and determines the reward through a generalized linear function. The algorithm “Generalized Low-Rank Tensor Exploration Subspace then Refine” (G-LowTESTR) is introduced to effectively trade off exploration and exploitation.</li>
<li>results: The paper shows that the regret bound of G-LowTESTR is superior to those in vectorization and matricization cases through theoretical analysis and simulations&#x2F;real data experiments. The algorithm is able to capitalize on the low-rank tensor structure for enhanced learning.<details>
<summary>Abstract</summary>
In this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. To this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus can be represented by a tensor. In this formulation, the reward is determined through a generalized linear function applied to the inner product of the action's feature tensor and a fixed but unknown parameter tensor with a low tubal rank. To effectively achieve the trade-off between exploration and exploitation, we introduce a novel algorithm called "Generalized Low-Rank Tensor Exploration Subspace then Refine" (G-LowTESTR). This algorithm first collects raw data to explore the intrinsic low-rank tensor subspace information embedded in the decision-making scenario, and then converts the original problem into an almost lower-dimensional generalized linear contextual bandits problem. Rigorous theoretical analysis shows that the regret bound of G-LowTESTR is superior to those in vectorization and matricization cases. We conduct a series of simulations and real data experiments to further highlight the effectiveness of G-LowTESTR, leveraging its ability to capitalize on the low-rank tensor structure for enhanced learning.
</details>
<details>
<summary>摘要</summary>
在本文中，我们目标建立一种新的带刺数据搜索算法，能够全面利用多维数据的力量和奖励函数的内在非线性，提供高可用和可负责的决策服务。为此，我们引入一种泛化低级张量上下文ual bandits模型，其中一个动作可以由三个特征向量组成，并且可以表示为一个张量。在这种形式下，奖励由一个通用线性函数应用于动作特征张量和一个固定 pero unknown 参数张量的内积来确定。为实现探索和利用之间的负荷平衡，我们引入一种新的算法called "泛化低级张量探索空间然后精细" (G-LowTESTR)。这个算法首先收集原始数据，以探索决策场景中附加的低级张量信息，然后将原始问题转换为一个几乎两维的通用线性contextual bandits问题。我们的理论分析表明，G-LowTESTR的 regret bound高于vectorization和matricization情况。我们进行了一系列的仿真和实际数据实验，以证明G-LowTESTR的效果，利用其能够利用低级张量结构进行加强学习。
</details></li>
</ul>
<hr>
<h2 id="Solving-Kernel-Ridge-Regression-with-Gradient-Descent-for-a-Non-Constant-Kernel"><a href="#Solving-Kernel-Ridge-Regression-with-Gradient-Descent-for-a-Non-Constant-Kernel" class="headerlink" title="Solving Kernel Ridge Regression with Gradient Descent for a Non-Constant Kernel"></a>Solving Kernel Ridge Regression with Gradient Descent for a Non-Constant Kernel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01762">http://arxiv.org/abs/2311.01762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oskar Allerbo</li>
<li>For: 该研究探讨了 kernel ridge regression（KRR）中kernel的变化在训练过程中对模型复杂性和泛化性的影响，并提出了一种在训练过程中逐步递减带宽的更新方案。* Methods: 该研究使用了KRR的迭代法，并 investigate了在训练过程中变化kernel的影响。* Results: 研究发现，逐步递减带宽可以使KRR模型在训练Error和泛化性之间取得平衡，并且可以实现零训练Error和良好的泛化性。此外，研究还发现了一种double descent现象，其中在某些情况下，逐步递减带宽可以使模型的泛化性提高。<details>
<summary>Abstract</summary>
Kernel ridge regression, KRR, is a generalization of linear ridge regression that is non-linear in the data, but linear in the parameters. The solution can be obtained either as a closed-form solution, which includes a matrix inversion, or iteratively through gradient descent. Using the iterative approach opens up for changing the kernel during training, something that is investigated in this paper. We theoretically address the effects this has on model complexity and generalization. Based on our findings, we propose an update scheme for the bandwidth of translational-invariant kernels, where we let the bandwidth decrease to zero during training, thus circumventing the need for hyper-parameter selection. We demonstrate on real and synthetic data how decreasing the bandwidth during training outperforms using a constant bandwidth, selected by cross-validation and marginal likelihood maximization. We also show theoretically and empirically that using a decreasing bandwidth, we are able to achieve both zero training error in combination with good generalization, and a double descent behavior, phenomena that do not occur for KRR with constant bandwidth but are known to appear for neural networks.
</details>
<details>
<summary>摘要</summary>
kernel ridge regression（KRR）是Linear Ridge Regression的推广，在数据上是非线性的，但在参数上是线性的。解决方案可以通过关键值矩阵 inverse 或者迭代的梯度下降来获得。使用迭代方法可以在训练过程中改变kernel，这在本文中被调查。我们从理论角度解决了这些改变对模型复杂度和泛化性的影响。基于我们的发现，我们提出了一种更新策略，其中在训练过程中减小了带宽，从而消除了hyperparameter选择的需要。我们在实际和Synthetic data上示出了使用减小带宽在训练中的优化性。此外，我们还 theoretically和Empirically验证了使用减小带宽可以实现零训练误差、良好的泛化和神经网络上知道的双峰现象。
</details></li>
</ul>
<hr>
<h2 id="TinyFormer-Efficient-Transformer-Design-and-Deployment-on-Tiny-Devices"><a href="#TinyFormer-Efficient-Transformer-Design-and-Deployment-on-Tiny-Devices" class="headerlink" title="TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices"></a>TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01759">http://arxiv.org/abs/2311.01759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianlei Yang, Jiacheng Liao, Fanding Lei, Meichen Liu, Junyi Chen, Lingkun Long, Han Wan, Bei Yu, Weisheng Zhao</li>
<li>for: 本研究旨在开发和部署在微控制器单元（MCU）上的深度学习模型，以满足各种嵌入式互联网应用的需求。</li>
<li>methods: 该研究提出了一个名为TinyFormer的框架，用于开发和部署MCU上的资源有效的转换器模型。TinyFormer包括SuperNAS、SparseNAS和SparseEngine三部分。SuperNAS用于在庞大的搜索空间中搜索适合MCU的超网络模型。SparseNAS用于评估最佳缺省单路模型，包括转换器架构。最后，SparseEngine高效地将搜索到的缺省模型部署到MCU上进行推理。</li>
<li>results: 根据CIFAR-10数据集的评估结果，TinyFormer可以开发高效的转换器模型，具有$96.1%$的准确率，同时遵循MCU的硬件限制，即$1$MB存储和$320$KB内存。此外，TinyFormer在缺省推理中具有显著的速度提升，达到$12.2\times$的提升，相比CMSIS-NN库。TinyFormer被认为可以将强大的转换器带入天线ML场景，扩大深度学习应用的范围。<details>
<summary>Abstract</summary>
Developing deep learning models on tiny devices (e.g. Microcontroller units, MCUs) has attracted much attention in various embedded IoT applications. However, it is challenging to efficiently design and deploy recent advanced models (e.g. transformers) on tiny devices due to their severe hardware resource constraints. In this work, we propose TinyFormer, a framework specifically designed to develop and deploy resource-efficient transformers on MCUs. TinyFormer mainly consists of SuperNAS, SparseNAS and SparseEngine. Separately, SuperNAS aims to search for an appropriate supernet from a vast search space. SparseNAS evaluates the best sparse single-path model including transformer architecture from the identified supernet. Finally, SparseEngine efficiently deploys the searched sparse models onto MCUs. To the best of our knowledge, SparseEngine is the first deployment framework capable of performing inference of sparse models with transformer on MCUs. Evaluation results on the CIFAR-10 dataset demonstrate that TinyFormer can develop efficient transformers with an accuracy of $96.1\%$ while adhering to hardware constraints of $1$MB storage and $320$KB memory. Additionally, TinyFormer achieves significant speedups in sparse inference, up to $12.2\times$, when compared to the CMSIS-NN library. TinyFormer is believed to bring powerful transformers into TinyML scenarios and greatly expand the scope of deep learning applications.
</details>
<details>
<summary>摘要</summary>
发展深度学习模型在微控制器单元（MCU）上（例如，微控制器单元）已经吸引了各种嵌入互联网应用的广泛关注。然而，由于MCU的硬件资源有限制，使得不可避免地将现代高级模型（如转换器）部署到MCU上是一项挑战。在这项工作中，我们提出了TinyFormer框架，用于开发和部署MCU上的资源有效的转换器模型。TinyFormer主要由SuperNAS、SparseNAS和SparseEngine三部分组成。每一部分都扮演着重要的角色。首先，SuperNAS通过巨量搜索空间来搜索适合MCU的超网络。然后，SparseNAS根据找到的超网络来评估最佳的稀疏单轨模型，包括转换器架构。最后，SparseEngine高效地将搜索到的稀疏模型部署到MCU上。到目前为止，SparseEngine是首个可以在MCU上进行稀疏模型执行的投影引擎。我们的评估结果表明，TinyFormer可以在CIFAR-10数据集上开发高效的转换器模型，具有$96.1\%$的准确率，同时遵守MCU的硬件限制，即$1$MB存储和$320$KB内存。此外，TinyFormer在稀疏执行中实现了与CMSIS-NN库相比的速度提升，达到$12.2\times$。TinyFormer被认为将带来强大的转换器到天然语言应用场景，扩大深度学习应用的范围。
</details></li>
</ul>
<hr>
<h2 id="Epidemic-Decision-making-System-Based-Federated-Reinforcement-Learning"><a href="#Epidemic-Decision-making-System-Based-Federated-Reinforcement-Learning" class="headerlink" title="Epidemic Decision-making System Based Federated Reinforcement Learning"></a>Epidemic Decision-making System Based Federated Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01749">http://arxiv.org/abs/2311.01749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangxi Zhou, Junping Du, Zhe Xue, Zhenhui Pan, Weikang Chen</li>
<li>for: 该论文旨在帮助政府通过对公共安全和经济发展进行全面考虑，应对公共卫生和安全紧急情况。</li>
<li>methods: 该论文提出了一种基于联合学习的方法，通过将各省的疫情情况数据进行合作训练，以提高疫情决策的准确性和效率。</li>
<li>results: 实验结果显示，联合学习方法可以在疫情决策中获得更优化的性能和返回，并可以加速训练模型的收敛速度。此外，对比试验还表明，A2C模型是适合疫情决策场景的最佳强化学习模型，其次是PPO模型，而DDPG模型的性能较差。<details>
<summary>Abstract</summary>
Epidemic decision-making can effectively help the government to comprehensively consider public security and economic development to respond to public health and safety emergencies. Epidemic decision-making can effectively help the government to comprehensively consider public security and economic development to respond to public health and safety emergencies. Some studies have shown that intensive learning can effectively help the government to make epidemic decision, thus achieving the balance between health security and economic development. Some studies have shown that intensive learning can effectively help the government to make epidemic decision, thus achieving the balance between health security and economic development. However, epidemic data often has the characteristics of limited samples and high privacy. However, epidemic data often has the characteristics of limited samples and high privacy. This model can combine the epidemic situation data of various provinces for cooperative training to use as an enhanced learning model for epidemic situation decision, while protecting the privacy of data. The experiment shows that the enhanced federated learning can obtain more optimized performance and return than the enhanced learning, and the enhanced federated learning can also accelerate the training convergence speed of the training model. accelerate the training convergence speed of the client. At the same time, through the experimental comparison, A2C is the most suitable reinforcement learning model for the epidemic situation decision-making. learning model for the epidemic situation decision-making scenario, followed by the PPO model, and the performance of DDPG is unsatisfactory.
</details>
<details>
<summary>摘要</summary>
《医疫决策》可以有效地帮助政府全面考虑公共安全和经济发展，以应对公共卫生和安全紧急情况。一些研究表明，高效学习可以帮助政府做出医疫决策，从而实现健康安全和经济发展的平衡。然而，医疫数据经常具有有限的样本和高隐私性。为此，本文提出了一种基于联合学习的医疫决策模型，可以结合各省的医疫情况数据进行合作训练，以保护数据隐私。实验表明，加强联合学习可以在训练模型性能和训练速度两个方面取得更高的优化效果，而且在训练速度方面，加强联合学习可以加速客户端的训练速度。同时，通过实验对比，A2C模型在医疫决策场景中表现最佳，其次是PPO模型，而DDPG模型的表现不满足。
</details></li>
</ul>
<hr>
<h2 id="Global-Optimization-A-Machine-Learning-Approach"><a href="#Global-Optimization-A-Machine-Learning-Approach" class="headerlink" title="Global Optimization: A Machine Learning Approach"></a>Global Optimization: A Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01742">http://arxiv.org/abs/2311.01742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Dimitris Bertsimas, Georgios Margaritis</li>
<li>for:  solves black-box global optimization problems with nonlinear constraints.</li>
<li>methods:  uses hyperplane-based Decision-Trees and mixed integer optimization (MIO) approximation, with extensions to other ML models and adaptive sampling procedures.</li>
<li>results:  shows improvements in solution feasibility and optimality in the majority of instances compared to BARON, with improved optimality gaps or solution times in 11 instances.<details>
<summary>Abstract</summary>
Many approaches for addressing Global Optimization problems typically rely on relaxations of nonlinear constraints over specific mathematical primitives. This is restricting in applications with constraints that are black-box, implicit or consist of more general primitives. Trying to address such limitations, Bertsimas and Ozturk (2023) proposed OCTHaGOn as a way of solving black-box global optimization problems by approximating the nonlinear constraints using hyperplane-based Decision-Trees and then using those trees to construct a unified mixed integer optimization (MIO) approximation of the original problem. We provide extensions to this approach, by (i) approximating the original problem using other MIO-representable ML models besides Decision Trees, such as Gradient Boosted Trees, Multi Layer Perceptrons and Suport Vector Machines, (ii) proposing adaptive sampling procedures for more accurate machine learning-based constraint approximations, (iii) utilizing robust optimization to account for the uncertainty of the sample-dependent training of the ML models, and (iv) leveraging a family of relaxations to address the infeasibilities of the final MIO approximation. We then test the enhanced framework in 81 Global Optimization instances. We show improvements in solution feasibility and optimality in the majority of instances. We also compare against BARON, showing improved optimality gaps or solution times in 11 instances.
</details>
<details>
<summary>摘要</summary>
多种方法通常用于解决全球优化问题，通常基于非线性约束的松弛。这限制了应用中的约束是黑盒、隐藏或更一般的 primitives。 Trying to address these limitations, Bertsimas and Ozturk (2023) proposed OCTHaGOn to solve black-box global optimization problems by approximating nonlinear constraints using hyperplane-based Decision-Trees and then using those trees to construct a unified mixed integer optimization (MIO) approximation of the original problem. We provide extensions to this approach, by (i) approximating the original problem using other MIO-representable ML models besides Decision Trees, such as Gradient Boosted Trees, Multi Layer Perceptrons and Suport Vector Machines, (ii) proposing adaptive sampling procedures for more accurate machine learning-based constraint approximations, (iii) utilizing robust optimization to account for the uncertainty of the sample-dependent training of the ML models, and (iv) leveraging a family of relaxations to address the infeasibilities of the final MIO approximation. We then test the enhanced framework in 81 Global Optimization instances. We show improvements in solution feasibility and optimality in the majority of instances. We also compare against BARON, showing improved optimality gaps or solution times in 11 instances.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="CDGraph-Dual-Conditional-Social-Graph-Synthesizing-via-Diffusion-Model"><a href="#CDGraph-Dual-Conditional-Social-Graph-Synthesizing-via-Diffusion-Model" class="headerlink" title="CDGraph: Dual Conditional Social Graph Synthesizing via Diffusion Model"></a>CDGraph: Dual Conditional Social Graph Synthesizing via Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01729">http://arxiv.org/abs/2311.01729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jui-Yi Tsai, Ya-Wen Teng, Ho Chiok Yew, De-Nian Yang, Lydia Y. Chen</li>
<li>for: 本研究旨在提出一种基于两个指定条件的 conditional diffusion model for social networks，以生成符合条件的社交图。</li>
<li>methods: 该模型通过对 dual conditions 的协同依赖进行杜琪处理，以捕捉两个条件之间的互dependent关系，同时还包括社交同类和社交感染来保持节点之间的连接，并通过对 dual conditions 的互dependent关系进行指导，进行 diffusion 过程的训练。</li>
<li>results: 对四个 dataset 进行评估，与四种现有的图生成方法进行比较，结果显示 CDGraph 可以生成符合 dual-conditional 的社交图，并且在多种社交网络指标中具有较低的不一致性和较高的 dual-conditional 有效性。<details>
<summary>Abstract</summary>
The social graphs synthesized by the generative models are increasingly in demand due to data scarcity and concerns over user privacy. One of the key performance criteria for generating social networks is the fidelity to specified conditionals, such as users with certain membership and financial status. While recent diffusion models have shown remarkable performance in generating images, their effectiveness in synthesizing graphs has not yet been explored in the context of conditional social graphs. In this paper, we propose the first kind of conditional diffusion model for social networks, CDGraph, which trains and synthesizes graphs based on two specified conditions. We propose the co-evolution dependency in the denoising process of CDGraph to capture the mutual dependencies between the dual conditions and further incorporate social homophily and social contagion to preserve the connectivity between nodes while satisfying the specified conditions. Moreover, we introduce a novel classifier loss, which guides the training of the diffusion process through the mutual dependency of dual conditions. We evaluate CDGraph against four existing graph generative methods, i.e., SPECTRE, GSM, EDGE, and DiGress, on four datasets. Our results show that the generated graphs from CDGraph achieve much higher dual-conditional validity and lower discrepancy in various social network metrics than the baselines, thus demonstrating its proficiency in generating dual-conditional social graphs.
</details>
<details>
<summary>摘要</summary>
社交图表生成的生成模型受到数据缺乏和用户隐私问题的增加需求。生成社交图表中的一个关键性能标准是对指定的条件进行忠实性，例如用户具有某些会员和财务状况。而最近的扩散模型在生成图像方面已经表现出色，但它们在生成图表方面的效果尚未得到研究。在这篇论文中，我们提出了首个基于条件的扩散模型 для社交图表，即CDGraph，它在两个指定的条件下训练和生成图表。我们提出了在杂化过程中的共演化依赖性，以捕捉图表中节点之间的互相依赖关系，并进一步包括社交同类和社交感染，以保持节点之间的连接而满足指定的条件。此外，我们引入了一种新的分类损失函数，用于导航扩散过程的训练，该损失函数通过两个条件之间的互相依赖关系来引导扩散过程的训练。我们对CDGraph与四种现有的图生成方法，即SPECTRE、GSM、EDGE和DiGress进行评估，结果显示，生成从CDGraph得到的图表在多种社交网络指标中的双重条件有效性和不一致性均较低，这表明CDGraph在生成双重条件的社交图表方面具有极高的效果。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-federated-collaborative-filtering-using-FAIR-Federated-Averaging-in-Random-Subspaces"><a href="#Heterogeneous-federated-collaborative-filtering-using-FAIR-Federated-Averaging-in-Random-Subspaces" class="headerlink" title="Heterogeneous federated collaborative filtering using FAIR: Federated Averaging in Random Subspaces"></a>Heterogeneous federated collaborative filtering using FAIR: Federated Averaging in Random Subspaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01722">http://arxiv.org/abs/2311.01722</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apd10/flcf">https://github.com/apd10/flcf</a></li>
<li>paper_authors: Aditya Desai, Benjamin Meisburger, Zichang Liu, Anshumali Shrivastava</li>
<li>for: 这个论文的目的是推荐系统（RS）的实现，尤其是在面临数据隐私和GDPR等法规的情况下，通过联合学习来实现推荐模型，而不是在中央服务器上训练。</li>
<li>methods: 这个论文使用了联合学习的方法，特别是在 embedding 表格中进行收敛，而不是在中央服务器上进行训练。它使用了哈希基Random projection来实现Device capacity-aware federated averaging，使得各种设备都可以参与训练。</li>
<li>results: 论文通过多个数据集进行实验，证明了 FAIR 可以在各种设备上进行训练，并且可以处理不同设备的数据，以实现在线学习。此外，论文还证明了 FAIR 的整体收敛性。<details>
<summary>Abstract</summary>
Recommendation systems (RS) for items (e.g., movies, books) and ads are widely used to tailor content to users on various internet platforms. Traditionally, recommendation models are trained on a central server. However, due to rising concerns for data privacy and regulations like the GDPR, federated learning is an increasingly popular paradigm in which data never leaves the client device. Applying federated learning to recommendation models is non-trivial due to large embedding tables, which often exceed the memory constraints of most user devices. To include data from all devices in federated learning, we must enable collective training of embedding tables on devices with heterogeneous memory capacities. Current solutions to heterogeneous federated learning can only accommodate a small range of capacities and thus limit the number of devices that can participate in training. We present Federated Averaging in Random subspaces (FAIR), which allows arbitrary compression of embedding tables based on device capacity and ensures the participation of all devices in training. FAIR uses what we call consistent and collapsible subspaces defined by hashing-based random projections to jointly train large embedding tables while using varying amounts of compression on user devices. We evaluate FAIR on Neural Collaborative Filtering tasks with multiple datasets and verify that FAIR can gather and share information from a wide range of devices with varying capacities, allowing for seamless collaboration. We prove the convergence of FAIR in the homogeneous setting with non-i.i.d data distribution. Our code is open source at {https://github.com/apd10/FLCF}
</details>
<details>
<summary>摘要</summary>
（traditional Chinese translation）推荐系统（RS） для Item（例如，电影、书籍）和广告是广泛使用来适应用户在互联网平台上的内容。传统上，推荐模型是在中央服务器上训练的。但由于隐私权和GDPR等法规的问题，联合学习是一种越来越受欢迎的方法，它可以让数据保留在客户端上。将联合学习应用到推荐模型是非常具有挑战，因为推荐模型的嵌入表通常会超过大多数用户端的内存限制。为了包括所有设备的数据在联合学习中，我们必须在设备之间实现嵌入表的集体训练。现有的联合学习解决方案只能涵盖一小段的设备 capacities，因此仅能参与训练的设备有限。我们提出了Federated Averaging in Random subspaces（FAIR），它可以根据设备capacity进行不同程度的压缩，并确保所有设备都可以参与训练。FAIR使用我们称为“一致和可拓展的子空间”的哈希基于随机投射来实现大嵌入表的联合训练。我们使用多个数据集进行评估，并证明FAIR在几何同步设定下的异步数据分布下可以实现收敛。我们的代码可以在<https://github.com/apd10/FLCF> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Generator-Encoder-Adversarial-Networks-with-Latent-Space-Matching-for-Stochastic-Differential-Equations"><a href="#Physics-Informed-Generator-Encoder-Adversarial-Networks-with-Latent-Space-Matching-for-Stochastic-Differential-Equations" class="headerlink" title="Physics-Informed Generator-Encoder Adversarial Networks with Latent Space Matching for Stochastic Differential Equations"></a>Physics-Informed Generator-Encoder Adversarial Networks with Latent Space Matching for Stochastic Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01708">http://arxiv.org/abs/2311.01708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruisong Gao, Min Yang, Jin Zhang</li>
<li>for: 解决随机差分方程中的前进、逆向和混合问题，即系统参数只有有限的快照数据。</li>
<li>methods: 提出了一种新的物理学 Informed Generator-Encoder Adversarial Networks（PIG-EA），通过在各种随机差分方程中直接使用生成器和编码器来解决问题。</li>
<li>results: 经过数学实验证明，PIG-EA方法可以更高精度地解决不同类型的随机差分方程问题，并且可以有效地mitigate训练不稳定性问题。<details>
<summary>Abstract</summary>
We propose a new class of physics-informed neural networks, called Physics-Informed Generator-Encoder Adversarial Networks, to effectively address the challenges posed by forward, inverse, and mixed problems in stochastic differential equations. In these scenarios, while the governing equations are known, the available data consist of only a limited set of snapshots for system parameters. Our model consists of two key components: the generator and the encoder, both updated alternately by gradient descent. In contrast to previous approaches of directly matching the approximated solutions with real snapshots, we employ an indirect matching that operates within the lower-dimensional latent feature space. This method circumvents challenges associated with high-dimensional inputs and complex data distributions, while yielding more accurate solutions compared to existing neural network solvers. In addition, the approach also mitigates the training instability issues encountered in previous adversarial frameworks in an efficient manner. Numerical results provide compelling evidence of the effectiveness of the proposed method in solving different types of stochastic differential equations.
</details>
<details>
<summary>摘要</summary>
我们提出一种新的物理学 Informed Neural Network（Physics-Informed Generator-Encoder Adversarial Network，PIG-ENA），用于有效地解决涉及到前向、反向和混合问题的随机 diferencial equations 中的挑战。在这些情况下，规定方程知道，但可用的数据只是系统参数的有限集。我们的模型包括两个关键组成部分：生成器和编码器，两者都通过梯度下降更新。与之前的直接匹配实际解与真实Snapshot的方法不同，我们采用了间接匹配，该操作在具有较低维度的封闭特征空间中进行。这种方法可以避免高维度输入和复杂数据分布所带来的挑战，同时提供更高精度的解决方案，与现有的神经网络解决方案相比。此外，我们的方法还能有效地解决过去的反对抗框架中的训练不稳定问题。数字实验证明了我们提出的方法在不同类型的随机 diffeq 中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-on-Cooperative-Multi-agent-Bandits"><a href="#Adversarial-Attacks-on-Cooperative-Multi-agent-Bandits" class="headerlink" title="Adversarial Attacks on Cooperative Multi-agent Bandits"></a>Adversarial Attacks on Cooperative Multi-agent Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01698">http://arxiv.org/abs/2311.01698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinhang Zuo, Zhiyao Zhang, Xuchuang Wang, Cheng Chen, Shuai Li, John C. S. Lui, Mohammad Hajiesmaili, Adam Wierman</li>
<li>for: 这个论文研究了合作多体智能机器人在共享多臂抓拍游戏中的潜在漏洞，以及对这些合作的攻击。</li>
<li>methods: 这篇论文使用了对一些代理人进行攻击，以影响其他代理人的决策。具体来说，在同质性设定下，我们提出了一种target arm攻击策略，可以在$T$轮内让所有代理人选择特定的目标臂$T-o(T)$次，而具有$o(T)$攻击成本。在不同质性设定下，我们证明了target臂攻击需要线性攻击成本，并提出了一种可以让最多代理人受到线性悔化的攻击策略。</li>
<li>results: 数值实验证明了我们提出的攻击策略的有效性。<details>
<summary>Abstract</summary>
Cooperative multi-agent multi-armed bandits (CMA2B) consider the collaborative efforts of multiple agents in a shared multi-armed bandit game. We study latent vulnerabilities exposed by this collaboration and consider adversarial attacks on a few agents with the goal of influencing the decisions of the rest. More specifically, we study adversarial attacks on CMA2B in both homogeneous settings, where agents operate with the same arm set, and heterogeneous settings, where agents have distinct arm sets. In the homogeneous setting, we propose attack strategies that, by targeting just one agent, convince all agents to select a particular target arm $T-o(T)$ times while incurring $o(T)$ attack costs in $T$ rounds. In the heterogeneous setting, we prove that a target arm attack requires linear attack costs and propose attack strategies that can force a maximum number of agents to suffer linear regrets while incurring sublinear costs and only manipulating the observations of a few target agents. Numerical experiments validate the effectiveness of our proposed attack strategies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Communication-Efficient-Federated-Non-Linear-Bandit-Optimization"><a href="#Communication-Efficient-Federated-Non-Linear-Bandit-Optimization" class="headerlink" title="Communication-Efficient Federated Non-Linear Bandit Optimization"></a>Communication-Efficient Federated Non-Linear Bandit Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01695">http://arxiv.org/abs/2311.01695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanhao Li, Chong Liu, Yu-Xiang Wang</li>
<li>for: 该论文研究了多客户端协同优化问题，以保持数据隐私和实现大规模计算，并采用中央服务器协调。</li>
<li>methods: 该论文提出了一种新的算法 named Fed-GO-UCB，用于联合非线性目标函数的联合弗链优化。</li>
<li>results: 论文通过了一些强制条件，证明了 Fed-GO-UCB 算法可以实现下线速率的总征领导和通信成本。<details>
<summary>Abstract</summary>
Federated optimization studies the problem of collaborative function optimization among multiple clients (e.g. mobile devices or organizations) under the coordination of a central server. Since the data is collected separately by each client and always remains decentralized, federated optimization preserves data privacy and allows for large-scale computing, which makes it a promising decentralized machine learning paradigm. Though it is often deployed for tasks that are online in nature, e.g., next-word prediction on keyboard apps, most works formulate it as an offline problem. The few exceptions that consider federated bandit optimization are limited to very simplistic function classes, e.g., linear, generalized linear, or non-parametric function class with bounded RKHS norm, which severely hinders its practical usage. In this paper, we propose a new algorithm, named Fed-GO-UCB, for federated bandit optimization with generic non-linear objective function. Under some mild conditions, we rigorously prove that Fed-GO-UCB is able to achieve sub-linear rate for both cumulative regret and communication cost. At the heart of our theoretical analysis are distributed regression oracle and individual confidence set construction, which can be of independent interests. Empirical evaluations also demonstrate the effectiveness of the proposed algorithm.
</details>
<details>
<summary>摘要</summary>
“联邦优化”是一个研究多个客户端（例如移动设备或组织）在中央服务器协调下实现协同函数优化的问题。由于每个客户端都将数据收集和保留在本地，因此“联邦优化”可以保持数据隐私和实现大规模计算，这使其成为一种吸引人的分散式机器学习概念。尽管通常是在线上问题上进行部署，例如键盘应用程序上的下一个词的预测，但大多数研究都是以假设为offline问题进行设计。仅有一些例外情况是考虑联邦投机优化，并且仅对非 Parametric 函数类型进行设计，这严重限制其实际应用。在这篇文章中，我们提出了一个新的算法，名为 Fed-GO-UCB，用于联邦投机优化。我们严谨地证明了 Fed-GO-UCB 能够在一些轻微的条件下 achieves 次线性速率两个总 regret 和通信成本。我们的理论分析的核心是分布式回归资料和个人信任集建构，这些可能会具有独立的价值。实验评估也证明了我们的提案的有效性。
</details></li>
</ul>
<hr>
<h2 id="Amide-Proton-Transfer-APT-imaging-in-tumor-with-a-machine-learning-approach-using-partially-synthetic-data"><a href="#Amide-Proton-Transfer-APT-imaging-in-tumor-with-a-machine-learning-approach-using-partially-synthetic-data" class="headerlink" title="Amide Proton Transfer (APT) imaging in tumor with a machine learning approach using partially synthetic data"></a>Amide Proton Transfer (APT) imaging in tumor with a machine learning approach using partially synthetic data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01683">http://arxiv.org/abs/2311.01683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malvika Viswanathan, Leqi Yin, Yashwant Kurmi, Zhongliang Zu</li>
<li>for: 这项研究旨在使用机器学习（ML）模型量化化学交换吸收转移（CEST）效应。</li>
<li>methods: 这项研究使用了一种新的平台，它将模拟和实验数据结合起来生成部分合成CEST数据，以评估这种方法在训练ML模型预测蛋白质氨基转移（APT）效应方面的可行性。</li>
<li>results: 研究结果表明，使用部分合成CEST数据可以准确地预测APT效应，并且在实验中比使用实际数据和完全模拟数据更为稳定和准确。<details>
<summary>Abstract</summary>
Machine learning (ML) has been increasingly used to quantify chemical exchange saturation transfer (CEST) effect. ML models are typically trained using either measured data or fully simulated data. However, training with measured data often lacks sufficient training data, while training with fully simulated data may introduce bias due to limited simulations pools. This study introduces a new platform that combines simulated and measured components to generate partially synthetic CEST data, and to evaluate its feasibility for training ML models to predict amide proton transfer (APT) effect. Partially synthetic CEST signals were created using an inverse summation of APT effects from simulations and the other components from measurements. Training data were generated by varying APT simulation parameters and applying scaling factors to adjust the measured components, achieving a balance between simulation flexibility and fidelity. First, tissue-mimicking CEST signals along with ground truth information were created using multiple-pool model simulations to validate this method. Second, an ML model was trained individually on partially synthetic data, in vivo data, and fully simulated data, to predict APT effect in rat brains bearing 9L tumors. Experiments on tissue-mimicking data suggest that the ML method using the partially synthetic data is accurate in predicting APT. In vivo experiments suggest that our method provides more accurate and robust prediction than the training using in vivo data and fully synthetic data. Partially synthetic CEST data can address the challenges in conventional ML methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Maximum-Likelihood-Estimation-of-Flexible-Survival-Densities-with-Importance-Sampling"><a href="#Maximum-Likelihood-Estimation-of-Flexible-Survival-Densities-with-Importance-Sampling" class="headerlink" title="Maximum Likelihood Estimation of Flexible Survival Densities with Importance Sampling"></a>Maximum Likelihood Estimation of Flexible Survival Densities with Importance Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01660">http://arxiv.org/abs/2311.01660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mert Ketenci, Shreyas Bhave, Noémie Elhadad, Adler Perotte</li>
<li>for: 这个论文的目的是提出一种新的生存分析方法，以减少实践者需要调整的 гиперparameters数量，包括分类模型中的分配数量和离散模型中的分 Bin 数量。</li>
<li>methods: 该方法使用了一种新的分类模型，以减少模型中的 гиперparameters数量，并且不需要进行优化。它还使用了一种新的离散模型，以提高模型的稳定性和数值稳定性。</li>
<li>results: 研究人员通过实验研究表明，该方法可以与基eline方法匹配或超越其性能，并且可以减少实践者需要调整的时间和努力。<details>
<summary>Abstract</summary>
Survival analysis is a widely-used technique for analyzing time-to-event data in the presence of censoring. In recent years, numerous survival analysis methods have emerged which scale to large datasets and relax traditional assumptions such as proportional hazards. These models, while being performant, are very sensitive to model hyperparameters including: (1) number of bins and bin size for discrete models and (2) number of cluster assignments for mixture-based models. Each of these choices requires extensive tuning by practitioners to achieve optimal performance. In addition, we demonstrate in empirical studies that: (1) optimal bin size may drastically differ based on the metric of interest (e.g., concordance vs brier score), and (2) mixture models may suffer from mode collapse and numerical instability. We propose a survival analysis approach which eliminates the need to tune hyperparameters such as mixture assignments and bin sizes, reducing the burden on practitioners. We show that the proposed approach matches or outperforms baselines on several real-world datasets.
</details>
<details>
<summary>摘要</summary>
In addition, we found in our empirical studies that the optimal bin size can vary significantly depending on the metric of interest (e.g., concordance vs brier score), and mixture models may suffer from mode collapse and numerical instability. To address these issues, we propose a survival analysis approach that eliminates the need to tune hyperparameters such as mixture assignments and bin sizes, reducing the burden on practitioners. We show that our proposed approach matches or outperforms baselines on several real-world datasets.
</details></li>
</ul>
<hr>
<h2 id="Calibrate-and-Boost-Logical-Expressiveness-of-GNN-Over-Multi-Relational-and-Temporal-Graphs"><a href="#Calibrate-and-Boost-Logical-Expressiveness-of-GNN-Over-Multi-Relational-and-Temporal-Graphs" class="headerlink" title="Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational and Temporal Graphs"></a>Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational and Temporal Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01647">http://arxiv.org/abs/2311.01647</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hdmmblz/multi-graph">https://github.com/hdmmblz/multi-graph</a></li>
<li>paper_authors: Yeyuan Chen, Dingmin Wang</li>
<li>for: 这个论文主要研究了图像学习框架Graph Neural Networks (GNNs)的逻辑表达能力，具体来说是在多关系图上使用GNNs来表示Boolean node classifiers。</li>
<li>methods: 作者使用了R$^2$-GNN架构，该架构是通过将本地消息传递GNN扩展到全球读取来提高表达能力。</li>
<li>results: 研究发现，R$^2$-GNN模型在某些限制性的情况下可以完全表达Boolean node classifiers，但在总体情况下则不行。作者也提出了一种简单的图变换技术，可以在线性时间内执行，以便使R$^2$-GNN模型能够有效地表达任何Boolean node classifiers。<details>
<summary>Abstract</summary>
As a powerful framework for graph representation learning, Graph Neural Networks (GNNs) have garnered significant attention in recent years. However, to the best of our knowledge, there has been no formal analysis of the logical expressiveness of GNNs as Boolean node classifiers over multi-relational graphs, where each edge carries a specific relation type. In this paper, we investigate $\mathcal{FOC}_2$, a fragment of first-order logic with two variables and counting quantifiers. On the negative side, we demonstrate that the R$^2$-GNN architecture, which extends the local message passing GNN by incorporating global readout, fails to capture $\mathcal{FOC}_2$ classifiers in the general case. Nevertheless, on the positive side, we establish that R$^2$-GNNs models are equivalent to $\mathcal{FOC}_2$ classifiers under certain restricted yet reasonable scenarios. To address the limitations of R$^2$-GNNs regarding expressiveness, we propose a simple graph transformation technique, akin to a preprocessing step, which can be executed in linear time. This transformation enables R$^2$-GNNs to effectively capture any $\mathcal{FOC}_2$ classifiers when applied to the "transformed" input graph. Moreover, we extend our analysis of expressiveness and graph transformation to temporal graphs, exploring several temporal GNN architectures and providing an expressiveness hierarchy for them. To validate our findings, we implement R$^2$-GNNs and the graph transformation technique and conduct empirical tests in node classification tasks against various well-known GNN architectures that support multi-relational or temporal graphs. Our experimental results consistently demonstrate that R$^2$-GNN with the graph transformation outperforms the baseline methods on both synthetic and real-world datasets
</details>
<details>
<summary>摘要</summary>
“graph neural networks（GNNs）是一种强大的图表示学习框架，在过去几年内吸引了广泛的关注。然而，据我们所知，对于多关系图上的Boolean节点分类问题，GNNs的逻辑表达能力没有正式的分析。在这篇论文中，我们 investigate $\mathcal{FOC}_2$，一种first-order logic中的两个变量和计数量论 fragment。我们的负面结果表明，R$^2$-GNN架构，通过扩展本地消息传递GNN，不能在总体情况下捕捉 $\mathcal{FOC}_2$ 分类器。然而，我们的积极结果表明，R$^2$-GNN模型与 $\mathcal{FOC}_2$ 分类器在某些有限但合理的情况下是等价的。为了解决R$^2$-GNN的表达能力的局限性，我们提出了一种简单的图变换技术，可以在线性时间内执行。这种变换可以使R$^2$-GNN模型有效地捕捉任何 $\mathcal{FOC}_2$ 分类器。此外，我们还扩展了我们的表达能力和图变换分析到temporal graph，探讨了多种temporal GNN架构，并提出了temporal GNN的表达能力层次。为了证明我们的发现，我们实现了R$^2$-GNN和图变换技术，并在节点分类任务中进行了实验，与多种支持多关系或temporal graph的GNNA architectures进行比较。我们的实验结果表明，R$^2$-GNN与图变换技术在多种synthetic和实际数据集上均有优于基eline方法”
</details></li>
</ul>
<hr>
<h2 id="Should-Under-parameterized-Student-Networks-Copy-or-Average-Teacher-Weights"><a href="#Should-Under-parameterized-Student-Networks-Copy-or-Average-Teacher-Weights" class="headerlink" title="Should Under-parameterized Student Networks Copy or Average Teacher Weights?"></a>Should Under-parameterized Student Networks Copy or Average Teacher Weights?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01644">http://arxiv.org/abs/2311.01644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Berfin Şimşek, Amire Bendjeddou, Wulfram Gerstner, Johanni Brea</li>
<li>for: 本文研究的目标是研究一种具有较少神经元数的神经网络如何对一个具有较多神经元数的神经网络进行近似。</li>
<li>methods: 本文使用了神经网络的拟合方法来研究这个问题，并证明了在某些情况下，使用较少神经元数的神经网络可以对一个具有较多神经元数的神经网络进行高度的近似。</li>
<li>results: 本文的结果表明，在使用 erf 活化函数和标准正态输入分布的情况下，较少神经元数的神经网络可以达到最佳的近似效果，并且这个效果可以通过调整每个学生神经元是否复制教师神经元来实现。<details>
<summary>Abstract</summary>
Any continuous function $f^*$ can be approximated arbitrarily well by a neural network with sufficiently many neurons $k$. We consider the case when $f^*$ itself is a neural network with one hidden layer and $k$ neurons. Approximating $f^*$ with a neural network with $n< k$ neurons can thus be seen as fitting an under-parameterized "student" network with $n$ neurons to a "teacher" network with $k$ neurons. As the student has fewer neurons than the teacher, it is unclear, whether each of the $n$ student neurons should copy one of the teacher neurons or rather average a group of teacher neurons. For shallow neural networks with erf activation function and for the standard Gaussian input distribution, we prove that "copy-average" configurations are critical points if the teacher's incoming vectors are orthonormal and its outgoing weights are unitary. Moreover, the optimum among such configurations is reached when $n-1$ student neurons each copy one teacher neuron and the $n$-th student neuron averages the remaining $k-n+1$ teacher neurons. For the student network with $n=1$ neuron, we provide additionally a closed-form solution of the non-trivial critical point(s) for commonly used activation functions through solving an equivalent constrained optimization problem. Empirically, we find for the erf activation function that gradient flow converges either to the optimal copy-average critical point or to another point where each student neuron approximately copies a different teacher neuron. Finally, we find similar results for the ReLU activation function, suggesting that the optimal solution of underparameterized networks has a universal structure.
</details>
<details>
<summary>摘要</summary>
任何连续函数 $f^{*}$ 都可以被 sufficiently many neurons $k$ 的神经网络 arbitrarily well 近似。我们考虑 $f^{*}$ 本身是一个具有一个隐藏层和 $k$ 个神经元的神经网络。对 $f^{*}$ 使用一个具有 $n<k$ 个神经元的神经网络可以看作是将一个 under-parameterized "学生" 网络适应一个 "老师" 网络。由于学生网络有 fewer neurons than the teacher, 这使得每个学生神经元应该是 copy 一个老师神经元还是 average 一群老师神经元。对于 shallow neural networks with erf activation function 和 standard Gaussian input distribution，我们证明了 "copy-average" 配置是 critical points ，假设老师的 incoming vectors 是正交的和 its outgoing weights 是单位的。此外，该配置的最佳为 $n-1$ 学生神经元各copy一个老师神经元，并 $n$-th 学生神经元 average 余下 $k-n+1$ 老师神经元。对学生网络的 $n=1$ 神经元，我们还提供了一个关键的对应的 constrained optimization problem 的关键解。实验结果显示，使用 erf activation function 时，演化流程会落在最佳 copy-average critical point 或另一个点，每个学生神经元接近一个不同的老师神经元。另外，我们还发现使用 ReLU activation function 时，最佳解也有一个通用结构。
</details></li>
</ul>
<hr>
<h2 id="Robust-Adversarial-Reinforcement-Learning-via-Bounded-Rationality-Curricula"><a href="#Robust-Adversarial-Reinforcement-Learning-via-Bounded-Rationality-Curricula" class="headerlink" title="Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula"></a>Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01642">http://arxiv.org/abs/2311.01642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aryaman Reddi, Maximilian Tölle, Jan Peters, Georgia Chalvatzaki, Carlo D’Eramo</li>
<li>for: 本文目标是提高对抗攻击和分布变化的Robustness Reinforcement Learning（RL）。</li>
<li>methods: 本文提出了一种基于Entropy regularization的新方法，该方法可以简化了极点优化问题。</li>
<li>results: 对多个MuJoCo游戏和导航问题进行了广泛的实验，并证明了QARL在总性和Robustness方面超过了RARL和最近的基准值。<details>
<summary>Abstract</summary>
Robustness against adversarial attacks and distribution shifts is a long-standing goal of Reinforcement Learning (RL). To this end, Robust Adversarial Reinforcement Learning (RARL) trains a protagonist against destabilizing forces exercised by an adversary in a competitive zero-sum Markov game, whose optimal solution, i.e., rational strategy, corresponds to a Nash equilibrium. However, finding Nash equilibria requires facing complex saddle point optimization problems, which can be prohibitive to solve, especially for high-dimensional control. In this paper, we propose a novel approach for adversarial RL based on entropy regularization to ease the complexity of the saddle point optimization problem. We show that the solution of this entropy-regularized problem corresponds to a Quantal Response Equilibrium (QRE), a generalization of Nash equilibria that accounts for bounded rationality, i.e., agents sometimes play random actions instead of optimal ones. Crucially, the connection between the entropy-regularized objective and QRE enables free modulation of the rationality of the agents by simply tuning the temperature coefficient. We leverage this insight to propose our novel algorithm, Quantal Adversarial RL (QARL), which gradually increases the rationality of the adversary in a curriculum fashion until it is fully rational, easing the complexity of the optimization problem while retaining robustness. We provide extensive evidence of QARL outperforming RARL and recent baselines across several MuJoCo locomotion and navigation problems in overall performance and robustness.
</details>
<details>
<summary>摘要</summary>
RL中的稳定性对抗难以控制的攻击和分布变化是一个长期目标。为此，我们提出了Robust Adversarial Reinforcement Learning（RARL），它在竞争性零和游戏中训练一个主角，以适应由敌方所予以的破坏性力量。然而，在找到约束 Nash 平衡点的过程中，可能会遇到复杂的锐点优化问题，特别是在高维控制下。在这篇论文中，我们提出了一种基于 entropy 规范的新方法，以缓解锐点优化问题的复杂性。我们证明了这种 entropy-regularized 目标函数对应于一种Quantal Response Equilibrium（QRE），这是约束 Nash 平衡点的一种扩展，考虑到 bounded rationality，即代理人可能会采取Random 动作而不是最优动作。这种关系允许我们通过调整温度系数来自由地调节代理人的合理性。我们利用这一点，提出了我们的新算法Quantal Adversarial RL（QARL），它逐渐增加了敌方的合理性，直到它完全合理，从而缓解优化问题的复杂性。我们在多个 MuJoCo 步行和导航问题上提供了广泛的证明，证明 QARL 在总性和稳定性方面超过 RARL 和最新的基elines。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/03/cs.LG_2023_11_03/" data-id="closbrosa00sf0g88aml31jnj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/03/eess.IV_2023_11_03/" class="article-date">
  <time datetime="2023-11-03T09:00:00.000Z" itemprop="datePublished">2023-11-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/03/eess.IV_2023_11_03/">eess.IV - 2023-11-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Quantitative-Evaluation-of-a-Multi-Modal-Camera-Setup-for-Fusing-Event-Data-with-RGB-Images"><a href="#Quantitative-Evaluation-of-a-Multi-Modal-Camera-Setup-for-Fusing-Event-Data-with-RGB-Images" class="headerlink" title="Quantitative Evaluation of a Multi-Modal Camera Setup for Fusing Event Data with RGB Images"></a>Quantitative Evaluation of a Multi-Modal Camera Setup for Fusing Event Data with RGB Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01881">http://arxiv.org/abs/2311.01881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Moosmann, Jakub Mandula, Philipp Mayer, Luca Benini, Michele Magno</li>
<li>for: 这个论文的目的是提出一种多模式摄像头设置，用于将高分辨率DVS数据与RGB图像数据进行融合，以便使用两种技术 simultaneously。</li>
<li>methods: 这个论文使用了几种时间基于的同步方法来帮助将DVS数据与RGB图像数据进行对应，并进行了相关的Camera alignment和镜头影响的分析。</li>
<li>results: 实验结果表明，提出的系统具有较低的图像校准误差（less than 0.90px）和像素十分之偏差（1.6px），而使用8毫米 focal length镜头可以检测到距离350米的30厘米大小的 объекts against homogeneous background。<details>
<summary>Abstract</summary>
Event-based cameras, also called silicon retinas, potentially revolutionize computer vision by detecting and reporting significant changes in intensity asynchronous events, offering extended dynamic range, low latency, and low power consumption, enabling a wide range of applications from autonomous driving to longtime surveillance. As an emerging technology, there is a notable scarcity of publicly available datasets for event-based systems that also feature frame-based cameras, in order to exploit the benefits of both technologies. This work quantitatively evaluates a multi-modal camera setup for fusing high-resolution DVS data with RGB image data by static camera alignment. The proposed setup, which is intended for semi-automatic DVS data labeling, combines two recently released Prophesee EVK4 DVS cameras and one global shutter XIMEA MQ022CG-CM RGB camera. After alignment, state-of-the-art object detection or segmentation networks label the image data by mapping boundary boxes or labeled pixels directly to the aligned events. To facilitate this process, various time-based synchronization methods for DVS data are analyzed, and calibration accuracy, camera alignment, and lens impact are evaluated. Experimental results demonstrate the benefits of the proposed system: the best synchronization method yields an image calibration error of less than 0.90px and a pixel cross-correlation deviation of1.6px, while a lens with 8mm focal length enables detection of objects with size 30cm at a distance of 350m against homogeneous background.
</details>
<details>
<summary>摘要</summary>
Event-based 摄像头，也称为silicon retina，有 potential 革命化计算机视觉，因为它可以检测和报告快速变化的强度 asynchronous 事件，提供扩展的动态范围，低延迟，和低功耗，因此可以应用于自动驾驶到长期监测等多种应用。作为新兴技术，公共可用的 dataset  для event-based 系统和 frame-based 摄像头的混合还是罕见的。本研究使用多模式摄像头设置，将高分辨率 DVS 数据与 RGB 图像数据混合，并通过静态摄像头对齐来实现。这种设置是为 semi-automatic DVS 数据标注而设计，使用两个最新发布的 Prophesee EVK4 DVS 摄像头和一个全球闭环 XIMEA MQ022CG-CM RGB 摄像头。在对齐后，使用现状的对象检测或分割网络将图像数据标注为对齐事件。为此，我们分析了多种时间基准的同步方法，并评估了相机对齐精度、镜头影响和摄像头对齐精度。实验结果表明，我们的方法具有优秀的效果：最佳同步方法的图像准确性错误低于0.90px，像素十分之偏移低于1.6px，而8mm focal length 镜头可以检测到30cm大小的 объек的到达350m 距离。
</details></li>
</ul>
<hr>
<h2 id="3-Dimensional-residual-neural-architecture-search-for-ultrasonic-defect-detection"><a href="#3-Dimensional-residual-neural-architecture-search-for-ultrasonic-defect-detection" class="headerlink" title="3-Dimensional residual neural architecture search for ultrasonic defect detection"></a>3-Dimensional residual neural architecture search for ultrasonic defect detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01867">http://arxiv.org/abs/2311.01867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaun McKnight, Christopher MacKinnon, S. Gareth Pierce, Ehsan Mohseni, Vedran Tunukovic, Charles N. MacLeod, Randika K. W. Vithanage, Tom OHare</li>
<li>for: 这种研究使用深度学习方法检测碳纤维复合材料中的缺陷，通过用三维卷积神经网络处理三维超声测试数据。</li>
<li>methods: 这种方法使用了一种新的数据生成方法，通过保留完整的三维数据，使得复杂的预处理步骤减少，神经网络可以利用空间和时间信息，提高模型的性能。</li>
<li>results: 研究 comparing三种体制，包括一种自定义的卷积神经网络，一种使用立方体卷积神经网络，以及一种通过神经网络搜索生成的三维差异神经网络。结果显示，使用全连接层进行维度减少，比使用最大池化层更高的性能。此外，在训练时添加域特性增强方法，也有显著提高模型性能的效果。<details>
<summary>Abstract</summary>
This study presents a deep learning methodology using 3-dimensional (3D) convolutional neural networks to detect defects in carbon fiber reinforced polymer composites through volumetric ultrasonic testing data. Acquiring large amounts of ultrasonic training data experimentally is expensive and time-consuming. To address this issue, a synthetic data generation method was extended to incorporate volumetric data. By preserving the complete volumetric data, complex preprocessing is reduced, and the model can utilize spatial and temporal information that is lost during imaging. This enables the model to utilise important features that might be overlooked otherwise. The performance of three architectures were compared. The first two architectures were hand-designed to address the high aspect ratios between the spatial and temporal dimensions. The first architecture reduced dimensionality in the time domain and used cubed kernels for feature extraction. The second architecture used cuboidal kernels to account for the large aspect ratios. The evaluation included comparing the use of max pooling and convolutional layers for dimensionality reduction, with the fully convolutional layers consistently outperforming the models using max pooling. The third architecture was generated through neural architecture search from a modified 3D Residual Neural Network (ResNet) search space. Additionally, domain-specific augmentation methods were incorporated during training, resulting in significant improvements in model performance for all architectures. The mean accuracy improvements ranged from 8.2% to 22.4%. The best performing models achieved mean accuracies of 91.8%, 92.2%, and 100% for the reduction, constant, and discovered architectures, respectively. Whilst maintaining a model size smaller than most 2-dimensional (2D) ResNets.
</details>
<details>
<summary>摘要</summary>
Three architecture designs were compared: the first two were hand-designed to address high aspect ratios between spatial and temporal dimensions. The first architecture reduced dimensionality in the time domain using cubed kernels for feature extraction, while the second architecture used cuboidal kernels to account for large aspect ratios. The third architecture was generated through neural architecture search from a modified 3D Residual Neural Network (ResNet) search space.During training, domain-specific augmentation methods were incorporated, resulting in significant improvements in model performance for all architectures. The mean accuracy improvements ranged from 8.2% to 22.4%. The best-performing models achieved mean accuracies of 91.8%, 92.2%, and 100% for the reduction, constant, and discovered architectures, respectively, while maintaining a model size smaller than most 2D ResNets.
</details></li>
</ul>
<hr>
<h2 id="Neural-SPDE-solver-for-uncertainty-quantification-in-high-dimensional-space-time-dynamics"><a href="#Neural-SPDE-solver-for-uncertainty-quantification-in-high-dimensional-space-time-dynamics" class="headerlink" title="Neural SPDE solver for uncertainty quantification in high-dimensional space-time dynamics"></a>Neural SPDE solver for uncertainty quantification in high-dimensional space-time dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01783">http://arxiv.org/abs/2311.01783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Beauchamp, Ronan Fablet, Hugo Georgenthum</li>
<li>for: 这篇论文目的是对大型地球物理数据进行插值和资料融合。</li>
<li>methods: 这篇论文使用了Stochastic Partial Differential Equations（SPDE）和 Gaussian Markov Random Fields（GMRF）来处理大数据，并使用了简短精度矩阵来实现插值。</li>
<li>results: 这篇论文的解法提高了Optimal Interpolation（OI）的基eline，并能够quantify the associated uncertainties。它还能够与神经网络结合，实现资料融合和线上参数估测。<details>
<summary>Abstract</summary>
Historically, the interpolation of large geophysical datasets has been tackled using methods like Optimal Interpolation (OI) or model-based data assimilation schemes. However, the recent connection between Stochastic Partial Differential Equations (SPDE) and Gaussian Markov Random Fields (GMRF) introduced a novel approach to handle large datasets making use of sparse precision matrices in OI. Recent advancements in deep learning also addressed this issue by incorporating data assimilation into neural architectures: it treats the reconstruction task as a joint learning problem involving both prior model and solver as neural networks. Though, it requires further developments to quantify the associated uncertainties. In our work, we leverage SPDEbased Gaussian Processes to estimate complex prior models capable of handling nonstationary covariances in space and time. We develop a specific architecture able to learn both state and SPDE parameters as a neural SPDE solver, while providing the precisionbased analytical form of the SPDE sampling. The latter is used as a surrogate model along the data assimilation window. Because the prior is stochastic, we can easily draw samples from it and condition the members by our neural solver, allowing flexible estimation of the posterior distribution based on large ensemble. We demonstrate this framework on realistic Sea Surface Height datasets. Our solution improves the OI baseline, aligns with neural prior while enabling uncertainty quantification and online parameter estimation.
</details>
<details>
<summary>摘要</summary>
En el pasado, la interpolación de grandes conjuntos de datos geofísicos se ha abordado utilizando métodos como Interpolación Óptima (OI) o esquemas de asimilación de datos basados en modelos. Sin embargo, la reciente conexión entre Ecuaciones Parciales Diferenciales Estocásticas (SPDE) y Campos de Markov Gaussianos (GMRF) presentó una nueva aproximación para manejar grandes conjuntos de datos utilizando matrices de precisión esparcas en OI. Los avances recientes en aprendizaje profundo también abordaron este problema al incorporar la asimilación de datos en arquitecturas neurales: se trata la tarea de reconstrucción como un problema de aprendizaje conjunto que involucra tanto el modelo previo como el solver como redes neuronales. Aunque requiere desarrollos adicionales para cuantificar las incertidumbres asociadas. En nuestro trabajo, utilizamos Procesos de Gaussianas Basadas en SPDE para estimar modelos priorizados complejos capaces de manejar covarianzas no estacionarias en el espacio y el tiempo. Desarrollamos una arquitectura específica que aprende tanto los parámetros del estado como los parámetros de SPDE como un solucionador neural SPDE, mientras proporciona la forma analítica de la SPDE sampling. La última se utiliza como un modelo de surrogato a lo largo de la ventana de asimilación de datos. Como el prior es estocástico, podemos fácilmente extraer muestras de él y condicionarlos con nuestro solucionador neural, lo que permite una estimación flexible de la distribución posterior en función de un gran ensamble. Demostramos este marco en conjuntos de datos de Altura de la Surface del Mar realistas. Nuestra solución mejora el umbral de OI, se alinea con el prior neural y permite la cuantificación de incertidumbres y la estimación en línea de parámetros.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/03/eess.IV_2023_11_03/" data-id="closbroz9019b0g88bw9j39l0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/03/eess.SP_2023_11_03/" class="article-date">
  <time datetime="2023-11-03T08:00:00.000Z" itemprop="datePublished">2023-11-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/03/eess.SP_2023_11_03/">eess.SP - 2023-11-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="HPC-based-Solvers-of-Minimisation-Problems-for-Signal-Processing"><a href="#HPC-based-Solvers-of-Minimisation-Problems-for-Signal-Processing" class="headerlink" title="HPC-based Solvers of Minimisation Problems for Signal Processing"></a>HPC-based Solvers of Minimisation Problems for Signal Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02039">http://arxiv.org/abs/2311.02039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Cammarasana, Giuseppe Patanè</li>
<li>for:  solves two minimization problems (approximation and denoising) with different constraints using high-performance computing.</li>
<li>methods:  compares and analyzes different minimization methods in terms of functional computation, convergence, execution time, and scalability properties.</li>
<li>results:  PRAXIS is the best optimizer in terms of minima computation, with an efficiency of 38% for approximation and 46% for denoising.Here is the full text in Simplified Chinese:</li>
<li>for: 本研究 solves two minimization problems (approximation和denoising) with different constraints using high-performance computing.</li>
<li>methods:  Compares and analyzes different minimization methods in terms of functional computation, convergence, execution time, and scalability properties.</li>
<li>results:  PRAXIS is the best optimizer in terms of minima computation, with an efficiency of 38% for approximation and 46% for denoising.<details>
<summary>Abstract</summary>
Several physics and engineering applications involve the solution of a minimisation problem to compute an approximation of the input signal. Modern computing hardware and software apply high-performance computing to solve and considerably reduce the execution time. We compare and analyse different minimisation methods in terms of functional computation, convergence, execution time, and scalability properties, for the solution of two minimisation problems (i.e., approximation and denoising) with different constraints that involve computationally expensive operations. These problems are attractive due to their numerical and analytical properties, and our general analysis can be extended to most signal-processing problems. We perform our tests on the Cineca Marconi100 cluster, at the 26th position in the top500 list. Our experimental results show that PRAXIS is the best optimiser in terms of minima computation: the efficiency of the approximation is 38% with 256 processes, while the denoising has 46% with 32 processes.
</details>
<details>
<summary>摘要</summary>
许多物理和工程应用中需要解决最小化问题以计算输入信号的近似值。现代计算硬件和软件通过高性能计算解决这些问题，大大减少了执行时间。我们对不同的最小化方法进行比较和分析，考虑其函数计算、收敛、执行时间和可扩展性特性，以解决两个最小化问题（即近似和净化），它们具有不同的计算约束。这些问题具有数字和分析性质，我们的总分析可以扩展到大多数信号处理问题。我们在Cineca Marconi100集群上进行测试，该集群在top500列表中排名第26位。我们的实验结果显示，PRAXIS是最佳优化器，在256个进程中，近似计算效率为38%，而净化计算效率为46%。
</details></li>
</ul>
<hr>
<h2 id="Terahertz-Communication-Testbeds-Challenges-and-Opportunities"><a href="#Terahertz-Communication-Testbeds-Challenges-and-Opportunities" class="headerlink" title="Terahertz Communication Testbeds: Challenges and Opportunities"></a>Terahertz Communication Testbeds: Challenges and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01972">http://arxiv.org/abs/2311.01972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eray Guven, Gunes Karabulut Kurt</li>
<li>for: 这项研究探讨了一个实验性的 Software Defined Radio（SDR）实现在180 GHz频率域。</li>
<li>methods: 本研究使用了硬件瓶颈问题的率缺乏和频率缺乏来解释实验困难，并提出了相应的系统模型。</li>
<li>results: 该SDR-THz测试平台可以达到3.2 Mbps的传输速率，并且可以通过使用反射板来精细调整频率错误和偏斜错误，但需要至少14.91 dB的信号噪声比。结果表明SDR基带信号生成在THz通信中完全可行，并且开 up了许多机会来超越实验研究中的硬件限制。<details>
<summary>Abstract</summary>
This study investigates an experimental software defined radio (SDR) implementation on 180 GHz. Rate scarcity and frequency sparsity are discussed as hardware bottlenecks. Experimental challenges are explained along with the derived system model of such a cascaded structure. Multiple error metrics for the terahertz (THz) signal are acquired, and various case scenarios are subsequently compared. The SDR-THz testbed reaches 3.2 Mbps with < 1 degree skew error. The use of a reflector plate can fine-tune the frequency error and gain imbalance in the expense of at least 14.91 dB signal-to-noise ratio. The results demonstrate the complete feasibility of SDR-based baseband signal generation in THz communication, revealing abundant opportunities to overcome hardware limitations in experimental research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reconfigurable-Intelligent-Surface-Edge-–-An-Introduction-of-an-EM-manipulation-structure-on-obstacles’-edge"><a href="#Reconfigurable-Intelligent-Surface-Edge-–-An-Introduction-of-an-EM-manipulation-structure-on-obstacles’-edge" class="headerlink" title="Reconfigurable Intelligent Surface &amp; Edge – An Introduction of an EM manipulation structure on obstacles’ edge"></a>Reconfigurable Intelligent Surface &amp; Edge – An Introduction of an EM manipulation structure on obstacles’ edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01919">http://arxiv.org/abs/2311.01919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianqi Xiang, Zhiwei Jiang, Weijun Hong, Xin Zhang, Yuehong Gao</li>
<li>for: 提高遮挡区域信号覆盖性能</li>
<li>methods: 使用各种部署位置和电磁振荡结构设计来改变电磁环境</li>
<li>results: 在不同场景下，提出了一种新的电磁振荡结构，可以在障碍物边缘进行静态电磁环境修改，并且在不同场景下实现更好的覆盖性能。<details>
<summary>Abstract</summary>
Reconfigurable Intelligent Surface (RIS) or metasurface is one of the important enabling technologies in mobile cellular networks that can effectively enhance the signal coverage performance in obstructed regions, and it is generally deployed on surfaces different from obstacles to redirect electromagnetic (EM) waves by reflection, or covered on objects' surfaces to manipulate EM waves by refraction. In this paper, Reconfigurable Intelligent Surface & Edge (RISE) is proposed to extend RIS' abilities of reflection and refraction over surfaces to diffraction around obstacles' edge for better adaptation to specific coverage scenarios. Based on that, this paper analyzes the performance of several different deployment locations and EM manipulation structure designs for different coverage scenarios. Then a novel EM manipulation structure deployed at the obstacles' edge is proposed to achieve static EM environment modification. Simulations validate the preference of the schemes for different scenarios and the new structure achieves better coverage performance than other typical structures in the static scheme.
</details>
<details>
<summary>摘要</summary>
reh-kon-fig-yur-uh-ble in-tel-li-jent sur-fis (RIS) or meh-tah-sur-fis is one of the important en-abling tech-nolo-gies in mo-bile cel-lu-lar net-works that can ef-fect-ively en-hance the sig-nal cov-er-age per-for-mance in ob-structed re-gions, and it is gen-er-ally de-ployed on sur-faces dif-fer-ent from ob-stacles to re-dir-ect EM waves by re-flec-tion, or cov-ered on ob-jects' sur-faces to man-i-pulate EM waves by re-frac-tion. In this pa-per, Re-config-urable In-tel-li-gent Sur-face & Edge (RISE) is pro-posed to ex-tend RIS' abil-i-ties of re-flec-tion and re-frac-tion over sur-faces to dif-fraction around ob-stacles' edge for bet-ter ad-ap-tion to spec-i-fic cov-er-age scena-rios. Based on that, this pa-per an-a-lyzes the per-for-mance of se-ver-al dif-fer-ent de-ploy-ment lo-ca-tions and EM man-i-pu-la-tion struc-ture de-signs for dif-fer-ent cov-er-age scena-rios. Then a no-vel EM man-i-pu-la-tion struc-ture de-ployed at the ob-stacles' edge is pro-posed to achieve stat-ic EM en-vi-ron-ment mod-i-fi-ca-tion. Sim-u-la-tions val-id-ate the pref-er-ence of the schemes for dif-fer-ent scena-rios and the new struc-ture achieves bet-ter cov-er-age per-for-mance than other typ-i-cal struc-tures in the stat-ic scheme.
</details></li>
</ul>
<hr>
<h2 id="Random-ISAC-Signals-Deserve-Dedicated-Precoding"><a href="#Random-ISAC-Signals-Deserve-Dedicated-Precoding" class="headerlink" title="Random ISAC Signals Deserve Dedicated Precoding"></a>Random ISAC Signals Deserve Dedicated Precoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01822">http://arxiv.org/abs/2311.01822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shihang Lu, Fan Liu, Fuwang Dong, Yifeng Xiong, Jie Xu, Ya-Feng Liu, Shi Jin</li>
<li>for: 本文研究了使用随机信号在多天线系统中进行目标探测和通信系统的性能分析。</li>
<li>methods: 本文提出了一种新的感知性能指标——随机信号平均线性最小均方误差（ELMMSE），用于评估随机ISAC信号在探测场景下的性能。此外，本文还提出了一种数据依赖 precoding（DDP）算法和一种数据独立 precoding（DIP）算法，用于优化随机ISAC信号的探测性能。</li>
<li>results: 本文的数据分析表明，DDP和DIP方法可以大幅提高ISAC信号中的探测性能，而且这些方法可以适应不同的随机信号模式。此外，本文还提出了一种可靠的算法来解决ISAC信号中的随机信号设计问题。<details>
<summary>Abstract</summary>
Radar systems typically employ well-designed deterministic signals for target sensing, while integrated sensing and communications (ISAC) systems have to adopt random signals to convey useful information. This paper analyzes the sensing and ISAC performance relying on random signaling in a multiantenna system. Towards this end, we define a new sensing performance metric, namely, ergodic linear minimum mean square error (ELMMSE), which characterizes the estimation error averaged over random ISAC signals. Then, we investigate a data-dependent precoding (DDP) scheme to minimize the ELMMSE in sensing-only scenarios, which attains the optimized performance at the cost of high implementation overhead. To reduce the cost, we present an alternative data-independent precoding (DIP) scheme by stochastic gradient projection (SGP). Moreover, we shed light on the optimal structures of both sensing-only DDP and DIP precoders. As a further step, we extend the proposed DDP and DIP approaches to ISAC scenarios, which are solved via a tailored penalty-based alternating optimization algorithm. Our numerical results demonstrate that the proposed DDP and DIP methods achieve substantial performance gains over conventional ISAC signaling schemes that treat the signal sample covariance matrix as deterministic, which proves that random ISAC signals deserve dedicated precoding designs.
</details>
<details>
<summary>摘要</summary>
射频系统通常使用良好设计的决定性信号进行目标探测，而统合探测通信（ISAC）系统则必须运用随机信号传递有用信息。本文分析了使用随机信号的探测和ISAC性能，并定义了一个新的探测性能指标，即随机线性最小平均方差（ELMMSE），用于描述随机ISAC信号中的估计误差。然后，我们调查了一个基于数据依赖的实现（DDP）策略，以减少ELMMSE的值，并提出了一个基于随机投影（SGP）的替代策略。此外，我们还考虑了两种探测只的DDP和DIP预设的结构。最后，我们将这些方法扩展到ISAC场景，这些问题透过一个特别设计的罚则基于交互运算法解决。我们的数据分析结果显示，提案的DDP和DIP方法可以与传统ISAC信号传递方案相比，实现substantial的性能提升，这证明了随机ISAC信号值得特别的预设设计。
</details></li>
</ul>
<hr>
<h2 id="Carrier-Frequency-Offset-Estimation-for-OCDM-with-Null-Subchirps"><a href="#Carrier-Frequency-Offset-Estimation-for-OCDM-with-Null-Subchirps" class="headerlink" title="Carrier Frequency Offset Estimation for OCDM with Null Subchirps"></a>Carrier Frequency Offset Estimation for OCDM with Null Subchirps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01812">http://arxiv.org/abs/2311.01812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sidong Guo, Yiyin Wang, Xiaoli Ma</li>
<li>for:  investigate the carrier frequency offset (CFO) identifiability problem in orthogonal chirp division multiplexing (OCDM) systems.</li>
<li>methods:  propose a transmission scheme by inserting consecutive null subchirps, and develop a CFO estimator to achieve a full acquisition range.</li>
<li>results:  demonstrate that the proposed transmission scheme not only helps to resolve CFO identifiability issues but also enables multipath diversity for OCDM systems, and simulation results corroborate the theoretical findings.Here’s the full translation in Simplified Chinese:</li>
<li>for: 这篇论文是 investigate orthogonal chirp division multiplexing (OCDM) 系统中的载波频率偏移 (CFO) 可识别性问题。</li>
<li>methods: 提议在 OCDM 系统中插入连续的null subchirp transmission scheme，并开发一种CFO估计器以实现全覆盖范围。</li>
<li>results: 表明提议的传输方案不仅能够解决 CFO 可识别性问题，还能够为 OCDM 系统带来多Path 多普适性。 simulation results 证明了理论发现。<details>
<summary>Abstract</summary>
In this paper, we investigate the carrier frequency offset (CFO) identifiability problem in orthogonal chirp division multiplexing (OCDM) systems. We propose a transmission scheme by inserting consecutive null subchirps. A CFO estimator is accordingly developed to achieve a full acquisition range.   We further demonstrate that the proposed transmission scheme not only help to resolve CFO identifiability issues but also enable multipath diversity for OCDM systems. Simulation results corroborate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了扩展幂分多路复用（OCDM）系统中的载波频率偏移（CFO）可识别问题。我们提议了插入连续null子频谱的传输方案，并开发了一种CFO估计器以实现全范围的获得。我们还证明了我们的传输方案不仅能够解决CFO可识别问题，还能够启用OCDM系统中的多路幂资源。实验结果与我们的理论发现相符。Here's the word-for-word translation:在这篇论文中，我们研究了扩展幂分多路复用（OCDM）系统中的载波频率偏移（CFO）可识别问题。我们提议了插入连续null子频谱的传输方案，并开发了一种CFO估计器以实现全范围的获得。我们还证明了我们的传输方案不仅能够解决CFO可识别问题，还能够启用OCDM系统中的多路幂资源。实验结果与我们的理论发现相符。
</details></li>
</ul>
<hr>
<h2 id="Moving-Target-Sensing-for-ISAC-Systems-in-Clutter-Environment"><a href="#Moving-Target-Sensing-for-ISAC-Systems-in-Clutter-Environment" class="headerlink" title="Moving Target Sensing for ISAC Systems in Clutter Environment"></a>Moving Target Sensing for ISAC Systems in Clutter Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01700">http://arxiv.org/abs/2311.01700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongqi Luo, Huihui Wu, Hongliang Luo, Bo Lin, Feifei Gao</li>
<li>for: 这篇论文关注整合感知通信（ISAC）系统在噪压环境中的运动目标探测问题。</li>
<li>methods: 作者采用扫描干扰来搜寻运动目标候选者，并将受到干扰信号中的响应高频率滤除以便识别候选目标。然后，他们使用根据MUSIC算法估算候选目标的角度、距离和垂直速度。</li>
<li>results: 作者透过实验结果显示了这些提议的方法的有效性。<details>
<summary>Abstract</summary>
In this paper, we consider the moving target sensing problem for integrated sensing and communication (ISAC) systems in clutter environment. Scatterers produce strong clutter, deteriorating the performance of ISAC systems in practice. Given that scatterers are typically stationary and the targets of interest are usually moving, we here focus on sensing the moving targets. Specifically, we adopt a scanning beam to search for moving target candidates. For the received signal in each scan, we employ high-pass filtering in the Doppler domain to suppress the clutter within the echo, thereby identifying candidate moving targets according to the power of filtered signal. Then, we adopt root-MUSIC-based algorithms to estimate the angle, range, and radial velocity of these candidate moving targets. Subsequently, we propose a target detection algorithm to reject false targets. Simulation results validate the effectiveness of these proposed methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了 интеграцион感知通信（ISAC）系统在噪声环境中的移动目标感知问题。噪声会对ISAC系统的性能产生负面影响，因此我们专注于感知移动目标。具体来说，我们采用扫描射频搜索移动目标候选人。对每次扫描得到的信号，我们使用高频滤波器在Doppler域进行高频滤波，以便从响应中排除噪声，并根据过滤后的信号强度确定候选移动目标。然后，我们采用基于MUSIC算法的根幂算法来估算候选移动目标的角度、距离和垂线速度。最后，我们提出了一种目标检测算法，以排除假目标。实验结果证明了我们提出的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Sensing-and-Communications-in-Clutter-Environment"><a href="#Integrated-Sensing-and-Communications-in-Clutter-Environment" class="headerlink" title="Integrated Sensing and Communications in Clutter Environment"></a>Integrated Sensing and Communications in Clutter Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01674">http://arxiv.org/abs/2311.01674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongliang Luo, Yucong Wang, Jianwei Zhao, Huihui Wu, Shaodan Ma, Feifei Gao</li>
<li>for: 本研究提出一种实用的综合感知通信（ISAC）框架，以感知动态目标从噪声环境中，同时保证用户通信质量。</li>
<li>methods: 我们设计了多个通信束，可以与用户进行通信，同时设计了一个扫描整个空间的感知束。为降低现有通信系统干扰，我们将服务区分为感知束（S4S）和通信束（C4S）两部分，并提供了束形设计和功率分配优化策略。</li>
<li>results: 我们的方案在实际实验中得到了证明，比较于忽略环境噪声的现有ISAC方法，我们的方案可以更好地探测动态目标并估计其角度、距离和速度。特别是，动态目标探测和角度估计通过角度-Doppler光谱估计（ADSE）和多个子载波（MSJD）的共同检测，而距离和速度估计通过扩展子空间算法。<details>
<summary>Abstract</summary>
In this paper, we propose a practical integrated sensing and communications (ISAC) framework to sense dynamic targets from clutter environment while ensuring users communications quality. To implement communications function and sensing function simultaneously, we design multiple communications beams that can communicate with the users as well as one sensing beam that can rotate and scan the entire space. To minimize the interference of sensing beam on existing communications systems, we divide the service area into sensing beam for sensing (S4S) sector and communications beam for sensing (C4S) sector, and provide beamforming design and power allocation optimization strategies for each type sector. Unlike most existing ISAC studies that ignore the interference of static environmental clutter on target sensing, we construct a mixed sensing channel model that includes both static environment and dynamic targets. When base station receives the echo signals, the mean phasor cancellation (MPC) method is employed to filter out the interference from static environmental clutter and to extract the effective dynamic target echoes. Then a complete and practical dynamic target sensing scheme is designed to detect the presence of dynamic targets and to estimate their angles, distances, and velocities. In particular, dynamic target detection and angle estimation are realized through angle-Doppler spectrum estimation (ADSE) and joint detection over multiple subcarriers (MSJD), while distance and velocity estimation are realized through the extended subspace algorithm. Simulation results demonstrate the effectiveness of the proposed scheme and its superiority over the existing methods that ignore environmental clutter.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种实用的集成感知通信（ISAC）框架，以感知动态目标在噪声环境中的探测，并保证用户的通信质量。为实现通信功能和感知功能的同时实现，我们设计了多个通信ibeam，可以与用户进行通信，同时还有一个感知ibeam，可以在整个空间中旋转和扫描。为了减少感知ibeam对现有通信系统的干扰，我们将服务区分为感知ibeam（S4S） сектор和通信ibeam（C4S） сектор，并提供了ibeamforming设计和功率分配优化策略 для每种类型的 сектор。与大多数现有ISAC研究不同，我们构建了一种混合感知通道模型，包括了静止环境和动态目标。当基站接收回射信号时，我们使用meanphasor抑制（MPC）方法来筛选静止环境干扰，并提取有效的动态目标回射信号。然后，我们设计了一种实用的动态目标探测方案，以检测动态目标的存在和角度、距离和速度的估算。具体来说，动态目标检测和角度估算通过角度-Doppler спектrum估算（ADSE）和多个子载波（MSJD）的联合检测来实现，而距离和速度估算则通过扩展子空间算法来实现。 simulation结果表明我们的方案的效果和现有方法的差异，并且我们的方案在感知环境中的干扰下表现更佳。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/03/eess.SP_2023_11_03/" data-id="closbrp0u01d40g88hzmp1i25" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/02/cs.CV_2023_11_02/" class="article-date">
  <time datetime="2023-11-02T13:00:00.000Z" itemprop="datePublished">2023-11-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/02/cs.CV_2023_11_02/">cs.CV - 2023-11-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Idempotent-Generative-Network"><a href="#Idempotent-Generative-Network" class="headerlink" title="Idempotent Generative Network"></a>Idempotent Generative Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01462">http://arxiv.org/abs/2311.01462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Assaf Shocher, Amil Dravid, Yossi Gandelsman, Inbar Mosseri, Michael Rubinstein, Alexei A. Efros</li>
<li>for: 本研究旨在提出一种基于神经网络培养的生成模型，该模型可以在一步中生成输出，同时保持一致的幂次空间，并允许顺序应用 для细化。</li>
<li>methods: 该模型使用了一种新的培养方法，即培养神经网络成为可递归的操作。该操作的目标是将源分布（例如泊松噪声）映射到目标分布（例如真实图像）。</li>
<li>results: 该研究表明，通过使用这种培养方法，可以提取到一种能够在一步中生成输出，同时保持一致的幂次空间的模型。此外，该模型还能够处理来自目标和源分布的输入，并将其映射回目标分布。<details>
<summary>Abstract</summary>
We propose a new approach for generative modeling based on training a neural network to be idempotent. An idempotent operator is one that can be applied sequentially without changing the result beyond the initial application, namely $f(f(z))=f(z)$. The proposed model $f$ is trained to map a source distribution (e.g, Gaussian noise) to a target distribution (e.g. realistic images) using the following objectives: (1) Instances from the target distribution should map to themselves, namely $f(x)=x$. We define the target manifold as the set of all instances that $f$ maps to themselves. (2) Instances that form the source distribution should map onto the defined target manifold. This is achieved by optimizing the idempotence term, $f(f(z))=f(z)$ which encourages the range of $f(z)$ to be on the target manifold. Under ideal assumptions such a process provably converges to the target distribution. This strategy results in a model capable of generating an output in one step, maintaining a consistent latent space, while also allowing sequential applications for refinement. Additionally, we find that by processing inputs from both target and source distributions, the model adeptly projects corrupted or modified data back to the target manifold. This work is a first step towards a ``global projector'' that enables projecting any input into a target data distribution.
</details>
<details>
<summary>摘要</summary>
我们提出一新的生成模型基于将神经网络训练为无效的操作。一个无效的操作是可以递进行无效的操作，而不会改变结果，即 $f(f(z)) = f(z) $。我们的模型 $f $ 将从源分布（例如 Gaussian 噪声）映射到目标分布（例如真实的图像），使用以下目标：1. 目标分布中的所有实例都应该变映射到自己，即 $f(x) = x $。我们称这为目标扩展。2. 源分布中的所有实例都应该变映射到定义的目标扩展中，这是通过优化无效性项目 $f(f(z)) = f(z) $ 来实现。这个过程会导致 $f(z) $ 的范围在目标扩展中，从而使得模型能够在一步骤内产生出PUT。在理想的假设下，这个过程可以将input转换为目标分布中的实例。此外，我们发现当处理来自目标和源分布的输入时，模型能够优化受到损害或修改的数据，将其转换回目标扩展中。这项工作是一个“全球投影器”的第一步，可以将任何输入投射到目标数据分布中。
</details></li>
</ul>
<hr>
<h2 id="Align-Your-Prompts-Test-Time-Prompting-with-Distribution-Alignment-for-Zero-Shot-Generalization"><a href="#Align-Your-Prompts-Test-Time-Prompting-with-Distribution-Alignment-for-Zero-Shot-Generalization" class="headerlink" title="Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization"></a>Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01459">http://arxiv.org/abs/2311.01459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jameelhassan/PromptAlign">https://github.com/jameelhassan/PromptAlign</a></li>
<li>paper_authors: Jameel Hassan, Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fahad Shahbaz Khan, Salman Khan</li>
<li>For: 本研究旨在解决预训练后Prompt Learning的难点，即对未seen频率的频率不适应。* Methods: 我们使用了一种新的提问调整方法，即在测试阶段使用提问调整来将测试阶段样本的特征分布与源频率样本的特征分布进行对应。* Results: 我们的方法可以在零shot上提高vision-language模型的性能，比基elineMaPLe提高3.08%。在跨数据集总结检验中，我们的方法在所有数据集上表现出了一致性的提升。Here’s the English version for reference:* For: The paper aims to solve the problem of poor generalization to unseen frequencies in prompt learning, which is a challenge in pre-training and fine-tuning vision-language models.* Methods: We propose a new prompt tuning method that aligns the test-time sample statistics with the source data statistics, using a single test sample to adapt multi-modal prompts at test time.* Results: Our method can improve the performance of vision-language models on zero-shot tasks, with a 3.08% improvement over the baseline MaPLe. Our method consistently improves performance across all datasets in cross-dataset generalization with unseen categories.<details>
<summary>Abstract</summary>
The promising zero-shot generalization of vision-language models such as CLIP has led to their adoption using prompt learning for numerous downstream tasks. Previous works have shown test-time prompt tuning using entropy minimization to adapt text prompts for unseen domains. While effective, this overlooks the key cause for performance degradation to unseen domains -- distribution shift. In this work, we explicitly handle this problem by aligning the out-of-distribution (OOD) test sample statistics to those of the source data using prompt tuning. We use a single test sample to adapt multi-modal prompts at test time by minimizing the feature distribution shift to bridge the gap in the test domain. Evaluating against the domain generalization benchmark, our method improves zero-shot top- 1 accuracy beyond existing prompt-learning techniques, with a 3.08% improvement over the baseline MaPLe. In cross-dataset generalization with unseen categories across 10 datasets, our method improves consistently across all datasets compared to the existing state-of-the-art. Our source code and models are available at https://jameelhassan.github.io/promptalign.
</details>
<details>
<summary>摘要</summary>
“CLIP的零基础通用化扩展已经导致它的应用使用提示学习。先前的研究表明，在测试时使用 entropy 最小化来调整文本提示以适应未看过的领域是有效的。然而，这些方法忽略了性能下降的主要原因——分布shift。在这种情况下，我们明确处理这个问题，使用提示调整来对测试样本的数据分布进行对应。我们使用单个测试样本来在测试时调整多modal的提示，以减少测试领域中的特征分布差异，从而bridging测试领域中的差距。通过对域泛化标准 benchmark 进行评估，我们的方法在零基础情况下提高了顶部 1 的准确率，与基eline MaPLe 相比，提高了3.08%。在不同的数据集之间进行交叉预测时，我们的方法在所有数据集上具有一致性的提高，与现有的状态之artefact 相比。我们的源代码和模型可以在 <https://jameelhassan.github.io/promptalign> 中获取。”
</details></li>
</ul>
<hr>
<h2 id="Detecting-Deepfakes-Without-Seeing-Any"><a href="#Detecting-Deepfakes-Without-Seeing-Any" class="headerlink" title="Detecting Deepfakes Without Seeing Any"></a>Detecting Deepfakes Without Seeing Any</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01458">http://arxiv.org/abs/2311.01458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/talreiss/factor">https://github.com/talreiss/factor</a></li>
<li>paper_authors: Tal Reiss, Bar Cavia, Yedid Hoshen</li>
<li>for: 防止深伪攻击，对社会造成严重威胁的媒体恶意修改。</li>
<li>methods: 引入“事实检查”概念，从 fake news 检测中获得灵感，以检查伪媒体中的 false facts，并将其与观察到的媒体进行比较，以进行深伪攻击的检测。</li>
<li>results: 提出 FACTOR，一个实用的深伪攻击检测方法，在面对重要攻击设定下表现出色，包括脸部交换和音视声合成。优点包括不需训练，仅使用现有的特征，易于实现，并且不需要见到深伪攻击。<details>
<summary>Abstract</summary>
Deepfake attacks, malicious manipulation of media containing people, are a serious concern for society. Conventional deepfake detection methods train supervised classifiers to distinguish real media from previously encountered deepfakes. Such techniques can only detect deepfakes similar to those previously seen, but not zero-day (previously unseen) attack types. As current deepfake generation techniques are changing at a breathtaking pace, new attack types are proposed frequently, making this a major issue. Our main observations are that: i) in many effective deepfake attacks, the fake media must be accompanied by false facts i.e. claims about the identity, speech, motion, or appearance of the person. For instance, when impersonating Obama, the attacker explicitly or implicitly claims that the fake media show Obama; ii) current generative techniques cannot perfectly synthesize the false facts claimed by the attacker. We therefore introduce the concept of "fact checking", adapted from fake news detection, for detecting zero-day deepfake attacks. Fact checking verifies that the claimed facts (e.g. identity is Obama), agree with the observed media (e.g. is the face really Obama's?), and thus can differentiate between real and fake media. Consequently, we introduce FACTOR, a practical recipe for deepfake fact checking and demonstrate its power in critical attack settings: face swapping and audio-visual synthesis. Although it is training-free, relies exclusively on off-the-shelf features, is very easy to implement, and does not see any deepfakes, it achieves better than state-of-the-art accuracy.
</details>
<details>
<summary>摘要</summary>
深刻的假动态攻击（deepfake attacks）对社会造成了严重的忧虑。传统的深刻检测方法通过训练监督分类器来分辨真伪媒体。这些技术只能检测已经见过的深刻攻击，但不能检测 Zero-day（未见过）攻击型。随着现有的深刻生成技术在极其快速的进步，新的攻击型不断提出，这成为了一个重要的问题。我们的主要观察结果是：一、在许多有效的深刻攻击中，假媒体必须被附加 false facts（说法），例如指出假媒体是谁（例如Obama）；二、目前的生成技术无法完美地Synthesize false facts。我们因此引入了“ факчекінグ”（fact checking）概念，从 fake news 检测中获得灵感，用于检测 Zero-day 深刻攻击。fact checking 检查假媒体中的 false facts 是否与观察到的媒体相符，因此可以区分真伪媒体。因此，我们引入 FACTOR，一个实用的深刻实现 fact checking 的方法，并在重要的攻击设定中进行评估：脸部调换和音频视觉合成。这个方法不需要训练，仅仅使用现有的特征，易于实现，并且无法检测任何深刻。它在critical attack settings中实现了更好的准确性。
</details></li>
</ul>
<hr>
<h2 id="UltraLiDAR-Learning-Compact-Representations-for-LiDAR-Completion-and-Generation"><a href="#UltraLiDAR-Learning-Compact-Representations-for-LiDAR-Completion-and-Generation" class="headerlink" title="UltraLiDAR: Learning Compact Representations for LiDAR Completion and Generation"></a>UltraLiDAR: Learning Compact Representations for LiDAR Completion and Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01448">http://arxiv.org/abs/2311.01448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwen Xiong, Wei-Chiu Ma, Jingkang Wang, Raquel Urtasun</li>
<li>for: 增强 LiDAR 点云的精度和覆盖率，提高自驾抵达系统的性能。</li>
<li>methods: 基于数据驱动的 UltraLiDAR 框架，包括点云的数据驱动编码、点云的精度和覆盖率的改进、点云的生成和 manipulate。</li>
<li>results: 对实际点云数据进行训练，可以达到densify  sparse point clouds 的目的，并且可以生成更加真实和可信的 LiDAR 点云数据，比 Priors 方法更有优势。<details>
<summary>Abstract</summary>
LiDAR provides accurate geometric measurements of the 3D world. Unfortunately, dense LiDARs are very expensive and the point clouds captured by low-beam LiDAR are often sparse. To address these issues, we present UltraLiDAR, a data-driven framework for scene-level LiDAR completion, LiDAR generation, and LiDAR manipulation. The crux of UltraLiDAR is a compact, discrete representation that encodes the point cloud's geometric structure, is robust to noise, and is easy to manipulate. We show that by aligning the representation of a sparse point cloud to that of a dense point cloud, we can densify the sparse point clouds as if they were captured by a real high-density LiDAR, drastically reducing the cost. Furthermore, by learning a prior over the discrete codebook, we can generate diverse, realistic LiDAR point clouds for self-driving. We evaluate the effectiveness of UltraLiDAR on sparse-to-dense LiDAR completion and LiDAR generation. Experiments show that densifying real-world point clouds with our approach can significantly improve the performance of downstream perception systems. Compared to prior art on LiDAR generation, our approach generates much more realistic point clouds. According to A/B test, over 98.5\% of the time human participants prefer our results over those of previous methods.
</details>
<details>
<summary>摘要</summary>
利达（LiDAR）提供了高精度的三维世界几何测量。然而，高密度的LiDAR仪器非常昂贵，而且低密度LiDAR所捕获的点云经常是稀疏的。为解决这些问题，我们介绍了UltraLiDAR，一个数据驱动的场景级LiDAR完成、生成和修改框架。UltraLiDAR的核心思想是一种含有点云几何结构的紧凑、离散表示方法，具有噪声抗性和易于操作的特点。我们表明，通过将稀疏点云的表示与高密度点云的表示进行对应，可以将稀疏点云灵活地填充为如果被真实高密度LiDAR捕获的样式，减少成本。此外，通过学习点云codebook的前提，我们可以生成多种、现实主义的LiDAR点云，用于自动驾驶。我们对稀疏点云完成和LiDAR生成进行评估。实验表明，使用我们的方法可以在下游识别系统中显著提高稀疏点云的性能。相比之前的LiDAR生成方法，我们的方法可以生成更加真实的点云。根据A/B测试，人类参与者超过98.5%的时间 prefer我们的结果。
</details></li>
</ul>
<hr>
<h2 id="CADSim-Robust-and-Scalable-in-the-wild-3D-Reconstruction-for-Controllable-Sensor-Simulation"><a href="#CADSim-Robust-and-Scalable-in-the-wild-3D-Reconstruction-for-Controllable-Sensor-Simulation" class="headerlink" title="CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Controllable Sensor Simulation"></a>CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Controllable Sensor Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01447">http://arxiv.org/abs/2311.01447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingkang Wang, Sivabalan Manivasagam, Yun Chen, Ze Yang, Ioan Andrei Bârsan, Anqi Joyce Yang, Wei-Chiu Ma, Raquel Urtasun</li>
<li>For: The paper is written for the development of realistic simulation for self-driving vehicles, specifically focusing on sensor simulation and the reconstruction of vehicle geometry.* Methods: The paper proposes a new method called CADSim, which combines part-aware object-class priors with differentiable rendering to automatically reconstruct vehicle geometry, including articulated wheels, with high-quality appearance.* Results: The paper shows that CADSim recovers more accurate shapes from sparse data compared to existing approaches, and it trains and renders efficiently. The reconstructed vehicles are demonstrated in several applications, including accurate testing of autonomy perception systems.Here is the same information in Simplified Chinese:* For: 本文为自动驾驶车辆实时模拟实现真实simulation，特别是感知器模拟。* Methods: 本文提出了一种新方法 called CADSim，它将部件意识对象类预先知识与可微分渲染结合，自动重建车辆几何结构，包括摆脱的车轮。* Results: 本文表明，CADSim可以从稀疏数据中提取更高质量的形状信息，并具有高效训练和渲染能力。重建的车辆在多个应用中展示了高度准确的测试和感知系统。<details>
<summary>Abstract</summary>
Realistic simulation is key to enabling safe and scalable development of % self-driving vehicles. A core component is simulating the sensors so that the entire autonomy system can be tested in simulation. Sensor simulation involves modeling traffic participants, such as vehicles, with high quality appearance and articulated geometry, and rendering them in real time. The self-driving industry has typically employed artists to build these assets. However, this is expensive, slow, and may not reflect reality. Instead, reconstructing assets automatically from sensor data collected in the wild would provide a better path to generating a diverse and large set with good real-world coverage. Nevertheless, current reconstruction approaches struggle on in-the-wild sensor data, due to its sparsity and noise. To tackle these issues, we present CADSim, which combines part-aware object-class priors via a small set of CAD models with differentiable rendering to automatically reconstruct vehicle geometry, including articulated wheels, with high-quality appearance. Our experiments show our method recovers more accurate shapes from sparse data compared to existing approaches. Importantly, it also trains and renders efficiently. We demonstrate our reconstructed vehicles in several applications, including accurate testing of autonomy perception systems.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:现实化模拟是自动驾驶车辆开发中安全和扩展的关键。核心组件是模拟感知器，以便整个自主系统可以在模拟中测试。感知器模拟包括模拟交通参与者，如车辆，并在实时中渲染它们。自驾行业Typically, artists have been employed to build these assets, but this is expensive, slow, and may not reflect reality. Instead, automatically reconstructing assets from in-the-wild sensor data would provide a better path to generating a diverse and large set with good real-world coverage. However, current reconstruction approaches struggle with in-the-wild sensor data due to its sparsity and noise. To address these issues, we present CADSim, which combines part-aware object-class priors via a small set of CAD models with differentiable rendering to automatically reconstruct vehicle geometry, including articulated wheels, with high-quality appearance. Our experiments show that our method recovers more accurate shapes from sparse data compared to existing approaches. Importantly, it also trains and renders efficiently. We demonstrate our reconstructed vehicles in several applications, including accurate testing of autonomy perception systems.
</details></li>
</ul>
<hr>
<h2 id="Adv3D-Generating-Safety-Critical-3D-Objects-through-Closed-Loop-Simulation"><a href="#Adv3D-Generating-Safety-Critical-3D-Objects-through-Closed-Loop-Simulation" class="headerlink" title="Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop Simulation"></a>Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01446">http://arxiv.org/abs/2311.01446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jay Sarva, Jingkang Wang, James Tu, Yuwen Xiong, Sivabalan Manivasagam, Raquel Urtasun</li>
<li>for: 这个论文旨在测试自动驾驶车辆（SDV）在各种场景下的可靠性，以确保其安全部署。</li>
<li>methods: 这篇论文提出了一个框架，名为Adv3D，可以在真实世界场景下进行closed-loop感知 simulation，以评估自动驾驶系统的性能。</li>
<li>results: 该框架可以在真实世界场景下找到影响自动驾驶系统性能的场景变化，并且发现这些变化在交互 Setting下更加有效。<details>
<summary>Abstract</summary>
Self-driving vehicles (SDVs) must be rigorously tested on a wide range of scenarios to ensure safe deployment. The industry typically relies on closed-loop simulation to evaluate how the SDV interacts on a corpus of synthetic and real scenarios and verify it performs properly. However, they primarily only test the system's motion planning module, and only consider behavior variations. It is key to evaluate the full autonomy system in closed-loop, and to understand how variations in sensor data based on scene appearance, such as the shape of actors, affect system performance. In this paper, we propose a framework, Adv3D, that takes real world scenarios and performs closed-loop sensor simulation to evaluate autonomy performance, and finds vehicle shapes that make the scenario more challenging, resulting in autonomy failures and uncomfortable SDV maneuvers. Unlike prior works that add contrived adversarial shapes to vehicle roof-tops or roadside to harm perception only, we optimize a low-dimensional shape representation to modify the vehicle shape itself in a realistic manner to degrade autonomy performance (e.g., perception, prediction, and motion planning). Moreover, we find that the shape variations found with Adv3D optimized in closed-loop are much more effective than those in open-loop, demonstrating the importance of finding scene appearance variations that affect autonomy in the interactive setting.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a framework called Adv3D that takes real-world scenarios and performs closed-loop sensor simulation to evaluate autonomy performance. We also find vehicle shapes that make the scenario more challenging, resulting in autonomy failures and uncomfortable SDV maneuvers. Unlike prior works that add contrived adversarial shapes to vehicle roof-tops or roadside to harm perception only, we optimize a low-dimensional shape representation to modify the vehicle shape itself in a realistic manner to degrade autonomy performance (e.g., perception, prediction, and motion planning).Moreover, we find that the shape variations found with Adv3D optimized in closed-loop are much more effective than those in open-loop, demonstrating the importance of finding scene appearance variations that affect autonomy in the interactive setting.
</details></li>
</ul>
<hr>
<h2 id="LabelFormer-Object-Trajectory-Refinement-for-Offboard-Perception-from-LiDAR-Point-Clouds"><a href="#LabelFormer-Object-Trajectory-Refinement-for-Offboard-Perception-from-LiDAR-Point-Clouds" class="headerlink" title="LabelFormer: Object Trajectory Refinement for Offboard Perception from LiDAR Point Clouds"></a>LabelFormer: Object Trajectory Refinement for Offboard Perception from LiDAR Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01444">http://arxiv.org/abs/2311.01444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anqi Joyce Yang, Sergio Casas, Nikita Dvornik, Sean Segal, Yuwen Xiong, Jordan Sir Kwang Hu, Carter Fang, Raquel Urtasun</li>
<li>for: 这个论文旨在提出一种简单、高效、有效的路径水平纠正方法，以提高自动驾驶观察系统的训练效果。</li>
<li>methods: 该论文提出了一种两阶段方法，首先探测和跟踪对象，然后使用学习的纠正模型进行精度提高。而 LabelFormer 方法则是一种简单、高效、有效的 trajectory-level 纠正方法，它首先对每帧观察进行编码，然后利用自我注意力来理解轨迹的全 temporal 上下文，最后对轨迹进行解码，以获得精度提高后的对象大小和每帧 pose。</li>
<li>results: 论文的实验结果表明，LabelFormer 方法可以在城市和高速公路上的数据集上大幅度超越现有的方法。此外，论文还表明，通过使用 LabelFormer 生成的自动标签进行训练，可以提高下游探测性能。详细信息请参考 <a target="_blank" rel="noopener" href="https://waabi.ai/labelformer">https://waabi.ai/labelformer</a><details>
<summary>Abstract</summary>
A major bottleneck to scaling-up training of self-driving perception systems are the human annotations required for supervision. A promising alternative is to leverage "auto-labelling" offboard perception models that are trained to automatically generate annotations from raw LiDAR point clouds at a fraction of the cost. Auto-labels are most commonly generated via a two-stage approach -- first objects are detected and tracked over time, and then each object trajectory is passed to a learned refinement model to improve accuracy. Since existing refinement models are overly complex and lack advanced temporal reasoning capabilities, in this work we propose LabelFormer, a simple, efficient, and effective trajectory-level refinement approach. Our approach first encodes each frame's observations separately, then exploits self-attention to reason about the trajectory with full temporal context, and finally decodes the refined object size and per-frame poses. Evaluation on both urban and highway datasets demonstrates that LabelFormer outperforms existing works by a large margin. Finally, we show that training on a dataset augmented with auto-labels generated by our method leads to improved downstream detection performance compared to existing methods. Please visit the project website for details https://waabi.ai/labelformer
</details>
<details>
<summary>摘要</summary>
很多自动驾驶感知系统的训练Scaling-up受到人工标注的瓶颈。一种有前途的方法是利用“自动标注”的Board perception模型，可以自动生成标注从原始LiDAR点云数据，并且只需要一小部分的成本。自动标注通常通过两个阶段进行：首先探测和跟踪对象，然后每个对象轨迹通过学习改进模型来提高准确性。现有的改进模型太复杂，缺乏高级时间逻辑能力，因此在这里我们提出了LabelFormer，一种简单、高效、有效的轨迹级别改进方法。我们的方法首先编码每帧观察数据，然后利用自我注意力来理解轨迹的全 temporal 上下文，并最后解码出改进后的对象大小和每帧姿态。我们的LabelFormer在都市和高速公路 dataset 上进行评估，与现有方法比较，表现出了大幅度的提高。最后，我们展示了通过我们的方法生成的自动标注来训练下游检测模型，对现有方法进行训练带来了改进的检测性能。详细信息请参考我们的项目网站：<https://waabi.ai/labelformer>
</details></li>
</ul>
<hr>
<h2 id="Transformation-Decoupling-Strategy-based-on-Screw-Theory-for-Deterministic-Point-Cloud-Registration-with-Gravity-Prior"><a href="#Transformation-Decoupling-Strategy-based-on-Screw-Theory-for-Deterministic-Point-Cloud-Registration-with-Gravity-Prior" class="headerlink" title="Transformation Decoupling Strategy based on Screw Theory for Deterministic Point Cloud Registration with Gravity Prior"></a>Transformation Decoupling Strategy based on Screw Theory for Deterministic Point Cloud Registration with Gravity Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01432">http://arxiv.org/abs/2311.01432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Li, Zijian Ma, Yinlong Liu, Walter Zimmer, Hu Cao, Feihu Zhang, Alois Knoll</li>
<li>for: 这 paper 是为了解决受重 OUTLIER 干扰的点云注册问题，特别是在实际应用中经常出现的相对性基于重力方向的注册问题。</li>
<li>methods: 该 paper 提出了一种基于扭轴理论的转换分解策略，将原始的 4-DOF 问题分解成 3 个 sub-problems 中的 1-DOF、2-DOF 和 1-DOF，从而提高计算效率。特别是，第一个 1-DOF 表示矢量在旋转轴上的翻译，我们提出了一种间隔刺激方法来解决它。第二个 2-DOF 表示枢轴，我们利用 branch-and-bound 方法来解决它。最后一个 1-DOF 表示旋转角度，我们提出了一种全局投票方法来估算它。</li>
<li>results: 该 paper 的方法可以高效地和 deterministic 地进行注册，特别是在 OUTLIER 率超过 99% 的情况下。广泛的实验表明，与现有方法相比，该方法更高效和更稳定。<details>
<summary>Abstract</summary>
Point cloud registration is challenging in the presence of heavy outlier correspondences. This paper focuses on addressing the robust correspondence-based registration problem with gravity prior that often arises in practice. The gravity directions are typically obtained by inertial measurement units (IMUs) and can reduce the degree of freedom (DOF) of rotation from 3 to 1. We propose a novel transformation decoupling strategy by leveraging screw theory. This strategy decomposes the original 4-DOF problem into three sub-problems with 1-DOF, 2-DOF, and 1-DOF, respectively, thereby enhancing the computation efficiency. Specifically, the first 1-DOF represents the translation along the rotation axis and we propose an interval stabbing-based method to solve it. The second 2-DOF represents the pole which is an auxiliary variable in screw theory and we utilize a branch-and-bound method to solve it. The last 1-DOF represents the rotation angle and we propose a global voting method for its estimation. The proposed method sequentially solves three consensus maximization sub-problems, leading to efficient and deterministic registration. In particular, it can even handle the correspondence-free registration problem due to its significant robustness. Extensive experiments on both synthetic and real-world datasets demonstrate that our method is more efficient and robust than state-of-the-art methods, even when dealing with outlier rates exceeding 99%.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文 Cloud 注册是在具有重大外liers的对应关系时具有挑战性。这篇论文关注了在实践中常出现的强对应关系基础注册问题，并使用重力方向作为准确度提高的一种方法。重力方向通常由惯性测量设备（IMU）获得，可以将旋转的度量量reduced to 1。我们提议了一种新的变换分解策略，基于螺旋理论。这种策略将原始的4DOF问题分解成3个子问题，每个子问题都是1DOF、2DOF和1DOF，从而提高计算效率。 Specifically, the first 1DOF represents the translation along the rotation axis, and we propose an interval stabbing-based method to solve it. The second 2DOF represents the pole, which is an auxiliary variable in screw theory, and we utilize a branch-and-bound method to solve it. The last 1DOF represents the rotation angle, and we propose a global voting method for its estimation. The proposed method sequentially solves three consensus maximization sub-problems, leading to efficient and deterministic registration. In particular, it can even handle the correspondence-free registration problem due to its significant robustness. Extensive experiments on both synthetic and real-world datasets demonstrate that our method is more efficient and robust than state-of-the-art methods, even when dealing with outlier rates exceeding 99%.<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Efficient-Vision-Transformer-for-Accurate-Traffic-Sign-Detection"><a href="#Efficient-Vision-Transformer-for-Accurate-Traffic-Sign-Detection" class="headerlink" title="Efficient Vision Transformer for Accurate Traffic Sign Detection"></a>Efficient Vision Transformer for Accurate Traffic Sign Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01429">http://arxiv.org/abs/2311.01429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javad Mirzapour Kaleybar, Hooman Khaloo, Avaz Naghipour</li>
<li>for: 本研究论文主要targets traffic sign detection in self-driving vehicles and driver assistance systems, with the goal of developing reliable and highly accurate algorithms for widespread adoption in diverse real-life scenarios.</li>
<li>methods: 本研究使用了Transformer模型，尤其是Vision Transformer variants，来解决 traffic sign detection task。Transformer的注意机制，原本设计用于自然语言处理，在图像识别领域提供了并行效率的改进。</li>
<li>results: 实验评估表明，该策略可以在GTSDB数据集上实现显著的进步，特别是在速度和准确率两个方面。<details>
<summary>Abstract</summary>
This research paper addresses the challenges associated with traffic sign detection in self-driving vehicles and driver assistance systems. The development of reliable and highly accurate algorithms is crucial for the widespread adoption of traffic sign recognition and detection (TSRD) in diverse real-life scenarios. However, this task is complicated by suboptimal traffic images affected by factors such as camera movement, adverse weather conditions, and inadequate lighting. This study specifically focuses on traffic sign detection methods and introduces the application of the Transformer model, particularly the Vision Transformer variants, to tackle this task. The Transformer's attention mechanism, originally designed for natural language processing, offers improved parallel efficiency. Vision Transformers have demonstrated success in various domains, including autonomous driving, object detection, healthcare, and defense-related applications. To enhance the efficiency of the Transformer model, the research proposes a novel strategy that integrates a locality inductive bias and a transformer module. This includes the introduction of the Efficient Convolution Block and the Local Transformer Block, which effectively capture short-term and long-term dependency information, thereby improving both detection speed and accuracy. Experimental evaluations demonstrate the significant advancements achieved by this approach, particularly when applied to the GTSDB dataset.
</details>
<details>
<summary>摘要</summary>
To enhance the efficiency of the Transformer model, the research proposes a novel strategy that integrates a locality inductive bias and a transformer module. This includes the introduction of the Efficient Convolution Block and the Local Transformer Block, which effectively capture short-term and long-term dependency information, thereby improving both detection speed and accuracy.Experimental evaluations demonstrate the significant advancements achieved by this approach, particularly when applied to the GTSDB dataset. The proposed method is able to detect traffic signs more accurately and efficiently, which is crucial for the widespread adoption of traffic sign recognition and detection (TSRD) in diverse real-life scenarios.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Deep-Learning-Techniques-for-Glaucoma-Detection-A-Comprehensive-Review"><a href="#Exploring-Deep-Learning-Techniques-for-Glaucoma-Detection-A-Comprehensive-Review" class="headerlink" title="Exploring Deep Learning Techniques for Glaucoma Detection: A Comprehensive Review"></a>Exploring Deep Learning Techniques for Glaucoma Detection: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01425">http://arxiv.org/abs/2311.01425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aized Amin Soofi, Fazal-e-Amin<br>for: 本文旨在提供一种全面的深度学习方法，用于检测和诊断眼内压病（ Glaucoma）。methods: 本文使用的方法包括深度学习的分割、分类和检测技术，以提高眼内压病的检测精度和效率。results: 根据文献分析，深度学习方法在眼内压病检测中表现出色，可以提高检测精度和效率，并且具有可重复性和可靠性。但是，深度学习方法还存在一些限制和挑战，需要进一步的研究和改进。<details>
<summary>Abstract</summary>
Glaucoma is one of the primary causes of vision loss around the world, necessitating accurate and efficient detection methods. Traditional manual detection approaches have limitations in terms of cost, time, and subjectivity. Recent developments in deep learning approaches demonstrate potential in automating glaucoma detection by detecting relevant features from retinal fundus images. This article provides a comprehensive overview of cutting-edge deep learning methods used for the segmentation, classification, and detection of glaucoma. By analyzing recent studies, the effectiveness and limitations of these techniques are evaluated, key findings are highlighted, and potential areas for further research are identified. The use of deep learning algorithms may significantly improve the efficacy, usefulness, and accuracy of glaucoma detection. The findings from this research contribute to the ongoing advancements in automated glaucoma detection and have implications for improving patient outcomes and reducing the global burden of glaucoma.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese: glaucoma是全球主要导致视力损失的疾病之一，需要精准和高效的检测方法。传统的手动检测方法受到成本、时间和主观性的限制。current deep learning approaches demonstrate potential in automating glaucoma detection by detecting relevant features from retinal fundus images. This article provides a comprehensive overview of cutting-edge deep learning methods used for the segmentation, classification, and detection of glaucoma. By analyzing recent studies, the effectiveness and limitations of these techniques are evaluated, key findings are highlighted, and potential areas for further research are identified. The use of deep learning algorithms may significantly improve the efficacy, usefulness, and accuracy of glaucoma detection. The findings from this research contribute to the ongoing advancements in automated glaucoma detection and have implications for improving patient outcomes and reducing the global burden of glaucoma.
</details></li>
</ul>
<hr>
<h2 id="CenterRadarNet-Joint-3D-Object-Detection-and-Tracking-Framework-using-4D-FMCW-Radar"><a href="#CenterRadarNet-Joint-3D-Object-Detection-and-Tracking-Framework-using-4D-FMCW-Radar" class="headerlink" title="CenterRadarNet: Joint 3D Object Detection and Tracking Framework using 4D FMCW Radar"></a>CenterRadarNet: Joint 3D Object Detection and Tracking Framework using 4D FMCW Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01423">http://arxiv.org/abs/2311.01423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jen-Hao Cheng, Sheng-Yao Kuan, Hugo Latapie, Gaowen Liu, Jenq-Neng Hwang</li>
<li>for: 提高自动驾驶和协助驾驶技术的安全性，增强雷达感知的稳定性和可靠性。</li>
<li>methods: 提出一种高效的中心雷达网络（CenterRadarNet），利用4D雷达数据进行高级别表示学习，实现3D对象检测和重新识别（re-ID）任务。</li>
<li>results: 在K-Radar3D对象检测数据集上达到了状态之Art的result，并实现了雷达数据集V2上的首个3D对象跟踪结果。在多种驾驶场景下，CenterRadarNet表现了一致、稳定的性能，强调其广泛应用性。<details>
<summary>Abstract</summary>
Robust perception is a vital component for ensuring safe autonomous and assisted driving. Automotive radar (77 to 81 GHz), which offers weather-resilient sensing, provides a complementary capability to the vision- or LiDAR-based autonomous driving systems. Raw radio-frequency (RF) radar tensors contain rich spatiotemporal semantics besides 3D location information. The majority of previous methods take in 3D (Doppler-range-azimuth) RF radar tensors, allowing prediction of an object's location, heading angle, and size in bird's-eye-view (BEV). However, they lack the ability to at the same time infer objects' size, orientation, and identity in the 3D space. To overcome this limitation, we propose an efficient joint architecture called CenterRadarNet, designed to facilitate high-resolution representation learning from 4D (Doppler-range-azimuth-elevation) radar data for 3D object detection and re-identification (re-ID) tasks. As a single-stage 3D object detector, CenterRadarNet directly infers the BEV object distribution confidence maps, corresponding 3D bounding box attributes, and appearance embedding for each pixel. Moreover, we build an online tracker utilizing the learned appearance embedding for re-ID. CenterRadarNet achieves the state-of-the-art result on the K-Radar 3D object detection benchmark. In addition, we present the first 3D object-tracking result using radar on the K-Radar dataset V2. In diverse driving scenarios, CenterRadarNet shows consistent, robust performance, emphasizing its wide applicability.
</details>
<details>
<summary>摘要</summary>
robust 感知是自动驾驶和助动驾驶安全的关键组件。汽车雷达（77至81 GHz），具有天气抵抗性，提供了视觉或 LiDAR 自动驾驶系统的补充能力。 raw 电磁波（RF）雷达张量包含了具有3D位置信息的辐射学 semantics。大多数前一代方法接受3D（Doppler-range-azimuth）RF雷达张量，允许预测目标的位置、方向角和大小在鸟瞰视图（BEV）中。然而，它们缺乏同时推断目标的大小、方向和身份在3D空间的能力。为了解决这些限制，我们提出了一种高效的联合体系结构，称之为 CenterRadarNet，用于从4D（Doppler-range-azimuth-elevation）雷达数据中进行高级表示学习，以实现3D объек的检测和重新识别（re-ID）任务。作为单个阶段3D对象探测器，CenterRadarNet直接生成了BEV对象分布信息可信度地图，相应的3D包围框属性和外观嵌入。此外，我们建立了在线跟踪器，使用学习的外观嵌入进行重新识别。 CenterRadarNet在K-Radar 3D对象检测标准准则上实现了状态的最佳结果。此外，我们还提供了基于雷达的K-Radar数据集 V2 上的首个3D对象跟踪结果。在多样化的驾驶enario中，CenterRadarNet表现了一致、可靠的性，强调了其广泛的应用能力。
</details></li>
</ul>
<hr>
<h2 id="The-Blessing-of-Randomness-SDE-Beats-ODE-in-General-Diffusion-based-Image-Editing"><a href="#The-Blessing-of-Randomness-SDE-Beats-ODE-in-General-Diffusion-based-Image-Editing" class="headerlink" title="The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing"></a>The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01410">http://arxiv.org/abs/2311.01410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, Chongxuan Li</li>
<li>for: 这个论文旨在提出一种统一的概率形式化方法，用于基于扩散的图像编辑，其中一个隐藏变量在任务特定的方式下被编辑，并且通常与原始的随机或偏微分方程（SDE或ODE）中的相应变量偏离。</li>
<li>methods: 论文使用的方法包括编辑SDE和ODE，以及基于SDE的扩散基eline。</li>
<li>results: 论文的实验结果表明，在不同任务中，SDE具有明显的优势和多样性，可以在图像编辑中提供更高质量的结果，并且可以与现有的扩散基eline相比，显示出更好的性能。<details>
<summary>Abstract</summary>
We present a unified probabilistic formulation for diffusion-based image editing, where a latent variable is edited in a task-specific manner and generally deviates from the corresponding marginal distribution induced by the original stochastic or ordinary differential equation (SDE or ODE). Instead, it defines a corresponding SDE or ODE for editing. In the formulation, we prove that the Kullback-Leibler divergence between the marginal distributions of the two SDEs gradually decreases while that for the ODEs remains as the time approaches zero, which shows the promise of SDE in image editing. Inspired by it, we provide the SDE counterparts for widely used ODE baselines in various tasks including inpainting and image-to-image translation, where SDE shows a consistent and substantial improvement. Moreover, we propose SDE-Drag -- a simple yet effective method built upon the SDE formulation for point-based content dragging. We build a challenging benchmark (termed DragBench) with open-set natural, art, and AI-generated images for evaluation. A user study on DragBench indicates that SDE-Drag significantly outperforms our ODE baseline, existing diffusion-based methods, and the renowned DragGAN. Our results demonstrate the superiority and versatility of SDE in image editing and push the boundary of diffusion-based editing methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种统一的概率形式化方法 дляDiffusion-based图像编辑，其中一个隐藏变量在任务特定的方式下被编辑，通常与原始的随机或ordinary differential equation（SDE或ODE）中的相应 marginal distribution不同。相反，它定义了一个对应的SDE或ODE для编辑。在这种形式化中，我们证明了两个SDE的margin distribution的Kullback-Leibler divergence逐渐减少，而ODE中的 margin distribution的Kullback-Leibler divergence保持不变，这表明SDE在图像编辑中的承诺。 inspirited by it, we provide SDE counterparts for widely used ODE baselines in various tasks, including inpainting and image-to-image translation, where SDE shows consistent and substantial improvement. Moreover, we propose SDE-Drag -- a simple yet effective method built upon the SDE formulation for point-based content dragging. We build a challenging benchmark (termed DragBench) with open-set natural, art, and AI-generated images for evaluation. A user study on DragBench indicates that SDE-Drag significantly outperforms our ODE baseline, existing diffusion-based methods, and the renowned DragGAN. Our results demonstrate the superiority and versatility of SDE in image editing and push the boundary of diffusion-based editing methods.
</details></li>
</ul>
<hr>
<h2 id="Learning-to-See-Physical-Properties-with-Active-Sensing-Motor-Policies"><a href="#Learning-to-See-Physical-Properties-with-Active-Sensing-Motor-Policies" class="headerlink" title="Learning to See Physical Properties with Active Sensing Motor Policies"></a>Learning to See Physical Properties with Active Sensing Motor Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01405">http://arxiv.org/abs/2311.01405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel B. Margolis, Xiang Fu, Yandong Ji, Pulkit Agrawal</li>
<li>for: 这个论文是为了帮助机器人更有效地行走，通过利用图像中的物理特性来做出计划。</li>
<li>methods: 这个方法使用自我超vised labeling，使用实际行走中采集的图像和实际物理参数估计器在模拟环境中训练。另外，我们还引入了活动感知 дви作策略（ASMP），以便增强物理参数估计的准确性。</li>
<li>results: 我们的方法可以准确地预测物理参数，并且可以在不同的摄像头和机器人上工作，即使是在飞行器拍摄的过头图像中。<details>
<summary>Abstract</summary>
Knowledge of terrain's physical properties inferred from color images can aid in making efficient robotic locomotion plans. However, unlike image classification, it is unintuitive for humans to label image patches with physical properties. Without labeled data, building a vision system that takes as input the observed terrain and predicts physical properties remains challenging. We present a method that overcomes this challenge by self-supervised labeling of images captured by robots during real-world traversal with physical property estimators trained in simulation. To ensure accurate labeling, we introduce Active Sensing Motor Policies (ASMP), which are trained to explore locomotion behaviors that increase the accuracy of estimating physical parameters. For instance, the quadruped robot learns to swipe its foot against the ground to estimate the friction coefficient accurately. We show that the visual system trained with a small amount of real-world traversal data accurately predicts physical parameters. The trained system is robust and works even with overhead images captured by a drone despite being trained on data collected by cameras attached to a quadruped robot walking on the ground.
</details>
<details>
<summary>摘要</summary>
知识来自景色的物理属性可以帮助机器人制定有效的移动计划。然而，与人类标注图像不同，将图像块标注为物理属性是不直观的。没有标注数据，建立一个接受图像作为输入，预测物理属性的视觉系统仍然是一个挑战。我们提出了一种方法，通过自我超vised标注实际行走中捕捉的图像来解决这个问题。为确保准确标注，我们引入了活动感知电动政策（ASMP），这些政策在实际行走中被训练以探索提高物理参数估计的行走方式。例如，四肢动物机器人学会在地面上滑块来精准估计透震率。我们显示，使用小量实际行走数据训练的视觉系统可以准确预测物理参数。受训系统是Robust的，甚至在悬停在空中的飞机拍摄的图像上也能正常工作，即使它们在地面上行走时被训练。
</details></li>
</ul>
<hr>
<h2 id="Learning-Realistic-Traffic-Agents-in-Closed-loop"><a href="#Learning-Realistic-Traffic-Agents-in-Closed-loop" class="headerlink" title="Learning Realistic Traffic Agents in Closed-loop"></a>Learning Realistic Traffic Agents in Closed-loop</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01394">http://arxiv.org/abs/2311.01394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Zhang, James Tu, Lunjun Zhang, Kelvin Wong, Simon Suo, Raquel Urtasun</li>
<li>for: 本研究旨在开发一种可靠和可扩展的自动驾驶软件，通过在实际上使用人工智能来模拟实际交通情况，以避免在真实世界中发生的危险。</li>
<li>methods: 本研究使用了拟合学习（IL）和奖励学习（RL）两种方法，通过一种叫做“强制规则恢复”（RTR）的关闭Loop学习目标，将这两种方法结合起来，以实现更加人类化的交通行为。</li>
<li>results: 实验结果表明，使用RTR方法可以让交通策略更加真实和普遍，在正常和偏值情况下都能够达到更好的平衡点，并且可以用作预测模型训练数据生成工具，从而提高下游预测指标。<details>
<summary>Abstract</summary>
Realistic traffic simulation is crucial for developing self-driving software in a safe and scalable manner prior to real-world deployment. Typically, imitation learning (IL) is used to learn human-like traffic agents directly from real-world observations collected offline, but without explicit specification of traffic rules, agents trained from IL alone frequently display unrealistic infractions like collisions and driving off the road. This problem is exacerbated in out-of-distribution and long-tail scenarios. On the other hand, reinforcement learning (RL) can train traffic agents to avoid infractions, but using RL alone results in unhuman-like driving behaviors. We propose Reinforcing Traffic Rules (RTR), a holistic closed-loop learning objective to match expert demonstrations under a traffic compliance constraint, which naturally gives rise to a joint IL + RL approach, obtaining the best of both worlds. Our method learns in closed-loop simulations of both nominal scenarios from real-world datasets as well as procedurally generated long-tail scenarios. Our experiments show that RTR learns more realistic and generalizable traffic simulation policies, achieving significantly better tradeoffs between human-like driving and traffic compliance in both nominal and long-tail scenarios. Moreover, when used as a data generation tool for training prediction models, our learned traffic policy leads to considerably improved downstream prediction metrics compared to baseline traffic agents. For more information, visit the project website: https://waabi.ai/rtr
</details>
<details>
<summary>摘要</summary>
现实主义交通模拟是自动驾驶软件开发中非常重要的，以确保在真实世界中安全部署。通常，模仿学习（IL）用于直接从真实世界观察中学习人类交通代理，但是不具体地规定交通规则，则代理训练出来的不具有真实的交通规则遵从性，导致很多偏差和脱离道路的情况。这个问题在不同的情况和长尾情况下更加严重。相反，奖励学习（RL）可以训练交通代理避免偏差，但是使用RLalone会导致不人类化的驾驶行为。我们提出了一种名为“强制交通规则”（RTR）的整体循环学习目标，用于匹配专家示例，并且自然地组合了IL + RL两种方法，从而获得最佳的两个世界。我们的方法在循环 simulations of both nominal scenarios from real-world datasets as well as procedurally generated long-tail scenarios中学习。我们的实验表明，RTR可以更加真实和普遍的交通模拟策略，在nominal和长尾情况下都能够获得更好的平衡。此外，当用作预测模型训练数据生成工具时，我们学习的交通策略会导致下游预测 metric 明显提高，相比基eline traffic agents。更多信息请访问我们的项目网站：https://waabi.ai/rtr。
</details></li>
</ul>
<hr>
<h2 id="Sim2Real-Bilevel-Adaptation-for-Object-Surface-Classification-using-Vision-Based-Tactile-Sensors"><a href="#Sim2Real-Bilevel-Adaptation-for-Object-Surface-Classification-using-Vision-Based-Tactile-Sensors" class="headerlink" title="Sim2Real Bilevel Adaptation for Object Surface Classification using Vision-Based Tactile Sensors"></a>Sim2Real Bilevel Adaptation for Object Surface Classification using Vision-Based Tactile Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01380">http://arxiv.org/abs/2311.01380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsp-iit/sim2real-surface-classification">https://github.com/hsp-iit/sim2real-surface-classification</a></li>
<li>paper_authors: Gabriele M. Caddeo, Andrea Maracani, Paolo D. Alfano, Nicola A. Piga, Lorenzo Rosasco, Lorenzo Natale</li>
<li>for:  bridging the Sim2Real gap in vision-based tactile sensors for classifying object surfaces</li>
<li>methods:  training a Diffusion Model using a small dataset of real-world images and aligning features of the two domains using an adversarial procedure</li>
<li>results:  a total accuracy of 81.9%, a significant improvement compared to the 34.7% achieved by the classifier trained solely on simulated images<details>
<summary>Abstract</summary>
In this paper, we address the Sim2Real gap in the field of vision-based tactile sensors for classifying object surfaces. We train a Diffusion Model to bridge this gap using a relatively small dataset of real-world images randomly collected from unlabeled everyday objects via the DIGIT sensor. Subsequently, we employ a simulator to generate images by uniformly sampling the surface of objects from the YCB Model Set. These simulated images are then translated into the real domain using the Diffusion Model and automatically labeled to train a classifier. During this training, we further align features of the two domains using an adversarial procedure. Our evaluation is conducted on a dataset of tactile images obtained from a set of ten 3D printed YCB objects. The results reveal a total accuracy of 81.9%, a significant improvement compared to the 34.7% achieved by the classifier trained solely on simulated images. This demonstrates the effectiveness of our approach. We further validate our approach using the classifier on a 6D object pose estimation task from tactile data.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们处理视觉基于感觉器的Surface classification问题中的Sim2Real gap。我们使用一个扩散模型来跨越这个差距，使用一个相对较小的实际世界图像集来训练。然后，我们使用一个模拟器生成图像，通过对物体表面进行均匀采样，从YCB模型集中获取的图像。这些模拟图像然后通过扩散模型进行翻译，并自动将其标注为训练一个分类器。在这个训练过程中，我们还使用一种对抗性方法来对两个领域的特征进行对齐。我们的评估是基于一组从3D打印YCB对象中获取的感觉图像集。结果表明，我们的方法可以达到81.9%的总准确率，与 solely 在模拟图像上训练的分类器（34.7%）相比，这表明我们的方法的效iveness。我们进一步验证了我们的方法，使用感觉数据进行6D对象姿态估计任务。
</details></li>
</ul>
<hr>
<h2 id="Robust-Identity-Perceptual-Watermark-Against-Deepfake-Face-Swapping"><a href="#Robust-Identity-Perceptual-Watermark-Against-Deepfake-Face-Swapping" class="headerlink" title="Robust Identity Perceptual Watermark Against Deepfake Face Swapping"></a>Robust Identity Perceptual Watermark Against Deepfake Face Swapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01357">http://arxiv.org/abs/2311.01357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Wang, Mengxiao Huang, Harry Cheng, Bin Ma, Yinglong Wang</li>
<li>for: 防止 Deepfake 面孔替换的隐私问题</li>
<li>methods: 植入不可见信号进行探测和追溯</li>
<li>results: 实现了对 Deepfake 面孔替换的检测和追溯，并且在不同的数据集和替换方法下达到了状态之最的性能<details>
<summary>Abstract</summary>
Notwithstanding offering convenience and entertainment to society, Deepfake face swapping has caused critical privacy issues with the rapid development of deep generative models. Due to imperceptible artifacts in high-quality synthetic images, passive detection models against face swapping in recent years usually suffer performance damping regarding the generalizability issue. Therefore, several studies have been attempted to proactively protect the original images against malicious manipulations by inserting invisible signals in advance. However, the existing proactive defense approaches demonstrate unsatisfactory results with respect to visual quality, detection accuracy, and source tracing ability. In this study, we propose the first robust identity perceptual watermarking framework that concurrently performs detection and source tracing against Deepfake face swapping proactively. We assign identity semantics regarding the image contents to the watermarks and devise an unpredictable and unreversible chaotic encryption system to ensure watermark confidentiality. The watermarks are encoded and recovered by jointly training an encoder-decoder framework along with adversarial image manipulations. Extensive experiments demonstrate state-of-the-art performance against Deepfake face swapping under both cross-dataset and cross-manipulation settings.
</details>
<details>
<summary>摘要</summary>
不смотря于对社会提供便捷和娱乐，深入模型的发展使得深伪肖面换技术导致了严重的隐私问题。由于高质量 sintetic 图像中的不可见artefacts，以往的检测模型在面 swap 问题上通常会表现出性能下降，尤其是在泛化问题上。因此，一些研究尝试了在先进行反应性保护原始图像，以防止恶意操作。然而，现有的反应性防御方法在视觉质量、检测精度和来源追踪能力等方面均表现不满意。在这种情况下，我们提出了首个可靠性感知水印框架，可同时进行检测和来源追踪对 Deepfake 面 swap 进行反应性防御。我们将图像内容中的Identify semantics分配给水印，并设计了不可预测、不可逆的混沌加密系统，以保证水印的机密性。水印被编码和还原通过在encoder-decoder框架中进行共同训练，并与对采样图像进行恶意修改。广泛的实验表明，我们的方法在不同的 dataset 和修改设定下均达到了顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-Image-Compression-for-Microscopy-Images-An-Empirical-Study"><a href="#Deep-learning-based-Image-Compression-for-Microscopy-Images-An-Empirical-Study" class="headerlink" title="Deep learning based Image Compression for Microscopy Images: An Empirical Study"></a>Deep learning based Image Compression for Microscopy Images: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01352">http://arxiv.org/abs/2311.01352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Zhou, Jan Sollman, Jianxu Chen</li>
<li>for: 本研究旨在分析 классические和深度学习基于图像压缩方法，以及它们对深度学习基于图像处理模型的影响。</li>
<li>methods: 本研究使用了多种класси型损失图像压缩技术和深度学习基于图像压缩模型，并对它们进行比较，包括CompressAI工具箱提供的多种压缩模型。</li>
<li>results: 研究发现，深度学习基于图像压缩技术可以大幅提高压缩率，而不会对下游的标签自由预测模型造成重大影响。在2D情况下，AI基于压缩技术的表现远胜于класси型压缩技术。<details>
<summary>Abstract</summary>
With the fast development of modern microscopes and bioimaging techniques, an unprecedentedly large amount of imaging data are being generated, stored, analyzed, and even shared through networks. The size of the data poses great challenges for current data infrastructure. One common way to reduce the data size is by image compression. This present study analyzes classic and deep learning based image compression methods, and their impact on deep learning based image processing models. Deep learning based label-free prediction models (i.e., predicting fluorescent images from bright field images) are used as an example application for comparison and analysis. Effective image compression methods could help reduce the data size significantly without losing necessary information, and therefore reduce the burden on data management infrastructure and permit fast transmission through the network for data sharing or cloud computing. To compress images in such a wanted way, multiple classical lossy image compression techniques are compared to several AI-based compression models provided by and trained with the CompressAI toolbox using python. These different compression techniques are compared in compression ratio, multiple image similarity measures and, most importantly, the prediction accuracy from label-free models on compressed images. We found that AI-based compression techniques largely outperform the classic ones and will minimally affect the downstream label-free task in 2D cases. In the end, we hope the present study could shed light on the potential of deep learning based image compression and the impact of image compression on downstream deep learning based image analysis models.
</details>
<details>
<summary>摘要</summary>
随着现代微镜和生物成像技术的快速发展，生成的成像数据量已达到历史高点，对当今数据基础设施pose了巨大挑战。一种常见的方法是图像压缩，以减少数据大小。本研究 Compares classic and deep learning based image compression methods, and their impact on deep learning based image processing models. 用作比较和分析的例子应用是label-free预测模型（即从明亮图像预测 fluorescent image）。有效地压缩图像可以减少数据大小，而无需产生重要信息的损失，因此可以减轻数据管理基础设施的负担和允许数据在网络上快速传输或云计算。在python中使用CompressAI工具箱进行了多种класси型损失图像压缩技术的比较，以及几种基于AI的压缩模型。这些不同的压缩技术在压缩率、多个图像相似度度量和最重要的预测精度上进行了比较。我们发现，基于AI的压缩技术在2D情况下较Classic的压缩技术有很大的优势，并且对下游label-free任务的影响较小。我们希望这一研究可以把关于深度学习基于图像压缩的潜在能力和图像压缩对深度学习基于图像分析模型的影响 shed some light on。
</details></li>
</ul>
<hr>
<h2 id="Towards-Evaluating-Transfer-based-Attacks-Systematically-Practically-and-Fairly"><a href="#Towards-Evaluating-Transfer-based-Attacks-Systematically-Practically-and-Fairly" class="headerlink" title="Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly"></a>Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01323">http://arxiv.org/abs/2311.01323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen</li>
<li>for: 本研究旨在提供一个标准化的攻击比较 bencmark，以系统地、公平地、实际地评估对黑盒神经网络模型的攻击方法。</li>
<li>methods: 本研究使用了30多种转移基于攻击方法，包括各种攻击模式、攻击方法、攻击评估方法等。</li>
<li>results: 本研究对25种受到攻击的substitute&#x2F;victim模型进行了完整的评估，获得了新的见解和指导方针，可以帮助未来的攻击评估。<details>
<summary>Abstract</summary>
The adversarial vulnerability of deep neural networks (DNNs) has drawn great attention due to the security risk of applying these models in real-world applications. Based on transferability of adversarial examples, an increasing number of transfer-based methods have been developed to fool black-box DNN models whose architecture and parameters are inaccessible. Although tremendous effort has been exerted, there still lacks a standardized benchmark that could be taken advantage of to compare these methods systematically, fairly, and practically. Our investigation shows that the evaluation of some methods needs to be more reasonable and more thorough to verify their effectiveness, to avoid, for example, unfair comparison and insufficient consideration of possible substitute/victim models. Therefore, we establish a transfer-based attack benchmark (TA-Bench) which implements 30+ methods. In this paper, we evaluate and compare them comprehensively on 25 popular substitute/victim models on ImageNet. New insights about the effectiveness of these methods are gained and guidelines for future evaluations are provided. Code at: https://github.com/qizhangli/TA-Bench.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）的敌对攻击漏洞引起了广泛的关注，因为它们在实际应用中的安全风险较高。基于传输性的攻击方法的开发，随着黑盒模型的应用，逐渐增加了一些传输性的攻击方法，以欺骗无法访问模型的 Architecture 和参数的黑盒模型。虽然努力很大，但是目前还缺乏一个标准化的准则，可以系统、公平、实用地比较这些方法。我们的调查发现，评估一些方法的需要更加合理、更加全面，以避免例如不公平的比较和可能的代用/受害者模型的不足考虑。因此，我们建立了一个基于传输的攻击准则（TA-Bench），它实现了30多种方法。在这篇论文中，我们对25个popular substitute/victim模型进行了全面的评估和比较，从而获得了新的洞察和指导。代码在：https://github.com/qizhangli/TA-Bench。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Fusion-Transformer-for-Multisequence-MRI"><a href="#Hybrid-Fusion-Transformer-for-Multisequence-MRI" class="headerlink" title="Hybrid-Fusion Transformer for Multisequence MRI"></a>Hybrid-Fusion Transformer for Multisequence MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01308">http://arxiv.org/abs/2311.01308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihoon Cho, Jinah Park</li>
<li>for: 这个论文主要目标是为了提高多模态MRI图像分割的精度。</li>
<li>methods: 该论文提出了一种hybrid fusion transformer（HFTrans）方法，利用不同的多模态MRI序列特性，并通过Transformer层进行特征集成。</li>
<li>results: 实验表明，提出的hybrid-fusion方法在三维医疗图像分割任务中表现出色，在BraTS2020和MRBrainS18两个公共数据集上比前一个状态的方法更高精度。<details>
<summary>Abstract</summary>
Medical segmentation has grown exponentially through the advent of a fully convolutional network (FCN), and we have now reached a turning point through the success of Transformer. However, the different characteristics of the modality have not been fully integrated into Transformer for medical segmentation. In this work, we propose the novel hybrid fusion Transformer (HFTrans) for multisequence MRI image segmentation. We take advantage of the differences among multimodal MRI sequences and utilize the Transformer layers to integrate the features extracted from each modality as well as the features of the early fused modalities. We validate the effectiveness of our hybrid-fusion method in three-dimensional (3D) medical segmentation. Experiments on two public datasets, BraTS2020 and MRBrainS18, show that the proposed method outperforms previous state-of-the-art methods on the task of brain tumor segmentation and brain structure segmentation.
</details>
<details>
<summary>摘要</summary>
医学分割技术在全 convolutional network（FCN）的出现后 exponentiates 快速增长，而现在已经达到了转折点，这是由于 transformer 的成功。然而，不同的模态特征还没有被完全 integrate 到 transformer 中 для医学分割。在这项工作中，我们提议一种新的 hybrid fusion transformer（HFTrans） для多sequences MRI图像分割。我们利用不同的多modal MRI sequence的特征，并使用 transformer 层将每个模式中提取的特征和早期合并的特征集成。我们在三维医学分割中验证了我们的 hybrid-fusion 方法的有效性。在 BraTS2020 和 MRBrainS18 两个公共数据集上进行了实验，并确认了我们的方法在脑肿瘤分割和脑结构分割任务上的优于前一个state-of-the-art方法。
</details></li>
</ul>
<hr>
<h2 id="DP-Mix-Mixup-based-Data-Augmentation-for-Differentially-Private-Learning"><a href="#DP-Mix-Mixup-based-Data-Augmentation-for-Differentially-Private-Learning" class="headerlink" title="DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning"></a>DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01295">http://arxiv.org/abs/2311.01295</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenxuan-bao/dp-mix">https://github.com/wenxuan-bao/dp-mix</a></li>
<li>paper_authors: Wenxuan Bao, Francesco Pittaluga, Vijay Kumar B G, Vincent Bindschaedler</li>
<li>for: 提高计算机视觉模型的通用性，特别是在训练数据有限的情况下。</li>
<li>methods: 使用多样化数据 augmentation技术，如简单的图像变换和组合，以提高计算机视觉模型的泛化能力。</li>
<li>results: 提出了两种专门针对权谱学习的数据增强技术，包括DP-Mix_Self和DP-Mix_Diff，可以在多个数据集和设置下达到最佳性能。<details>
<summary>Abstract</summary>
Data augmentation techniques, such as simple image transformations and combinations, are highly effective at improving the generalization of computer vision models, especially when training data is limited. However, such techniques are fundamentally incompatible with differentially private learning approaches, due to the latter's built-in assumption that each training image's contribution to the learned model is bounded. In this paper, we investigate why naive applications of multi-sample data augmentation techniques, such as mixup, fail to achieve good performance and propose two novel data augmentation techniques specifically designed for the constraints of differentially private learning. Our first technique, DP-Mix_Self, achieves SoTA classification performance across a range of datasets and settings by performing mixup on self-augmented data. Our second technique, DP-Mix_Diff, further improves performance by incorporating synthetic data from a pre-trained diffusion model into the mixup process. We open-source the code at https://github.com/wenxuan-Bao/DP-Mix.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的数据扩充技术，如简单的图像变换和组合，对计算机视觉模型的通用性进行了高度的改进，尤其是在训练数据scarce情况下。然而，这些技术与异质性学习方法不兼容，因为后者假设每个训练图像对学习的模型做出的贡献是有限的。在这篇论文中，我们 investigate why naive应用多样样本数据扩充技术，如mixup，无法达到好性能，并提出了两种专门针对异质性学习的数据扩充技术。我们的第一种技术，DP-Mix_Self，在多个 dataset 和设置中达到了 SoTA 分类性能，通过在自增强数据上进行mixup。我们的第二种技术，DP-Mix_Diff，进一步提高性能，通过在扩充过程中包含Synthetic数据来增强mixup。我们将代码开源在https://github.com/wenxuan-Bao/DP-Mix上。
</details></li>
</ul>
<hr>
<h2 id="Joint-3D-Shape-and-Motion-Estimation-from-Rolling-Shutter-Light-Field-Images"><a href="#Joint-3D-Shape-and-Motion-Estimation-from-Rolling-Shutter-Light-Field-Images" class="headerlink" title="Joint 3D Shape and Motion Estimation from Rolling Shutter Light-Field Images"></a>Joint 3D Shape and Motion Estimation from Rolling Shutter Light-Field Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01292">http://arxiv.org/abs/2311.01292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hermes McGriff, Renato Martins, Nicolas Andreff, Cédric Demonceaux</li>
<li>for:  Addresses the problem of 3D reconstruction of scenes from a single image captured by a light-field camera equipped with a rolling shutter sensor.</li>
<li>methods:  Leverages the 3D information cues present in the light-field and the motion information provided by the rolling shutter effect, with a generic model for the imaging process and a two-stage algorithm that minimizes the re-projection error.</li>
<li>results:  Provides an instantaneous 3D shape-and-pose-and-velocity sensing paradigm, with a new benchmark dataset and several experiments conducted for different scenes and types of motions to demonstrate the effectiveness and advantages of the approach.Here is the same information in Traditional Chinese:</li>
<li>for:  Addresses the problem of 3D reconstruction of scenes from a single image captured by a light-field camera equipped with a rolling shutter sensor.</li>
<li>methods:  Leverages the 3D information cues present in the light-field and the motion information provided by the rolling shutter effect, with a generic model for the imaging process and a two-stage algorithm that minimizes the re-projection error.</li>
<li>results:  Provides an instantaneous 3D shape-and-pose-and-velocity sensing paradigm, with a new benchmark dataset and several experiments conducted for different scenes and types of motions to demonstrate the effectiveness and advantages of the approach.<details>
<summary>Abstract</summary>
In this paper, we propose an approach to address the problem of 3D reconstruction of scenes from a single image captured by a light-field camera equipped with a rolling shutter sensor. Our method leverages the 3D information cues present in the light-field and the motion information provided by the rolling shutter effect. We present a generic model for the imaging process of this sensor and a two-stage algorithm that minimizes the re-projection error while considering the position and motion of the camera in a motion-shape bundle adjustment estimation strategy. Thereby, we provide an instantaneous 3D shape-and-pose-and-velocity sensing paradigm. To the best of our knowledge, this is the first study to leverage this type of sensor for this purpose. We also present a new benchmark dataset composed of different light-fields showing rolling shutter effects, which can be used as a common base to improve the evaluation and tracking the progress in the field. We demonstrate the effectiveness and advantages of our approach through several experiments conducted for different scenes and types of motions. The source code and dataset are publicly available at: https://github.com/ICB-Vision-AI/RSLF
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法，用于从单个拍摄的图像中重建场景的3D形态。我们的方法利用了光场中的3D信息征ifiers和滚动镜头效果提供的运动信息。我们提出了一个通用的捕捉过程模型和一种两个阶段算法，以最小化投影误差，同时考虑摄像机的位置和运动。因此，我们提供了一种实时的3D形态、位置和速度探测方法。根据我们所知，这是首次利用这种传感器来实现这种目的。我们还提供了一个新的比较基准数据集，包括不同的光场，这可以用作评估和跟踪领域的共同基准。我们通过对不同场景和运动类型进行多个实验，证明了我们的方法的有效性和优势。源代码和数据集可以在以下链接中下载：https://github.com/ICB-Vision-AI/RSLF。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Knowledge-from-CNN-Transformer-Models-for-Enhanced-Human-Action-Recognition"><a href="#Distilling-Knowledge-from-CNN-Transformer-Models-for-Enhanced-Human-Action-Recognition" class="headerlink" title="Distilling Knowledge from CNN-Transformer Models for Enhanced Human Action Recognition"></a>Distilling Knowledge from CNN-Transformer Models for Enhanced Human Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01283">http://arxiv.org/abs/2311.01283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamid Ahmadabadi, Omid Nejati Manzari, Ahmad Ayatollahi</li>
<li>for: 提高人体动作识别的性能和效率，通过知识传播和 CNN 和 ViT 模型的组合。</li>
<li>methods: 使用 Transformer 视网膜作为学生模型，而 convolutional network 作为教师模型。教师模型提取本地图像特征，而学生模型通过注意力机制关注全图像特征。采用 Vision Transformer（ViT）框架，并评估多种变体的 ViT，包括 PVT、Convit、MVIT、Swin Transformer 和 Twins。</li>
<li>results: 对 Stanford 40 数据集进行人体动作识别任务，通过知识传播训练学生模型，相比常规训练方法，得到了显著提高的准确率和 mAP。这些结果表明，将本地和全图像特征结合在一起可以提高动作识别任务的性能。<details>
<summary>Abstract</summary>
This paper presents a study on improving human action recognition through the utilization of knowledge distillation, and the combination of CNN and ViT models. The research aims to enhance the performance and efficiency of smaller student models by transferring knowledge from larger teacher models. The proposed method employs a Transformer vision network as the student model, while a convolutional network serves as the teacher model. The teacher model extracts local image features, whereas the student model focuses on global features using an attention mechanism. The Vision Transformer (ViT) architecture is introduced as a robust framework for capturing global dependencies in images. Additionally, advanced variants of ViT, namely PVT, Convit, MVIT, Swin Transformer, and Twins, are discussed, highlighting their contributions to computer vision tasks. The ConvNeXt model is introduced as a teacher model, known for its efficiency and effectiveness in computer vision. The paper presents performance results for human action recognition on the Stanford 40 dataset, comparing the accuracy and mAP of student models trained with and without knowledge distillation. The findings illustrate that the suggested approach significantly improves the accuracy and mAP when compared to training networks under regular settings. These findings emphasize the potential of combining local and global features in action recognition tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-Deep-Learning-Image-Super-Resolution-for-Iris-Recognition"><a href="#Exploring-Deep-Learning-Image-Super-Resolution-for-Iris-Recognition" class="headerlink" title="Exploring Deep Learning Image Super-Resolution for Iris Recognition"></a>Exploring Deep Learning Image Super-Resolution for Iris Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01241">http://arxiv.org/abs/2311.01241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Ribeiro, Andreas Uhl, Fernando Alonso-Fernandez, Reuben A. Farrugia</li>
<li>for: 这个论文是为了检验深度学习方法在低分辨率图像到高分辨率图像的映射问题上的能力。</li>
<li>methods: 这个论文使用了两种深度学习单图超解决方法：堆叠自适应网络（SAE）和卷积神经网络（CNN），以实现快速速度、保持地方信息和减少噪声的目的。</li>
<li>results: 实验结果表明，深度学习方法在一个 Near-infrared iris 图像库中的评估和识别实验中表现出色，超过了与之比较的算法。<details>
<summary>Abstract</summary>
In this work we test the ability of deep learning methods to provide an end-to-end mapping between low and high resolution images applying it to the iris recognition problem. Here, we propose the use of two deep learning single-image super-resolution approaches: Stacked Auto-Encoders (SAE) and Convolutional Neural Networks (CNN) with the most possible lightweight structure to achieve fast speed, preserve local information and reduce artifacts at the same time. We validate the methods with a database of 1.872 near-infrared iris images with quality assessment and recognition experiments showing the superiority of deep learning approaches over the compared algorithms.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们测试了深度学习方法是否可以提供低到高分辨率图像的端到端映射，并应用于芳心识别问题。我们提议使用两种深度学习单图超解析方法：堆式自适应神经网络（SAE）和卷积神经网络（CNN），以达到快速速度、保持本地信息和减少噪声的目的。我们验证了这些方法使用1.872个近红外芳心图像库，并进行评估和识别实验，显示深度学习方法比比较算法更出色。
</details></li>
</ul>
<hr>
<h2 id="Log-Likelihood-Score-Level-Fusion-for-Improved-Cross-Sensor-Smartphone-Periocular-Recognition"><a href="#Log-Likelihood-Score-Level-Fusion-for-Improved-Cross-Sensor-Smartphone-Periocular-Recognition" class="headerlink" title="Log-Likelihood Score Level Fusion for Improved Cross-Sensor Smartphone Periocular Recognition"></a>Log-Likelihood Score Level Fusion for Improved Cross-Sensor Smartphone Periocular Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01237">http://arxiv.org/abs/2311.01237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Alonso-Fernandez, Kiran B. Raja, Christoph Busch, Josef Bigun</li>
<li>for: 提高不同摄像头数据的可比性和识别率</li>
<li>methods: 使用多比较器的拟合方法，基于线性逻辑回归，将各摄像头的分布调整到共同的概率领域</li>
<li>results: 实现对不同摄像头数据的融合，提高 périocular 性能，降低cross-sensor EER达40%<details>
<summary>Abstract</summary>
The proliferation of cameras and personal devices results in a wide variability of imaging conditions, producing large intra-class variations and a significant performance drop when images from heterogeneous environments are compared. However, many applications require to deal with data from different sources regularly, thus needing to overcome these interoperability problems. Here, we employ fusion of several comparators to improve periocular performance when images from different smartphones are compared. We use a probabilistic fusion framework based on linear logistic regression, in which fused scores tend to be log-likelihood ratios, obtaining a reduction in cross-sensor EER of up to 40% due to the fusion. Our framework also provides an elegant and simple solution to handle signals from different devices, since same-sensor and cross-sensor score distributions are aligned and mapped to a common probabilistic domain. This allows the use of Bayes thresholds for optimal decision-making, eliminating the need of sensor-specific thresholds, which is essential in operational conditions because the threshold setting critically determines the accuracy of the authentication process in many applications.
</details>
<details>
<summary>摘要</summary>
“由于相机和个人设备的普遍存在，导致图像环境的差异较大，从不同设备获取的图像之间存在大量的内类差异，这导致对图像进行比较时表现下降。然而，许多应用程序需要定期处理来自不同源的数据，因此需要解决这些可操作性问题。我们采用多比较器的合并方法来改进 périocular 性能，使用线性логистиック回归框架，在这个框架中，融合后的分数倾向于是Log-likelihood比率，从而实现了降低跨传感器EER的目标，最多降低40%。我们的框架还提供了一种简单和易于处理不同设备的信号的方法，因为同传感器和跨传感器分布是被映射到一个共同的 probabilistic 领域，这使得可以使用 bayes 阈值进行优化的决策，从而消除传感器特定的阈值的需求，这在操作条件下是非常重要的，因为阈值设定对图像认证过程中的精度具有 kritical 的作用。”
</details></li>
</ul>
<hr>
<h2 id="Robust-Feature-Learning-and-Global-Variance-Driven-Classifier-Alignment-for-Long-Tail-Class-Incremental-Learning"><a href="#Robust-Feature-Learning-and-Global-Variance-Driven-Classifier-Alignment-for-Long-Tail-Class-Incremental-Learning" class="headerlink" title="Robust Feature Learning and Global Variance-Driven Classifier Alignment for Long-Tail Class Incremental Learning"></a>Robust Feature Learning and Global Variance-Driven Classifier Alignment for Long-Tail Class Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01227">http://arxiv.org/abs/2311.01227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JAYATEJAK/GVAlign">https://github.com/JAYATEJAK/GVAlign</a></li>
<li>paper_authors: Jayateja Kalla, Soma Biswas</li>
<li>for: 增强长尾类逐步学习，使模型逐步学习新类，并mitigate catastrophic forgetting在长尾数据分布下。</li>
<li>methods: 利用全球差异作为有用的度量，并在第二阶段使用类prototype来实现类ifierAlignment，从而Capture类属性，消除数据平衡或另外层次调整的需求。</li>
<li>results: 在CIFAR-100和ImageNet-Subset datasets上进行了广泛的实验，证明了该方法在多种长尾CIL场景中的超越性，并且在不同的长尾类 incremental learning情况下保持了优异性。<details>
<summary>Abstract</summary>
This paper introduces a two-stage framework designed to enhance long-tail class incremental learning, enabling the model to progressively learn new classes, while mitigating catastrophic forgetting in the context of long-tailed data distributions. Addressing the challenge posed by the under-representation of tail classes in long-tail class incremental learning, our approach achieves classifier alignment by leveraging global variance as an informative measure and class prototypes in the second stage. This process effectively captures class properties and eliminates the need for data balancing or additional layer tuning. Alongside traditional class incremental learning losses in the first stage, the proposed approach incorporates mixup classes to learn robust feature representations, ensuring smoother boundaries. The proposed framework can seamlessly integrate as a module with any class incremental learning method to effectively handle long-tail class incremental learning scenarios. Extensive experimentation on the CIFAR-100 and ImageNet-Subset datasets validates the approach's efficacy, showcasing its superiority over state-of-the-art techniques across various long-tail CIL settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimal-Transport-Guided-Conditional-Score-Based-Diffusion-Models"><a href="#Optimal-Transport-Guided-Conditional-Score-Based-Diffusion-Models" class="headerlink" title="Optimal Transport-Guided Conditional Score-Based Diffusion Models"></a>Optimal Transport-Guided Conditional Score-Based Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01226">http://arxiv.org/abs/2311.01226</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xjtu-xgu/otcs">https://github.com/xjtu-xgu/otcs</a></li>
<li>paper_authors: Xiang Gu, Liwei Yang, Jian Sun, Zongben Xu</li>
<li>for:  conditional generation of target data with paired data as condition</li>
<li>methods:  optimal transport-guided conditional score-based diffusion model (OTCS)</li>
<li>results:  effective training of the conditional score-based model for unpaired or partially paired settings, with theoretical proof of data transport in optimal transport.Here’s the full text in Simplified Chinese:</li>
<li>for: 本文提出了一种基于匹配关系的条件分布模型（OTCS），用于无或半对数据的条件生成。</li>
<li>methods:  OTCS 使用 $L_2$-正则化不监督或半监督的最优运输来建立对不对数据的 Coupling 关系，然后基于这个 Coupling 关系来训练条件分布模型。</li>
<li>results:  OTCS 在无或半对super-resolution 和图像转换 tasks 上进行了广泛的实验，并证明了其效果性。从Optimal Transport的视角来看，OTCS 实现了数据的传输，这是一个对大规模数据集的挑战。我们还提供了一个 theoretically 的证明，证明 OTCS 实现了数据传输。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/XJTU-XGU/OTCS%7D">https://github.com/XJTU-XGU/OTCS}</a> 上获取。<details>
<summary>Abstract</summary>
Conditional score-based diffusion model (SBDM) is for conditional generation of target data with paired data as condition, and has achieved great success in image translation. However, it requires the paired data as condition, and there would be insufficient paired data provided in real-world applications. To tackle the applications with partially paired or even unpaired dataset, we propose a novel Optimal Transport-guided Conditional Score-based diffusion model (OTCS) in this paper. We build the coupling relationship for the unpaired or partially paired dataset based on $L_2$-regularized unsupervised or semi-supervised optimal transport, respectively. Based on the coupling relationship, we develop the objective for training the conditional score-based model for unpaired or partially paired settings, which is based on a reformulation and generalization of the conditional SBDM for paired setting. With the estimated coupling relationship, we effectively train the conditional score-based model by designing a ``resampling-by-compatibility'' strategy to choose the sampled data with high compatibility as guidance. Extensive experiments on unpaired super-resolution and semi-paired image-to-image translation demonstrated the effectiveness of the proposed OTCS model. From the viewpoint of optimal transport, OTCS provides an approach to transport data across distributions, which is a challenge for OT on large-scale datasets. We theoretically prove that OTCS realizes the data transport in OT with a theoretical bound. Code is available at \url{https://github.com/XJTU-XGU/OTCS}.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用 Conditional Score-based Diffusion Model（SBDM）可以实现目标数据的条件生成，但是它需要对condition paired的数据。在实际应用中，可能无法获得充分的paired数据。为了解决这个问题，我们在这篇论文中提出了一种新的 Optimal Transport-guided Conditional Score-based Diffusion Model（OTCS）。我们通过 $L_2$-regularized unsupervised或半supervised Optimal Transport来建立coupling关系，并基于这个coupling关系来定义Objective для条件 SBDM 的训练。通过使用估计的coupling关系，我们可以有效地训练条件 Score-based Model。我们设计了一种“重采样-by-compatibility”策略，以选择与高兼容性的样本作为导航。我们在无对应数据和半对应数据上进行了广泛的实验，并证明了我们的 OTCS 模型的有效性。从optimal transport的视角来看，OTCS 提供了将数据传输到分布上的方法，这是对大规模数据集的optimal transport而言是一个挑战。我们理论上证明了 OTCS 实现了数据传输在 OT 中的理论上的 bound。代码可以在 \url{https://github.com/XJTU-XGU/OTCS} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Convergent-plug-and-play-with-proximal-denoiser-and-unconstrained-regularization-parameter"><a href="#Convergent-plug-and-play-with-proximal-denoiser-and-unconstrained-regularization-parameter" class="headerlink" title="Convergent plug-and-play with proximal denoiser and unconstrained regularization parameter"></a>Convergent plug-and-play with proximal denoiser and unconstrained regularization parameter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01216">http://arxiv.org/abs/2311.01216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Hurault, Antonin Chambolle, Arthur Leclaire, Nicolas Papadakis</li>
<li>for: 这篇论文的目的是提供新的抽象证明，以解决图像反问题中的抽象问题。</li>
<li>methods: 这篇论文使用的方法是基于插入预训练的噪声矩阵的PnP算法，包括Proximal Gradient Descent（PGD）和Douglas-Rachford Splitting（DRS）。</li>
<li>results: 该论文的实验研究表明，使用这两种解决方案可以提高图像恢复的准确率。<details>
<summary>Abstract</summary>
In this work, we present new proofs of convergence for Plug-and-Play (PnP) algorithms. PnP methods are efficient iterative algorithms for solving image inverse problems where regularization is performed by plugging a pre-trained denoiser in a proximal algorithm, such as Proximal Gradient Descent (PGD) or Douglas-Rachford Splitting (DRS). Recent research has explored convergence by incorporating a denoiser that writes exactly as a proximal operator. However, the corresponding PnP algorithm has then to be run with stepsize equal to $1$. The stepsize condition for nonconvex convergence of the proximal algorithm in use then translates to restrictive conditions on the regularization parameter of the inverse problem. This can severely degrade the restoration capacity of the algorithm. In this paper, we present two remedies for this limitation. First, we provide a novel convergence proof for PnP-DRS that does not impose any restrictions on the regularization parameter. Second, we examine a relaxed version of the PGD algorithm that converges across a broader range of regularization parameters. Our experimental study, conducted on deblurring and super-resolution experiments, demonstrate that both of these solutions enhance the accuracy of image restoration.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提供了新的收敛证明 для插入式游戏（PnP）算法。PnP方法是高效的迭代算法，用于解决图像反转问题，其中的正则化是通过插入预训练的噪声除除器来实现，如距离梯度下降（PGD）或道格拉斯-蕾舍分裂（DRS）。Recent research has explored convergence by incorporating a denoiser that writes exactly as a proximal operator. However, the corresponding PnP algorithm has then to be run with stepsize equal to $1$. The stepsize condition for nonconvex convergence of the proximal algorithm in use then translates to restrictive conditions on the regularization parameter of the inverse problem. This can severely degrade the restoration capacity of the algorithm. In this paper, we present two remedies for this limitation. First, we provide a novel convergence proof for PnP-DRS that does not impose any restrictions on the regularization parameter. Second, we examine a relaxed version of the PGD algorithm that converges across a broader range of regularization parameters. Our experimental study, conducted on deblurring and super-resolution experiments, demonstrate that both of these solutions enhance the accuracy of image restoration.Note: The translation is in Simplified Chinese, which is one of the two standard Chinese writing systems. The other system is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="High-Quality-Animatable-Dynamic-Garment-Reconstruction-from-Monocular-Videos"><a href="#High-Quality-Animatable-Dynamic-Garment-Reconstruction-from-Monocular-Videos" class="headerlink" title="High-Quality Animatable Dynamic Garment Reconstruction from Monocular Videos"></a>High-Quality Animatable Dynamic Garment Reconstruction from Monocular Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01214">http://arxiv.org/abs/2311.01214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiongzheng Li, Jinsong Zhang, Yu-Kun Lai, Jingyu Yang, Kun Li</li>
<li>for:  reconstruction of high-quality animatable dynamic garments from monocular videos</li>
<li>methods: learnable garment deformation network, multi-hypothesis deformation module</li>
<li>results: high-quality dynamic garments with coherent surface details, can be easily animated under unseen poses<details>
<summary>Abstract</summary>
Much progress has been made in reconstructing garments from an image or a video. However, none of existing works meet the expectations of digitizing high-quality animatable dynamic garments that can be adjusted to various unseen poses. In this paper, we propose the first method to recover high-quality animatable dynamic garments from monocular videos without depending on scanned data. To generate reasonable deformations for various unseen poses, we propose a learnable garment deformation network that formulates the garment reconstruction task as a pose-driven deformation problem. To alleviate the ambiguity estimating 3D garments from monocular videos, we design a multi-hypothesis deformation module that learns spatial representations of multiple plausible deformations. Experimental results on several public datasets demonstrate that our method can reconstruct high-quality dynamic garments with coherent surface details, which can be easily animated under unseen poses. The code will be provided for research purposes.
</details>
<details>
<summary>摘要</summary>
很多进步已经被成功地应用于从图像或视频中重建衣服。然而，现有的所有方法都不能满足高质量动态衣服的数字化，可以根据不同的未知pose进行调整。在这篇论文中，我们提出了首个不 dependence on scanned data 的简单视频中高质量动态衣服重建方法。为了生成不同pose下的合理的变形，我们提议了一种学习型衣服变形网络，将衣服重建任务定义为pose驱动的变形问题。为了解决来自单视频中的3D衣服估计的ambiguity，我们设计了多种可能性变形模块，这些模块学习了多个可能的变形的空间表示。我们的方法在多个公共数据集上进行了实验，结果表明我们可以重建高质量的动态衣服，并且可以轻松地在未知pose下进行动画。我们将提供代码供研究用途。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Scene-Graph-Generation-Based-on-an-Edge-Dual-Scene-Graph-and-Message-Passing-Neural-Network"><a href="#Semantic-Scene-Graph-Generation-Based-on-an-Edge-Dual-Scene-Graph-and-Message-Passing-Neural-Network" class="headerlink" title="Semantic Scene Graph Generation Based on an Edge Dual Scene Graph and Message Passing Neural Network"></a>Semantic Scene Graph Generation Based on an Edge Dual Scene Graph and Message Passing Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01192">http://arxiv.org/abs/2311.01192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeongjin Kim, Sangwon Kim, Jong Taek Lee, Byoung Chul Ko</li>
<li>for: 提高Scene Graph Generation（SGG）的精度和可靠性，使其能够更好地捕捉图像中对象之间的复杂关系和互动。</li>
<li>methods: 基于Edge Dual Scene Graph（EdgeSGG）和Dual Message Passing Neural Network（DualMPNN），可以更好地捕捉图像中对象之间的richContextual interactions，并且可以更精确地预测对象之间的关系。</li>
<li>results: 与State-of-the-Art（SoTA）方法进行比较，提出的模型在三个SGG任务上显示了substantial性能提升，并且在长尾分布上进行实验表明，在 integrate对象之间的关系时，可以有效 mitigate Existing long-tail problems。<details>
<summary>Abstract</summary>
Along with generative AI, interest in scene graph generation (SGG), which comprehensively captures the relationships and interactions between objects in an image and creates a structured graph-based representation, has significantly increased in recent years. However, relying on object-centric and dichotomous relationships, existing SGG methods have a limited ability to accurately predict detailed relationships. To solve these problems, a new approach to the modeling multiobject relationships, called edge dual scene graph generation (EdgeSGG), is proposed herein. EdgeSGG is based on a edge dual scene graph and Dual Message Passing Neural Network (DualMPNN), which can capture rich contextual interactions between unconstrained objects. To facilitate the learning of edge dual scene graphs with a symmetric graph structure, the proposed DualMPNN learns both object- and relation-centric features for more accurately predicting relation-aware contexts and allows fine-grained relational updates between objects. A comparative experiment with state-of-the-art (SoTA) methods was conducted using two public datasets for SGG operations and six metrics for three subtasks. Compared with SoTA approaches, the proposed model exhibited substantial performance improvements across all SGG subtasks. Furthermore, experiment on long-tail distributions revealed that incorporating the relationships between objects effectively mitigates existing long-tail problems.
</details>
<details>
<summary>摘要</summary>
accompanies the rise of generative AI, scene graph generation (SGG) has gained significant attention in recent years. However, existing SGG methods are limited in their ability to accurately predict detailed relationships due to their reliance on object-centric and dichotomous relationships. To address these issues, a new approach called edge dual scene graph generation (EdgeSGG) is proposed. EdgeSGG is based on an edge dual scene graph and a Dual Message Passing Neural Network (DualMPNN), which can capture rich contextual interactions between unconstrained objects. To facilitate the learning of edge dual scene graphs with a symmetric graph structure, the proposed DualMPNN learns both object- and relation-centric features for more accurately predicting relation-aware contexts and allows fine-grained relational updates between objects. A comparative experiment with state-of-the-art (SoTA) methods was conducted using two public datasets for SGG operations and six metrics for three subtasks. Compared with SoTA approaches, the proposed model exhibited substantial performance improvements across all SGG subtasks. Furthermore, experiment on long-tail distributions revealed that incorporating the relationships between objects effectively mitigates existing long-tail problems.Here is the translation in Traditional Chinese:随着生成AI的出现，Scene Graph Generation (SGG)在最近的年分内得到了很大的关注。然而，现有的SGG方法受到物件中心和二分法的限制，它们的预测细节关系的能力有限。为解决这些问题，一种新的方法called EdgeSGG被提出。EdgeSGG基于边dual scene graph和Dual Message Passing Neural Network (DualMPNN)，可以捕捉无结构物件之间的丰富contextual互动。为了促进边dual scene graph的学习，提出的DualMPNN将学习物件和关系中心的特征，以更精确地预测关系意识的上下文，并允许细化的关系更新。对于SoTA方法进行比较实验，使用了两个公共的数据集和六个度量来评估三个SGG任务。与SoTA方法相比，提出的模型在所有SGG任务上表现出substantial的性能改善。此外，对于长尾分布的实验显示，将物件之间的关系 интеグrez effectively mitigates existing long-tail problems。
</details></li>
</ul>
<hr>
<h2 id="Terrain-Informed-Self-Supervised-Learning-Enhancing-Building-Footprint-Extraction-from-LiDAR-Data-with-Limited-Annotations"><a href="#Terrain-Informed-Self-Supervised-Learning-Enhancing-Building-Footprint-Extraction-from-LiDAR-Data-with-Limited-Annotations" class="headerlink" title="Terrain-Informed Self-Supervised Learning: Enhancing Building Footprint Extraction from LiDAR Data with Limited Annotations"></a>Terrain-Informed Self-Supervised Learning: Enhancing Building Footprint Extraction from LiDAR Data with Limited Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01188">http://arxiv.org/abs/2311.01188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anuja Vats, David Völgyes, Martijn Vermeer, Marius Pedersen, Kiran Raja, Daniele S. M. Fantin, Jacob Alexander Hay</li>
<li>for: 这个研究的目的是提出一种基于深度学习的建筑图像分类方法，以便从remote sensing数据中提取 preciselocation building footprint maps。</li>
<li>methods: 这个方法使用自动生成的地形模型，从LiDAR数据中学习特点特征，并透过自我超级vised learning来对building segmentation进行推导。</li>
<li>results: 这个方法可以从仅有1%的标签（相当于25个标签的例子）中提取出高性能的建筑分类表现，并在几何对应中进一步提高表现。此外，这个方法还能够在实际应用中应用，并且比其他基于ImageNet预训练的方法表现更好。<details>
<summary>Abstract</summary>
Estimating building footprint maps from geospatial data is of paramount importance in urban planning, development, disaster management, and various other applications. Deep learning methodologies have gained prominence in building segmentation maps, offering the promise of precise footprint extraction without extensive post-processing. However, these methods face challenges in generalization and label efficiency, particularly in remote sensing, where obtaining accurate labels can be both expensive and time-consuming. To address these challenges, we propose terrain-aware self-supervised learning, tailored to remote sensing, using digital elevation models from LiDAR data. We propose to learn a model to differentiate between bare Earth and superimposed structures enabling the network to implicitly learn domain-relevant features without the need for extensive pixel-level annotations. We test the effectiveness of our approach by evaluating building segmentation performance on test datasets with varying label fractions. Remarkably, with only 1% of the labels (equivalent to 25 labeled examples), our method improves over ImageNet pre-training, showing the advantage of leveraging unlabeled data for feature extraction in the domain of remote sensing. The performance improvement is more pronounced in few-shot scenarios and gradually closes the gap with ImageNet pre-training as the label fraction increases. We test on a dataset characterized by substantial distribution shifts and labeling errors to demonstrate the generalizability of our approach. When compared to other baselines, including ImageNet pretraining and more complex architectures, our approach consistently performs better, demonstrating the efficiency and effectiveness of self-supervised terrain-aware feature learning.
</details>
<details>
<summary>摘要</summary>
估算建筑地图从地ospatial数据中是城市规划、开发、灾害管理等应用中的关键任务。深度学习方法在建筑分割地图中得到了广泛应用，可以准确地提取建筑地图 без需要大量后处理。然而，这些方法面临通用化和标签效率的挑战，特别是在遥感中，获得准确的标签可能会是时间和成本的投资。为解决这些挑战，我们提议使用地形自适应自监学习，适应遥感，使用雷达数据获得的数字高程模型。我们提议学习一种模型，可以区分无附加结构和地表上的结构，使得网络可以隐式地学习领域相关的特征，不需要大量的像素级标注。我们测试了我们的方法的效果，对测试数据集进行了不同标签分布的评估。非常remarkably，只使用1%的标签（相当于25个标注示例），我们的方法可以超越图像网络预训练，显示了在遥感领域中利用无标注数据进行特征提取的优势。性能提升随着标签分布的增加，在几个批量场景中，我们的方法的性能相对较高，示示了我们的方法的效率和效果。我们对一个具有显著的分布偏移和标签错误的数据集进行了测试，以示我们的方法的普适性。与其他基elines，包括图像网络预训练和更复杂的架构相比，我们的方法一直表现出色， demonstrating the efficiency and effectiveness of self-supervised terrain-aware feature learning.
</details></li>
</ul>
<hr>
<h2 id="Learning-Intra-and-Inter-Camera-Invariance-for-Isolated-Camera-Supervised-Person-Re-identification"><a href="#Learning-Intra-and-Inter-Camera-Invariance-for-Isolated-Camera-Supervised-Person-Re-identification" class="headerlink" title="Learning Intra and Inter-Camera Invariance for Isolated Camera Supervised Person Re-identification"></a>Learning Intra and Inter-Camera Invariance for Isolated Camera Supervised Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01155">http://arxiv.org/abs/2311.01155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Wang, Xiaojin Gong</li>
<li>for: 这种论文是为了研究在受到isoled camera supervised（ISCS）设置下进行人识别的情况。</li>
<li>methods: 该论文提出了一种新的方法，通过充分利用训练数据的变化来解决人识别下来。该方法包括在每个环境中构建style-consistent的环境，并在每个环境中进行prototype contrastive learning。同时，通过强制实施 intra-camera 增强不变性来消除相机偏见的影响。</li>
<li>results: 该论文在多个 benchmark 上进行了广泛的实验，并证明了该方法的有效性和超越性。<details>
<summary>Abstract</summary>
Supervised person re-identification assumes that a person has images captured under multiple cameras. However when cameras are placed in distance, a person rarely appears in more than one camera. This paper thus studies person re-ID under such isolated camera supervised (ISCS) setting. Instead of trying to generate fake cross-camera features like previous methods, we explore a novel perspective by making efficient use of the variation in training data. Under ISCS setting, a person only has limited images from a single camera, so the camera bias becomes a critical issue confounding ID discrimination. Cross-camera images are prone to being recognized as different IDs simply by camera style. To eliminate the confounding effect of camera bias, we propose to learn both intra- and inter-camera invariance under a unified framework. First, we construct style-consistent environments via clustering, and perform prototypical contrastive learning within each environment. Meanwhile, strongly augmented images are contrasted with original prototypes to enforce intra-camera augmentation invariance. For inter-camera invariance, we further design a much improved variant of multi-camera negative loss that optimizes the distance of multi-level negatives. The resulting model learns to be invariant to both subtle and severe style variation within and cross-camera. On multiple benchmarks, we conduct extensive experiments and validate the effectiveness and superiority of the proposed method. Code will be available at https://github.com/Terminator8758/IICI.
</details>
<details>
<summary>摘要</summary>
受监测人重识别假设有多个摄像头捕捉到同一个人的图像。然而，当摄像头远离时，人很少会出现在多个摄像头中。这篇论文因此研究了在孤立摄像头超级vised（ISCS）设定下进行人重识别。而不是尝试生成虚假的交叉摄像头特征，我们explore一种新的视角，即efficient地利用训练数据的变化。在ISCS设定下，一个人只有限制的图像来自单个摄像头，因此摄像头偏见成为人识别中的关键问题。交叉摄像头图像容易被识别为不同的ID，只是因为摄像头风格。为了消除摄像头偏见的影响，我们提议学习 both intra-和inter-摄像头不变性于一个统一框架下。首先，我们使用 clustering 构建 style-consistent 环境，并在每个环境中进行 prototypical contrastive learning。同时，我们使用强制加工的图像与原始评原核对进行 intra-camera 增强不变性。为了保证 inter-camera 不变性，我们还提出了一个大幅提高的多摄像头负面损失的改进版本。这使得模型学习到了内部和交叉摄像头中的不变性，并且对于严重和柔性的样式变化都具有抗预测能力。在多个标准列表上进行了广泛的实验，并证明了我们的方法的有效性和超越性。代码将在 https://github.com/Terminator8758/IICI 上提供。
</details></li>
</ul>
<hr>
<h2 id="AeroPath-An-airway-segmentation-benchmark-dataset-with-challenging-pathology"><a href="#AeroPath-An-airway-segmentation-benchmark-dataset-with-challenging-pathology" class="headerlink" title="AeroPath: An airway segmentation benchmark dataset with challenging pathology"></a>AeroPath: An airway segmentation benchmark dataset with challenging pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01138">http://arxiv.org/abs/2311.01138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/raidionics/aeropath">https://github.com/raidionics/aeropath</a></li>
<li>paper_authors: Karen-Helene Støverud, David Bouget, Andre Pedersen, Håkon Olav Leira, Thomas Langø, Erlend Fagertun Hofstad</li>
<li>for: 提高肺病患者的诊断和治疗效果，需要早期诊断和治疗。CT图像分析是诊断的关键之一，而高质量的气管树分割是 intervención 规划和直到 bronchoscopy 操作的必要条件。</li>
<li>methods: 我们提出了一种新的公共benchmark dataset（AeroPath），包含27个CT图像，来评估新的 automatic airway segmentation 方法。此外，我们还提出了一种多尺度融合设计，以便自动气管分割。</li>
<li>results: 我们的提案的模型在AeroPath dataset上预测了所有患者的正确的分割结果，并且能够抗衡各种病理变化，至少到第五代气管。此外，我们还开发了一个公开可用的在线应用程序，以便在新数据上测试我们的模型。<details>
<summary>Abstract</summary>
To improve the prognosis of patients suffering from pulmonary diseases, such as lung cancer, early diagnosis and treatment are crucial. The analysis of CT images is invaluable for diagnosis, whereas high quality segmentation of the airway tree are required for intervention planning and live guidance during bronchoscopy. Recently, the Multi-domain Airway Tree Modeling (ATM'22) challenge released a large dataset, both enabling training of deep-learning based models and bringing substantial improvement of the state-of-the-art for the airway segmentation task. However, the ATM'22 dataset includes few patients with severe pathologies affecting the airway tree anatomy. In this study, we introduce a new public benchmark dataset (AeroPath), consisting of 27 CT images from patients with pathologies ranging from emphysema to large tumors, with corresponding trachea and bronchi annotations. Second, we present a multiscale fusion design for automatic airway segmentation. Models were trained on the ATM'22 dataset, tested on the AeroPath dataset, and further evaluated against competitive open-source methods. The same performance metrics as used in the ATM'22 challenge were used to benchmark the different considered approaches. Lastly, an open web application is developed, to easily test the proposed model on new data. The results demonstrated that our proposed architecture predicted topologically correct segmentations for all the patients included in the AeroPath dataset. The proposed method is robust and able to handle various anomalies, down to at least the fifth airway generation. In addition, the AeroPath dataset, featuring patients with challenging pathologies, will contribute to development of new state-of-the-art methods. The AeroPath dataset and the web application are made openly available.
</details>
<details>
<summary>摘要</summary>
要改善患有肺病的患者的诊断和治疗效果，早期诊断和治疗是关键。CT图像分析是诊断的不可或缺的工具，而高质量的气管树分 segmentation 则是用于操作规划和直到生 bronchoscopy 的live导航中的必要条件。最近，多域气管树模型大会（ATM'22）挑战发布了大量数据，为深度学习基于模型的训练提供了条件，并为气管分 segmentation 任务带来了显著的状态艺术提升。然而，ATM'22 数据集包含少量患有肺动脉病理的患者，这些病理可能会影响气管树的解剖结构。在这项研究中，我们介绍了一个新的公共数据集（AeroPath），包含 27 个 CT 图像，这些图像来自患有肺动脉病理的患者，包括肺脏病和大型肿瘤，同时还包括气管和支气管的注释。其次，我们提出了一种多尺度融合设计用于自动气管分 segmentation。我们在 ATM'22 数据集上训练了模型，在 AeroPath 数据集上进行测试，并对开源方法进行比较。使用 ATM'22 挑战中使用的同样效果指标进行比较。最后，我们开发了一个开放的网络应用程序，以便轻松地在新数据上测试我们的提议方法。结果表明，我们的提议体系在 AeroPath 数据集上预测了所有患者的正确分 segmentation。我们的方法具有抗难度和能够处理多种畸形的能力，至少到第五代气管。此外，AeroPath 数据集， featuring 患有复杂病理的患者，将为开发新的状态艺术方法提供贡献。AeroPath 数据集和网络应用程序都是公开可用。
</details></li>
</ul>
<hr>
<h2 id="A-deep-learning-experiment-for-semantic-segmentation-of-overlapping-characters-in-palimpsests"><a href="#A-deep-learning-experiment-for-semantic-segmentation-of-overlapping-characters-in-palimpsests" class="headerlink" title="A deep learning experiment for semantic segmentation of overlapping characters in palimpsests"></a>A deep learning experiment for semantic segmentation of overlapping characters in palimpsests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01130">http://arxiv.org/abs/2311.01130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michela Perino, Michele Ginolfi, Anna Candida Felici, Michela Rosellini</li>
<li>for: 这项研究的目的是提出一种基于深度学习的Semantic Segmentation方法，用于在重叠的字符上分割个letter。</li>
<li>methods: 该方法使用了多spectral imaging技术和人工智能技术，包括深度学习的Semantic Segmentation算法，用于识别和分割重叠的字符。</li>
<li>results: 实验结果表明，该方法可以准确地分割重叠的字符，并且可以提高对Palimpsests的识别和分析效率。<details>
<summary>Abstract</summary>
Palimpsests refer to historical manuscripts where erased writings have been partially covered by the superimposition of a second writing. By employing imaging techniques, e.g., multispectral imaging, it becomes possible to identify features that are imperceptible to the naked eye, including faded and erased inks. When dealing with overlapping inks, Artificial Intelligence techniques can be utilized to disentangle complex nodes of overlapping letters. In this work, we propose deep learning-based semantic segmentation as a method for identifying and segmenting individual letters in overlapping characters. The experiment was conceived as a proof of concept, focusing on the palimpsests of the Ars Grammatica by Prisciano as a case study. Furthermore, caveats and prospects of our approach combined with multispectral imaging are also discussed.
</details>
<details>
<summary>摘要</summary>
某些抄写物件被称为磁带文献，这些文献中的字符串被部分覆盖了第二种写作。通过使用多spectral imaging技术，可以检测出覆盖不明文字符的特征，包括淡入的字符和抹除的字符。在多个字符之间相互重叠时，人工智能技术可以用来分解复杂的节点。在这种情况下，我们提出了深度学习基于semantic segmentation的方法，用于在重叠字符中识别和分类个字符。我们的实验是以证明性为目的，案例研究了普里斯尼亚诺的《语法 Grammatica》抄写物件。此外，我们还讨论了我们的方法的限制和前景。
</details></li>
</ul>
<hr>
<h2 id="Cheating-Depth-Enhancing-3D-Surface-Anomaly-Detection-via-Depth-Simulation"><a href="#Cheating-Depth-Enhancing-3D-Surface-Anomaly-Detection-via-Depth-Simulation" class="headerlink" title="Cheating Depth: Enhancing 3D Surface Anomaly Detection via Depth Simulation"></a>Cheating Depth: Enhancing 3D Surface Anomaly Detection via Depth Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01117">http://arxiv.org/abs/2311.01117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vitjanz/3dsr">https://github.com/vitjanz/3dsr</a></li>
<li>paper_authors: Vitjan Zavrtanik, Matej Kristan, Danijel Skočaj</li>
<li>for: 提高RGB基于表面异常检测方法的准确率和处理速度</li>
<li>methods: 提出了一种新的深度感知分割自动编码器（DADA）架构，以便同时学习RGB和3D数据的整体离散特征空间，以便3D表面异常检测</li>
<li>results: 实验结果表明，提出的方法可以在MVTec3D异常检测标准套件上达到最高精度和处理速度，超过所有现有的状态之异常检测方法<details>
<summary>Abstract</summary>
RGB-based surface anomaly detection methods have advanced significantly. However, certain surface anomalies remain practically invisible in RGB alone, necessitating the incorporation of 3D information. Existing approaches that employ point-cloud backbones suffer from suboptimal representations and reduced applicability due to slow processing. Re-training RGB backbones, designed for faster dense input processing, on industrial depth datasets is hindered by the limited availability of sufficiently large datasets. We make several contributions to address these challenges. (i) We propose a novel Depth-Aware Discrete Autoencoder (DADA) architecture, that enables learning a general discrete latent space that jointly models RGB and 3D data for 3D surface anomaly detection. (ii) We tackle the lack of diverse industrial depth datasets by introducing a simulation process for learning informative depth features in the depth encoder. (iii) We propose a new surface anomaly detection method 3DSR, which outperforms all existing state-of-the-art on the challenging MVTec3D anomaly detection benchmark, both in terms of accuracy and processing speed. The experimental results validate the effectiveness and efficiency of our approach, highlighting the potential of utilizing depth information for improved surface anomaly detection.
</details>
<details>
<summary>摘要</summary>
（i）我们提出了一种新的深度意识Discrete Autoencoder（DADA）建筑，它允许学习一个通用的离散准则空间，该空间同时模型RGB和3D数据 для3D表面异常检测。（ii）我们解决了工业深度数据集的有限性问题，通过引入一种学习深度特征的 simulations process。（iii）我们提出了一种新的3DSR方法，它在MVTec3D异常检测benchmark上表现出了比所有现有的国际状态最佳的性能，both in terms of accuracy和processing speed。我们的实验结果证明了我们的方法的有效性和高效性， highlighting the potential of utilizing depth information for improved surface anomaly detection.
</details></li>
</ul>
<hr>
<h2 id="H-NeXt-The-next-step-towards-roto-translation-invariant-networks"><a href="#H-NeXt-The-next-step-towards-roto-translation-invariant-networks" class="headerlink" title="H-NeXt: The next step towards roto-translation invariant networks"></a>H-NeXt: The next step towards roto-translation invariant networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01111">http://arxiv.org/abs/2311.01111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/karellat/h-next">https://github.com/karellat/h-next</a></li>
<li>paper_authors: Tomas Karella, Filip Sroubek, Jan Flusser, Jan Blazek, Vasek Kosik</li>
<li>for: 该论文目的是提出一种可以快速学习并在不同orientation下保持性的网络模型。</li>
<li>methods: 该论文使用了一种名为H-NeXt的网络模型，它包括一个对称矩阵的后向层、一个不变性池化层和一个分类层。</li>
<li>results: 该论文通过在不含扩展图像的训练集上训练H-NeXt网络，并在扩展测试集上进行分类，得到了与当前状态集成比的更高的表现。<details>
<summary>Abstract</summary>
The widespread popularity of equivariant networks underscores the significance of parameter efficient models and effective use of training data. At a time when robustness to unseen deformations is becoming increasingly important, we present H-NeXt, which bridges the gap between equivariance and invariance. H-NeXt is a parameter-efficient roto-translation invariant network that is trained without a single augmented image in the training set. Our network comprises three components: an equivariant backbone for learning roto-translation independent features, an invariant pooling layer for discarding roto-translation information, and a classification layer. H-NeXt outperforms the state of the art in classification on unaugmented training sets and augmented test sets of MNIST and CIFAR-10.
</details>
<details>
<summary>摘要</summary>
广泛的equivariant网络的普及，强调了参数效率模型和有效使用训练数据的重要性。在当今不可忽略的不visible deformation Robustness era，我们提出了H-NeXt，它在 equivariance和invariance之间填补了空白。H-NeXt是一种parameter-efficient的旋转翻译不变的网络，在没有一个扩展图像的训练集上培养。我们的网络包括三部分：一个恒等背景，用于学习旋转翻译独立的特征，一个不变pooling层，用于抛弃旋转信息，以及一个分类层。H-NeXt在MNIST和CIFAR-10的未扩展训练集和扩展测试集上的分类性能高于当前状态。
</details></li>
</ul>
<hr>
<h2 id="Learning-A-Multi-Task-Transformer-Via-Unified-And-Customized-Instruction-Tuning-For-Chest-Radiograph-Interpretation"><a href="#Learning-A-Multi-Task-Transformer-Via-Unified-And-Customized-Instruction-Tuning-For-Chest-Radiograph-Interpretation" class="headerlink" title="Learning A Multi-Task Transformer Via Unified And Customized Instruction Tuning For Chest Radiograph Interpretation"></a>Learning A Multi-Task Transformer Via Unified And Customized Instruction Tuning For Chest Radiograph Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01092">http://arxiv.org/abs/2311.01092</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/medhk23/omnifm-dr">https://github.com/medhk23/omnifm-dr</a></li>
<li>paper_authors: Lijian Xu, Ziyu Ni, Xinglong Liu, Xiaosong Wang, Hongsheng Li, Shaoting Zhang</li>
<li>For: 这个研究旨在探讨多模式深度学习模型在医学应用中的应用，并将多项任务集成为一个统一的变数损失函数，以提高诊断的可解释性。* Methods: 本研究使用自定义指令调整的变数深度学习模型，并将多项颜ppo任务集成为一个共同训练架构，以增加诊断的可解释性。* Results: 该模型在多个胸部X射影benchmark上 exhibits 高度的直接推论和调整性，并且还经过了三位放射学家的评估，证明了模型的解释性。<details>
<summary>Abstract</summary>
The emergence of multi-modal deep learning models has made significant impacts on clinical applications in the last decade. However, the majority of models are limited to single-tasking, without considering disease diagnosis is indeed a multi-task procedure. Here, we demonstrate a unified transformer model specifically designed for multi-modal clinical tasks by incorporating customized instruction tuning. We first compose a multi-task training dataset comprising 13.4 million instruction and ground-truth pairs (with approximately one million radiographs) for the customized tuning, involving both image- and pixel-level tasks. Thus, we can unify the various vision-intensive tasks in a single training framework with homogeneous model inputs and outputs to increase clinical interpretability in one reading. Finally, we demonstrate the overall superior performance of our model compared to prior arts on various chest X-ray benchmarks across multi-tasks in both direct inference and finetuning settings. Three radiologists further evaluate the generated reports against the recorded ones, which also exhibit the enhanced explainability of our multi-task model.
</details>
<details>
<summary>摘要</summary>
随着多modal深度学习模型的出现，在过去的一代，它们在临床应用中产生了重要的影响。然而，大多数模型都是单任务的，没有考虑到疾病诊断实际上是多任务的过程。在这里，我们演示了一种特有的转换器模型，专门为多modal临床任务而设计，通过自定义指令调整。我们首先组织了一个多任务训练集，包括1340万个指令和真实数据对（约100万个X射像），用于自定义调整。因此，我们可以在单一的训练框架中，将多种视觉沉浸任务集成起来，使得临床解释性提高。最后，我们比较了我们的模型与之前艺术的性能，在多任务情况下，Direct inference和微调设置中，都达到了总体更高的性能。三名医生还评估了我们生成的报告与记录的报告，这也表明了我们的多任务模型的解释性得到了进一步提高。
</details></li>
</ul>
<hr>
<h2 id="Enriching-Phrases-with-Coupled-Pixel-and-Object-Contexts-for-Panoptic-Narrative-Grounding"><a href="#Enriching-Phrases-with-Coupled-Pixel-and-Object-Contexts-for-Panoptic-Narrative-Grounding" class="headerlink" title="Enriching Phrases with Coupled Pixel and Object Contexts for Panoptic Narrative Grounding"></a>Enriching Phrases with Coupled Pixel and Object Contexts for Panoptic Narrative Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01091">http://arxiv.org/abs/2311.01091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianrui Hui, Zihan Ding, Junshi Huang, Xiaoming Wei, Xiaolin Wei, Jiao Dai, Jizhong Han, Si Liu</li>
<li>for: 本研究旨在提高图文描述的对应关系，即图像中的物体和文本描述之间的相互作用。</li>
<li>methods: 该研究提出了一种新的phrase-pixel-object transformer decoder（PPO-TD），该模型可以同时捕捉图像中细节和概念级别的信息，并且通过对应描述文本进行学习。此外，研究者还提出了一种phraseObject Contrastive Loss（POCL），用于更精准地聚合对应的phrase-object对。</li>
<li>results: 实验表明，该方法可以在PNG benchmark上达到新的状态值性能，与之前的方法相比，具有大的margin。<details>
<summary>Abstract</summary>
Panoptic narrative grounding (PNG) aims to segment things and stuff objects in an image described by noun phrases of a narrative caption. As a multimodal task, an essential aspect of PNG is the visual-linguistic interaction between image and caption. The previous two-stage method aggregates visual contexts from offline-generated mask proposals to phrase features, which tend to be noisy and fragmentary. The recent one-stage method aggregates only pixel contexts from image features to phrase features, which may incur semantic misalignment due to lacking object priors. To realize more comprehensive visual-linguistic interaction, we propose to enrich phrases with coupled pixel and object contexts by designing a Phrase-Pixel-Object Transformer Decoder (PPO-TD), where both fine-grained part details and coarse-grained entity clues are aggregated to phrase features. In addition, we also propose a PhraseObject Contrastive Loss (POCL) to pull closer the matched phrase-object pairs and push away unmatched ones for aggregating more precise object contexts from more phrase-relevant object tokens. Extensive experiments on the PNG benchmark show our method achieves new state-of-the-art performance with large margins.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Infusion-Internal-Diffusion-for-Video-Inpainting"><a href="#Infusion-Internal-Diffusion-for-Video-Inpainting" class="headerlink" title="Infusion: Internal Diffusion for Video Inpainting"></a>Infusion: Internal Diffusion for Video Inpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01090">http://arxiv.org/abs/2311.01090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Cherel, Andrés Almansa, Yann Gousseau, Alasdair Newson</li>
<li>for: 视频填充（video inpainting）任务是在视频中填充某个区域，以达到视觉上的满意度。</li>
<li>methods: 我们采用了扩散模型，它可以模型复杂的数据分布，包括图像和视频。我们采用了内部学习方法，这也使得我们的网络规模减少了。</li>
<li>results: 我们的方法可以在视频填充任务中达到状态机器的性能，特别是在动态背景和Texture中。我们的方法不需要支持元素，如光学流计算，因此它在动态Texture中表现更好。<details>
<summary>Abstract</summary>
Video inpainting is the task of filling a desired region in a video in a visually convincing manner. It is a very challenging task due to the high dimensionality of the signal and the temporal consistency required for obtaining convincing results. Recently, diffusion models have shown impressive results in modeling complex data distributions, including images and videos. Diffusion models remain nonetheless very expensive to train and perform inference with, which strongly restrict their application to video. We show that in the case of video inpainting, thanks to the highly auto-similar nature of videos, the training of a diffusion model can be restricted to the video to inpaint and still produce very satisfying results. This leads us to adopt an internal learning approch, which also allows for a greatly reduced network size. We call our approach "Infusion": an internal learning algorithm for video inpainting through diffusion. Due to our frugal network, we are able to propose the first video inpainting approach based purely on diffusion. Other methods require supporting elements such as optical flow estimation, which limits their performance in the case of dynamic textures for example. We introduce a new method for efficient training and inference of diffusion models in the context of internal learning. We split the diffusion process into different learning intervals which greatly simplifies the learning steps. We show qualititative and quantitative results, demonstrating that our method reaches state-of-the-art performance, in particular in the case of dynamic backgrounds and textures.
</details>
<details>
<summary>摘要</summary>
视频填充是填充一个 désirée 区域在视频中，以达到可观的效果。这是一个非常具有挑战性的任务，因为视频信号的维度很高，并且需要在时间上保持一致性以获得可靠的结果。最近，扩散模型在处理复杂数据分布中表现出色，包括图像和视频。但是，扩散模型在训练和推理中非常昂贵，这限制了它在视频填充中的应用。我们发现，在视频填充中，由于视频的高自动相似性，可以通过仅训练在填充视频中的扩散模型，以达到非常满意的结果。我们称这种方法为“扩散融合”（Infusion）。由于我们的网络较为减少，我们可以提出第一个基于扩散的视频填充方法。其他方法通常需要支持元素，如光流估计，这限制了它们在动态Texture中的表现。我们提出了一种高效的训练和推理扩散模型的方法，将扩散过程分成不同的学习间隔。我们显示了qualitative和quantitative结果，证明我们的方法可以达到领先的性能，特别是在动态背景和Texture中。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Multimodal-Information-Bottleneck-for-Multimodality-Classification"><a href="#Dynamic-Multimodal-Information-Bottleneck-for-Multimodality-Classification" class="headerlink" title="Dynamic Multimodal Information Bottleneck for Multimodality Classification"></a>Dynamic Multimodal Information Bottleneck for Multimodality Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01066">http://arxiv.org/abs/2311.01066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bii-wushuang/dmib">https://github.com/bii-wushuang/dmib</a></li>
<li>paper_authors: Yingying Fang, Shuang Wu, Sheng Zhang, Chaoyan Huang, Tieyong Zeng, Xiaodan Xing, Simon Walsh, Guang Yang</li>
<li>for: 这篇论文的目的是提高多 modal 数据的使用，以提高医疗诊断和预测的精度。</li>
<li>methods: 本文使用了一种称为多元数据信息瓶颈框架的方法，以减少数据繁殖和噪音，并保持适当的预测信息精度。</li>
<li>results: 实验结果显示，本文的方法在两个内部COVID-19数据集和两个公共生物医学数据集上的诊断和预测任务中，比前一代方法更高效和更Robust，能够在大规模噪音渠道存在时保持高度的预测性。<details>
<summary>Abstract</summary>
Effectively leveraging multimodal data such as various images, laboratory tests and clinical information is gaining traction in a variety of AI-based medical diagnosis and prognosis tasks. Most existing multi-modal techniques only focus on enhancing their performance by leveraging the differences or shared features from various modalities and fusing feature across different modalities. These approaches are generally not optimal for clinical settings, which pose the additional challenges of limited training data, as well as being rife with redundant data or noisy modality channels, leading to subpar performance. To address this gap, we study the robustness of existing methods to data redundancy and noise and propose a generalized dynamic multimodal information bottleneck framework for attaining a robust fused feature representation. Specifically, our information bottleneck module serves to filter out the task-irrelevant information and noises in the fused feature, and we further introduce a sufficiency loss to prevent dropping of task-relevant information, thus explicitly preserving the sufficiency of prediction information in the distilled feature. We validate our model on an in-house and a public COVID19 dataset for mortality prediction as well as two public biomedical datasets for diagnostic tasks. Extensive experiments show that our method surpasses the state-of-the-art and is significantly more robust, being the only method to remain performance when large-scale noisy channels exist. Our code is publicly available at https://github.com/BII-wushuang/DMIB.
</details>
<details>
<summary>摘要</summary>
通过有效地利用多模态数据，如各种图像、实验室测试和临床信息，在许多基于人工智能的医疗诊断和预测任务中占据主导地位。现有的多模态技术大多只是利用不同或共同特征之间的差异和共同特征的融合来提高性能。这些方法在临床设置下不是最佳选择，因为存在有限的训练数据，以及充斥着重复的数据或噪声的渠道，导致表现下降。为解决这个差距，我们研究了现有方法对数据重复和噪声的Robustness，并提出一种通用的动态多模态信息瓶颈框架，以获得一个Robust的融合特征表示。具体来说，我们的信息瓶颈模块可以筛除任务不关的信息和噪声在融合特征中，并引入一种充分loss来防止任务相关信息的排除，从而Explicitly preserved任务适用信息的完整性。我们在一个内部和一个公共COVID-19数据集上进行了 Mortality 预测和两个公共生物医学数据集上进行了诊断任务的验证。广泛的实验表明，我们的方法超过了当前状态的表现，并且在大规模噪声渠道存在时具有显著的Robust性，是唯一一个能够保持表现的方法。我们的代码可以在https://github.com/BII-wushuang/DMIB上获取。
</details></li>
</ul>
<hr>
<h2 id="Novel-View-Synthesis-from-a-Single-RGBD-Image-for-Indoor-Scenes"><a href="#Novel-View-Synthesis-from-a-Single-RGBD-Image-for-Indoor-Scenes" class="headerlink" title="Novel View Synthesis from a Single RGBD Image for Indoor Scenes"></a>Novel View Synthesis from a Single RGBD Image for Indoor Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01065">http://arxiv.org/abs/2311.01065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Congrui Hetang, Yuping Wang</li>
<li>for: 这篇论文提出了一种基于单个RGBD输入的新视图图像合成方法。</li>
<li>methods: 该方法将RGBD图像转换为点云，然后通过渲染从不同视角来实现新视图图像的合成。它将NVS任务转换为图像翻译问题，并使用生成器抗抗网络进行风格传递。</li>
<li>results: 该方法可以实现高质量的新视图图像合成，并且可以 circumvent了传统多图像技术的限制，如NeRF和MVS。<details>
<summary>Abstract</summary>
In this paper, we propose an approach for synthesizing novel view images from a single RGBD (Red Green Blue-Depth) input. Novel view synthesis (NVS) is an interesting computer vision task with extensive applications. Methods using multiple images has been well-studied, exemplary ones include training scene-specific Neural Radiance Fields (NeRF), or leveraging multi-view stereo (MVS) and 3D rendering pipelines. However, both are either computationally intensive or non-generalizable across different scenes, limiting their practical value. Conversely, the depth information embedded in RGBD images unlocks 3D potential from a singular view, simplifying NVS. The widespread availability of compact, affordable stereo cameras, and even LiDARs in contemporary devices like smartphones, makes capturing RGBD images more accessible than ever. In our method, we convert an RGBD image into a point cloud and render it from a different viewpoint, then formulate the NVS task into an image translation problem. We leveraged generative adversarial networks to style-transfer the rendered image, achieving a result similar to a photograph taken from the new perspective. We explore both unsupervised learning using CycleGAN and supervised learning with Pix2Pix, and demonstrate the qualitative results. Our method circumvents the limitations of traditional multi-image techniques, holding significant promise for practical, real-time applications in NVS.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法，可以从单个RGBD（红绿蓝深度）输入中生成新视图图像。新视图合成（NVS）是计算机视觉领域的一项有趣的任务，具有广泛的应用前景。传统的多张图像方法（如训练场景特定的神经辐射场（NeRF）或者利用多视图ステレオ（MVS）和3D渲染管道），尽管都是计算机昂贵或者不能普适应用于不同场景，但是它们的实际价值受限。相反，RGBD图像中嵌入的深度信息，使得3D的潜在能力从单个视图中解锁，使得NVS更加简单。现在， Compact和Affordable的斯tereo相机和甚至LiDAR在现代设备中的普及，使得捕捉RGBD图像更加容易。在我们的方法中，我们将RGBD图像转换为点云，然后从不同视图点渲染图像，最后将NVS任务转化为图像翻译问题。我们利用生成对抗网络进行风格转换，实现了从新视图点渲染的结果，与真实的新视图图像几乎相同。我们在无监督学习和监督学习两种情况下进行了质量检验，并展示了相关的结果。我们的方法可以绕过传统的多张图像技术的限制，具有实用的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Foundation-Models-for-Zero-shot-Animal-Species-Recognition-in-Camera-Trap-Images"><a href="#Multimodal-Foundation-Models-for-Zero-shot-Animal-Species-Recognition-in-Camera-Trap-Images" class="headerlink" title="Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images"></a>Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01064">http://arxiv.org/abs/2311.01064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zalan Fabian, Zhongqi Miao, Chunyuan Li, Yuanhan Zhang, Ziwei Liu, Andrés Hernández, Andrés Montes-Rojas, Rafael Escucha, Laura Siabatto, Andrés Link, Pablo Arbeláez, Rahul Dodhia, Juan Lavista Ferres</li>
<li>for: 这个研究目的是为了发展一个大规模的野生动物追踪解决方案，以减少人工劳动成本。</li>
<li>methods: 这个研究使用了多 modal 基础模型，包括视觉语言模型，将camera trap图像描述为文本，然后与外部知识库进行对比，以验证物种。</li>
<li>results: 研究发现，使用培育学习技术可以对camera trap图像进行零条件物种分类，并且可以增强描述质量。<details>
<summary>Abstract</summary>
Due to deteriorating environmental conditions and increasing human activity, conservation efforts directed towards wildlife is crucial. Motion-activated camera traps constitute an efficient tool for tracking and monitoring wildlife populations across the globe. Supervised learning techniques have been successfully deployed to analyze such imagery, however training such techniques requires annotations from experts. Reducing the reliance on costly labelled data therefore has immense potential in developing large-scale wildlife tracking solutions with markedly less human labor. In this work we propose WildMatch, a novel zero-shot species classification framework that leverages multimodal foundation models. In particular, we instruction tune vision-language models to generate detailed visual descriptions of camera trap images using similar terminology to experts. Then, we match the generated caption to an external knowledge base of descriptions in order to determine the species in a zero-shot manner. We investigate techniques to build instruction tuning datasets for detailed animal description generation and propose a novel knowledge augmentation technique to enhance caption quality. We demonstrate the performance of WildMatch on a new camera trap dataset collected in the Magdalena Medio region of Colombia.
</details>
<details>
<summary>摘要</summary>
WildMatch uses vision-language models to generate detailed visual descriptions of camera trap images, using similar terminology to experts. Then, it matches the generated caption to an external knowledge base of descriptions to determine the species in a zero-shot manner. To build the instruction tuning datasets for detailed animal description generation, we investigate techniques such as knowledge augmentation.We demonstrate the performance of WildMatch on a new camera trap dataset collected in the Magdalena Medio region of Colombia. With the ability to analyze wildlife images without relying on expensive labeled data, WildMatch has the potential to revolutionize large-scale wildlife tracking solutions, reducing the need for human labor and increasing the efficiency of conservation efforts.
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Adapt-CLIP-for-Few-Shot-Monocular-Depth-Estimation"><a href="#Learning-to-Adapt-CLIP-for-Few-Shot-Monocular-Depth-Estimation" class="headerlink" title="Learning to Adapt CLIP for Few-Shot Monocular Depth Estimation"></a>Learning to Adapt CLIP for Few-Shot Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01034">http://arxiv.org/abs/2311.01034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueting Hu, Ce Zhang, Yi Zhang, Bowen Hai, Ke Yu, Zhihai He</li>
<li>for: 本研究的目的是提出一种ew-shot基于的离散depth estimation方法，以增强VLMs的泛化能力。</li>
<li>methods: 该方法使用CLIP作为VLMs，并使用固定的depth bins来实现Zero-shot depth estimation。此外，还包括一些learnable prompts来预处理输入文本，以使模型更好地理解文本。</li>
<li>results: 对NYU V2和KITTI dataset进行了广泛的实验，并证明了该方法可以在MARE指标上比前一个状态的方法提高至多10.6%。<details>
<summary>Abstract</summary>
Pre-trained Vision-Language Models (VLMs), such as CLIP, have shown enhanced performance across a range of tasks that involve the integration of visual and linguistic modalities. When CLIP is used for depth estimation tasks, the patches, divided from the input images, can be combined with a series of semantic descriptions of the depth information to obtain similarity results. The coarse estimation of depth is then achieved by weighting and summing the depth values, called depth bins, corresponding to the predefined semantic descriptions. The zero-shot approach circumvents the computational and time-intensive nature of traditional fully-supervised depth estimation methods. However, this method, utilizing fixed depth bins, may not effectively generalize as images from different scenes may exhibit distinct depth distributions. To address this challenge, we propose a few-shot-based method which learns to adapt the VLMs for monocular depth estimation to balance training costs and generalization capabilities. Specifically, it assigns different depth bins for different scenes, which can be selected by the model during inference. Additionally, we incorporate learnable prompts to preprocess the input text to convert the easily human-understood text into easily model-understood vectors and further enhance the performance. With only one image per scene for training, our extensive experiment results on the NYU V2 and KITTI dataset demonstrate that our method outperforms the previous state-of-the-art method by up to 10.6\% in terms of MARE.
</details>
<details>
<summary>摘要</summary>
预训练的视觉语言模型（VLM），如CLIP，在涉及视觉语言模式的多种任务上表现出色。当用CLIP进行深度估计任务时，将图像中分割的小块与一系列 semantic description of depth information相结合，可以获得相似性结果。通过对 depth value 的权重和总和，可以实现粗略的深度估计。这种零批学习方法可以避免传统的干扰和时间consuming的深度估计方法。然而，这种方法使用固定的depth bin，可能无法有效泛化为不同的场景中的图像。为了解决这个挑战，我们提出了一种几批学习基于的方法，可以在训练成本和泛化能力之间寻找平衡。具体来说，它在不同的场景中分配不同的 depth bin，可以由模型在推理时选择。此外，我们还在输入文本中添加了学习的提示，以将人类可以理解的文本转换为模型可以理解的向量，并进一步提高性能。只需要一张图像 per scene 进行训练，我们在 NYU V2 和 KITTI 数据集上进行了广泛的实验，结果显示，我们的方法可以与之前的状态的艺术比出至多 10.6% 的 MARE 提高。
</details></li>
</ul>
<hr>
<h2 id="Nonnegative-Binary-Matrix-Factorization-for-Image-Classification-using-Quantum-Annealing"><a href="#Nonnegative-Binary-Matrix-Factorization-for-Image-Classification-using-Quantum-Annealing" class="headerlink" title="Nonnegative&#x2F;Binary Matrix Factorization for Image Classification using Quantum Annealing"></a>Nonnegative&#x2F;Binary Matrix Factorization for Image Classification using Quantum Annealing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01028">http://arxiv.org/abs/2311.01028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hinako Asaoka, Kazue Kudo</li>
<li>for: 图像分类问题的解决</li>
<li>methods: 使用量子热化法实现矩阵分解法和多类分类模型</li>
<li>results: 1. 数据量、特征数、轮次都较少时，使用NBMF模型的准确率高于传统机器学习方法，如神经网络；2. 使用量子热化法计算器可以减少计算时间。<details>
<summary>Abstract</summary>
Classical computing has borne witness to the development of machine learning. The integration of quantum technology into this mix will lead to unimaginable benefits and be regarded as a giant leap forward in mankind's ability to compute. Demonstrating the benefits of this integration now becomes essential. With the advance of quantum computing, several machine-learning techniques have been proposed that use quantum annealing. In this study, we implement a matrix factorization method using quantum annealing for image classification and compare the performance with traditional machine-learning methods. Nonnegative/binary matrix factorization (NBMF) was originally introduced as a generative model, and we propose a multiclass classification model as an application. We extract the features of handwritten digit images using NBMF and apply them to solve the classification problem. Our findings show that when the amount of data, features, and epochs is small, the accuracy of models trained by NBMF is superior to classical machine-learning methods, such as neural networks. Moreover, we found that training models using a quantum annealing solver significantly reduces computation time. Under certain conditions, there is a benefit to using quantum annealing technology with machine learning.
</details>
<details>
<summary>摘要</summary>
Nonnegative/binary matrix factorization (NBMF) was originally introduced as a generative model, and we propose a multiclass classification model as an application. We extract the features of handwritten digit images using NBMF and apply them to solve the classification problem. Our findings show that when the amount of data, features, and epochs is small, the accuracy of models trained by NBMF is superior to classical machine-learning methods, such as neural networks. Moreover, we found that training models using a quantum annealing solver significantly reduces computation time.Under certain conditions, the use of quantum annealing technology with machine learning can bring benefits.
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Language-Driven-Appearance-Knowledge-Units-with-Visual-Cues-in-Pedestrian-Detection"><a href="#Incorporating-Language-Driven-Appearance-Knowledge-Units-with-Visual-Cues-in-Pedestrian-Detection" class="headerlink" title="Incorporating Language-Driven Appearance Knowledge Units with Visual Cues in Pedestrian Detection"></a>Incorporating Language-Driven Appearance Knowledge Units with Visual Cues in Pedestrian Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01025">http://arxiv.org/abs/2311.01025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungjune Park, Hyunjun Kim, Yong Man Ro</li>
<li>for: 本研究旨在利用大型自然语言模型（LLM）对文本描述中的语义和上下文信息，提高人体检测 task 的性能。</li>
<li>methods: 我们提出了一种新的语言驱动的人体检测方法，通过将文本描述 integrate 到视觉cue中，以提高人体检测 task 的性能。我们首先构建了大量的描述集，其中包含了各种人体的描述，然后通过 LLM 进行学习，提取出语义上下文信息。最后，我们将语言驱动的知识单元与视觉cue相结合，以提供丰富的描述信息。</li>
<li>results: 我们通过对多种人体检测器进行广泛的实验，证明了我们的方法的有效性，并实现了人体检测 task 的最新表现。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown their capability in understanding contextual and semantic information regarding appearance knowledge of instances. In this paper, we introduce a novel approach to utilize the strength of an LLM in understanding contextual appearance variations and to leverage its knowledge into a vision model (here, pedestrian detection). While pedestrian detection is considered one of crucial tasks directly related with our safety (e.g., intelligent driving system), it is challenging because of varying appearances and poses in diverse scenes. Therefore, we propose to formulate language-driven appearance knowledge units and incorporate them with visual cues in pedestrian detection. To this end, we establish description corpus which includes numerous narratives describing various appearances of pedestrians and others. By feeding them through an LLM, we extract appearance knowledge sets that contain the representations of appearance variations. After that, we perform a task-prompting process to obtain appearance knowledge units which are representative appearance knowledge guided to be relevant to a downstream pedestrian detection task. Finally, we provide plentiful appearance information by integrating the language-driven knowledge units with visual cues. Through comprehensive experiments with various pedestrian detectors, we verify the effectiveness of our method showing noticeable performance gains and achieving state-of-the-art detection performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）有示出理解上下文和 semantics 信息的能力，这里我们提出一种新的方法，利用 LLM 理解上下文的应用变化和对于视觉模型（例如人员探测）的知识。由于人员探测是安全系统中一项重要的任务，但是它受到不同的场景和姿势的影响，因此我们提出使用语言驱动的应用知识单元来应对这个挑战。我们建立了一个描述库，包括许多描述不同人员和其他物品的故事。我们通过 LLM 处理这些故事，从中提取了应用知识集，这些集包括了不同人员的应用形式。然后，我们进行了任务激发过程，从中获得了具有应用知识导向的语言驱动知识单元。最后，我们通过与视觉提示集成而提供了丰富的应用信息。通过对不同人员探测器进行了详细的实验，我们证明了我们的方法的有效性，并 achieved state-of-the-art 的探测性能。
</details></li>
</ul>
<hr>
<h2 id="Expanding-Expressiveness-of-Diffusion-Models-with-Limited-Data-via-Self-Distillation-based-Fine-Tuning"><a href="#Expanding-Expressiveness-of-Diffusion-Models-with-Limited-Data-via-Self-Distillation-based-Fine-Tuning" class="headerlink" title="Expanding Expressiveness of Diffusion Models with Limited Data via Self-Distillation based Fine-Tuning"></a>Expanding Expressiveness of Diffusion Models with Limited Data via Self-Distillation based Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01018">http://arxiv.org/abs/2311.01018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiwan Hur, Jaehyun Choi, Gyojin Han, Dong-Jae Lee, Junmo Kim</li>
<li>for: 提高限制 dataset 上 diffusion model 的表达能力和生成能力，以解决各种下游任务中使用预训练 diffusion model 的不满result.</li>
<li>methods: 提出 Self-Distillation for Fine-Tuning diffusion models (SDFT) 方法，利用源 dataset 中多种特征，提高 diffusion model 的生成能力和表达能力。</li>
<li>results: 实验结果表明，SDFT 可以在限制 dataset 上提高 diffusion model 的表达能力和生成能力，并且可以在多种下游任务中提高生成效果。<details>
<summary>Abstract</summary>
Training diffusion models on limited datasets poses challenges in terms of limited generation capacity and expressiveness, leading to unsatisfactory results in various downstream tasks utilizing pretrained diffusion models, such as domain translation and text-guided image manipulation. In this paper, we propose Self-Distillation for Fine-Tuning diffusion models (SDFT), a methodology to address these challenges by leveraging diverse features from diffusion models pretrained on large source datasets. SDFT distills more general features (shape, colors, etc.) and less domain-specific features (texture, fine details, etc) from the source model, allowing successful knowledge transfer without disturbing the training process on target datasets. The proposed method is not constrained by the specific architecture of the model and thus can be generally adopted to existing frameworks. Experimental results demonstrate that SDFT enhances the expressiveness of the diffusion model with limited datasets, resulting in improved generation capabilities across various downstream tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>训练扩散模型在有限的数据集上存在限制生成能力和表达能力的挑战，导致使用预训练扩散模型的下游任务获得不满足的结果，如域转换和文本引导图像修饰。在这篇论文中，我们提出了自适应精炼扩散模型（SDFT），一种方法来解决这些挑战，通过利用大源数据集预训练的多样的特征。SDFT从源模型中提取更通用的特征（形状、颜色等），而不是域特定的特征（Texture、细节等），使得知识传递成功不会对目标数据集的训练过程产生影响。提出的方法不受特定模型的架构限制，因此可以通用于现有框架。实验结果表明，SDFT可以提高有限数据集上扩散模型的表达能力，从而在多种下游任务中提高生成能力。
</details></li>
</ul>
<hr>
<h2 id="Visual-Analytics-for-Efficient-Image-Exploration-and-User-Guided-Image-Captioning"><a href="#Visual-Analytics-for-Efficient-Image-Exploration-and-User-Guided-Image-Captioning" class="headerlink" title="Visual Analytics for Efficient Image Exploration and User-Guided Image Captioning"></a>Visual Analytics for Efficient Image Exploration and User-Guided Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01016">http://arxiv.org/abs/2311.01016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiran Li, Junpeng Wang, Prince Aboagye, Michael Yeh, Yan Zheng, Liang Wang, Wei Zhang, Kwan-Liu Ma</li>
<li>for: 本研究旨在帮助读者更好地理解大规模图像数据集中的Semantic结构和可能存在的数据偏见，以及提高语言-图像模型在caption生成过程中的表达能力。</li>
<li>methods: 本研究采用了一种新的视觉分析方法，利用大规模语言-图像模型的预训练技术，可以快速浏览大规模图像数据集，并自动生成图像的caption，以帮助读者更好地理解图像的Semantic结构和数据偏见。</li>
<li>results: 研究结果表明，通过使用这种新的视觉分析方法，可以快速发现大规模图像数据集中的数据偏见，并且可以提高语言-图像模型在caption生成过程中的表达能力。此外，研究还发现了一些可能存在的数据偏见，并提出了一些建议来改进语言-图像模型的caption生成能力。<details>
<summary>Abstract</summary>
Recent advancements in pre-trained large-scale language-image models have ushered in a new era of visual comprehension, offering a significant leap forward. These breakthroughs have proven particularly instrumental in addressing long-standing challenges that were previously daunting. Leveraging these innovative techniques, this paper tackles two well-known issues within the realm of visual analytics: (1) the efficient exploration of large-scale image datasets and identification of potential data biases within them; (2) the evaluation of image captions and steering of their generation process. On the one hand, by visually examining the captions automatically generated from language-image models for an image dataset, we gain deeper insights into the semantic underpinnings of the visual contents, unearthing data biases that may be entrenched within the dataset. On the other hand, by depicting the association between visual contents and textual captions, we expose the weaknesses of pre-trained language-image models in their captioning capability and propose an interactive interface to steer caption generation. The two parts have been coalesced into a coordinated visual analytics system, fostering mutual enrichment of visual and textual elements. We validate the effectiveness of the system with domain practitioners through concrete case studies with large-scale image datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Efficient exploration of large-scale image datasets and identification of potential biases within them.2. Evaluation of image captions and steering of their generation process.On one hand, by visually examining the captions automatically generated from language-image models for an image dataset, we gain a deeper understanding of the semantic underpinnings of the visual content, revealing any biases that may be present in the dataset.On the other hand, by depicting the association between visual contents and textual captions, we expose the weaknesses of pre-trained language-image models in their captioning capability and propose an interactive interface to steer caption generation.These two parts have been integrated into a coordinated visual analytics system, which fosters the mutual enrichment of visual and textual elements. We validate the effectiveness of the system through concrete case studies with large-scale image datasets, and demonstrate its potential for practical applications in the field.</details></li>
</ol>
<hr>
<h2 id="Act-As-You-Wish-Fine-Grained-Control-of-Motion-Diffusion-Model-with-Hierarchical-Semantic-Graphs"><a href="#Act-As-You-Wish-Fine-Grained-Control-of-Motion-Diffusion-Model-with-Hierarchical-Semantic-Graphs" class="headerlink" title="Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs"></a>Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01015">http://arxiv.org/abs/2311.01015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jpthu17/graphmotion">https://github.com/jpthu17/graphmotion</a></li>
<li>paper_authors: Peng Jin, Yang Wu, Yanbo Fan, Zhongqian Sun, Yang Wei, Li Yuan</li>
<li>for:  fine-grained control over human motion generation</li>
<li>methods:  hierarchical semantic graphs, text-to-motion diffusion process</li>
<li>results:  superior performance on two benchmark datasets, ability to continuously refine generated motion<details>
<summary>Abstract</summary>
Most text-driven human motion generation methods employ sequential modeling approaches, e.g., transformer, to extract sentence-level text representations automatically and implicitly for human motion synthesis. However, these compact text representations may overemphasize the action names at the expense of other important properties and lack fine-grained details to guide the synthesis of subtly distinct motion. In this paper, we propose hierarchical semantic graphs for fine-grained control over motion generation. Specifically, we disentangle motion descriptions into hierarchical semantic graphs including three levels of motions, actions, and specifics. Such global-to-local structures facilitate a comprehensive understanding of motion description and fine-grained control of motion generation. Correspondingly, to leverage the coarse-to-fine topology of hierarchical semantic graphs, we decompose the text-to-motion diffusion process into three semantic levels, which correspond to capturing the overall motion, local actions, and action specifics. Extensive experiments on two benchmark human motion datasets, including HumanML3D and KIT, with superior performances, justify the efficacy of our method. More encouragingly, by modifying the edge weights of hierarchical semantic graphs, our method can continuously refine the generated motion, which may have a far-reaching impact on the community. Code and pre-training weights are available at https://github.com/jpthu17/GraphMotion.
</details>
<details>
<summary>摘要</summary>
大多数文本驱动人体动作生成方法采用顺序模型，如 transformer，自动提取文本表达并用于人体动作生成。然而，这些紧凑的文本表达可能会强调动作名称的代价，而忽略其他重要特性，并且缺乏细节来指导动作生成。在这篇论文中，我们提议使用层次 semantic graphs 来实现细化控制 sobre 动作生成。具体来说，我们将动作描述分解成三级层次结构，包括动作、动作特征和具体动作。这些全局到本地结构可以帮助我们更好地理解动作描述，并且为动作生成提供细化控制。与此同时，我们将文本到动作协同扩散过程分解成三个semantic层次，以捕捉整体动作、本地动作和动作特征。经验表明，我们的方法在 HumanML3D 和 KIT 两个人体动作数据集上表现出色，并且可以不断细化生成的动作，这可能会对社区产生深远的影响。代码和预训练 веса可以在 https://github.com/jpthu17/GraphMotion 上获取。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Unified-Perspective-For-Fast-Shapley-Value-Estimation"><a href="#Exploring-Unified-Perspective-For-Fast-Shapley-Value-Estimation" class="headerlink" title="Exploring Unified Perspective For Fast Shapley Value Estimation"></a>Exploring Unified Perspective For Fast Shapley Value Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01010">http://arxiv.org/abs/2311.01010</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/user-tian/simshap">https://github.com/user-tian/simshap</a></li>
<li>paper_authors: Borui Zhang, Baotong Tian, Wenzhao Zheng, Jie Zhou, Jiwen Lu</li>
<li>for: 这篇论文旨在解决深度神经网络模型中的黑盒问题，使用了Shapley值作为可靠的工具。</li>
<li>methods: 这篇论文使用了多种方法，包括ApproSemivalue、KernelSHAP和FastSHAP，以减少计算复杂性。</li>
<li>results: 该论文通过分析现有工作的一致性，提出了一种简单和高效的估计方法，称为SimSHAP，并通过了大量的表格和图像数据的实验，证明了其效果性。<details>
<summary>Abstract</summary>
Shapley values have emerged as a widely accepted and trustworthy tool, grounded in theoretical axioms, for addressing challenges posed by black-box models like deep neural networks. However, computing Shapley values encounters exponential complexity in the number of features. Various approaches, including ApproSemivalue, KernelSHAP, and FastSHAP, have been explored to expedite the computation. We analyze the consistency of existing works and conclude that stochastic estimators can be unified as the linear transformation of importance sampling of feature subsets. Based on this, we investigate the possibility of designing simple amortized estimators and propose a straightforward and efficient one, SimSHAP, by eliminating redundant techniques. Extensive experiments conducted on tabular and image datasets validate the effectiveness of our SimSHAP, which significantly accelerates the computation of accurate Shapley values.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用Shapley值 Addressing Deep Learning模型的挑战============================================Shapley值已成为深度学习模型的挑战的一种广泛accepted和可靠的工具，基于理论axioms。然而，计算Shapley值遇到了特征数量的指数复杂性。各种方法，包括ApproSemivalue、KernelSHAP和FastSHAP，已经被探索以减少计算复杂性。我们对现有的工作进行了一致性分析，并发现了特征子抽样的重要性。基于这一点，我们提出了一种简单的总结器，SimSHAP，并进行了广泛的实验，证明了我们的SimSHAP可以快速和高效地计算准确的Shapley值。>>>Here's the translation:使用Shapley值 Addressing Deep Learning模型的挑战============================================Shapley值已成为深度学习模型的挑战的一种广泛accepted和可靠的工具，基于理论axioms。然而，计算Shapley值遇到了特征数量的指数复杂性。各种方法，包括ApproSemivalue、KernelSHAP和FastSHAP，已经被探索以减少计算复杂性。我们对现有的工作进行了一致性分析，并发现了特征子抽样的重要性。基于这一点，我们提出了一种简单的总结器，SimSHAP，并进行了广泛的实验，证明了我们的SimSHAP可以快速和高效地计算准确的Shapley值。
</details></li>
</ul>
<hr>
<h2 id="VCISR-Blind-Single-Image-Super-Resolution-with-Video-Compression-Synthetic-Data"><a href="#VCISR-Blind-Single-Image-Super-Resolution-with-Video-Compression-Synthetic-Data" class="headerlink" title="VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data"></a>VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00996">http://arxiv.org/abs/2311.00996</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kiteretsu77/vcisr-official">https://github.com/kiteretsu77/vcisr-official</a></li>
<li>paper_authors: Boyang Wang, Bowen Liu, Shiyu Liu, Fengyu Yang</li>
<li>for: 这个论文主要针对的是在单个视频帧输入下，使用低分辨率图像数据进行盲目超分辨率（SISR）任务。</li>
<li>methods: 我们提出了一种基于视频压缩的质量模型，用于生成低分辨率图像数据，并将其整合到现有的图像集中。这种方法可以广泛应用于现有的图像集，从而保持训练效率。</li>
<li>results: 我们的方法在无参考图像质量评估中达到了最高水平，并在多个数据集上显示了更好的视觉质量。此外，我们还评估了使用我们的杂化模型训练的SISR神经网络在视频超分辨率（VSR）任务中的性能，并发现其与专门为VSRS设计的建筑物 exhibits 相似或更好的性能，这说明了我们的策略可以普适地应用于更复杂的压缩残留 artifacts。<details>
<summary>Abstract</summary>
In the blind single image super-resolution (SISR) task, existing works have been successful in restoring image-level unknown degradations. However, when a single video frame becomes the input, these works usually fail to address degradations caused by video compression, such as mosquito noise, ringing, blockiness, and staircase noise. In this work, we for the first time, present a video compression-based degradation model to synthesize low-resolution image data in the blind SISR task. Our proposed image synthesizing method is widely applicable to existing image datasets, so that a single degraded image can contain distortions caused by the lossy video compression algorithms. This overcomes the leak of feature diversity in video data and thus retains the training efficiency. By introducing video coding artifacts to SISR degradation models, neural networks can super-resolve images with the ability to restore video compression degradations, and achieve better results on restoring generic distortions caused by image compression as well. Our proposed approach achieves superior performance in SOTA no-reference Image Quality Assessment, and shows better visual quality on various datasets. In addition, we evaluate the SISR neural network trained with our degradation model on video super-resolution (VSR) datasets. Compared to architectures specifically designed for the VSR purpose, our method exhibits similar or better performance, evidencing that the presented strategy on infusing video-based degradation is generalizable to address more complicated compression artifacts even without temporal cues.
</details>
<details>
<summary>摘要</summary>
在单影像超分辨率（SISR）任务中，现有的工作已经成功地恢复图像级未知降低。然而，当单个视频帧为输入时，这些工作通常无法处理由视频压缩引起的降低，如蚊子噪声、环形噪声、块状噪声和扫描噪声。在这种情况下，我们为首次提出了基于视频压缩的降低模型，用于生成低分辨率图像数据。我们的提议的图像生成方法可以广泛应用于现有的图像数据集，以便一个降低图像包含视频压缩所引起的扰动。这些扰动可以增加图像数据的特征多样性，从而保持训练效率。通过将视频编码扰动引入到SISR降低模型中，神经网络可以在恢复图像时恢复视频压缩所引起的降低，并且能够更好地恢复图像压缩所引起的扰动。我们的提出的方法在SOTA无参考图像质量评估中显示出优秀的性能，并在不同的数据集上显示更好的视觉质量。此外，我们对SISR神经网络经过我们的降低模型进行训练后，对视超分辨率（VSR）数据集进行评估。相比特制为VSRL的建筑，我们的方法在视觉质量方面显示相似或更好的性能，证明了我们提出的策略可以普适地应用于更复杂的压缩扰动，甚至 без时间证明。
</details></li>
</ul>
<hr>
<h2 id="A-Chronological-Survey-of-Theoretical-Advancements-in-Generative-Adversarial-Networks-for-Computer-Vision"><a href="#A-Chronological-Survey-of-Theoretical-Advancements-in-Generative-Adversarial-Networks-for-Computer-Vision" class="headerlink" title="A Chronological Survey of Theoretical Advancements in Generative Adversarial Networks for Computer Vision"></a>A Chronological Survey of Theoretical Advancements in Generative Adversarial Networks for Computer Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00995">http://arxiv.org/abs/2311.00995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishikesh Sharma</li>
<li>for: This paper aims to provide a chronological overview of the development of Generative Adversarial Networks (GANs) in the research field of computer vision, highlighting the key challenges and solutions in the evolution of GAN models.</li>
<li>methods: The paper uses a chronological approach to present the landmark research works on GANs, focusing on the theoretical advancements and applications of GANs in computer vision.</li>
<li>results: The paper highlights the significant improvements in the training of GAN models over time, and the various applications of GANs in computer vision tasks such as image generation, image-to-image translation, and image synthesis.<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) have been workhorse generative models for last many years, especially in the research field of computer vision. Accordingly, there have been many significant advancements in the theory and application of GAN models, which are notoriously hard to train, but produce good results if trained well. There have been many a surveys on GANs, organizing the vast GAN literature from various focus and perspectives. However, none of the surveys brings out the important chronological aspect: how the multiple challenges of employing GAN models were solved one-by-one over time, across multiple landmark research works. This survey intends to bridge that gap and present some of the landmark research works on the theory and application of GANs, in chronological order.
</details>
<details>
<summary>摘要</summary>
生成对抗网络（GAN）在过去几年中成为计算机视觉领域的重要生成模型，因此有很多重要的进展在GAN模型的理论和应用方面。尽管有很多关于GAN的评论文章，但 none of them 探讨了GAN模型的多个挑战如何逐步解决，从多个重要研究工作的角度出发。这篇评论文章的目的是弥补这一漏洞，并介绍一些GAN模型的理论和应用的各个阶段进展。
</details></li>
</ul>
<hr>
<h2 id="LaughTalk-Expressive-3D-Talking-Head-Generation-with-Laughter"><a href="#LaughTalk-Expressive-3D-Talking-Head-Generation-with-Laughter" class="headerlink" title="LaughTalk: Expressive 3D Talking Head Generation with Laughter"></a>LaughTalk: Expressive 3D Talking Head Generation with Laughter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00994">http://arxiv.org/abs/2311.00994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kim Sung-Bin, Lee Hyun, Da Hye Hong, Suekyeong Nam, Janghoon Ju, Tae-Hyun Oh</li>
<li>for: 这篇论文旨在提出一种能够同时表达语言和笑声的3D人物生成方法。</li>
<li>methods: 该方法使用了一种新的数据集，该数据集包括2D笑声视频和 Pseudo-annotated和人类验证的3D FLAME参数和顶点。该方法还使用了一种两个阶段的训练方案，首先学习语言朗读，然后学习表达笑声信号。</li>
<li>results: 对比现有方法，该方法在语言朗读和表达笑声信号方面都有出色的表现。此外，该方法还可以用于生成真实的人物模型。<details>
<summary>Abstract</summary>
Laughter is a unique expression, essential to affirmative social interactions of humans. Although current 3D talking head generation methods produce convincing verbal articulations, they often fail to capture the vitality and subtleties of laughter and smiles despite their importance in social context. In this paper, we introduce a novel task to generate 3D talking heads capable of both articulate speech and authentic laughter. Our newly curated dataset comprises 2D laughing videos paired with pseudo-annotated and human-validated 3D FLAME parameters and vertices. Given our proposed dataset, we present a strong baseline with a two-stage training scheme: the model first learns to talk and then acquires the ability to express laughter. Extensive experiments demonstrate that our method performs favorably compared to existing approaches in both talking head generation and expressing laughter signals. We further explore potential applications on top of our proposed method for rigging realistic avatars.
</details>
<details>
<summary>摘要</summary>
幽默是人类社交交流中的一种重要表达方式，但现有3D讲话头生成方法往往无法准确捕捉幽默和笑容的细节和重要性。在这篇论文中，我们介绍了一个新的任务：生成3D讲话头，能够同时具备流畅的语音和真实的笑容表达。我们新编译的数据集包括2D笑容视频和 Pseudo-注释和人类验证的3D FLAME参数和顶点。我们提议的训练方案包括两个阶段：首先学习讲话，然后学习表达笑容信号。我们的方法在讲话头生成和表达笑容信号方面都表现出了优异的成绩。我们还探讨了基于我们的提议方法的可能的应用，如制作真实的人物替身。
</details></li>
</ul>
<hr>
<h2 id="IR-UWB-Radar-based-Situational-Awareness-System-for-Smartphone-Distracted-Pedestrians"><a href="#IR-UWB-Radar-based-Situational-Awareness-System-for-Smartphone-Distracted-Pedestrians" class="headerlink" title="IR-UWB Radar-based Situational Awareness System for Smartphone-Distracted Pedestrians"></a>IR-UWB Radar-based Situational Awareness System for Smartphone-Distracted Pedestrians</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00991">http://arxiv.org/abs/2311.00991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamsheed Manja Ppallan, Ruchi Pandey, Yellappa Damam, Vijay Narayan Tiwari, Karthikeyan Arunachalam, Antariksha Ray</li>
<li>for: 提高智能手机使用者在路上行人安全性</li>
<li>methods: 使用IR-UWB雷达和人工神经网络实现实时障碍探测和警示</li>
<li>results: 实现了97%的障碍检测精度和95%的障碍分类精度，检测延迟26.8毫秒<details>
<summary>Abstract</summary>
With the widespread adoption of smartphones, ensuring pedestrian safety on roads has become a critical concern due to smartphone distraction. This paper proposes a novel and real-time assistance system called UWB-assisted Safe Walk (UASW) for obstacle detection and warns users about real-time situations. The proposed method leverages Impulse Radio Ultra-Wideband (IR-UWB) radar embedded in the smartphone, which provides excellent range resolution and high noise resilience using short pulses. We implemented UASW specifically for Android smartphones with IR-UWB connectivity. The framework uses complex Channel Impulse Response (CIR) data to integrate rule-based obstacle detection with artificial neural network (ANN) based obstacle classification. The performance of the proposed UASW system is analyzed using real-time collected data. The results show that the proposed system achieves an obstacle detection accuracy of up to 97% and obstacle classification accuracy of up to 95% with an inference delay of 26.8 ms. The results highlight the effectiveness of UASW in assisting smartphone-distracted pedestrians and improving their situational awareness.
</details>
<details>
<summary>摘要</summary>
随着智能手机的普及，保障行人安全在路上已成为一个重要问题，因为智能手机的分心。这篇论文提出了一种新的实时协助系统，名为UWB-assisted Safe Walk（UASW），用于避免障碍物检测和警示用户实时情况。该方法利用冲击式 радио Ultra-Wideband（IR-UWB）雷达，嵌入在智能手机中，它具有出色的范围分辨率和高噪声抗性，使用短报波。我们专门为Android智能手机开发了UASW。框架使用复杂的通道冲击响应（CIR）数据集成规则基于障碍物检测和人工神经网络（ANN）基于障碍物分类。我们对提出的UASW系统进行了实时数据收集和分析，结果显示，该系统可以达到97%的障碍物检测精度和95%的障碍物分类精度，检测延迟为26.8毫秒。结果表明，UASW系统有效地帮助智能手机分心行人提高情况意识。
</details></li>
</ul>
<hr>
<h2 id="VideoDreamer-Customized-Multi-Subject-Text-to-Video-Generation-with-Disen-Mix-Finetuning"><a href="#VideoDreamer-Customized-Multi-Subject-Text-to-Video-Generation-with-Disen-Mix-Finetuning" class="headerlink" title="VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning"></a>VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00990">http://arxiv.org/abs/2311.00990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Chen, Xin Wang, Guanning Zeng, Yipeng Zhang, Yuwei Zhou, Feilin Han, Wenwu Zhu</li>
<li>for: 这篇论文旨在提出一种个性化多主题文本到视频生成模型，即VideoDreamer框架，以生成具有独特视觉特征的多主题文本导向视频。</li>
<li>methods:  VideoDreamer模型基于预训练的稳定扩散和 latent-code 动力，并利用 temporal cross-frame attention 进行视频生成。在生成过程中，VideoDreamer还采用了Disen-Mix Finetuning和 Human-in-the-Loop Re-finetuning策略，以解决多主题生成中的 attribute binding 问题。</li>
<li>results: 在评估中，VideoDreamer模型能够生成具有新内容的、适应多主题的文本导向视频，例如新的事件和背景。同时，VideoDreamer还能够保持视频的时间协调和可读性。<details>
<summary>Abstract</summary>
Customized text-to-video generation aims to generate text-guided videos with customized user-given subjects, which has gained increasing attention recently. However, existing works are primarily limited to generating videos for a single subject, leaving the more challenging problem of customized multi-subject text-to-video generation largely unexplored. In this paper, we fill this gap and propose a novel VideoDreamer framework. VideoDreamer can generate temporally consistent text-guided videos that faithfully preserve the visual features of the given multiple subjects. Specifically, VideoDreamer leverages the pretrained Stable Diffusion with latent-code motion dynamics and temporal cross-frame attention as the base video generator. The video generator is further customized for the given multiple subjects by the proposed Disen-Mix Finetuning and Human-in-the-Loop Re-finetuning strategy, which can tackle the attribute binding problem of multi-subject generation. We also introduce MultiStudioBench, a benchmark for evaluating customized multi-subject text-to-video generation models. Extensive experiments demonstrate the remarkable ability of VideoDreamer to generate videos with new content such as new events and backgrounds, tailored to the customized multiple subjects. Our project page is available at https://videodreamer23.github.io/.
</details>
<details>
<summary>摘要</summary>
自定义文本到视频生成技术已经引起了越来越多的关注，目前的工作主要是为单个主题生成视频，忽略了更加挑战性的多主题文本到视频生成问题。在这篇论文中，我们填补这一漏洞，并提出了一个名为VideoDreamer的框架。VideoDreamer可以生成具有稳定的时间性和多主题视频特征的文本引导视频。具体来说，VideoDreamer利用了预训练的稳定扩散方法和带有动态混合的时间跨帧注意力，作为基础视频生成器。此外，我们还提出了一种名为Disen-MixFinetuning和人工约束重新训练策略，可以解决多主题生成中的特征绑定问题。我们还提出了一个名为MultiStudioBench的多主题生成评价指标，用于评估自定义多主题文本到视频生成模型。广泛的实验表明，VideoDreamer可以生成具有新的内容，如新的事件和背景，适应自定义多主题。我们的项目页面可以在https://videodreamer23.github.io/查看。
</details></li>
</ul>
<hr>
<h2 id="CML-MOTS-Collaborative-Multi-task-Learning-for-Multi-Object-Tracking-and-Segmentation"><a href="#CML-MOTS-Collaborative-Multi-task-Learning-for-Multi-Object-Tracking-and-Segmentation" class="headerlink" title="CML-MOTS: Collaborative Multi-task Learning for Multi-Object Tracking and Segmentation"></a>CML-MOTS: Collaborative Multi-task Learning for Multi-Object Tracking and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00987">http://arxiv.org/abs/2311.00987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Cui, Cheng Han, Dongfang Liu</li>
<li>for: 这个论文旨在提出一个能够同时进行物件检测、实例分割和多个物件追踪的可效框架，以便在视觉分析中进行实例级别的分析。</li>
<li>methods: 本文提出的方法是基于一个称为“相互连接”的新结构，这个结构在一个端到端学习的CNN中实现了多任务之间的相互连接，以便帮助这些任务同时进行。</li>
<li>results: 在KITTI MOTS和MOTS Challenge datasets上进行了广泛的评估，结果表明了本文提出的方法在多个物件追踪和实例分割任务中的表现都很出色。<details>
<summary>Abstract</summary>
The advancement of computer vision has pushed visual analysis tasks from still images to the video domain. In recent years, video instance segmentation, which aims to track and segment multiple objects in video frames, has drawn much attention for its potential applications in various emerging areas such as autonomous driving, intelligent transportation, and smart retail. In this paper, we propose an effective framework for instance-level visual analysis on video frames, which can simultaneously conduct object detection, instance segmentation, and multi-object tracking. The core idea of our method is collaborative multi-task learning which is achieved by a novel structure, named associative connections among detection, segmentation, and tracking task heads in an end-to-end learnable CNN. These additional connections allow information propagation across multiple related tasks, so as to benefit these tasks simultaneously. We evaluate the proposed method extensively on KITTI MOTS and MOTS Challenge datasets and obtain quite encouraging results.
</details>
<details>
<summary>摘要</summary>
“计算机视觉的发展使得视力分析任务从静止图像转移到视频域。近年来，视频实例分割，即在视频帧中跟踪和分割多个对象，吸引了很多关注，因为它在自动驾驶、智能交通和智能商业等领域可能得到广泛的应用。在这篇论文中，我们提出一种高效的视频帧级别的实例分析框架，可以同时进行对象检测、实例分割和多对象跟踪。我们的方法的核心思想是在结构化多任务学习中实现协同学习，通过将检测、分割和跟踪任务头部之间建立相关连接来实现信息传递。这些额外连接使得多个相关任务之间的信息可以相互传递，从而对多个任务产生共同的改进。我们对提出的方法进行了广泛的测试，并在KITTI MOTS和MOTS Challenge数据集上获得了很好的结果。”
</details></li>
</ul>
<hr>
<h2 id="M-M3D-Multi-Dataset-Training-and-Efficient-Network-for-Multi-view-3D-Object-Detection"><a href="#M-M3D-Multi-Dataset-Training-and-Efficient-Network-for-Multi-view-3D-Object-Detection" class="headerlink" title="M&amp;M3D: Multi-Dataset Training and Efficient Network for Multi-view 3D Object Detection"></a>M&amp;M3D: Multi-Dataset Training and Efficient Network for Multi-view 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00986">http://arxiv.org/abs/2311.00986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hang Zhang</li>
<li>for: 本研究提出了一种多视图3D物体检测网络结构，使用Camera-only数据和Bird’s-Eye-View地图，以解决当前键问题域适应和视觉数据传输。</li>
<li>methods: 该研究基于域适应和视觉数据传输的挑战，提出了一种基于Transformer的传输学习方法和3DanchorQuery的检测头，以实现数据迁移和效率检测。</li>
<li>results: 通过多个数据集训练和使用小量源数据和现有大型模型预训练参数，该网络实现了竞争性的结果，并且利用3D信息作为可用的semantic信息和2D多视图图像特征融合到视语言传输设计中。<details>
<summary>Abstract</summary>
In this research, I proposed a network structure for multi-view 3D object detection using camera-only data and a Bird's-Eye-View map. My work is based on a current key challenge domain adaptation and visual data transfer. Although many excellent camera-only 3D object detection has been continuously proposed, many research work risk dramatic performance drop when the networks are trained on the source domain but tested on a different target domain. Then I found it is very surprising that predictions on bounding boxes and classes are still replied to on 2D networks. Based on the domain gap assumption on various 3D datasets, I found they still shared a similar data extraction on the same BEV map size and camera data transfer. Therefore, to analyze the domain gap influence on the current method and to make good use of 3D space information among the dataset and the real world, I proposed a transfer learning method and Transformer construction to study the 3D object detection on NuScenes-mini and Lyft. Through multi-dataset training and a detection head from the Transformer, the network demonstrated good data migration performance and efficient detection performance by using 3D anchor query and 3D positional information. Relying on only a small amount of source data and the existing large model pre-training weights, the efficient network manages to achieve competitive results on the new target domain. Moreover, my study utilizes 3D information as available semantic information and 2D multi-view image features blending into the visual-language transfer design. In the final 3D anchor box prediction and object classification, my network achieved good results on standard metrics of 3D object detection, which differs from dataset-specific models on each training domain without any fine-tuning.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我提出了一种多视图3D物体检测网络结构，使用了摄像头数据和鸟瞰图。我的工作基于当前关键挑战的领域适应和视觉数据传递。虽然许多出色的摄像头只3D物体检测方法已经不断提出，但是许多研究工作在不同目标领域进行训练后会导致性能巨大下降。然而，我发现了一个很奇怪的现象：在2D网络上预测矩形框和类别时，预测结果仍然受到2D网络的影响。基于领域差异假设，我发现了许多3D数据集之间的数据EXTRACTOR都是类似的，因此可以在同一个BEV图像大小和摄像头数据传输下进行数据传输。为了分析当前方法中领域差异的影响和在数据集和实际世界中利用3D空间信息，我提出了一种传输学习方法和Transformer结构。通过多个数据集训练和Transformer检测头，网络实现了良好的数据迁移性和高效的检测性能，使用3D锚定Query和3D位势信息。只需要少量的源数据和现有大型模型预训练 весов，高效的网络可以在新的目标领域中获得竞争性的结果。此外，我的研究利用了3D信息作为可用的semantic信息和2D多视图图像特征融合到视语言传输设计中。最终，我的网络在标准3D物体检测指标上实现了好的结果，与不同预训练领域的模型不需要任何微调。
</details></li>
</ul>
<hr>
<h2 id="MAAIG-Motion-Analysis-And-Instruction-Generation"><a href="#MAAIG-Motion-Analysis-And-Instruction-Generation" class="headerlink" title="MAAIG: Motion Analysis And Instruction Generation"></a>MAAIG: Motion Analysis And Instruction Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00980">http://arxiv.org/abs/2311.00980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Hsin Yeh, Pei Hsin Lin, Yu-An Su, Wen Hsiang Cheng, Lun-Wei Ku</li>
<li>for: 提供个人体育训练home自动化指导，帮助用户提高运动技巧和避免伤害。</li>
<li>methods: 使用MAAIG应用框架，通过对用户提供的运动动作视频进行分析，生成每帧的嵌入向量，并将其与预训练T5模型结合，生成专业教练般的运动指导。</li>
<li>results: 能够识别和解决用户可能存在的问题，提供实时指导，帮助用户改善运动技巧和避免伤害。<details>
<summary>Abstract</summary>
Many people engage in self-directed sports training at home but lack the real-time guidance of professional coaches, making them susceptible to injuries or the development of incorrect habits. In this paper, we propose a novel application framework called MAAIG(Motion Analysis And Instruction Generation). It can generate embedding vectors for each frame based on user-provided sports action videos. These embedding vectors are associated with the 3D skeleton of each frame and are further input into a pretrained T5 model. Ultimately, our model utilizes this information to generate specific sports instructions. It has the capability to identify potential issues and provide real-time guidance in a manner akin to professional coaches, helping users improve their sports skills and avoid injuries.
</details>
<details>
<summary>摘要</summary>
很多人在家中自己进行体育训练，但是缺乏专业教练的实时指导，导致他们容易受伤或形成错误的习惯。在这篇论文中，我们提出了一种新的应用框架，即Motion Analysis And Instruction Generation（MAAIG）。它可以基于用户提供的体育动作视频生成嵌入向量，这些嵌入向量与每帧3D骨架相关，然后输入到预训练的T5模型中。最终，我们的模型可以利用这些信息生成特定的体育指导。它可以识别用户的可能问题，并在专业教练的方式下提供实时指导，帮助用户提高体育技巧，避免伤害。
</details></li>
</ul>
<hr>
<h2 id="Overhead-Line-Defect-Recognition-Based-on-Unsupervised-Semantic-Segmentation"><a href="#Overhead-Line-Defect-Recognition-Based-on-Unsupervised-Semantic-Segmentation" class="headerlink" title="Overhead Line Defect Recognition Based on Unsupervised Semantic Segmentation"></a>Overhead Line Defect Recognition Based on Unsupervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00979">http://arxiv.org/abs/2311.00979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weixi Wang, Xichen Zhong, Xin Li, Sizhe Li, Xun Ma</li>
<li>for:  automatic defect recognition in overhead lines</li>
<li>methods: Faster RCNN network + unsupervised semantic segmentation</li>
<li>results: improved accuracy and adaptability in identifying equipment issues<details>
<summary>Abstract</summary>
Overhead line inspection greatly benefits from defect recognition using visible light imagery. Addressing the limitations of existing feature extraction techniques and the heavy data dependency of deep learning approaches, this paper introduces a novel defect recognition framework. This is built on the Faster RCNN network and complemented by unsupervised semantic segmentation. The approach involves identifying the type and location of the target equipment, utilizing semantic segmentation to differentiate between the device and its backdrop, and finally employing similarity measures and logical rules to categorize the type of defect. Experimental results indicate that this methodology focuses more on the equipment rather than the defects when identifying issues in overhead lines. This leads to a notable enhancement in accuracy and exhibits impressive adaptability. Thus, offering a fresh perspective for automating the inspection of distribution network equipment.
</details>
<details>
<summary>摘要</summary>
Overhead line inspection 受到缺陷识别使用可见光影像的巨大 beneficial 影响。现有的特征提取技术和深度学习方法存在局限性，这篇文章提出了一种新的缺陷识别框架。这基于Faster RCNN网络，并且 complemented  by 无supervised semantic segmentation。该方法包括：首先，确定目标设备的类型和位置；其次，使用semantic segmentation来分 differentiate 设备和背景；最后，使用相似度度量和逻辑规则来分类缺陷类型。实验结果表明，该方法更强调设备而不是缺陷，从而提高了准确性。此外，它具有出色的适应性。因此，这种方法可以提供一种新的自动化分配网络设备检查的新 perspectives。
</details></li>
</ul>
<hr>
<h2 id="Lightweight-super-resolution-network-for-point-cloud-geometry-compression"><a href="#Lightweight-super-resolution-network-for-point-cloud-geometry-compression" class="headerlink" title="Lightweight super resolution network for point cloud geometry compression"></a>Lightweight super resolution network for point cloud geometry compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00970">http://arxiv.org/abs/2311.00970</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lidq92/lsrn-pcgc">https://github.com/lidq92/lsrn-pcgc</a></li>
<li>paper_authors: Wei Zhang, Dingquan Li, Ge Li, Wen Gao</li>
<li>for: 本文提出了一种基于缓冲点云的压缩方法，通过利用轻量级超Resolution网络来实现。</li>
<li>methods: 该方法首先将点云分解为基点云和重建原点云的 interpolate 模式。而 interpolation 模式的处理 strategy 则是通过一个轻量级超Resolution网络来学习，而不是直接压缩 interpolate 模式。</li>
<li>results: 实验表明，与lookup table-based方法相比，该方法可以更加准确地获得 interpolate 模式，同时在接受ABLE computational cost下可以访问更广泛的邻近 voxels。这使得该方法在 MPEG Cat1 (Solid) 和 Cat2 数据集上实现了显著的压缩性能。<details>
<summary>Abstract</summary>
This paper presents an approach for compressing point cloud geometry by leveraging a lightweight super-resolution network. The proposed method involves decomposing a point cloud into a base point cloud and the interpolation patterns for reconstructing the original point cloud. While the base point cloud can be efficiently compressed using any lossless codec, such as Geometry-based Point Cloud Compression, a distinct strategy is employed for handling the interpolation patterns. Rather than directly compressing the interpolation patterns, a lightweight super-resolution network is utilized to learn this information through overfitting. Subsequently, the network parameter is transmitted to assist in point cloud reconstruction at the decoder side. Notably, our approach differentiates itself from lookup table-based methods, allowing us to obtain more accurate interpolation patterns by accessing a broader range of neighboring voxels at an acceptable computational cost. Experiments on MPEG Cat1 (Solid) and Cat2 datasets demonstrate the remarkable compression performance achieved by our method.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这篇论文提出了一种基于轻量级超解算法的点云减少方法。该方法将点云分解为基点云和重建原点云的 interpolating 模式。而基点云可以使用任何lossless 编码器高效地压缩，而 interpolating 模式则通过一个轻量级超解网络学习。然后，网络参数将被传输到决策端，以帮助重建点云。与lookup 表格基于方法不同，我们可以通过访问更广泛的邻近 voxel 来获取更准确的 interpolating 模式，而不会影响计算成本。实验结果表明，我们的方法在 MPEG Cat1 (Solid) 和 Cat2 数据集上实现了出色的压缩性能。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Generated-Images-by-Real-Images-Only"><a href="#Detecting-Generated-Images-by-Real-Images-Only" class="headerlink" title="Detecting Generated Images by Real Images Only"></a>Detecting Generated Images by Real Images Only</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00962">http://arxiv.org/abs/2311.00962</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Xiuli Bi, Bo Liu, Fan Yang, Bin Xiao, Weisheng Li, Gao Huang, Pamela C. Cosman</li>
<li>for: 这篇论文旨在探讨生成模型产生的图像是否真实，并提出了一种新的检测方法，即从真实图像开始，找出它们共同点，然后将它们映射到对应的紧密空间中，以此检测生成图像。</li>
<li>methods: 本文使用了一种新的检测方法，即将真实图像映射到一个紧密的空间中，以检测生成图像。这种方法不需要大量的训练数据，仅需使用实际的图像进行训练，并且可以实现高效的检测。</li>
<li>results: 实验结果显示，本文提出的方法可以实现高效的生成图像检测，并且具有较好的响应性和稳定性。它可以检测出不同的生成模型，并且可以应对各种后期处理。这些优点使得本方法可以应用在实际的应用中。<details>
<summary>Abstract</summary>
As deep learning technology continues to evolve, the images yielded by generative models are becoming more and more realistic, triggering people to question the authenticity of images. Existing generated image detection methods detect visual artifacts in generated images or learn discriminative features from both real and generated images by massive training. This learning paradigm will result in efficiency and generalization issues, making detection methods always lag behind generation methods. This paper approaches the generated image detection problem from a new perspective: Start from real images. By finding the commonality of real images and mapping them to a dense subspace in feature space, the goal is that generated images, regardless of their generative model, are then projected outside the subspace. As a result, images from different generative models can be detected, solving some long-existing problems in the field. Experimental results show that although our method was trained only by real images and uses 99.9\% less training data than other deep learning-based methods, it can compete with state-of-the-art methods and shows excellent performance in detecting emerging generative models with high inference efficiency. Moreover, the proposed method shows robustness against various post-processing. These advantages allow the method to be used in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
deep learning技术继续发展，生成模型中的图像越来越真实，让人们开始 вопро问图像的真实性。现有的生成图像检测方法检测生成图像中的视觉瑕疵或者从实际和生成图像中学习特征，通过大规模训练。这种学习模式会导致效率和泛化问题，使检测方法总是落后于生成方法。本文从新的角度解决生成图像检测问题：从实际图像开始。通过找到实际图像的共同点，将它们映射到封闭的子空间中，使得生成图像，无论它们的生成模型，都将被投影到外部子空间。因此，不同的生成模型中的图像可以被检测出来，解决了领域中一些长期存在的问题。实验结果表明，我们的方法只使用实际图像进行训练，使用99.9% menos的深度学习基于数据进行训练，可以与当前状态的方法竞争，并且在检测新兴的生成模型方面表现出色，具有高速检测效率和Robustness against various post-processing。这些优点使得方法可以在实际场景中使用。
</details></li>
</ul>
<hr>
<h2 id="Concatenated-Masked-Autoencoders-as-Spatial-Temporal-Learner"><a href="#Concatenated-Masked-Autoencoders-as-Spatial-Temporal-Learner" class="headerlink" title="Concatenated Masked Autoencoders as Spatial-Temporal Learner"></a>Concatenated Masked Autoencoders as Spatial-Temporal Learner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00961">http://arxiv.org/abs/2311.00961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minhoooo1/catmae">https://github.com/minhoooo1/catmae</a></li>
<li>paper_authors: Zhouqiang Jiang, Bowen Wang, Tong Xiang, Zhaofeng Niu, Hong Tang, Guangshun Li, Liangzhi Li</li>
<li>for: 这篇论文的目的是学习视频表示，包括理解视频中的连续运动和视觉匹配。</li>
<li>methods: 这篇论文提出了一种新的自我supervised视频表示学习方法，即 Concatenated Masked Autoencoders (CatMAE)，它使用了一个掩码（95%）来遮盖视频帧的后续帧，并使用了一个Encoder和一个Decoder来编码和重建视频帧。</li>
<li>results: 与之前最先进的预训练方法相比，CatMAE在视频分割任务和动作识别任务中表现出了领先的水平。<details>
<summary>Abstract</summary>
Learning representations from videos requires understanding continuous motion and visual correspondences between frames. In this paper, we introduce the Concatenated Masked Autoencoders (CatMAE) as a spatial-temporal learner for self-supervised video representation learning. For the input sequence of video frames, CatMAE keeps the initial frame unchanged while applying substantial masking (95%) to subsequent frames. The encoder in CatMAE is responsible for encoding visible patches for each frame individually; subsequently, for each masked frame, the decoder leverages visible patches from both previous and current frames to reconstruct the original image. Our proposed method enables the model to estimate the motion information between visible patches, match the correspondences between preceding and succeeding frames, and ultimately learn the evolution of scenes. Furthermore, we propose a new data augmentation strategy, Video-Reverse (ViRe), which uses reversed video frames as the model's reconstruction targets. This further encourages the model to utilize continuous motion details and correspondences to complete the reconstruction, thereby enhancing the model's capabilities. Compared to the most advanced pre-training methods, CatMAE achieves a leading level in video segmentation tasks and action recognition tasks.
</details>
<details>
<summary>摘要</summary>
学习视频中的表示需要理解连续的运动和视觉匹配 между帧。在这篇论文中，我们介绍了嵌入式马SKAd（CatMAE）作为自我超级视频表示学习的空间-时间学习器。对于输入序列中的视频帧，CatMAE保留初始帧不变，而对后续帧应用了95%的压缩（masking）。CatMAE的编码器负责为每帧图像中的可见区域进行编码；对于每帧压缩图像，CatMAE的解码器利用前一帧和当前帧中的可见区域来重建原始图像。我们提议的方法使得模型可以估计图像中的运动信息，匹配前一帧和当前帧之间的对应关系，并最终学习场景的演化。此外，我们还提出了一种新的数据增强策略，名为视频反向（ViRe），它使用反转的视频帧作为模型的重建目标。这使得模型更加强调连续的运动细节和对应关系，从而提高模型的能力。相比最先进的预训练方法，CatMAE在视频分割任务和动作认知任务中达到了领先水平。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Noise-pursuit-for-Augmenting-Text-to-Video-Generation"><a href="#Optimal-Noise-pursuit-for-Augmenting-Text-to-Video-Generation" class="headerlink" title="Optimal Noise pursuit for Augmenting Text-to-Video Generation"></a>Optimal Noise pursuit for Augmenting Text-to-Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00949">http://arxiv.org/abs/2311.00949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Ma, Huayi Xu, Mengjian Li, Weidong Geng, Meng Wang, Yaxiong Wang</li>
<li>for: 提高文本到视频生成器的稳定性和质量，尤其是在不同噪音输入下。</li>
<li>methods: 提出了一种 aproaches 使用倒推视频映射来寻找最佳噪音，并通过搜索和反向映射来实现。此外，还提出了一种semantic-preserving rewriter来优化文本提示。</li>
<li>results: 通过extensive experiments on WebVid-10M benchmark，显示了提高文本到视频生成器的稳定性和质量，而且无需优化。<details>
<summary>Abstract</summary>
Despite the remarkable progress in text-to-video generation, existing diffusion-based models often exhibit instability in terms of noise during inference. Specifically, when different noises are fed for the given text, these models produce videos that differ significantly in terms of both frame quality and temporal consistency. With this observation, we posit that there exists an optimal noise matched to each textual input; however, the widely adopted strategies of random noise sampling often fail to capture it. In this paper, we argue that the optimal noise can be approached through inverting the groundtruth video using the established noise-video mapping derived from the diffusion model. Nevertheless, the groundtruth video for the text prompt is not available during inference. To address this challenge, we propose to approximate the optimal noise via a search and inversion pipeline. Given a text prompt, we initially search for a video from a predefined candidate pool that closely relates to the text prompt. Subsequently, we invert the searched video into the noise space, which serves as an improved noise prompt for the textual input. In addition to addressing noise, we also observe that the text prompt with richer details often leads to higher-quality videos. Motivated by this, we further design a semantic-preserving rewriter to enrich the text prompt, where a reference-guided rewriting is devised for reasonable details compensation, and a denoising with a hybrid semantics strategy is proposed to preserve the semantic consistency. Extensive experiments on the WebVid-10M benchmark show that our proposed method can improve the text-to-video models with a clear margin, while introducing no optimization burden.
</details>
<details>
<summary>摘要</summary>
尽管文本到视频生成技术已经取得了非常出色的进步，但现有的扩散基本模型在推理过程中仍然存在噪声稳定性问题。具体来说，对于不同的噪声输入，这些模型会生成具有不同框架质量和时间一致性的视频。从这个观察出发，我们认为存在一个与每个文本输入匹配的最佳噪声，但通常采用的随机噪声抽样策略往往无法捕捉到它。在这篇论文中，我们 argue that 最佳噪声可以通过推理模型确定的噪声-视频映射来接近。但是，在推理过程中不可以获得真实的地面视频。为解决这个挑战，我们提议一种搜索和反向映射管线来估算最佳噪声。给定一个文本提示，我们首先从预定的候选池中搜索一个与文本提示高度相关的视频，然后将搜索到的视频反向映射到噪声空间，作为改进的噪声提示。此外，我们还发现文本提示具有更多的细节时，会导致更高质量的视频。驱动于这一点，我们进一步设计了一种具有语义保持的重写器，以提高文本提示的细节和语义一致性。我们对WebVid-10M测试集进行了广泛的实验，结果表明，我们的提议方法可以帮助提高文本到视频模型，而且无需进行优化升级。
</details></li>
</ul>
<hr>
<h2 id="SatBird-Bird-Species-Distribution-Modeling-with-Remote-Sensing-and-Citizen-Science-Data"><a href="#SatBird-Bird-Species-Distribution-Modeling-with-Remote-Sensing-and-Citizen-Science-Data" class="headerlink" title="SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data"></a>SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00936">http://arxiv.org/abs/2311.00936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rolnicklab/satbird">https://github.com/rolnicklab/satbird</a></li>
<li>paper_authors: Mélisande Teng, Amna Elmustafa, Benjamin Akera, Yoshua Bengio, Hager Radi Abdelwahed, Hugo Larochelle, David Rolnick</li>
<li>for: 本研究旨在提高生物多样性监测和生态系统模拟，通过预测卫星图像中种群遇Rate的方法，以帮助保护生态系统。</li>
<li>methods: 本研究使用卫星图像和公民科学工具收集种群观察数据，并提供了环境数据和种群范围地图。</li>
<li>results: 本研究提供了一个新的任务，即使用卫星图像预测种群遇Rate，并在美国和肯尼亚提供了数据集。这些数据集可以帮助扩大生物多样性监测和生态系统模拟。<details>
<summary>Abstract</summary>
Biodiversity is declining at an unprecedented rate, impacting ecosystem services necessary to ensure food, water, and human health and well-being. Understanding the distribution of species and their habitats is crucial for conservation policy planning. However, traditional methods in ecology for species distribution models (SDMs) generally focus either on narrow sets of species or narrow geographical areas and there remain significant knowledge gaps about the distribution of species. A major reason for this is the limited availability of data traditionally used, due to the prohibitive amount of effort and expertise required for traditional field monitoring. The wide availability of remote sensing data and the growing adoption of citizen science tools to collect species observations data at low cost offer an opportunity for improving biodiversity monitoring and enabling the modelling of complex ecosystems. We introduce a novel task for mapping bird species to their habitats by predicting species encounter rates from satellite images, and present SatBird, a satellite dataset of locations in the USA with labels derived from presence-absence observation data from the citizen science database eBird, considering summer (breeding) and winter seasons. We also provide a dataset in Kenya representing low-data regimes. We additionally provide environmental data and species range maps for each location. We benchmark a set of baselines on our dataset, including SOTA models for remote sensing tasks. SatBird opens up possibilities for scalably modelling properties of ecosystems worldwide.
</details>
<details>
<summary>摘要</summary>
生物多样性正在不可预期的速度下降，对生物圈服务的确保食物、水和人类健康和幸福具有重要作用。了解种群的分布是保护政策规划中非常重要。然而，传统生态学中的种群分布模型（SDM）通常将注意力集中在特定的种群或特定的地理区域，并且还有许多不明之处关于种群的分布。一个主要原因是传统的场景监测数据的有限性，由于场景监测所需的努力和专业技能的成本很高。然而，卫星数据的广泛可用性和公民科学工具的普及使得生物多样性监测变得更加容易，并且可以提高生态系统的模拟。我们介绍了一个新的任务，即通过卫星图像预测鸟类种群的分布，并提出了SatBird数据集，该数据集包括美国各地的卫星图像位置和基于公民科学数据库eBird的存在或缺失观察数据，覆盖夏季（繁殖期）和冬季两个季节。此外，我们还提供了每个位置的环境数据和种群范围地图。我们对我们的数据集进行了一系列的基线测试，包括当今最佳实践的卫星数据处理模型。SatBird开销了全球范围内生态系统的可扩展模拟的可能性。
</details></li>
</ul>
<hr>
<h2 id="Towards-High-quality-HDR-Deghosting-with-Conditional-Diffusion-Models"><a href="#Towards-High-quality-HDR-Deghosting-with-Conditional-Diffusion-Models" class="headerlink" title="Towards High-quality HDR Deghosting with Conditional Diffusion Models"></a>Towards High-quality HDR Deghosting with Conditional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00932">http://arxiv.org/abs/2311.00932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingsen Yan, Tao Hu, Yuan Sun, Hao Tang, Yu Zhu, Wei Dong, Luc Van Gool, Yanning Zhang</li>
<li>for: 本研究旨在使用深度神经网络技术来重建高动态范围（HDR）图像，从多个低动态范围（LDR）图像中提取HDR图像，并解决阻挡现实场景中的应用。</li>
<li>methods: 本研究使用了Diffusion Model来生成HDR图像，包括Feature Condition Generator和Noise Predictor两部分。Feature Condition Generator使用了注意力和Domain Feature Alignment（DFA）层来转换中间特征，以避免幽灵残影。Noise Predictor使用了随机迭代的抽象过程来生成HDR图像。此外，为了减少LDR图像的饱和问题所引起的语义混乱，我们设计了滑块窗口噪声估计器来采样平滑的噪声。</li>
<li>results: 我们对HDR图像重建 benchmark datasets进行了实验，结果表明，我们的方法可以达到状态机器人的性能，并且在实际图像中进行了良好的泛化。<details>
<summary>Abstract</summary>
High Dynamic Range (HDR) images can be recovered from several Low Dynamic Range (LDR) images by existing Deep Neural Networks (DNNs) techniques. Despite the remarkable progress, DNN-based methods still generate ghosting artifacts when LDR images have saturation and large motion, which hinders potential applications in real-world scenarios. To address this challenge, we formulate the HDR deghosting problem as an image generation that leverages LDR features as the diffusion model's condition, consisting of the feature condition generator and the noise predictor. Feature condition generator employs attention and Domain Feature Alignment (DFA) layer to transform the intermediate features to avoid ghosting artifacts. With the learned features as conditions, the noise predictor leverages a stochastic iterative denoising process for diffusion models to generate an HDR image by steering the sampling process. Furthermore, to mitigate semantic confusion caused by the saturation problem of LDR images, we design a sliding window noise estimator to sample smooth noise in a patch-based manner. In addition, an image space loss is proposed to avoid the color distortion of the estimated HDR results. We empirically evaluate our model on benchmark datasets for HDR imaging. The results demonstrate that our approach achieves state-of-the-art performances and well generalization to real-world images.
</details>
<details>
<summary>摘要</summary>
高动态范围（HDR）图像可以从多个低动态范围（LDR）图像中恢复使用现有深度神经网络（DNN）技术。 DESPITE remarkable progress, DNN-based methods still generate ghosting artifacts when LDR images have saturation and large motion, which hinders potential applications in real-world scenarios. To address this challenge, we formulate the HDR deghosting problem as an image generation that leverages LDR features as the diffusion model's condition, consisting of the feature condition generator and the noise predictor. Feature condition generator employs attention and Domain Feature Alignment（DFA）layer to transform the intermediate features to avoid ghosting artifacts. With the learned features as conditions, the noise predictor leverages a stochastic iterative denoising process for diffusion models to generate an HDR image by steering the sampling process. Furthermore, to mitigate semantic confusion caused by the saturation problem of LDR images, we design a sliding window noise estimator to sample smooth noise in a patch-based manner. In addition, an image space loss is proposed to avoid the color distortion of the estimated HDR results. We empirically evaluate our model on benchmark datasets for HDR imaging. The results demonstrate that our approach achieves state-of-the-art performances and well generalization to real-world images.
</details></li>
</ul>
<hr>
<h2 id="RPCANet-Deep-Unfolding-RPCA-Based-Infrared-Small-Target-Detection"><a href="#RPCANet-Deep-Unfolding-RPCA-Based-Infrared-Small-Target-Detection" class="headerlink" title="RPCANet: Deep Unfolding RPCA Based Infrared Small Target Detection"></a>RPCANet: Deep Unfolding RPCA Based Infrared Small Target Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00917">http://arxiv.org/abs/2311.00917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengyi Wu, Tianfang Zhang, Lei Li, Yian Huang, Zhenming Peng</li>
<li>for: 提高探测远赤外小目标的准确率和可解释性。</li>
<li>methods: 提出了一种可解释的深度学习网络（RPCANet），通过对探测任务的归纳为归纳矩阵分解、低级背景估计和图像重建的抽象，将深度学习与域知识结合起来。</li>
<li>results: 在实验中，RPCANet 得到了优于基eline方法的良好效果，并且可以准确地检测小目标，同时保留图像的内在特征。<details>
<summary>Abstract</summary>
Deep learning (DL) networks have achieved remarkable performance in infrared small target detection (ISTD). However, these structures exhibit a deficiency in interpretability and are widely regarded as black boxes, as they disregard domain knowledge in ISTD. To alleviate this issue, this work proposes an interpretable deep network for detecting infrared dim targets, dubbed RPCANet. Specifically, our approach formulates the ISTD task as sparse target extraction, low-rank background estimation, and image reconstruction in a relaxed Robust Principle Component Analysis (RPCA) model. By unfolding the iterative optimization updating steps into a deep-learning framework, time-consuming and complex matrix calculations are replaced by theory-guided neural networks. RPCANet detects targets with clear interpretability and preserves the intrinsic image feature, instead of directly transforming the detection task into a matrix decomposition problem. Extensive experiments substantiate the effectiveness of our deep unfolding framework and demonstrate its trustworthy results, surpassing baseline methods in both qualitative and quantitative evaluations.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）网络在红外小目标检测（ISTD）中已经实现了很好的表现。然而，这些结构具有解释性不足的问题，被广泛视为黑盒子，因为它们忽略了ISTD领域知识。为解决这个问题，本研究提出了可解释的深度网络，称为RPCANet，用于检测红外暗目标。具体来说，我们的方法将ISTD任务解释为稀疏目标提取、低级背景估计和图像重建的一种松散的Robust Principle Component Analysis（RPCA）模型。通过将迭代优化更新步骤转化为深度学习框架，时间consuming和复杂的矩阵计算被替换为理论导向的神经网络。RPCANet可以清晰地解释目标，并保留内在的图像特征，而不是直接将检测任务转化为矩阵分解问题。我们的深度嵌入框架在实验中证明了其效果，超过了基eline方法的质量和量化评价。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/02/cs.CV_2023_11_02/" data-id="closbropr00l80g8853yv0hf8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/02/cs.AI_2023_11_02/" class="article-date">
  <time datetime="2023-11-02T12:00:00.000Z" itemprop="datePublished">2023-11-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/02/cs.AI_2023_11_02/">cs.AI - 2023-11-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Implicit-Chain-of-Thought-Reasoning-via-Knowledge-Distillation"><a href="#Implicit-Chain-of-Thought-Reasoning-via-Knowledge-Distillation" class="headerlink" title="Implicit Chain of Thought Reasoning via Knowledge Distillation"></a>Implicit Chain of Thought Reasoning via Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01460">http://arxiv.org/abs/2311.01460</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/da03/implicit_chain_of_thought">https://github.com/da03/implicit_chain_of_thought</a></li>
<li>paper_authors: Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, Stuart Shieber</li>
<li>for: 本研究旨在增强语言模型的逻辑能力，通过让模型生成链式思维步骤来解决问题。</li>
<li>methods: 本研究使用语言模型的内部隐藏状态进行做implicit reasoning，通过将教师模型在explicit链式思维上受训练的步骤进行压缩，使 reasoning 从”水平”（一旦一旦）变为”垂直”（在不同层次）进行。</li>
<li>results: 在多位数乘法任务和小学数学问题集上进行实验，发现这种方法可以解决无需explicit链式思维的任务，速度与无链式思维相当。<details>
<summary>Abstract</summary>
To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning "horizontally" by producing intermediate words one-by-one, we distill it such that the reasoning happens "vertically" among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.
</details>
<details>
<summary>摘要</summary>
通常，为了让语言模型具备理智能力，研究人员通常会提示或调整它们生成链式思维步骤，然后生成答案。然而，人们在自然语言中理智很有效，可能是语言模型可以更加有效地进行一些不是自然语言的中间计算。在这项工作中，我们尝试了一种不同的理智方法：而不是显式地生成链式思维步骤，我们使用语言模型的内部隐藏状态来进行隐藏式理智。我们从一个用于显式链式思维的教师模型中提取了隐藏式理智步骤，并不是在水平方向（一个个）进行理智，而是在不同层次中的隐藏状态之间进行垂直的理智。我们在多位数乘法任务和小学数学题目集合上进行了实验，发现这种方法可以解决无法用显式链式思维解决的任务，并且速度与无链式思维相当。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Policy-Learning-for-Sensorimotor-Control-Under-Distribution-Shifts"><a href="#Conformal-Policy-Learning-for-Sensorimotor-Control-Under-Distribution-Shifts" class="headerlink" title="Conformal Policy Learning for Sensorimotor Control Under Distribution Shifts"></a>Conformal Policy Learning for Sensorimotor Control Under Distribution Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01457">http://arxiv.org/abs/2311.01457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huang Huang, Satvik Sharma, Antonio Loquercio, Anastasios Angelopoulos, Ken Goldberg, Jitendra Malik</li>
<li>for: 检测和应对感知器的观测分布变化</li>
<li>methods: 使用具有正式统计保证的均值折衔策略，包括使用均值折衔策略进行安全性和速度的选择或直接将策略观测添加到量化和强化学习中</li>
<li>results: 虽然在 simulations 和物理 quadruped 上进行了丰富的评估，但是与五个基准相比，OUR 方法表现出了优异的成果，同时也是最简单的基准策略之一。<details>
<summary>Abstract</summary>
This paper focuses on the problem of detecting and reacting to changes in the distribution of a sensorimotor controller's observables. The key idea is the design of switching policies that can take conformal quantiles as input, which we define as conformal policy learning, that allows robots to detect distribution shifts with formal statistical guarantees. We show how to design such policies by using conformal quantiles to switch between base policies with different characteristics, e.g. safety or speed, or directly augmenting a policy observation with a quantile and training it with reinforcement learning. Theoretically, we show that such policies achieve the formal convergence guarantees in finite time. In addition, we thoroughly evaluate their advantages and limitations on two compelling use cases: simulated autonomous driving and active perception with a physical quadruped. Empirical results demonstrate that our approach outperforms five baselines. It is also the simplest of the baseline strategies besides one ablation. Being easy to use, flexible, and with formal guarantees, our work demonstrates how conformal prediction can be an effective tool for sensorimotor learning under uncertainty.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RoboGen-Towards-Unleashing-Infinite-Data-for-Automated-Robot-Learning-via-Generative-Simulation"><a href="#RoboGen-Towards-Unleashing-Infinite-Data-for-Automated-Robot-Learning-via-Generative-Simulation" class="headerlink" title="RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"></a>RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01455">http://arxiv.org/abs/2311.01455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, Chuang Gan</li>
<li>for: 本研究旨在将大规模模型中嵌入的广泛和多元知识转移到机器人领域，并实现机器人自动学习多种机器人技能。</li>
<li>methods: 本研究使用生成模型来自动生成多样化的任务、景象和训练监督，以扩大机器人技能学习的规模。</li>
<li>results: 本研究可以实现自动生成多样化的机器人技能，并且可以在无人指导下实现机器人自动学习。<details>
<summary>Abstract</summary>
We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly using or adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates corresponding simulation environments by populating pertinent objects and assets with proper spatial configurations. Afterwards, the agent decomposes the proposed high-level task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.
</details>
<details>
<summary>摘要</summary>
我们介绍RoboGen，一种生成式机器人代理人，可以自动学习多样化机器人技能的扩展。RoboGen利用了最新的基础和生成模型的进步。而不是直接使用或修改这些模型来生成策略或低级动作，我们提议使用生成方案，使用这些模型自动生成多样化的任务、场景和训练监督。我们的方法使机器人代理人具有自顾探索-生成-学习循环：代理人首先提出有趣的任务和技能要发展，然后生成相应的 simulations环境，通过填充相关的物体和资产，并对其进行适当的空间配置。然后，代理人将高级任务分解成子任务，选择最佳学习方法（强化学习、运动规划或轨迹优化），生成所需的训练监督，然后学习策略以获得提案的技能。我们的工作尝试抽取大规模模型中嵌入的广泛和多样化知识，将其传递到机器人领域。我们的完全生成管道可以重复查询，生成无数个关联有多样化任务和环境的技能示例。
</details></li>
</ul>
<hr>
<h2 id="NOIR-Neural-Signal-Operated-Intelligent-Robots-for-Everyday-Activities"><a href="#NOIR-Neural-Signal-Operated-Intelligent-Robots-for-Everyday-Activities" class="headerlink" title="NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities"></a>NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01454">http://arxiv.org/abs/2311.01454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruohan Zhang, Sharon Lee, Minjune Hwang, Ayano Hiranaka, Chen Wang, Wensi Ai, Jin Jie Ryan Tan, Shreya Gupta, Yilun Hao, Gabrael Levine, Ruohan Gao, Anthony Norcia, Li Fei-Fei, Jiajun Wu</li>
<li>for: 本研究开发了一个通用的智能脑机器人接口系统（NOIR），让人类通过脑征号控制机器人进行日常活动。</li>
<li>methods: 本研究使用电生物学测定（EEG）捕捉人类脑征号，并结合机器人学习算法，让NOIR适应个人用户并预测他们的意图。</li>
<li>results: 本研究成功完成了20个日常家居活动，包括cooking、cleaning、personal care和娱乐等，并提高了系统的效能。<details>
<summary>Abstract</summary>
We present Neural Signal Operated Intelligent Robots (NOIR), a general-purpose, intelligent brain-robot interface system that enables humans to command robots to perform everyday activities through brain signals. Through this interface, humans communicate their intended objects of interest and actions to the robots using electroencephalography (EEG). Our novel system demonstrates success in an expansive array of 20 challenging, everyday household activities, including cooking, cleaning, personal care, and entertainment. The effectiveness of the system is improved by its synergistic integration of robot learning algorithms, allowing for NOIR to adapt to individual users and predict their intentions. Our work enhances the way humans interact with robots, replacing traditional channels of interaction with direct, neural communication. Project website: https://noir-corl.github.io/.
</details>
<details>
<summary>摘要</summary>
我们现在推介Neural Signal Operated Intelligent Robots（NOIR），一个通用的智能大脑机器人接口系统，允许人类通过脑信号控制机器人完成日常活动。通过这个接口，人类通过电enzephalography（EEG）传达自己的意图对象和动作到机器人。我们的新系统在20种日常家务中展示了成功，包括厨艺、干净、个人护理和娱乐等。系统的效果通过机器人学习算法的同化，使得NOIR能够适应个人用户和预测他们的意图。我们的工作改善了人类与机器人之间的交互方式，将传统的通信途径替换为直接的 neural 通信。项目网站：https://noir-corl.github.io/.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Time-Series-Anomaly-Detection-using-Diffusion-based-Models"><a href="#Time-Series-Anomaly-Detection-using-Diffusion-based-Models" class="headerlink" title="Time Series Anomaly Detection using Diffusion-based Models"></a>Time Series Anomaly Detection using Diffusion-based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01452">http://arxiv.org/abs/2311.01452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fbrad/diffusionae">https://github.com/fbrad/diffusionae</a></li>
<li>paper_authors: Ioana Pintilie, Andrei Manolache, Florin Brad</li>
<li>for: 这 paper 探讨了使用 diffusion models 进行多变量时间序列中的异常检测 (AD)。</li>
<li>methods: 这 paper 测试了两种基于 diffusion 的模型，并与多个强大的神经网络基准进行比较。它们还扩展了 PA%K 协议，通过计算一个不依赖检测阈值和 K 的正确检测点的 ROCK-AUC 指标。</li>
<li>results: 这 paper 的模型在synthetic datasets 上表现出色，并在实际 datasets 上与基准集成比较，illustrating  diffusion-based methods 的潜在用于 AD 中。<details>
<summary>Abstract</summary>
Diffusion models have been recently used for anomaly detection (AD) in images. In this paper we investigate whether they can also be leveraged for AD on multivariate time series (MTS). We test two diffusion-based models and compare them to several strong neural baselines. We also extend the PA%K protocol, by computing a ROCK-AUC metric, which is agnostic to both the detection threshold and the ratio K of correctly detected points. Our models outperform the baselines on synthetic datasets and are competitive on real-world datasets, illustrating the potential of diffusion-based methods for AD in multivariate time series.
</details>
<details>
<summary>摘要</summary>
Diffusion models 最近在图像异常检测（AD）中使用，本文我们调查是否可以将其应用于多变量时间序列（MTS）上的异常检测。我们测试了两种扩散模型，并与一些强大的神经网络基线进行比较。我们还扩展了PA%K协议，计算一个不受检测阈值和K正确检测点的比率的ROCK-AUC指标。我们的模型在 sintetic 数据集上表现出色，并在实际数据集上与基线集成比较，这表明扩散基本方法在 MTS 上的异常检测具有潜在的潜力。
</details></li>
</ul>
<hr>
<h2 id="DreamSmooth-Improving-Model-based-Reinforcement-Learning-via-Reward-Smoothing"><a href="#DreamSmooth-Improving-Model-based-Reinforcement-Learning-via-Reward-Smoothing" class="headerlink" title="DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing"></a>DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01450">http://arxiv.org/abs/2311.01450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vint Lee, Pieter Abbeel, Youngwoon Lee</li>
<li>for: 这篇论文是关于Model-based reinforcement learning（MBRL）的研究，旨在通过生成假象轨迹来计划行为，学习复杂的行为。</li>
<li>methods: 这篇论文提出了一种简单 yet effective的奖金平滑方法，叫做DreamSmooth，它通过预测短时间内的奖金来训练MBRL算法，而不是固定时间内的奖金。</li>
<li>results: 经验表明，DreamSmooth可以在长期间遇到罕见奖金的任务上达到最佳性能，包括样本效率和最终性能，而不失去常见的benchmark测试。<details>
<summary>Abstract</summary>
Model-based reinforcement learning (MBRL) has gained much attention for its ability to learn complex behaviors in a sample-efficient way: planning actions by generating imaginary trajectories with predicted rewards. Despite its success, we found that surprisingly, reward prediction is often a bottleneck of MBRL, especially for sparse rewards that are challenging (or even ambiguous) to predict. Motivated by the intuition that humans can learn from rough reward estimates, we propose a simple yet effective reward smoothing approach, DreamSmooth, which learns to predict a temporally-smoothed reward, instead of the exact reward at the given timestep. We empirically show that DreamSmooth achieves state-of-the-art performance on long-horizon sparse-reward tasks both in sample efficiency and final performance without losing performance on common benchmarks, such as Deepmind Control Suite and Atari benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Distilling-Out-of-Distribution-Robustness-from-Vision-Language-Foundation-Models"><a href="#Distilling-Out-of-Distribution-Robustness-from-Vision-Language-Foundation-Models" class="headerlink" title="Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models"></a>Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01441">http://arxiv.org/abs/2311.01441</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andyz245/discreteadversarialdistillation">https://github.com/andyz245/discreteadversarialdistillation</a></li>
<li>paper_authors: Andy Zhou, Jindong Wang, Yu-Xiong Wang, Haohan Wang</li>
<li>for: 提高视觉模型的鲁棒性（out-of-distribution robustness）</li>
<li>methods: 组合知识塑化和数据增强，使用robust teacher生成对抗样本，并使用VQGAN积累对抗样本</li>
<li>results: 在不同的学生架构上显示了强大的对抗样本生成和清洁精度提升，而且计算 overhead 相对较少，可以轻松地与其他数据增强技术结合使用<details>
<summary>Abstract</summary>
We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmentations for further improvements.
</details>
<details>
<summary>摘要</summary>
我们提出了一种概念简单且轻量级的框架，用于提高视觉模型的鲁棒性通过知识塑化和数据扩展。我们证明了大型模型不一定是优秀的教师，我们通过显示含义更强的外部 robustness 提升。基于这一发现，我们提出了分割对抗塑化（DAD），它利用一个鲁棒的教师生成对抗例子，并使用 VQGAN 精炼它们，创造更有信息的样本。我们提供了在知识塑化和数据扩展设置下使用鲁棒教师的理论框架，并在不同的学生架构上显示了强大的 OUT-OF-distribution 鲁棒性和清晰率。值得注意的是，我们的方法相对于类似技术增加了微量的计算成本，可以轻松地与其他数据扩展技术结合使用，以实现更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Tailoring-Mixup-to-Data-using-Kernel-Warping-functions"><a href="#Tailoring-Mixup-to-Data-using-Kernel-Warping-functions" class="headerlink" title="Tailoring Mixup to Data using Kernel Warping functions"></a>Tailoring Mixup to Data using Kernel Warping functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01434">http://arxiv.org/abs/2311.01434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ensta-u2is/torch-uncertainty">https://github.com/ensta-u2is/torch-uncertainty</a></li>
<li>paper_authors: Quentin Bouniot, Pavlo Mozharovskyi, Florence d’Alché-Buc</li>
<li>for: 本研究旨在提高深度学习模型的效率和准确性，通过调整数据的 interpolate 方法来实现。</li>
<li>methods: 本文提出了一种基于插值的数据采样方法，通过调整插值系数的分布来实现更好的数据混合。</li>
<li>results: 经过广泛的 classification 和 regression 任务实验， authors 发现，使用该方法可以提高模型的性能和准确性，同时保持模型的多样性。<details>
<summary>Abstract</summary>
Data augmentation is an essential building block for learning efficient deep learning models. Among all augmentation techniques proposed so far, linear interpolation of training data points, also called mixup, has found to be effective for a large panel of applications. While the majority of works have focused on selecting the right points to mix, or applying complex non-linear interpolation, we are interested in mixing similar points more frequently and strongly than less similar ones. To this end, we propose to dynamically change the underlying distribution of interpolation coefficients through warping functions, depending on the similarity between data points to combine. We define an efficient and flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves both performance and calibration of models. Code available in https://github.com/ENSTA-U2IS/torch-uncertainty
</details>
<details>
<summary>摘要</summary>
“数据扩充是深度学习模型学习的重要基础之一。迄今为止所提出的所有扩充技术中，线性 interpolate 训练数据点，也称为 mixup，已经在许多应用场景中证明有效。然而，大多数工作都是关注选择要混合的点，或者应用复杂非线性 interpolate，我们则关注更频繁地混合类似点，并强制混合类似点更强大一些。为实现这一目标，我们提议动态更改混合过程中的基础分布，通过扭曲函数，根据数据点的相似性来确定混合。我们定义了高效可靠的框架，不会失去多样性。我们在分类和回归任务中进行了广泛的实验，显示我们的提议方法可以提高模型的性能和准确性。代码可以在 <https://github.com/ENSTA-U2IS/torch-uncertainty> 查看。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Castor-Causal-Temporal-Regime-Structure-Learning"><a href="#Castor-Causal-Temporal-Regime-Structure-Learning" class="headerlink" title="Castor: Causal Temporal Regime Structure Learning"></a>Castor: Causal Temporal Regime Structure Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01412">http://arxiv.org/abs/2311.01412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdellah Rahmani, Pascal Frossard</li>
<li>for: 本研究旨在探讨多变量时间序列数据中的 causal 关系，以解决各种领域中的关键问题。</li>
<li>methods: CASTOR 方法基于 EM 算法，可以学习不同模式下的 causal 关系，并且可以准确地找到每个模式下的唯一 режи。</li>
<li>results: 实验表明，CASTOR 方法在 causal discovery 中具有稳定性和可解释性，并且在 synthetic 数据和实际数据上都表现出色。<details>
<summary>Abstract</summary>
The task of uncovering causal relationships among multivariate time series data stands as an essential and challenging objective that cuts across a broad array of disciplines ranging from climate science to healthcare. Such data entails linear or non-linear relationships, and usually follow multiple a priori unknown regimes. Existing causal discovery methods can infer summary causal graphs from heterogeneous data with known regimes, but they fall short in comprehensively learning both regimes and the corresponding causal graph. In this paper, we introduce CASTOR, a novel framework designed to learn causal relationships in heterogeneous time series data composed of various regimes, each governed by a distinct causal graph. Through the maximization of a score function via the EM algorithm, CASTOR infers the number of regimes and learns linear or non-linear causal relationships in each regime. We demonstrate the robust convergence properties of CASTOR, specifically highlighting its proficiency in accurately identifying unique regimes. Empirical evidence, garnered from exhaustive synthetic experiments and two real-world benchmarks, confirm CASTOR's superior performance in causal discovery compared to baseline methods. By learning a full temporal causal graph for each regime, CASTOR establishes itself as a distinctly interpretable method for causal discovery in heterogeneous time series.
</details>
<details>
<summary>摘要</summary>
“探索多变量时间序列数据中的 causal 关系是一项非常重要且挑战性强的任务，覆盖了各种领域，从气候科学到医疗。这种数据通常具有线性或非线性关系，并且可能遵循多个未知的模式。现有的 causal 发现方法可以从不同类型的数据中推导摘要的 causal 图，但它们缺乏完整地学习多个模式和相应的 causal 图。在这篇论文中，我们提出了 CASTOR，一种新的框架，用于在不同模式下学习时间序列数据中的 causal 关系。通过 Maximize 一个分数函数的 EM 算法，CASTOR 可以推导模式的数量和每个模式中的线性或非线性 causal 关系。我们证明了 CASTOR 的稳定性和可靠性，并且在多种 synthetic 实验和实际应用中证明了它的超越性。通过学习每个模式的全 temporal causal 图，CASTOR 成为一种可解释的 causal 发现方法。”
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Information-Propagation-in-Ethereum-Network-Using-Combined-Graph-Attention-Network-and-Reinforcement-Learning-to-Optimize-Network-Efficiency-and-Scalability"><a href="#Analysis-of-Information-Propagation-in-Ethereum-Network-Using-Combined-Graph-Attention-Network-and-Reinforcement-Learning-to-Optimize-Network-Efficiency-and-Scalability" class="headerlink" title="Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability"></a>Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01406">http://arxiv.org/abs/2311.01406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Kambiz Behfar, Jon Crowcroft</li>
<li>for: 这个研究的目的是分析以太网络中信息传递的动态模式，以提高网络的效率、安全性和扩展性。</li>
<li>methods: 这个研究使用图 convolutional neural networks (GCNs) 分析以太网络中信息传递的图结构，并使用 combined graph attention network (GAT) 和 reinforcement learning (RL) 模型优化网络的效率和扩展性。</li>
<li>results: 实验评估表明，我们提出的 GAT-RL 模型在大规模以太网络 dataset 上表现出色，可以有效地传递信息 across the network，优化 gas limits for block processing，并提高网络的效率。<details>
<summary>Abstract</summary>
Blockchain technology has revolutionized the way information is propagated in decentralized networks. Ethereum plays a pivotal role in facilitating smart contracts and decentralized applications. Understanding information propagation dynamics in Ethereum is crucial for ensuring network efficiency, security, and scalability. In this study, we propose an innovative approach that utilizes Graph Convolutional Networks (GCNs) to analyze the information propagation patterns in the Ethereum network. The first phase of our research involves data collection from the Ethereum blockchain, consisting of blocks, transactions, and node degrees. We construct a transaction graph representation using adjacency matrices to capture the node embeddings; while our major contribution is to develop a combined Graph Attention Network (GAT) and Reinforcement Learning (RL) model to optimize the network efficiency and scalability. It learns the best actions to take in various network states, ultimately leading to improved network efficiency, throughput, and optimize gas limits for block processing. In the experimental evaluation, we analyze the performance of our model on a large-scale Ethereum dataset. We investigate effectively aggregating information from neighboring nodes capturing graph structure and updating node embeddings using GCN with the objective of transaction pattern prediction, accounting for varying network loads and number of blocks. Not only we design a gas limit optimization model and provide the algorithm, but also to address scalability, we demonstrate the use and implementation of sparse matrices in GraphConv, GraphSAGE, and GAT. The results indicate that our designed GAT-RL model achieves superior results compared to other GCN models in terms of performance. It effectively propagates information across the network, optimizing gas limits for block processing and improving network efficiency.
</details>
<details>
<summary>摘要</summary>
Blockchain技术已经改变了分布式网络中信息的传播方式。以太币扮演着重要的角色，它使得智能合约和分布式应用得以实现。为了确保网络的效率、安全性和可扩展性，理解以太币网络中信息传播的动态非常重要。在这项研究中，我们提出了一种创新的方法，使用图 convolutional neural networks（GCNs）来分析以太币网络中信息传播的模式。我们的首个阶段是收集以太币链上的数据，包括块、交易和节点度。我们使用邻居矩阵来构造交易图表示，并通过我们的主要贡献—— combining Graph Attention Network（GAT）和强化学习（RL）模型来优化网络效率和可扩展性。这个模型学习在不同的网络状态下，选择最佳的行为，最终导致网络效率的提高，通过缓存限制和块处理的优化。在实验评估中，我们对大规模的以太币数据进行分析，研究如何有效地从邻居节点中收集信息，更新节点嵌入，使用 GCN 进行交易模式预测，考虑不同的网络负载和块数。此外，我们还设计了一个 gas 限制优化模型，并提供算法。为了解决扩展性问题，我们在 GraphConv、GraphSAGE 和 GAT 中使用稀疏矩阵。结果显示，我们设计的 GAT-RL 模型在性能方面取得了更好的结果，能够有效地在网络中传播信息，优化缓存限制和块处理，提高网络效率。
</details></li>
</ul>
<hr>
<h2 id="Vision-Language-Foundation-Models-as-Effective-Robot-Imitators"><a href="#Vision-Language-Foundation-Models-as-Effective-Robot-Imitators" class="headerlink" title="Vision-Language Foundation Models as Effective Robot Imitators"></a>Vision-Language Foundation Models as Effective Robot Imitators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01378">http://arxiv.org/abs/2311.01378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, Tao Kong</li>
<li>for: 本研究旨在使用现有的视力语言模型（VLM）进行简单的微调，以解决机器人操作任务。</li>
<li>methods: 我们提出了一种简单的视力语言操作框架，名为RoboFlamingo，基于开源的VLM。 RoboFlamingo使用预训练的VLM进行单步视力语言理解，并使用显式策略头来记录Sequential history information。</li>
<li>results: 我们的实验结果显示，RoboFlamingo可以在语言控制 datasets 上达到最佳性能，并且在低性能平台上进行开Loop控制。我们的研究还发现了不同预训练VLM的不同表现在操作任务中。我们认为RoboFlamingo可以成为一种cost-effective和易于使用的机器人操作解决方案。<details>
<summary>Abstract</summary>
Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:近来的视觉语言基础模型进步，表明它们可以理解多模态数据，解决复杂的视觉语言任务，包括机器人控制。我们寻找一种简单、直观地使用现有的视觉语言模型（VLM），并在机器人数据上进行简单的微调。为此，我们提出了一个简单而新的视觉语言控制框架，名为RoboFlamingo，基于开源的VLM，OpenFlamingo。与前作不同，RoboFlamingo使用预训练的VLM进行单步视觉语言理解，模型序列历史信息使用显式策略头，并通过仅在语言条件 manipulate 数据上进行微调学习。这种分解提供了 RoboFlamingo 对于开loop控制和低性能平台部署的灵活性。我们通过在测试 benchmark 上以大幅度超越状态艺术表现，显示 RoboFlamingo 可以作为适用 VLM 到机器人控制的有效和竞争力强的解决方案。我们的广泛的实验结果还揭示了不同预训练 VLM 在 manipulate 任务上的行为有趣的结论。我们认为 RoboFlamingo 具有成本效果和易用的特点，可以让每个人通过微调自己的机器人策略来掌控机器人。
</details></li>
</ul>
<hr>
<h2 id="Recognize-Any-Regions"><a href="#Recognize-Any-Regions" class="headerlink" title="Recognize Any Regions"></a>Recognize Any Regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01373">http://arxiv.org/abs/2311.01373</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Surrey-UPLab/Recognize-Any-Regions">https://github.com/Surrey-UPLab/Recognize-Any-Regions</a></li>
<li>paper_authors: Haosen Yang, Chuofan Ma, Bin Wen, Yi Jiang, Zehuan Yuan, Xiatian Zhu</li>
<li>for: 本研究旨在解决计算机视觉中开放世界对象检测中个体区域或块的 semantics 问题，即在不受限制的图像中理解每个区域或块的 semantics。</li>
<li>methods: 该研究基于现有的图像视语(ViL)基础模型，如 CLIP，并将其用于开放世界对象检测。研究者们使用了各种方法，包括对region-label对的预训练和对检测模型的输出的图像级别表示的对接。</li>
<li>results: 研究者们提出了一种新的、通用的和高效的区域认可架构，名为RegionSpot，可以将位置意识的本地化知识与图像级别的semantics相结合。该架构可以在开放世界对象检测中提高性能，同时减少计算成本。例如，在300万个数据集上训练，只需一天内使用8个V100 GPU，并且与GLIP相比，提高了6.5%的mean average precision(mAP)，对更难和罕见的类型的提高更大的14.8%。<details>
<summary>Abstract</summary>
Understanding the semantics of individual regions or patches within unconstrained images, such as in open-world object detection, represents a critical yet challenging task in computer vision. Building on the success of powerful image-level vision-language (ViL) foundation models like CLIP, recent efforts have sought to harness their capabilities by either training a contrastive model from scratch with an extensive collection of region-label pairs or aligning the outputs of a detection model with image-level representations of region proposals. Despite notable progress, these approaches are plagued by computationally intensive training requirements, susceptibility to data noise, and deficiency in contextual information. To address these limitations, we explore the synergistic potential of off-the-shelf foundation models, leveraging their respective strengths in localization and semantics. We introduce a novel, generic, and efficient region recognition architecture, named RegionSpot, designed to integrate position-aware localization knowledge from a localization foundation model (e.g., SAM) with semantic information extracted from a ViL model (e.g., CLIP). To fully exploit pretrained knowledge while minimizing training overhead, we keep both foundation models frozen, focusing optimization efforts solely on a lightweight attention-based knowledge integration module. Through extensive experiments in the context of open-world object recognition, our RegionSpot demonstrates significant performance improvements over prior alternatives, while also providing substantial computational savings. For instance, training our model with 3 million data in a single day using 8 V100 GPUs. Our model outperforms GLIP by 6.5 % in mean average precision (mAP), with an even larger margin by 14.8 % for more challenging and rare categories.
</details>
<details>
<summary>摘要</summary>
本文描述了一种新的、通用、高效的区域识别架构，即RegionSpot，用于在开放世界 объек特点检测中理解图像中的各个区域或区域提案。我们利用了一个本地化基本模型（如SAM）和一个视力语言基本模型（如CLIP）的各自优势，通过一种简单的注意力机制来结合这两者的知识。我们不会更新基本模型，只是对注意力机制进行优化。我们通过对300万个数据进行训练，使用8个V100 GPU，并证明了我们的模型在开放世界 объек特点检测中表现出了显著的改进，同时也减少了计算成本。例如，我们的模型在GLIP模型的6.5%的mean average precision（mAP）上表现出了6.5%的提升，而在更为困难和罕见的类别上则是14.8%的提升。
</details></li>
</ul>
<hr>
<h2 id="Simplicial-Models-for-the-Epistemic-Logic-of-Faulty-Agents"><a href="#Simplicial-Models-for-the-Epistemic-Logic-of-Faulty-Agents" class="headerlink" title="Simplicial Models for the Epistemic Logic of Faulty Agents"></a>Simplicial Models for the Epistemic Logic of Faulty Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01351">http://arxiv.org/abs/2311.01351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Goubault, Roman Kniazev, Jeremy Ledent, Sergio Rajsbaum</li>
<li>for: 这篇论文研究了基于高维结构 simplicial complexes 的 simplicial models，并探讨了这些模型在不同设计选择下的性质。</li>
<li>methods: 作者使用了不同的设计选择来定义不纯的 simplicial models，并axiomatized了这些模型的逻辑。</li>
<li>results: 作者通过应用于分布式计算中进程可能在执行系统时崩溃的例子，ILLUSTRATE了这些逻辑的应用。<details>
<summary>Abstract</summary>
In recent years, several authors have been investigating simplicial models, a model of epistemic logic based on higher-dimensional structures called simplicial complexes. In the original formulation, simplicial models were always assumed to be pure, meaning that all worlds have the same dimension. This is equivalent to the standard S5n semantics of epistemic logic, based on Kripke models. By removing the assumption that models must be pure, we can go beyond the usual Kripke semantics and study epistemic logics where the number of agents participating in a world can vary. This approach has been developed in a number of papers, with applications in fault-tolerant distributed computing where processes may crash during the execution of a system. A difficulty that arises is that subtle design choices in the definition of impure simplicial models can result in different axioms of the resulting logic. In this paper, we classify those design choices systematically, and axiomatize the corresponding logics. We illustrate them via distributed computing examples of synchronous systems where processes may crash.
</details>
<details>
<summary>摘要</summary>
近年来，一些作者已经在调查 simplicial 模型，一种基于高维结构called simplicial complexes的epistemic logic模型。在原始表述中，simplicial模型总是被认为是纯净的，意味着所有世界都有相同的维度。这与标准的 S5n  semantics of epistemic logic相等，基于 Kripke 模型。由 removing the assumption that models must be pure，我们可以超越常见的 Kripke  semantics 和研究 epistemic logics 中参与世界数量的变化。这种方法在一些文章中被发展，并应用于容易受到进程崩溃的分布式计算系统中。然而，由于 subtle design choices 的定义而导致不同的论据。在这篇文章中，我们系统地分类这些设计选择，并对它们的论据进行 axiomatization。我们通过分布式计算的同步系统示例来说明它们。
</details></li>
</ul>
<hr>
<h2 id="Like-an-Open-Book-Read-Neural-Network-Architecture-with-Simple-Power-Analysis-on-32-bit-Microcontrollers"><a href="#Like-an-Open-Book-Read-Neural-Network-Architecture-with-Simple-Power-Analysis-on-32-bit-Microcontrollers" class="headerlink" title="Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers"></a>Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01344">http://arxiv.org/abs/2311.01344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Joud, Pierre-Alain Moellic, Simon Pontie, Jean-Baptiste Rigaud</li>
<li>for: 本研究旨在探讨如何通过EM侧通道诊断深度学习模型的架构信息，以便对相关的AI系统进行安全保护。</li>
<li>methods: 本研究使用了 тео리тиче知识和ARM CMSIS-NN库的分析，提出了一种基于简单模式识别分析的EXTRACTION方法，用于从EM侧通道诊断传感器上提取深度学习模型的架构信息。</li>
<li>results: 研究发现，即使面临一些困难的特例，EXTRACTION方法仍可以成功地提取深度学习模型的架构信息，而且攻击复杂度较低。研究也指出了相关的安全保护措施需要适应强大的内存和延迟要求。<details>
<summary>Abstract</summary>
Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction, the complexity of the attack is relatively low and we highlight the urgent need for practicable protections that could fit the strong memory and latency requirements of such platforms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>模型提取是人工智能系统安全的一个快速增长的问题。深度神经网络模型的架构是恶意者最重要的目标信息。作为一个序列的重复计算块，深度神经网络模型在边缘设备上部署时会生成特征的侧annel泄露。这些侧annel泄露可以被利用来提取关键信息，当目标平台可以访问时。通过结合深度学习实践知识和ARM CMSIS-NN实现库的分析，我们的目的是回答这个关键问题：可以通过仅仅分析EM侧annel跟踪来提取架构信息多远？我们首次提出了一种EXTRACTION方法，可以在高端32位微控制器（Cortex-M7）上运行传统的MLP和CNN模型，不需要复杂的算法或特殊的硬件。虽有一些困难的案例，但我们宣称，相比于参数提取，这种攻击的复杂性相对较低。我们高亮了对这些平台的实用防护措施的急需，以满足它们的强大内存和延迟需求。
</details></li>
</ul>
<hr>
<h2 id="Offline-Imitation-from-Observation-via-Primal-Wasserstein-State-Occupancy-Matching"><a href="#Offline-Imitation-from-Observation-via-Primal-Wasserstein-State-Occupancy-Matching" class="headerlink" title="Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching"></a>Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01331">http://arxiv.org/abs/2311.01331</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaiyan289/pw-dice">https://github.com/kaiyan289/pw-dice</a></li>
<li>paper_authors: Kai Yan, Alexander G. Schwing, Yu-xiong Wang</li>
<li>For: 本研究旨在降低在实际场景中的环境侵入成本，以及专家示范行为不一定可用。为此，Offline Learning from Observations (LfO) 得到了广泛的研究，旨在使用仅专家状态和任务无关的非专家状态-动作对组成一个问题解决方案。* Methods: 现有的 DIstribution Correction Estimation (DICE) 方法尝试将学习者和专家政策之间的状态占用差异降到最小。然而，这些方法受到 $f$- divergence（KL 和 $\chi^2$）或 Wasserstein 距离的限制，后者限制了在 Wasserstein 基于解决方案中使用的下面距离的metric。为了解决这个问题，我们提出了 Primal Wasserstein DICE（PW-DICE），它将学习者和专家状态占用之间的 primal Wasserstein 距离降到最小，并使用一个对比学习的距离作为下面距离的metric。* Results: 我们理论上证明了 PW-DICE 框架是 SMODICE 的一种总结，并将 $f$- divergence 和 Wasserstein 最小化联系起来。实验结果表明，PW-DICE 在多个测试床上超越了多种状态之前的方法。<details>
<summary>Abstract</summary>
In real-world scenarios, arbitrary interactions with the environment can often be costly, and actions of expert demonstrations are not always available. To reduce the need for both, Offline Learning from Observations (LfO) is extensively studied, where the agent learns to solve a task with only expert states and \textit{task-agnostic} non-expert state-action pairs. The state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the state occupancy divergence between the learner and expert policies. However, they are limited to either $f$-divergences (KL and $\chi^2$) or Wasserstein distance with Rubinstein duality, the latter of which constrains the underlying distance metric crucial to the performance of Wasserstein-based solutions. To address this problem, we propose Primal Wasserstein DICE (PW-DICE), which minimizes the primal Wasserstein distance between the expert and learner state occupancies with a pessimistic regularizer and leverages a contrastively learned distance as the underlying metric for the Wasserstein distance. Theoretically, we prove that our framework is a generalization of the state-of-the-art, SMODICE, and unifies $f$-divergence and Wasserstein minimization. Empirically, we find that PW-DICE improves upon several state-of-the-art methods on multiple testbeds.
</details>
<details>
<summary>摘要</summary>
在实际场景中，不可预知的环境交互可能会很昂贵，而专家示范的动作不总是可获得。为了减少这两种成本，半线性学习从观察（LfO）得到了广泛的研究，其中agent learns to solve a task with only expert states and task-agnostic non-expert state-action pairs。当前的DIstribution Correction Estimation（DICE）方法 minimum the state occupancy divergence between the learner and expert policies，但它们受到 $f$-divergence（KL和$\chi^2）或 Wasserstein distance with Rubinstein duality的限制，后者对于 Wasserstein-based solutions的性能具有关键的下面距离度量。为解决这个问题，我们提出了 Primal Wasserstein DICE（PW-DICE），它 minimum the primal Wasserstein distance between the expert and learner state occupancies with a pessimistic regularizer，并使用一个 contrastively learned distance as the underlying metric for the Wasserstein distance。理论上，我们证明了我们的框架是 state-of-the-art SMODICE 的一般化，并将 $f$-divergence和Wasserstein minimization unify。实际上，我们发现 PW-DICE 在多个测试床上比多种 state-of-the-art 方法表现更好。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Solution-for-Offline-Imitation-from-Observations-and-Examples-with-Possibly-Incomplete-Trajectories"><a href="#A-Simple-Solution-for-Offline-Imitation-from-Observations-and-Examples-with-Possibly-Incomplete-Trajectories" class="headerlink" title="A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories"></a>A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01329">http://arxiv.org/abs/2311.01329</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaiyan289/tailo">https://github.com/kaiyan289/tailo</a></li>
<li>paper_authors: Kai Yan, Alexander G. Schwing, Yu-Xiong Wang</li>
<li>for: 解决在缺乏专家动作的情况下，从观察中学习模式动作的问题。</li>
<li>methods: 使用权重行为做假的方法，并使用一个识别器来识别专家状态。</li>
<li>results: 在多个测试平台上，TAILO表现更加稳定和有效，特别是在 incomplete trajectories 的情况下。<details>
<summary>Abstract</summary>
Offline imitation from observations aims to solve MDPs where only task-specific expert states and task-agnostic non-expert state-action pairs are available. Offline imitation is useful in real-world scenarios where arbitrary interactions are costly and expert actions are unavailable. The state-of-the-art "DIstribution Correction Estimation" (DICE) methods minimize divergence of state occupancy between expert and learner policies and retrieve a policy with weighted behavior cloning; however, their results are unstable when learning from incomplete trajectories, due to a non-robust optimization in the dual domain. To address the issue, in this paper, we propose Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses a discounted sum along the future trajectory as the weight for weighted behavior cloning. The terms for the sum are scaled by the output of a discriminator, which aims to identify expert states. Despite simplicity, TAILO works well if there exist trajectories or segments of expert behavior in the task-agnostic data, a common assumption in prior work. In experiments across multiple testbeds, we find TAILO to be more robust and effective, particularly with incomplete trajectories.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified ChineseOffline imitation from observations aims to solve MDPs where only task-specific expert states and task-agnostic non-expert state-action pairs are available. Offline imitation is useful in real-world scenarios where arbitrary interactions are costly and expert actions are unavailable. The state-of-the-art "DIstribution Correction Estimation" (DICE) methods minimize divergence of state occupancy between expert and learner policies and retrieve a policy with weighted behavior cloning; however, their results are unstable when learning from incomplete trajectories, due to a non-robust optimization in the dual domain. To address the issue, in this paper, we propose Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses a discounted sum along the future trajectory as the weight for weighted behavior cloning. The terms for the sum are scaled by the output of a discriminator, which aims to identify expert states. Despite simplicity, TAILO works well if there exist trajectories or segments of expert behavior in the task-agnostic data, a common assumption in prior work. In experiments across multiple testbeds, we find TAILO to be more robust and effective, particularly with incomplete trajectories.中文简体版：<<SYS>>将文本翻译成中文简体版从观察中进行假扮，目标是解决MDPs，只有任务专家状态和任务非专家动作对组合可用。假扮在实际场景中很有用，因为专家动作是不可预测的。现状的“分布式修正估计”（DICE）方法减少专家和学习政策之间状态占据的差异，并提取一个政策，但是它们在学习部分轨迹时不稳定，这是因为附加的双域优化不稳定。为解决这个问题，在这篇论文中，我们提议使用轨迹意识的假扮学习（TAILO）。TAILO使用未来轨迹的折扣和学习器输出来权重假扮行为。在实验中，我们发现TAILO比DICE更加稳定和有效，特别是在部分轨迹时。
</details></li>
</ul>
<hr>
<h2 id="Better-Together-Enhancing-Generative-Knowledge-Graph-Completion-with-Language-Models-and-Neighborhood-Information"><a href="#Better-Together-Enhancing-Generative-Knowledge-Graph-Completion-with-Language-Models-and-Neighborhood-Information" class="headerlink" title="Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information"></a>Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01326">http://arxiv.org/abs/2311.01326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/screemix/kgc-t5-with-neighbors">https://github.com/screemix/kgc-t5-with-neighbors</a></li>
<li>paper_authors: Alla Chepurova, Aydar Bulatov, Yuri Kuratov, Mikhail Burtsev</li>
<li>for: 本研究旨在解决现实世界知识图（KG）中的不完teness问题，提高知识图完teness。</li>
<li>methods: 本研究使用语音模型（如T5和KGT5）来预测尾节点，并包含节点邻居信息以改进知识图完teness方法。</li>
<li>results: 研究表明，包含节点邻居信息可以提高知识图完teness方法的性能，在 inductive 和 transductive Wikidata 子集上都超过 KGT5 和传统知识图完teness方法。 Additionally, the study shows the importance of neighborhood information in model prediction and points out a way to significantly improve KGC through more effective neighborhood selection.<details>
<summary>Abstract</summary>
Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which limits their potential performance. Knowledge Graph Completion (KGC) techniques aim to address this issue. However, traditional KGC methods are computationally intensive and impractical for large-scale KGs, necessitating the learning of dense node embeddings and computing pairwise distances. Generative transformer-based language models (e.g., T5 and recent KGT5) offer a promising solution as they can predict the tail nodes directly. In this study, we propose to include node neighborhoods as additional information to improve KGC methods based on language models. We examine the effects of this imputation and show that, on both inductive and transductive Wikidata subsets, our method outperforms KGT5 and conventional KGC approaches. We also provide an extensive analysis of the impact of neighborhood on model prediction and show its importance. Furthermore, we point the way to significantly improve KGC through more effective neighborhood selection.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Scattering-Vision-Transformer-Spectral-Mixing-Matters"><a href="#Scattering-Vision-Transformer-Spectral-Mixing-Matters" class="headerlink" title="Scattering Vision Transformer: Spectral Mixing Matters"></a>Scattering Vision Transformer: Spectral Mixing Matters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01310">http://arxiv.org/abs/2311.01310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Badri N. Patro, Vijay Srinivas Agneeswaran</li>
<li>for: 这个论文主要针对 Computer Vision 领域中的图像分类、实例分割和对象检测任务，尝试解决注意力复杂性和图像细节捕捉问题。</li>
<li>methods: 该论文提出了一种新的方法 called Scattering Vision Transformer (SVT)，它包括一个spectral scattering网络，可以帮助捕捉图像细节。SVT还引入了一种特殊的 spectral gating 网络，使得计算复杂度得到了降低。</li>
<li>results: 根据论文的实验结果，SVT在 ImageNet 数据集上达到了 state-of-the-art 性能，与 LiTv2 和 iFormer 相比，SVT-H-S 达到了 84.2% 的 top-1 准确率，SVT-H-B 达到了 85.2%（基本版本中的 state-of-the-art），SVT-H-L 达到了 85.7%（大版本中的 state-of-the-art）。SVT 还在其他视觉任务中表现出色，比如实例分割任务。此外，SVT 在标准的 CIFAR10、CIFAR100、Oxford Flower 和 Stanford Car 数据集上也表现出优异的转移学习能力。<details>
<summary>Abstract</summary>
Vision transformers have gained significant attention and achieved state-of-the-art performance in various computer vision tasks, including image classification, instance segmentation, and object detection. However, challenges remain in addressing attention complexity and effectively capturing fine-grained information within images. Existing solutions often resort to down-sampling operations, such as pooling, to reduce computational cost. Unfortunately, such operations are non-invertible and can result in information loss. In this paper, we present a novel approach called Scattering Vision Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally scattering network that enables the capture of intricate image details. SVT overcomes the invertibility issue associated with down-sampling operations by separating low-frequency and high-frequency components. Furthermore, SVT introduces a unique spectral gating network utilizing Einstein multiplication for token and channel mixing, effectively reducing complexity. We show that SVT achieves state-of-the-art performance on the ImageNet dataset with a significant reduction in a number of parameters and FLOPS. SVT shows 2\% improvement over LiTv2 and iFormer. SVT-H-S reaches 84.2\% top-1 accuracy, while SVT-H-B reaches 85.2\% (state-of-art for base versions) and SVT-H-L reaches 85.7\% (again state-of-art for large versions). SVT also shows comparable results in other vision tasks such as instance segmentation. SVT also outperforms other transformers in transfer learning on standard datasets such as CIFAR10, CIFAR100, Oxford Flower, and Stanford Car datasets. The project page is available on this webpage.\url{https://badripatro.github.io/svt/}.
</details>
<details>
<summary>摘要</summary>
“vision transformer”已经受到了广泛关注，并在不同的计算机视觉任务中取得了前一等的性能，包括图像分类、实例分类和物体检测。然而，在处理注意力复杂性和细节资讯方面仍然存在挑战。现有的解决方案通常是透过下推运算，例如滤波器，以减少计算成本。然而，这些运算是不可逆的，可能会导致资讯损失。在本文中，我们提出了一个新的方法 called Scattering Vision Transformer (SVT)，以解决这些挑战。SVT包括一个spectrally scattering网络，可以对图像中的细节进行捕捉。SVT绕过下推运算所带来的倒数易变性问题，并且将低频和高频 ком成分分离。此外，SVT引入了单一的 спектраль闸道网络，使用爱因斯坦 multiplication 进行对token和通道的混合，实现了缩减复杂性。我们展示了SVT在ImageNet dataset上实现了前一等的性能，并且显著减少了总参数和FLOPS数。SVT与LiTv2和iFormer相比，提高了2%的性能。SVT-H-S实现了84.2%的顶部一致率，SVT-H-B实现了85.2%的顶部一致率（大版本的最佳性能），SVT-H-L实现了85.7%的顶部一致率（大版本的最佳性能）。SVT还在其他视觉任务中展示了相似的结果，例如实例分类。此外，SVT在标准的dataset上，如CIFAR10、CIFAR100、牛津花园和斯坦福汽车dataset上也展示了比较好的结果。SVT的专案页面可以在以下网址中找到：\url{https://badripatro.github.io/svt/}.
</details></li>
</ul>
<hr>
<h2 id="AWEQ-Post-Training-Quantization-with-Activation-Weight-Equalization-for-Large-Language-Models"><a href="#AWEQ-Post-Training-Quantization-with-Activation-Weight-Equalization-for-Large-Language-Models" class="headerlink" title="AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models"></a>AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01305">http://arxiv.org/abs/2311.01305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baisong Li, Xingwang Wang, Haixiao Xu</li>
<li>for: 提高大型语言模型（LLMs）的计算和存储成本，以提高模型的可扩展性和可靠性。</li>
<li>methods: 提出了一种无需额外训练的post-training方法，通过通道平衡来弥合权重和活动量的量化难度差异，从而实现模型的最佳性能。</li>
<li>results: 对各种流行的模型（如LLaMA和OPT）进行了广泛的实验，证明了AWEQ方法在post-training量化中的优越性，并且在8位权重和活动（W8A8）量化中达到了最高性能。<details>
<summary>Abstract</summary>
Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA and OPT demonstrate that AWEQ outperforms all existing post-training quantization methods for large models.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有多种任务的出色表现，但它们带来了重要的计算和储存成本。量化这些模型是一种有效的方法来解决这个问题。然而，现有的方法难以寻求模型精度和硬件效率之间的平衡。这是我们引入AWEQ，一种不需要额外训练成本的后训练方法。AWEQ在超低位数量化和8位构成元素（W8A8）量化中表现出色。对于模型的量化难度，权重量化比 activation 量化更容易。AWEQ将活动量化问题转移到权重中，实现了两者之间的平衡，因此提高了性能。我们进一步改进了均衡方法，以减少量化偏误错误，保证模型的稳定性。实验结果显示，AWEQ在各种流行的模型，如LLaMA和OPT上都大大超越了现有的后训练量化方法。
</details></li>
</ul>
<hr>
<h2 id="TRIALSCOPE-A-Unifying-Causal-Framework-for-Scaling-Real-World-Evidence-Generation-with-Biomedical-Language-Models"><a href="#TRIALSCOPE-A-Unifying-Causal-Framework-for-Scaling-Real-World-Evidence-Generation-with-Biomedical-Language-Models" class="headerlink" title="TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models"></a>TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01301">http://arxiv.org/abs/2311.01301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier González, Cliff Wong, Zelalem Gero, Jass Bagga, Risa Ueno, Isabel Chien, Eduard Orakvin, Emre Kiciman, Aditya Nori, Roshanthi Weerasinghe, Rom S. Leidner, Brian Piening, Tristan Naumann, Carlo Bifulco, Hoifung Poon</li>
<li>for: 该论文旨在 оптимизиATION OF HEALTHCARE DELIVERY AND ACCELERATING BIOMEDICAL DISCOVERY 通过利用实际数据，以提高医疗服务质量和生物医学发现。</li>
<li>methods: 该论文使用了生物医学语言模型来结构化临床文本，并使用高级概率模型进行噪声除除和替换，同时应用了现代 causal inference 技术来解决常见的干扰因素。</li>
<li>results: 该论文通过使用临床试验规范来生成和理解临床假设，并在一个大规模的真实世界数据集上进行了广泛的实验和分析，并得到了高质量的结构化数据和与知名肿瘤试验的相似结果。<details>
<summary>Abstract</summary>
The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million cancer patients from a large US healthcare network, we show that TRIALSCOPE can produce high-quality structuring of real-world data and generates comparable results to marquee cancer trials. In addition to facilitating in-silicon clinical trial design and optimization, TRIALSCOPE may be used to empower synthetic controls, pragmatic trials, post-market surveillance, as well as support fine-grained patient-like-me reasoning in precision diagnosis and treatment.
</details>
<details>
<summary>摘要</summary>
随着数字化的迅速进程，现实世界中的数据提供了不可思议的机会，以便优化医疗服务和加速生物医学发现。然而，实际上，这些数据通常存在干扰和干扰因素。在这篇论文中，我们介绍了一种名为TRIALSCOPE的框架，用于从人口水平的观察数据中提取现实世界的证据。TRIALSCOPE利用生物医学语言模型来结构临床文本，在大规模上进行混杂和替换，并应用了当前的 causal inference 技术来战胜常见的干扰因素。使用临床试验规范作为普通表示，TRIALSCOPE提供了一个启用和理解临床假设的全自动解决方案。在对一个大型现实世界数据集（包含 более一百万美国医疗网络中的肿瘤患者）进行了广泛的实验和分析后，我们发现TRIALSCOPE可以生成高质量的现实世界数据结构，并且与知名肿瘤试验的结果相比较。除了促进固态临床试验设计和优化之外，TRIALSCOPE还可以用于强化 synthetic control， Pragmatic trials， post-market surveillance，以及支持精细化的患者如我 reasoning 在精准诊断和治疗方面。
</details></li>
</ul>
<hr>
<h2 id="UniFolding-Towards-Sample-efficient-Scalable-and-Generalizable-Robotic-Garment-Folding"><a href="#UniFolding-Towards-Sample-efficient-Scalable-and-Generalizable-Robotic-Garment-Folding" class="headerlink" title="UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding"></a>UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01267">http://arxiv.org/abs/2311.01267</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaoxiaoxh/UniFolding">https://github.com/xiaoxiaoxh/UniFolding</a></li>
<li>paper_authors: Han Xue, Yutong Li, Wenqiang Xu, Huanyu Li, Dongzhe Zheng, Cewu Lu</li>
<li>for: 这 paper 探讨了一种Sample-Efficient, Scalable, and Generalizable Robotic System for Unfolding and Folding Various Garments。</li>
<li>methods: 这 paper 使用了提议的 UFONet 神经网络，将 unfolding 和 folding 决策集成到一个单一的策略模型中，可以适应不同的衣物类型和状态。</li>
<li>results: 这 paper 测试了两种衣物类型：长袖和短袖衬衣，并对 20 件衣物进行了性能评估，结果表明 UniFolding 系统可以在不同的 texture、shape 和材料下提供高效的 unfolding 和 folding 功能。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This paper explores the development of UniFolding, a sample-efficient, scalable, and generalizable robotic system for unfolding and folding various garments. UniFolding employs the proposed UFONet neural network to integrate unfolding and folding decisions into a single policy model that is adaptable to different garment types and states. The design of UniFolding is based on a garment's partial point cloud, which aids in generalization and reduces sensitivity to variations in texture and shape. The training pipeline prioritizes low-cost, sample-efficient data collection. Training data is collected via a human-centric process with offline and online stages. The offline stage involves human unfolding and folding actions via Virtual Reality, while the online stage utilizes human-in-the-loop learning to fine-tune the model in a real-world setting. The system is tested on two garment types: long-sleeve and short-sleeve shirts. Performance is evaluated on 20 shirts with significant variations in textures, shapes, and materials. More experiments and videos can be found in the supplementary materials and on the website: https://unifolding.robotflow.ai
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文探讨了一种可靠、扩展性强、通用的机器人系统，用于不同类型的衣服的打包和卷起。该系统使用提议的UFONet神经网络，将打包和卷起的决策集成到一个单一的政策模型中，以适应不同的衣服类型和状态。设计基于衣服的部分点云，帮助总体化和降低不同 texture和形状的敏感性。训练管道强调低成本、样本效率的数据采集。训练数据通过人类中心的过程收集，包括在虚拟现实环境中完成人类 unfolding和folding 动作。在线阶段通过人类 loops 学习来细化模型，并在实际环境中进行测试。系统在长袖和短袖上测试了20件衣服，其中具有显著的文本ure、形状和材料的变化。更多实验和视频可以在补充材料和网站：https://unifolding.robotflow.ai 中找到。
</details></li>
</ul>
<hr>
<h2 id="Expressive-TTS-Driven-by-Natural-Language-Prompts-Using-Few-Human-Annotations"><a href="#Expressive-TTS-Driven-by-Natural-Language-Prompts-Using-Few-Human-Annotations" class="headerlink" title="Expressive TTS Driven by Natural Language Prompts Using Few Human Annotations"></a>Expressive TTS Driven by Natural Language Prompts Using Few Human Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01260">http://arxiv.org/abs/2311.01260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanglei Zhang, Yiwei Guo, Sen Liu, Xie Chen, Kai Yu</li>
<li>for: 这研究旨在提供一种可控制的 expresive TTS 模型，无需大量的风格标注数据。</li>
<li>methods: 该方法使用大型自然语言模型（LLM）将 expresive TTS 转化为一种风格检索任务，通过选择最佳匹配的风格参考语音来控制 TTS 管道 Synthesize 语音。</li>
<li>results: 实验结果表明，FS-TTS 可以充分利用 LLM 的 semantics 推理能力，从输入文本或用户定义的风格描述中检索所需的风格。这 führt 到通过 TTS 管道 Synthesize 出的语音与指定的风格高度吻合。<details>
<summary>Abstract</summary>
Expressive text-to-speech (TTS) aims to synthesize speeches with human-like tones, moods, or even artistic attributes. Recent advancements in expressive TTS empower users with the ability to directly control synthesis style through natural language prompts. However, these methods often require excessive training with a significant amount of style-annotated data, which can be challenging to acquire. Moreover, they may have limited adaptability due to fixed style annotations. In this work, we present FreeStyleTTS (FS-TTS), a controllable expressive TTS model with minimal human annotations. Our approach utilizes a large language model (LLM) to transform expressive TTS into a style retrieval task. The LLM selects the best-matching style references from annotated utterances based on external style prompts, which can be raw input text or natural language style descriptions. The selected reference guides the TTS pipeline to synthesize speeches with the intended style. This innovative approach provides flexible, versatile, and precise style control with minimal human workload. Experiments on a Mandarin storytelling corpus demonstrate FS-TTS's proficiency in leveraging LLM's semantic inference ability to retrieve desired styles from either input text or user-defined descriptions. This results in synthetic speeches that are closely aligned with the specified styles.
</details>
<details>
<summary>摘要</summary>
文本译文：文本调读技术（TTS）目的是实时生成人工语音，具有人类语音的调读风格、情感和艺术性。现代的表达式TTS技术允许用户通过自然语言提示来直接控制合成类型。然而，这些方法通常需要大量的类型标注数据，实现可能困难。此外，它们可能具有固定类型标注的局限性。在这个工作中，我们提出了FreeStyleTTS（FS-TTS），一个可控的表达式TTS模型，仅需少量人工标注。我们的方法利用大型自然语言模型（LLM）将表达式TTS转换为一个类型搜寻任务。LLM选择基于标注utterance的最佳匹配式 referent，并将其用于合成语音的指导。这个创新的方法提供了高度可调、多元化和精确的类型控制，仅需 minimal human workload。实验结果显示，FS-TTS可以充分利用LLM的semantic inference能力，从input text或用户定义的描述中找到所需的类型。这 resulted in 合成语音具有所需的类型。
</details></li>
</ul>
<hr>
<h2 id="Formal-Methods-for-Autonomous-Systems"><a href="#Formal-Methods-for-Autonomous-Systems" class="headerlink" title="Formal Methods for Autonomous Systems"></a>Formal Methods for Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01258">http://arxiv.org/abs/2311.01258</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Tichakorn Wongpiromsarn, Mahsa Ghasemi, Murat Cubuktepe, Georgios Bakirtzis, Steven Carr, Mustafa O. Karabag, Cyrus Neary, Parham Gohari, Ufuk Topcu</li>
<li>for: 本文提供了形式方法在自动化系统领域的应用现状的报告。</li>
<li>methods: 本文使用了多种形式方法，包括关闭系统、反应式和概率设定，以验证和生成系统行为的正式保证。</li>
<li>results: 本文描述了一些应用形式方法的成果，包括对不确定性的处理、学习使用形式方法的限制、监控系统的设计等。<details>
<summary>Abstract</summary>
Formal methods refer to rigorous, mathematical approaches to system development and have played a key role in establishing the correctness of safety-critical systems. The main building blocks of formal methods are models and specifications, which are analogous to behaviors and requirements in system design and give us the means to verify and synthesize system behaviors with formal guarantees.   This monograph provides a survey of the current state of the art on applications of formal methods in the autonomous systems domain. We consider correct-by-construction synthesis under various formulations, including closed systems, reactive, and probabilistic settings. Beyond synthesizing systems in known environments, we address the concept of uncertainty and bound the behavior of systems that employ learning using formal methods. Further, we examine the synthesis of systems with monitoring, a mitigation technique for ensuring that once a system deviates from expected behavior, it knows a way of returning to normalcy. We also show how to overcome some limitations of formal methods themselves with learning. We conclude with future directions for formal methods in reinforcement learning, uncertainty, privacy, explainability of formal methods, and regulation and certification.
</details>
<details>
<summary>摘要</summary>
Formal methods refer to rigorous, mathematical approaches to system development and have played a key role in establishing the correctness of safety-critical systems. The main building blocks of formal methods are models and specifications, which are analogous to behaviors and requirements in system design and give us the means to verify and synthesize system behaviors with formal guarantees.  This monograph provides a survey of the current state of the art on applications of formal methods in the autonomous systems domain. We consider correct-by-construction synthesis under various formulations, including closed systems, reactive, and probabilistic settings. Beyond synthesizing systems in known environments, we address the concept of uncertainty and bound the behavior of systems that employ learning using formal methods. Further, we examine the synthesis of systems with monitoring, a mitigation technique for ensuring that once a system deviates from expected behavior, it knows a way of returning to normalcy. We also show how to overcome some limitations of formal methods themselves with learning. We conclude with future directions for formal methods in reinforcement learning, uncertainty, privacy, explainability of formal methods, and regulation and certification.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="An-energy-based-comparative-analysis-of-common-approaches-to-text-classification-in-the-Legal-domain"><a href="#An-energy-based-comparative-analysis-of-common-approaches-to-text-classification-in-the-Legal-domain" class="headerlink" title="An energy-based comparative analysis of common approaches to text classification in the Legal domain"></a>An energy-based comparative analysis of common approaches to text classification in the Legal domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01256">http://arxiv.org/abs/2311.01256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sinan Gultekin, Achille Globo, Andrea Zugarini, Marco Ernandes, Leonardo Rigutini</li>
<li>for: 这篇论文的目的是评估大自然语言处理器（LLM）和传统方法（如支持向量机）在LexGLUE测试benchmark上的表现，并考虑到了性能（标准指标）以外的因素，如时间、能耗和成本。</li>
<li>methods: 这篇论文使用了详细的量化比较，包括训练-验证-测试循环的评估，以及生产阶段和实际应用阶段的评估。</li>
<li>results: 结果表明， simplest algorithms 经常可以达到大LLMs的性能，但具有较低的能耗和资源需求。这些结果可能会导致公司在选择机器学习（ML）解决方案时包括额外评估。<details>
<summary>Abstract</summary>
Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they follow different implementation procedures and also require different resources. The results indicate that very often, the simplest algorithms achieve performance very close to that of large LLMs but with very low power consumption and lower resource demands. The results obtained could suggest companies to include additional evaluations in the choice of Machine Learning (ML) solutions.
</details>
<details>
<summary>摘要</summary>
大多数机器学习研究强调最佳解决方案的性能。然而，在尝试创造最高性能的模型时，有许多重要因素经常被忽略，而这些因素在实际应用中应该仔细考虑。事实上，有时性能之间的差异非常小，而生产成本、能源消耗和碳脚印则应该被考虑。大型自然语言模型（LLM）在学术和产业中广泛应用，以解决自然语言处理（NLP）问题。在这项工作中，我们提供了 LexGLUE 竞赛奖励的详细量化比较，包括性能（标准指标）和代表性指标（如时间、能源消耗和成本）。在我们的分析中，我们分 separately 评估预测阶段（模型选择）和生产阶段，因为它们采用不同的实现方式和需要不同的资源。结果显示，经常情况下，最简单的算法可以与大型 LLM 的性能几乎相当，但具有非常低的电力消耗和资源需求。这些结果可能会让公司包括机器学习（ML）解决方案的评估在内。
</details></li>
</ul>
<hr>
<h2 id="Push-it-to-the-Demonstrated-Limit-Multimodal-Visuotactile-Imitation-Learning-with-Force-Matching"><a href="#Push-it-to-the-Demonstrated-Limit-Multimodal-Visuotactile-Imitation-Learning-with-Force-Matching" class="headerlink" title="Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching"></a>Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01248">http://arxiv.org/abs/2311.01248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trevor Ablett, Oliver Limoyo, Adam Sigal, Affan Jilani, Jonathan Kelly, Kaleem Siddiqi, Francois Hogan, Gregory Dudek</li>
<li>for: 这个论文主要针对的是使用光学皮肤感知器进行机器人 manipulate 任务中的粘质感知。</li>
<li>methods: 该论文使用了灵活的光学皮肤感知器，可以同时获取视觉和皮肤信息。在训练过程中，使用了感觉学习的方法，通过对人类示范者的力学特征进行学习，生成一个更加符合人类的力学特征的力学Profile。</li>
<li>results: 研究结果表明，通过结合视觉和皮肤感知，可以提高机器人的 manipulate 任务性能。在多种观察配置下，对比视觉数据和视觉&#x2F;皮肤感知数据，研究发现，皮肤感知对于力学学习和任务反馈具有重要的作用。<details>
<summary>Abstract</summary>
Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a real robotic manipulator with door-opening and closing tasks, including over 3,000 real test episodes. Our results highlight the importance of tactile sensing for imitation learning, both for data collection to allow force matching, and for policy execution to allow accurate task feedback.
</details>
<details>
<summary>摘要</summary>
optical tactile sensors 已经成为了在机器人操作中获取密集的触感信息的有效手段。一种最近引入的 `see-through-your-skin'（STS）变体的这种感应器具有视觉和感觉两种模式，通过利用半透明表面和可控的照明来实现。在这项工作中，我们研究了将视觉感觉与模仿学习结合使用以提高接触充满的抓取任务。首先，我们使用了拟合力测量和一种新的算法来从抗阻教学中获得更好地匹配人类示范者的力脉冲。其次，我们添加了视觉/感觉 STS 模式切换作为控制策略输出，使感应器的应用更加简单。最后，我们研究了多种观察配置，比较和对比视觉数据和感觉数据（均有和无模式切换）的价值，以及视觉数据来自机器人手臂上的眼在手中摄像头。我们在一个真实的机器人抓取机器上进行了大量实验，包括超过 3,000 个真实测试集。我们的结果表明，感觉感知对于模仿学习是非常重要的，不仅用于数据采集以允许力脉冲匹配，还用于策略执行以提供精准任务反馈。
</details></li>
</ul>
<hr>
<h2 id="FacadeNet-Conditional-Facade-Synthesis-via-Selective-Editing"><a href="#FacadeNet-Conditional-Facade-Synthesis-via-Selective-Editing" class="headerlink" title="FacadeNet: Conditional Facade Synthesis via Selective Editing"></a>FacadeNet: Conditional Facade Synthesis via Selective Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01240">http://arxiv.org/abs/2311.01240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiangos Georgiou, Marios Loizou, Tom Kelly, Melinos Averkiou</li>
<li>for: 这个论文是为了Synthesizing building facade images from diverse viewpoints，即生成不同视角的建筑facade图像。</li>
<li>methods: 这个方法使用了一种conditional GAN，接受一个建筑facade的单一视图以及所需的视点信息，并生成图像。为了精确地修改视点依赖的元素（如窗户和门）而保留视角无关的元素（如墙壁），我们引入了选择性编辑模块。这个模块利用了一种预训练的视Transformer来提取图像嵌入。</li>
<li>results: 我们的实验表明，这种方法可以达到建筑facade生成领域的州际性表现，超过了其他方法。<details>
<summary>Abstract</summary>
We introduce FacadeNet, a deep learning approach for synthesizing building facade images from diverse viewpoints. Our method employs a conditional GAN, taking a single view of a facade along with the desired viewpoint information and generates an image of the facade from the distinct viewpoint. To precisely modify view-dependent elements like windows and doors while preserving the structure of view-independent components such as walls, we introduce a selective editing module. This module leverages image embeddings extracted from a pre-trained vision transformer. Our experiments demonstrated state-of-the-art performance on building facade generation, surpassing alternative methods.
</details>
<details>
<summary>摘要</summary>
我们介绍了 FacadeNet，一种深度学习方法，用于从多个视角生成建筑外墙图像。我们的方法使用一个条件GAN，接受一个建筑外墙的单个视图以及所需视角信息，并生成该视角下的建筑外墙图像。为精准地修改视角依赖的元素，如窗户和门，而保留视角独立的元素，如墙壁，我们引入了选择性编辑模块。这个模块利用一个预训练的视Transformer来提取图像嵌入。我们的实验表明，FacadeNet可以在建筑外墙生成中实现状态机器人表现，超过其他方法。
</details></li>
</ul>
<hr>
<h2 id="Navigating-Complex-Search-Tasks-with-AI-Copilots"><a href="#Navigating-Complex-Search-Tasks-with-AI-Copilots" class="headerlink" title="Navigating Complex Search Tasks with AI Copilots"></a>Navigating Complex Search Tasks with AI Copilots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01235">http://arxiv.org/abs/2311.01235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryen W. White</li>
<li>for: 这篇论文旨在探讨人工智能技术在搜索方面的应用和发展，以帮助搜索引擎更好地支持复杂的搜索任务。</li>
<li>methods: 本论文使用了生成式人工智能技术和助手（AI copilots），以帮助搜索者更好地完成复杂的搜索任务。</li>
<li>results: 本论文预示了AI copilots在搜索方面的应用将有普遍的改善和发展，并可能导致搜索引擎的重新设计和未来发展。<details>
<summary>Abstract</summary>
As many of us in the information retrieval (IR) research community know and appreciate, search is far from being a solved problem. Millions of people struggle with tasks on search engines every day. Often, their struggles relate to the intrinsic complexity of their task and the failure of search systems to fully understand the task and serve relevant results. The task motivates the search, creating the gap/problematic situation that searchers attempt to bridge/resolve and drives search behavior as they work through different task facets. Complex search tasks require more than support for rudimentary fact finding or re-finding. Research on methods to support complex tasks includes work on generating query and website suggestions, personalizing and contextualizing search, and developing new search experiences, including those that span time and space. The recent emergence of generative artificial intelligence (AI) and the arrival of assistive agents, or copilots, based on this technology, has the potential to offer further assistance to searchers, especially those engaged in complex tasks. There are profound implications from these advances for the design of intelligent systems and for the future of search itself. This article, based on a keynote by the author at the 2023 ACM SIGIR Conference, explores these issues and charts a course toward new horizons in information access guided by AI copilots.
</details>
<details>
<summary>摘要</summary>
很多我们在信息检索（IR）研究社区知道和钦佩的事实是，搜寻并不是已经解决的问题。每天，百万人都在搜索引擎上进行各种任务。常常，这些任务的问题在搜寻系统不够理解任务的情况下，无法提供相应的结果。这些任务驱使搜寻，创造出问题的差距和问题，并且驱动搜寻行为。复杂的搜寻任务需要更进一步的支持，不仅是基本的事实查找或重新找。研究人员在发展新的搜寻技术方面做出了很多努力，例如生成查询和网站建议、个性化和 contextualizing搜寻、开发新的搜寻经验，包括在时空中进行的搜寻。受到生成人工智能（AI）的启发，助手或副驾驶器的出现，将对搜寻者，特别是进行复杂任务的人，提供更多的帮助。这些进步将对智能系统的设计和未来的搜寻产生深远的影响。本文，基于作者在2023年ACM SIGIR会议上的关键演讲，探讨了这些问题，并寻找新的搜寻方向，受到AI助手带领。
</details></li>
</ul>
<hr>
<h2 id="Long-Story-Short-a-Summarize-then-Search-Method-for-Long-Video-Question-Answering"><a href="#Long-Story-Short-a-Summarize-then-Search-Method-for-Long-Video-Question-Answering" class="headerlink" title="Long Story Short: a Summarize-then-Search Method for Long Video Question Answering"></a>Long Story Short: a Summarize-then-Search Method for Long Video Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01233">http://arxiv.org/abs/2311.01233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiwan Chung, Youngjae Yu</li>
<li>for: This paper explores the ability of large language models like GPT-3 to adapt to new tasks without task-specific training data, specifically in the context of long multimodal narratives in multimedia content like drama, movies, and animation.</li>
<li>methods: The proposed framework, called Long Story Short, first summarizes the narrative of the video into a short plot and then searches for relevant parts of the video using CLIPCheck.</li>
<li>results: The model outperforms state-of-the-art supervised models by a large margin, demonstrating the potential of zero-shot QA for long videos.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文探讨了大语言模型如GPT-3在不需要任务特定训练数据的情况下是否能够扩展到新任务，特别是在叙事视频内容如电影、动画等中的长Multimodal narratives。</li>
<li>methods: 提议的框架是Long Story Short，它首先摘要了视频的叙事情节，然后使用CLIPCheck搜索问题相关的视频部分。</li>
<li>results: 模型超过了现有的超vised模型，强调了零shot QA的潜在性。<details>
<summary>Abstract</summary>
Large language models such as GPT-3 have demonstrated an impressive capability to adapt to new tasks without requiring task-specific training data. This capability has been particularly effective in settings such as narrative question answering, where the diversity of tasks is immense, but the available supervision data is small. In this work, we investigate if such language models can extend their zero-shot reasoning abilities to long multimodal narratives in multimedia content such as drama, movies, and animation, where the story plays an essential role. We propose Long Story Short, a framework for narrative video QA that first summarizes the narrative of the video to a short plot and then searches parts of the video relevant to the question. We also propose to enhance visual matching with CLIPCheck. Our model outperforms state-of-the-art supervised models by a large margin, highlighting the potential of zero-shot QA for long videos.
</details>
<details>
<summary>摘要</summary>
大型语言模型如GPT-3已经表现出适应新任务的能力，不需要专门的任务特有的训练数据。这种能力在叙事问答中 especial 有效，因为任务的多样性很大，但可用的监督数据很少。在这项工作中，我们 investigates 如果这些语言模型可以扩展其零shot 理解能力到长 multimedia 媒体内容，如电影、电视剧和动画，其中故事扮演着关键性的角色。我们提出了 Long Story Short，一个用于叙事视频问答的框架，首先摘要视频的叙事，然后在问题相关的部分搜索视频。我们还提出了CLIPCheck的Visual Matching Enhancement，我们的模型在比较之上大幅超越了现有的超vised模型，这 highlights 零shot QA 的潜在能力在长视频上。
</details></li>
</ul>
<hr>
<h2 id="Multi-Operational-Mathematical-Derivations-in-Latent-Space"><a href="#Multi-Operational-Mathematical-Derivations-in-Latent-Space" class="headerlink" title="Multi-Operational Mathematical Derivations in Latent Space"></a>Multi-Operational Mathematical Derivations in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01230">http://arxiv.org/abs/2311.01230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuro-symbolic-ai/latent_mathematical_reasoning">https://github.com/neuro-symbolic-ai/latent_mathematical_reasoning</a></li>
<li>paper_authors: Marco Valentino, Jordan Meadows, Lan Zhang, André Freitas</li>
<li>for: 这个论文研究了在潜在空间中对多个数学运算的合并，以实现表达推导的可能性。</li>
<li>methods: 作者引入了不同的多操作表示模式，将数学运算视为显式的几何变换，并利用符号计算机件构建了一个大规模的 derivation step 集合，包括 61K premises 和 6 种运算，分析每种模式在使用现有的神经编码器时的属性。</li>
<li>results: 研究发现，多操作模式是分离不同运算的关键，而单一运算的推导结论可以在原始表达编码器中分配。此外，作者还发现了不同的建筑选择对训练动力、结构组织和泛化造成了重大的影响，导致不同模式和编码器类型之间存在显著的差异。<details>
<summary>Abstract</summary>
This paper investigates the possibility of approximating multiple mathematical operations in latent space for expression derivation. To this end, we introduce different multi-operational representation paradigms, modelling mathematical operations as explicit geometric transformations. By leveraging a symbolic engine, we construct a large-scale dataset comprising 1.7M derivation steps stemming from 61K premises and 6 operators, analysing the properties of each paradigm when instantiated with state-of-the-art neural encoders. Specifically, we investigate how different encoding mechanisms can approximate equational reasoning in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation. Our empirical analysis reveals that the multi-operational paradigm is crucial for disentangling different operators, while discriminating the conclusions for a single operation is achievable in the original expression encoder. Moreover, we show that architectural choices can heavily affect the training dynamics, structural organisation, and generalisation of the latent space, resulting in significant variations across paradigms and classes of encoders.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diffusion-Models-for-Reinforcement-Learning-A-Survey"><a href="#Diffusion-Models-for-Reinforcement-Learning-A-Survey" class="headerlink" title="Diffusion Models for Reinforcement Learning: A Survey"></a>Diffusion Models for Reinforcement Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01223">http://arxiv.org/abs/2311.01223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apexrl/diff4rlsurvey">https://github.com/apexrl/diff4rlsurvey</a></li>
<li>paper_authors: Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Yong Yu, Weinan Zhang</li>
<li>for: 本文提供了Diffusion模型在强化学习（Reinforcement Learning，RL）领域的进展概述，并希望通过这篇评论来鼓励新的研究方向。</li>
<li>methods: 本文分析了当前RL算法遇到的一些挑战，然后提出了基于Diffusion模型的RL方法的分类，并详细介绍了它们如何解决这些挑战。</li>
<li>results: 本文介绍了Diffusion模型在各种RL相关任务中的成功应用，同时讨论了现有方法的局限性，并提出了未来研究方向的想法，包括提高模型性能和应用Diffusion模型到更广泛的任务。Here’s the full text in Simplified Chinese:本文提供了Diffusion模型在强化学习（Reinforcement Learning，RL）领域的进展概述，并希望通过这篇评论来鼓励新的研究方向。Diffusion模型在RL领域的应用已经超过了之前的方法，包括轨迹规划、表达政策类、数据生成器等。本文分析了当前RL算法遇到的一些挑战，然后提出了基于Diffusion模型的RL方法的分类，并详细介绍了它们如何解决这些挑战。此外，本文介绍了Diffusion模型在各种RL相关任务中的成功应用，同时讨论了现有方法的局限性，并提出了未来研究方向的想法，包括提高模型性能和应用Diffusion模型到更广泛的任务。您可以在<a target="_blank" rel="noopener" href="https://github.com/apexrl/Diff4RLSurvey%E4%B8%8A%E6%89%BE%E5%88%B0%E6%9B%B4%E5%A4%9A%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E5%92%8C%E6%96%87%E7%8C%AE%E3%80%82">https://github.com/apexrl/Diff4RLSurvey上找到更多相关资源和文献。</a><details>
<summary>Abstract</summary>
Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion models to broader tasks. We are actively maintaining a GitHub repository for papers and other related resources in applying diffusion models in RL: https://github.com/apexrl/Diff4RLSurvey .
</details>
<details>
<summary>摘要</summary>
Diffusion models 已经成为一种显著的生成模型，超过了之前的方法在样本质量和训练稳定性方面。最近的研究表明 diffusion models 在改进强化学习（RL）解决方案方面具有优势，包括轨迹规划器、表达政策类、数据合成器等。这篇评论旨在为这个emerging field提供一个概述，并希望能启发新的研究方向。首先，我们考虑了现在RL算法遇到的一些挑战。然后，我们提出了基于 diffusion models 在 RL 中扮演的不同角色的分类，并详细介绍了现有的挑战如何被解决。然后，我们详细介绍了 diffusion models 在各种 RL 相关任务中的成功应用，同时讨论了现有方法的局限性。最后，我们结束这篇评论，并对未来研究方向做出了一些建议，主要是增强模型性能和将 diffusion models 应用于更广泛的任务。我们 aktif maintenanceng a GitHub repository for papers and other related resources in applying diffusion models in RL: https://github.com/apexrl/Diff4RLSurvey。
</details></li>
</ul>
<hr>
<h2 id="Multi-view-Relation-Learning-for-Cross-domain-Few-shot-Hyperspectral-Image-Classification"><a href="#Multi-view-Relation-Learning-for-Cross-domain-Few-shot-Hyperspectral-Image-Classification" class="headerlink" title="Multi-view Relation Learning for Cross-domain Few-shot Hyperspectral Image Classification"></a>Multi-view Relation Learning for Cross-domain Few-shot Hyperspectral Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01212">http://arxiv.org/abs/2311.01212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henulwy/stbdip">https://github.com/henulwy/stbdip</a></li>
<li>paper_authors: Chun Liu, Longwei Yang, Zheng Li, Wei Yang, Zhigang Han, Jianzhong Guo, Junyong Yu</li>
<li>for: 这个论文主要针对跨Domain少数shot颜色成像分类问题，探讨如何将来自源Domain的大量标签样本中的专业知识转移到目标Domain中的任务中，仅具够几个标签样本进行分类。</li>
<li>methods: 本文提出了一个基于对比学习的方法，从不同的视角学习样本之间的关系，并将这些关系纳入模型学习过程中，以提高跨Domain少数shot颜色成像分类的性能。这个方法首先从不同的视角EXTRACT样本的特征，然后使用对比学习来学习样本之间的关系，最后将这些关系纳入模型学习过程中。</li>
<li>results: 我们的实验结果显示，在跨Domain少数shot颜色成像分类任务中，这个基于对比学习的方法能够提高模型的性能，并且与现有的方法相比，具有更好的一致性和稳定性。<details>
<summary>Abstract</summary>
Cross-domain few-shot hyperspectral image classification focuses on learning prior knowledge from a large number of labeled samples from source domain and then transferring the knowledge to the tasks which contain only few labeled samples in target domains. Following the metric-based manner, many current methods first extract the features of the query and support samples, and then directly predict the classes of query samples according to their distance to the support samples or prototypes. The relations between samples have not been fully explored and utilized. Different from current works, this paper proposes to learn sample relations from different views and take them into the model learning process, to improve the cross-domain few-shot hyperspectral image classification. Building on current DCFSL method which adopts a domain discriminator to deal with domain-level distribution difference, the proposed method applys contrastive learning to learn the class-level sample relations to obtain more discriminable sample features. In addition, it adopts a transformer based cross-attention learning module to learn the set-level sample relations and acquire the attentions from query samples to support samples. Our experimental results have demonstrated the contribution of the multi-view relation learning mechanism for few-shot hyperspectral image classification when compared with the state of the art methods.
</details>
<details>
<summary>摘要</summary>
Unlike current methods, this paper proposes a new approach that learns sample relations from different views and incorporates them into the model learning process to improve cross-domain few-shot hyperspectral image classification. Building on the current DCFSL method, which uses a domain discriminator to handle domain-level distribution differences, the proposed method uses contrastive learning to learn class-level sample relations and obtain more discriminative sample features. Additionally, it employs a transformer-based cross-attention learning module to learn set-level sample relations and acquire attention from query samples to support samples.Experimental results have shown that the proposed method outperforms state-of-the-art methods in few-shot hyperspectral image classification, thanks to the multi-view relation learning mechanism.
</details></li>
</ul>
<hr>
<h2 id="Attacking-Graph-Neural-Networks-with-Bit-Flips-Weisfeiler-and-Lehman-Go-Indifferent"><a href="#Attacking-Graph-Neural-Networks-with-Bit-Flips-Weisfeiler-and-Lehman-Go-Indifferent" class="headerlink" title="Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent"></a>Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01205">http://arxiv.org/abs/2311.01205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenz Kummer, Samir Moustafa, Nils N. Kriege, Wilfried N. Gansterer</li>
<li>for: 这篇论文旨在攻击图神经网络（Graph Neural Network，GNN）的Weight和Biases，而不是 tradicional的Graph Poisoning和Evasion攻击。</li>
<li>methods: 我们提出了首个特性为图神经网络的Bit Flip攻击， targets learned neighborhood aggregation functions in quantized message passing neural networks，使其难以识别图结构和丢失表达力。</li>
<li>results: 我们的研究表明，通过利用图神经网络特有的数学性质，可以大幅提高其对Bit Flip攻击的感受性。我们的攻击可以使最大表达能力的图同构网络（Graph Isomorphism Networks）的输出变为随机值，只需要flipping一小部分网络的比特。<details>
<summary>Abstract</summary>
Prior attacks on graph neural networks have mostly focused on graph poisoning and evasion, neglecting the network's weights and biases. Traditional weight-based fault injection attacks, such as bit flip attacks used for convolutional neural networks, do not consider the unique properties of graph neural networks. We propose the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and losing the expressivity of the Weisfeiler-Lehman test. Our findings suggest that exploiting mathematical properties specific to certain graph neural network architectures can significantly increase their vulnerability to bit flip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive Graph Isomorphism Networks trained on various graph property prediction datasets to random output by flipping only a small fraction of the network's bits, demonstrating its higher destructive power compared to a bit flip attack transferred from convolutional neural networks. Our attack is transparent and motivated by theoretical insights which are confirmed by extensive empirical results.
</details>
<details>
<summary>摘要</summary>
先前的攻击对图 neural network 主要集中在恶意修改图和逃脱，忽视了网络的权重和偏好。传统的权重基于的攻击，如 convolutional neural network 中的 bit flip 攻击，不考虑图 neural network 的独特特性。我们提出了 Injectivity Bit Flip Attack，首先针对 quantized message passing neural network 中的学习可能的邻接聚合函数，使其失去Distinguish 图结构的能力和 Weisfeiler-Lehman 测试的表达能力。我们的发现表明，特定的图 neural network 架构的数学性质可以使其更容易受到 bit flip 攻击。我们的攻击可以通过只flipping 一小部分网络的比特来使最大表达能力的图Isomorphism Networks 输出Random， demonstarting its higher destructive power compared to transferred bit flip attack from convolutional neural networks。我们的攻击是透明的，基于理论启示，并经过了广泛的实验验证。
</details></li>
</ul>
<hr>
<h2 id="Cross-Modal-Information-Guided-Network-using-Contrastive-Learning-for-Point-Cloud-Registration"><a href="#Cross-Modal-Information-Guided-Network-using-Contrastive-Learning-for-Point-Cloud-Registration" class="headerlink" title="Cross-Modal Information-Guided Network using Contrastive Learning for Point Cloud Registration"></a>Cross-Modal Information-Guided Network using Contrastive Learning for Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01202">http://arxiv.org/abs/2311.01202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ivanxie416/cmignet">https://github.com/ivanxie416/cmignet</a></li>
<li>paper_authors: Yifan Xie, Jihua Zhu, Shiqi Li, Pengcheng Shi</li>
<li>for: 本研究旨在提出一种新的多模态信息引导网络（CMIGNet），用于实现精度和稳定的点云注册。</li>
<li>methods: 我们首先将点云图像投影到2D图像上，并将多模态特征进行融合使用注意力机制。然后，我们采用两种对比学习策略，即重叠对比学习和跨模态对比学习，以确定关键点云特征。</li>
<li>results: 我们在多个 benchmark 数据集上进行了广泛的实验，结果显示，我们的网络可以准确地进行点云注册。<details>
<summary>Abstract</summary>
The majority of point cloud registration methods currently rely on extracting features from points. However, these methods are limited by their dependence on information obtained from a single modality of points, which can result in deficiencies such as inadequate perception of global features and a lack of texture information. Actually, humans can employ visual information learned from 2D images to comprehend the 3D world. Based on this fact, we present a novel Cross-Modal Information-Guided Network (CMIGNet), which obtains global shape perception through cross-modal information to achieve precise and robust point cloud registration. Specifically, we first incorporate the projected images from the point clouds and fuse the cross-modal features using the attention mechanism. Furthermore, we employ two contrastive learning strategies, namely overlapping contrastive learning and cross-modal contrastive learning. The former focuses on features in overlapping regions, while the latter emphasizes the correspondences between 2D and 3D features. Finally, we propose a mask prediction module to identify keypoints in the point clouds. Extensive experiments on several benchmark datasets demonstrate that our network achieves superior registration performance.
</details>
<details>
<summary>摘要</summary>
Specifically, we first incorporate the projected images from the point clouds and fuse the cross-modal features using the attention mechanism. Furthermore, we employ two contrastive learning strategies, namely overlapping contrastive learning and cross-modal contrastive learning. The former focuses on features in overlapping regions, while the latter emphasizes the correspondences between 2D and 3D features. Finally, we propose a mask prediction module to identify keypoints in the point clouds.Extensive experiments on several benchmark datasets demonstrate that our network achieves superior registration performance.
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-on-Edge-Sensing-Devices-A-Review"><a href="#Federated-Learning-on-Edge-Sensing-Devices-A-Review" class="headerlink" title="Federated Learning on Edge Sensing Devices: A Review"></a>Federated Learning on Edge Sensing Devices: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01201">http://arxiv.org/abs/2311.01201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Berrenur Saylam, Özlem Durmaz İncel</li>
<li>for: 本研究实际应用于Edge设备上的聚合学习，以解决传统机器学习技术所面临的隐私、硬件和连接限制问题。</li>
<li>methods: 本研究使用 Federated Learning（FL）策略，将聚合学习模型训练在Edge设备上，而不需要分享实际数据。</li>
<li>results: 本研究提出了一个基于FL的聚合学习方法，可以在Edge设备上进行实时数据分析和决策，并维护隐私和安全性。<details>
<summary>Abstract</summary>
The ability to monitor ambient characteristics, interact with them, and derive information about the surroundings has been made possible by the rapid proliferation of edge sensing devices like IoT, mobile, and wearable devices and their measuring capabilities with integrated sensors. Even though these devices are small and have less capacity for data storage and processing, they produce vast amounts of data. Some example application areas where sensor data is collected and processed include healthcare, environmental (including air quality and pollution levels), automotive, industrial, aerospace, and agricultural applications. These enormous volumes of sensing data collected from the edge devices are analyzed using a variety of Machine Learning (ML) and Deep Learning (DL) approaches. However, analyzing them on the cloud or a server presents challenges related to privacy, hardware, and connectivity limitations. Federated Learning (FL) is emerging as a solution to these problems while preserving privacy by jointly training a model without sharing raw data. In this paper, we review the FL strategies from the perspective of edge sensing devices to get over the limitations of conventional machine learning techniques. We focus on the key FL principles, software frameworks, and testbeds. We also explore the current sensor technologies, properties of the sensing devices and sensing applications where FL is utilized. We conclude with a discussion on open issues and future research directions on FL for further studies
</details>
<details>
<summary>摘要</summary>
“随着边缘感应设备的普及，例如IoT、手持式和穿戴式设备的数据量和感应功能，实现了监测环境特点、互动和获取环境信息的能力。这些设备小巧，储存空间和处理能力有限，但生成了巨量数据。一些应用领域包括医疗、环境（包括空气质量和污染水平）、汽车、工业、航空和农业应用。这些边缘感应数据通过多种机器学习（ML）和深度学习（DL）方法进行分析。但是，将数据分析到云端或服务器端存在隐私、硬件和连接限制的问题。联邦学习（FL）正在解决这些问题，并保持隐私性，无需共享原始数据。本文从边缘感应设备的角度，检视FL策略，并评估适用于边缘感应应用的软件框架和实验室。我们也探讨目前的感应技术、感应设备的性能和感应应用中FL的应用。我们结束时讨论未解决的问题和未来研究方向。”
</details></li>
</ul>
<hr>
<h2 id="AiluRus-A-Scalable-ViT-Framework-for-Dense-Prediction"><a href="#AiluRus-A-Scalable-ViT-Framework-for-Dense-Prediction" class="headerlink" title="AiluRus: A Scalable ViT Framework for Dense Prediction"></a>AiluRus: A Scalable ViT Framework for Dense Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01197">http://arxiv.org/abs/2311.01197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caddyless/ailurus">https://github.com/caddyless/ailurus</a></li>
<li>paper_authors: Jin Li, Yaoming Wang, Xiaopeng Zhang, Bowen Shi, Dongsheng Jiang, Chenglin Li, Wenrui Dai, Hongkai Xiong, Qi Tian</li>
<li>For: 提高 vision transformer (ViT) 模型在长序列处理方面的性能，特别是在高分辨率输入的 dense prediction 任务中。* Methods: 使用适应分辨率技术，将图像中不同区域的分辨率调整为不同的水平。在 ViT 中间层使用空间感知密度基于的聚合算法，选择代表性的 токен。然后，将其他 tokens 聚合到最近的代表 токен 中。这种策略可以减少 токен数量，使后续层可以处理减少的 токен序列，实现加速。* Results: 在三个不同的 dataset 上进行测试，并观察了promising的性能。例如，可以通过不需要微调的方式，将 “Segmenter ViT-L” 模型加速48% FPS。此外，我们的方法还可以加速 fine-tuning 过程。实验结果表明，可以在训练时间上产生52%的减少，同时加速2.46倍 FPS，只有0.09%的性能下降。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/caddyless/ailurus/tree/main">https://github.com/caddyless/ailurus/tree/main</a> 上找到。<details>
<summary>Abstract</summary>
Vision transformers (ViTs) have emerged as a prevalent architecture for vision tasks owing to their impressive performance. However, when it comes to handling long token sequences, especially in dense prediction tasks that require high-resolution input, the complexity of ViTs increases significantly. Notably, dense prediction tasks, such as semantic segmentation or object detection, emphasize more on the contours or shapes of objects, while the texture inside objects is less informative. Motivated by this observation, we propose to apply adaptive resolution for different regions in the image according to their importance. Specifically, at the intermediate layer of the ViT, we utilize a spatial-aware density-based clustering algorithm to select representative tokens from the token sequence. Once the representative tokens are determined, we proceed to merge other tokens into their closest representative token. Consequently, semantic similar tokens are merged together to form low-resolution regions, while semantic irrelevant tokens are preserved independently as high-resolution regions. This strategy effectively reduces the number of tokens, allowing subsequent layers to handle a reduced token sequence and achieve acceleration. We evaluate our proposed method on three different datasets and observe promising performance. For example, the "Segmenter ViT-L" model can be accelerated by 48% FPS without fine-tuning, while maintaining the performance. Additionally, our method can be applied to accelerate fine-tuning as well. Experimental results demonstrate that we can save 52% training time while accelerating 2.46 times FPS with only a 0.09% performance drop. The code is available at https://github.com/caddyless/ailurus/tree/main.
</details>
<details>
<summary>摘要</summary>
vision transformers (ViTs) 已经成为视觉任务中广泛使用的主流架构，尤其是在处理长token序列时，它们的性能很出色。然而，在 dense prediction 任务中，特别是 semantic segmentation 或 object detection，需要高分辨率的输入，这时 ViTs 的复杂度会增加显著。我们发现， dense prediction 任务中，对象的 outline 或形状更加重要，而内部的文字则更加不重要。基于这一点，我们提议应用适应性分辨率，对不同的图像区域进行不同的分辨率处理。在 ViT 的中间层次结构中，我们使用空间意识度的density-based clustering算法来选择表示性的token。然后，我们将其他token合并到最近的表示token中。因此，semantic相似的token会合并成低分辨率区域，而semantic不相关的token则独立保留为高分辨率区域。这种策略有效地减少了token数量，使后续层次可以处理减少后的token序列，并实现加速。我们在三个不同的dataset上进行了评估，并观察到了有前景的性能。例如，"Segmenter ViT-L" 模型可以通过48% FPS 的加速而不需要微调。此外，我们的方法还可以用于加速微调。实验结果表明，我们可以在训练时间上Save 52%，并且在加速2.46倍 FPS 时，只减少了0.09%的性能。代码可以在 <https://github.com/caddyless/ailurus/tree/main> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Batch-Bayesian-Optimization-for-Replicable-Experimental-Design"><a href="#Batch-Bayesian-Optimization-for-Replicable-Experimental-Design" class="headerlink" title="Batch Bayesian Optimization for Replicable Experimental Design"></a>Batch Bayesian Optimization for Replicable Experimental Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01195">http://arxiv.org/abs/2311.01195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongxiang Dai, Quoc Phong Nguyen, Sebastian Shenghong Tay, Daisuke Urano, Richalynn Leong, Bryan Kian Hsiang Low, Patrick Jaillet</li>
<li>for: 本文针对实验设计问题提出了一个框架，即批 Thompson 抽样体系 (BTS-RED)，用于处理多元实验条件的评估和重复测量。</li>
<li>methods: 本文提出了三种算法，分别为 BTS-RED-Known、BTS-RED-Unknown 和 Mean-Var-BTS-RED，用于处理不同的错误分布和对应的风险偏好。</li>
<li>results: 本文透过实验证明了这三种算法的可靠性和无损 regret 性，并在精确农业和 AutoML 实验中显示了它们的实际效果。<details>
<summary>Abstract</summary>
Many real-world experimental design problems (a) evaluate multiple experimental conditions in parallel and (b) replicate each condition multiple times due to large and heteroscedastic observation noise. Given a fixed total budget, this naturally induces a trade-off between evaluating more unique conditions while replicating each of them fewer times vs. evaluating fewer unique conditions and replicating each more times. Moreover, in these problems, practitioners may be risk-averse and hence prefer an input with both good average performance and small variability. To tackle both challenges, we propose the Batch Thompson Sampling for Replicable Experimental Design (BTS-RED) framework, which encompasses three algorithms. Our BTS-RED-Known and BTS-RED-Unknown algorithms, for, respectively, known and unknown noise variance, choose the number of replications adaptively rather than deterministically such that an input with a larger noise variance is replicated more times. As a result, despite the noise heteroscedasticity, both algorithms enjoy a theoretical guarantee and are asymptotically no-regret. Our Mean-Var-BTS-RED algorithm aims at risk-averse optimization and is also asymptotically no-regret. We also show the effectiveness of our algorithms in two practical real-world applications: precision agriculture and AutoML.
</details>
<details>
<summary>摘要</summary>
多个实验设计问题（a）会同时评估多个实验条件，并且每个条件会被重复多次，这是因为观察噪声很大且不均匀。给定一个固定的总预算，这会导致评估更多的独特条件 vs. 评估更少的独特条件的费用之间的权衡。此外，在这些问题中，实践者可能会偏爱风险观，因此偏好一个具有良好平均性和小变异性的输入。为了解决这两个挑战，我们提出了批 Thompson 采样 для可重现实验设计（BTS-RED）框架，该框架包括三种算法。我们的 BTS-RED-known 和 BTS-RED-unknown 算法，分别针对已知和未知噪声 variance，选择复制的数量适应而不是决定性地，以便在噪声不均匀的情况下，输入具有更大的噪声 variance 会被复制更多次。由于这些算法具有理论保证和朴素观察的折衔，它们在噪声不均匀情况下是 asymptotically no-regret。我们的 Mean-Var-BTS-RED 算法则是针对偏爱风险优化的，并且也是 asymptotically no-regret。我们还在精准农业和 AutoML 两个实际应用中证明了我们的算法的效果。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Confidence-and-Generative-AI"><a href="#Contextual-Confidence-and-Generative-AI" class="headerlink" title="Contextual Confidence and Generative AI"></a>Contextual Confidence and Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01193">http://arxiv.org/abs/2311.01193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shrey Jain, Zoë Hitzig, Pamela Mishkin</li>
<li>for: 该论文旨在面对生成式人工智能模型对有效人类communication的威胁，描述一些稳定communication的策略。</li>
<li>methods: 该论文使用的方法包括工具、技术和政策，分为两大类：含容策略和推动策略。含容策略目的是在生成式AI模型威胁下重新确定communication的上下文，而推动策略则是利用AI的进步提高mediated communication的隐私和真实性的期望。</li>
<li>results: 该论文的结果表明，采用合适的策略可以稳定communication在生成式AI模型的威胁下，并提高mediated communication的隐私和真实性。<details>
<summary>Abstract</summary>
Generative AI models perturb the foundations of effective human communication. They present new challenges to contextual confidence, disrupting participants' ability to identify the authentic context of communication and their ability to protect communication from reuse and recombination outside its intended context. In this paper, we describe strategies--tools, technologies and policies--that aim to stabilize communication in the face of these challenges. The strategies we discuss fall into two broad categories. Containment strategies aim to reassert context in environments where it is currently threatened--a reaction to the context-free expectations and norms established by the internet. Mobilization strategies, by contrast, view the rise of generative AI as an opportunity to proactively set new and higher expectations around privacy and authenticity in mediated communication.
</details>
<details>
<summary>摘要</summary>
生成AI模型对人类交流的基础产生了巨大的挑战。它们使得参与者无法正确地识别交流的 authentics 上下文和保护交流从其不良上下文中的重用和复制。在这篇论文中，我们描述了一些策略——工具、技术和政策——以稳定交流面临这些挑战。我们所讨论的策略分为两个大类。封装策略 aim to reassert context in environments where it is currently threatened——一种应对互联网所建立的无上下文期望和规范的反应。 mobilization strategies, by contrast, view the rise of generative AI as an opportunity to proactively set new and higher expectations around privacy and authenticity in mediated communication.
</details></li>
</ul>
<hr>
<h2 id="VIGraph-Self-supervised-Learning-for-Class-Imbalanced-Node-Classification"><a href="#VIGraph-Self-supervised-Learning-for-Class-Imbalanced-Node-Classification" class="headerlink" title="VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification"></a>VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01191">http://arxiv.org/abs/2311.01191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu</li>
<li>for: 此研究旨在解决图数据中类别不整齐的问题，提高类别不整齐的节点预测性能。</li>
<li>methods: 本研究提出了一种基于自助学习（SSL）的新方法，利用自身数据自带的缺失数据进行生成缺失类节点，从而提高类别不整齐的预测性能。</li>
<li>results: 实验结果表明，基于VGAE的VIGraph方法可以生成高质量的缺失类节点，提高类别不整齐的节点预测性能。<details>
<summary>Abstract</summary>
Class imbalance in graph data poses significant challenges for node classification. Existing methods, represented by SMOTE-based approaches, partially alleviate this issue but still exhibit limitations during imbalanced scenario construction. Self-supervised learning (SSL) offers a promising solution by synthesizing minority nodes from the data itself, yet its potential remains unexplored. In this paper, we analyze the limitations of SMOTE-based approaches and introduce VIGraph, a novel SSL model based on the self-supervised Variational Graph Auto-Encoder (VGAE) that leverages Variational Inference (VI) to generate minority nodes. Specifically, VIGraph strictly adheres to the concept of imbalance when constructing imbalanced graphs and utilizes the generative VGAE to generate minority nodes. Moreover, VIGraph introduces a novel Siamese contrastive strategy at the decoding phase to improve the overall quality of generated nodes. VIGraph can generate high-quality nodes without reintegrating them into the original graph, eliminating the "Generating, Reintegrating, and Retraining" process found in SMOTE-based methods. Experiments on multiple real-world datasets demonstrate that VIGraph achieves promising results for class-imbalanced node classification tasks.
</details>
<details>
<summary>摘要</summary>
classe 不均衡在图数据中存在 significativ 挑战，现有的方法，表示 SMOTE 基于方法， partially 缓解了这种情况，但仍然在不均衡enario 构建中存在限制。自我supervised 学习（SSL）提供了一个有前途的解决方案，可以自动生成少数节点，但其潜力仍然没有得到充分利用。本文分析了 SMOTE 基于方法的局限性，并引入 VIGraph，一种新的 SSL 模型，基于自我supervised Variational Graph Auto-Encoder（VGAE），利用 Variational Inference（VI）生成少数节点。具体来说，VIGraph 严格遵循不均衡概念在构建不均衡图时，并利用生成的 VGAE 来生成少数节点。此外，VIGraph 引入了一种新的对比策略在解码阶段，以提高生成节点的质量。VIGraph 可以生成高质量节点，无需将其重新 integrate 到原始图中，从而消除 SMOTE 基于方法中的 "生成、重新集成、重新训练" 过程。实验表明，VIGraph 在多个真实世界数据集上取得了优秀的结果 для类均衡节点分类任务。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Healthcare-Image-Analysis-in-Pandemic-Based-Fog-Cloud-Computing-Architectures"><a href="#Revolutionizing-Healthcare-Image-Analysis-in-Pandemic-Based-Fog-Cloud-Computing-Architectures" class="headerlink" title="Revolutionizing Healthcare Image Analysis in Pandemic-Based Fog-Cloud Computing Architectures"></a>Revolutionizing Healthcare Image Analysis in Pandemic-Based Fog-Cloud Computing Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01185">http://arxiv.org/abs/2311.01185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Al Zahraa Elsayed, Khalil Mohamed, Hany Harb<br>for: 这篇研究paper的目的是来提出一个创新的医疗架构，以解决医疗数据分析中的效率和准确性问题。methods: 这篇paper使用了fog computing和改进的Convolutional Neural Network（CNN）来进行医疗影像分析。不同的CNN层架构被充分探讨和评估，以最大化整体性能。results: 比较过去的模型如VGG16、VGG19、MobileNet以及相关研究，提出的方法实现了99.88%的正常案例准确率，以及96.5%的验证率、100%的精度和回传率，以及100%的F1分数。这些结果显示fog computing和改进的CNN在医疗影像分析和诊断中具有广泛的应用前景，不仅在疫情期间，而且在未来也具有巨大的潜力。<details>
<summary>Abstract</summary>
The emergence of pandemics has significantly emphasized the need for effective solutions in healthcare data analysis. One particular challenge in this domain is the manual examination of medical images, such as X-rays and CT scans. This process is time-consuming and involves the logistical complexities of transferring these images to centralized cloud computing servers. Additionally, the speed and accuracy of image analysis are vital for efficient healthcare image management. This research paper introduces an innovative healthcare architecture that tackles the challenges of analysis efficiency and accuracy by harnessing the capabilities of Artificial Intelligence (AI). Specifically, the proposed architecture utilizes fog computing and presents a modified Convolutional Neural Network (CNN) designed specifically for image analysis. Different architectures of CNN layers are thoroughly explored and evaluated to optimize overall performance. To demonstrate the effectiveness of the proposed approach, a dataset of X-ray images is utilized for analysis and evaluation. Comparative assessments are conducted against recent models such as VGG16, VGG19, MobileNet, and related research papers. Notably, the proposed approach achieves an exceptional accuracy rate of 99.88% in classifying normal cases, accompanied by a validation rate of 96.5%, precision and recall rates of 100%, and an F1 score of 100%. These results highlight the immense potential of fog computing and modified CNNs in revolutionizing healthcare image analysis and diagnosis, not only during pandemics but also in the future. By leveraging these technologies, healthcare professionals can enhance the efficiency and accuracy of medical image analysis, leading to improved patient care and outcomes.
</details>
<details>
<summary>摘要</summary>
随着疫情的出现，医疗数据分析领域面临着有效解决方案的强烈需求。一个特定的挑战在这个领域是手动检查医疗图像，如X光和CT扫描图像。这个过程浪费时间，同时也存在将图像传输到中央云计算服务器的logistical复杂性。此外，图像分析的速度和准确率对医疗图像管理是非常重要的。本研究论文提出了一种革命性的医疗架构，通过人工智能（AI）技术解决了分析效率和准确率的挑战。具体来说，该架构利用了fog computing技术，并提出了一种特殊的卷积神经网络（CNN），用于图像分析。不同的CNN层的架构被全面探讨和评估，以便优化总性性能。为证明提出的方法的效果，本文使用了一个X光图像集进行分析和评估。与之比较的是，VGG16、VGG19、MobileNet等现有模型和相关研究论文。结果表明，提出的方法在分类正常情况时 achieved an exceptional accuracy rate of 99.88%，并且 validation rate为96.5%，准确率和回归率均为100%，F1分数也为100%。这些结果表明，fog computing和特殊的CNN可以在医疗图像分析和诊断方面发挥革命性的作用，不仅在疫情期间，而且在未来也会发挥重要作用。通过利用这些技术，医疗专业人员可以提高医疗图像分析的效率和准确率，从而提高患者的病情和结果。
</details></li>
</ul>
<hr>
<h2 id="Generative-Input-Towards-Next-Generation-Input-Methods-Paradigm"><a href="#Generative-Input-Towards-Next-Generation-Input-Methods-Paradigm" class="headerlink" title="Generative Input: Towards Next-Generation Input Methods Paradigm"></a>Generative Input: Towards Next-Generation Input Methods Paradigm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01166">http://arxiv.org/abs/2311.01166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keyu Ding, Yongcan Wang, Zihang Xu, Zhenzhen Jia, Shijin Wang, Cong Liu, Enhong Chen</li>
<li>for: 这个论文旨在探讨如何使用生成模型提高中文输入法的性能。</li>
<li>methods: 该论文提出了一种新的生成输入模式，名为生成输入模式（GeneInput），它使用提示来处理所有输入场景，并使用用户反馈来优化模型并提供个性化结果。</li>
<li>results: 研究结果显示，GeneInput在全模式键序列到字符（FK2C）任务中首次实现了国际级表现，并且提出了一种新的奖励模型训练方法，可以消除额外的手动注释和表现超越GPT-4在智能关联和对话协助等任务中。相比传统模式，GeneInput不仅表现出了更高的性能，还展现出了更好的抗衡性、扩展性和在线学习能力。<details>
<summary>Abstract</summary>
Since the release of ChatGPT, generative models have achieved tremendous success and become the de facto approach for various NLP tasks. However, its application in the field of input methods remains under-explored. Many neural network approaches have been applied to the construction of Chinese input method engines(IMEs).Previous research often assumed that the input pinyin was correct and focused on Pinyin-to-character(P2C) task, which significantly falls short of meeting users' demands. Moreover, previous research could not leverage user feedback to optimize the model and provide personalized results. In this study, we propose a novel Generative Input paradigm named GeneInput. It uses prompts to handle all input scenarios and other intelligent auxiliary input functions, optimizing the model with user feedback to deliver personalized results. The results demonstrate that we have achieved state-of-the-art performance for the first time in the Full-mode Key-sequence to Characters(FK2C) task. We propose a novel reward model training method that eliminates the need for additional manual annotations and the performance surpasses GPT-4 in tasks involving intelligent association and conversational assistance. Compared to traditional paradigms, GeneInput not only demonstrates superior performance but also exhibits enhanced robustness, scalability, and online learning capabilities.
</details>
<details>
<summary>摘要</summary>
desde el lanzamiento de ChatGPT, los modelos generativos han logrado un gran éxito y se han convertido en el enfoque por defecto para diversas tareas de procesamiento de lenguaje natural. Sin embargo, su aplicación en el campo de los métodos de entrada aún se ha explorado insuficientemente. Muchas aproximaciones basadas en redes neuronales se han aplicado en la construcción de motores de entrada de caracteres chinos(IMEs). La investigación previa suponía que la entrada de pinyin era correcta y se centró en la tarea de Pinyin-to-Character(P2C), lo que significativamente se aparta de las demandas de los usuarios. Además, la investigación anterior no podía aproveitar la retroalimentación del usuario para optimizar el modelo y proporcionar resultados personalizados. En este estudio, propusimos un paradigma de entrada generativa llamado GeneInput. Utiliza prompts para manejar todas las escenas de entrada y otras funciones de entrada inteligente auxiliar, optimizando el modelo con la retroalimentación del usuario para entregar resultados personalizados. Los resultados demuestran que hemos logrado el rendimiento estado-de-arte por primera vez en la tarea de Full-mode Key-sequence to Characters(FK2C). Proponemos un método de entrenamiento de modelo de reward que elimina la necesidad de anotaciones manuales adicionales y el rendimiento supera a GPT-4 en tareas involucradas en asociación y asistencia conversacional. En comparación con los enfoques tradicionales, GeneInput no solo demuestra un rendimiento superior, sino también exhibe mayor robustez, escalabilidad y capacidades de aprendizaje en línea.
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Semantic-Parsing-with-Execution-based-Spurious-Program-Filtering"><a href="#Weakly-Supervised-Semantic-Parsing-with-Execution-based-Spurious-Program-Filtering" class="headerlink" title="Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering"></a>Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01161">http://arxiv.org/abs/2311.01161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kang-il Lee, Segwang Kim, Kyomin Jung</li>
<li>for: 本研究旨在推断SemanticParser训练 FROM weak supervision中的假计划问题。</li>
<li>methods: 我们提议一种基于程序执行结果的领域无关筛选机制，具体来说，对每个通过搜索获得的程序，我们首先构建一个捕捉程序 semantics的表示，然后对这些表示进行多数投票，以识别并过滤有显著不同Semantics的程序。</li>
<li>results: 我们的方法可以轻松地与现有的weakly supervised SemanticParser Frameworks堆叠，并在Natural Language Visual Reasoning和WikiTableQuestions上进行了实验，发现将我们的方法应用于现有的SemanticParserinduces significantly improved performances。<details>
<summary>Abstract</summary>
The problem of spurious programs is a longstanding challenge when training a semantic parser from weak supervision. To eliminate such programs that have wrong semantics but correct denotation, existing methods focus on exploiting similarities between examples based on domain-specific knowledge. In this paper, we propose a domain-agnostic filtering mechanism based on program execution results. Specifically, for each program obtained through the search process, we first construct a representation that captures the program's semantics as execution results under various inputs. Then, we run a majority vote on these representations to identify and filter out programs with significantly different semantics from the other programs. In particular, our method is orthogonal to the program search process so that it can easily augment any of the existing weakly supervised semantic parsing frameworks. Empirical evaluations on the Natural Language Visual Reasoning and WikiTableQuestions demonstrate that applying our method to the existing semantic parsers induces significantly improved performances.
</details>
<details>
<summary>摘要</summary>
“伪函数问题”是强度指导下训练 semantic parser 的长standing挑战。以往的方法则是利用领域专业知识来推导类似性，以删除具有错误semantics yet correct denotation 的程式。在这篇论文中，我们提出了一种领域共享 Filtering 机制，基于程式执行结果。具体来说，我们将每个通过搜索过程获得的程式转换为执行结果的表示，然后对这些表示进行多数决，以识别和删除与其他程式semantics 不同的程式。我们的方法与程式搜索过程不相互干扰，因此可以轻松地将其与现有的弱指导 semantic parsing 框架结合使用。实验评估在 Natural Language Visual Reasoning 和 WikiTableQuestions 上显示，将我们的方法应用到现有的 semantic parsers 可以导致明显改善的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Digital-Twins-and-their-Application-in-Cybersecurity-based-on-Artificial-Intelligence"><a href="#A-Review-of-Digital-Twins-and-their-Application-in-Cybersecurity-based-on-Artificial-Intelligence" class="headerlink" title="A Review of Digital Twins and their Application in Cybersecurity based on Artificial Intelligence"></a>A Review of Digital Twins and their Application in Cybersecurity based on Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01154">http://arxiv.org/abs/2311.01154</a></li>
<li>repo_url: None</li>
<li>paper_authors: MohammadHossein Homaei, Oscar Mogollon Gutierrez, Jose Carlos Sancho Nunez, Mar Avila Vegas, Andres Caro Lindo</li>
<li>for: 本研究旨在探讨虚拟链技术在不同领域的应用和潜在问题，以及如何通过人工智能技术来提供数字各种领域的安全性。</li>
<li>methods: 本研究使用了许多不同的方法，包括文献综述、实践报告、采访调查等，以探讨虚拟链技术的应用和潜在问题。</li>
<li>results: 本研究发现了虚拟链技术在各种领域的应用和潜在问题，包括虚拟产品、虚拟服务、虚拟产业等。同时，该研究还发现了虚拟链技术的安全性问题，包括数据隐私和安全性问题。<details>
<summary>Abstract</summary>
The potential of digital twin technology is yet to be fully realized due to its diversity and untapped potential. Digital twins enable systems' analysis, design, optimization, and evolution to be performed digitally or in conjunction with a cyber-physical approach to improve speed, accuracy, and efficiency over traditional engineering methods. Industry 4.0, factories of the future, and digital twins continue to benefit from the technology and provide enhanced efficiency within existing systems. Due to the lack of information and security standards associated with the transition to cyber digitization, cybercriminals have been able to take advantage of the situation. Access to a digital twin of a product or service is equivalent to threatening the entire collection. There is a robust interaction between digital twins and artificial intelligence tools, which leads to strong interaction between these technologies, so it can be used to improve the cybersecurity of these digital platforms based on their integration with these technologies. This study aims to investigate the role of artificial intelligence in providing cybersecurity for digital twin versions of various industries, as well as the risks associated with these versions. In addition, this research serves as a road map for researchers and others interested in cybersecurity and digital security.
</details>
<details>
<summary>摘要</summary>
“数字双工程技术的潜力仍未得到完全实现，这主要归功于其多样性和未发掘的潜力。数字双工程技术可以在数字或融合物理方式下进行系统分析、设计、优化和演化，从而提高速度、准确性和效率，并且可以与工业4.0、未来的制造厂和数字双工程技术相结合，提高现有系统的效率。然而，由于数字化转型的缺乏信息和安全标准，黑客有机会利用这种情况。访问一个产品或服务的数字双版本等于对整个收藏的威胁。数字双工程技术和人工智能工具之间存在强烈的互动，因此可以通过这些技术的结合来提高数字平台的安全性。本研究旨在调查不同领务中数字双版本的人工智能在提供网络安全方面的作用，以及这些版本的风险。此外，这项研究还可 serve as a roadmap for researchers and others interested in cybersecurity and digital security.”Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Revisiting-the-Knowledge-Injection-Frameworks"><a href="#Revisiting-the-Knowledge-Injection-Frameworks" class="headerlink" title="Revisiting the Knowledge Injection Frameworks"></a>Revisiting the Knowledge Injection Frameworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01150">http://arxiv.org/abs/2311.01150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Fu, Yiming Zhang, Haobo Wang, Weikang Qiu, Junbo Zhao</li>
<li>for: 这篇论文旨在解决如何使用外部知识来适应垂直领域特定任务，以提高大语言模型（LLM）的性能。</li>
<li>methods: 这篇论文使用了一种Alignment Heuristic的方法，通过将相关的知识元组注入到相应的文本样本中来实现外部知识的注入。然而， authors发现，Random Knowledge Injection（随机注入外部知识）可以达到类似或更好的结果，而不需要对知识元组进行对齐。</li>
<li>results: 作者们发现，采用Random Knowledge Injection可以超越现有的Alignment Heuristic，并且可以提高垂直领域特定任务中LLM的性能。此外， authors还提出了一种简单的修复方法，通过约束外部知识库的淘汰和纯化来解决这个问题。<details>
<summary>Abstract</summary>
In recent years, large language models (LLMs), such as GPTs, have attained great impact worldwide. However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved. Indeed, there have emerged a few works on this line where most of them rely on an alignment heuristic that is built to inject the corresponding knowledge tuple into the associated text sample.   However, despite the promise, we identify a pivotal problem in this work ubiquitously. Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected. We therefore take a thorough investigation of this frustrating finding on a variety of related prior work and further provide a chain of potential interpretations for the phenomenon. Based on all that, we offer a simple remediated technique. Briefly, the core of this technique is rooted in an ideological emphasis on the pruning and purification of the external knowledge base to be injected into LLMs. At last, we show that by integrating this technique into most (if not all) knowledge injection frameworks and recent LLMs, it manages to overcome the aforementioned sanity problem and further pushes the boundary of the performance of the domain-adaptive LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GREEMA-Proposal-and-Experimental-Verification-of-Growing-Robot-by-Eating-Environmental-MAterial-for-Landslide-Disaster"><a href="#GREEMA-Proposal-and-Experimental-Verification-of-Growing-Robot-by-Eating-Environmental-MAterial-for-Landslide-Disaster" class="headerlink" title="GREEMA: Proposal and Experimental Verification of Growing Robot by Eating Environmental MAterial for Landslide Disaster"></a>GREEMA: Proposal and Experimental Verification of Growing Robot by Eating Environmental MAterial for Landslide Disaster</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01107">http://arxiv.org/abs/2311.01107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusuke Tsunoda, Yuya Sato, Koichi Osuka</li>
<li>for: 这个研究是为了开发一种能够在无法人类进入的区域，如月面和滑坡现场，进行多个自动移动机械系统的替代。具体来说，在河道堵塞现场，需要为该地点移除水和泥土的任务。传统上，需要多部建筑机械来进行 цивіLENGINEERING工作，但由于这些机械的大小和重量，将它们运输到现场是具有巨大成本和时间的问题。</li>
<li>methods: 这个研究使用了一种名为GREEMA的新型生长机械，这是一种轻量级和压缩的 durante transportation，但可以在到达现场后使用环境材料来运作。GREEMA可以活动地吸收环境材料，如水和泥土，并将它们转换为自己的结构，并从自己运走。</li>
<li>results: 这个研究实验了两种GREEMA的类型。首先，我们开发了一种 fins-type swimming robot，这个机器人可以通过吸收水来实现游泳功能。其次，我们建立了一种 arm-type robot，这个机器人可以吃泥土来增加自己的韧性。我们对这两个实验的结果进行了Explicit-Implicit控制的探讨，并描述了GREEMA的设计理论。<details>
<summary>Abstract</summary>
In areas that are inaccessible to humans, such as the lunar surface and landslide sites, there is a need for multiple autonomous mobile robot systems that can replace human workers. In particular, at landslide sites such as river channel blockages, robots are required to remove water and sediment from the site as soon as possible. Conventionally, several construction machines have been deployed to the site for civil engineering work. However, because of the large size and weight of conventional construction equipment, it is difficult to move multiple units of construction equipment to the site, resulting in significant transportation costs and time. To solve such problems, this study proposes a novel growing robot by eating environmental material called GREEMA, which is lightweight and compact during transportation, but can function by eating on environmental materials once it arrives at the site. GREEMA actively takes in environmental materials such as water and sediment, uses them as its structure, and removes them by moving itself. In this paper, we developed and experimentally verified two types of GREEMAs. First, we developed a fin-type swimming robot that passively takes water into its body using a water-absorbing polymer and forms a body to express its swimming function. Second, we constructed an arm-type robot that eats soil to increase the rigidity of its body. We discuss the results of these two experiments from the viewpoint of Explicit-Implicit control and describe the design theory of GREEMA.
</details>
<details>
<summary>摘要</summary>
在人类无法进入的区域，如月面和滥覆现场，需要多个自主移动 робо辅助人工工作。特别是在河道堵塞现场，机器人需要尽快将水和淤泥从现场除去。 conventionally，数量多的建筑机械被派往现场进行土木工程。然而，由于传统的建筑机械庞大和重量，运输成本和时间均很高。为解决这些问题，本研究提出了一种新型增长机器人，即吃环境材料called GREEMA，它轻量级和压缩的交通时间。GREEMA在到达现场后通过吃环境材料来形成结构，并将其移除。在这篇论文中，我们开发并实验验证了两种GREEMA的类型。首先，我们开发了一种螺旋型游泳机器人，通过吸收水的水吸收聚合物来形成身体表现游泳功能。其次，我们建立了一种吃土的机器人，通过吃土来增加机器人的体硬度。我们从Explicit-Implicit控制的视角来讲述GREEMA的设计理论。
</details></li>
</ul>
<hr>
<h2 id="Ultra-Efficient-On-Device-Object-Detection-on-AI-Integrated-Smart-Glasses-with-TinyissimoYOLO"><a href="#Ultra-Efficient-On-Device-Object-Detection-on-AI-Integrated-Smart-Glasses-with-TinyissimoYOLO" class="headerlink" title="Ultra-Efficient On-Device Object Detection on AI-Integrated Smart Glasses with TinyissimoYOLO"></a>Ultra-Efficient On-Device Object Detection on AI-Integrated Smart Glasses with TinyissimoYOLO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01057">http://arxiv.org/abs/2311.01057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Moosmann, Pietro Bonazzi, Yawei Li, Sizhen Bian, Philipp Mayer, Luca Benini, Michele Magno</li>
<li>for: The paper is written for researchers and developers who are interested in integrating AI into smart glasses, specifically those who are looking to achieve prolonged continuous operation with limited battery capacity.</li>
<li>methods: The paper describes the design and implementation of tiny machine-learning algorithms that exploit novel low-power processors to enable energy- and latency-efficient object detection on smart glasses. The authors developed a family of novel tiny deep-learning models based on YOLO with sub-million parameters customized for microcontroller-based inference.</li>
<li>results: The paper reports that the proposed TinyissimoYOLO models achieve an inference latency of 17ms and energy consumption of 1.59mJ per inference, with acceptable detection accuracy. The end-to-end latency from image capturing to algorithm prediction is 56ms (equivalent to 18 fps), with a total power consumption of 62.9mW, which is equivalent to 9.3 hours of continuous run time on a 154mAh battery. These results outperform MCUNet (TinyNAS+TinyEngine), which achieves a simpler task (image classification) at just 7.3 fps per second.<details>
<summary>Abstract</summary>
Smart glasses are rapidly gaining advanced functionality thanks to cutting-edge computing technologies, accelerated hardware architectures, and tiny AI algorithms. Integrating AI into smart glasses featuring a small form factor and limited battery capacity is still challenging when targeting full-day usage for a satisfactory user experience. This paper illustrates the design and implementation of tiny machine-learning algorithms exploiting novel low-power processors to enable prolonged continuous operation in smart glasses. We explore the energy- and latency-efficient of smart glasses in the case of real-time object detection. To this goal, we designed a smart glasses prototype as a research platform featuring two microcontrollers, including a novel milliwatt-power RISC-V parallel processor with a hardware accelerator for visual AI, and a Bluetooth low-power module for communication. The smart glasses integrate power cycling mechanisms, including image and audio sensing interfaces. Furthermore, we developed a family of novel tiny deep-learning models based on YOLO with sub-million parameters customized for microcontroller-based inference dubbed TinyissimoYOLO v1.3, v5, and v8, aiming at benchmarking object detection with smart glasses for energy and latency. Evaluations on the prototype of the smart glasses demonstrate TinyissimoYOLO's 17ms inference latency and 1.59mJ energy consumption per inference while ensuring acceptable detection accuracy. Further evaluation reveals an end-to-end latency from image capturing to the algorithm's prediction of 56ms or equivalently 18 fps, with a total power consumption of 62.9mW, equivalent to a 9.3 hours of continuous run time on a 154mAh battery. These results outperform MCUNet (TinyNAS+TinyEngine), which runs a simpler task (image classification) at just 7.3 fps per second.
</details>
<details>
<summary>摘要</summary>
智能眼镜在技术上不断提高，感谢于高级计算技术、加速器硬件体系和小型AI算法。但是在将AI集成到智能眼镜中，具有小型化的形态和有限的电池容量仍然是一大挑战，以实现满意的用户体验。本文描述了在智能眼镜中实现小型机器学习算法的设计和实现，以提高智能眼镜的连续运行时间。我们开发了一款智能眼镜原型，其包括两个微控制器，包括一个新的低功耗RISC-V并行处理器和一个蓝牙低功耗模块。智能眼镜还包括图像和音频感知接口。此外，我们开发了一家小型深度学习模型基于YOLO，称为TinyissimoYOLO v1.3、v5和v8，以实现智能眼镜中对物体检测的能效评估。我们对智能眼镜原型进行评估，发现TinyissimoYOLO的推理延迟时间为17毫秒，电能消耗为1.59毫瓦，并保持了可接受的检测精度。进一步的评估表明，从图像捕获到算法预测的总时间为56毫秒（相当于18帧/秒），总电力消耗为62.9毫瓦，等于9.3小时的连续运行时间。这些结果超过了MCUNet（TinyNAS+TinyEngine），它在更简单的任务（图像分类）中只能达到7.3帧/秒。
</details></li>
</ul>
<hr>
<h2 id="Multi-dimensional-data-refining-strategy-for-effective-fine-tuning-LLMs"><a href="#Multi-dimensional-data-refining-strategy-for-effective-fine-tuning-LLMs" class="headerlink" title="Multi-dimensional data refining strategy for effective fine-tuning LLMs"></a>Multi-dimensional data refining strategy for effective fine-tuning LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01049">http://arxiv.org/abs/2311.01049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanh Nguyen Ngoc, Quang Nhat Tran, Arthur Tang, Bao Nguyen, Thuy Nguyen, Thanh Pham</li>
<li>for: 本研究旨在提供大型语言模型精度调整的数据Foundation，但获得适合的数据仍然具有挑战性。</li>
<li>methods: 本研究使用了多维度的策略，包括利用英语语料集和开发自定义数据爬虫脚本，并利用生成AI工具来帮助。</li>
<li>results: 使用结果的 Vietnamese 语言模型在生成文章的任务中表现了良好的表现。研究提供了实践的解决方案和指导，对未来针对语言如 Vietnamese 的模型精度调整具有重要意义。I hope that helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Data is a cornerstone for fine-tuning large language models, yet acquiring suitable data remains challenging. Challenges encompassed data scarcity, linguistic diversity, and domain-specific content. This paper presents lessons learned while crawling and refining data tailored for fine-tuning Vietnamese language models. Crafting such a dataset, while accounting for linguistic intricacies and striking a balance between inclusivity and accuracy, demands meticulous planning. Our paper presents a multidimensional strategy including leveraging existing datasets in the English language and developing customized data-crawling scripts with the assistance of generative AI tools. A fine-tuned LLM model for the Vietnamese language, which was produced using resultant datasets, demonstrated good performance while generating Vietnamese news articles from prompts. The study offers practical solutions and guidance for future fine-tuning models in languages like Vietnamese.
</details>
<details>
<summary>摘要</summary>
数据是大语言模型精度调整的基estone，但获得适合的数据仍然是一大挑战。这些挑战包括数据稀缺、语言多样性和领域特定内容。本文介绍了在爬取和修剪适合精度调整越南语言模型的数据时所学到的经验。制作这类数据集需要仔细规划，考虑语言细节和兼顾准确性和包容性。我们的文章提出了多维度策略，包括利用英语语料库和开发自定义爬取脚本，并通过生成AI工具来帮助。通过使用结果数据集，我们生成的越南语言模型进行了良好的表现，从提示生成越南新闻文章。这种研究提供了实用的解决方案和指导，以便未来的语言模型精度调整。
</details></li>
</ul>
<hr>
<h2 id="AI-assisted-Learning-for-Electronic-Engineering-Courses-in-High-Education"><a href="#AI-assisted-Learning-for-Electronic-Engineering-Courses-in-High-Education" class="headerlink" title="AI-assisted Learning for Electronic Engineering Courses in High Education"></a>AI-assisted Learning for Electronic Engineering Courses in High Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01048">http://arxiv.org/abs/2311.01048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanh Nguyen Ngoc, Quang Nhat Tran, Arthur Tang, Bao Nguyen, Thuy Nguyen, Thanh Pham</li>
<li>for: This paper is written to evaluate the effectiveness of ChatGPT as a teaching and learning support tool in an integrated circuit systems course at a higher education institution in an Asian country.</li>
<li>methods: The study uses various question types to assess ChatGPT’s responses and gain valuable insights for further investigation. The study also includes the evaluation and reflection of different stakeholders: students, lecturers, and engineers.</li>
<li>results: The findings of this study shed light on the benefits and limitations of ChatGPT as an AI tool, paving the way for innovative learning approaches in technical disciplines. The study contributes to our understanding of how digital transformation is likely to unfold in the education sector.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了评估聊天GPT在大学学习支持中的效果，specifically in an integrated circuit systems course at a higher education institution in an Asian country.</li>
<li>methods: 这篇论文使用了多种问题类型来评估聊天GPT的回答，并通过不同参与者的评估和反思（包括学生、讲师和工程师）来获得有价值的发现。</li>
<li>results: 这篇论文的发现探讨了聊天GPT作为AI工具的优缺点，为技术领域的学习方法做出了贡献，并为教育领域的数字变革做出了贡献。<details>
<summary>Abstract</summary>
This study evaluates the efficacy of ChatGPT as an AI teaching and learning support tool in an integrated circuit systems course at a higher education institution in an Asian country. Various question types were completed, and ChatGPT responses were assessed to gain valuable insights for further investigation. The objective is to assess ChatGPT's ability to provide insights, personalized support, and interactive learning experiences in engineering education. The study includes the evaluation and reflection of different stakeholders: students, lecturers, and engineers. The findings of this study shed light on the benefits and limitations of ChatGPT as an AI tool, paving the way for innovative learning approaches in technical disciplines. Furthermore, the study contributes to our understanding of how digital transformation is likely to unfold in the education sector.
</details>
<details>
<summary>摘要</summary>
这项研究评估了 chatGPT 在大学技术课程中作为人工智能教学支持工具的效果。在一个亚洲国家的高等教育机构中，学生、讲师和工程师参与了多种问题的回答，以获得有价值的发现和反思。研究的目的是评估 chatGPT 是否能提供个性化支持、互动式学习体验和工程教育中的洞察。这项研究还包括不同参与者的评估和反思：学生、讲师和工程师。研究结果为我们提供了 chatGPT 作为人工智能工具的优缺点，并为我们更好地理解技术领域教育领域的数字变革。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Large-Language-Models-for-Autonomous-Driving"><a href="#A-Survey-of-Large-Language-Models-for-Autonomous-Driving" class="headerlink" title="A Survey of Large Language Models for Autonomous Driving"></a>A Survey of Large Language Models for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01043">http://arxiv.org/abs/2311.01043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thinklab-sjtu/awesome-llm4ad">https://github.com/thinklab-sjtu/awesome-llm4ad</a></li>
<li>paper_authors: Zhenjie Yang, Xiaosong Jia, Hongyang Li, Junchi Yan</li>
<li>for: 本研究旨在探讨大语言模型（LLM）在自动驾驶技术中的应用，以提高自动驾驶系统的可解释性和可Traceability。</li>
<li>methods: 本研究使用了大语言模型（LLM）与基础视觉模型的组合，以实现开放世界理解、逻辑推理和几步学习等功能。</li>
<li>results: 本研究系统性地回顾了现有的LLM4AD技术发展，并特别强调了当前技术的挑战和未来研究的方向。同时，我们还提供了实时更新的最新进展和相关开源资源，以便学术和工业研究人员快速入手。<details>
<summary>Abstract</summary>
Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their ``black box" nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper, we systematically review a research line about \textit{Large Language Models for Autonomous Driving (LLM4AD)}. This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field. For the convenience of researchers in academia and industry, we provide real-time updates on the latest advances in the field as well as relevant open-source resources via the designated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.
</details>
<details>
<summary>摘要</summary>
自主驾驶技术，一种可以革新交通和城市流动的技术，正在从规则基于系统向数据驱动策略过渡。传统的模块化系统受到累加误差的限制，以及硬性预先设置的规则。相比之下，端到端自主驾驶系统具有避免误差累加的潜力，尽管它们常常lack transparency due to their "black box" nature，复杂化决策的验证和跟踪。现在，大型语言模型（LLM）已经展示了理解上下文、逻辑推理和生成答案的能力。一种自然的想法是利用这些能力来 empower autonomous driving。将 LLM 与基础视觉模型结合，可以开启开放世界理解、逻辑推理和几招学习，现在的自主驾驶系统缺乏。在这篇论文中，我们系统地回顾了关于《大型语言模型 для自主驾驶（LLM4AD）》的研究线。这项研究评估了当前技术前进的状况，明确地描述了主要挑战和未来方向。为研究人员在学术和industry中方便，我们提供了实时更新的最新进展和相关开源资源，via the designated link: <https://github.com/Thinklab-SJTU/Awesome-LLM4AD>.
</details></li>
</ul>
<hr>
<h2 id="Learn-to-Refuse-Making-Large-Language-Models-More-Controllable-and-Reliable-through-Knowledge-Scope-Limitation-and-Refusal-Mechanism"><a href="#Learn-to-Refuse-Making-Large-Language-Models-More-Controllable-and-Reliable-through-Knowledge-Scope-Limitation-and-Refusal-Mechanism" class="headerlink" title="Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism"></a>Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01041">http://arxiv.org/abs/2311.01041</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/windszzlang/Learn-to-Refuse">https://github.com/windszzlang/Learn-to-Refuse</a></li>
<li>paper_authors: Lang Cao<br>for: 这篇论文的目的是如何使用拒绝机制来减少语言模型中的幻视（hallucination），尤其是在问答中。methods: 本文使用了一个简单 yet 有效的解决方案called Learn to Refuse (L2R)，它将拒绝机制与语言模型相结合，让语言模型可以识别和拒绝处理难以回答的问题。此外，本文还提出了一种自动和高效地扩展语言模型知识库的方法。results: 根据质量和量度分析，我们展示了我们的方法可以提高语言模型的可控性和可靠性。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM's understanding of the world, enabling it to provide traceable gold knowledge. This knowledge base is separate from the LLM and initially empty, and it is progressively expanded with validated knowledge. When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently. Additionally, we introduce a method for automatically and efficiently expanding the knowledge base of LLMs. Through qualitative and quantitative analysis, we demonstrate that our approach enhances the controllability and reliability of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）有示出了卓越的语言理解和生成能力，能够回答各种领域的问题。然而，这些模型并不完美，常会产生错误或不实的回答，称为“幻视”。这些幻视使得 LLM 成为不可靠和无法使用的。在这篇文章中，我们专注于对 LLM 中的幻视问题进行缓和，尤其是在问答领域。而不是尝试回答所有问题，我们探索了一种拒绝机制，让 LLM 当面困难的问题时拒绝回答，以避免错误。我们then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM's understanding of the world, enabling it to provide traceable gold knowledge. This knowledge base is separate from the LLM and initially empty, and it is progressively expanded with validated knowledge. When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently. Additionally, we introduce a method for automatically and efficiently expanding the knowledge base of LLMs. Through qualitative and quantitative analysis, we demonstrate that our approach enhances the controllability and reliability of LLMs.
</details></li>
</ul>
<hr>
<h2 id="ATHENA-Mathematical-Reasoning-with-Thought-Expansion"><a href="#ATHENA-Mathematical-Reasoning-with-Thought-Expansion" class="headerlink" title="ATHENA: Mathematical Reasoning with Thought Expansion"></a>ATHENA: Mathematical Reasoning with Thought Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01036">http://arxiv.org/abs/2311.01036</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/the-jb/athena-math">https://github.com/the-jb/athena-math</a></li>
<li>paper_authors: JB. Kim, Hazel Kim, Joonghyuk Hahn, Yo-Sub Han</li>
<li>for: 解决实际 math 问题需要如何表述问题，模型如何理解人类语言表达。</li>
<li>methods: 我们介绍了 Attention-based THought Expansion Network Architecture (ATHENA)，它模仿人类思维扩展机制，通过神经网络传播来解决实际实践中的挑战。</li>
<li>results: 我们的实验显示，ATHENA可以达到新的state-of-the-art Water准，在 variant 问题中表现出色，即使训练示例的信息含量有限。<details>
<summary>Abstract</summary>
Solving math word problems depends on how to articulate the problems, the lens through which models view human linguistic expressions. Real-world settings count on such a method even more due to the diverse practices of the same mathematical operations. Earlier works constrain available thinking processes by limited prediction strategies without considering their significance in acquiring mathematical knowledge. We introduce Attention-based THought Expansion Network Architecture (ATHENA) to tackle the challenges of real-world practices by mimicking human thought expansion mechanisms in the form of neural network propagation. A thought expansion recurrently generates the candidates carrying the thoughts of possible math expressions driven from the previous step and yields reasonable thoughts by selecting the valid pathways to the goal. Our experiments show that ATHENA achieves a new state-of-the-art stage toward the ideal model that is compelling in variant questions even when the informativeness in training examples is restricted.
</details>
<details>
<summary>摘要</summary>
解决数学word问题取决于如何表达问题，模型通过人类语言表达的镜像来看待人类语言表达。现实世界中的各种实践更加依赖这种方法，因为这些操作的方式异常多样。先前的工作压缩了可用的思维过程，未能考虑这些知识获得的重要性。我们介绍了注意力基于的思维扩展网络架构（ATHENA），以模拟人类思维扩展机制，通过神经网络传播来解决现实世界中的挑战。一个思维扩展循环产生可能会携带思维的数学表达的候选者，通过选择前一步的有效路径来得到合理的思维。我们的实验表明，ATHENA已经达到了新的状态级模型，在变化的问题中具有吸引力，即使在受限的训练示例中也能达到优秀的表现。
</details></li>
</ul>
<hr>
<h2 id="Non-Autoregressive-Diffusion-based-Temporal-Point-Processes-for-Continuous-Time-Long-Term-Event-Prediction"><a href="#Non-Autoregressive-Diffusion-based-Temporal-Point-Processes-for-Continuous-Time-Long-Term-Event-Prediction" class="headerlink" title="Non-Autoregressive Diffusion-based Temporal Point Processes for Continuous-Time Long-Term Event Prediction"></a>Non-Autoregressive Diffusion-based Temporal Point Processes for Continuous-Time Long-Term Event Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01033">http://arxiv.org/abs/2311.01033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang-Tao Zhou, Zhao Kang, Ling Tian</li>
<li>for: 预测长期事件序列</li>
<li>methods: 基于扩散过程的非自回归模型</li>
<li>results: 比基于当前状态最佳的方法提高了预测质量<details>
<summary>Abstract</summary>
Continuous-time long-term event prediction plays an important role in many application scenarios. Most existing works rely on autoregressive frameworks to predict event sequences, which suffer from error accumulation, thus compromising prediction quality. Inspired by the success of denoising diffusion probabilistic models, we propose a diffusion-based non-autoregressive temporal point process model for long-term event prediction in continuous time. Instead of generating events one at a time in an autoregressive way, our model predicts the future event sequence entirely as a whole. In order to perform diffusion processes on event sequences, we develop a bidirectional map between target event sequences and the Euclidean vector space. Furthermore, we design a novel denoising network to capture both sequential and contextual features for better sample quality. Extensive experiments are conducted to prove the superiority of our proposed model over state-of-the-art methods on long-term event prediction in continuous time. To the best of our knowledge, this is the first work to apply diffusion methods to long-term event prediction problems.
</details>
<details>
<summary>摘要</summary>
continuous-time long-term event prediction在许多应用场景中扮演着重要的角色。现有大多数工作都是基于autoregressive框架进行预测，这会导致预测误差积累，从而降低预测质量。我们受到了denoising diffusion probabilistic models的成功 inspiration，提出了一种基于diffusion的非autoregressive时间点进程模型 для长期事件预测。而不是一个一个事件进行autoregressive预测，我们的模型会预测未来事件序列的整体。为了在事件序列上进行diffusion过程，我们开发了一种双向映射 между目标事件序列和几何空间的Euclidean vector。此外，我们还设计了一种novel的denoising网络，以捕捉事件序列中的sequential和contextual特征，以提高样本质量。我们进行了广泛的实验，证明了我们提出的模型在长期事件预测任务中的优越性，并且这是首次应用diffusion方法于长期事件预测问题。
</details></li>
</ul>
<hr>
<h2 id="Joint-Learning-of-Local-and-Global-Features-for-Aspect-based-Sentiment-Classification"><a href="#Joint-Learning-of-Local-and-Global-Features-for-Aspect-based-Sentiment-Classification" class="headerlink" title="Joint Learning of Local and Global Features for Aspect-based Sentiment Classification"></a>Joint Learning of Local and Global Features for Aspect-based Sentiment Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01030">http://arxiv.org/abs/2311.01030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Niu, Yun Xiong, Xiaosu Wang, Philip S. Yu</li>
<li>for: 本文主要针对 aspect-based sentiment classification (ASC) 问题，即根据给定的方面词语判断句子中的 sentiment polarity。</li>
<li>methods: 本文提出了一种基于 local 和 global 特征的模型，包括 Gaussian 层和 covariance self-attention 层，以及一种 dual-level graph attention 网络。这些方法可以强制地模型 local 和 global 信息，从而更好地解决 ASC 问题。</li>
<li>results: 本文在 SemEval 2014 和 Twitter  datasets 上 achieved state-of-the-art 性能。<details>
<summary>Abstract</summary>
Aspect-based sentiment classification (ASC) aims to judge the sentiment polarity conveyed by the given aspect term in a sentence. The sentiment polarity is not only determined by the local context but also related to the words far away from the given aspect term. Most recent efforts related to the attention-based models can not sufficiently distinguish which words they should pay more attention to in some cases. Meanwhile, graph-based models are coming into ASC to encode syntactic dependency tree information. But these models do not fully leverage syntactic dependency trees as they neglect to incorporate dependency relation tag information into representation learning effectively. In this paper, we address these problems by effectively modeling the local and global features. Firstly, we design a local encoder containing: a Gaussian mask layer and a covariance self-attention layer. The Gaussian mask layer tends to adjust the receptive field around aspect terms adaptively to deemphasize the effects of unrelated words and pay more attention to local information. The covariance self-attention layer can distinguish the attention weights of different words more obviously. Furthermore, we propose a dual-level graph attention network as a global encoder by fully employing dependency tag information to capture long-distance information effectively. Our model achieves state-of-the-art performance on both SemEval 2014 and Twitter datasets.
</details>
<details>
<summary>摘要</summary>
In this paper, we address these problems by effectively modeling local and global features. First, we design a local encoder containing:1. Gaussian mask layer: 可以 adaptively adjust the receptive field around aspect terms to deemphasize the effects of unrelated words and pay more attention to local information.2. Covariance self-attention layer: can distinguish the attention weights of different words more obviously.Furthermore, we propose a dual-level graph attention network as a global encoder, fully employing dependency tag information to capture long-distance information effectively. Our model achieves state-of-the-art performance on both SemEval 2014 and Twitter datasets.
</details></li>
</ul>
<hr>
<h2 id="Distance-Based-Propagation-for-Efficient-Knowledge-Graph-Reasoning"><a href="#Distance-Based-Propagation-for-Efficient-Knowledge-Graph-Reasoning" class="headerlink" title="Distance-Based Propagation for Efficient Knowledge Graph Reasoning"></a>Distance-Based Propagation for Efficient Knowledge Graph Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01024">http://arxiv.org/abs/2311.01024</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harryshomer/tagnet">https://github.com/harryshomer/tagnet</a></li>
<li>paper_authors: Harry Shomer, Yao Ma, Juanhui Li, Bo Wu, Charu C. Aggarwal, Jiliang Tang</li>
<li>for: 这个论文的目的是解决知识 graphs（KGs）中的新的边预测问题，以便发现新的事实。</li>
<li>methods: 这些方法使用路径信息的汇集来解决这个问题，但它们受到效率问题的困扰。虽有一些最近的尝试通过学习路径剪辑来解决这个问题，但它们通常会牺牲性能来换取效率。</li>
<li>results: 本文提出了一种新的方法TAGNet，可以高效地传播信息。这是通过只在每个源-目标对的固定窗口内汇集路径来实现的。我们示出了TAGNet的复杂性与层数无关。经验表明，TAGNet可以在多个KG数据集上剪枝90%的消息，同时保持与其他方法的竞争性。代码可以在<a target="_blank" rel="noopener" href="https://github.com/HarryShomer/TAGNet%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HarryShomer/TAGNet上获取。</a><details>
<summary>Abstract</summary>
Knowledge graph completion (KGC) aims to predict unseen edges in knowledge graphs (KGs), resulting in the discovery of new facts. A new class of methods have been proposed to tackle this problem by aggregating path information. These methods have shown tremendous ability in the task of KGC. However they are plagued by efficiency issues. Though there are a few recent attempts to address this through learnable path pruning, they often sacrifice the performance to gain efficiency. In this work, we identify two intrinsic limitations of these methods that affect the efficiency and representation quality. To address the limitations, we introduce a new method, TAGNet, which is able to efficiently propagate information. This is achieved by only aggregating paths in a fixed window for each source-target pair. We demonstrate that the complexity of TAGNet is independent of the number of layers. Extensive experiments demonstrate that TAGNet can cut down on the number of propagated messages by as much as 90% while achieving competitive performance on multiple KG datasets. The code is available at https://github.com/HarryShomer/TAGNet.
</details>
<details>
<summary>摘要</summary>
知识图完成（KGC）目标是预测知识图（KG）中未被观测到的边，从而发现新的事实。一些新的方法已经被提出来解决这个问题，它们通过聚合路径信息来实现。这些方法在KGC任务中表现出了惊人的能力，但它们受到效率问题的困扰。虽然有一些最近的尝试通过学习路径剪辑来解决这个问题，但它们经常牺牲性能来获得效率。在这种情况下，我们发现了两种知识图完成方法的内在限制，它们影响了效率和表示质量。为了解决这些限制，我们提出了一种新的方法，TAGNet，它可以有效地传播信息。这是通过在每个源-目标对的固定窗口内聚合路径来实现的。我们证明TAGNet的复杂度独立于层数。广泛的实验表明，TAGNet可以将传播的消息数量减少为90%，同时在多个知识图 datasets 上实现竞争性的性能。代码可以在 <https://github.com/HarryShomer/TAGNet> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Augmentation-is-AUtO-Net-Augmentation-Driven-Contrastive-Multiview-Learning-for-Medical-Image-Segmentation"><a href="#Augmentation-is-AUtO-Net-Augmentation-Driven-Contrastive-Multiview-Learning-for-Medical-Image-Segmentation" class="headerlink" title="Augmentation is AUtO-Net: Augmentation-Driven Contrastive Multiview Learning for Medical Image Segmentation"></a>Augmentation is AUtO-Net: Augmentation-Driven Contrastive Multiview Learning for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01023">http://arxiv.org/abs/2311.01023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanming Guo</li>
<li>for: 这篇论文的目的是对医疗影像诊断中使用深度学习分类算法来提高视觉能力，特别是针对视网膜血管分类任务。</li>
<li>methods: 这篇论文使用了深度学习分类算法，包括多观察者学习框架和混合网络架构，以及注意力机制来捕捉视网膜血管的复杂构造。</li>
<li>results: 这篇论文使用CHASE-DB1资料集进行验证，其中提出了83.46%的F1分数和71.62%的交集顶点分数（IOU），两者都高于现有的参考方法。此外，这篇论文还指出了现有方法的两个主要限制，即数据量紧张和高计算资源的依赖。<details>
<summary>Abstract</summary>
The utilisation of deep learning segmentation algorithms that learn complex organs and tissue patterns and extract essential regions of interest from the noisy background to improve the visual ability for medical image diagnosis has achieved impressive results in Medical Image Computing (MIC). This thesis focuses on retinal blood vessel segmentation tasks, providing an extensive literature review of deep learning-based medical image segmentation approaches while comparing the methodologies and empirical performances. The work also examines the limitations of current state-of-the-art methods by pointing out the two significant existing limitations: data size constraints and the dependency on high computational resources. To address such problems, this work proposes a novel efficient, simple multiview learning framework that contrastively learns invariant vessel feature representation by comparing with multiple augmented views by various transformations to overcome data shortage and improve generalisation ability. Moreover, the hybrid network architecture integrates the attention mechanism into a Convolutional Neural Network to further capture complex continuous curvilinear vessel structures. The result demonstrates the proposed method validated on the CHASE-DB1 dataset, attaining the highest F1 score of 83.46% and the highest Intersection over Union (IOU) score of 71.62% with UNet structure, surpassing existing benchmark UNet-based methods by 1.95% and 2.8%, respectively. The combination of the metrics indicates the model detects the vessel object accurately with a highly coincidental location with the ground truth. Moreover, the proposed approach could be trained within 30 minutes by consuming less than 3 GB GPU RAM, and such characteristics support the efficient implementation for real-world applications and deployments.
</details>
<details>
<summary>摘要</summary>
utilization of deep learning segmentation algorithms that learn complex organs and tissue patterns and extract essential regions of interest from the noisy background to improve the visual ability for medical image diagnosis has achieved impressive results in Medical Image Computing (MIC). This thesis focuses on retinal blood vessel segmentation tasks, providing an extensive literature review of deep learning-based medical image segmentation approaches while comparing the methodologies and empirical performances. The work also examines the limitations of current state-of-the-art methods by pointing out the two significant existing limitations: data size constraints and the dependency on high computational resources. To address such problems, this work proposes a novel efficient, simple multiview learning framework that contrastively learns invariant vessel feature representation by comparing with multiple augmented views by various transformations to overcome data shortage and improve generalisation ability. Moreover, the hybrid network architecture integrates the attention mechanism into a Convolutional Neural Network to further capture complex continuous curvilinear vessel structures. The result demonstrates the proposed method validated on the CHASE-DB1 dataset, attaining the highest F1 score of 83.46% and the highest Intersection over Union (IOU) score of 71.62% with UNet structure, surpassing existing benchmark UNet-based methods by 1.95% and 2.8%, respectively. The combination of the metrics indicates the model detects the vessel object accurately with a highly coincidental location with the ground truth. Moreover, the proposed approach could be trained within 30 minutes by consuming less than 3 GB GPU RAM, and such characteristics support the efficient implementation for real-world applications and deployments.
</details></li>
</ul>
<hr>
<h2 id="NeuroWrite-Predictive-Handwritten-Digit-Classification-using-Deep-Neural-Networks"><a href="#NeuroWrite-Predictive-Handwritten-Digit-Classification-using-Deep-Neural-Networks" class="headerlink" title="NeuroWrite: Predictive Handwritten Digit Classification using Deep Neural Networks"></a>NeuroWrite: Predictive Handwritten Digit Classification using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01022">http://arxiv.org/abs/2311.01022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kottakota Asish, P. Sarath Teja, R. Kishan Chander, Dr. D. Deva Hema</li>
<li>for: 这篇文章是为了探讨一种基于深度神经网络的手写数字识别方法，即NeuroWrite。</li>
<li>methods: 这篇文章使用了对于手写数字识别的构建方法，包括对于手写数字的资料准备、网络设计和训练方法。文章还使用了现代技术，例如卷积神经网络（CNNs）和回传神经网络（RNNs），以提高模型的准确性和适用性。</li>
<li>results: 根据文章的结果，NeuroWrite模型在识别和分类手写数字方面表现出色，具有高的准确性和适用性。文章还评估了NeuroWrite模型在实际应用中的性能，包括手写数字文档中的数字识别、签名验证和自动邮政区识别等。<details>
<summary>Abstract</summary>
The rapid evolution of deep neural networks has revolutionized the field of machine learning, enabling remarkable advancements in various domains. In this article, we introduce NeuroWrite, a unique method for predicting the categorization of handwritten digits using deep neural networks. Our model exhibits outstanding accuracy in identifying and categorising handwritten digits by utilising the strength of convolutional neural networks (CNNs) and recurrent neural networks (RNNs).In this article, we give a thorough examination of the data preparation methods, network design, and training methods used in NeuroWrite. By implementing state-of-the-art techniques, we showcase how NeuroWrite can achieve high classification accuracy and robust generalization on handwritten digit datasets, such as MNIST. Furthermore, we explore the model's potential for real-world applications, including digit recognition in digitized documents, signature verification, and automated postal code recognition. NeuroWrite is a useful tool for computer vision and pattern recognition because of its performance and adaptability.The architecture, training procedure, and evaluation metrics of NeuroWrite are covered in detail in this study, illustrating how it can improve a number of applications that call for handwritten digit classification. The outcomes show that NeuroWrite is a promising method for raising the bar for deep neural network-based handwritten digit recognition.
</details>
<details>
<summary>摘要</summary>
“深度神经网络的快速演化已经革命化了机器学习领域，使得各种领域得到了无前例的进步。在这篇文章中，我们介绍了一种叫做NeuroWrite的手写数字预测方法，使用深度神经网络（CNNs）和循环神经网络（RNNs）来预测手写数字的分类。我们在这篇文章中对NeuroWrite的数据准备方法、网络设计和训练方法进行了详细的介绍，并通过应用现代技术，证明了NeuroWrite在手写数字 dataset（如MNIST）上的高分类精度和robust适应能力。此外，我们还探讨了NeuroWrite在实际应用中的潜在应用，包括手写数字在扫描文档中的识别、电子签名验证和自动化邮政编码识别。NeuroWrite因其性能和适应性而成为计算机视觉和Pattern recognition中的有用工具。本文中还详细介绍了NeuroWrite的架构、训练过程和评价指标， ilustrating its potential for improving a wide range of applications that require handwritten digit classification.结果表明，NeuroWrite是一种有前途的方法，可以提高深度神经网络基于手写数字的识别水平。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Unsupervised-World-Models-for-Autonomous-Driving-via-Discrete-Diffusion"><a href="#Learning-Unsupervised-World-Models-for-Autonomous-Driving-via-Discrete-Diffusion" class="headerlink" title="Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion"></a>Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01017">http://arxiv.org/abs/2311.01017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, Raquel Urtasun</li>
<li>for: This paper aims to improve the efficiency and effectiveness of world modeling for robotic applications such as autonomous driving.</li>
<li>methods: The proposed approach uses a novel combination of VQVAE and discrete diffusion to tokenize and predict the future of sensor observations.</li>
<li>results: The proposed method achieves significant improvements in reducing prior SOTA Chamfer distance for 1s and 3s predictions on three datasets (NuScenes, KITTI Odometry, and Argoverse2). Specifically, it reduces the Chamfer distance by more than 65% for 1s predictions and more than 50% for 3s predictions.<details>
<summary>Abstract</summary>
Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, and more than 50% for 3s prediction, across NuScenes, KITTI Odometry, and Argoverse2 datasets. Our results demonstrate that discrete diffusion on tokenized agent experience can unlock the power of GPT-like unsupervised learning for robotic agents.
</details>
<details>
<summary>摘要</summary>
学习世界模型可以教导一个机器人如何在无监督的方式下理解世界。尽管它可以视为语言模型的特殊情况，但对于机器人应用程序如自动驾驶而言，进展 slower than language models with Generative Pre-trained Transformers (GPT)。我们认为这有两个主要瓶颈：处理复杂和不结构化的感知空间，以及拥有可扩展的生成模型。因此，我们提出了一种新的世界模型方法，即首先使用VQVAE卷积编码感知数据，然后预测未来via粒子扩散。为了高效地解码和减噪token，我们将Masked Generative Image Transformer重新定义为粒子扩散框架中，并对其进行一些简单的改进，从而实现了明显的改善。当应用于学习世界模型的点云观测数据时，我们的模型可以在NuScenes、KITTI Odometry和Argoverse2 datasets上降低先前的SOTA Chamfer距离，即在1秒预测中降低65%以上，在3秒预测中降低50%以上。我们的结果表明，在Tokenized Agent Experience上应用粒子扩散可以解锁GPT-like无监督学习的能力。
</details></li>
</ul>
<hr>
<h2 id="Revamping-AI-Models-in-Dermatology-Overcoming-Critical-Challenges-for-Enhanced-Skin-Lesion-Diagnosis"><a href="#Revamping-AI-Models-in-Dermatology-Overcoming-Critical-Challenges-for-Enhanced-Skin-Lesion-Diagnosis" class="headerlink" title="Revamping AI Models in Dermatology: Overcoming Critical Challenges for Enhanced Skin Lesion Diagnosis"></a>Revamping AI Models in Dermatology: Overcoming Critical Challenges for Enhanced Skin Lesion Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01009">http://arxiv.org/abs/2311.01009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deval Mehta, Brigid Betz-Stablein, Toan D Nguyen, Yaniv Gal, Adrian Bowling, Martin Haskett, Maithili Sashindranath, Paul Bonnington, Victoria Mar, H Peter Soyer, Zongyuan Ge</li>
<li>for: 针对皮肤病变的诊断图像分析领域的深度学习模型的开发呈现了明显的增长趋势，然而这些模型在临床实践中受到一些挑战。现有的皮肤科AI模型具有一些局限性，如有限的诊断输出数量、对不常见皮肤病变的测试不充分、无法检测不符合分布图像等。</li>
<li>methods: 我们提出了一种全面的Hierarchical-\textbf{O}ut of Distribution-\textbf{C}linical Triage（HOT）模型，用于诊断皮肤病变。该模型对一个临床图像进行三种输出：层次预测、对不符合分布图像发出警告，以及如果临床图像alone不充分进行诊断，则建议使用德维斯科术图像。当建议被追究时，我们的模型将临床和德维斯科术图像集成，以实现最终诊断。</li>
<li>results: 我们在一个代表性的皮肤病变数据集上进行了广泛的实验，并证明了我们的框架中每个组件的有效性和互补性。我们的多功能模型为皮肤病变诊断提供了有价值的决策支持，并为医学AI应用领域设置了一个可喜的先例。<details>
<summary>Abstract</summary>
The surge in developing deep learning models for diagnosing skin lesions through image analysis is notable, yet their clinical black faces challenges. Current dermatology AI models have limitations: limited number of possible diagnostic outputs, lack of real-world testing on uncommon skin lesions, inability to detect out-of-distribution images, and over-reliance on dermoscopic images. To address these, we present an All-In-One \textbf{H}ierarchical-\textbf{O}ut of Distribution-\textbf{C}linical Triage (HOT) model. For a clinical image, our model generates three outputs: a hierarchical prediction, an alert for out-of-distribution images, and a recommendation for dermoscopy if clinical image alone is insufficient for diagnosis. When the recommendation is pursued, it integrates both clinical and dermoscopic images to deliver final diagnosis. Extensive experiments on a representative cutaneous lesion dataset demonstrate the effectiveness and synergy of each component within our framework. Our versatile model provides valuable decision support for lesion diagnosis and sets a promising precedent for medical AI applications.
</details>
<details>
<summary>摘要</summary>
开发深度学习模型用于诊断皮肤病变的图像分析已经很流行，但它们在临床面临挑战。目前的皮肤科AI模型有一些局限性，包括有限的诊断输出数量、lack of real-world testing on rare skin lesions、无法检测非标量图像和过度依赖于皮肤镜像。为了解决这些问题，我们提出了一种All-In-One层次-\out of Distribution-\临床排序（HOT）模型。对于临床图像，我们的模型可以生成三种输出：层次预测、 alert for non-standard images 和皮肤镜像建议。当建议被追究时，它将 integrate both clinical and dermoscopic images to deliver final diagnosis。我们在一个代表性的皮肤病变数据集上进行了广泛的实验，并证明了我们的框架的有效性和各Component的相互作用。我们的多功能模型可以为皮肤病变诊断提供有价值的决策支持，并为医疗AI应用领域设置了一个可能的先例。
</details></li>
</ul>
<hr>
<h2 id="Effective-Human-AI-Teams-via-Learned-Natural-Language-Rules-and-Onboarding"><a href="#Effective-Human-AI-Teams-via-Learned-Natural-Language-Rules-and-Onboarding" class="headerlink" title="Effective Human-AI Teams via Learned Natural Language Rules and Onboarding"></a>Effective Human-AI Teams via Learned Natural Language Rules and Onboarding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01007">http://arxiv.org/abs/2311.01007</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clinicalml/onboarding_human_ai">https://github.com/clinicalml/onboarding_human_ai</a></li>
<li>paper_authors: Hussein Mozannar, Jimin J Lee, Dennis Wei, Prasanna Sattigeri, Subhro Das, David Sontag</li>
<li>for: 本研究旨在学习基于数据区域和自然语言描述的人工智能（AI）和人合作规则，以提高人AI团队的准确性。</li>
<li>methods: 本研究使用了一种新的区域发现算法，可以在数据空间中找到本地区域，并使用迭代和对比过程将这些区域描述以便人类理解。</li>
<li>results: 通过对物体检测和问答任务进行人类学习和评估，研究发现，通过使用本研究提出的方法，人AI团队的准确性可以得到进一步提高。此外，研究还分别评估了区域发现和描述算法的效果。<details>
<summary>Abstract</summary>
People are relying on AI agents to assist them with various tasks. The human must know when to rely on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to learn rules grounded in data regions and described in natural language that illustrate how the human should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data as neighborhoods in an embedding space that corrects the human prior. Each region is then described using an iterative and contrastive procedure where a large language model describes the region. We then teach these rules to the human via an onboarding stage. Through user studies on object detection and question-answering tasks, we show that our method can lead to more accurate human-AI teams. We also evaluate our region discovery and description algorithms separately.
</details>
<details>
<summary>摘要</summary>
人们正在依靠人工智能代理人 assistance 完成各种任务。人类需要知道何时依靠代理人、合作与代理人或忽略其建议。在这项工作中，我们提议通过学习基于数据区域的规则，以便人类与AI合作更加准确。我们的新区域发现算法在嵌入空间中找到地方，并将其描述为人类可以理解的形式。然后，我们通过一种迭代和对比的过程，使用大型自然语言处理模型描述这些地方。最后，我们将这些规则传递给人类，以便他们可以更好地与AI合作。通过对物体检测和问答任务的用户研究，我们显示了我们的方法可以带来更加准确的人类-AI团队。我们还分别评估了我们的区域发现和描述算法。
</details></li>
</ul>
<hr>
<h2 id="Sam-Guided-Enhanced-Fine-Grained-Encoding-with-Mixed-Semantic-Learning-for-Medical-Image-Captioning"><a href="#Sam-Guided-Enhanced-Fine-Grained-Encoding-with-Mixed-Semantic-Learning-for-Medical-Image-Captioning" class="headerlink" title="Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning"></a>Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01004">http://arxiv.org/abs/2311.01004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaoang Wang, Zhenyu Zhang, Benlu Wang, Weijie Liang, Yizhi Li, Xuechen Guo, Guanhong Wang, Shiyan Li</li>
<li>for: 这篇论文旨在提出一种基于深度学习的医疗影像描述方法，以提供更好的诊断建议。</li>
<li>methods: 本论文使用了Segment Anything Model（SAM）来实现更好的缩寸和细部特征提取，并且运用混合semantic learning的独特预训策略，同时捕捉医疗影像的全面信息和细部细节。</li>
<li>results: 本论文证明了这种方法的效iveness，与预训BLIP2模型相比，在不同的评估指标上表现出色，能够更好地描述医疗影像的内容。<details>
<summary>Abstract</summary>
With the development of multimodality and large language models, the deep learning-based technique for medical image captioning holds the potential to offer valuable diagnostic recommendations. However, current generic text and image pre-trained models do not yield satisfactory results when it comes to describing intricate details within medical images. In this paper, we present a novel medical image captioning method guided by the segment anything model (SAM) to enable enhanced encoding with both general and detailed feature extraction. In addition, our approach employs a distinctive pre-training strategy with mixed semantic learning to simultaneously capture both the overall information and finer details within medical images. We demonstrate the effectiveness of this approach, as it outperforms the pre-trained BLIP2 model on various evaluation metrics for generating descriptions of medical images.
</details>
<details>
<summary>摘要</summary>
随着多模态和大语言模型的发展，深度学习基于医疗图像描述技术具有诊断建议的潜在价值。然而，当前的通用文本和图像预训练模型无法准确描述医疗图像中的细节。在这篇论文中，我们提出了一种基于segment anything模型（SAM）的新型医疗图像描述方法，以便增强通用特征提取和细节特征提取。此外，我们的方法采用混合semantic学习策略，同时捕捉医疗图像的总体信息和细节信息。我们的实验表明，这种方法可以超过预训练的BLIP2模型，在不同的评价指标上为医疗图像生成描述具有更高的效果。
</details></li>
</ul>
<hr>
<h2 id="Robust-Data-Pruning-under-Label-Noise-via-Maximizing-Re-labeling-Accuracy"><a href="#Robust-Data-Pruning-under-Label-Noise-via-Maximizing-Re-labeling-Accuracy" class="headerlink" title="Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy"></a>Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01002">http://arxiv.org/abs/2311.01002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongmin Park, Seola Choi, Doyoung Kim, Hwanjun Song, Jae-Gil Lee</li>
<li>for: 降低深度学习的计算成本，通过减少大规模训练集来实现数据采样。</li>
<li>methods: 提出了一种基于预测信息的数据采样算法，通过计算邻域例子的预测信息来选择最有用的示例集。</li>
<li>results: 对四个真实数据集和一个 sintetic 数据集进行了广泛的实验，结果显示，相比基eline，\algname{}可以提高标注模型的泛化性能和预测精度，最高提高9.1%和21.6%。<details>
<summary>Abstract</summary>
Data pruning, which aims to downsize a large training set into a small informative subset, is crucial for reducing the enormous computational costs of modern deep learning. Though large-scale data collections invariably contain annotation noise and numerous robust learning methods have been developed, data pruning for the noise-robust learning scenario has received little attention. With state-of-the-art Re-labeling methods that self-correct erroneous labels while training, it is challenging to identify which subset induces the most accurate re-labeling of erroneous labels in the entire training set. In this paper, we formalize the problem of data pruning with re-labeling. We first show that the likelihood of a training example being correctly re-labeled is proportional to the prediction confidence of its neighborhood in the subset. Therefore, we propose a novel data pruning algorithm, Prune4Rel, that finds a subset maximizing the total neighborhood confidence of all training examples, thereby maximizing the re-labeling accuracy and generalization performance. Extensive experiments on four real and one synthetic noisy datasets show that \algname{} outperforms the baselines with Re-labeling models by up to 9.1% as well as those with a standard model by up to 21.6%.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>现代深度学习的计算成本很高，因此大规模数据集的减小成本是至关重要的。尽管大规模数据集总是含有注释噪声和许多Robust学习方法已经开发出来，但是对于噪声Robust学习场景，数据减小尚未得到足够的关注。使用现代重新标注方法可以在训练过程中自动更正错误标签，但是寻找整个训练集中最精确地重新标注错误标签的子集是挑战。在这篇论文中，我们正式定义了数据减小与重新标注的问题。我们首先表明，训练示例 Correctly重新标注的可能性与其邻域在子集中的预测信心直接相关。因此，我们提出了一种新的数据减小算法，名为Prune4Rel，它找到一个最大化全局邻域信心的所有训练示例的子集，以最大化重新标注准确性和泛化性能。我们对四个真实的噪声数据集和一个synthetic数据集进行了广泛的实验，结果显示，\algname{}相比基eline模型和标准模型，提高了9.1%和21.6%。
</details></li>
</ul>
<hr>
<h2 id="Fully-Quantized-Always-on-Face-Detector-Considering-Mobile-Image-Sensors"><a href="#Fully-Quantized-Always-on-Face-Detector-Considering-Mobile-Image-Sensors" class="headerlink" title="Fully Quantized Always-on Face Detector Considering Mobile Image Sensors"></a>Fully Quantized Always-on Face Detector Considering Mobile Image Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01001">http://arxiv.org/abs/2311.01001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haechang Lee, Wongi Jeong, Dongil Ryu, Hyunwoo Je, Albert No, Kijeong Kim, Se Young Chun</li>
<li>for: 这个研究旨在对 Always-on 面部检测scenario for 移动像感应器应用进行 bridging  gap.</li>
<li>methods: 我们的提案使用感应器 Raw 输入，模拟 Always-on 面部检测过程 “before” ISP 链接. 我们的方法使用三元 (-1, 0, 1) 的权重，实现了实际上的图像感应器实现.</li>
<li>results: 我们的方法在模拟研究中展示了理想的面部检测性和优秀的效率.<details>
<summary>Abstract</summary>
Despite significant research on lightweight deep neural networks (DNNs) designed for edge devices, the current face detectors do not fully meet the requirements for "intelligent" CMOS image sensors (iCISs) integrated with embedded DNNs. These sensors are essential in various practical applications, such as energy-efficient mobile phones and surveillance systems with always-on capabilities. One noteworthy limitation is the absence of suitable face detectors for the always-on scenario, a crucial aspect of image sensor-level applications. These detectors must operate directly with sensor RAW data before the image signal processor (ISP) takes over. This gap poses a significant challenge in achieving optimal performance in such scenarios. Further research and development are necessary to bridge this gap and fully leverage the potential of iCIS applications. In this study, we aim to bridge the gap by exploring extremely low-bit lightweight face detectors, focusing on the always-on face detection scenario for mobile image sensor applications. To achieve this, our proposed model utilizes sensor-aware synthetic RAW inputs, simulating always-on face detection processed "before" the ISP chain. Our approach employs ternary (-1, 0, 1) weights for potential implementations in image sensors, resulting in a relatively simple network architecture with shallow layers and extremely low-bitwidth. Our method demonstrates reasonable face detection performance and excellent efficiency in simulation studies, offering promising possibilities for practical always-on face detectors in real-world applications.
</details>
<details>
<summary>摘要</summary>
尽管有大量研究关于轻量级深度神经网络（DNN），目前的脸部检测器还没有完全满足智能CMOS图像传感器（iCIS）的需求。这些检测器在实际应用中非常重要，例如能效的手机和 Always-on surveillance系统。一个吸引人的限制是 absent 的适用于 Always-on 场景的脸部检测器，这是图像感知器（ISP）链的一个关键环节。这个差距使得实现最佳性能在这些场景变得非常困难。为了跨越这个差距，我们在这个研究中尝试通过探索极低位数轻量级脸部检测器来bridging 这个差距。我们的提议的模型使用感知器 Raw 输入，模拟 Always-on 脸部检测场景，并使用 (-1, 0, 1) 的ternary 权重。这种方法使得我们的网络架构非常简单，具有极低的位数宽。我们的方法在模拟研究中表现出了合理的脸部检测性能和优秀的效率，提供了实用 Always-on 脸部检测器的可能性。
</details></li>
</ul>
<hr>
<h2 id="Replicable-Benchmarking-of-Neural-Machine-Translation-NMT-on-Low-Resource-Local-Languages-in-Indonesia"><a href="#Replicable-Benchmarking-of-Neural-Machine-Translation-NMT-on-Low-Resource-Local-Languages-in-Indonesia" class="headerlink" title="Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource Local Languages in Indonesia"></a>Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource Local Languages in Indonesia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00998">http://arxiv.org/abs/2311.00998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exqrch/indonesiannmt">https://github.com/exqrch/indonesiannmt</a></li>
<li>paper_authors: Lucky Susanto, Ryandito Diandaru, Adila Krisnadhi, Ayu Purwarianti, Derry Wijaya</li>
<li>for: 本研究旨在解决印度尼西亚低资源本地语言中文机器翻译 faces  significiant challenges，包括需要代表性的标准和有限的数据可用性。</li>
<li>methods: 本研究使用了多种训练方法、概念和数据大小，并进行了一些初步的大语言模型为低资源语言平行数据生成的研究。</li>
<li>results: 我们的研究发现，尽管有限的计算资源和文本数据，several of our NMT systems 仍然可以达到竞争性的翻译质量，与零批gpt-3.5-turbo的翻译质量相当。这些发现对低资源语言翻译具有重要的进步，对研究人员在类似情况下具有很大的价值。<details>
<summary>Abstract</summary>
Neural machine translation (NMT) for low-resource local languages in Indonesia faces significant challenges, including the need for a representative benchmark and limited data availability. This work addresses these challenges by comprehensively analyzing training NMT systems for four low-resource local languages in Indonesia: Javanese, Sundanese, Minangkabau, and Balinese. Our study encompasses various training approaches, paradigms, data sizes, and a preliminary study into using large language models for synthetic low-resource languages parallel data generation. We reveal specific trends and insights into practical strategies for low-resource language translation. Our research demonstrates that despite limited computational resources and textual data, several of our NMT systems achieve competitive performances, rivaling the translation quality of zero-shot gpt-3.5-turbo. These findings significantly advance NMT for low-resource languages, offering valuable guidance for researchers in similar contexts.
</details>
<details>
<summary>摘要</summary>
神经机器翻译（NMT） для低资源本地语言在印度尼西亚面临重大挑战，包括需要代表性的标准和有限的数据可用性。这项工作解决这些挑战，通过对四种低资源本地语言的NMT系统进行全面分析： javanese、Sundanese、Minangkabau 和 Balinese。我们的研究包括不同的训练方法、概念、数据大小和初步研究使用大型语言模型生成低资源语言平行数据。我们发现了特定的趋势和策略，以及在实际翻译中的实用性。我们的研究表明，尽管计算机资源有限和文本数据有限，但是一些我们的NMT系统可以达到竞争的翻译质量，与零shot gpt-3.5-turbo相当。这些发现对NMT的低资源语言翻译做出了重要贡献，为研究人员提供了有价值的指南。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Inventory-Routing-A-Decision-Focused-Learning-Approach-using-Neural-Networks"><a href="#Optimizing-Inventory-Routing-A-Decision-Focused-Learning-Approach-using-Neural-Networks" class="headerlink" title="Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks"></a>Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00983">http://arxiv.org/abs/2311.00983</a></li>
<li>repo_url: None</li>
<li>paper_authors: MD Shafikul Islam, Azmine Toushik Wasi</li>
<li>for: 解决供应链管理中的货物 Routing 问题 (IRP)，这是一个关键的挑战，因为它涉及到最优化的路径选择，同时考虑到货物需求预测的不确定性。</li>
<li>methods: 我们的实验表明，通常使用两个阶段方法来解决 IRP，首先使用机器学习技术预测需求，然后使用优化算法来最小化 Routing 成本。</li>
<li>results: 然而，我们发现机器学习模型无法达到完美准确性，因为货物储备水平受到动态商业环境的影响，这将在下一阶段的优化问题中产生不优化的决策。在这篇论文中，我们提出了一种专注于决策的学习基本方法，以解决真实世界中的 IRP。这种方法直接将货物预测和 Routing 优化 integrate 到一个综合系统中，可能提供一个有力的供应链策略。<details>
<summary>Abstract</summary>
Inventory Routing Problem (IRP) is a crucial challenge in supply chain management as it involves optimizing efficient route selection while considering the uncertainty of inventory demand planning. To solve IRPs, usually a two-stage approach is employed, where demand is predicted using machine learning techniques first, and then an optimization algorithm is used to minimize routing costs. Our experiment shows machine learning models fall short of achieving perfect accuracy because inventory levels are influenced by the dynamic business environment, which, in turn, affects the optimization problem in the next stage, resulting in sub-optimal decisions. In this paper, we formulate and propose a decision-focused learning-based approach to solving real-world IRPs. This approach directly integrates inventory prediction and routing optimization within an end-to-end system potentially ensuring a robust supply chain strategy.
</details>
<details>
<summary>摘要</summary>
供应链管理中的存储路径问题（IRP）是一个重要的挑战，因为它涉及到有效地选择路径，同时考虑存储需求规划的不确定性。通常，解决IRP需要采用两个阶段的方法，其中首先使用机器学习技术预测需求，然后使用优化算法减少路径成本。我们的实验表明，机器学习模型无法达到完美准确性，因为存储水平受到动态商业环境的影响，这有利于下一阶段的优化问题，导致偏低的决策。在这篇论文中，我们提出了一种专注于决策的学习基于方法，以解决现实世界中的IRP。这种方法直接 интегрируiert存储预测和路径优化在一个综合系统中，有可能确保一个强大的供应链策略。
</details></li>
</ul>
<hr>
<h2 id="An-Integrated-Framework-Integrating-Monte-Carlo-Tree-Search-and-Supervised-Learning-for-Train-Timetabling-Problem"><a href="#An-Integrated-Framework-Integrating-Monte-Carlo-Tree-Search-and-Supervised-Learning-for-Train-Timetabling-Problem" class="headerlink" title="An Integrated Framework Integrating Monte Carlo Tree Search and Supervised Learning for Train Timetabling Problem"></a>An Integrated Framework Integrating Monte Carlo Tree Search and Supervised Learning for Train Timetabling Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00971">http://arxiv.org/abs/2311.00971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feiyu Yang</li>
<li>for: 解决单轨铁路列车时间安排问题 (TTP)，这是一个重要和复杂的问题。</li>
<li>methods: 本文提出了一个整合 Monte Carlo Tree Search (MCTS) 计算框架，该框架结合了规则方法、无监督学习方法和监督学习方法来解决 TTP 中的 discrete action space 问题。</li>
<li>results: 实验显示，提出的启发式 MCTS 方法对 TTP 有利，并且将 learners 应用于 MCTS 搜索过程可以提高数据效率。这种方法提供了一个新的 TTP 解决方案。<details>
<summary>Abstract</summary>
The single-track railway train timetabling problem (TTP) is an important and complex problem. This article proposes an integrated Monte Carlo Tree Search (MCTS) computing framework that combines heuristic methods, unsupervised learning methods, and supervised learning methods for solving TTP in discrete action spaces. This article first describes the mathematical model and simulation system dynamics of TTP, analyzes the characteristics of the solution from the perspective of MCTS, and proposes some heuristic methods to improve MCTS. This article considers these methods as planners in the proposed framework. Secondly, this article utilizes deep convolutional neural networks to approximate the value of nodes and further applies them to the MCTS search process, referred to as learners. The experiment shows that the proposed heuristic MCTS method is beneficial for solving TTP; The algorithm framework that integrates planners and learners can improve the data efficiency of solving TTP; The proposed method provides a new paradigm for solving TTP.
</details>
<details>
<summary>摘要</summary>
单轨铁路列车时刻表（TTP）是一个重要和复杂的问题。这篇文章提出了一个集成 Monte Carlo Tree Search（MCTS）计算框架，该框架结合了规则方法、无监督学习方法和监督学习方法来解决 TTP 中的离散行动空间问题。文章首先描述了 TTP 的数学模型和 simulate 系统动力学，分析了 MCTS 方法解决 TTP 的特点，并提出了一些规则方法来改进 MCTS。这些方法被视为计划者在提出的框架中。其次，文章利用深度卷积神经网络来估算节点的值，并将其应用到 MCTS 搜索过程中，被称为学习者。实验表明，提出的规则 MCTS 方法有利于解决 TTP。集成计划者和学习者的算法框架可以提高解决 TTP 的数据效率；该方法提供了一个新的 TTP 解决方法。
</details></li>
</ul>
<hr>
<h2 id="Video2Music-Suitable-Music-Generation-from-Videos-using-an-Affective-Multimodal-Transformer-model"><a href="#Video2Music-Suitable-Music-Generation-from-Videos-using-an-Affective-Multimodal-Transformer-model" class="headerlink" title="Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model"></a>Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00968">http://arxiv.org/abs/2311.00968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amaai-lab/video2music">https://github.com/amaai-lab/video2music</a></li>
<li>paper_authors: Jaeyong Kang, Soujanya Poria, Dorien Herremans</li>
<li>for: 这个研究旨在开发一个可生成音乐的 AI 框架，以匹配提供的视频。</li>
<li>methods: 研究人员首先筹集了一个独特的音乐视频集，然后分析了这些音乐视频，从而获得了semantic、scene offset、motion和emotion等特征。这些特征被用作音乐生成模型的引导输入。</li>
<li>results: 研究人员通过一种名为 Affective Multimodal Transformer (AMT) 的新型模型，使得生成的音乐与视频内容的情感相似。此外，研究人员还使用了一种基于 bigGRU 的回归模型来估算视频特征中的音符密度和响度，以确保生成的和声与视频的匹配性。<details>
<summary>Abstract</summary>
Numerous studies in the field of music generation have demonstrated impressive performance, yet virtually no models are able to directly generate music to match accompanying videos. In this work, we develop a generative music AI framework, Video2Music, that can match a provided video. We first curated a unique collection of music videos. Then, we analysed the music videos to obtain semantic, scene offset, motion, and emotion features. These distinct features are then employed as guiding input to our music generation model. We transcribe the audio files into MIDI and chords, and extract features such as note density and loudness. This results in a rich multimodal dataset, called MuVi-Sync, on which we train a novel Affective Multimodal Transformer (AMT) model to generate music given a video. This model includes a novel mechanism to enforce affective similarity between video and music. Finally, post-processing is performed based on a biGRU-based regression model to estimate note density and loudness based on the video features. This ensures a dynamic rendering of the generated chords with varying rhythm and volume. In a thorough experiment, we show that our proposed framework can generate music that matches the video content in terms of emotion. The musical quality, along with the quality of music-video matching is confirmed in a user study. The proposed AMT model, along with the new MuVi-Sync dataset, presents a promising step for the new task of music generation for videos.
</details>
<details>
<summary>摘要</summary>
许多音乐生成研究已经表现出卓越表现，但是几乎没有模型可以直接生成与视频相匹配的音乐。在这个工作中，我们开发了一个生成音乐AI框架，即Video2Music，可以匹配提供的视频。我们首先筹集了一个独特的音乐视频集。然后，我们分析了音乐视频，以获取Semantic、Scene Offset、Motion和Emotion等特征。这些特征被用作音乐生成模型的导入输入。我们将音频文件转译成MIDI和和声，并提取特征 such as note density和 loudness。这结果了一个丰富的多模态数据集，称为MuVi-Sync，在这个数据集上我们训练了一个新的Affective Multimodal Transformer（AMT）模型，以生成音乐给视频。这个模型包括一个新的机制，以保证视频和音乐之间的情感相似性。最后，基于一个biGRU-based regression模型，我们进行了后处理，以估算视频特征基于的音乐的Note density和Loudness。这确保了生成的和声在不同的节奏和音量上进行了动态渲染。在一项全面的实验中，我们证明了我们的提议框架可以根据视频内容生成匹配的音乐，并且音乐质量和音乐-视频匹配质量得到了用户研究的证实。提出的AMT模型，加之新的MuVi-Sync数据集，对于新的音乐生成 для视频任务提出了一个可能的步骤。
</details></li>
</ul>
<hr>
<h2 id="Vision-Language-Interpreter-for-Robot-Task-Planning"><a href="#Vision-Language-Interpreter-for-Robot-Task-Planning" class="headerlink" title="Vision-Language Interpreter for Robot Task Planning"></a>Vision-Language Interpreter for Robot Task Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00967">http://arxiv.org/abs/2311.00967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/omron-sinicx/vilain">https://github.com/omron-sinicx/vilain</a></li>
<li>paper_authors: Keisuke Shirai, Cristian C. Beltran-Hernandez, Masashi Hamaya, Atsushi Hashimoto, Shohei Tanaka, Kento Kawaharazuka, Kazutoshi Tanaka, Yoshitaka Ushiku, Shinsuke Mori</li>
<li>for: 本研究的目的是提出一个新的任务，即多模态规划问题说明（Multimodal Planning Problem Specification，简称MPPS），以便使用语言指导的 симвоlic planner 来解决问题。</li>
<li>methods: 本研究使用了 state-of-the-art 的语言模型和视觉语言模型来生成问题描述（Problem Description，简称PD），并通过Symbolic planner 的反馈来纠正生成的PD。</li>
<li>results: 实验结果表明，ViLaIn 可以生成有正确 syntax 的问题描述，并且可以生成有效的机器人计划，其中有效性高于 58%。<details>
<summary>Abstract</summary>
Large language models (LLMs) are accelerating the development of language-guided robot planners. Meanwhile, symbolic planners offer the advantage of interpretability. This paper proposes a new task that bridges these two trends, namely, multimodal planning problem specification. The aim is to generate a problem description (PD), a machine-readable file used by the planners to find a plan. By generating PDs from language instruction and scene observation, we can drive symbolic planners in a language-guided framework. We propose a Vision-Language Interpreter (ViLaIn), a new framework that generates PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine generated PDs via error message feedback from the symbolic planner. Our aim is to answer the question: How accurately can ViLaIn and the symbolic planner generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset called the problem description generation (ProDG) dataset. The framework is evaluated with four new evaluation metrics. Experimental results show that ViLaIn can generate syntactically correct problems with more than 99% accuracy and valid plans with more than 58% accuracy.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）正在推动语言导航 robot 规划器的发展。然而，符号规划器具有可解释性的优势。本文提出了一个新任务，即多modal 规划问题规定。目标是生成一个问题描述（PD），一个机器可读的文件，用于由规划器找到一个计划。通过将语言指令和场景观察转换为PD，我们可以在语言导航框架下驱动符号规划器。我们提出了一个名为视力语言 интерпреTER（ViLaIn）的新框架，它使用当前的 LLM 和视力语言模型来生成PD。ViLaIn 可以通过符号规划器返回错误消息来精细地修正生成的PD。我们的目标是回答这个问题：ViLaIn 和符号规划器能够生成有效的机器人计划吗？为了评估 ViLaIn，我们创建了一个名为问题描述生成（ProDG）数据集。框架在四个新的评价指标下进行了评估。实验结果显示，ViLaIn 可以生成符合语法规则的问题描述，准确率高于 99%，并且可以生成有效的计划，准确率高于 58%。
</details></li>
</ul>
<hr>
<h2 id="IndoToD-A-Multi-Domain-Indonesian-Benchmark-For-End-to-End-Task-Oriented-Dialogue-Systems"><a href="#IndoToD-A-Multi-Domain-Indonesian-Benchmark-For-End-to-End-Task-Oriented-Dialogue-Systems" class="headerlink" title="IndoToD: A Multi-Domain Indonesian Benchmark For End-to-End Task-Oriented Dialogue Systems"></a>IndoToD: A Multi-Domain Indonesian Benchmark For End-to-End Task-Oriented Dialogue Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00958">http://arxiv.org/abs/2311.00958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dehanalkautsar/indotod">https://github.com/dehanalkautsar/indotod</a></li>
<li>paper_authors: Muhammad Dehan Al Kautsar, Rahmah Khoirussyifa’ Nurdini, Samuel Cahyawijaya, Genta Indra Winata, Ayu Purwarianti</li>
<li>for: 这个论文主要是为了开发高级语言（如英语和中文）以外的地域语言Task-oriented dialogue（ToD）系统，以拓宽对对话上下文的理解能力。</li>
<li>methods: 这篇论文使用了两个英语ToD数据集的泛化，通过去 lexicalization 来减少笔记注释的大小，并雇用了本地母语 speaker 手动翻译对话。</li>
<li>results: 这篇论文引入了一个综合多个领域的Indonesian ToDbenchmark，可以用于评估英语和INDONESIAN ToD系统，以及探索跨语言和双语权重学习方法的潜在利器。<details>
<summary>Abstract</summary>
Task-oriented dialogue (ToD) systems have been mostly created for high-resource languages, such as English and Chinese. However, there is a need to develop ToD systems for other regional or local languages to broaden their ability to comprehend the dialogue contexts in various languages. This paper introduces IndoToD, an end-to-end multi domain ToD benchmark in Indonesian. We extend two English ToD datasets to Indonesian, comprising four different domains by delexicalization to efficiently reduce the size of annotations. To ensure a high-quality data collection, we hire native speakers to manually translate the dialogues. Along with the original English datasets, these new Indonesian datasets serve as an effective benchmark for evaluating Indonesian and English ToD systems as well as exploring the potential benefits of cross-lingual and bilingual transfer learning approaches.
</details>
<details>
<summary>摘要</summary>
高度资源语言如英语和中文的任务对话（ToD）系统已经大多创建，但是有必要开发ToD系统 для其他地区或本地语言，以扩大对对话上下文的理解能力。这篇文章介绍了印度ToD，一个综合多领域的ToD benchmark在印度尼西亚语中。我们将英语ToD数据集扩展到印度尼西亚语，包括四个不同的领域，通过去除语言标记来有效地减少注释大小。为保证高质量数据采集，我们雇佣了本地母语speaker来手动翻译对话。与原始英语数据集一起，这些新的印度尼西亚语数据集将成为评估印度尼西亚语和英语ToD系统的有效 benchamark，以及探索跨语言和双语传输学习方法的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Mixture-Solvers-for-Diffusion-Models"><a href="#Gaussian-Mixture-Solvers-for-Diffusion-Models" class="headerlink" title="Gaussian Mixture Solvers for Diffusion Models"></a>Gaussian Mixture Solvers for Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00941">http://arxiv.org/abs/2311.00941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guohanzhong/gms">https://github.com/guohanzhong/gms</a></li>
<li>paper_authors: Hanzhong Guo, Cheng Lu, Fan Bao, Tianyu Pang, Shuicheng Yan, Chao Du, Chongxuan Li</li>
<li>for: 这个论文主要针对的是 diffusion models 的生成任务中的样本生成问题。</li>
<li>methods: 该论文提出了一种新的 SDE-based 生成器，称为 Gaussian Mixture Solvers (GMS)，它可以在批处理中更好地控制样本质量。</li>
<li>results: 实验表明，GMS 可以在各种 diffusion models 中提供更高质量的样本，并且在 stroke-based synthesis 等任务中表现更好。<details>
<summary>Abstract</summary>
Recently, diffusion models have achieved great success in generative tasks. Sampling from diffusion models is equivalent to solving the reverse diffusion stochastic differential equations (SDEs) or the corresponding probability flow ordinary differential equations (ODEs). In comparison, SDE-based solvers can generate samples of higher quality and are suited for image translation tasks like stroke-based synthesis. During inference, however, existing SDE-based solvers are severely constrained by the efficiency-effectiveness dilemma. Our investigation suggests that this is because the Gaussian assumption in the reverse transition kernel is frequently violated (even in the case of simple mixture data) given a limited number of discretization steps. To overcome this limitation, we introduce a novel class of SDE-based solvers called \emph{Gaussian Mixture Solvers (GMS)} for diffusion models. Our solver estimates the first three-order moments and optimizes the parameters of a Gaussian mixture transition kernel using generalized methods of moments in each step during sampling. Empirically, our solver outperforms numerous SDE-based solvers in terms of sample quality in image generation and stroke-based synthesis in various diffusion models, which validates the motivation and effectiveness of GMS. Our code is available at https://github.com/Guohanzhong/GMS.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Bridging-the-Gap-Addressing-Discrepancies-in-Diffusion-Model-Training-for-Classifier-Free-Guidance"><a href="#Bridging-the-Gap-Addressing-Discrepancies-in-Diffusion-Model-Training-for-Classifier-Free-Guidance" class="headerlink" title="Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance"></a>Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00938">http://arxiv.org/abs/2311.00938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niket Patel, Luis Salamanca, Luis Barba</li>
<li>for: 本研究旨在探讨Diffusion模型的训练方法和生成结果之间的矛盾，以及如何改进Diffusion模型的生成质量。</li>
<li>methods: 本研究使用了一种更新的损失函数，以更好地对准Diffusion模型的训练目标和生成行为。</li>
<li>results: 实验结果表明，使用该更新后的损失函数可以生成更高质量的样本，并且可以降低指导缩放参数$w$的选择对生成结果的影响。<details>
<summary>Abstract</summary>
Diffusion models have emerged as a pivotal advancement in generative models, setting new standards to the quality of the generated instances. In the current paper we aim to underscore a discrepancy between conventional training methods and the desired conditional sampling behavior of these models. While the prevalent classifier-free guidance technique works well, it's not without flaws. At higher values for the guidance scale parameter $w$, we often get out of distribution samples and mode collapse, whereas at lower values for $w$ we may not get the desired specificity. To address these challenges, we introduce an updated loss function that better aligns training objectives with sampling behaviors. Experimental validation with FID scores on CIFAR-10 elucidates our method's ability to produce higher quality samples with fewer sampling timesteps, and be more robust to the choice of guidance scale $w$. We also experiment with fine-tuning Stable Diffusion on the proposed loss, to provide early evidence that large diffusion models may also benefit from this refined loss function.
</details>
<details>
<summary>摘要</summary>
各种扩散模型在生成模型中已经成为了重要的进步，为生成实例提供了新的标准。在当前的论文中，我们想要强调普遍的导航方法和扩散模型的 conditional sampling 行为之间的不一致。虽然无类别导航技术在高于 $w$ 的值下能够工作良好，但是存在误差。在较低的 $w$ 值下，我们可能无法获得所需的特定性，而在更高的 $w$ 值下，我们可能会得到偏差的样本。为了解决这些挑战，我们提出了一个更新的损失函数，该函数更好地对应培训目标和抽样行为。实验证明，我们的方法可以在 CIFAR-10 上获得更高质量的样本，并且更加敏感于导航缩放参数 $w$。我们还对 Stable Diffusion 进行了微调，以提供早期的证据，表明大扩散模型也可以从这种精细的损失函数中受益。
</details></li>
</ul>
<hr>
<h2 id="Scalable-Counterfactual-Distribution-Estimation-in-Multivariate-Causal-Models"><a href="#Scalable-Counterfactual-Distribution-Estimation-in-Multivariate-Causal-Models" class="headerlink" title="Scalable Counterfactual Distribution Estimation in Multivariate Causal Models"></a>Scalable Counterfactual Distribution Estimation in Multivariate Causal Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00927">http://arxiv.org/abs/2311.00927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thong Pham, Shohei Shimizu, Hideitsu Hino, Tam Le</li>
<li>for: 估计多个量关注（例如结果）的共轭事件分布 Function: 多ivariate causal model中的共轭事件分布估计问题</li>
<li>methods: 利用一个可靠的一维隐藏空间，基于所有维度信息构建，以便更好地捕捉相关结构并生成良好的共轭事件分布估计</li>
<li>results: 相比现有方法，提供更好的共轭事件分布估计，并在实际数据上显示出更高的准确性和稳定性<details>
<summary>Abstract</summary>
We consider the problem of estimating the counterfactual joint distribution of multiple quantities of interests (e.g., outcomes) in a multivariate causal model extended from the classical difference-in-difference design. Existing methods for this task either ignore the correlation structures among dimensions of the multivariate outcome by considering univariate causal models on each dimension separately and hence produce incorrect counterfactual distributions, or poorly scale even for moderate-size datasets when directly dealing with such multivariate causal model. We propose a method that alleviates both issues simultaneously by leveraging a robust latent one-dimensional subspace of the original high-dimension space and exploiting the efficient estimation from the univariate causal model on such space. Since the construction of the one-dimensional subspace uses information from all the dimensions, our method can capture the correlation structures and produce good estimates of the counterfactual distribution. We demonstrate the advantages of our approach over existing methods on both synthetic and real-world data.
</details>
<details>
<summary>摘要</summary>
我团队正在考虑一个多量 interess 的 causal 模型的问题，即在多量 outcome 上的 counterfactual  JOINT 分布的估计问题。现有的方法可能忽略多量 outcome 的 correlation 结构，通过对每个维度 separately 处理 causal 模型，从而生成错误的 counterfactual 分布，或者对大型数据集进行 direct 处理时会产生差异。我们提出了一种方法，可以同时解决这两个问题，通过利用一个 robust 的 latent 一维空间，并利用这一空间上的 efficient 估计来避免直接处理高维 causal 模型时的问题。由于构造一维空间使用了所有维度的信息，我们的方法可以捕捉 correlation 结构，并生成良好的 counterfactual 分布估计。我们在 synthetic 数据和实际数据上证明了我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="M2T2-Multi-Task-Masked-Transformer-for-Object-centric-Pick-and-Place"><a href="#M2T2-Multi-Task-Masked-Transformer-for-Object-centric-Pick-and-Place" class="headerlink" title="M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place"></a>M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00926">http://arxiv.org/abs/2311.00926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Yuan, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox</li>
<li>for: 这个论文的目的是提出一个单一的模型，能够在不同的物体上进行多种低层运动 Primitives，并在各种不同的场景中进行稳定的物体搬运。</li>
<li>methods: 这个模型使用了 transformer 模型，它可以根据触碰点来决定合适的握持位置，并预测不同的动作模式下的有效握持 pose。</li>
<li>results: 在一个大规模的 sintetic 数据集上进行训练，这个模型在真实机器人上进行零 shot sim2real 转移，比基eline系统表现出来19%的总性能和37.5%的挑战场景中的性能提升。此外，这个模型也在RLBench中的一 subset of language conditioned tasks 上得到了状况的最佳结果。<details>
<summary>Abstract</summary>
With the advent of large language models and large-scale robotic datasets, there has been tremendous progress in high-level decision-making for object manipulation. These generic models are able to interpret complex tasks using language commands, but they often have difficulties generalizing to out-of-distribution objects due to the inability of low-level action primitives. In contrast, existing task-specific models excel in low-level manipulation of unknown objects, but only work for a single type of action. To bridge this gap, we present M2T2, a single model that supplies different types of low-level actions that work robustly on arbitrary objects in cluttered scenes. M2T2 is a transformer model which reasons about contact points and predicts valid gripper poses for different action modes given a raw point cloud of the scene. Trained on a large-scale synthetic dataset with 128K scenes, M2T2 achieves zero-shot sim2real transfer on the real robot, outperforming the baseline system with state-of-the-art task-specific models by about 19% in overall performance and 37.5% in challenging scenes where the object needs to be re-oriented for collision-free placement. M2T2 also achieves state-of-the-art results on a subset of language conditioned tasks in RLBench. Videos of robot experiments on unseen objects in both real world and simulation are available on our project website https://m2-t2.github.io.
</details>
<details>
<summary>摘要</summary>
With the advent of large language models and large-scale robotic datasets, there has been tremendous progress in high-level decision-making for object manipulation. These generic models are able to interpret complex tasks using language commands, but they often have difficulties generalizing to out-of-distribution objects due to the inability of low-level action primitives. In contrast, existing task-specific models excel in low-level manipulation of unknown objects, but only work for a single type of action. To bridge this gap, we present M2T2, a single model that supplies different types of low-level actions that work robustly on arbitrary objects in cluttered scenes. M2T2 is a transformer model which reasons about contact points and predicts valid gripper poses for different action modes given a raw point cloud of the scene. Trained on a large-scale synthetic dataset with 128K scenes, M2T2 achieves zero-shot sim2real transfer on the real robot, outperforming the baseline system with state-of-the-art task-specific models by about 19% in overall performance and 37.5% in challenging scenes where the object needs to be re-oriented for collision-free placement. M2T2 also achieves state-of-the-art results on a subset of language conditioned tasks in RLBench. Videos of robot experiments on unseen objects in both real world and simulation are available on our project website (https://m2-t2.github.io).
</details></li>
</ul>
<hr>
<h2 id="The-Power-of-the-Senses-Generalizable-Manipulation-from-Vision-and-Touch-through-Masked-Multimodal-Learning"><a href="#The-Power-of-the-Senses-Generalizable-Manipulation-from-Vision-and-Touch-through-Masked-Multimodal-Learning" class="headerlink" title="The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning"></a>The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00924">http://arxiv.org/abs/2311.00924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carmelo Sferrazza, Younggyo Seo, Hao Liu, Youngwoon Lee, Pieter Abbeel</li>
<li>for: 本研究旨在开发一种可以结合视觉和感觉信息的多模态学习方法，以提高机器人 manipulate 物体的能力。</li>
<li>methods: 本研究提出了Masked Multimodal Learning（M3L）方法，它通过对视觉和感觉信息进行卷积自编码，同时学习策略和多模态表示。</li>
<li>results: 研究表明，通过在多模态 setting 学习，可以提高 sample efficiency 和泛化能力，并且vision-only 策略在测试时也受益于多模态学习。研究在三个 simulated 环境中进行了测试：机器人插入、门开合和灵活手部操作，结果表明了多模态策略的优势。<details>
<summary>Abstract</summary>
Humans rely on the synergy of their senses for most essential tasks. For tasks requiring object manipulation, we seamlessly and effectively exploit the complementarity of our senses of vision and touch. This paper draws inspiration from such capabilities and aims to find a systematic approach to fuse visual and tactile information in a reinforcement learning setting. We propose Masked Multimodal Learning (M3L), which jointly learns a policy and visual-tactile representations based on masked autoencoding. The representations jointly learned from vision and touch improve sample efficiency, and unlock generalization capabilities beyond those achievable through each of the senses separately. Remarkably, representations learned in a multimodal setting also benefit vision-only policies at test time. We evaluate M3L on three simulated environments with both visual and tactile observations: robotic insertion, door opening, and dexterous in-hand manipulation, demonstrating the benefits of learning a multimodal policy. Code and videos of the experiments are available at https://sferrazza.cc/m3l_site.
</details>
<details>
<summary>摘要</summary>
人类在日常任务中借靠感觉的协同才能完成大多数任务。在对物体操作任务中，我们会自然地和高效地利用视觉感和触觉感的相互补充。这篇论文从这些能力中得到灵感，旨在在强化学习设置下系统地融合视觉和触觉信息。我们提议的Masked Multimodal Learning（M3L）方法，同时学习策略和视觉和触觉的表示，基于遮盲自动编码。这些共同学习的表示，从视觉和触觉两种感知中各自提高样本效率，并在单独使用的感知方面也具有扩展的能力。更 remarkably，在多模态设置下学习的表示，也对视觉只的策略在测试时具有改善的效果。我们在三个 simulated 环境中进行了 inserting、开门和灵活的手部操作等三种任务的测试，证明了多模态策略的优势。代码和实验视频可以在 https://sferrazza.cc/m3l_site 上获取。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-Ethics-Education-in-Cybersecurity-Challenges-and-Opportunities-a-focus-group-report"><a href="#Artificial-Intelligence-Ethics-Education-in-Cybersecurity-Challenges-and-Opportunities-a-focus-group-report" class="headerlink" title="Artificial Intelligence Ethics Education in Cybersecurity: Challenges and Opportunities: a focus group report"></a>Artificial Intelligence Ethics Education in Cybersecurity: Challenges and Opportunities: a focus group report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00903">http://arxiv.org/abs/2311.00903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diane Jackson, Sorin Adam Matei, Elisa Bertino</li>
<li>for: 这篇论文的目的是探讨人工智能工具在网络安全领域中的应用和挑战。</li>
<li>methods: 论文使用了ocus组研讨方法，即与高水平的硬件学生进行面对面讨论，以了解在网络安全领域中人工智能工具的挑战和机遇。</li>
<li>results: 论文发现了在网络安全领域中人工智能工具的使用带来的挑战和机遇，包括访问开源或免费工具、文档、课程多样性和伦理原则的明确表述。  additionally, the study found that addressing the “black box” mentality in AI cybersecurity work and improving systems thinking and effective communication skills are crucial.<details>
<summary>Abstract</summary>
The emergence of AI tools in cybersecurity creates many opportunities and uncertainties. A focus group with advanced graduate students in cybersecurity revealed the potential depth and breadth of the challenges and opportunities. The salient issues are access to open source or free tools, documentation, curricular diversity, and clear articulation of ethical principles for AI cybersecurity education. Confronting the "black box" mentality in AI cybersecurity work is also of the greatest importance, doubled by deeper and prior education in foundational AI work. Systems thinking and effective communication were considered relevant areas of educational improvement. Future AI educators and practitioners need to address these issues by implementing rigorous technical training curricula, clear documentation, and frameworks for ethically monitoring AI combined with critical and system's thinking and communication skills.
</details>
<details>
<summary>摘要</summary>
人工智能在网络安全领域的出现带来了多种机遇和不确定性。一个关注组与高等研究生共同研究了人工智能在网络安全教育中的挑战和机遇。关键问题包括对开源或免费工具的访问、文档、课程多样性和伦理原则的明确表述。 Additionally, confronting the "black box" mentality in AI cybersecurity work is of great importance, and deeper and prior education in foundational AI work is also crucial. 系统思维和有效沟通技巧被认为是教育改进的重要领域。未来的AI教育和实践人员需要通过实施严格的技术训练课程、明确的文档和伦理监测框架来解决这些问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/02/cs.AI_2023_11_02/" data-id="closbrol7006l0g88coz584pd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/02/cs.CL_2023_11_02/" class="article-date">
  <time datetime="2023-11-02T11:00:00.000Z" itemprop="datePublished">2023-11-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/02/cs.CL_2023_11_02/">cs.CL - 2023-11-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TopicGPT-A-Prompt-based-Topic-Modeling-Framework"><a href="#TopicGPT-A-Prompt-based-Topic-Modeling-Framework" class="headerlink" title="TopicGPT: A Prompt-based Topic Modeling Framework"></a>TopicGPT: A Prompt-based Topic Modeling Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01449">http://arxiv.org/abs/2311.01449</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chtmp223/topicgpt">https://github.com/chtmp223/topicgpt</a></li>
<li>paper_authors: Chau Minh Pham, Alexander Hoyle, Simeng Sun, Mohit Iyyer</li>
<li>for: 用于探索文本集合中的 latent topics，并提供高质量和可读性的主题分类。</li>
<li>methods: 使用大型自然语言模型 (LLMs) 来揭示文本集合中的 latent topics，并使用提示来控制主题的 semantics。</li>
<li>results: 比基eline方法高的0.74的和� proprio的主题纯度，以及更加可读性的主题描述和自然语言标签。<details>
<summary>Abstract</summary>
Topic modeling is a well-established technique for exploring text corpora. Conventional topic models (e.g., LDA) represent topics as bags of words that often require "reading the tea leaves" to interpret; additionally, they offer users minimal semantic control over topics. To tackle these issues, we introduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to uncover latent topics within a provided text collection. TopicGPT produces topics that align better with human categorizations compared to competing methods: for example, it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for the strongest baseline. Its topics are also more interpretable, dispensing with ambiguous bags of words in favor of topics with natural language labels and associated free-form descriptions. Moreover, the framework is highly adaptable, allowing users to specify constraints and modify topics without the need for model retraining. TopicGPT can be further extended to hierarchical topical modeling, enabling users to explore topics at various levels of granularity. By streamlining access to high-quality and interpretable topics, TopicGPT represents a compelling, human-centered approach to topic modeling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Server-side-Rescoring-of-Spoken-Entity-centric-Knowledge-Queries-for-Virtual-Assistants"><a href="#Server-side-Rescoring-of-Spoken-Entity-centric-Knowledge-Queries-for-Virtual-Assistants" class="headerlink" title="Server-side Rescoring of Spoken Entity-centric Knowledge Queries for Virtual Assistants"></a>Server-side Rescoring of Spoken Entity-centric Knowledge Queries for Virtual Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01398">http://arxiv.org/abs/2311.01398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youyuan Zhang, Sashank Gondala, Thiago Fraga-Silva, Christophe Van Gysel</li>
<li>for: 这篇论文主要关注在处理语音识别领域中的处理问题，尤其是针对具有实体信息的查询。</li>
<li>methods: 本文使用了不同类型的语言模型（LM），包括N-gram字串LM和子字串神经LM，并考虑了在设备和服务器端的信号整合。</li>
<li>results: 本文的实验结果显示，通过在服务器端使用不同类型的LM，可以实现23%-35%的话语识别误差提升，并且模型融合多个服务器端的LM可以最有效地结合各模型的优点和对域特定数据的学习知识。<details>
<summary>Abstract</summary>
On-device Virtual Assistants (VAs) powered by Automatic Speech Recognition (ASR) require effective knowledge integration for the challenging entity-rich query recognition. In this paper, we conduct an empirical study of modeling strategies for server-side rescoring of spoken information domain queries using various categories of Language Models (LMs) (N-gram word LMs, sub-word neural LMs). We investigate the combination of on-device and server-side signals, and demonstrate significant WER improvements of 23%-35% on various entity-centric query subpopulations by integrating various server-side LMs compared to performing ASR on-device only. We also perform a comparison between LMs trained on domain data and a GPT-3 variant offered by OpenAI as a baseline. Furthermore, we also show that model fusion of multiple server-side LMs trained from scratch most effectively combines complementary strengths of each model and integrates knowledge learned from domain-specific data to a VA ASR system.
</details>
<details>
<summary>摘要</summary>
在设备上的虚拟助手（VAs）通过自动语音识别（ASR）需要有效的知识集成以处理复杂的实体rich查询。在这篇论文中，我们进行了实验室研究，使用不同类型的语言模型（LMs）（N-gram字符LMs、子字符神经LMs）来模型服务器端的重新评分语音信息域查询。我们研究了在设备和服务器端的信号组合，并证明了通过将不同类型的服务器端LMs集成到VA ASR系统中，可以实现23%-35%的话语识别错误率（WER）下降。此外，我们还进行了基于域数据进行LMs的训练和OpenAI提供的GPT-3变体作为基准。此外，我们还发现，通过将多个服务器端LMs从零开始训练并融合这些模型的 complementary strengths 可以最好地将域特定数据中学习的知识集成到VA ASR系统中。
</details></li>
</ul>
<hr>
<h2 id="Can-Language-Models-Be-Tricked-by-Language-Illusions-Easier-with-Syntax-Harder-with-Semantics"><a href="#Can-Language-Models-Be-Tricked-by-Language-Illusions-Easier-with-Syntax-Harder-with-Semantics" class="headerlink" title="Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics"></a>Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01386">http://arxiv.org/abs/2311.01386</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/forrestdavis/languageillusions">https://github.com/forrestdavis/languageillusions</a></li>
<li>paper_authors: Yuhan Zhang, Edward Gibson, Forrest Davis</li>
<li>for: 这个研究是为了检验语言模型（LM）是否可以模仿人类语言处理的行为。</li>
<li>methods: 研究使用了三种语言玄学（illusion）测试语言模型的能力：比较玄学（example：”更多人去过俄罗斯than I have”）、深度炸弹玄学（example：”没有头部伤害是过分的”）和负极性项（NPI）玄学（example：”不信任的猎人不会打熊”）。</li>
<li>results: 研究发现，LMs对NPI玄学的评估更容易与人类的判断相符，相比之下，对比玄学和深度炸弹玄学的评估更容易与人类的判断不符。 none of the LMs or metrics yielded results that were entirely consistent with human behavior。这些结果表明，LMs在语言处理方面的能力有限，并且不能够完全模仿人类的语言处理行为。<details>
<summary>Abstract</summary>
Language models (LMs) have been argued to overlap substantially with human beings in grammaticality judgment tasks. But when humans systematically make errors in language processing, should we expect LMs to behave like cognitive models of language and mimic human behavior? We answer this question by investigating LMs' more subtle judgments associated with "language illusions" -- sentences that are vague in meaning, implausible, or ungrammatical but receive unexpectedly high acceptability judgments by humans. We looked at three illusions: the comparative illusion (e.g. "More people have been to Russia than I have"), the depth-charge illusion (e.g. "No head injury is too trivial to be ignored"), and the negative polarity item (NPI) illusion (e.g. "The hunter who no villager believed to be trustworthy will ever shoot a bear"). We found that probabilities represented by LMs were more likely to align with human judgments of being "tricked" by the NPI illusion which examines a structural dependency, compared to the comparative and the depth-charge illusions which require sophisticated semantic understanding. No single LM or metric yielded results that are entirely consistent with human behavior. Ultimately, we show that LMs are limited both in their construal as cognitive models of human language processing and in their capacity to recognize nuanced but critical information in complicated language materials.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）已经被论证为与人类语言处理能力 overlap substantially 。然而，当人类系统atically makes errors in language processing 时，我们该预期LMs behaving like cognitive models of language and mimic human behavior? We answer this question by investigating LMs' more subtle judgments associated with "language illusions" -- sentences that are vague in meaning, implausible, or ungrammatical but receive unexpectedly high acceptability judgments by humans. We looked at three illusions: the comparative illusion (e.g. "More people have been to Russia than I have"), the depth-charge illusion (e.g. "No head injury is too trivial to be ignored"), and the negative polarity item (NPI) illusion (e.g. "The hunter who no villager believed to be trustworthy will ever shoot a bear"). We found that probabilities represented by LMs were more likely to align with human judgments of being "tricked" by the NPI illusion, which examines a structural dependency, compared to the comparative and the depth-charge illusions, which require sophisticated semantic understanding. No single LM or metric yielded results that are entirely consistent with human behavior. Ultimately, we show that LMs are limited both in their construal as cognitive models of human language processing and in their capacity to recognize nuanced but critical information in complicated language materials.
</details></li>
</ul>
<hr>
<h2 id="GPT-4V-ision-as-a-Generalist-Evaluator-for-Vision-Language-Tasks"><a href="#GPT-4V-ision-as-a-Generalist-Evaluator-for-Vision-Language-Tasks" class="headerlink" title="GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks"></a>GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01361">http://arxiv.org/abs/2311.01361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, Linda Ruth Petzold</li>
<li>for: 这篇论文旨在评估 GPT-4V 是否可以作为多媒体任务的自动评估器。</li>
<li>methods: 论文使用 GPT-4V 进行了多种多媒体任务的评估，包括图文生成、文本图生成、图像翻译和多图文对齐等。两种评估方法：单答评估和对比评估。</li>
<li>results: GPT-4V 在多种任务上与人类评估结果高度相似，表明 GPT-4V 可以作为多媒体任务的自动评估器。尽管有一些限制，如视觉清晰度评估和实际世界复杂的理解，但 GPT-4V 能够提供人类相似的分数以及详细的解释，表示它在多媒体 LLM 中具有潜力。<details>
<summary>Abstract</summary>
Automatically evaluating vision-language tasks is challenging, especially when it comes to reflecting human judgments due to limitations in accounting for fine-grained details. Although GPT-4V has shown promising results in various multi-modal tasks, leveraging GPT-4V as a generalist evaluator for these tasks has not yet been systematically explored. We comprehensively validate GPT-4V's capabilities for evaluation purposes, addressing tasks ranging from foundational image-to-text and text-to-image synthesis to high-level image-to-image translations and multi-images to text alignment. We employ two evaluation methods, single-answer grading and pairwise comparison, using GPT-4V. Notably, GPT-4V shows promising agreement with humans across various tasks and evaluation methods, demonstrating immense potential for multi-modal LLMs as evaluators. Despite limitations like restricted visual clarity grading and real-world complex reasoning, its ability to provide human-aligned scores enriched with detailed explanations is promising for universal automatic evaluator.
</details>
<details>
<summary>摘要</summary>
自动评估视觉语言任务是具有挑战性的，尤其是在准确地考虑细节方面有限制。虽然GPT-4V在多模态任务中表现出了可喜的结果，但是利用GPT-4V作为多模态评估器并没有得到系统性的探讨。我们全面验证GPT-4V的评估能力，涵盖图像到文本和文本到图像生成、高级图像到图像翻译以及多个图像到文本对齐等任务。我们采用了两种评估方法：单答题评估和对比评估，使用GPT-4V进行评估。值得注意的是，GPT-4V与人类的评估结果有良好的一致性，表现出了大量的可能性作为多模态LLM的评估器。尽管有限制的视觉清晰度评估和实际世界的复杂逻辑 reasoning 存在限制，但GPT-4V能够提供人类相似的分数，并且具有详细的解释，这是一个有前途的自动评估器。
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-Scaling-Retrieval-Augmentation-and-Form-on-the-Factual-Consistency-of-Language-Models"><a href="#The-Effect-of-Scaling-Retrieval-Augmentation-and-Form-on-the-Factual-Consistency-of-Language-Models" class="headerlink" title="The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models"></a>The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01307">http://arxiv.org/abs/2311.01307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lovisa Hagström, Denitsa Saynova, Tobias Norlund, Moa Johansson, Richard Johansson</li>
<li>for: 本研究旨在解释语言模型（LLM）在实际知识交互中的缺点，即它们在semantic equivalence的问题上具有不一致的答案问题。</li>
<li>methods: 本研究采用了两种缓解方法：升级和通过检索库补充语言模型（LM）。我们对LLaMA和Atlas模型进行了测试，并证明了这两种方法都能够减少不一致性，而检索补充方法更加高效。</li>
<li>results: 我们发现，不同的组件在Atlas模型中对一致性做出了不同的贡献。此外，我们还发现了不同的语言模型在不同的语言任务上 exhibit 不同的一致性问题。为所有评估过的语言模型而言，我们发现了语法形式和其他评估任务的假设 artifacts 对一致性具有影响。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might predict both "Anne Redpath passed away in Edinburgh." and "Anne Redpath's life ended in London." In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and other evaluation task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）可以作为自然界面来访问事实知识，但它们的用途受到它们对 semantically equivalent 问题的答案不一致的限制。例如，一个模型可能会预测 "安妮·雷普薇在Edinburgh去世" 和 "安妮·雷普薇的生命在London conclude"。在这项工作中，我们确定了可能导致不一致的原因，并评估了两种缓解策略：缩放和通过检索库补充语言模型。我们的结果表明，两种策略都可以减少不一致，而检索补充是远远更高效。我们进一步考虑了Atlas模型中的一致性贡献，并发现了不同组件的一致性贡献。对所有评估模型来说，我们发现了语法形式和其他评估任务的artifacts对一致性有影响。总之，我们的结果为语言模型的事实一致性提供了更好的理解。
</details></li>
</ul>
<hr>
<h2 id="FlashDecoding-Faster-Large-Language-Model-Inference-on-GPUs"><a href="#FlashDecoding-Faster-Large-Language-Model-Inference-on-GPUs" class="headerlink" title="FlashDecoding++: Faster Large Language Model Inference on GPUs"></a>FlashDecoding++: Faster Large Language Model Inference on GPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01282">http://arxiv.org/abs/2311.01282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong, Yu Wang</li>
<li>for: 提高 LLM 推理引擎的速度</li>
<li>methods: 使用异步 softmax 更新、双缓存 flat GEMM 优化、根据硬件资源进行智能数据流优化</li>
<li>results: 实现了up to 4.86x和2.18x的速度提升 compared to Hugging Face 实现，以及平均比state-of-the-art LLM 推理引擎提高1.37倍<details>
<summary>Abstract</summary>
As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and >50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.   We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++ creatively proposes: (1) Asynchronized softmax with unified max value. FlashDecoding++ introduces a unified max value technique for different partial softmax computations to avoid synchronization. (2) Flat GEMM optimization with double buffering. FlashDecoding++ points out that flat GEMMs with different shapes face varied bottlenecks. Then, techniques like double buffering are introduced. (3) Heuristic dataflow with hardware resource adaptation. FlashDecoding++ heuristically optimizes dataflow using different hardware resource considering input dynamics. Due to the versatility of optimizations in FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on both NVIDIA and AMD GPUs compared to Hugging Face implementations. FlashDecoding++ also achieves an average speedup of 1.37x compared to state-of-the-art LLM inference engines on mainstream LLMs.
</details>
<details>
<summary>摘要</summary>
As the Large Language Model (LLM) becomes increasingly important in various domains, there are still several challenges that need to be addressed in order to accelerate LLM inference. These challenges include:1. Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to approximately 20% overhead for attention computation in LLMs.2. Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and a performance loss of over 50% after padding zeros in previous designs.3. Performance loss due to static dataflow. The performance of LLM inference kernels depends on various factors such as input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.To address these challenges, we present FlashDecoding++, a fast LLM inference engine that supports mainstream LLMs and hardware back-ends. Our approach includes:1. Asynchronized softmax with unified max value. We introduce a unified max value technique for different partial softmax computations to avoid synchronization.2. Flat GEMM optimization with double buffering. We point out that flat GEMMs with different shapes face varied bottlenecks, and techniques like double buffering are introduced to optimize the computation.3. Heuristic dataflow with hardware resource adaptation. We heuristically optimize the dataflow using different hardware resources considering input dynamics.Thanks to the versatility of optimizations in FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on both NVIDIA and AMD GPUs compared to Hugging Face implementations. FlashDecoding++ also achieves an average speedup of 1.37x compared to state-of-the-art LLM inference engines on mainstream LLMs.
</details></li>
</ul>
<hr>
<h2 id="Finding-Common-Ground-Annotating-and-Predicting-Common-Ground-in-Spoken-Conversations"><a href="#Finding-Common-Ground-Annotating-and-Predicting-Common-Ground-in-Spoken-Conversations" class="headerlink" title="Finding Common Ground: Annotating and Predicting Common Ground in Spoken Conversations"></a>Finding Common Ground: Annotating and Predicting Common Ground in Spoken Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01273">http://arxiv.org/abs/2311.01273</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cogstates/2023-emnlp-common-ground">https://github.com/cogstates/2023-emnlp-common-ground</a></li>
<li>paper_authors: Magdalena Markowska, Mohammad Taghizadeh, Adil Soubki, Seyed Abolghasem Mirroshandel, Owen Rambow</li>
<li>for: This paper is written for researchers and scientists in the field of cognitive science and natural language processing.</li>
<li>methods: The paper introduces a new annotation and corpus to capture common ground, and describes initial experiments extracting propositions from dialog and tracking their status in the common ground from the perspective of each speaker.</li>
<li>results: The paper presents initial experiments extracting propositions from dialog and tracking their status in the common ground from the perspective of each speaker, with the goal of capturing common ground in natural language processing.<details>
<summary>Abstract</summary>
When we communicate with other humans, we do not simply generate a sequence of words. Rather, we use our cognitive state (beliefs, desires, intentions) and our model of the audience's cognitive state to create utterances that affect the audience's cognitive state in the intended manner. An important part of cognitive state is the common ground, which is the content the speaker believes, and the speaker believes the audience believes, and so on. While much attention has been paid to common ground in cognitive science, there has not been much work in natural language processing. In this paper, we introduce a new annotation and corpus to capture common ground. We then describe some initial experiments extracting propositions from dialog and tracking their status in the common ground from the perspective of each speaker.
</details>
<details>
<summary>摘要</summary>
当我们与其他人交流时，我们不仅是生成一个字串的序列。而是使用我们的认知状态（信念、愿望、意图）和我们对听众认知状态的模型来创造影响听众认知状态的语言表达。认知状态中的共同知识是speaker认为自己和听众认为自己相信的内容，以及这些内容在听众和speaker之间的共同认知。虽然认知科学中对共同知识进行了大量研究，但是自然语言处理领域中对其进行了 relativamente little research。在这篇论文中，我们介绍了一个新的注释和 корпуス来捕捉共同知识。然后，我们描述了一些初步的实验，从每个speaker的角度提取对话中的提案，并跟踪它们在共同知识中的状态。
</details></li>
</ul>
<hr>
<h2 id="People-Make-Better-Edits-Measuring-the-Efficacy-of-LLM-Generated-Counterfactually-Augmented-Data-for-Harmful-Language-Detection"><a href="#People-Make-Better-Edits-Measuring-the-Efficacy-of-LLM-Generated-Counterfactually-Augmented-Data-for-Harmful-Language-Detection" class="headerlink" title="People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection"></a>People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01270">http://arxiv.org/abs/2311.01270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Indira Sen, Dennis Assenmacher, Mattia Samory, Isabelle Augenstein, Wil van der Aalst, Claudia Wagne</li>
<li>For: The paper aims to improve the robustness of NLP models to spurious features by automating the process of generating Counterfactually Augmented Data (CADs).* Methods: The authors use three generative NLP models - Polyjuice, ChatGPT, and Flan-T5 - to automatically generate CADs, and evaluate their effectiveness in improving model robustness compared to manually-generated CADs.* Results: The authors find that while manually-generated CADs are still the most effective, CADs generated by ChatGPT come a close second. However, the changes introduced by the automated methods are often insufficient to flip the original label, which limits their performance.<details>
<summary>Abstract</summary>
NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most effective, CADs generated by ChatGPT come a close second. One key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Study-of-Continual-Learning-Under-Language-Shift"><a href="#A-Study-of-Continual-Learning-Under-Language-Shift" class="headerlink" title="A Study of Continual Learning Under Language Shift"></a>A Study of Continual Learning Under Language Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01200">http://arxiv.org/abs/2311.01200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evangelia Gogoulou, Timothée Lesort, Magnus Boman, Joakim Nivre</li>
<li>for: 本研究探讨了在新语言数据可用时更新语言模型的benefits和缺点，特别是在语言shift情况下的持续学习。</li>
<li>methods: 研究人员从英语单语言模型开始，逐步添加挪威语和冰岛语数据，研究如何在不同的语言顺序和模型大小下实现转移学习的效果。</li>
<li>results: 研究结果表明，前向传递效果几乎是无关语言顺序的，而后向传递效果则可能受到语言顺序和语言特征的影响，并且与不同的学习率规则相关。<details>
<summary>Abstract</summary>
The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metrics and find that syntactic similarity appears to have the best correlation with our results.
</details>
<details>
<summary>摘要</summary>
We begin with a monolingual English language model and incrementally add data from Norwegian and Icelandic to examine how forward and backward transfer effects depend on the pre-training order and characteristics of languages, as well as different model sizes and learning rate schedulers. Our findings show that forward transfer is generally positive and independent of language order, while backward transfer can be either positive or negative, depending on the order and characteristics of the new languages.To understand these patterns, we investigate several language similarity metrics and find that syntactic similarity is the most strongly correlated with our results. Our study provides valuable insights into the benefits and challenges of continual learning under language shift, and highlights the importance of considering language similarity when updating a language model with new data.
</details></li>
</ul>
<hr>
<h2 id="CRUSH4SQL-Collective-Retrieval-Using-Schema-Hallucination-For-Text2SQL"><a href="#CRUSH4SQL-Collective-Retrieval-Using-Schema-Hallucination-For-Text2SQL" class="headerlink" title="CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL"></a>CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01173">http://arxiv.org/abs/2311.01173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayank Kothyari, Dhruva Dhingra, Sunita Sarawagi, Soumen Chakrabarti</li>
<li>for: 本研究旨在提高大规模数据库中的文本到SQL生成器效率，并且可以在不需要整个schema的情况下进行文本到SQL生成。</li>
<li>methods: 本研究提出了一种两stage的方法，首先使用LLM hallucinate一个最小的数据库 schema，然后使用这个hallucinated schema进行多个dense retrieval来选择实际schema中的一个子集。</li>
<li>results: 研究发现，使用hallucination可以提高文本到SQL生成器的准确率，并且与现有的State-of-the-Art retrieval-based augmentation方法相比，本方法可以获得更高的回归率。<details>
<summary>Abstract</summary>
Existing Text-to-SQL generators require the entire schema to be encoded with the user text. This is expensive or impractical for large databases with tens of thousands of columns. Standard dense retrieval techniques are inadequate for schema subsetting of a large structured database, where the correct semantics of retrieval demands that we rank sets of schema elements rather than individual elements. In response, we propose a two-stage process for effective coverage during retrieval. First, we instruct an LLM to hallucinate a minimal DB schema deemed adequate to answer the query. We use the hallucinated schema to retrieve a subset of the actual schema, by composing the results from multiple dense retrievals. Remarkably, hallucination $\unicode{x2013}$ generally considered a nuisance $\unicode{x2013}$ turns out to be actually useful as a bridging mechanism. Since no existing benchmarks exist for schema subsetting on large databases, we introduce three benchmarks. Two semi-synthetic datasets are derived from the union of schemas in two well-known datasets, SPIDER and BIRD, resulting in 4502 and 798 schema elements respectively. A real-life benchmark called SocialDB is sourced from an actual large data warehouse comprising 17844 schema elements. We show that our method1 leads to significantly higher recall than SOTA retrieval-based augmentation methods.
</details>
<details>
<summary>摘要</summary>
现有的文本到SQL生成器需要整个 schema 被编码到用户文本中。这是costly或实际不切实际的 для大型数据库，其中包含 tens of thousands 的列。标准稠密检索技术无法对大型结构化数据库进行schemasubsetting，因为正确的 semantics of retrieval 需要我们将set of schema elements 排序，而不是单个元素。因此，我们提出了一个两stage的过程，以实现有效的覆盖。首先，我们请求 LLM 生成一个最小的DB schema，可以回答查询。我们使用生成的 schema 来 retrieve 一个实际 schema 的子集，通过多个稠密检索的结果进行组合。 Surprisingly，hallucination  $\unicode{x2013}$ ，一直被视为幻觉 $\unicode{x2013}$ ，实际上是一种有用的桥接机制。由于现有的 benchmark 不存在于大型数据库上的schema subsetting，我们引入了三个 benchmark。这三个 benchmark 包括两个 semi-synthetic 数据集，它们来自 SPIDER 和 BIRD 两个well-known数据集，共计4502 和 798 的 schema element。此外，我们还引入了一个实际的大数据库，即 SocialDB，它包含 17844 的 schema element。我们表明，我们的方法1 可以达到 significantly higher recall than SOTA retrieval-based augmentation methods。
</details></li>
</ul>
<hr>
<h2 id="ACES-Translation-Accuracy-Challenge-Sets-at-WMT-2023"><a href="#ACES-Translation-Accuracy-Challenge-Sets-at-WMT-2023" class="headerlink" title="ACES: Translation Accuracy Challenge Sets at WMT 2023"></a>ACES: Translation Accuracy Challenge Sets at WMT 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01153">http://arxiv.org/abs/2311.01153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chantal Amrhein, Nikita Moghe, Liane Guillou</li>
<li>for: 本研究使用ACES挑战集(Amrhein et al., 2022)进行了对 segmentlevel  метри克的性能评估，以便为 WMT 2023 提供评估metric。</li>
<li>methods: 研究使用了68种现象和146种语言对的36,000个示例进行了评估，并为每个 метри克提供了错误类型的细化分布图表以及一个总的ACES-Score，以便快速比较。此外，研究还测试了2023和2022年度metric的增量性能。</li>
<li>results: 研究发现，1）没有明确的赢家 Among the metrics submitted to WMT 2023，2）2023和2022年度metric的性能变化很大。研究建议， metric developer should focus on：建立多家 metric ensemble，开发更注重源语言和避免surface-level overlap的 metric，以及仔细确定多语言嵌入的影响于MT评估。<details>
<summary>Abstract</summary>
We benchmark the performance of segmentlevel metrics submitted to WMT 2023 using the ACES Challenge Set (Amrhein et al., 2022). The challenge set consists of 36K examples representing challenges from 68 phenomena and covering 146 language pairs. The phenomena range from simple perturbations at the word/character level to more complex errors based on discourse and real-world knowledge. For each metric, we provide a detailed profile of performance over a range of error categories as well as an overall ACES-Score for quick comparison. We also measure the incremental performance of the metrics submitted to both WMT 2023 and 2022. We find that 1) there is no clear winner among the metrics submitted to WMT 2023, and 2) performance change between the 2023 and 2022 versions of the metrics is highly variable. Our recommendations are similar to those from WMT 2022. Metric developers should focus on: building ensembles of metrics from different design families, developing metrics that pay more attention to the source and rely less on surface-level overlap, and carefully determining the influence of multilingual embeddings on MT evaluation.
</details>
<details>
<summary>摘要</summary>
我们对WTM 2023中提交的segmentlevel metric进行了性能测试，使用ACES Challenge Set（Amrhein et al., 2022）。这个挑战集包含36K个例子，表示68种现象和146种语言对。这些现象包括单个字/字符级别的简单扰乱到更加复杂的错误基于话语和实际知识。对每个指标，我们提供了错误类别的详细分布图以及一个总的ACES-Score，方便比较。我们还测量了2023和2022两年度metric submission中的增量性能。我们发现：1）WTM 2023中提交的指标没有一个明确的赢家，2）2023和2022两年度metric submission中的性能变化很大。我们的建议与WTM 2022相似：指标开发者应该：1）建立不同设计家族的metric ensemble，2）开发更加关注源语和 superficialevel overlap的metric，3）仔细确定多语言嵌入影响MT评估。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Question-Answering-Performance-of-Large-Language-Models-through-Semantic-Consistency"><a href="#Predicting-Question-Answering-Performance-of-Large-Language-Models-through-Semantic-Consistency" class="headerlink" title="Predicting Question-Answering Performance of Large Language Models through Semantic Consistency"></a>Predicting Question-Answering Performance of Large Language Models through Semantic Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01152">http://arxiv.org/abs/2311.01152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ella Rabinovich, Samuel Ackerman, Orna Raz, Eitan Farchi, Ateret Anaby-Tavor</li>
<li>for: 本研究旨在评估当代大语言模型（LLM）的问答（QA） semantic consistency，通过手动创建高质量重句替换的底库，并将其发布给社区。</li>
<li>methods: 本研究使用的方法包括创建底库，以及与先前的工作相关的其他测量方法，用于评估 LLM QA 性能。</li>
<li>results: 研究结果表明，与基elines比较，本 frameworks 能够显著地提高 LLM 的问答性能， demonstrating encouraging results.<details>
<summary>Abstract</summary>
Semantic consistency of a language model is broadly defined as the model's ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.   We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction -- predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.
</details>
<details>
<summary>摘要</summary>
Semantic consistency of a language model is broadly defined as the model's ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.  We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction -- predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.Here's the translation in Traditional Chinese as well:Semantic consistency of a language model is broadly defined as the model's ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.  We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction -- predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.
</details></li>
</ul>
<hr>
<h2 id="Chinesewebtext-Large-scale-high-quality-Chinese-web-text-extracted-with-effective-evaluation-model"><a href="#Chinesewebtext-Large-scale-high-quality-Chinese-web-text-extracted-with-effective-evaluation-model" class="headerlink" title="Chinesewebtext: Large-scale high-quality Chinese web text extracted with effective evaluation model"></a>Chinesewebtext: Large-scale high-quality Chinese web text extracted with effective evaluation model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01149">http://arxiv.org/abs/2311.01149</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/casia-lm/chinesewebtext">https://github.com/casia-lm/chinesewebtext</a></li>
<li>paper_authors: Jianghao Chen, Pu Jian, Tengxiao Xi, Yidong Yi, Chenglin Ding, Qianlong Du, Guibo Zhu, Chengqing Zong, Jinqiao Wang, Jiajun Zhang</li>
<li>for: 提高中文大型语言模型（LLM）的研究，需要大量和高质量的预训练数据。现有的大规模数据集主要集中在英文上，中文数据缺乏完整的工具链和高质量的预训练数据。</li>
<li>methods: 我们提出了一个新的完整工具链EvalWeb，可以从不 cleaner web 数据中提取高质量的中文文本。我们使用手动编写的规则排除Raw crawled web 内容中的直接的噪音文本。然后，我们使用一种高效的评估模型，对剩下的相对清洁数据进行评分，并将每个文本分配一个特定的质量分数。</li>
<li>results: 我们采用EvalWeb工具链，从不 cleaner web 数据中提取了1.42 TB的高质量中文文本，每个文本都有一个质量分数。此外，我们还释放了600 GB的更加干净的中文数据，其质量超过90%。<details>
<summary>Abstract</summary>
During the development of large language models (LLMs), the scale and quality of the pre-training data play a crucial role in shaping LLMs' capabilities. To accelerate the research of LLMs, several large-scale datasets, such as C4 [1], Pile [2], RefinedWeb [3] and WanJuan [4], have been released to the public. However, most of the released corpus focus mainly on English, and there is still lack of complete tool-chain for extracting clean texts from web data. Furthermore, fine-grained information of the corpus, e.g. the quality of each text, is missing. To address these challenges, we propose in this paper a new complete tool-chain EvalWeb to extract Chinese clean texts from noisy web data. First, similar to previous work, manually crafted rules are employed to discard explicit noisy texts from the raw crawled web contents. Second, a well-designed evaluation model is leveraged to assess the remaining relatively clean data, and each text is assigned a specific quality score. Finally, we can easily utilize an appropriate threshold to select the high-quality pre-training data for Chinese. Using our proposed approach, we release the largest and latest large-scale high-quality Chinese web text ChineseWebText, which consists of 1.42 TB and each text is associated with a quality score, facilitating the LLM researchers to choose the data according to the desired quality thresholds. We also release a much cleaner subset of 600 GB Chinese data with the quality exceeding 90%.
</details>
<details>
<summary>摘要</summary>
在大语言模型（LLM）的研发过程中，预训练数据的规模和质量对LML的能力产生关键作用。为加速LLM研究，许多大规模数据集，如C4 [1]、Pile [2]、RefinedWeb [3]和WanJuan [4]，已经公开发布给广大科学家。然而，大多数发布的 corpus 都主要关注英语，中文 corpus 缺乏完整的工具链，并且每个文本的质量信息缺失。为解决这些挑战，我们在本纸提出一种新的完整工具链 EvalWeb，用于从不 cleaner 的网络数据中提取中文高质量文本。首先，与前一工作类似，我们手动制定规则来排除 raw 爬取网络内容中的直接不良文本。其次，我们设计了一种可以评估剩下的相对清洁数据的评价模型，每个文本都分配了特定的质量分数。最后，我们可以轻松地使用适当的阈值来选择中文高质量预训练数据。使用我们的提议方法，我们发布了1.42 TB 的最新和最大规模高质量中文网络文本 ChineseWebText，每个文本都有质量分数，以便 LLM 研究人员根据需要的质量阈值来选择数据。此外，我们还发布了90%以上的更加清洁的600 GB 中文数据。
</details></li>
</ul>
<hr>
<h2 id="Noise-Robust-Fine-Tuning-of-Pretrained-Language-Models-via-External-Guidance"><a href="#Noise-Robust-Fine-Tuning-of-Pretrained-Language-Models-via-External-Guidance" class="headerlink" title="Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance"></a>Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01108">http://arxiv.org/abs/2311.01108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Wang, Zhen Tan, Ruocheng Guo, Jundong Li</li>
<li>for: 这篇论文目的是为了提高预训练语言模型（PLM）在自然语言处理领域中的性能，并且解决实际应用中的数据标签噪音问题。</li>
<li>methods: 本文提出了一个创新的PLM fine-tuning策略，利用噪音标签和大型语言模型（LLM） like ChatGPT 的指导，帮助精确地分辨清洁和噪音标签的标签，并提供了额外的信息，以推动PLM的学习过程中。</li>
<li>results: 实验结果显示，本文的框架在Synthetic和实际的噪音标签 datasets 上具有明显的优势，较前一些基eline的性能。<details>
<summary>Abstract</summary>
Adopting a two-stage paradigm of pretraining followed by fine-tuning, Pretrained Language Models (PLMs) have achieved substantial advancements in the field of natural language processing. However, in real-world scenarios, data labels are often noisy due to the complex annotation process, making it essential to develop strategies for fine-tuning PLMs with such noisy labels. To this end, we introduce an innovative approach for fine-tuning PLMs using noisy labels, which incorporates the guidance of Large Language Models (LLMs) like ChatGPT. This guidance assists in accurately distinguishing between clean and noisy samples and provides supplementary information beyond the noisy labels, thereby boosting the learning process during fine-tuning PLMs. Extensive experiments on synthetic and real-world noisy datasets further demonstrate the superior advantages of our framework over the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
使用两阶段模型的预训练和精度调整方法，预训练语言模型（PLM）在自然语言处理领域已经实现了重要进步。然而，在实际场景中，数据标签往往受到复杂的注释过程影响，导致标签上的噪声，因此需要开发适应这类噪声标签的精度调整策略。为此，我们提出了一种新的精度调整策略，利用大型语言模型（LLM）如ChatGPT的指导，以帮助在精度调整PLM时更加准确地分辨清洁和噪声样本，并提供额外的信息，以便在精度调整过程中增强学习。我们在 sintetic和实际噪声数据集上进行了广泛的实验，并证明了我们的框架在状态静态基准上具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="DistilWhisper-Efficient-Distillation-of-Multi-task-Speech-Models-via-Language-Specific-Experts"><a href="#DistilWhisper-Efficient-Distillation-of-Multi-task-Speech-Models-via-Language-Specific-Experts" class="headerlink" title="DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts"></a>DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01070">http://arxiv.org/abs/2311.01070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Palmeira Ferraz, Marcely Zanon Boito, Caroline Brun, Vassilina Nikoulina</li>
<li>for: 提高少数语言自动语音识别（ASR）性能</li>
<li>methods: 使用轻量级模块化精度调教和知识填充法</li>
<li>results: 比标准精度调教或LoRA适应器更有效，提高目标语言ASR性能，并增加仅一个微 Parameters overhead during inference.<details>
<summary>Abstract</summary>
Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still under-performs on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we propose DistilWhisper, an approach able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both in- and out-of-domain test sets, while introducing only a negligible parameter overhead at inference.
</details>
<details>
<summary>摘要</summary>
喷零是一个多任务多语言的语音模型，覆盖99种语言。它在一部分覆盖语言上表现出色，但模型仍然在一些被束缚语言上表现不佳，这问题在较小的模型版本中变得更加严重。在这项工作中，我们提出了DistilWhisper这种方法，能够bridging ASR表现差距在这些语言上，同时保留多任务多语言预训练的优点。我们的方法包括两个关键策略：使用语言特定专家进行轻量级模块ASR精度调整，以及从whisper-large-v2中进行知识填充。这种双重方法allow我们可以很好地提高ASR表现，保留多任务多语言预训练中的稳定性。结果表明，我们的方法比标准精度调整或LoRA扩展器更有效，在目标语言上对 both in-和out-of-domain测试集中提高表现，而且只增加了一个微 parameter overhead在推理过程中。
</details></li>
</ul>
<hr>
<h2 id="COPAL-ID-Indonesian-Language-Reasoning-with-Local-Culture-and-Nuances"><a href="#COPAL-ID-Indonesian-Language-Reasoning-with-Local-Culture-and-Nuances" class="headerlink" title="COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances"></a>COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01012">http://arxiv.org/abs/2311.01012</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haryoa/copal-id">https://github.com/haryoa/copal-id</a></li>
<li>paper_authors: Haryo Akbarianto Wibowo, Erland Hilman Fuadi, Made Nindyatama Nityasya, Radityo Eko Prasojo, Alham Fikri Aji</li>
<li>for: 这个论文是为了描述一个新的印度尼西亚语言常识逻辑数据集（COPAL-ID）。</li>
<li>methods: 这个数据集是由本地native写作，具有更加自然的日常 causal 逻辑，而不同于之前的印度尼西亚COPA数据集（XCOPA-ID）。</li>
<li>results: 研究发现，even the current best open-source, multilingual model struggles to perform well on COPAL-ID，其中的accuracy是65.47%， significanly lower than XCOPA-ID（79.40%）。这显示这些语言模型仍然很难理解印度尼西亚的地方特点。<details>
<summary>Abstract</summary>
We present publicly available COPAL-ID, a novel Indonesian language common sense reasoning dataset. Unlike the previous Indonesian COPA dataset (XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and therefore, provides a more natural portrayal of day-to-day causal reasoning within the Indonesian cultural sphere. Professionally written by natives from scratch, COPAL-ID is more fluent and free from awkward phrases, unlike the translated XCOPA-ID. In addition, we present COPAL-ID in both standard Indonesian and in Jakartan Indonesian--a dialect commonly used in daily conversation. COPAL-ID poses a greater challenge for existing open-sourced and closed state-of-the-art multilingual language models, yet is trivially easy for humans. Our findings suggest that even the current best open-source, multilingual model struggles to perform well, achieving 65.47% accuracy on COPAL-ID, significantly lower than on the culturally-devoid XCOPA-ID (79.40%). Despite GPT-4's impressive score, it suffers the same performance degradation compared to its XCOPA-ID score, and it still falls short of human performance. This shows that these language models are still way behind in comprehending the local nuances of Indonesian.
</details>
<details>
<summary>摘要</summary>
我们公开提供了一个新的印度尼西亚语言常识理解数据集，称为COPAL-ID。与前一代印度尼西亚COPA数据集（XCOPA-ID）不同，COPAL-ID包含了印度尼西亚当地的本地文化特点，因此更能呈现日常生活中印度尼西亚文化圈中的 causal reasoning 的自然场景。由本地Native编写，COPAL-ID更加流畅，无awkward phrase，与XCOPA-ID不同。此外，我们还提供了COPAL-ID的标准印度尼西亚和雅加达印度尼西亚语言版本--一种日常交流中广泛使用的方言。COPAL-ID对现有的开源和关闭式状态艺术语言模型提出了更大的挑战，然而对人类来说是极其容易的。我们的发现表明， même GPT-4 在COPAL-ID 上表现出色，但它仍然落后于人类表现，只有65.47%的准确率，与 XCOPA-ID 的79.40%的准确率相比有所下降。这显示了这些语言模型仍然远远落后于理解印度尼西亚本地特点。
</details></li>
</ul>
<hr>
<h2 id="Blending-Reward-Functions-via-Few-Expert-Demonstrations-for-Faithful-and-Accurate-Knowledge-Grounded-Dialogue-Generation"><a href="#Blending-Reward-Functions-via-Few-Expert-Demonstrations-for-Faithful-and-Accurate-Knowledge-Grounded-Dialogue-Generation" class="headerlink" title="Blending Reward Functions via Few Expert Demonstrations for Faithful and Accurate Knowledge-Grounded Dialogue Generation"></a>Blending Reward Functions via Few Expert Demonstrations for Faithful and Accurate Knowledge-Grounded Dialogue Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00953">http://arxiv.org/abs/2311.00953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyu Du, Yangfeng Ji<br>for: 实现对话式资讯搜寻系统的可靠性和准确性，需要对话模型能够基于相关知识文本生成 faithful和 precisel 的回答。methods: 我们使用了增强学习算法，创建了一个新的赏罚函数，让模型能够在不可靠的知识文本中快速地学习并生成高质量的回答。results: 我们的方法在两个对话式资讯搜寻 datasets 上进行了实验，结果显示我们的方法可以与其他强制学习基于标签的基底搜寻方法竞争。<details>
<summary>Abstract</summary>
The development of trustworthy conversational information-seeking systems relies on dialogue models that can generate faithful and accurate responses based on relevant knowledge texts. However, two main challenges hinder this task. Firstly, language models may generate hallucinations due to data biases present in their pretraining corpus. Secondly, knowledge texts often contain redundant and irrelevant information that distracts the model's attention from the relevant text span. Previous works use additional data annotations on the knowledge texts to learn a knowledge identification module in order to bypass irrelevant information, but collecting such high-quality span annotations can be costly. In this work, we leverage reinforcement learning algorithms to overcome the above challenges by introducing a novel reward function. Our reward function combines an accuracy metric and a faithfulness metric to provide a balanced quality judgment of generated responses, which can be used as a cost-effective approximation to a human preference reward model when only a few preference annotations are available. Empirical experiments on two conversational information-seeking datasets demonstrate that our method can compete with other strong supervised learning baselines.
</details>
<details>
<summary>摘要</summary>
发展可靠的对话信息搜索系统需要对话模型可以基于相关知识文本生成准确和 faithful的回答。然而，两大挑战妨碍这项任务。首先，语言模型可能会生成幻觉，因为它们的预训练集中存在数据偏见。其次，知识文本经常包含无关和不必要的信息，这会让模型的注意力偏离重要的文本段落。在这种情况下，先前的工作是通过添加知识文本上的数据标注来学习知识标识模块，以避免无关信息的影响。然而，收集这些高质量的标注数据可以是昂贵的。在这种情况下，我们利用了强化学习算法，通过引入一个新的奖励函数来解决以上挑战。我们的奖励函数组合了一个准确度指标和一个忠实度指标，以提供一个平衡的质量评价标准，可以用作只有几个偏好标注available的情况下的cost-effective approximation。我们的实验结果表明，我们的方法可以与其他强制学习基elines相比竞争。
</details></li>
</ul>
<hr>
<h2 id="E3-TTS-Easy-End-to-End-Diffusion-based-Text-to-Speech"><a href="#E3-TTS-Easy-End-to-End-Diffusion-based-Text-to-Speech" class="headerlink" title="E3 TTS: Easy End-to-End Diffusion-based Text to Speech"></a>E3 TTS: Easy End-to-End Diffusion-based Text to Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00945">http://arxiv.org/abs/2311.00945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Gao, Nobuyuki Morioka, Yu Zhang, Nanxin Chen</li>
<li>for: 这种研究的目的是开发一种简单、高效的端到端文本译 speech模型，该模型基于扩散过程来生成高质量的声音波形。</li>
<li>methods: 这种模型使用扩散过程来模拟声音波形的时间结构，而不需要任何中间表示或对齐信息。</li>
<li>results: 实验表明，这种模型可以生成高 fideltity 的声音，与当前最佳 neural TTS 系统的性能相似。声音样本可以在 <a target="_blank" rel="noopener" href="https://e3tts.github.io/">https://e3tts.github.io</a> 中找到。<details>
<summary>Abstract</summary>
We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.
</details>
<details>
<summary>摘要</summary>
我们提出了易用的端到端扩散基于文本到语音模型E3 TTS，这是一种简单、高效的端到端文本到语音模型。E3 TTS直接从普通文本输入中获取 Audio waveform ，通过迭代精化过程来生成。与许多先前的工作不同，E3 TTS不依赖于任何中间表示，如spectrogram特征或投入信息。相反，E3 TTS通过扩散过程来模型波形的时间结构。不需要额外的条件信息，E3 TTS可以轻松地适应零扩展任务，如编辑，无需进行额外的训练。实验表明，E3 TTS可以生成高品质的Audio，与当前的 neural TTS 系统性能相似。Audio 样本可以在https://e3tts.github.io 找到。
</details></li>
</ul>
<hr>
<h2 id="Task-Agnostic-Low-Rank-Adapters-for-Unseen-English-Dialects"><a href="#Task-Agnostic-Low-Rank-Adapters-for-Unseen-English-Dialects" class="headerlink" title="Task-Agnostic Low-Rank Adapters for Unseen English Dialects"></a>Task-Agnostic Low-Rank Adapters for Unseen English Dialects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00915">http://arxiv.org/abs/2311.00915</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zedian/hyperlora">https://github.com/zedian/hyperlora</a></li>
<li>paper_authors: Zedian Xiao, William Held, Yanchen Liu, Diyi Yang</li>
<li>for: 这个研究旨在对英文口语方言的多样性进行融合，以便实现语言科技的普遍化。</li>
<li>methods: 这个研究使用了专家语言知识来构建HyperLoRA方法，它可以对不同口语方言进行资源有效的适应。</li>
<li>results: HyperLoRA方法在5种口语方言的零条件设定下实现了最好或最竞争的性能，并且比传统方法更加数据效率。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies. In practice, these speakers often accommodate their speech to be better understood. Our work shares the belief that language technologies should be designed to accommodate the diversity in English dialects and not the other way around. However, prior works on dialect struggle with generalizing to evolving and emerging dialects in a scalable manner. To fill this gap, our method, HyperLoRA, leverages expert linguistic knowledge to enable resource-efficient adaptation via hypernetworks. By disentangling dialect-specific and cross-dialectal information, HyperLoRA improves generalization to unseen dialects in a task-agnostic fashion. Not only is HyperLoRA more scalable in the number of parameters, but it also achieves the best or most competitive performance across 5 dialects in a zero-shot setting. In this way, our approach facilitates access to language technology for billions of English dialect speakers who are traditionally underrepresented.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Self-Influence-Guided-Data-Reweighting-for-Language-Model-Pre-training"><a href="#Self-Influence-Guided-Data-Reweighting-for-Language-Model-Pre-training" class="headerlink" title="Self-Influence Guided Data Reweighting for Language Model Pre-training"></a>Self-Influence Guided Data Reweighting for Language Model Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00913">http://arxiv.org/abs/2311.00913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megh Thakkar, Tolga Bolukbasi, Sriram Ganapathy, Shikhar Vashishth, Sarath Chandar, Partha Talukdar</li>
<li>for: 这篇研究旨在提出一种样本重量化方法，以提高预训练语言模型（LM）的质量和稳定性。</li>
<li>methods: 该方法基于自我影响（SI）分数，用于重量化预训练数据中的样本。</li>
<li>results: 经过广泛的分析， authors 发现 PRESENCE 可以提高模型的新鲜度和稳定性，并且适用于不同的模型大小、数据集和任务。<details>
<summary>Abstract</summary>
Language Models (LMs) pre-trained with self-supervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pre-training data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in the research direction of sample reweighting for pre-training language models.
</details>
<details>
<summary>摘要</summary>
Language Models (LMs) 预先训练于大量文本资料上已成为开发不同NLPTask的默认开始点。在LM预训练中，所有数据样本均受到相同的重要性对待。然而，由于数据样本的相关性和质量可能存在差异，那么对所有数据样本都受到相同的重要性可能并不是最佳选择。而数据重新分配已在任务特定的超vised学习和LM精度调整中被探索，但模型驱动的数据重新分配 для预训练数据尚未被探索。我们填补了这一重要的空白，并提出了PRESENCE，一种通过自我影响（SI）分数作为样本重要性指标来重新分配样本的方法。PRESENCE提高了模型预训练的新鲜性和稳定性。通过多个模型大小、数据集和任务的广泛分析，我们展示了PRESENCE作为预训练语言模型样本重新分配的重要首步。
</details></li>
</ul>
<hr>
<h2 id="Re-weighting-Tokens-A-Simple-and-Effective-Active-Learning-Strategy-for-Named-Entity-Recognition"><a href="#Re-weighting-Tokens-A-Simple-and-Effective-Active-Learning-Strategy-for-Named-Entity-Recognition" class="headerlink" title="Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition"></a>Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00906">http://arxiv.org/abs/2311.00906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haocheng Luo, Wei Tan, Ngoc Dang Nguyen, Lan Du</li>
<li>for: 提高Named Entity Recognition（NER）模型的效果，这是因为数据不均衡的问题，使得活跃学习效果不佳。</li>
<li>methods: 使用重新定Weighting策略，将个别字符的 weights给动态调整，这个策略可以与不同的字符获取函数相容。</li>
<li>results: 实验结果显示，将重新定Weighting策略与现有的获取函数搭配使用，可以实现大幅提高NER模型的性能。<details>
<summary>Abstract</summary>
Active learning, a widely adopted technique for enhancing machine learning models in text and image classification tasks with limited annotation resources, has received relatively little attention in the domain of Named Entity Recognition (NER). The challenge of data imbalance in NER has hindered the effectiveness of active learning, as sequence labellers lack sufficient learning signals. To address these challenges, this paper presents a novel reweighting-based active learning strategy that assigns dynamic smoothed weights to individual tokens. This adaptable strategy is compatible with various token-level acquisition functions and contributes to the development of robust active learners. Experimental results on multiple corpora demonstrate the substantial performance improvement achieved by incorporating our re-weighting strategy into existing acquisition functions, validating its practical efficacy.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:活动学习，一种广泛采用的技术以增强机器学习模型在文本和图像分类任务中，尤其是在有限的标注资源的情况下，受到了NER领域的Relative little attention。NER数据不均衡的挑战使得活动学习效果受到了限制，因为序列标签器缺乏足够的学习信号。为Address these challenges, this paper presents a novel reweighting-based active learning strategy that assigns dynamic smoothed weights to individual tokens. This adaptable strategy is compatible with various token-level acquisition functions and contributes to the development of robust active learners. Experimental results on multiple corpora demonstrate the significant performance improvement achieved by incorporating our re-weighting strategy into existing acquisition functions, validating its practical effectiveness.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/02/cs.CL_2023_11_02/" data-id="closbroni00dt0g88hcyva695" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/02/cs.LG_2023_11_02/" class="article-date">
  <time datetime="2023-11-02T10:00:00.000Z" itemprop="datePublished">2023-11-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/02/cs.LG_2023_11_02/">cs.LG - 2023-11-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="PPI-Efficient-Prediction-Powered-Inference"><a href="#PPI-Efficient-Prediction-Powered-Inference" class="headerlink" title="PPI++: Efficient Prediction-Powered Inference"></a>PPI++: Efficient Prediction-Powered Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01453">http://arxiv.org/abs/2311.01453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aangelopoulos/ppi_py">https://github.com/aangelopoulos/ppi_py</a></li>
<li>paper_authors: Anastasios N. Angelopoulos, John C. Duchi, Tijana Zrnic</li>
<li>for: 这个论文是为了提出一种 computationally lightweight 的方法来进行估计和推理，使用小量标注数据和大量机器学习预测结果。</li>
<li>methods: 这个方法使用 prediction-powered inference (PPI) 方法，通过自动适应可用预测的质量，计算出高效的自信量集，用于估计参数的任意维度。</li>
<li>results: 实验和 sintetic experiments 表明，提出的修改可以提高计算和统计效率，并且在不同的预测质量下都能够得到更好的结果。<details>
<summary>Abstract</summary>
We present PPI++: a computationally lightweight methodology for estimation and inference based on a small labeled dataset and a typically much larger dataset of machine-learning predictions. The methods automatically adapt to the quality of available predictions, yielding easy-to-compute confidence sets -- for parameters of any dimensionality -- that always improve on classical intervals using only the labeled data. PPI++ builds on prediction-powered inference (PPI), which targets the same problem setting, improving its computational and statistical efficiency. Real and synthetic experiments demonstrate the benefits of the proposed adaptations.
</details>
<details>
<summary>摘要</summary>
我团队现请您耳机提供PPI++：一种 computationally 轻量级的方法，用于基于小量标注数据集和大量机器学习预测数据集的估计和推理。该方法自动适应可用预测的质量，生成可算法的信任区间，用于任何维度的参数。PPI++ 基于预测力量推理（PPI），targeting 同一个问题设定，提高其计算和统计效率。实际和Synthetic 实验表明提案的修改带来了优势。Note that "PPI" in the text refers to "prediction-powered inference", which is a methodology for estimation and inference based on a small labeled dataset and a larger dataset of machine-learning predictions.
</details></li>
</ul>
<hr>
<h2 id="Deep-Double-Descent-for-Time-Series-Forecasting-Avoiding-Undertrained-Models"><a href="#Deep-Double-Descent-for-Time-Series-Forecasting-Avoiding-Undertrained-Models" class="headerlink" title="Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models"></a>Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01442">http://arxiv.org/abs/2311.01442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentino Assandri, Sam Heshmati, Burhaneddin Yaman, Anton Iakovlev, Ariel Emiliano Repetur</li>
<li>for: 本研究旨在探讨深度学习模型在时间序列预测中的训练策略，即如何训练深度学习模型不受模型架构的限制。</li>
<li>methods: 我们采用了广泛的实验方法来研究深度学习模型在时间序列预测中的深度双峰现象，以及使用更多的迭代可以抑制过拟合。</li>
<li>results: 我们在72个公共时间序列数据集中实现了长序时间序列预测的状态对应率，达到了70%左右。此外，我们还提出了一种分类方法来描述训练策略的修改，包括数据增强、模型输入、模型目标、时间序列数据量和计算预算。<details>
<summary>Abstract</summary>
Deep learning models, particularly Transformers, have achieved impressive results in various domains, including time series forecasting. While existing time series literature primarily focuses on model architecture modifications and data augmentation techniques, this paper explores the training schema of deep learning models for time series; how models are trained regardless of their architecture. We perform extensive experiments to investigate the occurrence of deep double descent in several Transformer models trained on public time series data sets. We demonstrate epoch-wise deep double descent and that overfitting can be reverted using more epochs. Leveraging these findings, we achieve state-of-the-art results for long sequence time series forecasting in nearly 70% of the 72 benchmarks tested. This suggests that many models in the literature may possess untapped potential. Additionally, we introduce a taxonomy for classifying training schema modifications, covering data augmentation, model inputs, model targets, time series per model, and computational budget.
</details>
<details>
<summary>摘要</summary>
深度学习模型，特别是转换器，在不同领域中已经达到了吸引人的成绩，包括时间序列预测。现有的时间序列文献主要关注模型建构修改和数据扩充技术，而这篇论文则探索了深度学习模型在时间序列训练中的Schema;无论模型的建构如何。我们进行了广泛的实验，探讨了深度双峰现象在多种转换器模型中的发生，并证明了训练epoch数的增加可以抑制过拟合。基于这些发现，我们在72个benchmark测试集中实现了长序时间序列预测的州OF-the-art结果，达到了70%左右。这表示许多在文献中出现的模型可能具有未发挥的潜力。此外，我们还提出了训练schema修改的分类法，涵盖了数据扩充、模型输入、模型目标、时间序列每个模型和计算预算。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Moments-Unsupervised-Halfspace-Learning-in-Polynomial-Time"><a href="#Contrastive-Moments-Unsupervised-Halfspace-Learning-in-Polynomial-Time" class="headerlink" title="Contrastive Moments: Unsupervised Halfspace Learning in Polynomial Time"></a>Contrastive Moments: Unsupervised Halfspace Learning in Polynomial Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01435">http://arxiv.org/abs/2311.01435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyuan Cao, Santosh S. Vempala</li>
<li>for: 学习高维半空间，保证TV距离 Within desired 程度。</li>
<li>methods: 使用一种多项式时间算法，不需要标签，可以快速地学习高维半空间。</li>
<li>results: 算法的样本和时间复杂度是多项式增长，可以保证学习结果的准确性。<details>
<summary>Abstract</summary>
We give a polynomial-time algorithm for learning high-dimensional halfspaces with margins in $d$-dimensional space to within desired TV distance when the ambient distribution is an unknown affine transformation of the $d$-fold product of an (unknown) symmetric one-dimensional logconcave distribution, and the halfspace is introduced by deleting at least an $\epsilon$ fraction of the data in one of the component distributions. Notably, our algorithm does not need labels and establishes the unique (and efficient) identifiability of the hidden halfspace under this distributional assumption. The sample and time complexity of the algorithm are polynomial in the dimension and $1/\epsilon$. The algorithm uses only the first two moments of suitable re-weightings of the empirical distribution, which we call contrastive moments; its analysis uses classical facts about generalized Dirichlet polynomials and relies crucially on a new monotonicity property of the moment ratio of truncations of logconcave distributions. Such algorithms, based only on first and second moments were suggested in earlier work, but hitherto eluded rigorous guarantees.   Prior work addressed the special case when the underlying distribution is Gaussian via Non-Gaussian Component Analysis. We improve on this by providing polytime guarantees based on Total Variation (TV) distance, in place of existing moment-bound guarantees that can be super-polynomial. Our work is also the first to go beyond Gaussians in this setting.
</details>
<details>
<summary>摘要</summary>
我们提供一个多项式时间算法，用于在$d$-维空间中学习高维半空间，并且保证与所需的总体变化距离（TV距离）相匹配。在这个设定下，我们假设拥有一个未知的一维对称凹降分布，并且半空间是通过删除至少$\epsilon$ fraction的数据来引入的。我们的算法不需要标签，并且可以高效地识别隐藏的半空间，具有 polynomial 的样本和时间复杂度，其中 $1/\epsilon$ 是随着维度和 $\epsilon$ 的函数。我们的算法使用适当的重新权重的 empirical distribution 的第一个和第二个 момен数，我们称之为对比分oment，并且我们的分析基于通用的 Generalized Dirichlet  polynomials 的特性，以及一个新的准确性Property of the moment ratio of truncations of logconcave distributions。这种算法在之前的工作中已经提出，但是它们没有得到正式的保证。在先前的工作中，人们已经处理了下面特殊情况：在下面的设定下，所下面的分布是 Gaussian。我们超越了这个特殊情况，并提供了基于 TV 距离的多项式时间 guarantees，而不是之前的时间 bound 保证，这些保证可能是超 polynomial。此外，我们的工作也是首次超越了 Gaussian 的情况。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Alzheimer-Disease-Dementia-Levels-Using-Machine-Learning-Methods"><a href="#Identifying-Alzheimer-Disease-Dementia-Levels-Using-Machine-Learning-Methods" class="headerlink" title="Identifying Alzheimer Disease Dementia Levels Using Machine Learning Methods"></a>Identifying Alzheimer Disease Dementia Levels Using Machine Learning Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01428">http://arxiv.org/abs/2311.01428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Gulzar Hussain, Ye Shiren</li>
<li>for: 本研究旨在用机器学习或深度学习模型为您的AD分类提供有效方法。</li>
<li>methods: 本研究使用RF、SVM和CNN算法，并将水池分 segmentation作为特征提取方法。</li>
<li>results: 我们的结果表明，SVM与水池特征的组合可以达到96.25%的高精度，超过其他分类方法。<details>
<summary>Abstract</summary>
Dementia, a prevalent neurodegenerative condition, is a major manifestation of Alzheimer's disease (AD). As the condition progresses from mild to severe, it significantly impairs the individual's ability to perform daily tasks independently, necessitating the need for timely and accurate AD classification. Machine learning or deep learning models have emerged as effective tools for this purpose. In this study, we suggested an approach for classifying the four stages of dementia using RF, SVM, and CNN algorithms, augmented with watershed segmentation for feature extraction from MRI images. Our results reveal that SVM with watershed features achieves an impressive accuracy of 96.25%, surpassing other classification methods. The ADNI dataset is utilized to evaluate the effectiveness of our method, and we observed that the inclusion of watershed segmentation contributes to the enhanced performance of the models.
</details>
<details>
<summary>摘要</summary>
德мен谱，一种常见的神经元 degeneration 病种，是阿尔ц海默病（AD）的主要表现。随着病情从轻到严重，它会significantly 减退个体独立完成日常任务的能力，导致了时间和准确的 AD 分类的需求。机器学习或深度学习模型已成为有效的工具。在这项研究中，我们建议了基于 RF、SVM 和 CNN 算法的四个阶段 демен谱分类方法，并使用 watershed 分 segmentation 来提取 MRI 图像的特征。我们的结果表明，SVM 与 watershed 特征达到了96.25%的准确率，超过了其他分类方法。我们使用 ADNI 数据集来评估我们的方法的效果，并发现了包括 watershed 分 segmentation 的模型表现更佳。
</details></li>
</ul>
<hr>
<h2 id="Holistic-Transfer-Towards-Non-Disruptive-Fine-Tuning-with-Partial-Target-Data"><a href="#Holistic-Transfer-Towards-Non-Disruptive-Fine-Tuning-with-Partial-Target-Data" class="headerlink" title="Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data"></a>Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01420">http://arxiv.org/abs/2311.01420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Hao Tu, Hong-You Chen, Zheda Mai, Jike Zhong, Vardaan Pahuja, Tanya Berger-Wolf, Song Gao, Charles Stewart, Yu Su, Wei-Lun Chao</li>
<li>for: 本研究旨在适应预训练模型到目标频谱中进行类别化，使用目标数据覆盖部分标签空间。这种问题在实际应用中很重要，因为收集全部类别数据是不现实的。然而，这个问题在文献中得到了有限的关注。本文通过构建 benchmark 数据集和广泛的实验来探讨这个问题，并发现一个困境：在新目标频谱中适应是重要的，但是保持缺失类别的准确率是极其困难的。</li>
<li>methods: 我们提出了两个关键方向来解决这个困境：1）分离频谱差异和分类差异，2）保持类别关系。我们提出了多种有效的解决方案，以保持缺失类别的准确率，并提高总性能。</li>
<li>results: 我们的实验结果显示，我们的方法可以维持缺失类别的准确率，同时提高总性能，建立了基于部分目标数据的预训练模型的坚实基准。<details>
<summary>Abstract</summary>
We propose a learning problem involving adapting a pre-trained source model to the target domain for classifying all classes that appeared in the source data, using target data that covers only a partial label space. This problem is practical, as it is unrealistic for the target end-users to collect data for all classes prior to adaptation. However, it has received limited attention in the literature. To shed light on this issue, we construct benchmark datasets and conduct extensive experiments to uncover the inherent challenges. We found a dilemma -- on the one hand, adapting to the new target domain is important to claim better performance; on the other hand, we observe that preserving the classification accuracy of classes missing in the target adaptation data is highly challenging, let alone improving them. To tackle this, we identify two key directions: 1) disentangling domain gradients from classification gradients, and 2) preserving class relationships. We present several effective solutions that maintain the accuracy of the missing classes and enhance the overall performance, establishing solid baselines for holistic transfer of pre-trained models with partial target data.
</details>
<details>
<summary>摘要</summary>
我们提出了一个实际问题，即适应预训练源模型到目标频谱中的全类划分，使用目标数据覆盖部分标签空间。这个问题在文献中受到了限制的关注。为了照明这个问题，我们构建了标准 benchmark 数据集并进行了广泛的实验，以揭示内在的挑战。我们发现了一个困境：一个方面是适应新的目标频谱非常重要，以提高性能；另一方面，我们发现保持缺失在目标适应数据中的分类精度非常困难，更何决定提高。为了解决这个问题，我们确定了两个关键方向：1）分离频谱方向和分类方向的融合，2）保持分类关系。我们提出了多种有效的解决方案，以保持缺失分类的精度并提高总性能，建立了坚实的基准值，以便整体传输预训练模型。
</details></li>
</ul>
<hr>
<h2 id="A-Coreset-based-Tempered-Variational-Posterior-for-Accurate-and-Scalable-Stochastic-Gaussian-Process-Inference"><a href="#A-Coreset-based-Tempered-Variational-Posterior-for-Accurate-and-Scalable-Stochastic-Gaussian-Process-Inference" class="headerlink" title="A Coreset-based, Tempered Variational Posterior for Accurate and Scalable Stochastic Gaussian Process Inference"></a>A Coreset-based, Tempered Variational Posterior for Accurate and Scalable Stochastic Gaussian Process Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01409">http://arxiv.org/abs/2311.01409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mert Ketenci, Adler Perotte, Noémie Elhadad, Iñigo Urteaga</li>
<li>for: 本研究提出了一种新的Stochastic Variational Gaussian Process（SVGP）推理方法，用于寻找高精度的GP模型参数。</li>
<li>methods: 该方法基于一个可学习的pseudo输入输出点集（coreset），并使用一种基于GP prior和数据可能性的可变温度家族来定义推理模型。</li>
<li>results: 研究人员通过分析CVTGP的下界，发现CVTGP可以减少学习参数的大小，保持数据 sparse和可解释的表示，并且在实际推理任务中提供了更好的证据下界估计和预测平均方差。<details>
<summary>Abstract</summary>
We present a novel stochastic variational Gaussian process ($\mathcal{GP}$) inference method, based on a posterior over a learnable set of weighted pseudo input-output points (coresets). Instead of a free-form variational family, the proposed coreset-based, variational tempered family for $\mathcal{GP}$s (CVTGP) is defined in terms of the $\mathcal{GP}$ prior and the data-likelihood; hence, accommodating the modeling inductive biases. We derive CVTGP's lower bound for the log-marginal likelihood via marginalization of the proposed posterior over latent $\mathcal{GP}$ coreset variables, and show it is amenable to stochastic optimization. CVTGP reduces the learnable parameter size to $\mathcal{O}(M)$, enjoys numerical stability, and maintains $\mathcal{O}(M^3)$ time- and $\mathcal{O}(M^2)$ space-complexity, by leveraging a coreset-based tempered posterior that, in turn, provides sparse and explainable representations of the data. Results on simulated and real-world regression problems with Gaussian observation noise validate that CVTGP provides better evidence lower-bound estimates and predictive root mean squared error than alternative stochastic $\mathcal{GP}$ inference methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的随机变量 Gaussian process（$\mathcal{GP}$）推理方法，基于一个可学习的Weighted pseudo input-output点（coreset）的 posterior。而不是一个自由变量的变ational家族，我们定义了一个基于 $\mathcal{GP}$ 先验和数据可能性的 coreset-based, variational tempered家族（CVTGP）。我们 derivates CVTGP 的下界 для吞吐量 marginal likelihood via marginalization of the proposed posterior over latent $\mathcal{GP}$ coreset variables，并证明它是可数学化的。CVTGP 减少可学习参数的大小至 $\mathcal{O}(M)$，享受到数学稳定性，并保持 $\mathcal{O}(M^3)$ 时间-和 $\mathcal{O}(M^2)$ 空间复杂性，通过利用一个 coreset-based tempered posterior，从而提供稀疏和可解释的数据表示。在 simulate 和实际世界 regression 问题中，我们 obtainted results show that CVTGP 提供更好的证据下界估计和预测根据标准差Error than alternative stochastic $\mathcal{GP}$ inference methods。
</details></li>
</ul>
<hr>
<h2 id="Normalizing-flows-as-approximations-of-optimal-transport-maps-via-linear-control-neural-ODEs"><a href="#Normalizing-flows-as-approximations-of-optimal-transport-maps-via-linear-control-neural-ODEs" class="headerlink" title="Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs"></a>Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01404">http://arxiv.org/abs/2311.01404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Scagliotti, Sara Farinelli</li>
<li>for: 本研究目的是计算两个概率分布 $\mu$ 和 $\nu$ 之间的最优运输图。</li>
<li>methods: 本文使用了深度神经网络来构造可逆运输图，并使用了线性控制的启动Vector场来实现 $W_2$-优化运输图。</li>
<li>results: 本文提出了一种基于 $\Gamma$-收敛原理的数值方法，可以实现 praktisch 计算最优运输图。<details>
<summary>Abstract</summary>
The term "Normalizing Flows" is related to the task of constructing invertible transport maps between probability measures by means of deep neural networks. In this paper, we consider the problem of recovering the $W_2$-optimal transport map $T$ between absolutely continuous measures $\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$ as the flow of a linear-control neural ODE. We first show that, under suitable assumptions on $\mu,\nu$ and on the controlled vector fields, the optimal transport map is contained in the $C^0_c$-closure of the flows generated by the system. Assuming that discrete approximations $\mu_N,\nu_N$ of the original measures $\mu,\nu$ are available, we use a discrete optimal coupling $\gamma_N$ to define an optimal control problem. With a $\Gamma$-convergence argument, we prove that its solutions correspond to flows that approximate the optimal transport map $T$. Finally, taking advantage of the Pontryagin Maximum Principle, we propose an iterative numerical scheme for the resolution of the optimal control problem, resulting in an algorithm for the practical computation of the approximated optimal transport map.
</details>
<details>
<summary>摘要</summary>
“Normalizing Flows”是指通过深度神经网络建立可逆传输映射，以实现probability measures之间的寻常化。在这篇论文中，我们考虑了将$W_2$-优化的传输 map $T$ между绝对连续概率分布 $\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$的问题。我们首先表明，在适当的假设下，优化的传输 map 包含在$C^0_c$-闭合的流动中。假设可用的柯西分布 $\mu_N,\nu_N$ 是原始概率分布 $\mu,\nu$ 的精炼版本，我们使用一个最优对应问题来定义一个优化控制问题。通过 $\Gamma$-收敛证明，我们证明其解对应于流动，这些流动与优化传输 map 相似。最后，我们利用彭特拉金最大原理，提出了一种可行的数值方法，用于实现优化控制问题的解决方案，从而实现了优化传输 map 的实际计算。
</details></li>
</ul>
<hr>
<h2 id="Time-series-Generation-by-Contrastive-Imitation"><a href="#Time-series-Generation-by-Contrastive-Imitation" class="headerlink" title="Time-series Generation by Contrastive Imitation"></a>Time-series Generation by Contrastive Imitation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01388">http://arxiv.org/abs/2311.01388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Jarrett, Ioana Bica, Mihaela van der Schaar</li>
<li>for: 这个论文的目标是学习时间序列数据的生成模型，并且解决了序列设置中的独特挑战，即生成器需要捕捉转移的Conditional Dynamics，同时其打包rollouts也需要保持多步轨迹的 JOINT 分布。</li>
<li>methods: 这个论文使用了一种混合学习的框架，即通过对比估计来学习一个全局的能量模型，并且通过优化一个本地（但是前向looking）的转移策略来学习一个生成器。在训练时，这两个 ком成分被学习了合作地，避免了对抗性目标的不稳定。在推断时，学习的策略服务为生成器，并且学习的能量服务为轨迹级别的评估标准。</li>
<li>results: 该论文的实验结果表明，这种方法可以生成有用的预测样本，并且与现有的标准准确。<details>
<summary>Abstract</summary>
Consider learning a generative model for time-series data. The sequential setting poses a unique challenge: Not only should the generator capture the conditional dynamics of (stepwise) transitions, but its open-loop rollouts should also preserve the joint distribution of (multi-step) trajectories. On one hand, autoregressive models trained by MLE allow learning and computing explicit transition distributions, but suffer from compounding error during rollouts. On the other hand, adversarial models based on GAN training alleviate such exposure bias, but transitions are implicit and hard to assess. In this work, we study a generative framework that seeks to combine the strengths of both: Motivated by a moment-matching objective to mitigate compounding error, we optimize a local (but forward-looking) transition policy, where the reinforcement signal is provided by a global (but stepwise-decomposable) energy model trained by contrastive estimation. At training, the two components are learned cooperatively, avoiding the instabilities typical of adversarial objectives. At inference, the learned policy serves as the generator for iterative sampling, and the learned energy serves as a trajectory-level measure for evaluating sample quality. By expressly training a policy to imitate sequential behavior of time-series features in a dataset, this approach embodies "generation by imitation". Theoretically, we illustrate the correctness of this formulation and the consistency of the algorithm. Empirically, we evaluate its ability to generate predictively useful samples from real-world datasets, verifying that it performs at the standard of existing benchmarks.
</details>
<details>
<summary>摘要</summary>
请考虑学习一种生成模型 для时间序列数据。这种顺序设置具有独特的挑战：不仅 generator应该捕捉（步骤）转移的Conditional动力，而且其开放Loop执行应该保持多步轨迹的共同分布。一方面，基于MLE的autoregressive模型可以学习并计算Explicit转移分布，但它们在执行过程中会受到堆叠的错误影响。另一方面，基于GAN的 adversarial模型可以消除这种曝露偏见，但转移是隐式的难以评估。在这种工作中，我们研究了一种生成框架，它将 autoregressive 模型和 GAN 模型的优点相结合。我们在这个框架中采用了一个 momentum-matching 目标函数来减少堆叠错误的影响，并且通过对 local (但是前向的) 转移策略进行优化，使得 reinforcement 信号来自 global (但是步骤可分解的) energy 模型，该模型通过对比估计来训练。在训练时，这两个组件被学习共同，以避免 adversarial 目标函数的不稳定性。在推理时，学习的策略将服务为生成器，用于逐步采样，而学习的能量将用于评估样本质量。通过直接在时间序列特征上学习一种策略，这种方法实现了"通过模仿来生成'。理论上，我们证明了这种形式的正确性和算法的一致性。实际上，我们评估了它在真实数据上的预测用途性能，并证明它与现有的标准准确。
</details></li>
</ul>
<hr>
<h2 id="Monotone-Generative-Modeling-via-a-Gromov-Monge-Embedding"><a href="#Monotone-Generative-Modeling-via-a-Gromov-Monge-Embedding" class="headerlink" title="Monotone Generative Modeling via a Gromov-Monge Embedding"></a>Monotone Generative Modeling via a Gromov-Monge Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01375">http://arxiv.org/abs/2311.01375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wonjun Lee, Yifei Yang, Dongmian Zou, Gilad Lerman</li>
<li>for: 这个论文的目的是提出一种基于深度生成模型的方法，用于解决生成器激素网络（GANs）中的初始化条件敏感和模式崩溃问题。</li>
<li>methods: 该方法利用了格罗莫夫-蒙日 embedding（GME）来识别数据的低维结构，并将其映射到一个低维 latent space 中，保持了数据的几何结构。这个映射是通过使用 GME 和 $c$-cyclical monotonicity 的生成映射来确保，其中 $c$ 是一个内在嵌入成本。</li>
<li>results: 数值实验表明，该方法可以生成高质量的图像，避免模式崩溃，并在不同的初始化条件下展现出较好的Robustness。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) are powerful tools for creating new content, but they face challenges such as sensitivity to starting conditions and mode collapse. To address these issues, we propose a deep generative model that utilizes the Gromov-Monge embedding (GME). It helps identify the low-dimensional structure of the underlying measure of the data and then maps it, while preserving its geometry, into a measure in a low-dimensional latent space, which is then optimally transported to the reference measure. We guarantee the preservation of the underlying geometry by the GME and $c$-cyclical monotonicity of the generative map, where $c$ is an intrinsic embedding cost employed by the GME. The latter property is a first step in guaranteeing better robustness to initialization of parameters and mode collapse. Numerical experiments demonstrate the effectiveness of our approach in generating high-quality images, avoiding mode collapse, and exhibiting robustness to different starting conditions.
</details>
<details>
<summary>摘要</summary>
生成 adversarial networks (GANs) 是一种强大的创新工具，但它们面临敏感性和模式塌缩的挑战。为解决这些问题，我们提议一种深度生成模型，利用 Gromov-Monge 嵌入 (GME)。它可以识别数据的下面结构，并将其映射到一个低维度的隐藏空间中，保持其几何结构。然后，我们可以使用最优的运输算法将其传输到参照掌握中的掌握。我们保证 GME 的嵌入精度和 $c$-cyclical 幂环境的生成映射具有良好的Robustness，其中 $c$ 是 GME 使用的内在嵌入成本。这个性质是保证更好的模型初始化参数和模式塌缩的第一步。数值实验证明我们的方法可以生成高质量的图像，避免模式塌缩，并在不同的初始参数下展现出良好的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Respiratory-Anomaly-Detection-using-Reflected-Infrared-Light-wave-Signals"><a href="#Respiratory-Anomaly-Detection-using-Reflected-Infrared-Light-wave-Signals" class="headerlink" title="Respiratory Anomaly Detection using Reflected Infrared Light-wave Signals"></a>Respiratory Anomaly Detection using Reflected Infrared Light-wave Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01367">http://arxiv.org/abs/2311.01367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Zobaer Islam, Brenden Martin, Carly Gotcher, Tyler Martinez, John F. O’Hara, Sabit Ekin</li>
<li>for: 这个研究旨在开发一种无接触呼吸异常检测方法，使用机械人的胸部反射的异步光波信号来检测呼吸 Parameters。</li>
<li>methods: 该方法使用低成本、普遍存在的红外照明LED和光电传感器，并使用机器学习模型来识别不同类型的呼吸数据中的异常 Parameters。</li>
<li>results: 实验结果表明，该方法可以在0.5米至1.5米的距离内准确地识别7种不同类型的呼吸数据，准确率高达96.6%。此外，系统还可以排除不含呼吸信息的数据。该系统可以在家庭或医疗机构中作为一种智能、无接触、谨慎的呼吸监测方法应用。<details>
<summary>Abstract</summary>
In this study, we present a non-contact respiratory anomaly detection method using incoherent light-wave signals reflected from the chest of a mechanical robot that can breathe like human beings. In comparison to existing radar and camera-based sensing systems for vitals monitoring, this technology uses only a low-cost ubiquitous light source (e.g., infrared light emitting diode) and sensor (e.g., photodetector). This light-wave sensing (LWS) system recognizes different breathing anomalies from the variations of light intensity reflected from the chest of the robot within a 0.5m-1.5m range. The anomaly detection model demonstrates up to 96.6% average accuracy in classifying 7 different types of breathing data using machine learning. The model can also detect faulty data collected by the system that does not contain breathing information. The developed system can be utilized at home or healthcare facilities as a smart, non-contact and discreet respiration monitoring method.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一种无接触呼吸异常检测方法，使用机械人的胸部反射的不协调光波信号。与现有的雷达和摄像头基本感知系统相比，这技术只需使用低成本的普遍存在的光源（例如，红外光晶闻）和检测器（例如，光电池）。这个光波检测（LWS）系统可以从机械人胸部反射的光INTENSITY变化中识别出不同的呼吸异常。模型可以达到96.6%的均值准确率，在7种不同的呼吸数据类型中进行分类。模型还可以检测系统收集的数据中不含呼吸信息的异常数据。已开发的系统可以在家庭或医疗机构中使用，作为智能、无接触和谨慎的呼吸监测方法。
</details></li>
</ul>
<hr>
<h2 id="On-the-Lipschitz-constant-of-random-neural-networks"><a href="#On-the-Lipschitz-constant-of-random-neural-networks" class="headerlink" title="On the Lipschitz constant of random neural networks"></a>On the Lipschitz constant of random neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01356">http://arxiv.org/abs/2311.01356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Geuchen, Thomas Heindl, Dominik Stöger, Felix Voigtlaender</li>
<li>for: 这篇论文旨在研究随机ReLU神经网络的Lipschitz常数。</li>
<li>methods: 该论文使用 teorethical 方法来研究随机ReLU神经网络的Lipschitz常数。</li>
<li>results: 论文得到了随机ReLU神经网络的Lipschitz常数的 bounds，其中深度取决于宽度和深度。<details>
<summary>Abstract</summary>
Empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. The worst-case robustness against these so-called adversarial examples can be quantified by the Lipschitz constant of the neural network. However, only few theoretical results regarding this quantity exist in the literature. In this paper, we initiate the study of the Lipschitz constant of random ReLU neural networks, i.e., neural networks whose weights are chosen at random and which employ the ReLU activation function. For shallow neural networks, we characterize the Lipschitz constant up to an absolute numerical constant. Moreover, we extend our analysis to deep neural networks of sufficiently large width where we prove upper and lower bounds for the Lipschitz constant. These bounds match up to a logarithmic factor that depends on the depth.
</details>
<details>
<summary>摘要</summary>
empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. the worst-case robustness against these so-called adversarial examples can be quantified by the lipschitz constant of the neural network. however, only few theoretical results regarding this quantity exist in the literature. in this paper, we initiate the study of the lipschitz constant of random relu neural networks, i.e., neural networks whose weights are chosen at random and which employ the relu activation function. for shallow neural networks, we characterize the lipschitz constant up to an absolute numerical constant. moreover, we extend our analysis to deep neural networks of sufficiently large width where we prove upper and lower bounds for the lipschitz constant. these bounds match up to a logarithmic factor that depends on the depth.Here's the translation in Traditional Chinese:empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. the worst-case robustness against these so-called adversarial examples can be quantified by the lipschitz constant of the neural network. however, only few theoretical results regarding this quantity exist in the literature. in this paper, we initiate the study of the lipschitz constant of random relu neural networks, i.e., neural networks whose weights are chosen at random and which employ the relu activation function. for shallow neural networks, we characterize the lipschitz constant up to an absolute numerical constant. moreover, we extend our analysis to deep neural networks of sufficiently large width where we prove upper and lower bounds for the lipschitz constant. these bounds match up to a logarithmic factor that depends on the depth.
</details></li>
</ul>
<hr>
<h2 id="Unreading-Race-Purging-Protected-Features-from-Chest-X-ray-Embeddings"><a href="#Unreading-Race-Purging-Protected-Features-from-Chest-X-ray-Embeddings" class="headerlink" title="Unreading Race: Purging Protected Features from Chest X-ray Embeddings"></a>Unreading Race: Purging Protected Features from Chest X-ray Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01349">http://arxiv.org/abs/2311.01349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Weber, Michael Ingrisch, Bernd Bischl, David Rügamer</li>
<li>for: 这份研究是为了测试和移除医疗影像模型内的保护特征效应。</li>
<li>methods: 这份研究使用了一种叫做“orthogonalization”的方法，以移除医疗影像模型内的保护特征影响。</li>
<li>results: 研究发现，在医疗影像预测中，保护特征会有影响，但是使用“orthogonalization”方法可以移除这些影响，同时维持预测性能。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Purpose: To analyze and remove protected feature effects in chest radiograph embeddings of deep learning models.   Materials and Methods: An orthogonalization is utilized to remove the influence of protected features (e.g., age, sex, race) in chest radiograph embeddings, ensuring feature-independent results. To validate the efficacy of the approach, we retrospectively study the MIMIC and CheXpert datasets using three pre-trained models, namely a supervised contrastive, a self-supervised contrastive, and a baseline classifier model. Our statistical analysis involves comparing the original versus the orthogonalized embeddings by estimating protected feature influences and evaluating the ability to predict race, age, or sex using the two types of embeddings.   Results: Our experiments reveal a significant influence of protected features on predictions of pathologies. Applying orthogonalization removes these feature effects. Apart from removing any influence on pathology classification, while maintaining competitive predictive performance, orthogonalized embeddings further make it infeasible to directly predict protected attributes and mitigate subgroup disparities.   Conclusion: The presented work demonstrates the successful application and evaluation of the orthogonalization technique in the domain of chest X-ray classification.
</details>
<details>
<summary>摘要</summary>
目的：分析并消除深度学习模型中保护特征的影响（例如年龄、性别、种族）的影响。材料和方法：使用正交化来消除深度学习模型中的保护特征影响，以确保不受特定特征影响的结果。为验证方法的有效性，我们对MIMIC和CheXpert数据集使用三个预训练模型，namely一个监督对比模型、一个自监督对比模型和一个基eline类ifier模型。我们的统计分析包括比较原始与正交化的嵌入之间的差异，并评估使用两种类型的嵌入来预测年龄、性别和种族。结果：我们的实验显示，保护特征对疾病预测产生了显著的影响。通过正交化，消除这些特征的影响。除了消除疾病预测中的保护特征影响，正交化嵌入还使得不可直接预测保护属性，减少了 subgroup disparities。结论：本研究示出了在胸部X射线分类领域中正交化技术的成功应用和评估。
</details></li>
</ul>
<hr>
<h2 id="High-dimensional-Linear-Bandits-with-Knapsacks"><a href="#High-dimensional-Linear-Bandits-with-Knapsacks" class="headerlink" title="High-dimensional Linear Bandits with Knapsacks"></a>High-dimensional Linear Bandits with Knapsacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01327">http://arxiv.org/abs/2311.01327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanteng Ma, Dong Xia, Jiashuo Jiang</li>
<li>for: 本研究探讨了高维度上的上下文抽象鸟（CBwK）问题，即每个抽象鸟的奖励等于高维度稀疏加权向量与当前到达者特征的乘积，加上随机噪声。</li>
<li>methods: 本研究提出了一种在高维度上实现稀疏估计的在线变体，并将其与 primal-dual 框架相结合，以控制杯具容量的消耗。</li>
<li>results: 研究人员表明，这种整合方法可以实现依赖于特征维度的下线 regret，并在数据缺乏和数据充沛两个 Régime 中达到最优 regret。同时，他们还进行了数值实验，证明了他们的算法在高维度上的实际性。<details>
<summary>Abstract</summary>
We study the contextual bandits with knapsack (CBwK) problem under the high-dimensional setting where the dimension of the feature is large. The reward of pulling each arm equals the multiplication of a sparse high-dimensional weight vector and the feature of the current arrival, with additional random noise. In this paper, we investigate how to exploit this sparsity structure to achieve improved regret for the CBwK problem. To this end, we first develop an online variant of the hard thresholding algorithm that performs the sparse estimation in an online manner. We further combine our online estimator with a primal-dual framework, where we assign a dual variable to each knapsack constraint and utilize an online learning algorithm to update the dual variable, thereby controlling the consumption of the knapsack capacity. We show that this integrated approach allows us to achieve a sublinear regret that depends logarithmically on the feature dimension, thus improving the polynomial dependency established in the previous literature. We also apply our framework to the high-dimension contextual bandit problem without the knapsack constraint and achieve optimal regret in both the data-poor regime and the data-rich regime. We finally conduct numerical experiments to show the efficient empirical performance of our algorithms under the high dimensional setting.
</details>
<details>
<summary>摘要</summary>
我们研究了高维度上的上下文抽奖问题（CBwK），其中抽奖奖劵的值等于高维度稀疏质量 вектор与当前到达者的特征进行乘法，加上随机噪声。在这篇论文中，我们探讨如何利用这种稀疏结构来实现CBwK问题的改善 regret。为此，我们首先开发了在线的坚持阈值算法，该算法在线上进行稀疏估计。然后，我们将我们的在线估计与 primal-dual 框架相结合，其中我们将每个knapsack约束分配一个 dual 变量，并使用在线学习算法来更新 dual 变量，以控制knapsack资源的使用。我们证明这种整合方法可以实现依赖于特征维度的倒数的 regret，而不是在先前的文献中所确立的多项式依赖。此外，我们还应用了我们的框架到高维度上的无约束上下文抽奖问题，并在数据稀缺和数据充沛两种情况下实现了最优的 regret。最后，我们进行了数据分析，证明了我们的算法在高维度设置下的有效性。
</details></li>
</ul>
<hr>
<h2 id="Long-Range-Neural-Atom-Learning-for-Molecular-Graphs"><a href="#Long-Range-Neural-Atom-Learning-for-Molecular-Graphs" class="headerlink" title="Long-Range Neural Atom Learning for Molecular Graphs"></a>Long-Range Neural Atom Learning for Molecular Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01276">http://arxiv.org/abs/2311.01276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Li, Zhanke Zhou, Jiangchao Yao, Yu Rong, Lu Zhang, Bo Han</li>
<li>for: 提高 Graph Neural Networks (GNNs) 在药物发现中的性能，特别是捕捉长距离交互 (LRI)，以提高分子质量的预测。</li>
<li>methods: 提出一种方法，使用归纳神经元（Neural Atoms）将原始原子归纳到一些抽象的表示中，以捕捉分子中各个原子组的共同信息。该方法通过明确地交换归纳神经元之间的信息，使归纳神经元成为分子中不同原子之间的通信渠道，从而减少了任意两个节点之间的交互范围。</li>
<li>results: 通过对三个长距离图 benchmark 进行广泛的实验，证明该方法可以与任何 GNN 结合使用，并帮助捕捉 LRI。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current GNNs are mainly good at leveraging short-range interactions (SRI) but struggle to capture long-range interactions (LRI), both of which are crucial for determining molecular properties. To tackle this issue, we propose a method that implicitly projects all original atoms into a few Neural Atoms, which abstracts the collective information of atomic groups within a molecule. Specifically, we explicitly exchange the information among neural atoms and project them back to the atoms' representations as an enhancement. With this mechanism, neural atoms establish the communication channels among distant nodes, effectively reducing the interaction scope of arbitrary node pairs into a single hop. To provide an inspection of our method from a physical perspective, we reveal its connection with the traditional LRI calculation method, Ewald Summation. We conduct extensive experiments on three long-range graph benchmarks, covering both graph-level and link-level tasks on molecular graphs. We empirically justify that our method can be equipped with an arbitrary GNN and help to capture LRI.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 已经广泛应用于分子图中的药物探索。然而，目前的 GNNs 主要能够利用短距离交互（SRI），而忽略长距离交互（LRI），这两者都是决定分子性质的关键因素。为解决这个问题，我们提议一种方法，将原始原子Projects into几个神经原子，这些神经原子抽象了分子中原子群的共同信息。具体来说，我们显式地交换神经原子之间的信息，并将其投影回原子表示中作为增强。通过这种机制，神经原子建立了跨节点通信频道，有效地将任意节点对的交互范围减少为单个跳远。为了物理上 inspect 我们的方法，我们揭示了它与传统的 LRI 计算方法，Ewald Summation 之间的连接。我们在三个长距离图 benchmark 上进行了广泛的实验，覆盖了分子图级别和链接级别任务。我们经验表明，我们的方法可以与任意 GNN 结合使用，帮助捕捉 LRI。
</details></li>
</ul>
<hr>
<h2 id="Sanitized-Clustering-against-Confounding-Bias"><a href="#Sanitized-Clustering-against-Confounding-Bias" class="headerlink" title="Sanitized Clustering against Confounding Bias"></a>Sanitized Clustering against Confounding Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01252">http://arxiv.org/abs/2311.01252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/evaflower/scab">https://github.com/evaflower/scab</a></li>
<li>paper_authors: Yinghua Yao, Yuangang Pan, Jing Li, Ivor W. Tsang, Xin Yao<br>for:This paper aims to address the issue of confounding bias in cluster analysis by proposing a new framework called Sanitized Clustering Against confounding Bias (SCAB).methods:The SCAB framework uses a Variational Auto-Encoder (VAE) to eliminate the confounding bias in the semantic latent space of complex data by minimizing the mutual information between the confounding factor and the latent representation.results:The proposed SCAB framework achieves a significant gain in clustering performance by removing the confounding bias, as demonstrated through extensive experiments on complex datasets. The code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/EvaFlower/SCAB%7D.Here's">https://github.com/EvaFlower/SCAB}.Here&#39;s</a> the Chinese version:for:这篇论文目的是解决复杂数据中干扰因素的影响，提出了一种新的框架叫做Sanitized Clustering Against confounding Bias (SCAB)。methods:SCAB使用Variational Auto-Encoder (VAE)来消除复杂数据中干扰因素的影响，在semantic latent space中减少干扰因素和数据的相互信息。results:提出的SCAB框架在复杂数据集上实现了干扰因素的消除，从而提高了聚类性能，经过广泛的实验 validate 这一结论。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/EvaFlower/SCAB%7D">https://github.com/EvaFlower/SCAB}</a> 上获取。<details>
<summary>Abstract</summary>
Real-world datasets inevitably contain biases that arise from different sources or conditions during data collection. Consequently, such inconsistency itself acts as a confounding factor that disturbs the cluster analysis. Existing methods eliminate the biases by projecting data onto the orthogonal complement of the subspace expanded by the confounding factor before clustering. Therein, the interested clustering factor and the confounding factor are coarsely considered in the raw feature space, where the correlation between the data and the confounding factor is ideally assumed to be linear for convenient solutions. These approaches are thus limited in scope as the data in real applications is usually complex and non-linearly correlated with the confounding factor. This paper presents a new clustering framework named Sanitized Clustering Against confounding Bias (SCAB), which removes the confounding factor in the semantic latent space of complex data through a non-linear dependence measure. To be specific, we eliminate the bias information in the latent space by minimizing the mutual information between the confounding factor and the latent representation delivered by Variational Auto-Encoder (VAE). Meanwhile, a clustering module is introduced to cluster over the purified latent representations. Extensive experiments on complex datasets demonstrate that our SCAB achieves a significant gain in clustering performance by removing the confounding bias. The code is available at \url{https://github.com/EvaFlower/SCAB}.
</details>
<details>
<summary>摘要</summary>
实际世界数据集往往含有偏见，这些偏见来自于不同的来源或条件 durante 数据采集。这些不一致性本身就是一个干扰因素，对 clustering 分析造成干扰。现有方法通过将数据投影到偏见因素的 ortogonal complement 上进行 clustering。在这些方法中，数据和偏见因素在原始特征空间中被粗略地考虑，其中数据和偏见因素之间的相关性被假设为线性，以便解决方便。这些方法因此受到限制，因为实际数据通常是复杂的和非线性相关的。这篇文章提出了一种新的 clustering 框架，名为 Sanitized Clustering Against confounding Bias (SCAB)，它可以在复杂数据中除掉偏见因素。SCAB 使用非线性依赖度度量来除掉偏见因素在 semantic 隐藏空间中。具体来说，我们通过 Variational Auto-Encoder (VAE) 来提取 latent representation，并将偏见因素的信息从 latent representation 中除掉。然后，我们引入 clustering 模块，将 purified 的 latent representation 进行 clustering。我们在复杂数据集上进行了广泛的实验，得到的结果表明，SCAB 可以减少偏见 bias，并且在 clustering 性能上具有显著的提升。代码可以在 \url{https://github.com/EvaFlower/SCAB} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Processes-on-Cellular-Complexes"><a href="#Gaussian-Processes-on-Cellular-Complexes" class="headerlink" title="Gaussian Processes on Cellular Complexes"></a>Gaussian Processes on Cellular Complexes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01198">http://arxiv.org/abs/2311.01198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathieu Alain, So Takao, Brooks Paige, Marc Peter Deisenroth</li>
<li>for: 这篇论文旨在开发基于图的机器学习模型，以利用图的拓扑逻辑假设来训练模型。</li>
<li>methods: 该论文提出了基于维度较高的维度复杂系统（cellular complexes）的加aussian proceses（GP）模型，并提出了两种新型的kernels：一种是图Matér kernel的推广，另一种是将不同维度类型的信息混合在一起。</li>
<li>results: 该论文的实验结果表明，基于维度复杂系统的GP模型在处理复杂的图数据时具有更高的准确率和更好的一致性，而且可以更好地捕捉图的拓扑结构。<details>
<summary>Abstract</summary>
In recent years, there has been considerable interest in developing machine learning models on graphs in order to account for topological inductive biases. In particular, recent attention was given to Gaussian processes on such structures since they can additionally account for uncertainty. However, graphs are limited to modelling relations between two vertices. In this paper, we go beyond this dyadic setting and consider polyadic relations that include interactions between vertices, edges and one of their generalisations, known as cells. Specifically, we propose Gaussian processes on cellular complexes, a generalisation of graphs that captures interactions between these higher-order cells. One of our key contributions is the derivation of two novel kernels, one that generalises the graph Mat\'ern kernel and one that additionally mixes information of different cell types.
</details>
<details>
<summary>摘要</summary>
近年来，有很大的兴趣在开发机器学习模型，以利用图structure上的拓扑假设。特别是，有人关注在图上的Gaussian процессе，因为它可以同时考虑不确定性。然而，图只能模型两个顶点之间的关系。在这篇论文中，我们超越这个二元设定，考虑多元关系，包括顶点、边和其总是一个通用的扩展，即细胞。我们提议在细胞复杂体系上的Gaussian проце序，这是图的扩展，可以捕捉不同细胞类型之间的交互。我们的一个关键贡献是 derivation of two novel kernels，一个总结图Matér kernel，另一个同时混合不同细胞类型的信息。
</details></li>
</ul>
<hr>
<h2 id="Combating-Bilateral-Edge-Noise-for-Robust-Link-Prediction"><a href="#Combating-Bilateral-Edge-Noise-for-Robust-Link-Prediction" class="headerlink" title="Combating Bilateral Edge Noise for Robust Link Prediction"></a>Combating Bilateral Edge Noise for Robust Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01196">http://arxiv.org/abs/2311.01196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tmlr-group/rgib">https://github.com/tmlr-group/rgib</a></li>
<li>paper_authors: Zhanke Zhou, Jiangchao Yao, Jiaxu Liu, Xiawei Guo, Quanming Yao, Li He, Liang Wang, Bo Zheng, Bo Han</li>
<li>for: 本研究旨在提高图神经网络（GNN）对边际噪声的抗针对性。</li>
<li>methods: 本研究提出了一种信息论指导的原则，即强健图信息瓶颈（RGIB），以提取可靠的监督信号，并避免表示归一化。RGIB不同于基本信息瓶颈，更加强调和谐地处理图结构、目标标签和表示之间的互相依赖关系，建立了新的学习目标，以提高对边际噪声的抗针对性表示。</li>
<li>results: 对六个dataset和三种GNN进行了广泛的实验，证明了RGIB实例的效iveness。<details>
<summary>Abstract</summary>
Although link prediction on graphs has achieved great success with the development of graph neural networks (GNNs), the potential robustness under the edge noise is still less investigated. To close this gap, we first conduct an empirical study to disclose that the edge noise bilaterally perturbs both input topology and target label, yielding severe performance degradation and representation collapse. To address this dilemma, we propose an information-theory-guided principle, Robust Graph Information Bottleneck (RGIB), to extract reliable supervision signals and avoid representation collapse. Different from the basic information bottleneck, RGIB further decouples and balances the mutual dependence among graph topology, target labels, and representation, building new learning objectives for robust representation against the bilateral noise. Two instantiations, RGIB-SSL and RGIB-REP, are explored to leverage the merits of different methodologies, i.e., self-supervised learning and data reparameterization, for implicit and explicit data denoising, respectively. Extensive experiments on six datasets and three GNNs with diverse noisy scenarios verify the effectiveness of our RGIB instantiations. The code is publicly available at: https://github.com/tmlr-group/RGIB.
</details>
<details>
<summary>摘要</summary>
尽管图гра数据预测已经在图神经网络（GNNs）的发展过程中取得了很大的成功，但图像预测下边的稳定性仍然得不到充分的研究。为了填补这一漏洞，我们首先进行了一项实验，发现图像预测双方干扰了输入图гра和目标标签，导致性能下降和表示塌陷。为解决这个问题，我们提出了一种基于信息理论的原则，即Robust Graph Information Bottleneck（RGIB），用于提取可靠的监督信号并避免表示塌陷。与基本信息瓶颈不同，RGIB进一步解耦了图гра结构、目标标签和表示之间的互相关系，建立了新的学习目标，以避免双方干扰下的表示塌陷。我们还explored two instantiations, RGIB-SSL和RGIB-REP，以利用不同的方法oloiges，即自监督学习和数据重parameterization，进行隐式和显式数据干扰。我们在六个dataset和三种GNN中进行了广泛的实验，证明了我们的RGIB实现的效果。代码可以在 GitHub上获取：https://github.com/tmlr-group/RGIB。
</details></li>
</ul>
<hr>
<h2 id="Add-and-Thin-Diffusion-for-Temporal-Point-Processes"><a href="#Add-and-Thin-Diffusion-for-Temporal-Point-Processes" class="headerlink" title="Add and Thin: Diffusion for Temporal Point Processes"></a>Add and Thin: Diffusion for Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01139">http://arxiv.org/abs/2311.01139</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Lüdke, Marin Biloš, Oleksandr Shchur, Marten Lienen, Stephan Günnemann</li>
<li>for: 这个论文是为了提出一种可靠的概率噪声扩散模型，用于识别和预测连续时间事件数据。</li>
<li>methods: 这个模型使用了TPP框架，并使用了概率噪声扩散的方法来处理整个事件序列。这种方法不同于现有的扩散方法，可以自然地处理数据中的绝对和连续组成部分。</li>
<li>results: 在 synthetic 和实际数据集上的实验中，这个模型与现状的TPP模型匹配在概率密度估计方面，而且在预测方面强力突出于现状模型。<details>
<summary>Abstract</summary>
Autoregressive neural networks within the temporal point process (TPP) framework have become the standard for modeling continuous-time event data. Even though these models can expressively capture event sequences in a one-step-ahead fashion, they are inherently limited for long-term forecasting applications due to the accumulation of errors caused by their sequential nature. To overcome these limitations, we derive ADD-THIN, a principled probabilistic denoising diffusion model for TPPs that operates on entire event sequences. Unlike existing diffusion approaches, ADD-THIN naturally handles data with discrete and continuous components. In experiments on synthetic and real-world datasets, our model matches the state-of-the-art TPP models in density estimation and strongly outperforms them in forecasting.
</details>
<details>
<summary>摘要</summary>
自适应神经网络在时间点过程（TPP）框架中已成为连续时间事件数据的标准模型。尽管这些模型可以表达性地捕捉事件序列，但它们因为顺序性的限制而难以长期预测。为了超越这些限制，我们 derivate ADD-THIN，一种基于概率的减噪扩散模型，用于整个事件序列。与现有扩散方法不同，ADD-THIN自然处理数据中的离散和连续组成部分。在synthetic和实际数据集上的实验中，我们的模型与状态态准TPP模型匹配在概率分布估计方面，而且在预测方面强制超越它们。
</details></li>
</ul>
<hr>
<h2 id="Generating-QM1B-with-PySCF-text-IPU"><a href="#Generating-QM1B-with-PySCF-text-IPU" class="headerlink" title="Generating QM1B with PySCF$_{\text{IPU}}$"></a>Generating QM1B with PySCF$_{\text{IPU}}$</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01135">http://arxiv.org/abs/2311.01135</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graphcore-research/pyscf-ipu">https://github.com/graphcore-research/pyscf-ipu</a></li>
<li>paper_authors: Alexander Mathiasen, Hatem Helal, Kerstin Klaser, Paul Balanca, Josef Dean, Carlo Luschi, Dominique Beaini, Andrew Fitzgibbon, Dominic Masters</li>
<li>for: 这个论文的目的是探索使用增强计算机见解和自然语言处理的基础模型，以提高量子化学任务的进步。</li>
<li>methods: 这个论文使用了PySCF$_{\text{IPU}$数据生成器和智能处理单元（IPU）来创建大量的训练样例，并生成了100亿个训练样例的 dataset QM1B。</li>
<li>results: 这个论文的结果表明，一个简单的基线神经网络（SchNet 9M）可以通过增加训练样例来提高性能，而无需添加额外的逻辑假设。<details>
<summary>Abstract</summary>
The emergence of foundation models in Computer Vision and Natural Language Processing have resulted in immense progress on downstream tasks. This progress was enabled by datasets with billions of training examples. Similar benefits are yet to be unlocked for quantum chemistry, where the potential of deep learning is constrained by comparatively small datasets with 100k to 20M training examples. These datasets are limited in size because the labels are computed using the accurate (but computationally demanding) predictions of Density Functional Theory (DFT). Notably, prior DFT datasets were created using CPU supercomputers without leveraging hardware acceleration. In this paper, we take a first step towards utilising hardware accelerators by introducing the data generator PySCF$_{\text{IPU}$ using Intelligence Processing Units (IPUs). This allowed us to create the dataset QM1B with one billion training examples containing 9-11 heavy atoms. We demonstrate that a simple baseline neural network (SchNet 9M) improves its performance by simply increasing the amount of training data without additional inductive biases. To encourage future researchers to use QM1B responsibly, we highlight several limitations of QM1B and emphasise the low-resolution of our DFT options, which also serves as motivation for even larger, more accurate datasets. Code and dataset are available on Github: http://github.com/graphcore-research/pyscf-ipu
</details>
<details>
<summary>摘要</summary>
“基于Foundational模型的进步在计算机视觉和自然语言处理领域已经取得了巨大的进展，这些进展得到了大量的训练例子的支持。然而，量子化化领域的进展仍然受到限制，因为deep learning的潜力受到了相对较少的训练例子的限制，这些训练例子的数量在100k到20M之间。这些训练例子的有限性是因为labels是通过精度高但计算复杂的密度函数理论（DFT）来计算的。尽管之前的DFT数据集都是使用CPU超级计算机创建的，但我们在这篇论文中首次利用硬件加速器。我们开发了一个名为PySCF$_{\text{IPU}$的数据生成器，使用智能处理单元（IPU）来生成QM1B数据集，该数据集包含10亿个训练例子，其中9-11个重元素。我们示示了一个简单的基线神经网络（SchNet 9M）在增加训练数据量后，性能得到了提高。为鼓励未来的研究人员负责QM1B，我们列出了QM1B的一些限制和我们的DFT选项的低分辨率，这也作为鼓励更大、更准确的数据集的动机。代码和数据集可以在GitHub上获取：http://github.com/graphcore-research/pyscf-ipu。”
</details></li>
</ul>
<hr>
<h2 id="AI-for-Interpretable-Chemistry-Predicting-Radical-Mechanistic-Pathways-via-Contrastive-Learning"><a href="#AI-for-Interpretable-Chemistry-Predicting-Radical-Mechanistic-Pathways-via-Contrastive-Learning" class="headerlink" title="AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning"></a>AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01118">http://arxiv.org/abs/2311.01118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadamin Tavakoli, Yin Ting T. Chiu, Alexander Shmakov, Ann Marie Carlton, David Van Vranken, Pierre Baldi<br>for:This paper aims to address the limitations of existing deep learning-based reaction predictors by introducing a new system called RMechRP, which leverages contrastive learning and mechanistic pathways to provide more interpretable and generalizable predictions of radical reactions.methods:The authors use a public database of radical reactions, RMechDB, to develop and train multiple deep-learning models. They employ contrastive learning to learn a representation of chemical reactions that is based on mechanistic pathways, which are the most interpretable representation of chemical reactions.results:The authors demonstrate the effectiveness of RMechRP in providing accurate and interpretable predictions of radical reactions, and its potential for various applications in atmospheric chemistry. Their results show that RMechRP outperforms existing reaction predictors in terms of accuracy and interpretability, and has the potential to be applied in a variety of domains beyond radical chemistry.Here is the text in Simplified Chinese:for:这篇论文的目的是解决现有的深度学习基于反应预测器的限制，通过引入对比学习和机制路径来提供更加可读性和普适性的反应预测。methods:作者使用了一个公共的自由 radical reaction 数据库，RMechDB，来开发和训练多个深度学习模型。他们使用对比学习来学习化学反应的表示，基于机制路径，这是化学反应最可读的表示。results:作者表明了 RMechRP 的有效性，可以提供高度可读性和精度的 radical reaction 预测，并且在大气化学中有广泛的应用前景。他们的结果表明，RMechRP 在准确性和可读性方面超过了现有的反应预测器，并且在不同的预测任务中具有普适性。<details>
<summary>Abstract</summary>
Deep learning-based reaction predictors have undergone significant architectural evolution. However, their reliance on reactions from the US Patent Office results in a lack of interpretable predictions and limited generalization capability to other chemistry domains, such as radical and atmospheric chemistry. To address these challenges, we introduce a new reaction predictor system, RMechRP, that leverages contrastive learning in conjunction with mechanistic pathways, the most interpretable representation of chemical reactions. Specifically designed for radical reactions, RMechRP provides different levels of interpretation of chemical reactions. We develop and train multiple deep-learning models using RMechDB, a public database of radical reactions, to establish the first benchmark for predicting radical reactions. Our results demonstrate the effectiveness of RMechRP in providing accurate and interpretable predictions of radical reactions, and its potential for various applications in atmospheric chemistry.
</details>
<details>
<summary>摘要</summary>
深度学习基于的反应预测器在建筑方面已经经历了重要的变革。然而，它们仍然依赖于美国专利局的反应，导致预测的解释性和泛化能力受限，无法应用于其他化学领域，如自由基和大气化学。为解决这些挑战，我们提出了一种新的反应预测系统，RMechRP，它利用对比学习并与机理路径相结合，以获得最可读的化学反应表示。RMechRP特意设计为自由基反应，可提供不同水平的化学反应解释。我们使用RMechDB，一个公共的自由基反应数据库，来训练和开发多种深度学习模型，以建立自由基反应预测的首个benchmark。我们的结果表明RMechRP可以提供高度准确和可读的自由基反应预测，并有很好的应用前景在大气化学领域。
</details></li>
</ul>
<hr>
<h2 id="In-Defense-of-Softmax-Parametrization-for-Calibrated-and-Consistent-Learning-to-Defer"><a href="#In-Defense-of-Softmax-Parametrization-for-Calibrated-and-Consistent-Learning-to-Defer" class="headerlink" title="In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer"></a>In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01106">http://arxiv.org/abs/2311.01106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhou Cao, Hussein Mozannar, Lei Feng, Hongxin Wei, Bo An</li>
<li>for: 本研究旨在提高机器学习分类器的安全性和性能，通过学习如何分类和如何委托专家来实现。</li>
<li>methods: 本研究使用学习如何委托专家的框架，并利用 asymmetric softmax 基于的代表函数来解决过去的误差潜在问题。</li>
<li>results: 本研究表明，使用 asymmetric softmax 基于的代表函数可以生成有效的估计，并且不受过去误差的影响。  addition, the study shows that the proposed method has good non-asymptotic properties and is empirically validated on benchmark datasets.<details>
<summary>Abstract</summary>
Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and performance. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring which makes them uncalibrated. However, it remains unknown whether this is due to the widely used softmax parameterization and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate losses used and not due to softmax. We then propose a novel statistically consistent asymmetric softmax-based surrogate loss that can produce valid estimates without the issue of unboundedness. We further analyze the non-asymptotic properties of our method and empirically validate its performance and calibration on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
使机器学习分类器延迟决策到下游专家更准确的决策将提高安全性和性能。这个目标可以通过学习延迟框架来实现，该框架旨在同时学习分类和延迟专家的决策。在latest studies中，已经证明了使用softmax参数化的popular estimators可以导致无限大的投票概率，从而使得其不准确。然而，还未知道这是由于广泛使用的softmax参数化还是由其他因素引起的。在这个工作中，我们首先表明了先前文献中的不准确和无限大的估计器的起因是由surrogate losses的对称性所致，而不是由softmax。然后，我们提出了一种新的 statistically consistent asymmetric softmax-based surrogate loss，可以生成有效的估计器而不受 boundedness问题的影响。我们进一步分析了我们的方法的非几何性质，并通过 benchmark datasets进行了实验验证。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Modules-with-Temporal-Attention-for-Multi-Task-Reinforcement-Learning"><a href="#Contrastive-Modules-with-Temporal-Attention-for-Multi-Task-Reinforcement-Learning" class="headerlink" title="Contrastive Modules with Temporal Attention for Multi-Task Reinforcement Learning"></a>Contrastive Modules with Temporal Attention for Multi-Task Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01075">http://arxiv.org/abs/2311.01075</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/niiceMing/CMTA">https://github.com/niiceMing/CMTA</a></li>
<li>paper_authors: Siming Lan, Rui Zhang, Qi Yi, Jiaming Guo, Shaohui Peng, Yunkai Gao, Fan Wu, Ruizhi Chen, Zidong Du, Xing Hu, Xishan Zhang, Ling Li, Yunji Chen</li>
<li>for: 这篇论文主要关注在多任务强化学习中的模块原则，即将功能特性分成不同模块并合适地联合使用，以避免多任务间的负面转移问题。</li>
<li>methods: 本论文提出了对抗式模块（CMTA）方法，它通过对模块进行对比学习，并在细节水平上联合共享模块，透过时间注意力，解决多任务间的负面转移问题，提高模块方法的通用性和表现力。</li>
<li>results: 根据Meta-World多任务强化学习benchmark的实验结果，CMTA方法比单独学习每个任务的情况下，首次取得了性能提升，并在基eline之上取得了substantial的表现改善。<details>
<summary>Abstract</summary>
In the field of multi-task reinforcement learning, the modular principle, which involves specializing functionalities into different modules and combining them appropriately, has been widely adopted as a promising approach to prevent the negative transfer problem that performance degradation due to conflicts between tasks. However, most of the existing multi-task RL methods only combine shared modules at the task level, ignoring that there may be conflicts within the task. In addition, these methods do not take into account that without constraints, some modules may learn similar functions, resulting in restricting the model's expressiveness and generalization capability of modular methods. In this paper, we propose the Contrastive Modules with Temporal Attention(CMTA) method to address these limitations. CMTA constrains the modules to be different from each other by contrastive learning and combining shared modules at a finer granularity than the task level with temporal attention, alleviating the negative transfer within the task and improving the generalization ability and the performance for multi-task RL. We conducted the experiment on Meta-World, a multi-task RL benchmark containing various robotics manipulation tasks. Experimental results show that CMTA outperforms learning each task individually for the first time and achieves substantial performance improvements over the baselines.
</details>
<details>
<summary>摘要</summary>
在多任务强化学习领域，模块原则广泛应用，即将功能分解为不同模块，并合理地组合它们，以避免多任务之间的负面传递问题。然而，大多数现有的多任务RL方法只是将共享模块在任务级别 combinable，忽略了任务之间的冲突。此外，这些方法不考虑模块之间可能存在相似函数学习，从而限制模块的表达能力和多模块方法的泛化能力。在这篇论文中，我们提出了对这些限制的解决方案，即对比模块学习（CMTA）方法。CMTA方法通过对模块进行对比学习，并将共享模块在更细粒度的级别combine，使得在任务之间避免负面传递，提高多任务RL的表达能力和性能。我们在Meta-World，一个包含多种 робо控制任务的多任务RL benchmark上进行了实验。实验结果表明，CMTA方法可以在每个任务上学习一个新的任务，并且在基eline上达到了显著性能提升。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-real-time-neural-decoding-of-grasp"><a href="#Deep-Learning-for-real-time-neural-decoding-of-grasp" class="headerlink" title="Deep Learning for real-time neural decoding of grasp"></a>Deep Learning for real-time neural decoding of grasp</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01061">http://arxiv.org/abs/2311.01061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paolo Viviani, Ilaria Gesmundo, Elios Ghinato, Andres Agudelo-Toro, Chiara Vercellino, Giacomo Vitali, Letizia Bergamasco, Alberto Scionti, Marco Ghislieri, Valentina Agostini, Olivier Terzo, Hansjörg Scherberger</li>
<li>for: 这个论文的目的是提出一种基于深度学习的 neural decoding 方法，用于 классифика grasp type 的 neural 信号。</li>
<li>methods: 这个方法使用 LSTM 网络来处理时间序列中的 neural 数据（即脉冲列），并将其分类为不同的 grasp 类型。</li>
<li>results: 论文提出的方法在使用真实的 neural 记录数据上实现了显著的提升，并且在 simulated real-time decoding 中也表现出了优于之前的成果。<details>
<summary>Abstract</summary>
Neural decoding involves correlating signals acquired from the brain to variables in the physical world like limb movement or robot control in Brain Machine Interfaces. In this context, this work starts from a specific pre-existing dataset of neural recordings from monkey motor cortex and presents a Deep Learning-based approach to the decoding of neural signals for grasp type classification. Specifically, we propose here an approach that exploits LSTM networks to classify time series containing neural data (i.e., spike trains) into classes representing the object being grasped. The main goal of the presented approach is to improve over state-of-the-art decoding accuracy without relying on any prior neuroscience knowledge, and leveraging only the capability of deep learning models to extract correlations from data. The paper presents the results achieved for the considered dataset and compares them with previous works on the same dataset, showing a significant improvement in classification accuracy, even if considering simulated real-time decoding.
</details>
<details>
<summary>摘要</summary>
neural decoding 涉及将Brain Machine Interfaces中获取的脑电信号与物理世界中的变量相关联，如手部运动或机器人控制。在这种情况下，这项工作从一个具体的先前存在的脑电听记录数据集开始，并提出了基于深度学习的方法来解码脑电信号，以分类抓取类型。特别是，我们提议使用LSTM网络来分类时间序列中的脑电数据（即电听轨迹）为抓取物品的类别。我们的主要目标是超越现有的解码精度，不依赖任何先前的神经科学知识，只依靠深度学习模型来从数据中提取相关性。文章介绍的结果与同一数据集的先前工作进行比较，显示了明显的提高分类精度，即使考虑实时解码。
</details></li>
</ul>
<hr>
<h2 id="Adapt-On-the-Go-Behavior-Modulation-for-Single-Life-Robot-Deployment"><a href="#Adapt-On-the-Go-Behavior-Modulation-for-Single-Life-Robot-Deployment" class="headerlink" title="Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment"></a>Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01059">http://arxiv.org/abs/2311.01059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Annie S. Chen, Govind Chada, Laura Smith, Archit Sharma, Zipeng Fu, Sergey Levine, Chelsea Finn<br>for: 这种研究旨在帮助机器人在部署时适应不同于训练seen的情况，以便在实际世界中成功。methods: 这种方法基于在测试时一个episode内选择和适应已经预训练的行为的价值感知机制。results: 我们的方法能够在模拟和实际Go1四脚机器人上快速适应动力学变化，并在一些out-of-distribution情况下效果更高于现有方法，提高了机器人在部署时的适应能力。<details>
<summary>Abstract</summary>
To succeed in the real world, robots must cope with situations that differ from those seen during training. We study the problem of adapting on-the-fly to such novel scenarios during deployment, by drawing upon a diverse repertoire of previously learned behaviors. Our approach, RObust Autonomous Modulation (ROAM), introduces a mechanism based on the perceived value of pre-trained behaviors to select and adapt pre-trained behaviors to the situation at hand. Crucially, this adaptation process all happens within a single episode at test time, without any human supervision. We provide theoretical analysis of our selection mechanism and demonstrate that ROAM enables a robot to adapt rapidly to changes in dynamics both in simulation and on a real Go1 quadruped, even successfully moving forward with roller skates on its feet. Our approach adapts over 2x as efficiently compared to existing methods when facing a variety of out-of-distribution situations during deployment by effectively choosing and adapting relevant behaviors on-the-fly.
</details>
<details>
<summary>摘要</summary>
中文翻译：为了在真实世界中成功，机器人需要适应不同于训练中的情况。我们的方法RObust Autonomous Modulation（ROAM）使机器人能够根据每种情况选择和适应已经学习的行为。这个适应过程发生在测试时间内一个话，无需人工监督。我们的方法能够快速适应实际中的动力学变化，包括在真实的Go1 quadruped上并在轮式滑块上进行成功移动。相比已有方法，ROAM在面对多种异常情况时的适应效率高达2倍，通过选择和适应相应的行为来实现这一点。
</details></li>
</ul>
<hr>
<h2 id="Resilient-Multiple-Choice-Learning-A-learned-scoring-scheme-with-application-to-audio-scene-analysis"><a href="#Resilient-Multiple-Choice-Learning-A-learned-scoring-scheme-with-application-to-audio-scene-analysis" class="headerlink" title="Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis"></a>Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01052">http://arxiv.org/abs/2311.01052</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/victorletzelter/code-rmcl">https://github.com/victorletzelter/code-rmcl</a></li>
<li>paper_authors: Victor Letzelter, Mathieu Fontaine, Mickaël Chen, Patrick Pérez, Gael Richard, Slim Essid</li>
<li>for: 这篇论文是用于探讨多个目标的条件分布估计在回采测 Settings 中。</li>
<li>methods: 这篇论文使用了强化多选择学习 (rMCL)，是一种将多个假设组合成一个条件分布估计的方法。</li>
<li>results: rMCL 可以维持多个预测的多样性，并且可以从 Voronoi 分布中获得一个条件分布估计的概率解释。 实验显示，rMCL 可以在声音源localization问题中实现实用的应用和解释。<details>
<summary>Abstract</summary>
We introduce Resilient Multiple Choice Learning (rMCL), an extension of the MCL approach for conditional distribution estimation in regression settings where multiple targets may be sampled for each training input. Multiple Choice Learning is a simple framework to tackle multimodal density estimation, using the Winner-Takes-All (WTA) loss for a set of hypotheses. In regression settings, the existing MCL variants focus on merging the hypotheses, thereby eventually sacrificing the diversity of the predictions. In contrast, our method relies on a novel learned scoring scheme underpinned by a mathematical framework based on Voronoi tessellations of the output space, from which we can derive a probabilistic interpretation. After empirically validating rMCL with experiments on synthetic data, we further assess its merits on the sound source localization problem, demonstrating its practical usefulness and the relevance of its interpretation.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种强化多选学习（rMCL）方法，用于在回归 Setting 中估计 condition distribution，在每个训练输入上可能会采样多个目标。多选学习是一个简单的框架，用于处理多模态概率分布，使用 Winner-Takes-All（WTA）损失函数来评估一组假设。现有的 MCL 变体在回归 Setting 中强制合并假设，从而最终牺牲预测的多样性。相比之下，我们的方法基于一种新的学习得分方案，受到输出空间 Voronoi 分割的数学基础，从而可以 derivate 一种概率解释。经验 validate 了 rMCL 方法在 sintetic 数据上，然后进一步评估了它在声音源localization问题上的实用性和解释的相关性。
</details></li>
</ul>
<hr>
<h2 id="Application-and-Energy-Aware-Data-Aggregation-using-Vector-Synchronization-in-Distributed-Battery-less-IoT-Networks"><a href="#Application-and-Energy-Aware-Data-Aggregation-using-Vector-Synchronization-in-Distributed-Battery-less-IoT-Networks" class="headerlink" title="Application and Energy-Aware Data Aggregation using Vector Synchronization in Distributed Battery-less IoT Networks"></a>Application and Energy-Aware Data Aggregation using Vector Synchronization in Distributed Battery-less IoT Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01050">http://arxiv.org/abs/2311.01050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chetna Singhal, Subhrajit Barick, Rishabh Sonkar</li>
<li>for: 提供一种机制来聚合感知器数据并提供可持续的应用支持在分布式电池less IoT网络中。</li>
<li>methods: 提出了一种应用感知器 Task 和能量管理器（ATEM），以及一种基于向量同步的数据聚合器（VSDA）。ATEM 支持设备级联合能量收集和系统级energy-aware多应用管理。VSDA 使用 LSTM 模型预测可用 ambient 能量，并根据设备 Profiling 和应用任务速率设置相应的设备配置。</li>
<li>results: 提出的方案可以满足多样化应用需求，降低数据损失和包开销，提高硬件组件可用性，并使组件更早可用。相比之前的状态对照，提出的方案具有明显的优势。<details>
<summary>Abstract</summary>
The battery-less Internet of Things (IoT) devices are a key element in the sustainable green initiative for the next-generation wireless networks. These battery-free devices use the ambient energy, harvested from the environment. The energy harvesting environment is dynamic and causes intermittent task execution. The harvested energy is stored in small capacitors and it is challenging to assure the application task execution. The main goal is to provide a mechanism to aggregate the sensor data and provide a sustainable application support in the distributed battery-less IoT network. We model the distributed IoT network system consisting of many battery-free IoT sensor hardware modules and heterogeneous IoT applications that are being supported in the device-edge-cloud continuum. The applications require sensor data from a distributed set of battery-less hardware modules and there is provision of joint control over the module actuators. We propose an application-aware task and energy manager (ATEM) for the IoT devices and a vector-synchronization based data aggregator (VSDA). The ATEM is supported by device-level federated energy harvesting and system-level energy-aware heterogeneous application management. In our proposed framework the data aggregator forecasts the available power from the ambient energy harvester using long-short-term-memory (LSTM) model and sets the device profile as well as the application task rates accordingly. Our proposed scheme meets the heterogeneous application requirements with negligible overhead; reduces the data loss and packet delay; increases the hardware component availability; and makes the components available sooner as compared to the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
“无电池互联网关关（IoT）设备是下一代无线网络的可持续绿色计划的重要元素。这些无电池设备使用周围环境的能源，通过环境能量收集。环境能量收集是动态的，导致间歇性任务执行。收集到的能源储存在小电容器中，并困难保证应用程序执行。我们的目标是提供一个机制，将散布在多个无电池IoT感知硬件模组之间的感知数据聚合，并提供可持续的应用程序支持。我们模型了分布式IoT网络系统，该系统包括许多无电池IoT感知硬件模组和多种不同的IoT应用程序。这些应用程序需要从分布式的无电池硬件模组中获取感知数据，并且实现共同控制模组扭转器。我们提出了应用程序相关的任务和能量管理器（ATEM），以及基于分布式device级联合能源征收和系统级能源敏感多应用程序管理的vector-synchronization基于数据聚合器（VSDA）。ATEM运行在IoT设备上，并使用LSTM模型预测周围环境能量收集器中可用的电力，然后设定设备oprofile和应用程序任务速率。我们的提案可以满足多种不同的应用程序需求，并且对应用程序执行造成轻微过载; 实现数据损失和封包延误的减少; 提高硬件元件可用性; 并让元件更早地可用。”
</details></li>
</ul>
<hr>
<h2 id="Improving-Robustness-via-Tilted-Exponential-Layer-A-Communication-Theoretic-Perspective"><a href="#Improving-Robustness-via-Tilted-Exponential-Layer-A-Communication-Theoretic-Perspective" class="headerlink" title="Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective"></a>Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01047">http://arxiv.org/abs/2311.01047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bhagyapuranik/texp_for_robustness">https://github.com/bhagyapuranik/texp_for_robustness</a></li>
<li>paper_authors: Bhagyashree Puranik, Ahmad Beirami, Yao Qin, Upamanyu Madhow</li>
<li>for: 提高深度网络的 robustness，不仅仅靠 empirical risk minimization 和合适的数据增强。</li>
<li>methods: 提议使用通信理论来提高深度网络层输出的信号噪比，通过在学习和推理过程中进行神经竞争。</li>
<li>results: 实验表明，使用 TEXP 学习和推理可以提高图像数据集上的 robustness  against 噪音和其他常见损害，无需数据增强。此外，可以通过合适地结合 TEXP 和数据增强技术来获得更加累积的 robustness 提升。<details>
<summary>Abstract</summary>
State-of-the-art techniques for enhancing robustness of deep networks mostly rely on empirical risk minimization with suitable data augmentation. In this paper, we propose a complementary approach motivated by communication theory, aimed at enhancing the signal-to-noise ratio at the output of a neural network layer via neural competition during learning and inference. In addition to minimization of a standard end-to-end cost, neurons compete to sparsely represent layer inputs by maximization of a tilted exponential (TEXP) objective function for the layer. TEXP learning can be interpreted as maximum likelihood estimation of matched filters under a Gaussian model for data noise. Inference in a TEXP layer is accomplished by replacing batch norm by a tilted softmax, which can be interpreted as computation of posterior probabilities for the competing signaling hypotheses represented by each neuron. After providing insights via simplified models, we show, by experimentation on standard image datasets, that TEXP learning and inference enhances robustness against noise and other common corruptions, without requiring data augmentation. Further cumulative gains in robustness against this array of distortions can be obtained by appropriately combining TEXP with data augmentation techniques.
</details>
<details>
<summary>摘要</summary>
现代深度网络技术 mostly 依靠 empirical risk minimization 提高网络的Robustness。在这篇论文中，我们提出了一种基于通信理论的 complementary 方法，通过在学习和推理过程中进行神经竞争来提高神经网络层输出的信号噪声比。此外，我们还 minimization 标准的 end-to-end 成本函数，并且使 neurons 竞争 representation 层输入数据，通过 maximization 倾斜的 exponential 目标函数 (TEXP) 来提高层输出的稳定性。TEXP 学习可以 interpret 为 Gaussian 模型下的数据噪声的最大 likelihood estimation ，而 inference 可以通过 replacing batch norm with tilted softmax 来实现，这可以 interpret 为 computed posterior probabilities  для竞争的信号推测说。通过简化模型的分析和实验研究，我们发现了在标准图像数据集上，TEXP 学习和推理可以提高噪声和其他常见损害的Robustness，不需要数据增强。此外，可以通过合理地组合 TEXP 和数据增强技术来获得更加总的Robustness 提升。
</details></li>
</ul>
<hr>
<h2 id="Time-Independent-Information-Theoretic-Generalization-Bounds-for-SGLD"><a href="#Time-Independent-Information-Theoretic-Generalization-Bounds-for-SGLD" class="headerlink" title="Time-Independent Information-Theoretic Generalization Bounds for SGLD"></a>Time-Independent Information-Theoretic Generalization Bounds for SGLD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01046">http://arxiv.org/abs/2311.01046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Futoshi Futami, Masahiro Fujisawa</li>
<li>for: 这篇论文是为了提供一种新的信息理论性的普适化 bound，用于随机梯度劳伦豪玄（SGLD）的 sampling 和非托管优化研究。</li>
<li>methods: 这篇论文使用了对smoothness和dissipativity的假设，并通过关注时间演化的Kullback–Leibler差异来 derivethe generalization error bounds。</li>
<li>results: 这篇论文提供了一种时间独立的普适化 bound，其 decay to zero 随着样本大小增加，不管迭代次数和步长是否固定。此外，这篇论文还提供了首次在training和test loss相同时获得的信息理论性普适化 bound，该 bound 是时间独立的和步长无关的，从而导致了一个改进的过失 bound。<details>
<summary>Abstract</summary>
We provide novel information-theoretic generalization bounds for stochastic gradient Langevin dynamics (SGLD) under the assumptions of smoothness and dissipativity, which are widely used in sampling and non-convex optimization studies. Our bounds are time-independent and decay to zero as the sample size increases, regardless of the number of iterations and whether the step size is fixed. Unlike previous studies, we derive the generalization error bounds by focusing on the time evolution of the Kullback--Leibler divergence, which is related to the stability of datasets and is the upper bound of the mutual information between output parameters and an input dataset. Additionally, we establish the first information-theoretic generalization bound when the training and test loss are the same by showing that a loss function of SGLD is sub-exponential. This bound is also time-independent and removes the problematic step size dependence in existing work, leading to an improved excess risk bound by combining our analysis with the existing non-convex optimization error bounds.
</details>
<details>
<summary>摘要</summary>
我们提供了一个新的信息理论基准，用于测量泊reno-Langevin dynamics（SGLD）的扩展 bound，假设具有光滑性和耗散性的假设。我们的 bound 是时间独立的，随着样本大小增加而降至零，不过步长是固定的或者是变化的。与先前的研究不同，我们通过专注在 Kullback-Leibler 构成函数的时间演化中，从而得到了一个与实际数据之间的稳定性相关的一个上限。此外，我们还提出了第一个具有同一个训练和测试损失的信息理论扩展 bound，通过证明 SGLD 的损失函数是下减几何的，从而移除了先前的步长问题，导致一个改进的过失率 bound。
</details></li>
</ul>
<hr>
<h2 id="Better-with-Less-A-Data-Active-Perspective-on-Pre-Training-Graph-Neural-Networks"><a href="#Better-with-Less-A-Data-Active-Perspective-on-Pre-Training-Graph-Neural-Networks" class="headerlink" title="Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks"></a>Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01038">http://arxiv.org/abs/2311.01038</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/galina0217/apt">https://github.com/galina0217/apt</a></li>
<li>paper_authors: Jiarong Xu, Renhong Huang, Xin Jiang, Yuxuan Cao, Carl Yang, Chunping Wang, Yang Yang</li>
<li>for: 预训文件（Pre-training on graph neural networks）的目的是学习可转移的知识，以便在无标注数据下进行下游任务。</li>
<li>methods: 提议一种“更好减少”（Better-with-less）框架，即使用 fewer, but carefully chosen 数据进行预训。该框架包括一个图选择器和一个预训模型。图选择器选择最有代表性和指导性的数据点，根据图的内在特性和预测uncertainty。预训模型采用predictive uncertainty作为反馈，以测量模型对数据的信任程度。</li>
<li>results: 实验结果表明，提议的 APT 可以在 fewer 训练数据下得到更高的下游性能。<details>
<summary>Abstract</summary>
Pre-training on graph neural networks (GNNs) aims to learn transferable knowledge for downstream tasks with unlabeled data, and it has recently become an active research area. The success of graph pre-training models is often attributed to the massive amount of input data. In this paper, however, we identify the curse of big data phenomenon in graph pre-training: more training data do not necessarily lead to better downstream performance. Motivated by this observation, we propose a better-with-less framework for graph pre-training: fewer, but carefully chosen data are fed into a GNN model to enhance pre-training. The proposed pre-training pipeline is called the data-active graph pre-training (APT) framework, and is composed of a graph selector and a pre-training model. The graph selector chooses the most representative and instructive data points based on the inherent properties of graphs as well as predictive uncertainty. The proposed predictive uncertainty, as feedback from the pre-training model, measures the confidence level of the model in the data. When fed with the chosen data, on the other hand, the pre-training model grasps an initial understanding of the new, unseen data, and at the same time attempts to remember the knowledge learned from previous data. Therefore, the integration and interaction between these two components form a unified framework (APT), in which graph pre-training is performed in a progressive and iterative way. Experiment results show that the proposed APT is able to obtain an efficient pre-training model with fewer training data and better downstream performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>预训 graphs neural networks（GNNs）以学习可转移的知识，以便在无标注数据上下游任务中表现出色。预训图模型的成功通常归功于大量输入数据。但在这篇论文中，我们发现了“巨量数据诅咒”现象：更多的训练数据并不一定能提高下游性能。基于这一观察，我们提出了“更好减少”框架，即在GNN模型中更少，但更精心选择的数据来提高预训。我们称之为数据活跃图预训（APT）框架。APT框架由图选择器和预训模型组成，图选择器根据图的内在特性和预测不确定性选择最有代表性和指导性的数据点。预训模型则在被选择的数据点上学习初步理解新、未见数据，同时尝试记忆之前数据中学习的知识。因此，图选择器和预训模型之间的结合和互动形成了一个综合框架，在这个框架中，图预训是进行进行逐步和轮循的。实验结果表明，我们的APT能够在更少的训练数据下提取更好的预训模型，并且在下游任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Tensor-Trust-Interpretable-Prompt-Injection-Attacks-from-an-Online-Game"><a href="#Tensor-Trust-Interpretable-Prompt-Injection-Attacks-from-an-Online-Game" class="headerlink" title="Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game"></a>Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01011">http://arxiv.org/abs/2311.01011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell</li>
<li>for: 研究人员可以使用这些攻击和防御样本来研究LLMs中的攻击表现和防御机制。</li>
<li>methods: 这些攻击和防御样本都是由在线游戏《Tensor Trust》中的玩家生成的。</li>
<li>results: 许多模型对于这些攻击策略是易受攻击的，而且一些攻击策略可以在不同的环境中广泛应用。Translation:</li>
<li>for: 研究人员可以使用这些攻击和防御样本来研究LLMs中的攻击表现和防御机制。</li>
<li>methods: 这些攻击和防御样本都是由在线游戏《Tensor Trust》中的玩家生成的。</li>
<li>results: 许多模型对于这些攻击策略是易受攻击的，而且一些攻击策略可以在不同的环境中广泛应用。<details>
<summary>Abstract</summary>
While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based "defenses" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable stucture, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release all data and source code at https://tensortrust.ai/paper
</details>
<details>
<summary>摘要</summary>
While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third-party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based "defenses" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable structure, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release all data and source code at https://tensortrust.ai/paper.Here's the translation in Traditional Chinese:而 Large Language Models (LLMs) 在实际应用中越来越普遍，但它们仍然受到伪造第三方提示的攻击：这些攻击可以让系统设计者的意愿被覆盖。为了帮助研究人员研究这个问题，我们提供了超过 126,000 个提示插入攻击和 46,000 个防御措施，这些攻击和防御都是在线上游戏“Tensor Trust”中被玩家所创造的。根据我们所知，这是目前最大的人工生成的反对例集，用于测试 instruction-following LLMs。我们发现，这些攻击有许多易于理解的结构，并给出 LLMs 的弱点。我们还使用这个数据集来建立了两种提示插入的标准参考，即提取提示和夺取提示。我们的参考结果显示许多模型受到了这些攻击战略的威胁。此外，我们发现一些攻击战略可以通过游戏的不同组态对应到现场 LLM-based 应用中的攻击。我们在 <https://tensortrust.ai/paper> 发布了所有数据和源代码。
</details></li>
</ul>
<hr>
<h2 id="Scalable-Probabilistic-Forecasting-in-Retail-with-Gradient-Boosted-Trees-A-Practitioner’s-Approach"><a href="#Scalable-Probabilistic-Forecasting-in-Retail-with-Gradient-Boosted-Trees-A-Practitioner’s-Approach" class="headerlink" title="Scalable Probabilistic Forecasting in Retail with Gradient Boosted Trees: A Practitioner’s Approach"></a>Scalable Probabilistic Forecasting in Retail with Gradient Boosted Trees: A Practitioner’s Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00993">http://arxiv.org/abs/2311.00993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueying Long, Quang Bui, Grady Oktavian, Daniel F. Schmidt, Christoph Bergmeir, Rakshitha Godahewa, Seong Per Lee, Kaifeng Zhao, Paul Condylis</li>
<li>for: 这篇论文旨在解决大型电子商务公司面临的预测挑战，并且考虑到电子商务与传统零售之间的重要区别。</li>
<li>methods: 本论文提出了一个两层 Hierarchy 架构，首先采用一个顶部预测方法，将大量时间序列转换为较少的数量和较少的干扰，然后将预测结果转换为决策层预测。另外，本论文还提出了直接在下一层训练的方法，使用子样本进行训练。</li>
<li>results: 本论文通过评估多个数据集，包括自有数据集和 M5 竞赛数据集，并证明了两层 Hierarchy 架构的可扩展性和精准性。此外，本论文还证明了电子商务和传统零售数据集之间的重要区别。<details>
<summary>Abstract</summary>
The recent M5 competition has advanced the state-of-the-art in retail forecasting. However, we notice important differences between the competition challenge and the challenges we face in a large e-commerce company. The datasets in our scenario are larger (hundreds of thousands of time series), and e-commerce can afford to have a larger assortment than brick-and-mortar retailers, leading to more intermittent data. To scale to larger dataset sizes with feasible computational effort, firstly, we investigate a two-layer hierarchy and propose a top-down approach to forecasting at an aggregated level with less amount of series and intermittency, and then disaggregating to obtain the decision-level forecasts. Probabilistic forecasts are generated under distributional assumptions. Secondly, direct training at the lower level with subsamples can also be an alternative way of scaling. Performance of modelling with subsets is evaluated with the main dataset. Apart from a proprietary dataset, the proposed scalable methods are evaluated using the Favorita dataset and the M5 dataset. We are able to show the differences in characteristics of the e-commerce and brick-and-mortar retail datasets. Notably, our top-down forecasting framework enters the top 50 of the original M5 competition, even with models trained at a higher level under a much simpler setting.
</details>
<details>
<summary>摘要</summary>
最近的M5竞赛已经提高了零售预测的状态艺。然而，我们注意到竞赛挑战和我们在大型电商公司面临的挑战之间存在重要的差异。我们的场景中的数据集大于竞赛挑战的数据集，而电商可以拥有更多的产品种类，导致更多的间歇性数据。为了可靠地扩展到更大的数据集大小，我们首先investigate了两层层次结构，并提出了一种从上向下的预测方法，首先预测了汇总层次的预测，然后细化到获得决策层次的预测。我们采用了分布式预测方法。其次，直接在下一层使用子样本进行训练也可以作为扩展的方法。我们对主数据集进行评估模型的性能。除了自己的专有数据集外，我们还使用了Favorita数据集和M5数据集进行评估。我们能够发现电商和面向店铺零售数据集之间的不同特征。值得一提的是，我们的顶部预测框架在原M5竞赛中的top50中排名，即使在更简单的设置下进行训练。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Learning-of-Generative-Models-with-Chemical-Reaction-Network-Ensembles"><a href="#Autonomous-Learning-of-Generative-Models-with-Chemical-Reaction-Network-Ensembles" class="headerlink" title="Autonomous Learning of Generative Models with Chemical Reaction Network Ensembles"></a>Autonomous Learning of Generative Models with Chemical Reaction Network Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00975">http://arxiv.org/abs/2311.00975</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Poole, Thomas E. Ouldridge, Manoj Gopalkrishnan</li>
<li>for: 这篇论文目的是研究一种能够自主学习复杂环境的微米大小的袋型分子系统。</li>
<li>methods: 该论文使用控制理论、机器学习理论、化学反应网络理论和统计物理来开发一种通用的化学系统自主学习复杂分布的架构。该架构基于化学实现机器学习优化工具：Relative entropy cost function的梯度下降。</li>
<li>results: 该方法可以优化任何均衡的化学反应网络，并且可以使用隐藏单元学习复杂分布。这一结果被重新归类为一种形式的积分反馈控制。此外，由于使用了Explicit физи学模型，因此可以 derivate thermodynamic costs和trade-offs相关于这个过程。<details>
<summary>Abstract</summary>
Can a micron sized sack of interacting molecules autonomously learn an internal model of a complex and fluctuating environment? We draw insights from control theory, machine learning theory, chemical reaction network theory, and statistical physics to develop a general architecture whereby a broad class of chemical systems can autonomously learn complex distributions. Our construction takes the form of a chemical implementation of machine learning's optimization workhorse: gradient descent on the relative entropy cost function. We show how this method can be applied to optimize any detailed balanced chemical reaction network and that the construction is capable of using hidden units to learn complex distributions. This result is then recast as a form of integral feedback control. Finally, due to our use of an explicit physical model of learning, we are able to derive thermodynamic costs and trade-offs associated to this process.
</details>
<details>
<summary>摘要</summary>
可以把一个微米大小的袋形分子之间互动的系统视为自主学习一个复杂且波动的环境的内部模型吗？我们从控制理论、机器学习理论、化学反应网络理论和统计物理学中着手吸取灵感，开发了一种通用的化学系统自主学习复杂分布的总体体系。我们的设计通过使用化学实现机器学习优化工具：相对 entropy 成本函数的梯度下降。我们证明这种方法可以应用于优化任何均衡的化学反应网络，并且使用隐藏单元学习复杂分布。这一结果最后被重新映射为一种类型的积分反馈控制。由于我们使用了明确的物理学习模型，我们能够计算这个过程中的热力学成本和折衔。
</details></li>
</ul>
<hr>
<h2 id="Federated-Linear-Bandits-with-Finite-Adversarial-Actions"><a href="#Federated-Linear-Bandits-with-Finite-Adversarial-Actions" class="headerlink" title="Federated Linear Bandits with Finite Adversarial Actions"></a>Federated Linear Bandits with Finite Adversarial Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00973">http://arxiv.org/abs/2311.00973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Fan, Ruida Zhou, Chao Tian, Cong Shen</li>
<li>for: 这个论文是解决联合 linear 上下文选择问题的 federated 学习方法。</li>
<li>methods: 该方法基于 SupLinUCB 和 OFUL 算法的扩展，针对 adversarial 有穷finite action set 问题。</li>
<li>results: 该方法可以达到 $\tilde{O}(\sqrt{d T})$ 的总 regret，与最小最优下界匹配，是order-optimal（以polylog term为准）。<details>
<summary>Abstract</summary>
We study a federated linear bandits model, where $M$ clients communicate with a central server to solve a linear contextual bandits problem with finite adversarial action sets that may be different across clients. To address the unique challenges of adversarial finite action sets, we propose the FedSupLinUCB algorithm, which extends the principles of SupLinUCB and OFUL algorithms in linear contextual bandits. We prove that FedSupLinUCB achieves a total regret of $\tilde{O}(\sqrt{d T})$, where $T$ is the total number of arm pulls from all clients, and $d$ is the ambient dimension of the linear model. This matches the minimax lower bound and thus is order-optimal (up to polylog terms). We study both asynchronous and synchronous cases and show that the communication cost can be controlled as $O(d M^2 \log(d)\log(T))$ and $O(\sqrt{d^3 M^3} \log(d))$, respectively. The FedSupLinUCB design is further extended to two scenarios: (1) variance-adaptive, where a total regret of $\tilde{O} (\sqrt{d \sum \nolimits_{t=1}^{T} \sigma_t^2})$ can be achieved with $\sigma_t^2$ being the noise variance of round $t$; and (2) adversarial corruption, where a total regret of $\tilde{O}(\sqrt{dT} + d C_p)$ can be achieved with $C_p$ being the total corruption budget. Experiment results corroborate the theoretical analysis and demonstrate the effectiveness of FedSupLinUCB on both synthetic and real-world datasets.
</details>
<details>
<summary>摘要</summary>
我们研究了一个联邦线性帆风模型，其中 $M$ 客户端与中央服务器进行交互，以解决一个线性上下文帆风问题，其中可能存在不同客户端的敌对行动集。为了解决这些独特挑战，我们提议了 FedSupLinUCB 算法，该算法基于 SupLinUCB 和 OFUL 算法，并在线性上下文帆风中进行扩展。我们证明了 FedSupLinUCB 算法的总后悔为 $\tilde{O}(\sqrt{dT})$，其中 $T$ 是所有客户端共同抽取的枪下数，$d$ 是线性模型的 ambient 维度。这与最佳下界匹配，因此是顺序优化的（即polylog 项）。我们研究了同步和异步情况，并证明了通信成本可以控制为 $O(dM^2 \log(d) \log(T))$ 和 $O(\sqrt{d^3M^3} \log(d))$, 分别。FedSupLinUCB 设计还可以在以下两个方面进行扩展：（1）变量适应，可以实现 $\tilde{O}(\sqrt{d \sum_{t=1}^{T} \sigma_t^2})$ 的总后悔，其中 $\sigma_t^2$ 是循环 $t$ 的噪声方差；（2）敌对损害，可以实现 $\tilde{O}(\sqrt{dT} + dC_p)$ 的总后悔，其中 $C_p$ 是敌对损害预算。实验结果证明了理论分析的可靠性，并在 synthetic 和实际数据上证明了 FedSupLinUCB 的有效性。
</details></li>
</ul>
<hr>
<h2 id="Invariant-Feature-Subspace-Recovery-A-New-Class-of-Provable-Domain-Generalization-Algorithms"><a href="#Invariant-Feature-Subspace-Recovery-A-New-Class-of-Provable-Domain-Generalization-Algorithms" class="headerlink" title="Invariant-Feature Subspace Recovery: A New Class of Provable Domain Generalization Algorithms"></a>Invariant-Feature Subspace Recovery: A New Class of Provable Domain Generalization Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00966">http://arxiv.org/abs/2311.00966</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/InvarianceUnitTests">https://github.com/facebookresearch/InvarianceUnitTests</a></li>
<li>paper_authors: Haoxiang Wang, Gargi Balasubramaniam, Haozhe Si, Bo Li, Han Zhao<br>for:* 这个论文的目的是提出一种新的预测模型，以实现鲁棒的预测在不同环境下。methods:* 这个论文使用了一种新的算法，即非参数隐藏特征子空间恢复（ISR），来解决预测模型在不同环境下的鲁棒性问题。results:* 这个论文的实验结果表明，ISR可以在不同环境下实现鲁棒的预测，并且可以减少训练环境的数量，从而提高预测的效率。<details>
<summary>Abstract</summary>
Domain generalization asks for models trained over a set of training environments to generalize well in unseen test environments. Recently, a series of algorithms such as Invariant Risk Minimization (IRM) have been proposed for domain generalization. However, Rosenfeld et al. (2021) shows that in a simple linear data model, even if non-convexity issues are ignored, IRM and its extensions cannot generalize to unseen environments with less than $d_s+1$ training environments, where $d_s$ is the dimension of the spurious-feature subspace. In this work, we propose Invariant-feature Subspace Recovery (ISR): a new class of algorithms to achieve provable domain generalization across the settings of classification and regression problems. First, in the binary classification setup of Rosenfeld et al. (2021), we show that our first algorithm, ISR-Mean, can identify the subspace spanned by invariant features from the first-order moments of the class-conditional distributions, and achieve provable domain generalization with $d_s+1$ training environments. Our second algorithm, ISR-Cov, further reduces the required number of training environments to $O(1)$ using the information of second-order moments. Notably, unlike IRM, our algorithms bypass non-convexity issues and enjoy global convergence guarantees. Next, we extend ISR-Mean to the more general setting of multi-class classification and propose ISR-Multiclass, which leverages class information and provably recovers the invariant-feature subspace with $\lceil d_s/k\rceil+1$ training environments for $k$-class classification. Finally, for regression problems, we propose ISR-Regression that can identify the invariant-feature subspace with $d_s+1$ training environments. Empirically, we demonstrate the superior performance of our ISRs on synthetic benchmarks. Further, ISR can be used as post-processing methods for feature extractors such as neural nets.
</details>
<details>
<summary>摘要</summary>
领域总则要求训练在多个环境下的模型能够在未经见过的测试环境中general化良好。近年来，一些算法如不变风险最小化（IRM）在领域总则方面得到了提出。然而， Rosenfeld等人（2021）显示，在简单的线性数据模型中，即使忽略非拟合问题，IRM和其扩展也无法在未经见过环境中general化，需要至少有$d_s+1$个训练环境，其中$d_s$是假值特征空间的维度。在这种情况下，我们提出了不变特征子空间恢复（ISR）：一种新的算法，以实现领域总则的证明性普适性。在 Rosenfeld等人（2021）中的 binary 分类设置下，我们首先示出了我们的第一个算法，ISR-Mean，可以从类别Conditional distribution的首要幂中找到不变特征子空间的拟合，并实现领域总则的证明性，需要至少有$d_s+1$个训练环境。我们的第二个算法，ISR-Cov，进一步减少了需要的训练环境数量，使用类别的次要幂信息，可以在$O(1)$个训练环境下实现领域总则。与 IRM 不同的是，我们的算法不会遇到非拟合问题，并且具有全球整合性。接下来，我们扩展了 ISR-Mean 到更加一般的多类分类问题，并提出了 ISR-Multiclass，它可以在 $k$-class 分类问题中利用类信息来证明性地恢复不变特征子空间，需要至少有 $\lceil d_s/k\rceil+1$ 个训练环境。最后，我们为回归问题提出了 ISR-Regression，可以在 $d_s+1$ 个训练环境下实现领域总则。在实际中，我们通过synthetic benchmarks进行了证明性的实验，并证明了 ISR 可以作为特征提取器如 neural nets 的后处理方法。
</details></li>
</ul>
<hr>
<h2 id="On-Finding-Bi-objective-Pareto-optimal-Fraud-Prevention-Rule-Sets-for-Fintech-Applications"><a href="#On-Finding-Bi-objective-Pareto-optimal-Fraud-Prevention-Rule-Sets-for-Fintech-Applications" class="headerlink" title="On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications"></a>On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00964">http://arxiv.org/abs/2311.00964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyao Wen, Yin Lou</li>
<li>For: 这个论文关注于从初始规则集中找到高质量的规则子集，以提高风险预测决策的精度。* Methods: 该论文采用了解决问题选择在前（SSF）问题来找到非占优规则子集，并提出了一种基于谱分析的规则选择算法called SpectralRules。* Results: 实验表明，该方法可以在实际应用场景中提高风险预测的精度，并且比现有方法更有优势。<details>
<summary>Abstract</summary>
Rules are widely used in Fintech institutions to make fraud prevention decisions, since rules are highly interpretable thanks to their intuitive if-then structure. In practice, a two-stage framework of fraud prevention decision rule set mining is usually employed in large Fintech institutions. This paper is concerned with finding high-quality rule subsets in a bi-objective space (such as precision and recall) from an initial pool of rules. To this end, we adopt the concept of Pareto optimality and aim to find a set of non-dominated rule subsets, which constitutes a Pareto front. We propose a heuristic-based framework called PORS and we identify that the core of PORS is the problem of solution selection on the front (SSF). We provide a systematic categorization of the SSF problem and a thorough empirical evaluation of various SSF methods on both public and proprietary datasets. We also introduce a novel variant of sequential covering algorithm called SpectralRules to encourage the diversity of the initial rule set and we empirically find that SpectralRules further improves the quality of the found Pareto front. On two real application scenarios within Alipay, we demonstrate the advantages of our proposed methodology compared to existing work.
</details>
<details>
<summary>摘要</summary>
��worts are widely used in Fintech institutions to make fraud prevention decisions, since ��worts are highly interpretable thanks to their intuitive if-then structure. In practice, a two-stage framework of fraud prevention decision rule set mining is usually employed in large Fintech institutions. This paper is concerned with finding high-quality rule subsets in a bi-objective space (such as precision and recall) from an initial pool of ��worts. To this end, we adopt the concept of Pareto optimality and aim to find a set of non-dominated rule subsets, which constitutes a Pareto front. We propose a heuristic-based framework called PORS and we identify that the core of PORS is the problem of solution selection on the front (SSF). We provide a systematic categorization of the SSF problem and a thorough empirical evaluation of various SSF methods on both public and proprietary datasets. We also introduce a novel variant of sequential covering algorithm called SpectralRules to encourage the diversity of the initial rule set and we empirically find that SpectralRules further improves the quality of the found Pareto front. On two real application scenarios within Alipay, we demonstrate the advantages of our proposed methodology compared to existing work.Note:* "��worts" is the Simplified Chinese term for "rules"* "Pareto optimality" is translated as "Pareto优化" (Pareto yòu jì)* "non-dominated rule subsets" is translated as "非主导��wort subset" (fēi zhǔ dǎo ��wort subset)* "solution selection on the front" is translated as "前台解决方案选择" (qián tai jiě jué fāng yì xuǎn chōu)* "SSF problem" is translated as "SSF问题" (SSF wèn tí)* "sequential covering algorithm" is translated as "顺序覆盖算法" (shù xì fù kè algoritmo)* "SpectralRules" is translated as "光谱规则" (guāng xiàng guī fā)
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Fair-Federated-Learning-Based-on-Reinforcement-Learning"><a href="#Dynamic-Fair-Federated-Learning-Based-on-Reinforcement-Learning" class="headerlink" title="Dynamic Fair Federated Learning Based on Reinforcement Learning"></a>Dynamic Fair Federated Learning Based on Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00959">http://arxiv.org/abs/2311.00959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weikang Chen, Junping Du, Yingxia Shao, Jia Wang, Yangxi Zhou</li>
<li>for: 提高 Federated Learning 中设备之间不公正 Representation 问题的解决方案</li>
<li>methods: 使用动态q公平 Federated Learning 算法（DQFFL），结合 reinforcement learning，以减少设备聚合不公正性和提高 Federated Learning 中所有组合的待遇公平性</li>
<li>results: DQFFL 在评估全局 Federated 模型性能、公平性和收敛速度三个方面，与现有方法相比，表现更佳，可以减少设备之间的不公正 Representation 问题，提高 Federated Learning 的总性能和公平性。<details>
<summary>Abstract</summary>
Federated learning enables a collaborative training and optimization of global models among a group of devices without sharing local data samples. However, the heterogeneity of data in federated learning can lead to unfair representation of the global model across different devices. To address the fairness issue in federated learning, we propose a dynamic q fairness federated learning algorithm with reinforcement learning, called DQFFL. DQFFL aims to mitigate the discrepancies in device aggregation and enhance the fairness of treatment for all groups involved in federated learning. To quantify fairness, DQFFL leverages the performance of the global federated model on each device and incorporates {\alpha}-fairness to transform the preservation of fairness during federated aggregation into the distribution of client weights in the aggregation process. Considering the sensitivity of parameters in measuring fairness, we propose to utilize reinforcement learning for dynamic parameters during aggregation. Experimental results demonstrate that our DQFFL outperforms the state-of-the-art methods in terms of overall performance, fairness and convergence speed.
</details>
<details>
<summary>摘要</summary>
“ federated learning 可以实现多个设备之间的共同训练和优化全域模型，而不需要分享本地数据样本。但是，跨设备数据的不均衡可能导致全球模型的不公平代表性。为了解决联邦学习中的公平问题，我们提出了一个动态q公平联邦学习算法（DQFFL）。DQFFL 的目的是在联邦学习中缓和设备聚合的差异，并对所有参与联邦学习的集群进行公平的对待。为了量化公平，DQFFL 利用每个设备上全球联邦模型的表现，并通过 α-公平来转换维护公平的保存到联邦聚合过程中的分布。考虑到联邦学习中的参数敏感性，我们提出了在联邦聚合过程中使用循环学习来动态参数。实验结果显示，我们的 DQFFL 在全面性、公平性和融合速度三方面比前者方法更好。”
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Smoothed-Gradient-Descent-Ascent-for-Federated-Minimax-Optimization"><a href="#Stochastic-Smoothed-Gradient-Descent-Ascent-for-Federated-Minimax-Optimization" class="headerlink" title="Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization"></a>Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00944">http://arxiv.org/abs/2311.00944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Shen, Minhui Huang, Jiawei Zhang, Cong Shen</li>
<li>for: 这个论文是为了研究如何在联合学习中使用缓和技术来解决非中心非对称最小最大值问题。</li>
<li>methods: 这个论文提出了一种新的算法 called Federated Stochastic Smoothed Gradient Descent Ascent (FESS-GDA), 它利用了缓和技术来解决联合学习中的非中心非对称最小最大值问题。</li>
<li>results: 论文证明FESS-GDA可以在几种联合学习任务中uniformly使用，并提供了新的或更好的分析融合结果。  Additionally, the paper shows the practical efficiency of FESS-GDA in training generative adversarial networks (GANs) and fair classification.<details>
<summary>Abstract</summary>
In recent years, federated minimax optimization has attracted growing interest due to its extensive applications in various machine learning tasks. While Smoothed Alternative Gradient Descent Ascent (Smoothed-AGDA) has proved its success in centralized nonconvex minimax optimization, how and whether smoothing technique could be helpful in federated setting remains unexplored. In this paper, we propose a new algorithm termed Federated Stochastic Smoothed Gradient Descent Ascent (FESS-GDA), which utilizes the smoothing technique for federated minimax optimization. We prove that FESS-GDA can be uniformly used to solve several classes of federated minimax problems and prove new or better analytical convergence results for these settings. We showcase the practical efficiency of FESS-GDA in practical federated learning tasks of training generative adversarial networks (GANs) and fair classification.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Defect-Prediction-from-Unrealistic-Data"><a href="#Learning-Defect-Prediction-from-Unrealistic-Data" class="headerlink" title="Learning Defect Prediction from Unrealistic Data"></a>Learning Defect Prediction from Unrealistic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00931">http://arxiv.org/abs/2311.00931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamel Alrashedy, Vincent J. Hellendoorn, Alessandro Orso</li>
<li>for: 本研究的目的是调查大规模synthetic数据生成的问题，以及如何使用这些数据来改进代码理解和生成任务的模型性能。</li>
<li>methods: 本研究使用了一种方法来识别大量但不真实的数据集中的有用样本，以提高代码理解和生成任务中的模型性能。这种方法基于模型学习的高维嵌入，对真实和人工添加bug的程序进行评分，并且只允许使用最相似的样本进行训练。</li>
<li>results: 研究结果表明，使用这种方法可以在两种流行的代码预训练模型中提高代码理解和生成任务的性能。此外，研究还发现了大规模synthetic数据生成的限制，并提出了在实际应用中预测漏洞和bug的AI模型的限制。<details>
<summary>Abstract</summary>
Pretrained models of code, such as CodeBERT and CodeT5, have become popular choices for code understanding and generation tasks. Such models tend to be large and require commensurate volumes of training data, which are rarely available for downstream tasks. Instead, it has become popular to train models with far larger but less realistic datasets, such as functions with artificially injected bugs. Models trained on such data, however, tend to only perform well on similar data, while underperforming on real world programs. In this paper, we conjecture that this discrepancy stems from the presence of distracting samples that steer the model away from the real-world task distribution. To investigate this conjecture, we propose an approach for identifying the subsets of these large yet unrealistic datasets that are most similar to examples in real-world datasets based on their learned representations. Our approach extracts high-dimensional embeddings of both real-world and artificial programs using a neural model and scores artificial samples based on their distance to the nearest real-world sample. We show that training on only the nearest, representationally most similar samples while discarding samples that are not at all similar in representations yields consistent improvements across two popular pretrained models of code on two code understanding tasks. Our results are promising, in that they show that training models on a representative subset of an unrealistic dataset can help us harness the power of large-scale synthetic data generation while preserving downstream task performance. Finally, we highlight the limitations of applying AI models for predicting vulnerabilities and bugs in real-world applications
</details>
<details>
<summary>摘要</summary>
预训模型，如CodeBERT和CodeT5，在代码理解和生成任务中变得流行。这些模型通常很大，需要相应的培训数据量，但这些数据很少存在下游任务中。因此，有人开始使用更大的 yet less realistic 的数据集来训练模型，例如在函数中 искусственного注入漏洞。但这些模型通常只在类似数据上表现出色，在实际世界程序中表现不佳。在这篇论文中，我们推测这种差异是因为大量的干扰样本，使模型偏离实际世界任务分布。为了研究这一点，我们提出了一种方法，可以在大规模的不真实数据集中标识最类似实际世界数据的子集。我们的方法使用神经网络模型提取实际世界和人工生成的程序高维表示，然后根据这些表示的距离对人工样本进行评分。我们发现，只具有最类似实际世界样本的表示，而不是所有人工样本，可以提高两种流行的预训模型在两个代码理解任务上的性能。我们的结果启示，可以通过在大规模的不真实数据集中提取代码的表示，来使用大规模的人工数据生成来帮助我们利用大规模的人工数据来提高下游任务性能。最后，我们强调了在实际应用中使用人工智能模型预测漏洞和漏洞的限制。
</details></li>
</ul>
<hr>
<h2 id="A-Review-and-Roadmap-of-Deep-Causal-Model-from-Different-Causal-Structures-and-Representations"><a href="#A-Review-and-Roadmap-of-Deep-Causal-Model-from-Different-Causal-Structures-and-Representations" class="headerlink" title="A Review and Roadmap of Deep Causal Model from Different Causal Structures and Representations"></a>A Review and Roadmap of Deep Causal Model from Different Causal Structures and Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00923">http://arxiv.org/abs/2311.00923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hang Chen, Keqing Du, Chenguang Li, Xinyu Yang</li>
<li>for: 本研究旨在探讨 causal 模型与深度学习在复杂数据集上的整合，包括图像或文本组件之间的 causal 关系。</li>
<li>methods: 本研究提出了将 causal 数据分类为三类：定态数据、半定态数据和未定态数据，根据 causal 结构和表示方式。</li>
<li>results: 研究表明，定态数据主要应用于传统 causal 场景中的统计数据，半定态数据包括深度学习中常见的时间序列、图像、文本等数据格式，而未定态数据是一个新兴的研究领域，尚未得到充分发展。<details>
<summary>Abstract</summary>
The fusion of causal models with deep learning introducing increasingly intricate data sets, such as the causal associations within images or between textual components, has surfaced as a focal research area. Nonetheless, the broadening of original causal concepts and theories to such complex, non-statistical data has been met with serious challenges. In response, our study proposes redefinitions of causal data into three distinct categories from the standpoint of causal structure and representation: definite data, semi-definite data, and indefinite data. Definite data chiefly pertains to statistical data used in conventional causal scenarios, while semi-definite data refers to a spectrum of data formats germane to deep learning, including time-series, images, text, and others. Indefinite data is an emergent research sphere inferred from the progression of data forms by us. To comprehensively present these three data paradigms, we elaborate on their formal definitions, differences manifested in datasets, resolution pathways, and development of research. We summarize key tasks and achievements pertaining to definite and semi-definite data from myriad research undertakings, present a roadmap for indefinite data, beginning with its current research conundrums. Lastly, we classify and scrutinize the key datasets presently utilized within these three paradigms.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate text into Simplified ChineseThe fusion of causal models with deep learning, introducing increasingly intricate data sets such as the causal associations within images or between textual components, has emerged as a major research focus. However, the extension of original causal concepts and theories to such complex, non-statistical data has posed significant challenges. In response, our study proposes redefinitions of causal data into three distinct categories based on causal structure and representation: definite data, semi-definite data, and indefinite data. Definite data primarily refers to statistical data used in conventional causal scenarios, while semi-definite data encompasses a range of data formats relevant to deep learning, including time-series, images, text, and others. Indefinite data is an emerging research area that we have inferred from the evolution of data forms. To comprehensively present these three data paradigms, we delineate their formal definitions, differences manifest in datasets, resolution pathways, and development of research. We summarize key tasks and achievements pertaining to definite and semi-definite data from numerous research endeavors, present a roadmap for indefinite data, beginning with its current research conundrums. Finally, we categorize and scrutinize the key datasets currently employed within these three paradigms.
</details></li>
</ul>
<hr>
<h2 id="MIST-Defending-Against-Membership-Inference-Attacks-Through-Membership-Invariant-Subspace-Training"><a href="#MIST-Defending-Against-Membership-Inference-Attacks-Through-Membership-Invariant-Subspace-Training" class="headerlink" title="MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training"></a>MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00919">http://arxiv.org/abs/2311.00919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacheng Li, Ninghui Li, Bruno Ribeiro</li>
<li>For: The paper is written to address the problem of membership inference attacks in machine learning, specifically defending against attacks that try to determine whether a particular instance was used to train a model.* Methods: The paper introduces a novel method called Membership-Invariant Subspace Training (MIST) that leverages counterfactually-invariant representations and subspace learning methods to defend against MI attacks. MIST avoids overfitting the vulnerable instances without significantly impacting other instances.* Results: The paper shows that MIST outperforms other state-of-the-art MI defenses while resulting in minimal reduction in testing accuracy, based on extensive experimental studies against several SOTA MI attacks.<details>
<summary>Abstract</summary>
In Member Inference (MI) attacks, the adversary try to determine whether an instance is used to train a machine learning (ML) model. MI attacks are a major privacy concern when using private data to train ML models. Most MI attacks in the literature take advantage of the fact that ML models are trained to fit the training data well, and thus have very low loss on training instances. Most defenses against MI attacks therefore try to make the model fit the training data less well. Doing so, however, generally results in lower accuracy. We observe that training instances have different degrees of vulnerability to MI attacks. Most instances will have low loss even when not included in training. For these instances, the model can fit them well without concerns of MI attacks. An effective defense only needs to (possibly implicitly) identify instances that are vulnerable to MI attacks and avoids overfitting them. A major challenge is how to achieve such an effect in an efficient training process. Leveraging two distinct recent advancements in representation learning: counterfactually-invariant representations and subspace learning methods, we introduce a novel Membership-Invariant Subspace Training (MIST) method to defend against MI attacks. MIST avoids overfitting the vulnerable instances without significant impact on other instances. We have conducted extensive experimental studies, comparing MIST with various other state-of-the-art (SOTA) MI defenses against several SOTA MI attacks. We find that MIST outperforms other defenses while resulting in minimal reduction in testing accuracy.
</details>
<details>
<summary>摘要</summary>
在 Member Inference（MI）攻击中，敌方尝试确定训练机器学习（ML）模型中使用的实例。MI攻击是使用私人数据训练ML模型的隐私问题。大多数MI攻击在文献中利用了ML模型对训练数据的适应性，因此大多数防御措施是通过减少模型对训练实例的适应程度来进行。但是，这通常会导致减少准确性。我们发现训练实例有不同的抵触MI攻击的程度。大多数实例会在没有包含在训练中时仍然有低损失。这些实例上，模型可以很好地适应它们无需MI攻击的担忧。一个有效的防御仅需要（可能是隐式的）标识MI攻击的抵触实例，并避免对它们进行过度适应。一个主要挑战是如何在有效的训练过程中实现这一目标。我们利用了两种最近的表示学习技术：对于抵触不变的表示和子空间学习方法，我们提出了一种新的Membership-Invariant Subspace Training（MIST）方法，以防止MI攻击。MIST不会过度适应抵触实例，而不会对其他实例产生重要的影响。我们对MIST方法进行了广泛的实验研究，与其他多种现状顶峰MI防御方法进行比较，并对多个SOTAMI攻击进行了测试。我们发现MIST方法在减少测试损失的同时，与其他防御方法相比，具有最佳的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/02/cs.LG_2023_11_02/" data-id="closbros800sb0g88ac6tbt9e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/02/eess.IV_2023_11_02/" class="article-date">
  <time datetime="2023-11-02T09:00:00.000Z" itemprop="datePublished">2023-11-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/02/eess.IV_2023_11_02/">eess.IV - 2023-11-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unveiling-the-deep-plumbing-system-of-a-volcano-by-a-reflection-matrix-analysis-of-seismic-noise"><a href="#Unveiling-the-deep-plumbing-system-of-a-volcano-by-a-reflection-matrix-analysis-of-seismic-noise" class="headerlink" title="Unveiling the deep plumbing system of a volcano by a reflection matrix analysis of seismic noise"></a>Unveiling the deep plumbing system of a volcano by a reflection matrix analysis of seismic noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01296">http://arxiv.org/abs/2311.01296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elsa Giraudat, Arnaud Burtin, Arthur Le Ber, Mathias Fink, Jean-Christophe Komorowski, Alexandre Aubry</li>
<li>for: 这篇论文是为了研究加德鲁瓦岛拉索韦雷尔火山的内部结构和水热系统而写的。</li>
<li>methods: 这篇论文使用了震动噪声记录在罕见的地震仪数组上的方法，以揭示火山内部的液体和岩石多尺度不均质和非线性动态。</li>
<li>results: 研究人员通过分析震动噪声的时空横列相关性，实际获得了火山内部的各种反射特征，以及水热系统的几何结构和动态行为。这些结果为火山的概念模型和高级监测带来了新的视角和信息。<details>
<summary>Abstract</summary>
In geophysics, volcanoes are particularly difficult to image because of the multi-scale heterogeneities of fluids and rocks that compose them and their complex non-linear dynamics. By exploiting seismic noise recorded by a sparse array of geophones, we are able to reveal the magmatic and hydrothermal plumbing system of La Soufri\`ere volcano in Guadeloupe. Spatio-temporal cross-correlation of seismic noise actually provides the impulse responses between virtual geophones located inside the volcano. The resulting reflection matrix can be exploited to numerically perform an auto-focus of seismic waves on any reflector of the underground. An unprecedented view on the volcano's inner structure is obtained at a half-wavelength resolution. This innovative observable provides fundamental information for the conceptual modeling and high-resolution monitoring of volcanoes.
</details>
<details>
<summary>摘要</summary>
在地球物理学中，火山特别难以成像，因为它们由多级不同性的液体和岩石组成，以及复杂非线性动态。我们通过利用地震噪声记录的稀疏阵列地震仪，能够揭示拉撒韦尔火山（La Soufri\`ere）在格瑞达岛的 магмати和 hidrotermal 沟管系统。在空间时间交叉相关的地震噪声中，实际提供了虚拟地震仪内部火山中的响应函数。得到的反射矩阵可以 numerically 进行自动фокус处理，以获得任何反射器的地下高分辨率视图。这种创新的可见提供了基本信息 для概念模型和高分辨率监测火山。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/02/eess.IV_2023_11_02/" data-id="closbroz801990g880zfr9i39" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/02/eess.SP_2023_11_02/" class="article-date">
  <time datetime="2023-11-02T08:00:00.000Z" itemprop="datePublished">2023-11-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/02/eess.SP_2023_11_02/">eess.SP - 2023-11-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Supervised-Learning-Based-Real-Time-Adaptive-Beamforming-On-board-Multibeam-Satellites"><a href="#Supervised-Learning-Based-Real-Time-Adaptive-Beamforming-On-board-Multibeam-Satellites" class="headerlink" title="Supervised Learning Based Real-Time Adaptive Beamforming On-board Multibeam Satellites"></a>Supervised Learning Based Real-Time Adaptive Beamforming On-board Multibeam Satellites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01334">http://arxiv.org/abs/2311.01334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Flor Ortiz, Juan A. Vasquez-Peralvo, Jorge Querol, Eva Lagunas, Jorge L. Gonzalez Rios, Marcele O. K. Mendonca, Luis Garces, Victor Monzon Baeza, Symeon Chatzinotas</li>
<li>for: 本研究旨在提高卫星通信系统的资源管理效率，以满足动态交通需求和宽频带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽带宽�<details>
<summary>Abstract</summary>
Satellite communications (SatCom) are crucial for global connectivity, especially in the era of emerging technologies like 6G and narrowing the digital divide. Traditional SatCom systems struggle with efficient resource management due to static multibeam configurations, hindering quality of service (QoS) amidst dynamic traffic demands. This paper introduces an innovative solution - real-time adaptive beamforming on multibeam satellites with software-defined payloads in geostationary orbit (GEO). Utilizing a Direct Radiating Array (DRA) with circular polarization in the 17.7 - 20.2 GHz band, the paper outlines DRA design and a supervised learning-based algorithm for on-board beamforming. This adaptive approach not only meets precise beam projection needs but also dynamically adjusts beamwidth, minimizes sidelobe levels (SLL), and optimizes effective isotropic radiated power (EIRP).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Statistical-Results-of-Multivariate-Fox-H-Function-for-Exact-Performance-Analysis-of-RIS-Assisted-Wireless-Communication"><a href="#Statistical-Results-of-Multivariate-Fox-H-Function-for-Exact-Performance-Analysis-of-RIS-Assisted-Wireless-Communication" class="headerlink" title="Statistical Results of Multivariate Fox-H Function for Exact Performance Analysis of RIS-Assisted Wireless Communication"></a>Statistical Results of Multivariate Fox-H Function for Exact Performance Analysis of RIS-Assisted Wireless Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01312">http://arxiv.org/abs/2311.01312</a></li>
<li>repo_url: None</li>
<li>paper_authors: vinay kumar chapala, S. M. Zafaruddin<br>for: This paper aims to provide an exact analysis of the ergodic capacity and outage probability of RIS-assisted wireless systems using a multivariate Fox-H function to characterize the statistical properties of the signal-to-noise ratio (SNR).methods: The proposed approach uses a novel method to obtain the distribution of the sum of independent and non-identically distributed (i.ni.d) random variables characterized by the multivariate Fox-H function. The authors also develop a general framework for an exact analysis of the ergodic capacity when the multivariate Fox-H function characterizes the statistics of SNR.results: The paper derives exact expressions for the outage probability and ergodic capacity of RIS-assisted wireless systems under Rician fading channels with phase errors. The results are validated through computer simulations, and the performance of the RIS-assisted system is demonstrated under various practically relevant scenarios for a better performance assessment.<details>
<summary>Abstract</summary>
Existing research provides statistical results on the sum of single-variate Fox-H functions to analyze the performance of diversity receivers and reconfigurable intelligent surfaces (RIS) based wireless systems. There is a research gap in exact performance analysis when more than a single-variate Fox-H function represents the statistical characterization of wireless systems. In this paper, we propose a novel approach to obtain the distribution of the sum of independent and non-identically distributed (i.ni.d) random variables characterized by the multivariate Fox-H function. Further, we develop a general framework for an exact analysis of the ergodic capacity when the multivariate Fox-H function characterizes the statistics of signal-to-noise ratio (SNR). We apply the derived results to conduct an exact performance analysis of outage probability and ergodic capacity, taking an example of RIS-assisted communication over Rician fading channels with phase errors. We conduct computer simulations to validate the exact analysis and demonstrate performance of the RIS-assisted system under various practically relevant scenarios for a better performance assessment.
</details>
<details>
<summary>摘要</summary>
existingu research provides statistical results on the sum of single-variate Fox-H functions to analyze the performance of diversity receivers and reconfigurable intelligent surfaces (RIS) based wireless systems. There is a research gap in exact performance analysis when more than a single-variate Fox-H function represents the statistical characterization of wireless systems. In this paper, we propose a novel approach to obtain the distribution of the sum of independent and non-identically distributed (i.ni.d) random variables characterized by the multivariate Fox-H function. Further, we develop a general framework for an exact analysis of the ergodic capacity when the multivariate Fox-H function characterizes the statistics of signal-to-noise ratio (SNR). We apply the derived results to conduct an exact performance analysis of outage probability and ergodic capacity, taking an example of RIS-assisted communication over Rician fading channels with phase errors. We conduct computer simulations to validate the exact analysis and demonstrate performance of the RIS-assisted system under various practically relevant scenarios for a better performance assessment.Here's the word-for-word translation of the text into Simplified Chinese:现有研究提供了单variate Fox-H函数的统计结果，以分析多样化接收机和智能表面（RIS）基于无线系统的性能。研究存在多variate Fox-H函数表征无线系统的统计分析凌隙。在这篇论文中，我们提出了一种新的方法，以获得独立和非相同分布（i.ni.d）随机变量的总和的分布，这些随机变量由多variate Fox-H函数表征。此外，我们还提出了一个通用的框架，用于准确分析吞吐量（ergodic capacity），当多variate Fox-H函数表征无线系统的统计。我们应用得出的结果，进行了准确的性能分析，包括失业概率和吞吐量的分析，使用RIS协助通信系统在Rician折射投射通道上的示例。我们还进行了计算机实验，以验证准确分析，并在不同的实际情况下，展示RIS协助系统的性能。
</details></li>
</ul>
<hr>
<h2 id="Map-assisted-TDOA-Localization-Enhancement-Based-On-CNN"><a href="#Map-assisted-TDOA-Localization-Enhancement-Based-On-CNN" class="headerlink" title="Map-assisted TDOA Localization Enhancement Based On CNN"></a>Map-assisted TDOA Localization Enhancement Based On CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01291">http://arxiv.org/abs/2311.01291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Chen, Tianqi Xiang, Xi Chen, Xin Zhang</li>
<li>for: 本研究旨在提高无线位置准备技术中的NLOS多Path效应 correction。</li>
<li>methods: 该研究提出一种基于Convolutional Neural Network (CNN)的本地化错误修正方法，通过映射中的障碍物特征提取来预测NLOS多Path效应引起的本地化错误。</li>
<li>results: 研究表明，使用CNN预测NLOS多Path效应引起的本地化错误后，对比TDOA本地化算法的结果，NLOS多Path效应 correction表现出色，可以大幅提高TDOA的准备精度。<details>
<summary>Abstract</summary>
For signal processing related to localization technologies, non line of sight (NLOS) multipaths have great impact over the localization error level. This study proposes a localization correction method based on convolution neural network (CNN) that extracts obstacles' features from maps to predict the localization errors caused by NLOS effects. A novel compensation scheme is developed and structured around the localization error predicted by the CNN. Four prediction tasks are executed over the building distributions within the maps and the propagation model in urban zones, resulting in CNN models with high prediction accuracy. Finally, a thorough comparison of the accuracy performance between the time difference of arrival (TDOA) localization algorithm and the results after the error compensation reveals that, generally, the CNN prediction approach demonstrates a great localization error correction performance. It can be observed that the powerful feature extraction capability of CNN can be exploited by processing surrounding maps to predict localization error distribution, which has great potential in further enhancement of TDOA performance under challenging scenarios with rich multi-path propagation.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为信号处理相关的本地化技术，非线视程（NLOS）多paths有着很大的影响于本地化错误水平。本研究提出了基于卷积神经网络（CNN）的本地化修正方法，该方法通过从地图中提取障碍物特征来预测NLOS效应所导致的本地化错误。基于这个预测结果，我们开发了一种新的补偿方案，并将其结构化为本地化错误预测值。在 urbana 区域内，执行了四个预测任务，这些任务是基于建筑物分布和propagation模型。结果显示，CNN模型具有高度预测精度。最后，对TDOA本地化算法和补偿后的结果进行了严格的比较，可以看到，通过CNN预测方法可以对本地化错误进行高度的修正。这表明，CNN的强大特征提取能力可以通过处理周围的地图来预测本地化错误分布，这有很大的潜力，可以进一步提高TDOA性能在复杂的场景下。
</details></li>
</ul>
<hr>
<h2 id="ExPECA-An-Experimental-Platform-for-Trustworthy-Edge-Computing-Applications"><a href="#ExPECA-An-Experimental-Platform-for-Trustworthy-Edge-Computing-Applications" class="headerlink" title="ExPECA: An Experimental Platform for Trustworthy Edge Computing Applications"></a>ExPECA: An Experimental Platform for Trustworthy Edge Computing Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01279">http://arxiv.org/abs/2311.01279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samie Mostafavi, Vishnu Narayanan Moothedath, Stefan Rönngren, Neelabhro Roy, Gourav Prateek Sharma, Sangwon Seo, Manuel Olguín Muñoz, James Gross</li>
<li>for: 这篇论文旨在提供一个拥有综合终端到终端实验和高度复制性的edge计算和无线通信研发测试环境。</li>
<li>methods: 该测试环境基于OpenStack-based Chameleon Infrastructure（CHI）框架，利用其灵活性和操作 convenienceto provide a highly controlled underground facility for wireless experiments.</li>
<li>results: 通过使用OpenRTiST应用程序，研究人员可以在ExPECA测试环境中进行灵活的实验和性能分析，并且可以利用容器化计算环境来支持多种研究领域和实验设置。<details>
<summary>Abstract</summary>
This paper presents ExPECA, an edge computing and wireless communication research testbed designed to tackle two pressing challenges: comprehensive end-to-end experimentation and high levels of experimental reproducibility. Leveraging OpenStack-based Chameleon Infrastructure (CHI) framework for its proven flexibility and ease of operation, ExPECA is located in a unique, isolated underground facility, providing a highly controlled setting for wireless experiments. The testbed is engineered to facilitate integrated studies of both communication and computation, offering a diverse array of Software-Defined Radios (SDR) and Commercial Off-The-Shelf (COTS) wireless and wired links, as well as containerized computational environments. We exemplify the experimental possibilities of the testbed using OpenRTiST, a latency-sensitive, bandwidth-intensive application, and analyze its performance. Lastly, we highlight an array of research domains and experimental setups that stand to gain from ExPECA's features, including closed-loop applications and time-sensitive networking.
</details>
<details>
<summary>摘要</summary>
To demonstrate the experimental capabilities of the testbed, we use OpenRTiST, a latency-sensitive, bandwidth-intensive application, and analyze its performance. Additionally, we highlight a variety of research domains and experimental setups that can benefit from ExPECA's features, including closed-loop applications and time-sensitive networking.Translated into Simplified Chinese:这篇论文介绍了ExPECA，一个Edge computing和无线通信研究测试床，旨在解决两个紧迫的挑战：全面的端到端实验和高水平的实验复制性。利用OpenStack基础设施的Chameleon基础设施（CHI）框架，ExPECA位于一个独特的地下设施中，提供了一个高度控制的无线实验环境。测试床设计用于探索无线通信和计算的集成研究，提供了一系列Software-Defined Radio（SDR）和商业可用的无线和有线链路，以及容器化的计算环境。我们使用OpenRTiST，一个延迟敏感、带宽敏感的应用程序，来示例测试床的实验可能性，并分析其性能。此外，我们还高亮了一些研究领域和实验设置，可以从ExPECA的特点中受益，包括关闭Loop应用和时间敏感网络。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Federated-Learning-on-the-Edge-over-Wireless-Mesh-Networks"><a href="#Decentralized-Federated-Learning-on-the-Edge-over-Wireless-Mesh-Networks" class="headerlink" title="Decentralized Federated Learning on the Edge over Wireless Mesh Networks"></a>Decentralized Federated Learning on the Edge over Wireless Mesh Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01186">http://arxiv.org/abs/2311.01186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelaziz Salama, Achilleas Stergioulis, Syed Ali Zaidi, Des McLernon</li>
<li>for: 这个研究旨在提出一个替代方案，即分布式机器学习（Decentralized Federated Learning，DFL），并在无线网络作为通信基础建构。</li>
<li>methods: 这个研究使用了Stochastic Geometry Theory和物理干扰模型进行网络性能分析，并且对应不同的统计汇编方法（FedAvg、Krum和Median方法）进行系统实验。</li>
<li>results: 研究结果显示，将使用Genetic Algorithm进行压缩可以将 particiants的本地模型大小压缩到基线中的一半，并且与中央化 Federated Learning 和传统的分布式机器学习相比，在类别任务中实现相似的精度和平均损失。此外，它还可以实现大量数据的压缩和通信带宽的减少。<details>
<summary>Abstract</summary>
The rapid growth of Internet of Things (IoT) devices has generated vast amounts of data, leading to the emergence of federated learning as a novel distributed machine learning paradigm. Federated learning enables model training at the edge, leveraging the processing capacity of edge devices while preserving privacy and mitigating data transfer bottlenecks. However, the conventional centralized federated learning architecture suffers from a single point of failure and susceptibility to malicious attacks. In this study, we delve into an alternative approach called decentralized federated learning (DFL) conducted over a wireless mesh network as the communication backbone. We perform a comprehensive network performance analysis using stochastic geometry theory and physical interference models, offering fresh insights into the convergence analysis of DFL. Additionally, we conduct system simulations to assess the proposed decentralized architecture under various network parameters and different aggregator methods such as FedAvg, Krum and Median methods. Our model is trained on the widely recognized EMNIST dataset for benchmarking handwritten digit classification. To minimize the model's size at the edge and reduce communication overhead, we employ a cutting-edge compression technique based on genetic algorithms. Our simulation results reveal that the compressed decentralized architecture achieves performance comparable to the baseline centralized architecture and traditional DFL in terms of accuracy and average loss for our classification task. Moreover, it significantly reduces the size of shared models over the wireless channel by compressing participants' local model sizes to nearly half of their original size compared to the baselines, effectively reducing complexity and communication overhead.
</details>
<details>
<summary>摘要</summary>
“因互联网物联网（IoT）装置的快速增长，导致机器学习（ML）模型训练的大量数据生成，带来了分布式机器学习（FedML）的出现。 FedML可以在边缘进行模型训练，利用边缘设备的处理能力，同时保持隐私和减少数据传输瓶须。但是，传统中央化的 FedML架构受到单点失效和黑客攻击的威胁。在这篇研究中，我们研究了一种分布式 FedML（DFL）架构，通过无线 mesh 网络作为通信基础建构。我们使用Stochastic Geometry Theory和物理干扰模型进行网络性能分析，提供新的混合分析方法。此外，我们还进行系统实验，评估我们的对称架构在不同网络参数和组合方法（如FedAvg、Krum和Median方法）下的表现。我们的模型是基于EMNIST资料集，用于测试手写数字分类。为了优化模型在边缘的存储和减少通信负载，我们使用 cutting-edge 干扰技术，基于遗传算法实现模型压缩。我们的实验结果显示，压缩的分布式架构可以与中央化架构和传统的 DFL 相比，在准确性和平均损失方面具有相似的表现，同时对于我们的分类任务，对模型的共享大小进行了有效的压缩，从而减少了网络负载和复杂度。”
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Traffic-Congestion-Management-with-Fog-Computing-A-Simulation-based-Investigation-using-iFog-Simulator"><a href="#Enhanced-Traffic-Congestion-Management-with-Fog-Computing-A-Simulation-based-Investigation-using-iFog-Simulator" class="headerlink" title="Enhanced Traffic Congestion Management with Fog Computing: A Simulation-based Investigation using iFog-Simulator"></a>Enhanced Traffic Congestion Management with Fog Computing: A Simulation-based Investigation using iFog-Simulator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01181">http://arxiv.org/abs/2311.01181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alzahraa Elsayed, Khalil Mohamed, Hany Harb</li>
<li>for: 提高智能城市压力堵塞管理系统的准确延迟计算，以便在云计算中处理大量数据。</li>
<li>methods: fog computing技术，以实现在边缘处理而不是云计算。</li>
<li>results: 比较其他方法，包括IOV和STL，并确定提出的系统模型在各种纪录中的优劣表现。<details>
<summary>Abstract</summary>
Accurate latency computation is essential for the Internet of Things (IoT) since the connected devices generate a vast amount of data that is processed on cloud infrastructure. However, the cloud is not an optimal solution. To overcome this issue, fog computing is used to enable processing at the edge while still allowing communication with the cloud. Many applications rely on fog computing, including traffic management. In this paper, an Intelligent Traffic Congestion Mitigation System (ITCMS) is proposed to address traffic congestion in heavily populated smart cities. The proposed system is implemented using fog computing and tested in a crowded city. Its performance is evaluated based on multiple metrics, such as traffic efficiency, energy savings, reduced latency, average traffic flow rate, and waiting time. The obtained results are compared with similar techniques that tackle the same issue. The results obtained indicate that the execution time of the simulation is 4,538 seconds, and the delay in the application loop is 49.67 seconds. The paper addresses various issues, including CPU usage, heap memory usage, throughput, and the total average delay, which are essential for evaluating the performance of the ITCMS. Our system model is also compared with other models to assess its performance. A comparison is made using two parameters, namely throughput and the total average delay, between the ITCMS, IOV (Internet of Vehicle), and STL (Seasonal-Trend Decomposition Procedure based on LOESS). Consequently, the results confirm that the proposed system outperforms the others in terms of higher accuracy, lower latency, and improved traffic efficiency.
</details>
<details>
<summary>摘要</summary>
优质延迟计算是智能物联网（IoT）中不可或缺的，因为连接设备生成的数据量非常大，需要在云基础设施上处理。但云不是最佳解决方案。为了解决这个问题，fog计算被用来启用边缘处理，同时仍允许与云进行通信。许多应用程序依赖于fog计算，包括交通管理。在这篇论文中，一个智能交通堵塞缓解系统（ITCMS）被提出，以解决智能城市中高度拥堵的交通问题。该系统采用fog计算实现，并在热点城市进行测试。其性能被评估基于多个纪录，包括交通效率、能源储存、延迟、平均交通流速和等待时间。获得的结果与其他解决方案进行比较，结果表明，提案系统在多个纪录中表现更高精度、更低延迟和更好的交通效率。system model comparison also is made with other models to assess its performance. A comparison is made using two parameters, namely throughput and the total average delay, between the ITCMS, IOV (Internet of Vehicle), and STL (Seasonal-Trend Decomposition Procedure based on LOESS). Consequently, the results confirm that the proposed system outperforms the others in terms of higher accuracy, lower latency, and improved traffic efficiency.
</details></li>
</ul>
<hr>
<h2 id="Modulation-Design-and-Optimization-for-RIS-Assisted-Symbiotic-Radios"><a href="#Modulation-Design-and-Optimization-for-RIS-Assisted-Symbiotic-Radios" class="headerlink" title="Modulation Design and Optimization for RIS-Assisted Symbiotic Radios"></a>Modulation Design and Optimization for RIS-Assisted Symbiotic Radios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01167">http://arxiv.org/abs/2311.01167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hu Zhou, Bowen Cai, Qianqian Zhang, Ruizhe Long, Yiyang Pei, Ying-Chang Liang</li>
<li>For: 提高RIS协助SR系统中干扰链的性能，解决SR系统中简单干扰链的干扰问题。* Methods: 提出一种新的Modulation scheme，将RIS的频率差分成两部分：征文件不变部分和征文件变部分，用于协助主传和传输次信号。通过解决复合信号的检测问题，提高主和次传信号的比特错误率性能。* Results: 对比 conventional modulation scheme，提出的新Modulation scheme能够提高SR系统中干扰链的性能，并且可以在主传链受阻时提供更好的性能。<details>
<summary>Abstract</summary>
In reconfigurable intelligent surface (RIS)-assisted symbiotic radio (SR), the RIS acts as a secondary transmitter by modulating its information bits over the incident primary signal and simultaneously assists the primary transmission, then a cooperative receiver is used to jointly decode the primary and secondary signals. Most existing works of SR focus on using RIS to enhance the reflecting link while ignoring the ambiguity problem for the joint detection caused by the multiplication relationship of the primary and secondary signals. Particularly, in case of a blocked direct link, joint detection will suffer from severe performance loss due to the ambiguity, when using the conventional on-off keying and binary phase shift keying modulation schemes for RIS. To address this issue, we propose a novel modulation scheme for RIS-assisted SR that divides the phase-shift matrix into two components: the symbol-invariant and symbol-varying components, which are used to assist the primary transmission and carry the secondary signal, respectively. To design these two components, we focus on the detection of the composite signal formed by the primary and secondary signals, through which a problem of minimizing the bit error rate (BER) of the composite signal is formulated to improve both the BER performance of the primary and secondary ones. By solving the problem, we derive the closed-form solution of the optimal symbol-invariant and symbol-varying components, which is related to the channel strength ratio of the direct link to the reflecting link. Moreover, theoretical BER performance is analyzed. Finally, simulation results show the superiority of the proposed modulation scheme over its conventional counterpart.
</details>
<details>
<summary>摘要</summary>
在协助式广播（SR）中，协助器（RIS） behave as a secondary transmitter by modulating its information bits over the incident primary signal and simultaneously assisting the primary transmission，然后使用协同接收器来联合解码主要和次要信号。大多数现有的SR工作都忽略了在共同检测中的混淆问题，特别是在直接链路被阻断时，共同检测将受到严重的性能损失due to the multiplication relationship between the primary and secondary signals。为解决这个问题，我们提出了一种新的模ulation scheme for RIS-assisted SR，该方案将分解阶跃矩阵into two components：符号不变和符号变components，用于帮助主传输和传输次要信号，分别。为设计这两个组成部分，我们关注了主传输和次要传输的复合信号的检测，并通过解决这个问题，我们得到了closed-form solution of the optimal symbol-invariant and symbol-varying components，该解决方案与直接链路到反射链路的通信强度比进行关系。此外，我们还进行了理论性能分析。最后，我们通过实验结果展示了我们的提案方案的优越性。
</details></li>
</ul>
<hr>
<h2 id="Combating-Inter-Operator-Pilot-Contamination-in-Reconfigurable-Intelligent-Surfaces-Assisted-Multi-Operator-Networks"><a href="#Combating-Inter-Operator-Pilot-Contamination-in-Reconfigurable-Intelligent-Surfaces-Assisted-Multi-Operator-Networks" class="headerlink" title="Combating Inter-Operator Pilot Contamination in Reconfigurable Intelligent Surfaces Assisted Multi-Operator Networks"></a>Combating Inter-Operator Pilot Contamination in Reconfigurable Intelligent Surfaces Assisted Multi-Operator Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01151">http://arxiv.org/abs/2311.01151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doğa Gürgünoğlu, Emil Björnson, Gábor Fodor</li>
<li>for: 研究了在多个运营商协助网络中出现的新型飞行器污染现象， где多个运营商为其各自的用户提供服务。运营商使用专用频率带，但每个智能表层在多个频率带中不可避免地反射用户设备的传输上行信号。因此，同时反射的试验信号在渠道估计阶段引入了一种新的交互运营商飞行器污染效应。</li>
<li>methods:  investigate the implications of this effect in systems with either deterministic or correlated Rayleigh fading channels, specifically focusing on its impact on channel estimation quality, signal equalization, and channel capacity.</li>
<li>results:  numerical results demonstrate the substantial degradation in system performance caused by this phenomenon and highlight the pressing need to address inter-operator pilot contamination in multi-operator RIS deployments. To combat the negative effect of this new type of pilot contamination, we propose to use orthogonal RIS configurations during uplink pilot transmission, which can mitigate or eliminate the negative effect of inter-operator pilot contamination at the expense of some inter-operator information exchange and orchestration.<details>
<summary>Abstract</summary>
In this paper, we study a new kind of pilot contamination appearing in multi-operator reconfigurable intelligent surfaces (RIS) assisted networks, where multiple operators provide services to their respective served users. The operators use dedicated frequency bands, but each RIS inadvertently reflects the transmitted uplink signals of the user equipment devices in multiple bands. Consequently, the concurrent reflection of pilot signals during the channel estimation phase introduces a new inter-operator pilot contamination effect. We investigate the implications of this effect in systems with either deterministic or correlated Rayleigh fading channels, specifically focusing on its impact on channel estimation quality, signal equalization, and channel capacity. The numerical results demonstrate the substantial degradation in system performance caused by this phenomenon and highlight the pressing need to address inter-operator pilot contamination in multi-operator RIS deployments. To combat the negative effect of this new type of pilot contamination, we propose to use orthogonal RIS configurations during uplink pilot transmission, which can mitigate or eliminate the negative effect of inter-operator pilot contamination at the expense of some inter-operator information exchange and orchestration.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Comparison-of-Different-Segmentations-in-Automated-Detection-of-Hypertension-Using-Electrocardiography-with-Empirical-Mode-Decomposition"><a href="#Comparison-of-Different-Segmentations-in-Automated-Detection-of-Hypertension-Using-Electrocardiography-with-Empirical-Mode-Decomposition" class="headerlink" title="Comparison of Different Segmentations in Automated Detection of Hypertension Using Electrocardiography with Empirical Mode Decomposition"></a>Comparison of Different Segmentations in Automated Detection of Hypertension Using Electrocardiography with Empirical Mode Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01142">http://arxiv.org/abs/2311.01142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Y. E. Erdoğan, A. Narin</li>
<li>for: 旨在早期和准确地诊断高血压（HPT）病例，使用电子心脏agram（ECG）信号进行自动识别。</li>
<li>methods: 使用Empirical Mode Decomposition方法提取5层Intrinsic Mode Function（IMF）信号，并从每个IMF中提取9个特征进行分类。</li>
<li>results: 使用5-fold cross-validation技术，在ECG数据集中实现了99.9991%和99.9989%的准确率，表明该方法在诊断HPT中具有潜在的用途。<details>
<summary>Abstract</summary>
Hypertension (HPT) refers to a condition where the pressure exerted on the walls of arteries by blood pumped from the heart to the body reaches levels that can lead to various ailments. Annually, a significant number of lives are lost globally due to diseases linked to HPT. Therefore, the early and accurate diagnosis of HPT is of utmost importance. This study aimed to automatically and with minimal error detect patients suffering from HPT by utilizing electrocardiogram (ECG) signals. The research involved the collection of ECG signals from two distinct groups. These groups consisted of ECG data of both five thousand and ten thousand data points in length, respectively. The performance in HPT detection was evaluated using entropy measurements derived from the 5-layer Intrinsic Mode Function(IMF) signals through the application of the Empirical Mode Decomposition method. The resulting performances were compared based on the nine features extracted from each IMF. To summarize, employing the 5-fold cross-validation technique, the most exceptional accuracy rates achieved were 99.9991% and 99.9989% for ECG data of lengths five thousand and ten thousand,respectively, using decision tree algorithms. These remarkable performance results indicate the potential usefulness of this method in assisting medical professionals to identify individuals with HPT.
</details>
<details>
<summary>摘要</summary>
高血压（HPT）是指心脏吐出到体内的血液压力超过了正常范围，可能导致多种疾病。每年全球都有很多人因与HPT相关的疾病而丧生。因此，早期准确诊断HPT的重要性是自然的。本研究目的是使用电子心电团（ECG）信号自动、准确地诊断患有HPT的患者。研究中收集了ECG信号的两个组。这两个组分别包括5000和10000个数据点的ECG数据。通过使用预测方法，对5层内含函数（IMF）信号进行了Entropy测量，以评估HPT检测的性能。基于每个IMF提取的9个特征进行比较。总结来说，通过使用5fold交叉验证法，使用决策树算法的情况下，ECG数据的5000和10000个数据点的性能最高达99.9991%和99.9989%。这些优异的性能结果表明这种方法在帮助医生诊断HPT患者有潜在的用途。
</details></li>
</ul>
<hr>
<h2 id="Noncontact-Detection-of-Sleep-Apnea-Using-Radar-and-Expectation-Maximization-Algorithm"><a href="#Noncontact-Detection-of-Sleep-Apnea-Using-Radar-and-Expectation-Maximization-Algorithm" class="headerlink" title="Noncontact Detection of Sleep Apnea Using Radar and Expectation-Maximization Algorithm"></a>Noncontact Detection of Sleep Apnea Using Radar and Expectation-Maximization Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01084">http://arxiv.org/abs/2311.01084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takato Koda, Shigeaki Okumura, Hirofumi Taki, Satoshi Hamada, Hironobu Sunadome, Susumu Sato, Kazuo Chin, Takuya Sakamoto</li>
<li>for: 这研究旨在提出一种基于激光的新方法，用于准确地检测呼吸障碍事件。</li>
<li>methods: 这方法使用了望前预设的预算-最大化算法，将正常和不正常呼吸模式中的呼吸特征提取出来，实现了灵活的呼吸检测能力，无需任何实验参数。</li>
<li>results: 这研究通过对五名呼吸障碍症状患者进行同时的诊断和激光测量，发现这方法可以每小时检测出呼吸障碍事件4.8次，与传统的阈值基本方法相比，提高了准确性1.8倍，显示了我们提出的方法的有效性。<details>
<summary>Abstract</summary>
Sleep apnea syndrome requires early diagnosis because this syndrome can lead to a variety of health problems. If sleep apnea events can be detected in a noncontact manner using radar, we can then avoid the discomfort caused by the contact-type sensors that are used in conventional polysomnography. This study proposes a novel radar-based method for accurate detection of sleep apnea events. The proposed method uses the expectation-maximization algorithm to extract the respiratory features that form normal and abnormal breathing patterns, resulting in an adaptive apnea detection capability without any requirement for empirical parameters. We conducted an experimental quantitative evaluation of the proposed method by performing polysomnography and radar measurements simultaneously in five patients with the symptoms of sleep apnea syndrome. Through these experiments, we show that the proposed method can detect the number of apnea and hypopnea events per hour with an error of 4.8 times/hour; this represents an improvement in the accuracy by 1.8 times when compared with the conventional threshold-based method and demonstrates the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
睡眠呼吸暂停综合症需早期诊断，因为这种病种可能会导致多种健康问题。如果可以使用雷达探测sleep apnea事件而不用触摸式传感器，我们就可以避免由传统多somnography所带来的不适感。本研究提出了一种基于雷达的新方法，能够准确检测sleep apnea事件。该方法使用了望望-最大化算法提取呼吸特征，从而实现了不需任何参数的自适应apnea检测能力。我们在五名睡眠呼吸暂停症病人身上同时进行了多somnography和雷达测量，并通过实验证明了我们的提议方法可以准确地检测每小时的apnea和低吸量事件数量，误差为4.8次/小时，与传统的阈值基于方法相比提高精度1.8倍，这表明了我们的提议方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Fourier-Analysis-of-Signals-on-Directed-Acyclic-Graphs-DAG-Using-Graph-Zero-Padding"><a href="#Fourier-Analysis-of-Signals-on-Directed-Acyclic-Graphs-DAG-Using-Graph-Zero-Padding" class="headerlink" title="Fourier Analysis of Signals on Directed Acyclic Graphs (DAG) Using Graph Zero-Padding"></a>Fourier Analysis of Signals on Directed Acyclic Graphs (DAG) Using Graph Zero-Padding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01073">http://arxiv.org/abs/2311.01073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ljubisa Stankovic, Milos Dakovic, Ali Bagheri Bardi, Milos Brajovic, Isidora Stankovic</li>
<li>for: 这篇论文是为了解决directed acyclic graphs（DAGs）中的causal关系、依赖关系和流动关系模型中的一个问题，即spectral analysis变得不实用，因为adjacency matrix的特征值全部为零。</li>
<li>methods: 该论文提出了一种graph zero-paddingapproach，即在原有的DAG结构上添加更多的vertices，并将这些vertices的信号值设为零。这种方法可以帮助实现DAG上的spectral评估，即计算顶点域散射无 aliasing的问题。</li>
<li>results: 该论文的研究结果表明，通过使用graph zero-paddingapproach，可以实现DAG上的spectral评估，并且不会因为Graph结构的变化而带来干扰。这种方法可以帮助解决DAG中的一些问题，如causal关系、依赖关系和流动关系的模型。<details>
<summary>Abstract</summary>
Directed acyclic graphs (DAGs) are used for modeling causal relationships, dependencies, and flows in various systems. However, spectral analysis becomes impractical in this setting because the eigendecomposition of the adjacency matrix yields all eigenvalues equal to zero. This inherent property of DAGs results in an inability to differentiate between frequency components of signals on such graphs. This problem can be addressed by adding edges in DAG. However, this approach changes the physics of the considered problem. To address this limitation, we propose a graph zero-padding approach. This approach involves augmenting the original DAG with additional vertices that are connected to the existing structure. The added vertices are characterized by signal values set to zero. The proposed technique enables the spectral evaluation of system outputs on DAGs, that is the computation of vertex-domain convolution without the adverse effects of aliasing due to changes in graph structure.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Continuous-Fluid-Antenna-Systems-Modeling-and-Analysis"><a href="#Continuous-Fluid-Antenna-Systems-Modeling-and-Analysis" class="headerlink" title="Continuous Fluid Antenna Systems: Modeling and Analysis"></a>Continuous Fluid Antenna Systems: Modeling and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01058">http://arxiv.org/abs/2311.01058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Constantinos Psomas, Peter J. Smith, Himal A. Suraweera, Ioannis Krikidis</li>
<li>for: 这篇论文是关于流体天线（FA）技术的研究，FA 技术可以带来无线网络中的灵活性和重新配置能力。</li>
<li>methods: 该论文使用了一个综合的框架来设计和分析流体天线系统（CFAS），并 derive了关于水平交叉率（LCR）和均匀干扰比率（SIR）过程中的closed-form analytical表达。</li>
<li>results: 研究结果表明，CFAS 在比较 discrete 天线系统时表现更好，并提供了 FA 系统的性能限制。<details>
<summary>Abstract</summary>
Fluid antennas (FAs) is a promising technology for introducing flexibility and reconfigurability in wireless networks. Recent research efforts have highlighted the potential gains that can be achieved in comparison to conventional antennas. These works assume that the FA has a discrete number of positions that the liquid can take. However, from a practical standpoint, the liquid moves in a continuous fashion to any point inside the FA. In this paper, we focus on a continuous FA system (CFAS) and present a general framework for its design and analytical evaluation. In particular, we derive closed-form analytical expressions for the level crossing rate (LCR) and the average fade duration of the continuous signal-to-interference ratio (SIR) process over the FA's length. Then, by leveraging the LCR expression, we characterize the system's outage performance with a bound on the cumulative distribution function of the SIR's supremum. Our results confirm that the CFAS outperforms its discrete counterpart and thus provides the performance limits of FA-based systems.
</details>
<details>
<summary>摘要</summary>
“流体天线（FA）是一种可能带来flexibility和重新配置的无线网络技术。近期的研究努力表明，相比于传统天线，FA可以获得更大的优化。”“这些研究假设FA具有确定数量的位置，但实际上，流体可以在FA中任意位置移动。在本文中，我们专注于连续FA系统（CFAS），并提出一个通用的设计框架和分析评估。”“具体来说，我们 derivated 点横过率（LCR）和平均障碍时间的关注率表达式，并使用LCR表达式来描述系统的失灵性表现。”“我们的结果显示，CFAS在比较于确定FA系统时表现更好，因此提供了FA-based系统的性能限制。”
</details></li>
</ul>
<hr>
<h2 id="From-5G-to-6G-Revolutionizing-Satellite-Networks-through-TRANTOR-Foundation"><a href="#From-5G-to-6G-Revolutionizing-Satellite-Networks-through-TRANTOR-Foundation" class="headerlink" title="From 5G to 6G: Revolutionizing Satellite Networks through TRANTOR Foundation"></a>From 5G to 6G: Revolutionizing Satellite Networks through TRANTOR Foundation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01055">http://arxiv.org/abs/2311.01055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pol Henarejos, Xavier Artiga, Miguel A. Vázquez, Màrius Caus, Musbah Shaat, Joan Bas, Lluís Blanco, Ana I. Pérez-Neira</li>
<li>for: 本研究旨在开发一个标准化的5G生态系统，以满足卫星互联网提供商的需求，提供更高的数据速率、更大的网络容量、更低的延迟、更可靠的连接和更高的可用性。</li>
<li>methods: 本研究使用了5G技术，包括开发多频道多轨道天线、gNodeB和UE5G非地面网络设备，以满足卫星交通的多种需求和负载。</li>
<li>results: 本研究实现了一个可扩展、安全、高效的卫星网络管理解决方案，可以满足卫星互联网提供商的增长需求和多样化需求。<details>
<summary>Abstract</summary>
5G technology will drastically change the way satellite internet providers deliver services by offering higher data speeds, massive network capacity, reduced latency, improved reliability and increased availability. A standardised 5G ecosystem will enable adapting 5G to satellite needs. The EU-funded TRANTOR project will seek to develop novel and secure satellite network management solutions that allow scaling up heterogeneous satellite traffic demands and capacities in a cost-effective and highly dynamic way. Researchers also target the development of flexible 6G non-terrestrial access architectures. The focus will be on the design of a multi-orbit and multi-band antenna for satellite user equipment (UE), as well as the development of gNodeB (gNB) and UE 5G non-terrestrial network equipment to support multi-connectivity.
</details>
<details>
<summary>摘要</summary>
5G技术将完全改变卫星互联网提供商如何提供服务，提供更高的数据速率、更大的网络容量、减少延迟、改善可靠性和提高可用性。标准化的5G生态系统将帮助适应5G卫星需求。欧盟资金支持的TRANTOR项目将努力开发新的安全卫星网络管理解决方案，以满足卫星流量需求和容量的扩展和弹性scaling。研究人员还将 targets developing flexible 6G non-terrestrial access architectures。关注的方面包括卫星用户设备（UE）多天线多频段设计，以及支持多连接的gNodeB（gNB）和UE 5G非地面网络设备的开发。
</details></li>
</ul>
<hr>
<h2 id="Mathematical-Properties-of-the-Zadoff-Chu-Sequences"><a href="#Mathematical-Properties-of-the-Zadoff-Chu-Sequences" class="headerlink" title="Mathematical Properties of the Zadoff-Chu Sequences"></a>Mathematical Properties of the Zadoff-Chu Sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01035">http://arxiv.org/abs/2311.01035</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Gregoratti, Xavier Arteaga, Joaquim Broquetas</li>
<li>for: 这篇论文是一份收集了知名结果的赫杯-珠 sequences 论文，包括所有证明，使用一致的数学符号，为易引用。</li>
<li>methods: 论文 derivates a formula to compute the first term (频率为零) of the discrete Fourier transform of a Zadoff-Chu sequence $x_u[n]$ of prime length $N_{\text{ZC}$ and root index $u$, with constant complexity independent of the sequence length.</li>
<li>results: 论文得到了一个公式，可以计算 Zadoff-Chu sequence $x_u[n]$ 的抽象傅立叶变换的首项（频率为零），并且这个公式与赫杯-珠 sequences 的特性有关。<details>
<summary>Abstract</summary>
This paper is a compilation of well-known results about Zadoff-Chu sequences, including all proofs with a consistent mathematical notation, for easy reference. Moreover, for a Zadoff-Chu sequence $x_u[n]$ of prime length $N_{\text{ZC}$ and root index $u$, a formula is derived that allows computing the first term (frequency zero) of its discrete Fourier transform, $X_u[0]$, with constant complexity independent of the sequence length, as opposed to accumulating all its $N_{\text{ZC}$ terms. The formula stems from a famous result in analytic number theory and is an interesting complement to the fact that the discrete Fourier transform of a Zadoff-Chu sequence is itself a Zadoff-Chu sequence whose terms are scaled by $X_u[0]$. Finally, the paper concludes with a brief analysis of time-continuous signals derived from Zadoff-Chu sequences, especially those obtained by OFDM-modulating a Zadoff-Chu sequence.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is a translation of the text into Traditional Chinese, which is the standard writing system used in Taiwan and other countries.Here's the translation of the text into Simplified Chinese, which is used in mainland China:这篇论文收集了关于佐道夫-楚 sequences的Well-known Results，包括所有证明，使用一致的数学符号，为易参照。此外，对佐道夫-楚序列 $x_u[n]$ 的 prime length $N_{\text{ZC}$ 和根指数 $u$， derivation 一个公式，可以计算其抽象傅立叙 Transform 的首项（频率为零），X_u[0]，与Constant complexity 独立于序列长度，不同于积累所有 $N_{\text{ZC}$ 项。这个公式基于分析数论中著名的结果，是一种有趣的补充，因为抽象傅立叙 Transform 的佐道夫-楚序列自身是一个扩展的佐道夫-楚序列，其项被扩展为 $X_u[0]$。最后，文章 briefly analyzes time-continuous signals derived from Zadoff-Chu sequences, particularly those obtained by OFDM-modulating a Zadoff-Chu sequence.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/02/eess.SP_2023_11_02/" data-id="closbrp0t01d20g88d2bj33tg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/6/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
