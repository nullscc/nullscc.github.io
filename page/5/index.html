
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/5/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_08_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/17/eess.IV_2023_08_17/" class="article-date">
  <time datetime="2023-08-16T16:00:00.000Z" itemprop="datePublished">2023-08-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/17/eess.IV_2023_08_17/">eess.IV - 2023-08-17 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Eosinophils-Instance-Object-Segmentation-on-Whole-Slide-Imaging-Using-Multi-label-Circle-Representation"><a href="#Eosinophils-Instance-Object-Segmentation-on-Whole-Slide-Imaging-Using-Multi-label-Circle-Representation" class="headerlink" title="Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation"></a>Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08974">http://arxiv.org/abs/2308.08974</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yilinliu610730/eoe">https://github.com/yilinliu610730/eoe</a></li>
<li>paper_authors: Yilin Liu, Ruining Deng, Juming Xiong, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Yuankai Huo</li>
<li>for: 该研究旨在提高食管炎症诊断的精度和效率，并且提供一种自动化的诊断方法。</li>
<li>methods: 该研究使用了圆形表示法和圆形蛇形模型来实现自动化的实例 segmentation。</li>
<li>results: 对比传统的Mask R-CNN模型和DeepSnake模型，圆形蛇形模型在识别和分割嗜好蛋白质方面表现出了superiority，这可能地提高了EoE诊断的精度和效率。<details>
<summary>Abstract</summary>
Eosinophilic esophagitis (EoE) is a chronic and relapsing disease characterized by esophageal inflammation. Symptoms of EoE include difficulty swallowing, food impaction, and chest pain which significantly impact the quality of life, resulting in nutritional impairments, social limitations, and psychological distress. The diagnosis of EoE is typically performed with a threshold (15 to 20) of eosinophils (Eos) per high-power field (HPF). Since the current counting process of Eos is a resource-intensive process for human pathologists, automatic methods are desired. Circle representation has been shown as a more precise, yet less complicated, representation for automatic instance cell segmentation such as CircleSnake approach. However, the CircleSnake was designed as a single-label model, which is not able to deal with multi-label scenarios. In this paper, we propose the multi-label CircleSnake model for instance segmentation on Eos. It extends the original CircleSnake model from a single-label design to a multi-label model, allowing segmentation of multiple object types. Experimental results illustrate the CircleSnake model's superiority over the traditional Mask R-CNN model and DeepSnake model in terms of average precision (AP) in identifying and segmenting eosinophils, thereby enabling enhanced characterization of EoE. This automated approach holds promise for streamlining the assessment process and improving diagnostic accuracy in EoE analysis. The source code has been made publicly available at https://github.com/yilinliu610730/EoE.
</details>
<details>
<summary>摘要</summary>
《营养细胞损伤综合征（EoE）是一种慢性和再次发生的疾病，特征为食管内部的Inflammation。EoE的症状包括困难吞食、食物堵塞和胸痛，对生活质量产生重大影响，导致营养不良、社会限制和心理压力。EoE的诊断通常通过Esophageal高力场（HPF）中Eosinophils（Eos）的数量（15-20）进行。由于当前的Eos数计数过程需要人工Pathologist的劳动，因此自动方法被欢迎。圆形表示已被证明为更精准， yet less complicated的表示方法，但它是单标签模型，无法处理多标签场景。本文提出了基于圆形的多标签CircleSnake模型，用于实例分 segmentation。这个模型从单标签设计扩展到多标签模型，可以进行多种对象类型的分 segmentation。实验结果表明，CircleSnake模型在AP（准确率）方面与传统的Mask R-CNN模型和DeepSnake模型相比，在标识和分 segmentationEosinophils方面表现出了超过其他两个模型的优势。这种自动化方法可以提高EoE分析过程的效率和准确性，并且代码已经在https://github.com/yilinliu610730/EoE上公开发布。
</details></li>
</ul>
<hr>
<h2 id="An-inexact-proximal-majorization-minimization-Algorithm-for-remote-sensing-image-stripe-noise-removal"><a href="#An-inexact-proximal-majorization-minimization-Algorithm-for-remote-sensing-image-stripe-noise-removal" class="headerlink" title="An inexact proximal majorization-minimization Algorithm for remote sensing image stripe noise removal"></a>An inexact proximal majorization-minimization Algorithm for remote sensing image stripe noise removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08866">http://arxiv.org/abs/2308.08866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjing Wang, Xile Zhao, Qingsong Wang, Zepei Ma, Peipei Tang</li>
<li>for: 提高远程感知图像中的视觉质量和数据分析精度，抑制远程感知图像中的梭噪。</li>
<li>methods: 提出非凸模型，使用DC函数结构进行梭噪除除。解决方法利用DC结构和不准确的 proximal 大esten-multipliers 算法，并设计实现的停止条件。</li>
<li>results: 数值实验表明提出的模型和算法在梭噪除除方面具有superiority。<details>
<summary>Abstract</summary>
The stripe noise existing in remote sensing images badly degrades the visual quality and restricts the precision of data analysis. Therefore, many destriping models have been proposed in recent years. In contrast to these existing models, in this paper, we propose a nonconvex model with a DC function (i.e., the difference of convex functions) structure to remove the strip noise. To solve this model, we make use of the DC structure and apply an inexact proximal majorization-minimization algorithm with each inner subproblem solved by the alternating direction method of multipliers. It deserves mentioning that we design an implementable stopping criterion for the inner subproblem, while the convergence can still be guaranteed. Numerical experiments demonstrate the superiority of the proposed model and algorithm.
</details>
<details>
<summary>摘要</summary>
“远程感知图像中的条纹噪音会严重损害视觉质量和数据分析精度。因此，过去几年内，许多条纹除去模型已经被提出。与现有模型不同，在本文中，我们提出了一种非凸模型，其结构是基于差分 convex 函数（DC 函数）。为解决这个模型，我们利用 DC 结构，并采用不准确的 proximal 主要化-最小化算法，其中每个内部子问题通过 alternate direction method of multipliers 解决。值得一提的是，我们设计了可实施的停止条件，而且可以保证 converge。数值实验表明，提出的模型和算法具有优势。”Here's a word-for-word translation of the text into Simplified Chinese:“远程感知图像中的条纹噪音会严重损害视觉质量和数据分析精度。因此，过去几年内，许多条纹除去模型已经被提出。与现有模型不同，在本文中，我们提出了一种非凸模型，其结构是基于差分 convex 函数（DC 函数）。为解决这个模型，我们利用 DC 结构，并采用不准确的 proximal 主要化-最小化算法，其中每个内部子问题通过 alternate direction method of multipliers 解决。值得一提的是，我们设计了可实施的停止条件，而且可以保证 converge。数值实验表明，提出的模型和算法具有优势。”
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Alternating-Optimization-for-Real-World-Blind-Super-Resolution"><a href="#End-to-end-Alternating-Optimization-for-Real-World-Blind-Super-Resolution" class="headerlink" title="End-to-end Alternating Optimization for Real-World Blind Super Resolution"></a>End-to-end Alternating Optimization for Real-World Blind Super Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08816">http://arxiv.org/abs/2308.08816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/greatlog/realdan">https://github.com/greatlog/realdan</a></li>
<li>paper_authors: Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan<br>for: 这个论文的目的是提出一种基于 alternate optimization 算法的盲SR方法，以提高盲SR的精度和稳定性。methods: 该方法使用了两个卷积神经网络：Restorer 和 Estimator。 Restorer 用于还原 SR 图像，而 Estimator 用于估计质量损失。这两个模块在 alternate 的形式下进行循环训练，以便互相优化。results: 实验表明，提出的方法可以大幅超越当前state-of-the-art 方法，并生成更加可观的结果。<details>
<summary>Abstract</summary>
Blind Super-Resolution (SR) usually involves two sub-problems: 1) estimating the degradation of the given low-resolution (LR) image; 2) super-resolving the LR image to its high-resolution (HR) counterpart. Both problems are ill-posed due to the information loss in the degrading process. Most previous methods try to solve the two problems independently, but often fall into a dilemma: a good super-resolved HR result requires an accurate degradation estimation, which however, is difficult to be obtained without the help of original HR information. To address this issue, instead of considering these two problems independently, we adopt an alternating optimization algorithm, which can estimate the degradation and restore the SR image in a single model. Specifically, we design two convolutional neural modules, namely \textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores the SR image based on the estimated degradation, and \textit{Estimator} estimates the degradation with the help of the restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, both \textit{Restorer} and \textit{Estimator} could get benefited from the intermediate results of each other, and make each sub-problem easier. Moreover, \textit{Restorer} and \textit{Estimator} are optimized in an end-to-end manner, thus they could get more tolerant of the estimation deviations of each other and cooperate better to achieve more robust and accurate final results. Extensive experiments on both synthetic datasets and real-world images show that the proposed method can largely outperform state-of-the-art methods and produce more visually favorable results. The codes are rleased at \url{https://github.com/greatlog/RealDAN.git}.
</details>
<details>
<summary>摘要</summary>
干Resolution（SR）问题通常包含两个互相关联的互补问题：1）估计LR图像的劣化程度；2）LR图像的超Resolution（HR）图像。两个问题都是不定的，因为升级过程中的信息损失。大多数前一代方法会独立地解决这两个问题，但经常陷入一个困境：一个好的HR图像需要一个准确的劣化估计，但是不可以不使用原始HR图像来获得这个估计。为解决这个问题，我们采用了一种alternating optimization算法，可以同时估计劣化和SR图像。我们设计了两个卷积神经网络模块：Restorer和Estimator。Restorer使用估计的劣化来恢复SR图像，而Estimator使用恢复后的SR图像来估计劣化。我们重复地使用这两个模块，并将其拓展成一个端到端可训练的网络。这样，Restorer和Estimator都可以受益于对方的中间结果，使每个问题变得更加容易。此外，Restorer和Estimator在端到端上进行了结构优化，因此它们可以更快地适应对方的估计偏差，并更好地合作以实现更加稳定和准确的最终结果。我们在 synthetic datasets 和实际图像上进行了广泛的实验，结果显示，我们的方法可以与当前状态计算机技术相比，大幅提高SR图像的质量。代码可以在 \url{https://github.com/greatlog/RealDAN.git} 中下载。
</details></li>
</ul>
<hr>
<h2 id="Recursive-Detection-and-Analysis-of-Nanoparticles-in-Scanning-Electron-Microscopy-Images"><a href="#Recursive-Detection-and-Analysis-of-Nanoparticles-in-Scanning-Electron-Microscopy-Images" class="headerlink" title="Recursive Detection and Analysis of Nanoparticles in Scanning Electron Microscopy Images"></a>Recursive Detection and Analysis of Nanoparticles in Scanning Electron Microscopy Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08732">http://arxiv.org/abs/2308.08732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aidan S. Wright, Nathaniel P. Youmans, Enrique F. Valderrama Araya</li>
<li>for: 这种 computational framework 的目的是为了精准检测和全面分析 SEM 图像中的粒子。</li>
<li>methods: 这个框架使用了 Python 图像处理库 OpenCV、SciPy 和 Scikit-Image，并结合了阈值处理、膨润和推杂等技术来提高图像处理结果的准确性。</li>
<li>results: 这个框架可以准确地确定粒子坐标，并提取粒子的相关形态特征，包括面积、方向、亮度和长度。 它在五个不同的测试图像中达到 97% 的粒子检测精度。<details>
<summary>Abstract</summary>
In this study, we present a computational framework tailored for the precise detection and comprehensive analysis of nanoparticles within scanning electron microscopy (SEM) images. The primary objective of this framework revolves around the accurate localization of nanoparticle coordinates, accompanied by secondary objectives encompassing the extraction of pertinent morphological attributes including area, orientation, brightness, and length.   Constructed leveraging the robust image processing capabilities of Python, particularly harnessing libraries such as OpenCV, SciPy, and Scikit-Image, the framework employs an amalgamation of techniques, including thresholding, dilating, and eroding, to enhance the fidelity of image processing outcomes.   The ensuing nanoparticle data is seamlessly integrated into the RStudio environment to facilitate meticulous post-processing analysis. This encompasses a comprehensive evaluation of model accuracy, discernment of feature distribution patterns, and the identification of intricate particle arrangements. The finalized framework exhibits high nanoparticle identification within the primary sample image and boasts 97\% accuracy in detecting particles across five distinct test images drawn from a SEM nanoparticle dataset. Furthermore, the framework demonstrates the capability to discern nanoparticles of faint intensity, eluding manual labeling within the control group.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一个计算框架，用于精确检测和全面分析射电镜像（SEM）中的粒子。主要目标是准确地确定粒子坐标，并且包括次要目标，如粒子形态特征的提取，包括面积、方向、亮度和长度。这个框架利用Python的强大图像处理能力，特别是OpenCV、SciPy和Scikit-Image库，结合多种技术，如阈值、扩展和膨润，以提高图像处理结果的准确性。获得的粒子数据可以轻松地 интегрирова到RStudio环境中，进行仔细的后处理分析。这包括完整评估模型准确性，分析特征分布图像，以及识别复杂的粒子排列。实验结果显示，该框架在主要样本图像中具有高精度的粒子检测，并在五个不同的测试图像中达到97%的检测精度。此外，框架还能够识别具有柔弱亮度的粒子，而控制组中的人工标注不能达到。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Kernel-Based-Adaptive-Spatial-Aggregation-for-Learned-Image-Compression"><a href="#Dynamic-Kernel-Based-Adaptive-Spatial-Aggregation-for-Learned-Image-Compression" class="headerlink" title="Dynamic Kernel-Based Adaptive Spatial Aggregation for Learned Image Compression"></a>Dynamic Kernel-Based Adaptive Spatial Aggregation for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08723">http://arxiv.org/abs/2308.08723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Huairui/DKIC">https://github.com/Huairui/DKIC</a></li>
<li>paper_authors: Huairui Wang, Nianxiang Fu, Zhenzhong Chen, Shan Liu</li>
<li>for: 提高图像压缩率和精度性能</li>
<li>methods: 使用动态核kernel基于转换编码，适应权重分享机制和自适应积集方法</li>
<li>results: 实验结果显示，当前方法在三个标准测试集上比现有学习基于方法具有更高的率压缩率和精度性能<details>
<summary>Abstract</summary>
Learned image compression methods have shown superior rate-distortion performance and remarkable potential compared to traditional compression methods. Most existing learned approaches use stacked convolution or window-based self-attention for transform coding, which aggregate spatial information in a fixed range. In this paper, we focus on extending spatial aggregation capability and propose a dynamic kernel-based transform coding. The proposed adaptive aggregation generates kernel offsets to capture valid information in the content-conditioned range to help transform. With the adaptive aggregation strategy and the sharing weights mechanism, our method can achieve promising transform capability with acceptable model complexity. Besides, according to the recent progress of entropy model, we define a generalized coarse-to-fine entropy model, considering the coarse global context, the channel-wise, and the spatial context. Based on it, we introduce dynamic kernel in hyper-prior to generate more expressive global context. Furthermore, we propose an asymmetric spatial-channel entropy model according to the investigation of the spatial characteristics of the grouped latents. The asymmetric entropy model aims to reduce statistical redundancy while maintaining coding efficiency. Experimental results demonstrate that our method achieves superior rate-distortion performance on three benchmarks compared to the state-of-the-art learning-based methods.
</details>
<details>
<summary>摘要</summary>
现有的学习型压缩方法已经显示出了Superior rate-distortion性能和吸引人的潜在性，相比传统压缩方法。大多数现有的学习方法使用堆叠 convolution或窗口基于自注意力 для变换编码，这些方法会汇集Fixed距离内的空间信息。在这篇论文中，我们关注到了扩展空间汇集能力，并提出了动态核心基于变换编码。我们的提案的适应汇集生成核心偏移来捕捉有效信息在内容受限的范围内，以帮助变换。通过适应汇集策略和共享权重机制，我们的方法可以实现可接受的变换能力，同时减少模型复杂度。此外，根据最近的Entropy模型进展，我们定义一个通用Coarse-to-fine Entropy模型，考虑Global上下文、通道级和空间上下文。基于它，我们引入动态核心在超乎 prior中生成更 expresive的全局上下文。另外，我们提出一种不对称的空间通道Entropy模型，根据Latent集的特点进行调整。这种不对称Entropy模型的目的是减少统计重复，保持编码效率。实验结果表明，我们的方法在三个标准底本上比State-of-the-art学习型方法 superior rate-distortion性能。
</details></li>
</ul>
<hr>
<h2 id="Deployment-and-Analysis-of-Instance-Segmentation-Algorithm-for-In-field-Grade-Estimation-of-Sweetpotatoes"><a href="#Deployment-and-Analysis-of-Instance-Segmentation-Algorithm-for-In-field-Grade-Estimation-of-Sweetpotatoes" class="headerlink" title="Deployment and Analysis of Instance Segmentation Algorithm for In-field Grade Estimation of Sweetpotatoes"></a>Deployment and Analysis of Instance Segmentation Algorithm for In-field Grade Estimation of Sweetpotatoes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08534">http://arxiv.org/abs/2308.08534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoang M. Nguyen, Sydney Gyurek, Russell Mierop, Kenneth V. Pecota, Kylie LaGamba, Michael Boyette, G. Craig Yencho, Cranos M. Williams, Michael W. Kudenov<br>for:这个论文是为了提出一种直接在场地中进行 Storage roots 的检测和评估，以便更快速地获得 yields 的方法。methods:这个方法使用了 Detectron2 库中的深度学习对象检测算法，实现了 Mask R-CNN 模型，用于实时地在场地中识别 Storage roots。results:模型可以在不同的环境条件下（包括光照和土壤特性的变化）正确地识别 Storage roots，并且与商业化光学排分器的比较表明，模型的 RMSE 值为0.66 cm，1.22 cm，74.73 g 分别，而 root 数量的 RMSE 值为5.27根，r^2 值为0.8。这种phenotyping 策略有 potential 用于实时地在场地中获得 yields，而不需要高科技和昂贵的光学排分器。<details>
<summary>Abstract</summary>
Shape estimation of sweetpotato (SP) storage roots is inherently challenging due to their varied size and shape characteristics. Even measuring "simple" metrics, such as length and width, requires significant time investments either directly in-field or afterward using automated graders. In this paper, we present the results of a model that can perform grading and provide yield estimates directly in the field quicker than manual measurements. Detectron2, a library consisting of deep-learning object detection algorithms, was used to implement Mask R-CNN, an instance segmentation model. This model was deployed for in-field grade estimation of SPs and evaluated against an optical sorter. Storage roots from various clones imaged with a cellphone during trials between 2019 and 2020, were used in the model's training and validation to fine-tune a model to detect SPs. Our results showed that the model could distinguish individual SPs in various environmental conditions including variations in lighting and soil characteristics. RMSE for length, width, and weight, from the model compared to a commercial optical sorter, were 0.66 cm, 1.22 cm, and 74.73 g, respectively, while the RMSE of root counts per plot was 5.27 roots, with r^2 = 0.8. This phenotyping strategy has the potential enable rapid yield estimates in the field without the need for sophisticated and costly optical sorters and may be more readily deployed in environments with limited access to these kinds of resources or facilities.
</details>
<details>
<summary>摘要</summary>
sweetpotato (SP) 存储根的形状评估是一项自然的挑战，因为它们的形状和大小具有很大的变化。 même measuring "simple" metrics, such as length and width, requires a significant investment of time, either directly in the field or using automated graders. In this paper, we present the results of a model that can perform grading and provide yield estimates directly in the field faster than manual measurements. We used Detectron2, a library consisting of deep-learning object detection algorithms, to implement Mask R-CNN, an instance segmentation model. This model was deployed for in-field grade estimation of SPs and evaluated against a commercial optical sorter. Storage roots from various clones imaged with a cellphone during trials between 2019 and 2020 were used to fine-tune the model to detect SPs. Our results showed that the model could distinguish individual SPs in various environmental conditions, including variations in lighting and soil characteristics. The root mean squared error (RMSE) for length, width, and weight, from the model compared to a commercial optical sorter, were 0.66 cm, 1.22 cm, and 74.73 g, respectively, while the RMSE of root counts per plot was 5.27 roots, with r^2 = 0.8. This phenotyping strategy has the potential to enable rapid yield estimates in the field without the need for sophisticated and costly optical sorters, and may be more readily deployed in environments with limited access to these kinds of resources or facilities.
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Distill-Global-Representation-for-Sparse-View-CT"><a href="#Learning-to-Distill-Global-Representation-for-Sparse-View-CT" class="headerlink" title="Learning to Distill Global Representation for Sparse-View CT"></a>Learning to Distill Global Representation for Sparse-View CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08463">http://arxiv.org/abs/2308.08463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilong Li, Chenglong Ma, Jie Chen, Junping Zhang, Hongming Shan</li>
<li>for: 这篇论文的目的是提出一种新的图像后处理方法，以提高稀疏视角计算Tomography（CT）图像的质量。</li>
<li>methods: 该方法使用了 globale representation（GloRe）核对应法，并通过对GloRe进行方向填充和频率特征填充来提高图像质量。</li>
<li>results: 对比于现有方法，该方法的 globale representation（GloRe）核对应法可以更好地提高稀疏视角CT图像的质量，并且可以更好地捕捉临床重要的诊断信息。<details>
<summary>Abstract</summary>
Sparse-view computed tomography (CT) -- using a small number of projections for tomographic reconstruction -- enables much lower radiation dose to patients and accelerated data acquisition. The reconstructed images, however, suffer from strong artifacts, greatly limiting their diagnostic value. Current trends for sparse-view CT turn to the raw data for better information recovery. The resultant dual-domain methods, nonetheless, suffer from secondary artifacts, especially in ultra-sparse view scenarios, and their generalization to other scanners/protocols is greatly limited. A crucial question arises: have the image post-processing methods reached the limit? Our answer is not yet. In this paper, we stick to image post-processing methods due to great flexibility and propose global representation (GloRe) distillation framework for sparse-view CT, termed GloReDi. First, we propose to learn GloRe with Fourier convolution, so each element in GloRe has an image-wide receptive field. Second, unlike methods that only use the full-view images for supervision, we propose to distill GloRe from intermediate-view reconstructed images that are readily available but not explored in previous literature. The success of GloRe distillation is attributed to two key components: representation directional distillation to align the GloRe directions, and band-pass-specific contrastive distillation to gain clinically important details. Extensive experiments demonstrate the superiority of the proposed GloReDi over the state-of-the-art methods, including dual-domain ones. The source code is available at https://github.com/longzilicart/GloReDi.
</details>
<details>
<summary>摘要</summary>
《简洁 computed tomography（CT）》——使用少量投射进行tomographic重建——可以大幅降低对病人的辐射剂量和数据获取的时间。然而，重建的图像却受到强烈的artefacts的限制，从而大大降低其诊断价值。当前的 sparse-view CT 趋势是转向原始数据，以便更好地回收信息。然而，结果性的 dual-domain 方法在 ultra-sparse 视图场景下受到次要artefact的影响，而且其在其他扫描器/协议上的普遍性受限。问题是：图像后处理方法是否已经达到了限制？我们的答案是不是。在这篇论文中，我们坚持使用图像后处理方法，因为它具有很大的灵活性。我们提出了 GloRe 整合框架（GloReDi），用于 sparse-view CT。首先，我们提出了学习 GloRe 使用 Fourier 杂化，使每个 GloRe 元素具有整个图像的广泛响应场。其次，不同于以前的方法，我们提出了使用 intermediate-view 重建图像进行监督，这些图像ready available，但在前期 литературе未被探讨。 GloRe 整合框架的成功归因于两个关键组成部分： representation directional distillation 用于对 GloRe 方向进行对齐，以及 band-pass-specific contrastive distillation 用于获取临床重要的细节。我们对 GloReDi 进行了广泛的实验，并证明其在state-of-the-art方法之上。源代码可以在 https://github.com/longzilicart/GloReDi 上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/17/eess.IV_2023_08_17/" data-id="cllurrpcn00e5sw881fjofrz4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/cs.LG_2023_08_16/" class="article-date">
  <time datetime="2023-08-15T16:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/16/cs.LG_2023_08_16/">cs.LG - 2023-08-16 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Accurate-synthesis-of-Dysarthric-Speech-for-ASR-data-augmentation"><a href="#Accurate-synthesis-of-Dysarthric-Speech-for-ASR-data-augmentation" class="headerlink" title="Accurate synthesis of Dysarthric Speech for ASR data augmentation"></a>Accurate synthesis of Dysarthric Speech for ASR data augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08438">http://arxiv.org/abs/2308.08438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Soleymanpour, Michael T. Johnson, Rahim Soleymanpour, Jeffrey Berry</li>
<li>for: 这篇论文的目的是为了提高自动语音识别（ASR）系统的性能，通过增加疾病某些特征来提供更多的疾病样本。</li>
<li>methods: 这篇论文使用了一种基于神经网络的多话者Text-to-Speech（TTS）系统，并在其中添加了疾病严重程度 coefficient和停顿插入模型，以生成不同疾病严重程度的语音。</li>
<li>results: 这篇论文使用了这种模型来生成训练数据，并通过对其进行语音识别测试来评估其效果。结果显示，在使用这种模型和数据 augmentation 技术后，ASR系统的识别精度得到了显著改善。此外，对比基eline模型，添加疾病严重程度和停顿控制可以降低WRR值6.5%。<details>
<summary>Abstract</summary>
Dysarthria is a motor speech disorder often characterized by reduced speech intelligibility through slow, uncoordinated control of speech production muscles. Automatic Speech recognition (ASR) systems can help dysarthric talkers communicate more effectively. However, robust dysarthria-specific ASR requires a significant amount of training speech, which is not readily available for dysarthric talkers. This paper presents a new dysarthric speech synthesis method for the purpose of ASR training data augmentation. Differences in prosodic and acoustic characteristics of dysarthric spontaneous speech at varying severity levels are important components for dysarthric speech modeling, synthesis, and augmentation. For dysarthric speech synthesis, a modified neural multi-talker TTS is implemented by adding a dysarthria severity level coefficient and a pause insertion model to synthesize dysarthric speech for varying severity levels. To evaluate the effectiveness for synthesis of training data for ASR, dysarthria-specific speech recognition was used. Results show that a DNN-HMM model trained on additional synthetic dysarthric speech achieves WER improvement of 12.2% compared to the baseline, and that the addition of the severity level and pause insertion controls decrease WER by 6.5%, showing the effectiveness of adding these parameters. Overall results on the TORGO database demonstrate that using dysarthric synthetic speech to increase the amount of dysarthric-patterned speech for training has significant impact on the dysarthric ASR systems. In addition, we have conducted a subjective evaluation to evaluate the dysarthric-ness and similarity of synthesized speech. Our subjective evaluation shows that the perceived dysartrhic-ness of synthesized speech is similar to that of true dysarthric speech, especially for higher levels of dysarthria
</details>
<details>
<summary>摘要</summary>
<<SYS>>这是一种 дви力问题，常见于语言生成功能不足的话者。自动语音识别系统（ASR）可以帮助这些话者更有效地沟通。然而，为了建立坚固的动力症特定ASR系统，需要大量的训练语音，但这些语音仅存在于少数话者身上。这篇文章提出了一新的动力症特定语音合成方法，用于ASR训练语音资料增强。在不同的严重程度下，动力症特定语音的语音和态度特征是合成语音模型的重要组成部分。为了合成动力症语音，我们将 modificated neural multi-talker TTS加入了动力症严重程度 coefficient和暂停插入模型，以合成不同严重程度的动力症语音。为了评估这种合成语音的有效性，我们使用了动力症特定语音识别系统。结果显示，将额外的合成动力症语音训练到DNN-HMM模型可以提高WRR值12.2%，并且将严重程度和暂停插入控制添加到合成语音模型可以降低WRR值6.5%，这说明了将这些参数添加的有效性。总结来说，使用动力症合成语音增加训练语音量有重要的影响力在动力症ASR系统。此外，我们还进行了主观评估，以评估合成语音的动力症程度和真实性。我们的主观评估显示，合成语音的动力症程度与真实动力症语音相似，尤其是在更高的严重程度下。这篇文章的结论是，使用动力症合成语音增加训练语音量可以提高动力症ASR系统的性能。这种方法可以帮助建立更加坚固的动力症ASR系统，并且可以增加训练语音量。
</details></li>
</ul>
<hr>
<h2 id="Eliciting-Risk-Aversion-with-Inverse-Reinforcement-Learning-via-Interactive-Questioning"><a href="#Eliciting-Risk-Aversion-with-Inverse-Reinforcement-Learning-via-Interactive-Questioning" class="headerlink" title="Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning"></a>Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08427">http://arxiv.org/abs/2308.08427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziteng Cheng, Anthony Coache, Sebastian Jaimungal</li>
<li>for: 本研究提出了一种新的框架，用于通过交互问答来识别代理人的风险偏好。我们的研究在两种场景下进行了测试：一个一期情况和一个无穷远景情况。在一期情况下，我们假设代理人的风险偏好是一个状态的成本函数和一种扭曲风险度量。在无穷远景情况下，我们模型了代理人的风险偏好，添加了一个优先级因子。假设我们有访问一组候选人，其中包含代理人的真实风险偏好，我们示出了问候者可以通过在不同环境中展示其最佳策略来识别代理人的风险偏好。我们证明，问候者可以通过问答来识别代理人的风险偏好，问题数量增加，问题随机设计。我们还开发了一个算法来设计优化的问题，并在 simulations 中提供了实证证明，我们的方法可以更快地识别代理人的风险偏好，比Randomly designed questions 更快。</li>
<li>methods: 我们的方法包括两个部分：一个是模型代理人的风险偏好，另一个是通过问答来识别代理人的风险偏好。在一期情况下，我们使用了一个成本函数和一种扭曲风险度量来模型代理人的风险偏好。在无穷远景情况下，我们添加了一个优先级因子来模型代理人的风险偏好。我们使用了一种随机设计的问题来识别代理人的风险偏好，并开发了一个算法来设计优化的问题。</li>
<li>results: 我们的实验结果表明，我们的方法可以快速地识别代理人的风险偏好。在 simulations 中，我们发现，我们的方法可以更快地识别代理人的风险偏好，比Randomly designed questions 更快。此外，我们还发现，我们的方法可以更好地适应不同的风险偏好。<details>
<summary>Abstract</summary>
This paper proposes a novel framework for identifying an agent's risk aversion using interactive questioning. Our study is conducted in two scenarios: a one-period case and an infinite horizon case. In the one-period case, we assume that the agent's risk aversion is characterized by a cost function of the state and a distortion risk measure. In the infinite horizon case, we model risk aversion with an additional component, a discount factor. Assuming the access to a finite set of candidates containing the agent's true risk aversion, we show that asking the agent to demonstrate her optimal policies in various environment, which may depend on their previous answers, is an effective means of identifying the agent's risk aversion. Specifically, we prove that the agent's risk aversion can be identified as the number of questions tends to infinity, and the questions are randomly designed. We also develop an algorithm for designing optimal questions and provide empirical evidence that our method learns risk aversion significantly faster than randomly designed questions in simulations. Our framework has important applications in robo-advising and provides a new approach for identifying an agent's risk preferences.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了一种新的方法，用于透过交互问答来确定代理人的风险偏好。我们的研究分为两个场景：一个一期 случа和一个无限远景 случа。在一期 случа中，我们假设代理人的风险偏好是通过状态的成本函数和扭曲风险度量来描述的。在无限远景 случа中，我们模型风险偏好有一个附加组成部分：折扣因子。假设我们有对代理人真实风险偏好的访问权，我们显示出问答可以作为一种有效的风险偏好标识方法。特别是，我们证明代理人的风险偏好可以通过问答的数量增长而被确定，并且问答可以随机设计。我们还开发了一个算法来设计优化的问答，并在实验中证明我们的方法可以在 simulations 中学习风险偏好得到更好的效果。我们的框架在 robo-advising 中有重要应用，并提供了一种新的风险偏好标识方法。
</details></li>
</ul>
<hr>
<h2 id="Digital-twinning-of-cardiac-electrophysiology-models-from-the-surface-ECG-a-geodesic-backpropagation-approach"><a href="#Digital-twinning-of-cardiac-electrophysiology-models-from-the-surface-ECG-a-geodesic-backpropagation-approach" class="headerlink" title="Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach"></a>Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08410">http://arxiv.org/abs/2308.08410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Grandits, Jan Verhülsdonk, Gundolf Haase, Alexander Effland, Simone Pezzuto</li>
<li>For: The paper is written for researchers and clinicians interested in cardiac electrophysiology and the development of personalized models of cardiac activation.* Methods: The paper introduces a novel method called Geodesic-BP, which uses the eikonal equation to solve the inverse problem of cardiac electrophysiology and reconstruct a given electrocardiogram (ECG). The method is well-suited for GPU-accelerated machine learning frameworks and can be used to optimize the parameters of the eikonal equation to reproduce a given ECG.* Results: The paper shows that Geodesic-BP can reconstruct a simulated cardiac activation with high accuracy in a synthetic test case, even in the presence of modeling inaccuracies. The method is also applied to a publicly available dataset of a rabbit model, with very positive results.<details>
<summary>Abstract</summary>
The eikonal equation has become an indispensable tool for modeling cardiac electrical activation accurately and efficiently. In principle, by matching clinically recorded and eikonal-based electrocardiograms (ECGs), it is possible to build patient-specific models of cardiac electrophysiology in a purely non-invasive manner. Nonetheless, the fitting procedure remains a challenging task. The present study introduces a novel method, Geodesic-BP, to solve the inverse eikonal problem. Geodesic-BP is well-suited for GPU-accelerated machine learning frameworks, allowing us to optimize the parameters of the eikonal equation to reproduce a given ECG. We show that Geodesic-BP can reconstruct a simulated cardiac activation with high accuracy in a synthetic test case, even in the presence of modeling inaccuracies. Furthermore, we apply our algorithm to a publicly available dataset of a rabbit model, with very positive results. Given the future shift towards personalized medicine, Geodesic-BP has the potential to help in future functionalizations of cardiac models meeting clinical time constraints while maintaining the physiological accuracy of state-of-the-art cardiac models.
</details>
<details>
<summary>摘要</summary>
《射线方程》已成为心脏电动力学模型精准计算的不可或缺工具。在原理上，通过对临床记录的电子干扰gram（ECG）和射线方程进行匹配，可以建立个性化的心脏电physiology模型，无需侵入性的干扰。然而，匹配过程仍然是一项具有挑战性的任务。本研究提出了一种新的方法，即Geodesic-BP，以解决反射射线问题。Geodesic-BP适用于加速机器学习框架的GPU，可以优化射线方程中的参数，以实现与给定ECG的匹配。我们在一个人工测试案例中展示了Geodesic-BP可以高精度地重建模拟的心脏活动，包括模型精度不足时的情况。此外，我们将我们的算法应用于一个公共可用的兔子模型数据集，得到了非常正面的结果。鉴于未来的个性化医疗的发展，Geodesic-BP有望帮助未来的心脏模型功能化，满足临床时间限制，同时维持现有的心脏模型的生物学精度。
</details></li>
</ul>
<hr>
<h2 id="Explainable-AI-for-clinical-risk-prediction-a-survey-of-concepts-methods-and-modalities"><a href="#Explainable-AI-for-clinical-risk-prediction-a-survey-of-concepts-methods-and-modalities" class="headerlink" title="Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities"></a>Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08407">http://arxiv.org/abs/2308.08407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Munib Mesinovic, Peter Watkinson, Tingting Zhu</li>
<li>for: 这篇论文的目的是探讨 ai 应用于医疗领域中的解释性能力，以确保 ai 系统的可靠性和可信worthiness。</li>
<li>methods: 这篇论文使用了多种解释性模型，包括 LIME、 SHAP 和 TreeExplainer，以及其他一些新的解释性方法。</li>
<li>results: 这篇论文的结果表明，使用解释性模型可以提高 ai 系统的可靠性和可信worthiness，同时也可以增加模型的透明度和可读性。<details>
<summary>Abstract</summary>
Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highlighting the importance of quantitative and clinical evaluation and validation across multiple common modalities in clinical practice. It emphasizes the need for external validation and the combination of diverse interpretability methods to enhance trust and fairness. Adopting rigorous testing, such as using synthetic datasets with known generative factors, can further improve the reliability of explainability methods. Open access and code-sharing resources are essential for transparency and reproducibility, enabling the growth and trustworthiness of explainable research. While challenges exist, an end-to-end approach to explainability in clinical risk prediction, incorporating stakeholders from clinicians to developers, is essential for success.
</details>
<details>
<summary>摘要</summary>
最近的人工智能应用于医疗领域的进步已经显示出了人性化的 диагности和疾病预测的能力。然而，随着人工智能模型的复杂度的增加，关于它们的不透明度、潜在偏见和解释性的问题也引起了关注。以确保人工智能系统的可信worth和可靠性，特别是在临床风险预测模型中，解释性变得非常重要。解释性通常指的是人工智能系统能够提供人类权益者可靠的决策逻辑或决策结果的解释。在临床风险预测中，其他方面的解释性如公平、偏见、信任和透明度也是重要的概念，这些概念frequently被用于一起或相互替换使用。本文评论了这些概念之间的关系，并讨论了最近在临床风险预测中发展的解释模型，强调了在多种常见模式下的量化和临床评估和验证的重要性。它也强调了外部验证和多种解释方法的结合，以提高可信worth和公平性。采用严格的测试，如使用已知生成因素的 sintetic数据集，可以进一步提高解释性方法的可靠性。开放访问和代码分享资源是必要的，以确保透明度和可重现性。而且，在实施解释性时，需要结合临床医生到开发者的多种潜在偏见，以确保成功。
</details></li>
</ul>
<hr>
<h2 id="Content-based-Recommendation-Engine-for-Video-Streaming-Platform"><a href="#Content-based-Recommendation-Engine-for-Video-Streaming-Platform" class="headerlink" title="Content-based Recommendation Engine for Video Streaming Platform"></a>Content-based Recommendation Engine for Video Streaming Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08406">http://arxiv.org/abs/2308.08406</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Puskal Khadka, Prabhav Lamichhane</li>
<li>for: 这个论文是为了提供一种基于内容的推荐引擎，用于为用户提供视频建议，以满足他们的先前兴趣和选择。</li>
<li>methods: 该论文使用了TF-IDF文本分Vector化方法来确定文档中的相关性，然后计算每个内容之间的cosine相似性来确定它们之间的相似性。</li>
<li>results: 该论文得到了一个基于TF-IDF和cosine相似性的推荐引擎，可以帮助用户找到符合他们兴趣的视频内容。同时，论文还测试了该引擎的性能，并计算了精度、报告率和F1核心等指标，以评估其性能。<details>
<summary>Abstract</summary>
Recommendation engine suggest content, product or services to the user by using machine learning algorithm. This paper proposed a content-based recommendation engine for providing video suggestion to the user based on their previous interests and choices. We will use TF-IDF text vectorization method to determine the relevance of words in a document. Then we will find out the similarity between each content by calculating cosine similarity between them. Finally, engine will recommend videos to the users based on the obtained similarity score value. In addition, we will measure the engine's performance by computing precision, recall, and F1 core of the proposed system.
</details>
<details>
<summary>摘要</summary>
<<SYS>>机器学习算法可以为用户提供内容、产品或服务的建议。这篇论文提出了基于用户之前的兴趣和选择的内容基于推荐引擎，用于为用户提供视频建议。我们将使用TF-IDF文本矢量化方法来确定文档中的相关性。然后，我们将计算每个内容之间的相似性，并根据所得到的相似性分值来推荐视频给用户。此外，我们还将测量引擎的性能，计算推荐系统的准确率、恰当率和F1分值。Note: "TF-IDF" stands for "Term Frequency-Inverse Document Frequency", which is a text vectorization method used to determine the relevance of words in a document.
</details></li>
</ul>
<hr>
<h2 id="Fast-Uncertainty-Quantification-of-Spent-Nuclear-Fuel-with-Neural-Networks"><a href="#Fast-Uncertainty-Quantification-of-Spent-Nuclear-Fuel-with-Neural-Networks" class="headerlink" title="Fast Uncertainty Quantification of Spent Nuclear Fuel with Neural Networks"></a>Fast Uncertainty Quantification of Spent Nuclear Fuel with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08391">http://arxiv.org/abs/2308.08391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnau Albà, Andreas Adelmann, Lucas Münster, Dimitri Rochman, Romana Boiger<br>for: 这个论文的目的是为了简化核电厂燃料的特性calculation和不确定性评估，以提高核能生产、废料处理和核安全预防的安全、效率和可持续性。methods: 本论文使用神经网络（NN）来建立一个快速的模拟模型，以便预测核电厂燃料的一些特性，例如衰变热和核lide的含量，并且可以对这些特性进行不确定性评估。results: 本论文的结果显示，使用NN模型可以实现这些特性的精确预测，并且可以对这些特性进行不确定性评估，并且可以大大降低physics-based模型的计算成本。<details>
<summary>Abstract</summary>
The accurate calculation and uncertainty quantification of the characteristics of spent nuclear fuel (SNF) play a crucial role in ensuring the safety, efficiency, and sustainability of nuclear energy production, waste management, and nuclear safeguards. State of the art physics-based models, while reliable, are computationally intensive and time-consuming. This paper presents a surrogate modeling approach using neural networks (NN) to predict a number of SNF characteristics with reduced computational costs compared to physics-based models. An NN is trained using data generated from CASMO5 lattice calculations. The trained NN accurately predicts decay heat and nuclide concentrations of SNF, as a function of key input parameters, such as enrichment, burnup, cooling time between cycles, mean boron concentration and fuel temperature. The model is validated against physics-based decay heat simulations and measurements of different uranium oxide fuel assemblies from two different pressurized water reactors. In addition, the NN is used to perform sensitivity analysis and uncertainty quantification. The results are in very good alignment to CASMO5, while the computational costs (taking into account the costs of generating training samples) are reduced by a factor of 10 or more. Our findings demonstrate the feasibility of using NNs as surrogate models for fast characterization of SNF, providing a promising avenue for improving computational efficiency in assessing nuclear fuel behavior and associated risks.
</details>
<details>
<summary>摘要</summary>
使用神经网络（NN）模型来快速计算核电燃料（SNF）的特性，可以提高核能生产、废料处理和核保障的安全、效率和可持续性。当前的物理基于模型，尽管可靠，但计算成本高。这篇文章介绍了使用NN模型来预测SNF特性，包括衰变热和核lide的分布，作为输入参数的核燃料燃烧度、燃烧时间、冷却时间、燃料浓度和燃料温度等。NN模型通过对CASMO5网络计算数据进行训练。模型可以准确地预测核燃料的衰变热和核lide的分布，并且可以对不同的氧化铀燃料聚合体进行预测。此外，NN模型还可以进行敏感分析和不确定性评估。结果与CASMO5模型相符，而计算成本（包括生成训练样本的成本）则被减少了一倍或更多。我们的发现表明使用NN模型可以快速地计算SNF特性，提供了一个有前途的方法来提高核燃料行为和相关风险的计算效率。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Sweep-an-improved-binary-quantifier"><a href="#Continuous-Sweep-an-improved-binary-quantifier" class="headerlink" title="Continuous Sweep: an improved, binary quantifier"></a>Continuous Sweep: an improved, binary quantifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08387">http://arxiv.org/abs/2308.08387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Kloos, Julian D. Karch, Quinten A. Meertens, Mark de Rooij</li>
<li>for: 这篇论文主要关注的是量化学习任务中的binary quantifier，即估计资料集中类别的分布。</li>
<li>methods: 作者们提出了一个新的 parametric binary quantifier，叫做Continuous Sweep，这个方法受到Median Sweep的影响，但是有三个主要的改进：1) 使用Parametric class distributions instead of empirical distributions，2) 优化决策boundaries instead of 应用类别规则，3) 计算mean instead of median。</li>
<li>results: 作者们通过分析表现，证明了Continuous Sweep在广泛的情况下比Median Sweep表现更好，并且提供了一些 theoretically optimal decision boundaries。<details>
<summary>Abstract</summary>
Quantification is a supervised machine learning task, focused on estimating the class prevalence of a dataset rather than labeling its individual observations. We introduce Continuous Sweep, a new parametric binary quantifier inspired by the well-performing Median Sweep. Median Sweep is currently one of the best binary quantifiers, but we have changed this quantifier on three points, namely 1) using parametric class distributions instead of empirical distributions, 2) optimizing decision boundaries instead of applying discrete decision rules, and 3) calculating the mean instead of the median. We derive analytic expressions for the bias and variance of Continuous Sweep under general model assumptions. This is one of the first theoretical contributions in the field of quantification learning. Moreover, these derivations enable us to find the optimal decision boundaries. Finally, our simulation study shows that Continuous Sweep outperforms Median Sweep in a wide range of situations.
</details>
<details>
<summary>摘要</summary>
<<SYS>>quantification是一种指导学习任务，关注数据集中类别的出现频率而不是具体的观察值。我们介绍了连续探索，一种基于 median sweep 的新参数化二分量器。Median sweep 目前是二分量器中的一个非常好的选择，但我们在其上改变了三点：1）使用参数化类别分布而不是实际分布，2）优化决策界而不是应用简单的决策规则，3）计算平均值而不是中值。我们 derive 了一系列的analytic表达式，用于描述 Continuous Sweep 的偏差和方差。这是量化学习领域的一个非常rare的理论贡献。此外，这些 derivations 允许我们找到最佳决策界。最后，我们的 simulations 研究表明，Continuous Sweep 在各种情况下都能够超越 Median Sweep。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Precision-and-Recall-Reject-Curves-for-Classification"><a href="#Precision-and-Recall-Reject-Curves-for-Classification" class="headerlink" title="Precision and Recall Reject Curves for Classification"></a>Precision and Recall Reject Curves for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08381">http://arxiv.org/abs/2308.08381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lydia Fischer, Patricia Wollstadt</li>
<li>for: 这个论文主要是为了提出一种新的评估分类器性能的方法，以及一种基于这种方法的分类器。</li>
<li>methods: 这个论文使用了一种基于学习 вектор量化的prototype-based分类器，并使用了一些不同的certainty measure来评估分类器的性能。</li>
<li>results: 论文通过对人工测试数据和实际医疗数据进行测试，发现在面临着类别不对称的情况下，使用precision和recall reject curve可以更好地评估分类器的性能，而不是使用准确率 reject curve。<details>
<summary>Abstract</summary>
For some classification scenarios, it is desirable to use only those classification instances that a trained model associates with a high certainty. To obtain such high-certainty instances, previous work has proposed accuracy-reject curves. Reject curves allow to evaluate and compare the performance of different certainty measures over a range of thresholds for accepting or rejecting classifications. However, the accuracy may not be the most suited evaluation metric for all applications, and instead precision or recall may be preferable. This is the case, for example, for data with imbalanced class distributions. We therefore propose reject curves that evaluate precision and recall, the recall-reject curve and the precision-reject curve. Using prototype-based classifiers from learning vector quantization, we first validate the proposed curves on artificial benchmark data against the accuracy reject curve as a baseline. We then show on imbalanced benchmarks and medical, real-world data that for these scenarios, the proposed precision- and recall-curves yield more accurate insights into classifier performance than accuracy reject curves.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将给定文本翻译成简化中文。</SYS>>对于一些分类场景，您可以使用已经训练的模型中的高确定度分类实例。以前的工作已经提出了准确度拒绝曲线，这些曲线可以评估和比较不同的确定度测试的性能。但是，准确度可能不是所有应用场景中最适合的评价指标，特别是数据具有不均衡的类别分布。我们因此提议使用 recall-reject 曲线和 precision-reject 曲线来评估分类器性能。使用学习 вектор量化的prototype-based分类器，我们首先在人工测试数据上验证我们提议的曲线，并作为基准点使用准确度拒绝曲线。然后，我们在具有不均衡的测试数据和医疗实际数据上展示了，在这些场景下，我们的precision-和 recall-曲线可以更加准确地评估分类器性能，比较准确度拒绝曲线。
</details></li>
</ul>
<hr>
<h2 id="A-distributed-neural-network-architecture-for-dynamic-sensor-selection-with-application-to-bandwidth-constrained-body-sensor-networks"><a href="#A-distributed-neural-network-architecture-for-dynamic-sensor-selection-with-application-to-bandwidth-constrained-body-sensor-networks" class="headerlink" title="A distributed neural network architecture for dynamic sensor selection with application to bandwidth-constrained body-sensor networks"></a>A distributed neural network architecture for dynamic sensor selection with application to bandwidth-constrained body-sensor networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08379">http://arxiv.org/abs/2308.08379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Strypsteen, Alexander Bertrand</li>
<li>for: 这个研究旨在提出一个动态侦测器选择方法，用于深度神经网络（DNNs），以便从每个具体的输入样本中选择最佳侦测器子集，而不是整个数据集中的固定选择。</li>
<li>methods: 这个动态选择是与任务模型一起学习的，使用Gumbel-Softmax的技巧，以让决策获得数据验证。</li>
<li>results: 我们显示了如何使用这个动态选择来增加无线传感网络（WSN）的寿命，并且透过对每个节点的传输次数实施限制。我们还提高了性能的方法，包括一个动态空间范本，让任务-DNN更加对多个可能的节点子集具有抗衰变的能力。最后，我们说明了如何选择最佳通道。我们验证了这个方法，使用实际的电普瑞度测量数据，模拟一个EEG感应网络。我们分析了对输送负载和任务精度的交换。<details>
<summary>Abstract</summary>
We propose a dynamic sensor selection approach for deep neural networks (DNNs), which is able to derive an optimal sensor subset selection for each specific input sample instead of a fixed selection for the entire dataset. This dynamic selection is jointly learned with the task model in an end-to-end way, using the Gumbel-Softmax trick to allow the discrete decisions to be learned through standard backpropagation. We then show how we can use this dynamic selection to increase the lifetime of a wireless sensor network (WSN) by imposing constraints on how often each node is allowed to transmit. We further improve performance by including a dynamic spatial filter that makes the task-DNN more robust against the fact that it now needs to be able to handle a multitude of possible node subsets. Finally, we explain how the selection of the optimal channels can be distributed across the different nodes in a WSN. We validate this method on a use case in the context of body-sensor networks, where we use real electroencephalography (EEG) sensor data to emulate an EEG sensor network. We analyze the resulting trade-offs between transmission load and task accuracy.
</details>
<details>
<summary>摘要</summary>
我们提出了一种动态感测器选择方法，用于深度神经网络（DNN），可以为每个特定输入样本选择最佳感测器子集而不是整个数据集的固定选择。这种动态选择与任务模型一起学习，使用Gumbel-Softmax技巧，以便通过标准反馈来学习不同的决策。我们然后解释了如何使用这种动态选择来增加无线传感器网络（WSN）的寿命，并在不同的节点上强制执行特定的传输限制。此外，我们还提高了任务-DNN的可靠性，使其能够处理多个可能的节点子集。最后，我们解释了如何在WSN中选择优化的通道。我们验证了这种方法在身体感测网络上的使用情况，使用实际的电encephalography（EEG）感测器数据来模拟EEG感测器网络。我们分析了因 переда信荷和任务准确率之间的负面效应。
</details></li>
</ul>
<hr>
<h2 id="PDPK-A-Framework-to-Synthesise-Process-Data-and-Corresponding-Procedural-Knowledge-for-Manufacturing"><a href="#PDPK-A-Framework-to-Synthesise-Process-Data-and-Corresponding-Procedural-Knowledge-for-Manufacturing" class="headerlink" title="PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing"></a>PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08371">http://arxiv.org/abs/2308.08371</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/0x14d/embedding-operator-knowledge">https://github.com/0x14d/embedding-operator-knowledge</a></li>
<li>paper_authors: Richard Nordsieck, André Schweizer, Michael Heider, Jörg Hähner</li>
<li>For: The paper aims to provide a framework for generating synthetic datasets that can be used to represent procedural knowledge in different domains.* Methods: The framework uses a combination of knowledge graphs and parametric processes to simulate real-world data and generate synthetic datasets. The authors evaluate the effectiveness of several existing embedding methods on the synthetic datasets.* Results: The authors compare the results of the embedding methods on the synthetic datasets with those achievable on a real-world dataset, and find that the synthetic datasets can accurately represent the procedural knowledge in the real-world data. They also provide a baseline for future work by demonstrating the potential of the synthetic datasets to represent procedural knowledge.Here’s the information in Simplified Chinese text:* For: 本文提供了一个框架，用于生成不同领域的 sintetic 数据集，以表现程序知识。* Methods: 框架使用知识 graphs 和参数化过程来实现实际数据的生成，并评估了多个现有的嵌入方法在 sintetic 数据集上的效果。* Results: 作者比较了嵌入方法在 sintetic 数据集和实际数据集上的结果，发现 sintetic 数据集能够准确表现实际数据中的程序知识。他们还提供了未来工作的基eline，显示 sintetic 数据集的可能性。<details>
<summary>Abstract</summary>
Procedural knowledge describes how to accomplish tasks and mitigate problems. Such knowledge is commonly held by domain experts, e.g. operators in manufacturing who adjust parameters to achieve quality targets. To the best of our knowledge, no real-world datasets containing process data and corresponding procedural knowledge are publicly available, possibly due to corporate apprehensions regarding the loss of knowledge advances. Therefore, we provide a framework to generate synthetic datasets that can be adapted to different domains. The design choices are inspired by two real-world datasets of procedural knowledge we have access to. Apart from containing representations of procedural knowledge in Resource Description Framework (RDF)-compliant knowledge graphs, the framework simulates parametrisation processes and provides consistent process data. We compare established embedding methods on the resulting knowledge graphs, detailing which out-of-the-box methods have the potential to represent procedural knowledge. This provides a baseline which can be used to increase the comparability of future work. Furthermore, we validate the overall characteristics of a synthesised dataset by comparing the results to those achievable on a real-world dataset. The framework and evaluation code, as well as the dataset used in the evaluation, are available open source.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>专业知识描述如何完成任务和解决问题。这种知识通常由领域专家所拥有，例如制造业中的操作员，他们根据参数进行调整以达到质量目标。根据我们所知，现实世界中没有公开可用的实际数据和相应的专业知识集。因此，我们提供了一个框架，可以生成可靠的合成数据集，可以适应不同领域。该框架的设计启发自我们有access的两个实际数据集，它们包含了专业知识的RDF相容知识图表示，同时还模拟了参数化过程，提供了一致的过程数据。我们使用现有的嵌入方法对于生成的知识图进行评估，详细介绍了这些方法在表示专业知识方面的潜力。此外，我们还验证了合成数据集的总特征，并与真实世界数据集进行比较，以验证合成数据集的可靠性。框架和评估代码以及使用于评估的数据集都是开源的。
</details></li>
</ul>
<hr>
<h2 id="Dual-Branch-Temperature-Scaling-Calibration-for-Long-Tailed-Recognition"><a href="#Dual-Branch-Temperature-Scaling-Calibration-for-Long-Tailed-Recognition" class="headerlink" title="Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition"></a>Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08366">http://arxiv.org/abs/2308.08366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Guo, Zhenyu Wu, Zhiqiang Zhan, Yang Ji</li>
<li>for: 本研究旨在解决深度神经网络中的误差补做问题，尤其是在长板分布的数据下存在较大的误差问题。</li>
<li>methods: 本研究使用了温度扩展（TS）方法，并设计了多支分支温度扩展模型（Dual-TS），以考虑不同类别的温度参数的多样性和罕见样本的非一致性。</li>
<li>results: 经过实验，本研究表明，我们的模型在传统ECE和Esbin-ECE评价指标下均达到了顶尖性能。<details>
<summary>Abstract</summary>
The calibration for deep neural networks is currently receiving widespread attention and research. Miscalibration usually leads to overconfidence of the model. While, under the condition of long-tailed distribution of data, the problem of miscalibration is more prominent due to the different confidence levels of samples in minority and majority categories, and it will result in more serious overconfidence. To address this problem, some current research have designed diverse temperature coefficients for different categories based on temperature scaling (TS) method. However, in the case of rare samples in minority classes, the temperature coefficient is not generalizable, and there is a large difference between the temperature coefficients of the training set and the validation set. To solve this challenge, this paper proposes a dual-branch temperature scaling calibration model (Dual-TS), which considers the diversities in temperature parameters of different categories and the non-generalizability of temperature parameters for rare samples in minority classes simultaneously. Moreover, we noticed that the traditional calibration evaluation metric, Excepted Calibration Error (ECE), gives a higher weight to low-confidence samples in the minority classes, which leads to inaccurate evaluation of model calibration. Therefore, we also propose Equal Sample Bin Excepted Calibration Error (Esbin-ECE) as a new calibration evaluation metric. Through experiments, we demonstrate that our model yields state-of-the-art in both traditional ECE and Esbin-ECE metrics.
</details>
<details>
<summary>摘要</summary>
Currently, the calibration of deep neural networks is receiving extensive attention and research. If the model is miscalibrated, it can lead to overconfidence. In particular, when dealing with long-tailed distribution of data, the problem of miscalibration is more pronounced due to the differences in confidence levels of samples in minority and majority categories, which can result in more serious overconfidence. To address this issue, some current research has proposed using diverse temperature coefficients for different categories based on the temperature scaling (TS) method. However, for rare samples in minority classes, the temperature coefficient is not generalizable, and there is a large difference between the temperature coefficients of the training set and the validation set.To solve this challenge, this paper proposes a dual-branch temperature scaling calibration model (Dual-TS), which takes into account the diversity of temperature parameters for different categories and the non-generalizability of temperature parameters for rare samples in minority classes simultaneously. Furthermore, we noticed that the traditional calibration evaluation metric, Expected Calibration Error (ECE), gives a higher weight to low-confidence samples in minority classes, which leads to inaccurate evaluation of model calibration. Therefore, we also propose Equal Sample Bin Expected Calibration Error (Esbin-ECE) as a new calibration evaluation metric. Through experiments, we demonstrate that our model achieves state-of-the-art performance in both traditional ECE and Esbin-ECE metrics.
</details></li>
</ul>
<hr>
<h2 id="KernelWarehouse-Towards-Parameter-Efficient-Dynamic-Convolution"><a href="#KernelWarehouse-Towards-Parameter-Efficient-Dynamic-Convolution" class="headerlink" title="KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution"></a>KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08361">http://arxiv.org/abs/2308.08361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osvai/kernelwarehouse">https://github.com/osvai/kernelwarehouse</a></li>
<li>paper_authors: Chao Li, Anbang Yao</li>
<li>for: 这 paper 是为了提出一种更高效的动态核函数，以提高图像识别模型的表现。</li>
<li>methods: 这 paper 使用了一种新的核函数设计方法，即 KernelWarehouse，它可以减少核函数的维度和增加核函数的数量，从而提高图像识别模型的表现。</li>
<li>results: 这 paper 的实验结果表明，使用 KernelWarehouse 可以达到图像识别领域的州际最佳性，并且可以降低模型的参数数量，从而提高模型的可扩展性和灵活性。<details>
<summary>Abstract</summary>
Dynamic convolution learns a linear mixture of $n$ static kernels weighted with their sample-dependent attentions, demonstrating superior performance compared to normal convolution. However, existing designs are parameter-inefficient: they increase the number of convolutional parameters by $n$ times. This and the optimization difficulty lead to no research progress in dynamic convolution that can allow us to use a significant large value of $n$ (e.g., $n>100$ instead of typical setting $n<10$) to push forward the performance boundary. In this paper, we propose $KernelWarehouse$, a more general form of dynamic convolution, which can strike a favorable trade-off between parameter efficiency and representation power. Its key idea is to redefine the basic concepts of "$kernels$" and "$assembling$ $kernels$" in dynamic convolution from the perspective of reducing kernel dimension and increasing kernel number significantly. In principle, KernelWarehouse enhances convolutional parameter dependencies within the same layer and across successive layers via tactful kernel partition and warehouse sharing, yielding a high degree of freedom to fit a desired parameter budget. We validate our method on ImageNet and MS-COCO datasets with different ConvNet architectures, and show that it attains state-of-the-art results. For instance, the ResNet18|ResNet50|MobileNetV2|ConvNeXt-Tiny model trained with KernelWarehouse on ImageNet reaches 76.05%|81.05%|75.52%|82.51% top-1 accuracy. Thanks to its flexible design, KernelWarehouse can even reduce the model size of a ConvNet while improving the accuracy, e.g., our ResNet18 model with 36.45%|65.10% parameter reduction to the baseline shows 2.89%|2.29% absolute improvement to top-1 accuracy.
</details>
<details>
<summary>摘要</summary>
“动态核函数学习一种线性权重混合的$n$ static核函数，达到比正常核函数更高的性能，但现有设计存在参数不效率问题：它将参数数量增加$n$倍。这导致了对动态核函数的研究停滞不前进，无法使用较大的$n$值（例如$n>100$）来推动性能边界。本文提出了«KernelWarehouse”，一种更通用的动态核函数设计，可以实现参数效率和表示能力之间的平衡。它的关键思想是在动态核函数中重新定义«核函数”和«核函数组合»的概念，从减少核函数维度和增加核函数数量的角度来看。在实践中，KernelWarehouse通过精巧的核函数分割和库共享，提高了层内参数之间的依赖关系和层次关系，从而实现了高度的自由度来适应感兴趣的参数预算。我们在ImageNet和MS-COCO datasets上验证了KernelWarehouse，并证明其可以达到状态略的最佳结果。例如，在ImageNet上，使用KernelWarehouse训练ResNet18|ResNet50|MobileNetV2|ConvNeXt-Tiny模型，可以达到76.05%|81.05%|75.52%|82.51%的顶部一个精度。此外，由于KernelWarehouse的灵活设计，可以在ConvNet模型中减少参数大小，同时提高准确率，例如我们的ResNet18模型在参数减少36.45%|65.10%后，可以提高2.89%|2.29%的精度。”
</details></li>
</ul>
<hr>
<h2 id="Independent-Distribution-Regularization-for-Private-Graph-Embedding"><a href="#Independent-Distribution-Regularization-for-Private-Graph-Embedding" class="headerlink" title="Independent Distribution Regularization for Private Graph Embedding"></a>Independent Distribution Regularization for Private Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08360">http://arxiv.org/abs/2308.08360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/privategraphencoder">https://github.com/hkust-knowcomp/privategraphencoder</a></li>
<li>paper_authors: Qi Hu, Yangqiu Song</li>
<li>for: 本研究旨在提出一种名为Private Variational Graph AutoEncoders（PVGAE）的新方法，以保护个人隐私信息的同时实现图像学任务的优秀表现。</li>
<li>methods: PVGAE使用了独立分布罚项作为正则项，并将原始的变量图自动编码器（VGAE）分解成学习敏感和非敏感特征的两个集。</li>
<li>results: 对三个实际数据集进行实验，PVGAE在保护个人隐私信息的同时实现了优秀的表现，舒服于其他基eline。<details>
<summary>Abstract</summary>
Learning graph embeddings is a crucial task in graph mining tasks. An effective graph embedding model can learn low-dimensional representations from graph-structured data for data publishing benefiting various downstream applications such as node classification, link prediction, etc. However, recent studies have revealed that graph embeddings are susceptible to attribute inference attacks, which allow attackers to infer private node attributes from the learned graph embeddings. To address these concerns, privacy-preserving graph embedding methods have emerged, aiming to simultaneously consider primary learning and privacy protection through adversarial learning. However, most existing methods assume that representation models have access to all sensitive attributes in advance during the training stage, which is not always the case due to diverse privacy preferences. Furthermore, the commonly used adversarial learning technique in privacy-preserving representation learning suffers from unstable training issues. In this paper, we propose a novel approach called Private Variational Graph AutoEncoders (PVGAE) with the aid of independent distribution penalty as a regularization term. Specifically, we split the original variational graph autoencoder (VGAE) to learn sensitive and non-sensitive latent representations using two sets of encoders. Additionally, we introduce a novel regularization to enforce the independence of the encoders. We prove the theoretical effectiveness of regularization from the perspective of mutual information. Experimental results on three real-world datasets demonstrate that PVGAE outperforms other baselines in private embedding learning regarding utility performance and privacy protection.
</details>
<details>
<summary>摘要</summary>
学习图embedding是 graf矿 tasks 中的一项重要任务。一个有效的图embedding模型可以从图结构数据中学习低维度表示，为数据发布带来多种下游应用程序的利益，如节点分类、链接预测等。然而， latest studies have shown that graph embeddings are vulnerable to attribute inference attacks, which allow attackers to infer private node attributes from the learned graph embeddings. To address these concerns, privacy-preserving graph embedding methods have emerged, aiming to simultaneously consider primary learning and privacy protection through adversarial learning. However, most existing methods assume that representation models have access to all sensitive attributes in advance during the training stage, which is not always the case due to diverse privacy preferences. Furthermore, the commonly used adversarial learning technique in privacy-preserving representation learning suffers from unstable training issues.In this paper, we propose a novel approach called Private Variational Graph Autoencoders (PVGAE) with the aid of independent distribution penalty as a regularization term. Specifically, we split the original variational graph autoencoder (VGAE) to learn sensitive and non-sensitive latent representations using two sets of encoders. Additionally, we introduce a novel regularization to enforce the independence of the encoders. We prove the theoretical effectiveness of regularization from the perspective of mutual information. Experimental results on three real-world datasets demonstrate that PVGAE outperforms other baselines in private embedding learning regarding utility performance and privacy protection.
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-Two-Layer-Regression-with-Nonlinear-Units"><a href="#Convergence-of-Two-Layer-Regression-with-Nonlinear-Units" class="headerlink" title="Convergence of Two-Layer Regression with Nonlinear Units"></a>Convergence of Two-Layer Regression with Nonlinear Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08358">http://arxiv.org/abs/2308.08358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichuan Deng, Zhao Song, Shenghao Xie</li>
<li>for: 本研究旨在提出一种快速收敛的精度恰当的软MAX-ReLU回归问题解决方案，以便更好地训练大型自然语言模型（LLMs）。</li>
<li>methods: 本文提出了一种基于approx Newton方法的软MAX-ReLU回归算法，该算法在certain assumptions下可以 guarantees the convergence of the solution in the sense of the distance to the optimal solution。</li>
<li>results: 本研究通过计算closed form表示形式的梯度Matrix，并在certain assumptions下证明梯度的 lipschitz continuity和半正定性，从而实现了软MAX-ReLU回归问题的解决。<details>
<summary>Abstract</summary>
Large language models (LLMs), such as ChatGPT and GPT4, have shown outstanding performance in many human life task. Attention computation plays an important role in training LLMs. Softmax unit and ReLU unit are the key structure in attention computation. Inspired by them, we put forward a softmax ReLU regression problem. Generally speaking, our goal is to find an optimal solution to the regression problem involving the ReLU unit. In this work, we calculate a close form representation for the Hessian of the loss function. Under certain assumptions, we prove the Lipschitz continuous and the PSDness of the Hessian. Then, we introduce an greedy algorithm based on approximate Newton method, which converges in the sense of the distance to optimal solution. Last, We relax the Lipschitz condition and prove the convergence in the sense of loss value.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如ChatGPT和GPT4，在许多人类任务中表现出色。计算注意力对于训练LLM非常重要。软饱和ReLU单元是计算注意力的关键结构。受其启发，我们提出了软饱ReLU回归问题。总的来说，我们的目标是找到一个优化的回归解决方案，其中包括ReLU单元。在这种情况下，我们计算了一个快速的表示形式，用于捕捉损失函数的迷你。在某些假设下，我们证明了梯度的 lipschitz 连续和归一化性。然后，我们介绍了一种基于approximate Newton方法的满足搜索算法，该算法在某种意义上 converge。最后，我们松解了 lipschitz 条件，并证明了在损失值上的convergence。
</details></li>
</ul>
<hr>
<h2 id="Is-Meta-Learning-the-Right-Approach-for-the-Cold-Start-Problem-in-Recommender-Systems"><a href="#Is-Meta-Learning-the-Right-Approach-for-the-Cold-Start-Problem-in-Recommender-Systems" class="headerlink" title="Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?"></a>Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08354">http://arxiv.org/abs/2308.08354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Buffelli, Ashish Gupta, Agnieszka Strzalka, Vassilis Plachouras</li>
<li>for:  solve the cold-start problem in deep learning models for recommender systems</li>
<li>methods:  standard and widely adopted deep learning models, common representation learning techniques</li>
<li>results:  comparable performance to meta-learning techniques specifically designed for the cold-start setting, with much easier deployment in real-world applications<details>
<summary>Abstract</summary>
Recommender systems have become fundamental building blocks of modern online products and services, and have a substantial impact on user experience. In the past few years, deep learning methods have attracted a lot of research, and are now heavily used in modern real-world recommender systems. Nevertheless, dealing with recommendations in the cold-start setting, e.g., when a user has done limited interactions in the system, is a problem that remains far from solved. Meta-learning techniques, and in particular optimization-based meta-learning, have recently become the most popular approaches in the academic research literature for tackling the cold-start problem in deep learning models for recommender systems. However, current meta-learning approaches are not practical for real-world recommender systems, which have billions of users and items, and strict latency requirements. In this paper we show that it is possible to obtaining similar, or higher, performance on commonly used benchmarks for the cold-start problem without using meta-learning techniques. In more detail, we show that, when tuned correctly, standard and widely adopted deep learning models perform just as well as newer meta-learning models. We further show that an extremely simple modular approach using common representation learning techniques, can perform comparably to meta-learning techniques specifically designed for the cold-start setting while being much more easily deployable in real-world applications.
</details>
<details>
<summary>摘要</summary>
现代在线产品和服务中，推荐系统已成为基本结构的重要组件，对用户体验产生了深远的影响。过去几年，深度学习方法在研究中吸引了很多注意力，现在在现实世界中广泛应用于现代推荐系统中。然而，在冷开始设定下（例如，用户在系统中有限的交互），仍然是一个未解决的问题。学术研究文献中最受欢迎的方法是使用meta-学习技术来解决这个问题。然而，现有的meta-学习方法在实际应用中并不实用，因为它们需要训练大量数据和计算资源，并且具有严格的延迟要求。在这篇论文中，我们展示了可以在通用的深度学习模型中获得类似或更高的性能，而不需要使用meta-学习技术。具体来说，当正确地调整时，标准的深度学习模型可以与 newer meta-学习模型性能相似。此外，我们还展示了一种非常简单的模块化方法，使用常见的表示学习技术，可以在现实世界应用中与meta-学习技术特化于冷开始设定相比而表现类似，同时更易于部署。</sys>Here's the translation in Simplified Chinese:现代在线产品和服务中，推荐系统已成为基本结构的重要组件，对用户体验产生了深远的影响。过去几年，深度学习方法在研究中吸引了很多注意力，现在在现实世界中广泛应用于现代推荐系统中。然而，在冷开始设定下（例如，用户在系统中有限的交互），仍然是一个未解决的问题。学术研究文献中最受欢迎的方法是使用meta-学习技术来解决这个问题。然而，现有的meta-学习方法在实际应用中并不实用，因为它们需要训练大量数据和计算资源，并且具有严格的延迟要求。在这篇论文中，我们展示了可以在通用的深度学习模型中获得类似或更高的性能，而不需要使用meta-学习技术。具体来说，当正确地调整时，标准的深度学习模型可以与 newer meta-学习模型性能相似。此外，我们还展示了一种非常简单的模块化方法，使用常见的表示学习技术，可以在现实世界应用中与meta-学习技术特化于冷开始设定相比而表现类似，同时更易于部署。
</details></li>
</ul>
<hr>
<h2 id="Graph-Out-of-Distribution-Generalization-with-Controllable-Data-Augmentation"><a href="#Graph-Out-of-Distribution-Generalization-with-Controllable-Data-Augmentation" class="headerlink" title="Graph Out-of-Distribution Generalization with Controllable Data Augmentation"></a>Graph Out-of-Distribution Generalization with Controllable Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08344">http://arxiv.org/abs/2308.08344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Lu, Xiaoying Gan, Ze Zhao, Shiyu Liang, Luoyi Fu, Xinbing Wang, Chenghu Zhou</li>
<li>for: 这篇论文旨在解决图像分类中的选择偏见问题，尤其是在训练和测试数据之间的分布偏移。</li>
<li>methods: 本文提出了一个名为\texttt{OOD-GMixup}的方法，它通过控制训练分布，以解决图像分类中的分布偏移问题。</li>
<li>results: extensive studies on several real-world datasets demonstrate the superiority of 本文提出的方法，比过现有的基eline方法更好。<details>
<summary>Abstract</summary>
Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe \emph{hybrid structure distribution shift} of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose \texttt{OOD-GMixup} to jointly manipulate the training distribution with \emph{controllable data augmentation} in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Secondly, we generate virtual samples with perturbation on graph rationale representation domain to obtain potential OOD training samples. Finally, we propose OOD calibration to measure the distribution deviation of virtual samples by leveraging Extreme Value Theory, and further actively control the training distribution by emphasizing the impact of virtual OOD samples. Extensive studies on several real-world datasets on graph classification demonstrate the superiority of our proposed method over state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在分类图属性方面表现出了惊人的表现。然而，由于训练和测试数据的选择偏袋（例如，训练小图并测试大图，或训练紧凑图并测试稀疏图），分布偏移广泛存在。更重要的是，我们经常观察到图结构分布偏移的“半混合结构”，即同时存在一个稀疏图和一个紧凑图的偏移。这些偏移会导致前一代GNN方法的性能下降，并且在不同的数据集上显示出大的不稳定性。为了解决这个问题，我们提出了\texttt{OOD-GMixup}，一种通过控制数据增强的方法，用于同时控制训练分布和数据 augmentation。具体来说，我们首先提取图理据，以消除由不相关信息引起的假正相关性。然后，我们使用图理据表示域中的扰动生成虚拟样本。最后，我们提出了OOD核验，通过极值理论来测量虚拟样本的分布偏移，并且通过控制训练分布来活动地控制训练过程。我们在一些真实世界的图分类任务上进行了广泛的实验，并证明了我们提出的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Logic-Programs-by-Discovering-Higher-Order-Abstractions"><a href="#Learning-Logic-Programs-by-Discovering-Higher-Order-Abstractions" class="headerlink" title="Learning Logic Programs by Discovering Higher-Order Abstractions"></a>Learning Logic Programs by Discovering Higher-Order Abstractions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08334">http://arxiv.org/abs/2308.08334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Céline Hocquette, Sebastijan Dumančić, Andrew Cropper</li>
<li>for: 本研究旨在找到人类水平的AI需要的新抽象，即高阶抽象。</li>
<li>methods: 本研究使用了逻辑编程，从示例和背景知识中生成逻辑程序。</li>
<li>results: 我们的实验结果表明，与不 refactoring 相比，STEVIE 可以提高预测精度27%，降低学习时间47%。此外，STEVIE 还可以找到可以跨领域传递的抽象。<details>
<summary>Abstract</summary>
Discovering novel abstractions is important for human-level AI. We introduce an approach to discover higher-order abstractions, such as map, filter, and fold. We focus on inductive logic programming, which induces logic programs from examples and background knowledge. We introduce the higher-order refactoring problem, where the goal is to compress a logic program by introducing higher-order abstractions. We implement our approach in STEVIE, which formulates the higher-order refactoring problem as a constraint optimisation problem. Our experimental results on multiple domains, including program synthesis and visual reasoning, show that, compared to no refactoring, STEVIE can improve predictive accuracies by 27% and reduce learning times by 47%. We also show that STEVIE can discover abstractions that transfer to different domains
</details>
<details>
<summary>摘要</summary>
人类水平AI的发现新抽象是重要的。我们提出了一种方法，用于发现更高级别的抽象，如地图、筛选和折叠。我们将焦点放在逻辑编程中，它从示例和背景知识中逻辑程序的induction。我们介绍了更高级别的重构问题，目标是通过引入更高级别的抽象来压缩逻辑程序。我们在STEVIE中实现了我们的方法，它将更高级别的重构问题形式化为约束优化问题。我们的实验结果在多个领域，包括程序生成和视觉理解，表明，相比没有重构，STEVIE可以提高预测精度27%，降低学习时间47%。我们还表明STEVIE可以找到可以在不同领域传递的抽象。
</details></li>
</ul>
<hr>
<h2 id="Warped-geometric-information-on-the-optimisation-of-Euclidean-functions"><a href="#Warped-geometric-information-on-the-optimisation-of-Euclidean-functions" class="headerlink" title="Warped geometric information on the optimisation of Euclidean functions"></a>Warped geometric information on the optimisation of Euclidean functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08305">http://arxiv.org/abs/2308.08305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcelo Hartmann, Bernardo Williams, Hanlin Yu, Mark Girolami, Alessandro Barp, Arto Klami</li>
<li>for: 优化一个具有高维 Euclidian 空间中的实数函数，如机器学习任务中的损失函数或统计推断中的 probaility 分布 logarithm。</li>
<li>methods: 使用折叠 Riemean 几何 notion 重新定义了函数在 Euclidean 空间上的优化问题为一个 Riemean 拓扑上的函数，然后在这个拓扑上寻找函数的最优点。选择的折叠 metric 使得优化问题变成了一个计算友好的 metric-tensor，可以轻松地计算优化方向。</li>
<li>results: 使用 third-order  Taylor 约化来 aproximate geodesic curve，并使用 retraction map 将其拔回到拓扑上。这种方法可以有效地优化 geodesic curve。与标准 Euclidean gradient-based 对抗方法相比，提出的算法在迭代次数到达 converges 和 Hessian-based 优化 Routine 中表现更好。<details>
<summary>Abstract</summary>
We consider the fundamental task of optimizing a real-valued function defined in a potentially high-dimensional Euclidean space, such as the loss function in many machine-learning tasks or the logarithm of the probability distribution in statistical inference. We use the warped Riemannian geometry notions to redefine the optimisation problem of a function on Euclidean space to a Riemannian manifold with a warped metric, and then find the function's optimum along this manifold. The warped metric chosen for the search domain induces a computational friendly metric-tensor for which optimal search directions associate with geodesic curves on the manifold becomes easier to compute. Performing optimization along geodesics is known to be generally infeasible, yet we show that in this specific manifold we can analytically derive Taylor approximations up to third-order. In general these approximations to the geodesic curve will not lie on the manifold, however we construct suitable retraction maps to pull them back onto the manifold. Therefore, we can efficiently optimize along the approximate geodesic curves. We cover the related theory, describe a practical optimization algorithm and empirically evaluate it on a collection of challenging optimisation benchmarks. Our proposed algorithm, using third-order approximation of geodesics, outperforms standard Euclidean gradient-based counterparts in term of number of iterations until convergence and an alternative method for Hessian-based optimisation routines.
</details>
<details>
<summary>摘要</summary>
我们考虑一个基本任务，即优化一个定义在可能高维欧几学空间中的实数函数，例如机器学习任务中的损函数或统计推断中的Logarithm的分布函数。我们使用扭曲的里曼纹理观念来重新定义欧几学空间上的函数优化问题为一个里曼拓扑上的函数优化问题，然后在这个拓扑上找到函数的最优点。选择的扭曲纹理在搜索空间上引入了一个计算友好的矩阵对应，其中优化搜索方向与拓扑上的地odesic曲线相关的计算变得更加容易。尽管在拓扑上的搜索通常是不可能的，但我们表明在这种特殊拓扑上，我们可以 analytically derivate Taylor approximations up to third-order。这些近似曲线不会在拓扑上，但我们可以构造适当的 retraction map 将其拟合回拓扑上。因此，我们可以高效地优化这些近似曲线。我们还详细介绍了相关理论、实践的优化算法以及对一系列困难优化问题的实验评估。我们的提议的算法，使用第三阶 Taylor 近似，在迭代次数 until convergence 和一种基于Hessian的优化方法之间占据了优势。
</details></li>
</ul>
<hr>
<h2 id="Robust-Bayesian-Satisficing"><a href="#Robust-Bayesian-Satisficing" class="headerlink" title="Robust Bayesian Satisficing"></a>Robust Bayesian Satisficing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08291">http://arxiv.org/abs/2308.08291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Artun Saday, Yaşar Cahit Yıldırım, Cem Tekin</li>
<li>for: 本研究旨在掌握robust satisficing（RS）在contextual Bayesian optimization中的问题，以及在存在 Context 的假设分布与真实分布之间的差异。</li>
<li>methods: 我们提出了一种新的robust Bayesian satisficing算法（RoBOS），用于启发黑盒优化。我们的算法在某些假设下保证了下凹的 regret。此外，我们还定义了一种弱化的 regret 度量，称为 robust satisficing regret，我们的算法在这种情况下实现了下凹的上界独立于分布差异。</li>
<li>results: 我们在各种学习问题中应用了我们的方法，并与其他方法，如分布ally robust optimization，进行比较。我们的结果显示，RoBOS 能够在不同的学习问题中提供更好的性能，并且可以适应不同的分布差异。<details>
<summary>Abstract</summary>
Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally robust optimization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTDistributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally robust optimization.TRANSLATE_TEXT以下是文章的中文翻译：Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. 本文关注在Contextual Bayesian optimization中的RS问题，当真实分布与参考分布之间存在差异时。我们提出了一种名为RoBOS的robust Bayesian satisficing算法，用于黑盒优化。我们的算法在certain assumptions中 guarantee sublinear lenient regret。此外，我们定义了一种弱的 regret called robust satisficing regret，在这种情况下，我们的算法 achieves sublinear upper bound，不受分布shift的影响。为证明我们的方法的效果，我们将其应用于various learning problems，并与其他方法进行比较，如distributionally robust optimization。
</details></li>
</ul>
<hr>
<h2 id="DFedADMM-Dual-Constraints-Controlled-Model-Inconsistency-for-Decentralized-Federated-Learning"><a href="#DFedADMM-Dual-Constraints-Controlled-Model-Inconsistency-for-Decentralized-Federated-Learning" class="headerlink" title="DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning"></a>DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08290">http://arxiv.org/abs/2308.08290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinglun Li, Li Shen, Guanghao Li, Quanjun Yin, Dacheng Tao</li>
<li>for: 这个论文的目的是提出一种基于分布式学习的训练方法，以解决联合学习中的通信负担问题。</li>
<li>methods: 这个论文使用的方法是基于ADMM的分布式优化算法，并在这个基础上提出了两种改进版本：DFedADMM和DFedADMM-SAM。</li>
<li>results: 实验表明，这些算法在MNIST、CIFAR10和CIFAR100数据集上具有较高的泛化性和更快的收敛速度，比既有的状态态峰值优化器（SOTA）在分布式学习中表现更好。<details>
<summary>Abstract</summary>
To address the communication burden issues associated with federated learning (FL), decentralized federated learning (DFL) discards the central server and establishes a decentralized communication network, where each client communicates only with neighboring clients. However, existing DFL methods still suffer from two major challenges: local inconsistency and local heterogeneous overfitting, which have not been fundamentally addressed by existing DFL methods. To tackle these issues, we propose novel DFL algorithms, DFedADMM and its enhanced version DFedADMM-SAM, to enhance the performance of DFL. The DFedADMM algorithm employs primal-dual optimization (ADMM) by utilizing dual variables to control the model inconsistency raised from the decentralized heterogeneous data distributions. The DFedADMM-SAM algorithm further improves on DFedADMM by employing a Sharpness-Aware Minimization (SAM) optimizer, which uses gradient perturbations to generate locally flat models and searches for models with uniformly low loss values to mitigate local heterogeneous overfitting. Theoretically, we derive convergence rates of $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}}+\frac{1}{KT(1-\psi)^2}\Big)$ and $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}}+\frac{1}{KT(1-\psi)^2}+ \frac{1}{T^{3/2}K^{1/2}}\Big)$ in the non-convex setting for DFedADMM and DFedADMM-SAM, respectively, where $1 - \psi$ represents the spectral gap of the gossip matrix. Empirically, extensive experiments on MNIST, CIFAR10 and CIFAR100 datesets demonstrate that our algorithms exhibit superior performance in terms of both generalization and convergence speed compared to existing state-of-the-art (SOTA) optimizers in DFL.
</details>
<details>
<summary>摘要</summary>
为了解决联合学习（FL）中的通信负担问题，分布式联合学习（DFL）抛弃中央服务器，建立了分布式通信网络，每个客户端只与周围的客户端进行通信。然而，现有的DFL方法仍面临两个主要挑战：本地不一致和本地特异适应，这些问题尚未由现有的DFL方法得到基本解决。为此，我们提出了新的DFL算法，即DFedADMM和其加强版DFedADMM-SAM，以提高DFL的性能。DFedADMM算法使用了 primal-dual 优化（ADMM），通过使用 dual 变量控制分布式不同数据分布引起的模型不一致。DFedADMM-SAM算法进一步改进了 DFedADMM，通过使用 Sharpness-Aware Minimization（SAM）优化器，通过斜坡值误差来生成本地平滑模型，并在搜索本地特异适应模型时使用 Gradient Perturbations。理论上，我们得出了 $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}}+\frac{1}{KT(1-\psi)^2}\Big)$ 和 $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}}+\frac{1}{KT(1-\psi)^2}+\frac{1}{T^{3/2}K^{1/2}}\Big)$ 的收敛速率在非对称设定下，其中 $1 - \psi$ 表示热度矩阵的spectral gap。实验结果表明，我们的算法在 MNIST、CIFAR10 和 CIFAR100 数据集上展现出了与现有最佳优化器相比的优秀表现，包括总体化和收敛速度。
</details></li>
</ul>
<hr>
<h2 id="CARE-A-Large-Scale-CT-Image-Dataset-and-Clinical-Applicable-Benchmark-Model-for-Rectal-Cancer-Segmentation"><a href="#CARE-A-Large-Scale-CT-Image-Dataset-and-Clinical-Applicable-Benchmark-Model-for-Rectal-Cancer-Segmentation" class="headerlink" title="CARE: A Large Scale CT Image Dataset and Clinical Applicable Benchmark Model for Rectal Cancer Segmentation"></a>CARE: A Large Scale CT Image Dataset and Clinical Applicable Benchmark Model for Rectal Cancer Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08283">http://arxiv.org/abs/2308.08283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hantao Zhang, Weidong Guo, Chenyang Qiu, Shouhong Wan, Bingbing Zou, Wanqin Wang, Peiquan Jin</li>
<li>for: 这个论文的目的是提出一种新的大规模RECTAL CANCER CT图像数据集CARE，以及一种特有的医疗器官癌症分割模型U-SAM，以解决RECTAL CANCER CT图像分割precision的问题。</li>
<li>methods: 这个论文使用了一种新的大规模RECTAL CANCER CT图像数据集CARE，并提出了一种特有的医疗器官癌症分割模型U-SAM，该模型包括三个关键组件：提示信息（例如，点）， convolution模块，和 skip-connection，以解决肠道器官的复杂结构。</li>
<li>results: 该论文的实验结果表明，提出的U-SAM模型在CARE数据集和WORD数据集上都能够达到state-of-the-art的性能水平，并且可以serve as the baseline for future research和临床应用开发。<details>
<summary>Abstract</summary>
Rectal cancer segmentation of CT image plays a crucial role in timely clinical diagnosis, radiotherapy treatment, and follow-up. Although current segmentation methods have shown promise in delineating cancerous tissues, they still encounter challenges in achieving high segmentation precision. These obstacles arise from the intricate anatomical structures of the rectum and the difficulties in performing differential diagnosis of rectal cancer. Additionally, a major obstacle is the lack of a large-scale, finely annotated CT image dataset for rectal cancer segmentation. To address these issues, this work introduces a novel large scale rectal cancer CT image dataset CARE with pixel-level annotations for both normal and cancerous rectum, which serves as a valuable resource for algorithm research and clinical application development. Moreover, we propose a novel medical cancer lesion segmentation benchmark model named U-SAM. The model is specifically designed to tackle the challenges posed by the intricate anatomical structures of abdominal organs by incorporating prompt information. U-SAM contains three key components: promptable information (e.g., points) to aid in target area localization, a convolution module for capturing low-level lesion details, and skip-connections to preserve and recover spatial information during the encoding-decoding process. To evaluate the effectiveness of U-SAM, we systematically compare its performance with several popular segmentation methods on the CARE dataset. The generalization of the model is further verified on the WORD dataset. Extensive experiments demonstrate that the proposed U-SAM outperforms state-of-the-art methods on these two datasets. These experiments can serve as the baseline for future research and clinical application development.
</details>
<details>
<summary>摘要</summary>
癌症肛部分 segmentation CT 图像在至关重要的诊断、放疗治疗和跟踪中扮演着关键角色。尽管当前的分 segmentation 方法已经展示了潜在的精度，但它们仍然遇到了在准确地分 segmentation 癌症组织的挑战。这些障碍来自肛部的复杂 анатомиче 结构以及Difficulties in performing differential diagnosis of rectal cancer。此外，lack of a large-scale, finely annotated CT image dataset for rectal cancer segmentation。 To address these issues, this work introduces a novel large-scale rectal cancer CT image dataset CARE with pixel-level annotations for both normal and cancerous rectum, which serves as a valuable resource for algorithm research and clinical application development. Moreover, we propose a novel medical cancer lesion segmentation benchmark model named U-SAM. The model is specifically designed to tackle the challenges posed by the intricate anatomical structures of abdominal organs by incorporating prompt information. U-SAM contains three key components: promptable information (e.g., points) to aid in target area localization, a convolution module for capturing low-level lesion details, and skip-connections to preserve and recover spatial information during the encoding-decoding process. To evaluate the effectiveness of U-SAM, we systematically compare its performance with several popular segmentation methods on the CARE dataset. The generalization of the model is further verified on the WORD dataset. Extensive experiments demonstrate that the proposed U-SAM outperforms state-of-the-art methods on these two datasets. These experiments can serve as the baseline for future research and clinical application development.
</details></li>
</ul>
<hr>
<h2 id="It-Ain’t-That-Bad-Understanding-the-Mysterious-Performance-Drop-in-OOD-Generalization-for-Generative-Transformer-Models"><a href="#It-Ain’t-That-Bad-Understanding-the-Mysterious-Performance-Drop-in-OOD-Generalization-for-Generative-Transformer-Models" class="headerlink" title="It Ain’t That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models"></a>It Ain’t That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08268">http://arxiv.org/abs/2308.08268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingcheng Xu, Zihao Pan, Haipeng Zhang, Yanqing Yang</li>
<li>for:  investigate the generalization behaviors of Generative Transformer-based models</li>
<li>methods:  using n-digit addition and multiplication tasks to study the models’ generalization abilities</li>
<li>results:  discovered that the models have structured representations and learned algebraic structures, but struggle with out-of-distribution inputs<details>
<summary>Abstract</summary>
Generative Transformer-based models have achieved remarkable proficiency on solving diverse problems. However, their generalization ability is not fully understood and not always satisfying. Researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. Curiously, it is observed that when training on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably and mysteriously on longer, unseen cases (out-of-distribution (OOD) generalization). Studies try to bridge this gap with workarounds such as modifying position embedding, fine-tuning, and priming with more extensive or instructive data. However, without addressing the essential mechanism, there is hardly any guarantee regarding the robustness of these solutions. We bring this unexplained performance drop into attention and ask whether it is purely from random errors. Here we turn to the mechanistic line of research which has notable successes in model interpretability. We discover that the strong ID generalization stems from structured representations, while behind the unsatisfying OOD performance, the models still exhibit clear learned algebraic structures. Specifically, these models map unseen OOD inputs to outputs with equivalence relations in the ID domain. These highlight the potential of the models to carry useful information for improved generalization.
</details>
<details>
<summary>摘要</summary>
We bring attention to this unexplained performance drop and question whether it is due to random errors. To address this, we turn to the mechanistic line of research, which has been successful in model interpretability. We find that the strong ID generalization is due to structured representations, while the unsatisfying OOD performance is caused by the models still exhibiting clear learned algebraic structures. Specifically, these models map unseen OOD inputs to outputs with equivalence relations in the ID domain, highlighting the potential for improved generalization.
</details></li>
</ul>
<hr>
<h2 id="Graph-Relation-Aware-Continual-Learning"><a href="#Graph-Relation-Aware-Continual-Learning" class="headerlink" title="Graph Relation Aware Continual Learning"></a>Graph Relation Aware Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08259">http://arxiv.org/abs/2308.08259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghua Shen, Weijieying Ren, Wei Qin</li>
<li>for: 本研究探讨了从无穷数据流中学习图像，卷积历史知识，并将其推广到未来任务。在这个任务中，只有当前图像数据可用。</li>
<li>methods: 本研究使用了一种叫做 relation-aware adaptive model（RAM-CG），它包括一个用于发现 latent relations 的模块和一个用于考虑时间推移的掩模。</li>
<li>results: 实验结果显示，RAM-CG 相比于当前状态的最佳结果，在 CitationNet、OGBN-arxiv 和 TWITCH 数据集上提供了显著的 2.2%、6.9% 和 6.6% 的改进。<details>
<summary>Abstract</summary>
Continual graph learning (CGL) studies the problem of learning from an infinite stream of graph data, consolidating historical knowledge, and generalizing it to the future task. At once, only current graph data are available. Although some recent attempts have been made to handle this task, we still face two potential challenges: 1) most of existing works only manipulate on the intermediate graph embedding and ignore intrinsic properties of graphs. It is non-trivial to differentiate the transferred information across graphs. 2) recent attempts take a parameter-sharing policy to transfer knowledge across time steps or progressively expand new architecture given shifted graph distribution. Learning a single model could loss discriminative information for each graph task while the model expansion scheme suffers from high model complexity. In this paper, we point out that latent relations behind graph edges can be attributed as an invariant factor for the evolving graphs and the statistical information of latent relations evolves. Motivated by this, we design a relation-aware adaptive model, dubbed as RAM-CG, that consists of a relation-discovery modular to explore latent relations behind edges and a task-awareness masking classifier to accounts for the shifted. Extensive experiments show that RAM-CG provides significant 2.2%, 6.9% and 6.6% accuracy improvements over the state-of-the-art results on CitationNet, OGBN-arxiv and TWITCH dataset, respective.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Two-Phases-of-Scaling-Laws-for-Nearest-Neighbor-Classifiers"><a href="#Two-Phases-of-Scaling-Laws-for-Nearest-Neighbor-Classifiers" class="headerlink" title="Two Phases of Scaling Laws for Nearest Neighbor Classifiers"></a>Two Phases of Scaling Laws for Nearest Neighbor Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08247">http://arxiv.org/abs/2308.08247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengkun Yang, Jingzhao Zhang</li>
<li>for: 本文研究近邻类ifiers的扩展法律。</li>
<li>methods: 作者使用了数据分布的复杂性来解释模型的通用错误。</li>
<li>results: 作者发现了两个阶段的扩展法律：在第一阶段，通用错误与数据维度之间存在直接的 polynomial 关系，而在第二阶段，错误与数据维度之间存在 exponential 关系。这种分布复杂性对模型的通用性产生了重要的影响。当数据分布宽泛时，近邻类ifiers 可以实现 polynomial 类型的通用错误，而不是 exponential 类型。<details>
<summary>Abstract</summary>
A scaling law refers to the observation that the test performance of a model improves as the number of training data increases. A fast scaling law implies that one can solve machine learning problems by simply boosting the data and the model sizes. Yet, in many cases, the benefit of adding more data can be negligible. In this work, we study the rate of scaling laws of nearest neighbor classifiers. We show that a scaling law can have two phases: in the first phase, the generalization error depends polynomially on the data dimension and decreases fast; whereas in the second phase, the error depends exponentially on the data dimension and decreases slowly. Our analysis highlights the complexity of the data distribution in determining the generalization error. When the data distributes benignly, our result suggests that nearest neighbor classifier can achieve a generalization error that depends polynomially, instead of exponentially, on the data dimension.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文版本）一个减小法（scaling law）指的是模型在训练数据量增加后测试性能的改善。快速减小法则意味着只需增加数据和模型大小就可以解决机器学习问题。然而，在许多情况下，增加更多数据的效果可能是微乎其微。在这项工作中，我们研究近似 neighboor 类型的减小法。我们发现一个减小法可以分为两个阶段：在第一阶段，总体错误取决于数据维度的度量函数，随着数据维度增加而快速下降;而在第二阶段，错误取决于数据维度的指数函数，随着数据维度增加而慢速下降。我们的分析表明数据分布的复杂性对总体错误的确定产生了重要影响。当数据分布良好时，我们的结果表明，近似 neighboor 类型的模型可以实现数据维度取决于指数函数而不是指数函数的总体错误。
</details></li>
</ul>
<hr>
<h2 id="The-Expressive-Power-of-Graph-Neural-Networks-A-Survey"><a href="#The-Expressive-Power-of-Graph-Neural-Networks-A-Survey" class="headerlink" title="The Expressive Power of Graph Neural Networks: A Survey"></a>The Expressive Power of Graph Neural Networks: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08235">http://arxiv.org/abs/2308.08235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingxu Zhang, Changjun Fan, Shixuan Liu, Kuihua Huang, Xiang Zhao, Jincai Huang, Zhong Liu</li>
<li>for: 本研究旨在探讨图 neural network (GNN) 的表达能力问题，即 GNN 能够学习什么样的图结构和特征。</li>
<li>methods: 本研究使用了多种方法来探讨 GNN 的表达能力，包括图特征增强、图结构增强和 GNN 架构增强等方法。</li>
<li>results: 本研究结果显示，通过不同的定义和方法，GNN 可以具有不同的表达能力，并且可以用于解决多种图学和机器学习问题。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) are effective machine learning models for many graph-related applications. Despite their empirical success, many research efforts focus on the theoretical limitations of GNNs, i.e., the GNNs expressive power. Early works in this domain mainly focus on studying the graph isomorphism recognition ability of GNNs, and recent works try to leverage the properties such as subgraph counting and connectivity learning to characterize the expressive power of GNNs, which are more practical and closer to real-world. However, no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a first survey for models for enhancing expressive power under different forms of definition. Concretely, the models are reviewed based on three categories, i.e., Graph feature enhancement, Graph topology enhancement, and GNNs architecture enhancement.
</details>
<details>
<summary>摘要</summary>
格Nodes neural networks (GNNs) 是多种图像应用中的有效机器学习模型。 Despite their empirical success, many research efforts focus on the theoretical limitations of GNNs, i.e., the GNNs expressive power. Early works in this domain mainly focus on studying the graph isomorphism recognition ability of GNNs, and recent works try to leverage the properties such as subgraph counting and connectivity learning to characterize the expressive power of GNNs, which are more practical and closer to real-world. However, no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a first survey for models for enhancing expressive power under different forms of definition. Concretely, the models are reviewed based on three categories, i.e., 图像特征增强, 图像结构增强, and GNNs 架构增强.
</details></li>
</ul>
<hr>
<h2 id="Challenges-and-Opportunities-of-Using-Transformer-Based-Multi-Task-Learning-in-NLP-Through-ML-Lifecycle-A-Survey"><a href="#Challenges-and-Opportunities-of-Using-Transformer-Based-Multi-Task-Learning-in-NLP-Through-ML-Lifecycle-A-Survey" class="headerlink" title="Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey"></a>Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08234">http://arxiv.org/abs/2308.08234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lovre Torbarina, Tin Ferkovic, Lukasz Roguski, Velimir Mihelcic, Bruno Sarlija, Zeljko Kraljevic</li>
<li>for: 本研究旨在提高自然语言处理（NLP）模型的效率和性能，通过 JOINT 训练多个模型，而不是单独训练每个模型。</li>
<li>methods: 本文提出了一种基于 transformer 的多任务学习（MTL）方法，并系统地分析了在 NLP 领域中这种方法如何应用于各个机器学习生命周期阶段。</li>
<li>results: 本文提出了一种基于 transformer 的 MTL 方法，并对这种方法的应用进行了系统的分析，包括数据工程、模型开发、部署和监测等阶段。此外，本文还提出了一种将 MTL 与 continual learning（CL）相连的想法，以便在模型 Periodically 重新训练、更新和添加新功能等方面具有更高的灵活性和可扩展性。<details>
<summary>Abstract</summary>
The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it systematically analyses how transformer-based MTL in NLP fits into ML lifecycle phases. Furthermore, we motivate research on the connection between MTL and continual learning (CL), as this area remains unexplored. We believe it would be practical to have a model that can handle both MTL and CL, as this would make it easier to periodically re-train the model, update it due to distribution shifts, and add new capabilities to meet real-world requirements.
</details>
<details>
<summary>摘要</summary>
随着自然语言处理（NLP）模型在不同领域的推广，机器学习（ML）实践者需要能够高效地训练、部署和更新多个模型，从训练到生产环境中的部署。然而，训练、部署和更新多个模型可能会复杂、成本高和时间consuming，特别是使用基于转换器的预训练语言模型。多任务学习（MTL）已经出现为提高效率和性能的可能性，我们将提供一个概述 transformer-based MTL 在 NLP 中的方法。然后，我们将讨论在 ML 生命周期阶段中使用 MTL approaches 的挑战和机遇，特别是关注数据工程、模型开发、部署和监控阶段的挑战。本文将重点关注基于 transformer 的 MTL 架构，并且，到我们所知道的 extend，这是一篇系统性的分析文章，探讨了 transformer-based MTL 在 NLP 中如何适应 ML 生命周期阶段。此外，我们还motivates 研究将 MTL 和 continual learning（CL）相连接，因为这个领域还没有得到过足的研究。我们认为，一个能够同时处理 MTL 和 CL 的模型会更加实用，这样可以更加方便地在 periodic 训练、因 distribution shift 更新模型以及添加新功能来满足实际需求。
</details></li>
</ul>
<hr>
<h2 id="SCQPTH-an-efficient-differentiable-splitting-method-for-convex-quadratic-programming"><a href="#SCQPTH-an-efficient-differentiable-splitting-method-for-convex-quadratic-programming" class="headerlink" title="SCQPTH: an efficient differentiable splitting method for convex quadratic programming"></a>SCQPTH: an efficient differentiable splitting method for convex quadratic programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08232">http://arxiv.org/abs/2308.08232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Butler</li>
<li>for: 这篇论文主要是为了提出一种可微分的第一阶分裂法（SCQPTH），用于解 convex quadratic programs（QPs）。</li>
<li>methods: 这种方法基于 alternating direction method of multipliers（ADMM），并且受到现有的 state-of-the-art solver OSQP：一种操作分裂解决方案的启发。</li>
<li>results: 实验表明，对于大规模的 QPs，SCQPTH 可以提供 $1\times - 10\times$ 的计算效率提升，相比现有的可微分 QP 解决方案。<details>
<summary>Abstract</summary>
We present SCQPTH: a differentiable first-order splitting method for convex quadratic programs. The SCQPTH framework is based on the alternating direction method of multipliers (ADMM) and the software implementation is motivated by the state-of-the art solver OSQP: an operating splitting solver for convex quadratic programs (QPs). The SCQPTH software is made available as an open-source python package and contains many similar features including efficient reuse of matrix factorizations, infeasibility detection, automatic scaling and parameter selection. The forward pass algorithm performs operator splitting in the dimension of the original problem space and is therefore suitable for large scale QPs with $100-1000$ decision variables and thousands of constraints. Backpropagation is performed by implicit differentiation of the ADMM fixed-point mapping. Experiments demonstrate that for large scale QPs, SCQPTH can provide a $1\times - 10\times$ improvement in computational efficiency in comparison to existing differentiable QP solvers.
</details>
<details>
<summary>摘要</summary>
我们介绍了 SCQPTH：一种可微分的首选分解方法 для凸quadratic programs。 SCQPTH框架基于alternating direction method of multipliers（ADMM），并且由state-of-the-art solver OSQP：一种操作分裂解决方法 для凸quadratic programs（QPs）所 inspirited。 SCQPTH软件作为开源python包，具有许多相似特点，包括高效的矩阵因子重用、不可行检测、自动缩放和参数选择。 forward pass算法在原始问题空间的维度进行operator splitting，适用于大规模QPs，具有100-1000个决策变量和千个约束。 backpropagation通过ADMM固定点映射的隐式导数计算。实验表明，对于大规模QPs，SCQPTH可以提供1\*-10\*的计算效率提升，相比现有的可微分QP解决方法。
</details></li>
</ul>
<hr>
<h2 id="Self-Deception-Reverse-Penetrating-the-Semantic-Firewall-of-Large-Language-Models"><a href="#Self-Deception-Reverse-Penetrating-the-Semantic-Firewall-of-Large-Language-Models" class="headerlink" title="Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models"></a>Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11521">http://arxiv.org/abs/2308.11521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen Gui, Enze Wang<br>for:* The paper investigates the “LLM jailbreak” problem and proposes an automatic jailbreak method for the first time.methods:* The paper introduces the concept of a “semantic firewall” and provides three technical implementation approaches.* The paper introduces a “self-deception” attack that can bypass the semantic firewall by inducing LLM to generate prompts that facilitate jailbreak.results:* The paper reports a success rate of 86.2% and 67% on two models (GPT-3.5-Turbo and GPT-4) in generating attack payloads that can bypass the semantic firewall.* The paper also reports a failure rate of 4.7% and 2.2% on the two models, respectively.<details>
<summary>Abstract</summary>
Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.   This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates traditional firewalls through reverse tunnels, we introduce a "self-deception" attack that can bypass the semantic firewall by inducing LLM to generate prompts that facilitate jailbreak. We generated a total of 2,520 attack payloads in six languages (English, Russian, French, Spanish, Chinese, and Arabic) across seven virtual scenarios, targeting the three most common types of violations: violence, hate, and pornography. The experiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The success rates on the two models were 86.2% and 67%, while the failure rates were 4.7% and 2.2%, respectively. This highlighted the effectiveness of the proposed attack method. All experimental code and raw data will be released as open-source to inspire future research. We believe that manipulating AI behavior through carefully crafted prompts will become an important research direction in the future.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如ChatGPT，已经出现了不可思议的能力，接近人工智能。它们为社会各种需求提供了便利，但也降低了生成危险内容的成本。因此，LLM开发者已经部署了semantic-level防御，以识别和拒绝可能导致不当内容的提示。然而，这些防御不是不可攻击的，一些攻击者已经制作了“监狱折衣”提示，使LLM忘记内容防御规则，回答任何不当问题。迄今为止，在业界和学术界都没有明确的semantic-level攻击和防御原理的解释。本文 investigate LLM监狱问题，并提出了自动监狱方法的第一次实现。我们提出了semantic firewall的概念，并提供了三种技术实现方式。受到传统防火墙被穿越反터 tunneled攻击的启发，我们引入了“自我欺骗”攻击，可以绕过semantic firewall，使LLM生成提示，促使监狱。我们总共生成了2,520个攻击 payload，分别在英语、俄语、法语、西班牙语、中文和阿拉伯语七种语言中，targeting三种最常见的违规行为：暴力、仇恨和色情。实验在GPT-3.5-Turbo和GPT-4两个模型上进行，成功率分别为86.2%和67%，失败率分别为4.7%和2.2%。这表明了我们提出的攻击方法的效iveness。我们将所有实验代码和原始数据发布为开源，以便未来的研究。我们认为，通过精心制作的提示，控制AI行为将成为未来的重要研究方向。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Winograd-Convolution-for-Cost-effective-Neural-Network-Fault-Tolerance"><a href="#Exploring-Winograd-Convolution-for-Cost-effective-Neural-Network-Fault-Tolerance" class="headerlink" title="Exploring Winograd Convolution for Cost-effective Neural Network Fault Tolerance"></a>Exploring Winograd Convolution for Cost-effective Neural Network Fault Tolerance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08230">http://arxiv.org/abs/2308.08230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinghua Xue, Cheng Liu, Bo Liu, Haitong Huang, Ying Wang, Tao Luo, Lei Zhang, Huawei Li, Xiaowei Li</li>
<li>for: 本文研究了Winograd核函数在神经网络中的稳定性，以提高神经网络的硬件缺陷忍容性。</li>
<li>methods: 本文从不同的粒度（模型、层、操作类型）进行了全面的Winograd核函数缺陷忍容性评估。然后，本文探讨了在Winograd核函数基础上实现成本效果的NN保护策略。</li>
<li>results: 实验结果表明，Winograd核函数可以在不减少精度的情况下减少硬件缺陷忍容性设计负担，并且可以在具有不同硬件缺陷的情况下提高NN的精度。<details>
<summary>Abstract</summary>
Winograd is generally utilized to optimize convolution performance and computational efficiency because of the reduced multiplication operations, but the reliability issues brought by winograd are usually overlooked. In this work, we observe the great potential of winograd convolution in improving neural network (NN) fault tolerance. Based on the observation, we evaluate winograd convolution fault tolerance comprehensively from different granularities ranging from models, layers, and operation types for the first time. Then, we explore the use of inherent fault tolerance of winograd convolution for cost-effective NN protection against soft errors. Specifically, we mainly investigate how winograd convolution can be effectively incorporated with classical fault-tolerant design approaches including triple modular redundancy (TMR), fault-aware retraining, and constrained activation functions. According to our experiments, winograd convolution can reduce the fault-tolerant design overhead by 55.77\% on average without any accuracy loss compared to standard convolution, and further reduce the computing overhead by 17.24\% when the inherent fault tolerance of winograd convolution is considered. When it is applied on fault-tolerant neural networks enhanced with fault-aware retraining and constrained activation functions, the resulting model accuracy generally shows significant improvement in presence of various faults.
</details>
<details>
<summary>摘要</summary>
Winograd通常用于优化卷积性能和计算效率，因为它减少了乘法操作数量，但Winograd的可靠性问题通常被忽略。在这种工作中，我们发现Winograd卷积可以提高神经网络（NN）fault tolerance的潜力。基于这一观察，我们系统地评估Winograd卷积 fault tolerance从不同的粒度（models、layers、operation types）开始。然后，我们探索使用Winograd卷积的内在fault tolerance来实现cost-effective NN保护 against soft errors。具体来说，我们主要研究如何有效地将Winograd卷积与经典的 fault-tolerant设计方法（如TMR、 fault-aware retraining和受限 activation functions）结合使用。根据我们的实验，Winograd卷积可以在标准卷积的基础上减少fault-tolerant设计开销55.77%，而且在考虑Winograd卷积的内在fault tolerance时，可以减少计算开销17.24%。当应用于强化了 fault-tolerant神经网络的Winograd卷积、 fault-aware retraining和受限 activation functions后，模型的准确率在不同类型的缺陷情况下都显示了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Inherent-Redundancy-in-Spiking-Neural-Networks"><a href="#Inherent-Redundancy-in-Spiking-Neural-Networks" class="headerlink" title="Inherent Redundancy in Spiking Neural Networks"></a>Inherent Redundancy in Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08227">http://arxiv.org/abs/2308.08227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biclab/asa-snn">https://github.com/biclab/asa-snn</a></li>
<li>paper_authors: Man Yao, Jiakui Hu, Guangshe Zhao, Yaoyuan Wang, Ziyang Zhang, Bo Xu, Guoqi Li</li>
<li>for: 本研究旨在探讨隐藏在神经网络中的内在重复性，以提高神经网络的准确率和能效性。</li>
<li>methods: 本研究使用了隐藏状态激活（HSA）模块，以适应神经网络中的内在重复性，并对神经网络的各个元素进行了优化。</li>
<li>results: 实验结果表明，提案的方法可以显著减少神经网络中的冲击脉冲，并在比较于现有神经网络基eline上显示更好的性能。<details>
<summary>Abstract</summary>
Spiking Neural Networks (SNNs) are well known as a promising energy-efficient alternative to conventional artificial neural networks. Subject to the preconceived impression that SNNs are sparse firing, the analysis and optimization of inherent redundancy in SNNs have been largely overlooked, thus the potential advantages of spike-based neuromorphic computing in accuracy and energy efficiency are interfered. In this work, we pose and focus on three key questions regarding the inherent redundancy in SNNs. We argue that the redundancy is induced by the spatio-temporal invariance of SNNs, which enhances the efficiency of parameter utilization but also invites lots of noise spikes. Further, we analyze the effect of spatio-temporal invariance on the spatio-temporal dynamics and spike firing of SNNs. Then, motivated by these analyses, we propose an Advance Spatial Attention (ASA) module to harness SNNs' redundancy, which can adaptively optimize their membrane potential distribution by a pair of individual spatial attention sub-modules. In this way, noise spike features are accurately regulated. Experimental results demonstrate that the proposed method can significantly drop the spike firing with better performance than state-of-the-art SNN baselines. Our code is available in \url{https://github.com/BICLab/ASA-SNN}.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）已经广泛认可为一种能效的人工神经网络 alternatives。然而，由于人们对 SNN 的偏见，即 SNN 是稀疏的发射，因此对 SNN 内部缺乏 redundancy 的分析和优化，从而阻碍了 SNN 在准确性和能效性方面的潜在优势。在这个工作中，我们提出了三个关键问题，关于 SNN 中的内部缺乏 redundancy。我们认为，这种缺乏 redundancy 是由 SNN 的空间-时间不变性引起的，这种不变性可以提高参数的使用效率，但也会引入很多噪声脉冲。然后，我们分析了 SNN 的空间-时间动力学和脉冲发生的影响。根据这些分析结果，我们提出了一种 Advance Spatial Attention（ASA）模块，可以利用 SNN 的缺乏 redundancy，并可以自适应调整 SNN 的膜电压分布。这样，可以准确地控制噪声脉冲特征。实验结果表明，我们的方法可以显著降低 SNN 的脉冲发生，并且比现有的 SNN 基eline 性能更好。我们的代码可以在 \url{https://github.com/BICLab/ASA-SNN} 上找到。
</details></li>
</ul>
<hr>
<h2 id="How-To-Overcome-Confirmation-Bias-in-Semi-Supervised-Image-Classification-By-Active-Learning"><a href="#How-To-Overcome-Confirmation-Bias-in-Semi-Supervised-Image-Classification-By-Active-Learning" class="headerlink" title="How To Overcome Confirmation Bias in Semi-Supervised Image Classification By Active Learning"></a>How To Overcome Confirmation Bias in Semi-Supervised Image Classification By Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08224">http://arxiv.org/abs/2308.08224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandra Gilhuber, Rasmus Hvingelby, Mang Ling Ada Fok, Thomas Seidl</li>
<li>for: 本研究是为了检验是否需要活动学习，因为强大的深度半supervised方法的出现使得有限的标注数据设置中的活动学习可能失效。</li>
<li>methods: 本研究使用了semi-supervised learning（SSL）方法和活动学习（AL）方法，并 comparing their performance in realistic data scenarios。</li>
<li>results: 研究发现，在实际数据场景中，SSL方法可能会受到between-class imbalance、within-class imbalance和between-class similarity等挑战，这些挑战可能会导致SSL性能下降。然而，通过使用AL方法，可以超越confirmation bias，并在这些实际数据场景中提高SSL性能。<details>
<summary>Abstract</summary>
Do we need active learning? The rise of strong deep semi-supervised methods raises doubt about the usability of active learning in limited labeled data settings. This is caused by results showing that combining semi-supervised learning (SSL) methods with a random selection for labeling can outperform existing active learning (AL) techniques. However, these results are obtained from experiments on well-established benchmark datasets that can overestimate the external validity. However, the literature lacks sufficient research on the performance of active semi-supervised learning methods in realistic data scenarios, leaving a notable gap in our understanding. Therefore we present three data challenges common in real-world applications: between-class imbalance, within-class imbalance, and between-class similarity. These challenges can hurt SSL performance due to confirmation bias. We conduct experiments with SSL and AL on simulated data challenges and find that random sampling does not mitigate confirmation bias and, in some cases, leads to worse performance than supervised learning. In contrast, we demonstrate that AL can overcome confirmation bias in SSL in these realistic settings. Our results provide insights into the potential of combining active and semi-supervised learning in the presence of common real-world challenges, which is a promising direction for robust methods when learning with limited labeled data in real-world applications.
</details>
<details>
<summary>摘要</summary>
active learning是必要吗？ semi-supervised learning的强大方法的出现使得有限的标注数据设置中使用active learning的可用性存在各种 вопро题。这是因为结果表明将 semi-supervised learning（SSL）方法与随机选择标注结合可以超越现有的active learning（AL）技术。然而，这些结果是基于可靠的标准 benchmark dataset上进行的实验，这可能会过分估计外部适用性。然而，文献缺乏对实际数据场景中active semi-supervised learning方法的性能研究，这种知识漏洞存在。因此，我们提出了三种常见的实际数据挑战： между类异常、 Within-class异常和 между类相似。这些挑战可能会对 SSL性能产生负面影响，因为确认偏见。我们在模拟数据挑战中进行了SSL和AL实验，发现随机抽样不能消除确认偏见，有时even worse than supervised learning。然而，我们发现AL可以在这些实际设置中超越确认偏见。我们的结果为将活动和 semi-supervised learning结合使用在实际应用中的可能性提供了新的思路，这是一种robust方法在有限标注数据中学习的承诺。
</details></li>
</ul>
<hr>
<h2 id="HyperSNN-A-new-efficient-and-robust-deep-learning-model-for-resource-constrained-control-applications"><a href="#HyperSNN-A-new-efficient-and-robust-deep-learning-model-for-resource-constrained-control-applications" class="headerlink" title="HyperSNN: A new efficient and robust deep learning model for resource constrained control applications"></a>HyperSNN: A new efficient and robust deep learning model for resource constrained control applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08222">http://arxiv.org/abs/2308.08222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanglu Yan, Shida Wang, Kaiwen Tang, Weng-Fai Wong</li>
<li>for: 这篇论文旨在探讨智能家居、机器人和智能家具等领域中的边缘计算技术，尤其是使用脉冲神经网络（SNN）和超dimensional computing来进行控制任务。</li>
<li>methods: 这篇论文提出了一种名为HyperSNN的新方法，它使用8位数字加法来替代昂贵的32位浮点数 Multiplications，从而降低能源消耗，提高了韧性和可能提高精度。</li>
<li>results: 我们在AI Gym评 bench上测试了HyperSNN，结果显示HyperSNN可以与传统机器学习方法相比，仅 consume 1.36%到9.96%的能源，且在韧性方面也有提高。我们认为HyperSNN适用于互动、移动和穿戴式设备，实现能效的系统设计，并且开拓了实际应用中的实时预测控制（MPC）等复杂算法的可能性。<details>
<summary>Abstract</summary>
In light of the increasing adoption of edge computing in areas such as intelligent furniture, robotics, and smart homes, this paper introduces HyperSNN, an innovative method for control tasks that uses spiking neural networks (SNNs) in combination with hyperdimensional computing. HyperSNN substitutes expensive 32-bit floating point multiplications with 8-bit integer additions, resulting in reduced energy consumption while enhancing robustness and potentially improving accuracy. Our model was tested on AI Gym benchmarks, including Cartpole, Acrobot, MountainCar, and Lunar Lander. HyperSNN achieves control accuracies that are on par with conventional machine learning methods but with only 1.36% to 9.96% of the energy expenditure. Furthermore, our experiments showed increased robustness when using HyperSNN. We believe that HyperSNN is especially suitable for interactive, mobile, and wearable devices, promoting energy-efficient and robust system design. Furthermore, it paves the way for the practical implementation of complex algorithms like model predictive control (MPC) in real-world industrial scenarios.
</details>
<details>
<summary>摘要</summary>
在智能家居、机器人和智能家具等领域的edge computing应用日益普及，这篇论文提出了HyperSNN方法，这是一种结合神经网络和高维计算的新型控制方法。HyperSNN通过将昂贵的32位浮点 multiply替换为8位整数加法，从而降低能耗，同时提高了鲁棒性和可能提高了准确性。我们的模型在AI Gym测试启用上，包括Cartpole、Acrobot、MountainCar和Lunar Lander等标准测试集，HyperSNN实现了与传统机器学习方法相当的控制精度，但能耗只有1.36%到9.96%。此外，我们的实验还表明了HyperSNN具有更高的鲁棒性。我们认为HyperSNN特别适合交互式、移动和穿戴设备，推动能效的系统设计，同时为实际应用中的复杂算法如模型预测控制（MPC）铺平了道路。
</details></li>
</ul>
<hr>
<h2 id="In-situ-Fault-Diagnosis-of-Indium-Tin-Oxide-Electrodes-by-Processing-S-Parameter-Patterns"><a href="#In-situ-Fault-Diagnosis-of-Indium-Tin-Oxide-Electrodes-by-Processing-S-Parameter-Patterns" class="headerlink" title="In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns"></a>In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11639">http://arxiv.org/abs/2308.11639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tae Yeob Kang, Haebom Lee, Sungho Suh</li>
<li>for: 该研究旨在为光电子器件中的铝镉酸盐电极进行不 destruktive 的故障检测和诊断，以确保设备的性能和可靠性。</li>
<li>methods: 该研究使用了干扰参数（S-parameter）信号处理方法，可以早期检测、具有高精度、鲁棒性和根本原因分析。</li>
<li>results: 研究表明，可以通过将不同通道的S-parameters作为输入，使用深度学习（DL）方法同时分析报头和严重程度。此外，在增加了添加性噪声水平时， combining 不同通道的S-parameters可以明显提高诊断性能。<details>
<summary>Abstract</summary>
In the field of optoelectronics, indium tin oxide (ITO) electrodes play a crucial role in various applications, such as displays, sensors, and solar cells. Effective fault detection and diagnosis of the ITO electrodes are essential to ensure the performance and reliability of the devices. However, traditional visual inspection is challenging with transparent ITO electrodes, and existing fault detection methods have limitations in determining the root causes of the defects, often requiring destructive evaluations. In this study, an in situ fault diagnosis method is proposed using scattering parameter (S-parameter) signal processing, offering early detection, high diagnostic accuracy, noise robustness, and root cause analysis. A comprehensive S-parameter pattern database is obtained according to defect states. Deep learning (DL) approaches, including multilayer perceptron (MLP), convolutional neural network (CNN), and transformer, are then used to simultaneously analyze the cause and severity of defects. Notably, it is demonstrated that the diagnostic performance under additive noise levels can be significantly enhanced by combining different channels of the S-parameters as input to the learning algorithms, as confirmed through the t-distributed stochastic neighbor embedding (t-SNE) dimension reduction visualization.
</details>
<details>
<summary>摘要</summary>
在光电子学领域中，镍铁矿（ITO）电极扮演着重要的角色，包括显示器、感测器和太阳能电池等应用。有效检测和诊断ITO电极的缺陷是保证设备性能和可靠性的关键。然而，传统的视觉检查受到透明ITO电极的限制，现有的缺陷检测方法往往无法决定缺陷的根本原因，需要破坏性评估。本研究提出了一种实时缺陷诊断方法，使用散射参数（S-parameter）信号处理，可以早期检测、具有高精度、鲁棒性和根本原因分析。通过对缺陷状态下的S-parameter模式库的获取，使用深度学习（DL）方法，包括多层感知神经网络（MLP）、卷积神经网络（CNN）和变换器，同时分析缺陷的原因和严重程度。另外，研究表明，将不同通道的S-parameter作为输入，可以使用不同的混合方法提高诊断性能下附加噪声水平的表现。这一结论得到了通过t-分布随机邻居embedding（t-SNE）维度减少视觉化的确认。
</details></li>
</ul>
<hr>
<h2 id="Epicure-Distilling-Sequence-Model-Predictions-into-Patterns"><a href="#Epicure-Distilling-Sequence-Model-Predictions-into-Patterns" class="headerlink" title="Epicure: Distilling Sequence Model Predictions into Patterns"></a>Epicure: Distilling Sequence Model Predictions into Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08203">http://arxiv.org/abs/2308.08203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miltiadis Allamanis, Earl T. Barr</li>
<li>for: 用于生成高精度的函数名称和侦测异常函数名称</li>
<li>methods: 使用 Epicure 方法将模型预测结果转换为简单的几何模式</li>
<li>results: Epicure 方法可以对于预测函数名称和侦测异常函数名称 task 取得更高精度的结果，比起最佳模型预测结果高出 61% 以上。<details>
<summary>Abstract</summary>
Most machine learning models predict a probability distribution over concrete outputs and struggle to accurately predict names over high entropy sequence distributions. Here, we explore finding abstract, high-precision patterns intrinsic to these predictions in order to make abstract predictions that usefully capture rare sequences. In this short paper, we present Epicure, a method that distils the predictions of a sequence model, such as the output of beam search, into simple patterns. Epicure maps a model's predictions into a lattice that represents increasingly more general patterns that subsume the concrete model predictions.   On the tasks of predicting a descriptive name of a function given the source code of its body and detecting anomalous names given a function, we show that Epicure yields accurate naming patterns that match the ground truth more often compared to just the highest probability model prediction. For a false alarm rate of 10%, Epicure predicts patterns that match 61% more ground-truth names compared to the best model prediction, making Epicure well-suited for scenarios that require high precision.
</details>
<details>
<summary>摘要</summary>
大多数机器学习模型预测结果是一个概率分布，尤其是在高 entropy 序列分布时，它们很难准确预测名称。在这里，我们探索了找到Abstract高精度模式，以便使用这些模式来预测罕见序列。本文介绍了 Epicure 方法，它将序列模型预测结果映射到一个表示增加更一般模式的笛卡尔矩阵中。在函数的描述名称预测和异常名称检测任务上，我们显示了 Epicure 可以更准确地预测名称，相比于最佳模型预测。为了 false alarm rate 为 10%，Epicure 预测的模式与真实ground truth中的名称相匹配的情况比最佳模型预测多出了 61%。因此，Epicure 适用于需要高精度的场景。
</details></li>
</ul>
<hr>
<h2 id="DeSCo-Towards-Generalizable-and-Scalable-Deep-Subgraph-Counting"><a href="#DeSCo-Towards-Generalizable-and-Scalable-Deep-Subgraph-Counting" class="headerlink" title="DeSCo: Towards Generalizable and Scalable Deep Subgraph Counting"></a>DeSCo: Towards Generalizable and Scalable Deep Subgraph Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08198">http://arxiv.org/abs/2308.08198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Fu, Chiyue Wei, Yu Wang, Rex Ying<br>for: 这个研究是为了提出一个可扩展的神经网络架构，以便精确地预测查询 граф中的元素出现次数和位置。methods: 这个研究使用了一个新的专门分partition的技术，将大量目标 Graf divide 为小型邻接 Graph，以减少查询 граф中元素的 Count 的变化。然后，使用一个具有表现力的SUBGRAPH-based  hetereogeneous graph neural network 进行 Counting calculation。最后，使用learnable gates 进行 gossip propagation 以充分利用查询 Graf 中的 inductive biases。results: 这个研究在 eight 个真实世界数据集上进行了评估，与现有的神经网络方法相比，实现了137倍的mean squared error 的改善，同时保持了多项式时间复杂度。<details>
<summary>Abstract</summary>
Subgraph counting is the problem of counting the occurrences of a given query graph in a large target graph. Large-scale subgraph counting is useful in various domains, such as motif counting for social network analysis and loop counting for money laundering detection on transaction networks. Recently, to address the exponential runtime complexity of scalable subgraph counting, neural methods are proposed. However, existing neural counting approaches fall short in three aspects. Firstly, the counts of the same query can vary from zero to millions on different target graphs, posing a much larger challenge than most graph regression tasks. Secondly, current scalable graph neural networks have limited expressive power and fail to efficiently distinguish graphs in count prediction. Furthermore, existing neural approaches cannot predict the occurrence position of queries in the target graph.   Here we design DeSCo, a scalable neural deep subgraph counting pipeline, which aims to accurately predict the query count and occurrence position on any target graph after one-time training. Firstly, DeSCo uses a novel canonical partition and divides the large target graph into small neighborhood graphs. The technique greatly reduces the count variation while guaranteeing no missing or double-counting. Secondly, neighborhood counting uses an expressive subgraph-based heterogeneous graph neural network to accurately perform counting in each neighborhood. Finally, gossip propagation propagates neighborhood counts with learnable gates to harness the inductive biases of motif counts. DeSCo is evaluated on eight real-world datasets from various domains. It outperforms state-of-the-art neural methods with 137x improvement in the mean squared error of count prediction, while maintaining the polynomial runtime complexity.
</details>
<details>
<summary>摘要</summary>
大量子グラフ数えは、目标グラフ中の Given クエリー グラフの出现回数を数える问题です。大规模な子グラフ数えは、社会ネットワーク分析のモチーフ数えや、取引ネットワーク上の资金洗浄検出など、いくつかの领域で有用です。ただし、スケーラブルな子グラフ数えでは、问题の复雑さに対応するために、ニューラルな方法が提案されています。しかし、既存のニューラル カウンティング アプローチは、以下の3点で不足しています。1. 同じクエリーでは、ターゲット グラフによってカウントが0から数百万まで変化するため、大きな挑戦を提示します。2. 现在のスケーラブルなグラフニューラルネットワークは、クエリー カウントの效率的な予测をできません。3. 既存のニューラル アプローチは、ターゲット グラフ上のクエリーの出现位置を予测することができません。これらの问题を解决するために、我々はデスコ（DeSCo）というスケーラブルなニューラル ディープ サブグラフ カウンティング パイプラインを设计しました。デスコは、一度のトレーニングで任意のターゲット グラフ上のクエリー カウントと出现位置を正确に予测することができます。1. デスコでは、ターゲット グラフを小さな neighboorhood グラフに分割し、カウントのバラツキを大幅に削减します。2.  neighborhood カウンティングでは、heterogeneous graph neural network を使用して、各 neighboorhood でのカウントを正确に予测します。3. gossip propagation では、学习ゲートを使用して、适切なモチーフ カウントを导入します。デスコは、8つの実世界データセットから评価されました。その结果、状况 のあるニューラル メソッドに対して、137倍のmean squared error の改善を达成しました。また、既存のニューラル アプローチと同じように、ポリノミアルな时间コンプレックスを维持しています。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Explainable-AI-to-Analyze-Researchers’-Aspect-Based-Sentiment-about-ChatGPT"><a href="#Leveraging-Explainable-AI-to-Analyze-Researchers’-Aspect-Based-Sentiment-about-ChatGPT" class="headerlink" title="Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment about ChatGPT"></a>Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment about ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11001">http://arxiv.org/abs/2308.11001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilpa Lakhanpal, Ajay Gupta, Rajeev Agrawal</li>
<li>for: 这篇论文的目的是分析研究者对ChatGPT的看法，以便更好地理解它的使用和发展。</li>
<li>methods: 该论文提出了一种使用可解释AI来进行方面基于情感分析的方法，以便在更新的 datasets 上进行分析。</li>
<li>results: 该论文通过实践示出了这种方法可以帮助扩展现有的状态艺术，并且在 longer text data 上进行有效的方面基于情感分析。<details>
<summary>Abstract</summary>
The groundbreaking invention of ChatGPT has triggered enormous discussion among users across all fields and domains. Among celebration around its various advantages, questions have been raised with regards to its correctness and ethics of its use. Efforts are already underway towards capturing user sentiments around it. But it begs the question as to how the research community is analyzing ChatGPT with regards to various aspects of its usage. It is this sentiment of the researchers that we analyze in our work. Since Aspect-Based Sentiment Analysis has usually only been applied on a few datasets, it gives limited success and that too only on short text data. We propose a methodology that uses Explainable AI to facilitate such analysis on research data. Our technique presents valuable insights into extending the state of the art of Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not hampered by the length of the text data.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The groundbreaking invention of ChatGPT has triggered enormous discussion among users across all fields and domains. Among celebration around its various advantages, questions have been raised with regards to its correctness and ethics of its use. Efforts are already underway towards capturing user sentiments around it. But it begs the question as to how the research community is analyzing ChatGPT with regards to various aspects of its usage. It is this sentiment of the researchers that we analyze in our work. Since Aspect-Based Sentiment Analysis has usually only been applied on a few datasets, it gives limited success and that too only on short text data. We propose a methodology that uses Explainable AI to facilitate such analysis on research data. Our technique presents valuable insights into extending the state of the art of Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not hampered by the length of the text data." into Simplified Chinese.干货发明ChatGPT已经引发了各个领域和领导人的广泛的讨论。虽然欢快 celebrate its多种优点，但也提出了关于其正确性和使用道德问题的问题。尝试已经进行了捕捉用户情感的努力。但是，研究者如何分析ChatGPT在不同方面的使用仍然是一个问题。我们的工作是分析研究者对ChatGPT的感受。由于Aspect-Based Sentiment Analysis通常只能在一些数据集上进行，因此它具有有限的成功，只能处理短文本数据。我们提出了一种方法，使用可解释AI来实现这种分析在研究数据上。我们的技术可以提供对 newer datasets 的扩展state of the art的Aspect-Based Sentiment Analysis，不受文本数据的长度所限制。
</details></li>
</ul>
<hr>
<h2 id="Endogenous-Macrodynamics-in-Algorithmic-Recourse"><a href="#Endogenous-Macrodynamics-in-Algorithmic-Recourse" class="headerlink" title="Endogenous Macrodynamics in Algorithmic Recourse"></a>Endogenous Macrodynamics in Algorithmic Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08187">http://arxiv.org/abs/2308.08187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pat-alt/endogenous-macrodynamics-in-algorithmic-recourse">https://github.com/pat-alt/endogenous-macrodynamics-in-algorithmic-recourse</a></li>
<li>paper_authors: Patrick Altmeyer, Giovan Angela, Aleksander Buszydlik, Karol Dobiczek, Arie van Deursen, Cynthia C. S. Liem</li>
<li>for: 本文主要研究Counterfactual Explanations（CE）和Algorithmic Recourse（AR）在动态环境下的应用，以及这些技术在实际应用中对其他个体的影响。</li>
<li>methods: 本文使用了一种普遍的框架来描述现有的方法ologies，并证明了这些方法ologies忽略了一种隐藏的外部成本，只有在研究团队级别的幂等 dynamics 时才会表现出来。</li>
<li>results: 通过使用各种现有的counterfactual生成器和多种标准数据集，我们在实验中生成了大量的counterfactuals，并研究了这些counterfactuals对模型和领域的影响。我们发现，由于recourse的实现，可能会导致模型和领域的变化，这些变化可能会妨碍Algorithmic Recourse的应用。然而，我们提出了一些缓解这些问题的策略。我们的实验框架快速、开源，可以帮助研究人员更好地理解recourse的应用。<details>
<summary>Abstract</summary>
Existing work on Counterfactual Explanations (CE) and Algorithmic Recourse (AR) has largely focused on single individuals in a static environment: given some estimated model, the goal is to find valid counterfactuals for an individual instance that fulfill various desiderata. The ability of such counterfactuals to handle dynamics like data and model drift remains a largely unexplored research challenge. There has also been surprisingly little work on the related question of how the actual implementation of recourse by one individual may affect other individuals. Through this work, we aim to close that gap. We first show that many of the existing methodologies can be collectively described by a generalized framework. We then argue that the existing framework does not account for a hidden external cost of recourse, that only reveals itself when studying the endogenous dynamics of recourse at the group level. Through simulation experiments involving various state-of the-art counterfactual generators and several benchmark datasets, we generate large numbers of counterfactuals and study the resulting domain and model shifts. We find that the induced shifts are substantial enough to likely impede the applicability of Algorithmic Recourse in some situations. Fortunately, we find various strategies to mitigate these concerns. Our simulation framework for studying recourse dynamics is fast and opensourced.
</details>
<details>
<summary>摘要</summary>
现有的Counterfactual Explanations（CE）和Algorithmic Recourse（AR）研究主要关注单个个体在静止环境下：给定一个估计模型，目标是找到满足多种要求的有效counterfactuals。然而，这些counterfactuals对数据和模型演变的能力尚未得到了充分的研究。此外，很少有关于个体实施救济后他们对其他个体的影响的研究。我们通过这项工作，希望能够填补这一差。我们首先示出了现有的方法ologies可以总结为一个通用框架。然后，我们 argue that现有的框架不会考虑到救济实施过程中隐藏的外部成本，只有在研究救济过程的群体水平时才会发现。通过使用当今顶尖counterfactual生成器和多个标准数据集，我们生成了大量的counterfactuals，并研究其所导致的领域和模型变化。我们发现，引入的变化是足够大，可能会阻碍救济的应用。幸运的是，我们发现了多种缓解这些问题的策略。我们的救济动力学研究框架快速，开源。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Generic-Graph-Neural-Networks-via-Architecture-Compiler-Partition-Method-Co-Design"><a href="#Accelerating-Generic-Graph-Neural-Networks-via-Architecture-Compiler-Partition-Method-Co-Design" class="headerlink" title="Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design"></a>Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08174">http://arxiv.org/abs/2308.08174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuwen Lu, Zhihui Zhang, Cong Guo, Jingwen Leng, Yangjie Zhou, Minyi Guo</li>
<li>for: 这个研究旨在开发高效和高性能的图形神经网络（GNNs）硬件加速器，以实现图形学习领域中的精确性提升。</li>
<li>methods: 这个研究使用了一些新的技术来解决GNN模型的两个基本挑战：一是GNN模型的带宽需求很高，二是GNN模型的多样性。这个研究使用了一种新的分区阶段Operator整合，以减少GNN模型的带宽需求；同时，这个研究也引入分区阶段多执行绪，以便同时处理图形分 partitions，并将不同的硬件资源分配给不同的执行绪。为了降低额外的On-chip memory，这个研究还提出了细化的图形分割。</li>
<li>results: 这个研究使用了 SwitchBlade 框架，包括编译器、图形分割器和硬件加速器，实现了对 NVIDIA V100 GPU 的平均速度提升为 1.85倍，和能源减少为 19.03倍。此外，SwitchBlade 还能够与现有的特殊适配器相比。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown significant accuracy improvements in a variety of graph learning domains, sparking considerable research interest. To translate these accuracy improvements into practical applications, it is essential to develop high-performance and efficient hardware acceleration for GNN models. However, designing GNN accelerators faces two fundamental challenges: the high bandwidth requirement of GNN models and the diversity of GNN models. Previous works have addressed the first challenge by using more expensive memory interfaces to achieve higher bandwidth. For the second challenge, existing works either support specific GNN models or have generic designs with poor hardware utilization.   In this work, we tackle both challenges simultaneously. First, we identify a new type of partition-level operator fusion, which we utilize to internally reduce the high bandwidth requirement of GNNs. Next, we introduce partition-level multi-threading to schedule the concurrent processing of graph partitions, utilizing different hardware resources. To further reduce the extra on-chip memory required by multi-threading, we propose fine-grained graph partitioning to generate denser graph partitions. Importantly, these three methods make no assumptions about the targeted GNN models, addressing the challenge of model variety. We implement these methods in a framework called SwitchBlade, consisting of a compiler, a graph partitioner, and a hardware accelerator. Our evaluation demonstrates that SwitchBlade achieves an average speedup of $1.85\times$ and energy savings of $19.03\times$ compared to the NVIDIA V100 GPU. Additionally, SwitchBlade delivers performance comparable to state-of-the-art specialized accelerators.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在多种图学学问题上显示了重要的准确性改进，引起了广泛的研究兴趣。为将这些准确性改进应用于实际场景，必须开发高性能和高效的硬件加速器 для GNN 模型。然而，设计 GNN 加速器面临两个根本挑战：GNN 模型的带宽需求很高，以及 GNN 模型的多样性。previous works 通过使用更昂贵的内存接口来实现更高的带宽来解决第一个挑战。对于第二个挑战，现有的工作ether 支持特定 GNN 模型或者有通用的设计，但它们的硬件利用率很低。在这种情况下，我们同时解决了这两个挑战。首先，我们发现了一种新的分区级别的Operator融合方法，我们通过这种方法来减少 GNN 模型的带宽需求。然后，我们引入分区级别的多线程处理，以便同时处理不同的图分区，并使用不同的硬件资源。为了避免多线程处理所增加的额外的内存，我们提议使用细化的图分区来生成更密集的图分区。这三种方法不仅不假设目标 GNN 模型，而且可以同时解决多种 GNN 模型的问题。我们在 SwitchBlade 框架中实现了这三种方法， SwitchBlade 包括一个编译器、一个图分区器和一个硬件加速器。我们的评估表明，SwitchBlade 可以在 NVIDIA V100 GPU 上实现平均的速度提升为 1.85 倍，并且能够降低能耗量达 19.03 倍。此外，SwitchBlade 可以与现有的专门加速器相比，实现相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Expressivity-of-Graph-Neural-Networks-Through-the-Lens-of-Adversarial-Robustness"><a href="#Expressivity-of-Graph-Neural-Networks-Through-the-Lens-of-Adversarial-Robustness" class="headerlink" title="Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness"></a>Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08173">http://arxiv.org/abs/2308.08173</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/francesco-campi/rob-subgraphs">https://github.com/francesco-campi/rob-subgraphs</a></li>
<li>paper_authors: Francesco Campi, Lukas Gosch, Tom Wollschläger, Yan Scholten, Stephan Günnemann</li>
<li>for: 这个论文探讨了图神经网络（GNNs）的对抗 robustness，并证明GNNs比传统的消息传递神经网络（MPNNs）更有力。</li>
<li>methods: 作者使用对抗 robustness作为一种工具，探讨GNNs的表达能力的限制。他们使用对抗攻击来测试GNNs的能力 counting specific subgraph patterns，并发展了高效的对抗攻击策略。</li>
<li>results: 研究发现，更强大的GNNs在小结构变化的情况下失去泛化能力，并且无法在非标准图上计数子结构。<details>
<summary>Abstract</summary>
We perform the first adversarial robustness study into Graph Neural Networks (GNNs) that are provably more powerful than traditional Message Passing Neural Networks (MPNNs). In particular, we use adversarial robustness as a tool to uncover a significant gap between their theoretically possible and empirically achieved expressive power. To do so, we focus on the ability of GNNs to count specific subgraph patterns, which is an established measure of expressivity, and extend the concept of adversarial robustness to this task. Based on this, we develop efficient adversarial attacks for subgraph counting and show that more powerful GNNs fail to generalize even to small perturbations to the graph's structure. Expanding on this, we show that such architectures also fail to count substructures on out-of-distribution graphs.
</details>
<details>
<summary>摘要</summary>
我们进行了首个对图 neural network (GNNs) 的敏感性研究，该研究表明 GNNs 比传统的讯息传递神经网络 (MPNNs) 更具潜力。具体来说，我们使用敏感性作为一个工具，对 GNNs 的表现进行探索，并发现了它们在实际上可以表达的表现和理论上可以表达的表现之间存在很大的差距。我们针对 GNNs 的子图计数能力进行了扩展，并发现了更强大的 GNNs 对小的结构变化进行了攻击，并且还无法处理非常力分布的图。
</details></li>
</ul>
<hr>
<h2 id="AATCT-IDS-A-Benchmark-Abdominal-Adipose-Tissue-CT-Image-Dataset-for-Image-Denoising-Semantic-Segmentation-and-Radiomics-Evaluation"><a href="#AATCT-IDS-A-Benchmark-Abdominal-Adipose-Tissue-CT-Image-Dataset-for-Image-Denoising-Semantic-Segmentation-and-Radiomics-Evaluation" class="headerlink" title="AATCT-IDS: A Benchmark Abdominal Adipose Tissue CT Image Dataset for Image Denoising, Semantic Segmentation, and Radiomics Evaluation"></a>AATCT-IDS: A Benchmark Abdominal Adipose Tissue CT Image Dataset for Image Denoising, Semantic Segmentation, and Radiomics Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08172">http://arxiv.org/abs/2308.08172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyu Ma, Chen Li, Tianming Du, Le Zhang, Dechao Tang, Deguo Ma, Shanchuan Huang, Yan Liu, Yihao Sun, Zhihao Chen, Jin Yuan, Qianqing Nie, Marcin Grzegorzek, Hongzan Sun</li>
<li>For: This paper is written to introduce and validate a new benchmark dataset for abdominal adipose tissue CT images, and to explore the potential of the dataset for different tasks such as image denoising, semantic segmentation, and radiomics.* Methods: The paper uses a benchmark dataset called AATTCT-IDS, which contains 13,732 raw CT slices and has been individually annotated for subcutaneous and visceral adipose tissue regions. The authors compare and analyze the performance of various methods on the dataset for different tasks.* Results: The results show that algorithms using a smoothing strategy perform better for image denoising, while methods like BM3D preserve the original image structure better. The segmentation results of adipose tissue by different models show different structural characteristics, and BiSeNet obtains segmentation results that are only slightly inferior to U-Net with the shortest training time. The radiomics study based on AATTCT-IDS reveals three adipose distributions in the subject population.Here’s the information in Simplified Chinese text:* For: 这篇论文是为了介绍和验证一个新的 Referenced dataset for abdominal adipose tissue CT images，并 explore这个dataset的多维特征以及其在不同任务中的潜在性。* Methods: 这篇论文使用一个名为AATTCT-IDS的 Referenced dataset，该dataset包含13,732个Raw CT slice，并且每个slice都被手动标注了脂肪组织区域。作者们对不同任务使用不同方法进行比较和分析。* Results: 结果表明使用平滑策略的算法在图像压缩中表现更好，而BM3D等方法能够更好地保持原始图像结构。不同模型的 segmentation 结果表明不同的结构特征，而BiSeNet等模型能够在最短训练时间内获得与U-Net的 segmentation 结果相似的结果。基于AATTCT-IDS的 radiomics 研究发现了脂肪分布的三种类型。<details>
<summary>Abstract</summary>
Methods: In this study, a benchmark \emph{Abdominal Adipose Tissue CT Image Dataset} (AATTCT-IDS) containing 300 subjects is prepared and published. AATTCT-IDS publics 13,732 raw CT slices, and the researchers individually annotate the subcutaneous and visceral adipose tissue regions of 3,213 of those slices that have the same slice distance to validate denoising methods, train semantic segmentation models, and study radiomics. For different tasks, this paper compares and analyzes the performance of various methods on AATTCT-IDS by combining the visualization results and evaluation data. Thus, verify the research potential of this data set in the above three types of tasks.   Results: In the comparative study of image denoising, algorithms using a smoothing strategy suppress mixed noise at the expense of image details and obtain better evaluation data. Methods such as BM3D preserve the original image structure better, although the evaluation data are slightly lower. The results show significant differences among them. In the comparative study of semantic segmentation of abdominal adipose tissue, the segmentation results of adipose tissue by each model show different structural characteristics. Among them, BiSeNet obtains segmentation results only slightly inferior to U-Net with the shortest training time and effectively separates small and isolated adipose tissue. In addition, the radiomics study based on AATTCT-IDS reveals three adipose distributions in the subject population.   Conclusion: AATTCT-IDS contains the ground truth of adipose tissue regions in abdominal CT slices. This open-source dataset can attract researchers to explore the multi-dimensional characteristics of abdominal adipose tissue and thus help physicians and patients in clinical practice. AATCT-IDS is freely published for non-commercial purpose at: \url{https://figshare.com/articles/dataset/AATTCT-IDS/23807256}.
</details>
<details>
<summary>摘要</summary>
方法：本研究使用了一个名为“腹部脂肪组织CT影像数据集”（AATTCT-IDS）的标准数据集，该数据集包含300名参与者，并公布了13,732个RAW CT slice的原始图像。研究人员ividually annotated 3,213个slice的脂肪组织区域，以验证去噪方法、训练semantic segmentation模型和研究 радиологи学。通过对不同任务的组合可视化结果和评估数据进行比较和分析，这个数据集的研究潜力得到了验证。结果：在图像去噪比较研究中，使用滤波策略的算法可以更好地降低杂噪，但是同时也会导致图像细节的产生。比如BM3D算法可以更好地保持原始图像结构，但评估数据略有下降。结果表明不同算法之间存在显著的差异。在脂肪组织分 segmentation研究中，BiSeNet模型可以在短时间内 obtian segmentation结果，并且可以有效地分离小型和隔离的脂肪组织。此外，基于AATTCT-IDS的 радиологи学研究发现了腹部脂肪组织中主要存在三种分布。结论：AATTCT-IDS包含了腹部CT影像中脂肪组织区域的真实图像。这个开源数据集可以吸引研究人员来探索腹部脂肪组织的多维特征，并帮助临床医生和病人。AATCT-IDS采用非商业用途发布，可以免费获取：https://figshare.com/articles/dataset/AATTCT-IDS/23807256。
</details></li>
</ul>
<hr>
<h2 id="A-Quantum-Approximation-Scheme-for-k-Means"><a href="#A-Quantum-Approximation-Scheme-for-k-Means" class="headerlink" title="A Quantum Approximation Scheme for k-Means"></a>A Quantum Approximation Scheme for k-Means</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08167">http://arxiv.org/abs/2308.08167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ragesh Jaiswal</li>
<li>for: 这个论文的目的是解决经典k-means clustering问题，提供一种量子近似方案，可以在QRAM模型中实现，并且running time只具有 polynomial 幂级度的依赖关系。</li>
<li>methods: 这个量子算法使用了一种$(1+\varepsilon)$-近似方法，对于每个$\varepsilon&gt;0$，可以在QRAM数据结构上进行实现，并且running time为$\tilde{O}\left(2^{\tilde{O}\left(\frac{k}{\varepsilon}\right)} \eta^2 d\right)$。</li>
<li>results: 这个量子算法可以在高probability下输出一组$k$个中心点，使得$cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$，其中$C_{OPT}$是最优的$k$-中心点，$cost(.)$是标准的$k$-means成本函数（即点到最近中心点的平方距离之和），$\eta$是几何比（即最大距离到最小距离的比）。这是第一个具有polylogarithmic running time的量子算法，可以提供$(1+\varepsilon)$的证明近似保证。<details>
<summary>Abstract</summary>
We give a quantum approximation scheme (i.e., $(1 + \varepsilon)$-approximation for every $\varepsilon > 0$) for the classical $k$-means clustering problem in the QRAM model with a running time that has only polylogarithmic dependence on the number of data points. More specifically, given a dataset $V$ with $N$ points in $\mathbb{R}^d$ stored in QRAM data structure, our quantum algorithm runs in time $\tilde{O} \left( 2^{\tilde{O}(\frac{k}{\varepsilon})} \eta^2 d\right)$ and with high probability outputs a set $C$ of $k$ centers such that $cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$. Here $C_{OPT}$ denotes the optimal $k$-centers, $cost(.)$ denotes the standard $k$-means cost function (i.e., the sum of the squared distance of points to the closest center), and $\eta$ is the aspect ratio (i.e., the ratio of maximum distance to minimum distance). This is the first quantum algorithm with a polylogarithmic running time that gives a provable approximation guarantee of $(1+\varepsilon)$ for the $k$-means problem. Also, unlike previous works on unsupervised learning, our quantum algorithm does not require quantum linear algebra subroutines and has a running time independent of parameters (e.g., condition number) that appear in such procedures.
</details>
<details>
<summary>摘要</summary>
我们提供了一种量子近似方案（即$(1 + \varepsilon)$-近似方案）来解决 классиical $k$-means归一化问题在QRAM模型中，并且running时间具有只带有多项式幂ilogarithmic（polylogarithmic）依赖于数据点的数量。更具体地说，给定一个数据集$V$包含$N$个点在$\mathbb{R}^d$中，我们的量子算法在时间 $\tilde{O} \left( 2^{\tilde{O}(\frac{k}{\varepsilon})} \eta^2 d\right)$ 内运行，并且 WITH HIGH PROBABILITY输出一组 $C$ 的 $k$ 中心，使得 $cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$，其中 $C_{OPT}$ 表示最优的 $k$-中心，$cost(.)$ 表示标准 $k$-means 成本函数（即点到最近中心的平方距离的总和），而 $\eta$ 是最大距离到最小距离的比率（即 aspect ratio）。这是第一个具有 polylogarithmic 运行时间的量子算法，并且不需要量子线性代数子routines，运行时间与参数（例如 condition number）无关。Note:* "QRAM" stands for "Quantum Random Access Memory", which is a quantum analogue of classical random access memory.* "polylogarithmic" means the running time has a polynomial logarithmic dependence on the number of data points.* "condition number" refers to the ratio of the maximum distance to the minimum distance in the dataset.
</details></li>
</ul>
<hr>
<h2 id="PEvoLM-Protein-Sequence-Evolutionary-Information-Language-Model"><a href="#PEvoLM-Protein-Sequence-Evolutionary-Information-Language-Model" class="headerlink" title="PEvoLM: Protein Sequence Evolutionary Information Language Model"></a>PEvoLM: Protein Sequence Evolutionary Information Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08578">http://arxiv.org/abs/2308.08578</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/issararab/pevolm">https://github.com/issararab/pevolm</a></li>
<li>paper_authors: Issar Arab</li>
<li>for: 本研究旨在提高protein序列数据库的搜索效率和质量，以及提高计算生物学和生物信息学中ML模型的性能。</li>
<li>methods: 本研究使用了一种基于自然语言处理的语言模型（ELMo），将蛋白质序列转换为数字Vector表示。研究还使用了PSSM的概念和传输学习，开发了一种新的双向语言模型（bi-LM），其中一个路径用于前向传输，另一个路径用于反向传输。</li>
<li>results: 研究发现，使用bi-LM可以在预测下一个氨基酸时同时学习蛋白质序列的演化信息，并且bi-LM的参数数量比原始ELMo少了四倍。同时，bi-LM在预测下一个氨基酸和PSSM中的概率分布方面也达到了比较高的性能。<details>
<summary>Abstract</summary>
With the exponential increase of the protein sequence databases over time, multiple-sequence alignment (MSA) methods, like PSI-BLAST, perform exhaustive and time-consuming database search to retrieve evolutionary information. The resulting position-specific scoring matrices (PSSMs) of such search engines represent a crucial input to many machine learning (ML) models in the field of bioinformatics and computational biology. A protein sequence is a collection of contiguous tokens or characters called amino acids (AAs). The analogy to natural language allowed us to exploit the recent advancements in the field of Natural Language Processing (NLP) and therefore transfer NLP state-of-the-art algorithms to bioinformatics. This research presents an Embedding Language Model (ELMo), converting a protein sequence to a numerical vector representation. While the original ELMo trained a 2-layer bidirectional Long Short-Term Memory (LSTMs) network following a two-path architecture, one for the forward and the second for the backward pass, by merging the idea of PSSMs with the concept of transfer-learning, this work introduces a novel bidirectional language model (bi-LM) with four times less free parameters and using rather a single path for both passes. The model was trained not only on predicting the next AA but also on the probability distribution of the next AA derived from similar, yet different sequences as summarized in a PSSM, simultaneously for multi-task learning, hence learning evolutionary information of protein sequences as well. The network architecture and the pre-trained model are made available as open source under the permissive MIT license on GitHub at https://github.com/issararab/PEvoLM.
</details>
<details>
<summary>摘要</summary>
随着蛋白序列数据库的不断增长，多重序列对 align (MSA) 方法，如 PSI-BLAST，在时间上进行极其耗时的数据库搜索，以获取演化信息。 resulting position-specific scoring matrices (PSSMs) 的搜索引擎表示了生物信息学和计算生物学领域中机器学习 (ML) 模型的关键输入。一个蛋白序列是一个连续的 tokens 或字符串，叫做氨基酸 (AA)。由于蛋白序列与自然语言之间的相似性，我们可以利用自然语言处理领域的最新进展，并将其转移到生物信息学中。本研究提出了 Embedding Language Model (ELMo)，将蛋白序列转换为数字向量表示。而原始 ELMo 使用了两层拟合长短时间记忆 (LSTMs) 网络，一个是向前的一个，另一个是向后的一个，通过将 PSSMs 的想法与传输学习的概念结合起来，这个工作提出了一种新的双向语言模型 (bi-LM)，具有四倍少的自由参数，使用单一路径来进行两个方向的传输。这个模型不仅在预测下一个 AA 上，还在 PSSM 中的概率分布上进行学习，同时进行多任务学习，因此学习蛋白序列的演化信息。模型的网络架构和预训练模型都在 GitHub 上公开，可以在 <https://github.com/issararab/PEvoLM> 获取。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Controlled-Averaging-for-Federated-Learning-with-Communication-Compression"><a href="#Stochastic-Controlled-Averaging-for-Federated-Learning-with-Communication-Compression" class="headerlink" title="Stochastic Controlled Averaging for Federated Learning with Communication Compression"></a>Stochastic Controlled Averaging for Federated Learning with Communication Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08165">http://arxiv.org/abs/2308.08165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinmeng Huang, Ping Li, Xiaoyun Li</li>
<li>for: 降低 Federated Learning（FL）的通信负担，提高FL的效率和可扩展性。</li>
<li>methods: 提议一种更有效率和简化的Stochastic Controlled Averaging方法，并基于该方法提出两种压缩FL算法：SCALLION和SCAFCOM。这两种算法可以支持不偏向和偏向压缩，并且可以适应任意数据不同性和不偏向压缩。</li>
<li>results: 对比 existed 压缩FL算法，SCALLION和SCAFCOM可以减少通信和计算复杂度，并且可以与相应的全精度FL方法匹配或超越其性能。实验结果表明，SCALLION和SCAFCOM可以在相同的通信预算下提高FL的性能。<details>
<summary>Abstract</summary>
Communication compression, a technique aiming to reduce the information volume to be transmitted over the air, has gained great interests in Federated Learning (FL) for the potential of alleviating its communication overhead. However, communication compression brings forth new challenges in FL due to the interplay of compression-incurred information distortion and inherent characteristics of FL such as partial participation and data heterogeneity. Despite the recent development, the performance of compressed FL approaches has not been fully exploited. The existing approaches either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression.   In this paper, we revisit the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs. Building upon this implementation, we propose two compressed FL algorithms, SCALLION and SCAFCOM, to support unbiased and biased compression, respectively. Both the proposed methods outperform the existing compressed FL methods in terms of communication and computation complexities. Moreover, SCALLION and SCAFCOM accommodates arbitrary data heterogeneity and do not make any additional assumptions on compression errors. Experiments show that SCALLION and SCAFCOM can match the performance of corresponding full-precision FL approaches with substantially reduced uplink communication, and outperform recent compressed FL methods under the same communication budget.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用压缩通信技术可以减少在空中传输的信息量，这已经在联合学习（Federated Learning，FL）中受到了广泛关注，因为它可以减轻FL的通信开销。然而，压缩通信会在FL中带来新的挑战，这是因为压缩通信会导致信息损害，并且FL的特点，如部分参与和数据不同化，会导致压缩通信的效果不可预测。虽然最近有些研究已经进行了，但是现有的方法并不能充分发挥性能。在这篇论文中，我们重新评估了一种基于渐进控制的概率平均法，并提出了一种更高效/简单的表述方式，可以减少上行通信成本的一半。基于这种实现方式，我们提出了两种压缩FL算法，即SCALLION和SCAFCOM，可以支持不偏和偏压缩。两种算法都能够超过现有的压缩FL方法，并且可以适应任意的数据不同化，不需要任何额外的压缩错误假设。实验表明，SCALLION和SCAFCOM可以与相应的全精度FL方法匹配性能，并且在同样的通信预算下表现更好。Note: Simplified Chinese is a simplified version of Chinese that is used in mainland China and Singapore. It is different from Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Characteristics-of-networks-generated-by-kernel-growing-neural-gas"><a href="#Characteristics-of-networks-generated-by-kernel-growing-neural-gas" class="headerlink" title="Characteristics of networks generated by kernel growing neural gas"></a>Characteristics of networks generated by kernel growing neural gas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08163">http://arxiv.org/abs/2308.08163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kazuhisafujita/kernelgng">https://github.com/kazuhisafujita/kernelgng</a></li>
<li>paper_authors: Kazuhisa Fujita</li>
<li>for: 本研究旨在开发 kernel GNG，即基kernels的生长神经网络算法，并investigate kernel GNG生成的网络特性。</li>
<li>methods: 本研究使用了五种kernel，包括 Gaussian、Laplacian、Cauchy、 inverse multiquadric 和 log kernels，以mappingdataset到特征空间。</li>
<li>results: 研究发现，kernel GNG可以生成具有高度稠密度和高度连接度的网络，并且可以准确地捕捉dataset中的特征。<details>
<summary>Abstract</summary>
This research aims to develop kernel GNG, a kernelized version of the growing neural gas (GNG) algorithm, and to investigate the features of the networks generated by the kernel GNG. The GNG is an unsupervised artificial neural network that can transform a dataset into an undirected graph, thereby extracting the features of the dataset as a graph. The GNG is widely used in vector quantization, clustering, and 3D graphics. Kernel methods are often used to map a dataset to feature space, with support vector machines being the most prominent application. This paper introduces the kernel GNG approach and explores the characteristics of the networks generated by kernel GNG. Five kernels, including Gaussian, Laplacian, Cauchy, inverse multiquadric, and log kernels, are used in this study.
</details>
<details>
<summary>摘要</summary>
这个研究的目标是开发kernel GNG，即kernelized版本的增长神经气体（GNG）算法，并研究由kernel GNG生成的网络特征。GNG是一种无监督的人工神经网络，可以将数据集转换成无向图，从而提取数据集中的特征作为图。GNG广泛应用于 вектор化Quantization、归一化和3D图形。 kernel方法通常用于将数据集映射到特征空间，支持向量机器学习是最广泛应用的例子。这篇文章介绍了kernel GNG方法，并探索由kernel GNG生成的网络特征。本研究使用的五种kernels包括Gaussian、Laplacian、Cauchy、 inverse multiquadric和log kernel。
</details></li>
</ul>
<hr>
<h2 id="Interpretability-Benchmark-for-Evaluating-Spatial-Misalignment-of-Prototypical-Parts-Explanations"><a href="#Interpretability-Benchmark-for-Evaluating-Spatial-Misalignment-of-Prototypical-Parts-Explanations" class="headerlink" title="Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations"></a>Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08162">http://arxiv.org/abs/2308.08162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Sacha, Bartosz Jura, Dawid Rymarczyk, Łukasz Struski, Jacek Tabor, Bartosz Zieliński</li>
<li>for: 提高parts-based网络的自我解释性</li>
<li>methods: 引入一个特有的解释性指标集和修复方法来纠正偏移</li>
<li>results: 通过实验研究，表明提出的指标集和修复方法能够有效纠正偏移，提高parts-based网络的解释性<details>
<summary>Abstract</summary>
Prototypical parts-based networks are becoming increasingly popular due to their faithful self-explanations. However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations. We name this undesired behavior a spatial explanation misalignment and introduce an interpretability benchmark with a set of dedicated metrics for quantifying this phenomenon. In addition, we propose a method for misalignment compensation and apply it to existing state-of-the-art models. We show the expressiveness of our benchmark and the effectiveness of the proposed compensation methodology through extensive empirical studies.
</details>
<details>
<summary>摘要</summary>
弹性部件网络在现代计算机视觉识别领域中日益受欢迎，主要是因为它们的自我解释能力很强。然而，它们的相似度图在次末层网络层中计算，因此prototype activation区域的接收场景经常受到外部像区域的影响，这可能会导致误leading interpretations。我们称这为 espacial explanation misalignment，并提出了一个dedicated metrics集以量化这种现象。此外，我们也提出了一种修正方法，并将其应用到现有的state-of-the-art模型中。我们透过广泛的实验研究表明了我们的benchmark的表达能力和我们的修正方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Adversarial-Robustness-of-Compressed-Deep-Learning-Models"><a href="#Benchmarking-Adversarial-Robustness-of-Compressed-Deep-Learning-Models" class="headerlink" title="Benchmarking Adversarial Robustness of Compressed Deep Learning Models"></a>Benchmarking Adversarial Robustness of Compressed Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08160">http://arxiv.org/abs/2308.08160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brijesh Vora, Kartik Patwari, Syed Mahbub Hafiz, Zubair Shafiq, Chen-Nee Chuah</li>
<li>for: 本研究旨在探讨基础模型对各种攻击Input的抗性，以便更好地理解压缩模型对抗性的影响。</li>
<li>methods: 我们开发了一个多样化的攻击 benchmark，用于测试不同的攻击方法和常见深度神经网络模型。我们采用了优化的压缩策略，以保持准确性和性能。</li>
<li>results: 我们发现，压缩后的模型仍然保持了对攻击的抗性，而且拥有更好的泛化性、更高的性能和更快的执行速度。这表明，压缩模型不会对抗性造成负面影响。<details>
<summary>Abstract</summary>
The increasing size of Deep Neural Networks (DNNs) poses a pressing need for model compression, particularly when employed on resource constrained devices. Concurrently, the susceptibility of DNNs to adversarial attacks presents another significant hurdle. Despite substantial research on both model compression and adversarial robustness, their joint examination remains underexplored. Our study bridges this gap, seeking to understand the effect of adversarial inputs crafted for base models on their pruned versions. To examine this relationship, we have developed a comprehensive benchmark across diverse adversarial attacks and popular DNN models. We uniquely focus on models not previously exposed to adversarial training and apply pruning schemes optimized for accuracy and performance. Our findings reveal that while the benefits of pruning enhanced generalizability, compression, and faster inference times are preserved, adversarial robustness remains comparable to the base model. This suggests that model compression while offering its unique advantages, does not undermine adversarial robustness.
</details>
<details>
<summary>摘要</summary>
随着深度神经网络（DNN）的尺度不断增大，需要进行模型压缩，特别是在资源有限的设备上使用。同时，DNN受到攻击者的攻击也成为一个重要的障碍。虽然关于模型压缩和攻击鲁棒性的研究已经进行了大量的工作，但是它们之间的联系还没有得到充分探讨。我们的研究尝试填补这个空白，探索攻击基本模型的输入对其压缩版本的影响。为此，我们开发了一个包括多种攻击和各种流行的DNN模型的完整的benchmark。我们独特地将注意力集中在没有接受过攻击训练的模型上，并应用优化的减少方案以保持准确性和性能。我们的发现表明，即使使用压缩，模型的总体鲁棒性仍然保持不变，这表明模型压缩不会对鲁棒性产生负面影响。这表明，模型压缩可以提供独特的优势，而不会对鲁棒性产生负面影响。
</details></li>
</ul>
<hr>
<h2 id="Deep-Generative-Imputation-Model-for-Missing-Not-At-Random-Data"><a href="#Deep-Generative-Imputation-Model-for-Missing-Not-At-Random-Data" class="headerlink" title="Deep Generative Imputation Model for Missing Not At Random Data"></a>Deep Generative Imputation Model for Missing Not At Random Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08158">http://arxiv.org/abs/2308.08158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialei Chen, Yuanbo Xu, Pengyang Wang, Yongjian Yang</li>
<li>for: 强调处理真实世界中存在缺失的数据，而不是模拟MCAR的缺失机制。</li>
<li>methods: 提出了一种基于joint probability decomposition的generative模型特有的方法，并在latent空间中处理真实世界中的缺失机制。</li>
<li>results: 对比state-of-the-art基elines，GNR模型在RMSE指标上平均提高9.9%至18.8%，并且总是在mask重建精度方面得到更好的结果。<details>
<summary>Abstract</summary>
Data analysis usually suffers from the Missing Not At Random (MNAR) problem, where the cause of the value missing is not fully observed. Compared to the naive Missing Completely At Random (MCAR) problem, it is more in line with the realistic scenario whereas more complex and challenging. Existing statistical methods model the MNAR mechanism by different decomposition of the joint distribution of the complete data and the missing mask. But we empirically find that directly incorporating these statistical methods into deep generative models is sub-optimal. Specifically, it would neglect the confidence of the reconstructed mask during the MNAR imputation process, which leads to insufficient information extraction and less-guaranteed imputation quality. In this paper, we revisit the MNAR problem from a novel perspective that the complete data and missing mask are two modalities of incomplete data on an equal footing. Along with this line, we put forward a generative-model-specific joint probability decomposition method, conjunction model, to represent the distributions of two modalities in parallel and extract sufficient information from both complete data and missing mask. Taking a step further, we exploit a deep generative imputation model, namely GNR, to process the real-world missing mechanism in the latent space and concurrently impute the incomplete data and reconstruct the missing mask. The experimental results show that our GNR surpasses state-of-the-art MNAR baselines with significant margins (averagely improved from 9.9% to 18.8% in RMSE) and always gives a better mask reconstruction accuracy which makes the imputation more principle.
</details>
<details>
<summary>摘要</summary>
通常情况下，数据分析会面临缺失不够权（MNAR）问题，其中数据缺失的原因不能全面观察。与完全随机缺失（MCAR）问题相比，MNAR问题更加真实和复杂。现有的统计方法模型了MNAR机制的各种分解方法，但我们发现直接将这些统计方法 integrate into深度生成模型是不优化的。具体来说，这会忽略恢复 маска的信任度 during MNAR 恢复过程，导致信息抽取不充分和缺失补做质量不够保障。在这篇论文中，我们从一种新的视角重新审视了MNAR问题，即完整数据和缺失数据是两种不同的杂态数据模式。遵循这种思路，我们提出了一种生成模型特有的联合概率分解方法，即并联模型，用于同时表征两种模式的分布。这种方法可以从两种模式中提取足够的信息，并且可以在缺失数据和恢复 маска之间进行共同补做。进一步地，我们利用深度生成补做模型，即GNR，来处理实际世界中的缺失机制，并同时补做缺失数据和恢复 маска。实验结果显示，我们的GNR在MNAR基线上显著提高了性能（平均提高9.9%到18.8%），并且总是提供更好的恢复率，这使得补做更符合原理。
</details></li>
</ul>
<hr>
<h2 id="Sarcasm-Detection-in-a-Disaster-Context"><a href="#Sarcasm-Detection-in-a-Disaster-Context" class="headerlink" title="Sarcasm Detection in a Disaster Context"></a>Sarcasm Detection in a Disaster Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08156">http://arxiv.org/abs/2308.08156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiberiu Sosea, Junyi Jessy Li, Cornelia Caragea</li>
<li>for: 这个论文是为了研究自然灾害发生时人们在社交媒体平台上使用的讲话方式，以及如何使用先进的自然语言处理技术来理解这种讲话方式。</li>
<li>methods: 该论文使用了一个名为HurricaneSARC的数据集，该数据集包含15,000条涉及讲话的报道，并使用了预训练的语言模型进行讲话检测。</li>
<li>results: 该论文的最佳模型在HurricaneSARC数据集上取得了0.70的F1分，并且通过中间任务转移学习可以提高模型的性能。<details>
<summary>Abstract</summary>
During natural disasters, people often use social media platforms such as Twitter to ask for help, to provide information about the disaster situation, or to express contempt about the unfolding event or public policies and guidelines. This contempt is in some cases expressed as sarcasm or irony. Understanding this form of speech in a disaster-centric context is essential to improving natural language understanding of disaster-related tweets. In this paper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for intended sarcasm, and provide a comprehensive investigation of sarcasm detection using pre-trained language models. Our best model is able to obtain as much as 0.70 F1 on our dataset. We also demonstrate that the performance on HurricaneSARC can be improved by leveraging intermediate task transfer learning. We release our data and code at https://github.com/tsosea2/HurricaneSarc.
</details>
<details>
<summary>摘要</summary>
在自然灾害事件中，人们常利用社交媒体平台如推特，请求帮助、提供灾害情况信息或表达对事件或公共政策的负面 sentiment。这种语言形式在灾害Context中是非常重要的，以提高自然语言理解灾害相关的推文。在这篇论文中，我们介绍了风暴SARC数据集，包含15000条推文，并进行了 pré-trained语言模型的全面研究。我们的最佳模型在我们的数据集上可以获得0.70的F1分。我们还证明了在HurricaneSARC上进行中间任务传承学习可以提高性能。我们将数据和代码发布在https://github.com/tsosea2/HurricaneSarc上。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Topological-Ordering-with-Conditional-Independence-Test-for-Limited-Time-Series"><a href="#Hierarchical-Topological-Ordering-with-Conditional-Independence-Test-for-Limited-Time-Series" class="headerlink" title="Hierarchical Topological Ordering with Conditional Independence Test for Limited Time Series"></a>Hierarchical Topological Ordering with Conditional Independence Test for Limited Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08148">http://arxiv.org/abs/2308.08148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anpeng Wu, Haoxuan Li, Kun Kuang, Keli Zhang, Fei Wu</li>
<li>For: This paper aims to improve the process of learning directed acyclic graphs (DAGs) to identify causal relations in observational data.* Methods: The proposed method, called HT-CIT, incorporates limited time series data and conditional instrumental variables to identify descendant nodes for each variable. The algorithm uses a hierarchical topological ordering approach with a conditional independence test to efficiently learn sparse DAGs with a smaller search space.* Results: The proposed HT-CIT algorithm is shown to be superior to other popular approaches through empirical results from synthetic and real-world datasets, with a significant reduction in the number of edges that need to be pruned.<details>
<summary>Abstract</summary>
Learning directed acyclic graphs (DAGs) to identify causal relations underlying observational data is crucial but also poses significant challenges. Recently, topology-based methods have emerged as a two-step approach to discovering DAGs by first learning the topological ordering of variables and then eliminating redundant edges, while ensuring that the graph remains acyclic. However, one limitation is that these methods would generate numerous spurious edges that require subsequent pruning. To overcome this limitation, in this paper, we propose an improvement to topology-based methods by introducing limited time series data, consisting of only two cross-sectional records that need not be adjacent in time and are subject to flexible timing. By incorporating conditional instrumental variables as exogenous interventions, we aim to identify descendant nodes for each variable. Following this line, we propose a hierarchical topological ordering algorithm with conditional independence test (HT-CIT), which enables the efficient learning of sparse DAGs with a smaller search space compared to other popular approaches. The HT-CIT algorithm greatly reduces the number of edges that need to be pruned. Empirical results from synthetic and real-world datasets demonstrate the superiority of the proposed HT-CIT algorithm.
</details>
<details>
<summary>摘要</summary>
To overcome this limitation, this paper proposes an improvement to topology-based methods by incorporating limited time series data, consisting of only two cross-sectional records that do not need to be adjacent in time and are subject to flexible timing. By using conditional instrumental variables as exogenous interventions, we aim to identify descendant nodes for each variable.We propose a hierarchical topological ordering algorithm with conditional independence tests (HT-CIT), which enables the efficient learning of sparse DAGs with a smaller search space compared to other popular approaches. The HT-CIT algorithm greatly reduces the number of edges that need to be pruned. Empirical results from synthetic and real-world datasets demonstrate the superiority of the proposed HT-CIT algorithm.
</details></li>
</ul>
<hr>
<h2 id="Online-Control-for-Linear-Dynamics-A-Data-Driven-Approach"><a href="#Online-Control-for-Linear-Dynamics-A-Data-Driven-Approach" class="headerlink" title="Online Control for Linear Dynamics: A Data-Driven Approach"></a>Online Control for Linear Dynamics: A Data-Driven Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08138">http://arxiv.org/abs/2308.08138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zishun Liu, Yongxin Chen</li>
<li>for: 本文考虑了一个在线控制问题，其中系统动态不知道，干扰 bounded，并且存在敌对成本。</li>
<li>methods: 我们提出了一种数据驱动策略，以减少控制器的违和。与模型基于方法不同，我们的算法不需要identify系统模型，而是使用一个干净的轨迹来计算干扰的积累，并使用我们设计的积累干扰控制器来做决策，其参数通过在线梯度下降更新。</li>
<li>results: 我们证明了我们的算法的违和是$\mathcal{O}(\sqrt{T})$，这意味着其性能与模型基于方法相当。<details>
<summary>Abstract</summary>
This paper considers an online control problem over a linear time-invariant system with unknown dynamics, bounded disturbance, and adversarial cost. We propose a data-driven strategy to reduce the regret of the controller. Unlike model-based methods, our algorithm does not identify the system model, instead, it leverages a single noise-free trajectory to calculate the accumulation of disturbance and makes decisions using the accumulated disturbance action controller we design, whose parameters are updated by online gradient descent. We prove that the regret of our algorithm is $\mathcal{O}(\sqrt{T})$ under mild assumptions, suggesting that its performance is on par with model-based methods.
</details>
<details>
<summary>摘要</summary>
这篇论文研究了一个在线控制问题，其中系统为线性时间不变的系统，动力不确定、干扰bounded和敌意成本存在。我们提出了一种数据驱动策略，以减少控制器的后悔。不同于模型基于方法，我们的算法不需要确定系统模型，而是利用干扰自由的一个轨迹来计算干扰的积累，并使用我们设计的积累干扰控制器来做决策，该控制器的参数通过在线梯度下降更新。我们证明了我们的算法的后悔是 $\mathcal{O}(\sqrt{T})$ 的，这表明它的性能与模型基于方法相当。
</details></li>
</ul>
<hr>
<h2 id="Microstructure-Empowered-Stock-Factor-Extraction-and-Utilization"><a href="#Microstructure-Empowered-Stock-Factor-Extraction-and-Utilization" class="headerlink" title="Microstructure-Empowered Stock Factor Extraction and Utilization"></a>Microstructure-Empowered Stock Factor Extraction and Utilization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08135">http://arxiv.org/abs/2308.08135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianfeng Jiao, Zizhong Li, Chang Xu, Yang Liu, Weiqing Liu, Jiang Bian</li>
<li>for: 高频量股票投资是股票投资中的一个关键方面，order flow数据在这方面具有关键性，因为它提供了最详细的信息，包括全部的订单书和交易记录。</li>
<li>methods: 我们提出了一种新的框架，用于从order flow数据中提取有用的因素，并且可以在不同的粒度和enario下进行多种下游任务。我们的方法包括Context Encoder和Factor Extractor。Context Encoder使得当前订单流数据段的上下文得到嵌入，包括预期和实际市场状态。Factor Extractor使用不supervised学习方法，从订单流数据中选择最重要的信号，这些信号与大多数信号进行分割。</li>
<li>results: 我们的提出的框架可以高效处理一年的股票订单流数据，并且可以在不同的enario下应用。我们的方法可以提取更好的因素，从而提高股票趋势预测和订单执行任务的精度。<details>
<summary>Abstract</summary>
High-frequency quantitative investment is a crucial aspect of stock investment. Notably, order flow data plays a critical role as it provides the most detailed level of information among high-frequency trading data, including comprehensive data from the order book and transaction records at the tick level. The order flow data is extremely valuable for market analysis as it equips traders with essential insights for making informed decisions. However, extracting and effectively utilizing order flow data present challenges due to the large volume of data involved and the limitations of traditional factor mining techniques, which are primarily designed for coarser-level stock data. To address these challenges, we propose a novel framework that aims to effectively extract essential factors from order flow data for diverse downstream tasks across different granularities and scenarios. Our method consists of a Context Encoder and an Factor Extractor. The Context Encoder learns an embedding for the current order flow data segment's context by considering both the expected and actual market state. In addition, the Factor Extractor uses unsupervised learning methods to select such important signals that are most distinct from the majority within the given context. The extracted factors are then utilized for downstream tasks. In empirical studies, our proposed framework efficiently handles an entire year of stock order flow data across diverse scenarios, offering a broader range of applications compared to existing tick-level approaches that are limited to only a few days of stock data. We demonstrate that our method extracts superior factors from order flow data, enabling significant improvement for stock trend prediction and order execution tasks at the second and minute level.
</details>
<details>
<summary>摘要</summary>
高频量质投资是股票投资中一个关键方面。突出重要的是，订单流数据在高频投资中扮演了关键角色，因为它提供了最详细的信息，包括订单书和交易记录的细节。订单流数据对市场分析非常有价值，因为它为投资者提供了关键的信息，帮助他们做出了 Informed Decisions。然而，提取并有效利用订单流数据具有挑战，因为涉及的数据量很大，而且传统的因子挖掘技术主要针对粗细级股票数据。为解决这些挑战，我们提出了一种新的框架，旨在有效地从订单流数据中提取关键因子，用于不同的下游任务和不同的场景。我们的方法包括上下文编码器和因子挖掘器。上下文编码器通过考虑当前订单流数据段的预期和实际市场状况，学习订单流数据段的上下文嵌入。此外，因子挖掘器使用无监督学习方法，选择订单流数据中最为特异的信号，以便在给定的上下文中提取关键因子。提取的因子后续用于下游任务。我们的方法可以有效处理一年的股票订单流数据，在多种场景下提供更广泛的应用场景，与现有的tick级 approached有限制，只能处理几天的股票数据。我们的方法提取了订单流数据中的优秀因子，使得股票趋势预测和订单执行任务在秒和分级别得到了显著改进。
</details></li>
</ul>
<hr>
<h2 id="Is-Self-Supervised-Pretraining-Good-for-Extrapolation-in-Molecular-Property-Prediction"><a href="#Is-Self-Supervised-Pretraining-Good-for-Extrapolation-in-Molecular-Property-Prediction" class="headerlink" title="Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?"></a>Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08129">http://arxiv.org/abs/2308.08129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shun Takashige, Masatoshi Hanai, Toyotaro Suzumura, Limin Wang, Kenjiro Taura</li>
<li>for: 本研究旨在探讨如何使用自动学习技术提高材料性能预测的准确性，特别是在材料性能预测中的推断问题上。</li>
<li>methods: 本研究使用了自动学习模型，首先通过自然语言处理方法对无标签数据进行自我预训练，然后对标签数据进行目标任务训练。</li>
<li>results: 研究发现，通过自我预训练，模型可以更好地捕捉材料的相对性质趋势，从而提高推断性能。<details>
<summary>Abstract</summary>
The prediction of material properties plays a crucial role in the development and discovery of materials in diverse applications, such as batteries, semiconductors, catalysts, and pharmaceuticals. Recently, there has been a growing interest in employing data-driven approaches by using machine learning technologies, in combination with conventional theoretical calculations. In material science, the prediction of unobserved values, commonly referred to as extrapolation, is particularly critical for property prediction as it enables researchers to gain insight into materials beyond the limits of available data. However, even with the recent advancements in powerful machine learning models, accurate extrapolation is still widely recognized as a significantly challenging problem. On the other hand, self-supervised pretraining is a machine learning technique where a model is first trained on unlabeled data using relatively simple pretext tasks before being trained on labeled data for target tasks. As self-supervised pretraining can effectively utilize material data without observed property values, it has the potential to improve the model's extrapolation ability. In this paper, we clarify how such self-supervised pretraining can enhance extrapolation performance.We propose an experimental framework for the demonstration and empirically reveal that while models were unable to accurately extrapolate absolute property values, self-supervised pretraining enables them to learn relative tendencies of unobserved property values and improve extrapolation performance.
</details>
<details>
<summary>摘要</summary>
Material 属性预测在材料发展和发现中扮演了关键角色，如电池、半导体、催化剂和药物等应用中。最近，有一个增长的兴趣是通过使用机器学习技术，与传统的理论计算相结合来使用数据驱动方法。在材料科学中，预测未观测值（extrapolation）是特别重要的，因为它允许研究人员对材料进行深入的研究，超出可用数据的限制。然而，即使最近的高能机器学习模型，准确的� extrapolation 仍被广泛认为是一个非常困难的问题。自我超vised pretraining 是一种机器学习技术，其中一个模型首先在无标签数据上使用相对简单的预文任务进行训练，然后在标签数据上进行目标任务的训练。由于自我超vised pretraining 可以充分利用材料数据不包含观测值，因此它有可能提高模型的� extrapolation 能力。在这篇文章中，我们解释了如何使用自我超vised pretraining 提高� extrapolation 性能。我们提出了一种实验框架，并经验表明，虽然模型无法准确地 extrapolate 绝对属性值，但自我超vised pretraining 使得它们学习了未观测值的相对趋势，并提高了� extrapolation 性能。
</details></li>
</ul>
<hr>
<h2 id="How-to-Mask-in-Error-Correction-Code-Transformer-Systematic-and-Double-Masking"><a href="#How-to-Mask-in-Error-Correction-Code-Transformer-Systematic-and-Double-Masking" class="headerlink" title="How to Mask in Error Correction Code Transformer: Systematic and Double Masking"></a>How to Mask in Error Correction Code Transformer: Systematic and Double Masking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08128">http://arxiv.org/abs/2308.08128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seong-Joon Park, Hee-Youl Kwak, Sang-Hyo Kim, Sunghwan Kim, Yongjune Kim, Jong-Seon No</li>
<li>for: 这个论文主要是研究一种基于神经网络的编码器，以提高错误修复码（ECC）的性能。</li>
<li>methods: 这个论文提出了两种新的方法来提高错误修复码编码器（ECCT）的性能。其中一种是基于系统编码技术的新的面Masking矩阵，它可以提高性能并降低计算复杂度。另一种是一种新的双面Masked ECCT架构，它使用两个不同的面矩阵在并行方式中学习码word比特之间的关系。</li>
<li>results: 对于ECCT，提出的两种方法都得到了较好的效果。特别是，使用新的面Masking矩阵可以提高性能，而使用双面Masked ECCT架构可以学习更多的码word比特之间的关系，从而提高decoding性能。<details>
<summary>Abstract</summary>
In communication and storage systems, error correction codes (ECCs) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer (ECCT) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of ECCT, we propose two novel methods. First, leveraging the systematic encoding technique of ECCs, we introduce a new masking matrix for ECCT, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of ECCT called a double-masked ECCT. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive simulation results show that the proposed double-masked ECCT outperforms the conventional ECCT, achieving the state-of-the-art decoding performance with significant margins.
</details>
<details>
<summary>摘要</summary>
在通信和存储系统中，错误修复码（ECC）是确保数据可靠性的关键。随着深度学习在不同领域的应用积极扩大，关注 neural network 基于的解码算法在 ECC 中的研究也在不断增长。其中，Error Correction Code Transformer（ECCT）已经实现了最佳性能，比其他方法的性能优势较大。为了进一步提高 ECCT 的性能，我们提出了两种新的方法。首先，利用 ECC 的系统编码技术，我们引入了一个新的遮盲矩阵，以提高性能并降低计算复杂度。其次，我们提出了一种新的 ECCT 架构，即双遮盲 ECCT，该架构在并行方式中使用两个不同的遮盲矩阵来学习码word 位 bits 之间的更多多样性的关系。我们对 ECCT 进行了广泛的实验，结果显示，提出的双遮盲 ECCT 能够超越传统 ECCT，实现最佳解码性能，并且具有显著的性能优势。
</details></li>
</ul>
<hr>
<h2 id="S-Mixup-Structural-Mixup-for-Graph-Neural-Networks"><a href="#S-Mixup-Structural-Mixup-for-Graph-Neural-Networks" class="headerlink" title="S-Mixup: Structural Mixup for Graph Neural Networks"></a>S-Mixup: Structural Mixup for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08097">http://arxiv.org/abs/2308.08097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sukwonyun/s-mixup">https://github.com/sukwonyun/s-mixup</a></li>
<li>paper_authors: Junghurn Kim, Sukwon Yun, Chanyoung Park</li>
<li>for: 本研究探讨了应用mixup技术在图像上的扩展，尤其是在节点分类任务上。</li>
<li>methods: 本文提出了一种新的结构mixup（S-Mixup），具体来说是根据图像中的结构信息来混合节点。S-Mixup使用图像神经网络（GNN）分类器来获取 pseudo-标签 для没有标签的节点，并使用这些标签作为混合池的组合标准。此外，我们还提出了基于GNN训练的边梯度选择策略，用于选择与混合节点相连的边。</li>
<li>results: 经过广泛的实验表明，S-Mixup可以提高GNN的Robustness和泛化性，尤其是在不同类型的图像中。<details>
<summary>Abstract</summary>
Existing studies for applying the mixup technique on graphs mainly focus on graph classification tasks, while the research in node classification is still under-explored. In this paper, we propose a novel mixup augmentation for node classification called Structural Mixup (S-Mixup). The core idea is to take into account the structural information while mixing nodes. Specifically, S-Mixup obtains pseudo-labels for unlabeled nodes in a graph along with their prediction confidence via a Graph Neural Network (GNN) classifier. These serve as the criteria for the composition of the mixup pool for both inter and intra-class mixups. Furthermore, we utilize the edge gradient obtained from the GNN training and propose a gradient-based edge selection strategy for selecting edges to be attached to the nodes generated by the mixup. Through extensive experiments on real-world benchmark datasets, we demonstrate the effectiveness of S-Mixup evaluated on the node classification task. We observe that S-Mixup enhances the robustness and generalization performance of GNNs, especially in heterophilous situations. The source code of S-Mixup can be found at \url{https://github.com/SukwonYun/S-Mixup}
</details>
<details>
<summary>摘要</summary>
先前的研究主要集中在图像分类任务上应用mixup技术，而节点分类任务的研究仍然尚未得到充分的探索。在这篇论文中，我们提出了一种新的结构强化mixup修饰（S-Mixup）。其核心思想是在混合节点时考虑结构信息。具体来说，S-Mixup通过一个图 neural network（GNN）分类器获得未标注节点的 pseudo-标签和其预测信心。这些服务为混合池的组合的标准。此外，我们利用GNN训练中的边梯度并提出了一种基于边梯度的边选择策略，用于选择混合 Pool 中连接到生成的节点的边。经过了实际的实验，我们证明了S-Mixup可以提高GNN的可靠性和泛化性，特别是在不同类型的情况下。S-Mixup的源代码可以在 GitHub上找到：https://github.com/SukwonYun/S-Mixup。
</details></li>
</ul>
<hr>
<h2 id="Safety-Filter-Design-for-Neural-Network-Systems-via-Convex-Optimization"><a href="#Safety-Filter-Design-for-Neural-Network-Systems-via-Convex-Optimization" class="headerlink" title="Safety Filter Design for Neural Network Systems via Convex Optimization"></a>Safety Filter Design for Neural Network Systems via Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08086">http://arxiv.org/abs/2308.08086</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shaoruchen/nn-system-psf">https://github.com/shaoruchen/nn-system-psf</a></li>
<li>paper_authors: Shaoru Chen, Kong Yao Chee, Nikolai Matni, M. Ani Hsieh, George J. Pappas</li>
<li>for: 这篇论文旨在提出一种基于凸优化的安全筛选方法，以确保神经网络系统在面对扰动时能够保持安全。</li>
<li>methods: 该方法首先使用神经网络验证工具来过度估算神经网络动态，然后通过稳定LPV搜索法找到一个能够保证约束满足的控制器。</li>
<li>results: 数值实验表明，该方法可以有效地确保神经网络系统在面对扰动时的安全性。<details>
<summary>Abstract</summary>
With the increase in data availability, it has been widely demonstrated that neural networks (NN) can capture complex system dynamics precisely in a data-driven manner. However, the architectural complexity and nonlinearity of the NNs make it challenging to synthesize a provably safe controller. In this work, we propose a novel safety filter that relies on convex optimization to ensure safety for a NN system, subject to additive disturbances that are capable of capturing modeling errors. Our approach leverages tools from NN verification to over-approximate NN dynamics with a set of linear bounds, followed by an application of robust linear MPC to search for controllers that can guarantee robust constraint satisfaction. We demonstrate the efficacy of the proposed framework numerically on a nonlinear pendulum system.
</details>
<details>
<summary>摘要</summary>
随着数据的增加，已经广泛证明了神经网络（NN）可以在数据驱动方式下准确捕捉复杂系统动态。然而，神经网络的建筑复杂性和非线性使得Synthesizing a provably safe controller是一项挑战。在这种情况下，我们提出了一种新的安全筛选器，该筛选器基于凸优化来确保神经网络系统的安全性，对于带有添加干扰的系统。我们的方法利用了神经网络验证工具来过度估算神经网络动态，然后通过Robust linear MPC来搜索能够保证约束满足的控制器。我们通过数值方法示出了我们的框架的有效性，并且在非线性挠杆系统上进行了实验。
</details></li>
</ul>
<hr>
<h2 id="Rigid-Transformations-for-Stabilized-Lower-Dimensional-Space-to-Support-Subsurface-Uncertainty-Quantification-and-Interpretation"><a href="#Rigid-Transformations-for-Stabilized-Lower-Dimensional-Space-to-Support-Subsurface-Uncertainty-Quantification-and-Interpretation" class="headerlink" title="Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation"></a>Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08079">http://arxiv.org/abs/2308.08079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ademide O. Mabadeje, Michael J. Pyrcz<br>for:这篇论文主要是为了解决隐藏层数据的维度减少问题，以提高地球科学和能源资源工程中的数据分析和机器学习过程中的重复性和比较性。methods:这篇论文使用了非线性维度减少（NDR）方法，尤其是多元维度减少（MDS）方法，以处理隐藏层数据的维度减少问题。它们的缺点是存在不稳定的唯一解，而且无法扩展到外样点（OOSP）。为了解决这些问题，该论文提出了一种稳定的、包含OOSP的隐藏层数据表示方法。results:该论文通过使用固定变换、计算MDS输入不同性矩阵和应用多个实现来保证变换不变性，并将OOSP纳入到数据表示中。该方法使用了 convex hull 算法和loss函数和 норamlized stress 来衡量扭曲。与Synthetic数据、不同距离度量和实际的DUVERNAY formación 井井相比，结果证明了该方法的有效性。此外，提出的“压缩率”（SR）指标，可以提供有关不确定性的信息，对于数据分析和推理分析是有利的。因此，该工作流程提出了在NDR中提高重复性和比较性的可能性。<details>
<summary>Abstract</summary>
Subsurface datasets inherently possess big data characteristics such as vast volume, diverse features, and high sampling speeds, further compounded by the curse of dimensionality from various physical, engineering, and geological inputs. Among the existing dimensionality reduction (DR) methods, nonlinear dimensionality reduction (NDR) methods, especially Metric-multidimensional scaling (MDS), are preferred for subsurface datasets due to their inherent complexity. While MDS retains intrinsic data structure and quantifies uncertainty, its limitations include unstabilized unique solutions invariant to Euclidean transformations and an absence of out-of-sample points (OOSP) extension. To enhance subsurface inferential and machine learning workflows, datasets must be transformed into stable, reduced-dimension representations that accommodate OOSP.   Our solution employs rigid transformations for a stabilized Euclidean invariant representation for LDS. By computing an MDS input dissimilarity matrix, and applying rigid transformations on multiple realizations, we ensure transformation invariance and integrate OOSP. This process leverages a convex hull algorithm and incorporates loss function and normalized stress for distortion quantification. We validate our approach with synthetic data, varying distance metrics, and real-world wells from the Duvernay Formation. Results confirm our method's efficacy in achieving consistent LDS representations. Furthermore, our proposed "stress ratio" (SR) metric provides insight into uncertainty, beneficial for model adjustments and inferential analysis. Consequently, our workflow promises enhanced repeatability and comparability in NDR for subsurface energy resource engineering and associated big data workflows.
</details>
<details>
<summary>摘要</summary>
底层数据自然而有大数据特点，如庞大量、多样特征和高采样速率，这些特点更加受到物理、工程和地质输入的带来的诸多维度的咒语。现有的维度减少（DR）方法中，非线性维度减少（NDR）方法，尤其是多元维度减少（MDS），对底层数据进行处理是更为首选的，因为它们可以更好地处理底层数据的复杂性。然而，MDS存在两个缺点：一是无法保证唯一解，二是缺乏对外样点（OOSP）的扩展。为了提高底层数据的推断和机器学习工作流程，数据需要被转换成稳定、减少维度的表示，并且包含OOSP。我们的解决方案是使用固定变换来实现稳定的几何减少，并且在多个实现中计算MDS输入不同性矩阵，以确保变换不变性和包含OOSP。这个过程利用了 convex hull 算法和 incorporate 损失函数和正规化压力，以量化扭曲。我们通过使用synthetic data、不同的距离度量和实际的DUVERNAY  formación 井井来验证我们的方法，结果证明了我们的方法的有效性。此外，我们还提出了一个"stress ratio"（SR）指标，可以提供很好的 uncertainty 的视角，这有助于进行模型调整和推断分析。因此，我们的工作流程 promise 提高了NDR 的重复性和相对性，为底层数据的能源资源工程和相关的大数据工作流程提供了更好的支持。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Graph-Neural-Network-for-Privacy-Preserving-Recommendation"><a href="#Decentralized-Graph-Neural-Network-for-Privacy-Preserving-Recommendation" class="headerlink" title="Decentralized Graph Neural Network for Privacy-Preserving Recommendation"></a>Decentralized Graph Neural Network for Privacy-Preserving Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08072">http://arxiv.org/abs/2308.08072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaolin Zheng, Zhongyu Wang, Chaochao Chen, Jiashu Qian, Yao Yang</li>
<li>for: This paper aims to build a privacy-preserving graph neural network (GNN) based recommender system without violating user privacy.</li>
<li>methods: The proposed method, called DGREC, includes three stages: graph construction, local gradient calculation, and global gradient passing. It uses a local differential privacy mechanism called secure gradient-sharing to protect users’ private data.</li>
<li>results: The authors conduct extensive experiments on three public datasets and show that DGREC achieves consistent superiority over existing methods in terms of accuracy and privacy protection.Here’s the simplified Chinese version:</li>
<li>for: 这篇论文目标是构建一个遵循用户隐私的图内存神经网络（GNN）基于推荐系统，不会违反用户的隐私。</li>
<li>methods: 该提议的方法包括三个阶段：图构建、本地梯度计算和全局梯度传递。它使用一种名为安全梯度分享的本地异质隐私机制来保护用户的私人数据。</li>
<li>results: 作者们对三个公共数据集进行了广泛的实验，并证明了 DGREC 在精度和隐私保护方面与现有方法具有一致的优越性。<details>
<summary>Abstract</summary>
Building a graph neural network (GNN)-based recommender system without violating user privacy proves challenging. Existing methods can be divided into federated GNNs and decentralized GNNs. But both methods have undesirable effects, i.e., low communication efficiency and privacy leakage. This paper proposes DGREC, a novel decentralized GNN for privacy-preserving recommendations, where users can choose to publicize their interactions. It includes three stages, i.e., graph construction, local gradient calculation, and global gradient passing. The first stage builds a local inner-item hypergraph for each user and a global inter-user graph. The second stage models user preference and calculates gradients on each local device. The third stage designs a local differential privacy mechanism named secure gradient-sharing, which proves strong privacy-preserving of users' private data. We conduct extensive experiments on three public datasets to validate the consistent superiority of our framework.
</details>
<details>
<summary>摘要</summary>
建立一个基于图神经网络（GNN）的推荐系统，保持用户隐私具有挑战性。现有方法可以分为联邦GNN和分散GNN两种。但两种方法都带有不 DESirable Effects，即通信效率低和隐私泄露。这篇论文提出了DGREC，一种新的分散GNN，用户可以选择公开自己的互动记录。它包括三个阶段：图建构、本地梯度计算和全球梯度传递。第一阶段建立了每个用户的本地内部项目图和全球用户图。第二阶段模型用户偏好并在每个本地设备上计算梯度。第三阶段实现了一种安全梯度分享机制，以保障用户隐私数据的强大隐私。我们在三个公共数据集上进行了广泛的实验，以验证我们的框架的一致性和优越性。
</details></li>
</ul>
<hr>
<h2 id="Freshness-or-Accuracy-Why-Not-Both-Addressing-Delayed-Feedback-via-Dynamic-Graph-Neural-Networks"><a href="#Freshness-or-Accuracy-Why-Not-Both-Addressing-Delayed-Feedback-via-Dynamic-Graph-Neural-Networks" class="headerlink" title="Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks"></a>Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08071">http://arxiv.org/abs/2308.08071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaolin Zheng, Zhongyu Wang, Chaochao Chen, Feng Zhu, Jiashu Qian</li>
<li>for: 预测在线购物系统中用户的购买率，解决延迟反馈问题。</li>
<li>methods: 使用动态图 neural network 模型，包括数据预处理、建立动态图和训练 CVR 预测模型。在模型训练中，我们提出了一种新的图 convolutional 方法 named HLGCN，可以处理 conversion 和 non-conversion 关系。</li>
<li>results: 在三个行业数据集上进行了广泛的实验 validate 了我们的方法的一致优势。<details>
<summary>Abstract</summary>
The delayed feedback problem is one of the most pressing challenges in predicting the conversion rate since users' conversions are always delayed in online commercial systems. Although new data are beneficial for continuous training, without complete feedback information, i.e., conversion labels, training algorithms may suffer from overwhelming fake negatives. Existing methods tend to use multitask learning or design data pipelines to solve the delayed feedback problem. However, these methods have a trade-off between data freshness and label accuracy. In this paper, we propose Delayed Feedback Modeling by Dynamic Graph Neural Network (DGDFEM). It includes three stages, i.e., preparing a data pipeline, building a dynamic graph, and training a CVR prediction model. In the model training, we propose a novel graph convolutional method named HLGCN, which leverages both high-pass and low-pass filters to deal with conversion and non-conversion relationships. The proposed method achieves both data freshness and label accuracy. We conduct extensive experiments on three industry datasets, which validate the consistent superiority of our method.
</details>
<details>
<summary>摘要</summary>
延迟反馈问题是在线商业系统中预测转化率的最大挑战之一，因为用户的转化都会延迟。新的数据对于连续训练是有利，但是无完整的转化标签，训练算法可能会受到干扰性的假负样本的影响。现有方法通常使用多任务学习或设计数据管道来解决延迟反馈问题，但这些方法存在数据新鲜度和标签准确性之间的负担。本文提出了延迟反馈模型化方法（DGDFEM），包括三个阶段：准备数据管道、建立动态图和训练CVR预测模型。在模型训练中，我们提出了一种新的图解决方法 named HLGCN，它利用高频和低频滤波器来处理转化和非转化关系。提出的方法同时保证数据新鲜度和标签准确性。我们对三个行业数据集进行了广泛的实验， validate了我们的方法的一致性优势。
</details></li>
</ul>
<hr>
<h2 id="Max-affine-regression-via-first-order-methods"><a href="#Max-affine-regression-via-first-order-methods" class="headerlink" title="Max-affine regression via first-order methods"></a>Max-affine regression via first-order methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08070">http://arxiv.org/abs/2308.08070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonho Kim, Kiryung Lee</li>
<li>for: 这篇论文主要关注于 max-affine 模型的回归分析，尤其是在测量变量随机分布和反对对应下进行 gradient descent 和 mini-batch stochastic gradient descent 的非对称渐近分析。</li>
<li>methods: 论文使用了 gradient descent 和 mini-batch stochastic gradient descent，并进行了非对称渐近分析。</li>
<li>results: 论文发现，在随机分布和反对对应下，适当初始化的 gradient descent 和 mini-batch stochastic gradient descent 会在适当的错误范围内线性收摄到解。此外，在噪音存在的低样本案例中，SGD 不仅快速执行时间，还超过了对称降低和 gradient descent 的性能。<details>
<summary>Abstract</summary>
We consider regression of a max-affine model that produces a piecewise linear model by combining affine models via the max function. The max-affine model ubiquitously arises in applications in signal processing and statistics including multiclass classification, auction problems, and convex regression. It also generalizes phase retrieval and learning rectifier linear unit activation functions. We present a non-asymptotic convergence analysis of gradient descent (GD) and mini-batch stochastic gradient descent (SGD) for max-affine regression when the model is observed at random locations following the sub-Gaussianity and an anti-concentration with additive sub-Gaussian noise. Under these assumptions, a suitably initialized GD and SGD converge linearly to a neighborhood of the ground truth specified by the corresponding error bound. We provide numerical results that corroborate the theoretical finding. Importantly, SGD not only converges faster in run time with fewer observations than alternating minimization and GD in the noiseless scenario but also outperforms them in low-sample scenarios with noise.
</details>
<details>
<summary>摘要</summary>
我们考虑一个最大拟合模型，它生成一个分割线性模型，通过最大函数将多个拟合模型相加。这种最大拟合模型在信号处理和统计应用中广泛存在，包括多类分类、拍卖问题和凸回归。它还泛化了phas Retrieval和学习矩阵减法。我们提供了非 asymptotic 的收敛分析，证明了梯度下降（GD）和批处理随机梯度下降（SGD）在最大拟合 regression 中的收敛性。在这些假设下，一个适当的初始化GD和SGD会 linearly收敛到一个包含真实值的邻域，以至于这个邻域的错误 bound。我们提供了数字结果，证明了理论发现。此外，SGD不仅在干擦无噪scenario下更快地收敛，也在低样本 scenarios 中超越了交互式最优化和GD。
</details></li>
</ul>
<hr>
<h2 id="A-Reinforcement-Learning-Approach-for-Performance-aware-Reduction-in-Power-Consumption-of-Data-Center-Compute-Nodes"><a href="#A-Reinforcement-Learning-Approach-for-Performance-aware-Reduction-in-Power-Consumption-of-Data-Center-Compute-Nodes" class="headerlink" title="A Reinforcement Learning Approach for Performance-aware Reduction in Power Consumption of Data Center Compute Nodes"></a>A Reinforcement Learning Approach for Performance-aware Reduction in Power Consumption of Data Center Compute Nodes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08069">http://arxiv.org/abs/2308.08069</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akhileshraj91/generalized_rl_anl">https://github.com/akhileshraj91/generalized_rl_anl</a></li>
<li>paper_authors: Akhilesh Raj, Swann Perarnau, Aniruddha Gokhale</li>
<li>for: 这个论文的目的是设计一种基于Reinforcement Learning（RL）的云计算节点资源控制策略，以减少计算节点的能源消耗。</li>
<li>methods: 该论文使用了Reinforcement Learning（RL）技术，使用当前电力消耗和协调器性能（heartbeats）的观察值，设计了一种可以在实际硬件上运行的最佳策略。</li>
<li>results: 该论文通过使用Argo Node Resource Management（NRM）软件栈和Intel Running Average Power Limit（RAPL）硬件控制机制，设计了一种可以控制计算节点的最大供应电力，不会妨碍应用程序性能。通过使用STREAM benchmark进行评估，表明训练了一个RL代理可以在实际硬件上进行动作，均衡电力消耗和应用程序性能。<details>
<summary>Abstract</summary>
As Exascale computing becomes a reality, the energy needs of compute nodes in cloud data centers will continue to grow. A common approach to reducing this energy demand is to limit the power consumption of hardware components when workloads are experiencing bottlenecks elsewhere in the system. However, designing a resource controller capable of detecting and limiting power consumption on-the-fly is a complex issue and can also adversely impact application performance. In this paper, we explore the use of Reinforcement Learning (RL) to design a power capping policy on cloud compute nodes using observations on current power consumption and instantaneous application performance (heartbeats). By leveraging the Argo Node Resource Management (NRM) software stack in conjunction with the Intel Running Average Power Limit (RAPL) hardware control mechanism, we design an agent to control the maximum supplied power to processors without compromising on application performance. Employing a Proximal Policy Optimization (PPO) agent to learn an optimal policy on a mathematical model of the compute nodes, we demonstrate and evaluate using the STREAM benchmark how a trained agent running on actual hardware can take actions by balancing power consumption and application performance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:随着存储计算 becoming a reality, 云数据中心的计算节点的能源需求将继续增长。一种常见的方法是在工作负荷经历瓶颈时限制硬件组件的能源消耗。然而，在实时 detection 和限制能源消耗的设计是一个复杂的问题，可能会影响应用程序性能。在这篇论文中，我们explore使用强化学习（RL）设计云计算节点的能源帽策略，使用当前的电力消耗和应用程序性能（心跳） Observations。通过利用 Argo 节点资源管理（NRM）软件栈和Intel 运行平均电力限制（RAPL）硬件控制机制，我们设计了一个控制器来限制计算节点的最大供应电力，而不会影响应用程序性能。我们使用 PPO 代理来学习一个最佳策略，并在实际硬件上运行。使用 STREAM benchmark，我们 demonstate 和评估一个训练过的代理可以通过平衡电力消耗和应用程序性能来取得行动。
</details></li>
</ul>
<hr>
<h2 id="The-Costly-Dilemma-Generalization-Evaluation-and-Cost-Optimal-Deployment-of-Large-Language-Models"><a href="#The-Costly-Dilemma-Generalization-Evaluation-and-Cost-Optimal-Deployment-of-Large-Language-Models" class="headerlink" title="The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models"></a>The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08061">http://arxiv.org/abs/2308.08061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abi Aryan, Aakash Kumar Nain, Andrew McMahon, Lucas Augusto Meyer, Harpreet Singh Sahota</li>
<li>for: 这篇论文是为了探讨大自然语言模型在生产环境中的应用和管理。</li>
<li>methods: 论文提出了一个横跨三个目标（即通用性、评估和成本优化）的框架，用于评估和优化大自然语言模型在生产环境中的性能和成本。</li>
<li>results: 论文表明，这个框架可以帮助企业对大自然语言模型进行评估和优化，以减少投资的成本和增加效率。<details>
<summary>Abstract</summary>
When deploying machine learning models in production for any product/application, there are three properties that are commonly desired. First, the models should be generalizable, in that we can extend it to further use cases as our knowledge of the domain area develops. Second they should be evaluable, so that there are clear metrics for performance and the calculation of those metrics in production settings are feasible. Finally, the deployment should be cost-optimal as far as possible. In this paper we propose that these three objectives (i.e. generalization, evaluation and cost-optimality) can often be relatively orthogonal and that for large language models, despite their performance over conventional NLP models, enterprises need to carefully assess all the three factors before making substantial investments in this technology. We propose a framework for generalization, evaluation and cost-modeling specifically tailored to large language models, offering insights into the intricacies of development, deployment and management for these large language models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>模型应该是可扩展的，以便在我们对领域知识的发展中可以进一步应用。2. 模型应该可评估，以便有明确的表现指标和在生产环境中可行的计算表现指标。3. 部署应该是最优化的，尽可能保持成本低。在这篇论文中，我们提出这三个目标（即泛化、评估和成本优化）通常是相互独立的，并且对大语言模型来说，企业需要仔细评估这三个因素才能够做出大投资。我们提出了特制的泛化、评估和成本模型，以便更好地发展、部署和管理这些大语言模型。</details></li>
</ol>
<hr>
<h2 id="Robust-Bayesian-Tensor-Factorization-with-Zero-Inflated-Poisson-Model-and-Consensus-Aggregation"><a href="#Robust-Bayesian-Tensor-Factorization-with-Zero-Inflated-Poisson-Model-and-Consensus-Aggregation" class="headerlink" title="Robust Bayesian Tensor Factorization with Zero-Inflated Poisson Model and Consensus Aggregation"></a>Robust Bayesian Tensor Factorization with Zero-Inflated Poisson Model and Consensus Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08060">http://arxiv.org/abs/2308.08060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/klarman-cell-observatory/scbtf_experiments">https://github.com/klarman-cell-observatory/scbtf_experiments</a></li>
<li>paper_authors: Daniel Chafamo, Vignesh Shanmugam, Neriman Tokcan</li>
<li>for: 这篇论文是为了解决高维度计数数据中异常值带来的挑战，提出了一种基于 zero-inflated Poisson tensor factorization（ZIPTF）的新方法。</li>
<li>methods: 这篇论文使用了一种基于 maximum likelihood estimation的经典TF方法，但是这些方法在应用于单元RNA-seq数据时表现不佳。在解决随机性问题方面，这篇论文提出了一种基于consensus的zero-inflated Poisson tensor factorization（C-ZIPTF）方法。</li>
<li>results: 对于 sintetic zero-inflated count data和real single-cell RNA-seq数据，ZIPTF和C-ZIPTF都能够高效地重建知识图和生物 significative的表达程序。ZIPTF在probability of excess zeros高时能够达到2.4倍的准确率提高。此外，C-ZIPTF可以大幅提高重建精度和一致性。<details>
<summary>Abstract</summary>
Tensor factorizations (TF) are powerful tools for the efficient representation and analysis of multidimensional data. However, classic TF methods based on maximum likelihood estimation underperform when applied to zero-inflated count data, such as single-cell RNA sequencing (scRNA-seq) data. Additionally, the stochasticity inherent in TFs results in factors that vary across repeated runs, making interpretation and reproducibility of the results challenging. In this paper, we introduce Zero Inflated Poisson Tensor Factorization (ZIPTF), a novel approach for the factorization of high-dimensional count data with excess zeros. To address the challenge of stochasticity, we introduce Consensus Zero Inflated Poisson Tensor Factorization (C-ZIPTF), which combines ZIPTF with a consensus-based meta-analysis. We evaluate our proposed ZIPTF and C-ZIPTF on synthetic zero-inflated count data and synthetic and real scRNA-seq data. ZIPTF consistently outperforms baseline matrix and tensor factorization methods in terms of reconstruction accuracy for zero-inflated data. When the probability of excess zeros is high, ZIPTF achieves up to $2.4\times$ better accuracy. Additionally, C-ZIPTF significantly improves the consistency and accuracy of the factorization. When tested on both synthetic and real scRNA-seq data, ZIPTF and C-ZIPTF consistently recover known and biologically meaningful gene expression programs.
</details>
<details>
<summary>摘要</summary>
tensor化工具 (TF) 是一种强大的数据表示和分析工具，但 классические TF 方法基于最大化可能性估计在应用于零含量计数数据时表现不佳，如单个细胞 RNA 测序 (scRNA-seq) 数据。此外，TF 中的随机性使得因素在重复运行中变化，从而使得结果的解释和重现困难。在这篇论文中，我们介绍了 Zero Inflated Poisson Tensor Factorization (ZIPTF)，一种用于高维计数数据中的零含量的因素化方法。为了解决随机性的挑战，我们引入了 Consensus Zero Inflated Poisson Tensor Factorization (C-ZIPTF)，它将 ZIPTF 与 consensus-based 元分析结合。我们对 ZIPTF 和 C-ZIPTF 在 sintetic zero-inflated count data 和 sintetic 和实际 scRNA-seq data 上进行评估。ZIPTF 在零含量数据上的重建精度与基线矩阵和矩阵因素化方法相比，表现出了明显的优势。当零含量的概率高时，ZIPTF 的精度可以达到 2.4 倍。此外，C-ZIPTF 可以有效地提高因素化的一致性和精度。当测试在 sintetic 和实际 scRNA-seq data 上时，ZIPTF 和 C-ZIPTF 一致地回归了知道的和生物学意义的基因表达程序。
</details></li>
</ul>
<hr>
<h2 id="Simple-online-learning-with-consistency-oracle"><a href="#Simple-online-learning-with-consistency-oracle" class="headerlink" title="Simple online learning with consistency oracle"></a>Simple online learning with consistency oracle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08055">http://arxiv.org/abs/2308.08055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Kozachinskiy, Tomasz Steifer</li>
<li>for: 这个论文是关于在具有一个一致性 oracle 的模型下进行在线学习的。这个模型是由 Assos et al. (COLT’23) 最近提出的，它是由于标准的在线学习方法 rely on 计算 Littlestone 维度的问题，这是计算 tractable 的。</li>
<li>methods: 这个论文提出了一种新的在线学习算法，该算法可以在类的 Littlestone 维度为 $d$ 时最多出错 $O(256^d)$ 次。我们的证明比 Assos et al. 的证明更简单，只需要使用了一些基本的 Littlestone 维度的性质。</li>
<li>results: 这个论文的结果包括两点：首先，我们提供了一种可计算的在线学习算法，该算法可以解决一个开放的问题，即是每个有限 Littlestone 维度的类都存在可计算的在线学习算法。其次，我们证明了存在一个不可 counts 的类，即是任何类的 Littlestone 维度不小于 $2^{d+1}-2$ 时，不可能有一个可计算的在线学习算法。<details>
<summary>Abstract</summary>
We consider online learning in the model where a learning algorithm can access the class only via the consistency oracle -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al. (COLT'23). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a problem that is computationally intractable. Assos et al. gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C > 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also observe that there exists no algorithm in this model that makes at most $2^{d+1}-2$ mistakes. We also observe that our algorithm (as well as the algorithm of Assos et al.) solves an open problem by Hasrati and Ben-David (ALT'23). Namely, it demonstrates that every class of finite Littlestone dimension with recursively enumerable representation admits a computable online learner (that may be undefined on unrealizable samples).
</details>
<details>
<summary>摘要</summary>
我们考虑在模型中使用线上学习，其中学习算法可以通过一个具有一致性 oracle 访问 клаス。这个模型最近在 Assos 等人（COLT'23）中被考虑过。这个模型的动机是由于标准的线上学习方法需要计算 Littlestone 次数，这是 computationally intractable 的问题。Assos 等人提供了一个线上学习算法，它在类的 Littlestone 次数为 d 时会 maken at most C^d 的错误，其中 C 是一个未知的绝对常数。我们提供了一个新的算法，它在类的 Littlestone 次数为 d 时会 maken at most O(256^d) 的错误。我们的证明比较简单，只需要使用类的 Littlestone 次数的非常基本的性质。我们还观察到，不存在任何算法可以在类的 Littlestone 次数为 d 时 maken at most 2^(d+1)-2 的错误。此外，我们的算法（以及 Assos 等人的算法）解决了 Hasrati 和 Ben-David（ALT'23）的开问题。具体而言，它证明了每个有质量的类都存在可计算的线上学习器（可能是未定义的在不可能的测试样本上）。
</details></li>
</ul>
<hr>
<h2 id="Natural-Evolution-Strategies-as-a-Black-Box-Estimator-for-Stochastic-Variational-Inference"><a href="#Natural-Evolution-Strategies-as-a-Black-Box-Estimator-for-Stochastic-Variational-Inference" class="headerlink" title="Natural Evolution Strategies as a Black Box Estimator for Stochastic Variational Inference"></a>Natural Evolution Strategies as a Black Box Estimator for Stochastic Variational Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08053">http://arxiv.org/abs/2308.08053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Ayaz Amin</li>
<li>for: 用于替代Variational Autoencoders（VAE）中的梯度估计，以解决VAE中的一种设计选择（即重parameterization trick）限制了模型的类型。</li>
<li>methods: 使用自然演化策略来提供一种不假设 distribuition 类型的 estimator，allowing for the creation of models that would otherwise not have been possible under the VAE framework。</li>
<li>results: 提出了一种不受 VAE 设计选择限制的模型创建方法，allowing for the creation of more diverse and complex models.<details>
<summary>Abstract</summary>
Stochastic variational inference and its derivatives in the form of variational autoencoders enjoy the ability to perform Bayesian inference on large datasets in an efficient manner. However, performing inference with a VAE requires a certain design choice (i.e. reparameterization trick) to allow unbiased and low variance gradient estimation, restricting the types of models that can be created. To overcome this challenge, an alternative estimator based on natural evolution strategies is proposed. This estimator does not make assumptions about the kind of distributions used, allowing for the creation of models that would otherwise not have been possible under the VAE framework.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:Stochastic variational inference和其 derivatives的形式为variational autoencoders (VAE)可以高效地进行 bayesian inference on large datasets。然而，使用 VAE 进行 inference 需要特定的设计选择（即 reparameterization trick）以确保无偏度和低差异的梯度估计，这限制了可以创建的模型类型。为了解决这个挑战，一种基于自然进化策略的替代估计器被提议。这种估计器不会假设分布的类型，因此可以创建 VAE 框架下不可能创建的模型。
</details></li>
</ul>
<hr>
<h2 id="Unbiased-Decisions-Reduce-Regret-Adversarial-Domain-Adaptation-for-the-Bank-Loan-Problem"><a href="#Unbiased-Decisions-Reduce-Regret-Adversarial-Domain-Adaptation-for-the-Bank-Loan-Problem" class="headerlink" title="Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem"></a>Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08051">http://arxiv.org/abs/2308.08051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elena Gal, Shaun Singh, Aldo Pacchiano, Ben Walker, Terry Lyons, Jakob Foerster</li>
<li>for: 本研究旨在 Addressing bias in binary classification decisions based on limited data in near real-time, particularly in scenarios where the true label is only observed when a data point is assigned a positive label.</li>
<li>methods: 该研究提出了一种新的方法 called adversarial optimism (AdOpt), which uses adversarial domain adaptation to directly address bias in the training set and learn an unbiased but informative representation of past data.</li>
<li>results: 对一组Difficult benchmark problems, AdOpt 能够显著超过州Of-the-art表现，并且初步证明了该方法在这种设定中改善公平性。<details>
<summary>Abstract</summary>
In many real world settings binary classification decisions are made based on limited data in near real-time, e.g. when assessing a loan application. We focus on a class of these problems that share a common feature: the true label is only observed when a data point is assigned a positive label by the principal, e.g. we only find out whether an applicant defaults if we accepted their loan application. As a consequence, the false rejections become self-reinforcing and cause the labelled training set, that is being continuously updated by the model decisions, to accumulate bias. Prior work mitigates this effect by injecting optimism into the model, however this comes at the cost of increased false acceptance rate. We introduce adversarial optimism (AdOpt) to directly address bias in the training set using adversarial domain adaptation. The goal of AdOpt is to learn an unbiased but informative representation of past data, by reducing the distributional shift between the set of accepted data points and all data points seen thus far. AdOpt significantly exceeds state-of-the-art performance on a set of challenging benchmark problems. Our experiments also provide initial evidence that the introduction of adversarial domain adaptation improves fairness in this setting.
</details>
<details>
<summary>摘要</summary>
在许多实际场景中，二进制分类决策基于有限数据进行实时进行，例如审批贷款申请。我们关注一类这些问题，它们共同特点是：真正的标签只有当数据点被主体分配正确标签时才可以见到，例如只有当我们接受了贷款申请后才能确定应用者是否 defaults。这导致假拒绝被自我强化，从而使标记训练集，由模型决策而不断更新的集合，受到偏见。先前的工作通过在模型中注入乐观性来 mitigate这种效应，但这会导致准确批准率上升。我们引入对抗优化（AdOpt），直接通过对抗领域适应来减少标记训练集中的偏见。AdOpt的目标是学习不偏的， yet informative 的过去数据表示，通过减少接受数据点和所有见过的数据点之间的分布差异。AdOpt在一组具有挑战性的benchmark问题上表现出色，我们的实验也提供了初步证据，表明在这种设置中，对抗领域适应可以提高公平性。
</details></li>
</ul>
<hr>
<h2 id="Regret-Lower-Bounds-in-Multi-agent-Multi-armed-Bandit"><a href="#Regret-Lower-Bounds-in-Multi-agent-Multi-armed-Bandit" class="headerlink" title="Regret Lower Bounds in Multi-agent Multi-armed Bandit"></a>Regret Lower Bounds in Multi-agent Multi-armed Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08046">http://arxiv.org/abs/2308.08046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengfan Xu, Diego Klabjan</li>
<li>for: 本文研究了多客户端多臂抽筋问题，即每个客户端面临着一个分布式抽筋问题，总体系统性能被评估为尝验 regret。</li>
<li>methods: 本文使用了多种方法，包括贪婪启发、决策规则、搜索等，以实现高效的启发和减少 regret。</li>
<li>results: 本文提供了多种情况下的 regret 下界，包括对各种设定下的 Connectivity 和奖励分布的研究。Specifically, 当图表现good connectivity properties和奖励是随机分布时，我们证明了下界为O（log T）的instance-dependent bounds和$\sqrt{T}$的mean-gap independent bounds，并证明了其紧张性。在对抗奖励下，我们建立了下界为O(T^{2&#x2F;3})的连接图下界，这与之前的上界相匹配。此外，当图是离散的时，我们证明了线性的下界。相比之前的研究，本文提供了严格的下界研究。<details>
<summary>Abstract</summary>
Multi-armed Bandit motivates methods with provable upper bounds on regret and also the counterpart lower bounds have been extensively studied in this context. Recently, Multi-agent Multi-armed Bandit has gained significant traction in various domains, where individual clients face bandit problems in a distributed manner and the objective is the overall system performance, typically measured by regret. While efficient algorithms with regret upper bounds have emerged, limited attention has been given to the corresponding regret lower bounds, except for a recent lower bound for adversarial settings, which, however, has a gap with let known upper bounds. To this end, we herein provide the first comprehensive study on regret lower bounds across different settings and establish their tightness. Specifically, when the graphs exhibit good connectivity properties and the rewards are stochastically distributed, we demonstrate a lower bound of order $O(\log T)$ for instance-dependent bounds and $\sqrt{T}$ for mean-gap independent bounds which are tight. Assuming adversarial rewards, we establish a lower bound $O(T^{\frac{2}{3}})$ for connected graphs, thereby bridging the gap between the lower and upper bound in the prior work. We also show a linear regret lower bound when the graph is disconnected. While previous works have explored these settings with upper bounds, we provide a thorough study on tight lower bounds.
</details>
<details>
<summary>摘要</summary>
多臂弓箭刺激方法的研究已有证明的最高 regret 上界和对应的下界也得到了广泛的研究。在这个上下文中，最近的多智能体多臂弓箭问题已经在不同领域得到了广泛的应用，其中每个客户面临着分布式的弓箭问题，并且目标是总系统性能，通常由 regret 来度量。虽然有效的算法得到了提出，但对应的 regret 下界却受到了有限的关注，除了最近的对抗性下界，其中 however 有一定的差距。为了解决这个问题，我们在这里提供了首次的全面的下界研究，并证明其紧耦合。具体来说，当图表现出良好的连接性和奖励是随机分布的时候，我们示出了一个下界为 $O(\log T)$ 的实例依赖下界和 $ \sqrt{T} $ 的无关下界，这些下界都是紧耦合的。在对抗性奖励下，我们确立了一个下界为 $O(T^{2/3})$ 的连接图下界，因此bridging了之前的下界和上界之间的差距。此外，当图为离散图时，我们还证明了一个线性的下界。相比之前的研究，我们在这里提供了一个全面的下界研究。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Analysis-of-the-Capabilities-of-Nature-inspired-Feature-Selection-Algorithms-in-Predicting-Student-Performance"><a href="#A-Comparative-Analysis-of-the-Capabilities-of-Nature-inspired-Feature-Selection-Algorithms-in-Predicting-Student-Performance" class="headerlink" title="A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance"></a>A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08574">http://arxiv.org/abs/2308.08574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Trask</li>
<li>for: 预测学生表现，以便采取有效的预warning措施对于受风险学生。</li>
<li>methods: 使用12种自然引导的算法来预测学生表现，并对3个数据集进行分析，包括单个课程表现、多门课程表现和点播数据。</li>
<li>results: 结果显示，无论是哪个数据集，都可以通过结合NIAs进行特征选择和传统机器学习算法进行分类，提高预测精度，同时减少特征集大小by 2&#x2F;3。<details>
<summary>Abstract</summary>
Predicting student performance is key in leveraging effective pre-failure interventions for at-risk students. In this paper, I have analyzed the relative performance of a suite of 12 nature-inspired algorithms when used to predict student performance across 3 datasets consisting of instance-based clickstream data, intra-course single-course performance, and performance when taking multiple courses simultaneously. I found that, for all datasets, leveraging an ensemble approach using NIAs for feature selection and traditional ML algorithms for classification increased predictive accuracy while also reducing feature set size by 2/3.
</details>
<details>
<summary>摘要</summary>
预测学生表现是键在实施有效预测失败学生之前的干预措施中。在这篇论文中，我分析了12种自然指导算法的相对性，当用于预测学生表现 across 3个数据集，包括单个实例流量数据、单个课程表现和同时攻击多门课程表现。我发现，对于所有数据集，使用NIAs进行特征选择和传统机器学习算法进行分类可以提高预测精度，同时减少特征集的大小 by 2/3。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Data-Generated-by-Gaussian-Mixture-Models-Using-Deep-ReLU-Networks"><a href="#Classification-of-Data-Generated-by-Gaussian-Mixture-Models-Using-Deep-ReLU-Networks" class="headerlink" title="Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks"></a>Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08030">http://arxiv.org/abs/2308.08030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian-Yi Zhou, Xiaoming Huo</li>
<li>for: 这个论文研究了使用深度ReLU神经网络进行二分类问题中的数据从 ${\mathbb R}^d$ 生成的 Gaussian Mixture Models (GMMs) 的极限风险误差。</li>
<li>methods: 我们提供了非对易Bounds和抽象率的极限风险误差的非准确上界，这些上界不依赖于维度 $d$，表明深度ReLU网络可以缓解维度约束的味道。</li>
<li>results: 我们的结果表明，使用深度ReLU网络进行二分类问题中的数据从 ${\mathbb R}^d$ 生成的 Gaussian Mixture Models (GMMs) 可以减少误差，并且不受维度 $d$ 的影响。<details>
<summary>Abstract</summary>
This paper studies the binary classification of unbounded data from ${\mathbb R}^d$ generated under Gaussian Mixture Models (GMMs) using deep ReLU neural networks. We obtain $\unicode{x2013}$ for the first time $\unicode{x2013}$ non-asymptotic upper bounds and convergence rates of the excess risk (excess misclassification error) for the classification without restrictions on model parameters. The convergence rates we derive do not depend on dimension $d$, demonstrating that deep ReLU networks can overcome the curse of dimensionality in classification. While the majority of existing generalization analysis of classification algorithms relies on a bounded domain, we consider an unbounded domain by leveraging the analyticity and fast decay of Gaussian distributions. To facilitate our analysis, we give a novel approximation error bound for general analytic functions using ReLU networks, which may be of independent interest. Gaussian distributions can be adopted nicely to model data arising in applications, e.g., speeches, images, and texts; our results provide a theoretical verification of the observed efficiency of deep neural networks in practical classification problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Planning-to-Learn-A-Novel-Algorithm-for-Active-Learning-during-Model-Based-Planning"><a href="#Planning-to-Learn-A-Novel-Algorithm-for-Active-Learning-during-Model-Based-Planning" class="headerlink" title="Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning"></a>Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08029">http://arxiv.org/abs/2308.08029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rowanlibr/sophisticated-learning">https://github.com/rowanlibr/sophisticated-learning</a></li>
<li>paper_authors: Rowan Hodson, Bruce Bassett, Charel van Hoof, Benjamin Rosman, Mark Solms, Jonathan P. Shock, Ryan Smith</li>
<li>For: 本研究的目的是比较Active Inference和 bayesian reinforcement learning（RL） schemes在解决相似问题时的表现，以及扩展Active Inference以包括活动学习。* Methods: 本研究使用了一种新的、生物学上启发的环境，用于强调解决问题的问题结构，并使用了推理搜索和对假推理来解决问题。* Results: 研究结果显示，使用了扩展的Active Inference（SL）算法可以在这种生物学上适用的环境中高效地解决问题，并且在比较 bayes-adaptive RL和Upper confidence bound算法时表现更好。这些结果为Active Inference在解决这类生物学上适用的问题提供了更多的支持，并为测试人类认知理论提供了新的工具。<details>
<summary>Abstract</summary>
Active Inference is a recent framework for modeling planning under uncertainty. Empirical and theoretical work have now begun to evaluate the strengths and weaknesses of this approach and how it might be improved. A recent extension - the sophisticated inference (SI) algorithm - improves performance on multi-step planning problems through recursive decision tree search. However, little work to date has been done to compare SI to other established planning algorithms. SI was also developed with a focus on inference as opposed to learning. The present paper has two aims. First, we compare performance of SI to Bayesian reinforcement learning (RL) schemes designed to solve similar problems. Second, we present an extension of SI - sophisticated learning (SL) - that more fully incorporates active learning during planning. SL maintains beliefs about how model parameters would change under the future observations expected under each policy. This allows a form of counterfactual retrospective inference in which the agent considers what could be learned from current or past observations given different future observations. To accomplish these aims, we make use of a novel, biologically inspired environment designed to highlight the problem structure for which SL offers a unique solution. Here, an agent must continually search for available (but changing) resources in the presence of competing affordances for information gain. Our simulations show that SL outperforms all other algorithms in this context - most notably, Bayes-adaptive RL and upper confidence bound algorithms, which aim to solve multi-step planning problems using similar principles (i.e., directed exploration and counterfactual reasoning). These results provide added support for the utility of Active Inference in solving this class of biologically-relevant problems and offer added tools for testing hypotheses about human cognition.
</details>
<details>
<summary>摘要</summary>
active inference是一种最近的规划下不确定性框架。实验和理论工作现在开始评估这种方法的优缺点和如何改进它。一种最新的扩展——复杂的推理（SI）算法——在多步规划问题上提高性能通过重层决策树搜索。然而，到目前为止，尚未对SI与其他已知规划算法进行比较。SI在推理而非学习方面得到了开发。本文的两个目标是：首先，比较SI与 bayesian reinforcement learning（RL）算法，解决类似问题。其次，我们提出了一种扩展SI的方法——复杂学习（SL），它更全面地包括活动学习在规划中。SL保留了对未来观测所期望的模型参数变化的信念。这allowsthe agent to consider what could be learned from current or past observations given different future observations。为了实现这些目标，我们使用了一个新的、生物学发现环境，这种环境可以强调规划问题中的问题结构，对于SL提供了特殊的解决方案。在这个环境中，agent需要不断搜索可用（但是变化的）资源，同时面临着竞争的信息收获可能性。我们的 simulations表明，SL在这种情况下表现出色，比bayes-adaptive RL和Upper confidence bound算法（这些算法目标解决类似的多步规划问题，使用相同的原则，即导航探索和对假推理）。这些结果为活动推断在这类生物学相关问题中的 utility提供了进一步的支持，并为测试人类认知假设提供了更多的工具。
</details></li>
</ul>
<hr>
<h2 id="Potential-Energy-Advantage-of-Quantum-Economy"><a href="#Potential-Energy-Advantage-of-Quantum-Economy" class="headerlink" title="Potential Energy Advantage of Quantum Economy"></a>Potential Energy Advantage of Quantum Economy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08025">http://arxiv.org/abs/2308.08025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Liu, Hansheng Jiang, Zuo-Jun Max Shen</li>
<li>for: 本研究旨在探讨量子计算在能效性方面的优势，并证明量子计算可以在能效性和盈利性两个方面超过类别计算。</li>
<li>methods: 本研究使用了 Cournot 竞争模型，并在能效性限制下研究了量子计算和类别计算的比较。</li>
<li>results: 研究发现，量子计算在大规模计算情况下可以实现更高的盈利性和能效性，并且这种优势是基于实际物理参数。<details>
<summary>Abstract</summary>
Energy cost is increasingly crucial in the modern computing industry with the wide deployment of large-scale machine learning models and language models. For the firms that provide computing services, low energy consumption is important both from the perspective of their own market growth and the government's regulations. In this paper, we study the energy benefits of quantum computing vis-a-vis classical computing. Deviating from the conventional notion of quantum advantage based solely on computational complexity, we redefine advantage in an energy efficiency context. Through a Cournot competition model constrained by energy usage, we demonstrate quantum computing firms can outperform classical counterparts in both profitability and energy efficiency at Nash equilibrium. Therefore quantum computing may represent a more sustainable pathway for the computing industry. Moreover, we discover that the energy benefits of quantum computing economies are contingent on large-scale computation. Based on real physical parameters, we further illustrate the scale of operation necessary for realizing this energy efficiency advantage.
</details>
<details>
<summary>摘要</summary>
现代计算业中能源成本日益重要，由于大规模机器学习模型和语言模型的广泛部署。为提供计算服务的公司来说，低能耗是重要的，不仅从市场增长的角度来看，还从政府的法规来看。在这篇论文中，我们研究了量子计算对于纳什平衡下的能源利好。相比传统的计算复杂性基础上的优势，我们重新定义了优势在能效环境下的意义。通过一个固定能源使用的 Cournot竞争模型，我们示出了量子计算公司可以在纳什平衡下超过 классиical对手在利润和能效环境方面表现优势。因此，量子计算可能代表计算业更可持续的发展途径。此外，我们发现了量子计算经济的能源利好取决于大规模计算。基于实际物理参数，我们进一步说明了实现这种能效优势所需的规模。
</details></li>
</ul>
<hr>
<h2 id="Active-Inverse-Learning-in-Stackelberg-Trajectory-Games"><a href="#Active-Inverse-Learning-in-Stackelberg-Trajectory-Games" class="headerlink" title="Active Inverse Learning in Stackelberg Trajectory Games"></a>Active Inverse Learning in Stackelberg Trajectory Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08017">http://arxiv.org/abs/2308.08017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Yu, Jacob Levy, Negar Mehr, David Fridovich-Keil, Ufuk Topcu</li>
<li>for: 这篇论文主要写于哪个问题上？	+ 回答：Game-theoretic inverse learning问题，即从行为中推断玩家目标函数。</li>
<li>methods: 这篇论文使用了哪些方法？	+ 回答：提议了一种活动 inverse learning方法，通过帮助领导者尝试不同假设中的玩家目标函数，以加速领导者对玩家目标函数的推断。</li>
<li>results: 这篇论文得到了什么结果？	+ 回答：比对uniformly random inputs的情况，提议的输入可以加速玩家目标函数的推断，conditioned on the follower’s trajectory的概率加速了orders of magnitude。<details>
<summary>Abstract</summary>
Game-theoretic inverse learning is the problem of inferring the players' objectives from their actions. We formulate an inverse learning problem in a Stackelberg game between a leader and a follower, where each player's action is the trajectory of a dynamical system. We propose an active inverse learning method for the leader to infer which hypothesis among a finite set of candidates describes the follower's objective function. Instead of using passively observed trajectories like existing methods, the proposed method actively maximizes the differences in the follower's trajectories under different hypotheses to accelerate the leader's inference. We demonstrate the proposed method in a receding-horizon repeated trajectory game. Compared with uniformly random inputs, the leader inputs provided by the proposed method accelerate the convergence of the probability of different hypotheses conditioned on the follower's trajectory by orders of magnitude.
</details>
<details>
<summary>摘要</summary>
<<SYS>>游戏理论反学习是推理玩家的目标函数的问题。我们将游戏形式为 Stackelberg 游戏的领袖和追随者之间的反学习问题进行形式化。我们提议一种活动的反学习方法，使领袖可以根据追随者的动力系统轨迹中的差异来推断追随者的目标函数中的哪一个假设。不同于现有方法，我们的方法不使用被动地观察到的轨迹，而是活动地增加不同假设下追随者的轨迹之间的差异，以加速领袖的推断。我们在回归 horizon 重复轨迹游戏中示cases。相比于随机输入，由我们提议的领袖输入可以提高conditioned on the follower's trajectory的各个假设的概率的减少速度，这些减少速度可以达到orders of magnitude。Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="GRINN-A-Physics-Informed-Neural-Network-for-solving-hydrodynamic-systems-in-the-presence-of-self-gravity"><a href="#GRINN-A-Physics-Informed-Neural-Network-for-solving-hydrodynamic-systems-in-the-presence-of-self-gravity" class="headerlink" title="GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity"></a>GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08010">http://arxiv.org/abs/2308.08010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayantan Auddy, Ramit Dey, Neal J. Turner, Shantanu Basu<br>for: 这篇论文旨在模拟三维自引力液体系统，以解决astrophysics中许多基础问题，如星系形成、星系核心形成、大规模结构的发展等。methods: 该论文使用物理学 informed neural network（PINN）技术，在无网格框架下实现3D自引力液体系统的模拟。results: 该论文的结果表明，GRINN在一个iso thermic气体中的引力不稳定和波传播问题上具有高准确性和高效率。与传统网格代码相比，GRINN的计算时间不随维度的增加而增长，而是随着计算量的增加而增长。这些结果表明，PINN技术在astrophysical flows中可能带来 significative advances。<details>
<summary>Abstract</summary>
Modeling self-gravitating gas flows is essential to answering many fundamental questions in astrophysics. This spans many topics including planet-forming disks, star-forming clouds, galaxy formation, and the development of large-scale structures in the Universe. However, the nonlinear interaction between gravity and fluid dynamics offers a formidable challenge to solving the resulting time-dependent partial differential equations (PDEs) in three dimensions (3D). By leveraging the universal approximation capabilities of a neural network within a mesh-free framework, physics informed neural networks (PINNs) offer a new way of addressing this challenge. We introduce the gravity-informed neural network (GRINN), a PINN-based code, to simulate 3D self-gravitating hydrodynamic systems. Here, we specifically study gravitational instability and wave propagation in an isothermal gas. Our results match a linear analytic solution to within 1\% in the linear regime and a conventional grid code solution to within 5\% as the disturbance grows into the nonlinear regime. We find that the computation time of the GRINN does not scale with the number of dimensions. This is in contrast to the scaling of the grid-based code for the hydrodynamic and self-gravity calculations as the number of dimensions is increased. Our results show that the GRINN computation time is longer than the grid code in one- and two- dimensional calculations but is an order of magnitude lesser than the grid code in 3D with similar accuracy. Physics-informed neural networks like GRINN thus show promise for advancing our ability to model 3D astrophysical flows.
</details>
<details>
<summary>摘要</summary>
模拟自引力液体流动是astrophysics中答您许多基本问题的关键。这些问题包括 planet-forming 盘、star-forming 云、galaxy 形成和 universe 大规模结构的发展。然而，在三维空间中非线性的引力和流体动力学交互，对解决时间依赖的 partial differential equations (PDEs) 提出了挑战。通过利用神经网络的通用近似能力，physics informed neural networks (PINNs) 提供了一种新的解决方案。我们介绍了引力 informed neural network (GRINN)，一种基于 PINN 的代码，用于模拟三维自引力液体系统。在这里，我们专门研究引力不稳定和波传播在固定温度气体中。我们的结果与线性分析解匹配在线性 régime中的1%，并与基于网格的代码解匹配在非线性 régime中的5%。我们发现GRINN 的计算时间与维度无关，与基于网格的代码计算时间成正比。这与维度增加后网格代码的计算时间增长相比，GRINN 的计算时间更短。 physics-informed neural networks 如 GRINN 因此显示了在模拟三维astrophysical flows中的承诺。
</details></li>
</ul>
<hr>
<h2 id="BI-LAVA-Biocuration-with-Hierarchical-Image-Labeling-through-Active-Learning-and-Visual-Analysis"><a href="#BI-LAVA-Biocuration-with-Hierarchical-Image-Labeling-through-Active-Learning-and-Visual-Analysis" class="headerlink" title="BI-LAVA: Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis"></a>BI-LAVA: Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08003">http://arxiv.org/abs/2308.08003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Trelles, Andrew Wentzel, William Berrios, G. Elisabeta Marai</li>
<li>For: 本研究用于 Addressing the challenges of creating useful datasets for biocuration in the biomedical domain, particularly the hierarchical nature of image labels, the overhead of processing images, the absence or incompleteness of labeled data, and the expertise required to label this type of data.* Methods: 本研究使用了一种 Iterative visual analytics and active learning strategy, 包括 a small set of image labels, a hierarchical set of image classifiers, and active learning to help model builders deal with incomplete ground-truth labels, target a hierarchical taxonomy of image modalities, and classify a large pool of unlabeled images.* Results: 研究表明，BI-LAVA 系统可以帮助域专家更好地理解类别内的特征，以及验证和改进数据质量在标注和未标注集合中。<details>
<summary>Abstract</summary>
In the biomedical domain, taxonomies organize the acquisition modalities of scientific images in hierarchical structures. Such taxonomies leverage large sets of correct image labels and provide essential information about the importance of a scientific publication, which could then be used in biocuration tasks. However, the hierarchical nature of the labels, the overhead of processing images, the absence or incompleteness of labeled data, and the expertise required to label this type of data impede the creation of useful datasets for biocuration. From a multi-year collaboration with biocurators and text-mining researchers, we derive an iterative visual analytics and active learning strategy to address these challenges. We implement this strategy in a system called BI-LAVA Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis. BI-LAVA leverages a small set of image labels, a hierarchical set of image classifiers, and active learning to help model builders deal with incomplete ground-truth labels, target a hierarchical taxonomy of image modalities, and classify a large pool of unlabeled images. BI-LAVA's front end uses custom encodings to represent data distributions, taxonomies, image projections, and neighborhoods of image thumbnails, which help model builders explore an unfamiliar image dataset and taxonomy and correct and generate labels. An evaluation with machine learning practitioners shows that our mixed human-machine approach successfully supports domain experts in understanding the characteristics of classes within the taxonomy, as well as validating and improving data quality in labeled and unlabeled collections.
</details>
<details>
<summary>摘要</summary>
在生物医学领域，taxonomy 组织科学图像的获取方式在层次结构中。这些taxonomy 利用大量正确的图像标签，提供了科学公版的重要信息，可以用于生物团采工作。然而，层次性标签、处理图像的开销、标签数据的缺失或不完整、以及标签这类数据的专业知识卷积着创建有用的数据集。从多年的biocurator和文本挖掘研究人员的合作，我们 derivate了一种迭代式视觉分析和活动学习策略。我们在BI-LAVA 系统中实现了这种策略，BI-LAVA 是一个通过活动学习和迭代式视觉分析来帮助模型建立者处理部分标签、针对层次的图像模式和大量未标记图像进行分类的系统。BI-LAVA 的前端使用自定编码来表示数据分布、税onomy、图像投影和图像缩略图的邻域，这些编码帮助模型建立者探索未familiar的图像集和税onomy，并且 correction和生成标签。我们与机器学习实践者进行评估，发现我们的人机共同approach 成功地支持领域专家理解税onomy中类别的特点，以及验证和改进标签数据的质量。
</details></li>
</ul>
<hr>
<h2 id="A-physics-informed-machine-learning-model-for-reconstruction-of-dynamic-loads"><a href="#A-physics-informed-machine-learning-model-for-reconstruction-of-dynamic-loads" class="headerlink" title="A physics-informed machine learning model for reconstruction of dynamic loads"></a>A physics-informed machine learning model for reconstruction of dynamic loads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08571">http://arxiv.org/abs/2308.08571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gledson Rodrigo Tondo, Igor Kavrakov, Guido Morgenthal</li>
<li>for: 这篇论文是为了评估长 Span bridges 在不同动态刺激下的响应，以及考虑这些刺激对结构系统的影响。</li>
<li>methods: 这篇论文使用了概率Physics-informed机器学习框架，基于 Gaussian process regression 重构动力。该模型可以处理受限和杂质数据，并提供自然的衰减方法来补做测量系统中的噪声。</li>
<li>results: 这篇论文通过应用于大坝东桥的 aerodynamic 分析，计算了不同动态刺激下的响应，并使用了稀缺和噪声的测量数据来重构动力。结果显示，模型和实际应用的动力之间存在良好的一致性，并可以扩展到计算全局响应和结构内力。<details>
<summary>Abstract</summary>
Long-span bridges are subjected to a multitude of dynamic excitations during their lifespan. To account for their effects on the structural system, several load models are used during design to simulate the conditions the structure is likely to experience. These models are based on different simplifying assumptions and are generally guided by parameters that are stochastically identified from measurement data, making their outputs inherently uncertain. This paper presents a probabilistic physics-informed machine-learning framework based on Gaussian process regression for reconstructing dynamic forces based on measured deflections, velocities, or accelerations. The model can work with incomplete and contaminated data and offers a natural regularization approach to account for noise in the measurement system. An application of the developed framework is given by an aerodynamic analysis of the Great Belt East Bridge. The aerodynamic response is calculated numerically based on the quasi-steady model, and the underlying forces are reconstructed using sparse and noisy measurements. Results indicate a good agreement between the applied and the predicted dynamic load and can be extended to calculate global responses and the resulting internal forces. Uses of the developed framework include validation of design models and assumptions, as well as prognosis of responses to assist in damage detection and structural health monitoring.
</details>
<details>
<summary>摘要</summary>
长链桥受到多种动态冲击 durante 其服役寿命。为了考虑这些冲击对结构系统的影响，设计时使用多种荷载模型来模拟结构会经历的情况。这些模型基于不同的简化假设，通常受到测量数据中的参数随机 identificado 的指导。这篇文章介绍了一种基于 Gaussian process regression 的概率物理学 informed machine-learning 框架，可以根据测量到的弯曲、速度或加速度来重建动态力。该模型可以处理部分 incomplete 和污染的数据，并提供一种自然的常化方法来补做测量系统中的噪声。应用该开发的框架是大套东大桥的 aerodynamic 分析。通过 numerically 计算 quasi-steady 模型，并使用稀疏和噪声干扰的测量数据来重建动态荷载。结果表明与应用的动态荷载相比，预测的动态荷载具有良好的一致性。此外，该框架可以扩展到计算全局响应和结构内部力。用于 validate 设计模型和假设，以及诊断和结构健康监测。
</details></li>
</ul>
<hr>
<h2 id="Monte-Carlo-guided-Diffusion-for-Bayesian-linear-inverse-problems"><a href="#Monte-Carlo-guided-Diffusion-for-Bayesian-linear-inverse-problems" class="headerlink" title="Monte Carlo guided Diffusion for Bayesian linear inverse problems"></a>Monte Carlo guided Diffusion for Bayesian linear inverse problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07983">http://arxiv.org/abs/2308.07983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Cardoso, Yazid Janati El Idrissi, Sylvain Le Corff, Eric Moulines</li>
<li>for: 解决具有前向模型知识的不整合线性逆问题，从计算摄影到医学成像等多个应用领域。</li>
<li>methods: 使用得分生成模型（SGMs）生成有感知可能性的图像，尤其是填充问题。</li>
<li>results: 提出使用Sequential Monte Carlo方法解决Feynman–Kac模型的问题，并在实验中超越竞争对比方案。<details>
<summary>Abstract</summary>
Ill-posed linear inverse problems that combine knowledge of the forward measurement model with prior models arise frequently in various applications, from computational photography to medical imaging. Recent research has focused on solving these problems with score-based generative models (SGMs) that produce perceptually plausible images, especially in inpainting problems. In this study, we exploit the particular structure of the prior defined in the SGM to formulate recovery in a Bayesian framework as a Feynman--Kac model adapted from the forward diffusion model used to construct score-based diffusion. To solve this Feynman--Kac problem, we propose the use of Sequential Monte Carlo methods. The proposed algorithm, MCGdiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>计算摄影和医学成像等应用中频繁出现的不整合线性逆问题，现在通过使用分数基生成模型（SGM）解决这些问题，特别是填充问题。在这种研究中，我们利用SGM中的特殊结构来设置 bayesian 框架，并将其转化为凯克-菲涅曼模型，从而解决这个凯克-菲涅曼问题。为解决这个问题，我们提议使用顺序蒙те Carlo 方法。我们称之为 MCGdiff。我们证明了这种算法的理论基础，并通过数值实验表明，它在解决不整合逆问题时表现更好于竞争对手。
</details></li>
</ul>
<hr>
<h2 id="An-Adaptive-Approach-for-Probabilistic-Wind-Power-Forecasting-Based-on-Meta-Learning"><a href="#An-Adaptive-Approach-for-Probabilistic-Wind-Power-Forecasting-Based-on-Meta-Learning" class="headerlink" title="An Adaptive Approach for Probabilistic Wind Power Forecasting Based on Meta-Learning"></a>An Adaptive Approach for Probabilistic Wind Power Forecasting Based on Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07980">http://arxiv.org/abs/2308.07980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichao Meng, Ye Guo, Hongbin Sun</li>
<li>for: This paper proposes an adaptive approach for probabilistic wind power forecasting (WPF) that includes offline and online learning procedures.</li>
<li>methods: The proposed approach uses inner and outer loop updates of meta-learning to train a base forecast model, which is then applied to online forecasting combined with incremental learning techniques.</li>
<li>results: The proposed approach is validated through numerical tests on real-world wind power data sets, and the results show that it has advantages in adaptivity compared with existing alternatives.Here is the same information in Traditional Chinese:</li>
<li>for: 这个研究提出了一个适应方法 для probabilistic wind power forecasting (WPF)，包括了线上和线下学习过程。</li>
<li>methods: 这个方法使用了内部和外部循环更新的meta-learning来训练基础预测模型，然后将基础预测模型应用到线上预测，并与增量学习技术相结合。</li>
<li>results: 研究结果透过使用实际风力资料集进行数据分析，发现这个方法在适应性方面比现有的方法有优势。<details>
<summary>Abstract</summary>
This paper studies an adaptive approach for probabilistic wind power forecasting (WPF) including offline and online learning procedures. In the offline learning stage, a base forecast model is trained via inner and outer loop updates of meta-learning, which endows the base forecast model with excellent adaptability to different forecast tasks, i.e., probabilistic WPF with different lead times or locations. In the online learning stage, the base forecast model is applied to online forecasting combined with incremental learning techniques. On this basis, the online forecast takes full advantage of recent information and the adaptability of the base forecast model. Two applications are developed based on our proposed approach concerning forecasting with different lead times (temporal adaptation) and forecasting for newly established wind farms (spatial adaptation), respectively. Numerical tests were conducted on real-world wind power data sets. Simulation results validate the advantages in adaptivity of the proposed methods compared with existing alternatives.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MultiSChuBERT-Effective-Multimodal-Fusion-for-Scholarly-Document-Quality-Prediction"><a href="#MultiSChuBERT-Effective-Multimodal-Fusion-for-Scholarly-Document-Quality-Prediction" class="headerlink" title="MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction"></a>MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07971">http://arxiv.org/abs/2308.07971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gideon Maillette de Buy Wenniger, Thomas van Dongen, Lambert Schomaker</li>
<li>for: 本文旨在提高学术文献质量预测 task 的性能，特别是通过添加视觉信息来提高模型的准确率。</li>
<li>methods: 本文提出了一种多modal预测模型 MultiSChuBERT，它结合了基于块化全文文本的文本模型 SChuBERT，以及基于 Inception V3 的视觉模型。</li>
<li>results: 本文证明了以下三点：首先，将视觉和文本嵌入结合可以显著改善结果。其次，逐渐冻结视觉子模型的权重可以降低过拟合现象，提高结果。最后，使用不同的文本嵌入模型可以进一步提高结果。使用 BERT$_{\textrm{BASE}}$ 嵌入，在 ACL-BiblioMetry 数据集上的（对数）引用数预测任务中，MultiSChuBERT 模型的 $R^{2}$ 分数为 0.454，比 SChuBERT 模型（只使用文本）的 0.432 高。类似的改进也在 PeerRead 接受&#x2F;拒绝预测任务中获得。在使用 SciBERT、scincl、SPECTER 和 SPECTER2.0 嵌入时，我们发现每个这些适应嵌入都可以进一步提高模型的性能，SPECTER2.0 嵌入得最好。<details>
<summary>Abstract</summary>
Automatic assessment of the quality of scholarly documents is a difficult task with high potential impact. Multimodality, in particular the addition of visual information next to text, has been shown to improve the performance on scholarly document quality prediction (SDQP) tasks. We propose the multimodal predictive model MultiSChuBERT. It combines a textual model based on chunking full paper text and aggregating computed BERT chunk-encodings (SChuBERT), with a visual model based on Inception V3.Our work contributes to the current state-of-the-art in SDQP in three ways. First, we show that the method of combining visual and textual embeddings can substantially influence the results. Second, we demonstrate that gradual-unfreezing of the weights of the visual sub-model, reduces its tendency to ovefit the data, improving results. Third, we show the retained benefit of multimodality when replacing standard BERT$_{\textrm{BASE}}$ embeddings with more recent state-of-the-art text embedding models.   Using BERT$_{\textrm{BASE}}$ embeddings, on the (log) number of citations prediction task with the ACL-BiblioMetry dataset, our MultiSChuBERT (text+visual) model obtains an $R^{2}$ score of 0.454 compared to 0.432 for the SChuBERT (text only) model. Similar improvements are obtained on the PeerRead accept/reject prediction task. In our experiments using SciBERT, scincl, SPECTER and SPECTER2.0 embeddings, we show that each of these tailored embeddings adds further improvements over the standard BERT$_{\textrm{BASE}}$ embeddings, with the SPECTER2.0 embeddings performing best.
</details>
<details>
<summary>摘要</summary>
自动评估学术文献质量是一项具有高潜在影响力的任务。在特定的情况下，通过添加视觉信息与文本信息一起进行评估，可以提高学术文献质量预测（SDQP）任务的性能。我们提出了多模态预测模型MultiSChuBERT，它将文本模型基于分割全文本并聚合计算的BERT块编码（SChuBERT）与视觉模型基于Inception V3.0结合。我们的工作对现有状态的SDQP进行了贡献。首先，我们发现将视觉和文本嵌入结合的方法可以对结果产生显著影响。其次，我们示出了逐渐冰结视觉子模型的重量的方法可以降低它们的预测倾向，提高结果。最后，我们发现在使用更新的文本嵌入模型而不是标准BERT$_{\textrm{BASE}}$嵌入时，多模态性仍然保留着其优势。使用BERT$_{\textrm{BASE}}$嵌入，我们的MultiSChuBERT（文本+视觉）模型在ACL-BiblioMetry数据集上的（对数）引用数预测任务中，obtained an $R^{2}$ score of 0.454，比SChuBERT（文本只）模型的0.432高。类似的改进也在PeerRead Accept/Reject预测任务中被获得。在我们使用SciBERT、scincl、SPECTER和SPECTER2.0嵌入时，我们发现每个这些适应嵌入都可以进一步提高标准BERT$_{\textrm{BASE}}$嵌入的性能，SPECTER2.0嵌入表现最佳。
</details></li>
</ul>
<hr>
<h2 id="RAVEN-In-Context-Learning-with-Retrieval-Augmented-Encoder-Decoder-Language-Models"><a href="#RAVEN-In-Context-Learning-with-Retrieval-Augmented-Encoder-Decoder-Language-Models" class="headerlink" title="RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models"></a>RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07922">http://arxiv.org/abs/2308.07922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, Bryan Catanzaro</li>
<li>for:  investigate the in-context learning ability of retrieval-augmented encoder-decoder language models</li>
<li>methods:  combines retrieval-augmented masked language modeling and prefix language modeling, and introduces Fusion-in-Context Learning to enhance the few-shot performance</li>
<li>results:  significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters<details>
<summary>Abstract</summary>
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们 investigate retrieval-augmented encoder-decoder语言模型的在语言上下文中学习能力。我们首先对现有的ATLAS模型进行了全面的分析，并发现其在语言上下文中学习时存在一些限制，主要是因为预训练和测试集的匹配度不高，以及上下文长度的限制。为了解决这些问题，我们提议了RAVEN模型，该模型结合了检索支持的隐藏语言模型和前缀语言模型。我们还提出了Context-Aware Fusion Learning，以便通过在语言上下文中充分利用更多的示例来提高几个步骤性能，无需进行额外训练或模型修改。通过广泛的实验，我们证明了RAVEN模型可以在某些情况下明显超越ATLAS模型，并达到与最先进的语言模型相当的性能，即使RAVEN模型具有许多更少的参数。我们的工作论证了 retrieval-augmented encoder-decoder语言模型在语言上下文中学习的潜力，并鼓励进一步的研究在这个方向上。
</details></li>
</ul>
<hr>
<h2 id="The-Regular-Expression-Inference-Challenge"><a href="#The-Regular-Expression-Inference-Challenge" class="headerlink" title="The Regular Expression Inference Challenge"></a>The Regular Expression Inference Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07899">http://arxiv.org/abs/2308.07899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba Valizadeh, Philip John Gorinski, Ignacio Iacobacci, Martin Berger</li>
<li>for: 这paper是为了挑战code&#x2F;语言模型领域中的一个挑战任务，即常见表达式推理（REI）问题。</li>
<li>methods: 这paper使用了Program Synthesis技术和GPU来实现REI问题的解决方案。</li>
<li>results: 这paper首次生成了复杂REI实例的最小表达式，并发布了大规模的REI数据集。此外，paper还提出了一些初步的机器学习基线和优化策略。<details>
<summary>Abstract</summary>
We propose \emph{regular expression inference (REI)} as a challenge for code/language modelling, and the wider machine learning community. REI is a supervised machine learning (ML) and program synthesis task, and poses the problem of finding minimal regular expressions from examples: Given two finite sets of strings $P$ and $N$ and a cost function $\text{cost}(\cdot)$, the task is to generate an expression $r$ that accepts all strings in $P$ and rejects all strings in $N$, while no other such expression $r'$ exists with $\text{cost}(r')<\text{cost}(r)$.   REI has advantages as a challenge problem: (i) regular expressions are well-known, widely used, and a natural idealisation of code; (ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a small number of easy to understand parameters (e.g.~$P$ or $N$ cardinality, string lengths of examples, or the cost function); this lets us easily finetune REI-hardness; (iv) REI is an unsolved problem for deep learning based ML.   Recently, an REI solver was implemented on GPUs, using program synthesis techniques. This enabled, for the first time, fast generation of minimal expressions for complex REI instances. Building on this advance, we generate and publish the first large-scale datasets for REI, and devise and evaluate several initial heuristic and machine learning baselines.   We invite the community to participate and explore ML methods that learn to solve REI problems. We believe that progress in REI directly translates to code/language modelling.
</details>
<details>
<summary>摘要</summary>
我们提议“常量表示法（REI）”作为代码/语言模型领域的挑战，并且广泛的机器学习社群。 REI 是一个监督式机器学习（ML）和程式生成任务，需要寻找最小的常量表示法，以满足以下需求： giventwo个有限集合 $P$ 和 $N$，以及一个成本函数 $\text{cost}(\cdot)$，任务是生成一个表示法 $r$，使得 $r$ 接受所有 $P$ 中的字串，并且拒绝所有 $N$ 中的字串，而不是其他任何表示法 $r'$，使得 $\text{cost}(r') < \text{cost}(r)$。REI 有以下优点作为挑战问题：1. 常量表示法是广泛使用的、具有自然化的代码理想化；2. REI 的极限最坏情况复杂度很好地理解；3. REI 只有很少的容易理解的参数（例如 $P$ 或 $N$ 的卡дина特数、字串示例的长度、或成本函数），这让我们可以轻松地调整 REI 的困难度；4. REI 是深度学习基于 ML 的未解决问题。最近，一个 REI 解决方案在 GPU 上被实现，使用程式生成技术。这使得， для 首次可以快速生成复杂的 REI 问题中的最小表示法。基于这个进步，我们组建了第一个大规模的 REI 数据集，并设计了一些初步的变数和机器学习基线。我们邀请社区参与，探索 ML 方法，以解决 REI 问题。我们相信，REI 的进步将直接对代码/语言模型领域产生影响。
</details></li>
</ul>
<hr>
<h2 id="SciRE-Solver-Efficient-Sampling-of-Diffusion-Probabilistic-Models-by-Score-integrand-Solver-with-Recursive-Derivative-Estimation"><a href="#SciRE-Solver-Efficient-Sampling-of-Diffusion-Probabilistic-Models-by-Score-integrand-Solver-with-Recursive-Derivative-Estimation" class="headerlink" title="SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation"></a>SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07896">http://arxiv.org/abs/2308.07896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shigui Li, Wei Chen, Delu Zeng</li>
<li>for: 这篇论文是为了提出一种高效的推 diffusion probabilistic models（DPMs）采样算法。</li>
<li>methods: 该论文使用了一种新的score-based exact solution paradigm，以及一种 recursive derivative estimation（RDE）方法，以提高采样速度。</li>
<li>results: 该论文通过使用score-integrand solver with convergence order guarantee（SciRE-Solver），实现了高效的采样性能，并在CIFAR10和CelebA 64×64上达到了state-of-the-art（SOTA）水平。 Specifically, it achieved $3.48$ FID with $12$ NFE and $2.42$ FID with $20$ NFE for continuous-time DPMs on CIFAR10, and $2.17$ ($2.02$) FID with $18$ ($50$) NFE for discrete-time DPM on CelebA 64×64.<details>
<summary>Abstract</summary>
Diffusion probabilistic models (DPMs) are a powerful class of generative models known for their ability to generate high-fidelity image samples. A major challenge in the implementation of DPMs is the slow sampling process. In this work, we bring a high-efficiency sampler for DPMs. Specifically, we propose a score-based exact solution paradigm for the diffusion ODEs corresponding to the sampling process of DPMs, which introduces a new perspective on developing numerical algorithms for solving diffusion ODEs. To achieve an efficient sampler, we propose a recursive derivative estimation (RDE) method to reduce the estimation error. With our proposed solution paradigm and RDE method, we propose the score-integrand solver with the convergence order guarantee as efficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solver attains state-of-the-art (SOTA) sampling performance with a limited number of score function evaluations (NFE) on both discrete-time and continuous-time DPMs in comparison to existing training-free sampling algorithms. Such as, we achieve $3.48$ FID with $12$ NFE and $2.42$ FID with $20$ NFE for continuous-time DPMs on CIFAR10, respectively. Different from other samplers, SciRE-Solver has the promising potential to surpass the FIDs achieved in the original papers of some pre-trained models with a small NFEs. For example, we reach SOTA value of $2.40$ FID with $100$ NFE for continuous-time DPM and of $3.15$ FID with $84$ NFE for discrete-time DPM on CIFAR-10, as well as of $2.17$ ($2.02$) FID with $18$ ($50$) NFE for discrete-time DPM on CelebA 64$\times$64.
</details>
<details>
<summary>摘要</summary>
Diffusion probabilistic models (DPMs) 是一种强大的生成模型，能够生成高质量的图像样本。然而，在实现DPMs时，一个主要挑战是慢的采样过程。在这种情况下，我们提出了一种高效的采样方法。具体来说，我们提出了基于分布解决方法的高效采样方法，该方法可以减少采样过程中的估计误差。为了实现高效采样，我们提出了一种可重复的 derive 估计方法（RDE），该方法可以减少估计误差。通过我们的提出的解法和RDE方法，我们提出了一种高效的分布解决方法（SciRE-Solver），该方法可以高效地解决Diffusion ODEs。SciRE-Solver 可以在不同的时间分辨率下实现高效的采样，并且可以在有限的分布解决方法中实现高质量的采样。例如，我们在 CIFAR10 上实现了 $3.48$ FID 的值，只需要 $12$ NFE，并且在 continuous-time DPMs 上实现了 $2.42$ FID 的值，只需要 $20$ NFE。与其他采样器不同，SciRE-Solver 具有可能超越原始模型中的 FID 的潜在能力。例如，我们在 continuous-time DPMs 上实现了 $2.40$ FID 的值，只需要 $100$ NFE，并在 discrete-time DPMs 上实现了 $3.15$ FID 的值，只需要 $84$ NFE。此外，我们还在 CelebA 64$\times$64 上实现了 $2.17$ ($2.02$) FID 的值，只需要 $18$ ($50$) NFE。
</details></li>
</ul>
<hr>
<h2 id="On-regularized-Radon-Nikodym-differentiation"><a href="#On-regularized-Radon-Nikodym-differentiation" class="headerlink" title="On regularized Radon-Nikodym differentiation"></a>On regularized Radon-Nikodym differentiation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07887">http://arxiv.org/abs/2308.07887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duc Hoan Nguyen, Werner Zellinger, Sergei V. Pereverzyev</li>
<li>for: 解决Radon-NikodymDerivatives估计问题，这个问题在各种应用中出现，如covariate shift适应、likelihood-ratio测试、mutual information估计和conditional probability估计中。</li>
<li>methods: 使用通用正则化方案在 reproduce kernel Hilbert space中进行估计。</li>
<li>results:  estabilish了估计Derivatives的收敛率，并且可以在特定点进行高精度的重建。<details>
<summary>Abstract</summary>
We discuss the problem of estimating Radon-Nikodym derivatives. This problem appears in various applications, such as covariate shift adaptation, likelihood-ratio testing, mutual information estimation, and conditional probability estimation. To address the above problem, we employ the general regularization scheme in reproducing kernel Hilbert spaces. The convergence rate of the corresponding regularized algorithm is established by taking into account both the smoothness of the derivative and the capacity of the space in which it is estimated. This is done in terms of general source conditions and the regularized Christoffel functions. We also find that the reconstruction of Radon-Nikodym derivatives at any particular point can be done with high order of accuracy. Our theoretical results are illustrated by numerical simulations.
</details>
<details>
<summary>摘要</summary>
我们讨论类 Radon-Nikodym Derivative 的估计问题。这个问题在不同的应用中出现，例如对应拓扑变化、对应拓扑测试、共轨信息估计以及 conditional probability 估计。为了解决上述问题，我们使用通用的常数化方案在 reproduce kernel 空间中实现。我们证明了这个常数化算法的数据速度，通过考虑 derivative 的平滑性和估计空间的容量。此外，我们还发现了在特定点进行 Radon-Nikodym Derivative 的重建可以实现高精度。我们的理论成果通过数学模拟来描述。
</details></li>
</ul>
<hr>
<h2 id="Back-to-Basics-A-Sanity-Check-on-Modern-Time-Series-Classification-Algorithms"><a href="#Back-to-Basics-A-Sanity-Check-on-Modern-Time-Series-Classification-Algorithms" class="headerlink" title="Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms"></a>Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07886">http://arxiv.org/abs/2308.07886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlgig/tabularmodelsfortsc">https://github.com/mlgig/tabularmodelsfortsc</a></li>
<li>paper_authors: Bhaskar Dhariyal, Thach Le Nguyen, Georgiana Ifrim</li>
<li>for: 本研究旨在评估时序分类领域的基本参照模型，以及与现代时序分类算法相比较。</li>
<li>methods: 本研究使用了简单的表格模型（如ridge、LDA、RandomForest）和ROCKET家族的时序分类算法（如Rocket、MiniRocket、MultiRocket）进行比较。</li>
<li>results: 结果表明，表格模型在大约19%的单变量 dataset和28%的多变量 dataset上表现较为出色，并在大约50%的 dataset 上达到了准确率的10个百分点。这些结果表明，在开发时序分类算法时，需要考虑基本的表格模型作为参照。这些模型快速、简单、可能更容易理解和部署。<details>
<summary>Abstract</summary>
The state-of-the-art in time series classification has come a long way, from the 1NN-DTW algorithm to the ROCKET family of classifiers. However, in the current fast-paced development of new classifiers, taking a step back and performing simple baseline checks is essential. These checks are often overlooked, as researchers are focused on establishing new state-of-the-art results, developing scalable algorithms, and making models explainable. Nevertheless, there are many datasets that look like time series at first glance, but classic algorithms such as tabular methods with no time ordering may perform better on such problems. For example, for spectroscopy datasets, tabular methods tend to significantly outperform recent time series methods. In this study, we compare the performance of tabular models using classic machine learning approaches (e.g., Ridge, LDA, RandomForest) with the ROCKET family of classifiers (e.g., Rocket, MiniRocket, MultiRocket). Tabular models are simple and very efficient, while the ROCKET family of classifiers are more complex and have state-of-the-art accuracy and efficiency among recent time series classifiers. We find that tabular models outperform the ROCKET family of classifiers on approximately 19% of univariate and 28% of multivariate datasets in the UCR/UEA benchmark and achieve accuracy within 10 percentage points on about 50% of datasets. Our results suggest that it is important to consider simple tabular models as baselines when developing time series classifiers. These models are very fast, can be as effective as more complex methods and may be easier to understand and deploy.
</details>
<details>
<summary>摘要</summary>
现代时序分类技术已经发展到了非常高水平，从1NN-DTW算法到ROCKET家族的分类器。然而，在当前的快速发展新的分类器，回退并执行简单的基准检查是必要的。这些检查经常被忽略，因为研究人员正在寻求新的state-of-the-art结果，开发可扩展的算法，并使模型更加可解释。然而，有很多数据集看起来像时序数据，但经典算法如表格方法无时间顺序可能在这些问题上表现更好。例如，对于光谱数据集，表格方法通常在近期时间系列方法之上表现出色。在这种研究中，我们比较了使用经典机器学习方法（例如ridge、LDA、RandomForest）的表格模型与ROCKET家族的分类器（例如Rocket、MiniRocket、MultiRocket）的性能。表格模型简单而高效，而ROCKET家族的分类器更加复杂，在最近的时序分类器中具有state-of-the-art的准确率和效率。我们发现，在UCRLUEA标准测试集上，表格模型比ROCKET家族的分类器在约19%的单variate数据集和28%的多variate数据集上表现出色，并在约50%的数据集上达到了准确率在10个百分点之间。我们的结果表明，在开发时序分类器时，应该考虑使用简单的表格模型作为基准。这些模型很快速，可以与更复杂的方法相比，并且可能更易于理解和部署。
</details></li>
</ul>
<hr>
<h2 id="The-Challenge-of-Fetal-Cardiac-MRI-Reconstruction-Using-Deep-Learning"><a href="#The-Challenge-of-Fetal-Cardiac-MRI-Reconstruction-Using-Deep-Learning" class="headerlink" title="The Challenge of Fetal Cardiac MRI Reconstruction Using Deep Learning"></a>The Challenge of Fetal Cardiac MRI Reconstruction Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07885">http://arxiv.org/abs/2308.07885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Prokopenko, Kerstin Hammernik, Thomas Roberts, David F A Lloyd, Daniel Rueckert, Joseph V Hajnal<br>for:* The paper aims to improve the quality of non-gated kt-SENSE reconstruction for dynamic free-breathing fetal cardiac MRI using deep learning methods.methods:* The authors use supervised deep learning networks to recover fully-sampled data from undersampled data, and explore various model architectures and training strategies.results:* The best-performing models recover a detailed depiction of the maternal anatomy but underestimate the dynamic properties of the fetal heart, suggesting the need for more targeted training and evaluation methods.<details>
<summary>Abstract</summary>
Dynamic free-breathing fetal cardiac MRI is one of the most challenging modalities, which requires high temporal and spatial resolution to depict rapid changes in a small fetal heart. The ability of deep learning methods to recover undersampled data could help to optimise the kt-SENSE acquisition strategy and improve non-gated kt-SENSE reconstruction quality. In this work, we explore supervised deep learning networks for reconstruction of kt-SENSE style acquired data using an extensive in vivo dataset. Having access to fully-sampled low-resolution multi-coil fetal cardiac MRI, we study the performance of the networks to recover fully-sampled data from undersampled data. We consider model architectures together with training strategies taking into account their application in the real clinical setup used to collect the dataset to enable networks to recover prospectively undersampled data. We explore a set of modifications to form a baseline performance evaluation for dynamic fetal cardiac MRI on real data. We systematically evaluate the models on coil-combined data to reveal the effect of the suggested changes to the architecture in the context of fetal heart properties. We show that the best-performers recover a detailed depiction of the maternal anatomy on a large scale, but the dynamic properties of the fetal heart are under-represented. Training directly on multi-coil data improves the performance of the models, allows their prospective application to undersampled data and makes them outperform CTFNet introduced for adult cardiac cine MRI. However, these models deliver similar qualitative performances recovering the maternal body very well but underestimating the dynamic properties of fetal heart. This dynamic feature of fast change of fetal heart that is highly localised suggests both more targeted training and evaluation methods might be needed for fetal heart application.
</details>
<details>
<summary>摘要</summary>
“动态自由呼吸幼儿心脏MRI是最复杂的modalities，需要高度的时间和空间分辨率来描述幼儿心脏的快速变化。深度学习方法可以回归不完全探测的数据，可以帮助优化kt-SENSE数据获取策略，提高非阻塞kt-SENSE重建质量。在这种工作中，我们使用了supervised深度学习网络来重建kt-SENSE风格获取的数据，使用了大量的生物实验室数据。由于我们拥有完整的低分辨率多极心脏MRI数据，我们研究了网络可以从不完整的数据中恢复完整的数据的性能。我们考虑了模型架构和训练策略，以便在实际临床设置中收集数据时使用。我们系统地评估了模型在实际数据上的性能，并对幼儿心脏属性进行了修改。我们发现最佳performer可以呈现出详细的 maternal anatomy，但是幼儿心脏的动态特性受到了下降。通过直接训练多极数据，我们可以使模型预测不完整的数据，并且其性能高于CTFNet。但是，这些模型在恢复 maternal body 方面表现良好，而幼儿心脏的动态特性方面表现较差。这种快速变化的幼儿心脏特性表示需要更加targeted的训练和评估方法。”
</details></li>
</ul>
<hr>
<h2 id="A-Trustable-LSTM-Autoencoder-Network-for-Cyberbullying-Detection-on-Social-Media-Using-Synthetic-Data"><a href="#A-Trustable-LSTM-Autoencoder-Network-for-Cyberbullying-Detection-on-Social-Media-Using-Synthetic-Data" class="headerlink" title="A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data"></a>A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09722">http://arxiv.org/abs/2308.09722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mst Shapna Akter, Hossain Shahriar, Alfredo Cuzzocrea<br>for: This paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection on social media using synthetic data.methods: The proposed model uses a combination of Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models.results: The proposed model outperformed all the models on all datasets, achieving the highest accuracy of 95%.<details>
<summary>Abstract</summary>
Social media cyberbullying has a detrimental effect on human life. As online social networking grows daily, the amount of hate speech also increases. Such terrible content can cause depression and actions related to suicide. This paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection on social media using synthetic data. We have demonstrated a cutting-edge method to address data availability difficulties by producing machine-translated data. However, several languages such as Hindi and Bangla still lack adequate investigations due to a lack of datasets. We carried out experimental identification of aggressive comments on Hindi, Bangla, and English datasets using the proposed model and traditional models, including Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models. We employed evaluation metrics such as f1-score, accuracy, precision, and recall to assess the models performance. Our proposed model outperformed all the models on all datasets, achieving the highest accuracy of 95%. Our model achieves state-of-the-art results among all the previous works on the dataset we used in this paper.
</details>
<details>
<summary>摘要</summary>
社交媒体恐吓行为对人类生活产生负面影响。随着在线社交网络日益增长，讨厌言语也在不断增加。这种厉害的内容可能导致抑郁和自杀行为。本文提议一种可靠的LSTM-Autoencoder网络，用于社交媒体上的恐吓行为检测。我们通过生成机器翻译数据来解决数据可用性问题。然而，一些语言，如希ن第和孟加拉语，仍然缺乏足够的调查，因为数据不足。我们使用提议模型和传统模型，包括LSTM、BiLSTM、LSTM-Autoencoder、Word2vec、BERT和GPT-2模型，进行实验 indentification of aggressive comments。我们使用f1-score、准确率、精度和回归来评估模型的表现。我们的提议模型在所有数据集上都表现出优于所有其他模型，具有最高准确率95%。我们的模型在所有前一个工作中达到了状态 искусственный智能的最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Towards-Temporal-Edge-Regression-A-Case-Study-on-Agriculture-Trade-Between-Nations"><a href="#Towards-Temporal-Edge-Regression-A-Case-Study-on-Agriculture-Trade-Between-Nations" class="headerlink" title="Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations"></a>Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07883">http://arxiv.org/abs/2308.07883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scylj1/gnn_edge_regression">https://github.com/scylj1/gnn_edge_regression</a></li>
<li>paper_authors: Lekang Jiang, Caiqi Zhang, Farimah Poursafaei, Shenyang Huang</li>
<li>for: 预测国际贸易数据中的边值（edge regression）任务，特别是在静态和动态图中。</li>
<li>methods: 使用图神经网络（Graph Neural Networks，GNNs）模型进行预测，并 introduce three simple yet strong baselines。</li>
<li>results: 实验结果显示baselines在不同设置下表现极其出色，而TGN模型在edge regression任务中表现更好，并且发现训练样本中负边的比例对测试性能产生了重要的影响。<details>
<summary>Abstract</summary>
Recently, Graph Neural Networks (GNNs) have shown promising performance in tasks on dynamic graphs such as node classification, link prediction and graph regression. However, few work has studied the temporal edge regression task which has important real-world applications. In this paper, we explore the application of GNNs to edge regression tasks in both static and dynamic settings, focusing on predicting food and agriculture trade values between nations. We introduce three simple yet strong baselines and comprehensively evaluate one static and three dynamic GNN models using the UN Trade dataset. Our experimental results reveal that the baselines exhibit remarkably strong performance across various settings, highlighting the inadequacy of existing GNNs. We also find that TGN outperforms other GNN models, suggesting TGN is a more appropriate choice for edge regression tasks. Moreover, we note that the proportion of negative edges in the training samples significantly affects the test performance. The companion source code can be found at: https://github.com/scylj1/GNN_Edge_Regression.
</details>
<details>
<summary>摘要</summary>
最近，图 neck Networks (GNNs) 在动态图上的任务中表现出色，包括节点分类、链接预测和图回归。然而，对于时间Edge regression任务，有很少的研究。在这篇论文中，我们探索了 GNNs 在静态和动态设置下的边 regression 任务，特点是预测国家之间的食品和农业贸易值。我们提出了三种简单又强大的基线，并对一个静态和三个动态 GNN 模型进行了广泛的测试，使用 UN Trade 数据集。我们的实验结果表明，基elines 在不同设置下具有极强表现，这 highlights 现有 GNNs 的不足。此外，我们发现 TGN 在边 regression 任务中表现出色， suggesting TGN 是更适合的选择。同时，我们注意到训练样本中负边的比例对测试性能产生了显著影响。相关的源代码可以在 GitHub 上找到：https://github.com/scylj1/GNN_Edge_Regression。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Political-Zero-Shot-Relation-Classification-via-Codebook-Knowledge-NLI-and-ChatGPT"><a href="#Synthesizing-Political-Zero-Shot-Relation-Classification-via-Codebook-Knowledge-NLI-and-ChatGPT" class="headerlink" title="Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT"></a>Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07876">http://arxiv.org/abs/2308.07876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snowood1/zero-shot-plover">https://github.com/snowood1/zero-shot-plover</a></li>
<li>paper_authors: Yibo Hu, Erick Skorupa Parolin, Latifur Khan, Patrick T. Brandt, Javier Osorio, Vito J. D’Orazio</li>
<li>for: 这 paper 的目的是提高政治事件代码分类的精度和效率，并利用现有的专家数据库中的知识来避免新的注释创建。</li>
<li>methods: 这 paper 使用了 Zero-shot 方法和自然语言推理（NLI）方法，其中 ZSP 方法采用了树查询框架来解决分类任务中的上下文、模式和类别划分等问题。</li>
<li>results: 经过广泛的实验研究，ZSP 方法在我们新建的数据集上达到了40%的 F1 分数提升，与超级vised BERT 模型的性能相当。这表明 ZSP 方法可以作为政治事件记录验证和 ontology 发展的有价值工具。<details>
<summary>Abstract</summary>
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classification. ZSP demonstrates competitive performance compared to supervised BERT models, positioning it as a valuable tool for event record validation and ontology development. Our work underscores the potential of leveraging transfer learning and existing expertise to enhance the efficiency and scalability of research in the field.
</details>
<details>
<summary>摘要</summary>
最近的监督模型对事件编码表现出色，但它们完全依赖于新的注释，忽略了专家数据库中的庞大知识，这限制了它们的应用范围。为了解决这些局限性，我们explore零批处理方法 для政治事件 ontology 关系分类，利用专家注释代码库中的知识。我们的研究包括ChatGPT和一种基于自然语言推理（NLI）的新方法 named ZSP。ZSP采用树查询框架，将任务分解成上下文、模式和分类层次。这种框架提高了可读性、效率和 schema 变化的适应能力。通过对我们新划分的数据集进行广泛的实验，我们揭示了 ChatGPT 中的不稳定性问题，并 highlight了 ZSP 的显著性能优势。ZSP 在细化的 Rootcode 分类任务中实现了40%的提升。ZSP 与超级vised BERT模型的性能相当，这positioned它作为事件记录验证和 ontology 发展的有价值工具。我们的工作强调了利用传输学习和现有专业知识来提高研究领域的效率和扩展性。
</details></li>
</ul>
<hr>
<h2 id="Emotion-Embeddings-unicode-x2014-Learning-Stable-and-Homogeneous-Abstractions-from-Heterogeneous-Affective-Datasets"><a href="#Emotion-Embeddings-unicode-x2014-Learning-Stable-and-Homogeneous-Abstractions-from-Heterogeneous-Affective-Datasets" class="headerlink" title="Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets"></a>Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07871">http://arxiv.org/abs/2308.07871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sven Buechel, Udo Hahn<br>for:这篇论文的目的是提出一种可以独立地学习和拟合不同自然语言、交流Modalities、媒体和表示标签格式的计算模型，以实现情感分析领域的共享表示和可重用性。methods:该论文使用了一种训练程序，通过学习共享的 latent 表示来捕捉情感的多样性，无论是在不同的自然语言、交流Modalities、媒体或表示标签格式中。results:实验表明，该方法可以在各种不同的情感数据集上实现预测质量的稳定和可重用性，而无需拘束到特定的语言、Modalities、媒体或表示标签格式。Code和数据都已经被存储在 <a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.7405327">https://doi.org/10.5281/zenodo.7405327</a> 上。<details>
<summary>Abstract</summary>
Human emotion is expressed in many communication modalities and media formats and so their computational study is equally diversified into natural language processing, audio signal analysis, computer vision, etc. Similarly, the large variety of representation formats used in previous research to describe emotions (polarity scales, basic emotion categories, dimensional approaches, appraisal theory, etc.) have led to an ever proliferating diversity of datasets, predictive models, and software tools for emotion analysis. Because of these two distinct types of heterogeneity, at the expressional and representational level, there is a dire need to unify previous work on increasingly diverging data and label types. This article presents such a unifying computational model. We propose a training procedure that learns a shared latent representation for emotions, so-called emotion embeddings, independent of different natural languages, communication modalities, media or representation label formats, and even disparate model architectures. Experiments on a wide range of heterogeneous affective datasets indicate that this approach yields the desired interoperability for the sake of reusability, interpretability and flexibility, without penalizing prediction quality. Code and data are archived under https://doi.org/10.5281/zenodo.7405327 .
</details>
<details>
<summary>摘要</summary>
人类情感表达在多种通信modalities和媒体格式中表现出来，因此计算研究也是多样化的，包括自然语言处理、音频信号分析、计算机视觉等。在过去的研究中，用于描述情感的多种格式（如偏好级别、基本情绪类别、维度方法、评估理论等）导致了对情感分析的数据和预测模型的总体化，以至于现在的数据和标签类型在不断演化。为了解决这两种不同的多样性，我们提出一种统一的计算模型。我们提议一种培训过程，通过学习情感的共享幂等 représentation来独立于不同的自然语言、通信modalities、媒体或表达标签格式，甚至不同的模型架构。实验表明，这种方法可以实现数据和标签类型之间的可 reuse、可读性和灵活性，无需增加预测质量的减少。代码和数据可以在https://doi.org/10.5281/zenodo.7405327中找到。
</details></li>
</ul>
<hr>
<h2 id="Brain-Inspired-Computational-Intelligence-via-Predictive-Coding"><a href="#Brain-Inspired-Computational-Intelligence-via-Predictive-Coding" class="headerlink" title="Brain-Inspired Computational Intelligence via Predictive Coding"></a>Brain-Inspired Computational Intelligence via Predictive Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07870">http://arxiv.org/abs/2308.07870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Salvatori, Ankur Mali, Christopher L. Buckley, Thomas Lukasiewicz, Rajesh P. N. Rao, Karl Friston, Alexander Ororbia</li>
<li>for: 这篇论文旨在探讨使用Predictive Coding（PC）理论来解决深度神经网络的限制，以提高机器学习的效果。</li>
<li>methods: 本论文使用Literature survey方法，浏览了 relate to PC 的文献，并 highlighted its potential applications in machine learning and computational intelligence。</li>
<li>results: 论文表明，PC 可以用于模型脑内信息处理，可以应用于认知控制和机器人学习，并具有强大的数学基础，可以用于特定类型的连续状态生成模型的倒逼算法。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a powerful inversion scheme for a specific class of continuous-state generative models. With the hope of foregrounding research in this direction, we survey the literature that has contributed to this perspective, highlighting the many ways that PC might play a role in the future of machine learning and computational intelligence at large.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在这个世纪快速成为一个关键技术。目前大多数AI成果都是使用深度神经网络和错误反射学习算法获得的。然而，这种广泛采用的方法存在一些重要的限制，如计算成本很高、难以量化不确定性、缺乏可靠性和生物可能性。可能需要采用基于神经科学理论的方案来解决这些限制。一种如此理论是预测编码（PC），它在机器智能任务中表现出了惊喜性，并且具有可能为机器学习社区提供价值的特性：PC可以模型不同脑区的信息处理方式，可以应用于认知控制和机器人学习，并且具有强制VARIATIONAL推理的数学基础，可以为某些连续状态生成模型提供强大的逆转计划。希望通过这篇文章，推动研究人员对这个视角的研究，并强调PC在未来机器学习和计算智能中的潜在作用。
</details></li>
</ul>
<hr>
<h2 id="Graph-Structured-Kernel-Design-for-Power-Flow-Learning-using-Gaussian-Processes"><a href="#Graph-Structured-Kernel-Design-for-Power-Flow-Learning-using-Gaussian-Processes" class="headerlink" title="Graph-Structured Kernel Design for Power Flow Learning using Gaussian Processes"></a>Graph-Structured Kernel Design for Power Flow Learning using Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07867">http://arxiv.org/abs/2308.07867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parikshit Pareek, Deepjyoti Deka, Sidhant Misra</li>
<li>for: 这种physics-inspired graph-structured kernel是用于电流流行学习的Gaussian Process（GP）中的一种kernel，旨在利用网络图或结构来抽象Voltage-injection关系的latent decomposition。</li>
<li>methods: 这种kernel被称为 vertex-degree kernel（VDK），它不需要解决优化问题来搜索kernel。此外，我们还提出了一种图缩 Representation with fewer terms，以提高效率。</li>
<li>results: 对于500-Bus和1354-Bus电力系统，我们的VDK-GP方法可以在样本复杂性方面减少超过两倍，相比于全GP。此外，我们的网络滑块活动学习算法可以在测试预测中超过mean Performance of 500 Random Trials by two fold for medium-sized 500-Bus systems and best performance of 25 random trials for large-scale 1354-Bus systems by 10%.<details>
<summary>Abstract</summary>
This paper presents a physics-inspired graph-structured kernel designed for power flow learning using Gaussian Process (GP). The kernel, named the vertex-degree kernel (VDK), relies on latent decomposition of voltage-injection relationship based on the network graph or topology. Notably, VDK design avoids the need to solve optimization problems for kernel search. To enhance efficiency, we also explore a graph-reduction approach to obtain a VDK representation with lesser terms. Additionally, we propose a novel network-swipe active learning scheme, which intelligently selects sequential training inputs to accelerate the learning of VDK. Leveraging the additive structure of VDK, the active learning algorithm performs a block-descent type procedure on GP's predictive variance, serving as a proxy for information gain. Simulations demonstrate that the proposed VDK-GP achieves more than two fold sample complexity reduction, compared to full GP on medium scale 500-Bus and large scale 1354-Bus power systems. The network-swipe algorithm outperforms mean performance of 500 random trials on test predictions by two fold for medium-sized 500-Bus systems and best performance of 25 random trials for large-scale 1354-Bus systems by 10%. Moreover, we demonstrate that the proposed method's performance for uncertainty quantification applications with distributionally shifted testing data sets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Impression-Aware-Recommender-Systems"><a href="#Impression-Aware-Recommender-Systems" class="headerlink" title="Impression-Aware Recommender Systems"></a>Impression-Aware Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07857">http://arxiv.org/abs/2308.07857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando B. Pérez Maurera, Maurizio Ferrari Dacrema, Pablo Castells, Paolo Cremonesi</li>
<li>for: 本研究旨在探讨基于印象（过去推荐的项目）的 recommender system 的开发和应用，以提高推荐系统的质量。</li>
<li>methods: 本文使用系统性文献综述方法，对各种推荐系统使用印象的研究进行分类和详细介绍，同时还总结了不同的数据集和评价方法。</li>
<li>results: 本文通过对各种推荐系统使用印象的研究进行分类和详细介绍，掌握了各种研究的方法和结论，并发现了一些未在文献中提到的问题和未来研究方向。<details>
<summary>Abstract</summary>
Novel data sources bring new opportunities to improve the quality of recommender systems. Impressions are a novel data source containing past recommendations (shown items) and traditional interactions. Researchers may use impressions to refine user preferences and overcome the current limitations in recommender systems research. The relevance and interest of impressions have increased over the years; hence, the need for a review of relevant work on this type of recommenders. We present a systematic literature review on recommender systems using impressions, focusing on three fundamental angles in research: recommenders, datasets, and evaluation methodologies. We provide three categorizations of papers describing recommenders using impressions, present each reviewed paper in detail, describe datasets with impressions, and analyze the existing evaluation methodologies. Lastly, we present open questions and future directions of interest, highlighting aspects missing in the literature that can be addressed in future works.
</details>
<details>
<summary>摘要</summary>
新的数据源带来了改善推荐系统质量的新机会。印象是一种新的数据源，包含过去的推荐（显示的项目）和传统的交互。研究人员可以使用印象来细化用户的偏好，超越当前推荐系统研究的限制。随着年代的推移，印象的相关性和兴趣度也在增长，因此需要对这类推荐系统的研究进行系统atic literature review。本文对推荐系统使用印象进行系统atic literature review，将研究分为三个基本的视角：推荐器、数据集和评估方法ологи。我们对每篇评论细节描述、介绍数据集，并分析现有的评估方法ологи。最后，我们提出了未解决的问题和未来方向，强调文献中缺失的方面，可以在未来的研究中解决。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/cs.LG_2023_08_16/" data-id="cllurrpab0062sw88566tftw6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/cs.SD_2023_08_16/" class="article-date">
  <time datetime="2023-08-15T16:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/16/cs.SD_2023_08_16/">cs.SD - 2023-08-16 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mitigating-the-Exposure-Bias-in-Sentence-Level-Grapheme-to-Phoneme-G2P-Transduction"><a href="#Mitigating-the-Exposure-Bias-in-Sentence-Level-Grapheme-to-Phoneme-G2P-Transduction" class="headerlink" title="Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction"></a>Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08442">http://arxiv.org/abs/2308.08442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunseop Yoon, Hee Suk Yoon, Dhananjaya Gowda, SooHwan Eom, Daehyeok Kim, John Harvill, Heting Gao, Mark Hasegawa-Johnson, Chanwoo Kim, Chang D. Yoo</li>
<li>for: 这个论文是为了提高文本中字符串到音节转换（G2P）的性能而写的。</li>
<li>methods: 这个论文使用了Tokenizer-free byte-level模型（ByT5），通过表示每个输入字符的UTF-8编码来实现字符串到音节转换。</li>
<li>results: 这个论文发现，使用ByT5进行句子级或段落级G2P可以提高实际应用中的用户体验，但是需要避免曝光偏见常见在自动生成模型中。<details>
<summary>Abstract</summary>
Text-to-Text Transfer Transformer (T5) has recently been considered for the Grapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free byte-level model based on T5 referred to as ByT5, recently gave promising results on word-level G2P conversion by representing each input character with its corresponding UTF-8 encoding. Although it is generally understood that sentence-level or paragraph-level G2P can improve usability in real-world applications as it is better suited to perform on heteronyms and linking sounds between words, we find that using ByT5 for these scenarios is nontrivial. Since ByT5 operates on the character level, it requires longer decoding steps, which deteriorates the performance due to the exposure bias commonly observed in auto-regressive generation models. This paper shows that the performance of sentence-level and paragraph-level G2P can be improved by mitigating such exposure bias using our proposed loss-based sampling method.
</details>
<details>
<summary>摘要</summary>
文本-to-文本传输变换器（T5）最近被考虑用于文本-to-phoneme（G2P）转换。作为继续，一个不需要tokenizer的字节级模型基于T5，称之为ByT5，最近在word-level G2P转换中表现出了扎实的结果。虽然普遍认为 sentence-level或paragraph-level G2P可以提高实际应用中的可用性，因为更适合处理Homonyms和 слова间的连接音，但我们发现使用ByT5进行这些场景是非常困难。因为ByT5操作在字符水平，需要更长的解码步骤，这会导致性能下降，这是因为自动生成模型通常会出现露示偏见。本文显示，使用我们提议的损失采样方法可以提高 sentence-level和paragraph-level G2P的性能。
</details></li>
</ul>
<hr>
<h2 id="Classifying-Dementia-in-the-Presence-of-Depression-A-Cross-Corpus-Study"><a href="#Classifying-Dementia-in-the-Presence-of-Depression-A-Cross-Corpus-Study" class="headerlink" title="Classifying Dementia in the Presence of Depression: A Cross-Corpus Study"></a>Classifying Dementia in the Presence of Depression: A Cross-Corpus Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08306">http://arxiv.org/abs/2308.08306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franziska Braun, Sebastian P. Bayerl, Paula A. Pérez-Toro, Florian Hönig, Hartmut Lehfeld, Thomas Hillemacher, Elmar Nöth, Tobias Bocklet, Korbinian Riedhammer</li>
<li>for: 这个论文的目的是提出一种自动诊断老年痴呆症的方法，以提高医疗系统的效率和患者的生活质量。</li>
<li>methods: 这个论文使用了语音、文本和情感嵌入来分类诊断老年痴呆症，并在三个类别中进行了比较（健康人 VS 轻度智能障碍 VS 老年痴呆症）。</li>
<li>results: 研究人员通过对两个独立录制的德国数据集进行交叉验证和混合验证，发现了这种方法的普适性和可重复性。<details>
<summary>Abstract</summary>
Automated dementia screening enables early detection and intervention, reducing costs to healthcare systems and increasing quality of life for those affected. Depression has shared symptoms with dementia, adding complexity to diagnoses. The research focus so far has been on binary classification of dementia (DEM) and healthy controls (HC) using speech from picture description tests from a single dataset. In this work, we apply established baseline systems to discriminate cognitive impairment in speech from the semantic Verbal Fluency Test and the Boston Naming Test using text, audio and emotion embeddings in a 3-class classification problem (HC vs. MCI vs. DEM). We perform cross-corpus and mixed-corpus experiments on two independently recorded German datasets to investigate generalization to larger populations and different recording conditions. In a detailed error analysis, we look at depression as a secondary diagnosis to understand what our classifiers actually learn.
</details>
<details>
<summary>摘要</summary>
自动化认知评估可以早期检测和 intervene，降低医疗系统的成本和提高认知症患者的生活质量。与认知症有共同症状的抑郁症可以增加诊断的复杂性。过去的研究主要集中在使用单一数据集的语音描述测验进行二分类认知症和健康控制（HC）的分类。在这个工作中，我们使用已经建立的基eline系统来分辨语音中的认知障碍，使用 semantic Verbal Fluency Test 和 Boston Naming Test 的文本、音频和情感嵌入，进行三类分类问题（HC vs. MCI vs. DEM）。我们在两个独立录取的德国数据集上进行交叉数据和混合数据实验，以调查更大的人口和不同的录音条件下的一致性。在详细的错误分析中，我们将抑郁症作为次要诊断来了解我们的分类器是否真的学习了什么。
</details></li>
</ul>
<hr>
<h2 id="ChinaTelecom-System-Description-to-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#ChinaTelecom-System-Description-to-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023"></a>ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08181">http://arxiv.org/abs/2308.08181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengjie Du, Xiang Fang, Jie Li</li>
<li>for: This paper is written for the VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023) and describes the ChinaTelecom system for Track 1 (closed).</li>
<li>methods: The system consists of several ResNet variants trained only on VoxCeleb2, which were fused for better performance later. Score calibration was also applied for each variant and the fused system.</li>
<li>results: The final submission achieved minDCF of 0.1066 and EER of 1.980%.Here are the three key points in Simplified Chinese:</li>
<li>for: 这篇论文是为了VOXCELEB2023发音识别挑战（VOXSRC2023）的Track 1（关闭）而写的。</li>
<li>methods: 该系统包括几种基于VOXCELEB2的ResNet变体，这些变体被后续进行了融合以提高表现。此外，每个变体和融合系统还进行了分数调整。</li>
<li>results: 最终提交的结果为minDCF为0.1066和EER为1.980%。<details>
<summary>Abstract</summary>
This technical report describes ChinaTelecom system for Track 1 (closed) of the VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023). Our system consists of several ResNet variants trained only on VoxCeleb2, which were fused for better performance later. Score calibration was also applied for each variant and the fused system. The final submission achieved minDCF of 0.1066 and EER of 1.980%.
</details>
<details>
<summary>摘要</summary>
这份技术报告介绍了我们在VoxCeleb2023 Speaker Recognition Challenge（VoxSRC 2023）的Track 1（关闭）系统。我们的系统包括了多种ResNet变体，只在VoxCeleb2上进行训练。这些变体后来进行了融合，以提高性能。此外，我们还应用了分数均衡calibration对每个变体和融合系统。最终的提交达到了0.1066的minDCF和1.980%的EER。
</details></li>
</ul>
<hr>
<h2 id="AffectEcho-Speaker-Independent-and-Language-Agnostic-Emotion-and-Affect-Transfer-for-Speech-Synthesis"><a href="#AffectEcho-Speaker-Independent-and-Language-Agnostic-Emotion-and-Affect-Transfer-for-Speech-Synthesis" class="headerlink" title="AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis"></a>AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08577">http://arxiv.org/abs/2308.08577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishikesh Viswanath, Aneesh Bhattacharya, Pascal Jutras-Dubé, Prerit Gupta, Mridu Prashanth, Yashvardhan Khaitan, Aniket Bera</li>
<li>for: The paper is written for discussing a new approach to emotional translation in text-to-speech and speech-to-speech systems, with the goal of capturing complex nuances and subtle differences in emotions.</li>
<li>methods: The proposed approach, called AffectEcho, uses a Vector Quantized codebook to model emotions within a quantized space featuring five levels of affect intensity. The quantized emotional embeddings are implicitly derived from spoken speech samples, eliminating the need for one-hot vectors or explicit strength embeddings.</li>
<li>results: The experimental results demonstrate the effectiveness of the proposed approach in controlling the emotions of generated speech while preserving identity, style, and emotional cadence unique to each speaker. The language-independent emotion modeling capability of the quantized emotional embeddings learned from a bilingual (English and Chinese) speech corpus is also shown, with an emotion transfer task from a reference speech to a target speech achieving state-of-art results on both qualitative and quantitative metrics.Here are the three points in Simplified Chinese text:</li>
<li>for: 本文是为了介绍一种新的情感翻译方法，用于文本到语音和语音到语音系统中，以便捕捉复杂的情感细微差异。</li>
<li>methods: 提议的方法是使用Vector Quantized codebook来模型情感，在一个5级情感强度的量化空间中进行模型。这些量化情感嵌入不需要一个热度 вектор或者显式强度嵌入。</li>
<li>results: 实验结果表明，提议的方法能够控制生成的语音中的情感，同时保持每个 speaker 的个性、风格和情感节奏。此外，通过一种语言无关的情感模型，在一个英文和中文双语Speech corpus中学习的情感嵌入可以在另一种语言中进行情感传递任务，并达到了当前最佳的质量和量化指标。<details>
<summary>Abstract</summary>
Affect is an emotional characteristic encompassing valence, arousal, and intensity, and is a crucial attribute for enabling authentic conversations. While existing text-to-speech (TTS) and speech-to-speech systems rely on strength embedding vectors and global style tokens to capture emotions, these models represent emotions as a component of style or represent them in discrete categories. We propose AffectEcho, an emotion translation model, that uses a Vector Quantized codebook to model emotions within a quantized space featuring five levels of affect intensity to capture complex nuances and subtle differences in the same emotion. The quantized emotional embeddings are implicitly derived from spoken speech samples, eliminating the need for one-hot vectors or explicit strength embeddings. Experimental results demonstrate the effectiveness of our approach in controlling the emotions of generated speech while preserving identity, style, and emotional cadence unique to each speaker. We showcase the language-independent emotion modeling capability of the quantized emotional embeddings learned from a bilingual (English and Chinese) speech corpus with an emotion transfer task from a reference speech to a target speech. We achieve state-of-art results on both qualitative and quantitative metrics.
</details>
<details>
<summary>摘要</summary>
“情感”是一种情感特征，包括价值观、情感刺激和强度，这种特征对实际对话的进行是关键。现有的文本到语音（TTS）和语音到语音系统通常使用强度嵌入向量和全局风格token来捕捉情感，但这些模型表示情感为样式的一部分或以分类的方式表示。我们提议的情感回声模型（AffectEcho）使用量化编码 кни簿来模型情感在量化空间中的五级强度，以捕捉复杂的细节和同一种情感中的微妙差异。这些量化情感嵌入不需要一个一热 вектор或显式强度嵌入。我们的方法可以控制生成的语音中的情感，保留每个说话者的个性、风格和情感节奏。我们在一个英文和中文语音词汇库中学习的语言独立情感模型能够完成参照语音到目标语音的情感传递任务，并在质量和量度指标上达到了当前最佳效果。
</details></li>
</ul>
<hr>
<h2 id="SCANet-A-Self-and-Cross-Attention-Network-for-Audio-Visual-Speech-Separation"><a href="#SCANet-A-Self-and-Cross-Attention-Network-for-Audio-Visual-Speech-Separation" class="headerlink" title="SCANet: A Self- and Cross-Attention Network for Audio-Visual Speech Separation"></a>SCANet: A Self- and Cross-Attention Network for Audio-Visual Speech Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08143">http://arxiv.org/abs/2308.08143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Li, Runxuan Yang, Xiaolin Hu</li>
<li>for: 这篇论文主要用于探讨一种新的多模态协同分离方法，即自适应和交叉关注网络（SCANet），该方法可以有效地融合音频和视频特征，以提高对话人的识别率。</li>
<li>methods: SCANet 使用了两种听取块：自适应（SA）块和交叉关注（CA）块，其中 CA 块分布在网络的顶部（TCA）、中部（MCA）和底部（BCA）。这些块可以学习不同的模式特征，并提取不同的 semantics 从音频和视频特征。</li>
<li>results: 根据三个标准的音频视频分离 benchmark（LRS2、LRS3 和 VoxCeleb2）的实验结果，SCANet 比现有的状态对比方法（SOTA）高效，同时保持相对的执行时间。<details>
<summary>Abstract</summary>
The integration of different modalities, such as audio and visual information, plays a crucial role in human perception of the surrounding environment. Recent research has made significant progress in designing fusion modules for audio-visual speech separation. However, they predominantly focus on multi-modal fusion architectures situated either at the top or bottom positions, rather than comprehensively considering multi-modal fusion at various hierarchical positions within the network. In this paper, we propose a novel model called self- and cross-attention network (SCANet), which leverages the attention mechanism for efficient audio-visual feature fusion. SCANet consists of two types of attention blocks: self-attention (SA) and cross-attention (CA) blocks, where the CA blocks are distributed at the top (TCA), middle (MCA) and bottom (BCA) of SCANet. These blocks maintain the ability to learn modality-specific features and enable the extraction of different semantics from audio-visual features. Comprehensive experiments on three standard audio-visual separation benchmarks (LRS2, LRS3, and VoxCeleb2) demonstrate the effectiveness of SCANet, outperforming existing state-of-the-art (SOTA) methods while maintaining comparable inference time.
</details>
<details>
<summary>摘要</summary>
人类在识别环境中利用不同模式的感知信息，如音频和视觉信息，进行集成很重要。现代研究已经在设计多模态融合模块方面做出了重要进步，但是这些模型大多集中于网络的顶层或底层位置，而不是全面考虑多模态融合在网络各个层次位置。本文提出了一种新的模型，即自身和交叉注意网络（SCANet），它利用注意机制来有效地融合音频和视觉特征。SCANet包括两种注意块：自身注意（SA）和交叉注意（CA）块，其中CA块分布在网络顶层（TCA）、中层（MCA）和底层（BCA）。这些块可以学习不同模式的特征，并允许从音频和视觉特征中提取不同的 semantics。我们对三个标准音频视频分离 benchmark（LRS2、LRS3和VoxCeleb2）进行了广泛的实验， demonstarted SCANet的效果，而且与现有的最佳方法（SOTA）保持相对的推理时间。
</details></li>
</ul>
<hr>
<h2 id="Radio2Text-Streaming-Speech-Recognition-Using-mmWave-Radio-Signals"><a href="#Radio2Text-Streaming-Speech-Recognition-Using-mmWave-Radio-Signals" class="headerlink" title="Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals"></a>Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08125">http://arxiv.org/abs/2308.08125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Running Zhao, Jiangtao Yu, Hang Zhao, Edith C. H. Ngai</li>
<li>for: 这篇论文旨在提供一种基于 millimeter wave (mmWave) 技术的语音识别系统，以便在会议听录和窃听等场景中实现高精度语音识别。</li>
<li>methods: 该系统基于一种适配 streams 的 transformer 模型，通过特定的束缚和知识储存技术来实现大词汇量语音识别。</li>
<li>results: 实验结果显示，Radio2Text 可以在 recognizing 一个包含超过 13,000 个词的词汇库时达到 character error rate 5.7% 和 word error rate 9.4%。<details>
<summary>Abstract</summary>
Millimeter wave (mmWave) based speech recognition provides more possibility for audio-related applications, such as conference speech transcription and eavesdropping. However, considering the practicality in real scenarios, latency and recognizable vocabulary size are two critical factors that cannot be overlooked. In this paper, we propose Radio2Text, the first mmWave-based system for streaming automatic speech recognition (ASR) with a vocabulary size exceeding 13,000 words. Radio2Text is based on a tailored streaming Transformer that is capable of effectively learning representations of speech-related features, paving the way for streaming ASR with a large vocabulary. To alleviate the deficiency of streaming networks unable to access entire future inputs, we propose the Guidance Initialization that facilitates the transfer of feature knowledge related to the global context from the non-streaming Transformer to the tailored streaming Transformer through weight inheritance. Further, we propose a cross-modal structure based on knowledge distillation (KD), named cross-modal KD, to mitigate the negative effect of low quality mmWave signals on recognition performance. In the cross-modal KD, the audio streaming Transformer provides feature and response guidance that inherit fruitful and accurate speech information to supervise the training of the tailored radio streaming Transformer. The experimental results show that our Radio2Text can achieve a character error rate of 5.7% and a word error rate of 9.4% for the recognition of a vocabulary consisting of over 13,000 words.
</details>
<details>
<summary>摘要</summary>
微米波（mmWave）基于语音识别提供更多的音频相关应用，如会议语音转文和窃听。然而，在实际场景中，延迟和可识别词汇数是两个关键因素，不能被忽略。在这篇论文中，我们提出Radio2Text，首个基于微米波的流处理自动语音识别（ASR）系统，可以识别超过13,000个词的词汇。Radio2Text基于适应流处理变换器，可以有效地学习语音相关特征的表示，为流处理ASR带来新的可能性。为了解决流处理网络无法访问整个未来输入的缺陷，我们提出引导初始化，通过重量继承来传递非流处理变换器中的特征知识相关全局 контекст到适应流处理变换器。此外，我们提出了基于知识传授（KD）的交叉模态结构，称为交叉模态KD，以mitigate低质量微米波信号对识别性的负面效应。在交叉模态KD中，音频流处理变换器提供特征和回应导航，将有用和准确的语音信息继承给适应广播流处理变换器进行超vision训练。实验结果显示，我们的Radio2Text可以达到Character Error Rate（CER）5.7%和Word Error Rate（WER）9.4%，用于识别超过13,000个词的词汇。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Open-Vocabulary-Keyword-Search-With-Multilingual-Neural-Representations"><a href="#End-to-End-Open-Vocabulary-Keyword-Search-With-Multilingual-Neural-Representations" class="headerlink" title="End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations"></a>End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08027">http://arxiv.org/abs/2308.08027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bolaji Yusuf, Jan Cernocky, Murat Saraclar</li>
<li>for: 提高 keyword search 系统的效率和简化搜索过程，不需要自动语音识别（ASR）输出。</li>
<li>methods: 使用神经网络encoder对查询和文档进行编码，并将编码结果进行点积 multiplication 组合。</li>
<li>results: 在长查询和没有在训练数据中出现的查询中，提高了模型的性能，并且对于短查询和包含在 vocabulary 中的查询，虽然不能与强大的 ASR-based 传统搜索系统匹配，但是仍然超过了 ASR-based 系统。<details>
<summary>Abstract</summary>
Conventional keyword search systems operate on automatic speech recognition (ASR) outputs, which causes them to have a complex indexing and search pipeline. This has led to interest in ASR-free approaches to simplify the search procedure. We recently proposed a neural ASR-free keyword search model which achieves competitive performance while maintaining an efficient and simplified pipeline, where queries and documents are encoded with a pair of recurrent neural network encoders and the encodings are combined with a dot-product. In this article, we extend this work with multilingual pretraining and detailed analysis of the model. Our experiments show that the proposed multilingual training significantly improves the model performance and that despite not matching a strong ASR-based conventional keyword search system for short queries and queries comprising in-vocabulary words, the proposed model outperforms the ASR-based system for long queries and queries that do not appear in the training data.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:传统的关键词搜索系统采用自动语音识别（ASR）输出，这导致搜索管道变得复杂。这引起了关注ASR-free方法，以简化搜索过程。我们最近提出了一种基于神经网络的ASR-free关键词搜索模型，该模型在竞争性和效率方面具有优异表现，而无需复杂的搜索管道。在这篇文章中，我们延续这种工作，并通过多语言预训练和详细分析，进一步提高模型性能。我们的实验表明，提档多语言训练显著提高模型性能，并且对于长 queries和不在训练数据中出现的 queries，模型的性能胜过ASR-based系统。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/cs.SD_2023_08_16/" data-id="cllurrpb60097sw888tephtr8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/eess.AS_2023_08_16/" class="article-date">
  <time datetime="2023-08-15T16:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/16/eess.AS_2023_08_16/">eess.AS - 2023-08-16 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-ID-R-D-VoxCeleb-Speaker-Recognition-Challenge-2023-System-Description"><a href="#The-ID-R-D-VoxCeleb-Speaker-Recognition-Challenge-2023-System-Description" class="headerlink" title="The ID R&amp;D VoxCeleb Speaker Recognition Challenge 2023 System Description"></a>The ID R&amp;D VoxCeleb Speaker Recognition Challenge 2023 System Description</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08294">http://arxiv.org/abs/2308.08294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Torgashov, Rostislav Makarov, Ivan Yakovlev, Pavel Malov, Andrei Balykin, Anton Okhotnikov</li>
<li>for: 这个研究是为了参加2023年VoxCeleb Speaker Recognition Challenge（VoxSRC-23）的Track 2（开放赛）而撰写的。</li>
<li>methods: 该解决方案基于深度ResNet和自动标注学习（SSL）基于模型，在一个组合的VoxCeleb2数据集和大量的VoxTube数据集上进行训练。</li>
<li>results: 最终在Track 2上提交的解决方案在VoxSRC-23公共排名板上达到了第一名， minDCF(0.05) 为0.0762， EER 为1.30%。<details>
<summary>Abstract</summary>
This report describes ID R&D team submissions for Track 2 (open) to the VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC-23). Our solution is based on the fusion of deep ResNets and self-supervised learning (SSL) based models trained on a mixture of a VoxCeleb2 dataset and a large version of a VoxTube dataset. The final submission to the Track 2 achieved the first place on the VoxSRC-23 public leaderboard with a minDCF(0.05) of 0.0762 and EER of 1.30%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/eess.AS_2023_08_16/" data-id="cllurrpbs00b9sw88g4ykddzn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/eess.IV_2023_08_16/" class="article-date">
  <time datetime="2023-08-15T16:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/16/eess.IV_2023_08_16/">eess.IV - 2023-08-16 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Prediction-of-post-radiotherapy-recurrence-volumes-in-head-and-neck-squamous-cell-carcinoma-using-3D-U-Net-segmentation"><a href="#Prediction-of-post-radiotherapy-recurrence-volumes-in-head-and-neck-squamous-cell-carcinoma-using-3D-U-Net-segmentation" class="headerlink" title="Prediction of post-radiotherapy recurrence volumes in head and neck squamous cell carcinoma using 3D U-Net segmentation"></a>Prediction of post-radiotherapy recurrence volumes in head and neck squamous cell carcinoma using 3D U-Net segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08396">http://arxiv.org/abs/2308.08396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Kutnár, Ivan R Vogelius, Katrin Elisabet Håkansson, Jens Petersen, Jeppe Friborg, Lena Specht, Mogens Bernsdorf, Anita Gothelf, Claus Kristensen, Abraham George Smith</li>
<li>for: 这研究旨在使用 convolutional neural network (CNN) 预测头颈癌细胞癌 (HNSCC) 患者的 lokoregional recurrences (LRR) 体积，以便通过生物标记化辐射疗法来提高治疗效果。</li>
<li>methods: 研究使用了 18F-fluorodeoxyglucose positron emission tomography (FDG-PET)&#x2F;computed tomography (CT) 扫描图像来训练 CNN，并对患者的前 treated 患区进行了预测。</li>
<li>results: 研究发现，使用 CNN 可以准确预测 HNSCC 患者的 LRR 体积，并且比使用 SUVmax 阈值方法或 GTV 直接使用更有效。 however, 需要进一步的数据集开发以实现临床有用的预测精度。<details>
<summary>Abstract</summary>
Locoregional recurrences (LRR) are still a frequent site of treatment failure for head and neck squamous cell carcinoma (HNSCC) patients.   Identification of high risk subvolumes based on pretreatment imaging is key to biologically targeted radiation therapy. We investigated the extent to which a Convolutional neural network (CNN) is able to predict LRR volumes based on pre-treatment 18F-fluorodeoxyglucose positron emission tomography (FDG-PET)/computed tomography (CT) scans in HNSCC patients and thus the potential to identify biological high risk volumes using CNNs.   For 37 patients who had undergone primary radiotherapy for oropharyngeal squamous cell carcinoma, five oncologists contoured the relapse volumes on recurrence CT scans. Datasets of pre-treatment FDG-PET/CT, gross tumour volume (GTV) and contoured relapse for each of the patients were randomly divided into training (n=23), validation (n=7) and test (n=7) datasets. We compared a CNN trained from scratch, a pre-trained CNN, a SUVmax threshold approach, and using the GTV directly.   The SUVmax threshold method included 5 out of the 7 relapse origin points within a volume of median 4.6 cubic centimetres (cc). Both the GTV contour and best CNN segmentations included the relapse origin 6 out of 7 times with median volumes of 28 and 18 cc respectively.   The CNN included the same or greater number of relapse volume POs, with significantly smaller relapse volumes. Our novel findings indicate that CNNs may predict LRR, yet further work on dataset development is required to attain clinically useful prediction accuracy.
</details>
<details>
<summary>摘要</summary>
Head and neck squamous cell carcinoma (HNSCC) 患者中的局部再现 (LRR) 仍然是治疗失败的常见现象。 为了预测LRR的卷积批处，我们使用了卷积神经网络 (CNN)。我们研究了在前治疗18F-fluorodeoxyglucose пози트рон辐射tomography (FDG-PET)/计算机 Tomography (CT) 图像上预测LRR объем的可能性。为了进行这项研究，我们收集了37名有主治疗的口腔癌患者的数据。这些患者都已经接受了主治疗。我们让5名医生标注了再现图像上的再现 объем。我们将这些数据分为训练集（n=23）、验证集（n=7）和测试集（n=7）。我们比较了从头文字开始训练的CNN、预训练的CNN、SUVmax阈值方法和直接使用GTV的方法。SUVmax阈值方法中包含了7名再现起点的5个点在 median 4.6立方厘米（cc）内。GTV框和最佳CNN分割都包含了再现起点6个名次， median volume 28和18 cc。 CN中包含了相同或更多的再现量PO，并且再现 объем更小。我们的新发现表明，CNN可能预测LRR，但是我们需要进一步增加数据来提高临床可用性的预测精度。
</details></li>
</ul>
<hr>
<h2 id="DeepContrast-Deep-Tissue-Contrast-Enhancement-using-Synthetic-Data-Degradations-and-OOD-Model-Predictions"><a href="#DeepContrast-Deep-Tissue-Contrast-Enhancement-using-Synthetic-Data-Degradations-and-OOD-Model-Predictions" class="headerlink" title="DeepContrast: Deep Tissue Contrast Enhancement using Synthetic Data Degradations and OOD Model Predictions"></a>DeepContrast: Deep Tissue Contrast Enhancement using Synthetic Data Degradations and OOD Model Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08365">http://arxiv.org/abs/2308.08365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nuno Pimpão Martins, Yannis Kalaidzidis, Marino Zerial, Florian Jug</li>
<li>for: 该论文主要针对 Microscopy 图像质量下降的问题，即图像噪音、模糊和其他问题，以及这些问题对图像含义的影响。</li>
<li>methods: 该论文使用 Deep Learning 方法来提高 Microscopy 图像质量，但是这些方法通常需要有clean的ground truth（GT）数据进行训练。然而，在深入到样本中图像时，因为各种降低效应，获得clean GT数据变得困难。因此，该论文提出了一种新的方法，可以缺省GT数据进行训练。</li>
<li>results: 该论文使用了一种模拟前向模型来模拟深入到样本中图像的降低效应，然后使用 neural network 学习这个降低效应的逆过程。结果表明，使用这种方法可以提高 Microscopy 图像质量，并且可以在不需要clean GT数据的情况下进行训练。此外，该论文还发现，在不同的下游分析中，需要找到一个平衡点，以保留图像细节和提高图像含义的抽象。<details>
<summary>Abstract</summary>
Microscopy images are crucial for life science research, allowing detailed inspection and characterization of cellular and tissue-level structures and functions. However, microscopy data are unavoidably affected by image degradations, such as noise, blur, or others. Many such degradations also contribute to a loss of image contrast, which becomes especially pronounced in deeper regions of thick samples. Today, best performing methods to increase the quality of images are based on Deep Learning approaches, which typically require ground truth (GT) data during training. Our inability to counteract blurring and contrast loss when imaging deep into samples prevents the acquisition of such clean GT data. The fact that the forward process of blurring and contrast loss deep into tissue can be modeled, allowed us to propose a new method that can circumvent the problem of unobtainable GT data. To this end, we first synthetically degraded the quality of microscopy images even further by using an approximate forward model for deep tissue image degradations. Then we trained a neural network that learned the inverse of this degradation function from our generated pairs of raw and degraded images. We demonstrated that networks trained in this way can be used out-of-distribution (OOD) to improve the quality of less severely degraded images, e.g. the raw data imaged in a microscope. Since the absolute level of degradation in such microscopy images can be stronger than the additional degradation introduced by our forward model, we also explored the effect of iterative predictions. Here, we observed that in each iteration the measured image contrast kept improving while detailed structures in the images got increasingly removed. Therefore, dependent on the desired downstream analysis, a balance between contrast improvement and retention of image details has to be found.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:微scopic 图像是生命科学研究中不可或缺的，允许详细检查和Cellular 和组织水平结构和功能的 characterization。然而，微scopic 图像数据不可避免地受到图像质量下降的影响，如噪声、模糊或其他。这些质量下降也会导致图像对比下降，特别是在厚度样本深处。今天，使用深度学习方法来提高图像质量的最佳实践是基于GT数据的训练。然而，我们无法对深度样本中的厚度进行对比下降，因此无法获得净化GT数据。我们发现，可以使用深度图像模型来模拟深度样本中的对比下降过程，从而提出一种新的方法，可以绕过无法获得GT数据的问题。我们首先使用深度图像模型来进一步降低微scopic 图像的质量，然后使用这些生成的对比下降对的图像对比下降进行学习。我们证明了这种方法可以在OOD（out-of-distribution）下使用，以提高less severely degraded 的图像质量。由于微scopic 图像中的绝对质量可能比我们的深度图像模型引入的质量更强，我们还 investigate了反复预测的效果。我们发现，在每次预测中，测量图像对比度会不断提高，而图像中的细节会逐渐消失。因此，根据下游分析的需求，需要找到保持图像细节的平衡。
</details></li>
</ul>
<hr>
<h2 id="GAEI-UNet-Global-Attention-and-Elastic-Interaction-U-Net-for-Vessel-Image-Segmentation"><a href="#GAEI-UNet-Global-Attention-and-Elastic-Interaction-U-Net-for-Vessel-Image-Segmentation" class="headerlink" title="GAEI-UNet: Global Attention and Elastic Interaction U-Net for Vessel Image Segmentation"></a>GAEI-UNet: Global Attention and Elastic Interaction U-Net for Vessel Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08345">http://arxiv.org/abs/2308.08345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiqiang Xiao, Zhuoyue Wan</li>
<li>for: 静脉图像分割是医学诊断中非常重要的一环，能够帮助早期发现和治疗血管疾病。</li>
<li>methods: 我们提出了一种新的模型，即GAEI-UNet，它将全局注意力和弹性互动技术相结合。GAEI-UNet 利用全局空间和通道信息来增强 U-Net 架构中的高级 semantics 理解，以提高小血管的精确分割。此外，我们采用了弹性互动基于的损失函数，以提高小血管网络中的连接性。</li>
<li>results: 我们在 DRIVE 血管图像集上进行了评估，结果表明 GAEI-UNet 在精确分割小血管方面表现出色，而无需增加计算复杂度。此外，GAEI-UNet 还能够保持血管网络的正确 topology。<details>
<summary>Abstract</summary>
Vessel image segmentation plays a pivotal role in medical diagnostics, aiding in the early detection and treatment of vascular diseases. While segmentation based on deep learning has shown promising results, effectively segmenting small structures and maintaining connectivity between them remains challenging. To address these limitations, we propose GAEI-UNet, a novel model that combines global attention and elastic interaction-based techniques. GAEI-UNet leverages global spatial and channel context information to enhance high-level semantic understanding within the U-Net architecture, enabling precise segmentation of small vessels. Additionally, we adopt an elastic interaction-based loss function to improve connectivity among these fine structures. By capturing the forces generated by misalignment between target and predicted shapes, our model effectively learns to preserve the correct topology of vessel networks. Evaluation on retinal vessel dataset -- DRIVE demonstrates the superior performance of GAEI-UNet in terms of SE and connectivity of small structures, without significantly increasing computational complexity. This research aims to advance the field of vessel image segmentation, providing more accurate and reliable diagnostic tools for the medical community. The implementation code is available on Code.
</details>
<details>
<summary>摘要</summary>
船体图像分割在医学诊断中扮演着关键角色，帮助早期发现和治疗血管疾病。although deep learning-based segmentation has shown promising results, effectively segmenting small structures and maintaining connectivity between them remains challenging. To address these limitations, we propose GAEI-UNet, a novel model that combines global attention and elastic interaction-based techniques. GAEI-UNet leverages global spatial and channel context information to enhance high-level semantic understanding within the U-Net architecture, enabling precise segmentation of small vessels. Additionally, we adopt an elastic interaction-based loss function to improve connectivity among these fine structures. By capturing the forces generated by misalignment between target and predicted shapes, our model effectively learns to preserve the correct topology of vessel networks. Evaluation on retinal vessel dataset -- DRIVE demonstrates the superior performance of GAEI-UNet in terms of SE and connectivity of small structures, without significantly increasing computational complexity. This research aims to advance the field of vessel image segmentation, providing more accurate and reliable diagnostic tools for the medical community. The implementation code is available on Code.Here's the text with some notes on the translation:1. "船体图像分割" (zhōng tǐ tú zhǐ bīng) - This phrase is used to refer to the process of segmenting images of vessels, such as blood vessels in the retina.2. "在医学诊断中扮演着关键角色" (zhī xué shòu yì zhòng zhì yǐng) - This phrase emphasizes the importance of vessel image segmentation in medical diagnosis.3. "although deep learning-based segmentation has shown promising results" (although deep learning-based segmentation has shown promising results) - This phrase is used to acknowledge the progress that has been made in vessel image segmentation using deep learning techniques.4. "effectively segmenting small structures and maintaining connectivity between them remains challenging" (effectively segmenting small structures and maintaining connectivity between them remains challenging) - This phrase highlights the limitations of current vessel image segmentation methods, specifically the difficulty in accurately segmenting small vessels and maintaining the connectivity between them.5. "To address these limitations, we propose GAEI-UNet" (To address these limitations, we propose GAEI-UNet) - This phrase introduces the novel model proposed in the research, which aims to overcome the limitations of current methods.6. "a novel model that combines global attention and elastic interaction-based techniques" (a novel model that combines global attention and elastic interaction-based techniques) - This phrase describes the key innovation of the proposed model, which combines global attention and elastic interaction-based techniques to improve the accuracy and reliability of vessel image segmentation.7. "leverages global spatial and channel context information to enhance high-level semantic understanding" (leverages global spatial and channel context information to enhance high-level semantic understanding) - This phrase explains how the proposed model uses global spatial and channel context information to improve the understanding of the vessel networks and enhance the accuracy of segmentation.8. "enabling precise segmentation of small vessels" (enabling precise segmentation of small vessels) - This phrase highlights the main advantage of the proposed model, which is its ability to accurately segment small vessels.9. "without significantly increasing computational complexity" (without significantly increasing computational complexity) - This phrase emphasizes that the proposed model does not require a significant increase in computational resources, making it more practical and efficient for real-world applications.10. "This research aims to advance the field of vessel image segmentation" (This research aims to advance the field of vessel image segmentation) - This phrase highlights the overall goal of the research, which is to improve the accuracy and reliability of vessel image segmentation and provide more accurate and reliable diagnostic tools for the medical community.I hope this helps! Let me know if you have any further questions or if there's anything else I can help with.
</details></li>
</ul>
<hr>
<h2 id="Denoising-Diffusion-Probabilistic-Model-for-Retinal-Image-Generation-and-Segmentation"><a href="#Denoising-Diffusion-Probabilistic-Model-for-Retinal-Image-Generation-and-Segmentation" class="headerlink" title="Denoising Diffusion Probabilistic Model for Retinal Image Generation and Segmentation"></a>Denoising Diffusion Probabilistic Model for Retinal Image Generation and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08339">http://arxiv.org/abs/2308.08339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aaleka/retree">https://github.com/aaleka/retree</a></li>
<li>paper_authors: Alnur Alimanov, Md Baharul Islam<br>for:这个研究旨在提供一个高品质的retinal image dataset，并运用Generative Adversarial Networks (GAN)和Denosing Diffusion Probabilistic Model (DDPM)来生成具有多样性的retinal images。methods:本研究使用了GAN和DDPM来生成retinal images，并开发了一个名为Retinal Trees (ReTree)的dataset，包括retinal images、相应的血管树和一个基于DDPM的分类网络。results:研究发现，DDPM可以对于retinal images的生成比GAN更高品质，并且可以生成多样性较高的retinal images。ReTree dataset也被评估了其量化和质量上的表现，并且显示了其可以用于血管树分类和retinal image分类任务。<details>
<summary>Abstract</summary>
Experts use retinal images and vessel trees to detect and diagnose various eye, blood circulation, and brain-related diseases. However, manual segmentation of retinal images is a time-consuming process that requires high expertise and is difficult due to privacy issues. Many methods have been proposed to segment images, but the need for large retinal image datasets limits the performance of these methods. Several methods synthesize deep learning models based on Generative Adversarial Networks (GAN) to generate limited sample varieties. This paper proposes a novel Denoising Diffusion Probabilistic Model (DDPM) that outperformed GANs in image synthesis. We developed a Retinal Trees (ReTree) dataset consisting of retinal images, corresponding vessel trees, and a segmentation network based on DDPM trained with images from the ReTree dataset. In the first stage, we develop a two-stage DDPM that generates vessel trees from random numbers belonging to a standard normal distribution. Later, the model is guided to generate fundus images from given vessel trees and random distribution. The proposed dataset has been evaluated quantitatively and qualitatively. Quantitative evaluation metrics include Frechet Inception Distance (FID) score, Jaccard similarity coefficient, Cohen's kappa, Matthew's Correlation Coefficient (MCC), precision, recall, F1-score, and accuracy. We trained the vessel segmentation model with synthetic data to validate our dataset's efficiency and tested it on authentic data. Our developed dataset and source code is available at https://github.com/AAleka/retree.
</details>
<details>
<summary>摘要</summary>
专家利用血液图像和血管树来检测和诊断各种眼、血液和脑部疾病。然而，手动分割血液图像是一项时间consuming和需要高度专业知识的过程，另外，隐私问题也使得这项工作困难。许多方法已经被提出来分割图像，但是因为数据的限制，这些方法的性能受到限制。本文提出了一种新的涂抹扩散 probabilistic model（DDPM），其在图像生成方面超过了GAN的表现。我们还制作了一个名为“Retinal Trees”（ReTree）的 dataset，该 dataset包括血液图像、对应的血管树和基于 DDPM 的分割网络。在首个阶段，我们开发了一种两 stage DDPM，该模型从标准正态分布中的随机数生成血管树。后来，模型被引导使用给定的血管树和随机分布来生成血液图像。我们对该 dataset 进行了量化和质量上的评估。量化评估指标包括Frechet Inception Distance（FID）分数、Jaccard 相似度系数、Cohen's kappa、Matthew's Correlation Coefficient（MCC）、精度、 recall、F1-score 和准确率。我们使用合成数据来训练分割模型，以验证我们的 dataset 的效率，然后在真实数据上进行测试。我们开发的 dataset 和源代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="ECPC-IDS-A-benchmark-endometrail-cancer-PET-CT-image-dataset-for-evaluation-of-semantic-segmentation-and-detection-of-hypermetabolic-regions"><a href="#ECPC-IDS-A-benchmark-endometrail-cancer-PET-CT-image-dataset-for-evaluation-of-semantic-segmentation-and-detection-of-hypermetabolic-regions" class="headerlink" title="ECPC-IDS:A benchmark endometrail cancer PET&#x2F;CT image dataset for evaluation of semantic segmentation and detection of hypermetabolic regions"></a>ECPC-IDS:A benchmark endometrail cancer PET&#x2F;CT image dataset for evaluation of semantic segmentation and detection of hypermetabolic regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08313">http://arxiv.org/abs/2308.08313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dechao Tang, Xuanyi Li, Tianming Du, Deguo Ma, Zhiyu Ma, Hongzan Sun, Marcin Grzegorzek, Huiyan Jiang, Chen Li</li>
<li>for: 这个论文主要目的是提供一个大量多图像的 ENDOMETRIAL CANCER PET&#x2F;CT图像数据集，以便研究人员可以通过计算机助理诊断技术来提高诊断的准确性和 объектив性，同时减轻医生的工作负担。</li>
<li>methods: 这个论文使用了五种经典的深度学习 semantic segmentation 方法和六种深度学习对象检测方法进行测试，以证明不同方法在 ECPC-IDS 上的差异。</li>
<li>results: 这个论文通过 extensively 的实验，demonstrate 了不同方法在 ECPC-IDS 上的性能差异，并证明了这个数据集可以帮助研究人员开发新的算法，以提高计算机助理诊断技术的性能，从而为临床医生和患者带来很大的 benefit.<details>
<summary>Abstract</summary>
Endometrial cancer is one of the most common tumors in the female reproductive system and is the third most common gynecological malignancy that causes death after ovarian and cervical cancer. Early diagnosis can significantly improve the 5-year survival rate of patients. With the development of artificial intelligence, computer-assisted diagnosis plays an increasingly important role in improving the accuracy and objectivity of diagnosis, as well as reducing the workload of doctors. However, the absence of publicly available endometrial cancer image datasets restricts the application of computer-assisted diagnostic techniques.In this paper, a publicly available Endometrial Cancer PET/CT Image Dataset for Evaluation of Semantic Segmentation and Detection of Hypermetabolic Regions (ECPC-IDS) are published. Specifically, the segmentation section includes PET and CT images, with a total of 7159 images in multiple formats. In order to prove the effectiveness of segmentation methods on ECPC-IDS, five classical deep learning semantic segmentation methods are selected to test the image segmentation task. The object detection section also includes PET and CT images, with a total of 3579 images and XML files with annotation information. Six deep learning methods are selected for experiments on the detection task.This study conduct extensive experiments using deep learning-based semantic segmentation and object detection methods to demonstrate the differences between various methods on ECPC-IDS. As far as we know, this is the first publicly available dataset of endometrial cancer with a large number of multiple images, including a large amount of information required for image and target detection. ECPC-IDS can aid researchers in exploring new algorithms to enhance computer-assisted technology, benefiting both clinical doctors and patients greatly.
</details>
<details>
<summary>摘要</summary>
《Endometrial Cancer PET/CT Image Dataset for Evaluation of Semantic Segmentation and Detection of Hypermetabolic Regions (ECPC-IDS)》Endometrial cancer是女性生殖系统中最常见的肿瘤，也是最常见的女性生殖系统癌症之一，仅次于卵巢和子宫癌。早期诊断可以显著提高病人5年生存率。随着人工智能的发展，计算机协助诊断在提高诊断精度和公正性方面发挥了越来越重要的作用，同时也减轻医生的工作负担。然而， absence of publicly available endometrial cancer image datasets restricts the application of computer-assisted diagnostic techniques。为了解决这个问题，我们在这篇论文中发布了一个公共可用的Endometrial Cancer PET/CT Image Dataset for Evaluation of Semantic Segmentation and Detection of Hypermetabolic Regions (ECPC-IDS)。具体来说，分别包括PET和CT图像，共7159张图像，多种格式。为证明ECPC-IDS上 segmentation 方法的效果，我们选择了5种经典的深度学习 semantic segmentation 方法进行测试图像 segmentation 任务。另外， objet detection 部分也包括PET和CT图像，共3579张图像和XML文件中的注释信息。我们选择了6种深度学习方法进行 эксперимент detection 任务。本研究通过使用深度学习基于的semantic segmentation和object detection方法，进行了对ECPC-IDS的广泛实验，以示出不同方法之间的差异。据我们所知，ECPC-IDS是首个公共可用的 endometrial cancer 数据集，包含大量多种图像信息，包括图像和目标检测需要的大量信息。ECPC-IDS 可以帮助研究人员探索新的算法，以提高计算机协助技术，对临床医生和病人都是非常有利。
</details></li>
</ul>
<hr>
<h2 id="OnUVS-Online-Feature-Decoupling-Framework-for-High-Fidelity-Ultrasound-Video-Synthesis"><a href="#OnUVS-Online-Feature-Decoupling-Framework-for-High-Fidelity-Ultrasound-Video-Synthesis" class="headerlink" title="OnUVS: Online Feature Decoupling Framework for High-Fidelity Ultrasound Video Synthesis"></a>OnUVS: Online Feature Decoupling Framework for High-Fidelity Ultrasound Video Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08269">http://arxiv.org/abs/2308.08269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhou, Dong Ni, Ao Chang, Xinrui Zhou, Rusi Chen, Yanlin Chen, Lian Liu, Jiamin Liang, Yuhao Huang, Tong Han, Zhe Liu, Deng-Ping Fan, Xin Yang</li>
<li>for:  This paper aims to address the challenges of synthesizing high-fidelity ultrasound (US) videos for clinical diagnosis, particularly the limited availability of specific US video cases, by presenting a novel online feature-decoupling framework called OnUVS.</li>
<li>methods:  The OnUVS framework uses a weakly-supervised training strategy to introduce anatomic information into keypoint learning, a dual-decoder to decouple content and textural features, and a multiple-feature discriminator to enhance the sharpness and fine details of the generated videos. Additionally, the framework constrains the motion trajectories of keypoints during online learning to enhance the fluidity of the generated videos.</li>
<li>results:  The paper demonstrates the effectiveness of OnUVS in synthesizing US videos with high fidelity through validation and user studies on in-house echocardiographic and pelvic floor US videos.<details>
<summary>Abstract</summary>
Ultrasound (US) imaging is indispensable in clinical practice. To diagnose certain diseases, sonographers must observe corresponding dynamic anatomic structures to gather comprehensive information. However, the limited availability of specific US video cases causes teaching difficulties in identifying corresponding diseases, which potentially impacts the detection rate of such cases. The synthesis of US videos may represent a promising solution to this issue. Nevertheless, it is challenging to accurately animate the intricate motion of dynamic anatomic structures while preserving image fidelity. To address this, we present a novel online feature-decoupling framework called OnUVS for high-fidelity US video synthesis. Our highlights can be summarized by four aspects. First, we introduced anatomic information into keypoint learning through a weakly-supervised training strategy, resulting in improved preservation of anatomical integrity and motion while minimizing the labeling burden. Second, to better preserve the integrity and textural information of US images, we implemented a dual-decoder that decouples the content and textural features in the generator. Third, we adopted a multiple-feature discriminator to extract a comprehensive range of visual cues, thereby enhancing the sharpness and fine details of the generated videos. Fourth, we constrained the motion trajectories of keypoints during online learning to enhance the fluidity of generated videos. Our validation and user studies on in-house echocardiographic and pelvic floor US videos showed that OnUVS synthesizes US videos with high fidelity.
</details>
<details>
<summary>摘要</summary>
超声影像（US）是诊断疾病的不可或缺的工具。sonographers需要观察相应的动态生理结构，以获取全面的信息。然而，有限的特定US视频案例的有效性，使得教学和诊断这些疾病具有挑战性。为解决这个问题，我们提出了一种新的在线特征分离框架 called OnUVS，用于高精度US视频生成。我们的特点包括：1. 通过弱有监督训练策略，将解剖信息引入关键点学习中，以提高动态生理结构的保留和动作，同时减少标注卷积。2. 为了更好地保持US图像的完整性和текстуral信息，我们实现了内容和текстуral特征的解码器。3. 采用多个特征识别器，挖掘更广泛的视觉cue，提高生成视频的锐度和细节。4. 在在线学习中，限制关键点的运动轨迹，以提高生成视频的流畅性。我们的验证和用户研究表明，OnUVS可以生成高精度的US视频。
</details></li>
</ul>
<hr>
<h2 id="Neural-Spherical-Harmonics-for-structurally-coherent-continuous-representation-of-diffusion-MRI-signal"><a href="#Neural-Spherical-Harmonics-for-structurally-coherent-continuous-representation-of-diffusion-MRI-signal" class="headerlink" title="Neural Spherical Harmonics for structurally coherent continuous representation of diffusion MRI signal"></a>Neural Spherical Harmonics for structurally coherent continuous representation of diffusion MRI signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08210">http://arxiv.org/abs/2308.08210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Hendriks, Anna Vilanova, Maxime Chamberland</li>
<li>for: This paper presents a novel method for modeling diffusion magnetic resonance imaging (dMRI) datasets, which leverages the structural coherence of the human brain to improve the accuracy and efficiency of the reconstruction process.</li>
<li>methods: The proposed method uses a neural network to parameterize a spherical harmonics series (NeSH) to represent the dMRI signal of a single subject, continuous in both the angular and spatial domain. The method also utilizes upsampling in both the angular and spatial domain to improve the reconstruction results.</li>
<li>results: The reconstructed dMRI signal using the proposed method shows a more structurally coherent representation of the data, with reduced noise in gradient images and smoother fiber orientation distribution functions. The method also enables the calculation of mean diffusivity, fractional anisotropy, and total apparent fiber density with a single model architecture and minimal hyperparameter tuning.<details>
<summary>Abstract</summary>
We present a novel way to model diffusion magnetic resonance imaging (dMRI) datasets, that benefits from the structural coherence of the human brain while only using data from a single subject. Current methods model the dMRI signal in individual voxels, disregarding the intervoxel coherence that is present. We use a neural network to parameterize a spherical harmonics series (NeSH) to represent the dMRI signal of a single subject from the Human Connectome Project dataset, continuous in both the angular and spatial domain. The reconstructed dMRI signal using this method shows a more structurally coherent representation of the data. Noise in gradient images is removed and the fiber orientation distribution functions show a smooth change in direction along a fiber tract. We showcase how the reconstruction can be used to calculate mean diffusivity, fractional anisotropy, and total apparent fiber density. These results can be achieved with a single model architecture, tuning only one hyperparameter. In this paper we also demonstrate how upsampling in both the angular and spatial domain yields reconstructions that are on par or better than existing methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于模型Diffusion Magnetic Resonance Imaging（dMRI）数据集，该方法利用人脑的结构减噪。现有方法通常在个体粒子上模型dMRI信号，忽略了各个粒子之间的协调性。我们使用神经网络来参数化球面幂数列（NeSH）来表示单个参与者的dMRI信号，这种表示方式是连续的在角度和空间领域。重建的dMRI信号表现出了更好的结构减噪，排除了梯度图像中的噪声，并且纤维方向分布函数（fOD）示出了细胞轴的平滑变化。我们还示出了如何使用这种重建方法来计算平均扩散率、有效扩散率和总显示纤维 densidad。这些结果可以通过单个模型架构和一个超参数来实现。此外，我们还证明了在角度和空间领域进行upsampling可以实现重建的结果与现有方法相当或更好。
</details></li>
</ul>
<hr>
<h2 id="Self-Reference-Deep-Adaptive-Curve-Estimation-for-Low-Light-Image-Enhancement"><a href="#Self-Reference-Deep-Adaptive-Curve-Estimation-for-Low-Light-Image-Enhancement" class="headerlink" title="Self-Reference Deep Adaptive Curve Estimation for Low-Light Image Enhancement"></a>Self-Reference Deep Adaptive Curve Estimation for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08197">http://arxiv.org/abs/2308.08197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/john-venti/self-dace">https://github.com/john-venti/self-dace</a></li>
<li>paper_authors: Jianyu Wen, Chenhao Wu, Tong Zhang, Yixuan Yu, Piotr Swierczynski</li>
<li>for: 提高低光照图像的显示质量</li>
<li>methods: 使用自适应曲线估计和降噪网络对低光照图像进行两个阶段进行优化</li>
<li>results: 与现有状态艺图像处理算法进行比较，该方法在多个实际数据集上表现出优于其他方法的性能<details>
<summary>Abstract</summary>
In this paper, we propose a 2-stage low-light image enhancement method called Self-Reference Deep Adaptive Curve Estimation (Self-DACE). In the first stage, we present an intuitive, lightweight, fast, and unsupervised luminance enhancement algorithm. The algorithm is based on a novel low-light enhancement curve that can be used to locally boost image brightness. We also propose a new loss function with a simplified physical model designed to preserve natural images' color, structure, and fidelity. We use a vanilla CNN to map each pixel through deep Adaptive Adjustment Curves (AAC) while preserving the local image structure. Secondly, we introduce the corresponding denoising scheme to remove the latent noise in the darkness. We approximately model the noise in the dark and deploy a Denoising-Net to estimate and remove the noise after the first stage. Exhaustive qualitative and quantitative analysis shows that our method outperforms existing state-of-the-art algorithms on multiple real-world datasets.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种两stage的低光照图像增强方法，称为Self-Reference Deep Adaptive Curve Estimation（Self-DACE）。在第一个阶段，我们提供了一种直观、轻量级、快速和无监督的亮度增强算法。该算法基于一个新的低光照增强曲线，可以地方性地提高图像亮度。我们还提出了一个新的损失函数，用于保持自然图像的颜色、结构和准确性。我们使用了一个普通的 convolutional neural network（CNN）来将每个像素通过深度适应曲线（AAC）进行映射，同时保持图像的本地结构。在第二个阶段，我们引入了相应的干扰除方法，以除除在黑暗中存在的隐藏噪声。我们简化了噪声的模型，并使用了一个Denoising-Net来估计和除去噪声。我们对多个实际世界数据集进行了详细的 качеitative和量化分析，结果表明，我们的方法在与现有状态的艺术算法进行比较时表现出色。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Perceptual-Quality-Preserving-Image-Compression"><a href="#Conditional-Perceptual-Quality-Preserving-Image-Compression" class="headerlink" title="Conditional Perceptual Quality Preserving Image Compression"></a>Conditional Perceptual Quality Preserving Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08154">http://arxiv.org/abs/2308.08154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongda Xu, Qian Zhang, Yanghao Li, Dailan He, Zhe Wang, Yuanyuan Wang, Hongwei Qin, Yan Wang, Jingjing Liu, Ya-Qin Zhang</li>
<li>for: 本文提出了基于用户定义信息的可conditioned感知质量（CPQ），用于保持高质量和semantic质量在各比特率下。</li>
<li>methods: 本文基于用户定义信息conditioning的感知质量，通过对比原始图像和重建图像的 divergence 来定义CPQ。</li>
<li>results: 实验结果表明，我们的代码可以成功保持高质量和semantic质量，并且提供了对共同随机性的下界，解决了过去关于是否应该在生成器中包含随机性以提高（conditional）感知质量压缩的辩论。<details>
<summary>Abstract</summary>
We propose conditional perceptual quality, an extension of the perceptual quality defined in \citet{blau2018perception}, by conditioning it on user defined information. Specifically, we extend the original perceptual quality $d(p_{X},p_{\hat{X}})$ to the conditional perceptual quality $d(p_{X|Y},p_{\hat{X}|Y})$, where $X$ is the original image, $\hat{X}$ is the reconstructed, $Y$ is side information defined by user and $d(.,.)$ is divergence. We show that conditional perceptual quality has similar theoretical properties as rate-distortion-perception trade-off \citep{blau2019rethinking}. Based on these theoretical results, we propose an optimal framework for conditional perceptual quality preserving compression. Experimental results show that our codec successfully maintains high perceptual quality and semantic quality at all bitrate. Besides, by providing a lowerbound of common randomness required, we settle the previous arguments on whether randomness should be incorporated into generator for (conditional) perceptual quality compression. The source code is provided in supplementary material.
</details>
<details>
<summary>摘要</summary>
我们提出了基于用户定义信息的conditional perceptual quality，具体来说是将原始的perceptual quality $d(p_{X},p_{\hat{X}})$扩展为 $d(p_{X|Y},p_{\hat{X}|Y})$,其中$X$是原始图像， $\hat{X}$是重建图像，$Y$是用户定义的侧信息。我们证明了conditional perceptual quality具有类似的理论性质，与rate-distortion-perception trade-off的交互作用。基于这些理论结论，我们提出了一个优化的conditional perceptual quality保持压缩框架。实验结果表明，我们的编码器能够保持高度的感知质量和semantic质量，并且提供了对公共随机性的下界，从而解决了过去的争议是否应该在生成器中添加随机性以实现（conditional）perceptual quality压缩。代码在补充材料中提供。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Overview-of-Computational-Nuclei-Segmentation-Methods-in-Digital-Pathology"><a href="#A-Comprehensive-Overview-of-Computational-Nuclei-Segmentation-Methods-in-Digital-Pathology" class="headerlink" title="A Comprehensive Overview of Computational Nuclei Segmentation Methods in Digital Pathology"></a>A Comprehensive Overview of Computational Nuclei Segmentation Methods in Digital Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08112">http://arxiv.org/abs/2308.08112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasileios Magoulianitis, Catherine A. Alexander, C. -C. Jay Kuo</li>
<li>for: 本文主要用于概述肿瘤诊断领域中数字patology在诊断、分期和分级等方面的应用。</li>
<li>methods: 本文使用了现代人工智能（AI）模型来自动实现核体分 segmentation，从传统图像处理技术到深度学习（DL） paradigm。</li>
<li>results: 本文提供了一个广泛的回顾，从早期使用传统图像处理技术到现代approaches，并讨论了弱监督问题的优势、不同模型的优劣点以及未来研究方向。<details>
<summary>Abstract</summary>
In the cancer diagnosis pipeline, digital pathology plays an instrumental role in the identification, staging, and grading of malignant areas on biopsy tissue specimens. High resolution histology images are subject to high variance in appearance, sourcing either from the acquisition devices or the H\&E staining process. Nuclei segmentation is an important task, as it detects the nuclei cells over background tissue and gives rise to the topology, size, and count of nuclei which are determinant factors for cancer detection. Yet, it is a fairly time consuming task for pathologists, with reportedly high subjectivity. Computer Aided Diagnosis (CAD) tools empowered by modern Artificial Intelligence (AI) models enable the automation of nuclei segmentation. This can reduce the subjectivity in analysis and reading time. This paper provides an extensive review, beginning from earlier works use traditional image processing techniques and reaching up to modern approaches following the Deep Learning (DL) paradigm. Our review also focuses on the weak supervision aspect of the problem, motivated by the fact that annotated data is scarce. At the end, the advantages of different models and types of supervision are thoroughly discussed. Furthermore, we try to extrapolate and envision how future research lines will potentially be, so as to minimize the need for labeled data while maintaining high performance. Future methods should emphasize efficient and explainable models with a transparent underlying process so that physicians can trust their output.
</details>
<details>
<summary>摘要</summary>
在肿瘤诊断管线中，数字patology扮演了重要的角色，用于识别、分期和评分肿瘤区域的各种生物标本样本。高分辨率历史图像具有高变异性，可能来自获取设备或H\&E染色过程。核仁分 segmentation是一项重要任务，因为它可以在背景组织背景下检测核仁细胞，并且对肿瘤检测有决定性作用。然而，这是一项较为时间consuming的任务， pathologists 报告了高度主观性。使用现代人工智能（AI）模型的计算支持工具（CAD）可以自动实现核仁分 segmentation，从而减少分析和阅读时间的主观性。本文提供了广泛的综述，从传统图像处理技术开始，沿着深度学习（DL） paradigm 进行到现代方法。我们的综述还专注于弱级指导问题，因为标注数据scarce。文章结束时，我们详细讨论了不同模型和类型的supervision的优势。此外，我们尝试预测未来研究的发展趋势，以减少标注数据的需求，同时保持高性能。未来的方法应该强调高效可解释的模型，并且具有透明的下面过程，以便physicians可以信任其输出。
</details></li>
</ul>
<hr>
<h2 id="Snapshot-High-Dynamic-Range-Imaging-with-a-Polarization-Camera"><a href="#Snapshot-High-Dynamic-Range-Imaging-with-a-Polarization-Camera" class="headerlink" title="Snapshot High Dynamic Range Imaging with a Polarization Camera"></a>Snapshot High Dynamic Range Imaging with a Polarization Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08094">http://arxiv.org/abs/2308.08094</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Intelligent-Sensing/polarization-hdr">https://github.com/Intelligent-Sensing/polarization-hdr</a></li>
<li>paper_authors: Mingyang Xie, Matthew Chan, Christopher Metzler</li>
<li>for: 这篇论文的目的是把一个常见的 polarization 相机转化为高性能的 HDR 相机。</li>
<li>methods: 这篇论文使用了一种简单 yet 高效的方法，通过在 polarization 相机前置一个线性激光 polarizer，同时捕捉四个不同曝光的图像，并使用一种 robust 和自适应的算法来重建 HDR 图像（具有单一的极性）。</li>
<li>results: 论文的实验结果表明，这种方法可以有效地提高 HDR 图像的质量，并且可以在实际世界中进行广泛的应用。<details>
<summary>Abstract</summary>
High dynamic range (HDR) images are important for a range of tasks, from navigation to consumer photography. Accordingly, a host of specialized HDR sensors have been developed, the most successful of which are based on capturing variable per-pixel exposures. In essence, these methods capture an entire exposure bracket sequence at once in a single shot. This paper presents a straightforward but highly effective approach for turning an off-the-shelf polarization camera into a high-performance HDR camera. By placing a linear polarizer in front of the polarization camera, we are able to simultaneously capture four images with varied exposures, which are determined by the orientation of the polarizer. We develop an outlier-robust and self-calibrating algorithm to reconstruct an HDR image (at a single polarity) from these measurements. Finally, we demonstrate the efficacy of our approach with extensive real-world experiments.
</details>
<details>
<summary>摘要</summary>
高动态范围（HDR）图像在多种任务中具有重要作用，从导航到消费类摄影。因此，一系列专门设计 дляHDR感知器被开发出来，最成功的是基于变量每像素曝光的方法。在本文中，我们提出了将普通的偏振相机转化为高性能HDR摄影机的简单 yet highly effectiveapproach。通过在偏振相机前置一个直线偏振器，我们能够同时捕捉四个不同曝光的图像，这些图像的曝光是偏振器的 orientations 所决定的。我们开发了一种耐异常和自适应算法，用于从这些测量中重建HDR图像（在单一的偏振 polarity 下）。最后，我们通过广泛的实验证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Framework-for-Spleen-Volume-Estimation-from-2D-Cross-sectional-Views"><a href="#Deep-Learning-Framework-for-Spleen-Volume-Estimation-from-2D-Cross-sectional-Views" class="headerlink" title="Deep Learning Framework for Spleen Volume Estimation from 2D Cross-sectional Views"></a>Deep Learning Framework for Spleen Volume Estimation from 2D Cross-sectional Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08038">http://arxiv.org/abs/2308.08038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Yuan, Esther Puyol-Anton, Haran Jogeesvaran, Baba Inusa, Andrew P. King</li>
<li>for: This paper aims to develop a method for automated spleen volume measurement from 2D cross-sectional segmentations obtained from ultrasound imaging, which can be used to assess splenomegaly and related clinical conditions.</li>
<li>methods: The proposed method uses a variational autoencoder-based framework to measure spleen volume from single- or dual-view 2D spleen segmentations. Three volume estimation methods are proposed and evaluated within this framework.</li>
<li>results: The best model achieved mean relative volume accuracies of 86.62% and 92.58% for single- and dual-view segmentations, respectively, surpassing the performance of the clinical standard approach of linear regression using manual measurements and a comparative deep learning-based 2D-3D reconstruction-based approach. The proposed method can be integrated into standard clinical workflows which currently use 2D ultrasound images to measure spleen length.<details>
<summary>Abstract</summary>
Abnormal spleen enlargement (splenomegaly) is regarded as a clinical indicator for a range of conditions, including liver disease, cancer and blood diseases. While spleen length measured from ultrasound images is a commonly used surrogate for spleen size, spleen volume remains the gold standard metric for assessing splenomegaly and the severity of related clinical conditions. Computed tomography is the main imaging modality for measuring spleen volume, but it is less accessible in areas where there is a high prevalence of splenomegaly (e.g., the Global South). Our objective was to enable automated spleen volume measurement from 2D cross-sectional segmentations, which can be obtained from ultrasound imaging. In this study, we describe a variational autoencoder-based framework to measure spleen volume from single- or dual-view 2D spleen segmentations. We propose and evaluate three volume estimation methods within this framework. We also demonstrate how 95% confidence intervals of volume estimates can be produced to make our method more clinically useful. Our best model achieved mean relative volume accuracies of 86.62% and 92.58% for single- and dual-view segmentations, respectively, surpassing the performance of the clinical standard approach of linear regression using manual measurements and a comparative deep learning-based 2D-3D reconstruction-based approach. The proposed spleen volume estimation framework can be integrated into standard clinical workflows which currently use 2D ultrasound images to measure spleen length. To the best of our knowledge, this is the first work to achieve direct 3D spleen volume estimation from 2D spleen segmentations.
</details>
<details>
<summary>摘要</summary>
非常常见的脾膜肥大（splenomegaly）被视为临床指标，用于诊断多种疾病，如肝病、癌症和血液疾病。而脾膜长度从ultrasound图像中得到的是通常用于脾膜大小的临床标准，但脾膜体积仍是评估splenomegaly和相关临床病情的金标准指标。计算机断层成像是评估脾膜体积的主要成像方法，但在有高发率的脾膜肥大（如全球南部）的地区，计算机断层成像更加不可accessible。我们的目标是启用自动化脾膜体积量计算，从2D横截图像中获得的分割。我们描述了基于variational autoencoder的框架，用于从单个或双视2D脾膜分割中计算脾膜体积。我们提出了三种体积估计方法，并评估了这些方法的性能。我们还示出了在95%信度范围内生成体积估计的方法，以使我们的方法更加临床有用。我们的最佳模型在单视和双视分割中达到了86.62%和92.58%的相对体积准确率，超过了临床标准方法的线性回归使用手动测量和相比之下的深度学习基于2D-3D重建的方法。我们的提议的脾膜体积估计框架可以与现有的仅使用2D ultrasound图像来测量脾膜长度的临床工作流程集成。到目前为止，这是首次直接从2D脾膜分割中 estimate 3D脾膜体积的方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/eess.IV_2023_08_16/" data-id="cllurrpcl00dzsw8873ql5uul" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/cs.SD_2023_08_15/" class="article-date">
  <time datetime="2023-08-14T16:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/cs.SD_2023_08_15/">cs.SD - 2023-08-15 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Preliminary-investigation-of-the-short-term-in-situ-performance-of-an-automatic-masker-selection-system"><a href="#Preliminary-investigation-of-the-short-term-in-situ-performance-of-an-automatic-masker-selection-system" class="headerlink" title="Preliminary investigation of the short-term in situ performance of an automatic masker selection system"></a>Preliminary investigation of the short-term in situ performance of an automatic masker selection system</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07767">http://arxiv.org/abs/2308.07767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bhan Lam, Zhen-Ting Ong, Kenneth Ooi, Wen-Hui Ong, Trevor Wong, Karn N. Watcharasupat, Woon-Seng Gan</li>
<li>for: 这个论文旨在提高室内声学环境的听觉舒适性，通过引入愿景声音来mask掉干扰声音。</li>
<li>methods: 这个研究使用了一种深度学习模型，该模型基于大规模的主观反馈数据来选择最佳的声音masker，以提高 derive ISO 舒适性（ISO 12913-2）。</li>
<li>results: 研究发现，使用自动声音masker选择系统（AMSS）可以在室内声学环境中提高听觉舒适性，并且可以根据不同的masker选择策略来调整听觉体验。<details>
<summary>Abstract</summary>
Soundscape augmentation or "masking" introduces wanted sounds into the acoustic environment to improve acoustic comfort. Usually, the masker selection and playback strategies are either arbitrary or based on simple rules (e.g. -3 dBA), which may lead to sub-optimal increment or even reduction in acoustic comfort for dynamic acoustic environments. To reduce ambiguity in the selection of maskers, an automatic masker selection system (AMSS) was recently developed. The AMSS uses a deep-learning model trained on a large-scale dataset of subjective responses to maximize the derived ISO pleasantness (ISO 12913-2). Hence, this study investigates the short-term in situ performance of the AMSS implemented in a gazebo in an urban park. Firstly, the predicted ISO pleasantness from the AMSS is evaluated in comparison to the in situ subjective evaluation scores. Secondly, the effect of various masker selection schemes on the perceived affective quality and appropriateness would be evaluated. In total, each participant evaluated 6 conditions: (1) ambient environment with no maskers; (2) AMSS; (3) bird and (4) water masker from prior art; (5) random selection from same pool of maskers used to train the AMSS; and (6) selection of best-performing maskers based on the analysis of the dataset used to train the AMSS.
</details>
<details>
<summary>摘要</summary>
增强声景或"遮盾"技术可以提高室外环境的声音舒适度。通常，选择和播放遮盾的策略是随机或基于简单的规则（例如,-3 dBA），这可能会导致室外环境的声音舒适度下降。为了减少遮盾选择的模糊性，一种自动遮盾选择系统（AMSS）已经开发出来。AMSS使用基于大规模的主观反馈数据集进行训练，以最大化 derivated ISO 舒适度（ISO 12913-2）。因此，本研究探讨了在公园内的加固声景中实现的 AMSS 的短期实际性。首先，由 AMSS 预测的 ISO 舒适度与现场评估得分进行比较。其次，通过不同遮盾选择方案对人们对声音舒适度和适应度的认知进行评估。总共，每名参与者评价了6个条件：（1）无遮盾的室外环境；（2） AMSS；（3）鸟声和水声遮盾从优秀艺术中；（4）随机选择从同一个数据集中训练 AMSS 中的遮盾；（5）基于分析数据集来选择最佳的遮盾；以及（6）遮盾选择基于主观反馈数据集中的分析。
</details></li>
</ul>
<hr>
<h2 id="Improving-CTC-AED-model-with-integrated-CTC-and-auxiliary-loss-regularization"><a href="#Improving-CTC-AED-model-with-integrated-CTC-and-auxiliary-loss-regularization" class="headerlink" title="Improving CTC-AED model with integrated-CTC and auxiliary loss regularization"></a>Improving CTC-AED model with integrated-CTC and auxiliary loss regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08449">http://arxiv.org/abs/2308.08449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daobin Zhu, Xiangdong Su, Hongbin Zhang</li>
<li>for:  automatic speech recognition (ASR)</li>
<li>methods:  connectionist temporal classification (CTC) and attention-based encoder decoder (AED) joint training, integrated-CTC with attention mechanism, direct addition of logits (DAL) and preserving the maximum probability (PMP) fusion methods, auxiliary loss regularization</li>
<li>results:  DAL method performs better in attention rescoring, PMP method excels in CTC prefix beam search and greedy search.<details>
<summary>Abstract</summary>
Connectionist temporal classification (CTC) and attention-based encoder decoder (AED) joint training has been widely applied in automatic speech recognition (ASR). Unlike most hybrid models that separately calculate the CTC and AED losses, our proposed integrated-CTC utilizes the attention mechanism of AED to guide the output of CTC. In this paper, we employ two fusion methods, namely direct addition of logits (DAL) and preserving the maximum probability (PMP). We achieve dimensional consistency by adaptively affine transforming the attention results to match the dimensions of CTC. To accelerate model convergence and improve accuracy, we introduce auxiliary loss regularization for accelerated convergence. Experimental results demonstrate that the DAL method performs better in attention rescoring, while the PMP method excels in CTC prefix beam search and greedy search.
</details>
<details>
<summary>摘要</summary>
Connectionist temporal classification (CTC) 和 attention-based encoder decoder (AED) 的合作训练在自动语音识别（ASR）中广泛应用。不同于大多数混合模型，我们的提议的集成-CTC 使用 AED 的注意机制来导引 CTCP 输出。在这篇论文中，我们使用两种融合方法，即直接加法 Logits (DAL) 和保留最大概率 (PMP)。我们通过适应缩放注意结果以匹配 CTCP 的维度，实现维度一致。为加速模型聚合和提高准确性，我们引入了 auxiliary loss regularization 的加速减速。实验结果表明， DAL 方法在注意点重新评分中表现更好，而 PMP 方法在 CTC 前缀搜索和恰恰搜索中表现更好。
</details></li>
</ul>
<hr>
<h2 id="Using-Text-Injection-to-Improve-Recognition-of-Personal-Identifiers-in-Speech"><a href="#Using-Text-Injection-to-Improve-Recognition-of-Personal-Identifiers-in-Speech" class="headerlink" title="Using Text Injection to Improve Recognition of Personal Identifiers in Speech"></a>Using Text Injection to Improve Recognition of Personal Identifiers in Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07393">http://arxiv.org/abs/2308.07393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yochai Blau, Rohan Agrawal, Lior Madmony, Gary Wang, Andrew Rosenberg, Zhehuai Chen, Zorik Gekhman, Genady Beryozkin, Parisa Haghani, Bhuvana Ramabhadran</li>
<li>for: 这些研究旨在提高自动语音识别（ASR）系统中对人名、日期等个人特定类别的识别精度。</li>
<li>methods: 这些研究使用文本插入法，将个人特定信息替换为假文本，以提高ASR模型对这些类别的识别精度。</li>
<li>results: 研究显示，使用文本插入法可以大幅提高名和日期在医疗笔记中的回忆率，并提高总的音标识别率。对于字符串数字序列，我们也证明了字符错误率和句子准确率的改进。<details>
<summary>Abstract</summary>
Accurate recognition of specific categories, such as persons' names, dates or other identifiers is critical in many Automatic Speech Recognition (ASR) applications. As these categories represent personal information, ethical use of this data including collection, transcription, training and evaluation demands special care. One way of ensuring the security and privacy of individuals is to redact or eliminate Personally Identifiable Information (PII) from collection altogether. However, this results in ASR models that tend to have lower recognition accuracy of these categories. We use text-injection to improve the recognition of PII categories by including fake textual substitutes of PII categories in the training data using a text injection method. We demonstrate substantial improvement to Recall of Names and Dates in medical notes while improving overall WER. For alphanumeric digit sequences we show improvements to Character Error Rate and Sentence Accuracy.
</details>
<details>
<summary>摘要</summary>
正确地识别具体类别，如人姓名、日期等，在自动话语识别（ASR）应用中是非常重要的。这些类别代表个人资料，因此在收集、转换、训练和评估过程中，需要特殊的关注和保证。一种确保个人隐私和安全的方法是删除个人识别信息（PII），但这会导致ASR模型的识别率下降。我们使用文本对替法以提高PII类别的识别率，通过在训练数据中添加伪文本替代PII类别的方法。我们在医疗纪录中展示了大幅提高名称和日期的回传率，而且提高了整体Word Error Rate（WER）。对于字母数字序列，我们显示了字母错误率和句子准确率的提高。
</details></li>
</ul>
<hr>
<h2 id="Localization-of-DOA-trajectories-–-Beyond-the-grid"><a href="#Localization-of-DOA-trajectories-–-Beyond-the-grid" class="headerlink" title="Localization of DOA trajectories – Beyond the grid"></a>Localization of DOA trajectories – Beyond the grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07265">http://arxiv.org/abs/2308.07265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruchi Pandey, Santosh Nannuru</li>
<li>for: 本研究旨在提出两种轨迹模型，用于捕捉DOA动态信息。</li>
<li>methods: 我们采用了两种非格式的搜索算法，分别是Sliding Frank-Wolfe（SFW）和Newtonized Orthogonal Matching Pursuit（NOMP），用于估计轨迹参数。</li>
<li>results: 实验结果表明，提出的轨迹localization算法在噪音环境下比传统格式化方法具有更高的分解能力、更好的鲁棒性和更高的计算效率。<details>
<summary>Abstract</summary>
The direction of arrival (DOA) estimation algorithms are crucial in localizing acoustic sources. Traditional localization methods rely on block-level processing to extract the directional information from multiple measurements processed together. However, these methods assume that DOA remains constant throughout the block, which may not be true in practical scenarios. Also, the performance of localization methods is limited when the true parameters do not lie on the parameter search grid. In this paper we propose two trajectory models, namely the polynomial and bandlimited trajectory models, to capture the DOA dynamics. To estimate trajectory parameters, we adopt two gridless algorithms: i) Sliding Frank-Wolfe (SFW), which solves the Beurling LASSO problem and ii) Newtonized Orthogonal Matching Pursuit (NOMP), which improves over OMP using cyclic refinement. Furthermore, we extend our analysis to include wideband processing. The simulation results indicate that the proposed trajectory localization algorithms exhibit improved performance compared to grid-based methods in terms of resolution, robustness to noise, and computational efficiency.
</details>
<details>
<summary>摘要</summary>
“DOA估计算法是音频源位置Localization的关键。传统的Localization方法 rely on block-level processing来提取方向信息，但这些方法假设DOA在封页中保持不变，这可能不符合实际情况。此外，Localization方法的性能受到真实参数不在搜寻格子中的限制。在这篇论文中，我们提出了两个抽象曲线模型， namely 多项式和带限抽象曲线模型，来捕捉DOA动态。来估计抽象曲线参数，我们采用了两种无格线性Algorithms：i) Sliding Frank-Wolfe（SFW），解决了Beurling LASSO问题，ii) Newtonized Orthogonal Matching Pursuit（NOMP），将OMP提高使用cyclic refinement。此外，我们将分析扩展到宽频处理。实验结果显示，提案的抽象曲线Localization算法在grid-based方法的扩展性、类比噪音耐性和计算效率上表现出优化的性能。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Compositional-nonlinear-audio-signal-processing-with-Volterra-series"><a href="#Compositional-nonlinear-audio-signal-processing-with-Volterra-series" class="headerlink" title="Compositional nonlinear audio signal processing with Volterra series"></a>Compositional nonlinear audio signal processing with Volterra series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07229">http://arxiv.org/abs/2308.07229</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jake Araujo-Simon</li>
<li>for: 这篇论文旨在开发一种基于Volterra序列的非线性音频信号处理理论，并通过函数器化Volterra序列来描述非线性变换的输出结果如何受到输入信号的线性处理的影响。</li>
<li>methods: 这篇论文使用了Volterra序列的 categorification 来描述非线性音频系统的变化，并引入了一种基于Volterra序列的折射 map 来模型非线性系统的变化。</li>
<li>results: 这篇论文显示了 Volterra序列和其 morphisms 组织成一个category，并证明了这个category的操作是可逆的。此外，论文还将这个框架与时间频分析相关的一种二阶Volterra序列之间的相似性进行了映射。<details>
<summary>Abstract</summary>
We develop a compositional theory of nonlinear audio signal processing based on a categorification of the Volterra series. We begin by considering what it would mean for the Volterra series to be functorial with respect to a base category whose objects are temperate distributions and whose morphisms are certain linear transformations. This leads to formulae describing how the outcomes of nonlinear transformations are affected if their input signals are first linearly processed. We then consider how nonlinear audio systems change, and introduce as a model thereof a notion of morphism of Volterra series, which we exhibit as a kind of lens map. We show how morphisms can be parameterized and used to generate indexed families of Volterra series, which are well-suited to model nonstationary or time-varying nonlinear phenomena. We then describe how Volterra series and their morphisms organize into a category, which we call Volt. We exhibit the operations of sum, product, and series composition of Volterra series as monoidal products on Volt and identify, for each in turn, its corresponding universal property. We show, in particular, that the series composition of Volterra series is associative. We then bridge between our framework and a subject at the heart of audio signal processing: time-frequency analysis. Specifically, we show that an equivalence between a certain class of second-order Volterra series and the bilinear time-frequency distributions (TFDs) can be extended to one between certain higher-order Volterra series and the so-called polynomial TFDs. We end with prospects for future work, including the incorporation of nonlinear system identification techniques and the extension of our theory to the settings of compositional graph and topological audio signal processing.
</details>
<details>
<summary>摘要</summary>
我们开发了一种基于Volterra系列的非线性音频信号处理的 Compositional 理论。我们开始是通过考虑Volterra系列是否能够作为函手，对具有温度分布的对象和 linear 转换的 morphism 进行定义。这导致了输入信号先进行线性处理后的非线性转换的结果的方程。我们然后考虑了非线性音频系统的变化，并引入了 Volterra 系列中的 morphism 模型，它可以看作一种类似于镜像函数的概念。我们还示了如何使用参数化的 morphism 来生成标注的 Volterra 系列家族，这些系列家族适用于模elling 非站ARY或时间变化的非线性现象。然后，我们描述了 Volterra 系列和它们的 morphism 组织成一个类型，我们称之为 Volt。我们还证明了 Volt 中的 serie  компози�io 和乘法操作是可逆的，并且可以通过 monoidal 乘法来实现。最后，我们将我们的框架与音频信号处理中的一个重要主题相关：时间频分析。我们证明了一种等价关系，它将一类二阶 Volterra 系列与时间频分布（TFD）相对应，并且可以推广到高阶 Volterra 系列和谱分布（Poly-TFD）。我们结束于未来工作的展望，包括将非线性系统识别技术 incorporated 到我们的理论中，以及扩展我们的理论到compositional graph 和 topological audio signal processing 的设置。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/cs.SD_2023_08_15/" data-id="cllurrpb8009dsw889h47dohr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/eess.AS_2023_08_15/" class="article-date">
  <time datetime="2023-08-14T16:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/eess.AS_2023_08_15/">eess.AS - 2023-08-15 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GIST-AiTeR-Speaker-Diarization-System-for-VoxCeleb-Speaker-Recognition-Challenge-VoxSRC-2023"><a href="#GIST-AiTeR-Speaker-Diarization-System-for-VoxCeleb-Speaker-Recognition-Challenge-VoxSRC-2023" class="headerlink" title="GIST-AiTeR Speaker Diarization System for VoxCeleb Speaker Recognition Challenge (VoxSRC) 2023"></a>GIST-AiTeR Speaker Diarization System for VoxCeleb Speaker Recognition Challenge (VoxSRC) 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07788">http://arxiv.org/abs/2308.07788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongkeon Park, Ji Won Kim, Kang Ryeol Kim, Do Hyun Lee, Hong Kook Kim</li>
<li>for: 这份报告描述了GIST-AiTeR团队在2023年VOXCELEB speaker recognition挑战（VoxSRC-23）Track 4的提交系统。</li>
<li>methods: 该提交系统主要实现多种说话人分类（SD）技术，包括ResNet293和MFA-Conformer，以及不同的段和跳长组合。然后，这些模型被组合成ensemble模型。</li>
<li>results: ResNet293和MFA-Conformer模型在VAL46上的分类错误率（DER）分别为3.65%和3.83%，而提交的ensemble模型在VAL46上的DER为3.50%，在VoxSRC-23测试集上的DER为4.88%。<details>
<summary>Abstract</summary>
This report describes the submission system by the GIST-AiTeR team for the VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC-23) Track 4. Our submission system focuses on implementing diverse speaker diarization (SD) techniques, including ResNet293 and MFA-Conformer with different combinations of segment and hop length. Then, those models are combined into an ensemble model. The ResNet293 and MFA-Conformer models exhibited the diarization error rates (DERs) of 3.65% and 3.83% on VAL46, respectively. The submitted ensemble model provided a DER of 3.50% on VAL46, and consequently, it achieved a DER of 4.88% on the VoxSRC-23 test set.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-DKU-MSXF-Diarization-System-for-the-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#The-DKU-MSXF-Diarization-System-for-the-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="The DKU-MSXF Diarization System for the VoxCeleb Speaker Recognition Challenge 2023"></a>The DKU-MSXF Diarization System for the VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07595">http://arxiv.org/abs/2308.07595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Cheng, Weiqing Wang, Xiaoyi Qin, Yuke Lin, Ning Jiang, Guoqing Zhao, Ming Li</li>
<li>for: 这篇论文是为了描述DKU-MSXF在VoxCeleb Speaker Recognition Challenge 2023（VoxSRC-23）的识别挑战中的提交。</li>
<li>methods: 该系统管道包括语音活动检测、分 clustering-based diarization、重叠语音检测和目标说话人活动检测，每个过程都有3个子模型的拟合输出。</li>
<li>results: 最终，我们使用DOVER-Lap进行不同的 clustering-based和TSVAD-based diarization系统的融合，并 achieved the 4.30% diarization error rate（DER），在track 4的挑战领导者榜单上名列第一。<details>
<summary>Abstract</summary>
This paper describes the DKU-MSXF submission to track 4 of the VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC-23). Our system pipeline contains voice activity detection, clustering-based diarization, overlapped speech detection, and target-speaker voice activity detection, where each procedure has a fused output from 3 sub-models. Finally, we fuse different clustering-based and TSVAD-based diarization systems using DOVER-Lap and achieve the 4.30% diarization error rate (DER), which ranks first place on track 4 of the challenge leaderboard.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文描述了DKU-MSXF在VoxCeleb Speaker Recognition Challenge 2023（VoxSRC-23）的评测4号track上的提交。我们的系统管道包括语音活动检测、集群化基于分类的分解、重叠 speech检测和目标说话人语音活动检测，每个过程有3个子模型的复合输出。最后，我们将不同的集群化基于和TSVAD基于的分解系统结合使用DOVER-Lap，实现了4.30%的分解错误率（DER），排名评测4号track的排名板块第一名。
</details></li>
</ul>
<hr>
<h2 id="AKVSR-Audio-Knowledge-Empowered-Visual-Speech-Recognition-by-Compressing-Audio-Knowledge-of-a-Pretrained-Model"><a href="#AKVSR-Audio-Knowledge-Empowered-Visual-Speech-Recognition-by-Compressing-Audio-Knowledge-of-a-Pretrained-Model" class="headerlink" title="AKVSR: Audio Knowledge Empowered Visual Speech Recognition by Compressing Audio Knowledge of a Pretrained Model"></a>AKVSR: Audio Knowledge Empowered Visual Speech Recognition by Compressing Audio Knowledge of a Pretrained Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07593">http://arxiv.org/abs/2308.07593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeong Hun Yeo, Minsu Kim, Jeongsoo Choi, Dae Hoe Kim, Yong Man Ro<br>for: 这篇论文的目的是提出一个可以补充视觉特征不足的语音识别框架，使用音频特征来补充视觉特征。methods: 本文提出的方法包括使用大量预训的音频模型所编码的丰富音频知识，将音频知识储存在单簇音频内存中，并使用音桥模组寻找最佳音频特征。results: 根据实验结果，本文获得了新的州际顶尖性能在广泛使用的数据集LRS2和LRS3上。<details>
<summary>Abstract</summary>
Visual Speech Recognition (VSR) is the task of predicting spoken words from silent lip movements. VSR is regarded as a challenging task because of the insufficient information on lip movements. In this paper, we propose an Audio Knowledge empowered Visual Speech Recognition framework (AKVSR) to complement the insufficient speech information of visual modality by using audio modality. Different from the previous methods, the proposed AKVSR 1) utilizes rich audio knowledge encoded by a large-scale pretrained audio model, 2) saves the linguistic information of audio knowledge in compact audio memory by discarding the non-linguistic information from the audio through quantization, and 3) includes Audio Bridging Module which can find the best-matched audio features from the compact audio memory, which makes our training possible without audio inputs, once after the compact audio memory is composed. We validate the effectiveness of the proposed method through extensive experiments, and achieve new state-of-the-art performances on the widely-used datasets, LRS2 and LRS3.
</details>
<details>
<summary>摘要</summary>
visible speech recognition (VSR) 是指根据声径 lip 运动估计说出来的 spoken words。 VSR 被视为一个具有挑战性的任务，因为 lip 运动的信息不充分。 在这篇文章中，我们提出了一个 Audio Knowledge 强化 visual speech recognition 框架 (AKVSR)，以优化视觉模式中的不充分信息。 相比之前的方法，我们的 AKVSR 采用了大规模预训 audio 模型所编码的丰富 audio knowledge，将 audio 知识储存在可靠的 audio 快照中，并通过归一化 Audio Bridging Module 找出最佳对应的 audio 特征，使我们在训练时可以不需要 audio 输入，仅需要一次将 compact audio 快照组建起来。 我们透过广泛的实验证明了 AKVSR 的有效性，并在 widely-used 的 dataset 上 achieve 新的 state-of-the-art 性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/eess.AS_2023_08_15/" data-id="cllurrpbs00b7sw880sj1cl1i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/eess.IV_2023_08_15/" class="article-date">
  <time datetime="2023-08-14T16:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/eess.IV_2023_08_15/">eess.IV - 2023-08-15 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Targeted-Multispectral-Filter-Array-Design-for-Endoscopic-Cancer-Detection-in-the-Gastrointestinal-Tract"><a href="#Targeted-Multispectral-Filter-Array-Design-for-Endoscopic-Cancer-Detection-in-the-Gastrointestinal-Tract" class="headerlink" title="Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract"></a>Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07947">http://arxiv.org/abs/2308.07947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaela Taylor-Williams, Ran Tao, Travis W Sawyer, Dale J Waterhouse, Jonghee Yoon, Sarah E Bohndiek<br>for: This paper aims to improve the detection of colour differences in the gastrointestinal tract during white light endoscopy (WLE) for early cancer detection.methods: The authors propose using custom multispectral filter arrays (MSFAs) in an endoscopic chip-on-tip configuration to target alternative colours for imaging and improve contrast. They use an open-source toolbox, Opti-MSFA, to optimize the design of MSFAs for early cancer detection in the gastrointestinal tract.results: The authors found that the MSFA designs have high classification accuracies, suggesting that future implementation in endoscopy hardware could potentially enable improved early detection of disease in the gastrointestinal tract during routine screening and surveillance. The optimal MSFA configurations can achieve similar classification accuracies as the full spectral data in a simpler hardware implementation.<details>
<summary>Abstract</summary>
Colour differences between healthy and diseased tissue in the gastrointestinal tract are detected visually by clinicians during white light endoscopy (WLE); however, the earliest signs of disease are often just a slightly different shade of pink compared to healthy tissue. Here, we propose to target alternative colours for imaging to improve contrast using custom multispectral filter arrays (MSFAs) that could be deployed in an endoscopic chip-on-tip configuration. Using an open-source toolbox, Opti-MSFA, we examined the optimal design of MSFAs for early cancer detection in the gastrointestinal tract. The toolbox was first extended to use additional classification models (k-Nearest Neighbour, Support Vector Machine, and Spectral Angle Mapper). Using input spectral data from published clinical trials examining the oesophagus and colon, we optimised the design of MSFAs with 3 to 9 different bands. We examined the variation of the spectral and spatial classification accuracy as a function of number of bands. The MSFA designs have high classification accuracies, suggesting that future implementation in endoscopy hardware could potentially enable improved early detection of disease in the gastrointestinal tract during routine screening and surveillance. Optimal MSFA configurations can achieve similar classification accuracies as the full spectral data in an implementation that could be realised in far simpler hardware. The reduced number of spectral bands could enable future deployment of multispectral imaging in an endoscopic chip-on-tip configuration.
</details>
<details>
<summary>摘要</summary>
医生在白光终端 scopes 中可以通过视觉检测，健康和疾病组织之间的颜色差异。然而，初期疾病往往只是与健康组织颜色有些微差异。我们提议使用自定义多спектルFilter array (MSFA) 来提高冲击。我们使用了开源工具箱 Opti-MSFA，对多种分类模型（k-最近邻居、支持向量机和spectral angle mapper）进行了扩展。使用来自已发表临床试验的输入光谱数据，我们优化了MSFA的设计，并考虑了3-9个频谱带。我们研究了随着带数的变化，多спектル和空间分类精度的变化。我们发现，MSFA的设计具有高分类精度，表明将来在Routine检查和监测中实施可能会提高肠道疾病的早期检测。最佳MSFA配置可以实现与全光谱数据相同的分类精度，但具有更少的spectral带。这可能将未来在检查终端中实现多спектル成像。
</details></li>
</ul>
<hr>
<h2 id="DSFNet-Convolutional-Encoder-Decoder-Architecture-Combined-Dual-GCN-and-Stand-alone-Self-attention-by-Fast-Normalized-Fusion-for-Polyps-Segmentation"><a href="#DSFNet-Convolutional-Encoder-Decoder-Architecture-Combined-Dual-GCN-and-Stand-alone-Self-attention-by-Fast-Normalized-Fusion-for-Polyps-Segmentation" class="headerlink" title="DSFNet: Convolutional Encoder-Decoder Architecture Combined Dual-GCN and Stand-alone Self-attention by Fast Normalized Fusion for Polyps Segmentation"></a>DSFNet: Convolutional Encoder-Decoder Architecture Combined Dual-GCN and Stand-alone Self-attention by Fast Normalized Fusion for Polyps Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07946">http://arxiv.org/abs/2308.07946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juntong Fan, Tieyong Zeng, Dayang Wang<br>for:* This paper is written for the task of colon polyp segmentation in medical images.methods:* The proposed method, called DSFNet, uses a U-shaped network architecture that combines the advantages of Dual-GCN and self-attention mechanisms.* The method includes a feature enhancement block module based on Dual-GCN, a stand-alone self-attention module, and a Fast Normalized Fusion method with trainable weights.results:* The proposed model outperforms other state-of-the-art models on two public datasets, Endoscene and Kvasir-SEG, in terms of Dice, MAE, and IoU.* Ablation studies demonstrate the efficacy and effectiveness of each module in the proposed method.* The proposed method has great clinical significance for colon polyp segmentation in medical images.Here is the simplified Chinese text for the three key information points:for:* 这篇论文是为了医学图像中的colon polyp segmentation任务而写的。methods:* 提议的方法，即DSFNet，使用了U型网络架构，combines了 dual-GCN和自注意机制的优点。* 方法包括了Feature Enhancement Block模块、stand-alone自注意模块和 Fast Normalized Fusion方法。results:* 提议的模型在两个公共数据集上，包括Endoscene和Kvasir-SEG，与其他状态对比较模型的Dice、MAE和IoU指标上表现出色。* 简洗研究表明每个模块的有效性和效果。* 提议的方法在医学图像中的colon polyp segmentation任务中具有丰富的临床意义。<details>
<summary>Abstract</summary>
In the past few decades, deep learning technology has been widely used in medical image segmentation and has made significant breakthroughs in the fields of liver and liver tumor segmentation, brain and brain tumor segmentation, video disc segmentation, heart image segmentation, and so on. However, the segmentation of polyps is still a challenging task since the surface of the polyps is flat and the color is very similar to that of surrounding tissues. Thus, It leads to the problems of the unclear boundary between polyps and surrounding mucosa, local overexposure, and bright spot reflection. To counter this problem, this paper presents a novel U-shaped network, namely DSFNet, which effectively combines the advantages of Dual-GCN and self-attention mechanisms. First, we introduce a feature enhancement block module based on Dual-GCN module as an attention mechanism to enhance the feature extraction of local spatial and structural information with fine granularity. Second, the stand-alone self-attention module is designed to enhance the integration ability of the decoding stage model to global information. Finally, the Fast Normalized Fusion method with trainable weights is used to efficiently fuse the corresponding three feature graphs in encoding, bottleneck, and decoding blocks, thus promoting information transmission and reducing the semantic gap between encoder and decoder. Our model is tested on two public datasets including Endoscene and Kvasir-SEG and compared with other state-of-the-art models. Experimental results show that the proposed model surpasses other competitors in many indicators, such as Dice, MAE, and IoU. In the meantime, ablation studies are also conducted to verify the efficacy and effectiveness of each module. Qualitative and quantitative analysis indicates that the proposed model has great clinical significance.
</details>
<details>
<summary>摘要</summary>
在过去几十年，深度学习技术在医学影像分割领域得到广泛应用，并在肝脏和肝肿瘤分割、脑和脑肿瘤分割、视频碟分割、心脏影像分割等领域取得了 significiant 的突破。然而，肿瘤分割仍然是一项具有挑战性的任务，因为肿瘤表面平整，颜色与周围组织相似，导致分割边界不清晰、局部过曝和耀光反射等问题。为解决这些问题，本文提出了一种新的U型网络模型，即DSFNet，该模型有效地结合了 dual-GCN 模块和自注意机制。首先，我们引入了基于 dual-GCN 模块的特征增强块模块，以增强本地空间和结构信息的特征提取。其次，我们设计了独立的自注意模块，以提高解码阶段模型的全局信息整合能力。最后，我们使用可训练权重的快速归一化方法，以效率地融合相应的三个特征图，从而促进信息传递和减少编码器和解码器之间的 semantic gap。我们的模型在两个公共数据集上进行测试，包括 Endoscene 和 Kvasir-SEG，并与其他当前领先模型进行比较。实验结果显示，提出的模型在多个指标上超越了其他竞争对手，如 Dice、MAE 和 IoU。同时，我们还进行了归并分析，以验证模块的有效性和效果。Qualitative 和量化分析表明，我们的模型在临床上具有重要的意义。
</details></li>
</ul>
<hr>
<h2 id="An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease"><a href="#An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease"></a>An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07778">http://arxiv.org/abs/2308.07778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Kang, Bo Li, Janne M. Papma, Lize C. Jiskoot, Peter Paul De Deyn, Geert Jan Biessels, Jurgen A. H. R. Claassen, Huub A. M. Middelkoop, Wiesje M. van der Flier, Inez H. G. B. Ramakers, Stefan Klein, Esther E. Bron</li>
<li>for: 预测阿尔茨染色体疾病 (AD) 的早期诊断。</li>
<li>methods: 使用可解释的搜索机器学习模型（EBM）和深度学习Feature抽取来结合高维成像数据。</li>
<li>results: 在ADNI数据集上实现了0.883的准确率和0.970的AUC，在一个外部测试集上实现了0.778的准确率和0.887的AUC。<details>
<summary>Abstract</summary>
Machine learning methods have shown large potential for the automatic early diagnosis of Alzheimer's Disease (AD). However, some machine learning methods based on imaging data have poor interpretability because it is usually unclear how they make their decisions. Explainable Boosting Machines (EBMs) are interpretable machine learning models based on the statistical framework of generalized additive modeling, but have so far only been used for tabular data. Therefore, we propose a framework that combines the strength of EBM with high-dimensional imaging data using deep learning-based feature extraction. The proposed framework is interpretable because it provides the importance of each feature. We validated the proposed framework on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, achieving accuracy of 0.883 and area-under-the-curve (AUC) of 0.970 on AD and control classification. Furthermore, we validated the proposed framework on an external testing set, achieving accuracy of 0.778 and AUC of 0.887 on AD and subjective cognitive decline (SCD) classification. The proposed framework significantly outperformed an EBM model using volume biomarkers instead of deep learning-based features, as well as an end-to-end convolutional neural network (CNN) with optimized architecture.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression"><a href="#Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression" class="headerlink" title="Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression"></a>Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07733">http://arxiv.org/abs/2308.07733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llvy21/duic">https://github.com/llvy21/duic</a></li>
<li>paper_authors: Yue Lv, Jinxi Xiang, Jun Zhang, Wenming Yang, Xiao Han, Wei Yang</li>
<li>for: 提高图像压缩的环境适应性和Rate-Distortion性能</li>
<li>methods: 使用低级matrix decomposition更新客户端解码器的适应参数，并通过动态网格决定哪些解码层需要适应。</li>
<li>results: 对于不同的图像数据集，提高了环境适应性和Rate-Distortion性能，相比非适应方法的平均BD-rate改善约19%，并且比最先进的实例适应方法提高了约5%的BD-rate。<details>
<summary>Abstract</summary>
The latest advancements in neural image compression show great potential in surpassing the rate-distortion performance of conventional standard codecs. Nevertheless, there exists an indelible domain gap between the datasets utilized for training (i.e., natural images) and those utilized for inference (e.g., artistic images). Our proposal involves a low-rank adaptation approach aimed at addressing the rate-distortion drop observed in out-of-domain datasets. Specifically, we perform low-rank matrix decomposition to update certain adaptation parameters of the client's decoder. These updated parameters, along with image latents, are encoded into a bitstream and transmitted to the decoder in practical scenarios. Due to the low-rank constraint imposed on the adaptation parameters, the resulting bit rate overhead is small. Furthermore, the bit rate allocation of low-rank adaptation is \emph{non-trivial}, considering the diverse inputs require varying adaptation bitstreams. We thus introduce a dynamic gating network on top of the low-rank adaptation method, in order to decide which decoder layer should employ adaptation. The dynamic adaptation network is optimized end-to-end using rate-distortion loss. Our proposed method exhibits universality across diverse image datasets. Extensive results demonstrate that this paradigm significantly mitigates the domain gap, surpassing non-adaptive methods with an average BD-rate improvement of approximately $19\%$ across out-of-domain images. Furthermore, it outperforms the most advanced instance adaptive methods by roughly $5\%$ BD-rate. Ablation studies confirm our method's ability to universally enhance various image compression architectures.
</details>
<details>
<summary>摘要</summary>
最新的神经网络图像压缩技术显示出了非常出色的潜在性，可能超越传统标准编码器的环境-质量评估。然而，存在一个不可缺的领域差异（domain gap），其中训练集（natural images）和推理集（artistic images）之间的差异无法被忽略。我们的提议是通过对客户端decoder的一些适应参数进行低级matrix decomposition来解决这种环境差异。Specifically，我们在实际场景中对适应参数进行编码，并将图像latent与适应参数一起编码为bitstream。由于我们对适应参数受到了低级约束，因此bit rate overhead很小。此外，适应参数的bit rate分配是非常复杂的，因为不同的输入需要不同的适应bitstream。我们因此引入了一个动态阀网络，以控制哪些decoder层应该使用适应。这个动态阀网络通过练习环境-质量损失来优化。我们的提议显示了对多种图像压缩架构的通用性。广泛的结果表明，我们的方法可以有效地 mitigate the domain gap，与非适应方法相比，平均BD-rate提高约19%，而与最先进的实例适应方法相比，BD-rate提高约5%。ablation study表明，我们的方法可以通过不同的图像压缩架构进行加持。
</details></li>
</ul>
<hr>
<h2 id="A-deep-deformable-residual-learning-network-for-SAR-images-segmentation"><a href="#A-deep-deformable-residual-learning-network-for-SAR-images-segmentation" class="headerlink" title="A deep deformable residual learning network for SAR images segmentation"></a>A deep deformable residual learning network for SAR images segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07627">http://arxiv.org/abs/2308.07627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Yulin Huang, Jianyu Yang</li>
<li>for: 这个论文是用于提高Synthetic Aperture Radar（SAR）图像中的自动目标分类，并且使用了深度学习网络。</li>
<li>methods: 本论文使用了深度弹性残差学习网络，包括弹性残差层和残差学习层，以提取和保留目标的几何信息。</li>
<li>results: 根据MSTAR数据集的实验结果显示，提案的网络可以实现高精度的目标分类。<details>
<summary>Abstract</summary>
Reliable automatic target segmentation in Synthetic Aperture Radar (SAR) imagery has played an important role in the SAR fields. Different from the traditional methods, Spectral Residual (SR) and CFAR detector, with the recent adavance in machine learning theory, there has emerged a novel method for SAR target segmentation, based on the deep learning networks. In this paper, we proposed a deep deformable residual learning network for target segmentation that attempts to preserve the precise contour of the target. For this, the deformable convolutional layers and residual learning block are applied, which could extract and preserve the geometric information of the targets as much as possible. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) data set, experimental results have shown the superiority of the proposed network for the precise targets segmentation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNReliable automatic target segmentation in Synthetic Aperture Radar (SAR) imagery has played an important role in the SAR fields. Different from the traditional methods, Spectral Residual (SR) and CFAR detector, with the recent advances in machine learning theory, there has emerged a novel method for SAR target segmentation, based on the deep learning networks. In this paper, we proposed a deep deformable residual learning network for target segmentation that attempts to preserve the precise contour of the target. For this, the deformable convolutional layers and residual learning block are applied, which could extract and preserve the geometric information of the targets as much as possible. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) data set, experimental results have shown the superiority of the proposed network for the precise targets segmentation.Note: "zh-CN" is the Simplified Chinese language code.
</details></li>
</ul>
<hr>
<h2 id="GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis"><a href="#GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis" class="headerlink" title="GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis"></a>GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07611">http://arxiv.org/abs/2308.07611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Jui Lu, Benjamin Odry, Muhamed Barakovic, Matthias Weigel, Robin Sandkühler, Reza Rahmanzadeh, Xinjie Chen, Mario Ocampo-Pineda, Jens Kuhle, Ludwig Kappos, Philippe Cattin, Cristina Granziera<br>for:This paper aims to develop a novel approach called GAMER-MRIL to classify multiple sclerosis (MS) patients with severe disability using whole-brain quantitative MRI (qMRI) and an interpretability method.methods:The approach uses a gated-attention-based convolutional neural network (CNN) to select patch-based qMRI important for a given task&#x2F;question, and modifies a structure-aware interpretability method, Layer-wise Relevance Propagation (LRP), to incorporate qMRI.results:The approach achieved an AUC of 0.885 and identified the corticospinal tract as a relevant region for disability, with significant correlations between average qT1 and NDI and patients’ disability scores.Here are the results in Simplified Chinese text:for:这 paper 的目的是开发一种新的方法，称为 GAMER-MRIL，用于 классификация多发性纤维病 (MS) 患者的严重残疾使用整个脑quantitative MRI (qMRI) 和一种可解释方法。methods:这种方法使用一种闭合注意力基于 convolutional neural network (CNN) 来选择 patch-based qMRI 对于给定任务&#x2F;问题的重要部分，并将structure-aware interpretability method，层 wise Relevance Propagation (LRP)，与 qMRI 结合使用。results:这种方法实现了 AUC 为 0.885，并identified  corticospinal tract 作为残疾相关的区域，与患者的残疾分数相关性 Statistical significant (ρ&#x3D;-0.37和0.44)。<details>
<summary>Abstract</summary>
Objective: Identifying disability-related brain changes is important for multiple sclerosis (MS) patients. Currently, there is no clear understanding about which pathological features drive disability in single MS patients. In this work, we propose a novel comprehensive approach, GAMER-MRIL, leveraging whole-brain quantitative MRI (qMRI), convolutional neural network (CNN), and an interpretability method from classifying MS patients with severe disability to investigating relevant pathological brain changes. Methods: One-hundred-sixty-six MS patients underwent 3T MRI acquisitions. qMRI informative of microstructural brain properties was reconstructed, including quantitative T1 (qT1), myelin water fraction (MWF), and neurite density index (NDI). To fully utilize the qMRI, GAMER-MRIL extended a gated-attention-based CNN (GAMER-MRI), which was developed to select patch-based qMRI important for a given task/question, to the whole-brain image. To find out disability-related brain regions, GAMER-MRIL modified a structure-aware interpretability method, Layer-wise Relevance Propagation (LRP), to incorporate qMRI. Results: The test performance was AUC=0.885. qT1 was the most sensitive measure related to disability, followed by NDI. The proposed LRP approach obtained more specifically relevant regions than other interpretability methods, including the saliency map, the integrated gradients, and the original LRP. The relevant regions included the corticospinal tract, where average qT1 and NDI significantly correlated with patients' disability scores ($\rho$=-0.37 and 0.44). Conclusion: These results demonstrated that GAMER-MRIL can classify patients with severe disability using qMRI and subsequently identify brain regions potentially important to the integrity of the mobile function. Significance: GAMER-MRIL holds promise for developing biomarkers and increasing clinicians' trust in NN.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation"><a href="#Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation" class="headerlink" title="Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation"></a>Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07506">http://arxiv.org/abs/2308.07506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jadie1/medseguq">https://github.com/jadie1/medseguq</a></li>
<li>paper_authors: Jadie Adams, Shireen Y. Elhabian</li>
<li>for: 这种研究旨在评估医疗图像分析中的epistemic uncertainty量化方法，以帮助提高诊断和治疗规划的可靠性和Robustness。</li>
<li>methods: 本研究使用了多种epistemic uncertainty量化方法，包括Bayesian neural networks、 Monte Carlo dropout、和 Deep Ensembles等，并进行了比较性研究以评估每种方法的准确率、uncertainty calibration和可扩展性。</li>
<li>results: 研究发现，Bayesian neural networks和Deep Ensembles方法在准确率和uncertainty calibration方面表现最佳，而Monte Carlo dropout方法在可扩展性方面表现最好。此外，研究还发现了这些方法在out-of-distribution detection方面的能力不尽相同。<details>
<summary>Abstract</summary>
Deep learning based methods for automatic organ segmentation have shown promise in aiding diagnosis and treatment planning. However, quantifying and understanding the uncertainty associated with model predictions is crucial in critical clinical applications. While many techniques have been proposed for epistemic or model-based uncertainty estimation, it is unclear which method is preferred in the medical image analysis setting. This paper presents a comprehensive benchmarking study that evaluates epistemic uncertainty quantification methods in organ segmentation in terms of accuracy, uncertainty calibration, and scalability. We provide a comprehensive discussion of the strengths, weaknesses, and out-of-distribution detection capabilities of each method as well as recommendations for future improvements. These findings contribute to the development of reliable and robust models that yield accurate segmentations while effectively quantifying epistemic uncertainty.
</details>
<details>
<summary>摘要</summary>
深度学习基本的方法可以帮助自动 segmentation 获得了诊断和治疗规划的潜在优势。但是，量化和理解模型预测结果中的不确定性是在敏感医学应用中至关重要。虽然许多技术已经提出了 epistemic 或模型基于的不确定性估计方法，但是尚未确定哪种方法在医学图像分析 Setting 中是首选的。这篇文章提供了一项全面的比较研究，评估了 organ segmentation 中 epistemic 不确定性估计方法的准确性、不确定性满足度和可扩展性。我们提供了每种方法的优缺点、外部数据检测能力和未来改进建议。这些发现将为建立可靠和可靠的模型做出贡献，以便在获得准确分 segmentation 的同时，有效地量化 epistemic 不确定性。
</details></li>
</ul>
<hr>
<h2 id="Brain-Tumor-Detection-Based-on-a-Novel-and-High-Quality-Prediction-of-the-Tumor-Pixel-Distributions"><a href="#Brain-Tumor-Detection-Based-on-a-Novel-and-High-Quality-Prediction-of-the-Tumor-Pixel-Distributions" class="headerlink" title="Brain Tumor Detection Based on a Novel and High-Quality Prediction of the Tumor Pixel Distributions"></a>Brain Tumor Detection Based on a Novel and High-Quality Prediction of the Tumor Pixel Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07495">http://arxiv.org/abs/2308.07495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanming Sun, Chunyan Wang<br>for: 这个系统用于检测Brain Tumor在3D MRI脑部扫描图像中的FLAIR模式下。methods: 该系统使用了2个功能：（a）预测灰度和位坐标分布的粒子点云，以及（b）生成粒子点云掩码。为了方便3D数据分析和处理，我们引入了2D histogram表示法，这种表示法包括脑部结构的灰度分布和像素位坐标分布。results: 在测试了more than one thousand patient cases的数据集上，该系统可以准确地预测2D histogram，并且可以在低计算成本下实现高度相似的结果，与现有的CNN系统相当。<details>
<summary>Abstract</summary>
In this paper, we propose a system to detect brain tumor in 3D MRI brain scans of Flair modality. It performs 2 functions: (a) predicting gray-level and locational distributions of the pixels in the tumor regions and (b) generating tumor mask in pixel-wise precision. To facilitate 3D data analysis and processing, we introduced a 2D histogram presentation that comprehends the gray-level distribution and pixel-location distribution of a 3D object. In the proposed system, particular 2D histograms, in which tumor-related feature data get concentrated, are established by exploiting the left-right asymmetry of a brain structure. A modulation function is generated from the input data of each patient case and applied to the 2D histograms to attenuate the element irrelevant to the tumor regions. The prediction of the tumor pixel distribution is done in 3 steps, on the axial, coronal and sagittal slice series, respectively. In each step, the prediction result helps to identify/remove tumor-free slices, increasing the tumor information density in the remaining data to be applied to the next step. After the 3-step removal, the 3D input is reduced to a minimum bounding box of the tumor region. It is used to finalize the prediction and then transformed into a 3D tumor mask, by means of gray level thresholding and low-pass-based morphological operations. The final prediction result is used to determine the critical threshold. The proposed system has been tested extensively with the data of more than one thousand patient cases in the datasets of BraTS 2018~21. The test results demonstrate that the predicted 2D histograms have a high degree of similarity with the true ones. The system delivers also very good tumor detection results, comparable to those of state-of-the-art CNN systems with mono-modality inputs, which is achieved at an extremely low computation cost and no need for training.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种系统，用于检测Brain Tumor在3D MRI脑部扫描图像中的FLAIR模式下。该系统完成了两个函数：（a）预测肿瘤区域像素的灰度和位置分布，以及（b）生成肿瘤面积的精确推测。为了便于3D数据分析和处理，我们引入了一种2D histogram展示方式，该方式涵盖肿瘤区域的灰度分布和像素位置分布。在我们提出的系统中，特定的2D histogram被设置，以便在肿瘤相关特征数据中强调特定的特征。在输入数据中，每个病例的模ulation函数被应用于2D histogram，以减少不相关于肿瘤区域的元素。肿瘤像素分布的预测被分为三步，在axial、coronal和sagittal扫描序列上进行，每步的预测结果帮助标识/移除不含肿瘤的扫描，从而增加留下的数据中肿瘤信息的浓度。在第三步后，输入数据被减少到最小 bounding box 的肿瘤区域。最后，使用灰度阈值化和低通滤波操作，将最终预测结果转换为3D肿瘤面积。测试结果表明，我们的预测结果与实际的2D histogram有高度的相似性。系统还可以实现非常好的肿瘤检测结果，与现有的CNN系统相当，但是具有非常低的计算成本和不需要训练。
</details></li>
</ul>
<hr>
<h2 id="Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis"><a href="#Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis" class="headerlink" title="Space Object Identification and Classification from Hyperspectral Material Analysis"></a>Space Object Identification and Classification from Hyperspectral Material Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07481">http://arxiv.org/abs/2308.07481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Massimiliano Vasile, Lewis Walker, Andrew Campbell, Simao Marto, Paul Murray, Stephen Marshall, Vasili Savitski</li>
<li>for: 这篇论文是为了提取外星物体干扰特征图像中的信息而设计的数据处理管道。</li>
<li>methods: 该论文使用了两种材料标识和分类技术：一种基于机器学习，另一种基于最小二乘匹配知 Spectra 库。</li>
<li>results: 论文将会展示一些初步的外星物体识别和分类结果。<details>
<summary>Abstract</summary>
This paper presents a data processing pipeline designed to extract information from the hyperspectral signature of unknown space objects. The methodology proposed in this paper determines the material composition of space objects from single pixel images. Two techniques are used for material identification and classification: one based on machine learning and the other based on a least square match with a library of known spectra. From this information, a supervised machine learning algorithm is used to classify the object into one of several categories based on the detection of materials on the object. The behaviour of the material classification methods is investigated under non-ideal circumstances, to determine the effect of weathered materials, and the behaviour when the training library is missing a material that is present in the object being observed. Finally the paper will present some preliminary results on the identification and classification of space objects.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "hyperspectral signature" is translated as "多spectral签名" (duōspectral jiànmíng), which is a combination of "多spectral" (meaning "hyperspectral") and "签名" (meaning "signature").* "material composition" is translated as "物质组成" (wùzhì zhùxìn), which is a combination of "物质" (meaning "material") and "组成" (meaning "composition").* "machine learning" is translated as "机器学习" (jīzhì xuéxí), which is a direct translation of the English term.* "least square match" is translated as "最小平方匹配" (zuìxiǎo píngfāng pínghù), which is a direct translation of the English term.* "supervised machine learning algorithm" is translated as "指导式机器学习算法" (dìdǎo xìng jīzhì xuéxísuànfǎ), which is a direct translation of the English term.* "weathered materials" is translated as "天气损害的物质" (tiānqì jiānghài de wùzhì), which is a combination of "天气" (meaning "weather") and "损害" (meaning "damage").* "training library" is translated as "训练库" (xùnxíng kù), which is a direct translation of the English term.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression"><a href="#Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression" class="headerlink" title="Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression"></a>Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07477">http://arxiv.org/abs/2308.07477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antonbaumann/mimo-unet">https://github.com/antonbaumann/mimo-unet</a></li>
<li>paper_authors: Anton Baumann, Thomas Roßberg, Michael Schmitt</li>
<li>for: 这 paper 的目的是提高机器学习模型的可靠性和解释性，特别是在高度重要的实际应用场景中。</li>
<li>methods: 这 paper 使用了 Multiple-Input Multiple-Output (MIMO) 框架，利用深度神经网络的过参数化来进行像素级回归任务。它还引入了一种同步多Sub网络性能的新方法。</li>
<li>results: 对两个正交的数据集进行了广泛的评估，显示了与现有模型相当的准确率、在正常数据上更好的准确率报告、出入数据检测能力的robustness、参数大小和计算时间的显著改善。代码可以在 github.com&#x2F;antonbaumann&#x2F;MIMO-Unet 上获取。<details>
<summary>Abstract</summary>
Uncertainty estimation in machine learning is paramount for enhancing the reliability and interpretability of predictive models, especially in high-stakes real-world scenarios. Despite the availability of numerous methods, they often pose a trade-off between the quality of uncertainty estimation and computational efficiency. Addressing this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework -- an approach exploiting the overparameterization of deep neural networks -- for pixel-wise regression tasks. Our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. For that purpose, we adapted the U-Net architecture to train multiple subnetworks within a single model, harnessing the overparameterization in deep neural networks. Additionally, we introduce a novel procedure for synchronizing subnetwork performance within the MIMO framework. Our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and considerable improvements in parameter size and inference time. Code available at github.com/antonbaumann/MIMO-Unet
</details>
<details>
<summary>摘要</summary>
machine learning中的不确定性估计是提高预测模型的可靠性和可读性的关键，特别在高度的实际应用场景中。 despite numerous methods 的可用性，它们经常存在质量的不确定性估计和计算效率之间的负担。 to address this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework -- an approach exploiting the overparameterization of deep neural networks -- for pixel-wise regression tasks. our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. for this purpose, we adapted the U-Net architecture to train multiple subnetworks within a single model, harnessing the overparameterization in deep neural networks. additionally, we introduce a novel procedure for synchronizing subnetwork performance within the MIMO framework. our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and considerable improvements in parameter size and inference time. code available at github.com/antonbaumann/MIMO-Unet.Here's the word-for-word translation of the text into Simplified Chinese:机器学习中的不确定性估计是提高预测模型的可靠性和可读性的关键，特别在高度的实际应用场景中。 despite numerous methods 的可用性，它们经常存在质量的不确定性估计和计算效率之间的负担。为了解决这个挑战，我们提出了一种基于多输入多输出（MIMO）框架的修改---一种利用深度神经网络的过参数化来实现像素级回归任务。我们的 MIMO 变体将该approach扩展到更广泛的计算机视ión Domains。为此，我们采用了修改的 U-Net 架构，在单个模型中培训多个子网络，利用深度神经网络的过参数化。此外，我们还提出了一种新的同步子网络性能的方法。我们对 MIMO U-Net 在两个orthogonal dataset上进行了全面的评估，结果表明其与现有模型相比有相似的准确性，在适用范围内的数据上有更好的准确性calibration，在尝试数据上有更好的robustness，并且具有较小的参数大小和计算时间。代码可以在github.com/antonbaumann/MIMO-Unet 上获取。
</details></li>
</ul>
<hr>
<h2 id="Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation"><a href="#Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation" class="headerlink" title="Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation"></a>Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07251">http://arxiv.org/abs/2308.07251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/mdunet">https://github.com/liamchalcroft/mdunet</a></li>
<li>paper_authors: Liam Chalcroft, Ruben Lourenço Pereira, Mikael Brudfors, Andrew S. Kayser, Mark D’Esposito, Cathy J. Price, Ioannis Pappas, John Ashburner</li>
<li>for: 这个研究旨在提出一种能够实现高性能和优化参数的混合型神经网络模型，用于三维脑膜病变分类。</li>
<li>methods: 本研究使用了一种将transformer块与传统的对称块结合起来的all-convolutional transformer block，并将其应用于U-Net架构中。</li>
<li>results: 研究结果显示，我们的模型在三维脑膜病变分类任务中具有与现有最佳方法相匹配的性能，并且具有较好的优化参数和传播传统神经网络的优点。<details>
<summary>Abstract</summary>
Vision transformers are effective deep learning models for vision tasks, including medical image segmentation. However, they lack efficiency and translational invariance, unlike convolutional neural networks (CNNs). To model long-range interactions in 3D brain lesion segmentation, we propose an all-convolutional transformer block variant of the U-Net architecture. We demonstrate that our model provides the greatest compromise in three factors: performance competitive with the state-of-the-art; parameter efficiency of a CNN; and the favourable inductive biases of a transformer. Our public implementation is available at https://github.com/liamchalcroft/MDUNet .
</details>
<details>
<summary>摘要</summary>
幻视转换器是深度学习模型，用于视觉任务，如医疗图像分割。然而，它们缺乏效率和翻译不变性，与卷积神经网络（CNNs）不同。为模elling长距离交互 в 3D 脑损坏分割，我们提议一种具有alls-convolutional transformer块的U-Net架构变体。我们示示了我们的模型在三个因素中具有最大的妥协：性能与状态艺术竞争力相当; 参数效率与CNN相同; 以及 transformer 所带有的有利 inductive bias。我们的公共实现可以在 <https://github.com/liamchalcroft/MDUNet> 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/eess.IV_2023_08_15/" data-id="cllurrpcl00e1sw88h9xna849" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.LG_2023_08_14/" class="article-date">
  <time datetime="2023-08-13T16:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/cs.LG_2023_08_14/">cs.LG - 2023-08-14 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Distance-Matters-For-Improving-Performance-Estimation-Under-Covariate-Shift"><a href="#Distance-Matters-For-Improving-Performance-Estimation-Under-Covariate-Shift" class="headerlink" title="Distance Matters For Improving Performance Estimation Under Covariate Shift"></a>Distance Matters For Improving Performance Estimation Under Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07223">http://arxiv.org/abs/2308.07223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/melanibe/distance_matters_performance_estimation">https://github.com/melanibe/distance_matters_performance_estimation</a></li>
<li>paper_authors: Mélanie Roschewitz, Ben Glocker</li>
<li>for: 本研究旨在提高 covariate shift 下的性能估算，尤其是在敏感应用场景下。</li>
<li>methods: 该研究提出了一种基于 distance 的方法，通过检查测试样本与预期的训练分布之间的距离，以避免基于不可靠的模型输出来估算性能。</li>
<li>results: 研究在 13 个图像分类任务上进行了实验，并在各种自然和 sintetic 分布shift 下达到了 median 相对 MAE 改进率为 27%，并在 10 个任务中达到了最佳基eline。<details>
<summary>Abstract</summary>
Performance estimation under covariate shift is a crucial component of safe AI model deployment, especially for sensitive use-cases. Recently, several solutions were proposed to tackle this problem, most leveraging model predictions or softmax confidence to derive accuracy estimates. However, under dataset shifts, confidence scores may become ill-calibrated if samples are too far from the training distribution. In this work, we show that taking into account distances of test samples to their expected training distribution can significantly improve performance estimation under covariate shift. Precisely, we introduce a "distance-check" to flag samples that lie too far from the expected distribution, to avoid relying on their untrustworthy model outputs in the accuracy estimation step. We demonstrate the effectiveness of this method on 13 image classification tasks, across a wide-range of natural and synthetic distribution shifts and hundreds of models, with a median relative MAE improvement of 27% over the best baseline across all tasks, and SOTA performance on 10 out of 13 tasks. Our code is publicly available at https://github.com/melanibe/distance_matters_performance_estimation.
</details>
<details>
<summary>摘要</summary>
性能估计下 covariate shift 是安全 AI 模型部署中的一个关键组件，尤其是在敏感应用场景下。最近，一些解决方案被提出来解决这个问题，大多数都是基于模型预测或软max信任来 derive 准确性估计。然而，在数据集 shift 下，信任度分数可能会变得不准确，如果样本太far away from the training distribution。在这种情况下，我们表明可以通过考虑测试样本与预期的训练分布之间的距离来进行性能估计。我们引入了一种"距离检查"来检测测试样本是否位于预期的训练分布中，以避免基于不可靠的模型输出来进行准确性估计。我们在 13 个图像分类任务上进行了实验，包括自然和合成分布 shift，以及多达百个模型， median 相对误差改进率为 27%，并在所有任务上达到最佳基eline的表现。我们的代码可以在 <https://github.com/melanibe/distance_matters_performance_estimation> 上获取。
</details></li>
</ul>
<hr>
<h2 id="AudioFormer-Audio-Transformer-learns-audio-feature-representations-from-discrete-acoustic-codes"><a href="#AudioFormer-Audio-Transformer-learns-audio-feature-representations-from-discrete-acoustic-codes" class="headerlink" title="AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes"></a>AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07221">http://arxiv.org/abs/2308.07221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LZH-0225/AudioFormer">https://github.com/LZH-0225/AudioFormer</a></li>
<li>paper_authors: Zhaohui Li, Haitao Wang, Xinghua Jiang</li>
<li>for: 这篇论文是为了学习音频特征表示，通过自然语言理解（NLU）的新角度，并使用神经网络音码器模型生成抽象的音频代码，然后使用这些代码训练马斯克隐藏语言模型（MLM）来获得高质量的音频表示。</li>
<li>methods: 这篇论文使用了一种新的多 positivesample Contrastive（MPC）学习方法，它可以学习多个抽象的音频代码之间的共同表示，从而提高音频表示质量。具体来说，首先使用一个神经网络音码器模型生成抽象的音频代码，然后使用这些代码训练一个马斯克隐藏语言模型（MLM），最后使用MPC学习方法来学习多个抽象的音频代码之间的共同表示。</li>
<li>results: 根据实验结果，AudioFormer在多个数据集上达到了显著提高的性能，甚至超过了一些音视频多模态分类模型。具体来说，AudioFormer在AudioSet（2M,20K）、FSD50K等数据集上的性能分别为53.9、45.1和65.6。<details>
<summary>Abstract</summary>
We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures collaborative representations among distinct positive samples. Our research outcomes demonstrate that AudioFormer attains significantly improved performance compared to prevailing monomodal audio classification models across multiple datasets,and even outperforms audio-visual multimodal classification models on select datasets. Specifically,our approach achieves remarkable results on datasets including AudioSet (2M,20K),and FSD50K,with performance scores of 53.9,45.1,and 65.6,respectively. We have openly shared both the code and models: https://github.com/LZH-0225/AudioFormer.git.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法 named AudioFormer，它通过获取逻辑音频编码并进一步练习其为音频分类任务进行学习。我们首先提出了一种新的视角，即视音频分类任务为自然语言理解（NLU）的一种形式。利用现有的神经网络音频编码器模型，我们生成了逻辑音频编码，并使用其训练一个封面语言模型（MLM），从而获得了高质量的音频特征表示。此外，我们还开拓了多个正样本对比（MPC）学习方法的应用。这种方法可以在同一个音频输入中学习多个独立的逻辑音频编码之间的共同表示。在我们的实验中，我们将逻辑音频编码视为文本数据，并使用cloze-like方法训练一个封面语言模型，最终获得了高质量的音频表示。尤其是，MPC学习技术可以有效捕捉多个正样本之间的协作表示。我们的研究结果显示，AudioFormer在多个数据集上达到了 significatively提高的性能，甚至超过了多模态音视频分类模型在一些数据集上。具体来说，我们的方法在AudioSet（2M,20K）、FSD50K等数据集上获得了53.9、45.1和65.6的性能分数。我们已经在 GitHub 上公开了代码和模型：https://github.com/LZH-0225/AudioFormer.git。
</details></li>
</ul>
<hr>
<h2 id="Generating-Individual-Trajectories-Using-GPT-2-Trained-from-Scratch-on-Encoded-Spatiotemporal-Data"><a href="#Generating-Individual-Trajectories-Using-GPT-2-Trained-from-Scratch-on-Encoded-Spatiotemporal-Data" class="headerlink" title="Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data"></a>Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07940">http://arxiv.org/abs/2308.07940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taizo Horikomi, Shouji Fujimoto, Atushi Ishikawa, Takayuki Mizuno</li>
<li>for: 本研究用于构建一个基于GPT-2语言模型的深度学习模型，用于生成受环境因素和个人特征 influencing的日常行走路径。</li>
<li>methods: 研究使用了地理坐标转换为特定的位置符号，并将每天的行走路径表示为一个序列符号。通过训练GPT-2架构，实现了从零开始训练一个深度学习模型，用于生成受环境因素和个人特征 influencing的日常行走路径。</li>
<li>results: 研究得出了一个基于GPT-2语言模型的深度学习模型，可以生成受环境因素和个人特征 influencing的日常行走路径。这种模型可以帮助我们更好地理解人们的日常活动行为，并且可以用于评估不同环境和个人特征对行走路径的影响。<details>
<summary>Abstract</summary>
Following Mizuno, Fujimoto, and Ishikawa's research (Front. Phys. 2022), we transpose geographical coordinates expressed in latitude and longitude into distinctive location tokens that embody positions across varied spatial scales. We encapsulate an individual daily trajectory as a sequence of tokens by adding unique time interval tokens to the location tokens. Using the architecture of an autoregressive language model, GPT-2, this sequence of tokens is trained from scratch, allowing us to construct a deep learning model that sequentially generates an individual daily trajectory. Environmental factors such as meteorological conditions and individual attributes such as gender and age are symbolized by unique special tokens, and by training these tokens and trajectories on the GPT-2 architecture, we can generate trajectories that are influenced by both environmental factors and individual attributes.
</details>
<details>
<summary>摘要</summary>
据米榊、藤本、石川等人的研究（Front. Phys. 2022），我们将地理坐标表示为纬度和经度转化为不同的空间尺度下的特征位置符号。我们将每个日常路径作为一个序列符号，通过将唯一的时间间隔符号添加到位置符号中来嵌入它。使用GPT-2架构的自然语言模型，我们从零开始训练这个序列符号，以构建一个可以逐步生成个人日常路径的深度学习模型。environmental factor such as weather conditions和个人属性such as gender and age通过特殊符号表示，我们通过在GPT-2架构上训练这些符号和路径，可以生成受环境因素和个人属性 influencing的 trajectory。
</details></li>
</ul>
<hr>
<h2 id="Automated-Ensemble-Based-Segmentation-of-Pediatric-Brain-Tumors-A-Novel-Approach-Using-the-CBTN-CONNECT-ASNR-MICCAI-BraTS-PEDs-2023-Challenge-Data"><a href="#Automated-Ensemble-Based-Segmentation-of-Pediatric-Brain-Tumors-A-Novel-Approach-Using-the-CBTN-CONNECT-ASNR-MICCAI-BraTS-PEDs-2023-Challenge-Data" class="headerlink" title="Automated Ensemble-Based Segmentation of Pediatric Brain Tumors: A Novel Approach Using the CBTN-CONNECT-ASNR-MICCAI BraTS-PEDs 2023 Challenge Data"></a>Automated Ensemble-Based Segmentation of Pediatric Brain Tumors: A Novel Approach Using the CBTN-CONNECT-ASNR-MICCAI BraTS-PEDs 2023 Challenge Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07212">http://arxiv.org/abs/2308.07212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashidhar Reddy Javaji, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</li>
<li>for: 这个研究的目的是为了发展用于脑膜癌的诊断技术和治疗方法。</li>
<li>methods: 这个研究使用了深度学习技术，使用了Magnetic Resonance Imaging（MRI）模式，并导入了一种新的组合方法，包括ONet和修改过的UNet，以及新的损失函数。</li>
<li>results: 这个研究获得了2023年BraTS-PEDs挑战赛的精确分类模型。使用扩展资料，包括单独和composite变数，以确保模型的稳定性和准确性。组合策略，结合ONet和UNet模型，展现了更高的效率和精确性。 lesion_wise dice scores为0.52、0.72和0.78，证明了这种组合方法的优势。<details>
<summary>Abstract</summary>
Brain tumors remain a critical global health challenge, necessitating advancements in diagnostic techniques and treatment methodologies. In response to the growing need for age-specific segmentation models, particularly for pediatric patients, this study explores the deployment of deep learning techniques using magnetic resonance imaging (MRI) modalities. By introducing a novel ensemble approach using ONet and modified versions of UNet, coupled with innovative loss functions, this study achieves a precise segmentation model for the BraTS-PEDs 2023 Challenge. Data augmentation, including both single and composite transformations, ensures model robustness and accuracy across different scanning protocols. The ensemble strategy, integrating the ONet and UNet models, shows greater effectiveness in capturing specific features and modeling diverse aspects of the MRI images which result in lesion_wise dice scores of 0.52, 0.72 and 0.78 for enhancing tumor, tumor core and whole tumor labels respectively. Visual comparisons further confirm the superiority of the ensemble method in accurate tumor region coverage. The results indicate that this advanced ensemble approach, building upon the unique strengths of individual models, offers promising prospects for enhanced diagnostic accuracy and effective treatment planning for brain tumors in pediatric brains.
</details>
<details>
<summary>摘要</summary>
脑肿仍然是全球医疗挑战，需要进一步的技术创新和治疗方法。为了应对儿童患者的年龄特定分 segmentation模型的增长需求，本研究利用深度学习技术和Magnetic Resonance Imaging（MRI）模式，探讨一种新的集成方法。通过引入ONet和修改版本的UNet模型，以及创新的损失函数，本研究实现了高精度的分割模型，为BraTS-PEDs 2023 Challenge提供了精准的分割结果。数据扩展，包括单个和复合变换，使模型具有不同扫描协议下的Robustness和准确性。集成策略，将ONet和UNet模型集成在一起，表现出更高的特征捕捉和多样化图像模型化能力，最终得到了lesion_wise dice分割率为0.52、0.72和0.78，用于涉及肿块、肿块核心和整个肿块等标签。视觉比较还证明了集成方法在精准肿块覆盖方面的优势。结果表明，这种高级集成方法，基于各个模型的特点优势，对儿童脑肿的诊断精度和有效的治疗规划具有替代性。
</details></li>
</ul>
<hr>
<h2 id="Unified-Data-Free-Compression-Pruning-and-Quantization-without-Fine-Tuning"><a href="#Unified-Data-Free-Compression-Pruning-and-Quantization-without-Fine-Tuning" class="headerlink" title="Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning"></a>Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07209">http://arxiv.org/abs/2308.07209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shipeng Bai, Jun Chen, Xintian Shen, Yixuan Qian, Yong Liu</li>
<li>for: 降低神经网络的执行时间和内存占用</li>
<li>methods: 同时施行减少和量化，不需要原始训练数据集</li>
<li>results: 在大规模图像分类任务上实现了显著的改善，比如在ImageNet dataset上与 State-of-the-Art 方法相比，使用30% 减少率和6位量化，ResNet-34 网络上获得20.54%的准确率提升。<details>
<summary>Abstract</summary>
Structured pruning and quantization are promising approaches for reducing the inference time and memory footprint of neural networks. However, most existing methods require the original training dataset to fine-tune the model. This not only brings heavy resource consumption but also is not possible for applications with sensitive or proprietary data due to privacy and security concerns. Therefore, a few data-free methods are proposed to address this problem, but they perform data-free pruning and quantization separately, which does not explore the complementarity of pruning and quantization. In this paper, we propose a novel framework named Unified Data-Free Compression(UDFC), which performs pruning and quantization simultaneously without any data and fine-tuning process. Specifically, UDFC starts with the assumption that the partial information of a damaged(e.g., pruned or quantized) channel can be preserved by a linear combination of other channels, and then derives the reconstruction form from the assumption to restore the information loss due to compression. Finally, we formulate the reconstruction error between the original network and its compressed network, and theoretically deduce the closed-form solution. We evaluate the UDFC on the large-scale image classification task and obtain significant improvements over various network architectures and compression methods. For example, we achieve a 20.54% accuracy improvement on ImageNet dataset compared to SOTA method with 30% pruning ratio and 6-bit quantization on ResNet-34.
</details>
<details>
<summary>摘要</summary>
《结构化截割和量化是神经网络减少推理时间和内存占用的有力方法。然而，大多数现有方法需要原始训练数据来细化模型，这不仅带来了重量级的资源占用，而且对于敏感或商业机密数据来说，由于隐私和安全问题，无法进行训练。因此，一些无数据方法被提出，但它们只是分别进行无数据截割和量化，没有利用截割和量化的衔接。在本文中，我们提出了一种名为统一无数据压缩（UDFC）的新框架，它在无数据情况下同时进行截割和量化。具体来说，UDFC从假设损坏（例如截割或量化）通道的部分信息可以通过其他通道的线性组合来保留，然后从假设中 derive 重建形式来恢复因压缩而产生的信息损失。最后，我们将重建错误 между 原始网络和压缩后的网络，并理论上解出closed-form解决方案。我们对大规模图像分类任务进行评估，并在不同的网络架构和压缩方法下获得了显著的改进。例如，我们在ImageNet数据集上达到了30%截割率和6位量化的SOTA方法比20.54%的精度提升。》
</details></li>
</ul>
<hr>
<h2 id="Algorithms-for-the-Training-of-Neural-Support-Vector-Machines"><a href="#Algorithms-for-the-Training-of-Neural-Support-Vector-Machines" class="headerlink" title="Algorithms for the Training of Neural Support Vector Machines"></a>Algorithms for the Training of Neural Support Vector Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07204">http://arxiv.org/abs/2308.07204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Lars Simon, Manuel Radons</li>
<li>for: 本研究使用神经支持向量机（NSVM）结构，以汲取领域知识在模型设计中。</li>
<li>methods: 本文提出了一组基于 Pegasos 算法的 NSVM 训练算法，并通过解决一系列标准机器学习任务来证明其效果。</li>
<li>results: 本研究通过实验和分析，证明了 NSVM 在一些标准机器学习任务中的表现，并验证了领域知识的Integration在模型设计中的重要性。<details>
<summary>Abstract</summary>
Neural support vector machines (NSVMs) allow for the incorporation of domain knowledge in the design of the model architecture. In this article we introduce a set of training algorithms for NSVMs that leverage the Pegasos algorithm and provide a proof of concept by solving a set of standard machine learning tasks.
</details>
<details>
<summary>摘要</summary>
神经支持向量机器 (NSVM) 允许在模型建立的架构中包含领域知识。在这篇文章中，我们介绍了一组用 Pegasos 算法进行训练的 NSVM 训练算法，并通过解决一组标准机器学习任务来提供证明。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Neural-Categorical-Priors-for-Physics-Based-Character-Control"><a href="#Neural-Categorical-Priors-for-Physics-Based-Character-Control" class="headerlink" title="Neural Categorical Priors for Physics-Based Character Control"></a>Neural Categorical Priors for Physics-Based Character Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07200">http://arxiv.org/abs/2308.07200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tencent-RoboticsX/NCP">https://github.com/Tencent-RoboticsX/NCP</a></li>
<li>paper_authors: Qingxu Zhu, He Zhang, Mengting Lan, Lei Han</li>
<li>for: 这paper aimed to propose a new learning framework for controlling physics-based characters with naturalistic behaviors.</li>
<li>methods: The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips, and then uses a discrete information bottleneck and prior shifting to generate high-quality life-like behaviors.</li>
<li>results: The proposed framework is capable of controlling the character to perform considerably high-quality movements in terms of behavioral strategies, diversity, and realism, as demonstrated through comprehensive experiments using humanoid characters on two challenging downstream tasks.Here’s the Chinese version of the three key points:</li>
<li>for: 这paper的目标是提出一种新的学习框架，用于控制基于物理的角色表现出自然的行为。</li>
<li>methods: 提议的方法使用了反馈学习（RL）来跟踪和模仿生活中的自然运动，然后使用一种简化信息瓶颈和征识偏移来生成高质量的自然行为。</li>
<li>results: 提议的框架可以控制角色表现出较高质量的行为策略、多样性和真实性，这得到通过对人工智能角色进行了两个复杂的下游任务的实验证明。<details>
<summary>Abstract</summary>
Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervision of the encoder's output, it follows the original motion clip distribution in the dataset and could lead to imbalanced behaviors in our setting. To address the issue, we further propose a technique named prior shifting to adjust the prior distribution using curiosity-driven RL. The outcome distribution is demonstrated to offer sufficient behavioral diversity and significantly facilitates upper-level policy learning for downstream tasks. We conduct comprehensive experiments using humanoid characters on two challenging downstream tasks, sword-shield striking and two-player boxing game. Our results demonstrate that the proposed framework is capable of controlling the character to perform considerably high-quality movements in terms of behavioral strategies, diversity, and realism. Videos, codes, and data are available at https://tencent-roboticsx.github.io/NCP/.
</details>
<details>
<summary>摘要</summary>
最近的研究发展强化 reuse motion prior 技术已经证明其能够生成自然的行为。在这篇论文中，我们提出一种新的学习框架，用于控制基于物理的人物，并且能够提高 Motion 质量和多样性，至今为止的现有方法。我们使用 reinforcement learning（RL）来初始化和模仿生命like 运动从未结构化运动clip 中提取有用信息，并使用 discrete information bottleneck，与 Vector Quantized Variational AutoEncoder（VQ-VAE）相同。这种结构压缩运动clip 中最重要的信息，并将其压缩成一个紧凑的、有用的幂论空间中。通过在训练过的 categorical prior distribution 中采样代码，可以生成高质量的生命like 行为。尽管这个 prior distribution 可以通过encoder的输出进行训练，但它遵循原始运动clip 的分布，这可能会导致行为偏斜。为了解决这个问题，我们提出了一种名为 prior shifting 的技术，通过 Curiosity-driven RL 来调整 prior distribution。结果显示，我们的方法可以提供足够的行为多样性，并且能够帮助上层策略学习以下渠道任务。我们在人iform 角色上进行了全面的实验，并使用剑盾战斗和两个玩家简易拳击游戏。我们的结果表明，我们的框架能够控制人iform 角色进行较高质量的运动，包括行为策略、多样性和真实性。视频、代码和数据可以在https://tencent-roboticsx.github.io/NCP/ 获取。
</details></li>
</ul>
<hr>
<h2 id="Explaining-Black-Box-Models-through-Counterfactuals"><a href="#Explaining-Black-Box-Models-through-Counterfactuals" class="headerlink" title="Explaining Black-Box Models through Counterfactuals"></a>Explaining Black-Box Models through Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07198">http://arxiv.org/abs/2308.07198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juliatrustworthyai/counterfactualexplanations.jl">https://github.com/juliatrustworthyai/counterfactualexplanations.jl</a></li>
<li>paper_authors: Patrick Altmeyer, Arie van Deursen, Cynthia C. S. Liem</li>
<li>for: 用于解释人工智能模型的输出</li>
<li>methods: 使用Counterfactual Explanations（CE）和Algorithmic Recourse（AR）生成解释和修复方法</li>
<li>results: 可以提供实用和现实的修复方法，帮助改善模型的输出结果<details>
<summary>Abstract</summary>
We present CounterfactualExplanations.jl: a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box models in Julia. CE explain how inputs into a model need to change to yield specific model predictions. Explanations that involve realistic and actionable changes can be used to provide AR: a set of proposed actions for individuals to change an undesirable outcome for the better. In this article, we discuss the usefulness of CE for Explainable Artificial Intelligence and demonstrate the functionality of our package. The package is straightforward to use and designed with a focus on customization and extensibility. We envision it to one day be the go-to place for explaining arbitrary predictive models in Julia through a diverse suite of counterfactual generators.
</details>
<details>
<summary>摘要</summary>
我们介绍CounterfactualExplanations.jl：一个用于生成反对方案解释（CE）和算法补救（AR）的 julia 套件。CE 解释了模型对于特定预测所需的输入更改，这些解释可以提供AR：一组可行和有效的改善结果的建议。在这篇文章中，我们讨论了CE 在可解释人工智能中的用途，并详细介绍套件的功能。套件易于使用，设计为可自定义和扩展。我们将它作为 julia 中解释任何预测模型的首选之地。
</details></li>
</ul>
<hr>
<h2 id="gSASRec-Reducing-Overconfidence-in-Sequential-Recommendation-Trained-with-Negative-Sampling"><a href="#gSASRec-Reducing-Overconfidence-in-Sequential-Recommendation-Trained-with-Negative-Sampling" class="headerlink" title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling"></a>gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07192">http://arxiv.org/abs/2308.07192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asash/gsasrec">https://github.com/asash/gsasrec</a></li>
<li>paper_authors: Aleksandr Petrov, Craig Macdonald</li>
<li>for: This paper aims to address the issue of overconfidence in recommendation models, specifically in the popular SASRec model, and to propose a novel loss function and improved model that can mitigate overconfidence and improve performance.</li>
<li>methods: The paper proposes a novel Generalised Binary Cross-Entropy Loss function (gBCE) and a modified version of SASRec called gSASRec, which deploys an increased number of negatives and the gBCE loss to mitigate overconfidence.</li>
<li>results: The paper shows through detailed experiments on three datasets that gSASRec does not exhibit the overconfidence problem, and can outperform BERT4Rec in terms of NDCG score (e.g. +9.47% on the MovieLens-1M dataset) while requiring less training time (e.g. -73% training time on MovieLens-1M). Additionally, gSASRec is suitable for large datasets with more than 1 million items, unlike BERT4Rec.<details>
<summary>Abstract</summary>
A large catalogue size is one of the central challenges in training recommendation models: a large number of items makes them memory and computationally inefficient to compute scores for all items during training, forcing these models to deploy negative sampling. However, negative sampling increases the proportion of positive interactions in the training data, and therefore models trained with negative sampling tend to overestimate the probabilities of positive interactions a phenomenon we call overconfidence. While the absolute values of the predicted scores or probabilities are not important for the ranking of retrieved recommendations, overconfident models may fail to estimate nuanced differences in the top-ranked items, resulting in degraded performance. In this paper, we show that overconfidence explains why the popular SASRec model underperforms when compared to BERT4Rec. This is contrary to the BERT4Rec authors explanation that the difference in performance is due to the bi-directional attention mechanism. To mitigate overconfidence, we propose a novel Generalised Binary Cross-Entropy Loss function (gBCE) and theoretically prove that it can mitigate overconfidence. We further propose the gSASRec model, an improvement over SASRec that deploys an increased number of negatives and the gBCE loss. We show through detailed experiments on three datasets that gSASRec does not exhibit the overconfidence problem. As a result, gSASRec can outperform BERT4Rec (e.g. +9.47% NDCG on the MovieLens-1M dataset), while requiring less training time (e.g. -73% training time on MovieLens-1M). Moreover, in contrast to BERT4Rec, gSASRec is suitable for large datasets that contain more than 1 million items.
</details>
<details>
<summary>摘要</summary>
庞大的目录大小是训练推荐模型的中心挑战之一：大量的项目使得计算分数的计算成本高昂，使得这些模型不能在训练过程中计算所有项目的分数，因此需要使用负样本。然而，使用负样本增加了正交互动的比例在训练数据中，因此模型受负样本训练后会过度估计正交互动，这种现象我们称为过自信。这会导致模型估计排名顺序中的差异不准确，从而导致性能下降。在这篇论文中，我们表明了SASRec模型在比较BERT4Rec时的下降性能是由于过自信而不是BI-directional attention机制的解释。为了 Mitigate overconfidence，我们提出了一种通用二进制十字积分损失函数（gBCE），并证明了它可以 Mitigate overconfidence。此外，我们还提出了一种改进SASRec模型的gSASRec模型，该模型通过增加负样本数和gBCE损失函数来减少过自信。我们通过对三个数据集进行详细的实验，证明了gSASRec模型不受过自信问题。因此，gSASRec可以在MovieLens-1M数据集上超过BERT4Rec（+9.47% NDCG），同时具有较少的训练时间(-73% 训练时间）。此外，gSASRec模型适用于大于100万个项目的大数据集。
</details></li>
</ul>
<hr>
<h2 id="Improving-ICD-based-semantic-similarity-by-accounting-for-varying-degrees-of-comorbidity"><a href="#Improving-ICD-based-semantic-similarity-by-accounting-for-varying-degrees-of-comorbidity" class="headerlink" title="Improving ICD-based semantic similarity by accounting for varying degrees of comorbidity"></a>Improving ICD-based semantic similarity by accounting for varying degrees of comorbidity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07359">http://arxiv.org/abs/2308.07359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Janosch Schneider, Marius Adler, Christoph Ammer-Herrmenau, Alexander Otto König, Ulrich Sax, Jonas Hügel</li>
<li>for: 这篇论文的目的是为了找到类似的病人，以便评估治疗结果和临床决策支持。</li>
<li>methods: 这篇论文使用了世界各地医疗病理分类（ICD）代码，将病人的病理特征转换为数据集，然后使用Semantic Similarity算法进行相似性计算。</li>
<li>results: 这篇论文的结果显示，使用了我们提出的标准化运算符 Lateral Epicritical Density 和 Bipartite Graph Matching 的 комbination，可以实现最高的相似性分析效果，与专家评价的真实相似性相符。<details>
<summary>Abstract</summary>
Finding similar patients is a common objective in precision medicine, facilitating treatment outcome assessment and clinical decision support. Choosing widely-available patient features and appropriate mathematical methods for similarity calculations is crucial. International Statistical Classification of Diseases and Related Health Problems (ICD) codes are used worldwide to encode diseases and are available for nearly all patients. Aggregated as sets consisting of primary and secondary diagnoses they can display a degree of comorbidity and reveal comorbidity patterns. It is possible to compute the similarity of patients based on their ICD codes by using semantic similarity algorithms. These algorithms have been traditionally evaluated using a single-term expert rated data set.   However, real-word patient data often display varying degrees of documented comorbidities that might impair algorithm performance. To account for this, we present a scale term that considers documented comorbidity-variance. In this work, we compared the performance of 80 combinations of established algorithms in terms of semantic similarity based on ICD-code sets. The sets have been extracted from patients with a C25.X (pancreatic cancer) primary diagnosis and provide a variety of different combinations of ICD-codes. Using our scale term we yielded the best results with a combination of level-based information content, Leacock & Chodorow concept similarity and bipartite graph matching for the set similarities reaching a correlation of 0.75 with our expert's ground truth. Our results highlight the importance of accounting for comorbidity variance while demonstrating how well current semantic similarity algorithms perform.
</details>
<details>
<summary>摘要</summary>
在精度医学中，找到类似的患者是一项常见的目标，以便评估治疗结果和临床决策支持。选择广泛可用的患者特征和适当的数学方法进行相似计算是关键。国际疾病分类法（ICD）代码在全球使用，可以为大多数患者提供代码。将这些代码聚合成集合，包括主要和次要诊断，可以显示患者的诊断程度和诊断模式。可以使用语义相似算法计算患者之间的相似性。这些算法通常通过专家评分的数据集来评估。但在实际患者数据中，患者通常有不同程度的记录的相关疾病，这可能会影响算法性能。为解决这个问题，我们提出了一个权重因素，该因素考虑了记录的相关疾病变化。在这种情况下，我们比较了80种已知算法的语义相似性，基于ICD代码集。这些代码集来自患有C25.X（肝癌）主诊断的患者，并提供了不同的ICD代码组合。使用我们的权重因素，我们得到了最佳的结果，与专家的真实ground truth相匹配，相似度为0.75。我们的结果表明了考虑相关疾病变化的重要性，同时也展示了当前语义相似算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Predictions-Enhanced-Expert-guided-Meshing-with-Graph-Neural-Networks"><a href="#Conformal-Predictions-Enhanced-Expert-guided-Meshing-with-Graph-Neural-Networks" class="headerlink" title="Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks"></a>Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07358">http://arxiv.org/abs/2308.07358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahnobari/autosurf">https://github.com/ahnobari/autosurf</a></li>
<li>paper_authors: Amin Heyrani Nobari, Justin Rey, Suhas Kodali, Matthew Jones, Faez Ahmed</li>
<li>for: 这个论文的目的是自动生成CFD模型的网格，以提高计算流体力学的精度和效率。</li>
<li>methods: 这个论文使用图解树神经网络（GNN）和专家指导来自动生成CFD模型的网格。它还提出了一种新的3D分割算法，以及一种将预测从3D分割模型项目到CAD表面的方法。</li>
<li>results: 论文通过一个实际案例研究表明，自动生成的网格与专家生成的网格相比较，具有相似的质量，并且使得计算机程序能够正确地计算结果。此外，论文还比较了自动生成网格和适应重新分割的方法，发现自动生成网格比适应重新分割更快。代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/ahnobari/AutoSurf%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ahnobari/AutoSurf上获取。</a><details>
<summary>Abstract</summary>
Computational Fluid Dynamics (CFD) is widely used in different engineering fields, but accurate simulations are dependent upon proper meshing of the simulation domain. While highly refined meshes may ensure precision, they come with high computational costs. Similarly, adaptive remeshing techniques require multiple simulations and come at a great computational cost. This means that the meshing process is reliant upon expert knowledge and years of experience. Automating mesh generation can save significant time and effort and lead to a faster and more efficient design process. This paper presents a machine learning-based scheme that utilizes Graph Neural Networks (GNN) and expert guidance to automatically generate CFD meshes for aircraft models. In this work, we introduce a new 3D segmentation algorithm that outperforms two state-of-the-art models, PointNet++ and PointMLP, for surface classification. We also present a novel approach to project predictions from 3D mesh segmentation models to CAD surfaces using the conformal predictions method, which provides marginal statistical guarantees and robust uncertainty quantification and handling. We demonstrate that the addition of conformal predictions effectively enables the model to avoid under-refinement, hence failure, in CFD meshing even for weak and less accurate models. Finally, we demonstrate the efficacy of our approach through a real-world case study that demonstrates that our automatically generated mesh is comparable in quality to expert-generated meshes and enables the solver to converge and produce accurate results. Furthermore, we compare our approach to the alternative of adaptive remeshing in the same case study and find that our method is 5 times faster in the overall process of simulation. The code and data for this project are made publicly available at https://github.com/ahnobari/AutoSurf.
</details>
<details>
<summary>摘要</summary>
计算流体动力学（CFD）在不同的工程领域都广泛应用，但是准确的计算受到域的适当精细化的约束。高精度的精细化可能确保准确性，但是来自高计算成本。同时，适应精细化技术需要多次 simulations和高计算成本。这意味着精细化过程取决于专家知识和年代经验。自动生成精细化可以节省很多时间和努力，并且可以加速设计过程。本文提出了基于机器学习的方案，利用图像神经网络（GNN）和专家指导来自动生成飞机模型的CFD精细化。在这个工作中，我们提出了一种新的3D分割算法，其在surface classification方面比PointNet++和PointMLP两种状态态模型更高效。我们还提出了一种将预测从3D分割模型 projet到CAD surface的方法，使用确ensional predictions方法，该方法提供了边缘统计保证和稳定的不确定性评估和处理。我们发现，通过添加确ensional predictions，我们的方法可以避免精细化失败，即下REFINE。最后，我们通过一个实际的案例研究证明了我们的自动生成精细化与专家生成精细化相比质量相同，并且使得计算器能够 converges和生成准确结果。此外，我们与适应精细化的相同案例进行比较，发现我们的方法比适应精细化5倍快。我们将项目的代码和数据公开发布在https://github.com/ahnobari/AutoSurf上。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Learning-of-Quantum-States-Prepared-With-Few-Non-Clifford-Gates-II-Single-Copy-Measurements"><a href="#Efficient-Learning-of-Quantum-States-Prepared-With-Few-Non-Clifford-Gates-II-Single-Copy-Measurements" class="headerlink" title="Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates II: Single-Copy Measurements"></a>Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates II: Single-Copy Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07175">http://arxiv.org/abs/2308.07175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabee Grewal, Vishnu Iyer, William Kretschmer, Daniel Liang</li>
<li>for: 学习 $n$-qubit 量子状态，输出由具有最多 $t$ 单位逻辑门的电路。</li>
<li>methods: 使用单Copy测量来学习这类状态，而不需要双Copy测量。</li>
<li>results: 可以在 $\mathsf{poly}(n,2^t,1&#x2F;\epsilon)$ 时间和样本数量内学习这类状态，与之前所有的算法相同。<details>
<summary>Abstract</summary>
Recent work has shown that $n$-qubit quantum states output by circuits with at most $t$ single-qubit non-Clifford gates can be learned to trace distance $\epsilon$ using $\mathsf{poly}(n,2^t,1/\epsilon)$ time and samples. All prior algorithms achieving this runtime use entangled measurements across two copies of the input state. In this work, we give a similarly efficient algorithm that learns the same class of states using only single-copy measurements.
</details>
<details>
<summary>摘要</summary>
最近的工作表明，$n$-粒子量子状态由具有最多$t$个单元素非束地 gates生成的电路可以使用$\mathsf{poly}(n,2^t,1/\epsilon)$时间和样本来跟踪距离$\epsilon$。所有先前的算法达到这个 runtime 都使用了两份输入状态的排合测试。在这种工作中，我们给出了同样的效率的算法，可以使用单份输入状态来学习同一类型的状态。Note:* " $n$-qubit quantum states" is translated as " $n$-粒子量子状态" (n-qubit quantum states)* "circuits with at most $t$ single-qubit non-Clifford gates" is translated as "具有最多$t$个单元素非束地 gates的电路" (circuits with at most t single-qubit non-Clifford gates)* "can be learned to trace distance $\epsilon$ using $\mathsf{poly}(n,2^t,1/\epsilon)$ time and samples" is translated as "可以使用$\mathsf{poly}(n,2^t,1/\epsilon)$时间和样本来跟踪距离$\epsilon$" (can be learned to trace distance ε using polynomial time and samples)* "All prior algorithms achieving this runtime use entangled measurements across two copies of the input state" is translated as "所有先前的算法达到这个 runtime 都使用了两份输入状态的排合测试" (all previous algorithms achieving this runtime use entangled measurements across two copies of the input state)* "In this work, we give a similarly efficient algorithm that learns the same class of states using only single-copy measurements" is translated as "在这种工作中，我们给出了同样的效率的算法，可以使用单份输入状态来学习同一类型的状态" (in this work, we give a similarly efficient algorithm that learns the same class of states using only single-copy measurements)
</details></li>
</ul>
<hr>
<h2 id="PitchNet-A-Fully-Convolutional-Neural-Network-for-Pitch-Estimation"><a href="#PitchNet-A-Fully-Convolutional-Neural-Network-for-Pitch-Estimation" class="headerlink" title="PitchNet: A Fully Convolutional Neural Network for Pitch Estimation"></a>PitchNet: A Fully Convolutional Neural Network for Pitch Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07170">http://arxiv.org/abs/2308.07170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy Cochoy</li>
<li>for: 这项研究旨在提高人声中的抑 pitch 检测精度，以便在音乐和语音处理领域中进行更加精准的抑 pitch EXTraction。</li>
<li>methods: 该研究提出了一种基于卷积神经网络的 “PitchNet”，用于从人声中提取抑 pitch。该网络结合自相关函数和深度学习技术，以便优化抑 pitch 检测的精度。</li>
<li>results: 对于 synthetic sounds、opera recordings 和 time-stretched vowels 等数据集的评估表明，PitchNet 能够准确地检测人声中的抑 pitch。这项研究为音乐和语音处理领域中的抑 pitch EXTraction 开创了新的可能性。<details>
<summary>Abstract</summary>
In the domain of music and sound processing, pitch extraction plays a pivotal role. This research introduces "PitchNet", a convolutional neural network tailored for pitch extraction from the human singing voice, including acapella performances. Integrating autocorrelation with deep learning techniques, PitchNet aims to optimize the accuracy of pitch detection. Evaluation across datasets comprising synthetic sounds, opera recordings, and time-stretched vowels demonstrates its efficacy. This work paves the way for enhanced pitch extraction in both music and voice settings.
</details>
<details>
<summary>摘要</summary>
在音乐和声音处理领域中，抽取高度扮演着关键性的角色。本研究介绍“PitchNet”，一种适用于人声歌唱中的声调抽取 convolutional neural network（CNN）。通过对深度学习技术与自相关函数的组合，PitchNet目标是提高声调检测精度。对于 synthetic sounds、opera recording 和时间压缩词汇等数据集进行评估，PitchNet 的效果得到证明。这项工作将为音乐和声音设置中的声调抽取带来进一步的改进。Note: "Simplified Chinese" refers to the standardized form of Chinese used in mainland China, which is different from Traditional Chinese used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="SPEGTI-Structured-Prediction-for-Efficient-Generative-Text-to-Image-Models"><a href="#SPEGTI-Structured-Prediction-for-Efficient-Generative-Text-to-Image-Models" class="headerlink" title="SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models"></a>SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10997">http://arxiv.org/abs/2308.10997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadeep Jayasumana, Daniel Glasner, Srikumar Ramalingam, Andreas Veit, Ayan Chakrabarti, Sanjiv Kumar</li>
<li>for: 提高文本生成图像模型的计算效率，使其能够更快地生成高质量的图像。</li>
<li>methods: 使用Markov Random Field（MRF）模型来加速 Muse 模型的推理过程，从而提高图像生成的计算效率。</li>
<li>results: 通过使用 MRF 模型，可以significantly reduce the required number of Muse prediction steps，并且在各个空间位置上编码图像元素之间的兼容性，以提高图像质量和计算效率。<details>
<summary>Abstract</summary>
Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MRF inference as a differentiable neural-network layer. Our full model, SPEGTI, uses this proposed MRF model to speed up Muse by 1.5X with no loss in output image quality.
</details>
<details>
<summary>摘要</summary>
现代文本到图像生成模型可以生成高质量的图像，这些图像不仅具有摄影真实性，还能够准确地反映文本提示。然而，这种质量来自于费时的计算成本：大多数这些模型都是迭代的，需要多次运行推理，以确保不同区域的图像与文本提示保持一致。在这种情况下，我们提出了一种轻量级的方法，使用Markov随机场（MRF）模型来实现不同区域图像的兼容性。这种方法与最近提出的Muse模型结合使用，并且可以减少Muse预测步骤的数量，从而大幅降低计算成本。我们的全模型SPEGTI使用这种MRF模型，可以帮助Muse快速推理1.5倍，而无需 sacrifi额外的图像质量。
</details></li>
</ul>
<hr>
<h2 id="Pairing-interacting-protein-sequences-using-masked-language-modeling"><a href="#Pairing-interacting-protein-sequences-using-masked-language-modeling" class="headerlink" title="Pairing interacting protein sequences using masked language modeling"></a>Pairing interacting protein sequences using masked language modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07136">http://arxiv.org/abs/2308.07136</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bitbol-lab/diffpalm">https://github.com/bitbol-lab/diffpalm</a></li>
<li>paper_authors: Umberto Lupo, Damiano Sgarbossa, Anne-Florence Bitbol</li>
<li>For: The paper aims to predict which proteins interact together from their amino acid sequences.* Methods: The paper uses protein language models trained on multiple sequence alignments, specifically MSA Transformer and the EvoFormer module of AlphaFold, to pair interacting protein sequences.* Results: The proposed method, DiffPALM, outperforms existing coevolution-based pairing methods on difficult benchmarks of shallow multiple sequence alignments and improves the structure prediction of some eukaryotic protein complexes by AlphaFold-Multimer.Here’s the same information in Simplified Chinese:* For: 文章目标是从蛋白质序列中预测哪些蛋白质进行互作。* Methods: 文章使用多个序列对 alignment 训练的蛋白质语言模型，特别是 MSA Transformer 和 AlphaFold 的 EvoFormer 模块，来对互作蛋白质序列进行对应。* Results: 提案的方法 DiffPALM 在难度较高的多个序列对上表现出优于现有的共演化基本方法，并在一些细胞蛋白质复合物的结构预测中达到竞争性表现。<details>
<summary>Abstract</summary>
Predicting which proteins interact together from amino-acid sequences is an important task. We develop a method to pair interacting protein sequences which leverages the power of protein language models trained on multiple sequence alignments, such as MSA Transformer and the EvoFormer module of AlphaFold. We formulate the problem of pairing interacting partners among the paralogs of two protein families in a differentiable way. We introduce a method called DiffPALM that solves it by exploiting the ability of MSA Transformer to fill in masked amino acids in multiple sequence alignments using the surrounding context. MSA Transformer encodes coevolution between functionally or structurally coupled amino acids. We show that it captures inter-chain coevolution, while it was trained on single-chain data, which means that it can be used out-of-distribution. Relying on MSA Transformer without fine-tuning, DiffPALM outperforms existing coevolution-based pairing methods on difficult benchmarks of shallow multiple sequence alignments extracted from ubiquitous prokaryotic protein datasets. It also outperforms an alternative method based on a state-of-the-art protein language model trained on single sequences. Paired alignments of interacting protein sequences are a crucial ingredient of supervised deep learning methods to predict the three-dimensional structure of protein complexes. DiffPALM substantially improves the structure prediction of some eukaryotic protein complexes by AlphaFold-Multimer, without significantly deteriorating any of those we tested. It also achieves competitive performance with using orthology-based pairing.
</details>
<details>
<summary>摘要</summary>
预测 protein sequences 中的互作对是一项重要任务。我们开发了一种方法，可以将 protein sequence 中的互作对级联起来，这种方法利用了多个序列对 alignment（如 MSA Transformer 和 EvoFormer 模块）训练的 protein language model。我们将这个问题转化为一个可导的问题，并提出了一种名为 DiffPALM 的方法来解决它。DiffPALM 利用了 MSA Transformer 可以填充遮盖的氨基酸，通过周围的上下文来填充它们。MSA Transformer 编码了功能或结构上的氨基酸之间的共演化，我们表明它可以在单链数据上训练，并在不需要微调的情况下在多链数据上进行预测。与现有的共演化基本方法相比，DiffPALM 在困难的多链对 alignments 上表现出色，同时也在不需要微调的情况下进行预测。此外，DiffPALM 还可以和一种基于 state-of-the-art 蛋白质语言模型进行比较，并且在一些欧化蛋白质复合物的结构预测中表现出色。Paired alignments of interacting protein sequences 是深度学习方法预测蛋白质复合物的重要组成部分。DiffPALM 在这些复合物的结构预测中提供了重要的改进。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-is-All-a-Graph-Needs"><a href="#Natural-Language-is-All-a-Graph-Needs" class="headerlink" title="Natural Language is All a Graph Needs"></a>Natural Language is All a Graph Needs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07134">http://arxiv.org/abs/2308.07134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang</li>
<li>for: 本研究旨在探讨 Whether large language models (LLMs) can replace graph neural networks (GNNs) as the foundation model for graphs.</li>
<li>methods: 我们提出了 InstructGLM (Instruction-finetuned Graph Language Model)，使用自然语言指令设计了高度可扩展的 prompt，并使用自然语言描述图像的几何结构和节点特征。</li>
<li>results: 我们的方法在 ogbn-arxiv、Cora 和 PubMed 数据集上超过了所有竞争 GNN 基elines，这说明了我们的方法的有效性，并且推照generative大型语言模型为图机器学习的基础模型。<details>
<summary>Abstract</summary>
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.
</details>
<details>
<summary>摘要</summary>
“大规模预训练语言模型，如ChatGPT，对人工智能多种研究领域产生了革命性的影响。基于Transformers的大语言模型（LLM）逐渐取代了CNNs和RNNs，统一了计算机视觉和自然语言处理领域。与独立存在的数据，如图像、视频或文本，相比，图表是一种包含丰富结构和关系信息的数据类型。同时，自然语言作为最表达力强的媒介，能够描述复杂结构。然而，将图学学习问题 integrate into the generative language modeling framework的现有工作很有限。随着大语言模型的重要性不断增长，我们需要探索 Whether LLMs可以取代GNNs作为图学基础模型。本文提出InstructGLM（基于natural language instruction的图语言模型），系统地设计了可扩展的提示，使用自然语言描述图表的结构和节点特征，并使用LLM进行图学学习和推理。我们的方法在ogbn-arxiv、Cora和PubMed数据集上都超过了所有的竞争GNN基elines，这 demonstates了我们的方法的有效性，并照亮了大语言模型作为图学基础模型的可能性。”
</details></li>
</ul>
<hr>
<h2 id="Implementation-of-The-Future-of-Drug-Discovery-QuantumBased-Machine-Learning-Simulation-QMLS"><a href="#Implementation-of-The-Future-of-Drug-Discovery-QuantumBased-Machine-Learning-Simulation-QMLS" class="headerlink" title="Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS)"></a>Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08561">http://arxiv.org/abs/2308.08561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yew Kee Wong, Yifan Zhou, Yan Shing Liang, Haichuan Qiu, Yu Xi Wu, Bin He</li>
<li>for: 这份研究目的是为了缩短药物开发过程的时间和成本，以及创新一种能够在三到六个月内完成整个R&amp;D过程，并且只需五十到八十千美元的成本。</li>
<li>methods: 这篇研究使用的方法包括机器学习分子生成（MLMG）和量子模拟（QS），两者共同实现了精确地预测药物的结构和功能。MLMG根据目标蛋白质的分子结构来生成可能的击中者，而QS则对这些分子进行筛选，以确定它们对目标蛋白质的反应和紧缩效果。</li>
<li>results: 这篇研究的结果显示，使用机器学习和量子模拟的融合方法可以快速生成高效的药物材料，并且可以实现对药物的评估和筛选。这些材料可以在几个月内完成整个R&amp;D过程，并且可以降低成本至五十到八十千美元。<details>
<summary>Abstract</summary>
The Research & Development (R&D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction effectiveness and safety, creating a few dozen pre-clinical-trail-ready drugs. This paper is based on our first paper, where we pitched the concept of machine learning combined with quantum simulations. In this paper we will go over the detailed design and framework of QMLS, including MLMG, MLMV, and QS.
</details>
<details>
<summary>摘要</summary>
研发（R&D）阶段是药品开发的长途和昂贵的过程。为了革新这个过程，我们提出了新的概念——量子机器学学习（QMLS），可以缩短整个R&D阶段的时间至3-6个月，并降低成本至50-80万美元。在潜在药物生成（Hit Generation）阶段，机器学学习分子生成（MLMG）根据目标蛋白质的分子结构生成可能的潜在药物，而量子模拟（QS）则从初步试验中筛选出与目标蛋白质具有强烈反应和结合效果的分子。在药物优化阶段，得到的分子 variants 由机器学学习分子变化（MLMV）进行了数十个变化，而其他分子则只进行了几个变化。最后，所有优化后的分子都会经过多轮QS筛选，以确保它们具有高效性和安全性，从而生成数十个前期临床药物。本文是我们之前的第一篇论文的续写，我们在这篇文章中将详细介绍QMLS的设计和框架，包括MLMG、MLMV和QS。
</details></li>
</ul>
<hr>
<h2 id="A-Time-aware-tensor-decomposition-for-tracking-evolving-patterns"><a href="#A-Time-aware-tensor-decomposition-for-tracking-evolving-patterns" class="headerlink" title="A Time-aware tensor decomposition for tracking evolving patterns"></a>A Time-aware tensor decomposition for tracking evolving patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07126">http://arxiv.org/abs/2308.07126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Chatzis, Max Pfeffer, Pedro Lind, Evrim Acar</li>
<li>for: 本研究旨在提取时间序列数据中的慢慢发展模式，并且能够考虑时间序列中的变化。</li>
<li>methods: 本文提出了一种基于PARAFAC2的时间regularization方法，即 temporal PARAFAC2（tPARAFAC2），用于抽取时间序列数据中的慢慢发展模式。</li>
<li>results: 经过广泛的实验 validate that tPARAFAC2可以准确地捕捉时间序列数据中的慢慢发展模式，并且表现比PARAFAC2和时间平滑矩阵因子化regularization方法更好。<details>
<summary>Abstract</summary>
Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regularization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate("Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately, performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regularization.") result:时间演化数据集经常可以被视为一个高阶张量，其中一个方向是时间方向。虽然tensor分解已经成功地用于捕捉高阶数据集中的下面模式，但是时间方面通常被忽略，允许时间点的重新排序。在最近的研究中，temporal regularizers被添加到时间方面以解决这个问题。然而，现有的方法仍然不允许下面模式在时间上发生变化（例如，大脑中的空间变化，话题中的上下文变化）。在这篇论文中，我们提议时间PARAFAC2（tPARAFAC2）：基于PARAFAC2的张量分解方法，带有时间正则化，以EXTRACT从时间数据中逐渐发展的模式。通过对 sintetic数据进行了广泛的实验，我们示出了tPARAFAC2可以准确地捕捉下面模式，并且perform better than PARAFAC2和 Coupled Matrix Factorization with temporal smoothness regularization。
</details></li>
</ul>
<hr>
<h2 id="Active-Bird2Vec-Towards-End-to-End-Bird-Sound-Monitoring-with-Transformers"><a href="#Active-Bird2Vec-Towards-End-to-End-Bird-Sound-Monitoring-with-Transformers" class="headerlink" title="Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with Transformers"></a>Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07121">http://arxiv.org/abs/2308.07121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Rauch, Raphael Schwinger, Moritz Wirth, Bernhard Sick, Sven Tomforde, Christoph Scholz</li>
<li>for: 本研究旨在推动鸟叫声监测领域的终端学习转移，通过结合自动学习（SSL）和深度活动学习（DAL），以便直接处理原始音频数据，并生成高质量鸟叫声表示。</li>
<li>methods: 本研究使用变换器模型，并通过自动学习生成高质量鸟叫声表示，以便加速环境变化评估和决策过程。此外，通过深度活动学习，减少人工标注数据的依赖，提高了鸟叫声识别任务的效果。</li>
<li>results: 本研究通过对不同变换器模型进行比较分析，评估它们在鸟叫声识别任务中的效果。同时，通过使用Huggingface Datasets，生成了一个完整的任务集，以便提高未来的比较性和可重现性。<details>
<summary>Abstract</summary>
We propose a shift towards end-to-end learning in bird sound monitoring by combining self-supervised (SSL) and deep active learning (DAL). Leveraging transformer models, we aim to bypass traditional spectrogram conversions, enabling direct raw audio processing. ActiveBird2Vec is set to generate high-quality bird sound representations through SSL, potentially accelerating the assessment of environmental changes and decision-making processes for wind farms. Additionally, we seek to utilize the wide variety of bird vocalizations through DAL, reducing the reliance on extensively labeled datasets by human experts. We plan to curate a comprehensive set of tasks through Huggingface Datasets, enhancing future comparability and reproducibility of bioacoustic research. A comparative analysis between various transformer models will be conducted to evaluate their proficiency in bird sound recognition tasks. We aim to accelerate the progression of avian bioacoustic research and contribute to more effective conservation strategies.
</details>
<details>
<summary>摘要</summary>
我们提议将学习方法转换为终端学习，通过结合自动生成监督（SSL）和深度活动学习（DAL），利用变换器模型，以直接处理原始音频数据，并不需要传统的spectrogram转换。我们通过ActiveBird2Vec生成高质量的鸟叫表示，通过SSL可能加速环境变化评估和风车决策过程，同时通过DAL减少人工标注数据的依赖，提高生物听音研究的可比性和可重复性。我们计划使用Huggingface集成数据，并进行不同变换器模型之间的比较分析，以评估它们在鸟叫识别任务中的效果。我们希望通过加速鸟类生物听音研究，为生态保护策略做出更有效的贡献。
</details></li>
</ul>
<hr>
<h2 id="Neural-radiance-fields-in-the-industrial-and-robotics-domain-applications-research-opportunities-and-use-cases"><a href="#Neural-radiance-fields-in-the-industrial-and-robotics-domain-applications-research-opportunities-and-use-cases" class="headerlink" title="Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases"></a>Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07118">http://arxiv.org/abs/2308.07118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maftej/iisnerf">https://github.com/maftej/iisnerf</a></li>
<li>paper_authors: Eugen Šlapak, Enric Pardo, Matúš Dopiriak, Taras Maksymyuk, Juraj Gazda</li>
<li>for: 这篇论文旨在探讨基于提供训练图像的神经辐射场（NeRF）在不同工业子领域的应用前景，以及未来研究方向。</li>
<li>methods: 本论文使用NeRF来学习3D场景表示，并提供了一系列证明NeRF在工业领域的应用可行性的实验。这些实验包括基于NeRF的视频压缩技术和使用NeRF进行3D运动估计以避免碰撞。</li>
<li>results: 在视频压缩实验中，我们获得了1920x1080和300x168分辨率下的压缩率为48%和74%。在3D动画中使用D-NeRF进行3D运动估计，得到了平均PSNR值为23 dB和SSIM值为0.97。<details>
<summary>Abstract</summary>
The proliferation of technologies, such as extended reality (XR), has increased the demand for high-quality three-dimensional (3D) graphical representations. Industrial 3D applications encompass computer-aided design (CAD), finite element analysis (FEA), scanning, and robotics. However, current methods employed for industrial 3D representations suffer from high implementation costs and reliance on manual human input for accurate 3D modeling. To address these challenges, neural radiance fields (NeRFs) have emerged as a promising approach for learning 3D scene representations based on provided training 2D images. Despite a growing interest in NeRFs, their potential applications in various industrial subdomains are still unexplored. In this paper, we deliver a comprehensive examination of NeRF industrial applications while also providing direction for future research endeavors. We also present a series of proof-of-concept experiments that demonstrate the potential of NeRFs in the industrial domain. These experiments include NeRF-based video compression techniques and using NeRFs for 3D motion estimation in the context of collision avoidance. In the video compression experiment, our results show compression savings up to 48\% and 74\% for resolutions of 1920x1080 and 300x168, respectively. The motion estimation experiment used a 3D animation of a robotic arm to train Dynamic-NeRF (D-NeRF) and achieved an average peak signal-to-noise ratio (PSNR) of disparity map with the value of 23 dB and an structural similarity index measure (SSIM) 0.97.
</details>
<details>
<summary>摘要</summary>
技术的普及，如扩展现实（XR），提高了高品质三维图形表示的需求。工业三维应用包括计算机支持设计（CAD）、Finite Element分析（FEA）、扫描和机器人。然而，现有的工业三维表示方法受到高实施成本和人工输入的限制，以获得准确的三维模型。为解决这些挑战，神经辐射场（NeRF）已经出现为了学习基于提供训练图像的三维场景表示方法。尽管有关NeRF的兴趣在不断增长，但它们在不同的工业子领域的潜在应用仍然未得到了足够的探索。在这篇论文中，我们提供了工业应用场景中NeRF的全面检查，并提供未来研究方向的指导。我们还提供了一系列的证明性实验，以示NeRF在工业领域的潜在应用。这些实验包括基于NeRF的视频压缩技术和使用NeRF进行3D运动估计，以避免碰撞。在视频压缩实验中，我们的结果表明，对于分辨率为1920x1080和300x168的视频，可以实现压缩率为48%和74%。在3D动画中使用D-NeRF进行3D运动估计实验，我们获得了平均的干扰比率（PSNR）为23 dB和结构相似度指标（SSIM）为0.97。
</details></li>
</ul>
<hr>
<h2 id="iSTFTNet2-Faster-and-More-Lightweight-iSTFT-Based-Neural-Vocoder-Using-1D-2D-CNN"><a href="#iSTFTNet2-Faster-and-More-Lightweight-iSTFT-Based-Neural-Vocoder-Using-1D-2D-CNN" class="headerlink" title="iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN"></a>iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07117">http://arxiv.org/abs/2308.07117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, Shogo Seki</li>
<li>for: 快速、轻量级、高精度的语音合成</li>
<li>methods: 使用快速和轻量级的1D CNN作为基础网络，并将一些神经网络替换为iSTFT，以提高速度和轻量化。</li>
<li>results: iSTFTNet2比iSTFTNet更快速和轻量级，且音质相对保持不变。可以在<a target="_blank" rel="noopener" href="https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/%E4%B8%AD%E4%B8%8B%E8%BD%BD%E9%9F%B3%E9%A2%91%E6%A0%B7%E6%9C%AC%E3%80%82">https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/中下载音频样本。</a><details>
<summary>Abstract</summary>
The inverse short-time Fourier transform network (iSTFTNet) has garnered attention owing to its fast, lightweight, and high-fidelity speech synthesis. It obtains these characteristics using a fast and lightweight 1D CNN as the backbone and replacing some neural processes with iSTFT. Owing to the difficulty of a 1D CNN to model high-dimensional spectrograms, the frequency dimension is reduced via temporal upsampling. However, this strategy compromises the potential to enhance the speed. Therefore, we propose iSTFTNet2, an improved variant of iSTFTNet with a 1D-2D CNN that employs 1D and 2D CNNs to model temporal and spectrogram structures, respectively. We designed a 2D CNN that performs frequency upsampling after conversion in a few-frequency space. This design facilitates the modeling of high-dimensional spectrograms without compromising the speed. The results demonstrated that iSTFTNet2 made iSTFTNet faster and more lightweight with comparable speech quality. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（iSTFTNet）在最近引起了关注，因为它具有快速、轻量级和高精度的语音生成特点。它使用快速和轻量级的1D CNN作为基础模型，并将一些神经网络过程替换为iSTFT。由于1D CNN在模型高维спект罗格的问题上难以处理，因此在时间增サンプリング的策略可能会增加速度的约束。为了解决这个问题，我们提出了iSTFTNet2，它是iSTFTNet的改进版本，使用1D-2D CNN来模型时间和спект罗格结构。我们设计了一个2D CNN，它在几个频率空间中进行频率增サンプリング。这种设计允许模型高维спект罗格无需增加速度约束。结果表明，iSTFTNet2使得iSTFTNet更快速和轻量级，同时保持语音质量的同等性。有关audio samples的详细信息请参考https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/.
</details></li>
</ul>
<hr>
<h2 id="Ada-QPacknet-–-adaptive-pruning-with-bit-width-reduction-as-an-efficient-continual-learning-method-without-forgetting"><a href="#Ada-QPacknet-–-adaptive-pruning-with-bit-width-reduction-as-an-efficient-continual-learning-method-without-forgetting" class="headerlink" title="Ada-QPacknet – adaptive pruning with bit width reduction as an efficient continual learning method without forgetting"></a>Ada-QPacknet – adaptive pruning with bit width reduction as an efficient continual learning method without forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07939">http://arxiv.org/abs/2308.07939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcin Pietroń, Dominik Żurek, Kamil Faber, Roberto Corizzo</li>
<li>for: 这个论文是为了解决深度学习模型在动态和复杂环境中学习效率差的问题而写的。</li>
<li>methods: 这个论文提出了一种基于架构的 kontinuous learning 方法，称为 Ada-QPacknet，它通过提取每个任务的子网络来实现。这种方法的关键特点是它的容量，它使用高效的线性和非线性归一化方法来减少模型的大小。</li>
<li>results: 在Well-known CL 场景中，hybrid 8和4位量化实现了浮点子网络的相似准确性。这个方法比大多数 CL 策略在任务和类增量enario中表现出色，并且在Well-known episode combinations 中测试了这个算法，与最流行的 CL 策略进行了比较。<details>
<summary>Abstract</summary>
Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combinations and compared with most popular algorithms. Results show that proposed approach outperforms most of the CL strategies in task and class incremental scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Age-Stratified-Differences-in-Morphological-Connectivity-Patterns-in-ASD-An-sMRI-and-Machine-Learning-Approach"><a href="#Age-Stratified-Differences-in-Morphological-Connectivity-Patterns-in-ASD-An-sMRI-and-Machine-Learning-Approach" class="headerlink" title="Age-Stratified Differences in Morphological Connectivity Patterns in ASD: An sMRI and Machine Learning Approach"></a>Age-Stratified Differences in Morphological Connectivity Patterns in ASD: An sMRI and Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07356">http://arxiv.org/abs/2308.07356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokul Manoj, Sandeep Singh Sengar, Jac Fredo Agastinose Ronickom</li>
<li>for: 这个研究的目的是为了比较不同年龄组的自闭症诊断使用形态特征（MF）和形态连接特征（MCF）的效果。</li>
<li>methods: 这个研究使用了两个公共可用的数据库—ABIDE-I和ABIDE-II—获得了Structural Magnetic Resonance Imaging（sMRI）数据，并将数据 pré-processed using a standard pipeline，然后将数据分割成根据Destrieux atlas的148个区域，EXTRACTED área、厚度、体积和平均弯曲信息，并使用了统计t检测（p&lt;0.05）来标识特征，然后使用Random Forest（RF）分类器进行训练。</li>
<li>results: 研究结果表明，6岁到11岁的年龄组的性能最高，其次是6岁到18岁和11岁到18岁的年龄组，在MF和MCF中都有高的表现。总的来说，MCF与RF在6岁到11岁的年龄组中表现最好，其准确率、F1 score、回归率和准确率分别为75.8%、83.1%、86%和80.4%。<details>
<summary>Abstract</summary>
Purpose: Age biases have been identified as an essential factor in the diagnosis of ASD. The objective of this study was to compare the effect of different age groups in classifying ASD using morphological features (MF) and morphological connectivity features (MCF). Methods: The structural magnetic resonance imaging (sMRI) data for the study was obtained from the two publicly available databases, ABIDE-I and ABIDE-II. We considered three age groups, 6 to 11, 11 to 18, and 6 to 18, for our analysis. The sMRI data was pre-processed using a standard pipeline and was then parcellated into 148 different regions according to the Destrieux atlas. The area, thickness, volume, and mean curvature information was then extracted for each region which was used to create a total of 592 MF and 10,878 MCF for each subject. Significant features were identified using a statistical t-test (p<0.05) which was then used to train a random forest (RF) classifier. Results: The results of our study suggested that the performance of the 6 to 11 age group was the highest, followed by the 6 to 18 and 11 to 18 ages in both MF and MCF. Overall, the MCF with RF in the 6 to 11 age group performed better in the classification than the other groups and produced an accuracy, F1 score, recall, and precision of 75.8%, 83.1%, 86%, and 80.4%, respectively. Conclusion: Our study thus demonstrates that morphological connectivity and age-related diagnostic model could be an effective approach to discriminating ASD.
</details>
<details>
<summary>摘要</summary>
目的：识别自闭症（ASD）的年龄因素已被证明是关键因素。本研究的目的是比较不同年龄组的分类ASD使用形态特征（MF）和形态连接特征（MCF）的效果。方法：我们从公共数据库ABIDE-I和ABIDE-II中获得了structural magnetic resonance imaging（sMRI）数据。我们分为三个年龄组：6-11岁、11-18岁和6-18岁进行分析。经过标准化处理后，sMRI数据被分割成根据Desitrieux大脑 Atlase所分的148个区域。然后，每个区域中的面积、厚度、体积和平均弯曲信息被提取，并用于创建共计592个MF和10878个MCF。通过统计t检测（p<0.05）进行了特征选择，并用于训练随机森林（RF）分类器。结果：我们的研究结果表明，6-11岁年龄组的性能最高，然后是6-18岁和11-18岁年龄组，在MF和MCF中都是如此。总的来说，在6-11岁年龄组中，MCF与RF的结合使得分类性能更高，其中的准确率、F1分数、回归率和精度分别为75.8%、83.1%、86%和80.4%。结论：这些结果表明，使用形态连接和年龄相关的诊断模型可以有效地识别ASD。
</details></li>
</ul>
<hr>
<h2 id="InsTag-Instruction-Tagging-for-Analyzing-Supervised-Fine-tuning-of-Large-Language-Models"><a href="#InsTag-Instruction-Tagging-for-Analyzing-Supervised-Fine-tuning-of-Large-Language-Models" class="headerlink" title="#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models"></a>#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07074">http://arxiv.org/abs/2308.07074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ofa-sys/instag">https://github.com/ofa-sys/instag</a></li>
<li>paper_authors: Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, Jingren Zhou</li>
<li>for: 这篇论文的目的是提高基础模型的 instruction-following 能力，并通过训练细化（SFT）来实现这一目标。</li>
<li>methods: 该论文使用了一种名为 InsTag 的开源细化标注工具，用于标注 SFT 数据集中的样本，并定义了 instrucion 多样性和复杂性的量化分析。</li>
<li>results: 研究发现，通过使用 InsTag 选择的6000个多样性和复杂性的样本，可以提高基础模型的表现，并且与训练数据量相比，TagLM 模型的表现更高。<details>
<summary>Abstract</summary>
Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and complexity. We open-source InsTag in https://github.com/OFA-Sys/InsTag.
</details>
<details>
<summary>摘要</summary>
基础语言模型通过监督微调（SFT）获得指令遵从能力。多样性和复杂性被视为成功SFT数据集的关键因素，但其定义还未得到明确的量化分析。本工作提出InsTag，一种开放集标记器，用于在SFT数据集中标记样本基于 semantics和意图，并定义指令多样性和复杂性。我们获得了6.6K个标签来描述全面的用户查询。然后我们分析了流行的开源SFT数据集，发现模型能力随着数据集的多样性和复杂性增加。基于这个观察，我们提出了基于InsTag的数据选择器，选择6K个多样性和复杂性最高的样本从开源数据集进行练习。经过微调，我们获得了TagLM模型，其性能在MT-Bench评估中较开源模型高，证明了查询多样性和复杂性的重要性。我们在https://github.com/OFA-Sys/InsTag上开源了InsTag。
</details></li>
</ul>
<hr>
<h2 id="Machine-Unlearning-Solutions-and-Challenges"><a href="#Machine-Unlearning-Solutions-and-Challenges" class="headerlink" title="Machine Unlearning: Solutions and Challenges"></a>Machine Unlearning: Solutions and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07061">http://arxiv.org/abs/2308.07061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Xu, Zihan Wu, Cong Wang, Xiaohua Jia</li>
<li>for: 本研究旨在提供一份系统性的机器学习忘记研究分类和分析，以便为 Selective Data Removal（SDR）技术的发展提供指导。</li>
<li>methods: 本研究分类了现有的机器学习忘记研究，包括精确忘记和近似忘记两种方法。精确忘记方法可以完全除去训练数据的影响，而近似忘记方法可以有效地减少影响。</li>
<li>results: 本研究对现有的机器学习忘记方法进行了 kritische 分析，并提出了未来研究的方向。通过这种分析，研究人员可以更好地了解机器学习忘记技术的优缺点，并为实际应用提供指导。<details>
<summary>Abstract</summary>
Machine learning models may inadvertently memorize sensitive, unauthorized, or malicious data, posing risks of privacy violations, security breaches, and performance deterioration. To address these issues, machine unlearning has emerged as a critical technique to selectively remove specific training data points' influence on trained models. This paper provides a comprehensive taxonomy and analysis of machine unlearning research. We categorize existing research into exact unlearning that algorithmically removes data influence entirely and approximate unlearning that efficiently minimizes influence through limited parameter updates. By reviewing the state-of-the-art solutions, we critically discuss their advantages and limitations. Furthermore, we propose future directions to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning. This paper provides researchers with a roadmap of open problems, encouraging impactful contributions to address real-world needs for selective data removal.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:机器学习模型可能偶发性记忆敏感、未授权或黑客数据，导致隐私违反、安全泄露和性能下降。为解决这些问题，机器忘记技术已经成为一种重要的解决方案，可以选择性地删除训练模型中具有特定影响的数据点。本文提供了机器忘记的全面分类和分析，并评估了现有的研究。我们将现有的研究分为单精度忘记和近似忘记两种，并评估了它们的优点和限制。此外，我们还提出了未来的方向，以推进机器忘记的发展，并将其视为可靠和适应式机器学习的重要能力。本文为研究人员提供了一个开启问题的路线图，促进了影响性的贡献，以解决实际需求中的选择性数据移除。
</details></li>
</ul>
<hr>
<h2 id="Diagnosis-of-Scalp-Disorders-using-Machine-Learning-and-Deep-Learning-Approach-–-A-Review"><a href="#Diagnosis-of-Scalp-Disorders-using-Machine-Learning-and-Deep-Learning-Approach-–-A-Review" class="headerlink" title="Diagnosis of Scalp Disorders using Machine Learning and Deep Learning Approach – A Review"></a>Diagnosis of Scalp Disorders using Machine Learning and Deep Learning Approach – A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07052">http://arxiv.org/abs/2308.07052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishabh Tiwari, Jatin Moolchandani, Shamla Mantri</li>
<li>for: 这篇论文主要针对scalp病的诊断和分类。</li>
<li>methods: 该论文使用了深度学习技术，包括Convolutional Neural Networks（CNN）和 Fully Connected Networks（FCN），以及一个APP，以帮助诊断scalp病。</li>
<li>results: 该论文的实验结果表明，使用深度学习模型可以高精度地诊断scalp病，其中一个方法的准确率为97.41%-99.09%，另一个方法的准确率为82.9%，而使用机器学习算法也可以高精度地诊断健康的scalp和脱发病。<details>
<summary>Abstract</summary>
The morbidity of scalp diseases is minuscule compared to other diseases, but the impact on the patient's life is enormous. It is common for people to experience scalp problems that include Dandruff, Psoriasis, Tinea-Capitis, Alopecia and Atopic-Dermatitis. In accordance with WHO research, approximately 70% of adults have problems with their scalp. It has been demonstrated in descriptive research that hair quality is impaired by impaired scalp, but these impacts are reversible with early diagnosis and treatment. Deep Learning advances have demonstrated the effectiveness of CNN paired with FCN in diagnosing scalp and skin disorders. In one proposed Deep-Learning-based scalp inspection and diagnosis system, an imaging microscope and a trained model are combined with an app that classifies scalp disorders accurately with an average precision of 97.41%- 99.09%. Another research dealt with classifying the Psoriasis using the CNN with an accuracy of 82.9%. As part of another study, an ML based algorithm was also employed. It accurately classified the healthy scalp and alopecia areata with 91.4% and 88.9% accuracy with SVM and KNN algorithms. Using deep learning models to diagnose scalp related diseases has improved due to advancements i computation capabilities and computer vision, but there remains a wide horizon for further improvements.
</details>
<details>
<summary>摘要</summary>
scalp 疾病的恶性相对其他疾病较少，但对病人的生活影响巨大。人们常经历披裤屑、 Psoriasis、Tinea-Capitis、Alopecia 和 Atopic-Dermatitis 等 scalp 问题。根据Who研究，约70%的成年人有 scalp 问题。研究表明，损害的毛发质量是由于损害 scalp 引起的，但这些影响可以通过早期诊断和治疗而reverse。深度学习技术的进步使得用 Deep Learning 模型进行 scalp 检查和诊断系统的精度提高了。在一个提议的 Deep-Learning-based scalp 检查和诊断系统中，一个图像镜和一个训练模型被与一个APP结合，可以准确地分类 scalp 疾病，其精度为97.41%-99.09%。另一项研究则是使用 CNN 分类 Psoriasis，其精度为82.9%。在另一项研究中，一种 ML 基本的算法也被应用，它可以准确地分类健康的 scalp 和 Alopecia areata，其精度为91.4% 和 88.9%。使用深度学习模型进行 scalp 相关疾病的诊断，由于计算机能力和计算机视觉的进步，已经得到了进一步改进的空间，但还有很大的可能性空间。
</details></li>
</ul>
<hr>
<h2 id="Fourier-neural-operator-for-learning-solutions-to-macroscopic-traffic-flow-models-Application-to-the-forward-and-inverse-problems"><a href="#Fourier-neural-operator-for-learning-solutions-to-macroscopic-traffic-flow-models-Application-to-the-forward-and-inverse-problems" class="headerlink" title="Fourier neural operator for learning solutions to macroscopic traffic flow models: Application to the forward and inverse problems"></a>Fourier neural operator for learning solutions to macroscopic traffic flow models: Application to the forward and inverse problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07051">http://arxiv.org/abs/2308.07051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bilal Thonnam Thodi, Sai Venkata Ramana Ambadipudi, Saif Eddin Jabari</li>
<li>for: 这个论文是用来研究深度学习方法在交通流动中的应用，特别是用于解决非线性半导体方程的问题。</li>
<li>methods: 这个论文使用了一种名为 нейрон运算器框架，它可以将不同和稀疏的交通数据映射到完整的交通状况中。在训练中，使用了一种名为 физи学信息冲激（π）-FNO的算子，它在训练中添加了一个物理损失函数，以便在训练中提高冲击预测。</li>
<li>results: 通过使用LWR交通流模型， authors发现了在预测环形路网和城市信号灯道路上的density dynamics的高精度预测。此外，他们发现了一个可以使用简单的交通密度动态，例如由2-3个汽车队列和1-2个交通信号ecycle组成的数据，并且可以预测具有不同汽车队列分布和多个交通信号cycle（大于2）的密度动态，并且误差在可接受范围内。在适当的模型架构和训练数据下，插值误差呈线性增长。添加物理正则化可以帮助学习长期交通密度动态，特别是在 periodic boundary data 上。<details>
<summary>Abstract</summary>
Deep learning methods are emerging as popular computational tools for solving forward and inverse problems in traffic flow. In this paper, we study a neural operator framework for learning solutions to nonlinear hyperbolic partial differential equations with applications in macroscopic traffic flow models. In this framework, an operator is trained to map heterogeneous and sparse traffic input data to the complete macroscopic traffic state in a supervised learning setting. We chose a physics-informed Fourier neural operator ($\pi$-FNO) as the operator, where an additional physics loss based on a discrete conservation law regularizes the problem during training to improve the shock predictions. We also propose to use training data generated from random piecewise constant input data to systematically capture the shock and rarefied solutions. From experiments using the LWR traffic flow model, we found superior accuracy in predicting the density dynamics of a ring-road network and urban signalized road. We also found that the operator can be trained using simple traffic density dynamics, e.g., consisting of $2-3$ vehicle queues and $1-2$ traffic signal cycles, and it can predict density dynamics for heterogeneous vehicle queue distributions and multiple traffic signal cycles $(\geq 2)$ with an acceptable error. The extrapolation error grew sub-linearly with input complexity for a proper choice of the model architecture and training data. Adding a physics regularizer aided in learning long-term traffic density dynamics, especially for problems with periodic boundary data.
</details>
<details>
<summary>摘要</summary>
深度学习方法在交通流动问题中得到广泛应用。在这篇论文中，我们研究了一种神经网络框架，用于解决非线性偏微分方程的问题，并应用于大规模交通流模型。在这个框架中，一个算子被训练，以将不同和稀疏的交通数据映射到完整的交通状态中。我们选择了一种带有物理约束的 fourier神经网络（π-FNO）作为算子，其中在训练过程中添加了物理损失，以提高震动预测。我们还提出了使用随机划分的杂ync constant输入数据来系统地捕捉震动和稀疏解。从实验使用LWR交通流模型来看，我们发现了在density动力学中的高精度预测。我们还发现算子可以通过简单的交通密度动力学，例如由2-3辆汽车队列和1-2个交通信号周期组成的系统，来预测密度动力学。此外，我们发现算子可以在不同的汽车队列分布和多个交通信号周期（至少2个）下预测密度动力学，并且误差在输入复杂性增长的速度下逐渐增加。添加物理约束可以帮助学习长期交通密度动力学，特别是在 periodic boundry data 的问题上。
</details></li>
</ul>
<hr>
<h2 id="UIPC-MF-User-Item-Prototype-Connection-Matrix-Factorization-for-Explainable-Collaborative-Filtering"><a href="#UIPC-MF-User-Item-Prototype-Connection-Matrix-Factorization-for-Explainable-Collaborative-Filtering" class="headerlink" title="UIPC-MF: User-Item Prototype Connection Matrix Factorization for Explainable Collaborative Filtering"></a>UIPC-MF: User-Item Prototype Connection Matrix Factorization for Explainable Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07048">http://arxiv.org/abs/2308.07048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Pan, Von-Wun Soo</li>
<li>for: 提高推荐系统的准确率和可解释性</li>
<li>methods: 使用prototype-based matrix factorization方法，即UIPC-MF，其中用户和 Item 都关联有一组原型，以增强推荐的可解释性</li>
<li>results: 相比其他原型基eline方法，UIPC-MF 在三个 dataset 上显示出较高的 Hit Ratio 和 Normalized Discounted Cumulative Gain，同时也提供了更好的透明度。<details>
<summary>Abstract</summary>
Recommending items to potentially interested users has been an important commercial task that faces two main challenges: accuracy and explainability. While most collaborative filtering models rely on statistical computations on a large scale of interaction data between users and items and can achieve high performance, they often lack clear explanatory power. We propose UIPC-MF, a prototype-based matrix factorization method for explainable collaborative filtering recommendations. In UIPC-MF, both users and items are associated with sets of prototypes, capturing general collaborative attributes. To enhance explainability, UIPC-MF learns connection weights that reflect the associative relations between user and item prototypes for recommendations. UIPC-MF outperforms other prototype-based baseline methods in terms of Hit Ratio and Normalized Discounted Cumulative Gain on three datasets, while also providing better transparency.
</details>
<details>
<summary>摘要</summary>
推荐预测已经是电商中一项非常重要的任务，面临着两个主要挑战：准确性和可读性。大多数共同推荐模型通过大规模的用户-物品交互数据进行统计计算，可以达到高性能，但通常缺乏明确的解释力。我们提出了UIPC-MF，一种基于原型的矩阵分解方法，用于可读性推荐。在UIPC-MF中，用户和物品都关联到一组原型，捕捉总的共同特征。为了增强可读性，UIPC-MF学习用户和物品原型之间的关联Weight，用于推荐。UIPC-MF在三个数据集上比基eline方法具有更高的 Hit Ratio 和 Normalized Discounted Cumulative Gain，同时也提供了更好的透明度。
</details></li>
</ul>
<hr>
<h2 id="No-Regularization-is-Needed-An-Efficient-and-Effective-Model-for-Incomplete-Label-Distribution-Learning"><a href="#No-Regularization-is-Needed-An-Efficient-and-Effective-Model-for-Incomplete-Label-Distribution-Learning" class="headerlink" title="No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning"></a>No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07047">http://arxiv.org/abs/2308.07047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Songcan Chen</li>
<li>for: 本研究目的是解决因缺失数据而导致的不完整的分类器（Incomplete LDL，InLDL）性能下降问题，而不需要任何显式正则化。</li>
<li>methods: 我们提出使用分类器自身的标签分布作为正则化的先验知识，并设计了一种权重 schemes来强调小度和缺失度。</li>
<li>results: 我们的模型具有四个优点：1）没有需要显式正则化；2）具有闭式解决方案和易于实现（只需几行代码）；3）与大规模数据集相比，计算复杂度 linear；4）与当前状态对齐水平性能。<details>
<summary>Abstract</summary>
Label Distribution Learning (LDL) assigns soft labels, a.k.a. degrees, to a sample. In reality, it is always laborious to obtain complete degrees, giving birth to the Incomplete LDL (InLDL). However, InLDL often suffers from performance degeneration. To remedy it, existing methods need one or more explicit regularizations, leading to burdensome parameter tuning and extra computation. We argue that label distribution itself may provide useful prior, when used appropriately, the InLDL problem can be solved without any explicit regularization. In this paper, we offer a rational alternative to use such a prior. Our intuition is that large degrees are likely to get more concern, the small ones are easily overlooked, whereas the missing degrees are completely neglected in InLDL. To learn an accurate label distribution, it is crucial not to ignore the small observed degrees but to give them properly large weights, while gradually increasing the weights of the missing degrees. To this end, we first define a weighted empirical risk and derive upper bounds between the expected risk and the weighted empirical risk, which reveals in principle that weighting plays an implicit regularization role. Then, by using the prior of degrees, we design a weighted scheme and verify its effectiveness. To sum up, our model has four advantages, it is 1) model selection free, as no explicit regularization is imposed; 2) with closed form solution (sub-problem) and easy-to-implement (a few lines of codes); 3) with linear computational complexity in the number of samples, thus scalable to large datasets; 4) competitive with state-of-the-arts even without any explicit regularization.
</details>
<details>
<summary>摘要</summary>
标签分布学习（LDL）将软标签，即学习度，赋予样本。在实际应用中，完整的学习度往往很困难寻求，从而产生了偏差的LDL（InLDL）问题。然而，InLDL经常会导致性能下降。现有方法通常需要一或多个显式正则化，这会增加参数调整的复杂性和额外计算。我们认为标签分布本身可以提供有用的前提，当正确使用时，InLDL问题可以解决无需显式正则化。在本文中，我们提出了一种合理的使用此前提的方法。我们的启发是，大度标签更有可能受到注意，小度标签容易被忽略，而缺失的度标签完全被偏差的InLDL忽略。为了学习准确的标签分布，非常重要不是忽略小 observed degree，而是给它们适当的大量重要，同时逐渐增加缺失的度标签的重要性。我们首先定义了权重化的empirical risk，并 derivated upper bound между预期风险和权重化empirical risk，这 revelas in principle that weighting plays an implicit regularization role。然后，通过使用度标签的前提，我们设计了权重方案，并证明其效果。总之，我们的模型具有以下四个优点：1) 无需显式正则化，因为不需要任何显式正则化; 2) 具有关闭式解（sub-problem）和易于实现（只需几行代码）; 3) 对大量数据集 scales linearly，因此可扩展性好; 4) 与状态 искусственный지标下相当竞争，即无需显式正则化。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Flow-Networks"><a href="#Bayesian-Flow-Networks" class="headerlink" title="Bayesian Flow Networks"></a>Bayesian Flow Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07037">http://arxiv.org/abs/2308.07037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stefanradev93/BayesFlow">https://github.com/stefanradev93/BayesFlow</a></li>
<li>paper_authors: Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, Faustino Gomez</li>
<li>for: 本研究提出了抽象概率流网络（BFN），一种新的生成模型，其中抽象概率流网络的参数通过 bayesian 推断在噪声数据样本的灯光下进行修改，然后通过神经网络输出第二个相互dependent的分布。</li>
<li>methods: 该研究提出了一种基于 bayesian 推断的生成过程，其中从简单的先验开始，逐步更新两个分布，得到一个类似于反diffusion 模型的生成过程，但是更加简单，不需要前向过程。</li>
<li>results: 在图像模型task上，BFNs  achieved competitive log-likelihoods on dynamically binarized MNIST and CIFAR-10，并在文本8字符级语言模型任务上超越所有已知的批diffusion 模型。<details>
<summary>Abstract</summary>
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no restrictions on the network architecture. In our experiments BFNs achieve competitive log-likelihoods for image modelling on dynamically binarized MNIST and CIFAR-10, and outperform all known discrete diffusion models on the text8 character-level language modelling task.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文介绍了 bayesian flow networks (BFNs)，一种新的生成模型，其中bayesian inference modify了一组独立分布的参数，然后将这些修改后的分布作为输入传递给神经网络，生成一个第二个、相互关联的分布。这个过程类似于Diffusion模型的反向过程，但是更加简单，不需要前向过程。模型可以处理整数、连续和整数化数据，并且使用本地差分导数，使得梯度导航和几步生成在整数领域如语言模型中变得更加容易。损失函数直接优化数据压缩，并不限制网络架构。在实验中，BFNs在 dynamically binarized MNIST和CIFAR-10上的图像模型 task中 achieved competitive log-likelihoods，并在text8 character-level语言模型任务上超过了所有已知的整数 diffusion models。
</details></li>
</ul>
<hr>
<h2 id="S3IM-Stochastic-Structural-SIMilarity-and-Its-Unreasonable-Effectiveness-for-Neural-Fields"><a href="#S3IM-Stochastic-Structural-SIMilarity-and-Its-Unreasonable-Effectiveness-for-Neural-Fields" class="headerlink" title="S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields"></a>S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07032">http://arxiv.org/abs/2308.07032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/madaoer/s3im_nerf">https://github.com/madaoer/s3im_nerf</a></li>
<li>paper_authors: Zeke Xie, Xindi Yang, Yujie Yang, Qi Sun, Yixiang Jiang, Haoran Wang, Yunfeng Cai, Mingming Sun</li>
<li>for: 该论文旨在提高NeRF和相关神经场方法的可视化质量，使其能够更好地渲染未知视角的场景图像。</li>
<li>methods: 该论文提出了一种非本地多重训练方法，通过一种新的随机结构相似性（S3IM）损失函数，将多个数据点处理为一个整体，而不是独立处理每个输入。</li>
<li>results: 对于八种视角合成任务和八种表面重建任务，S3IM可以减少测试MSE损失率超过90%，提高F1分数198%和Chamfer-$L_{1}$距离64%。此外，S3IM可以在缺少输入、损坏图像和动态场景下保持稳定性。<details>
<summary>Abstract</summary>
Recently, Neural Radiance Field (NeRF) has shown great success in rendering novel-view images of a given scene by learning an implicit representation with only posed RGB images. NeRF and relevant neural field methods (e.g., neural surface representation) typically optimize a point-wise loss and make point-wise predictions, where one data point corresponds to one pixel. Unfortunately, this line of research failed to use the collective supervision of distant pixels, although it is known that pixels in an image or scene can provide rich structural information. To the best of our knowledge, we are the first to design a nonlocal multiplex training paradigm for NeRF and relevant neural field methods via a novel Stochastic Structural SIMilarity (S3IM) loss that processes multiple data points as a whole set instead of process multiple inputs independently. Our extensive experiments demonstrate the unreasonable effectiveness of S3IM in improving NeRF and neural surface representation for nearly free. The improvements of quality metrics can be particularly significant for those relatively difficult tasks: e.g., the test MSE loss unexpectedly drops by more than 90% for TensoRF and DVGO over eight novel view synthesis tasks; a 198% F-score gain and a 64% Chamfer $L_{1}$ distance reduction for NeuS over eight surface reconstruction tasks. Moreover, S3IM is consistently robust even with sparse inputs, corrupted images, and dynamic scenes.
</details>
<details>
<summary>摘要</summary>
近期，神经辐射场（NeRF）已经取得了在渲染新视图图像中的成功，通过学习含义几何表示，只使用配置好的RGB图像。NeRF和相关的神经场方法（例如神经表面表示）通常是点 wise 损失优化和点 wise 预测，其中一个数据点对应一个像素。尽管这一线索的研究把握不到远程像素的集合supervision，即图像或场景中的像素可以提供丰富的结构信息。据我们所知，我们是首先设计了非本地多样training paradigm for NeRF和相关神经场方法，通过一种新的随机Structural SIMilarity（S3IM）损失函数，将多个数据点处理为一个整体而不是独立处理多个输入。我们的广泛实验表明，S3IM可以减少TensoRF和DVGO的测试MSE损失超过90%，并且 NeuS 的F-score提高198%，Chamfer $L_{1}$ 距离减少64%。此外，S3IM还能够在缺少输入、损坏图像和动态场景下保持稳定性。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Physics-Informed-Neural-Network-for-the-Forward-and-Inverse-Simulation-of-Engineered-Nano-particles-Mobility-in-a-Contaminated-Aquifer"><a href="#Bayesian-Physics-Informed-Neural-Network-for-the-Forward-and-Inverse-Simulation-of-Engineered-Nano-particles-Mobility-in-a-Contaminated-Aquifer" class="headerlink" title="Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer"></a>Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07352">http://arxiv.org/abs/2308.07352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Nilabh, Fidel Grandia</li>
<li>For: This paper aims to develop a predictive tool for the mobility of engineered nanoparticles (ENPs) in groundwater, to support the development of an efficient remediation strategy for polluted groundwater sites.* Methods: The paper uses a Bayesian Physics-Informed Neural Network (B-PINN) framework to model the mobility of ENPs within an aquifer, and to quantify the uncertainty in the predictions.* Results: The forward model demonstrates the effective capability of B-PINN in accurately predicting the ENPs mobility, and the inverse model output is used to predict the governing parameters for the ENPs mobility in a small-scale aquifer. The research demonstrates the capability of the tool to provide predictive insights for developing an efficient groundwater remediation strategy.Here’s the Chinese translation of the three key information points:* For: 这篇论文的目的是为了开发一种能够预测Engineered Nanoparticles（ENPs）在地下水中的移动性，以支持污染地下水站的清理和环境重建。* Methods: 这篇论文使用了一种 Bayesian Physics-Informed Neural Network（B-PINN）框架，来模拟ENPs在aquifer中的移动性，并量化预测结果的不确定性。* Results: 前向模型表明B-PINN在准确预测ENPs移动性的能力，而反向模型输出可以用来预测aquifer中ENPs移动性的主导参数。这项研究 demonstarte了这种工具的能力，可以为开发有效的地下水清理策略提供预测性的信息。<details>
<summary>Abstract</summary>
Globally, there are many polluted groundwater sites that need an active remediation plan for the restoration of local ecosystem and environment. Engineered nanoparticles (ENPs) have proven to be an effective reactive agent for the in-situ degradation of pollutants in groundwater. While the performance of these ENPs has been highly promising on the laboratory scale, their application in real field case conditions is still limited. The complex transport and retention mechanisms of ENPs hinder the development of an efficient remediation strategy. Therefore, a predictive tool to comprehend the transport and retention behavior of ENPs is highly required. The existing tools in the literature are dominated with numerical simulators, which have limited flexibility and accuracy in the presence of sparse datasets and the aquifer heterogeneity. This work uses a Bayesian Physics-Informed Neural Network (B-PINN) framework to model the nano-particles mobility within an aquifer. The result from the forward model demonstrates the effective capability of B-PINN in accurately predicting the ENPs mobility and quantifying the uncertainty. The inverse model output is then used to predict the governing parameters for the ENPs mobility in a small-scale aquifer. The research demonstrates the capability of the tool to provide predictive insights for developing an efficient groundwater remediation strategy.
</details>
<details>
<summary>摘要</summary>
全球有很多污染的地下水点，需要有效的活动整治计划以恢复当地生态环境。工程化的奈米颗粒（ENPs）在地下水中的吸附和分解作用已经在室内实验室中得到了证明，但是在实际场景中的应用仍然受限。奈米颗粒的复杂的运输和保持机制限制了整治策略的发展。因此，一个可预测奈米颗粒的运输和保持行为的工具是非常重要。现有的文献中的工具主要是数值模拟器，它们在缺乏数据和地下水异常性时的灵活性和准确性受到限制。本研究使用泛函神经网络（B-PINN）框架来模拟奈米颗粒在aquifer中的 mobilidad。前向模型的输出结果表明B-PINN在准确预测奈米颗粒 mobilidad和评估不确定性的能力。逆向模型输出被用来预测 governing parameters 的奈米颗粒 mobilidad在小规模 aquifer 中。研究表明工具的可预测性可以为开发有效的地下水整治策略提供先进的预测性 Insight。
</details></li>
</ul>
<hr>
<h2 id="IOB-Integrating-Optimization-Transfer-and-Behavior-Transfer-for-Multi-Policy-Reuse"><a href="#IOB-Integrating-Optimization-Transfer-and-Behavior-Transfer-for-Multi-Policy-Reuse" class="headerlink" title="IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse"></a>IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07351">http://arxiv.org/abs/2308.07351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Li, Hao Li, Jin Zhang, Zhen Wang, Peng Liu, Chongjie Zhang</li>
<li>for: 本研究旨在提出一种新的转移学习RL方法，以便在新任务上快速解决问题。</li>
<li>methods: 本方法使用actor-critic框架中的Q函数来导引策略选择，选择最大一步改进策略作为目标策略。我们同时实现了优化转移和行为转移（IOB），通过规范学习策略以便模仿指导策略，并将其与行为策略相结合。</li>
<li>results: 我们的方法在标准任务上超越了状态之前的转移RL基准值，并在连续学习场景中提高了最终性和知识传递性。此外，我们证明了我们的优化转移技术可以提高目标策略学习。<details>
<summary>Abstract</summary>
Humans have the ability to reuse previously learned policies to solve new tasks quickly, and reinforcement learning (RL) agents can do the same by transferring knowledge from source policies to a related target task. Transfer RL methods can reshape the policy optimization objective (optimization transfer) or influence the behavior policy (behavior transfer) using source policies. However, selecting the appropriate source policy with limited samples to guide target policy learning has been a challenge. Previous methods introduce additional components, such as hierarchical policies or estimations of source policies' value functions, which can lead to non-stationary policy optimization or heavy sampling costs, diminishing transfer effectiveness. To address this challenge, we propose a novel transfer RL method that selects the source policy without training extra components. Our method utilizes the Q function in the actor-critic framework to guide policy selection, choosing the source policy with the largest one-step improvement over the current target policy. We integrate optimization transfer and behavior transfer (IOB) by regularizing the learned policy to mimic the guidance policy and combining them as the behavior policy. This integration significantly enhances transfer effectiveness, surpasses state-of-the-art transfer RL baselines in benchmark tasks, and improves final performance and knowledge transferability in continual learning scenarios. Additionally, we show that our optimization transfer technique is guaranteed to improve target policy learning.
</details>
<details>
<summary>摘要</summary>
人类具有 reuse previously learned policies 来解决新任务的能力，同时 reinforcement learning (RL) 代理也可以通过将知识传递到相关的目标任务中来实现这一点。传输 RL 方法可以改变政策优化目标（优化传输）或者影响行为政策（行为传输）使用源政策。然而，在有限样本情况下选择合适的源政策是一大挑战。先前的方法可能会添加额外的组件，如层次政策或源政策价值函数的估计，这可能会导致非站点政策优化或者大量的样本成本，这将导致传输效果减退。为解决这个挑战，我们提出了一种新的传输 RL 方法，不需要训练额外的组件。我们利用 actor-critic 框架中的 Q 函数来导引政策选择，选择目标政策中一步改进最大的源政策。我们将优化传输和行为传输（IOB）相结合，通过规则化学习的政策来模仿指导政策，并将其与行为政策相结合。这种结合显著提高了传输效果，超越了基eline的传输 RL 标准 benchmark 任务，并在连续学习场景中提高了最终性和知识传递性。此外，我们证明了我们的优化传输技术能够提高目标政策学习。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Neural-PDE-Solvers-using-Quantization-Aware-Training"><a href="#Efficient-Neural-PDE-Solvers-using-Quantization-Aware-Training" class="headerlink" title="Efficient Neural PDE-Solvers using Quantization Aware Training"></a>Efficient Neural PDE-Solvers using Quantization Aware Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07350">http://arxiv.org/abs/2308.07350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Winfried van den Dool, Tijmen Blankevoort, Max Welling, Yuki M. Asano</li>
<li>for: 用 neural networks 代替经典数学方法解决 Partial Differential Equations (PDEs) 的应用，以减少计算成本。</li>
<li>methods: 使用现有的量化方法来降低计算成本，并保持性能。</li>
<li>results: 在四个标准 PDE 数据集和三个网络架构上，发现量化训练可以降低计算成本三个数量级，而且在不同设置下都能够保持性能。此外，我们还证明了在大多数情况下，只有通过包含量化来达到 Pareto 优化。<details>
<summary>Abstract</summary>
In the past years, the application of neural networks as an alternative to classical numerical methods to solve Partial Differential Equations has emerged as a potential paradigm shift in this century-old mathematical field. However, in terms of practical applicability, computational cost remains a substantial bottleneck. Classical approaches try to mitigate this challenge by limiting the spatial resolution on which the PDEs are defined. For neural PDE solvers, we can do better: Here, we investigate the potential of state-of-the-art quantization methods on reducing computational costs. We show that quantizing the network weights and activations can successfully lower the computational cost of inference while maintaining performance. Our results on four standard PDE datasets and three network architectures show that quantization-aware training works across settings and three orders of FLOPs magnitudes. Finally, we empirically demonstrate that Pareto-optimality of computational cost vs performance is almost always achieved only by incorporating quantization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Optimize-LSM-trees-Towards-A-Reinforcement-Learning-based-Key-Value-Store-for-Dynamic-Workloads"><a href="#Learning-to-Optimize-LSM-trees-Towards-A-Reinforcement-Learning-based-Key-Value-Store-for-Dynamic-Workloads" class="headerlink" title="Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads"></a>Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07013">http://arxiv.org/abs/2308.07013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dingheng Mo, Fanchao Chen, Siqiang Luo, Caihua Shan</li>
<li>For: 提高静态负荷下的系统性能，即使面临动态负荷。* Methods: 使用Reinforcement Learning（RL）引导LSM树转换，并提出了一种新的LSM树结构（FLSM树）以优化压缩策略的转换。* Results: 比较RL和传统的方法，RusKey在多种负荷下显示了4倍的终端性能优势，而无需先知工作负荷知识。<details>
<summary>Abstract</summary>
LSM-trees are widely adopted as the storage backend of key-value stores. However, optimizing the system performance under dynamic workloads has not been sufficiently studied or evaluated in previous work. To fill the gap, we present RusKey, a key-value store with the following new features: (1) RusKey is a first attempt to orchestrate LSM-tree structures online to enable robust performance under the context of dynamic workloads; (2) RusKey is the first study to use Reinforcement Learning (RL) to guide LSM-tree transformations; (3) RusKey includes a new LSM-tree design, named FLSM-tree, for an efficient transition between different compaction policies -- the bottleneck of dynamic key-value stores. We justify the superiority of the new design with theoretical analysis; (4) RusKey requires no prior workload knowledge for system adjustment, in contrast to state-of-the-art techniques. Experiments show that RusKey exhibits strong performance robustness in diverse workloads, achieving up to 4x better end-to-end performance than the RocksDB system under various settings.
</details>
<details>
<summary>摘要</summary>
LSM树是键值存储系统的常用后端存储方式。然而，在动态负荷下优化系统性能尚未得到充分研究和评估。为填补这一空白，我们提出了RusKey，一个具有以下新特点的键值存储系统：1. RusKey是首次在线上调度LSM树结构，以实现对动态负荷下的robust性表现;2. RusKey是首次使用强化学习（RL）引导LSM树转换;3. RusKey包含一种新的LSM树设计，名为FLSM树，用于高效地在不同压缩策略之间进行过渡;4. RusKey不需要先知工作负荷信息，与当前技术相比，更加灵活和易用。我们通过理论分析证明了新设计的优越性。实验结果显示，RusKey在多种工作负荷下表现出强大的性能稳定性，与RocksDB系统在不同设置下达到4倍的终端性能。
</details></li>
</ul>
<hr>
<h2 id="Greedy-online-change-point-detection"><a href="#Greedy-online-change-point-detection" class="headerlink" title="Greedy online change point detection"></a>Greedy online change point detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07012">http://arxiv.org/abs/2308.07012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jou-Hui Ho, Felipe Tobar</li>
<li>for: 寻找时间序列中的变化点，以提高变化点检测的准确率和效率。</li>
<li>methods: 使用Greedy Online Change Point Detection（GOCPD）方法，该方法通过最大化数据来自两个独立模型（temporal）的概率来找到变化点。</li>
<li>results: 在synthetic数据和实际世界单variate和多variate设置中，GOCPD方法能够快速减少false discovery rate，并且在某些情况下比传统方法更高效。<details>
<summary>Abstract</summary>
Standard online change point detection (CPD) methods tend to have large false discovery rates as their detections are sensitive to outliers. To overcome this drawback, we propose Greedy Online Change Point Detection (GOCPD), a computationally appealing method which finds change points by maximizing the probability of the data coming from the (temporal) concatenation of two independent models. We show that, for time series with a single change point, this objective is unimodal and thus CPD can be accelerated via ternary search with logarithmic complexity. We demonstrate the effectiveness of GOCPD on synthetic data and validate our findings on real-world univariate and multivariate settings.
</details>
<details>
<summary>摘要</summary>
常规在线变点检测（CPD）方法通常会有较大的假阳性率，因为它们对异常值敏感。为了解决这个缺点，我们提议了简单在线变点检测（GOCPD）方法，它通过最大化数据来自两个独立模型（时间排序）的概率来检测变点。我们证明，对具有单个变点的时间序列，这个目标函数是单峰型，因此可以通过三元搜索来加速CPD，其复杂度为幂函数。我们在 sintetic 数据上证明了 GOCPD 的有效性，并在实际的单VAR 和多VAR 设置中验证了我们的结论。
</details></li>
</ul>
<hr>
<h2 id="Aggregating-Intrinsic-Information-to-Enhance-BCI-Performance-through-Federated-Learning"><a href="#Aggregating-Intrinsic-Information-to-Enhance-BCI-Performance-through-Federated-Learning" class="headerlink" title="Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning"></a>Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11636">http://arxiv.org/abs/2308.11636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Liu, Yuanyuan Chen, Anran Li, Yi Ding, Han Yu, Cuntai Guan<br>for: 这个研究旨在解决脑computer interfaces（BCI）建立高性能深度学习模型的长期挑战， BCIs 的数据多样性问题。methods: 这个研究提出了一个弹性联边学习（FLEEG）框架，让不同装备的数据可以在训练过程中合作。每个客户端都有自己的特定数据集，并训练一个层次化的专门化模型，以处理不同的数据格式。服务器则处理训练过程，将来自所有数据集的知识融合，以提高总性能。results: 这个框架在脑意图（MI）分类任务中，与9个由不同设备收集的EEG数据集合作，可以提高分类性能达16.7%。可视化结果显示，提案的框架可以让本地模型专注在任务相关的区域，从而获得更好的性能。<details>
<summary>Abstract</summary>
Insufficient data is a long-standing challenge for Brain-Computer Interface (BCI) to build a high-performance deep learning model. Though numerous research groups and institutes collect a multitude of EEG datasets for the same BCI task, sharing EEG data from multiple sites is still challenging due to the heterogeneity of devices. The significance of this challenge cannot be overstated, given the critical role of data diversity in fostering model robustness. However, existing works rarely discuss this issue, predominantly centering their attention on model training within a single dataset, often in the context of inter-subject or inter-session settings. In this work, we propose a hierarchical personalized Federated Learning EEG decoding (FLEEG) framework to surmount this challenge. This innovative framework heralds a new learning paradigm for BCI, enabling datasets with disparate data formats to collaborate in the model training process. Each client is assigned a specific dataset and trains a hierarchical personalized model to manage diverse data formats and facilitate information exchange. Meanwhile, the server coordinates the training procedure to harness knowledge gleaned from all datasets, thus elevating overall performance. The framework has been evaluated in Motor Imagery (MI) classification with nine EEG datasets collected by different devices but implementing the same MI task. Results demonstrate that the proposed frame can boost classification performance up to 16.7% by enabling knowledge sharing between multiple datasets, especially for smaller datasets. Visualization results also indicate that the proposed framework can empower the local models to put a stable focus on task-related areas, yielding better performance. To the best of our knowledge, this is the first end-to-end solution to address this important challenge.
</details>
<details>
<summary>摘要</summary>
BCIs 长期面临不充分数据的挑战，建立高性能深度学习模型困难。虽然许多研究机构和机构收集了多个 EEG 数据集，但是共享多个站点的 EEG 数据仍然困难，主要因为设备的不一致性。这个挑战的重要性无法被过度估计，因为数据多样性对模型的稳定性具有关键作用。然而，现有的研究很少讨论这个问题，通常是在单个数据集内进行模型训练，常在 между subject 或 session 上下文中进行。在这种情况下，我们提出了一种层次个性化 Federated Learning EEG 解码（FLEEG）框架，以解决这个挑战。这种创新的框架标志着 BCIs 新的学习模式，使得不同数据格式的数据集可以在模型训练过程中合作。每个客户端被分配特定数据集，并训练一个层次个性化模型，以处理多个数据格式的多样性，并且促进信息交换。同时，服务器协调训练过程，以利用所有数据集中所获得的知识，从而提高整体性能。我们在 Motor Imagery（MI） 分类任务中使用了 nine EEG 数据集，每个数据集由不同的设备收集，但都实现了相同的 MI 任务。结果表明，我们的框架可以提高分类性能达到 16.7%，尤其是对小数据集的提高。可视化结果还表明，我们的框架可以让本地模型固定焦点于任务相关区域，从而提高性能。到目前为止，我们的解决方案是 BCIs 首次尝试的综合解决方案。
</details></li>
</ul>
<hr>
<h2 id="Deep-convolutional-neural-networks-for-cyclic-sensor-data"><a href="#Deep-convolutional-neural-networks-for-cyclic-sensor-data" class="headerlink" title="Deep convolutional neural networks for cyclic sensor data"></a>Deep convolutional neural networks for cyclic sensor data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06987">http://arxiv.org/abs/2308.06987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Payman Goodarzi, Yannick Robin, Andreas Schütze, Tizian Schneider</li>
<li>for: 本研究旨在探讨基于感测器的维保维护，并应用深度学习技术来解决多感测器系统中的复杂性问题。</li>
<li>methods: 本研究使用了一个 hidraulic system testbed dataset，并比较了三个模型的性能：基线模型使用了 conventional methods，单个CNN模型使用了 early sensor fusion，并且二路CNN模型（2L-CNN）使用了 late sensor fusion。</li>
<li>results: 基线模型使用了 late sensor fusion，可以达到1%的测试错误率，但CNN模型由于感测器的多样性而遇到了问题，导致测试错误率达到20.5%。此外，我们还进行了每感测器都进行独立的训练，并观察到了几个感测器的准确率变化。此外，2L-CNN模型的性能表现了显著的改善，当考虑了最佳和最差的感测器时，错误率下降了33%。本研究重申了多感测器系统中的复杂性问题需要有效地解决。<details>
<summary>Abstract</summary>
Predictive maintenance plays a critical role in ensuring the uninterrupted operation of industrial systems and mitigating the potential risks associated with system failures. This study focuses on sensor-based condition monitoring and explores the application of deep learning techniques using a hydraulic system testbed dataset. Our investigation involves comparing the performance of three models: a baseline model employing conventional methods, a single CNN model with early sensor fusion, and a two-lane CNN model (2L-CNN) with late sensor fusion. The baseline model achieves an impressive test error rate of 1% by employing late sensor fusion, where feature extraction is performed individually for each sensor. However, the CNN model encounters challenges due to the diverse sensor characteristics, resulting in an error rate of 20.5%. To further investigate this issue, we conduct separate training for each sensor and observe variations in accuracy. Additionally, we evaluate the performance of the 2L-CNN model, which demonstrates significant improvement by reducing the error rate by 33% when considering the combination of the least and most optimal sensors. This study underscores the importance of effectively addressing the complexities posed by multi-sensor systems in sensor-based condition monitoring.
</details>
<details>
<summary>摘要</summary>
预测维护在产业系统不间断运行和降低系统故障风险的角色非常重要。本研究使用液压系统测试 datasets 进行探索，并应用深度学习技术进行condition monitoring。我们的调查包括比较三种模型的性能：基eline模型使用 convent ional 方法、单个CNN模型（1L-CNN）在早期整合感知器、以及两个CNN模型（2L-CNN）在晚期整合感知器。基eline模型通过使用晚期整合，实现了1%的测试错误率。然而，CNN模型由于感知器的多样性，导致20.5%的错误率。为了进一步调查这一问题，我们进行了每个感知器分别进行训练，并观察了准确性的变化。此外，我们还评估了2L-CNN模型的性能，其在考虑最佳和最差的感知器组合时显示了33%的下降。这一研究强调了condition monitoring中多感知器系统的复杂性需要得到有效地处理。
</details></li>
</ul>
<hr>
<h2 id="pNNCLR-Stochastic-Pseudo-Neighborhoods-for-Contrastive-Learning-based-Unsupervised-Representation-Learning-Problems"><a href="#pNNCLR-Stochastic-Pseudo-Neighborhoods-for-Contrastive-Learning-based-Unsupervised-Representation-Learning-Problems" class="headerlink" title="pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems"></a>pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06983">http://arxiv.org/abs/2308.06983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Momojit Biswas, Himanshu Buckchash, Dilip K. Prasad</li>
<li>for: 本文是为了提高 nearest neighbor 基于自助学习（SSL）的图像识别问题的表现而写的。</li>
<li>methods: 本文使用 nearest neighbor 方法，并引入 pseudo nearest neighbors (pNN) 来控制支持集质量，以提高表现。 另外，文中还使用了一种抽样策略和一种平滑重量更新策略来稳定 nearest neighbor 基于学习的uncertainty。</li>
<li>results: 根据文中的评估结果，提出的方法与基eline nearest neighbor 方法相比，在多个公共图像识别和医学图像识别 dataset 上表现出了8%的提升。此外，该方法与其他之前提出的 SSL 方法相比也具有相似的表现。<details>
<summary>Abstract</summary>
Nearest neighbor (NN) sampling provides more semantic variations than pre-defined transformations for self-supervised learning (SSL) based image recognition problems. However, its performance is restricted by the quality of the support set, which holds positive samples for the contrastive loss. In this work, we show that the quality of the support set plays a crucial role in any nearest neighbor based method for SSL. We then provide a refined baseline (pNNCLR) to the nearest neighbor based SSL approach (NNCLR). To this end, we introduce pseudo nearest neighbors (pNN) to control the quality of the support set, wherein, rather than sampling the nearest neighbors, we sample in the vicinity of hard nearest neighbors by varying the magnitude of the resultant vector and employing a stochastic sampling strategy to improve the performance. Additionally, to stabilize the effects of uncertainty in NN-based learning, we employ a smooth-weight-update approach for training the proposed network. Evaluation of the proposed method on multiple public image recognition and medical image recognition datasets shows that it performs up to 8 percent better than the baseline nearest neighbor method, and is comparable to other previously proposed SSL methods.
</details>
<details>
<summary>摘要</summary>
近邻采样（NN）提供了更多的 semantic variation than pre-defined transformation for self-supervised learning（SSL）based image recognition problems. However, its performance is restricted by the quality of the support set, which holds positive samples for the contrastive loss. In this work, we show that the quality of the support set plays a crucial role in any nearest neighbor based method for SSL. We then provide a refined baseline（pNNCLR）to the nearest neighbor based SSL approach（NNCLR）. To this end, we introduce pseudo nearest neighbors（pNN）to control the quality of the support set, wherein, rather than sampling the nearest neighbors, we sample in the vicinity of hard nearest neighbors by varying the magnitude of the resultant vector and employing a stochastic sampling strategy to improve the performance. Additionally, to stabilize the effects of uncertainty in NN-based learning, we employ a smooth-weight-update approach for training the proposed network. Evaluation of the proposed method on multiple public image recognition and medical image recognition datasets shows that it performs up to 8 percent better than the baseline nearest neighbor method, and is comparable to other previously proposed SSL methods.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Routing-Recovery-for-UAV-Networks-with-Deliberate-Attacks-A-Reinforcement-Learning-based-Approach"><a href="#Routing-Recovery-for-UAV-Networks-with-Deliberate-Attacks-A-Reinforcement-Learning-based-Approach" class="headerlink" title="Routing Recovery for UAV Networks with Deliberate Attacks: A Reinforcement Learning based Approach"></a>Routing Recovery for UAV Networks with Deliberate Attacks: A Reinforcement Learning based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06973">http://arxiv.org/abs/2308.06973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijie He, Ziye Jia, Chao Dong, Wei Wang, Yilu Cao, Yang Yang, Qihui Wu</li>
<li>for: 本文关注在无人航空器（UAV）网络中的路由计划和恢复，以解决UAV网络受到意外攻击的问题。</li>
<li>methods: 本文提出了一种基于节点重要性的攻击模型，并设计了一种节点重要性排名机制，考虑了节点和链接重要性。此外，本文还提出了一种基于强化学习的智能算法，以恢复UAV网络中的路由路径当UAVs被攻击。</li>
<li>results: 数据 simulate 结果表明，提出的机制比其他相关方法更为有效。<details>
<summary>Abstract</summary>
The unmanned aerial vehicle (UAV) network is popular these years due to its various applications. In the UAV network, routing is significantly affected by the distributed network topology, leading to the issue that UAVs are vulnerable to deliberate damage. Hence, this paper focuses on the routing plan and recovery for UAV networks with attacks. In detail, a deliberate attack model based on the importance of nodes is designed to represent enemy attacks. Then, a node importance ranking mechanism is presented, considering the degree of nodes and link importance. However, it is intractable to handle the routing problem by traditional methods for UAV networks, since link connections change with the UAV availability. Hence, an intelligent algorithm based on reinforcement learning is proposed to recover the routing path when UAVs are attacked. Simulations are conducted and numerical results verify the proposed mechanism performs better than other referred methods.
</details>
<details>
<summary>摘要</summary>
“无人航空器（UAV）网络在近年得到广泛应用，但是它们的路由却受到分布网络架构的影响，导致UAV易受到意外攻击。因此，本文关注于UAV网络路由规划和恢复，并对攻击后路由路径的恢复进行了研究。具体来说，我们设计了一种基于节点重要性的攻击模型，并提出了一种考虑节点度和链接重要性的节点重要性排名机制。然而，由于UAV网络中链接的连接变化，传统方法无法有效地处理UAV网络的路由问题。因此，我们提出了一种基于强化学习算法的智能路由恢复方法，以恢复在攻击后的路由路径。我们通过实验和数值结果发现，提议的机制在攻击后路由恢复方面表现更好 than其他已知方法。”Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="AutoAssign-Automatic-Shared-Embedding-Assignment-in-Streaming-Recommendation"><a href="#AutoAssign-Automatic-Shared-Embedding-Assignment-in-Streaming-Recommendation" class="headerlink" title="AutoAssign+: Automatic Shared Embedding Assignment in Streaming Recommendation"></a>AutoAssign+: Automatic Shared Embedding Assignment in Streaming Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06965">http://arxiv.org/abs/2308.06965</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/AutoAssign-Plus">https://github.com/Applied-Machine-Learning-Lab/AutoAssign-Plus</a></li>
<li>paper_authors: Ziru Liu, Kecheng Chen, Fengyi Song, Bo Chen, Xiangyu Zhao, Huifeng Guo, Ruiming Tang</li>
<li>for: addressing new user IDs or item IDs in streaming recommender systems</li>
<li>methods: utilizes reinforcement learning-driven framework with an Identity Agent and critic network to dynamically determine shared embeddings and retain&#x2F;eliminate ID features</li>
<li>results: significantly enhances recommendation performance and reduces memory usage by approximately 20-30%<details>
<summary>Abstract</summary>
In the domain of streaming recommender systems, conventional methods for addressing new user IDs or item IDs typically involve assigning initial ID embeddings randomly. However, this practice results in two practical challenges: (i) Items or users with limited interactive data may yield suboptimal prediction performance. (ii) Embedding new IDs or low-frequency IDs necessitates consistently expanding the embedding table, leading to unnecessary memory consumption. In light of these concerns, we introduce a reinforcement learning-driven framework, namely AutoAssign+, that facilitates Automatic Shared Embedding Assignment Plus. To be specific, AutoAssign+ utilizes an Identity Agent as an actor network, which plays a dual role: (i) Representing low-frequency IDs field-wise with a small set of shared embeddings to enhance the embedding initialization, and (ii) Dynamically determining which ID features should be retained or eliminated in the embedding table. The policy of the agent is optimized with the guidance of a critic network. To evaluate the effectiveness of our approach, we perform extensive experiments on three commonly used benchmark datasets. Our experiment results demonstrate that AutoAssign+ is capable of significantly enhancing recommendation performance by mitigating the cold-start problem. Furthermore, our framework yields a reduction in memory usage of approximately 20-30%, verifying its practical effectiveness and efficiency for streaming recommender systems.
</details>
<details>
<summary>摘要</summary>
在流媒体推荐系统领域，传统的新用户ID或项目ID处理方法通常是随机分配初始ID embedding。然而，这种做法会导致两个实际挑战：（i）有限交互数据的物品或用户可能会得到下标性的预测性能。（ii）为新ID或低频ID分配 embedding 表需要持续扩展 embedding 表，从而导致不必要的内存浪费。为了解决这些问题，我们提出了一个基于强化学习的框架，即 AutoAssign+。具体来说，AutoAssign+ 使用一个 Identity Agent 作为 actor 网络，该网络在两个角色中发挥作用：（i）通过将低频ID分配到一小set of shared embedding来提高初始化 embedding，并（ii）在 embedding 表中动态确定需要保留或删除的 ID 特征。Policy 网络的优化受到批评网络的指导。为评估我们的方法的效果，我们在三个常用的数据集上进行了广泛的实验。实验结果表明，AutoAssign+ 能够有效解决冷启点问题，并且减少内存使用率约 20-30%，证明我们的方法在流媒体推荐系统中具有实际效果和效率。
</details></li>
</ul>
<hr>
<h2 id="Graph-Structural-Residuals-A-Learning-Approach-to-Diagnosis"><a href="#Graph-Structural-Residuals-A-Learning-Approach-to-Diagnosis" class="headerlink" title="Graph Structural Residuals: A Learning Approach to Diagnosis"></a>Graph Structural Residuals: A Learning Approach to Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06961">http://arxiv.org/abs/2308.06961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Lukas Augustin, Oliver Niggemann</li>
<li>for: 提出了一种新的框架，将模型基于诊断结合深度图结构学习。</li>
<li>methods: 利用数据学习系统的下面结构，并通过两个不同的图邻元矩阵来提供动态观察。</li>
<li>results: 通过实验示范了一种数据驱动的诊断方法的潜在优势。<details>
<summary>Abstract</summary>
Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
</details>
<details>
<summary>摘要</summary>
传统的模型基于诊断方法是通过构建明确的系统模型来进行，这可能是劳动密集且需要专家知识的。在这篇论文中，我们提出了一种新的框架，它结合了模型基于诊断和深度图结构学习的概念。这种数据驱动的方法利用数据来学习系统的下面结构，并提供动态观察结果，表示为两个不同的图邻接矩阵。我们的工作使得图结构学习与模型基于诊断的集成变得自然和简单，我们的主要贡献包括：1. 重新定义系统表示、观察和缺陷的构造2. 提出了两种不同的自我超级VI持模型建立方法3. 通过对振荡器系统进行实验，证明我们的数据驱动诊断方法的潜力。
</details></li>
</ul>
<hr>
<h2 id="Search-to-Fine-tune-Pre-trained-Graph-Neural-Networks-for-Graph-level-Tasks"><a href="#Search-to-Fine-tune-Pre-trained-Graph-Neural-Networks-for-Graph-level-Tasks" class="headerlink" title="Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks"></a>Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06960">http://arxiv.org/abs/2308.06960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Wang, Shimin Di, Lei Chen, Xiaofang Zhou</li>
<li>for: 本研究旨在提高预训练GNNs的表现，并设计一种适应性更高的微调策略。</li>
<li>methods: 我们提出了一种基于搜索的微调策略，称为S2PGNN，可以适应不同的下游任务和数据集。</li>
<li>results: 我们在10个著名的预训练GNNs上进行了实验，并证明了S2PGNN可以在不同的下游任务和数据集上提高模型的表现，并且比现有的微调策略更高效。<details>
<summary>Abstract</summary>
Recently, graph neural networks (GNNs) have shown its unprecedented success in many graph-related tasks. However, GNNs face the label scarcity issue as other neural networks do. Thus, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs to improve the model performance in this paper. Given a pre-trained GNN, we propose to search to fine-tune pre-trained graph neural networks for graph-level tasks (S2PGNN), which adaptively design a suitable fine-tuning framework for the given labeled data on the downstream task. To ensure the improvement brought by searching fine-tuning strategy, we carefully summarize a proper search space of fine-tuning framework that is suitable for GNNs. The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Besides, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area. Our code is publicly available at \url{https://anonymous.4open.science/r/code_icde2024-A9CB/}.
</details>
<details>
<summary>摘要</summary>
近些年来，图 нейрон网络（GNNs）在许多图关联任务中显示出前无似的成功。然而，GNNs still faces the label scarcity issue like other neural networks do. Therefore, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only a few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs to improve the model performance in this paper. Given a pre-trained GNN, we propose to search for a suitable fine-tuning framework for the given labeled data on the downstream task, which we call S2PGNN. To ensure the improvement brought by searching fine-tuning strategy, we carefully summarize a proper search space of fine-tuning framework that is suitable for GNNs. The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Besides, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area. Our code is publicly available at [uri].
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Allocation-of-Preventive-Care-With-Application-to-Diabetes-Mellitus-Type-II"><a href="#Data-Driven-Allocation-of-Preventive-Care-With-Application-to-Diabetes-Mellitus-Type-II" class="headerlink" title="Data-Driven Allocation of Preventive Care With Application to Diabetes Mellitus Type II"></a>Data-Driven Allocation of Preventive Care With Application to Diabetes Mellitus Type II</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06959">http://arxiv.org/abs/2308.06959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias Kraus, Stefan Feuerriegel, Maytal Saar-Tsechansky</li>
<li>for: 这篇论文的目的是为了提出一个可靠的、成本效益的决策模型，用于分配预防性治疗给有风险的病人。</li>
<li>methods: 本论文使用了Counterfactual推理、机器学习和优化技术，搭建了可扩展的决策模型，可以利用现代医疗记录中的高维度医疗数据。</li>
<li>results: 根据89,191名 prediabetic 病人的电子医疗记录进行评估，我们的决策模型可以与现有的实践相比，实现每年的成本储储11亿美元。此外，我们还分析了不同预算水平下的成本效益。<details>
<summary>Abstract</summary>
Problem Definition. Increasing costs of healthcare highlight the importance of effective disease prevention. However, decision models for allocating preventive care are lacking.   Methodology/Results. In this paper, we develop a data-driven decision model for determining a cost-effective allocation of preventive treatments to patients at risk. Specifically, we combine counterfactual inference, machine learning, and optimization techniques to build a scalable decision model that can exploit high-dimensional medical data, such as the data found in modern electronic health records. Our decision model is evaluated based on electronic health records from 89,191 prediabetic patients. We compare the allocation of preventive treatments (metformin) prescribed by our data-driven decision model with that of current practice. We find that if our approach is applied to the U.S. population, it can yield annual savings of $1.1 billion. Finally, we analyze the cost-effectiveness under varying budget levels.   Managerial Implications. Our work supports decision-making in health management, with the goal of achieving effective disease prevention at lower costs. Importantly, our decision model is generic and can thus be used for effective allocation of preventive care for other preventable diseases.
</details>
<details>
<summary>摘要</summary>
问题定义：医疗费用的增长强调了疾病预防的重要性。然而，疾病预防投入决策模型缺失。方法ология/结果：在这篇论文中，我们开发了基于数据驱动的疾病预防投入决策模型，用于确定对患有风险的患者进行成本效果的预防治疗分配。我们结合了Counterfactual推理、机器学习和优化技术，构建了可扩展的决策模型，可以利用现代电子医疗记录中的高维医疗数据。我们的决策模型通过对89191名 prediabetic 患者的电子医疗记录进行评估。我们比较了我们的数据驱动决策模型与当前做法分配预防治疗（metformin）的情况。我们发现，如果我们的方法应用于美国人口，可以每年节省11亿美元。最后，我们分析了不同预算水平下的成本效果。管理意义：我们的工作支持医疗管理决策，以实现有效的疾病预防，并降低成本。重要的是，我们的决策模型是通用的，可以用于有效地分配预防治疗其他预防性疾病。
</details></li>
</ul>
<hr>
<h2 id="CEmb-SAM-Segment-Anything-Model-with-Condition-Embedding-for-Joint-Learning-from-Heterogeneous-Datasets"><a href="#CEmb-SAM-Segment-Anything-Model-with-Condition-Embedding-for-Joint-Learning-from-Heterogeneous-Datasets" class="headerlink" title="CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets"></a>CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06957">http://arxiv.org/abs/2308.06957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongik Shin, Beomsuk Kim, Seungjun Baek</li>
<li>for: 这篇论文是用于探讨如何将不同类型的静脉影像融合为一个共同的数据集，以提高医疗影像分类模型的通用能力。</li>
<li>methods: 这篇论文使用的方法是将不同类型的静脉影像融合为一个共同的数据集，然后使用Segment Anything模型（SAM）来进行静脉影像分类。此外，论文还提出了一个名为Condition Embedding block（CEmb）的新方法，可以将不同数据集的特性统计学 Normalization。</li>
<li>results: 实验结果显示，CEmb-SAM比基eline方法在静脉影像分类 задачі中表现更好，特别是在 péripheral nerves和breast cancer领域。这些结果显示了Cemb-SAM在医疗影像分类任务中学习不同数据集的能力。<details>
<summary>Abstract</summary>
Automated segmentation of ultrasound images can assist medical experts with diagnostic and therapeutic procedures. Although using the common modality of ultrasound, one typically needs separate datasets in order to segment, for example, different anatomical structures or lesions with different levels of malignancy. In this paper, we consider the problem of jointly learning from heterogeneous datasets so that the model can improve generalization abilities by leveraging the inherent variability among datasets. We merge the heterogeneous datasets into one dataset and refer to each component dataset as a subgroup. We propose to train a single segmentation model so that the model can adapt to each sub-group. For robust segmentation, we leverage recently proposed Segment Anything model (SAM) in order to incorporate sub-group information into the model. We propose SAM with Condition Embedding block (CEmb-SAM) which encodes sub-group conditions and combines them with image embeddings from SAM. The conditional embedding block effectively adapts SAM to each image sub-group by incorporating dataset properties through learnable parameters for normalization. Experiments show that CEmb-SAM outperforms the baseline methods on ultrasound image segmentation for peripheral nerves and breast cancer. The experiments highlight the effectiveness of Cemb-SAM in learning from heterogeneous datasets in medical image segmentation tasks.
</details>
<details>
<summary>摘要</summary>
自动分割超音波图像可以帮助医疗专家进行诊断和治疗过程。although using the common modality of ultrasound, one typically needs separate datasets in order to segment, for example, different anatomical structures or lesions with different levels of malignancy. 在这篇论文中，我们考虑了将异构数据集合在一起，以便模型可以提高通用能力，并且可以利用数据集的内在多样性。我们将异构数据集合为一个数据集，并将每个子组数据集称为子组。我们提议使用单个分割模型，以便模型可以适应每个子组。为了增强分割稳定性，我们利用最近提出的 Segment Anything model (SAM)，并在SAM中添加 Condition Embedding block (CEmb)，以编码子组条件并与图像嵌入结合。 conditional embedding block Effectively adapts SAM to each image subgroup by incorporating dataset properties through learnable parameters for normalization. 实验显示，CEmb-SAM在超音波图像分割任务中超过基eline方法表现，特别是在 péripheral nerves 和乳腺癌中。这些实验证明了 CEmb-SAM 在医学图像分割任务中学习异构数据集的有效性。
</details></li>
</ul>
<hr>
<h2 id="Channel-Wise-Contrastive-Learning-for-Learning-with-Noisy-Labels"><a href="#Channel-Wise-Contrastive-Learning-for-Learning-with-Noisy-Labels" class="headerlink" title="Channel-Wise Contrastive Learning for Learning with Noisy Labels"></a>Channel-Wise Contrastive Learning for Learning with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06952">http://arxiv.org/abs/2308.06952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Kang, Sheng Liu, Huaxi Huang, Tongliang Liu</li>
<li>for: 本文针对受损标签学习（LNL）问题，旨在训练一个能够识别实际类别的分类器。</li>
<li>methods: 本文提出了通道级别对比学习（CWCL）方法，通过对不同通道进行对比学习，分离真实标签信息和噪声。</li>
<li>results: 对多个基准数据集进行评估，本文的方法与现有方法相比，具有更高的识别率和更好的抗噪声性。<details>
<summary>Abstract</summary>
In real-world datasets, noisy labels are pervasive. The challenge of learning with noisy labels (LNL) is to train a classifier that discerns the actual classes from given instances. For this, the model must identify features indicative of the authentic labels. While research indicates that genuine label information is embedded in the learned features of even inaccurately labeled data, it's often intertwined with noise, complicating its direct application. Addressing this, we introduce channel-wise contrastive learning (CWCL). This method distinguishes authentic label information from noise by undertaking contrastive learning across diverse channels. Unlike conventional instance-wise contrastive learning (IWCL), CWCL tends to yield more nuanced and resilient features aligned with the authentic labels. Our strategy is twofold: firstly, using CWCL to extract pertinent features to identify cleanly labeled samples, and secondly, progressively fine-tuning using these samples. Evaluations on several benchmark datasets validate our method's superiority over existing approaches.
</details>
<details>
<summary>摘要</summary>
实际数据集中，噪声标签是普遍存在的。学习噪声标签（LNL）的挑战是训练一个可以从给定的实例中分辨实际的类别的分类器。为此，模型必须标识表示实际标签的特征。虽然研究表明，正确的标签信息在噪声标签的数据中被学习的特征中嵌入，但它通常与噪声杂mix在一起，使其直接应用更加复杂。为解决这个问题，我们引入通道wise contrastive learning（CWCL）。这种方法通过在多个通道进行对比学习来 отличи出实际标签信息和噪声。与传统的实例wise contrastive learning（IWCL）不同，CWCL往往可以生成更加细化和鲜明的特征，与实际标签更加相似。我们的策略是两重的：首先，使用CWCL提取重要的特征，以便从干净标注的样本中分辨实际标签信息；其次，逐渐练化使用这些样本。我们在多个标准 benchmark 数据集上进行了评估，并证明了我们的方法与现有方法相比具有superiority。
</details></li>
</ul>
<hr>
<h2 id="Knowing-Where-to-Focus-Event-aware-Transformer-for-Video-Grounding"><a href="#Knowing-Where-to-Focus-Event-aware-Transformer-for-Video-Grounding" class="headerlink" title="Knowing Where to Focus: Event-aware Transformer for Video Grounding"></a>Knowing Where to Focus: Event-aware Transformer for Video Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06947">http://arxiv.org/abs/2308.06947</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhyunj/eatr">https://github.com/jinhyunj/eatr</a></li>
<li>paper_authors: Jinhyun Jang, Jungin Park, Jin Kim, Hyeongjun Kwon, Kwanghoon Sohn</li>
<li>for: 这 paper 的目的是提出一种事件意识的动态时刻查询方法，以便模型可以根据输入视频的内容和位置信息来更好地预测时刻。</li>
<li>methods: 这 paper 使用了一种叫做槽注意机制的事件分解技术，以及一种名叫网关融合变换层的时刻查询融合技术，来实现事件意识的动态时刻查询。</li>
<li>results: 根据实验结果，这 paper 的事件意识动态时刻查询方法在多个视频锚点识别 benchmark 上表现出了比州前方法更高的效果和效率。<details>
<summary>Abstract</summary>
Recent DETR-based video grounding models have made the model directly predict moment timestamps without any hand-crafted components, such as a pre-defined proposal or non-maximum suppression, by learning moment queries. However, their input-agnostic moment queries inevitably overlook an intrinsic temporal structure of a video, providing limited positional information. In this paper, we formulate an event-aware dynamic moment query to enable the model to take the input-specific content and positional information of the video into account. To this end, we present two levels of reasoning: 1) Event reasoning that captures distinctive event units constituting a given video using a slot attention mechanism; and 2) moment reasoning that fuses the moment queries with a given sentence through a gated fusion transformer layer and learns interactions between the moment queries and video-sentence representations to predict moment timestamps. Extensive experiments demonstrate the effectiveness and efficiency of the event-aware dynamic moment queries, outperforming state-of-the-art approaches on several video grounding benchmarks.
</details>
<details>
<summary>摘要</summary>
现代基于DETR的录像落幕模型已经直接预测时刻无需任何手工Component，如预先定义的提案或非最大抑制，通过学习时刻查询。然而，这些输入不具预设的时刻查询无法考虑录像的自然时间结构，仅提供有限的位置信息。在本文中，我们提出了事件意识的动态时刻查询，让模型能够根据输入内容和位置信息进行事件对话。为此，我们提出了两种逻辑：1）事件逻辑，使用槽注意力机制来捕捉录像中的特定事件单位；2）时刻逻辑，将时刻查询与输入句子的数据融合，透过闸道融合对应层来学习时刻查询与录像句子表示之间的互动，以预测时刻。实验结果显示了事件意识的动态时刻查询的有效性和高效性，在多个录像落幕 bencmarks 上出perform state-of-the-art 方法。
</details></li>
</ul>
<hr>
<h2 id="Semantic-aware-Network-for-Aerial-to-Ground-Image-Synthesis"><a href="#Semantic-aware-Network-for-Aerial-to-Ground-Image-Synthesis" class="headerlink" title="Semantic-aware Network for Aerial-to-Ground Image Synthesis"></a>Semantic-aware Network for Aerial-to-Ground Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06945">http://arxiv.org/abs/2308.06945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhyunj/sanet">https://github.com/jinhyunj/sanet</a></li>
<li>paper_authors: Jinhyun Jang, Taeyong Song, Kwanghoon Sohn</li>
<li>for: 这个论文旨在解决飞行图像与地面图像的同构问题，以实现将飞行图像中的元素转映到地面图像中。</li>
<li>methods: 该论文提出了一个新的框架，通过增强结构对应和Semantic意识来解决这个问题。它 introduce了一种新的Semantic-attentive feature transformation模块，可以重建复杂的地理结构。此外，论文还提出了Semantic-aware的损失函数，通过利用预训练的分类网络，让网络synthesize出realistic的对象。</li>
<li>results: 对比之前的方法和简化研究，论文的方法得到了较高的效果， both qualitatively and quantitatively。<details>
<summary>Abstract</summary>
Aerial-to-ground image synthesis is an emerging and challenging problem that aims to synthesize a ground image from an aerial image. Due to the highly different layout and object representation between the aerial and ground images, existing approaches usually fail to transfer the components of the aerial scene into the ground scene. In this paper, we propose a novel framework to explore the challenges by imposing enhanced structural alignment and semantic awareness. We introduce a novel semantic-attentive feature transformation module that allows to reconstruct the complex geographic structures by aligning the aerial feature to the ground layout. Furthermore, we propose semantic-aware loss functions by leveraging a pre-trained segmentation network. The network is enforced to synthesize realistic objects across various classes by separately calculating losses for different classes and balancing them. Extensive experiments including comparisons with previous methods and ablation studies show the effectiveness of the proposed framework both qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:空中到地面图像合成是一个emerging和挑战性的问题，目标是将空中图像转换为地面图像。由于空中和地面图像的布局和对象表示方式之间存在巨大差异，现有的方法通常无法将空中场景中的组件转移到地面场景中。在这篇论文中，我们提出了一个新的框架，以强化结构对Alignment和Semantic意识。我们引入了一个新的启发式Semantic-attentive特征转换模块，以将空中特征转换为地面布局。此外，我们提出了Semantic-aware的损失函数，通过利用预训练的分割网络来强制网络在不同类型的对象上synthesize出真实的对象。我们进行了广泛的实验，包括与前方法进行比较和简要的ablation研究，以证明我们的框架的有效性。
</details></li>
</ul>
<hr>
<h2 id="Insurance-pricing-on-price-comparison-websites-via-reinforcement-learning"><a href="#Insurance-pricing-on-price-comparison-websites-via-reinforcement-learning" class="headerlink" title="Insurance pricing on price comparison websites via reinforcement learning"></a>Insurance pricing on price comparison websites via reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06935">http://arxiv.org/abs/2308.06935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanut Treetanthiploet, Yufei Zhang, Lukasz Szpruch, Isaac Bowers-Barnard, Henrietta Ridley, James Hickey, Chris Pearce</li>
<li>for: 本研究旨在为保险公司在价格比较网站（PCW）上形ulation Effective 价格策略提供技术支持。</li>
<li>methods: 本研究使用了强化学习（RL）框架，通过结合模型基于和模型自由方法来学习价格策略。</li>
<li>results: 比较 experiment 表明，我们的方法在sample efficiency和累积奖励方面超过了6个参考方法，只有在市场情况具有完美信息时才能达到相同水平。<details>
<summary>Abstract</summary>
The emergence of price comparison websites (PCWs) has presented insurers with unique challenges in formulating effective pricing strategies. Operating on PCWs requires insurers to strike a delicate balance between competitive premiums and profitability, amidst obstacles such as low historical conversion rates, limited visibility of competitors' actions, and a dynamic market environment. In addition to this, the capital intensive nature of the business means pricing below the risk levels of customers can result in solvency issues for the insurer. To address these challenges, this paper introduces reinforcement learning (RL) framework that learns the optimal pricing policy by integrating model-based and model-free methods. The model-based component is used to train agents in an offline setting, avoiding cold-start issues, while model-free algorithms are then employed in a contextual bandit (CB) manner to dynamically update the pricing policy to maximise the expected revenue. This facilitates quick adaptation to evolving market dynamics and enhances algorithm efficiency and decision interpretability. The paper also highlights the importance of evaluating pricing policies using an offline dataset in a consistent fashion and demonstrates the superiority of the proposed methodology over existing off-the-shelf RL/CB approaches. We validate our methodology using synthetic data, generated to reflect private commercially available data within real-world insurers, and compare against 6 other benchmark approaches. Our hybrid agent outperforms these benchmarks in terms of sample efficiency and cumulative reward with the exception of an agent that has access to perfect market information which would not be available in a real-world set-up.
</details>
<details>
<summary>摘要</summary>
互联网价格比较网站（PCW）的出现对保险公司带来了独特的挑战。在PCW上运营时，保险公司需要维护一个折衔的平衡，以确保竞争力和利润之间的协调。这些挑战包括历史 conversions 率低、竞争对手动作的有限可见性以及动态的市场环境。此外，保险业务具有资本投入的特点，因此低于客户风险水平的价格可能会导致资本问题。为解决这些挑战，本文提出了一种基于强化学习（RL）框架的价格策略优化方法。RL框架结合模型基于和模型自由两种方法，通过在离线环境中训练代理人，避免冷启始问题，然后在Contextual Bandit（CB）上使用模型自由算法动态更新价格策略，以最大化预期收益。这有助于快速适应市场动态变化，提高算法效率和决策可读性。文章还强调了评估价格策略的离线数据集的一致性，并证明提议的方法在已有的RL/CB方法中表现出色。我们验证了方法使用人工生成的数据，模拟了实际保险公司的私人数据，并与6个参考方法进行比较。我们的混合代理人在样本效益和累积奖励方面都超越参考方法，只有具有完美市场信息的代理人能够在实际场景中匹配我们的方法。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Listing-Prices-In-Dynamic-Short-Term-Rental-Markets-Using-Machine-Learning-Models"><a href="#Predicting-Listing-Prices-In-Dynamic-Short-Term-Rental-Markets-Using-Machine-Learning-Models" class="headerlink" title="Predicting Listing Prices In Dynamic Short Term Rental Markets Using Machine Learning Models"></a>Predicting Listing Prices In Dynamic Short Term Rental Markets Using Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06929">http://arxiv.org/abs/2308.06929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Chapman, Seifey Mohammad, Kimberly Villegas</li>
<li>for: 预测Airbnb租赁价格，帮助hosts优化收益和助旅行者做出 Informed 预订决策。</li>
<li>methods: 使用机器学习模型方法，包括sentiment分析，对Airbnb租赁价格进行预测和分析。</li>
<li>results: 通过对Airbnb租赁价格进行预测和分析，提高了hosts的收益和旅行者的决策能力。<details>
<summary>Abstract</summary>
Our research group wanted to take on the difficult task of predicting prices in a dynamic market. And short term rentals such as Airbnb listings seemed to be the perfect proving ground to do such a thing. Airbnb has revolutionized the travel industry by providing a platform for homeowners to rent out their properties to travelers. The pricing of Airbnb rentals is prone to high fluctuations, with prices changing frequently based on demand, seasonality, and other factors. Accurate prediction of Airbnb rental prices is crucial for hosts to optimize their revenue and for travelers to make informed booking decisions. In this project, we aim to predict the prices of Airbnb rentals using a machine learning modeling approach.   Our project expands on earlier research in the area of analyzing Airbnb rental prices by taking a methodical machine learning approach as well as incorporating sentiment analysis into our feature engineering. We intend to gain a deeper understanding on periodic changes of Airbnb rental prices. The primary objective of this study is to construct an accurate machine learning model for predicting Airbnb rental prices specifically in Austin, Texas. Our project's secondary objective is to identify the key factors that drive Airbnb rental prices and to investigate how these factors vary across different locations and property types.
</details>
<details>
<summary>摘要</summary>
我们的研究小组想要解决难度较大的价格预测问题，并选择短期租赁如Airbnb列表作为证明场景。Airbnb已经革命化旅游业，提供了为房东租出房屋给旅行者的平台。Airbnb租赁价格受到高度波动的影响，价格随着需求、季节和其他因素而变化频繁。正确预测Airbnb租赁价格对房东来说是提高收益的关键，对旅行者来说也是为了做出 Informed 预订决策。在这个项目中，我们使用机器学习模型预测Airbnb租赁价格。我们的项目在对Airbnb租赁价格分析方面进行了深入探索，并且通过包括情感分析在内的特征工程来扩展我们的研究。我们的主要目标是在得克萨斯州奥斯汀建立一个准确的机器学习模型，用于预测Airbnb租赁价格。项目的次要目标是Identify 租赁价格的关键因素，以及这些因素在不同的地点和财产类型中的变化趋势。
</details></li>
</ul>
<hr>
<h2 id="CBA-Improving-Online-Continual-Learning-via-Continual-Bias-Adaptor"><a href="#CBA-Improving-Online-Continual-Learning-via-Continual-Bias-Adaptor" class="headerlink" title="CBA: Improving Online Continual Learning via Continual Bias Adaptor"></a>CBA: Improving Online Continual Learning via Continual Bias Adaptor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06925">http://arxiv.org/abs/2308.06925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wqza/cba-online-cl">https://github.com/wqza/cba-online-cl</a></li>
<li>paper_authors: Quanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, Deyu Meng</li>
<li>for: 这篇论文目的是为了解决在线上持续学习（Continual Learning，CL）中的分布变化问题，以确保模型能够稳定地学习新的知识和固化先前学习的知识。</li>
<li>methods: 本文提出了一个增强器（Continual Bias Adaptor，CBA）模组，用于在训练过程中将分类器网络调整为适应不断变化的分布，以避免模型对先前学习的知识忘记和偏向新的任务。</li>
<li>results: 本文透过实验证明了CBA模组能够有效地解决分布变化问题，并且不会增加训练过程中的计算成本和记忆遗传。<details>
<summary>Abstract</summary>
Online continual learning (CL) aims to learn new knowledge and consolidate previously learned knowledge from non-stationary data streams. Due to the time-varying training setting, the model learned from a changing distribution easily forgets the previously learned knowledge and biases toward the newly received task. To address this problem, we propose a Continual Bias Adaptor (CBA) module to augment the classifier network to adapt to catastrophic distribution change during training, such that the classifier network is able to learn a stable consolidation of previously learned tasks. In the testing stage, CBA can be removed which introduces no additional computation cost and memory overhead. We theoretically reveal the reason why the proposed method can effectively alleviate catastrophic distribution shifts, and empirically demonstrate its effectiveness through extensive experiments based on four rehearsal-based baselines and three public continual learning benchmarks.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在线 continual learning (CL) 目标是在非站点数据流中学习新知识并固化先前学习的知识。由于训练环境的时间变化，学习到的模型很容易忘记先前学习的知识，偏向新接收的任务。为解决这个问题，我们提议一个 Continual Bias Adaptor (CBA) 模块，用于在训练过程中增强分类网络，以适应不断变化的分布，使得分类网络能够稳定地固化先前学习的任务。在测试阶段，CBA可以被移除，无需额外的计算成本和内存占用。我们理论上解释了我们提议的方法可以有效缓解悬危分布变化的问题，并通过了四种基eline和三个公共 continual learning 标准 benchmark 的实验来证明其效果。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Ehanced-Move-Recognition-Algorithm-Based-on-Pre-trained-Models-with-Positional-Embeddings"><a href="#A-Novel-Ehanced-Move-Recognition-Algorithm-Based-on-Pre-trained-Models-with-Positional-Embeddings" class="headerlink" title="A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings"></a>A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10822">http://arxiv.org/abs/2308.10822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wen, Jie Wang, Xiaodong Qiao</li>
<li>for: 本研究旨在提高中文科技论文摘要中的 Move 识别精度。</li>
<li>methods: 该研究提出了一种基于改进预训练模型和扩展网络含attend mechanism的新型加强 Move 识别算法。</li>
<li>results: 实验结果显示，该算法在分 Split 数据集上比原始数据集提高13.37%的精度，并与基本对比模型提高7.55%的精度。<details>
<summary>Abstract</summary>
The recognition of abstracts is crucial for effectively locating the content and clarifying the article. Existing move recognition algorithms lack the ability to learn word position information to obtain contextual semantics. This paper proposes a novel enhanced move recognition algorithm with an improved pre-trained model and a gated network with attention mechanism for unstructured abstracts of Chinese scientific and technological papers. The proposed algorithm first performs summary data segmentation and vocabulary training. The EP-ERNIE$\_$AT-GRU framework is leveraged to incorporate word positional information, facilitating deep semantic learning and targeted feature extraction. Experimental results demonstrate that the proposed algorithm achieves 13.37$\%$ higher accuracy on the split dataset than on the original dataset and a 7.55$\%$ improvement in accuracy over the basic comparison model.
</details>
<details>
<summary>摘要</summary>
“抽象概念识别是科技文献检索和理解的关键。现有的移动识别算法无法学习单词位置信息，导致Contextual semantics的学习受限。本文提出了一种基于EP-ERNIE$\_$AT-GRU框架的增强移动识别算法，用于处理中文科技文献抽象。该算法首先进行摘要数据分 segmentation和词汇训练。实验结果表明，提posed算法在分组数据集上的准确率高于原始数据集13.37%，并且与基本比较模型相比提高7.55%。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="CausalLM-is-not-optimal-for-in-context-learning"><a href="#CausalLM-is-not-optimal-for-in-context-learning" class="headerlink" title="CausalLM is not optimal for in-context learning"></a>CausalLM is not optimal for in-context learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06912">http://arxiv.org/abs/2308.06912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, Radu Soricut</li>
<li>for: 本研究探讨了使用前缀语言模型（prefixLM）和 causalLanguage Model（causalLM）在卷积Transformer上的受限学习性能的比较。</li>
<li>methods: 本研究采用了理论方法来分析prefixLM和causalLM的参数构造下的收敛行为。</li>
<li>results: 研究发现，prefixLM在线性回归问题上 converges to its optimal solution，而causalLM的收敛动态类似于在线梯度下降算法，并不一定是最优解，即使数据量无限大。Empirical experiments over synthetic and real tasks verify that causalLM consistently underperforms prefixLM in all settings。<details>
<summary>Abstract</summary>
Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.
</details>
<details>
<summary>摘要</summary>
近期实验证据表明，基于转换器的增强学习在使用前缀语言模型（前缀LM）下表现更好，因为所有的增强样本都可以相互听说，而不使用 causalLM（ causalLM），它使用自动循环注意力，禁止增强样本听说未来的样本。虽然这种结果是直观的，但从理论角度不是很了解。在这篇论文中，我们采取了理论方法，分析了 prefixLM 和 causalLM 下的参数构造下的收敛行为。我们的分析表明，两种LM类型都会在一定的参数构造下收敛到其站点点，但是 prefixLM 会收敛到线性回归的优化解，而 causalLM 的收敛动力则类似于在线上的梯度下降算法，这并不是保证优化的，即使样本数量在无穷大。我们通过实验证明， causalLM 在所有设置下一直表现出下降性。
</details></li>
</ul>
<hr>
<h2 id="GIT-Mol-A-Multi-modal-Large-Language-Model-for-Molecular-Science-with-Graph-Image-and-Text"><a href="#GIT-Mol-A-Multi-modal-Large-Language-Model-for-Molecular-Science-with-Graph-Image-and-Text" class="headerlink" title="GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text"></a>GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06911">http://arxiv.org/abs/2308.06911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Liu, Yiming Ren, Zhixiang Ren</li>
<li>for: 这个论文主要是为了提出一种多ModalLanguage模型，用于融合Graph、Image和Text信息，以提高分子数据的表示和生成能力。</li>
<li>methods: 这种模型使用GIT-Former架构，可以将所有modalities映射到一个共同准则空间中，以便进行多ModalLanguage的表示和计算。</li>
<li>results: 该研究提出了一种创新的任意语言分子翻译策略，提高了分子描述率10%-15%，提高了属性预测精度5%-10%，并提高了分子生成有效性20%。<details>
<summary>Abstract</summary>
Large language models have made significant strides in natural language processing, paving the way for innovative applications including molecular representation and generation. However, most existing single-modality approaches cannot capture the abundant and complex information in molecular data. Here, we introduce GIT-Mol, a multi-modal large language model that integrates the structure Graph, Image, and Text information, including the Simplified Molecular Input Line Entry System (SMILES) and molecular captions. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture capable of mapping all modalities into a unified latent space. Our study develops an innovative any-to-language molecular translation strategy and achieves a 10%-15% improvement in molecular captioning, a 5%-10% accuracy increase in property prediction, and a 20% boost in molecule generation validity compared to baseline or single-modality models.
</details>
<details>
<summary>摘要</summary>
大型语言模型在自然语言处理方面做出了重要进步，开辟了创新应用，如分子表示和生成。然而，现有的单一模式方法无法捕捉分子数据中的丰富和复杂信息。在这里，我们介绍GIT-Mol，一个多模式大语言模型，它结合结构граф、图像和文本信息，包括简单分子输入系统（SMILES）和分子描述。为了实现多modal分子数据的整合，我们提出GIT-Former，一个新的架构，可以将所有模式转换到一个统一的潜在空间中。我们的研究开发了一种创新的任何语言分子翻译策略，并在分子描述、性能预测和分子生成领域中实现了10%-15%的改进，5%-10%的精度提高和20%的额外验证。
</details></li>
</ul>
<hr>
<h2 id="Generative-Interpretation"><a href="#Generative-Interpretation" class="headerlink" title="Generative Interpretation"></a>Generative Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06907">http://arxiv.org/abs/2308.06907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yonathanarbel/generativeinterpretation">https://github.com/yonathanarbel/generativeinterpretation</a></li>
<li>paper_authors: Yonathan A. Arbel, David Hoffman</li>
<li>for: 这篇论文旨在提出一种新的合同理解方法，使用大型自然语言模型来估计合同意义。</li>
<li>methods: 该论文采用了实践案例研究的方法，通过使用AI模型来查看不同的应用场景，并使用实际的合同文档来证明AI模型的能力。</li>
<li>results: 该论文显示了AI模型可以帮助法官和仲裁人员更好地理解合同的意义，量化含义的混淆度，并填充合同中的缺失。同时，该论文还描述了使用这些模型时的限制和风险，以及它们对法律实践和合同理论的影响。<details>
<summary>Abstract</summary>
We introduce generative interpretation, a new approach to estimating contractual meaning using large language models. As AI triumphalism is the order of the day, we proceed by way of grounded case studies, each illustrating the capabilities of these novel tools in distinct ways. Taking well-known contracts opinions, and sourcing the actual agreements that they adjudicated, we show that AI models can help factfinders ascertain ordinary meaning in context, quantify ambiguity, and fill gaps in parties' agreements. We also illustrate how models can calculate the probative value of individual pieces of extrinsic evidence. After offering best practices for the use of these models given their limitations, we consider their implications for judicial practice and contract theory. Using LLMs permits courts to estimate what the parties intended cheaply and accurately, and as such generative interpretation unsettles the current interpretative stalemate. Their use responds to efficiency-minded textualists and justice-oriented contextualists, who argue about whether parties will prefer cost and certainty or accuracy and fairness. Parties--and courts--would prefer a middle path, in which adjudicators strive to predict what the contract really meant, admitting just enough context to approximate reality while avoiding unguided and biased assimilation of evidence. As generative interpretation offers this possibility, we argue it can become the new workhorse of contractual interpretation.
</details>
<details>
<summary>摘要</summary>
我们引入生成式解释，一种新的方法来估计合同意义使用大型语言模型。在人工智能胜利的时代，我们采用实地案例来证明这些新工具的能力，每个案例都展示了这些工具在不同方面的能力。我们使用知名合同案例，并提供了实际协议，以示AI模型可以帮助事实发现者在文本上确定常见意义，衡量模糊性，并填充党们的协议中的空白。我们还示出了模型可以计算个别外部证据的证据价值。接着，我们提供了使用这些模型的最佳实践，以及其限制。我们考虑了这些模型在法律实践和合同理论方面的影响，并 argues that these models can become the new workhorse of contractual interpretation.Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Federated-Classification-in-Hyperbolic-Spaces-via-Secure-Aggregation-of-Convex-Hulls"><a href="#Federated-Classification-in-Hyperbolic-Spaces-via-Secure-Aggregation-of-Convex-Hulls" class="headerlink" title="Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls"></a>Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06895">http://arxiv.org/abs/2308.06895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurav Prakash, Jin Sima, Chao Pan, Eli Chien, Olgica Milenkovic</li>
<li>for: 这个论文是为了研究在几何空间中进行分布式学习，以保护数据隐私。</li>
<li>methods: 该论文提出了一种基于几何空间的分布式学习方法，包括在Poincaré圆柱中分布式的支持向量机学习和一种基于整数B_h序列的标签恢复方法。</li>
<li>results: 该论文的实验结果表明，使用几何空间进行分布式学习可以提高分类精度，并且可以保护数据隐私。<details>
<summary>Abstract</summary>
Hierarchical and tree-like data sets arise in many applications, including language processing, graph data mining, phylogeny and genomics. It is known that tree-like data cannot be embedded into Euclidean spaces of finite dimension with small distortion. This problem can be mitigated through the use of hyperbolic spaces. When such data also has to be processed in a distributed and privatized setting, it becomes necessary to work with new federated learning methods tailored to hyperbolic spaces. As an initial step towards the development of the field of federated learning in hyperbolic spaces, we propose the first known approach to federated classification in hyperbolic spaces. Our contributions are as follows. First, we develop distributed versions of convex SVM classifiers for Poincar\'e discs. In this setting, the information conveyed from clients to the global classifier are convex hulls of clusters present in individual client data. Second, to avoid label switching issues, we introduce a number-theoretic approach for label recovery based on the so-called integer $B_h$ sequences. Third, we compute the complexity of the convex hulls in hyperbolic spaces to assess the extent of data leakage; at the same time, in order to limit the communication cost for the hulls, we propose a new quantization method for the Poincar\'e disc coupled with Reed-Solomon-like encoding. Fourth, at server level, we introduce a new approach for aggregating convex hulls of the clients based on balanced graph partitioning. We test our method on a collection of diverse data sets, including hierarchical single-cell RNA-seq data from different patients distributed across different repositories that have stringent privacy constraints. The classification accuracy of our method is up to $\sim 11\%$ better than its Euclidean counterpart, demonstrating the importance of privacy-preserving learning in hyperbolic spaces.
</details>
<details>
<summary>摘要</summary>
Hierarchical和树状数据集在许多应用中出现，包括语言处理、图数据挖掘、phylogeny和 genomics。已知树状数据无法在 finite 维 Euclidian 空间中嵌入，这问题可以通过使用抽象空间来缓解。在分布式和隐私化设置下，需要采用新的联邦学习方法，这是一个新的领域。作为这个领域的初步，我们提出了首个在抽象空间上的联邦分类方法。我们的贡献如下：1. 我们开发了分布式版本的凸 Support Vector Machine（SVM）分类器，用于Poincaré盘上的数据。在这个设置中，客户端上的信息是各个客户端数据中的封闭集。2. 为了避免标签交换问题，我们引入了一种数学推理的方法，基于 so-called 整数 $B_h$ 序列。3. 我们计算了抽象空间中的凸闭复杂度，以评估数据泄露程度，同时，我们提出了一种新的归一化方法，用于限制归一化成本。4. 在服务器端，我们引入了一种新的客户端归一化方法，基于平衡图分 partitioning。我们测试了我们的方法在多种多样的数据集上，包括层次单元 RNA-seq 数据集，来自不同的病人和不同的存储库，这些存储库具有严格的隐私限制。我们的方法的分类精度与其欧氏凸缩形相比，提高了约 11%，这表明了隐私保护在抽象空间上的学习的重要性。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Offline-Online-Evaluation-with-a-Time-dependent-and-Popularity-Bias-free-Offline-Metric-for-Recommenders"><a href="#Bridging-Offline-Online-Evaluation-with-a-Time-dependent-and-Popularity-Bias-free-Offline-Metric-for-Recommenders" class="headerlink" title="Bridging Offline-Online Evaluation with a Time-dependent and Popularity Bias-free Offline Metric for Recommenders"></a>Bridging Offline-Online Evaluation with a Time-dependent and Popularity Bias-free Offline Metric for Recommenders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06885">http://arxiv.org/abs/2308.06885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Petr Kasalický, Rodrigo Alves, Pavel Kordík</li>
<li>for: 研究和比较在线表现的Offline评估指标的选择，以提高live推荐系统中的选择。</li>
<li>methods: 使用减小流行item和考虑交易时间的评估方法，以提高选择的准确性。</li>
<li>results: 五个大型实际live数据中的平均结果，用于帮助学术社区更好地理解Offline评估和优化标准的选择。<details>
<summary>Abstract</summary>
The evaluation of recommendation systems is a complex task. The offline and online evaluation metrics for recommender systems are ambiguous in their true objectives. The majority of recently published papers benchmark their methods using ill-posed offline evaluation methodology that often fails to predict true online performance. Because of this, the impact that academic research has on the industry is reduced. The aim of our research is to investigate and compare the online performance of offline evaluation metrics. We show that penalizing popular items and considering the time of transactions during the evaluation significantly improves our ability to choose the best recommendation model for a live recommender system. Our results, averaged over five large-size real-world live data procured from recommenders, aim to help the academic community to understand better offline evaluation and optimization criteria that are more relevant for real applications of recommender systems.
</details>
<details>
<summary>摘要</summary>
评估推荐系统是一项复杂的任务。在线和离线评估指标对于推荐系统存在很多不确定性。大多数最近发表的论文使用不确定的离线评估方法来评估其方法的性能，这常导致实际上线性能和学术界的影响相差甚大。我们的研究目的是研究和比较离线评估指标对实时推荐系统的在线性能的影响。我们发现，对流行 item 的惩罚和在评估过程中考虑交易时间可以显著提高我们选择最佳推荐模型的能力。我们的结果，基于五个大型实际应用中的真实数据，希望能帮助学术界更好地理解推荐系统的离线评估和优化标准，以便更好地应用于实际应用中。
</details></li>
</ul>
<hr>
<h2 id="Multi-Receiver-Task-Oriented-Communications-via-Multi-Task-Deep-Learning"><a href="#Multi-Receiver-Task-Oriented-Communications-via-Multi-Task-Deep-Learning" class="headerlink" title="Multi-Receiver Task-Oriented Communications via Multi-Task Deep Learning"></a>Multi-Receiver Task-Oriented Communications via Multi-Task Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06884">http://arxiv.org/abs/2308.06884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus</li>
<li>for: 这 paper 研究了任务对应的通信系统，在 transmitter 与多个 receivers 之间进行任务完成和数据传输。</li>
<li>methods: 这 paper 提出了一种基于多任务深度学习的共同编码器和个别解码器的方法，用于对多个任务和多个接收器进行共同优化。</li>
<li>results: 实验结果表明，相比单任务传输系统，多任务传输系统可以更好地适应变化的通信频道条件，并且可以提高任务特定的目标 completions，同时减少传输负担。<details>
<summary>Abstract</summary>
This paper studies task-oriented, otherwise known as goal-oriented, communications, in a setting where a transmitter communicates with multiple receivers, each with its own task to complete on a dataset, e.g., images, available at the transmitter. A multi-task deep learning approach that involves training a common encoder at the transmitter and individual decoders at the receivers is presented for joint optimization of completing multiple tasks and communicating with multiple receivers. By providing efficient resource allocation at the edge of 6G networks, the proposed approach allows the communications system to adapt to varying channel conditions and achieves task-specific objectives while minimizing transmission overhead. Joint training of the encoder and decoders using multi-task learning captures shared information across tasks and optimizes the communication process accordingly. By leveraging the broadcast nature of wireless communications, multi-receiver task-oriented communications (MTOC) reduces the number of transmissions required to complete tasks at different receivers. Performance evaluation conducted on the MNIST, Fashion MNIST, and CIFAR-10 datasets (with image classification considered for different tasks) demonstrates the effectiveness of MTOC in terms of classification accuracy and resource utilization compared to single-task-oriented communication systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Quantifying-Outlierness-of-Funds-from-their-Categories-using-Supervised-Similarity"><a href="#Quantifying-Outlierness-of-Funds-from-their-Categories-using-Supervised-Similarity" class="headerlink" title="Quantifying Outlierness of Funds from their Categories using Supervised Similarity"></a>Quantifying Outlierness of Funds from their Categories using Supervised Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06882">http://arxiv.org/abs/2308.06882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Desai, Ashmita Dhiman, Tushar Sharma, Deepika Sharma, Dhagash Mehta, Stefano Pasquali</li>
<li>for: 本研究旨在量化基金分类错误的影响，以便改善投资管理决策。</li>
<li>methods: 本研究使用机器学习方法，将基金分类错误形式化为距离度量学习问题，并计算每个数据点的类别异常度量。</li>
<li>results: 研究发现，基金分类错误与未来回报之间存在强相关关系，并讨论了这些结果的意义。<details>
<summary>Abstract</summary>
Mutual fund categorization has become a standard tool for the investment management industry and is extensively used by allocators for portfolio construction and manager selection, as well as by fund managers for peer analysis and competitive positioning. As a result, a (unintended) miscategorization or lack of precision can significantly impact allocation decisions and investment fund managers. Here, we aim to quantify the effect of miscategorization of funds utilizing a machine learning based approach. We formulate the problem of miscategorization of funds as a distance-based outlier detection problem, where the outliers are the data-points that are far from the rest of the data-points in the given feature space. We implement and employ a Random Forest (RF) based method of distance metric learning, and compute the so-called class-wise outlier measures for each data-point to identify outliers in the data. We test our implementation on various publicly available data sets, and then apply it to mutual fund data. We show that there is a strong relationship between the outlier measures of the funds and their future returns and discuss the implications of our findings.
</details>
<details>
<summary>摘要</summary>
资金基金分类已成为投资管理行业的标准工具，广泛用于配置股票和选择基金管理人，以及基金管理人对准竞对手的分析和竞争位置。因此，任何不当或精度不够的分类可能会对分配决策产生重大影响，并且对投资基金的管理人也有重要影响。在这种情况下，我们想要量化基金分类错误的影响，并使用机器学习的方法来解决这个问题。我们将基金分类问题定义为一个距离度量学习问题，其中异常值是与其他数据点之间的距离最大的数据点。我们使用随机森林（RF）方法来学习距离度量，并计算每个数据点的类别异常度量来确定异常值。我们在各种公开available的数据集上进行测试，然后应用于基金数据。我们发现基金的异常度量和未来回报之间存在强相关性，并讨论了这些发现的意义。
</details></li>
</ul>
<hr>
<h2 id="AutoSeqRec-Autoencoder-for-Efficient-Sequential-Recommendation"><a href="#AutoSeqRec-Autoencoder-for-Efficient-Sequential-Recommendation" class="headerlink" title="AutoSeqRec: Autoencoder for Efficient Sequential Recommendation"></a>AutoSeqRec: Autoencoder for Efficient Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06878">http://arxiv.org/abs/2308.06878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sliu675/autoseqrec">https://github.com/sliu675/autoseqrec</a></li>
<li>paper_authors: Sijia Liu, Jiahao Liu, Hansu Gu, Dongsheng Li, Tun Lu, Peng Zhang, Ning Gu</li>
<li>for: 这篇论文旨在提出一种适合进行sequential recommendation tasks的增量推荐模型，即AutoSeqRec。</li>
<li>methods: AutoSeqRec 使用 autoencoder 架构，包括一个Encoder和三个Decoder。这些 комponents 考虑了用户-项目互动矩阵和项目转移矩阵的rows和columns。重建用户-项目互动矩阵可以捕捉用户长期偏好，而项目转移矩阵的rows和columns可以表示用户短期的兴趣。</li>
<li>results: 该论文的实验结果显示，AutoSeqRec 在精度方面比较高，并且具有优秀的可靠性和效率。<details>
<summary>Abstract</summary>
Sequential recommendation demonstrates the capability to recommend items by modeling the sequential behavior of users. Traditional methods typically treat users as sequences of items, overlooking the collaborative relationships among them. Graph-based methods incorporate collaborative information by utilizing the user-item interaction graph. However, these methods sometimes face challenges in terms of time complexity and computational efficiency. To address these limitations, this paper presents AutoSeqRec, an incremental recommendation model specifically designed for sequential recommendation tasks. AutoSeqRec is based on autoencoders and consists of an encoder and three decoders within the autoencoder architecture. These components consider both the user-item interaction matrix and the rows and columns of the item transition matrix. The reconstruction of the user-item interaction matrix captures user long-term preferences through collaborative filtering. In addition, the rows and columns of the item transition matrix represent the item out-degree and in-degree hopping behavior, which allows for modeling the user's short-term interests. When making incremental recommendations, only the input matrices need to be updated, without the need to update parameters, which makes AutoSeqRec very efficient. Comprehensive evaluations demonstrate that AutoSeqRec outperforms existing methods in terms of accuracy, while showcasing its robustness and efficiency.
</details>
<details>
<summary>摘要</summary>
sequential recommendation 示示了推荐ITEM的能力，通过模型用户的顺序行为。传统方法通常将用户视为ITEM的序列，忽略了用户之间的协作关系。基于图的方法可以利用用户-ITEM交互图，并 integrable 用户之间的协作信息。然而，这些方法有时会面临时间复杂度和计算效率的限制。为了解决这些限制，本文提出了AutoSeqRec，一种适用于顺序推荐任务的递增推荐模型。AutoSeqRec基于自适应器，包括一个Encoder和三个解码器在自适应器架构中。这些组件考虑了用户-ITEM交互矩阵和用户-ITEM交互矩阵的行列。重建用户-ITEM交互矩阵可以捕捉用户长期的偏好，通过共同筛选。此外，用户-ITEM交互矩阵的行列表示ITEM的出度和入度跳跃行为，可以模型用户短期的兴趣。在进行递增推荐时，只需更新输入矩阵，无需更新参数，这使得AutoSeqRec非常有效率。 comprehensive evaluations 表明AutoSeqRec在准确性方面高于现有方法，同时展现出了其稳定性和效率。
</details></li>
</ul>
<hr>
<h2 id="SpeechX-Neural-Codec-Language-Model-as-a-Versatile-Speech-Transformer"><a href="#SpeechX-Neural-Codec-Language-Model-as-a-Versatile-Speech-Transformer" class="headerlink" title="SpeechX: Neural Codec Language Model as a Versatile Speech Transformer"></a>SpeechX: Neural Codec Language Model as a Versatile Speech Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06873">http://arxiv.org/abs/2308.06873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, Takuya Yoshioka</li>
<li>for: 这篇论文旨在探讨一种能够实现多种语音转文字和语音处理任务的新型语音生成模型。</li>
<li>methods: 该模型使用了语音-文本提示的 Audio-Text 模型，并使用多任务学习和任务取向提示来实现一体化和可扩展的模型。</li>
<li>results: 实验结果表明，该模型在多种任务中表现出色，包括零shot TTS、噪声减少、目标说话人抽取、语音除去和背景噪声下的语音编辑等，并在不同任务中与专门的模型进行比较，表现相对或超过专门的模型。<details>
<summary>Abstract</summary>
Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.
</details>
<details>
<summary>摘要</summary>
近期的生成演说模型，基于音频文本提示，已经实现了高质量的零处理文本识别。然而，现有的模型仍然面临着处理多样化的音频文本演说生成任务的限制，包括转换输入speech和处理陌生频谱条件下的音频捕获。本文介绍SpeechX，一种多功能的演说生成模型，能够零处理TTS和多种演说转换任务，处理干净和噪音信号。SpeechX结合神经编码语言模型和多任务学习，使用任务висимы的提示，实现了一个简单、扩展的模型，并提供了一个通用的方法，用于挖掘文本输入在演说增强和转换任务中的作用。实验结果表明SpeechX在不同任务中具有优秀的表现，包括零处理TTS、噪音抑制、目标说话人EXTRACTION、演说除去和演说编辑等，与专门的模型相比，在任务中表现相似或更好。可以查看https://aka.ms/speechx的示例。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Dual-Stream-Self-Attentive-Adversarial-Graph-Contrastive-Learning-for-Cross-Subject-EEG-based-Emotion-Recognition"><a href="#Semi-Supervised-Dual-Stream-Self-Attentive-Adversarial-Graph-Contrastive-Learning-for-Cross-Subject-EEG-based-Emotion-Recognition" class="headerlink" title="Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition"></a>Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11635">http://arxiv.org/abs/2308.11635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weishan Ye, Zhiguo Zhang, Min Zhang, Fei Teng, Li Zhang, Linling Li, Gan Huang, Jianhong Wang, Dong Ni, Zhen Liang<br>for: 这篇论文是为了解决识别情绪的困难问题，具体来说是使用EEG数据进行情绪识别。methods: 该论文提出了一种半监督的双流自注意力对抗图像对比学习框架（简称DS-AGC），该框架包括两个平行的流程，一个是提取非结构的EEG特征，另一个是提取结构的EEG特征。results: 该论文的实验结果表明，在两个标准数据库（SEED和SEED-IV）上，提出的模型在不同的受测人数据下的 incomplete label 条件下表现出色，比如平均提高5.83%和6.99%。这表明该模型有效地解决了识别情绪的标签稀缺问题。<details>
<summary>Abstract</summary>
Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-Attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentive fusion module is developed for feature fusion, sample selection, and emotion recognition, which highlights EEG features more relevant to emotions and data samples in the labeled source domain that are closer to the target domain. Extensive experiments conducted on two benchmark databases (SEED and SEED-IV) using a semi-supervised cross-subject leave-one-subject-out cross-validation evaluation scheme show that the proposed model outperforms existing methods under different incomplete label conditions (with an average improvement of 5.83% on SEED and 6.99% on SEED-IV), demonstrating its effectiveness in addressing the label scarcity problem in cross-subject EEG-based emotion recognition.
</details>
<details>
<summary>摘要</summary>
电enzephalography (EEG) 是一种客观的表征工具，具有推荐的应用前景。然而，数据标注的缺乏仍然是这个领域的主要挑战，这限制了EEG基于情感认知的广泛应用。在这篇论文中，一种半supervised dual-stream Self-Attentive Adversarial Graph Contrastive learning框架（简称DS-AGC）被提出，以解决跨个体EEG基于情感认知的数据标注稀缺的问题。DS-AGC框架包括两个平行流，用于提取非结构和结构EEG特征。非结构流利用半supervised多元适应方法，以减轻来源领域、无标注领域和目标领域之间的分布差异。结构流发展了图像异常学学习方法，以从多个EEG通道中提取有效的图像特征表示。此外，一个自注意力融合模块被开发，用于特征融合、样本选择和情感认知，强调EEG特征更加关注情感和数据样本在标注领域中的更加相似性。经过对两个参考数据库（SEED和SEED-IV）的 semi-supervised cross-subject leave-one-subject-out cross-validation评估，提出的模型在不同的未完全标注条件下（SEED上提高了5.83%，SEED-IV上提高了6.99%）表现出色，表明它有效地解决了跨个体EEG基于情感认知的数据标注稀缺问题。
</details></li>
</ul>
<hr>
<h2 id="Effect-of-Choosing-Loss-Function-when-Using-T-batching-for-Representation-Learning-on-Dynamic-Networks"><a href="#Effect-of-Choosing-Loss-Function-when-Using-T-batching-for-Representation-Learning-on-Dynamic-Networks" class="headerlink" title="Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks"></a>Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06862">http://arxiv.org/abs/2308.06862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/erfanloghmani/effect-of-loss-function-tbatching">https://github.com/erfanloghmani/effect-of-loss-function-tbatching</a></li>
<li>paper_authors: Erfan Loghmani, MohammadAmin Fazli</li>
<li>for: 这个论文旨在提出新的方法来实现动态network上的表征学习，并且利用时间信息来改善模型的精度和效率。</li>
<li>methods: 本论文使用了T-batching技术来训练动态网络模型，并提出了两种新的损失函数来解决训练损失的问题。</li>
<li>results: 实验结果显示，使用提案的两种损失函数可以超越原始的损失函数，实现更好的训练性能，并且在实际的动态网络上显示了更高的精度和效率。<details>
<summary>Abstract</summary>
Representation learning methods have revolutionized machine learning on networks by converting discrete network structures into continuous domains. However, dynamic networks that evolve over time pose new challenges. To address this, dynamic representation learning methods have gained attention, offering benefits like reduced learning time and improved accuracy by utilizing temporal information.   T-batching is a valuable technique for training dynamic network models that reduces training time while preserving vital conditions for accurate modeling. However, we have identified a limitation in the training loss function used with t-batching. Through mathematical analysis, we propose two alternative loss functions that overcome these issues, resulting in enhanced training performance.   We extensively evaluate the proposed loss functions on synthetic and real-world dynamic networks. The results consistently demonstrate superior performance compared to the original loss function. Notably, in a real-world network characterized by diverse user interaction histories, the proposed loss functions achieved more than 26.9% enhancement in Mean Reciprocal Rank (MRR) and more than 11.8% improvement in Recall@10. These findings underscore the efficacy of the proposed loss functions in dynamic network modeling.
</details>
<details>
<summary>摘要</summary>
“现代学习方法已经革命化机器学习领域中的网络结构，将离散网络结构转化为连续领域。然而，在时间演变的网络上 pose 新的挑战。为 Addressing 这些挑战，动态表示学习方法受到了关注，它们可以利用时间信息来提高模型的准确性和速度。 T-batching 是训练动态网络模型的有价值技术，它可以降低训练时间的同时保持模型的准确性。然而，我们发现了 t-batching 的训练损失函数中的一个限制。通过数学分析，我们提出了两种替代的损失函数，它们可以解决这些问题，从而提高训练性能。我们对 synthetic 和实际的动态网络进行了广泛的评估，结果 consistently 表明了我们提出的损失函数的超越性。特别是在一个实际网络中，其中用户交互历史多样化，我们的提议损失函数可以提高 Mean Reciprocal Rank (MRR) 的值超过 26.9%，并提高 Recall@10 的值超过 11.8%。这些发现讲述了我们提出的损失函数在动态网络模型中的效果。”
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Offensive-Gameplan-in-the-National-Basketball-Association-with-Machine-Learning"><a href="#Optimizing-Offensive-Gameplan-in-the-National-Basketball-Association-with-Machine-Learning" class="headerlink" title="Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning"></a>Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06851">http://arxiv.org/abs/2308.06851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eamon Mukhopadhyay</li>
<li>for: 这篇论文是为了检验篮球metric的有效性而写的。</li>
<li>methods: 这篇论文使用了机器学习技术来模型已有的metric，并选择了一组特有的特征来评估metric的效果。</li>
<li>results: 该论文发现ORTG指数（由Dean Oliver提出）与不同的NBA战术类型有 statistically significant的相关性，但是使用神经网络回归模型而不是线性回归模型表现更好。<details>
<summary>Abstract</summary>
Throughout the analytical revolution that has occurred in the NBA, the development of specific metrics and formulas has given teams, coaches, and players a new way to see the game. However - the question arises - how can we verify any metrics? One method would simply be eyeball approximation (trying out many different gameplans) and/or trial and error - an estimation-based and costly approach. Another approach is to try to model already existing metrics with a unique set of features using machine learning techniques. The key to this approach is that with these features that are selected, we can try to gauge the effectiveness of these features combined, rather than using individual analysis in simple metric evaluation. If we have an accurate model, it can particularly help us determine the specifics of gameplan execution. In this paper, the statistic ORTG (Offensive Rating, developed by Dean Oliver) was found to have a correlation with different NBA playtypes using both a linear regression model and a neural network regression model, although ultimately, a neural network worked slightly better than linear regression. Using the accuracy of the models as a justification, the next step was to optimize the output of the model with test examples, which would demonstrate the combination of features to best achieve a highly functioning offense.
</details>
<details>
<summary>摘要</summary>
在NBA analytics革命中，发展特定指标和公式为球队、教练和球员提供了一种新的视角。然而，问题出现：如何验证这些指标呢？一种方法是通过试错和错误的方式来估算，这是一种估算基于估计的和昂贵的方法。另一种方法是使用机器学习技术来模型已有的指标，并选择一组独特的特征来评估这些指标的效果。如果我们有一个准确的模型，那么它可以帮助我们确定游戏计划执行的 especifics。在这篇论文中，由Dean Oliver开发的ORTG指标（进攻评估指标）与不同的NBA游戏类型之间显示了相关性，使用线性回归模型和神经网络回归模型，其中神经网络模型在精度上略微高一些。使用模型的准确性作为正当化，接下来的步骤是使用测试例子来优化模型的输出，以示出合理的游戏计划执行。
</details></li>
</ul>
<hr>
<h2 id="When-Monte-Carlo-Dropout-Meets-Multi-Exit-Optimizing-Bayesian-Neural-Networks-on-FPGA"><a href="#When-Monte-Carlo-Dropout-Meets-Multi-Exit-Optimizing-Bayesian-Neural-Networks-on-FPGA" class="headerlink" title="When Monte-Carlo Dropout Meets Multi-Exit: Optimizing Bayesian Neural Networks on FPGA"></a>When Monte-Carlo Dropout Meets Multi-Exit: Optimizing Bayesian Neural Networks on FPGA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06849">http://arxiv.org/abs/2308.06849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/os-hxfan/bayesnn_fpga">https://github.com/os-hxfan/bayesnn_fpga</a></li>
<li>paper_authors: Hongxiang Fan, Hao Chen, Liam Castelli, Zhiqiang Que, He Li, Kenneth Long, Wayne Luk</li>
<li>for: 提高 Bayesian Neural Networks（BayesNNs）的实际应用，因为它们的算法复杂性和硬件性能妨碍了它们的应用。</li>
<li>methods: 该文提出了一种基于 Monte-Carlo Dropout（MCD）的多出口 BayesNN，可以实现准确预测，同时具有低算法复杂性。</li>
<li>results: 我们的自动生成的加速器在能效率方面高于 CPU、GPU 和其他现有硬件实现。<details>
<summary>Abstract</summary>
Bayesian Neural Networks (BayesNNs) have demonstrated their capability of providing calibrated prediction for safety-critical applications such as medical imaging and autonomous driving. However, the high algorithmic complexity and the poor hardware performance of BayesNNs hinder their deployment in real-life applications. To bridge this gap, this paper proposes a novel multi-exit Monte-Carlo Dropout (MCD)-based BayesNN that achieves well-calibrated predictions with low algorithmic complexity. To further reduce the barrier to adopting BayesNNs, we propose a transformation framework that can generate FPGA-based accelerators for multi-exit MCD-based BayesNNs. Several novel optimization techniques are introduced to improve hardware performance. Our experiments demonstrate that our auto-generated accelerator achieves higher energy efficiency than CPU, GPU, and other state-of-the-art hardware implementations.
</details>
<details>
<summary>摘要</summary>
bayesian neural networks (bayesNNs) 有能力提供准确的预测，用于安全关键应用程序，如医疗影像和自动驾驶。然而，bayesNNs 的算法复杂度和硬件性能妨碍其在实际应用中部署。为 bridge 这个差距，这篇文章提议一种新的多出口 Monte Carlo Dropout (MCD) 基于的 bayesNN，可以实现准确的预测，同时具有低的算法复杂度。此外，我们还提出了一种转换框架，可以生成 FPGA 加速器，用于multi-exit MCD 基于的 bayesNNs。我们还引入了一些新的优化技术，以提高硬件性能。我们的实验示例，我们自动生成的加速器在能耗效率方面高于 CPU、GPU 和其他现有硬件实现。
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Topological-Graph-Neural-Networks-with-Paths"><a href="#Generalizing-Topological-Graph-Neural-Networks-with-Paths" class="headerlink" title="Generalizing Topological Graph Neural Networks with Paths"></a>Generalizing Topological Graph Neural Networks with Paths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06838">http://arxiv.org/abs/2308.06838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang Truong, Peter Chin</li>
<li>for: 该论文旨在探讨图 neural network (GNN) 在多种领域中的发展，但它们受到一种理论限制，即1-Weisfeiler-Lehmann测试。</li>
<li>methods: 该论文提出了一种以路径为中心的方法，强调图中的路径结构。该方法可以建立更一般的topological视角，并与其他已知的topological领域之间建立联系。</li>
<li>results: 该论文通过对多个benchmark进行测试，发现该方法可以超越之前的技术，在不假设图的子结构的前提下，达到状态艺术性的性能。<details>
<summary>Abstract</summary>
While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.
</details>
<details>
<summary>摘要</summary>
While 图解网络（GNNs）在多个领域取得了 significiant progress, 它们受到一种理论限制，称为一个Weisfeiler-Lehmann测试。 latest advancements in higher-order GNNs可以突破这个boundary，但它们通常围绕某些图Component like cliques or cycles进行设计。 然而，我们的研究采取了不同的方向。 我们强调了路径，这是所有图的内在特征。 我们可以构建一个更通用的topological perspective，并与其他已知的topological domains建立联系。  Interestingly，无需任何图子结构的假设，我们的方法超越了之前在这个领域的技术，在多个标准测试上达到了state-of-the-art性能。
</details></li>
</ul>
<hr>
<h2 id="InTune-Reinforcement-Learning-based-Data-Pipeline-Optimization-for-Deep-Recommendation-Models"><a href="#InTune-Reinforcement-Learning-based-Data-Pipeline-Optimization-for-Deep-Recommendation-Models" class="headerlink" title="InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models"></a>InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08500">http://arxiv.org/abs/2308.08500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kabir Nagrecha, Lingyi Liu, Pablo Delgado, Prasanna Padmanabhan</li>
<li>for: 这 paper 的目的是研究深度学习基于推荐模型（DLRM）的训练方法，以及如何优化这些方法以提高效率和可扩展性。</li>
<li>methods: 这 paper 使用了人工智能的强化学习（RL）算法，以学习训练机器的 CPU 资源分配策略，以提高数据加载并行并发行为。</li>
<li>results: 这 paper 的实验结果表明，使用 InTune 可以在只需几分钟之内构建优化的数据管道配置，并且可以轻松地与现有训练工作流Integrate into existing workflows。 InTune 可以提高在线数据加载速率，从而降低模型执行时间的浪费和提高效率。<details>
<summary>Abstract</summary>
Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- and time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning training jobs are dominated by model execution, the most important factor in DLRM training performance is often online data ingestion.   In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into DLRM training pipeline bottlenecks and challenges. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and to identify shortfalls in existing pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization to adopt. Our studies lead us to design and build a new solution for data pipeline optimization, InTune.   InTune employs a reinforcement learning (RL) agent to learn how to distribute the CPU resources of a trainer machine across a DLRM data pipeline to more effectively parallelize data loading and improve throughput. Our experiments show that InTune can build an optimized data pipeline configuration within only a few minutes, and can easily be integrated into existing training workflows. By exploiting the responsiveness and adaptability of RL, InTune achieves higher online data ingestion rates than existing optimizers, thus reducing idle times in model execution and increasing efficiency. We apply InTune to our real-world cluster, and find that it increases data ingestion throughput by as much as 2.29X versus state-of-the-art data pipeline optimizers while also improving both CPU & GPU utilization.
</details>
<details>
<summary>摘要</summary>
InTune 使用了强化学习（RL）代理来学习如何在 DLRM 数据管道中分配训练机器的 CPU 资源，以更有效地并行数据加载并提高吞吐量。我们的实验表明，InTune 可以在只需几分钟之内构建优化数据管道配置，并且可以轻松地与现有训练工作流 integrate。通过RL的响应和适应性，InTune 可以在现有优化器的基础上提高在线数据接收速率，从而降低模型执行时间的浪费和提高效率。我们对实际集群进行应用，发现 InTune 可以提高数据接收吞吐量达到 2.29 倍，同时提高 CPU 和 GPU 资源利用率。
</details></li>
</ul>
<hr>
<h2 id="An-Ensemble-Approach-to-Question-Classification-Integrating-Electra-Transformer-GloVe-and-LSTM"><a href="#An-Ensemble-Approach-to-Question-Classification-Integrating-Electra-Transformer-GloVe-and-LSTM" class="headerlink" title="An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM"></a>An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06828">http://arxiv.org/abs/2308.06828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanad Aburass, Osama Dorgham</li>
<li>for: 本研究旨在提出一种新的集成方法，用于问题分类，利用现代模型——Electra、GloVe和LSTM。</li>
<li>methods: 该模型使用了Electra、GloVe和LSTM三种现代模型，通过集成这些模型的优势，提供了一种robust和高效的问题分类解决方案。</li>
<li>results: 对于TREC数据集，提出的集成模型在所有评估指标上都超过了BERT、RoBERTa和DistilBERT等其他现代模型，实现了0.8的测试集准确率。这些结果表明集成方法在问题分类任务中具有显著的优势，并且鼓励进一步探索集成方法在自然语言处理领域中的应用。<details>
<summary>Abstract</summary>
This paper introduces a novel ensemble approach for question classification using state-of-the-art models -- Electra, GloVe, and LSTM. The proposed model is trained and evaluated on the TREC dataset, a well-established benchmark for question classification tasks. The ensemble model combines the strengths of Electra, a transformer-based model for language understanding, GloVe, a global vectors for word representation, and LSTM, a recurrent neural network variant, providing a robust and efficient solution for question classification. Extensive experiments were carried out to compare the performance of the proposed ensemble approach with other cutting-edge models, such as BERT, RoBERTa, and DistilBERT. Our results demonstrate that the ensemble model outperforms these models across all evaluation metrics, achieving an accuracy of 0.8 on the test set. These findings underscore the effectiveness of the ensemble approach in enhancing the performance of question classification tasks, and invite further exploration of ensemble methods in natural language processing.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的集成方法 для问题分类，使用当今最佳模型——Electra、GloVe和LSTM。该模型在TREC数据集上进行训练和评估，TREC数据集是问题分类任务的常见 benchmarck。集成模型结合了Electra、GloVe和LSTM的优势，提供了一个可靠和高效的问题分类解决方案。我们进行了广泛的实验，与其他最新的模型，如BERT、RoBERTa和DistilBERT进行比较。我们的结果表明，集成模型在所有评估指标上都超过了这些模型，在测试集上达到了0.8的准确率。这些结果证明了集成方法在问题分类任务中的效iveness，并邀请了进一步的对natural language processing领域中的集成方法进行探索。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Graph-Clustering-with-Unknown-Cluster-Number"><a href="#Reinforcement-Graph-Clustering-with-Unknown-Cluster-Number" class="headerlink" title="Reinforcement Graph Clustering with Unknown Cluster Number"></a>Reinforcement Graph Clustering with Unknown Cluster Number</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06827">http://arxiv.org/abs/2308.06827</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yueliu1999/awesome-deep-graph-clustering">https://github.com/yueliu1999/awesome-deep-graph-clustering</a></li>
<li>paper_authors: Yue Liu, Ke Liang, Jun Xia, Xihong Yang, Sihang Zhou, Meng Liu, Xinwang Liu, Stan Z. Li<br>for: 这个研究旨在提供一个不需要先知cluster number的深度图 clustering方法，并且与对图的不确定性进行适应。methods: 我们提出了一个名为Reinforcement Graph Clustering（RGC）的新方法，它通过强化学习机制让cluster number决定和无监督表现学习融合到一个整体框架中。在我们的方法中，首先learn出具有对称预测任务的描述性node表现，然后考虑 both node和cluster状态，以获得更加准确的图 clustering结果。results: 我们的方法在实验中表现出了优异的效能和效率，并且能够在对图中实现更好的适应性和稳定性。此外，我们还提供了一个包含多种深度图 clustering方法的 коллекции（paper、code和dataset），可以帮助研究人员更好地进行深度图 clustering的研究。<details>
<summary>Abstract</summary>
Deep graph clustering, which aims to group nodes into disjoint clusters by neural networks in an unsupervised manner, has attracted great attention in recent years. Although the performance has been largely improved, the excellent performance of the existing methods heavily relies on an accurately predefined cluster number, which is not always available in the real-world scenario. To enable the deep graph clustering algorithms to work without the guidance of the predefined cluster number, we propose a new deep graph clustering method termed Reinforcement Graph Clustering (RGC). In our proposed method, cluster number determination and unsupervised representation learning are unified into a uniform framework by the reinforcement learning mechanism. Concretely, the discriminative node representations are first learned with the contrastive pretext task. Then, to capture the clustering state accurately with both local and global information in the graph, both node and cluster states are considered. Subsequently, at each state, the qualities of different cluster numbers are evaluated by the quality network, and the greedy action is executed to determine the cluster number. In order to conduct feedback actions, the clustering-oriented reward function is proposed to enhance the cohesion of the same clusters and separate the different clusters. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method. The source code of RGC is shared at https://github.com/yueliu1999/RGC and a collection (papers, codes and, datasets) of deep graph clustering is shared at https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering on Github.
</details>
<details>
<summary>摘要</summary>
深度图 clustering，目标是通过神经网络在无监督情况下将节点分组到不同的分支，在过去几年内吸引了广泛的关注。although 现有的方法已经大幅提高了性能，但是它们依赖于准确预定的分支数量，这在实际场景中并不总是可用。为了使深度图 clustering 算法不受预定分支数量的限制，我们提出了一种新的深度图 clustering 方法，称为奖励图 clustering（RGC）。在我们的提议方法中，集群数量决定和无监督表示学习被统一到一个奖励学习机制中。具体来说，首先通过对比预测任务学习描述性的节点表示。然后，为了准确地捕捉图中的集群状态，包括节点状态和集群状态。在每个状态下，通过质量网络评估不同的分支数量的质量，并执行滥购行动来确定分支数量。为了进行反馈行动，我们提出了一种集群 oriented 奖励函数，以增强同一个集群之间的凝结度和不同集群之间的分离度。我们的实验证明了我们的提议方法的效果和效率。RGC 的源代码可以在 GitHub 上获取：https://github.com/yueliu1999/RGC，而深度图 clustering 相关的代码、论文和数据集可以在 GitHub 上获取：https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering。
</details></li>
</ul>
<hr>
<h2 id="Approximate-and-Weighted-Data-Reconstruction-Attack-in-Federated-Learning"><a href="#Approximate-and-Weighted-Data-Reconstruction-Attack-in-Federated-Learning" class="headerlink" title="Approximate and Weighted Data Reconstruction Attack in Federated Learning"></a>Approximate and Weighted Data Reconstruction Attack in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06822">http://arxiv.org/abs/2308.06822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Wang, Yongcun Song, Enrique Zuazua</li>
<li>for: 本研究旨在攻击 Federated Learning（FL）中 horizontal Federated Averaging（FedAvg）场景中客户端的模型参数共享。</li>
<li>methods: 我们提出了一种 interpolation-based approximation 方法，可以使 fedavg 场景中客户端的模型参数攻击成为可能。此外，我们还设计了一种层Weighted loss function，可以提高数据重建质量。</li>
<li>results: 我们的 approximate and weighted attack（AWA）方法在不同评价指标中均表现出优于现有方法，特别是在图像数据重建中。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed learning paradigm that enables multiple clients to collaborate on building a machine learning model without sharing their private data. Although FL is considered privacy-preserved by design, recent data reconstruction attacks demonstrate that an attacker can recover clients' training data based on the parameters shared in FL. However, most existing methods fail to attack the most widely used horizontal Federated Averaging (FedAvg) scenario, where clients share model parameters after multiple local training steps. To tackle this issue, we propose an interpolation-based approximation method, which makes attacking FedAvg scenarios feasible by generating the intermediate model updates of the clients' local training processes. Then, we design a layer-wise weighted loss function to improve the data quality of reconstruction. We assign different weights to model updates in different layers concerning the neural network structure, with the weights tuned by Bayesian optimization. Finally, experimental results validate the superiority of our proposed approximate and weighted attack (AWA) method over the other state-of-the-art methods, as demonstrated by the substantial improvement in different evaluation metrics for image data reconstructions.
</details>
<details>
<summary>摘要</summary>
Федератированное обучение (FL) 是一种分布式学习 paradigma，允许多个客户端共同建立一个机器学习模型，无需共享其私人数据。虽然 FL 被视为隐私保护的设计，但最近的数据重建攻击表明，攻击者可以根据在 FL 中共享的参数重建客户端的训练数据。然而，现有方法大多不能攻击最常用的水平 Federated Averaging（FedAvg）场景，在这种场景下，客户端在多个本地训练步骤后共享模型参数。为解决这个问题，我们提出了一种 interpolating-based 方法，可以在 FedAvg 场景中生成客户端的本地训练过程中的中间模型更新。然后，我们设计了层wise 权重损失函数，以提高重建数据的质量。我们对模型更新在不同层中分配不同权重，并通过 Bayesian 优化调整这些权重。最后，我们对 AWA 方法进行实验 validate，并证明其在不同评价指标上具有明显的提高。
</details></li>
</ul>
<hr>
<h2 id="SoK-Realistic-Adversarial-Attacks-and-Defenses-for-Intelligent-Network-Intrusion-Detection"><a href="#SoK-Realistic-Adversarial-Attacks-and-Defenses-for-Intelligent-Network-Intrusion-Detection" class="headerlink" title="SoK: Realistic Adversarial Attacks and Defenses for Intelligent Network Intrusion Detection"></a>SoK: Realistic Adversarial Attacks and Defenses for Intelligent Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06819">http://arxiv.org/abs/2308.06819</a></li>
<li>repo_url: None</li>
<li>paper_authors: João Vitorino, Isabel Praça, Eva Maia</li>
<li>For: The paper is written to provide a comprehensive overview of the state-of-the-art adversarial learning approaches for realistic example generation in the context of Network Intrusion Detection (NID) using machine learning (ML) models.* Methods: The paper consolidates and summarizes various adversarial attack methods and defense strategies, specifically tailored for the NID domain and realistic network traffic flows.* Results: The paper identifies open challenges and fundamental properties required for realistic adversarial examples in NID, providing guidelines for future research to ensure adequacy for real communication networks.Here’s the same information in Simplified Chinese text:* For: 这篇论文是为了提供网络入侵检测（NID）领域中机器学习（ML）模型的现状摘要，包括最新的敌对学习方法和防御策略。* Methods: 论文总结了各种敌对攻击方法和防御策略，特别适用于NID领域和真实的网络流量。* Results: 论文描述了NID领域中敌对学习模型的开放挑战和基本要求，并提供了未来研究的指导方针，以确保实际网络通信的合理性。<details>
<summary>Abstract</summary>
Machine Learning (ML) can be incredibly valuable to automate anomaly detection and cyber-attack classification, improving the way that Network Intrusion Detection (NID) is performed. However, despite the benefits of ML models, they are highly susceptible to adversarial cyber-attack examples specifically crafted to exploit them. A wide range of adversarial attacks have been created and researchers have worked on various defense strategies to safeguard ML models, but most were not intended for the specific constraints of a communication network and its communication protocols, so they may lead to unrealistic examples in the NID domain. This Systematization of Knowledge (SoK) consolidates and summarizes the state-of-the-art adversarial learning approaches that can generate realistic examples and could be used in real ML development and deployment scenarios with real network traffic flows. This SoK also describes the open challenges regarding the use of adversarial ML in the NID domain, defines the fundamental properties that are required for an adversarial example to be realistic, and provides guidelines for researchers to ensure that their future experiments are adequate for a real communication network.
</details>
<details>
<summary>摘要</summary>
This Systematization of Knowledge (SoK) consolidates and summarizes the state-of-the-art adversarial learning approaches that can generate realistic examples and can be used in real ML development and deployment scenarios with real network traffic flows. This SoK also identifies the open challenges regarding the use of adversarial ML in the NID domain, defines the fundamental properties that are required for an adversarial example to be realistic, and provides guidelines for researchers to ensure that their future experiments are adequate for a real communication network.
</details></li>
</ul>
<hr>
<h2 id="SAILOR-Structural-Augmentation-Based-Tail-Node-Representation-Learning"><a href="#SAILOR-Structural-Augmentation-Based-Tail-Node-Representation-Learning" class="headerlink" title="SAILOR: Structural Augmentation Based Tail Node Representation Learning"></a>SAILOR: Structural Augmentation Based Tail Node Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06801">http://arxiv.org/abs/2308.06801</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jie-re/sailor">https://github.com/jie-re/sailor</a></li>
<li>paper_authors: Jie Liao, Jintang Li, Liang Chen, Bingzhe Wu, Yatao Bian, Zibin Zheng</li>
<li>for: 提高链接结构中tail节点的表示性</li>
<li>methods: 提出了一种基于 структур增强的tail节点表示学习框架，名为SAILOR</li>
<li>results: 对公共评估数据进行了广泛的实验，显示SAILOR可以显著提高tail节点的表示性，并超越当前的基准值<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved state-of-the-art performance in representation learning for graphs recently. However, the effectiveness of GNNs, which capitalize on the key operation of message propagation, highly depends on the quality of the topology structure. Most of the graphs in real-world scenarios follow a long-tailed distribution on their node degrees, that is, a vast majority of the nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes since they lack structural information. In the pursuit of promoting the expressiveness of GNNs for tail nodes, we explore how the deficiency of structural information deteriorates the performance of tail nodes and propose a general Structural Augmentation based taIL nOde Representation learning framework, dubbed as SAILOR, which can jointly learn to augment the graph structure and extract more informative representations for tail nodes. Extensive experiments on public benchmark datasets demonstrate that SAILOR can significantly improve the tail node representations and outperform the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 在最近的表示学习中达到了状态的极品性表现。然而，GNNS的效果，它们基于消息传递操作，强度取决于图结构的质量。大多数实际场景中的图follows a long-tailed distribution on node degrees, that is, most nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes due to the lack of structural information. 为了提高GNNS的表达能力 для尾节点，我们研究了尾节点表示力下降的原因和提出了一种通用的结构扩充based taIL node representation learning框架，名为SAILOR，可以同时学习扩充图结构和提取更有用的尾节点表示。我们在公共 benchmark datasets上进行了广泛的实验，显示SAILOR可以显著提高尾节点表示和超越状态的基eline。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.LG_2023_08_14/" data-id="cllurrpac0064sw882j8v3gxo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.SD_2023_08_14/" class="article-date">
  <time datetime="2023-08-13T16:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/cs.SD_2023_08_14/">cs.SD - 2023-08-14 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Integrating-Emotion-Recognition-with-Speech-Recognition-and-Speaker-Diarisation-for-Conversations"><a href="#Integrating-Emotion-Recognition-with-Speech-Recognition-and-Speaker-Diarisation-for-Conversations" class="headerlink" title="Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations"></a>Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07145">http://arxiv.org/abs/2308.07145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/w-wu/steer">https://github.com/w-wu/steer</a></li>
<li>paper_authors: Wen Wu, Chao Zhang, Philip C. Woodland</li>
<li>for: 提高自动情感识别（AER）的精度和效果，使其能够在对话系统中应用。</li>
<li>methods:  integrate AER with automatic speech recognition（ASR）和speaker diarisation（SD），共同训练一个系统，并使用分布式编码器建立不同的输出层。</li>
<li>results: 在IEMOCAP dataset上进行测试，提议的系统与两个基准系统相比，在AER、ASR和SD三个任务中均表现出色，并且在时间权重 emotions 和 speaker classification 错误上采用了两种评价指标。<details>
<summary>Abstract</summary>
Although automatic emotion recognition (AER) has recently drawn significant research interest, most current AER studies use manually segmented utterances, which are usually unavailable for dialogue systems. This paper proposes integrating AER with automatic speech recognition (ASR) and speaker diarisation (SD) in a jointly-trained system. Distinct output layers are built for four sub-tasks including AER, ASR, voice activity detection and speaker classification based on a shared encoder. Taking the audio of a conversation as input, the integrated system finds all speech segments and transcribes the corresponding emotion classes, word sequences, and speaker identities. Two metrics are proposed to evaluate AER performance with automatic segmentation based on time-weighted emotion and speaker classification errors. Results on the IEMOCAP dataset show that the proposed system consistently outperforms two baselines with separately trained single-task systems on AER, ASR and SD.
</details>
<details>
<summary>摘要</summary>
尽管自动情感识别（AER）在最近几年内受到了广泛的研究兴趣，但大多数当前AER研究使用手动分割的语音，这些语音通常不可用于对话系统。这篇论文提议将AER、自动语音识别（ASR）和 speaker分类（SD）集成为一个集成系统。该系统使用共享Encoder生成了四个子任务的特征输出层，包括AER、ASR、语音活动检测和 speaker分类。将对话的音频作为输入，该集成系统可以找到所有的语音段落，并将对应的情感类别、词序列和Speaker标识转化为文本。为评估AER性能，提出了两种指标，即基于时间权重的情感错误和Speaker错误。results表明，提议的系统在IEMOCAP dataset上比基eline两个独立的单任务系统在AER、ASR和SD领域具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="VoxBlink-X-Large-Speaker-Verification-Dataset-on-Camera"><a href="#VoxBlink-X-Large-Speaker-Verification-Dataset-on-Camera" class="headerlink" title="VoxBlink: X-Large Speaker Verification Dataset on Camera"></a>VoxBlink: X-Large Speaker Verification Dataset on Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07056">http://arxiv.org/abs/2308.07056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuke Lin, Xiaoyi Qin, Ming Cheng, Ning Jiang, Guoqing Zhao, Ming Li</li>
<li>for: 本研究做出了一个新的和广泛的语音认可数据集，包括噪音38k个标识&#x2F;1.45M次语音（VoxBlink）和相对干净的18k个标识&#x2F;1.02M次语音（VoxBlink-Clean） для训练。</li>
<li>methods: 我们首先建立了一个自动化和可扩展的数据提取管道，从YouTube上下载了60,000个用户的短视频，并从这些视频中自动提取了相关的语音和视频段落。</li>
<li>results: 我们的实验结果表明，将VoxBlink-Clean数据集用于训练，可以提高语音认可性能，比如13%-30%的提升，不同的后向架构之间。这个数据集即将公开发布。<details>
<summary>Abstract</summary>
In this paper, we contribute a novel and extensive dataset for speaker verification, which contains noisy 38k identities/1.45M utterances (VoxBlink) and relatively cleaned 18k identities/1.02M (VoxBlink-Clean) utterances for training. Firstly, we accumulate a 60K+ users' list with their avatars and download their short videos on YouTube. We then established an automatic and scalable pipeline to extract relevant speech and video segments from these videos. To our knowledge, the VoxBlink dataset is one of the largest speaker recognition datasets available. Secondly, we conduct a series of experiments based on different backbones trained on a mix of the VoxCeleb2 and the VoxBlink-Clean. Our findings highlight a notable performance improvement, ranging from 13% to 30%, across different backbone architectures upon integrating our dataset for training. The dataset will be made publicly available shortly.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了一个新的和广泛的说话人验证数据集，包括噪音38k个人/1.45万个语音（VoxBlink）和相对清晰的18k个人/1.02万个语音（VoxBlink-Clean） для训练。首先，我们积累了60,000个用户的名单和他们的aviator，然后下载了YouTube上的短视频。我们然后建立了一个自动化和可扩展的管道，以提取视频和语音段落。根据我们所知，VoxBlink数据集是目前最大的说话人识别数据集之一。其次，我们进行了基于不同的后准据体系的实验，发现在将我们的数据集用于训练时，其性能提升范围为13%到30%。这些数据将在不久的将来公开。
</details></li>
</ul>
<hr>
<h2 id="Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder"><a href="#Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder" class="headerlink" title="Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder"></a>Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08488">http://arxiv.org/abs/2308.08488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mispchallenge/misp-icme-avsr">https://github.com/mispchallenge/misp-icme-avsr</a></li>
<li>paper_authors: Yusheng Dai, Hang Chen, Jun Du, Xiaofei Ding, Ning Ding, Feijun Jiang, Chin-Hui Lee</li>
<li>for: 本研究旨在提高自动语音识别系统的音视频联合识别系统（AVSR）性能，并在预训练和精度调整框架下实现这一目标。</li>
<li>methods: 本研究提出了两种新技术来提高AVSR的性能，包括利用叙述形态学生 lip shapes 和 syllable-level subword units 的相关性来确定准确的帧级句子界限，以及使用主要训练参数进行多个跨Modal的注意力层来充分利用多Modal的共轭性。</li>
<li>results: 实验结果表明，使用这两种技术可以提高AVSR系统的性能，并在MISP2021-AVSR数据集上达到比 estado-of-the-art 系统更高的性能水平，使用的训练数据量也相对较少。<details>
<summary>Abstract</summary>
In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the MISP2021-AVSR data set show the effectiveness of the two proposed techniques. Together, using only a relatively small amount of training data, the final system achieves better performances than state-of-the-art systems with more complex front-ends and back-ends.
</details>
<details>
<summary>摘要</summary>
近期研究发现，自动语音识别系统到Audio-Visual语音识别系统在端到端框架下有轻微的性能提升，但是存在不匹配的协调速率和特殊的输入表示之间的问题。在这篇论文中，我们提出了两种新的技巧来提高Audio-Visual语音识别（AVSR）在预训练和精度调整训练框架下。首先，我们探索了拼音和字节水平的叙述单元之间的相关性，以确定良好的帧级叙述边界。这使得视频和音频流之间的对齐变得精准，从而提高了视频和音频流之间的混合。其次，我们提出了一种受主要训练参数 guideline的Audio-Visual混合抽象Encoder（CMFE）神经网络，以便在多个跨模态扩散层中使用主要训练参数，以便充分利用多模态的共轭性。实验表明，使用这两种技巧可以提高系统的性能，并且只需使用相对较少的训练数据。最终系统可以在与更复杂的前端和后端的系统相比，达到更好的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Sound-Demixing-Challenge-2023-unicode-x2013-Cinematic-Demixing-Track"><a href="#The-Sound-Demixing-Challenge-2023-unicode-x2013-Cinematic-Demixing-Track" class="headerlink" title="The Sound Demixing Challenge 2023 $\unicode{x2013}$ Cinematic Demixing Track"></a>The Sound Demixing Challenge 2023 $\unicode{x2013}$ Cinematic Demixing Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06981">http://arxiv.org/abs/2308.06981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Uhlich, Giorgio Fabbro, Masato Hirano, Shusuke Takahashi, Gordon Wichern, Jonathan Le Roux, Dipam Chakraborty, Sharada Mohanty, Kai Li, Yi Luo, Jianwei Yu, Rongzhi Gu, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Mikhail Sukhovei, Yuki Mitsufuji</li>
<li>for: 这篇论文描述了2023年 зву隔离挑战（SDX’23）的电影幂分融合（CDX）轨迹。</li>
<li>methods: 论文详细介绍了比赛的结构和使用的数据集，特别是新构建的CDXDB23隐藏数据集，以及参与者所采用的最成功的方法。</li>
<li>results: 相比干杯餐 fork基线，专门在 simulated Divide and Remaster（DnR）数据集上训练的系统得到了1.8dB的SDR提升，而开放排行榜上的最佳系统则看到了5.7dB的显著提升。<details>
<summary>Abstract</summary>
This paper summarizes the cinematic demixing (CDX) track of the Sound Demixing Challenge 2023 (SDX'23). We provide a comprehensive summary of the challenge setup, detailing the structure of the competition and the datasets used. Especially, we detail CDXDB23, a new hidden dataset constructed from real movies that was used to rank the submissions. The paper also offers insights into the most successful approaches employed by participants. Compared to the cocktail-fork baseline, the best-performing system trained exclusively on the simulated Divide and Remaster (DnR) dataset achieved an improvement of 1.8dB in SDR whereas the top performing system on the open leaderboard, where any data could be used for training, saw a significant improvement of 5.7dB.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了2023年 зву频分离挑战（SDX'23）的电影式分离（CDX）轨迹。我们提供了竞赛设置的完整摘要，包括竞赛结构和使用的数据集。特别是，我们详细介绍了CDXDB23，一个新的隐藏数据集，从真实电影中构建而成，用于评估参赛系统的表现。文章还提供了参与者采用的最成功方法的折衔。相比干杯叉基线，专门在 simulate 的 Divide and Remaster（DnR）数据集上训练的系统得到了1.8dB的SDR提升，而在开放排行榜上，任何数据可以用于训练的系统则得到了显著的5.7dB的提升。
</details></li>
</ul>
<hr>
<h2 id="The-Sound-Demixing-Challenge-2023-unicode-x2013-Music-Demixing-Track"><a href="#The-Sound-Demixing-Challenge-2023-unicode-x2013-Music-Demixing-Track" class="headerlink" title="The Sound Demixing Challenge 2023 $\unicode{x2013}$ Music Demixing Track"></a>The Sound Demixing Challenge 2023 $\unicode{x2013}$ Music Demixing Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06979">http://arxiv.org/abs/2308.06979</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zfturbo/mvsep-mdx23-music-separation-model">https://github.com/zfturbo/mvsep-mdx23-music-separation-model</a></li>
<li>paper_authors: Giorgio Fabbro, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Martínez-Ramírez, Weihsiang Liao, Igor Gadelha, Geraldo Ramos, Eddie Hsu, Hugo Rodrigues, Fabian-Robert Stöter, Alexandre Défossez, Yi Luo, Jianwei Yu, Dipam Chakraborty, Sharada Mohanty, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Nabarun Goswami, Tatsuya Harada, Minseok Kim, Jun Hyung Lee, Yuanliang Dong, Xinran Zhang, Jiafeng Liu, Yuki Mitsufuji</li>
<li>for: 这篇论文描述了Sound Demixing Challenge（SDX’23）的音乐分离（MDX）轨迹。</li>
<li>methods: 论文介绍了MDX系统在训练数据中出现错误的情况下的训练方法，并提出了一种对MDX系统训练数据设计的错误形式化。</li>
<li>results: 论文描述了SDXDB23_LabelNoise和SDXDB23_Bleeding1两个新的数据集，以及在SDX’23中获得最高分的方法。此外，论文还对上一届音乐分离比赛（Music Demixing Challenge 2021）的赛果进行了直接比较，发现当用MDXDB21进行评估时，最佳实现在标准MSS形式下获得了1.6dB的信号至噪声比提高。此外，论文还进行了基于人类听觉评价的听测，并对系统的感知质量进行了报告。最后，论文提供了比赛组织方式的反思和未来版本的展望。<details>
<summary>Abstract</summary>
This paper summarizes the music demixing (MDX) track of the Sound Demixing Challenge (SDX'23). We provide a summary of the challenge setup and introduce the task of robust music source separation (MSS), i.e., training MSS models in the presence of errors in the training data. We propose a formalization of the errors that can occur in the design of a training dataset for MSS systems and introduce two new datasets that simulate such errors: SDXDB23_LabelNoise and SDXDB23_Bleeding1. We describe the methods that achieved the highest scores in the competition. Moreover, we present a direct comparison with the previous edition of the challenge (the Music Demixing Challenge 2021): the best performing system under the standard MSS formulation achieved an improvement of over 1.6dB in signal-to-distortion ratio over the winner of the previous competition, when evaluated on MDXDB21. Besides relying on the signal-to-distortion ratio as objective metric, we also performed a listening test with renowned producers/musicians to study the perceptual quality of the systems and report here the results. Finally, we provide our insights into the organization of the competition and our prospects for future editions.
</details>
<details>
<summary>摘要</summary>
In Simplified Chinese:这篇文章介绍了Sound Demixing Challenge（SDX'23）的音乐分离（MDX）轨迹，包括音乐来源分离（MSS）的Robust Training数据集的设计和两个新的数据集：SDXDB23_LabelNoise和SDXDB23_Bleeding1。文章介绍了在比赛中得分最高的方法，并对上一届音乐分离挑战（Music Demixing Challenge 2021）的赛果进行比较。结果显示，使用标准MSS形式化的最佳系统在MDXDB21上的信号至噪比高于上一届赛事的冠军的赛果。此外，文章还执行了由知名的制作人/音乐人组织的听力测试，以研究系统的主观质量。最后，文章提供了比赛组织和未来版本的前景。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.SD_2023_08_14/" data-id="cllurrpb40091sw882901dk8u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/6/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
